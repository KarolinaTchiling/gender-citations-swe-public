FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Pang, HB
   Han, LX
   Liu, CM
   Ma, RQ
AF Pang, Haibo
   Han, Linxuan
   Liu, Chengming
   Ma, Rongqi
TI Siamese object tracking based on multi-frequency enhancement feature
SO VISUAL COMPUTER
LA English
DT Article
DE Target tracking; Siamese net; Multi-frequency; Dense net
AB Tracker based on the Siamese network is an important research direction of target tracking. However, Siamese trackers have the problem of insufficient characteristic expression ability extensively. To improve the situation, we introduced the effective attention module multi-frequency attention, which considers the attention mechanism from the frequency domain perspective and effectively raises the efficiency of feature representation of the tracker. Meanwhile, densely connected neural networks are applied to the tracker, which integrates the surface orientation information and deep-seated semantic features about the object, which will contribute to enhancing the localization and expression ability of the tracker. The proposed method achieves excellent results on Siamese network tracking. In order to prove the effectiveness of our strategy, we conducted experiments on benchmarks, including OTB100, OTB2013, VOT2016, and VOT2018, and the obtained results indicate that strategy we proposed achieves high accuracy and efficiency and still keeps outstanding with the effect of occlusion and illumination. Experiments show that the proposed method fulfills real-time tracking.
C1 [Pang, Haibo; Han, Linxuan; Liu, Chengming; Ma, Rongqi] Zhengzhou Univ, Sch Cyber Sci & Engn, Zhengzhou 450000, Peoples R China.
C3 Zhengzhou University
RP Liu, CM (corresponding author), Zhengzhou Univ, Sch Cyber Sci & Engn, Zhengzhou 450000, Peoples R China.
EM cmliu@zzu.edu.cn
RI Zhang, Can/JUU-9511-2023
FU National Key Research and Development Program of China [2020YFB1712401,
   2018******4402]
FX Funding was provided by National Key Research and Development Program of
   China. Grant Numbers (2020YFB1712401, 2018******4402).
CR Abbass MY, 2021, VISUAL COMPUT, V37, P831, DOI 10.1007/s00371-020-01833-5
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.465
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Chen YB, 2001, PROCEEDINGS OF THE 44TH IEEE 2001 MIDWEST SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOLS 1 AND 2, P910, DOI 10.1109/MWSCAS.2001.986335
   Chen Z, 2020, PROC INT CONF PARAL, DOI 10.1145/3404397.3404399
   Chu LT, 2019, IEEE INT CONF INDUST, P893, DOI 10.1109/ICIT.2019.8755125
   Cleary K, 2010, ANNU REV BIOMED ENG, V12, P119, DOI 10.1146/annurev-bioeng-070909-105249
   Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   He AF, 2018, PROC CVPR IEEE, P4834, DOI 10.1109/CVPR.2018.00508
   He Kaiming, 2016, P INT C COMP VIS PAT, DOI 10.1109/CVPR.2016.90
   Held D, 2016, LECT NOTES COMPUT SC, V9905, P749, DOI 10.1007/978-3-319-46448-0_45
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Jaimes A, 2007, COMPUT VIS IMAGE UND, V108, P116, DOI 10.1016/j.cviu.2006.10.019
   Jie H., 2017, PROC CVPR IEEE, VPP
   Kamijo S., 2000, IEEE Transactions on Intelligent Transportation Systems, V1, P108, DOI 10.1109/6979.880968
   Levinson J, 2011, IEEE INT VEH SYM, P163, DOI 10.1109/IVS.2011.5940562
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li Y., 2019, ARXIV
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Nam H., 2016, Modeling and propagating CNNs in a tree structure for visual tracking
   Patel H.A., 2013, Int. J. Comput. Sci. Mob. Comput, V2, P326, DOI [DOI 10.1109/ICACCT.2018.8529402, DOI 10.1016/J.JVCIR.2006.03.004]
   Pu S, 2018, ADV NEUR IN, V31
   Qin ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P763, DOI 10.1109/ICCV48922.2021.00082
   Valera M, 2005, IEE P-VIS IMAGE SIGN, V152, P192, DOI 10.1049/ip-vis:20041147
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Wang Q., 2017, ARXIV PREPRINT ARXIV
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yang K., 2021, IEEE Trans. Multimedia
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Zhang WC, 2021, VISUAL COMPUT, V37, P881, DOI 10.1007/s00371-020-01839-z
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882
   Zolfaghari M, 2020, VISUAL COMPUT, V36, P701, DOI 10.1007/s00371-019-01652-3
NR 50
TC 3
Z9 3
U1 3
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 261
EP 271
DI 10.1007/s00371-023-02779-0
EA FEB 2023
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000940278500001
DA 2024-07-18
ER

PT J
AU Nawaz, M
   Javed, A
   Irtaza, A
AF Nawaz, Marriam
   Javed, Ali
   Irtaza, Aun
TI ResNet-Swish-Dense54: a deep learning approach for deepfakes detection
SO VISUAL COMPUTER
LA English
DT Article
DE Deepfakes; Deep learning; Swish; Visual manipulation; ResNet50
ID IMAGES
AB Development in artificial intelligence has brought a new revolution to technologies and approaches that have been employed for malicious purposes specifically after the introduction of generative adversarial networks (GANs) in 2014. GANs are empowered of generating fake visual samples with high realism. Several refined ML-based methods can produce highly realistic deepfakes videos that can be employed for harassing and blackmailing people. Moreover, deepfakes have introduced political stress by navigating disinformation which can result in societal, and political encounters. The prevailing situation has induced a severe danger to the privacy of humans and thus, urged for the introduction of automated approaches to identify deepfakes. In the presented approach, we have used deep learning (DL)-based approach namely ResNet-Swish-Dense54 for reliable and accurate detection of deepfakes. Initially, human faces are extracted from input video frames. Then, the extracted faces are passed to the ResNet-Swish-Dense54 model to perform the content classification as being real or manipulated. We have evaluated our model over the challenging datasets namely DFDC, FaceForensic++, and CelebDF datasets, and confirmed the robustness of the proposed approach through experimentation. Moreover, we have evaluated our approach for adversarial attacks and proved the explainability power of the ResNet-Swish-Dense54 model by generating heatmaps and performing cross-dataset validation. Both the quantitative and qualitative results demonstrated the effectiveness of our approach for visual manipulation detection.
C1 [Nawaz, Marriam; Javed, Ali] UET Taxila, Dept Software Engn, Taxila 47050, Pakistan.
   [Irtaza, Aun] UET Taxila, Dept Comp Sci, Taxila 47050, Pakistan.
RP Nawaz, M (corresponding author), UET Taxila, Dept Software Engn, Taxila 47050, Pakistan.
EM marriam.nawaz@uettaxila.edu.pk
RI Irtaza, Aun/HTP-2773-2023; Nawaz, Marriam/JVD-9229-2023; JAVED,
   ALI/X-3334-2019
OI Irtaza, Aun/0000-0001-7757-5839; 
FU Punjab Higher Education Commission (PHEC) of Pakistan
   [PHEC/ARA/PIRCA/20527/21]
FX This work was supported by the grant of the Punjab Higher Education
   Commission (PHEC) of Pakistan via Award No. (PHEC/ARA/PIRCA/20527/21).
CR Afchar D, 2018, IEEE INT WORKS INFOR
   Amerini I, 2019, IEEE INT CONF COMP V, P1205, DOI 10.1109/ICCVW.2019.00152
   [Anonymous], 2021, SOUND FORGE
   [Anonymous], 2020, REFACE APP
   [Anonymous], 2022, DEEPFAKES GITHUB
   Ballester P, 2016, AAAI CONF ARTIF INTE, P1124
   Baltrusaitis T, 2016, IEEE WINT CONF APPL
   Boylan J.F., 2018, NEW YORK TIMES   OCT, V17
   Carlini N, 2020, IEEE COMPUT SOC CONF, P2804, DOI 10.1109/CVPRW50498.2020.00337
   Carvalho T, 2017, 2017 16TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P866, DOI 10.1109/ICMLA.2017.00-47
   Chan C, 2019, IEEE I CONF COMP VIS, P5932, DOI 10.1109/ICCV.2019.00603
   Chen ZK, 2021, PROC CVPR IEEE, P9010, DOI 10.1109/CVPR46437.2021.00890
   Chintha A, 2020, IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2020), DOI 10.1109/ijcb48548.2020.9304936
   Ciftci Umur Aybars, 2020, IEEE Trans Pattern Anal Mach Intell, VPP, DOI 10.1109/TPAMI.2020.3009287
   Couillaud J, 2020, VISUAL COMPUT, V36, P237, DOI 10.1007/s00371-018-1599-2
   Dolhansky B., 2020, arXiv
   Ferreira CA, 2018, LECT NOTES COMPUT SC, V10882, P763, DOI 10.1007/978-3-319-93000-8_86
   Fydanaki A, 2018, FOREN SCI RES, V3, P202, DOI 10.1080/20961790.2018.1523703
   Gandhi A, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207034
   Ganguly S, 2022, EXPERT SYST APPL, V210, DOI 10.1016/j.eswa.2022.118423
   Guera D., 2019, ARXIV
   Gupta S, 2021, VISUAL COMPUT, V37, P447, DOI 10.1007/s00371-020-01814-8
   Harwell D., 2018, Washington Post
   He D, 2023, VISUAL COMPUT, V39, P1423, DOI 10.1007/s00371-022-02420-6
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hernandez-Ortega J., 2020, arXiv
   Hussain S., 2021, DIGIT THREATS RES PR
   Hussain S, 2021, IEEE WINT CONF APPL, P3347, DOI 10.1109/WACV48630.2021.00339
   Jack K., Video Demystified: A Handbook for the Digital Engineer, P577
   Jung T, 2020, IEEE ACCESS, V8, P83144, DOI 10.1109/ACCESS.2020.2988660
   Khan AR, 2021, MICROSC RES TECHNIQ, V84, P2666, DOI 10.1002/jemt.23816
   Kolagati S., 2022, INT J INF MANAG DATA, V2, P100054
   Koonce B., 2021, Convolutional Neural Networks with Swift for Tensorflow: Image Recognition and Dataset Categorization, P51, DOI 10.1007/978-1-4842-6168-2_5
   Kreso I., 2018, ARXIV
   Kusniadi Imam, 2021, 2021 4th International Conference on Information and Communications Technology (ICOIACT), P104, DOI 10.1109/ICOIACT53268.2021.9563923
   Li YZ, 2017, ADV NEUR IN, V30
   Trinh L, 2021, IEEE WINT CONF APPL, P1972, DOI 10.1109/WACV48630.2021.00202
   Mahmood MT, 2008, MICROSC RES TECHNIQ, V71, P897, DOI 10.1002/jemt.20635
   Marques G, 2020, APPL SOFT COMPUT, V96, DOI 10.1016/j.asoc.2020.106691
   Masood M., 2021, ARXIV
   Masoodi M, 2021, INT SYMP TECHNOL SOC, DOI 10.1109/ISTAS52410.2021.9629182
   Mehta V, 2021, 26TH INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES (IUI '21 COMPANION), P61, DOI 10.1145/3397482.3450726
   Nawaz M, 2021, CMC-COMPUT MATER CON, V69, P1927, DOI 10.32604/cmc.2021.018052
   Nawaz M, 2021, J INTELL FUZZY SYST, V40, P10351, DOI 10.3233/JIFS-191700
   Nawaz M, 2021, MULTIMED TOOLS APPL, V80, P28953, DOI 10.1007/s11042-021-11120-7
   Nazir Tahira, 2021, Proceedings of 2021 International Conference on Artificial Intelligence (ICAI), P33, DOI 10.1109/ICAI52203.2021.9445228
   Nguyen HH, 2019, INT CONF ACOUST SPEE, P2307, DOI 10.1109/ICASSP.2019.8682602
   Palotas AB, 1996, MICROSC RES TECHNIQ, V33, P266, DOI 10.1002/(SICI)1097-0029(19960215)33:3<266::AID-JEMT4>3.0.CO;2-O
   Pan ZG, 2021, MULTIMEDIA SYST, V27, P353, DOI 10.1007/s00530-021-00756-y
   Patwardhan N., 2018, arXiv
   Ranjan P, 2020, 2020 3RD INTERNATIONAL CONFERENCE ON INFORMATION AND COMPUTER TECHNOLOGIES (ICICT 2020), P86, DOI 10.1109/ICICT50521.2020.00021
   Ranjan R, 2019, IEEE T PATTERN ANAL, V41, P121, DOI 10.1109/TPAMI.2017.2781233
   Rossi A, 2020, IEEE T INTELL TRANSP, V21, P2980, DOI 10.1109/TITS.2019.2922002
   Roy R, 2022, 3D CNN ARCHITECTURES
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Saxen F, 2019, INT SYMP IMAGE SIG, P176, DOI 10.1109/ISPA.2019.8868585
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Setiaji H, 2018, IOP CONF SER-MAT SCI, V325, DOI 10.1088/1757-899X/325/1/012005
   Soukupova T., 2016, 21 COMPUTER VISION W
   Sun ZK, 2021, PROC CVPR IEEE, P3608, DOI 10.1109/CVPR46437.2021.00361
   Theckedath D., 2020, SN COMPUT SCI, V1, P1
   Theobalt Chris- tian, 2016, P IEEE C COMPUTER VI, DOI DOI 10.1109/CVPR.2016.262
   Thies J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323035
   Tyagi S, 2023, VISUAL COMPUT, V39, P813, DOI 10.1007/s00371-021-02347-4
   Vinolin V, 2021, VISUAL COMPUT, V37, P2369, DOI 10.1007/s00371-020-01992-5
   Wang C, 2019, IEEE ACCESS, V7, P146533, DOI 10.1109/ACCESS.2019.2946000
   Xia XL, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC 2017), P783, DOI 10.1109/ICIVC.2017.7984661
   Xu Y, 2022, IEEE WINT CONF APPL, P379, DOI 10.1109/WACVW54805.2022.00044
   Yang G., 2022, VISUAL COMPUT, P1
   Yang X, 2019, INT CONF ACOUST SPEE, P8261, DOI 10.1109/ICASSP.2019.8683164
   Zhang Y, 2017, 2017 IEEE 2ND INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING (ICSIP), P15, DOI 10.1109/SIPROCESS.2017.8124497
   Zhou DY, 2022, VISUAL COMPUT, V38, P119, DOI 10.1007/s00371-020-02007-z
   Zhu XL, 2020, VISUAL COMPUT, V36, P743, DOI 10.1007/s00371-019-01660-3
NR 73
TC 9
Z9 9
U1 3
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6323
EP 6344
DI 10.1007/s00371-022-02732-7
EA DEC 2022
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000912588900001
DA 2024-07-18
ER

PT J
AU Liang, XZ
   Wang, LX
   Cheng, W
   Yan, XY
   Chen, Q
AF Liang, Xingzhu
   Wang, Lixin
   Cheng, Wei
   Yan, Xinyun
   Chen, Qing
TI ACKSNet: adaptive center keypoint selection for object detection
SO VISUAL COMPUTER
LA English
DT Article
DE Keypoint-based; Object detection; Center keypoint; Adaptive; Scale
   information
AB Keypoint-based detectors generate a large number of false positives due to incorrect keypoint matching in the object detection task. In this paper, we propose an adaptive center keypoint selection method (ACKSNet) to address the false-positive drawback. We first roughly group detected corners by associative embeddings, which flexibly localize objects of various shapes and scales to obtain a large number of initial candidate proposals. Then, ACKSNet associates introduced extra center keypoints with corner pairs through the geometric method to add location information in the candidate regions. And it independently generates a threshold for each center keypoint according to their statistical characteristics to ensure the high quality of center keypoints. Furthermore, we enrich the scale information of the output feature maps by equipping the backbone network with dilated convolution modules. On the MS COCO dataset, our model achieves an AP of 44.5%, surpassing most existing anchor-free detectors.
C1 [Liang, Xingzhu; Wang, Lixin; Cheng, Wei] Anhui Univ Sci & Technol, Sch Comp Sci & Engn, Huainan 232001, Peoples R China.
   [Liang, Xingzhu] Anhui Univ Sci & Technol, Inst Environm Friendly Mat & Occupat Hlth, Wuhu 241003, Peoples R China.
   [Yan, Xinyun] Jinling Inst Technol, Jiangsu AI Transportat Innovat & Applicat Engn Re, Nanjing 211169, Jiangsu, Peoples R China.
   [Chen, Qing] Nanjing Highway Dev Grp Co Ltd, Nanjing 210000, Peoples R China.
C3 Anhui University of Science & Technology; Anhui University of Science &
   Technology; Jinling Institute of Technology
RP Wang, LX (corresponding author), Anhui Univ Sci & Technol, Sch Comp Sci & Engn, Huainan 232001, Peoples R China.
EM 2020201055@aust.edu.cn
OI Liang, Xingzhu/0000-0002-8674-7302
FU Science and Technology Research Project of Wuhu City [2020yf48];
   Research Foundation of the Institute of Environment-friendly Materials
   and Occupational Health (Wuhu); Anhui University of Science and
   Technology [ALW2021YF04]
FX This research was supported by the Science and Technology Research
   Project of Wuhu City (No. 2020yf48), the Research Foundation of the
   Institute of Environment-friendly Materials and Occupational Health
   (Wuhu), Anhui University of Science and Technology (No. ALW2021YF04).
CR Ben Fredj H, 2021, VISUAL COMPUT, V37, P217, DOI 10.1007/s00371-020-01794-9
   Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Bodla N, 2017, IEEE I CONF COMP VIS, P5562, DOI 10.1109/ICCV.2017.593
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chen J, 2020, THIRD INTERNATIONAL CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2020), P285, DOI 10.1109/MIPR49039.2020.00066
   Chenchen Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P91, DOI 10.1007/978-3-030-58545-7_6
   Cui YM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8118, DOI 10.1109/ICCV48922.2021.00803
   Dong Z., 2020, P IEEE C COMP VIS PA, P10519, DOI DOI 10.1109/CVPR42600.2020.01053
   Duan K., 2022, ARXIV, DOI DOI 10.48550/ARXIV.2204.08394
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Fan Q, 2020, PROC CVPR IEEE, P4012, DOI 10.1109/CVPR42600.2020.00407
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hengduo Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10585, DOI 10.1109/CVPR42600.2020.01060
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Huang K, 2022, PATTERN RECOGN LETT, V160, P122, DOI 10.1016/j.patrec.2022.06.006
   Huang L., 2015, ARXIV, DOI [10.48550/arXiv.1509.04874, DOI 10.48550/ARXIV.1509.04874]
   Huang X, 2021, ARXIV, DOI DOI 10.48550/ARXIV.2104.10419
   Jiale Cao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11482, DOI 10.1109/CVPR42600.2020.01150
   Kingma D. P., 2014, arXiv
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li F., 2022, ARXIV, DOI DOI 10.48550/ARXIV.2203.01305
   Li YH, 2019, IEEE I CONF COMP VIS, P6053, DOI 10.1109/ICCV.2019.00615
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu DF, 2021, PROC CVPR IEEE, P9811, DOI 10.1109/CVPR46437.2021.00969
   Liu DF, 2020, NEUROCOMPUTING, V409, P1, DOI 10.1016/j.neucom.2020.05.027
   Liu S., 2022, INT C LEARN REPR, DOI [10.48550/arXiv.2201.12329, DOI 10.48550/ARXIV.2201.12329]
   Lu X, 2019, PROC CVPR IEEE, P7355, DOI 10.1109/CVPR.2019.00754
   Newell A., 2016, P ADV NEURAL INFORM, DOI [10.48550/arXiv.1611.05424, DOI 10.48550/ARXIV.1611.05424]
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091
   Qin Z, 2019, IEEE I CONF COMP VIS, P6717, DOI 10.1109/ICCV.2019.00682
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Roy K, 2022, VISUAL COMPUT, V38, P2801, DOI 10.1007/s00371-021-02157-8
   Saeidi M, 2022, VISUAL COMPUT, V38, P2223, DOI 10.1007/s00371-021-02280-6
   Shrivastava A., 2016, ARXIV, DOI DOI 10.48550/ARXIV.1612.06851
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tian Z, 2022, IEEE T PATTERN ANAL, V44, P1922, DOI 10.1109/TPAMI.2020.3032166
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Vaswani Ashish, 2017, Advances in Neural Information Processing Systems (NeurIPS), V17, P6000, DOI DOI 10.48550/ARXIV.1706.03762
   Wei LX, 2021, VISUAL COMPUT, V37, P133, DOI 10.1007/s00371-019-01787-3
   Xiao JY, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12031742
   Xie XX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3500, DOI 10.1109/ICCV48922.2021.00350
   Zhang H., 2023, 200303605 ARXIV, p2203.03605v3, DOI [10.48550/arxiv.2203.03605, DOI 10.48550/ARXIV.2203.03605, 10.48550/arXiv.2203.03605]
   Zhang HX, 2021, VISUAL COMPUT, V37, P2433, DOI 10.1007/s00371-020-01997-0
   Zhang S., 2020, P IEEECVF C COMPUTER, P9759
   Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442
   Zhang TS, 2023, VISUAL COMPUT, V39, P569, DOI 10.1007/s00371-021-02357-2
   Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644
   Zhou XY, 2019, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2019.00094
NR 55
TC 0
Z9 0
U1 5
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6073
EP 6084
DI 10.1007/s00371-022-02712-x
EA NOV 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000877975900001
DA 2024-07-18
ER

PT J
AU Shao, MW
   Zhang, W
   Li, YH
   Fan, BB
AF Shao, Mingwen
   Zhang, Wei
   Li, Yunhao
   Fan, Bingbing
TI Branch aware assignment for object detection
SO VISUAL COMPUTER
LA English
DT Article
DE Label assignment; Object detection; Machine learning; Computer vision
AB In object detection, deciding whether each anchor box should be assigned as a positive or negative sample is a very important procedure, known as label assignment, which greatly influences the performance of detectors. Current detectors use the Intersection-of-Union (IoU) as the criterion for assigning labels ( 0 for negative and 1 for positive ) to each anchor box while ignoring the importance of classification scores for defining samples. In this paper, we propose a novel label assignment strategy that directly assigns all ground-truths to corresponding anchor boxes based on their classification scores and determines the category belongs to each anchor box. Simultaneously, taking into account the classification accuracy and localization precision, we design a branch alignment module that enables each branch to acquire information from others as an additional supervisory signal based on the loss of different branches. Extensive competitive experiments on MS COCO benchmark demonstrate the effectiveness of our approach, which has a significant effect on the improvement of the baseline.
C1 [Shao, Mingwen; Zhang, Wei; Li, Yunhao; Fan, Bingbing] China Univ Petr, Coll Comp Sci & Technol, Qingdao, Peoples R China.
C3 China University of Petroleum
RP Shao, MW (corresponding author), China Univ Petr, Coll Comp Sci & Technol, Qingdao, Peoples R China.
EM smw278@126.com; M19070006@s.upc.edu.cn
RI Li, Yunhao/HCI-0205-2022
OI Li, Yunhao/0000-0002-5359-2930
FU National Key Research and development Program of China [2021YFA1000102];
   National Natural Science Foundation of China [61673396, 61976245];
   Natural Science Foundation of Shandong Province [ZR2022MF260]
FX The authors are very indebted to the anonymous referees for their
   critical comments and suggestions for the improvement of this paper.
   This work was supported by National Key Research and development Program
   of China (2021YFA1000102), and in part by the grants from the National
   Natural Science Foundation of China (Nos. 61673396, 61976245), Natural
   Science Foundation of Shandong Province (No: ZR2022MF260).
CR Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Fan BB, 2022, INT J MACH LEARN CYB, V13, P2189, DOI 10.1007/s13042-022-01514-w
   Ge Z, 2021, PROC CVPR IEEE, P303, DOI 10.1109/CVPR46437.2021.00037
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hengduo Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10585, DOI 10.1109/CVPR42600.2020.01060
   Kang Kim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P355, DOI 10.1007/978-3-030-58595-2_22
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li YH, 2022, SIGNAL IMAGE VIDEO P, V16, P705, DOI 10.1007/s11760-021-02010-4
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091
   Paszke A, 2019, ADV NEUR IN, V32
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shao MW, 2022, J ELECTRON IMAGING, V31, DOI 10.1117/1.JEI.31.1.013030
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang JQ, 2019, PROC CVPR IEEE, P2960, DOI 10.1109/CVPR.2019.00308
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yang T, 2018, ADV NEUR IN, V31
   Yang Y, 2021, FRONT INFORM TECH EL, V22, P1551, DOI 10.1631/FITEE.2100463
   Yang Z, 2019, IEEE I CONF COMP VIS, P9656, DOI 10.1109/ICCV.2019.00975
   Yuhang Cao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11580, DOI 10.1109/CVPR42600.2020.01160
   Zhang S., 2020, P IEEECVF C COMPUTER, P9759
   Zhang T., 2022, VISUAL COMPUT, P1
   Zhang XS, 2022, IEEE T PATTERN ANAL, V44, P3096, DOI 10.1109/TPAMI.2021.3050494
   Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644
   Zhou X., 2019, arXiv
   Zhu B. Z., 2020, ARXIV
NR 32
TC 3
Z9 3
U1 2
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5719
EP 5728
DI 10.1007/s00371-022-02691-z
EA OCT 2022
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000869606600002
DA 2024-07-18
ER

PT J
AU Wang, L
   Cai, LK
   Chen, CX
   Fu, X
   Yu, J
   Ge, RJ
   Yuan, BR
   Yang, X
   Shao, Q
   Lv, Q
AF Wang, Liang
   Cai, Lingkai
   Chen, Chunxiao
   Fu, Xue
   Yu, Jie
   Ge, Rongjun
   Yuan, Baorui
   Yang, Xiao
   Shao, Qiang
   Lv, Qiang
TI A novel DAVnet3+method for precise segmentation of bladder cancer in MRI
SO VISUAL COMPUTER
LA English
DT Article
DE Bladder cancer; MRI; Segmentation; Deep learning; Attention mechanism
ID PREDICTION
AB Muscular invasion is a critical factor in the surgical selection of bladder cancer (BC) patients. Precise segmentation of bladder outer wall (OW), bladder inner wall (IW) and bladder tumor (BT) using MRI is an essential step in the computer-assisted diagnosis of muscle invasiveness in BC. Our study is aimed to develop a novel deep learning-based model to segment the OW, IW and BT, which could effectively aid physicians in BC diagnosis. We proposed a Dual Attention Vnet triple plus (DAVnet3+) network for the accurate segmentation of BC MRI. To fully utilize the feature maps information, full-scale skip connections between down-sampling layers and up-sampling layers were added. Moreover, spatial and channel attention were introduced to increase the weight of the target region, while suppressing the irrelevant background regions for the segmentation task. Two datasets are used to validate the segmentation performance of the proposed DAVnet3+ model. Dataset1 comes from the 2018 China University Computer Design Competition and includes 2200 T2WI BC slices, of which 1760 slices are used for training and 440 slices for testing. Dataset2 contains 1545 T2WI slices provided by the Urology Department of the First Affiliated Hospital of Nanjing Medical University, of which 1236 slices are used for training and 309 slices for testing. Compared to the state-of-the-art deep learning-based methods, the DAVnet3+ model achieves the relatively best segmentation results in terms of dice similarity coefficient (DSC) and average symmetric surface distance (ASSD) on both datasets. (Dataset1: IW: DSC = 0.9626 +/- 0.0041, ASSD = 0.3327 +/- 0.0363; OW: DSC = 0.8170 +/- 0.0266, ASSD = 0.4001 +/- 0.0847; BT: DSC = 0.8635 +/- 0.0246, ASSD = 0.4884 +/- 0.1521. Dataset2: IW: DSC = 0.9840 +/- 0.0061, ASSD = 0.2772 +/- 0.0268; OW: DSC = 0.8392 +/- 0.0188, ASSD = 0.3363 +/- 0.0681; BT: DSC = 0.9391 +/- 0.0144, ASSD = 0.2921 +/- 0.1154). Our findings suggest that DAVnet3+ network is capable of segmenting the OW, IW and BT accurately in T2-weighted MRI. Additionally, it also substantiates the feasibility of the deep learning-based model for multi-region segmentation of BC.
C1 [Wang, Liang; Chen, Chunxiao; Fu, Xue; Yu, Jie] Nanjing Univ Aeronaut & Astronaut, Coll Automat Engn, Nanjing, Peoples R China.
   [Cai, Lingkai; Yuan, Baorui; Yang, Xiao; Lv, Qiang] Nanjing Med Univ, Dept Urol, Affiliated Hosp 1, Nanjing, Peoples R China.
   [Ge, Rongjun] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing, Peoples R China.
   [Shao, Qiang] Nanjing Med Univ, Dept Urol, Affiliated Suzhou Hosp, Suzhou, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics; Nanjing Medical
   University; Nanjing University of Aeronautics & Astronautics; Nanjing
   Medical University
RP Chen, CX (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Automat Engn, Nanjing, Peoples R China.; Lv, Q (corresponding author), Nanjing Med Univ, Dept Urol, Affiliated Hosp 1, Nanjing, Peoples R China.; Shao, Q (corresponding author), Nanjing Med Univ, Dept Urol, Affiliated Suzhou Hosp, Suzhou, Peoples R China.
EM ccxbme@nuaa.edu.cn; doctorlvqiang@sina.com; sq7166822@163.com
RI jin, chen/KBQ-8592-2024; Cai, Lingkai/KIC-3593-2024
OI Yang, Xiao/0000-0002-5429-9162; Chen, Chunxiao/0000-0002-9440-9426
FU National Natural Science Foundation of China [12071215]
FX This study was supported by the National Natural Science Foundation of
   China (Grant Nos.12071215).
CR Chamie K, 2013, CANCER-AM CANCER SOC, V119, P3219, DOI 10.1002/cncr.28147
   Chen XY, 2023, VISUAL COMPUT, V39, P255, DOI 10.1007/s00371-021-02326-9
   Cheng ZM, 2022, VISUAL COMPUT, V38, P749, DOI 10.1007/s00371-021-02075-9
   Pizzi AD, 2021, EUR RADIOL, V31, P3874, DOI 10.1007/s00330-020-07473-6
   Dolz J, 2018, MED PHYS, V45, P5482, DOI 10.1002/mp.13240
   Duan CJ, 2010, IEEE T MED IMAGING, V29, P903, DOI 10.1109/TMI.2009.2039756
   Fang Y, 2022, INT J IMAG SYST TECH, V32, P528, DOI 10.1002/ima.22639
   Funt SA, 2017, NAT REV CLIN ONCOL, V14, P221, DOI 10.1038/nrclinonc.2016.188
   Ge RQ, 2021, COMPUT BIOL CHEM, V93, DOI 10.1016/j.compbiolchem.2021.107510
   Green DA, 2013, BJU INT, V111, P404, DOI 10.1111/j.1464-410X.2012.11370.x
   Hammouda K, 2019, IEEE CONF IMAGING SY, DOI 10.1109/ist48021.2019.9010233
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang HM, 2020, INT CONF ACOUST SPEE, P1055, DOI [10.1109/ICASSP40776.2020.9053405, 10.1109/icassp40776.2020.9053405]
   Isensee F, 2021, NAT METHODS, V18, P203, DOI 10.1038/s41592-020-01008-z
   Jemal A, 2010, CA-CANCER J CLIN, V60, P277, DOI 10.3322/caac.20073
   Kamat AM, 2016, LANCET, V388, P2796, DOI 10.1016/S0140-6736(16)30512-8
   Karakiewicz PI, 2006, EUR UROL, V50, P1254, DOI 10.1016/j.eururo.2006.06.010
   Li C, 2020, COMPUT GRAPH-UK, V90, P11, DOI 10.1016/j.cag.2020.05.003
   Li DY, 2023, VISUAL COMPUT, V39, P1929, DOI 10.1007/s00371-022-02456-8
   Liu JX, 2019, I S BIOMED IMAGING, P28, DOI [10.1109/ISBI.2019.8759422, 10.1109/isbi.2019.8759422]
   McKibben MJ, 2015, CURR UROL REP, V16, DOI 10.1007/s11934-015-0496-8
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Oktay O., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1804.03999
   Pinto JR, 2017, P I MECH ENG H, V231, P871, DOI 10.1177/0954411917714294
   Qin XJ, 2014, IEEE J BIOMED HEALTH, V18, P1707, DOI 10.1109/JBHI.2013.2288935
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Svatek RS, 2011, BJU INT, V107, P898, DOI 10.1111/j.1464-410X.2010.09628.x
   Ueno Y, 2019, EUR UROL, V76, P54, DOI 10.1016/j.eururo.2019.03.012
   Verma S, 2012, RADIOGRAPHICS, V32, P371, DOI 10.1148/rg.322115125
   Witjes JA, 2017, EUR UROL, V71, P462, DOI 10.1016/j.eururo.2016.06.020
   Xiao D, 2016, INT J COMPUT ASS RAD, V11, P89, DOI 10.1007/s11548-015-1234-x
   Xu XP, 2019, J MAGN RESON IMAGING, V49, P1489, DOI 10.1002/jmri.26327
   Yiyao Liu, 2021, Image and Graphics: 11th International Conference, ICIG 2021, Proceedings. Lecture Notes in Computer Science, Image Processing, Computer Vision, Pattern Recognition, and Graphics (12889), P559, DOI 10.1007/978-3-030-87358-5_45
NR 34
TC 1
Z9 1
U1 1
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4737
EP 4749
DI 10.1007/s00371-022-02622-y
EA AUG 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000844899900002
DA 2024-07-18
ER

PT J
AU Li, HH
   Xiao, XL
   Liu, XY
   Guo, JH
   Wen, GH
   Liang, P
AF Li, Huihui
   Xiao, Xiangling
   Liu, Xiaoyong
   Guo, Jianhua
   Wen, Guihua
   Liang, Peng
TI Heuristic objective for facial expression recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Facial expression recognition; Domain knowledge; Objective function;
   Heuristics
ID FACE; NETWORK
AB Facial expression recognition has been widely used in lots of fields such as health care and intelligent robot systems. However, recognizing facial expression in the wild is still very challenging due to variations, light intensity, occlusions and the ambiguity of human emotion. When training samples cannot include all these environments, the classification can easily lead to errors. Therefore, this paper proposes a new heuristic objective function based on the domain knowledge so as to better optimize deep neural networks for facial expression recognition. Moreover, we take the specific relationship between the facial expression and facial action units as the domain knowledge. By analyzing the mixing relationship between different expression categories and then enlarging the distance of easily confused categories, we define a new heuristic objective function which can guide deep neural network to learn better features and then improve the accuracy of facial expression recognition. The experimental results verify the effectiveness, universality and the superior performance of our methods.
C1 [Li, Huihui; Xiao, Xiangling; Liu, Xiaoyong; Guo, Jianhua; Liang, Peng] Guangdong Polytech Normal Univ, Sch Comp Sci, Guangzhou 510665, Peoples R China.
   [Wen, Guihua] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510064, Peoples R China.
C3 Guangdong Polytechnic Normal University; South China University of
   Technology
RP Liu, XY; Guo, JH (corresponding author), Guangdong Polytech Normal Univ, Sch Comp Sci, Guangzhou 510665, Peoples R China.
EM 29777562@qq.com; 979432400@qq.com; 35643506@qq.com;
   guojianhua@gpnu.edu.cn; crghwen@scut.edu.cn; 330282302@qq.com
RI liang, peng/F-9547-2010
OI Liu, Xiaoyong/0000-0002-0795-841X; LI, HUIHUI/0000-0003-0463-8178
FU National Natural Science Foundation of China [62006049, 62176095,
   62172113, 62072123]; Guangdong Province Key Area RD Plan Project
   [2020B1111120001]; Guangzhou Science and Technology Planning Project
   [201803010088]; Ministry of Education Humanities and Social Science
   project [18JDGC012]
FX This study was supported by National Natural Science Foundation of China
   (Grant Nos. 62006049, 62176095, 62172113 and 62072123), Guangdong
   Province Key Area R&D Plan Project (Grant No. 2020B1111120001),
   Guangzhou Science and Technology Planning Project (Grant No.
   201803010088), Ministry of Education Humanities and Social Science
   project (Grant No. 18JDGC012).
CR Albanie S, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P292, DOI 10.1145/3240508.3240578
   [Anonymous], 1986, MACHINE LEARNING ART
   Barsoum E, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P279, DOI 10.1145/2993148.2993165
   Benitez-Quiroz CF, 2016, PROC CVPR IEEE, P5562, DOI 10.1109/CVPR.2016.600
   Chen BY, 2021, PATTERN RECOGN, V115, DOI 10.1016/j.patcog.2021.107893
   Chen JY, 2022, IEEE T IND INFORM, V18, P16, DOI 10.1109/TII.2021.3075989
   Chen TS, 2022, IEEE T PATTERN ANAL, V44, P9887, DOI 10.1109/TPAMI.2021.3131222
   Chen TS, 2022, IEEE T PATTERN ANAL, V44, P1371, DOI 10.1109/TPAMI.2020.3025814
   Chen TS, 2019, IEEE I CONF COMP VIS, P522, DOI 10.1109/ICCV.2019.00061
   Chen TS, 2019, PROC CVPR IEEE, P6156, DOI 10.1109/CVPR.2019.00632
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Dhall A, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   EKMAN P, 1971, J PERS SOC PSYCHOL, V17, P124, DOI 10.1037/h0030377
   Ekman P., 1978, Facial action coding system
   Ekman Paul, 1997, What the Face Reveals: Basic and Applied Studies of Spontaneous Expression using the Facial Action Coding System (FACS)
   García M, 2020, IEEE LAT AM T, V18, P1311, DOI 10.1109/TLA.2020.9099774
   Gera D, 2022, PATTERN RECOGN LETT, V155, P9, DOI 10.1016/j.patrec.2022.01.013
   Goodfellow Ian J., 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P117, DOI 10.1007/978-3-642-42051-1_16
   He J, 2021, J MULTIMODAL USER IN, V15, P429, DOI 10.1007/s12193-020-00363-7
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jin X, 2021, IEEE T IMAGE PROCESS, V30, P7143, DOI 10.1109/TIP.2021.3101820
   Joseph JL, 2021, 2021 IEEE 4 INT C CO
   Li M., 2018, IEEE T AFFECT COMPUT, V2, P71
   Li M, 2021, IEEE T AFFECT COMPUT, V12, P544, DOI 10.1109/TAFFC.2018.2880201
   Li S, 2022, IEEE T AFFECT COMPUT, V13, P1195, DOI 10.1109/TAFFC.2020.2981446
   Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277
   Lian Z., P 1 AS C AFF COMP IN, P1, DOI 10.1109/ACIIAsia.2018.8470391
   Liu CJ, 2023, VISUAL COMPUT, V39, P2637, DOI 10.1007/s00371-022-02483-5
   Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212
   Ma FY, 2023, IEEE T AFFECT COMPUT, V14, P1236, DOI 10.1109/TAFFC.2021.3122146
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Nuanes T, 2021, IEEE COMPUT SOC CONF, P2840, DOI 10.1109/CVPRW53098.2021.00319
   Pan HY, 2018, PROC CVPR IEEE, P5285, DOI 10.1109/CVPR.2018.00554
   Pu T, 2021, IEEE INT CONF ROBOT, P11154, DOI 10.1109/ICRA48506.2021.9561252
   Ruan DL, 2021, PROC CVPR IEEE, P7656, DOI 10.1109/CVPR46437.2021.00757
   Sandhya M, 2023, VISUAL COMPUT, V39, P1571, DOI 10.1007/s00371-022-02429-x
   She JH, 2021, PROC CVPR IEEE, P6244, DOI 10.1109/CVPR46437.2021.00618
   Shikai Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13981, DOI 10.1109/CVPR42600.2020.01400
   Sun YX, 2017, NEUROCOMPUTING, V230, P397, DOI 10.1016/j.neucom.2016.12.043
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vo TH, 2020, IEEE ACCESS, V8, P131988, DOI 10.1109/ACCESS.2020.3010018
   Wang F, 2018, IEEE SIGNAL PROC LET, V25, P926, DOI 10.1109/LSP.2018.2822810
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang Jiheng, 2014, 2014 IEEE INT C MULT, P1
   Wang Jun, 2015, FACIAL EXPRESSION AC
   Wang K, 2020, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR42600.2020.00693
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Wang ZX, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1021
   Wen GH, 2020, IEEE T MULTIMEDIA, V22, P2914, DOI 10.1109/TMM.2020.2966858
   Wen GH, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/1945630
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Xie Y, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1255, DOI 10.1145/3394171.3413822
   Xue FL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3581, DOI 10.1109/ICCV48922.2021.00358
   Zhang HF, 2021, IEEE T COGN DEV SYST, V13, P898, DOI 10.1109/TCDS.2020.3034807
   Zhang T, 2022, IEEE T KNOWL DATA EN, V34, P544, DOI 10.1109/TKDE.2020.2985365
   Zhang YH, 2021, ADV NEUR IN, V34
   Zhao ZQ, 2021, AAAI CONF ARTIF INTE, V35, P3510
   Zhao ZQ, 2021, IEEE T IMAGE PROCESS, V30, P6544, DOI 10.1109/TIP.2021.3093397
   Zhu DD, 2021, SIGNAL IMAGE VIDEO P, V15, P263, DOI 10.1007/s11760-020-01753-w
NR 59
TC 8
Z9 8
U1 10
U2 38
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4709
EP 4720
DI 10.1007/s00371-022-02619-7
EA AUG 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000838568700001
DA 2024-07-18
ER

PT J
AU Bäuerle, A
   Albus, P
   Störk, R
   Seufert, T
   Ropinski, T
AF Bauerle, Alex
   Albus, Patrick
   Stork, Raphael
   Seufert, Tina
   Ropinski, Timo
TI exploRNN: teaching recurrent neural networks through visual exploration
SO VISUAL COMPUTER
LA English
DT Article
DE Neural network education; Recurrent neural networks; Sequential data;
   Visual education
AB Due to the success and growing job market of deep learning (DL), students and researchers from many areas are interested in learning about DL technologies. Visualization has been used as a modern medium during this learning process. However, despite the fact that sequential data tasks, such as text and function analysis, are at the forefront of DL research, there does not yet exist an educational visualization that covers recurrent neural networks (RNNs). Additionally, the benefits and trade-offs between using visualization environments and conventional learning material for DL have not yet been evaluated. To address these gaps, we propose exploRNN, the first interactively explorable educational visualization for RNNs. exploRNNis accessible online and provides an overview of the training process of RNNs at a coarse level, as well as detailed tools for the inspection of data flow within LSTM cells. In an empirical between-subjects study with 37 participants, we investigate the learning outcomes and cognitive load of exploRNN compared to a classic text-based learning environment. While learners in the text group are ahead in superficial knowledge acquisition, exploRNN is particularly helpful for deeper understanding. Additionally, learning with exploRNN is perceived as significantly easier and causes less extraneous load. In conclusion, for difficult learning material, such as neural networks that require deep understanding, interactive visualizations such as exploRNN can be helpful.
C1 [Bauerle, Alex; Ropinski, Timo] Ulm Univ, Visual Comp Grp, Ulm, Germany.
   [Albus, Patrick] Ulm Univ, Inst Psychol & Educ, Dept Learning & Instruct, Ulm, Germany.
   [Stork, Raphael; Seufert, Tina] Ulm Univ, Ulm, Germany.
C3 Ulm University; Ulm University; Ulm University
RP Bäuerle, A (corresponding author), Ulm Univ, Visual Comp Grp, Ulm, Germany.
EM alex@a13x.io
OI Ropinski, Timo/0000-0002-7857-5512; Bauerle, Alex/0000-0003-3886-8799
FU Carl-Zeiss Scholarship
FX This work was funded by the Carl-Zeiss Scholarship for Ph.D. students.
   The datasets generated during and analyzed during the presented study
   are available from the corresponding author on reasonable request.
CR Amadieu F, 2011, COMPUT HUM BEHAV, V27, P36, DOI 10.1016/j.chb.2010.05.009
   [Anonymous], 2018, ARXIV180804018
   Bangor A, 2009, J USABILITY STUD, V4, P114
   Bartram L., 1997, P WORKSH NEW PAR INF, P3, DOI 10.1145/275519.275520
   Bauerle A., 2021, IEEE T VISUAL COMPUT
   Beck, 2014, ISR ROB 2014 41 INT, P83, DOI DOI 10.2312/EUROVISSTAR.20141174
   Bengio Y, 2013, INT CONF ACOUST SPEE, P8624, DOI 10.1109/ICASSP.2013.6639349
   Berthold K, 2009, J EDUC PSYCHOL, V101, P70, DOI 10.1037/a0013247
   Bloom B. S., 1956, Taxonomy of educational ob- jectives. vol. 1: Cognitive domain, V20, P24
   Brachten F, 2020, INF SYST E-BUS MANAG, V18, P187, DOI 10.1007/s10257-020-00471-7
   Britz D., 2017, P 2017 C EMPIRICAL M, P1442, DOI [10.18653/v1/D17-1151, DOI 10.18653/V1/D17-1151]
   Brooke J, 1996, USABILITY EVALUATION, V189, P4
   Cashman D, 2018, IEEE COMPUT GRAPH, V38, P39, DOI 10.1109/MCG.2018.2878902
   Chevalier N. H., 2016, P INT WORK C ADV VIS, P280
   Cho K., 2014, ARXIV14061078
   Chung S., 2016, KDD 16 WORKSH INT DA
   CRONBACH LJ, 1951, PSYCHOMETRIKA, V16, P297, DOI [10.1007/BF02310555, DOI 10.1007/BF02310555]
   de Koning BB, 2010, LEARN INSTR, V20, P111, DOI 10.1016/j.learninstruc.2009.02.010
   Ding, 2018, DRAW CONVNET
   Drosos I, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20)
   Elmqvist N, 2010, IEEE T VIS COMPUT GR, V16, P439, DOI 10.1109/TVCG.2009.84
   Garcia Rafael, 2020, VINCI '20: Proceedings of the 13th International Symposium on Visual Information Communication and Interaction, DOI 10.1145/3430036.3430047
   Gleicher M, 2013, IEEE T VIS COMPUT GR, V19, P2042, DOI 10.1109/TVCG.2013.157
   GLENBERG AM, 1982, MEM COGNITION, V10, P597, DOI 10.3758/BF03202442
   Graves A, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P273, DOI 10.1109/ASRU.2013.6707742
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137
   Guo PJ, 2015, PROCEEDINGS 2015 IEEE SYMPOSIUM ON VISUAL LANGUAGES AND HUMAN-CENTRIC COMPUTING (VL/HCC), P79, DOI 10.1109/VLHCC.2015.7357201
   Guo Philip J., 2013, P 44 ACM TECHN S COM, P579
   Harley AW, 2015, LECT NOTES COMPUT SC, V9474, P867, DOI 10.1007/978-3-319-27857-5_77
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hundhausen CD, 2002, J VISUAL LANG COMPUT, V13, P259, DOI 10.1006/S1045-926X(02)00028-9
   Hundhausen CD, 2007, J VISUAL LANG COMPUT, V18, P22, DOI 10.1016/j.jvlc.2006.03.002
   Jenny B., 2007, CARTOGR PERSPECT, V58, P61, DOI [DOI 10.14714/CP58.270, 10.14714/CP58.270]
   Kahng M, 2019, IEEE T VIS COMPUT GR, V25, P310, DOI 10.1109/TVCG.2018.2864500
   Kang H., 2003, P 2003 ANN NAT C DIG, P1
   Karpathy A., 2015, UNREASONABLE EFFECTI
   Karpathy A., 2015, ARXIV150602078
   Khomenko V, 2016, PROCEEDINGS OF THE 2016 IEEE FIRST INTERNATIONAL CONFERENCE ON DATA STREAM MINING & PROCESSING (DSMP), P100, DOI 10.1109/DSMP.2016.7583516
   Klepsch M, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01997
   Kolic-Vrhovec S., 2011, Review of psychology, V18, P81
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kürschner C, 2006, Z PSYCHOL, V214, P117, DOI 10.1026/0044-3409.214.3.117
   Lee S, 2016, IEEE T VIS COMPUT GR, V22, P499, DOI 10.1109/TVCG.2015.2467195
   McCarthy, 2020, P5 JS
   Ming Y, 2017, IEEE CONF VIS ANAL, P13, DOI 10.1109/VAST.2017.8585721
   Munzner T, 2014, VISUALIZATION ANAL D, DOI DOI 10.1201/B17511
   Norton AP, 2017, IEEE SYM VIS CYB SEC
   Olah C., 2015, Understanding LSTM Networks, DOI DOI 10.1007/S13398-014-0173-7.2
   Park SH, 2018, IEEE INT VEH SYM, P1672, DOI 10.1109/IVS.2018.8500658
   Peters O., 2000, The International Review of Research in Open and Distance Learning, V1, P1, DOI [10.19173/irrodl.v1i1.3, DOI 10.19173/IRRODL.V1I1.3]
   PINTRICH PR, 1993, EDUC PSYCHOL MEAS, V53, P801, DOI 10.1177/0013164493053003024
   Robertson G, 2008, IEEE T VIS COMPUT GR, V14, P1325, DOI 10.1109/TVCG.2008.125
   Sak H, 2014, INTERSPEECH, P338
   Schweitzer D, 2007, SIGCSE 2007: PROCEEDINGS OF THE THIRTY-EIGHTH SIGCSE TECHNICAL SYMPOSIUM ON COMPUTER SCIENCE EDUCATION, P208, DOI 10.1145/1227504.1227384
   Shen QM, 2020, IEEE PAC VIS SYMP, P61, DOI 10.1109/PacificVis48177.2020.2785
   Shneiderman B, 1996, IEEE SYMPOSIUM ON VISUAL LANGUAGES, PROCEEDINGS, P336, DOI 10.1109/VL.1996.545307
   Smilkov D., 2019, ARXIV190105350
   Smilkov Daniel, 2017, arXiv:1708.03788
   Soltau H, 2017, INTERSPEECH, P3707, DOI 10.21437/Interspeech.2017-1566
   Strobelt H, 2019, IEEE T VIS COMPUT GR, V25, P353, DOI 10.1109/TVCG.2018.2865044
   Strobelt H, 2018, IEEE T VIS COMPUT GR, V24, P667, DOI 10.1109/TVCG.2017.2744158
   Sweller J, 2011, PSYCHOL LEARN MOTIV, V55, P37
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Uchida, 2019, CONVNET DRAWER
   Weiss G, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P740
   Wu Y., 2016, GOOGLES NEURAL MACHI
   Ynnerman A, 2018, IEEE COMPUT GRAPH, V38, P13, DOI 10.1109/MCG.2018.032421649
   Zhu HY, 2018, I S WORKL CHAR PROC, P88, DOI 10.1109/IISWC.2018.8573476
NR 70
TC 1
Z9 1
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 4323
EP 4338
DI 10.1007/s00371-022-02593-0
EA JUL 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000825226300005
OA hybrid
DA 2024-07-18
ER

PT J
AU Klötzl, D
   Krake, T
   Zhou, YJ
   Hotz, I
   Wang, B
   Weiskopf, D
AF Kloetzl, Daniel
   Krake, Tim
   Zhou, Youjia
   Hotz, Ingrid
   Wang, Bei
   Weiskopf, Daniel
TI Local bilinear computation of Jacobi sets
SO VISUAL COMPUTER
LA English
DT Article
DE Jacobi set; Topological data analysis; Multi-fields; Visualization
   techniques
AB We propose a novel method for the computation of Jacobi sets in 2D domains. The Jacobi set is a topological descriptor based on Morse theory that captures gradient alignments among multiple scalar fields, which is useful for multi-field visualization. Previous Jacobi set computations use piecewise linear approximations on triangulations that result in discretization artifacts like zig-zag patterns. In this paper, we utilize a local bilinear method to obtain a more precise approximation of Jacobi sets by preserving the topology and improving the geometry. Consequently, zig-zag patterns on edges are avoided, resulting in a smoother Jacobi set representation. Our experiments show a better convergence with increasing resolution compared to the piecewise linear method. We utilize this advantage with an efficient local subdivision scheme. Finally, our approach is evaluated qualitatively and quantitatively in comparison with previous methods for different mesh resolutions and across a number of synthetic and real-world examples.
C1 [Kloetzl, Daniel; Krake, Tim; Weiskopf, Daniel] Univ Stuttgart VISUS, Stuttgart, Germany.
   [Zhou, Youjia; Wang, Bei] Univ Utah SCI, Salt Lake City, UT USA.
   [Hotz, Ingrid] Linkoping Univ ITN, Linkoping, Sweden.
C3 University of Stuttgart
RP Klötzl, D (corresponding author), Univ Stuttgart VISUS, Stuttgart, Germany.
EM daniel.kloetzl@visus.uni-stuttgart.de; tim.krake@visus.uni-stuttgart.de;
   zhou325@sci.utah.edu; ingrid.hotz@liu.se; beiwang@sci.utah.edu;
   daniel.weiskopf@visus.uni-stuttgart.de
OI Zhou, Youjia/0000-0002-4501-8496; Hotz, Ingrid/0000-0001-7285-0483;
   Krake, Tim/0009-0004-7084-3633; Weiskopf, Daniel/0000-0003-1174-1026;
   Klotzl, Daniel/0000-0002-4222-3320
FU Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) [DFG
   270852890-GRK 2160/2, DFG 251654672-TRR 161]; Swedish Research Council
   (VR) [2019-05487]; U.S. Department of Energy (DOE) [DOE DE-SC0021015];
   National Science Foundation (NSF) [NSF IIS-1910733]; U.S. Department of
   Energy (DOE) [DE-SC0021015] Funding Source: U.S. Department of Energy
   (DOE); Swedish Research Council [2019-05487] Funding Source: Swedish
   Research Council
FX This work was supported by the Deutsche Forschungsgemeinschaft (DFG,
   German Research Foundation) through the grants DFG 270852890-GRK 2160/2
   and DFG 251654672-TRR 161, the Swedish Research Council (VR) under the
   grant 2019-05487, the U.S. Department of Energy (DOE) under the grant
   DOE DE-SC0021015, and the National Science Foundation (NSF) through the
   grant NSF IIS-1910733.
CR Artamonova IV, 2017, J PHYS CONF SER, V798, DOI 10.1088/1742-6596/798/1/012040
   Barnett TP, 2001, SCIENCE, V292, P270, DOI 10.1126/science.1058304
   Bhatia H, 2015, COMP GEOM-THEOR APPL, V48, P311, DOI 10.1016/j.comgeo.2014.10.009
   Bremer PT, 2007, J PHYS CONF SER, V78, DOI 10.1088/1742-6596/78/1/012007
   Carlsson G, 2009, DISCRETE COMPUT GEOM, V42, P71, DOI 10.1007/s00454-009-9176-0
   Carr H, 2014, IEEE T VIS COMPUT GR, V20, P1100, DOI 10.1109/TVCG.2013.269
   Chattopadhyay A, 2014, EuroVis-Short Papers
   Edelsbrunner H, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P275, DOI 10.1109/VISUAL.2004.68
   EDELSBRUNNER H, 1990, ACM T GRAPHIC, V9, P66, DOI 10.1145/77635.77639
   Edelsbrunner H., 2002, Foundations of Computational Mathematics, P37, DOI [DOI 10.1017/CBO9781139106962.003, 10.1017/CBO9781139106962.003, DOI 10.1017/CBO9781139106962.003.2]
   Edelsbrunner H., 2010, AM MATH SOC, DOI DOI 10.1007/978-3-540-33259-6_7
   Edelsbrunner H, 2011, MATH VIS, P27
   Edelsbrunner H, 2008, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SGG'08), P242, DOI 10.1145/1377676.1377720
   Günther T, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073684
   Hatcher A., 2003, ALGEBRAIC TOPOLOGY
   He XY, 2019, J VISUAL-JAPAN, V22, P897, DOI 10.1007/s12650-019-00584-3
   Huettenberger L, 2013, COMPUT GRAPH FORUM, V32, P341, DOI 10.1111/cgf.12121
   Huettenberger L, 2015, MATH VIS, P125, DOI 10.1007/978-3-662-44900-4__8
   Iuricich F, 2016, SIGGRAPH ASIA 2016 S, P1, DOI DOI 10.1145/3002151.3002166
   Luo CJ, 2009, COMPUT GRAPH FORUM, V28, P1497, DOI 10.1111/j.1467-8659.2009.01526.x
   Makela K., 2020, COMPUT RES REP ARXIV, DOI [10.48550/arXiv.2010.08691, DOI 10.48550/ARXIV.2010.08691]
   Nagaraj S, 2011, IEEE T VIS COMPUT GR, V17, P182, DOI 10.1109/TVCG.2010.64
   Natarajan V., 2004, THESIS DUKE U
   Norgard G, 2013, COMPUT AIDED GEOM D, V30, P597, DOI 10.1016/j.cagd.2012.03.015
   Peikert R., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P263, DOI 10.1109/VISUAL.1999.809896
   Singh G., 2007, EUR S POINT BAS GRAP, P91, DOI [DOI 10.2312/SPBG/SPBG07/091-100, 10.2312/SPBG/SPBG07/091-100]
   Suthambhara N, 2011, MATH VIS, P91
   Theisel H, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P631
   Van Gelder A, 2009, IEEE T VIS COMPUT GR, V15, P682, DOI 10.1109/TVCG.2009.11
   Yan L, 2021, COMPUT GRAPH FORUM, V40, P599, DOI 10.1111/cgf.14331
NR 30
TC 2
Z9 2
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3435
EP 3448
DI 10.1007/s00371-022-02557-4
EA JUN 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000819263900001
OA hybrid
DA 2024-07-18
ER

PT J
AU Luo, Y
   Wu, MH
   Huang, QD
   Zhu, J
   Ling, J
   Sheng, B
AF Luo, Yu
   Wu, Menghua
   Huang, Qingdong
   Zhu, Jian
   Ling, Jie
   Sheng, Bin
TI Joint feedback and recurrent deraining network with ensemble learning
SO VISUAL COMPUTER
LA English
DT Article
DE Single-image deraining; Feedback mechanism; Attention mechanism;
   Residual dilated aggregation; Ensemble learning
ID IMAGE; REMOVAL
AB Rainy images typically contain heterogeneous rain distributions; however, many existing methods perform well in simple homogeneous rain and fail to handle complex heterogeneous rain effectively. In this paper, we try to solve this problem by fully exploiting the complementary contextual information in the manner of a Joint Feedback and Recurrent deraining scheme with Ensemble Learning (JFREL). First, the proposed JFREL is built on a recurrent multistage architecture, and the output of each stage is fused automatically via ensemble learning. Second, the feedback mechanism is utilized to refine information from inter- and intra-stages. Third, at each stage the residual dilated aggregation attention module is recursively adopted to adequately characterize complementary high-level contextual information in multiple receptive fields and adaptively aggregate beneficial details to achieve feature compensation. Extensive experiments demonstrate that the proposed JFREL can achieve a competitive performance over the state-of-the-art methods on both synthetic and real-world datasets.
C1 [Luo, Yu; Wu, Menghua; Huang, Qingdong; Zhu, Jian; Ling, Jie] Guangdong Univ Technol, Sch Comp Sci, Guangzhou, Peoples R China.
   [Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
C3 Guangdong University of Technology; Shanghai Jiao Tong University
RP Zhu, J; Ling, J (corresponding author), Guangdong Univ Technol, Sch Comp Sci, Guangzhou, Peoples R China.
EM rockeyzhu@163.com; jling@gdut.edu.cn
RI Luo, Yu/KVC-0220-2024; Ling, Jie/JJF-9995-2023
OI Zhu, Jian/0000-0002-2551-2024; luo, yu/0000-0003-3968-9725
FU Project of Guangzhou Science and Technology [202102020591, 202007010004,
   202007040005]
FX We are sincerely grateful to anonymous reviewers for their insightful
   comments that improve the quality of this paper. This work was supported
   by the Project of Guangzhou Science and Technology (Nos. 202102020591,
   202007010004 and 202007040005).
CR Ahn N, 2022, IEEE T CIRC SYST VID, V32, P608, DOI 10.1109/TCSVT.2021.3068985
   Cai LW, 2019, IEEE IMAGE PROC, P2756, DOI [10.1109/icip.2019.8803308, 10.1109/ICIP.2019.8803308]
   Chen YL, 2013, IEEE I CONF COMP VIS, P1968, DOI 10.1109/ICCV.2013.247
   Deng LJ, 2018, APPL MATH MODEL, V59, P662, DOI 10.1016/j.apm.2018.03.001
   Ding JJ, 2021, INFORM SCIENCES, V572, P611, DOI 10.1016/j.ins.2021.02.080
   Fu XY, 2020, IEEE T NEUR NET LEAR, V31, P1794, DOI 10.1109/TNNLS.2019.2926481
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802
   Fu YH, 2011, INT CONF ACOUST SPEE, P1453
   Gu SH, 2017, IEEE I CONF COMP VIS, P1717, DOI 10.1109/ICCV.2017.189
   Guo ZK, 2022, SIGNAL IMAGE VIDEO P, V16, P185, DOI 10.1007/s11760-021-01972-9
   Hu XW, 2019, PROC CVPR IEEE, P8014, DOI 10.1109/CVPR.2019.00821
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   Jin X, 2019, IEEE IMAGE PROC, P2761, DOI [10.1109/ICIP.2019.8803238, 10.1109/icip.2019.8803238]
   Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057
   Kingma D. P., 2014, arXiv
   Kui Jiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8343, DOI 10.1109/CVPR42600.2020.00837
   Li, 2017, ARXIV171206830
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299
   Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Ren DW, 2020, IEEE T IMAGE PROCESS, V29, P6852, DOI 10.1109/TIP.2020.2994443
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Sen Deng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14548, DOI 10.1109/CVPR42600.2020.01457
   Takikawa T, 2019, IEEE I CONF COMP VIS, P5228, DOI 10.1109/ICCV.2019.00533
   Wang C, 2022, NEUROCOMPUTING, V467, P242, DOI 10.1016/j.neucom.2021.10.029
   Wang TY, 2019, PROC CVPR IEEE, P12262, DOI 10.1109/CVPR.2019.01255
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei W, 2019, PROC CVPR IEEE, P3872, DOI 10.1109/CVPR.2019.00400
   Yang W., 2017, PROC CVPR IEEE, P1685, DOI DOI 10.1109/CVPR.2017.183
   Yang YZ, 2019, IEEE INT CON MULTI, P1378, DOI 10.1109/ICME.2019.00239
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhu L, 2017, IEEE I CONF COMP VIS, P2545, DOI 10.1109/ICCV.2017.276
NR 34
TC 4
Z9 4
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3109
EP 3119
DI 10.1007/s00371-022-02567-2
EA JUN 2022
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000817073900002
DA 2024-07-18
ER

PT J
AU Yang, H
   Zhou, DM
   Li, M
   Zhao, Q
AF Yang, Hao
   Zhou, Dongming
   Li, Miao
   Zhao, Qian
TI A two-stage network with wavelet transformation for single-image
   deraining
SO VISUAL COMPUTER
LA English
DT Article
DE Single-image deraining; Multi-stage network; Image restoration; Neural
   networks
ID RAIN STREAKS
AB Image deraining is still a vital and challenging low-level computer vision task, the purpose of which is to restore rain-free images from images degraded by rain streaks. Recently, many convolutional neural network (CNN)-based methods have made significant progress for the task of image deraining. However, there are still two issues with these methods: The rain streaks in heavy rain images cannot be effectively removed, and the high-quality images with clear details cannot be reconstructed. To solve the first issue, we adopt a two-stage structure to gradually remove rain streaks and use dilated convolution in the first stage network to rapidly enlarge the receptive field to capture the spatial characteristics of heavy rain streaks. For the latter issue, we introduced a structure-preserving network (SPN) without any up- and down-sampling and designed a feature extraction module based on wavelet transform in SPN to help SPN restore clear high-frequency details. In addition, we also designed a feature filter (FF) and multi-level feature fusion module (MLFFM), so that the valuable features in the first stage can be fully utilized in the second stage. Extensive experiments on six synthetic datasets and one real-world dataset indicate that our method can achieve excellent performance, compared with the current state-of-the-art methods. To further demonstrate the practical applicability of the proposed method, we use it as a semantic segmentation preprocessing step and display the semantic segmentation results to verify the effectiveness of our deraining method in downstream vision tasks.
C1 [Yang, Hao; Zhou, Dongming; Li, Miao; Zhao, Qian] Yunnan Univ, Sch Informat Sci & Engn, Kunming 650504, Yunnan, Peoples R China.
C3 Yunnan University
RP Zhou, DM (corresponding author), Yunnan Univ, Sch Informat Sci & Engn, Kunming 650504, Yunnan, Peoples R China.
EM zhoudm@ynu.edu.cn
RI li, miao/JED-3109-2023
OI li, miao/0000-0001-6196-2041; Zhou, Dongming/0000-0003-0139-9415
FU National Natural Science Foundation of China [62066047, 61966037];
   Yunnan University Postgraduate Practice Innovation Project [2021Y186]
FX The authors thank the editors and anonymous reviewers for their detailed
   reviews, valuable comments, and constructive suggestions for this study.
   This work was supported by National Natural Science Foundation of China
   (62066047, 61966037) and Yunnan University Postgraduate Practice
   Innovation Project (2021Y186).
CR CHARBONNIER P, 1994, IEEE IMAGE PROC, P168
   Chen CH, 2021, PROC CVPR IEEE, P7738, DOI 10.1109/CVPR46437.2021.00765
   Chen GC, 2022, VISUAL COMPUT, V38, P1051, DOI 10.1007/s00371-021-02067-9
   Chen LY, 2021, IEEE COMPUT SOC CONF, P182, DOI 10.1109/CVPRW53098.2021.00027
   Chen X, 2021, IEEE COMPUT SOC CONF, P872, DOI 10.1109/CVPRW53098.2021.00097
   Chen YL, 2013, IEEE I CONF COMP VIS, P1968, DOI 10.1109/ICCV.2013.247
   Cho SJ., 2021, P IEEE CVF INT C COM
   Devi RB, 2021, VISUAL COMPUT, V37, P1207, DOI 10.1007/s00371-020-01862-0
   Du SL, 2018, PATTERN RECOGN, V79, P303, DOI 10.1016/j.patcog.2018.02.016
   Du YJ, 2020, IEEE T IMAGE PROCESS, V29, P6288, DOI 10.1109/TIP.2020.2990606
   Fan JW, 2023, VISUAL COMPUT, V39, P319, DOI 10.1007/s00371-021-02331-y
   Feng Q, 2019, VISUAL COMPUT, V35, P1081, DOI 10.1007/s00371-019-01697-4
   Fu X., 2021, PROC AAAI C ARTIF IN, P19
   Fu XY, 2020, IEEE T NEUR NET LEAR, V31, P1794, DOI 10.1109/TNNLS.2019.2926481
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802
   Gu SH, 2017, IEEE I CONF COMP VIS, P1717, DOI 10.1109/ICCV.2017.189
   He JY, 2021, IEEE T IMAGE PROCESS, V30, P7200, DOI 10.1109/TIP.2021.3102509
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   Jiang K, 2021, IEEE T IMAGE PROCESS, V30, P7404, DOI 10.1109/TIP.2021.3102504
   Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057
   Kim JH, 2013, IEEE IMAGE PROC, P914, DOI 10.1109/ICIP.2013.6738189
   Ko K, 2022, IEEE T IMAGE PROCESS, V31, P1657, DOI 10.1109/TIP.2022.3145160
   Kui Jiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8343, DOI 10.1109/CVPR42600.2020.00837
   Li BY, 2021, INT J COMPUT VISION, V129, P1754, DOI 10.1007/s11263-021-01431-5
   Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299
   Lin ZK, 2023, VISUAL COMPUT, V39, P597, DOI 10.1007/s00371-021-02360-7
   Liu CX, 2019, PROC CVPR IEEE, P82, DOI 10.1109/CVPR.2019.00017
   Liu X, 2019, PROC CVPR IEEE, P7000, DOI 10.1109/CVPR.2019.00717
   Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388
   Ran W, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102800
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Sharma PK, 2021, VISUAL COMPUT, V37, P2083, DOI 10.1007/s00371-020-01971-w
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Vidanpathirana M, 2020, VISUAL COMPUT, V36, P1501, DOI 10.1007/s00371-019-01757-9
   Wang C, 2021, VISUAL COMPUT, V37, P1851, DOI 10.1007/s00371-020-01944-z
   Wang GQ, 2021, INT J COMPUT VISION, V129, P1650, DOI 10.1007/s11263-020-01425-9
   Wang H, 2020, PROC CVPR IEEE, P3100, DOI 10.1109/CVPR42600.2020.00317
   Wang HX, 2023, VISUAL COMPUT, V39, P639, DOI 10.1007/s00371-021-02363-4
   Wang TY, 2019, PROC CVPR IEEE, P12262, DOI 10.1109/CVPR.2019.01255
   Wang YL, 2017, IEEE T IMAGE PROCESS, V26, P3936, DOI 10.1109/TIP.2017.2708502
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei YY, 2021, IEEE T IMAGE PROCESS, V30, P4788, DOI 10.1109/TIP.2021.3074804
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu HP, 2023, VISUAL COMPUT, V39, P99, DOI 10.1007/s00371-021-02316-x
   Xu J., 2012, 2012 IEEE INT C COMP, P304
   Yang W., 2017, PROC CVPR IEEE, P1685, DOI DOI 10.1109/CVPR.2017.183
   Yang YZ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1814, DOI 10.1145/3343031.3351149
   Yasarla R, 2021, IEEE T IMAGE PROCESS, V30, P6570, DOI 10.1109/TIP.2021.3096323
   Yasarla R, 2020, IEEE T IMAGE PROCESS, V29, P4544, DOI 10.1109/TIP.2020.2973802
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zheng YP, 2022, IEEE T NEUR NET LEAR, V33, P1310, DOI 10.1109/TNNLS.2020.3041752
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu L, 2017, IEEE I CONF COMP VIS, P2545, DOI 10.1109/ICCV.2017.276
NR 59
TC 10
Z9 10
U1 1
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 3887
EP 3903
DI 10.1007/s00371-022-02533-y
EA JUN 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000810352600003
DA 2024-07-18
ER

PT J
AU Tan, X
   Xu, JC
   Cao, Y
   Xu, K
   Ma, LZ
   Lau, RWH
AF Tan, Xin
   Xu, Jiachen
   Cao, Ying
   Xu, Ke
   Ma, Lizhuang
   Lau, Rynson W. H.
TI HSNet: hierarchical semantics network for scene parsing
SO VISUAL COMPUTER
LA English
DT Article
DE Hierarchical semantics; Scene parsing; Cross-level feature;
   Bidirectional network
AB Scene parsing is one of the fundamental tasks in computer vision. Humans tend to perceive a scene in a hierarchical manner, i.e., first identifying the coarse category (e.g., vehicle) of a group of objects and then the fine category (e.g., bicycle, truck or car) of each of them. Despite recent tremendous progress on scene parsing, such a hierarchical semantics prior (HSP) has not been explicitly exploited. In this paper, we aim to introduce the HSP into scene parsing, by proposing a hierarchical semantics network (HSNet). Our key contribution is a bidirectional cross-level feature matching framework, which enables us to learn multi-level, hierarchy-aware features via forward feature transfer and backward feature regularization. In the forward stage, we train a coarse-to-fine module to learn fine-category features that explicitly encode hierarchical semantics information. In the backward stage, we introduce a fine-to-coarse module to collapse fine-category features to coarse-category features that are used to regularize the feature learning of our network. Experimental results on Cityscapes and Pascal Context show that our method achieves state-of-the-art performances. Our visualization also shows that our learned features capture semantic hierarchy favorably.
C1 [Tan, Xin; Xu, Jiachen; Ma, Lizhuang] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Tan, Xin; Cao, Ying; Xu, Ke; Lau, Rynson W. H.] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 Shanghai Jiao Tong University; City University of Hong Kong
RP Cao, Y; Lau, RWH (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
EM tanxin2017@sjtu.edu.cn; xujiachen@sjtu.edu.cn; caoying59@gmail.com;
   kkangwing@gmail.com; ma-lz@cs.sjtu.edu.cn; Rynson.Lau@cityu.edu.hk
RI , kk/AAH-8764-2020; Tan, Xin/GRJ-0367-2022; Sun, Peng/KDO-4243-2024; Li,
   Shiyu/KHE-1376-2024
OI Tan, Xin/0000-0001-9346-1196; XU, Ke/0000-0001-5855-3810
FU National Key Research and Development Program of China [2019YFC1521104];
   National Natural Science Foundation of China [72192821, 61972157];
   Shanghai Municipal Science, Technology Major Project [2021SHZDZX0102];
   Shanghai Science and Technology Commission [21511101200, 22YF1420300];
   RGC of Hong Kong [11205620]; City University of Hong Kong [7005674];
   City University of Hong Kong
FX This work was supported by the National Key Research and Development
   Program of China (2019YFC1521104), National Natural Science Foundation
   of China (72192821, 61972157), Shanghai Municipal Science, Technology
   Major Project (2021SHZDZX0102), Shanghai Science and Technology
   Commission (21511101200, 22YF1420300), a General Research Fund from RGC
   of Hong Kong (RGC Ref.: 11205620) and a Strategic Research Grant from
   City University of Hong Kong (Ref.: 7005674). Xin Tan is also supported
   by the Postgraduate Studentship (by Mainland Schemes) from City
   University of Hong Kong.
CR Ahmed K, 2016, LECT NOTES COMPUT SC, V9911, P516, DOI 10.1007/978-3-319-46478-7_32
   BILAL A, 2017, TVCG, V24, P152
   Boureau Y.-L., 2010, ICML
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Ding H., 2019, CVPR
   Ding HH, 2019, IEEE I CONF COMP VIS, P6818, DOI 10.1109/ICCV.2019.00692
   Fan C., IEEE T INTELL TRANSP, P111
   Fu J., 2019, CVPR
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   He J., 2019, ICCV
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu Hanzhe, 2020, ECCV
   Huang Z., 2019, ICCV CCNET
   Ji J, 2021, IEEE T CIRC SYST VID, V31, P1926, DOI 10.1109/TCSVT.2020.3015866
   Jiang M, 2022, VISUAL COMPUT, V38, P2473, DOI 10.1007/s00371-021-02124-3
   Jiao JB, 2019, PROC CVPR IEEE, P2864, DOI 10.1109/CVPR.2019.00298
   Krüger N, 2013, IEEE T PATTERN ANAL, V35, P1847, DOI 10.1109/TPAMI.2012.272
   Lee K, 2018, PROC CVPR IEEE, P1034, DOI 10.1109/CVPR.2018.00114
   Li X., 2020, CVPR
   Lin D, 2018, LECT NOTES COMPUT SC, V11207, P622, DOI 10.1007/978-3-030-01219-9_37
   Lin G., 2017, CVPR
   Liu MY, 2021, PROC CVPR IEEE, P9721, DOI 10.1109/CVPR46437.2021.00960
   Long J., 2015, P IEEE C COMP VIS PA, P3431, DOI DOI 10.48550/ARXIV.1411.4038
   Mottaghi R, 2014, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2014.119
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Peng Y, 2022, BUILD ENVIRON, V210, DOI 10.1016/j.buildenv.2021.108711
   Takikawa T., 2019, ICCV GATED SCNN
   Tamaazousti Y, 2017, PROC CVPR IEEE, P5282, DOI 10.1109/CVPR.2017.561
   Tan X, 2021, IEEE T IMAGE PROCESS, V30, P9085, DOI 10.1109/TIP.2021.3122004
   Wang D, 2021, VISUAL COMPUT, V37, P1101, DOI 10.1007/s00371-020-01855-z
   Wang K, 2022, VISUAL COMPUT, V38, P2329, DOI 10.1007/s00371-021-02115-4
   Wang X., 2018, CVPR
   Wang Y., 2020, ECCV
   Wu T., 2021, IEEE T MULTIMEDIA
   Xu K., 2018, EFFICIENT IMAGE SUPE
   Xu K., 2021, IEEE TIP
   Yan Z., 2015, ICCV
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Ye X, 2022, VISUAL COMPUT, V38, P2527, DOI 10.1007/s00371-021-02128-z
   Yu C., 2018, ECCV
   Yu C., 2020, CVPR
   Yu CQ, 2018, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR.2018.00199
   Yuan Y., 2019, ECCV
   Zhang Fan, 2019, ICCV
   Zhang H., 2018, CVPR
   Zhang L., 2019, BMVC
   Zhang Z., 2018, ECCV EXFUSE
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng CX, 2018, VISUAL COMPUT, V34, P735, DOI 10.1007/s00371-017-1411-8
   Zheng XY, 2021, IEEE T CIRC SYST VID, V31, P4370, DOI 10.1109/TCSVT.2021.3049408
   Zhu JY, 2017, ICCV
   Zhu Z, 2019, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2019.00068
   Zifeng W., 2019, PATTERN RECOGN
NR 54
TC 0
Z9 0
U1 1
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2543
EP 2554
DI 10.1007/s00371-022-02477-3
EA MAY 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000790216200001
DA 2024-07-18
ER

PT J
AU Kim, T
   Kim, GJ
AF Kim, Taehyung
   Kim, Gerard J.
TI Real-time and on-line removal of moving human figures in hand-held
   mobile augmented reality
SO VISUAL COMPUTER
LA English
DT Article
DE Handheld augmented reality; Diminished reality; Dynamic object removal;
   Perceptual issue
ID ERROR
AB In this paper, we present a real time on-line augmented/diminished reality system that runs entirely on the hand-held moving mobile device. Specifically, we introduce an improved inpainting algorithm that is designed for the on-line usage (i.e., live streaming video) with the moving view point. Unlike other previous approaches, the proposed algorithm produces a reasonably high-quality inpainting imagery by taking advantage of the 3D camera tracking and scene information from the SLAM in selecting the proper source frame from which to extract the parts to replace the masked region, and robustly applying homography to fill it in from the source. The algorithm is evaluated and compared with the state-of-the-art video inpainting methods in terms of the execution time, and objective and subjective inpainted image quality. Although the algorithm is applicable to any dynamic objects, the current implementation is limited to removing and filling in for human figures only. The evaluation results have shown the quality of the inpainted image quality was on par with those by the off-line state-of-the-art systems and yet ran at an interactive rate on a mobile device. A user study was also conducted to assess the user perception and experience in an outdoor interactive AR application using the proposed algorithm to remove the interfering pedestrians. The algorithm significantly reduced the level of distraction and improved the AR user experience by lowering the visual inconsistency and artifacts, when compared to other nominal test conditions.
C1 [Kim, Taehyung; Kim, Gerard J.] Korea Univ, Dept Comp Sci & Engn, Seoul 02841, South Korea.
C3 Korea University
RP Kim, GJ (corresponding author), Korea Univ, Dept Comp Sci & Engn, Seoul 02841, South Korea.
EM taehyung_kim@korea.ac.kr; gjkim@korea.ac.kr
OI kim, taehyung/0000-0002-6619-2039; Kim, Gerard/0000-0001-9880-8021
FU IITP/MSIT of Korea, under the ITRC support program
   [IITP-2021-2016-0-00312]; KEA/KIAT/MOTIE Competency Development Program
   for Industry Specialist [N000999]; NRF Korea [2019R1A2C1086649]
FX This research was supported in part by the IITP/MSIT of Korea, under the
   ITRC support program (IITP-2021-2016-0-00312) and, also KEA/KIAT/MOTIE
   Competency Development Program for Industry Specialist (N000999), and
   the NRF Korea through the Basic Science Research (2019R1A2C1086649).
CR Apple, ARKit
   Barath D, 2020, PROC CVPR IEEE, P1301, DOI 10.1109/CVPR42600.2020.00138
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bradski G, 2000, DR DOBBS J, V25, P120
   Chen Gao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P713, DOI 10.1007/978-3-030-58610-2_42
   Chum O, 2005, COMPUT VIS IMAGE UND, V97, P86, DOI 10.1016/j.cviu.2004.03.004
   Ebdelli M, 2015, IEEE T IMAGE PROCESS, V24, P3034, DOI 10.1109/TIP.2015.2437193
   Granados M, 2012, LECT NOTES COMPUT SC, V7572, P682, DOI 10.1007/978-3-642-33718-5_49
   Herling J, 2014, IEEE T VIS COMPUT GR, V20, P866, DOI 10.1109/TVCG.2014.2298016
   Huang JB, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982398
   Kim D, 2019, PROC CVPR IEEE, P5785, DOI 10.1109/CVPR.2019.00594
   Kim H, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10080900
   Kim H, 2020, IEEE ICC, DOI 10.1109/icc40277.2020.9149211
   Lee S, 2019, IEEE I CONF COMP VIS, P4412, DOI 10.1109/ICCV.2019.00451
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Lucas B., 1981, P INT JOINT C ART IN, P674, DOI DOI 10.1364/J0SAA.19.002142
   Mori S, 2020, IEEE T VIS COMPUT GR, V26, P2994, DOI 10.1109/TVCG.2020.3003768
   Newson A, 2014, SIAM J IMAGING SCI, V7, P1993, DOI 10.1137/140954933
   Niantic Inc, POKEMONGO
   Queguiner G, 2018, ADJUNCT PROCEEDINGS OF THE 2018 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P226, DOI 10.1109/ISMAR-Adjunct.2018.00073
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Siltanen S, 2017, VISUAL COMPUT, V33, P193, DOI 10.1007/s00371-015-1174-z
   Tareen Shaharyar Ahmed Khan, 2018, 2018 International Conference on Computing, Mathematics and Engineering Technologies (iCoMET). Proceedings, DOI 10.1109/ICOMET.2018.8346440
   Telea A., 2004, Journal of Graphics Tools, V9, P23, DOI 10.1080/10867651.2004.10487596
   Le TT, 2017, IEEE IMAGE PROC, P2094, DOI 10.1109/ICIP.2017.8296651
   Unity Technologies, ARFOUNDATION
   Wang C, 2019, AAAI CONF ARTIF INTE, P5232
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Woo S., 2020, 31 BRIT MACH VIS VIR
   Wu X, 2017, TSINGHUA SCI TECHNOL, V22, P660, DOI 10.23919/TST.2017.8195348
   Xu R, 2019, PROC CVPR IEEE, P3718, DOI 10.1109/CVPR.2019.00384
   Yagi K, 2017, ADJUNCT PROCEEDINGS OF THE 2017 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT), P334, DOI 10.1109/ISMAR-Adjunct.2017.101
   Yanhong Zeng, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P528, DOI 10.1007/978-3-030-58517-4_31
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
NR 36
TC 3
Z9 3
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2571
EP 2582
DI 10.1007/s00371-022-02479-1
EA APR 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000789001000001
DA 2024-07-18
ER

PT J
AU Marnissi, MA
   Fradi, H
   Sahbani, A
   Ben Amara, NE
AF Marnissi, Mohamed Amine
   Fradi, Hajer
   Sahbani, Anis
   Ben Amara, Najoua Essoukri
TI Feature distribution alignments for object detection in the thermal
   domain
SO VISUAL COMPUTER
LA English
DT Article
DE Unsupervised domain adaptation; Adversarial loss; Object detection;
   Thermal and visible; Domain classifier
AB Infrared imaging has recently played an important role in a wide range of applications including video surveillance, robotics and night vision. However, the manufacturing cost of high-resolution infrared cameras is more expensive regarding similar quality in visible cameras. This could explain the fact that thermal databases are less available compared to visible ones. In this paper, we mainly emphasis the need for aligning features from visible and thermal domains for object detection in order to ensure effective results in both domains without the need to retrain data and to perform additional annotations. To address that, we incorporate feature distribution alignments into faster R-CNN architecture at different levels. The resulting proposed adaptive detector has the advantage of covering different aspects of the domain shift in order to improve the overall performance. Using KAIST and FLIR ADAS datasets, the effectiveness of the proposed detector is assessed and better results are obtained compared to the baseline detector and to the obtained results by other existing works.
C1 [Marnissi, Mohamed Amine] Univ Sfax, Ecole Natl Ingenieurs Sfax, Sfax 3038, Tunisia.
   [Marnissi, Mohamed Amine; Ben Amara, Najoua Essoukri] Univ Sousse, LATIS Lab Adv Technol & Intelligent Syst, Ecole Natl Ingenieurs Sousse, Sousse 4023, Tunisia.
   [Fradi, Hajer] Univ Sousse, Inst Super Sci Appl, LATIS Lab Adv Technol & Intelligent Syst, Sousse 4023, Tunisia.
   [Sahbani, Anis] Enova Robot, Sousse 4023, Tunisia.
C3 Universite de Sfax; Ecole Nationale dIngenieurs de Sfax (ENIS);
   Universite de Sousse; Universite de Sousse
RP Fradi, H (corresponding author), Univ Sousse, Inst Super Sci Appl, LATIS Lab Adv Technol & Intelligent Syst, Sousse 4023, Tunisia.
EM marnissi.mohamedamine@eniso.u-sousse.tn; hajer.fradi@issatso.rnu.tn;
   anis.sahbani@enovarobotics.com; najoua.benamara@eniso.rnu.tn
FU DGVR research fund from the Tunisian Ministry of Higher Education and
   Scientific Research
FX This work has been funded by the DGVR research fund from the Tunisian
   Ministry of Higher Education and Scientific Research that is gratefully
   acknowledged.
CR Bayoudh K, 2022, VISUAL COMPUT, V38, P2939, DOI 10.1007/s00371-021-02166-7
   Berg A, 2018, IEEE COMPUT SOC CONF, P1224, DOI 10.1109/CVPRW.2018.00159
   Busto PP, 2017, IEEE I CONF COMP VIS, P754, DOI 10.1109/ICCV.2017.88
   Chang-Dong Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11721, DOI 10.1109/CVPR42600.2020.01174
   Chen Chaoqi, 2020, P IEEE CVF C COMP VI, P8869, DOI DOI 10.1109/CVPR42600.2020.00889
   Chen YH, 2021, INT J COMPUT VISION, V129, P2223, DOI 10.1007/s11263-021-01447-x
   Chen Y, 2018, PROC CVPR IEEE, P3339, DOI 10.1109/CVPR.2018.00352
   Dai XR, 2021, APPL INTELL, V51, P1244, DOI 10.1007/s10489-020-01882-2
   Devaguptapu C, 2019, IEEE COMPUT SOC CONF, P1029, DOI 10.1109/CVPRW.2019.00135
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gautam A, 2022, VISUAL COMPUT, V38, P4111, DOI 10.1007/s00371-021-02284-2
   Ghose D, 2019, IEEE COMPUT SOC CONF, P988, DOI 10.1109/CVPRW.2019.00130
   Huang, 2021, VISUAL COMPUT, P1
   Hwang S, 2015, PROC CVPR IEEE, P1037, DOI 10.1109/CVPR.2015.7298706
   Jiang BY, 2020, NEURAL COMPUT APPL, V32, P4743, DOI 10.1007/s00521-018-3846-x
   Jinyu Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P480, DOI 10.1007/978-3-030-58583-9_29
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kieu M., 2020, COMPUTER VISION ECCV
   Kim M, 2019, IEEE IMAGE PROC, P1650, DOI [10.1109/icip.2019.8803098, 10.1109/ICIP.2019.8803098]
   Kuang XD, 2020, INFRARED PHYS TECHN, V107, DOI 10.1016/j.infrared.2020.103338
   Li W, 2018, IEEE T PATTERN ANAL, V40, P1114, DOI 10.1109/TPAMI.2017.2704624
   Li XD, 2021, NEUROCOMPUTING, V429, P12, DOI 10.1016/j.neucom.2020.11.063
   Lin CZ, 2020, IEEE T IMAGE PROCESS, V29, P3820, DOI 10.1109/TIP.2020.2966371
   Liu H, 2020, NEUROCOMPUTING, V411, P510, DOI 10.1016/j.neucom.2020.06.066
   Liu Q, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3847, DOI 10.1145/3394171.3413922
   Liu W, 2019, PROC CVPR IEEE, P5182, DOI 10.1109/CVPR.2019.00533
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mhalla A, 2019, IEEE T INTELL TRANSP, V20, P4006, DOI 10.1109/TITS.2018.2876614
   MohamedAmine M., 2020, INT C PATTERN RECOGN
   Motiian S, 2017, IEEE I CONF COMP VIS, P5716, DOI 10.1109/ICCV.2017.609
   Nasiri A, 2019, APPL THERM ENG, V163, DOI 10.1016/j.applthermaleng.2019.114410
   Ouyang WL, 2016, INT J COMPUT VISION, V120, P14, DOI 10.1007/s11263-016-0890-9
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Park S, 2020, LASER SURG MED, V52, P218, DOI 10.1002/lsm.23157
   Rahman MM, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107124
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Saito K, 2019, PROC CVPR IEEE, P6949, DOI 10.1109/CVPR.2019.00712
   Sankaranarayanan S, 2018, PROC CVPR IEEE, P8503, DOI 10.1109/CVPR.2018.00887
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Wang M, 2018, NEUROCOMPUTING, V312, P135, DOI 10.1016/j.neucom.2018.05.083
   Wei LX, 2021, VISUAL COMPUT, V37, P133, DOI 10.1007/s00371-019-01787-3
   Xu D, 2017, PROC CVPR IEEE, P4236, DOI 10.1109/CVPR.2017.451
   Xu MH, 2020, AAAI CONF ARTIF INTE, V34, P6502
   Zellinger W, 2021, ANN MATH ARTIF INTEL, V89, P333, DOI 10.1007/s10472-020-09719-x
   Zhang H, 2020, IEEE IMAGE PROC, P276, DOI [10.1109/ICIP40778.2020.9191080, 10.1109/icip40778.2020.9191080]
   Zhang Hongyi, 2019, INT C LEARN REPR
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   ZOETGNANDE YWK, 2019, IEEE IJCNN
   Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI [10.1007/978-3-030-01219-9_, 10.1007/978-3-030-01219-9_18]
NR 49
TC 1
Z9 1
U1 5
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 1081
EP 1093
DI 10.1007/s00371-021-02386-x
EA FEB 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000762161100001
DA 2024-07-18
ER

PT J
AU Liang, XC
   Xu, L
   Zhang, WX
   Zhang, Y
   Liu, JF
   Liu, ZP
AF Liang, Xingcan
   Xu, Linsen
   Zhang, Wenxiang
   Zhang, Yan
   Liu, Jinfu
   Liu, Zhipeng
TI A convolution-transformer dual branch network for head-pose and
   occlusion facial expression recognition
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT ACIAT202
CY 2022
CL ELECTR NETWORK
DE Facial expression recognition; CNNs; Transformers; Feature fusion;
   Robust on occlusions and head-pose variations
AB Facial expression recognition (FER) has attracted much more attention due to its broad range of applications. Occlusions and head-pose variations are two major obstacles for automatic FER. In this paper, we propose a convolution-transformer dual branch network (CT-DBN) that takes advantage of local and global facial information to tackle the real-word occlusions and head-pose variant robust FER. The CT-DBN contains two branches. Taking into account local modeling ability of CNN, the first branch utilizes CNN to capture local edge information. Inspired by transformers' successful application in natural language processing, we employ transformer to the second branch to be responsible for obtaining better global representation. Then, a local-global feature fusion module is proposed to adaptively integrate both features to hybrid features and model the relationship between them. With the help of feature fusion module, our network not only integrates local and global features in an adaptive weighting manner but can also learn the corresponding distinguishable features autonomously. Experimental results under inner-database and cross-database evaluation on four leading facial expression databases illustrate that our proposed CT-DBN outperforms other state-of-the-art methods and achieves robust performance under in-the-wild condition.
C1 [Liang, Xingcan; Liu, Jinfu; Liu, Zhipeng] Chinese Acad Sci, Inst Intelligent Machines, Hefei Inst Phys Sci, Hefei 230031, Peoples R China.
   [Liang, Xingcan; Liu, Jinfu; Liu, Zhipeng] Univ Sci & Technol China, Hefei 230026, Peoples R China.
   [Zhang, Wenxiang] Changzhou Univ, Sch Microelect & Control Engn, Changzhou 213164, Peoples R China.
   [Zhang, Yan] Anhui Jianzhu Univ, Sch Elect & Informat Engn, Hefei 230009, Peoples R China.
   [Xu, Linsen] Hohai Univ, Coll Mech & Elect Engn, Changzhou 213022, Peoples R China.
C3 Chinese Academy of Sciences; Hefei Institutes of Physical Science, CAS;
   Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Changzhou University; Anhui Jianzhu University; Hohai
   University
RP Xu, L (corresponding author), Hohai Univ, Coll Mech & Elect Engn, Changzhou 213022, Peoples R China.
EM lxcan@mail.ustc.edu.cn; lsxu@hhu.edu.cn; 20080902034@smail.cczu.edu.cn;
   1213400288@qq.com; liujinfu@mail.ustc.edu.cn;
   liuzhipeng@mail.ustc.edu.cn
RI liu, zhipeng/HCI-3638-2022; LIU, ZHIPENG/GWQ-5972-2022
OI liang, xingcan/0000-0002-3407-2905
FU National Key RAMP;D Program of China [2017YFB1303200]; Jiangsu Special
   Project for Frontier Leading Base Technology [BK20192004]; Key Support
   Project of Dean Fund of Hefei Institutes of Physical Science, CAS
   [YZJJZX202017]; Strategic High-tech Innovation Fund of Chinese Academy
   of Sciences [GQRC-19-15]
FX This work was partially supported by National Key R&D Program of China
   (Grant No. 2017YFB1303200), Jiangsu Special Project for Frontier Leading
   Base Technology (Grant No. BK20192004), Key Support Project of Dean Fund
   of Hefei Institutes of Physical Science, CAS (Grant No. YZJJZX202017),
   and Strategic High-tech Innovation Fund of Chinese Academy of Sciences
   (Grant No. GQRC-19-15).
CR Agrawal A, 2020, VISUAL COMPUT, V36, P405, DOI 10.1007/s00371-019-01630-9
   Barsoum E, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P279, DOI 10.1145/2993148.2993165
   Cai J, 2018, IEEE INT CONF AUTOMA, P302, DOI 10.1109/FG.2018.00051
   Chen LF, 2018, INFORM SCIENCES, V428, P49, DOI 10.1016/j.ins.2017.10.044
   Dahmane Mohamed, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P884, DOI 10.1109/FG.2011.5771368
   Dai YM, 2021, IEEE WINT CONF APPL, P3559, DOI 10.1109/WACV48630.2021.00360
   Ding H., 2020, ARXIV PREPRINT ARXIV
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Falcon W., 2019, PYTORCH LIGHTNING
   Farzaneh AH, 2021, IEEE WINT CONF APPL, P2401, DOI 10.1109/WACV48630.2021.00245
   Girdhar R, 2019, PROC CVPR IEEE, P244, DOI 10.1109/CVPR.2019.00033
   Goodfellow IJ, 2015, NEURAL NETWORKS, V64, P59, DOI 10.1016/j.neunet.2014.09.005
   Han Kai, 2020, ARXIV201212556
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huiyu Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P108, DOI 10.1007/978-3-030-58548-8_7
   Kharghanian R, 2016, IEEE ENG MED BIO, P419, DOI 10.1109/EMBC.2016.7590729
   Kollias D, 2020, INT J COMPUT VISION, V128, P1455, DOI 10.1007/s11263-020-01304-3
   Li K, 2020, VISUAL COMPUT, V36, P391, DOI 10.1007/s00371-019-01627-4
   Li S, 2022, IEEE T AFFECT COMPUT, V13, P1195, DOI 10.1109/TAFFC.2020.2981446
   Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277
   Li W, 2018, IEEE T PATTERN ANAL, V40, P2583, DOI 10.1109/TPAMI.2018.2791608
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P2439, DOI 10.1109/TIP.2018.2886767
   Liang XC, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21030833
   Liu DZ, 2020, NEUROCOMPUTING, V413, P145, DOI 10.1016/j.neucom.2020.06.062
   Liu XF, 2019, PATTERN RECOGN, V88, P1, DOI 10.1016/j.patcog.2018.11.001
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lopes AT, 2017, PATTERN RECOGN, V61, P610, DOI 10.1016/j.patcog.2016.07.026
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949
   Ma F., 2021, ARXIV PREPRINT ARXIV
   Miao S, 2019, IEEE ACCESS, V7, P78000, DOI 10.1109/ACCESS.2019.2921220
   Mollahosseini A, 2016, IEEE WINT CONF APPL
   Qu XY, 2022, IEEE T EM TOP COMP I, V6, P580, DOI 10.1109/TETCI.2021.3070713
   Rouast PV, 2021, IEEE T AFFECT COMPUT, V12, P524, DOI 10.1109/TAFFC.2018.2890471
   Rudovic O, 2013, IEEE T PATTERN ANAL, V35, P1357, DOI 10.1109/TPAMI.2012.233
   Sariyanidi E, 2015, IEEE T PATTERN ANAL, V37, P1113, DOI 10.1109/TPAMI.2014.2366127
   Shan CF, 2009, IMAGE VISION COMPUT, V27, P803, DOI 10.1016/j.imavis.2008.08.005
   Sikander G, 2019, IEEE T INTELL TRANSP, V20, P2339, DOI 10.1109/TITS.2018.2868499
   Sikka K, 2012, LECT NOTES COMPUT SC, V7584, P250, DOI 10.1007/978-3-642-33868-7_25
   Cruz EAS, 2018, PATTERN RECOGN LETT, V114, P13, DOI 10.1016/j.patrec.2017.08.008
   Srinivas A, 2021, PROC CVPR IEEE, P16514, DOI 10.1109/CVPR46437.2021.01625
   Tang Y, 2021, IEEE T IMAGE PROCESS, V30, P444, DOI 10.1109/TIP.2020.3037467
   Touvron Hugo, 2020, ARXIV201212877
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang K, 2020, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR42600.2020.00693
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Wang ZN, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107694
   Wei W, 2020, J MULTIMODAL USER IN, V14, P17, DOI 10.1007/s12193-019-00308-9
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Xie WC, 2021, IEEE T CYBERNETICS, V51, P2787, DOI 10.1109/TCYB.2019.2925095
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Zeng GH, 2018, IEEE INT CONF AUTOMA, P423, DOI 10.1109/FG.2018.00068
   Zhang FF, 2018, PROC CVPR IEEE, P3359, DOI 10.1109/CVPR.2018.00354
   Zhang H., 2020, IEEE T COGNIT DEV SY, V6, P66
   Zhao GY, 2011, IMAGE VISION COMPUT, V29, P607, DOI 10.1016/j.imavis.2011.07.002
   Zheng MM, 2019, ACMIEEE INT CONF HUM, P604, DOI [10.1109/HRI.2019.8673163, 10.1109/hri.2019.8673163]
   Zhong L, 2015, IEEE T CYBERNETICS, V45, P1499, DOI 10.1109/TCYB.2014.2354351
NR 57
TC 20
Z9 20
U1 7
U2 43
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2023
VL 39
IS 6
SI SI
BP 2277
EP 2290
DI 10.1007/s00371-022-02413-5
EA FEB 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA J1YY4
UT WOS:000754455000001
DA 2024-07-18
ER

PT J
AU Gonzalez-Toledo, D
   Cuevas-Rodriguez, M
   Molina-Tanco, L
   Reyes-Lecuona, A
AF Gonzalez-Toledo, Daniel
   Cuevas-Rodriguez, Maria
   Molina-Tanco, Luis
   Reyes-Lecuona, Arcadio
TI Still room for improvement in traditional 3D interaction: selecting the
   fixed axis in the virtual trackball
SO VISUAL COMPUTER
LA English
DT Article
DE Usability; 3D interaction; 3D User interfaces; Virtual trackball
AB Virtual trackball techniques are widely used when 3D interaction is performed through interfaces with a reduced number of degrees of freedom such as mice and touchscreens. For decades, most implementations fix a vertical axis of rotation, which is a suitable choice when the vertical axis should indeed be fixed, according to some mental model of the user. We conducted an experiment involving the use of a mouse and a touch device to study usability in terms of performance, perceived usability and mental workload when selecting different fixed axes in accordance with the user's mental model. The results we obtained indicate that the consistency between the axis fixed by the technique and the object's intrinsic axis has a positive effect on usability. We believe that implementations that allow to select different fixed axis for each specific object should be considered when designing future reduced-DoF interaction interfaces.
C1 [Gonzalez-Toledo, Daniel; Cuevas-Rodriguez, Maria; Molina-Tanco, Luis; Reyes-Lecuona, Arcadio] Univ Malaga, Dept Tecnol Elect, Malaga 29071, Spain.
C3 Universidad de Malaga
RP Reyes-Lecuona, A (corresponding author), Univ Malaga, Dept Tecnol Elect, Malaga 29071, Spain.
EM dgonzalezt@uma.es; mariacuevas@uma.es; lmtanco@uma.es; areyes@uma.es
RI Reyes-Lecuona, Arcadio/M-7022-2014
OI Reyes-Lecuona, Arcadio/0000-0002-3699-4065; Cuevas Rodriguez,
   Maria/0000-0002-4698-5170; Molina Tanco, Luis/0000-0002-0476-0547;
   Gonzalez Toledo, Daniel/0000-0002-2698-2896
FU PLUGGY project within the European Union's Horizon 2020 research and
   innovation programme [726765]; Universidad de Malaga/CBUA of the
   CRUE-CSIC; Springer Nature; H2020 Societal Challenges Programme [726765]
   Funding Source: H2020 Societal Challenges Programme
FX This work was supported by the PLUGGY project
   (https://www.pluggyproject.eu/), within the European Union's Horizon
   2020 research and innovation programme, under grant agreement No 726765.
   The data and the application used to conduct the experiment are
   available upon request. Open Access funding provided by Universidad de
   Malaga/CBUA in the framework of the CRUE-CSIC agreement with Springer
   Nature.
CR [Anonymous], 2000, 159SC ISOTC
   [Anonymous], 2019, SKETCHF PUBL FIND 3D
   [Anonymous], 2019, Google SketchUp: 3D Design Software 3D Modeling on the Web SketchUp
   [Anonymous], 2012, AUTODESK 3DS MAX 3D
   Bade R, 2005, LECT NOTES COMPUT SC, V3638, P138
   Besançon L, 2021, COMPUT GRAPH FORUM, V40, P293, DOI 10.1111/cgf.14189
   Besancon L, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P4727, DOI 10.1145/3025453.3025863
   Blender Foundation, 2015, HOM BLEND PROJ FREE
   Borsci S, 2009, COGN PROCESS, V10, P193, DOI 10.1007/s10339-009-0268-9
   Brooke J, 1996, USABILITY EVALUATION, V189, P4
   Buda V., 2012, ROTATION TECHNIQUES
   Chen M., 1988, Computer Graphics, V22, P121, DOI 10.1145/378456.378497
   Cowan G., 1998, STAT DATA ANAL 1 EDI
   Cumming G, 2014, PSYCHOL SCI, V25, P7, DOI 10.1177/0956797613504966
   deArquer I., 2001, 544 NTP GOB ESP
   Decle F, 2009, P MOBILEHCI 09 NEW Y, DOI 10.1145/1613858.1613899
   Devin F., Sistema de Escalas de Usabilidad: (sic)que es y para que sirve?
   Forlines C, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P647
   Gonzalez-Toledo D., 2018, P EUROVR2018, V1, P3, DOI [10.5281/zenodo.2593170, DOI 10.5281/ZENODO.2593170]
   Gonzalez-Toledo D., 2017, J VIRT REAL BROADCAS, DOI [10.20385/1860-2037/14.2017.3, DOI 10.20385/1860-2037/14.2017.3]
   Hand C, 1997, COMPUT GRAPH FORUM, V16, P269, DOI 10.1111/1467-8659.00194
   Hart S. G., 2006, P HUM FACT ERG SOC A, V50, P904, DOI DOI 10.1177/154193120605000909
   Hommel B, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.01318
   Kratz S, 2010, IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI 2010), P111, DOI 10.1109/3DUI.2010.5444712
   Kunde W, 2007, HUM FACTORS, V49, P661, DOI 10.1518/001872007X215737
   Lewis JR, 2009, LECT NOTES COMPUT SC, V5619, P94, DOI 10.1007/978-3-642-02806-9_12
   Marchand É, 2002, VISUAL COMPUT, V18, P1, DOI 10.1007/s003710100122
   Martinez D, 2010, VISUAL COMPUT, V26, P619, DOI 10.1007/s00371-010-0499-x
   Müsseler J, 2011, HUM FACTORS, V53, P383, DOI 10.1177/0018720811408599
   Norman Don, 1983, Mental Models
   OfficialGoogle B., 2016, 3D WAR
   Proctor R.W., 2008, Human Factors in Simple and Complex Systems Boca Raton
   Rybicki S., 2016, P GRAPHICS INTERFACE, P93
   Sauro J, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P2347
   Sauro Jeff., 2011, MeasuringU
   SCHMIDT RA, 1975, PSYCHOL REV, V82, P225, DOI 10.1037/h0076770
   Shaw C, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P185
   SHOEMAKE K, 1992, GRAPH INTER, P151
   Shoemake K., 1994, Graphics Gems IV, P175
   Thornton R.W., 1979, P 6 ANN C COMPUTER G, V13, P102, DOI [10.1145/800249.807430, DOI 10.1145/800249.807430]
   VandenBos G.R., 2009, PUBL MAN AM PSYCH AS
   Yu LY, 2010, IEEE T VIS COMPUT GR, V16, P1613, DOI 10.1109/TVCG.2010.157
NR 42
TC 2
Z9 2
U1 2
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 1149
EP 1162
DI 10.1007/s00371-021-02394-x
EA JAN 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000749149200001
OA hybrid, Green Published
DA 2024-07-18
ER

PT J
AU Song, XJ
   Huang, JJ
   Cao, JZ
   Song, DW
AF Song, Xijuan
   Huang, Jijiang
   Cao, Jianzhong
   Song, Dawei
TI Feature spatial pyramid network for low-light image enhancement
SO VISUAL COMPUTER
LA English
DT Article
DE Low-light image enhancement; Feature spatial pyramid network;
   Illumination image; Reflection image; Color loss
ID HISTOGRAM EQUALIZATION
AB Low-light images usually contain high noise and low contrast. This brings bad visual feelings and hinders subsequent computer vision work. At present, many algorithms have been proposed to enhance low-light images. However, the existing methods still have some problems, such as insufficient enhancement, color distortion, or overexposure. In this paper, we propose a low-light image enhancement network based on the spatial pyramid to solve the problems existing in other methods, so as to make the enhancement result closer to the normal illumination image in brightness and color. The network is divided into two parts. Firstly, the decomposition network is designed based on Retinex theory, and the image is decomposed into the illumination image and reflection image. Then, the illumination image is processed through the three convolution kernels on the spatial pyramid module to obtain three sets of features with different scales. Next, we concatenate these three groups of features together. And the concatenated features are extracted through a convolution kernel to obtain the enhanced illumination image. Finally, the enhanced illumination image and the decomposed reflection image are multiplied pixel by pixel to obtain an enhanced image. In addition, we introduce a color loss function to solve the problem of color distortion. The experimental results show that the proposed algorithm has better visual feelings than other algorithms. We also calculate the peak signal-to-noise ratio, structural similarity index and average brightness of the enhanced results of different algorithms, and the results show that the proposed algorithm performs better.
C1 [Song, Xijuan; Huang, Jijiang; Cao, Jianzhong; Song, Dawei] Chinese Acad Sci, Xian Inst Opt & Precis Mech, Xian 710119, Peoples R China.
   [Song, Xijuan; Song, Dawei] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Xi'an Institute of Optics & Precision
   Mechanics, CAS; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS
RP Huang, JJ (corresponding author), Chinese Acad Sci, Xian Inst Opt & Precis Mech, Xian 710119, Peoples R China.
EM songxijuan@opt.cn; huangjijiang@opt.ac.cn; cjz@opt.ac.cn;
   songdawei2018@opt.cn
CR Abdullah-Al-Wadud M, 2007, IEEE T CONSUM ELECTR, V53, P593, DOI 10.1109/TCE.2007.381734
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen SD, 2003, IEEE T CONSUM ELECTR, V49, P1310, DOI 10.1109/TCE.2003.1261234
   Chen ZH, 2020, VISUAL COMPUT, V36, P2189, DOI 10.1007/s00371-020-01929-y
   Cui B, 2016, INT CONF SYST INFORM, P891, DOI 10.1109/ICSAI.2016.7811077
   DHAWAN AP, 1986, IEEE T MED IMAGING, V5, P8, DOI 10.1109/TMI.1986.4307733
   DHAWAN AP, 1988, COMPUT METH PROG BIO, V27, P23, DOI 10.1016/0169-2607(88)90100-9
   Dong XC, 2011, INT C PAR DISTRIB SY, P9, DOI 10.1109/ICPADS.2011.115
   Fang J, 2021, NEUROCOMPUTING, V424, P193, DOI 10.1016/j.neucom.2019.12.057
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Gonzales R.C., 2002, Digital image processing
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Ignatov A, 2017, IEEE I CONF COMP VIS, P3297, DOI 10.1109/ICCV.2017.355
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Joshi P, 2020, VISUAL COMPUT, V36, P71, DOI 10.1007/s00371-018-1587-6
   Kim YT, 1997, IEEE T CONSUM ELECTR, V43, P1, DOI 10.1109/30.580378
   Lee CH, 2013, 2013 INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY & INTERNET-BASED SYSTEMS (SITIS), P43, DOI 10.1109/SITIS.2013.19
   Lee J, 2020, VISUAL COMPUT, V36, P2129, DOI 10.1007/s00371-020-01921-6
   Li CY, 2018, PATTERN RECOGN LETT, V104, P15, DOI 10.1016/j.patrec.2018.01.010
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Li YC, 2012, 2012 13TH INTERNATIONAL CONFERENCE ON PARALLEL AND DISTRIBUTED COMPUTING, APPLICATIONS, AND TECHNOLOGIES (PDCAT 2012), P653, DOI 10.1109/PDCAT.2012.97
   Liu Y, 2016, IEEE SIGNAL PROC LET, V23, P1882, DOI 10.1109/LSP.2016.2618776
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Pisano ED, 1998, J DIGIT IMAGING, V11, P193, DOI 10.1007/BF03178082
   Poddar S, 2013, IET IMAGE PROCESS, V7, P641, DOI 10.1049/iet-ipr.2012.0507
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412
   Ren XT, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351427
   Ren YR, 2019, IEEE T CIRC SYST VID, V29, P968, DOI 10.1109/TCSVT.2018.2828141
   Shen L., 2017, ARXIV PREPRINT ARXIV, P171102488
   Song DW, 2021, IEEE T IMAGE PROCESS, V30, P7567, DOI 10.1109/TIP.2021.3106798
   Song XJ, 2021, SIGNAL IMAGE VIDEO P, V15, P1257, DOI 10.1007/s11760-021-01856-y
   Stark JA, 2000, IEEE T IMAGE PROCESS, V9, P889, DOI 10.1109/83.841534
   Wang CX, 2021, VISUAL COMPUT, V37, P77, DOI 10.1007/s00371-020-01888-4
   Wang LW, 2020, IEEE T IMAGE PROCESS, V29, P7984, DOI 10.1109/TIP.2020.3008396
   Wang Q, 2007, IEEE T CONSUM ELECTR, V53, P757, DOI 10.1109/TCE.2007.381756
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang Y, 1999, IEEE T CONSUM ELECTR, V45, P68, DOI 10.1109/30.754419
   Wang YQ, 2021, VISUAL COMPUT, V37, P261, DOI 10.1007/s00371-020-01797-6
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Xiaochu W., 2019, J CHINA U POSTS TELE, V(5), P6
   Ying ZQ, 2017, LECT NOTES COMPUT SC, V10425, P36, DOI 10.1007/978-3-319-64698-5_4
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
NR 48
TC 6
Z9 6
U1 1
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 489
EP 499
DI 10.1007/s00371-021-02343-8
EA JAN 2022
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000749149200002
DA 2024-07-18
ER

PT J
AU Ye, WJ
   Zhu, XK
   Liu, YJ
AF Ye, Wujian
   Zhu, Xueke
   Liu, Yijun
TI Multi-semantic preserving neural style transfer based on Y channel
   information of image
SO VISUAL COMPUTER
LA English
DT Article
DE Neural style transfer; Semantic preservation; Multi-semantic loss; Y
   channel information
AB Neural style transfer, as a new auxiliary means for digital art design, can reduce the threshold of technical design and improve the efficiency of creation. The existing methods have achieved good results in terms of speed and style quantity, but most of them change or erase the semantic information of the original content image to varying degrees during the process of stylization, resulting in the loss of most of the original content features and emotion; although some methods can maintain specific original semantic mentioned above, they need to introduce a corresponding semantic description network, leading to a relatively complex stylization framework. In this paper, we propose a multi-semantic preserving fast style transfer approach based on Y channel information. By constructing a multi-semantic loss consisting of a feature loss and a structure loss derived from a pre-trained VGG network with the input of Y channel image and content image, the training of stylization model is constrained to realize the multi-semantic preservation. The experiments indicate that our stylization model is relatively light and simple, and the generated artworks can effectively maintain the original multi-semantic information including salience, depth and edge semantics, emphasize the original content features and emotional expression and show better visual effects.
C1 [Ye, Wujian; Zhu, Xueke; Liu, Yijun] Guangdong Univ Technol, Sch Informat Engn, Guangzhou 510006, Guangdong, Peoples R China.
C3 Guangdong University of Technology
RP Liu, YJ (corresponding author), Guangdong Univ Technol, Sch Informat Engn, Guangzhou 510006, Guangdong, Peoples R China.
EM 371785144@qq.com
FU Key-Area Research and Development Program of Guangdong Province
   [2018B030338001, 2018B010115002, 2018B010107003]; Innovative Talents
   Program of Guangdong Education Department and Young Hundred Talents
   Project of Guangdong University of Technology [220413548]
FX This work was supported in part by Key-Area Research and Development
   Program of Guangdong Province under Grant Nos. 2018B030338001,
   2018B010115002, 2018B010107003, and in part by Innovative Talents
   Program of Guangdong Education Department and Young Hundred Talents
   Project of Guangdong University of Technology under grant No. 220413548.
CR [Anonymous], 2016, ARXIV160104568
   Champandard AJ, ARXIV160301768
   Chen W, 2016, ADV NEUR IN, V29
   Cheng MM, 2020, IEEE T IMAGE PROCESS, V29, P909, DOI 10.1109/TIP.2019.2936746
   Deng X, 2019, IEEE I CONF COMP VIS, P3076, DOI 10.1109/ICCV.2019.00317
   Dumoulin V.., 2017, P INT C LEARN REPR I
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jing YC, 2020, IEEE T VIS COMPUT GR, V26, P3365, DOI 10.1109/TVCG.2019.2921336
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li P, 2020, IEEE ACCESS, V8, P199147, DOI 10.1109/ACCESS.2020.3034653
   Li R, 2021, IEEE T IMAGE PROCESS, V30, P374, DOI 10.1109/TIP.2020.3036754
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24
   Liu X.C., 2017, Proceedings of the Symposium on Non-Photorealistic Animation and Rendering, page, P1
   Liu Yan., 2017, ARXIV170701926, V1707, P01926
   Liu YJ, 2019, IEEE ACCESS, V7, P40027, DOI 10.1109/ACCESS.2019.2891576
   Reimann M, 2019, VISUAL COMPUT, V35, P1531, DOI 10.1007/s00371-019-01654-1
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tian J., 2018, COMPUT SYST APPL, V27, P43
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 26
TC 3
Z9 3
U1 2
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2023
VL 39
IS 2
BP 609
EP 623
DI 10.1007/s00371-021-02361-6
EA JAN 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8T2TV
UT WOS:000741634600002
DA 2024-07-18
ER

PT J
AU Zhang, XX
   Xi, YF
   Huang, ZT
   Zheng, LT
   Huang, H
   Xiong, YS
   Xu, K
AF Zhang, Xinxin
   Xi, Yuefeng
   Huang, Zhentao
   Zheng, Lintao
   Huang, Hui
   Xiong, Yueshan
   Xu, Kai
TI Active hand-eye calibration via online accuracy-driven next-best-view
   selection
SO VISUAL COMPUTER
LA English
DT Article
DE Hand-eye calibration; Automatic calibration; Next-best-view selection
ID SIMULTANEOUS ROBOT-WORLD; FORM AX; EQUATIONS
AB We propose a novel high-accuracy active hand-eye calibration approach. In our method, the robot movement and camera view selection are both driven by and targeting the improvement of calibration accuracy. During the calibration process, the data acquisition is guided by an online estimated discrete viewing quality field (DVQF), representing the calibration quality of different views in various 3D locations. The view quality is measured by how much it reduces the uncertainty of calibration results and increases the diversity of robot poses, contributing to the calibration precision. Based on DVQF, we select the next-best-view as the target moving pose for each time step. A fully automatic system is presented to perform the overall hand-eye calibration process without any human intervention. Numerous experiments are conducted both in real-world and simulated scenarios. The proposed algorithm outperforms other approaches and shows much superiority in accuracy and robustness.
C1 [Zhang, Xinxin; Xi, Yuefeng; Zheng, Lintao; Xiong, Yueshan; Xu, Kai] Natl Univ Def Technol, Changsha 410000, Peoples R China.
   [Huang, Zhentao] Univ Nottingham Ningbo China, Ningbo 315100, Peoples R China.
   [Huang, Hui] Shenzhen Univ, Shenzhen 518061, Peoples R China.
C3 National University of Defense Technology - China; University of
   Nottingham Ningbo China; Shenzhen University
RP Zheng, LT (corresponding author), Natl Univ Def Technol, Changsha 410000, Peoples R China.
EM lintaozheng1991@gmail.com; kevin.kai.xu@gmail.com
RI Huang, Zhentao/HNC-3467-2023; Huang, Hui/JGB-1049-2023
OI Huang, Hui/0000-0003-3212-0544
FU National Key Research and Development Program of China [2018AAA0102200]
FX This work was supported by the National Key Research and Development
   Program of China (No. 2018AAA0102200).
CR Ali I, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19122837
   Antonello M, 2017, 2017 EUROPEAN CONFERENCE ON MOBILE ROBOTS (ECMR)
   Chai XJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION (ICMA), P57, DOI 10.1109/ICMA.2013.6617893
   Dekel A., 2020, P IEEECVF C COMPUTER, P13598
   Dornaika F, 1998, IEEE T ROBOTIC AUTOM, V14, P617, DOI 10.1109/70.704233
   Flandin G., 2000, PROC IEEE INT C ROBO, P2741
   Freese, 2019, V REP
   HORAUD R, 1995, INT J ROBOT RES, V14, P195, DOI 10.1177/027836499501400301
   Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6
   Levine S, 2018, INT J ROBOT RES, V37, P421, DOI 10.1177/0278364917710318
   Li AG, 2010, INT J PHYS SCI, V5, P1530
   Pachtrachai K, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P2485, DOI 10.1109/IROS.2016.7759387
   PARK FC, 1994, IEEE T ROBOTIC AUTOM, V10, P717, DOI 10.1109/70.326576
   Quigley M, 2009, IEEE INT CONF ROBOT, P3604
   Schmidt J., 2005, JOINT PATT REC S
   Shah M, 2013, J MECH ROBOT, V5, DOI 10.1115/1.4024473
   SHIU YC, 1989, IEEE T ROBOTIC AUTOM, V5, P16, DOI 10.1109/70.88014
   Tabb A, 2017, MACH VISION APPL, V28, P569, DOI 10.1007/s00138-017-0841-7
   TSAI RY, 1989, IEEE T ROBOTIC AUTOM, V5, P345, DOI 10.1109/70.34770
   Virtanen P, 2020, NAT METHODS, V17, P261, DOI 10.1038/s41592-019-0686-2
   Wang J, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4193, DOI 10.1109/IROS.2016.7759617
   ZHUANG HQ, 1994, IEEE T ROBOTIC AUTOM, V10, P549, DOI 10.1109/70.313105
NR 22
TC 2
Z9 2
U1 2
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 381
EP 391
DI 10.1007/s00371-021-02336-7
EA JAN 2022
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000737733100001
DA 2024-07-18
ER

PT J
AU Akay, S
   Arica, N
AF Akay, Simge
   Arica, Nafiz
TI Stacking multiple cues for facial action unit detection
SO VISUAL COMPUTER
LA English
DT Article
DE Facial action unit detection; Deep neural network; Stacking classifiers;
   Facial expression analysis
ID EXPRESSION; REPRESENTATION; RECOGNITION; FACE
AB In this study, we develop a deep learning-based stacking scheme to detect facial action units (AU) in video data. Given a sequence of video frames, it combines multiple cues extracted from the AU detectors employing in frame, segment, and transition levels. Frame-based detector takes a single frame to determine the existence of AU by employing static face features. Segment-based detector examines various length of subsequences in the neighborhood of a frame to detect whether that frame is an element of an AU segment. Transition-based detector attempts to find the transitions from neutral faces containing no AUs to emotional faces or vice versa, by analyzing fixed size subsequences. The frame subsequences in segment and transition detectors are represented by motion history image, which models the temporal changes in faces. Each detector employs a separate convolutional neural network and, then their results are fed into a meta-classifier to learn the combining method. Combining multiple cues in different levels with a framework containing entirely deep networks improves the detection performance by both locating subtle AUs and tracking small changes in the facial muscles' movements. In performance analysis, it is shown that the proposed approach significantly outperforms the state of the art methods, when compared on CK+, DISFA, and BP4D databases.
C1 [Akay, Simge; Arica, Nafiz] Bahcesehir Univ, Istanbul, Turkey.
C3 Bahcesehir University
RP Akay, S (corresponding author), Bahcesehir Univ, Istanbul, Turkey.
EM simge.akay@eng.bau.edu.tr
RI Arica, Nafiz/ADD-3793-2022
FU Scientific and Technological Research Council of Turkey (TUBITAK)
   [115E310]
FX This work is supported by The Scientific and Technological Research
   Council of Turkey (TUBITAK) under Grant No. 115E310.
CR Asthana A, 2014, PROC CVPR IEEE, P1859, DOI 10.1109/CVPR.2014.240
   Bihan Jiang, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P314, DOI 10.1109/FG.2011.5771416
   Bravo JA, 2011, P NATL ACAD SCI USA, V108, P16050, DOI 10.1073/pnas.1102999108
   Broekens J, 2007, LECT NOTES COMPUT SC, V4451, P113
   Chen Y, 2021, IEEE T CLOUD COMPUT, V9, P1050, DOI 10.1109/TCC.2019.2898657
   Chu W. S., 2016, ARXIV160800911
   Corneanu C, 2018, LECT NOTES COMPUT SC, V11216, P309, DOI 10.1007/978-3-030-01258-8_19
   Cui Zijun, 2020, NeurIPS, V33, P14338
   Danelakis A, 2016, VISUAL COMPUT, V32, P257, DOI 10.1007/s00371-015-1142-7
   Davis JW, 1997, PROC CVPR IEEE, P928, DOI 10.1109/CVPR.1997.609439
   De la Torre F, 2011, LECT NOTES COMPUT SC, V6974, P57, DOI 10.1007/978-3-642-24600-5_9
   Ding XY, 2013, IEEE I CONF COMP VIS, P2400, DOI 10.1109/ICCV.2013.298
   Duan H, 2009, PATTERN RECOGN LETT, V30, P1384, DOI 10.1016/j.patrec.2009.07.006
   EKMAN P, 1971, J PERS SOC PSYCHOL, V17, P124, DOI 10.1037/h0030377
   Huang YB, 2022, VISUAL COMPUT, V38, P871, DOI 10.1007/s00371-021-02056-y
   Jaiswal S, 2016, IEEE WINT CONF APPL
   Joseph A, 2020, VISUAL COMPUT, V36, P529, DOI 10.1007/s00371-019-01628-3
   Li GB, 2019, AAAI CONF ARTIF INTE, P8594
   Li W, 2017, PROC CVPR IEEE, P6766, DOI 10.1109/CVPR.2017.716
   Li W, 2017, IEEE INT CONF AUTOMA, P103, DOI 10.1109/FG.2017.136
   Liu ZL, 2020, LECT NOTES COMPUT SC, V11962, P489, DOI 10.1007/978-3-030-37734-2_40
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Martinez B, 2019, IEEE T AFFECT COMPUT, V10, P325, DOI 10.1109/TAFFC.2017.2731763
   Mavadati SM, 2013, IEEE T AFFECT COMPUT, V4, P151, DOI 10.1109/T-AFFC.2013.4
   Pantic M, 2006, IEEE T SYST MAN CY B, V36, P433, DOI 10.1109/TSMCB.2005.859075
   Pei WJ, 2018, IEEE T NEUR NET LEAR, V29, P920, DOI 10.1109/TNNLS.2017.2651018
   Romero A., 2018, MULTIVIEW DYNAMIC FA
   Rudovic O, 2012, LECT NOTES COMPUT SC, V7584, P260, DOI 10.1007/978-3-642-33868-7_26
   Shao Z., 2018, IEEE T AFFECT COMPUT
   Shao ZW, 2022, IEEE T AFFECT COMPUT, V13, P1274, DOI 10.1109/TAFFC.2019.2948635
   Shao ZW, 2018, LECT NOTES COMPUT SC, V11217, P725, DOI 10.1007/978-3-030-01261-8_43
   Song TF, 2021, AAAI CONF ARTIF INTE, V35, P5993
   Sumathi CP., 2012, International Journal of Computer Science Engineering Survey, P47
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tang CG, 2017, IEEE INT CONF AUTOMA, P878, DOI 10.1109/FG.2017.113
   Valstar MF, 2012, IEEE T SYST MAN CY B, V42, P28, DOI 10.1109/TSMCB.2011.2163710
   Vinolin V, 2021, VISUAL COMPUT, V37, P2369, DOI 10.1007/s00371-020-01992-5
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Zeng JB, 2015, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2015.413
   Zhang X, 2014, IMAGE VISION COMPUT, V32, P692, DOI 10.1016/j.imavis.2014.06.002
   Zhang ZY, 2018, LECT NOTES COMPUT SC, V11335, P226, DOI 10.1007/978-3-030-05054-2_17
   Zhao KL, 2016, PROC CVPR IEEE, P3391, DOI 10.1109/CVPR.2016.369
   Zhao KL, 2015, PROC CVPR IEEE, P2207, DOI 10.1109/CVPR.2015.7298833
   Zhi RC, 2020, VISUAL COMPUT, V36, P1067, DOI 10.1007/s00371-019-01707-5
   Zhong L, 2015, IEEE T CYBERNETICS, V45, P1499, DOI 10.1109/TCYB.2014.2354351
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 47
TC 4
Z9 4
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 4235
EP 4250
DI 10.1007/s00371-021-02291-3
EA SEP 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000698142800001
DA 2024-07-18
ER

PT J
AU Wang, L
   Sun, Y
   Wang, Z
AF Wang, Lei
   Sun, Yu
   Wang, Zheng
TI CCS-GAN: a semi-supervised generative adversarial network for image
   classification
SO VISUAL COMPUTER
LA English
DT Article
DE Generative adversarial network; Semi-supervised image classification;
   Cluster consistency loss; Enhanced feature matching
AB Generative adversarial network (GAN) has been successfully extended to solve semi-supervised image classification tasks recently. However, it is still a great challenge for GAN to exploit the unlabeled images for boosting its classification ability when labeled images are very limited. In this paper, we propose a novel CCS-GAN model for semi-supervised image classification, which aims to improve its classification ability by utilizing the cluster structure of unlabeled images and 'bad' generated images. Specifically, it employs a new cluster consistency loss to constrain its classifier to keep the local discriminative consistency in each cluster of unlabeled images and thus provides implicit supervised information to boost the classifier. Meanwhile, it adopts an enhanced feature matching approach to encourage its generator to produce adversarial images from the low-density regions of real distribution, which can enhance the discriminative ability of the classifier during adversarial training and suppress the mode collapse problem. Extensive experiments on four benchmark datasets show that: the proposed CCS-GAN achieves very competitive performance in semi-supervised image classification tasks when compared with several state-of-the-art competitors.
C1 [Wang, Lei; Wang, Zheng] Southwestern Univ Finance & Econ, Sch Econ Informat Engn, Chengdu, Peoples R China.
   [Sun, Yu] Univ Cent Arkansas, Dept Comp Sci, Conway, AR USA.
C3 Southwestern University of Finance & Economics - China; University of
   Central Arkansas
RP Sun, Y (corresponding author), Univ Cent Arkansas, Dept Comp Sci, Conway, AR USA.
EM dr.wangl@163.com; yusun@uca.edu; 2410391371@qq.com
FU Research Planning Fund of Humanities and Social Sciences of the Ministry
   of Education, China [16XJAZH002]; Fundamental Research Funds for the
   Central Universities, China [JBK2102049]; China Scholarship Council
   (CSC); Faculty Sabbatical Leave Fund from University of Central
   Arkansas, USA
FX This work is partially supported by the Research Planning Fund of
   Humanities and Social Sciences of the Ministry of Education, China (No.
   16XJAZH002), the Fundamental Research Funds for the Central
   Universities, China (No. JBK2102049), the China Scholarship Council
   (CSC) and the Faculty Sabbatical Leave Fund from University of Central
   Arkansas, USA.
CR Alpaydin E., 2010, INTRO MACHINE LEARNI, V1191, P68485
   [Anonymous], 2015, P INT C LEARNING REP
   [Anonymous], 2018, 6 INT C LEARN REPR I
   Bengio Y., 2007, Advances in neural information processing systems, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024
   Brock Andrew, 2018, ARXIV180911096
   Chapelle O., 2002, P 15 INT C NEURAL IN, P601
   Chen Z., 2020, ARXIV200703844
   Dai ZH, 2017, 31 ANN C NEURAL INFO, V30
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hu C, 2019, IEEE T COGN DEV SYST, V11, P539, DOI 10.1109/TCDS.2018.2875462
   [金炜东 Jin Weidong], 2018, [中国科学. 信息科学, Scientia Sinica Informationis], V48, P888
   Kingma D. P., 2014, arXiv
   Kingma DP, 2014, ADV NEUR IN, V27
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar A, 2017, ADV NEUR IN, V30
   Kuncheva LI, 2010, IEEE T MED IMAGING, V29, P531, DOI 10.1109/TMI.2009.2037756
   Laine S., 2016, ARXIV161002242
   LeCun Y., 2010, MNIST HANDWRITTEN DI
   Li CX, 2017, ADV NEUR IN, V30
   Li WY, 2020, MACH VISION APPL, V31, DOI 10.1007/s00138-020-01096-z
   Liang YQ, 2020, INTEGR COMPUT-AID E, V27, P417, DOI 10.3233/ICA-200641
   Luo YC, 2018, PROC CVPR IEEE, P8896, DOI 10.1109/CVPR.2018.00927
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Netzer Yuval, 2011, ADV NEUR INF PROC SY
   Ni Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2588
   Odena A, 2017, PR MACH LEARN RES, V70
   Pan ZQ, 2019, IEEE ACCESS, V7, P36322, DOI 10.1109/ACCESS.2019.2905015
   Qi GJ, 2018, PROC CVPR IEEE, P1517, DOI 10.1109/CVPR.2018.00164
   Qiao SY, 2018, LECT NOTES COMPUT SC, V11219, P142, DOI 10.1007/978-3-030-01267-0_9
   Rasmus A, 2015, ADV NEUR IN, V28
   Salimans T, 2016, ADV NEUR IN, V29
   Song HT, 2021, VISUAL COMPUT, V37, P2285, DOI 10.1007/s00371-020-01986-3
   Song MF, 2021, VISUAL COMPUT, V37, P497, DOI 10.1007/s00371-020-01819-3
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tarvainen A, 2017, ADV NEUR IN, V30
   Wei X, 2018, ARXIV180301541
   Xu ZW, 2021, MULTIMED TOOLS APPL, V80, P17461, DOI 10.1007/s11042-020-09602-1
   Yalniz Ismet Zeki, 2019, Billion-scale semi-supervised learning for image classification
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu X., 2009, Synth. Lect. Artif. Intell. Mach. Learn, V3, P1, DOI [10.2200/S00196ED1V01Y200906AIM006, DOI 10.2200/S00196ED1V01Y200906AIM006]
NR 42
TC 12
Z9 13
U1 4
U2 35
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2022
VL 38
IS 6
BP 2009
EP 2021
DI 10.1007/s00371-021-02262-8
EA JUL 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1C6YN
UT WOS:000679324400001
DA 2024-07-18
ER

PT J
AU Sun, Q
   Chen, Y
   Tao, WY
   Jiang, H
   Zhang, M
   Chen, K
   Erdt, M
AF Sun, Qian
   Chen, Yan
   Tao, Wenyuan
   Jiang, Han
   Zhang, Mu
   Chen, Kan
   Erdt, Marius
TI A GAN-based approach toward architectural line drawing colorization
   prototyping
SO VISUAL COMPUTER
LA English
DT Article
DE Architectural illustration; GAN; Colorization; Line drawing
AB Line drawing with colorization is a popular art format and tool for architectural illustration. The goal of this research is toward generating a high-quality and natural-looking colorization based on an architectural line drawing. This paper presents a new Generative Adversarial Network (GAN)-based method, named ArchGANs, including ArchColGAN and ArchShdGAN. ArchColGAN is a GAN-based line-feature-aware network for stylized colorization generation. ArchShdGAN is a lighting effects generation network, from which the building depiction in 3D can benefit. In particular, ArchColGAN is able to maintain the important line features and the correlation property of building parts as well as reduce the uneven colorization caused by sparse lines. Moreover, we proposed a color enhancement method to further improve ArchColGAN. Besides the single line drawing images, we also extend our method to handle line drawing image sequences and achieve rotation animation. Experiments and studies demonstrate the effectiveness and usefulness of our proposed method for colorization prototyping.
C1 [Sun, Qian; Tao, Wenyuan] Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China.
   [Chen, Yan; Jiang, Han; Zhang, Mu] Tianjin Univ, Tianjin, Peoples R China.
   [Tao, Wenyuan] Tianjin Univ, Sch Comp Software, Tianjin, Peoples R China.
   [Chen, Kan; Erdt, Marius] Fraunhofer Singapore, Singapore, Singapore.
   [Erdt, Marius] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
C3 Tianjin University; Tianjin University; Tianjin University; Nanyang
   Technological University
RP Chen, K (corresponding author), Fraunhofer Singapore, Singapore, Singapore.
EM kchen1@e.ntu.edu.sg
RI sun, qian/JMQ-8920-2023
OI Sun, Qian/0000-0002-2098-9878
FU NSFC, China [61702363, 51978441]; National Research Foundation,
   Singapore under its International Research Centres in Singapore Funding
   Initiative; Projekt DEAL
FX Open Access funding enabled and organized by Projekt DEAL. This research
   is supported by NSFC Grants (61702363, 51978441), China and the National
   Research Foundation, Singapore under its International Research Centres
   in Singapore Funding Initiative. Any opinions, findings and conclusions
   or recommendations expressed in this material are those of the author(s)
   and do not reflect the views of National Research Foundation, Singapore.
CR Abadi M, 2016, ACM SIGPLAN NOTICES, V51, P1, DOI [10.1145/3022670.2976746, 10.1145/2951913.2976746]
   Bousseau A., 2006, P NPAR, P141, DOI [DOI 10.1145/1124728.1124751, 10.1145/1124728.1124751]
   Byeon W, 2018, LECT NOTES COMPUT SC, V11220, P781, DOI 10.1007/978-3-030-01270-0_46
   Capcom, 2008, STREET FIGHT 4
   Chen DD, 2017, IEEE I CONF COMP VIS, P1114, DOI 10.1109/ICCV.2017.126
   Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296
   Chu NSH, 2005, ACM T GRAPHIC, V24, P504, DOI 10.1145/1073204.1073221
   Clark Aidan, 2019, Adversarial video generation on complex datasets
   Corel, 2011, PAINT 12
   Curtis C. J., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P421, DOI 10.1145/258734.258896
   DeCarlo D, 2003, ACM T GRAPHIC, V22, P848, DOI 10.1145/882262.882354
   Fang L., 2013, VISUAL COMPUT, V35, P1681
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hertzmann A, 2003, IEEE COMPUT GRAPH, V23, P70, DOI 10.1109/MCG.2003.1210867
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang H, 2011, VISUAL COMPUT, V27, P861, DOI 10.1007/s00371-011-0596-5
   Huang XW, 2018, PROCEEDINGS OF 2018 IEEE INTERNATIONAL CONFERENCE ON INTEGRATED CIRCUITS, TECHNOLOGIES AND APPLICATIONS (ICTA 2018), P172, DOI 10.1109/CICTA.2018.8706048
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Judd T, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239470
   Karras T., 2018, arXiv, DOI [10.48550/arXiv.1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Kim H, 2019, IEEE I CONF COMP VIS, P9055, DOI 10.1109/ICCV.2019.00915
   Kim T, 2017, PR MACH LEARN RES, V70
   Kolomenkin M, 2009, PROC CVPR IEEE, P2759
   Lei SIE, 2004, LECT NOTES COMPUT SC, V3333, P474
   Liao J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073683
   Liu MY, 2017, ADV NEUR IN, V30
   LONG QX, 2020, NAT MED, V26, P845, DOI [10.1101/2020.03.18.20038018, DOI 10.1038/S41591-020-0897-1, 10.1038/s41591-020-0897-1]
   Luft T., 2006, Proceedings of the 4th International Symposium on Non-Photorealistic Animation and Rendering, NPAR'06, P11, DOI [10.1145/1124728.1124732, DOI 10.1145/1124728.1124732]
   Luft T., 2008, COMPUTATIONAL AESTHE, P57
   Ohtake Y, 2004, ACM T GRAPHIC, V23, P609, DOI 10.1145/1015706.1015768
   Okaichi N, 2008, VISUAL COMPUT, V24, P753, DOI 10.1007/s00371-008-0257-5
   Schaller T.W., 1997, The Art of Architectural Drawing: Imagination and Technique
   Shahroudy Amir, 2018, IEEE Transactions on Pattern Analysis and Machine Intelligence, V40, P1045, DOI 10.1109/TPAMI.2017.2691321
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tao WY, 2020, 2020 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2020), P33, DOI 10.1109/CW49994.2020.00013
   Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165
   Van Laerhoven T, 2005, COMPUT ANIMAT VIRT W, V16, P429, DOI 10.1002/cav.95
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Zang Y, 2014, VISUAL COMPUT, V30, P969, DOI 10.1007/s00371-013-0881-6
   Zhang L., 2014, P 18 M ACM SIGGRAPH, P135, DOI DOI 10.1145/2556700.2556703
   Zhang L, 2010, VISUAL COMPUT, V26, P399, DOI 10.1007/s00371-010-0454-x
   Zhu JY, 2017, ADV NEUR IN, V30
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 46
TC 11
Z9 11
U1 1
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2022
VL 38
IS 4
SI SI
BP 1283
EP 1300
DI 10.1007/s00371-021-02219-x
EA JUL 2021
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0D9DX
UT WOS:000676074700002
OA hybrid
DA 2024-07-18
ER

PT J
AU Song, C
   Wu, J
   Zhu, L
   Zuo, X
AF Song, Can
   Wu, Jin
   Zhu, Lei
   Zuo, Xin
TI Weight correlation reduction and features normalization: improving the
   performance for shallow networks
SO VISUAL COMPUTER
LA English
DT Article
DE Convolution neural networks; Weight correlation Reduction; Features
   normalization; Image classification
AB Although convolutional neural networks (CNNs) show great abilities in image classification, improving their performance is still challenging for shallownetworks. The redundancy of the network increases when more convolution kernels are adopted in the network. To alleviate this defect, we propose two methods including Weight Correlation Reduction (WCR) and Features Normalization (FN) to boost the performance of shallow networks. The formal method is designed to eliminate weight redundancy, while the latter is used to increase the sparsity of learned deep features. On benchmarks CIFAR-10 and STL-10, the accuracy rate increased by 2.29% and 4.79% for shallow networks, respectively, which indicates the effectiveness of the proposed methods.
C1 [Song, Can; Wu, Jin; Zhu, Lei] Wuhan Univ Sci & Technol, Sch Informat Sci & Engn, Wuhan, Peoples R China.
   [Zuo, Xin] Jiangsu Univ Sci & Technol, Sch Comp Sci & Engn, Zhenjiang, Jiangsu, Peoples R China.
C3 Wuhan University of Science & Technology; Jiangsu University of Science
   & Technology
RP Zhu, L (corresponding author), Wuhan Univ Sci & Technol, Sch Informat Sci & Engn, Wuhan, Peoples R China.
EM zhulei@wust.edu.cn
RI Zhu, Lei/KUD-1330-2024; Song, Can/O-1541-2018
OI Zhu, Lei/0000-0001-7001-5775
FU National Natural Science Foundation of China [61502358, 61903164]
FX This study was funded by National Natural Science Foundation of China
   (No. 61502358, 61903164).
CR [Anonymous], 2015, All you need is a good init
   Attwell D, 2001, J CEREBR BLOOD F MET, V21, P1133, DOI 10.1097/00004647-200110000-00001
   Benesty J., 2009, NOISE REDUCTION SPEE, P1, DOI [10.1007/978-3-642-00296-0_5, DOI 10.1007/978-3-642-00296-05]
   Changqian Yu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12413, DOI 10.1109/CVPR42600.2020.01243
   Chen Hao, 2020, 2020 IEEE C COMP VIS, DOI [DOI 10.48550/ARXIV.2001.00309, DOI 10.1109/CVPR42600.2020.00860]
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen WJ, 2021, VISUAL COMPUT, V37, P805, DOI 10.1007/s00371-020-01831-7
   Chen YP, 2019, IEEE I CONF COMP VIS, P3434, DOI 10.1109/ICCV.2019.00353
   Cheng Y, 2015, IEEE I CONF COMP VIS, P2857, DOI 10.1109/ICCV.2015.327
   Coates A., 2011, P 14 INT C ART INT S, P215
   Denil M., 2013, P 26 INT C NEUR INF, P2148
   Ding SF, 2020, APPL INTELL, V50, P2280, DOI 10.1007/s10489-020-01655-x
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Glorot X., 2011, JMLR Proceedings, V15, P315, DOI DOI 10.1002/ECS2.1832
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Jia, 2019, IEEE ACCESS, V8, P4594
   Kabbai L, 2019, VISUAL COMPUT, V35, P679, DOI 10.1007/s00371-018-1503-0
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee H., 2007, ADV NEURAL INFORM PR, P801, DOI DOI 10.7551/MITPRESS/7503.003.0105
   Lennie P, 2003, CURR BIOL, V13, P493, DOI 10.1016/S0960-9822(03)00135-0
   Li X., 2020, P IEEE CVF C COMP VI, P8950, DOI 10.1109/CVPR42600.2020.00897
   Liu, 2017, ARXIV170510748 CORR
   Lu HY, 2015, PROC CVPR IEEE, P806, DOI 10.1109/CVPR.2015.7298681
   Mairal J., 2008, NIPS, V21, P1033
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shi SH, 2020, APPL INTELL, V50, P681, DOI 10.1007/s10489-019-01536-y
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Wang D, 2021, VISUAL COMPUT, V37, P1101, DOI 10.1007/s00371-020-01855-z
   Wen W, 2016, ADV NEUR IN, V29
   Wu BZ, 2017, LECT NOTES COMPUT SC, V10614, P49, DOI 10.1007/978-3-319-68612-7_6
   Xie D, 2017, PROC CVPR IEEE, P5075, DOI 10.1109/CVPR.2017.539
   Yuhui Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P489, DOI 10.1007/978-3-030-58610-2_29
   Zhang N, 2020, INFORM SCIENCES, V516, P142, DOI 10.1016/j.ins.2019.12.062
   Zhao CL, 2019, PROC CVPR IEEE, P2775, DOI 10.1109/CVPR.2019.00289
   Zhong X, 2019, IEEE IMAGE PROC, P395, DOI [10.1109/icip.2019.8803000, 10.1109/ICIP.2019.8803000]
   Zhou X., 2019, arXiv
NR 44
TC 2
Z9 2
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2022
VL 38
IS 7
BP 2489
EP 2498
DI 10.1007/s00371-021-02125-2
EA APR 2021
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2B0WL
UT WOS:000642386600001
DA 2024-07-18
ER

PT J
AU Tamiru, NK
   Tekeba, M
   Salau, AO
AF Tamiru, Nigus Kefyalew
   Tekeba, Menore
   Salau, Ayodeji Olalekan
TI Recognition of Amharic sign language with Amharic alphabet signs using
   ANN and SVM
SO VISUAL COMPUTER
LA English
DT Article
DE Amharic sign language; Fourier descriptor; Artificial neural network;
   Support vector machine
AB Sign language is a natural language mostly used by persons with hearing- and speech-based impairments to communicate with other people. In these modern times, sign language guides are used to eliminate the communication gap between people having hearing impairments and those with or without hearing impairments; however, they are very limited in number. To solve this challenge, automatic sign language recognition systems are developed to better reduce the communication gap for people with hearing disabilities. This paper presents the development of an automatic Amharic sign language translator which translates Amharic alphabet signs into their corresponding text using digital image processing and machine learning algorithms. The proposed system has four major developmental stages which include preprocessing, segmentation, feature extraction and classification. A total number of thirty-four features were extracted from shape, motion and color of hand gestures to represent both the base and derived class of Amharic sign characters. Classification models were built using artificial neural network (ANN) and multi-class support vector machine (SVM). The results show that the recognition system is capable of recognizing the Amharic alphabet signs with an average accuracy of 80.82% and 98.06% using the ANN and SVM classifiers, respectively.
C1 [Tamiru, Nigus Kefyalew] Debre Markos Univ, Dept Elect & Comp Engn, Debre Markos, Ethiopia.
   [Tekeba, Menore] Addis Ababa Univ, Dept Elect & Comp Engn, Addis Ababa, Ethiopia.
   [Salau, Ayodeji Olalekan] Afe Babalola Univ, Dept Elect Elect & Comp Engn, Ado Ekiti, Nigeria.
C3 Addis Ababa University
RP Salau, AO (corresponding author), Afe Babalola Univ, Dept Elect Elect & Comp Engn, Ado Ekiti, Nigeria.
EM kefyalewnigus@gmail.com; menore.tekeba@aait.edu.et;
   ayodejisalau98@gmail.com
RI salau, ayodeji Olalekan/C-1016-2018
OI salau, ayodeji Olalekan/0000-0002-6264-9783; Mengistu, Menore
   Tekeba/0000-0001-8434-7746
CR Admasu Y. F., 2010, Proceedings 10th International Conference on Intelligent Systems Design and Applications (ISDA 2010), P995, DOI 10.1109/ISDA.2010.5687057
   [Anonymous], 2011, IEEE WORKSH CONS DEP
   [Anonymous], 2012, AUTOMATIC RECOGNITIO
   Balasubramanian, 2015, INT J ADV RES COMPUT, V4, P28
   Belay B, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10031117
   Chen YQ, 2016, 2016 2ND IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATIONS (ICCC), P1947, DOI 10.1109/CompComm.2016.7925041
   Chimdi, 2015, J LANG CULT EDUC, V6, P9, DOI [10.5897/jlc2014.0298, DOI 10.5897/JLC2014.0298]
   Deorankar, 2014, INT J ENG COMPUT SCI, V3, P364
   Ekbote J, 2017, 2017 INT C INN INF
   Ethiopian National Association for Deaf. BERTAT, 1997, YEARLY MAGAZINE ETHI
   Gimbi T., 2014, RECOGNITION ISOLATED
   Hammouda G, 2020, VISUAL COMPUT, V36, P279, DOI 10.1007/s00371-018-1604-9
   Kumar B., 2016, INDIAN J SCI TECHNOL, V9, P1, DOI DOI 10.17485/ijst/2016/v9i33/99490
   QUAN Y, 2008, ISCSCT 2008 INT S CO
   Ricco S, 2010, LECT NOTES COMPUT SC, V5996, P214
   Rosalina L., 2018, INT CONF COMP APPL I, P1, DOI 10.1109/CAIPT.2017.8320692
   Rupe J., 2005, THESIS ROCHESTER I T
   Salau Ayodeji Olalekan, 2019, 2019 International Conference on Signal Processing and Communication (ICSC), P158
   Sharma Swati, 2019, INT J SCI TECHNOL RE, V8, P981
   Singha J, 2013, INT J ADV COMPUT SC, V4, P188
   Tesfaye M., 2010, 2 P IEEE INT C, P252
   Tsegaye A, 2011, THESIS ADDIS ABABA U
   Zerubabel L., 2008, THESIS ADDIS ABABA U
NR 23
TC 14
Z9 14
U1 3
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2022
VL 38
IS 5
BP 1703
EP 1718
DI 10.1007/s00371-021-02099-1
EA MAR 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0M9JP
UT WOS:000630988700001
DA 2024-07-18
ER

PT J
AU Wan, Y
   Meng, XJ
   Wang, YF
   Qiang, HP
AF Wan, Yuan
   Meng, Xiaojing
   Wang, Yufei
   Qiang, Haopeng
TI Dynamic time warping similarity measurement based on low-rank sparse
   representation
SO VISUAL COMPUTER
LA English
DT Article
DE Phase space reconstruction; Low-rank sparse representation; Morphology
   maintenance; Manifold learning; Time series similarity
ID DIMENSIONALITY REDUCTION; COMPONENT ANALYSIS
AB Similarity measurement of time series is one of the focus issues in time series analysis and mining. Morphology maintenance of time series is a better way for performing a similarity measurement, and phase space reconstruction has the advantage of analyzing the morphology of time series, whereas it is prone to generate high-dimensional data. Linear dimensionality reduction methods have difficulty preserving complete information of time series data. Manifold-based learning methods can better preserve the local characteristics of data. Low-rank representation (LRR) finds the lowest rank representation of all data and is capable of capturing the global structure of data. Therefore, in this paper, we propose a dynamic time warping similarity measurement method based on low-rank sparse representation (LRSE_DTW) to reduce the dimensionality of time series data. We learn the low-rank sparse representation of the phase space and then embed it into low-dimensional space to maintain the morphology of the phase space. DTW is used to measure the distance between discriminant information obtained from the l(2,1)-norm constraint on the projection matrix. To confirm the effectiveness of LRSE_DTW, time series classification experiments are carried out on public UCR time series classification archive. The results show that LRSE_DTW is superior to several other state-of-the-art time series similarity measurement methods.
C1 [Wan, Yuan; Meng, Xiaojing; Wang, Yufei; Qiang, Haopeng] Wuhan Univ Technol, Dept Math, Wuhan 430070, Peoples R China.
C3 Wuhan University of Technology
RP Wan, Y (corresponding author), Wuhan Univ Technol, Dept Math, Wuhan 430070, Peoples R China.
EM wanyuan@whut.edu.cn
RI Wan, Yuan/D-5204-2015
OI Meng, Xiaojing/0000-0003-0284-2343
FU Excellent Dissertation Cultivation Funds of Wuhan University of
   Technology [2018-YS-076]; Fundamental Research Funds for the Central
   Universities [2019IB010]
FX We would like to extend our sincere appreciation to the Excellent
   Dissertation Cultivation Funds of Wuhan University of Technology
   (2018-YS-076) and the Fundamental Research Funds for the Central
   Universities (No. 2019IB010) for their support. We also gratefully
   acknowledge Yan ping Chen, Eamonn Keogh, Bing Hu, and others for their
   devoted work on the collection of the UCR datasets that support our
   work.
CR Afrasiabi M, 2020, VISUAL COMPUT, V36, P1127, DOI 10.1007/s00371-019-01722-6
   [Anonymous], 1981, DYN SYST TURBUL WARW
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Berndt DJ, 1994, P 3 INT C KNOWL DISC, P359, DOI DOI 10.5555/3000850.3000887
   Chen JH, 2014, IEEE T CYBERNETICS, V44, P1432, DOI 10.1109/TCYB.2013.2286106
   Dadkhahi H, 2017, IEEE T IMAGE PROCESS, V26, P5435, DOI 10.1109/TIP.2017.2735189
   Du S., 2016, 2016 CHIN CONTR DEC
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Gong Y., 2004, INT ACM SIGIR C RES
   Guo ZQ, 2013, SOFT COMPUT, V17, P805, DOI 10.1007/s00500-012-0953-y
   Han N, 2018, NEURAL NETWORKS, V108, P202, DOI 10.1016/j.neunet.2018.08.003
   He XF, 2005, IEEE I CONF COMP VIS, P1208
   He XF, 2004, ADV NEUR IN, V16, P153
   Hou CP, 2014, IEEE T CYBERNETICS, V44, P793, DOI 10.1109/TCYB.2013.2272642
   Huang Y, 2017, EUR J OPER RES, V258, P692, DOI 10.1016/j.ejor.2016.08.058
   Hyvärinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5
   Izakian H, 2015, ENG APPL ARTIF INTEL, V39, P235, DOI 10.1016/j.engappai.2014.12.015
   Jiang QC, 2014, ISA T, V53, P1516, DOI 10.1016/j.isatra.2014.05.031
   Li Hai-Iin, 2013, Application Research of Computers, V30, P1285, DOI 10.3969/j.issn.1001-3695.2013.05.002
   Liu G., 2011, INT C COMP VIS
   Liu QC, 2018, IEEE ACCESS, V6, P9690, DOI 10.1109/ACCESS.2017.2788639
   Lu K, 2012, IEEE T INTELL TRANSP, V13, P1515, DOI 10.1109/TITS.2012.2220965
   Lu XQ, 2013, IEEE T GEOSCI REMOTE, V51, P4009, DOI 10.1109/TGRS.2012.2226730
   Nie F., 2010, NIPS
   O'Reilly C, 2017, KNOWL-BASED SYST, V133, P1, DOI 10.1016/j.knosys.2017.05.026
   Qiao LS, 2010, PATTERN RECOGN, V43, P331, DOI 10.1016/j.patcog.2009.05.005
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Senin P, 2013, IEEE DATA MINING, P1175, DOI 10.1109/ICDM.2013.52
   Shnitzer T, 2017, IEEE T SIGNAL PROCES, V65, P904, DOI 10.1109/TSP.2016.2616334
   Wan Y, 2018, EXPERT SYST APPL, V93, P134, DOI 10.1016/j.eswa.2017.10.008
   Wan Y, 2017, J COMPUT APPL MATH, V319, P514, DOI 10.1016/j.cam.2017.01.004
   Wang J, 2015, NEUROCOMPUTING, V156, P68, DOI 10.1016/j.neucom.2014.12.084
   Wang X., 2015, MICROCOMPUT APPL, V34, P48
   Wang XS, 2016, NEUROCOMPUTING, V188, P275, DOI 10.1016/j.neucom.2014.12.127
   Wu JX, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON AUTOMATION AND LOGISTICS, VOLS 1-6, P95, DOI 10.1109/ICAL.2007.4338537
   Xu M, 2019, IEEE T CIRC SYST VID, V29, P3516, DOI [10.1109/TCSVT.2018.2886277, 10.1080/17445302.2018.1558727]
   Yin M, 2013, IEEE IMAGE PROC, P3770, DOI 10.1109/ICIP.2013.6738777
   Zhang Z, 2015, INFORM SCIENCES, V315, P88, DOI 10.1016/j.ins.2015.04.007
   Zhou W., 2012, IEEE 3 INT C PRIV
   Zhu YH, 2017, MULTIMED TOOLS APPL, V76, P17525, DOI 10.1007/s11042-016-4121-8
NR 40
TC 3
Z9 3
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2022
VL 38
IS 5
BP 1731
EP 1740
DI 10.1007/s00371-021-02101-w
EA MAR 2021
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0M9JP
UT WOS:000630256700001
DA 2024-07-18
ER

PT J
AU Liu, YT
   Dou, Y
   Jin, RC
   Li, RC
   Qiao, P
AF Liu, Yuntao
   Dou, Yong
   Jin, Ruochun
   Li, Rongchun
   Qiao, Peng
TI Hierarchical learning with backtracking algorithm based on the Visual
   Confusion Label Tree for large-scale image classification
SO VISUAL COMPUTER
LA English
DT Article
DE Image classification; Convolution neural network; Confusion graph; Label
   tree; Bayesian Neural Network
AB In this paper, a hierarchical learning algorithm based on the Bayesian Neural Network classifier with backtracking is proposed to support large-scale image classification, where a Visual Confusion Label Tree is established for constructing a hierarchical structure for large numbers of categories in image datasets and determining the hierarchical learning tasks automatically. Specifically, the Visual Confusion Label Tree is established based on outputs of convolution neural network models. One parent node on the Visual Confusion Label Tree contains a set of sibling coarse-grained categories, and child nodes have several sets of fine-grained categories which are partitions of categories on the parent node. The proposed Hierarchical Bayesian Neural Network with backtracking algorithm can benefit from the hierarchical structure of the Visual Confusion Label Tree. Focusing on those confusion subsets instead of the entire set of categories makes the classification ability of the tree classifier stronger. The backtracking algorithm can utilize the uncertainty information captured from the Bayesian Neural Network to make a second classification to re-correct samples that were classified incorrectly in the previous classification process. Experiments on four large-scale datasets show that our tree classifier obtains a significant improvement over the state-of-the-art tree classifier, which have demonstrated the discriminative hierarchical structure of our Visual Confusion Label Tree and the effectiveness of our Hierarchical Bayesian Neural Network with backtracking algorithm.
C1 [Liu, Yuntao; Dou, Yong; Jin, Ruochun; Li, Rongchun; Qiao, Peng] Natl Univ Def Technol, Natl Lab Parallel & Distributed Proc, Changsha 410073, Hunan, Peoples R China.
C3 National University of Defense Technology - China
RP Liu, YT (corresponding author), Natl Univ Def Technol, Natl Lab Parallel & Distributed Proc, Changsha 410073, Hunan, Peoples R China.
EM liuyuntao.me@gmail.com; yongdou@nudt.edu.cn; jinruochun@nudt.edu.cn;
   rongchunli@nudt.edu.cn; pengqiao@nudt.edu.cn
RI 刘, 云涛/GQQ-5223-2022
FU Ministry of Science and Technology of the People's Republic of China
   [2018YFB1003400]; National Natural Science Foundation of China
   [61902415]
FX This study was funded by the Ministry of Science and Technology of the
   People's Republic of China (Grant Number 2018YFB1003400), National
   Natural Science Foundation of China (Grant Number 61802419) and National
   Natural Science Foundation of China (Grant Number 61902415).
CR Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   [Anonymous], 2011, P 28 INT C MACHINE L
   [Anonymous], ARXIV PREPRINT ARXIV
   [Anonymous], 2010, Advances in Neural Information Processing Systems
   Blondel VD, 2008, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2008/10/P10008
   Chen Lin, 2017, [Computational Visual Media, 计算可视媒体], V3, P83
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng Jia, 2011, NIPS, P567
   Fan JP, 2017, IEEE T IMAGE PROCESS, V26, P1923, DOI 10.1109/TIP.2017.2667405
   Fan JP, 2015, IEEE T IMAGE PROCESS, V24, P4172, DOI 10.1109/TIP.2015.2457337
   Fan JP, 2012, IEEE T MULTIMEDIA, V14, P1414, DOI 10.1109/TMM.2012.2197604
   Gilks, 1996, MARKOV CHAIN MONTE C, V1, P19, DOI DOI 10.1201/B14835
   Guillaumin M, 2012, PROC CVPR IEEE, P3202, DOI 10.1109/CVPR.2012.6248055
   Jia D, 2012, PROC CVPR IEEE, P3450, DOI 10.1109/CVPR.2012.6248086
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jin RC, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1980
   KELLER JM, 1985, IEEE T SYST MAN CYB, V15, P580, DOI 10.1109/TSMC.1985.6313426
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar MP, 2007, IEEE I CONF COMP VIS, P1665
   LeCun Y., 1995, P INT C ART NEUR NET, V60, P53
   Lei H, 2016, NEUROCOMPUTING, V208, P46, DOI 10.1016/j.neucom.2016.01.100
   Li LJ, 2010, PROC CVPR IEEE, P3336, DOI 10.1109/CVPR.2010.5540027
   Liu BY, 2013, PROC CVPR IEEE, P843, DOI 10.1109/CVPR.2013.114
   Liu Y., 2018, P IEEE INT C MULT EX, P1
   Marszalek M, 2008, LECT NOTES COMPUT SC, V5305, P479, DOI 10.1007/978-3-540-88693-8_35
   Neal R. M, 2012, Bayesian Learning for Neural Networks
   Oh S, 2017, AAAI CONF ARTIF INTE, P2450
   Pietro P., 2007, CALIFORNIA I TECHNOL
   Ristin M, 2015, PROC CVPR IEEE, P231, DOI 10.1109/CVPR.2015.7298619
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun M, 2013, IEEE I CONF COMP VIS, P265, DOI 10.1109/ICCV.2013.40
   Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Verma N, 2012, PROC CVPR IEEE, P2280, DOI 10.1109/CVPR.2012.6247938
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9
   Xie SN, 2015, PROC CVPR IEEE, P2645, DOI 10.1109/CVPR.2015.7298880
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang WY, 2012, PROC CVPR IEEE, P2144, DOI 10.1109/CVPR.2012.6247921
   Zhao B., 2011, P ADV NEUR INF PROC, P1251
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zheng Y, 2020, IEEE T IMAGE PROCESS, V29, P883, DOI 10.1109/TIP.2019.2938321
   Zheng Y, 2017, PATTERN RECOGN, V67, P97, DOI 10.1016/j.patcog.2017.01.029
NR 45
TC 9
Z9 9
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2022
VL 38
IS 3
BP 897
EP 917
DI 10.1007/s00371-021-02058-w
EA FEB 2021
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZQ8YX
UT WOS:000614677500002
DA 2024-07-18
ER

PT J
AU Kaur, G
   Agarwal, R
   Patidar, V
AF Kaur, Gurpreet
   Agarwal, Rekha
   Patidar, Vinod
TI Color image encryption scheme based on fractional Hartley transform and
   chaotic substitution-permutation
SO VISUAL COMPUTER
LA English
DT Article
DE Color image encryption; Fractional Hartley transform; Chaos;
   Substitution; Permutation
ID FOURIER-TRANSFORM; FRESNEL TRANSFORM; STANDARD MAP; ALGORITHM; SYSTEM;
   DOMAIN; COSINE; MIXTURE; SINE
AB We propose a novel opto-digital method of color image encryption which utilizes compound chaotic mappings, the reality preserving fractional Hartley transformation and piecewise linear chaotic map for substitution, optical processing and permutation of image pixels, respectively. The image to be encrypted initially undergoes a chaos-based substitution in the spatial domain through the compound chaotic maps followed by a transformation to the combined time-frequency domain using the fractional Hartley transform. A reality preserving version of the fractional Hartley transform is used to eliminate the complexity associated with transform coefficients. Optical transformation of the image, in the fractional Hartley domain, is followed by a permutation through piecewise linear chaotic maps. Due to the intertwined application of optical transformation and chaos-based substitution and permutation processes, the proposed image encryption scheme possesses higher security. The input parameters (initial conditions, control parameters, and number of iterations) of chaotic maps along with fractional orders of the fractional Hartley transform collectively form the secret keys for encryption/decryption. The proposed scheme is a lossless and symmetric encryption scheme. The level of security provided in terms of high sensitivity to keys, resistivity to brute-force attack, classical attacks, differential attacks, entropy attack, noise and occlusion attack along with the elimination of complex coefficients proves its better efficacy as compared to other similar state-of-the-art schemes.
C1 [Kaur, Gurpreet] Guru Gobind Singh Indraprastha Univ, USICT, New Delhi 110078, India.
   [Agarwal, Rekha] Amity Sch Engn & Technol, Dept ECE, New Delhi 110061, India.
   [Patidar, Vinod] Sir Padampat Singhania Univ, Dept Phys, Udaipur 313601, Rajasthan, India.
C3 GGS Indraprastha University; Sir Padampat Singhania University
RP Kaur, G (corresponding author), Guru Gobind Singh Indraprastha Univ, USICT, New Delhi 110078, India.
EM gurpreet.preeti.82@gmail.com
RI Patidar, Vinod/G-5906-2010; Agarwal, Rekha/ABA-2186-2021; Kaur,
   Gurpreet/AAP-3401-2020
OI Patidar, Vinod/0000-0002-1270-3454; Kaur, Gurpreet/0000-0002-2611-5143
CR Abuturab MR, 2012, OPT LASER ENG, V50, P1383, DOI 10.1016/j.optlaseng.2012.04.011
   Al-shameri WFH., 2013, INT J MATH ANAL, V7, P1433, DOI DOI 10.12988/IJMA.2013.3361
   Alawida M, 2019, SIGNAL PROCESS, V164, P249, DOI 10.1016/j.sigpro.2019.06.013
   Alligood K.T., 1997, Physics Today, V50, P67
   ALMEIDA LB, 1994, IEEE T SIGNAL PROCES, V42, P3084, DOI 10.1109/78.330368
   Alvarez G, 2006, INT J BIFURCAT CHAOS, V16, P2129, DOI 10.1142/S0218127406015970
   Alzaidi AA, 2018, IEEE ACCESS, V6, P55405, DOI 10.1109/ACCESS.2018.2871557
   Arroyo D, 2008, CHAOS, V18, DOI 10.1063/1.2959102
   Azoug SE, 2016, OPT COMMUN, V359, P85, DOI 10.1016/j.optcom.2015.09.054
   Behnia S, 2008, CHAOS SOLITON FRACT, V35, P408, DOI 10.1016/j.chaos.2006.05.011
   Ben Farah MA, 2020, OPT LASER TECHNOL, V121, DOI 10.1016/j.optlastec.2019.105777
   BRACEWELL RN, 1994, P IEEE, V82, P381, DOI 10.1109/5.272142
   Chai XL, 2017, SIGNAL PROCESS, V134, P35, DOI 10.1016/j.sigpro.2016.11.016
   Chang XY, 2020, OPT LASER ENG, V126, DOI 10.1016/j.optlaseng.2019.105901
   Chen GR, 2004, CHAOS SOLITON FRACT, V21, P749, DOI 10.1016/j.chaos.2003.12.022
   Chen JX, 2017, IEEE ACCESS, V5, P16124, DOI 10.1109/ACCESS.2017.2735420
   Chiaraluce F, 2002, IEEE T CONSUM ELECTR, V48, P838, DOI 10.1109/TCE.2003.1196410
   Condon EU, 1937, REV MOD PHYS, V9, P0432, DOI 10.1103/RevModPhys.9.432
   Enayatifar R, 2015, OPT LASER ENG, V71, P33, DOI 10.1016/j.optlaseng.2015.03.007
   Faragallah OS, 2019, IEEE ACCESS, V7, P4184, DOI 10.1109/ACCESS.2018.2879857
   Ghadirli HM, 2019, SIGNAL PROCESS, V164, P163, DOI 10.1016/j.sigpro.2019.06.010
   Hennelly B, 2003, OPT LETT, V28, P269, DOI 10.1364/OL.28.000269
   Huang CK, 2009, OPT COMMUN, V282, P2123, DOI 10.1016/j.optcom.2009.02.044
   Hwang HE, 2012, OPT COMMUN, V285, P567, DOI 10.1016/j.optcom.2011.11.007
   Kang XJ, 2019, IEEE T CIRC SYST VID, V29, P1595, DOI 10.1109/TCSVT.2018.2851983
   Kang XJ, 2016, IEEE T SIGNAL PROCES, V64, P3402, DOI 10.1109/TSP.2016.2544740
   Kaur G, 2020, ENG SCI TECHNOL, V23, P998, DOI 10.1016/j.jestch.2020.02.007
   Kaur G, 2019, 2019 6TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND INTEGRATED NETWORKS (SPIN), P399, DOI 10.1109/SPIN.2019.8711777
   Kumar P, 2016, SPRINGER SER OPT SCI, V198, P367, DOI 10.1007/978-1-4939-3028-9_13
   Lan RS, 2018, SIGNAL PROCESS, V147, P133, DOI 10.1016/j.sigpro.2018.01.026
   Lang J, 2012, OPT LASER ENG, V50, P929, DOI 10.1016/j.optlaseng.2012.02.012
   Li CQ, 2009, IMAGE VISION COMPUT, V27, P1371, DOI 10.1016/j.imavis.2008.12.008
   Li GD, 2019, VISUAL COMPUT, V35, P1267, DOI 10.1007/s00371-018-1574-y
   Li HJ, 2011, OPT LASER ENG, V49, P753, DOI 10.1016/j.optlaseng.2011.03.017
   Li SJ, 2005, INT J BIFURCAT CHAOS, V15, P3119, DOI 10.1142/S0218127405014052
   Lian S., 2008, Multimedia Content Encryption: Techniques And Applications
   Liu HJ, 2015, SIGNAL PROCESS, V113, P104, DOI 10.1016/j.sigpro.2015.01.016
   Liu HJ, 2014, AEU-INT J ELECTRON C, V68, P676, DOI 10.1016/j.aeue.2014.02.002
   Liu ZJ, 2013, OPT LASER ENG, V51, P967, DOI 10.1016/j.optlaseng.2013.02.015
   Liu ZJ, 2011, OPT LASER ENG, V49, P542, DOI 10.1016/j.optlaseng.2010.12.005
   Luo X, 2018, CHIN C IM GRAPH TECH, DOI [10.1007/978-981-13-1702-6_25, DOI 10.1007/978-981-13-1702-6_25]
   MAY RM, 1976, NATURE, V261, P459, DOI 10.1038/261459a0
   MCBRIDE AC, 1987, IMA J APPL MATH, V39, P159, DOI 10.1093/imamat/39.2.159
   MENDLOVIC D, 1993, J OPT SOC AM A, V10, P1875, DOI 10.1364/JOSAA.10.001875
   Mishra DC, 2017, J INF SECUR APPL, V37, P65, DOI 10.1016/j.jisa.2017.09.006
   Murillo-Escobar MA, 2015, SIGNAL PROCESS, V109, P119, DOI 10.1016/j.sigpro.2014.10.033
   NAMIAS V, 1980, J I MATH APPL, V25, P241
   OZAKTAS HM, 1993, J OPT SOC AM A, V10, P2522, DOI 10.1364/JOSAA.10.002522
   Ozaktas HM, 1996, IEEE T SIGNAL PROCES, V44, P2141, DOI 10.1109/78.536672
   Patidar V, 2011, OPT COMMUN, V284, P4331, DOI 10.1016/j.optcom.2011.05.028
   Pei SC, 2002, IEEE T SIGNAL PROCES, V50, P1661, DOI 10.1109/TSP.2002.1011207
   Pei SC, 1998, IEEE T CIRCUITS-II, V45, P665, DOI 10.1109/82.686685
   Ran QW, 2015, OPT COMMUN, V348, P43, DOI 10.1016/j.optcom.2015.03.016
   Ran QW, 2009, OPT LETT, V34, P1729, DOI 10.1364/OL.34.001729
   REFREGIER P, 1995, OPT LETT, V20, P767, DOI 10.1364/OL.20.000767
   Sen Teh J, 2020, J INF SECUR APPL, V50, DOI 10.1016/j.jisa.2019.102421
   Shan MG, 2012, OPT COMMUN, V285, P4227, DOI 10.1016/j.optcom.2012.06.023
   SHANNON CE, 1949, BELL SYST TECH J, V28, P656, DOI 10.1002/j.1538-7305.1949.tb00928.x
   Sharma J.B, 2016, IEEE INT C REC TREND, DOI [10.1109/RTEICT.2016.7808152, DOI 10.1109/RTEICT.2016.7808152]
   Singh N, 2010, OPTIK, V121, P918, DOI 10.1016/j.ijleo.2008.09.049
   Singh N, 2009, OPT LASER ENG, V47, P539, DOI 10.1016/j.optlaseng.2008.10.013
   Singh Phool, 2019, Engineering Vibration, Communication and Information Processing. ICoEVCI 2018, India. Lecture Notes in Electrical Engineering (LNEE 478), P317, DOI 10.1007/978-981-13-1642-5_29
   Singh P, 2017, OPT LASER ENG, V91, P187, DOI 10.1016/j.optlaseng.2016.11.022
   Souyah A, 2016, NONLINEAR DYNAM, V86, P639, DOI 10.1007/s11071-016-2912-0
   Sui LS, 2015, OPT LASER ENG, V75, P17, DOI 10.1016/j.optlaseng.2015.06.005
   Sui LS, 2014, OPT LASER ENG, V62, P139, DOI 10.1016/j.optlaseng.2014.06.003
   Sui LS, 2013, OPT LASER TECHNOL, V48, P117, DOI 10.1016/j.optlastec.2012.10.016
   Talhaoui MZ, 2021, VISUAL COMPUT, V37, P1757, DOI 10.1007/s00371-020-01936-z
   Tong XJ, 2015, J VIS COMMUN IMAGE R, V33, P219, DOI 10.1016/j.jvcir.2015.09.014
   Unnikrishnan G, 2000, OPT ENG, V39, P2853, DOI 10.1117/1.1313498
   Vashisth S, 2014, OPTIK, V125, P5309, DOI 10.1016/j.ijleo.2014.06.068
   Venturini I, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P205
   Wang Q, 2014, OPT COMMUN, V313, P1, DOI 10.1016/j.optcom.2013.09.058
   Wang XY, 2018, MULTIMED TOOLS APPL, V77, P6243, DOI 10.1007/s11042-017-4534-z
   Wang Y, 2015, OPT COMMUN, V344, P147, DOI 10.1016/j.optcom.2015.01.045
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weber A. G., 1997, USC SIPI REPORT, V315
   Wu JH, 2014, OPTIK, V125, P4474, DOI 10.1016/j.ijleo.2014.02.026
   Wu XJ, 2015, APPL SOFT COMPUT, V37, P24, DOI 10.1016/j.asoc.2015.08.008
   Wu Y, 2011, Cyber J Multidiscip J Sci Technol J Sel Areas Telecommun (JSAT), V1, P31
   Wu Y, 2013, INFORM SCIENCES, V222, P323, DOI 10.1016/j.ins.2012.07.049
   Zhang YQ, 2016, OPT LASER ENG, V82, P95, DOI 10.1016/j.optlaseng.2016.02.002
   Zhang YS, 2013, OPT LASER ENG, V51, P472, DOI 10.1016/j.optlaseng.2012.11.001
   Zhao DM, 2008, OPT COMMUN, V281, P5326, DOI 10.1016/j.optcom.2008.07.049
   Zhou LH, 2000, IEEE T CIRCUITS-II, V47, P1107, DOI 10.1109/82.877154
   Zhou NR, 2015, OPT COMMUN, V343, P10, DOI 10.1016/j.optcom.2014.12.084
   Zhou NR, 2011, OPT COMMUN, V284, P3234, DOI 10.1016/j.optcom.2011.02.065
   Zhou YC, 2015, IEEE T CYBERNETICS, V45, P2001, DOI 10.1109/TCYB.2014.2363168
   Zhou YC, 2014, SIGNAL PROCESS, V97, P172, DOI 10.1016/j.sigpro.2013.10.034
NR 89
TC 30
Z9 31
U1 2
U2 41
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2022
VL 38
IS 3
BP 1027
EP 1050
DI 10.1007/s00371-021-02066-w
EA JAN 2021
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZQ8YX
UT WOS:000609982000001
DA 2024-07-18
ER

PT J
AU Khan, MJ
   Khan, MJ
   Siddiqui, AM
   Khurshid, K
AF Khan, Muhammad Junaid
   Khan, Muhammad Jaleed
   Siddiqui, Adil Masood
   Khurshid, Khurram
TI An automated and efficient convolutional architecture for
   disguise-invariant face recognition using noise-based data augmentation
   and deep transfer learning
SO VISUAL COMPUTER
LA English
DT Article
DE Disguise-invariant face recognition; Noise-based data augmentation; Deep
   learning; Pre-trained convolutional neural network
ID ILLUMINATION; SYSTEM; AGE
AB Face recognition is diversely used in modern biometric and security applications. Most of the current face recognition techniques show good results in a constrained environment. However, these techniques face many problems in real-world scenarios such as low-quality images, temporal variations and facial disguises creating variations in facial features. The reason for these deteriorating results is the employment of handcrafted features having weak generalization capabilities and neglecting the complexities associated with domain adaption in case of deep learning models. In this paper, we have studied the efficacy of deep learning methods incorporating simple noise-based data augmentation for disguise invariant face recognition (DIFR). The proposed method detects face in an image using Viola Jones face detector and classifies it using a pre-trained Convolutional Neural Network (CNN) fine-tuned for DIFR. During transfer learning, a pre-trained CNN learns generalized disguise-invariant features from facial images of several subjects to correctly identify them under varying facial disguises. We have compared four different pre-trained 2D CNNs, each with different number of learning parameters, based on their classification accuracy and execution time for selecting a suitable model for DIFR. Comprehensive experiments and comparative analysis have been conducted on six challenging facial disguise datasets. Resnet-18 gives the best trade-off between accuracy and efficiency, by achieving an average accuracy of 98.19% with an average execution time of 0.32 seconds. The promising results achieved in these experiments reflect the efficiency of the proposed method and outperforms the existing methods in all aspects.
C1 [Khan, Muhammad Junaid; Siddiqui, Adil Masood] Natl Univ Sci & Technol NUST, Mil Coll Signals, Dept Elect Engn, Islamabad 44000, Pakistan.
   [Khan, Muhammad Jaleed] Natl Univ Ireland Galway, Data Sci Inst, Galway H91 TK33, Ireland.
   [Khurshid, Khurram] Inst Space Technol, Dept Elect Engn, Artificial Intelligence & Comp Vis iVis Lab, Islamabad 44000, Pakistan.
C3 National University of Sciences & Technology - Pakistan
RP Khan, MJ (corresponding author), Natl Univ Sci & Technol NUST, Mil Coll Signals, Dept Elect Engn, Islamabad 44000, Pakistan.
EM mjk0853@gmail.com; m.khan12@nuigalway.ie; dradil@mcs.edu.pk;
   khurram.khurshid@ist.edu.pk
RI Khan, Jaleed/P-5082-2019
OI Khan, Jaleed/0000-0003-4727-4722; Khan, M Junaid/0000-0002-5912-0504
CR Abdurrahim SH, 2018, VISUAL COMPUT, V34, P1617, DOI 10.1007/s00371-017-1428-z
   Ahmad HM, 2020, CURR MED IMAGING, V16, P946, DOI 10.2174/1573405615666191219100824
   Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], 2020, FACE ID ADV TECHNOLO
   [Anonymous], 2020, USE FACE ID YOUR IPH
   Arsenovic M, 2017, I S INTELL SYST INFO, P53, DOI 10.1109/SISY.2017.8080587
   Bansal A, 2018, IEEE COMPUT SOC CONF, P10, DOI 10.1109/CVPRW.2018.00009
   Ben Fredj H, 2021, VISUAL COMPUT, V37, P217, DOI 10.1007/s00371-020-01794-9
   CAO ZM, 2010, PROC CVPR IEEE, P2707, DOI DOI 10.1109/CVPR.2010.5539992
   Chen C, 2017, CONFERENCE PROCEEDINGS OF 2017 3RD IEEE INTERNATIONAL CONFERENCE ON CONTROL SCIENCE AND SYSTEMS ENGINEERING (ICCSSE), P120, DOI 10.1109/CCSSE.2017.8087907
   Chen CJ, 2013, INT CONF BIOMETR
   Chen CJ, 2016, INFORM FUSION, V32, P80, DOI 10.1016/j.inffus.2015.09.005
   Cheng Y, 2017, VISUAL COMPUT, V33, P1483, DOI 10.1007/s00371-017-1357-x
   Dantcheva A., 2012, 2012 IEEE 5 INT C BI, P391, DOI DOI 10.1109/BTAS.2012.6374605
   Deng J., 2019, ARXIV, DOI 10.48550/arXiv.1905.00641
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng JK, 2019, IEEE INT CONF COMP V, P485, DOI 10.1109/ICCVW.2019.00061
   Deng WH, 2019, IEEE T PATTERN ANAL, V41, P758, DOI 10.1109/TPAMI.2018.2800008
   Dhamecha TI, 2013, INT CONF BIOMETR
   Dhamecha TI, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0099212
   Gao Y, 2019, NEURAL COMPUT APPL, V31, P607, DOI 10.1007/s00521-017-3035-3
   Guo GD, 2014, IEEE T CIRC SYST VID, V24, P814, DOI 10.1109/TCSVT.2013.2280076
   Gupta S, 2021, VISUAL COMPUT, V37, P447, DOI 10.1007/s00371-020-01814-8
   Huang G. B., 2008, WORKSH FAC REAL LIF
   Hung KM, 2018, PROCEEDINGS OF 2018 INTERNATIONAL CONFERENCE ON DIGITAL MEDICINE AND IMAGE PROCESSING (DMIP 2018), P65, DOI 10.1145/3299852.3299858
   Khan Muhammad Jaleed, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1097, DOI 10.1109/ICDAR.2019.00178
   Khan M. J., 2017, J SPACE TECHNOL, V7, P44, DOI DOI 10.3390/S22031147
   Khan MJ, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.5.053001
   Khan MJ, 2018, 2018 13TH IAPR INTERNATIONAL WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS (DAS), P393, DOI 10.1109/DAS.2018.26
   Khan MJ, 2018, IEEE ACCESS, V6, P14118, DOI 10.1109/ACCESS.2018.2812999
   Kim J, 2005, LECT NOTES ARTIF INT, V3533, P65
   Kohli N, 2018, IEEE COMPUT SOC CONF, P17, DOI 10.1109/CVPRW.2018.00010
   Kose Neslihan, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163104
   Kotwal Ketan, 2020, IEEE Transactions on Biometrics, Behavior, and Identity Science, V2, P15, DOI 10.1109/TBIOM.2019.2946175
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar A, 2016, ISBA 2016, DOI [10.1109/ISBA.2016.7477243, DOI 10.1109/ISBA.2016.7477243]
   Kushwaha V, 2018, IEEE COMPUT SOC CONF, P1, DOI 10.1109/CVPRW.2018.00008
   Lei Z, 2014, IEEE T PATTERN ANAL, V36, P289, DOI 10.1109/TPAMI.2013.112
   Li Y, 2017, ARXIV170903654
   Liu CJ, 2002, IEEE T IMAGE PROCESS, V11, P467, DOI 10.1109/TIP.2002.999679
   Martínez AM, 2002, IEEE T PATTERN ANAL, V24, P748, DOI 10.1109/TPAMI.2002.1008382
   Min R., 2011, IEEE INT C AUT FAC G, P442, DOI DOI 10.1109/FG.2011.5771439
   Peri SV, 2018, IEEE COMPUT SOC CONF, P25, DOI 10.1109/CVPRW.2018.00011
   Ramanathan N, 2004, IEEE IMAGE PROC, P1999
   Ranjan R., 2016, ARXIV161100851
   Rasti S, 2018, IET BIOMETRICS, V7, P530, DOI 10.1049/iet-bmt.2018.5059
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sajid M, 2018, MATH PROBL ENG, V2018, DOI 10.1155/2018/2850632
   Shu XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P35, DOI 10.1145/2733373.2806216
   Singh Maneet, 2019, IEEE Transactions on Biometrics, Behavior, and Identity Science, V1, P97, DOI 10.1109/TBIOM.2019.2903860
   Singh R, 2009, IMAGE VISION COMPUT, V27, P245, DOI 10.1016/j.imavis.2007.06.010
   Smirnov E, 2018, IEEE COMPUT SOC CONF, P37, DOI 10.1109/CVPRW.2018.00013
   Sun Y, 2017, PATTERN RECOGN, V66, P153, DOI 10.1016/j.patcog.2017.01.011
   Suri S, 2018, INT CONF BIOMETR THE
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tang JH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2998574
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang ZF, 2014, VISUAL COMPUT, V30, P359, DOI 10.1007/s00371-013-0861-x
   Wu FY, 2019, IEEE INT CONF MULTI, P537, DOI 10.1109/ICMEW.2019.00098
   Yang M, 2010, LECT NOTES COMPUT SC, V6316, P448, DOI 10.1007/978-3-642-15567-3_33
   Yousaf A, 2020, EXPERT SYST, V37, DOI 10.1111/exsy.12503
   Yousaf A, 2019, 2019 2ND INTERNATIONAL CONFERENCE ON COMPUTING, MATHEMATICS AND ENGINEERING TECHNOLOGIES (ICOMET), DOI [10.1109/icomet.2019.8673459, 10.1111/petr.13610]
   Zhang KP, 2018, IEEE COMPUT SOC CONF, P32, DOI 10.1109/CVPRW.2018.00012
   Zhao J, 2018, PROC CVPR IEEE, P2207, DOI 10.1109/CVPR.2018.00235
NR 65
TC 22
Z9 22
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2022
VL 38
IS 2
BP 509
EP 523
DI 10.1007/s00371-020-02031-z
EA JAN 2021
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZE7IA
UT WOS:000605853500002
DA 2024-07-18
ER

PT J
AU Kim, Y
   Lee, SH
AF Kim, Yeonjoon
   Lee, Sung-Hee
TI Keyframe-based multi-contact motion synthesis
SO VISUAL COMPUTER
LA English
DT Article
DE Character animation; Procedural animation; Motion planning;
   Multi-contact
AB Most of the human daily activities include acyclic multi-contact motions. Yet, generating such motions is challenging because of its high-dimensional and nonlinear solution space made by combinations of individual movements of body parts. In this paper, we present a novel keyframe-based framework to automatically generate multi-contact character motions. Our system consists of two components: key-pose planning and interpolation. Given initial and goal poses in which each contact can be repositioned at most one time during the transition, our key-pose planning step generates intermediate key-poses that represent contact changes, taking into account a set of principles for goal-directed movements. Next, the key-poses of each joint are independently interpolated to generate an acyclic multi-contact motion. We demonstrate that our framework can synthesize plausible interaction motions with a number of man-made objects, such as chairs and bicycles, without using any motion data. In addition, we show the scalability of our method by creating a long-term motion of climbing a ladder.
C1 [Kim, Yeonjoon; Lee, Sung-Hee] Korea Adv Inst Sci & Technol, Grad Sch Culture Technol GSCT, 291 Daehak Ro, Daejeon 34141, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST)
RP Lee, SH (corresponding author), Korea Adv Inst Sci & Technol, Grad Sch Culture Technol GSCT, 291 Daehak Ro, Daejeon 34141, South Korea.
EM yeonjoon@kaist.ac.kr; sunghee.lee@kaist.ac.kr
OI Lee, Sung-Hee/0000-0001-6604-4709
FU Korea Creative Content Agency [NRF-2020R1A2C2011541];  [R2020040211]
FX Funding was provided byNational Research Foundation (KR) (Grant No.
   NRF-2020R1A2C2011541) and by Korea Creative Content Agency (Grand No.
   R2020040211).
CR Agrawal S, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925893
   Al-Asqhar Rami Ali, 2013, P 12 ACM SIGGRAPHEUR, P45
   [Anonymous], 2014, ACM Transactions on Graphics, DOI DOI 10.1145/2601097.2601117
   [Anonymous], 2014, PRINCIPLES NEURAL SC
   Boulic R., 1990, Visual Computer, V6, P344, DOI 10.1007/BF01901021
   Bruderlin A., 1989, Computer Graphics, V23, P233, DOI 10.1145/74334.74357
   Carpentier J, 2016, IEEE INT CONF ROBOT, P3555, DOI 10.1109/ICRA.2016.7487538
   Coleman P., 2008, Proceedings of sca 2008, P137
   Escande A, 2013, ROBOT AUTON SYST, V61, P428, DOI 10.1016/j.robot.2013.01.008
   Ha D, 2008, VISUAL COMPUT, V24, P587, DOI 10.1007/s00371-008-0239-7
   Hämäläinen P, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2767002
   Hauser K, 2005, IEEE-RAS INT C HUMAN, P7
   Hauser K, 2008, SPRINGER TRAC ADV RO, V47, P507
   Heess N., 2017, CoRR, abs/1707.02286.
   Ho ESL, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778770
   Holden D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073663
   Igarashi Takeo., 2007, ACM SIGGRAPH 2007 CO
   Kang C, 2017, GRAPH MODELS, V94, P25, DOI 10.1016/j.gmod.2017.10.002
   Kang CG, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2983619
   Kang CG, 2014, COMPUT GRAPH FORUM, V33, P1, DOI 10.1111/cgf.12468
   Kim Y, 2016, IEEE T VIS COMPUT GR, V22, P2405, DOI 10.1109/TVCG.2016.2593780
   Kitagawa N, 2016, GAIT POSTURE, V45, P110, DOI 10.1016/j.gaitpost.2016.01.014
   Koyama Y, 2019, VISUAL COMPUT, V35, P1131, DOI 10.1007/s00371-019-01693-8
   LEE B, 2019, P 2019 CHI C HUM FAC, P584
   Lee Jehee., 2002, Proceedings of the 29th annual conference on Computer graphics and interactive techniques, P491, DOI DOI 10.1145/566570.566607
   Lee KH, 2006, ACM T GRAPHIC, V25, P898, DOI 10.1145/1141911.1141972
   MEREL J., 2017, CoRR
   Naderi K, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073707
   Peng XB, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073602
   Roberts R, 2019, COMPUT VIS MEDIA, V5, P171, DOI 10.1007/s41095-019-0138-z
   Savva M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925867
   Terra S.C. L., 2004, Proceedings of ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P253, DOI DOI 10.1145/1028523.1028556
   Tonneau S, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3213773
   Tonneau S, 2018, IEEE T ROBOT, V34, P586, DOI 10.1109/TRO.2018.2819658
   Tonneau S, 2016, COMPUT GRAPH FORUM, V35, P127, DOI 10.1111/cgf.12817
   Wang Q, 2020, VISUAL COMPUT, V36, P141, DOI 10.1007/s00371-018-1594-7
   Wang YM, 2017, VISUAL COMPUT, V33, P971, DOI 10.1007/s00371-017-1378-5
   Wu JC, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778809
   Yoo I, 2014, VISUAL COMPUT, V30, P213, DOI 10.1007/s00371-013-0797-1
NR 39
TC 1
Z9 1
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 1949
EP 1963
DI 10.1007/s00371-020-01956-9
EA AUG 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000559413000001
DA 2024-07-18
ER

PT J
AU Kloiber, S
   Settgast, V
   Schinko, C
   Weinzerl, M
   Fritz, J
   Schreck, T
   Preiner, R
AF Kloiber, Simon
   Settgast, Volker
   Schinko, Christoph
   Weinzerl, Martin
   Fritz, Johannes
   Schreck, Tobias
   Preiner, Reinhold
TI Immersive analysis of user motion in VR applications
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual reality; Immersive analytics; Movement analysis; Trajectory
   visualisation; Virtual training evaluation
ID VIRTUAL-REALITY; TRAJECTORY DATA; VISUALIZATION; PERSPECTIVE; ANALYTICS;
   SYSTEM; SKILLS
AB With the rise of virtual reality experiences for applications in entertainment, industry, science and medicine, the evaluation of human motion in immersive environments is becoming more important. By analysing the motion of virtual reality users, design choices and training progress in the virtual environment can be understood and improved. Since the motion is captured in a virtual environment, performing the analysis in the same environment provides a valuable context and guidance for the analysis. We have created a visual analysis system that is designed for immersive visualisation and exploration of human motion data. By combining suitable data mining algorithms with immersive visualisation techniques, we facilitate the reasoning and understanding of the underlying motion. We apply and evaluate this novel approach on a relevant VR application domain to identify and interpret motion patterns in a meaningful way.
C1 [Kloiber, Simon; Schreck, Tobias; Preiner, Reinhold] Graz Univ Technol, Inst Comp Graph & Knowledge Visualisat, Graz, Austria.
   [Settgast, Volker; Schinko, Christoph] Fraunhofer Austria Res GmbH, Graz, Austria.
   [Weinzerl, Martin; Fritz, Johannes] AVL List GmbH, Graz, Austria.
C3 Graz University of Technology; Anstalt fur Verbrennungskraftmaschinen
   List
RP Kloiber, S (corresponding author), Graz Univ Technol, Inst Comp Graph & Knowledge Visualisat, Graz, Austria.
EM s.kloiber@cgv.tugraz.at
OI Preiner, Reinhold/0000-0002-5167-1977; Settgast,
   Volker/0000-0003-3842-5349; Weinzerl, Martin/0000-0002-3591-1598;
   Kloiber, Simon/0000-0003-1186-7630; Schreck, Tobias/0000-0003-0778-8665
FU Graz University of Technology
FX Open access funding provided by Graz University of Technology. Thanks to
   our project partners, division for Visual Computing of Fraunhofer
   Austria Research GmbH and AVL List GmbH.
CR Anagnostakis D, 2018, J COMPUT INF SCI ENG, V18, DOI 10.1115/1.4039194
   Andrienko G, 2014, VISUALIZATION TRAJEC, P157
   Andrienko G, 2018, IEEE T VIS COMPUT GR, V24, P34, DOI 10.1109/TVCG.2017.2744322
   Andrienko N, 2013, INFORM VISUAL, V12, P3, DOI 10.1177/1473871612457601
   [Anonymous], 2015, VR BOOK
   [Anonymous], LECT NOTES COMPUTER
   Assa J, 2005, ACM T GRAPHIC, V24, P667, DOI 10.1145/1073204.1073246
   Babar Z, 2017, IEEE COMMUN LETT, V21, P2005, DOI 10.1109/LCOMM.2017.2705707
   Batch A., 2019, IEEE T VIS COMPUT GR
   Bernard J, 2013, IEEE T VIS COMPUT GR, V19, P2257, DOI 10.1109/TVCG.2013.178
   Brooke J., 1996, USABILITY EVALUATION, P189, DOI DOI 10.1201/9781498710411-35
   Büschel W, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE SURFACES AND SPACES (ACM ISS 2017), P62, DOI 10.1145/3132272.3134125
   Buschmann S, 2016, VISUAL COMPUT, V32, P371, DOI 10.1007/s00371-015-1185-9
   Chandler T., 2015, 2015 BIG DATA VIS AN, P529
   Coffey D, 2012, COMPUT GRAPH FORUM, V31, P1215, DOI 10.1111/j.1467-8659.2012.03114.x
   Cordeil M, 2017, IEEE PAC VIS SYMP, P46, DOI 10.1109/PACIFICVIS.2017.8031578
   Cordeil M, 2017, IEEE T VIS COMPUT GR, V23, P441, DOI 10.1109/TVCG.2016.2599107
   Covaci A, 2015, IEEE COMPUT GRAPH, V35, P55, DOI 10.1109/MCG.2015.95
   de Sá AG, 1999, COMPUT GRAPH-UK, V23, P389, DOI 10.1016/S0097-8493(99)00047-3
   Gonzalez-Badillo G, 2014, ASSEMBLY AUTOM, V34, P41, DOI 10.1108/AA-03-2013-023
   Gonzalez-Badillo G, 2013, PROC TECH, V7, P265, DOI 10.1016/j.protcy.2013.04.033
   Guo HQ, 2011, IEEE PAC VIS SYMP, P163, DOI 10.1109/PACIFICVIS.2011.5742386
   Han J, 2012, MOR KAUF D, P1
   Hentschel B, 2008, IEEE T VIS COMPUT GR, V14, P1515, DOI 10.1109/TVCG.2008.142
   Hurter C, 2019, IEEE T VIS COMPUT GR, V25, P704, DOI 10.1109/TVCG.2018.2865191
   Lange D., 2017, EUROVIS 2017
   Lewis JR, 2018, J USABILITY STUD, V13, P158
   Lim T, 2010, ADVANCES IN HAPTICS, P693
   Murtagh F, 2017, WIRES DATA MIN KNOWL, V7, DOI 10.1002/widm.1219
   Price B., 1989, BUSINESS EC HIST 2 S, V18, P88
   Sacha D, 2017, COMPUT GRAPH FORUM, V36, P305, DOI 10.1111/cgf.13189
   Sas C, 2005, FUTURE GENER COMP SY, V21, P1157, DOI 10.1016/j.future.2004.04.003
   Schreck T, 2008, IEEE CONF VIS ANAL, P3, DOI 10.1109/VAST.2008.4677350
   Ssin SY, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P210, DOI [10.1109/VR.2019.8797812, 10.1109/vr.2019.8797812]
   Tashiro Y, 2009, CLIN ORTHOP RELAT R, V467, P546, DOI 10.1007/s11999-008-0497-8
   Tominski C, 2012, IEEE T VIS COMPUT GR, V18, P2565, DOI 10.1109/TVCG.2012.265
   Wagner Filho J., 2018, IEEE VIRTUAL REALITY
   Wagner Filho J.A., 2019, IEEE T VIS COMPUT GR
   Yuksel C., 2009, 2009 SIAM/ACM Joint Conference on Geometric and Physical Modeling, DOI [10.1145/1629255.1629262, DOI 10.1145/1629255.1629262]
   Zhang MJ, 2015, 8th International Symposium on Visual Information Communication and Interaction (VINCI 2015), P168, DOI 10.1145/2801040.2801072
   Zia A, 2018, INT J COMPUT ASS RAD, V13, P443, DOI 10.1007/s11548-018-1704-z
NR 41
TC 29
Z9 30
U1 4
U2 27
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 1937
EP 1949
DI 10.1007/s00371-020-01942-1
EA AUG 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000559296000002
OA hybrid
DA 2024-07-18
ER

PT J
AU Liu, L
   Chen, SQ
   Chen, XX
   Wang, TS
   Zhang, L
AF Liu, Li
   Chen, Siqi
   Chen, Xiuxiu
   Wang, Tianshi
   Zhang, Long
TI Fuzzy weighted sparse reconstruction error-steered semi-supervised
   learning for face recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Membership function; Sparse representation; Fuzzy
ID LABEL PROPAGATION; REPRESENTATION
AB Since the number of labeled data is limited in the semi-supervised learning settings, we propose a fuzzy weighted sparse reconstruction error-steered semi-supervised learning method for face recognition. The fuzzy membership functions are introduced to the reconstruction error calculation for the unlabeled data. A weight function is utilized to capture the locality property of data when learning the sparse coefficients. The fuzzy weighted sparse reconstruction error-steered semi-supervised learning not only inherits the advantages of sparse representation classification techniques and neighborhood methods, but also steers the reconstruction errors of unlabeled data. Experimental studies on well-known face image datasets demonstrate that the proposed method outperforms the comparative approaches.
C1 [Liu, Li; Chen, Siqi; Chen, Xiuxiu; Wang, Tianshi; Zhang, Long] Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250014, Shandong, Peoples R China.
C3 Shandong Normal University
RP Liu, L (corresponding author), Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250014, Shandong, Peoples R China.
EM liuli_790209@163.com
RI Chen, Siqi/IZE-8631-2023
CR [Anonymous], 2008, Advances in Neural Information Processing Systems
   Bezdek James C., 1981, PATTERN RECOGN
   Cheng B, 2010, IEEE T IMAGE PROCESS, V19, P858, DOI 10.1109/TIP.2009.2038764
   Cheng H, 2009, IEEE I CONF COMP VIS, P317, DOI 10.1109/ICCV.2009.5459267
   Fan MY, 2011, PATTERN RECOGN, V44, P1777, DOI 10.1016/j.patcog.2011.02.013
   Kobayashi T, 2014, PATTERN RECOGN, V47, P1994, DOI 10.1016/j.patcog.2013.11.011
   Lafferty J. D., 2003, P INT C MACH LEARN, P912, DOI DOI 10.5555/3041838.3041953
   Müller KR, 2001, IEEE T NEURAL NETWOR, V12, P181, DOI 10.1109/72.914517
   Pei XB, 2015, IEEE T CYBERNETICS, V45, P1681, DOI 10.1109/TCYB.2014.2358592
   Peng Y, 2015, NEURAL NETWORKS, V65, P1, DOI 10.1016/j.neunet.2015.01.001
   Rakotomamonjy A, 2008, J MACH LEARN RES, V9, P2491
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Shang FH, 2013, PATTERN RECOGN, V46, P2323, DOI 10.1016/j.patcog.2013.01.009
   Song X, 2013, APPL MECH MAT, V347, P3797, DOI [10.4028/www.scientific.net/AMM.347-350.3797, DOI 10.4028/WWW.SCIENTIFIC.NET/AMM.347-350.3797]
   Tang JH, 2007, LECT NOTES COMPUT SC, V4426, P793
   Wang F, 2008, IEEE T KNOWL DATA EN, V20, P55, DOI 10.1109/TKDE.2007.190672
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wang SH, 2012, IEEE T MULTIMEDIA, V14, P1259, DOI 10.1109/TMM.2012.2193120
   Wang Z, 2016, VISUAL COMPUT, V32, P307, DOI 10.1007/s00371-015-1067-1
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Xiang SM, 2010, IEEE T PATTERN ANAL, V32, P2039, DOI 10.1109/TPAMI.2010.35
   Zang F, 2012, NEUROCOMPUTING, V97, P267, DOI 10.1016/j.neucom.2012.03.017
   Zhang Z, 2015, IEEE T KNOWL DATA EN, V27, P2362, DOI 10.1109/TKDE.2013.182
   Zhao ZQ, 2012, IEEE T IMAGE PROCESS, V21, P4218, DOI 10.1109/TIP.2012.2197631
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zhuang L., 2012, P 25 IEEE C COMP VIS
NR 26
TC 10
Z9 11
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2020
VL 36
IS 8
BP 1521
EP 1534
DI 10.1007/s00371-019-01746-y
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ML7DA
UT WOS:000549620700001
DA 2024-07-18
ER

PT J
AU Dulecha, TG
   Fanni, FA
   Ponchio, F
   Pellacini, F
   Giachetti, A
AF Dulecha, Tinsae G.
   Fanni, Filippo A.
   Ponchio, Federico
   Pellacini, Fabio
   Giachetti, Andrea
TI Neural reflectance transformation imaging
SO VISUAL COMPUTER
LA English
DT Article
DE Reflectance transformation imaging; Relighting; Neural network;
   Autoencoder; Benchmark
AB Reflectance transformation imaging (RTI) is a computational photography technique widely used in the cultural heritage and material science domains to characterize relieved surfaces. It basically consists of capturing multiple images from a fixed viewpoint with varying lights. Handling the potentially huge amount of information stored in an RTI acquisition that consists typically of 50-100 RGB values per pixel, allowing data exchange, interactive visualization, and material analysis, is not easy. The solution used in practical applications consists of creating "relightable images" by approximating the pixel information with a function of the light direction, encoded with a small number of parameters. This encoding allows the estimation of images relighted from novel, arbitrary lights, with a quality that, however, is not always satisfactory. In this paper, we present NeuralRTI, a framework for pixel-based encoding and relighting of RTI data. Using a simple autoencoder architecture, we show that it is possible to obtain a highly compressed representation that better preserves the original information and provides increased quality of virtual images relighted from novel directions, especially in the case of challenging glossy materials. We also address the problem of validating the relight quality on different surfaces, proposing a specific benchmark, SynthRTI, including image collections synthetically created with physical-based rendering and featuring objects with different materials and geometric complexity. On this dataset and as well on a collection of real acquisitions performed on heterogeneous surfaces, we demonstrate the advantages of the proposed relightable image encoding.
C1 [Dulecha, Tinsae G.; Fanni, Filippo A.; Giachetti, Andrea] Univ Verona, Dept Comp Sci, Verona, Italy.
   [Ponchio, Federico] CNR, ISTI, Pisa, Italy.
   [Pellacini, Fabio] Sapienza Univ Rome, Comp Sci, Rome, Italy.
   [Pellacini, Fabio] Sapienza Univ Rome, Computat Design Lab, Rome, Italy.
C3 University of Verona; Consiglio Nazionale delle Ricerche (CNR); Istituto
   di Scienza e Tecnologie dell'Informazione "Alessandro Faedo" (ISTI-CNR);
   Sapienza University Rome; Sapienza University Rome
RP Dulecha, TG (corresponding author), Univ Verona, Dept Comp Sci, Verona, Italy.
EM tinsae.dulecha@univr.it; filippoandrea.fanni@univr.it;
   federico.ponchio@isti.cnr.it; pellacini@di.uniroma1.it;
   andrea.giachetti@univr.it
RI Ponchio, Federico/AAX-7043-2020
OI Fanni, Filippo Andrea/0000-0002-1737-9876; Ponchio,
   Federico/0000-0002-2974-0577; Giachetti, Andrea/0000-0002-7523-6806;
   Dulecha, Tinsae Gebrechristos/0000-0003-3955-3019
FU Universita degli Studi di Verona within the CRUI-CARE Agreement; DSURF
   (PRIN 2015) project - Italian Ministry of University and Research; MIUR
   Excellence Departments 2018-2022
FX Open access funding provided by Universita degli Studi di Verona within
   the CRUI-CARE Agreement. This work was supported by the DSURF (PRIN
   2015) project funded by the Italian Ministry of University and Research
   and by the MIUR Excellence Departments 2018-2022.
CR [Anonymous], P ACM WEB3D
   [Anonymous], EUROGRAPHICS TUTORIA
   [Anonymous], 2006, 7 INT S VIRT REAL AR, DOI DOI 10.2312/VAST/VAST06/195-202
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Chen GY, 2019, PROC CVPR IEEE, P8731, DOI 10.1109/CVPR.2019.00894
   Earl G., 2011, Electronic Visualisation and the Arts (EVA) 2011, P147, DOI [10.14236/ewic/EVA2011.27, DOI 10.14236/EWIC/EVA2011.27]
   Giachetti A, 2018, COMPUT VIS IMAGE UND, V168, P118, DOI 10.1016/j.cviu.2017.05.014
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Kingma D.P., 2014, ARXIV14126980
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Malzbender T, 2001, COMP GRAPH, P519, DOI 10.1145/383259.383320
   Palma G, 2010, ACM J COMPUT CULT HE, V3, DOI 10.1145/1841317.1841321
   Pintus R, 2019, COMPUT GRAPH FORUM, V38, P909, DOI 10.1111/cgf.13732
   Pintus R., 2018, EUROGRAPHICS WORKSHO, P87, DOI [10.2312/gch.20181344, DOI 10.2312/GCH.20181344]
   Pitard G, 2017, MACH VISION APPL, V28, P607, DOI 10.1007/s00138-017-0856-0
   Pitard G, 2015, PROC SPIE, V9525, DOI 10.1117/12.2184840
   Rainer G, 2019, COMPUT GRAPH FORUM, V38, P235, DOI 10.1111/cgf.13633
   Ren PR, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766899
   Shi BX, 2016, PROC CVPR IEEE, P3707, DOI 10.1109/CVPR.2016.403
   Smilkov D., 2019, ABS190105350 CORR
   Xie J., 2012, ADV NEURAL INFORM PR, P341
   Zhang WH, 2018, COMPUT IND, V98, P56, DOI 10.1016/j.compind.2018.02.006
   Zheng BL, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.4.043037
   Zhou Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201292
   Zini S., 2019, ARXIV190306117
NR 25
TC 8
Z9 8
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2161
EP 2174
DI 10.1007/s00371-020-01910-9
EA JUL 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000549695100001
OA Green Published, hybrid
DA 2024-07-18
ER

PT J
AU Koseoglu, B
   Kaya, E
   Balcisoy, S
   Bozkaya, B
AF Koseoglu, Baran
   Kaya, Erdem
   Balcisoy, Selim
   Bozkaya, Burcin
TI ST Sequence Miner: visualization and mining of spatio-temporal event
   sequences
SO VISUAL COMPUTER
LA English
DT Article
DE Sequence mining; Event sequences; Spatio-temporal data; Information
   visualization; Visual analytics
ID VISUAL ANALYTICS; PATTERNS
AB As a promising field of research, event sequence analysis seems to assist in facilitating clear reasoning behind human decisions by mining reality behind the sequential actions. Mining frequent patterns from event sequences has proved to be promising in extracting actionable insights, which plays an important role in many application domains. Much of the related work challenges the problem solely from the temporal perspective omitting the information that could be gained from the spatial part. This could be in part due to the fact that analysis of event sequences with references to both time and space is attributed as a challenging task due to the additional variance in the data introduced by the spatial aspect. We propose a visual analytics approach that incorporates spatio-temporal pattern extraction leveraging an extended sequential pattern mining algorithm and a pattern discovery guidance mechanism operating on geographic query and selection capabilities. As an implementation of our approach, we introduce a visual analytics tool, namely ST Sequence Miner, enabling event pattern exploration in time-location space. We evaluate our approach over a credit card transaction dataset by adopting case study methodology. Our study unveils that patterns mined from event sequences can better explain possible relationships with proper visualization of time-location data.
C1 [Koseoglu, Baran; Kaya, Erdem; Balcisoy, Selim; Bozkaya, Burcin] Sabanci Univ, Behav Analyt & Visualizat Lab, TR-34956 Istanbul, Turkey.
   [Bozkaya, Burcin] Sabanci Univ, Sabanci Business Sch, TR-34956 Istanbul, Turkey.
   [Bozkaya, Burcin] New Coll Florida, 5800 Bay Shore Rd, Sarasota, FL 34243 USA.
C3 Sabanci University; Sabanci University; State University System of
   Florida; New College Florida
RP Koseoglu, B (corresponding author), Sabanci Univ, Behav Analyt & Visualizat Lab, TR-34956 Istanbul, Turkey.
EM bkoseoglu@sabanciuniv.edu
RI Bozkaya, Burcin/AAY-8995-2020
OI Bozkaya, Burcin/0000-0002-3685-4791; Koseoglu,
   Baran/0000-0002-5402-1603; Balcisoy, Selim/0000-0002-6495-7341
CR AGRAWAL R, 1995, PROC INT CONF DATA, P3, DOI 10.1109/ICDE.1995.380415
   Agrawal R., P 20 INT C VERY LARG
   Andrienko G., 2004, P WORK C ADV VIS INT, P417
   Andrienko G, 2006, J INTELL INF SYST, V27, P187, DOI 10.1007/s10844-006-9949-3
   Andrienko G, 2017, IEEE T INTELL TRANSP, V18, P2232, DOI 10.1109/TITS.2017.2683539
   Andrienko G, 2010, INT J GEOGR INF SCI, V24, P1577, DOI 10.1080/13658816.2010.508043
   Andrienko N, 2013, INFORM VISUAL, V12, P3, DOI 10.1177/1473871612457601
   [Anonymous], 1996, ADV DATABASE TECHNOL, DOI DOI 10.1007/BFB0014140
   Bertin J., 1983, SEMIOLOGY GRAPHICS D
   Bonilla E., 2007, Proceedings of the Advances in Neural Information Processing Systems, P153
   Carter E., 2013, P WORKSH INT SYST HE
   Chen YZ, 2018, IEEE T VIS COMPUT GR, V24, P45, DOI 10.1109/TVCG.2017.2745083
   Cressie N, 2010, J COMPUT GRAPH STAT, V19, P724, DOI 10.1198/jcgs.2010.09051
   Di Clemente R, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-05690-8
   Du F, 2017, IEEE T VIS COMPUT GR, V23, P1636, DOI 10.1109/TVCG.2016.2539960
   Elmqvist N, 2010, IEEE T VIS COMPUT GR, V16, P439, DOI 10.1109/TVCG.2009.84
   Fournier-Viger P., 2017, DATA SCI PATTERN REC, V1, P54, DOI DOI 10.1007/978-3-030-04921-8_4
   Guo HQ, 2011, IEEE PAC VIS SYMP, P163, DOI 10.1109/PACIFICVIS.2011.5742386
   Hicks D., 1996, DISCOURSE LEARNING S, P104, DOI DOI 10.1017/CBO9780511720390
   Liu DY, 2019, IEEE T VIS COMPUT GR, V25, P1, DOI 10.1109/TVCG.2018.2865018
   Liu SX, 2014, VISUAL COMPUT, V30, P1373, DOI 10.1007/s00371-013-0892-3
   Liu ZC, 2017, IEEE T VIS COMPUT GR, V23, P321, DOI 10.1109/TVCG.2016.2598797
   Meyer T.E., 2013, TECHNICAL REPORT
   Morup M, 2011, WIRES DATA MIN KNOWL, V1, P24, DOI 10.1002/widm.1
   Nguyen PH, 2019, IEEE T VIS COMPUT GR, V25, P2838, DOI 10.1109/TVCG.2018.2859969
   North C, 2006, IEEE COMPUT GRAPH, V26, P6, DOI 10.1109/MCG.2006.70
   Pei J, 2004, IEEE T KNOWL DATA EN, V16, P1424, DOI 10.1109/TKDE.2004.77
   Pennacchioli D, 2014, EPJ DATA SCI, V3, DOI 10.1140/epjds/s13688-014-0033-x
   Perer Adam., 2014, P 19 INT C INTELLIGE, P153
   Scheepens R, 2011, IEEE T VIS COMPUT GR, V17, P2518, DOI 10.1109/TVCG.2011.181
   Shneiderman B, 1996, IEEE SYMPOSIUM ON VISUAL LANGUAGES, PROCEEDINGS, P336, DOI 10.1109/VL.1996.545307
   Teng L, 2015, CANCER GENE THER, V22, P518, DOI 10.1038/cgt.2015.45
   Tominski C, 2012, IEEE T VIS COMPUT GR, V18, P2565, DOI 10.1109/TVCG.2012.265
   Tsoukatos I, 2001, LECT NOTES COMPUT SC, V2121, P425
   Vrotsou K, 2019, IEEE T VIS COMPUT GR, V25, P2597, DOI 10.1109/TVCG.2018.2848247
   Wang JM, 2004, PROC INT C TOOLS ART, P14
   Ward M. O., 2002, Information Visualization, V1, P194, DOI 10.1057/palgrave.ivs.9500025
   Yi JS, 2007, IEEE T VIS COMPUT GR, V13, P1224, DOI 10.1109/TVCG.2007.70515
   Zaki MJ, 2001, MACH LEARN, V42, P31, DOI 10.1023/A:1007652502315
   Zheng Y, 2014, ACM T INTEL SYST TEC, V5, DOI 10.1145/2629592
NR 40
TC 3
Z9 4
U1 2
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2369
EP 2381
DI 10.1007/s00371-020-01894-6
EA JUL 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000549317900001
DA 2024-07-18
ER

PT J
AU Fu, YP
   Yan, QA
   Liao, J
   Chow, ALH
   Xiao, CX
AF Fu, Yanping
   Yan, Qingan
   Liao, Jie
   Chow, Alix L. H.
   Xiao, Chunxia
TI Real-time dense 3D reconstruction and camera tracking via embedded
   planes representation
SO VISUAL COMPUTER
LA English
DT Article
DE 3D reconstruction; RGB-D reconstruction; Camera tracking
AB This paper proposes a novel approach for robust plane matching and real-time RGB-D fusion based on the representation of plane parameter space. In contrast to previous planar-based SLAM algorithms estimating correspondences for each plane-pair independently, our method instead explores the holistic topology of all relevant planes. We note that by adopting the low-dimensionality parameter space representation, the plane matching can be intuitively reformulated and solved as a point cloud registration problem. Besides estimating the plane correspondences, we contribute an efficient optimization framework, which employs both frame-to-frame and frame-to-model planar consistency constraints. We propose a global plane map to dynamically represent the reconstructed scene and alleviate accumulation errors that exist in camera pose tracking. We validate the proposed algorithm on standard benchmark datasets and additional challenging real-world environments. The experimental results demonstrate its outperformance to current state-of-the-art methods in tracking robustness and reconstruction fidelity.
C1 [Fu, Yanping; Liao, Jie; Xiao, Chunxia] Wuhan Univ, Sch Comp Sci, Wuhan, Hubei, Peoples R China.
   [Yan, Qingan] JD Com, Silicon Valley Res Ctr, Mountain View, CA USA.
   [Chow, Alix L. H.] Xiaomi Technol Co LTD, Beijing, Peoples R China.
C3 Wuhan University
RP Xiao, CX (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan, Hubei, Peoples R China.
EM ypfu@whu.edu.cn; qingan.yan@jd.com; liaojie@whu.edu.cn;
   zhouliheng@xiaomi.com; cxxiao@whu.edu.cn
RI Fu, Yanping/AAE-4921-2022
OI Fu, Yanping/0000-0002-4191-4779
FU NSFC [61972298, 61672390]; National Key Research and Development Program
   of China [2017YFB1002600]; Key Technological Innovation Projects of
   Hubei Province [2018AAA062]
FX Funding was provided by NSFC (Grant Nos. 61972298, 61672390), the
   National Key Research and Development Program of China (Grant No.
   2017YFB1002600), and the Key Technological Innovation Projects of Hubei
   Province (Grant No. 2018AAA062).
CR Adrien Kaiser A.Y., 2018, ECCV
   Agamennoni G., 2016, IROS
   [Anonymous], 2013, ICCV
   [Anonymous], CVPR
   [Anonymous], 2018, ECCV
   [Anonymous], 2012, IROS
   [Anonymous], 2015, ACM T GRAPHIC
   [Anonymous], 2013, CVPR
   [Anonymous], 2014, ICRA
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bylow E., 2013, Robotics: Science and Systems, V2
   Concha Alejo, 2015, IROS
   Dai A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3054739
   Dou M., 2013, ACCV WORKSH
   Dzitsiuk Maksym, 2017, ICRA
   Feng Chen, 2014, ICRA
   Fernandez- Moral E., 2013, ICRA
   Flint A., 2010, CVPR
   Fu Y., 2018, CVPR
   Halber M., 2017, CVPR
   Jeon J, 2016, VISUAL COMPUT, V32, P955, DOI 10.1007/s00371-016-1249-5
   Kerl C., 2013, ICRA
   Kim P., 2018, ICRA
   Kim Pyojin, 2018, ECCV
   Le P.H., 2017, IROS
   Lee Jun-Tae, 2017, ICCV
   Li R., 2016, CASE
   Ma L., 2016, ICRA
   Ming H., 2017, ICRA
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Newcombe R.A., 2012, ISMAR
   Okada K., 2001, ICRA
   Proença PF, 2018, ROBOT AUTON SYST, V104, P25, DOI 10.1016/j.robot.2018.02.018
   Pumarola Albert, 2017, ICRA
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Salas M., 2015, IROS
   Salas-Moreno Renato F, 2014, ISMAR
   Sun Q., 2012, IM AN SIGN PROC IASP
   Taguchi Y., 2013, ICRA
   Taguchi Y., 2012, ISMAR
   Wei MQ, 2019, VISUAL COMPUT, V35, P797, DOI 10.1007/s00371-019-01688-5
   Whelan T, 2015, ROBOT AUTON SYST, V69, P3, DOI 10.1016/j.robot.2014.08.019
   Whelan T, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI
   Yang L, 2018, IEEE T VIS COMPUT GR, V24, P1190, DOI 10.1109/TVCG.2017.2657766
   Yang L, 2017, VISUAL COMPUT, V33, P385, DOI 10.1007/s00371-016-1208-1
   Yang SC, 2019, IEEE T ROBOT, V35, P925, DOI 10.1109/TRO.2019.2909168
   Zuo Xingxing, 2017, IROS
NR 47
TC 11
Z9 11
U1 4
U2 48
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2215
EP 2226
DI 10.1007/s00371-020-01899-1
EA JUL 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000548816100001
OA Bronze
DA 2024-07-18
ER

PT J
AU Wan, J
   Mougeot, G
   Yang, XB
AF Wan, Jerome
   Mougeot, Guillaume
   Yang, Xubo
TI Dense feature pyramid network for cartoon dog parsing
SO VISUAL COMPUTER
LA English
DT Article
DE Cartoon character parsing; Semantic part segmentation; Pyramid network;
   Encoder-decoder; Deep learning for vision
AB While traditional cartoon character drawings are simple for humans to create, it remains a highly challenging task for machines to interpret. Parsing is a way to alleviate the issue with fine-grained semantic segmentation of images. Although well studied on naturalistic images, research toward cartoon parsing is very sparse. Due to the lack of available dataset and the diversity of artwork styles, the difficulty of the cartoon character parsing task is greater than the well-known human parsing task. In this paper, we study one type of cartoon instance: cartoon dogs. We introduce a novel dataset toward cartoon dog parsing and create a new deep convolutional neural network (DCNN) to tackle the problem. Our dataset contains 965 precisely annotated cartoon dog images with seven semantic part labels. Our new model, called dense feature pyramid network (DFPnet), makes use of recent popular techniques on semantic segmentation to efficiently handle cartoon dog parsing. We achieve a mIoU of 68.39%, a Mean Accuracy of 79.4% and a Pixel Accuracy of 93.5% on our cartoon dog validation set. Our method outperforms state-of-the-art models of similar tasks trained on our dataset: CE2P for single human parsing and Mask R-CNN for instance segmentation. We hope this work can be used as a starting point for future research toward digital artwork understanding with DCNN. Our DFPnet and dataset will be publicly available.
C1 [Wan, Jerome; Mougeot, Guillaume] Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
   [Yang, Xubo] Shanghai Jiao Tong Univ, Sch Software, Shanghai, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University
RP Wan, J (corresponding author), Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
EM jerome.wan@sjtu.edu.cn; gmougeot@sjtu.edu.cn; yangxubo@sjtu.edu.cn
OI Mougeot, Guillaume/0000-0003-3576-7300
FU National Key Research and Development Program of China [2018YFB1004902];
   National Natural Science Foundation of China [61772329, 61373085]
FX This work was partially supported by the National Key Research and
   Development Program of China (2018YFB1004902) and the National Natural
   Science Foundation of China (61772329, 61373085).
CR [Anonymous], 2014, ARXIV14064729 CORR
   [Anonymous], 2015, NIPS
   [Anonymous], 2016, ARXIV 1606 00915 CS
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Chen L., 2018, ARXIV180202611 CORR
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Chen X., 2014, ARXIV14062031 CORR
   Cordts M., 2016, ARXIV160401685 CORR
   de Juan C.N., 2006, P 2006 ACM SIGGRAPH
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dutta Abhishek, 2019, MM '19: Proceedings of the 27th ACM International Conference on Multimedia, P2276, DOI 10.1145/3343031.3350535
   Entem E, 2019, COMPUT GRAPH-UK, V81, P125, DOI 10.1016/j.cag.2019.04.006
   Entem E, 2015, COMPUT GRAPH-UK, V46, P221, DOI 10.1016/j.cag.2014.09.037
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Feng LL, 2017, P IEEE VIRT REAL ANN, P195, DOI 10.1109/VR.2017.7892247
   Ghiasi G., 2016, ARXIV160502264 CORR
   Gong K., 2017, ARXIV170305446 CORR
   Gong K., 2018, ARXIV180800157 CORR
   Guérin C, 2013, PROC INT CONF DOC, P1145, DOI 10.1109/ICDAR.2013.232
   He K., 2017, ARXIV170306870 CORR
   He Kaiming, 2016, P INT C COMP VIS PAT, DOI 10.1109/CVPR.2016.90
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Li Y., 2018, 2018 15 IEEE INT C A, P1, DOI [10.1109/AVSS.2018.8639428, DOI 10.1109/AVSS.2018.8639428]
   Liang X., 2018, ARXIV180401984 CORR
   Lin G., 2016, ARXIV161106612
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   LIN TY, 2016, ARXIV161203144 CORR
   Liu T., 2018, ARXIV180905996 CORR
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Matsui Y., 2015, ARXIV151004389 CORR
   Mottaghi R, 2014, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2014.119
   Ogawa T., 2018, ARXIV180308670 CORR
   Papandreou G, 2018, LECT NOTES COMPUT SC, V11218, P282, DOI 10.1007/978-3-030-01264-9_17
   Paszke A., 2019, ADV NEURAL INFORM PR, P8026, DOI DOI 10.48550/ARXIV.1912.01703
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Waleed A, 2017, GITHUB REPOSITORY
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Weng C., 2018, ARXIV181202246 CORR
   Wu HS, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING (ICSIP), P277, DOI 10.1109/SIPROCESS.2016.7888267
   Xu B., 2015, Empirical evaluation of rectified activations in convolutional network, DOI DOI 10.48550/ARXIV.1505.00853
   Yamaguchi K, 2012, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2012.6248101
   Yang L., 2018, ARXIV181112596 CORR
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao J., 2018, ARXIV180403287 CORR
   Zhou YQ, 2018, PROCEEDINGS OF THE 16TH ACM SIGGRAPH INTERNATIONAL CONFERENCE ON VIRTUAL-REALITY CONTINUUM AND ITS APPLICATIONS IN INDUSTRY (VRCAI 2018), DOI 10.1145/3284398.3284403
   Zhu Z., 2019, ARXIV190807678 CORR
NR 48
TC 1
Z9 1
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2471
EP 2483
DI 10.1007/s00371-020-01887-5
EA JUL 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000546849000002
DA 2024-07-18
ER

PT J
AU Bari, ASMH
   Sieu, B
   Gavrilova, ML
AF Bari, A. S. M. Hossain
   Sieu, Brandon
   Gavrilova, Marina L.
TI AestheticNet: deep convolutional neural network for person
   identification from visual aesthetic
SO VISUAL COMPUTER
LA English
DT Article
DE Visual aesthetic; Behavioral biometric; Deep learning; Principal
   component analysis; Residual learning-based convolutional neural network
AB A person's visual aesthetics is an emerging behavioral biometric. Visual aesthetics can be defined as a person's principles pertaining to their sense of beauty or fondness. Utilizing a person's preference to certain images as discriminatory features forms the basis of person identification from visual aesthetics. This paper proposes a novel three-stage framework based on the convolutional neural network, AestheticNet, for the extraction of high-level features and identification of individuals from visual aesthetics. The rank-1 accuracy of 97.73% and rank-5 accuracy of 99.85% are achieved on the publicly available benchmark dataset, which outperforms all state-of-the-art methods.
C1 [Bari, A. S. M. Hossain; Sieu, Brandon; Gavrilova, Marina L.] Univ Calgary, Calgary, AB T2N 1N4, Canada.
C3 University of Calgary
RP Bari, ASMH (corresponding author), Univ Calgary, Calgary, AB T2N 1N4, Canada.
EM asmhossain.bati@ucalgary.ca; brandon.sieu@ucalgary.ca;
   mgavrilo@ucalgary.ca
RI Gavrilova, Marina/AHD-3605-2022
OI Gavrilova, Marina/0000-0002-5338-1834; Bari, A S M
   Hossain/0000-0003-1850-4816
FU NSERC [10007544, 10022972]
FX The NSERC Discovery Grant #10007544 and NSERC Strategic Planning Grant
   #10022972 are the acknowledgments of the funding agencies.
CR Ahmed F, 2020, IEEE ACCESS, V8, P11761, DOI 10.1109/ACCESS.2019.2963113
   Ali S, 2017, 2017 INTERNATIONAL CONFERENCE ON ELECTRICAL AND COMPUTING TECHNOLOGIES AND APPLICATIONS (ICECTA), P100
   Bari ASMH, 2019, IEEE ACCESS, V7, P162708, DOI 10.1109/ACCESS.2019.2952065
   Boureau Y. L., 2010, P 27 INT C MACH LEAR, P111
   Cheng SW, 2012, J MULTIMODAL USER IN, V5, P77, DOI 10.1007/s12193-011-0064-6
   Csurka G, 2011, VISUAL COMPUT, V27, P1039, DOI 10.1007/s00371-011-0657-9
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ekárt A, 2011, LECT NOTES COMPUT SC, V6625, P303, DOI 10.1007/978-3-642-20520-0_31
   Gavrilova ML, 2018, IEEE CONSUM ELECTR M, V7, P88, DOI 10.1109/MCE.2017.2755498
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Guo SH, 2017, VISUAL COMPUT, V33, P63, DOI 10.1007/s00371-016-1329-6
   Hastings EJ, 2009, IEEE T COMP INTEL AI, V1, P245, DOI 10.1109/TCIAIG.2009.2038365
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Kingma D. P., 2014, arXiv
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2
   Li Y, 2013, GENET PROGRAM EVOL M, V14, P315, DOI 10.1007/s10710-013-9188-7
   Lovato P., 2012, P AS C COMP VIS, P45, DOI DOI 10.1007/978.3.642.37331.2_4
   Luo Y, 2008, INT J PATTERN RECOGN, V22, P555, DOI 10.1142/S0218001408006399
   Magnenat-Thalmann N, 2005, VISUAL COMPUT, V21, P997, DOI 10.1007/s00371-005-0363-6
   Moshagen M, 2010, INT J HUM-COMPUT ST, V68, P689, DOI 10.1016/j.ijhcs.2010.05.006
   Nygren N, 2011, IEEE CONF COMPU INTE, P55, DOI 10.1109/CIG.2011.6031989
   Pak M, 2017, INT CONF COMP APPL I, P367
   Segalin C, 2014, IEEE IMAGE PROC, P4982, DOI 10.1109/ICIP.2014.7026009
   Segalin C., 2014, Proceedings of the 16th International Conference on Multimodal Interaction, ICMI'14, page, P180, DOI DOI 10.1145/2663204.2663259
   Sieu B, 2019, 2019 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P279, DOI 10.1109/CW.2019.00053
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sultana M, 2015, INT J PATTERN RECOGN, V29, DOI 10.1142/S0218001415560133
   Yampolskiy Roman V., 2014, International Journal of Natural Computing Research, V4, P85, DOI 10.4018/ijncr.2014070105
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhao L, 2015, VISUAL COMPUT, V31, P765, DOI 10.1007/s00371-015-1091-1
NR 30
TC 9
Z9 9
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2395
EP 2405
DI 10.1007/s00371-020-01893-7
EA JUL 2020
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000545293900001
DA 2024-07-18
ER

PT J
AU Yu, J
   Kim, DY
   Yoon, Y
   Jeon, M
AF Yu, Jongmin
   Kim, Du Yong
   Yoon, Yongsang
   Jeon, Moongu
TI Action matching network: open-set action recognition using
   spatio-temporal representation matching
SO VISUAL COMPUTER
LA English
DT Article
DE Open-set action; Action recognition; Spatio-temporal representation; 3D
   convolution
AB In this paper, we address an open-set action recognition problem. While the closed-set action recognition classifies test samples into the same classes of actions used for model training, the problem of the open-set action recognition is more challenging because there is a possibility that the trained model has to recognize actions which do not appear in the training set. To address this issue, we propose an action matching network (AMN) that can identify and classify both actions in the training dataset and the actions not included in the set. AMN extracts spatio-temporal representations from the given video clips and constructs an action dictionary using the given samples. Then, AMN classifies an action by computing the similarity based on Euclidean distance or generates a new action class in the constructed dictionary if it is necessary. Experimental results on UCF101 dataset and a large human motion dataset (a.k.a., HMDB dataset) demonstrate the benefits of AMN over the state-of-the-art approaches to open-set action recognition problems.
C1 [Yu, Jongmin] Curtin Univ, Perth, WA 6152, Australia.
   [Kim, Du Yong] RMIT Univ, Melbourne, Vic 3000, Australia.
   [Yu, Jongmin; Yoon, Yongsang; Jeon, Moongu] Gwangju Inst Sci & Technol GIST, Gwangju 61005, South Korea.
C3 Curtin University; Royal Melbourne Institute of Technology (RMIT);
   Gwangju Institute of Science & Technology (GIST)
RP Yu, J (corresponding author), Curtin Univ, Perth, WA 6152, Australia.; Yu, J (corresponding author), Gwangju Inst Sci & Technol GIST, Gwangju 61005, South Korea.
EM jm.andrew.yu@gmail.com; duyong.kim@rmit.edu.au; nil@gist.ac.kr;
   mgjeon@gist.ac.kr
RI Jeon, Moongu/A-1009-2012
FU Ministry of Culture, Sport and Tourism (MCST); Korea Creative Content
   Agency (KOCCA) in the Culture Technology (CT) Research & Development
   Program 2019 through the Korea Culture Technology Institute (KCTI);
   Gwangju Institute of Science and Technology(GIST); Institute for
   Information & communications Technology Promotion (IITP) - Korea
   government (MSIP) [B0101-15-0525]
FX This work was supported in part by Ministry of Culture, Sport and
   Tourism (MCST) and Korea Creative Content Agency (KOCCA) in the Culture
   Technology (CT) Research & Development Program 2019 through the Korea
   Culture Technology Institute (KCTI), Gwangju Institute of Science and
   Technology(GIST), and Institute for Information & communications
   Technology Promotion (IITP) Grant & funded by the Korea government
   (MSIP) (No. B0101-15-0525, Development of global multi-target tracking
   and event prediction techniques based on real-time large-scale video
   analysis).
CR [Anonymous], ARXIVABS170506950 CO
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Biggs David S C, 2010, Curr Protoc Cytom, VChapter 12, DOI 10.1002/0471142956.cy1219s52
   Bilen H, 2018, IEEE T PATTERN ANAL, V40, P2799, DOI 10.1109/TPAMI.2017.2769085
   Brand M, 1997, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.1997.609450
   Bucilua C., 2006, P 12 ACM SIGKDD INT, P535, DOI DOI 10.1145/1150402.1150464
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen FS, 2003, IMAGE VISION COMPUT, V21, P745, DOI 10.1016/S0262-8856(03)00070-2
   Choi JY, 2018, VISUAL COMPUT, V34, P1535, DOI 10.1007/s00371-017-1429-y
   Dean J., 2015, NIPS DEEP LEARNING R
   Diba A, 2017, PROC CVPR IEEE, P1541, DOI 10.1109/CVPR.2017.168
   Dietich B, 2017, PR IEEE COMP DESIGN, P1, DOI 10.1109/ICCD.2017.10
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Eitel A, 2015, IEEE INT C INT ROBOT, P681, DOI 10.1109/IROS.2015.7353446
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Girdhar R., 2017, PROC CVPR IEEE, P971, DOI DOI 10.1109/CVPR.2017.337
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Hara K, 2017, IEEE INT CONF COMP V, P3154, DOI 10.1109/ICCVW.2017.373
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hong S, 2016, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR.2016.349
   Hou XN, 2016, VISUAL COMPUT, V32, P479, DOI 10.1007/s00371-015-1079-x
   Hou YH, 2018, IEEE T CIRC SYST VID, V28, P807, DOI 10.1109/TCSVT.2016.2628339
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Koch G., 2015, ICML DEEP LEARNING W, V2
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Kumar P, 2018, IEEE IMAGE PROC, P3503, DOI 10.1109/ICIP.2018.8451295
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li WL, 2011, 3RD INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY AND COMPUTER SCIENCE (ITCS 2011), PROCEEDINGS, P1
   Liou CY, 2008, NEUROCOMPUTING, V71, P3150, DOI 10.1016/j.neucom.2008.04.030
   Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shuai B, 2018, IEEE T PATTERN ANAL, V40, P1480, DOI 10.1109/TPAMI.2017.2712691
   Simonyan K, 2014, ADV NEUR IN, V27
   Singh C, 2012, VISUAL COMPUT, V28, P1085, DOI 10.1007/s00371-011-0659-7
   Socher R., 2012, NIPS, V3, P8
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Suk H.I., 2008, 8 IEEE INT C AUTOMAT, P1
   Suk HI, 2010, PATTERN RECOGN, V43, P3059, DOI 10.1016/j.patcog.2010.03.016
   Sun L, 2015, IEEE I CONF COMP VIS, P4597, DOI 10.1109/ICCV.2015.522
   Sun YP, 2015, ADV DIFFER EQU-NY, P1, DOI 10.1186/s13662-015-0433-7
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tang YL, 2016, VISUAL COMPUT, V32, P111, DOI 10.1007/s00371-014-1059-6
   Varol G, 2018, IEEE T PATTERN ANAL, V40, P1510, DOI 10.1109/TPAMI.2017.2712608
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang CC, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON CYBERNETICS AND INTELLIGENT SYSTEMS (CIS) AND IEEE CONFERENCE ON ROBOTICS, AUTOMATION AND MECHATRONICS (RAM), P742, DOI 10.1109/ICCIS.2017.8274871
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang LM, 2016, INT J COMPUT VISION, V119, P254, DOI 10.1007/s11263-015-0859-0
   Wang PC, 2016, IEEE T HUM-MACH SYST, V46, P498, DOI 10.1109/THMS.2015.2504550
   Wang Y, 2011, CHIN CONTR CONF, P5299
   Wang ZF, 2014, VISUAL COMPUT, V30, P359, DOI 10.1007/s00371-013-0861-x
   Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140
   Yang Y, 2019, PATTERN RECOGN, V85, P60, DOI 10.1016/j.patcog.2018.07.030
   Yu J, 2019, IEEE T INTELL TRANSP, V20, P4206, DOI 10.1109/TITS.2018.2883823
   Yu J, 2018, IEEE IMAGE PROC, P1658, DOI 10.1109/ICIP.2018.8451494
   Yu RC, 2017, IEEE I CONF COMP VIS, P1068, DOI 10.1109/ICCV.2017.121
   Yuanyuan Shi, 2018, 2018 IEEE Power & Energy Society General Meeting (PESGM), DOI 10.1109/PESGM.2018.8586227
   Zhang N., 2007, Tech. Rep. 07-49, P7
   Zhu JG, 2018, INT C PATT RECOG, P645, DOI 10.1109/ICPR.2018.8545710
NR 69
TC 11
Z9 11
U1 2
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2020
VL 36
IS 7
BP 1457
EP 1471
DI 10.1007/s00371-019-01751-1
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LO1PW
UT WOS:000533401000012
DA 2024-07-18
ER

PT J
AU Chen, ZC
   Lv, XB
   Sun, TL
   Zhao, CR
   Chen, W
AF Chen, Zhicheng
   Lv, Xinbi
   Sun, Tianli
   Zhao, Cairong
   Chen, Wei
TI FLAG: feature learning with additional guidance for person search
SO VISUAL COMPUTER
LA English
DT Article
DE Person search; Feature learning; Additional guidance
ID NETWORK
AB Person search is a challenging computer vision task that handles and optimizes both pedestrian detection and person re-identification simultaneously. Person search is also closer to real-world applications compared to person re-identification. Existing person search works mainly focused on refining loss functions, using more complex network structures or redefining the person search as another task. However, few of them attempted to solve this problem from a feature representation perspective. In this paper, we embark on this point and present a novel method called FLAG to learn a better feature representation for person search. Specifically, partition pooling and cross-level feature hybridization are proposed to guide the model to learn more discriminative person features. Experiments show that the proposed method achieves encouraging performance improvement and outperforms similar end-to-end person search methods.
C1 [Chen, Zhicheng; Lv, Xinbi; Sun, Tianli; Zhao, Cairong] Tongji Univ, Dept Comp Sci & Technol, Shanghai 201804, Peoples R China.
   [Chen, Wei] Shanghai Key Lab Crime Scene Evidence, Shanghai, Peoples R China.
   [Chen, Wei] Shanghai Res Inst Criminal Sci & Technol, Shanghai, Peoples R China.
C3 Tongji University
RP Zhao, CR (corresponding author), Tongji Univ, Dept Comp Sci & Technol, Shanghai 201804, Peoples R China.; Chen, W (corresponding author), Shanghai Key Lab Crime Scene Evidence, Shanghai, Peoples R China.; Chen, W (corresponding author), Shanghai Res Inst Criminal Sci & Technol, Shanghai, Peoples R China.
EM 1910075@tongji.edu.cn; zhaocairong@tongji.edu.cn; chen_wei@yuansi.com.cn
RI Zhang, Weihua/IAQ-1186-2023
OI Zhang, Weihua/0000-0003-0662-7018
FU National Natural Science Foundation of China (NSFC) [61673299, 61203247,
   61573259, 61573255]; Fundamental Research Funds for the Central
   Universities; Open Project Program of the National Laboratory of Pattern
   Recognition (NLPR)
FX The authors would like to thank the anonymous reviewers for their
   critical and constructive comments and suggestions. This work was
   supported by the National Natural Science Foundation of China (NSFC)
   under Grant Nos. 61673299, 61203247, 61573259, and 61573255. This work
   was also supported by the Fundamental Research Funds for the Central
   Universities and the Open Project Program of the National Laboratory of
   Pattern Recognition (NLPR).
CR Cai ZW, 2015, IEEE I CONF COMP VIS, P3361, DOI 10.1109/ICCV.2015.384
   Chen D, 2018, LECT NOTES COMPUT SC, V11211, P764, DOI 10.1007/978-3-030-01234-2_45
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Ding SY, 2015, PATTERN RECOGN, V48, P2993, DOI 10.1016/j.patcog.2015.04.005
   Dollar D, 2009, DIR DEV, P1
   Dollár P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479
   Dong YY, 2017, 2016 INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION, BIG DATA & SMART CITY (ICITBS), P225, DOI 10.1109/ICITBS.2016.57
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Gao GW, 2020, INFORM SCIENCES, V506, P19, DOI 10.1016/j.ins.2019.08.004
   Gao GW, 2017, PATTERN RECOGN, V66, P129, DOI 10.1016/j.patcog.2016.12.021
   Hosang J, 2015, PROC CVPR IEEE, P4073, DOI 10.1109/CVPR.2015.7299034
   Huang QQ, 2018, LECT NOTES COMPUT SC, V11217, P437, DOI 10.1007/978-3-030-01261-8_26
   Ji Z, 2018, PATTERN RECOGN LETT, V116, P205, DOI 10.1016/j.patrec.2018.10.020
   Lan X, 2018, LECT NOTES COMPUT SC, V11205, P553, DOI 10.1007/978-3-030-01246-5_33
   Li S, 2009, 2009 5TH INTERNATIONAL CONFERENCE ON WIRELESS COMMUNICATIONS, NETWORKING AND MOBILE COMPUTING, VOLS 1-8, P970
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liu H, 2017, IEEE I CONF COMP VIS, P493, DOI 10.1109/ICCV.2017.61
   Liu H, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1668, DOI 10.1109/ICASSP.2018.8462484
   Shi W, 2018, IEEE IMAGE PROC, P4108, DOI 10.1109/ICIP.2018.8451028
   Tian YL, 2015, IEEE I CONF COMP VIS, P1904, DOI 10.1109/ICCV.2015.221
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xiao JM, 2019, PATTERN RECOGN, V87, P332, DOI 10.1016/j.patcog.2018.10.028
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140
   Xiao Tong, 2016, arXiv preprint arXiv:1604.01850, V2
   Xu YL, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P937, DOI 10.1145/2647868.2654965
   Yang J, 2013, IEEE T NEUR NET LEAR, V24, P1023, DOI 10.1109/TNNLS.2013.2249088
   Yang J, 2012, PATTERN RECOGN, V45, P1104, DOI 10.1016/j.patcog.2011.08.022
   Yang JF, 2017, COMM COM INF SC, V773, P315, DOI 10.1007/978-981-10-7305-2_28
   Ye M, 2019, PROC CVPR IEEE, P6203, DOI 10.1109/CVPR.2019.00637
   Ye M, 2019, IEEE T IMAGE PROCESS, V28, P2976, DOI 10.1109/TIP.2019.2893066
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Zhang SS, 2015, PROC CVPR IEEE, P1751, DOI 10.1109/CVPR.2015.7298784
   Zhang SS, 2014, PROC CVPR IEEE, P947, DOI 10.1109/CVPR.2014.126
   Zhang Xiangyu, 2017, CoRRabs/1711.08184
   Zhao CR, 2020, PATTERN RECOGN, V97, DOI 10.1016/j.patcog.2019.107014
   Zhao CR, 2017, PATTERN RECOGN, V71, P218, DOI 10.1016/j.patcog.2017.06.011
   Zhao YR, 2019, PROC CVPR IEEE, P4908, DOI 10.1109/CVPR.2019.00505
   Zheng L, 2017, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2017.357
NR 40
TC 5
Z9 5
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2021
VL 37
IS 4
SI SI
BP 685
EP 693
DI 10.1007/s00371-020-01880-y
EA JUN 2020
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RL0LI
UT WOS:000541310600001
DA 2024-07-18
ER

PT J
AU Zhi, RC
   Liu, MY
   Zhang, DZ
AF Zhi, Ruicong
   Liu, Mengyi
   Zhang, Dezheng
TI A comprehensive survey on automatic facial action unit analysis
SO VISUAL COMPUTER
LA English
DT Article
DE Facial Action Coding System; Action unit; Facial representation; Video;
   3D; Survey
ID EXPRESSION RECOGNITION; INTENSITY ESTIMATION; EMOTION RECOGNITION; FACE
   DETECTION; POSTOPERATIVE PAIN; TEMPORAL SEGMENTS; IMAGE; 3D; MACHINE;
   FEATURES
AB Facial Action Coding System is the most influential sign judgment method for facial behavior, and it is a comprehensive and anatomical system which could encode various facial movements by the combination of basic AUs (Action Units). AUs define certain facial configurations caused by contraction of one or more facial muscles, and they are independent of interpretation of emotions. However, automatic facial action unit recognition remains challenging due to several open questions such as rigid and non-rigid facial motions, multiple AUs detection, intensity estimation and naturalistic context application. This paper introduces recent advances in automatic facial action unit recognition, focusing on the fundamental components of face detection and registration, facial action representation, feature selection and classification. The comprehensive analysis of facial representations is presented according to the facial data properties (image and video, 2D and 3D) and characteristics of facial features (predesign and learning, appearance and geometry, hybrid and fusion). Facial action unit recognition involves AUs occurrence detection, AUs temporal segment detection and AUs intensity estimation. We discussed the role of each component, main techniques with their characteristics, challenges and potential directions of facial action unit analysis.
C1 [Zhi, Ruicong; Liu, Mengyi; Zhang, Dezheng] Univ Sci & Technol Beijing, Sch Comp & Commun Engn, Beijing 100083, Peoples R China.
   [Zhi, Ruicong; Liu, Mengyi; Zhang, Dezheng] Beijing Key Lab Knowledge Engn Mat Sci, Beijing 100083, Peoples R China.
C3 University of Science & Technology Beijing
RP Zhi, RC (corresponding author), Univ Sci & Technol Beijing, Sch Comp & Commun Engn, Beijing 100083, Peoples R China.; Zhi, RC (corresponding author), Beijing Key Lab Knowledge Engn Mat Sci, Beijing 100083, Peoples R China.
EM zhirc_research@126.com
FU National Natural Science Foundation of China [61673052]; National
   Research and Development Major Project [2017YFD0400100]; Fundamental
   Research Fund for the Central Universities of China
   [2302018FRF-TP-18-014A2]; Chinese Scholarship Council (CSC)
FX This work was supported by the National Natural Science Foundation of
   China (61673052), the National Research and Development Major Project
   (2017YFD0400100), the Fundamental Research Fund for the Central
   Universities of China (2302018FRF-TP-18-014A2) and the grant from
   Chinese Scholarship Council (CSC).
CR Corneanu CA, 2016, IEEE T PATTERN ANAL, V38, P1548, DOI 10.1109/TPAMI.2016.2515606
   Ambadar Z, 2009, J NONVERBAL BEHAV, V33, P17, DOI 10.1007/s10919-008-0059-5
   Andrieu C, 2004, P IEEE, V92, P423, DOI 10.1109/JPROC.2003.823142
   [Anonymous], 2010, IEEE CVPR 10 WORKSHO
   [Anonymous], 2006, P 2006 C COMPUTER VI
   [Anonymous], 2005, P 2005 JOINT C SMART
   [Anonymous], 2016, CORR
   [Anonymous], 2010, CHAPTER AFFECTIVE CO
   Bänziger T, 2012, EMOTION, V12, P1161, DOI 10.1037/a0025827
   Baltruaitis T., 2015, 11 IEEE INT C WORKSH, P1
   Baltrusaitis T, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P354, DOI 10.1109/ICCVW.2013.54
   Bartlett MS, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P223, DOI 10.1109/fgr.2006.55
   Bartlett MS, 1999, PSYCHOPHYSIOLOGY, V36, P253, DOI 10.1017/S0048577299971664
   Bartlett MS, 2005, PROC CVPR IEEE, P568
   Bartlett MS, 2006, J MULTIMED, V1, P35
   BASSILI JN, 1979, J PERS SOC PSYCHOL, V37, P2049, DOI 10.1037/0022-3514.37.11.2049
   Bazzo JJ, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P505, DOI 10.1109/AFGR.2004.1301583
   Benitez-Quiroz C.F., 2017, P IEEE INT C COMPUTE
   Bevilacqua F, 2016, INT CONF GAMES VIRTU
   Bihan Jiang, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P314, DOI 10.1109/FG.2011.5771416
   Bishay M, 2017, IEEE INT CONF AUTOMA, P681, DOI 10.1109/FG.2017.86
   Blom P.M., 2021, P AAAI C ART INT INT, VVolume 10, P30, DOI [10.1609/aiide.v10i1.12707, DOI 10.1609/AIIDE.V10I1.12707]
   Brahnam S, 2007, DECIS SUPPORT SYST, V43, P1242, DOI 10.1016/j.dss.2006.02.004
   Cakir D, 2016, 7TH IEEE ANNUAL INFORMATION TECHNOLOGY, ELECTRONICS & MOBILE COMMUNICATION CONFERENCE IEEE IEMCON-2016
   Chu Wen-Sheng, 2013, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit, V2013, P3515
   COHEN J, 1960, EDUC PSYCHOL MEAS, V20, P37, DOI 10.1177/001316446002000104
   Cohn J.F., 2006, Proceedings of the 8th international conference on Multimodal interfaces-ICMI'06, P233, DOI DOI 10.1145/1180995.1181043
   Cohn JF, 2004, IEEE SYS MAN CYBERN, P610
   Cohn JF., 2009, 3 INT C AFFECTIVE CO
   Cohn JF, 1999, FACS CODING PSYCHOPH, V36, P43
   Corneanu C.A., 2018, ARXIV 180305873V2
   Cosker D, 2011, IEEE I CONF COMP VIS, P2296, DOI 10.1109/ICCV.2011.6126510
   Cotter SF, 2010, INT CONF ACOUST SPEE, P838, DOI 10.1109/ICASSP.2010.5494903
   Cowie R, 2001, IEEE SIGNAL PROC MAG, V18, P32, DOI 10.1109/79.911197
   Cruz A, 2011, LECT NOTES COMPUT SC, V6975, P341, DOI 10.1007/978-3-642-24571-8_45
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Danelakis A, 2016, VISUAL COMPUT, V32, P1001, DOI 10.1007/s00371-016-1243-y
   De la Torre F, 2011, LECT NOTES COMPUT SC, V6974, P57, DOI 10.1007/978-3-642-24600-5_9
   DeVault D., [No title captured]
   Dhall Abhinav, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P878, DOI 10.1109/FG.2011.5771366
   Ding Xiaoyu, 2013, Proc IEEE Int Conf Comput Vis, V2013, P2400, DOI 10.1109/ICCV.2013.298
   Donato G, 1999, IEEE T PATTERN ANAL, V21, P974, DOI 10.1109/34.799905
   Duan XD, 2016, NEUROCOMPUTING, V217, P27, DOI 10.1016/j.neucom.2016.03.090
   EKMAN P, 1987, J PERS SOC PSYCHOL, V53, P712, DOI 10.1037/0022-3514.53.4.712
   Ekman P., 2005, WHAT FACE REVEALS BA
   Ekman P, 2002, FACIAL ACTION CODING
   Ekman P., 1978, Facial action coding system
   el Kaliouby P, 2005, REAL TIME VISION HUM, V181, P200
   Eleftheriadis S, 2016, IEEE T IMAGE PROCESS, V25, P5727, DOI 10.1109/TIP.2016.2615288
   Eleftheriadis S, 2015, IEEE I CONF COMP VIS, P3792, DOI 10.1109/ICCV.2015.432
   Frank MG, 2004, J PERS SOC PSYCHOL, V86, P486, DOI 10.1037/0022-3514.86.3.486
   Friesen W. V., 1983, Uni Cali SF
   Gilbert CA, 1999, CLIN J PAIN, V15, p192 200
   Girard JM, 2015, PATTERN RECOGN LETT, V66, P13, DOI 10.1016/j.patrec.2014.10.004
   Girard JM, 2014, IMAGE VISION COMPUT, V32, P641, DOI 10.1016/j.imavis.2013.12.007
   Gong B., 2009, Proceedings of the 17th ACM International Conference on Multimedia, P569
   Hammal Z, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P47, DOI 10.1145/2388676.2388688
   He S, 2013, INT CONF AFFECT, P239, DOI 10.1109/ACII.2013.46
   Heylen D., 2008, LREC WORKSHOP CORPOR, P1
   Hjortsjo C., 1970, Man's face and mimic language
   Hu Q., 2018, 2018 IEEE INT C MULT, P1
   Izard CE, 1979, [No title captured]
   Izard CE, 1982, CAMBRIDGE STUDIES SO, V114, P116
   Jabid T, 2010, ETRI J, V32, P784, DOI 10.4218/etrij.10.1510.0132
   Jeni LA, 2013, IEEE INT CONF AUTOMA
   Jeni LA, 2013, INT CONF AFFECT, P245, DOI 10.1109/ACII.2013.47
   Ji Q, 2006, IEEE T SYST MAN CY A, V36, p862 875
   Jiang B, 2014, IEEE T CYBERN, V44, P174
   Jiang BH, 2014, INT C PATT RECOG, P1776, DOI 10.1109/ICPR.2014.312
   Jorn O, 2003, FACE ANIMATION MPEG, V17, P55
   Ju Q, 2013, INT J COMPUT INT SYS, V6, P669, DOI 10.1080/18756891.2013.802873
   Kaiser M, 2010, IEEE INT CONF ROBOT, P1002, DOI 10.1109/ROBOT.2010.5509629
   Kaltwang S, 2012, LECT NOTES COMPUT SC, V7432, P368, DOI 10.1007/978-3-642-33191-6_36
   Kanade T., 2000, P 4 IEEE INT C AUT F, P46, DOI [10.1109/AFGR.2000.840611, DOI 10.1109/AFGR.2000.840611]
   Kapoor A, 2003, IEEE INTERNATIONAL WORKSHOP ON ANALYSIS AND MODELING OF FACE AND GESTURES, P195
   Khorrami P, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P19, DOI 10.1109/ICCVW.2015.12
   Kim M, 2010, LECT NOTES COMPUT SC, V6313, P649
   Koelstra S., 2008, IEEE INT C AUTOMATIC, P1
   Koelstra S, 2010, IEEE T PATTERN ANAL, V32, P1940, DOI 10.1109/TPAMI.2010.50
   Kotsia I, 2007, IEEE T IMAGE PROCESS, V16, P172, DOI 10.1109/TIP.2006.884954
   KRIPPEND.K, 1970, EDUC PSYCHOL MEAS, V30, P61, DOI 10.1177/001316447003000105
   Li W, 2018, IEEE T PATTERN ANAL, V40, P2583, DOI 10.1109/TPAMI.2018.2791608
   Li X., 2017 IEEE 12 INT C A
   Li Y., 2013, 10 IEEE INT C WORKSH, P22
   Li YQ, 2016, PATTERN RECOGN, V60, P890, DOI 10.1016/j.patcog.2016.07.009
   Li YQ, 2013, IEEE T AFFECT COMPUT, V4, P127, DOI 10.1109/T-AFFC.2013.5
   Li Z, 2016, 2016 IEEE 15TH INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS & COGNITIVE COMPUTING (ICCI*CC), P373, DOI 10.1109/ICCI-CC.2016.7862062
   Lifkooee MZ, 2018, MACH VISION APPL, V30, P57
   Liong ST, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P665, DOI 10.1109/ACPR.2015.7486586
   Littlewort G., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P897, DOI 10.1109/FG.2011.5771370
   Littlewort G., 2004, P 2004 IEEE C COMPUT, P80
   Littlewort G, 2006, IMAGE VISION COMPUT, V24, P615, DOI 10.1016/j.imavis.2005.09.011
   Littlewort GC, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P15
   Liu P, 2014, PROC CVPR IEEE, P1805, DOI 10.1109/CVPR.2014.233
   Liu Y, 2016, IEEE T AFFECT COMPUT, V7, p299 310
   Long F, 2012, NEUROCOMPUTING, V93, P126, DOI 10.1016/j.neucom.2012.04.017
   Lucas GM, 2015, INT CONF AFFECT, P539, DOI 10.1109/ACII.2015.7344622
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Lucey P, 2012, IMAGE VISION COMPUT, V30, P197, DOI 10.1016/j.imavis.2011.12.003
   Lucey P, 2011, IEEE T SYST MAN CY B, V41, P664, DOI 10.1109/TSMCB.2010.2082525
   Lucey S, 2007, FACE RECOGNITION BOO, V395, P406
   Lucey S, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P155
   Mahoor Mohammad H., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P336, DOI 10.1109/FG.2011.5771420
   Mahoor Mohammad H., 2009, 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P74, DOI 10.1109/CVPR.2009.5204259
   Martinez B, 2017, IEEE T AFFECT COMPUT, V13, P22
   Mavadati SM, 2014, INT C PATT RECOG, P4648, DOI 10.1109/ICPR.2014.795
   Mavadati SM, 2013, IEEE T AFFECT COMPUT, V4, P151, DOI 10.1109/T-AFFC.2013.4
   McDuff D, 2013, IEEE COMPUT SOC CONF, P881, DOI 10.1109/CVPRW.2013.130
   McKeown G, 2012, IEEE T AFFECT COMPUT, V3, P17
   Meng Z, 2018, IEEE T CYBERN, V49, P3306
   Meng ZB, 2019, IEEE T AFFECT COMPUT, V10, P537, DOI 10.1109/TAFFC.2017.2749299
   Ming ZH, 2015, IEEE INT CONF AUTOMA
   Modrow D, 2008, PROC SPIE, V6805, DOI 10.1117/12.766216
   Mohammadi MR, 2016, IEEE T CYBERNETICS, V46, P817, DOI 10.1109/TCYB.2015.2416317
   Mohammadian A, 2016, EXPERT SYST APPL, V56, P282, DOI 10.1016/j.eswa.2016.03.023
   Nicolle J, 2015, IEEE INT CONF AUTOMA
   Nicolle J, 2016, IMAGE VISION COMPUT, V52, P1, DOI 10.1016/j.imavis.2016.03.004
   Nishtha N.R., 2016, INT C INT RES ENG TE, P83
   Oh YH, 2016, INT CONF ACOUST SPEE, P1851, DOI 10.1109/ICASSP.2016.7471997
   Pantic M, 2005, IEEE SYS MAN CYBERN, P3358
   Pantic M, 2004, IEEE T SYST MAN CY B, V34, P1449, DOI 10.1109/TSMCB.2004.825931
   Pantic M., 2005, IEEE INT C MULTIMEDI
   Pantic M., 2004, IEEE INT C MULTIMEDI, P49
   Pantic M, 2007, FACE RECOGNITION, V377, P416
   Pantic M, 2006, IEEE T SYST MAN CY B, V36, P433, DOI 10.1109/TSMCB.2005.859075
   Petajan E, 2005, HCI REAL TIME VISION, V249, P268
   Peters JWB, 2003, CLIN J PAIN, V19, P353, DOI 10.1097/00002508-200311000-00003
   Ranjan R, 2019, IEEE T PATTERN ANAL, V41, P121, DOI 10.1109/TPAMI.2017.2781233
   Rashid M, 2013, VISUAL COMPUT, V29, P1269, DOI 10.1007/s00371-012-0768-y
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rojo R, 2015, MED CLIN-BARCELONA, V145, p350 355
   Rosenberg EL, 1998, HEALTH PSYCHOL, V17, P380
   Rudovic O, 2012, LECT NOTES COMPUT SC, V7584, P260, DOI 10.1007/978-3-642-33868-7_26
   Russell JamesA., 1997, The Psychology of Facial Expression
   Salter T, 2009, AMD NEWSLETT
   Sánchez-Lozano E, 2016, LECT NOTES COMPUT SC, V9912, P645, DOI 10.1007/978-3-319-46484-8_39
   Sandbach G., 2012, Image and Vision Computing, V30
   Sandbach G, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P738, DOI 10.1109/ICCVW.2013.101
   Sandbach G, 2012, IEEE IMAGE PROC, P1813, DOI 10.1109/ICIP.2012.6467234
   Sariyanidi E, 2015, IEEE T PATTERN ANAL, V37, P1113, DOI 10.1109/TPAMI.2014.2366127
   Savran A., 2009, IEEE 12 INT C COMPUT, P612
   Savran A, 2011, EUR SIGNAL PR CONF, P1969
   Savran A, 2012, IMAGE VISION COMPUT, V30, P774, DOI 10.1016/j.imavis.2011.11.008
   Savran A, 2012, PATTERN RECOGN, V45, P767, DOI 10.1016/j.patcog.2011.07.022
   Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6
   Sayette MA, 2012, PSYCHOL SCI, V23, P869, DOI 10.1177/0956797611435134
   Scherer K.R., 2008, NEW HDB METHODS NONV
   Senechal T, 2012, IEEE T SYST MAN CY B, V42, P993, DOI 10.1109/TSMCB.2012.2193567
   Senechal T, 2011, LECT NOTES COMPUT SC, V6915, P495, DOI 10.1007/978-3-642-23687-7_45
   Seshadri K, 2016, IEEE T PATTERN ANAL, V38, P2110, DOI 10.1109/TPAMI.2015.2505301
   Shreve M., 2009, 2009 WORKSHOP APPL C
   Sikka K.:., 2014, Proceedings of the 16th International Conference on Multimodal Interaction, P349
   Sikka K, 2012, LECT NOTES COMPUT SC, V7584, P250, DOI 10.1007/978-3-642-33868-7_25
   Simon T, 2010, PROC CVPR IEEE, P2737, DOI 10.1109/CVPR.2010.5539998
   Siritanawan P, 2015, INT CONF SOFT COMPUT, P161, DOI 10.1109/SOCPAR.2015.7492801
   Steidl S, 2005, INT CONF ACOUST SPEE, P317
   Stratou Giota, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P611, DOI 10.1109/FG.2011.5771467
   Sun Y., 2008, IEEE INT C AUTOMATIC, P1
   Taheri S, 2014, IEEE T IMAGE PROCESS, V23, P3590, DOI 10.1109/TIP.2014.2331141
   Tam GKL, 2013, IEEE T VIS COMPUT GR, V19, P1199, DOI 10.1109/TVCG.2012.310
   Tan C.T., 2012, Proceedings of The 8th Australasian Conference on Interactive Entertainment: Playing the System (p. 5:1-5:10)
   Tang C., 2017, IEEE 12 INT C AUTOMA
   Tian YL, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P229, DOI 10.1109/AFGR.2002.1004159
   Tian YL, 2000, PROC CVPR IEEE, P294, DOI 10.1109/CVPR.2000.855832
   Tingfan Wu, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P889, DOI 10.1109/FG.2011.5771369
   Tong Y, 2007, IEEE T PATTERN ANAL, V29, P1683, DOI 10.1109/TPAMI.2007.1094
   Tong Y, 2010, IEEE T PATTERN ANAL, V32, P258, DOI 10.1109/TPAMI.2008.293
   Tsalakanidou F, 2009, PROC CVPR IEEE, P763
   Tsalakanidou F, 2010, PATTERN RECOGN, V43, P1763, DOI 10.1016/j.patcog.2009.12.009
   Tzimiropoulos G, 2014, PROC CVPR IEEE, P1851, DOI 10.1109/CVPR.2014.239
   Valstar M, 2004, IEEE SYS MAN CYBERN, P635
   VALSTAR M., 2005, COMPUTER VISION PATT, P76, DOI DOI 10.1109/CVPR.2005.457
   Valstar M., 2010, Proceedings of 3rd intern. workshop on EMOTION (satellite of LREC): Corpora for research on emotion and affect, P65
   Valstar M., 2006, COMP VIS PATT REC WO, P149
   Valstar MF, 2007, LECT NOTES COMPUT SC, V4796, P118
   Valstar Michel F., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P921, DOI 10.1109/FG.2011.5771374
   Valstar M.F., 2017, 2017 IEEE 12 INT C A
   Valstar M.F., 2015, P 2015 IEEE INT C WO, P1
   Valstar M.F., 2006, International Conference on Multimodal Interfaces, P162, DOI DOI 10.1145/1180995.1181031
   Valstar MF, 2012, IEEE T SYST MAN CY B, V42, P966, DOI 10.1109/TSMCB.2012.2200675
   Valstar MF, 2012, IEEE T SYST MAN CY B, V42, P28, DOI 10.1109/TSMCB.2011.2163710
   Valstar MF, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P38
   Vinciarelli A, 2009, IMAGE VISION COMPUT, V27, P1743, DOI 10.1016/j.imavis.2008.11.007
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang LW, 2005, IEEE T PATTERN ANAL, V27, P1334, DOI 10.1109/TPAMI.2005.165
   Wang N., 2014, INT J COMPUT VISION, P1
   Wang S, 2015, IEEE T IMAGE PROCESS, V24
   Wang SF, 2017, PATTERN RECOGN, V61, P78, DOI 10.1016/j.patcog.2016.07.028
   Wang Y., 2017, ARXIV PREPRINT ARXIV
   Wang ZH, 2013, IEEE I CONF COMP VIS, P3304, DOI 10.1109/ICCV.2013.410
   Wendin K, 2011, FOOD QUAL PREFER, V22, P346, DOI 10.1016/j.foodqual.2011.01.002
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu BY, 2015, PATTERN RECOGN, V48, P2279, DOI 10.1016/j.patcog.2015.01.022
   Wu H, 2018, IEEE ACCESS, V6, P49563, DOI 10.1109/ACCESS.2018.2869465
   Wu Q, 2011, LECT NOTES COMPUT SC, V6975, P152, DOI 10.1007/978-3-642-24571-8_16
   Wu SZ, 2017, NEUROCOMPUTING, V221, P138, DOI 10.1016/j.neucom.2016.09.072
   Wu Tingfan., 2010, IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P42
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yadav PC, 2016, 2016 INTERNATIONAL CONFERENCE ON EMERGING TRENDS IN ELECTRICAL ELECTRONICS & SUSTAINABLE ENERGY SYSTEMS (ICETEESES), P347, DOI 10.1109/ICETEESES.2016.7581407
   Yan WJ, 2013, IEEE INT CONF AUTOMA
   Yan WJ, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0086041
   Yang CF, 2007, I C COMP GRAPH IM VI, P349, DOI 10.1109/CGIV.2007.84
   Yang MH, 2002, IEEE T PATTERN ANAL, V24, P34, DOI 10.1109/34.982883
   Yang P, 2009, PATTERN RECOGN LETT, V30, P132, DOI 10.1016/j.patrec.2008.03.014
   Ying-li Tian, 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P484, DOI 10.1109/AFGR.2000.840678
   Zafeiriou S., 2010, 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Workshops, P32
   Zafeiriou S, 2015, COMPUT VIS IMAGE UND, V138, P1, DOI 10.1016/j.cviu.2015.03.015
   Zamzmi G., 2016, INT C PATTERN RECOGN
   Zeng J, 2016, IEEE T IMAGE PROCESS, V25
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zhang C, 2014, IEEE WINT CONF APPL, P1036, DOI 10.1109/WACV.2014.6835990
   Zhang C, 2010, C ELECT INSUL DIEL P
   Zhang L, 2008, LECT NOTES COMPUT SC, V5303, P706, DOI 10.1007/978-3-540-88688-4_52
   Zhang P, 2016, OPTIK, V127, P1400
   Zhang X., 2013, 10 IEEE INT C AUTOMA, P1
   Zhang X, 2016, PATTERN RECOGN, V51, P187, DOI 10.1016/j.patcog.2015.08.026
   Zhang X, 2014, INT C PATT RECOG, P1863, DOI 10.1109/ICPR.2014.326
   Zhang X, 2014, IMAGE VISION COMPUT, V32, P692, DOI 10.1016/j.imavis.2014.06.002
   Zhang Y, 2015, EXPERT SYST APPL, V42, P1446, DOI 10.1016/j.eswa.2014.08.042
   Zhang Y, 2018, PROC CVPR IEEE, P2314, DOI 10.1109/CVPR.2018.00246
   Zhao JF, 2018, VISUAL COMPUT, V34, P1461, DOI 10.1007/s00371-018-1477-y
   Zhao K, 2016, IEEE T IMAGE PROCESS, V25
   Zhao KL, 2016, PROC CVPR IEEE, P3391, DOI 10.1109/CVPR.2016.369
   Zhao KL, 2015, PROC CVPR IEEE, P2207, DOI 10.1109/CVPR.2015.7298833
   Zhao SQ, 2008, IEEE IMAGE PROC, P2144, DOI 10.1109/ICIP.2008.4712212
   Zhao X., 2010, 4 IEEE INT C BIOMETR, P1
   Zhi RC, 2011, IEEE T SYST MAN CY B, V41, P38, DOI 10.1109/TSMCB.2010.2044788
   Zhou Y., 2017, 2017 IEEE 12 INT C A
   Zhou ZH, 2006, ACM T INFORM SYST, V24, P219, DOI 10.1145/1148020.1148023
   Zisheng Li, 2009, RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication, P961, DOI 10.1109/ROMAN.2009.5326323
NR 230
TC 52
Z9 55
U1 8
U2 64
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2020
VL 36
IS 5
BP 1067
EP 1093
DI 10.1007/s00371-019-01707-5
PG 27
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA LJ3BL
UT WOS:000530043100015
DA 2024-07-18
ER

PT J
AU Jamróz, D
AF Jamroz, Dariusz
TI Multidimensional virtual reality-MVR method: a new method of
   visualization of multidimensional worlds
SO VISUAL COMPUTER
LA English
DT Article
DE Multidimensional virtual reality; MVR method; Multidimensional world;
   Multidimensional labyrinth; Multidimensional visualization; Perception
AB The paper presents a new, original method of multidimensional worlds' visualization. It allows to present views of any dimension objects out of which it is possible to construct even the most complicated multidimensional virtual world on a computer screen. Due to this, it is possible to observe multidimensional worlds modeled in this way, analyze mutual relations between multidimensional objects, move between them and, most importantly, verify whether human brain is able to adapt to the perception of more than three-dimensional space. This paper presents example interior views of four-dimensional and five-dimensional labyrinths. It also presents results of the research performed on 97 IT students at the AGH University of Science and Technology. Students in total made 357 attempts to leave virtual four-dimensional and five-dimensional labyrinths, each having three difficulty levels. The method presented in this paper is sufficiently general to allow observation of objects in an n-dimensional space for any n >= 3. Simultaneously, it is the natural extension of our reality perception because using this method for n=3 we obtain views known to us from our human experience from the three-dimensional space.
C1 [Jamroz, Dariusz] AGH Univ Sci & Technol, Dept Appl Comp Sci, Al Mickiewicza 30, PL-30059 Krakow, Poland.
C3 AGH University of Krakow
RP Jamróz, D (corresponding author), AGH Univ Sci & Technol, Dept Appl Comp Sci, Al Mickiewicza 30, PL-30059 Krakow, Poland.
EM jamroz@agh.edu.pl
RI Jamroz, Dariusz/V-1277-2018
OI Jamroz, Dariusz/0000-0002-8126-3595
CR Banchoff T. F, 1990, Beyond the third dimension
   CAREY SA, 1987, COMPUT GRAPH WORLD, V10, P93
   DEWDNEY AK, 1986, SCI AM, V254, P14
   HANSON AJ, 1992, IEEE COMPUT GRAPH, V12, P54, DOI 10.1109/38.144827
   Inselberg A., 2009, Parallel Coordinates. Visual Multidimensional Geometry and its Applications, DOI DOI 10.1007/978-0-387-68628-8
   Jamroz D, 2018, LECT NOTES ARTIF INT, V10842, P364, DOI 10.1007/978-3-319-91262-2_33
   Jamroz D, 2018, LECT NOTES COMPUT SC, V10862, P675, DOI 10.1007/978-3-319-93713-7_64
   Jamróz D, 2017, ARCH MIN SCI, V62, P445, DOI 10.1515/amsc-2017-0034
   Jamroz D, 2017, INFORM VISUAL, V16, P346, DOI 10.1177/1473871616686634
   Jamroz D, 2009, ADV INTELL SOFT COMP, V59, P445
   Jolliffe I. T., 2002, PRINCIPAL COMPONENT
   Kim SS, 2000, METRIKA, V51, P39, DOI 10.1007/s001840000043
   Niedoba T, 2015, ARCH MIN SCI, V60, P93, DOI 10.1515/amsc-2015-0007
   Niedoba T, 2014, PHYSICOCHEM PROBL MI, V50, P575, DOI 10.5277/ppmp140213
   NOLL AM, 1967, COMMUN ACM, V10, P469, DOI 10.1145/363534.363544
   STEINER KV, 1987, COMPUT GRAPH WORLD, V10, P71
   Wang YC, 2019, VISUAL COMPUT, V35, P1567, DOI 10.1007/s00371-018-1558-y
NR 17
TC 2
Z9 2
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2020
VL 36
IS 4
BP 733
EP 742
DI 10.1007/s00371-019-01653-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KW0AL
UT WOS:000520835800006
OA hybrid
DA 2024-07-18
ER

PT J
AU Ranganath, A
   Senapati, MR
   Sahu, PK
AF Ranganath, Abadhan
   Senapati, Manas Ranjan
   Sahu, Pradip Kumar
TI Estimating the fractal dimension of images using pixel range calculation
   technique
SO VISUAL COMPUTER
LA English
DT Article
DE Reduction factor; Differential box count (DBC); Pixel range calculation
   (PRC); Gray level
ID TEXTURE REPRESENTATION; FACE RECOGNITION
AB In this paper, our main purpose is to demonstrate a new box counting method called 'pixel range calculation (PRC) method' to carry out image analysis to discover its smoothness or roughness. In the proposed method, we have mainly focused on the pixel intensity and their ranges so that it will give the appropriate gray-level count covered by each box. By getting the appropriate box count, we can measure the level of roughness or smoothness of various images. Lena and Baboon image of 64 x 64 pixels and then images with resolution of 256 x 256 pixels of Female, Areal, Moon surface, House, Tree, Female2 and Jelly Bean image have been taken for carrying out the experiment. The experiment was conducted using some of the existing methods available in the literature and the proposed method on the images mentioned, to verify the applicability and accuracy of the proposed method. The result of the experiment based on the box count shows that the proposed method is giving better result in terms of efficiency and accuracy as compared to other techniques.
C1 [Ranganath, Abadhan; Senapati, Manas Ranjan; Sahu, Pradip Kumar] Veer Surendra Sai Univ Technol, Dept Informat Technol, Burla, Sambalpur, India.
C3 Veer Surendra Sai University of Technology
RP Senapati, MR (corresponding author), Veer Surendra Sai Univ Technol, Dept Informat Technol, Burla, Sambalpur, India.
EM aranganath_phdit@vssut.ac.in; manassena@gmail.com; pksahu_it@vssut.ac.in
RI Ranganath, Abadhan/HKO-6680-2023
OI Ranganath, Abadhan/0000-0002-9556-5084
FU Veer Surendra Sai University of Technology, India
FX The authors acknowledge the support given by Veer Surendra Sai
   University of Technology, India, under TEQIP-III.
CR Amirolad A, 2016, VISUAL COMPUT, V32, P1633, DOI 10.1007/s00371-016-1220-5
   Barnsley M.F, 1988, The Science of Fractal Images
   Bisoi AK, 2001, PATTERN RECOGN LETT, V22, P631, DOI 10.1016/S0167-8655(00)00132-X
   Caballero D, 2018, J FOOD ENG, V227, P1, DOI 10.1016/j.jfoodeng.2018.02.005
   CHAUDHURI BB, 1995, IEEE T PATTERN ANAL, V17, P72, DOI 10.1109/34.368149
   Dash S, 2021, EVOL INTELL, V14, P217, DOI 10.1007/s12065-018-0164-2
   Dash S, 2018, EGYPT INFORM J, V19, P133, DOI 10.1016/j.eij.2018.01.003
   Dirnberger A, 2018, IEEE T PLASMA SCI, V46, P2550, DOI 10.1109/TPS.2017.2778710
   Dong YS, 2018, VISUAL COMPUT, V34, P1315, DOI 10.1007/s00371-017-1415-4
   Faraji MR, 2016, NEUROCOMPUTING, V199, P16, DOI 10.1016/j.neucom.2016.01.094
   Faraji MR, 2014, IEEE SIGNAL PROC LET, V21, P1457, DOI 10.1109/LSP.2014.2343213
   GAGNEPAIN JJ, 1986, WEAR, V109, P119, DOI 10.1016/0043-1648(86)90257-7
   Ghazel M, 2003, IEEE T IMAGE PROCESS, V12, P1560, DOI 10.1109/TIP.2003.818038
   Harrington S, 1987, COMPUTER GRAPHICS PR, V2, P109
   Ivanovici M, 2011, IEEE T IMAGE PROCESS, V20, P227, DOI 10.1109/TIP.2010.2059032
   KELLER JM, 1989, COMPUT VISION GRAPH, V45, P150, DOI 10.1016/0734-189X(89)90130-8
   Khmag A, 2018, VISUAL COMPUT, V34, P575, DOI 10.1007/s00371-017-1362-0
   Khoury M, 2010, IEEE T VIS COMPUT GR, V16, P1198, DOI 10.1109/TVCG.2010.182
   Li J, 1999, IEEE T IMAGE PROCESS, V8, P868, DOI 10.1109/83.766863
   Lin KH, 2001, IEE P-VIS IMAGE SIGN, V148, P413, DOI 10.1049/ip-vis:20010709
   Liu CX, 2017, VISUAL COMPUT, V33, P769, DOI 10.1007/s00371-017-1380-y
   Liu L, 2019, INT J COMPUT VISION, V127, P74, DOI 10.1007/s11263-018-1125-z
   Liu SC, 1997, IEEE T IMAGE PROCESS, V6, P1176, DOI 10.1109/83.605414
   Mandelbrot B.B., 1982, The fractal geometry of nature
   Peitgen H. O., 1992, CHAOS FRACTALS NEW F
   PENTLAND AP, 1986, ARTIF INTELL, V29, P147, DOI 10.1016/0004-3702(86)90017-2
   Quan YH, 2017, COMPUT VIS IMAGE UND, V165, P85, DOI 10.1016/j.cviu.2017.10.008
   Quan YH, 2014, PROC CVPR IEEE, P160, DOI 10.1109/CVPR.2014.28
   Quan YH, 2014, IMAGE VISION COMPUT, V32, P250, DOI 10.1016/j.imavis.2014.02.004
   Ranganath A, 2017, IEEE INT ADV COMPUT, P678, DOI [10.1109/IACC.2017.0142, 10.1109/IACC.2017.133]
   SARKAR N, 1994, IEEE T SYST MAN CYB, V24, P115, DOI 10.1109/21.259692
   Senapati MR, 2013, ARTIF INTELL REV, V39, P151, DOI 10.1007/s10462-011-9263-5
   Voss Richard F., 1991, SCALING PHENOMENA DI, P1
   Xu Y, 2015, IEEE T IMAGE PROCESS, V24, P2098, DOI 10.1109/TIP.2015.2413298
   Xu Y, 2011, IEEE I CONF COMP VIS, P1219, DOI 10.1109/ICCV.2011.6126372
   Yang LN, 2015, IEEE ACM T COMPUT BI, V12, P348, DOI 10.1109/TCBB.2014.2363480
NR 36
TC 13
Z9 13
U1 2
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2021
VL 37
IS 3
BP 635
EP 650
DI 10.1007/s00371-020-01829-1
EA MAR 2020
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QY5QT
UT WOS:000561626400002
DA 2024-07-18
ER

PT J
AU Zhang, DJ
   Zhang, Z
   Zou, L
   Xie, ZY
   He, FZ
   Wu, YQ
   Tu, ZG
AF Zhang, Dejun
   Zhang, Zhao
   Zou, Lu
   Xie, Zhuyang
   He, Fazhi
   Wu, Yiqi
   Tu, Zhigang
TI Part-based visual tracking with spatially regularized correlation
   filters
SO VISUAL COMPUTER
LA English
DT Article
DE Correlation filter tracking; Discriminative Correlation Filter;
   Part-based tracking; Spatially regularized filter
ID ROBUST OBJECT TRACKING; ADAPTIVE FUSION
AB Discriminative Correlation Filters (DCFs) have demonstrated excellent performance in visual object tracking. These methods utilize a periodic assumption of the training samples to efficiently learn a classifier on image patches; unfortunately, this also introduces unwanted boundary effects. Recently, Spatially Regularized Discriminative Correlation Filters (SRDCFs) were proposed to resolve this issue by introducing penalization weights to the filter coefficients, thereby efficiently reducing boundary effects by assigning higher weights to the background. However, due to the variable target scale, defining the penalization ratio is non trivial; thus, it is possible to penalize the image content while also penalizing the background. In this paper, we investigate SRDCFs and present a novel and efficient part-based tracking framework by exploiting multiple SRDCFs. Compared with existing trackers, the proposed method has several advantages. (1) We define multiple correlation filters to extract features within the range of the object, thereby alleviating the boundary effect problem and avoiding penalization of the target content. (2) Through the combination of cyclic object shifts with penalized filters to build part-based object trackers, there is no need to divide training samples into parts. (3) Comprehensive comparisons demonstrate that our approach achieves a performance equivalent to that of the baseline SRDCF tracker on a set of benchmark datasets, namely, OTB2013, OTB2015 and VOT2017. In addition, compared with other state-of-the-art trackers, our approach demonstrates superior performance.
C1 [Zhang, Dejun] China Univ Geosci, Fac Informat Engn, Wuhan 430074, Peoples R China.
   [Zhang, Zhao; Zou, Lu; Xie, Zhuyang] Sichuan Agr Univ, Coll Informat & Engn, Yaan 625014, Peoples R China.
   [He, Fazhi] Wuhan Univ, Sch Comp Sci & Technol, Wuhan 430072, Peoples R China.
   [Wu, Yiqi] China Univ Geosci, Coll Comp Sci, Wuhan 430074, Peoples R China.
   [Tu, Zhigang] Nanyang Technol Univ, Sch Elect & Elect Engn, 50 Nanyang Ave, Singapore 639798, Singapore.
C3 China University of Geosciences; Sichuan Agricultural University; Wuhan
   University; China University of Geosciences; Nanyang Technological
   University
RP Zhang, DJ (corresponding author), China Univ Geosci, Fac Informat Engn, Wuhan 430074, Peoples R China.
EM zhangdejun@cug.edu.cn
RI He, Fazhi/Q-3691-2018; Tu, Zhigang/AAG-2255-2020; zou, lu/IQR-7160-2023
OI Wu, Yiqi/0000-0003-3148-8101; Zhang, Dejun/0000-0001-9129-534X; Tu,
   Zhigang/0000-0001-5003-2260
FU National Natural Science Foundation of China [61702350, 61472289]; Open
   Project Program of State Key Laboratory of Digital Manufacturing
   Equipment and Technology at HUST [DMETKF2017016]
FX This study was funded by the National Natural Science Foundation of
   China (Grant nos. 61702350 and 61472289) and the Open Project Program of
   State Key Laboratory of Digital Manufacturing Equipment and Technology
   at HUST (Grant no. DMETKF2017016).
CR Bai B, 2018, NEUROCOMPUTING, V286, P109, DOI 10.1016/j.neucom.2018.01.068
   Bibi A, 2016, LECT NOTES COMPUT SC, V9910, P419, DOI 10.1007/978-3-319-46466-4_25
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Cehovin L, 2013, IEEE T PATTERN ANAL, V35, P941, DOI 10.1109/TPAMI.2012.145
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Dinh TB, 2011, PROC CVPR IEEE, P1177, DOI 10.1109/CVPR.2011.5995733
   Fan H, 2014, SCI WORLD J, DOI 10.1155/2014/402185
   Galoogahi HK, 2015, PROC CVPR IEEE, P4630, DOI 10.1109/CVPR.2015.7299094
   Gao J, 2010, CHIN CONTR CONF, P3188
   Godec M, 2013, COMPUT VIS IMAGE UND, V117, P1245, DOI 10.1016/j.cviu.2012.11.005
   Guan H, 2018, VISUAL COMPUT, V34, P1701, DOI 10.1007/s00371-017-1445-y
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hu X., 2017, ARXIV170600140
   Hwang JP, 2015, INT J CONTROL AUTOM, V13, P443, DOI 10.1007/s12555-013-0483-0
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kristan M, 2017, IEEE INT CONF COMP V, P1949, DOI 10.1109/ICCVW.2017.230
   Kwon J, 2013, IEEE T PATTERN ANAL, V35, P2427, DOI 10.1109/TPAMI.2013.32
   Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821
   Li Y, 2015, PROC CVPR IEEE, P353, DOI 10.1109/CVPR.2015.7298632
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Li ZY, 2015, VISUAL COMPUT, V31, P1633, DOI 10.1007/s00371-014-1044-0
   Li ZY, 2015, VISUAL COMPUT, V31, P1319, DOI 10.1007/s00371-014-1014-6
   Liu BY, 2013, IEEE T PATTERN ANAL, V35, P2968, DOI 10.1109/TPAMI.2012.215
   Liu S, 2016, PROC CVPR IEEE, P4312, DOI 10.1109/CVPR.2016.467
   Liu T, 2015, PROC CVPR IEEE, P4902, DOI 10.1109/CVPR.2015.7299124
   Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177
   Matthews I, 2004, IEEE T PATTERN ANAL, V26, P810, DOI 10.1109/TPAMI.2004.16
   Mbelwa JT, 2019, VISUAL COMPUT, V35, P371, DOI 10.1007/s00371-018-1470-5
   Quan W, 2015, VISUAL COMPUT, V31, P1307, DOI 10.1007/s00371-014-1012-8
   Quan W, 2014, VISUAL COMPUT, V30, P351, DOI 10.1007/s00371-013-0860-y
   Tu ZG, 2018, PATTERN RECOGN, V79, P32, DOI 10.1016/j.patcog.2018.01.020
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   van de Weijer J, 2009, IEEE T IMAGE PROCESS, V18, P1512, DOI 10.1109/TIP.2009.2019809
   Wang Q., 2017, arXiv preprint arXiv:1704.04057
   Wang Z, 2016, VISUAL COMPUT, V32, P307, DOI 10.1007/s00371-015-1067-1
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Wu YX, 2015, VISUAL COMPUT, V31, P471, DOI 10.1007/s00371-014-0942-5
   Yang M, 2007, IEEE I CONF COMP VIS, P897
   Zhan J, 2015, VISUAL COMPUT, V31, P575, DOI 10.1007/s00371-014-0984-8
   Zhang HY, 2018, VISUAL COMPUT, V34, P41, DOI 10.1007/s00371-016-1310-4
   Zhang T., 2012, LOW RANK SPARSE LEAR
   Zhang TZ, 2014, PROC CVPR IEEE, P1258, DOI 10.1109/CVPR.2014.164
   Zhang TZ, 2012, PROC CVPR IEEE, P2042, DOI 10.1109/CVPR.2012.6247908
   Zhao LJ, 2017, VISUAL COMPUT, V33, P1169, DOI 10.1007/s00371-016-1279-z
   Zhong BN, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0161808
   Zhong B, 2014, PATTERN RECOGN, V47, P1395, DOI 10.1016/j.patcog.2013.10.002
   Zhong BN, 2014, NEUROCOMPUTING, V123, P344, DOI 10.1016/j.neucom.2013.06.044
   Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882
NR 52
TC 14
Z9 15
U1 1
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2020
VL 36
IS 3
BP 509
EP 527
DI 10.1007/s00371-019-01634-5
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KL4EO
UT WOS:000513378500006
DA 2024-07-18
ER

PT J
AU Yildiz, ZC
   Oztireli, AC
   Capin, T
AF Yildiz, Zeynep Cipiloglu
   Oztireli, A. Cengiz
   Capin, Tolga
TI A machine learning framework for full-reference 3D shape quality
   assessment
SO VISUAL COMPUTER
LA English
DT Article
DE Visual quality assessment; Mesh quality; Perceptual computer graphics;
   Crowdsourcing; Metric learning
ID VISUAL QUALITY; MESH
AB To decide whether the perceived quality of a mesh is influenced by a certain modification such as compression or simplification, a metric for estimating the visual quality of 3D meshes is required. Today, machine learning and deep learning techniques are getting increasingly popular since they present efficient solutions to many complex problems. However, these techniques are not much utilized in the field of 3D shape perception. We propose a novel machine learning-based approach for evaluating the visual quality of 3D static meshes. The novelty of our study lies in incorporating crowdsourcing in a machine learning framework for visual quality evaluation. We deliberate that this is an elegant way since modeling human visual system processes is a tedious task and requires tuning many parameters. We employ crowdsourcing methodology for collecting data of quality evaluations and metric learning for drawing the best parameters that well correlate with the human perception. Experimental validation of the proposed metric reveals a promising correlation between the metric output and human perception. Results of our crowdsourcing experiments are publicly available for the community.
C1 [Yildiz, Zeynep Cipiloglu] Celal Bayar Univ, Dept Comp Engn, Manisa, Turkey.
   [Oztireli, A. Cengiz] Swiss Fed Inst Technol, Comp Graph Lab, Zurich, Switzerland.
   [Capin, Tolga] TED Univ, Dept Comp Engn, TR-06420 Kolej Ankara, Turkey.
C3 Celal Bayar University; Swiss Federal Institutes of Technology Domain;
   ETH Zurich; Ted University
RP Yildiz, ZC (corresponding author), Celal Bayar Univ, Dept Comp Engn, Manisa, Turkey.
EM zeynep.cipiloglu@cbu.edu.tr; cengizo@inf.ethz.ch;
   tolga.capin@tedu.edu.tr
RI Yildiz, Zeynep Cipiloglu/AAF-6305-2020; Yıldız, Zeynep/KBA-3063-2024;
   Yildiz, Zeynep Cipiloglu/T-3389-2017
OI Yildiz, Zeynep Cipiloglu/0000-0003-4129-591X; Yildiz, Zeynep
   Cipiloglu/0000-0003-4129-591X
CR Abouelaziz I., 2018, MULTIMED TOOLS APPL, P1
   Abouelaziz I, 2016, LECT NOTES COMPUT SC, V9680, P369, DOI 10.1007/978-3-319-33618-3_37
   Alliez P, 2003, ACM T GRAPHIC, V22, P485, DOI 10.1145/882262.882296
   [Anonymous], 2003, S GEOM PROC
   [Anonymous], 2017, Electron Imaging
   Bulbul A, 2011, IEEE SIGNAL PROC MAG, V28, P80, DOI 10.1109/MSP.2011.942466
   Chetouani A, 2017, Electronic Imaging, V29, P4
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Cignoni P, 2008, ERCIM NEWS, P45
   Corsini M, 2013, COMPUT GRAPH FORUM, V32, P101, DOI 10.1111/cgf.12001
   Corsini M, 2007, IEEE T MULTIMEDIA, V9, P247, DOI 10.1109/TMM.2006.886261
   DALY S, 1992, P SOC PHOTO-OPT INS, V1666, P2, DOI 10.1117/12.135952
   Dong L, 2015, IEEE T MULTIMEDIA, V17, P2174, DOI 10.1109/TMM.2015.2484221
   Dongoran ESS, 2015, 2015 3rd International Conference on Information and Communication Technology (ICoICT), P59, DOI 10.1109/ICoICT.2015.7231397
   Garces Elena, 2014, ACM Transactions on Graphics, V33, DOI 10.1145/2601097.2601131
   Gingold Y, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2231816.2231817
   Heer J, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P203
   Kai Wang, 2010, Proceedings of the Shape Modeling International (SMI 2010), P231, DOI 10.1109/SMI.2010.33
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Kleiman Y, 2016, VISUAL COMPUT, V32, P1045, DOI 10.1007/s00371-016-1266-4
   KOENDERINK JJ, 1992, IMAGE VISION COMPUT, V10, P557, DOI 10.1016/0262-8856(92)90076-F
   Kulis B, 2013, FOUND TRENDS MACH LE, V5, P287, DOI 10.1561/2200000019
   Kundu D, 2017, IEEE T IMAGE PROCESS, V26, P2957, DOI 10.1109/TIP.2017.2685941
   Lavoue G., 2006, OPTICS PHOTONICS
   Lavoue G., 2015, Visual Signal Quality Assessment, P243, DOI [10.1007/978-3-319-10368-69, DOI 10.1007/978-3-319-10368-69]
   Lavoué G, 2013, IEEE SYS MAN CYBERN, P3271, DOI 10.1109/SMC.2013.557
   Lavoué G, 2011, COMPUT GRAPH FORUM, V30, P1427, DOI 10.1111/j.1467-8659.2011.02017.x
   Lavoué G, 2009, ACM T APPL PERCEPT, V5, DOI 10.1145/1462048.1462052
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu TQ, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766898
   Lun ZL, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766929
   Maglo A, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2693443
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Ramanarayanan G., 2007, ACM SIGGRAPH 2007 SI
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Secord A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2019627.2019628
   Taniguchi S, 2014, IEEE INT ULTRA SYM, P65, DOI 10.1109/ULTSYM.2014.0017
   Torkhani F., 2014, Machine Graphics Vision, V23, P59
   Torkhani F, 2015, SIGNAL PROCESS-IMAGE, V31, P185, DOI 10.1016/j.image.2014.12.008
   Vása L, 2012, COMPUT GRAPH FORUM, V31, P1715, DOI 10.1111/j.1467-8659.2012.03176.x
   Wang K, 2012, COMPUT GRAPH-UK, V36, P808, DOI 10.1016/j.cag.2012.06.004
   Yildiz ZC, 2017, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-016-0157-y
   Yumer ME, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766908
NR 45
TC 12
Z9 12
U1 2
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2020
VL 36
IS 1
BP 127
EP 139
DI 10.1007/s00371-018-1592-9
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ3OJ
UT WOS:000511966800011
DA 2024-07-18
ER

PT J
AU Oshita, M
   Inao, T
   Ineno, S
   Mukai, T
   Kuriyama, S
AF Oshita, Masaki
   Inao, Takumi
   Ineno, Shunsuke
   Mukai, Tomohiko
   Kuriyama, Shigeru
TI Development and evaluation of a self-training system for tennis shots
   with motion feature assessment and visualization
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 17th International Conference on Cyberworlds (CW)
CY OCT 03-05, 2018
CL Fraunhofer Singapore, Singapore, SINGAPORE
SP Nanyang Technol Univ Singapore, ACM SIGGRAPH, Eurograph Assoc, Int Federat Informat Proc, ACM
HO Fraunhofer Singapore
DE Training system; Sports form; Motion feature; Visualization; Motion
   capture
AB In this paper, we propose a prototype of a self-training system for tennis forehand shots that allows trainees to practice their motion forms by themselves. Our system includes a motion capture device to record the trainee's motion, and the system visualizes the differences between the features of the trainee's motion and the correct motion performed by an expert. The system enables trainees to understand the errors in their motion and how to reduce or eliminate them. In this study, we classify the motion features and corresponding visualization methods based on the one-dimensional spatial, rotational, and temporal features of key poses. We also develop a statistical model for the motion features so that the system can assess and prioritize all features of a trainee's motion. Related features are simultaneously visualized by analyzing their correlations. We describe the process of defining the motion features for the tennis forehand shot of an expert. We evaluated our prototype through several user experiments and demonstrated its feasibility as a self-training system.
C1 [Oshita, Masaki] Kyushu Inst Technol, Iizuka, Fukuoka, Japan.
   [Inao, Takumi; Ineno, Shunsuke] Kyushu Inst Technol, Grad Sch, Iizuka, Fukuoka, Japan.
   [Mukai, Tomohiko] Tokyo Metropolitan Univ, Dept Ind Art, Hino, Tokyo, Japan.
   [Kuriyama, Shigeru] Toyohashi Univ Technol, Toyohashi, Aichi, Japan.
C3 Kyushu Institute of Technology; Kyushu Institute of Technology; Tokyo
   Metropolitan University; Toyohashi University of Technology
RP Oshita, M (corresponding author), Kyushu Inst Technol, Iizuka, Fukuoka, Japan.
EM oshita@ces.kyutech.ac.jp
OI Oshita, Masaki/0000-0001-9844-7713
FU Japan Society for the Promotion of Science [15H02704]; Grants-in-Aid for
   Scientific Research [15H02704] Funding Source: KAKEN
FX This work was supported in part by a Grant-in-Aid for Scientific
   Research (No. 15H02704) from the Japan Society for the Promotion of
   Science.
CR Anderson F., 2013, P 26 ANN ACM S US IN, P311, DOI [DOI 10.1145/2501988.2502045, 10.1145/2501988.2502045]
   [Anonymous], 1998, Proc. SIGGRAPH, DOI 10.1145/280814.280820
   Assa J, 2005, ACM T GRAPHIC, V24, P667, DOI 10.1145/1073204.1073246
   Baek S, 2003, VISUAL COMPUT, V19, P222, DOI 10.1007/s00371-003-0194-2
   Chan JCP, 2011, IEEE T LEARN TECHNOL, V4, P187, DOI 10.1109/TLT.2010.27
   Guay M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508397
   Khan A, 2015, PROCEEDINGS OF THE 2015 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP 2015), P1155, DOI 10.1145/2750858.2807534
   Kyan M, 2015, ACM T INTEL SYST TEC, V6, DOI 10.1145/2735951
   Laraba S, 2016, COMPUT ANIMAT VIRT W, V27, P321, DOI 10.1002/cav.1715
   Lau M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618517
   Min JY, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640452
   Natural Point, 2013, OPT MOT
   Neff Michael., 2009, P ACM SIGGRAPHEUROGR, P103
   Oshita M, 2018, 2018 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P82, DOI 10.1109/CW.2018.00025
   SHAPIRO SS, 1965, BIOMETRIKA, V52, P591, DOI 10.1093/biomet/52.3-4.591
   Sok KwangWon., 2010, P 2010 ACM SIGGRAPHE, P11
   Tang R, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P4123, DOI 10.1145/2702123.2702401
   Tao L, 2016, COMPUT VIS IMAGE UND, V148, P136, DOI 10.1016/j.cviu.2015.11.016
NR 18
TC 9
Z9 9
U1 1
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2019
VL 35
IS 11
SI SI
BP 1517
EP 1529
DI 10.1007/s00371-019-01662-1
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA JD5IZ
UT WOS:000490018000003
DA 2024-07-18
ER

PT J
AU Feng, Q
   Fei, H
   Wang, WC
AF Feng, Qiu
   Fei, Hou
   Wang Wencheng
TI Blind image deblurring with reinforced use of edges
SO VISUAL COMPUTER
LA English
DT Article
DE Image deblurring; Ringing artifacts; Edge detection
AB Blind image deblurring tries to restore a blurred image to a clear image without the blurring kernel known in advance, which is widely required in applications such as computer vision and medical image processing. With regard to this, the key issues here are to accurately estimate the blurring kernel for deconvolution of a blurred image, and avoid the ringing artifacts in the restored image, which are both related to high-quality detection of edge information in the blurred image. Though much endeavor has been made, it is still difficult to extract edge information well in blurred images and lacks investigation how edge information causes ringing artifacts. In this paper, we make a study on this and develop novel measures to optimize edge extraction and determine suitable width and weights for the extracted edges for reinforcing their use in deblurring, by which image deblurring can be improved with ringing artifacts considerably suppressed. Experimental results demonstrate our improvements over the existing methods.
C1 [Feng, Qiu; Fei, Hou; Wang Wencheng] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100190, Peoples R China.
   [Feng, Qiu; Fei, Hou; Wang Wencheng] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS
RP Wang, WC (corresponding author), Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100190, Peoples R China.; Wang, WC (corresponding author), Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
EM qiuf@ios.ac.cn; houfei@ios.ac.cn; whn@ios.ac.cn
RI Wang, Wencheng/A-3828-2009
FU National Natural Science Foundation of China [61661146002, 61872347];
   Special Plan for the Development of Distinguished Young Scientists of
   ISCAS [Y8RC535018]
FX This project was partially supported by the National Natural Science
   Foundation of China (Grant Nos. 61661146002, 61872347) and Special Plan
   for the Development of Distinguished Young Scientists of ISCAS
   (Y8RC535018).
CR Almeida G., 2015, J PHYS C SERIES, V630
   [Anonymous], 2006, CVPR
   [Anonymous], 2007, P IEEE C COMP VIS PA
   Chan SH, 2011, IEEE T IMAGE PROCESS, V20, P3097, DOI 10.1109/TIP.2011.2158229
   [陈蕾 Chen Lei], 2015, [计算机学报, Chinese Journal of Computers], V38, P1357
   Cho TS, 2010, PROC CVPR IEEE, P169, DOI 10.1109/CVPR.2010.5540214
   Dollár P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715
   Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956
   Gupta A, 2010, LECT NOTES COMPUT SC, V6311, P171, DOI 10.1007/978-3-642-15549-9_13
   Haralick R. M., 1992, IEEE ROB AUTOM MAG, V18, P121
   He QH, 2007, AEU-INT J ELECTRON C, V61, P546, DOI 10.1016/j.aeue.2006.09.008
   Hu Z, 2014, PROC CVPR IEEE, P3382, DOI 10.1109/CVPR.2014.432
   Javaran TA, 2017, COMPUT VIS IMAGE UND, V154, P16, DOI 10.1016/j.cviu.2016.09.013
   Jin-Yu Z, 2009, 2009 INT C IM AN SIG, P31, DOI DOI 10.1109/IASP.2009.5054605
   Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521
   Kupyn O., 2017, Deblurgan: Blind motion deblurring using conditional adversarial networks
   Levin A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239521
   Levin A, 2007, IEEE T PATTERN ANAL, V29, P1647, DOI 10.1109/TPAMI.2007.1106
   [刘鹏飞 Liu Pengfei], 2015, [电子学报, Acta Electronica Sinica], V43, P2001
   LUCY LB, 1974, ASTRON J, V79, P745, DOI 10.1086/111605
   Mokhtarian F, 1998, IEEE T PATTERN ANAL, V20, P1376, DOI 10.1109/34.735812
   OZKAN MK, 1994, IEEE T IMAGE PROCESS, V3, P450, DOI 10.1109/83.298398
   Pan JS, 2013, SIGNAL PROCESS-IMAGE, V28, P1156, DOI 10.1016/j.image.2013.05.001
   Ramponi G., 1995, Proceedings. International Conference on Image Processing (Cat. No.95CB35819), P151, DOI 10.1109/ICIP.1995.529062
   RICHARDSON WH, 1972, J OPT SOC AM, V62, P55, DOI 10.1364/JOSA.62.000055
   ROSENFELD A, 1981, IEEE T PATTERN ANAL, V3, P101, DOI 10.1109/TPAMI.1981.4767056
   Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672
   Shi JP, 2014, PROC CVPR IEEE, P2965, DOI 10.1109/CVPR.2014.379
   Tikhonov A. N., 1978, MATH COMPUT, V32, P491, DOI [10.2307/2006360, DOI 10.2307/2006360]
   Wang Xin, 2008, Computer Engineering and Applications, V44, P89
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Welk M, 2005, LECT NOTES COMPUT SC, V3663, P485
   Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157
   Xu Li, 2014, P ANN C NEUR INF PRO, P1790
   Yang H, 2014, IMAGING SCI J, V62, P178, DOI 10.1179/1743131X12Y.0000000040
   Yuan L, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360673
   Zitnick CL, 2010, LECT NOTES COMPUT SC, V6312, P170, DOI 10.1007/978-3-642-15552-9_13
NR 37
TC 10
Z9 10
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 1081
EP 1090
DI 10.1007/s00371-019-01697-4
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200025
OA Bronze
DA 2024-07-18
ER

PT J
AU Zhang, Y
   Lai, YK
   Zhang, FL
AF Zhang, Yun
   Lai, Yu-Kun
   Zhang, Fang-Lue
TI Stereoscopic image stitching with rectangular boundaries
SO VISUAL COMPUTER
LA English
DT Article
DE Stereoscopic image stitching; Rectangular boundaries; Global
   optimization; Rectangling; Disparity consistency
AB This paper proposes a novel algorithm for stereoscopic image stitching, which aims to produce stereoscopic panoramas with rectangular boundaries. As a result, it provides wider field of view and better viewing experience for users. To achieve this, we formulate stereoscopic image stitching and boundary rectangling in a global optimization framework that simultaneously handles feature alignment, disparity consistency and boundary regularity. Given two (or more) stereoscopic images with overlapping content, each containing two views (for left and right eyes), we represent each view using a mesh and our algorithm contains three main steps: We first perform a global optimization to stitch all the left views and right views simultaneously, which ensures feature alignment and disparity consistency. Then, with the optimized vertices in each view, we extract the irregular boundary in the stereoscopic panorama, by performing polygon Boolean operations in left and right views, and construct the rectangular boundary constraints. Finally, through a global energy optimization, we warp left and right views according to feature alignment, disparity consistency and rectangular boundary constraints. To show the effectiveness of our method, we further extend our method to disparity adjustment and stereoscopic stitching with large horizon. Experimental results show that our method can produce visually pleasing stereoscopic panoramas without noticeable distortion or visual fatigue, thus resulting in satisfactory 3D viewing experience.
C1 [Zhang, Yun] Commun Univ Zhejiang, Inst Zhejiang Radio & TV Technol, Hangzhou 310018, Zhejiang, Peoples R China.
   [Lai, Yu-Kun] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF24 3AA, S Glam, Wales.
   [Zhang, Fang-Lue] Victoria Univ Wellington, Sch Engn & Comp Sci, Wellington 6012, New Zealand.
C3 Communication University of Zhejiang; Cardiff University; Victoria
   University Wellington
RP Zhang, Y (corresponding author), Commun Univ Zhejiang, Inst Zhejiang Radio & TV Technol, Hangzhou 310018, Zhejiang, Peoples R China.
EM zhangyun_zju@zju.edu.cn; LaiY4@cardiff.ac.uk;
   fanglue.zhang@ecs.vuw.ac.nz
RI Lai, Yu-Kun/D-2343-2010
FU National Natural Science Foundation of China [61602402]; Zhejiang
   Province Public Welfare Technology Application Research [LGG19F020001];
   Royal Society; IES [\R1\180126]
FX This work was supported by the National Natural Science Foundation of
   China (Grant No. 61602402), Zhejiang Province Public Welfare Technology
   Application Research (Grant No. LGG19F020001), and the Royal Society
   (Grant No. IES\R1\180126).
CR Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   Chang CH, 2014, PROC CVPR IEEE, P3254, DOI 10.1109/CVPR.2014.422
   Chang CH, 2011, IEEE T MULTIMEDIA, V13, P589, DOI 10.1109/TMM.2011.2116775
   Chen YS, 2016, LECT NOTES COMPUT SC, V9909, P186, DOI 10.1007/978-3-319-46454-1_12
   Du SP, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508387
   Du SP, 2013, IEEE T VIS COMPUT GR, V19, P1288, DOI 10.1109/TVCG.2013.14
   Gao JH, 2011, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2011.5995433
   Guo H, 2016, IEEE T IMAGE PROCESS, V25, P5491, DOI 10.1109/TIP.2016.2607419
   HE K, 2010, ACM T GRAPHIC, V32
   Lang M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778812
   Lee S, 2014, VISUAL COMPUT, V30, P455, DOI 10.1007/s00371-013-0868-3
   Lin CC, 2015, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2015.7298719
   Lin KM, 2016, LECT NOTES COMPUT SC, V9907, P370, DOI 10.1007/978-3-319-46487-9_23
   Lin KM, 2016, COMPUT GRAPH FORUM, V35, P479, DOI 10.1111/cgf.12848
   Liu S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461995
   Liu Y., 2015, COMPUT VIS MEDIA, V1, P119
   Martínez F, 2009, COMPUT GEOSCI-UK, V35, P1177, DOI 10.1016/j.cageo.2008.08.009
   Mu TJ, 2015, VISUAL COMPUT, V31, P675, DOI 10.1007/s00371-014-0994-6
   Mu TJ, 2014, VISUAL COMPUT, V30, P833, DOI 10.1007/s00371-014-0961-2
   Szeliski R, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000009
   Tang YL, 2016, VISUAL COMPUT, V32, P111, DOI 10.1007/s00371-014-1059-6
   Tong RF, 2013, IEEE T VIS COMPUT GR, V19, P1375, DOI 10.1109/TVCG.2012.319
   von Gioi RG, 2010, IEEE T PATTERN ANAL, V32, P722, DOI 10.1109/TPAMI.2008.300
   Wang HQ, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1812, DOI 10.1109/ICASSP.2018.8461411
   Wang M., 2016, COMPUT VIS MEDIA, V1, P3, DOI DOI 10.1007/s41095-016-0037-5
   Yan WQ, 2017, IEEE T CIRC SYST VID, V27, P1934, DOI 10.1109/TCSVT.2016.2564838
   Yan WQ, 2017, MULTIMED TOOLS APPL, V76, P10465, DOI 10.1007/s11042-016-3442-y
   Zaragoza J, 2014, IEEE T PATTERN ANAL, V36, P1285, DOI 10.1109/TPAMI.2013.247
   Zhang F, 2015, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2015.7298811
   Zhang F, 2014, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2014.423
   Zhang Y., 2018, arXiv e-prints
   Zhu Z, 2018, IEEE T IMAGE PROCESS, V27, P2952, DOI 10.1109/TIP.2018.2808766
NR 32
TC 8
Z9 8
U1 2
U2 27
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 823
EP 835
DI 10.1007/s00371-019-01694-7
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200005
OA Bronze, Green Accepted
DA 2024-07-18
ER

PT J
AU Zhu, Z
   Toyoura, M
   Go, K
   Fujishiro, I
   Kashiwagi, K
   Mao, XY
AF Zhu, Zhenyang
   Toyoura, Masahiro
   Go, Kentaro
   Fujishiro, Issei
   Kashiwagi, Kenji
   Mao, Xiaoyang
TI Processing images for red-green dichromats compensation via naturalness
   and information-preservation considered recoloring
SO VISUAL COMPUTER
LA English
DT Article
DE Red-green dichromacy; Recoloring; Naturalness; Contrast; Subjective
   evaluation
ID COLOR-VISION; SIMULATION
AB Color vision deficiency (CVD) is caused by anomalies in the cone cells of the human retina. It affects approximately 200 million individuals throughout the world. Although previous studies have proposed compensation methods, contrast and naturalness preservation have not been adequately and simultaneously addressed in the state-of-the-art studies. This paper focuses on red-green dichromats' compensation and proposes a recoloring algorithm that combines contrast enhancement and naturalness preservation in a unified optimization model. In this implementation, representative color extraction and edit propagation methods are introduced to maintain global and local information in the recolored image. The quantitative evaluation results showed that the proposed method is competitive with state-of-the-art methods. A subjective experiment was also conducted and the evaluation results revealed that the proposed method obtained the best scores in preserving both naturalness and information for individuals with severe red-green CVD.
C1 [Zhu, Zhenyang] Univ Yamanashi, Grad Sch Engn, Kofu, Yamanashi 4008511, Japan.
   [Toyoura, Masahiro; Go, Kentaro; Mao, Xiaoyang] Univ Yamanashi, Dept Comp Sci & Engn, Kofu, Yamanashi 4008511, Japan.
   [Fujishiro, Issei] Keio Univ, Dept Informat & Comp Sci, Yokohama, Kanagawa 2238522, Japan.
   [Kashiwagi, Kenji] Univ Yamanashi, Dept Ophthalmol, Chuo 4093898, Japan.
   [Mao, Xiaoyang] Hangzhou Dianzi Univ, Sch Comp Sci, 1156 2nd St, Hangzhou 310018, Zhejiang, Peoples R China.
C3 University of Yamanashi; University of Yamanashi; Keio University;
   University of Yamanashi; Hangzhou Dianzi University
RP Mao, XY (corresponding author), Univ Yamanashi, Dept Comp Sci & Engn, Kofu, Yamanashi 4008511, Japan.; Mao, XY (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci, 1156 2nd St, Hangzhou 310018, Zhejiang, Peoples R China.
EM zhuyamanashi2016@gmail.com; mtoyoura@yamanashi.ac.jp;
   go@yamanashi.ac.jp; fuji@ics.keio.ac.jp; kenjik@yamanashi.ac.jp;
   mao@yamanashi.ac.jp
RI Zhu, Zhenyang/AAZ-6197-2021
OI Zhu, Zhenyang/0000-0003-1023-3193; Toyoura, Masahiro/0000-0002-5897-7573
FU JSPS [17H00738]; Grants-in-Aid for Scientific Research [17H00738]
   Funding Source: KAKEN
FX The authors declare that they have no known competing financial
   interests or personal relationships that could have appeared to
   influence the work reported in this paper. This work is supported by
   JSPS Grants-in-Aid for Scientific Research (Grant No. 17H00738).
CR [Anonymous], 1999, COLOR VISION GENES P
   Barrett R, 1994, TEMPLATES SOLUTION L, DOI DOI 10.1137/1.9781611971538
   Brettel H, 1997, J OPT SOC AM A, V14, P2647, DOI 10.1364/JOSAA.14.002647
   Chang HW, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766978
   Chen XW, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366151
   CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568
   Chua SH, 2015, ACM T COMPUT-HUM INT, V21, DOI 10.1145/2687923
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Gibson KB, 2013, IEEE T IMAGE PROCESS, V22, P3982, DOI 10.1109/TIP.2013.2265884
   Gooch AA, 2005, ACM T GRAPHIC, V24, P634, DOI 10.1145/1073204.1073241
   GRAHAM CH, 1959, P NATL ACAD SCI USA, V45, P96, DOI 10.1073/pnas.45.1.96
   Hassan MF, 2019, MULTIDIMENS SYST SIG
   Hassan MF, 2017, SIGNAL PROCESS-IMAGE, V57, P126, DOI 10.1016/j.image.2017.05.011
   Heckbert P., 1982, Computer Graphics, V16, P297, DOI 10.1145/965145.801294
   Huang HB, 2007, IEEE SIGNAL PROC LET, V14, P711, DOI 10.1109/LSP.2007.898333
   Huang JB, 2009, INT CONF ACOUST SPEE, P1161, DOI 10.1109/ICASSP.2009.4959795
   Huang LK, 2010, IEEE INT CON MULTI, P637, DOI 10.1109/ICME.2010.5582922
   Hung P., 2013, COLOUR CONVERSION ME
   Ichikawa A, 2004, IEEE SYS MAN CYBERN, P36
   Ishihara S., 1979, TESTS COLOUR BLINDNE
   Jefferson L, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1535
   Jefferson Luke., 2006, P 8 INT ACM SIGACCES, P40, DOI DOI 10.1145/1168987.1168996
   JUDD DB, 1948, J RES NAT BUR STAND, V41, P247, DOI 10.6028/jres.041.027
   JUDD DB, 1966, P NATL ACAD SCI USA, V55, P1313, DOI 10.1073/pnas.55.6.1313
   Kuhn GR, 2008, IEEE T VIS COMPUT GR, V14, P1747, DOI 10.1109/TVCG.2008.112
   Laccarino G., 2006, WWW, P919
   Langlotz T, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173964
   Lau C, 2011, IEEE I CONF COMP VIS, P1172, DOI 10.1109/ICCV.2011.6126366
   Li Y, 2010, COMPUT GRAPH FORUM, V29, P2049, DOI 10.1111/j.1467-8659.2010.01791.x
   Liu Chunxiao, 2016, Journal of Computer Aided Design & Computer Graphics, V28, P433
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Ma MD, 2012, WIREL NETW MOB COMMU, P1
   Machado GM, 2010, COMPUT GRAPH FORUM, V29, P933, DOI 10.1111/j.1467-8659.2009.01701.x
   Machado GM, 2009, IEEE T VIS COMPUT GR, V15, P1291, DOI 10.1109/TVCG.2009.113
   MEYER GW, 1988, IEEE COMPUT GRAPH, V8, P28, DOI 10.1109/38.7759
   Rasche K, 2005, COMPUT GRAPH FORUM, V24, P423, DOI 10.1111/j.1467-8659.2005.00867.x
   Rasche K, 2005, IEEE COMPUT GRAPH, V25, P22, DOI 10.1109/MCG.2005.54
   Rigden C., 1999, BRIT TELECOMMUN ENG, V17, P2
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Sajadi B, 2013, IEEE T VIS COMPUT GR, V19, P118, DOI 10.1109/TVCG.2012.93
   Shen WY, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925878
   Tanuwidjaja E, 2014, UBICOMP'14: PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P799, DOI 10.1145/2632048.2632091
   Wakita Ken., 2005, Assets '05: Proceedings of the 7th international ACM SIGACCESS conference on Computers and accessibility, P158, DOI [10.1145/1090785.1090815, DOI 10.1145/1090785.1090815]
NR 43
TC 15
Z9 15
U1 1
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 1053
EP 1066
DI 10.1007/s00371-019-01689-4
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200023
OA hybrid
DA 2024-07-18
ER

PT J
AU Zhang, J
   Wang, CB
   Qin, H
   Chen, Y
   Gao, Y
AF Zhang, Jian
   Wang, Chang-bo
   Qin, Hong
   Chen, Yi
   Gao, Yan
TI Procedural modeling of rivers from single image toward natural scene
   production
SO VISUAL COMPUTER
LA English
DT Article
DE Procedural modeling; Natural river generation from images; Natural
   phenomena
ID TERRAIN GENERATION
AB The rapid and flexible design of natural environments is an important yet challenging task in graphics simulation, virtual reality, and video game productions. This is particularly difficult for natural river modeling due to its complex topology, geometric diversity, and its natural interaction with the complicated terrain. In this paper, we introduce an integrated method for example-based procedural modeling to overcome such difficulties. First, we propose a compact parametric model to represent the certain river, which inherits typical features of natural rivers such as tributary, distributary, tortuosity, possible lakes adjacent to the river. Then, we demonstrate our method for generating 3D river scene solely based on the parametric model. However, choosing appropriate parameters is a tedious undertaking in practice. To further enhance our method's functionality, we rely upon a natural river image to extract meaningful parameters toward the rapid procedural production of the new river scene. Finally, we design a new method to compare two river scenes and iteratively optimize the river network by using the simulated annealing technique. Our method can produce natural river scenes from an example river network and single terrain image with little interaction, and the synthesized scene is visually consistent with the input example in terms of feature similarity. We also demonstrate that our procedural modeling approach is highly automatic toward rapid scene production through various graphics examples.
C1 [Zhang, Jian; Wang, Chang-bo; Chen, Yi; Gao, Yan] East China Normal Univ, Sch Comp Sci & Software Engn, Shanghai, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 East China Normal University; State University of New York (SUNY)
   System; State University of New York (SUNY) Stony Brook
RP Wang, CB (corresponding author), East China Normal Univ, Sch Comp Sci & Software Engn, Shanghai, Peoples R China.
EM cbwangcg@gmail.com
FU Natural Science Foundation of China [61532002, 61672237]; Natural
   Science Foundation [NSF IIS-1715985]; National High-tech R&D Program of
   China (863 Program) [2015AA016404]
FX This paper is partially supported by Natural Science Foundation of China
   (No.61532002, 61672237), Natural Science Foundation Grant NSF
   IIS-1715985, and National High-tech R&D Program of China (863 Program)
   under Grant 2015AA016404.
CR Aliaga DG, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409113
   [Anonymous], 2008, Proceedings of the 2008 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '08
   Argudo O, 2017, VISUAL COMPUT, V33, P1005, DOI 10.1007/s00371-017-1393-6
   BRANDT JW, 1992, CVGIP-IMAG UNDERSTAN, V55, P329, DOI 10.1016/1049-9660(92)90030-7
   Cordonnier G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073667
   Cordonnier G, 2016, COMPUT GRAPH FORUM, V35, P165, DOI 10.1111/cgf.12820
   Derzapf E, 2011, COMPUT GRAPH FORUM, V30, P2031, DOI 10.1111/j.1467-8659.2011.02052.x
   Emilien A, 2015, COMPUT GRAPH FORUM, V34, P22, DOI 10.1111/cgf.12515
   Emilien A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766975
   Emilien A, 2012, VISUAL COMPUT, V28, P809, DOI 10.1007/s00371-012-0699-7
   Fisher M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366154
   Funkhouser T, 2004, ACM T GRAPHIC, V23, P652, DOI 10.1145/1015706.1015775
   Games E., 2007, Unreal engine
   Génevaux JD, 2015, COMPUT GRAPH FORUM, V34, P198, DOI 10.1111/cgf.12530
   Génevaux JD, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461996
   Guerrero P, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766933
   Han JW, 2006, VISUAL COMPUT, V22, P918, DOI 10.1007/s00371-006-0078-3
   Hnaidi H, 2010, COMPUT GRAPH FORUM, V29, P2179, DOI 10.1111/j.1467-8659.2010.01806.x
   HORTON RE, 1945, GEOL SOC AM BULL, V56, P275, DOI 10.1130/0016-7606(1945)56[275:edosat]2.0.co;2
   Hou F, 2016, VISUAL COMPUT, V32, P151, DOI 10.1007/s00371-015-1061-7
   Huijser R., 2010, Proceedings 2010 Brazilian Symposium on Games and Digital Entertainment (SBGAMES 2010), P189, DOI 10.1109/SBGAMES.2010.31
   Ijiri T, 2008, COMPUT GRAPH FORUM, V27, P429, DOI 10.1111/j.1467-8659.2008.01140.x
   Kelley AD, 1988, TERRAIN SIMULATION U
   Kelly G., 2006, I TECHNOLOGY BLANCHA, V14, P87
   Landes PE, 2013, COMPUT GRAPH FORUM, V32, P67, DOI 10.1111/cgf.12152
   Mei X, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P47, DOI 10.1109/PG.2007.15
   Merrell P, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P105
   Nishida G, 2016, COMPUT GRAPH FORUM, V35, P5, DOI 10.1111/cgf.12728
   Nishida G, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925951
   Pajarola R, 2007, VISUAL COMPUT, V23, P583, DOI [10.1007/s00371-007-0163-2, 10.1007/S00371-007-0163-2]
   Prusinkiewicz Przemyslaw., 1993, Graphics Interface, V93, P174
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Rusnell B, 2009, VISUAL COMPUT, V25, P573, DOI 10.1007/s00371-009-0332-6
   Samavati F, 2016, VISUAL COMPUT, V32, P1293, DOI 10.1007/s00371-016-1227-y
   Smelik RM, 2011, COMPUT GRAPH-UK, V35, P352, DOI 10.1016/j.cag.2010.11.011
   Smelik RM, 2014, COMPUT GRAPH FORUM, V33, P31, DOI 10.1111/cgf.12276
   Stava O, 2014, COMPUT GRAPH FORUM, V33, P118, DOI 10.1111/cgf.12282
   Tan P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239538
   Wang F, 2015, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2015.7298797
   Xu K, 2002, PROC GRAPH INTERF, P25
   Yu QZ, 2009, COMPUT GRAPH FORUM, V28, P239, DOI 10.1111/j.1467-8659.2009.01363.x
   Zhang HJ, 2016, IEEE ACCESS, V4, P6238, DOI 10.1109/ACCESS.2016.2612700
   Zhou H, 2007, IEEE T VIS COMPUT GR, V13, P834, DOI 10.1109/TVCG.2007.1027
NR 43
TC 2
Z9 2
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2019
VL 35
IS 2
BP 223
EP 237
DI 10.1007/s00371-017-1465-7
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HN3DS
UT WOS:000460064600006
DA 2024-07-18
ER

PT J
AU Lim, J
   Lee, K
AF Lim, Jeonghun
   Lee, Kunwoo
TI 3D object recognition using scale-invariant features
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 6th International Conference on Virtual Reality and Visualization
   (ICVRV)
CY SEP 24-26, 2016
CL Hangzhou, PEOPLES R CHINA
SP China Soc Image & Graph, China Comp Federat, China Syst Simulat Assoc, IEEE Comp Soc, Connected Universal Experiences Labs Inc, China Soc Image & Graph, VR Comm, China Comp Federat, VR & Visualizat Comm, China Syst Simulat Assoc, VR Comm, China Syst Simulat Assoc, Digital Entertainment Comm, China Syst Simulat Assoc, Surgery Simulat Comm
DE 3D feature selection; 3D feature descriptor; 3D object recognition;
   Scale-invariant recognition
ID UNIQUE SIGNATURES; SURFACE-FEATURE; IMAGES; REPRESENTATION;
   REGISTRATION; HISTOGRAMS
AB As 3D scanning technology develops, it becomes easier to acquire various 3D surface data; thus, there is a growing need for 3D data registration and recognition technology. Many existing studies use local descriptors using local surface patches, and most of them use a fixed support radius, so they cannot cope perfectly when the model and scene have different scales. In this study, we propose a perfectly scale-invariant feature selection algorithm by extending the 2D SIFT algorithm (Lowe in Int J Comput Vis 60(2):91-110, 2004) to a 3D mesh. The feature selection method proposed in this study can obtain highly repeatable feature points and support radii regardless of mesh scale. The selected features can effectively describe the local information by the new shape descriptor proposed in this study. Unlike existing shape descriptors, it is possible to perform scale-invariant 3D object recognition and achieve a high recognition rate when combined with the feature point selection algorithm proposed in this study by using the gradients of the scalar functions defined on the 3D surface. We also reduced the searching space and lowered the false positive rate by suggesting a new RANSAC-based transformation hypotheses generation algorithm. Our 3D object recognition algorithm achieves recognition rates of 100 and 98.5%, respectively, when tested on the U3OR and CFVD datasets, exceeding the results of previous studies.
C1 [Lim, Jeonghun; Lee, Kunwoo] Seoul Natl Univ, Inst Engn Res, Sch Mech Engn, Seoul, South Korea.
C3 Seoul National University (SNU)
RP Lee, K (corresponding author), Seoul Natl Univ, Inst Engn Res, Sch Mech Engn, Seoul, South Korea.
EM kunwoohccl@gmail.com
CR [Anonymous], 2009, IEEE INT C ROB AUT
   Bariya P, 2012, INT J COMPUT VISION, V99, P232, DOI 10.1007/s11263-012-0526-7
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Botsch M., 2004, PROC EUROGRAPHICS AC, P185, DOI [10.1145/1057432.1057457, DOI 10.1145/1057432.1057457]
   Castellani U, 2008, COMPUT GRAPH FORUM, V27, P643, DOI 10.1111/j.1467-8659.2008.01162.x
   Chen H, 2007, PATTERN RECOGN LETT, V28, P1252, DOI 10.1016/j.patrec.2007.02.009
   Darom T, 2012, IEEE T IMAGE PROCESS, V21, P2758, DOI 10.1109/TIP.2012.2183142
   Frome A, 2004, LECT NOTES COMPUT SC, V3023, P224
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Guo YL, 2016, INT J COMPUT VISION, V116, P66, DOI 10.1007/s11263-015-0824-y
   Guo YL, 2015, INFORM SCIENCES, V293, P196, DOI 10.1016/j.ins.2014.09.015
   Guo YL, 2013, INT J COMPUT VISION, V105, P63, DOI 10.1007/s11263-013-0627-y
   Harris C., 1988, ALVEY VISION C, P147151
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kokkinos I, 2012, PROC CVPR IEEE, P159, DOI 10.1109/CVPR.2012.6247671
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matei B, 2006, IEEE T PATTERN ANAL, V28, P1111, DOI 10.1109/TPAMI.2006.148
   Mian A, 2010, INT J COMPUT VISION, V89, P348, DOI 10.1007/s11263-009-0296-z
   Mian AS, 2006, IEEE T PATTERN ANAL, V28, P1584, DOI 10.1109/TPAMI.2006.213
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Mokhtarian F, 2001, IMAGE VISION COMPUT, V19, P271, DOI 10.1016/S0262-8856(00)00076-7
   Novatnack J, 2008, LECT NOTES COMPUT SC, V5304, P440, DOI 10.1007/978-3-540-88690-7_33
   Papazov C, 2012, INT J ROBOT RES, V31, P538, DOI 10.1177/0278364911436019
   Papazov C, 2011, LECT NOTES COMPUT SC, V6492, P135, DOI 10.1007/978-3-642-19315-6_11
   Pottmann H, 2009, COMPUT AIDED GEOM D, V26, P37, DOI 10.1016/j.cagd.2008.01.002
   Rodolà E, 2013, INT J COMPUT VISION, V102, P129, DOI 10.1007/s11263-012-0568-x
   Salti S, 2014, COMPUT VIS IMAGE UND, V125, P251, DOI 10.1016/j.cviu.2014.04.011
   Shah SAA, 2016, NEUROCOMPUTING, V205, P1, DOI 10.1016/j.neucom.2015.11.019
   Sipiran I, 2011, VISUAL COMPUT, V27, P963, DOI 10.1007/s00371-011-0610-y
   Taati B, 2011, COMPUT VIS IMAGE UND, V115, P681, DOI 10.1016/j.cviu.2010.11.021
   Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26
   Yamany SM, 2002, IEEE T PATTERN ANAL, V24, P1105, DOI 10.1109/TPAMI.2002.1023806
   Yu Zhong, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P689, DOI 10.1109/ICCVW.2009.5457637
   Yulan Guo, 2013, GRAPP IVAPP, P86, DOI DOI 10.5220/0004277600860093
   Zaharescu A, 2009, PROC CVPR IEEE, P373, DOI 10.1109/CVPRW.2009.5206748
NR 35
TC 7
Z9 7
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2019
VL 35
IS 1
BP 71
EP 84
DI 10.1007/s00371-017-1453-y
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA HL6ZO
UT WOS:000458885900007
DA 2024-07-18
ER

PT J
AU Hachmann, H
   Awiszus, M
   Rosenhahn, B
AF Hachmann, Hendrik
   Awiszus, Maren
   Rosenhahn, Bodo
TI 3D braid guide hair reconstruction using electroluminescent wires
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Hair modeling; Image-based modeling; 3D reconstruction; Braids
ID SNAKES
AB In this paper we propose a novel braid acquisition and 3D guide hair reconstruction method. Low-cost electroluminescent wires are woven into the braided hair strands which are thereby illuminated from the inside. Unlike state-of-the-art hair reconstruction approaches, we do not need image texture information, data-driven prior knowledge or manual editing. Instead, our workflow reconstructs braid guide hairs fully automatically using semi-open-end 3D active curves on images recorded from multiple views. The proposed pipeline extracts non-surface, internal 3D information which enables morphing and inter-character hairdo-transfer. In state-of-the-art methods, those abilities typically exist for virtually created hairstyles and not for reconstructed hairstyles. Furthermore, using the new acquisition scheme we provide a novel type of data set to the community.
C1 [Hachmann, Hendrik; Awiszus, Maren; Rosenhahn, Bodo] Leibniz Univ Hannover, Inst Informat Verarbeitung, Appelstr 9A, D-30167 Hannover, Germany.
C3 Leibniz University Hannover
RP Hachmann, H (corresponding author), Leibniz Univ Hannover, Inst Informat Verarbeitung, Appelstr 9A, D-30167 Hannover, Germany.
EM hachmann@tnt.uni-hannover.de
OI Awiszus, Maren/0000-0002-0029-2707
CR [Anonymous], 1989, Shape from shading
   Bouguet J., CAMERA CALIBRATION T
   Chai ML, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925961
   Chai ML, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818112
   Chai ML, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461990
   Chai ML, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185612
   Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291
   Choe BW, 2005, IEEE T VIS COMPUT GR, V11, P160, DOI 10.1109/TVCG.2005.20
   COHEN LD, 1991, CVGIP-IMAG UNDERSTAN, V53, P211, DOI 10.1016/1049-9660(91)90028-N
   Destriau G., 1936, J CHIM PHYS, V33, P587, DOI DOI 10.1051/JCP/1936330587
   Hadap S, 2000, SPRING COMP SCI, P87
   Herrera TL, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366165
   Hu LW, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661254
   Hu LW, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766931
   Ishikawa Takahito, 2007, ACM SIGGRAPH 2007 PO
   Jakob W, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618510
   Kass M., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P259, DOI 10.1007/BF00133570
   LAURENTINI A, 1994, IEEE T PATTERN ANAL, V16, P150, DOI 10.1109/34.273735
   Luo LJ, 2013, PROC CVPR IEEE, P265, DOI 10.1109/CVPR.2013.41
   Müller O, 2013, IEEE I CONF COMP VIS, P1129, DOI 10.1109/ICCV.2013.144
   Paris S, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360629
   Scheuermann B, 2014, INT J PATTERN RECOGN, V28, DOI 10.1142/S0218001414600155
   Smith MB, 2010, CYTOSKELETON, V67, P693, DOI 10.1002/cm.20481
   von Marcard T, 2017, COMPUT GRAPH FORUM, V36, P349, DOI 10.1111/cgf.13131
   Wang LD, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531362
   Ward K, 2007, IEEE T VIS COMPUT GR, V13, P213, DOI 10.1109/TVCG.2007.30
   Xu CY, 1998, IEEE T IMAGE PROCESS, V7, P359, DOI 10.1109/83.661186
   Yu YH, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P295, DOI 10.1109/PCCGA.2001.962885
NR 28
TC 1
Z9 1
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 793
EP 804
DI 10.1007/s00371-018-1526-6
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400005
DA 2024-07-18
ER

PT J
AU Kao, CC
   Hsu, WC
AF Kao, Chih-Chen
   Hsu, Wei-Chung
TI Exploring hidden coherency of Ray-Tracing for heterogeneous systems
   using online feedback methodology
SO VISUAL COMPUTER
LA English
DT Article
DE Ray-Tracing; Heterogeneous systems; Irregular program; HSA; Shared
   virtual memory
AB Although naturally adopting an embarrassingly parallel paradigm, Ray-Tracing is also categorized as an irregular program that is troublesome to run on graphics processing units (GPUs). Conventional designs suffer from a performance penalty due to the irregularity of the control flow and memory access caused by incoherent rays. This work aims to explore the hidden coherency of rays by designing a feedback-guided mechanism that serves the following concept: extraction of the hidden regular portions out of the irregular execution flow. The method records the correlation of ray attributes and the traversed path and groups the newly generated rays to reduce potential irregularities for the ongoing execution. This mechanism captures the information from the entire ray space and can extract the hidden coherency from both primary and derived rays. The result leads to performance gains and an increase in resource utilization. The performance becomes 2 to 2.5 times higher than the original GPU and CPU versions.
C1 [Kao, Chih-Chen; Hsu, Wei-Chung] Natl Taiwan Univ, Taipei, Taiwan.
C3 National Taiwan University
RP Kao, CC (corresponding author), Natl Taiwan Univ, Taipei, Taiwan.
EM elros.morlin@gmail.com
OI Kao, Chih-Chen/0000-0002-7631-2284
FU Ministry of Science and Technology of Taiwan [MOST 104-2622-8-002-002];
   MediaTek Inc., Taiwan
FX This work was financially supported by the Ministry of Science and
   Technology of Taiwan under Grants MOST 104-2622-8-002-002, and sponsored
   by MediaTek Inc., Taiwan.
CR Afra Attila, 2016, P HIGH PERF GRAPH, P119, DOI DOI 10.2312/HPG.20161198
   Aila T., 2009, P C HIGH PERFORMANCE, P145, DOI [DOI 10.1145/1572769.1572792, 10.1145/1572769.1572792]
   Aila Timo, 2010, PROC ACM C HIGH PERF, P113
   AMD and GPUOpen, RAD RAYS
   [Anonymous], 1991, COMPUTATIONAL MODELS
   [Anonymous], 2013, PROC IEEE HCS, DOI DOI 10.1109/HOTCHIPS.2013.7478286
   Antwerpen D., 2011, P ACM SIGGRAPH S HIG, P41, DOI [10.1145/2018323.2018330, DOI 10.1145/2018323.2018330]
   Barringer R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601222
   Benthin C, 2012, IEEE T VIS COMPUT GR, V18, P1438, DOI 10.1109/TVCG.2011.277
   Boulos S, 2008, RT08: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2008, PROCEEDINGS, P131, DOI 10.1109/RT.2008.4634633
   Bouvier D, 2014, IEEE HOT CHIP SYMP
   Dammertz H, 2008, COMPUT GRAPH FORUM, V27, P1225, DOI 10.1111/j.1467-8659.2008.01261.x
   Davidovic T, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2602144
   Eisenacher C, 2013, COMPUT GRAPH FORUM, V32, P125, DOI 10.1111/cgf.12158
   Garanzha K, 2010, COMPUT GRAPH FORUM, V29, P289, DOI 10.1111/j.1467-8659.2009.01598.x
   Gribble CR, 2008, RT08: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2008, PROCEEDINGS, P59, DOI 10.1109/RT.2008.4634622
   Günther J, 2007, RT07: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2007, P113, DOI 10.1109/RT.2007.4342598
   Jeffers J., 2013, Intel Xeon Phi coprocessor high-performance programming
   Kao C. C., 2016, P IEEE INT C MULT EX, P1
   Kao CC, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING (DSP), P100, DOI 10.1109/ICDSP.2015.7251838
   Laine S., 2013, 5 HIGH PERFORM GRAPH, P137, DOI [10.1145/2492045.2492060, DOI 10.1145/2492045.2492060]
   Moon B, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1805964.1805972
   Munshi A., 2011, OpenCL programming guide
   Novak J., 2010, P EUROGRAPHICS
   Overbeck R, 2008, RT08: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2008, PROCEEDINGS, P41, DOI 10.1109/RT.2008.4634619
   Pajot A, 2011, COMPUT GRAPH FORUM, V30, P315, DOI 10.1111/j.1467-8659.2011.01863.x
   Parker SG, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778803
   Pharr M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P101, DOI 10.1145/258734.258791
   Pharr M., 2004, Physically Based Rendering: From Theory to Implementation
   Ramani K, 2009, ACM SIGPLAN NOTICES, V44, P325, DOI 10.1145/1508284.1508282
   Sung K, 1998, PACIFIC GRAPHICS '98, PROCEEDINGS, P150, DOI 10.1109/PCCGA.1998.732097
   Tong W., 2013, ACM SIGGRAPH ASIA 20, P31
   Tsakok J.A., 2009, Proc. High Performance Graphics, P151, DOI DOI 10.1145/1572769.15727932
   Tzeng Stanley, 2010, P C HIGH PERF GRAPH, P29
   Wald I, 2001, COMPUT GRAPH FORUM, V20, pC153, DOI 10.1111/1467-8659.00508
   Wald I.:., 2011, BOOK ACTIVE THREAD C, P51, DOI DOI 10.1145/2018323.2018331
   Wald I, 2008, RT08: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2008, PROCEEDINGS, P49, DOI 10.1109/RT.2008.4634620
   Wald I, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601199
NR 38
TC 2
Z9 2
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2018
VL 34
IS 5
BP 633
EP 643
DI 10.1007/s00371-017-1403-8
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GC3AW
UT WOS:000429657100004
DA 2024-07-18
ER

PT J
AU Andries, B
   Lemeire, J
   Munteanu, A
AF Andries, Bob
   Lemeire, Jan
   Munteanu, Adrian
TI Scalable texture compression using the wavelet transform
SO VISUAL COMPUTER
LA English
DT Article
DE Texture compression; Texture mapping; Wavelet transform; Quantization
AB 2D texture data represent one of the main data sources in 3D graphics, requiring large amounts of memory and bandwidth. Texture compression is of critical importance in this context to cope with these bottlenecks. To improve upon the available supported texture compression systems, several transform-based solutions have been proposed. These solutions, however, are not suitable for real-time texture sampling or provide insufficient image quality at medium to low rates. We propose a new scalable texture codec based on the 2D wavelet transform suitable for real-time rendering and filtering, using a new subband coding technique. The codec offers superior compression performance compared to the state-of-the-art, resolution scalability coupled with a wide variety of quality versus rate trade-offs as well as complexity scalability supported by the use of different wavelet filters.
C1 [Andries, Bob; Munteanu, Adrian] Vrije Univ Brussel, ETRO, Pl Laan 2, B-1050 Brussels, Belgium.
   [Lemeire, Jan] Vrije Univ Brussel, ETRO INDI, Pl Laan 2, B-1050 Brussels, Belgium.
C3 Vrije Universiteit Brussel; Vrije Universiteit Brussel
RP Andries, B (corresponding author), Vrije Univ Brussel, ETRO, Pl Laan 2, B-1050 Brussels, Belgium.
EM bandries@etro.vub.ac.be; jlemeire@etro.vub.ac.be;
   acmuntea@etro.vub.ac.be
RI Lemeire, Jan/AAJ-6474-2020; Munteanu, Adrian/HKO-9955-2023
OI Lemeire, Jan/0000-0002-2106-448X; Munteanu, Adrian/0000-0001-7290-0428
FU Agency for Innovation by Science and Technology in Flanders (IWT)
   [SB-536]; iMinds institute through the ICON project BAHAMAS; Research
   Foundation - Flanders (FWO)
FX The Kodim test images used in this paper are courtesy of KODAK [28].
   This work was funded by the Agency for Innovation by Science and
   Technology in Flanders (IWT) through bursary SB-536, by the iMinds
   institute through the ICON project BAHAMAS and by the Research
   Foundation - Flanders (FWO).
CR Alecu A, 2003, J ELECTRON IMAGING, V12, P522, DOI 10.1117/1.1581731
   Alecu A, 2006, IEEE T IMAGE PROCESS, V15, P2499, DOI 10.1109/TIP.2006.877416
   Andries B, 2014, IEEE INT C IM PROC 2
   [Anonymous], ASTC ENCODER
   [Anonymous], NVIDIA Texture Tools for Adobe Photoshop
   [Anonymous], 2003, HWWS'03: Proceedings of the ACM SIGGRAPH/EUROGRAPHICS conference on Graphics hardware
   Beers A. C., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P373, DOI 10.1145/237170.237276
   Bjontegaard G, 2001, VCEGM33ITUTQ616
   COHEN A, 1992, COMMUN PUR APPL MATH, V45, P485, DOI 10.1002/cpa.3160450502
   DELP EJ, 1979, IEEE T COMMUN, V27, P1335, DOI 10.1109/TCOM.1979.1094560
   DiVerdi S., 2005, Real-time Rendering with Wavelet Compressed Multi-Dimensional Datasets on the GPU
   Grund N, 2010, WSCG 2010: COMMUNICATION PAPERS PROCEEDINGS, P207
   Hollemeersch CF, 2012, VISUAL COMPUT, V28, P371, DOI 10.1007/s00371-011-0621-8
   Iourcha KI, 2003, US Patent, Patent No. [6,658,146, 6658146]
   Kilgard M. J., LATC OPENGL EXTENSIO
   Mallat S., 1998, WAVELET TOUR SIGNAL, V16
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   Malvar H., 2003, YCOCG R COLOR SPACE
   Mavridis P, 2012, COMPUT GRAPH FORUM, V31, P2107, DOI 10.1111/j.1467-8659.2012.03203.x
   Nystad A., 2012, Proceedings of the Fourth ACM SIGGRAPH/Eurographics Conference on High-Performance Graphics, P105, DOI 10.2312/EGGH/HPG12/105-114
   Pereberin A. V., 1999, P GRAPHICON 99, P195
   Sun CH, 2009, IEEE T MULTIMEDIA, V11, P589, DOI 10.1109/TMM.2009.2017637
   Tenllado C., 2004, IASTED VIS IM IM PRO
   Tenllado C, 2008, IEEE T PARALL DISTR, V19, P299, DOI 10.1109/TPDS.2007.70716
   Treib M, 2012, COMPUT GRAPH FORUM, V31, P383, DOI 10.1111/j.1467-8659.2012.03017.x
   van der Laan WJ, 2011, IEEE T PARALL DISTR, V22, P132, DOI 10.1109/TPDS.2010.143
   Wong TT, 2007, IEEE T MULTIMEDIA, V9, P668, DOI 10.1109/TMM.2006.887994
NR 27
TC 1
Z9 1
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2017
VL 33
IS 9
BP 1121
EP 1139
DI 10.1007/s00371-016-1269-1
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FD1CS
UT WOS:000407275600004
DA 2024-07-18
ER

PT J
AU Chen, J
   Zheng, YC
   Song, Y
   Sun, HQ
   Bao, HJ
   Huang, J
AF Chen, Jiong
   Zheng, Yicun
   Song, Ying
   Sun, Hanqiu
   Bao, Hujun
   Huang, Jin
TI Cloth compression using local cylindrical coordinates
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th Conference on Computer Graphics International (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Data compression; Cloth animation; Strain limiting; Local cylindrical
   coordinate
ID MESH; DEFORMATION
AB Dense triangular mesh is required to represent fine wrinkle details, which leads to heavy cost of storage and network transmission payload for cloth animation. This paper describes a simple and efficient compression method based on the nearly inextensible property of cloth, whose main degrees of freedom are the dihedral angles. Given a single frame as the reference, we build a local cylindrical coordinate system and encode the vertex as three channels: dihedral angle, change of radius and height w.r.t. the reference. The values of latter two channels are close to zero due to the inextensibility of cloth, which helps for a high compression ratio. Compared with previous approaches, our method can achieve a higher compression ratio with lower computational cost.
C1 [Chen, Jiong] Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.
   [Zheng, Yicun; Bao, Hujun] Zhejiang Univ, Sch Comp Sci & Technol, Hangzhou, Zhejiang, Peoples R China.
   [Huang, Jin] Zhejiang Univ, Comp Sci Dept, Hangzhou, Zhejiang, Peoples R China.
   [Song, Ying] Zhejiang Sci Tech Univ, Comp Sci, Hangzhou, Zhejiang, Peoples R China.
   [Sun, Hanqiu] Chinese Univ Hong Kong, Shatin, Hong Kong, Peoples R China.
C3 Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang
   Sci-Tech University; Chinese University of Hong Kong
RP Huang, J (corresponding author), Zhejiang Univ, Comp Sci Dept, Hangzhou, Zhejiang, Peoples R China.
EM ysong@zstu.edu.cn; hj@cad.zju.edu.cn
FU NSFC [61522209, 61210007, 61602416, 61379087, 61602183]; RGC [416212];
   UGC [4055060]
FX We would like to thank the anonymous reviewers for their valuable
   comments and suggestions. This work is partially supported by NSFC (Nos.
   61522209, 61210007). Prof. Ying Song is supported by NSFC (No. 61602416)
   and Prof. Hanqiu Sun is supported by RGC research grant (Ref. 416212),
   UGC grant (No. 4055060), NSFC funds (Nos. 61379087, 61602183).
CR Ahn JK, 2013, IEEE T MULTIMEDIA, V15, P485, DOI 10.1109/TMM.2012.2235417
   Alexa M, 2000, COMPUT GRAPH FORUM, V19, pC411, DOI 10.1111/1467-8659.00433
   Alliez P, 2001, COMPUT GRAPH FORUM, V20, pC480, DOI 10.1111/1467-8659.00541
   [Anonymous], ACM T GRAPH
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Bici MO, 2011, J VIS COMMUN IMAGE R, V22, P577, DOI 10.1016/j.jvcir.2011.07.006
   English E, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360665
   Goldenthal R, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239500
   Huang J, 2006, COMPUT ANIMAT VIRT W, V17, P383, DOI 10.1002/cav.141
   Huang QX, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766890
   Ibarria L., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P126
   James DL, 2005, ACM T GRAPHIC, V24, P399, DOI 10.1145/1073204.1073206
   Karni Z, 2004, COMPUT GRAPH-UK, V28, P25, DOI 10.1016/j.cag.2003.10.002
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Lewis JP, 2000, COMP GRAPH, P165, DOI 10.1145/344779.344862
   Liu T., 2013, ACM T GRAPHIC, V32, DOI DOI 10.1145/2508363.2508406
   Mamou K, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P1121, DOI 10.1109/ICME.2008.4607636
   Müller M, 2007, J VIS COMMUN IMAGE R, V18, P109, DOI 10.1016/j.jvcir.2007.01.005
   Narain R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366171
   Rossignac J, 1999, IEEE T VIS COMPUT GR, V5, P47, DOI 10.1109/2945.764870
   Sattler M., 2005, P 2005 ACM SIGGRAPH, P209
   Shikhare D., 2001, Vision, Modeling, and Visualization 2001. Proceedings, P233
   STEFANOSKI N, 2007, 3DTV C 2007, P1, DOI DOI 10.1109/3DTV.2007.4379461
   Stefanoski N, 2006, IEEE IMAGE PROC, P2973, DOI 10.1109/ICIP.2006.312961
   Stefanoski N, 2010, COMPUT GRAPH FORUM, V29, P101, DOI 10.1111/j.1467-8659.2009.01547.x
   Thomaszewski B., 2007, EUROGRAPHICS 2007 TU
   Thomaszewski B, 2009, COMPUT GRAPH FORUM, V28, P569, DOI 10.1111/j.1467-8659.2009.01397.x
   Touma C, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P26
   WANG H, 2010, ACM T GRAPHIC, V29
   Wang HM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964966
NR 30
TC 5
Z9 5
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 801
EP 810
DI 10.1007/s00371-017-1389-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800012
DA 2024-07-18
ER

PT J
AU Wong, KM
   Wong, TT
AF Wong, Kin-Ming
   Wong, Tien-Tsin
TI Blue noise sampling using an N-body simulation-based method
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th International Conference on Computer Graphics (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Blue noise sampling; Physically based method; N-body simulation
AB We present a physically based blue noise sampling approach which can be evaluated efficiently by using the N-body simulation method. A set of sample points is modeled as electrically charged particles on an imaginary 2D plane where they self-organize by movement to minimize the electrostatic force that they each experience. The resulting particles' positions at equilibrium exhibit an equidistant neighborhood characteristic that fulfills the essential requirement of a quality blue noise point set. We propose to use the Velocity Verlet algorithm commonly used in molecular dynamics simulation as our integration method, and we apply custom adaptation to improve the convergence rate for our purpose. Our method uses the magnitude of electrical charge of particles as an intuitive control parameter of the spectral behavior of the generated blue noise point sets. We are able to obtain high-quality blue noise point sets comparable to the state-of-the-art results, and we have also implemented a simple GPU application to evaluate our method on the image stippling application.
C1 [Wong, Kin-Ming; Wong, Tien-Tsin] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Sha Tin, Hong Kong, Peoples R China.
   [Wong, Tien-Tsin] Chinese Acad Sci, Shenzhen Inst Adv Technol, Guangdong Prov Key Lab Comp Vis & Virtual Real Te, Shenzhen, Peoples R China.
C3 Chinese University of Hong Kong; Chinese Academy of Sciences; Shenzhen
   Institute of Advanced Technology, CAS
RP Wong, KM (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Sha Tin, Hong Kong, Peoples R China.
EM kmwong@cse.cuhk.edu.hk
RI Wong, Kin-Ming/AAU-1616-2021
OI Wong, Kin-Ming/0000-0003-4736-0831
FU Shenzhen Science and Technology Program [JCYJ 20160429190300857];
   Research Grants Council of the Hong Kong Special Administrative Region,
   under RGC General Research Fund [14217516]
FX We would like to thank the anonymous reviewers for their invaluable
   suggestions. This work is supported by Shenzhen Science and Technology
   Program (No. JCYJ 20160429190300857), and Research Grants Council of the
   Hong Kong Special Administrative Region, under RGC General Research Fund
   (Project No. 14217516).
CR Ahmed AGM, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980218
   [Anonymous], P C GRAPH INT
   [Anonymous], SCIENCE
   [Anonymous], INVESTIGATIVE OPHTHA
   [Anonymous], ACM SIGGRAPH COMPUTE
   Atkins CB, 2000, J ELECTRON IMAGING, V9, P151, DOI 10.1117/1.482735
   Balzer M., 2009, CAPACITY CONSTRAINED, V28
   Bridson R., 2007, SIGGRAPH SKETCHES, P22
   COOK RL, 1986, ACM T GRAPHIC, V5, P51, DOI 10.1145/7529.8927
   de Goes F, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366190
   Dunbar D, 2006, ACM T GRAPHIC, V25, P503, DOI 10.1145/1141911.1141915
   Fattal R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964943
   FLOYD RW, 1976, P SID, V17, P75
   Gwosdek P, 2014, J REAL-TIME IMAGE PR, V9, P379, DOI 10.1007/s11554-011-0236-3
   Hanson K.M., 2005, HALFTONING QUASIMONT, P430
   Hardy DJ, 2009, PARALLEL COMPUT, V35, P164, DOI 10.1016/j.parco.2008.12.005
   Heck D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487233
   Idé T, 2003, J OPT SOC AM A, V20, P248, DOI 10.1364/JOSAA.20.000248
   Jiang M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818102
   Kopf J., 2006, RECURSIVE WANG TILES, V25
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Ostromoukhov V, 2004, ACM T GRAPHIC, V23, P488, DOI 10.1145/1015706.1015750
   Schlomer T., 2011, P ACM SIGGRAPH S HIG, P135, DOI DOI 10.1145/2018323.2018345
   Schmaltz C, 2010, COMPUT GRAPH FORUM, V29, P2313, DOI 10.1111/j.1467-8659.2010.01716.x
   SWOPE WC, 1982, J CHEM PHYS, V76, P637, DOI 10.1063/1.442716
   ULICHNEY RA, 1988, P IEEE, V76, P56, DOI 10.1109/5.3288
   Wei LY, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964945
   Wei LY, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360619
   Zhou YH, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185572
NR 29
TC 3
Z9 4
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 823
EP 832
DI 10.1007/s00371-017-1382-9
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800014
DA 2024-07-18
ER

PT J
AU Lelardeux, CP
   Panzoli, D
   Lubrano, V
   Minville, V
   Lagarrigue, P
   Jessel, JP
AF Lelardeux, Catherine Pons
   Panzoli, David
   Lubrano, Vincent
   Minville, Vincent
   Lagarrigue, Pierre
   Jessel, Jean-Pierre
TI Communication system and team situation awareness in a multiplayer
   real-time learning environment: application to a virtual operating room
SO VISUAL COMPUTER
LA English
DT Article
DE Digital collaborative environment; Team situation awareness;
   Communication; Information; Decision making; Learning game; Virtual
   environment; Socio-technical system; Non technical skills
ID MACROCOGNITION; SAFETY; MODELS
AB Digital multi-player learning games are believed to represent an important step forward in risk management training, especially related to human factors, where they are trusted to improve the performance of a team of learners in reducing serious adverse events, near-misses and crashes in complex socio-technical systems. Team situation awareness is one of the critical factors that can lead the team to consider the situation with an erroneous mental representation. Then, inadequate decisions are likely to be made regarding the actual situation. This paper describes an innovative communication system designed to be used in digital learning games. The system aims at enabling the learners to share information and build a common representation of the situation to help them take appropriate actions, anticipate failures, identify, reduce or correct errors. This innovative system is neither based on voice-chat nor branching dialogues, but on the idea that pieces of information can be manipulated as tangible objects in a virtual environment. To that end, it provides a handful of graphic interactions allowing users to collect, memorize, exchange, listen and broadcast information, ask and answer questions, debate and vote. The communication system was experimented on a healthcare training context with students and their teacher. The training scenario is set in a virtual operating room and features latent critical events (wrong-patient or wrong-side surgery). Teams have to manage such a critical situation, detect anomalies hidden in the environment and share them to make the most suitable decision. Analyzing the results demonstrated the efficacy of the communication system as per the ability for the players to actually exchange information, build a common representation of the situation and make collaborative decisions accordingly. The communication system was considered user-friendly by the users and successfully exposed lifelike behaviors such as debate, conflict or irritation. More importantly, every matter or implicit disagreement was raised while playing the game and led to an argued discussion, although eventually the right decision was not always taken by the team. So, improving the gameplay should help theplayers to manage a conflict and to make them agree on the most suitable decision.
C1 [Lelardeux, Catherine Pons; Panzoli, David] Univ Toulouse, IRIT, INU Champoll, Serious Game Res Network, Toulouse, France.
   [Lubrano, Vincent] Univ Toulouse, Univ Hosp, ToNIC, INSERM,UPS,Serious Game Res Network, Toulouse, France.
   [Minville, Vincent] Univ Toulouse, Univ Hosp, MATN, UPS,Serious Game Res Network, Toulouse, France.
   [Lagarrigue, Pierre] Univ Toulouse, ICA, INU Champoll, Serious Game Res Network, Toulouse, France.
   [Jessel, Jean-Pierre] Univ Toulouse, IRIT, UPS, Serious Game Res Network, Toulouse, France.
C3 Universite Federale Toulouse Midi-Pyrenees (ComUE); Universite de
   Toulouse; Institut National Polytechnique de Toulouse; Universite
   Toulouse III - Paul Sabatier; CHU de Toulouse; Institut National de la
   Sante et de la Recherche Medicale (Inserm); Universite de Toulouse;
   Universite Toulouse III - Paul Sabatier; CHU de Toulouse; Universite de
   Toulouse; Universite Toulouse III - Paul Sabatier; Universite de
   Toulouse; Institut Superieur de l'Aeronautique et de l'Espace
   (ISAE-SUPAERO); Universite Toulouse III - Paul Sabatier; Universite
   Federale Toulouse Midi-Pyrenees (ComUE); Universite de Toulouse;
   Institut National Polytechnique de Toulouse; Universite Toulouse III -
   Paul Sabatier
RP Lelardeux, CP (corresponding author), Univ Toulouse, IRIT, INU Champoll, Serious Game Res Network, Toulouse, France.
EM catherine.lelardeux@univ-jfc.fr; david.panzoli@univ-jfc.fr;
   v.lubrano@chu-toulouse.fr; v.minville@chu-toulouse.fr;
   pierre.lagarrigue@univ-jfc.fr; jean-pierre.jessel@irit.fr
RI JESSEL, Jean-Pierre/HNB-6304-2023
OI JESSEL, Jean-Pierre/0000-0001-7197-2235; LAGARRIGUE,
   Pierre/0000-0003-4376-8119
FU French National Funding : Bpifrance Financement
FX The steering committee of 3DVOR is composed of Pr. Pierre Lagarrigue,
   M.D. Ph.D. Vincent Lubrano, M.D. Ph.D. Vincent Minville and Catherine
   Pons-Lelardeux. The following authors are also grateful to contributors
   to the project 3D operating room: Thomas Rodsphon, Cyrielle Guimbal,
   Michel Galaup and Jules de Guglielmi. The experiment described in Sect.
   6.3 has been conducted under the supervision of Christiane Paban
   (teacher) and two students of the anesthetist nurse school of Toulouse:
   Hoang and Amelie. These works are part of a global national innovative
   IT program whose partners are KTM Advance company, Novamotion company,
   Serious Game Research Network and University Hospital of Toulouse
   (France). This R&D project is supported by French National Funding :
   Bpifrance Financement.
CR [Anonymous], 2012, DISNEY STARS VIRTUAL
   Authority P.P.S, 2007, ANN REPORT
   Authority P. P. S, 2012, AUTH PPS ANN REP 201
   Busemann A, 2012, DTSCH ARZTEBL INT, V109, P693, DOI 10.3238/arztebl.2012.0693
   Capin TK, 1997, IEEE COMPUT GRAPH, V17, P42, DOI 10.1109/38.574680
   Carayon P, 2006, APPL ERGON, V37, P525, DOI 10.1016/j.apergo.2006.04.011
   Cassell J., 2000, Embodied Conversational Agents
   Chin TJ, 2009, VISUAL COMPUT, V25, P25, DOI 10.1007/s00371-008-0283-3
   Cowan B, 2015, VISUAL COMPUT, V31, P1207, DOI 10.1007/s00371-014-1006-6
   Csikszentmihalyi M., 2005, HDB COMPETENCE MOTIV, P598
   Dev P, 2011, STUD HEALTH TECHNOL, V163, P173, DOI 10.3233/978-1-60750-706-2-173
   Devreux G., 2014, 28 INT C APPL PSYCH
   Duncan Starkey., 1973, SEMIOTICA, V9, P29, DOI [DOI 10.1515/SEMI.1973.9.1.29, 10.1515/semi.1973.9.1.29]
   Duranti A., 1997, LINGUISTIC ANTHR
   Effken JA, 2002, INT J MED INFORM, V65, P59, DOI 10.1016/S1386-5056(02)00003-5
   Egges A, 2007, VISUAL COMPUT, V23, P317, DOI 10.1007/s00371-007-0113-z
   ENDSLEY MR, 1995, HUM FACTORS, V37, P32, DOI 10.1518/001872095779049543
   Endsley MR, 2000, INT J IND ERGONOM, V26, P301, DOI 10.1016/S0169-8141(99)00073-6
   Fracker M. L., 1990, SITUATIONAL AWARENES
   Fraser N., 1997, Handbook on Standards and Resources for Spoken Language Systems, P564
   Fraser N. M., 1991, Computer Speech and Language, V5, P81, DOI 10.1016/0885-2308(91)90019-M
   GODDEN DR, 1975, BRIT J PSYCHOL, V66, P325, DOI 10.1111/j.2044-8295.1975.tb01468.x
   Goodwin C., 1981, CONVERSATIONAL ORG I
   Grice H. P., 1975, SYNTAX SEMANTICS, V3, P41, DOI DOI 10.1163/9789004368811_003
   Halverson AL, 2011, SURGERY, V149, P305, DOI 10.1016/j.surg.2010.07.051
   Hartel C., 1989, 5 INT S AV PSYCH
   Hennigan B., 2012, 8 INT C NAT LANG PRO
   Herring S., 1999, J COMPUT-MEDIAT COMM, V4
   Johnson M. T., 2008, 2 LIF ED COMM C SLED
   Joint Commission, 2008, IMPR AM HOSP JOINT C, V25
   Kaber DB, 1998, PROCESS SAF PROG, V17, P43, DOI 10.1002/prs.680170110
   Keyton J, 2010, THEOR ISS ERGON SCI, V11, P272, DOI 10.1080/14639221003729136
   Keyton J, 2010, HUM FACTORS, V52, P335, DOI 10.1177/0018720810371338
   Kohn L, 2000, To err is human: Building a safer health system
   Kolb David A, 2014, EXPERIENTIAL LEARNIN, DOI [10.1002/job.4030080408, DOI 10.1016/B978-0-7506-7223-8.50017-4]
   Kopp S, 2004, COMPUT ANIMAT VIRT W, V15, P39, DOI 10.1002/cav.6
   Lagarrigue P., 2012, THE 3DVOR PROJECT
   Leckie G. J., 1996, MODELING INFORM SEEK, P161
   Lelardeux C, 2013, SERIOUS GAMES FOR HEALTHCARE: APPLICATIONS AND IMPLICATIONS, P23, DOI 10.4018/978-1-4666-1903-6.ch002
   Lepper M., 1987, Aptitude, Learning, and Instruction, V3, P255
   Liarokapis F, 2009, VISUAL COMPUT, V25, P1109, DOI 10.1007/s00371-009-0388-3
   Lingard L, 2004, QUAL SAF HEALTH CARE, V13, P330, DOI 10.1136/qshc.2003.008425
   Lipovic I., 2011, SPEECH LANGUAGE TECH
   Ma JY, 2004, VISUAL COMPUT, V20, P86, DOI 10.1007/s00371-003-0234-y
   Malone Thomas W, 2021, Aptitude, learning, and instruction, P223, DOI DOI 10.1016/S0037-6337(09)70509-1
   Malone TW., 1980, P 3 ACM SIGSMALL S 1, P162, DOI DOI 10.1145/800088.802839
   Mateas M., 2004, P C TECHN INT DIG ST
   Mathieu JE, 2000, J APPL PSYCHOL, V85, P273, DOI 10.1037//0021-9010.85.2.273
   Michael D.R., 2005, Serious games: Games that educate, train, and inform
   Mirzaei MR, 2014, VISUAL COMPUT, V30, P245, DOI 10.1007/s00371-013-0841-1
   Morningstar C., 1990, 1 INT C CYB U TEX AU
   Navarathna R., 2010, 2010 10th International Conference on Information Sciences, Signal Processing and their Applications (ISSPA 2010), P598, DOI 10.1109/ISSPA.2010.5605429
   Pandzic IS, 1999, VISUAL COMPUT, V15, P330, DOI 10.1007/s003710050182
   Panzoli D., 2014, P 6 INT C VIRT WORLD
   Plasters C.L., 2003, P AMIA ANN S
   Plsek PE, 2001, BRIT MED J, V323, P625, DOI 10.1136/bmj.323.7313.625
   Poling N.D., 2013, THESIS
   Pons-Lelardeux C, 2015, PROCEDIA ENGINEER, V132, P242, DOI 10.1016/j.proeng.2015.12.476
   Potier V., 2016, GAMIFICATION ENG ED, V1, P1
   Reason J, 2000, BMJ-BRIT MED J, V320, P768, DOI 10.1136/bmj.320.7237.768
   Reason J., 2013, A life in error
   Reddy M., 2010, COLLABORATIVE INFORM
   Reddy M., 2006, MODEL UNDERSTANDING
   Rilling S, 2011, VISUAL COMPUT, V27, P287, DOI 10.1007/s00371-011-0550-6
   Rus V, 2013, AI MAG, V34, P42, DOI 10.1609/aimag.v34i3.2485
   SACKS H, 1974, LANGUAGE, V50, P696, DOI 10.2307/412243
   SALAS E, 1995, HUM FACTORS, V37, P123, DOI 10.1518/001872095779049525
   Sanselone M., 2014, IEEE C COMP INT GAM, P208
   Sarter N.B., 1991, INT J AVIAT PSYCHOL, V1, P45, DOI DOI 10.1207/S15327108IJAP0101_4
   Seiden SC, 2006, ARCH SURG-CHICAGO, V141, P931, DOI 10.1001/archsurg.141.9.931
   Taekman J. M., 2007, ANESTHESIOLOGY, V107, pA2145
   Tear M. F. M., 2002, ACM COMPUTING SURVEY, V34
   Thomas DI, 2008, STUD COMPUT INTELL, V97, P97
   Vicente Kim J., 1999, Cognitive Work Analysis Toward Safe, Productive
   World Health Organization, 2009, Surgical Safety Checklist
   Zichermann G., 2011, GAMIFICATION DESIGN
NR 76
TC 10
Z9 10
U1 2
U2 33
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2017
VL 33
IS 4
BP 489
EP 515
DI 10.1007/s00371-016-1280-6
PG 27
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA ER4JZ
UT WOS:000398767100008
DA 2024-07-18
ER

PT J
AU Huska, M
   Morigi, S
AF Huska, Martin
   Morigi, Serena
TI A meshless strategy for shape diameter analysis
SO VISUAL COMPUTER
LA English
DT Article
DE Shape analysis; Shape diameter function; Meshless surface processing;
   Variational partitioning
ID SEGMENTATION
AB An approach to computing an intuitive local thickness from surface meshes was introduced with the shape diameter function (SDF) in Shapira et al. (Vis Comput 24(4):249-259, 2008). In this paper, we present a new dynamic approach to the computation of the SDF for a cloud of points on the boundary of a volumetric object. We employ a particle flow driven by a simple collision test. The resulting SDF scalar field can be naturally exploited as a shape property for the volume-oriented object decomposition. Experimental results show the effectiveness and efficiency of our proposals.
C1 [Huska, Martin; Morigi, Serena] Univ Bologna, Dept Math, Pzza Porta San Donato 5, Bologna, Italy.
   [Huska, Martin; Morigi, Serena] Univ Padua, Dept Math, Via Trieste 63, Padua, Italy.
C3 University of Bologna; University of Padua
RP Morigi, S (corresponding author), Univ Padua, Dept Math, Via Trieste 63, Padua, Italy.
EM martin@math.unipd.it; serena.morigi@unibo.it
OI HUSKA, MARTIN/0000-0002-5342-1752
CR [Anonymous], 2008, MESHLAB OPEN SOURCE, DOI DOI 10.2312/LOCALCHAPTEREVENTS/ITALCHAP/ITALIANCHAPCONF2008/129-136
   Asafi S, 2013, COMPUT GRAPH FORUM, V32, P23, DOI 10.1111/cgf.12169
   Attene M, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P14
   Au OKC, 2012, IEEE T VIS COMPUT GR, V18, P1125, DOI 10.1109/TVCG.2011.131
   Casciola G, 2006, COMPUT MATH APPL, V51, P1185, DOI 10.1016/j.camwa.2006.04.002
   Chen XB, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531379
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   Fayolle PA, 2013, VISUAL COMPUT, V29, P449, DOI 10.1007/s00371-012-0749-1
   Heider P, 2012, VISUAL COMPUT, V28, P919, DOI 10.1007/s00371-012-0725-9
   KOVACIC M, 2010, P WORKSH COMP GRAPH
   Lien J.-M., 2007, S SOL PHYS MOD, DOI [10.1145/1186415.1186418, DOI 10.1145/1186415.1186418]
   Meyer N, 2003, VISUALIZATION AND MATHEMATICS III, P35
   Morigi S, 2014, VISUAL COMPUT, V30, P479, DOI 10.1007/s00371-013-0873-6
   Morigi S, 2012, LECT NOTES COMPUT SC, V6667, P38, DOI 10.1007/978-3-642-24785-9_4
   Pinkall U., 1993, Exp. Math., V2, P15, DOI 10.1080/10586458.1993.10504266
   Rolland-Nevière X, 2013, GRAPH MODELS, V75, P279, DOI 10.1016/j.gmod.2013.06.001
   Shamir A, 2008, COMPUT GRAPH FORUM, V27, P1539, DOI 10.1111/j.1467-8659.2007.01103.x
   Shapira L, 2010, INT J COMPUT VISION, V89, P309, DOI 10.1007/s11263-009-0279-0
   Shapira L, 2008, VISUAL COMPUT, V24, P249, DOI 10.1007/s00371-007-0197-5
   Staten M.L., 2012, P 20 INT MESH ROUNDT, P293
   Unger M., 2008, P BRIT MACHINE VISIO
   van Kaick O, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2611811
   Yan DM, 2012, COMPUT AIDED DESIGN, V44, P1072, DOI 10.1016/j.cad.2012.04.005
NR 23
TC 6
Z9 8
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2017
VL 33
IS 3
BP 303
EP 315
DI 10.1007/s00371-015-1198-4
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EL1KM
UT WOS:000394379300005
DA 2024-07-18
ER

PT J
AU Chen, Y
   Cheng, ZQ
   Martin, RR
AF Chen, Yin
   Cheng, Zhi-Quan
   Martin, Ralph R.
TI Parametric editing of clothed 3D avatars
SO VISUAL COMPUTER
LA English
DT Article
DE Parametric human; Editing; 3D avatar; Clothing
ID SHAPE; POSE
AB Easy editing of a clothed 3D human avatar is central to many practical applications. However, it is easy to produce implausible, unnatural looking results, since subtle reshaping or pose alteration of avatars requires global consistency and agreement with human anatomy. Here, we present a parametric editing system for a clothed human body, based on use of a revised SCAPE model. We show that the parameters of the model can be estimated directly from a clothed avatar, and that it can be used as a basis for realistic, real-time editing of the clothed avatar mesh via a novel 3D body-aware warping scheme. The avatar can be easily controlled by a few semantically meaningful parameters, 12 biometric attributes controlling body shape, and 17 bones controlling pose. Our experiments demonstrate that our system can interactively produce visually pleasing results.
C1 [Chen, Yin] PLA Univ Sci & Technol, Nanjing, Jiangsu, Peoples R China.
   [Cheng, Zhi-Quan] Avatar Sci Co, Changsha, Hunan, Peoples R China.
   [Martin, Ralph R.] Cardiff Univ, Sch Comp Sci & Informat, Cardiff, S Glam, Wales.
C3 Army Engineering University of PLA; Cardiff University
RP Chen, Y (corresponding author), PLA Univ Sci & Technol, Nanjing, Jiangsu, Peoples R China.
EM chris.leo.chan@gmail.com
RI Martin, Ralph R/D-2366-2010
OI Martin, Ralph/0000-0002-8495-8536
FU National Natural Science Foundation of China [61272334]; State Key
   Laboratory for Novel Software Technology [KFKT2014B12]
FX We thank all the reviewers for their valuable comments. The work is
   supported in part by the National Natural Science Foundation of China
   (No. 61272334) and the Foundation of State Key Laboratory for Novel
   Software Technology under Grant (No. KFKT2014B12).
CR Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311
   Allen B., 2006, P 2006 ACM SIGGRAPHE, P147
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], 2014, P IEEE C COMP VIS PA
   Balan A. O., 2007, IEEE C COMP VIS PATT
   Balan AO, 2008, LECT NOTES COMPUT SC, V5303, P15, DOI 10.1007/978-3-540-88688-4_2
   Botsch M, 2008, IEEE T VIS COMPUT GR, V14, P213, DOI 10.1109/TVCG.2007.1054
   Chen YQ, 2008, ACM T MATH SOFTWARE, V35, DOI 10.1145/1391989.1391995
   Chen YP, 2013, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2013.21
   Ebeida MS, 2012, COMPUT GRAPH FORUM, V31, P785, DOI 10.1111/j.1467-8659.2012.03059.x
   Guan P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185531
   Guan P, 2009, IEEE I CONF COMP VIS, P1381, DOI 10.1109/iccv.2009.5459300
   Hasler N, 2009, COMPUT GRAPH FORUM, V28, P337, DOI 10.1111/j.1467-8659.2009.01373.x
   Hasler N, 2009, COMPUT GRAPH-UK, V33, P211, DOI 10.1016/j.cag.2009.03.026
   Jain A, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866174
   Kavan L, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P39
   Le BH, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366218
   SAE, 2013, CAES 3D ANTHR DAT
   Sumner RW, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239531
   Vaillant R, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461960
   Weiss A, 2011, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2011.6126465
   Zhou SZ, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778863
NR 22
TC 1
Z9 1
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2016
VL 32
IS 11
BP 1405
EP 1414
DI 10.1007/s00371-015-1120-0
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EA2BS
UT WOS:000386397000005
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Wang, QC
   Tao, YB
   Lin, H
AF Wang, Qichao
   Tao, Yubo
   Lin, Hai
TI Surface carving-based automatic volume data reduction
SO VISUAL COMPUTER
LA English
DT Article
DE Automatic; Volume reduction; Surface carving
AB Many fields, such as medicine and biology, are producing an increasingly large volume using high-resolution digital imaging techniques, and this makes effective data analysis and visualization of these volumes more and more difficult. Volume reduction, decreasing the volume size, is one of the promising directions to solve this challenge for interactive volume visualization. In this paper, we present an automatic volume data reduction method called surface carving. It intelligently removes contextual voxels while preserving important features, and finally generates an optimal volume at the desired reduction size/rate. For large volume data sets, a multilevel banded method is introduced to gracefully overcome the memory limit and speed up volume reduction. We compare our technique with traditional cropping and scaling approaches and demonstrate the effectiveness and efficiency of our method with several volume data sets.
C1 [Wang, Qichao; Tao, Yubo; Lin, Hai] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
C3 Zhejiang University
RP Tao, YB (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
EM wqcriver@gmail.com; taoyubo@cad.zju.edu.cn; lin@cad.zju.edu.cn
FU 863 Program [2012AA12A404]; National Natural Science Foundation of China
   [61472354]; Fundamental Research Funds for the Central Universities
   [2013QNA5010]
FX The authors would like to thank the anonymous reviewers for their
   valuable comments. This work was partially supported by 863 Program
   Project 2012AA12A404, National Natural Science Foundation of China No.
   61472354 and the Fundamental Research Funds for the Central Universities
   (2013QNA5010). The data sets are courtesy of Osirix DICOM, VolVis, and
   Siemens Medical Systems.
CR Agrawal A, 2010, J SUPERCOMPUT, V51, P3, DOI 10.1007/s11227-009-0289-2
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Bürger K, 2008, IEEE T VIS COMPUT GR, V14, P1388, DOI 10.1109/TVCG.2008.120
   Congote J., 2011, P 16 INT C 3D WEB TE, P137, DOI [DOI 10.1145/2010425.2010449, 10.1145/2010425.2010449]
   Giachetti A., 2010, EG IT 2010, V10, P17
   Kindlmann G., 1998, P 1998 IEEE S VOL VI
   Kohli P, 2007, IEEE T PATTERN ANAL, V29, P2079, DOI 10.1109/TPAMI.2007.1128
   La Mar E.C., 1999, P 10 IEEE VIS 1999 C
   Lombaert H., 2005, P 10 IEEE INT C COMP
   Moser M., 2008, Vision, Modeling, and Visualization VMV '08 Conference Proceedings, P217
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Vineet Vibhav, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563095
   Viola I, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P139, DOI 10.1109/VISUAL.2004.48
   Wang Q., 2013, C COMP AID DES COMP
   Wang YS, 2011, IEEE T VIS COMPUT GR, V17, P171, DOI 10.1109/TVCG.2010.34
   Weiler M., 2000, P 2000 IEEE S VOL VI
   Westermann Rudiger., 1994, Proceedings of the 1994 Symposium on Volume Visualization, VVS '94, P51
   Wiebel A, 2012, IEEE T VIS COMPUT GR, V18, P2236, DOI 10.1109/TVCG.2012.292
   Yuan XR, 2005, VISUAL COMPUT, V21, P745, DOI 10.1007/s00371-005-0330-2
   Zhao X, 2012, IEEE T VIS COMPUT GR, V18, P1928, DOI 10.1109/TVCG.2012.70
   Zhao Y., 2010, P 1 INT C INT INT TE, P231
NR 22
TC 1
Z9 3
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2015
VL 31
IS 11
BP 1459
EP 1470
DI 10.1007/s00371-014-1026-2
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CT3BJ
UT WOS:000362680600003
DA 2024-07-18
ER

PT J
AU Yamaguchi, S
AF Yamaguchi, Satoshi
TI A parametric reflectance approximation for rendering Japanese
   lacquerware and Maki-e
SO VISUAL COMPUTER
LA English
DT Article
DE Reflectance; BRDF; Measurement-based Modeling; Japanese lacquerware;
   Maki-e
AB This paper proposes a reflectance distribution model that is able to express complex reflections from materials such as Japanese lacquerware and Maki-e to produce highly realistic computer graphics. Our method improves the Ward model for materials with anisotropic reflection, and uses the approximate coefficients calculated from measured reflectance distribution data to reconstruct the actual reflectance. Our research derives a Gaussian distribution summation method and a modified Fresnel reflectance approximation function from measured reflectance data. Our Fresnel reflectance approximation function adds parameters to Schlick's approximation function to control the whole curve and extinction component, and is thus closer to the actual Fresnel reflectance. Based on these functions, a decision process is developed for the approximation coefficients, and this makes it possible to easily reconstruct the actual reflectance. The reconstructed reflectance distributions obtained using these coefficients are compared with measured reflectance data using the L2 norm of the difference. In addition, reconstructed Fresnel reflection effects are compared with those from actual sample materials. Using these coefficient values, this paper shows a sample of simulated Maki-e images under two different lighting environments.
C1 Hosei Univ, Grad Sch Comp & Informat Sci, Tokyo, Japan.
C3 Hosei University
RP Yamaguchi, S (corresponding author), Hosei Univ, Grad Sch Comp & Informat Sci, Tokyo, Japan.
EM satoshi.yamaguchi.yt@stu.hosei.ac.jp
CR [Anonymous], 1991, ACM SIGGRAPH COMPUTE, DOI DOI 10.1145/127719
   [Anonymous], 1982, ACM T GRAPHIC, DOI DOI 10.1145/357290.357293
   Bagher MM, 2012, COMPUT GRAPH FORUM, V31, P1509, DOI 10.1111/j.1467-8659.2012.03147.x
   Koenderink J. J., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P28
   Lafortune E. P. F., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P117, DOI 10.1145/258734.258801
   Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343
   McCool MD, 2001, COMP GRAPH, P171, DOI 10.1145/383259.383276
   Ngan A., 2005, Eurographics Symposium on Rendering, P117, DOI [DOI 10.2312/EGWR/EGSR05/117-1261,2,8, DOI 10.2312/EGWR/EGSR05/117-126]
   Oren M., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P239, DOI 10.1145/192161.192213
   Sasaki E, 1986, SHITSU GEI NO DENTO
   SCHLICK C, 1994, COMPUT GRAPH FORUM, V13, pC233, DOI 10.1111/1467-8659.1330233
   Schroder P., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P161, DOI 10.1145/218380.218439
   Torrance K.E., 1992, RADIOMETRY, P32
   WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078
   Weidlich A, 2007, GRAPHITE 2007: 5TH INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES IN AUSTRALASIA AND SOUTHERN ASIA, PROCEEDINGS, P171
NR 15
TC 0
Z9 0
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2015
VL 31
IS 11
BP 1553
EP 1564
DI 10.1007/s00371-014-1033-3
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CT3BJ
UT WOS:000362680600010
DA 2024-07-18
ER

PT J
AU Wang, SF
   Kong, DH
   Xue, J
   Zhu, WJ
   Xu, M
   Yin, BC
   Roth, H
AF Wang, Shaofan
   Kong, Dehui
   Xue, Juan
   Zhu, Weijia
   Xu, Min
   Yin, Baocai
   Roth, Hubert
TI Connectivity-preserving geometry images
SO VISUAL COMPUTER
LA English
DT Article
DE GIM; CGIM; Remeshing; Mesh parametrization; Connectivity-preserving
AB We propose connectivity-preserving geometry images (CGIMs), which map a triangular mesh onto a rectangular regular array of an image, such that the reconstructed mesh produces no sampling errors, but merely round-off errors over the coordinates of vertices. Using permutation techniques on vertices, CGIMs first obtain a V-matrix whose elements are vertices of the original mesh, which intrinsically preserves the vertex-set and connectivity of the original mesh, and then generate a CGIM array by transforming the Cartesian coordinates of corresponding vertices of the V-matrix into RGB values. Compared with traditional geometry images (GIMs), CGIMs achieve the minimum reconstruction error with a parametrization-free algorithm. We apply CGIMs to lossy compression of meshes. Experimental results show that while CGIMs produce a lower efficiency in both encoding and decoding time and larger resolutions than traditional GIMs, CGIMs perform better peak signal-to-noise ratios and preserve details better than GIMs especially with the multi-stage base color and index map scheme, because CGIMs treat details and non-details of meshes evenly as all elements of the V-matrix.
C1 [Wang, Shaofan; Kong, Dehui; Xue, Juan; Zhu, Weijia; Yin, Baocai] Beijing Univ Technol, Coll Metropolitan Transportat, Beijing Key Lab Multimedia Intelligent Software T, Beijing 100124, Peoples R China.
   [Xu, Min] Dalian Univ Technol, Sch Math Sci, Dalian 116085, Peoples R China.
   [Roth, Hubert] Univ Siegen, Inst Automat Control Engn, D-57068 Siegen, Germany.
C3 Beijing University of Technology; Dalian University of Technology;
   Universitat Siegen
RP Wang, SF (corresponding author), Beijing Univ Technol, Coll Metropolitan Transportat, Beijing Key Lab Multimedia Intelligent Software T, Beijing 100124, Peoples R China.
EM wangshaofan@bjut.edu.cn
RI Zhu, Weijia/A-1598-2019; Zhu, Weijia/C-3087-2017
OI Zhu, Weijia/0000-0001-9202-1260
FU Natural Science Foundation of China [61227004, 61100130, 61033004,
   61370120, 61300065, 11301052]
FX We thank Praun and Hoppe [7] for providing their open spherical
   parametrization data (for cow, horse, bunny and venus). The models in
   this paper are courtesy of Stanford University and AIM@SHAPE Repository.
   This work was supported by the Natural Science Foundation of China (nos.
   61227004, 61100130, 61033004, 61370120, 61300065, 11301052), and was
   partially carried out while Shaofan Wang visited University of Siegen
   under Project-based Personnel Exchange Program from December 2013 to
   March 2014.
CR [Anonymous], 2005, P ACM S SOL PHYS MOD
   Carr N., 2006, P EUROGRAPHICSSIGGRA, P181
   Feng WW, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731049
   Gauthier M, 2009, COMPUT GRAPH FORUM, V28, P2201, DOI 10.1111/j.1467-8659.2009.01434.x
   Gu XF, 2002, ACM T GRAPHIC, V21, P355
   Hoppe H, 2005, MATH VIS, P27, DOI 10.1007/3-540-26808-1_2
   Losasso F., 2003, Symposium on Geometry Processing, P138
   Meng WL, 2010, VISUAL COMPUT, V26, P51, DOI 10.1007/s00371-009-0376-7
   Passalis G, 2007, IEEE T PATTERN ANAL, V29, P218, DOI 10.1109/TPAMI.2007.37
   Praun E, 2003, ACM T GRAPHIC, V22, P340, DOI 10.1145/882262.882274
   Qin Aihong, 2007, Journal of Computer Aided Design & Computer Graphics, V19, P898
   Sander P. V., 2003, Symposium on Geometry Processing, P146
   Tutte W.T., 1963, Proc. London Math. Soc., V13, P743, DOI DOI 10.1112/PLMS/S3-13.1.743
   Xia JZ, 2012, IEEE T CIRC SYST VID, V22, P77, DOI 10.1109/TCSVT.2011.2158337
   Yao CY, 2008, IEEE T VIS COMPUT GR, V14, P948, DOI 10.1109/TVCG.2008.39
   Zhou K, 2004, J COMPUT SCI TECH-CH, V19, P596, DOI 10.1007/BF02945585
NR 16
TC 3
Z9 3
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2015
VL 31
IS 9
BP 1163
EP 1178
DI 10.1007/s00371-014-1000-z
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CO8AY
UT WOS:000359388100002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Rodríguez-Cerro, A
   García-Fernández, I
   Martínez-Durá, RJ
   Pla-Castells, M
AF Rodriguez-Cerro, Angel
   Garcia-Fernandez, Ignacio
   Martinez-Dura, Rafael J.
   Pla-Castells, Marta
TI Texture advection on discontinuous flows
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Flow visualization; Texture advection; Discontinuity; Computer animation
ID AVALANCHES; MODEL
AB Texture advection techniques, which transport textures using a velocity field, are used to visualize the dynamics of a flow on a triangle mesh. Some flow phenomena lead to velocity fields with discontinuities that cause the deformation of the texture which is not properly controlled by these techniques. We propose a method to detect and visualize discontinuities on a flow, keeping consistent texture advection at both sides of the discontinuity. The method handles the possibility that the discontinuity travels across the domain of the flow with arbitrary velocity, estimating its speed with least-squares approximation. The technique is tested with different sample scenarios and with two avalanche scenes, showing that it can run at interactive rates.
C1 [Rodriguez-Cerro, Angel; Martinez-Dura, Rafael J.] Univ Valencia, LSyM, Valencia, Spain.
   [Garcia-Fernandez, Ignacio] Univ Valencia, Dept Informat, Valencia, Spain.
   [Pla-Castells, Marta] Univ Valencia, LISITT, Valencia, Spain.
C3 University of Valencia; University of Valencia; University of Valencia
RP García-Fernández, I (corresponding author), Univ Valencia, Dept Informat, Valencia, Spain.
EM angel.rodriguez@uv.es; ignacio.garcia@uv.es
RI Martínez, Rafael J./K-1852-2012; Pla-Castells, Marta/C-2084-2011;
   Martínez Durá, Rafael Javier/F-4650-2018; García-Fernández,
   Ignacio/C-2107-2011
OI Pla-Castells, Marta/0000-0002-2088-2536; Martínez Durá, Rafael
   Javier/0000-0003-2714-0524; García-Fernández,
   Ignacio/0000-0001-6901-0790
FU Generalitat Valenciana, Valencia, Spain [GV/2012/007]
FX This research has been partially supported by Grant GV/2012/007, from
   Generalitat Valenciana, Valencia, Spain.
CR [Anonymous], VISUAL COMPUTER
   [Anonymous], 2004, COMPUTER ANIMATION 2, DOI DOI 10.1145/1028523.1028549
   Bargteil AdamW., 2006, Proceedings of ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P345
   Bargteil AW, 2006, ACM T GRAPHIC, V25, P19, DOI 10.1145/1122501.1122503
   BOUCHAUD JP, 1994, J PHYS I, V4, P1383, DOI 10.1051/jp1:1994195
   Correa CD, 2011, IEEE T VIS COMPUT GR, V17, P305, DOI 10.1109/TVCG.2009.105
   Daerr A, 2001, PHYS FLUIDS, V13, P2115, DOI 10.1063/1.1377864
   Daerr A, 1999, NATURE, V399, P241, DOI 10.1038/20392
   El Hajjar JF, 2009, VISUAL COMPUT, V25, P87, DOI 10.1007/s00371-007-0207-7
   Hadeler KP, 1999, GRANUL MATTER, V2, P9, DOI 10.1007/s100350050029
   Ihmsen M, 2012, VISUAL COMPUT, V28, P669, DOI 10.1007/s00371-012-0697-9
   Jobard B, 2001, IEEE VISUAL, P53, DOI 10.1109/VISUAL.2001.964493
   Kwatra V, 2007, IEEE T VIS COMPUT GR, V13, P939, DOI 10.1109/TVCG.2007.1044
   Laramee RS, 2004, COMPUT GRAPH FORUM, V23, P203, DOI 10.1111/j.1467-8659.2004.00753.x
   Laramee RS, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P131, DOI 10.1109/VISUAL.2003.1250364
   Lefebvre S, 2006, ACM T GRAPHIC, V25, P541, DOI 10.1145/1141911.1141921
   Lever J, 2012, VISUAL COMPUT, V28, P691, DOI 10.1007/s00371-012-0684-1
   Max Nelson., 1996, Flow visualization using moving textures
   Neyret F., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P147
   Onoue K, 2005, COMPUT GRAPH FORUM, V24, P51, DOI 10.1111/j.1467-8659.2005.00828.x
   Pla-Castells M, 2008, EUROGRAPHICS 2008 SH, P21
   Strikwerda J. C., 2004, FINITE DIFFERENCE SC
   Sumner RW, 1999, COMPUT GRAPH FORUM, V18, P17, DOI 10.1111/1467-8659.00299
   Tsuda Y, 2010, VISUAL COMPUT, V26, P883, DOI 10.1007/s00371-010-0491-5
   van Wijk JJ, 2002, ACM T GRAPHIC, V21, P745, DOI 10.1145/566570.566646
   Weiskopf D, 2004, PROC GRAPH INTERF, P263
   Yu QZ, 2011, IEEE T VIS COMPUT GR, V17, P1612, DOI 10.1109/TVCG.2010.263
   Yu QZ, 2009, COMPUT GRAPH FORUM, V28, P239, DOI 10.1111/j.1467-8659.2009.01363.x
NR 28
TC 0
Z9 0
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 1033
EP 1043
DI 10.1007/s00371-015-1118-7
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500028
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, XC
   Hu, JP
   Zhang, DB
   Qin, H
AF Wang, Xiaochao
   Hu, Jianping
   Zhang, Dongbo
   Qin, Hong
TI Efficient EMD and Hilbert spectra computation for 3D geometry processing
   and analysis via space-filling curve
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Empirical mode decomposition; Hilbert spectra; Space-filling curve;
   Surface processing
ID EMPIRICAL MODE DECOMPOSITION; ALGORITHM
AB Empirical Mode Decomposition (EMD) has proved to be an effective and powerful analytical tool for non-stationary time series and starts to exhibit its modeling potential for 3D geometry analysis. Yet, existing EMD-based geometry processing algorithms only concentrate on multi-scale data decomposition by way of computing intrinsic mode functions. More in-depth analytical properties, such as Hilbert spectra, are hard to study for 3D surface signals due to the lack of theoretical and algorithmic tools. This has hindered much more broader penetration of EMD-centric algorithms into various new applications on 3D surface. To tackle this challenge, in this paper we propose a novel and efficient EMD and Hilbert spectra computational scheme for 3D geometry processing and analysis. At the core of our scheme is the strategy of dimensionality reduction via space-filling curve. This strategy transforms the problem of 3D geometry analysis to 1D time series processing, leading to two major advantages. First, the envelope computation is carried out for 1D signal by cubic spline interpolation, which is much faster than existing envelope computation directly over 3D surface. Second, it enables us to calculate Hilbert spectra directly on 3D surface. We could take advantages of Hilbert spectra that contain a wealth of unexploited properties and utilize them as a viable indicator to guide our EMD-based 3D surface processing. Furthermore, to preserve sharp features, we develop a divide-and-conquer scheme of EMD by explicitly separating the feature signals from non-feature signals. Extensive experiments have been carried out to demonstrate that our new EMD and Hilbert spectra based method is both fast and powerful for 3D surface processing and analysis.
C1 [Wang, Xiaochao; Zhang, Dongbo] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Hu, Jianping] Northeast Dianli Univ, Coll Sci, Chuanying 132012, Jilin, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 Beihang University; Northeast Electric Power University; State
   University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook
RP Hu, JP (corresponding author), Northeast Dianli Univ, Coll Sci, Chuanying 132012, Jilin, Peoples R China.
EM wangxiaochao18@gmail.com; neduhjp307@163.com; zhangdongbo9212@163.com;
   qin@cs.stonybrook.edu
RI Hu, Jianping/IWM-3698-2023
OI Wang, Xiaochao/0000-0001-7852-2106
FU National Science Foundation of USA [IIS-0949467, IIS-1047715,
   IIS-1049448]; National Natural Science Foundation of China [61190120,
   61190121, 61190125, 61202261]; China Postdoctoral Science Foundation
   [2014M560875]; Scientific and Technological Development Plan of Jilin
   Province [20130522113JH]
FX We are thankful to Dr. Topraj Gurung and Dr. Mark Luffel for discussing
   and providing the code of Hamilton cycle generation. This work is
   supported in part by National Science Foundation of USA (IIS-0949467,
   IIS-1047715, and IIS-1049448), National Natural Science Foundation of
   China (No. 61190120, 61190121, 61190125, 61202261), China Postdoctoral
   Science Foundation (No. 2014M560875), Scientific and Technological
   Development Plan of Jilin Province (No. 20130522113JH). Models are
   courtesy ofAIM@SHAPE Repository.
CR Akleman E, 2013, COMPUT GRAPH-UK, V37, P316, DOI 10.1016/j.cag.2013.01.004
   Ali H, 2015, EXPERT SYST APPL, V42, P1261, DOI 10.1016/j.eswa.2014.08.049
   [Anonymous], 1890, Math. Ann.
   Arkin EM, 1996, VISUAL COMPUT, V12, P429
   Belyaev Alexander., 2003, P ISRAEL KOREA BINAT, P83
   Coifman RR, 2006, APPL COMPUT HARMON A, V21, P53, DOI 10.1016/j.acha.2006.04.004
   Di C., 2014, PLOS ONE, V9, pe104
   Fleishman S, 2003, ACM T GRAPHIC, V22, P950, DOI 10.1145/882262.882368
   Gao Y, 2013, IEEE T VIS COMPUT GR, V19, P178, DOI 10.1109/TVCG.2012.117
   Gurung T, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964962
   Guskov I, 1999, COMP GRAPH, P325, DOI 10.1145/311535.311577
   Hou TB, 2013, IEEE T VIS COMPUT GR, V19, P3, DOI 10.1109/TVCG.2012.111
   Hu JP, 2014, GRAPH MODELS, V76, P340, DOI 10.1016/j.gmod.2014.03.006
   Huang NE, 1998, P ROY SOC A-MATH PHY, V454, P903, DOI 10.1098/rspa.1998.0193
   Jones TR, 2003, ACM T GRAPHIC, V22, P943, DOI 10.1145/882262.882367
   Kopsinis Y, 2009, IEEE T SIGNAL PROCES, V57, P1351, DOI 10.1109/TSP.2009.2013885
   Mandic DP, 2013, IEEE SIGNAL PROC MAG, V30, P74, DOI 10.1109/MSP.2013.2267931
   Qin XJ, 2009, INT C COMP AID DES C, P606, DOI 10.1109/CADCG.2009.5246829
   Quinn JA, 2013, IEEE T VIS COMPUT GR, V19, P1143, DOI 10.1109/TVCG.2012.305
   Ren B, 2013, IEEE T VIS COMPUT GR, V19, P1708, DOI 10.1109/TVCG.2013.73
   Schroder P., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P161, DOI 10.1145/218380.218439
   Solomon J., 2014, ABS14054734 CORR
   Subr K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618493
   Taubin G, 2003, VISUALIZATION AND MATHEMATICS III, P69
   Taubin G., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P351, DOI 10.1145/218380.218473
   Wang H, 2012, GRAPH MODELS, V74, P173, DOI 10.1016/j.gmod.2012.04.005
   Wang RM, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2557449
   Wang SF, 2011, VISUAL COMPUT, V27, P429, DOI 10.1007/s00371-011-0582-y
   Wang XC, 2012, J ZHEJIANG U-SCI C, V13, P440, DOI 10.1631/jzus.C1100324
   Wang XC, 2012, COMPUT GRAPH-UK, V36, P101, DOI 10.1016/j.cag.2011.12.007
   Wu ZH, 2009, ADV DATA SCI ADAPT, V1, P339, DOI 10.1142/S1793536909000187
   Yoshizawa S, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P38
   Zang Y, 2014, IEEE T VIS COMPUT GR, V20, P1253, DOI 10.1109/TVCG.2014.2298017
   Zhang J, 2013, VISUAL COMPUT, V29, P717, DOI 10.1007/s00371-013-0808-2
   Zhao W, 2007, VISUAL COMPUT, V23, P987, DOI 10.1007/s00371-007-0167-y
   Zhong M, 2014, VISUAL COMPUT, V30, P751, DOI 10.1007/s00371-014-0971-0
NR 36
TC 13
Z9 17
U1 0
U2 18
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 1135
EP 1145
DI 10.1007/s00371-015-1100-4
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500037
DA 2024-07-18
ER

PT J
AU Abson, K
   Palmer, I
AF Abson, Karl
   Palmer, Ian
TI Motion capture: capturing interaction between human and animal
SO VISUAL COMPUTER
LA English
DT Article
DE Motion capture; Quadruped animation; Biomechanics; Data driven animation
ID LOCOMOTION
AB We introduce a new "marker-based" model for use in capturing equine movement. This model is informed by a sound biomechanical study of the animal and can be deployed in the pursuit of many undertakings. Unlike many other approaches, our method provides a high level of automation and hides the intricate biomechanical knowledge required to produce realistic results. Due to this approach, it is possible to acquire solved data with minimal manual intervention even in real-time conditions. The approach introduced can be replicated for the production of many other animals. The model is first informed by the veterinary world through studies of the subject's anatomy. Second, further medical studies aimed at understanding and addressing surface processes, inform model creation. The latter studies address items such as skin sliding. If not otherwise corrected these processes may hinder marker based capture. The resultant model has been tested in feasibility studies for practicality and subject acceptance during production. Data is provided for scrutiny along with the subject digitally captured through a variety of methods. The digital subject in mesh form as well as the motion capture model aid in comparison and show the level of accurateness achieved. The video reference and digital renders provide an insight into the level of realism achieved.
C1 [Abson, Karl; Palmer, Ian] Univ Bradford, Sch Engn & Informat, Bradford BD7 1DP, W Yorkshire, England.
C3 University of Bradford
RP Abson, K (corresponding author), Univ Bradford, Sch Engn & Informat, Bradford BD7 1DP, W Yorkshire, England.
EM K.Abson@scim.brad.ac.uk; I.Palmer@scim.brad.ac.uk
CR Abson K., 2013, MOTION CAPTURE CAPTU
   Activision, 2004, ROM TOT WAR
   Alton F, 1998, CLIN BIOMECH, V13, P434, DOI 10.1016/S0268-0033(98)00012-6
   American Humane, 2009, GUID SAF US AN FILM
   Andel C, 2009, GAIT POSTURE, V29, P123
   Andersson Technologies LLC, 2014, SYNTHEYES
   [Anonymous], 2012, BBC
   Back W., 2013, Equine Locomotion, V2
   Burn J F, 2001, Equine Vet J Suppl, P50
   Centroid 3D, 2012, HORS MOT CAPT SUITS
   Coros S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964954
   Dearborn M., 2009, FORD USES HOLLYWOOD
   Entertainment Weekly, 2011, ENTERTAINMENT WEEKLY
   Failes I., 2012, LIFE PI TIGERS TALE
   Favreau Laurent., 2004, Proceedings of the 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation, P277
   Grand National Guide: The Grand National, 2013, GRAND NAT GUID GRAND
   Hecke C., 2008, SIGGRAPH 08
   Hedges M., 2009, TREADMILLS NOT PREFE
   Higgins Gillian., 2012, Horse Anatomy for Performance: A Practical Guide to Training, Riding and Horse Care Devon
   Horváth G, 2009, CURR BIOL, V19, pR61, DOI 10.1016/j.cub.2008.12.011
   Hutchinson J., 2010, OUTDOOR MOTION CAPTU
   Ikinema, 2014, IK FOR MAYA
   Kenny C., 2013, FMX C VES SPEC
   Kirk AG, 2005, PROC CVPR IEEE, P782
   Lawson SEM, 2010, EQUINE VET J, V42, P552, DOI 10.1111/j.2042-3306.2010.00289.x
   Lee A., 2012, LIFE OF PI
   McGowan CP, 2008, J ANAT, V212, P153, DOI 10.1111/j.1469-7580.2007.00841.x
   Muybridge Eadweard., 2000, Animals in Motion, V1st
   New Line Cinema, 2011, LORD RINGS EXT ED AP
   Qiong Wu, 2011, 2011 XIII Symposium on Virtual Reality (SVR), P161, DOI 10.1109/SVR.2011.35
   Razali S., 2012, P 8 INT C INF SYST, P67
   Rockstar, 2010, Red Dead Redemption
   Sibley Brian., 2002, The Lord of the Rings: The Making of the Movie Trilogy
   Simmons M, 2002, NATION, V274, P2
   Skrba L, 2009, COMPUT GRAPH FORUM, V28, P1541, DOI 10.1111/j.1467-8659.2008.01312.x
   Spielberg Steven., 2011, War Horse
   Stavrakakis S., 2012, SOPHIAS PIGS FEATURE
   Ubisoft, 2007, PIPP FUNN 2
   Vicon, 2013, BLAD 2
   Vicon, 2012, OPT SYST
   Wampler K, 2013, COMPUT GRAPH FORUM, V32, P153, DOI 10.1111/cgf.12035
   Warner SM, 2010, EQUINE VET J, V42, P417, DOI 10.1111/j.2042-3306.2010.00200.x
   Xsens, 2011, SENS BAS EQ SUIT
NR 43
TC 3
Z9 4
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2015
VL 31
IS 3
BP 341
EP 353
DI 10.1007/s00371-014-0929-2
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CC3DB
UT WOS:000350223900008
DA 2024-07-18
ER

PT J
AU Zhang, L
   Xu, QK
   Nie, LZ
   Huang, H
AF Zhang, Lei
   Xu, Qian-Kun
   Nie, Lei-Zheng
   Huang, Hua
TI VideoGraph: a non-linear video representation for efficient exploration
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computational Visual Media Conference
CY 2013
CL Geneva, SWITZERLAND
DE Video scene; Sub-shot; Exploration
ID COLLAGE
AB In this paper we introduce VideoGraph, a novel non-linear representation for scene structure of a video. Unlike classical linear sequential organization, VideoGraph concentrates the video content across the time line by structuring scenes and materializes with two-dimensional graph, which enables non-linear exploration on the scenes and their transitions. To construct VideoGraph, we adopt a sub-shot induced method to evaluate the spatio-temporal similarity between shot segments of video. Then, scene structure is derived by grouping similar shots and identifying the valid transitions between scenes. The final stage is to represent the scene structure using a graph with respect to scene transition topology. Our VideoGraph can provide a condensed representation in the scene level and facilitate a non-linear manner to browse videos. Experimental results are presented to demonstrate the effectiveness and efficiency by using VideoGraph to explore and access the video content.
C1 [Zhang, Lei; Xu, Qian-Kun; Nie, Lei-Zheng; Huang, Hua] Beijing Inst Technol, Beijing Key Lab Intelligent Informat Technol, Sch Comp Sci, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology
RP Huang, H (corresponding author), Beijing Inst Technol, Beijing Key Lab Intelligent Informat Technol, Sch Comp Sci, Beijing 100081, Peoples R China.
EM huahuang@bit.edu.cn
RI Huang, Hua/M-9684-2013; Zhang, Lei/AAI-5726-2021
OI Huang, Hua/0000-0003-2587-1702; Zhang, Lei/0000-0002-1763-0397
FU National Natural Science Foundation of China [61133008, 61103159];
   Excellent Young Scholars Research Fund of Beijing Institute of
   Technology [2012YR0709]
FX We thank the anonymous reviewers for their valuable comments. This work
   was partly supported by the grants from the National Natural Science
   Foundation of China (No. 61133008, 61103159) and Excellent Young
   Scholars Research Fund of Beijing Institute of Technology (No.
   2012YR0709).
CR [Anonymous], 2009, P 17 ACM INT C MULT, DOI DOI 10.1145/1631272.1631383
   [Anonymous], P WWW
   [Anonymous], P 2006 IEEE COMP SOC
   Barnes C, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778826
   Caspi Y, 2006, VISUAL COMPUT, V22, P642, DOI 10.1007/s00371-006-0046-y
   Chen M, 2006, IEEE T VIS COMPUT GR, V12, P1093, DOI 10.1109/TVCG.2006.194
   Chen T, 2012, COMPUT GRAPH-UK, V36, P241, DOI 10.1016/j.cag.2012.02.010
   Cong L., 2001, VISUAL COMPUT, V27, P187
   Correa CD, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778825
   Corrigan T., 2004, The film experience: An introduction
   Daniel G, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P409, DOI 10.1109/VISUAL.2003.1250401
   Ellson J, 2002, LECT NOTES COMPUT SC, V2265, P483
   Erol B, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL III, PROCEEDINGS, P25
   Feng BL, 2011, VISUAL COMPUT, V27, P21, DOI 10.1007/s00371-010-0510-6
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Harary F., 1994, GRAPH THEORY
   Hu SM, 2013, VISUAL COMPUT, V29, P393, DOI 10.1007/s00371-013-0792-6
   Jia YT, 2005, VISUAL COMPUT, V21, P601, DOI 10.1007/s00371-005-0313-3
   Karrer T, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P247
   Kim JG, 2003, INT J IMAG SYST TECH, V13, P267, DOI 10.1002/ima.10067
   Kim Kihwan., 2006, ACM Multimedia, P655
   Li F. C., 2000, CHI 2000 Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2000. The Future is Here, P169, DOI 10.1145/332040.332425
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Lu S, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P1959, DOI 10.1109/ICME.2004.1394645
   Lu SP, 2013, IEEE T VIS COMPUT GR, V19, P1218, DOI 10.1109/TVCG.2012.145
   Mei T, 2009, VISUAL COMPUT, V25, P39, DOI 10.1007/s00371-008-0282-4
   Rother C, 2006, ACM T GRAPHIC, V25, P847, DOI 10.1145/1141911.1141965
   Schoeffmann K., 2002, INT WORKSH CONT BAS, V7, P243
   Smith MA, 1997, PROC CVPR IEEE, P775, DOI 10.1109/CVPR.1997.609414
   Sundaram Hari., 2002, COMUNICA O APRESENTA, DOI [DOI 10.1145/641007.641042, 10.1145/641007.641042]
   Tang Lin-Xie., 2009, Proceedings of the 17th ACM international conference on Multimedia, P351
   Teodosio L, 2005, ACM T MULTIM COMPUT, V1, P16, DOI 10.1145/1047936.1047940
   Tollis I.G., 1998, Graph drawing: Algorithms for the visualization of graphs, V1
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Yu L, 2010, COMPUT GRAPH FORUM, V29, P2271, DOI 10.1111/j.1467-8659.2010.01816.x
   Zhang L, 2012, COMPUT GRAPH FORUM, V31, P2173, DOI 10.1111/j.1467-8659.2012.03210.x
NR 36
TC 9
Z9 9
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2014
VL 30
IS 10
BP 1123
EP 1132
DI 10.1007/s00371-013-0882-5
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AP6NF
UT WOS:000342193800006
DA 2024-07-18
ER

PT J
AU Bhattacharya, P
   Gavrilova, M
AF Bhattacharya, Priyadarshi
   Gavrilova, Marina L.
TI Rotation invariance for dense features inside regions of interest
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Computer vision; Landmark recognition; Rotation invariance; Regions of
   interest; Dense features
ID SCALE; CONSISTENCY
AB Interest points have traditionally been favoured over dense features for image retrieval tasks, where the goal is to retrieve images similar to a query image from an image corpus. While interest points are invariant to scale and rotation, their coverage of the image is not adequate for sub-image retrieval problems, where the query image occupies a small part of the corpus image. On the other hand, dense features provide excellent coverage but lack invariance, as they are computed at a fixed scale and orientation. Recently, we proposed a novel technique of combining dense features with interest points (Bhattacharya and Gavrilova, Vis Comput 29(6-8):491-499, 2013) to leverage the benefits of both worlds. This allows dense features to be scale invariant but not rotation invariant. In this paper, we build on this framework by incorporating rotation invariance for dense features and introducing several improvements in the voting and match score computation stages. Our method can produce high-quality recognition results that outperform bag of words even with geometric verification and several state-of-art methods that have considered spatial information. We achieve significant improvements in terms of both search speed and accuracy over (Bhattacharya and Gavrilova, Vis Comput 29(6-8):491-499, 2013). Experiments on Oxford Buildings, Holidays and UKbench datasets reveal that our method is not only robust to viewpoint and scale changes that occur in real-world photographs but also to geometric transformations.
C1 [Bhattacharya, Priyadarshi; Gavrilova, Marina L.] Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
C3 University of Calgary
RP Bhattacharya, P (corresponding author), Univ Calgary, Dept Comp Sci, 2500 Univ Dr NW, Calgary, AB T2N 1N4, Canada.
EM bhattacp@ucalgary.ca; mgavrilo@ucalgary.ca
RI Gavrilova, Marina/AHD-3605-2022
OI Gavrilova, Marina/0000-0002-5338-1834
CR [Anonymous], 2007, IEEE C COMP VIS PATT
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Bhattacharya P, 2013, VISUAL COMPUT, V29, P491, DOI 10.1007/s00371-013-0813-5
   Bosch A., 2007, P 6 ACM INT C IMAGE, P401, DOI DOI 10.1145/1282280.1282340
   Cao Y, 2010, PROC CVPR IEEE, P3352, DOI 10.1109/CVPR.2010.5540021
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2011, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2011.5995601
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jégou H, 2009, PROC CVPR IEEE, P1169, DOI 10.1109/CVPRW.2009.5206609
   Lin Z, 2010, LECT NOTES COMPUT SC, V6316, P294, DOI 10.1007/978-3-642-15567-3_22
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matas J., 2002, Electronic Proceedings of the 13th British Machine Vision Conference, P384
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Mikulik A, 2013, INT J COMPUT VISION, V103, P163, DOI 10.1007/s11263-012-0600-1
   Nister David, 2006, CVPR
   Philbin J., 2008, P CVPR, P1
   Vedaldi A., VLFEAT OPEN PORTABLE
   Wang XY, 2011, IEEE I CONF COMP VIS, P209, DOI 10.1109/ICCV.2011.6126244
   Wu Z, 2009, PROC CVPR IEEE, P25, DOI 10.1109/CVPRW.2009.5206566
   Zhang YM, 2011, PROC CVPR IEEE, P809, DOI 10.1109/CVPR.2011.5995528
   Zhao W., LIP VIREO LOCAL INTE
NR 23
TC 1
Z9 1
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 569
EP 578
DI 10.1007/s00371-014-0964-z
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700002
DA 2024-07-18
ER

PT J
AU Fortunato, HE
   Oliveira, MM
AF Fortunato, Horacio E.
   Oliveira, Manuel M.
TI Fast high-quality non-blind deconvolution using sparse adaptive priors
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Non-blind deconvolution; Adaptive priors; Deblurring; Computational
   photography
ID IMAGE; CAMERA; DEPTH
AB We present an efficient approach for high-quality non-blind deconvolution based on the use of sparse adaptive priors. Its regularization term enforces preservation of strong edges while removing noise. We model the image-prior deconvolution problem as a linear system, which is solved in the frequency domain. This clean formulation lends to a simple and efficient implementation. We demonstrate its effectiveness by performing an extensive comparison with existing non-blind deconvolution methods, and by using it to deblur photographs degraded by camera shake. Our experiments show that our solution is faster and its results tend to have higher peak signal-to-noise ratio than the state-of-the-art techniques. Thus, it provides an attractive alternative to perform high-quality non-blind deconvolution of large images, as well as to be used as the final step of blind-deconvolution algorithms.
C1 [Fortunato, Horacio E.] Laureate Int Univ, Uniritter, Porto Alegre, RS, Brazil.
   [Oliveira, Manuel M.] Univ Fed Rio Grande do Sul, Inst Informat, Porto Alegre, RS, Brazil.
C3 Universidade Federal do Rio Grande do Sul
RP Fortunato, HE (corresponding author), Laureate Int Univ, Uniritter, Porto Alegre, RS, Brazil.
EM horacio_fortunato@uniritter.edu.br; oliveira@inf.ufrgs.br
RI ; Menezes de Oliveira Neto, Manuel/H-1508-2011
OI Fortunato, Horacio E./0009-0000-1480-1392; Menezes de Oliveira Neto,
   Manuel/0000-0003-4957-9984
FU CNPq [482271/2012-4, 308936/2010-8]
FX This work was sponsored by CNPq ( Grants 482271/2012-4 and
   308936/2010-8). We thank the authors of the compared techniques for
   making their code available, and Shan et al. for providing the
   photographs and kernels shown in Figs. 9 and 10.
CR [Anonymous], 2009, IEEE INT C COMPUTATI
   [Anonymous], SIGNAL PROCESS MAG I
   [Anonymous], ACM TOG
   Ben-Ezra M, 2004, IEEE T PATTERN ANAL, V26, P689, DOI 10.1109/TPAMI.2004.1
   Bonesky T, 2009, INVERSE PROBL, V25, DOI 10.1088/0266-5611/25/1/015015
   Cai JF, 2009, PROC CVPR IEEE, P104, DOI 10.1109/CVPRW.2009.5206743
   Campisi P., 2007, Blindimagedeconvolution:theoryandapplications
   Cho S.-M., 2011, Proceedings of AISTech 2011, P1
   Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491
   Cho TS, 2012, IEEE T PATTERN ANAL, V34, P683, DOI 10.1109/TPAMI.2011.166
   Engl H.W., 1996, REGULARIZATION INVER
   Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956
   Fortunato HE, 2012, COMPUT GRAPH FORUM, V31, P459, DOI 10.1111/j.1467-8659.2012.03025.x
   Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964
   GEMAN D, 1995, IEEE T IMAGE PROCESS, V4, P932, DOI 10.1109/83.392335
   Joshi N, 2009, PROC CVPR IEEE, P1550, DOI 10.1109/CVPRW.2009.5206802
   Kodak, KOD LOSSL TRUE COL I
   Krishnan D., 2009, P ADV NEURAL INFORM, V22, P1033
   Levin A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2657, DOI 10.1109/CVPR.2011.5995308
   Levin A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239521
   Levin A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360670
   Liu RT, 2008, IEEE IMAGE PROC, P505, DOI 10.1109/ICIP.2008.4711802
   LUCY LB, 1974, ASTRON J, V79, P745, DOI 10.1086/111605
   Shan Q., 2007, ICCV 2007, P1
   Sibarita JB, 2005, ADV BIOCHEM ENG BIOT, V95, P201, DOI 10.1007/b102215
   Starck JL, 2002, PUBL ASTRON SOC PAC, V114, P1051, DOI 10.1086/342606
   Tai YW, 2013, IEEE T PATTERN ANAL, V35, P2498, DOI 10.1109/TPAMI.2013.40
   Tikhonov A., 1977, Solution of Ill-Posed Problems
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wang YL, 2008, SIAM J IMAGING SCI, V1, P248, DOI 10.1137/080724265
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157
   Yuan L, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360673
NR 33
TC 30
Z9 39
U1 1
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 661
EP 671
DI 10.1007/s00371-014-0966-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700010
DA 2024-07-18
ER

PT J
AU Luboz, V
   Kyaw-Tun, J
   Sen, S
   Kneebone, R
   Dickinson, R
   Kitney, R
   Bello, F
AF Luboz, Vincent
   Kyaw-Tun, Jim
   Sen, Sayan
   Kneebone, Roger
   Dickinson, Robert
   Kitney, Richard
   Bello, Fernando
TI Real-time stent and balloon simulation for stenosis treatment
SO VISUAL COMPUTER
LA English
DT Article
DE Simulation; Interventional radiology; Training; Mass-spring model;
   Stent; Balloon
ID PERFORMANCE
AB The base of all training in interventional radiology relies on the development of the core skills in manipulating the instruments. Computer simulators are emerging to help in this task with virtual reality simulators and haptic devices. This paper extends our previous interventional radiology training framework with two new virtual instruments: a stent and a balloon, both driven by the previously existing catheters and guidewires. Both the balloon and stent are used to treat stenosis, a partial or total blockage of an artery. The inflating balloon and/or the expanding stent are used to increase and maintain the radius of the artery during the intervention. A mass-spring particle system is used to model the new instruments. This modelling allows realistic instrument behaviour in real time, even in complex vascular anatomies, because of efficient collision interactions. The collision response is based on four constraints applied to the instruments: external force, spring force, and 2D and 3D bending forces. The behaviour of our simulated instruments has been visually assessed by experienced interventional radiologists, who described it as realistic and potentially helpful for training.
C1 [Luboz, Vincent; Kyaw-Tun, Jim; Kneebone, Roger; Bello, Fernando] Univ London Imperial Coll Sci Technol & Med, SORA, Biosurg & Surg Technol Dept, St Marys Hosp, London W2 1NY, England.
   [Luboz, Vincent] Equipe GMCAO, Lab TIMC, F-38706 La Tronche, France.
   [Sen, Sayan] Univ London Imperial Coll Sci Technol & Med, Natl Heart & Lung Inst, Int Ctr Circulatory Hlth, London, England.
   [Dickinson, Robert; Kitney, Richard] Univ London Imperial Coll Sci Technol & Med, Dept Bioengn, London SW7 2AZ, England.
C3 Imperial College London; Communaute Universite Grenoble Alpes;
   Universite Grenoble Alpes (UGA); Imperial College London; Imperial
   College London
RP Luboz, V (corresponding author), Univ London Imperial Coll Sci Technol & Med, SORA, Biosurg & Surg Technol Dept, St Marys Hosp, London W2 1NY, England.
EM vluboz@imag.fr; j.kyaw-tun@imperial.ac.uk; sayan.sen@imperial.ac.uk;
   roger.kneebone@imperial.ac.uk; robert.dickinson@imperial.ac.uk;
   r.kitney@imperial.ac.uk; fernando.bello@imperial.ac.uk
RI Bello, Fernando/E-3081-2013
OI Kitney, Richard/0000-0002-6499-5209; Bello, Fernando/0000-0003-4136-0355
FU EPSRC, the UK Engineering and Physical Sciences Research Council; EPSRC
   [EP/E005020/1] Funding Source: UKRI
FX This work was partly funded by the Digital Economy program of the EPSRC,
   the UK Engineering and Physical Sciences Research Council.
CR Chaer RA, 2006, ANN SURG, V244, P343, DOI 10.1097/01.sla.0000234932.88487.75
   Duerig TW, 2002, MINIM INVASIV THER, V11, P173, DOI 10.1080/136457002760273386
   Lenoir J, 2006, STUD HEALTH TECHNOL, V119, P305
   Luboz V, 2009, VISUAL COMPUT, V25, P827, DOI 10.1007/s00371-009-0312-x
   Seymour NE, 2002, ANN SURG, V236, P458, DOI 10.1097/00000658-200210000-00008
   Zahedmanesh H, 2010, J BIOMECH, V43, P2126, DOI 10.1016/j.jbiomech.2010.03.050
   Záhora J, 2007, PHYSIOL RES, V56, pS115, DOI 10.33549/physiolres.931309
   Zhou X., 2010, P VIRT REAL INT C VR
NR 8
TC 7
Z9 7
U1 1
U2 13
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2014
VL 30
IS 3
BP 341
EP 349
DI 10.1007/s00371-013-0859-4
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AB3LU
UT WOS:000331693100008
DA 2024-07-18
ER

PT J
AU Yoo, I
   Vanek, J
   Nizovtseva, M
   Adamo-Villani, N
   Benes, B
AF Yoo, Innfarn
   Vanek, Juraj
   Nizovtseva, Maria
   Adamo-Villani, Nicoletta
   Benes, Bedrich
TI Sketching human character animations by composing sequences from large
   motion database
SO VISUAL COMPUTER
LA English
DT Article
DE Sketching; Character animation; Interactive systems
ID INTERFACE
AB Quick creation of 3D character animations is an important task in game design, simulations, forensic animation, education, training, and more. We present a framework for creating 3D animated characters using a simple sketching interface coupled with a large, unannotated motion database that is used to find the appropriate motion sequences corresponding to the input sketches. Contrary to the previous work that deals with static sketches, our input sketches can be enhanced by motion and rotation curves that improve matching in the context of the existing animation sequences. Our framework uses animated sequences as the basic building blocks of the final animated scenes, and allows for various operations with them such as trimming, resampling, or connecting by use of blending and interpolation. A database of significant and unique poses, together with a two-pass search running on the GPU, allows for interactive matching even for large amounts of poses in a template database. The system provides intuitive interfaces, an immediate feedback, and poses very small requirements on the user. A user study showed that the system can be used by novice users with no animation experience or artistic talent, as well as by users with an animation background. Both groups were able to create animated scenes consisting of complex and varied actions in less than 20 minutes.
C1 [Yoo, Innfarn; Benes, Bedrich] Purdue Univ, Dept Comp Graph Technol, W Lafayette, IN 47907 USA.
C3 Purdue University System; Purdue University
RP Yoo, I (corresponding author), Purdue Univ, Dept Comp Graph Technol, W Lafayette, IN 47907 USA.
EM yooi@purdue.edu
RI Benes, Bedrich/A-8150-2016
OI Benes, Bedrich/0000-0002-5293-2112; Adamo, Nicoletta/0000-0001-8311-5302
FU NSF [EIA-0196217]
FX The data used in this project was obtained from mocap.cs.cmu.edu. The
   database was created with funding from NSF EIA-0196217.
CR [Anonymous], 2002, ACM T GRAPHIC
   [Anonymous], 2010, SCA'10: proceedings of the 2010 ACM SIGGRAPH/Eurographics symposium on computer animation, DOI [10.2312/SCA/SCA10/001-010, DOI 10.2312/SCA/SCA10/001-010]
   [Anonymous], 2005, P ACM SIGGRAPH EUR S, DOI [10.1145/1073368.1073377, DOI 10.1145/1073368.1073377]
   [Anonymous], 2012, IEEE T VIS COMPUT GR, DOI DOI 10.1109/TVCG.2011.53
   Arikan O, 2002, ACM T GRAPHIC, V21, P483, DOI 10.1145/566570.566606
   Chaudhuri P, 2004, COMPUT GRAPH FORUM, V23, P411, DOI 10.1111/j.1467-8659.2004.00772.x
   Choi MG, 2012, COMPUT GRAPH FORUM, V31, P2057, DOI 10.1111/j.1467-8659.2012.03198.x
   Davis J., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P320
   Gingold Y, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1618452.1616494, 10.1145/1618452.1618494]
   Hecker C., 2008, ACM SIGGRAPH 2008 SI, P27
   Ho ESL, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778770
   Igarashi T, 1999, COMP GRAPH, P409, DOI 10.1145/311535.311602
   Igarashi T., 2005, Proceedings of the 2005 acm siggraph/eurographics symposium on computer animation, P107, DOI DOI 10.1145/1073368.1073383
   Ishigaki S., 2009, ACM SIGGRAPH 2009 SI
   Jain Eakta., 2009, Proceedings of SCA 2009, P93, DOI DOI 10.1145/1599470.1599483
   Lau M, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640446
   Lee YJ, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964922
   Li Q., 2006, Proceedings of SCA 2006, P233
   Lin Y., 2006, Proceedings of the 4th international conference on computer graphics and interactive techniques in australasia and southeast asia, P93, DOI DOI 10.1145/1174429.1174444
   Lo WY, 2010, COMPUT GRAPH FORUM, V29, P563, DOI 10.1111/j.1467-8659.2009.01626.x
   Mao C., 2006, P 2006 ACM SIGCHI IN
   Mao C., 2005, EUR WORKSH SKETCH BA, P175
   Min JY, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640452
   Muico U, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966395
   Parameswaran V, 2004, PROC CVPR IEEE, P16
   Pullen K, 2002, ACM T GRAPHIC, V21, P501
   Rivers A., 2010, ACM SIGGRAPH 2010 SI
   Sakamoto Y., 2004, Proceedings of the 2004 acm siggraph/eurographics symposium on computer animation, P259, DOI DOI 10.1145/1028523.1028557
   Seol Y, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024196
   Shoemaker K., 1985, Computer Graphics, V19, P245, DOI 10.1145/325165.325242
   Thorne M, 2004, ACM T GRAPHIC, V23, P424, DOI 10.1145/1015706.1015740
   Ulicny Branislav, 2004, SCA'04'- Proc. of the 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation, P243, DOI [DOI 10.2312/SCA/SCA04/243-252, 10.1145/1028523.1028555, DOI 10.1145/1028523.1028555]
   Wang J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330512
   Wei XLK, 2011, IEEE COMPUT GRAPH, V31, P78, DOI 10.1109/MCG.2009.132
   Williams R., 2009, The animator's survival kit
   Zeleznik R. C., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P163, DOI 10.1145/237170.237238
NR 36
TC 19
Z9 20
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2014
VL 30
IS 2
BP 213
EP 227
DI 10.1007/s00371-013-0797-1
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AA9DL
UT WOS:000331393900007
DA 2024-07-18
ER

PT J
AU Xie, HR
   Miyata, K
AF Xie, Haoran
   Miyata, Kazunori
TI Real-time simulation of lightweight rigid bodies
SO VISUAL COMPUTER
LA English
DT Article
DE Natural phenomenon; Motion synthesis; Lightweight rigid body; Real-time
   simulation
AB Unlike common rigid bodies, lightweight rigid bodies have special and spectacular motions that are known as free fall, such as fluttering (oscillation from side to side) and tumbling (rotation and sideways drifting). However, computer graphics applications cannot simulate the dynamics of lightweight rigid bodies in various environments realistically and efficiently. In this study, we first analyze the physical characteristics of free-fall motions in quiescent flow and propose a new procedural motion-synthesis method for modeling free-fall motions in interactive environments. Six primitive motions of lightweight rigid bodies are defined in a phase diagram and analyzed separately using a trajectory-search tree and precomputed trajectory database. The global paths of free-fall motions are synthesized on the basis of these primitive motions by using a free-fall motion graph whose edges are connected in the Markov-chain model. Then, our approach integrates external forces (e.g., a wind field) by using an improved noise-based algorithm under different force magnitudes and object release heights. This approach exhibits not only realistic simulation results in various environments but also fast computation to meet real-time requirements.
C1 [Xie, Haoran; Miyata, Kazunori] Japan Adv Inst Sci & Technol, Sch Knowledge Sci, Kanazawa, Ishikawa, Japan.
C3 Japan Advanced Institute of Science & Technology (JAIST)
RP Xie, HR (corresponding author), Japan Adv Inst Sci & Technol, Sch Knowledge Sci, Kanazawa, Ishikawa, Japan.
EM xiehr@jaist.ac.jp; miyata@jaist.ac.jp
RI Xie, Haoran/ADP-8087-2022; Xie, Haoran/V-6397-2019
OI Xie, Haoran/0000-0002-6926-3082; Xie, Haoran/0000-0002-6926-3082;
   Miyata, Kazunori/0000-0002-1582-0058
CR Andersen A, 2005, J FLUID MECH, V541, P65, DOI 10.1017/S002211200500594X
   Aoki K., 2004, J ADV COMPUT INTELL, V8, P223
   Arikan O, 2002, ACM T GRAPHIC, V21, P483, DOI 10.1145/566570.566606
   Field SB, 1997, NATURE, V388, P252, DOI 10.1038/40817
   Haevre W.V., 2006, EUR WORKSH NAT PHEN, P75, DOI [10.2312/NPH/NPH06/075-082, DOI 10.2312/NPH/NPH06/075-082]
   Haiyan L., 2010, 2010 INT C COMP DES, V5
   Jones MA, 2005, J FLUID MECH, V540, P393, DOI 10.1017/S0022112005005859
   Khorloo Oyundolgor, 2011, International Journal of Virtual Reality, V10, P53
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Mahadevan L, 1999, PHYS FLUIDS, V11, P1, DOI 10.1063/1.869919
   Maxwell JC, 1854, Cambridge Dublin Math. J, V9, P145, DOI DOI 10.1017/CBO9780511698095.008
   OLESEN HR, 1984, BOUND-LAY METEOROL, V29, P285, DOI 10.1007/BF00119794
   Ota S, 2004, VISUAL COMPUT, V20, P613, DOI 10.1007/s00371-004-0266-y
   Razavi P., 2010, ARXIV E PRINTS
   Reissell LM, 2001, COMPUT GRAPH FORUM, V20, pC339
   Shi L, 2005, VISUAL COMPUT, V21, P474, DOI 10.1007/s00371-005-0296-0
   Shinya M., 1992, Computer Graphics Forum, V11, pC119, DOI 10.1111/1467-8659.1130119
   Stam J., 1993, Computer Graphics Proceedings, P369, DOI 10.1145/166117.166163
   STRINGHAM GE, 1969, GEOL SURV PROF PAP C, V562
   TANABE Y, 1994, PHYS REV LETT, V73, P1372, DOI 10.1103/PhysRevLett.73.1372
   Thuillier R.H. U.O. Lappe., 1964, J. Appl. Meteor, V3, P299, DOI DOI 10.1175/1520-0450(1964)003,0299:WATPCF.2.0.CO;2
   Vazquez P.-P., 2008, J VIRTUAL REAL BROAD, V5
   Wei XM, 2004, IEEE T VIS COMPUT GR, V10, P719, DOI 10.1109/TVCG.2004.48
   Weissmann S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185600
   WILLMARTH WW, 1964, PHYS FLUIDS, V7, P197, DOI 10.1063/1.1711133
   Zhang L, 2007, COMPUT ANIMAT VIRT W, V18, P371, DOI 10.1002/cav.205
   Zhong HJ, 2011, PHYS FLUIDS, V23, DOI 10.1063/1.3541844
NR 27
TC 1
Z9 1
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2014
VL 30
IS 1
BP 81
EP 92
DI 10.1007/s00371-013-0783-7
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 291AR
UT WOS:000329800500007
DA 2024-07-18
ER

PT J
AU Shaikh, AA
   Kumar, DK
   Gubbi, J
AF Shaikh, Ayaz A.
   Kumar, Dinesh K.
   Gubbi, Jayavardhana
TI Automatic visual speech segmentation and recognition using directional
   motion history images and Zernike moments
SO VISUAL COMPUTER
LA English
DT Article
DE Motion analysis; Temporal segmentation; Directional motion history
   image; Optical flow; Zernike moments
ID FEATURES; EXTRACTION
AB Appearance-based visual speech recognition using only video signals is presented. The proposed technique is based on the use of directional motion history images (DMHIs), which is an extension of the popular optical-flow method for object tracking. Zernike moments of each DMHI are computed in order to perform the classification. The technique incorporates automatic temporal segmentation of isolated utterances. The segmentation of isolated utterance is achieved using pair-wise pixel comparison. Support vector machine is used for classification and the results are based on leave-one-out paradigm. Experimental results show that the proposed technique achieves better performance in visemes recognition than others reported in literature. The benefit of this proposed visual speech recognition method is that it is suitable for real-time applications due to quick motion tracking system and the fast classification method employed. It has applications in command and control using lip movement to text conversion and can be used in noisy environment and also for assisting speech impaired persons.
C1 [Shaikh, Ayaz A.; Kumar, Dinesh K.] RMIT Univ, Sch Elect & Comp Engn, Melbourne, Vic 3001, Australia.
   [Shaikh, Ayaz A.; Kumar, Dinesh K.] RMIT Univ, Hlth Innovat Res Inst, Melbourne, Vic 3001, Australia.
   [Gubbi, Jayavardhana] Univ Melbourne, Dept Elect & Elect Engn, ISSNIP, Melbourne, Vic 3010, Australia.
C3 Royal Melbourne Institute of Technology (RMIT); Royal Melbourne
   Institute of Technology (RMIT); University of Melbourne
RP Shaikh, AA (corresponding author), RMIT Univ, Sch Elect & Comp Engn, Melbourne, Vic 3001, Australia.
EM ayazahmed.shaikh@rmit.edu.au; dinesh@rmit.edu.au; jgl@unimelb.edu.au
RI Gubbi, Jayavardhana/ABF-9315-2020; KUMAR, DINESH/GRJ-5448-2022; KUMAR,
   DINESH/JTT-7703-2023; Kumar, Dinesh k/D-8488-2011
OI KUMAR, DINESH/0000-0002-9641-6662; Kumar, Dinesh/0000-0003-3602-4023
CR Ahad Md Atiqur Rahman, 2010, Journal of Multimedia, V5, P36, DOI 10.4304/jmm.5.1.36-46
   [Anonymous], 1997, The HTK book
   [Anonymous], INT C AUD VIS SPEECH
   Arjunan Sridhar P, 2006, Conf Proc IEEE Eng Med Biol Soc, V2006, P2191
   Babu RV, 2004, IMAGE VISION COMPUT, V22, P597, DOI 10.1016/j.imavis.2003.11.004
   BREGLER C, 1994, INT CONF ACOUST SPEE, P669, DOI 10.1109/ICASSP.1994.389567
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Da Silveira L, 2003, XVI BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, PROCEEDINGS, P399, DOI 10.1109/SIBGRA.2003.1241036
   Duchnowski P., 1994, ICSLP 94. 1994 International Conference on Spoken Language Processing, P547
   Ganapathiraju A., 2000, Speech Transcription Workshop, P504
   GOLDSCHEN AJ, 1994, CONF REC ASILOMAR C, P572, DOI 10.1109/ACSSC.1994.471517
   Gordan M, 2002, EURASIP J APPL SIG P, V2002, P1248, DOI 10.1155/S1110865702207039
   Govokhina O., 2007, LEARNING OPTIMAL AUD
   Hueber T., 2007, 16 INT C PHON SCI, P2193
   Iwano K., 2007, SIGNAL PROCESS, V4
   Iwano K, 2007, EURASIP J AUDIO SPEE, DOI 10.1155/2007/64506
   Kass M., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P259, DOI 10.1007/BF00133570
   KHOTANZAD A, 1990, IEEE T PATTERN ANAL, V12, P489, DOI 10.1109/34.55109
   Koprinska I, 2001, SIGNAL PROCESS-IMAGE, V16, P477, DOI 10.1016/S0923-5965(00)00011-4
   KRONE G, 1997, P EUR TUT WORKS AUD, P57
   Luettin J., 1996, SPEECHREADING HUMANS, V150, P383
   Ma JY, 2004, COMPUT ANIMAT VIRT W, V15, P485, DOI 10.1002/cav.11
   Mase K., 1991, Systems and Computers in Japan, V22, P67
   Matthews I, 2002, IEEE T PATTERN ANAL, V24, P198, DOI 10.1109/34.982900
   Meng HY, 2007, VISAPP 2007: PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOLUME IU/MTSV, P21
   Musti U, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1401
   OTANI K, 1995, IEEE J SEL AREA COMM, V13, P42, DOI 10.1109/49.363147
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Papandreou G, 2009, IEEE T AUDIO SPEECH, V17, P423, DOI 10.1109/TASL.2008.2011515
   Petajan E. D., 1984, IEEE Global Telecommunications Conference, GLOBECOM '84 Conference Record. `Communications in the Information Age' (Cat. No. 84CH2064-4), P265
   Potamianos G, 2003, P IEEE, V91, P1306, DOI 10.1109/JPROC.2003.817150
   Potamianos G, 2001, INT CONF ACOUST SPEE, P165, DOI 10.1109/ICASSP.2001.940793
   Schultz T, 2010, SPEECH COMMUN, V52, P341, DOI 10.1016/j.specom.2009.12.002
   Silsbee PL, 1996, IEEE T SPEECH AUDI P, V4, P337, DOI 10.1109/89.536928
   Soquet A., 1999, COMPLEMENTARY CUES S, P1645
   Su Q., 2002, P INT C SPOK LANG PR, P42
   Sun DQ, 2008, LECT NOTES COMPUT SC, V5304, P83
   Tan ZH, 2010, IEEE J-STSP, V4, P798, DOI 10.1109/JSTSP.2010.2057192
   TEH CH, 1988, IEEE T PATTERN ANAL, V10, P496, DOI 10.1109/34.3913
   Valstar M, 2004, RO-MAN 2004: 13TH IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, PROCEEDINGS, P253, DOI 10.1109/ROMAN.2004.1374768
   Weinland D, 2006, COMPUT VIS IMAGE UND, V104, P249, DOI 10.1016/j.cviu.2006.07.013
   Xiang T, 2006, INT J COMPUT VISION, V67, P21, DOI 10.1007/s11263-006-4329-6
   Xiaodong Cui, 2005, IEEE Transactions on Speech and Audio Processing, V13, P1161, DOI 10.1109/TSA.2005.853002
   Xu HT, 2008, IEEE SIGNAL PROC LET, V15, P701, DOI 10.1109/LSP.2008.2001820
   Yau W. C, 2008, VIDEO ANAL MOUTH MOV
   Yau WC, 2008, LECT NOTES COMPUT SC, V5197, P610, DOI 10.1007/978-3-540-85920-8_74
   Yau WC, 2008, INT J IMAGE GRAPH, V8, P419, DOI 10.1142/S0219467808003167
   Yau WC, 2006, C RES PRACTICE INFOR, P93
   YUHAS BP, 1989, IEEE COMMUN MAG, V27, P65, DOI 10.1109/35.41402
   YUILLE AL, 1992, INT J COMPUT VISION, V8, P99, DOI 10.1007/BF00127169
   Zhao GY, 2009, IEEE T MULTIMEDIA, V11, P1254, DOI 10.1109/TMM.2009.2030637
NR 51
TC 4
Z9 4
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2013
VL 29
IS 10
SI SI
BP 969
EP 982
DI 10.1007/s00371-012-0751-7
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 227LV
UT WOS:000325115400001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Kong, Y
   Dong, WM
   Mei, X
   Zhang, XP
   Paul, JC
AF Kong, Yan
   Dong, Weiming
   Mei, Xing
   Zhang, Xiaopeng
   Paul, Jean-Claude
TI SimLocator: robust locator of similar objects in images
SO VISUAL COMPUTER
LA English
DT Article
DE Similar objects; Object descriptor; Object matching; Stable locations;
   Contour extraction
ID ALGORITHM
AB Similar objects commonly appear in natural images, and locating and cutting out these objects can be tedious when using classical interactive image segmentation methods. In this paper, we propose SimLocator, a robust method oriented to locate and cut out similar objects with minimum user interaction. After extracting an arbitrary object template from the input image, candidate locations of similar objects are roughly detected by distinguishing the shape and color features of each image. A novel optimization method is then introduced to select accurate locations from the two sets of candidates. Additionally, a matting-based method is used to improve the results and to ensure that all similar objects are located in the image. Finally, a method based on alpha matting is utilized to extract the precise object contours. To ensure the performance of the matting operation, this work has developed a new method for foreground extraction. Experiments show that SimLocator is more robust and more convenient to use compared to other more advanced repetition detection and interactive image segmentation methods, in terms of locating similar objects in images.
C1 [Kong, Yan; Dong, Weiming; Mei, Xing; Zhang, Xiaopeng] Chinese Acad Sci, LIAMA NLPR, Inst Automat, Beijing, Peoples R China.
   [Paul, Jean-Claude] INRIA, Project CAD, Paris, France.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Inria
RP Dong, WM (corresponding author), Chinese Acad Sci, LIAMA NLPR, Inst Automat, Beijing, Peoples R China.
EM wmdong@nlpr.ia.ac.cn; xmei@nlpr.ia.ac.cn; xpzhang@nlpr.ia.ac.cn
RI DONG, Weiming/AAG-7678-2020
OI DONG, Weiming/0000-0001-6502-145X
FU National Natural Science Foundation of China [61172104, 61271430,
   61201402, 61202324]; Beijing Natural Science Foundation [4112061]; SRF
   for ROCS, SEM
FX We thank anonymous reviewers for their valuable input. We thank Fuzhang
   Wu for making some Dual-Bound results. We thank the flickr members who
   kindly share their images under Creative Commons License: Bestfriend
   (tea cups), RenateEurope (purple flowers), acaffery (balloons), Luko
   Gecko (fish school), Bold Huang (petunia), Swami Stream (chocolate
   cake). We thank the users of 500px.com who have shared their images
   through public domain: Ricky Marek (pomegranates), Nitin Prabhudesai
   (dandelions), Joram Huyben (green lanterns). We also thank the users of
   pinterest.com who have put the media in their spaces: Gus's Mom (panda
   cakes), Rhonda E. Peterson (meatballs), Cindy Loo (fried balls). The two
   fruit cake images are borrowed from [10]. This work is supported by
   National Natural Science Foundation of China under project Nos.
   61172104, 61271430, 61201402 and 61202324, by Beijing Natural Science
   Foundation (Content Aware Image Synthesis and its Applications, No.
   4112061), and by SRF for ROCS, SEM.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Ahuja N, 2007, IEEE I CONF COMP VIS, P770
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Berg AC, 2005, PROC CVPR IEEE, P26
   Chen QF, 2012, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2012.6247760
   Cheng MM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778820
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Forssen P.-E., 2007, IEEE International Conference on Computer Vision (ICCV), P1, DOI DOI 10.1109/CVPR.2007.383120
   Huang H, 2011, COMPUT GRAPH FORUM, V30, P2059, DOI 10.1111/j.1467-8659.2011.02044.x
   Joulin Armand, 2010, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2010.5539868
   Kim E, 2012, PROC CVPR IEEE, P686, DOI 10.1109/CVPR.2012.6247737
   Krages B., 2005, Photography: the art of composition
   Leung T., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P546, DOI 10.1007/BFb0015565
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Liu JY, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531375
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Meng FM, 2012, IEEE T MULTIMEDIA, V14, P1429, DOI 10.1109/TMM.2012.2197741
   Pauly M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360642
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Schweitzer H, 2011, IEEE T PATTERN ANAL, V33, P459, DOI 10.1109/TPAMI.2010.105
   Thompson D. W., 1942, On growth and form.
NR 23
TC 6
Z9 8
U1 0
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2013
VL 29
IS 9
BP 861
EP 870
DI 10.1007/s00371-013-0847-8
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 186CB
UT WOS:000322018100002
DA 2024-07-18
ER

PT J
AU Bhattacharya, P
   Gavrilova, ML
AF Bhattacharya, Priyadarshi
   Gavrilova, Marina L.
TI Spatial consistency of dense features within interest regions for
   efficient landmark recognition
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Computer vision; Landmark recognition; Dense features; Feature grouping;
   Inverted index; Spatial information
AB Recently, feature grouping has been proposed as a method for improving retrieval results for logos and web images. This relies on the idea that a group of features matching over a local region in an image is more discriminative than a single feature match. In this paper, we evolve this concept further and apply it to the more challenging task of landmark recognition. We propose a novel combination of dense sampling of SIFT features with interest regions which represent the more salient parts of the image in greater detail. In place of conventional dense sampling used in category recognition that computes features on a regular grid at a number of fixed scales, we allow the sampling density and scale to vary based on the scale of the interest region. We develop new techniques for exploring stronger geometric constraints inside the feature groups and computing the match score. The spatial information is stored efficiently in an inverted index structure. The proposed approach considers part-based matching of interest regions instead of matching entire images using a histogram under bag-of-words. This helps reducing the influence of background clutter and works better under occlusion. Experiments reveal that directing more attention to the salient regions of the image and applying proposed geometric constraints helps in vastly improving recognition rates for reasonable vocabulary sizes.
C1 [Bhattacharya, Priyadarshi; Gavrilova, Marina L.] Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
C3 University of Calgary
RP Bhattacharya, P (corresponding author), Univ Calgary, Dept Comp Sci, 2500 Univ Dr NW, Calgary, AB T2N 1N4, Canada.
EM bhattacp@ucalgary.ca; mgavrilo@ucalgary.ca
RI Gavrilova, Marina/AHD-3605-2022
OI Gavrilova, Marina/0000-0002-5338-1834
CR [Anonymous], 2007, CVPR
   [Anonymous], 2007, MIR
   [Anonymous], 2008, An Open and Portable Library of Computer Vision Algorithms
   [Anonymous], 2008, CVPR
   Cao Y, 2010, PROC CVPR IEEE, P3352, DOI 10.1109/CVPR.2010.5540021
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen DM, 2011, PROC CVPR IEEE, P737, DOI 10.1109/CVPR.2011.5995610
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gehler P, 2009, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2009.5459169
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Kalantidis Y, 2011, P 1 ACM INT C MULT R, P20
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Tuytelaars T, 2007, FOUND TRENDS COMPUT, V3, P177, DOI 10.1561/0600000017
   Wu Z, 2009, PROC CVPR IEEE, P25, DOI 10.1109/CVPRW.2009.5206566
   Zhipeng Wu, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P842, DOI 10.1109/ICPR.2010.212
NR 18
TC 2
Z9 2
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 491
EP 499
DI 10.1007/s00371-013-0813-5
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400004
DA 2024-07-18
ER

PT J
AU Li, CY
   Ben Hamza, A
AF Li, Chunyuan
   Ben Hamza, A.
TI A multiresolution descriptor for deformable 3D shape retrieval
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Spectral graph wavelet; Laplace-Beltrami; Shape retrieval;
   Multiresolution
AB In this paper, we present a spectral graph wavelet framework for the analysis and design of efficient shape signatures for nonrigid 3D shape retrieval. Although this work focuses primarily on shape retrieval, our approach is, however, fairly general and can be used to address other 3D shape analysis problems. In a bid to capture the global and local geometry of 3D shapes, we propose a multiresolution signature via a cubic spline wavelet generating kernel. The parameters of the proposed signature can be easily determined as a trade-off between effectiveness and compactness. Experimental results on two standard 3D shape benchmarks demonstrate the much better performance of the proposed shape retrieval approach in comparison with three state-of-the-art methods. Additionally, our approach yields a higher retrieval accuracy when used in conjunction with the intrinsic spatial partition matching.
C1 [Li, Chunyuan; Ben Hamza, A.] Concordia Univ, Concordia Inst Informat Syst Engn, Montreal, PQ, Canada.
C3 Concordia University - Canada
RP Ben Hamza, A (corresponding author), Concordia Univ, Concordia Inst Informat Syst Engn, Montreal, PQ, Canada.
EM hamza@ciise.concordia.ca
RI Li, Chunyuan/AAG-1303-2020; li, chunyuan/IQW-1618-2023; Hamza,
   Abdessamad Ben/G-4571-2013
OI Ben Hamza, Abdessamad/0000-0002-3778-8167
FU NSERC Discovery Grant
FX This work was supported in part by NSERC Discovery Grant.
CR [Anonymous], WAVELET TOUR SIGNAL
   [Anonymous], 2012, P 3DOR
   [Anonymous], 2011, PROC EUROGRAPHICS 20, P79, DOI DOI 10.2312/3DOR/3DOR11/079-088
   Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   BRONSTEIN AM, 2011, ABS11105015 CORR
   Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006
   Gebal K, 2009, COMPUT GRAPH FORUM, V28, P1405, DOI 10.1111/j.1467-8659.2009.01517.x
   Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005
   Jarvelin K., 2000, SIGIR Forum, V34, P41
   Kazhdan M., 2003, Symposium on Geometry Processing, P156
   Kim W., 2012, P NIPS, P1135
   Kokkinos I, 2012, PROC CVPR IEEE, P159, DOI 10.1109/CVPR.2012.6247671
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Levy B, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P66
   Lian Z., 2010, Eurographics Workshop on 3D Object Retrieval, V10, P101, DOI [10.2312/3DOR/3DOR10/101-108, 10.1109/CVPR.2014.491, DOI 10.2312/3DOR/3DOR10/101-108]
   Lian ZH, 2013, PATTERN RECOGN, V46, P449, DOI 10.1016/j.patcog.2012.07.014
   Meyer N, 2003, VISUALIZATION AND MATHEMATICS III, P35
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Reuter M, 2010, INT J COMPUT VISION, V89, P287, DOI 10.1007/s11263-009-0278-1
   Rosenberg S., 1997, The Laplacian on a Riemannian Manifold: An Introduction to Analysis on Manifolds
   Rustamov Raif M, 2007, P S GEOM PROC, V257, P225
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0
   Wardetzky M., 2007, P EUROGRAPHICS S GEO, P33, DOI [DOI 10.2312/SGP/SGP07/033-037, 10.2312/SGP/SGP07/033-037]
   Yang YB, 2007, IEEE T SYST MAN CY C, V37, P1081, DOI 10.1109/TSMCC.2007.905756
NR 29
TC 77
Z9 85
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 513
EP 524
DI 10.1007/s00371-013-0815-3
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400006
DA 2024-07-18
ER

PT J
AU Moya, S
   Grau, S
   Tost, D
AF Moya, Sergio
   Grau, Sergi
   Tost, Dani
TI The wise cursor: assisted selection in 3D serious games
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Accessible user interfaces; Interactive selection; Navigation in virtual
   environments; Serious games
AB In recent years, the evolution of 3D graphics hardware and software has lead to a growing interest for serious games in three-dimensional virtual environments for learning, training, and rehabilitation. Many of these games are based on a first-person-shooter paradigm in which users navigate through the environment, select, and manipulate virtual objects. The target users of these applications are not necessarily usual gamers, and they often have difficulties in navigating and interacting in the 3D environment.
   This paper proposes the wise cursor, a new method for selection that improves the usability and accessibility of mouse-driven serious games in 3D environments. At each user click, the proposed method computes a list of objects candidates to be selected and their probability of being the desired one. Depending on the uncertainty of the probability distribution, either one object is selected or a mechanism to clarify the selection is proposed. In the former case, if the selected object is within the user avatar's scope, the action associated to it is realized, otherwise the application automatically navigates toward it. In this way, selection and navigation are eased. The empirical results of the usability tests show that this technique is fast, practical, and that it requires little user's skills. Thus, it can make serious games usable for a wider range of users who can concentrate on the training objectives without technological barriers.
C1 [Moya, Sergio; Grau, Sergi; Tost, Dani] Polytech Univ Catalonia UPC, Graph Div CREB, Barcelona 08028, Spain.
C3 Universitat Politecnica de Catalunya
RP Grau, S (corresponding author), Polytech Univ Catalonia UPC, Graph Div CREB, Avda Diagonal 647, Barcelona 08028, Spain.
EM smoya@lsi.upc.edu; sgrau@lsi.upc.edu; dani@lsi.upc.edu
RI Moya, Sergio E/I-1446-2015; Grau, Sergi/AAB-1682-2019; Tost,
   Dani/H-6289-2015
OI Grau, Sergi/0000-0003-1927-0476; Tost, Dani/0000-0001-9619-605X
CR Accot Johnny, 1997, P ACM SIGCHI C HUMAN, P295, DOI [10.1145/258549.258760, DOI 10.1145/258549.258760]
   Andujar C, 2007, 3DUI: IEEE SYMPOSIUM ON 3D USER INTERFACES 2007, PROCEEDINGS, P99
   Argelaguet F, 2010, LECT NOTES COMPUT SC, V6133, P115, DOI 10.1007/978-3-642-13544-6_11
   Balakrishnan R, 2004, INT J HUM-COMPUT ST, V61, P857, DOI 10.1016/j.ijhcs.2004.09.002
   Baudisch P., 2003, Proceedings of INTERACT, P57
   Bérard F, 2009, LECT NOTES COMPUT SC, V5727, P400, DOI 10.1007/978-3-642-03658-3_45
   Blanch R., 2004, P SIGCHI C HUMAN FAC, P519, DOI DOI 10.1145/985692.985758
   BORDOLOI U. D, 2005, IEEE VISUALIZATION, P62, DOI 10.1109/VIS.2005.110.2
   Bowman DA, 1999, J VISUAL LANG COMPUT, V10, P37, DOI 10.1006/jvlc.1998.0111
   Buxton W, 1987, HAPTIC CHANNEL
   Card S. K., 1987, EVALUATION MOUSE RAT, P386
   Chen M, 2010, IEEE T VIS COMPUT GR, V16, P1206, DOI 10.1109/TVCG.2010.132
   Christie M, 2008, COMPUT GRAPH FORUM, V27, P2197, DOI 10.1111/j.1467-8659.2008.01181.x
   Elmqvist Niklas., 2008, P GRAPHICS INTERFACE, P243, DOI 10.5555/1375714.1375755
   Findlater L., 2010, Proceedings of ACM Symposium on User Interface Software and Technology (UIST). (New York, NY, P153, DOI [10.1145/1866029, DOI 10.1145/1866029]
   FITTS PM, 1954, J EXP PSYCHOL, V47, P381, DOI 10.1037/h0055392
   Graafland M, 2012, BRIT J SURG, V99, P1322, DOI 10.1002/bjs.8819
   Grammenos D., 2009, ACM COMPUTERS ENTERT, V7, P1, DOI [10.1145/1486508.1486516, DOI 10.1145/1486508.1486516]
   Grossman Tovi., 2005, CHI 05, P281, DOI DOI 10.1145/1054972.1055012
   Guiard Y, 2004, PROC GRAPH INTERF, P9
   Gutwin C., 2002, Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2002, P267, DOI 10.1145/503376.503424
   Kabbash P., 1995, Human Factors in Computing Systems. CHI'95 Conference Proceedings, P273, DOI 10.1145/223904.223939
   Koesling H., 2011, P 1 C NOV GAZ CONTR, P4
   McGuffin M., 2002, Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2002, P57, DOI 10.1145/503376.503388
   Mine M. R., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P19, DOI 10.1145/258734.258747
   Pierce J. S., 1999, Proceedings 1999 Symposium on Interactive 3D Graphics, P141, DOI 10.1145/300523.300540
   Poupyrev I., 1996, P 9 ANN ACM S USER I, P79, DOI [DOI 10.1145/237091.237102, 10.1145/237091.237102]
   Prensky M., 2003, Computers in Entertainment (CIE), V1, P21, DOI DOI 10.1145/950566.950596
   Reese B., 1999, Artificial Intelligence and Computer Games. Papers from the 1999 AAAI Symposium, P69
   Rizzo A, 2005, PRESENCE-TELEOP VIRT, V14, P119, DOI 10.1162/1054746053967094
   Stoakley R., 1995, Human Factors in Computing Systems. CHI'95 Conference Proceedings, P265
   Teather R. J., 2011, Proceedings 2011 IEEE Symposium on 3D User Interfaces (3DUI 2011), P87, DOI 10.1109/3DUI.2011.5759222
   Tost D., 2012, PERSONALIZATION VIRT, P135
   van Driel L, 2009, LECT NOTES COMPUT SC, V5884, P146, DOI 10.1007/978-3-642-10347-6_14
   Van Eck R., 2006, EDUCAUSE REV, V41, P16, DOI DOI 10.1145/950566.950596
   Vanacken L., 2007, 3D USER INTERFACES
   Worden Aileen., 1997, P SIGCHI C HUMAN FAC, P266, DOI DOI 10.1145/258549.258724
   ZHAI SM, 1994, HUMAN FACTORS IN COMPUTING SYSTEMS, CHI '94 CONFERENCE PROCEEDINGS - CELEBRATING INTERDEPENDENCE, P459, DOI 10.1145/191666.191822
NR 38
TC 5
Z9 7
U1 0
U2 18
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 795
EP 804
DI 10.1007/s00371-013-0831-3
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400032
DA 2024-07-18
ER

PT J
AU Pan, N
   Zhao, Y
   Guo, XM
   Chen, X
   Chen, W
   Peng, QS
AF Pan, Bin
   Zhao, Yong
   Guo, Xiaoming
   Chen, Xiang
   Chen, Wei
   Peng, Qunsheng
TI Perception-motivated visualization for 3D city scenes
SO VISUAL COMPUTER
LA English
DT Article
DE Large-scale; City scenes; Expressive rendering; Landmarks
AB Many approaches have been developed to visualize 3D city scenes, most of which exhibit the visualization results in a uniform rendering style. This paper presents an expressive rendering approach for visualizing large-scale 3D city scenes with various rendering styles integrated in a seamless way. Each view is actually a combination of the photorealistic rendering and the nonphotorealistic rendering to highlight the information that is interesting for the users and de-emphasize the other that is less important. At run-time, the users are allowed to specify their interested locations interactively. Our system automatically computes the salience of each location and illustrates the entire scene with emphasis in the area of interests. The GPU-based implementation enables interactive realtime performance. Our implementation of a system demonstrates benefits in many applications such as 3D GPS navigation, tourist information, etc. We have performed a pilot user evaluation of the effect for users to access information in 3D city.
C1 [Pan, Bin; Guo, Xiaoming] Liaoning Shihua Univ, Sch Sci, Fushun, Peoples R China.
   [Chen, Xiang; Chen, Wei; Peng, Qunsheng] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Zhejiang, Peoples R China.
   [Zhao, Yong] Ocean Univ China, Sch Math Sci, Qingdao, Peoples R China.
C3 Liaoning Petrochemical University; Zhejiang University; Ocean University
   of China
RP Pan, N (corresponding author), Liaoning Shihua Univ, Sch Sci, Fushun, Peoples R China.
EM bpflyingeagle@gmail.com
FU National Basic Research Program of China (973 Program) [2009CB320802];
   National Natural Science Foundation of China [60970020, 60873123];
   Foundation of Liaoning Educational Committee [L2012131]; Research Award
   Fund of Shandong Province, China [BS2012DX043]; Fundamental Research
   Funds for the Central Universities [201313005]
FX This research is supported by National Basic Research Program of China
   (973 Program, No. 2009CB320802), National Natural Science Foundation of
   China (Nos. 60970020, 60873123), Foundation of Liaoning Educational
   Committee (No. L2012131), Research Award Fund of Shandong Province,
   China (No. BS2012DX043), and the Fundamental Research Funds for the
   Central Universities (No. 201313005).
CR Arge L, 2008, ACM T ALGORITHMS, V4, DOI 10.1145/1328911.1328920
   Brosz J, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P33
   Cole F., 2008, NPAR 2008
   Cole F., 2009, Proceedings of the 2009 Symposium on Interactive 3D Graphics and Games, P115
   Cole F., 2006, EUROGRAPHICS S RENDE, P377
   Döllner J, 2005, PROC SPIE, V5669, P42, DOI 10.1117/12.587118
   Döllner J, 2003, IEEE INFOR VIS, P245, DOI 10.1109/IV.2003.1217986
   Forberg A., 2002, INT ARCH PHOTOGRAMME, VXXXIV, P225
   Glander T., 2007, 4 INT S LBS TEL
   Glander T., 2007, 2 INT WORKSH 3D GEOI, P381
   Glander T., 2007, P 15 INT S ADV GEOGR, P1
   Gooch A., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P447, DOI 10.1145/280814.280950
   Gooch B., 1999, Proceedings 1999 Symposium on Interactive 3D Graphics, P31, DOI 10.1145/300523.300526
   Grabler F, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360699
   Guttman A., R TREES DYNAMIC INDE, P599
   Isenberg T, 2003, IEEE COMPUT GRAPH, V23, P28, DOI 10.1109/MCG.2003.1210862
   Jobst M., 2008, REAL CORP P VIENN
   Kosara R, 2001, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2001, PROCEEDINGS, P97, DOI 10.1109/INFVIS.2001.963286
   Kosara R., 2003, STATE OF THE ART P E, P123
   Lorenz H, 2009, LECT NOTES GEOINF CA, P301
   Mayer H., 1999, INT ARCH PHOTOGRAMME, P639
   Möser S, 2008, COMPUT GRAPH FORUM, V27, P1853, DOI 10.1111/j.1467-8659.2008.01332.x
   Qu HM, 2009, IEEE T VIS COMPUT GR, V15, P1547, DOI 10.1109/TVCG.2009.144
   Rheingans P., 1995, PERCEPTUAL ISSUES VI, P59, DOI [10.1007/978-3-642-79057-7_6, DOI 10.1007/978-3-642-79057-7_6]
   Semmo A, 2012, COMPUT GRAPH FORUM, V31, P885, DOI 10.1111/j.1467-8659.2012.03081.x
   Stoev SL, 2002, P IEEE VIRT REAL ANN, P285, DOI 10.1109/VR.2002.996541
   Thiemann F., 2002, P JOINT INT S GEOSPA, P225
   Trapp M., 2008, P INFOVIS 08, P225
NR 28
TC 4
Z9 4
U1 2
U2 19
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2013
VL 29
IS 4
SI SI
BP 277
EP 286
DI 10.1007/s00371-012-0773-1
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 115AM
UT WOS:000316784200005
DA 2024-07-18
ER

PT J
AU Yang, XN
   Zheng, JM
AF Yang, Xunnian
   Zheng, Jianmin
TI Shape aware normal interpolation for curved surface shading from
   polyhedral approximation
SO VISUAL COMPUTER
LA English
DT Article
DE Surface rendering; Triangular meshes; Gregory normal interpolation;
   Side-vertex normal interpolation
AB Independent interpolation of local surface patches and local normal patches is an efficient way for fast rendering of smooth curved surfaces from rough polyhedral meshes. However, the independently interpolating normals may deviate greatly from the analytical normals of local interpolating surfaces, and the normal deviation may cause severe rendering defects when the surface is shaded using the interpolating normals. In this paper we propose two novel normal interpolation schemes along with interpolation of cubic Bezier triangles for rendering curved surfaces from rough triangular meshes. Firstly, the interpolating normal is computed by a Gregory normal patch to each Bezier triangle by a new definition of quadratic normal functions along cubic space curves. Secondly, the interpolating normal is obtained by blending side-vertex normal functions along side-vertex parametric curves of the interpolating Bezier surface. The normal patches by these two methods can not only interpolate given normals at vertices or boundaries of a triangle but also match the shape of the local interpolating surface very well. As a result, more realistic shading results are obtained by either of the two new normal interpolation schemes than by the traditional quadratic normal interpolation method for rendering rough triangular meshes.
C1 [Yang, Xunnian] Zhejiang Univ, Dept Math, Hangzhou 310003, Zhejiang, Peoples R China.
   [Zheng, Jianmin] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
C3 Zhejiang University; Nanyang Technological University
RP Yang, XN (corresponding author), Zhejiang Univ, Dept Math, Hangzhou 310003, Zhejiang, Peoples R China.
EM yxn@zju.edu.cn; ASJMZheng@ntu.edu.sg
RI Zheng, Jianmin/A-3717-2011
OI Zheng, Jianmin/0000-0002-5062-6226; Yang, Xunnian/0000-0001-7501-1378
FU NNSF of China [60970077]; ARC of Singapore [MOE2008-T2-1-075]
FX This work is supported by NNSF of China grant (60970077) and the ARC
   9/09 Grant (MOE2008-T2-1-075) of Singapore.
CR Alexa M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409095
   Boubekeur T., 2005, P EUROGRAPHICS 05, P17
   Boubekeur T, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P453, DOI 10.1109/PG.2007.20
   Boubekeur T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409094
   Farin G., 1986, Computer-Aided Geometric Design, V3, P83, DOI 10.1016/0167-8396(86)90016-6
   Farin G., 2002, GUIDE
   Funfzig C, 2008, P GRAPHICS INTERFACE, P219
   Gregory J., 1974, COMPUT AIDED GEOM D, P71
   Hahmann S, 2003, IEEE T VIS COMPUT GR, V9, P99, DOI 10.1109/TVCG.2003.1175100
   Lee YC, 2001, VISUAL COMPUT, V17, P337, DOI 10.1007/s003710100111
   Li GQ, 2011, IEEE T VIS COMPUT GR, V17, P500, DOI 10.1109/TVCG.2010.83
   Loop C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618497
   Loop C, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330519
   Max N., 1999, Journal of Graphics Tools, V4, P1, DOI 10.1080/10867651.1999.10487501
   Ni T., 2009, ACM SIGGRAPH 2009 CO
   NIELSON GM, 1979, J APPROX THEORY, V25, P318, DOI 10.1016/0021-9045(79)90020-0
   Peters J., 1990, Computer-Aided Geometric Design, V7, P191, DOI 10.1016/0167-8396(90)90030-U
   PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839
   Schwarz M, 2009, COMPUT GRAPH FORUM, V28, P365, DOI 10.1111/j.1467-8659.2009.01376.x
   Shirman L. A., 1987, Computer-Aided Geometric Design, V4, P279, DOI 10.1016/0167-8396(87)90003-3
   vanOverveld CWAM, 1997, ACM T GRAPHIC, V16, P397, DOI 10.1145/263834.263849
   Vlachos A., 2001, P 2001 S INTERACTIVE, P159, DOI [DOI 10.1145/364338.364387, 10.1145/364338.364387]
   Walia E., 2006, INT C GEOM MOD IM NE, P69
   Walton DJ, 1996, COMPUT AIDED DESIGN, V28, P113, DOI 10.1016/0010-4485(95)00046-1
   Wang L, 2008, IEEE T VIS COMPUT GR, V14, P640, DOI 10.1109/TVCG.2008.8
NR 25
TC 2
Z9 2
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2013
VL 29
IS 3
BP 189
EP 201
DI 10.1007/s00371-012-0715-y
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 115AI
UT WOS:000316783800003
DA 2024-07-18
ER

PT J
AU Ghosh, SK
   Goswami, PP
   Maheshwari, A
   Nandy, SC
   Pal, SP
   Sarvattomananda, S
AF Ghosh, Subir Kumar
   Goswami, Partha Pratim
   Maheshwari, Anil
   Nandy, Subhas Chandra
   Pal, Sudebkumar Prasant
   Sarvattomananda, Swami
TI Algorithms for computing diffuse reflection paths in polygons
SO VISUAL COMPUTER
LA English
DT Article
DE Diffuse reflection; Specular reflection; Visibility polygon; Visibility
   graph; Shortest path; Minimum link path; Greedy path
ID VISIBILITY GRAPH; POINT
AB Let s be a point source of light inside a polygon P of n vertices. A polygonal path from s to some point t inside P is called a diffuse reflection path if the turning points of the path lie on edges of P. A diffuse reflection path is said to be optimal if it has the minimum number of reflections on the path. The problem of computing a diffuse reflection path from s to t inside P has not been considered explicitly in the past. We present three different algorithms for this problem which produce suboptimal paths. For constructing such a path, the first algorithm uses a greedy method, the second algorithm uses a transformation of a minimum link path, and the third algorithm uses the edge-edge visibility graph of P. The first two algorithms are for polygons without holes, and they run in O(n + k logn) time, where k denotes the number of reflections in the constructed path. The third algorithm is for polygons with or without holes, and it runs in O(n(2)) time. The number of reflections in the path produced by this third algorithm can be at most three times that of an optimal diffuse reflection path. Though the combinatorial approach used in the third algorithm gives a better bound on the number of reflections on the path, the first and the second algorithms stand on the merit of their elegant geometric approaches based on local geometric information.
C1 [Ghosh, Subir Kumar] Tata Inst Fundamental Res, Sch Technol & Comp Sci, Bombay 400005, Maharashtra, India.
   [Goswami, Partha Pratim] Univ Calcutta, Inst Radiophys & Elect, Kolkata 700009, India.
   [Maheshwari, Anil] Carleton Univ, Sch Comp Sci, Ottawa, ON K1S 5B6, Canada.
   [Nandy, Subhas Chandra] Indian Stat Inst, Adv Comp & Microelect Unit, Kolkata 700108, India.
   [Pal, Sudebkumar Prasant] Indian Inst Technol, Dept Comp Sci & Engn, Kharagpur 721302, W Bengal, India.
   [Sarvattomananda, Swami] Ramakrishna Mission Vivekananda Univ, Sch Math Sci, Belur 711202, India.
C3 Tata Institute of Fundamental Research (TIFR); University of Calcutta;
   Carleton University; Indian Statistical Institute; Indian Statistical
   Institute Kolkata; Indian Institute of Technology System (IIT System);
   Indian Institute of Technology (IIT) - Kharagpur
RP Ghosh, SK (corresponding author), Tata Inst Fundamental Res, Sch Technol & Comp Sci, Bombay 400005, Maharashtra, India.
EM ghosh@tifr.res.in; ppg.rpe@caluniv.ac.in; anil@scs.carleton.ca;
   nandysc@isical.ac.in; spp@cse.iitkgp.ernet.in; shreesh@rkmvu.ac.in
CR Aronov B, 1998, DISCRETE COMPUT GEOM, V19, P553, DOI 10.1007/PL00009368
   Aronov B, 1998, DISCRETE COMPUT GEOM, V20, P61, DOI 10.1007/PL00009378
   Aronov B, 2006, LECT NOTES COMPUT SC, V3887, P93, DOI 10.1007/11682462_13
   CHAZELLE B, 1991, DISCRETE COMPUT GEOM, V6, P485, DOI 10.1007/BF02574703
   CHAZELLE B, 1994, ALGORITHMICA, V12, P54, DOI 10.1007/BF01377183
   Davis A.R., 1998, THESIS POLYTECHNIC U
   DEBERG M, 1993, LECT NOTES COMPUTER, V703
   Foley J. D., 1994, Introduction to Computer Graphics", V55
   Ghosh S.K., 2007, Visibility Algorithms in the Plane
   GHOSH SK, 1991, SIAM J COMPUT, V20, P888, DOI 10.1137/0220055
   GHOSH SK, 1991, J ALGORITHM, V12, P75, DOI 10.1016/0196-6774(91)90024-S
   GUIBAS LJ, 1989, J COMPUT SYST SCI, V39, P126, DOI 10.1016/0022-0000(89)90041-X
   HERSHBERGER J, 1989, ALGORITHMICA, V4, P141, DOI 10.1007/BF01553883
   KLEE V, 1969, AM MATH MON, V76, P180, DOI 10.2307/2317271
   LEE DT, 1984, NETWORKS, V14, P393, DOI 10.1002/net.3230140304
   Mitchell J. S. B., 2011, P 27 EUR WORKSH COMP, P19
   MITCHELL JSB, 1992, ALGORITHMICA, V8, P431, DOI 10.1007/BF01758855
   O'Rourke J, 1998, COMP GEOM-THEOR APPL, V10, P105, DOI 10.1016/S0925-7721(97)00011-4
   PAL SP, 2004, J GEOMETRY, V81, P5
   PIATKO CD, 1993, THESIS CORNELL U
   Pocchiola M, 1996, INT J COMPUT GEOM AP, V6, P279, DOI 10.1142/S0218195996000204
   Pocchiola M, 1996, DISCRETE COMPUT GEOM, V16, P419, DOI 10.1007/BF02712876
   Prasad DC, 1998, COMP GEOM-THEOR APPL, V10, P187, DOI 10.1016/S0925-7721(97)00021-7
   Tokarsky GW, 1995, AM MATH MON, V102, P867, DOI 10.2307/2975263
NR 24
TC 2
Z9 2
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2012
VL 28
IS 12
BP 1229
EP 1237
DI 10.1007/s00371-011-0670-z
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 029ZZ
UT WOS:000310538700007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Acosta, E
   Liu, A
AF Acosta, Eric
   Liu, Alan
TI A pipeline virtual environment architecture for multicore processor
   systems
SO VISUAL COMPUTER
LA English
DT Article
DE Parallel virtual environment; Physics simulation; Multicore processors;
   Medical simulation
ID FRAMEWORK
AB We present a novel architecture to develop Virtual Environments (VEs) for multicore CPU systems. An object-centric method provides a uniform representation of VEs. The representation enables VEs to be processed in parallel using a multistage, dual-frame pipeline. Dynamic work distribution and load balancing is accomplished using a thread migration strategy with minimal overhead. This paper describes our approach, and shows it is efficient and scalable with performance experiments. Near linear speed-ups have been observed in experiments involving up to 1,000 deformable objects on a six-core i7 CPU. This approach's practicality is demonstrated with the development of a medical simulation trainer for a craniotomy procedure.
C1 [Acosta, Eric; Liu, Alan] USUHS, NCA Med Simulat Ctr, Silver Spring, MD USA.
RP Acosta, E (corresponding author), USUHS, NCA Med Simulat Ctr, Silver Spring, MD USA.
EM Eric.Acosta.ctr@simcen.usuhs.edu; Alan.Liu@simcen.usuhs.edu
CR Acosta E, 2007, IEEE VIRTUAL REALITY 2007, PROCEEDINGS, P247
   Agus M., 2002, Computing and Visualization in Science, V5, P35, DOI 10.1007/s00791-002-0085-5
   Aldinucci M., 2009, TR0912 U PIS DIP INF
   Allard J, 2007, STUD HEALTH TECHNOL, V125, P13
   Allard J., 2004, P EUR 2004
   Allard J, 2006, P IEEE VIRT REAL ANN, P89, DOI 10.1109/VR.2006.53
   [Anonymous], NVIDIA PERKIT
   [Anonymous], OPENGL EXT REF
   [Anonymous], NVIDIA CUDA C PROGR
   Augonnet C, 2011, CONCURR COMP-PRACT E, V23, P187, DOI 10.1002/cpe.1631
   Cavusoglu M. C., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P2458, DOI 10.1109/ROBOT.2000.846397
   Çavusoglu MC, 2004, ST HEAL T, V98, P46
   Chi-Keung Luk, 2009, Proceedings of the 2009 42nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2009), P45
   de Farias TSMC, 2008, INT SYM COMP ARCHIT, P45, DOI 10.1109/SBAC-PAD.2008.11
   Deligiannidis L, 2005, J SUPERCOMPUT, V33, P155, DOI 10.1007/s11227-005-0243-x
   Eilemann S, 2009, IEEE T VIS COMPUT GR, V15, P436, DOI 10.1109/TVCG.2008.104
   Figueiredo M, 2004, TENTH INTERNATIONAL CONFERENCE ON PARALLEL AND DISTRIBUTED SYSTEMS, PROCEEDINGS, P249, DOI 10.1109/ICPADS.2004.1316102
   Frigo M, 1998, ACM SIGPLAN NOTICES, V33, P212, DOI 10.1145/277652.277725
   Gautier T., 2007, PASCO 07 PROC 2007 I, P15
   Govindaraju N.K., 2006, VIRTUAL REALITY, P59
   Gropp W., 1994, SCI ENG COMPUTATION
   HERMANN E, 2009, EUR WORKSH PAR GRAPH
   Hermann E., 2010, EUR C PAR PROC 2
   Huagen W., 2001, CAD GRAPHICS 2001
   HUANG J, 2000, P EUR WORKSH PAR GRA
   Humphreys G, 2002, ACM T GRAPHIC, V21, P693, DOI 10.1145/566570.566639
   Jerabkova L., 2007, P VIRT ERW REAL 4 WO
   Kicherer M, 2011, P INT C HIGH PERF EM, P137, DOI DOI 10.1145/1944862.1944883
   MOLNAR S, 1992, COMP GRAPH, V26, P231, DOI 10.1145/142920.134067
   Montgomery Kevin., 2002, Medicine Meets Virtual Reality MMVR 2002, P23
   Muraki S., 2001, Proceedings of the 2001 ACM/IEEE Conference on Supercomputing, SC '01, P51
   Schulze J. P., 2002, Fourth Eurographics Workshop on Parallel Graphics and Visualization, P61
   Thomaszewski B, 2008, COMPUT GRAPH-UK, V32, P25, DOI 10.1016/j.cag.2007.11.003
   Vo HT, 2010, COMPUT GRAPH FORUM, V29, P1073, DOI 10.1111/j.1467-8659.2009.01704.x
   Voss G., 2002, Fourth Eurographics Workshop on Parallel Graphics and Visualization, P33
   Wernsing JR, 2010, ACM SIGPLAN NOTICES, V45, P115, DOI 10.1145/1755951.1755906
   Wieland F, 2001, 15TH WORKSHOP ON PARALLEL AND DISTRIBUTED SIMULATION, PROCEEDINGS, P117, DOI 10.1109/PADS.2001.924628
   Wittenbrink CM, 1998, INTERNATIONAL CONFERENCE ON PARALLEL AND DISTRIBUTED PROCESSING TECHNIQUES AND APPLICATIONS, VOLS I-IV, PROCEEDINGS, P1329
   Zhao W, 2008, 7TH INTERNATIONAL CONFERENCE ON SYSTEM SIMULATION AND SCIENTIFIC COMPUTING ASIA SIMULATION CONFERENCE 2008, VOLS 1-3, P786, DOI 10.1109/ASC-ICSC.2008.4675468
NR 39
TC 0
Z9 0
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2012
VL 28
IS 11
BP 1099
EP 1114
DI 10.1007/s00371-011-0661-0
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 021JO
UT WOS:000309881300004
OA hybrid
DA 2024-07-18
ER

PT J
AU Ahn, JK
   Lee, DY
   Ahn, M
   Kim, CS
AF Ahn, Jae-Kyun
   Lee, Dae-Youn
   Ahn, Minsu
   Kim, Chang-Su
TI R-D optimized progressive compression of 3D meshes using prioritized
   gate selection and curvature prediction
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Triangular mesh; Data compression; 3D mesh coding; Scalable coding;
   Progressive transmission; View-dependent transmission
AB A rate-distortion (R-D) optimized progressive coding algorithm for three-dimensional (3D) meshes is proposed in this work. We propose the prioritized gate selection and the curvature prediction to improve the connectivity and geometry compression performance, respectively. Furthermore, based on the bit plane coding, we develop a progressive transmission method, which improves the qualities of intermediate meshes as well as that of the fully reconstructed mesh, and extend it to the view-dependent transmission method. Experiments on various 3D mesh models show that the proposed algorithm provides significantly better compression performance than the conventional algorithms, while supporting progressive reconstruction efficiently.
C1 [Ahn, Jae-Kyun; Lee, Dae-Youn; Kim, Chang-Su] Korea Univ, Sch Elect Engn, Seoul, South Korea.
   [Ahn, Minsu] Samsung Adv Inst Technol, Yongin, Kyunggi Do, South Korea.
C3 Korea University; Samsung
RP Kim, CS (corresponding author), Korea Univ, Sch Elect Engn, Seoul, South Korea.
EM demian@korea.ac.kr; inomi@korea.ac.kr; minsu.ahn@samsung.com;
   changsukim@korea.ac.kr
OI Kim, Chang-Su/0000-0002-4276-1831
CR Ahn JK, 2010, IEEE IMAGE PROC, P3417, DOI 10.1109/ICIP.2010.5653130
   Alliez P, 2001, COMP GRAPH, P195, DOI 10.1145/383259.383281
   ALLIEZ P, 2001, EUROGRAPHICS, P480
   Aspert N, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P705, DOI 10.1109/ICME.2002.1035879
   DEERING M, 1995, P 22 ANN C COMP GRAP, P13
   Gandoin PM, 2002, ACM T GRAPHIC, V21, P372, DOI 10.1145/566570.566591
   Gumhold S, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P59
   GUMHOLD S, 1998, P SIGGRAPH 98, P133
   Heu JH, 2009, J VIS COMMUN IMAGE R, V20, P439, DOI 10.1016/j.jvcir.2009.05.003
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   KALBERER F, 2005, EUR 05, P469
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Kobbelt L, 2000, COMP GRAPH, P103, DOI 10.1145/344779.344835
   Meyer M., 2002, Journal of Graphics Tools, V7, P13, DOI 10.1080/10867651.2002.10487551
   Moffat A., 1995, Proceedings. DCC '95 Data Compression Conference (Cat. No.95TH8037), P202, DOI 10.1109/DCC.1995.515510
   Peng JL, 2005, J VIS COMMUN IMAGE R, V16, P688, DOI 10.1016/j.jvcir.2005.03.001
   Peng JL, 2005, ACM T GRAPHIC, V24, P609, DOI 10.1145/1073204.1073237
   Rossignac J, 1999, IEEE T VIS COMPUT GR, V5, P47, DOI 10.1109/2945.764870
   Sim JY, 2005, IEEE T CIRC SYST VID, V15, P854, DOI 10.1109/TCSVT.2005.848349
   Taubin G, 1998, ACM T GRAPHIC, V17, P84, DOI 10.1145/274363.274365
   Touma C, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P26
   Valette S, 2004, IEEE T VIS COMPUT GR, V10, P123, DOI 10.1109/TVCG.2004.1260764
NR 22
TC 10
Z9 13
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 769
EP 779
DI 10.1007/s00371-011-0565-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600035
DA 2024-07-18
ER

PT J
AU Tobler, RF
AF Tobler, Robert F.
TI Separating semantics from rendering: a scene graph based architecture
   for graphics applications
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Scene graph; Systems architecture; Semantics; Rendering; Design pattern
AB A large number of rendering and graphics applications developed in research and industry are based on scene graphs. Traditionally, scene graphs encapsulate the hierarchical structure of a complete 3D scene, and combine both semantic and rendering aspects. In this paper, we propose a clean separation of the semantic and rendering parts of the scene graph. This leads to a generally applicable architecture for graphics applications that is loosely based on the well-known Model-View-Controller (MVC) design pattern for separating the user interface and computation parts of an application. We explore the benefits of this new design for various rendering and modeling tasks, such as rendering dynamic scenes, out-of-core rendering of large scenes, generation of geometry for trees and vegetation, and multi-view rendering. Finally, we show some of the implementation details that have been solved in the process of using this software architecture in a large framework for rapid development of visualization and rendering applications.
C1 VRVis Res Ctr, Vienna, Austria.
RP Tobler, RF (corresponding author), VRVis Res Ctr, Vienna, Austria.
EM rft@vrvis.at
CR [Anonymous], SIGGRAPH 94
   BOSI M, 2009, VISUALIZATION LIB 1
   Burns Don, 2004, VIRTUAL REALITY C, P265
   COOK RL, 1984, P 11 ANN C COMP GRAP, P223, DOI DOI 10.1145/800031.808602
   Deutsch PeterL., 1984, Proceedings of the 11th ACM SIGACT-SIGPLAN symposium on Principles of Programming Languages, POPL '84, P297, DOI [10.1145/800017.800542, DOI 10.1145/800017.800542]
   Gervautz M, 1996, VISUAL COMPUT, V12, P62
   KUNZ H, 2009, NVIDIA SCENIX SCENE
   Lintermann B, 1998, COMPUT GRAPH FORUM, V17, P73, DOI 10.1111/1467-8659.00216
   Mendez E, 2008, IEEE COMPUT GRAPH, V28, P48, DOI 10.1109/MCG.2008.53
   Reenskaug T., 1979, MODELS VIEWS CONTROL
   Reitmayr G, 2005, P IEEE VIRT REAL ANN, P51
   Schmalstieg D., 1997, VRST'97. ACM Symposium on Virtual Reality Software and Technology 1997, P209, DOI 10.1145/261135.261173
   STRAUSS PS, 1992, COMP GRAPH, V26, P341, DOI 10.1145/142920.134089
   STREETING S, 2005, OGRE3D OBJECT ORIENT
   Voss G., 2002, Fourth Eurographics Workshop on Parallel Graphics and Visualization, P33
   Wernecke Josie., 1993, INVENTOR MENTOR PROG
   ZELEZNIK B, 2000, P EUR 2000, P200
NR 17
TC 13
Z9 23
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 687
EP 695
DI 10.1007/s00371-011-0572-0
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600027
DA 2024-07-18
ER

PT J
AU Aittala, M
AF Aittala, Miika
TI Inverse lighting and photorealistic rendering for augmented reality
SO VISUAL COMPUTER
LA English
DT Article
DE Inverse rendering; l(1)-regularization; Sparsity; Real-time rendering;
   Image processing
ID ILLUMINATION
AB We present a practical and robust photorealistic rendering pipeline for augmented reality. We solve the real world lighting conditions from observations of a diffuse sphere or a rotated marker. The solution method is based on l (1)-regularized least squares minimization, yielding a sparse set of light sources readily usable with most rendering methods. The framework also supports the use of more complex light source representations. Once the lighting conditions are solved, we render the image using modern real-time rendering methods such as shadow maps with variable softness, ambient occlusion, advanced BRDF's and approximate reflections and refractions. Finally, we perform post-processing on the resulting images in order to match the various aberrations and defects typically found in the underlying real-world video.
C1 VTT Tech Res Ctr Finland, Espoo 02044, VTT, Finland.
C3 VTT Technical Research Center Finland
RP Aittala, M (corresponding author), VTT Tech Res Ctr Finland, Vuorimiehentie 3, Espoo 02044, VTT, Finland.
EM miika.aittala@vtt.fi
FU Tekes projects
FX I would like to thank Professor Charles Woodward and the rest of the
   Augmented Reality team at VTT for comments and support. The work was
   funded by Tekes projects AR4BC and ARSisustus. Dibidogs animation
   character appears in Figs. 1 and 3 by permission of Futurecode Oy.
CR Agusanto K, 2003, SECOND IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P208, DOI 10.1109/ISMAR.2003.1240704
   Annen T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360633
   [Anonymous], 2018, Real-Time Rendering
   [Anonymous], 2009, IEEE INT C COMP VIS
   Ashikhmin M., 2000, Journal of Graphics Tools, V5, P25, DOI 10.1080/10867651.2000.10487522
   Azuma R, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.963459
   Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153
   Bavoil L., 2009, SHADERX 7 ADV RENDER, P425
   Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010
   Debevec P., 1998, SIGGRAPH98, P189, DOI DOI 10.1145/280814.280864
   DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242
   Franc V, 2005, LECT NOTES COMPUT SC, V3691, P407
   GIERLINGER T, 2009, J REAL TIME IMAGE PR
   Golub G.H., 1989, MATRIX COMPUTATIONS
   GRITZ L, 2007, GPU GEMS, V3, P529
   Haller M., 2003, Proceedings of the ACM symposium on Virtual reality software and technology, P56, DOI DOI 10.1145/1008653.1008665
   Hensley J, 2005, COMPUT GRAPH FORUM, V24, P547, DOI 10.1111/j.1467-8659.2005.00880.x
   Jacobs K, 2006, COMPUT GRAPH FORUM, V25, P29, DOI 10.1111/j.1467-8659.2006.00816.x
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   KLEIN G, 2008, P ISMAR 08, P57
   Malgouyres F, 2009, INT J COMPUT VISION, V83, P294, DOI 10.1007/s11263-009-0227-z
   MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082
   Marschner SR, 1997, FIFTH COLOR IMAGING CONFERENCE: COLOR SCIENCE, SYSTEMS, AND APPLICATIONS, P262
   Okumura B., 2006, P IEEE ACM INT S MIX, P219, DOI DOI 10.1109/ISMAR.2006.297817
   PAPAGIANNAKIS G, 2005, P VIRT SYST MULT 200, P189
   PILET J, 2006, P 5 IEEE ACM INT S M, P69
   Porter T., 1984, Proceedings of the 11th annual conference on Computer graphics and interactive techniques, P253, DOI 10.1145/800031.808606
   RAMAMOORTHI R, 2001, P SIGGRAPH 01, P407
   Sato I, 1999, IEEE T VIS COMPUT GR, V5, P1, DOI 10.1109/2945.764865
   Sato I, 2003, IEEE T PATTERN ANAL, V25, P290, DOI 10.1109/TPAMI.2003.1182093
   SCHLICK C, 1994, COMPUT GRAPH FORUM, V13, pC233, DOI 10.1111/1467-8659.1330233
   SLOAN PP, 2002, P ACM SIGGRAPH, P527
   Sugano N, 2003, SECOND IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P76, DOI 10.1109/ISMAR.2003.1240690
NR 33
TC 36
Z9 52
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 669
EP 678
DI 10.1007/s00371-010-0501-7
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800027
DA 2024-07-18
ER

PT J
AU Gobron, S
   Ahn, J
   Paltoglou, G
   Thelwall, M
   Thalmann, D
AF Gobron, Stephane
   Ahn, Junghyun
   Paltoglou, Georgios
   Thelwall, Michael
   Thalmann, Daniel
TI From sentence to emotion: a real-time three-dimensional graphics
   metaphor of emotions extracted from text
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual reality; Distribution functions; Data mining; Text analysis;
   Psychology and sociology; Facial animation
ID CHARACTERS; EXPRESSION; MODEL
AB This paper presents a novel concept: a graphical representation of human emotion extracted from text sentences. The major contributions of this paper are the following. First, we present a pipeline that extracts, processes, and renders emotion of 3D virtual human (VH). The extraction of emotion is based on data mining statistic of large cyberspace databases. Second, we propose methods to optimize this computational pipeline so that real-time virtual reality rendering can be achieved on common PCs. Third, we use the Poisson distribution to transfer database extracted lexical and language parameters into coherent intensities of valence and arousal-parameters of Russell's circumplex model of emotion. The last contribution is a practical color interpretation of emotion that influences the emotional aspect of rendered VHs. To test our method's efficiency, computational statistics related to classical or untypical cases of emotion are provided. In order to evaluate our approach, we applied our method to diverse areas such as cyberspace forums, comics, and theater dialogs.
C1 [Gobron, Stephane; Ahn, Junghyun; Paltoglou, Georgios; Thelwall, Michael; Thalmann, Daniel] Ecole Polytech Fed Lausanne, IC ISIM VRLAB, Stn 14, CH-1015 Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Gobron, S (corresponding author), Ecole Polytech Fed Lausanne, IC ISIM VRLAB, Stn 14, CH-1015 Lausanne, Switzerland.
EM stephane.gobron@epfl.ch
RI Thalmann, Daniel/AAL-1097-2020; Thalmann, Daniel/A-4347-2008; Thelwall,
   Mike/JDV-4700-2023
OI Thalmann, Daniel/0000-0002-0451-7491; Thelwall, Mike/0000-0001-6065-205X
FU European Union; SNF
FX The authors wish to thank Dr. Arvid Kappas for his advices concerning
   our basic model of emotion, as well as Mr. Quentin Silvestre for his
   technical help. Our thoughts also go to Dr. Barbara Yersin and Dr.
   Jonathan Maim, the original CS developer of the VH animation engine
   called YaQ. This work was supported by two grants: the European Union
   CYBEREMOTIONS and the SNF AERIALCROWDS projects.
CR ALLBECK J, 2002, AAMAS 2002 WORKSH EM
   Alm CO, 2005, P C HUM LANG TECHN E, P579, DOI DOI 10.3115/1220575.1220648
   Amaya K, 1996, PROC GRAPH INTERF, P222
   [Anonymous], 2008, FDN TRENDS INF RETRI, DOI DOI 10.1561/1500000001
   [Anonymous], 2003, MANUEL SCI AFFECTIVE
   Badler N, 2002, COMP ANIM CONF PROC, P133, DOI 10.1109/CA.2002.1017521
   Becheiraz P., 1996, Proceedings. Computer Animation '96, P58, DOI 10.1109/CA.1996.540488
   Cassell J, 2001, COMP GRAPH, P477, DOI 10.1145/383259.383315
   Cassell J., 1994, P 21 ANN C COMP GRAP, P413, DOI DOI 10.1145/192161.192272
   Cassell J., 2000, Embodied Conversational Agents
   Chittaro L, 2004, COMPUT ANIMAT VIRT W, V15, P319, DOI 10.1002/cav.35
   COSTA M, 2000, EMOTE MODEL EFFORT S, P173
   Coyne B, 2001, COMP GRAPH, P487, DOI 10.1145/383259.383316
   Egges A, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P121, DOI 10.1109/PCCGA.2004.1348342
   Ekman P, 1978, FACIAL ACTION CODING
   Fagerberg P, 2004, PERS UBIQUIT COMPUT, V8, P377, DOI 10.1007/s00779-004-0301-z
   FELDMANBARRETT L, 1999, CURR DIR PSYCHOL SCI, V18, P10
   Fontaine JRJ, 2007, PSYCHOL SCI, V18, P1050, DOI 10.1111/j.1467-9280.2007.02024.x
   Grillon H, 2009, COMPUT ANIMAT VIRT W, V20, P111, DOI 10.1002/cav.293
   Hatzivassiloglou Vasileios., 2000, P INT C COMPUTATIONA, P299, DOI DOI 10.3115/990820.990864
   Hess U, 2009, J VISION, V9, DOI 10.1167/9.12.19
   Kappas A, 2002, BEHAV PROCESS, V60, P85, DOI 10.1016/S0376-6357(02)00084-0
   Krumhuber E, 2007, EMOTION, V7, P730, DOI 10.1037/1528-3542.7.4.730
   KSHIRSAGAR S, 2002, SMARTGRAPH 02, P107
   MACDONALD C, 2008, TREC 2008
   MAIANO C, 2007, VRIC 07 LAVAL VIRTUA, P83
   Manning C.D., 1999, FDN STAT NATURAL LAN
   MASUKO S, 2007, EG 07, P303
   Musse SR, 2001, IEEE T VIS COMPUT GR, V7, P152, DOI 10.1109/2945.928167
   PARK KH, 2008, MMSP, P861
   Pelachaud C, 2009, PHILOS T R SOC B, V364, P3539, DOI 10.1098/rstb.2009.0186
   Pelachaud C, 2009, SPEECH COMMUN, V51, P630, DOI 10.1016/j.specom.2008.04.009
   PERLIN K, 1996, SIGGRAPH 96, P205
   Pfeiffer P.E., 1973, INTRO APPL PROBABILI
   Picard RosalindW., 1998, Affective Computing, V2
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Russell JA, 2003, ANNU REV PSYCHOL, V54, P329, DOI 10.1146/annurev.psych.54.101601.145102
   Ruttkay Z, 2003, COMPUT GRAPH FORUM, V22, P49, DOI 10.1111/1467-8659.t01-1-00645
   Sebastiani F, 2002, ACM COMPUT SURV, V34, P1, DOI 10.1145/505282.505283
   Shao W, 2007, GRAPH MODELS, V69, P246, DOI 10.1016/j.gmod.2007.09.001
   Stone M, 2004, ACM T GRAPHIC, V23, P506, DOI 10.1145/1015706.1015753
   Su WP, 2007, IEEE T VIS COMPUT GR, V13, P281, DOI 10.1109/TVCG.2007.44
   Unuma Munetoshi, 1995, SIGGRAPH, P91
   Velasquez J.D., 1997, P AAAI97
   WIEBE J, 2000, IAAI 00, P735
   Wilson T., 2005, P C HUM LANG TECHN E, P347, DOI [10.3115/1220575.1220619, DOI 10.3115/1220575.1220619]
   Yu QX, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P119
   ZHE X, 2002, INT S COMM SYST NETW, P164
NR 48
TC 21
Z9 26
U1 2
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 505
EP 519
DI 10.1007/s00371-010-0446-x
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 602JQ
UT WOS:000278135800012
DA 2024-07-18
ER

PT J
AU Wuhrer, S
   Brunton, A
AF Wuhrer, Stefanie
   Brunton, Alan
TI Segmenting animated objects into near-rigid components
SO VISUAL COMPUTER
LA English
DT Article
DE Segmentation; Geometry processing; Computational geometry; Clustering
ID MESH SEGMENTATION; DECOMPOSITION
AB We present a novel approach to solve the problem of segmenting a sequence of animated objects into near-rigid components based on k given poses of the same non-rigid object. We model the segmentation problem as a clustering problem in dual space and find near-rigid segments with the property that segment boundaries are located at regions of large deformation. The presented approach is asymptotically faster than previous approaches that achieve the same property and does not require any user-specified parameters. However, if desired, the user may interactively change the number of segments. We demonstrate the practical value of our approach using experiments.
C1 [Wuhrer, Stefanie; Brunton, Alan] Natl Res Council Canada, Ottawa, ON, Canada.
C3 National Research Council Canada
RP Wuhrer, S (corresponding author), Natl Res Council Canada, Ottawa, ON, Canada.
EM stefanie.wuhrer@nrc-cnrc.gc.ca; alan.brunton@nrc-cnrc.gc.ca
FU HPCVL; OGS
FX Research supported in part by HPCVL.; We thank Doug James and Dong-Yee
   Lee for their kind permission to use their images. This work was
   partially supported by OGS.
CR ANGUELOV D, 2004, UNC ART INT C
   [Anonymous], 2004, P SHAP MOD INT
   Baran I, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239523, 10.1145/1276377.1276467]
   Bronstein AM, 2007, IEEE T VIS COMPUT GR, V13, P902, DOI 10.1109/TVCG.2007.1041
   Davies RH, 2002, LECT NOTES COMPUT SC, V2352, P3
   HUANG Q, 2008, COMPUT GRAPH FORUM, V27
   Jain Varun., 2007, INT J SHAPE MODELING, V13, P101, DOI DOI 10.1142/S0218654307000968
   James DL, 2005, ACM T GRAPHIC, V24, P399, DOI 10.1145/1073204.1073206
   Katz S, 2005, VISUAL COMPUT, V21, P649, DOI 10.1007/s00371-005-0344-9
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   Kleinberg Jon, 2005, ALGORITHM DESIGN
   Lee TY, 2006, VISUAL COMPUT, V22, P729, DOI 10.1007/s00371-006-0059-6
   Lee TY, 2005, COMPUT ANIMAT VIRT W, V16, P519, DOI 10.1002/cav.79
   Lewis JP., 2000, POSE SPACE DEFORMATI
   Lien JM, 2006, COMP GEOM-THEOR APPL, V35, P100, DOI 10.1016/j.comgeo.2005.10.005
   Liu R, 2007, COMPUT GRAPH FORUM, V26, P385, DOI 10.1111/j.1467-8659.2007.01061.x
   Sattler Mirko, 2005, P ACM SIGGRAPH EUR S, P209
   Shamir A, 2008, COMPUT GRAPH FORUM, V27, P1539, DOI 10.1111/j.1467-8659.2007.01103.x
   Siek J., 2002, The Boost Graph Library: User Guide and Reference Manual
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   TIERNY J, 2006, 3 IEEE INT S 3D DAT
   ZHANG H, 2008, COMPUT GRAPH FORUM, V27
NR 22
TC 17
Z9 20
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2010
VL 26
IS 2
BP 147
EP 155
DI 10.1007/s00371-009-0394-5
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 543RE
UT WOS:000273595200006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Burke, JW
   McNeill, MDJ
   Charles, DK
   Morrow, PJ
   Crosbie, JH
   McDonough, SM
AF Burke, J. W.
   McNeill, M. D. J.
   Charles, D. K.
   Morrow, P. J.
   Crosbie, J. H.
   McDonough, S. M.
TI Optimising engagement for stroke rehabilitation using serious games
SO VISUAL COMPUTER
LA English
DT Article
DE Engagement; Serious games; Rehabilitation; Stroke; Video capture
ID UPPER EXTREMITY
AB Effective stroke rehabilitation must be early, intensive and repetitive, which can lead to problems with patient motivation and engagement. The design of video games, often associated with good user engagement, may offer insights into how more effective systems for stroke rehabilitation can be developed. In this paper we identify game design principles for upper limb stroke rehabilitation and present several games developed using these principles. The games use low-cost video-capture technology which may make them suitable for deployment at home. Results from evaluating the games with both healthy subjects and people with stroke in their home are encouraging.
C1 [Burke, J. W.; McNeill, M. D. J.; Charles, D. K.; Morrow, P. J.] Univ Ulster, Sch Comp & Informat Engn, Coleraine BT52 1SA, Londonderry, North Ireland.
   [Crosbie, J. H.; McDonough, S. M.] Univ Ulster, Sch Hlth Sci, Jordanstown, North Ireland.
C3 Ulster University; Ulster University
RP Burke, JW (corresponding author), Univ Ulster, Sch Comp & Informat Engn, Coleraine BT52 1SA, Londonderry, North Ireland.
EM burke-j5@email.ulster.ac.uk; mdj.mcneill@ulster.ac.uk;
   dk.charles@ulster.ac.uk; pj.morrow@ulster.ac.uk; j.crosbie@ulster.ac.uk;
   s.mcdonough@ulster.ac.uk
RI Charles, Darryl/AAJ-6664-2021
OI Charles, Darryl/0000-0002-8546-6066; McDonough,
   Suzanne/0000-0003-3758-3302
FU Department for Employment and Learning (DEL, Northern Ireland)
FX This work was supported by a Department for Employment and Learning
   (DEL, Northern Ireland) research studentship.
CR Anderson R., 1992, AFTERMATH STROKE EXP
   [Anonymous], 2005, Theory of fun for game design
   [Anonymous], 1 INT WORKSH VIRTL R
   BORG GAV, 1982, MED SCI SPORT EXER, V14, P377, DOI 10.1249/00005768-198205000-00012
   Burke JW, 2009, PROCEEDINGS OF THE IEEE VIRTUAL WORLDS FOR SERIOUS APPLICATIONS, P103, DOI 10.1109/VS-GAMES.2009.17
   CROSBIE JH, 2004, 5 INT C DIS VIRT REA, P215
   Desurvire H., 2004, EXTENDED ABSTRACTS 2, P1509, DOI [DOI 10.1145/985921.986102, 10.1145/985921.986102]
   Jack D, 2001, IEEE T NEUR SYS REH, V9, P308, DOI 10.1109/7333.948460
   Kwakkel G, 1999, LANCET, V354, P191, DOI 10.1016/S0140-6736(98)09477-X
   Ma MH, 2007, LECT NOTES COMPUT SC, V4555, P681
   Morrow K, 2006, 2006 INTERNATIONAL WORKSHOP ON VIRTUAL REHABILITATION, P6, DOI 10.1109/IWVR.2006.1707518
   Peek B., 2008, Managed library for Nintendo's Wiimote: A library for using a Nintendo Wii Remote(Wiimote) from .NET
   RAND D, 2004, P 5 INT C DIS VIRT R, P87
   Rizzo A, 2005, PRESENCE-TELEOP VIRT, V14, P119, DOI 10.1162/1054746053967094
   Tekinbas K. S., 2003, Rules of Play: Game Design Fundamentals
   van der Lee JH, 1999, STROKE, V30, P2369, DOI 10.1161/01.STR.30.11.2369
   WADE DT, 1985, J NEUROL NEUROSUR PS, V48, P7, DOI 10.1136/jnnp.48.1.7
   WEISS P, 2005, TXB NEURAL REPAIR NE, V2, P182
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   Yavuzer G, 2008, EUR J PHYS REHAB MED, V44, P237
NR 20
TC 340
Z9 383
U1 1
U2 99
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2009
VL 25
IS 12
BP 1085
EP 1099
DI 10.1007/s00371-009-0387-4
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 510LC
UT WOS:000271089000005
DA 2024-07-18
ER

PT J
AU Kang, KK
   Kim, JA
   Kim, D
AF Kang, Kyung-Kyu
   Kim, Jung-A
   Kim, Dongho
TI Development of a sensory gate-ball game system for the aged people
SO VISUAL COMPUTER
LA English
DT Article
DE Sensory game; Gate-ball game; 3D game; Serious game; User interface;
   Aged people
AB Recently, medical advances have been increasing the size of the aged population. With rapidly developing technology, computers are now essential parts in our daily life, and the number of the aged people using computers is also increasing continuously. But user interfaces and contents for the aged people have not been developed actively yet. In this paper, we present a 3D sensory gate-ball game system which can be played by the aged people easily. This game system is based on 3D graphics and uses a realistic gate-ball stick and balls as interfaces, so it can improve both physical and mental health of the aged people. Because our game is a sensory game, it is easy to play at home without an outdoor playground. Finally, we made a survey about an experiment after users played the game and show results of the survey.
C1 [Kang, Kyung-Kyu; Kim, Jung-A; Kim, Dongho] Soongsil Univ, Dept Media, Seoul, South Korea.
C3 Soongsil University
RP Kang, KK (corresponding author), Soongsil Univ, Dept Media, Seoul, South Korea.
EM rcrookie@magiclab.kr; jung@magiclab.kr; prof@magiclab.kr
CR BAE HJ, 2006, P KOR I INF SCI ENG, V33
   CONGER D, 2004, PHYS MODELING GAME P
   DELOURA M, 2001, GAME PROGRAMMING GEM, V2
   Deloura M., 2000, Game Programming Gems
   FALK J, 2002, TANGIBLE MUDDING INT
   KERN D, 2006, 15 IEEE INT WORKSH E
   Khoo E.T., 2006, The International Journal of Virtual Reality, V5, P45, DOI DOI 10.20870/IJVR.2006.5.2.2688
   KIM H, 1995, P 76 ARCH I KOR, P19
   KIM HS, 1995, THESIS HONGIK U
   Lynch P., 1999, WEB STYLE GUIDE BASI
   MAZALEK A, 2007, TANGIBLE PLAY, V2
   SEO CH, 2003, THESIS HONGIK U
   Young M., 1989, The Technical Writer's Handbook
NR 13
TC 5
Z9 5
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2009
VL 25
IS 12
BP 1073
EP 1083
DI 10.1007/s00371-009-0385-6
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 510LC
UT WOS:000271089000004
DA 2024-07-18
ER

PT J
AU Ancuti, C
   Ancuti, CO
   Bekaert, P
AF Ancuti, Cosmin
   Ancuti, Codruta Orniana
   Bekaert, Philippe
TI An efficient two steps algorithm for wide baseline image matching
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Widebaseline; Matching; Local future points; Descriptor
AB A recent study (Int. J. Comput. Vis. 73(3), 263-284, 2007) has shown that none of the detector/descriptor combinations perform well when the camera viewpoint is changed with more than 25-30A degrees. In this paper we introduce an efficient two-step method that increases significantly the number of correct matches of wide separated views of a given 3D scenes. First, a few kernel correspondences are identified in the images and then, based on their neighbor information, the geometric distortion that relates the surrounding regions of these seed keypoints is estimated iteratively. Next, based on these estimated parameters combined with a rough segmentation that reduces the searching space of the keypoint descriptors, the neighbor regions around every keypoint are warped accordingly. In our experiments the method has been tested extensively, yielding promising results over a wide range of viewpoints of known 3D models images.
C1 [Ancuti, Cosmin; Ancuti, Codruta Orniana; Bekaert, Philippe] Hasselt Univ, Expertise Ctr Digital Media, tUL, IBBT, B-3590 Diepenbeek, Belgium.
C3 Hasselt University
RP Ancuti, C (corresponding author), Hasselt Univ, Expertise Ctr Digital Media, tUL, IBBT, Wetenschapspk 2, B-3590 Diepenbeek, Belgium.
EM cosmin.ancuti@uhasselt.be
OI ancuti, codruta/0000-0001-8782-4341
CR ABDELHAKIM AE, 2006, P IEEE C COMP VIS PA
   ANCUTI C, 2008, P 8 INT C VIS IM IM
   [Anonymous], 1994, COMPUTER VISION PATT
   [Anonymous], P IEEE C COMP VIS IC
   [Anonymous], P IEEE C COMP VIS PA
   Baumberg A., 2000, P IEEE C COMP VIS PA
   BEIS J, 1997, P IEEE C COMP VIS PA
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Dufournaud Y, 2000, PROC CVPR IEEE, P612, DOI 10.1109/CVPR.2000.855876
   FORSSEN PE, 2007, P IEEE C COMP VIS IC
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Lindeberg T, 1997, IMAGE VISION COMPUT, V15, P415, DOI 10.1016/S0262-8856(97)01144-X
   Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matas J., 2002, Electronic Proceedings of the 13th British Machine Vision Conference, P384
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Mikolajczyk K., 2005, INT J COMPUT VIS
   MIKOLAJCZYK K, 2004, P IEEE C COMP VIS PA, V30, P257
   Moreels P, 2007, INT J COMPUT VISION, V73, P263, DOI 10.1007/s11263-006-9967-1
   Pritchett P, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P754, DOI 10.1109/ICCV.1998.710802
   SCHAFFALITZKY F, 2001, P IEEE C COMP VIS IC
   Schmid C, 1997, IEEE T PATTERN ANAL, V19, P530, DOI 10.1109/34.589215
   Tuytelaars T, 2004, INT J COMPUT VISION, V59, P61, DOI 10.1023/B:VISI.0000020671.28016.e8
   XIAO J, 2003, P IEEE C COMP VIS IC
NR 25
TC 4
Z9 5
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 677
EP 686
DI 10.1007/s00371-009-0353-1
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300033
DA 2024-07-18
ER

PT J
AU Vázquez, PP
AF Vazquez, Pere-Pau
TI Automatic view selection through depth-based view stability analysis
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Best view; Automatic view selection; View stability
ID IMAGE
AB Although the real world is composed of three-dimensional objects, we communicate information using two-dimensional media. The initial 2D view we see of an object has great importance on how we perceive it. Deciding which of the all possible 2D representations of 3D objects communicates the maximum information to the user is still challenging, and it may be highly dependent on the addressed task. Psychophysical experiments have shown that three-quarter views (oblique views between frontal view and profile view) are often preferred as representative views for 3D objects; however, for most models, no knowledge of its proper orientation is provided. Our goal is the selection of informative views without any user intervention. In order to do so, we analyze some stability-based view descriptors and present a new one that computes view stability through the use of depth maps, without prior knowledge on the geometry or orientation of the object. We will show that it produces good views that, in most of the analyzed cases, are close to three-quarter views.
C1 Univ Politecn Cataluna, Dept Llenguatges & Sistemes Informat, Barcelona, Spain.
C3 Universitat Politecnica de Catalunya
RP Vázquez, PP (corresponding author), Univ Politecn Cataluna, Dept Llenguatges & Sistemes Informat, Barcelona, Spain.
EM ppau@lsi.upc.edu
RI Vázquez, Pere-Pau/L-4697-2014; Vázquez, Pere-Pau/HTP-9691-2023
OI Vázquez, Pere-Pau/0000-0003-4638-4065; Vázquez,
   Pere-Pau/0000-0003-4638-4065
CR Andújar C, 2007, COMPUT GRAPH FORUM, V26, P553, DOI 10.1111/j.1467-8659.2007.01078.x
   [Anonymous], 1981, Attention and Performance
   [Anonymous], C INF SCI SYST
   Bennett CH, 1998, IEEE T INFORM THEORY, V44, P1407, DOI 10.1109/18.681318
   Blanz V, 1999, PERCEPTION, V28, P575, DOI 10.1068/p2897
   Bordoloi UD, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P487
   Cebrián M, 2007, IEEE T INFORM THEORY, V53, P1895, DOI 10.1109/TIT.2007.894669
   Cilibrasi R, 2005, IEEE T INFORM THEORY, V51, P1523, DOI 10.1109/TIT.2005.844059
   Cleju Ioan., 2006, P 3 S APPL PERCEPTIO, P41, DOI DOI 10.1145/1140491.1140499
   Cyr CM, 2004, INT J COMPUT VISION, V57, P5, DOI 10.1023/B:VISI.0000013088.59081.4c
   Freeman W. T., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P347, DOI 10.1109/ICCV.1993.378193
   Freeman WT, 1996, INT J COMPUT VISION, V20, P243, DOI 10.1007/BF00208721
   Fu HB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360641
   IKEUCHI K, 1988, P IEEE, V76, P1016, DOI 10.1109/5.5972
   Ji GF, 2006, IEEE T VIS COMPUT GR, V12, P1109, DOI 10.1109/TVCG.2006.137
   KOENDERINK JJ, 1980, OPT ACTA, V27, P981, DOI 10.1080/713820338
   Li M, 2004, IEEE T INFORM THEORY, V50, P3250, DOI 10.1109/TIT.2004.838101
   Li M., 2019, An introduction to Kolmogorov complexity and its applications, V4, DOI [DOI 10.1007/978-3-030-11298-1, 10.1007/978-3-030-11298-1]
   MUHLER K, 2007, EG IEEE VGTC S VIS, P267
   PLANTINGA H, 1990, INT J COMPUT VISION, V5, P137, DOI 10.1007/BF00054919
   PLEMENOS D, INT C GRAPHICON 96
   Polonsky O, 2005, VISUAL COMPUT, V21, P840, DOI 10.1007/s00371-005-0326-y
   ROBERTS D, 1998, P BRIT MACH VIS C
   SBERT M, 2005, COMPUTATIONAL AESTHE
   Takahashi S, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P495
   Tarr MJ, 2001, VISION RES, V41, P1981, DOI 10.1016/S0042-6989(01)00024-4
   Todd JT, 2004, TRENDS COGN SCI, V8, P115, DOI 10.1016/j.tics.2004.01.006
   Vázquez PP, 2008, LECT NOTES COMPUT SC, V5166, P106, DOI 10.1007/978-3-540-85412-8_10
   Vázquez PP, 2003, COMPUT GRAPH FORUM, V22, P689, DOI 10.1111/j.1467-8659.2003.00717.x
   Viola I, 2006, IEEE T VIS COMPUT GR, V12, P933, DOI 10.1109/TVCG.2006.152
   Weinshall D, 1997, IEEE T PATTERN ANAL, V19, P97, DOI 10.1109/34.574783
   WEINSHALL D, 1996, P APRA IU WORKSH
NR 32
TC 15
Z9 20
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 441
EP 449
DI 10.1007/s00371-009-0326-4
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300008
DA 2024-07-18
ER

PT J
AU Ghali, S
AF Ghali, Sherif
TI Sense and sidedness in the graphics pipeline via a passage through a
   separable space
SO VISUAL COMPUTER
LA English
DT Article
DE Graphics pipeline; Separability; Projective geometry; Oriented
   projective geometry; Homogeneous coordinates
ID TRIP
AB Computer graphics is ostensibly based on projective geometry. The graphics pipeline-the sequence of functions applied to 3D geometric primitives to determine a 2D image-is described in the graphics literature as taking the primitives from Euclidean to projective space, and then back to Euclidean space.
   This is a weak foundation for computer graphics. An instructor is at a loss: one day entering the classroom and invoking the established and venerable theory of projective geometry while asserting that projective spaces are not separable, and then entering the classroom the following week to tell the students that the standard graphics pipeline performs clipping not in Euclidean, but in projective space-precisely the operation (deciding sidedness, which depends on separability) that was deemed nonsensical.
   But there is no need to present Blinn and Newell's algorithm (Comput. Graph. 12, 245-251, 1978; Commun. ACM 17, 32-42, 1974)-the crucial clipping step in the graphics pipeline and, perhaps, the most original knowledge a student learns in a fourth-year computer graphics class as a clever trick that just works. Jorge Stolfi described in 1991 oriented projective geometry. By declaring the two vectors (x, y, z, w)(T) and (-x, -y, -z, -w)(T) distinct, Blinn and Newell were already unknowingly working in oriented projective space. This paper presents the graphics pipeline on this stronger foundation.
C1 KinLoop Inc, Edmonton, AB, Canada.
RP Ghali, S (corresponding author), KinLoop Inc, Edmonton, AB, Canada.
EM shghalil@gmail.com
CR [Anonymous], 1937, MEN MATH
   [Anonymous], 1991, Oriented Projective Geometry: A Framework for Geometric Computations
   [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   [Anonymous], 2004, HIST ANAL GEOMETRY
   BLINN J, 1978, COMPUT GRAPH, V12, P245
   BLINN JF, 1993, IEEE COMPUT GRAPH, V13, P75, DOI 10.1109/38.210494
   BLINN JF, 1991, IEEE COMPUT GRAPH, V11, P98, DOI 10.1109/38.67707
   BLOOMENTHAL J, 1994, VISUAL COMPUT, V11, P15, DOI 10.1007/BF01900696
   Coxeter H. S. M., 1987, PROJECTIVE GEOMETRY, V2nd
   COXETER H.S.M., 1998, Spectrum Series
   Crowe M.J., 1994, A History of Vector Analysis
   Ghali S, 2008, INTRO GEOMETRIC COMP
   Goldman R, 2002, ACM T GRAPHIC, V21, P52, DOI 10.1145/504789.504792
   Hartley RI, 1998, INT J COMPUT VISION, V26, P41, DOI 10.1023/A:1007984508483
   HERMAN Ivan -., 1992, The use of projective geometry in computer graphics
   HILBERT D, 2002, FDN GEOMETRY
   HILL FS, 2001, COMPUTER GRAPHICS
   Kline JR, 1916, ANN MATH, V18, P31, DOI 10.2307/2007148
   LAVEAU S, 1996, EUR C COMP VIS
   PENNA Michael N., 1986, Projective Geometry and its applications to computer graphics
   Riesenfeld R. F., 1981, IEEE Computer Graphics and Applications, V1, P50, DOI 10.1109/MCG.1981.1673814
   Roberts, 1965, MACHINE PERCEPTION 3
   SEMPLE JG, 1952, ALGEBRAIC PROJECTIVE
   SUTHERLAND IE, 1974, COMMUN ACM, V17, P32, DOI 10.1145/360767.360802
NR 24
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2009
VL 25
IS 4
BP 367
EP 375
DI 10.1007/s00371-008-0302-4
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 413YH
UT WOS:000263830600006
DA 2024-07-18
ER

PT J
AU Martinez, J
   Weerakkody, W
   Fernando, W
   Fernandez-Escribano, G
   Kalva, H
   Garrido, A
AF Martinez, J. L.
   Weerakkody, W. A. R. J.
   Fernando, W. A. C.
   Fernandez-Escribano, G.
   Kalva, H.
   Garrido, A.
TI Distributed Video Coding using Turbo Trellis Coded Modulation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th International Multimedia Modeling Conference (MMM 2008)
CY JAN 09-11, 2008
CL Kyoto Univ, Kyoto, JAPAN
HO Kyoto Univ
DE Algebraic triangulation; Partition of unity implicits; Orthogonal
   polynomials
ID INFORMATION
AB Distributed Video Coding (DVC) has been proposed for increasingly new application domains. This rise is apparently motivated by the very attractive features of its flexibility for building very low cost video encoders and the very high built-in error resilience when applied over noisy communication channels. Yet, the compression efficiency of DVC is notably lagging behind the state-of-the-art in video coding and compression, H.264/AVC in particular. In this context, a novel coding solution for DVC is presented in this paper, which promises to improve its rate-distortion (RD) performance towards the state-of-the-art. Here, Turbo Trellis Coded Modulation (TTCM), with its attractive coding gain in channel coding, is utilized and its resultant impact in both pixel domain and transform domain DVC framework is discussed herein. Simulations have shown a significant gain in the RD performance when compared with the state-of-the-art Turbo coding based DVC implementations.
C1 [Martinez, J. L.; Fernandez-Escribano, G.; Garrido, A.] Univ Castilla La Mancha, Inst Invest Informat, Albacete 02071, Spain.
   [Weerakkody, W. A. R. J.; Fernando, W. A. C.] Univ Surrey, Ctr Commun Res, Guildford GU2 7XH, Surrey, England.
   [Kalva, H.] Florida Atlantic Univ, Dept Comp Sci & Engn, Boca Raton, FL 33431 USA.
C3 Universidad de Castilla-La Mancha; University of Surrey; State
   University System of Florida; Florida Atlantic University
RP Martinez, J (corresponding author), Univ Castilla La Mancha, Inst Invest Informat, Campus Univ S-N, Albacete 02071, Spain.
EM joseluismm@dsi.uclm.es; r.weerakkody@surrey.ac.uk;
   w.fernando@surrey.ac.uk; gerardo@dsi.uclm.es; hari@cse.fau.edu;
   antonio@dsi.uclm.es
RI Martinez, Jose Luis/ABA-2535-2021; Fernández-Escribano,
   Gerardo/I-1167-2015; Garrido del Solo, Antonio/C-1302-2017
OI Martinez, Jose Luis/0000-0001-5119-2418; Fernández-Escribano,
   Gerardo/0000-0002-0037-2061; Garrido del Solo,
   Antonio/0000-0003-2630-6721; Kalva, Hari/0000-0002-7165-5499
CR Aaron A, 2002, IEEE DATA COMPR CONF, P252, DOI 10.1109/DCC.2002.999963
   AARON A, 2004, P VIS COMM IM PROC S
   [Anonymous], AS C SIGN SYST COMP
   [Anonymous], 2001, P 13 VCEG M33 M AUST
   Bernardini R, 2006, P IEEE INT C IM PROC
   BRITES C, 2006, IEEE INT C AC SPEECH
   *ISO IEC, 14496102003 ISOIEC
   Liveris AD, 2002, IEEE COMMUN LETT, V6, P440, DOI 10.1109/LCOMM.2002.804244
   Pradhan SS, 1999, IEEE DATA COMPR CONF, P158, DOI 10.1109/DCC.1999.755665
   Robertson P, 1998, IEEE J SEL AREA COMM, V16, P206, DOI 10.1109/49.661109
   SLEPIAN D, 1973, IEEE T INFORM THEORY, V19, P471, DOI 10.1109/TIT.1973.1055037
   UNGERBOECK G, 1982, IEEE T INFORM THEORY, V28, P55, DOI 10.1109/TIT.1982.1056454
   WANG A, 2006, P IEEE INT C IM PROC
   WEERAKKODY WAR, 2006, P INT C IM PROC ICIP
   WYNER AD, 1976, IEEE T INFORM THEORY, V22, P1, DOI 10.1109/TIT.1976.1055508
NR 15
TC 5
Z9 5
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2009
VL 25
IS 1
BP 69
EP 82
DI 10.1007/s00371-008-0279-z
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 379RC
UT WOS:000261412900007
DA 2024-07-18
ER

PT J
AU Rossinelli, D
   Koumoutsakos, P
AF Rossinelli, Diego
   Koumoutsakos, Petros
TI Vortex methods for incompressible flow simulations on the GPU
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE vortex methods; particles; fluids; graphics processors
ID SMOOTHED PARTICLE HYDRODYNAMICS; GRAPHICS
AB We present a remeshed vortex particle method for incompressible flow simulations on GPUs. The particles are convected in a Lagrangian frame and are periodically reinitialized on a regular grid. The grid is used in addition to solve for the velocity-vorticity Poisson equation and for the computation of the diffusion operators. In the present GPU implementation of particle methods, the remeshing and the solution of the Poisson equation rely on fast and efficient mesh-particle interpolations. We demonstrate that particle remeshing introduces minimal artificial dissipation, enables a faster computation of differential operators on particles over grid-free techniques and can be efficiently implemented on GPUs. The results demonstrate that, contrary to common practice in particle simulations, it is necessary to remesh the (vortex) particle locations in order to solve accurately the equations they discretize, without compromising the speed of the method. The present method leads to simulations of incompressible vortical flows on GPUs with unprecedented accuracy and efficiency.
C1 [Rossinelli, Diego; Koumoutsakos, Petros] ETH, Chair Computat Sci, CH-8092 Zurich, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; ETH Zurich
RP Rossinelli, D (corresponding author), ETH, Chair Computat Sci, CH-8092 Zurich, Switzerland.
EM diegor@inf.ethz.ch; petros@ethz.ch
RI Koumoutsakos, Petros/KDN-6339-2024; Koumoutsakos, Petros/P-4535-2019
OI Koumoutsakos, Petros/0000-0001-8337-2122; Koumoutsakos,
   Petros/0000-0001-8337-2122
CR Amada T., 2004, ACM WORKSH GEN PURP, V41, P42
   [Anonymous], 2000, Vortex Methods: Theory and Practice
   Bergdorf M, 2006, MULTISCALE MODEL SIM, V5, P980, DOI 10.1137/060652877
   Briggs William, 2000, A Multigrid Tutorial, Vsecond
   Chaniotis AK, 2002, J COMPUT PHYS, V182, P67, DOI 10.1006/jcph.2002.7152
   Cottet GH, 2006, MATH MOD METH APPL S, V16, P415, DOI 10.1142/S0218202506001212
   Foster N, 1997, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P178, DOI 10.1109/CGI.1997.601299
   Georgii J, 2005, SIMUL MODEL PRACT TH, V13, P693, DOI 10.1016/j.simpat.2005.08.004
   Hagen TR, 2006, LECT NOTES COMPUT SC, V3994, P220
   Harada T., 2007, P COMP GRAPH INT
   HARRIS MJ, 2004, GPU GEMS, P637
   Hegeman K, 2006, LECT NOTES COMPUT SC, V3994, P228
   Kolb A., 2004, P ACM SIGGRAPHEUROGR, P123
   Kolb Andreas., 2005, Proceedings of the 18th Symposium on Simulation Technique, P722
   Koumoutsakos P, 2005, ANNU REV FLUID MECH, V37, P457, DOI 10.1146/annurev.fluid.37.061903.175753
   Koumoutsakos P, 1997, J COMPUT PHYS, V138, P821, DOI 10.1006/jcph.1997.5749
   KRUGER J, 2004, PARSIM 2004
   Minion ML, 1997, J COMPUT PHYS, V138, P734, DOI 10.1006/jcph.1997.5843
   Monaghan JJ, 2005, REP PROG PHYS, V68, P1703, DOI 10.1088/0034-4885/68/8/R01
   Muller M., 2003, SCA, P154
   Pfister H, 2004, IEEE COMPUT GRAPH, V24, P22, DOI 10.1109/MCG.2004.15
   Scheidegger CE, 2005, COMPUT GRAPH FORUM, V24, P715, DOI 10.1111/j.1467-8659.2005.00897.x
   Selle A, 2005, ACM T GRAPHIC, V24, P910, DOI 10.1145/1073204.1073282
   Stam J., 2001, Journal of Graphics Tools, V6, P43, DOI 10.1080/10867651.2001.10487540
   Treuille A, 2006, ACM T GRAPHIC, V25, P826, DOI 10.1145/1141911.1141962
   Wu EH, 2007, VISUAL COMPUT, V23, P299, DOI 10.1007/s00371-007-0106-y
NR 26
TC 17
Z9 20
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 699
EP 708
DI 10.1007/s00371-008-0250-z
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800025
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Zhao, HL
   Jin, XG
   Shen, JB
   Mao, XY
   Feng, JQ
AF Zhao, Hanli
   Jin, Xiaogang
   Shen, Jianbing
   Mao, Xiaoyang
   Feng, Jieqing
TI Real-time feature-aware video abstraction
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE non-photorealistic rendering; visual communication; real-time video
   processing; image processing
AB This paper presents a novel feature-aware rendering system that automatically abstracts videos and images with the goal of improving the effectiveness of imagery for visual communication tasks. We integrate the bilateral grid to simplify regions of low contrast, which is faster than the separable approximation to the bilateral filter, and use a feature flow-guided anisotropic edge detection filter to enhance regions of high contrast. The edges detected in this paper are smoother, more coherent and stylistic than those of the isotropic difference-of-Gaussian filter. The presented algorithms are highly parallel, allowing a real-time performance on modern GPUs. The implementation of our approach is straightforward. Several experimental examples are given at the end of the paper to demonstrate the effectiveness of our approach.
C1 [Zhao, Hanli; Jin, Xiaogang; Feng, Jieqing] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
   [Shen, Jianbing] Indiana Univ Purdue Univ, Indianapolis, IN 46202 USA.
   [Mao, Xiaoyang] Univ Yamanashi, Yamanashi, Japan.
C3 Zhejiang University; Indiana University System; Indiana University
   Indianapolis; University of Yamanashi
RP Jin, XG (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
EM hanlizhao@gmail.com; jin@cad.zju.edu.cn; shenjian@iupui.edu;
   mao@yamanashi.ac.jp; jqfeng@cad.zju.edu.cn
RI Shen, Jianbing/U-8796-2019
OI Shen, Jianbing/0000-0002-4109-8353; mao, xiaoyang/0000-0001-9531-3197
CR [Anonymous], P SIGGRAPH
   [Anonymous], 1995, P MUSTERERKENNUNG 19
   [Anonymous], P ACM SIGGRAPH BOST
   [Anonymous], 2002, PROC ACM T GRAPH SIG, DOI DOI 10.1145/566570.566574
   Barash D, 2002, IEEE T PATTERN ANAL, V24, P844, DOI 10.1109/TPAMI.2002.1008390
   BLYTHE D, 2006, P ACM SIGGRAPH, P724
   Bousseau A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276507
   Cabral B., 1993, P 20 ANN C COMP GRAP, P263, DOI DOI 10.1145/166117.166151
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   CHEN J, 2007, P ACM SIGGRAPH 07 NE
   Collomosse JP, 2005, IEEE T VIS COMPUT GR, V11, P540, DOI 10.1109/TVCG.2005.85
   Fischer J, 2005, P IEEE VIRT REAL ANN, P195
   Gooch B, 2004, ACM T GRAPHIC, V23, P27, DOI 10.1145/966131.966133
   KANG H, 2007, P ACM S NONPH AN REN, P43
   MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020
   *MICR CORP, 2007, DIR 10 HLSL COMP DIR
   ORZAN A, 2007, P ACM S NONPH AN REN, P103
   Paris S., 2006, Proc. European Conference on Computer Vision, P568
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Pham TQ, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P454, DOI 10.1109/ICME.2005.1521458
   Smith SM, 1997, INT J COMPUT VISION, V23, P45, DOI 10.1023/A:1007963824710
   Son MJ, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P333, DOI 10.1109/PG.2007.63
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   WANG J, 2004, P ACM SIGGRAPH, P574
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Wyszecki G., 1982, Color science: Concepts and methods, quantitative data and formulae
NR 26
TC 23
Z9 31
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 727
EP 734
DI 10.1007/s00371-008-0254-8
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800028
DA 2024-07-18
ER

PT J
AU Sokolov, D
   Plemenos, D
AF Sokolov, Dmitry
   Plemenos, Dimitri
TI Virtual world explorations by using topological and semantic knowledge
SO VISUAL COMPUTER
LA English
DT Article
DE scene understanding; automatic virtual camera; virtual camera; semantic
   distance
AB This paper is dedicated to virtual world exploration techniques. Automatic camera control is important in many fields as computational geometry, visual servoing, robot motion, graph drawing, etc. The paper introduces a high-level camera controlling approach in virtual environments. The proposed method is related to real-time 3D scene exploration and is made of two steps. In the first step, a set of good viewpoints is chosen to give the user a maximum knowledge of the scene. The second step uses the viewpoints to compute a camera path between them. Finally, we define a notion of semantic distance between objects of the scene to improve the approach.
C1 [Sokolov, Dmitry; Plemenos, Dimitri] Univ Limoges, CNRS, UMR 6172, XLIM Lab, F-87000 Limoges, France.
C3 Universite de Limoges; Centre National de la Recherche Scientifique
   (CNRS)
RP Sokolov, D (corresponding author), Univ Limoges, CNRS, UMR 6172, XLIM Lab, 83 Rue Isle, F-87000 Limoges, France.
EM s@skisa.org; dimitrios.plemenos@unilim.fr
CR BARRAL P, 2000, P EUR 2000 SHORT PRE
   BARRAL P, 1999, GRAPHICON 99
   Blanz V, 1999, PERCEPTION, V28, P575, DOI 10.1068/p2897
   Chazelle B., 1995, SCG 95, P297
   Christofides N, 1976, 388 CARNEGIE MELLON 388 CARNEGIE MELLON
   DORME G, 2001, THESIS U LIMOGES FRA
   Feixas M, 1999, COMPUT GRAPH FORUM, V18, pC95, DOI 10.1111/1467-8659.00331
   FEIXAS M, 2002, THESIS U CATALONIA
   FOO N, 1992, SEMANTIC DISTANCE CO, P149
   Gooch B, 2001, SPRING EUROGRAP, P83
   KAMADA T, 1988, COMPUT VISION GRAPH, V41, P43, DOI 10.1016/0734-189X(88)90116-8
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   NOSER H, 1995, COMPUT GRAPH, V19, P7, DOI 10.1016/0097-8493(94)00117-H
   PLEMENOS D, 1991, THESIS
   PLEMENOS D, 2003, INT C GRAPHICON 2003
   PLEMENOS D, 1995, GRAPHICON 95
   Plemenos D., 1996, Graphicon'96
   SBERT M, 2002, INT C 3IA 2002 LIM F
   SBERT M, 2005, COMPUTATIONAL AESTHE
   SOKOLOV D, 2006, INT C COMP GRAPH THE
   SOKOLOV D, 2005, VAST 2005, P67
   Vazquez P.-P., 2001, Vision, Modeling, and Visualization 2001. Proceedings, P273
   Vazquez P.-P., 2003, P 6 INT C COMP GRAPH, P13
   VAZQUEZ PP, 2003, THESIS BARCELONA
   Zhong JW, 2002, LECT NOTES ARTIF INT, V2393, P92
   Zuckerberger E, 2002, COMPUT GRAPH-UK, V26, P733, DOI 10.1016/S0097-8493(02)00128-0
NR 26
TC 17
Z9 18
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2008
VL 24
IS 3
BP 173
EP 185
DI 10.1007/s00371-007-0182-z
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 255ZA
UT WOS:000252695900002
DA 2024-07-18
ER

PT J
AU Jin, C
   Fevens, T
   Li, S
   Mudur, S
AF Jin, Chao
   Fevens, Thomas
   Li, Shuo
   Mudur, Sudhir
TI Motion learning-based framework for unarticulated shape animation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE computer animation; keyframe; mesh deformation; motion learning
AB This paper presents a framework for generating animation sequences while maintaining desirable physical properties in a deformable shape. The framework consists of three important processes. Firstly, considering the given key pose configurations in the form of unarticulated meshes in high dimensional space, we cast our motion in low dimensional space using the unsupervised learning method of locally linear embedding (LLE). Corresponding to each point in LLE space, we can reconstruct the in-between pose using generalized radial basis functions. Next we create a map in the LLE space of the values for the different physical properties of the mesh, for example area, volume, etc. Finally, a probability distribution function in LLE space helps us rapidly choose the required number of in-between poses with desired physical properties. A significant advantage of this framework is that it relieves the animator the tedium of having to carefully provide key poses to suit the interpolant.
C1 Concordia Univ, Dept Comp Sci & Software Engn, Montreal, PQ, Canada.
C3 Concordia University - Canada
RP Jin, C (corresponding author), Concordia Univ, Dept Comp Sci & Software Engn, Montreal, PQ, Canada.
EM chao_jin@cse.concordia.ca; fevens@cse.concordia.ca; shuo.li@ge.com;
   mudur@cse.concordia.ca
RI Fevens, Thomas/T-1687-2019; Li, Shuo/N-5364-2019; Li, Shuo/F-9736-2017;
   Li, Shuo/GXV-6545-2022; Li, Shuo/HLV-7870-2023
OI Li, Shuo/0000-0002-5184-3230; 
CR Abdullah K, 2005, IEEE WORKSHOP ON VISUALIZATION FOR COMPUTER SECURITY 2005, PROCEEDINGS, P1
   [Anonymous], 2001, P 2001 S INTERACTIVE
   [Anonymous], 1993, Advances in neural information processing systems
   Arikan O, 2003, ACM T GRAPHIC, V22, P402, DOI 10.1145/882262.882284
   ARIKAN O, 2002, SIGGRAPH 02, P483
   Bishop CM, 1998, NEURAL COMPUT, V10, P215, DOI 10.1162/089976698300017953
   Bregler C., 1995, Advances in Neural Information Processing Systems 7, P973
   Bruderlin A., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P97, DOI 10.1145/218380.218421
   Der KG, 2006, ACM T GRAPHIC, V25, P1174, DOI 10.1145/1141911.1142011
   Elgammal A, 2005, PROC CVPR IEEE, P724
   Elgammal A, 2004, PROC CVPR IEEE, P681
   James DL, 2002, ACM T GRAPHIC, V21, P582, DOI 10.1145/566570.566621
   Kochanek D. H. U., 1984, Computers & Graphics, V18, P33
   Kohonen T., 1997, Self-Organizing Maps
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   KOVAR L, 2003, SCA 03, P214
   Lewis JP, 2000, COMP GRAPH, P165, DOI 10.1145/344779.344862
   LI Y, 2002, SIGGRAPH, P465
   Lipman Y, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1189762.1189767, 10.1145/1186644.1186649]
   Pentland A., 1989, IJCAI-89 Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, P1565
   POGGIO T, 1990, P IEEE, V78, P1481, DOI 10.1109/5.58326
   Rose C, 1998, IEEE COMPUT GRAPH, V18, P32, DOI 10.1109/38.708559
   Roweis SamT., 2003, J MACHINE LEARNING R, V4, P119
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Sumner RW, 2005, ACM T GRAPHIC, V24, P488, DOI 10.1145/1073204.1073218
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   THALMANN NM, 1985, COMPUTER ANIMATION T
   WITKIN A, 1995, SIGGRAPH 95, P105
   WITKIN A, 2001, ACM SIGGRAPH 2001
   ZHANG L, 1914, SIGGRAPH 04 ACM SIGG, P548
NR 30
TC 2
Z9 2
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 753
EP 761
DI 10.1007/s00371-007-0141-8
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600015
DA 2024-07-18
ER

PT J
AU Velho, L
   Sossai, J
AF Velho, Luiz
   Sossai, Jonas, Jr.
TI Projective texture atlas construction for 3D photography
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE 3D photography; texture; projective atlas
AB The use of attribute maps for 3D surfaces is an important issue in geometric modeling, visualization and simulation. Attribute maps describe various properties of a surface that are necessary in applications. In the case of visual properties, such as color, they are also called texture maps.
   Usually, the attribute representation exploits a parametrization g: U subset of R-2 -> R-3 of a surface in order to establish a two-dimensional domain where attributes are defined. However, it is not possible, in general, to find a global parametrization without introducing distortions into the mapping. For this reason, an atlas structure is often employed. The atlas is a set of charts defined by a piece-wise parametrization of a surface, which allows local mappings with small distortion.
   Texture atlas generation can be naturally posed as an optimization problem where the goal is to minimize both the number of charts and the distortion of each mapping.
   Additionally, specific applications can impose other restrictions, such as the type of mapping. An example is 3D photography, where the texture comes from images of the object captured by a camera [4]. Consequently, the underlying parametrization is a projective mapping. In this work, we investigate the problem of building and manipulating texture atlases for 3D photography applications. We adopt a variational approach to construct an atlas structure with the desired properties. For this purpose, we have extended the method of Cohen-Steiner et al. [6] to handle the texture mapping setup by minimizing distortion error when creating local charts. We also introduce a new metric tailored to projective maps that is suited to 3D photography.
C1 IMPA, Rio De Janeiro, Brazil.
C3 Instituto Nacional de Matematica Pura e Aplicada (IMPA)
RP Velho, L (corresponding author), IMPA, Rio De Janeiro, Brazil.
EM lvelho@impa.br; sossai@impa.br
RI Velho, Luiz/B-6979-2008
CR [Anonymous], 2002, Proceedings of the Symposium on Non-photorealistic animation and rendering
   Balmelli L, 2002, COMPUT GRAPH FORUM, V21, P411, DOI 10.1111/1467-8659.t01-1-00601
   Briggs W. L., MULTIGRID TUTORIAL
   Burt P. J., 1987, LAPLACIAN PYRAMID CO, P671
   Callieri M, 2002, VISION MODELING, AND VISUALIZATION 2002, PROCEEDINGS, P419
   Carr NA, 2004, ACM T GRAPHIC, V23, P845, DOI 10.1145/1015706.1015809
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   Hunter A, 2000, IEEE VISUAL, P243, DOI 10.1109/VISUAL.2000.885701
   Lévy B, 2002, ACM T GRAPHIC, V21, P362, DOI 10.1145/566570.566590
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   MALLAT S, 1992, IEEE T PATTERN ANAL, V14, P710, DOI 10.1109/34.142909
   Sander PV, 2001, COMP GRAPH, P409, DOI 10.1145/383259.383307
   SANDER PV, 2003, P EUR ACM SIGGRAPH S, P146
NR 14
TC 18
Z9 22
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 621
EP 629
DI 10.1007/s00371-007-0150-7
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Volino, P
   Magnenat-Thalmann, N
AF Volino, Pascal
   Magnenat-Thalmann, Nadia
TI Stop-and-go cloth draping
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE cloth simulation; draping; relaxation
ID PARTICLE-SYSTEM; ANIMATION
AB The aim of cloth draping is to compute the rest state of a piece of cloth, possibly in contact with other solid objects, as quickly as possible. The context of free motion and very large deformations specific to cloth simulation makes the usual energy minimization schemes traditionally used in mechanical engineering inefficient. Therefore, most cloth draping applications only rely on dynamic simulation with ad hoc viscous damping or the dissipative behavior of numerical integration methods for obtaining convergence to the rest position of the cloth. We propose a "stop-and-go" technique which cuts out the velocity of the object at particular times for converging to the rest state, while taking advantage of the natural cloth motion toward equilibrium. This scheme can very easily complement any existing dynamical cloth simulation system, using either implicit or explicit numerical integration methods.
C1 Univ Geneva, MIRA Lab, Geneva, Switzerland.
C3 University of Geneva
RP Volino, P (corresponding author), Univ Geneva, MIRA Lab, Geneva, Switzerland.
EM pascal@miralab.unige.ch; thalmann@miralab.unige.ch
RI Thalmann, Nadia/AAK-5195-2021
OI Thalmann, Nadia/0000-0002-1459-5960
CR [Anonymous], 2013, Theory and practice of finite elements
   BARAFF D, 1998, COMPUTER GRAPHICS, V32, P106
   Breen D. E., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P365, DOI 10.1145/192161.192259
   Bridson R., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P28
   CHOI KJ, 2002, COMPUTER GRAPHICS
   DEBRUNNE G, 2001, COMPUTER GRAPHICS, P31
   DESBRUN M, P GRAPH INT A K PET, P1
   Eberhardt B, 2000, SPRING COMP SCI, P137
   Eberhardt B, 1996, IEEE COMPUT GRAPH, V16, P52, DOI 10.1109/38.536275
   Eischen JW, 1996, IEEE COMPUT GRAPH, V16, P71, DOI 10.1109/38.536277
   Etzmuss O, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P244, DOI 10.1109/PCCGA.2003.1238266
   Etzmuss O, 2003, IEEE T VIS COMPUT GR, V9, P538, DOI 10.1109/TVCG.2003.1260747
   HAUTH M, 2001, EUROGRAPHICS 2001 P
   James DL, 1999, COMP GRAPH, P65, DOI 10.1145/311535.311542
   Meyer M, 2001, J VISUAL COMP ANIMAT, V12, P1, DOI 10.1002/vis.244
   Press W. H., 2002, Numerical Recipes in C: the Art of Scientific Computing, V2nd ed., DOI DOI 10.1119/1.14981
   PROVOT X, 1995, GRAPH INTER, P147
   Volino P, 2005, COMPUT ANIMAT VIRT W, V16, P163, DOI 10.1002/cav.78
   Volino P., 2005, Computer-Aided Design and Applications, V2, P645
   Volino P, 1997, INTERNATIONAL CONFERENCE ON VIRTUAL SYSTEMS AND MULTIMEDIA - VSMM'97, PROCEEDINGS, P109, DOI 10.1109/VSMM.1997.622337
   VOLINO P, 2001, COMP GRAPH INT P IEE
NR 21
TC 12
Z9 14
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 669
EP 677
DI 10.1007/s00371-007-0152-5
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600007
DA 2024-07-18
ER

PT J
AU De Luca, L
   Véron, P
   Florenzano, M
AF De Luca, Livio
   Veron, Philippe
   Florenzano, Michel
TI A generic formalism for the semantic modeling and representation of
   architectural elements
SO VISUAL COMPUTER
LA English
DT Article
DE architectural heritage; architectural knowledge; surveying;
   feature-based modeling; semantic shape
AB This article presents a methodological approach to the semantic description of architectural elements based both on theoretical reflections and research experiences. To develop this approach, a first process of extraction and formalization of architectural knowledge on the basis of the analysis of architectural treaties is proposed. Then, the identified features are used to produce a template shape library dedicated to buildings surveying. Finally, the problem of the overall model structuring and organization using semantic information is addressed for user handling purposes.
C1 Ecole Natl Super Architecture Marseille, CNRS, UMR MCC MAP Modeles & Simulat Architecture Urbani, F-13288 Marseille 09, France.
   Ecole Natl Super Arts & Metiers, CNRS, UMR SIS Sci Informat & Syst 6168, F-13617 Aix En Provence, France.
C3 Centre National de la Recherche Scientifique (CNRS); Centre National de
   la Recherche Scientifique (CNRS); Arts et Metiers Institute of
   Technology
RP De Luca, L (corresponding author), Ecole Natl Super Architecture Marseille, CNRS, UMR MCC MAP Modeles & Simulat Architecture Urbani, 184 Ave Luminy, F-13288 Marseille 09, France.
EM livio.deluca@map.archi.fr; philippe.veron@aix.ensam.fr;
   michel.florenzano@map.archi.fr
RI VERON, Philippe/I-3173-2012
OI Veron, Philippe/0000-0002-4062-2432; De Luca, Livio/0000-0003-0656-3165
CR ALBERTI LB, 1452, RE AEDIFICATORIA
   [Anonymous], 1619, TUTTE LOPERE DARCHIT
   [Anonymous], 1998, ARCHITECTURAL PRINCI
   [Anonymous], 1990, The logic of architecture: Design computation and cognition
   [Anonymous], 1799, GEOMETRIE DESCRIPTIV
   [Anonymous], 1996, PRIOR ANAL
   [Anonymous], 2000, GEOMETRIA ARCHITETTU
   [Anonymous], REV INT CFAO INFOGRA
   BLAISE JY, 2003, THESIS U AIX MARSEIL
   BLAISE JY, 2004, P 26 INT C INF TECHN
   De Luca L, 2006, COMPUT GRAPH-UK, V30, P160, DOI 10.1016/j.cag.2006.01.020
   DESARGUES G, 1640, BRUILLON PROJET EXEM
   Falcidieno B, 1998, P INT C COMP GRAPH I
   FORSSMAN E, 1978, B CTR INT STUDI ARCH, V20
   Fuchs F, 2000, LECT NOTES COMPUT SC, V1876, P427
   Gaiani M., 1999, P HER APPL 3D DIG IM
   GINOUVES Rene, 1992, DICT METHODIQUE ARCH, V2
   Gould D., 2002, COMPLETE MAYA PROGRA
   GOULETTE F, 1999, INTELLECTICA REV ASS, V29, P9
   HEINE E, 1999, P ICOMOS ISPRS COMM
   Palladio Andrea., 1965, 4 BOOKS ARCHITECTURE
   Perouse de Montclos JM, 1972, ARCHITECTURE VOCABUL
   PLASS M, 1983, P SIGGRAPH 83, P229
   Quintrand P., 1985, CAO ARCHITECTURE
   RAMAMOORTHI R, 1999, P SIGGRAPH 99
   Rattner D. M., 1998, PARALLEL CLASSICAL O
   REMONDINO F, 2003, P ISPRS INT WORKSH V
   SAINT-AUBIN Jean -Paul., 1992, RELEVE REPRESENTATIO
   SCHOLFIELD PH, 1958, THEORY PROPORTIONS A
   SPACCAPIETRA S, 2000, P INT WORKSH EM TECH
   Thompson DW., 1917, GROWTH FORM
   Tzonis A., 1986, CLASSICAL ARCHITECTU
   UNGERS OM, 1994, RINASCIMENTO BRUNELL
   Vallee L, 1853, SPECIMEN COUPE PIERR
   WITTKOVER R, 1968, B CTR INT STUDI ARCH, V10
NR 35
TC 46
Z9 46
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2007
VL 23
IS 3
BP 181
EP 205
DI 10.1007/s00371-006-0092-5
PG 25
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 134PM
UT WOS:000244095300003
DA 2024-07-18
ER

PT J
AU Cardin, S
   Thalmann, D
   Vexo, F
AF Cardin, Sylvain
   Thalmann, Daniel
   Vexo, Frederic
TI A wearable system for mobility improvement of visually impaired people
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 2005 HAPTEX Workshop
CY DEC 01, 2005
CL Hannover, GERMANY
DE wearable; vibrotactile; visually impaired
AB Degradation of the visual system can lead to a dramatic reduction of mobility by limiting a person to his sense of touch and hearing. This paper presents the development of an obstacle detection system for visually impaired people. While moving in his environment the user is alerted to close obstacles in range. The system we propose detects an obstacle surrounding the user by using a multi-sonar system and sending appropriate vibrotactile feedback. The system aims at increasing the mobility of visually impaired people by offering new sensing abilities.
C1 Ecole Polytech Fed Lausanne, Virtual Real Lab, CH-1015 Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Cardin, S (corresponding author), Ecole Polytech Fed Lausanne, Virtual Real Lab, CH-1015 Lausanne, Switzerland.
EM sylvain.cardin@epfl.ch; daniel.thalmann@epfl.ch; frederic.vexo@epfl.ch
RI Thalmann, Daniel/AAL-1097-2020; Thalmann, Daniel/A-4347-2008
OI Thalmann, Daniel/0000-0002-0451-7491; 
CR Benjamin J.M., 1973, P SAN DIEGO BIOMEDIC, V12, P53
   CRAIG JC, 1987, PERCEPT PSYCHOPHYS, V42, P309, DOI 10.3758/BF03203085
   CRAIG JC, 1972, PERCEPT PSYCHOPHYS, V11, P150, DOI 10.3758/BF03210362
   Espinosa MA, 1998, J ENVIRON PSYCHOL, V18, P277, DOI 10.1006/jevp.1998.0097
   Fusiello A, 2002, FOURTH IEEE INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, PROCEEDINGS, P39, DOI 10.1109/ICMI.2002.1166966
   Gallace A, 2005, World Haptics Conference: First Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virutual Environment and Teleoperator Systems, Proceedings, P12
   Gescheider G., 1974, Cutaneous Communications Systems and Devices
   GOFF GD, 1967, J EXP PSYCHOL, V74, P294, DOI 10.1037/h0024561
   HAHN JF, 1966, J EXP PSYCHOL, V71, P655, DOI 10.1037/h0023094
   Hillstrom AP, 2002, PERCEPT PSYCHOPHYS, V64, P1068, DOI 10.3758/BF03194757
   Ito K., 2005, Procedings of CHI05, Portland, OR, P1483, DOI [DOI 10.1145/1056808.1056947, 10.1145]
   RUPERT AH, 2002, RTO HFM S SPAT DIS M
   Schmidt R. F., 1979, FUNDAMENTALS SENSORY
   Strothotte T., 1996, Proceedings of the Second Annual ACM Conference on Assistive Technologies, P139, DOI [DOI 10.1145/228347.228369, 10.1145/228347.228369]
   van Erp J., 2003, Proceedings of Eurohaptics 2003, P405
   Van Erp J.B. F., 2002, P EUROHAPTICS 2002, P18
   van Erp JBF, 2005, WORLD HAPTICS CONFERENCE: FIRST JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRUTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P80
   VANERP J, 2005, P EUROHAPTICS 04 ACM, P166
   VERRILLO RT, 1965, J ACOUST SOC AM, V37, P843, DOI 10.1121/1.1909458
   Wexler M, 2001, NATURE, V409, P85, DOI 10.1038/35051081
   YANG U, 2002, P INT C ART REAL TEL, P4
NR 21
TC 103
Z9 107
U1 1
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2007
VL 23
IS 2
BP 109
EP 118
DI 10.1007/s00371-006-0032-4
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 125NZ
UT WOS:000243450200003
DA 2024-07-18
ER

PT J
AU Volino, P
   Davy, P
   Bonanni, U
   Luible, C
   Magnenat-Thalmann, N
   Mäkinen, M
   Meinander, H
AF Volino, Pascal
   Davy, Pierre
   Bonanni, Ugo
   Luible, Christiane
   Magnenat-Thalmann, Nadia
   Maekinen, Mailis
   Meinander, Harriet
TI From measured physical parameters to the haptic feeling of fabric
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 2005 HAPTEX Workshop
CY DEC 01, 2005
CL Hannover, GERMANY
DE haptics; force feedback; real-time; cloth-simulation; elastic
   deformation
AB Abstract real-time cloth simulation involves the solution of many computational challenges, particularly in the context of haptic applications, where high frame rates are necessary for obtaining a satisfactory tactile experience. In this paper, we present a real-time cloth simulation system that offers a compromise between a realistic physically-based simulation of fabrics and a haptic application with high requirements in terms of computation speed. We place emphasis on architecture and algorithmic choices for obtaining the best compromise in the context of haptic applications. A first implementation using a haptic device demonstrates the features of the proposed system and leads to the development of new approaches for haptic rendering using the proposed approach.
C1 Univ Geneva, MIRALab, CH-1211 Geneva 4, Switzerland.
   Tech Univ, Smartwear Lab, Tampere, Finland.
C3 University of Geneva
RP Volino, P (corresponding author), Univ Geneva, MIRALab, CH-1211 Geneva 4, Switzerland.
EM volino@miralab.unige.ch; davy@miralab.unige.ch;
   bonanni@miralab.unige.ch; luible@miralab.unige.ch;
   thalmann@miralab.unige.ch; mailis.makinen@TUT.FI;
   harriet.meinander@TUT.FI
RI Thalmann, Nadia/AAK-5195-2021
OI Thalmann, Nadia/0000-0002-1459-5960
CR *AATCC COMM, 2001, AATCC EV PROC, V5
   Astley OR, 1998, IEEE INT CONF ROBOT, P989, DOI 10.1109/ROBOT.1998.677216
   Baraff D, 2003, ACM T GRAPHIC, V22, P862, DOI 10.1145/882262.882357
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   BISHOP DP, 1996, TEXTILE PROGR, V26
   BRIDSON R, 2003, P 2003 ACM SIGGRAPH, P28
   Choi KJ, 2002, ACM T GRAPHIC, V21, P604, DOI 10.1145/566570.566624
   Desbrun M, 1999, PROC GRAPH INTERF, P1
   DILLON P, 2001, P HAPT HUM COMP INT
   Eberhardt B, 2000, SPRING COMP SCI, P137
   Eberhardt B, 1996, IEEE COMPUT GRAPH, V16, P52, DOI 10.1109/38.536275
   Eischen JW, 1996, IEEE COMPUT GRAPH, V16, P71, DOI 10.1109/38.536277
   GOVINDARAJ M, 2003, EUR C 2003
   Hauth M, 2001, COMPUT GRAPH FORUM, V20, pC319, DOI 10.1111/1467-8659.00524
   HUANG G, 2003, P 2003 ACM SIGGR EUR
   HUI CL, 2004, TEXT RES J       MAY
   MAGNENATTHALMAN.N, 2004, P EUR 2004
   MAKINEN M, 2005, P HAPTEX WORKSH U HA
   MEYER M, 2000, J VIS COMPUT ANIM
   Mezger J, 2003, WSCG'2003, VOL 11, NO 2, CONFERENCE PROCEEDINGS, P322
   Morgenbesser H. B., 1996, Proceedings of the ASME Dynamic Systems and Control Division, P407
   Ruspini D. C., 1997, ANN C COMP GRAPH INT, P345, DOI DOI 10.1145/258734.258878
   SALISBURY JK, 1995, P ACM S INT 3D GRAPH
   SALSEDO F, 2005, P HAPTEX WORKSH U HA
   Volino P, 2000, COMPUTER GRAPHICS INTERNATIONAL 2000, PROCEEDINGS, P257, DOI 10.1109/CGI.2000.852341
   VOLINO P, COMPUT AID DES APPL, V2
   VOLINO P, 2001, P C COMP GRAPH INT C
   ZILLES CB, 1995, IROS '95 - 1995 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS: HUMAN ROBOT INTERACTION AND COOPERATIVE ROBOTS, PROCEEDINGS, VOL 3, P146, DOI 10.1109/IROS.1995.525876
NR 28
TC 21
Z9 23
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2007
VL 23
IS 2
BP 133
EP 142
DI 10.1007/s00371-006-0034-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 125NZ
UT WOS:000243450200005
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Elber, G
   Cohen, E
AF Elber, Gershon
   Cohen, Elaine
TI Probabilistic silhouette based importance toward line-art
   non-photorealistic rendering
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE silhouettes; NPR rendering; Gaussian sphere; visibility determination;
   feature and suggestive contours
AB When pictorial information is presented, details of importance are typically emphasized. These include discontinuities in the geometry, highly curved regions, silhouettes, etc.
   This work analyzes the probability that certain smooth surface regions or polygonal edges possess silhouettes. This probability analysis is then associated with the visual importance of the local neighborhood, which is capable of capturing discontinuities and highly curved regions.
   A non-photorealistic rendering technique is subsequently proposed to take advantage of the silhouette-based importance. Based on this importance analysis, we present a completely automatic algorithm that creates line-art that captures visual features in the model in an appealing way.
C1 Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.
   Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.
C3 Technion Israel Institute of Technology; Utah System of Higher
   Education; University of Utah
RP Elber, G (corresponding author), Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.
EM gershon@cs.technion.ac.il; cohen@cs.utah.edu
CR [Anonymous], 1952, Geometry and the Imagination
   Benichou F., 1999, Proceedings. Seventh Pacific Conference on Computer Graphics and Applications (Cat. No.PR00293), P60, DOI 10.1109/PCCGA.1999.803349
   Bremer David., 1998, Implicit Surfaces, P155
   DECARLO D, 2003, COMPUGER GRAPHICS, P848
   Diepstraten J, 2002, COMPUT GRAPH FORUM, V21, P317, DOI 10.1111/1467-8659.t01-1-00591
   Do Carmo M., 1976, Differential Geometry of Curves and Surfaces
   Elber G., 1990, Computer Graphics, V24, P95, DOI 10.1145/97880.97890
   ELBER G, 1999, COMPUT GRAPH FORUM, V18, P1
   ELBER G, 2004, IRIT 9 5 USERS MANUA
   Hoschek J., 1993, FUNDAMENTALS COMPUTE
   INTERRANTE V, 1997, SIGGRAPH 97, P109
   Isenberg T, 2003, IEEE COMPUT GRAPH, V23, P28, DOI 10.1109/MCG.2003.1210862
   LU A, 2002, IEEE VISUALIZATION 2
   Markosian L., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P415, DOI 10.1145/258734.258894
   Meek DS, 2000, COMPUT AIDED GEOM D, V17, P521, DOI 10.1016/S0167-8396(00)00006-6
   Plantinga Simon., 2003, SM'03: Proceedings of the eighth ACM symposium on Solid modeling and applications, P23
   Pop Mihai., 2001, P 17 ANN S COMPUTATI, P60, DOI [10. 1145/378583. 378618, DOI 10.1145/378583.378618]
   RASKAR R, 1999, S INT 3D GRAPH, P135
   Salisbury MichaelP., 1997, SIGGRAPH 97, P401
   Sander PV, 2000, COMP GRAPH, P327, DOI 10.1145/344779.344935
   Schein S, 2004, VISUAL COMPUT, V20, P243, DOI 10.1007/s00371-003-0230-2
   Sousa MC, 2003, COMPUT GRAPH FORUM, V22, P381, DOI 10.1111/1467-8659.00685
   Sousa MC, 2003, COMPUT GRAPH FORUM, V22, P369, DOI 10.1111/1467-8659.00684
   THIRION JP, 1995, COMPUT VIS IMAGE UND, V61, P190, DOI 10.1006/cviu.1995.1015
NR 24
TC 2
Z9 2
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 793
EP 804
DI 10.1007/s00371-006-0065-8
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000022
DA 2024-07-18
ER

PT J
AU Li, M
   Gao, XS
   Chou, SC
AF Li, Ming
   Gao, Xiao-Shan
   Chou, Shang-Ching
TI Quadratic approximation to plane parametric curves and its application
   in approximate implicitization
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE rational approximation; conics; parametric curves; approximate
   implicitization
ID REAL ALGEBRAIC-CURVES; CONIC SPLINES; CURVATURE CONTINUITY
AB Expressing complex curves with simple parametric curve segments is widely used in computer graphics, CAD and so on. This paper applies rational quadratic B-spline curves to give a global C-1 continuous approximation to a large class of plane parametric curves including rational parametric curves. Its application in approximate implicitization is also explored. The approximated parametric curve is first divided into intrinsic triangle convex segments which can be efficiently approximated with rational quadratic Bezier curves. With this approximation, we keep the convexity and the cusp (sharp) points of the approximated curve with simple computations. High accuracy approximation is achieved with a small number of quadratic segments. Experimental results are given to demonstrate the operation and efficiency of the algorithm.
C1 Acad Sinica, Key Lab Math Mechanizat, Beijing, Peoples R China.
   Cardiff Univ, Sch Comp Sci, Cardiff, S Glam, Wales.
   Wichita State Univ, Dept Comp Sci, Wichita, KS 67208 USA.
C3 Chinese Academy of Sciences; Cardiff University; Wichita State
   University
RP Gao, XS (corresponding author), Acad Sinica, Key Lab Math Mechanizat, Beijing, Peoples R China.
EM xgao@mmrc.iss.ac.cn
RI GAO, XIAO/JED-3257-2023
CR Ahn YJ, 2001, COMPUT AIDED DESIGN, V33, P867, DOI 10.1016/S0010-4485(00)00110-X
   [Anonymous], 1998, QUANTIFIER ELIMINATI
   [Anonymous], MATH METHODS CAGD
   [Anonymous], GEOMETRIC MODELING A
   Bajaj CL, 1997, J COMPUT MATH, V15, P55
   Bazarra M.S., 1993, NONLINEAR PROGRAMMIN, V2nd
   BLOMGREN R, 1982, ALGORITHM CONVERT RA
   CHANG G, 1998, OVER OVER AGAIN
   Cho WJ, 1996, COMPUT AIDED GEOM D, V13, P497, DOI 10.1016/0167-8396(95)00042-9
   CHUANG JH, 1989, ACM T GRAPHIC, V8, P298, DOI 10.1145/77269.77272
   de Boor C., 1987, Computer-Aided Geometric Design, V4, P269, DOI 10.1016/0167-8396(87)90002-1
   DEGEN WLF, 1993, COMPUT AIDED GEOM D, V10, P293, DOI 10.1016/0167-8396(93)90043-3
   FARIN G, 1989, ACM T GRAPHIC, V8, P89, DOI 10.1145/62054.62056
   Gao XS, 2004, COMPUT AIDED GEOM D, V21, P805, DOI 10.1016/j.cagd.2004.07.009
   HU JY, 1991, IEEE COMPUT GRAPH, V11, P89, DOI 10.1109/38.67705
   KOTSIREAS I, 2003, COMPUTER MATH
   Li YM, 1997, COMPUT AIDED GEOM D, V14, P491, DOI 10.1016/S0167-8396(96)00041-6
   MONTAUDOUIN Y, 1986, COMPUT AIDED DESIGN, V18, P93
   Park H, 2001, COMPUT AIDED DESIGN, V33, P967, DOI 10.1016/S0010-4485(00)00133-0
   PIEGL L, 1987, IEEE COMPUT GRAPH, V7, P45, DOI [10.1109/MCG.1987.276964, 10.1109/MCG.1987.276871]
   POTTMANN H, 1991, ACM T GRAPHIC, V10, P366, DOI 10.1145/116913.116916
   POTTMANN H, 2002, PAC GRAPH 2002 P
   Pratt V., 1985, Computer Graphics, V19, P151, DOI 10.1145/325165.325225
   Quan L, 1996, IEEE T PATTERN ANAL, V18, P151, DOI 10.1109/34.481540
   Sánchez-Reyes J, 2003, COMPUT AIDED DESIGN, V35, P1305, DOI 10.1016/S0010-4485(03)00045-9
   Sederberg TW, 1999, GRAPH MODEL IM PROC, V61, P177, DOI 10.1006/gmip.1999.0497
   SHALABY M, 2002, AUTOMATED DEDUCTION, P161
   SHERBROOKE EC, 1993, COMPUT AIDED GEOM D, V10, P379, DOI 10.1016/0167-8396(93)90019-Y
   Wang K, 1997, ADSORPTION, V3, P267, DOI 10.1007/BF01653629
   Wang WP, 2006, ACM T GRAPHIC, V25, P214, DOI 10.1145/1138450.1138453
   Yang X, 2004, COMPUT AIDED DESIGN, V36, P461, DOI 10.1016/S0010-4485(03)00119-2
NR 31
TC 7
Z9 9
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING STREET, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 906
EP 917
DI 10.1007/s00371-006-0075-6
PG 12
WC Computer Science, Software Engineering
WE Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 082IW
UT WOS:000240381000033
DA 2024-07-18
ER

PT J
AU Seong, JK
   Kim, KJ
   Kim, MS
   Elber, G
AF Seong, JK
   Kim, KJ
   Kim, MS
   Elber, G
TI Perspective silhouette of a general swept volume
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 5th Israel-Korea Bi-National Conference on Geometric Modeling and
   Computer Graphics
CY OCT 11-12, 2004
CL Seoul Natl Univ, Seoul, SOUTH KOREA
HO Seoul Natl Univ
DE perspective silhouette; sweep surface; topology; zero-set computation;
   time varying silhouette
AB We present an efficient and robust algorithm for computing the perspective silhouette of the boundary of a general swept volume. We also construct the topology of connected components of the silhouette. At each instant t, a three-dimensional object moving along a trajectory touches the envelope surface of its swept volume along a characteristic curve Kt. The same instance of the moving object has a silhouette curve L-t on its own boundary. The intersection K-t boolean AND L-t contributes to the silhouette of the general swept volume. We reformulate this problem as a system of two polynomial equations in three variables. The connected components of the resulting silhouette curves are constructed by detecting the instances where the two curves K-t and L-t intersect each other tangentially on the surface of the moving object. We also consider a general case where the eye position changes while moving along a predefined path. The problem is reformulated as a system of two polynomial equations in four variables, where the zero-set is a two-manifold. By analyzing the topology of the zero-set, we achieve an efficient algorithm for generating a continuous animation of perspective silhouettes of a general swept volume.
C1 Seoul Natl Univ, Sch Engn & Comp Sci, Seoul 151, South Korea.
   Kyungpook Natl Univ, Dept Comp Engn, Taejon, South Korea.
   Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.
C3 Seoul National University (SNU); Kyungpook National University; Technion
   Israel Institute of Technology
RP Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.
EM seong@cs.utah.edu; mskim@cse.snu.ac.kr; kujinkim@mail.knu.ac.kr;
   gershon@cs.technion.ac.il
CR *AL WAV TECHN, 2003, MAY 5 0 US MAN
   [Anonymous], 2001, P 6 ACM S SOL MOD AP
   BAJAJ C, 1988, COMPUT AIDED GEOM D, V5, P309
   Bajaj CL, 1994, ADV COMPUT MATH, V2, P1
   Cipolla R., 2000, VISUAL MOTION CURVES
   Elber G, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P263, DOI 10.1109/SMI.2004.1314513
   *FLTK, 2002, FAST LIGHT TOOL KIT
   Foley J.D., 1990, Computer graphics: Principles and practice
   GOOCH A, 1998, SIGGRAPH 98, P447
   Gooch B., 2001, Non-photorealistic rendering
   *IRIT, 2002, IRIT 9 0 US MAN TECH
   Isenberg T, 2003, IEEE COMPUT GRAPH, V23, P28, DOI 10.1109/MCG.2003.1210862
   Joy K. I., 1999, Proceedings. Seventh Pacific Conference on Computer Graphics and Applications (Cat. No.PR00293), P82, DOI 10.1109/PCCGA.1999.803351
   Kalnins RD, 2003, ACM T GRAPHIC, V22, P856, DOI 10.1145/882262.882355
   Kim KJ, 2003, COMPUT GRAPH FORUM, V22, P15, DOI 10.1111/1467-8659.t01-1-00642
   KIM MS, 2000, MATH SURFACES, V9, P82
   MARTIN RR, 1990, COMPUT AIDED DESIGN, V22, P223, DOI 10.1016/0010-4485(90)90051-D
   Patrikalakis N., 2002, Shape Interrogation for Computer Aided Design and Manufacturing
   *VTK, 2003, VIS TOOL KIT VERS 4
NR 19
TC 5
Z9 6
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2006
VL 22
IS 2
BP 109
EP 116
DI 10.1007/s00371-006-0371-1
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 018GC
UT WOS:000235751200006
DA 2024-07-18
ER

PT J
AU Franken, T
   Dellepiane, M
   Ganovelli, F
   Cignoni, P
   Montani, C
   Scopigno, R
AF Franken, T
   Dellepiane, M
   Ganovelli, F
   Cignoni, P
   Montani, C
   Scopigno, R
TI Minimizing user intervention in registering 2D images to 3D models
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE image registration; 3D scanning; automatic acquisition
AB This paper proposes a novel technique to speed up the registration of 2D images to 3D models. This problem often arises in the process of digitalization of real objects, because pictures are often taken independently from the 3D geometry. Although there are a number of methods for solving the problem of registration automatically, they all need some further assumptions, so in the most general case the process still requires the user to provide some information about how the image corresponds to geometry, for example providing point-to-point correspondences. We propose a method based on a graph representation where the nodes represent the 2D photos and the 3D object, and arcs encode correspondences, which are either image-to-geometry or image-to-image point pairs. This graph is used to infer new correspondences from the ones specified by the user and from successful alignment of single images and to factually encode the state of the registration process. After each action performed by the user, our system explores the states space to find the shortest path from the current state to a state where all the images are aligned, i.e. a final state and, therefore, guides the user in the selection of further alignment actions for a faster completion of the job. Experiments on empirical data are reported to show the effectiveness of the system in reducing the user workload considerably.
C1 CNR, ISTI, I-56100 Pisa, Italy.
C3 Consiglio Nazionale delle Ricerche (CNR); Istituto di Scienza e
   Tecnologie dell'Informazione "Alessandro Faedo" (ISTI-CNR)
RP CNR, ISTI, Via Moruzzi 1, I-56100 Pisa, Italy.
EM thomas.franken@isti.cnr.it; matteo.dellepiane@isti.cnr.it;
   fabio.gaovelli@isti.cnr.it; paolo.cignoni@isti.cnr.it;
   claudio.montani@isti.cnr.it; roberto.scopigno@isti.cnr.it
RI Cignoni, Paolo/B-7192-2012; scopigno, roberto/AAH-7645-2020
OI Cignoni, Paolo/0000-0002-2686-8567; 
CR [Anonymous], 1999, Computer Graphics Forum (Eurographics '99)
   Bernardini F, 2002, COMPUT GRAPH FORUM, V21, P149, DOI 10.1111/1467-8659.00574
   Callieri M, 2004, IEEE COMPUT GRAPH, V24, P16, DOI 10.1109/MCG.2004.1274056
   Callieri M., 2002, 7 INT FALL WORK VIS, P419
   Dornaika F, 1997, P SOC PHOTO-OPT INS, V3174, P123, DOI 10.1117/12.279773
   Faugeras O. D., 1986, Proceedings CVPR '86: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.86CH2290-5), P15
   GOESELE M, 2004, THESIS U SAARBRUCKEN
   Lensch HPA, 2003, ACM T GRAPHIC, V22, P234, DOI 10.1145/636886.636891
   Lensch HPA, 2000, EIGHTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P317, DOI 10.1109/PCCGA.2000.883955
   LOWE DG, 1991, IEEE T PATTERN ANAL, V13, P441, DOI 10.1109/34.134043
   Neugebauer PJ, 1999, COMPUT GRAPH FORUM, V18, pC245
   Pulli K., 1997, Rendering Techniques '97. Proceedings of the Eurographics Workshop. Eurographics, P23
   Russell S., 1995, Prentice Hall series in artificial intelligence, V25, P27
   Sato Y., 1997, Proceedings of the 24th annual conference on Computer graphics and interactive techniques, P379
   Tsai R., 1987, IEEE Journal on Robotics and Automation, V3, p323,344
   *URL, 2001, CORP 1 OP SOURC COMP
   WINKLER S, 1997, P 19 DAGM S BRAUNSCH, P129
   Zhang Z., 1998, FLEXIBLE NEW TECHNIQ
NR 18
TC 34
Z9 43
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 619
EP 628
DI 10.1007/s00371-005-0309-z
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400014
DA 2024-07-18
ER

PT J
AU Prat, S
   Gioia, P
   Bertrand, Y
   Meneveaux, D
AF Prat, S
   Gioia, P
   Bertrand, Y
   Meneveaux, D
TI Connectivity compression in an arbitrary dimension
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE compression; connectivity; topological structure; generalized maps
AB This paper presents a general lossless connectivity compression scheme for manifolds in any dimension with arbitrary cells, orientable or not, with or without borders. Relying on a generic topological model called generalized maps, our method performs a region-growing traversal of its primitive elements while describing connectivity relations with symbols. The set of produced symbols is compressed using standard data compression techniques. These algorithms have been successfully applied to various models (surface, tetrahedral and hexahedral meshes), showing the efficiency and genericity of the proposed scheme.
C1 France Telecom R&D, Lannion, France.
   Lab SIC, Poitiers, France.
C3 Orange SA; Universite de Poitiers
RP France Telecom R&D, Lannion, France.
EM sylvain.prat@rd.francetelecom.com; patrick.gioia@rd.francetelecom.com;
   yves.bertrand@univ-poitiers.fr; daniel@sic.univ-poitiers.fr
OI Meneveaux, Daniel/0000-0001-7160-3026
CR ALLIEZ P, 2003, P S MULT GEOM MOD CA
   BRISSON E, 1989, PROCEEDINGS OF THE FIFTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY, P218, DOI 10.1145/73833.73858
   Gumhold S., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P51, DOI 10.1109/VISUAL.1999.809868
   Isenburg M, 2003, GRAPH MODELS, V65, P239, DOI 10.1016/S1524-0703(03)00044-4
   Isenburg M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P141, DOI 10.1109/VISUAL.2002.1183768
   Isenburg M, 2002, PROC GRAPH INTERF, P161
   Lienhardt P, 1994, INT J COMPUT GEOM AP, V4, P275, DOI 10.1142/S0218195994000173
   Szymczak Andrzej., 1999, Proceedings of the 5th ACM Symposium on Solid Modeling and Applications, P54
   WELCH TA, 1984, COMPUTER, V17, P8, DOI 10.1109/MC.1984.1659158
   Yang CK, 2000, IEEE VISUAL, P101, DOI 10.1109/VISUAL.2000.885682
   ZHANG Y, 2004, 0426 ICES U TEX
NR 11
TC 7
Z9 8
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 876
EP 885
DI 10.1007/s00371-005-0325-z
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400041
DA 2024-07-18
ER

PT J
AU Waschbüsch, M
   Würmlin, S
   Cotting, D
   Sadlo, F
   Gross, M
AF Waschbüsch, M
   Würmlin, S
   Cotting, D
   Sadlo, F
   Gross, M
TI Scalable 3D video of dynamic scenes
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE 3D video; free-viewpoint video; scene acquisition; point-based graphics
AB In this paper we present a scalable 3D video framework for capturing and rendering dynamic scenes. The acquisition system is based on multiple sparsely placed 3D video bricks, each comprising a projector, two grayscale cameras, and a color camera. Relying on structured light with complementary patterns, texture images and pattern-augmented views of the scene are acquired simultaneously by time-multiplexed projections and synchronized camera exposures. Using space-time stereo on the acquired pattern images, high-quality depth maps are extracted, whose corresponding surface samples are merged into a view-independent, point-based 3D data structure. This representation allows for effective photo-consistency enforcement and outlier removal, leading to a significant decrease of visual artifacts and a high resulting rendering quality using EWA volume splatting. Our framework and its view-independent representation allow for simple and straightforward editing of 3D video. In order to demonstrate its flexibility, we show compositing techniques and spatiotemporal effects.
C1 ETH, Dept Comp Sci, Comp Graph Lab, Zurich, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; ETH Zurich
RP ETH, Dept Comp Sci, Comp Graph Lab, Zurich, Switzerland.
EM vaschbuesch@inf.ethz.ch; wuermlin@inf.ethz.ch; dcotting@inf.ethz.ch;
   sadlof@inf.ethz.ch; grossm@inf.ethz.ch
CR ALEXA M, 2004, SIGGRAPH 04 COURSE N
   BAYAKOVSKI Y, 2002, INT C IM PROC ICIP02, V3, P25
   Bouguet J., CAMERA CALIBRATION T
   Broadhurst A, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P388, DOI 10.1109/ICCV.2001.937544
   Buehler C, 2001, COMP GRAPH, P425, DOI 10.1145/383259.383309
   Carranza J, 2003, ACM T GRAPHIC, V22, P569, DOI 10.1145/882262.882309
   COCKSHOTT WP, 2003, VISION IMAGE SIGNAL, P28
   Cotting D, 2004, ISMAR 2004: THIRD IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P100, DOI 10.1109/ISMAR.2004.30
   Goldlücke B, 2002, VISION MODELING, AND VISUALIZATION 2002, PROCEEDINGS, P455
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   GROSS M, 2003, SIGGRAPH03, P819
   HOFSETZ C, 2005, IEEE CG A, V24, P34
   Iddan GJ, 2001, P SOC PHOTO-OPT INS, V4298, P48, DOI 10.1117/12.424913
   Kanade T, 1997, IEEE MULTIMEDIA, V4, P34, DOI 10.1109/93.580394
   KANG S, 2004, CVPRW 04
   KANG SB, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P88, DOI 10.1109/ICCV.1995.466802
   Levin D, 2004, MATH VISUAL, P37
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Matusik W, 2001, SPRING EUROGRAP, P115
   Matusik W, 2000, COMP GRAPH, P369, DOI 10.1145/344779.344951
   MATUSIK W, 2004, SIGGRAPH 04
   Mulligan J, 2000, IEEE AND ACM INTERNATIONAL SYMPOSIUM ON AUGMENTED REALITY, PROCEEDING, P105, DOI 10.1109/ISAR.2000.880933
   Pauly M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P163, DOI 10.1109/VISUAL.2002.1183771
   PAULY M, 2001, SIGGRAPH 01
   Pfister H, 2000, COMP GRAPH, P335, DOI 10.1145/344779.344936
   Redert A, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P313
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Shade J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P231, DOI 10.1145/280814.280882
   SMOLIC A, 2003, JTC1SC29WG11N6194 IS
   VEDULA S, 2002, EGRW 02, P65
   WEYRICH T, 2004, EUR S POINT BAS GRAP
   Wilburn B, 2005, ACM T GRAPHIC, V24, P765, DOI 10.1145/1073204.1073259
   Würmlin S, 2004, COMPUT GRAPH-UK, V28, P3, DOI 10.1016/j.cag.2003.10.015
   Würmlin S, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P325, DOI 10.1109/PCCGA.2002.1167876
   YANG JC, 2002, EGRW 02, P77
   Zhang L, 2004, ACM T GRAPHIC, V23, P548, DOI 10.1145/1015706.1015759
   Zhang L, 2003, PROC CVPR IEEE, P367
   Zitnick CL, 2004, ACM T GRAPHIC, V23, P600, DOI 10.1145/1015706.1015766
   Zwicker M, 2002, IEEE T VIS COMPUT GR, V8, P223, DOI 10.1109/TVCG.2002.1021576
NR 39
TC 48
Z9 55
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 629
EP 638
DI 10.1007/s00371-005-0346-7
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400015
DA 2024-07-18
ER

PT J
AU Yang, ZW
   Deng, JS
   Chen, FL
AF Yang, ZW
   Deng, JS
   Chen, FL
TI Fitting unorganized point clouds with active implicit B-spline curves
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE curve reconstruction; active implicit B-spline curve; trust region
   algorithm; geometric distance
AB In computer-aided geometric design and computer graphics, fitting point clouds with a smooth curve (known as curve reconstruction) is a widely investigated problem. In this paper, we propose an active model to solve the curve reconstruction problem, where the point clouds are approximated by an implicit B-spline curve, i.e., the zero set of a bivariate tensor-product B-spline function. We minimize the geometric distance between the point clouds and the implicit B-spline curve and an energy term (or smooth term) which helps to extrude the possible extra branches of the implicit curve. In each step of the iteration, the trust region algorithm in optimization theory is applied to solve the corresponding minimization problem. We also discuss the proper choice of the initial shape of the approximation curve. Examples are provided to illustrate the effectiveness and robustness of our algorithm. The examples show that the proposed algorithm is capable of handling point clouds with complicated topologies.
C1 Univ Sci & Technol China, Dept Math, Hefei 230026, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Univ Sci & Technol China, Dept Math, Hefei 230026, Anhui, Peoples R China.
EM yangzw@ustc.edu.cn
RI Chen, Falai/C-3846-2013; Yang, Zhouwang/C-4339-2013; Deng,
   Jiansong/F-1869-2010
OI Deng, Jiansong/0000-0003-4093-373X
CR Bajaj ChandrajitL., 1995, Proc. Conf. on Computer graphics and interactive techniques, P109
   Blake A., 1998, ACTIVE CONTOURS
   BLOOMENTHAL J, 1998, INTRO IMPLICIT SURFA
   Boissonnat J.-D., 2000, P 16 ANN S COMPUTATI, P223
   Brunnett G., 1993, Surveys on Mathematics for Industry, V3, P1
   CARR JC, 2001, P SIGGRAPH 01, P6776
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   Hartmann E, 1998, COMPUT AIDED GEOM D, V15, P377, DOI 10.1016/S0167-8396(97)00040-X
   Hilton A, 1998, COMPUT VIS IMAGE UND, V69, P273, DOI 10.1006/cviu.1998.0664
   Hoppe Hugues, 1992, P SIGGRAPH 92, P71, DOI DOI 10.1145/133994.134011
   Hoschek J., 1993, FUNDAMENTALS COMPUTE
   Jüttler B, 2002, ADV COMPUT MATH, V17, P135, DOI 10.1023/A:1015200504295
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Kuhn H.W., 1951, P 2 BERKELEY S MATH, DOI DOI 10.1007/BF01582292
   MA WY, 1995, COMPUT AIDED DESIGN, V27, P663, DOI 10.1016/0010-4485(94)00018-9
   MORSE BS, 2001, SMI 01 P INT C SHAP, P8998
   MURAKI S, 1991, COMP GRAPH, V25, P227, DOI 10.1145/127719.122743
   Pottmann H, 2003, VISUALIZATION AND MATHEMATICS III, P221
   Pottmann H, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P8, DOI 10.1109/PCCGA.2002.1167835
   POWELL MJD, 1984, MATH PROGRAM, V29, P297, DOI 10.1007/BF02591998
   Turk G, 1999, COMP GRAPH, P335, DOI 10.1145/311535.311580
   WANG W, IN PRESS ACM T GRAPH
   YANG ZW, IN PRESS P 1 KOR CHI
   Zhao H., 2002, Geometric Level Set Methods in Imaging, Vision, and Graphics
   Zhao HK, 2000, COMPUT VIS IMAGE UND, V80, P295, DOI 10.1006/cviu.2000.0875
NR 25
TC 47
Z9 56
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 831
EP 839
DI 10.1007/s00371-005-0340-0
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400036
DA 2024-07-18
ER

PT J
AU Ou, SQ
   Bin, HZ
AF Ou, SQ
   Bin, HZ
TI Subdivision method to create furcating object with multibranches
SO VISUAL COMPUTER
LA English
DT Article
DE subdivision surface; furcating object modeling; joint mesh
ID SURFACES; DESIGN
AB A novel interim core scheme (ICS) is presented in this paper to construct a furcating object with multibranches. These M branches with arbitrary N-sided boundaries can be positioned freely but cannot be overlapped with each other. A furcating object can be built by blending these branches. The essence of the scheme is to construct a joint mesh that blends the initial control meshes of the M branches, and the smoothness of the resulting surfaces will only depend on the joint mesh and subdivision scheme applied. Some illustrative objects are given to verify the feasibility of ICS.
C1 Huazhong Univ Sci & Technol, Sch Mech Sci & Engn, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology
RP Huazhong Univ Sci & Technol, Sch Mech Sci & Engn, Wuhan 430074, Peoples R China.
EM smart_vr@21cn.com
CR Akleman E, 2000, EIGHTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P192, DOI 10.1109/PCCGA.2000.883941
   AKLEMAN E, 2001, SHAPE MODELING 2001, P183
   [Anonymous], 1998, CAD CAM PRINCIPLES P
   [Anonymous], 1996, THESIS U WASHINGTON
   BLOOMENTHAL J, 1991, COMP GRAPH, V25, P251, DOI 10.1145/127719.122757
   Bloomenthal J., 1995, THESIS U CALGARY
   CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   Chen J, 1997, THEOR COMPUT SCI, V181, P247, DOI 10.1016/S0304-3975(96)00273-3
   DOO D, 1978, COMPUT AIDED DESIGN, V10, P356, DOI 10.1016/0010-4485(78)90111-2
   Felkel P, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P70, DOI 10.1109/CGI.2004.1309194
   Galbraith C, 2002, VISUAL COMPUT, V18, P70, DOI 10.1007/s003710100136
   Galbraitht C, 2004, COMPUT GRAPH FORUM, V23, P351, DOI 10.1111/j.1467-8659.2004.00766.x
   Hoffmann CM, 1996, IEEE T VIS COMPUT GR, V2, P3, DOI 10.1109/2945.489381
   JEVANS D, 1988, 8829204 U CALG DEP C
   Khodakovsky A, 2000, COMP GRAPH, P271, DOI 10.1145/344779.344922
   KOBBELT L, 1998, 9 EUR WORKSH REND P, P69
   Levin A, 1999, COMP GRAPH, P57, DOI 10.1145/311535.311541
   Levin A, 1999, COMPUT AIDED GEOM D, V16, P345, DOI 10.1016/S0167-8396(98)00051-X
   LEVIN A, 2000, THESIS TEL AVIV U IS
   Loop C, 1987, THESIS U UTAH
   MACMURCHY P, 2004, THESIS U CALGARY
   Oeltze S., 2004, Proceedings of IEEE/Eurographics Symposium on Visualization (VisSym), P311
   PASKO A, 1995, VISUAL COMPUT, V11, P429, DOI 10.1007/BF02464333
   PASKO A, 2002, 4 INT WORKSH ADG 200, P132
   Pasko G, 2002, SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P95, DOI 10.1109/SMI.2002.1003533
   Piegl L., 1997, The Nurbs Book, Vsecond
   Piegl LA, 1999, ENG COMPUT, V15, P171, DOI 10.1007/s003660050012
   Piegl LA, 1999, VISUAL COMPUT, V15, P77, DOI 10.1007/s003710050163
   Piponi D, 2000, COMP GRAPH, P471, DOI 10.1145/344779.344990
   REIF U, 1995, COMPUT AIDED GEOM D, V12, P153, DOI 10.1016/0167-8396(94)00007-F
   Reif U, 1996, P AM MATH SOC, V124, P2167, DOI 10.1090/S0002-9939-96-03366-7
   SCHRODER P, 1995, 19954 U S CAR DEP MA
   Stam J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P395, DOI 10.1145/280814.280945
   Tobler RF, 2002, SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P35, DOI 10.1109/SMI.2002.1003526
   WYWILL B, 1999, COMPUT GRAPH FORUM, V18, P149
   Ye XZ, 2002, COMPUT AIDED GEOM D, V19, P513, DOI 10.1016/S0167-8396(02)00131-0
   ZORIN D, 1997, THESIS CALTECH PASAD
NR 37
TC 1
Z9 2
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2005
VL 21
IS 3
BP 170
EP 187
DI 10.1007/s00371-005-0280-8
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 921UV
UT WOS:000228791700004
DA 2024-07-18
ER

PT J
AU Song, WH
   Yang, XN
AF Song, WH
   Yang, XN
TI Free-form deformation with weighted T-spline
SO VISUAL COMPUTER
LA English
DT Article
DE free-form deformation; T-spline; w-TFFD
AB A new method of free-form deformation, w-TFFD, is proposed, for which an original shape is deformed using weighted T-spline volumes. We generalize T-splines to weighted T-spline volumes that also permit T-junctions. Weighted T-spline volumes are a natural generalization of NURBS volumes but permit more flexible control lattices. Thus, w-TFFD holds many virtues of traditional FFDs and is more adaptive to objects with arbitrary topology or complex shape. The lattices can be automatically generated and approximate the shape of the object arbitrarily close by octree subdivision.
   Besides constructing and deforming a multiresolution lattice, users can also sculpt specific local details to their required shape by modifying weights. A set of direct-acting tools that are similar to previously proposed techniques can be applied to w-TFFD.
C1 Zhejiang Univ, Dept Math, Hangzhou 310027, Peoples R China.
C3 Zhejiang University
RP Zhejiang Univ, Dept Math, Hangzhou 310027, Peoples R China.
EM songwenhao@msn.com; yxn@zju.edu.cn
OI Yang, Xunnian/0000-0001-7501-1378
CR [Anonymous], 2002, P 2002 ACM SIGGRAPHE
   [Anonymous], 2001, P 2001 S INT 3D GRAP
   Bechmann D, 1997, COMPUT NETWORKS ISDN, V29, P1715, DOI 10.1016/S0169-7552(97)00082-2
   Bloomenthal J, 1999, SHAPE MODELING INTERNATIONAL '99 - INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P44, DOI 10.1109/SMA.1999.749322
   Chadwick J. E., 1989, ACM SIGGRAPH COMPUTE, V23, P243
   Chua C., 2000, P ACM SIGGRAPHEUROGR, P33
   Coquillart S., 1990, J. Computer Graphics, V24, P187, DOI DOI 10.1145/97880.97900
   COQUILLART S, 1991, ACM COMPUTER GRAPHIC, V25, P23
   DAVIES GC, 1991, BR J FAM PLANN, V17, P4
   Gottschalk S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P171, DOI 10.1145/237170.237244
   Griessmair J, 1989, P EUR 89, P137
   GUDUKBAY U, 1990, COMPUT GRAPH, V14, P491, DOI 10.1016/0097-8493(90)90071-5
   HSU WM, 1992, COMP GRAPH, V26, P177, DOI 10.1145/142920.134036
   Jorge N., 1999, Numerical Optimization
   Kalra P., 1992, Computer Graphics Forum, V11, pC59, DOI 10.1111/1467-8659.1130059
   KAZUYA G, 2003, P 8 ACM S SOL MOD AP, P226
   LAMOUSIN HJ, 1994, IEEE COMPUT GRAPH, V14, P59, DOI 10.1109/38.329096
   Lewis JP, 2000, COMP GRAPH, P165, DOI 10.1145/344779.344862
   MacCracken Ron., 1996, SIGGRAPH, P181, DOI DOI 10.1145/237170.237247
   Moccozet L, 1997, COMP ANIM CONF PROC, P93, DOI 10.1109/CA.1997.601047
   NOBLE RA, 1999, P INT C INF VIS
   Ono Y, 2002, FIRST INTERNATIONAL SYMPOSIUM ON CYBER WORLDS, PROCEEDINGS, P472, DOI 10.1109/CW.2002.1180915
   Sederberg T. W., 1986, Computer Graphics, V20, P151, DOI 10.1145/15886.15903
   Sederberg TN, 2003, ACM T GRAPHIC, V22, P477, DOI 10.1145/882262.882295
   SHAO J, 2003, P CAD COMPU GRAPH 29
   Singh K, 2000, PROC GRAPH INTERF, P35
   Yoshizawa Shin., 2003, P 8 ACM S SOLID MODE, P247, DOI DOI 10.1145/781606.781643
NR 27
TC 30
Z9 40
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2005
VL 21
IS 3
BP 139
EP 151
DI 10.1007/s00371-004-0277-8
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 921UV
UT WOS:000228791700002
DA 2024-07-18
ER

PT J
AU Chen, YS
   Ip, HHS
AF Chen, YS
   Ip, HHS
TI Texture evolution: 3D texture synthesis from single 2D growable texture
   pattern
SO VISUAL COMPUTER
LA English
DT Article
DE texture synthesis; solid texture; point rendering
AB An efficient model-independent 3D texture synthesis algorithm based on texture growing and texture turbulence is presented to create vivid 3D solid texture from a single 2D growable texture pattern. Given a 2D texture pattern of some growable material, our technique is able to create an anisotropic 3D volumetric texture cube to simulate the evolution of the material in 3D. An effective tiling scheme is designed to save computation and storage costs. Target objects are directly dipped into the synthesized 3D texture volume to generate creative, sculpture-like models that can be visualized with interactive speed. Our method is conceptually intuitive, computationally fast, and storage efficient compared with other solid texturing methods. As opposed to conventional 2D texture mapping work on polygonal surfaces, our approach is capable of decorating 3D point-rendering systems seamlessly. Furthermore, our combination of texture turbulence and texture growing techniques provides an attractive way to synthesize and tile natural 2D texture patterns, or generate simple but interesting motion textures.
C1 City Univ Hong Kong, Ctr Innovat Applicat Internet & Multimedia Techno, Dept Comp Sci, Image Comp Grp, Kowloon, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP City Univ Hong Kong, Ctr Innovat Applicat Internet & Multimedia Techno, Dept Comp Sci, Image Comp Grp, Tat Chee Ave, Kowloon, Hong Kong, Peoples R China.
EM yschen@cityu.edu.hk; cship@cityu.edu.hk
OI IP, Ho Shing Horace/0000-0002-1509-9002
CR [Anonymous], ACM S1GGRAPH 1997 C
   [Anonymous], 2001, Schooling for Tomorrow
   [Anonymous], TEXTURING MODELLING
   Barnsley M.F, 1988, The Science of Fractal Images
   BENSON D, 2002, COMPUTER GRAPHICS P, P101
   Cabral B., 1993, Computer Graphics Proceedings, P263, DOI 10.1145/166117.166151
   DEBRY D, 2002, P ACM SIGGRAPH, P763
   Dischler JM, 2001, COMPUT GRAPH-UK, V25, P135, DOI 10.1016/S0097-8493(00)00113-8
   Dischler JM, 1998, COMPUT GRAPH FORUM, V17, pC87, DOI 10.1111/1467-8659.00256
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Grossman J. P., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P181
   Heeger D. J., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P229, DOI 10.1145/218380.218446
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Lefebvre L, 2000, PROC GRAPH INTERF, P77
   LEVOY M, 1985, 85022 TR U N CAR
   Lewis J. P., 1989, Computer Graphics, V23, P263, DOI 10.1145/74334.74360
   Neyret F, 1999, COMP GRAPH, P235, DOI 10.1145/311535.311561
   Neyret F., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P147
   Peachey D. R., 1985, Computer Graphics, V19, P279, DOI 10.1145/325165.325246
   Perlin K, 2002, ACM T GRAPHIC, V21, P681, DOI 10.1145/566570.566636
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   Perlin K.H., 1989, 16 ANN C COMP GRAPH, P253, DOI 10.1145/74333.74359
   Pfister H, 2000, COMP GRAPH, P335, DOI 10.1145/344779.344936
   Praun E, 2000, COMP GRAPH, P465, DOI 10.1145/344779.344987
   Prusinkiewicz P., 1990, ALGORITHMIC BEAUTY P
   Rusinkiewicz S, 2000, COMP GRAPH, P343, DOI 10.1145/344779.344940
   Sander PV, 2001, COMP GRAPH, P409, DOI 10.1145/383259.383307
   Schödl A, 2000, COMP GRAPH, P489, DOI 10.1145/344779.345012
   Soatto S, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P439, DOI 10.1109/ICCV.2001.937658
   Soler C, 2002, ACM T GRAPHIC, V21, P673, DOI 10.1145/566570.566635
   TURK G, 1991, COMP GRAPH, V25, P289, DOI 10.1145/127719.122749
   Turk G, 2001, COMP GRAPH, P347, DOI 10.1145/383259.383297
   van Wijk JJ, 2002, ACM T GRAPHIC, V21, P745, DOI 10.1145/566570.566646
   VANWIJK JJ, 1991, COMP GRAPH, V25, P309, DOI 10.1145/127719.122751
   Wang YZ, 2002, LECT NOTES COMPUT SC, V2350, P583
   Wei LY, 2001, COMP GRAPH, P355, DOI 10.1145/383259.383298
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   WEI LY, 2001, THESIS STANFORD U
   Zwicker M, 2001, COMP GRAPH, P371, DOI 10.1145/383259.383300
NR 40
TC 10
Z9 12
U1 2
U2 28
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2004
VL 20
IS 10
BP 650
EP 664
DI 10.1007/s00371-004-0262-2
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 876TH
UT WOS:000225520000004
DA 2024-07-18
ER

PT J
AU Xue, QQ
   Hu, HP
   Bai, YP
   Cheng, R
   Wang, P
   Song, N
AF Xue, Qianqian
   Hu, Hongping
   Bai, Yanping
   Cheng, Rong
   Wang, Peng
   Song, Na
TI Underwater image enhancement algorithm based on color correction and
   contrast enhancement
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Underwater image enhancement; Color correction; Contrast enhancement;
   Multi-scale fusion; Multi-scale decomposition
ID OPTIMIZATION; RESTORATION; RETINEX
AB Due to the complex underwater environment and the selective absorption and scattering effect of water on light waves, underwater images often suffer from issues such as low contrast, color distortion, and blurred details. This paper presents a stable and effective algorithm for enhancing underwater images to address these challenges. Firstly, an improved color correction algorithm based on the gray world and minimum information loss is employed to remove the blue-green bias present in the images. Secondly, a contrast enhancement algorithm is based on the guided filter and wavelet decomposition to make the texture details of the image clearer. Then, the normalized weight map of the image is obtained to carry out multi-scale fusion. Finally, the fused image is applied to perform the multi-scale decomposition. The experimental results show that the algorithm proposed in this paper can correct the image color deviation, improve the image contrast and enhance the image details.
C1 [Xue, Qianqian; Hu, Hongping; Bai, Yanping; Cheng, Rong; Wang, Peng; Song, Na] North Univ China, Sch Math, Taiyuan 030051, Shanxi, Peoples R China.
C3 North University of China
RP Hu, HP (corresponding author), North Univ China, Sch Math, Taiyuan 030051, Shanxi, Peoples R China.
EM hhp92@163.com
FU This work was supported in part by Fundamental Research Program of
   Shanxi Province (Grant Numbers 20210302123019, 202103021224195,
   202103021224212, 202103021223189, 20210302123031) and Shanxi Scholarship
   Council of China (Grant numbers 2020-104, 2021-108). [20210302123019,
   202103021224195, 202103021224212, 202103021223189, 20210302123031];
   Fundamental Research Program of Shanxi Province [2020-104, 2021-108];
   Shanxi Scholarship Council of China
FX This work was supported in part by Fundamental Research Program of
   Shanxi Province (Grant Numbers 20210302123019, 202103021224195,
   202103021224212, 202103021223189, 20210302123031) and Shanxi Scholarship
   Council of China (Grant numbers 2020-104, 2021-108).
CR Ancuti CO, 2018, IEEE T IMAGE PROCESS, V27, P379, DOI 10.1109/TIP.2017.2759252
   Ancuti C, 2012, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.2012.6247661
   BUCHSBAUM G, 1980, J FRANKLIN I, V310, P1, DOI 10.1016/0016-0032(80)90058-7
   Burt P. J., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P173, DOI 10.1109/ICCV.1993.378222
   Chang HH, 2020, IEEE ACCESS, V8, P38650, DOI 10.1109/ACCESS.2020.2971019
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cox L., 1977, OPT ACTA INT J OPT, V24, P779, DOI [10.1080/713819629, DOI 10.1080/713819629]
   Drews PLJ, 2016, IEEE COMPUT GRAPH, V36, P24, DOI 10.1109/MCG.2016.26
   [付青青 Fu Qingqing], 2020, [海洋学报, Acta Oceanologica Sinica], V42, P130
   Fu XY, 2017, I S INTELL SIG PROC, P789, DOI 10.1109/ISPACS.2017.8266583
   Galdran A, 2015, J VIS COMMUN IMAGE R, V26, P132, DOI 10.1016/j.jvcir.2014.11.006
   Gasparini F, 2004, PATTERN RECOGN, V37, P1201, DOI 10.1016/j.patcog.2003.12.007
   Gonzalez R. C., 1977, Digital image processing, DOI 10.1109/TASSP.1980.1163437
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hou GJ, 2018, IET IMAGE PROCESS, V12, P292, DOI 10.1049/iet-ipr.2017.0359
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Kumar M, 2020, IEEE T IMAGE PROCESS, V29, P7525, DOI 10.1109/TIP.2020.3004036
   Lai Y, 2023, VISUAL COMPUT, V39, P4133, DOI 10.1007/s00371-022-02580-5
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Li C., 2019, Pattern Recogn.Recogn, V98
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li CY, 2018, IEEE SIGNAL PROC LET, V25, P323, DOI 10.1109/LSP.2018.2792050
   Liang Z, 2022, IEEE T CIRC SYST VID, V32, P4879, DOI 10.1109/TCSVT.2021.3114230
   Lim J., 2012, JW POW STUD RES C
   Limare N, 2011, IMAGE PROCESS ON LIN, V1, P297, DOI 10.5201/ipol.2011.llmps-scb
   Lin RJ, 2022, VISUAL COMPUT, V38, P4419, DOI 10.1007/s00371-021-02305-0
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo WL, 2021, IEEE ACCESS, V9, P31792, DOI 10.1109/ACCESS.2021.3060947
   McGlamery B. L., 1979, Proceedings of the Society of Photo-Optical Instrumentation Engineers, V208, P221
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Muniraj M., 2022, Comput. Electr. Eng.. Electr. Eng, V100, P1
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Peng YT, 2017, IEEE T IMAGE PROCESS, V26, P1579, DOI 10.1109/TIP.2017.2663846
   Polesel A, 2000, IEEE T IMAGE PROCESS, V9, P505, DOI 10.1109/83.826787
   Rahman ZU, 1996, P SOC PHOTO-OPT INS, V2847, P183, DOI 10.1117/12.258224
   Wang KY, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11131591
   Wang SQ, 2022, EARTH SCI INFORM, V15, P1647, DOI 10.1007/s12145-022-00804-9
   Weng CC, 2005, IEEE INT SYMP CIRC S, P3801
   Xu ZF, 2020, IEEE ACCESS, V8, P55289, DOI 10.1109/ACCESS.2020.2981823
   Zhang WD, 2022, IEEE T IMAGE PROCESS, V31, P3997, DOI 10.1109/TIP.2022.3177129
   Zhang WD, 2022, IEEE J OCEANIC ENG, V47, P718, DOI 10.1109/JOE.2022.3140563
   Zhang WD, 2019, IEEE ACCESS, V7, P72492, DOI 10.1109/ACCESS.2019.2920403
   Zhang WB, 2022, J MAR SCI ENG, V10, DOI 10.3390/jmse10030430
   Zhou JC, 2019, IEEE ACCESS, V7, P122459, DOI 10.1109/ACCESS.2019.2934981
   Zhuang PX, 2022, IEEE T IMAGE PROCESS, V31, P5442, DOI 10.1109/TIP.2022.3196546
NR 46
TC 3
Z9 3
U1 21
U2 28
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 9
PY 2023
DI 10.1007/s00371-023-03117-0
EA OCT 2023
PG 28
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8PY5
UT WOS:001080561500001
DA 2024-07-18
ER

PT J
AU Lian, J
   Zhang, JJ
   Liu, JZ
   Dong, ZL
   Zhang, HK
AF Lian, Jing
   Zhang, Jiajun
   Liu, Jizhao
   Dong, Zilong
   Zhang, Huaikun
TI Guiding image inpainting via structure and texture features with dual
   encoder
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Image inpainting; Two-stage inpainting; Dual encoder
   structure
AB Image inpainting techniques have made rapid progresses in recent years. Recent advancements focus mainly on generating realistic and semantically plausible structure and texture features in missing regions. However, current popular inpainting methods rely typically on a single encoder-decoder or two separate encoder-decoders, which lead to inconsistent contextual semantics and blurry textures. To address the above issue, a dual-feature encoder implemented by structure and texture features is proposed. It utilizes skip connection to guide its corresponding decoder to fill image structure information (deep layer) and texture information (shallow layer) in the attention-based latent space. Additionally, we design multi-scale receptive fields to further improve the consistency of contextual semantics and image details. The experimental findings demonstrate that our method can effectively repair the structure and texture information of missing images with superior performance on three commonly used datasets. Furthermore, we build a mural dataset from the Mogao Grottoes and successfully restore them using our network.
C1 [Lian, Jing; Zhang, Jiajun; Dong, Zilong] Lanzhou Jiaotong Univ, Sch Elect & Informat Engn, Lanzhou, Peoples R China.
   [Lian, Jing; Liu, Jizhao; Zhang, Huaikun] Lanzhou Univ, Sch Informat Sci & Engn, Lanzhou, Peoples R China.
C3 Lanzhou Jiaotong University; Lanzhou University
RP Zhang, JJ (corresponding author), Lanzhou Jiaotong Univ, Sch Elect & Informat Engn, Lanzhou, Peoples R China.
EM lian322scc@163.com; zhangjiajunwork@163.com
OI Zhang, Jiajun/0009-0008-1989-1196
FU The authors would like to thank the anonymous reviewers and an editor
   for their insightful comments and suggestions which help to improve the
   quality of the work. This study was funded in National Natural Science
   Foundation of China (Grant Nos. 62061023, 8 [62061023, 82260364,
   61941109]; National Natural Science Foundation of China [21JR7RA345];
   Distinguished Young Scholars of Gansu Province of China [22JRJ5RA166,
   21JR1RA024, 23JRRA1485, 21JR1RA252]; Natural Science Foundation of Gansu
   Province of China [20JR10RA273]; Science and Technology Plan of Gansu
   Province
FX The authors would like to thank the anonymous reviewers and an editor
   for their insightful comments and suggestions which help to improve the
   quality of the work. This study was funded in National Natural Science
   Foundation of China (Grant Nos. 62061023, 82260364 and 61941109),
   Distinguished Young Scholars of Gansu Province of China (Grant No.
   21JR7RA345), Natural Science Foundation of Gansu Province of China
   (Grant Nos. 22JRJ5RA166, 21JR1RA024, 23JRRA1485 and 21JR1RA252), and
   Science and Technology Plan of Gansu Province (Grant No. 20JR10RA273).
CR Bao JM, 2017, IEEE I CONF COMP VIS, P2764, DOI 10.1109/ICCV.2017.299
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Cao ZW, 2021, IEEE WINT CONF APPL, P1187, DOI 10.1109/WACV48630.2021.00123
   Chan TF, 2001, J VIS COMMUN IMAGE R, V12, P436, DOI 10.1006/jvci.2001.0487
   Chan TF, 2002, SIAM J APPL MATH, V62, P1019, DOI 10.1137/S0036139900368844
   Chen MY, 2020, NEUROCOMPUTING, V405, P259, DOI 10.1016/j.neucom.2020.03.090
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Cui YM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8118, DOI 10.1109/ICCV48922.2021.00803
   Demir U., 2018, arXiv
   Doersch C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185597
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Huang JB, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601205
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Jingyuan Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7757, DOI 10.1109/CVPR42600.2020.00778
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T., 2018, arXiv, DOI [10.48550/arXiv.1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Li L., 2023, ARXIV
   Liang D., 2023, IEEE T MULTIMED
   Liang D, 2022, AAAI CONF ARTIF INTE, P1555
   Liu DF, 2021, PROC CVPR IEEE, P9811, DOI 10.1109/CVPR46437.2021.00969
   Liu DF, 2021, AAAI CONF ARTIF INTE, V35, P6101
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu Hongyu, 2020, P EUR C COMP VIS, P725, DOI DOI 10.1007/978-3-030-58536-5_43
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Nazeri K, 2019, IEEE INT CONF COMP V, P3265, DOI 10.1109/ICCVW.2019.00408
   OShea K., 2015, arXiv
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Peng JL, 2021, PROC CVPR IEEE, P10770, DOI 10.1109/CVPR46437.2021.01063
   Qin Z, 2021, DISPLAYS, V69, DOI 10.1016/j.displa.2021.102028
   Quan WZ, 2022, IEEE T IMAGE PROCESS, V31, P2405, DOI 10.1109/TIP.2022.3152624
   Razavi A, 2019, ADV NEUR IN, V32
   Ren YR, 2019, IEEE I CONF COMP VIS, P181, DOI 10.1109/ICCV.2019.00027
   Xie CH, 2019, IEEE I CONF COMP VIS, P8857, DOI 10.1109/ICCV.2019.00895
   Xu LM, 2020, NEUROCOMPUTING, V402, P220, DOI 10.1016/j.neucom.2020.04.011
   Yan ZY, 2018, LECT NOTES COMPUT SC, V11218, P3, DOI 10.1007/978-3-030-01264-9_1
   Yang C, 2017, PROC CVPR IEEE, P4076, DOI 10.1109/CVPR.2017.434
   Yang SY, 2022, NEUROCOMPUTING, V510, P149, DOI 10.1016/j.neucom.2022.09.041
   Yi Z., 2020, CVPR, P7508
   Yoshida Y., 2017, arXiv
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zhang H, 2019, 36 INT C MACHINE LEA, V97
   Zhang YL, 2020, NEUROCOMPUTING, V396, P1, DOI 10.1016/j.neucom.2020.01.068
   Zhao L, 2020, PROC CVPR IEEE, P5740, DOI 10.1109/CVPR42600.2020.00578
   Zheng CX, 2019, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2019.00153
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
NR 50
TC 1
Z9 1
U1 2
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4303
EP 4317
AR s00371-023-03083-7
DI 10.1007/s00371-023-03083-7
EA SEP 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001072285000001
DA 2024-07-18
ER

PT J
AU Hu, QQ
   Wang, ZF
   Yao, ZM
   Dong, WQ
AF Hu, Qianqian
   Wang, Zhifang
   Yao, Zhenmin
   Dong, Wenqing
TI A family of hybrid iterative approximation methods for fitting blending
   curves
SO VISUAL COMPUTER
LA English
DT Article
DE Progressive iterative approximation; Least-squares fitting;
   Moore-Penrose generalized inverse; Convergence rate
ID B-SPLINE CURVE; SURFACE
AB Data fitting is a fundamental research problem in many scientific fields. The progressive iterative approximation for least-squares fitting is an effective method for fitting many data points, because of its simple iterative algorithm and intuitive operation. We combine an iterative method for computing Moore-Penrose generalized inverse with the classical progressive iterative approximation for least-squares fitting method to construct a family of accelerated iterative methods for fitting curves to data. The resulting iterative curve sequence converges to the least-squares fitting result for the given data set. We prove that the proposed method has faster average convergence rate and asymptotic convergence rate than previous methods. Some numerical examples illustrate the feasibility and efficiency of our method.
C1 [Hu, Qianqian; Wang, Zhifang; Yao, Zhenmin; Dong, Wenqing] Zhejiang Gongshang Univ, Sch Stat & Math, Hangzhou 310018, Peoples R China.
C3 Zhejiang Gongshang University
RP Hu, QQ (corresponding author), Zhejiang Gongshang Univ, Sch Stat & Math, Hangzhou 310018, Peoples R China.
EM qianqian_hu@163.com
OI Hu, Qianqian/0000-0002-0880-8173
FU National Natural Science Foundation of China [62272406]
FX The authors are very grateful to the referees for their helpful
   suggestions and comments. This work is supported by the National Natural
   Science Foundation of China (No. 62272406).
CR Carnicer JM, 2011, COMPUT AIDED GEOM D, V28, P523, DOI 10.1016/j.cagd.2011.09.005
   Chen HB, 2011, APPL MATH COMPUT, V218, P4012, DOI 10.1016/j.amc.2011.05.066
   Datta BN, 2010, NUMERICAL LINEAR ALGEBRA AND APPLICATIONS, SECOND EDITION, P1, DOI 10.1137/1.9780898717655
   de Boor C, 1979, ARO report 79-3, P299
   Deng CY, 2014, COMPUT AIDED DESIGN, V47, P32, DOI 10.1016/j.cad.2013.08.012
   Ebrahimi A, 2019, J COMPUT APPL MATH, V359, P1, DOI 10.1016/j.cam.2019.03.025
   Hamza YF, 2020, COMPUT AIDED GEOM D, V77, DOI 10.1016/j.cagd.2020.101817
   Horn RA., 1991, MATRIX ANAL, DOI DOI 10.1017/CBO9780511840371
   Hu QQ, 2022, VISUAL COMPUT, V38, P3819, DOI 10.1007/s00371-021-02223-1
   [蒋旖旎 Jiang Yini], 2022, [中国科学. 信息科学, Scientia Sinica Informationis], V52, P1251
   Kineri Y, 2012, COMPUT AIDED DESIGN, V44, P697, DOI 10.1016/j.cad.2012.02.011
   Lawson C. L., 1995, SOLVING LEAST SQUARE
   Lin HW, 2018, J SYST SCI COMPLEX, V31, P1618, DOI 10.1007/s11424-018-7443-y
   Lin HW, 2015, COMPUT AIDED DESIGN, V67-68, P107, DOI 10.1016/j.cad.2015.05.004
   Lin HW, 2013, SIAM J SCI COMPUT, V35, pA3052, DOI 10.1137/120888569
   Lin HW, 2010, COMPUT AIDED GEOM D, V27, P322, DOI 10.1016/j.cagd.2010.01.003
   Lin HW, 2005, COMPUT MATH APPL, V50, P575, DOI 10.1016/j.camwa.2005.01.023
   Lin HW, 2004, SCI CHINA SER F, V47, P315, DOI 10.1360/02yf0529
   Liu CZ, 2020, J COMPUT APPL MATH, V366, DOI 10.1016/j.cam.2019.112389
   Lu LZ, 2010, COMPUT AIDED GEOM D, V27, P129, DOI 10.1016/j.cagd.2009.11.001
   Marco A, 2007, LINEAR ALGEBRA APPL, V422, P616, DOI 10.1016/j.laa.2006.11.020
   Martin T, 2009, COMPUT AIDED GEOM D, V26, P648, DOI 10.1016/j.cagd.2008.09.008
   Piegl L., 2012, The NURBS book
   Qi D, 1975, Acta Math Sin, V18, P173
   Sajavicius S, 2023, J COMPUT APPL MATH, V422, DOI 10.1016/j.cam.2022.114888
   Toutounian F, 2013, APPL MATH COMPUT, V224, P671, DOI 10.1016/j.amc.2013.08.086
   Varga RS., 1962, Matrix Iterative Analysis
   Wang HD, 2022, VISUAL COMPUT, V38, P591, DOI 10.1007/s00371-020-02036-8
   Zhang L, 2016, VISUAL COMPUT, V32, P1109, DOI 10.1007/s00371-015-1170-3
   Zhang XA, 2006, APPL MATH COMPUT, V183, P522, DOI 10.1016/j.amc.2006.05.098
NR 30
TC 1
Z9 1
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4287
EP 4301
DI 10.1007/s00371-023-03082-8
EA SEP 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001161701400001
DA 2024-07-18
ER

PT J
AU Qin, YM
   Chi, XY
   Sheng, B
   Lau, RWH
AF Qin, Yiming
   Chi, Xiaoyu
   Sheng, Bin
   Lau, Rynson W. H.
TI GuideRender: large-scale scene navigation based on multi-modal view
   frustum movement prediction
SO VISUAL COMPUTER
LA English
DT Article
DE Distributed parallel rendering; Multi-modal; View frustum movement
   prediction; Attentional guidance fusion
ID GAZE PREDICTION; FRAMEWORK
AB Distributed parallel rendering provides a valuable way to navigate large-scale scenes. However, previous works typically focused on outputting ultra-high-resolution images. In this paper, we target on improving the interactivity of navigation and propose a large-scale scene navigation method, GuideRender, based on multi-modal view frustum movement prediction. Given previous frames, user inputs and object information, GuideRender first extracts frames, user inputs and objects features spatially and temporally using the multi-modal extractor. To obtain effective fused features for prediction, we introduce an attentional guidance fusion module to fuse these features of different domains with attention. Finally, we predict the movement of the view frustum based on the attentional fused features and obtain its future state for loading data in advance to reduce latency. In addition, to facilitate GuideRender, we design an object hierarchy hybrid tree for scene management based on the object distribution and hierarchy, and an adaptive virtual sub-frustum decomposition method based on the relationship between the rendering cost and the rendering node capacity for task decomposition. Experimental results show that GuideRender outperforms baselines in navigating large-scale scenes. We also conduct a user study to show that our method satisfies the navigation requirements in large-scale scenes.
C1 [Qin, Yiming; Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Chi, Xiaoyu] Beihang Univ, Qingdao Res Inst, Qingdao, Peoples R China.
   [Qin, Yiming; Lau, Rynson W. H.] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
C3 Shanghai Jiao Tong University; Beihang University; City University of
   Hong Kong
RP Sheng, B (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
EM shengbin@sjtu.edu.cn
OI Qin, Yiming/0000-0001-8831-1644
FU National Key Research and Development Program of China [2022YFC2407000];
   Interdisciplinary Program of Shanghai Jiao Tong University [YG2023LC11,
   YG2022ZD007]; National Natural Science Foundation of China [62272298,
   62077037]; College-level Project Fund of Shanghai Jiao Tong University
   Affiliated Sixth People's Hospital [ynlc201909]; Medical-industrial
   Cross-fund of Shanghai Jiao Tong University [YG2022QN089]
FX & nbsp;This work was supported by the National Key Research and
   Development Program of China under grant number 2022YFC2407000, the
   Interdisciplinary Program of Shanghai Jiao Tong University under grant
   number YG2023LC11 and YG2022ZD007, National Natural Science Foundation
   of China under grant number 62272298 and 62077037, the College-level
   Project Fund of Shanghai Jiao Tong University Affiliated Sixth People's
   Hospital under grant number ynlc201909, the Medical-industrial
   Cross-fund of Shanghai Jiao Tong University under grant number
   YG2022QN089.
CR Armeni Iro, 2017, arXiv
   Borji A, 2012, PROC CVPR IEEE, P470, DOI 10.1109/CVPR.2012.6247710
   Chim J. H. P., 1998, Proceedings ACM Multimedia 98, P171, DOI 10.1145/290747.290769
   Cornia M, 2018, IEEE T IMAGE PROCESS, V27, P5142, DOI 10.1109/TIP.2018.2851672
   Crockett TW, 1997, PARALLEL COMPUT, V23, P819, DOI 10.1016/S0167-8191(97)00028-8
   Lai DQ, 2015, IEEE T VIS COMPUT GR, V21, P714, DOI 10.1109/TVCG.2015.2398439
   Eilemann S, 2020, IEEE T VIS COMPUT GR, V26, P1292, DOI 10.1109/TVCG.2018.2870822
   Eilemann S, 2009, IEEE T VIS COMPUT GR, V15, P436, DOI 10.1109/TVCG.2008.104
   Han MJ, 2020, 2020 IEEE VISUALIZATION CONFERENCE - SHORT PAPERS (VIS 2020), P11, DOI 10.1109/VIS47514.2020.00009
   Hartmann D, 2021, SEMA SIMAI SPRING S, V5, P3, DOI 10.1007/978-3-030-61844-5_1
   Hu ZM, 2021, IEEE T VIS COMPUT GR, V27, P2681, DOI 10.1109/TVCG.2021.3067779
   Hu ZM, 2020, IEEE T VIS COMPUT GR, V26, P1902, DOI 10.1109/TVCG.2020.2973473
   Humphreys G, 2002, ACM T GRAPHIC, V21, P693, DOI 10.1145/566570.566639
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Johnson GP, 2012, IEEE INT C CL COMP, P239, DOI 10.1109/CLUSTER.2012.78
   Karras Tero., 2013, Proceedings of the 5th High-Performance Graphics Conference, P89, DOI DOI 10.1145/2492045.2492055
   Koulieris GA, 2016, P IEEE VIRT REAL ANN, P113, DOI 10.1109/VR.2016.7504694
   Kroner A, 2020, NEURAL NETWORKS, V129, P261, DOI 10.1016/j.neunet.2020.05.004
   Kümmerer M, 2017, IEEE I CONF COMP VIS, P4799, DOI 10.1109/ICCV.2017.513
   Lee D, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21113678
   Marrinan T, 2019, SYMP LARG DATA ANAL, P37, DOI [10.1109/ldav48142.2019.8944369, 10.1109/LDAV48142.2019.8944369]
   Meagher D, 1980, OCTREE ENCODING NEW
   Moloney B, 2011, IEEE T VIS COMPUT GR, V17, P1164, DOI 10.1109/TVCG.2010.116
   Ning H., 2021, arXiv
   Paszke A, 2019, ADV NEUR IN, V32
   Pribyl J., 2010, Proceedings 2010 IEEE/IFIP 8th International Conference on Embedded and Ubiquitous Computing (EUC 2010), P247, DOI 10.1109/EUC.2010.43
   Ren XW, 2021, INT S HIGH PERF COMP, P709, DOI 10.1109/HPCA51647.2021.00065
   Repplinger M, 2009, LECT NOTES COMPUT SC, V5875, P975, DOI 10.1007/978-3-642-10331-5_91
   Theis Lucas, 2018, ARXIV
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Wald I, 2017, IEEE T VIS COMPUT GR, V23, P931, DOI 10.1109/TVCG.2016.2599041
   Wu CL, 2020, AAAI CONF ARTIF INTE, V34, P14003
   Xu M, 2019, IEEE T PATTERN ANAL, V41, P2693, DOI 10.1109/TPAMI.2018.2858783
   Xu YY, 2018, PROC CVPR IEEE, P5333, DOI 10.1109/CVPR.2018.00559
   Yao JL, 2009, LECT NOTES COMPUT SC, V5709, P264
NR 35
TC 21
Z9 21
U1 2
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3597
EP 3607
DI 10.1007/s00371-023-02922-x
EA JUN 2023
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001016490300002
DA 2024-07-18
ER

PT J
AU Sommerhoff, H
   Kolb, A
AF Sommerhoff, Hendrik
   Kolb, Andreas
TI Hashed, binned A-buffer for real-time outlier removal and rendering of
   noisy point clouds
SO VISUAL COMPUTER
LA English
DT Article
DE Point rendering; Splatting; Denoising
ID VISUALIZATION; ROBUST
AB Typical point-based rendering algorithms cannot directly handle large outliers or high amounts of noise without either a costly pre-processing of the point cloud or using multiple render passes. In this paper, we propose an A-buffer-based approach that directly renders unprocessed point clouds and filters out outliers in real time, without introducing additional render passes. The core concept of our approach uses bins along each pixel ray in which intermediate information is accumulated. To improve storage efficiency, we extend the binned A-buffer approach using per-ray bin hashing. Our method significantly improves visual quality when rendering noisy point clouds with varying noise levels and large outliers, while only requiring little performance and memory overhead compared to traditional point rendering methods.
C1 [Sommerhoff, Hendrik; Kolb, Andreas] Univ Siegen, Ctr Sensor Syst ZESS, Comp Graph Grp, D-57076 Siegen, Germany.
C3 Universitat Siegen
RP Sommerhoff, H (corresponding author), Univ Siegen, Ctr Sensor Syst ZESS, Comp Graph Grp, D-57076 Siegen, Germany.
EM hendrik.sommerhoff@uni-siegen.de; andreas.kolb@uni-siegen.de
RI Kolb, Andreas/S-8085-2019
OI Kolb, Andreas/0000-0003-4753-7801; Sommerhoff,
   Hendrik/0000-0001-5262-821X
FU Projekt DEAL
FX Open Access funding enabled and organized by Projekt DEAL.
CR [Anonymous], 1998, The art of computer programming: Sorting and searching
   Bavoil L., 2008, ORDER INDEPENDENT TR
   Bavoil L, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P97
   Bienert A., 2007, Proceedings of International Society for Photogrammetry and Remote Sensing Workshop on Laserscanning 2007 and SilviLaser, P50
   Botsch M, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P335, DOI 10.1109/PCCGA.2003.1238275
   Botsch M., 2004, Proceedings Eurographics Symposium on Point-Based Graphics 2004, P25, DOI 10.5555/2386332.2386338
   Botsch Mario, 2005, P EUROGRAPHICSIEEE V, P17, DOI [DOI 10.2312/SPBG/SPBG05/017-024, 10.1109/PBG.2005.194059.6]
   Carpenter L., 1984, Computers & Graphics, V18, P103
   Choi S, 2015, PROC CVPR IEEE, P5556, DOI 10.1109/CVPR.2015.7299195
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   Dai P, 2020, P IEEECVF C COMPUTER, P7830
   Diankov R, 2007, GRAPP 2007: PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL GM/R, P228
   Dong Z, 2020, ISPRS J PHOTOGRAMM, V163, P327, DOI 10.1016/j.isprsjprs.2020.03.013
   Everitt C, 2001, INTERACTIVE ORDER IN
   Goswami P, 2013, VISUAL COMPUT, V29, P69, DOI 10.1007/s00371-012-0675-2
   Handa A, 2014, IEEE INT CONF ROBOT, P1524, DOI 10.1109/ICRA.2014.6907054
   Jouppi N. P., 1999, Proceedings 1999 EUROGRAPHICS/SIGGRAPH Workshop on Graphics Hardware, P85, DOI 10.1145/311534.311582
   Kellert M., 2013, 2013 Conference on Lasers & Electro-Optics. Europe & International Quantum Electronics Conference (CLEO EUROPE/IQEC), DOI 10.1109/CLEOE-IQEC.2013.6800663
   Kim D, 2021, COMPUT GRAPH FORUM, V40, P149, DOI 10.1111/cgf.14409
   Kivi PEJ, 2022, IEEE ACCESS, V10, P13151, DOI 10.1109/ACCESS.2022.3146768
   Kolb A, 2010, COMPUT GRAPH FORUM, V29, P141, DOI 10.1111/j.1467-8659.2009.01583.x
   Lambers M, 2015, IEEE SENS J, V15, P4019, DOI 10.1109/JSEN.2015.2409816
   Lefebvre S., 2013, THESIS INRIA
   Lefloch D, 2017, IEEE T PATTERN ANAL, V39, P2349, DOI 10.1109/TPAMI.2017.2648803
   Levoy Marc, 1985, The use of points as a display primitive
   Liu F., 2010, Proceedings of the 2010 ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, P75, DOI [DOI 10.1145/1730804.1730817, DOI 10.1145/1730804]
   Liu Fang., 2009, Proceedings of High Performance Graphics, P51, DOI DOI 10.1145/1572769.1572779
   McGuire M., 2017, Computer Graphics Archive
   Mekuria R, 2017, IEEE T CIRC SYST VID, V27, P828, DOI 10.1109/TCSVT.2016.2543039
   Metzer G, 2022, COMPUT GRAPH FORUM, V41, P461, DOI 10.1111/cgf.14487
   Pauly M, 2003, ACM T GRAPHIC, V22, P641, DOI 10.1145/882262.882319
   Pintus R., 2011, Proceedings of the 12th International conference on Virtual Reality, Archaeology and Cultural Heritage, P105
   Pomerleau Francois, 2015, Found. Trends Robot., V4, P1
   Preiner Reinhold, 2012, EGPGV, P139
   Rusinkiewicz S, 2000, COMP GRAPH, P343, DOI 10.1145/344779.344940
   Seemann P., 2018, P EUR S REND EXP ID, P95
   Uchida T, 2020, ISPRS J PHOTOGRAMM, V161, P124, DOI 10.1016/j.isprsjprs.2020.01.004
   Vasilakis AA, 2020, COMPUT GRAPH FORUM, V39, P623, DOI 10.1111/cgf.14019
   Wendland H., 1995, Advances in Computational Mathematics, V4, P389, DOI 10.1007/BF02123482
   Whelan T, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI
   Yang JC, 2010, COMPUT GRAPH FORUM, V29, P1297, DOI 10.1111/j.1467-8659.2010.01725.x
   Zwicker M, 2001, COMP GRAPH, P371, DOI 10.1145/383259.383300
NR 42
TC 0
Z9 0
U1 1
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1825
EP 1838
DI 10.1007/s00371-023-02888-w
EA JUN 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:001007611500001
OA hybrid
DA 2024-07-18
ER

PT J
AU Li, Y
   Liu, YY
   Zhou, CE
   Xu, DZ
   Tao, WB
AF Li, Yi
   Liu, Youyu
   Zhou, Chuanen
   Xu, Dezhang
   Tao, Wanbao
TI A lightweight scheme of deep appearance extraction for robust online
   multi-object tracking
SO VISUAL COMPUTER
LA English
DT Article
DE Online multi-object tracking; Tracking-by-detection paradigm;
   Re-identification; Exponential moving average
AB Appearance-based Multi-Object Tracking (MOT) methods rely on the appearance cues of objects. However, existing deep appearance extraction schemes struggle to balance speed, performance, and memory footprint. In this article, a lightweight Re-identification network named Fast OSNet is proposed by simplifying the OSNet structure, adding attention modules, and introducing a global and partial-level feature fusion mechanism. To reduce the impact of occlusion noise on trajectory appearance states, the Hierarchical Adaptive Exponential Moving Average (HAEMA) is proposed, which employs adaptive update weights with a two-stage linear transformation. Together, Fast OSNet and HAEMA make up the proposed lightweight scheme. To validate the proposed scheme, it is combined with the full detection-association algorithm BYTE and proposed Fast Deep BYTE Track (FDBTrack). On the MOT17 test set, it achieves 63.2 High-Order Tracking Accuracy (HOTA) and 77.7 Identification F1-score (IDF1). On the more challenging MOT20 test set, it achieves 62.0 HOTA and 75.9 IDF1. It can serve as an auxiliary mean to improve the tracking performance of online MOT methods. The codes are available at https:// github.com/LiYi199983/FDBTrack.
C1 [Li, Yi; Liu, Youyu; Xu, Dezhang; Tao, Wanbao] Anhui Res Ctr Gener Technol Robot Ind, Wuhu 241000, Peoples R China.
   [Zhou, Chuanen] Anhui Naike Equipment Technol Co LTD, Tongling 244000, Peoples R China.
   [Li, Yi; Liu, Youyu; Xu, Dezhang; Tao, Wanbao] Anhui Polytech Univ, Sch Mech Engn, Wuhu 241000, Peoples R China.
C3 Anhui Polytechnic University
RP Liu, YY (corresponding author), Anhui Res Ctr Gener Technol Robot Ind, Wuhu 241000, Peoples R China.; Liu, YY (corresponding author), Anhui Polytech Univ, Sch Mech Engn, Wuhu 241000, Peoples R China.
EM ahpulyy3@163.com; liuyyu@ahpu.edu.cn; zoubaozhu@nike277.wecom.work;
   xdz@ahpu.edu.cn; 2210120152@stu.ahpu.edu.cn
OI Liu, Youyu/0000-0002-9836-0869
FU cooperation project of Anhui Institute of Future Technologies with
   enterprises [18]; practice and innovation project of Anhui Polytechnic
   University [2]
FX This work was supported in part by cooperation project of Anhui
   Institute of Future Technologies with enterprises under Grant (No. 18),
   and by practice and innovation project of Anhui Polytechnic University
   for postgraduate under Grant (No. 2).
CR Aharon N., 2022, ARXIV, DOI [DOI 10.48550/ARXIV.2206.14651, 10.48550/arXiv.2206.14651]
   Ahmed I, 2021, INT J MACH LEARN CYB, V12, P3053, DOI 10.1007/s13042-020-01220-5
   Bae SH, 2018, IEEE T PATTERN ANAL, V40, P595, DOI 10.1109/TPAMI.2017.2691769
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Cai ZW, 2021, PROC CVPR IEEE, P194, DOI 10.1109/CVPR46437.2021.00026
   Cao JK, 2023, PROC CVPR IEEE, P9686, DOI 10.1109/CVPR52729.2023.00934
   Chen J, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10202479
   Dendorfer P., 2020, Mot20: A benchmark for multi object tracking in crowded scenes
   Ding XH, 2022, PROC CVPR IEEE, P11953, DOI 10.1109/CVPR52688.2022.01166
   Du Y, 2023, IEEE Transactions on Multimedia
   Du YH, 2021, IEEE INT CONF COMP V, P2809, DOI 10.1109/ICCVW54120.2021.00315
   Ericsson L, 2022, IEEE SIGNAL PROC MAG, V39, P42, DOI 10.1109/MSP.2021.3134634
   Ge Z., 2021, ARXIV, DOI DOI 10.48550/ARXIV.2107.08430
   Gómez-Silva MJ, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9111757
   Guo W, 2022, MULTIMEDIA SYST, V28, P925, DOI 10.1007/s00530-022-00895-w
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   Kalman R E., 1960, J BASIC ENG, V82, P35, DOI DOI 10.1115/1.3662552
   Kaya M, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11091066
   Kishi N, 2021, IEEE GLOB COMM CONF, DOI 10.1109/GLOBECOM46510.2021.9686010
   Liang C, 2022, IEEE T IMAGE PROCESS, V31, P3182, DOI 10.1109/TIP.2022.3165376
   Luiten J, 2021, INT J COMPUT VISION, V129, P548, DOI 10.1007/s11263-020-01375-2
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Maggiolino G., 2023, ARXIV, DOI DOI 10.48550/ARXIV.2302.11813
   Milan A., 2016, ARXIV160300831
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sener O, 2018, ADV NEUR IN, V31
   Shen H, 2020, IEEE T SYST MAN CY-S, V50, P4796, DOI 10.1109/TSMC.2018.2866632
   Stadler D., 2021, 2021 17 IEEE INT C A, P1, DOI DOI 10.1109/AVSS52988.2021.9663836
   Sun KH, 2020, IEEE ACCESS, V8, P129169, DOI 10.1109/ACCESS.2020.3009852
   Sun P., 2020, ARXIV, DOI DOI 10.48550/ARXIV.2012.15460
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Traneva V, 2019, LECT NOTES COMPUT SC, V11189, P167, DOI 10.1007/978-3-030-10692-8_19
   Tsai CY, 2023, ENG APPL ARTIF INTEL, V119, DOI 10.1016/j.engappai.2022.105770
   Tsai CY, 2022, MULTIMED TOOLS APPL, V81, P9915, DOI 10.1007/s11042-022-12095-9
   Vidanpathirana M, 2020, VISUAL COMPUT, V36, P1501, DOI 10.1007/s00371-019-01757-9
   Wang WC, 2017, IEEE T MULTIMEDIA, V19, P1142, DOI 10.1109/TMM.2017.2652069
   Wang Z., 2020, COMPUTER VISION ECCV, P107, DOI [10.1007/978-3-030-58621-8_7, DOI 10.1007/978-3-030-58621-8_7]
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xie Ben, 2020, Pattern Recognition and Computer Vision. Third Chinese Conference, PRCV 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12307), P16, DOI 10.1007/978-3-030-60636-7_2
   Yu E, 2023, IEEE T MULTIMEDIA, V25, P2686, DOI 10.1109/TMM.2022.3150169
   Zhang H, 2022, IEEE COMPUT SOC CONF, P2735, DOI 10.1109/CVPRW56347.2022.00309
   Zhang J, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3105424
   Zhang X., 2017, ARXIV, DOI DOI 10.48550/ARXIV.1711.08184
   Zhang XQ, 2021, VISUAL COMPUT, V37, P1089, DOI 10.1007/s00371-020-01854-0
   Zhang YF, 2022, LECT NOTES COMPUT SC, V13682, P1, DOI 10.1007/978-3-031-20047-2_1
   Zhang YF, 2021, INT J COMPUT VISION, V129, P3069, DOI 10.1007/s11263-021-01513-4
   Zhang Z, 2018, IEEE ACCESS, V6, P36887, DOI 10.1109/ACCESS.2018.2852712
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhou X, 2019, PSYCHOL HEALTH, V34, P811, DOI 10.1080/08870446.2019.1574348
NR 56
TC 4
Z9 4
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 2049
EP 2065
DI 10.1007/s00371-023-02901-2
EA JUN 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:001000721000001
DA 2024-07-18
ER

PT J
AU Wu, JH
   Zhu, Y
   Jiang, XB
   Liu, YT
   Lin, JJ
AF Wu, Jiahui
   Zhu, Yu
   Jiang, Xiaoben
   Liu, Yatong
   Lin, Jiajun
TI Local attention and long-distance interaction of rPPG for deepfake
   detection
SO VISUAL COMPUTER
LA English
DT Article
DE Digital video forensics; Deepfake; PPG; CNN
AB With the development of generative models, abused Deepfakes have aroused public concerns. As a defense mechanism, face forgery detection methods have been intensively studied. Remote photoplethysmography (rPPG) technology extract heartbeat signal from recorded videos by examining the subtle changes in skin color caused by cardiac activity. Since the face forgery process inevitably disrupts the periodic changes in facial color, rPPG signal proves to be a powerful biological indicator for Deepfake detection. Motivated by the key observation that rPPG signals produce unique rhythmic patterns in terms of different manipulation methods, we regard Deepfake detection also as a source detection task. The Multi-scale Spatial-Temporal PPG map is adopted to further exploit heartbeat signal from multiple facial regions. Moreover, to capture both spatial and temporal inconsistencies, we propose a two-stage network consisting of a Mask-Guided Local Attention module (MLA) to capture unique local patterns of PPG maps, and a Temporal Transformer to interact features of adjacent PPG maps in long distance. Abundant experiments on FaceForensics + + and Celeb-DF datasets prove the superiority of our method over all other rPPG-based approaches. Visualization also demonstrates the effectiveness of the proposed method.
C1 [Wu, Jiahui; Zhu, Yu; Jiang, Xiaoben; Liu, Yatong; Lin, Jiajun] East China Univ Sci & Technol, Sch Informat Sci & Engn, Shanghai 200237, Peoples R China.
   [Zhu, Yu] Shanghai Engn Res Ctr Internet Things Resp Med, Shanghai 200032, Peoples R China.
C3 East China University of Science & Technology
RP Zhu, Y (corresponding author), East China Univ Sci & Technol, Sch Informat Sci & Engn, Shanghai 200237, Peoples R China.; Zhu, Y (corresponding author), Shanghai Engn Res Ctr Internet Things Resp Med, Shanghai 200032, Peoples R China.
EM zhuyu@ecust.edu.cn
RI Chen, Jin/KBQ-0163-2024; zheng, Li/JVN-7465-2024
OI Chen, Jin/0009-0005-5844-635X; Zhu, Yu/0000-0003-1535-6520
FU Natural Science Foundation of Shanghai [22ZR1444700]; National Natural
   Science Foundation of China [82170110]; Science and Technology
   Commission of Shanghai Municipality [20DZ2261200]
FX The authors greatly appreciate the financial supports of Natural Science
   Foundation of Shanghai under Grant 22ZR1444700, National Natural Science
   Foundation of China under Grant 82170110, Science and Technology
   Commission of Shanghai Municipality under Grant 20DZ2261200.
CR Afchar D, 2018, IEEE INT WORKS INFOR
   Baltrusaitis T, 2016, IEEE WINT CONF APPL
   Boccignone G, 2022, LECT NOTES COMPUT SC, V13232, P186, DOI 10.1007/978-3-031-06430-2_16
   Buchana P, 2016, IEEE IMAGE PROC, P146, DOI 10.1109/ICIP.2016.7532336
   Chen S, 2021, AAAI CONF ARTIF INTE, V35, P1081
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Ciftci U. A., 2020, FAKECATCHER DETECTIO
   Ciftci UA, 2020, IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2020)
   Dang H, 2020, PROC CVPR IEEE, P5780, DOI 10.1109/CVPR42600.2020.00582
   de Lima O., 2020, arXiv
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Goljan M., 2015, MWSF, V9409, P279
   Goodfellow I. J., 2014, ARXIV
   Haliassos A, 2021, PROC CVPR IEEE, P5037, DOI 10.1109/CVPR46437.2021.00500
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hertzman AB, 1937, P SOC EXP BIOL MED, V37, P529
   Jayakumar Krishnakripa, 2022, 2022 7th International Conference on Information Technology Research (ICITR), P1, DOI 10.1109/ICITR57877.2022.9993294
   Khan SA, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1821, DOI 10.1145/3474085.3475332
   Li, 2019, FACESHIFTER HIGH FID
   Li LZ, 2020, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR42600.2020.00505
   Li YZ, 2020, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR42600.2020.00327
   Liang Jing, 2021, 2021 IEEE International Conference on Multimedia and Expo (ICME), P1, DOI 10.1109/ICME51207.2021.9428309
   Liy C.M., 2018, 2018 IEEE INT G NAT, P1
   Malolan B, 2020, 2020 3RD INTERNATIONAL CONFERENCE ON INFORMATION AND COMPUTER TECHNOLOGIES (ICICT 2020), P289, DOI 10.1109/ICICT50521.2020.00051
   Montserrat DM, 2020, IEEE COMPUT SOC CONF, P2851, DOI 10.1109/CVPRW50498.2020.00342
   Nguyen H, 2019, ARXIV
   Pan X, 2012, INT CONF E BUS ENG, P17, DOI 10.1109/ICEBE.2012.13
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Perov I., 2020, DEEPFACELAB INTEGRAT
   PITIE F, 2007, AUTOMATED COLOUR GRA, V107, P123
   Pu Y., 2016, Advances in neural information processing systems, P29
   Qi H, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4318, DOI 10.1145/3394171.3413707
   Radford A., 2015, ARXIV
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Rössler A, 2019, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2019.00009
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Shang ZH, 2021, PATTERN RECOGN, V116, DOI 10.1016/j.patcog.2021.107950
   Tan MX, 2021, PR MACH LEARN RES, V139, P7102
   Thies J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323035
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Vaswani A, 2017, ADV NEUR IN, V30
   Verkruysse W, 2008, OPT EXPRESS, V16, P21434, DOI 10.1364/OE.16.021434
   Wang WJ, 2017, IEEE T BIO-MED ENG, V64, P1479, DOI 10.1109/TBME.2016.2609282
   Wu HY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185561
   Xuesong Niu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P295, DOI 10.1007/978-3-030-58536-5_18
   Yang X, 2019, INT CONF ACOUST SPEE, P8261, DOI 10.1109/ICASSP.2019.8683164
   Yuan X, 2021, RIVER RES APPL, V37, P1134, DOI [10.1002/rra.3785, 10.1145/3471621.3471854]
   Zheng YL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15024, DOI 10.1109/ICCV48922.2021.01477
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 51
TC 4
Z9 4
U1 6
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 1083
EP 1094
DI 10.1007/s00371-023-02833-x
EA MAR 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000967291000001
PM 37361461
OA Bronze, Green Published
DA 2024-07-18
ER

PT J
AU Zhao, S
   Gong, ZK
   Zhao, DY
AF Zhao, Shuen
   Gong, Zhikun
   Zhao, Dongyu
TI Traffic signs and markings recognition based on lightweight
   convolutional neural network
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Autonomous driving; Traffic signs and markings; Deep learning;
   Lightweight convolutional neural networks
ID ENHANCEMENT
AB Intelligent recognition of traffic signs and markings is an important component of autonomous driving and intelligent transportation systems. It is also an important theoretical basis for autonomous driving path planning. To address the low accuracy and poor real-time performance of the traffic signs and markings detection in complex and multivariate scenes, a lightweight convolutional neural network recognition method in multiple interference scenes is proposed. Firstly, based on Gamma correction and contrast limited histogram equalization algorithm, the traffic signs and markings image under the low illumination condition is enhanced adaptively. Then, the MobileNet-V2 fuse together with DeepLab-V3 + algorithm to segment traffic signs and markings in multiple scenes. Finally, an identification model is established to realize the adaptive recognition of traffic signs and markings, which are based on Lightweight Convolutional Neural Networks (Lw-CNN). Besides, the recognition method proposed in this paper is verified by the CSUST Chinese Traffic Sign Detection Benchmark (CCTSDB). The experimental results show that the Mean Intersection Over Union (MIOU) of the MobileNet-V2 and DeepLab-V3 + algorithm reaches 83.07%, it increased by 25.8% than before image enhancement. The recognition accuracy of algorithm based on lightweight convolutional neural network is 99.92%, higher than the algorithms of MobileNet-V2 and VGG16.
C1 [Zhao, Shuen; Gong, Zhikun; Zhao, Dongyu] Chongqing Jiaotong Univ, Sch Mechatron & Vehicle Engn, Chongqing 400074, Peoples R China.
C3 Chongqing Jiaotong University
RP Gong, ZK (corresponding author), Chongqing Jiaotong Univ, Sch Mechatron & Vehicle Engn, Chongqing 400074, Peoples R China.
EM 2496963458@qq.com
OI Zhikun, Gong/0000-0001-5309-0327
FU National Natural Science Foundation of China [52072054]; Special Key
   Project of Chongqing Technology Innovation and Application Development
   [cstc2021jscx-cylh0026]; Open Funding Key Laboratory of Industry and
   Information Technology [2021KFKT01]
FX This work was supported by the National Natural Science Foundation of
   China (52072054), the Special Key Project of Chongqing Technology
   Innovation and Application Development (cstc2021jscx-cylh0026), and the
   Open Funding Key Laboratory of Industry and Information Technology
   (2021KFKT01).
CR Alam A, 2020, INT J INTELL TRANSP, V18, P98, DOI 10.1007/s13177-019-00178-1
   Bar Hillel A, 2014, MACH VISION APPL, V25, P727, DOI 10.1007/s00138-011-0404-2
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Lee S, 2017, IEEE I CONF COMP VIS, P1965, DOI 10.1109/ICCV.2017.215
   Lee S, 2010, IEEE T CONSUM ELECTR, V56, P2636, DOI 10.1109/TCE.2010.5681151
   Li YD, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REAL-TIME COMPUTING AND ROBOTICS (IEEE RCAR), P411, DOI 10.1109/RCAR.2016.7784064
   Liu SG, 2019, IEEE T CONSUM ELECTR, V65, P303, DOI 10.1109/TCE.2019.2893644
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Neven D, 2018, IEEE INT VEH SYM, P286
   Pan XG, 2018, AAAI CONF ARTIF INTE, P7276
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun K, 2019, Arxiv, DOI arXiv:1904.04514
   Tang H, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108792
   Tang H, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P610, DOI 10.1145/3394171.3413884
   Tong GF, 2017, C IND ELECT APPL, P2066, DOI 10.1109/ICIEA.2017.8283178
   Wang K, 2022, VISUAL COMPUT, V38, P2329, DOI 10.1007/s00371-021-02115-4
   Wu LX, 2019, J PHYS CONF SER, V1176, DOI 10.1088/1742-6596/1176/3/032045
   [肖进胜 Xiao Jinsheng], 2014, [自动化学报, Acta Automatica Sinica], V40, P697
   You F, 2013, J OPT SOC KOREA, V17, P188, DOI 10.3807/JOSK.2013.17.2.188
   Yu H, 2022, INT CONF ACOUST SPEE, P1456, DOI 10.1109/ICASSP43922.2022.9747597
   Yu N., 2022, VISUAL COMPUT, P1
   Yu XY, 2023, VISUAL COMPUT, V39, P4165, DOI 10.1007/s00371-022-02582-3
   Zha ZC, 2023, IEEE T CIRC SYST VID, V33, P3947, DOI 10.1109/TCSVT.2023.3236636
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang JM, 2020, IEEE ACCESS, V8, P29742, DOI 10.1109/ACCESS.2020.2972338
   Zhang JM, 2017, ALGORITHMS, V10, DOI 10.3390/a10040127
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
NR 30
TC 3
Z9 3
U1 22
U2 61
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 MAR 1
PY 2023
DI 10.1007/s00371-023-02801-5
EA MAR 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9K2FI
UT WOS:000940686700001
DA 2024-07-18
ER

PT J
AU Cai, ZJ
   Zhang, SL
   Guo, P
   Zhang, JF
   Hu, LH
AF Cai, Zhenjiao
   Zhang, Sulan
   Guo, Ping
   Zhang, Jifu
   Hu, Lihua
TI AM-RP Stacking PILers: Random projection stacking pseudoinverse learning
   algorithm based on attention mechanism
SO VISUAL COMPUTER
LA English
DT Article
DE Stacking pseudoinverse learner; Random projection; Attention mechanism
ID CLASSIFICATION; REGRESSION; NETWORKS
AB The stacking pseudoinverse learning algorithm can effectively improve the classification accuracy of the model and reduce the training time. However, the effect of random projection blocks on the neural network is often ignored, which reduces the performance of stacked generalization. To improve the generalization performance of neural networks and obtain high-quality classification results, we propose a random projection stacking pseudoinverse learning algorithm based on attention mechanism named AM-RP stacking PILers. Firstly, the input weight matrices with specific distributions are randomly generated, and then different random projection blocks are obtained through a pseudoinverse learning algorithm. Secondly, the training results of different random projection blocks are taken as a new input dataset, and the idea of attention mechanism is introduced to assign different weights to different random projection blocks. Finally, we use the stacking pseudoinverse learning algorithm to train a single hidden layer neural network and obtain the classification results with high generalization performance. The experimental results on a total of 76788 images in three public datasets of Salinas, MNIST and COIL-20 show that our algorithm achieves better performance in accuracy, precision, recall, F1 score and training time.
C1 [Cai, Zhenjiao; Zhang, Sulan; Zhang, Jifu; Hu, Lihua] Taiyuan Univ Sci & Technol, Sch Comp Sci & Technol, Taiyuan 030024, Peoples R China.
   [Guo, Ping] Beijing Normal Univ, Sch Syst Sci, Beijing 100875, Peoples R China.
C3 Taiyuan University of Science & Technology; Beijing Normal University
RP Zhang, SL (corresponding author), Taiyuan Univ Sci & Technol, Sch Comp Sci & Technol, Taiyuan 030024, Peoples R China.
EM zhsulan@126.com
RI GUO, Ping/AAG-2160-2019
OI GUO, Ping/0000-0002-7122-1084
FU Natural Science Foundation of Shanxi Province, China [202103021224285]
FX AcknowledgementsThis work was supported by the Natural Science
   Foundation of Shanxi Province, China (Grant No. 202103021224285).
CR Ahmad M, 2019, OPTIK, V180, P370, DOI 10.1016/j.ijleo.2018.10.142
   Bateni P., 2019, P IEEECVF C COMPUTER, P14493
   Bruzzone L, 2006, IEEE T GEOSCI REMOTE, V44, P3363, DOI 10.1109/TGRS.2006.877950
   Chen JM, 2011, COMPUT NETW, V55, P2481, DOI 10.1016/j.comnet.2011.04.010
   Chen Ting, 2020, ADV NEURAL INFORM PR, V33, P22243
   Deng XD, 2022, COMPLEX INTELL SYST, V8, P1445, DOI 10.1007/s40747-021-00588-3
   Deng XD, 2021, NEUROCOMPUTING, V457, P74, DOI 10.1016/j.neucom.2021.06.041
   Eftekhari A, 2011, SIGNAL PROCESS, V91, P1589, DOI 10.1016/j.sigpro.2011.01.002
   Fang U, 2023, WORLD WIDE WEB, V26, P1667, DOI 10.1007/s11280-022-01110-6
   Feng SB, 2019, IEEE SYS MAN CYBERN, P1893, DOI 10.1109/SMC.2019.8914405
   Frenkel C, 2020, IEEE INT SYMP CIRC S, DOI [10.1109/ISCAS45731.2020.9180440, 10.1109/iscas45731.2020.9180440]
   Guo P, 2004, NEUROCOMPUTING, V56, P101, DOI 10.1016/S0925-2312(03)00385-0
   Guo P., 2020, ARXIV
   Guo P, 2019, INNS BIG DAT DEEP LE, DOI [10.1007/978-3-030-16841-4_17, DOI 10.1007/978-3-030-16841-4_17]
   Guo P., 2001, NEURAL INFORM PROCES, P3763
   Guo Ping, 2001, Advances in Neural Networks and Applications, P321
   Huang G, 2014, IEEE T CYBERNETICS, V44, P2405, DOI 10.1109/TCYB.2014.2307349
   Huang GB, 2012, IEEE T SYST MAN CY B, V42, P513, DOI 10.1109/TSMCB.2011.2168604
   Izquierdo-Cordova R, 2021, IEEE COMPUT SOC CONF, P1241, DOI 10.1109/CVPRW53098.2021.00136
   Jing PG, 2021, INFORM SCIENCES, V544, P155, DOI 10.1016/j.ins.2020.06.068
   LeCun Y., 2010, MNIST HANDWRITTEN DI
   Li B, 2021, PROC CVPR IEEE, P14313, DOI 10.1109/CVPR46437.2021.01409
   Li CY, 2021, IEEE T CYBERNETICS, V51, P88, DOI 10.1109/TCYB.2020.2969255
   Li M, 2020, IEEE T MED IMAGING, V39, P2289, DOI 10.1109/TMI.2020.2968472
   Li S., 2018, 2018 IEEE C EVOLUTIO, P1
   Li XL, 2021, IEEE T CYBERNETICS, V51, P913, DOI 10.1109/TCYB.2019.2914351
   Li Z., 2020, J PET SCI ENG, V195
   Lin Junhong, 2020, J MACH LEARN RES, V21, P1
   Liu YT, 2022, VISUAL COMPUT, V38, P897, DOI 10.1007/s00371-021-02058-w
   Ma ZH, 2022, VISUAL COMPUT, V38, P3163, DOI 10.1007/s00371-022-02535-w
   Mahmoud MAB, 2021, NEURAL PROCESS LETT, V53, P4225, DOI 10.1007/s11063-021-10595-7
   Mahmoud MAB, 2021, NEW ASTRON, V85, DOI 10.1016/j.newast.2020.101561
   Mahmoud MAB, 2021, SOFT COMPUT, V25, P4003, DOI 10.1007/s00500-020-05429-y
   Mahmoud MAB, 2020, MULTIMED TOOLS APPL, V79, P26245, DOI 10.1007/s11042-020-09239-0
   Mahmoud MAB, 2019, IEEE ACCESS, V7, P74602, DOI 10.1109/ACCESS.2019.2919125
   Nozaripour A, 2023, VISUAL COMPUT, V39, P1731, DOI 10.1007/s00371-022-02441-1
   Ojha V, 2022, NEURAL NETWORKS, V149, P66, DOI 10.1016/j.neunet.2022.02.003
   Omidiran D, 2010, J MACH LEARN RES, V11, P2361
   Pei YT, 2021, IEEE T PATTERN ANAL, V43, P1239, DOI 10.1109/TPAMI.2019.2950923
   Peng JBA, 2023, VISUAL COMPUT, V39, P971, DOI 10.1007/s00371-021-02378-x
   Peng Y, 2020, APPL SOFT COMPUT, V97, DOI 10.1016/j.asoc.2020.106756
   Ping G., 1995, P INT C NEURAL INFOR
   Ping Guo, 1995, Proceedings of International Conference on Neural Information Processing (ICONIP '95), P1041
   Qiu R, 2023, VISUAL COMPUT, V39, P2933, DOI 10.1007/s00371-022-02501-6
   Requeima J, 2019, ADV NEUR IN, V32
   Retrieva C.R.C., 2011, COMPUTER, V5
   Scardapane S, 2016, INFORM SCIENCES, V364, P156, DOI 10.1016/j.ins.2015.07.060
   Shao WX, 2015, LECT NOTES ARTIF INT, V9284, P318, DOI 10.1007/978-3-319-23528-8_20
   Shi QS, 2021, PATTERN RECOGN, V117, DOI 10.1016/j.patcog.2021.107978
   Shrestha A, 2021, DES AUT CON, P367, DOI 10.1109/DAC18074.2021.9586323
   Sima H., 2021, IEEE T CYBERNETICS, V99, P1
   Snell J, 2017, ADV NEUR IN, V30
   Sun XX, 2020, IEEE SYS MAN CYBERN, P2136, DOI [10.1109/SMC42975.2020.9283232, 10.1109/smc42975.2020.9283232]
   Surasinghe S, 2021, MATHEMATICS-BASEL, V9, DOI 10.3390/math9212803
   Tan XX, 2022, VISUAL COMPUT, DOI 10.1007/s00371-022-02686-w
   Tang H, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108792
   Tang H, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P610, DOI 10.1145/3394171.3413884
   Tian CW, 2020, NEURAL NETWORKS, V131, P251, DOI 10.1016/j.neunet.2020.07.025
   Tian CW, 2020, NEURAL NETWORKS, V124, P117, DOI 10.1016/j.neunet.2019.12.024
   Nguyen TT, 2019, INFORM SCIENCES, V490, P36, DOI 10.1016/j.ins.2019.03.067
   Varga LG, 2023, VISUAL COMPUT, V39, P1557, DOI 10.1007/s00371-022-02428-y
   Wang GH, 2023, VISUAL COMPUT, V39, P2969, DOI 10.1007/s00371-022-02503-4
   Wang J, 2022, COMPLEX INTELL SYST, V8, P2039, DOI 10.1007/s40747-021-00516-5
   Wang K, 2021, COGN COMPUT, V13, P724, DOI 10.1007/s12559-021-09853-6
   Wang K, 2021, IEEE T INTELL TRANSP, V22, P3303, DOI 10.1109/TITS.2020.2980555
   Wang K, 2017, MON NOT R ASTRON SOC, V465, P4311, DOI 10.1093/mnras/stw2894
   Wang L, 2022, VISUAL COMPUT, V38, P2009, DOI 10.1007/s00371-021-02262-8
   Wang WR, 2015, PR MACH LEARN RES, V37, P1083
   Wang X, 2021, KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1726, DOI 10.1145/3447548.3467415
   Wang XJ, 2023, VISUAL COMPUT, V39, P4801, DOI 10.1007/s00371-022-02628-6
   Wang Y, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3383-y
   Werbos P. J., 1981, SYSTEM MODELING OPTI, P762, DOI [10.1007/BFb0006203, DOI 10.1007/BFB0006203]
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Xu BX, 2019, 2019 15TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY (CIS 2019), P73, DOI 10.1109/CIS.2019.00024
   Yang S, 2023, VISUAL COMPUT, V39, P6015, DOI 10.1007/s00371-022-02709-6
   Young J, 2012, SIAM J SCI COMPUT, V34, pA2344, DOI 10.1137/11084666X
   Zhan K, 2019, IEEE T IMAGE PROCESS, V28, P1261, DOI 10.1109/TIP.2018.2877335
   Zhan K, 2018, IEEE T CYBERNETICS, V48, P2887, DOI 10.1109/TCYB.2017.2751646
   Zhang B, 2020, IEEE T PATTERN ANAL, V42, P154, DOI 10.1109/TPAMI.2018.2876404
   Zhang B, 2020, INT CONF ACOUST SPEE, P2083, DOI [10.1109/icassp40776.2020.9053332, 10.1109/ICASSP40776.2020.9053332]
   Zhang L, 2017, IEEE COMPUT INTELL M, V12, P61, DOI 10.1109/MCI.2017.2742867
   Zhang L, 2016, INFORM SCIENCES, V367, P1094, DOI 10.1016/j.ins.2015.09.025
   Zhang SL, 2022, MULTIMED TOOLS APPL, V81, P39963, DOI 10.1007/s11042-022-12618-4
   Zhao ZY, 2021, NEURAL NETWORKS, V139, P326, DOI 10.1016/j.neunet.2021.04.002
NR 84
TC 0
Z9 0
U1 11
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 273
EP 285
DI 10.1007/s00371-023-02780-7
EA FEB 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000923783300001
DA 2024-07-18
ER

PT J
AU Liu, P
   Li, G
   Zhao, W
   Tang, XL
AF Liu, Peng
   Li, Gong
   Zhao, Wei
   Tang, Xianglong
TI A coupling method of learning structured support correlation filters for
   visual tracking
SO VISUAL COMPUTER
LA English
DT Article
DE Visual tracking; Correlation filters; Structured SVM; Background
   awareness
ID OBJECT TRACKING
AB The correlation filtering method is one of the mainstream methods in the visual target tracking task. One of the reasons is that the introduction of cyclic samples facilitates the calculation of optimizing filters. But usual correlation filtering frameworks which are attributable to a ridge regression model based on least squares error with various regularizations put emphasis on modeling a linear system for samples themselves, so maybe likely result in over-fitting. With the appearance of either target or background varying, the credibility of the response obtained after filtering would be decrease. Some researchers thus have tried to incorporate other models such as SVMs to obtain better robustness. In this study, we propose a natural coupling method called StrucSCF of integrating structured SVM by background awareness into the correlation filtering framework, which put more emphasis on the discrepancy between the target and background samples to enhance the discrimination and robustness of tracking. Meanwhile, for the sake of online updating the filters based on structured SVM with real-time performance, we take advantage of the fast Fourier transform on the circulant samples to speed up solving the structured SVM-based filters. In addition, we extend the StrucSCF method with Laplacian temporal regularization to demonstrate that it has as good quality of extension as the conventional correlation filtering framework. The proposed StrucSCF has achieved competitive performance compared with the baseline and other advanced methods in mainstream benchmarks.
C1 [Liu, Peng; Li, Gong; Zhao, Wei; Tang, Xianglong] Harbin Inst Technol, Dept Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
C3 Harbin Institute of Technology
RP Li, G (corresponding author), Harbin Inst Technol, Dept Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
EM pengliu@hit.edu.cn; ligong101@126.com; zhaowei@hit.edu.cn;
   tangxl@hit.edu.cn
RI Liu, Pengju/U-3399-2019
OI Liu, Pengju/0000-0001-8413-9621
FU National Natural Science Foundation of China [51935005]; Natural Science
   Foundation of Heilongjiang Province, China [LH2021F023]; Basic
   Scientific Research Program [JCKY20200603C010]
FX AcknowledgementsThis work was partially supported by the National
   Natural Science Foundation of China under Grant 51935005; the Natural
   Science Foundation of Heilongjiang Province, China, under Grant
   LH2021F023 and the Basic Scientific Research Program (Grant No.
   JCKY20200603C010).
CR Abbass MY, 2021, VISUAL COMPUT, V37, P993, DOI 10.1007/s00371-020-01848-y
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Boyd S., 2011, FOUND TRENDS MACH LE, V3, P1, DOI DOI 10.1561/2200000016
   Dai KN, 2019, PROC CVPR IEEE, P4665, DOI 10.1109/CVPR.2019.00480
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2016, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2016.159
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Fan CX, 2022, VISUAL COMPUT, V38, P4291, DOI 10.1007/s00371-021-02296-y
   Fan JW, 2023, VISUAL COMPUT, V39, P319, DOI 10.1007/s00371-021-02331-y
   Fazl-Ersi E, 2019, VISUAL COMPUT, V35, P1447, DOI 10.1007/s00371-018-1510-1
   Feng W, 2019, IEEE T IMAGE PROCESS, V28, P3232, DOI 10.1109/TIP.2019.2895411
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Huang Z., 2017, P IEEE C COMP VIS PA, P4021, DOI DOI 10.1109/CVPR.2017.510
   Huang ZY, 2019, IEEE I CONF COMP VIS, P2891, DOI 10.1109/ICCV.2019.00298
   Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1
   Kristan M, 2017, IEEE INT CONF COMP V, P1949, DOI 10.1109/ICCVW.2017.230
   Lee CP, 2013, NEURAL COMPUT, V25, P1302, DOI 10.1162/NECO_a_00434
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li X, 2019, PROC CVPR IEEE, P1369, DOI 10.1109/CVPR.2019.00146
   Li Y, 2019, AAAI CONF ARTIF INTE, P8666
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905
   Liao JW, 2021, IEEE T MULTIMEDIA, V23, P3346, DOI 10.1109/TMM.2020.3023794
   Lin FL, 2021, IEEE T CIRC SYST VID, V31, P2160, DOI 10.1109/TCSVT.2020.3023440
   Lu H., 2019, IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Computer Vision Foundation / IEEE, P5783
   Lukezic A, 2018, INT J COMPUT VISION, V126, P671, DOI 10.1007/s11263-017-1061-3
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Miao Q, 2023, VISUAL COMPUT, V39, P1237, DOI 10.1007/s00371-022-02401-9
   Mueller M, 2017, PROC CVPR IEEE, P1387, DOI 10.1109/CVPR.2017.152
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Ning JF, 2016, PROC CVPR IEEE, P4266, DOI 10.1109/CVPR.2016.462
   PARIKH N., 2014, FDN TRENDS OPTIM, V1, P3, DOI DOI 10.1561/2400000003
   Platt JC, 1999, ADVANCES IN KERNEL METHODS, P185
   Qu ZY, 2023, VISUAL COMPUT, V39, P625, DOI 10.1007/s00371-021-02362-5
   Ramanan, 2013, ARXIV
   Rodriguez A, 2013, IEEE T IMAGE PROCESS, V22, P631, DOI 10.1109/TIP.2012.2220151
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song YB, 2017, IEEE I CONF COMP VIS, P2574, DOI 10.1109/ICCV.2017.279
   Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang Y, 2019, VISUAL COMPUT, V35, P1641, DOI 10.1007/s00371-018-1563-1
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xu TY, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2919201
   Yang SD, 2022, VISUAL COMPUT, V38, P2107, DOI 10.1007/s00371-021-02271-7
   Ye J., 2007, Artificial intelligence and statistics, P644
   Yiming Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11920, DOI 10.1109/CVPR42600.2020.01194
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhang K, 2022, IEEE T CIRC SYST VID, V32, P3708, DOI 10.1109/TCSVT.2021.3108176
   Zhang WC, 2021, VISUAL COMPUT, V37, P881, DOI 10.1007/s00371-020-01839-z
   Zuo WM, 2019, IEEE T PATTERN ANAL, V41, P1158, DOI 10.1109/TPAMI.2018.2829180
NR 60
TC 0
Z9 0
U1 8
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 181
EP 199
DI 10.1007/s00371-023-02774-5
EA JAN 2023
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000922469400001
DA 2024-07-18
ER

PT J
AU Yin, WX
   He, KJ
   Xu, D
   Yue, YY
   Luo, YY
AF Yin, Wenxia
   He, Kangjian
   Xu, Dan
   Yue, Yingying
   Luo, Yueying
TI Adaptive low light visual enhancement and high-significant target
   detection for infrared and visible image fusion
SO VISUAL COMPUTER
LA English
DT Article
DE Image fusion; Adaptive low light visual enhancement; High-significant
   target detection; Hybrid decomposition
ID GENERATIVE ADVERSARIAL NETWORK; MULTISCALE TRANSFORM; QUALITY
   ASSESSMENT; DECOMPOSITION; ALGORITHM
AB Infrared and visible image fusion aim to obtain a fused image with salient targets and preserve abundant texture detail information as much as possible, which can potentially improve the reliability of some target detection and tracking tasks. However, some visible images taken from low-illumination conditions are subjected to losing many details and cannot obtain a good fusion result. To address this issue, we proposed a novel adaptive visual enhancement and high-significant targets detection-based fusion scheme in this paper. First, a bright-pass bilateral filter and adaptive-gamma correction-based algorithm are proposed to enhance the visible image adaptively. Second, an iterative guided and infrared patch-tensor-based algorithm are proposed to extract the infrared target. Third, an efficient hybrid l(1)- l(0) model decomposes the infrared and visible image into base and detail layers and then fuses them by weight map strategy. The final fused image is obtained by merging the fused base layers, detail layers, and infrared targets. Qualitative and quantitative experimental results demonstrate that the proposed method is superior to 9 state-of-the-art image fusion methods as more valuable texture details and significant infrared targets are preserved.
C1 [Yin, Wenxia; He, Kangjian; Xu, Dan; Yue, Yingying; Luo, Yueying] Yunnan Univ, Sch Informat Sci & Engn, Kunming 650091, Peoples R China.
C3 Yunnan University
RP He, KJ; Xu, D (corresponding author), Yunnan Univ, Sch Informat Sci & Engn, Kunming 650091, Peoples R China.
EM hekj@ynu.edu.cn; danxu@ynu.edu.cn
RI Xu, Dan/KPA-7396-2024; He, Kangjian/CAG-0300-2022
OI Xu, Dan/0000-0003-4602-3550; He, Kangjian/0000-0001-6207-9728
FU provincial major science and technology special plan projects
   [202202AD080003]; National Natural Science Foundation of China
   [62202416, 62162068, 62172354, 61761049]; Yunnan Province Ten Thousand
   Talents Program and Yunling Scholars Special Project
   [YNWR-YLXZ-2018-022]; Yunnan Provincial Science and Technology
   Department-Yunnan University "Double First Class" Construction Joint
   Fund Project [2019FY003012]; Science Research Fund Project of Yunnan
   Provincial Department of Education [2021Y027]; Graduate Research and
   Innovation Foundation of Yunnan University [2021Y176, 2021Y272]
FX This work was supported in part by the provincial major science and
   technology special plan projects under Grant 202202AD080003, in part by
   the National Natural Science Foundation of China under Grant 62202416,
   Grant 62162068, Grant 62172354, Grant 61761049, in part by the Yunnan
   Province Ten Thousand Talents Program and Yunling Scholars Special
   Project under Grant YNWR-YLXZ-2018-022, in part by the Yunnan Provincial
   Science and Technology Department-Yunnan University "Double First Class"
   Construction Joint Fund Project under Grant No. 2019FY003012, in part by
   the Science Research Fund Project of Yunnan Provincial Department of
   Education under grant 2021Y027, in part by the Graduate Research and
   Innovation Foundation of Yunnan University (No.2021Y176 and No.
   2021Y272)
CR Aghamaleki JA, 2023, VISUAL COMPUT, V39, P1181, DOI 10.1007/s00371-021-02396-9
   Bashir R, 2019, MULTIMED TOOLS APPL, V78, P1235, DOI 10.1007/s11042-018-6229-5
   Bavirisetti DP, 2019, CIRC SYST SIGNAL PR, V38, P5576, DOI 10.1007/s00034-019-01131-z
   Bavirisetti DP, 2017, 2017 20TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P701
   Bavirisetti DP, 2016, IEEE SENS J, V16, P203, DOI 10.1109/JSEN.2015.2478655
   Chen H, 2007, INFORM FUSION, V8, P193, DOI 10.1016/j.inffus.2005.10.001
   Chen J, 2020, INFORM SCIENCES, V508, P64, DOI 10.1016/j.ins.2019.08.066
   Chen Y, 2009, IMAGE VISION COMPUT, V27, P1421, DOI 10.1016/j.imavis.2007.12.002
   Chen YY, 2020, SIGNAL PROCESS, V174, DOI 10.1016/j.sigpro.2020.107645
   Chen YH, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3069116
   Dehaene S, 2003, TRENDS COGN SCI, V7, P145, DOI 10.1016/S1364-6613(03)00055-X
   ECKSTEIN J, 1992, MATH PROGRAM, V55, P293, DOI 10.1007/BF01581204
   Fu Y, 2021, INFORM FUSION, V72, P110, DOI 10.1016/j.inffus.2021.02.019
   Ghosh S, 2019, IEEE IMAGE PROC, P205, DOI [10.1109/ICIP.2019.8802986, 10.1109/icip.2019.8802986]
   He K, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3058365
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Jagtap NS, 2022, VISUAL COMPUT, V38, P4353, DOI 10.1007/s00371-021-02300-5
   Jin X, 2017, INFRARED PHYS TECHN, V85, P478, DOI 10.1016/j.infrared.2017.07.010
   Kong X., 2022, "IEEE Trans.Geosci. Remote Sens., V60, DOI DOI 10.1109/TGRS.2021.3068465
   Li GF, 2021, INFORM FUSION, V71, P109, DOI 10.1016/j.inffus.2021.02.008
   Li H, 2021, INFORM FUSION, V73, P72, DOI 10.1016/j.inffus.2021.02.023
   Li QL, 2021, IEEE SENS J, V21, P7458, DOI 10.1109/JSEN.2019.2921803
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Liang ZT, 2018, PROC CVPR IEEE, P4758, DOI 10.1109/CVPR.2018.00500
   Liu J., 2022, VISUAL COMPUT, V1, P1
   Liu Y, 2017, INFORM FUSION, V36, P191, DOI 10.1016/j.inffus.2016.12.001
   Liu Y, 2015, INFORM FUSION, V24, P147, DOI 10.1016/j.inffus.2014.09.004
   Lu R., 2022, VISUAL COMPUT, V1, P1
   Luo Y, 2022, OPTIK, V258, DOI 10.1016/j.ijleo.2022.168914
   Luo YY, 2022, J ELECTRON IMAGING, V31, DOI 10.1117/1.JEI.31.1.013032
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JY, 2019, INFORM FUSION, V45, P153, DOI 10.1016/j.inffus.2018.02.004
   Ma JL, 2017, INFRARED PHYS TECHN, V82, P8, DOI 10.1016/j.infrared.2017.02.005
   Maurya L, 2017, APPL SOFT COMPUT, V52, P575, DOI 10.1016/j.asoc.2016.10.012
   Mitianoudis N, 2007, INFORM FUSION, V8, P131, DOI 10.1016/j.inffus.2005.09.001
   Panigrahy C, 2022, NEUROCOMPUTING, V514, P21, DOI 10.1016/j.neucom.2022.09.157
   Piella G., 2003, Information Fusion, V4, P259, DOI 10.1016/S1566-2535(03)00046-0
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shreyamsha Kumar BK, 2015, SIGNAL IMAGE VIDEO P, V9, P1193, DOI 10.1007/s11760-013-0556-9
   Soroush R, 2023, VISUAL COMPUT, V39, P2725, DOI 10.1007/s00371-022-02488-0
   Tan ZY, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3050551
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wang CM, 2021, VISUAL COMPUT, V37, P1233, DOI 10.1007/s00371-021-02079-5
   Wang XJ, 2023, VISUAL COMPUT, V39, P4801, DOI 10.1007/s00371-022-02628-6
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu J, 2020, IEEE T IMAGE PROCESS, V29, P5022, DOI 10.1109/TIP.2020.2974060
   Xydeas CS, 2000, ELECTRON LETT, V36, P308, DOI 10.1049/el:20000267
   Yin W., 2022, NEURAL COMPUT APPL, V1, P1
   Yin WX, 2022, INFRARED PHYS TECHN, V121, DOI 10.1016/j.infrared.2022.104041
   Ying ZQ, 2017, LECT NOTES COMPUT SC, V10425, P36, DOI 10.1007/978-3-319-64698-5_4
   Zhan L., 2017, J. Comput., V28, P57
   Zhang XC, 2020, IEEE COMPUT SOC CONF, P468, DOI 10.1109/CVPRW50498.2020.00060
   Zhao ZX, 2020, SIGNAL PROCESS, V177, DOI 10.1016/j.sigpro.2020.107734
   Zheng YB, 2020, IEEE T GEOSCI REMOTE, V58, P734, DOI 10.1109/TGRS.2019.2940534
   Zhou ZQ, 2016, INFORM FUSION, V30, P15, DOI 10.1016/j.inffus.2015.11.003
NR 56
TC 12
Z9 12
U1 13
U2 47
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6723
EP 6742
DI 10.1007/s00371-022-02759-w
EA JAN 2023
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000917955300001
DA 2024-07-18
ER

PT J
AU Cheng, T
   Sun, L
   Zhang, JN
   Hou, DC
   Shi, Q
   Chen, J
AF Cheng, Teng
   Sun, Lei
   Zhang, Junning
   Hou, Dengchao
   Shi, Qin
   Chen, Jiong
TI Based on real and virtual datasets adaptive joint training in
   multi-modal networks with applications in monocular 3D target detection
SO VISUAL COMPUTER
LA English
DT Article
DE Autonomous driving; Multi-modal fusion; 3D target detection; Joint
   training; Adaptive optimizer
AB Semantic segmentation is important for the accuracy of target detection. Semantic labels are difficult to obtain for real driving; however, they are easy to obtain in virtual datasets. So this paper presents an adaptive joint training strategy based on real and virtual datasets: (1) building multi(-)modal fusion networks using image, depth and semantic information. (2) A joint training strategy of virtual and real datasets and data sharing is used for semantic information, and an adaptive optimizer is provided. The monocular detection network obtained by training with this strategy has a large improvement in its effectiveness relative to the conventional network.
C1 [Cheng, Teng; Sun, Lei; Hou, Dengchao; Shi, Qin] Hefei Univ Technol, Engn Res Ctr Intelligent Transportat & Cooperat Ve, Sch Automot & Transportat, Hefei 230041, Peoples R China.
   [Zhang, Junning] Natl Univ Def Technol, Sch Elect Countermeasures, Hefei 230041, Peoples R China.
   [Chen, Jiong] Nio Automot Technol Anhui Co LTD, Hefei 230041, Peoples R China.
C3 Hefei University of Technology; National University of Defense
   Technology - China
RP Cheng, T (corresponding author), Hefei Univ Technol, Engn Res Ctr Intelligent Transportat & Cooperat Ve, Sch Automot & Transportat, Hefei 230041, Peoples R China.
EM cht616@hfut.edu.cn; 954062845@qq.com; zjn20101796@sina.cn;
   daya88888888@qq.com; shiqin@hfut.edu.cn; jiong.chen@nio.com
OI Cheng, Teng/0000-0001-7318-1365
FU Fundamental Research Funds for the Central Universities
   [PA2021KCPY0041]; Innovation Project of New Energy Vehicle and
   Intelligent Connected Vehicle of Anhui Province; University Synergy
   Innovation Program of Anhui Province [GXXT-2020-076]
FX This work was supported by "the Fundamental Research Funds for the
   Central Universities, PA2021KCPY0041","Innovation Project of New Energy
   Vehicle and Intelligent Connected Vehicle of Anhui Province" and "The
   University Synergy Innovation Program of Anhui Province, GXXT-2020-076".
   The authors thank the anonymous reviewers for their instructive
   comments.
CR Ahmadian K, 2013, VISUAL COMPUT, V29, P123, DOI 10.1007/s00371-012-0741-9
   Berlin SJ, 2022, VISUAL COMPUT, V38, P223, DOI 10.1007/s00371-020-02012-2
   Brazil Garrick, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P135, DOI 10.1007/978-3-030-58592-1_9
   Chen XZ, 2015, ADV NEUR IN, V28
   Chen Y., 2020, CVPR, P10337
   Chunmei Liu, 2021, ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education, P354, DOI 10.1145/3482632.3482707
   Ding Y, 2022, VISUAL COMPUT, V38, P1871, DOI 10.1007/s00371-021-02246-8
   Garcia-Garcia A., 2017, ARXIV
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Hackett J. K., 1990, Proceedings 1990 IEEE International Conference on Robotics and Automation (Cat. No.90CH2876-1), P1324, DOI 10.1109/ROBOT.1990.126184
   Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775
   He D, 2023, VISUAL COMPUT, V39, P1423, DOI 10.1007/s00371-022-02420-6
   Kingma D. P., 2014, arXiv
   Kumar A, 2021, PROC CVPR IEEE, P8969, DOI 10.1109/CVPR46437.2021.00886
   Li B., 2016, arXiv
   Li Peiliang, 2019, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
   Li PX, 2021, IEEE ROBOT AUTOM LET, V6, P5565, DOI 10.1109/LRA.2021.3061343
   Li Peixuan, 2020, EUROPEAN C COMPUTER, P644
   Li YZ, 2021, VISUAL COMPUT, V37, P2567, DOI 10.1007/s00371-021-02206-2
   Liu ZC, 2020, IEEE COMPUT SOC CONF, P4289, DOI 10.1109/CVPRW50498.2020.00506
   Liu ZD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15621, DOI 10.1109/ICCV48922.2021.01535
   Luo H, 1997, AIAA J, V35, P1522, DOI 10.2514/2.7480
   Manhardt F, 2019, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2019.00217
   Ng MY, 2020, IEEE COMPUT SOC CONF, P4306, DOI 10.1109/CVPRW50498.2020.00508
   Park J, 2021, INT J CONTROL AUTOM, V19, P1103, DOI 10.1007/s12555-019-1014-4
   Pengxin Ding, 2020, Man-Machine-Environment System Engineering. Proceedings of the 20th International Conference on MMESE. Lecture Notes in Electrical Engineering (LNEE 645), P469, DOI 10.1007/978-981-15-6978-4_55
   Qin ZY, 2022, IEEE T PATTERN ANAL, V44, P5170, DOI 10.1109/TPAMI.2021.3074363
   Qin ZY, 2019, AAAI CONF ARTIF INTE, P8851
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ros G., 2016, Proceedings of the IEEE conference on computer vision and pattern recognition, P3234, DOI DOI 10.1109/CVPR.2016.352
   Shi X., 2021, P IEEECVF INT C COMP, P15172
   Vaswani A, 2017, ADV NEUR IN, V30
   Vu TH, 2019, IEEE I CONF COMP VIS, P7363, DOI 10.1109/ICCV.2019.00746
   Yu F, 2018, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2018.00255
   Zhang GY, 2017, IEEE SIGNAL PROC LET, V24, P1666, DOI 10.1109/LSP.2017.2731952
   Zhang GY, 2018, J VIS COMMUN IMAGE R, V52, P13, DOI 10.1016/j.jvcir.2018.01.013
   Zhang G, 2016, INT CONF ACOUST SPEE, P2004, DOI 10.1109/ICASSP.2016.7472028
   Zhang JN, 2023, IEEE T NEUR NET LEAR, V34, P2710, DOI 10.1109/TNNLS.2021.3107362
   Zhang JN, 2020, NEUROCOMPUTING, V403, P182, DOI 10.1016/j.neucom.2020.03.076
   Zhao JF, 2018, VISUAL COMPUT, V34, P1461, DOI 10.1007/s00371-018-1477-y
   Zhou XN, 2019, VISUAL COMPUT, V35, P385, DOI 10.1007/s00371-018-1471-4
   Zhou Z., 2021, IEEE ROBOT AUTOM LET
   Zhuang FZ, 2021, P IEEE, V109, P43, DOI 10.1109/JPROC.2020.3004555
NR 43
TC 2
Z9 2
U1 3
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6367
EP 6377
DI 10.1007/s00371-022-02734-5
EA DEC 2022
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000898696000002
DA 2024-07-18
ER

PT J
AU Xu, XS
   Mo, JQ
AF Xu, Xiaoshun
   Mo, Jinqiu
TI Visual explanation and robustness assessment optimization of saliency
   maps for image classification
SO VISUAL COMPUTER
LA English
DT Article
DE Visual explanation; Data analysis; Robustness assessment; Black-box
   models
AB For image classification using Deep Learning, applying visual explanations allows end-users to understand better the basis of model decisions in the inference process. Our method optimizes the black-box visual explanation called Randomized Input Sampling for Explanation (RISE) by proposing the concept of Decisive Saliency Map (DSM) and the corresponding quantitative metric. The introduction of DSM makes the discriminative salient regions more prominent and easier to understand with ignorable extra costs. Moreover, DSM efficiently correlates robustness assessment with the visual explanation via saliency value distribution. It provides a reference indicator for the reliability and robustness assessment of the model predictions, complementing the common-used Softmax confidence score. Experiments demonstrate that the utilization of DSM and the related quantitative metric can improve the visualization of mainstream CNN models, and differentiate the concrete importance of confusingly similar salient regions. By quantitatively assessing the robustness of the inference process, DSM identifies the potential misclassification risk of high-performance CNN models accurately.
C1 [Xu, Xiaoshun; Mo, Jinqiu] Shanghai Jiao Tong Univ, Sch Mech Engn, Shanghai 200240, Peoples R China.
   [Xu, Xiaoshun] SAIC Gen Motors Corp Ltd, Shanghai 201206, Peoples R China.
C3 Shanghai Jiao Tong University
RP Xu, XS (corresponding author), Shanghai Jiao Tong Univ, Sch Mech Engn, Shanghai 200240, Peoples R China.; Xu, XS (corresponding author), SAIC Gen Motors Corp Ltd, Shanghai 201206, Peoples R China.
EM xxs_sgm@aliyun.com
OI Xu, Xiaoshun/0000-0002-7553-5558
CR Adadi A, 2018, IEEE ACCESS, V6, P52138, DOI 10.1109/ACCESS.2018.2870052
   Artificial Intelligence (AI), 2021, 2402912021 ISO IEC
   Bargal SA, 2021, IEEE T PATTERN ANAL, V43, P4196, DOI [10.1109/TPAMI.2021.3054303, 10.1109/TPAMI.2020.3054303]
   Arrieta AB, 2020, INFORM FUSION, V58, P82, DOI 10.1016/j.inffus.2019.12.012
   Brundage Miles, 2020, ARXIV
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Chen W., 2020, GUIDE DATA VISUALIZA
   [成科扬 Cheng Keyang], 2020, [计算机研究与发展, Journal of Computer Research and Development], V57, P1208
   Emami M, 2013, IMAGE VISION COMPUT, V31, P796, DOI 10.1016/j.imavis.2013.08.004
   Finale Doshi-Velez, 2017, ARXIV
   Fong R, 2019, IEEE I CONF COMP VIS, P2950, DOI 10.1109/ICCV.2019.00304
   Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371
   Guidotti R, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3236009
   Gunning D, 2019, AI MAG, V40, P44, DOI 10.1609/aimag.v40i2.2850
   Guo CA, 2017, PR MACH LEARN RES, V70
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Ji S.L, 2019, J. Comput. Res. Devel., V56, P2071
   JOHNSON NL, 1949, BIOMETRIKA, V36, P149, DOI 10.1093/biomet/36.1-2.149
   Keller P.R., 1993, Visual Cues: Practical Data Visualization
   Khorram Saeed, 2021, CHIL '21: Proceedings of the Conference on Health, Inference, and Learning, P174, DOI 10.1145/3450439.3451865
   Koffka K., 1935, PRINCIPLES GESTALT P, DOI [DOI 10.4324/9781315009292, 10.4324/9781315009292]
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Li XH, 2021, KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P3200, DOI 10.1145/3447548.3467148
   Martin D., 2022, ANN HAWAII INT C SYS, DOI [10.24251/hicss.2022.146, DOI 10.24251/HICSS.2022.146]
   Morales D, 2021, NEURAL COMPUT APPL, V33, P16937, DOI 10.1007/s00521-021-06282-2
   Naseer M.M., 2021, Advances in Neural Information Processing Systems, P23296
   Paleyes A, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3533378
   Patel Nikunjkumar, 2018, Smart Multimedia. First International Conference, ICSM 2018. Revised Selected Papers: Lecture Notes in Computer Science (LNCS 11010), P448, DOI 10.1007/978-3-030-04375-9_39
   Pei HN, 2023, VISUAL COMPUT, V39, P2981, DOI 10.1007/s00371-022-02506-1
   Petsiuk V., 2018, RISE RANDOMIZED INPU
   Ribeiro MT, 2018, AAAI CONF ARTIF INTE, P1527
   Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778
   Riche N, 2013, IEEE I CONF COMP VIS, P1153, DOI 10.1109/ICCV.2013.147
   Ruoyi Du, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P153, DOI 10.1007/978-3-030-58565-5_10
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Simonyan K, 2013, ARXIV
   Springenberg Jost Tobias, 2015, Striving for simplicity: The all convolutional net, DOI DOI 10.48550/ARXIV.1412.6806
   Tan MX, 2019, PR MACH LEARN RES, V97
   Vermeire T, 2021, COMM COM INF SC, V1524, P521, DOI 10.1007/978-3-030-93736-2_39
   Wagner J, 2019, PROC CVPR IEEE, P9089, DOI 10.1109/CVPR.2019.00931
   Wang HJ, 2020, Cambria Sinophone Wo, P111, DOI 10.1109/CVPRW50498.2020.00020
   Yang TJ, 2021, VISUAL COMPUT, V37, P1559, DOI 10.1007/s00371-020-01901-w
   Yosinski J., 2015, ARXIV150606579, V2015, P12
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou B., 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.319
NR 48
TC 3
Z9 3
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6097
EP 6113
DI 10.1007/s00371-022-02715-8
EA NOV 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000881913500002
DA 2024-07-18
ER

PT J
AU Amirkhani, A
   Karimi, MP
   Banitalebi-Dehkordi, A
AF Amirkhani, Abdollah
   Karimi, Mohammad Parsa
   Banitalebi-Dehkordi, Amin
TI A survey on adversarial attacks and defenses for object detection and
   their applications in autonomous vehicles
SO VISUAL COMPUTER
LA English
DT Article
DE Object detection; Adversarial attack; Robust detector; Adversarial
   defense
ID DEEP NEURAL-NETWORKS; COMPUTER VISION
AB Object detection is considered as one of the most important applications of deep learning. However, the object detection techniques lose their effectiveness and reliability when they fall victim to adversarial attacks. This big flaw has made it challenging to fully adopt the object detection applications in important products and essential industries such as autonomous vehicles. While the field of adversarial robustness has witnessed a great deal of achievement in building sophisticated methods of attack and defense, the majority of the work has been focused on the task of image classification due to its simplicity in theory and practice. In this paper, we provide an up-to-date survey of recent advancements in the field of adversarial robustness for object detection. We review the prominent attack and defense mechanisms presented in the research community and provide discussions and insights on their strengths and weaknesses. In addition, we review the recent literature on adversarial robustness for applications related to autonomous vehicles, as a critical aspect of this high-impact emerging industry, in which the robustness of models is of vital importance.
C1 [Amirkhani, Abdollah; Karimi, Mohammad Parsa] Iran Univ Sci & Technol, Sch Automot Engn, Tehran 1684613114, Iran.
   [Banitalebi-Dehkordi, Amin] Huawei Technol Canada Co Ltd, Big Data & Intelligence Platform Lab, Markham, ON, Canada.
C3 Iran University Science & Technology; Huawei Technologies
RP Amirkhani, A (corresponding author), Iran Univ Sci & Technol, Sch Automot Engn, Tehran 1684613114, Iran.
EM amirkhani@iust.ac.ir
RI Amirkhani, Abdollah/C-6743-2019
OI Amirkhani, Abdollah/0000-0001-6891-4528
CR Ackerman Evan., 2017, DRIVEAI IS MASTERING
   Akhtar N, 2018, IEEE ACCESS, V6, P14410, DOI 10.1109/ACCESS.2018.2807385
   Alaifari R., 2019, P INT C LEARN REPR I
   Amirkhani A, 2022, VISUAL COMPUT, V38, P1929, DOI 10.1007/s00371-021-02256-6
   Aprilpyone M, 2019, IEEE ACCESS, V7, P177932, DOI 10.1109/ACCESS.2019.2958358
   Arnab A, 2018, PROC CVPR IEEE, P888, DOI 10.1109/CVPR.2018.00099
   Arora S, 2022, VISUAL COMPUT, V38, P2461, DOI 10.1007/s00371-021-02123-4
   Aung A.M., 2017, ARXIV
   Boloor A, 2019, IEEE I C EMBED SOFTW, DOI 10.1109/icess.2019.8782514
   Bose AK, 2018, IEEE SENSOR, P1161
   Braunegg A., 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P35, DOI 10.1007/978-3-030-58589-1_3
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chow K.H., 2020, ADVERSARIAL OBJECTNE
   Cisse M, 2017, PR MACH LEARN RES, V70
   Cococcioni M, 2021, IEEE SIGNAL PROC MAG, V38, P97, DOI 10.1109/MSP.2020.2988436
   Cococcioni M, 2018, 2018 INTERNATIONAL CONFERENCE OF ELECTRICAL AND ELECTRONIC TECHNOLOGIES FOR AUTOMOTIVE
   Cui Y., 2021, 2021 IEEE INT C COMP
   Deng L, 2019, IEEE INT C BIOINFORM, P74, DOI 10.1109/BIBM47256.2019.8983051
   Deng Y, 2021, IEEE T IND INFORM, V17, P7897, DOI 10.1109/TII.2021.3071405
   Deng Y, 2020, INT CONF PERVAS COMP, DOI 10.1109/percom45495.2020.9127389
   Dodge S, 2016, 2016 EIGHTH INTERNATIONAL CONFERENCE ON QUALITY OF MULTIMEDIA EXPERIENCE (QOMEX)
   Dziugaite GK, 2016, ARXIV
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Goodfellow I. J., 2015, 3 INT C LEARNING REP
   Han D., 2021, ARXIV
   Han JW, 2015, IEEE T CIRC SYST VID, V25, P1309, DOI 10.1109/TCSVT.2014.2381471
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hu HX, 2017, IEEE T IND INFORM, V13, P2106, DOI 10.1109/TII.2017.2683528
   Huang G, 2016, arXiv
   Husnoo MA, 2021, AD HOC NETW, V122, DOI 10.1016/j.adhoc.2021.102627
   Jia Y., 2014, arXiv
   Karimi MP, 2021, IRAN CONF ELECTR ENG, P187, DOI 10.1109/ICEE52715.2021.9544499
   Kaushik P, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTING, COMMUNICATION AND AUTOMATION (ICCCA), P350, DOI 10.1109/CCAA.2017.8229841
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar A, 2019, ARTIF INTELL REV, V52, P927, DOI 10.1007/s10462-018-9650-2
   Kyrkou C, 2020, IEEE COMP SOC ANN, P476, DOI 10.1109/ISVLSI49217.2020.00-11
   Lee H., 2017, ARXIV
   Li DB, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107584
   Li GB, 2018, IEEE T NEUR NET LEAR, V29, P6038, DOI 10.1109/TNNLS.2018.2817540
   Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306
   Li Xiaoyan, 2023, Infect Dis Immun, V3, P20, DOI [10.1097/ID9.0000000000000076, 10.1109/TAI.2021.3107807]
   Li Y., 2018, British Machine Vision Conference, P231
   Liskowski P, 2016, IEEE T MED IMAGING, V35, P2369, DOI 10.1109/TMI.2016.2546227
   Liu AS, 2021, IEEE T IMAGE PROCESS, V30, P5769, DOI 10.1109/TIP.2021.3082317
   Liu DF, 2021, PROC CVPR IEEE, P9811, DOI 10.1109/CVPR46437.2021.00969
   Liu DF, 2020, NEUROCOMPUTING, V409, P1, DOI 10.1016/j.neucom.2020.05.027
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Y., 2016, ARXIV
   Liu ZY, 2020, VISUAL COMPUT, V36, P1823, DOI 10.1007/s00371-019-01778-4
   Lu JJ, 2017, IEEE I CONF COMP VIS, P446, DOI 10.1109/ICCV.2017.56
   Mahmood F, 2020, IEEE T MED IMAGING, V39, P3257, DOI 10.1109/TMI.2019.2927182
   Mahmood F, 2018, IEEE T MED IMAGING, V37, P2572, DOI 10.1109/TMI.2018.2842767
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Mollahosseini A, 2016, IEEE WINT CONF APPL
   Moosavi-Dezfooli SM, 2017, PROC CVPR IEEE, P86, DOI 10.1109/CVPR.2017.17
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Nsaif AK, 2021, IEEE ACCESS, V9, P15708, DOI 10.1109/ACCESS.2021.3052851
   Okuyama T, 2018, 2018 INTERNATIONAL CONFERENCE ON INTELLIGENT AUTONOMOUS SYSTEMS (ICOIAS), P201, DOI 10.1109/ICoIAS.2018.8494053
   Papernot N, 2017, PROCEEDINGS OF THE 2017 ACM ASIA CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (ASIA CCS'17), P506, DOI 10.1145/3052973.3053009
   Perez Juan C., 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P450, DOI 10.1007/978-3-030-58545-7_26
   Poudel B, 2021, IEEE INT C INTELL TR, P3652, DOI 10.1109/ITSC48978.2021.9564671
   Poursaeed O, 2018, PROC CVPR IEEE, P4422, DOI 10.1109/CVPR.2018.00465
   Prakash A, 2018, IEEE DATA COMPR CONF, P137, DOI 10.1109/DCC.2018.00022
   Rajan JP, 2019, J MED SYST, V44, DOI 10.1007/s10916-019-1500-5
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Seifert C, 2017, STUD BIG DATA, V32, P123, DOI 10.1007/978-3-319-54024-5_6
   Shah M, 2017, 2017 INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING AND CONTROL SYSTEMS (ICICCS), P787, DOI 10.1109/ICCONS.2017.8250570
   Shah SAA, 2020, IEEE IMAGE PROC, P718, DOI 10.1109/ICIP40778.2020.9191084
   Sitawarin Chawin, 2018, ARXIV
   Su H, 2020, IEEE ROBOT AUTOM LET, V5, P2943, DOI 10.1109/LRA.2020.2974445
   Su JW, 2019, IEEE T EVOLUT COMPUT, V23, P828, DOI 10.1109/TEVC.2019.2890858
   Sze V, 2017, P IEEE, V105, P2295, DOI 10.1109/JPROC.2017.2761740
   Szegedy C, 2014, INT C LEARN REPR
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tramer Florian, 2017, Ensemble adversarial training: Attacks and defenses
   Wang L, 2021, IEEE T IMAGE PROCESS, V30, P7541, DOI 10.1109/TIP.2021.3106807
   Wang YJ, 2020, J NETW COMPUT APPL, V161, DOI 10.1016/j.jnca.2020.102634
   Wu F, 2019, IEEE INT C NETW SENS, P363, DOI [10.1109/ICNSC.2019.8743246, 10.1109/icnsc.2019.8743246]
   Wu XF, 2023, IEEE T NEUR NET LEAR, V34, P2896, DOI 10.1109/TNNLS.2021.3110109
   Xiao C., 2018, ARXIV
   Xiao YT, 2021, PATTERN RECOGN, V115, DOI 10.1016/j.patcog.2021.107903
   Xie CH, 2017, IEEE I CONF COMP VIS, P1378, DOI 10.1109/ICCV.2017.153
   Xu HY, 2021, IEEE T PATTERN ANAL, V43, P1914, DOI 10.1109/TPAMI.2019.2957780
   Xu J., 2017, 1 INT C ELECT INSTRU, P1, DOI DOI 10.1109/EIIS.2017.8298723
   Yan Z, 2018, ADV NEUR IN, V31
   Yang J., 2020, ARXIV
   Yu XY, 2019, IEEE T NEUR NET LEAR, V30, P2805, DOI 10.1109/TNNLS.2018.2886017
   Zhang HC, 2019, IEEE I CONF COMP VIS, P421, DOI 10.1109/ICCV.2019.00051
   Zhang WJ, 2019, IEEE ACCESS, V7, P151103, DOI 10.1109/ACCESS.2019.2946461
   Zhang Yang, 2019, INT C LEARN REPR
   Zhang YG, 2020, IEEE T IMAGE PROCESS, V29, P4804, DOI 10.1109/TIP.2020.2975918
   Zheng X, 2018, IEEE SYST J, V12, P1667, DOI 10.1109/JSYST.2016.2614599
   Zheng Z., 2018, 32 C NEURAL INFORM P
   Zhou HS, 2020, PROC INT CONF SOFTW, P347, DOI 10.1145/3377811.3380422
   Zhou XZ, 2014, IEEE IMAGE PROC, P843, DOI 10.1109/ICIP.2014.7025169
   Zhu JS, 2018, IEEE J-STARS, V11, P4968, DOI 10.1109/JSTARS.2018.2879368
NR 98
TC 4
Z9 4
U1 2
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5293
EP 5307
DI 10.1007/s00371-022-02660-6
EA SEP 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000852929100001
DA 2024-07-18
ER

PT J
AU Wang, F
   Yin, D
   Song, RY
AF Wang, Fan
   Yin, Dong
   Song, Ruiyuan
TI Image super-resolution using only low-resolution images
SO VISUAL COMPUTER
LA English
DT Article
DE Image super-resolution; Unknown degradation mode; Low-resolution images;
   Image quality
ID QUALITY ASSESSMENT
AB Currently, most image super-resolution methods use the paired training dataset which is difficult to collect. So bicubic mode is often used to generate paired low-resolution and high-resolution images, but this does not match the actual degradation mode which is often unknown. This results in deteriorated performance of these methods. Considering that only low-resolution images can actually be acquired, we propose a novel model architecture named Cycle-SRGAN which uses only low-resolution images to achieve image super-resolution. The Cycle-SRGAN consists of three components: an up-sampler, a down-sampler and a discriminator. The up-sampler generates high-resolution images, and the down-sampler learns the degradation mode of the images. The discriminator is adopted to help learn the downscaling mode. And we apply two cycle losses, an adversarial loss and a regularization term to implement training. Our method can realize downscaling kernel estimation and image super-resolution at the meantime. In the end, the experimental results indicate the images generated by Cycle-SRGAN own higher image quality than some other methods. And our method has good stability and generalization performance for unknown degradation modes.
C1 [Wang, Fan; Yin, Dong] Univ Sci & Technol China, Sch Informat Sci & Technol, 96 JinZhai Rd, Hefei 230026, Anhui, Peoples R China.
   [Song, Ruiyuan] Univ Sci & Technol China, Sch Cyber Sci & Technol, 96 JinZhai Rd, Hefei 230026, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Yin, D (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, 96 JinZhai Rd, Hefei 230026, Anhui, Peoples R China.
EM wangfan1998@mail.ustc.edu.cn; yindong@ustc.edu.cn;
   rysong@mail.ustc.edu.cn
FU GPU cluster built by MCC Lab of Information Science and Technology
   Institution, USTC
FX We acknowledge the support of GPU cluster built by MCC Lab of
   Information Science and Technology Institution, USTC.
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Ahn N, 2020, IEEE COMPUT SOC CONF, P1953, DOI 10.1109/CVPRW50498.2020.00245
   Amaranageswarao G, 2022, VISUAL COMPUT, V38, P31, DOI 10.1007/s00371-020-01998-z
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bell-Kligler S, 2019, ADV NEUR IN, V32
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chen SJ, 2020, IEEE COMPUT SOC CONF, P1924, DOI 10.1109/CVPRW50498.2020.00242
   Chudasama V, 2022, VISUAL COMPUT, V38, P3643, DOI 10.1007/s00371-021-02193-4
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu JJ, 2019, PROC CVPR IEEE, P1604, DOI 10.1109/CVPR.2019.00170
   Guo Y, 2020, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR42600.2020.00545
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hou Yangshuan, 2020, 2020 3rd International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE). Proceedings, P256, DOI 10.1109/AEMCSE50948.2020.00062
   Hu XC, 2019, PROC CVPR IEEE, P1575, DOI 10.1109/CVPR.2019.00167
   Hu YT, 2020, IEEE T CIRC SYST VID, V30, P3911, DOI 10.1109/TCSVT.2019.2915238
   HUANG JB, 2015, PROC CVPR IEEE, P5197, DOI DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082
   Ignatov A, 2017, IEEE I CONF COMP VIS, P3297, DOI 10.1109/ICCV.2017.355
   IRANI M, 1991, CVGIP-GRAPH MODEL IM, V53, P231, DOI 10.1016/1049-9652(91)90045-L
   Gulrajani I, 2017, ADV NEUR IN, V30
   Jiang K, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10111700
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim J, 2016, IEEE CONF COMPUT
   Kim KI, 2010, IEEE T PATTERN ANAL, V32, P1127, DOI 10.1109/TPAMI.2010.25
   Kingma D. P., 2014, arXiv
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Radford A., 2015, ARXIV
   Shi WL, 2021, VISUAL COMPUT, V37, P1569, DOI 10.1007/s00371-020-01903-8
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shi WZ, 2013, LECT NOTES COMPUT SC, V8151, P9, DOI 10.1007/978-3-642-40760-4_2
   Shocher A, 2018, PROC CVPR IEEE, P3118, DOI 10.1109/CVPR.2018.00329
   Song DH, 2021, PROC CVPR IEEE, P15643, DOI 10.1109/CVPR46437.2021.01539
   Szegedy C., 2017, AAAI, V4, P12
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Venkatanath N, 2015, NATL CONF COMMUN
   Wang PR, 2019, INT GEOSCI REMOTE SE, P3117, DOI [10.1109/igarss.2019.8898648, 10.1109/IGARSS.2019.8898648]
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Xu B., 2015, ARXIV
   Yang X, 2022, VISUAL COMPUT, V38, P4307, DOI 10.1007/s00371-021-02297-x
   Yuan Y, 2018, IEEE COMPUT SOC CONF, P814, DOI 10.1109/CVPRW.2018.00113
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang YB, 2020, IEEE T IMAGE PROCESS, V29, P1101, DOI 10.1109/TIP.2019.2938347
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu XY, 2022, IEEE T CIRC SYST VID, V32, P1273, DOI 10.1109/TCSVT.2021.3078436
NR 58
TC 2
Z9 2
U1 6
U2 30
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 5069
EP 5084
DI 10.1007/s00371-022-02646-4
EA AUG 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000844899700001
DA 2024-07-18
ER

PT J
AU Yao, H
   Qin, HW
   Wu, Q
   Bi, ZS
   Wang, XZ
AF Yao, Hai
   Qin, Huawang
   Wu, Qian
   Bi, Zhisong
   Wang, Xuezhu
TI A multi-expose fusion image dehazing based on scene depth information
SO VISUAL COMPUTER
LA English
DT Article
DE Image defogging; Gamma correction; Image patches; Fusion weight maps
ID DARK CHANNEL PRIOR; HAZE REMOVAL; ALGORITHM; CONTRAST
AB Haze weather can lead to reduced visibility of captured images, affecting daily production and life. In this paper, a new defogging technique for multi-exposure images combined with prior algorithm is proposed. Firstly, the transmittance of different regions of the haze image is calculated to obtain more accurate prior information. Secondly, gamma correction is applied to the prior map to obtain a set of multiple underexposure images. Thirdly, for the difference between global features and local details in image defogging, the multiple underexposure image set is decomposed into base and detail layers using guided filtering, and the fusion weight maps of the base layers image patches and the detail layers Laplacian decomposition are constructed, respectively. Finally, the haze-free image is reconstructed and restored. The haze image is selected from a standard dataset with different haze concentrations and compared with the commonly used haze removal algorithms. The defogging effect of this algorithm has better performance in visual effect and objective evaluation index.
C1 [Yao, Hai; Qin, Huawang] Nanjing Univ Informat Sci & Technol, Sch Elect & Informat Engn, Nanjing 210044, Peoples R China.
   [Wu, Qian] Anhui Jianzhu Univ, Sch Elect & Informat Engn, Hefei 230041, Peoples R China.
   [Bi, Zhisong; Wang, Xuezhu] Northwest Univ, Sch Informat Sci & Technol, Xian 710127, Peoples R China.
C3 Nanjing University of Information Science & Technology; Anhui Jianzhu
   University; Northwest University Xi'an
RP Qin, HW (corresponding author), Nanjing Univ Informat Sci & Technol, Sch Elect & Informat Engn, Nanjing 210044, Peoples R China.
EM yaohaiyaohai666@163.com; qin_h_w@163.com; 1079845443@qq.com;
   1327326897@qq.com; 846949503@qq.com
RI LI, Xiang/JBJ-8387-2023; zhu, zhu/JDN-0159-2023; wang,
   wei/JBS-7400-2023; yang, zhou/KBB-6972-2024; zhan, xiao/JEZ-3810-2023
CR Al-Ameen Z, 2015, SCANNING, V37, P116, DOI 10.1002/sca.21187
   Al-Rawi M., 2016, INTENSITY NORMALIZAT
   Ancuti CO, 2013, IEEE T IMAGE PROCESS, V22, P3271, DOI 10.1109/TIP.2013.2262284
   [Anonymous], 2016, VIDEO DEHAZING SURVE
   Babu GH, 2020, J VIS COMMUN IMAGE R, V72, DOI 10.1016/j.jveir.2020.102912
   Bansal Bindu, 2017, International Journal of Image, Graphics and Signal Processing, V9, P62, DOI 10.5815/ijigsp.2017.11.07
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   Galdran A, 2018, SIGNAL PROCESS, V149, P135, DOI 10.1016/j.sigpro.2018.03.008
   Gao Y, 2020, SIGNAL PROCESS, V167, DOI 10.1016/j.sigpro.2019.107284
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hong S, 2021, SIGNAL PROCESS, V178, DOI 10.1016/j.sigpro.2020.107798
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Jackson J, 2020, IEEE ACCESS, V8, P73330, DOI 10.1109/ACCESS.2020.2988144
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Kaur Jaswinder, 2017, 2017 Third International Conference on Advances in Electrical, Electronics, Information, Communication and Bio-Informatics (AEEICB). Proceedings, P357, DOI 10.1109/AEEICB.2017.7972331
   Kuthirummal S, 2011, IEEE T PATTERN ANAL, V33, P58, DOI 10.1109/TPAMI.2010.66
   Li ZG, 2021, IEEE T IMAGE PROCESS, V30, P9270, DOI 10.1109/TIP.2021.3123551
   Liang W, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3391297
   Ling ZG, 2018, IEEE T MULTIMEDIA, V20, P1699, DOI 10.1109/TMM.2017.2778565
   Liu JP, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21113926
   Liu Q., 2011, RES IMAGE DEHAZING A
   Liu Q, 2017, SIGNAL PROCESS, V137, P33, DOI 10.1016/j.sigpro.2017.01.036
   Mao R, 2018, 2018 3RD INTERNATIONAL CONFERENCE ON MECHANICAL, CONTROL AND COMPUTER ENGINEERING (ICMCCE), P568, DOI 10.1109/ICMCCE.2018.00125
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Nunez J., 1996, INFORM ENTROPY
   Panagopoulos A., 2010, ESTIMATING SHADOWS B
   Parihar A.S., 2020, 2020 5 INT C COMM EL, P766
   Rahman S, 2016, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-016-0138-1
   Rahman Z, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P1003, DOI 10.1109/ICIP.1996.560995
   Salazar-Colores S, 2020, IEEE ACCESS, V8, P149176, DOI 10.1109/ACCESS.2020.3015724
   Schmidt M, 2017, MATH PROGRAM, V162, P83, DOI 10.1007/s10107-016-1030-6
   Singh Dilbag, 2019, Archives of Computational Methods in Engineering, V26, P1395, DOI 10.1007/s11831-018-9294-z
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei ZL, 2020, INT J COMPUT SCI ENG, V23, P115, DOI 10.1504/IJCSE.2020.110556
   Wu H., 2019, IMAGE DEHAZING ALGOR
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2603, DOI 10.1109/TMM.2019.2904879
   Yang Y., 2022, VISUAL COMPUT, P1
   Zhang YT, 2021, IEEE ACCESS, V9, P87826, DOI 10.1109/ACCESS.2021.3090202
   Zhao JT, 2019, LASER OPTOELECTRON P, V56, DOI 10.3788/LOP56.111005
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
   Zhu ZQ, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3024335
   Zhuang LY, 2020, IEEE ACCESS, V8, P207275, DOI 10.1109/ACCESS.2020.3038239
NR 44
TC 3
Z9 3
U1 6
U2 27
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4855
EP 4867
DI 10.1007/s00371-022-02632-w
EA AUG 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000843733800002
DA 2024-07-18
ER

PT J
AU Xia, CX
   Sun, YG
   Gao, XJ
   Ge, B
   Duan, SS
AF Xia, Chenxing
   Sun, Yanguang
   Gao, Xiuju
   Ge, Bin
   Duan, Songsong
TI DMINet: dense multi-scale inference network for salient object detection
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Fully convolutional networks; Multi-scale contextual
   features; Salient object detection
ID IMAGE; MODEL
AB Although the salient object detection (SOD) methods based on fully convolutional networks have made extraordinary achievements, it is still a challenge to accurately detect salient objects with complicated structure from cluttered real-world scenes due to their rarely considering the effectiveness and correlation of the captured different scale context and how to efficient interaction of complementary information. Motivate by this, in this paper, a novel Dense Multi-scale Inference Network (DMINet) is proposed for the accurate SOD task, which mainly consists of a dual-stream multi-receptive field module and a residual multi-mode interaction strategy. The former uses the well-designed different receptive field convolution operations and dense guidance connections to efficiently capture and utilize multi-scale contextual features for better salient objects inferring, while the latter adopts diverse interaction manners to adequately interact complementary information from multi-level features, generating powerful feature representations for predicting high-quality saliency maps. Quantitative and qualitative comparison results on five SOD datasets convincingly demonstrate that our DMINet performs favorably compared with 17 state-of-the-art SOD methods under different evaluation metrics.
C1 [Xia, Chenxing; Sun, Yanguang; Ge, Bin; Duan, Songsong] Anhui Univ Sci & Technol, Coll Comp Sci & Engn, Huainan 232001, Peoples R China.
   [Xia, Chenxing] Hefei Comprehens Natl Sci Ctr, Inst Energy, Hefei 230031, Peoples R China.
   [Gao, Xiuju] Anhui Univ Sci & Technol, Coll Elect & Informat Engn, Huainan, Anhui, Peoples R China.
C3 Anhui University of Science & Technology; Anhui University of Science &
   Technology
RP Sun, YG (corresponding author), Anhui Univ Sci & Technol, Coll Comp Sci & Engn, Huainan 232001, Peoples R China.
EM syg1513@163.com
RI Xia, Chenxing/GYQ-6472-2022
FU National Science Foundation of China [6210071479]; Anhui Natural Science
   Foundation [2108085QF258]; Natural Science Research Project of Colleges
   and Universities in Anhui Province [KJ2020A0299]; University-level key
   projects of Anhui University of science and technology [QN2019102];
   University-level general projects of Anhui University of science and
   technology [xjyb2020-04]
FX This work was supported by the National Science Foundation of China
   (6210071479), the Anhui Natural Science Foundation (2108085QF258), the
   Natural Science Research Project of Colleges and Universities in Anhui
   Province (KJ2020A0299), the University-level key projects of Anhui
   University of science and technology (QN2019102) and the
   University-level general projects of Anhui University of science and
   technology (xjyb2020-04).
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Chen S., 2022, VISUAL COMPUT, P1
   Chen SH, 2020, IEEE T IMAGE PROCESS, V29, P3763, DOI 10.1109/TIP.2020.2965989
   Cheng MM, 2021, IEEE T PATTERN ANAL
   Das DK, 2022, VISUAL COMPUT, V38, P3803, DOI 10.1007/s00371-021-02222-2
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Donoser M, 2009, IEEE I CONF COMP VIS, P817, DOI 10.1109/ICCV.2009.5459296
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Feng MY, 2020, IEEE T IMAGE PROCESS, V29, P4696, DOI 10.1109/TIP.2020.2975919
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li JX, 2021, IEEE T MULTIMEDIA, V23, P1397, DOI 10.1109/TMM.2020.2997192
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24
   Liu Y, 2020, IEEE T IMAGE PROCESS, V29, P360, DOI 10.1109/TIP.2019.2930906
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZY, 2020, VISUAL COMPUT, V36, P1823, DOI 10.1007/s00371-019-01778-4
   Long J., 2015, P IEEE C COMP VIS PA, P3431
   Margolin R, 2013, PROC CVPR IEEE, P1139, DOI 10.1109/CVPR.2013.151
   Máttyus G, 2017, IEEE I CONF COMP VIS, P3458, DOI 10.1109/ICCV.2017.372
   Peng HW, 2017, IEEE T PATTERN ANAL, V39, P818, DOI 10.1109/TPAMI.2016.2562626
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Ramanishka V, 2017, PROC CVPR IEEE, P3135, DOI 10.1109/CVPR.2017.334
   Ren QH, 2021, IEEE T MULTIMEDIA, V23, P1442, DOI 10.1109/TMM.2020.2997178
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Tong N, 2015, PROC CVPR IEEE, P1884, DOI 10.1109/CVPR.2015.7298798
   Wang B, 2020, VISUAL COMPUT, V36, P1897, DOI 10.1007/s00371-019-01779-3
   Wang LS, 2021, IEEE T CIRC SYST VID, V31, P728, DOI 10.1109/TCSVT.2020.2988768
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330
   Wu Z, 2019, IEEE I CONF COMP VIS, P7263, DOI 10.1109/ICCV.2019.00736
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xia CX, 2021, SIGNAL PROCESS-IMAGE, V98, DOI 10.1016/j.image.2021.116372
   Xia CX, 2020, APPL INTELL, V50, P2977, DOI 10.1007/s10489-020-01691-7
   Xia CX, 2020, NEUROCOMPUTING, V383, P194, DOI 10.1016/j.neucom.2019.09.096
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Zhang JM, 2016, PROC CVPR IEEE, P5733, DOI 10.1109/CVPR.2016.618
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhao X., 2020, P EUR C COMP VIS, P35, DOI 10.1007/ 978-3-030-58536-5_3
   Zhou H., 2020, P IEEE C COMP VIS PA, P9141, DOI 10.1109/CVPR42600.2020.00916
NR 53
TC 6
Z9 6
U1 3
U2 27
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3059
EP 3072
DI 10.1007/s00371-022-02561-8
EA JUL 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000830968400001
DA 2024-07-18
ER

PT J
AU Sonam
   Sehra, K
   Singh, RP
   Singh, S
   Wadhera, S
   Kasturi, P
   Saxena, GJ
   Saxena, M
AF Sonam
   Sehra, Khushwant
   Singh, Raghvendra Pratap
   Singh, Sharat
   Wadhera, Shweta
   Kasturi, Poonam
   Saxena, Geetika Jain
   Saxena, Manoj
TI Secure digital image watermarking using memristor-based hyperchaotic
   circuit
SO VISUAL COMPUTER
LA English
DT Article
DE Hyperchaotic signals; Memristor; Discrete cosine transform (DCT);
   Extreme learning machine (ELM); Human visual system (HVS); Histogram of
   oriented gradients (HOG); Arnold transform
ID EXTREME LEARNING-MACHINE; ARNOLD TRANSFORM; SCHEME; ROBUST
AB Due to the rapid access to the Internet, digital data can be easily accessed by unauthorized users. The digital watermarking technique is a crucial way in which carrier signal hides digital information in the form of a watermark to prevent the stakeholders' authenticity by changing different coefficients. Some of the basic requirements that must be satisfied for digital image watermarking are robustness, security, and imperceptibility. To ensure robustness against image processing attacks, this paper proposes a secure digital image watermarking system based on a memristor-based hyperchaotic oscillator. To simulate real-world usage, the human visual system (HVS) model is utilized to assess image quality. The HOG model is used to extract the important features from the host image, while the ELM model is utilized for faster training. Hardware-level encryption is implemented using memristor-based hyperchaotic signals, which is combined with Arnold transformation to generate highly secure keys. This paper also presents a detailed analysis to compare and evaluate robustness and security trade-offs using benchmarks such as PSNR, NC, and SSIM with the presence of various attacks. The PSNR value of signed image is up to 41.02 dB and the SSIM value is 0.999, which indicates the imperceptibility and high security. The NC value approaches unity which dictates the high robustness of the watermarking scheme to various image processing attacks. Simulated results are verified using hyperchaotic signals generated from the hyperchaotic oscillator, which evinces exceptional security against data crimes dealing with watermarks and image processing tasks.
C1 [Sonam] Univ Delhi, Inst Informat & Commun, South Campus, New Delhi 110021, India.
   [Sehra, Khushwant] Univ Delhi, Dept Elect Sci, South Campus, New Delhi 110021, India.
   [Singh, Raghvendra Pratap; Singh, Sharat; Kasturi, Poonam; Saxena, Manoj] Univ Delhi, Deen Dayal Upadhyaya Coll, Dept Elect, New Delhi 110078, India.
   [Wadhera, Shweta] Univ Delhi, Deen Dayal Upadhyaya Coll, Dept Comp Sci, New Delhi 110078, India.
   [Saxena, Geetika Jain] Univ Delhi, Maharaja Agrasen Coll, Dept Elect, New Delhi 110096, India.
C3 University of Delhi; University of Delhi; Deen Dayal Upadhyaya College;
   University of Delhi; Deen Dayal Upadhyaya College; University of Delhi;
   University of Delhi
RP Saxena, M (corresponding author), Univ Delhi, Deen Dayal Upadhyaya Coll, Dept Elect, New Delhi 110078, India.
EM sonam.octer@gmail.com; sehrakhushwant@gmail.com; msaxena@ddu.du.ac.in
RI Sehra, Khushwant/AAY-2093-2020; Saxena, Manoj/K-3863-2015; Singh,
   Raghvendra Pratap/AEG-9176-2022
OI Sehra, Khushwant/0000-0002-1385-5572; Saxena, Manoj/0000-0002-9368-4194;
   Singh, Raghvendra Pratap/0000-0001-7519-6165; Singh,
   Sharat/0000-0001-8707-4317; Saxena, Manoj/0000-0001-5010-0753; ,
   SONAM/0000-0003-4777-9659
FU Society for Microelectronics and VLSI, New Delhi; University of Delhi
FX The authors would like to acknowledge DBT Star Laboratory at Deen Dayal
   Upadhyaya College, University of Delhi, under DBT Star Scheme,
   Department of Biotechnology, Government of India; Research Project
   funded by Society for Microelectronics and VLSI, New Delhi; and
   University of Delhi for providing necessary tools and financial
   assistance for the completion of this work.
CR Agarwal C, 2015, EGYPT INFORM J, V16, P83, DOI 10.1016/j.eij.2015.01.002
   Ahmadi SBB, 2021, APPL INTELL, V51, P1701, DOI 10.1007/s10489-020-01903-0
   Anand A, 2022, IEEE T COMPUT SOC SY, V9, P1594, DOI 10.1109/TCSS.2021.3126628
   Anand A, 2021, SUSTAIN CITIES SOC, V75, DOI 10.1016/j.scs.2021.103398
   [Anonymous], NATL LIB MED
   [Anonymous], AC U LIC VERS R2021A
   Ahmadi SBB, 2021, VISUAL COMPUT, V37, P385, DOI 10.1007/s00371-020-01808-6
   Begum M, 2022, J KING SAUD UNIV-COM, V34, P5856, DOI 10.1016/j.jksuci.2021.07.012
   Benziane A, 2021, INT ARAB J INF TECHN, V18, P140, DOI 10.34028/iajit/18/2/2
   Bi HB, 2008, 2008 CHINESE CONTROL AND DECISION CONFERENCE, VOLS 1-11, P4629, DOI 10.1109/CCDC.2008.4598207
   Cao ZL, 2019, MULTIMED TOOLS APPL, V78, P26089, DOI 10.1007/s11042-019-07809-5
   Chen SY, 2022, VISUAL COMPUT, V38, P2189, DOI 10.1007/s00371-021-02277-1
   Delaigle JF, 1998, SIGNAL PROCESS, V66, P319, DOI 10.1016/S0165-1684(98)00013-9
   Deng WY, 2009, 2009 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE AND DATA MINING, P389, DOI 10.1109/CIDM.2009.4938676
   Thakkar F, 2021, MULTIMED TOOLS APPL, V80, P12275, DOI 10.1007/s11042-020-10220-0
   Fares K, 2020, OPTIK, V208, DOI 10.1016/j.ijleo.2020.164562
   Hassan FS, 2022, J KING SAUD UNIV-COM, V34, P2017, DOI 10.1016/j.jksuci.2020.07.008
   HIGUCHI T, 1988, PHYSICA D, V31, P277, DOI 10.1016/0167-2789(88)90081-4
   Hsu LY, 2019, IEEE ACCESS, V7, P107438, DOI 10.1109/ACCESS.2019.2932077
   Huang GB, 2004, IEEE IJCNN, P985
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Izquierdo SS, 2018, JASSS-J ARTIF SOC S, V21, DOI 10.18564/jasss.3660
   Joseph Hilkiya, 2020, 2020 Proceedings of the International Conference on Communication and Signal Processing (ICCSP), P0940, DOI 10.1109/ICCSP48568.2020.9182052
   KATZ MJ, 1988, COMPUT BIOL MED, V18, P145, DOI 10.1016/0010-4825(88)90041-8
   Kaur M., 2016, Int J Multimed Ubiquitous Eng, V11, P37, DOI DOI 10.14257/IJMUE.2016.11.4.05
   Kumar S, 2021, MULTIMED TOOLS APPL, V80, P15487, DOI 10.1007/s11042-020-10322-9
   Kutter M, 2002, IEEE T IMAGE PROCESS, V11, P16, DOI 10.1109/83.977879
   Li M, 2013, ADV INTEL SYS RES, V84, P1309
   Liu DC, 2021, SIGNAL PROCESS-IMAGE, V95, DOI 10.1016/j.image.2021.116292
   Liu DR, 2022, VEHICLE SYST DYN, V60, P433, DOI 10.1080/00423114.2020.1817508
   Loan NA, 2018, IEEE ACCESS, V6, P19876, DOI 10.1109/ACCESS.2018.2808172
   Long M, 2013, RADIOENGINEERING, V22, P208
   Mahto DK, 2021, COMPUT ELECTR ENG, V93, DOI 10.1016/j.compeleceng.2021.107255
   Mehta R, 2016, J SIGNAL PROCESS SYS, V84, P265, DOI 10.1007/s11265-015-1055-8
   MISHRA A, 2012, IEEE IJCNN
   Mishra A., 2018, P 6 INT C LEARN REPR, P1
   Mittal M, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12050822
   Nazir H, 2021, SECUR COMMUN NETW, V2021, DOI 10.1155/2021/6617944
   Prabha K, 2022, J KING SAUD UNIV-COM, V34, P2982, DOI 10.1016/j.jksuci.2020.04.003
   Rohani M, 2009, 2009 14TH INTERNATIONAL COMPUTER CONFERENCE, P417
   Roy S, 2019, IJST-T ELECTR ENG, V43, P201, DOI 10.1007/s40998-018-0109-x
   Rustad S, 2022, J KING SAUD UNIV-COM, V34, P3559, DOI 10.1016/j.jksuci.2020.12.017
   Sahila K. M., 2021, Intelligent Data Communication Technologies and Internet of Things. Proceedings of ICICI 2020. Lecture Notes on Data Engineering and Communications Technologies (LNDECT 57), P805, DOI 10.1007/978-981-15-9509-7_65
   Sahin ME, 2020, COMPUT ELECTR ENG, V88, DOI 10.1016/j.compeleceng.2020.106826
   Sarkar T., 2014, ARXIV PREPRINT ARXIV
   Sehra K, 2021, IEEE ACCESS, V9, P72465, DOI 10.1109/ACCESS.2021.3079319
   Sharma S., 2020, IEEE T ENG MANAGE, DOI [10.1109/ TEM.2020.3019033, DOI 10.1109/TEM.2020.3019033]
   Singh OP, 2023, CONCURR COMP-PRACT E, V35, DOI 10.1002/cpe.6251
   Singha A., 2020, J KING SAUD UNIVER C
   Sinhal R, 2021, PATTERN RECOGN LETT, V145, P171, DOI 10.1016/j.patrec.2021.02.011
   Sleit Azzam, 2018, 2018 5th International Conference on Computational Science and Computational Intelligence (CSCI), P397, DOI 10.1109/CSCI46756.2018.00081
   Su QT, 2020, MULTIMED TOOLS APPL, V79, P30023, DOI 10.1007/s11042-020-09436-x
   Su QT, 2019, IEEE ACCESS, V7, P30398, DOI 10.1109/ACCESS.2019.2895062
   Su QT, 2012, OPT COMMUN, V285, P1792, DOI 10.1016/j.optcom.2011.12.065
   Tabassum F, 2021, J KING SAUD UNIV-COM, V33, P542, DOI 10.1016/j.jksuci.2019.03.005
   Tong XJ, 2016, NONLINEAR DYNAM, V84, P2333, DOI 10.1007/s11071-016-2648-x
   Voloshynovskiy S, 2001, IEEE COMMUN MAG, V39, P118, DOI 10.1109/35.940053
   [王步宇 Wang Buyu], 2005, [振动与冲击, Journal of Vibration and Shock], V24, P87
   Wang J, 2020, EXPERT SYST APPL, V140, DOI 10.1016/j.eswa.2019.112868
   Ye GD, 2012, NONLINEAR DYNAM, V69, P2079, DOI 10.1007/s11071-012-0409-z
   Yuan ZH, 2021, VISUAL COMPUT, V37, P1867, DOI 10.1007/s00371-020-01945-y
   Zhang Y.H., 2009, WSEAS INT C P MATH C
NR 62
TC 2
Z9 2
U1 3
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4459
EP 4485
DI 10.1007/s00371-022-02601-3
EA JUL 2022
PG 27
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000825908300002
DA 2024-07-18
ER

PT J
AU Song, YC
   Sun, ZX
   Li, Q
   Wu, YJ
   Sun, YH
   Luo, ST
AF Song, Youcheng
   Sun, Zhengxing
   Li, Qian
   Wu, Yunjie
   Sun, Yunhan
   Luo, Shoutong
TI Learning indoor point cloud semantic segmentation from image-level
   labels
SO VISUAL COMPUTER
LA English
DT Article
DE Point cloud segmentation; Scene understanding; Weakly supervised
   learning
AB The data-hungry nature of deep learning and the high cost of annotating point-level labels make it difficult to apply semantic segmentation methods to indoor point cloud scenes. Therefore, exploring how to make point cloud segmentation methods less rely on point-level labels is a promising research topic. In this paper, we introduce a weakly supervised framework for semantic segmentation on indoor point clouds. To reduce the labor cost in data annotation, we use image-level weak labels that only indicate the classes that appeared in the rendered images of point clouds. The experiments validate the effectiveness and scalability of our framework. Our segmentation results on both ScanNet and S3DIS datasets outperform the state-of-the-art method using a similar level of weak supervision.
C1 [Song, Youcheng; Sun, Zhengxing; Wu, Yunjie; Sun, Yunhan; Luo, Shoutong] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China.
   [Li, Qian] Natl Univ Def Technol, Coll Meteorol & Oceanog, Changsha, Peoples R China.
C3 Nanjing University; National University of Defense Technology - China
RP Sun, ZX (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China.
EM njumagic@nju.edu.cn
RI ZHAO, S/IWV-4219-2023; wu, jun/ISB-8607-2023; wang, yue/ISA-4119-2023;
   Liu, Han/HMD-9231-2023; LI, XIAO/IQV-9318-2023; XU, nan/KDP-0628-2024;
   LI, XIAO/JCE-6169-2023; zhang, yuyang/IVV-5089-2023; Zhang,
   Jiaqi/JCO-6818-2023; zhang, yue/JAC-3705-2023; Wang, lili/IXD-9828-2023;
   Sun, Zhengxing/A-7411-2011; Zhang, Can/JUU-9511-2023; LIU,
   HAO/JBI-9623-2023; liu, junyang/IXD-1201-2023; Zhang,
   Wenxiao/KCK-3295-2024; fang, yu/KCK-2014-2024; zhang, yu/HNS-5948-2023;
   li, yang/IQV-3559-2023; li, bo/JJC-2664-2023; liu,
   junyang/IXD-1252-2023; Li, Xiaoli/JVZ-4089-2024; Lu,
   Xiaomei/IUQ-2139-2023; Li, Li/IAQ-0885-2023; cheng, shu/IZE-4788-2023;
   liang, YU/IYT-4334-2023
OI Liu, Han/0000-0002-5269-8477; Qian, Li/0000-0002-9530-4925; liang,
   YU/0009-0007-3922-3454
FU National Natural Science Foundation of China [42075139, 42077232,
   61272219]; National High Technology Research and Development Program of
   China [2007AA01Z334]; Science and technology program of Jiangsu Province
   [BE2020082, BE2010072, BE2011058, BY2012190]; China Postdoctoral Science
   Foundation [2017M621700]; Innovation Fund of State Key Laboratory for
   Novel Software Technology [ZZKT2018A09]
FX This work is supported by: The National Natural Science Foundation of
   China Nos. 42075139, 42077232, 61272219; The National High Technology
   Research and Development Program of China No. 2007AA01Z334; The Science
   and technology program of Jiangsu Province Nos. BE2020082, BE2010072,
   BE2011058, BY2012190; and The China Postdoctoral Science Foundation No.
   2017M621700 and Innovation Fund of State Key Laboratory for Novel
   Software Technology No. ZZKT2018A09.
CR Ahn J, 2019, PROC CVPR IEEE, P2204, DOI 10.1109/CVPR.2019.00231
   Ahn J, 2018, PROC CVPR IEEE, P4981, DOI 10.1109/CVPR.2018.00523
   [Anonymous], 2016, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, DOI [DOI 10.1109/CVPR.2016.170, 10.1109/CVPR.2016.170]
   Bearman A, 2016, LECT NOTES COMPUT SC, V9911, P549, DOI 10.1007/978-3-319-46478-7_34
   Boulch A., 2017, P WORKSH 3D OBJ RETR, V3, P1
   Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319
   Dai A, 2018, LECT NOTES COMPUT SC, V11214, P458, DOI 10.1007/978-3-030-01249-6_28
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191
   Durand T, 2017, PROC CVPR IEEE, P5957, DOI 10.1109/CVPR.2017.631
   Fan JS, 2020, AAAI CONF ARTIF INTE, V34, P10762
   He K., 2016, P IEEE C COMP VIS PA, P770, DOI DOI 10.48550/ARXIV.1512.03385
   Hong S, 2017, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2017.239
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Huang ZL, 2018, PROC CVPR IEEE, P7014, DOI 10.1109/CVPR.2018.00733
   Kundu A., 2020, COMPUTER VISION ECCV, V12369, P518, DOI [10.1007/978-3-030-58586-0_31, DOI 10.1007/978-3-030-58586-0_31]
   Li HY, 2021, NEUROCOMPUTING, V437, P227, DOI 10.1016/j.neucom.2021.01.091
   Lin D, 2016, PROC CVPR IEEE, P3159, DOI 10.1109/CVPR.2016.344
   Lin YB, 2018, ISPRS J PHOTOGRAMM, V143, P39, DOI 10.1016/j.isprsjprs.2018.05.004
   Milioto A, 2019, IEEE INT C INT ROBOT, P4213, DOI 10.1109/IROS40897.2019.8967762
   Oh SJ, 2017, PROC CVPR IEEE, P5038, DOI 10.1109/CVPR.2017.535
   Oquab M, 2015, PROC CVPR IEEE, P685, DOI 10.1109/CVPR.2015.7298668
   Pinheiro PO, 2015, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2015.7298780
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qingyong Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11105, DOI 10.1109/CVPR42600.2020.01112
   Saleh F, 2016, LECT NOTES COMPUT SC, V9912, P413, DOI 10.1007/978-3-319-46484-8_25
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shi X., ARXIV PREPRINT ARXIV
   Song Y., 2017, PACIFIC RIM C MULTIM, P619
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Tchapmi LP, 2017, INT CONF 3D VISION, P537, DOI 10.1109/3DV.2017.00067
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Tokmakov P, 2016, LECT NOTES COMPUT SC, V9908, P388, DOI 10.1007/978-3-319-46493-0_24
   Wang HB, 2020, IEEE MULTIMEDIA, V27, P112, DOI 10.1109/MMUL.2020.2999464
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wei JC, 2020, PROC CVPR IEEE, P4383, DOI 10.1109/CVPR42600.2020.00444
   Wei YC, 2017, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR.2017.687
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Xu MT, 2021, PROC CVPR IEEE, P3172, DOI 10.1109/CVPR46437.2021.00319
   Xun Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13703, DOI 10.1109/CVPR42600.2020.01372
   Yao YZ, 2021, PROC CVPR IEEE, P2623, DOI 10.1109/CVPR46437.2021.00265
   Zhang Y., 2021, P IEEE CVF INT C COM, P15520
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou ZH, 2018, NATL SCI REV, V5, P44, DOI 10.1093/nsr/nwx106
NR 45
TC 2
Z9 2
U1 5
U2 47
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3253
EP 3265
DI 10.1007/s00371-022-02569-0
EA JUL 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000819903900002
DA 2024-07-18
ER

PT J
AU Zeng, LP
   Duan, XL
   Pan, YH
   Deng, MJ
AF Zeng, Lingpeng
   Duan, Xuliang
   Pan, Yuhao
   Deng, Minjiang
TI Research on the algorithm of helmet-wearing detection based on the
   optimized yolov4
SO VISUAL COMPUTER
LA English
DT Article
DE Hard hat-wearing detection; Real-time monitoring; Yolov4; Feature layer;
   Anchors
AB At construction sites, wearing hard hats is an important and effective measure to protect workers from accidental injury. In order to remind workers to wear hard hats at all times, it is necessary to automate the detection process of hard hat-wearing. Limited to the environment, human posture, personal privacy and other elements, traditional detection methods often cannot detect the wearing of hard hats in a handy and quick manner. In the paper, an improved deep learning model based on yolov4 is proposed to detect hard hat-wearing. The accuracy and speed of the model are optimized by replacing the cumbersome overlapping of multiple convolution modules in the feature pyramid of yolov4 with cross-stage hierarchy modules. At the same time, since hard hat detection is to detect small targets, we improve the performance of yolov4 to detect small targets by changing the yolov4 feature layer output and linear transformation of anchors. The final algorithm obtains a mean average precision of 93.37% in hard hat detection, with an increase of 3.15% compared with that of the original yolov4.
C1 [Zeng, Lingpeng; Duan, Xuliang; Pan, Yuhao; Deng, Minjiang] Sichuan Agr Univ, Yaan 625014, Peoples R China.
C3 Sichuan Agricultural University
RP Duan, XL (corresponding author), Sichuan Agr Univ, Yaan 625014, Peoples R China.
EM 13880109848@163.com; duanxuliang@sicau.edu.cn; kaka0109@foxmail.com;
   2019217013@stu.sicau.edu.cn
OI Duan, Xuliang/0000-0001-7559-346X
FU Sichuan Key Laboratory of Agricultural Information Engineering
FX This work was supported in part by the Sichuan Key Laboratory of
   Agricultural Information Engineering.
CR AKBARKHANZADEH F, 1995, APPL ERGON, V26, P195, DOI 10.1016/0003-6870(95)00017-7
   Alhmiedat T.A., 2007, SURVEY LOCALIZATION
   [Anonymous], 2019, BEREAU LABOR STAT CO
   Barro-Torres S, 2012, COMPUT COMMUN, V36, P42, DOI 10.1016/j.comcom.2012.01.005
   Bochkovskiy A., 2020, PREPRINT
   Chen X, 2015, Microsoft coco captions: Data collection and evaluation server
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fang Q, 2018, AUTOMAT CONSTR, V85, P1, DOI 10.1016/j.autcon.2017.09.018
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Kelm A, 2013, AUTOMAT CONSTR, V36, P38, DOI 10.1016/j.autcon.2013.08.009
   Konda S, 2016, AM J IND MED, V59, P212, DOI 10.1002/ajim.22557
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Nath ND, 2020, AUTOMAT CONSTR, V112, DOI 10.1016/j.autcon.2020.103085
   Redmon J., 2018, COMPUTER VISION PATT
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Tan MX, 2019, PR MACH LEARN RES, V97
   Wang CY, 2020, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW50498.2020.00203
   Wang L, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20071868
   Yu X., 2013, Performance and Challenges in Utilizing Non-Intrusive Sensors for Traffic Data Collection
   Zhang H, 2019, J CONSTR ENG M, V145, DOI 10.1061/(ASCE)CO.1943-7862.0001629
   Zheng LX, 2016, SIMUL MODEL PRACT TH, V65, P45, DOI 10.1016/j.simpat.2016.01.003
NR 28
TC 15
Z9 17
U1 12
U2 72
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 2165
EP 2175
DI 10.1007/s00371-022-02471-9
EA MAY 2022
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000797750800002
DA 2024-07-18
ER

PT J
AU Qin, N
   Gong, ZT
AF Qin, Na
   Gong, Zengtai
TI Color image denoising by means of three-dimensional discrete fuzzy
   numbers
SO VISUAL COMPUTER
LA English
DT Article
DE Color image denoising; Random value impulse noise; Three-dimensional
   discrete fuzzy numbers; Mass center; Ambiguity degree
ID AGGREGATION; FILTER
AB The quality of color image is degraded by the random value impulse noise, which not only affects the overall visual effect of color image, but also causes errors in the application of color image, such as image recognition and image understanding. In this article, a new color image denoising method that uses the three-dimensional discrete fuzzy numbers (3DFN) is presented. Firstly, the definition of three-dimensional discrete fuzzy numbers is introduced based on a special non-empty finite subset of three-dimensional real number space. Secondly, the 3DFN is employed to interpret each pixel value of a color image. In addition, the mass centers and the ambiguity degree are defined to describe the properties of 3DFN. Then, these properties are selected to determine corrupted pixels and replace the noisy pixels; meanwhile, the non-noisy pixel is not changed. At last, some experiments are included to demonstrate that this algorithm can effectively remove the random value impulse noise and preserve the texture and edge of the color images. At the same time, the algorithm also brings forward a new idea in the application of discrete fuzzy numbers.
C1 [Qin, Na; Gong, Zengtai] Northwest Normal Univ, Coll Math & Stat, Lanzhou, Peoples R China.
   [Qin, Na] Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou, Peoples R China.
C3 Northwest Normal University - China; Northwest Normal University - China
RP Gong, ZT (corresponding author), Northwest Normal Univ, Coll Math & Stat, Lanzhou, Peoples R China.
EM super_qn@126.com; zt-gong@163.com
OI Qin, Na/0000-0002-9511-3280
FU National Natural Science Foundation of China [12161082, 12061067,
   61861040]
FX This study was funded by the National Natural Science Foundation of
   China (Grant Nos. 12161082, 12061067 and 61861040).
CR ASTOLA J, 1990, P IEEE, V78, P678, DOI 10.1109/5.54807
   Casasnovas J, 2011, FUZZY SET SYST, V167, P65, DOI 10.1016/j.fss.2010.09.016
   CHANG SSL, 1972, IEEE T SYST MAN CYB, VSMC2, P30, DOI 10.1109/TSMC.1972.5408553
   CVG.: Available: University of Granada, 2014, CVG UGR IM DAT
   DIAMOND P, 1989, FUZZY SET SYST, V29, P341, DOI 10.1016/0165-0114(89)90045-6
   Dubois D., 1993, READINGS FUZZY SETS, P112, DOI [DOI 10.1016/B978-1-4832-1450-4.50015-8, 10.1016/b978-1-4832-1450-4.50015-8]
   Gao X, 2023, VISUAL COMPUT, V39, P1583, DOI 10.1007/s00371-022-02430-4
   Jin LH, 2017, J VIS COMMUN IMAGE R, V48, P54, DOI 10.1016/j.jvcir.2017.05.012
   Kapoor A, 2017, VISUAL COMPUT, V33, P665, DOI 10.1007/s00371-016-1216-1
   Khosravanian A, 2021, VISUAL COMPUT, V37, P1185, DOI 10.1007/s00371-020-01861-1
   Kumawat A, 2022, VISUAL COMPUT, V38, P3681, DOI 10.1007/s00371-021-02196-1
   Liu L, 2020, VISUAL COMPUT, V36, P1521, DOI 10.1007/s00371-019-01746-y
   Liu ZY, 2020, VISUAL COMPUT, V36, P1823, DOI 10.1007/s00371-019-01778-4
   Lukac R, 2004, MULTIDIM SYST SIGN P, V15, P169, DOI 10.1023/B:MULT.0000017024.66297.a0
   Masood S, 2014, APPL SOFT COMPUT, V21, P107, DOI 10.1016/j.asoc.2014.03.006
   Massanet S, 2014, INFORM SCIENCES, V258, P277, DOI 10.1016/j.ins.2013.06.055
   Pandey R, 2015, AEU-INT J ELECTRON C, V69, P523, DOI 10.1016/j.aeue.2014.11.001
   Perfilieva I, 2006, FUZZY SET SYST, V157, P993, DOI 10.1016/j.fss.2005.11.012
   Riera JV, 2015, FUZZY SET SYST, V264, P121, DOI 10.1016/j.fss.2014.05.005
   Riera JV, 2015, APPL SOFT COMPUT, V36, P383, DOI 10.1016/j.asoc.2015.07.022
   Roy A, 2019, MULTIMED TOOLS APPL, V78, P1785, DOI 10.1007/s11042-018-6303-z
   Roy A, 2018, IEEE T IND ELECTRON, V65, P7268, DOI 10.1109/TIE.2018.2793225
   Riera JV, 2014, FUZZY SET SYST, V241, P76, DOI 10.1016/j.fss.2013.09.001
   Riera JV, 2012, FUZZY SET SYST, V191, P21, DOI 10.1016/j.fss.2011.10.004
   Voxman W, 2001, FUZZY SET SYST, V118, P457, DOI 10.1016/S0165-0114(99)00053-6
   Wang G., 2005, Southeast Asian Bull. Math, V29, P1003
   Wang GH, 2015, OPTIK, V126, P2428, DOI 10.1016/j.ijleo.2015.06.005
   Wang GH, 2014, SIGNAL PROCESS, V102, P216, DOI 10.1016/j.sigpro.2014.03.027
   Wang GX, 2015, INT J FUZZY SYST, V17, P531, DOI 10.1007/s40815-015-0038-z
   Wang GX, 2016, INFORM SCIENCES, V326, P258, DOI 10.1016/j.ins.2015.07.045
   Wu CM, 2023, VISUAL COMPUT, V39, P149, DOI 10.1007/s00371-021-02319-8
   Zhang KB, 2013, IEEE T NEUR NET LEAR, V24, P1648, DOI 10.1109/TNNLS.2013.2262001
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhao M, 2019, SOFT COMPUT, V23, P13569, DOI 10.1007/s00500-019-03895-7
NR 34
TC 5
Z9 5
U1 1
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 2051
EP 2063
DI 10.1007/s00371-022-02464-8
EA APR 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000781196400001
DA 2024-07-18
ER

PT J
AU Ametefe, DS
   Sarnin, SS
   Ali, DM
   Muhammad, ZZ
AF Ametefe, Divine Senanu
   Sarnin, Suzi Seroja
   Ali, Darmawaty Mohd
   Muhammad, Zaigham Zaheer
TI Fingerprint pattern classification using deep transfer learning and data
   augmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Fingerprint pattern classification; Data augmentation; VGG16; VGG19;
   DenseNet121
AB Decreasing the number of matching comparisons between presented fingerprints and their respective templates in automated fingerprint identification systems (AFIS) is salient, especially when dealing with large databases. Fingerprint classification abets the achievement of this goal by stratifying fingerprints into their respective pattern profiles. However, the significant inter-class variation among patterns and minor intra-class variations among fingerprint patterns belonging to a similar class remains a obstacle. Unlike the verification process of fingerprints that requires 1:1 matching of templates, the identification process of fingerprint patterns requires 1:N matching to attest the presence of fingerprint in the database, which leads to a higher number of comparisons. Motivated by this problem, we employed the use of deep transfer learning and data augmentation to develop a fingerprint pattren clasifier to clasify six fingerprint patterns. Three separate models were birth from the utilization of the VGG16, VGG19, and DenseNet121 pre-trained models following some preliminary experiment. Results from the implementation of the proposed deep transfer learning with some data augmentation schemes on the selected VGG16, VGG19, and DenseNet121 pre-trained models manifested classification accuracy of 98.2%, 97%, and 97.8%, respectively, as compared to the 93.9%, 93.7% and 92% rendered by the same models devoid of data augmentation. Hence, experimental results proved that data augmentation improves the efficacy of fingerprint pattern classifier models.
C1 [Ametefe, Divine Senanu; Sarnin, Suzi Seroja; Ali, Darmawaty Mohd] Univ Teknol MARA, Coll Engn, Wireless Commun Technol Grp WiCOT, Shah Alam 40450, Selangor, Malaysia.
   [Muhammad, Zaigham Zaheer] Univ Sci & Technol, Elect & Telecommun Res Inst, Daejeon, South Korea.
C3 Universiti Teknologi MARA; University of Science & Technology (UST);
   Electronics & Telecommunications Research Institute - Korea (ETRI)
RP Ametefe, DS (corresponding author), Univ Teknol MARA, Coll Engn, Wireless Commun Technol Grp WiCOT, Shah Alam 40450, Selangor, Malaysia.
EM 2019813378@isiswa.uitm.edu.my; suzis045@uitm.edu.my;
   darma504@uitm.edu.my; mzz.pieas@gmail.com
RI Mohd Ali, Darmawaty/G-3177-2011; Ametefe, Divine Senanu/JGC-6998-2023
OI Ametefe, Divine Senanu/0000-0002-2887-336X
CR Abbood AA., 2014, INT J COMPUT SCI ISS, V11, P111
   Alghamdi A, 2024, MULTIMED TOOLS APPL, V83, P14913, DOI 10.1007/s11042-020-08769-x
   Awad AI, 2012, COMM COM INF SC, V322, P524
   Balti A, 2013, CONTROL ENG APPL INF, V15, P53
   Chugh T., 2019, ARXIV PREPRINT ARXIV
   Crouse D, 2015, INT CONF BIOMETR, P135, DOI 10.1109/ICB.2015.7139043
   de Jongh A, 2019, J FORENSIC SCI, V64, P108, DOI 10.1111/1556-4029.13838
   Engelsma JJ, 2019, INT CONF BIOMETR
   Fiumara Gregory P, 2018, NIST special database 301
   Flovik V., DEEP TRANSFER LEARNI
   Ghiani L., 2015, 2013 INT C BIOMETRIC, P1
   Ghiani L, 2017, IMAGE VISION COMPUT, V58, P110, DOI 10.1016/j.imavis.2016.07.002
   Ghiani L, 2013, INT CONF BIOMETR, DOI 10.1109/ICB.2013.6613027
   Hong JH, 2008, PATTERN RECOGN, V41, P662, DOI 10.1016/j.patcog.2007.07.004
   Hsieh CT, 2009, J APPL SCI ENG, V12, P169
   Nguyen HT, 2019, ALGORITHMS, V12, DOI 10.3390/a12110241
   Iula A, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19102317
   Iwamoto M., 1962, J Anthropol Soc Nippon, V69, P101
   Jeon WS, 2017, INT J FUZZY LOG INTE, V17, P170, DOI 10.5391/IJFIS.2017.17.3.170
   Jiang X., 2009, FINGERPRINT CLASSIFI
   KAMIJO M, 1993, 1993 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOLS 1-3, P1932, DOI 10.1109/ICNN.1993.298852
   Kaushal N., 2011, J BIOMET BIOSTAT, V2, P1, DOI [10.4172/2155-6180.1000123, DOI 10.4172/2155-6180.1000123]
   Klimanee C, 2004, IEEE IMAGE PROC, P849
   Kouamo S., 2016, Journal of Intelligent Learning Systems and Applications, V8, P39
   Labati RD, 2018, PATTERN RECOGN LETT, V113, P58, DOI 10.1016/j.patrec.2017.04.001
   Maltoni D, 2005, LECT NOTES COMPUT SC, V3161, P43
   Marcelino P., Transfer learning from pretrained models
   Marcialis GL, 2009, LECT NOTES COMPUT SC, V5716, P12, DOI 10.1007/978-3-642-04146-4_4
   Michelsanti D, 2017, PROCEEDINGS OF THE 12TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2017), VOL 5, P202, DOI 10.5220/0006116502020209
   Mohamed SM, 2002, PROCEEDINGS OF THE 2002 IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS, VOL 1 & 2, P358, DOI 10.1109/FUZZ.2002.1005016
   Msiza IS, 2009, IEEE SYS MAN CYBERN, P510, DOI 10.1109/ICSMC.2009.5346860
   Peralta D, 2018, INT J INTELL SYST, V33, P213, DOI 10.1002/int.21948
   Raphael A., 2010, P 2021, V68
   Roy A, 2017, IEEE T INF FOREN SEC, V12, P2013, DOI 10.1109/TIFS.2017.2691658
   Sedik A, 2022, NEURAL COMPUT APPL, V34, P11423, DOI 10.1007/s00521-020-05410-8
   Sedik A, 2020, VIRUSES-BASEL, V12, DOI 10.3390/v12070769
   Sharma RP, 2019, VISUAL COMPUT, V35, P1393, DOI 10.1007/s00371-018-01618-x
   Wang R., 2014, ARXIV PREPRINT ARXIV
   Win KN, 2020, FUTURE GENER COMP SY, V110, P758, DOI 10.1016/j.future.2019.10.019
   Wu F, 2020, NEURAL COMPUT APPL, V32, P5725, DOI 10.1007/s00521-019-04499-w
   Zhang QZ, 2004, PATTERN RECOGN, V37, P2233, DOI 10.1016/j.patcog.2003.12.020
   Zhang YL, 2019, IEEE ACCESS, V7, P91476, DOI 10.1109/ACCESS.2019.2927357
NR 42
TC 8
Z9 8
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1703
EP 1716
DI 10.1007/s00371-022-02437-x
EA APR 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000781250200001
DA 2024-07-18
ER

PT J
AU Lv, GR
   Dong, LL
   Zhang, WW
   Xu, WH
AF Lv, Guangrui
   Dong, Lili
   Zhang, Wenwen
   Xu, Wenhai
TI Region-based adaptive association learning for robust image scene
   recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Scene recognition; Regional correlation; Strip context; Context gating;
   Recurrent attention; Gated attention
ID CLASSIFICATION; NETWORK; REPRESENTATION; ATTENTION; GRADIENT
AB Scene recognition is challenging due to the complicated spatial arrangement and varied object distribution inside the scene images. Deep learning methods, especially convolutional neural networks (CNNs), have boosted the performance of scene recognition significantly because of the multistage global feature learning ability. However, the CNNs ignore the local semantics and diverse relationships among local regions, which play an important role in scene recognition. In this paper, a region-based adaptive association learning framework is proposed to tackle these challenges. This framework contains two sub-networks to separately extract the features of semantic distribution and contextual arrangement, and deep fusion networks to fuse such features in a joint and boosting fashion. Firstly, we simplify the current CNN structure so that our feature extractor can maintain high-level spatial region representation ability. Then, we propose a novel regional correlation learning architecture to explore the arbitrary direction dependence of the region and the four directions' long-range strip context dependence of the region in different spatial dimensions. Moreover, we design different attention strategies which include context gating, recurrent attention, and gated attention to further refine the extracted regional relationships and highlight local semantic information. Finally, the collaborative relationship features are deeply fused and then fed into a classification layer so that the whole framework is optimized as a whole. Extensive experimental evaluations on both ocean scene and land scene datasets coming from some different fields show that the proposed method achieves better accurate prediction on scene recognition than other state-of-the-art models.
C1 [Lv, Guangrui; Dong, Lili; Zhang, Wenwen; Xu, Wenhai] Dalian Maritime Univ, Coll Informat Sci & Technol, Dalian 116026, Peoples R China.
C3 Dalian Maritime University
RP Dong, LL (corresponding author), Dalian Maritime Univ, Coll Informat Sci & Technol, Dalian 116026, Peoples R China.
EM donglili@dlmu.edu.cn
RI Zhang, Wenwen/AAI-9384-2021
OI Zhang, Wenwen/0000-0002-3183-0344
FU National Natural Science Foundation of China; Central Universities of
   China
FX This work was conducted within Optoelectronic Information Engineering
   Laboratory with the support of the National Natural Science Foundation
   of China and the Central Universities of China.
CR [Anonymous], 2007, P IEEE INT C COMP VI
   Anwer RM, 2018, ISPRS J PHOTOGRAMM, V138, P74, DOI 10.1016/j.isprsjprs.2018.01.023
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Bi Q, 2020, IEEE GEOSCI REMOTE S, V17, P1603, DOI 10.1109/LGRS.2019.2949930
   Bi Q, 2020, IEEE T IMAGE PROCESS, V29, P4911, DOI 10.1109/TIP.2020.2975718
   Bi Q, 2019, IEEE IMAGE PROC, P2501, DOI [10.1109/ICIP.2019.8803322, 10.1109/icip.2019.8803322]
   Bi Q, 2020, NEUROCOMPUTING, V377, P345, DOI 10.1016/j.neucom.2019.11.068
   Chaudhuri U, 2021, INT C PATT RECOG, P7335, DOI 10.1109/ICPR48806.2021.9412344
   Cheng G, 2017, P IEEE, V105, P1865, DOI 10.1109/JPROC.2017.2675998
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dong LL, 2020, J INFRARED MILLIM W, V39, P650, DOI 10.11972/j.issn.1001-9014.2020.05.016
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Bui HM, 2016, IEEE ACCESS, V4, P10059, DOI 10.1109/ACCESS.2016.2639543
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Javed S.A., 2017, ARXIV PREPRINT ARXIV
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Kalantidis Yannis, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P685, DOI 10.1007/978-3-319-46604-0_48
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li EZ, 2017, IEEE T GEOSCI REMOTE, V55, P5653, DOI 10.1109/TGRS.2017.2711275
   Li J, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12091366
   Li M., 2021, VISUAL COMPUT, P1
   Li Q, 2018, COMPUT SCI ENG, V20, P52, DOI 10.1109/MCSE.2018.108164530
   Liu BD, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11050518
   Liu SP, 2019, NEUROCOMPUTING, V338, P191, DOI 10.1016/j.neucom.2019.01.090
   Liu YS, 2018, IEEE GEOSCI REMOTE S, V15, P183, DOI 10.1109/LGRS.2017.2779469
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lv YF, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11243006
   Margolin R, 2014, LECT NOTES COMPUT SC, V8695, P377, DOI 10.1007/978-3-319-10584-0_25
   Mnih V, 2014, ADV NEUR IN, V27
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Sak H, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1468
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Sun C., 2019, IEEE ACCESS, V7, P1, DOI [10.1109/ACCESS.2018.2876146, DOI 10.1109/ACCESS.2018.2876146]
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wang C, 2021, NEUROCOMPUTING, V423, P179, DOI 10.1016/j.neucom.2020.08.077
   Wang P, 2013, PROC CVPR IEEE, P2858, DOI 10.1109/CVPR.2013.368
   Wang Q, 2019, IEEE T GEOSCI REMOTE, V57, P1155, DOI 10.1109/TGRS.2018.2864987
   Wang RZ, 2019, IEEE I CONF COMP VIS, P3056, DOI 10.1109/ICCV.2019.00315
   Wu JX, 2011, IEEE T PATTERN ANAL, V33, P1489, DOI 10.1109/TPAMI.2010.224
   Xia GS, 2017, IEEE T GEOSCI REMOTE, V55, P3965, DOI 10.1109/TGRS.2017.2685945
   Xie L, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107205
   Xue W, 2020, IEEE ACCESS, V8, P28746, DOI 10.1109/ACCESS.2020.2968771
   Yang Y., 2010, P 18 SIGSPATIAL INT, P270, DOI [10.1145/1869790.1869829, DOI 10.1145/1869790.1869829]
   Yin QW, 2019, MATEC WEB CONF, V277, DOI 10.1051/matecconf/201927702001
   Yu YL, 2018, IEEE GEOSCI REMOTE S, V15, P287, DOI 10.1109/LGRS.2017.2786241
   Zhang C., 2018, A study on overfitting in deep reinforcement learning
   Zhang F, 2016, IEEE T GEOSCI REMOTE, V54, P1793, DOI 10.1109/TGRS.2015.2488681
   Zhao FA, 2020, GEOCARTO INT, V35, P1603, DOI 10.1080/10106049.2019.1583772
   Zheng XT, 2019, IEEE T GEOSCI REMOTE, V57, P4799, DOI 10.1109/TGRS.2019.2893115
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhu QQ, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10040568
   Zou Q, 2015, IEEE GEOSCI REMOTE S, V12, P2321, DOI 10.1109/LGRS.2015.2475299
   Zuo Z, 2015, 2015 IEEE C COMP VIS, P18, DOI [10.1109/CVPRW.2015.7301268, DOI 10.1109/CVPRW.2015.7301268]
NR 62
TC 5
Z9 5
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1629
EP 1649
DI 10.1007/s00371-022-02433-1
EA MAR 2022
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000770223500001
DA 2024-07-18
ER

PT J
AU Attaoui, MO
   Dif, N
   Azzag, H
   Lebbah, M
AF Attaoui, Mohammed Oualid
   Dif, Nassima
   Azzag, Hanene
   Lebbah, Mustapha
TI Regions of interest selection in histopathological images using subspace
   and multi-objective stream clustering
SO VISUAL COMPUTER
LA English
DT Article
DE Regions of interests selection; Histopathological datasets; Subspace
   clustering; Multi-objective clustering; Data stream
ID EVOLUTIONARY APPROACH; ALGORITHM
AB The advances of deep learning in histopathology show the ability to assist pathologists in reducing workload and avoiding subjective decisions. Such algorithms lead to a more reliable diagnosis because they give computer-based second opinions to the clinician. However, in histopathology cancer image analysis, pathologists mostly diagnose the pathology as positive if a small part of it is considered cancer tissue. These small parts are called regions of interest or patches. Finding the relevant patches is crucial as it can save computation time and memory. Also, deep learning systems can receive only small inputs, and these patches represent the best input. This paper proposes a new clustering algorithm for the patch selection based on subspace clustering. This technique discovers clusters embedded in multiple, overlapping subspaces of high-dimensional data. The proposed algorithm manages to find the data's best partitioning and the images' relevant patches.
C1 [Attaoui, Mohammed Oualid; Azzag, Hanene; Lebbah, Mustapha] Univ Sorbonne Paris Nord, LIPN UMR 7030, Av 99 J-B Clement, F-93430 Villetaneuse, France.
   [Dif, Nassima] Univ Djillali Liabes Sidi Bel Abbes, EEDIS Lab, Sidi Bel Abbes, Algeria.
C3 University Djillali Liabes Sidi Bel Abbes
RP Attaoui, MO (corresponding author), Univ Sorbonne Paris Nord, LIPN UMR 7030, Av 99 J-B Clement, F-93430 Villetaneuse, France.
EM attaoui@lipn.univ-paris13.fr; nassima.dif@univ-sba.dz;
   azzag@lipn.univ-paris13.fr; mustapha.lebbah@lipn.univ-paris13.fr
OI Lebbah, Mustapha/0000-0001-7245-6371; ATTAOUI, Mohammed
   Oualid/0000-0003-3117-9018
CR Aggarwal C.C., 2003, P 2003 VLDB C, P81, DOI [DOI 10.1016/B978-012722442-8/50016-1, 10.1016/B978-, DOI 10.1016/B978]
   Aresta G, 2019, MED IMAGE ANAL, V56, P122, DOI 10.1016/j.media.2019.05.010
   Attaoui M.O., 2020, NEURAL COMPUT APPL, V33, P1
   Benhammou Y., P INT C LEARN OPT AL, P1
   Cao F, 2006, SIAM PROC S, P328, DOI 10.1137/1.9781611972764.29
   Chan EY, 2004, PATTERN RECOGN, V37, P943, DOI 10.1016/j.patcog.2003.11.003
   Chen L, 2020, SCI CHINA TECHNOL SC, V63, P1302, DOI 10.1007/s11431-020-1587-y
   DAVIES DL, 1979, IEEE T PATTERN ANAL, V1, P224, DOI 10.1109/TPAMI.1979.4766909
   de Andrade Silva J., 2011, Proceedings of the 2011 Tenth International Conference on Machine Learning and Applications (ICMLA 2011), P14, DOI 10.1109/ICMLA.2011.67
   Deng ZH, 2016, INFORM SCIENCES, V348, P84, DOI 10.1016/j.ins.2016.01.101
   Domeniconi C., 2009, ACM Transactions on Knowledge Discovery from Data, V2, P1, DOI [10.1145/1460797.1460800, DOI 10.1145/1460797.1460800]
   Dutta D, 2019, EXPERT SYST APPL, V137, P357, DOI 10.1016/j.eswa.2019.06.056
   Dyer KB, 2014, IEEE T NEUR NET LEAR, V25, P12, DOI 10.1109/TNNLS.2013.2277712
   Ferreira CA, 2018, LECT NOTES COMPUT SC, V10882, P763, DOI 10.1007/978-3-319-93000-8_86
   Gancarski P, 2008, PATTERN RECOGN, V41, P983, DOI 10.1016/j.patcog.2007.07.008
   Garza-Fabre M, 2018, IEEE T EVOLUT COMPUT, V22, P515, DOI 10.1109/TEVC.2017.2726341
   Gong CC, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0188815
   Hahsler M, 2016, IEEE T KNOWL DATA EN, V28, P1449, DOI 10.1109/TKDE.2016.2522412
   Handl J, 2007, IEEE T EVOLUT COMPUT, V11, P56, DOI 10.1109/TEVC.2006.877146
   Huang JZX, 2005, IEEE T PATTERN ANAL, V27, P657, DOI 10.1109/TPAMI.2005.95
   HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075
   Inkaya T, 2015, APPL SOFT COMPUT, V28, P301, DOI 10.1016/j.asoc.2014.11.060
   Janowczyk Andrew, 2016, J Pathol Inform, V7, P29, DOI 10.4103/2153-3539.186902
   Jimenez-del-Toro O, 2017, ELS MIC SOC BOOK SER, P281, DOI 10.1016/B978-0-12-812133-7.00010-7
   Kaymak S, 2017, PROCEDIA COMPUT SCI, V120, P126, DOI 10.1016/j.procs.2017.11.219
   Keller A, 2000, INT J UNCERTAIN FUZZ, V8, P735, DOI 10.1016/S0218-4885(00)00053-8
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuo RJ, 2020, SOFT COMPUT, V24, P11545, DOI 10.1007/s00500-019-04620-0
   Law MHC, 2004, PROC CVPR IEEE, P424
   Luo JJ, 2016, PATTERN RECOGN, V60, P37, DOI 10.1016/j.patcog.2016.05.004
   MacQueen J, 1967, SOME METHODS CLASSIF
   Maguolo G., 2019, ARXIV PREPRINT ARXIV
   McIntyre A, 2006, GECCO 2006: GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, VOL 1 AND 2, P863
   Mescher A.L., 2018, Junqueira's Basic Histology: Text and Atlas, V15e
   Modha DS, 2003, MACH LEARN, V52, P217, DOI 10.1023/A:1024016609528
   Nanni Loris, 2019, Innovation in Medicine and Healthcare Systems, and Multimedia. Proceedings of KES-InMed-19 and KES-IIMSS-19 Conferences. Smart Innovation, Systems and Technologies (SIST 145), P61, DOI 10.1007/978-981-13-8566-7_6
   Nanni L, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20061626
   Nawaz W, 2018, LECT NOTES COMPUT SC, V10882, P869, DOI 10.1007/978-3-319-93000-8_99
   Niu B, 2007, APPL MATH COMPUT, V185, P1050, DOI 10.1016/j.amc.2006.07.026
   Parsons L., 2004, ACM SIGKDD EXPLOR NE, V6, P90, DOI DOI 10.1145/1007730.1007731
   Saini N, 2019, APPL INTELL, V49, P1803, DOI 10.1007/s10489-018-1350-8
   Shamir L, 2008, MED BIOL ENG COMPUT, V46, P943, DOI 10.1007/s11517-008-0380-5
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Souza V.M.A., P SIAM INT C DAT MIN, P873
   Spanhol FA, 2016, IEEE T BIO-MED ENG, V63, P1455, DOI 10.1109/TBME.2015.2496264
   Spanhol FA, 2016, IEEE IJCNN, P2560, DOI 10.1109/IJCNN.2016.7727519
   Strehl A, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P93, DOI 10.1162/153244303321897735
   Sun JM, 2017, 2017 IEEE CONFERENCE ON BIG DATA AND ANALYTICS (ICBDA), P43, DOI 10.1109/ICBDAA.2017.8284105
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tu L, 2009, ACM T KNOWL DISCOV D, V3, DOI 10.1145/1552303.1552305
   Wang R, 2018, INFORM SCIENCES, V450, P128, DOI 10.1016/j.ins.2018.03.047
   Xia H, 2013, PATTERN RECOGN, V46, P2562, DOI 10.1016/j.patcog.2013.02.005
   Xu J, 2017, ADV COMPUT VIS PATT, P73, DOI 10.1007/978-3-319-42999-1_6
   Zhi WM, 2017, LECT NOTES COMPUT SC, V10637, P669, DOI 10.1007/978-3-319-70093-9_71
   Zhu XL, 2017, PROC CVPR IEEE, P6855, DOI 10.1109/CVPR.2017.725
NR 55
TC 0
Z9 0
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1683
EP 1701
DI 10.1007/s00371-022-02436-y
EA MAR 2022
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000766099400001
DA 2024-07-18
ER

PT J
AU Lu, RT
   Gao, F
   Yang, XG
   Fan, JW
   Li, DL
AF Lu, Ruitao
   Gao, Fan
   Yang, Xiaogang
   Fan, Jiwei
   Li, Dalei
TI A novel infrared and visible image fusion method based on multi-level
   saliency integration
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT ACIAT202
CY 2022
CL ELECTR NETWORK
DE Image fusion; Infrared feature extraction; Saliency integration
ID ADAPTIVE FUSION; TRANSFORM; PERFORMANCE; NETWORK; LIGHT
AB Infrared and visible image fusion makes full use of abundant detailed information of multi-sensor to help people better understand various scenarios. In this paper, a novel method of infrared and visible image fusion based on multi-level saliency integration is proposed. First, the background image of each sub-image is reconstructed by the means of Bessel interpolation after the quadtree decomposition on the infrared image, and the difference saliency is extracted by the difference between the source infrared image and the estimated background. Then, the sparse saliency is calculated from the infrared image using the sparsity of salient objects and the low rank of background. Third, the multi-scale saliency is obtained by Laplacian transformation between the visible image and infrared image to preserve the detailed information. At last, the fusion strategy based on the adaptive weighting coefficient is present to get more natural fusion results. Experimental results on 20 pairs of source images demonstrate that the proposed method outperforms the other state-of-the-art methods in terms of subjective vision and objective evaluation.
C1 [Lu, Ruitao; Gao, Fan; Yang, Xiaogang; Fan, Jiwei] Rocket Force Univ Engn, Xian 710000, Peoples R China.
   [Lu, Ruitao; Li, Dalei] Sci & Technol Electroopt Control Lab, Luoyang 471009, Peoples R China.
C3 Rocket Force University of Engineering
RP Gao, F (corresponding author), Rocket Force Univ Engn, Xian 710000, Peoples R China.
EM lrt19880220@163.com; gyraky@163.com; doctoryxg@163.com;
   fjw19900619@163.com; nwpuhust@163.com
OI Lu, Ruitao/0000-0002-7527-4298
FU National Natural Science Foundation of China [61806209]; Natural Science
   Foundation of Shaanxi Province [2020JQ-490]; Aeronautical Science Fund
   [201851U8012]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61806209, in part by the Natural Science
   Foundation of Shaanxi Province under Grant 2020JQ-490, and in part by
   the Aeronautical Science Fund under Grant 201851U8012.
CR [Anonymous], 2016, PROC 24 ACM INT C MU, DOI [DOI 10.1145/2964284.2984065, 10.1145/2964284.2984065]
   Bai XZ, 2015, INFORM FUSION, V22, P105, DOI 10.1016/j.inffus.2014.05.003
   Bavirisetti DP, 2016, INFRARED PHYS TECHN, V76, P52, DOI 10.1016/j.infrared.2016.01.009
   Chen J, 2020, INFORM SCIENCES, V508, P64, DOI 10.1016/j.ins.2019.08.066
   Dai LY, 2021, INFRARED PHYS TECHN, V114, DOI 10.1016/j.infrared.2020.103621
   Duan CW, 2021, OPTIK, V228, DOI 10.1016/j.ijleo.2020.165775
   Duan PH, 2021, IEEE T GEOSCI REMOTE, V59, P7726, DOI 10.1109/TGRS.2020.3031928
   Feng YF, 2020, MULTIMED TOOLS APPL, V79, P15001, DOI 10.1007/s11042-019-08579-w
   Gao C, 2021, IEEE ACCESS, V9, P91462, DOI 10.1109/ACCESS.2021.3090436
   Gao C, 2021, IEEE ACCESS, V9, P91883, DOI 10.1109/ACCESS.2021.3086096
   Hu J, 2022, IEEE T CIRC SYST VID, V32, P1089, DOI 10.1109/TCSVT.2021.3074259
   Jian LH, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3022438
   Jin X, 2018, INFRARED PHYS TECHN, V88, P1, DOI 10.1016/j.infrared.2017.10.004
   Kamboj A, 2022, VISUAL COMPUT, V38, P2383, DOI 10.1007/s00371-021-02119-0
   Kong WW, 2014, INFRARED PHYS TECHN, V67, P161, DOI 10.1016/j.infrared.2014.07.019
   Li H, 2019, INFRARED PHYS TECHN, V102, DOI 10.1016/j.infrared.2019.103039
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li J, 2020, INFORM SCIENCES, V529, P28, DOI 10.1016/j.ins.2020.04.035
   Li QL, 2020, J INTELL TRANSPORT S, V24, P254, DOI 10.1080/15472450.2019.1643725
   Liao X, 2022, IEEE T DEPEND SECURE, V19, P897, DOI 10.1109/TDSC.2020.3004708
   Liao X, 2020, IEEE J-STSP, V14, P955, DOI 10.1109/JSTSP.2020.3002391
   Liao X, 2020, IEEE T CIRC SYST VID, V30, P685, DOI 10.1109/TCSVT.2019.2896270
   Liu FQ, 2021, J INTELL FUZZY SYST, V40, P10603, DOI 10.3233/JIFS-201494
   Lu RT, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2020.3026546
   Lu RT, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2020.3038784
   Ma JY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3038013
   Ma JY, 2020, IEEE T IMAGE PROCESS, V29, P4980, DOI 10.1109/TIP.2020.2977573
   Ma JY, 2020, INFORM FUSION, V54, P85, DOI 10.1016/j.inffus.2019.07.005
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JL, 2017, INFRARED PHYS TECHN, V82, P8, DOI 10.1016/j.infrared.2017.02.005
   Madheswari K, 2017, QUANT INFR THERM J, V14, P24, DOI 10.1080/17686733.2016.1229328
   Meng FJ, 2017, COMPUT ELECTR ENG, V62, P375, DOI 10.1016/j.compeleceng.2016.09.019
   Meng FJ, 2016, NEUROCOMPUTING, V177, P1, DOI 10.1016/j.neucom.2015.10.080
   Mo Y, 2021, INFORM FUSION, V75, P41, DOI 10.1016/j.inffus.2021.04.005
   Rajakumar C, 2021, CIRC SYST SIGNAL PR, V40, P6146, DOI 10.1007/s00034-021-01757-y
   Ren L, 2021, SIGNAL PROCESS, V186, DOI 10.1016/j.sigpro.2021.108108
   Ren L, 2021, INFRARED PHYS TECHN, V114, DOI 10.1016/j.infrared.2021.103662
   Song ZZ, 2021, IEEE ACCESS, V9, P29686, DOI 10.1109/ACCESS.2021.3058526
   Vanmali AV, 2017, SADHANA-ACAD P ENG S, V42, P1063, DOI 10.1007/s12046-017-0673-1
   Wang QQ, 2021, IEEE T IMAGE PROCESS, V30, P1771, DOI 10.1109/TIP.2020.3048626
   Wang SY, 2020, IET IMAGE PROCESS, V14, P3188, DOI 10.1049/iet-ipr.2019.1319
   Xi JX, 2020, IEEE T CIRCUITS-I, V67, P2442, DOI 10.1109/TCSI.2020.2975383
   Xi JX, 2020, IEEE T CYBERNETICS, V50, P4585, DOI 10.1109/TCYB.2019.2963172
   Xu DD, 2020, IEEE ACCESS, V8, P206445, DOI 10.1109/ACCESS.2020.3037770
   Yan L, 2020, IEEE ACCESS, V8, P59976, DOI 10.1109/ACCESS.2020.2982712
   Yang Y, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3011766
   Yang Y, 2021, SIGNAL IMAGE VIDEO P, V15, P1221, DOI 10.1007/s11760-021-01852-2
   Yu XL, 2014, OPTIK, V125, P6010, DOI 10.1016/j.ijleo.2014.07.059
   Zhang L, 2008, IEEE T PATTERN ANAL, V30, P1786, DOI 10.1109/TPAMI.2007.70830
   Zhang Y, 2017, INFRARED PHYS TECHN, V83, P227, DOI 10.1016/j.infrared.2017.05.007
   Zhao C, 2019, INFRARED PHYS TECHN, V102, DOI 10.1016/j.infrared.2019.102976
   Zhao ZX, 2020, SIGNAL PROCESS, V177, DOI 10.1016/j.sigpro.2020.107734
   Zhu ZQ, 2018, INFORM SCIENCES, V432, P516, DOI 10.1016/j.ins.2017.09.010
NR 53
TC 3
Z9 3
U1 4
U2 49
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2023
VL 39
IS 6
SI SI
BP 2321
EP 2335
DI 10.1007/s00371-022-02438-w
EA MAR 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA J1YY4
UT WOS:000765191600002
DA 2024-07-18
ER

PT J
AU Lebedeva, I
   Ying, FL
   Gu, Y
AF Lebedeva, Irina
   Ying, Fangli
   Gu, Yi
TI Personalized facial beauty assessment: a meta-learning approach
SO VISUAL COMPUTER
LA English
DT Article
DE Meta-learning; Facial beauty prediction; Deep learning
ID ATTRACTIVENESS; CLASSIFICATION; BENCHMARK
AB Automatic facial beauty assessment has recently attracted a growing interest and achieved impressive results. However, despite the obvious subjectivity of beauty perception, most studies are addressed to predict generic or universal beauty and only few works investigate an individual's preferences in facial attractiveness. Unlike universal beauty assessment, an effective personalized method is required to produce a reasonable accuracy on a small amount of training images as the number of annotated samples from an individual is limited in real-world applications. In this work, a novel personalized facial beauty assessment approach based on meta-learning is introduced. First of all, beauty preferences shared by an extensive number of individuals are learnt during meta-training. Then, the model is adapted to a new individual with a few rated image samples in the meta-testing phase. The experiments are conducted on a facial beauty dataset that includes faces of various ethnic, gender, age groups and rated by hundreds of volunteers with different social and cultural backgrounds. The results demonstrate that the proposed method is capable of effectively learning personal beauty preferences from a limited number of annotated images and outperforms the facial beauty prediction state-of-the-art on quantitative comparisons.
C1 [Lebedeva, Irina] East China Univ Sci & Technol, Sch Informat Sci & Engn, Shanghai 200237, Peoples R China.
   [Ying, Fangli; Gu, Yi] East China Univ Sci & Technol, Dept Comp Sci & Engn, Shanghai 200237, Peoples R China.
   [Gu, Yi] Business Intelligence & Visualizat Res Ctr, Natl Engn Lab Big Data Distribut & Exchange Techn, Shanghai 200436, Peoples R China.
   [Gu, Yi] Shanghai Engn Res Ctr Big Data & Internet Audienc, Shanghai 200072, Peoples R China.
   [Ying, Fangli] East China Univ Sci & Technol, State Key Lab Bioreactor Engn, Shanghai 200237, Peoples R China.
C3 East China University of Science & Technology; East China University of
   Science & Technology; East China University of Science & Technology
RP Ying, FL; Gu, Y (corresponding author), East China Univ Sci & Technol, Dept Comp Sci & Engn, Shanghai 200237, Peoples R China.; Gu, Y (corresponding author), Business Intelligence & Visualizat Res Ctr, Natl Engn Lab Big Data Distribut & Exchange Techn, Shanghai 200436, Peoples R China.; Gu, Y (corresponding author), Shanghai Engn Res Ctr Big Data & Internet Audienc, Shanghai 200072, Peoples R China.; Ying, FL (corresponding author), East China Univ Sci & Technol, State Key Lab Bioreactor Engn, Shanghai 200237, Peoples R China.
EM irina@mail.ecust.edu.cn; yfangli@ecust.edu.cn; guoyi@ecust.edu.cn
FU National Key Research and Development Program of China [2020YFA0907800,
   2018YFC0807105]; Science and Technology Committee of Shanghai
   Municipality (STCSM) [17DZ1101003, 18511106602, 18DZ2252300]; Open
   Funding Project of the State Key Laboratory of Bioreactor Engineering,
   East China University of Science and Technology, Shanghai, China
FX This research is financially supported by The National Key Research and
   Development Program of China (grant number 2018YFC0807105) and Science
   and Technology Committee of Shanghai Municipality (STCSM) (under grant
   numbers 17DZ1101003, 18511106602 and 18DZ2252300). Partially Supported
   by Open Funding Project of the State Key Laboratory of Bioreactor
   Engineering, East China University of Science and Technology, Shanghai,
   China; Also Supported by National Key Research and Development Program
   of China (No. 2020YFA0907800).
CR Aarabi P, 2002, IEEE SYS MAN CYBERN, P2644
   Agthe M, 2016, EVOL PSYCHOL-US, V14, DOI 10.1177/1474704916653968
   Altwaijry H, 2013, IEEE WORK APP COMP, P117, DOI 10.1109/WACV.2013.6475008
   [Anonymous], 2016, Computer models for facial beauty analysis
   Cao KR, 2020, INFORMATION, V11, DOI 10.3390/info11080391
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chen FM, 2010, LECT NOTES COMPUT SC, V6165, P21, DOI 10.1007/978-3-642-13923-9_3
   Cui CR, 2020, INFORM SCIENCES, V512, P780, DOI 10.1016/j.ins.2019.10.011
   Deng J., 2009, IEEE C COMP VIS PATT
   Dornaika F, 2020, ENG APPL ARTIF INTEL, V95, DOI 10.1016/j.engappai.2020.103831
   Dornaika F, 2020, MULTIMED TOOLS APPL, V79, P3005, DOI 10.1007/s11042-019-08206-8
   Dornaika F, 2020, EXPERT SYST APPL, V142, DOI 10.1016/j.eswa.2019.112990
   Dornaika F, 2019, J ELECTRON IMAGING, V28, DOI 10.1117/1.JEI.28.4.043013
   Eisenthal Y, 2006, NEURAL COMPUT, V18, P119, DOI 10.1162/089976606774841602
   Fan JT, 2012, PATTERN RECOGN, V45, P2326, DOI 10.1016/j.patcog.2011.11.024
   Gan JY, 2014, NEUROCOMPUTING, V144, P295, DOI 10.1016/j.neucom.2014.05.028
   Gessert N., 2020, ARXIV PREPRINT ARXIV
   Gray D, 2010, LECT NOTES COMPUT SC, V6316, P434, DOI 10.1007/978-3-642-15567-3_32
   Gunes H, 2006, INT J HUM-COMPUT ST, V64, P1184, DOI 10.1016/j.ijhcs.2006.07.004
   Jekel C.F., 2018, ARXIV PREPRINT ARXIV
   Kagian A, 2007, ADV NEURAL INFORM PR, P649
   Labati RD., 2020, INT J HIGH PERFORM S, V9, P97, DOI [10.1504/IJHPSA.2020.111559, DOI 10.1504/IJHPSA.2020.111559]
   Lebedeva I., 2021, Journal of Physics: Conference Series, V1922, DOI 10.1088/1742-6596/1922/1/012004
   Lebedeva I, 2021, PROC SPIE, V11878, DOI 10.1117/12.2599699
   Lee H, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1073, DOI 10.1145/3292500.3330859
   Li JS, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P793, DOI 10.1145/2733373.2807966
   Li LD, 2020, IEEE T IMAGE PROCESS, V29, P3898, DOI 10.1109/TIP.2020.2968285
   Liang LY, 2018, INT C PATT RECOG, P1598, DOI 10.1109/ICPR.2018.8546038
   Liao QQ, 2012, IEEE T VIS COMPUT GR, V18, P1704, DOI 10.1109/TVCG.2012.26
   Lin LJ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P847
   Liu LQ, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2659234
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lu YF, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1563, DOI 10.1145/3394486.3403207
   Mao HY, 2009, IEEE SYS MAN CYBERN, P4842, DOI 10.1109/ICSMC.2009.5346057
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Nguyen Tam V., 2012, PROC ACM INT C MULTI, P239
   O'Donovan P., 2014, Proceedings of the Workshop on Computational Aesthetics, P33
   Ren J, 2017, IEEE I CONF COMP VIS, P638, DOI 10.1109/ICCV.2017.76
   Rodriguez-Pardo Carlos, 2019, Pattern Recognition and Image Analysis. 9th Iberian Conference, IbPRIA 2019. Proceedings. Lecture Notes in Computer Science (LNCS 11867), P508, DOI 10.1007/978-3-030-31332-6_44
   Rothe R, 2016, PROC CVPR IEEE, P5553, DOI 10.1109/CVPR.2016.599
   Shi SJ, 2019, INT CONF ACOUST SPEE, P4045, DOI [10.1109/icassp.2019.8683112, 10.1109/ICASSP.2019.8683112]
   Sutic Davor, 2010, 2010 33rd International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO), P1339
   Swaminathan A, 2020, PROCEDIA COMPUT SCI, V167, P2634, DOI 10.1016/j.procs.2020.03.342
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tong S, 2021, VISION RES, V178, P93, DOI 10.1016/j.visres.2020.10.001
   Vahdati Elham, 2020, Pattern Recognition and Artificial Intelligence. International Conference, ICPRAI 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12068), P441, DOI 10.1007/978-3-030-59830-3_38
   Vahdati E, 2019, LECT NOTES COMPUT SC, V11663, P255, DOI 10.1007/978-3-030-27272-2_22
   Whitehill J, 2008, IEEE INT CONF AUTOMA, P17
   Xiao QJ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1800, DOI 10.1145/3394171.3413873
   Xiao QJ, 2021, COMPUT GRAPH-UK, V98, P11, DOI 10.1016/j.cag.2021.04.023
   Xie D., 2015, ARXIV PREPRINT ARXIV
   Xie DR, 2015, IEEE SYS MAN CYBERN, P1821, DOI 10.1109/SMC.2015.319
   Xu J, 2017, INT CONF ACOUST SPEE, P1657, DOI 10.1109/ICASSP.2017.7952438
   Xu L., 2018, Synthesizing Tabular Data using Generative Adversarial Networks
   Xu L., 2020, ARXIV PREPRINT ARXIV
   Xu L, 2019, IEEE IMAGE PROC, P3861, DOI [10.1109/icip.2019.8803614, 10.1109/ICIP.2019.8803614]
   Yan HB, 2014, NEUROCOMPUTING, V129, P334, DOI 10.1016/j.neucom.2013.09.025
   Yikui Zhai, 2020, Recent Trends in Decision Science and Management. Proceedings of ICDSM 2019. Advances in Intelligent Systems and Computing (AISC 1142), P149, DOI 10.1007/978-981-15-3588-8_18
   Yili Chen, 2010, 2010 International Conference on Audio, Language and Image Processing (ICALIP), P1382, DOI 10.1109/ICALIP.2010.5685007
   Zhang D, 2011, PATTERN RECOGN, V44, P940, DOI 10.1016/j.patcog.2010.10.013
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhao HM, 2018, IEEE COMPUT GRAPH, V38, P77, DOI 10.1109/MCG.2018.011461529
NR 62
TC 4
Z9 4
U1 6
U2 27
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 1095
EP 1107
DI 10.1007/s00371-021-02387-w
EA FEB 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000754122600001
DA 2024-07-18
ER

PT J
AU Loddo, A
   Di Ruberto, C
   Vale, AMPG
   Ucchesu, M
   Soares, JM
   Bacchetta, G
AF Loddo, A.
   Di Ruberto, C.
   Vale, A. M. P. G.
   Ucchesu, M.
   Soares, J. M.
   Bacchetta, G.
TI An effective and friendly tool for seed image analysis
SO VISUAL COMPUTER
LA English
DT Article
DE Seeds image processing; Feature extraction; Seeds image classification;
   Machine learning; Deep learning
ID MORPHO-COLOURIMETRIC ANALYSIS; PRUNUS-DOMESTICA L.; COMPUTER VISION;
   VITIS-VINIFERA; CULTIVAR; COMPLEMENTARY; RECOGNITION; DIVERSITY
AB Image analysis is an essential field for several topics in the life sciences, such as biology or botany. In particular, the analysis of seeds (e.g. fossil research) can provide important information on their evolution, the history of agriculture, plant domestication, and diets knowledge in ancient times. This work presents software that performs image analysis for feature extraction and classification from images containing seeds through a novel and unique framework. In detail, we propose two plugins for ImageJ, one able to extract morphological, texture, and colour features from seed images, and another to classify seeds using the extracted features. The experimental results demonstrated the correctness and validity of both the extracted features and the classification predictions on two public seeds datasets, showing that combining the handcrafted features with the Random Forest classifier can reach outstanding performance on both datasets. The proposed tool is easily extendable to other fields of image analysis.
C1 [Loddo, A.; Di Ruberto, C.] Univ Cagliari, Dept Math & Comp Sci, Via Osped 72, I-09124 Cagliari, Italy.
   [Vale, A. M. P. G.] Univ Fed Rio Grande do Norte UFRN, Escola Agr Jundiai EAJ, Rodovia RN 160,Km 03, BR-59280000 Macaiba, RN, Brazil.
   [Ucchesu, M.; Bacchetta, G.] Univ Cagliari, Dipartimento Sci Vita & Ambiente DiSVA, Ctr Conservaz Biodiversita CCB, Viale S Ignazio da Laconi 13, I-09123 Cagliari, Italy.
   [Bacchetta, G.] Univ Cagliari, Hortus Bot Karalitanus HBK, Viale S Lgnazio da Laconi 11, I-09123 Cagliari, Italy.
   [Soares, J. M.] Univ Fed Rio Grande do Norte UFRN, Rodovia RN 160,Km 03, BR-59280000 Macaiba, RN, Brazil.
C3 University of Cagliari; Universidade Federal do Rio Grande do Norte;
   University of Cagliari; University of Cagliari; Universidade Federal do
   Rio Grande do Norte
RP Loddo, A (corresponding author), Univ Cagliari, Dept Math & Comp Sci, Via Osped 72, I-09124 Cagliari, Italy.
EM andrea.loddo@unica.it; dirubert@unica.it
RI Bacchetta, Gianluigi/AAD-5329-2020; LODDO, ANDREA/AAI-6506-2020; Di
   Ruberto, Cecilia/G-6915-2014
OI LODDO, ANDREA/0000-0002-6571-3816; Di Ruberto,
   Cecilia/0000-0003-4641-0307; BACCHETTA, GIANLUIGI/0000-0002-1714-3978
CR Ahmad N, 2022, VISUAL COMPUT, V38, P2751, DOI 10.1007/s00371-021-02153-y
   Alejo R., 2013, Pattern Recognition. 5th Mexican Conference, MCPR 2013. Proceedings: LNCS 7914, P335, DOI 10.1007/978-3-642-38989-4_34
   Amara J., 2017, DATENBANKSYSTEME BUS
   Bacchetta G, 2011, FLORA, V206, P468, DOI 10.1016/j.flora.2011.01.006
   Bianco M. Lo, 2015, Australian Journal of Crop Science, V9, P1022
   Bohl E, 2015, VISUAL COMPUT, V31, P819, DOI 10.1007/s00371-015-1108-9
   Bouby L, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0063195
   Campanile G, 2019, IEEE ENABL TECHNOL, P162, DOI 10.1109/WETICE.2019.00042
   Di Ruberto C, 2020, COMPUT BIOL MED, V116, DOI 10.1016/j.compbiomed.2019.103530
   Di Ruberto C, 2015, LECT NOTES COMPUT SC, V9163, P3, DOI 10.1007/978-3-319-20904-3_1
   Di Ruberto C, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS (VISAPP), VOL 1, P601
   Di Ruberto C, 2009, IMAGE VISION COMPUT, V27, P1097, DOI 10.1016/j.imavis.2008.10.009
   Frigau L, 2020, PLANT BIOSYST, V154, P877, DOI 10.1080/11263504.2019.1701126
   Gad R, 2019, 2019 2ND INTERNATIONAL CONFERENCE ON COMPUTER APPLICATIONS & INFORMATION SECURITY (ICCAIS)
   Gajjar R, 2022, VISUAL COMPUT, V38, P2923, DOI 10.1007/s00371-021-02164-9
   Gonzales R.C., 2002, Digital image processing
   Gulzar Y, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12122018
   Hall D, 2015, IEEE WINT CONF APPL, P797, DOI 10.1109/WACV.2015.111
   Hall M., 2009, ACM SIGKDD Explor. Newsl, V11, P18, DOI DOI 10.1145/1656274.1656278
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Hu SJ, 2017, VISUAL COMPUT, V33, P1017, DOI 10.1007/s00371-017-1377-6
   Jing HY, 2014, NEUROCOMPUTING, V129, P114, DOI 10.1016/j.neucom.2013.02.048
   Junos MH, 2022, VISUAL COMPUT, V38, P2341, DOI 10.1007/s00371-021-02116-3
   Kamilaris A, 2018, COMPUT ELECTRON AGR, V147, P70, DOI 10.1016/j.compag.2018.02.016
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kussul N, 2017, IEEE GEOSCI REMOTE S, V14, P778, DOI 10.1109/LGRS.2017.2681128
   Landini G., 2008, Proceedings of the Second ImageJ User and Developer Conference, Luxembourg, 6-7 November, P116
   Lind R, 2012, WOODH PUB SER BIOMED, P131
   Lo Bianco M, 2017, PLANT BIOLOGY, V19, P183, DOI 10.1111/plb.12529
   Lo Bianco M, 2017, PLANT BIOLOGY, V19, P90, DOI 10.1111/plb.12481
   Loddo A, 2021, J IMAGING, V7, DOI 10.3390/jimaging7090171
   Loddo A, 2021, COMPUT ELECTRON AGR, V187, DOI 10.1016/j.compag.2021.106269
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419
   Mortensen A. K., 2016, CIGR-AgEng Conference, 26-29 June 2016, Aarhus, Denmark. Abstracts and Full papers, P1
   Orrù M, 2013, VEG HIST ARCHAEOBOT, V22, P231, DOI 10.1007/s00334-012-0362-2
   Orrù M, 2012, CR BIOL, V335, P602, DOI 10.1016/j.crvi.2012.08.002
   Peng JL, 2015, MULTIMED TOOLS APPL, V74, P4469, DOI 10.1007/s11042-013-1817-x
   Piras F, 2016, COMPUT ELECTRON AGR, V122, P86, DOI 10.1016/j.compag.2016.01.021
   Przybylo J, 2019, COMPUT ELECTRON AGR, V156, P490, DOI 10.1016/j.compag.2018.12.001
   Putzu L, 2016, LECT NOTES COMPUT SC, V10016, P570, DOI 10.1007/978-3-319-48680-2_50
   Rebetez J., 2016, P ESANN 2016 EUR S A, P6
   Remeseiro B, 2021, VISUAL COMPUT, V37, P1247, DOI 10.1007/s00371-020-01863-z
   Sabato D, 2015, SCI HORTIC-AMSTERDAM, V192, P441, DOI 10.1016/j.scienta.2015.06.006
   Sarigu M, 2017, COMPUT ELECTRON AGR, V136, P25, DOI 10.1016/j.compag.2017.02.009
   Sau S, 2019, COMPUT ELECTRON AGR, V162, P373, DOI 10.1016/j.compag.2019.04.027
   Sau S, 2018, COMPUT ELECTRON AGR, V151, P118, DOI 10.1016/j.compag.2018.06.002
   Sladojevic S, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/3289801
   Terral JF, 2010, ANN BOT-LONDON, V105, P443, DOI 10.1093/aob/mcp298
   Ucchesu M, 2017, VEG HIST ARCHAEOBOT, V26, P539, DOI 10.1007/s00334-017-0622-2
   Ucchesu M, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0149814
   Ucchesu M, 2015, VEG HIST ARCHAEOBOT, V24, P587, DOI 10.1007/s00334-014-0512-9
   Vale A.M.P.G., 2020, ARXIV201206414
   Zhang J, 2019, VISUAL COMPUT, V35, P1181, DOI 10.1007/s00371-019-01667-w
   Zhu HF, 2021, COMPUT ELECTRON AGR, V186, DOI 10.1016/j.compag.2021.106185
NR 54
TC 10
Z9 10
U1 3
U2 33
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 335
EP 352
DI 10.1007/s00371-021-02333-w
EA JAN 2022
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000742981500001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Huang, XJ
AF Huang, Xiaojie
TI Moving object detection in low-luminance images
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Moving object detection; Low luminance
ID NETWORK; VISION
AB Moving object detection in low-luminance Images is one of the most fundamental and difficult issues in machine vision. Therefore, in this paper, deep self-adaptive network (DSA-Net) is proposed to effectively detect moving objects in low-luminance scenes. Particularly, (1) three mechanisms are developed in this joint learning framework: graph-based unsupervised feature selection, feature representations ranking, and multiple-way feature interaction. (2) Both anti-occlusion and multi-object handling module are explored simultaneously in the unified DSA-Net model. (3) A weakly fine-tuning strategy is presented, including the easiness and group curriculum term. It leverages helpful prior-knowledge to guide the learner to select confident training samples. The experimental results show that DSA-Net outperforms the state-of-the-art methods.
C1 [Huang, Xiaojie] Yichun Univ, Yichun 336000, Jiangxi, Peoples R China.
C3 Yichun University
RP Huang, XJ (corresponding author), Yichun Univ, Yichun 336000, Jiangxi, Peoples R China.
EM wfh_jt@163.com
RI Huang, Xiaojie/U-6572-2018
OI Huang, Xiaojie/0000-0001-9499-8294
CR Ali H, 2021, VISUAL COMPUT, V37, P939, DOI 10.1007/s00371-020-01845-1
   Arad B, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19061390
   Arora H., 2019, 2019 3 INT C EL COMM, P1
   Beaulieu M, 2018, INT GEOSCI REMOTE SE, P2611, DOI 10.1109/IGARSS.2018.8517655
   Castellani A., 2020, IEEE Transactions on Industrial Informatics, P1
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Choe J, 2021, IEEE T PATTERN ANAL, V43, P4256, DOI 10.1109/TPAMI.2020.2999099
   Cui LS, 2022, IEEE T CYBERNETICS, V52, P2300, DOI 10.1109/TCYB.2020.3004636
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
   Deguerre B, 2020, ARXIV200605732
   Dirgantoro Kevin Putra, 2020, 2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC), P039, DOI 10.1109/ICAIIC48513.2020.9065231
   Dong ZP, 2020, IEEE T GEOSCI REMOTE, V58, P2104, DOI 10.1109/TGRS.2019.2953119
   Dorner S., 2020, PROC IEEE INT WORKSH, P1
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Guo J., 2021, IEEE Internet of Things Journal, P1
   Guo TD, 2021, VISUAL COMPUT, V37, P2069, DOI 10.1007/s00371-020-01964-9
   He Kaiming, 2016, P INT C COMP VIS PAT, DOI 10.1109/CVPR.2016.90
   Jo I, 2019, IEEE T HUM-MACH SYST, V49, P430, DOI 10.1109/THMS.2019.2919735
   Kim J, 2021, INT C PATT RECOG, P1604, DOI 10.1109/ICPR48806.2021.9412715
   Lalitha V. Lakshmi, 2021, Proceedings of the 2021 International Conference on Artificial Intelligence and Smart Systems (ICAIS), P1397, DOI 10.1109/ICAIS50930.2021.9395913
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu TR, 2021, IEEE T IMAGE PROCESS, V30, P754, DOI 10.1109/TIP.2020.3038371
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Mohanty Subrata Kumar, 2019, 2019 International Conference on Information Technology (ICIT), P420, DOI 10.1109/ICIT48102.2019.00080
   Perroud B, 2020, IEEE T VIS COMPUT GR, V26, P3128, DOI 10.1109/TVCG.2019.2909881
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren HY, 2020, IEEE IMAGE PROC, P2760, DOI 10.1109/ICIP40778.2020.9191126
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sasagawa Yukihiro, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P345, DOI 10.1007/978-3-030-58589-1_21
   Shakeri M, 2019, PROC CVPR IEEE, P7214, DOI 10.1109/CVPR.2019.00739
   Sultana M., 2020, IEEE Transactions on Multimedia, P1
   Wang YN, 2021, VISUAL COMPUT, V37, P1467, DOI 10.1007/s00371-020-01882-w
   Wang YZ, 2021, IEEE J-STSP, V15, P954, DOI 10.1109/JSTSP.2021.3058895
   Wei LX, 2021, VISUAL COMPUT, V37, P133, DOI 10.1007/s00371-019-01787-3
   Xiao YX, 2020, IEEE ACCESS, V8, P123075, DOI 10.1109/ACCESS.2020.3007610
   Xu N, 2019, INT CONF ACOUST SPEE, P2162, DOI 10.1109/ICASSP.2019.8682681
   Xue B, 2020, IEEE T CYBERNETICS, V50, P4256, DOI 10.1109/TCYB.2019.2933224
   Xue B, 2019, IEEE SENS J, V19, P1073, DOI 10.1109/JSEN.2018.2879669
   Xue B, 2019, IEEE T CYBERNETICS, V49, P3991, DOI 10.1109/TCYB.2018.2856821
   Yu F., ARXIV PREPRINT ARXIV
   Zhang XY, 2023, IEEE T NEUR NET LEAR, V34, P1852, DOI 10.1109/TNNLS.2019.2962815
NR 42
TC 0
Z9 0
U1 3
U2 30
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 183
EP 195
DI 10.1007/s00371-021-02320-1
EA NOV 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000716855600002
DA 2024-07-18
ER

PT J
AU Zhang, L
   Yan, LL
   Zhang, MQ
   Lu, JG
AF Zhang, Li
   Yan, Leilei
   Zhang, Mengqian
   Lu, Jingang
TI T<SUP>2</SUP>CNN: a novel method for crowd counting via two-task
   convolutional neural network
SO VISUAL COMPUTER
LA English
DT Article
DE Crowd counting; Density map; Dense degree; Convolutional neural network;
   Multi-scale network
AB This paper investigates the issue of crowd counting for crowd images. A novel method named two-task convolutional neural network ((TCNN)-C-2) is proposed to simultaneously learn two tasks for crowd counting: the tasks of dense degree classification and density map estimation. Basically, (TCNN)-C-2 contains two main modules: dense degree classification (DDC) and density map estimation. Generally, it is sufficient for crowd counting to estimate density maps of images. It is known that images may have different dense degrees and the local regions in the same image have different dense degrees. Thus, dense degree may provide the high-level prior of images and is excepted to help the estimation of density maps. To better estimate density maps of images, (TCNN)-C-2 incorporates the DDC module into the task of density map estimation. (TCNN)-C-2 takes the scale-adaptive convolutional neural network as the estimator of density maps, which adopts multi-scale layers to tackle the scale and perspective variations that exist in crowd images commonly. To verify the effectiveness and robustness of our model, we conduct extensive experiments on the Shanghaitech, UCF_CC_50 and WorldExpo'10 datasets. Experimental results indicate that (TCNN)-C-2 achieves good performance.
C1 [Zhang, Li; Yan, Leilei; Zhang, Mengqian; Lu, Jingang] Soochow Univ, Sch Comp Sci & Technol, Lab Machine Learning & Neuromorph Comp, Suzhou 215006, Jiangsu, Peoples R China.
   [Zhang, Li; Yan, Leilei; Zhang, Mengqian; Lu, Jingang] Soochow Univ, Joint Int Res, Suzhou 215006, Jiangsu, Peoples R China.
C3 Soochow University - China; Soochow University - China
RP Zhang, L (corresponding author), Soochow Univ, Sch Comp Sci & Technol, Lab Machine Learning & Neuromorph Comp, Suzhou 215006, Jiangsu, Peoples R China.; Zhang, L (corresponding author), Soochow Univ, Joint Int Res, Suzhou 215006, Jiangsu, Peoples R China.
EM zhangliml@suda.edu.cn
RI Yan, Leilei/GWC-6231-2022
OI Zhang, Li/0000-0001-7914-0679
FU Natural Science Foundation of the Jiangsu Higher Education Institutions
   of China [19KJA550002]; Six Talent Peak Project of Jiangsu Province of
   China [XYDXX-054]; Collaborative Innovation Center of Novel Software
   Technology and Industrialization
FX We would like to thank two anonymous reviewers and Editor-in-Chief
   Magnenat-Thalmann, for their valuable comments and suggestions, which
   have significantly improved this paper. This study was funded by the
   Natural Science Foundation of the Jiangsu Higher Education Institutions
   of China under Grant No. 19KJA550002, by the Six Talent Peak Project of
   Jiangsu Province of China under Grant No. XYDXX-054 and by the
   Collaborative Innovation Center of Novel Software Technology and
   Industrialization.
CR [Anonymous], 2006, COMPUTER VISION PATT, DOI 10.1109/CVPR.2006.92
   Bai S, 2020, PROC CVPR IEEE, P4593, DOI 10.1109/CVPR42600.2020.00465
   Boominathan L, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P640, DOI 10.1145/2964284.2967300
   Brostow G.J., 2006, CVPR, P594, DOI DOI 10.1109/CVPR.2006.320
   Chan AB, 2008, PROC CVPR IEEE, P1766, DOI 10.1109/cvpr.2008.4587569
   Chan AB, 2009, IEEE I CONF COMP VIS, P545, DOI 10.1109/ICCV.2009.5459191
   Chen K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.21
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dollár P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Ford M., 2017, ATLANTIC
   Idrees H, 2013, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2013.329
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jiang HY, 2019, APPL INTELL, V49, P2415, DOI 10.1007/s10489-018-1394-9
   Jiang XH, 2020, PROC CVPR IEEE, P4705, DOI 10.1109/CVPR42600.2020.00476
   Khan SD, 2021, VISUAL COMPUT, V37, P2127, DOI 10.1007/s00371-020-01974-7
   Kingma D. P., 2014, AUTOENCODING VARIATI, P3581
   Kong D., 2005, BMVC, p63.1
   Leibe B, 2005, PROC CVPR IEEE, P878
   Lempitsky V., 2010, P ADV NEUR INF PROC, V23, P1, DOI DOI 10.5555/2997189.2997337
   Li B, 2015, IEEE I CONF COMP VIS, P4175, DOI 10.1109/ICCV.2015.475
   Li H, 2020, VISUAL COMPUT, V36, P1693, DOI 10.1007/s00371-019-01769-5
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Ryan D, 2009, 2009 DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA 2009), P81, DOI 10.1109/DICTA.2009.22
   Sam DB, 2017, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2017.429
   Sindagi VA, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Stewart R, 2016, PROC CVPR IEEE, P2325, DOI 10.1109/CVPR.2016.255
   Tian Youzhi, 2018, CORR
   Topkaya IS, 2014, 2014 11TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P313, DOI 10.1109/AVSS.2014.6918687
   Tripathi G, 2019, VISUAL COMPUT, V35, P753, DOI 10.1007/s00371-018-1499-5
   Viola P, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P734
   Wang C, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1299, DOI 10.1145/2733373.28063370-12345-67-8/90/01
   Wang M, 2011, PROC CVPR IEEE
   Wu B, 2005, IEEE I CONF COMP VIS, P90
   Wu XJ, 2020, IET IMAGE PROCESS, V14, P2376, DOI 10.1049/iet-ipr.2019.1308
   Yan ZY, 2019, IEEE I CONF COMP VIS, P952, DOI 10.1109/ICCV.2019.00104
   Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684
   Zhang L, 2018, IEEE WINT CONF APPL, P1113, DOI 10.1109/WACV.2018.00127
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
NR 38
TC 4
Z9 4
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 73
EP 85
DI 10.1007/s00371-021-02313-0
EA OCT 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000708368700001
DA 2024-07-18
ER

PT J
AU Kalsotra, R
   Arora, S
AF Kalsotra, Rudrika
   Arora, Sakshi
TI Background subtraction for moving object detection: explorations of
   recent developments and challenges
SO VISUAL COMPUTER
LA English
DT Article
DE Background subtraction; Challenges; Deep neural networks; Detection;
   Foreground; Features; Moving objects
ID STABLE FOREGROUND SEGMENTATION; FULLY CONVOLUTIONAL NETWORK;
   ROBUST-SUBSPACE-TRACKING; NEURAL-NETWORK; DETECTION ALGORITHMS; MIXTURE;
   MODEL; COLOR; FEATURES; IMAGE
AB Background subtraction, although being a very well-established field, has required significant research efforts to tackle unsolved challenges and to accelerate the progress toward generalized moving object detection framework for real-time applications. The performance of subsequent steps in higher level video analytical tasks totally depends on the performance of background subtraction. Recent years have witnessed a remarkable performance of deep neural networks for background subtraction. The deep leaning has paved the way for improving background subtraction to counter the major challenges in this area. Also, the fusion of multiple features leads to the improvement of conventional background subtraction methods. In this context, we provide the comprehensive review of conventional as well as recent developments in background subtraction to analyze the success and current challenges in this field. Firstly, this paper introduces the overview of background subtraction process along with challenges and benchmark video datasets released for evaluation purpose. Then, we briefly summarize the background subtraction methods and report a comparison of the most promising state-of-the-art algorithms. Moreover, we comprehensively investigate some of the recent methods in order to find out how they have achieved their reported performances. Finally, we conclude with the shortcomings in the current developments and outline the promising research directions for background subtraction.
C1 [Kalsotra, Rudrika; Arora, Sakshi] Shri Mata Vaishno Devi Univ, Dept Comp Sci & Engn, Katra 182320, India.
C3 Shri Mata Vaishno Devi University
RP Arora, S (corresponding author), Shri Mata Vaishno Devi Univ, Dept Comp Sci & Engn, Katra 182320, India.
EM sakshi@smvdu.ac.in
RI Kalsotra, Rudrika/ISA-6624-2023
CR Abdelhedi S, 2014, 2014 6TH INTERNATIONAL CONFERENCE OF SOFT COMPUTING AND PATTERN RECOGNITION (SOCPAR), P440, DOI 10.1109/SOCPAR.2014.7008047
   Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Akula A, 2013, J OPT SOC AM A, V30, P1492, DOI 10.1364/JOSAA.30.001492
   Ramirez-Quintana JA, 2015, PATTERN RECOGN, V48, P1137, DOI 10.1016/j.patcog.2014.09.009
   Allili MS, 2007, FOURTH CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION, PROCEEDINGS, P503, DOI 10.1109/CRV.2007.7
   Alvar M, 2014, MACH VISION APPL, V25, P1133, DOI 10.1007/s00138-013-0550-9
   Ferreira JCA, 2009, IEEE INTL CONF IND I, P25, DOI 10.1109/INDIN.2009.5191765
   [Anonymous], 2006, Institution of Engineering and Technology Conference on Crime and Security, P445
   [Anonymous], 2006, NETTIES 2006
   [Anonymous], 2010, P 2010 7 IEEE INT C
   [Anonymous], 2019, DEEPPBM DEEP PROBABI
   [Anonymous], 2005, INTELL SYST NEURAL N
   [Anonymous], 2016, BME SCI STUD C
   [Anonymous], 2018, ACTA AUTOMATICASINIC
   [Anonymous], 2013, PARAMETRIC REGION BA
   [Anonymous], 2019, ILLUMINATION INVARIA
   [Anonymous], 2012, P INT C IM PROC COMP
   Azab MM, 2010, IEEE IMAGE PROC, P3453, DOI 10.1109/ICIP.2010.5653748
   Babaee M, 2018, PATTERN RECOGN, V76, P635, DOI 10.1016/j.patcog.2017.09.040
   Baf FE, 2008, IEEE IMAGE PROC, P2648, DOI 10.1109/ICIP.2008.4712338
   Bakkay MC, 2018, IEEE IMAGE PROC, P4018, DOI 10.1109/ICIP.2018.8451603
   Balcilar M, 2014, APPL MATH INFORM SCI, V8, P1755, DOI 10.12785/amis/080433
   Baltieri Davide, 2010, Proceedings 7th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2010), P165, DOI 10.1109/AVSS.2010.43
   Bansod SD, 2020, VISUAL COMPUT, V36, P609, DOI 10.1007/s00371-019-01647-0
   Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   Benezeth Y., 2014, BACKGROUND SUBTRACTI
   Bianco S, 2017, IEEE T EVOLUT COMPUT, V21, P914, DOI 10.1109/TEVC.2017.2694160
   Bloisi DD, 2015, 2015 12TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Bouwmans T., 2019, Background Subtraction in Real Applications: Challenges, Current Models and Future Directions
   Bouwmans T, 2019, NEURAL NETWORKS, V117, P8, DOI 10.1016/j.neunet.2019.04.024
   Bouwmans T, 2017, COMPUT SCI REV, V23, P1, DOI 10.1016/j.cosrev.2016.11.001
   Bouwmans T, 2018, COMPUT SCI REV, V28, P26, DOI 10.1016/j.cosrev.2018.01.004
   Bouwmans T, 2014, COMPUT SCI REV, V11-12, P31, DOI 10.1016/j.cosrev.2014.04.001
   Bouwmans T, 2017, PATTERN RECOGN LETT, V96, P3, DOI 10.1016/j.patrec.2016.12.024
   Braham M, 2016, INT CONF SYST SIGNAL, P113
   Brutzer S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1937, DOI 10.1109/CVPR.2011.5995508
   Burgos-Artizzu XP, 2012, PROC CVPR IEEE, P1322, DOI 10.1109/CVPR.2012.6247817
   Butler DE, 2005, EURASIP J APPL SIG P, V2005, P2292, DOI 10.1155/ASP.2005.2292
   Calderara Simone., 2006, P 4 ACM BIBLIO 72 IN, P211
   Camplani M, 2017, LECT NOTES COMPUT SC, V10590, P219, DOI 10.1007/978-3-319-70742-6_21
   Camplani M, 2014, MACH VISION APPL, V25, P1197, DOI 10.1007/s00138-013-0557-2
   Camplani M, 2014, J VIS COMMUN IMAGE R, V25, P122, DOI 10.1016/j.jvcir.2013.03.009
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Cevher V, 2008, LECT NOTES COMPUT SC, V5303, P155, DOI 10.1007/978-3-540-88688-4_12
   Chacon-Murguia MI, 2012, IEEE T IND ELECTRON, V59, P3286, DOI 10.1109/TIE.2011.2106093
   Chang R, 2004, ITSC 2004: 7TH INTERNATIONAL IEEE CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS, PROCEEDINGS, P971, DOI 10.1109/ITSC.2004.1399038
   Chapel MN, 2020, COMPUT SCI REV, V38, DOI 10.1016/j.cosrev.2020.100310
   Chen ATY, 2019, J REAL-TIME IMAGE PR, V16, P2319, DOI 10.1007/s11554-018-0750-7
   Chen YY, 2015, IEEE IMAGE PROC, P3946, DOI 10.1109/ICIP.2015.7351545
   Chen Y, 2017, IEEE CUST INTEGR CIR
   Chen YQ, 2020, IEEE T IND ELECTRON, V67, P601, DOI 10.1109/TIE.2019.2893824
   Chiranjeevi P, 2012, IMAGE VISION COMPUT, V30, P829, DOI 10.1016/j.imavis.2012.06.015
   Chiranjeevi P, 2014, IEEE T CYBERNETICS, V44, P870, DOI 10.1109/TCYB.2013.2274330
   Choudhury SK, 2016, IEEE ACCESS, V4, P6133, DOI 10.1109/ACCESS.2016.2608847
   Chouvardas S, 2015, IEEE T SIGNAL PROCES, V63, P5060, DOI 10.1109/TSP.2015.2449254
   Christiansen P, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16111904
   Chu YQ, 2010, PROC SPIE, V7546, DOI 10.1117/12.853445
   Chung-Ming Kuo, 2009, 2009 Fourth International Conference on Innovative Computing, Information and Control (ICICIC 2009), P480, DOI 10.1109/ICICIC.2009.82
   Cioppa A, 2020, J IMAGING, V6, DOI 10.3390/jimaging6060050
   Cuevas C, 2016, COMPUT VIS IMAGE UND, V152, P103, DOI 10.1016/j.cviu.2016.08.005
   Culibrk D, 2007, IEEE T NEURAL NETWOR, V18, P1614, DOI 10.1109/TNN.2007.896861
   Davis JW, 2007, COMPUT VIS IMAGE UND, V106, P162, DOI 10.1016/j.cviu.2006.06.010
   Davis JW, 2005, WACV 2005: SEVENTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P364
   Dey B, 2018, IEEE T CIRC SYST VID, V28, P616, DOI 10.1109/TCSVT.2016.2614984
   Dikmen M, 2008, INT C PATT RECOG, P2527
   Dou JF, 2014, OPTIK, V125, P435, DOI 10.1016/j.ijleo.2013.06.079
   Du X., 2016, INT J SMART SENS INT, V9, P4
   El Baf Fida, 2009, 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P60, DOI 10.1109/CVPR.2009.5204109
   El Baf F., 2007, 2007 14th International Workshop in Systems, Signals and Image Processing and 6th EURASIP Conference focused on Speech and Image Processing, Multimedia Communications and Services - EC-SIPMCS 2007, P385, DOI 10.1109/IWSSIP.2007.4381122
   El Baf F, 2008, LECT NOTES COMPUT SC, V5358, P772, DOI 10.1007/978-3-540-89639-5_74
   Elgammal A, 2000, EUR C COMP VIS, P751, DOI DOI 10.1007/3-540-45053-X_48
   Elguebaly T, 2013, COMPUT VIS IMAGE UND, V117, P1659, DOI 10.1016/j.cviu.2013.07.007
   Farcas Diana, 2010, Proceedings of the 2010 International Conference on Image Processing and Computer Vision (IVPCV-10), P1
   Farcas D, 2012, MACH VISION APPL, V23, P1083, DOI 10.1007/s00138-012-0421-9
   Faro A, 2011, IEEE T INTELL TRANSP, V12, P1398, DOI 10.1109/TITS.2011.2159266
   Fernandez-Sanchez EJ, 2014, MACH VISION APPL, V25, P1211, DOI 10.1007/s00138-013-0562-5
   Fernandez-Sanchez EJ, 2013, SENSORS-BASEL, V13, P8895, DOI 10.3390/s130708895
   del Postigo CG, 2015, IET INTELL TRANSP SY, V9, P835, DOI 10.1049/iet-its.2014.0090
   [甘超 Gan Chao], 2013, [中国图象图形学报, Journal of Image and Graphics], V18, P1124
   Gao DS, 2001, 2001 IEEE INTELLIGENT TRANSPORTATION SYSTEMS - PROCEEDINGS, P330, DOI 10.1109/ITSC.2001.948678
   Gao YQ, 2018, INT C PATT RECOG, P1271, DOI 10.1109/ICPR.2018.8545320
   García-González J, 2019, PATTERN RECOGN LETT, V125, P481, DOI 10.1016/j.patrec.2019.06.006
   García-González J, 2018, LECT NOTES ARTIF INT, V11160, P341, DOI 10.1007/978-3-030-00374-6_32
   Gemignani G, 2016, IEEE T IMAGE PROCESS, V25, P5239, DOI 10.1109/TIP.2016.2605004
   Giraldo J. H., 2020, GRAPHBGS BACKGROUND
   Giraldo-Zuluaga JH, 2019, VISUAL COMPUT, V35, P335, DOI 10.1007/s00371-017-1463-9
   Giraldo-Zuluaga JH, 2017, PROC INT C TOOLS ART, P53, DOI 10.1109/ICTAI.2017.00020
   Goyette N., 2012, 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), DOI 10.1109/CVPRW.2012.6238919
   Gracewell J, 2020, MULTIMED TOOLS APPL, V79, P4639, DOI 10.1007/s11042-019-7411-0
   Hadi RA, 2017, ARAB J SCI ENG, V42, P817, DOI 10.1007/s13369-016-2351-8
   Haines TSF, 2014, IEEE T PATTERN ANAL, V36, P670, DOI 10.1109/TPAMI.2013.239
   Han B, 2012, IEEE T PATTERN ANAL, V34, P1017, DOI 10.1109/TPAMI.2011.243
   Han G, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16040456
   Hati KK, 2013, IEEE SIGNAL PROC LET, V20, P759, DOI 10.1109/LSP.2013.2263800
   He J, 2012, PROC CVPR IEEE, P1568, DOI 10.1109/CVPR.2012.6247848
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Heikkilä M, 2006, IEEE T PATTERN ANAL, V28, P657, DOI 10.1109/TPAMI.2006.68
   Hofmann Martin., 2012, 2012 IEEE COMPUTER S, P38, DOI DOI 10.1109/CVPRW.2012.6238925
   Hongxun Z, 2006, LECT NOTES COMPUT SC, V4223, P887
   Hu WM, 2011, INT J COMPUT VISION, V91, P303, DOI 10.1007/s11263-010-0399-6
   Hu ZH, 2018, IEEE ACCESS, V6, P43450, DOI 10.1109/ACCESS.2018.2861223
   Huang Haozhi, 2017, [Computational Visual Media, 计算可视媒体], V3, P273
   Huang J., 2018, OPTICAL FLOW BASED R
   Huang JZ, 2011, J MACH LEARN RES, V12, P3371
   Huang JZ, 2009, IEEE I CONF COMP VIS, P64, DOI 10.1109/ICCV.2009.5459202
   Huang WH, 2017, 2017 IEEE 2ND ADVANCED INFORMATION TECHNOLOGY, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (IAEAC), P1098, DOI [10.1109/ICCTEC.2017.00240, 10.1109/IAEAC.2017.8054183]
   Huang XS, 2012, AASRI PROC, V1, P492, DOI 10.1016/j.aasri.2012.06.077
   Jalal AS, 2014, MULTIMED TOOLS APPL, V73, P779, DOI 10.1007/s11042-012-1326-3
   Jang D, 2008, LECT NOTES COMPUT SC, V5068, P222
   Javed S, 2018, IEEE T CIRC SYST VID, V28, P1315, DOI 10.1109/TCSVT.2016.2632302
   Javed S, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.4.043011
   López-Rubio FJ, 2015, COMPUT VIS IMAGE UND, V133, P30, DOI 10.1016/j.cviu.2014.12.007
   Ji ZJ, 2013, IEEE IMAGE PROC, P3441, DOI 10.1109/ICIP.2013.6738710
   Jianwei Ding, 2010, Computer Vision - ACCV 2010. 10th Asian Conference on Computer Vision. Revised Selected Papers, P82, DOI 10.1007/978-3-642-19309-5_7
   Jiménez-Hernández H, 2010, SENSORS-BASEL, V10, P6092, DOI 10.3390/s100606092
   Jodoin PM, 2017, IEEE T IMAGE PROCESS, V26, P5244, DOI 10.1109/TIP.2017.2728181
   Kalsotra R, 2019, IEEE ACCESS, V7, P59143, DOI 10.1109/ACCESS.2019.2914961
   Karman, 1990, P TIM VAR IM PROC
   Kavasidis I, 2014, MULTIMED TOOLS APPL, V70, P413, DOI 10.1007/s11042-013-1419-7
   Kim H, 2007, LECT NOTES COMPUT SC, V4843, P758
   Kim JY, 2020, IEEE ACCESS, V8, P159864, DOI 10.1109/ACCESS.2020.3020818
   Kim K, 2005, REAL-TIME IMAGING, V11, P172, DOI 10.1016/j.rti.2004.12.004
   Kim WJ, 2017, IEEE ACCESS, V5, P8369, DOI 10.1109/ACCESS.2017.2699227
   Kim W, 2012, IEEE SIGNAL PROC LET, V19, P127, DOI 10.1109/LSP.2011.2182648
   Komagal E, 2018, MULTIMED TOOLS APPL, V77, P22489, DOI 10.1007/s11042-018-6104-4
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee B., 2002, IMAGE VISION COMPUT, P315
   Lee Dar-Shyang., 2002, IAPR Workshop on Machine Vision Applications, P443
   Lee J, 2012, SENSORS-BASEL, V12, P12279, DOI 10.3390/s120912279
   Li CL, 2017, IEEE T CIRC SYST VID, V27, P725, DOI 10.1109/TCSVT.2016.2556586
   Li LY, 2004, IEEE T IMAGE PROCESS, V13, P1459, DOI 10.1109/TIP.2004.836169
   Li S, 2018, IEEE T IMAGE PROCESS, V27, P3918, DOI 10.1109/TIP.2018.2828329
   Li XD, 2019, IEEE T CIRC SYST VID, V29, P2538, DOI 10.1109/TCSVT.2017.2749620
   Liao J, 2018, LECT NOTES COMPUT SC, V11164, P524, DOI 10.1007/978-3-030-00776-8_48
   Lim Kyungsun., 2017, Advanced Video and Signal Based Surveillance (AVSS), 2017 14th IEEE International Conference on, P1
   Lim LA, 2020, PATTERN ANAL APPL, V23, P1369, DOI 10.1007/s10044-019-00845-9
   Lim LA, 2018, PATTERN RECOGN LETT, V112, P256, DOI 10.1016/j.patrec.2018.08.002
   Lin HH, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P893, DOI 10.1109/ICIP.2002.1039116
   Lin HH, 2011, IEEE T IMAGE PROCESS, V20, P822, DOI 10.1109/TIP.2010.2075938
   Liu Z, 2012, IEEE T IMAGE PROCESS, V21, P4204, DOI 10.1109/TIP.2012.2200492
   Luque RM, 2008, LECT NOTES COMPUT SC, V5112, P151, DOI 10.1007/978-3-540-69812-8_15
   Maddalena L., 2012, 2012 IEEE COMP SOC C, P21, DOI [10.1109/CVPRW.2012.6238922, DOI 10.1109/CVPRW.2012.6238922]
   Maddalena L, 2008, IEEE T IMAGE PROCESS, V17, P1168, DOI 10.1109/TIP.2008.924285
   Maddalena L, 2019, MULTIMED TOOLS APPL, V78, P11927, DOI 10.1007/s11042-018-6741-7
   Maddalena L, 2018, J IMAGING, V4, DOI 10.3390/jimaging4050071
   Maddalena L, 2015, LECT NOTES COMPUT SC, V9281, P469, DOI 10.1007/978-3-319-23222-5_57
   Maddalena L, 2014, COMPUT VIS IMAGE UND, V122, P65, DOI 10.1016/j.cviu.2013.11.006
   Maddalena L, 2009, LECT NOTES ARTIF INT, V5571, P263, DOI 10.1007/978-3-642-02282-1_33
   Magee DR, 2004, IMAGE VISION COMPUT, V22, P143, DOI 10.1016/S0262-8856(03)00145-8
   Mahadevan Vijay., 2008, Computer Vision and Pattern Recognition, IEEE Computer Society Conference on, P1, DOI 10.1109/CVPR.2008.4587576
   Mandal M, 2022, IEEE T INTELL TRANSP, V23, P2031, DOI 10.1109/TITS.2020.3030801
   Mandal M, 2019, IEEE SIGNAL PROC LET, V26, P1882, DOI 10.1109/LSP.2019.2952253
   Messelodi S, 2005, LECT NOTES COMPUT SC, V3617, P163, DOI 10.1007/11553595_20
   Minematsu T, 2018, J IMAGING, V4, DOI 10.3390/jimaging4060078
   Moyà-Alcover G, 2017, PATTERN RECOGN LETT, V96, P76, DOI 10.1016/j.patrec.2016.09.004
   Mukherjee D, 2012, PROCEDIA COMPUT SCI, V10, P153, DOI 10.1016/j.procs.2012.06.023
   Mukherjee S, 2015, VISUAL COMPUT, V31, P1405, DOI 10.1007/s00371-014-1022-6
   Muniruzzaman S., 2016, J BUILT ENV TECHNOL, V1, P111
   Narayana M, 2014, MACH VISION APPL, V25, P1163, DOI 10.1007/s00138-013-0569-y
   Nghiem AT, 2007, 2007 IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P476, DOI 10.1109/AVSS.2007.4425357
   Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684
   Palomo EJ, 2013, NEURAL PROCESS LETT, V37, P69, DOI 10.1007/s11063-012-9266-5
   Pnevmatikakis A, 2007, LECT NOTES COMPUT SC, V4122, P151
   Porikli F., 2005, P P WORKSH IM AN MUL
   Prati A, 2003, IEEE T PATTERN ANAL, V25, P918, DOI 10.1109/TPAMI.2003.1206520
   Qu Z., 2016, 2016 Third International Conference on Artificial Intelligence and Pattern Recognition (AIPR), P1
   Rafique A, 2014, INT CONF CONTR AUTO, P165, DOI 10.1109/ICCAIS.2014.7020551
   Rivera AR, 2013, IEEE T CIRC SYST VID, V23, P1375, DOI 10.1109/TCSVT.2013.2242551
   Romero JD, 2018, IEEE T IMAGE PROCESS, V27, P1243, DOI 10.1109/TIP.2017.2776742
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roy K, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Roy SD, 2020, IEEE T CIRCUITS SYST
   Sakkos D, 2018, MULTIMED TOOLS APPL, V77, P23023, DOI 10.1007/s11042-017-5460-9
   Schofield AJ, 1996, PATTERN RECOGN, V29, P1421, DOI 10.1016/0031-3203(95)00163-8
   Scott JamesC., 2009, ART NOT BEING GOVERN, P1
   Shafiee MJ, 2018, J SIGNAL PROCESS SYS, V90, P931, DOI 10.1007/s11265-017-1265-3
   Shafiee MJ, 2016, IEEE COMPUT SOC CONF, P837, DOI 10.1109/CVPRW.2016.109
   Shafiee MJ, 2016, IEEE ACCESS, V4, P1915, DOI 10.1109/ACCESS.2016.2551458
   Sheikh Y, 2005, IEEE T PATTERN ANAL, V27, P1778, DOI 10.1109/TPAMI.2005.213
   Shen YR, 2016, IEEE T MOBILE COMPUT, V15, P406, DOI 10.1109/TMC.2015.2418775
   Sheri AM, 2018, IET IMAGE PROCESS, V12, P1646, DOI 10.1049/iet-ipr.2017.1055
   Shi P, 2004, P SOC PHOTO-OPT INS, V5298, P168, DOI 10.1117/12.525553
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh RP., 2020, VISUAL COMPUT, V1, P17
   Singh Sanchit, 2010, Proceedings 7th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2010), P48, DOI 10.1109/AVSS.2010.63
   Sobral A, 2014, COMPUT VIS IMAGE UND, V122, P4, DOI 10.1016/j.cviu.2013.12.005
   St-Charles PL, 2015, IEEE WINT CONF APPL, P990, DOI 10.1109/WACV.2015.137
   St-Charles PL, 2015, IEEE T IMAGE PROCESS, V24, P359, DOI 10.1109/TIP.2014.2378053
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Suhr JK, 2011, IEEE T CIRC SYST VID, V21, P365, DOI 10.1109/TCSVT.2010.2087810
   Sultana M, 2019, MACH VISION APPL, V30, P375, DOI 10.1007/s00138-018-0993-0
   Tang P, 2007, PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON IMAGE AND GRAPHICS, P530, DOI 10.1109/ICIG.2007.61
   Tao F, 2010, INT CONF COMP SCI, P136, DOI 10.1109/ICCSIT.2010.5564529
   Tezcan MO, 2020, IEEE WINT CONF APPL, P2763, DOI [10.1109/WACV45572.2020.9093464, 10.1109/wacv45572.2020.9093464]
   Tezcan MO, 2021, IEEE ACCESS, V9, P53849, DOI 10.1109/ACCESS.2021.3071163
   Tezuka H, 2008, IEEE IMAGE PROC, P2732, DOI 10.1109/ICIP.2008.4712359
   Tezuka H, 2009, IEICE T FUND ELECTR, VE92A, P772, DOI 10.1587/transfun.E92.A.772
   Tiburzi F, 2008, IEEE IMAGE PROC, P17, DOI 10.1109/ICIP.2008.4711680
   Nguyen TP, 2019, IEEE T CIRC SYST VID, V29, P433, DOI 10.1109/TCSVT.2018.2795657
   Toyama K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P255, DOI 10.1109/ICCV.1999.791228
   Vacavant Antoine, 2013, Computer Vision - ACCV 2012 Workshops. ACCV 2012 International Workshops. Revised Selected Papers, P291, DOI 10.1007/978-3-642-37410-4_25
   Vasamsetti S, 2019, COGN COMPUT, V11, P18, DOI 10.1007/s12559-018-9594-5
   Vaswani N, 2018, IEEE SIGNAL PROC MAG, V35, P32, DOI 10.1109/MSP.2018.2826566
   Vosters L, 2012, IMAGE VISION COMPUT, V30, P1004, DOI 10.1016/j.imavis.2012.08.017
   Wang L, 2003, IEEE T PATTERN ANAL, V25, P1505, DOI 10.1109/TPAMI.2003.1251144
   Wang R, 2014, IEEE COMPUT SOC CONF, P420, DOI 10.1109/CVPRW.2014.68
   Wang Y, 2017, PATTERN RECOGN LETT, V96, P66, DOI 10.1016/j.patrec.2016.09.014
   Wang Y, 2014, IEEE COMPUT SOC CONF, P393, DOI 10.1109/CVPRW.2014.126
   Wren C.R., 2005, PERFORMANCE EVALUATI, P55
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   Wu MJ, 2010, AEU-INT J ELECTRON C, V64, P739, DOI 10.1016/j.aeue.2009.05.004
   Wu Z, 2014, IEEE COMPUT SOC CONF, P201, DOI 10.1109/CVPRW.2014.39
   Xiao M, 2008, 2008 ISECS INTERNATIONAL COLLOQUIUM ON COMPUTING, COMMUNICATION, CONTROL, AND MANAGEMENT, VOL 1, PROCEEDINGS, P47, DOI 10.1109/CCCM.2008.294
   Xiao M, 2006, 2006 9TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION, VOLS 1-4, P1193
   Xie Y, 2013, IEEE J-STSP, V7, P12, DOI 10.1109/JSTSP.2012.2234082
   Xu P, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P107, DOI 10.1145/2647868.2654914
   Xu Z, 2019, SIGNAL PROCESS-IMAGE, V71, P1, DOI 10.1016/j.image.2018.07.004
   Yan JH, 2016, IET COMPUT VIS, V10, P884, DOI 10.1049/iet-cvi.2016.0075
   Yan Li-feng, 2008, Journal of System Simulation, V20, P944, DOI 10.1016/j.sysarc.2008.04.011
   Yan YJ, 2018, LECT NOTES ARTIF INT, V10989, P75, DOI 10.1007/978-3-030-00563-4_8
   Yang L, 2018, IEEE T INTELL TRANSP, V19, P254, DOI 10.1109/TITS.2017.2754099
   Yang YX, 2017, IEEE T SYST MAN CY-S, V47, P950, DOI 10.1109/TSMC.2016.2523907
   Yao GL, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17091945
   Young D. P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P317
   Zeng DD, 2019, IEEE ACCESS, V7, P153869, DOI 10.1109/ACCESS.2019.2899348
   Zeng DD, 2018, IEEE GEOSCI REMOTE S, V15, P617, DOI 10.1109/LGRS.2018.2797538
   Zhang SP, 2009, INT J PATTERN RECOGN, V23, P1397, DOI 10.1142/S0218001409007569
   Zhang XG, 2014, IEEE T IMAGE PROCESS, V23, P4511, DOI 10.1109/TIP.2014.2352036
   Zhang YQ, 2015, NEUROCOMPUTING, V168, P454, DOI 10.1016/j.neucom.2015.05.082
   Zhao C, 2011, EURASIP J IMAGE VIDE, DOI 10.1155/2011/972961
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25
   Zhao PX, 2012, PROCEEDINGS OF THE 3RD IEEE INTERNATIONAL CONFERENCE ON NETWORK INFRASTRUCTURE AND DIGITAL CONTENT (IEEE IC-NIDC 2012), P438, DOI 10.1109/ICNIDC.2012.6418791
   Zhao X, 2012, ELECTRON LETT, V48, P825, DOI 10.1049/el.2012.0667
   Zheng WB, 2020, NEUROCOMPUTING, V394, P178, DOI 10.1016/j.neucom.2019.04.088
   Zhong J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P44, DOI 10.1109/ICCV.2003.1238312
   Zivkovic Z, 2006, PATTERN RECOGN LETT, V27, P773, DOI 10.1016/j.patrec.2005.11.005
NR 236
TC 26
Z9 27
U1 15
U2 68
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 4151
EP 4178
DI 10.1007/s00371-021-02286-0
EA AUG 2021
PG 28
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000685399300002
DA 2024-07-18
ER

PT J
AU Tschoerner, B
   Li, F
   Lan, ZR
   Liu, YS
   Lim, WL
   Cui, J
   Wong, YL
   Kho, K
   Lee, V
   Sourina, O
   Mueller-Wittig, W
AF Tschoerner, Benedikt
   Li, Fan
   Lan, Zirui
   Liu, Yisi
   Lim, Wei Lun
   Cui, Jian
   Wong, Yu Lian
   Kho, Kevin
   Lee, Vincent
   Sourina, Olga
   Mueller-Wittig, Wolfgang
TI Human factors evaluation in VR-based shunting training
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual Reality; Human factors; Non-technical skills; Shunting; Training
AB Shunting of trains is a task that requires meticulous adherence to all steps to guarantee safety for everyone involved during and after the procedure. These steps are currently taught using classical teaching materials, such as printouts, videos and training by experienced supervisors. However, due to limited availability of locomotives, hours for training and manpower, training of shunting operation becomes challenging in real life. In this paper, we implemented a lifelike, collaborative virtual environment for shunting training including a novel human factors evaluation system for fatigue and stress monitoring. An experiment with 12 subjects and 3 trainers has been designed and carried out to validate the usage of VR-based shunting training. Positive feedback toward the VR-based training was obtained from the subjects and trainers.
C1 [Tschoerner, Benedikt; Lan, Zirui; Liu, Yisi; Sourina, Olga; Mueller-Wittig, Wolfgang] Fraunhofer Singapore, 50 Nanyang Ave,Block NS 1,Level 5, Singapore 639798, Singapore.
   [Li, Fan; Lim, Wei Lun; Cui, Jian; Sourina, Olga; Mueller-Wittig, Wolfgang] Nanyang Technol Univ, 50 Nanyang Ave, Singapore 639798, Singapore.
   [Wong, Yu Lian; Kho, Kevin; Lee, Vincent] SMRT Train Ltd, 300 Bishan Rd, Singapore 579828, Singapore.
C3 Nanyang Technological University
RP Lan, ZR (corresponding author), Fraunhofer Singapore, 50 Nanyang Ave,Block NS 1,Level 5, Singapore 639798, Singapore.
EM lan.zirui@fraunhofer.sg
RI Li, Fan/HKE-1109-2023
FU National Research Foundation, Singapore, under its International
   Research Centers in Singapore Funding Initiative
FX This research is supported by the National Research Foundation,
   Singapore, under its International Research Centers in Singapore Funding
   Initiative. Any opinions, findings and conclusions or recommendations
   expressed in this material are those of the author(s) and do not reflect
   the views of National Research Foundation, Singapore.
CR Armougum A, 2020, APPL ERGON, V89, DOI 10.1016/j.apergo.2020.103180
   Cong Guan, 2013, 2013 5th International Conference on Computational Intelligence and Communication Networks (CICN), P631, DOI 10.1109/CICN.2013.138
   Cwil M, 2019, LECT NOTES COMPUT SC, V11575, P230, DOI 10.1007/978-3-030-21565-1_15
   Enomoto, 2011, STUDY EXTRACTING SHA
   Gegenfurtner A, 2020, INT J TRAIN DEV, V24, P5, DOI 10.1111/ijtd.12167
   Giannakakis G., 2019, IEEE Transactions on Affective Computing
   Herten N, 2017, NEUROBIOL LEARN MEM, V140, P134, DOI 10.1016/j.nlm.2017.02.016
   Hirt C, 2020, J AMB INTEL HUM COMP, V11, P5977, DOI 10.1007/s12652-020-01845-y
   James AT, 2017, INT J SYST ASSUR ENG, V8, P719, DOI 10.1007/s13198-017-0589-5
   Li F, 2020, J NAVIGATION, V73, P1340, DOI 10.1017/S0373463320000326
   Madigan R., 2015, CONT ERG HUM FACT 20
   Nayak S., 2019, INT J BUS EXCELL, V19, P168, DOI [10.1504/IJBEX.2019.102233, DOI 10.1504/IJBEX.2019.102233]
   Nayak S, 2018, INT J SYST ASSUR ENG, V9, P1120, DOI 10.1007/s13198-018-0715-z
   Nwaeke L.I., 2017, European Journal of Business and Management, V9, P153
   Papa S., 2018, COMPUTERS RAILWAYS 1, V181, P367
   RUSSELL, MODEL DEV RAILWAY TR
   Seinfeld S, 2016, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.01969
   Sitanyiova Dana, 2018, MATEC Web of Conferences, V235, DOI 10.1051/matecconf/201823500014
   Truong KP, 2007, SPEECH COMMUN, V49, P144, DOI 10.1016/j.specom.2007.01.001
   Xu JX, 2018, ENTERTAIN COMPUT, V27, P23, DOI 10.1016/j.entcom.2018.03.002
NR 20
TC 4
Z9 4
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2021
VL 37
IS 12
SI SI
BP 3063
EP 3076
DI 10.1007/s00371-021-02251-x
EA AUG 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP1DP
UT WOS:000683641300002
OA hybrid
DA 2024-07-18
ER

PT J
AU Wang, Y
   Xia, Y
   Zhang, YL
AF Wang, Yu
   Xia, Yi
   Zhang, Yongliang
TI Beyond view transformation: feature distribution consistent GANs for
   cross-view gait recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Cross-view gait recognition; Generative adversarial networks; Cross-view
   image converting; Recurrent cycle consistency loss
AB Gait recognition systems have shown great potentials in the field of biometric recognition. Unfortunately, the accuracy of gait recognition is easily affected by a large view angle. To address the problem, this study proposes a feature distribution consistent generative adversarial network (FDC-GAN) to transform gait images from arbitrary views to the target view and then perform identity recognition. Besides reconstruction loss, view classification and identity preserving loss are also introduced to guide the generator to produce gait images of the target views and keep identity information simultaneously. To further encourage the network to generate gait images whose feature distribution can well align the true distribution, we also exploit the recently proposed recurrent cycle consistency loss, which can help to remove the unnoticed and useless content preserved in the generated gait images. The experimental results on datasets CASIA-B and OU-MVLP demonstrate the state-of-the-art performance of our model compared to other GAN-based cross-view gait recognition models.
C1 [Wang, Yu; Xia, Yi] Anhui Univ, Sch Elect Engn & Automat, Hefei 230601, Peoples R China.
   [Zhang, Yongliang] Univ Sci & Technol China, Affiliated Hosp USTC 1, Div Life Sci & Med, Hefei 230036, Anhui, Peoples R China.
C3 Anhui University; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Xia, Y (corresponding author), Anhui Univ, Sch Elect Engn & Automat, Hefei 230601, Peoples R China.; Zhang, YL (corresponding author), Univ Sci & Technol China, Affiliated Hosp USTC 1, Div Life Sci & Med, Hefei 230036, Anhui, Peoples R China.
EM xiayi@ahu.edu.cn; zy12020@ustc.edu.cn
RI Zhang, Yongliang/AAE-7639-2021
OI Zhang, Yongliang/0000-0002-6790-1044
FU National Natural Science Foundation of China [61872004]; National Key
   R&D Program of China [2020YFC2005300]
FX The authors would like to thank the anonymous reviewers for their
   critical and constructive comments and suggestions. This work was
   supported by the National Natural Science Foundation of China (61872004)
   and also partly supported by National Key R&D Program of China (Grant
   No. 2020YFC2005300).
CR Ariyanto G., 2011, 2011 INT JOINT C BIO, P1, DOI DOI 10.1109/IJCB.2011.6117582
   Bi FK, 2022, VISUAL COMPUT, V38, P2581, DOI 10.1007/s00371-021-02133-2
   Chao HQ, 2019, AAAI CONF ARTIF INTE, P8126
   DENG WJ, 2018, CVPR, P994, DOI DOI 10.1109/CVPR.2018.00110
   Du HQ, 2021, IEEE W SP LANG TECH, P507, DOI 10.1109/SLT48900.2021.9383567
   Fang Z, 2022, VISUAL COMPUT, V38, P1151, DOI 10.1007/s00371-021-02074-w
   Fangneng Zhan, 2020, 2020 25th International Conference on Pattern Recognition (ICPR), P6889, DOI 10.1109/ICPR48806.2021.9412465
   Gad R, 2018, FUTURE GENER COMP SY, V89, P178, DOI 10.1016/j.future.2018.06.020
   Goffredo M, 2010, IEEE T SYST MAN CY B, V40, P997, DOI 10.1109/TSMCB.2009.2031091
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Han J, 2006, IEEE T PATTERN ANAL, V28, P316, DOI 10.1109/TPAMI.2006.38
   He YW, 2019, IEEE T INF FOREN SEC, V14, P102, DOI 10.1109/TIFS.2018.2844819
   Hensel M, 2017, ADV NEUR IN, V30
   Hu MD, 2013, IEEE T INF FOREN SEC, V8, P2034, DOI 10.1109/TIFS.2013.2287605
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Iwama H, 2012, IEEE T INF FOREN SEC, V7, P1511, DOI 10.1109/TIFS.2012.2204253
   Jiang WT, 2020, PROC CVPR IEEE, P5193, DOI 10.1109/CVPR42600.2020.00524
   Kingma D. P., 2014, arXiv
   Kusakunniran Worapan, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1058, DOI 10.1109/ICCVW.2009.5457587
   Kusakunniran W, 2010, PROC CVPR IEEE, P974, DOI 10.1109/CVPR.2010.5540113
   Liao RJ, 2020, IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2020), DOI 10.1109/ijcb48548.2020.9304910
   Lu JW, 2010, PATTERN RECOGN LETT, V31, P382, DOI 10.1016/j.patrec.2009.11.006
   Makihara Y, 2006, LECT NOTES COMPUT SC, V3953, P151, DOI 10.1007/11744078_12
   SANCHEZ E, 2018, ARXIV181103492
   Sanchez E, 2020, IEEE INT CONF AUTOMA, P53, DOI 10.1109/FG47880.2020.00015
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sharma RP, 2019, VISUAL COMPUT, V35, P1393, DOI 10.1007/s00371-018-01618-x
   Song CF, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.106988
   Song HT, 2021, VISUAL COMPUT, V37, P2285, DOI 10.1007/s00371-020-01986-3
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Takemura Noriko, 2018, IPSJ Transactions on Computer Vision and Applications, V10, DOI 10.1186/s41074-018-0039-6
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wang N, 2014, MULTIMED TOOLS APPL, V72, P2339, DOI 10.1007/s11042-013-1551-4
   Wang N, 2014, MULTIMED TOOLS APPL, V71, P1411, DOI 10.1007/s11042-012-1278-7
   Wang YY, 2019, NEUROCOMPUTING, V339, P245, DOI 10.1016/j.neucom.2019.02.025
   Xing XL, 2016, PATTERN RECOGN, V50, P107, DOI 10.1016/j.patcog.2015.08.011
   Xu C, 2021, IEEE T CIRC SYST VID, V31, P260, DOI 10.1109/TCSVT.2020.2975671
   Yu SQ, 2006, INT C PATT RECOG, P441
   Yu SQ, 2017, IEEE COMPUT SOC CONF, P532, DOI 10.1109/CVPRW.2017.80
   Zhang P, 2019, IEEE IJCNN, DOI 10.1109/ijcnn.2019.8852258
   Zhang PF, 2019, IEEE T PATTERN ANAL, V41, P1963, DOI 10.1109/TPAMI.2019.2896631
   Zhang Y, 2021, NEUROCOMPUTING, V450, P33, DOI 10.1016/j.neucom.2021.04.007
   Zhang Z, 2011, LECT NOTES COMPUT SC, V6523, P182
NR 43
TC 4
Z9 4
U1 2
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2022
VL 38
IS 6
BP 1915
EP 1928
DI 10.1007/s00371-021-02254-8
EA JUL 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1C6YN
UT WOS:000673194600001
DA 2024-07-18
ER

PT J
AU Liu, XC
   Chen, Y
   Zhang, HT
   Zou, YH
   Wang, ZY
   Peng, QS
AF Liu, Xincheng
   Chen, Yi
   Zhang, Haitong
   Zou, Yuhong
   Wang, Zhangye
   Peng, Qunsheng
TI Physically based modeling and rendering of avalanches
SO VISUAL COMPUTER
LA English
DT Article
DE Snow avalanche; Position-based dynamics; Granular flow; Elastoplastic;
   Snow fog
ID SNOW; ANIMATION
AB Avalanche is a natural disaster in the snow-covered mountainous area in winter, which may cause great disasters to human life and property. It is also a danger for skiers and climbers. This paper presents a new physically based algorithm to simulate the dynamic avalanches under position-based dynamics framework. To realistically simulate avalanches' dynamic characteristics, we introduce the Bingham plastic model from geodynamics to model snow flow motion in avalanches. The interaction between snow flow in the avalanche and the surrounding objects is simulated by a level set-based two-way fluid-solid coupling model. We also propose static and kinetic friction mixed model to determine the accumulated transition of the avalanche. To create an avalanche scene with more realistic details, we employ an aerodynamics-based snow drag force model to generate snow fog effect. Finally, by choosing different criterion shear rate and friction parameters, different kinds of wet and dry avalanche scenes are realistically rendered. Compared with the real photographs of avalanches, our simulated results are quite satisfactory.
C1 [Liu, Xincheng; Zhang, Haitong; Wang, Zhangye; Peng, Qunsheng] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Peoples R China.
   [Chen, Yi] Commun Univ Zhejiang, Hangzhou 310018, Peoples R China.
   [Zou, Yuhong] Shanghai Jiao Tong Univ, Sch Design, Shanghai 20040, Peoples R China.
C3 Zhejiang University; Communication University of Zhejiang; Shanghai Jiao
   Tong University
RP Wang, ZY (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Peoples R China.
EM 21921134@zju.edu.cn; chenyi@timeaxis.com.cn; haitong@zju.edu.cn;
   zyhsonny2019@sjtu.edu.cn; zywang@cad.zju.edu.cn; peng@cad.zju.edu.cn
FU 863 Program of China [2015AA016404]; National Key R&D Program of China
   [2017YFB1002703]; Natural Science Foundation of China [U1736109]
FX Funding was provided by 863 Program of China (No. 2015AA016404),
   National Key R&D Program of China (No. 2017YFB1002703) and Natural
   Science Foundation of China (No. U1736109).
CR Abdelrazek AM, 2014, RIVER FLOW 2014, P1581
   Alduan I., 2011, P 2011 ACM SIGGRAPH, P25, DOI DOI 10.1145/2019406.2019410
   [Anonymous], 2004, P 2004 ACM SIGGRAPH, DOI DOI 10.1145/1028523.1028541
   Bender J, 2014, COMPUT GRAPH-UK, V44, P1, DOI 10.1016/j.cag.2014.07.004
   Christoph Gissler Andreas, 2020, ACM T GRAPHIC, V39, P1
   Chudziak J, 2015, MONATSH MATH, V178, P521, DOI 10.1007/s00605-015-0784-x
   Cordonnier G, 2018, COMPUT GRAPH FORUM, V37, P497, DOI 10.1111/cgf.13379
   Cornel D, 2019, COMPUT GRAPH FORUM, V38, P25, DOI 10.1111/cgf.13669
   Dent J. D., 1983, Annals of Glaciology, V4, P42
   Fearing P, 2000, COMP GRAPH, P37, DOI 10.1145/344779.344809
   Gissler C, 2017, COMPUT GRAPH-UK, V69, P1, DOI 10.1016/j.cag.2017.09.002
   He XW, 2018, IEEE T VIS COMPUT GR, V24, P2589, DOI 10.1109/TVCG.2017.2755646
   Huang N, 2011, J GEOPHYS RES-ATMOS, V116, DOI 10.1029/2011JD016657
   Li XY, 2020, CRYOSPHERE, V14, P3381, DOI 10.5194/tc-14-3381-2020
   Macklin M, 2014, ACM T GRAPHIC, V33, DOI [10.1145/280/109/2601152, 10.1145/2601097.2601152]
   MILLER G, 1989, COMPUT GRAPH-UK, V13, P305, DOI 10.1016/0097-8493(89)90078-2
   Muller M., 2004, P 2004 ACM SIGGRAPHE, P141, DOI [DOI 10.1145/1028523.1028542, 10.1145/1028523.1028542, 10]
   Nishita T, 1997, COMPUT GRAPH FORUM, V16, pC357, DOI 10.1111/1467-8659.00173
   Oda K, 2011, ANN GLACIOL, V52, P57, DOI 10.3189/172756411797252284
   Sifakis E, 2012, ACM SIGGRAPH 2012 CO, DOI [10.1145/2343483.2343501, DOI 10.1145/2343483.2343501]
   Stomakhin A, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461948
   Tsuda Y, 2010, VISUAL COMPUT, V26, P883, DOI 10.1007/s00371-010-0491-5
   Wang CB, 2006, VISUAL COMPUT, V22, P315, DOI 10.1007/s00371-006-0012-8
   Wang CB, 2015, COMPUT ANIMAT VIRT W, V26, P3, DOI 10.1002/cav.1542
   Zhang SF, 2017, COMPUT ANIMAT VIRT W, V28, DOI 10.1002/cav.1758
   Zhao JW, 2019, VISUAL COMPUT, V35, P1223, DOI 10.1007/s00371-019-01709-3
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
NR 27
TC 2
Z9 2
U1 0
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2619
EP 2629
DI 10.1007/s00371-021-02215-1
EA JUL 2021
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000672248800001
DA 2024-07-18
ER

PT J
AU Ju, YX
   Zhang, JH
   Mao, XY
   Xu, JY
AF Ju, Yixuan
   Zhang, Jianhai
   Mao, Xiaoyang
   Xu, Jiayi
TI Adaptive semantic attribute decoupling for precise face image editing
SO VISUAL COMPUTER
LA English
DT Article
DE Face attribute editing; GAN; Quantitative control
AB Precisely editing user specified facial attributes has wide applications in areas such as virtual makeup, face aging, facial expression transfer, face synthesis. However, it is difficult to explicitly control individual facial attribute due to the gap between high level semantics in human perception and feature vectors in latent space. In this paper, a semantic disentanglement algorithm interpreting the latent space of GAN is proposed, which can be employed to extract attribute control vector adaptive to individual face. By adjusting the coefficient of extracted control vector, variation of single attribute is realized. Then, comprehensive modification effect of facial attributes is obtained through the superposition of control vector. Classification and content loss functions are introduced to limit modification occurs to the specified attribute without affecting the other attributes. As a result, precise editing control is realized.
C1 [Ju, Yixuan; Zhang, Jianhai; Mao, Xiaoyang; Xu, Jiayi] Hangzhou Dianzi Univ, Sch Comp Sci, Hangzhou 310018, Peoples R China.
   [Mao, Xiaoyang] Univ Yamanashi, Dept Comp Sci & Engn, Kofu, Yamanashi 4008511, Japan.
C3 Hangzhou Dianzi University; University of Yamanashi
RP Xu, JY (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci, Hangzhou 310018, Peoples R China.
EM juyixuan@hdu.edu.cn; jhzhang@hdu.edu.cn; mao@yamanashi.ac.jp;
   xujiayi@hdu.edu.cn
FU Public Welfare Research Project of Zhejiang Province, China
   [LGF18F020015]; Opening Foundation of Key Laboratory of Fundamental
   Science for National Defense on Vision Synthetization, Sichuan
   University, China [2020SCUVS007]; Opening Foundation of Zhejiang Police
   College, China [2020DSJSYS002]; JSPS, Japan [17H00737]; Key Laboratory
   of Brain Machine Collaborative Intelligence of Zhejiang Province, China;
   Grants-in-Aid for Scientific Research [17H00737] Funding Source: KAKEN
FX This paper was sponsored by the Public Welfare Research Project of
   Zhejiang Province, China (GrantNo. LGF18F020015), Opening Foundation of
   Key Laboratory of Fundamental Science for National Defense on Vision
   Synthetization, Sichuan University, China (Grant No. 2020SCUVS007),
   Opening Foundation of Zhejiang Police College, China (Grant
   No.2020DSJSYS002), and JSPS Grants-in-Aid for Scientific Research, Japan
   (Grant No. 17H00737), and Key Laboratory of Brain Machine Collaborative
   Intelligence of Zhejiang Province, China.
CR [Anonymous], 2017, ARXIV PREPRINT ARXIV
   Arjovsky M., 2017, ARXIV170107875
   Chen Xi, 2016, Advances in Neural Information Processing Systems (NIPS), V29
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Harkonen E., 2020, Advances in Neural Information Processing Systems, V33, P9841
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Jiang Songyao, 2019, 2019 11 INT S ADV TO, P1
   Jo Y, 2019, IEEE I CONF COMP VIS, P1745, DOI 10.1109/ICCV.2019.00183
   Karras T, 2018, P INT C LEARN REPR I
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kim TS, 2017, IEEE COMPUT SOC CONF, P1623, DOI 10.1109/CVPRW.2017.207
   Lample Guillaume, 2017, P ANN C NEUR INF PRO, P5967
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Natsume R., 2018, ARXIV180403447, P1, DOI [DOI 10.1145/3230744.3230818, 10.1145/3230744.3230818]
   Perarnau G., 2016, NIPS WORKSH ADV TRAI
   Shen W, 2017, PROC CVPR IEEE, P1225, DOI 10.1109/CVPR.2017.135
   SHEN Y., 2020, ARXIV200706600
   Shen Yujun, 2020, P IEEE CVF C COMP VI, P9243, DOI DOI 10.1109/CVPR42600.2020.00926
   Shoshan A., ARXIV PREPRINT ARXIV
   Yin W., 2017, CoRR
   Zhang ZF, 2017, PROC CVPR IEEE, P4352, DOI 10.1109/CVPR.2017.463
   Zhu J., 2019, P 13 INT WORKSH SEM
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 24
TC 5
Z9 5
U1 1
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2907
EP 2918
DI 10.1007/s00371-021-02198-z
EA JUL 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000668881000001
DA 2024-07-18
ER

PT J
AU Afraites, L
   Hadri, A
   Laghrib, A
   Nachaoui, M
AF Afraites, Lekbir
   Hadri, Aissam
   Laghrib, Amine
   Nachaoui, Mourad
TI A weighted parameter identification PDE-constrained optimization for
   inverse image denoising problem
SO VISUAL COMPUTER
LA English
DT Article
DE Image restoration; PDE-constrained; Parameter identification;
   Primal-dual; Tensor diffusion
ID BILEVEL PROGRAMS; MODEL; FILTER
AB This paper treats the inverse denoising problem which aims to compute simultaneously the clean image and the weighting parameter lambda. The formulated denoising problem is posed using a partial differential equation (PDE)-constrained optimization model. The minimized function imposes a Tikhonov regularization on the estimated lambda, while the proposed PDE encompasses two high-order diffusive tensors. The particularity of this PDE is that it does not over-smooth homogeneous regions and preserves sharp edges during the denoising process, even if its degree is high. A new optimization procedure to compute the weighting parameter is also elaborated inspired from the nonsmooth Primal-dual algorithm. This leads to control of the diffusivity rate generated by the two diffusive operators. Finally, expressive results show that the computed spatial parameter lambda leads to obtain a pleasant clean image. This is also confirmed by numerous comparisons with other competitive denoising approaches.
C1 [Afraites, Lekbir; Laghrib, Amine] Univ Sultan Moulay Slimane, LMA FST Beni Mellal, Beni Mellal, Morocco.
   [Hadri, Aissam] Univ Ibn Zohr, Fac Polydisciplinaire, Ouarzazate, Agadir, Morocco.
   [Nachaoui, Mourad] Univ Nantes, Lab Math Jean Leray, UMR6629, CNRS, Nantes, France.
C3 Sultan Moulay Slimane University of Beni Mellal; Ibn Zohr University of
   Agadir; Nantes Universite; Centre National de la Recherche Scientifique
   (CNRS); CNRS - National Institute for Mathematical Sciences (INSMI)
RP Afraites, L (corresponding author), Univ Sultan Moulay Slimane, LMA FST Beni Mellal, Beni Mellal, Morocco.; Nachaoui, M (corresponding author), Univ Nantes, Lab Math Jean Leray, UMR6629, CNRS, Nantes, France.
EM laghrib.amine@gmail.com; nachaoui@gmail.com
RI Laghrib, Amine/GSN-3177-2022; Nachaoui, Mourad/AFR-2408-2022; Nachaoui,
   Mourad M./AFE-7718-2022; Afraites, Lekbir/ABH-8256-2022
OI Laghrib, Amine/0000-0003-4851-3617; Nachaoui, Mourad
   M./0000-0002-4020-5401; 
CR Afraites L, 2020, INVERSE PROBL, V36, DOI 10.1088/1361-6420/ab5178
   Antil H, 2020, INVERSE PROBL, V36, DOI 10.1088/1361-6420/ab80d7
   Aubert G, 2008, SIAM J APPL MATH, V68, P925, DOI 10.1137/060671814
   Beckouche S, 2013, ASTRON ASTROPHYS, V556, DOI 10.1051/0004-6361/201220752
   Berdawood K, 2020, Adv. Math. Models Appl., V5, P131
   Bergam A, 2019, APPL MATH COMPUT, V346, P865, DOI 10.1016/j.amc.2018.09.069
   Bertero M, 2003, INVERSE PROBL, V19, P1427, DOI 10.1088/0266-5611/19/6/011
   Bredies K, 2010, SIAM J IMAGING SCI, V3, P492, DOI 10.1137/090769521
   Calatroni L, 2019, INVERSE PROBL, V35, DOI 10.1088/1361-6420/ab291a
   De los Reyes JC, 2013, INVERSE PROBL IMAG, V7, P1183, DOI 10.3934/ipi.2013.7.1183
   Chakib A, 2013, NUMER METH PART D E, V29, P1563, DOI 10.1002/num.21767
   Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1
   Clason C, 2017, SIAM J OPTIMIZ, V27, P1314, DOI 10.1137/16M1080859
   Clason C, 2012, SIAM J IMAGING SCI, V5, P505, DOI 10.1137/110826187
   Coupé P, 2008, IEEE T MED IMAGING, V27, P425, DOI 10.1109/TMI.2007.906087
   Dasgupta A, 2017, BIOMED SIGNAL PROCES, V31, P231, DOI 10.1016/j.bspc.2016.08.012
   De los Reyes JC, 2017, J MATH IMAGING VIS, V57, P1, DOI 10.1007/s10851-016-0662-8
   Dempe S, 2019, J GLOBAL OPTIM, V74, P297, DOI 10.1007/s10898-019-00758-1
   Ding K, 2014, VISUAL COMPUT, V30, P1311, DOI 10.1007/s00371-013-0888-z
   El Mourabit I, 2017, SIGNAL PROCESS, V132, P51, DOI 10.1016/j.sigpro.2016.09.014
   Elad M, 2002, IEEE T IMAGE PROCESS, V11, P1141, DOI 10.1109/TIP.2002.801126
   Engl H. W., 1996, REGULARIZATION INVER, V375
   Getreuer Pascal, 2011, Advances in Visual Computing. Proceedings 7th International Symposium, ISVC 2011, P686
   Hadamard J., 1923, Lectures on Cauchy's problem in linear partial differential equations
   Hintermüller M, 2017, J MATH IMAGING VIS, V59, P498, DOI 10.1007/s10851-017-0744-2
   Hintermüller M, 2017, J MATH IMAGING VIS, V59, P515, DOI 10.1007/s10851-017-0736-2
   Holler G, 2018, INVERSE PROBL, V34, DOI 10.1088/1361-6420/aade77
   Kabanikhin S. I., 1998, J INVERSE ILL-POSE P, V6, P403
   Kabanikhin SI., 2012, Inverse and Ill-Posed Problems
   Kallel M, 2015, INVERSE PROBL IMAG, V9, P853, DOI 10.3934/ipi.2015.9.853
   Konstantin P., 2016, PURE APPL FUNCT ANAL, V1, P505
   Kunisch K, 2013, SIAM J IMAGING SCI, V6, P938, DOI 10.1137/120882706
   Laghrib A, 2020, DISCRETE CONT DYN-B, V25, P415, DOI 10.3934/dcdsb.2019188
   Langer A, 2017, J MATH IMAGING VIS, V57, P239, DOI 10.1007/s10851-016-0676-2
   Lin GH, 2014, MATH PROGRAM, V144, P277, DOI 10.1007/s10107-013-0633-4
   Liu XY, 2015, INT J COMPUT MATH, V92, P608, DOI 10.1080/00207160.2014.904854
   Liu XW, 2011, APPL MATH LETT, V24, P1282, DOI 10.1016/j.aml.2011.01.028
   Lyaqini S, 2020, KNOWL INF SYST, V62, P3039, DOI 10.1007/s10115-020-01439-2
   Mitsos A, 2008, J GLOBAL OPTIM, V42, P475, DOI 10.1007/s10898-007-9260-z
   Nachaoui A, 2021, J COMPUT APPL MATH, V381, DOI 10.1016/j.cam.2020.113030
   Nachaoui M., 2020, Adv. Math. Models Appl, V5, P53
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Sun, 2020, ARXIV200205614
   Uhlmann, 2015, PANORAMAS SYNTHESES
   Ulbrich M., 2008, Optimization With PDE Constraints
   Van Chung C, 2017, INVERSE PROBL, V33, DOI 10.1088/1361-6420/33/7/074005
   Zeghal A, 2002, J MATH ANAL APPL, V272, P240, DOI 10.1016/S0022-247X(02)00155-5
   Zhang J, 2012, J MATH IMAGING VIS, V43, P39, DOI 10.1007/s10851-011-0285-z
   Zhang XQ, 2011, J SCI COMPUT, V46, P20, DOI 10.1007/s10915-010-9408-8
   Zhou, 2020, INVERSE PROBL, V37
   Zhu XD, 2017, MATH METHOD OPER RES, V86, P255, DOI 10.1007/s00186-017-0592-2
NR 51
TC 15
Z9 15
U1 1
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2022
VL 38
IS 8
BP 2883
EP 2898
DI 10.1007/s00371-021-02162-x
EA MAY 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3B2OE
UT WOS:000655399900001
DA 2024-07-18
ER

PT J
AU Fang, Z
   Liu, Z
   Liu, TT
   Hung, CC
   Xiao, JJ
   Feng, GJ
AF Fang, Zheng
   Liu, Zhen
   Liu, Tingting
   Hung, Chih-Chieh
   Xiao, Jiangjian
   Feng, Guangjin
TI Facial expression GAN for voice-driven face generation
SO VISUAL COMPUTER
LA English
DT Article
DE Expression reconstruction; Cross-model generation; Voice-to-face
   generation; Generative adversarial networks
AB Cross-modal audiovisual generation is an emerging topic in machine learning. In particular, voice-to-face is one of the most popular research branches, which aims to generate faces from human voice clips. Most recent works in voice-to-face generation do not take emotion information into account. However, it could be widely observed that expressions are the key face attributes to reconstruct sharper and more discriminative faces. In this paper, we propose a novel facial expression GAN (FE-GAN) which takes emotion and expressions into account in face generation. To achieve this goal, we use two auxiliary classifiers to learn more emotion and identity representations between different modalities, respectively. Moreover, we design two discriminators, each focusing on a different aspect of the faces, to measure identity and emotion semantic relevance in generating. The triple loss is designed to make FE-GAN robust to voice variety and keep balance in two different modalities. Extensive experiments are conducted on two real datasets to demonstrate the effectiveness of FE-GAN in both quantitative and qualitative perspectives. The experimental results show that FE-GAN can not only outperform the previous models in terms of FID and IS values, but also generate more realistic face images compared with previous models.
C1 [Fang, Zheng; Liu, Zhen; Feng, Guangjin] Ningbo Univ, Fac Elect Engn & Comp Sci, Ningbo, Peoples R China.
   [Liu, Tingting] Ningbo Univ, Coll Sci & Technol, Ningbo, Peoples R China.
   [Hung, Chih-Chieh] Natl Chung Hsing Univ, Taichung, Taiwan.
   [Xiao, Jiangjian] Chinese Acad Sci, Ningbo Inst Mat, Ningbo, Peoples R China.
C3 Ningbo University; Ningbo University; National Chung Hsing University;
   Chinese Academy of Sciences
RP Liu, Z (corresponding author), Ningbo Univ, Fac Elect Engn & Comp Sci, Ningbo, Peoples R China.
EM 1901100018@nbu.edu.cn; liuzhen@nbu.edu.cn
RI Hung, Chih-Chieh/AAA-1158-2019; liu, zhen/AHE-0113-2022
OI Hung, Chih-Chieh/0000-0002-6972-6577; Fang, Zheng/0000-0003-4838-4042
FU National Natural Science Foundation of China [61761166005]; Ministry of
   Science and Technology, Taiwan [MOST 106-2218-E-032-003-MY3]; National
   Natural Science Foundation of Zhejiang [LY20F020007]; Ningbo Science
   Technology Plan projects [2019B10032]; K.C. Wong Magna Fund in Ningbo
   University
FX This work was supported by the National Natural Science Foundation of
   China (Grant 61761166005), Ministry of Science and Technology, Taiwan
   (MOST 106-2218-E-032-003-MY3), and National Natural Science Foundation
   of Zhejiang (Grant LY20F020007), and the Ningbo Science Technology Plan
   projects (Grant 2019B10032) and the K.C. Wong Magna Fund in Ningbo
   University.
CR Aldeneh Z, 2017, INT CONF ACOUST SPEE, P2741, DOI 10.1109/ICASSP.2017.7952655
   [Anonymous], 2017, ICLR
   [Anonymous], 2018, BRIT MACH VIS C BMVC
   Chandrasekar P, 2014, 2014 INTERNATIONAL CONFERENCE ON CIRCUITS, SYSTEMS, COMMUNICATION AND INFORMATION TECHNOLOGY APPLICATIONS (CSCITA), P341, DOI 10.1109/CSCITA.2014.6839284
   Chen C, 2019, AAAI CONF ARTIF INTE, P8142
   Chen LL, 2019, PROC CVPR IEEE, P7824, DOI 10.1109/CVPR.2019.00802
   Chenchah F, 2015, INT J ADV COMPUT SC, V6, P135
   Chung J.S., 2017, BRIT MACH VIS C BMVC
   Dai B, 2017, IEEE I CONF COMP VIS, P2989, DOI 10.1109/ICCV.2017.323
   Duan, ARXIV PREPRINT ARXIV
   Duarte A, 2019, INT CONF ACOUST SPEE, P8633, DOI 10.1109/icassp.2019.8682970
   Dumpala, 2019, ARXIV PREPRINT ARXIV
   Gan Z., 2017, ADV NEUR IN, P5253
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Hensel M, 2017, ADV NEUR IN, V30
   Huang ZW, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P801, DOI 10.1145/2647868.2654984
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Jalalifar S. A., 2018, ARXIV PREPRINT ARXIV
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Li CX, 2017, ADV NEUR IN, V30
   Livi S, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0193508
   Martin O., 2006, 22 INT C DATA ENG WO, P8, DOI [DOI 10.1109/ICDEW.2006.145, 10.1109/ICDEW.2006.145]
   Nagrani A, 2018, PROC CVPR IEEE, P8427, DOI 10.1109/CVPR.2018.00879
   Nasir OR, 2019, 2019 IEEE FIFTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2019), P58, DOI [10.1109/BigMM.2019.00-42, 10.1109/BigMM.2019.00020]
   NGUYEN T., 2017, ADV NEURAL INF PROCE, V30, P2667
   Odena A, 2017, PR MACH LEARN RES, V70
   Oh TH, 2019, PROC CVPR IEEE, P7531, DOI 10.1109/CVPR.2019.00772
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Passricha V, 2020, J INTELL SYST, V29, P1261, DOI 10.1515/jisys-2018-0372
   Pavlovic, 2020, COMPUTER VISION PATT
   Poria S, 2017, INFORM FUSION, V37, P98, DOI 10.1016/j.inffus.2017.02.003
   Qiu Y., 2018, P IEEE C COMP VIS PA, P2510
   Radford A., 2016, INT C LEARN REPR
   Sadoughi N, 2021, IEEE T AFFECT COMPUT, V12, P1031, DOI 10.1109/TAFFC.2019.2916031
   Salimans T, 2016, ADV NEUR IN, V29
   Smith HMJ, 2016, ATTEN PERCEPT PSYCHO, V78, P868, DOI 10.3758/s13414-015-1045-8
   Sriram A, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5639, DOI 10.1109/ICASSP.2018.8462456
   Suwajanakorn S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073640
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Van Segbroeck M, 2013, INTERSPEECH, P704
   Vougioukas K, 2020, INT J COMPUT VISION, V128, P1398, DOI 10.1007/s11263-019-01251-8
   Waghmare V. B., 2014, 5 INT C ADV COMMUNIC
   Wan CH, 2019, INT CONF ACOUST SPEE, P496, DOI [10.1109/ICASSP.2019.8682383, 10.1109/icassp.2019.8682383]
   Watanabe S, 2017, IEEE J-STSP, V11, P1240, DOI 10.1109/JSTSP.2017.2763455
   Wen Y., 2019, Advances in Neural Information Processing Systems, P5266
   Xie Y, 2019, IEEE-ACM T AUDIO SPE, V27, P1675, DOI 10.1109/TASLP.2019.2925934
   Yi Ran, 2020, ARXIV PREPRINT ARXIV
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
NR 49
TC 18
Z9 19
U1 1
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2022
VL 38
IS 3
BP 1151
EP 1164
DI 10.1007/s00371-021-02074-w
EA FEB 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZQ8YX
UT WOS:000620455500002
DA 2024-07-18
ER

PT J
AU Liu, GH
   Li, F
AF Liu, Guohua
   Li, Fei
TI Fabric defect detection based on low-rank decomposition with structural
   constraints
SO VISUAL COMPUTER
LA English
DT Article
DE Fabric defect; Energy feature; Image fusion; Structured sparsity;
   Low-rank decomposition
ID AUTOMATED INSPECTION
AB Fabric defect detection is an important part of the fabric production process. To realize the automatic detection of fabric defects, many algorithms based on machine vision technology have been proposed. However, the defect detection algorithms for patterned fabrics are still not mature enough. This paper proposes a fabric defect detection method based on low-rank decomposition with structural constraints. This method extracts the energy features and then constructs a fusion image of the original image and the energy image to highlight the defective regions. By considering the spatial connection of defective pixels, a new low-rank decomposition model is constructed by introducing the structured sparsity-inducing norm. After the low-rank decomposition, we can get the sparse part containing the defective pixels with high spatial continuity. Finally, we obtain the defect detection result by thresholding the sparse part. Experimental comparisons show that our method is superior to several state-of-the-art fabric defect detection methods.
C1 [Liu, Guohua; Li, Fei] Tiangong Univ, Sch Mech Engn, Tianjin 300387, Peoples R China.
   [Liu, Guohua] Tiangong Univ, Adv Mechatron Equipment Technol Tianjin Major Lab, Tianjin 300387, Peoples R China.
C3 Tiangong University; Tiangong University
RP Liu, GH (corresponding author), Tiangong Univ, Sch Mech Engn, Tianjin 300387, Peoples R China.; Liu, GH (corresponding author), Tiangong Univ, Adv Mechatron Equipment Technol Tianjin Major Lab, Tianjin 300387, Peoples R China.
EM guohualiumail@163.com
FU Tianjin Science and Technology Plan Project [18JCTPJC62700]
FX This research was supported by Tianjin Science and Technology Plan
   Project (Grant No. 18JCTPJC62700). And the database employed in this
   research is provided by Industrial Automation Research Laboratory from
   Department of Electrical and Electronic Engineering of Hong Kong
   University. Link address of the dataset:
   https://ytngan.wordpress.com/codes.
CR Bi L, 2017, VISUAL COMPUT, V33, P1061, DOI 10.1007/s00371-017-1379-4
   Bissi L, 2013, J VIS COMMUN IMAGE R, V24, P838, DOI 10.1016/j.jvcir.2013.05.011
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Cao JJ, 2016, INT J CLOTH SCI TECH, V28, P516, DOI 10.1108/IJCST-10-2015-0117
   Chan CH, 2000, IEEE T IND APPL, V36, P1267, DOI 10.1109/28.871274
   COHEN FS, 1991, IEEE T PATTERN ANAL, V13, P803, DOI 10.1109/34.85670
   [董蓉 Dong Rong], 2016, [纺织学报, Journal of Textile Research], V37, P141
   Doyle L, 2019, VISUAL COMPUT, V35, P1489, DOI 10.1007/s00371-018-1513-y
   Gao GS, 2016, INT CONF SIGN PROCES, P1118, DOI 10.1109/ICSP.2016.7878002
   Guan SQ, 2018, J TEXT I, V109, P1133, DOI 10.1080/00405000.2017.1414669
   Gui Y, 2010, VISUAL COMPUT, V26, P951, DOI 10.1007/s00371-010-0470-x
   Hanbay K, 2016, OPTIK, V127, P11960, DOI 10.1016/j.ijleo.2016.09.110
   Huang HX, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON INFORMATION AND AUTOMATION (IEEE ICIA 2017), P370, DOI 10.1109/ICInfA.2017.8078936
   Krummenacher G, 2018, IEEE T INTELL TRANSP, V19, P1176, DOI 10.1109/TITS.2017.2720721
   [李敏 Li Min], 2015, [纺织学报, Journal of Textile Research], V36, P94
   Li YD, 2017, IEEE T AUTOM SCI ENG, V14, P1256, DOI 10.1109/TASE.2016.2520955
   Lin J., 2018, INT J ADV MANUF TECH, V97, P1
   Lin T, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON INFORMATION AND AUTOMATION (ICIA), P612, DOI 10.1109/ICInfA.2016.7831894
   Mairal J., 2010, NIPS
   Mak KL, 2008, ROBOT CIM-INT MANUF, V24, P359, DOI 10.1016/j.rcim.2007.02.019
   Mak KL, 2009, IMAGE VISION COMPUT, V27, P1585, DOI 10.1016/j.imavis.2009.03.007
   Ng MK, 2014, IEEE T AUTOM SCI ENG, V11, P943, DOI 10.1109/TASE.2014.2314240
   Ngan HYT, 2011, IMAGE VISION COMPUT, V29, P442, DOI 10.1016/j.imavis.2011.02.002
   Ngan HYT, 2005, PATTERN RECOGN, V38, P559, DOI 10.1016/j.patcog.2004.07.009
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Seker A, 2016, 2016 24TH SIGNAL PROCESSING AND COMMUNICATION APPLICATION CONFERENCE (SIU), P1437, DOI 10.1109/SIU.2016.7496020
   Shi MH, 2011, MULTIMED TOOLS APPL, V52, P147, DOI 10.1007/s11042-010-0472-8
   Stark JA, 2000, IEEE T IMAGE PROCESS, V9, P889, DOI 10.1109/83.841534
   Sun JH, 2017, NEUROCOMPUTING, V222, P127, DOI 10.1016/j.neucom.2016.10.018
   Tsang CSC, 2016, PATTERN RECOGN, V51, P378, DOI 10.1016/j.patcog.2015.09.022
   Wei XK, 2020, IEEE T INTELL TRANSP, V21, P947, DOI 10.1109/TITS.2019.2900385
   Yang X, 2005, IEE P-VIS IMAGE SIGN, V152, P715, DOI 10.1049/ip-vis:20045131
   Zhu DD, 2015, AUTEX RES J, V15, P226, DOI 10.1515/aut-2015-0001
   Zhu SC, 2005, INT J COMPUT VISION, V62, P121, DOI 10.1007/s11263-005-4638-1
NR 35
TC 14
Z9 15
U1 4
U2 66
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2022
VL 38
IS 2
BP 639
EP 653
DI 10.1007/s00371-020-02040-y
EA JAN 2021
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZE7IA
UT WOS:000604490300007
DA 2024-07-18
ER

PT J
AU Dua, Y
   Singh, RS
   Kumar, V
AF Dua, Yaman
   Singh, Ravi Shankar
   Kumar, Vinod
TI Compression of multi-temporal hyperspectral images based on RLS filter
SO VISUAL COMPUTER
LA English
DT Article
DE Multi-temporal images; Predictive compression; Lossless compression;
   Hyperspectral image; RLS filter
ID LOSSLESS COMPRESSION
AB The large-scale acquisition of multi-temporal hyperspectral images has increased the demand for a more efficient compression strategy to reduce the large size of such images. In this work, we propose a lossless prediction-based compression technique for multi-temporal images. It removes temporal correlations along with spatial and spectral correlation, reducing the size of time-lapse hyperspectral image significantly. It predicts the pixel value of the target image by a linear combination of pixels from already predicted spectral and temporal bands. The weight matrix used in the prediction is updated using the RLS filter. Experimental results demonstrate the optimal number of bands to be selected for prediction, the comparative strength of individual correlations, and effectiveness of the technique in terms of bit-rate. Our results show that including temporal correlations reduces the bit-rate by 24.07% and our model provides optimization of 18.15% in terms of bits per pixel compared to the state-of-the-art method.
C1 [Dua, Yaman; Singh, Ravi Shankar; Kumar, Vinod] IIT BHU, Dept Comp Sci & Engn, Varanasi, Uttar Pradesh, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology BHU Varanasi (IIT BHU Varanasi)
RP Dua, Y (corresponding author), IIT BHU, Dept Comp Sci & Engn, Varanasi, Uttar Pradesh, India.
EM yamandua.rs.cse18@iitbhu.ac.in
RI Kumar, Dr Vinod/JFS-3232-2023; Kumar, Vinod/HOF-0388-2023; Singh, R
   S/H-6142-2019; Kumar, P.Vinod/HKP-1552-2023
OI Kumar, Dr Vinod/0000-0001-7624-1730; Singh, R S/0000-0001-6358-489X;
   Singh, Ravi Shankar/0000-0002-5394-7551
CR Ballester-Ripoll R, 2016, VISUAL COMPUT, V32, P1433, DOI 10.1007/s00371-015-1130-y
   Cheggoju N, 2018, VISUAL COMPUT, V34, P563, DOI 10.1007/s00371-017-1361-1
   Christopoulos C, 2000, IEEE T CONSUM ELECTR, V46, P1103, DOI 10.1109/30.920468
   CRISP, PRINC REM SENS REM I
   Dua Y, 2020, OPT ENG, V59, DOI 10.1117/1.OE.59.9.090902
   Foster D, 2015, TIME LAPSE HYPERSPEC
   Foster DH, 2016, VISION RES, V120, P45, DOI 10.1016/j.visres.2015.03.012
   Gao ZC, 2011, INT C WAVELET AN JUL, P91
   Gupta S, 2019, CONCURR COMP-PRACT E, V31, DOI 10.1002/cpe.5251
   Huang BM, 2006, PROC SPIE, V6365, DOI 10.1117/12.690659
   Kiely AB, 2009, IEEE T GEOSCI REMOTE, V47, P2672, DOI 10.1109/TGRS.2009.2015291
   Kuanar S., 2019, ARXIV190200855
   Kuanar S, 2018, IEEE INT CONF MULTI
   Kuanar S, 2018, PICT COD SYMP, P164, DOI 10.1109/PCS.2018.8456278
   Kuanar S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2576, DOI 10.1109/ICASSP.2018.8462243
   Licciardi GA, 2020, DATA HANDL SCI TECHN, V32, P55, DOI 10.1016/B978-0-444-63977-6.00004-3
   Magli E, 2004, IEEE GEOSCI REMOTE S, V1, P21, DOI 10.1109/LGRS.2003.822312
   Mamaghanian H, 2011, IEEE T BIO-MED ENG, V58, P2456, DOI 10.1109/TBME.2011.2156795
   Mielikainen J, 2003, IEEE T GEOSCI REMOTE, V41, P2943, DOI 10.1109/TGRS.2003.820885
   Mielikainen J, 2012, IEEE GEOSCI REMOTE S, V9, P1118, DOI 10.1109/LGRS.2012.2191531
   Muñoz-Gómez J, 2010, PROC SPIE, V7810, DOI 10.1117/12.860545
   Nagendran R, 2020, INT J WAVELETS MULTI, V18, DOI 10.1142/S021969131941008X
   NASA, NASA1230B
   Sarkis M, 2009, IEEE IMAGE PROC, P737, DOI 10.1109/ICIP.2009.5414286
   Shen H.-W., 2016, Proceedings of the Eurographics/IEEE VGTC Conference on Visualization: Short Papers, P25
   Shen HD, 2018, J IMAGING, V4, DOI 10.3390/jimaging4120142
   Shen HF, 2014, IEEE T GEOSCI REMOTE, V52, P894, DOI 10.1109/TGRS.2013.2245509
   Song J, 2019, BIG EARTH DATA, V3, P232, DOI 10.1080/20964471.2019.1657720
   Song JW, 2013, ELECTRON LETT, V49, P992, DOI 10.1049/el.2013.1315
   Xi PC, 2020, VISUAL COMPUT, V36, P1869, DOI 10.1007/s00371-019-01775-7
   Zhu W, 2011, IEEE GEOSCI REMOTE S, V8, P416, DOI 10.1109/LGRS.2010.2081661
   Zikiou N, 2020, VISUAL COMPUT, V36, P1473, DOI 10.1007/s00371-019-01753-z
NR 32
TC 11
Z9 11
U1 1
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2022
VL 38
IS 1
BP 65
EP 75
DI 10.1007/s00371-020-02000-6
EA OCT 2020
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YW6DE
UT WOS:000582834600001
DA 2024-07-18
ER

PT J
AU Qu, HZ
   Wang, YW
AF Qu, Hangzhou
   Wang, Yinwei
TI The application of interactive methods under swarm computing and
   artificial intelligence in image retrieval and personalized analysis
SO VISUAL COMPUTER
LA English
DT Article
DE Swarm computing; Artificial intelligence; Interactive methods; Image
   retrieval; Personalized analysis
AB The aim is to explore the interactive methods based on swarm computing and improve the image retrieval effects and personalized recommendation accuracy. The interactive methods based on swarm computing are explored. The mechanism of swarm intelligence (SI) algorithm is analyzed, in which the particle swarm optimization (PSO) algorithm and its improved algorithm are selected. The selected algorithms are combined with content-based image retrieval technology and applied to the image retrieval process, thereby realizing personalized analysis and recommendation based on users' interests. Finally, the image retrieval behaviors of users are analyzed through simulation experiments, which verify the accuracy of the recommendation results. In the six sets of experiments, the image retrieval system based on the quantum behavior PSO (QPSO) has better performance compared to other PSO and SI evolution algorithms. The image retrieval accuracy of the proposed Bayesian personalized ranking (BPR) optimization algorithm (BPR-U2B) has significantly better performance compared to other recommendation algorithms. The QPSO algorithm is the best SI evolution algorithm for image retrieval. The BPR-U2B algorithm is combined with the collaborative filtering algorithm based on BPR. It optimizes the objective function to limit the ranking results of the BPR algorithm, which is beneficial to complete the image recommendations and improve the personalized recommendation effects for users.
C1 [Qu, Hangzhou; Wang, Yinwei] Xijing Univ, Sch Mech Engn, Xian 710100, Peoples R China.
C3 Xijing University
RP Wang, YW (corresponding author), Xijing Univ, Sch Mech Engn, Xian 710100, Peoples R China.
EM 1633036008@qq.com; 445949141@qq.com
OI qu, hangzhou/0000-0002-9706-3889
CR Angelov P, 2017, INT J INTELL SYST, V32, P82, DOI 10.1002/int.21837
   AuBuchon AM, 2019, J SPEECH LANG HEAR R, V62, P1016, DOI 10.1044/2018_JSLHR-H-18-0201
   Barber SJ, 2017, MEM COGNITION, V45, P716, DOI 10.3758/s13421-017-0694-3
   Broilo M, 2010, IEEE T MULTIMEDIA, V12, P267, DOI 10.1109/TMM.2010.2046269
   Carrara F, 2018, INFORM RETRIEVAL J, V21, P208, DOI 10.1007/s10791-017-9318-6
   Clos M, 2019, NEUROPSYCHOPHARMACOL, V44, P555, DOI 10.1038/s41386-018-0246-y
   Conroy P, 2018, BRAIN, V141, P1815, DOI 10.1093/brain/awy087
   de Oña J, 2016, TRANSPORTATION, V43, P725, DOI 10.1007/s11116-015-9615-0
   Haebig E, 2019, J SPEECH LANG HEAR R, V62, P944, DOI 10.1044/2018_JSLHR-L-18-0071
   Kanimozhi T, 2015, NEUROCOMPUTING, V151, P1099, DOI 10.1016/j.neucom.2014.07.078
   Lippert J, 2018, J CLIN ENDOCR METAB, V103, P4511, DOI 10.1210/jc.2018-01348
   Manjula R, 2019, CLUSTER COMPUT, V22, P10567, DOI 10.1007/s10586-017-1125-8
   Marshall J, 2018, APHASIOLOGY, V32, P1054, DOI 10.1080/02687038.2018.1488237
   Memon I, 2015, WIRELESS PERS COMMUN, V80, P1347, DOI 10.1007/s11277-014-2082-7
   Memon MH., 2019, Int J Comput Appl, V41, P449, DOI [10.1080/1206212X.2018.1468643, DOI 10.1080/1206212X.2018.1468643]
   Memon MH, 2017, MULTIMED TOOLS APPL, V76, P15377, DOI 10.1007/s11042-016-3834-z
   Meteyard L, 2018, J SPEECH LANG HEAR R, V61, P658, DOI 10.1044/2017_JSLHR-L-17-0214
   Niforatos Evangelos, 2017, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V1, DOI 10.1145/3090086
   Panchal SS, 2019, ADV J GRAD RES, V6, P31, DOI [10.21467/ajgr.6.1.31-40, DOI 10.21467/AJGR.6.1.31-40]
   Sheikh SR, 2019, INT J TRAIN RES, V17, P170, DOI 10.1080/14480220.2019.1639288
   Varish N, 2020, IEEE ACCESS, V8, P117639, DOI 10.1109/ACCESS.2020.3003911
   Wei GW, 2017, INFORMATICA-LITHUAN, V28, P547, DOI 10.15388/Informatica.2017.144
   Xia ZH, 2016, IEEE T INF FOREN SEC, V11, P2594, DOI 10.1109/TIFS.2016.2590944
NR 23
TC 2
Z9 2
U1 1
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2021
VL 37
IS 8
BP 2331
EP 2340
DI 10.1007/s00371-020-01989-0
EA OCT 2020
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA TT5FX
UT WOS:000582814800001
DA 2024-07-18
ER

PT J
AU Zhu, YS
   Liu, GC
AF Zhu, Yisheng
   Liu, Guangcan
TI Fine-grained action recognition using multi-view attentions
SO VISUAL COMPUTER
LA English
DT Article
DE Multi-view attention; Action recognition; Deep neural networks
AB Inflated 3D ConvNet (I3D) utilizes 3D convolution to enrich semantic information of features, forming a strong baseline for human action recognition. However, 3D convolution extracts features by mixing spatial, temporal and cross-channel information together, lacking the ability to emphasize meaningful features along specific dimensions, especially for the cross-channel information, which is, however, of crucial importance in recognizing fine-grained actions. In this paper, we propose a novel multi-view attention mechanism, named channel-spatial-temporal attention (CSTA) block, to guide the network to pay more attention to the clues useful for fine-grained action recognition. Specifically, CSTA consists of three branches: channel-spatial branch, channel-temporal branch and spatial-temporal branch. By directly plugging these branches into I3D, we further explore the impact of location information as well as the number of blocks in terms of recognition accuracy. We also examine two different strategies for designing a mixture of multiple CSTA blocks. Extensive experiments demonstrate the effectiveness of our CSTA. Namely, while using only RGB frames to train the network, I3D equipped with CSTA (I3D-CSTA) achieves accuracies of 95.76% and 73.97% on UCF101 and HMDB51, respectively. These results are indeed comparable with the results produced by the methods using both RGB frames and optical flow. Even more, with the assistance of optical flow, the recognition accuracies of CSTA-I3D rise to 98.2% on UCF101 and 82.9% on HMDB51, outperforming many state-of-the-art methods.
C1 [Zhu, Yisheng; Liu, Guangcan] Nanjing Univ Informat Sci & Technol, Nanjing, Peoples R China.
C3 Nanjing University of Information Science & Technology
RP Zhu, YS (corresponding author), Nanjing Univ Informat Sci & Technol, Nanjing, Peoples R China.
EM yszhu@nuist.edu.cn
RI Liu, Guangcan/J-1391-2014; Zhu, Yisheng/GYA-3445-2022
OI Liu, Guangcan/0000-0002-9428-4387; Zhu, Yisheng/0000-0001-9742-1984
CR [Anonymous], 2018, IEEE T PATTERN ANAL
   Bilen H, 2018, IEEE T PATTERN ANAL, V40, P2799, DOI 10.1109/TPAMI.2017.2769085
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Diba A., 2016, CoRR
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Girdhar R., 2017, PROC CVPR IEEE, P971, DOI DOI 10.1109/CVPR.2017.337
   Girdhar R., 2017, NIPS, P33
   Hara K., 2018, IEEE INT C COMP VIS
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Hu J., 2017, CoRR
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kay W., 2017, ARXIV170506950
   Kuehne H., 2013, HMDB51 LARGE VIDEO D
   Lan ZZ, 2017, IEEE COMPUT SOC CONF, P1219, DOI 10.1109/CVPRW.2017.161
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Li MK, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON SERVICE OPERATIONS AND LOGISTICS, AND INFORMATICS (SOLI), P200, DOI 10.1109/SOLI.2016.7551687
   Li YL, 2018, PROC CVPR IEEE, P5997, DOI 10.1109/CVPR.2018.00628
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Salakhutdinov, 2015, ARXIV151104119
   Sevilla-Lara L., 2017, CORR
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Tran Du, 2017, ARXIV170805038
   Wang C, 2019, IEEE GEOSCI REMOTE S, V16, P310, DOI 10.1109/LGRS.2018.2872355
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang LM, 2018, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2018.00155
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang X., 2017, PROC CVPR IEEE, P2097, DOI [DOI 10.1109/CVPR.2017.369, 10.1109/CVPR.2017.369]
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wu C., 2017, CoRR
   Yu Z, 2017, VISUAL COMPUT, V34, P1
   Yu ZB, 2018, NEUROCOMPUTING, V317, P50, DOI 10.1016/j.neucom.2018.07.028
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zhang BW, 2016, PROC CVPR IEEE, P2718, DOI 10.1109/CVPR.2016.297
   Zhou B., 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.319
   Zhou B., 2017, CoRR
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 41
TC 14
Z9 16
U1 2
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2020
VL 36
IS 9
BP 1771
EP 1781
DI 10.1007/s00371-019-01770-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NA2KY
UT WOS:000559645500005
DA 2024-07-18
ER

PT J
AU Mallett, I
   Yuksel, C
AF Mallett, Ian
   Yuksel, Cem
TI Constant-time energy-normalization for the Phong specular BRDFs
SO VISUAL COMPUTER
LA English
DT Article
DE Phong; BRDF; Phong BRDF; Modified Phong BRDF; Energy conservation;
   Energy normalization; Physically based rendering
AB The Phong and Modified Phong specular BRDFs, although of limited physical basis, are nevertheless some of the simplest BRDFs exhibiting glossy and specular qualities to understand and to implement, making them useful for validation and teaching. Unfortunately, although it is well-known how to make these BRDFsconserveenergy (that is, never gain energy), making them energy-normalized(that is, never lose nor gain energy) is far more difficult. Lesser-known algorithms exist, but require the specular exponentnto be integer-valued, and haveO(n) runtime cost. We express these algorithms as mathematical formulae and generalize to the real-valued specular exponent case. We then simplify and optimize to finally attain an algorithm that isO(1). Energy normalization makes the Phong BRDFs more physically plausible and therefore both more practically and theoretically useful-and our improvements allow for this energy normalization to be done efficiently and without arbitrary limitations.
C1 [Mallett, Ian; Yuksel, Cem] Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.
C3 Utah System of Higher Education; University of Utah
RP Mallett, I (corresponding author), Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.
EM imallett@cs.utah.edu; cem@cemyuksel.com
OI Mallett, Ian/0000-0002-2505-3649
CR [Anonymous], 1999, BOOST C LIB
   [Anonymous], 1987, SCATTERING ELECTROMA
   [Anonymous], 2010, ACM SIGGRAPH 2010 SI, DOI DOI 10.1145/1833349.1778803
   Arvo J., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P335, DOI 10.1145/218380.218467
   Dutre P, 2003, TECH REP
   Heitz E, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925895
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Kulla C., 2017, SIGGRAPH Course, Physically Based Shading, V2.
   Lecocq P, 2017, IEEE T VIS COMPUT GR, V23, P1428, DOI 10.1109/TVCG.2017.2656889
   Lee JH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275016
   LEWIS RR, 1994, COMPUT GRAPH FORUM, V13, P109, DOI 10.1111/1467-8659.1320109
   Meta, 2009, METANUMERICS METANUM
   NICODEMU.FE, 1965, APPL OPTICS, V4, P767, DOI 10.1364/AO.4.000767
   PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839
   TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105
   Turquin Emmanuel, 2019, Technical Report
NR 16
TC 1
Z9 1
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2029
EP 2038
DI 10.1007/s00371-020-01954-x
EA AUG 2020
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000564994600001
DA 2024-07-18
ER

PT J
AU Song, YJ
   Yang, CL
   Gai, W
   Bian, YL
   Liu, J
AF Song, Yingjie
   Yang, Chenglei
   Gai, Wei
   Bian, Yulong
   Liu, Juan
TI A new storytelling genre: combining handicraft elements and storytelling
   via mixed reality technology
SO VISUAL COMPUTER
LA English
DT Article
DE Handicraft; Storytelling; Tangible user interface; Mixed reality;
   Parent-child interaction
ID SYSTEM; PAPER
AB Handicraft is a traditional cultural form which can improve children's hands-on ability and spatial imagination ability. In addition, storytelling is another beneficial activity that can improve children's language skill and creative power in their growing process. By combining handicraft elements and storytelling via mixed reality technology, this paper presents a new storytelling genre, which supports children and parents learning how to make handicraft and then use them as tangible interaction tools to design, create, tell and share stories, together with gesture interaction. Taking origami as an example, we design and implement a system and conduct a user study. Findings illustrate that our system has good usability and can motivate users' interest in handicraft and storytelling and even promote parent-child interaction effectively.
C1 [Song, Yingjie; Yang, Chenglei; Gai, Wei; Liu, Juan] Shandong Univ, Sch Software, Jinan, Peoples R China.
   [Song, Yingjie; Yang, Chenglei; Gai, Wei; Bian, Yulong] Shandong Univ, Engn Res Ctr Digital Media Technol, MOE, Jinan, Peoples R China.
   [Bian, Yulong; Liu, Juan] Shandong Univ, Sch Mech Elect & Informat Engn, Weihai, Peoples R China.
C3 Shandong University; Shandong University; Shandong University
RP Yang, CL (corresponding author), Shandong Univ, Sch Software, Jinan, Peoples R China.; Yang, CL (corresponding author), Shandong Univ, Engn Res Ctr Digital Media Technol, MOE, Jinan, Peoples R China.
EM chl_yang@sdu.edu.cn; bianyulong_007@126.com
FU National Key Research and Development Program of China [2018YFC0831003];
   National Natural Science Foundation of China [61972233, 61802232]
FX This work is supported by the National Key Research and Development
   Program of China (2018YFC0831003) and the National Natural Science
   Foundation of China (61972233, 61802232).
CR Alofs T, 2015, INT J ARTS TECHNOL, V8, P188, DOI 10.1504/IJART.2015.071206
   Bae J. H., 2013, Journal of International Academy of Physical Therapy Research, V4, P588, DOI [10.5854/JIAPTR.2013.10.25.588, DOI 10.5854/JIAPTR.2013.10.25.588]
   Bayon V., 2003, Virtual Reality, V7, P54, DOI 10.1007/s10055-003-0109-6
   Boakes N., 2008, Mathidues, V1, P1
   Boakes N.J., 2009, RES MIDDLE LEVEL ED, V32, P1, DOI https://doi.org/10.1080/19404476.2009.11462060
   Cao XA, 2010, 2010 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK, P251
   Ch'ng E, 2012, PROCEEDINGS OF THE 2012 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P144, DOI 10.1109/CW.2012.27
   Chang CH, 2018, 2018 7TH INTERNATIONAL CONGRESS ON ADVANCED APPLIED INFORMATICS (IIAI-AAI 2018), P170, DOI 10.1109/IIAI-AAI.2018.00041
   Davis J, 2010, CHEM EDUC RES PRACT, V11, P43, DOI 10.1039/C001044H
   de Lima ES, 2014, ENTERTAIN COMPUT, V5, P33, DOI 10.1016/j.entcom.2013.06.004
   Gutierrez Posada Julian Esteban, 2015, Universal Access in Human-Computer Interaction. Access to Learning, Health and Well-Being. 9th International Conference, UAHCI 2015, held as part of HCI International 2015. Proceedings: LNCS 9177, P89, DOI 10.1007/978-3-319-20684-4_9
   Harris C., 1988, ALVEY VISION C, P147151
   Isbell R., 2004, Early Childhood Education Journal, V32, P157, DOI DOI 10.1023/B:ECEJ.0000048967.94189.A3
   Iyobe M, 2016, 2016 10TH INTERNATIONAL CONFERENCE ON INNOVATIVE MOBILE AND INTERNET SERVICES IN UBIQUITOUS COMPUTING (IMIS), P120, DOI 10.1109/IMIS.2016.45
   Ji Y, 2018, PROCEEDINGS OF CHINESE CHI 2018: SIXTH INTERNATIONAL SYMPOSIUM OF CHINESE CHI (CHINESE CHI 2018), P140, DOI 10.1145/3202667.3202691
   Kara N, 2014, INTERACT LEARN ENVIR, V22, P288, DOI 10.1080/10494820.2011.649767
   Mallan K., 1991, CHILDREN STORYTELLER
   Mi Haipeng, 2010, P 1 INT WORKSH INT S
   Morgan J., 1983, Once upon a time: Using stories in the language classroom
   Raffle H., 2007, P INT C COMPUTER GRA, P137, DOI DOI 10.1145/1297277.1297306
   Raffle H, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1583
   Rizkyandy R, 2017, INT C INT COLL LEARN, P551
   Sauro J, 2016, QUANTIFYING THE USER EXPERIENCE: PRACTICAL STATISTICS FOR USER RESEARCH, 2ND EDITION, P1
   Sin NSM, 2015, J TEKNOL, V75, P81
   Stanton D, 2003, COMPUT-SUPP COLLAB L, V2, P293
   Sud A., AUGMENTING TANGIBLE
   Sugimoto M, 2011, IEEE T LEARN TECHNOL, V4, P249, DOI 10.1109/TLT.2011.13
   Taylor HA, 2013, COGNITION INSTRUCT, V31, P434, DOI 10.1080/07370008.2013.828727
   Teepe R., 2017, STIMULATING PARENT C
   Teepe RC, 2017, J COMPUT ASSIST LEAR, V33, P123, DOI 10.1111/jcal.12169
   Tenbrink T, 2015, J PROBL SOLVING, V8, P2, DOI 10.7771/1932-6246.1154
   Watanabe T., 2012, PROC OPT FIBER COMMU, P1
   Wiwatwattana N, 2016, ADV INTELL SYST, V448, P1101, DOI 10.1007/978-3-319-32467-8_95
NR 33
TC 2
Z9 2
U1 2
U2 37
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2079
EP 2090
DI 10.1007/s00371-020-01924-3
EA JUL 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000554044000002
DA 2024-07-18
ER

PT J
AU Kaur, RP
   Jindal, MK
   Kumar, M
AF Kaur, Rupinder Pal
   Jindal, M. K.
   Kumar, Munish
TI Text and graphics segmentation of newspapers printed in Gurmukhi script:
   a hybrid approach
SO VISUAL COMPUTER
LA English
DT Article
DE Segmentation; Gurmukhi; RLSA; Projection profiles
AB Newspapers are always a standard medium to convey important information to masses of people in recent time as well as in old time. An automated system is required to convert information into a processable form so that information could be searchable. Many efforts have been done on Gurmukhi script documents in typed or written form, but very few articles are present on Gurmukhi script newspaper text recognition or text and image segmentation. Image/graphics segmentation of text is mandatory before feeding newspaper text to OCR for accurate results. In the literature, many techniques have been proposed for segmenting images and text, but many are complex in nature. In this article, the authors have proposed a very simple and effective hybrid approach based on run length smoothing algorithm and projection profile to segment an image from text in Gurmukhi script newspaper articles. Both horizontal and vertical run length smearing algorithm is used for labeling the regions. Logical AND operator is applied to resultant images to identify the text and image regions. To segment the image region among the labeled regions, projection profile technique is implemented. The combination of these two techniques has produced very good results.
C1 [Kaur, Rupinder Pal] Guru Nanak Coll, Dept Comp Applicat, Muktsar, Punjab, India.
   [Jindal, M. K.] Panjab Univ Reg Ctr, Dept Comp Sci & Applicat, Muktsar, Punjab, India.
   [Kumar, Munish] Maharaja Ranjit Singh Punjab Tech Univ, Dept Computat Sci, Bathinda, Punjab, India.
C3 Panjab University
RP Kumar, M (corresponding author), Maharaja Ranjit Singh Punjab Tech Univ, Dept Computat Sci, Bathinda, Punjab, India.
EM munishcse@gmail.com
RI Jindal, Manish/AAU-8820-2021; Kumar, Munish/P-7756-2018
OI Kumar, Munish/0000-0003-0115-1620
CR Acharyya M, 2002, IEEE T CIRC SYST VID, V12, P1117, DOI 10.1109/TCSVT.2002.806812
   Almutairi Abdullah, 2019, 2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA), P1371, DOI 10.1109/ICMLA.2019.00223
   [Anonymous], 2015, THESIS
   Antonacopoulos A., 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P1370, DOI 10.1109/ICDAR.2009.275
   Aparna K.H., 2002, P INT C UN KNOWL LAN, P881
   Barman R., 2020, Journal of Data Mining & Digital Humanities, HistoInformatics, HistoInformatics
   Bouressace H., 2019, IEEE INT S SIGN PROC, P1
   Caponetti L, 2008, APPL SOFT COMPUT, V8, P118, DOI 10.1016/j.asoc.2006.11.008
   Deng SL, 2001, PATTERN RECOGN, V34, P2533, DOI 10.1016/S0031-3203(00)00160-6
   FAN KC, 1994, PATTERN RECOGN LETT, V15, P1201, DOI 10.1016/0167-8655(94)90110-4
   Gatos B., 1999, Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318), P559, DOI 10.1109/ICDAR.1999.791849
   Grover S, 2009, ANNU IEEE IND CONF, P582
   Gupta P., 2000, ICVGIP, P51
   Hadjar K, 2001, PROC INT CONF DOC, P1186, DOI 10.1109/ICDAR.2001.953972
   Haneda E, 2011, IEEE T IMAGE PROCESS, V20, P1611, DOI 10.1109/TIP.2010.2101611
   Jain A. K., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P677, DOI 10.1109/CVPR.1992.223203
   Jindal MK., 2016, INT J ADV RES COMPUT, V7, P16
   Kaur RP, 2017, INT J COMPUT MAT SCI, V6, DOI 10.1142/S2047684117500130
   Khedekar S, 2003, PROC INT CONF DOC, P1265
   Kumar SS, 2016, PROCEDIA COMPUT SCI, V93, P469, DOI 10.1016/j.procs.2016.07.235
   Lee GB, 2007, 9TH INTERNATIONAL CONFERENCE ON ADVANCED COMMUNICATION TECHNOLOGY: TOWARD NETWORK INNOVATION BEYOND EVOLUTION, VOLS 1-3, P395, DOI 10.1109/ICACT.2007.358379
   Lin M., 2006, Journal of South African Computer, V36, P49
   Mao S, 2003, PROC SPIE, V5010, P197, DOI 10.1117/12.476326
   Mitchell PE, 2001, PROC INT CONF DOC, P1181, DOI 10.1109/ICDAR.2001.953971
   Rege PritiP., 2012, ACEEE INT J SIGNAL I, V3, P10
   Roy S, 2015, STUDY MULTIRESOLUTIO
   Safonov IV, 2019, DOCUMENT IMAGE PROCE, P107, DOI DOI 10.1007/978-3-030-05342-0_5
   Su BL, 2013, IEEE T IMAGE PROCESS, V22, P1408, DOI 10.1109/TIP.2012.2231089
   Sun H.-M., 2006, INT J APPL SCI ENG, V4, P297
   WONG KY, 1982, IBM J RES DEV, V26, P647, DOI 10.1147/rd.266.0647
   Wu H.Y., 2019, MULTILAYERED ANAL NE
   Wu V, 1999, IEEE T PATTERN ANAL, V21, P1224, DOI 10.1109/34.809116
NR 32
TC 13
Z9 13
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 1637
EP 1659
DI 10.1007/s00371-020-01927-0
EA JUL 2020
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000550657600001
DA 2024-07-18
ER

PT J
AU Yang, H
   Zhang, ZB
AF Yang, Hang
   Zhang, Zhongbo
TI Depth image upsampling based on guided filter with low gradient
   minimization
SO VISUAL COMPUTER
LA English
DT Article
DE Depth image; Upsampling; Low gradient minimization; Guided filter;
   Regularization method
ID HUMAN ACTIVITY RECOGNITION; RECOVERY; SPARSE; SYSTEM
AB In this paper, we present a novel upsampling framework to enhance the spatial resolution of the depth image. In our framework, the upscaling of a low-resolution depth image is guided by a corresponding intensity images; we formulate it as a cost aggregation problem with the guided filter. However, the guided filter does not make full use of the information of the depth image. Since depth images have quite sparse gradients, it inspires us to regularize the gradients for improving depth upscaling results. Statistics show a special property of depth images, that is, there is a non-ignorable part of pixels whose horizontal or vertical derivatives are equal to +/- 1. Based on this special property, we propose a low gradient regularization method which reduces the penalty for horizontal or vertical derivative +/- 1, and well describes the statistics of the depth image gradients. Then, we present a solution to the low gradient minimization problem based on threshold shrinkage. Finally, the proposed low gradient regularization is integrated with the guided filter into the depth image upsampling method. Experimental results demonstrate the effectiveness of our proposed approach both qualitatively and quantitatively compared with the state-of-the-art methods.
C1 [Yang, Hang] Chinese Acad Sci, Changchun Inst Opt Fine Mech & Phys, Changchun 130033, Peoples R China.
   [Zhang, Zhongbo] Jilin Univ, Dept Math, Changchun 130012, Peoples R China.
C3 Chinese Academy of Sciences; Changchun Institute of Optics, Fine
   Mechanics & Physics, CAS; Jilin University
RP Yang, H (corresponding author), Chinese Acad Sci, Changchun Inst Opt Fine Mech & Phys, Changchun 130033, Peoples R China.
EM yanghang09@mails.jlu.edu.cn
CR [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], EUR C COMP VIS ECCV
   [Anonymous], IEEE INT C ADV VID S
   Diebel J., 2005, P C ADV NEUR INF PRO, P291
   Ding K, 2014, VISUAL COMPUT, V30, P1311, DOI 10.1007/s00371-013-0888-z
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Ferstl D, 2015, IEEE I CONF COMP VIS, P513, DOI 10.1109/ICCV.2015.66
   Ferstl D, 2013, IEEE I CONF COMP VIS, P993, DOI 10.1109/ICCV.2013.127
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   Handa A, 2016, PROC CVPR IEEE, P4077, DOI 10.1109/CVPR.2016.442
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hui TW, 2016, LECT NOTES COMPUT SC, V9907, P353, DOI 10.1007/978-3-319-46487-9_22
   Jalal A, 2012, IEEE T CONSUM ELECTR, V58, P863, DOI 10.1109/TCE.2012.6311329
   Jalal A, 2017, PATTERN RECOGN, V61, P295, DOI 10.1016/j.patcog.2016.08.003
   Jalal A, 2014, SENSORS-BASEL, V14, P11735, DOI 10.3390/s140711735
   Jalal A, 2012, INDOOR BUILT ENVIRON, V21, P184, DOI 10.1177/1420326X11423163
   Jung C, 2017, J VIS COMMUN IMAGE R, V42, P132, DOI 10.1016/j.jvcir.2016.11.009
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276497, 10.1145/1239451.1239547]
   Lasang P, 2016, J VIS COMMUN IMAGE R, V39, P24, DOI 10.1016/j.jvcir.2016.05.006
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Liu MY, 2013, PROC CVPR IEEE, P169, DOI 10.1109/CVPR.2013.29
   Lu JJ, 2015, PROC CVPR IEEE, P2245, DOI 10.1109/CVPR.2015.7298837
   Lu JB, 2012, PROC CVPR IEEE, P430, DOI 10.1109/CVPR.2012.6247705
   Mac Aodha O, 2012, LECT NOTES COMPUT SC, V7574, P71, DOI 10.1007/978-3-642-33712-3_6
   Mahmoudi M, 2012, IEEE T IMAGE PROCESS, V21, P2909, DOI 10.1109/TIP.2012.2185940
   Nguyen RMH, 2015, IEEE I CONF COMP VIS, P208, DOI 10.1109/ICCV.2015.32
   Park J, 2011, IEEE I CONF COMP VIS, P1623, DOI 10.1109/ICCV.2011.6126423
   Riegler G, 2016, LECT NOTES COMPUT SC, V9907, P268, DOI 10.1007/978-3-319-46487-9_17
   Ruiz J, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2717
   Schulter S, 2015, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2015.7299003
   Storath M, 2014, IEEE T SIGNAL PROCES, V62, P3654, DOI 10.1109/TSP.2014.2329263
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Wang YL, 2008, SIAM J IMAGING SCI, V1, P248, DOI 10.1137/080724265
   Wu HY, 2007, IEEE I CONF COMP VIS, P628, DOI 10.1109/cvpr.2007.383211
   Xie J, 2016, IEEE T IMAGE PROCESS, V25, P428, DOI 10.1109/TIP.2015.2501749
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Xue HY, 2017, IEEE T IMAGE PROCESS, V26, P4311, DOI 10.1109/TIP.2017.2718183
   Yang H, 2017, SIGNAL PROCESS, V138, P16, DOI 10.1016/j.sigpro.2017.03.006
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang JY, 2014, IEEE T IMAGE PROCESS, V23, P3443, DOI 10.1109/TIP.2014.2329776
   Yanjie Li, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P152, DOI 10.1109/ICME.2012.30
NR 43
TC 4
Z9 4
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2020
VL 36
IS 7
BP 1411
EP 1422
DI 10.1007/s00371-019-01748-w
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LO1PW
UT WOS:000533401000009
DA 2024-07-18
ER

PT J
AU Gupta, K
   Walia, GS
   Sharma, K
AF Gupta, Keshav
   Walia, Gurjit Singh
   Sharma, Kapil
TI Novel approach for multimodal feature fusion to generate cancelable
   biometric
SO VISUAL COMPUTER
LA English
DT Article
DE Cancelable template; Feature fusion; Multimodal biometric
ID LEVEL FUSION; SCORE LEVEL; FINGERPRINT; RECOGNITION; PERFORMANCE;
   TEMPLATES; SYSTEM
AB Biometric systems provide various benefits over traditional pin-based authentication systems. However, the issue of data privacy and theft is of great concern. To resolve these issues, a novel cancelable multimodal biometric system is proposed that combines multiple traits by means of a projection-based approach. The proposed approach generates a cancelable biometric feature that is used to obtain revocable and noninvertible templates. Cancelable features are generated by projecting the feature points onto a random plane obtained using a user-specific key. The point of projection is then transformed into cylindrical coordinates and a combined cancelable feature is obtained. Extensive experiments are performed over 3 chimeric multimodal databases and results reveal high performance. The average DI and EER achieved by the proposed method are 16.63 and 0.004, respectively. Also, the proposed method is successfully analyzed for privacy concerns, namely revocability, non-invertibility, and unlinkability. Moreover, the proposed system demonstrated tolerance against various security attacks like brute force attacks, attacks via record multiplicity, and substitution attacks.
C1 [Gupta, Keshav] Delhi Technol Univ, Dept Comp Sci, New Delhi, India.
   [Walia, Gurjit Singh] Def Res & Dev Org, SAG, New Delhi, India.
   [Sharma, Kapil] Delhi Technol Univ, Dept Informat Technol, New Delhi, India.
C3 Delhi Technological University; Defence Research & Development
   Organisation (DRDO); Scientific Analysis Group (SAG); Delhi
   Technological University
RP Sharma, K (corresponding author), Delhi Technol Univ, Dept Informat Technol, New Delhi, India.
EM keshavgupta101@gmail.com; gurjit.walia@gmail.com; kapil@ieee.org
RI Gupta, Keshav/HSF-6769-2023
OI Gupta, Keshav/0000-0003-2097-7436; gupta, keshav/0000-0001-7851-1976;
   Sharma, Kapil/0000-0002-4412-7690
CR Abdellatef E., 2019, VISUAL COMPUT, V36, P1
   Ali SS, 2020, PATTERN RECOGN LETT, V129, P263, DOI 10.1016/j.patrec.2019.11.037
   [Anonymous], 2012, INT J SOFTW SCI COMP, DOI DOI 10.4018/jssci.2012070102
   [Anonymous], 2008, IEEE 19 INT C PATT R
   Cappelli R., 2007, Biom. Technol. Today, V15, P7, DOI DOI 10.1016/S0969-4765(07)70140-6
   Chin YJ, 2014, INFORM FUSION, V18, P161, DOI 10.1016/j.inffus.2013.09.001
   Das R, 2019, IEEE T INF FOREN SEC, V14, P360, DOI 10.1109/TIFS.2018.2850320
   Daugman J, 2004, IEEE T CIRC SYST VID, V14, P21, DOI 10.1109/TCSVT.2003.818350
   DAUGMAN JG, 1993, IEEE T PATTERN ANAL, V15, P1148, DOI 10.1109/34.244676
   Dwivedi R, 2019, APPL INTELL, V49, P1016, DOI 10.1007/s10489-018-1311-2
   Farina A, 1999, PATTERN RECOGN, V32, P877, DOI 10.1016/S0031-3203(98)00107-1
   Gomez-Barrero M, 2018, INFORM FUSION, V42, P37, DOI 10.1016/j.inffus.2017.10.003
   Gupta Keshav, 2019, 2019 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS), P387, DOI 10.1109/ICCCIS48478.2019.8974504
   Gupta K, 2019, APPL INTELL, V50, P2824
   Haghighat M, 2016, IEEE T INF FOREN SEC, V11, P1984, DOI 10.1109/TIFS.2016.2569061
   Jin ATB, 2004, PATTERN RECOGN, V37, P2245, DOI 10.1016/j.patcog.2004.04.011
   Jin Z, 2018, IEEE T INF FOREN SEC, V13, P393, DOI 10.1109/TIFS.2017.2753172
   Kacar U, 2019, IET BIOMETRICS, V8, P109, DOI 10.1049/iet-bmt.2018.5065
   Kaur H, 2019, IEEE T INF FOREN SEC, V14, P709, DOI 10.1109/TIFS.2018.2855669
   Kumar A, 2010, PATTERN RECOGN, V43, P1016, DOI 10.1016/j.patcog.2009.08.016
   Kumar N, 2018, APPL INTELL, V48, P2824, DOI 10.1007/s10489-017-1117-7
   Lacbarme P., 2013, 2013 INT C SEC CRYPT, P1
   Maiorana E, 2010, IEEE T SYST MAN CY A, V40, P525, DOI 10.1109/TSMCA.2010.2041653
   Nandakumar K, 2015, IEEE SIGNAL PROC MAG, V32, P88, DOI 10.1109/MSP.2015.2427849
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Ortega-Garcia J, 2003, IEE P-VIS IMAGE SIGN, V150, P395, DOI 10.1049/ip-vis:20031078
   Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P54, DOI 10.1109/MSP.2015.2434151
   Paul PP, 2014, VISUAL COMPUT, V30, P1059, DOI 10.1007/s00371-013-0907-0
   Pillai JK, 2011, IEEE T PATTERN ANAL, V33, P1877, DOI 10.1109/TPAMI.2011.34
   Pillai JK, 2010, INT CONF ACOUST SPEE, P1838, DOI 10.1109/ICASSP.2010.5495383
   Ratha NK, 2007, IEEE T PATTERN ANAL, V29, P561, DOI 10.1109/TPAMI.2007.1004
   Rathgeb C, 2014, COMPUT SECUR, V42, P1, DOI 10.1016/j.cose.2013.12.005
   Ross Arun, 2004, 2004 12th European Signal Processing Conference (EUSIPCO), P1221
   Ross A, 2003, PATTERN RECOGN LETT, V24, P2115, DOI 10.1016/S0167-8655(03)00079-5
   Sadhya D, 2019, IEEE T INF FOREN SEC, V14, P2972, DOI 10.1109/TIFS.2019.2907014
   Savvides M, 2004, INT C PATT RECOG, P922, DOI 10.1109/ICPR.2004.1334679
   Sharma RP, 2019, VISUAL COMPUT, V35, P1393, DOI 10.1007/s00371-018-01618-x
   Sudiro SA, 2007, 2007 IEEE WORKSHOP ON AUTOMATIC IDENTIFICATION ADVANCED TECHNOLOGIES, PROCEEDINGS, P41, DOI 10.1109/AUTOID.2007.380590
   Teoh ABJ, 2006, IEEE T PATTERN ANAL, V28, P1892, DOI 10.1109/TPAMI.2006.250
   Teoh ABJ, 2007, IEEE T SYST MAN CY B, V37, P1096, DOI 10.1109/TSMCB.2007.903538
   Teoh ABJ, 2010, PATTERN ANAL APPL, V13, P301, DOI 10.1007/s10044-009-0158-x
   Trivedi AK, 2020, COMPUT SECUR, V90, DOI 10.1016/j.cose.2019.101690
   Walia GS, 2020, IEEE T INF FOREN SEC, V15, P1945, DOI 10.1109/TIFS.2019.2954779
   Walia GS, 2019, IET BIOMETRICS, V8, P231, DOI 10.1049/iet-bmt.2018.5018
   Wang S, 2014, PATTERN RECOGN, V47, P1321, DOI 10.1016/j.patcog.2013.10.003
   Wang Y, 2010, IEEE T SYST MAN CY B, V40, P1280, DOI 10.1109/TSMCB.2009.2037131
   Wu SC, 2019, IEEE T INF FOREN SEC, V14, P1323, DOI 10.1109/TIFS.2018.2876838
   Xu Q, 2019, INT J HEAT MASS TRAN, V128, P12, DOI 10.1016/j.ijheatmasstransfer.2018.08.122
   Yang BW, 2010, BLOOD COAGUL FIBRIN, V21, P505, DOI 10.1097/MBC.0b013e328339cc5d
   Yang WC, 2018, PATTERN RECOGN, V78, P242, DOI 10.1016/j.patcog.2018.01.026
NR 50
TC 22
Z9 23
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2021
VL 37
IS 6
BP 1401
EP 1413
DI 10.1007/s00371-020-01873-x
EA JUN 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO2LW
UT WOS:000539941100003
DA 2024-07-18
ER

PT J
AU Pandey, G
   Ghanekar, U
AF Pandey, Garima
   Ghanekar, Umesh
TI Classification of priors and regularization techniques appurtenant to
   single image super-resolution
SO VISUAL COMPUTER
LA English
DT Article
DE Single image super-resolution; Neighbor embedding; Modified local linear
   embedding; Moment invariant similarity index
ID MARKOV RANDOM-FIELD; SPARSE REPRESENTATION; KERNEL REGRESSION;
   INTERPOLATION; RECONSTRUCTION; RESTORATION; COMPRESSION; ADMM
AB Image super-resolution (SR) is the process of restoration of high-resolution (HR) image from its degraded images/image. Exigency of high-quality images in different technical fields has led it to be one of the prominent research domains in the area of digital image processing. In SR process, image reconstruction from single low-resolution (LR) image is more onerous process than obtaining it from multi-LR images. Performance of single image SR (SISR) processes is increased by utilizing different image priors in form of regularization. Use of regularization in mathematical equation helps in better visualization and reconstruction of original HR image. Its implementation requires maneuver and is a very important part of the image reconstruction process. In this paper, an attempt has been made to classify the existing techniques based on the priors used. Types of image priors and regularization used so far in the field of SISR have been discussed in detailed, comprising the recent advancements in this area. It also consists of the problems and limitations of the existing image priors and regularizations.
C1 [Pandey, Garima; Ghanekar, Umesh] Natl Inst Technol, Dept Elect & Commun, Kurukshetra, Haryana, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Kurukshetra
RP Pandey, G (corresponding author), Natl Inst Technol, Dept Elect & Commun, Kurukshetra, Haryana, India.
EM garima_6160006@nitkkr.ac.in; ugnitk@nitkkr.ac.in
OI Ghanekar, Umesh/0000-0002-2529-3984
CR Ahmed J, 2016, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-016-0141-6
   [Anonymous], EUROGRAPHICS
   [Anonymous], 2003, IEEE WORKSH STAT COM
   [Anonymous], 2013, INT J COMPUT APPL, DOI [10.5120/10735-5580, DOI 10.5120/10735-5580]
   Bae E, 2010, LECT NOTES COMPUT SC, V5862, P1, DOI 10.1007/978-3-642-11620-9_1
   Bannore V, 2006, LECT NOTES ARTIF INT, V4252, P36
   Bevilacqua M., 2012, IEEE INT C AC SPEECH, DOI [10.1109/icassp.2012.6288125, DOI 10.1109/ICASSP.2012.6288125]
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Chappalli MB, 2005, IEEE SIGNAL PROC LET, V12, P772, DOI 10.1109/LSP.2005.856875
   Choi JS, 2017, IEEE T IMAGE PROCESS, V26, P1300, DOI 10.1109/TIP.2017.2651411
   Cruz C., ARXIV170404126 CORR
   Dai S., 2007, CVPR, P1, DOI DOI 10.1109/CVPR.2007.383028
   Dong WS, 2015, INT J COMPUT VISION, V114, P217, DOI 10.1007/s11263-015-0808-y
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1382, DOI 10.1109/TIP.2012.2231086
   Dong WS, 2011, IEEE I CONF COMP VIS, P1259, DOI 10.1109/ICCV.2011.6126377
   Dong Weisheng, 2011, IEEE Trans Image Process, V20, P1838, DOI 10.1109/TIP.2011.2108306
   Doshi M, 2022, J KING SAUD UNIV-COM, V34, P72, DOI 10.1016/j.jksuci.2018.10.005
   Ebrahimi M, 2007, LECT NOTES COMPUT SC, V4633, P117
   Egiazarian K., 2015, 23 EUR SIGN PROC C E, DOI [10.1109/eusipco.2015.7362905, DOI 10.1109/EUSIPCO.2015.7362905]
   El-Khamy SE, 2005, OPT ENG, V44, DOI 10.1117/1.2042947
   Fahmy G, 2015, IEEE INT S SIGN PROC, DOI [10.1109/isspit.2015.7394412, DOI 10.1109/ISSPIT.2015.7394412]
   Faramarzi E, 2013, IEEE T IMAGE PROCESS, V22, P2101, DOI 10.1109/TIP.2013.2237915
   Farsiu S, 2004, INT J IMAG SYST TECH, V14, P47, DOI 10.1002/ima.20007
   Feng Y., 2017, 2017 IEEE INT C IM P, DOI [10.1109/icip.2017.8297046, DOI 10.1109/ICIP.2017.8297046]
   Fernandez-Granda C, 2013, IEEE I CONF COMP VIS, P3336, DOI 10.1109/ICCV.2013.414
   Ferreira J.C., 2014, 2014 IEEE INT C IM P, DOI [10.1109/icip.2014.7025784, DOI 10.1109/ICIP.2014.7025784]
   Ferstl D, 2015, IEEE I CONF COMP VIS, P513, DOI 10.1109/ICCV.2015.66
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   Gao XB, 2012, IEEE T IMAGE PROCESS, V21, P3194, DOI 10.1109/TIP.2012.2190080
   Gao XB, 2012, IEEE T IMAGE PROCESS, V21, P469, DOI 10.1109/TIP.2011.2161482
   Garrigues P., 2010, Advances in neural information processing systems, V23, P676
   Getreuer P, 2011, IMAGE PROCESS ON LIN, V1, P98
   Gohshi S, 2013, PROCEEDINGS OF THE 10TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND MULTIMEDIA APPLICATIONS (SIGMAP 2013), P71
   Gong WG, 2017, NEUROCOMPUTING, V249, P157, DOI 10.1016/j.neucom.2017.03.067
   Goto T, 2014, INT C PATT RECOG, P4453, DOI 10.1109/ICPR.2014.762
   Gunturk B.K., 2002, P INT C IM PROC, V2, pII, DOI [10.1109/ICIP.2002.1040083, DOI 10.1109/ICIP.2002.1040083]
   Guo K, 2012, IEEE T IMAGE PROCESS, V21, P615, DOI 10.1109/TIP.2011.2165290
   Guo LY, 2012, PROCEEDINGS OF THE XI'AN 2012 INTERNATIONAL CONFERENCE OF SPORT SCIENCE & PHYSICAL EDUCATION, VOL I: SCIENCE AND INNOVATION IN SPORTS, P120
   Haris M, 2017, SIGNAL IMAGE VIDEO P, V11, P1, DOI 10.1007/s11760-016-0880-y
   He H., 2011, CVPR 2011, DOI [10.1109/cvpr.2011.5995713, DOI 10.1109/CVPR.2011.5995713]
   He L, 2013, PROC CVPR IEEE, P345, DOI 10.1109/CVPR.2013.51
   Hu J, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.3.033014
   Hung KW, 2012, IEEE T IMAGE PROCESS, V21, P1061, DOI 10.1109/TIP.2011.2168416
   Jiang J., 2016, 2016 INT C AUD LANG, DOI [10.1109/icalip.2016.7846641, DOI 10.1109/ICALIP.2016.7846641]
   Jiang JJ, 2017, IEEE T MULTIMEDIA, V19, P15, DOI 10.1109/TMM.2016.2599145
   Jiji CV, 2006, EURASIP J APPL SIG P, DOI 10.1155/ASP/2006/73767
   Kim J., 2016, AS PAC SIGN INF PROC, P1, DOI DOI 10.1109/APSIPA.2016.7820852
   Kim KI, 2008, LECT NOTES COMPUT SC, V5096, P456
   Kim KI, 2010, IEEE T PATTERN ANAL, V32, P1127, DOI 10.1109/TPAMI.2010.25
   Kwon Y, 2015, IEEE T PATTERN ANAL, V37, P1792, DOI 10.1109/TPAMI.2015.2389797
   Li JM, 2018, MULTIMED TOOLS APPL, V77, P1693, DOI 10.1007/s11042-017-4399-1
   Li L, 2014, NEUROCOMPUTING, V142, P551, DOI 10.1016/j.neucom.2014.02.045
   Li M, 2008, IEEE T IMAGE PROCESS, V17, P1121, DOI 10.1109/TIP.2008.924289
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537
   Li XY, 2015, IEEE T IMAGE PROCESS, V24, P2874, DOI 10.1109/TIP.2015.2432713
   Li XS, 2018, J VIS COMMUN IMAGE R, V55, P319, DOI 10.1016/j.jvcir.2018.06.012
   Li YY, 2016, INFORM SCIENCES, V372, P196, DOI 10.1016/j.ins.2016.08.049
   Li YB, 2018, IEEE T IMAGE PROCESS, V27, P4638, DOI 10.1109/TIP.2018.2837865
   Li YB, 2015, IEEE I CONF COMP VIS, P450, DOI 10.1109/ICCV.2015.59
   Liu CH, 2016, LECT NOTES COMPUT SC, V9810, P251, DOI 10.1007/978-3-319-42911-3_21
   Liu D, 2016, IEEE T IMAGE PROCESS, V25, P3194, DOI 10.1109/TIP.2016.2564643
   Liu XM, 2019, IEEE T IMAGE PROCESS, V28, P1636, DOI 10.1109/TIP.2018.2875506
   Liu XM, 2011, IEEE T IMAGE PROCESS, V20, P3455, DOI 10.1109/TIP.2011.2150234
   Lu XL, 2019, SIGNAL PROCESS-IMAGE, V70, P157, DOI 10.1016/j.image.2018.09.012
   Lu XL, 2018, J VIS COMMUN IMAGE R, V55, P374, DOI 10.1016/j.jvcir.2018.05.021
   Marquina A, 2008, J SCI COMPUT, V37, P367, DOI 10.1007/s10915-008-9214-8
   Nasrollahi K, 2014, MACH VISION APPL, V25, P1423, DOI 10.1007/s00138-014-0623-4
   Ni KS, 2007, IEEE T IMAGE PROCESS, V16, P1596, DOI 10.1109/TIP.2007.896644
   O'Grady PD, 2005, INT J IMAG SYST TECH, V15, P18, DOI 10.1002/ima.20035
   Osher S, 2005, MULTISCALE MODEL SIM, V4, P460, DOI 10.1137/040605412
   Pandey G, 2018, OPTIK, V166, P147, DOI 10.1016/j.ijleo.2018.03.103
   Park SC, 2003, IEEE SIGNAL PROC MAG, V20, P21, DOI 10.1109/MSP.2003.1203207
   Pickup LC, 2004, ADV NEUR IN, V16, P1587
   Protter M, 2009, IEEE T IMAGE PROCESS, V18, P36, DOI 10.1109/TIP.2008.2008067
   Rahiman VA, 2018, COMPUT ELECTR ENG, V70, P674, DOI 10.1016/j.compeleceng.2017.09.020
   Ren C, 2017, IEEE T IMAGE PROCESS, V26, P90, DOI 10.1109/TIP.2016.2619265
   Ren C, 2016, IEEE T IMAGE PROCESS, V25, P2168, DOI 10.1109/TIP.2016.2542442
   Rhee S., 1999, P INT C IM P, V3, P184, DOI DOI 10.1109/ICIP.1999.817096
   Shabaninia E, 2017, IET COMPUT VIS, V11, P683, DOI 10.1049/iet-cvi.2016.0373
   Shah AJ, 2016, PROCEDIA COMPUT SCI, V85, P100, DOI 10.1016/j.procs.2016.05.186
   Shaham TR, 2016, LECT NOTES COMPUT SC, V9910, P136, DOI 10.1007/978-3-319-46466-4_9
   Shi JH, 2016, IEEE IMAGE PROC, P1424, DOI 10.1109/ICIP.2016.7532593
   Singh A, 2014, INT C PATT RECOG, P4447, DOI 10.1109/ICPR.2014.761
   Sun J, 2008, PROC CVPR IEEE, P2471, DOI 10.1109/CVPR.2008.4587659
   Sun L, 2018, ISPRS INT J GEO-INF, V7, DOI 10.3390/ijgi7100412
   Takeda H, 2006, IEEE IMAGE PROC, P1257, DOI 10.1109/ICIP.2006.312573
   Tang Y, 2017, IEEE T IMAGE PROCESS, V26, P994, DOI [10.1109/TIP.2016.2639440, 10.1109/TIP.2017.2656474]
   Tang Y, 2013, J VIS COMMUN IMAGE R, V24, P148, DOI 10.1016/j.jvcir.2012.02.003
   Tappen B.C.R.M.F., 2003, 3 INT WORKSH STAT CO
   Thapa D, 2016, COMPUT ELECTR ENG, V54, P313, DOI 10.1016/j.compeleceng.2015.09.011
   Tian J, 2011, SIGNAL IMAGE VIDEO P, V5, P329, DOI 10.1007/s11760-010-0204-6
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Traonmilin Y., 2012, RES REPORT
   Vijayan A, 2014, INT J ENG TRENDS TEC, V12, P473, DOI [10.14445/22315381/ijett-v12p289, DOI 10.14445/22315381/IJETT-V12P289]
   Villena S, 2014, DIGIT SIGNAL PROCESS, V32, P1, DOI 10.1016/j.dsp.2014.05.017
   Villena S, 2013, DIGIT SIGNAL PROCESS, V23, P530, DOI 10.1016/j.dsp.2012.10.002
   Vishnukumar S, 2017, OPT COMMUN, V404, P80, DOI 10.1016/j.optcom.2017.05.074
   Wang SL, 2012, PROC CVPR IEEE, P2216, DOI 10.1109/CVPR.2012.6247930
   Wang ZW, 2015, IEEE I CONF COMP VIS, P370, DOI 10.1109/ICCV.2015.50
   Wei XY, 2016, IEEE T IMAGE PROCESS, V25, P3723, DOI 10.1109/TIP.2016.2563178
   Wu W, 2011, J ELECTRON IMAGING, V20, DOI 10.1117/1.3580750
   Xu K, 2018, VISUAL COMPUT, V34, P1065, DOI 10.1007/s00371-018-1554-2
   Yang J, 2008, PROC CVPR IEEE, P173
   Yang JC, 2013, PROC CVPR IEEE, P1059, DOI 10.1109/CVPR.2013.141
   Yang JC, 2012, IEEE T IMAGE PROCESS, V21, P3467, DOI 10.1109/TIP.2012.2192127
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang XM, 2019, VISUAL COMPUT, V35, P1755, DOI 10.1007/s00371-018-1570-2
   Yang YF, 2011, OPT ENG, V50, DOI 10.1117/1.3625417
   Yu S, 2015, J OPT SOC AM A, V32, P2264, DOI 10.1364/JOSAA.32.002264
   Yue LW, 2016, SIGNAL PROCESS, V128, P389, DOI 10.1016/j.sigpro.2016.05.002
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang CP, 2018, SIGNAL PROCESS-IMAGE, V67, P79, DOI 10.1016/j.image.2018.06.001
   Zhang HC, 2010, LECT NOTES COMPUT SC, V6313, P566
   Zhang KB, 2015, IEEE T IMAGE PROCESS, V24, P846, DOI 10.1109/TIP.2015.2389629
   Zhang KB, 2012, IEEE T IMAGE PROCESS, V21, P4544, DOI 10.1109/TIP.2012.2208977
NR 116
TC 11
Z9 11
U1 0
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2020
VL 36
IS 6
BP 1291
EP 1304
DI 10.1007/s00371-019-01729-z
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LI0TO
UT WOS:000529199400016
DA 2024-07-18
ER

PT J
AU Abbass, MY
   Kwon, KC
   Kim, N
   Abdelwahab, SA
   Abd El-Samie, FE
   Khalaf, AAM
AF Abbass, Mohammed Y.
   Kwon, Ki-Chul
   Kim, Nam
   Abdelwahab, Safey A.
   El-Samie, Fathi E. Abd
   Khalaf, Ashraf A. M.
TI Efficient object tracking using hierarchical convolutional features
   model and correlation filters
SO VISUAL COMPUTER
LA English
DT Article
DE Object tracking; Hierarchical convolutional features; Online learning;
   Correlation filters
ID INFRARED TARGET TRACKING
AB Visual object tracking is a very important task in computer vision. This paper develops a method based on the convolutional neural network (CNN) and correlation filters for visual object tracking. To implement a superior tracking method, we develop a multiple correlation tracker. This paper presents an effective method to track an object based on a combination of feature hierarchies of CNNs. We combine several feature hierarchies and compute the more discriminative map to track the object. Firstly, the correlation filters framework is selected to build the new tracker. Secondly, three feature maps from the CNN, which are inserted into the correlation filters framework, are adopted to evaluate the object location independently. Finally, a novel method of feature hierarchies integration based on Kullback-Leibler (KL) divergence is adopted. Experiments on the different sequences are carried out, and the outputs reveal that the proposed tracker has better results than those of the state-of-the-art methods, and it has the ability to handle various challenges.
C1 [Abbass, Mohammed Y.; Abdelwahab, Safey A.] Atom Energy Author, Dept Engn, Nucl Res Ctr, Cairo, Egypt.
   [Kwon, Ki-Chul; Kim, Nam] Chungbuk Natl Univ, Sch Informat & Commun Engn, Cheongju 28644, South Korea.
   [El-Samie, Fathi E. Abd] Menoufia Univ, Dept Elect & Elect Commun, Fac Elect Engn, Menoufia 32952, Egypt.
   [Khalaf, Ashraf A. M.] Menia Univ, Elect & Commun Dept, Fac Engn, Al Minya, Egypt.
C3 Egyptian Knowledge Bank (EKB); Egyptian Atomic Energy Authority (EAEA);
   Chungbuk National University; Egyptian Knowledge Bank (EKB); Menofia
   University; Egyptian Knowledge Bank (EKB); Minia University
RP Kim, N (corresponding author), Chungbuk Natl Univ, Sch Informat & Commun Engn, Cheongju 28644, South Korea.
EM myehiaa@yahoo.com; kwon@osp.chungbuk.ac.kr; namkim@chungbuk.ac.kr;
   safeyash@yahoo.com; fathi_sayed@yahoo.com; ashkhalaf@yahoo.com
RI Sayed, Fathi/HRA-4752-2023; Khalaf, Ashraf ِA. M./X-8289-2018
OI Sayed, Fathi/0000-0001-8749-9518; Khalaf, Ashraf ِA.
   M./0000-0003-3344-5420
FU 'The Cross-Ministry Giga KOREA Project' Grant - Korea government
   [GK17C0200]
FX This work was supported by 'The Cross-Ministry Giga KOREA Project' Grant
   funded by the Korea government (No. GK17C0200, Development of full-3D
   mobile display terminal and its contents).
CR [Anonymous], 2014, EUR C COMP VIS
   [Anonymous], 2014, P EUR C COMP VIS
   [Anonymous], 2016, IEEE C COMP VIS PATT
   [Anonymous], 2017, CVPR
   [Anonymous], 2014, IEEE C COMP VIS PATT
   Asha CS, 2017, INFRARED PHYS TECHN, V85, P114, DOI 10.1016/j.infrared.2017.05.022
   Avidan S, 2007, IEEE T PATTERN ANAL, V29, P261, DOI 10.1109/TPAMI.2007.35
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Chen ZR, 2018, POWDER TECHNOL, V328, P167, DOI 10.1016/j.powtec.2017.12.007
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Danelljan M., 2016, P EUR C COMP VIS, P472
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Dong Wang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1751, DOI 10.1109/ICPR.2010.433
   Fan JL, 2010, IEEE T NEURAL NETWOR, V21, P1610, DOI 10.1109/TNN.2010.2066286
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2017.128
   Galoogahi HK, 2015, PROC CVPR IEEE, P4630, DOI 10.1109/CVPR.2015.7299094
   Grabner H, 2008, LECT NOTES COMPUT SC, V5302, P234, DOI 10.1007/978-3-540-88682-2_19
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   He YJ, 2015, INFRARED PHYS TECHN, V73, P103, DOI 10.1016/j.infrared.2015.09.010
   He Z, 2020, VISUAL COMPUT, V36, P1157, DOI 10.1007/s00371-019-01724-4
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hu WM, 2011, INT J COMPUT VISION, V91, P303, DOI 10.1007/s11263-010-0399-6
   Jiang HL, 2016, NEUROCOMPUTING, V207, P189, DOI 10.1016/j.neucom.2016.03.074
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Leang I, 2018, PATTERN RECOGN, V74, P459, DOI 10.1016/j.patcog.2017.09.026
   Li X, 2016, KNOWL-BASED SYST, V113, P88, DOI 10.1016/j.knosys.2016.09.014
   Li Y., 2014, EUR C COMP VIS
   Li Y, 2015, PROC CVPR IEEE, P353, DOI 10.1109/CVPR.2015.7298632
   Lukezic A., 2017, PROC IEEE C COMPUT V, P6309
   Lukezic A, 2018, INT J COMPUT VISION, V126, P671, DOI 10.1007/s11263-017-1061-3
   Ma C., 2015, P IEEE C COMP VIS PA
   Ma C, 2019, IEEE T PATTERN ANAL, V41, P2709, DOI [10.1109/TPAMI.2018.2865311, 10.1109/INTMAG.2018.8508195]
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Mbelwa JT, 2020, VISUAL COMPUT, V36, P1173, DOI 10.1007/s00371-019-01727-1
   Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tang F, 2007, IEEE I CONF COMP VIS, P992
   Tian SJ, 2020, VISUAL COMPUT, V36, P1219, DOI 10.1007/s00371-019-01730-6
   Valmadre J., 2017, P IEEE C COMP VIS PA, P2805
   Vidanpathirana M, 2020, VISUAL COMPUT, V36, P1501, DOI 10.1007/s00371-019-01757-9
   Wang Q., 2017, arXiv preprint arXiv:1704.04057
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Xu F, 2012, PROCEEDINGS OF THE 10TH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION (WCICA 2012), P4618
   Zhang KH, 2014, IEEE T PATTERN ANAL, V36, P2002, DOI 10.1109/TPAMI.2014.2315808
   Zhang PY, 2019, NEUROCOMPUTING, V337, P129, DOI 10.1016/j.neucom.2019.01.060
   Zhang PY, 2018, LECT NOTES ARTIF INT, V11012, P57, DOI 10.1007/978-3-319-97304-3_5
NR 51
TC 21
Z9 21
U1 1
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2021
VL 37
IS 4
SI SI
BP 831
EP 842
DI 10.1007/s00371-020-01833-5
EA APR 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RL0LI
UT WOS:000527521600001
DA 2024-07-18
ER

PT J
AU Haouchine, N
   Roy, F
   Courtecuisse, H
   Niessner, M
   Cotin, S
AF Haouchine, Nazim
   Roy, Frederick
   Courtecuisse, Hadrien
   Niessner, Matthias
   Cotin, Stephane
TI Calipso: physics-based image and video editing through CAD model proxies
SO VISUAL COMPUTER
LA English
DT Article
DE Video and image manipulations; Interactive editing; Physics-based
   modeling; Scene dynamics
AB We present Calipso, an interactive method for editing images and videos in a physically coherent manner. Our main idea is to realize physics-based manipulations by running a full-physics simulation on proxy geometries given by non-rigidly aligned CAD models. Running these simulations allows us to apply new, unseen forces to move or deform selected objects, change physical parameters such as mass or elasticity, or even add entire new objects that interact with the rest of the underlying scene. In our method, the user makes edits directly in 3D; these edits are processed by the simulation and then transferred to the target 2D content using shape-to-image correspondences in a photo-realistic rendering process. To align the CAD models, we introduce an efficient CAD-to-image alignment procedure that jointly minimizes for rigid and non-rigid alignment while preserving the high-level structure of the input shape. Moreover, the user can choose to exploit image flow to estimate scene motion, producing coherent physical behavior with ambient dynamics. We demonstrate physics-based editing on a wide range of examples producing myriad physical behavior while preserving geometric and visual consistency.
C1 [Haouchine, Nazim; Roy, Frederick; Cotin, Stephane] INRIA, Strasbourg, France.
   [Courtecuisse, Hadrien] Univ Strasbourg, Strasbourg, France.
   [Courtecuisse, Hadrien] CNRS, AVR, ICube, Strasbourg, France.
   [Niessner, Matthias] Stanford Univ, Stanford, CA 94305 USA.
   [Niessner, Matthias] Tech Univ Munich, Visual Comp Lab, Munich, Germany.
C3 Inria; Universites de Strasbourg Etablissements Associes; Universite de
   Strasbourg; Centre National de la Recherche Scientifique (CNRS);
   Universites de Strasbourg Etablissements Associes; Universite de
   Strasbourg; Stanford University; Technical University of Munich
RP Haouchine, N (corresponding author), INRIA, Strasbourg, France.
EM nazim.haouchine@gmail.com
CR Anitescu M, 1999, COMPUT METHOD APPL M, V177, P183, DOI 10.1016/S0045-7825(98)00380-6
   [Anonymous], 2011, INT J MATH MATH SCI, DOI DOI 10.1155/2011/420192
   [Anonymous], 2008, SIGGRAPH 2008 Classes
   [Anonymous], 2008, P COMP GRAPH INT
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Bai JM, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185562
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Barrett WA, 2002, ACM T GRAPHIC, V21, P777, DOI 10.1145/566570.566651
   Bazin JC, 2016, COMPUT GRAPH FORUM, V35, P421, DOI 10.1111/cgf.13039
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Chang CS, 2016, COMPUT GRAPH FORUM, V35, P489, DOI 10.1111/cgf.12849
   Chen T, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508378
   Chen TC, 2009, PROC EUR SOLID-STATE, P1
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   Coquillart S., 1990, Computer Graphics, V24, P187, DOI 10.1145/97880.97900
   Courtecuisse H, 2014, MED IMAGE ANAL, V18, P394, DOI 10.1016/j.media.2013.11.001
   Davis A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818095
   Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191
   Fang H, 2004, ACM T GRAPHIC, V23, P354, DOI 10.1145/1015706.1015728
   Faure F., 2012, Studies in Mechanobiology, Tissue Engineering and Biomaterials,, V11, P283, DOI [DOI 10.1007/8415_2012_125, DOI 10.1007/84152012125]
   Goldberg C, 2012, COMPUT GRAPH FORUM, V31, P265, DOI 10.1111/j.1467-8659.2012.03005.x
   Hara K, 2005, IEEE T PATTERN ANAL, V27, P493, DOI 10.1109/TPAMI.2005.82
   Jain A., 2010, ACM T GRAPHIC, V29, P1, DOI DOI 10.1145/1882261.1866174
   Karst KL, 2011, SUPREME COURT REV, P1
   Khan EA, 2006, ACM T GRAPHIC, V25, P654, DOI 10.1145/1141911.1141937
   Kholgade N, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601209
   Kikuuwe R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1477926.1477934
   Lalonde JF, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276381, 10.1145/1239451.1239454]
   Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6
   Levin D, 2004, MATH VISUAL, P37
   Moin DA, 2016, INT J DENT, V2016, DOI 10.1155/2016/8242535
   Monszpart A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982421
   Nealen A, 2006, COMPUT GRAPH FORUM, V25, P809, DOI 10.1111/j.1467-8659.2006.01000.x
   Oh BM, 2001, COMP GRAPH, P433
   Okabe M, 2018, VISUAL COMPUT, V34, P347, DOI 10.1007/s00371-016-1337-6
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Shao TJ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661288
   Sorkine O, 2007, S GEOM PROC, V4, P109, DOI [10.1145/1073204.1073323, DOI 10.1145/1073204.1073323]
   Tan RT, 2005, IEEE T PATTERN ANAL, V27, P178, DOI 10.1109/TPAMI.2005.36
   Teramoto O, 2010, VISUAL COMPUT, V26, P1339, DOI 10.1007/s00371-009-0405-6
   Theobalt Chris- tian, 2016, P IEEE C COMPUTER VI, DOI DOI 10.1109/CVPR.2016.262
   Thies J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818056
   Xie Junyuan., 2016, Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks, P842
   Zheng YY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185595
   Zhou SZ, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778863
NR 48
TC 5
Z9 5
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2020
VL 36
IS 1
BP 211
EP 226
DI 10.1007/s00371-018-1600-0
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ3OJ
UT WOS:000511966800016
DA 2024-07-18
ER

PT J
AU Wang, B
   Chen, SH
   Wang, J
   Hu, XL
AF Wang, Ben
   Chen, Shuhan
   Wang, Jian
   Hu, Xuelong
TI Residual feature pyramid networks for salient object detection
SO VISUAL COMPUTER
LA English
DT Article
DE Saliency detection; Richer convolutional features; Residual feature
   pyramid networks
AB Existing fully convolutional networks-based salient object detection (SOD) methods are still struggling to detect salient objects of an image in challenging cases due to the incompetent convolutional features, such as complex background, low contrast, multi-tiny objects. To address it, we propose novel residual feature pyramid networks with richer convolutional features for accurate SOD. Specially, we first introduce richer convolutional features to fully exploit multi-scale and multi-level information of objects, which makes it more discriminative for challenging cases. Secondly, based on the powerful stage-wise convolutional features, we further propose residual feature pyramid networks by focusing on the non-predicted regions to learn residual details more effectively and efficiently, which resulted in high-resolution prediction. Experimental results on five standard datasets demonstrate that our model outperforms 17 recent state-of-the-art methods.
C1 [Wang, Ben; Chen, Shuhan; Wang, Jian; Hu, Xuelong] Yangzhou Univ, Sch Informat Engn, Yangzhou, Jiangsu, Peoples R China.
C3 Yangzhou University
RP Chen, SH (corresponding author), Yangzhou Univ, Sch Informat Engn, Yangzhou, Jiangsu, Peoples R China.
EM cn.ben.w@gmail.com; c.shuhan@gmail.com; Haixiaoqu@163.com;
   xlhu@yzu.edu.cn
FU Natural Science Foundation of China [61802336]; Jiangsu Province 7th
   Projects for Summit Talents in Six Main Industries; Electronic
   Information Industry [DZXX-149, 110]; Foundation of Yangzhou University
   [2017CXJ026]
FX This work was supported by the Natural Science Foundation of China (No.
   61802336), Jiangsu Province 7th Projects for Summit Talents in Six Main
   Industries, Electronic Information Industry (DZXX-149, No. 110),
   Foundation of Yangzhou University (No. 2017CXJ026).
CR [Anonymous], IJCAI
   [Anonymous], 2019, DEEPLY SUPERVISED NO
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, IEEE T IMAGE PROCESS, DOI DOI 10.1109/TIP.2015.2487833
   [Anonymous], 2019, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2018.2840724
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], 2014, COMPUT SCI
   [Anonymous], 2014, CVPR
   [Anonymous], 2019, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2019.00404
   [Anonymous], PROC CVPR IEEE
   [Anonymous], IEEE TVCG
   [Anonymous], 2013, CVPR
   [Anonymous], 2018, CVPR
   [Anonymous], SAC NET SPATIAL ATTE
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Choi J, 2018, PROC CVPR IEEE, P479, DOI 10.1109/CVPR.2018.00057
   Dai JF, 2016, LECT NOTES COMPUT SC, V9910, P534, DOI 10.1007/978-3-319-46466-4_32
   Hu P., 2017, CVPR, P2300
   Hu XW, 2018, AAAI CONF ARTIF INTE, P6943
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Ke W, 2017, PROC CVPR IEEE, P302, DOI 10.1109/CVPR.2017.40
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim J, 2016, LECT NOTES COMPUT SC, V9908, P455, DOI 10.1007/978-3-319-46493-0_28
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li GB, 2016, IEEE T IMAGE PROCESS, V25, P5012, DOI 10.1109/TIP.2016.2602079
   Li X, 2018, LECT NOTES COMPUT SC, V11219, P370, DOI 10.1007/978-3-030-01267-0_22
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Lin T.Y., 2017, P IEEE C COMP VIS PA
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu ZY, 2020, VISUAL COMPUT, V36, P843, DOI 10.1007/s00371-019-01659-w
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Shen JB, 2018, IEEE T IMAGE PROCESS, V27, P2688, DOI 10.1109/TIP.2018.2795740
   Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960
   Spurr A, 2018, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2018.00017
   Wang LJ, 2016, CIVIL ENGINEERING AND URBAN PLANNING IV, P825
   Wang MM, 2017, PROC CVPR IEEE, P4800, DOI 10.1109/CVPR.2017.510
   Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang Wenguan, 2018, IEEE Trans Image Process, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2017, IEEE T VIS COMPUT GR, V23, P2014, DOI 10.1109/TVCG.2016.2600594
   Wang WG, 2016, IEEE T IMAGE PROCESS, V25, P5025, DOI 10.1109/TIP.2016.2601784
   Wei YC, 2017, IEEE T PATTERN ANAL, V39, P2314, DOI 10.1109/TPAMI.2016.2636150
   Wenguan Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3395, DOI 10.1109/CVPR.2015.7298961
   Wu RM, 2019, PROC CVPR IEEE, P8142, DOI 10.1109/CVPR.2019.00834
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xu JH, 2019, SPRINGER SER GEOMECH, P1272, DOI 10.1007/978-981-10-7560-5_117
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yao K, 2018, INT CONF NANO MICRO, P234, DOI 10.1109/NEMS.2018.8556873
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang PP, 2017, IEEE I CONF COMP VIS, P212, DOI 10.1109/ICCV.2017.32
   Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
NR 58
TC 19
Z9 22
U1 2
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2020
VL 36
IS 9
BP 1897
EP 1908
DI 10.1007/s00371-019-01779-3
EA DEC 2019
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NA2KY
UT WOS:000541644800001
DA 2024-07-18
ER

PT J
AU Gao, Y
   Li, S
   Qin, H
   Xu, YH
   Hao, AM
AF Gao, Yang
   Li, Shuai
   Qin, Hong
   Xu, Yinghao
   Hao, Aimin
TI An efficient FLIP and shape matching coupled method for fluid-solid and
   two-phase fluid simulations
SO VISUAL COMPUTER
LA English
DT Article
DE FLIP; Shape matching; Position-based dynamics; Solid deformation;
   Immiscible fluid coupling
AB Solid dynamic deformation and multiphase fluid coupling driven by numerical simulation have manifested their significance for many graphics applications during the past 2 decades. For example, the fluid implicit particle (FLIP) method and shape matching constraint based on position-based dynamics (PBD) have demonstrated their unique graphics strength in fluid and solid animation, respectively. In this paper, we propose a novel integrated approach supporting the seamless unification of FLIP and dynamic shape matching. We devise new algorithms to tackle existing difficulties when handling new phenomena such as high-fidelity fluid-solid interactions, solid deformations, melting and immiscible fluid coupling. The key innovation of this paper is a unified Lagrangian framework that seamlessly blends FLIP- and PBD-based shape matching constraints toward the natural yet flexible coupling between fluid and deformable solid. Within our integrated framework, it enables many complicated fluid-solid phenomena with ease. We conduct various kinds of experiments, all the results demonstrate the advantages of our unified hybrid approach toward visual fidelity, computational efficiency, numerical stability, and application versatility.
C1 [Gao, Yang; Xu, Yinghao; Hao, Aimin] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Li, Shuai] Beihang Univ, Qingdao Res Inst, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 Beihang University; Beihang University; State University of New York
   (SUNY) System; State University of New York (SUNY) Stony Brook
RP Li, S (corresponding author), Beihang Univ, Qingdao Res Inst, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
EM gaoyang2963@163.com; lishuaiouc@126.com; qin@cs.stonybrook.edu;
   xuyinghao@buaa.edu.cn; ham_buaa@163.com
RI Gao, Yang/JQV-9627-2023
OI Gao, Yang/0000-0002-9149-3554
FU National Natural Science Foundation of China [61672077, 61532002];
   National Key R&D Program of China [2017YFF0106407]; Applied Basic
   Research Program of Qingdao [161013xx]; National Science Foundation of
   USA [IIS-0949467, IIS-1047715, IIS-1049448]; Academic Excellence
   Foundation of BUAA for Ph.D. Students
FX This research is supported in part by National Natural Science
   Foundation of China (Nos. 61672077 and 61532002), National Key R&D
   Program of China (No. 2017YFF0106407), Applied Basic Research Program of
   Qingdao (No. 161013xx), National Science Foundation of USA (Nos.
   IIS-0949467, IIS-1047715, and IIS-1049448) and the Academic Excellence
   Foundation of BUAA for Ph.D. Students.
CR Akinci N, 2013, COMPUT ANIMAT VIRT W, V24, P195, DOI 10.1002/cav.1499
   Akinci N, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185558
   Allard J., 2011, ACM SIGGRAPH 2001 TA, P52
   Ando R, 2012, IEEE T VIS COMPUT GR, V18, P1202, DOI 10.1109/TVCG.2012.87
   Batty C, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276502
   Bender J, 2014, COMPUT GRAPH FORUM, V33, P228251
   Bender J, 2014, COMPUT GRAPH-UK, V44, P1, DOI 10.1016/j.cag.2014.07.004
   Bender Jan, 2015, TUT P EUR
   Boyd L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2159516.2159522
   Carlson M, 2004, ACM T GRAPHIC, V23, P377, DOI 10.1145/1015706.1015733
   Clausen P, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2451236.2451243
   Cornelis J, 2018, VIS COMPUT
   Cornelis J, 2014, COMPUT GRAPH FORUM, V33, P255, DOI 10.1111/cgf.12324
   Ferstl F, 2016, COMPUT GRAPH FORUM, V35, P225, DOI 10.1111/cgf.12825
   Gagnon J, 2011, VISUAL COMPUT, V27, P451, DOI 10.1007/s00371-011-0584-9
   Gao Y, 2017, GRAPH MODELS, V94, P14, DOI 10.1016/j.gmod.2017.09.001
   Gao Y, 2017, INT CONF ADV CLOUD B, P111, DOI 10.1109/CBD.2017.27
   Gerszewski D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508430
   Ihmsen Markus, 2014, P 35 ANN C EUR ASS C, DOI [10.2312/egst.20141034, DOI 10.2312/EGST.20141034]
   Keiser R., 2005, UNIFIED LAGRANGIAN A, V2005, P125
   Lenaerts T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360648
   Lenaerts T, 2009, COMPUT GRAPH FORUM, V28, P213, DOI 10.1111/j.1467-8659.2009.01360.x
   Lii SY, 2014, VISUAL COMPUT, V30, P531, DOI 10.1007/s00371-013-0878-1
   Macklin M, 2014, ACM T GRAPHIC, V33, DOI [10.1145/280/109/2601152, 10.1145/2601097.2601152]
   Macklin M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461984
   Martinek M, 2015, VISUAL COMPUT, V31, P223, DOI 10.1007/s00371-014-1040-4
   Misztal M.K, 2012, P ACM SIGGRAPH EUR S
   M┬u┬aller M., 2016, POSITION BASED SIMUL
   Müller M, 2007, J VIS COMMUN IMAGE R, V18, P109, DOI 10.1016/j.jvcir.2007.01.005
   Müller M, 2005, ACM T GRAPHIC, V24, P471, DOI 10.1145/1073204.1073216
   Muller Matthias, 2014, P EG S COMP AN, V2, P149, DOI DOI 10.1016/J.JVCIR.2007.01.005
   Peer A, 2017, IEEE T VIS COMPUT GR, V23, P2656, DOI 10.1109/TVCG.2016.2636144
   Selino A, 2013, COMPUT GRAPH FORUM, V32, P75, DOI 10.1111/j.1467-8659.2012.03232.x
   Shao X, 2015, COMPUT GRAPH FORUM, V34, P191, DOI 10.1111/cgf.12467
   Tournier M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766969
   Wong TH, 2014, VISUAL COMPUT, V30, P729, DOI 10.1007/s00371-014-0954-1
   Yan X, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925897
   Yang LP, 2014, COMPUT GRAPH FORUM, V33, P199, DOI 10.1111/cgf.12488
   Yang LP, 2012, COMPUT GRAPH FORUM, V31, P2037, DOI 10.1111/j.1467-8659.2012.03196.x
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
NR 40
TC 2
Z9 5
U1 2
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2019
VL 35
IS 12
BP 1741
EP 1753
DI 10.1007/s00371-018-1569-8
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KI6XQ
UT WOS:000511494300005
DA 2024-07-18
ER

PT J
AU Tian, L
   Liu, J
   Ling, HB
   Guo, W
AF Tian, Liang
   Liu, Jing
   Ling, Haibin
   Guo, Wei
TI Disparity estimation in stereo video sequence with adaptive
   spatiotemporally consistent constraints
SO VISUAL COMPUTER
LA English
DT Article
DE Spatiotemporally consistent; Adaptive temporal segment confidence;
   Adaptive temporal predicted disparity; Reliable temporal neighborhood
ID DEPTH MAPS RECOVERY; IMAGE; SEGMENTATION; COST; FLOW
AB Numerous stereo matching algorithms have been proposed to obtain disparity estimation for a single pair of stereo images. However, simply even applying the best of them to temporal frames independently, i.e., without considering the temporal consistency between consecutive frames, may suffer from the undesirable artifacts. Here, we proposed an adaptive, spatiotemporally consistent, constraints-based systematic method that generates spatiotemporally consistent disparity maps for stereo video image sequences. Firstly, a reliable temporal neighborhood is used to enforce the "self-similarity" assumption and prevent errors caused by false optical flow matching from propagating between consecutive frames. Furthermore, we formulate the adaptive temporal predicted disparity map as prior knowledge of the current frame. It is used as a soft constraint to enhance the temporal consistency of disparities, increase the robustness to luminance variance, and restrict the range of the potential disparities for each pixel. Additionally, to further strengthen smooth variation of disparities, the adaptive temporal segment confidence is incorporated as a soft constraint to reduce ambiguities caused by under- and over-segmentation, and retain the disparity discontinuities that align with 3D object boundaries from geometrically smooth, but strong color gradient regions. Experimental evaluations demonstrate that our method significantly improves the spatiotemporal consistency both quantitatively and qualitatively compared with other state-of-the-art methods on the synthetic DCB and realistic KITTI datasets.
C1 [Tian, Liang; Liu, Jing; Guo, Wei] Hebei Normal Univ, Coll Math & Informat Sci, Key Lab Augmented Real, 20 Rd East,2nd Ring South, Shijiazhuang 050024, Hebei, Peoples R China.
   [Ling, Haibin] Temple Univ, Dept Comp & Informat Sci, Ctr Data Analyt & Biomed Informat, 382 SERC Bldg,1925 North 12th St, Philadelphia, PA 19122 USA.
C3 Hebei Normal University; Pennsylvania Commonwealth System of Higher
   Education (PCSHE); Temple University
RP Liu, J (corresponding author), Hebei Normal Univ, Coll Math & Informat Sci, Key Lab Augmented Real, 20 Rd East,2nd Ring South, Shijiazhuang 050024, Hebei, Peoples R China.
EM liujing01@ict.ac.cn
RI Liu, Jing/T-6504-2019
OI Liu, Jing/0000-0002-2217-0372
FU National Natural Science Foundation of China [61802109]; Natural Science
   Foundation of Hebei province [F2017205066]; Science Foundation of Hebei
   Normal University [L2017B06, L2018K02]
FX This study was funded by the National Natural Science Foundation of
   China (Grant No.: 61802109), the Natural Science Foundation of Hebei
   province (Grant No.: F2017205066), the Science Foundation of Hebei
   Normal University (Grant No.: L2017B06, L2018K02).
CR [Anonymous], 2016, ARXIV170100165
   [Anonymous], ARXIV160309302
   [Anonymous], 2016, BMVC
   [Anonymous], 2016, J MACH LEARN RES
   Bartczak B, 2008, REAL TIME NEIGHBORHO, P153
   Cech J., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3129, DOI 10.1109/CVPR.2011.5995442
   Chen ZY, 2015, IEEE I CONF COMP VIS, P972, DOI 10.1109/ICCV.2015.117
   Dahan MJ, 2012, VISUAL COMPUT, V28, P1181, DOI 10.1007/s00371-011-0667-7
   Davis J, 2003, PROC CVPR IEEE, P359
   Dobias M., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P704, DOI 10.1109/ICCVW.2011.6130317
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gidaris S., 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, P5248
   Gong ML, 2009, COMPUT VIS IMAGE UND, V113, P90, DOI 10.1016/j.cviu.2008.07.007
   Güney F, 2015, PROC CVPR IEEE, P4165, DOI 10.1109/CVPR.2015.7299044
   Guerrero P, 2018, VISUAL COMPUT, V34, P1165, DOI 10.1007/s00371-018-1551-5
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Hosni A, 2012, TEMPORALLY CONSISTEN, P165
   Hung CH, 2013, INT J COMPUT VISION, V102, P271, DOI 10.1007/s11263-012-0559-y
   Jiang J, 2014, NEUROCOMPUTING, V142, P335, DOI 10.1016/j.neucom.2014.04.027
   Kendall A., 2017, ARXIV170304309
   Khoshabeh R, 2011, INT CONF ACOUST SPEE, P885
   Kordelas GA, 2014, IEEE IMAGE PROC, P3803, DOI 10.1109/ICIP.2014.7025772
   Larsen ES, 2007, IEEE I CONF COMP VIS, P1440
   Li LC, 2017, APPL OPTICS, V56, P3411, DOI 10.1364/AO.56.003411
   Li X., 2016, ICIP
   Li YJ, 2019, VISUAL COMPUT, V35, P257, DOI 10.1007/s00371-018-1491-0
   Liu F., 2009, P BRIT MACH VIS C
   Liu J, 2016, VISUAL COMPUT, V32, P989, DOI 10.1007/s00371-016-1228-x
   Liu J, 2015, VISUAL COMPUT, V31, P1253, DOI 10.1007/s00371-014-1009-3
   LUO WJ, 2016, PROC CVPR IEEE, P5695, DOI DOI 10.1109/CVPR.2016.614
   Matsuo Takuya, 2013, Proceedings of the 8th International Conference on Computer Vision Theory and Applications. VISAPP 2013, P300
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925
   Min D., 2010, 3DTV CON JUN, P1
   Min DB, 2012, IEEE T IMAGE PROCESS, V21, P1176, DOI 10.1109/TIP.2011.2163164
   Pham CC, 2012, IEEE IMAGE PROC, P2965, DOI 10.1109/ICIP.2012.6467522
   Qi F, 2017, MULTIMED TOOLS APPL, V76, P3087, DOI 10.1007/s11042-015-3229-6
   Richardt C, 2010, LECT NOTES COMPUT SC, V6313, P510
   Shih-Hung Lin, 2014, 2014 International Conference on Information Science, Electronics and Electrical Engineering (ISEEE), P1897, DOI 10.1109/InfoSEEE.2014.6946252
   Sizintsev M, 2009, PROC CVPR IEEE, P493, DOI 10.1109/CVPRW.2009.5206728
   Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939
   Taniai T, 2017, PROC CVPR IEEE, P6891, DOI 10.1109/CVPR.2017.729
   Vogel C, 2015, INT J COMPUT VISION, V115, P1, DOI 10.1007/s11263-015-0806-0
   Vogel C, 2014, LECT NOTES COMPUT SC, V8692, P263, DOI 10.1007/978-3-319-10593-2_18
   Vretos N, 2014, IEEE IMAGE PROC, P3798, DOI 10.1109/ICIP.2014.7025771
   Wedel A, 2011, INT J COMPUT VISION, V95, P29, DOI 10.1007/s11263-010-0404-0
   Xing GY, 2016, VISUAL COMPUT, V32, P1013, DOI 10.1007/s00371-016-1238-8
   Xu SB, 2015, IEEE T IMAGE PROCESS, V24, P2182, DOI 10.1109/TIP.2015.2416654
   Yamaguchi K, 2014, LECT NOTES COMPUT SC, V8693, P756, DOI 10.1007/978-3-319-10602-1_49
   Yang WZ, 2012, PROC CVPR IEEE, P1466, DOI 10.1109/CVPR.2012.6247835
   Zeng HQ, 2012, IEEE IMAGE PROC, P3017, DOI 10.1109/ICIP.2012.6467535
   Zhang GF, 2009, IEEE T PATTERN ANAL, V31, P974, DOI 10.1109/TPAMI.2009.52
   Zhu SP, 2017, VISUAL COMPUT, V33, P1087, DOI 10.1007/s00371-016-1264-6
NR 53
TC 3
Z9 3
U1 3
U2 19
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2019
VL 35
IS 10
BP 1427
EP 1446
DI 10.1007/s00371-018-01622-1
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IU7PN
UT WOS:000483775900007
DA 2024-07-18
ER

PT J
AU Qian, X
   Li, XM
   Zhang, CM
AF Qian, Xin
   Li, Xuemei
   Zhang, Caiming
TI Weighted superpixel segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Superpixel; Boundary; Weight; Search range; Merge
ID CLASSIFICATION
AB Image boundaries and regularity are two important factors in superpixel segmentation. Balancing the influence of image boundaries and regularity is key to producing superpixels. In this paper, we present a novel superpixel segmentation algorithm, called weighted superpixel segmentation (WSS), which is capable of generating superpixels with high boundary adherence and regular shape in a linear time. In WSS, superpixels are generated according to a distance metric defined by the combination of a weight function term, color distance term and plane distance term. Unlike other superpixel algorithms, the weight function is calculated for each pixel to determine the weight of the color distance term and plane distance term in the distance metric. To increase superpixel regularity, superpixel seeds are initialized in a hexagonal manner. Then, we use the distance metric to obtain the initial superpixels. Determining the seed search range is an essential factor to improve algorithm accuracy; thus, a dynamic circle search range is designed in our algorithm that can provide better superpixel results. Finally, a merging strategy is applied to obtain the final superpixels and ensure that the number of superpixels agrees with expectations. Experimental results demonstrate that WSS performs as well as or even better than the existing methods in terms of several commonly used evaluation metrics in superpixel segmentation.
C1 [Qian, Xin; Zhang, Caiming] Shandong Univ, Sch Comp Sci & Technol, Jinan 250101, Shandong, Peoples R China.
   [Li, Xuemei] Shandong Univ, Sch Comp Sci & Technol, GD&IV Lab, Jinan, Shandong, Peoples R China.
   [Li, Xuemei; Zhang, Caiming] Shandong Co Innovat Ctr Future Intelligent Comp, Yantai 264025, Peoples R China.
   [Zhang, Caiming] Shandong Univ Finance & Econ, Shandong Prov Key Lab Digital Media Technol, Jinan 250061, Shandong, Peoples R China.
C3 Shandong University; Shandong University; Shandong University of Finance
   & Economics
RP Li, XM (corresponding author), Shandong Univ, Sch Comp Sci & Technol, GD&IV Lab, Jinan, Shandong, Peoples R China.; Li, XM (corresponding author), Shandong Co Innovat Ctr Future Intelligent Comp, Yantai 264025, Peoples R China.
EM 824547936@qq.com; xmli@sdu.edu.cn; czhang@sdu.edu.cn
RI Cheng, Lin/KFQ-3111-2024
FU NSFC Joint Fund with Zhejiang Integration of Informatization and
   Industrialization [U1609218]; NSFC [61572292, 61332015, 61602277,
   61873117]
FX This work was supported partly by the NSFC Joint Fund with Zhejiang
   Integration of Informatization and Industrialization under Key Project
   under Grant No. U1609218 and NSFC under Grant Nos. 61572292, 61332015,
   61602277, 61873117.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   [Anonymous], 2007, P IEEE COMP VIS PATT
   Bódis-Szomorú A, 2015, PROC CVPR IEEE, P2011, DOI 10.1109/CVPR.2015.7298812
   Chen JS, 2017, IEEE T IMAGE PROCESS, V26, P3317, DOI 10.1109/TIP.2017.2651389
   Chen WX, 2018, IEEE INT C BIOINFORM, P979, DOI 10.1109/BIBM.2018.8621244
   Cheng J, 2013, IEEE T MED IMAGING, V32, P1019, DOI 10.1109/TMI.2013.2247770
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   He XM, 2006, LECT NOTES COMPUT SC, V3951, P338
   Jampani V., 2018, P EUR C COMP VIS ECC, P352
   Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96
   Li ZG, 2012, PROC CVPR IEEE, P789, DOI [10.1109/CVPR.2012.6247750, 10.1109/ISRA.2012.6219309]
   Lim J, 2014, LECT NOTES COMPUT SC, V8693, P173, DOI 10.1007/978-3-319-10602-1_12
   Liu B, 2013, IEEE T GEOSCI REMOTE, V51, P907, DOI 10.1109/TGRS.2012.2203358
   Liu L, 2018, ARXIV180902165
   Ma B, 2016, IEEE T IMAGE PROCESS, V25, P4199, DOI 10.1109/TIP.2016.2588329
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Moore AP, 2010, PROC CVPR IEEE, P2117, DOI 10.1109/CVPR.2010.5539890
   Mori G, 2004, PROC CVPR IEEE, P326
   Nwogu I., 2008, IEEE C COMP VIS PATT, P1
   Pan X, 2017, IEEE T VIS COMPUT GR, V23, P2342, DOI 10.1109/TVCG.2016.2621763
   Peng B, 2011, IEEE T IMAGE PROCESS, V20, P3592, DOI 10.1109/TIP.2011.2157512
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10
   Sankaranarayanan S, 2018, PROC CVPR IEEE, P3752, DOI 10.1109/CVPR.2018.00395
   Shen JB, 2016, IEEE T IMAGE PROCESS, V25, P5933, DOI 10.1109/TIP.2016.2616302
   Shen JB, 2014, IEEE T IMAGE PROCESS, V23, P1451, DOI 10.1109/TIP.2014.2302892
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Tighe J, 2010, LECT NOTES COMPUT SC, V6315, P352, DOI 10.1007/978-3-642-15555-0_26
   Tu WC, 2018, PROC CVPR IEEE, P568, DOI 10.1109/CVPR.2018.00066
   Van den Bergh M, 2012, LECT NOTES COMPUT SC, V7578, P13, DOI 10.1007/978-3-642-33786-4_2
   Veksler O, 2010, LECT NOTES COMPUT SC, V6315, P211, DOI 10.1007/978-3-642-15555-0_16
   Wang S, 2011, IEEE I CONF COMP VIS, P1323, DOI 10.1109/ICCV.2011.6126385
   Wang WG, 2016, IEEE T IMAGE PROCESS, V25, P5025, DOI 10.1109/TIP.2016.2601784
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P4185, DOI 10.1109/TIP.2015.2460013
   Wang ZL, 2013, IEEE T IMAGE PROCESS, V22, P4341, DOI 10.1109/TIP.2013.2272514
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Zhang YX, 2017, IEEE T CIRC SYST VID, V27, P1502, DOI 10.1109/TCSVT.2016.2539839
   Zhou YZ, 2018, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2018.00399
   Zhou YJ, 2015, IEEE T IMAGE PROCESS, V24, P3834, DOI 10.1109/TIP.2015.2449552
   Zhou YF, 2017, IEEE T CIRC SYST VID, V27, P2281, DOI 10.1109/TCSVT.2016.2589781
NR 42
TC 8
Z9 8
U1 2
U2 15
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 985
EP 996
DI 10.1007/s00371-019-01682-x
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200018
OA Bronze
DA 2024-07-18
ER

PT J
AU Wang, ZY
   Qiu, SY
   Chen, QY
   Trayan, N
   Ringlein, A
   Dorsey, J
   Rushmeier, H
AF Wang, Zeyu
   Qiu, Shiyu
   Chen, Qingyang
   Trayan, Natallia
   Ringlein, Alexander
   Dorsey, Julie
   Rushmeier, Holly
TI AniCode: authoring coded artifacts for network-free personalized
   animations
SO VISUAL COMPUTER
LA English
DT Article
DE Authoring time-based media; Encoding animations; Personalized
   demonstrations; Network-free communication
AB Time-based media are used in applications ranging from demonstrating the operation of home appliances to explaining new scientific discoveries. However, creating effective time-based media is challenging. We introduce a new framework for authoring and consuming time-based media. An author encodes an animation in a printed code and affixes the code to an object. A consumer captures an image of the object through a mobile application, and the image together with the code is used to generate a video on their local device. Our system is designed to be low cost and easy to use. By not requiring an Internet connection to deliver the animation, the framework enhances privacy of the communication. By requiring the user to have a direct line-of-sight view of the object, the framework provides personalized animations that only decode in the intended context. Animation schemes in the system include 2D and 3D geometric transformations, color transformation, and annotation. We demonstrate the new framework with sample applications from a wide range of domains. We evaluate the ease of use and effectiveness of our system with a user study.
C1 [Wang, Zeyu; Qiu, Shiyu; Chen, Qingyang; Trayan, Natallia; Ringlein, Alexander; Dorsey, Julie; Rushmeier, Holly] Yale Univ, Dept Comp Sci, 51 Prospect St, New Haven, CT 06511 USA.
C3 Yale University
RP Qiu, SY (corresponding author), Yale Univ, Dept Comp Sci, 51 Prospect St, New Haven, CT 06511 USA.
EM zeyu.wang@yale.edu; sherry.qiu@yale.edu; qingyang.chen@yale.edu;
   natallia.trayan@yale.edu; alexander.ringlein@yale.edu;
   julie.dorsey@yale.edu; holly.rushmeier@yale.edu
OI Rush, Alexander/0000-0002-9900-1606; Rushmeier,
   Holly/0000-0001-5241-0886; Wang, Zeyu/0000-0001-5374-6330
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Agrawala M, 2003, ACM T GRAPHIC, V22, P828, DOI 10.1145/882262.882352
   Agrawala M, 2011, COMMUN ACM, V54, P60, DOI 10.1145/1924421.1924439
   Andolina S, 2012, 2012 SEVENTH INTERNATIONAL CONFERENCE ON BROADBAND, WIRELESS COMPUTING, COMMUNICATION AND APPLICATIONS (BWCCA 2012), P322, DOI 10.1109/BWCCA.2012.60
   [Anonymous], 2002, Cultural Tourism: The Partnership Between Tourism and Cultural Heritage Management
   [Anonymous], 2018, The GuardianJuly 11
   Appiah O., 2006, Journal of Current Issues Research in Advertising, V28, P73, DOI DOI 10.1080/10641734.2006.10505192
   Ashok A., 2014, THESIS
   Badam SK, 2019, INFORM VISUAL, V18, P68, DOI 10.1177/1473871617725907
   Barak M, 2011, COMPUT EDUC, V56, P839, DOI 10.1016/j.compedu.2010.10.025
   Carter S, 2015, IEEE PERVAS COMPUT, V14, P44, DOI 10.1109/MPRV.2015.59
   Carter S, 2014, EDUC INF TECHNOL, V19, P637, DOI 10.1007/s10639-013-9276-6
   Chang CS, 2016, COMPUT GRAPH FORUM, V35, P489, DOI 10.1111/cgf.12849
   Cho NH, 2016, 2016 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA), P711
   Chu J, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P250, DOI 10.1145/3083187.3084015
   Clarine Brenna., 2016, 11 Reasons Why Video is Better Than Any Other Medium
   FEINER SK, 1991, COMPUTER, V24, P33, DOI 10.1109/2.97249
   Fidas Christos., 2015, 2015 6th International Conference on Information, Intelligence, Systems and Applications (IISA), P1, DOI DOI 10.1109/IISA.2015.7388029
   Kaplan AM, 2010, BUS HORIZONS, V53, P59, DOI 10.1016/j.bushor.2009.09.003
   Karat CM, 2001, HUMAN-COMPUTER INTERACTION - INTERACT'01, P455
   Li DZY, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P449, DOI 10.1145/3126594.3126635
   Li ZQ, 2015, PROC CVPR IEEE, P1356, DOI 10.1109/CVPR.2015.7298741
   Liao Isaac, 2014, Smart Graphics. 12th International Symposium (SG 2014). Proceedings: LNCS 8698, P1, DOI 10.1007/978-3-319-11650-1_1
   Lin SS, 2015, IEEE T MULTIMEDIA, V17, P1515, DOI 10.1109/TMM.2015.2437711
   OpenCV team, 2017, OPENCV ANDR SDK
   Owen S., 2017, ZXING BARCODE SCANNI
   Parent R., 2012, Computer animation: algorithms and techniques
   Revell T., 2017, New Sci
   Schnotz W, 2002, EDUC PSYCHOL REV, V14, P101, DOI 10.1023/A:1013136727916
   Telea A., 2004, Journal of Graphics Tools, V9, P23, DOI 10.1080/10867651.2004.10487596
   UPSON C, 1989, IEEE COMPUT GRAPH, V9, P30, DOI 10.1109/38.31462
   Van den Bergh M, 2012, LECT NOTES COMPUT SC, V7578, P13, DOI 10.1007/978-3-642-33786-4_2
   Wouters P, 2008, REV EDUC RES, V78, P645, DOI 10.3102/0034654308320320
   Xiao C, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3152823
   Yeshurun Y, 1998, NATURE, V396, P72, DOI 10.1038/23936
   Yuan W., 2011, CVPR 2011 workshops, P37
   Yue YT, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P427, DOI 10.1145/3126594.3126601
   Zheng YY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185595
NR 38
TC 1
Z9 1
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 885
EP 897
DI 10.1007/s00371-019-01681-y
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200010
OA Bronze
DA 2024-07-18
ER

PT J
AU Mbelwa, JT
   Zhao, QJ
   Lu, Y
   Liu, H
   Wang, FS
   Mbise, M
AF Mbelwa, Jimmy T.
   Zhao, Qingjie
   Lu, Yao
   Liu, Hao
   Wang, Fasheng
   Mbise, Mercy
TI Objectness-based smoothing stochastic sampling and coherence approximate
   nearest neighbor for visual tracking
SO VISUAL COMPUTER
LA English
DT Article
DE Abrupt motion tracking; Coherence approximate nearest neighbor;
   Objectness; Stochastic Approximate
ID ABRUPT MOTION TRACKING; MODEL
AB In visual tracking, most of the tracking methods suffer from abrupt motions. To address this problem, we propose a novel method for tracking abrupt motions using objectness embedded in smoothing stochastic sampling and improved Tree coherency approximate nearest neighbor. An improved coherence approximate nearest neighbor is utilized to estimate the promising regions as prior knowledge. Moreover, objectness is employed as an objectness proposal function for handling dynamic motions. Finally, both prior knowledge and objectness proposal are integrated into the smoothing stochastic approximate Monte Carlo to predict a new state of the target object. Experimental comparison with other tracking methods and proposed method was carried on some of the challenging video sequences. Experimental results demonstrate that our proposed method outperforms other state-of-the-art tracking methods for dealing with abrupt motions in terms of effectiveness and robustness.
C1 [Mbelwa, Jimmy T.; Zhao, Qingjie; Lu, Yao; Liu, Hao] Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
   [Wang, Fasheng] Dalian Minzu Univ, Sch Informat & Commun Engn, Dalian 116600, Peoples R China.
   [Mbise, Mercy] Univ Dar Es Salaam, Coll Informat & Commun Technol, POB 35062, Dar Es Salaam, Tanzania.
C3 Beijing Institute of Technology; Dalian Minzu University; University of
   Dar es Salaam
RP Mbelwa, JT (corresponding author), Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
EM jmbelwa@bit.edu.cn; zhaoqj@bit.edu.cn; vis_yl@bit.edu.cn;
   liuhao@bit.edu.cn; fswang@dlut.edu.cn; mercymbise@gmail.com
RI Mbise, Mercy/AAN-1698-2021; Wang, Fasheng/AAD-9930-2020
OI Mbise, Mercy/0000-0002-7606-7231; 
FU Chinese Government Scholarship under China Scholarship Council (CSC);
   National Natural Science Foundation of China [61175096]; National
   Natural Science Foundation of China (NSFC) [61300082]; Liaoning Natural
   Science Foundation [2015020015]; China Postdoctoral Science Foundation
   [2016M601306]
FX This work was supported by Chinese Government Scholarship under China
   Scholarship Council (CSC), National Natural Science Foundation of China
   (Grant No. 61175096, NSFC No. 61300082), Liaoning Natural Science
   Foundation (No. 2015020015) and 2016 project funded by China
   Postdoctoral Science Foundation (No. 2016M601306).
CR Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374
   Cehovin L, 2011, IEEE I CONF COMP VIS, P1363, DOI 10.1109/ICCV.2011.6126390
   Chen S, 2013, MACH VISION APPL, V24, P1487, DOI 10.1007/s00138-012-0476-7
   Chen ZY, 2013, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR.2013.316
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Doucet A, 2001, STAT ENG IN, P3
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Gowsikhaa D, 2014, ARTIF INTELL REV, V42, P747, DOI 10.1007/s10462-012-9341-3
   Hare S, 2012, PROC CVPR IEEE, P1894, DOI 10.1109/CVPR.2012.6247889
   Hong S, 2013, IEEE I CONF COMP VIS, P2296, DOI 10.1109/ICCV.2013.285
   Hua Y, 2015, IEEE I CONF COMP VIS, P3092, DOI 10.1109/ICCV.2015.354
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Karami AH, 2015, ENG APPL ARTIF INTEL, V37, P307, DOI 10.1016/j.engappai.2014.09.018
   Korman S, 2011, IEEE I CONF COMP VIS, P1607, DOI 10.1109/ICCV.2011.6126421
   Kwon J, 2013, IEEE T PATTERN ANAL, V35, P1011, DOI 10.1109/TPAMI.2012.161
   Kwon J, 2011, IEEE I CONF COMP VIS, P1195, DOI 10.1109/ICCV.2011.6126369
   Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821
   Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039
   Li ZY, 2015, VISUAL COMPUT, V31, P1319, DOI 10.1007/s00371-014-1014-6
   Liang FM, 2007, J AM STAT ASSOC, V102, P305, DOI 10.1198/016214506000001202
   Liang FM, 2009, ANN STAT, V37, P2626, DOI 10.1214/07-AOS577
   Lu XQ, 2013, PATTERN RECOGN, V46, P1762, DOI 10.1016/j.patcog.2012.11.016
   Olonetsky I, 2012, LECT NOTES COMPUT SC, V7575, P602, DOI 10.1007/978-3-642-33765-9_43
   Oron S, 2012, PROC CVPR IEEE, P1940, DOI 10.1109/CVPR.2012.6247895
   Pérez P, 2002, LECT NOTES COMPUT SC, V2350, P661
   Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230
   Su YY, 2014, PATTERN RECOGN, V47, P1826, DOI 10.1016/j.patcog.2013.11.028
   Wang D, 2013, PROC CVPR IEEE, P2371, DOI 10.1109/CVPR.2013.307
   Wang FS, 2012, INT C PATT RECOG, P3066
   Wang FS, 2013, COMPUT J, V56, P1102, DOI 10.1093/comjnl/bxs141
   Wu HF, 2014, VISUAL COMPUT, V30, P229, DOI 10.1007/s00371-013-0823-3
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Wu YX, 2015, VISUAL COMPUT, V31, P471, DOI 10.1007/s00371-014-0942-5
   Yang HX, 2011, NEUROCOMPUTING, V74, P3823, DOI 10.1016/j.neucom.2011.07.024
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Zeng FX, 2013, J ELECTRON IMAGING, V22, DOI 10.1117/1.JEI.22.4.043022
   Zhang KH, 2014, LECT NOTES COMPUT SC, V8693, P127, DOI 10.1007/978-3-319-10602-1_9
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhang SL, 2015, PATTERN RECOGN, V48, P3881, DOI 10.1016/j.patcog.2015.06.005
   Zhou XZ, 2012, IEEE T IMAGE PROCESS, V21, P789, DOI 10.1109/TIP.2011.2168414
   Zhou XZ, 2010, PROC CVPR IEEE, P1847, DOI 10.1109/CVPR.2010.5539856
   Zhou Y, 2014, IEEE INT CONGR BIG, P1, DOI 10.1109/BigData.Congress.2014.11
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 44
TC 5
Z9 5
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2019
VL 35
IS 3
BP 371
EP 384
DI 10.1007/s00371-018-1470-5
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HP0MR
UT WOS:000461360600006
DA 2024-07-18
ER

PT J
AU Villamizar, M
   Sanfeliu, A
   Moreno-Noguer, F
AF Villamizar, Michael
   Sanfeliu, Alberto
   Moreno-Noguer, Francesc
TI Online learning and detection of faces with low human supervision
SO VISUAL COMPUTER
LA English
DT Article
ID BOOSTED RANDOM FERNS; OBJECT; REGRESSION; TRACKING
AB We present an efficient, online, and interactive approach for computing a classifier, called Wild Lady Ferns (WiLFs), for face learning and detection using small human supervision. More precisely, on the one hand, WiLFs combine online boosting and extremely randomized trees (random ferns) to compute progressively an efficient and discriminative classifier. On the other hand, WiLFs use an interactive human-machine approach that combines two complementary learning strategies to reduce considerably the degree of human supervision during learning. Whilethe first strategy corresponds to query-by-boosting active learning, that requests human assistance over difficult samples in function of the classifier confidence, the second strategy refers to a memory-based learning which uses exemplar-based nearest neighbors (ENN) to assist automatically the classifier. A pretrained convolutional neural network is used to perform ENN with high-level feature descriptors. The proposed approach is therefore fast (WilFs run in 1FPS using a code not fully optimized), accurate (we obtain detection rates over 82% in complex datasets), and labor-saving (human assistance percentages of less than20%). As a by-product, we demonstrate that WiLFs also perform semiautomatic annotation during learning, as while the classifier is being computed, WiLFs are discovering faces instances in input images which are used subsequently for training online the classifier. The advantages of our approach are demonstrated in synthetic and publicly available databases, showing comparable detection rates as offline approaches that require larger amounts of handmade trainingdata.
C1 [Villamizar, Michael] Idiap Res Inst, Martigny, Switzerland.
   [Sanfeliu, Alberto] Inst Robot & Informat Ind, Barcelona, Spain.
   [Moreno-Noguer, Francesc] Spanish Natl Res Council, Inst Robot & Informat Ind, Barcelona, Spain.
C3 Consejo Superior de Investigaciones Cientificas (CSIC); CSIC - Institut
   de Robotica i Informatica Industrial (IRII); Consejo Superior de
   Investigaciones Cientificas (CSIC); CSIC - Institut de Robotica i
   Informatica Industrial (IRII)
RP Villamizar, M (corresponding author), Idiap Res Inst, Martigny, Switzerland.
EM michael.villamizar@idiap.ch; sanfeliu@iri.upc.edu; fmoreno@iri.upc.edu
FU Spanish Ministry of Economy and Competitiveness under project HuMoUR
   [TIN2017-90086-R]; Spanish Ministry of Economy and Competitiveness under
   project ColRobTransp [DPI2016-78957]; Spanish Ministry of Economy and
   Competitiveness under project Maria de Maeztu Seal of Excellence
   [MDM-2016-0656]
FX This work is partially supported by the Spanish Ministry of Economy and
   Competitiveness under projects HuMoUR TIN2017-90086-R, ColRobTransp
   DPI2016-78957 and Maria de Maeztu Seal of Excellence MDM-2016-0656.
CR Abe N., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P1
   [Anonymous], BRIT MACH VIS C
   [Anonymous], NEURAL INFORM PROCES
   [Anonymous], ICPRAM
   [Anonymous], BRIT MACH VIS C
   [Anonymous], DAGM 2012 CVAW WORKS
   [Anonymous], 2010, U WISCONSIN MADISON, V52, P11, DOI DOI 10.1016/J.MATLET.2010.11.072
   [Anonymous], 2010, Fddb: A benchmark for face detection in unconstrained settings
   [Anonymous], 2006, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
   [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], 2012, MACHINE LEARNING PRO
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Calonder M, 2012, IEEE T PATTERN ANAL, V34, P1281, DOI 10.1109/TPAMI.2011.222
   Cheng Y, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1311
   Criminisil A, 2011, FOUND TRENDS COMPUT, V7, P81, DOI [10.1561/0600000035, 10.1501/0000000035]
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Ferrer G., 2013, INTELLIGENT SYSTEMS, V48, P185, DOI [10.1007/978-3-642-35932-3_11, DOI 10.1007/978-3-642-35932-3_11]
   Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223
   Gall J, 2011, IEEE T PATTERN ANAL, V33, P2188, DOI 10.1109/TPAMI.2011.70
   Godec M, 2013, COMPUT VIS IMAGE UND, V117, P1245, DOI 10.1016/j.cviu.2012.11.005
   Grabner H, 2008, LECT NOTES COMPUT SC, V5302, P234, DOI 10.1007/978-3-540-88682-2_19
   Hare S, 2012, PROC CVPR IEEE, P1894, DOI 10.1109/CVPR.2012.6247889
   Hu PY, 2017, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2017.166
   Huang G.B., 2008, PROC WORKSHOP FACES
   Jain V, 2011, PROC CVPR IEEE, P577, DOI 10.1109/CVPR.2011.5995317
   Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231
   KIM T.-K., 2010, 2010 IEEE COMPUTER S, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Krupka E, 2014, PROC CVPR IEEE, P3670, DOI 10.1109/CVPR.2014.469
   Kumar V, 2015, IEEE I CONF COMP VIS, P1994, DOI 10.1109/ICCV.2015.231
   Lewis D. D., 1994, SIGIR '94. Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, P3
   Li HX, 2015, PROC CVPR IEEE, P5325, DOI 10.1109/CVPR.2015.7299170
   Li HX, 2014, PROC CVPR IEEE, P1843, DOI 10.1109/CVPR.2014.238
   Li YX, 2011, LECT NOTES ARTIF INT, V6635, P321, DOI 10.1007/978-3-642-20847-8_27
   Liu BZ, 2018, VISUAL COMPUT, V34, P707, DOI 10.1007/s00371-017-1408-3
   Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229
   Mathias M, 2014, LECT NOTES COMPUT SC, V8692, P720, DOI 10.1007/978-3-319-10593-2_47
   Najibi M, 2017, IEEE I CONF COMP VIS, P4885, DOI 10.1109/ICCV.2017.522
   Özuysal M, 2010, IEEE T PATTERN ANAL, V32, P448, DOI 10.1109/TPAMI.2009.23
   Park JK, 2019, VISUAL COMPUT, V35, P1615, DOI 10.1007/s00371-018-1561-3
   Quan W, 2014, VISUAL COMPUT, V30, P351, DOI 10.1007/s00371-013-0860-y
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Santner J, 2010, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2010.5540145
   Seung H. S., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P287, DOI 10.1145/130385.130417
   Sharma P, 2014, IEEE WINT CONF APPL, P745, DOI 10.1109/WACV.2014.6836028
   Torralba A, 2007, IEEE T PATTERN ANAL, V29, P854, DOI 10.1109/TPAMI.2007.1055
   Villamizar M, 2014, IEEE INT CONF ROBOT, P4996, DOI 10.1109/ICRA.2014.6907591
   Villamizar M, 2012, INT C PATT RECOG, P2821
   Villamizar M, 2018, IEEE T PATTERN ANAL, V40, P272, DOI 10.1109/TPAMI.2017.2676778
   Villamizar M, 2016, COMPUT VIS IMAGE UND, V149, P51, DOI 10.1016/j.cviu.2016.03.010
   Villamizar M, 2012, PATTERN RECOGN, V45, P3141, DOI 10.1016/j.patcog.2012.03.025
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Yao A, 2012, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2012.6248060
   Zeisl B, 2010, PROC CVPR IEEE, P1879, DOI 10.1109/CVPR.2010.5539860
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
NR 58
TC 6
Z9 8
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2019
VL 35
IS 3
BP 349
EP 370
DI 10.1007/s00371-018-01617-y
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HP0MR
UT WOS:000461360600005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, YJ
   Zhang, JW
   Zhong, YZ
   Wang, MN
AF Li, Yingjiang
   Zhang, Jianwei
   Zhong, Yuzhong
   Wang, Maoning
TI An efficient stereo matching based on fragment matching
SO VISUAL COMPUTER
LA English
DT Article
DE Image edge; Same color region median filter; Segment matching; Stereo
   matching
ID COST AGGREGATION
AB We propose a stereo matching method based on image fragments. Unlike traditional pixel-based stereos matching methods, we use edge information in the reference image to divide it into small fragments, and we then use the segments to find the best matching fragments in another reference image from the horizontal and vertical directions. We obtain two disparity maps, and using the match confidence value for each disparity map, we can produce a more accurate disparity map. Next, we calculate the exact disparity value for each pixel within the fragment. Finally, the disparity map is filled and smoothed to obtain the final disparity result. Experiments demonstrated that the proposed method has low computation complexity, high matching accuracy, and the disparity of object edge is clear, and it achieved good performance with the Middlebury and KITTI benchmark.
C1 [Li, Yingjiang; Zhang, Jianwei; Zhong, Yuzhong; Wang, Maoning] Sichuan Univ, Chengdu, Sichuan, Peoples R China.
   [Zhang, Jianwei; Wang, Maoning] Wisesoft Co Ltd, Chengdu, Sichuan, Peoples R China.
   [Li, Yingjiang] Honghe Univ, Mengzi, Yunnan, Peoples R China.
C3 Sichuan University; Honghe University
RP Li, YJ (corresponding author), Sichuan Univ, Chengdu, Sichuan, Peoples R China.; Li, YJ (corresponding author), Honghe Univ, Mengzi, Yunnan, Peoples R China.
EM 45487743@qq.com
RI 李, 迎/HGU-1206-2022; Zhang, Jianwei/HJA-0011-2022; 李, 迎江/AAH-7170-2020
OI Zhang, Jianwei/0000-0002-5491-1745; Li, YingJiang/0000-0002-3450-5707
FU Graduate Student's Research and Innovation Fund of Sichuan University
   [2018YJSY073]; Department of Science and Technology of Sichuan Province
   (Science and Technology Department of Sichuan Province) [2018RZ0080]
FX Grant No. 2018YJSY073 from Graduate Student's Research and Innovation
   Fund of Sichuan University and Grant No. 2018RZ0080 from Department of
   Science and Technology of Sichuan Province (Science and Technology
   Department of Sichuan Province).
CR [Anonymous], C COMP VIS PATT REC
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], EUR C COMP VIS
   [Anonymous], P CVPR
   [Anonymous], P 2003 IEEE COMP SOC
   [Anonymous], 2016, J MACH LEARN RES
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Fu LM, 2016, IET COMPUT VIS, V10, P173, DOI 10.1049/iet-cvi.2014.0411
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hirschmüller H, 2007, PROC CVPR IEEE, P2134
   Hirschmüller H, 2009, IEEE T PATTERN ANAL, V31, P1582, DOI 10.1109/TPAMI.2008.221
   Kim KR, 2016, IEEE IMAGE PROC, P3429, DOI 10.1109/ICIP.2016.7532996
   Li LL, 2016, IEEE T FUZZY SYST, V24, P1320, DOI 10.1109/TFUZZ.2016.2514371
   Li Y, 2015, IEEE I CONF COMP VIS, P4006, DOI 10.1109/ICCV.2015.456
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Psota ET, 2015, IEEE I CONF COMP VIS, P2219, DOI 10.1109/ICCV.2015.256
   Rhemann C, 2011, PROC CVPR IEEE, DOI 10.1109/CVPR.2011.5995372
   Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977
   Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Ttofis C, 2016, IEEE T COMPUT, V65, P2678, DOI 10.1109/TC.2015.2506567
   Yamaguchi K, 2014, LECT NOTES COMPUT SC, V8693, P756, DOI 10.1007/978-3-319-10602-1_49
   Yang QX, 2014, IEEE T PATTERN ANAL, V36, P1026, DOI 10.1109/TPAMI.2013.186
   Yang QX, 2012, PROC CVPR IEEE, P1402, DOI 10.1109/CVPR.2012.6247827
   Zhan YL, 2016, IEEE T CIRC SYST VID, V26, P1632, DOI 10.1109/TCSVT.2015.2473375
   Zhang C, 2015, IEEE I CONF COMP VIS, P2057, DOI 10.1109/ICCV.2015.238
   Zhang K, 2014, PROC CVPR IEEE, P1590, DOI 10.1109/CVPR.2014.206
NR 27
TC 8
Z9 9
U1 1
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2019
VL 35
IS 2
BP 257
EP 269
DI 10.1007/s00371-018-1491-0
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HN3DS
UT WOS:000460064600008
DA 2024-07-18
ER

PT J
AU Guerrero, P
   Winnemöller, H
   Li, W
   Mitra, N
AF Guerrero, Paul
   Winnemoller, Holger
   Li, Wilmot
   Mitra, Niloy J.
TI DepthCut: improved depth edge estimation using multiple unreliable
   channels
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Depth estimation; Monocular; Stereo; Deep learning; Depth layering
ID EXTRACTION; COLOR
AB In the context of scene understanding, a variety of methods exists to estimate different information channels from mono or stereo images, including disparity, depth, and normals. Although several advances have been reported in the recent years for these tasks, the estimated information is often imprecise particularly near depth discontinuities or creases. Studies have however shown that precisely such depth edges carry critical cues for the perception of shape, and play important roles in tasks like depth-based segmentation or foreground selection. Unfortunately, the currently extracted channels often carry conflicting signals, making it difficult for subsequent applications to effectively use them. In this paper, we focus on the problem of obtaining high-precision depth edges (i.e., depth contours and creases) by jointly analyzing such unreliable information channels. We propose DepthCut, a data-driven fusion of the channels using a convolutional neural network trained on a large dataset with known depth. The resulting depth edges can be used for segmentation, decomposing a scene into depth layers with relatively flat depth, or improving the accuracy of the depth estimate near depth edges by constraining its gradients to agree with these edges. Quantitatively, we compare against 18 variants of baselines and demonstrate that our depth edges result in an improved segmentation performance and an improved depth estimate near depth edges compared to data-agnostic channel fusion. Qualitatively, we demonstrate that the depth edges result in superior segmentation and depth orderings. (Code and datasets will be made available.).
C1 [Guerrero, Paul] UCL, London, England.
   [Mitra, Niloy J.] UCL, Dept Comp Sci, Geometry Proc, London, England.
   [Winnemoller, Holger; Li, Wilmot] Adobe Res, Seattle, WA USA.
C3 University of London; University College London; University of London;
   University College London; Adobe Systems Inc.
RP Guerrero, P (corresponding author), UCL, London, England.
EM paulaugguerrero@gmail.com; hwinnemo@adobe.com; wilmotli@adobe.com;
   n.mitra@ucl.ac.uk
RI Guerrero, Paul/Y-4069-2018
OI Guerrero, Paul/0000-0002-7568-2849
FU ERC Starting Grant SmartGeometry [StG-2013-335373]; Open3D Project
   (EPSRC) [EP/M013685/1]; EPSRC [EP/M013685/1] Funding Source: UKRI
FX This work was supported by the ERC Starting Grant SmartGeometry
   (StG-2013-335373), the Open3D Project (EPSRC Grant EP/M013685/1), and
   gifts from Adobe.
CR [Anonymous], 2014, ACM T GRAPHICS
   [Anonymous], CORR
   [Anonymous], 1986, The Ecological Approach toVisual Perception
   [Anonymous], ACM TOG
   [Anonymous], 2012, NIPS
   [Anonymous], SIGGRAPH ASIA 2016 C
   [Anonymous], IEEE PAMI
   [Anonymous], 2015, ARXIV PREPRINT ARXIV
   [Anonymous], 2016, CORR
   [Anonymous], IEEE CVPR
   [Anonymous], ICIP
   [Anonymous], IEEE CVPR
   [Anonymous], IEEE ICCV
   [Anonymous], IEEE CVPR
   [Anonymous], ACM TOG
   [Anonymous], 2014, CORR
   [Anonymous], 2014, ECCV
   [Anonymous], 2013, NIPS
   [Anonymous], 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.123
   [Anonymous], CGF
   [Anonymous], 2013, CORR
   [Anonymous], 2013, ARXIV PREPRINT ARXIV
   [Anonymous], 2016, J MACH LEARN RES
   [Anonymous], SGP
   Chakrabarti Ayan, 2016, Advances in Neural Information Processing Systems, P2658
   Chen XW, 2013, COMPUT VIS IMAGE UND, V117, P42, DOI 10.1016/j.cviu.2012.10.001
   Dahan MJ, 2012, VISUAL COMPUT, V28, P1181, DOI 10.1007/s00371-011-0667-7
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Kingma D. P., 2015, INT C LEARNING REPRE
   Kolmogorov V, 2005, PROC CVPR IEEE, P407
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   McCormac J., 2016, CoRR
   Nitzberg M., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P138, DOI 10.1109/ICCV.1990.139511
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3
   Simonyan K., 2015, P 3 INT C LEARN REPR
   Sundberg P, 2011, PROC CVPR IEEE
   Yu CC, 2014, GRAPH MODELS, V76, P507, DOI 10.1016/j.gmod.2014.03.015
   Zhang YC, 2018, ICMLC 2020: 2020 12TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND COMPUTING, P145, DOI 10.1145/3383972.3383975
NR 41
TC 4
Z9 4
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2018
VL 34
IS 9
BP 1165
EP 1176
DI 10.1007/s00371-018-1551-5
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GQ5MB
UT WOS:000441727000004
OA Green Submitted, Green Published, hybrid
DA 2024-07-18
ER

PT J
AU Ma, L
   Chen, YY
   Qian, YL
   Sun, HQ
AF Ma, Lei
   Chen, Yanyun
   Qian, Yinling
   Sun, Hanqiu
TI Incremental Voronoi sets for instant stippling
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Real-time; Stippling; Multi-tones; Voronoi
ID WANG TILES; NOISE; ALGORITHM; IMAGE
AB This paper presents a fast digital stippling algorithm, which makes a fair balance on result quality and computational efficiency. The algorithm is based on precomputed blue noise point sets constructed by incremental Voronoi sets (IVS) and a real-time parallelized rejection strategy. The proposed technique is readily extended to generate multi-tone-level or multi-nib-size stippling results of increased pleasure visual impressions with smooth tone transition. The IVS can also be regressed to generate blue noise masks for digital halftoning.
C1 [Ma, Lei] Univ Chinese Acad Sci, Beijing, Peoples R China.
   [Ma, Lei; Chen, Yanyun] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing, Peoples R China.
   [Qian, Yinling] Chinese Univ Hong Kong, Dept Comp Sci & Technol, Hong Kong, Hong Kong, Peoples R China.
   [Sun, Hanqiu] Chinese Univ Hong Kong, Hong Kong, Hong Kong, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; Institute of Software, CAS; Chinese
   University of Hong Kong; Chinese University of Hong Kong
RP Ma, L (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.; Ma, L (corresponding author), Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing, Peoples R China.
EM malei@outlook.com; chenyy@ios.ac.cn; ylqian@cse.cuhk.edu.hk;
   hanqiu@cse.cuhk.edu.hk
OI Ma, Lei/0000-0001-6024-3854
FU UGC [4055060]; NSFC [61379087, 61602183]
FX This study is funded by UGC (Grant number 4055060), joint NSFC grants
   (no. 61379087, 61602183).
CR Ahmed AGM, 2017, IEEE T VIS COMPUT GR, V23, P2496, DOI 10.1109/TVCG.2016.2641963
   Ahmed AGM, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073588
   Ahmed AGM, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980218
   Ahmed AGM, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818139
   [Anonymous], 2002, P 2 INT S NONPH AN R, DOI DOI 10.1145/508535.508537
   [Anonymous], ACM T GRAPH
   Ascencio-Lopez I, 2010, J GRAPHICS GPU GAME, V15, P29, DOI DOI 10.1080/2151237X.2010.10390650
   Balzer M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531392
   Bayer B.E., 1973, IEEE INT C COMM, P11
   Chen RJ, 2012, COMPUT GRAPH FORUM, V31, P1775, DOI 10.1111/j.1467-8659.2012.03182.x
   Chen ZG, 2012, IEEE T VIS COMPUT GR, V18, P1784, DOI 10.1109/TVCG.2012.94
   Cline D, 2009, COMPUT GRAPH FORUM, V28, P1217, DOI 10.1111/j.1467-8659.2009.01499.x
   Cohen MF, 2003, ACM T GRAPHIC, V22, P287, DOI 10.1145/882262.882265
   COOK RL, 1986, ACM T GRAPHIC, V5, P51, DOI 10.1145/7529.8927
   de Goes F, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366190
   Deussen O, 2000, COMPUT GRAPH FORUM, V19, pC41, DOI 10.1111/1467-8659.00396
   Du Q, 1999, SIAM REV, V41, P637, DOI 10.1137/S0036144599352836
   Ebeida MS, 2012, COMPUT GRAPH FORUM, V31, P785, DOI 10.1111/j.1467-8659.2012.03059.x
   Ebeida MS, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964944
   Eldar Y, 1997, IEEE T IMAGE PROCESS, V6, P1305, DOI 10.1109/83.623193
   Fattal Raanan, 2011, ACM T GRAPHIC, V28, P48
   Georgiev I., 2016, ACM SIGGRAPH 2016 TA
   Gwosdek P, 2014, J REAL-TIME IMAGE PR, V9, P379, DOI 10.1007/s11554-011-0236-3
   Kang H.R., 1999, DIGITAL COLOR HALFTO
   Kopf J, 2006, ACM T GRAPHIC, V25, P509, DOI 10.1145/1141911.1141916
   Kriss Michael., 2015, Handbook of Digital Imaging
   Lagae A, 2008, COMPUT GRAPH FORUM, V27, P114, DOI 10.1111/j.1467-8659.2007.01100.x
   Lagae A, 2006, ACM T GRAPHIC, V25, P1442, DOI 10.1145/1183287.1183296
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Lu AD, 2003, IEEE T VIS COMPUT GR, V9, P127, DOI 10.1109/TVCG.2003.1196001
   Ma CY, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618456
   Martín D, 2017, COMPUT GRAPH-UK, V67, P24, DOI 10.1016/j.cag.2017.05.001
   MITSA T, 1992, J OPT SOC AM A, V9, P1920, DOI 10.1364/JOSAA.9.001920
   NIEDERREITER H, 1988, J NUMBER THEORY, V30, P51, DOI 10.1016/0022-314X(88)90025-X
   Niederreiter H., 1992, J AM STAT ASSOC, V88
   Ostromoukhov V, 2004, ACM T GRAPHIC, V23, P488, DOI 10.1145/1015706.1015750
   Ostromoukhov V, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239529, 10.1145/1276377.1276475]
   Pang W.-M., 2008, ACM T GRAPHIC, V27, P89
   Pnueli Y, 1996, GRAPH MODEL IM PROC, V58, P38, DOI 10.1006/gmip.1996.0003
   PURGATHOFER W, 1994, IEEE IMAGE PROC, P1032, DOI 10.1109/ICIP.1994.413512
   Schlomer T., 2011, P ACM SIGGRAPH S HIG, P135, DOI DOI 10.1145/2018323.2018345
   Schretter C., 2012, J Graph Tools, V16, P95, DOI [DOI 10.1080/2165347X.2012.679555, 10.1080/2165347x.2012.679555]
   Spicker M., 2017, P S NONPH AN REND NP
   Ulichney R., 1987, DITHERING BLUE NOISE
   Ulichney R., 1987, DIGITAL HALFTONING
   Ulichney RA., 1993, IS T SPIES S EL IM S, DOI [DOI 10.1117/12.152707, 10.1117/12.152707]
   Wachtel F, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601107
   Wong T-T, 1997, J GRAPHICS TOOLS, V2, P9, DOI DOI 10.1080/10867651.1997.10487471
   Xu Y, 2011, COMPUT GRAPH-UK, V35, P510, DOI 10.1016/j.cag.2011.03.031
   Yan DM, 2015, J COMPUT SCI TECH-CH, V30, P439, DOI 10.1007/s11390-015-1535-0
   Yan DM, 2014, COMPUT GRAPH FORUM, V33, P167, DOI 10.1111/cgf.12442
   Yan DM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2516971.2516973
   Yuksel C, 2015, COMPUT GRAPH FORUM, V34, P25, DOI 10.1111/cgf.12538
   Zhou BF, 2003, ACM T GRAPHIC, V22, P437, DOI 10.1145/882262.882289
NR 54
TC 7
Z9 10
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 863
EP 873
DI 10.1007/s00371-018-1541-7
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400011
DA 2024-07-18
ER

PT J
AU Sbert, M
   Havran, V
   Szirmay-Kalos, L
   Elvira, V
AF Sbert, Mateu
   Havran, Vlastimil
   Szirmay-Kalos, Laszlo
   Elvira, Victor
TI Multiple importance sampling characterization by weighted mean
   invariance
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Global illumination; Rendering equation analysis; Multiple importance
   sampling; Monte Carlo
ID POPULATION MONTE-CARLO
AB In this paper, we examine the linear combination of techniques and multiple importance sampling for Monte Carlo integration from a new perspective of quasi-arithmetic weighted means. The invariance property of these means allows us to define a new family of heuristics. We illustrate our results with several rendering examples, including environment mapping and path tracing.
C1 [Sbert, Mateu] Tianjin Univ, Sch Comp Sci & Technol, Tianjin, Peoples R China.
   [Sbert, Mateu] Girona Univ, Inst Informat & Applicat, Girona, Spain.
   [Havran, Vlastimil] Czech Tech Univ, Fac Elect Engn, Prague, Czech Republic.
   [Szirmay-Kalos, Laszlo] Budapest Univ Technol & Econ, Dept Control Engn & Informat Technol, Budapest, Hungary.
   [Elvira, Victor] IMT Lille Douai, Dept Commun Syst, Lille, France.
   [Elvira, Victor] CNRS, UMR 9189, CRIStAL Lab, Lille, France.
C3 Tianjin University; Universitat de Girona; Czech Technical University
   Prague; Budapest University of Technology & Economics; IMT - Institut
   Mines-Telecom; Universite de Lille; IMT Nord Europe; Centre National de
   la Recherche Scientifique (CNRS); Universite de Lille; Centrale Lille
RP Havran, V (corresponding author), Czech Tech Univ, Fac Elect Engn, Prague, Czech Republic.
EM havran@fel.cvut.cz
RI Szirmay-Kalos, Laszlo/H-3853-2012; Havran, Vlastimil/B-4530-2014; Sbert,
   Mateu/G-6711-2011
OI Szirmay-Kalos, Laszlo/0000-0002-8523-2315; Elvira,
   Victor/0000-0002-8967-4866; Havran, Vlastimil/0000-0002-3329-8814;
   Sbert, Mateu/0000-0003-2164-6858
FU Czech Science Foundation [GA14-19213S]; Spanish Government
   [TIN2016-75866-C3-3-R]; National Natural Science Foundation of China
   [61471261, 61771335]; Agence Nationale de la Recherche of France under
   PISCES project [ANR-17-CE40-0031-01]; Fulbright program; Marie Curie
   Fellowship (FP7) under REA grant, through the PRESTIGE program
   [PCOFUND-GA-2013-609102];  [OTKA K-124124];  [VKSZ-14 PET/MRI 7T]
FX The authors are funded in part by Czech Science Foundation research
   program GA14-19213S, by Grant TIN2016-75866-C3-3-R from the Spanish
   Government, by National Natural Science Foundation of China Grants Nos.
   61471261 and 61771335, and by Grant OTKA K-124124 and VKSZ-14 PET/MRI
   7T. V.E. acknowledges support from the Agence Nationale de la Recherche
   of France under PISCES project (ANR-17-CE40-0031-01), the Fulbright
   program, and the Marie Curie Fellowship (FP7/2007-2013) under REA grant
   agreement n. PCOFUND-GA-2013-609102, through the PRESTIGE program.
CR [Anonymous], 1997, ROBUST MONTE CARLO M
   [Anonymous], 2014, ARXIV14113954
   [Anonymous], 2003, Handbook of Means and Their Inequalities
   Belzunce F., 2016, INTRO STOCHASTIC ORD, pI, DOI [10.1016/B978-0-12-803768-3.09977-4, DOI 10.1016/B978-0-12-803768-3.09977-4]
   Bugallo MF, 2017, IEEE SIGNAL PROC MAG, V34, P60, DOI 10.1109/MSP.2017.2699226
   Cappé O, 2004, J COMPUT GRAPH STAT, V13, P907, DOI 10.1198/106186004X12803
   Cornuet JM, 2012, SCAND J STAT, V39, P798, DOI 10.1111/j.1467-9469.2011.00756.x
   DOUC R., 2007, ESAIM-PROBAB STAT, V11, P424
   Elvira V., 2016, STAT SIGN PROC WORKS, P1
   Elvira V, 2015, ARXIV E PRINTS
   Elvira V, 2015, IEEE SIGNAL PROC LET, V22, P1757, DOI 10.1109/LSP.2015.2432078
   Elvira V, 2017, SIGNAL PROCESS, V131, P77, DOI 10.1016/j.sigpro.2016.07.012
   Elvira V, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2600678
   Havran V., 2014, P 13 ACM SIGGRAPH IN, P141
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Lu H, 2013, COMPUT GRAPH FORUM, V32, P131, DOI 10.1111/cgf.12220
   Martino L, 2017, STAT COMPUT, V27, P599, DOI 10.1007/s11222-016-9642-5
   Neumann L, 1999, COMPUT GRAPH FORUM, V18, pC161, DOI 10.1111/1467-8659.00337
   Owen A.B., 2017, ARXIV171006965
   Sbert M, 2016, COMPUT GRAPH FORUM, V35, P451, DOI 10.1111/cgf.13042
   Sbert M, 2018, EURASIP J ADV SIG PR, DOI 10.1186/s13634-018-0531-2
   Sbert M, 2017, VISUAL COMPUT, V33, P845, DOI 10.1007/s00371-017-1398-1
   Sbert M, 2016, J INEQUAL APPL, DOI 10.1186/s13660-016-1233-7
   Shaked M, 2007, SPRINGER SER STAT, P3
   Veach E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P419, DOI 10.1145/218380.218498
NR 25
TC 10
Z9 10
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 843
EP 852
DI 10.1007/s00371-018-1522-x
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400009
DA 2024-07-18
ER

PT J
AU Fan, YL
   Wang, ML
   Geng, N
   He, DJ
   Chang, J
   Zhang, JJ
AF Fan, Yuling
   Wang, Meili
   Geng, Nan
   He, Dongjian
   Chang, Jian
   Zhang, Jian J.
TI A self-adaptive segmentation method for a point cloud
SO VISUAL COMPUTER
LA English
DT Article
DE Point cloud; Segmentation; Seed point; Region growing
ID RECONSTRUCTION
AB The segmentation of a point cloud is one of the key technologies for three-dimensional reconstruction, and the segmentation from three-dimensional views can facilitate reverse engineering. In this paper, we propose a self-adaptive segmentation algorithm, which can address challenges related to the region-growing algorithm, such as inconsistent or excessive segmentation. Our algorithm consists of two main steps: automatic selection of seed points according to extracted features and segmentation of the points using an improved region-growing algorithm. The benefits of our approach are the ability to select seed points without user intervention and the reduction of the influence of noise. We demonstrate the robustness and effectiveness of our algorithm on different point cloud models and the results show that the segmentation accuracy rate achieves 96%.
C1 [Fan, Yuling; Wang, Meili; Geng, Nan] Northwest A&F Univ, Coll Informat Engn, Xianyang, Peoples R China.
   [He, Dongjian] Northwest A&F Univ, Coll Mech & Elect Engn, Xianyang, Peoples R China.
   [Chang, Jian; Zhang, Jian J.] Bournemouth Univ, Media Sch, Natl Ctr Comp Animat, Bournemouth, Dorset, England.
C3 Northwest A&F University - China; Northwest A&F University - China;
   Bournemouth University
RP He, DJ (corresponding author), Northwest A&F Univ, Coll Mech & Elect Engn, Xianyang, Peoples R China.
EM yuling_fan@nwsuaf.edu.cn; wml@nwsuaf.edu.cn; hdj168@nwsuaf.edu.cn
FU National High-tech research and Development Program (863 Program) [2013A
   A10230402]; National Natural Science Foundation of China [61402374];
   China Postdoctoral Science Foundation [2014M562457]
FX This work was partially funded by the National High-tech research and
   Development Program (863 Program: 2013A A10230402), National Natural
   Science Foundation of China (61402374), and the China Postdoctoral
   Science Foundation (2014M562457). The authors acknowledge the authors of
   [26], Shenzhen Key Lab of Visual Computing and Visual Analytics for the
   source data and the models.
CR Aiteanu F, 2014, VISUAL COMPUT, V30, P763, DOI 10.1007/s00371-014-0977-7
   [Anonymous], 2006, INT ARCH PHOTOGRAMM, DOI DOI 10.1080/136588100750022796
   Carbonell J., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P335, DOI 10.1145/290941.291025
   Castillo E., 2013, POINT CLOUD SEGMENTA
   Chen J, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618492
   Clarenz U, 2004, VISUAL COMPUT, V20, P329, DOI 10.1007/s00371-004-0245-3
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Dai M, 2009, WORKSH COMM BAS 3D C
   Demir I, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766923
   DORNINGER P, 2007, INT ARCH PHOTOGRAMME, V35, P191
   Fayolle PA, 2013, VISUAL COMPUT, V29, P449, DOI 10.1007/s00371-012-0749-1
   Gelfand Natasha, 2004, Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing, P214
   Gomes RB, 2013, COMPUT GRAPH-UK, V37, P496, DOI 10.1016/j.cag.2013.03.005
   Guillaume L, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P10, DOI 10.1109/CGI.2004.1309187
   Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461913
   Lari Z., 2013, ISPRS ANN PHOTOGRA 5, V2, P151, DOI DOI 10.5194/ISPRSANNALS-II-5-W2-151-2013
   Li YY, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964947
   Marshall D, 2001, IEEE T PATTERN ANAL, V23, P304, DOI 10.1109/34.910883
   Ochmann S, 2016, COMPUT GRAPH-UK, V54, P94, DOI 10.1016/j.cag.2015.07.008
   RABBANI T., 2006, The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, V36, P248
   Richtsfeld M, 2009, LECT NOTES COMPUT SC, V5702, P955, DOI 10.1007/978-3-642-03767-2_116
   Rodriguez A, 2014, SCIENCE, V344, P1492, DOI 10.1126/science.1242072
   van Kaick O, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2611811
   Wang J., 2009, American Society for Photogramm. Remote Sens. Annual Conference, P9, DOI 10.1177/1753193409347428
   Yamauchi H, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P236, DOI 10.1109/SMI.2005.21
   Yücer K, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2876504
   Zhan Q., 2009, Laser scanning, V38, P155
   Zhang YH, 2016, COMPUT GRAPH-UK, V56, P31, DOI 10.1016/j.cag.2016.01.004
NR 28
TC 23
Z9 26
U1 7
U2 89
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2018
VL 34
IS 5
BP 659
EP 673
DI 10.1007/s00371-017-1405-6
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GC3AW
UT WOS:000429657100006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, QN
   Wang, YF
   Sharf, A
   Cao, Y
   Tu, CH
   Chen, BQ
   Yu, SY
AF Li, Qiannan
   Wang, Yafang
   Sharf, Andrei
   Cao, Ya
   Tu, Changhe
   Chen, Baoquan
   Yu, Shengyuan
TI Classification of gait anomalies from kinect
SO VISUAL COMPUTER
LA English
DT Article
DE Gait recognition; Kinect; Geometry processing
ID RECOGNITION; VARIABILITY
AB A persons manner of walking or their gait is an important feature in human recognition and classification tasks. Gait serves as an unobtrusive biometric modality which yields high quality results. In comparison with other biometric modalities, its main strength is its performance even in data that are captured at distance or at low resolution. In this paper, we present an algorithm for classification of gait disorders arising from neuro-degenerative diseases such as Parkinson and Hemiplegia. We focus on motion anomalies such as tremor, partial paralysis, gestural rigidity and postural instability. The analysis and classification of such motions are challenging since they consist of a multiplicity of subtle formations while lacking a regular pattern or major cycle. We introduce a gait representation which is invariant to the walking cycle and yields an efficient similarity metric. Our method performs on the joints' motion trajectories of a 3D human skeleton captured by a Kinect sensor. The algorithm is robust, in that it does not require calibration, synchronization or a careful capturing setup. We demonstrate its efficiency by classifying different degenerative cases with high accuracy even in the presence of noise and low-resolution acquisition.
C1 [Li, Qiannan; Wang, Yafang; Tu, Changhe; Chen, Baoquan] Shandong Univ, Sch Comp Sci & Technol, Jinan, Shandong, Peoples R China.
   [Sharf, Andrei] Ben Gurion Univ Negev, Dept Comp Sci, IL-84105 Beer Sheva, Israel.
   [Cao, Ya; Yu, Shengyuan] Chinese Peoples Liberat Army Gen Hosp, Dept Neurol, Beijing 100853, Peoples R China.
C3 Shandong University; Ben Gurion University; Chinese People's Liberation
   Army General Hospital
RP Wang, YF (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Jinan, Shandong, Peoples R China.
EM yafang.wang@sdu.edu.cn
RI Tu, Changhe/H-5162-2013; Sharf, Andrei/F-1370-2012
OI Sharf, Andrei/0000-0002-3963-4508
FU National 973 Program [2015CB352500]; National Natural Science Foundation
   of China [61503217]; Shandong Provincial Natural Science Foundation of
   China [ZR2014FP002]; Fundamental Research Funds of Shandong University
   [2014TB005, 2014JC001]
FX This project was sponsored by National 973 Program (No. 2015CB352500),
   National Natural Science Foundation of China (No. 61503217), Shandong
   Provincial Natural Science Foundation of China (No. ZR2014FP002), and
   the Fundamental Research Funds of Shandong University (No. 2014TB005,
   2014JC001).
CR Ahmed M., 2014, SPIE PHOTONICS EUROP
   Akae N, 2012, PROC CVPR IEEE, P1537, DOI 10.1109/CVPR.2012.6247844
   Akhter I, 2011, IEEE T PATTERN ANAL, V33, P1442, DOI 10.1109/TPAMI.2010.201
   Ball A, 2012, ACMIEEE INT CONF HUM, P225
   Boulgouris NV, 2004, 2004 IEEE 6TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P263
   Brodal P., 2010, The central nervous system structure and function, V4th
   Chattopadhyay Pratik, 2013, Pattern Recognition and Machine Intelligence. 5th International Conference, PReMI 2013. Proceedings: LNCS 8251, P196, DOI 10.1007/978-3-642-45062-4_27
   Cippitelli E, 2015, SENSORS-BASEL, V15, P1417, DOI 10.3390/s150101417
   Dikovski B, 2014, 2014 37TH INTERNATIONAL CONVENTION ON INFORMATION AND COMMUNICATION TECHNOLOGY, ELECTRONICS AND MICROELECTRONICS (MIPRO), P1304, DOI 10.1109/MIPRO.2014.6859769
   Fahn P. J. S., 1987, RECENT DEV PARKINSON
   Forstner W, 2003, Geodesy-the Challenge of the 3rd Millennium, P299, DOI [10.1007/978-3-662-05296-9_31, DOI 10.1007/978-3-662-05296-9_31]
   Gabel M, 2012, IEEE ENG MED BIO, P1960
   Galna B, 2014, GAIT POSTURE, V39, P1062, DOI 10.1016/j.gaitpost.2014.01.008
   Han J, 2006, IEEE T PATTERN ANAL, V28, P316, DOI 10.1109/TPAMI.2006.38
   Hausdorff JM, 2001, ARCH PHYS MED REHAB, V82, P1050, DOI 10.1053/apmr.2001.24893
   Hausdorff JM, 2000, J APPL PHYSIOL, V88, P2045, DOI 10.1152/jappl.2000.88.6.2045
   Johansson G., 1975, VISUAL MOTION PERCEP
   Keijsers NLW, 2006, MOVEMENT DISORD, V21, P34, DOI 10.1002/mds.20633
   Khan T., 2013, RECENT PATENTS BIOME, V6, P97
   Kumar M., 2012, Proceedings of the Eighth Indian Conference on Computer Vision, Graphics and Image Processing, P20
   Lam THW, 2007, PATTERN RECOGN, V40, P2563, DOI 10.1016/j.patcog.2006.11.014
   Lee L, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P155, DOI 10.1109/AFGR.2002.1004148
   Li XL, 2008, IEEE T SYST MAN CY C, V38, P145, DOI 10.1109/TSMCC.2007.913886
   Liu LF, 2009, LECT NOTES ARTIF INT, V5755, P652
   Lu JW, 2012, INT C PATT RECOG, P3284
   Lu JW, 2014, IEEE T INF FOREN SEC, V9, P51, DOI 10.1109/TIFS.2013.2291969
   Makihara Y, 2011, LECT NOTES COMPUT SC, V6493, P440
   Mansur A, 2014, PROC CVPR IEEE, P2521, DOI 10.1109/CVPR.2014.323
   Martín-Félez R, 2012, LECT NOTES COMPUT SC, V7572, P328, DOI 10.1007/978-3-642-33718-5_24
   Murase H, 1996, PATTERN RECOGN LETT, V17, P155, DOI 10.1016/0167-8655(95)00109-3
   Ng C., 2012, PRICAI 2012: Trends in Arti cial Intelligence, V7458, P335
   Parajuli M, 2012, 2012 FOURTH INTERNATIONAL CONFERENCE ON COMMUNICATIONS AND ELECTRONICS (ICCE), P309, DOI 10.1109/CCE.2012.6315918
   Rocha AP, 2014, IEEE ENG MED BIO, P3126, DOI 10.1109/EMBC.2014.6944285
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Sivapalan Sabesan, 2011, 2011 INT JOINT C BIO, P1, DOI [10.1109/IJCB.2011.6117504, 10.1155/2011/375897]
   Stone EE, 2014, IEEE ENG MED BIO, P2961, DOI 10.1109/EMBC.2014.6944244
   Stone EE, 2011, IEEE ENG MED BIO, P6491, DOI 10.1109/IEMBS.2011.6091602
   Tang J, 2014, SENSORS-BASEL, V14, P6124, DOI 10.3390/s140406124
   Uddin MZ, 2014, INDOOR BUILT ENVIRON, V23, P133, DOI 10.1177/1420326X14522670
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
NR 40
TC 39
Z9 41
U1 1
U2 37
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2018
VL 34
IS 2
BP 229
EP 241
DI 10.1007/s00371-016-1330-0
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FU6ZO
UT WOS:000424001800007
DA 2024-07-18
ER

PT J
AU Sipiran, I
   Lokoc, J
   Bustos, B
   Skopal, T
AF Sipiran, Ivan
   Lokoc, Jakub
   Bustos, Benjamin
   Skopal, Tomas
TI Scalable 3D shape retrieval using local features and the signature
   quadratic form distance
SO VISUAL COMPUTER
LA English
DT Article
DE 3D shape retrieval; Local features; Signature quadratic form distance
ID SEARCH
AB We present a scalable and unsupervised approach for content-based retrieval on 3D model collections. Our goal is to represent a 3D shape as a set of discriminative local features, which is important to maintain robustness against deformations such as non-rigid transformations and partial data. However, this representation brings up the problem on how to compare two 3D models represented by feature sets. For solving this problem, we apply the signature quadratic form distance (SQFD), which is suitable for comparing feature sets. Using SQFD, the matching between two 3D objects involves only their representations, so it is easy to add new models to the collection. A key characteristic of the feature signatures, required by the SQFD, is that the final object representation can be easily obtained in a unsupervised manner. Additionally, as the SQFD is an expensive distance function, to make the system scalable we present a novel technique to reduce the amount of features by detecting clusters of key points on a 3D model. Thus, with smaller feature sets, the distance calculation is more efficient. Our experiments on a large-scale dataset show that our proposed matching algorithm not only performs efficiently, but also its effectiveness is better than state-of-the-art matching algorithms for 3D models.
C1 [Sipiran, Ivan] PUCP, Secc Ingn Informat, Lima, Peru.
   [Lokoc, Jakub; Skopal, Tomas] Charles Univ Prague, SIRET Res Grp, Fac Math & Phys, Prague, Czech Republic.
   [Bustos, Benjamin] Univ Chile, Dept Comp Sci, Santiago, Chile.
C3 Pontificia Universidad Catolica del Peru; Charles University Prague;
   Universidad de Chile
RP Sipiran, I (corresponding author), PUCP, Secc Ingn Informat, Lima, Peru.
EM ivan.sipiran@gmail.com; lokoc@ksi.ms.mff.cuni.cz; bebustos@dcc.uchile.cl
RI Sipiran, Ivan/AAL-7603-2020; Lokoč, Jakub/P-1216-2017; Sipiran,
   Ivan/GRR-8629-2022; Bustos, Benjamin/G-1170-2010; Skopal,
   Tomas/G-2679-2017
OI Bustos, Benjamin/0000-0002-3955-361X; Skopal, Tomas/0000-0002-6591-0879;
   Sipiran, Ivan/0000-0002-8766-3581
FU Programa Nacional de Innovacion para la Competitividad y Productividad,
   INNOVATE Peru [280-PNICP-BRI-2015]; Charles University [P46,
   SVV-2016-260331]; FONDECYT (Chile) [1140783]; Millennium Nucleus Center
   for Semantic Web Research [NC120004]
FX This work has been partially supported by Programa Nacional de
   Innovacion para la Competitividad y Productividad, INNOVATE Peru, Grant
   Nr. 280-PNICP-BRI-2015. This work has been also supported by Charles
   University projects P46 and SVV-2016-260331. Benjamin Bustos has been
   funded by FONDECYT (Chile) Project 1140783 and the Millennium Nucleus
   Center for Semantic Web Research, Grant Nr. NC120004.
CR Abdelrahman M, 2015, PROC CVPR IEEE, P187, DOI 10.1109/CVPR.2015.7298614
   [Anonymous], 2013, THESIS
   Bai X, 2015, IEEE T PATTERN ANAL, V37, P2361, DOI 10.1109/TPAMI.2015.2424863
   Beecks C., 2011, P ACM INT C MULT RET, P24
   Beecks Christian, 2009, P 17 ACM INT C MULT, P697, DOI DOI 10.1145/1631272.1631391
   Beecks Christian., 2010, ACM International Conference on Image and Video Retrieval, P438, DOI [10.1145/1816041.1816105, DOI 10.1145/1816041.1816105]
   Belkin M, 2008, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SGG'08), P278, DOI 10.1145/1377676.1377725
   Borg I., 2005, Modern Multidimensional Scaling: Theory and Applications
   Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1
   BRONSTEIN AM, 2011, TOG, V30, P1, DOI DOI 10.1145/1899404.1899405
   Bronstein MM, 2010, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2010.5539838
   Bu SH, 2014, IEEE MULTIMEDIA, V21, P38, DOI 10.1109/MMUL.2014.52
   Bustos B, 2005, ACM COMPUT SURV, V37, P345, DOI 10.1145/1118890.1118893
   Chang A.X., 2015, ARXIV151203012 CORR
   Fang Y, 2015, PROC CVPR IEEE, P2319, DOI 10.1109/CVPR.2015.7298845
   HAFNER J, 1995, IEEE T PATTERN ANAL, V17, P729, DOI 10.1109/34.391417
   Hetland ML, 2013, INFORM SYST, V38, P989, DOI 10.1016/j.is.2012.05.011
   Kimmel R, 1998, P NATL ACAD SCI USA, V95, P8431, DOI 10.1073/pnas.95.15.8431
   Leow WK, 2004, COMPUT VIS IMAGE UND, V94, P67, DOI 10.1016/j.cviu.2003.10.010
   Litman R, 2014, COMPUT GRAPH FORUM, V33, P127, DOI 10.1111/cgf.12438
   Liu ZB, 2015, NEUROCOMPUTING, V151, P583, DOI 10.1016/j.neucom.2014.06.090
   Lokoc J, 2013, LECT NOTES COMPUT SC, V8199, P312, DOI 10.1007/978-3-642-41062-8_31
   MICO ML, 1994, PATTERN RECOGN LETT, V15, P9, DOI 10.1016/0167-8655(94)90095-7
   Navarro G, 2009, SISAP 2009: 2009 SECOND INTERNATIONAL WORKSHOP ON SIMILARITY SEARCH AND APPLICATIONS, PROCEEDINGS, P3, DOI 10.1109/SISAP.2009.17
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Savelonas M., 2015, P EUR WORKSH 3D OBJ, P23
   Sipiran I., 2015, P EUR WORKSH 3D OBJ
   Sipiran I, 2011, VISUAL COMPUT, V27, P963, DOI 10.1007/s00371-011-0610-y
   Sipiran I, 2013, VISUAL COMPUT, V29, P1319, DOI 10.1007/s00371-013-0870-9
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Tabia H, 2014, PROC CVPR IEEE, P4185, DOI 10.1109/CVPR.2014.533
   Xie J, 2015, PROC CVPR IEEE, P1275, DOI 10.1109/CVPR.2015.7298732
NR 33
TC 5
Z9 5
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2017
VL 33
IS 12
BP 1571
EP 1585
DI 10.1007/s00371-016-1301-5
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FK4JE
UT WOS:000413458600007
DA 2024-07-18
ER

PT J
AU Yang, CL
   Pu, JX
   Dong, YS
   Liu, ZH
   Liang, LF
   Wang, XH
AF Yang, Chunlei
   Pu, Jiexin
   Dong, Yongsheng
   Liu, Zhonghua
   Liang, Lingfei
   Wang, Xiaohong
TI Salient object detection in complex scenes via D-S evidence theory based
   region classification
SO VISUAL COMPUTER
LA English
DT Article
DE Salient object detection; Complex scene; D-S evidence theory; Multiple
   feature fusion; Region classification
ID CONTRAST; ATTENTION
AB In complex scenes, multiple objects are often concealed in cluttered backgrounds. Their saliency is difficult to be detected by using conventional methods, mainly because single color contrast can not shoulder the mission of saliency measure; other image features should be involved in saliency detection to obtain more accurate results. Using Dempster-Shafer (D-S) evidence theory based region classification, a novel method is presented in this paper. In the proposed framework, depth feature information extracted from a coarse map is employed to generate initial feature evidences which indicate the probabilities of regions belonging to foreground or background. Based on the D-S evidence theory, both uncertainty and imprecision are modeled, and the conflicts between different feature evidences are properly resolved. Moreover, the method can automatically determine the mass functions of the two-stage evidence fusion for region classification. According to the classification result and region relevance, a more precise saliency map can then be generated by manifold ranking. To further improve the detection results, a guided filter is utilized to optimize the saliency map. Both qualitative and quantitative evaluations on three publicly challenging benchmark datasets demonstrate that the proposed method outperforms the contrast state-of-the-art methods, especially for detection in complex scenes.
C1 [Yang, Chunlei; Pu, Jiexin; Dong, Yongsheng; Liu, Zhonghua; Liang, Lingfei; Wang, Xiaohong] Henan Univ Sci & Technol, 263 Kaiyuan Rd, Luoyang, Peoples R China.
C3 Henan University of Science & Technology
RP Pu, JX (corresponding author), Henan Univ Sci & Technol, 263 Kaiyuan Rd, Luoyang, Peoples R China.
EM pjx2014stu@163.com
RI feng, chen/JLM-8296-2023; Chunlei, Yang/KJL-7321-2024; huang,
   shan/JVN-1240-2024
FU International S & T Cooperation Program of China [2011DFR10480]; Natural
   Science Foundation of China [61301230]; Key Project of Science and
   Technology of Henan [142107000021]
FX This work was supported in part by the International S & T Cooperation
   Program of China (No. 2011DFR10480), the Natural Science Foundation of
   China (No. 61301230), and the Key Project of Science and Technology of
   Henan (No. 142107000021).
CR Achanta R, 2010, IEEE IMAGE PROC, P2653, DOI 10.1109/ICIP.2010.5652636
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   ANDO T, 1995, LINEAR ALGEBRA APPL, V224, P57
   [Anonymous], CHIN J ELECT
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2010, TECH REP
   [Anonymous], P SPIE INT SOC OPT E
   [Anonymous], ADV NEURAL INF PROCE
   [Anonymous], 2007, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2007.383017
   [Anonymous], 2007, PROC IEEE C COMPUT V, DOI 10.1109/CVPR.2007.383267
   Bhattacharyya A, 2015, APPL SOFT COMPUT, V37, P479, DOI 10.1016/j.asoc.2015.08.029
   Chen YS, 2012, PROC CVPR IEEE, P654, DOI 10.1109/CVPR.2012.6247733
   Cheng MM, 2014, VISUAL COMPUT, V30, P443, DOI 10.1007/s00371-013-0867-4
   DEMPSTER AP, 1967, ANN MATH STAT, V38, P325, DOI 10.1214/aoms/1177698950
   Dong GG, 2015, IEEE GEOSCI REMOTE S, V12, P1247, DOI 10.1109/LGRS.2015.2390914
   Fan Q, 2014, J VIS COMMUN IMAGE R, V25, P1823, DOI 10.1016/j.jvcir.2014.09.003
   Frintrop S, 2015, PROC CVPR IEEE, P82, DOI 10.1109/CVPR.2015.7298603
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   [李弼程 Li Bicheng], 2002, 数据采集与处理, V17, P33
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Lin L, 2014, IEEE T IMAGE PROCESS, V23, P3191, DOI 10.1109/TIP.2014.2326776
   Lin TC, 2012, NEURAL COMPUT APPL, V21, P695, DOI 10.1007/s00521-011-0648-9
   Lu Y, 2012, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2012.6247785
   Margolin R, 2013, PROC CVPR IEEE, P1139, DOI 10.1109/CVPR.2013.151
   Margolin R, 2013, VISUAL COMPUT, V29, P381, DOI 10.1007/s00371-012-0740-x
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Qiu YH, 2015, NEUROCOMPUTING, V168, P538, DOI 10.1016/j.neucom.2015.05.073
   Scharfenberger C, 2013, PROC CVPR IEEE, P979, DOI 10.1109/CVPR.2013.131
   Shafer G., 1978, TECHNOMETRICS, V20, P242
   Shi KY, 2013, PROC CVPR IEEE, P2115, DOI 10.1109/CVPR.2013.275
   Shi YJ, 2015, VISUAL COMPUT, V31, P1191, DOI 10.1007/s00371-014-1005-7
   Song ML, 2014, INFORM SCIENCES, V281, P573, DOI 10.1016/j.ins.2013.09.036
   Tang H, 2017, J SIGNAL PROCESS SYS, V87, P197, DOI 10.1007/s11265-016-1131-8
   Tang RD, 2015, ACSR ADV COMPUT, V18, P663
   Tong N, 2015, PROC CVPR IEEE, P1884, DOI 10.1109/CVPR.2015.7298798
   Tong N, 2014, IEEE SIGNAL PROC LET, V21, P1035, DOI 10.1109/LSP.2014.2323407
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Xu M, 2015, VISUAL COMPUT, V31, P355, DOI 10.1007/s00371-014-0930-9
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang C, 2013, IEEE SIGNAL PROC LET, V20, P637, DOI 10.1109/LSP.2013.2260737
   Ye Qing, 2006, Systems Engineering and Electronics, V28, P1014
   Yu H., 2010, PROC ANN ACM INT C M, P891
   Zadeh L. A., 1986, AI magazine, V7, P85, DOI 10.1609/aimag.v7i2.542
   Zhang YD, 2014, INFORM SCIENCES, V281, P586, DOI 10.1016/j.ins.2013.12.043
   Zhu S., 2011, 2011 IEEE Workshop on Microelectronics and Electron Devices, P1
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 51
TC 6
Z9 6
U1 2
U2 36
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2017
VL 33
IS 11
BP 1415
EP 1428
DI 10.1007/s00371-016-1288-y
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FJ0TP
UT WOS:000412423100005
DA 2024-07-18
ER

PT J
AU Zheng, Q
   Zheng, CW
AF Zheng, Quan
   Zheng, Changwen
TI Adaptive sparse polynomial regression for camera lens simulation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th Conference on Computer Graphics International (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Sparse polynomial regression; Camera lens simulation; Lens effects;
   Photo-realistic rendering
AB Lens effects are crucial visual elements in the synthetic imagery, but rendering lens effects with complex full lens models is time-consuming. This paper proposes a polynomial regression-based approach for constructing a sparse and accurate polynomial lens model. Terms of a polynomial are built adaptively in a bottom-up approach. Depending on the distribution of aberrations, this approach partitions the light field and builds separate polynomial models for local light fields. A line pupil-based sampling method is presented to accelerate the generation of camera rays. In addition, a new Monte Carlo estimator is derived to support general Monte Carlo rendering. Experiments show that this approach significantly reduces the time cost of constructing a polynomial lens model in comparison to state-of-the-art methods, while achieving high imaging accuracy.
C1 [Zheng, Quan; Zheng, Changwen] Chinese Acad Sci, Inst Software, Sci & Technol Integrated Informat Syst Lab, Beijing, Peoples R China.
   [Zheng, Quan] Univ Chinese Acad Sci, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS
RP Zheng, Q (corresponding author), Chinese Acad Sci, Inst Software, Sci & Technol Integrated Informat Syst Lab, Beijing, Peoples R China.; Zheng, Q (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.
EM quan.zheng@outlook.com; cwzheng@ieee.org
RI Zheng, Quan/HZH-4993-2023
OI Zheng, Quan/0000-0001-5053-5511
FU  [9140A21010115HT05003]
FX This work was supported in part by the research grant (ref.
   9140A21010115HT05003). The camera lens data are courtesy of Emanuel
   Schrade et al.
CR [Anonymous], 2016, PHYS BASED RENDERING
   Gauss C.F, 1841, DIOPTRISCHE UNTERSUC, V1
   Guyon I., 2003, Journal of Machine Learning Research, V3, P1157, DOI 10.1162/153244303322753616
   Hachisuka T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618487
   Hanika J, 2014, COMPUT GRAPH FORUM, V33, P323, DOI 10.1111/cgf.12301
   Heidrich W, 1997, PROC GRAPH INTERF, P68
   HOPKINS GW, 1976, J OPT SOC AM, V66, P405, DOI 10.1364/JOSA.66.000405
   Hullin M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1965003
   Hullin MB, 2012, COMPUT GRAPH FORUM, V31, P1375, DOI 10.1111/j.1467-8659.2012.03132.x
   Jakob W., 2012, ACM T GRAPHIC, V31, P58
   Jekabsons G., 2008, Proceedings of IADIS International Conference, Applied Computing, P269
   Kelemen C, 2002, COMPUT GRAPH FORUM, V21, P531, DOI 10.1111/1467-8659.t01-1-00703
   Kolb C., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P317, DOI 10.1145/218380.218463
   Lafortune E. P., 1993, EDUGRAPHICS '93. First International Conference on Graphics Education. COMPUGRAPHICS '93. Third International Conference on Computational Graphics and Visualization Techniques. Combined Proceedings, P145
   Lee S, 2013, COMPUT GRAPH FORUM, V32, P1, DOI 10.1111/cgf.12145
   Lee Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866160
   Potmesil M., 1981, Computer Graphics, V15, P297, DOI 10.1145/965161.806818
   PUDIL P, 1994, PATTERN RECOGN LETT, V15, P1119, DOI 10.1016/0167-8655(94)90127-9
   Schrade E, 2016, COMPUT GRAPH FORUM, V35, P89, DOI 10.1111/cgf.12952
   Smith W. J., 2005, MODERN LENS DESIGN
   Steinert B, 2011, COMPUT GRAPH FORUM, V30, P1643, DOI 10.1111/j.1467-8659.2011.01851.x
   Todorovski L, 2004, LECT NOTES COMPUT SC, V3201, P441
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Wu JZ, 2013, VISUAL COMPUT, V29, P41, DOI 10.1007/s00371-012-0673-4
   Zheng N, 2010, J OPT SOC AM A, V27, P1791, DOI 10.1364/JOSAA.27.001791
NR 25
TC 4
Z9 4
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 715
EP 724
DI 10.1007/s00371-017-1402-9
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800004
DA 2024-07-18
ER

PT J
AU Niu, DM
   Bremer, PT
   Lindstrom, P
   Hamann, B
   Zhou, YF
   Zhang, CM
AF Niu, Dongmei
   Bremer, Peer-Timo
   Lindstrom, Peter
   Hamann, Bernd
   Zhou, Yuanfeng
   Zhang, Caiming
TI Two-dimensional shape retrieval using the distribution of extrema of
   Laplacian eigenfunctions
SO VISUAL COMPUTER
LA English
DT Article
DE Shape retrieval; Shape matching; Shape descriptor; Laplace operator;
   Signed natural neighbor graph; Graph matching
ID CLASSIFICATION; REPRESENTATION; RECOGNITION; ASSIGNMENT; SURFACES;
   SPECTRA
AB We propose a new method using the distribution of extrema of Laplacian eigenfunctions for two-dimensional (2D) shape description and matching. We construct a weighted directed graph, which we call signed natural neighbor graph, to represent a Laplacian eigenfunction of a shape. The nodes of this sparse graph are the extrema of the corresponding eigenfunction, and the edge weights are defined by signed natural neighbor coordinates derived from the local spatial arrangement of extrema. We construct the signed natural neighbor graphs defined by a small number of low-frequency Laplacian eigenfunctions of a shape to describe it. This shape descriptor is invariant under rigid transformations and uniform scaling, and is also insensitive to minor boundary deformations. When using our shape descriptor for matching two shapes, we determine their similarity by comparing the graphs induced by corresponding Laplacian eigenfunctions of the two shapes. Our experimental shape-matching results demonstrate that our method is effective for 2D shape retrieval.
C1 [Niu, Dongmei; Zhou, Yuanfeng; Zhang, Caiming] Shandong Univ, Sch Comp Sci & Technol, 1500 Shunhua Rd, Jinan 250101, Peoples R China.
   [Bremer, Peer-Timo; Lindstrom, Peter] Lawrence Livermore Natl Lab, 7000 East Ave,L-422, Livermore, CA 94551 USA.
   [Hamann, Bernd] Univ Calif Davis, Dept Comp Sci, Inst Data Anal & Visualizat, One Shields Ave, Davis, CA 95616 USA.
C3 Shandong University; United States Department of Energy (DOE); Lawrence
   Livermore National Laboratory; University of California System;
   University of California Davis
RP Niu, DM (corresponding author), Shandong Univ, Sch Comp Sci & Technol, 1500 Shunhua Rd, Jinan 250101, Peoples R China.
EM dongmei_19881123@hotmail.com; bremer5@llnl.gov; pl@llnl.gov;
   hamann@cs.ucdavis.edu; yfzhou@sdu.edu.cn; czhang@sdu.edu.cn
RI Zhang, Caiming/AHD-6558-2022; liu, xinyi/KFB-4466-2024
OI Zhang, Caiming/0000-0002-6365-6221; Lindstrom, Peter/0000-0003-3817-4199
CR Adamek T, 2004, IEEE T CIRC SYST VID, V14, P742, DOI 10.1109/TCSVT.2004.826776
   [Anonymous], 2009, International Journal of Computer Vision
   [Anonymous], TOOLBOX GRAPH
   [Anonymous], IEEE INT C SHAP MOD
   [Anonymous], 1997, Image Databases and Multi-Media Search, DOI DOI 10.1142/9789812797988_
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Chavel I., 1984, EIGENVALUES RIEMANNI
   Courant R., 1953, Methods of Mathematical Physics
   Cui M, 2007, VISUAL COMPUT, V23, P607, DOI 10.1007/s00371-007-0164-1
   Gao XB, 2010, PATTERN ANAL APPL, V13, P113, DOI 10.1007/s10044-008-0141-y
   Gdalyahu Y, 1999, IEEE T PATTERN ANAL, V21, P1312, DOI 10.1109/34.817410
   GORDON C, 1992, INVENT MATH, V110, P1, DOI 10.1007/BF01231320
   Grauman K, 2004, PROC CVPR IEEE, P220
   Hu JX, 2013, VISUAL COMPUT, V29, P949, DOI 10.1007/s00371-013-0850-0
   Ion A, 2011, COMPUT VIS IMAGE UND, V115, P817, DOI 10.1016/j.cviu.2011.02.006
   Isaacs JC, 2011, IEEE SYS MAN CYBERN, P3347, DOI 10.1109/ICSMC.2011.6084186
   Kim WY, 2000, SIGNAL PROCESS-IMAGE, V16, P95, DOI 10.1016/S0923-5965(00)00019-9
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Laiche N, 2014, SIGNAL PROCESS-IMAGE, V29, P556, DOI 10.1016/j.image.2014.01.009
   Latecki LJ, 2000, PROC CVPR IEEE, P424, DOI 10.1109/CVPR.2000.855850
   Li S, 2009, IEEE T SYST MAN CY A, V39, P227, DOI 10.1109/TSMCA.2008.2007988
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41
   MUNKRES J, 1957, J SOC IND APPL MATH, V5, P32, DOI 10.1137/0105003
   Okabe A., 1992, Spatial Tessellations: Concepts and Applications of Voronoi Diagrams
   ORourke J., 1999, ACM SIGACT News, V30, P31, DOI DOI 10.1145/568547.568559
   Peinecke N, 2007, COMPUT AIDED DESIGN, V39, P460, DOI 10.1016/j.cad.2007.01.014
   Peyre G., 2009, TOOLBOX FAST MARCHIN
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Reuter M, 2009, COMPUT GRAPH-UK, V33, P381, DOI 10.1016/j.cag.2009.03.005
   Rustamov Raif M, 2007, P S GEOM PROC, V257, P225
   Sebastian TB, 2004, IEEE T PATTERN ANAL, V26, P550, DOI 10.1109/TPAMI.2004.1273924
   Sharvit D, 1998, J VIS COMMUN IMAGE R, V9, P366, DOI 10.1006/jvci.1998.0396
   Shekar BH, 2014, 2014 FIFTH INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING (ICSIP 2014), P218, DOI 10.1109/ICSIP.2014.41
   Shewchuk J. R., 1996, Applied Computational Geometry. Towards Geometric Engineering. FCRC'96 Workshop, WACG'96. Selected Papers, P203, DOI 10.1007/BFb0014497
   Shu X, 2011, IMAGE VISION COMPUT, V29, P286, DOI 10.1016/j.imavis.2010.11.001
   Sibson R., 1981, Interpreting Multivariate Data, P21, DOI DOI 10.1002/ADVS.201700552
   Taubin Gabriel., 2000, State of the Art Report, P81
   Taylor M.E., 1996, Partial Differential Equations III: Nonlinear Equations
   Wang JW, 2012, PATTERN RECOGN LETT, V33, P134, DOI 10.1016/j.patrec.2011.09.042
   Xu CJ, 2009, IEEE T PATTERN ANAL, V31, P180, DOI 10.1109/TPAMI.2008.199
   Zhang DS, 2004, PATTERN RECOGN, V37, P1, DOI 10.1016/j.patcog.2003.07.008
   Zhang DS, 2002, SIGNAL PROCESS-IMAGE, V17, P825, DOI 10.1016/S0923-5965(02)00084-X
NR 42
TC 3
Z9 3
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2017
VL 33
IS 5
BP 607
EP 624
DI 10.1007/s00371-016-1211-6
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ER4KB
UT WOS:000398767300006
DA 2024-07-18
ER

PT J
AU Liang, H
   Chang, J
   Kazmi, IK
   Zhang, JJ
   Jiao, PF
AF Liang, Hui
   Chang, Jian
   Kazmi, Ismail K.
   Zhang, Jian J.
   Jiao, Peifeng
TI Hand gesture-based interactive puppetry system to assist storytelling
   for children
SO VISUAL COMPUTER
LA English
DT Article
DE Serious game; Digital storytelling; Virtual puppet; Hand gesture
   recognizing
ID ABILITY; SKILLS
AB Digital techniques have been used to assist narrative and storytelling, especially in many pedagogical practices. With the rapid development of HCI techniques, saturated with digital media in their daily lives, young children, demands more interactive learning methods and meaningful immersive learning experiences. In this paper, we propose a novel hand gesture-based puppetry storytelling system which provides a more intuitive and natural human computer interaction method for young children to develop narrative ability in virtual story world. Depth motion sensing and hand gestures control technology is utilized in the implementation of user-friendly interaction. Young players could intuitively use hand gestures to manipulate virtual puppet to perform story and interact with different items in virtual environment to assist narration. Based on the result of the evaluation, this novel digital storytelling system shows positive pedagogical functions on children's narrating ability as well as the competencies of cognitive and motor coordination. The usability of the system is preliminary examined in our test, and the results which showed that young children can benefit from playing with Puppet Narrator.
C1 [Liang, Hui] Commun Univ China, Beijing, Peoples R China.
   [Chang, Jian; Kazmi, Ismail K.] Bournemouth Univ, Natl Ctr Comp Animat, Poole, Dorset, England.
   [Zhang, Jian J.] Bournemouth Univ, Natl Ctr Comp Animat, Comp Graph, Poole, Dorset, England.
   [Jiao, Peifeng] Southern Med Univ, Basic Med Sch, Guangzhou, Guangdong, Peoples R China.
C3 Communication University of China; Bournemouth University; Bournemouth
   University; Southern Medical University - China
RP Chang, J (corresponding author), Bournemouth Univ, Natl Ctr Comp Animat, Poole, Dorset, England.
EM jchang@bournemouth.ac.uk
OI Chang, Jian/0000-0003-4118-147X; Kazmi, Ismail/0000-0001-8005-7354;
   Zhang, Jian/0000-0002-7069-5771
FU People Programme (Marie Curie Actions) of the European Union [623883];
   National Natural Science Foundation of China (NSFC) [31200708]; 
   [FP7-ICT-2013.8.1 611383]
FX We would like to thank Mr. Mark Dodwell and Shujie Deng for proofreading
   the draft. The research leading to these results has received funding
   from the People Programme (Marie Curie Actions) of the European Union's
   Seventh Framework Programme FP7/2007-2013/under REA Grant Agreement No.
   [623883]-"AniM". The authors also acknowledge partial support from the
   FP7/2007-2013 ("Dr. Inventor", FP7-ICT-2013.8.1 611383) and the National
   Natural Science Foundation of China (NSFC, 31200708).
CR Al-Mousawi Z., 2012, P 14 INT C INF INT W, P26, DOI [10.1145/2428736.2428746, DOI 10.1145/2428736.2428746]
   [Anonymous], 1972, TDR DRAMA REV
   Arora N, 2015, 6TH INTERNATIONAL CONFERENCE ON COMPUTER & COMMUNICATION TECHNOLOGY (ICCCT-2015), P398, DOI 10.1145/2818567.2818681
   Backlund P., 2013, P 2013 5 INT C GAM V, P1, DOI [DOI 10.1109/VSGAMES.2013.6624226, DOI 10.1109/VS-GAMES.2013.6624226]
   BEARDSWORTH T, 1981, B PSYCHONOMIC SOC, V18, P19
   Best JB., 1999, COGNITIVE PSYCHOL, V5th ed, P15
   Bonsignore E, 2013, ACM T COMPUT-HUM INT, V20, DOI 10.1145/2491500.2491506
   Chappell G.E., 1980, LANG SPEECH HEAR SER, V11, P236
   Di Blas.N., 2009, Proceedings of the 8th International Conference on Interaction Design and Children (IDC '09), P44, DOI DOI 10.1145/1551788.1551797
   Di Blas N, 2012, INT J ARTS TECHNOL, V5, P271
   Eder D., 2010, Life lessons through storytelling: Children's exploration of ethics
   Feigenson L, 2004, TRENDS COGN SCI, V8, P307, DOI 10.1016/j.tics.2004.05.002
   Franceschini S, 2012, CURR BIOL, V22, P814, DOI 10.1016/j.cub.2012.03.013
   Gelman S.A., 2003, THE ESSENTIAL CHILD
   Greer R.D., 2002, Designing teaching strategies: An applied behavior analysis systems approach
   Greer R.D., 1991, Behavior and Social Issues, V1
   Greitzer FrankL., 2007, Journal on Educational Resources in Computing, V7, P2, DOI [10.1145/1281320.1281322, DOI 10.1145/1281320.1281322]
   Heinemann W., 1916, GREEK ANTHOLOGY
   Invitto S, 2015, PROCEEDINGS OF THE 2015 7TH INTERNATIONAL CONFERENCE ON INTELLIGENT TECHNOLOGIES FOR INTERACTIVE ENTERTAINMENT, P88, DOI 10.4108/icst.intetain.2015.259537
   Kelleher, 2006, CMUCS06171
   Knoblich G, 2001, PSYCHOL SCI, V12, P467, DOI 10.1111/1467-9280.00387
   Knoblich G, 2001, J EXP PSYCHOL HUMAN, V27, P456, DOI 10.1037/0096-1523.27.2.456
   Liu BL, 2010, NEUROSCI LETT, V485, P43, DOI 10.1016/j.neulet.2010.08.059
   Loban W., 1976, NCTE Committee on Research Report No. 18
   Lu F, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1919
   Luck SJ, 2014, INTRODUCTION TO THE EVENT-RELATED POTENTIAL TECHNIQUE, 2ND EDITION, P1
   MACLACHLAN BG, 1988, J SPEECH HEAR DISORD, V53, P2, DOI 10.1044/jshd.5301.02
   Mazalek Ali., FUN GAMES C LEUVEN B, DOI DOI 10.1145/1823818.1823825
   MERRITT DD, 1987, J SPEECH HEAR RES, V30, P539, DOI 10.1044/jshr.3004.539
   More C, 2008, INTERV SCH CLIN, V43, P168, DOI 10.1177/1053451207312919
   Mystakidis S., 2014, P 2014 WORKSH INT DE, P97
   Ness D., 2007, KNOWLEDGE CONSTRUCTI
   PAGE JL, 1985, TOP LANG DISORD, V5, P16, DOI 10.1097/00011363-198503000-00004
   Pedersen EM., 1995, FORUM GENOVA, V33, P2
   Psomos P, 2012, PROCD SOC BEHV, V46, P1213, DOI 10.1016/j.sbspro.2012.05.277
   Rubart J, 2015, P 2015 WORKSH NARR H, P33
   Russell A, 2010, P 4 INT C TANG EMB E, P271
   Simion F, 2008, P NATL ACAD SCI USA, V105, P809, DOI 10.1073/pnas.0707021105
   Skinner B.F., 1968, The technology of teaching
   Skinner RA, 2001, HUM MOVEMENT SCI, V20, P73, DOI 10.1016/S0167-9457(01)00029-X
   Stein N.L., 1975, ANAL STORY COMPREHEN
   Thompson P, 2013, COMPUT EDUC, V65, P12, DOI 10.1016/j.compedu.2012.12.022
   Vargas E.A., 1991, J BEHAV EDUC, V1, P235, DOI DOI 10.1007/BF00957006
   Wai J, 2009, J EDUC PSYCHOL, V101, P817, DOI 10.1037/a0016127
   Westby C., 1984, DEV NARRATIVE LANGUA, P103
   Widjajanto W., 2008, Proceedings of the 6th International Conference on Advances in Mobile Computing and Multimedia, P464
NR 46
TC 17
Z9 19
U1 1
U2 42
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2017
VL 33
IS 4
BP 517
EP 531
DI 10.1007/s00371-016-1272-6
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA ER4JZ
UT WOS:000398767100009
OA hybrid
DA 2024-07-18
ER

PT J
AU Szirmay-Kalos, L
   Magdics, M
   Tóth, B
AF Szirmay-Kalos, Laszlo
   Magdics, Milan
   Toth, Balazs
TI Volume enhancement with externally controlled anisotropic diffusion
SO VISUAL COMPUTER
LA English
DT Article
DE Medical imaging; Upsampling; Noise filtering; Sharpening; Anisotropic
   diffusion
AB This paper proposes a method to enhance volumetric data using anisotropic diffusion controlled by another voxel array representing the same object with different physical quantities. The main application of this approach is to enhance volumetric functional data (obtained e.g. with PET or SPECT) based on anatomic (e.g. CT or MRI) information. Enhancement includes noise removal, sharpening and resolution upsampling. As different modalities measure different physical quantities that may or may not be correlated, enhancement must be carefully designed not to introduce spurious features that are present only in one modality. Forward diffusion working with non-negative diffusivity guarantees this kind of causality but also limits the potential of enhancement. To allow the preservation or even the increase of the dynamic range, diffusion should also go backwards. Therefore, we propose a forward-backward diffusion scheme for the enhancement where stability and the avoidance of spurious features are provided by the automatic determination of parameters controlling the diffusion process.
C1 [Szirmay-Kalos, Laszlo; Magdics, Milan; Toth, Balazs] Budapest Univ Technol & Econ, Budapest, Hungary.
C3 Budapest University of Technology & Economics
RP Szirmay-Kalos, L (corresponding author), Budapest Univ Technol & Econ, Budapest, Hungary.
EM szirmay@iit.bme.hu
RI Szirmay-Kalos, Laszlo/H-3853-2012
OI Szirmay-Kalos, Laszlo/0000-0002-8523-2315
FU OTKA [K-104476, VKSZ-14 PET/MRI 7T]
FX This work has been supported by OTKA K-104476 and VKSZ-14 PET/MRI 7T
   projects.
CR Bini AA, 2014, VISUAL COMPUT, V30, P311, DOI 10.1007/s00371-013-0857-6
   CATTE F, 1992, SIAM J NUMER ANAL, V29, P182, DOI 10.1137/0729012
   Chan C, 2009, PHYS MED BIOL, V54, P7379, DOI 10.1088/0031-9155/54/24/009
   CRANK J, 1947, P CAMB PHILOS SOC, V43, P50, DOI 10.1007/BF02127704
   Erlandsson K, 2012, PHYS MED BIOL, V57, pR119, DOI 10.1088/0031-9155/57/21/R119
   Fei B, 2012, MED PHYS, V39, P3888, DOI 10.1118/1.4735870
   Gilboa G, 2002, IEEE T IMAGE PROCESS, V11, P689, DOI 10.1109/TIP.2002.800883
   Jung Y, 2013, VISUAL COMPUT, V29, P805, DOI 10.1007/s00371-013-0833-1
   Kopf J., 2007, ACM T GRAPHIC, V26
   Magdics M., 2010, WORLD MOL IM C
   Marta Zsolt, 2013, KEPAF 13, P144
   Marta Zsolt, 2012, P 16 CENTR EUR SEM C
   Papp L., 2014, IEE ENUCL SCI S MED
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Richardt C, 2012, COMPUT GRAPH FORUM, V31, P247, DOI 10.1111/j.1467-8659.2012.03003.x
   Rousset O, 2007, PET CLIN, V2, P235, DOI 10.1016/j.cpet.2007.10.005
   Salvado O, 2006, PROC SPIE, V6144, DOI 10.1117/12.649098
   Skretting A, 2009, EUR J NUCL MED MOL I, V36, P536, DOI 10.1007/s00259-008-1032-6
   Soret M, 2007, J NUCL MED, V48, P932, DOI 10.2967/jnumed.106.035774
   Suri JS, 2002, INT C PATT RECOG, P508, DOI 10.1109/ICPR.2002.1044779
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Weickert J., 1998, ANISOTROPIC DIFFUSIO, V1
NR 22
TC 3
Z9 3
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2017
VL 33
IS 3
BP 331
EP 342
DI 10.1007/s00371-015-1203-y
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EL1KM
UT WOS:000394379300007
DA 2024-07-18
ER

PT J
AU Chou, JK
   Yang, CK
AF Chou, Jia-Kai
   Yang, Chuan-Kai
TI Obfuscated volume rendering
SO VISUAL COMPUTER
LA English
DT Article
DE Obfuscation; Volume rendering; Privacy preserving processing
AB Analyzing and processing various data types in a privacy-preserving perspective has been researched in many disciplines; however, such an issue draws very limited attention in the research field of scientific visualization. We wondered if it is possible to delegate the rendering of a volume data set to a remote server(s) while still being able to preserve its privacy to certain extent. This paper presents a block-based volume data transformation algorithm that obfuscates a volume data set so as to reduce the user's privacy concern when the volume data set is to be uploaded to a remote server. In addition, a privacy-aware transfer function adjustment is proposed so that not only the privacy is protected during the rendering process, but also the computational loading could be leveraged to the server side as much as possible. Experimental results show that the proposed method yields visually satisfactory results compared with a normal direct volume rendering approach. Moreover, the decrease of the rendering efficiency caused by the proposed method is still controlled within an acceptable range. A case study proves that the proposed approach can be adopted in practice. This work explores the possibility of rendering a volume data set through remote server(s) while the privacy of data is still maintained.
C1 [Chou, Jia-Kai] Natl Taiwan Univ Sci & Technol, Informat Management, Taipei, Taiwan.
   [Yang, Chuan-Kai] Natl Taiwan Univ Sci & Technol, Informat Management Dept, Taipei, Taiwan.
C3 National Taiwan University of Science & Technology; National Taiwan
   University of Science & Technology
RP Chou, JK (corresponding author), Natl Taiwan Univ Sci & Technol, Informat Management, Taipei, Taiwan.
EM D9809101@mail.ntust.edu.tw; ckyang@cs.ntust.edu.tw
FU Ministry of Science and Technology, Taiwan [104-2917-I-564-068,
   101-2221-E-011-150-MY3]
FX This study is partially funded by the Ministry of Science and
   Technology, Taiwan (grant number 104-2917-I-564-068 and
   101-2221-E-011-150-MY3).
CR [Anonymous], IEEE SIGNAL PROCESS
   Chu K.-Y., 2013, Proceedings of the 21st ACM international conference on Multimedia, P597
   Dasgupta A., 2011, PRIVACY PRESERVING D
   Dasgupta A, 2011, IEEE T VIS COMPUT GR, V17, P2241, DOI 10.1109/TVCG.2011.163
   Demaine ED, 2007, GRAPH COMBINATOR, V23, P195, DOI 10.1007/s00373-007-0713-4
   Devore J.L., 1987, Probability and statistics for engineering and the sciences, V2nd
   Erkin Z., 2007, EURASIP Journal on Information Security, V7, P1
   Fontaine C, 2007, EURASIP J INF SECUR, DOI 10.1155/2007/13801
   Gautam Aditee, 2011, INT J ADV ENG SCI TE, V8, p90 
   Gentry C, 2010, COMMUN ACM, V53, P97, DOI 10.1145/1666420.1666444
   Jolfaei Alireza, 2010, Journal of Theoretical and Applied Information Technology, V19, P117
   Koller D, 2004, ACM T GRAPHIC, V23, P695, DOI 10.1145/1015706.1015782
   Lauter K, 2011, PROCEEDINGS OF THE 3RD ACM WORKSHOP CLOUD COMPUTING SECURITY WORKSHOP (CCSW'11), P113
   LEVOY M, 1988, IEEE COMPUT GRAPH, V8, P29, DOI 10.1109/38.511
   Lin SC, 2014, J NUCL MED, V55, P73, DOI 10.2967/jnumed.113.121897
   Liu W., 2009, 2 INT C INT SCI INF, P352
   Machanavajjhala A, 2007, ACM T KNOWL DISCOV D, V1, P1, DOI DOI 10.1145/1217299.1217302
   Mitra A., 2006, INT J COMPUTER SCI, V1, P127
   Mohanty M., 2012, Proceedings of the 20th ACM international conference on Multimedia, P1105
   Mohanty M, 2013, INT CONF CLOUD COMP, P531, DOI 10.1109/CloudCom.2013.77
   Pitman J., 1993, Probability
   SHAMIR A, 1979, COMMUN ACM, V22, P612, DOI 10.1145/359168.359176
   Sweeney L, 2002, INT J UNCERTAIN FUZZ, V10, P557, DOI 10.1142/S0218488502001648
   Wu YQ, 2001, PROCEEDINGS OF THE 6TH INTERNATIONAL SYMPOSIUM ON MOLTEN SALT CHEMISTRY AND TECHNOLOGY, P359, DOI 10.1109/CGI.2001.934699
   Yao A. C., 1982, 23rd Annual Symposium on Foundations of Computer Science, P160, DOI 10.1109/SFCS.1982.38
   Younes MAB, 2008, INT J COMPUT SCI NET, V8, P191
NR 26
TC 4
Z9 4
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2016
VL 32
IS 12
BP 1593
EP 1604
DI 10.1007/s00371-015-1143-6
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EB3NH
UT WOS:000387271200008
DA 2024-07-18
ER

PT J
AU Guo, H
   Zhu, DH
   Mordohai, P
AF Guo, Hao
   Zhu, Dehai
   Mordohai, Philippos
TI Correspondence estimation for non-rigid point clouds with automatic part
   discovery
SO VISUAL COMPUTER
LA English
DT Article
DE Non-rigid correspondence; Point clouds; Shape matching
ID SHAPE CORRESPONDENCE; REGISTRATION; SURFACE; DEFORMATION
AB We propose an approach for estimating non-rigid correspondences between two shapes that can handle articulation and deformation of the surfaces to be matched. It operates on open or closed surfaces represented by point clouds and, therefore, it can be applied on other representations that can be converted into point clouds. Our method is capable of automatically discovering the articulated parts of the surface without requiring knowledge of the topology or the number of rigid parts. Processing begins by estimating potential sparse correspondences between the source and the target surface. These are used to align the largest corresponding parts of the two surfaces. Fragments of the surface that are not consistent with this alignment generate part hypotheses on which the algorithm is applied recursively. We present qualitative and quantitative results on four datasets comprising open and closed surfaces.
C1 [Guo, Hao; Zhu, Dehai] China Agr Univ, Dept Geog & Geog Informat Sci, Beijing, Peoples R China.
   [Mordohai, Philippos] Stevens Inst Technol, Comp Sci, Hoboken, NJ 07030 USA.
C3 China Agricultural University; Stevens Institute of Technology
RP Mordohai, P (corresponding author), Stevens Inst Technol, Comp Sci, Hoboken, NJ 07030 USA.
EM guohaolys@cau.edu.cn; zhudehai@263.net; mordohai@cs.stevens.edu
OI Guo, Hao/0000-0001-9317-4015
FU China Scholarship Council (CSC); National Science Foundation [1217797];
   Google; Div Of Information & Intelligent Systems; Direct For Computer &
   Info Scie & Enginr [1217797] Funding Source: National Science Foundation
FX Hao Guo is supported by a China Scholarship Council (CSC) scholarship.
   This work has been partially supported by National Science Foundation
   Award #1217797 and a Google Research Award. The authors are grateful to
   Qin Ma and Ke Wang from the Key Laboratory of Agricultural Information
   Acquisition Technology of Ministry of Agriculture, China Agricultural
   University, for their help in data acquisition. They are also grateful
   to Yusuf Sahillioglu for providing details on how the SCAPE data were
   used in [30].
CR Allen B, 2002, ACM T GRAPHIC, V21, P612, DOI 10.1145/566570.566626
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], 2011, ICRA
   [Anonymous], 2009, IEEE INT C ROB AUT
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Boyer E., 2011, EUR WORKSH 3D OBJ RE
   Bronstein Alexander M, 2010, P EUROGRAPHICS WORKS
   Brown BJ, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276404, 10.1145/1239451.1239472]
   Budd C, 2013, INT J COMPUT VISION, V102, P256, DOI 10.1007/s11263-012-0553-4
   Chang W, 2009, COMPUT GRAPH FORUM, V28, P447, DOI 10.1111/j.1467-8659.2009.01384.x
   CHEN Y, 1992, IMAGE VISION COMPUT, V10, P145, DOI 10.1016/0262-8856(92)90066-C
   Frome A, 2004, LECT NOTES COMPUT SC, V3023, P224
   Huang QX, 2008, COMPUT GRAPH FORUM, V27, P1449, DOI 10.1111/j.1467-8659.2008.01285.x
   Huang QX, 2013, COMPUT GRAPH FORUM, V32, P177, DOI 10.1111/cgf.12184
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kanezaki A., 2014, 3DV
   Kim VG, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185550
   Kim VG, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964974
   Li H, 2008, COMPUT GRAPH FORUM, V27, P1421, DOI 10.1111/j.1467-8659.2008.01282.x
   Lipman Y, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531378
   Ma JY, 2014, IEEE T IMAGE PROCESS, V23, P1706, DOI 10.1109/TIP.2014.2307478
   Mitra NJ, 2006, ACM T GRAPHIC, V25, P560, DOI 10.1145/1141911.1141924
   Mitra NJ, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239514
   Mitra NJ, 2004, INT J COMPUT GEOM AP, V14, P261, DOI 10.1142/S0218195904001470
   Rodolà E, 2014, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2014.532
   Rodolà E, 2013, IEEE I CONF COMP VIS, P1169, DOI 10.1109/ICCV.2013.149
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Sahillioglu Y, 2014, COMPUT GRAPH FORUM, V33, P121, DOI 10.1111/cgf.12480
   Sahillioglu Y, 2014, COMPUT GRAPH FORUM, V33, P63, DOI 10.1111/cgf.12278
   Sahillioglu Y, 2013, COMPUT GRAPH FORUM, V32, P177, DOI 10.1111/cgf.12007
   Salti S, 2014, COMPUT VIS IMAGE UND, V125, P251, DOI 10.1016/j.cviu.2014.04.011
   Sharma A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2481, DOI 10.1109/CVPR.2011.5995455
   Sipiran I, 2013, IEEE I CONF COMP VIS, P817, DOI 10.1109/ICCV.2013.106
   Starck J., 2007, ICCV
   Surazhsky V, 2005, ACM T GRAPHIC, V24, P553, DOI 10.1145/1073204.1073228
   Tam GKL, 2013, IEEE T VIS COMPUT GR, V19, P1199, DOI 10.1109/TVCG.2012.310
   Tevs A, 2009, PROC CVPR IEEE, P1185, DOI 10.1109/CVPRW.2009.5206775
   van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P1681, DOI 10.1111/j.1467-8659.2011.01884.x
   Zeng Y, 2011, PROC CVPR IEEE, P1225, DOI 10.1109/CVPR.2011.5995513
   Zhang H, 2008, COMPUT GRAPH FORUM, V27, P1431, DOI 10.1111/j.1467-8659.2008.01283.x
NR 40
TC 6
Z9 6
U1 1
U2 16
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2016
VL 32
IS 12
BP 1511
EP 1524
DI 10.1007/s00371-015-1136-5
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EB3NH
UT WOS:000387271200002
DA 2024-07-18
ER

PT J
AU Pei, LS
   Ye, M
   Zhao, XZ
   Dou, YM
   Bao, J
AF Pei, Lishen
   Ye, Mao
   Zhao, Xuezhuan
   Dou, Yumin
   Bao, Jiao
TI Action recognition by learning temporal slowness invariant features
SO VISUAL COMPUTER
LA English
DT Article
DE Action recognition; Temporal slowness regularization; Spatio-temporal
   features; Independent subspace analysis; Support vector machine
AB Deep learning approaches emphasized on learning spatio-temporal features for action recognition. Different to previous works, we separate the spatio-temporal feature learning unity into the spatial feature learning and the spatial/temporal feature pooling procedures. Using the temporal slowness regularized independent subspace analysis network, we learn invariant spatial features from sampled video cubes. To be robust to the cluttered backgrounds, we incorporate the denoising criterion to our network. The local spatio-temporal features are obtained by pooling features from the spatial and the temporal aspects. The key points are that we learn spatial features from video cubes and pool features from spatial feature sequences. We evaluate the learned local spatio-temporal features on three benchmark action datasets. Extensive experiments demonstrate the effectiveness of the novel feature learning architecture.
C1 [Pei, Lishen; Ye, Mao; Dou, Yumin; Bao, Jiao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Ctr Robot, Key Lab NeuroInformat,Minist Educ, Chengdu 611731, Peoples R China.
   [Zhao, Xuezhuan] Chinese Acad Sci, Chengdu Inst Comp Applicat, Chengdu 610041, Peoples R China.
C3 University of Electronic Science & Technology of China; Chengdu
   Institute of Computer Application, CAS; Chinese Academy of Sciences
RP Ye, M (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Ctr Robot, Key Lab NeuroInformat,Minist Educ, Chengdu 611731, Peoples R China.
EM cvlab.uestc@gmail.com
RI Ye, Mao/K-3012-2019
OI Ye, Mao/0000-0001-9253-1332; Ye, Mao/0000-0003-4760-8702
FU National Natural Science Foundation of China [61375038]
FX This work was supported in part by the National Natural Science
   Foundation of China (61375038).
CR Andrew G., 2007, P 24 INT C MACH LEAR, V24, P33, DOI [10.1145/1273496.1273501, DOI 10.1145/1273496.1273501]
   [Anonymous], 2009, NIPS
   [Anonymous], IEEE INT C COMP VIS
   [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], P ACM INT C MULT
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen B, 2010, THESIS U BRIT COLUMB
   Cox DD, 2005, NAT NEUROSCI, V8, P1145, DOI 10.1038/nn1519
   Das Dawn D, 2016, VISUAL COMPUT, V32, P289, DOI 10.1007/s00371-015-1066-2
   Delaitre V., 2010, Proceedings of the British Machine Vision Conference, P1, DOI DOI 10.5244/C.24.97
   Hyvärinen A, 2009, COMPUT IMAGING VIS, V39, P1
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Ji S., 2012, INT C MACH LEARN, P3212
   Jiang XB, 2014, VISUAL COMPUT, V30, P1021, DOI 10.1007/s00371-014-0923-8
   Jiang ZL, 2012, IEEE T PATTERN ANAL, V34, P533, DOI 10.1109/TPAMI.2011.147
   Karlinsky L., 2010, IEEE C NEUR INF PROC
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Klaser A., 2008, P BMVC, P275, DOI DOI 10.5244/C.22.99
   Lan T, 2011, IEEE I CONF COMP VIS, P2003, DOI 10.1109/ICCV.2011.6126472
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Larochelle D., 2007, P 24 INT C MACHINE L, P473, DOI DOI 10.1145/1273496.1273556
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Li N, 2008, SCIENCE, V321, P1502, DOI 10.1126/science.1160028
   Liang X., 2013, ACM International Conference on Multimedia, P263
   Marszalek M, 2009, PROC CVPR IEEE, P2921, DOI 10.1109/CVPRW.2009.5206557
   Memisevic R., 2008, IEEE C COMP VIS PATT, P1
   Ng, 2007, ADV NEURAL INF PROCE, P801
   Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4
   Pei L., 2013, IEEE INT C IM PROC
   Pei LS, 2015, MULTIMED TOOLS APPL, V74, P10801, DOI 10.1007/s11042-014-2207-8
   Raptis M, 2012, PROC CVPR IEEE, P1242, DOI 10.1109/CVPR.2012.6247807
   Rodriguez M., 2008, IEEE INT C COMP VIS, P3361
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Shi F, 2013, PROC CVPR IEEE, P2595, DOI 10.1109/CVPR.2013.335
   Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang H., 2010, BRIT MACH VIS C
   Willems G, 2008, LECT NOTES COMPUT SC, V5303, P650, DOI 10.1007/978-3-540-88688-4_48
   Zou W., 2012, ADV NEURAL INFORM PR, V25, P3203, DOI DOI 10.5555/2999325.2999492
NR 40
TC 9
Z9 9
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2016
VL 32
IS 11
BP 1395
EP 1404
DI 10.1007/s00371-015-1090-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EA2BS
UT WOS:000386397000004
DA 2024-07-18
ER

PT J
AU Mao, AH
   Luo, J
   Chen, J
   Li, GQ
AF Mao Aihua
   Luo Jie
   Chen Jun
   Li Guiqing
TI A new fast normal-based interpolating subdivision scheme by cubic Bezier
   curves
SO VISUAL COMPUTER
LA English
DT Article
DE Interpolating subdivision; Normal vectors; Bezier curves; Curve design;
   Surface design
ID GEOMETRIC HERMITE INTERPOLATION; SURFACE INTERPOLATION; CATMULL-CLARK;
   DESIGN; MESHES
AB In this paper, we propose a new fast normal-based interpolating subdivision scheme for curve and surface design. Different from the 4-points interpolating subdivision scheme, it is based on cubic Bezier curves and the normal vectors are used to generate a circle. Both a convex edge and an inflexion edge can be subdivided into convex sub-edges and then generate smooth curves. Under proper angle conditions, this subdivision scheme converges and the limit curve will be smoothness. When applying it to subdivide surface on triangle/quadrilateral meshes, we use the normal vectors and have no need to consider the meshes neighboring to the current surface elements. Such advantage leads to that the subdivision scheme has fast rendering speed without changing the topology of the meshes. Subdivision examples and results by our scheme are illustrated and meantime is compared with those generated by other well-known schemes. It shows that this scheme can generate a more smooth curve based on both a convex edge and an inflexion edge, and the limit surface has better smoothness than those of other interpolating schemes.
C1 [Mao Aihua; Chen Jun; Li Guiqing] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Guangdong, Peoples R China.
   [Luo Jie] Guangzhou Univ, Sch Fine Art & Artist Design, Guangzhou 510006, Guangdong, Peoples R China.
C3 South China University of Technology; Guangzhou University
RP Mao, AH (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Guangdong, Peoples R China.
EM ahmao@scut.edu.cn; jieluo@gzhu.edu.cn
FU National Natural Science Foundations of China [61502112]; Science and
   Technology Project of Guangdong Province [2015A030401030,
   2015A020219014, 2013B021600011]; Science and Technology Project of
   Guangzhou City [2014J410 0158]; Fundamental Research Funds for the
   Central Universities [2015zz034]; Guangdong Province Universities and
   Colleges Excellent Youth Teacher Training Program
FX The work of this paper was financially supported by The National Natural
   Science Foundations of China (No. 61502112), The Science and Technology
   Project of Guangdong Province (No. 2015A030401030, No. 2015A020219014,
   No. 2013B021600011), The Science and Technology Project of Guangzhou
   City (No. 2014J410 0158) and The Fundamental Research Funds for the
   Central Universities (No. 2015zz034) and The Guangdong Province
   Universities and Colleges Excellent Youth Teacher Training Program.
CR Aiwu Zhang, 2010, 2010 2nd Conference on Environmental Science and Information Application Technology (ESIAT 2010), P36, DOI 10.1109/ESIAT.2010.5567272
   Cashman TJ, 2012, COMPUT GRAPH FORUM, V31, P42, DOI 10.1111/j.1467-8659.2011.02083.x
   CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   Conti C, 2011, ADV COMPUT MATH, V35, P217, DOI 10.1007/s10444-011-9175-6
   Deng CY, 2010, VISUAL COMPUT, V26, P137, DOI 10.1007/s00371-009-0393-6
   DOO D, 1978, COMPUT AIDED DESIGN, V10, P356, DOI 10.1016/0010-4485(78)90111-2
   DYN N, 1990, ACM T GRAPHIC, V9, P160, DOI 10.1145/78956.78958
   Dyn N, 2012, COMPUT AIDED GEOM D, V29, P332, DOI 10.1016/j.cagd.2012.02.004
   Egges A, 2007, VISUAL COMPUT, V23, P317, DOI 10.1007/s00371-007-0113-z
   Farin G, 2008, COMPUT AIDED DESIGN, V40, P476, DOI 10.1016/j.cad.2008.01.003
   Flotynski J, 2015, VISUAL COMPUT, V31, P1287, DOI 10.1007/s00371-014-1011-9
   Han XA, 2008, J COMPUT APPL MATH, V217, P180, DOI 10.1016/j.cam.2007.06.027
   Hernández-Mederos V, 2013, GRAPH MODELS, V75, P79, DOI 10.1016/j.gmod.2012.12.001
   Jüttler B, 2002, VISUAL COMPUT, V18, P326, DOI 10.1007/s003710100153
   Karbacher S., 2000, VISION MODELING VISU, P163
   Kobbelt L, 2000, COMP GRAPH, P103, DOI 10.1145/344779.344835
   Lai SH, 2006, VISUAL COMPUT, V22, P865, DOI 10.1007/s00371-006-0072-9
   Li G, 2007, COMPUT GRAPH FORUM, V26, P185, DOI 10.1111/j.1467-8659.2007.01015.x
   Li X, 2012, COMPUT AIDED GEOM D, V29, P474, DOI 10.1016/j.cagd.2012.03.008
   Lin SJ, 2013, GRAPH MODELS, V75, P247, DOI 10.1016/j.gmod.2013.03.002
   Liu CM, 2013, APPL MATH COMPUT, V219, P5339, DOI 10.1016/j.amc.2012.11.043
   Loop CT, 1987, THESIS
   Mao AH, 2008, COMPUT AIDED DESIGN, V40, P916, DOI 10.1016/j.cad.2008.06.002
   Mao AH, 2011, COMPUT AIDED DESIGN, V43, P1854, DOI 10.1016/j.cad.2011.06.009
   Mao Z.H., 2005, P 13 INT C CENTR EUR, P13
   Marinov M, 2005, MATH VISUAL, P301, DOI 10.1007/3-540-26808-1_17
   Schaefer S., 2003, CURVE SURFACE FITTIN, P373
   Xu LH, 2001, COMPUT AIDED GEOM D, V18, P817, DOI 10.1016/S0167-8396(01)00053-X
   Yang XN, 2006, COMPUT AIDED GEOM D, V23, P243, DOI 10.1016/j.cagd.2005.10.001
   Yang XN, 2005, COMPUT AIDED DESIGN, V37, P497, DOI 10.1016/j.cad.2004.10.008
   Zhao HX, 2009, SMI 2009: IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P119, DOI 10.1109/SMI.2009.5170172
   Zheng JM, 2006, IEEE T VIS COMPUT GR, V12, P301, DOI 10.1109/TVCG.2006.49
   Zhou L, 2014, COMPUT AIDED DESIGN, V49, P18, DOI 10.1016/j.cad.2013.12.004
NR 33
TC 12
Z9 13
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2016
VL 32
IS 9
BP 1085
EP 1095
DI 10.1007/s00371-015-1175-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DU4DE
UT WOS:000382161400003
DA 2024-07-18
ER

PT J
AU Krompiec, P
   Park, K
   Liang, D
   Lee, C
AF Krompiec, Przemyslaw
   Park, Kyoungju
   Liang, Dongxue
   Lee, Changmin
TI Deformable strokes towards temporally coherent video painting
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Deformable strokes; Non-photorealistic rendering; Temporal coherency;
   Video painting
ID ANIMATION
AB We present an automatic and robust technique for creating non-photorealistic rendering (NPR) and animation, starting from a video that depicts the shape details and follows the motion of underlying objects. We generate NPR from the initial frame of the source video using a greedy algorithm for stroke placements and models, in combination with a saliency map and a flow-guided difference-of-Gaussian filter. Our stroke model uses a set of triangles whose vertices are particles and whose edges are springs. Using a physics-based framework, the generated and rendered strokes are translated, rotated and deformed by forces exerted from the sequential frames. External forces acting on strokes are calculated according to temporally and spatially smoothed per-pixel optical flow vectors. After simulating each frame, we delete unnecessary strokes and add new strokes for disappearing and appearing objects, but only if necessary to avoid popping and scintillation. Our framework automatically generates the coherent animation of rendered strokes, preserving the appearance details and animating strokes along with the underlying objects. This had been difficult to achieve with previous user-guided methods and automatic but limited transformations methods.
C1 [Krompiec, Przemyslaw; Park, Kyoungju; Liang, Dongxue; Lee, Changmin] Digital Imaging Major Chung Ang Univ, Dept Integrated Engn, Seoul, South Korea.
C3 Chung Ang University
RP Park, K (corresponding author), Digital Imaging Major Chung Ang Univ, Dept Integrated Engn, Seoul, South Korea.
EM kjpark@cau.ac.kr
OI Krompiec, Przemyslaw/0000-0002-9444-6681
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Baraff D., SIGGRAPH 97
   Bousseau A., 2005, ACM T GRAPHIC, V26, P104
   Collomosse J.P., 2005, IEEE TVCG, V11
   Hays J., 2004, PROC NPAR 01, P113
   Hegde S, 2013, COMPUT ANIMAT VIRT W, V24, P43, DOI 10.1002/cav.1435
   Hertzmann A, 2003, IEEE COMPUT GRAPH, V23, P70, DOI 10.1109/MCG.2003.1210867
   Hertzmann A., 2000, NPAR, P7
   Huang H., 2010, COMP GRAPH FORUM, V28
   Kang H, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P43
   Kyprianidis J.E., 2013, IEEE TVCG, V10, P5
   Lang M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185530
   Li H., 2015, COMPUTATIONAL VISUAL, V1, P183
   Liang D, 2013, COMPUT ANIMAT VIRT W, V24, P307, DOI 10.1002/cav.1520
   Lin L, 2013, IEEE T CIRC SYST VID, V23, P577, DOI 10.1109/TCSVT.2012.2210804
   Litwinowicz P., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P407, DOI 10.1145/258734.258893
   Meier B. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P477, DOI 10.1145/237170.237288
   Metaxas D.N., 1996, PHYS BASED DEFORMABL
   O'Donovan P., 2012, IEEE TVCG, V18
   Park K, 2008, COMPUT ANIMAT VIRT W, V19, P527, DOI 10.1002/cav.241
   Seo S, 2014, MULTIMED TOOLS APPL, V71, P279, DOI 10.1007/s11042-013-1441-9
   Yoon JC, 2012, IEEE T VIS COMPUT GR, V18, P58, DOI 10.1109/TVCG.2011.47
   Zang Y, 2015, IEEE T VIS COMPUT GR, V21, P1015, DOI 10.1109/TVCG.2015.2410296
   Zeng K, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640445
NR 24
TC 3
Z9 3
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 813
EP 823
DI 10.1007/s00371-016-1256-6
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600014
DA 2024-07-18
ER

PT J
AU Frosio, I
   Turrini, C
   Alzati, A
AF Frosio, Iuri
   Turrini, Cristina
   Alzati, Alberto
TI Camera re-calibration after zooming based on sets of conics
SO VISUAL COMPUTER
LA English
DT Article
DE Camera calibration; Conics; Degenerate conics; Ellipses; Zoom lens; Line
   detection
ID HORIZON
AB We describe a method to compute the internal parameters (focal and principal point) of a camera with known position and orientation, based on the observation of two or more conics on a known plane. The conics can even be degenerate (e.g., pairs of lines). The proposed method can be used to re-estimate the internal parameters of a fully calibrated camera after zooming to a new, unknown, focal length. It also allows estimating the internal parameters when a second, fully calibrated camera observes the same conics. The parameters estimated through the proposed method are coherent with the output of more traditional procedures that require a higher number of calibration images. A deep analysis of the geometrical configurations that influence the proposed method is also reported.
C1 [Frosio, Iuri] NVIDIA, Santa Clara, CA USA.
   [Turrini, Cristina; Alzati, Alberto] Univ Milan, Dept Math, Milan, Italy.
C3 Nvidia Corporation; University of Milan
RP Alzati, A (corresponding author), Univ Milan, Dept Math, Milan, Italy.
EM ifrosio@nvidia.com; cristina.turrini@unimi.it; alberto.alzati@unimi.it
RI frosio, iuri/G-8917-2012
OI frosio, iuri/0000-0002-7230-4287
CR Abad F, 2004, LECT NOTES COMPUT SC, V3211, P688
   Agapito L, 2001, INT J COMPUT VISION, V45, P107, DOI 10.1023/A:1012471930694
   Ahmed M.T., 2000, P BRIT MACH VIS C 20
   [Anonymous], 2004, Technical Report
   [Anonymous], 1999, INT WORKSH VIS ALG
   Borghese NA, 2006, PATTERN RECOGN, V39, P1522, DOI 10.1016/j.patcog.2006.01.011
   Bouguet J., CAMERA CALIBRATION T
   Buch N, 2011, IEEE T INTELL TRANSP, V12, P920, DOI 10.1109/TITS.2011.2119372
   Calore E, 2012, IEEE IMTC P, P205
   Calore E, 2014, IMAGE VISION COMPUT, V32, P606, DOI 10.1016/j.imavis.2014.06.008
   Faugeras O., 1993, Three-dimensional computer vision: a geometric viewpoint
   Fitzgibbon A.W., 1995, P BRIT MACH VIS C BI
   Fraser CS, 2006, PHOTOGRAMM ENG REM S, V72, P1017, DOI 10.14358/PERS.72.9.1017
   Frosio I, 2012, PATTERN RECOGN, V45, P4169, DOI 10.1016/j.patcog.2012.05.020
   Kahl F, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P761, DOI 10.1109/ICCV.1998.710803
   Li CL, 2010, VISUAL COMPUT, V26, P227, DOI 10.1007/s00371-009-0400-y
   Lu X, 2013, VISUAL COMPUT, V29, P1107, DOI 10.1007/s00371-012-0754-4
   Sun XB, 2004, PROC SPIE, V5302, P79, DOI 10.1117/12.526972
   Troccoli A., 2012, SPIE ELECT IMAGING M
   Valera M., 2005, P VIS IM SIGN PROC, V152
   Wan DR, 2008, COMPUT VIS IMAGE UND, V112, P184, DOI 10.1016/j.cviu.2008.02.005
   Yang CJ, 2000, INT C PATT RECOG, P555, DOI 10.1109/ICPR.2000.905398
   Ying XH, 2007, LECT NOTES COMPUT SC, V4843, P138
   Yu C., P SPIE VIS COMM IM P
   Zhang Z., 1999, P ICCV99
   Zhang Z.:, 2004, EMERGIN TOPICS COMPU, V2, P4
NR 26
TC 5
Z9 6
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2016
VL 32
IS 5
BP 663
EP 674
DI 10.1007/s00371-015-1089-8
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DK5UI
UT WOS:000374985800011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ding, ZY
   Ding, Z
   Chen, WF
   Chen, HD
   Tao, YB
   Li, X
   Chen, W
AF Ding, Zhiyu
   Ding, Ziang
   Chen, Weifeng
   Chen, Haidong
   Tao, Yubo
   Li, Xin
   Chen, Wei
TI Visual inspection of multivariate volume data based on multi-class noise
   sampling
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computational Visual Media Conference (CVM)
CY APR 16-17, 2015
CL Tsinghua Univ, Beijing, PEOPLES R CHINA
HO Tsinghua Univ
DE Multivariate volume; Volume visualization; Blue noise; Sampling
ID DIMENSION PROJECTION; VISUALIZATION; VARIATE
AB Visualizing multivariate volume data is useful when the user wants to inspect the correlational distributions of multiple variables in a spatial field. Existing solutions commonly rely on color blending or weaving techniques to show multiple variables on a sampling point, probably causing heavy visual confusion. This paper presents an alternative solution that employs a multi-class sampling technique to generate spatially separated sampling points for multiple variables and illustrates the sampling points of each variable individually. We combine this new sampling scheme with the conventional direct volume rendering mode, iso-surface mode, and the cutting plane mode to support interactive inspection of volumetric distributions of multiple variables. The effectiveness of our approach is demonstrated with the IEEE VIS Contest 2004 Hurricane dataset and a 3D nuclear fusion simulation dataset.
C1 [Ding, Zhiyu; Chen, Haidong; Tao, Yubo; Chen, Wei] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310003, Zhejiang, Peoples R China.
   [Ding, Ziang] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
   [Chen, Weifeng] Zhejiang Univ Finance & Econ, Sch Informat, Hangzhou, Zhejiang, Peoples R China.
   [Li, Xin] China Univ Petr, Beijing, Peoples R China.
C3 Zhejiang University; Purdue University System; Purdue University;
   Zhejiang University of Finance & Economics; China University of
   Petroleum
RP Chen, W (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310003, Zhejiang, Peoples R China.
EM achilles.ding.zju@gmail.com; ding29@purdue.edu; cwf818@gmail.com;
   chenhd925@gmail.com; taoyubo@cad.zju.edu.cn; isabelincoln@gmail.com;
   chenwei@cad.zju.edu.cn
RI Chen, Wei/GZK-7348-2022; Chen, Wei/AAR-9817-2020; Chen,
   Weidong/IQS-2046-2023
OI Chen, Weidong/0000-0001-7197-1312; /0009-0006-1337-808X
FU National High Technology Research and Development Program of China
   [2012AA12090]; National Natural Science Foundation of China [61232012,
   61422211, 61303134]; Fundamental Research Funds for the Central
   Universities [2013QNA5010]
FX The authors would like to thank the anonymous reviewers for their
   valuable comments. This work was partially supported by the National
   High Technology Research and Development Program of China (2012AA12090),
   Major Program of National Natural Science Foundation of China
   (61232012), National Natural Science Foundation of China (61422211),
   National Natural Science Foundation of China (61303134), the Fundamental
   Research Funds for the Central Universities (2013QNA5010).
CR Akiba H., 2007, Proceedings of the Eurographics/IEEE-VGTC Symposium on Visualization, P115
   Akiba H, 2007, COMPUT SCI ENG, V9, P76, DOI 10.1109/MCSE.2007.42
   Balzer M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531392
   Barakat S, 2011, COMPUT GRAPH FORUM, V30, P961, DOI 10.1111/j.1467-8659.2011.01945.x
   Biswas A, 2013, IEEE T VIS COMPUT GR, V19, P2683, DOI 10.1109/TVCG.2013.133
   Cai WL, 1999, COMPUT GRAPH FORUM, V18, pC359, DOI 10.1111/1467-8659.00356
   Chuang J, 2009, IEEE T VIS COMPUT GR, V15, P1275, DOI 10.1109/TVCG.2009.150
   Cohen MF, 2003, ACM T GRAPHIC, V22, P287, DOI 10.1145/882262.882265
   COOK RL, 1986, ACM T GRAPHIC, V5, P51, DOI 10.1145/7529.8927
   Crawfis R., 1996, MULTIVARIATE VOLUME
   Djurcilov S, 2002, COMPUT GRAPH-UK, V26, P239, DOI 10.1016/S0097-8493(02)00055-9
   Fang ME, 2014, SCI CHINA INFORM SCI, V57, DOI 10.1007/s11432-013-4941-3
   Fuchs R, 2009, COMPUT GRAPH FORUM, V28, P1670, DOI 10.1111/j.1467-8659.2009.01429.x
   Guo HQ, 2011, IEEE PAC VIS SYMP, P19, DOI 10.1109/PACIFICVIS.2011.5742368
   Guo HQ, 2012, IEEE T VIS COMPUT GR, V18, P1397, DOI 10.1109/TVCG.2012.80
   Hagh-Shenas H, 2007, IEEE T VIS COMPUT GR, V13, P1270, DOI 10.1109/TVCG.2007.70623
   Jänicke H, 2008, IEEE T VIS COMPUT GR, V14, P1459, DOI 10.1109/TVCG.2008.116
   Johnson C, 2004, IEEE COMPUT GRAPH, V24, P13, DOI 10.1109/MCG.2004.20
   Kehrer J, 2013, IEEE T VIS COMPUT GR, V19, P495, DOI 10.1109/TVCG.2012.110
   Khlebnikov R, 2012, COMPUT GRAPH FORUM, V31, P1355, DOI 10.1111/j.1467-8659.2012.03127.x
   Khlebnikov R, 2013, IEEE T VIS COMPUT GR, V19, P2926, DOI 10.1109/TVCG.2013.180
   Kühne L, 2012, IEEE T VIS COMPUT GR, V18, P2122, DOI 10.1109/TVCG.2012.186
   Lagae A, 2005, ACM T GRAPHIC, V24, P1442, DOI 10.1145/1095878.1095888
   MAX N, 1995, IEEE T VIS COMPUT GR, V1, P99, DOI 10.1109/2945.468400
   Sauber N, 2006, IEEE T VIS COMPUT GR, V12, P917, DOI 10.1109/TVCG.2006.165
   Strengert M, 2006, VISUAL COMPUT, V22, P550, DOI 10.1007/s00371-006-0028-0
   Theisel H, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P631
   Tufte E.R., 1991, OPTOMETRY VISION SCI, V68, P322
   Tzeng FY, 2005, IEEE T VIS COMPUT GR, V11, P273, DOI 10.1109/TVCG.2005.38
   Urness T, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P115, DOI 10.1109/VISUAL.2003.1250362
   Wei L.-Y., 2010, ACM T GRAPH, V29
   Woodring J, 2006, IEEE T VIS COMPUT GR, V12, P909, DOI 10.1109/TVCG.2006.164
   Wu YC, 2013, COMPUT GRAPH FORUM, V32, P153, DOI 10.1111/cgf.12161
   YELLOTT JI, 1983, SCIENCE, V221, P382, DOI 10.1126/science.6867716
   Yuan XR, 2009, IEEE T VIS COMPUT GR, V15, P1001, DOI 10.1109/TVCG.2009.179
NR 35
TC 7
Z9 7
U1 1
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2016
VL 32
IS 4
BP 465
EP 478
DI 10.1007/s00371-015-1070-6
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DI6YD
UT WOS:000373645200005
DA 2024-07-18
ER

PT J
AU Takahashi, T
   Dobashi, Y
   Fujishiro, I
   Nishita, T
AF Takahashi, Tetsuya
   Dobashi, Yoshinori
   Fujishiro, Issei
   Nishita, Tomoyuki
TI Volume preserving viscoelastic fluids with large deformations using
   position-based velocity corrections
SO VISUAL COMPUTER
LA English
DT Article
DE Fluid simulation; Viscoelasticity; Deformation; Volume preservation;
   Velocity correction
ID SPH; SIMULATION
AB We propose a particle-based hybrid method for simulating volume preserving viscoelastic fluids with large deformations. Our method combines smoothed particle hydrodynamics (SPH) and position-based dynamics (PBD) to approximate the dynamics of viscoelastic fluids. While preserving their volumes using SPH, we exploit an idea of PBD and correct particle velocities for viscoelastic effects not to negatively affect volume preservation of materials. To correct particle velocities and simulate viscoelastic fluids, we use connections between particles which are adaptively generated and deleted based on the positional relations of the particles. Additionally, we weaken the effect of velocity corrections to address plastic deformations of materials. For one-way and two-way fluid-solid coupling, we incorporate solid boundary particles into our algorithm. Several examples demonstrate that our hybrid method can sufficiently preserve fluid volumes and robustly and plausibly generate a variety of viscoelastic behaviors, such as splitting and merging, large deformations, and Barus effect.
C1 [Takahashi, Tetsuya] Keio Univ, UEI Res, Tokyo, Japan.
   [Dobashi, Yoshinori] Hokkaido Univ, UEI Res, Sapporo, Hokkaido, Japan.
   [Fujishiro, Issei] Keio Univ, Tokyo, Japan.
   [Nishita, Tomoyuki] Hiroshima Shudo Univ, UEI Res, Tokyo, Japan.
C3 Keio University; Hokkaido University; Keio University
RP Takahashi, T (corresponding author), Keio Univ, UEI Res, Tokyo, Japan.
EM tetsuya.takahashi@uei.co.jp; doba@ime.ist.hokudai.ac.jp;
   fuji@ics.keio.ac.jp; tomoyuki.nishita@uei.co.jp
FU JST CREST; Grants-in-Aid for Scientific Research [26240015] Funding
   Source: KAKEN
FX This work has been partly supported by JST CREST. We would like to thank
   anonymous reviewers for their valuable suggestions and comments.
CR Akinci N, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185558
   Becker M, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P209
   Benda J., 2013, Encyclopedia of Computational Neuroscience, P1, DOI [DOI 10.1007/978-1-4614-7320-6_339-1, 10.1007/978-1-4614-7320-6_339-1]
   Bodin K, 2012, IEEE T VIS COMPUT GR, V18, P516, DOI 10.1109/TVCG.2011.29
   Chang Y., 2009, Proceedings of the 16th ACM Symposium on Virtual Reality Software and Technology, P111, DOI DOI 10.1145/1643928.1643954
   Clavet S., 2005, SCA '05, P219, DOI DOI 10.1145/1073368.1073400
   Cummins SJ, 1999, J COMPUT PHYS, V152, P584, DOI 10.1006/jcph.1999.6246
   Gerszewski D., 2009, SCA 09, P133, DOI DOI 10.1145/1599470.1599488
   He XW, 2012, COMPUT GRAPH FORUM, V31, P1948, DOI 10.1111/j.1467-8659.2012.03074.x
   Ihmsen M, 2014, IEEE T VIS COMPUT GR, V20, P426, DOI 10.1109/TVCG.2013.105
   Ihmsen M, 2011, COMPUT GRAPH FORUM, V30, P99, DOI [10.1111/j.1467-8659.2010.01832.x, 10.1111/j.1467-8659.2010.01834.x]
   Ihmsen Markus, 2014, P 35 ANN C EUR ASS C, DOI [10.2312/egst.20141034, DOI 10.2312/EGST.20141034]
   Jones B, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2560795
   Macklin M, 2014, ACM T GRAPHIC, V33, DOI [10.1145/280/109/2601152, 10.1145/2601097.2601152]
   Macklin M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461984
   Mao Hai., 2006, PARTICLE BASED NONNE
   Markus Becker, 2009, NPH, V9, P27, DOI [10.2312/EG/DL/conf/EG2009/nph/027-034, DOI 10.2312/EG/DL/CONF/EG2009/NPH/027-034]
   MILLER G, 1989, COMPUT GRAPH-UK, V13, P305, DOI 10.1016/0097-8493(89)90078-2
   Müller M, 2007, J VIS COMMUN IMAGE R, V18, P109, DOI 10.1016/j.jvcir.2007.01.005
   Müller M, 2005, ACM T GRAPHIC, V24, P471, DOI 10.1145/1073204.1073216
   Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154
   Muller M., 2004, P 2004 ACM SIGGRAPHE, P141, DOI [DOI 10.1145/1028523.1028542, 10.1145/1028523.1028542, 10]
   Solenthaler B, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531346
   Solenthaler B, 2007, COMPUT ANIMAT VIRT W, V18, P69, DOI 10.1002/cav.162
   Steele K, 2004, COMPUT ANIMAT VIRT W, V15, P183, DOI 10.1002/cav.20
   Takahashi T, 2014, P COMP GRAPH INT
   Takahashi T, 2014, COMPUT GRAPH-UK, V43, P21, DOI 10.1016/j.cag.2014.06.002
   Takamatsu Kenji, 2011, International Journal of Virtual Reality, V10, P29
   Tamura N, 2007, IEEE COMPUT GRAPH, V27, P87, DOI 10.1109/MCG.2007.157
   Terzopoulos D., 1991, Journal of Visualization and Computer Animation, V2, P68, DOI 10.1002/vis.4340020208
   Zhou YH, 2013, COMPUT GRAPH FORUM, V32, P215, DOI 10.1111/cgf.12229
NR 31
TC 5
Z9 5
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2016
VL 32
IS 1
BP 57
EP 66
DI 10.1007/s00371-014-1055-x
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DF9FQ
UT WOS:000371666800006
DA 2024-07-18
ER

PT J
AU Choi, JI
   Kang, SJ
   Kim, CH
   Lee, J
AF Choi, Jong-In
   Kang, Shin-Jin
   Kim, Chang-Hun
   Lee, Jung
TI Virtual ball player Synthesizing character animation to control a
   virtual ball from motion data using interaction patterns
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Virtual reality; Interaction motion; Motion capture; Motion analysis;
   Physics-based animation
AB It is very difficult and tedious work to synthesize an animation in which a character skillfully controls several balls. This is because all the virtual balls need to be synchronized with the motion of the character temporally and spatially as following the laws of physics. Moreover, a skillful actor is needed for capturing the motion. We introduce a simple but interesting method such that anyone can synthesize an animation of skillful ball-handling motion using interaction patterns without any actual ball. Interaction patterns involve regularly repeated human motions to control the virtual ball. We first capture the motion that mimics controlling a ball using various interaction patterns. Then we synthesize the trajectory of a virtual ball by analyzing the captured motion and correct the character motion to be fitted to the synthesized trajectory of a virtual ball. Experiments convincingly show the usefulness of proposed technique by synthesizing various ball-handling animations.
C1 [Choi, Jong-In; Kim, Chang-Hun; Lee, Jung] Korea Univ, Dept Comp Sci, Seoul, South Korea.
   [Kang, Shin-Jin] Hongik Univ, Sch Games, Seoul, South Korea.
C3 Korea University; Hongik University
RP Lee, J (corresponding author), Korea Univ, Dept Comp Sci, Seoul, South Korea.
EM gmcg2001@korea.ac.kr; directx@hongik.ac.kr; chkim@korea.ac.kr;
   airjung@korea.ac.kr
FU Korea University; National Research Foundation of Korea (NRF) - Ministry
   of Education, Science and Technology [NRF-2012R1A1A1012895,
   NRF-2013R1A1A2011602]; Convergence Technology Development [S2172401]
FX This research was supported by a Korea University Grant, Basic Science
   Research Program through the National Research Foundation of Korea (NRF)
   funded by the Ministry of Education, Science and Technology
   (NRF-2012R1A1A1012895, NRF-2013R1A1A2011602), and Convergence Technology
   Development (S2172401).
CR Al-Asqhar Rami Ali, 2013, P 12 ACM SIGGRAPHEUR, P45
   Alexa M, 2003, VISUAL COMPUT, V19, P105, DOI 10.1007/s00371-002-0180-0
   Arika O., 2005, P ACM SIGGRAPH EUR S, P56
   Cooper S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239456
   Ho ESL, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778770
   Jain Sumit., 2009, Proceedings of SCA 2009, P47
   Kim M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531385
   Kim TH, 2003, ACM T GRAPHIC, V22, P392, DOI 10.1145/882262.882283
   Komura T, 2005, COMPUT ANIMAT VIRT W, V16, P213, DOI 10.1002/cav.101
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Lee KH, 2006, ACM T GRAPHIC, V25, P898, DOI 10.1145/1141911.1141972
   Macchietto A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531386
   Nguyen Nam., 2010, Proceedings of the SCA 2010, P189
   Popovic J, 2003, ACM T GRAPHIC, V22, P1034, DOI 10.1145/944020.944025
   Popovic J, 2000, COMP GRAPH, P209, DOI 10.1145/344779.344880
   Shiratori T, 2006, COMPUT GRAPH FORUM, V25, P449, DOI 10.1111/j.1467-8659.2006.00964.x
   Shum HPH, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409067
   Wampler K, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1805964.1805970
   Yamane K, 2004, ACM T GRAPHIC, V23, P532, DOI 10.1145/1015706.1015756
   Ye YT, 2010, COMPUT GRAPH FORUM, V29, P555, DOI 10.1111/j.1467-8659.2009.01625.x
   Zordan VB, 2005, ACM T GRAPHIC, V24, P697, DOI 10.1145/1073204.1073249
   Zordan V, 2007, VRST 2007: ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, PROCEEDINGS, P81
NR 22
TC 4
Z9 4
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 905
EP 914
DI 10.1007/s00371-015-1116-9
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500016
DA 2024-07-18
ER

PT J
AU Jain, P
   Tyagi, V
AF Jain, Paras
   Tyagi, Vipin
TI An adaptive edge-preserving image denoising technique using tetrolet
   transforms
SO VISUAL COMPUTER
LA English
DT Article
DE Wavelet; Tetrolet transform; Local adaptive denoising; Edge preservation
ID WAVELET SHRINKAGE; BIVARIATE SHRINKAGE; FILTER
AB Denoising has always been one of the standard problems in image processing. It is always desirable to preserve important features, such as edges, corners and other sharp structures, during denoising. Wavelet transforms have been widely used in edge-preserving image denoising since these provide a suitable basis for suppressing noisy signals from the image. This paper presents a novel edge-preserving image denoising technique based on tetrolet transform (a Haar-type wavelet transform) and a locally adaptive thresholding method. The noisy image is decomposed into tetrolet (wavelets) coefficients through a tetrolet transform. A locally adaptive thresholding method exploiting interscale statistical dependency and based on computation of noise level is used to threshold the tetrolet coefficients to effectively reduce noise while preserving relevant features of the original image. Experimental results, compared to other approaches, demonstrate that the proposed method is suitable especially for the natural images contaminated by Gaussian noise.
C1 [Jain, Paras; Tyagi, Vipin] Jaypee Univ Engn & Technol, Guna 473226, MP, India.
RP Tyagi, V (corresponding author), Jaypee Univ Engn & Technol, Guna 473226, MP, India.
EM dr.vipin.tyagi@gmail.com
RI Jain, Dr. Paras/Y-5329-2018; Tyagi, Vipin/I-2451-2013
OI Jain, Dr. Paras/0000-0003-2990-3556; Tyagi, Vipin/0000-0003-4994-3686
CR [Anonymous], THESIS SAN JOSE U US
   [Anonymous], IMAGE DENOISING USIN
   [Anonymous], INT J INTELL TECHNOL
   Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942
   Ben Hamza A, 1999, J MATH IMAGING VIS, V11, P161, DOI 10.1023/A:1008395514426
   Blu T, 2007, IEEE T IMAGE PROCESS, V16, P2778, DOI 10.1109/TIP.2007.906002
   Breukelaar R, 2004, INT J COMPUT GEOM AP, V14, P41, DOI 10.1142/S0218195904001354
   Buades A, 2005, MULTISCALE MODEL SIM, V4, P490, DOI 10.1137/040616024
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chang SG, 2000, IEEE T IMAGE PROCESS, V9, P1522, DOI 10.1109/83.862630
   Chang SG, 2000, IEEE T IMAGE PROCESS, V9, P1532, DOI 10.1109/83.862633
   Chipman HA, 1997, J AM STAT ASSOC, V92, P1413, DOI 10.2307/2965411
   da Silva RD, 2013, PATTERN ANAL APPL, V16, P567, DOI 10.1007/s10044-012-0266-x
   Dabov K, 2006, SPIE ELECT IMAGING A, V6064
   DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009
   DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425
   Donoho DL, 1995, J AM STAT ASSOC, V90, P1200, DOI 10.1080/01621459.1995.10476626
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Egiazarian K, 1999, J ELECTRON IMAGING, V8, P233, DOI 10.1117/1.482673
   Elad M, 2002, IEEE T IMAGE PROCESS, V11, P1141, DOI 10.1109/TIP.2002.801126
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Gao HY, 1997, STAT SINICA, V7, P855
   Gao HY, 1998, J COMPUT GRAPH STAT, V7, P469
   Gonzalez R C, 2008, DIGITAL IMAGE PROCES
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   Jain P., 2013, IETE J. Educ, V54, P108, DOI DOI 10.1080/09747338.2013.10876113
   Krommweh J, 2010, J VIS COMMUN IMAGE R, V21, P364, DOI 10.1016/j.jvcir.2010.02.011
   Li CL, 2010, PROC SPIE, V7820, DOI 10.1117/12.866702
   Li Dai, 2013, Information Technology Journal, V12, P1995, DOI 10.3923/itj.2013.1995.2001
   Maggioni M, 2013, IEEE T IMAGE PROCESS, V22, P119, DOI 10.1109/TIP.2012.2210725
   Nason GP, 1996, J ROY STAT SOC B MET, V58, P463
   Orchard J, 2008, IEEE IMAGE PROC, P1732, DOI 10.1109/ICIP.2008.4712109
   Paris S., 2006, Proc. European Conference on Computer Vision, P568
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Porikli F., 2008, PROCEEDING IEEE C CO, P1
   Pratt W., 2007, DIGITAL IMAGE PROCES, V4th
   Qiu TS, 2013, IEEE T IMAGE PROCESS, V22, P80, DOI 10.1109/TIP.2012.2214052
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Sendur L, 2002, IEEE SIGNAL PROC LET, V9, P438, DOI 10.1109/LSP.2002.806054
   Sendur L, 2002, IEEE T SIGNAL PROCES, V50, P2744, DOI 10.1109/TSP.2002.804091
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Vidakovic B, 1998, J AM STAT ASSOC, V93, P173, DOI 10.2307/2669614
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weyrich N, 1998, IEEE T IMAGE PROCESS, V7, P82, DOI 10.1109/83.650852
   Yang QX, 2009, PROC CVPR IEEE, P557, DOI 10.1109/CVPRW.2009.5206542
   YANG RK, 1995, IEEE T SIGNAL PROCES, V43, P591, DOI 10.1109/78.370615
   Yuan XH, 2004, INT C PATT RECOG, P885
NR 47
TC 24
Z9 27
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2015
VL 31
IS 5
BP 657
EP 674
DI 10.1007/s00371-014-0993-7
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CH2CF
UT WOS:000353831400012
DA 2024-07-18
ER

PT J
AU Kim, S
   Guy, SJ
   Hillesland, K
   Zafar, B
   Gutub, AAA
   Manocha, D
AF Kim, Sujeong
   Guy, Stephen J.
   Hillesland, Karl
   Zafar, Basim
   Gutub, Adnan Abdul-Aziz
   Manocha, Dinesh
TI Velocity-based modeling of physical interactions in dense crowds
SO VISUAL COMPUTER
LA English
DT Article
DE Multi-agent simulation; Physical interactions
AB We present an interactive algorithm to model physics-based interactions in dense crowds. Our approach is capable of modeling both physical forces and interactions between agents and obstacles, while also allowing the agents to anticipate and avoid upcoming collisions during local navigation. We combine velocity-based collision-avoidance algorithms with external physical forces. The overall formulation produces various effects of forces acting on agents and crowds, including balance recovery motion and force propagation through the crowd. We further extend our method to model more complex behaviors involving social and cultural rules. We use finite-state machines to specify a series of behaviors and demonstrate our approach on many complex scenarios. Our algorithm can simulate a few thousand agents at interactive rates and can generate many emergent behaviors.
C1 [Kim, Sujeong; Manocha, Dinesh] Univ N Carolina, Comp Sci, Chapel Hill, NC 27599 USA.
   [Guy, Stephen J.] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN USA.
   [Hillesland, Karl] Adv Micro Devices Inc, Sunnyvale, CA USA.
   [Zafar, Basim] Umm Al Qura Univ, Hajj Res Inst, Mecca, Saudi Arabia.
   [Gutub, Adnan Abdul-Aziz] Umm Al Qura Univ, Custodian Two Holy Mosques Inst Hajj & Omrah Res, Mecca, Saudi Arabia.
C3 University of North Carolina; University of North Carolina Chapel Hill;
   University of Minnesota System; University of Minnesota Twin Cities;
   Advanced Micro Devices; Umm Al Qura University; Umm Al Qura University
RP Kim, S (corresponding author), Univ N Carolina, Comp Sci, Chapel Hill, NC 27599 USA.
EM sujeong@cs.unc.edu; sjguy@cs.umn.edu; karl.hillesland@amd.com;
   Bjzafar@uqu.edu.sa; aagutub@uqu.edu.sa; dm@cs.unc.edu
RI Gutub, Adnan Abdul-Aziz/O-1240-2016; Kim, Sujeong/KBQ-2681-2024
OI Gutub, Adnan Abdul-Aziz/0000-0003-0923-202X; Guy,
   Stephen/0000-0001-8986-5817
FU NSF [1000579, 1117127, 1305286]; Intel; AMD; Boeing Company; Center of
   Research Excellence in Hajj and Omrah (HajjCoRE); Div Of Civil,
   Mechanical, & Manufact Inn; Directorate For Engineering [1000579]
   Funding Source: National Science Foundation; Div Of Information &
   Intelligent Systems; Direct For Computer & Info Scie & Enginr [1117127]
   Funding Source: National Science Foundation
FX This work was supported by NSF awards 1000579, 1117127, 1305286, Intel,
   AMD, and a grant from the Boeing Company. We also thank the Center of
   Research Excellence in Hajj and Omrah (HajjCoRE) for its support through
   the collaboration project titled "Simulate the movement of individual in
   large-scale crowds during Tawaf".
CR [Anonymous], 1997, COMPUTER ANIMATION S, DOI DOI 10.1007/978-3-7091-6874-5_3
   Baraff David., 1997, An Introduction to Physically Based Modelling, SIGGRAPH '97 Course Notes, P97
   Buckland M., 2005, PROGRAMMING GAME AI
   Curtis S., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P128, DOI 10.1109/ICCVW.2011.6130234
   Curtis S., 2012, P PED EV DYN
   Curtis S., 2012, Proceedings of the ACM SIGGRAPH symposium on interactive 3D graphics and games. ACM, P15
   Durupinar F, 2011, IEEE COMPUT GRAPH, V31, P22, DOI 10.1109/MCG.2009.105
   Guy S.J., 2011, P 2011 ACM SIGGRAPH, P43, DOI [10.1145/2019406.2019413, DOI 10.1145/2019406.2019413]
   Guy SJ, 2012, PHYS REV E, V85, DOI 10.1103/PhysRevE.85.016110
   Helbing D, 2000, NATURE, V407, P487, DOI 10.1038/35035023
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   Hughes RL, 2003, ANNU REV FLUID MECH, V35, P169, DOI 10.1146/annurev.fluid.35.101101.161136
   Karamouzas I, 2012, IEEE T VIS COMPUT GR, V18, P394, DOI 10.1109/TVCG.2011.133
   Kim M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531385
   Kim S., 2013, EUR ACM SIGGRAPH S C
   Kim SH, 2012, KOREAN J ORTHOD, V42, P55, DOI 10.4041/kjod.2012.42.2.55
   Köster G, 2013, PHYS REV E, V87, DOI 10.1103/PhysRevE.87.063305
   Koshak N., 2008, 9 INT C DESIGN DECIS
   Lee KH, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P109
   Lemercier S, 2012, COMPUT GRAPH FORUM, V31, P489, DOI 10.1111/j.1467-8659.2012.03028.x
   Lerner A, 2009, LECT NOTES COMPUT SC, V5884, P75, DOI 10.1007/978-3-642-10347-6_7
   Maki BE, 2003, IEEE ENG MED BIOL, V22, P20, DOI 10.1109/MEMB.2003.1195691
   Muico U, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966395
   Narain R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618468
   Ondrej J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778860
   Pelechano N, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P99
   Pettre J., 2009, Proceedings of the 2009 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '09, P189, DOI DOI 10.1145/1599470.1599495
   Reynolds C. W., 1999, GAM DEV C 1999
   Sakuma T, 2005, COMPUT ANIMAT VIRT W, V16, P343, DOI 10.1002/cav.105
   Seyfried A, 2005, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2005/10/P10002
   Shao W., 2005, SCA 05 P 2005 ACM SI, P19, DOI DOI 10.1145/1073368.1073371
   Shapiro A, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P455, DOI 10.1109/PCCGA.2003.1238294
   Shiratori T., 2009, P ACM SIGGRAPH EUR S
   Shum HPH, 2012, IEEE T VIS COMPUT GR, V18, P741, DOI 10.1109/TVCG.2010.257
   Sok KwangWon., 2010, P 2010 ACM SIGGRAPHE, P11
   Still G.K., 2013, PROXIMATE DISTAL CAU
   Thalmann D., 2013, Crowd Simulation, P111, DOI DOI 10.1007/978-1-4471-4450-2_5
   Treuille A, 2006, ACM T GRAPHIC, V25, P1160, DOI 10.1145/1141911.1142008
   Ulicny B, 2002, COMPUT GRAPH FORUM, V21, P767, DOI 10.1111/1467-8659.00634
   van den Berg J, 2011, SPRINGER TRAC ADV RO, V70, P3
   YEH H., 2008, SCA 08 P 2005 ACM SI, P39
   Yu QX, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P119
   Yu W, 2009, 2009 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION, VOLS 1-7, CONFERENCE PROCEEDINGS, P1928, DOI 10.1109/ICMA.2009.5246563
   Yu W, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.046105
   Zordan VB, 2005, ACM T GRAPHIC, V24, P697, DOI 10.1145/1073204.1073249
NR 45
TC 51
Z9 54
U1 1
U2 29
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2015
VL 31
IS 5
BP 541
EP 555
DI 10.1007/s00371-014-0946-1
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CH2CF
UT WOS:000353831400004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yu, Y
   Zhou, Y
   Du, SD
   Jie, Y
   Wang, ZQ
   Cai, ZY
AF Yu, Yao
   Zhou, Yu
   Du, Sidan
   Jie, Yuan
   Wang, Ziqiang
   Cai, Zhengyu
TI Learning human shape model from multiple databases with correspondence
   considering kinematic consensus
SO VISUAL COMPUTER
LA English
DT Article
DE Deformations; Morphing; Non-rigid registration; Motion capture and
   synthesis; Correspondence
ID NONRIGID REGISTRATION; DEFORMATION; SPACE; PARAMETERIZATION;
   RECONSTRUCTION; SKELETON
AB In various applications of computer graphics and model-based computer vision, a human shape model cannot only model the kinematic properties of a subject to drive the mesh into various postures, but it can also be utilized to parameterize the shape variations across individuals. It is of great benefit to improve the diversity of the training databases by learning the model from multiple databases, once the correspondences among scans of these databases can be achieved. To accomplish this goal, we proposed a framework to match the scans from multiple databases, using the assistance of kinematic properties, to compute the correspondences. The resulting correspondence is accurate, robust, capable of handling scan incompletion, and is homogeneous across shapes and postures. In our approach, we start with evaluating how a correspondence, which is achieved via minimizing the deformation energy, agrees with the kinematic properties, and then, we jointly fit the source scans to the target scans to derive the correspondences between the databases. The extensive results show that our approach can generate a faithful correspondence even in extreme cases, without carefully selecting the deformation factors and markers. We also developed a method, with which a commendable and predictable result can be synthesized, to control the rendered shape in an intuitive way.
C1 [Yu, Yao; Zhou, Yu; Du, Sidan; Jie, Yuan; Wang, Ziqiang; Cai, Zhengyu] Nanjing Univ, Sch Elect Sci & Engn, Nanjing 210046, Jiangsu, Peoples R China.
C3 Nanjing University
RP Yu, Y (corresponding author), Nanjing Univ, Sch Elect Sci & Engn, Nanjing 210046, Jiangsu, Peoples R China.
EM allanyu@nju.edu.cn; nackzhou@nju.edu.cn; coff128@nju.edu.cn
RI Zhou, Yu/D-1999-2011; Du, Sidan/JVN-2413-2024; Yu, YAO/KHZ-0065-2024;
   Wang, Ziqiang/CAG-0209-2022
OI Du, Sidan/0000-0002-7079-0066; 
FU Natural Science Foundation of Jiangsu Province [BE2011169, BK2011563];
   Natural Science Foundation of China [61100111, 61300157, 61201425,
   61271231]
FX This work was partially supported by Grant No. BE2011169, BK2011563 from
   the Natural Science Foundation of Jiangsu Province and Grant No.
   61100111, 61300157, 61201425, 61271231 from the Natural Science
   Foundation of China.
CR Aiger D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360684
   Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   Anguelov D., 2005, ADV NEURAL INFORM PR, V17, P33
   [Anonymous], P 2002 ACM SIGGRAPH
   [Anonymous], SHAPE MODELING APPL
   [Anonymous], EUR WORKSH 3D OBJ RE
   [Anonymous], COMPUTER GRAPHICS FO
   Ben Azouz Z., 2006, Automatic locating of anthropometric landmarks on 3d human models
   Chang W, 2008, COMPUT GRAPH FORUM, V27, P1459, DOI 10.1111/j.1467-8659.2008.01286.x
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gelfand N., 2005, P 3 EUR S GEOM PROC, V2, P5
   Huang QX, 2008, COMPUT GRAPH FORUM, V27, P1449, DOI 10.1111/j.1467-8659.2008.01285.x
   Irani S, 1999, COMP GEOM-THEOR APPL, V12, P17, DOI 10.1016/S0925-7721(98)00033-9
   James DL, 2005, ACM T GRAPHIC, V24, P399, DOI 10.1145/1073204.1073206
   Jolliffe I.T., 1986, Principal component analysis, DOI DOI 10.1016/0169-7439(87)80084-9
   Kalogerakis E, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778839
   Kraevoy V, 2004, ACM T GRAPHIC, V23, P861, DOI 10.1145/1015706.1015811
   Lewis JP, 2000, COMP GRAPH, P165, DOI 10.1145/344779.344862
   Li H, 2008, COMPUT GRAPH FORUM, V27, P1421, DOI 10.1111/j.1467-8659.2008.01282.x
   Pekelny Y, 2008, COMPUT GRAPH FORUM, V27, P399, DOI 10.1111/j.1467-8659.2008.01137.x
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Straka M, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P41, DOI 10.1109/3DIMPVT.2012.18
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Xu K, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866206
   Ye M, 2011, IEEE I CONF COMP VIS, P731, DOI 10.1109/ICCV.2011.6126310
   Zhang H, 2008, COMPUT GRAPH FORUM, V27, P1431, DOI 10.1111/j.1467-8659.2008.01283.x
   Zheng Q, 2010, COMPUT GRAPH FORUM, V29, P635, DOI 10.1111/j.1467-8659.2009.01633.x
   Zhou SZ, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778863
NR 29
TC 3
Z9 3
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2015
VL 31
IS 1
BP 19
EP 33
DI 10.1007/s00371-013-0901-6
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AY8ZQ
UT WOS:000347839600003
DA 2024-07-18
ER

PT J
AU Ding, K
   Chen, WH
   Wu, XM
AF Ding, Kai
   Chen, Weihai
   Wu, Xingming
TI Optimum inpainting for depth map based on <i>L</i><sub>0</sub> total
   variation
SO VISUAL COMPUTER
LA English
DT Article
DE Depth map; Inpainting; Total variation; Energy minimization
ID REMOVAL
AB Depth map generated by the Kinect may have some pixels lost due to echo attenuation of infra-red light and mutual interference between neighboring pixels, which can cause pervasive problems when utilizing Kinect cameras as depth sensors. In this work, we propose a 2-step inpainting algorithm to infill the holes. First, a naive Bayesian estimation is conducted as preliminary inpainting scheme, utilizing neighboring pixels of the missing ones, and corresponding pixels in the color image as prior knowledge. After that, an optimization is implemented to improve the depth map, where the false edges in mistakenly inpainted regions are detected, then iteratively propelled to their true positions under total variation framework. Experimental results are included to show effectiveness of the proposed algorithm.
C1 [Ding, Kai; Chen, Weihai; Wu, Xingming] Beihang Univ, Sch Automat Sci & Elect Engn, Beijing 100191, Peoples R China.
C3 Beihang University
RP Chen, WH (corresponding author), Beihang Univ, Sch Automat Sci & Elect Engn, Beijing 100191, Peoples R China.
EM derby.ding@gmail.com; whchenbuaa@126.com; wuxingming307@126.com
RI Chen, Wei/GZK-7348-2022
OI /0009-0006-1337-808X
FU National Nature Science Foundation of China [61075075, 61175108];
   Beijing Municipal of Science and Technical Commission [D121104002812001]
FX This work has been supported by National Nature Science Foundation of
   China under the research project 61075075, 61175108, and by Beijing
   Municipal of Science and Technical Commission under Major Program
   D121104002812001. Here, we also express our gratitude to Shaoping Bai
   for his great help in revising the paper.
CR [Anonymous], 2007, IEEE COMP SOC C COMP
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Camplani M., 2012, P 3DIP APPL JAN, V8290, P1
   Chan TF, 2003, SIAM J APPL MATH, V63, P564
   Criminisi A, 2003, PROC CVPR IEEE, P721
   Kou F., 2013, LECT NOTES IN PRESS
   Lai KV, 2012, IEEE INT CONF ROBOT, P1330, DOI 10.1109/ICRA.2012.6225316
   Liu JY, 2012, INT C PATT RECOG, P2055
   Qi F, 2013, PATTERN RECOGN LETT, V34, P70, DOI 10.1016/j.patrec.2012.06.003
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Schönlieb CB, 2011, COMMUN MATH SCI, V9, P413
   Telea A., 2004, Journal of Graphics Tools, V9, P23, DOI 10.1080/10867651.2004.10487596
   Wang YL, 2008, SIAM J IMAGING SCI, V1, P248, DOI 10.1137/080724265
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
NR 14
TC 9
Z9 9
U1 0
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2014
VL 30
IS 12
BP 1311
EP 1320
DI 10.1007/s00371-013-0888-z
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AS8DX
UT WOS:000344481400001
DA 2024-07-18
ER

PT J
AU Donatsch, D
   Bigdeli, S
   Robert, P
   Zwicker, M
AF Donatsch, Daniel
   Bigdeli, Siavash Arjomand
   Robert, Philippe
   Zwicker, Matthias
TI Hand-held 3D light field photography and applications
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE 3D light fields; Computational photography; Disparity estimation;
   Digital refocusing
AB We propose a method to acquire 3D light fields using a hand-held camera, and describe several computational photography applications facilitated by our approach. As our input we take an image sequence from a camera translating along an approximately linear path with limited camera rotations. Users can acquire such data easily in a few seconds by moving a hand-held camera. We include a novel approach to resample the input into regularly sampled 3D light fields by aligning them in the spatio-temporal domain, and a technique for high-quality disparity estimation from light fields. We show applications including digital refocusing and synthetic aperture blur, foreground removal, selective colorization, and others.
C1 [Donatsch, Daniel; Zwicker, Matthias] Univ Bern, Bern, Switzerland.
   [Bigdeli, Siavash Arjomand] Univ Neuchatel, CH-2000 Neuchatel, Switzerland.
   [Bigdeli, Siavash Arjomand; Robert, Philippe] 3D Impact Media, Sarnen, Switzerland.
C3 University of Bern; University of Neuchatel
RP Donatsch, D (corresponding author), Univ Bern, Bern, Switzerland.
EM donatsch@iam.unibe.ch
RI Arjomand Bigdeli, Siavash/HSG-6682-2023; Robert, Philippe S/J-7003-2017
OI Arjomand Bigdeli, Siavash/0000-0003-2569-6473; Zwicker,
   Matthias/0000-0001-8630-5515
FU Swiss Commission for Technology and Innovation CTI [15592.1 PFES-ES]
FX This project was partially supported by funding from the Swiss
   Commission for Technology and Innovation CTI through project no. 15592.1
   PFES-ES.
CR [Anonymous], ACM SIGGRAPH 2011 NE
   Bac S, 2007, COMPUT GRAPH FORUM, V26, P571, DOI 10.1111/j.1467-8659.2007.01080.x
   Davis A, 2012, COMPUT GRAPH FORUM, V31, P305, DOI 10.1111/j.1467-8659.2012.03009.x
   Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   Irani M, 2002, INT J COMPUT VISION, V48, P173, DOI 10.1023/A:1016372015744
   Isaksen A, 2000, COMP GRAPH, P297, DOI 10.1145/344779.344929
   Joshi N., 2007, IEEE 11 INT C COMPUT, P1
   Joshi N, 2006, ACM T GRAPHIC, V25, P779, DOI 10.1145/1141911.1141955
   Kim C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461926
   Lippmann G., 1908, J. Phys. Theor. Appl, V7, P821, DOI [DOI 10.1051/JPHYSTAP:019080070082100, 10.1051/jphystap:019080070082100]
   Liu F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899408
   Liu F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531350
   Ng R, 2005, ACM T GRAPHIC, V24, P735, DOI 10.1145/1073204.1073256
   Rhemann C, 2011, PROC CVPR IEEE, DOI 10.1109/CVPR.2011.5995372
   Tanskanen P, 2013, IEEE I CONF COMP VIS, P65, DOI 10.1109/ICCV.2013.15
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wang YS, 2013, IEEE T VIS COMPUT GR, V19, P1354, DOI 10.1109/TVCG.2013.11
   Wanner S, 2012, PROC CVPR IEEE, P41, DOI 10.1109/CVPR.2012.6247656
NR 19
TC 16
Z9 19
U1 0
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 897
EP 907
DI 10.1007/s00371-014-0979-5
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700031
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Huo, YQ
   Yang, F
   Dong, L
   Brost, V
AF Huo, Yongqing
   Yang, Fan
   Dong, Le
   Brost, Vincent
TI Physiological inverse tone mapping based on retina response
SO VISUAL COMPUTER
LA English
DT Article
DE High Dynamic Range; Human visual system; Inverse tone mapping; Image
   quality metric; Local adaptive luminance
ID ENHANCEMENT; VIDEO
AB The mismatch between the Low Dynamic Range (LDR) content and the High Dynamic Range (HDR) display arouses the research on inverse tone mapping algorithms. In this paper, we present a physiological inverse tone mapping algorithm inspired by the property of the Human Visual System (HVS). It first imitates the retina response and deduce it to be local adaptive; then estimates local adaptation luminance at each point in the image; finally, the LDR image and local luminance are applied to the inversed local retina response to reconstruct the dynamic range of the original scene. The good performance and high-visual quality were validated by operating on 40 test images. Comparison results with several existing inverse tone mapping methods prove the conciseness and efficiency of the proposed algorithm.
C1 [Huo, Yongqing; Yang, Fan; Brost, Vincent] Univ Burgundy, CNRS, LE2I, Lab 6306, F-21078 Dijon, France.
   [Huo, Yongqing] Univ Elect Sci & Technol China, Sch Commun & Informat Engn, Chengdu 611731, Peoples R China.
   [Dong, Le] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
C3 Universite de Bourgogne; Centre National de la Recherche Scientifique
   (CNRS); University of Electronic Science & Technology of China;
   University of Electronic Science & Technology of China
RP Huo, YQ (corresponding author), Univ Burgundy, CNRS, LE2I, Lab 6306, F-21078 Dijon, France.
EM hyq980132@163.com; fanyang@u-bourgogne.fr; vincent.brost@u-bourgogne.fr
FU Postdoctoral Funds of the Burgundy Region of France; Fundamental
   Research Funds for the Central Universities [ZYGX2011J004,
   ZYGX2011X014]; National Natural Science Foundation of China [61003123]
FX This work was supported in part by the Postdoctoral Funds of the
   Burgundy Region of France, in part by the Fundamental Research Funds for
   the Central Universities (No. ZYGX2011J004), in part by the National
   Natural Science Foundation of China under Grant 61003123, and in part by
   the Fundamental Research Funds for the Central Universities (No.
   ZYGX2011X014).
CR Akyüz AG, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276425
   [Anonymous], 2005, High Dynamic Range Imaging: Acquisition, Display, and Image-Based Lighting (The Morgan Kaufmann Series in Computer Graphics
   [Anonymous], 1987, The Retina: An Approachable Part of the Brain
   [Anonymous], EUR S REND
   [Anonymous], 2009, Mathematical physiology 1: Cellular physiology
   Aydin TO, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360668
   Banterle F., 2006, P 4 INT C COMP GRAPH, P349
   Banterle F., 2008, P SCCG, P349
   Banterle F, 2007, VISUAL COMPUT, V23, P467, DOI 10.1007/s00371-007-0124-9
   Banterle F, 2009, COMPUT GRAPH FORUM, V28, P2343, DOI 10.1111/j.1467-8659.2009.01541.x
   Boev A., 2012, MODELLING STEREOSCOP
   DAUGMAN JG, 1980, VISION RES, V20, P847, DOI 10.1016/0042-6989(80)90065-6
   Didyk P, 2008, COMPUT GRAPH FORUM, V27, P1265, DOI 10.1111/j.1467-8659.2008.01265.x
   Dong L, 2012, IEEE T IMAGE PROCESS, V21, P2534, DOI 10.1109/TIP.2012.2187528
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Guarnieri G, 2011, IEEE T IMAGE PROCESS, V20, P1351, DOI 10.1109/TIP.2010.2092436
   Horiuchi T, 2010, EURASIP J IMAGE VIDE, DOI 10.1155/2010/438958
   Kovaleski RP, 2009, VISUAL COMPUT, V25, P539, DOI 10.1007/s00371-009-0327-3
   Masia B., 2010, P CEIG
   Masia B, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618506
   Meylan L., 2006, COLOR IMAGING C, V1, P333, DOI [10.2352/CIC.2006.14.1.ART00061, DOI 10.2352/CIC.2006.14.1.ART00061]
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Rempel AG, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239490
   Shapley R., 1984, Prog Retin Res, V3, P263, DOI [10.1016/0278-4327(84)90011-7, DOI 10.1016/0278-4327(84)90011~7]
NR 24
TC 88
Z9 96
U1 1
U2 22
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2014
VL 30
IS 5
BP 507
EP 517
DI 10.1007/s00371-013-0875-4
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AF2AN
UT WOS:000334515100004
DA 2024-07-18
ER

PT J
AU Wu, HF
   Li, GB
   Luo, XN
AF Wu, Hefeng
   Li, Guanbin
   Luo, Xiaonan
TI Weighted attentional blocks for probabilistic object tracking
SO VISUAL COMPUTER
LA English
DT Article
DE Visual tracking; Attentional block; Saliency map; Particle filter;
   Branch-and-bound search
AB In this paper we represent the object with multiple attentional blocks which reflect some findings of selective visual attention in human perception. The attentional blocks are extracted using a branch-and-bound search method on the saliency map, and meanwhile the weight of each block is determined. Independent particle filter tracking is applied to each attentional block and the tracking results of all the blocks are then combined in a linear weighting scheme to get the location of the entire target object. The attentional blocks are propagated to the object location found in each new frame and the state of the most likely particle in each block is also updated with the new propagated position. In addition, to avoid error accumulation caused by the appearance variations, the object template and the positions of the attentional blocks are adaptively updated while tracking. Experimental results show that the proposed algorithm is able to efficiently track salient objects and is better accounted for partial occlusions and large variations in appearance.
C1 [Wu, Hefeng; Li, Guanbin; Luo, Xiaonan] Sun Yat Sen Univ, Sch Informat Sci & Technol, Natl Engn Res Ctr Digital Life, State Prov Joint Lab Digital Home Interact Applic, Guangzhou 510006, Guangdong, Peoples R China.
   [Wu, Hefeng; Li, Guanbin; Luo, Xiaonan] Sun Yat Sen Univ Shenzhen, Res Inst, Shenzhen Digital Home Key Technol Engn Lab, Shenzhen 518057, Peoples R China.
C3 Sun Yat Sen University; Sun Yat Sen University
RP Luo, XN (corresponding author), Sun Yat Sen Univ, Sch Informat Sci & Technol, Natl Engn Res Ctr Digital Life, State Prov Joint Lab Digital Home Interact Applic, Guangzhou 510006, Guangdong, Peoples R China.
EM wuhefeng@gmail.com; ligb@student.sysu.edu.cn; lnslxn@mail.sysu.edu.cn
FU National Natural Science Foundation of China [61232011, 61202294];
   National Key Technology RD Program [2011BAH27B01, 2011BHA16B08];
   NSFC-Guangdong Joint Fund [U1135005, U1201252];
   Industry-academy-research Project of Guangdong [2011A091000032]; Special
   Foundation of Industry Development for Biology, Internet, New Energy and
   New Material of Shenzhen [JC201104220324A]
FX This research is supported by the National Natural Science Foundation of
   China (61232011, 61202294), the National Key Technology R&D Program (No.
   2011BAH27B01, 2011BHA16B08), NSFC-Guangdong Joint Fund (No. U1135005,
   U1201252), the Industry-academy-research Project of Guangdong (No.
   2011A091000032), the Special Foundation of Industry Development for
   Biology, Internet, New Energy and New Material of Shenzhen (No.
   JC201104220324A).
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2006, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2006.256
   [Anonymous], 2009, P IEEE INT C COMP VI
   [Anonymous], 2001, Sequential Monte Carlo methods in practice
   [Anonymous], CAVIAR DATASET
   Avidan S, 2007, IEEE T PATTERN ANAL, V29, P261, DOI 10.1109/TPAMI.2007.35
   Babu RV, 2007, IMAGE VISION COMPUT, V25, P1205, DOI 10.1016/j.imavis.2006.07.016
   Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd
   Bousetouane F, 2013, VISUAL COMPUT, V29, P155, DOI 10.1007/s00371-012-0677-0
   Bulbul A, 2010, VISUAL COMPUT, V26, P311, DOI 10.1007/s00371-010-0419-0
   Cho TS, 2010, IEEE T PATTERN ANAL, V32, P1489, DOI 10.1109/TPAMI.2009.133
   Chockalingam P, 2009, IEEE I CONF COMP VIS, P1530, DOI 10.1109/ICCV.2009.5459276
   Collins RT, 2003, PROC CVPR IEEE, P234
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Fan JL, 2010, LECT NOTES COMPUT SC, V6311, P480
   Gall J, 2011, IEEE T PATTERN ANAL, V33, P2188, DOI 10.1109/TPAMI.2011.70
   Grabner H., 2006, BMVC, P47
   Guanbin Li, 2011, 2011 International Conference on Multimedia Technology, P3643
   Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650
   Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821
   Lampert CH, 2009, IEEE T PATTERN ANAL, V31, P2129, DOI 10.1109/TPAMI.2009.144
   Liu Tie, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383047
   Luo Y, 2011, IEEE T CIRC SYST VID, V21, P1822, DOI 10.1109/TCSVT.2011.2147230
   Nejhum SMS, 2010, COMPUT VIS IMAGE UND, V114, P901, DOI 10.1016/j.cviu.2010.04.002
   Nummiaro K, 2003, IMAGE VISION COMPUT, V21, P99, DOI 10.1016/S0262-8856(02)00129-4
   Palmer S., 1999, VISION SCI PHOTONS P
   Pérez P, 2002, LECT NOTES COMPUT SC, V2350, P661
   Santner J, 2010, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2010.5540145
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Singh VK, 2011, VISUAL COMPUT, V27, P1115, DOI 10.1007/s00371-011-0656-x
   Valenti R, 2009, IEEE I CONF COMP VIS, P2185, DOI 10.1109/ICCV.2009.5459240
   Wagner D, 2010, IEEE T VIS COMPUT GR, V16, P355, DOI 10.1109/TVCG.2009.99
   Wu Y, 2009, PROC CVPR IEEE, P33, DOI 10.1109/CVPRW.2009.5206719
   Zhang KH, 2013, PATTERN RECOGN, V46, P397, DOI 10.1016/j.patcog.2012.07.013
   Zhang XG, 2010, ROBOT AUTON SYST, V58, P1197, DOI 10.1016/j.robot.2010.08.002
   Zhu JD, 2010, IEEE T CIRC SYST VID, V20, P223, DOI 10.1109/TCSVT.2009.2031395
NR 36
TC 36
Z9 38
U1 0
U2 21
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2014
VL 30
IS 2
BP 229
EP 243
DI 10.1007/s00371-013-0823-3
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AA9DL
UT WOS:000331393900008
DA 2024-07-18
ER

PT J
AU Nugjgar, P
   Chiba, N
AF Nugjgar, Purevtsogt
   Chiba, Norishige
TI Markov-Type Vector Field for endless surface animation of water stream
SO VISUAL COMPUTER
LA English
DT Article
DE Natural phenomena; Stochastic modeling; Endless animation; Water stream;
   Surface animation
ID SIMULATION
AB In this paper, we propose a hybrid (physical-stochastic) model of surface element (surfel) fluctuations for the visual simulation of an endlessly running water surface. This model comprises two main phases: preprocessing and endless animation phases. First, we simulate a physics-based method for a specific period of time during the preprocessing phase. We construct a stochastic vector field in the simulation, referred to as a Markov-Type Vector Field (MTVF), using only the surface values of the fluid flow. Next, we import the MTVF data into the main endless animation phase and create a surface fluctuation animation by surfels and temporary velocity field modeling of the MTVF using a random sample. In our approach, the surfel edges that cover the fluid flow domain are transferred simply via a temporary single velocity and the new flow surface is determined directly based on their positions. MTVF allows us to generate a water surface animation endlessly in real time without the time-consuming processes of solving the corresponding physical equations. We describe the MTVF construction method and the endless surface animation steps, as well as present the results of experiments that demonstrate the plausibility of our method.
C1 [Nugjgar, Purevtsogt] Natl Univ Mongolia, Sch Math & Comp Sci, Ulaanbaatar, Mongolia.
   [Chiba, Norishige] Iwate Univ, Grad Sch Engn, Morioka, Iwate 020, Japan.
C3 National University of Mongolia; Iwate University
RP Nugjgar, P (corresponding author), Natl Univ Mongolia, Sch Math & Comp Sci, Ulaanbaatar, Mongolia.
EM purevtsogt@smcs.num.edu.mn; nchiba@cis.iwate-u.ac.jp
RI Nugjgar, Purevtsogt/IUQ-0835-2023
CR Bridson R, 2007, ACM T GRAPHIC, V26, P26, DOI DOI 10.1145/1239451.1239497
   Burrell T, 2009, COMPUT ANIMAT VIRT W, V20, P163, DOI 10.1002/cav.288
   Chentanez N, 2010, P ACM SIGGRAPH EUROG
   Enright D, 2002, ACM T GRAPHIC, V21, P736, DOI [10.1145/566570.566581, 10.1145/566570.566645]
   Foster N, 2001, COMP GRAPH, P23, DOI 10.1145/383259.383261
   Glassner A, 2002, IEEE COMPUT GRAPH, V22, P88, DOI 10.1109/MCG.2002.1016702
   Kipfer P, 2006, PROC GRAPH INTERF, P41
   Koshizuka S, 1996, NUCL SCI ENG, V123, P421, DOI 10.13182/NSE96-A24205
   Koshizuka S., 1995, Computational Fluid Dynamics Journal, V113, P134
   Maes MarceloM., 2006, P 4 INT C COMPUTER G, P107
   MONAGHAN JJ, 1992, ANNU REV ASTRON ASTR, V30, P543, DOI 10.1146/annurev.aa.30.090192.002551
   Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154
   Neyret F, 2001, SPRING EUROGRAP, P53
   Norris J. R., 1998, MARKOV CHAINS
   Nugjgar P, 2011, P CGI2011 C FLUID EF
   Nugjgar P, 2012, VISUAL COMPUT, V28, P219, DOI 10.1007/s00371-011-0637-0
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   Premoze S, 2003, COMPUT GRAPH FORUM, V22, P401, DOI 10.1111/1467-8659.00687
   REEVES WT, 1983, COMPUT GRAPH, V17, P359
   Shi SX, 2007, SIMUL MODEL PRACT TH, V15, P635, DOI 10.1016/j.simpat.2007.01.004
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Tessendorf J, 2004, SIGGRAPH 2004, V31
   Thon S, 2002, ADVANCES IN MODELLING, ANIMATION AND RENDERING, P333
   Thurey N, 2006, P 2006 EUR ACM SIGGR
   Yu QZ, 2011, COMPUT ANIMAT VIRT W, V22, P91, DOI 10.1002/cav.391
   Yu QZ, 2009, COMPUT GRAPH FORUM, V28, P239, DOI 10.1111/j.1467-8659.2009.01363.x
   Yuksel C, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239550
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
NR 28
TC 3
Z9 4
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2013
VL 29
IS 9
BP 959
EP 968
DI 10.1007/s00371-013-0851-z
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 186CB
UT WOS:000322018100011
DA 2024-07-18
ER

PT J
AU Ren, ZG
   Gai, WJ
   Zhong, F
   Pettré, J
   Peng, QS
AF Ren, Zhiguo
   Gai, Wenjing
   Zhong, Fan
   Pettre, Julien
   Peng, Qunsheng
TI Inserting virtual pedestrians into pedestrian groups video with behavior
   consistency
SO VISUAL COMPUTER
LA English
DT Article
DE Mixed reality; Agent-based simulation; Steering methods; Path planning
ID CHARACTERS
AB In this paper, we propose a novel approach to integrate virtual pedestrians into a scene of real pedestrian groups with behavior consistency, and this is achieved by dynamic path planning of virtual pedestrians. Rather than accounting for the local collision avoidance only, our approach is capable of finding an optimized path for each virtual pedestrian on his way based on the current global distribution of the real groups in the scene. The big challenge is due to the information of both position and velocity of real pedestrians in the video being unavailable; also the distribution of the groups in the scene may vary dynamically. We therefore need to detect and track real pedestrians on each frame of the video to acquire their distribution and motion information. We save this information by an efficient data structure, called environment grid. During the way of a virtual pedestrian, the respective agent frequently emits the detection rays through the environment cells to find the situation of the real pedestrians ahead of him and adjust the original path if necessary. Virtual pedestrians are merged into the video finally with the occlusion between virtual characters and the real pedestrians correctly presented. Experiment results on several scenarios demonstrate the effectiveness of the proposed approach.
C1 [Ren, Zhiguo; Gai, Wenjing; Peng, Qunsheng] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310003, Zhejiang, Peoples R China.
   [Zhong, Fan] Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Peoples R China.
   [Pettre, Julien] INRIA Rennes, Mimet Team, Rennes, France.
C3 Zhejiang University; Shandong University; Universite de Rennes
RP Ren, ZG (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310003, Zhejiang, Peoples R China.
EM naicer@126.com
RI Pettré, Julien/AAB-2590-2022
FU National Basic Research Program of China [2009CB320802]; National
   Natural Science Foundation of China [61272302]
FX This paper was supported partly by National Basic Research Program of
   China under granted No. 2009CB320802 and National Natural Science
   Foundation of China under granted No. 61272302.
CR Abad F, 2002, P EUROGRAPH ICS
   Andriluka M, P COMP VIS PATT REC, P1
   [Anonymous], P IEEE INT C COMP VI
   Azuma RT, 1997, PRESENCE-VIRTUAL AUG, V6, P355, DOI 10.1162/pres.1997.6.4.355
   Benenson R, 2012, PROC CVPR IEEE, P2903, DOI 10.1109/CVPR.2012.6248017
   Breitenstein MD, 2011, IEEE T PATTERN ANAL, V33, P1820, DOI 10.1109/TPAMI.2010.232
   Fiorini P, 1998, INT J ROBOT RES, V17, P760, DOI 10.1177/027836499801700706
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   HENDERSON LF, 1971, NATURE, V229, P381, DOI 10.1038/229381a0
   Izadi S, 2011, P UIST, P559, DOI DOI 10.1145/2047196.2047270
   Jiang H, 2010, COMPUT GRAPH-UK, V34, P537, DOI 10.1016/j.cag.2010.05.013
   Karamouzas I., 2010, Proceedings of the 17th ACM Symposium on Virtual Reality Software and Technology, VRST '10, P183, DOI DOI 10.1145/1889863
   Kim H, 2005, SIGNAL PROCESS-IMAGE, V20, P61, DOI 10.1016/j.image.2004.10.004
   Kim S, P 10 INT WORKSH ALG
   MALLADI R, 1995, IEEE T PATTERN ANAL, V17, P158, DOI 10.1109/34.368173
   Narain R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618468
   Pellegrini S, 2009, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2009.5459260
   Pettre J., 2009, Proceedings of the 2009 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '09, P189, DOI DOI 10.1145/1599470.1599495
   Piccardi M, 2004, IEEE SYS MAN CYBERN, P3099, DOI 10.1109/ICSMC.2004.1400815
   Reynolds C. W., 1999, P GAM DEV C, P763
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Somasundaram A, 2003, COMP ANIM CONF PROC, P137, DOI 10.1109/CASA.2003.1199315
   Sudowe Patrick, 2011, Computer Vision Systems. Proceedings 8th International Conference (ICVS 2011), P11, DOI 10.1007/978-3-642-23968-7_2
   Treuille A, 2006, ACM T GRAPHIC, V25, P1160, DOI 10.1145/1141911.1142008
   van den Berg J, 2011, SPRINGER TRAC ADV RO, V70, P3
   Wu JX, 2011, IEEE INT CONF ROBOT, P860
   Zhang YJ, 2011, COMPUT ANIMAT VIRT W, V22, P499, DOI 10.1002/cav.427
NR 27
TC 6
Z9 8
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2013
VL 29
IS 9
BP 927
EP 936
DI 10.1007/s00371-013-0853-x
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 186CB
UT WOS:000322018100008
DA 2024-07-18
ER

PT J
AU Zuo, Q
   Qi, Y
   Qin, H
AF Zuo, Qing
   Qi, Yue
   Qin, Hong
TI A novel, integrated smoke simulation design method supporting local
   projection and guiding control over adaptive grids
SO VISUAL COMPUTER
LA English
DT Article
DE Smoke simulation; Guiding control; Adaptive grid
ID PARTICLE; WATER
AB High-fidelity smoke simulation in a large-scale complex environment is extremely time-consuming due to the expensive computational cost of using highly dense regular grids. There have been quite a few improved algorithms/techniques aiming to enhance the simulation's visual effects and reduce the time consumption during the last two decades. However, most of the state-of-the-art methods will encounter difficulties of not being able to model fine turbulent details during simulation or losing high-frequency shape details at the fine scale when simulated smoke interacting with nearby obstacles. One straightforward solution is to continue to refine spatial resolution at the expense of increased time complexity. This paper, however, advocates an improved strategy for smoke simulation design over adaptive grids, while simultaneously enabling the functionalities of local projection and guiding control. First, our new integrated method supports adaptive grid projection that can significantly reduce the computational cost during the velocity projection phase. During smoke simulation design, the use of adaptive grids flexibly accommodates finer cells near obstacles with fine details, and coarser cells anywhere else, as a result, fine-scale object features can be faithfully retained without the need of global grid refinement. Second, our integrated solution over adaptive grids can tightly couple guiding control with local projection, which is capable of handling tiny obstacles that are impossible to model with global coarse grids alone during simulation preview. Comprehensive experiments have shown that our integrated method has the advantage of generating turbulent phenomena when interacting with small-scale features of obstacles, and at the same time offering the preview mechanism for efficient large-scale smoke simulation design.
C1 [Zuo, Qing; Qi, Yue] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11790 USA.
C3 Beihang University; State University of New York (SUNY) System; State
   University of New York (SUNY) Stony Brook
RP Zuo, Q (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
EM zuoqing1988@aliyun.com
RI qi, yue/KLE-0386-2024
FU National Natural Science Foundation of China [61073078, 61272348,
   61202235, 61190120, 61190121, 61190125]; Ph.D. Program Foundation of
   Ministry of Education of China [20111102110018]; National Science
   Foundation of USA [IIS-0949467, IIS-1047715, IIS-1049448]; Direct For
   Computer & Info Scie & Enginr; Div Of Information & Intelligent Systems
   [1049448, 1047715] Funding Source: National Science Foundation
FX This research is supported in part by National Natural Science
   Foundation of China (Nos. 61073078, 61272348, 61202235, 61190120,
   61190121, and 61190125), Ph.D. Program Foundation of Ministry of
   Education of China (No. 20111102110018) and National Science Foundation
   of USA (Nos. IIS-0949467, IIS-1047715, and IIS-1049448).
CR [Anonymous], 2008, P 2008 ACM SIGGRAPHE
   Chen F, 2011, COMPUT GRAPH FORUM, V30, P435, DOI 10.1111/j.1467-8659.2011.01872.x
   Chorin A., 1979, A Mathematical Introduction to Fluid Mechanics
   Dobashi Y, 2008, COMPUT GRAPH FORUM, V27, P477, DOI 10.1111/j.1467-8659.2008.01145.x
   Elcott S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1189762.1189766
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Horvath C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531347
   Huang Ruoguan., 2011, Proceedings of the 2011 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA'11, P177, DOI DOI 10.1145/2019406.2019430
   Kim B, 2007, IEEE T VIS COMPUT GR, V13, P135, DOI 10.1109/TVCG.2007.3
   Kim T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360649
   Lentine M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778851
   Lentine Michael., 2011, Proceedings of the 2011 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '11, P91, DOI [10.1145/2019406.2019419, DOI 10.1145/2019406.2019419]
   Losasso F, 2004, ACM T GRAPHIC, V23, P457, DOI 10.1145/1015706.1015745
   McNamara A, 2004, ACM T GRAPHIC, V23, P449, DOI 10.1145/1015706.1015744
   Mullen P, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531344
   Narain R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409119
   Nielsen MB, 2010, COMPUT GRAPH FORUM, V29, P705, DOI 10.1111/j.1467-8659.2009.01640.x
   Nielsen MichaelB., 2009, SCA '09: Proc. of the 2009 ACM SIGGRAPH/Eurographics Symp. on Comput. Anim, P217
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Pfaff T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618467
   Rasmussen N, 2003, ACM T GRAPHIC, V22, P703, DOI 10.1145/882262.882335
   Schechter H., 2008, Symposium on Computer animation, P1
   Selle A, 2005, ACM T GRAPHIC, V24, P910, DOI 10.1145/1073204.1073282
   Selle A, 2008, J SCI COMPUT, V35, P350, DOI 10.1007/s10915-007-9166-4
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Thürey N, 2009, GRAPH MODELS, V71, P221, DOI 10.1016/j.gmod.2008.12.007
   Treuille A, 2003, ACM T GRAPHIC, V22, P716, DOI 10.1145/882262.882337
   Treuille A, 2006, ACM T GRAPHIC, V25, P826, DOI 10.1145/1141911.1141962
   Wicke M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531345
   Yuan Z, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024170
NR 30
TC 4
Z9 5
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2013
VL 29
IS 9
BP 883
EP 892
DI 10.1007/s00371-013-0848-7
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 186CB
UT WOS:000322018100004
DA 2024-07-18
ER

PT J
AU Nie, YW
   Zhang, Q
   Wang, RF
   Xiao, CX
AF Nie, Yongwei
   Zhang, Qing
   Wang, Renfang
   Xiao, Chunxia
TI Video retargeting combining warping and summarizing optimization
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Video summarization; Video retargeting; Video completion; Bidirectional
   similarity; Texture synthesis
ID IMAGE; COMPLETION
AB We construct a unified interactive video retargeting system for video summarization, completion, and reshuffling. Our system combines the advantages of both video warping and summarizing processing. We first warp the video to present initial editing results, then refine the results using patch-based summarizing optimization, which mainly eliminates possible distortion produced in the warping step. We develop a Mean Value Coordinate (MVC) warping method due to its simplicity and efficiency used in the initialization. For refining processing, the summarization optimization is built on a 3D bidirectional similarity measure between the original and edited video, to preserve the coherence and completeness of the final editing result. We further improve the quality of summarization by applying a color histogram matching during the optimization, and accelerate the summarization optimization by using a constrained 3D Patch-Match algorithm. Experiment results show that the proposed video retargeting system effectively supports video summarization, completion, and reshuffling while avoiding issues like texture broken, video jittering, and detail losing.
C1 [Nie, Yongwei; Zhang, Qing; Xiao, Chunxia] Wuhan Univ, Comp Sch, Wuhan 430072, Peoples R China.
   [Wang, Renfang] Zhejiang Wanli Univ, Sch Comp Sci & Informat, Coll Comp Sci & Informat Technol, Hangzhou, Zhejiang, Peoples R China.
C3 Wuhan University; Zhejiang Wanli University
RP Xiao, CX (corresponding author), Wuhan Univ, Comp Sch, Wuhan 430072, Peoples R China.
EM nieyongwei@gmail.com; zhangqing.whu.cs@gmail.com; renfangwang@gmail.com;
   cxxiao@whu.edu.cn
RI Zhang, Qing/ABB-1569-2021
FU National Basic Research Program of China [2012CB725303]; NSFC [61070081,
   41271431]; Open Project Program of the State Key Lab of CADCG [A1208];
   Luojia Outstanding Young Scholar Program of Wuhan University; Project of
   the Science and Technology Plan for Zhejiang Province [2012C21004];
   Fundamental Research Funds for the Central Universities; Academic Award
   for Excellent Ph.D. Candidates; Ministry of Education of China
   [5052012211001]
FX This work was partly supported by the National Basic Research Program of
   China (No. 2012CB725303), the NSFC (Nos. 61070081 and 41271431), the
   Open Project Program of the State Key Lab of CAD&CG (Grant No. A1208),
   the Luojia Outstanding Young Scholar Program of Wuhan University, the
   Project of the Science and Technology Plan for Zhejiang Province (Grant
   No. 2012C21004), the Fundamental Research Funds for the Central
   Universities, and the Academic Award for Excellent Ph.D. Candidates
   funded by Ministry of Education of China (No. 5052012211001).
CR [Anonymous], RENDERING TECHNIQUES
   [Anonymous], ICCV
   [Anonymous], 2006, P 14 ACM INT C MULT
   [Anonymous], 2007, 2007 IEEE 11 INT C C, DOI DOI 10.1109/ICCV.2007.4409010
   [Anonymous], INVERSE TEXTURE SYNT
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Dahan MJ, 2012, VISUAL COMPUT, V28, P1181, DOI 10.1007/s00371-011-0667-7
   Deselaers T., 2008, CVPR 2008, P1
   Floater MS, 2005, COMPUT AIDED GEOM D, V22, P623, DOI 10.1016/j.cagd.2005.06.004
   Hormann K, 2006, ACM T GRAPHIC, V25, P1424, DOI 10.1145/1183287.1183295
   Ju T, 2005, ACM T GRAPHIC, V24, P561, DOI 10.1145/1073204.1073229
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239453
   Krähenbühl P, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1616452.1618472, 10.1145/1618452.1618472]
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Rubinstein M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531329
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Saad Y., 1996, Iterative methods for sparse linear systems, V620
   Santella A., 2006, Conference on Human Factors in Computing Systems. CHI2006, P771
   Si Hang., 2006, A quality tetrahedral mesh generator and three-dimensional delaunay triangulator
   Simakov D, 2008, PROC CVPR IEEE, P3887
   Suh Bongwon, 2003, P ACM S US INT SOFTW, P95, DOI DOI 10.1145/964696.964707
   Wang D, 2011, VISUAL COMPUT, V27, P853, DOI 10.1007/s00371-011-0559-x
   Wang Y.-S., 2010, ACM T GRAPHIC, V29, P1
   Wang YS, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964983
   Wang YS, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618473
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
   Wu HS, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866185
   Xiao CX, 2008, COMPUT ANIMAT VIRT W, V19, P341, DOI 10.1002/cav.249
   Xiao CX, 2011, IEEE T VIS COMPUT GR, V17, P1122, DOI 10.1109/TVCG.2010.226
   Zhang GX, 2009, COMPUT GRAPH FORUM, V28, P1897, DOI 10.1111/j.1467-8659.2009.01568.x
   Zhang YF, 2008, COMPUT GRAPH FORUM, V27, P1797, DOI 10.1111/j.1467-8659.2008.01325.x
NR 32
TC 11
Z9 11
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 785
EP 794
DI 10.1007/s00371-013-0830-4
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400031
DA 2024-07-18
ER

PT J
AU Oh, YT
   Kim, YJ
   Lee, J
   Kim, MS
   Elber, G
AF Oh, Young-Taek
   Kim, Yong-Joon
   Lee, Jieun
   Kim, Myung-Soo
   Elber, Gershon
TI Continuous point projection to planar freeform curves using spiral
   curves
SO VISUAL COMPUTER
LA English
DT Article
DE Continuous point projection; Bounding volume hierarchy; Axis-aligned
   bounding box; Line swept sphere; Minimum distance; Hausdorff distance;
   Culling; Curve reparameterization; Geometric algorithms
ID MINIMUM DISTANCE
AB We present an efficient algorithm for projecting a continuously moving query point to a family of planar freeform curves. The algorithm is based on the one-sided Hausdorff distance from the trajectory curve (of the query point) to the planar curves. Using a bounding volume hierarchy (BVH) of the planar curves, we estimate an upper bound (h) over bar of the one-sided Hausdorff distance and eliminate redundant curve segments when they are more than distance (h) over bar away from the trajectory curve. Recursively subdividing the trajectory curve and repeating the same elimination procedure to the BVH of the remaining curves, we can efficiently determine where to project the moving query point. The explicit continuous point projection is then interpreted as a curve reparameterization problem, for which we propose a few simple approximation techniques. Using several experimental results, we demonstrate the effectiveness of the proposed approach.
C1 [Oh, Young-Taek; Kim, Yong-Joon; Kim, Myung-Soo] Seoul Natl Univ, Sch Engn & Comp Sci, Seoul 151744, South Korea.
   [Lee, Jieun] Chosun Univ, Sch Comp Engn, Kwangju 501759, South Korea.
   [Elber, Gershon] Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.
C3 Seoul National University (SNU); Chosun University; Technion Israel
   Institute of Technology
RP Kim, MS (corresponding author), Seoul Natl Univ, Sch Engn & Comp Sci, Seoul 151744, South Korea.
EM mskim@snu.ac.kr
RI Lee, Eun-ju/JAN-8749-2023; LEE, YU/JXY-2338-2024; LEE,
   JIEUN/HTN-1777-2023; Lee, Jieun/J-3909-2016
OI Lee, Jieun/0000-0001-5692-9263
FU Israeli Ministry of Science [3-38273]; NRF [2010-00787, 2011-0018017,
   2011-0005642]; Seoul Fellowship
FX This research was supported in part by the Israeli Ministry of Science
   Grant No. 3-38273, and in part by NRF Research Grants (Nos. 2010-00787,
   2011-0018017, and 2011-0005642). Y.-T. Oh was supported by Seoul
   Fellowship.
CR Barton M, 2011, GRAPH MODELS, V73, P50, DOI 10.1016/j.gmod.2010.10.005
   Barton M, 2010, COMPUT AIDED GEOM D, V27, P580, DOI 10.1016/j.cagd.2010.04.004
   BECKMANN N, 1990, ACM SIGMOD, P322, DOI DOI 10.1145/93597.98741
   Chen XD, 2008, COMPUT AIDED DESIGN, V40, P1051, DOI 10.1016/j.cad.2008.06.008
   Do Carmo M., 1976, Differential Geometry of Curves and Surfaces
   Elber G., 2001, P 6 ACM S SOLID MODE, P1, DOI [10.1145/376957.376958, DOI 10.1145/376957.376958]
   GILBERT EG, 1988, IEEE T ROBOTIC AUTOM, V4, P193, DOI 10.1109/56.2083
   Hu SM, 2005, COMPUT AIDED GEOM D, V22, P251, DOI 10.1016/j.cagd.2004.12.001
   JOHNSON D, 2005, THESIS U UTAH
   Johnson DE, 1998, IEEE INT CONF ROBOT, P3678, DOI 10.1109/ROBOT.1998.681403
   Kim YJ, 2010, VISUAL COMPUT, V26, P1007, DOI 10.1007/s00371-010-0477-3
   Larsen E., 2000, IEEE INT C ROB AUT
   Lee IK, 1998, GRAPH MODEL IM PROC, V60, P136, DOI 10.1006/gmip.1998.0464
   Lin M.C., 2004, HDB DISCRETE COMPUTA, Vsecond, P787
   Lin M.C., 1998, PROC IMA C MATH SURF, P37
   LIN MC, 1991, 1991 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-3, P1008, DOI 10.1109/ROBOT.1991.131723
   Liu XM, 2009, COMPUT AIDED GEOM D, V26, P593, DOI 10.1016/j.cagd.2009.01.004
   Ma YL, 2003, COMPUT AIDED GEOM D, V20, P79, DOI 10.1016/S0167-8396(03)00021-9
   OH YT, 2011, COMPUT AIDE IN PRESS, V28
   Peternell M, 2000, GRAPH MODELS, V62, P202, DOI 10.1006/gmod.1999.0521
   Selimovic I, 2006, COMPUT AIDED GEOM D, V23, P439, DOI 10.1016/j.cagd.2006.01.007
   SEONG JK, 2006, SPM 06, P197
   Tang M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531380
   *TECHN, IRIT 10 0 US MAN
NR 24
TC 7
Z9 7
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2012
VL 28
IS 1
BP 111
EP 123
DI 10.1007/s00371-011-0632-5
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 874YQ
UT WOS:000298995000010
DA 2024-07-18
ER

PT J
AU Na, KG
   Jung, MR
AF Na, Kyung-Gun
   Jung, Moon-Ryul
TI Local shape blending using coherent weighted regions
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Facial motion retargetting; Shape blending
ID CAPTURE; GEOMETRY; FACES
AB We present a novel local shape blending method that maps a sparse configuration of facial markers captured from an actor onto target models. The advantage of local shape blending methods is that, given a small set of key shapes for each local region, their combination can generate various facial expressions. However, they have the common problem that they use the pre-determined (fixed) regions and compute the combination of local key shapes for each region independently of each other. So, they have a risk of breaking natural correlations between the regions. We present a stochastic method of computing the regions and the blending weight vectors simultaneously. To do so, we formulate local shape blending as a problem of finding an optimal distribution of blending weight vectors of all control points in MAP-MRF framework.
C1 [Na, Kyung-Gun; Jung, Moon-Ryul] Sogang Univ, Dept Media Technol, Seoul, South Korea.
C3 Sogang University
RP Jung, MR (corresponding author), Sogang Univ, Dept Media Technol, Seoul, South Korea.
EM moon@sogang.ac.kr
CR [Anonymous], 2001, COMP SCI W
   [Anonymous], 2004, P 2004 EUR ACM SIGGR
   Bichke B., 2008, P 2008 ACM SIGGRAPH
   Bickel B, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239484
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Blanz V, 2003, COMPUT GRAPH FORUM, V22, P641, DOI 10.1111/1467-8659.t01-1-00712
   Broomhead D. S., 1988, Complex Systems, V2, P321
   BUCK I, 2000, NPAR 2000, P101
   Cao Y., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P225
   Choe B, 2001, COMP ANIM CONF PROC, P12, DOI 10.1109/CA.2001.982372
   Choe B, 2001, J VISUAL COMP ANIMAT, V12, P67, DOI 10.1002/vis.246
   Chuang E, 2005, ACM T GRAPHIC, V24, P331, DOI 10.1145/1061347.1061355
   CHUANG E., 2002, PERFORMANCE DRIVEN F
   Deng Z., 2007, COMPUTER FACIAL ANIM
   Essa I., 1996, Proceedings. Computer Animation '96, P68, DOI 10.1109/CA.1996.540489
   GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596
   Grinspun E., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P62
   Guenin BM, 1998, P IEEE SEMICOND THER, P55, DOI 10.1109/STHERM.1998.660387
   JOSHI P, 2003, S COMP AN, P187
   Ju E, 2008, COMPUT GRAPH FORUM, V27, P381, DOI 10.1111/j.1467-8659.2008.01135.x
   Lee Y.C., 1995, SIGGRAPH Proceedings, P55
   Lewis JP, 2005, P 2005 S INT 3D GRAP, P25
   Ma WC, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409074
   Na K, 2004, COMPUT GRAPH FORUM, V23, P687, DOI 10.1111/j.1467-8659.2004.00801.x
   Na KG, 2010, COMPUT ANIMAT VIRT W, V21, P255, DOI 10.1002/cav.346
   NOH J, 2001, SIGGRAPH 2001, V277
   Parke FrederickI., 1972, Proceedings of the ACM annual conference, V1, P451
   PIGHIN F, 2006, SIGGRAPH 06, P2
   PIGHIN F, 1998, SIGGRAPH P, P75
   PIGHIN FH, 1999, ICCV, P143
   Pyun H., 2003, SIGGRAPH/EUROGRAPHICS Symposium on Computer Animation, P167
   Sifakis E, 2005, ACM T GRAPHIC, V24, P417, DOI 10.1145/1073204.1073208
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Surazhsky V, 2005, ACM T GRAPHIC, V24, P553, DOI 10.1145/1073204.1073228
   Vlasic D, 2005, ACM T GRAPHIC, V24, P426, DOI 10.1145/1073204.1073209
   Wang Y, 2004, COMPUT GRAPH FORUM, V23, P677, DOI 10.1111/j.1467-8659.2004.00800.x
   Williams L., 1990, Computer Graphics, V24, P235, DOI 10.1145/97880.97906
   Zhang L, 2004, ACM T GRAPHIC, V23, P548, DOI 10.1145/1015706.1015759
   Zhang QS, 2006, IEEE T VIS COMPUT GR, V12, P48, DOI 10.1109/TVCG.2006.9
NR 39
TC 3
Z9 3
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 575
EP 584
DI 10.1007/s00371-011-0591-x
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600016
DA 2024-07-18
ER

PT J
AU Wang, YM
   Pan, G
   Liu, JZ
AF Wang, Yueming
   Pan, Gang
   Liu, Jianzhuang
TI A deformation model to reduce the effect of expressions in 3D face
   recognition
SO VISUAL COMPUTER
LA English
DT Article
DE 3D face recognition; Deformation; Expression; Poisson equation
ID FACIAL EXPRESSION
AB In 3D face recognition, most work utilizes the rigid parts of face surfaces for matching to exclude the distortion caused by expressions. However, across a broad range of expressions, the rigid parts may not always be uniform and cover large parts of faces. On the other hand, the non-rigid regions of face surfaces also contain useful information for recognition. In this paper, we include the non-rigid regions besides the rigid parts for 3D face recognition. A deformation model is proposed to deform the non-rigid regions to the shapes that are more similar between intra-personal samples but less similar between inter-personal samples. Together with the rigid regions, the deformed parts make samples more discriminable so that the effect of expressions is reduced. The first part of our model uses the target gradient fields from enrolled samples to depress the distortion of the non-rigid regions. The gradient field works in the differential domain. According to the Poisson equation, a smooth deformed shape can be computed by a linear system. The second part of the model is the definition of a surface property that determines the deformation ability of different face regions. Unlike the target gradient fields that improve the similarity of intra-personal samples, the original topology and surface property can keep inter-personal samples sufficiently dissimilar. Our deformation model can be used to improve existing 3D face recognition methods. Experiments are carried out on FRGC and BU-3DFE databases. There are about 8-10% improvements obtained after applying this deformation model to the baseline ICP method. Compared with other deformation models, the experimental results show that our model has advantages on both recognition performance and computational efficiency.
C1 [Pan, Gang] Zhejiang Univ, Coll Comp Sci, Hangzhou 310003, Zhejiang, Peoples R China.
   [Wang, Yueming; Liu, Jianzhuang] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Hong Kong, Peoples R China.
   [Wang, Yueming] Zhejiang Univ, Qiushi Acad Adv Studies, Hangzhou 310003, Zhejiang, Peoples R China.
C3 Zhejiang University; Chinese University of Hong Kong; Zhejiang
   University
RP Pan, G (corresponding author), Zhejiang Univ, Coll Comp Sci, Hangzhou 310003, Zhejiang, Peoples R China.
EM ymingwang@gmail.com; gpan@zju.edu.cn; jzliu@ie.cuhk.edu.hk
RI Pan, Gang/B-5978-2013
CR Achermann B, 1997, INTERNATIONAL CONFERENCE ON VIRTUAL SYSTEMS AND MULTIMEDIA - VSMM'97, PROCEEDINGS, P129, DOI 10.1109/VSMM.1997.622339
   Achermann B, 2000, INT C PATT RECOG, P809, DOI 10.1109/ICPR.2000.906199
   [Anonymous], P EUR WORKSH SCI VIS
   [Anonymous], 1988, MANIFOLDS TENSOR ANA, DOI DOI 10.1007/978-1-4612-1029-0
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Boehnen C, 2009, LECT NOTES COMPUT SC, V5558, P12, DOI 10.1007/978-3-642-01793-3_2
   Bowyer KW, 2006, COMPUT VIS IMAGE UND, V101, P1, DOI 10.1016/j.cviu.2005.05.005
   BRONSTEIN A, 2003, P AUD VID BAS BIOM P, P62
   Bronstein AM, 2006, LECT NOTES COMPUT SC, V3953, P396, DOI 10.1007/11744078_31
   Bronstein AM, 2006, P NATL ACAD SCI USA, V103, P1168, DOI 10.1073/pnas.0508601103
   Bronstein AM, 2005, INT J COMPUT VISION, V64, P5, DOI 10.1007/s11263-005-1085-y
   Chang K, 2005, PROC SPIE, V5779, P132, DOI 10.1117/12.604171
   Chang KI, 2006, IEEE T PATTERN ANAL, V28, P1695, DOI 10.1109/TPAMI.2006.210
   Chin-Seng Chua, 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P233, DOI 10.1109/AFGR.2000.840640
   DESBRUN M, 2002, P EUR
   Faltemier TC, 2008, IEEE T INF FOREN SEC, V3, P62, DOI 10.1109/TIFS.2007.916287
   Faltemier TC, 2008, COMPUT VIS IMAGE UND, V112, P114, DOI 10.1016/j.cviu.2008.01.004
   Gordon G. G., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P808, DOI 10.1109/CVPR.1992.223253
   Hesher C, 2003, SEVENTH INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND ITS APPLICATIONS, VOL 2, PROCEEDINGS, P201, DOI 10.1109/ISSPA.2003.1224850
   Kakadiaris IA, 2007, IEEE T PATTERN ANAL, V29, P640, DOI 10.1109/TPAMI.2007.1017
   Lee YH, 2004, IEEE IMAGE PROC, P1429
   Lu X., 2006, Proc. IEEE Conf. Computer Vision and Pattern Recognition, V2, P1377
   Lu XG, 2006, IEEE T PATTERN ANAL, V28, P31, DOI 10.1109/TPAMI.2006.15
   Lu XG, 2008, IEEE T PATTERN ANAL, V30, P1346, DOI 10.1109/TPAMI.2007.70784
   Markey M. K., 2007, P IEEE C COMP VIS PA
   Maurer T., 2005, IEEE WORKSH FAC REC
   Medioni G, 2003, IEEE INTERNATIONAL WORKSHOP ON ANALYSIS AND MODELING OF FACE AND GESTURES, P232
   Meyer M., 2002, P VISMATH
   Mian AS, 2008, INT J COMPUT VISION, V79, P1, DOI 10.1007/s11263-007-0085-5
   Mian AS, 2007, IEEE T PATTERN ANAL, V29, P1927, DOI 10.1109/TPAMI.2007.1105
   MORENO AB, 2003, P IR MACH VIS IM PRO
   NAGAMINE T, 1992, 11TH IAPR INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION, PROCEEDINGS, VOL I, P324, DOI 10.1109/ICPR.1992.201567
   Pan G, 2003, IEEE IJCNN, P2169
   Phillips P.J., 2006, FRVT 2006 and ICE 2006 large-scale results
   Phillips PJ, 2005, PROC CVPR IEEE, P947
   PHILLIPS PJ, FRVT 2002 OV SUMM
   Polthier K., 2002, VISUALIZATION MATH
   Queirolo CC, 2010, IEEE T PATTERN ANAL, V32, P206, DOI 10.1109/TPAMI.2009.14
   RUSS TD, 2005, P IEEE WORKSH FAC RE
   Samir C, 2006, IEEE T PATTERN ANAL, V28, P1858, DOI 10.1109/TPAMI.2006.235
   Scheenstra A, 2005, LECT NOTES COMPUT SC, V3546, P891
   Tanaka HT, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P372, DOI 10.1109/AFGR.1998.670977
   Tong YY, 2003, ACM T GRAPHIC, V22, P445, DOI 10.1145/882262.882290
   Wang Y., 2007, P IEEE C COMP VIS PA, P1
   Wang YM, 2006, LECT NOTES COMPUT SC, V3851, P581
   Wang YM, 2008, LECT NOTES COMPUT SC, V5302, P603, DOI 10.1007/978-3-540-88682-2_46
   XU C, 2006, P EUR C COMP VIS, P416
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Yu YZ, 2004, ACM T GRAPHIC, V23, P644, DOI 10.1145/1015706.1015774
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
NR 50
TC 1
Z9 2
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2011
VL 27
IS 5
BP 333
EP 345
DI 10.1007/s00371-010-0530-2
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 745ZI
UT WOS:000289209800001
DA 2024-07-18
ER

PT J
AU Nguyen, KT
   Jang, H
   Han, J
AF Nguyen, Kien T.
   Jang, Hanyoung
   Han, JungHyun
TI Layered occlusion map for soft shadow generation
SO VISUAL COMPUTER
LA English
DT Article
DE Soft shadow algorithm; Image processing; Hardware accelerated rendering;
   Real-time shadowing
AB This paper presents a high-quality high-performance algorithm to compute plausible soft shadows for complex dynamic scenes. Given a rectangular light source, the scene is rendered from the viewpoint placed at the center of the light source, and discretized into a layered depth map. For each scene point sampled in the depth map, the occlusion degree is computed, and stored in a layered occlusion map. When the scene is rendered from the camera's viewpoint, the occlusion degree of a scene point is computed by filtering the layered occlusion map. The proposed algorithm produces soft shadows the quality of which is quite close to that of the ground truth reference. As it runs very fast, a scene with a million polygons can be rendered in real-time. The proposed method does not require pre-processing and is easy to implement in contemporary graphic hardware.
C1 [Nguyen, Kien T.; Han, JungHyun] Korea Univ, Dept Comp Sci & Engn, Seoul, South Korea.
C3 Korea University
RP Han, J (corresponding author), Korea Univ, Dept Comp Sci & Engn, Seoul, South Korea.
EM jhan@korea.ac.kr
FU MKE, Korea under ITRC [NIPA-2010-(C1090-1021-0008)]
FX We would like to thank Gael Guennebaud and Kiwon Um for valuable
   discussions and suggestions. The Stanford Bunny and Dragon models were
   digitized and kindly provided by the Stanford University Computer
   Graphics Laboratory. The Waterlily, Robot, Drum set and Tree models
   appearing in this paper and in the accompanying video are downloaded
   from 3dxtras.com. This research was supported by MKE, Korea under ITRC
   NIPA-2010-(C1090-1021-0008).
CR Agrawala M, 2000, COMP GRAPH, P375, DOI 10.1145/344779.344954
   Akenine-Moller T., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P297
   [Anonymous], 1978, P 5 ANN C COMPUTER G, P270
   Arvo J, 2004, COMPUT GRAPH FORUM, V23, P271, DOI 10.1111/j.1467-8659.2004.00758.x
   Atty L, 2006, COMPUT GRAPH FORUM, V25, P725, DOI 10.1111/j.1467-8659.2006.00995.x
   Bavoil Louis, 2008, Journal of Graphics Tools, V13, P19
   BAVOIL L, 2006, TECHNICAL SKETCHES A
   CROW F, 1977, COMPUT GRAPH, V11, P242
   DRETTAKIS G, 1994, COMPUTER GRAPHICS, V28, P223
   Eisemann E, 2008, COMPUT GRAPH FORUM, V27, P13, DOI 10.1111/j.1467-8659.2007.01037.x
   Everitt C, 2001, INTERACTIVE ORDER IN
   Fernando Randima., 2005, SIGGRAPH 05, P35
   Forest V, 2008, COMPUT GRAPH FORUM, V27, P663, DOI 10.1111/j.1467-8659.2008.01164.x
   Forest V, 2009, COMPUT GRAPH FORUM, V28, P1111, DOI 10.1111/j.1467-8659.2009.01488.x
   Guennebaud G, 2007, COMPUT GRAPH FORUM, V26, P525, DOI 10.1111/j.1467-8659.2007.01075.x
   Guennebaud Gael., 2006, Proceedings of the Eurographics Symposium on Rendering, P227
   Hasenfratz JM, 2003, COMPUT GRAPH FORUM, V22, P753, DOI 10.1111/j.1467-8659.2003.00722.x
   Heidrich W, 2000, SPRING COMP SCI, P269
   Iwasaki Kei, 2007, Pro- ceedings of the Eurographics Symposium on Rendering, P35
   Laine S, 2005, ACM T GRAPHIC, V24, P1156, DOI 10.1145/1073204.1073327
   Pan M, 2007, COMPUT GRAPH FORUM, V26, P485, DOI 10.1111/j.1467-8659.2007.01071.x
   REEVES WT, 1987, COMPUTER GRAPHICS, V21, P283
   RONG G, 2006, JUMP FLOODING EFFICI, P185
   SCHWARZ M, 2008, EUROGRAPHICS 06 SHOR
   SCHWARZ M, 2008, P GRAPH INT 08
   Schwarz M, 2007, COMPUT GRAPH FORUM, V26, P515, DOI 10.1111/j.1467-8659.2007.01074.x
   Sintorn E, 2008, COMPUT GRAPH FORUM, V27, P1285, DOI 10.1111/j.1467-8659.2008.01267.x
   SLOAN PP, 2002, P ACM SIGGRAPH, P527
   Stamminger M, 2002, ACM T GRAPHIC, V21, P557, DOI 10.1145/566570.566616
   Wimmer Michael., 2004, Proceedings of the 15th Eurographics Conference on Rendering Techniques, P143, DOI DOI 10.2312/EGWR/EGSR04/143-151
   WOO A, 1990, IEEE COMPUT GRAPH, V10, P13, DOI 10.1109/38.62693
   Yang BG, 2009, COMPUT GRAPH FORUM, V28, P1121, DOI 10.1111/j.1467-8659.2009.01489.x
   Zhou K, 2005, ACM T GRAPHIC, V24, P1196, DOI 10.1145/1073204.1073332
NR 33
TC 2
Z9 3
U1 1
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2010
VL 26
IS 12
BP 1497
EP 1512
DI 10.1007/s00371-010-0507-1
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 678WA
UT WOS:000284112400006
DA 2024-07-18
ER

PT J
AU Hong, Y
   Zhu, DM
   Qiu, XJ
   Wang, ZQ
AF Hong, Yi
   Zhu, Dengming
   Qiu, Xianjie
   Wang, Zhaoqi
TI Geometry-based control of fire simulation
SO VISUAL COMPUTER
LA English
DT Article
DE Fire simulation; Modified closest-point method; Control blue core;
   L-Speed
ID EQUATIONS; SMOKE; POINT
AB High-level control of fire is very attractive to artists, as it facilitates a detail-free user interface to make desirable flame effects. In this paper, a unified framework is proposed for modeling and animating fire under general geometric constraints and evolving rules. To capture the fire projection on user's model animation, we develop a modified closest-point method (MCPM) to handle dynamic situations while maintaining the robustness of the closest-point method. A control blue core (CBC) is designed and generated automatically from the fire projection at each time step. It translates the geometric constraints and the user-specified evolving rules into implicit control conditions. Our L-Speed function leverages CBC's shape information and conducts the large-scale motion of fire, leaving the basic physically-based model to refine simulation details. The experimental results show the effectiveness of our method for modeling fire propagation along complex curves or surfaces, or forming a flaming shape and following its motion.
C1 [Hong, Yi; Zhu, Dengming; Qiu, Xianjie; Wang, Zhaoqi] Chinese Acad Sci, Inst Comp Technol, Virtual Real Lab, Beijing, Peoples R China.
   [Hong, Yi] Chinese Acad Sci, Grad Sch, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Hong, Y (corresponding author), Chinese Acad Sci, Inst Comp Technol, Virtual Real Lab, Beijing, Peoples R China.
EM hongyi@ict.ac.cn; mdzhu@ict.ac.cn; qxj@ict.ac.cn; zqwang@ict.ac.cn
FU National Natural Science Foundation of China [60603082, 60703019];
   National Key Technology RD Program [2008BAI-50B07]; NSFC [U0935003]
FX This work is supported and funded by National Natural Science Foundation
   of China Grant Nos. 60603082 and 60703019, National Key Technology R&D
   Program 2008BAI-50B07, and NSFC-Guangdong Joint Fund (U0935003). Also,
   we would like to thank the anonymous reviewers for their valuable
   comments and suggestions.
CR Beaudoin P., 2001, P GRAPH INT, P159, DOI [10.5555/780986.781006., DOI 10.5555/780986.781006]
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Breen DE, 2001, IEEE T VIS COMPUT GR, V7, P173, DOI 10.1109/2945.928169
   Chen XD, 2008, COMPUT AIDED DESIGN, V40, P1051, DOI 10.1016/j.cad.2008.06.008
   CHIBA N, 1994, J VISUAL COMP ANIMAT, V5, P37, DOI 10.1002/vis.4340050104
   Fattal R, 2004, ACM T GRAPHIC, V23, P441, DOI 10.1145/1015706.1015743
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Fendell F.E., 2001, FOREST FIRES BEHAV E, P171, DOI [10.1016/B978-012386660-8/50008-8, DOI 10.1016/B978-012386660-8/50008-8]
   HONG JM, 2007, SIGGRAPH 07, P1
   HORVATH C, 2009, SIGGRAPH 09, P1
   ISHIKAWA T, 2005, NICOGRAPH INTERNATIO, P43
   Jones MW, 1996, COMPUT GRAPH FORUM, V15, P311, DOI 10.1111/1467-8659.1550311
   Karabassi E.-A., 1999, Journal of Graphics Tools, V4, P5, DOI 10.1080/10867651.1999.10487510
   Lee H, 2001, SPRING EUROGRAP, P75
   Liu SG, 2009, VISUAL COMPUT, V25, P687, DOI 10.1007/s00371-009-0344-2
   Losasso F, 2006, IEEE T VIS COMPUT GR, V12, P343, DOI 10.1109/TVCG.2006.51
   Macdonald CB, 2008, J SCI COMPUT, V35, P219, DOI 10.1007/s10915-008-9196-6
   MALLET V, 2007, MODELING WILDLAND FI
   Melek Z, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P431, DOI 10.1109/PCCGA.2002.1167889
   Nguyen DQ, 2002, ACM T GRAPHIC, V21, P721, DOI 10.1145/566570.566643
   Nguyen DQ, 2001, J COMPUT PHYS, V172, P71, DOI 10.1006/jcph.2001.6812
   OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2
   Perrot C., 1994, Premieres rencontres autour des recherches sur les ruminants, Paris, France, l-2 decembre 1994., P1
   Qian J, 1998, J COMPUT PHYS, V144, P52, DOI 10.1006/jcph.1998.5991
   REEVES WT, 1983, SIGGRAPH 83, P359
   Shi L, 2005, ACM T GRAPHIC, V24, P140, DOI 10.1145/1037957.1037965
   Shi Lin., 2005, Proceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '05, P229, DOI DOI 10.1145/1073368.1073401
   Stam Jos., 1995, Proceedings of the 22nd annual conference on Computer graphics and interactive techniques, SIGGRAPH '95, P129
   Thurey N., 2006, Proceedings of the 2006 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA'06, P7, DOI [10.5555/1218064.1218066, DOI 10.5555/1218064.1218066]
   Wei XM, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P227, DOI 10.1109/VISUAL.2002.1183779
   Xu JJ, 2003, J SCI COMPUT, V19, P573, DOI 10.1023/A:1025336916176
   ZHAO Y, 2003, VIS 03, P36
NR 32
TC 17
Z9 19
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2010
VL 26
IS 9
BP 1217
EP 1228
DI 10.1007/s00371-009-0403-8
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 635JD
UT WOS:000280650500007
DA 2024-07-18
ER

PT J
AU Silveira, R
   Dapper, F
   Prestes, E
   Nedel, L
AF Silveira, Renato
   Dapper, Fabio
   Prestes, Edson
   Nedel, Luciana
TI Natural steering behaviors for virtual pedestrians
SO VISUAL COMPUTER
LA English
DT Article
DE Humanoid simulation; Path planning; Steering behavior; Harmonic
   functions; Boundary value problems
AB The animation of humanoids in real-time applications is yet a challenge if the problem involves attaining a precise location in a virtual world (path-planning), moving realistically according to its own personality, intentions and mood (motion planning). In this paper we propose a formally complete and low-cost solution based upon boundary value problems (BVP) to control steering behaviors of characters in dynamic environments. We use a potential field formalism that allows synthetic actors to move negotiating space, avoiding collisions, and attaining goals, while producing very individual paths. The individuality of each character can be set by changing its inner field parameters leading to a broad range of possible behaviors without jeopardizing its performance. To illustrate the technique potentialities, some results exploring situations as steering behavior in corridors with collision avoidance and competition for a goal, and searching for objects in unknown environments are presented and discussed. A proposal to automatically change the size of the field of view of each agent, producing different behaviors is also a contribution of this paper. Some comments about performance are also made to help the reader to evaluate the potential of the method.
C1 [Silveira, Renato; Dapper, Fabio; Prestes, Edson; Nedel, Luciana] Univ Fed Rio Grande do Sul, Inst Informat, BR-91501970 Porto Alegre, RS, Brazil.
C3 Universidade Federal do Rio Grande do Sul
RP Silveira, R (corresponding author), Univ Fed Rio Grande do Sul, Inst Informat, Av Bento Goncalves 9500,Campus Vale,Bloco 4, BR-91501970 Porto Alegre, RS, Brazil.
EM rsilveira@inf.ufrgs.br; fdapper@inf.ufrgs.br; prestes@inf.ufrgs.br;
   nedel@inf.ufrgs.br
RI Prestes, Edson/C-4947-2013; Nedel, Luciana/G-3506-2012
OI Nedel, Luciana/0000-0002-2390-1392
FU National Counsel of Technological and Scientific Development of Brazil
   (CNPQ); brazilian foundation Coordination for the Improvement Higher
   Education Personnel (CAPES); Research Foundation of the State of Rio
   Grande do Sul (FAPERGS)
FX The authors would like to thank National Counsel of Technological and
   Scientific Development of Brazil (CNPQ), the brazilian foundation
   Coordination for the Improvement Higher Education Personnel (CAPES) and
   Research Foundation of the State of Rio Grande do Sul (FAPERGS).
CR BURGESS RG, 2004, P BEH REPR MOD SIM B
   *CAVIAR, EC FUND CAV PROJ IST
   Choi MG, 2003, ACM T GRAPHIC, V22, P182, DOI 10.1145/636886.636889
   CONNOLLY CI, 1993, J ROBOTIC SYST, V10, P931, DOI 10.1002/rob.4620100704
   Dapper F., 2007, P COMPUT GRAPH INT, V1, P105
   JAMES J, 1998, INT WORKSH MOD MOT C, V1537, P171
   Kavraki LE, 1996, IEEE T ROBOTIC AUTOM, V12, P566, DOI 10.1109/70.508439
   KHATIB Oussama, 1980, Commande dynamique dans l'espace operationnel des robots manipulateurs en presence d'obstacles
   KOREN Y, 1991, 1991 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-3, P1398, DOI 10.1109/ROBOT.1991.131810
   LaValle S.M., 1998, RAPIDLY EXPLORING RA, V129, P98
   Mazarakis GP, 2005, Proceedings of the Second European Workshop on Wireless Sensor Networks, P415, DOI 10.1109/EWSN.2005.1462036
   Metoyer RA, 2004, VISUAL COMPUT, V20, P635, DOI 10.1007/s00371-004-0265-z
   Miller Freeman Game Group, 1999, P GAM DEV C SAN JOS, P763
   PELECHANO N, 2005, 1 INT WORKSH CROWD S, P21
   Pettré J, 2002, 2002 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-3, PROCEEDINGS, P3048, DOI 10.1109/IRDS.2002.1041736
   Pettré J, 2006, COMPUT ANIMAT VIRT W, V17, P445, DOI 10.1002/cav.147
   PRESTES E, 2003, IEEE RSJ INT C INT R, V3, P3239
   Reynolds C.W., 2006, SIGGRAPH symposium on Videogames, P113, DOI [DOI 10.1145/1183316.1183333, 10.1145/1183316.1183333]
   Shao W, 2007, GRAPH MODELS, V69, P246, DOI 10.1016/j.gmod.2007.09.001
   Silva EPE, 2002, ROBOT AUTON SYST, V40, P25, DOI 10.1016/S0921-8890(02)00209-9
   Tecchia F., 2001, P GAM TECHN GTEC, P17
   Treuille A, 2006, ACM T GRAPHIC, V25, P1160, DOI 10.1145/1141911.1142008
   Trevisan M, 2006, J INTELL ROBOT SYST, V45, P101, DOI 10.1007/s10846-005-9008-2
NR 23
TC 4
Z9 5
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2010
VL 26
IS 9
BP 1183
EP 1199
DI 10.1007/s00371-009-0399-0
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 635JD
UT WOS:000280650500005
DA 2024-07-18
ER

PT J
AU Lages, W
   Cordeiro, C
   Guedes, D
AF Lages, Wallace
   Cordeiro, Carlucio
   Guedes, Dorgival
TI Performance analysis of a parallel multi-view rendering architecture
   using light fields
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 21st Brazilian Symposium on Computer Graphics and Image Processing
CY OCT 12-15, 2008
CL Campo Grande, BRAZIL
SP Brazilian Comput Soc
DE Parallel rendering; Multiple viewpoint rendering; Light field rendering
AB Multiple view rendering is a common problem for applications where multiple users visualize a common dataset, as in multi-player games and collaborative engineering tools. For a system to be able to render a large number of views at interactive rates efficiently, parallel processing is an attractive technique. In this work, we present the implementation of a pipelined multiview light field renderer using a cluster with GPUs and MPI. We discuss the parallelization model and the problem of partitioning the tasks of the pipeline among the cluster machines based on the pipeline model and the costs of the stages. Our solution achieves 83% efficiency with ten machines, against only 11% efficiency of a naive parallelization.
C1 [Lages, Wallace; Cordeiro, Carlucio; Guedes, Dorgival] Univ Fed Minas Gerais, Deep Comp Visualizat Ctr, LVAD DCC ICEx Sala 4010, BR-31270010 Belo Horizonte, MG, Brazil.
C3 Universidade Federal de Minas Gerais
RP Guedes, D (corresponding author), Univ Fed Minas Gerais, Deep Comp Visualizat Ctr, LVAD DCC ICEx Sala 4010, Av Antonio Carlos 6627, BR-31270010 Belo Horizonte, MG, Brazil.
EM wlages@ufmg.br; carlucio@dcc.ufmg.br; dorgival@dcc.ufmg.br
RI InWeb, Inct/J-9839-2013
OI Lages, Wallace/0000-0003-2942-5793; Guedes, Dorgival/0000-0003-0865-1417
CR Adelson E.H., 1991, Computational Models of Visual Processing, P3
   ALIAGA D, 1999, I3D 99, P199
   ANNEN T, 2006, SPIE P, V6055, P231
   Buehler C, 2001, COMP GRAPH, P425, DOI 10.1145/383259.383309
   CAMAHORT E, 1998, UNIFORMLY SAMPLED LI
   Chai JX, 2000, COMP GRAPH, P307, DOI 10.1145/344779.344932
   Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   Halle M., 1998, SIGGRAPH 98, P243
   HASSELGREN J., 2006, P EUROGRAPHICS S REN, P61
   HEIDRICH W, 1999, P WSCG 99
   HUBNER T, 2006, GRAPHITE, P285
   Ihm I, 1997, FIFTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P59, DOI 10.1109/PCCGA.1997.626172
   JESCHKE S, 2005, I3D 05, P103
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Lin ZC, 2004, INT J COMPUT VISION, V58, P121, DOI 10.1023/B:VISI.0000015916.91741.27
   Lin ZC, 2000, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2000.855873
   McMillan L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P39, DOI 10.1145/218380.218398
   McMillan L, 1997, IMAGE BASED APPROACH
   SCHIRMACHER H, 2007, EFFICIENT FREE FORM
   SHADE J, 1996, SIGGRAPH 96 C P, P75
   Sloan P.-P., 1999, Proceedings 1999 IEEE Parallel Visualization and Graphics Symposium (Cat. No.99EX381), P7, DOI 10.1109/PVGS.1999.810133
   Stewart J., 2004, Proceedings of the ACM SIGGRAPH/EUROGRAPHICS Conference on Graphics Hardware, HWWS '04, P75
   Strasser J., 2006, EG PGV 2006. 6th Eurographics Symposium on Parallel Graphics and Visualization, P171
   Takahashi K, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P205, DOI 10.1109/ICME.2005.1521396
   Todt Severin., 2007, Fast (spherical) light field rendering with per-pixel depth
   VOGELGSANG C, 2000, 3 U ERL NUERNB
   Wilson A, 2003, ACM T GRAPHIC, V22, P678, DOI 10.1145/882262.882325
   Yang J, 2002, INFOVIS 2002: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2002, P77, DOI 10.1109/INFVIS.2002.1173151
NR 29
TC 1
Z9 1
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2009
VL 25
IS 10
SI SI
BP 947
EP 958
DI 10.1007/s00371-009-0371-z
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 500TP
UT WOS:000270328200006
DA 2024-07-18
ER

PT J
AU Mena-Chalco, JP
   Macêdo, I
   Velho, L
   Cesar, RM
AF Mena-Chalco, Jesus P.
   Macedo, Ives
   Velho, Luiz
   Cesar, Roberto M., Jr.
TI 3D face computational photography using PCA spaces
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 21st Brazilian Symposium on Computer Graphics and Image Processing
CY OCT 12-15, 2008
CL Campo Grande, BRAZIL
SP Brazilian Comput Soc
DE 3D face reconstruction; Principal components analysis; Computer vision;
   Computational photography
ID RECONSTRUCTION; MODELS
AB In this paper, we present a 3D face photography system based on a facial expression training dataset, composed of both facial range images (3D geometry) and facial texture (2D photography). The proposed system allows one to obtain a 3D geometry representation of a given face provided as a 2D photography, which undergoes a series of transformations through the texture and geometry spaces estimated. In the training phase of the system, the facial landmarks are obtained by an active shape model (ASM) extracted from the 2D gray-level photography. Principal components analysis (PCA) is then used to represent the face dataset, thus defining an orthonormal basis of texture and another of geometry. In the reconstruction phase, an input is given by a face image to which the ASM is matched. The extracted facial landmarks and the face image are fed to the PCA basis transform, and a 3D version of the 2D input image is built. Experimental tests using a new dataset of 70 facial expressions belonging to ten subjects as training set show rapid reconstructed 3D faces which maintain spatial coherence similar to the human perception, thus corroborating the efficiency and the applicability of the proposed system.
C1 [Mena-Chalco, Jesus P.; Cesar, Roberto M., Jr.] Univ Sao Paulo, IME, BR-05508090 Sao Paulo, Brazil.
   [Macedo, Ives; Velho, Luiz] Inst Matematica Pura & Aplicada, BR-22460320 Rio De Janeiro, Brazil.
C3 Universidade de Sao Paulo
RP Mena-Chalco, JP (corresponding author), Univ Sao Paulo, IME, Rua Matao 1010, BR-05508090 Sao Paulo, Brazil.
EM jmena@vision.ime.usp.br; ijamj@impa.br; lvelho@impa.br;
   cesar@vision.ime.usp.br
RI Mena-Chalco, Jesus P./C-7550-2014; Cesar-Jr, Roberto/C-4120-2012
OI Mena-Chalco, Jesus P./0000-0001-7509-5532; Macedo,
   Ives/0000-0002-7993-7298; Cesar-Jr, Roberto/0000-0003-2701-4288
CR [Anonymous], 1973, THESIS KYOTO U
   [Anonymous], 2005, P IEEE COMP SOC C CO, DOI DOI 10.1109/CVPR.2005.377
   Blanz V, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P617
   Bregler C., 1997, P 24 ANN C COMP GRAP, V31, P353, DOI DOI 10.1145/258734.258880
   CHAI JX, 2003, EUR SIGGRAPH S COMP, P193
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Dedeoglu G, 2006, LECT NOTES COMPUT SC, V3952, P83
   Dryden IL., 1998, Statistical Shape Analysis
   Elyan E, 2007, J COMPUT, V2, P1
   Golovinskiy A, 2006, ACM T GRAPHIC, V25, P1025, DOI 10.1145/1141911.1141988
   GOODALL C, 1991, J ROY STAT SOC B MET, V53, P285, DOI 10.1111/j.2517-6161.1991.tb01825.x
   Hastie T., 2003, The Elements of Statistical Learning: Data Mining, Inference, and Prediction
   HONG P, 2002, 3D DATA PROCESSING V, P713
   Jiang DL, 2005, PATTERN RECOGN, V38, P787, DOI 10.1016/j.patcog.2004.11.004
   LEE MW, 1999, INT C IM AN PROC, P260
   Leyvand T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360637
   Litke Nathan, 2005, Symposium on Geometry Processing, P207
   Macêdo I, 2006, SIBGRAPI, P239
   Mena-Chalco JP, 2008, SIBGRAPI, P313, DOI 10.1109/SIBGRAPI.2008.40
   ONOFRIO D, 2005, ICME, P1274
   Scholkopf B., 1997, Artificial Neural Networks - ICANN '97. 7th International Conference Proceedings, P583, DOI 10.1007/BFb0020217
   Soyel H, 2007, LECT NOTES COMPUT SC, V4633, P831
   TWINING CJ, 2001, BRIT MACH VIS C BRIT
   Vlasic D, 2005, ACM T GRAPHIC, V24, P426, DOI 10.1145/1073204.1073209
   Vollmer J, 1999, COMPUT GRAPH FORUM, V18, pC131, DOI 10.1111/1467-8659.00334
   Wang YJ, 2005, IMAGE VISION COMPUT, V23, P1018, DOI 10.1016/j.imavis.2005.07.005
   YABUI T, 2003, INT C IM PROC, P879
   Zhang Tao, 2007, Journal of the Tokyo University of Marine Science and Technology, V3, P39
NR 28
TC 6
Z9 6
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2009
VL 25
IS 10
SI SI
BP 899
EP 909
DI 10.1007/s00371-009-0373-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 500TP
UT WOS:000270328200002
DA 2024-07-18
ER

PT J
AU Aina, OO
AF Aina, Olusola O.
TI Generating anatomical substructures for physically-based facial
   animation. Part 1: A methodology for skull fitting
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Facial animation; Thin-plate splines; Kriging; Differential geometry;
   Semilandmarks
ID VARIATIONAL APPROACH
AB Physically-based facial animation techniques are capable of producing realistic facial deformations, but have failed to find meaningful use outside the academic community because they possess the disadvantages of being much harder to create, and even harder to reuse in comparison to other methods of facial animation. As a first step toward a physically-based facial animation system that is truly reusable, this paper outlines a landmark-based process for fitting a generic skull to any given face model, using thin-plate splines and extended kriging predictor deformers, and an interactive scaling technique for incorporating experimentally obtained soft-tissue depth data into the morphing process.
C1 Bournemouth Univ, Natl Ctr Comp Animat, Poole BH12 5BB, Dorset, England.
C3 Bournemouth University
RP Aina, OO (corresponding author), Bournemouth Univ, Natl Ctr Comp Animat, Poole BH12 5BB, Dorset, England.
EM oaina@bournemouth.ac.uk
CR ADLER FH, 1969, ADLERS TXB OPHTHALMO
   [Anonymous], 1990, CBMS NSF REG C SER A
   Bookstein F L, 1997, Med Image Anal, V1, P225
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Bui TD, 2003, LECT NOTES COMPUT SC, V2733, P251
   Cazals F, 2005, COMPUT AIDED GEOM D, V22, P121, DOI 10.1016/j.cagd.2004.09.004
   Duchon Jean, 1976, LECT NOTES MATH, V571, P85, DOI DOI 10.1007/BFB0086566
   Goldfeather J, 2004, ACM T GRAPHIC, V23, P45, DOI 10.1145/966131.966134
   Gunz P, 2005, DEV PRIMATOL-PROG PR, P73, DOI 10.1007/0-387-27614-9_3
   Isaaks E., 1989, Applied Geostatistics
   Kahler K., 2002, Eurographics Symp. on Comp. Animation, P55, DOI DOI 10.1145/545261.545271
   KENT JT, 1994, STAT OPTIMIZATION, P325
   Kybic J, 2002, IEEE T SIGNAL PROCES, V50, P1977, DOI 10.1109/TSP.2002.800386
   Kybic J, 2002, IEEE T SIGNAL PROCES, V50, P1965, DOI 10.1109/TSP.2002.800391
   Manheim MH, 2000, J FORENSIC SCI, V45, P48
   Mardia KV, 1996, BIOMETRIKA, V83, P207, DOI 10.1093/biomet/83.1.207
   Matheron G., 1981, DOWN TO EARTH STAT S, P77
   MEINGUET J, 1979, Z ANGEW MATH PHYS, V30, P292, DOI 10.1007/BF01601941
   Meyer M., 2002, VISUALIZATION MATH, V6, P35, DOI DOI 10.1007/978-3-662-05105-4_2
   Morse BS, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P89, DOI 10.1109/SMA.2001.923379
   Noh JY, 2001, COMP GRAPH, P277, DOI 10.1145/383259.383290
   Olea R. A., 1999, GEOSTATISTICS ENG EA
   ORVALHO VCT, 2007, THESIS U POLITECNICA
   Pyun H., 2003, SIGGRAPH/EUROGRAPHICS Symposium on Computer Animation, P167
   RACHIDI TE, 1999, P BRIT MACH VIS C 19
   Taylor K. T., 2000, Forensic Art and Illustration
   Turk G., 1999, Variational implicit surfaces
NR 27
TC 2
Z9 2
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 617
EP 625
DI 10.1007/s00371-009-0320-x
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300027
DA 2024-07-18
ER

PT J
AU Angelidis, A
   McCane, B
AF Angelidis, Alexis
   McCane, Brendan
TI Fur simulation with spring continuum
SO VISUAL COMPUTER
LA English
DT Article
DE Spring continuum; Fur animation
ID HAIR
AB We propose a practical method for generating and animating fur on parametrized surfaces using a spring continuum. Springs are physically simulated at the vertices of the polygon mesh, and spring behavior is interpolated across the mesh to provide realistic dense fur animation. Our method handles collisions between furry surfaces using a procedural model and self-collisions with a statistical model. The goal of this method is to make it easy for an animator to generate realistic dynamic fur which is efficient to simulate. The technique is most applicable to short fur, rather than long hair.
C1 [Angelidis, Alexis] Pixar Animat Studios, Emeryville, CA 94608 USA.
   [McCane, Brendan] Univ Otago, Dept Comp Sci, Dunedin, New Zealand.
C3 University of Otago
RP McCane, B (corresponding author), Univ Otago, Dept Comp Sci, POB 56, Dunedin, New Zealand.
EM alexis.angelidis@gmail.com; mccane@cs.otago.ac.nz
RI McCane, Brendan/KUD-8799-2024
OI McCane, Brendan/0000-0001-8055-3331
CR ANJYO K, 1992, COMP GRAPH, V26, P111, DOI 10.1145/142920.134021
   Aris R., 1989, VECTORS TENSORS BASI
   Bando Y, 2003, COMPUT GRAPH FORUM, V22, P411, DOI 10.1111/1467-8659.00688
   Bertails F., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P207
   BERTAILS F, 2005, P GRAPH INT, P71
   Blinn JF, 1977, P 4 ANN C COMP GRAPH, P192, DOI [10.1145/563858.563893, DOI 10.1145/563858.563893]
   Choe B., 2005, SCA 05, P153, DOI DOI 10.1145/1073368.1073389
   FONG M, 2001, SIGGRAPH COURS 36 IV
   GOLDMAN D, 1997, P ACM SIGGRAPH 97, P127
   Guerraz S, 2003, COMP ANIM CONF PROC, P73, DOI 10.1109/CASA.2003.1199306
   Hadap S, 2001, COMPUT GRAPH FORUM, V20, pC329, DOI 10.1111/1467-8659.00525
   Kajiya J. T., 1989, Computer Graphics, V23, P271, DOI 10.1145/74334.74361
   Kim TY, 2000, COMP ANIM CONF PROC, P104, DOI 10.1109/CA.2000.889055
   Lee DW, 2001, GRAPH MODELS, V63, P67, DOI 10.1006/gmod.2001.0547
   Lengyel J., 2001, P 2001 S INTERACTIVE, P227, DOI [10.1145/364338.364407, DOI 10.1145/364338.364407]
   Marschner SR, 2003, ACM T GRAPHIC, V22, P780, DOI 10.1145/882262.882345
   NEYRET F, 1995, EUR WORKSH AN SIM, V95, P97
   Perbet F., 2001, Objavljeno v Proceedings of the 2001 Symposium on Interactive 3D Graphics, I3D'01, strani, P103
   REDD JK, 2000, STUART LITTLE TALE F, P44
   Rosenblum R. E., 1991, Journal of Visualization and Computer Animation, V2, P141, DOI 10.1002/vis.4340020410
   VanGelder A, 1997, PROC GRAPH INTERF, P181
   Ward K, 2007, IEEE T VIS COMPUT GR, V13, P213, DOI 10.1109/TVCG.2007.30
NR 22
TC 3
Z9 3
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2009
VL 25
IS 3
BP 255
EP 265
DI 10.1007/s00371-008-0218-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 403RH
UT WOS:000263099200005
DA 2024-07-18
ER

PT J
AU Kraemer, P
   Cazier, D
   Bechmann, D
AF Kraemer, Pierre
   Cazier, David
   Bechmann, Dominique
TI Extension of half-edges for the representation of multiresolution
   subdivision surfaces
SO VISUAL COMPUTER
LA English
DT Article
DE Geometric modeling; Topological model; Combinatorial maps;
   Multiresolution meshes; Subdivision surfaces
ID SCHEME
AB We address in this paper the problem of the data structures used for the representation and the manipulation of multiresolution subdivision surfaces. The classically used data structures are based on quadtrees, straightforwardly derived from the nested hierarchy of faces generated by the subdivision schemes. Nevertheless, these structures have some drawbacks: specificity to the kind of mesh (triangle or quad); the time complexity of neighborhood queries is not optimal; topological cracks are created in the mesh in the adaptive subdivision case.
   We present in this paper a new topological model for encoding multiresolution subdivision surfaces. This model is an extension to the well-known half-edge data structure. It allows instant and efficient navigation at any resolution level of the mesh. Its generality allows the support of many subdivision schemes including primal and dual schemes. Moreover, subdividing the mesh adaptively does not create topological cracks in the mesh. The extension proposed here is formalized in the combinatorial maps framework. This allows us to give a very general formulation of our extension.
C1 [Kraemer, Pierre; Cazier, David; Bechmann, Dominique] Univ Strasbourg, CNRS, LSIIT, UMR 7005, Strasbourg, France.
C3 Universites de Strasbourg Etablissements Associes; Universite de
   Strasbourg; Centre National de la Recherche Scientifique (CNRS)
RP Kraemer, P (corresponding author), Univ Strasbourg, CNRS, LSIIT, UMR 7005, Strasbourg, France.
EM kraemer@lsiit.u-strasbg.fr; cazier@lsiit.u-strasbg.fr;
   bechmann@lsiit.u-strasbg.fr
OI Cazier, David/0000-0001-5247-6404
CR Bertram M, 2004, COMPUTING, V72, P29, DOI 10.1007/s00607-003-0044-0
   Biermann H, 2002, GRAPH MODELS, V64, P61, DOI 10.1006/gmod.2002.0570
   Biermann H, 2002, ACM T GRAPHIC, V21, P312, DOI 10.1145/566570.566583
   Biermann H, 2001, COMP GRAPH, P185, DOI 10.1145/383259.383280
   Biermann H, 2000, COMP GRAPH, P113, DOI 10.1145/344779.344841
   Brun L, 2006, PATTERN RECOGN, V39, P515, DOI 10.1016/j.patcog.2005.10.015
   Brun L, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 2, PROCEEDINGS, P33
   CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   Cazier D, 1999, VISUAL COMPUT, V15, P279, DOI 10.1007/s003710050178
   De Floriani L, 2005, MATH VIS, P49
   DOO D, 1978, COMPUT AIDED DESIGN, V10, P356, DOI 10.1016/0010-4485(78)90111-2
   DOO D, 1978, P INT TECHN COMP AID, P157
   Dufourd JF, 1997, VISUAL COMPUT, V13, P131, DOI 10.1007/s003710050095
   DYN N, 1990, ACM T GRAPHIC, V9, P160, DOI 10.1145/78956.78958
   Edmonds J. R., 1960, NOT AM MATH SOC, V7
   Grasset-Simon C, 2006, PATTERN RECOGN, V39, P527, DOI 10.1016/j.patcog.2005.10.004
   HERZEN BV, 1987, COMPUT GRAPH, V21, P103
   Kobbelt L, 2000, COMP GRAPH, P103, DOI 10.1145/344779.344835
   Lee A. W., 1998, Proceedings of the 25th annual conference on Computer graphics and interactive techniques, P95, DOI DOI 10.1145/280814.280828
   Lee M, 2000, ACM T GRAPHIC, V19, P79, DOI 10.1145/343593.343598
   LIENHARDT P, 1991, COMPUT AIDED DESIGN, V23, P59, DOI 10.1016/0010-4485(91)90082-8
   LIENHARDT P, 1989, 5TH P ACM S COMP GEO, P228
   Loop C, 1987, THESIS U UTAH
   Lounsbery M, 1997, ACM T GRAPHIC, V16, P34, DOI 10.1145/237748.237750
   Pajarola R, 1998, VISUALIZATION '98, PROCEEDINGS, P19, DOI 10.1109/VISUAL.1998.745280
   Pajarola R, 2004, IEEE T VIS COMPUT GR, V10, P353, DOI 10.1109/TVCG.2004.1272735
   Peters J, 1997, ACM T GRAPHIC, V16, P420, DOI 10.1145/263834.263851
   Peters J, 2004, ACM T GRAPHIC, V23, P980, DOI 10.1145/1027411.1027415
   SCHRACK G, 1992, CVGIP-IMAG UNDERSTAN, V55, P221, DOI 10.1016/1049-9660(92)90022-U
   SHIUE LJ, 2005, GI 05 P 2005 GRAPH I, V112, P153
   SHIUE LJ, 2005, ACM SE 43 P 43 ANN S, P104
   Stam J, 2003, COMPUT GRAPH FORUM, V22, P79, DOI 10.1111/1467-8659.t01-2-00647
   Taubin G, 2002, VISUAL COMPUT, V18, P357, DOI 10.1007/s003710100152
   Taubin G., 1995, P 22 ANN C COMP GRAP, P351, DOI DOI 10.1145/218380.218473
   VINCE A, 1983, J COMB THEORY B, V34, P1, DOI 10.1016/0095-8956(83)90002-3
   WEILER K, 1985, IEEE COMPUT GRAPH, V5, P21, DOI 10.1109/MCG.1985.276271
   ZORIN D, 2006, SIGGRAPH 06
   ZORIN D, 2000, SIGGRAPH 06
   ZORIN D, 1997, P SIGGRAPH 97, P259
   ZORIN D., 1996, P SIGGRAPH ANN C COM, P189
NR 40
TC 13
Z9 16
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2009
VL 25
IS 2
BP 149
EP 163
DI 10.1007/s00371-008-0211-6
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 394XE
UT WOS:000262485700005
DA 2024-07-18
ER

PT J
AU Gobbetti, E
   Marton, F
   Guitián, JAI
AF Gobbetti, Enrico
   Marton, Fabio
   Guitian, Jose Antonio Iglesias
TI A single-pass GPU ray casting framework for interactive out-of-core
   rendering of massive volumetric datasets
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE volume rendering; out-of-core rendering; GPU acceleration; ray casting
AB We present an adaptive out-of-core technique for rendering massive scalar volumes employing single-pass GPU ray casting. The method is based on the decomposition of a volumetric dataset into small cubical bricks, which are then organized into an octree structure maintained out-of-core. The octree contains the original data at the leaves, and a filtered representation of children at inner nodes. At runtime an adaptive loader, executing on the CPU, updates a view and transfer function-dependent working set of bricks maintained on GPU memory by asynchronously fetching data from the out-of-core octree representation. At each frame, a compact indexing structure, which spatially organizes the current working set into an octree hierarchy, is encoded in a small texture. This data structure is then exploited by an efficient stackless ray casting algorithm, which computes the volume rendering integral by visiting non-empty bricks in front-to-back order and adapting sampling density to brick resolution. Block visibility information is fed back to the loader to avoid refinement and data loading of occluded zones. The resulting method is able to interactively explore multi-gigavoxel datasets on a desktop PC.
C1 [Gobbetti, Enrico; Marton, Fabio; Guitian, Jose Antonio Iglesias] CRS4 Visual Comp Grp, I-09010 Pula, Italy.
RP Guitián, JAI (corresponding author), CRS4 Visual Comp Grp, Sardegna Ric Edificio 1,CP 25, I-09010 Pula, Italy.
EM gobbetti@crs4.it; marton@crs4.it; jalley@crs4.it
RI Marton, Fabio/KBC-4179-2024; Guitian, Jose A. Iglesias/A-9718-2017;
   Gobbetti, Enrico/O-2188-2015
OI Guitian, Jose A. Iglesias/0000-0002-0817-1010; Gobbetti,
   Enrico/0000-0003-0831-2458; Marton, Fabio/0000-0001-8611-1921
CR Bittner J, 2004, COMPUT GRAPH FORUM, V23, P615, DOI 10.1111/j.1467-8659.2004.00793.x
   Boada I, 2001, VISUAL COMPUT, V17, P185, DOI 10.1007/PL00013406
   Bruckner S, 2007, COMPUT GRAPH FORUM, V26, P715, DOI 10.1111/j.1467-8659.2007.01095.x
   Engel K, 2006, Real-Time Volume Graphics
   GOVINDARAJU NK, 2003, 2003 ACM S INT 3D GR, P103
   Guthe S, 2004, COMPUT GRAPH-UK, V28, P51, DOI 10.1016/j.cag.2003.10.018
   Havran V., 1998, P SPRING C COMPUTER, P130
   Hong W, 2005, VOLUME GRAPHICS 2005, P177
   Kaehler R., 2006, EUR IEEE VGTC WORKSH, P103
   KRAUS M, 2002, GRAPHICS HARDWARE, P7
   KRUEGER J, 2003, P IEEE VIS, P287
   LaMar E., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P355, DOI 10.1109/VISUAL.1999.809908
   LEFEBVRE S, 2005, OCTREE TEXTURES GPU
   LI W, 2003, VIS 03, P42
   Ljung P., 2006, P VOL GRAPH, P39
   Popov S, 2007, COMPUT GRAPH FORUM, V26, P415, DOI 10.1111/j.1467-8659.2007.01064.x
   ROETTGER S, 2003, P EG IEEE TCVG S VIS, P231
   SCHARSACH H, 2005, P CESCG 2005, P69
   Stegmaier S, 2005, VOLUME GRAPHICS 2005, P187
   VOLLRATH JE, 2006, EUR IEEE VGTC WORKSH, P55
   WILHELMS J, 1992, ACM T GRAPHIC, V11, P201, DOI 10.1145/130881.130882
NR 21
TC 96
Z9 116
U1 1
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 797
EP 806
DI 10.1007/s00371-008-0261-9
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800035
DA 2024-07-18
ER

PT J
AU Daniels, J
   Ochotta, T
   Ha, LK
   Silva, CT
AF Daniels, Joel, II
   Ochotta, Tilo
   Ha, Linh K.
   Silva, Claudio T.
TI Spline-based feature curves from point-sampled geometry
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 9th International Conference on Shape Modeling and Applications
CY JUN 13-15, 2007
CL Lyon, FRANCE
SP ACM SIGRAPT, CNRS, Groupement Rech Informat Mathemat, Reg Rhone Alpes, Univ Claude Bernard Lyon 1
DE feature extraction; moving least squares; point-based modeling; robust
   statistics; B-splines
ID SEGMENTATION
AB Defining sharp features in a 3D model facilitates a better understanding of the surface and aids geometric processing and graphics applications, such as reconstruction, filtering, simplification, reverse engineering, visualization, and non-photo realism. We present a robust method that identifies sharp features in a point-based model by returning a set of smooth spline curves aligned along the edges. Our feature extraction leverages the concepts of robust moving least squares to locally project points to potential features. The algorithm processes these points to construct arc-length parameterized spline curves fit using an iterative refinement method, aligning smooth and continuous curves through the feature points. We demonstrate the benefits of our method with three applications: surface segmentation, surface meshing and point-based compression.
C1 [Daniels, Joel, II; Ochotta, Tilo; Ha, Linh K.; Silva, Claudio T.] Univ Utah, Sci Comp & Imaging Inst, Salt Lake City, UT 84112 USA.
C3 Utah System of Higher Education; University of Utah
RP Daniels, J (corresponding author), Univ Utah, Sci Comp & Imaging Inst, 72 S Cent Campus Dr,3750 WEB, Salt Lake City, UT 84112 USA.
EM jdaniels@sci.utah.edu; ochotta@sci.utah.edu; lha@sci.utah.edu;
   csilva@sci.utah.edu
CR Alexa M, 2003, IEEE T VIS COMPUT GR, V9, P3, DOI 10.1109/TVCG.2003.1175093
   ALEXA M., 2004, Proceedings of Eurographics Symposium on Point-Based Graphics 2004, P149, DOI DOI 10.2312/SPBG/SPBG04/149-155
   Amenta N, 2004, ACM T GRAPHIC, V23, P264, DOI 10.1145/1015706.1015713
   Attene M, 2005, IEEE T VIS COMPUT GR, V11, P181, DOI 10.1109/TVCG.2005.34
   Daniels J, 2007, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2007, PROCEEDINGS, P123, DOI 10.1109/SMI.2007.32
   DEMARSIN K, 2006, 458 TW DEP COMP SCI
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Fleishman S, 2005, ACM T GRAPHIC, V24, P544, DOI 10.1145/1073204.1073227
   Garland M., 1997, PROC 24 C COMPUTER G, P209, DOI DOI 10.1145/258734.258849
   Gumhold S., 2001, P 10 INT MESH ROUNDT
   HILDEBRANDT K, 2005, P S GEOM PROC
   Hoppe H., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P59, DOI 10.1109/VISUAL.1999.809869
   Hubeli A, 2001, IEEE VISUAL, P287, DOI 10.1109/VISUAL.2001.964523
   Jenke P, 2006, COMPUT GRAPH FORUM, V25, P379, DOI 10.1111/j.1467-8659.2006.00957.x
   Jones TR, 2003, ACM T GRAPHIC, V22, P943, DOI 10.1145/882262.882367
   Lee IK, 2000, COMPUT AIDED GEOM D, V17, P161, DOI 10.1016/S0167-8396(99)00044-8
   Levin D, 2004, MATH VISUAL, P37
   LIPMAN Y, 2007, P S GEOM PROC 2007
   Ochotta T., 2004, EUROGRAPHICS S POINT, P103
   Pauly M, 2003, COMPUT GRAPH FORUM, V22, P281, DOI 10.1111/1467-8659.00675
   Pauly M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P163, DOI 10.1109/VISUAL.2002.1183771
   SCHEIDEGGER CE, 2005, P S GEOM PROC, P63
   Schreiner J, 2006, COMPUT GRAPH FORUM, V25, P527, DOI 10.1111/j.1467-8659.2006.00972.x
   Watanabe K, 2001, COMPUT GRAPH FORUM, V20, pC385, DOI 10.1111/1467-8659.00531
   Woo H, 2002, INT J MACH TOOL MANU, V42, P167, DOI 10.1016/S0890-6955(01)00120-1
   Yagou H, 2002, GEOMETRIC MODELING AND PROCESSING: THEORY AND APPLICATIONS, PROCEEDINGS, P124, DOI 10.1109/GMAP.2002.1027503
   Yang M, 1999, COMPUT AIDED DESIGN, V31, P449, DOI 10.1016/S0010-4485(99)00042-1
NR 27
TC 33
Z9 38
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2008
VL 24
IS 6
BP 449
EP 462
DI 10.1007/s00371-008-0223-2
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 299FP
UT WOS:000255741800007
DA 2024-07-18
ER

PT J
AU Lim, S
   Shin, BS
AF Lim, Sukhyun
   Shin, Byeong-Seok
TI A distance template for octree traversal in CPU-based volume ray casting
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 7th Korea-Israel Bi-National Conference on Geometric Modeling and
   Computer Graphics
CY JAN 29-30, 2007
CL Seoul, SOUTH KOREA
DE volume visualization; volume ray casting; empty-space leaping; octree
AB Several optimization techniques have been proposed to improve the speed of direct volume rendering. A hierarchical representation formed by an octree is a data structure to skip over transparent regions while requiring little preprocessing and data storage. However, in order to skip over an octant estimated to be transparent (a transparent octant), the distance from a boundary to another boundary of the octant should be calculated. Because the distance computation is expensive, we propose a precomputed data structure, the distance template, which stores direction and distance values from one boundary voxel on a face to all the boundary voxels on the remaining five faces. In the rendering step, if a ray reaches a transparent octant, it leaps over the octant by referring to the stored distance value.
C1 [Lim, Sukhyun; Shin, Byeong-Seok] Inha Univ, Media Lab, Inchon, South Korea.
C3 Inha University
RP Shin, BS (corresponding author), Inha Univ, Media Lab, 253 Yonghyun Dong, Inchon, South Korea.
EM s.lim@inha.ac.kr; bsshin@inha.ac.kr
CR COHEN D, 1994, VISUAL COMPUT, V11, P27, DOI 10.1007/BF01900824
   COOK RL, 1986, ACM T GRAPHIC, V5, P51, DOI 10.1145/7529.8927
   DANSKIN J, 1992, IEEE VOLUME VISUALIZ, P91
   Engel K, 2006, Real-Time Volume Graphics
   Grimm S, 2004, IEEE SYMPOSIUM ON VOLUME VISUALIZATION AND GRAPHICS 2004, PROCEEDINGS, P1
   Grimm S, 2004, COMPUT GRAPH-UK, V28, P719, DOI 10.1016/j.cag.2004.06.010
   Guthe S, 2001, IEEE VISUAL, P349, DOI 10.1109/VISUAL.2001.964531
   Hadwiger M, 2005, COMPUT GRAPH FORUM, V24, P303, DOI 10.1111/j.1467-8659.2005.00855.x
   HANIES E, 1989, INTRO RAY TRACING
   KNITTEL G, 2000, IEEE ACM SIGGRAP OCT, P71
   KRISHNAMURTHY B, 1999, DATA VISUALIZATION T
   LAUR D, 1991, COMP GRAPH, V25, P285, DOI 10.1145/127719.122748
   LEVOY M, 1990, ACM T GRAPHIC, V9, P245, DOI 10.1145/78964.78965
   LEVOY M, 1988, IEEE COMPUT GRAPH, V8, P29, DOI 10.1109/38.511
   Lim S, 2005, IEICE T INF SYST, VE88D, P2864, DOI 10.1093/ietisy/e88-d.12.2864
   Lim S, 2007, IEICE T INF SYST, VE90D, P1085, DOI 10.1093/ietisy/e90-d.7.1085
   Mora B, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P203, DOI 10.1109/VISUAL.2002.1183776
   Parker S, 1998, VISUALIZATION '98, PROCEEDINGS, P233, DOI 10.1109/VISUAL.1998.745713
   REVELLES J, 2001, P WSCG 2000, P212
   SAITO T, 1994, PATTERN RECOGN, V27, P1551, DOI 10.1016/0031-3203(94)90133-3
   Smits B., 1998, Journal of Graphics Tools, V3, P1, DOI 10.1080/10867651.1998.10487488
   Sramek M, 2000, IEEE T VIS COMPUT GR, V6, P236, DOI 10.1109/2945.879785
   STANDER BT, 1994, IEEE VOLUME VISUALIZ, P107
   Wan M., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P397, DOI 10.1109/VISUAL.1999.809914
   WILHELMS J, 1992, ACM T GRAPHIC, V11, P201, DOI 10.1145/130881.130882
   Yagel R., 1993, Proceedings Visualization '93. (Cat. No.93CH3354-8), P62, DOI 10.1109/VISUAL.1993.398852
   YAGEL R, 1992, IEEE COMPUT GRAPH, V12, P19, DOI 10.1109/38.156009
   ZHANG H, 1997, SIGGRAPH 97 C P ANN, P77
   ZUIDERVELD KJ, 1992, P SOC PHOTO-OPT INS, V1808, P324, DOI 10.1117/12.131088
NR 29
TC 6
Z9 7
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2008
VL 24
IS 4
BP 229
EP 237
DI 10.1007/s00371-007-0203-y
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 277UL
UT WOS:000254240100002
DA 2024-07-18
ER

PT J
AU Livny, Y
   Sokolovsky, N
   Grinshpoun, T
   El-Sana, J
AF Livny, Yotam
   Sokolovsky, Neta
   Grinshpoun, Tal
   El-Sana, Jihad
TI A GPU persistent grid mapping for terrain rendering
SO VISUAL COMPUTER
LA English
DT Article
DE terrain rendering; graphics processors; multiresolution hierarchies
AB In this paper we present persistent grid mapping (PGM), a novel framework for interactive view-dependent terrain rendering. Our algorithm is geared toward high utilization of modern GPUs, and takes advantage of ray tracing and mesh rendering. The algorithm maintains multiple levels of the elevation and color maps to achieve a faithful sampling of the viewed region. The rendered mesh ensures the absence of cracks and degenerate triangles that may cause the appearance of visual artifacts. In addition, an external texture memory support is provided to enable the rendering of terrains that exceed the size of texture memory. Our experimental results show that the PGM algorithm provides high quality images at steady frame rates.
C1 [Livny, Yotam; Sokolovsky, Neta; Grinshpoun, Tal; El-Sana, Jihad] Ben Gurion Univ Negev, Dept Comp Sci, IL-84105 Beer Sheva, Israel.
C3 Ben Gurion University
RP Livny, Y (corresponding author), Ben Gurion Univ Negev, Dept Comp Sci, IL-84105 Beer Sheva, Israel.
EM livnyy@cs.bgu.ac.il; netaso@cs.bgu.ac.il; grinshpo@cs.bgu.ac.il;
   el-sana@cs.bgu.ac.il
RI Grinshpoun, Tal/AGT-1853-2022
OI Grinshpoun, Tal/0000-0002-4106-3169
CR Asirvatham A., 2005, GPU GEMS 2, V2, P27
   Bao X., 2004, P VIS MOD VIS VMV, P413
   BLOW J, 2000, GAM DEV C
   BOLZ J, 2005, EVALUATION SUBDIVISI
   Cignoni P, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P147, DOI 10.1109/VISUAL.2003.1250366
   Cignoni P, 2004, ACM T GRAPHIC, V23, P796, DOI 10.1145/1015706.1015802
   Cignoni P, 2003, COMPUT GRAPH FORUM, V22, P505, DOI 10.1111/1467-8659.00698
   CohenOr D, 1996, IEEE VISUAL, P37, DOI 10.1109/VISUAL.1996.567600
   Dachsbacher C., 2004, EUROGRAPHICS S GEOME, P138
   De Floriani L, 1997, VISUALIZATION '97 - PROCEEDINGS, P103, DOI 10.1109/VISUAL.1997.663865
   DOGGETT M, 2000, HWWS 00, P59, DOI DOI 10.1145/346876.348220
   Döllner J, 2000, IEEE VISUAL, P227
   Duchaineau M, 1997, VISUALIZATION '97 - PROCEEDINGS, P81, DOI 10.1109/VISUAL.1997.663860
   El-Sana J, 1999, COMPUT GRAPH FORUM, V18, pC83, DOI 10.1111/1467-8659.00330
   Evans W, 2001, ALGORITHMICA, V30, P264, DOI 10.1007/s00453-001-0006-x
   GERASIMOV G, 2004, SHADER MODER 3 0 USI
   Gumhold S., 1999, Proceedings 1999 EUROGRAPHICS/SIGGRAPH Workshop on Graphics Hardware, P55, DOI 10.1145/311534.311578
   HITCHNER LE, 1993, P SOC PHOTO-OPT INS, V1913, P622, DOI 10.1117/12.152736
   Hoppe H, 1998, VISUALIZATION '98, PROCEEDINGS, P35, DOI 10.1109/VISUAL.1998.745282
   Hoppe H, 1999, COMP GRAPH, P269, DOI 10.1145/311535.311565
   Hwa LM, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P219, DOI 10.1109/VISUAL.2004.4
   Johanson C., 2004, THESIS LUND U LUND
   Kryachko Y., 2005, GPU GEMS, V2, P283
   Lario Roberto., 2003, P VIIP 2003, P733
   LARSEN BS, 2003, WSCG
   Levenberg J, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P259, DOI 10.1109/VISUAL.2002.1183783
   Lindstrom P, 2002, IEEE T VIS COMPUT GR, V8, P239, DOI 10.1109/TVCG.2002.1021577
   LINDSTROM Peter., 1996, P ACM SIGGRAPH 96, P109
   Losasso F, 2004, ACM T GRAPHIC, V23, P769, DOI 10.1145/1015706.1015799
   LOSASSO F, 2003, P EUR ACM SIGGRAPH S, P138
   Molde K, 2002, PROC GRAPH INTERF, P171
   NVIDIA, 2004, IMPR BATCH US TEXT A
   Pajarola R, 1998, VISUALIZATION '98, PROCEEDINGS, P19, DOI 10.1109/VISUAL.1998.745280
   Pomeranz A.A., 2000, THESIS U CALIFORNIA
   Rabinovich B, 1997, VISUALIZATION '97 - PROCEEDINGS, P95, DOI 10.1109/VISUAL.1997.663863
   SCHNEIDER J, 2006, WSCG, V14, P49
   SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x
   Southern R, 2003, COMPUT GRAPH FORUM, V22, P35, DOI 10.1111/1467-8659.t01-1-00644
   TANNER CHRISTOPHERC., 1998, P SIGGRAPH 1998, P151
   ULRICH T, 2002, P SIGGRAPH 02
   WAGNER D, 2004, TERRAIN GEOMORPHING, P9
   Yoon SE, 2005, IEEE T VIS COMPUT GR, V11, P369, DOI 10.1109/TVCG.2005.64
   [No title captured]
NR 43
TC 21
Z9 34
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2008
VL 24
IS 2
BP 139
EP 153
DI 10.1007/s00371-007-0180-1
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 252ZH
UT WOS:000252486200006
DA 2024-07-18
ER

PT J
AU Fournier, M
   Dischler, JM
   Bechmann, D
AF Fournier, Marc
   Dischler, Jean-Michel
   Bechmann, Dominique
TI A new vector field distance transform and its application to mesh
   processing from 3D scanned data
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE 3D vector field distance transform; volumetric mesh representation;
   marching cube triangulation algorithm; scanned data mesh reconstruction;
   triangle mesh denoising
AB In this paper we define a new 3D vector field distance transform to implicitly represent a mesh surface. We show that this new representation is more accurate than the classic scalar field distance transform by comparing both representations with an error metric evaluation. The widely used marching cube triangulation algorithm is adapted to the new vector field distance transform to correctly reconstruct the resulting explicit surface. In the reconstruction process of 3D scanned data, the useful mesh denoising operation is extended to the new vector field representation, which enables adaptive and selective filtering features. Results show that mesh processing with this new vector field representation is more accurate than with the scalar field distance transform and that it outperforms previous mesh filtering algorithms. Future work is discussed to extend this new vector field representation to other mesh useful operations and applications.
C1 Univ Strasbourg, LSIIT, UMR 7005, Image Sci Comp Sci & Remote Sensing Lab, Strasbourg, France.
C3 Universites de Strasbourg Etablissements Associes; Universite de
   Strasbourg
RP Fournier, M (corresponding author), Univ Strasbourg, LSIIT, UMR 7005, Image Sci Comp Sci & Remote Sensing Lab, Strasbourg, France.
EM fournier@lsiit.u-strasbg.fr; dischler@lsiit.u-strasbg.fr;
   bechmann@lsiit.u-strasbg.fr
CR BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   BOBENKO AI, 2005, 3 EUR S GEOM PROC VI, P101
   COHENOR D, 1998, ACM T GRAPH, V17
   CURLESS B, 1996, COMP GRAPH SIGGR 96, P221
   DAVIS J, 2002, 1 INT S 3D DAT PROC
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   FLEISHMAN S., 2003, ACM T GRAPH, V22
   GELLERT W, 1989, CONCISE ENCY MATH
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   Harvey E, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P31, DOI 10.1109/IM.2001.924390
   Hildebrandt K, 2004, COMPUT GRAPH FORUM, V23, P391, DOI 10.1111/j.1467-8659.2004.00770.x
   Hilton A, 1998, COMPUT VIS IMAGE UND, V69, P273, DOI 10.1006/cviu.1998.0664
   HILTON A, 1996, INT C IM PROC LAUS S, V1
   Hoppe Hugues, 1992, P SIGGRAPH 92, P71, DOI DOI 10.1145/133994.134011
   Kobbelt LP, 2001, COMP GRAPH, P57, DOI 10.1145/383259.383265
   LEVY B, 1999, ISAGOCAD INR LORR CN
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Mao ZH, 2006, VISUAL COMPUT, V22, P276, DOI 10.1007/s00371-006-0005-7
   MASHIKO T, 2004, 4 INT C COMP INF TEC
   Nikolaidis N., 2001, 3D image processing algorithm
   Nooruddin FS, 2003, IEEE T VIS COMPUT GR, V9, P191, DOI 10.1109/TVCG.2003.1196006
   OHTAKE Y, 2002, MESH SMOOTHING ADAPT
   Rocchini C., 2001, EUROGRAPHICS 2001, V20
   RUTISHAUSER M, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P573, DOI 10.1109/CVPR.1994.323797
   Sealy G, 1996, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P184, DOI 10.1109/CGI.1996.511800
   Sethian J., 1999, LEVEL SET METHODS FA
   TAUBIN G, 1995, ACM SIGGRAPH, P351
   Ward GJ., 1994, SIGGRAPH 94 P 21 ANN, DOI DOI 10.1145/192161.192241
   Yagou H, 2002, GEOMETRIC MODELING AND PROCESSING: THEORY AND APPLICATIONS, PROCEEDINGS, P124, DOI 10.1109/GMAP.2002.1027503
   Zhang N, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P505, DOI 10.1109/VISUAL.2004.27
NR 30
TC 4
Z9 4
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 915
EP 924
DI 10.1007/s00371-007-0143-6
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600032
DA 2024-07-18
ER

PT J
AU Chittaro, L
   Ieronutti, L
   Ranon, R
AF Chittaro, Luca
   Ieronutti, Lucio
   Ranon, Roberto
TI Adaptable visual presentation of 2D and 3D learning materials in
   web-based cyberworlds
SO VISUAL COMPUTER
LA English
DT Article
DE educational cyberworlds; visual presentation techniques; virtual humans
AB Educational cyberworlds allow one to circumvent physical, safety, and cost constraints that often affect real-world training and learning scenarios, allowing students to study the learning content flexibly and individually. However, this does not eliminate the need for presenting the educational materials in an effective way, using visualization, interaction and presentation techniques that are better suited to the learner's preferences, abilities, and the available devices. The system that we present in this paper is capable, starting from a set of available educational resources (e.g., text, pictures, movies and 3D models) and lesson structures (e.g., suitable ordering of topics), to automatically build a set of alternative web-based educational cyberworlds characterized by different visualization (2D and 3D), interaction and presentation techniques, including virtual humans as instructors. The learner (or her instructor) can then choose the cyberworld that best fits her preferences and needs.
C1 Univ Udine, HCI Lab, Dept Math & Comp Sci, I-33100 Udine, Italy.
C3 University of Udine
RP Chittaro, L (corresponding author), Univ Udine, HCI Lab, Dept Math & Comp Sci, Via Sci 206, I-33100 Udine, Italy.
EM chittaro@dimi.uniud.it; ieronutt@dimi.uniud.it; ranon@dimi.uniud.it
RI Ranon, Roberto/D-6145-2018
OI CHITTARO, Luca/0000-0001-5975-4294
CR ANIM H, 2005, 19774 ISOIEC FCD
   [Anonymous], 2000, INSTRUCTIONAL USE LE
   [Anonymous], 1999, Proceedings of the SIGCHI conference on human factors in computing systems, DOI [10.1145/302979.303150, DOI 10.1145/302979.303150]
   Atkinson RK, 2002, J EDUC PSYCHOL, V94, P416, DOI 10.1037//0022-0663.94.2.416
   Bandura A., 1977, Social Learning Theory
   Cassell J, 1999, APPL ARTIF INTELL, V13, P519, DOI 10.1080/088395199117360
   Chittaro L., 2004, The working conference on Advanced visual interfaces, P40, DOI DOI 10.1145/989863
   CHITTARO L, 2006, IN PRESS COMPUT ED
   CHITTARO L, 2006, IN PRESS IEEE T VISU
   Chung SK, 1999, COMP ANIM CONF PROC, P4, DOI 10.1109/CA.1999.781194
   CUGINI J, 2002, P DAT EXP SYST APPL, P259
   Ekman P., 1982, EMOTION HUMAN FACE
   Findlater Leah., 2004, P SIGCHI C HUMAN FAC, P89, DOI [DOI 10.1145/985692.985704, 10.1145/985692.985704]
   Hong JasonI., 2001, P 10 INT C WORLD WID, P717
   Huang Zhiyong, 1995, COMPUT GRAPHICS-US, P235
   Ieronutti L., 2004, P 9 INT C 3D WEB TEC, P61
   IERONUTTI L, 2005, P 10 IFIP INT C HUM, P873
   Ieronutti Lucio, 2005, WEB3D S P, P75, DOI [10.1145/1050491.1050502, DOI 10.1145/1050491.1050502]
   *ISO IEC FDIS, 2004, 19775 ISOIEC FDIS
   Kayssi A, 1999, COMPUT APPL ENG EDUC, V7, P1, DOI 10.1002/(SICI)1099-0542(1999)7:1<1::AID-CAE1>3.0.CO;2-2
   Lester J. C., 1997, Proceedings of the First International Conference on Autonomous Agents, P16, DOI 10.1145/267658.269943
   Lester J. C., 1997, P ACM SIGCHI C HUM F, P359, DOI [10.1145/258549.258797, DOI 10.1145/258549.258797]
   Lin BS, 2001, COMPUT EDUC, V37, P377, DOI 10.1016/S0360-1315(01)00060-4
   Mayer RE, 2003, J EDUC PSYCHOL, V95, P419, DOI 10.1037/0022-0663.95.2.419
   Moreno R, 2004, J EDUC PSYCHOL, V96, P165, DOI 10.1037/0022-0663.96.1.165
   Moreno R, 2001, COGNITION INSTRUCT, V19, P177, DOI 10.1207/S1532690XCI1902_02
   Norman D., 1997, Software agents, P49
   Shneiderman Ben, 1997, Interactions, V4, P42, DOI DOI 10.1145/267505.267514
   Sproull L, 1996, HUM-COMPUT INTERACT, V11, P97, DOI 10.1207/s15327051hci1102_1
   Takeuchi A., 1995, Human Factors in Computing Systems. CHI'95 Conference Proceedings, P450
   VANMULKEN S, 1998, P CHI 98, P56
   WELD DS, 2003, P 18 INT JOINT C ART, P1613
NR 32
TC 2
Z9 4
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2006
VL 22
IS 12
BP 1002
EP 1014
DI 10.1007/s00371-006-0038-y
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 120HE
UT WOS:000243074300006
DA 2024-07-18
ER

PT J
AU Liu, Q
   Sourin, A
AF Liu, Qi
   Sourin, Alexei
TI Function-defined shape metamorphoses in visual cyberworlds
SO VISUAL COMPUTER
LA English
DT Article
DE function-based shape modeling; Computer animation; 3D web visualization;
   VRML; X3D
AB Animated shape transformations should be an intrinsic part of visual cyberworlds. However, quite often only limited animation of the polygon-based shapes can be found there, specifically when using the virtual reality modeling language (VRML) and its successor extensible 3D (X3D). This greatly limits the expressive power of visual cyberworlds and has motivated our research in this direction. In this paper, we present function-based extensions of VRML and X3D, which allow for time-dependent shape modeling on the web. Our shape modeling approach is based on the concurrent use of implicit, explicit and parametric functions defining geometry, appearance and their transformations through time. The functions are typed straight in VRML/X3D code as individual formulas and as function scripts. We have also developed a web enabled interactive software tool for modeling function-based VRML/X3D objects.
C1 Nanyang Technol Univ, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Liu, Q (corresponding author), Nanyang Technol Univ, Nanyang Ave, Singapore 639798, Singapore.
EM liuqi@pmail.ntu.edu.sg; assourin@ntu.edu.sg
RI Sourin, Alexei/A-3701-2011
OI Sourin, Alexei/0000-0003-4051-2927
CR Alexa M., 2000, Proceedings Web3D - VRML 2000. Fifth Symposium on the Virtual Reality Modeling Language, P29, DOI 10.1145/330160.330172
   ALEXA M, 1999, P WSCG 99, P329
   Breton G., 2001, P 6 INT C 3D WEB TEC, P15
   Davies R., 2005, Proceedings of the tenth international conference on 3D Web technology, P143
   ESULI A, 2003, P ACM WEB3D 03, P147
   Krall A., 1998, P INT C PAR ARCH COM, P54
   LAI FM, 2002, EUROGRAPHICS 2002 SH, P207
   Levinski K, 2004, 2004 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P54, DOI 10.1109/CW.2004.41
   Liu Q., 2005, P ACM WEB3D 2005, P123
   Liu Q, 2006, COMPUT GRAPH-UK, V30, P629, DOI 10.1016/j.cag.2006.03.006
   Liu Y, 2003, MLMTA'03: INTERNATIONAL CONFERENCE ON MACHINE LEARNING; MODELS, TECHNOLOGIES AND APPLICATIONS, P129
   Naka T., 1999, Proceedings VRML 99. Fourth Symposium on the Virtual Reality Modeling Language, P63, DOI 10.1145/299246.299264
   Pandzic IgorS., 2002, Web3D'02: Proceedings of the seventh international conference on 3D Web technology, P27
   PASKO A, 1995, VISUAL COMPUT, V11, P429, DOI 10.1007/BF02464333
   ROBERTS JC, 1998, P VRSIG 98 EX U UK, P12
NR 15
TC 15
Z9 15
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2006
VL 22
IS 12
BP 977
EP 990
DI 10.1007/s00371-006-0044-0
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 120HE
UT WOS:000243074300004
DA 2024-07-18
ER

PT J
AU Caspi, Y
   Axelrod, A
   Matsushita, Y
   Gamliel, A
AF Caspi, Yaron
   Axelrod, Anat
   Matsushita, Yasuyuki
   Gamliel, Alon
TI Dynamic stills and clip trailers
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE video summaries; key pose selection
ID DIMENSIONALITY REDUCTION
AB We propose a method for generating visual summaries of video. It reduces browsing time, minimizes screen-space utilization, while preserving the crux of the video content and the sensation of motion. The outputs are images or short clips, denoted as dynamic stills or clip trailers, respectively. The method selects informative poses out of extracted video objects. Optimal rotations and transparency supports visualization of an increased number of poses, leading to concise activity visualization. Our method addresses previously avoided scenarios, e.g., activities occurring in one place, or scenes with non-static background. We demonstrate and evaluate the method for various types of videos.
C1 Weizmann Inst Sci, Fac Math & Comp Sci, IL-76100 Rehovot, Israel.
   Tel Aviv Univ, Sch Comp Sci, IL-69978 Tel Aviv, Israel.
   Microsoft Res Asia, Visual Comp Grp, Beijing, Peoples R China.
C3 Weizmann Institute of Science; Tel Aviv University; Microsoft Research
   Asia; Microsoft
RP Caspi, Y (corresponding author), Weizmann Inst Sci, Fac Math & Comp Sci, IL-76100 Rehovot, Israel.
EM yaron.caspi@weizmann.ac.il; anataxel@post.tau.ac.il;
   yasumat@microsoft.com; gamliela@post.tau.ac.il
RI Caspi, Yaron/O-2518-2016
OI Matsushita, Yasuyuki/0000-0002-1935-4752
CR ABDI H, 2002, EMPIRICAL EVALUATION, P42
   Agarwala A, 2005, ACM T GRAPHIC, V24, P821, DOI 10.1145/1073204.1073268
   Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718
   [Anonymous], 2006, CVPR, DOI DOI 10.1109/CVPR.2006.69
   Assa J, 2005, ACM T GRAPHIC, V24, P667, DOI 10.1145/1073204.1073246
   AXELROD A, 2006, THESIS TEL AVIV U
   AXELROD A, 2006, SIGGRAPH SKETCHES
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   BenAbdelkader C, 2004, EURASIP J APPL SIG P, V2004, P572, DOI 10.1155/S1110865704309236
   CASSINELLI A, 2005, SIGGRAPH EMERGING TE
   Chiu P, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P2059, DOI 10.1109/ICME.2004.1394670
   *CMU, 2002, MOT CAPT DAT
   *CNN, 2004, CABL NEWS NETW
   *GOOGL, 2005, GOOGL VID SHAR SERV
   IRANI M, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P605, DOI 10.1109/ICCV.1995.466883
   KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565
   Li Y, 2005, ACM T GRAPHIC, V24, P595, DOI 10.1145/1073204.1073234
   Liu F, 2003, COMPUT VIS IMAGE UND, V92, P265, DOI 10.1016/j.cviu.2003.06.001
   LOY G, 2003, WORKSH HIGH LEV KNOW, P66
   Massey M, 1996, IBM SYST J, V35, P557, DOI 10.1147/sj.353.0557
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Rav-Acha A, 2005, PROC CVPR IEEE, P58
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Sun J, 2004, ACM T GRAPHIC, V23, P315, DOI 10.1145/1015706.1015721
   SUN J, 2006, ECCV, P628
   Szeliski R, 1997, P 24 ANN C COMP GRAP, P251
   Taniguchi Y, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P427
   Wang J, 2005, ACM T GRAPHIC, V24, P585, DOI 10.1145/1073204.1073233
NR 28
TC 22
Z9 28
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 642
EP 652
DI 10.1007/s00371-006-0046-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000007
DA 2024-07-18
ER

PT J
AU Diaz-Gutierrez, P
   Bhushan, A
   Gopi, M
   Pajarola, R
AF Diaz-Gutierrez, Pablo
   Bhushan, Anusheel
   Gopi, M.
   Pajarola, Renato
TI Single-strips for fast interactive rendering
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference (CGI 2005)
CY JUN 22-24, 2005
CL Stony Brook, NY
SP IEEE Comp Soc, VGTC, ACM SIGGRAPH
DE single-strip; weighted perfect matching; Hamiltonian cycle; vertex
   cache; visibility culling
AB Representing a triangulated two manifold using a single triangle strip is an NP-complete problem. By introducing a few Steiner vertices, recent works find such a single-strip, and hence a linear ordering of edge-connected triangles of the entire triangulation. In this paper, we extend previous results [10] that exploit this linear ordering in efficient triangle-strip management for high-performance rendering. We present new algorithms to generate single-strip representations that follow different user defined constraints or preferences in the form of edge weights. These functional constraints are application dependent. For example, normal-based constraints can be used for efficient rendering after visibility culling, or spatial constraints for highly coherent vertex-caching. We highlight the flexibility of this approach by generating single-strips with preferences as arbitrary as the orientation of the edges. We also present a hierarchical single-strip management strategy for high-performance interactive 3D rendering.
C1 Univ Calif Irvine, Irvine, CA 92697 USA.
   Univ Zurich, Dept Informat, CH-8057 Zurich, Switzerland.
C3 University of California System; University of California Irvine;
   University of Zurich
RP Diaz-Gutierrez, P (corresponding author), Univ Calif Irvine, 444 Comp Sci Bldg, Irvine, CA 92697 USA.
EM pablo@ics.uci.edu; anusheel@ics.uci.edu; gopi@ics.uci.edu;
   pajarola@acm.org
OI Pajarola, Renato/0000-0002-6724-526X
CR [Anonymous], 1891, Acta Mathematica
   Cook W, 1999, INFORMS J COMPUT, V11, P138, DOI 10.1287/ijoc.11.2.138
NR 2
TC 11
Z9 16
U1 2
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2006
VL 22
IS 6
BP 372
EP 386
DI 10.1007/s00371-006-0018-2
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 053BL
UT WOS:000238278500002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Max, N
AF Max, N
TI Progress in scientific visualization
SO VISUAL COMPUTER
LA English
DT Article
DE molecular visualization; volume visualization; flow visualization
ID REPRESENTATION
AB This paper surveys recent work in the visualization of molecules, scalar fields, and vector fields.
C1 Lawrence Livermore Natl Lab, Livermore, CA 94550 USA.
C3 United States Department of Energy (DOE); Lawrence Livermore National
   Laboratory
RP Lawrence Livermore Natl Lab, POB 808,Mail Stop L-560, Livermore, CA 94550 USA.
EM max2@llnl.gov
CR [Anonymous], [No title captured], DOI DOI 10.1145/99308.99316
   BANKS DC, 1995, IEEE T VIS COMPUT GR, V1, P151, DOI 10.1109/2945.468404
   Bertram M, 2001, IEEE VISUAL, P303, DOI 10.1109/VISUAL.2001.964525
   Cabral B., 1993, P 20 ANN C COMP GRAP, P263, DOI DOI 10.1145/166117.166151
   CONNOLLY ML, 1983, J APPL CRYSTALLOGR, V16, P548, DOI 10.1107/S0021889883010985
   Drebin R. A., 1988, Computer Graphics, V22, P65, DOI 10.1145/378456.378484
   Gillet A, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P235, DOI 10.1109/VISUAL.2004.7
   HELMAN JL, 1991, IEEE COMPUT GRAPH, V11, P36, DOI 10.1109/38.79452
   Kreylos O, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P581, DOI 10.1109/VISUAL.2003.1250423
   LEVINTHAL C, 1966, SCI AM, V214, P42, DOI 10.1038/scientificamerican0666-42
   LEVOY M, 1988, IEEE COMPUT GRAPH, V8, P29, DOI 10.1109/38.511
   Lorensen W. E., 1987, COMPUTER GRAPHICS, V21, P163, DOI 10.1145/37401.37422
   Max N, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P210, DOI 10.1109/CGI.2003.1214468
   MAX N, 1995, IEEE T VIS COMPUT GR, V1, P99, DOI 10.1109/2945.468400
   MAX NL, 1983, IEEE COMPUT GRAPH, V3, P21, DOI 10.1109/MCG.1983.263183
   Nielson G. M., 1991, Proceedings Visualization '91 (Cat. No.91CH3046-0), P83, DOI 10.1109/VISUAL.1991.175782
   Nielson GM, 2003, IEEE T VIS COMPUT GR, V9, P283, DOI 10.1109/TVCG.2003.1207437
   Sabella P., 1988, Computer Graphics, V22, P51, DOI 10.1145/378456.378476
   SCHUSSMAN G, 2003, THESIS U CALIFORNIA
   Theisel H, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P225, DOI 10.1109/VISUAL.2003.1250376
   Turk G., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P453, DOI 10.1145/237170.237285
   Upson C., 1988, Computer Graphics, V22, P59, DOI 10.1145/378456.378482
   Weiler M, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P333, DOI 10.1109/VISUAL.2003.1250390
   Wyvill G., 1986, Visual Computer, V2, P227, DOI 10.1007/BF01900346
NR 24
TC 10
Z9 11
U1 1
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2005
VL 21
IS 12
BP 979
EP 984
DI 10.1007/s00371-005-0361-8
PG 6
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 996AB
UT WOS:000234145700004
DA 2024-07-18
ER

PT J
AU Desbenoit, B
   Galin, E
   Akkouche, S
AF Desbenoit, B
   Galin, E
   Akkouche, S
TI Modeling cracks and fractures
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE modeling; fractures; cracks
ID SIMULATION
AB This paper presents an interactive method for modeling cracks and fractures over a variety of materials such as glass, metal, wood, and stone. Existing physically based techniques are computationally demanding and lack control over the fracture propagation. Our approach consists in editing 2D fracture pattern and profile curves which are stored in an atlas according to material type. The fracture model is then automatically mapped onto the surface of the object and fractures are created by carving out a procedurally generated swept volume. Because the objects need not be voxelized or tetrahedralized as with physically based techniques, we are not limited in resolution when creating the geometry of cracks, which enables us to model small or very thin fractures.
C1 Univ Lyon 1, CNRS, LIRIS, F-69365 Lyon, France.
C3 Institut National des Sciences Appliquees de Lyon - INSA Lyon;
   Universite Claude Bernard Lyon 1; Centre National de la Recherche
   Scientifique (CNRS)
RP Univ Lyon 1, CNRS, LIRIS, F-69365 Lyon, France.
EM brett.desbenoit@liris.cnrs.fr; eric.galin@liris.cnrs.fr;
   samir.akkouche@liris.cnrs.fr
RI Galin, Eric/X-1938-2019
OI Galin, Eric/0000-0002-5946-4112
CR Bosch C, 2004, COMPUT GRAPH FORUM, V23, P361, DOI 10.1111/j.1467-8659.2004.00767.x
   BOSCH C, 2001, VISUAL COMPUT, V17, P30
   Cutler B, 2002, ACM T GRAPHIC, V21, P302
   DESBENOIT B, 2004, EUROGRAPHICS SHORT P, P37
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Dorsey J, 1999, COMP GRAPH, P225, DOI 10.1145/311535.311560
   Dorsey Julie., 1996, Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques, P411, DOI [10.1145/237170. 237280, DOI 10.1145/237170.237280]
   Federl P, 2004, LECT NOTES COMPUT SC, V3037, P138
   Federl P., 1996, Proceedings of Western Computer Graphics Symposium, P23
   Gobron S, 2001, VISUAL COMPUT, V17, P287, DOI 10.1007/s003710100099
   Hirota K, 1998, VISUAL COMPUT, V14, P126, DOI 10.1007/s003710050128
   Hirota K, 2000, VISUAL COMPUT, V16, P371, DOI 10.1007/s003710000069
   Jones TR, 2003, ACM T GRAPHIC, V22, P943, DOI 10.1145/882262.882367
   LEFEBVRE S, 2002, EUR WORKSH REND, P105
   Martinet A, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P346, DOI 10.1109/SMI.2004.1314524
   Mazarak O, 1999, PROC GRAPH INTERF, P211
   Müller M, 2001, SPRING EUROGRAP, P113
   Neff M, 1999, PROC GRAPH INTERF, P193
   Norton A., 1991, Visual Computer, V7, P210, DOI 10.1007/BF01900837
   O'Brien JF, 2002, ACM T GRAPHIC, V21, P291, DOI 10.1145/566570.566579
   O'Brien JF, 1999, COMP GRAPH, P137, DOI 10.1145/311535.311550
   Smith J, 2000, PROC GRAPH INTERF, P27
   Terzopoulos D., 1988, Proceedings of the 15th annual conference on Computer graphics and interactive techniques, P269
   Wang X, 2003, POWER SYST, P83
NR 24
TC 32
Z9 36
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 717
EP 726
DI 10.1007/s00371-005-0317-z
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400024
DA 2024-07-18
ER

PT J
AU Duan, Y
   Hua, J
   Qin, H
AF Duan, Y
   Hua, J
   Qin, H
TI Interactive shape modeling using Lagrangian surface flow
SO VISUAL COMPUTER
LA English
DT Article
DE shape modeling; partial differential equations (PDEs); geometric surface
   flow; distance field
AB In this paper, we propose a new shape-modeling paradigm based on the concept of Lagrangian surface flow. Given an input polygonal model, the user interactively defines a distance field around regions of interest; the locally or globally affected regions will then automatically deform according to the user-defined distance field. During the deformation process, the model can always maintain its regularity and can properly modify its topology by topology merging when collisions between two different parts of the model occur. Comparing with level-set based methods, our algorithm allows the user to work directly on existing polygonal models without any intermediate model conversion. Besides closed polygonal models, our algorithm also works for mesh models with open boundaries. Within our framework, we developed a number of shape-modeling operators including blending, cutting, drilling, free-hand sketching, and mesh warping. We applied our algorithm to a variety of examples that demonstrate the usefulness and efficacy of the new technique in interactive shape design and surface deformation.
C1 Univ Missouri, Dept Comp Sci, Columbia, MO 65203 USA.
   Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA.
   SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 University of Missouri System; University of Missouri Columbia; Wayne
   State University; State University of New York (SUNY) System; State
   University of New York (SUNY) Stony Brook
RP Univ Missouri, Dept Comp Sci, Columbia, MO 65203 USA.
EM duanye@missouri.edu; jinghua@cs.wayne.edu; qin@cs.sunysb.edu
FU Div Of Information & Intelligent Systems; Direct For Computer & Info
   Scie & Enginr [0830183] Funding Source: National Science Foundation
CR Breen DE, 2001, IEEE T VIS COMPUT GR, V7, P173, DOI 10.1109/2945.928169
   COHEROR D, 1997, ACM T GRAPH
   CRESPIN B, 1999, IMPL SURF 99 WORKSH, P17
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Frisken SF, 2000, COMP GRAPH, P249, DOI 10.1145/344779.344899
   Hoppe H., 1993, Computer Graphics Proceedings, P19, DOI 10.1145/166117.166119
   Hua J, 2004, VISUAL COMPUT, V20, P47, DOI 10.1007/s00371-003-0225-z
   Igarashi T, 1999, COMP GRAPH, P409, DOI 10.1145/311535.311602
   Kobbelt L., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P105, DOI 10.1145/280814.280831
   KOBBELT L, 2000, EUROGRAPHICS, P249
   Loop C, 1987, THESIS U UTAH
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Markosian L, 1999, COMP GRAPH, P393, DOI 10.1145/311535.311595
   OHATAKE Y, 2001, P SHAP MOD INT, P74
   OHATAKE Y, 2001, P EUR, P368
   PAYNE BA, 1992, IEEE COMPUT GRAPH, V12, P65, DOI 10.1109/38.135885
   Perry RN, 2001, COMP GRAPH, P47, DOI 10.1145/383259.383264
   Schneider R, 2001, COMPUT AIDED GEOM D, V18, P359, DOI 10.1016/S0167-8396(01)00036-X
   Schroeder W. J., 1994, Proceedings. Visualization '94 (Cat. No.94CH35707), P40, DOI 10.1109/VISUAL.1994.346339
   SHAPIRO V, 1999, P 5 ACM S SOL MOD AP
   SINGH K, 1995, IMPLICIT SURFACES 95, P113
   TAUBIN G, 1995, SIGGRAPH 95 C P, P351, DOI [DOI 10.1145/218380.218473, 10.1145/218380.218473]
   WELCH W, 1994, SIGGRAPH 94 C P, P247
   [No title captured]
NR 24
TC 12
Z9 14
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2005
VL 21
IS 5
BP 279
EP 288
DI 10.1007/s00371-005-0282-6
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 937OJ
UT WOS:000229935100001
DA 2024-07-18
ER

PT J
AU Goshtasby, AA
AF Goshtasby, AA
TI A weighted mean approach to smooth parametric representation of polygon
   meshes
SO VISUAL COMPUTER
LA English
DT Article
DE irregular mesh; weighted mean; multiresolution representation; free-form
   parametric surface; rational Gaussian surface
ID B-SPLINE SURFACES; SUBDIVISION SCHEME; INTERPOLATION
AB A method for representing genus-zero polygon meshes by smooth parametric surfaces is described. A surface is defined by a weighted sum of linear functions, each describing a polygon face in parametric form. Rational Gaussian blending functions that adapt to the size and shape of mesh faces are used as the weights. The proposed representation has a very high degree of continuity everywhere and provides a smoothness parameter that can be varied to produce surfaces at varying resolutions. It is shown that the representation facilitates geometry processing of meshes. The use of locally supported weight functions as an alternative to rational Gaussian weights is also discussed.
C1 Wright State Univ, Dept Comp Sci & Engn, Dayton, OH 45435 USA.
C3 University System of Ohio; Wright State University Dayton
RP Wright State Univ, Dept Comp Sci & Engn, Dayton, OH 45435 USA.
EM agoshtas@cs.wright.edu
CR [Anonymous], P 28 ANN C COMP GRAP, DOI DOI 10.1145/383259.383266
   [Anonymous], 1987, Algorithms for Approximation
   BABAUD J, 1986, IEEE T PATTERN ANAL, V8, P26, DOI 10.1109/TPAMI.1986.4767749
   BESL PJ, 1990, ANAL INTERPRETATION, P141
   Blinn J. F., 1982, Computer Graphics, V16, DOI 10.1145/965145.801290
   CASTLEMAN K, 1996, DIGITAL IMAGE PROCES, P173
   CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   Catmull E, 1974, inCom-puter Aided Geometric Design, P317, DOI [DOI 10.1016/B978-0-12-079050-0.50020-5, 10.1016/B978-0-12-079050-0.50020-5]
   CHANG SC, 1997, MED J CMCH, V2, P6
   Chui CK, 2000, COMPUT AIDED GEOM D, V17, P297, DOI 10.1016/S0167-8396(00)00005-4
   CLARK JJ, 1989, IEEE T PATTERN ANAL, V11, P43, DOI 10.1109/34.23112
   Clough R, 1965, P C MATRIX METHODS S, P515
   Cohen-Or D., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P67
   Costantini P, 1996, J COMPUT APPL MATH, V73, P45, DOI 10.1016/0377-0427(96)00033-7
   Costantini P, 1999, COMPUT AIDED GEOM D, V16, P385, DOI 10.1016/S0167-8396(99)00007-2
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   DeRose T., 1998, Computer Graphics (SIGGRAPH 98 Proceedings), P85, DOI DOI 10.1145/280814.280826
   Doo D. W. H., 1978, Proceeding of the International Conference Interactive Techniques in Computer Aided Design, P157
   DYN N, 1990, ACM T GRAPHIC, V9, P160, DOI 10.1145/78956.78958
   Eck M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P325, DOI 10.1145/237170.237271
   FARIN G, 1986, COMPUT AIDED GEOM D, V3, P47
   Floater MS, 1997, COMPUT AIDED GEOM D, V14, P231, DOI 10.1016/S0167-8396(96)00031-3
   Floater MS, 2003, COMPUT AIDED GEOM D, V20, P19, DOI 10.1016/S0167-8396(03)00002-5
   FRANKE R, 1980, INT J NUMER METH ENG, V15, P1691, DOI 10.1002/nme.1620151110
   GOSHTASBY A, 1995, COMPUT AIDED DESIGN, V27, P363, DOI 10.1016/0010-4485(95)96800-2
   Grimm C., 1995, PROC SIGGRAPH 1995, P359
   GU X, 2002, P SIGGRAPH, P95
   Guskov I, 2000, COMP GRAPH, P95, DOI 10.1145/344779.344831
   Guskov I, 1999, COMP GRAPH, P325, DOI 10.1145/311535.311577
   Hoppe H, 1998, COMPUT GRAPH-UK, V22, P27, DOI 10.1016/S0097-8493(97)00081-2
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Khodakovsky A, 2000, COMP GRAPH, P271, DOI 10.1145/344779.344922
   KHODAKOVSKY A, 2003, P SIGGRAPH
   Kobbelt L, 1996, COMPUT AIDED GEOM D, V13, P743, DOI 10.1016/0167-8396(96)00007-6
   Kobbelt L P., 1999, EUROGRAPHICS, V18
   Kobbelt LP, 2000, VISUAL COMPUT, V16, P142, DOI 10.1007/s003710050204
   Lee A, 2000, COMP GRAPH, P85, DOI 10.1145/344779.344829
   Lee A. W., 1998, Proceedings of the 25th annual conference on Computer graphics and interactive techniques, P95, DOI DOI 10.1145/280814.280828
   Lévy B, 2002, ACM T GRAPHIC, V21, P362, DOI 10.1145/566570.566590
   MAILLOT J, 2001, EUROGRAPHICS, V20
   MAUDE AD, 1973, COMPUT J, V16, P64, DOI 10.1093/comjnl/16.1.64
   Morse BS, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P89, DOI 10.1109/SMA.2001.923379
   Mortenson ME., 1997, Geometric modeling, V2nd
   NASRI A, 2000, COMPUT AIDED GEOM DI, V17, P643
   NAVAU J, 1991, COMPUT AIDED GEOM D, V8, P89
   OHTAKE Y, 2003, P SIGGRAPH
   Peters J, 1997, ACM T GRAPHIC, V16, P420, DOI 10.1145/263834.263851
   PETERS J, 1993, COMPUT AIDED GEOM D, V10, P347, DOI 10.1016/0167-8396(93)90046-6
   PRAUN E, 2003, P SIGGRAPH
   Schmidt JW, 2000, Z ANGEW MATH MECH, V80, P27
   Sheffer A, 2001, ENG COMPUT-GERMANY, V17, P326, DOI 10.1007/PL00013391
   Shepard D., 1968, P 1968 23 ACM NAT C, P517, DOI DOI 10.1145/800186.810616
   Shirman L. A., 1990, Computer-Aided Geometric Design, V7, P375, DOI 10.1016/0167-8396(90)90001-8
   Shroder P, 1998, SIGGRAPH
   Stam J, 2001, COMPUT AIDED GEOM D, V18, P383, DOI 10.1016/S0167-8396(01)00038-3
   Turk G, 2002, ACM T GRAPHIC, V21, P855, DOI 10.1145/571647.571650
   Wendland H., 1995, Advances in Computational Mathematics, V4, P389, DOI 10.1007/BF02123482
   Wood ZJ, 2000, IEEE VISUAL, P275, DOI 10.1109/VISUAL.2000.885705
   YUILLE AL, 1986, IEEE T PATTERN ANAL, V8, P15, DOI 10.1109/TPAMI.1986.4767748
   Zhao HK, 2001, IEEE WORKSHOP ON VARIATIONAL AND LEVEL SET METHODS IN COMPUTER VISION, PROCEEDINGS, P194, DOI 10.1109/VLSM.2001.938900
   Zorin D, 2001, COMPUT AIDED GEOM D, V18, P429, DOI 10.1016/S0167-8396(01)00040-1
   ZORIN D, 1997, P SIGGRAPH 97, P259
NR 62
TC 2
Z9 2
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2004
VL 20
IS 5
BP 344
EP 359
DI 10.1007/s00371-004-0247-1
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 842GL
UT WOS:000222991600005
DA 2024-07-18
ER

PT J
AU Schein, S
   Elber, G
AF Schein, S
   Elber, G
TI Adaptive extraction and visualization of silhouette curves from
   volumetric datasets
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 4th Israel-Korea Bi-National Conference on Geometric Modeling and
   Computer Graphics
CY FEB 12-14, 2003
CL Tel Aviv Univ, Tel Aviv, ISRAEL
HO Tel Aviv Univ
DE NPR; volume rendering; silhouette extraction; surface and volume
   modeling; visualization and rendering
AB An algorithm for silhouette extraction from volumetric data is presented. Trivariate tensor product B-spline functions are used to represent the data. An offline phase that arranges the data in a lookup table is employed to improve the computation time during an interactive session. A subdivision scheme is employed to extract the silhouette curves from an implicit trivariate B-spline function. The produced results are smooth, highquality silhouette curves when compared to voxel-based silhouette extraction schemes.
C1 Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.
C3 Technion Israel Institute of Technology
RP Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.
CR [Anonymous], P ANN C COMP GRAPH I, DOI DOI 10.1145/54852.378484
   BAREQUET G, 1999, OPTIMAL BOUNDING CON
   Benichou F., 1999, Proceedings. Seventh Pacific Conference on Computer Graphics and Applications (Cat. No.PR00293), P60, DOI 10.1109/PCCGA.1999.803349
   BUCHANAN JW, 2000, P 1 INT S NONPH AN R, P39
   Cohen E., 2001, Geometric modeling with splines: an introduction, V1st
   Csébfalvi B, 2001, COMPUT GRAPH FORUM, V20, pC452, DOI 10.1111/1467-8659.00538
   ELBER G, 1999, EUROGRAPHICS, V18, P1
   Elber G., 2001, P 6 ACM S SOLID MODE, P1, DOI [10.1145/376957.376958, DOI 10.1145/376957.376958]
   *FLTK, 2002, FAST LIGHT TOOL KIT
   GOOCH A, 1998, P 25 ANN C COMP GRAP, P447, DOI DOI 10.1145/280814.280950
   GOOCH A, 1999, P ACM S INT 3D GRAPH, P31
   INTERRANTE V, 1997, P SIGGRAPH 97, P109
   LEVOY M, 1988, IEEE COMPUT GRAPH, V8, P29, DOI 10.1109/38.511
   Lu AD, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P211, DOI 10.1109/VISUAL.2002.1183777
   MARKOSIAN L, 1997, P SIGGRAPH 97, P415
   Raviv A, 2001, IEEE T VIS COMPUT GR, V7, P109, DOI 10.1109/2945.928164
   Rheingans P, 2001, IEEE T VIS COMPUT GR, V7, P253, DOI 10.1109/2945.942693
   Sander PV, 2000, COMP GRAPH, P327, DOI 10.1145/344779.344935
   SCHREDER W, 1998, VISUALIZATION TOOLKI
   Treavett SMF, 2001, COMPUTER GRAPHICS INTERNATIONAL 2001, PROCEEDINGS, P99, DOI 10.1109/CGI.2001.934663
   WINKENBACH G, 1996, P SIGGRAPH 96, P469
   2000, IRIT SOLID MODELING
NR 22
TC 8
Z9 10
U1 1
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2004
VL 20
IS 4
BP 243
EP 252
DI 10.1007/s00371-003-0230-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 834XR
UT WOS:000222444700004
DA 2024-07-18
ER

PT J
AU Liu, YJ
   Yuen, MMF
AF Liu, YJ
   Yuen, MMF
TI Optimized triangle mesh reconstruction from unstructured points
SO VISUAL COMPUTER
LA English
DT Article
DE geometric modeling; three-dimensional shape recovery; mesh optimization;
   mesh simplification
AB A variety of approaches have been proposed for polygon mesh reconstruction from a set of unstructured sample points. Suffering from severe aliases at sharp features and having a large number of unnecessary faces, most resulting meshes need to be optimized using input sample points in a postprocess. In this paper, we propose a fast algorithm to reconstruct high-quality meshes from sample data. The core of our proposed algorithm is a new mesh evaluation criterion which takes full advantage of the relation between the sample points and the reconstructed mesh. Based on our proposed evaluation criterion, we develop necessary operations to efficiently incorporate the functions of data preprocessing, isosurface polygonization, mesh optimization and mesh simplification into one simple algorithm, which can generate high-quality meshes from unstructured point clouds with time and space efficiency.
C1 Hong Kong Univ Sci & Technol, Dept Mech Engn, Kowloon, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology
RP Hong Kong Univ Sci & Technol, Dept Mech Engn, Kowloon, Hong Kong, Peoples R China.
EM liuyj@ust.hk; meymf@ust.hk
RI Yuen, Matthew M.F./E-5621-2011
CR Amenta N., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P415, DOI 10.1145/280814.280947
   [Anonymous], 1997, Introduction to Implicit Surfaces
   Bardinet E, 1998, COMPUT VIS IMAGE UND, V71, P39, DOI 10.1006/cviu.1997.0595
   Berg M.d., 1997, COMPUTATIONAL GEOMET
   Ciampalini A, 1997, VISUAL COMPUT, V13, P228, DOI 10.1007/s003710050101
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Cohen J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P119, DOI 10.1145/237170.237220
   Cormen T.H., 1997, Introduction to Algorithms
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   Eck M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P325, DOI 10.1145/237170.237271
   Frisken SF, 2000, COMP GRAPH, P249, DOI 10.1145/344779.344899
   Garland M., 1997, PROC 24 C COMPUTER G, P209, DOI DOI 10.1145/258734.258849
   Gopi M, 2000, COMPUT GRAPH FORUM, V19, pC467, DOI 10.1111/1467-8659.00439
   GUSKOV I, 2001, P GRAPH INT, P19
   He TS, 1996, IEEE T VIS COMPUT GR, V2, P171, DOI 10.1109/2945.506228
   Hoppe H., 1993, Computer Graphics Proceedings, P19, DOI 10.1145/166117.166119
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   HOPPE H, 1994, THESIS U WASHINGTON
   Hoppe Hugues, 1992, P SIGGRAPH 92, P71, DOI DOI 10.1145/133994.134011
   KAUFMAN A, 1993, COMPUTER, V26, P51, DOI 10.1109/MC.1993.274942
   Kobbelt L, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P43
   KOBBELT L, 2000, COMPUT GRAPH FORUM, V19, P249
   KOBBELT L, 1998, P SIGGRAPH 98, P105, DOI DOI 10.1145/280814.280831
   Kobbelt L.P., 1999, Computer Graphics Forum, V18, P119
   Kobbelt LP, 2001, COMP GRAPH, P57, DOI 10.1145/383259.383265
   Lindenmayer D. B., 1999, Pacific Conservation Biology, V5, P2
   Lindstrom P, 2000, COMP GRAPH, P259, DOI 10.1145/344779.344912
   Lindstrom P, 1998, VISUALIZATION '98, PROCEEDINGS, P279, DOI 10.1109/VISUAL.1998.745314
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Montani C., 1994, Visual Computer, V10, P353, DOI 10.1007/BF01900830
   Qin H, 1996, IEEE T VIS COMPUT GR, V2, P85, DOI 10.1109/2945.489389
   Schroeder W., 1998, The visualization toolkit an object-oriented approach to 3D graphics
   TURK G, 1992, COMP GRAPH, V26, P55, DOI 10.1145/142920.134008
   Willmott AJ, 1999, SPRING EUROGRAP, P293
NR 34
TC 16
Z9 20
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2003
VL 19
IS 1
BP 23
EP 37
DI 10.1007/s00371-002-0162-2
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 671MB
UT WOS:000182467600003
DA 2024-07-18
ER

PT J
AU Hao, LB
   Wang, XC
   Shen, Y
   Xu, K
   Wang, HM
AF Hao, Linbo
   Wang, Xincheng
   Shen, Ying
   Xu, Ke
   Wang, Huaming
TI Both real-valued and binary multi-feature fusion histograms for 3D local
   shape representation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE 3D local feature description; Multi-feature fusion; Real-valued
   representation; Binary extension
ID OBJECT RECOGNITION; EFFICIENT; DESCRIPTOR
AB 3D Local feature description is an essential work in 3D computer vision since a lot of downstream techniques rely on point-to-point correspondences. Most of the existing descriptors perform the description on local surfaces from one single aspect, which inherently results in limited performance. So this paper first proposes a real-valued 3D local feature descriptor named multi-feature fusion histogram (MFFH), which combines five different types of well-designed geometric features to achieve comprehensive descriptions for local surfaces. In addition, to be available for platforms with limited computing and storage resources, we conduct a seamless extension of MFFH to its binary representation B-MFFH by three kinds of directed binarization methods toward different real-valued features. Through extensive evaluation experiments on the benchmark datasets, we prove the superiorities of the proposed MFFH and B-MFFH concerning comprehensive performance. Lastly, the practicability of the proposed MFFH and B-MFFH descriptors is visually demonstrated by the point cloud registration experiments.
C1 [Hao, Linbo; Wang, Xincheng; Shen, Ying; Xu, Ke; Wang, Huaming] Nanjing Univ Aeronaut & Astronaut, Coll Mech & Elect Engn, Nanjing, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics
RP Wang, HM (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Mech & Elect Engn, Nanjing, Peoples R China.
EM hmwang@nuaa.edu.cn
CR Choi S, 2015, PROC CVPR IEEE, P5556, DOI 10.1109/CVPR.2015.7299195
   Davis JJ., 2006, PROC INT C MACHINE L, DOI DOI 10.1145/1143844.1143874
   Dong Z, 2017, ISPRS J PHOTOGRAMM, V130, P431, DOI 10.1016/j.isprsjprs.2017.06.012
   Du ZH, 2022, IMAGE VISION COMPUT, V121, DOI 10.1016/j.imavis.2022.104421
   Guo YL, 2016, INT J COMPUT VISION, V116, P66, DOI 10.1007/s11263-015-0824-y
   Guo YL, 2013, INT J COMPUT VISION, V105, P63, DOI 10.1007/s11263-013-0627-y
   Han XF, 2021, MULTIMED TOOLS APPL, V80, P26121, DOI 10.1007/s11042-021-10794-3
   Hao L., 2021, Image Vis. Comput., V117
   Hao LB, 2023, J VIS COMMUN IMAGE R, V93, DOI 10.1016/j.jvcir.2023.103817
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Liu Y., 2023, IEEE Trans. Geosci. Remote Sens, V61
   Liu YF, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3133956
   Naffouti S, 2018, SIGNAL IMAGE VIDEO P, V12, P915, DOI 10.1007/s11760-018-1235-7
   Prakhya SM, 2017, AUTON ROBOT, V41, P1501, DOI 10.1007/s10514-016-9612-y
   Quan SW, 2018, INFORM SCIENCES, V444, P153, DOI 10.1016/j.ins.2018.02.070
   Rusu R. B., 2011, 2011 IEEE INT C ROBO, P1
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Song YN, 2023, VISUAL COMPUT, V39, P1109, DOI 10.1007/s00371-021-02391-0
   Sun TC, 2020, INFORM SCIENCES, V520, P209, DOI 10.1016/j.ins.2020.02.004
   Tabia H, 2015, IEEE T MULTIMEDIA, V17, P1591, DOI 10.1109/TMM.2015.2457676
   Tao WY, 2021, IEEE T GEOSCI REMOTE, V59, P801, DOI 10.1109/TGRS.2020.2998683
   Tombari F., 2010, P ACM WORKSH 3D OBJ, P57, DOI DOI 10.1145/1877808.1877821
   Tombari F, 2013, INT J COMPUT VISION, V102, P198, DOI 10.1007/s11263-012-0545-4
   Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26
   Wang Chang-Shuo, 2023, Journal of Software, P1962, DOI 10.13328/j.cnki.jos.006683
   Wang CS, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3170493
   Wang Q, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3181062
   Yang J., 2021, IEEE Transactions on Geoscience and Remote Sensing, V60, P1
   Yang J., 2022, Vis. Comput, P1
   Yang JQ, 2017, COMPUT VIS IMAGE UND, V160, P133, DOI 10.1016/j.cviu.2017.02.004
   Yang JQ, 2017, PATTERN RECOGN, V65, P175, DOI 10.1016/j.patcog.2016.11.019
   Yue XF, 2022, APPL INTELL, V52, P12569, DOI 10.1007/s10489-022-03201-3
   Zhang H, 2023, DISPLAYS, V79, DOI 10.1016/j.displa.2023.102456
   Zou Y, 2018, PATTERN RECOGN, V76, P522, DOI 10.1016/j.patcog.2017.11.029
NR 34
TC 0
Z9 0
U1 11
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 24
PY 2023
DI 10.1007/s00371-023-03196-z
EA DEC 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DO7Z7
UT WOS:001133073800001
DA 2024-07-18
ER

PT J
AU Fan, RZ
   Wang, LL
   Liu, XD
   Im, SK
   Lam, CT
AF Fan, Runze
   Wang, Lili
   Liu, Xinda
   Im, Sio Kei
   Lam, Chan Tong
TI Real-scene-constrained virtual scene layout synthesis for mixed reality
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Scene layout synthesis; Scene matching; Scene generating
ID GRAPH; ALGORITHM; NETWORKS
AB Given a real source scene and a virtual target scene, the real-scene-constrained virtual scene layout synthesis problem is defined as how to re-synthesize the layout of the virtual furniture in the virtual scene to form a new virtual scene such that the new scene not only looks similar to the input real and virtual scenes but also is interactive. The goal of this problem is to maximize interactivity and fidelity which are contradictory. To solve this problem, we propose a real-scene-constrained virtual scene layout synthesis method to synthesize the layout of the virtual furniture in the new virtual scene. We split the scene layout synthesis process into 3 interrelated steps: scene matching, matched furniture layout generating, and unmatched furniture layout generating. For scene matching, we propose a deep scene matching network to predict the matching relationship between real and virtual furniture. For matched furniture layout generating, we propose a layout parameters optimization algorithm to predict suitable layouts of the matched virtual furniture. For unmatched furniture layout generating, we propose a deep scene generating network to predict suitable layouts of unmatched virtual furniture. We evaluate the quality of our method to synthesize scenes of different kinds and sizes. The results show that, compared with the heuristic rules-based method, our method has better matching accuracy and location accuracy. We also design a user study to evaluate the interactivity and fidelity. Compared to the manual method and the heuristic rules-based method, our method has a significant improvement in interactivity and fidelity.
C1 [Fan, Runze; Wang, Lili; Liu, Xinda] Beihang Univ, State Key Lab Virtual Real Technol & Syst, XueYuan Rd, Beijing 100191, Peoples R China.
   [Wang, Lili] Peng Cheng Lab, Xingke Rd, Shenzhen 518055, Peoples R China.
   [Im, Sio Kei; Lam, Chan Tong] Macao Polytech Univ, R Luis Gonzaga Gomes, Macau 999078, Peoples R China.
C3 Beihang University; Peng Cheng Laboratory; Macao Polytechnic University
RP Wang, LL (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, XueYuan Rd, Beijing 100191, Peoples R China.; Wang, LL (corresponding author), Peng Cheng Lab, Xingke Rd, Shenzhen 518055, Peoples R China.
EM by2106131@buaa.edu.cn; wanglily@buaa.edu.cn; liuxinda@buaa.edu.cn;
   marcusim@mpu.edu.mo; ctlam@mpu.edu.mo
FU National Key R D plan
FX No Statement Available
CR Cant RJ, 2009, UKSIM INT CONF COMP, P431, DOI 10.1109/UKSIM.2009.69
   Cao SS, 2016, AAAI CONF ARTIF INTE, P1145
   Chen Y, 2022, VISUAL COMPUT, V38, P3449, DOI 10.1007/s00371-022-02548-5
   Cheng JH, 2018, CHI 2018: EXTENDED ABSTRACTS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3170427.3188467
   Chiang WL, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P257, DOI 10.1145/3292500.3330925
   Cho M, 2010, LECT NOTES COMPUT SC, V6315, P492
   Dong ZC, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3470847
   Dong ZC, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3345554
   Dong ZC, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130893
   Feng SY, 2023, IEEE WINT CONF APPL, P5119, DOI 10.1109/WACV56688.2023.00510
   Fisher M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818057
   Fu H, 2021, INT J COMPUT VISION, V129, P3313, DOI 10.1007/s11263-021-01534-z
   Fu Q, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-019-2930-x
   Fu Q, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130805
   Gilmer J, 2017, PR MACH LEARN RES, V70
   Hahn P, 1998, EUR J OPER RES, V108, P629, DOI 10.1016/S0377-2217(97)00063-5
   Huang CK, 2016, COMPUT GRAPH FORUM, V35, P33, DOI 10.1111/cgf.13001
   Huang W., 2018, NEURIPS, P4563
   Ji Z, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3367-y
   Kushinsky Y, 2019, SIAM J IMAGING SCI, V12, P716, DOI 10.1137/18M1196480
   Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482
   Li CE, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12157900
   Li MY, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3303766
   Li QM, 2018, AAAI CONF ARTIF INTE, P3538
   Lin JJ, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024217
   Liu CJ, 2023, VISUAL COMPUT, V39, P711, DOI 10.1007/s00371-021-02369-y
   Loiola EM, 2007, EUR J OPER RES, V176, P657, DOI 10.1016/j.ejor.2005.09.032
   Ma L, 2012, IEEE J-STSP, V6, P626, DOI 10.1109/JSTSP.2012.2211996
   Merrell P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964982
   Merrell P, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866203
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Peng CH, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601164
   Ryan PrescottAdams R.S.Z., 2011, Ranking via sinkhorn propagation
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Shamir Ariel., 2009, ACM SIGGRAPH ASIA 20, P1, DOI DOI 10.1145/1665817.1665828
   Spinelli I, 2021, IEEE T NEUR NET LEAR, V32, P4755, DOI 10.1109/TNNLS.2020.3025110
   Vasylevska K, 2017, IEEE SYMP 3D USER, P12, DOI 10.1109/3DUI.2017.7893312
   Wang K, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322941
   Wang K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201362
   Wang RZ, 2019, IEEE I CONF COMP VIS, P3056, DOI 10.1109/ICCV.2019.00315
   Wang T, 2018, IEEE T PATTERN ANAL, V40, P2853, DOI 10.1109/TPAMI.2017.2767591
   Wei H, 2023, PATTERN RECOGN, V133, DOI 10.1016/j.patcog.2022.108996
   Wu TT, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3427-2
   Wu ZH, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1907
   Yan JC, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P167, DOI 10.1145/2911996.2912035
   Zanfir A, 2018, PROC CVPR IEEE, P2684, DOI 10.1109/CVPR.2018.00284
   Zhang MH, 2018, AAAI CONF ARTIF INTE, P4438
   Zhang SY, 2019, VISUAL COMPUT, V35, P1157, DOI 10.1007/s00371-019-01691-w
   Zhao X, 2020, VISUAL COMPUT, V36, P361, DOI 10.1007/s00371-018-01625-y
   Zhou DY, 2022, VISUAL COMPUT, V38, P119, DOI 10.1007/s00371-020-02007-z
NR 50
TC 0
Z9 0
U1 10
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 12
PY 2023
DI 10.1007/s00371-023-03167-4
EA DEC 2023
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CF3G0
UT WOS:001123790500003
DA 2024-07-18
ER

PT J
AU Sun, DD
   Wang, SQ
   Xia, HL
   Zhang, CA
   Gao, JL
   Mao, MY
AF Sun, Dandan
   Wang, Siqi
   Xia, Hailun
   Zhang, Changan
   Gao, Jianlong
   Mao, Mingyu
TI Human pose estimation based on cross-view feature fusion
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Three-dimensional human pose estimation; Cross-view; Feature fusion;
   Deep learning
AB Multi-view human pose estimation can achieve high accuracy by leveraging complex spatial information from multiple perspectives. However, increasing the number of views can strain the network model, potentially compromising estimation accuracy under limited computing resources. Furthermore, in the current approach of using ResNet for feature extraction, traditional methods involve deconvolution to obtain large-sized feature maps, which can introduce artificial interference. To tackle the above challenges, we propose a perceptual network based on flexible combination view feature fusion. The network is comprised of three crucial modules. The flexible view combination policy module enables high accuracy from just a single reference view. It avoids the problem of increased complexity caused by a large number of views. The up-sampling module, based on sub-pixel convolution, is designed to achieve efficient high-resolution recovery. This resolves the issue of artificial interference introduced by deconvolution. Additionally, the feature fusion module maximizes the utilization of reference view cues to enhance the human pose estimation in the current view. Experiments conducted on the Human3.6m dataset demonstrate a reduction in the average MPJPE to 18.3 mm using our model.
C1 [Sun, Dandan; Wang, Siqi; Zhang, Changan; Gao, Jianlong; Mao, Mingyu] Beijing Univ Posts & Telecommun, Sch Elect Engn, Beijing, Peoples R China.
   [Xia, Hailun] Beijing Univ Posts & Telecommun, Sch Informat & Commun Engn, Beijing, Peoples R China.
C3 Beijing University of Posts & Telecommunications; Beijing University of
   Posts & Telecommunications
RP Sun, DD (corresponding author), Beijing Univ Posts & Telecommun, Sch Elect Engn, Beijing, Peoples R China.
EM sdd661@bupt.edu.cn; wangsiqi977@bupt.edu.cn; xiahailun@bupt.edu.cn;
   871936233@qq.com; gaojianlong@bupt.edu.cn; 814344423@qq.com
FU National Natural Science Foundation of China [61976022]; National
   Natural Science Foundation of China (NSFC)
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grant 61976022.
CR Amin S, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.45
   Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   [Anonymous], 2016, DISTILL, DOI [10.23915/distill.00003, DOI 10.23915/DISTILL.00003]
   Belghit H, 2018, Arxiv, DOI arXiv:1806.09316
   Bridgeman Lewis, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Proceedings, P2487, DOI 10.1109/CVPRW.2019.00304
   Chen L, 2020, PROC CVPR IEEE, P3276, DOI 10.1109/CVPR42600.2020.00334
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Chen Z, 2022, LECT NOTES COMPUT SC, V13665, P695, DOI 10.1007/978-3-031-20065-6_40
   Diaz-Arias A, 2023, Arxiv, DOI arXiv:2304.02147
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Erol A, 2007, COMPUT VIS IMAGE UND, V108, P52, DOI 10.1016/j.cviu.2006.10.012
   Gilbert A, 2019, INT J COMPUT VISION, V127, P381, DOI 10.1007/s11263-018-1118-y
   Gordon B, 2022, LECT NOTES COMPUT SC, V13693, P176, DOI 10.1007/978-3-031-19827-4_11
   He Y., 2020, P IEEE C COMP VIS PA, P7779
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang FY, 2020, IEEE WINT CONF APPL, P418, DOI [10.1109/wacv45572.2020.9093526, 10.1109/WACV45572.2020.9093526]
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Iskakov K, 2019, IEEE I CONF COMP VIS, P7717, DOI 10.1109/ICCV.2019.00781
   Jingbo Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P764, DOI 10.1007/978-3-030-58601-0_45
   Kadkhodamohammadi A, 2020, MACH VISION APPL, V32, DOI 10.1007/s00138-020-01120-2
   Kingma D. P., 2014, arXiv
   Liang JB, 2019, IEEE I CONF COMP VIS, P4351, DOI 10.1109/ICCV.2019.00445
   Lin FY, 2010, LECT NOTES COMPUT SC, V6474, P321, DOI 10.1007/978-3-642-17688-3_31
   Liu H., 2024, IEEE Transactions on Multimedia, V24, P124, DOI [10.1109/TMM.2022.3197364, DOI 10.1109/TMM.2022.3197364]
   Liu TT, 2024, IEEE T IND INFORM, V20, P8068, DOI 10.1109/TII.2023.3266366
   Ma HY, 2022, LECT NOTES COMPUT SC, V13665, P424, DOI 10.1007/978-3-031-20065-6_25
   Mehta D., 2016, 2017 INT C 3D VISION, DOI [10.48550/arXiv.1611.09813, DOI 10.48550/ARXIV.1611.09813]
   Pavlakos G, 2017, PROC CVPR IEEE, P1263, DOI 10.1109/CVPR.2017.139
   Pavlakos G, 2017, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2017.138
   Qiu HB, 2019, IEEE I CONF COMP VIS, P4341, DOI 10.1109/ICCV.2019.00444
   Remelli E, 2020, PROC CVPR IEEE, P6039, DOI 10.1109/CVPR42600.2020.00608
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Song Y, 2012, ACM T INTERACT INTEL, V2, DOI 10.1145/2133366.2133371
   Starner T, 2003, MACH VISION APPL, V14, P59, DOI 10.1007/s00138-002-0096-8
   Trumble Matt, 2017, BRIT MACH VIS C
   Wang R, 2023, VISUAL COMPUT, V39, P4751, DOI 10.1007/s00371-022-02623-x
   Wang Tao, 2021, ADV NEURAL INFORM PR, V34
   Wei S.E., 2016, P IEEE C COMPUTER VI
   Wu Q., 2023, IEEE Trans. Cogn. Dev. Syst.
   Zhang QY, 2023, VISUAL COMPUT, V39, P651, DOI 10.1007/s00371-021-02364-3
   Zhang T., 2021, Beijing Univ Posts Telecommun, DOI [10.26969/d.cnki.gbydu.2021.001184, DOI 10.26969/D.CNKI.GBYDU.2021.001184]
   Zhang Z, 2021, INT J COMPUT VISION, V129, P703, DOI 10.1007/s11263-020-01398-9
NR 43
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 12
PY 2023
DI 10.1007/s00371-023-03184-3
EA DEC 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CF3G0
UT WOS:001123790500001
DA 2024-07-18
ER

PT J
AU Molano, R
   Sancho, JC
   Avila, MM
   Rodriguez, PG
   Caro, A
AF Molano, R.
   Sancho, J. C.
   Avila, M. M.
   Rodriguez, P. G.
   Caro, A.
TI Obtaining the user-defined polygons inside a closed contour with holes
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE k-Gon; Closed contour; Polygon; Holes; Computational geometry
ID ARBITRARY ORIENTATION; RECTANGLE
AB In image processing, computer vision algorithms are applied to regions bounded by closed contours. These contours are often irregular, poorly defined, and contain holes or unavailable areas inside. A common problem in computational geometry includes finding the k-sided polygon (k-gon) of maximum area or maximum perimeter inscribed within a contour. This paper presents a generic method to obtain user-defined polygons within a region. Users can specify the number k of sides of the polygon to obtain. Additionally, users can also decide whether the calculated polygon should be the largest in area or perimeter. This algorithm produces a polygon or set of polygons that can be used to segment an image, allowing only relevant areas to be processed. In a real-world application, the validity and versatility of the proposed method are demonstrated. In addition, the source code developed in Java and Python is available in a GitHub repository so that researchers can use it freely.
C1 [Molano, R.] Univ Extremadura, Dept Math, Caceres 10003, Spain.
   [Sancho, J. C.; Avila, M. M.; Rodriguez, P. G.; Caro, A.] Univ Extremadura, Dept Comp & Telematics Syst Engn, Caceres 10003, Spain.
C3 Universidad de Extremadura; Universidad de Extremadura
RP Molano, R (corresponding author), Univ Extremadura, Dept Math, Caceres 10003, Spain.
EM rmolano@unex.es; jcsanchon@unex.es; mmavila@unex.es; pablogr@unex.es;
   andresc@unex.es
RI Sancho, Jose Carlos/B-3125-2016; Molano, Ruben/AAX-8459-2021; Rodriguez,
   Pablo/KFJ-9680-2024; Caro, Andres/G-7277-2015; Rodriguez, Pablo
   G./J-3063-2018
OI Molano, Ruben/0000-0001-5410-6589; Caro, Andres/0000-0002-6367-2694;
   Avila, Mar/0000-0002-8717-442X; Sancho, Jose Carlos/0000-0002-4584-6945;
   Rodriguez, Pablo G./0000-0001-8168-7892
CR Ali H, 2021, VISUAL COMPUT, V37, P939, DOI 10.1007/s00371-020-01845-1
   Alt H, 1995, P 7 CAN C COMP GEOM, P67
   Anderson SL, 2020, FRONT PLANT SCI, V11, DOI 10.3389/fpls.2020.511768
   Bast H., 2000, The area partitioning problem
   Boland R.P., 2001, P 13 CAN C COMP GEOM
   Cabello S, 2017, SIAM J COMPUT, V46, P1574, DOI 10.1137/16M1079695
   CHANG JS, 1986, DISCRETE COMPUT GEOM, V1, P155, DOI 10.1007/BF02187692
   Choi Y, 2021, COMP GEOM-THEOR APPL, V94, DOI 10.1016/j.comgeo.2020.101710
   Cormen T. H., 2009, Introduction to Algorithms, VSecond
   Daniels K, 1997, COMP GEOM-THEOR APPL, V7, P125, DOI 10.1016/0925-7721(95)00041-0
   FREEMAN H, 1975, COMMUN ACM, V18, P409, DOI 10.1145/360881.360919
   Gibert G, 2013, 2013 10TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS 2013), P449, DOI 10.1109/AVSS.2013.6636681
   Girard N, 2021, PROC CVPR IEEE, P5887, DOI 10.1109/CVPR46437.2021.00583
   Goodman J. E., 1981, Geom. Dedicata, V11, P99
   Jin Kai., 2018, Maximal parallelograms in convex polygons and a novel geometric structure
   Kallus Y, 2017, Arxiv, DOI arXiv:1706.03049
   Keikha V., 2020, Linear-time algorithms for largest inscribed quadrilateral
   Knauer C, 2012, J DISCRET ALGORITHMS, V13, P78, DOI 10.1016/j.jda.2012.01.002
   Lee S, 2021, COMP GEOM-THEOR APPL, V98, DOI 10.1016/j.comgeo.2021.101792
   Media Engineering Group (GIM), 2023, Source code, Scripts, and Documentation
   MELISSARATOS EA, 1992, SIAM J COMPUT, V21, P601, DOI 10.1137/0221038
   Ministry of Farming Fishing and Food of Spain, 2023, ABOUT US
   Molano R, 2021, IEEE ACCESS, V9, P103600, DOI 10.1109/ACCESS.2021.3098234
   Molano R, 2012, APPL MATH COMPUT, V218, P9866, DOI 10.1016/j.amc.2012.03.063
   Oksanen T, 2013, COMPUT ELECTRON AGR, V98, P252, DOI 10.1016/j.compag.2013.08.014
   Qasmieh IA., 2021, Int J Electr Comput Eng (IJECE), V11, P4037, DOI DOI 10.11591/IJECE.V11I5.PP4037-4049
   Rote G, 2019, Arxiv, DOI arXiv:1905.11203
   sigpac.mapa.gob, 2023, Visor SigPac v4.8
   VARBERG DE, 1985, AM MATH MON, V92, P584, DOI 10.2307/2323172
   Wei Q, 2021, J COMB OPTIM, V41, P625, DOI 10.1007/s10878-021-00705-5
   Xu ZH, 2022, IEEE ROBOT AUTOM LET, V7, P5063, DOI 10.1109/LRA.2022.3154052
NR 31
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 7
PY 2023
DI 10.1007/s00371-023-03170-9
EA DEC 2023
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AA2V2
UT WOS:001115674200003
DA 2024-07-18
ER

PT J
AU Dasari, MM
   Gorthi, RK
AF Dasari, Mohana Murali
   Gorthi, Rama Krishna
TI Icg: intensity and color gradient operator on RGB images for visual
   object tracking
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Visual object tracking; Gradient image features; Deep neural networks
ID FEATURES; NETWORK
AB The design of digital filters is now mostly automated with convolutional neural networks (CNNs). State-of-the-art works in tracking methods, including the well-known correlation and deep Siamese trackers, use features from such CNNs. However, deep learning requires huge data, high computational resources, and more training time. Hence, smart and simple alternative feature extraction strategies are needed in embedded applications. In this direction, a method is proposed for obtaining enriched "intensity and color gradient features" using the "three-dimensional gradient operator" on color images. This work considered the popular first-order gradient operator ([-1 0 1]) and outer product operator to generate various intensity and color gradients. The generated features contain rich information, including edges, color, and mid-level segmentation-like features. This simple yet effective operator does not involve any learning parameters. Despite not having learnable parameters, the proposed method's performance is comparable to lightweight learned CNNs such as MobileNet. The efficacy of the resultant features is demonstrated for the visual tracking task using well-known datasets in tracking, namely GOT-10k, LaSOT, OTB2015, UAV123, and VOT2018. The proposed features combined with other deep learning features have boosted the performance over baseline efficient convolution operator (ECO) tracker. This work will open new frontiers in designing hybrid features for visual tracking and other visual computation tasks. The code is available "https://github.com/dasari4321/ICG_tracker.git"
C1 [Dasari, Mohana Murali; Gorthi, Rama Krishna] Indian Inst Technol, Dept Elect Engn, Tirupati 517619, Andhra Pradesh, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Tirupati
RP Dasari, MM (corresponding author), Indian Inst Technol, Dept Elect Engn, Tirupati 517619, Andhra Pradesh, India.
EM ee18d001@iittp.ac.in; rkg@iittp.ac.in
FU We would like to thank Mr. Naveen for helping in drafting the ECO
   tracker details in the proposed method section. We also thank the
   authors of KCF and ECO for providing the GitHub codes of the baseline
   works.
FX We would like to thank Mr. Naveen for helping in drafting the ECO
   tracker details in the proposed method section. We also thank the
   authors of KCF and ECO for providing the GitHub codes of the baseline
   works.
CR Aggarwal S., 2023, IEEE INT C AC SPEECH, P1
   Bertinetto L, 2021, Arxiv, DOI arXiv:1606.09549
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Bhat G, 2018, LECT NOTES COMPUT SC, V11206, P493, DOI 10.1007/978-3-030-01216-8_30
   Bourennane M, 2022, ENG TECHNOL APPL SCI, V12, P8745
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Choi J., 2018, Comp. Vis. and Imag. Under. (CVIU), V171
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Danelljan M, 2020, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR42600.2020.00721
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Dasari M.M., 2020, 2020 IEEE INT C IMAG
   Dasari MM, 2023, DISPLAYS, V77, DOI 10.1016/j.displa.2023.102372
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Fang FM, 2021, IEEE T NEUR NET LEAR, V32, P3956, DOI 10.1109/TNNLS.2020.3016321
   Fu ZH, 2021, PROC CVPR IEEE, P13769, DOI 10.1109/CVPR46437.2021.01356
   Howard AG, 2017, Arxiv, DOI arXiv:1704.04861
   Guo J, 2018, IEEE IMAGE PROC, P226, DOI 10.1109/ICIP.2018.8451440
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Kim H., 2018, Electron. Lett., V55
   Kristan M, 2016, IEEE T PATTERN ANAL, V38, P2137, DOI 10.1109/TPAMI.2016.2516982
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li XJ, 2022, IEEE T CIRC SYST VID, V32, P1537, DOI 10.1109/TCSVT.2021.3077640
   Li XJ, 2021, ENG APPL ARTIF INTEL, V102, DOI 10.1016/j.engappai.2021.104266
   Li ZC, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3240195
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu XK, 2021, IEEE T CIRC SYST VID, V31, P1268, DOI 10.1109/TCSVT.2019.2944654
   Ma Z, 2020, Arxiv, DOI arXiv:2008.03467
   Mayer C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13424, DOI 10.1109/ICCV48922.2021.01319
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nam H, 2016, Arxiv, DOI arXiv:1510.07945
   Rout L, 2019, LECT NOTES COMPUT SC, V11129, P83, DOI 10.1007/978-3-030-11009-3_4
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937
   Tang H, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108792
   Tang H, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P610, DOI 10.1145/3394171.3413884
   Teng Z, 2020, PATTERN RECOGN, V101, DOI 10.1016/j.patcog.2019.107188
   Voigtlaender P, 2020, PROC CVPR IEEE, P6577, DOI 10.1109/CVPR42600.2020.00661
   Wang N, 2021, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR46437.2021.00162
   Wang X, 2018, PROC CVPR IEEE, P4864, DOI 10.1109/CVPR.2018.00511
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Yan B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10428, DOI 10.1109/ICCV48922.2021.01028
   Yan SL, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3310118
   Yin Y., 2018, Adversarial feature sampling learning for efficient visual tracking
   Yun S, 2017, PROC CVPR IEEE, P1349, DOI 10.1109/CVPR.2017.148
   Zha ZC, 2023, IEEE T CIRC SYST VID, V33, P3947, DOI 10.1109/TCSVT.2023.3236636
   Zhu Z., 2018, P EUROPEAN C COMPUTE
   zin G., 2019, Generative adversarial networks for online visual object tersarial networks for online visual object tracking systems
NR 53
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 11
PY 2023
DI 10.1007/s00371-023-03136-x
EA NOV 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7KE2
UT WOS:001100186900002
DA 2024-07-18
ER

PT J
AU Shastry, A
   George, S
   Bini, AA
   Jidesh, P
AF Shastry, Architha
   George, S.
   Bini, A. A.
   Jidesh, P.
TI AttentionDIP: attention-based deep image prior model to restore
   satellite and aerial images from gamma distributed speckle interference
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Despeckling; Attention network; Deep image prior; Gamma distribution;
   Image restoration
ID NOISE
AB Image restoration is an inevitable pre-processing step in most satellite imaging applications. The satellite imaging modality such as Synthetic Aperture Radar (SAR) is prone to speckle distortions due to constructive and destructive interference of the probing signals. Speckles being data correlated and multiplicative, their reduction is not so trivial. Since speckles are not purely noise interventions, a blind reduction process leads to spurious analysis at the later stages. Moreover, the image details are liable to get compromised during such a noise reduction process. An attention-based deep image prior (DIP) model with U-Net architecture has been proposed in this work to carefully address these setbacks. The attention block is used to scale the features extracted from the encoder, and they are concatenated with the features from the decoder to obtain both low- and high-level features. The attention module incorporated in the model helps to extract significant complex structures in SAR images. Further, the DIP model duly respects the noise distribution of speckles while performing the despeckling task. Various synthetic, natural, aerial, and satellite images are subjected to the testing and verification process, and the results obtained are in favor of the proposed model. The quantitative analysis carried out using various statistical metrics in this study also reveals the restoration ability of the proposed method in terms of both despeckling and structure preservation.
C1 [Shastry, Architha; George, S.; Jidesh, P.] Natl Inst Technol Karnataka, Dept Math & Computat Sci, Surathkal, India.
   [Bini, A. A.] Natl Inst Technol Calicut, Dept Elect & Commun Engn, Kozhikode, Kerala, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Karnataka; National Institute of Technology (NIT System);
   National Institute of Technology Calicut
RP Jidesh, P (corresponding author), Natl Inst Technol Karnataka, Dept Math & Computat Sci, Surathkal, India.
EM jidesh@nitk.edu.in
RI P, J/KCK-9262-2024; P, Jidesh/C-6030-2017
OI P, Jidesh/0000-0001-9448-1906
FU The authors wish to thank the Science and Engineering Research Board,
   Govt. of India, for providing financial support under grant no.
   CRG/2020/000476. [CRG/2020/000476]; Science and Engineering Research
   Board
FX The authors wish to thank the Science and Engineering Research Board,
   Govt. of India, for providing financial support under grant no.
   CRG/2020/000476.
CR Aja-Fernández S, 2006, IEEE T IMAGE PROCESS, V15, P2694, DOI 10.1109/TIP.2006.877360
   Aubert G, 2008, SIAM J APPL MATH, V68, P925, DOI 10.1137/060671814
   Buades A, 2011, IMAGE PROCESS ON LIN, V1, P208, DOI 10.5201/ipol.2011.bcm_nlm
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dalsasso E, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3128621
   Dalsasso E, 2021, IEEE J-STARS, V14, P4321, DOI 10.1109/JSTARS.2021.3071864
   Dalsasso E, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12162636
   Fan WS, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9050784
   Febin IP, 2022, VISUAL COMPUT, V38, P1413, DOI 10.1007/s00371-021-02076-8
   Febin IP, 2018, IMAGING SCI J, V66, P479, DOI 10.1080/13682199.2018.1518760
   FROST VS, 1982, IEEE T PATTERN ANAL, V4, P157, DOI 10.1109/TPAMI.1982.4767223
   Gilboa G, 2008, MULTISCALE MODEL SIM, V7, P1005, DOI 10.1137/070698592
   Gomez L, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9040389
   Gonzalez RC, 2009, DIGITAL IMAGE PROCES, DOI 10.1117/1.3115362
   GOODMAN JW, 1976, J OPT SOC AM, V66, P1145, DOI 10.1364/JOSA.66.001145
   Jetley S, 2018, Arxiv, DOI [arXiv:1804.02391, DOI 10.48550/ARXIV.1804.02391, 10.48550/arXiv.1804.02391]
   JetPropulsion Laboratory, 2021, Space radar image of Flevoland
   Jidesh P, 2018, COMPUT ELECTR ENG, V70, P631, DOI 10.1016/j.compeleceng.2017.09.013
   KUAN DT, 1985, IEEE T PATTERN ANAL, V7, P165, DOI 10.1109/TPAMI.1985.4767641
   Lalitha V, 2022, MATER TODAY-PROC, V62, P4772, DOI 10.1016/j.matpr.2022.03.341
   LEE JS, 1980, IEEE T PATTERN ANAL, V2, P165, DOI 10.1109/TPAMI.1980.4766994
   Lehtinen J, 2018, PR MACH LEARN RES, V80
   Lou YF, 2010, J SCI COMPUT, V42, P185, DOI 10.1007/s10915-009-9320-2
   Merced Universityof California, 2022, MERC 2020 AER PHOT
   Molini AB, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3065461
   Mousa A., 2022, Vis. Comput., V39, P1
   Oktay O, 2018, Arxiv, DOI arXiv:1804.03999
   Parrilli S, 2012, IEEE T GEOSCI REMOTE, V50, P606, DOI 10.1109/TGRS.2011.2161586
   Perera M.V., 2022, IGARSS 2022 2022 IEE
   Radford A, 2016, Arxiv, DOI [arXiv:1511.06434, DOI 10.48550/ARXIV.1511.06434]
   Rao J, 2022, VIRTUAL PHYS PROTOTY, V17, P1047, DOI 10.1080/17452759.2022.2086142
   Rasti B, 2022, IEEE GEOSC REM SEN M, V10, P201, DOI 10.1109/MGRS.2021.3121761
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Saad OM., 2021, IEEE Trans Geosci Remote Sens, V60, P1
   SandiaNational Laboratories, 2021, Pathfinder radar isr & sar systems:sar imagery
   Shastry A, 2022, PFG-J PHOTOGRAMM REM, V90, P497, DOI 10.1007/s41064-022-00226-8
   Shi WL, 2021, VISUAL COMPUT, V37, P1569, DOI 10.1007/s00371-020-01903-8
   Smitha A, 2021, J MOD OPTIC, V68, P1002, DOI 10.1080/09500340.2021.1968052
   Tian CW, 2020, NEURAL NETWORKS, V124, P117, DOI 10.1016/j.neunet.2019.12.024
   Tomasi C., 1998, 6 INT C COMP VIS IEE, P839
   Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang PY, 2017, 2017 IEEE 7TH INTERNATIONAL WORKSHOP ON COMPUTATIONAL ADVANCES IN MULTI-SENSOR ADAPTIVE PROCESSING (CAMSAP)
   Wei SJ, 2020, IEEE ACCESS, V8, P120234, DOI 10.1109/ACCESS.2020.3005861
   Yang H, 2020, MOB INF SYST, V2020, DOI 10.1155/2020/8819587
   Yang Y., 2010, P 18 SIGSPATIAL INT, P270, DOI DOI 10.1145/1869790.1869829
   Yu YJ, 2002, IEEE T IMAGE PROCESS, V11, P1260, DOI 10.1109/TIP.2002.804279
   Zhang JX, 2020, IEEE ACCESS, V8, P58533, DOI 10.1109/ACCESS.2020.2983075
   Zhang Q, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10020196
   Zhao YS, 2020, INT CONF ACOUST SPEE, P2668, DOI [10.1109/icassp40776.2020.9054658, 10.1109/ICASSP40776.2020.9054658]
NR 51
TC 0
Z9 0
U1 3
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 5
PY 2023
DI 10.1007/s00371-023-03101-8
EA OCT 2023
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T7QI9
UT WOS:001079888400004
DA 2024-07-18
ER

PT J
AU Masood, S
   Ali, SG
   Wang, XN
   Masood, A
   Li, P
   Li, HT
   Jung, Y
   Sheng, B
   Kim, J
AF Masood, Saleha
   Ali, Saba Ghazanfar
   Wang, Xiangning
   Masood, Afifa
   Li, Ping
   Li, Huating
   Jung, Younhyun
   Sheng, Bin
   Kim, Jinman
TI Deep choroid layer segmentation using hybrid features extraction from
   OCT images
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Choroid layer; OCT; Thickness map; Segmentation
ID AUTOMATIC SEGMENTATION; RETINAL LAYER; MACULAR DEGENERATION; COHERENCE;
   AMD; SET
AB The choroid layer, situated between the retina and sclera, is a tissue layer that contains blood vessels. Optical coherence tomography (OCT) is a method that utilizes light for imaging purposes to capture detailed images of this specific part of the retina. Although there have been notable advancements, the automated choroid segmentation persists difficult due to the inherently low contrast of OCT images. Handcrafted features, which provide domain-specific knowledge, and convolutional neural network (CNN) methods, which handle large sets of general features, are both employed in addressing this challenge. There is a plea to merge these two different classes of feature-generation methods. The challenge is to form a combined set of features that can outperform either feature extraction method. We proposed a cascaded method for choroid layer segmentation that logically combines a CNN feature set with handcrafted features. Our method used handcrafted features, Gabor features, Haar features, and gray-level co-occurrence features due to the robustness to segment low-contrast images. A support vector machine was independently trained using the CNN feature set and handcrafted feature set, which were then linearly combined for the final choroid segmentation. The method under consideration was assessed using a dataset comprising 525 images. Furthermore, we introduced two metrics to quantitatively evaluate the thickness of the layer: (i) the pixel-wise error in the segmentation and (ii) the average error in the generated thickness map. Through experimentation, the results demonstrated that our proposed method successfully accomplished the intended objective, a remarkable accuracy of 97 percent, with a mean error rate of 2.84. Moreover, it outperformed existing state-of-the-art segmentation methods.
C1 [Masood, Saleha] COMSATS Univ Islamabad, Dept Comp Sci, Wah, Pakistan.
   [Ali, Saba Ghazanfar; Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Wang, Xiangning; Li, Huating] Shanghai Jiao Tong Univ, Sch Med, Shanghai Peoples Hosp 6, Shanghai, Peoples R China.
   [Masood, Afifa] Shifa Int Hosp, Dept Urol, Islamabad, Pakistan.
   [Li, Ping] Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong, Peoples R China.
   [Li, Ping] Hong Kong Polytech Univ, Sch Design, Kowloon, Hong Kong, Peoples R China.
   [Jung, Younhyun] Gachon Univ, Dept Software, Seongnam, South Korea.
   [Kim, Jinman] Univ Sydney, Sch Comp Sci, Sydney, NSW, Australia.
C3 COMSATS University Islamabad (CUI); Shanghai Jiao Tong University;
   Shanghai Jiao Tong University; Hong Kong Polytechnic University; Hong
   Kong Polytechnic University; Gachon University; University of Sydney
RP Sheng, B (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
EM shengbin@sjtu.edu.cn
RI Chen, Jin/KBQ-0163-2024; Li, Ping/I-6143-2013; Wang,
   Yanan/JVZ-7957-2024; zhang, xinyi/JWA-0980-2024
OI Chen, Jin/0009-0005-5844-635X; Li, Ping/0000-0002-8391-6510; 
CR Ali SG, 2021, MULTIMED TOOLS APPL, V80, P35105, DOI 10.1007/s11042-020-09303-9
   Ali SG., 2022, IET IMAGE PROCESS, V5, P58
   Alonso-Caneiro D, 2013, BIOMED OPT EXPRESS, V4, P2795, DOI 10.1364/BOE.4.002795
   [Anonymous], 2018, ASIAN C COMPUTER VIS
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Ben-Cohen A., 2017, Retinal layers segmentation using fully convolutional network in Oct images, P1
   Bressler N.M., 2018, ARXIV180109749 CORR, P1
   Burlina P, 2017, COMPUT BIOL MED, V82, P80, DOI 10.1016/j.compbiomed.2017.01.018
   Burlina PM, 2017, JAMA OPHTHALMOL, V135, P1170, DOI 10.1001/jamaophthalmol.2017.3782
   Chen M, 2017, LECT NOTES COMPUT SC, V10554, P177, DOI 10.1007/978-3-319-67561-9_20
   Chen Q, 2015, OPT EXPRESS, V23, P8974, DOI 10.1364/OE.23.008974
   Danesh H, 2014, COMPUT MATH METHOD M, V2014, DOI 10.1155/2014/479268
   Djunaidi K., 2020, INDONES J ELECT ENG, V22, P187
   Fang LY, 2017, BIOMED OPT EXPRESS, V8, P2732, DOI 10.1364/BOE.8.002732
   Gopinath K, 2017, PROCEEDINGS 2017 4TH IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR), P888, DOI 10.1109/ACPR.2017.121
   He YF, 2017, LECT NOTES COMPUT SC, V10554, P202, DOI 10.1007/978-3-319-67561-9_23
   Hsia WP, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11125488
   Hu ZH, 2013, INVEST OPHTH VIS SCI, V54, P1722, DOI 10.1167/iovs.12-10578
   Ishikawa H, 2005, INVEST OPHTH VIS SCI, V46, P2012, DOI 10.1167/iovs.04-0335
   Kajic V, 2012, BIOMED OPT EXPRESS, V3, P86, DOI 10.1364/BOE.3.000086
   Keller B, 2016, J BIOMED OPT, V21, DOI 10.1117/1.JBO.21.7.076015
   Khaing TT, 2021, IEEE ACCESS, V9, P150951, DOI 10.1109/ACCESS.2021.3124993
   Kugelman J, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-49816-4
   Lee CS, 2017, OPHTHALMOL RETINA, V1, P322, DOI 10.1016/j.oret.2016.12.009
   Li QL, 2020, TRANSL VIS SCI TECHN, V9, DOI 10.1167/tvst.9.2.61
   Lienhart R, 2002, IEEE IMAGE PROC, P900
   Liu XX, 2019, BIOMED OPT EXPRESS, V10, P1601, DOI 10.1364/BOE.10.001601
   Lu HQ, 2013, IEEE ENG MED BIO, P5869, DOI 10.1109/EMBC.2013.6610887
   Maji S, 2008, PROC CVPR IEEE, P2245
   Masood S, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-39795-x
   Masood S, 2018, IET IMAGE PROCESS, V12, P53, DOI 10.1049/iet-ipr.2017.0273
   Minhas S, 2009, INT CONF EMERG TECHN, P252, DOI 10.1109/ICET.2009.5353166
   Niu SJ, 2016, BIOMED OPT EXPRESS, V7, P581, DOI 10.1364/BOE.7.000581
   Novosel J, 2015, MED IMAGE ANAL, V26, P146, DOI 10.1016/j.media.2015.08.008
   Oliveira J, 2017, BIOMED OPT EXPRESS, V8, P281, DOI 10.1364/BOE.8.000281
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roy AG, 2017, BIOMED OPT EXPRESS, V8, P3627, DOI 10.1364/BOE.8.003627
   Sara U., 2019, J COMPUT COMMUN, V7, P8, DOI [10.4236/jcc.2019.73002, DOI 10.4236/JCC.2019.73002]
   Sui XD, 2017, NEUROCOMPUTING, V237, P332, DOI 10.1016/j.neucom.2017.01.023
   Tian J, 2013, BIOMED OPT EXPRESS, V4, P397, DOI 10.1364/BOE.4.000397
   Wang C, 2017, IEEE J BIOMED HEALTH, V21, P1694, DOI 10.1109/JBHI.2017.2675382
   Yu, 2021, COMPUT MATH METHOD M, V2, P56
   Zhang HH, 2020, IEEE J BIOMED HEALTH, V24, P3408, DOI 10.1109/JBHI.2020.3023144
NR 43
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2775
EP 2792
DI 10.1007/s00371-023-02985-w
EA SEP 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001069495600002
DA 2024-07-18
ER

PT J
AU Qiu, JX
   Yin, ZX
   Cheng, MM
   Ren, B
AF Qiu, Jiaxiong
   Yin, Ze-Xin
   Cheng, Ming-Ming
   Ren, Bo
TI Rendering real-world unbounded scenes with cars by learning positional
   bias
SO VISUAL COMPUTER
LA English
DT Article
DE Unbounded scenes; Specular reflections; Positional bias; Regularization
AB In real-world unbounded outdoor scenes with cars, there are various specular reflections caused by the surrounding environment appearing on the reflective surfaces of cars. Background regions of unbounded scenes encode inherent ambiguity of rendering, and specular reflections on cars violates the multi-view consistency. NeRF++ struggles in these scenes because of the enormous ambiguity. To deal with the challenges of rendering unbounded scenes with cars, we present a novel module to strengthen the capability of the basic model in this task. We propose to learn the positional bias between sampled points along a camera ray and target points along the incident light by multi-layer perceptrons to reconstitute the input points and view direction with regularization constraints for physical rendering. Considering the variety of materials and textures in unbounded scenes, we implicitly separate learned foreground colors into two components, diffuse and specular colors, to acquire smooth results. Our module improves basic models by 2.5% on average SSIM in our extensive experiments, produces more photo-realistic novel views of real-world unbounded scenes than other compared methods, and achieves the physical color editing of cars.
C1 [Qiu, Jiaxiong; Yin, Ze-Xin; Cheng, Ming-Ming; Ren, Bo] Nankai Univ, Coll Comp Sci, TMCC, Tianjin 300000, Peoples R China.
C3 Nankai University
RP Ren, B (corresponding author), Nankai Univ, Coll Comp Sci, TMCC, Tianjin 300000, Peoples R China.
EM qiujiaxiong727@gmail.com; Zexin.Yin.cn@gmail.com; cmm@nankai.edu.cn;
   rb@nankai.edu.cn
RI Cheng, Ming-Ming/A-2527-2009; Qiu, Jiaxiong/KDM-8471-2024
OI Cheng, Ming-Ming/0000-0001-5550-8758; Qiu, Jiaxiong/0000-0002-6065-7296
FU Key Technologies Research and Development Program
FX No Statement Available
NR 0
TC 1
Z9 1
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4085
EP 4098
DI 10.1007/s00371-023-03070-y
EA SEP 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001064709500002
DA 2024-07-18
ER

PT J
AU Song, SF
   Chau, LP
   Lin, ZP
AF Song, Shufeng
   Chau, Lap-Pui
   Lin, Zhiping
TI Portrait matting using an attention-based memory network
SO VISUAL COMPUTER
LA English
DT Article
DE Auxiliary-free matting; Attention-based memory block; Self-attention;
   Memory network; Direct supervision
ID IMAGE; SEGMENTATION
AB We propose a novel network to perform auxiliary-free video matting task. Unlike most existing approaches that require trimaps or pre-captured backgrounds as auxiliary inputs, our method uses binary segmentation masks as priors and realizes the auxiliary-free matting. Furthermore, we design the attention-based memory block by combining the idea of the memory network and self-attention to compute pixel-level temporal coherence among video frames to enhance the overall performance. Moreover, we also provide direct supervision for the temporal-guided memory module to boost the network's robustness. The validation results on various testing datasets show that our method outperforms several state-of-the-art auxiliary-free matting methods in terms of the alpha and foreground prediction quality and temporal consistency.
C1 [Song, Shufeng; Lin, Zhiping] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore, Singapore.
   [Chau, Lap-Pui] Hong Kong Polytech Univ, Dept Elect & Elect Engn, Hong Kong, Peoples R China.
C3 Nanyang Technological University; Hong Kong Polytechnic University
RP Chau, LP (corresponding author), Hong Kong Polytech Univ, Dept Elect & Elect Engn, Hong Kong, Peoples R China.
EM SHUFENG001@e.ntu.edu; lap-pui.chau@polyu.edu.hk; ezplin@ntu.edu.sg
OI Chau, Lap-Pui/0000-0003-4932-0593
CR Ballas N., 2015, arXiv
   Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565
   Cai SF, 2019, IEEE I CONF COMP VIS, P8818, DOI 10.1109/iccv.2019.00891
   Cao GY, 2019, VISUAL COMPUT, V35, P133, DOI 10.1007/s00371-017-1424-3
   Changqian Yu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12413, DOI 10.1109/CVPR42600.2020.01243
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Forte M., 2020, ARXIV
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   GUPTA V, 2016, IN 2016 INT C SIGNAL, P1
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QQ, 2019, IEEE I CONF COMP VIS, P4129, DOI 10.1109/ICCV.2019.00423
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu YT, 2018, LECT NOTES COMPUT SC, V11212, P56, DOI 10.1007/978-3-030-01237-3_4
   Hu YT, 2017, ADV NEUR IN, V30
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jiang M, 2022, VISUAL COMPUT, V38, P2473, DOI 10.1007/s00371-021-02124-3
   Juan O, 2005, LECT NOTES COMPUT SC, V3752, P186
   Kahn JM, 2007, J CRIT CARE, V22, P97, DOI 10.1016/j.jcrc.2006.09.003
   Ke Z., 2020, ARXIV
   Kumar A, 2016, PR MACH LEARN RES, V48
   Li YY, 2020, AAAI CONF ARTIF INTE, V34, P11450
   Lin SC, 2022, IEEE WINT CONF APPL, P3132, DOI 10.1109/WACV51458.2022.00319
   Lin SC, 2021, PROC CVPR IEEE, P8758, DOI 10.1109/CVPR46437.2021.00865
   Lu H, 2019, IEEE I CONF COMP VIS, P3265, DOI 10.1109/ICCV.2019.00336
   Mixkit, 2022, US
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Oh SW, 2019, IEEE I CONF COMP VIS, P9225, DOI 10.1109/iccv.2019.00932
   Oh SW, 2019, PROC CVPR IEEE, P5242, DOI 10.1109/CVPR.2019.00539
   Rhemann C, 2009, PROC CVPR IEEE, P1826, DOI 10.1109/CVPRW.2009.5206503
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sengupta S, 2020, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR42600.2020.00236
   Shahrian E, 2013, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2013.88
   Steven A., 2022, MATTING HUMAN DATASE
   Sun J., 2021, ARXIV
   Sun Y., 2021, P IEEE CVF C COMP VI, P11120, DOI DOI 10.48550/ARXIV.2104.08201
   Sun YN, 2021, PROC CVPR IEEE, P6971, DOI 10.1109/CVPR46437.2021.00690
   Tang H, 2019, IEEE IMAGE PROC, P4255, DOI [10.1109/icip.2019.8803682, 10.1109/ICIP.2019.8803682]
   Tang JW, 2019, PROC CVPR IEEE, P3050, DOI 10.1109/CVPR.2019.00317
   Vaswani A, 2017, ADV NEUR IN, V30
   Vatolin Dmitriy S, 2015, BMVC, P99
   Vladislav S., 2019, FLOPS COUNTER PYTORC
   Voigtlaender P, 2019, PROC CVPR IEEE, P9473, DOI 10.1109/CVPR.2019.00971
   Wu HK, 2018, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2018.00197
   Xu N, 2017, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.2017.41
   Ye JW, 2020, IEEE T IMAGE PROCESS, V29, P1177, DOI 10.1109/TIP.2019.2930146
   Yoon JS, 2017, IEEE I CONF COMP VIS, P2186, DOI 10.1109/ICCV.2017.238
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu CQ, 2018, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR.2018.00199
   Zhang YN, 2019, PATTERN RECOGN IMAGE, V29, P230, DOI 10.1134/S105466181902010X
   Zhang YK, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5128, DOI 10.1145/3474085.3475623
NR 50
TC 1
Z9 1
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3733
EP 3746
DI 10.1007/s00371-023-03061-z
EA SEP 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001063147800001
DA 2024-07-18
ER

PT J
AU Zeng, HF
   Wu, Q
   Zhang, J
   Xia, HJ
AF Zeng, Hongfei
   Wu, Qiang
   Zhang, Jin
   Xia, Haojie
TI Lightweight subpixel sampling network for image super-resolution
SO VISUAL COMPUTER
LA English
DT Article
DE Super-resolution; Subpixel sampling; CNN; Attention mechanism
ID RECONSTRUCTION
AB Recently, convolutional neural networks (CNNs) have facilitated the rapid development of image super-resolution. Most deep networks are challenging to apply to the real world due to their high cost of memory storage and computational complexity. This paper addresses this issue by proposing a lightweight subpixel sampling network (SSN). Specifically, we use a traditional encoder-decoder structure and replace the deconvolution and pooling layers by subpixel up-sampling and down-sampling without parameters. Subpixel sampling retains more image information than other sampling methods. In addition, we propose parsimonious spatial and channel attention blocks through which multi-scale features are fused and more image textures can be recovered. Through extensive experiments, we validate the effectiveness of subpixel sampling, spatial attention block, and channel attention block. In terms of quantitative metrics and visual quality, our models achieve performance comparable to state-of-the-art methods.
C1 [Zeng, Hongfei; Wu, Qiang; Zhang, Jin; Xia, Haojie] Hefei Univ Technol, Sch Instrument Sci & Optoelect Engn, Anhui Prov Key Lab Measuring Theory & Precis Inst, Hefei 230009, Anhui, Peoples R China.
C3 Hefei University of Technology
RP Xia, HJ (corresponding author), Hefei Univ Technol, Sch Instrument Sci & Optoelect Engn, Anhui Prov Key Lab Measuring Theory & Precis Inst, Hefei 230009, Anhui, Peoples R China.
EM 2510398367@qq.com; 1075911702@qq.com; zhangjin@hfut.edu.cn;
   hjxia@hfut.edu.cn
OI Zeng, Hongfei/0009-0009-6748-3039; qiang, wu/0000-0002-0915-110X
FU National Natural Science Foundation of China
FX This work was supported by the National Natural Science Foundation of
   China [Grant No. 51975179] and the Anhui Provincial Science and
   Technique Program [Grant No. 202003a05020008].
CR Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Anwar S, 2022, IEEE T PATTERN ANAL, V44, P1192, DOI 10.1109/TPAMI.2020.3021088
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304
   Cao J, 2021, arXiv
   Chen X., 2023, P IEEECVF C COMPUTER
   Chen Y., 2023, Int. J. Mach. Learn. Cybern., P1
   Chen Y., 2023, Multimedia Tools and Applications, P1
   Chen YT, 2023, J KING SAUD UNIV-COM, V35, DOI 10.1016/j.jksuci.2023.101567
   Chen YT, 2023, J VIS COMMUN IMAGE R, V91, DOI 10.1016/j.jvcir.2023.103776
   Chen YT, 2024, VISUAL COMPUT, V40, P489, DOI 10.1007/s00371-023-02795-0
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Trinh DH, 2014, IEEE T IMAGE PROCESS, V23, P1882, DOI 10.1109/TIP.2014.2308422
   Dong C., 2016, ARXIV
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Han N, 2022, DISPLAYS, V73, DOI 10.1016/j.displa.2022.102192
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li JC, 2018, LECT NOTES COMPUT SC, V11212, P527, DOI 10.1007/978-3-030-01237-3_32
   Li W., 2021, ARXIV
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lin Z., 2022, ARXIV
   Lu XB, 2024, VISUAL COMPUT, V40, P41, DOI 10.1007/s00371-022-02764-z
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mei YQ, 2021, PROC CVPR IEEE, P3516, DOI 10.1109/CVPR46437.2021.00352
   Niu B., 2020, EUR C COMP VIS, P191, DOI [10.1007/978-3-030-58610-2_47, DOI 10.1007/978-3-030-58610-2_12]
   Robinson M. D., 2017, SUPER RESOLUTION IMA, P383, DOI DOI 10.1201/9781439819319-13
   Saharia C, 2023, IEEE T PATTERN ANAL, V45, P4713, DOI 10.1109/TPAMI.2022.3204461
   Sun L, 2021, IEEE-CAA J AUTOMATIC, V8, P1271, DOI 10.1109/JAS.2021.1004009
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Wang Y, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3093328
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Yuan C, 2022, ENTROPY-SWITZ, V24, DOI 10.3390/e24081030
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang HY, 2014, REMOTE SENS-BASEL, V6, P637, DOI 10.3390/rs6010637
   Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhao Y, 2018, IEEE ACCESS, V6, P39363, DOI 10.1109/ACCESS.2018.2855127
   Zhou S., 2020, P ADV NEUR INF PROC, V33, P3499
   Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]
   Zhuang YT, 2007, PATTERN RECOGN, V40, P3178, DOI 10.1016/j.patcog.2007.03.011
   Zou WB, 2022, IEEE COMPUT SOC CONF, P929, DOI 10.1109/CVPRW56347.2022.00107
NR 50
TC 0
Z9 0
U1 6
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3781
EP 3793
DI 10.1007/s00371-023-03064-w
EA SEP 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001056289600001
DA 2024-07-18
ER

PT J
AU Teng, G
   Jiang, RX
   Liu, XS
   Zhou, F
   Chen, YW
AF Teng, Ge
   Jiang, Rongxin
   Liu, Xuesong
   Zhou, Fan
   Chen, Yaowu
TI EARN: toward efficient and robust JPEG compression artifact reduction
SO VISUAL COMPUTER
LA English
DT Article
DE JPEG artifact reduction; Depthwise convolution; Adaptive normalization;
   Lightweight network
ID QUALITY ASSESSMENT; BLOCKING; SUPERRESOLUTION; NETWORK
AB JPEG is one of the most widely used lossy image compression algorithms, but artifacts are generated during compression. Various artifact reduction methods have been proposed, and many of them, especially deep learning-based approaches, showed promising performance. However, one major drawback that limited their deployment and application is their cumbersome and complicated model. To remedy this problem, we propose a simple and efficient network named Efficient Artifact Reduction Network. To achieve efficiency, we consider enlarging the receptive field and preserving pixel-wise information as significant concerns. On the one hand, we notice choosing a proper down-sampling ratio is important, as the down-sampling operation is a trade-off between these two aspects. On the other hand, we design a Large Kernel Depthwise Separable Convolution block that considers both aspects. For flexibility over different compression qualities, which is the focus of research in recent years, we design a Half Adaptive Instance Normalization-based approach that elegantly integrates information of the Quantization Matrix into the feature map. It adaptively normalizes half of the channels in the Encoder to embed the compression quality information and precise pixel-wise information is preserved through the other half channels. We also design a scalable architecture inspired by prior works to enable a post-training balance between computational cost and restoration performance. Experiments on various datasets show that our network achieves state-of-the-art restoration performance with much fewer parameters and less computational cost.
C1 [Teng, Ge; Jiang, Rongxin; Liu, Xuesong; Zhou, Fan; Chen, Yaowu] Zhejiang Univ, Inst Adv Digital Technol & Instrument, 38 Zheda Rd, Hangzhou 310027, Peoples R China.
   [Jiang, Rongxin; Zhou, Fan] Zhejiang Prov Key Lab Network Multimedia Technol, Hangzhou, Peoples R China.
   [Liu, Xuesong; Chen, Yaowu] Zhejiang Univ, Minist Educ China, Embedded Syst Engn Res Ctr, Hangzhou, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Jiang, RX (corresponding author), Zhejiang Univ, Inst Adv Digital Technol & Instrument, 38 Zheda Rd, Hangzhou 310027, Peoples R China.; Jiang, RX (corresponding author), Zhejiang Prov Key Lab Network Multimedia Technol, Hangzhou, Peoples R China.
EM 12115044@zju.edu.cn; rongxinj@zju.edu.cn
RI Zhou, fan/KIL-4066-2024; Liu, Xue-Song/N-7261-2014
OI Teng, Ge/0000-0002-1331-9868
CR Amaranageswarao G, 2022, VISUAL COMPUT, V38, P31, DOI 10.1007/s00371-020-01998-z
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Cavigelli L, 2017, IEEE IJCNN, P752, DOI 10.1109/IJCNN.2017.7965927
   Chen HG, 2022, IEEE T NEUR NET LEAR, V33, P430, DOI 10.1109/TNNLS.2021.3124370
   Chen HG, 2018, IEEE COMPUT SOC CONF, P824, DOI 10.1109/CVPRW.2018.00114
   Chen HG, 2019, IEEE SIGNAL PROC LET, V26, P79, DOI 10.1109/LSP.2018.2880146
   Chen HG, 2018, NEUROCOMPUTING, V285, P204, DOI 10.1016/j.neucom.2018.01.043
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Ehrlich Max, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P293, DOI 10.1007/978-3-030-58598-3_18
   Foi A, 2007, IEEE T IMAGE PROCESS, V16, P1395, DOI 10.1109/TIP.2007.891788
   Fu XY, 2019, IEEE I CONF COMP VIS, P2501, DOI 10.1109/ICCV.2019.00259
   Galteri L, 2019, IEEE T MULTIMEDIA, V21, P2131, DOI 10.1109/TMM.2019.2895280
   Golestaneh SA, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.1.013018
   Guo J, 2016, LECT NOTES COMPUT SC, V9905, P628, DOI 10.1007/978-3-319-46448-0_38
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jiang JX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4977, DOI 10.1109/ICCV48922.2021.00495
   Joshi P, 2018, VISUAL COMPUT, V34, P1739, DOI 10.1007/s00371-017-1460-z
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kayhan SK, 2016, VISUAL COMPUT, V32, P417, DOI 10.1007/s00371-015-1068-0
   Kim Y, 2020, IEEE T CIRC SYST VID, V30, P1121, DOI 10.1109/TCSVT.2019.2901919
   Kim Y, 2020, IEEE ACCESS, V8, P20160, DOI 10.1109/ACCESS.2020.2968944
   Lee K, 2005, IEEE T IMAGE PROCESS, V14, P36, DOI 10.1109/TIP.2004.838699
   Li BA, 2021, NEUROCOMPUTING, V419, P322, DOI 10.1016/j.neucom.2020.08.056
   Li LD, 2016, NEUROCOMPUTING, V177, P572, DOI 10.1016/j.neucom.2015.11.063
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu PJ, 2018, IEEE COMPUT SOC CONF, P886, DOI 10.1109/CVPRW.2018.00121
   Liu XM, 2015, PROC CVPR IEEE, P5171, DOI 10.1109/CVPR.2015.7299153
   Maas AL., 2013, P ICML WORKSHOP DEEP, V28, P1
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Florentín-Nuñez MN, 2013, NEUROCOMPUTING, V121, P32, DOI 10.1016/j.neucom.2012.10.029
   Rawzor, 2022, IMAGE COMPRESSION BE
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sheikh H.R., Live Image Quality Assessment Database
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Sun MD, 2020, NEUROCOMPUTING, V384, P335, DOI 10.1016/j.neucom.2019.12.015
   Svoboda P., ARXIV
   Wallace G. K., 1991, Communications of the ACM, V34, P30, DOI 10.1145/103085.103089
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZY, 2016, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR.2016.302
   Yim C, 2011, IEEE T IMAGE PROCESS, V20, P88, DOI 10.1109/TIP.2010.2061859
   Yu F., ARXIV
   Yu F, 2018, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2018.00255
   Zhang XS, 2018, IEEE IMAGE PROC, P390, DOI 10.1109/ICIP.2018.8451694
   Zheng BL, 2020, IEEE T CIRC SYST VID, V30, P3982, DOI 10.1109/TCSVT.2019.2931045
   Zheng BL, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.4.043037
   Zini S, 2020, IEEE ACCESS, V8, P63283, DOI 10.1109/ACCESS.2020.2984387
NR 48
TC 2
Z9 2
U1 1
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3033
EP 3053
DI 10.1007/s00371-023-03008-4
EA AUG 2023
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001042487800003
DA 2024-07-18
ER

PT J
AU Chi, P
   Liao, HP
   Zhang, Q
   Wu, XM
   Tian, JY
   Wang, ZM
AF Chi, Peng
   Liao, Haipeng
   Zhang, Qin
   Wu, Xiangmiao
   Tian, Jiyu
   Wang, Zhenmin
TI Online static point cloud map construction based on 3D point clouds and
   2D images
SO VISUAL COMPUTER
LA English
DT Article
DE Mapping; SLAM; Calibration and identification; Reconstruction
ID EXTRINSIC CALIBRATION; VERSATILE; ROBUST
AB With the development of science and technology, robots have been applied to many fields to free people's hands. Environment perception and map construction are one of the key technologies for robots to achieve autonomy. In this paper, a system based on 3D point cloud and 2D image fusion is proposed to solve the problem of dynamic object segmentation and static map construction during robot motion. Different from the existing methods, the current relatively mature target detection method is used to design the extrinsic parameters between the two coordinate systems of the images and the 3D point cloud, and the probabilistic method is used to reduce the error. The above calibration results are applied to map the image detection results to the 3D point cloud to improve the segmentation accuracy of the targets. At the same time, target tracking and filtering methods are used to classify 3D points as static and dynamic. The segmented dynamic points can be applied to obstacle avoidance, while the static points are applied to the construction of a 3D point cloud map. Finally, the open-source datasets KITTI and DAIR-V2X are used to verify the proposed method, and the results show that the method is feasible and superior.
C1 [Chi, Peng; Liao, Haipeng; Wu, Xiangmiao; Tian, Jiyu; Wang, Zhenmin] South China Univ Technol, Sch Mech & Automot Engn, WuShan St, Guangzhou 510641, Guangdong, Peoples R China.
   [Zhang, Qin] South China Univ Technol, Sch Comp Sci & Engn, Xiaoguwei St, Guangzhou 511400, Guangdong, Peoples R China.
C3 South China University of Technology; South China University of
   Technology
RP Tian, JY; Wang, ZM (corresponding author), South China Univ Technol, Sch Mech & Automot Engn, WuShan St, Guangzhou 510641, Guangdong, Peoples R China.
EM mechipeng@mail.scut.edu.cn; haipengliao_scut@163.com;
   cszhangq@scut.edu.cn; xmwu@scut.edu.cn; tianjiyu@scut.edu.com;
   wangzhm@scut.edu.com
RI Chi, Peng/JNS-1114-2023; Liao, Haipeng/JMP-2908-2023; CHI,
   PENG/JNS-1013-2023
OI Chi, Peng/0000-0002-8577-3656; Liao, Haipeng/0000-0003-1950-0016; CHI,
   PENG/0000-0002-8577-3656
CR Arora M, 2023, ROBOT AUTON SYST, V159, DOI 10.1016/j.robot.2022.104287
   Biber P, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P2743, DOI 10.1109/iros.2003.1249285
   Blackman SS, 2004, IEEE AERO EL SYS MAG, V19, P5, DOI 10.1109/MAES.2004.1263228
   Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Cattaneo D, 2022, IEEE T ROBOT, V38, P2074, DOI 10.1109/TRO.2022.3150683
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dhall A., 2017, LIDAR CAMERA CALIBRA
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fu B., 2019, IEEE T INSTRUM MEAS, VPP, P1
   Gao XS, 2003, IEEE T PATTERN ANAL, V25, P930, DOI 10.1109/TPAMI.2003.1217599
   Gao Y, 2023, VISUAL COMPUT, V39, P1137, DOI 10.1007/s00371-021-02393-y
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Iyer G, 2018, IEEE INT C INT ROBOT, P1110, DOI 10.1109/IROS.2018.8593693
   Kim A, 2021, IEEE INT CONF ROBOT, P11315, DOI 10.1109/ICRA48506.2021.9562072
   Kim G, 2020, IEEE INT C INT ROBOT, P10758, DOI 10.1109/IROS45743.2020.9340856
   Kim J, 2022, IEEE ROBOT AUTOM LET, V7, P8044, DOI 10.1109/LRA.2022.3186080
   Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298
   Lee S, 2020, IEEE ACCESS, V8, P175563, DOI 10.1109/ACCESS.2020.3025537
   Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6
   Li YY, 2018, REMOTE SENS LETT, V9, P789, DOI 10.1080/2150704X.2018.1480072
   Lim H, 2021, IEEE ROBOT AUTOM LET, V6, P2272, DOI 10.1109/LRA.2021.3061363
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Milioto A, 2019, IEEE INT C INT ROBOT, P4213, DOI 10.1109/IROS40897.2019.8967762
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Oussalah M, 2002, INFORM SCIENCES, V142, P195, DOI 10.1016/S0020-0255(02)00166-4
   Pagad Shishir, 2020, 2020 IEEE International Conference on Robotics and Automation (ICRA), P10765, DOI 10.1109/ICRA40945.2020.9197168
   Pandey G., 2010, IFAC Proc., V43, P336, DOI 10.3182/20100906-3-IT-2019.00059
   Pandey G, 2015, J FIELD ROBOT, V32, P696, DOI 10.1002/rob.21542
   Pang Z., 2021, SIMPLETRACK UNDERSTA
   Patel AS, 2023, VISUAL COMPUT, V39, P2127, DOI 10.1007/s00371-022-02469-3
   Pomerleau F, 2014, IEEE INT CONF ROBOT, P3712, DOI 10.1109/ICRA.2014.6907397
   Qin T, 2018, IEEE T ROBOT, V34, P1004, DOI 10.1109/TRO.2018.2853729
   Quigley M, 2009, IEEE INT CONF ROBOT, P3604
   Ramachandran S, 2019, 2019 14TH ANNUAL CONFERENCE SYSTEM OF SYSTEMS ENGINEERING (SOSE), P340, DOI [10.1109/SYSOSE.2019.8753827, 10.1109/sysose.2019.8753827]
   Shan TX, 2018, IEEE INT C INT ROBOT, P4758, DOI 10.1109/IROS.2018.8594299
   Sualeh M., 2020, IEEE ACCESS, VPP, P1
   Sun YX, 2020, IEEE ICC, DOI 10.1109/icc40277.2020.9148853
   Tamas L, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P668, DOI 10.1109/ICCVW.2013.92
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang WM, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9080851
   Weng X., 2020, arXiv
   Wojke N., 2017, ARXIV
   Xu W, 2022, IEEE T ROBOT, V38, P2053, DOI 10.1109/TRO.2022.3141876
   Yao ZT, 2023, OCEAN ENG, V272, DOI 10.1016/j.oceaneng.2023.113939
   Yu H., 2022, DAIR V2X LARGE SCALE
   Yuan CJ, 2021, IEEE ROBOT AUTOM LET, V6, P7517, DOI 10.1109/LRA.2021.3098923
   Zhang J., 2014, P ROB SCI SYST RSS 1
   Zhang Q., 2003, EXTRINSIC CALIBRATIO
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
   Zou C, 2018, IET COMPUT VIS, V12, P384, DOI 10.1049/iet-cvi.2017.0308
NR 52
TC 2
Z9 2
U1 7
U2 32
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2889
EP 2904
DI 10.1007/s00371-023-02992-x
EA JUL 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001040330500002
DA 2024-07-18
ER

PT J
AU Si, HP
   Wang, YP
   Liu, Q
   Li, WX
   Wan, L
   Song, JZ
   Zhao, WR
   Sun, CX
AF Si, Haiping
   Wang, Yunpeng
   Liu, Qian
   Li, Weixia
   Wan, Li
   Song, Jiazhen
   Zhao, Wenrui
   Sun, Changxia
TI A registration algorithm for the infrared and visible images of apple
   based on active contour model
SO VISUAL COMPUTER
LA English
DT Article
DE Image registration; Defect detection; Active coutour model; Infrared and
   visible image fusion; Partial hausdorff distance; Contour feature points
ID LEVEL SET EVOLUTION; SEGMENTATION; FEATURES
AB To study the fruit image fusion technology in the case of thermal infrared and visible heterogeneous sources and the method of online defect detection on fruit fusion images, this paper takes apple as the research object and proposes a registration algorithm for thermal infrared and visible images of apple based on the registration of feature points with an active contour model. First, by designing a thermal infrared and visible image acquisition system, thermal infrared and visible images of apple in the same scene are obtained simultaneously. Then, the improved Chan-vese model is adopted to obtain the active contour segmentation curves of the thermal infrared and visible images of apple respectively. Next, the average Euclidean distance of all adjacent edge points on the active contour segmentation curve is calculated, and the alignment feature point set is constructed by the linear interpolation method based on the obtained average distance, and the optimal scale transformation factor and the optimal horizontal transformation factor are obtained by calculating the partial Hausdorff distance between the two feature point sets. Finally, the registered visible image is acquired based on the obtained affine transformation matrix, thus realizing the registration of the thermal infrared and visible images of apple. The experimental results on the self-built image dataset indicate that the algorithm proposed in this paper can accurately match the heterogeneous images of intact fruit, calyx/stems, and defective fruit, and it performs excellently in terms of precise matching rate and root mean square error, and the high alignment success rate of 96%. Also, it has much better performance than other methods. The proposed registration algorithm can accurately match thermal infrared and visible images of apple, and lays the foundation for further research on the image fusion of thermal infrared and visible, apple surface defect detection, as well as the construction of online dual-light apple grading systems.
C1 [Si, Haiping; Wang, Yunpeng; Liu, Qian; Li, Weixia; Wan, Li; Song, Jiazhen; Zhao, Wenrui; Sun, Changxia] Henan Agr Univ, Coll Informat & Management Sci, Zhengzhou 450046, Peoples R China.
C3 Henan Agricultural University
RP Sun, CX (corresponding author), Henan Agr Univ, Coll Informat & Management Sci, Zhengzhou 450046, Peoples R China.
EM sunchang_xia@126.com
RI yan, su/KHT-1728-2024; Sun, Yang/KHY-5117-2024; zou, yao/KCK-8222-2024;
   liu, zhen/KFS-0275-2024; Liu, Zhen/KFS-2748-2024; Liu,
   Chang/KGL-6678-2024; Li, Hongbo/KHV-4191-2024; li, fang/KDO-8841-2024;
   Wang, Siying/KHX-1894-2024; Zhao, Hang/KCL-7278-2024; Wang,
   Yuhan/KGL-5855-2024; Wang, Ling/KBA-9814-2024; Wang, Ling/AGR-4917-2022;
   wang, wang/KGW-2828-2024; Ma, Wei/JXY-5019-2024; Wang,
   Yifan/KDO-8319-2024; Yang, Ning/KHD-1133-2024; wang,
   haoyu/KHY-6295-2024; wang, yue/KDO-9209-2024; chen, huan/KEC-2019-2024
OI Li, Hongbo/0000-0003-4495-0756; Wang, Ling/0000-0003-0272-2974; Wang,
   Ling/0000-0003-0272-2974; Ma, Wei/0000-0002-7344-998X; wang,
   haoyu/0009-0001-2467-5331; , liu qian/0009-0001-6279-9327
CR Baneh NM, 2018, J FOOD MEAS CHARACT, V12, P1135, DOI 10.1007/s11694-018-9728-1
   Baranowski P, 2009, POSTHARVEST BIOL TEC, V53, P91, DOI 10.1016/j.postharvbio.2009.04.006
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291
   Chang HH, 2019, IEEE GEOSCI REMOTE S, V16, P1363, DOI 10.1109/LGRS.2019.2899123
   Chen X, 2020, INFRARED PHYS TECHN, V111, DOI 10.1016/j.infrared.2020.103549
   Chen YJ, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9224838
   Chen YJ, 2018, MACH VISION APPL, V29, P113, DOI 10.1007/s00138-017-0879-6
   Deng JM, 2014, J MED SYST, V38, DOI 10.1007/s10916-014-0040-2
   Dong B, 2021, EXPERT SYST APPL, V177, DOI 10.1016/j.eswa.2021.114948
   Dong YY, 2022, J FOOD PROCESS ENG, V45, DOI 10.1111/jfpe.13981
   Dong YY, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10111719
   Ejaz K, 2022, COMPUT INTEL NEUROSC, V2022, DOI 10.1155/2022/5898479
   Fadiji T, 2016, POSTHARVEST BIOL TEC, V118, P111, DOI 10.1016/j.postharvbio.2016.04.001
   Gan H, 2018, BIOSYST ENG, V174, P89, DOI 10.1016/j.biosystemseng.2018.06.013
   Gao B, 2017, IEEE T SIGNAL PROCES, V65, P5885, DOI 10.1109/TSP.2017.2742981
   Huang CC, 2016, APPL MATH MODEL, V40, P7739, DOI 10.1016/j.apm.2016.03.039
   Jiang Q, 2021, IEEE T POWER DELIVER, V36, P2559, DOI 10.1109/TPWRD.2020.3011962
   Jiang XY, 2021, INFORM FUSION, V73, P22, DOI 10.1016/j.inffus.2021.02.012
   Li CM, 2011, IEEE T IMAGE PROCESS, V20, P2007, DOI 10.1109/TIP.2011.2146190
   Li H, 2020, IEEE T IMAGE PROCESS, V29, P4733, DOI 10.1109/TIP.2020.2975984
   Li H, 2018, INT C PATT RECOG, P2705, DOI 10.1109/ICPR.2018.8546006
   Li J, 2016, COMPUT ELECTRON AGR, V127, P633, DOI 10.1016/j.compag.2016.07.024
   Li L, 2010, IEEE T IMAGE PROCESS, V19, P1, DOI 10.1109/TIP.2009.2032341
   Liang B, 2022, ASTROPHYS J SUPPL S, V261, DOI 10.3847/1538-4365/ac7232
   Liu S, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12062828
   Loeckx D, 2010, IEEE T MED IMAGING, V29, P19, DOI 10.1109/TMI.2009.2021843
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu HF, 2011, J FOOD ENG, V104, P149, DOI 10.1016/j.jfoodeng.2010.12.007
   Lv CZ, 2019, MULTIMED TOOLS APPL, V78, P8875, DOI 10.1007/s11042-018-6580-6
   Ma JY, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01359-2
   Ma JY, 2019, INT J COMPUT VISION, V127, P512, DOI [10.1109/TMAG.2017.2763198, 10.1007/s11263-018-1117-z]
   Miao JQ, 2018, INFORM SCIENCES, V447, P52, DOI 10.1016/j.ins.2018.02.007
   Ministry of Commerce of the People's Republic of China, 1989, GB1065189 MIN COMM P
   Moallem Payman, 2017, Information Processing in Agriculture, V4, P33, DOI 10.1016/j.inpa.2016.10.003
   Moshkov N, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-61808-3
   MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503
   OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2
   Pallotta L, 2020, IEEE T GEOSCI REMOTE, V58, P4132, DOI 10.1109/TGRS.2019.2961245
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Unay D, 2006, POSTHARVEST BIOL TEC, V42, P271, DOI 10.1016/j.postharvbio.2006.06.010
   Valenti M, 2016, MED BIOL ENG COMPUT, V54, P1727, DOI 10.1007/s11517-016-1460-6
   Wang L, 2009, COMPUT MED IMAG GRAP, V33, P520, DOI 10.1016/j.compmedimag.2009.04.010
   Wang L, 2009, SIGNAL PROCESS, V89, P2435, DOI 10.1016/j.sigpro.2009.03.014
   Wang S, 2018, ISPRS J PHOTOGRAMM, V145, P148, DOI 10.1016/j.isprsjprs.2017.12.012
   Wang XZ, 2010, COMPUT ELECTRON AGR, V73, P74, DOI 10.1016/j.compag.2010.04.007
   Wang ZP, 2022, POSTHARVEST BIOL TEC, V185, DOI 10.1016/j.postharvbio.2021.111808
   Yu K, 2021, IEEE J-STARS, V14, P4441, DOI 10.1109/JSTARS.2021.3073573
   Zeng Q, 2020, J REAL-TIME IMAGE PR, V17, P1103, DOI 10.1007/s11554-019-00858-x
   Zeng XY, 2020, POSTHARVEST BIOL TEC, V161, DOI 10.1016/j.postharvbio.2019.111090
   Zhang C, 2017, J FOOD ENG, V203, P69, DOI 10.1016/j.jfoodeng.2017.02.008
   Zhang XP, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13224618
   Zhou Jianmin, 2010, Proceedings of the 2010 International Conference on Intelligent Computation Technology and Automation (ICICTA 2010), P47, DOI 10.1109/ICICTA.2010.568
NR 53
TC 1
Z9 1
U1 16
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2833
EP 2855
DI 10.1007/s00371-023-02989-6
EA JUL 2023
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001035510700003
DA 2024-07-18
ER

PT J
AU Song, YY
   Li, ZC
   Wu, JX
   Song, CY
   Xu, ZW
AF Song, Yuying
   Li, Zecheng
   Wu, Jingxuan
   Song, Chunyi
   Xu, Zhiwei
TI Leveraging front and side cues for occlusion handling in monocular 3D
   object detection
SO VISUAL COMPUTER
LA English
DT Article
DE Monocular object detection; Occlusion Handling; Compositional model;
   Uncertainty; Attention mechanism; Autonomous driving
AB 3D object detection, as an essential part of perception, plays a principal role in the autonomous driving system. The cost-competitive monocular 3D object detection has drawn increasing attention recently. However, it still suffers an inferior accuracy especially for occluded objects due to the limited camera view. Inspired by compositional models, in which an object is represented as a combination of multiple components, this paper proposes a new monocular 3D object detection method that decreases the impact of occlusion by utilizing an object's front and side cues. To do this, the features are extracted from a decoupled front and side representation and then fused by an attention-based module to obtain a more consistent feature distribution. An uncertainty-guided depth ensemble based on geometry is further applied to refine the depth prediction. Experiment results demonstrate that as compared to the conventional methods, the proposed method significantly improves the detection performance for occluded objects while still satisfying real-time efficiency, with the Average Precision on 40 recall positions (AP40), respectively, increasing by 10.23% for partly occluded objects and 12.22% for mostly occluded objects in the KITTI benchmark.
C1 [Song, Yuying; Li, Zecheng; Wu, Jingxuan; Song, Chunyi; Xu, Zhiwei] Zhejiang Univ, Inst Marine Elect & Intelligent Syst, Ocean Coll, Zhoushan 316036, Zhejiang, Peoples R China.
   [Song, Chunyi; Xu, Zhiwei] Minist Educ, Engn Res Ctr Ocean Sensing Technol & Equipment, Zhoushan 316036, Zhejiang, Peoples R China.
   [Song, Chunyi; Xu, Zhiwei] Donghai Lab, Zhoushan 316036, Zhejiang, Peoples R China.
C3 Zhejiang University; Donghai Laboratory
RP Song, CY (corresponding author), Zhejiang Univ, Inst Marine Elect & Intelligent Syst, Ocean Coll, Zhoushan 316036, Zhejiang, Peoples R China.; Song, CY (corresponding author), Minist Educ, Engn Res Ctr Ocean Sensing Technol & Equipment, Zhoushan 316036, Zhejiang, Peoples R China.; Song, CY (corresponding author), Donghai Lab, Zhoushan 316036, Zhejiang, Peoples R China.
EM cysong@zju.edu.cn
RI Song, Yuying/IVV-3947-2023
OI Song, Yuying/0000-0002-7733-2041; song, chunyi/0000-0002-3274-6806
FU Donghai Laboratory [DH-2022ZY0002]
FX This work is supported in part by the project of the Donghai Laboratory
   under Grant DH-2022ZY0002
CR Barabanau I., 2019, ARXIV
   Brazil G, 2019, IEEE I CONF COMP VIS, P9286, DOI 10.1109/ICCV.2019.00938
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Chen Q., 2020, EUROPEAN C COMPUTER
   Chen XZ, 2015, ADV NEUR IN, V28
   Chen Y., 2020, CVPR, P10337
   Cheng T, 2023, VISUAL COMPUT, V39, P6367, DOI 10.1007/s00371-022-02734-5
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He K, 2017, IEEE INT WORKSH MULT
   Huiyu Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P108, DOI 10.1007/978-3-030-58548-8_7
   Ji CF, 2023, VISUAL COMPUT, V39, P4543, DOI 10.1007/s00371-022-02607-x
   Kendall Alex, 2017, ADV NEURAL INFORM PR, V30, DOI DOI 10.5555/3295222.3295309
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar A, 2021, PROC CVPR IEEE, P8969, DOI 10.1109/CVPR46437.2021.00886
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li Peixuan, 2020, EUROPEAN C COMPUTER, P644
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu C, 2019, PROC CVPR IEEE, P10978, DOI 10.1109/CVPR.2019.01124
   Liu H, 2022, IEEE T IMAGE PROCESS, V31, P4050, DOI 10.1109/TIP.2022.3180210
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lu Y., 2021, ARXIV
   Luo SJ, 2021, PROC CVPR IEEE, P6141, DOI 10.1109/CVPR46437.2021.00608
   Ma XZ, 2021, PROC CVPR IEEE, P4719, DOI 10.1109/CVPR46437.2021.00469
   Manhardt F, 2019, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2019.00217
   Mousavian A., 2017, ARXIV
   Ouyang E., 2020, P ASIAN C COMPUTER V
   Peng SD, 2019, PROC CVPR IEEE, P4556, DOI 10.1109/CVPR.2019.00469
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Roddick Thomas, 2018, ARXIV181108188
   Saeidi M, 2022, VISUAL COMPUT, V38, P2223, DOI 10.1007/s00371-021-02280-6
   Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054
   Shi XP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15152, DOI 10.1109/ICCV48922.2021.01489
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang R, 2019, VISUAL COMPUT, V35, P399, DOI 10.1007/s00371-018-1472-3
   Wang T., 2022, PMLR, P1475
   Wang T, 2021, IEEE INT CONF COMP V, P913, DOI 10.1109/ICCVW54120.2021.00107
   Wang Y, 2019, PROC CVPR IEEE, P8437, DOI 10.1109/CVPR.2019.00864
   Wei LX, 2021, VISUAL COMPUT, V37, P133, DOI 10.1007/s00371-019-01787-3
   Wu P, 2023, VISUAL COMPUT, V39, P2425, DOI 10.1007/s00371-022-02672-2
   Xie Z, 2022, MDS NET MULTISCALE D
   Xinzhu Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P311, DOI 10.1007/978-3-030-58601-0_19
   Xu B, 2018, PROC CVPR IEEE, P2345, DOI 10.1109/CVPR.2018.00249
   Zhang T, 2023, VISUAL COMPUT, V39, P3865, DOI 10.1007/s00371-022-02528-9
   Zhang YP, 2021, PROC CVPR IEEE, P3288, DOI 10.1109/CVPR46437.2021.00330
   Zhao HS, 2022, VISUAL COMPUT, V38, P3765, DOI 10.1007/s00371-021-02217-z
NR 50
TC 0
Z9 0
U1 5
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1757
EP 1773
DI 10.1007/s00371-023-02884-0
EA JUN 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:001019061000001
DA 2024-07-18
ER

PT J
AU Turhal, U
   Yilmaz, AG
   Nabiyev, V
AF Turhal, Ugur
   Yilmaz, Asuman Gunay
   Nabiyev, Vasif
TI A new face presentation attack detection method based on face-weighted
   multi-color multi-level texture features
SO VISUAL COMPUTER
LA English
DT Article
DE Face recognition; Spoofing; Presentation attack detection; Color texture
   analysis; Local binary pattern
ID IMAGE QUALITY ASSESSMENT; SPOOFING DETECTION; ROBUST
AB Biometric data (facial, voice, fingerprint, and retinal scans, for example) are widely used in identification due to their unique and irreversible nature. Facial recognition technologies are employed in a wide range of applications due to their contactless nature and convenience. However, technological advancements and the availability of access to personal information have rendered these biometric systems susceptible to attacks utilizing fake faces. As a result, the issue of anti-spoofing has emerged as a critical one in the field of facial recognition. This study proposes a joint face presentation attack (FPA) detection method based on face-weighted multi-color multi-level LBP features extracted from the combination of device-dependent HSV and device-independent L*a*b* color spaces. The facial images were converted to HSV and L*a*b* color spaces. Three levels of regional LBP features were extracted from each color channel and then concatenated. Finally, a Multi-Color Multi-Level LBP (MCML_LBP) feature vector was obtained. In addition, the Face Weighted MCML_LBP feature vector was produced (FW_MCML_LBP) by adding the LBP histogram extracted from the central region of the normalized image. The feature vectors are used to train an SVM classifier after reducing their size using PCA. Twenty-five different test scenarios were subjected to experimentation on the CASIA and Replay-Attack databases. 2.11% EER and 0.19% HTER were achieved on CASIA (Overall) and Replay-Attack (Grandtest) databases, respectively, using the L*a*b color space and the proposed feature extraction method. The results of the study showed that the proposed method was successful in FPA detection compared to the state-of-the-art methods.
C1 [Turhal, Ugur] Bayburt Univ, Bayburt, Turkiye.
   [Yilmaz, Asuman Gunay; Nabiyev, Vasif] Karadeniz Tech Univ, Trabzon, Turkiye.
C3 Bayburt University; Karadeniz Technical University
RP Turhal, U (corresponding author), Bayburt Univ, Bayburt, Turkiye.
EM uturhal@bayburt.edu.tr
RI Günay Yılmaz, Asuman/AAR-5221-2020; TURHAL, Uğur/ABC-7688-2020
OI Günay Yılmaz, Asuman/0000-0003-3960-5085; TURHAL,
   UGUR/0000-0002-5627-1833
CR Abdullakutty F, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22145196
   Alotaibi A, 2017, SIGNAL IMAGE VIDEO P, V11, P713, DOI 10.1007/s11760-016-1014-2
   Anjos A, 2014, IET BIOMETRICS, V3, P147, DOI 10.1049/iet-bmt.2012.0071
   Arora S, 2022, VISUAL COMPUT, V38, P2461, DOI 10.1007/s00371-021-02123-4
   Bengio S., 2002, Information Fusion, V3, P267, DOI 10.1016/S1566-2535(02)00089-1
   Bora D.J., 2015, COMP PERFORMANCE L A
   Boulkenafet Z, 2018, IMAGE VISION COMPUT, V77, P1, DOI 10.1016/j.imavis.2018.04.007
   Boulkenafet Z, 2017, IEEE SIGNAL PROC LET, V24, P141, DOI 10.1109/LSP.2016.2630740
   Boulkenafet Z, 2016, IEEE T INF FOREN SEC, V11, P1818, DOI 10.1109/TIFS.2016.2555286
   Boulkenafet Z, 2015, IEEE IMAGE PROC, P2636, DOI 10.1109/ICIP.2015.7351280
   Chang HH, 2022, IMAGE VISION COMPUT, V121, DOI 10.1016/j.imavis.2022.104428
   Chingovska I., 2012, 2012 BIOSIG P INT C, P1
   Croux C, 2013, TECHNOMETRICS, V55, P202, DOI 10.1080/00401706.2012.727746
   Galbally J, 2014, IEEE T IMAGE PROCESS, V23, P710, DOI 10.1109/TIP.2013.2292332
   Gunay A., 2017, T RKIYE BILI IM VAKF, V9, P1
   Gunay A., 2015, FACIAL AGE ESTIMATIO
   Gunay Yilmaz A., 2020, EFFECT FEATURE SELEC, V5, P48
   He D, 2023, VISUAL COMPUT, V39, P1423, DOI 10.1007/s00371-022-02420-6
   Huang ZK, 2007, 2007 INTERNATIONAL CONFERENCE ON INFORMATION ACQUISITION, VOLS 1 AND 2, P317
   Imaoka H, 2021, APSIPA TRANS SIGNAL, V10, DOI 10.1017/ATSIP.2021.8
   Junwei Zhou, 2020, 2020 25th International Conference on Pattern Recognition (ICPR), P4221, DOI 10.1109/ICPR48806.2021.9412323
   Kavzoglu T., 2010, DESTEK VEKTOR MAKINE, P73
   Khurshid A, 2019, LECT NOTES COMPUT SC, V11786, P484, DOI 10.1007/978-3-030-30033-3_37
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Li HL, 2018, IEEE T INF FOREN SEC, V13, P2639, DOI 10.1109/TIFS.2018.2825949
   Li L, 2018, J VIS COMMUN IMAGE R, V54, P182, DOI 10.1016/j.jvcir.2018.05.009
   Li L, 2017, IEEE IMAGE PROC, P101, DOI 10.1109/ICIP.2017.8296251
   Murali S, 2013, CYBERN INF TECHNOL, V13, P95, DOI 10.2478/cait-2013-0009
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Parveen S, 2016, COMPUTERS, V5, DOI 10.3390/computers5020010
   Patel K, 2016, IEEE T INF FOREN SEC, V11, P2268, DOI 10.1109/TIFS.2016.2578288
   Peng F, 2018, MULTIMED TOOLS APPL, V77, P8883, DOI 10.1007/s11042-017-4780-0
   Pereira TD, 2014, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2014-2
   Shepley A.J., 2019, ARXIV
   Shu X, 2021, MULTIMEDIA SYST, V27, P161, DOI 10.1007/s00530-020-00719-9
   Trigueros DS, 2018, ARXIV
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wen D, 2015, IEEE T INF FOREN SEC, V10, P746, DOI 10.1109/TIFS.2015.2400395
   Zhang LB, 2018, J VIS COMMUN IMAGE R, V51, P56, DOI 10.1016/j.jvcir.2018.01.001
   Zhang Y.-J., 2022, TAIWAN UBIQUITOUS IN
   Zhang Z., 2012, P 2012 5 IAPR INT C, P2
NR 42
TC 1
Z9 1
U1 3
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1537
EP 1552
DI 10.1007/s00371-023-02866-2
EA MAY 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000994983400001
DA 2024-07-18
ER

PT J
AU Tong, GF
   Shao, YY
   Peng, H
AF Tong, Guofeng
   Shao, Yuyuan
   Peng, Hao
TI Learning local contextual features for 3D point clouds semantic
   segmentation by attentive kernel convolution
SO VISUAL COMPUTER
LA English
DT Article
DE 3D point clouds; Semantic segmentation; Deep learning
AB Unlike 2D images that are represented in regular grids, 3D point clouds are irregular and unordered, hence directly applying convolution neural networks (CNNs) to process point clouds is quite challenging. In this paper, we propose a novel deep neural network named AKNet to achieve point cloud semantic segmentation. The key to our AKNet is the attentive kernel convolution (AKConv), which is a deformed convolution operation for perceiving sufficient local context of 3D scenes. AKConv first constructs the Basic Weight Units that are robust to point's ordering. Then, for capturing the more distinctive local features, the convolution kernels of AKConv are associated with Attentive Weight Units through the self-attentive function acting on Basic Weight Units. Furthermore, 3D point clouds provide richer geometric shape information, which is helpful to recognize objects. However, inputting only raw point features to the convolution function could cause geometric information loss. Thus, we utilize augmented features as input of AKConv. Besides, to preserve the semantic information from the encoding to decoding layers, we introduce the backward encoding (BE) mechanism by utilizing higher-layer semantic features. We conduct experiments on three large-scale point clouds datasets. The experimental results demonstrate that our AKNet outperforms state-of-the-art (SOTA) networks.
C1 [Tong, Guofeng; Shao, Yuyuan; Peng, Hao] Northeastern Univ, Coll Informat Sci & Engn, Shenyang 110819, Liaoning, Peoples R China.
C3 Northeastern University - China
RP Shao, YY (corresponding author), Northeastern Univ, Coll Informat Sci & Engn, Shenyang 110819, Liaoning, Peoples R China.
EM ShaoyuyuanNEU@126.com
OI Yuyuan, Shao/0000-0002-0747-6058
FU National Key R&D Program of China [2019YFB1309905, 2020YFB1712802]
FX This work is supported by National Key R&D Program of China (Nos.
   2019YFB1309905, 2020YFB1712802).
CR [Anonymous], 2011, P ADV NEURAL INFORM
   Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Boulch A., 2017, P WORKSH 3D OBJ RETR, V3, P1
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Fan SQ, 2021, PROC CVPR IEEE, P14499, DOI 10.1109/CVPR46437.2021.01427
   Gong JY, 2021, PROC CVPR IEEE, P11668, DOI 10.1109/CVPR46437.2021.01150
   Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961
   Hackel T., 2017, ISPRS ANN PHOTOGRAMM, VIV-1/W1, P91
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu Q., 2021, ARXIV
   Hu QY, 2021, PROC CVPR IEEE, P4975, DOI 10.1109/CVPR46437.2021.00494
   Jiang M., 2018, ARXIV180700652
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Kumar N, 2020, VISUAL COMPUT, V36, P1809, DOI 10.1007/s00371-019-01777-5
   Lan SY, 2019, PROC CVPR IEEE, P998, DOI 10.1109/CVPR.2019.00109
   Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479
   Li HY, 2021, VISUAL COMPUT, V37, P325, DOI 10.1007/s00371-020-01801-z
   Li RK, 2021, NEUROCOMPUTING, V429, P187, DOI 10.1016/j.neucom.2020.10.086
   Li YY, 2018, ADV NEUR IN, V31
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qi CR, 2020, PROC CVPR IEEE, P4403, DOI 10.1109/CVPR42600.2020.00446
   Qingyong Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11105, DOI 10.1109/CVPR42600.2020.01112
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shao YY, 2022, NEUROCOMPUTING, V500, P191, DOI 10.1016/j.neucom.2022.05.060
   Shi WL, 2021, VISUAL COMPUT, V37, P1569, DOI 10.1007/s00371-020-01903-8
   Shu XB, 2022, IEEE T CIRC SYST VID, V32, P5281, DOI 10.1109/TCSVT.2022.3142771
   Shu XB, 2022, IEEE T PATTERN ANAL, V44, P3300, DOI 10.1109/TPAMI.2021.3050918
   Simonovsky M, 2017, PROC CVPR IEEE, P29, DOI 10.1109/CVPR.2017.11
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   [唐昀超 Tang Yunchao], 2022, [实验力学, Journal of Experimental Mechanics], V37, P209
   Tang YC, 2019, ROBOT CIM-INT MANUF, V59, P36, DOI 10.1016/j.rcim.2019.03.001
   Tatarchenko M, 2018, PROC CVPR IEEE, P3887, DOI 10.1109/CVPR.2018.00409
   Tateno K, 2015, IEEE INT C INT ROBOT, P4465, DOI 10.1109/IROS.2015.7354011
   Tchapmi LP, 2017, INT CONF 3D VISION, P537, DOI 10.1109/3DV.2017.00067
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Thomas H, 2018, INT CONF 3D VISION, P390, DOI 10.1109/3DV.2018.00052
   Wang L, 2019, PROC CVPR IEEE, P10288, DOI 10.1109/CVPR.2019.01054
   Wang PZ, 2022, ISPRS J PHOTOGRAMM, V188, P237, DOI 10.1016/j.isprsjprs.2022.04.016
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu J, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P971, DOI 10.1145/3343031.3351076
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Xu BQ, 2022, IEEE T IMAGE PROCESS, V31, P3852, DOI 10.1109/TIP.2022.3175605
   Xu MT, 2021, PROC CVPR IEEE, P3172, DOI 10.1109/CVPR46437.2021.00319
   Zhang ZY, 2019, IEEE I CONF COMP VIS, P1607, DOI 10.1109/ICCV.2019.00169
   Zhao HS, 2019, PROC CVPR IEEE, P5550, DOI 10.1109/CVPR.2019.00571
NR 48
TC 1
Z9 1
U1 19
U2 28
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 831
EP 847
DI 10.1007/s00371-023-02819-9
EA MAR 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000953235100001
DA 2024-07-18
ER

PT J
AU Song, S
   Huang, TC
   Zhu, QY
   Hu, HS
AF Song, Shuang
   Huang, Tengchao
   Zhu, Qingyuan
   Hu, Huosheng
TI ODSPC: deep learning-based 3D object detection using semantic point
   cloud
SO VISUAL COMPUTER
LA English
DT Article
DE Object detection; Semantic segmentation; Point cloud classification;
   Fused data; Extended Kalman filter
ID TRACKING
AB Three-dimensional object detection plays a key role in autonomous driving, which becomes extremely challenging in occlusion situations. This paper presents a novel multimodal 3D object detection framework which fuses visual semantic information and depth point cloud information to accurately detect targets with distant object features and occlusion situations. The framework consists of the four steps. Firstly, an improved semantic segmentation network is used to extract semantic information of objects containing similar features. Secondly, semantic images and point clouds are combined to generate pixel-level fusion data so that the semantic information and training capability of sparse and far-point clouds can be improved. Thirdly, a deep learning-based point cloud classification network is used for training of the fused data to output accurate detection frames. Fourthly, an extended Kalman filter is incorporated into point cloud prediction for image-based object detection to further enhance the robustness of object detection. Both Cityscapes and KITTI datasets are used in ablation study and experiments to validate the effectiveness of the proposed framework.
C1 [Song, Shuang; Huang, Tengchao; Zhu, Qingyuan] Xiamen Univ, Dept Mech & Elect Engn, Xiamen 361005, Peoples R China.
   [Hu, Huosheng] Univ Essex, Sch Comp Sci & Elect Engn, Colchester CO4 3SQ, Essex, England.
C3 Xiamen University; University of Essex
RP Zhu, QY (corresponding author), Xiamen Univ, Dept Mech & Elect Engn, Xiamen 361005, Peoples R China.
EM zhuqy@xmu.edu.cn
RI Hu, Huosheng/G-1758-2010
FU National Natural Science Foundation of China [52075461]; Key Project in
   Science and Technology Plan of Xiamen, China [3502Z20201015]; Innovation
   Method Special Project of Ministry of Science and Technology of China
   [2020IM010100]
FX This work was funded by the National Natural Science Foundation of China
   (Grant No.52075461), the Key Project in Science and Technology Plan of
   Xiamen, China (Grant No. 3502Z20201015), and the Innovation Method
   Special Project of Ministry of Science and Technology of China (Grant
   No. 2020IM010100).
CR Asvadi A, 2016, ROBOT AUTON SYST, V83, P299, DOI 10.1016/j.robot.2016.06.007
   Bai X., 2022, arXiv
   Bello SA, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12111729
   Cao LY, 2021, INT J REMOTE SENS, V42, P5253, DOI 10.1080/01431161.2021.1910371
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen L, 2017, IEEE T INTELL TRANSP, V18, P3093, DOI 10.1109/TITS.2017.2680538
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dames PM, 2020, AUTON ROBOT, V44, P673, DOI 10.1007/s10514-019-09840-9
   Du XX, 2018, IEEE INT CONF ROBOT, P3194
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   González D, 2016, IEEE T INTELL TRANSP, V17, P1135, DOI 10.1109/TITS.2015.2498841
   Huang TT, 2020, Arxiv, DOI arXiv:2007.08856
   Jafari OH, 2014, IEEE INT CONF ROBOT, P5636, DOI 10.1109/ICRA.2014.6907688
   Kesten R., 2019, Lyft level 5 perception dataset 2020
   Ku J, 2018, IEEE INT C INT ROBOT, P5750, DOI 10.1109/IROS.2018.8594049
   Maiettini E, 2020, AUTON ROBOT, V44, P739, DOI 10.1007/s10514-019-09894-9
   Mao JG, 2022, Arxiv, DOI arXiv:2206.09474
   Mao QC, 2020, APPL INTELL, V50, P3125, DOI 10.1007/s10489-020-01704-5
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Wang H, 2019, J SENSORS, V2019, DOI 10.1155/2019/8473980
   Wang YN, 2021, VISUAL COMPUT, V37, P1467, DOI 10.1007/s00371-020-01882-w
   Wang ZJ, 2021, IEEE INT CONF COMP V, P3113, DOI 10.1109/ICCVW54120.2021.00347
   Wu P, 2023, VISUAL COMPUT, V39, P2425, DOI 10.1007/s00371-022-02672-2
   Xie Q, 2021, INT J COMPUT VISION, V129, P1857, DOI 10.1007/s11263-021-01456-w
   Xu DF, 2018, PROC CVPR IEEE, P244, DOI 10.1109/CVPR.2018.00033
   Yan Z, 2020, AUTON ROBOT, V44, P147, DOI 10.1007/s10514-019-09883-y
   Yi CL, 2019, P I MECH ENG D-J AUT, V233, P2293, DOI 10.1177/0954407019867492
   Yuan JY, 2021, IEEE SENS J, V21, P11522, DOI 10.1109/JSEN.2020.3025613
   Zetong Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11037, DOI 10.1109/CVPR42600.2020.01105
   Zhang XQ, 2021, VISUAL COMPUT, V37, P1089, DOI 10.1007/s00371-020-01854-0
   Zhao JX, 2019, TRANSPORT RES C-EMER, V100, P68, DOI 10.1016/j.trc.2019.01.007
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
NR 36
TC 1
Z9 1
U1 19
U2 43
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 849
EP 863
DI 10.1007/s00371-023-02820-2
EA MAR 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000949600400001
DA 2024-07-18
ER

PT J
AU Hu, HJ
   Shen, DY
   Yan, S
   He, F
   Dong, JX
AF Hu, Haojie
   Shen, Danyao
   Yan, Shuai
   He, Fang
   Dong, Jiaxin
TI Ensemble graph Laplacian-based anomaly detector for hyperspectral
   imagery
SO VISUAL COMPUTER
LA English
DT Article
DE Anomaly detection; Hyperspectral image; Graph Laplacian; Ensemble
   learning
ID RX-ALGORITHM
AB Hyperspectral anomaly detection is an alluring topic in hyperspectral image processing. As one of the most famous hyperspectral anomaly detection algorithms, Reed-Xiaoli detector is widely studied since it is understandable and easy to implement. However, the estimation of inverse covariance matrix may be time-consuming and easily corrupted by the anomalies. To solve these problems, we propose a novel ensemble graph Laplacian-based anomaly detector which comprises two main steps. Firstly, a multiple random sampling strategy is applied to improve the detection accuracies and robustness. Secondly, we can obtain multiple detection results through a graph Laplacian-based solution, and these results are further fused through ensemble learning. Experimental results on one simulated and two real hyperspectral datasets demonstrate the superiority of the proposed method.
C1 [Hu, Haojie; Shen, Danyao; Yan, Shuai; He, Fang; Dong, Jiaxin] Xian Res Inst High Technol, Xian 710025, Peoples R China.
   [Yan, Shuai] Natl Univ Def Technol, Sch Informat & Commun, Wuhan 430019, Peoples R China.
C3 Rocket Force University of Engineering; National University of Defense
   Technology - China
RP He, F (corresponding author), Xian Res Inst High Technol, Xian 710025, Peoples R China.
EM haojiehu705@gmail.com; danyao.shn@gmail.com; yanshuai@nudt.edu.cn;
   fanghe1107@gmail.com; djx9525@qq.com
CR Bioucas-Dias JM, 2012, IEEE J-STARS, V5, P354, DOI 10.1109/JSTARS.2012.2194696
   Carlotto MJ, 2005, IEEE T GEOSCI REMOTE, V43, P374, DOI 10.1109/TGRS.2004.841481
   Chang CI, 2002, IEEE T GEOSCI REMOTE, V40, P1314, DOI 10.1109/TGRS.2002.800280
   Cui YM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8118, DOI 10.1109/ICCV48922.2021.00803
   Dua Y, 2022, VISUAL COMPUT, V38, P65, DOI 10.1007/s00371-020-02000-6
   Grady L., 2010, Discrete calculus: Applied analysis on graphs for computational science
   Hou ZF, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-020-2915-2
   Kang XD, 2017, IEEE T GEOSCI REMOTE, V55, P5600, DOI 10.1109/TGRS.2017.2710145
   Kerekes J, 2008, IEEE GEOSCI REMOTE S, V5, P251, DOI 10.1109/LGRS.2008.915928
   Kwon H, 2005, IEEE T GEOSCI REMOTE, V43, P388, DOI 10.1109/TGRS.2004.841487
   Li W, 2015, IEEE T GEOSCI REMOTE, V53, P1463, DOI 10.1109/TGRS.2014.2343955
   Liu DF, 2021, PROC CVPR IEEE, P9811, DOI 10.1109/CVPR46437.2021.00969
   Liu DF, 2021, AAAI CONF ARTIF INTE, V35, P6101
   Liu SC, 2019, IEEE GEOSC REM SEN M, V7, P140, DOI 10.1109/MGRS.2019.2898520
   Lu YH, 2023, SIGNAL PROCESS, V204, DOI 10.1016/j.sigpro.2022.108835
   Molero JM, 2013, IEEE J-STARS, V6, P801, DOI 10.1109/JSTARS.2013.2238609
   Nasrabadi NM, 2014, IEEE SIGNAL PROC MAG, V31, P34, DOI 10.1109/MSP.2013.2278992
   Qu JJ, 2021, IEEE T CYBERNETICS, V51, P4661, DOI [10.1109/TCYB.2020.3004851, 10.1109/TGRS.2020.3038722]
   REED IS, 1990, IEEE T ACOUST SPEECH, V38, P1760, DOI 10.1109/29.60107
   Romano JM, 2007, PROC SPIE, V6565, DOI 10.1117/12.719082
   Stein DWJ, 2002, IEEE SIGNAL PROC MAG, V19, P58, DOI 10.1109/79.974730
   Verdoja F, 2020, MACH VISION APPL, V31, DOI 10.1007/s00138-020-01059-4
   Wang R, 2020, SIGNAL PROCESS, V177, DOI 10.1016/j.sigpro.2020.107718
   Wang R, 2019, IEEE T GEOSCI REMOTE, V57, P7352, DOI 10.1109/TGRS.2019.2913004
   Wang W., 2022, PREPRINT
   Zhang C, 2013, IEEE SIGNAL PROC LET, V20, P106, DOI 10.1109/LSP.2012.2230165
   Zhang YX, 2016, IEEE T GEOSCI REMOTE, V54, P1376, DOI 10.1109/TGRS.2015.2479299
   Zikiou N, 2020, VISUAL COMPUT, V36, P1473, DOI 10.1007/s00371-019-01753-z
NR 28
TC 1
Z9 1
U1 5
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 201
EP 209
DI 10.1007/s00371-023-02775-4
EA MAR 2023
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000945767400002
DA 2024-07-18
ER

PT J
AU Chen, YT
   Xia, RL
   Yang, K
   Zou, K
AF Chen, Yuantao
   Xia, Runlong
   Yang, Kai
   Zou, Ke
TI MFFN: image super-resolution via multi-level features fusion network
SO VISUAL COMPUTER
LA English
DT Article
DE Residual learning; Multi-level features; Super-resolution; Convolutional
   neural network; Lightweight
AB Deep convolutional neural networks can effectively improve the performance of single-image super-resolution reconstruction. Deep networks tend to achieve better performance than others. However, the deep CNNs will lead to a dramatic increase in the size of parameters, limiting its application on embedding and resource-constrained devices, such as smart phone. To address the common problems of blurred image edges, inflexible convolution kernel size selection and slow convergence during training procedure due to redundant network structure in image super-resolution algorithms, this paper proposes a lightweight single-image super-resolution network that fusesmulti-level features. The components are mainly two-level nested residual blocks. To better extract features and reduce the number of parameters, each residual block adopts an asymmetric structure. Firstly, it expands twice and then compresses the number of channels twice. Secondly, in the residual block, the feature information of different channels is weighted and fused by adding an autocorrelation weight unit. The quality of the reconstructed image of the proposed method is superior to the existing image super-resolution reconstruction methods in both subjective perception and objective evaluation indicators, and the reconstruction performance is better when the factor is large.
C1 [Chen, Yuantao] Hunan Univ Informat Technol, Sch Comp Sci & Engn, Changsha, Hunan, Peoples R China.
   [Xia, Runlong] Mt Yuelu Breeding Innovat Ctr Ltd, Changsha, Peoples R China.
   [Xia, Runlong] Hunan Prov Sci & Technol Affairs Ctr, Changsha, Hunan, Peoples R China.
   [Yang, Kai] Hunan ZOOMLION Intelligent Technology Corp Ltd, Changsha, Hunan, Peoples R China.
   [Zou, Ke] Hunan WUJO High Tech Mat Corp Ltd, Loudi, Peoples R China.
RP Chen, YT (corresponding author), Hunan Univ Informat Technol, Sch Comp Sci & Engn, Changsha, Hunan, Peoples R China.
EM chenyt@hnuit.edu.cn; xiarunlong@vip.qq.com; yangkai@zoomlion.com;
   zouk@hnwjgk.cn
RI Chen, Yuantao/AAC-7165-2019
OI Chen, Yuantao/0000-0003-2277-1765
FU Natural Science Foundation of Hunan Province of China [2020JJ4623];
   Changsha Major Science and Technology Projects [KQ2102007, KQ1703018,
   KQ1706064]; Scientific Research Fund of Hunan
   ProvincialEducationDepartment [22A0701]; Scientific Research Project of
   Hunan University of Information Technology [XXY02ZD01]; College
   Students' Innovative Entrepreneurial Training Plan Program of Hunan
   University of Information Technology [X202213836002]; Smart
   Manufacturing Barcode Traceability Management System [20224301020010,
   CON202204070272]; University-Industry Collaborative Education Program
   [202102536008, 221003279124130]; China University Innovation Funding
   -Beslin Smart Education Project [2022BL055]; Teaching Reform Research
   Fund of Hunan Province General Higher Education Schools
   [HNJG-2022-1335]; 2022 Part-time Vice President of Science and
   Technology of Changsha Enterprises from Changsha Science and Technology
   Bureau
FX This work is supported by the Natural Science Foundation of Hunan
   Province of China under Grant 2020JJ4623, Changsha Major Science and
   Technology Projects under Grant KQ2102007, KQ1703018, KQ1706064, A
   Project Supported by Scientific Research Fund of Hunan
   ProvincialEducationDepartment underGrant 22A0701, Scientific Research
   Project of Hunan University of Information Technology under Grant
   XXY02ZD01, College Students' Innovative Entrepreneurial Training Plan
   Program of Hunan University of Information Technology under Grant
   X202213836002, Smart Manufacturing Barcode Traceability Management
   System under Grant 20224301020010 and CON202204070272 (Hunan WUJO
   High-Tech Material Corporation Limited), University-Industry
   Collaborative Education Program under Grant 202102536008 and
   221003279124130, China University Innovation Funding -Beslin Smart
   Education Project under Grant 2022BL055, A Project Supported by Teaching
   Reform Research Fund of Hunan Province General Higher Education Schools
   under Grant HNJG-2022-1335 and 2022 Part-time Vice President of Science
   and Technology of Changsha Enterprises from Changsha Science and
   Technology Bureau.
CR Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chen L.Y., P NTIRE 2022 CHALLEN
   Chen YT, 2021, APPL INTELL, V51, P4367, DOI 10.1007/s10489-020-02116-1
   Chen YT, 2021, VISUAL COMPUT, V37, P1691, DOI 10.1007/s00371-020-01932-3
   Dai Y, 2020, IMAGE VISION COMPUT, V93, DOI 10.1016/j.imavis.2019.10.007
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D. P., 2014, arXiv
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li JC, 2018, LECT NOTES COMPUT SC, V11212, P527, DOI 10.1007/978-3-030-01237-3_32
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Li ZY, 2022, IEEE COMPUT SOC CONF, P832, DOI 10.1109/CVPRW56347.2022.00099
   Liang J., 2022, P IEEE CVF C COMP VI, P1
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Luo YM, 2017, IEEE GEOSCI REMOTE S, V14, P2398, DOI 10.1109/LGRS.2017.2766204
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Mou C., P 2022 EUROPEAN C CO
   Salimans T, 2016, ADV NEUR IN, V29
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shi W., 2013, LECT NOTES COMPUT SC, P916
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xia RL, 2022, J KING SAUD UNIV-COM, V34, P6008, DOI 10.1016/j.jksuci.2022.02.004
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhu FY, 2019, IEEE INT CONF COMP V, P2453, DOI 10.1109/ICCVW.2019.00300
   Zou WWW, 2012, IEEE T IMAGE PROCESS, V21, P327, DOI 10.1109/TIP.2011.2162423
NR 40
TC 65
Z9 66
U1 49
U2 111
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 489
EP 504
DI 10.1007/s00371-023-02795-0
EA FEB 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000933559800001
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Xu, JJ
   Zhu, YS
   Wang, WQ
   Liu, GC
AF Xu, JieJie
   Zhu, Yisheng
   Wang, Wenqing
   Liu, Guangcan
TI A real-time semi-dense depth-guided depth completion network
SO VISUAL COMPUTER
LA English
DT Article
DE Depth completion; Neural networks; Multi-modal fusion
ID SPARSE; RECONSTRUCTION; PROPAGATION
AB Depth completion, the task of predicting dense depth maps from given depth maps of sparse, is an important topic in computer vision. To cope with the task, both traditional image processing- based and data-driven deep learning-based algorithms have been established in the literature. In general, traditional algorithms, built upon non-learnable methods such as interpolation and custom kernels, can handle well flat regions but may blunt sharp edges. Deep learning-based algorithms, despite their strengths in many aspects, still have several limits, e.g., their performance depends heavily on the quality of the given sparse maps, and the dense maps they produced may contain artifacts and are often poor in terms of geometric consistency. To tackle these issues, in this work we propose a simple yet effective algorithm that aims to combine the strengths of both the traditional image processing techniques and the prevalent deep learning methods. Namely, given a sparse depth map, our algorithm first generates a semi-dense map and a 3D pose map using the adaptive densification module (ADM) and the coordinate projection module (CPM), respectively, and then input the obtained maps into a two-branch convolutional neural network so as to produce the final dense depth map. The proposed algorithm is evaluated on both challenging outdoor dataset: KITTI and indoor dataset: NYUv2, the experimental results show that our method performs better than some existing methods.
C1 [Xu, JieJie; Zhu, Yisheng; Wang, Wenqing] Nanjing Univ Informat Sci & Technol, Sch Automat, Nanjing 210044, Peoples R China.
   [Liu, Guangcan] Southeast Univ, Sch Automat, Nanjing 210018, Peoples R China.
C3 Nanjing University of Information Science & Technology; Southeast
   University - China
RP Xu, JJ (corresponding author), Nanjing Univ Informat Sci & Technol, Sch Automat, Nanjing 210044, Peoples R China.
EM 20201222026@nuist.edu.cn
RI Zhu, Yisheng/GYA-3445-2022; Liu, Guangcan/J-1391-2014
OI Liu, Guangcan/0000-0002-9428-4387
CR Cheng XJ, 2020, IEEE T PATTERN ANAL, V42, P2361, DOI 10.1109/TPAMI.2019.2947374
   Du W., 2022, ARXIV
   Eldesokey A., 2018, ARXIV
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gu JQ, 2021, IEEE ROBOT AUTOM LET, V6, P1808, DOI 10.1109/LRA.2021.3060396
   Hawe S, 2011, IEEE I CONF COMP VIS, P2126, DOI 10.1109/ICCV.2011.6126488
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu M, 2021, IEEE INT CONF ROBOT, P13656, DOI 10.1109/ICRA48506.2021.9561035
   Huang ZX, 2020, IEEE T IMAGE PROCESS, V29, P3429, DOI 10.1109/TIP.2019.2960589
   Imran S, 2021, PROC CVPR IEEE, P2583, DOI 10.1109/CVPR46437.2021.00261
   Jaritz M, 2018, INT CONF 3D VISION, P52, DOI 10.1109/3DV.2018.00017
   Krauss B, 2021, IEEE INT VEH SYM, P824, DOI 10.1109/IV48863.2021.9575867
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Ku J, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P16, DOI 10.1109/CRV.2018.00013
   Li A, 2020, IEEE WINT CONF APPL, P32, DOI 10.1109/WACV45572.2020.9093407
   Liu LK, 2015, IEEE T IMAGE PROCESS, V24, P1983, DOI 10.1109/TIP.2015.2409551
   Liu L, 2021, POSTGRAD MED, V133, P265, DOI 10.1080/00325481.2020.1803666
   Liu Q, 2022, VISUAL COMPUT, V38, P3341, DOI 10.1007/s00371-022-02550-x
   Lopez-Rodriguez A., 2020, P ASIAN C COMPUTER V
   Ma FC, 2019, IEEE INT CONF ROBOT, P3288, DOI [10.1109/ICRA.2019.8793637, 10.1109/icra.2019.8793637]
   Mo H., 2022, VISUAL COMPUT, P1
   Mu TJ, 2014, VISUAL COMPUT, V30, P833, DOI 10.1007/s00371-014-0961-2
   Qiu JX, 2019, PROC CVPR IEEE, P3308, DOI 10.1109/CVPR.2019.00343
   Schuster R, 2021, IEEE WINT CONF APPL, P197, DOI 10.1109/WACV48630.2021.00024
   Shivakumar SS, 2019, IEEE INT C INTELL TR, P13, DOI [10.1109/ITSC.2019.8917294, 10.1109/itsc.2019.8917294]
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Uhrig J, 2017, INT CONF 3D VISION, P11, DOI 10.1109/3DV.2017.00012
   van Gent W, 2020, INT J HOUS POLICY, V20, P156, DOI 10.1080/19491247.2019.1682234
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang BY, 2022, AUTOMAT CONSTR, V133, DOI 10.1016/j.autcon.2021.103997
   Wang Y, 2019, PROC CVPR IEEE, P8063, DOI 10.1109/CVPR.2019.00826
   Wang YK, 2014, VISUAL COMPUT, V30, P1157, DOI 10.1007/s00371-013-0896-z
   Wu QY, 2018, ISPRS J PHOTOGRAMM, V143, P57, DOI 10.1016/j.isprsjprs.2018.04.024
   Xiao, 2021, VISUAL COMPUT, P1
   Yan L, 2021, ELECTRON LETT, V57, P754, DOI 10.1049/ell2.12254
   Yan Z., 2021, ARXIV
   Zhao SS, 2021, IEEE T IMAGE PROCESS, V30, P5264, DOI 10.1109/TIP.2021.3079821
   Zhao T, 2022, VISUAL COMPUT, V38, P1619, DOI 10.1007/s00371-021-02092-8
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
   Zhu Y.J., 2021, ARXIV
NR 40
TC 1
Z9 1
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 87
EP 97
DI 10.1007/s00371-022-02767-w
EA JAN 2023
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000921819700001
DA 2024-07-18
ER

PT J
AU Agrawal, SC
   Agarwal, R
AF Agrawal, Subhash Chand
   Agarwal, Rohit
TI A novel contrast and saturation prior for image dehazing
SO VISUAL COMPUTER
LA English
DT Article
DE Saturation; Contrast; Brightness; Contrast saturation prior;
   Transmission; Dark channel
ID QUALITY ASSESSMENT; HAZE REMOVAL
AB Images captured in bad weather conditions such as fog, mist, haze, etc., are severely degraded due to the scattering of the particles in the atmosphere. These images are inappropriate for various applications of computer vision, e.g., transportation, remote sensing, video surveillance object recognition, etc. Image dehazing is the process of removing the haze effect from an image so that these applications can be benefited. The physical model of haze formation is used to restore a hazy image which requires two parameters to estimate: transmission and airlight. The accuracy of the dehazing depends on the estimation of the transmission. Dark channel prior (DCP) is an effective method to compute the transmission. However, a dark channel underestimates the transmission when an object in the scene has a similar color to the atmospheric light or sky region, as a result, the dehazed image looks dark. In this paper, we explore the DCP from a new perspective and reformulate it into contrast, saturation and brightness. We proposed a method to estimate the transmission without computing the dark channel. To overcome the problem of over-enhancement and remove the haze effect, a nonlinear model based on inverse strategy is introduced. It prevents the transmission from becoming over-estimated or under-estimated. The experimental result section demonstrates the efficacy of the proposed method over the natural and synthetic hazy images along with qualitative and quantitative analysis.
C1 [Agrawal, Subhash Chand; Agarwal, Rohit] GLA Univ, Dept Comp Engn & Applicat, Mathura 281406, UP, India.
C3 GLA University
RP Agrawal, SC (corresponding author), GLA Univ, Dept Comp Engn & Applicat, Mathura 281406, UP, India.
EM subhash.agrawal@gla.ac.in; rohit.agrwal@gla.ac.in
CR Agrawal SC, 2022, IEEE T CIRC SYST VID, V32, P593, DOI 10.1109/TCSVT.2021.3068625
   Agrawal SC, 2021, J VIS COMMUN IMAGE R, V77, DOI 10.1016/j.jvcir.2021.103087
   Agrawal SC, 2022, VISUAL COMPUT, V38, P781, DOI 10.1007/s00371-020-02049-3
   Ancuti C, 2016, IEEE IMAGE PROC, P2226, DOI 10.1109/ICIP.2016.7532754
   Baig N, 2016, IEEE SIGNAL PROC LET, V23, P853, DOI 10.1109/LSP.2016.2559805
   Berman D, 2020, IEEE T PATTERN ANAL, V42, P720, DOI 10.1109/TPAMI.2018.2882478
   Bi GL, 2017, IEEE PHOTONICS J, V9, DOI 10.1109/JPHOT.2017.2726107
   Bui TM, 2018, IEEE T IMAGE PROCESS, V27, P999, DOI 10.1109/TIP.2017.2771158
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen ZY, 2021, PROC CVPR IEEE, P7176, DOI 10.1109/CVPR46437.2021.00710
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   Crete F, 2007, PROC SPIE, V6492, DOI 10.1117/12.702790
   Engin D, 2018, IEEE COMPUT SOC CONF, P938, DOI 10.1109/CVPRW.2018.00127
   Galdran A, 2018, SIGNAL PROCESS, V149, P135, DOI 10.1016/j.sigpro.2018.03.008
   Gao Y, 2020, SIGNAL PROCESS, V167, DOI 10.1016/j.sigpro.2019.107284
   Garg Hitendra, 2020, 2020 Sixth International Conference on Parallel, Distributed and Grid Computing (PDGC), P172, DOI 10.1109/PDGC50313.2020.9315786
   Golts A, 2020, IEEE T IMAGE PROCESS, V29, P2692, DOI 10.1109/TIP.2019.2952032
   Gupta N, 2022, ARTIF INTELL REV, V55, P3457, DOI 10.1007/s10462-021-10091-3
   Gupta N, 2022, VISUAL COMPUT, V38, P2315, DOI 10.1007/s00371-021-02114-5
   Hautiere Nicolas, 2008, Image Analysis & Stereology, V27, P87, DOI 10.5566/ias.v27.p87-95
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Ju MY, 2021, IEEE T IMAGE PROCESS, V30, P2180, DOI 10.1109/TIP.2021.3050643
   Ju MY, 2020, IEEE T IMAGE PROCESS, V29, P3104, DOI 10.1109/TIP.2019.2957852
   Dhara SK, 2021, IEEE T CIRC SYST VID, V31, P2076, DOI 10.1109/TCSVT.2020.3007850
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li BY, 2020, IEEE T IMAGE PROCESS, V29, P8457, DOI 10.1109/TIP.2020.3016134
   Li D, 2019, IEEE T PARALL DISTR, V30, P2743, DOI 10.1109/TPDS.2019.2921956
   Li LRH, 2020, IEEE T IMAGE PROCESS, V29, P2766, DOI 10.1109/TIP.2019.2952690
   Li ZG, 2018, IEEE T IMAGE PROCESS, V27, P442, DOI 10.1109/TIP.2017.2750418
   Liu CX, 2016, VISUAL COMPUT, V32, P911, DOI 10.1007/s00371-016-1259-3
   Liu PJ, 2019, IEEE T IMAGE PROCESS, V28, P2212, DOI 10.1109/TIP.2018.2823424
   Liu X, 2017, COMPUT VIS IMAGE UND, V162, P23, DOI 10.1016/j.cviu.2017.08.002
   Liu Z, 2019, IEEE SIGNAL PROC LET, V26, P833, DOI 10.1109/LSP.2019.2910403
   Lu ZW, 2020, IEEE SIGNAL PROC LET, V27, P665, DOI 10.1109/LSP.2020.2985570
   Ma KD, 2015, IEEE IMAGE PROC, P3600, DOI 10.1109/ICIP.2015.7351475
   Mandal S, 2020, IEEE T IMAGE PROCESS, V29, P2478, DOI 10.1109/TIP.2019.2957931
   Mehta A, 2021, IEEE WINT CONF APPL, P413, DOI 10.1109/WACV48630.2021.00046
   Mi ZT, 2016, IET IMAGE PROCESS, V10, P206, DOI 10.1049/iet-ipr.2015.0112
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy A. K., 2009, IEEE Signal Process. Lett, V17, P7
   Raikwar SC, 2020, IEEE T IMAGE PROCESS, V29, P4832, DOI 10.1109/TIP.2020.2975909
   Rathor S, 2019, J AMB INTEL HUM COMP, V10, P3617, DOI 10.1007/s12652-018-1087-6
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Santra S, 2018, IEEE T IMAGE PROCESS, V27, P4598, DOI 10.1109/TIP.2018.2841198
   Singh Dilbag, 2019, Archives of Computational Methods in Engineering, V26, P1395, DOI 10.1007/s11831-018-9294-z
   Singh D, 2019, SIGNAL PROCESS-IMAGE, V70, P131, DOI 10.1016/j.image.2018.09.011
   Wang JB, 2018, IEEE T CIRC SYST VID, V28, P2190, DOI 10.1109/TCSVT.2017.2728822
   Wang WC, 2017, IEEE T MULTIMEDIA, V19, P1142, DOI 10.1109/TMM.2017.2652069
   Woodell G, 2006, PROC SPIE, V6246, DOI 10.1117/12.666767
   Xu ZQ, 2018, PROCEEDINGS OF 2018 IEEE WORLD SYMPOSIUM ON COMMUNICATION ENGINEERING (WSCE), P1, DOI 10.1109/WSCE.2018.8690540
   Yang D, 2018, LECT NOTES COMPUT SC, V11211, P729, DOI 10.1007/978-3-030-01234-2_43
   Yuan F, 2018, IEEE T IMAGE PROCESS, V27, P4395, DOI 10.1109/TIP.2018.2837900
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zheng MY, 2020, IEEE SENS J, V20, P8062, DOI 10.1109/JSEN.2020.2981719
   Zhu MZ, 2018, IEEE SIGNAL PROC LET, V25, P174, DOI 10.1109/LSP.2017.2780886
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
   Zhu YY, 2018, NEUROCOMPUTING, V275, P499, DOI 10.1016/j.neucom.2017.08.055
NR 63
TC 4
Z9 4
U1 2
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5763
EP 5781
DI 10.1007/s00371-022-02694-w
EA NOV 2022
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000877975900003
DA 2024-07-18
ER

PT J
AU Wang, XD
   Xu, XW
   Wang, YL
   Wu, PT
   Yan, F
   Zeng, ZQ
AF Wang, Xiaodong
   Xu, Xianwei
   Wang, Yanli
   Wu, Pengtao
   Yan, Fei
   Zeng, Zhiqiang
TI A robust defect detection method for syringe scale without positive
   samples
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Defect detection; Image processing; Image segmentation
AB With the worldwide spread of the COVID-19 pandemic, the demand for medical syringes has increased dramatically. Scale defect, one of the most common defects on syringes, has become a major barrier to boosting syringe production. Existing methods for scale defect detection suffer from large volumes of data requirements and the inability to handle diverse and uncertain defects. In this paper, we propose a robust scale defects detection method with only negative samples and favorable detection performance to solve this problem. Different from conventional methods that work in a batch-mode defects detection manner, we propose to locate the defects on syringes with a two-stage framework, which consists of two components, that is, the scale extraction network and the scale defect discriminator. Concretely, the SeNet is first built to utilize the convolutional neural network to extract the main structure of the scale. After that, the scale defect discriminator is designed to detect and label the scale defects. To evaluate the performance of our method, we conduct experiments on one real-world syringe dataset. The competitive results, that is, 99.7% on F1, prove the effectiveness of our method.
C1 [Wang, Xiaodong; Xu, Xianwei; Wang, Yanli; Wu, Pengtao; Yan, Fei; Zeng, Zhiqiang] Xiamen Univ Technol, Coll Comp & Informat Engn, Xiamen 361024, Peoples R China.
C3 Xiamen University of Technology
RP Xu, XW (corresponding author), Xiamen Univ Technol, Coll Comp & Informat Engn, Xiamen 361024, Peoples R China.
EM 18665596731@163.com
RI Liu, Song/KCX-6842-2024; wang, xu/IAN-4886-2023; wang,
   xiao/HZI-9156-2023; Xu, Xianwei/HHM-2468-2022; Li, June/JEF-1173-2023;
   Chen, Shuo/JEO-6350-2023
FU National Natural Science Foundation of China [61871464, U1805264];
   National Natural Science Foundation of Fujian Province [2020J01266,
   2021J011186]; "Climbing" Program of XMUT [XPDKT20031];
   ScientificResearch Fund of Fujian Provincial Education Department
   [JAT200486]; University Industry Research Fund of Xiamen [2022CXY0416]
FX This paper was supported by National Natural Science Foundation of China
   (Grant Nos. 61871464, U1805264), National Natural Science Foundation of
   Fujian Province (Grant Nos. 2020J01266, 2021J011186), the "Climbing"
   Program of XMUT (Grant No. XPDKT20031), ScientificResearch Fund of
   Fujian Provincial Education Department (Grant No. JAT200486), the
   University Industry Research Fund of Xiamen (Grant No. 2022CXY0416).
CR Bengio S., 2016, COMMUN ACM
   Bulnes FG, 2016, J INTELL MANUF, V27, P431, DOI 10.1007/s10845-014-0876-9
   Chen JW, 2018, IEEE T INSTRUM MEAS, V67, P257, DOI 10.1109/TIM.2017.2775345
   Cheng L, 2023, MULTIMED TOOLS APPL, V82, P3101, DOI 10.1007/s11042-022-13568-7
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Dai WT, 2021, VISUAL COMPUT, V37, P3093, DOI 10.1007/s00371-021-02257-5
   Dai WT, 2022, VISUAL COMPUT, V38, P1181, DOI 10.1007/s00371-021-02137-y
   Devos O, 2009, CHEMOMETR INTELL LAB, V96, P27, DOI 10.1016/j.chemolab.2008.11.005
   DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242
   Fern C., 2012, SURFACE CLASSIFICATI, P600
   Hu WH, 2022, VISUAL COMPUT, V38, P3731, DOI 10.1007/s00371-021-02210-6
   Huang YQ, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3047190
   Huang YB, 2020, VISUAL COMPUT, V36, P85, DOI 10.1007/s00371-018-1588-5
   Kingma D. P., 2014, arXiv
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li K, 2020, VISUAL COMPUT, V36, P391, DOI 10.1007/s00371-019-01627-4
   Liu GH, 2021, VISUAL COMPUT, V37, P515, DOI 10.1007/s00371-020-01820-w
   Luo QW, 2020, IEEE T INSTRUM MEAS, V69, P626, DOI 10.1109/TIM.2019.2963555
   Masci J, 2012, IEEE IJCNN
   Mei S, 2018, IEEE T INSTRUM MEAS, V67, P1266, DOI 10.1109/TIM.2018.2795178
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Napoletano P, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18010209
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Tabernik D, 2020, J INTELL MANUF, V31, P759, DOI 10.1007/s10845-019-01476-x
   Tian H, 2019, PROCEEDINGS OF MVA 2019 16TH INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS (MVA)
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Tu KL, 2021, COMPUT ELECTRON AGR, V182, DOI 10.1016/j.compag.2021.106002
   Wang D, 2021, VISUAL COMPUT, V37, P1101, DOI 10.1007/s00371-020-01855-z
   Wang Q, 2022, VISUAL COMPUT, V38, P2591, DOI 10.1007/s00371-021-02134-1
   Weimer D, 2016, CIRP ANN-MANUF TECHN, V65, P417, DOI 10.1016/j.cirp.2016.04.072
   Wu W, 2022, VISUAL COMPUT, V38, P1665, DOI 10.1007/s00371-021-02095-5
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xu H, 2021, MATER LETT, V299, DOI 10.1016/j.matlet.2021.130065
   Xu L, 2020, IEEE ACCESS, V8, P42285, DOI 10.1109/ACCESS.2020.2977821
   Xu L, 2016, MEASUREMENT, V87, P255, DOI 10.1016/j.measurement.2016.02.048
   Yeung M, 2022, COMPUT MED IMAG GRAP, V95, DOI 10.1016/j.compmedimag.2021.102026
   Zhang X., 2011, P SPIE INT SOC OPT E, V8003, P11
   Zhao ZX, 2018, LECT NOTES ARTIF INT, V11013, P473, DOI 10.1007/978-3-319-97310-4_54
   Zheng ZD, 2021, INT J COMPUT VISION, V129, P1106, DOI 10.1007/s11263-020-01395-y
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
NR 41
TC 5
Z9 5
U1 5
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5451
EP 5467
DI 10.1007/s00371-022-02671-3
EA SEP 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000860371800001
PM 36185464
OA Green Published, Bronze
DA 2024-07-18
ER

PT J
AU Sun, BC
   Qi, YH
   Zhang, GY
   Liu, Y
AF Sun, Beichen
   Qi, Yuehan
   Zhang, Guanyu
   Liu, Yang
TI Edge guidance filtering for structure extraction
SO VISUAL COMPUTER
LA English
DT Article
DE Guided filtering; Texture filtering; Adaptive kernel scale scheme;
   Texture edges; Pixel-selected filter
ID IMAGE; ENHANCEMENT; COLOR; SPACE
AB Smoothing the multiscale, irregular, and high contrast textures while maintaining structures with small details is challenging for the existing texture filtering methods. In this paper, we put forward a novel edge guidance-based texture filter with an adaptive kernel scale scheme to address these challenges. The texture edges are identified by a texture edge detector first. Then, based on the texture edges, a variable per-pixel smoothing scale is selected to construct the scale map, which is used to guide the filtering. In the end, a novel pixel-selected filter is designed as post-processing to optimize the filtered images. The experimental results compared with the state-of-the-art methods show that our method has a better performance in suppressing different forms of textures while maintaining the main structure. In addition, our method can be applied well in a variety of image processing applications including: detail enhancement, inverse halftoning, virtual contour restoration and texture image segmentation.
C1 [Sun, Beichen; Zhang, Guanyu; Liu, Yang] Jilin Univ, Coll Instrumentat & Elect Engn, Ximinzhu Ave, Changchun 130012, Jilin, Peoples R China.
   [Qi, Yuehan] Queens Univ, Dept Art & Sci, 99 Univ Ave, Kingston, ON K7L 3N6, Canada.
C3 Jilin University; Queens University - Canada
RP Liu, Y (corresponding author), Jilin Univ, Coll Instrumentat & Elect Engn, Ximinzhu Ave, Changchun 130012, Jilin, Peoples R China.
EM sunbc20@mails.jlu.edu.cn; 18yq36@queensu.ca; zhangguanyu@jlu.edu.cn;
   liu_yang@jlu.edu.cn
OI Liu, Yang/0000-0002-8187-0555
FU China's national major scientific research instrument development
   project [42127807]; Key project of Guangdong Province for Promoting
   High-quality Economic Development [[2022]29]; Special fund for applied
   basic research of Changchun Science and Technology Department [21ZY21];
   Jilin Science and technology development plan, key R D projects
   [20220201055GX]; Southern Marine Science and Engineering Guangdong
   Laboratory (Zhanjiang) [ZJW-2019-04]
FX This work was supported by China's national major scientific research
   instrument development project (42127807), Key project of Guangdong
   Province for Promoting High-quality Economic Development (Marine
   Economic Development) in 2022: Research and development of key
   technology and equipment for Marine vibroseis system (GDNRC[2022]29),
   Special fund for applied basic research of Changchun Science and
   Technology Department(21ZY21), Jilin Science and technology development
   plan, key R & D projects (20220201055GX), and Southern Marine Science
   and Engineering Guangdong Laboratory (Zhanjiang) under contract
   No.ZJW-2019-04.
CR Bao LC, 2014, IEEE T IMAGE PROCESS, V23, P555, DOI 10.1109/TIP.2013.2291328
   Chen BL, 2018, IEEE T MULTIMEDIA, V20, P2882, DOI 10.1109/TMM.2018.2825883
   Chen XG, 2015, IEEE T CIRC SYST VID, V25, P897, DOI 10.1109/TCSVT.2014.2365654
   Cho H, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601188
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Deng G, 2016, IEEE T IMAGE PROCESS, V25, P439, DOI 10.1109/TIP.2015.2503699
   Eun H, 2016, IEEE SIGNAL PROC LET, V23, P1887, DOI 10.1109/LSP.2016.2630741
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Gao YY, 2018, IEEE T MULTIMEDIA, V20, P335, DOI 10.1109/TMM.2017.2740025
   Ghosh S, 2020, IEEE T CIRC SYST VID, V30, P2015, DOI 10.1109/TCSVT.2019.2916589
   Ham B, 2015, PROC CVPR IEEE, P4823, DOI 10.1109/CVPR.2015.7299115
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Jain P, 2015, VISUAL COMPUT, V31, P657, DOI 10.1007/s00371-014-0993-7
   Karacan L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508403
   Kim Y, 2019, IEEE T IMAGE PROCESS, V28, P2692, DOI 10.1109/TIP.2018.2889531
   Kou F, 2018, IEEE T MULTIMEDIA, V20, P484, DOI 10.1109/TMM.2017.2743988
   Li YJ, 2016, LECT NOTES COMPUT SC, V9908, P154, DOI 10.1007/978-3-319-46493-0_10
   Li ZG, 2015, IEEE T IMAGE PROCESS, V24, P120, DOI 10.1109/TIP.2014.2371234
   Lin TH, 2016, COMPUT GRAPH FORUM, V35, P57, DOI 10.1111/cgf.13003
   Lin TC, 2007, INFORM SCIENCES, V177, P1073, DOI 10.1016/j.ins.2006.07.030
   Liu W, 2020, AAAI CONF ARTIF INTE, V34, P11620
   Liu W, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3388887
   Liu W, 2020, IEEE T CIRC SYST VID, V30, P23, DOI 10.1109/TCSVT.2018.2890202
   Liu W, 2017, IEEE I CONF COMP VIS, pCP32, DOI 10.1109/ICCV.2017.624
   Liu WL, 2021, IET INTELL TRANSP SY, V15, P619, DOI 10.1049/itr2.12049
   Liu Y, 2020, IEEE ACCESS, V8, P43838, DOI 10.1109/ACCESS.2020.2977408
   Liu Y, 2019, IEEE ACCESS, V7, P160683, DOI 10.1109/ACCESS.2019.2951228
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Sener O, 2014, IEEE T MULTIMEDIA, V16, P1292, DOI 10.1109/TMM.2014.2314069
   Su Z, 2013, IEEE T MULTIMEDIA, V15, P535, DOI 10.1109/TMM.2012.2237025
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Xu PP, 2019, IEEE T IMAGE PROCESS, V28, P4354, DOI 10.1109/TIP.2019.2904847
   [余丽红 YU Li-hong], 2009, [中国图象图形学报, Journal of Image and Graphics], V14, P1950
   Zhang CX, 2020, IEEE T MULTIMEDIA, V22, P349, DOI 10.1109/TMM.2019.2929934
   Zhang FH, 2015, IEEE I CONF COMP VIS, P361, DOI 10.1109/ICCV.2015.49
   Zhang Q, 2014, LECT NOTES COMPUT SC, V8691, P815, DOI 10.1007/978-3-319-10578-9_53
   Zhang ZY, 2021, SIGNAL PROCESS-IMAGE, V96, DOI 10.1016/j.image.2021.116308
   Zhou ZQ, 2018, IEEE T MULTIMEDIA, V20, P1392, DOI 10.1109/TMM.2017.2772438
   Zhu FD, 2019, IEEE T IMAGE PROCESS, V28, P3556, DOI 10.1109/TIP.2019.2908778
   Zhu LF, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366146
NR 43
TC 2
Z9 2
U1 5
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5327
EP 5342
DI 10.1007/s00371-022-02662-4
EA SEP 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000849158400002
DA 2024-07-18
ER

PT J
AU Garg, P
   Kishore, RR
AF Garg, Preeti
   Kishore, R. Rama
TI A robust and secured adaptive image watermarking using social group
   optimization
SO VISUAL COMPUTER
LA English
DT Article
DE Digital watermarking; Meta-heuristic optimization; Optimization
   function; Social group optimization; Adaptive locations
ID ALGORITHM; CURVE
AB Watermarking is the process of inserting concealed data into carrier data to authenticate the owner of the material. To achieve optimal performance, we present an intelligent system for watermarking that combines a meta-heuristic method with an embedding technique. The suggested work proposes a blind watermarking technique that embeds the watermark bits in the best location of discrete cosine transform blocks while taking advantage of the discrete wavelet transform's features. To safeguard the embedded watermark, a two-step security mechanism is used: first, the column values are shuffled using a proposed shuffling algorithm, and then the scrambled watermark is encrypted using the Arnold encryption scheme. The primary goal of any watermarking technology is to protect embedded data from various attacks while maintaining the carrier data's quality. By tailoring the embedding site for watermark encapsulation, the recommended technique achieves an acceptable balance of these two characteristics known as imperceptibility and resilience. A meta-heuristic algorithm based on human social behavior is employed to optimize the placement. The social group optimization (SGO) algorithm is a new member of the family of meta-heuristic algorithms. No attempt has been made to include the social group optimization technique into applications for watermark embedding, to our knowledge. The SGO method can assist in striking a balance between various watermarking qualities. To demonstrate the utility of the suggested method, it is compared to a variety of existing watermarking techniques. The approach presented here is a robust solution that may be applied to a wide variety of multimedia applications, including telemedicine, media distribution, and security systems.
C1 [Garg, Preeti; Kishore, R. Rama] Guru Gobind Singh Indraprastha Univ, Univ Sch Informat Commun & Technol, New Delhi, India.
   [Garg, Preeti] Delhi NCR, KIET Grp Inst, Ghaziabad, India.
C3 GGS Indraprastha University; KIET Group of Institutions
RP Garg, P (corresponding author), Guru Gobind Singh Indraprastha Univ, Univ Sch Informat Commun & Technol, New Delhi, India.; Garg, P (corresponding author), Delhi NCR, KIET Grp Inst, Ghaziabad, India.
EM preeti.itgarg@gmail.com; ram_kish@yahoo.com
OI Garg, Preeti/0000-0001-6635-3296
CR Abbas M, 2014, MATH PROBL ENG, V2014, DOI 10.1155/2014/408492
   Abbasi R, 2021, MULTIMEDIA SYST, V27, P177, DOI 10.1007/s00530-020-00718-w
   Abbasi R, 2019, SECUR COMMUN NETW, DOI 10.1155/2019/8981240
   Abdelhakim AM, 2017, EXPERT SYST APPL, V72, P317, DOI 10.1016/j.eswa.2016.10.056
   Aditya K, 2017, 2017 INTERNATIONAL CONFERENCE ON NETWORKS & ADVANCES IN COMPUTATIONAL TECHNOLOGIES (NETACT), P29, DOI 10.1109/NETACT.2017.8076737
   AL-Nabhani Y, 2015, J KING SAUD UNIV-COM, V27, P393, DOI 10.1016/j.jksuci.2015.02.002
   Ali M, 2018, INT J SYST ASSUR ENG, V9, P602, DOI 10.1007/s13198-014-0288-4
   Allam M., 2014, DISTINCTIVE DATA HID, DOI [10.13140/2.1.2154.1121, DOI 10.13140/2.1.2154.1121]
   Ahmadi SBB, 2021, VISUAL COMPUT, V37, P385, DOI 10.1007/s00371-020-01808-6
   Bakhsh FY, 2018, J INF SECUR APPL, V41, P12, DOI 10.1016/j.jisa.2018.05.003
   Bashir U, 2013, APPL MATH COMPUT, V219, P10183, DOI 10.1016/j.amc.2013.03.110
   BiBi S, 2020, MATHEMATICS-BASEL, V8, DOI 10.3390/math8060967
   Bibi SMA, 2019, IEEE ACCESS, V7, P165779, DOI 10.1109/ACCESS.2019.2953496
   Chamlawi R., 2017, INT J ADV COMPUT SC, DOI [10.14569/IJACSA.2017.080527, DOI 10.14569/IJACSA.2017.080527]
   Dey N, 2018, SYMMETRY-BASEL, V10, DOI 10.3390/sym10020051
   Dubolia R., 2011, 2011 International Conference on Communication Systems and Network Technologies (CSNT), P593, DOI 10.1109/CSNT.2011.127
   Enrico M., 2007, EVALUATION STANDARD, P6505
   Fazli S, 2016, OPTIK, V127, P964, DOI 10.1016/j.ijleo.2015.09.205
   Garg P., 2021, COMPUTATIONAL METHOD, V1257, DOI [10.1007/978-981-15-7907-3_3, DOI 10.1007/978-981-15-7907-3_3]
   Garg P, 2020, MULTIMED TOOLS APPL, V79, P25921, DOI 10.1007/s11042-020-09262-1
   Hong W., 2006, IEEE T SIGNAL PROCES, V12, P1
   Hu DH, 2019, IEEE T KNOWL DATA EN, V31, P1024, DOI 10.1109/TKDE.2018.2851517
   Issa M, 2018, STUD COMPUT INTELL, V730, P683, DOI 10.1007/978-3-319-63754-9_30
   Jero SE, 2016, EXPERT SYST APPL, V49, P123, DOI 10.1016/j.eswa.2015.12.010
   Khare P, 2021, MULTIDIM SYST SIGN P, V32, P131, DOI 10.1007/s11045-020-00732-1
   Kumar S, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON RECENT TRENDS IN ELECTRONICS, INFORMATION & COMMUNICATION TECHNOLOGY (RTEICT), P1802, DOI 10.1109/RTEICT.2016.7808145
   Liu DC, 2021, EXPERT SYST APPL, V170, DOI 10.1016/j.eswa.2020.114540
   Majeed A, 2020, MATHEMATICS-BASEL, V8, DOI 10.3390/math8081246
   Maloo S, 2020, SENS IMAGING, V21, DOI 10.1007/s11220-020-00291-6
   Maqsood S, 2020, ADV DIFFER EQU-NY, V2020, DOI 10.1186/s13662-020-03001-4
   Maqsood S, 2020, MATH PROBL ENG, V2020, DOI 10.1155/2020/4036434
   Mata-Mendoza D, 2022, VISUAL COMPUT, V38, P2073, DOI 10.1007/s00371-021-02267-3
   Mehta R, 2016, MULTIMED TOOLS APPL, V75, P4129, DOI 10.1007/s11042-015-3084-5
   Moeinaddini E, 2019, SOFT COMPUT, V23, P9685, DOI 10.1007/s00500-018-3535-9
   Naik A, 2020, APPL SOFT COMPUT, V95, DOI 10.1016/j.asoc.2020.106524
   Nambakhsh MS, 2011, COMPUT METH PROG BIO, V104, P418, DOI 10.1016/j.cmpb.2010.08.016
   Nerurkar P., 2018, IEEE, P1, DOI [10.1109/ICCUBEA.2018.8697666, DOI 10.1109/ICCUBEA.2018.8697666]
   Parah S., 2016, ROBUST BLIND WATERMA, P1
   Perwej Y., 2012, Int J Multimed Its Appl, V4, P21, DOI [10.5121/ijma.2012.4202, DOI 10.5121/IJMA.2012.4202]
   Phi BN, 2010, LECT NOTES COMPUT SC, V6297, P685, DOI 10.1007/978-3-642-15702-8_63
   Rajani D, 2020, SIGNAL PROCESS, V172, DOI 10.1016/j.sigpro.2020.107556
   Satapathy S, 2016, COMPLEX INTELL SYST, V2, P173, DOI 10.1007/s40747-016-0022-8
   Sejpal Shveti, 2016, 2016 IEEE International Conference on Advances in Electronics, Communication and Computer Technology (ICAECCT), P38, DOI 10.1109/ICAECCT.2016.7942552
   Sharmin S., 2019, P IEMIS 2018, V3, DOI [10.1007/978-981-13-1501-5_18, DOI 10.1007/978-981-13-1501-5_18]
   Singla Anu, 2015, 2015 IEEE Power & Energy Society General Meeting, P1, DOI 10.1109/PESGM.2015.7286046
   Su QT, 2018, MULTIDIM SYST SIGN P, V29, P1055, DOI 10.1007/s11045-017-0487-7
   Sun L, 2018, NEURAL COMPUT APPL, V30, P2425, DOI 10.1007/s00521-016-2788-4
   Sundararajan M, 2018, MATER TODAY-PROC, V5, P1138, DOI 10.1016/j.matpr.2017.11.194
   Thakkar FN, 2019, MULTIDIM SYST SIGN P, V30, P1769, DOI 10.1007/s11045-018-0627-8
   ugr, DATA SOURCE
   Usman M, 2020, J ADV MECH DES SYST, V14, DOI 10.1299/jamdsm.2020jamdsm0048
   Vaidya SP, 2017, MULTIMED TOOLS APPL, V76, P25623, DOI 10.1007/s11042-017-4355-0
   Verma V, 2016, 2016 INTERNATIONAL CONFERENCE ON ELECTRICAL, ELECTRONICS, AND OPTIMIZATION TECHNIQUES (ICEEOT), P3198, DOI 10.1109/ICEEOT.2016.7755292
   Vishwakarma Virendra P., 2018, Procedia Computer Science, V132, P1012, DOI 10.1016/j.procs.2018.05.017
   Xu HC, 2018, INT J ELECTRON SECUR, V10, P79, DOI 10.1504/IJESDF.2018.089215
   Yuan ZH, 2021, VISUAL COMPUT, V37, P1867, DOI 10.1007/s00371-020-01945-y
   Zear A., 2016, PARDEEP KUMAR PROPOS, P1
   Zheng WB, 2018, C IND ELECT APPL, P1233, DOI 10.1109/ICIEA.2018.8397898
   Zhou NR, 2019, MULTIMED TOOLS APPL, V78, P2507, DOI 10.1007/s11042-018-6322-9
NR 59
TC 3
Z9 3
U1 1
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4839
EP 4854
DI 10.1007/s00371-022-02631-x
EA AUG 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000842420800001
DA 2024-07-18
ER

PT J
AU Krishnan, PT
   Balasubramanian, P
   Jeyakumar, V
   Mahadevan, S
   Raj, ANJ
AF Krishnan, Palani Thanaraj
   Balasubramanian, Parvathavarthini
   Jeyakumar, Vijay
   Mahadevan, Shriraam
   Noel Joseph Raj, Alex
TI Intensity matching through saliency maps for thermal and visible image
   registration for face detection applications
SO VISUAL COMPUTER
LA English
DT Article
DE Image registration; Saliency maps; Thermal image; Visible image;
   Particle swarm optimization; Face detection
ID MAGNETIC-RESONANCE; FUSION
AB Face identification is a common image processing problem that is used in a range of applications, including pedestrian tracking, population flow forecast, video surveillance, and fever screening. When combined with a visible-light camera, thermal imaging provides a more thorough image of the person. Face identification is commonly accomplished by combining data from many modalities, such as infrared(IR)/visible(VIS) pictures. The face detection system's success is contingent upon the image registration approach being of good quality. In this article, we demonstrate an improved intensity-based image registration approach for visual and thermal face pictures. The approach applies a saliency map strategy to balance the infrared and visible pictures' intensity levels in order to overcome intensity differences and ensure proper image registration. When applied to facial infrared and visible pictures, the proposed technique displays an improvement in registration quality when measured using the structural similarity index measure (SSIM) and universal image quality index (UQI) metrics. We report an average improvement of 16.93 % in terms of SSIM score and 7.02 % in terms of UQI score when the suggested image registration system is applied in comparison with unregistered images for the test IR/VIS face images. Furthermore, the proposed method outperformed the state-of-the-art Oriented FAST and Rotated BRIEF-based image registration methods in terms of image registration quality.
C1 [Krishnan, Palani Thanaraj] Anna Univ, Dept Elect & Instrumentat Engn, St Josephs Coll Engn, Chennai 600119, Tamil Nadu, India.
   [Balasubramanian, Parvathavarthini] Anna Univ, Dept Comp Sci & Engn, St Josephs Coll Engn, Chennai 600119, Tamil Nadu, India.
   [Jeyakumar, Vijay] Sri Sivasubramaniya Nadar Coll Engn, Dept Biomed Engn, Kalavakkam 603110, Tamil Nadu, India.
   [Mahadevan, Shriraam] Sri Ramachandra Med Coll & Res Inst, Dept Endocrinol, Porur 600116, Tamil Nadu, India.
   [Noel Joseph Raj, Alex] Shantou Univ, Dept Elect Engn, Shantou 515063, Guangdong, Peoples R China.
C3 St. Joseph's College of Engineering, Chennai; Anna University; Anna
   University Chennai; St. Joseph's College of Engineering, Chennai; Anna
   University; Anna University Chennai; SSN College of Engineering; Sri
   Ramachandra Institute of Higher Education & Research; Shantou University
RP Jeyakumar, V (corresponding author), Sri Sivasubramaniya Nadar Coll Engn, Dept Biomed Engn, Kalavakkam 603110, Tamil Nadu, India.
EM palanithanaraj.k@ieee.org; parvathavarthini@gmail.com;
   vijayj@ssn.edu.in; mshriraam@gmail.com; jalexnoel@stu.edu.cn
RI Jeyakumar, Vijay/AGT-7276-2022; Krishnan, Palani Thanaraj/F-1299-2014
OI Jeyakumar, Vijay/0000-0001-8408-2485; Krishnan, Palani
   Thanaraj/0000-0002-4214-9685; Mahadevan, Shriraam/0000-0002-6915-5639
FU Science and Engineering Research Board (SERB), Department of Science and
   Technology, India [CRG/2020/003042]
FX This work was supported by Science and Engineering Research Board
   (SERB), Department of Science and Technology, India, through the Core
   Research Grant (File No. CRG/2020/003042).
CR Abbas M, 2014, MATH PROBL ENG, V2014, DOI 10.1155/2014/408492
   Ali FE, 2010, APPL OPTICS, V49, P114, DOI 10.1364/AO.49.000114
   [Anonymous], Tufts Face Database | Kaggle
   Anzid H, 2023, VISUAL COMPUT, V39, P1667, DOI 10.1007/s00371-022-02435-z
   Artyushkova, AUTOMATIC IMAGE REGI
   Bhardwaj J., 2019, BIOPHOTONICS C OPTIC
   BiBi S, 2020, MATHEMATICS-BASEL, V8, DOI 10.3390/math8060967
   Chen K, 2019, NUMER ALGORITHMS, V80, P305, DOI 10.1007/s11075-018-0486-2
   Chen YJ, 2018, MACH VISION APPL, V29, P113, DOI 10.1007/s00138-017-0879-6
   D'Agostino E, 2003, MED IMAGE ANAL, V7, P565, DOI 10.1016/S1361-8415(03)00039-2
   Dixit M., 2015, INT J SIGNAL PROCESS, V8, P317
   Fei L., 2015, Visualization in Engineering, V3, P16, DOI [10.1186/s40327-015-0028-0, DOI 10.1186/S40327-015-0028-0]
   Foroosh H, 2002, IEEE T IMAGE PROCESS, V11, P188, DOI 10.1109/83.988953
   Fu YB, 2020, PHYS MED BIOL, V65, DOI 10.1088/1361-6560/ab843e
   Guo BL, 2008, CHIN OPT LETT, V6, P338
   Guo HQ, 2017, J OPT SOC AM A, V34, P1961, DOI 10.1364/JOSAA.34.001961
   Hou X., 2007, IEEE C COMP VIS PATT, V1, P1, DOI DOI 10.1109/CVPR.2007.383267
   Istenic R., 2007, 2007 14th International Workshop in Systems, Signals and Image Processing and 6th EURASIP Conference focused on Speech and Image Processing, Multimedia Communications and Services - EC-SIPMCS 2007, P106, DOI 10.1109/IWSSIP.2007.4381164
   Kennedy J, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS PROCEEDINGS, VOLS 1-6, P1942, DOI 10.1109/icnn.1995.488968
   Kim J, 2004, IEEE T MED IMAGING, V23, P1430, DOI 10.1109/TMI.2004.835313
   Krishnan PT, 2021, IEEE EUROCON 2021 - 19TH INTERNATIONAL CONFERENCE ON SMART TECHNOLOGIES, P222, DOI 10.1109/EUROCON52738.2021.9535615
   Kumawat A, 2022, VISUAL COMPUT, V38, P3681, DOI 10.1007/s00371-021-02196-1
   Kuppala K, 2020, INT J IMAGE DATA FUS, V11, P113, DOI 10.1080/19479832.2019.1707720
   Li M, 2020, VISUAL COMPUT, V36, P1725, DOI 10.1007/s00371-019-01771-x
   Liu CY, 2021, SCI PROGRAMMING-NETH, V2021, DOI 10.1155/2021/8509164
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   Maes F, 1997, IEEE T MED IMAGING, V16, P187, DOI 10.1109/42.563664
   Maqsood S, 2020, ADV DIFFER EQU-NY, V2020, DOI 10.1186/s13662-020-03001-4
   Panetta K, 2020, IEEE T PATTERN ANAL, V42, P509, DOI 10.1109/TPAMI.2018.2884458
   Qian WY, 2018, SOFT COMPUT, V22, P4047, DOI 10.1007/s00500-017-2615-6
   Raza SEA, 2015, PATTERN RECOGN, V48, P2119, DOI 10.1016/j.patcog.2015.01.027
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Rueckert D, 1999, IEEE T MED IMAGING, V18, P712, DOI 10.1109/42.796284
   Saleem S, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10031162
   Shi CL, 2020, PATTERN RECOGN LETT, V138, P520, DOI 10.1016/j.patrec.2020.08.021
   Shu XB, 2018, IEEE T PATTERN ANAL, V40, P905, DOI 10.1109/TPAMI.2017.2705122
   Shu XB, 2015, IEEE I CONF COMP VIS, P3970, DOI 10.1109/ICCV.2015.452
   So RWK, 2011, PATTERN RECOGN, V44, P2450, DOI 10.1016/j.patcog.2011.04.008
   Tang TWH, 2007, LECT NOTES COMPUT SC, V4791, P916
   Wan X, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19143117
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Yang Y, 2016, PROCEEDINGS OF 2016 INTERNATIONAL CONFERENCE ON AUDIO, LANGUAGE AND IMAGE PROCESSING (ICALIP), P279, DOI 10.1109/ICALIP.2016.7846571
   Ye HT, 2013, COMPUT INTEL NEUROSC, V2013, DOI 10.1155/2013/384125
   Zhang XY, 2017, J OPT SOC AM A, V34, P1400, DOI 10.1364/JOSAA.34.001400
NR 45
TC 3
Z9 4
U1 3
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4529
EP 4542
DI 10.1007/s00371-022-02605-z
EA JUL 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000823373600001
DA 2024-07-18
ER

PT J
AU Pervaiz, N
   Fraz, MM
   Shahzad, M
AF Pervaiz, N.
   Fraz, M. M.
   Shahzad, M.
TI Per-former: rethinking person re-identification using transformer
   augmented with self-attention and contextual mapping
SO VISUAL COMPUTER
LA English
DT Article
DE Person re-identification; Visual surveillance; Vision transformer;
   Self-attention; Self-context mapping
ID NEURAL-NETWORK
AB Person re-identification (re-id) is an autonomous process that uses raw surveillance images to identify a person across multiple non-overlapping camera views without requiring any kind of hard biometrics like fingerprints, retina patterns or the facial images. The CNN-based deep architectures are most frequently used to solve the person re-id problem. Generally these CNN architectures capture the attentive regions of a person at local neighborhood level with increased focal view at the deeper levels of the network. However these do not learn the self-attentions among distant parts of a person's image, which can play a vital role in person re-id especially to handle the inter-class and intra-class variances. In this work, we propose a novel person re-id approach to learn the self-attention among different parts of a person image whether these lie within local proximity or at the far distant regions for robust re-identification. We adapt the vision transformer architecture with a lightweight self-attention module, which learns the global associations among the distinct attentive regions having similar context within a person image. Further to this, we escalate the baseline model by acquainting it with a self-context mapping module, which coalesces the contextual embeddings into the self-attention learning for the neighboring and the distant image regions. It helps to capture the globally associated salient regions of a person to get the holistic view at the initial network layers. The proposed self-attention-based re-id architecture outperforms the vanilla CNN counterparts for both of the re-id performance measures, i.e., accuracy and mean average precision. The re-id accuracies are improved 5.5%, 4.6% and 17% for Market 1501, DukeMTMC-ReID and MSMT-17 datasets, respectively, as compared to the vanilla CNN-based re-id architectures. The implementation and trained models are made publicly available at https://git.io/JLH2S.
C1 [Pervaiz, N.; Fraz, M. M.; Shahzad, M.] Natl Univ Sci & Technol NUST, Islamabad, Pakistan.
   [Shahzad, M.] Tech Univ Munich TUM, Munich, Germany.
C3 National University of Sciences & Technology - Pakistan; Technical
   University of Munich
RP Fraz, MM (corresponding author), Natl Univ Sci & Technol NUST, Islamabad, Pakistan.
EM nazia.perwaiz@seecs.edu.pk; moazam.fraz@seecs.edu.pk;
   muhammad.shahzad@tum.de
OI Fraz, Muhammad Moazam/0000-0003-0495-463X
CR [Anonymous], 2017, 7 INT C IM PROC THEO
   Ansar W, 2018, PROGR PATTERN RECOGN, P654
   Bai X, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107036
   Batool S, 2018, 2018 IEEE THIRD INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, APPLICATIONS AND SYSTEMS (IPAS), P220, DOI 10.1109/IPAS.2018.8708882
   Chen GY, 2019, IEEE I CONF COMP VIS, P9636, DOI 10.1109/ICCV.2019.00973
   Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844
   Chen YF, 2022, PATTERN RECOGN, V126, DOI 10.1016/j.patcog.2022.108567
   Chen ZC, 2021, VISUAL COMPUT, V37, P685, DOI 10.1007/s00371-020-01880-y
   Dosovitskiy Alexey, 2021, HOULSBY IMAGE IS WOR
   Eiband M, 2018, IUI 2018: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P211, DOI 10.1145/3172944.3172961
   Faizan R., 2021, 2021 INT C DIGITAL F, P1
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Han K, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P2040, DOI 10.1145/3240508.3240550
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HE ST, 2021, TRANSREID, P14993, DOI DOI 10.1109/ICCV48922.2021.01474
   Hendrycks D., 2016, ARXIV160608415
   Hermans Alexander, 2017, ARXIV170307737
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jia ZQ, 2023, VISUAL COMPUT, V39, P1205, DOI 10.1007/s00371-022-02398-1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2194
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li Y, 2020, J PHYS CONF SER, V1642, DOI 10.1088/1742-6596/1642/1/012015
   Liu L., 2019, ARXIV190803265
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2905, DOI 10.1109/TMM.2020.2965491
   Mubariz N, 2018, VISAPP: PROCEEDINGS OF THE 13TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS - VOL 4: VISAPP, P348, DOI 10.5220/0006613303480355
   Perwaiz N, 2019, 2019 INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION IN INDUSTRY (ICRAI), DOI 10.1109/icrai47710.2019.8967389
   Perwaiz N, 2020, 2020 23 INT MULTITOP, P1
   Perwaiz N, 2021, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.447
   Perwiaz N, 2018, IEEE ACCESS, V6, P77334, DOI 10.1109/ACCESS.2018.2882254
   Radford A., 2019, LANGUAGE MODELS ARE
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Si JL, 2018, PROC CVPR IEEE, P5363, DOI 10.1109/CVPR.2018.00562
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang GC, 2019, AAAI CONF ARTIF INTE, P8933
   Wei D, 2023, VISUAL COMPUT, V39, P501, DOI 10.1007/s00371-021-02344-7
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Zahra A., 2022, ARXIV220213121
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zhedong Zheng, 2017, ACM Transactions on Multimedia Computing, Communications and Applications, V14, DOI 10.1145/3159171
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng Liang, 2016, arXiv preprint arXiv
   Zhong Z, 2019, IEEE T IMAGE PROCESS, V28, P1176, DOI 10.1109/TIP.2018.2874313
NR 53
TC 13
Z9 13
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 4087
EP 4102
DI 10.1007/s00371-022-02577-0
EA JUL 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000825006500003
DA 2024-07-18
ER

PT J
AU Wang, SK
   Yang, J
   Chen, D
   Huang, J
   Zhang, YD
   Liu, W
   Zheng, ZH
   Li, YA
AF Wang, Sikai
   Yang, Jin
   Chen, Deng
   Huang, Jin
   Zhang, Yanduo
   Liu, Wei
   Zheng, Zhaohui
   Li, Yanan
TI LiteCortexNet: toward efficient object detection at night
SO VISUAL COMPUTER
LA English
DT Article
DE Object detection; Retinex; Deep learning; Nighttime image
ID NETWORK
AB Efficiently detecting objects in the complex background at night with low illumination remains a challenge for image processing. To address this issue, this paper proposes LiteCortexNet, a lightweight deep learning object detection model inspired by the visual cortex. The model performs intrinsic image decomposition end-to-end to obtain the illumination-independent reflection component, fuses it with the output result of the depth-wise separable convolutional encoder, and then, sends it to the lightweight detection head for object classification and positioning. Leveraging the channel-wise attention mechanism, our model is optimized for detecting small objects as well as obscured objects. In order to evaluate our method, an image dataset of railway maintenance tools was constructed. Experimental results show that the proposed model achieves 90.56% mAP at 66FPS on this dataset, which outperforms state-of-the-art object detection models such as YoloV4 (Bochkovskiy et al. in arXiv:2004.10934) (82.34% mAP at 45FPS).
C1 [Wang, Sikai; Yang, Jin; Chen, Deng; Zhang, Yanduo; Liu, Wei; Li, Yanan] Wuhan Inst Technol, Sch Comp Sci & Engn, Wuhan, Peoples R China.
   [Huang, Jin] Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan, Peoples R China.
   [Zheng, Zhaohui] Wuhan Inst Technol, Sch Math & Phys, Wuhan, Peoples R China.
C3 Wuhan Institute of Technology; Wuhan Textile University; Wuhan Institute
   of Technology
RP Chen, D (corresponding author), Wuhan Inst Technol, Sch Comp Sci & Engn, Wuhan, Peoples R China.; Huang, J (corresponding author), Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan, Peoples R China.
EM dchen@wit.edu.cn; derick0320@foxmail.com
RI Li, Tao/IWU-9607-2023; Li, Juan/JEO-6872-2023; zheng, xin/JNS-5523-2023;
   LI, Wenhui/JCD-9947-2023; zhang, chen/JES-0371-2023; zhang,
   yue/JAC-3705-2023; Liu, Yujie/IWU-6535-2023; Liu, Song/KCX-6842-2024;
   Ma, Xiaodong/JAN-7473-2023; Lu, Wang/JVO-0416-2024; liu,
   junyang/IXD-1201-2023; zhou, you/KBC-3567-2024; Liu,
   Yilin/JWP-9153-2024; yang, zhou/KBB-6972-2024; yi, li/KFR-6141-2024; LI,
   Yueyi/JUF-7422-2023; Chen, Fang/JZE-4446-2024; liu,
   junyang/IXD-1252-2023; li, bo/JJC-2664-2023; Gu, Bingxin/JNS-4761-2023;
   Yang, Bo/JTS-4309-2023; Huang, Jingyi/KCY-2239-2024
OI Liu, Yujie/0000-0002-1153-6156; Liu, Yilin/0000-0002-7581-3933; Gu,
   Bingxin/0009-0005-5667-1430; 
FU National Natural Science Foundation of China [62171328]; Education
   Sciences Planning of Hubei Province of China [2019GA090]
FX This work was supported in part by (1) The National Natural Science
   Foundation of China (No. 62171328). (2) Education Sciences Planning of
   Hubei Province of China (No.2019GA090).
CR Andrasi P, 2017, TRANSP RES PROC, V28, P183, DOI 10.1016/j.trpro.2017.12.184
   Bo Wei, 2019, 2019 International Conference on Communications, Information System and Computer Engineering (CISCE). Proceedings, P249, DOI 10.1109/CISCE.2019.00064
   Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Boyu Lu, 2020, IEEE Transactions on Biometrics, Behavior, and Identity Science, V2, P26, DOI 10.1109/TBIOM.2019.2959133
   Bradski Gary, 2000, Dr. Dobb's journal of software tools, V3, P2
   Chi C, 2019, AAAI CONF ARTIF INTE, P8231
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding XH, 2019, IEEE I CONF COMP VIS, P1911, DOI 10.1109/ICCV.2019.00200
   El Madawi K, 2019, IEEE INT C INTELL TR, P7, DOI [10.1109/ITSC.2019.8917447, 10.1109/itsc.2019.8917447]
   Girshick Ross, 2011, Advances in Neural Information Processing Systems, V24, P2
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Hu SM, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-020-3097-4
   Iandola Forrest N, 2016, SQUEEZENET ALEXNET L
   Jacob B, 2018, PROC CVPR IEEE, P2704, DOI 10.1109/CVPR.2018.00286
   LAND EH, 1964, AM SCI, V52, P247
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524
   Loh YP, 2019, COMPUT VIS IMAGE UND, V178, P30, DOI 10.1016/j.cviu.2018.10.010
   Loshchilov I., 2019, DECOUPLED WEIGHT DEC
   Masood A, 2020, IEEE T IND INFORM, V16, P7791, DOI 10.1109/TII.2020.2972918
   Nazir A, 2022, IEEE T AFFECT COMPUT, V13, P845, DOI 10.1109/TAFFC.2020.2970399
   Qu HQ, 2018, 2018 11TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2018)
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Romero A., 2014, ARXIV14126550
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Yuan Y, 2019, LECT NOTES COMPUT SC, V11542, P336, DOI 10.1007/978-3-030-22514-8_28
   Zhou JC, 2019, IEEE ACCESS, V7, P122459, DOI 10.1109/ACCESS.2019.2934981
   Zhou X, 2019, PSYCHOL HEALTH, V34, P811, DOI 10.1080/08870446.2019.1574348
NR 37
TC 4
Z9 4
U1 6
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3073
EP 3085
DI 10.1007/s00371-022-02560-9
EA JUL 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000824873100001
DA 2024-07-18
ER

PT J
AU Raj, RJ
   Dharan, S
   Sunil, TT
AF Raj, R. Jisha
   Dharan, Smitha
   Sunil, T. T.
TI Optimal feature selection and classification of Indian classical dance
   hand gesture dataset
SO VISUAL COMPUTER
LA English
DT Article
DE Bag of visual words; Bharatanatyam mudra dataset; Ensemble classifiers;
   Multilayer perceptron; Random forest; SIFT
ID PERFORMANCE EVALUATION; FEATURE-EXTRACTION; VECTOR; SIFT; DESCRIPTORS;
   SURF
AB Classical dancers all over the world use various body gestures to communicate to the audience the intended meaning. The study of these gestures can help in better understanding of the dance forms and also for annotation purposes. Bharatanatyam, an Indian classical dance, uses elegant hand gestures (mudras), facial expressions and body movements to convey the meaning to the audience. There are 28 Asamyukta Hastas (single-hand gestures) and 23 Samyukta Hastas (double-hand gestures) in Bharatanatyam. Open datasets on Bharatanatyam dance gestures are not presently available. An exhaustive open dataset of 15,396 distinct single-hand gesture images and 13,035 distinct double-hand gesture images was created. In this paper, we intend to find an optimal feature descriptor for this dataset. Various feature descriptors like scale-invariant feature transform (SIFT), speeded-up robust features (SURF), oriented FAST and rotated BRIEF (ORB), KAZE, KAZE extended, accelerated-KAZE (A-KAZE), binary robust invariant scalable keypoints (BRISK) and SIKA were explored. The feature descriptors were coded using bag of visual words and classified using several classifiers like support vector machines (SVM), multilayer perceptron (MLP), Naive Bayes, logistic regression, decision tree, AdaBoost and random forest. From all these investigations, it is observed that KAZE descriptor and random forest combination has the topmost classification accuracy.
C1 [Raj, R. Jisha; Dharan, Smitha] APJ Abdul Kalam Technol Univ, Coll Engn, Chengannur 689121, Kerala, India.
   [Sunil, T. T.] Coll Engn, Attingal 695101, Kerala, India.
RP Raj, RJ (corresponding author), APJ Abdul Kalam Technol Univ, Coll Engn, Chengannur 689121, Kerala, India.
EM jisharaj@ceconline.edu; smitha@ceconline.edu; vu2swx@gmail.com
OI TT, Sunil/0000-0002-5113-9200
CR Alcantarilla PF, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.13
   Alcantarilla PF, 2012, LECT NOTES COMPUT SC, V7577, P214, DOI 10.1007/978-3-642-33783-3_16
   Anami BS, 2018, MULTIMED TOOLS APPL, V77, P31021, DOI 10.1007/s11042-018-6223-y
   [Anonymous], 2013, NIPS
   Azhar R, 2015, PROCEDIA COMPUT SCI, V72, P24, DOI 10.1016/j.procs.2015.12.101
   Balntas V, 2017, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2017.410
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bedruz RAR, 2018, TENCON IEEE REGION, P1824, DOI 10.1109/TENCON.2018.8650119
   BELSON WA, 1959, ROY STAT SOC C-APP, V8, P65
   Bing Zhong, 2019, 2019 IEEE 4th International Conference on Image, Vision and Computing (ICIVC), P489, DOI 10.1109/ICIVC47709.2019.8981329
   Bishop Christopher M., 2006, Pattern Recognition and Machine Learning, V4
   Boswell D., 2002, Introduction to Support Vector Machines
   Boulkenafet Z, 2017, IEEE SIGNAL PROC LET, V24, P141, DOI 10.1109/LSP.2016.2630740
   BOURLARD H, 1988, BIOL CYBERN, V59, P291, DOI 10.1007/BF00332918
   Brunet D, 2012, IEEE T IMAGE PROCESS, V21, P1488, DOI 10.1109/TIP.2011.2173206
   Cheng VCC, 2020, J INFECTION, V81, P107, DOI 10.1016/j.jinf.2020.04.024
   Chien HJ, 2016, INT CONF IMAG VIS, P110, DOI 10.1109/DRC.2016.7548433
   Coomaraswamy A., 1917, MIRROR GESTURES BEIN
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Gangrade J, 2022, IETE J RES, V68, P2953, DOI 10.1080/03772063.2020.1739569
   Geron A., 2017, Hands-On Machine Learning With Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems
   Ghosh M., 1956, NATYASASTRA
   Gupta S, 2021, VISUAL COMPUT, V37, P447, DOI 10.1007/s00371-020-01814-8
   Hariharan D, 2011, LECT NOTES COMPUT SC, V6744, P186, DOI 10.1007/978-3-642-21786-9_32
   Hui J., 2016, J COMPUTATIONAL THEO, V13, P2006, DOI [10.1166/jctn.2016.5147, DOI 10.1166/jctn.2016.5147]
   Jisha Raj R., 2022, INT J IMAGE GRAPH
   Kumar K.V.V., 2017, INT J ELECT COMPUT E, V7
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Li JY, 2012, PATTERN RECOGN LETT, V33, P2094, DOI 10.1016/j.patrec.2012.08.002
   [梁焕青 Liang Huanqing], 2017, [测绘学报, Acta Geodetica et Cartographica Sinica], V46, P900
   Liu Q, 2016, INT J PATTERN RECOGN, V30, DOI 10.1142/S0218001416550168
   Liu YX, 2016, OPTIK, V127, P5670, DOI 10.1016/j.ijleo.2016.03.072
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mandal B, 2016, NEUROCOMPUTING, V184, P107, DOI 10.1016/j.neucom.2015.07.121
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Mohanty A, 2016, SIGNAL PROCESS-IMAGE, V47, P529, DOI 10.1016/j.image.2016.05.019
   Mozarkar S., 2013, INT J COMPUT SCI NET, V2, P46
   Okawa M, 2018, PATTERN RECOGN, V79, P480, DOI 10.1016/j.patcog.2018.02.027
   Ordóñez A, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10050756
   Parameshwaran A.P., 2019, P IEEE CVF C COMP VI
   Parashar D, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3071223
   Parashar D, 2020, IEEE SENS J, V20, P12885, DOI 10.1109/JSEN.2020.3001972
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Pieropan A., 2016, ARXIV PREPRINT ARXIV
   Pinto Binoy, 2011, 2011 International Conference on Communications and Signal Processing (ICCSP), P527, DOI 10.1109/ICCSP.2011.5739378
   Probst P, 2019, WIRES DATA MIN KNOWL, V9, DOI 10.1002/widm.1301
   Raileanu LE, 2004, ANN MATH ARTIF INTEL, V41, P77, DOI 10.1023/B:AMAI.0000018580.96245.c6
   Ramachandrasekhar P., 2013, ABHINAYADARPANAM, P71
   Rish Irina, 2001, IJCAI 2001 WORKSHOP, V3, P41
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Saha Sriparna, 2013, 2013 5th International Conference on Computational Intelligence and Communication Networks (CICN), P331, DOI 10.1109/CICN.2013.75
   Salahat E, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1059, DOI 10.1109/ICIT.2017.7915508
   Scovanner P., 2007, P 15 ACM INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Srimani P., 2013, Int. J. Curr. Res., V5, P1457
   Srivastava S, 2016, APPL SOFT COMPUT, V46, P1056, DOI 10.1016/j.asoc.2015.12.014
   Vanschoren J, 2018, ARXIV PREPRINT ARXIV
   Verma NK, 2016, PROC ADAPT LEARN OPT, V5, P415, DOI 10.1007/978-3-319-27000-5_34
   Yang Y, 2008, IEEE IMAGE PROC, P1852, DOI 10.1109/ICIP.2008.4712139
   Zhiyong Wang, 2011, Proceedings of the 2011 International Conference on Digital Image Computing: Techniques and Applications (DICTA 2011), P650, DOI 10.1109/DICTA.2011.115
NR 59
TC 6
Z9 6
U1 2
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 4049
EP 4064
DI 10.1007/s00371-022-02572-5
EA JUN 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000817073900001
DA 2024-07-18
ER

PT J
AU Liu, Q
   Zhao, JC
   Cheng, CJ
   Sheng, B
   Ma, LZ
AF Liu, Qi
   Zhao, Jiacheng
   Cheng, Changjie
   Sheng, Bin
   Ma, Lizhuang
TI <i>PointALCR</i>: adversarial latent GAN and contrastive regularization
   for point cloud completion
SO VISUAL COMPUTER
LA English
DT Article
DE Point cloud completion; GAN; Contrastive regularization
AB Development of LiDAR and depth camera makes it easily to extract the point cloud data of practical items. However, some drawbacks, such as sparsity or loss of details of the point cloud, are evident. Therefore, quite different from the methods as developed so far which usually reconstructed incomplete point cloud either in terms of GAN-based or autoencoder-based networks, respectively. In this paper, we propose PointALCR, which combines GAN-based and autoencoder-based frameworks with contrastive regularization in order to improve the representative and generative abilities for completion of the point cloud. A module named Adversarial Latent GAN be employed for learning a latent space of input/target point cloud representation and extending the generative and discriminative abilities on GAN training procedures. Contrastive regularization ensures that the reconstructed items to be close to the ground truth and far from the incomplete input in feature space. Experimental results demonstrate that PointALCR has the capabilities better than previous methods on challenging point cloud completion tasks.
C1 [Liu, Qi; Cheng, Changjie; Sheng, Bin; Ma, Lizhuang] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
   [Zhao, Jiacheng] Zhejiang Univ, Dept Comp Sci & Engn, Yuhangtang Rd 388, Hangzhou, Peoples R China.
C3 Shanghai Jiao Tong University; Zhejiang University
RP Sheng, B; Ma, LZ (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
EM enerald@sjtu.edu.cn; 3180103606@zju.edu.cn; cjcheng@sjtu.edu.cn;
   shengbin@cs.sjtu.edu.cn; ma-lz@cs.sjtu.edu.cn
RI Liu, Qifa/A-1469-2013; Li, Shiyu/KHE-1376-2024; Zhao,
   Jiacheng/GRO-1565-2022; Sun, Peng/KDO-4243-2024
FU National Key Research and Development Program of China [2019YFC1521104];
   National Natural Science Foundation of China [72192821, 61972157,
   61872241]; Shanghai Municipal Science and Technology Major Project
   [2021SHZDZX0102]; Shanghai Science and Technology Commission
   [21511101200, 22YF1420300]
FX This work was supported by the National Key Research and Development
   Program of China (2019YFC1521104), National Natural Science Foundation
   of China (72192821, 61972157, 61872241), Shanghai Municipal Science and
   Technology Major Project (2021SHZDZX0102), Shanghai Science and
   Technology Commission (21511101200, 22YF1420300).
CR Achlioptas P, 2018, ICML
   [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Gadelha M, 2018, LECT NOTES COMPUT SC, V11211, P105, DOI 10.1007/978-3-030-01234-2_7
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Grill J.-B., 2020, P ADV NEUR INF PROC, V33, P21271
   Guo D., 2022, ARXIV220308485
   Henaff OJ, 2020, PR MACH LEARN RES, V119
   Hermosilla P, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275110
   Huang Z, 2020, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR42600.2020.00316
   Jiang M., 2018, ARXIV180700652
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kingma D. P., 2013, ARXIV13126114
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Li H, 2021, INT J CONTROL, V94, P2575, DOI 10.1080/00207179.2020.1718771
   Li RH, 2019, IEEE I CONF COMP VIS, P7202, DOI 10.1109/ICCV.2019.00730
   Li Y., 2018, NEUIPS
   Lin CH, 2018, AAAI CONF ARTIF INTE, P7114
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu MH, 2020, AAAI CONF ARTIF INTE, V34, P11596
   Norouzi, 2002, ARXIV200205709
   Pan L, 2021, PROC CVPR IEEE, P8520, DOI 10.1109/CVPR46437.2021.00842
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Saining Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P574, DOI 10.1007/978-3-030-58580-8_34
   Sermanet Pierre, 2018, 2018 IEEE International Conference on Robotics and Automation (ICRA), P1134, DOI 10.1109/ICRA.2018.8462891
   Tchapmi LP, 2019, PROC CVPR IEEE, P383, DOI 10.1109/CVPR.2019.00047
   Wang XG, 2020, PROC CVPR IEEE, P787, DOI 10.1109/CVPR42600.2020.00087
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yi L, 2017, PROC CVPR IEEE, P6584, DOI 10.1109/CVPR.2017.697
   Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238
   Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295
   Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088
   Zhang W., 2020, P EUR C COMP VIS, P512, DOI 10.1007/978-3-030-58595-2_31
   Zhao YH, 2019, PROC CVPR IEEE, P1009, DOI 10.1109/CVPR.2019.00110
NR 36
TC 6
Z9 6
U1 3
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3341
EP 3349
DI 10.1007/s00371-022-02550-x
EA JUN 2022
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000815400600001
OA Bronze
DA 2024-07-18
ER

PT J
AU Ma, ZH
   Yuan, MK
   Gu, JM
   Meng, WL
   Xu, SB
   Zhang, XP
AF Ma, Zhihao
   Yuan, Mengke
   Gu, Jiaming
   Meng, Weiliang
   Xu, Shibiao
   Zhang, Xiaopeng
TI Triple-strip attention mechanism-based natural disaster images
   classification and segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Natural disaster image analysis; Image segmentation; Attention mechanism
AB Fast and accurate semantic analysis of natural disaster images is crucial for rational rescue plans and resource allocation. However, the scarcity of meticulously labelled datasets and the ignorance of region-of-interest scale variations of popular general-purpose methods lead to undesirable performance. In this paper, we propose a novel triple-strip attention mechanism (TSAM) to solve the generalization problem of disaster images that can be combined into general networks. Our TSAM accumulates features of three parallel-strip attentions (row strip attention, column strip attention, and channel strip attention), and the output is multiplied with original input features for further processing. Our attention mechanism can effectively overcome the defect of ignoring global features caused by the convolution and enhance the performance of the network by weighting the features from both spatial and channel aspects more comprehensively. Besides, we employ both the compression and expansion operations in the weighting operation to reduce the amount of parameters, leading to negligible computational overhead. Experiments validate that our TSAM outperforms other state-of-the-art methods on natural disaster segmentation. Due to its concise form, plug-and-play pattern, and high promotion rate, our TSAM can be combined with many existing neural networks for better performance improvement.
C1 [Ma, Zhihao; Yuan, Mengke; Gu, Jiaming; Meng, Weiliang; Zhang, Xiaopeng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China.
   [Meng, Weiliang; Zhang, Xiaopeng] Zhejiang Lab, Hangzhou, Peoples R China.
   [Ma, Zhihao; Yuan, Mengke; Gu, Jiaming; Meng, Weiliang; Zhang, Xiaopeng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Xu, Shibiao] Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Zhejiang
   Laboratory; Chinese Academy of Sciences; University of Chinese Academy
   of Sciences, CAS; Beijing University of Posts & Telecommunications
RP Meng, WL (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China.; Meng, WL (corresponding author), Zhejiang Lab, Hangzhou, Peoples R China.; Meng, WL (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.; Xu, SB (corresponding author), Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing, Peoples R China.
EM weiliang.meng@ia.ac.cn; shibiaoxu@bupt.edu.cn
RI ma, zhihao/GZG-7257-2022
OI meng, wei liang/0000-0002-3221-4981
FU National Natural Science Foundation of China [U21A20515, 61972459,
   62172416, 62102414, U2003109, 62071157, 62171321, 62162044, 2021KE0AB07,
   TC210H00L/42]
FX This work was supported in part by the National Natural Science
   Foundation of China (Nos. U21A20515, 61972459, 62172416, 62102414,
   U2003109, 62071157, 62171321, and 62162044), in part by Open Research
   Projects ofZhejiangLab (No. 2021KE0AB07) and the Project TC210H00L/42.
CR [Anonymous], 2016, NIPS Workshop on Adversarial Training
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Barz B., 2018, ARCH DATA SCI SERIES ARCH DATA SCI SERIES, V5, pA06
   BOTTOU L, 1994, INT C PATT RECOG, P77, DOI 10.1109/ICPR.1994.576879
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen YC, 2021, J RES SCI TEACH, V58, P1083, DOI 10.1002/tea.21693
   Chen YP, 2018, ADV NEUR IN, V31
   CHOLLET F, 2017, PROC CVPR IEEE, P1800, DOI DOI 10.1109/CVPR.2017.195
   Chowdhury, ARXIV PREPRINT ARXIV
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Ghaffarian S, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13152965
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hou QB, 2020, PROC CVPR IEEE, P4002, DOI 10.1109/CVPR42600.2020.00406
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu XW, 2018, PROC CVPR IEEE, P7454, DOI 10.1109/CVPR.2018.00778
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Ji SP, 2020, LANDSLIDES, V17, P1337, DOI 10.1007/s10346-020-01353-2
   Li X, 2019, Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Liu W, 2016, INT WORKS EARTH OB
   Liu Z., ARXIV PREPRINT ARXIV
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Misra D, 2021, IEEE WINT CONF APPL, P3138, DOI 10.1109/WACV48630.2021.00318
   Niloy FF, 2021, INT C PATT RECOG, P5115, DOI 10.1109/ICPR48806.2021.9412504
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Park J., 2018, BRIT MACH VIS C, P147
   Rahnemoonfar M, 2021, IEEE ACCESS, V9, P89644, DOI 10.1109/ACCESS.2021.3090981
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rottensteiner F., 2013, ISPRS COMM 3 PHOT CO, P1, DOI DOI 10.5194/isprsannals-I-3-293-2012
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srinivas A, 2021, PROC CVPR IEEE, P16514, DOI 10.1109/CVPR46437.2021.01625
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Szegedy Christian, 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.308
   Tan MX, 2019, PR MACH LEARN RES, V97
   Walter SM, 2021, J NEUROSCI RURAL PRA, V12, P524, DOI 10.1055/s-0041-1727574
   Wang JH, 2020, IEEE T CYBERNETICS, V50, P2971, DOI 10.1109/TCYB.2019.2891265
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Zhang QL, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2235, DOI 10.1109/ICASSP39728.2021.9414568
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
NR 47
TC 9
Z9 9
U1 1
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3163
EP 3173
DI 10.1007/s00371-022-02535-w
EA JUN 2022
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000812611200001
OA Bronze
DA 2024-07-18
ER

PT J
AU Panda, J
   Nanda, PK
AF Panda, Jyotiranjan
   Nanda, Pradipta Kumar
TI Particle filter-based video object tracking using feature fusion in
   template partitions
SO VISUAL COMPUTER
LA English
DT Article
DE Feature fusion; LBP; EOH; HSV; Mean color histogram; Template
   partitions; Particle filter
ID ADAPTIVE FUSION; MODEL; CONDENSATION
AB Moving object tracking is one of the key issues in the domain of computer vision. A variety of challenges are posed while tracking the object in the real-world scenario. In this paper, we have proposed a particle filtering-based algorithm to track the moving object in a complex real-world environment having shadows, dynamic entities in the background, bad weather condition and illumination variation. Specifically, we have attempted to have an effective target and scene model by adhering to the notion of feature fusion. In this regard, a novel feature fusion strategy is proposed which is scene dependent. Besides, for an efficient target model, the template for tracking is partitioned with a view to capture the local attributes of the object. In every partition, two features, namely the local binary pattern (LBP) and the mean RGB color features, are fused in a probabilistic framework. The weights for the probabilistic fusion are computed based on the local scene dynamics. The feature that strongly favors the scene in a given template partition is assigned more weightage and vice versa in the fusion process. The combined feature over all the template partitions is used to model the target. The particles that represent the state of the object evolve through a dynamic state model to track the object. The proposed tracking algorithm is successfully tested on videos considered from LASIESTA, CDnet, and DAVIS 2016 data base and it showed improved tracking accuracy as compared to existing algorithms.
C1 [Panda, Jyotiranjan; Nanda, Pradipta Kumar] Siksha O Anusandhan, Image & Video Anal Lab, Dept Elect & Commun Engn, Inst Tech Educ & Res, Bhubaneswar 751030, India.
C3 Siksha 'O' Anusandhan University
RP Panda, J (corresponding author), Siksha O Anusandhan, Image & Video Anal Lab, Dept Elect & Commun Engn, Inst Tech Educ & Res, Bhubaneswar 751030, India.
EM jyotiranjanpanda@soa.ac.in; pknanda@soa.ac.in
RI Nanda, Pradipta/CAH-2467-2022
OI , prof. Pradipta Kumar Nanda/0000-0002-6147-5267; Panda,
   Jyotiranjan/0000-0002-2037-2341
CR Aherne FJ, 1998, KYBERNETIKA, V34, P363
   Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374
   Bhat PG, 2020, IEEE SENS J, V20, P2405, DOI 10.1109/JSEN.2019.2954331
   Birchfield S.T., 2005, IEEE COMPUTER SOC C, V2
   Bouaynaya N, 2009, IEEE T CIRC SYST VID, V19, P1068, DOI 10.1109/TCSVT.2009.2020477
   Chan YM, 2012, IET INTELL TRANSP SY, V6, P1, DOI 10.1049/iet-its.2011.0019
   Chen C, 2010, IEEE T IMAGE PROCESS, V19, P1625, DOI 10.1109/TIP.2010.2043009
   Cuevas C, 2016, COMPUT VIS IMAGE UND, V152, P103, DOI 10.1016/j.cviu.2016.08.005
   Ding DS, 2016, CHIN CONTR CONF, P4031, DOI 10.1109/ChiCC.2016.7553983
   Doucet A, 2001, STAT ENG IN, P3
   García J, 2013, IEEE T SYST MAN CY-S, V43, P606, DOI 10.1109/TSMCA.2012.2220540
   GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596
   GORDON NJ, 1993, IEE PROC-F, V140, P107, DOI 10.1049/ip-f-2.1993.0015
   Goyette N., 2012, 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), DOI 10.1109/CVPRW.2012.6238919
   Greminger MA, 2003, PROC CVPR IEEE, P289
   Heikkilä M, 2006, IEEE T PATTERN ANAL, V28, P657, DOI 10.1109/TPAMI.2006.68
   Huan RH, 2018, IET IMAGE PROCESS, V12, P1519, DOI 10.1049/iet-ipr.2017.1068
   Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650
   Isard M., 1996, COMPUTER VISIONECCV, V96
   Kalman R E., 1960, J BASIC ENG, V82, P35, DOI DOI 10.1115/1.3662552
   KITAGAWA G, 1987, J AM STAT ASSOC, V82, P1032, DOI 10.2307/2289375
   KOLLER D, 1993, INT J COMPUT VISION, V10, P257, DOI 10.1007/BF01539538
   Li ZY, 2017, J VIS COMMUN IMAGE R, V44, P1, DOI [10.1016/j.jvcir.2017.01.012, 10.16339/j.cnki.hdxbzkb.2017.11.001]
   Li ZY, 2015, VISUAL COMPUT, V31, P1319, DOI 10.1007/s00371-014-1014-6
   Lin SFD, 2015, IET IMAGE PROCESS, V9, P959, DOI 10.1049/iet-ipr.2014.0666
   Liu L, 2018, J VIS COMMUN IMAGE R, V57, P99, DOI 10.1016/j.jvcir.2018.10.020
   Lu X., 2012, 7 IEEE C IND EL APPL
   Niknejad HT, 2012, IEEE T INTELL TRANSP, V13, P748, DOI 10.1109/TITS.2012.2187894
   Nummiaro K, 2003, IMAGE VISION COMPUT, V21, P99, DOI 10.1016/S0262-8856(02)00129-4
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Panda Jyotiranjan, 2020, Advances in Electrical Control and Signal Systems. Select Proceedings of AECSS 2019. Lecture Notes in Electrical Engineering (LNEE 665), P945, DOI 10.1007/978-981-15-5262-5_73
   Panda J., 2021, Green Technology For Smart City And Society, P339
   Perazzi F., 2016, COMPUTER VISION PATT
   Pérez P, 2002, LECT NOTES COMPUT SC, V2350, P661
   Polat E, 2003, PATTERN RECOGN, V36, P2127, DOI 10.1016/S0031-3203(03)00041-4
   Shu XB, 2022, IEEE T PATTERN ANAL, V44, P3300, DOI 10.1109/TPAMI.2021.3050918
   Shu XB, 2021, IEEE T NEUR NET LEAR, V32, P663, DOI 10.1109/TNNLS.2020.2978942
   Shu XB, 2021, IEEE T PATTERN ANAL, V43, P1110, DOI 10.1109/TPAMI.2019.2942030
   Talha M, 2014, IEEE SENS J, V14, P159, DOI 10.1109/JSEN.2013.2271561
   Tang JH, 2022, IEEE T PATTERN ANAL, V44, P636, DOI 10.1109/TPAMI.2019.2928540
   Wang Y., 2014, 2014 IEEE C COMPUTER
   Wang Y, 2019, IEEE ACCESS, V7, P133694, DOI 10.1109/ACCESS.2019.2941365
   Xu F., 2012, P 10 WORLD C INTELLI
   Yang C., 2005, 10 IEEE INT C COMPUT, V1
   Zhao XK, 2019, IEEE ACCESS, V7, P10294, DOI 10.1109/ACCESS.2019.2891172
   Zhong Y, 2000, IEEE T PATTERN ANAL, V22, P544, DOI 10.1109/34.857008
   Zolfaghari M, 2022, VISUAL COMPUT, V38, P849, DOI 10.1007/s00371-020-02055-5
NR 47
TC 2
Z9 2
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2757
EP 2779
DI 10.1007/s00371-022-02490-6
EA MAY 2022
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000790216200003
DA 2024-07-18
ER

PT J
AU Wang, HX
   Chen, YQ
   Wu, M
   Zhang, X
   Huang, Z
   Mao, WP
AF Wang, Hongxing
   Chen, Yuquan
   Wu, Mei
   Zhang, Xin
   Huang, Zheng
   Mao, Weiping
TI Attentional and adversarial feature mimic for efficient object detection
SO VISUAL COMPUTER
LA English
DT Article
DE Object detection; Knowledge distillation; Attention network; Feature
   mimic
AB In this paper, we focus on learning efficient object detectors by knowledge (or network) distillation. More specifically, we mimic features from deeper and larger teacher networks to help train better efficient student networks. Unlike the previous method that mimics features through minimizing an L2 loss between feature generated by teacher and student networks, we propose an attentional and adversarial feature mimic (AAFM) method which consists of an attentional feature mimic module and an adversarial feature mimic module, where the former module uses an attentional L2 loss which learns to pay attention to important object-related regions for feature mimic, and the latter module uses an adversarial loss which makes features generated by teacher and student networks have similar distributions. We apply our AAFM method in the two-stage Faster R-CNN detector. Experiments on the PASCAL VOC 2007 and COCO datasets show that our method consistently improves the performance of detectors without feature mimic or with other feature mimic methods. In particular, our method obtains 72.1% mAP on the PASCAL VOC 2007 dataset using the ResNet-18-based detector.
C1 [Wang, Hongxing; Chen, Yuquan; Wu, Mei; Zhang, Xin; Huang, Zheng] Jiangsu Frontier Elect Power Technol Co Ltd, Nanjing 210036, Peoples R China.
   [Mao, Weiping] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology
RP Mao, WP (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
EM weipingmao@hust.edu.cn
RI Wang, Hongxing/F-4670-2011
CR [Anonymous], 2015, 3 INT C LEARN REPR
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Carion Nicolas, 2020, EUR C COMP VIS ECCV, DOI DOI 10.1007/978-3-030-58452-8_13
   Chatfield K., 2014, P BRIT MACH VIS C 20
   Chen GB, 2017, ADV NEUR IN, V30
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chorowski J, 2015, ADV NEUR IN, V28
   Dai X., ARXIV PREPRINT ARXIV
   Dean J., 2015, NIPS DEEP LEARNING R
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Gao Y, 2019, PROC CVPR IEEE, P3200, DOI 10.1109/CVPR.2019.00332
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gkioxari G., 2017, ICCV, P2961
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo J., ARXIV PREPRINT ARXIV
   Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309
   Hara K., 2017, ARXIV170201478
   He Kaiming, 2016, P INT C COMP VIS PAT, DOI 10.1109/CVPR.2016.90
   He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Kim K.-H., 2016, Pvanet: deep but lightweight neural networks for real-time object detection
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li JA, 2017, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2017.211
   Li QQ, 2017, PROC CVPR IEEE, P7341, DOI 10.1109/CVPR.2017.776
   Li Z, 2017, ARXIV171107264
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Long J., 2015, P IEEE C COMP VIS PA, P3431
   Maas AL., 2013, P ICML WORKSHOP DEEP, V28, P1
   Mittal D, 2018, IEEE WINT CONF APPL, P848, DOI 10.1109/WACV.2018.00098
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Ozturk C, 2018, P EUR C COMP VIS ECC
   Paszke A., 2017, AUTOMATIC DIFFERENTI
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Romero A., 2014, ARXIV14126550
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sukhbaatar Sainbayar, 2015, ADV NEURAL INFORM PR, P2440
   Sun Peize, 2020, ARXIV201112450
   Sun Peize, 2020, ARXIV201205780
   Sun RY, 2020, IEEE SIGNAL PROC MAG, V37, P95, DOI 10.1109/MSP.2020.3004124
   Tang P., 2018, ARXIV PREPRINT ARXIV
   Tang P, 2020, IEEE T PATTERN ANAL, V42, P176, DOI 10.1109/TPAMI.2018.2876304
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780
   Wang Chaoqi, 2020, INT C LEARN REPR
   Wang J., ARXIV PREPRINT ARXIV
   Wang T, 2019, PROC CVPR IEEE, P4928, DOI 10.1109/CVPR.2019.00507
   Wang X, 2017, Point linking network for object detection
   Wang YH, 2018, AAAI CONF ARTIF INTE, P4260
   Wei LX, 2021, VISUAL COMPUT, V37, P133, DOI 10.1007/s00371-019-01787-3
   Wei Y, 2018, LECT NOTES COMPUT SC, V11212, P274, DOI 10.1007/978-3-030-01237-3_17
   Xu Z., 2018, BMVC BRIT MACHINE VI
   Yang TJ, 2018, LECT NOTES COMPUT SC, V11214, P289, DOI 10.1007/978-3-030-01249-6_18
   Yao Z., ARXIV PREPRINT ARXIV
   Yu JH, 2019, IEEE I CONF COMP VIS, P1803, DOI 10.1109/ICCV.2019.00189
   Zhang KH, 2019, IEEE T IMAGE PROCESS, V28, P291, DOI 10.1109/TIP.2018.2867733
   Zhang KH, 2017, IEEE T IMAGE PROCESS, V26, P4193, DOI 10.1109/TIP.2017.2689999
   Zhou K, 2016, DESTECH TRANS COMP
   Zhou X., 2019, ABS190407850 ARXIV
   Zhu X., ARXIV PREPRINT ARXIV
   Zhu YS, 2019, IEEE T IMAGE PROCESS, V28, P113, DOI 10.1109/TIP.2018.2865280
NR 67
TC 5
Z9 5
U1 4
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2023
VL 39
IS 2
BP 639
EP 650
DI 10.1007/s00371-021-02363-4
EA JAN 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8T2TV
UT WOS:000741634600005
DA 2024-07-18
ER

PT J
AU Bahroun, S
   Abed, R
   Zagrouba, E
AF Bahroun, Sahbi
   Abed, Rahma
   Zagrouba, Ezzeddine
TI Deep 3D-LBP: CNN-based fusion of shape modeling and texture descriptors
   for accurate face recognition
SO VISUAL COMPUTER
LA English
DT Article
DE 3D morphable model; Mesh-LBP; CNN; Face recognition
ID FACIAL EXPRESSION RECOGNITION; SINGLE-SAMPLE
AB The key challenge of face recognition is to develop an effective feature representations for reducing intrapersonal variations while enlarging interpersonal differences. In this paper, we show that the face recognition accuracy may be enhanced with the combination of a 3D model-based alignment, and an LBP descriptor constructed on the 3D mesh. First, 3D face data are reconstructed from 2D images that aim to normalize the input image. Then, shape and texture features on the mesh are extracted using the mesh local binary patterns: mesh-LBP. With the use of the extracted 3D features and a simple CNN architecture, much higher accuracy rates can be achieved. We achieve the accuracy of 99.59% on the widely used labeled Faces in the Wild dataset. On YouTube Faces dataset, the proposed method achieves 94.97%, despite using a small training dataset.
C1 [Bahroun, Sahbi; Abed, Rahma; Zagrouba, Ezzeddine] Univ Tunis El Manar, Inst Superieur dInformat, Lab LIMTIC, 2 Rue Abou Rayhane Bayrouni, Ariana 2080, Tunisia.
C3 Universite de Tunis-El-Manar
RP Abed, R (corresponding author), Univ Tunis El Manar, Inst Superieur dInformat, Lab LIMTIC, 2 Rue Abou Rayhane Bayrouni, Ariana 2080, Tunisia.
EM sahbi.bahroun@isi.utm.tn; rahma.abed@etudiant-isi.utm.tn;
   ezzeddine.zagrouba@uvt.tn
RI Bahroun, Sahbi/HLW-7732-2023; Zagrouba, Ezzeddine/D-7896-2014
OI Zagrouba, Ezzeddine/0000-0002-2574-9080; ABED, rahma/0000-0003-1560-1848
CR Abbad A, 2018, COMPUT ELECTR ENG, V70, P525, DOI 10.1016/j.compeleceng.2017.08.017
   Abdurrahim SH, 2018, VISUAL COMPUT, V34, P1617, DOI 10.1007/s00371-017-1428-z
   Ahmed SB, 2020, ARTIF INTELL REV, V53, P2571, DOI 10.1007/s10462-019-09742-3
   Aldrian O, 2013, IEEE T PATTERN ANAL, V35, P1080, DOI 10.1109/TPAMI.2012.206
   Anil J., 2016, Circuit, Power and Computing Technologies (ICCPCT), 2016 International Conference on, P1, DOI [10.1109/ICCPCT.2016.7530173, DOI 10.1109/1CCPCT.2016.7530173]
   [Anonymous], 2014, ARXIV14117923
   Anwarul S, 2020, LECT NOTES ELECTR EN, V597, P495, DOI 10.1007/978-3-030-29407-6_36
   Arya K. V., 2019, Computational Intelligence: Theories, Applications and Future DirectionsVolume II. ICCI-2017. Advances in Intelligent Systems and Computing (AISC 799), P645, DOI 10.1007/978-981-13-1135-2_49
   Asthana A, 2011, IEEE I CONF COMP VIS, P937, DOI 10.1109/ICCV.2011.6126336
   Atik Muhammed Enes, 2021, Innovations in Smart Cities Applications. Proceedings of the 5th International Conference on Smart City Applications. Lecture Notes in Networks and Systems (LNNS 183), P797, DOI 10.1007/978-3-030-66840-2_60
   Azazi A, 2015, EXPERT SYST APPL, V42, P3056, DOI 10.1016/j.eswa.2014.10.042
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Berretti S, 2013, COMPUT GRAPH-UK, V37, P509, DOI 10.1016/j.cag.2013.04.001
   Bodini M, 2019, BIG DATA COGN COMPUT, V3, DOI 10.3390/bdcc3010014
   Cao J, 2020, INT J COMPUT VISION, V128, P1485, DOI 10.1007/s11263-019-01229-6
   Cao ZM, 2010, PROC CVPR IEEE, P2707, DOI 10.1109/CVPR.2010.5539992
   Chakraborty S, 2021, MULTIMED TOOLS APPL, V80, P4007, DOI 10.1007/s11042-020-09857-8
   Chang KI, 2005, IEEE T PATTERN ANAL, V27, P619, DOI 10.1109/TPAMI.2005.70
   Chen D, 2013, PROC CVPR IEEE, P3025, DOI 10.1109/CVPR.2013.389
   Cheng Y, 2017, VISUAL COMPUT, V33, P1483, DOI 10.1007/s00371-017-1357-x
   Chengeta K, 2018, LECT NOTES ARTIF INT, V11055, P513, DOI 10.1007/978-3-319-98443-8_47
   Chihaoui M, 2016, COMPUTERS, V5, DOI 10.3390/computers5040021
   Chu YJ, 2019, VISUAL COMPUT, V35, P239, DOI 10.1007/s00371-017-1468-4
   Dahl GE, 2013, INT CONF ACOUST SPEE, P8609, DOI 10.1109/ICASSP.2013.6639346
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng WH, 2017, PATTERN RECOGN, V68, P260, DOI 10.1016/j.patcog.2017.03.024
   Deng X, 2020, COMPUT ELECTR ENG, V85, DOI 10.1016/j.compeleceng.2020.106700
   Deng X, 2017, COMPUT ELECTR ENG, V62, P81, DOI 10.1016/j.compeleceng.2017.01.028
   Ding CX, 2017, PATTERN RECOGN, V66, P144, DOI 10.1016/j.patcog.2016.11.024
   Ding CX, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2845089
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Egger B, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3395208
   Ferrari C, 2016, INT C PATT RECOG, P1047, DOI 10.1109/ICPR.2016.7899774
   Ferreira L, 2016, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-016-0131-8
   Finizola JS, 2019, IEEE IJCNN
   Fredj H.B., 2020, FACE RECOGNITION UNC, P1
   Garcia-Garcia A, 2018, APPL SOFT COMPUT, V70, P41, DOI 10.1016/j.asoc.2018.05.018
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Gu JX, 2018, PATTERN RECOGN, V77, P354, DOI 10.1016/j.patcog.2017.10.013
   Guo GD, 2019, COMPUT VIS IMAGE UND, V189, DOI 10.1016/j.cviu.2019.102805
   Gupta S, 2021, VISUAL COMPUT, V37, P447, DOI 10.1007/s00371-020-01814-8
   Hariri W, 2017, ENG APPL ARTIF INTEL, V64, P25, DOI 10.1016/j.engappai.2017.05.009
   Hartley R., MULTIPLE VIEW GEOMET
   Hassaballah M, 2015, IET COMPUT VIS, V9, P614, DOI 10.1049/iet-cvi.2014.0084
   Hassner T, 2015, PROC CVPR IEEE, P4295, DOI 10.1109/CVPR.2015.7299058
   Hu C, 2020, IEEE ACCESS, V8, P130159, DOI 10.1109/ACCESS.2020.3009512
   Hu GS, 2017, PATTERN RECOGN, V67, P366, DOI 10.1016/j.patcog.2017.02.007
   Hu GS, 2016, LECT NOTES COMPUT SC, V9912, P73, DOI 10.1007/978-3-319-46484-8_5
   Hu YB, 2018, PROC CVPR IEEE, P8398, DOI 10.1109/CVPR.2018.00876
   Huang G. B., 2008, WORKSH FAC REAL LIF
   Huang GB, 2012, PROC CVPR IEEE, P2518, DOI 10.1109/CVPR.2012.6247968
   Huang R, 2017, IEEE I CONF COMP VIS, P2458, DOI 10.1109/ICCV.2017.267
   Huber Patrik, 2016, VISIGRAPP 2016. 11th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications. Proceedings: VISAPP 2016, P79
   Huibin Li, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3053, DOI 10.1109/ICIP.2011.6116308
   Nhat HTM, 2019, 2019 26TH INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS (ICT), P371, DOI [10.1109/ict.2019.8798816, 10.1109/ICT.2019.8798816]
   Ho HT, 2013, IEEE T IMAGE PROCESS, V22, P1571, DOI 10.1109/TIP.2012.2233489
   Juefei-Xu F, 2015, IEEE T IMAGE PROCESS, V24, P4780, DOI 10.1109/TIP.2015.2468173
   Kafai M, 2014, IEEE T INF FOREN SEC, V9, P2132, DOI 10.1109/TIFS.2014.2359548
   Kan M, 2014, PROC CVPR IEEE, P1883, DOI 10.1109/CVPR.2014.243
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   Kim M, 2021, INT C PATT RECOG, P8952, DOI 10.1109/ICPR48806.2021.9413039
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Koppen P, 2018, PATTERN RECOGN, V74, P617, DOI 10.1016/j.patcog.2017.09.006
   Kortli Y, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20020342
   Kumar A, 2019, ARTIF INTELL REV, V52, P927, DOI 10.1007/s10462-018-9650-2
   Lei YJ, 2016, PATTERN RECOGN, V52, P218, DOI 10.1016/j.patcog.2015.09.035
   Li AN, 2012, IEEE T IMAGE PROCESS, V21, P305, DOI 10.1109/TIP.2011.2160957
   Li HB, 2015, INT J COMPUT VISION, V113, P128, DOI 10.1007/s11263-014-0785-6
   Li PP, 2019, IEEE I CONF COMP VIS, P10042, DOI 10.1109/ICCV.2019.01014
   Li SX, 2012, LECT NOTES COMPUT SC, V7572, P102, DOI 10.1007/978-3-642-33718-5_8
   Liang J, 2020, NEUROCOMPUTING, V410, P12, DOI 10.1016/j.neucom.2020.05.076
   Liang Y, 2020, J ELECTRON IMAGING, V29, DOI 10.1117/1.JEI.29.5.053008
   Mahamdioua M, 2018, IET COMPUT VIS, V12, P623, DOI 10.1049/iet-cvi.2017.0190
   Mian AS, 2007, IEEE T PATTERN ANAL, V29, P1927, DOI 10.1109/TPAMI.2007.1105
   Napoléon T, 2017, OPT LASER ENG, V89, P150, DOI 10.1016/j.optlaseng.2016.06.019
   Oloyede MO, 2020, MULTIMED TOOLS APPL, V79, P27891, DOI 10.1007/s11042-020-09261-2
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Petpairote C, 2021, WIRELESS PERS COMMUN, V118, P2015, DOI 10.1007/s11277-020-07063-1
   Ren XD, 2018, LECT NOTES ELECTR EN, V423, P699, DOI 10.1007/978-981-10-3229-5_74
   Sagonas C, 2015, IEEE I CONF COMP VIS, P3871, DOI 10.1109/ICCV.2015.441
   Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Shi LL, 2020, OPTIK, V220, DOI 10.1016/j.ijleo.2020.165157
   Singh A, 2020, SOFTWARE PRACT EXPER, V50, P2012, DOI 10.1002/spe.2722
   Sushama M, 2018, PROCEEDINGS OF THE 2018 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATION AND SIGNAL PROCESSING (ICCSP), P994, DOI 10.1109/ICCSP.2018.8524427
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Taskiran M, 2020, DIGIT SIGNAL PROCESS, V106, DOI 10.1016/j.dsp.2020.102809
   Trigueros D. S., FACE RECOGNITION TRA
   Voronov V., 2019, C OP INN ASS FRUCT F, P783
   Wang F, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1041, DOI 10.1145/3123266.3123359
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang S, 2022, VISUAL COMPUT, V38, P2539, DOI 10.1007/s00371-021-02129-y
   Wang Y, 2020, SOFT COMPUT, V24, P5859, DOI 10.1007/s00500-019-04380-x
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Werghi N, 2016, IEEE T INF FOREN SEC, V11, P964, DOI 10.1109/TIFS.2016.2515505
   Wolf L, 2011, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2011.5995566
   Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032
   Xu JY, 2021, VISUAL COMPUT, V37, P765, DOI 10.1007/s00371-020-01976-5
   Yeung HWF, 2017, IEEE IJCNN, P1948, DOI 10.1109/IJCNN.2017.7966089
   Yi D., 2014, LEARNING FACE REPRES, P2892
   Yi D, 2013, PROC CVPR IEEE, P3539, DOI 10.1109/CVPR.2013.454
   Yim J, 2015, PROC CVPR IEEE, P676, DOI 10.1109/CVPR.2015.7298667
   Yin X, 2017, IEEE I CONF COMP VIS, P4010, DOI 10.1109/ICCV.2017.430
   Yin Y., DUAL ATTENTION GAN L
   Yin Yu, 2020, 2020 15 IEEE INT C A
   Zhang BR, 2016, 2016 2ND IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATIONS (ICCC), P365, DOI 10.1109/CompComm.2016.7924724
   Zhang HS, 2017, 2017 IEEE 2ND ADVANCED INFORMATION TECHNOLOGY, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (IAEAC), P544, DOI 10.1109/IAEAC.2017.8054074
   Zhang MMY, 2019, SIGNAL PROCESS-IMAGE, V75, P118, DOI 10.1016/j.image.2019.03.015
   Zhang SF, 2019, IEEE IMAGE PROC, P2384, DOI [10.1109/ICIP.2019.8803362, 10.1109/icip.2019.8803362]
   Zhang YH, 2018, IET IMAGE PROCESS, V12, P819, DOI 10.1049/iet-ipr.2017.1085
   Zhang Z., 2019, DATA FREE POINT CLOU
   Zhang Z., 2019, ARXIV191104731
   Zhou S, 2018, HUM-CENT COMPUT INFO, V8, DOI 10.1186/s13673-018-0157-2
   Zhu XY, 2015, PROC CVPR IEEE, P787, DOI 10.1109/CVPR.2015.7298679
   Zhu Z., DEEP LEARNING MULTIV
   Zhu Z., RECOVER CANONICAL VI
   Zhu ZY, 2013, IEEE I CONF COMP VIS, P113, DOI 10.1109/ICCV.2013.21
NR 118
TC 5
Z9 5
U1 2
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 239
EP 254
DI 10.1007/s00371-021-02324-x
EA OCT 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000712932700001
DA 2024-07-18
ER

PT J
AU Gökstorp, SGE
   Breckon, TP
AF Gokstorp, Simon G. E.
   Breckon, Toby P.
TI Temporal and non-temporal contextual saliency analysis for generalized
   wide-area search within unmanned aerial vehicle (UAV) video
SO VISUAL COMPUTER
LA English
DT Article
ID MODEL
AB Unmanned aerial vehicles (UAV) can be used to great effect for wide-area searches such as search and rescue operations. UAV enable search and rescue teams to cover large areas more efficiently and in less time. However, using UAV for this purpose involves the creation of large amounts of data, typically in video format, which must be analysed before any potential findings can be uncovered and actions taken. This is a slow and expensive process which can result in significant delays to the response time after a target is seen by the UAV. To solve this problem we propose a deep model architecture using a visual saliency approach to automatically analyse and detect anomalies in UAV video. Our Temporal Contextual Saliency (TeCS) approach is based on the state-of-the-art in visual saliency detection using deep Convolutional Neural Networks (CNN) and considers local and scene context, with novel additions in utilizing temporal information through a convolutional Long Short-Term Memory (LSTM) layer and modifications to the base model architecture. We additionally evaluate the impact of temporal vs non-temporal reasoning for this task. Our model achieves improved results on a benchmark dataset with the addition of temporal reasoning showing significantly improved results compared to the state-of-the-art in saliency detection.
C1 [Gokstorp, Simon G. E.; Breckon, Toby P.] Univ Durham, Dept Comp Sci, Durham, England.
   [Breckon, Toby P.] Univ Durham, Dept Engn, Durham, England.
C3 Durham University; Durham University
RP Breckon, TP (corresponding author), Univ Durham, Dept Comp Sci, Durham, England.; Breckon, TP (corresponding author), Univ Durham, Dept Engn, Durham, England.
EM toby.breckon@durham.ac.uk
RI Breckon, Toby/ABD-1451-2020
OI Breckon, Toby/0000-0003-1666-7590; Gokstorp, Simon/0000-0001-5518-6613
CR Azaza Aymen, 2018, 2018 International Conference on Advanced Systems and Electric Technologies (IC_ASET), P355, DOI 10.1109/ASET.2018.8379878
   Bozic-Stulic D, 2019, INT J COMPUT VISION, V127, P1256, DOI 10.1007/s11263-019-01177-1
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Gotovac S, 2016, INT CONF SOFTW, P378
   Imamoglu N, 2013, IEEE T MULTIMEDIA, V15, P96, DOI 10.1109/TMM.2012.2225034
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   King DB, 2015, ACS SYM SER, V1214, P1
   Krassanakis V, 2018, DRONES-BASEL, V2, DOI 10.3390/drones2040036
   Liu N, 2018, IEEE T IMAGE PROCESS, V27, P3264, DOI 10.1109/TIP.2018.2817047
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Perrin AF, 2019, LECT NOTES COMPUT SC, V11678, P311, DOI 10.1007/978-3-030-29888-3_25
   Peters RJ, 2005, VISION RES, V45, P2397, DOI 10.1016/j.visres.2005.03.019
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shi XJ, 2015, ADV NEUR IN, V28
   Sokalski J., 2010, PROC 25 INT C UNMANN, P1
   Song HM, 2018, LECT NOTES COMPUT SC, V11215, P744, DOI 10.1007/978-3-030-01252-6_44
   Wang C., 2016, THESIS JILIN U, P1
   Wang L, 2011, IEEE I CONF COMP VIS, P105, DOI 10.1109/ICCV.2011.6126231
   Zhang YJ, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10040652
   Zhang YQ, 2013, PROC SPIE, V8918, DOI 10.1117/12.2032141
NR 21
TC 7
Z9 7
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2022
VL 38
IS 6
BP 2033
EP 2040
DI 10.1007/s00371-021-02264-6
EA SEP 2021
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1C6YN
UT WOS:000692956800002
OA Green Accepted, Green Published, hybrid
DA 2024-07-18
ER

PT J
AU Chen, SY
   Su, QT
   Wang, HY
   Wang, G
AF Chen, Siyu
   Su, Qingtang
   Wang, Huanying
   Wang, Gang
TI A high-efficiency blind watermarking algorithm for double color image
   using Walsh Hadamard transform
SO VISUAL COMPUTER
LA English
DT Article
DE Color image; Watermarking; Frequency domain; Walsh Hadamard transform
AB This paper presents an efficient blind watermarking algorithm for double color images using Walsh Hadamard transform (WHT). In this algorithm, the energy gathering function of WHT and the strong correlation between the matrix coefficients in the frequency domain after transformation is used for embedding the digital watermark. Firstly, the color host image is divided into R, G, and B channels, and each channel is partitioned into 4 x 4 blocks. By researching the frequency domain coefficients of the WHT blocks, a strong correlation can be found between the coefficients of the first row of the frequency domain matrix, especially the first and second elements and the third and fourth elements in the first row. Therefore, the digital watermark can be embedded into the above matrix block by fine-tuning the mentioned coefficients, and the watermark information can be extracted by the difference between the two coefficients. According to a huge number of simulation results, the proposed algorithm shows better performance not only in invisibility but also in robustness, watermark capacity, and running time.
C1 [Chen, Siyu; Su, Qingtang; Wang, Huanying; Wang, Gang] Ludong Univ, Sch Informat & Elect Engn, Yantai 264025, Peoples R China.
C3 Ludong University
RP Su, QT (corresponding author), Ludong Univ, Sch Informat & Elect Engn, Yantai 264025, Peoples R China.
EM sdytsqt@163.com
RI Chen, Siqi/IZE-8631-2023
FU National Natural Science Foundations of China [61771231, 61772253,
   61873117, 61872170, 61803253]; Key Project of Shandong Natural Science
   Foundation [ZR2020KF023]
FX The work was supported by the National Natural Science Foundations of
   China (No. 61771231, 61772253, 61873117, 61872170, and 61803253), and
   the Key Project of Shandong Natural Science Foundation (No.
   ZR2020KF023).
CR Cedillo-Hernandez A, 2018, J VIS COMMUN IMAGE R, V52, P106, DOI 10.1016/j.jvcir.2018.02.007
   Chang CS, 2017, IEEE T IMAGE PROCESS, V26, P3921, DOI 10.1109/TIP.2017.2706502
   Fares K, 2020, OPTIK, V208, DOI 10.1016/j.ijleo.2020.164562
   Hsu LY, 2017, J VIS COMMUN IMAGE R, V46, P33, DOI 10.1016/j.jvcir.2017.03.009
   Li DM, 2019, INFORM SCIENCES, V479, P432, DOI 10.1016/j.ins.2018.02.060
   Liu DC, 2021, EXPERT SYST APPL, V170, DOI 10.1016/j.eswa.2020.114540
   Liu DC, 2021, VISUAL COMPUT, V37, P2355, DOI 10.1007/s00371-020-01991-6
   Liu DC, 2020, MULTIMED TOOLS APPL, V79, P7491, DOI 10.1007/s11042-019-08423-1
   Meenakshi K, 2014, 2014 INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY (ICIT), P167, DOI 10.1109/ICIT.2014.53
   Moosazadeh M, 2019, J INF SECUR APPL, V47, P28, DOI 10.1016/j.jisa.2019.04.001
   Sam IS, 2020, J KING SAUD UNIV-COM, V10, P1319
   Sharma S, 2019, APPL SOFT COMPUT, V84, DOI 10.1016/j.asoc.2019.105696
   Su Q, 2016, Color Image Watermarking: Algorithms and Technologies, DOI [10.1515/9783110487732, DOI 10.1515/9783110487732]
   Su QT, 2020, MULTIMED TOOLS APPL, V79, P30023, DOI 10.1007/s11042-020-09436-x
   Su QT, 2019, IEEE ACCESS, V7, P30398, DOI 10.1109/ACCESS.2019.2895062
   Su QT, 2019, IEEE ACCESS, V7, P4358, DOI 10.1109/ACCESS.2018.2888857
   Su QT, 2018, MULTIDIM SYST SIGN P, V29, P1055, DOI 10.1007/s11045-017-0487-7
   Su QT, 2017, MULTIMED TOOLS APPL, V76, P707, DOI 10.1007/s11042-015-3071-x
   Su QT, 2015, SIGNAL IMAGE VIDEO P, V9, P991, DOI 10.1007/s11760-013-0534-2
   Su QT, 2014, MULTIMED TOOLS APPL, V72, P987, DOI 10.1007/s11042-013-1653-z
   Su QT, 2012, OPT COMMUN, V285, P1792, DOI 10.1016/j.optcom.2011.12.065
   Tang MW, 2015, OPTIK, V126, P4136, DOI 10.1016/j.ijleo.2015.07.200
   University of Granada Computer Vision Group, CVG UGR IM DAT
   University of Southern California Signal and Inage Processing Institute, USC SIPI IMAGE DATAB
   Yuan ZH, 2021, VISUAL COMPUT, V37, P1867, DOI 10.1007/s00371-020-01945-y
   Yuan ZH, 2020, OPTIK, V204, DOI 10.1016/j.ijleo.2019.164152
   Zermi N, 2021, FORENSIC SCI INT, V320, DOI 10.1016/j.forsciint.2021.110691
   Zhang XT, 2020, OPTIK, V219, DOI 10.1016/j.ijleo.2020.165272
NR 28
TC 16
Z9 16
U1 2
U2 31
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2022
VL 38
IS 6
BP 2189
EP 2205
DI 10.1007/s00371-021-02277-1
EA AUG 2021
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1C6YN
UT WOS:000687503100001
DA 2024-07-18
ER

PT J
AU Bulut, F
AF Bulut, Faruk
TI Low dynamic range histogram equalization (LDR-HE) via quantized Haar
   wavelet transform
SO VISUAL COMPUTER
LA English
DT Article
DE Image enhancement; Scalable contrast improvement; Quantized Discrete
   Haar Wavelet Transform; Quantized probability mass function
ID CONTRAST ENHANCEMENT; IMAGE-ENHANCEMENT; BRIGHTNESS PRESERVATION;
   NOISE-REDUCTION
AB Conventional contrast enhancement methods stretch histogram bins to provide a uniform distribution. However, they also stretch the existing natural noises which cause abnormal distributions and annoying artifacts. Histogram equalization should mostly be performed in low dynamic range (LDR) in which noises are generally distributed in high dynamic range (HDR). In this study, a novel image contrast enhancement method, called low dynamic range histogram equalization (LDR-HE), is proposed based on the Quantized Discrete Haar Wavelet Transform (HWT). In the frequency domain, LDR-HE performs a de-boosting operation on the high-pass channel by stretching the high frequencies of the probability mass function to the nearby zero. For this purpose, greater amplitudes than the absolute mean frequency in the high pass band are divided by a hyper alpha parameter. This damping parameter, which regulates the global contrast on the processed image, is the coefficient of variations of high frequencies, i.e., standard deviation divided by mean. This fundamental procedure of LDR-HE definitely provides a scalable and controlled dynamic range reduction in the histograms when the inverse operation is done in the reconstruction phase in order to regulate the excessive contrast enhancement rate. In the experimental studies, LDR HE is compared with the 14 most popular local, global, adaptive, and brightness preserving histogram equalization methods. Experimental studies qualitatively and quantitatively show promising and encouraging results in terms of different quality measurement metrics such as mean squared error (MSE), peak signal-to-noise ratio (PSNR), Contrast Improvement Index (CII), Universal Image Quality Index (UIQ), Quality-aware Relative Contrast Measure (QRCM), and Absolute Mean Brightness Error (AMBE). These results are not only assessed through qualitative visual observations but are also benchmarked with the state-of-the-art quantitative performance metrics.
C1 [Bulut, Faruk] Istanbul Arel Univ, Fac Engn & Architecture, Dept Comp Engn, Istanbul, Turkey.
C3 Istanbul Arel University
RP Bulut, F (corresponding author), Istanbul Arel Univ, Fac Engn & Architecture, Dept Comp Engn, Istanbul, Turkey.
EM farukbulut@arel.edu.tr
RI BULUT, Faruk/P-6693-2017
OI BULUT, Faruk/0000-0003-2960-8725
CR Abdullah-Al-Wadud M, 2007, IEEE T CONSUM ELECTR, V53, P593, DOI 10.1109/TCE.2007.381734
   Akyüz AO, 2007, J VIS COMMUN IMAGE R, V18, P366, DOI 10.1016/j.jvcir.2007.04.001
   Alibabaie N, 2021, J MATH IMAGING VIS, V63, P503, DOI 10.1007/s10851-020-01004-0
   Bordé P, 2007, CR PHYS, V8, P349, DOI 10.1016/j.crhy.2007.04.004
   Bushberg J. T., 2011, ESSENTIAL PHYS MED I
   Celik T, 2016, IEEE T IMAGE PROCESS, V25, P4719, DOI 10.1109/TIP.2016.2599103
   Chen SD, 2003, IEEE T CONSUM ELECTR, V49, P1301, DOI 10.1109/TCE.2003.1261233
   Chen SD, 2003, IEEE T CONSUM ELECTR, V49, P1310, DOI 10.1109/TCE.2003.1261234
   Cho DW, 2014, SIGNAL PROCESS, V98, P295, DOI 10.1016/j.sigpro.2013.11.007
   Coltuc D, 2006, IEEE T IMAGE PROCESS, V15, P1143, DOI 10.1109/TIP.2005.864170
   Dhal KG, 2021, ARCH COMPUT METHOD E, V28, P1471, DOI 10.1007/s11831-020-09425-1
   Ezhilarasi R, 2020, MULTIMED TOOLS APPL, V79, P8539, DOI 10.1007/s11042-018-5960-2
   Fu XY, 2020, SIGNAL PROCESS-IMAGE, V86, DOI 10.1016/j.image.2020.115892
   Gonzalez R., 2017, DIGITAL IMAGE PROCES
   Huang H, 2010, VISUAL COMPUT, V26, P731, DOI 10.1007/s00371-010-0504-4
   Huang LD, 2015, IET IMAGE PROCESS, V9, P908, DOI 10.1049/iet-ipr.2015.0150
   Iqbal MZ, 2014, SIGNAL PROCESS, V105, P430, DOI 10.1016/j.sigpro.2014.05.011
   Jang CY, 2016, DIGIT SIGNAL PROCESS, V58, P1, DOI 10.1016/j.dsp.2016.04.009
   Joshi P, 2020, VISUAL COMPUT, V36, P71, DOI 10.1007/s00371-018-1587-6
   Kaur A, 2017, APPL SOFT COMPUT, V51, P180, DOI 10.1016/j.asoc.2016.11.046
   Khan SA, 2020, J MED IMAG HEALTH IN, V10, P1795, DOI 10.1166/jmihi.2020.3196
   Kim M, 2008, IEEE T CONSUM ELECTR, V54, P1389, DOI 10.1109/TCE.2008.4637632
   Kim YT, 1997, IEEE T CONSUM ELECTR, V43, P1, DOI 10.1109/30.580378
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Mishro PK, 2021, BIOCYBERN BIOMED ENG, V41, P540, DOI 10.1016/j.bbe.2021.04.003
   Poddar S, 2013, IET IMAGE PROCESS, V7, P641, DOI 10.1049/iet-ipr.2012.0507
   Shanmugavadivu P, 2014, VISUAL COMPUT, V30, P387, DOI 10.1007/s00371-013-0863-8
   Sheet D, 2010, IEEE T CONSUM ELECTR, V56, P2475, DOI 10.1109/TCE.2010.5681130
   Sim KS, 2007, PATTERN RECOGN LETT, V28, P1209, DOI 10.1016/j.patrec.2007.02.003
   Singh K, 2014, OPTIK, V125, P4646, DOI 10.1016/j.ijleo.2014.04.093
   Singh K, 2014, PATTERN RECOGN LETT, V36, P10, DOI 10.1016/j.patrec.2013.08.024
   Tan SF, 2019, IEEE ACCESS, V7, P70842, DOI 10.1109/ACCESS.2019.2918557
   Tian CW, 2022, IEEE T SYST MAN CY-S, V52, P3718, DOI 10.1109/TSMC.2021.3069265
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Tian CW, 2020, NEURAL NETWORKS, V121, P461, DOI 10.1016/j.neunet.2019.08.022
   Wang Y, 1999, IEEE T CONSUM ELECTR, V45, P68, DOI 10.1109/30.754419
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Wong CY, 2016, J MOD OPTIC, V63, P1618, DOI 10.1080/09500340.2016.1163428
NR 38
TC 14
Z9 14
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2022
VL 38
IS 6
BP 2239
EP 2255
DI 10.1007/s00371-021-02281-5
EA AUG 2021
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1C6YN
UT WOS:000686468800001
DA 2024-07-18
ER

PT J
AU Zeghoud, S
   Ali, SG
   Ertugrul, E
   Kamel, A
   Sheng, B
   Li, P
   Chi, XY
   Kim, J
   Mao, LJ
AF Zeghoud, Sofiane
   Ali, Saba Ghazanfar
   Ertugrul, Egemen
   Kamel, Aouaidjia
   Sheng, Bin
   Li, Ping
   Chi, Xiaoyu
   Kim, Jinman
   Mao, Lijuan
TI Real-time spatial normalization for dynamic gesture classification
SO VISUAL COMPUTER
LA English
DT Article
DE Human-computer interaction; Sequence data processing; Gesture
   recognition; Real-time; Data normalization; Data generalization;
   Recurrent neural networks
ID HAND POSE
AB In this paper, we provide a new spatial data generalization method which we applied in hand gesture recognition tasks. Data gathering can be a tedious task when it comes to gesture recognition, especially dynamic gestures. Nowadays, the standard solutions when lacking data still consist of either the expensive gathering of new data or the impractical employment of hand-crafted data augmentation algorithms. While these solutions may show improvement, they come with disadvantages. We believe that a better extrapolation of the limited data's common pattern, through an improved generalization, should first be considered. We, therefore, propose a dynamic generalization method that allows to capture and normalize in real-time the spatial evolution of the input. The latter procedure can be fully converted into a neural network processing layer which we call Evolution Normalization Layer. Experimental results on the SHREC2017 dataset showed that the addition of the proposed layer improved the prediction accuracy of a standard sequence-processing model while requiring 6 times fewer weights on average for a similar score. Furthermore, when trained on only 10% of the original training data, the standard model was able to reach a maximum accuracy of only 36.5% alone and 56.8% when applying a state-of-the-art processing method to the data, whereas the addition of our layer alone permitted to achieve a prediction accuracy of 81.5%.
C1 [Zeghoud, Sofiane; Ali, Saba Ghazanfar; Ertugrul, Egemen; Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Kamel, Aouaidjia] Algerian Space Agcy, Space Tech Ctr, Arzew, Algeria.
   [Li, Ping] Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong, Peoples R China.
   [Chi, Xiaoyu] Beihang Univ, Qingdao Res Inst, Qingdao, Peoples R China.
   [Kim, Jinman] Univ Sydney, Sch Informat Technol, Sydney, NSW, Australia.
   [Mao, Lijuan] Shanghai Univ Sport, Sch Phys Educ & Training, Shanghai, Peoples R China.
C3 Shanghai Jiao Tong University; Algerian Space Agency (ASAL); Hong Kong
   Polytechnic University; Beihang University; University of Sydney;
   Shanghai University of Sport
RP Sheng, B (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.; Mao, LJ (corresponding author), Shanghai Univ Sport, Sch Phys Educ & Training, Shanghai, Peoples R China.
EM chianman@sjtu.edu.cn; Saba_Ali@sjtu.edu.cn; egertu@sjtu.edu.cn;
   Kameldz40@gmail.com; shengbin@sjtu.edu.cn; p.li@polyu.edu.hk;
   terry.chi@goertek.com; jinman.kim@sydney.edu.au; maolijuan@sus.edu.cn
RI Li, Ping/AAO-2019-2020; Kim, Jin/AAS-5810-2021; Kim, Jin
   Man/HJO-8987-2023
OI Li, Ping/0000-0002-1503-0240; Kim, Jin/0000-0002-7667-9588; KAMEL,
   AOUAIDJIA/0000-0001-6286-9527; Sheng, Bin/0000-0001-8678-2784
FU National Natural Science Foundation of China [62077037, 61872241];
   Shanghai Municipal Science and Technology Major Project
   [2021SHZDZX0102]; Science and Technology Commission of Shanghai
   Municipality [18410750700, 17411952600]; Shanghai Municipal Health
   Commission [2018ZHYL0230]; Hong Kong Polytechnic University [P0030419,
   P0030929, P0035358]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62077037 and 61872241, in part by
   Shanghai Municipal Science and Technology Major Project under Grant
   2021SHZDZX0102, in part by the Science and Technology Commission of
   Shanghai Municipality under Grants 18410750700 and 17411952600, and in
   part by Project of Shanghai Municipal Health Commission (2018ZHYL0230),
   and in part by The Hong Kong Polytechnic University under Grants
   P0030419, P0030929, and P0035358.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Ameur S, 2020, ENTERTAIN COMPUT, V35, DOI 10.1016/j.entcom.2020.100373
   [Anonymous], 2013, Consumer Depth Cameras for Computer Vision
   [Anonymous], 2016, ECCV
   [Anonymous], 2016, ECCV
   [Anonymous], 2015, GitHub repository
   Baek S, 2019, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2019.00116
   Chen XH, 2017, IEEE IMAGE PROC, P2881, DOI 10.1109/ICIP.2017.8296809
   Chen YJ, 2019, IEEE I CONF COMP VIS, P6960, DOI 10.1109/ICCV.2019.00706
   Chen Yuxiao, 2019, ARXIV191210730PHYSIC
   Chung Junyoung, 2014, ARXIV14123555
   De Smedt Q., 2016, P IEEE C COMP VIS PA, P1
   Devineau G, 2018, IEEE INT CONF AUTOMA, P106, DOI 10.1109/FG.2018.00025
   Gao YF, 2019, IEEE ROBOT AUTOM LET, V4, P4239, DOI 10.1109/LRA.2019.2930425
   Guerry J., 2017, 10 EUR WORKSH 3D OBJ, P1
   Hakim NL, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19245429
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang G, 2020, IEEE ACCESS, V8, P211869, DOI 10.1109/ACCESS.2020.3037238
   Intel, REALS SDK WIND
   Keskin C, 2012, LECT NOTES COMPUT SC, V7577, P852, DOI 10.1007/978-3-642-33783-3_61
   Krupka E., 2016, ARXIV160204489
   Krupka E, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1887, DOI 10.1145/3025453.3025508
   Krupka E, 2014, PROC CVPR IEEE, P3670, DOI 10.1109/CVPR.2014.469
   Kwon B, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10144849
   Li J, 2019, ENG LET, V27, P490
   Li Y, 2019, EURASIP J IMAGE VIDE, V2019, DOI 10.1186/s13640-019-0476-x
   Lugaresi C., 2019, ABS190608172 CORR
   Min YC, 2020, PROC CVPR IEEE, P5760, DOI 10.1109/CVPR42600.2020.00580
   Min Yuecong., 2019, 30 BRIT MACH VIS C 2, P105
   Mueller F, 2018, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2018.00013
   Nguyen X.S., 2019, 2019 14 IEEE INT C A, P1
   Núñez JC, 2018, PATTERN RECOGN, V76, P80, DOI 10.1016/j.patcog.2017.10.033
   Oberweger M., 2015, COMP VIS WINT WORKSH, P21
   Oberweger M, 2015, IEEE I CONF COMP VIS, P3316, DOI 10.1109/ICCV.2015.379
   Oikonomidis I, 2012, PROC CVPR IEEE, P1862, DOI 10.1109/CVPR.2012.6247885
   Rehg J. M., 1994, Proceedings of the 1994 IEEE Workshop on Motion of Non-Rigid and Articulated Objects (Cat. No.94TH0671-8), P16, DOI 10.1109/MNRAO.1994.346260
   Salami D., 2020, 2020 IEEE 30 INT WOR, P1
   Sharp T, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3633, DOI 10.1145/2702123.2702179
   Simon T, 2017, PROC CVPR IEEE, P4645, DOI 10.1109/CVPR.2017.494
   Song Jie., 2014, UIST
   Sridhar S., 2014, P INT C 3D VIS 3DV
   Sridhar S, 2015, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2015.7298941
   Sridhar S, 2013, IEEE I CONF COMP VIS, P2456, DOI 10.1109/ICCV.2013.305
   Sun X, 2015, PROC CVPR IEEE, P824, DOI 10.1109/CVPR.2015.7298683
   Tang DH, 2014, PROC CVPR IEEE, P3786, DOI 10.1109/CVPR.2014.490
   Tompson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629500
   Tzionas D, 2016, INT J COMPUT VISION, V118, P172, DOI 10.1007/s11263-016-0895-4
   Wang RY, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531369
   Wu YH, 2018, CHIN AUTOM CONGR, P2446, DOI 10.1109/CAC.2018.8623035
   Xu C, 2013, IEEE I CONF COMP VIS, P3456, DOI 10.1109/ICCV.2013.429
   Zimmermann C, 2019, IEEE I CONF COMP VIS, P813, DOI 10.1109/ICCV.2019.00090
NR 51
TC 8
Z9 8
U1 1
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2022
VL 38
IS 4
SI SI
BP 1345
EP 1357
DI 10.1007/s00371-021-02229-9
EA JUL 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0D9DX
UT WOS:000673688400001
DA 2024-07-18
ER

PT J
AU Bukenberger, DR
   Lensch, HPA
AF Bukenberger, Dennis R.
   Lensch, Hendrik P. A.
TI Tetrahedra of varying density and their applications
SO VISUAL COMPUTER
LA English
DT Article
DE Volumetric models; Shape analysis; 3D printing
ID ALGORITHM; INTEGRATION
AB We propose concepts to utilize basic mathematical principles for computing the exact mass properties of objects with varying densities. For objects given as 3D triangle meshes, the method is analytically accurate and at the same time faster than any established approximation method. Our concept is based on tetrahedra as underlying primitives, which allows for the object's actual mesh surface to be incorporated in the computation. The density within a tetrahedron is allowed to vary linearly, i.e., arbitrary density fields can be approximated by specifying the density at all vertices of a tetrahedral mesh. Involved integrals are formulated in closed form and can be evaluated by simple, easily parallelized, vector-matrix multiplications. The ability to compute exact masses and centroids for objects of varying density enables novel or more exact solutions to several interesting problems: besides the accurate analysis of objects under given density fields, this includes the synthesis of parameterized density functions for the make-it-stand challenge or manufacturing of objects with controlled rotational inertia. In addition, based on the tetrahedralization of Voronoi cells we introduce a precise method to solve L-2 vertical bar infinity Lloyd relaxations by exact integration of the Chebyshev norm. In the context of additive manufacturing research, objects of varying density are a prominent topic. However, current state-of-the-art algorithms are still based on voxelizations, which produce rather crude approximations of masses and mass centers of 3D objects. Many existing frameworks will benefit by replacing approximations with fast and exact calculations.
C1 [Bukenberger, Dennis R.; Lensch, Hendrik P. A.] Univ Tubingen, Tubingen, Germany.
C3 Eberhard Karls University of Tubingen
RP Bukenberger, DR (corresponding author), Univ Tubingen, Tubingen, Germany.
EM dennis.bukenberger@uni-tuebingen.de
OI Bukenberger, Dennis/0000-0002-0659-7181
FU Projekt DEAL
FX Open Access funding enabled and organized by Projekt DEAL.
CR [Anonymous], 2020, ONLINE SOURCES RECEN
   Bächer M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601157
   Basselin J, 2021, COMPUT GRAPH FORUM, V40, P1, DOI 10.1111/cgf.142610
   BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069
   D'Urso MG, 2014, CELEST MECH DYN ASTR, V120, P349, DOI 10.1007/s10569-014-9578-z
   Du Q, 2006, SIAM J NUMER ANAL, V44, P102, DOI 10.1137/040617364
   Eberly D., 2002, Polyhedral mass properties (Revisited)-geometric tools
   Hansen RO, 1999, GEOPHYSICS, V64, P75, DOI 10.1190/1.1444532
   Hornus S, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392448
   Hu YX, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201353
   Kuipers T, 2019, COMPUT AIDED DESIGN, V114, P37, DOI 10.1016/j.cad.2019.05.003
   Leal R, 2017, INT J ADV MANUF TECH, V92, P1671, DOI 10.1007/s00170-017-0239-8
   Lévy B, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778856
   Li B, 2020, VIS COMPUT IND BIOME, V3, DOI 10.1186/s42492-020-0041-6
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   MEAGHER D, 1982, COMPUT VISION GRAPH, V19, P129, DOI 10.1016/0146-664X(82)90104-6
   Mirtich B., 1996, Journal of Graphics Tools, V1, P31, DOI DOI 10.1080/10867651.1996.10487458
   Musialski P, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766955
   POWELL MJD, 1964, COMPUT J, V7, P155, DOI 10.1093/comjnl/7.2.155
   Préost R, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461957
   Rathod HT, 2007, APPL MATH COMPUT, V191, P397, DOI 10.1016/j.amc.2007.02.104
   Rathod HT, 1996, COMPUT STRUCT, V59, P55, DOI 10.1016/0045-7949(95)00243-X
   Ray N, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275092
   Si H, 2015, ACM T MATH SOFTWARE, V41, DOI 10.1145/2629697
   Sokolov D, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2930662
   Sorkine-Hornung, 2016, VMV
   Tonon F., 2004, J MATH STAT, V1, P8, DOI DOI 10.3844/JMSSP.2005.8.11
   Wu J, 2018, IEEE T VIS COMPUT GR, V24, P1127, DOI 10.1109/TVCG.2017.2655523
   Yan DM, 2013, COMPUT AIDED DESIGN, V45, P843, DOI 10.1016/j.cad.2011.09.004
   Yan Q, 2018, ENGINEERING-PRC, V4, P729, DOI 10.1016/j.eng.2018.07.021
   Zhu CY, 1997, ACM T MATH SOFTWARE, V23, P550, DOI 10.1145/279232.279236
NR 32
TC 0
Z9 0
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2447
EP 2460
DI 10.1007/s00371-021-02189-0
EA JUL 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000669781600001
OA hybrid
DA 2024-07-18
ER

PT J
AU Nozawa, N
   Shum, HPH
   Feng, Q
   Ho, ESL
   Morishima, S
AF Nozawa, Naoki
   Shum, Hubert P. H.
   Feng, Qi
   Ho, Edmond S. L.
   Morishima, Shigeo
TI 3D car shape reconstruction from a contour sketch using GAN and lazy
   learning
SO VISUAL COMPUTER
LA English
DT Article
DE Generative adversarial network; Lazy learning; 3D reconstruction;
   Sketch-based interface; Car; Contour sketch
AB 3D car models are heavily used in computer games, visual effects, and even automotive designs. As a result, producing such models with minimal labour costs is increasingly more important. To tackle the challenge, we propose a novel system to reconstruct a 3D car using a single sketch image. The system learns from a synthetic database of 3D car models and their corresponding 2D contour sketches and segmentation masks, allowing effective training with minimal data collection cost. The core of the system is a machine learning pipeline that combines the use of a generative adversarial network (GAN) and lazy learning. GAN, being a deep learning method, is capable of modelling complicated data distributions, enabling the effective modelling of a large variety of cars. Its major weakness is that as a global method, modelling the fine details in the local region is challenging. Lazy learning works well to preserve local features by generating a local subspace with relevant data samples. We demonstrate that the combined use of GAN and lazy learning produces is able to produce high-quality results, in which different types of cars with complicated local features can be generated effectively with a single sketch. Our method outperforms existing ones using other machine learning structures such as the variational autoencoder.
C1 [Nozawa, Naoki; Feng, Qi] Waseda Univ, Dept Pure & Appl Phys, Tokyo, Japan.
   [Shum, Hubert P. H.; Ho, Edmond S. L.] Northumbria Univ, Dept Comp & Informat Sci, Newcastle Upon Tyne, Tyne & Wear, England.
   [Shum, Hubert P. H.] Univ Durham, Dept Comp Sci, Durham, England.
   [Morishima, Shigeo] Waseda Univ, Waseda Res Inst Sci & Engn, Tokyo, Japan.
C3 Waseda University; Northumbria University; Durham University; Waseda
   University
RP Shum, HPH (corresponding author), Northumbria Univ, Dept Comp & Informat Sci, Newcastle Upon Tyne, Tyne & Wear, England.; Shum, HPH (corresponding author), Univ Durham, Dept Comp Sci, Durham, England.
EM s112800563@akane.waseda.jp; hubert.shum@durham.ac.uk;
   fengqi@ruri.waseda.jp; e.ho@northumbria.ac.uk; shigeo@waseda.jp
RI Shum, Hubert P. H./E-8060-2015; Ho, Edmond S. L./JDW-1835-2023
OI Shum, Hubert P. H./0000-0001-5651-6039; Ho, Edmond S.
   L./0000-0001-5862-106X; Morishima, Shigeo/0000-0001-8859-6539
FU Royal Society [IES\R2\181024, IES\R1\191147]; JST ACCEL [JPMJAC1602];
   JSPS KAKENHI [JP19H01129]; JST-Mirai Program [JPMJMI19B2]
FX This project was supported in part by the Royal Society (Ref:
   IES\R2\181024 and IES\R1\191147), JST ACCEL (JPMJAC1602), JST-Mirai
   Program (JPMJMI19B2) and JSPS KAKENHI (JP19H01129).Y
CR Blanz Volker., 1999, P 26 ANN C COMPUTER, P187, DOI DOI 10.1145/311535.311556
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chai JX, 2005, ACM T GRAPHIC, V24, P686, DOI 10.1145/1073204.1073248
   Chang A. X., 2015, ARXIV
   Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   Corsini M, 2012, IEEE T VIS COMPUT GR, V18, P914, DOI 10.1109/TVCG.2012.34
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Delanoy J, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3203197
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Gauthier M., 2009, GRAPHICS INTERFACE, P1, DOI DOI 10.5555/1555880.1555890
   Gingold Y, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1618452.1616494, 10.1145/1618452.1618494]
   Goedicke D, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173739
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Han XG, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073629
   Hanocka, 2020, ARXIV200511084
   Henry J, 2014, IEEE T VIS COMPUT GR, V20, P211, DOI 10.1109/TVCG.2013.116
   Ho ESL, 2013, COMPUT GRAPH FORUM, V32, P61, DOI 10.1111/cgf.12212
   Huang H., 2013, ACM Trans. Graph., V32, P1
   Igarashi T., 2007, COMP GRAPH, P2007
   Igarashi T., 2006, ACM SIGGRAPH COURSES, P2006
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Jiang L, 2018, LECT NOTES COMPUT SC, V11212, P820, DOI 10.1007/978-3-030-01237-3_49
   Joshi P., 2008, P EUR WORKSH SKETCH, P49
   Kingma DP, 2014, ADV NEUR IN, V27
   Kraevoy V, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409064
   Li CJ, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275051
   Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43
   Li MT, 2019, IEEE WINT CONF APPL, P1403, DOI 10.1109/WACV.2019.00154
   Lim JJ, 2013, IEEE I CONF COMP VIS, P2992, DOI 10.1109/ICCV.2013.372
   Lun ZL, 2017, INT CONF 3D VISION, P67, DOI 10.1109/3DV.2017.00018
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Nishida G, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925951
   Nozawa N, 2020, PROCEEDINGS OF THE 15TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 1: GRAPP, P179, DOI 10.5220/0009157001790190
   Olsen L, 2009, COMPUT GRAPH-UK, V33, P85, DOI 10.1016/j.cag.2008.09.013
   Owada S., 2006, ACM SIGGRAPH CORUSES, P2006
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Qi CR, 2017, ADV NEUR IN, V30
   Rameau F, 2016, IEEE T VIS COMPUT GR, V22, P2395, DOI 10.1109/TVCG.2016.2593768
   Reddy ND, 2018, PROC CVPR IEEE, P1906, DOI 10.1109/CVPR.2018.00204
   Sela M, 2017, IEEE I CONF COMP VIS, P1585, DOI 10.1109/ICCV.2017.175
   Shao C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185541
   Shen YJ, 2018, COMPUT GRAPH FORUM, V37, P382, DOI 10.1111/cgf.13333
   Shen Yijun., 2019, IEEE Transactions on Visualization and Computer Graphics
   Shtof A, 2013, COMPUT GRAPH FORUM, V32, P245, DOI 10.1111/cgf.12044
   Shum HPH, 2013, IEEE T CYBERNETICS, V43, P1357, DOI 10.1109/TCYB.2013.2275945
   Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230
   Umetani N., 2017, ser. SA '17, P1, DOI [DOI 10.1145/3145749.3145758, 10.1145/3145749.3145758]
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
NR 51
TC 17
Z9 17
U1 2
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2022
VL 38
IS 4
SI SI
BP 1317
EP 1330
DI 10.1007/s00371-020-02024-y
EA APR 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0D9DX
UT WOS:000640468900002
OA Green Accepted, hybrid
DA 2024-07-18
ER

PT J
AU Li, ZX
   Zuo, WM
   Wang, ZQ
   Zhang, L
AF Li, Zhaoxin
   Zuo, Wangmeng
   Wang, Zhaoqi
   Zhang, Lei
TI Robust 3D reconstruction from uncalibrated small motion clips
SO VISUAL COMPUTER
LA English
DT Article
DE 3D reconstruction; Small motion; Multi-view stereo; Line segment;
   PatchMatch stereo
ID MULTIVIEW STEREO
AB Small motion can be induced from burst video clips captured by a handheld camera when the shutter button is pressed. Although uncalibrated burst video clip conveys valuable parallax information, it generally has small baseline between frames, making it difficult to reconstruct 3D scenes. Existing methods usually employ a simplified camera parameterization process with keypoint-based structure from small motion (SFSM), followed by a tailored dense reconstruction. However, such SFSM methods are sensitive to insufficient or unreliable keypoint features, and the subsequent dense reconstruction may fail to recover the detailed surface. In this paper, we propose a robust 3D reconstruction pipeline by leveraging both keypoint and line segment features from video clips to alleviate the uncertainty induced by small baseline. A joint feature-based structure from small motion method is first presented to improve the robustness of the self-calibration with line segment constraints, and then, a noise-aware PatchMatch stereo module is proposed to improve the accuracy of the dense reconstruction. Finally, a confidence weighted fusion process is utilized to further suppress depth noise and mitigate erroneous depth. The proposed method can reduce the failure cases of self-calibration when the keypoints are insufficient, while recovering the detailed 3D surfaces. In comparison with state of the arts, our method achieves more robust and accurate 3D reconstruction results for a variety of challenging scenes.
C1 [Li, Zhaoxin; Wang, Zhaoqi] Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China.
   [Zuo, Wangmeng] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin, Peoples R China.
   [Zhang, Lei] Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Harbin Institute of Technology; Hong Kong Polytechnic University
RP Li, ZX (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China.
EM cszli@hotmail.com; cswmzuo@gmail.com; zqwang@ict.ac.cn;
   cslzhang@comp.polyu.edu.hk
RI Zhang, Lei/P-8881-2014; Zuo, Wangmeng/B-3701-2008
OI Li, Zhaoxin/0000-0001-5450-8131
FU National Key Research and Development Program of China [2018AAA0103002];
   National Natural Science Foundation of China [61702482]
FX This work was supported by National Key Research and Development Program
   of China (No. 2018AAA0103002) and National Natural Science Foundation of
   China (No. 61702482).
CR Agrawal S, 2020, IEEE WINT CONF APPL, P81, DOI 10.1109/WACV45572.2020.9093455
   Bleyer M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.14
   El Hazzat S, 2018, VISUAL COMPUT, V34, P1443, DOI 10.1007/s00371-017-1451-0
   Frahm J.M, 2016, European Conference on Computer Vision (ECCV), P1
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Galliani S, 2015, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2015.106
   Gallup D, 2008, PROC CVPR IEEE, P2562
   Geiger A, 2017, P INT LEGAL INFORMAT, P1
   Goesele M, 2014, 2014 EUROGRAPHICS WO, P1
   Ha H, 2016, PROC CVPR IEEE, P5413, DOI 10.1109/CVPR.2016.584
   Ham C, 2017, INT CONF 3D VISION, P575, DOI 10.1109/3DV.2017.00071
   Häne C, 2017, IEEE T PATTERN ANAL, V39, P1730, DOI 10.1109/TPAMI.2016.2613051
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   Hedman P, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201384
   Heise P, 2013, IEEE I CONF COMP VIS, P2360, DOI 10.1109/ICCV.2013.293
   Heo YS, 2011, IEEE T PATTERN ANAL, V33, P807, DOI 10.1109/TPAMI.2010.136
   Hofer M, 2017, COMPUT VIS IMAGE UND, V157, P167, DOI 10.1016/j.cviu.2016.03.017
   Honauer K, 2017, LECT NOTES COMPUT SC, V10113, P19, DOI 10.1007/978-3-319-54187-7_2
   Huang JW, 2017, P IEEE VIRT REAL ANN, P37, DOI 10.1109/VR.2017.7892229
   Im S., 2019, IEEE T PATTERN ANAL, V1, P1
   Im S, 2019, IEEE T PATTERN ANAL, V41, P775, DOI 10.1109/TPAMI.2018.2819679
   Im S, 2016, LECT NOTES COMPUT SC, V9907, P156, DOI 10.1007/978-3-319-46487-9_10
   Im S, 2015, IEEE I CONF COMP VIS, P837, DOI 10.1109/ICCV.2015.102
   Jancosek M., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3121, DOI 10.1109/CVPR.2011.5995693
   Jenseny R., 2014, 2014 IEEE C COMP VIS, P1
   Kanade T, CMUCS91132
   Kautz J, 2018, 2018 IEEE C COMP VIS, P1558
   Khan Mansoor, 2020, 2020 IEEE 8th International Conference on Photonics (ICP), P1, DOI 10.1109/ICP46580.2020.9206421
   Khan MJ, 2018, IEEE ACCESS, V6, P14118, DOI 10.1109/ACCESS.2018.2812999
   Krishnamurthy P., 2006, 3 INT S 3D DATA PROC, P1
   Kuhn A, 2017, INT J COMPUT VISION, V124, P2, DOI 10.1007/s11263-016-0946-x
   Li SW, 2015, IEEE I CONF COMP VIS, P4283, DOI 10.1109/ICCV.2015.487
   Lin S., 2020, COMPUT VIS IMAGE UND, V200, P1
   Liu SH, 2020, IEEE ACCESS, V8, P117551, DOI 10.1109/ACCESS.2020.3004431
   Micusik Branislav, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P13, DOI 10.1109/3DV.2014.17
   Nurutdinova I, 2015, IEEE I CONF COMP VIS, P2363, DOI 10.1109/ICCV.2015.272
   Ramalingam S, 2015, PROC CVPR IEEE, P1238, DOI 10.1109/CVPR.2015.7298728
   Santana-Cedrés D, 2015, SIAM J IMAGING SCI, V8, P1574, DOI 10.1137/151006044
   Schilling H, 2018, PROC CVPR IEEE, P4530, DOI 10.1109/CVPR.2018.00476
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Snavely N, 2008, INT J COMPUT VISION, V80, P189, DOI 10.1007/s11263-007-0107-3
   Song P, 2010, VISUAL COMPUT, V26, P1435, DOI 10.1007/s00371-010-0429-y
   Strecha C., 2008, 2008 IEEE Conference on Computer Vision and Pattern Recognition, P1, DOI DOI 10.1109/CVPR.2008.4587706
   Sugiura T, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P264, DOI 10.1109/3DV.2015.37
   Suwajanakorn S, 2015, PROC CVPR IEEE, P3497, DOI 10.1109/CVPR.2015.7298972
   Tola E, 2012, MACH VISION APPL, V23, P903, DOI 10.1007/s00138-011-0346-8
   Vo M., 2016, IEEE T PATTERN ANAL, V1, P1710
   von Gioi RG, 2010, IEEE T PATTERN ANAL, V32, P722, DOI 10.1109/TPAMI.2008.300
   Wang TC, 2015, IEEE I CONF COMP VIS, P3487, DOI 10.1109/ICCV.2015.398
   Wei MQ, 2019, VISUAL COMPUT, V35, P797, DOI 10.1007/s00371-019-01688-5
   Xu Q., 2018, ARXIV180507920V1
   Xu QS, 2019, PROC CVPR IEEE, P5478, DOI 10.1109/CVPR.2019.00563
   Yang QX, 2012, PROC CVPR IEEE, P1402, DOI 10.1109/CVPR.2012.6247827
   Yao Y, 2019, PROC CVPR IEEE, P5520, DOI 10.1109/CVPR.2019.00567
   Yu F, 2014, PROC CVPR IEEE, P3986, DOI 10.1109/CVPR.2014.509
   Zhang S, 2016, COMPUT VIS IMAGE UND, V145, P148, DOI 10.1016/j.cviu.2015.12.007
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou H, 2019, IEEE ACCESS, V7, P73120, DOI 10.1109/ACCESS.2019.2920453
   Zhu H, 2017, IEEE J-STSP, V11, P965, DOI 10.1109/JSTSP.2017.2730818
NR 60
TC 4
Z9 4
U1 3
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2022
VL 38
IS 5
BP 1589
EP 1605
DI 10.1007/s00371-021-02090-w
EA MAR 2021
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0M9JP
UT WOS:000628071600001
DA 2024-07-18
ER

PT J
AU Szirmay-Kalos, L
   Kacsó, A
   Magdics, M
   Tóth, B
AF Szirmay-Kalos, Laszlo
   Kacso, Agota
   Magdics, Milan
   Toth, Balazs
TI Robust compartmental model fitting in direct emission tomography
   reconstruction
SO VISUAL COMPUTER
LA English
DT Article
DE Position emission tomography; ML&#8211; EM methods; GPU
ID 4D IMAGE-RECONSTRUCTION; DYNAMIC PET; PARAMETRIC IMAGES; SIMULATION
AB Dynamic tomography reconstructs a time activity curve (TAC) for every voxel assuming that the algebraic form of the function is known a priori. The algebraic form derived from the analysis of compartmental models depends nonlinearly on the nonnegative parameters to be determined. Direct methods apply fitting in every iteration step. Because of the iterative nature of the maximum likelihood-expectation maximization (ML-EM) reconstruction, the fitting result of the previous step can serve as a good starting point in the current step; thus, after the first iteration we have a guess that is not far from the solution, which allows the use of gradient-based local optimization methods. However, finding good initial guesses for the first ML-EM iteration is a critical problem since gradient-based local optimization algorithms do not guarantee convergence to the global optimum if they are started at an inappropriate location. This paper examines the robust solution of the fitting problem both in the initial phase and during the ML-EM iteration. This solution is implemented on GPUs and is built into the 4D reconstruction module of the TeraTomo software.
C1 [Szirmay-Kalos, Laszlo; Kacso, Agota; Magdics, Milan; Toth, Balazs] Budapest Univ Technol & Econ, Dept Control Engn & Informat Technol, Budapest, Hungary.
C3 Budapest University of Technology & Economics
RP Szirmay-Kalos, L (corresponding author), Budapest Univ Technol & Econ, Dept Control Engn & Informat Technol, Budapest, Hungary.
EM szirmay@iit.bme.hu
RI Szirmay-Kalos, Laszlo/H-3853-2012
OI Szirmay-Kalos, Laszlo/0000-0002-8523-2315
FU Budapest University of Technology and Economics
FX Open Access funding provided by Budapest University of Technology and
   Economics
CR Baydin AG, 2018, J MACH LEARN RES, V18
   Cheng XY, 2015, IEEE T MED IMAGING, V34, P1498, DOI 10.1109/TMI.2015.2403300
   FENG D, 1993, INT J BIOMED COMPUT, V32, P95, DOI 10.1016/0020-7101(93)90049-C
   Gallezot JD, 2020, IEEE T RADIAT PLASMA, V4, P1, DOI 10.1109/TRPMS.2019.2908633
   Gunn RN, 2001, J CEREBR BLOOD F MET, V21, P635, DOI 10.1097/00004647-200106000-00002
   Jan S, 2004, PHYS MED BIOL, V49, P4543, DOI 10.1088/0031-9155/49/19/007
   Kadrmas DJ, 2013, MED PHYS, V40, DOI 10.1118/1.4810937
   Kamasak ME, 2005, IEEE T MED IMAGING, V24, P636, DOI 10.1109/TMI.2005.845317
   Li XH, 2014, IEEE T GEOSCI REMOTE, V52, P7086, DOI 10.1109/TGRS.2014.2307354
   Liao HY, 2008, I S BIOMED IMAGING, P1375, DOI 10.1109/ISBI.2008.4541261
   Liao WH, 2002, IEEE T NUCL SCI, V49, P2291, DOI 10.1109/TNS.2002.803813
   Liu XL, 2015, VISUAL COMPUT, V31, P1431, DOI 10.1007/s00371-014-1024-4
   Magdics M., 2010, WORLD MOL IM C
   Matthews JC, 2010, IEEE NUCL SCI CONF R, P2435, DOI 10.1109/NSSMIC.2010.5874225
   More J. J., 1978, Proceedings of the Biennial Conference on numerical analysis, P105
   Rahmim A., 2014, FRONTIERS BIOMEDICAL, V1, P4
   Rahmim A, 2009, MED PHYS, V36, P3654, DOI 10.1118/1.3160108
   Reader AJ, 2014, PHYS MED BIOL, V59, pR371, DOI 10.1088/0031-9155/59/22/R371
   Reader AJ, 2006, IEEE NUCL SCI CONF R, P1752, DOI 10.1109/nssmic.2006.354235
   Reutter BW, 2005, IEEE NUCL SCI CONF R, P2337
   Szirmay-Kalos L., 2018, PERIOD POLYTECH, V62, P134, DOI DOI 10.3311/PPee.11739
   Szirmay-Kalos L, 2017, VISUAL COMPUT, V33, P331, DOI 10.1007/s00371-015-1203-y
   Szirmay-Kalos L, 2014, 2014 IEEE NUCLEAR SCIENCE SYMPOSIUM AND MEDICAL IMAGING CONFERENCE (NSS/MIC)
   Wang C, 2018, VISUAL COMPUT, V34, P1357, DOI 10.1007/s00371-017-1418-1
   Wang GB, 2020, IEEE T RADIAT PLASMA, V4, P663, DOI [10.1109/trpms.2020.3025086, 10.1109/TRPMS.2020.3025086]
   Wang GB, 2013, THERANOSTICS, V3, P802, DOI 10.7150/thno.5130
   Wang GB, 2012, IEEE T MED IMAGING, V31, P1977, DOI 10.1109/TMI.2012.2212203
   Wang GB, 2010, PHYS MED BIOL, V55, P1505, DOI 10.1088/0031-9155/55/5/016
   Watabe H, 2006, ANN NUCL MED, V20, P583, DOI 10.1007/BF02984655
   Yan JH, 2012, IEEE T MED IMAGING, V31, P2213, DOI 10.1109/TMI.2012.2212451
   Yan Jianhua, 2008, IEEE Nucl Sci Symp Conf Rec (1997), V4774103, P3625
   Ye JM, 2015, IEEE T INSTRUM MEAS, V64, P89, DOI 10.1109/TIM.2014.2329738
   ZUBAL IG, 1994, MED PHYS, V21, P299, DOI 10.1118/1.597290
NR 33
TC 2
Z9 2
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2022
VL 38
IS 2
BP 655
EP 668
DI 10.1007/s00371-020-02041-x
EA FEB 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZE7IA
UT WOS:000615564200001
OA hybrid
DA 2024-07-18
ER

PT J
AU Luo, Q
   Liu, BC
   Zhang, Y
   Han, Z
   Tang, YD
AF Luo, Qiong
   Liu, Baichen
   Zhang, Yang
   Han, Zhi
   Tang, Yandong
TI Low-rank decomposition on transformed feature maps domain for image
   denoising
SO VISUAL COMPUTER
LA English
DT Article
DE Low-rank; Domain transformation; Autoencoder; Denoising
ID RECOVERY; SPARSE
AB Low-rank based models are proved outstanding for denoising on the data with strong repetitive or redundant property. However, for natural images with complex structures or rich details, the performance drops down because of the weak low-rankness of the data. A feasible solution is to transform the data into a suitable domain to further explore the underlying low-rank information. In this paper, we present a novel approach to create such a domain via a fully replicated linear autoencoder network. By applying various low-rank models to the feature maps generated by the encoder rather than the original data, and then performing inverse transformation by the decoder, their denoising performances all get enhanced. In addition, feature maps also show good sparsity, hence we introduce a new measure combining sparse and low-rank regularity, and further propose corresponding single image denoising model. Extensive experiments show the superiority of our work.
C1 [Luo, Qiong; Liu, Baichen; Han, Zhi; Tang, Yandong] Chinese Acad Sci, Shenyang Inst Automat, State Key Lab Robot, Shenyang, Peoples R China.
   [Luo, Qiong; Liu, Baichen; Han, Zhi; Tang, Yandong] Chinese Acad Sci, Inst Robot & Intelligent Mfg, Shenyang, Peoples R China.
   [Luo, Qiong; Liu, Baichen] Univ Chinese Acad Sci, Beijing, Peoples R China.
   [Zhang, Yang] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
C3 Chinese Academy of Sciences; Shenyang Institute of Automation, CAS;
   Chinese Academy of Sciences; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; City University of Hong Kong
RP Han, Z (corresponding author), Chinese Acad Sci, Shenyang Inst Automat, State Key Lab Robot, Shenyang, Peoples R China.; Han, Z (corresponding author), Chinese Acad Sci, Inst Robot & Intelligent Mfg, Shenyang, Peoples R China.
EM hanzhi@sia.cn
RI Sun, Yuchen/JZD-1692-2024
OI Zhang, Yang/0000-0002-3812-7299
FU National Natural Science Foundation of China [61773367, 61303168,
   61821005]; Youth Innovation Promotion Association of the Chinese Academy
   of Sciences [2016183]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61773367, Grant 61303168, and Grant
   61821005, in part by the Youth Innovation Promotion Association of the
   Chinese Academy of Sciences under Grant 2016183.
CR [Anonymous], 2013, P AAAI C ART INT
   Ballester-Ripoll R, 2016, VISUAL COMPUT, V32, P1433, DOI 10.1007/s00371-015-1130-y
   Burger H., 2012, CVPR
   Burger HC, 2013, LECT NOTES COMPUT SC, V8142, P121, DOI 10.1007/978-3-642-40602-7_13
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Candès EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Chang Y, 2017, IEEE I CONF COMP VIS, P1735, DOI 10.1109/ICCV.2017.191
   Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P700, DOI 10.1109/TIP.2012.2221729
   DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009
   Giraldo-Zuluaga JH, 2019, VISUAL COMPUT, V35, P335, DOI 10.1007/s00371-017-1463-9
   Goldfarb D, 2014, SIAM J MATRIX ANAL A, V35, P225, DOI 10.1137/130905010
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   GRASEDYCK L., 2013, GAMM-Mitteilungen36, V36, P53, DOI 10.1002/gamm.201310004
   He W, 2016, IEEE T GEOSCI REMOTE, V54, P176, DOI 10.1109/TGRS.2015.2452812
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Huang T, 2017, IEEE T IMAGE PROCESS, V26, P3171, DOI 10.1109/TIP.2017.2676466
   Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665
   Jain V., 2009, ADV NEURAL INFORM PR, P769, DOI DOI 10.5555/2981780.2981876
   Lebrun M, 2015, IMAGE PROCESS ON LIN, V5, P1, DOI 10.5201/ipol.2015.125
   Li DP, 2015, PROC CVPR IEEE, P213, DOI 10.1109/CVPR.2015.7298617
   Li XZ, 2018, COMPUT GRAPH FORUM, V37, P155, DOI 10.1111/cgf.13556
   Lin Z., 2011, ADV NEURAL INFORM PR, V24, P612, DOI DOI 10.1007/S11263-013-0611-6
   Liu G., 2010, P INT C MACH LEARN, P663
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   LORENZ EN, 1963, J ATMOS SCI, V20, P130, DOI 10.1175/1520-0469(1963)020<0130:DNF>2.0.CO;2
   Luo Q, 2017, IEEE I CONF COMP VIS, P5029, DOI 10.1109/ICCV.2017.537
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   McGraw T, 2015, VISUAL COMPUT, V31, P601, DOI 10.1007/s00371-014-0986-6
   Meng DeYu, 2015, Science China Information Sciences, V58, DOI 10.1007/s11432-014-5223-4
   Peng J., 2018, ARXIV180906591
   Shi F, 2015, IEEE T MED IMAGING, V34, P2459, DOI 10.1109/TMI.2015.2437894
   Sutskever I., 2013, INT C MACHINE LEARNI, P1139
   Vincent Pascal, 2008, P 25 INT C MACHINE L, DOI DOI 10.1145/1390156.1390294
   Wei MQ, 2019, IEEE T VIS COMPUT GR, V25, P2910, DOI 10.1109/TVCG.2018.2865363
   Wright J, 2009, ADV NEURAL INFORM PR, P2080, DOI DOI 10.1109/NNSP.2000.889420
   Xie Junyuan, 2012, ADV NEURAL INFORM PR, P341, DOI [DOI 10.5555/2999134.2999173, DOI 10.1109/AGRO-GEOINFORMATICS.2012.6311605]
   XIE Q, 2016, PROC CVPR IEEE, P1692, DOI DOI 10.1109/CVPR.2016.187
   Xie Y, 2016, IEEE T GEOSCI REMOTE, V54, P4642, DOI 10.1109/TGRS.2016.2547879
   Xu J, 2015, IEEE I CONF COMP VIS, P244, DOI 10.1109/ICCV.2015.36
   Xue ZC, 2019, VISUAL COMPUT, V35, P1549, DOI 10.1007/s00371-018-1555-1
   Yokota T, 2018, PROC CVPR IEEE, P8251, DOI 10.1109/CVPR.2018.00861
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang Song Guang-Jing, 2018, ROBUST TENSOR COMPLE
   Zhu L, 2017, PROC CVPR IEEE, P493, DOI 10.1109/CVPR.2017.60
   Zhu L, 2016, COMPUT GRAPH FORUM, V35, P217, DOI 10.1111/cgf.13019
NR 48
TC 6
Z9 6
U1 2
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 1899
EP 1915
DI 10.1007/s00371-020-01951-0
EA AUG 2020
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000556153200001
DA 2024-07-18
ER

PT J
AU Wen, JH
   Ma, HM
   Luo, X
AF Wen, Jinghuan
   Ma, Huimin
   Luo, Xiong
TI Deep generative smoke simulator: connecting simulated and real data
SO VISUAL COMPUTER
LA English
DT Article
DE Smoke simulation; Generative adversarial networks; Real data;
   Autoencoder; LSTM
ID VORTEX PARTICLE METHOD
AB We propose a novel generative adversarial architecture to generate realistic smoke sequences. Physically based smoke simulation methods are difficult to match with real-captured data since smoke is vulnerable to disturbance. In our work, we design a generator that takes into account the temporal movement of smoke as well as detailed structures. With the help of convolutional neural networks and long short-term memory-based autoencoder, our generator can predict the future frames using temporal information while preserving details. We use generative adversarial networks to train the model on both simulated and real-captured data and propose a combined loss function that reflects both the physical laws and the data distributions. We also demonstrate a multi-phase training strategy that significantly speeds up convergence and increases stability of training on real-captured data. To test our approach, we set up experiments to capture real smoke sequences and show that our method can achieve realistic visual effects.
C1 [Wen, Jinghuan; Ma, Huimin] Tsinghua Univ, Beijing, Peoples R China.
   [Luo, Xiong] Univ Sci & Technol Beijing, Beijing, Peoples R China.
C3 Tsinghua University; University of Science & Technology Beijing
RP Ma, HM (corresponding author), Tsinghua Univ, Beijing, Peoples R China.
EM wenjh14@mails.tsinghua.edu.cn; mhmpub@tsinghua.edu.cn; xluo@ustb.edu.cn
RI 马, 会民/AAM-8054-2021
OI 马, 会民/0000-0001-6155-9076
FU National Key Basic Research Program of China [2016YFB0100900]; National
   Natural Science Foundation of China [61773231]
FX This work was funded by National Key Basic Research Program of China
   (No. 2016YFB0100900) and National Natural Science Foundation of China
   (No. 61773231).
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Angelidis Alexis., 2006, S COMPUTER ANIMATION, P25
   [Anonymous], ARXIV180210123
   [Anonymous], VIDEO BACKGROUND HD
   [Anonymous], P 26 ANN C COMP GRAP
   [Anonymous], INT SOC OPTICS PHOTO
   [Anonymous], DIGITAL MEALS SMOKE
   [Anonymous], 2012, Computer Animation 2012-ACM SIGGRAPH / Eurographics Symposium Proceedings, SCA
   [Anonymous], 2005, Technical report
   [Anonymous], 2005, Proceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, DOI DOI 10.1145/1073368.1073380
   [Anonymous], 2010, P 2010 ACM SIGGRAPHE
   [Anonymous], ARXIV180602071
   Arjovsky M., 2017, ARXIV170107875
   BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984
   BRACKBILL JU, 1986, J COMPUT PHYS, V65, P314, DOI 10.1016/0021-9991(86)90211-1
   Browning Mark., 2014, Proceedings of the Workshop on Non-Photorealistic Animation and Rendering, NPAR'14, P63
   Chu MY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073643
   Farimani A. B., 2017, Deep Learning the Physics of Transport Phenomena
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gregson J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185548
   Gregson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601147
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jones B, 2016, ACM T GRAPHIC, V35, DOI [10.1145/2956233, 10.1145/2897824.2925979]
   Kalchbrenner N., 2016, ARXIV161000527
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360649
   Kingma D. P., 2013, ARXIV13126114
   Koshizuka S, 1996, NUCL SCI ENG, V123, P421, DOI 10.13182/NSE96-A24205
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Ladicky L, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818129
   Lee A.X., 2018, CoRR abs/1804.01523
   Li XS, 2014, VISUAL COMPUT, V30, P787, DOI 10.1007/s00371-014-0969-7
   Long Z., 2017, ARXIV171009668
   Lotter W., 2015, ARXIV151106380
   Maggioni M, 2012, IEEE T IMAGE PROCESS, V21, P3952, DOI 10.1109/TIP.2012.2199324
   Mathieu M., 2015, PROC INT C LEARN REP
   Meng Z, 2015, IEEE COMPUT GRAPH, V35, P60, DOI 10.1109/MCG.2015.5
   Mercier O, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818115
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Okabe M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766958
   Okabe M, 2011, COMPUT GRAPH FORUM, V30, P1973, DOI 10.1111/j.1467-8659.2011.02062.x
   Pfaff T, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185608
   Radford A., 2015, ARXIV
   Ranzato M., 2014, ARXIV14126604
   Rasmussen N, 2003, ACM T GRAPHIC, V22, P703, DOI 10.1145/882262.882335
   Saito M, 2017, IEEE I CONF COMP VIS, P2849, DOI 10.1109/ICCV.2017.308
   Selle A, 2005, ACM T GRAPHIC, V24, P910, DOI 10.1145/1073204.1073282
   Selle A, 2008, J SCI COMPUT, V35, P350, DOI 10.1007/s10915-007-9166-4
   Shi XJ, 2015, ADV NEUR IN, V28
   Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Tompson J., 2016, CoRR
   Um K, 2018, COMPUT GRAPH FORUM, V37, P171, DOI 10.1111/cgf.13522
   Wang C, 2017, VISUAL COMPUT, V33, P1211, DOI 10.1007/s00371-016-1284-2
   Wen JH, 2019, VISUAL COMPUT, V35, P1279, DOI 10.1007/s00371-018-1514-x
   Xie Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201304
   Yoon JC, 2009, COMPUT GRAPH FORUM, V28, P1853, DOI 10.1111/j.1467-8659.2009.01563.x
   Zhang XX, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766982
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
NR 63
TC 4
Z9 4
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2020
VL 36
IS 7
BP 1385
EP 1399
DI 10.1007/s00371-019-01738-y
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LO1PW
UT WOS:000533401000007
DA 2024-07-18
ER

PT J
AU Tian, SJ
   Shen, SW
   Tian, GQ
   Liu, XP
   Yin, BC
AF Tian, Shengjing
   Shen, Shuwei
   Tian, Guoqiang
   Liu, Xiuping
   Yin, Baocai
TI End-to-end deep metric network for visual tracking
SO VISUAL COMPUTER
LA English
DT Article
DE Metric learning; Visual tracking; Deep neural networks; One-shot
   learning
AB In this paper, we propose an end-to-end deep metric network (DMN) for visual tracking, where any target can be accurately tracked given only a bounding box of the first frame. Our main motivation is to make the network learn to learn a deep distance metric by following the philosophy of one-shot learning. Instead of utilizing a hand-crafted distance metric like Euclidean distance, our DMN focuses on providing a learnable metric, which is more robust to appearance variations. Furthermore, we are the first to properly combine mean square errors and contrastive loss into a joint loss function for back-propagation. During online tracking, DMN firstly applies our instance initialization for obtaining sequence-specific information and then straightforwardly tracks the target without the help of box refinement, occlusion detection and online updating. The final tracking score considers both our DMN scalar output and the constrain of motion smoothness. Ablation analyses are carried out to validate the effectiveness of our proposed method. And experiments on the prevalent benchmarks show that our method can achieve a competitive performance when compared with some representative trackers, especially those existing metric learning-based algorithms.
C1 [Tian, Shengjing; Shen, Shuwei; Tian, Guoqiang; Liu, Xiuping] Dalian Univ Technol, Sch Math Sci, Dalian 116024, Peoples R China.
   [Yin, Baocai] Dalian Univ Technol, Fac Elect Informat & Elect Engn, Dalian 116024, Peoples R China.
C3 Dalian University of Technology; Dalian University of Technology
RP Tian, SJ (corresponding author), Dalian Univ Technol, Sch Math Sci, Dalian 116024, Peoples R China.
EM tye@mail.dlut.edu.cn; ybc@dlut.edu.cn
RI Liu, Xiufang/I-8003-2015; Liu, Xiu/IYJ-9134-2023
FU National Natural Science Foundation of China [U1811463]
FX This work was funded by the National Natural Science Foundation of China
   (Grant Number U1811463).
CR [Anonymous], 2005, IEEE C COMP VIS PATT
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.465
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Briechle K., 2001, P SPIE OPT PATT REC
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Davis J. V., 2007, ICML, P209
   Elgammal A, 2003, IEEE C COMP VIS PATT
   Han Zhenjun, 2018, IEEE T CIRCUITS SYST
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   He AF, 2018, PROC CVPR IEEE, P4834, DOI 10.1109/CVPR.2018.00508
   Held D, 2016, LECT NOTES COMPUT SC, V9905, P749, DOI 10.1007/978-3-319-46448-0_45
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hu JL, 2016, IEEE T CIRC SYST VID, V26, P2056, DOI 10.1109/TCSVT.2015.2477936
   Rechy-Ramirez EJ, 2019, VISUAL COMPUT, V35, P41, DOI 10.1007/s00371-017-1446-x
   Jiang N, 2012, PROC CVPR IEEE, P1956, DOI 10.1109/CVPR.2012.6247897
   Kim K, 2010, VISUAL COMPUT, V26, P1145, DOI 10.1007/s00371-010-0490-6
   Kristan M, 2017, IEEE INT CONF COMP V, P1949, DOI 10.1109/ICCVW.2017.230
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li X, 2012, PROC CVPR IEEE, P1760, DOI 10.1109/CVPR.2012.6247872
   Lu J., 2016, IEEE INT C MULT EXP
   Ma C, 2019, IEEE T PATTERN ANAL, V41, P2709, DOI [10.1109/TPAMI.2018.2865311, 10.1109/INTMAG.2018.8508195]
   Ma ZY, 2014, VISUAL COMPUT, V30, P1133, DOI 10.1007/s00371-013-0894-1
   Mei X, 2009, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2009.5459292
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158
   Tsagkatakis G, 2011, IEEE T CIRC SYST VID, V21, P1810, DOI 10.1109/TCSVT.2011.2133970
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Wang D, 2013, PROC CVPR IEEE, P2371, DOI 10.1109/CVPR.2013.307
   Wang N., 2015, IEEE C COMP VIS PATT
   Wang N, 2013, P ADV NEURAL INFORM
   Wang Q, 2018, PROC CVPR IEEE, P4854, DOI 10.1109/CVPR.2018.00510
   Wang S, 2011, IEEE I CONF COMP VIS, P1323, DOI 10.1109/ICCV.2011.6126385
   Wang X., 2010, EUR C COMP VIS ECCV
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Wu YW, 2014, IEEE T CIRC SYST VID, V24, P865, DOI 10.1109/TCSVT.2013.2291283
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhang TZ, 2019, IEEE T PATTERN ANAL, V41, P473, DOI 10.1109/TPAMI.2018.2797082
   Zhang TZ, 2013, INT J COMPUT VISION, V101, P367, DOI 10.1007/s11263-012-0582-z
   Zhu ZY, 2018, IEEE INT CONF COMM
NR 50
TC 8
Z9 8
U1 1
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2020
VL 36
IS 6
BP 1219
EP 1232
DI 10.1007/s00371-019-01730-6
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LI0TO
UT WOS:000529199400010
DA 2024-07-18
ER

PT J
AU Qi, D
   Milef, N
   De, S
AF Qi, Di
   Milef, Nicholas
   De, Suvranu
TI <i>Divided Voxels</i>: an efficient algorithm for interactive cutting of
   deformable objects
SO VISUAL COMPUTER
LA English
DT Article
DE Interactive cutting; Progressive cutting; Voxel-based; Topological
   operator; Deformable objects
ID SOFT-TISSUE MODELS; DEFORMATIONS; CUTS
AB Efficient algorithms that support dynamic topological updates are necessary for the simulation of progressive interactive cutting of deformable objects. Existing mesh-based techniques suffer from the generation of ill-shaped elements, whereas voxel grid-based methods require additional cut surfaces to be generated or the use of lookup tables for pre-computed cutting patterns. To overcome these limitations of existing methods, we propose a novel voxel-based topological operator, divide, which divides a voxel into two voxels identical to the original voxel's size by dynamically distributing its voxel elements (nodes, edges) into the newly divided voxels until the cutting of the original voxel is completed. The connectivity between the divided voxels and the neighbors of the original voxel is retained during the cut, and new connectivity between the adjacent divided voxels is generated to represent the continuity of the cut. As a result, the cut surface can be generated directly from the divided voxels on the fly, and the correspondence between the cut surface and the simulation voxels is maintained without any additional effort. We use several example problems to demonstrate the efficiency of our method and compare it with other existing approaches.
C1 [Qi, Di; De, Suvranu] Rensselaer Polytech Inst, Ctr Modeling Simulat & Imaging Med, Troy, NY 12180 USA.
   [Milef, Nicholas] Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX USA.
C3 Rensselaer Polytechnic Institute; Texas A&M University System; Texas A&M
   University College Station
RP Qi, D (corresponding author), Rensselaer Polytech Inst, Ctr Modeling Simulat & Imaging Med, Troy, NY 12180 USA.
EM qid@rpi.edu
RI Qi, Di/AAR-4974-2020
OI Qi, Di/0000-0002-0467-6628
FU NIBIB [R01EB005807, R01EB010037, R01EB009362, R01EB014305, R01EB025241];
   NHLBI [R01HL119248]; NCI [R01CA197491]
FX Research reported in this article was supported by NIBIB under Award
   Number R01EB005807, R01EB010037, R01EB009362, R01EB014305, R01EB025241;
   NHLBI under Award Number R01HL119248; NCI under Award Number
   R01CA197491. The content is solely the responsibility of the authors and
   does not necessarily represent the official views of the National
   Institutes of Health.
CR Berndt I, 2017, IEEE COMPUT GRAPH, V37, P24, DOI 10.1109/MCG.2017.45
   Bielser D, 2004, GRAPH MODELS, V66, P398, DOI 10.1016/j.gmod.2004.05.009
   Bielser D, 2000, EIGHTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P116, DOI 10.1109/PCCGA.2000.883933
   Bielser D, 1999, COMPUT GRAPH FORUM, V18, pC31, DOI 10.1111/1467-8659.00325
   Cotin S, 2000, VISUAL COMPUT, V16, P437, DOI 10.1007/PL00007215
   Dick C, 2011, IEEE T VIS COMPUT GR, V17, P1663, DOI 10.1109/TVCG.2010.268
   Frisken-Gibson SF, 1999, IEEE T VIS COMPUT GR, V5, P333, DOI 10.1109/2945.817350
   Jacobson A., 2018, LIBIGL SIMPLE C GEOM
   Jerábková L, 2010, PROG BIOPHYS MOL BIO, V103, P217, DOI 10.1016/j.pbiomolbio.2010.09.012
   Jerabkova L, 2009, IEEE COMPUT GRAPH, V29, P61, DOI 10.1109/MCG.2009.32
   Jia SY, 2018, COMPUT GRAPH FORUM, V37, P45, DOI 10.1111/cgf.13162
   Jia SY, 2015, INT J COMPUT ASS RAD, V10, P1477, DOI 10.1007/s11548-014-1147-0
   Jong BS, 2010, VISUAL COMPUT, V26, P121, DOI 10.1007/s00371-009-0392-7
   Ju T, 2002, ACM T GRAPHIC, V21, P339
   Kaufmann P, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531356
   Koschier D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073666
   Kremer Michael, 2013, P 21 INT MESH ROUNDT, P531, DOI [10.1007/978-3-642-33573-0_312, DOI 10.1007/978-3-642-33573-0_312]
   Lorensen W.E., 1987, P 14 ANN C COMP GRAP, P163
   MANTEAUX PL, 2015, P 8 ACM SIGGRAPH C M, P125, DOI DOI 10.1145/2822013.2822018
   Mitchell N, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766918
   Molino N, 2004, ACM T GRAPHIC, V23, P385, DOI 10.1145/1015706.1015734
   Mor AB, 2000, LECT NOTES COMPUT SC, V1935, P598
   MOUSAVI, 2011, INT J NUMER METHODS
   Müller M, 2007, J VIS COMMUN IMAGE R, V18, P109, DOI 10.1016/j.jvcir.2007.01.005
   Müller M, 2005, ACM T GRAPHIC, V24, P471, DOI 10.1145/1073204.1073216
   Müller M, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P26, DOI 10.1109/CGI.2004.1309189
   NIENHUYS H, 2000, EUROGRAPHICS SHORT P
   Nienhuys H.-W., 2001, MICCAI 01, P145
   Pan JJ, 2015, COMPUT ANIMAT VIRT W, V26, P321, DOI 10.1002/cav.1655
   Paulus CJ, 2015, VISUAL COMPUT, V31, P831, DOI 10.1007/s00371-015-1123-x
   Pébay PP, 2003, MATH COMPUT, V72, P1817, DOI 10.1090/S0025-5718-03-01485-6
   Pietroni N, 2009, VISUAL COMPUT, V25, P227, DOI 10.1007/s00371-008-0216-1
   Seiler M, 2011, VISUAL COMPUT, V27, P519, DOI 10.1007/s00371-011-0561-3
   SHEWCHUK, 2002, P 11 INT MESH ROUNDT
   Sifakis E, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P73
   Steinemann D, 2006, P IEEE VIRT REAL ANN, P35, DOI 10.1109/VR.2006.74
   Wang MN, 2018, INT J MED ROBOT COMP, V14, DOI 10.1002/rcs.1923
   WANG Y, 2014, P ACM SIGGRAPH EUR S
   Wicke M, 2007, COMPUT GRAPH FORUM, V26, P355, DOI 10.1111/j.1467-8659.2007.01058.x
   Wu J, 2015, COMPUT GRAPH FORUM, V34, P161, DOI 10.1111/cgf.12528
   Wu J, 2011, INT CONF ACOUST SPEE, P25
NR 41
TC 4
Z9 5
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2021
VL 37
IS 5
BP 1113
EP 1127
DI 10.1007/s00371-020-01856-y
EA MAY 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RV2QF
UT WOS:000534468100001
PM 34024967
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Kim, S
   Kim, D
   Choi, S
AF Kim, Suzi
   Kim, Dodam
   Choi, Sunghee
TI CityCraft: 3D virtual city creation from a single image
SO VISUAL COMPUTER
LA English
DT Article
DE Image-based modeling; City modeling; Urban construction; Building
   modeling; Inverse procedural modeling
ID PROCEDURAL GENERATION
AB This paper introduces a method to generate a three-dimensional (3D) virtual model of an imaginary city from a single street-view image to represent the appearance of the city in a given input photograph. The proposed approach differs from reconstruction approaches, which generate a city model by guessing the city name from the input photograph. In contrast, we use machine learning to identify where to generate the city, what to allocate in the city, and how to arrange the components. We employ generative adversarial networks and convolutional neural networks to create a terrain map and identify the components and styles that represent the virtual city appearance. We demonstrate that our system creates 3D virtual cities that are visually similar in terms of plausibility and naturalness to actual cities corresponding to input photographs from around the world. To the best of our knowledge, this is the first work to generate a city model including all general city components, including streets, buildings, and vegetation, to match the style of a single input image.
C1 [Kim, Suzi; Choi, Sunghee] Korea Adv Inst Sci & Technol, Sch Comp, Daejeon, South Korea.
   [Kim, Dodam] Samsung Elect Co Ltd, Samsung Res, Suwon, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST); Samsung;
   Samsung Electronics
RP Choi, S (corresponding author), Korea Adv Inst Sci & Technol, Sch Comp, Daejeon, South Korea.
EM kimsuzi@kaist.ac.kr; dodam.kim@samsung.com; sunghee@kaist.edu
RI Kim, Suzi/GLU-0055-2022
FU National Research Foundation of Korea (NRF) [2015R1D1A1A09060399]
FX This study was funded by National Research Foundation of Korea (NRF)
   (Grant Number 2015R1D1A1A09060399).
CR AlHalawani S, 2014, COMPUT GRAPH FORUM, V33, P157, DOI 10.1111/cgf.12441
   Aliaga DG, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409113
   Aliaga DanielG., 2016, ACM SIGGRAPH 2016 Courses, page, P16
   [Anonymous], 2009, P 18 INT C WORLD WID
   [Anonymous], 2016, P 33 INT C MACH LEAR
   [Anonymous], ACM SIGGRAPH 2017 PO
   [Anonymous], GEOINFORMATION DISAS
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], CITY PLANNING CIVIL
   [Anonymous], 2016, ARXIV161102163
   [Anonymous], 2017, ACM T GRAPHIC, DOI DOI 10.1145/3130800.3130804
   [Anonymous], P 2 INT WORKSH PROC
   [Anonymous], EUROGRAPHICS 2010 SH
   [Anonymous], P PIA
   [Anonymous], 2014, ARXIV PREPRINT ARXIV
   Argudo O, 2017, VISUAL COMPUT, V33, P1005, DOI 10.1007/s00371-017-1393-6
   Bao F, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421644
   Bellotti F, 2011, COMPUT GRAPH-UK, V35, P1001, DOI 10.1016/j.cag.2011.07.004
   Chen GN, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360702
   Cordonnier G, 2016, COMPUT GRAPH FORUM, V35, P165, DOI 10.1111/cgf.12820
   Doersch C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185597
   Fan LB, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661265
   Gain J, 2017, COMPUT GRAPH FORUM, V36, P63, DOI 10.1111/cgf.13107
   Galin E, 2010, COMPUT GRAPH FORUM, V29, P429, DOI 10.1111/j.1467-8659.2009.01612.x
   Galin E, 2011, COMPUT GRAPH FORUM, V30, P2021, DOI 10.1111/j.1467-8659.2011.02055.x
   Génevaux JD, 2015, COMPUT GRAPH FORUM, V34, P198, DOI 10.1111/cgf.12530
   Génevaux JD, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461996
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Goodfellow IJ, 2014, ADV NEUR IN, P2672, DOI DOI 10.1145/3422622
   Guérin E, 2016, COMPUT GRAPH FORUM, V35, P177, DOI 10.1111/cgf.12821
   HENRICSSON O, 1996, AUTOMATED 3 D RECONS
   Hou F, 2016, VISUAL COMPUT, V32, P151, DOI 10.1007/s00371-015-1061-7
   Huijser R., 2010, Proceedings 2010 Brazilian Symposium on Games and Digital Entertainment (SBGAMES 2010), P189, DOI 10.1109/SBGAMES.2010.31
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Kanuk J, 2015, LANDSCAPE URBAN PLAN, V139, P40, DOI 10.1016/j.landurbplan.2015.02.015
   Kelly T, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130823
   Kingma D. P., 2014, arXiv
   Lipp M, 2011, COMPUT GRAPH FORUM, V30, P345, DOI 10.1111/j.1467-8659.2011.01865.x
   Lyu XM, 2017, TRANSP RES PROC, V25, P3337, DOI 10.1016/j.trpro.2017.05.194
   Ma R, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980223
   Müller P, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276484, 10.1145/1239451.1239536]
   Nishida G, 2018, COMPUT GRAPH FORUM, V37, P415, DOI 10.1111/cgf.13372
   Nishida G, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925951
   Parish YIH, 2001, COMP GRAPH, P301, DOI 10.1145/383259.383292
   Radford A., 2015, ARXIV
   Sharma R, 2016, PROCEEDINGS OF THE 5TH INTERNATIONAL CONFERENCE ON SYSTEM MODELING & ADVANCEMENT IN RESEARCH TRENDS (SMART-2016), P213, DOI 10.1109/SYSMART.2016.7894522
   Sinha SN, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409112
   Smelik RM, 2011, COMPUT GRAPH-UK, V35, P352, DOI 10.1016/j.cag.2010.11.011
   Smelik RM, 2014, COMPUT GRAPH FORUM, V33, P31, DOI 10.1111/cgf.12276
   Snavely N, 2008, INT J COMPUT VISION, V80, P189, DOI 10.1007/s11263-007-0107-3
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Sun J., 2002, P ACM S VIRTUAL REAL, P33, DOI [DOI 10.1145/585740.585747, 10 . 1145 / 585740 . 585747]
   Talton JO, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944851
   Vanegas CA, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366187
   Vanegas CA, 2012, COMPUT GRAPH FORUM, V31, P681, DOI 10.1111/j.1467-8659.2012.03047.x
   Vanek J, 2011, IEEE COMPUT GRAPH, V31, P35, DOI 10.1109/MCG.2011.66
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wolberg G, 2018, VISUAL COMPUT, V34, P605, DOI 10.1007/s00371-017-1365-x
   Yang YL, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508405
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
NR 60
TC 17
Z9 18
U1 6
U2 27
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2020
VL 36
IS 5
BP 911
EP 924
DI 10.1007/s00371-019-01701-x
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LJ3BL
UT WOS:000530043100004
DA 2024-07-18
ER

PT J
AU Niu, DM
   Guo, H
   Zhao, XY
   Zhang, CM
AF Niu, Dongmei
   Guo, Han
   Zhao, Xiuyang
   Zhang, Caiming
TI Three-dimensional salient point detection based on the Laplace-Beltrami
   eigenfunctions
SO VISUAL COMPUTER
LA English
DT Article
DE 3D salient point detection; Laplace-Beltrami eigenfunctions; Clustering;
   Representatives
ID 3D; FEATURES
AB Three-dimensional (3D) salient point detection is a fundamental problem in computer graphics and computer vision. We propose a new method using the Laplace-Beltrami eigenfunctions for detecting the salient points of 3D models. We compute the extrema of the low-frequency Laplace-Beltrami eigenfunctions and remove the redundancy of the extrema. Merging the extrema of the eigenfunctions, we keep the extrema appearing simultaneously on no less than a certain number of eigenfunctions as the candidate salient points. Clustering these extrema, we consider the representatives of the clusters as the final salient points. Our experimental results demonstrate that the proposed method is effective for 3D salient point detection. Besides that, some experiments are also conducted to verify that the proposed method is insensitive to small boundary noise.
C1 [Niu, Dongmei; Guo, Han; Zhao, Xiuyang] Univ Jinan, Shandong Prov Key Lab Network Based Intelligent C, Jinan 250022, Peoples R China.
   [Zhang, Caiming] Shandong Univ, Sch Comp Sci & Technol, 1500 Shunhua Rd, Jinan 250101, Peoples R China.
C3 University of Jinan; Shandong University
RP Zhao, XY (corresponding author), Univ Jinan, Shandong Prov Key Lab Network Based Intelligent C, Jinan 250022, Peoples R China.
EM dongmei_19881123@hotmail.com; 1723153491@qq.com; xiuyangzhao@gmail.com;
   czhang@sdu.edu.cn
RI Cheng, Lin/KFQ-3111-2024
FU Natural Science Foundation of Shandong province [ZR2019BF026,
   ZR2019MF013, ZR2017BF031]; Project of Jinan Scientific Research Leader's
   Laboratory [2018GXRC023]; Doctoral Program of University of Jinan
   [160100313]
FX This research was supported by Natural Science Foundation of Shandong
   province (No. ZR2019BF026, ZR2019MF013, ZR2017BF031), Project of Jinan
   Scientific Research Leader's Laboratory (No. 2018GXRC023) and Doctoral
   Program of University of Jinan (No. 160100313).
CR [Anonymous], 2009, MATLAB CENTRAL FILE
   [Anonymous], ARXIV160408806
   [Anonymous], SCALE INVARIANT FEAT
   Bremer PT, 2004, IEEE T VIS COMPUT GR, V10, P385, DOI 10.1109/TVCG.2004.3
   Castellani U, 2008, COMPUT GRAPH FORUM, V27, P643, DOI 10.1111/j.1467-8659.2008.01162.x
   Chavel Isaac., 1984, PURE APPL MATH
   Courant R., 1953, Methods of Mathematical Physics
   Dong S, 2006, ACM T GRAPHIC, V25, P1057, DOI 10.1145/1141911.1141993
   Dutagaci H, 2012, VISUAL COMPUT, V28, P901, DOI 10.1007/s00371-012-0746-4
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Gebal K, 2009, COMPUT GRAPH FORUM, V28, P1405, DOI 10.1111/j.1467-8659.2009.01517.x
   Gelfand N., 2005, P 3 EUR S GEOM PROC, V2, P5
   Godil A, 2011, PROC SPIE, V7864, DOI 10.1117/12.872984
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   Hu JX, 2013, VISUAL COMPUT, V29, P949, DOI 10.1007/s00371-013-0850-0
   Hu JX, 2009, VISUAL COMPUT, V25, P667, DOI 10.1007/s00371-009-0340-6
   Katz S, 2005, VISUAL COMPUT, V21, P649, DOI 10.1007/s00371-005-0344-9
   Knopp J, 2010, LECT NOTES COMPUT SC, V6316, P589, DOI 10.1007/978-3-642-15567-3_43
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Lian ZH, 2013, PATTERN RECOGN, V46, P449, DOI 10.1016/j.patcog.2012.07.014
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Novatnack J, 2007, IEEE I CONF COMP VIS, P2001
   ORourke J., 1999, ACM SIGACT News, V30, P31, DOI DOI 10.1145/568547.568559
   Pratikakis I., 2010, EUR WORKSH 3D OBJ RE, P7, DOI DOI 10.2312/3DOR/3DOR10/007-014
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Reuter M, 2010, INT J COMPUT VISION, V89, P287, DOI 10.1007/s11263-009-0278-1
   Reuter M, 2009, COMPUT GRAPH-UK, V33, P381, DOI 10.1016/j.cag.2009.03.005
   Ruggeri MR, 2010, INT J COMPUT VISION, V89, P248, DOI 10.1007/s11263-009-0250-0
   Shah SAA, 2017, PATTERN RECOGN, V64, P29, DOI 10.1016/j.patcog.2016.10.028
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Taubin Gabriel., 2000, State of the Art Report, P81
   Taylor Michael E., 1996, Partial Differential Equations: Basic Theory, V1, DOI DOI 10.1007/978-1-4684-9320-7
   Teran L, 2014, LECT NOTES COMPUT SC, V8689, P159, DOI 10.1007/978-3-319-10590-1_11
   Yu TH, 2013, INT J COMPUT VISION, V102, P180, DOI 10.1007/s11263-012-0563-2
NR 34
TC 2
Z9 2
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2020
VL 36
IS 4
BP 767
EP 784
DI 10.1007/s00371-019-01658-x
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KW0AL
UT WOS:000520835800009
DA 2024-07-18
ER

PT J
AU Wang, Y
   Wei, X
   Ding, L
   Tang, XL
   Zhang, HL
AF Wang, Yong
   Wei, Xian
   Ding, Lu
   Tang, Xiaoliang
   Zhang, Huanlong
TI A robust visual tracking method via local feature extraction and
   saliency detection
SO VISUAL COMPUTER
LA English
DT Article
DE Visual object tracking; Locally adaptive regression kernel; Correlation
   filter tracking; Saliency detection
ID OBJECT TRACKING
AB Visual object tracking is a fundamental problem in computer vision. It heavily relies on feature description for the appearance of object. In this paper, we present a robust algorithm which exploits the locally adaptive regression kernel (LARK) feature for visual tracking. The proposed approach formulates the LARK feature in a tracking by detection framework. In addition, we compute a target-specific saliency map as LARK feature with the guidance of the tracking framework. The tracking problem is solved by maximizing an object location likelihood function. We adopt Fast Fourier Transform for fast learning and detection in this work. Extensive experimental results on challenging videos show that the proposed algorithm performs favorably against state-of-the-art methods in terms of accuracy and robustness.
C1 [Wang, Yong] Univ Ottawa, Sch Elect Engn & Comp Sci, Ottawa, ON, Canada.
   [Wang, Yong; Wei, Xian; Tang, Xiaoliang] Chinese Acad Sci, Fujian Inst Res Struct Matter, Fuzhou 350002, Peoples R China.
   [Ding, Lu] Shanghai Jiao Tong Univ, Sch Aeronaut & Astronaut, Shanghai, Peoples R China.
   [Zhang, Huanlong] Zhengzhou Univ Light Ind, Coll Elect & Informat Engn, Zhengzhou, Peoples R China.
C3 University of Ottawa; Chinese Academy of Sciences; Fujian Institute of
   Research on the Structure of Matter, CAS; Shanghai Jiao Tong University;
   Zhengzhou University of Light Industry
RP Wei, X (corresponding author), Chinese Acad Sci, Fujian Inst Res Struct Matter, Fuzhou 350002, Peoples R China.
EM xian.wei@fjirsm.ac.cn
RI Ding, Lu/AAJ-2179-2020; Tang, Xiaoliang/O-6667-2018
OI , yong/0000-0001-6559-9550
FU CAS Pioneer Hundred Talents Program (Type C) [2017-122]; National
   Science Found for Young Scholars [61806186]; National Natural Science
   Foundation of China [61503173, 61873246]
FX This work was jointly supported by CAS Pioneer Hundred Talents Program
   (Type C) under Grant No. 2017-122, National Science Found for Young
   Scholars under Grant No. 61806186 and the National Natural Science
   Foundation of China (No. 61503173, No. 61873246).
CR [Anonymous], 2006, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2006.256
   [Anonymous], 2011, ICCV
   [Anonymous], 2012, CVPR
   [Anonymous], 2012, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2012.6247881
   [Anonymous], 2012, CVPR
   [Anonymous], 2018, 2018 10 INT C WIRELE, DOI DOI 10.1109/WCSP.2018.8555892
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Cheng Y, 2018, INT CONF INFO SCI, P472, DOI 10.1109/ICIST.2018.8426080
   Collins R. T., 2003, CVPR
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dalal N., 2005, IEEE C COMP VIS PATT
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dinh TB, 2011, PROC CVPR IEEE, P1177, DOI 10.1109/CVPR.2011.5995733
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Galoogahi H.K., 2016, 2016 IEEE WINTER C A, P1, DOI 10.1109/wacv.2016.7477554
   Grabner H., 2006, BMVC, P47
   Grabner H, 2008, LECT NOTES COMPUT SC, V5302, P234, DOI 10.1007/978-3-540-88682-2_19
   Hae Jong Seo, 2009, 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P45, DOI 10.1109/CVPR.2009.5204207
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2013, IEEE I CONF COMP VIS, P2760, DOI 10.1109/ICCV.2013.343
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hu HW, 2018, IEEE T NEUR NET LEAR, V29, P1786, DOI 10.1109/TNNLS.2017.2688448
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231
   Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821
   Li F, 2017, IMAGE VISION COMPUT, V60, P124, DOI 10.1016/j.imavis.2017.01.003
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905
   Liu BY, 2011, PROC CVPR IEEE, P1313, DOI 10.1109/CVPR.2011.5995730
   Liu FY, 2016, IMAGE VISION COMPUT, V51, P84, DOI 10.1016/j.imavis.2016.04.008
   Ma B, 2016, IEEE T IMAGE PROCESS, V25, P5867, DOI 10.1109/TIP.2016.2615812
   Ma B, 2016, IEEE T CYBERNETICS, V46, P2411, DOI 10.1109/TCYB.2015.2477879
   Ma B, 2016, IEEE T IMAGE PROCESS, V25, P4199, DOI 10.1109/TIP.2016.2588329
   Ma B, 2015, IEEE T MULTIMEDIA, V17, P1818, DOI 10.1109/TMM.2015.2463221
   Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177
   Ma C, 2009, ADVANCES IN WATER RESOURCES AND HYDRAULIC ENGINEERING, VOLS 1-6, P744, DOI 10.1007/978-3-540-89465-0_131
   Mei X, 2011, IEEE T PATTERN ANAL, V33, P2259, DOI 10.1109/TPAMI.2011.66
   Perez P., 2002, ECCV
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Seo HJ, 2011, IEEE T INF FOREN SEC, V6, P1275, DOI 10.1109/TIFS.2011.2159205
   Seo HJ, 2011, IEEE T PATTERN ANAL, V33, P867, DOI 10.1109/TPAMI.2010.156
   Seo HJ, 2010, IEEE T PATTERN ANAL, V32, P1688, DOI 10.1109/TPAMI.2009.153
   Seo HJ, 2009, J VISION, V9, DOI 10.1167/9.12.15
   Sevilla-Lara L., 2012, CVPR
   Shen JB, 2018, IEEE T INTELL TRANSP, V19, P162, DOI 10.1109/TITS.2017.2750082
   Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230
   Stalder S., 2009, ICCV WORKSH
   Ta D.-N., 2009, CVPR
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang WG, 2016, IEEE T IMAGE PROCESS, V25, P5025, DOI 10.1109/TIP.2016.2601784
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P4185, DOI 10.1109/TIP.2015.2460013
   Wang Y, 2017, IEEE INT CONF COMP V, P2129, DOI 10.1109/ICCVW.2017.249
   Wei X, 2019, IEEE T NEUR NET LEAR, V30, P175, DOI 10.1109/TNNLS.2018.2836802
   Wei X, 2017, IEEE T IMAGE PROCESS, V26, P2929, DOI 10.1109/TIP.2017.2691549
   Wu Y., 2015, P BRIT MACHINE VISIO, P1
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang KH, 2014, LECT NOTES COMPUT SC, V8693, P127, DOI 10.1007/978-3-319-10602-1_9
   Zhang KH, 2014, IEEE T PATTERN ANAL, V36, P2002, DOI 10.1109/TPAMI.2014.2315808
   Zhang SY, 2018, IEEE IPCCC
   Zhang TZ, 2012, PROC CVPR IEEE, P2042, DOI 10.1109/CVPR.2012.6247908
   Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882
NR 75
TC 20
Z9 22
U1 0
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2020
VL 36
IS 4
BP 683
EP 700
DI 10.1007/s00371-019-01646-1
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KW0AL
UT WOS:000520835800003
DA 2024-07-18
ER

PT J
AU An, FP
   Liu, ZW
AF An, Fengping
   Liu, Zhiwen
TI Facial expression recognition algorithm based on parameter adaptive
   initialization of CNN and LSTM
SO VISUAL COMPUTER
LA English
DT Article
DE Model parameter initialization method; CNN; Facial expression
   recognition; Deep learning; LSTM; SVM
AB In view of the high dimensionality, nonrigidity, multiscale variation and the influence of illumination and angle on facial expressions, it is quite difficult to obtain facial expression images or videos using computers and analyze facial morphology and changes to accurately obtain the emotional changes of the subjects. Existing facial expression recognition algorithms have the following problems in the application process: the existing shallow feature extraction model has lost a lot of effective feature information and low recognition accuracy. The facial expression recognition method based on deep learning has problems such as overfitting, gradient explosion and parameter initialization. Therefore, this paper develops a facial expression recognition algorithm based on the deep learning method. An adaptive model parameter initialization based on the multilayer maxout network linear activation function is proposed to initialize the convolutional neural network (CNN) and the long-short-term memory network (LSTM) method. It can effectively overcome the gradient disappearance and gradient explosion problems in the deep learning model training process. At the same time, the convolutional neural network with an LSTM memory unit is used to extract the related information from the image sequence, and the facial expression judgment is based on a single-frame image and historical-related information. However, the top-level structure of the CNN model is a fully connected feedforward neural network, which undertakes the task of expression classification. Therefore, the SVM classification method replaces the top-level classifier to further improve the expression classification accuracy. Experiments show that the facial expression recognition method proposed in this paper not only accurately identifies various expressions but also has good adaptive ability. This is because the method achieves the adaptive initialization of the parameters of the deep learning model construction process and also analyzes the relevance of the expression database expression, thereby improving the accuracy of expression recognition.
C1 [An, Fengping] Huaiyin Normal Univ, Sch Phys & Elect Elect Engn, Huaian 223300, Peoples R China.
   [An, Fengping; Liu, Zhiwen] Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China.
C3 Huaiyin Normal University; Beijing Institute of Technology
RP An, FP (corresponding author), Huaiyin Normal Univ, Sch Phys & Elect Elect Engn, Huaian 223300, Peoples R China.; An, FP (corresponding author), Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China.
EM anfengping@163.com; liuzhiwen1961@163.com
OI AN, FENGPING/0000-0002-2220-2987
FU National Natural Science Foundation of China [61701188]
FX This paper is supported by National Natural Science Foundation of China
   (No. 61701188).
CR [Anonymous], 2014, PROCEDIA IEEE COMPUT, DOI DOI 10.1109/CVPR.2014.233
   [Anonymous], T INT J ELECT ELECT
   [Anonymous], 2005, Int. J. Inf. Technol.
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Chen JH, 2015, INT CONF AFFECT, P636, DOI 10.1109/ACII.2015.7344636
   Ding H, 2017, IEEE INT CONF AUTOMA, P118, DOI 10.1109/FG.2017.23
   Eleftheriadis S, 2015, IEEE T IMAGE PROCESS, V24, P189, DOI 10.1109/TIP.2014.2375634
   Goodfellow IJ, 2015, NEURAL NETWORKS, V64, P59, DOI 10.1016/j.neunet.2014.09.005
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Jung H, 2015, IEEE I CONF COMP VIS, P2983, DOI 10.1109/ICCV.2015.341
   Kacem A, 2017, IEEE I CONF COMP VIS, P3199, DOI 10.1109/ICCV.2017.345
   Liu MY, 2015, LECT NOTES COMPUT SC, V9006, P143, DOI 10.1007/978-3-319-16817-3_10
   Liu MY, 2014, PROC CVPR IEEE, P1749, DOI 10.1109/CVPR.2014.226
   Lopes AT, 2017, PATTERN RECOGN, V61, P610, DOI 10.1016/j.patcog.2016.07.026
   Lopes AT, 2015, SIBGRAPI, P273, DOI 10.1109/SIBGRAPI.2015.14
   Maninchedda F, 2017, PROC CVPR IEEE, P4608, DOI 10.1109/CVPR.2017.490
   Mehrabian A., 2008, Communication theory, V6, P193, DOI [10.4324/9781315080918, DOI 10.4324/9781315080918]
   Pickett L, 2018, QUALITY, V57, p12A
   Pransky J, 2018, IND ROBOT, V45, P307, DOI 10.1108/IR-04-2018-0060
   Shan CF, 2009, IMAGE VISION COMPUT, V27, P803, DOI 10.1016/j.imavis.2008.08.005
   Soyel H., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P585, DOI 10.1109/FG.2011.5771463
   Vouloutsi V., 2018, LIVING MACHINES HDB, P327
   Wu Y., 2016, GOOGLES NEURAL MACHI
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Yu ZD, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P435
   Zhang FF, 2016, FRONT COMPUT SCI-CHI, V10, P832, DOI 10.1007/s11704-015-5323-3
   Zhang KH, 2017, IEEE T IMAGE PROCESS, V26, P4193, DOI 10.1109/TIP.2017.2689999
   Zhang T., 2017, ADV INTELLIGENT SYST, P345
   Zhao GY, 2011, IMAGE VISION COMPUT, V29, P607, DOI 10.1016/j.imavis.2011.07.002
   Zhao XY, 2016, LECT NOTES COMPUT SC, V9906, P425, DOI 10.1007/978-3-319-46475-6_27
NR 30
TC 36
Z9 41
U1 7
U2 59
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2020
VL 36
IS 3
BP 483
EP 498
DI 10.1007/s00371-019-01635-4
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KL4EO
UT WOS:000513378500004
OA Bronze
DA 2024-07-18
ER

PT J
AU Chang, WC
   Wu, CH
AF Chang, Wen-Chung
   Wu, Chia-Hung
TI Candidate-based matching of 3-D point clouds with axially switching pose
   estimation
SO VISUAL COMPUTER
LA English
DT Article
DE Computed closer point (CCP); Iterative closest point (ICP); KD-tree;
   Point cloud; 3-D registration
ID 3D SCENES; REGISTRATION; SCANNER; ICP
AB In 3-D pattern matching, a reconstructed 3-D data point cloud of an object can be matched with the 3-D model point cloud to determine the object pose by employing the iterative closest point (ICP) algorithm. Typical ICP algorithm is time-consuming and may get stuck in local minimum if the pose difference between the two point clouds is not small enough. To resolve this uncertainty problem and enhance the matching capability, the candidate-based axially switching (CBAS) computed closer point (CCP) approach is proposed which is efficient, effective, and robust. It is based on evaluating origin candidates in the model point cloud to determine the approximate pose of the data point cloud. The proposed CBAS-CCP approach allows large uncertainty of the pose difference between the model and the object. The applicability and effectiveness of the proposed approach has been successfully validated by experimenting with 3-D data of real objects.
C1 [Chang, Wen-Chung] Natl Taipei Univ Technol, Dept Elect Engn, Taipei Tech Box 2125, Taipei 10608, Taiwan.
   [Wu, Chia-Hung] Ind Technol Res Inst, Mech & Mechatron Syst Res Labs, Hsinchu 31040, Taiwan.
C3 National Taipei University of Technology; Industrial Technology Research
   Institute - Taiwan
RP Chang, WC (corresponding author), Natl Taipei Univ Technol, Dept Elect Engn, Taipei Tech Box 2125, Taipei 10608, Taiwan.
EM wchang@ntut.edu.tw
RI Chang, Wen-Chung/I-4025-2013
OI Chang, Wen-Chung/0000-0002-2472-1772
FU Industrial Technology Research Institute, Taiwan, ROC [D301AA4111-FY103]
FX This research was supported by Industrial Technology Research Institute,
   Taiwan, ROC under Grant D301AA4111-FY103.
CR Altantsetseg E, 2018, VISUAL COMPUT, V34, P1021, DOI 10.1007/s00371-018-1534-6
   [Anonymous], 2009, THESIS
   Arrigoni F, 2016, LECT NOTES COMPUT SC, V9908, P489, DOI 10.1007/978-3-319-46493-0_30
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Chang WC, 2018, ICCMA 2018: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON CONTROL, MECHATRONICS AND AUTOMATION, P159, DOI 10.1145/3284516.3284525
   Chen J, 2013, OPT LASER TECHNOL, V45, P414, DOI 10.1016/j.optlastec.2012.06.015
   Cheng Y, 2008, IEEE T AUTOM SCI ENG, V5, P140, DOI 10.1109/TASE.2007.912058
   Chetverikov D, 2005, IMAGE VISION COMPUT, V23, P299, DOI 10.1016/j.imavis.2004.05.007
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   Daniels J, 2008, VISUAL COMPUT, V24, P449, DOI 10.1007/s00371-008-0223-2
   Diez Y, 2012, PATTERN RECOGN LETT, V33, P2127, DOI 10.1016/j.patrec.2012.07.006
   Du SY, 2015, NEUROCOMPUTING, V157, P187, DOI 10.1016/j.neucom.2015.01.019
   Du SY, 2010, PATTERN RECOGN LETT, V31, P791, DOI 10.1016/j.patrec.2010.01.020
   El Hazzat S, 2018, VISUAL COMPUT, V34, P1443, DOI 10.1007/s00371-017-1451-0
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Grant D, 2012, ISPRS J PHOTOGRAMM, V72, P16, DOI 10.1016/j.isprsjprs.2012.05.007
   Greenspan M, 2003, FOURTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P442, DOI 10.1109/IM.2003.1240280
   Guo H, 2016, VISUAL COMPUT, V32, P1511, DOI 10.1007/s00371-015-1136-5
   Guo XY, 2018, VISUAL COMPUT, V34, P93, DOI 10.1007/s00371-016-1316-y
   Guo YL, 2014, IEEE T PATTERN ANAL, V36, P2270, DOI 10.1109/TPAMI.2014.2316828
   He BW, 2013, OPT LASER TECHNOL, V46, P53, DOI 10.1016/j.optlastec.2012.04.027
   Hu SY, 2019, VISUAL COMPUT, V35, P1841, DOI 10.1007/s00371-018-1578-7
   Huang J., 2012, COMP VIS PATT REC WO, P41, DOI [DOI 10.1109/CVPRW.2012.6238913, 10.1109/CVPRW.2012.6238913]
   Jani A, 2013, 3D RES, V4, DOI 10.1007/3DRes.03(2013)2
   Jiang J, 2009, NEUROCOMPUTING, V72, P3839, DOI 10.1016/j.neucom.2009.05.013
   Li XD, 2013, COMPUT IND, V64, P1129, DOI 10.1016/j.compind.2013.06.003
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P494, DOI 10.1109/TPAMI.2019.2894422
   Mavridis P, 2015, COMPUT AIDED GEOM D, V35-36, P16, DOI 10.1016/j.cagd.2015.03.022
   McKinnon C, 2014, IEEE T AUTOM SCI ENG, V11, P935, DOI 10.1109/TASE.2014.2308011
   Mellado N, 2014, COMPUT GRAPH FORUM, V33, P205, DOI 10.1111/cgf.12446
   Meng Y, 2011, VISUAL COMPUT, V27, P543, DOI 10.1007/s00371-011-0580-0
   Nozaki T, 2013, IEEE T IND INFORM, V9, P2043, DOI 10.1109/TII.2012.2232934
   Oztireli AC, 2008, VISUAL COMPUT, V24, P679, DOI 10.1007/s00371-008-0248-6
   Park SY, 2011, MACH VISION APPL, V22, P563, DOI 10.1007/s00138-010-0248-1
   Phillips JM, 2007, 3DIM 2007: SIXTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P427
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Sharp GC, 2004, IEEE T PATTERN ANAL, V26, P1037, DOI 10.1109/TPAMI.2004.49
   Shi JL, 2018, VISUAL COMPUT, V34, P377, DOI 10.1007/s00371-016-1339-4
   Steder B, 2011, IEEE INT CONF ROBOT, P2601, DOI 10.1109/ICRA.2011.5980187
   Turk G., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P311, DOI 10.1145/192161.192241
   Xie ZX, 2010, IMAGE VISION COMPUT, V28, P563, DOI 10.1016/j.imavis.2009.09.006
   Xu GL, 2016, IEEE IJCNN, P4627, DOI 10.1109/IJCNN.2016.7727806
   Xu Y, 2015, IEEE T AUTOM SCI ENG, V12, P775, DOI 10.1109/TASE.2015.2396041
   Yang JL, 2016, IEEE T PATTERN ANAL, V38, P2241, DOI 10.1109/TPAMI.2015.2513405
   Ying SH, 2009, IEEE T AUTOM SCI ENG, V6, P559, DOI 10.1109/TASE.2009.2021337
   Zhang R, 2018, ISPRS J PHOTOGRAMM, V143, P85, DOI 10.1016/j.isprsjprs.2018.04.022
   Zhang XG, 2012, IEEE T IND INFORM, V8, P148, DOI 10.1109/TII.2011.2172453
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
NR 50
TC 11
Z9 12
U1 0
U2 34
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2020
VL 36
IS 3
BP 593
EP 607
DI 10.1007/s00371-019-01642-5
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KL4EO
UT WOS:000513378500011
DA 2024-07-18
ER

PT J
AU Li, GH
   Yang, S
   Cao, SM
   Zhu, WD
   Ke, YL
AF Li, Guanhua
   Yang, Shuang
   Cao, Siming
   Zhu, Weidong
   Ke, Yinglin
TI A semi-supervised deep learning approach for circular hole detection on
   composite parts
SO VISUAL COMPUTER
LA English
DT Article
DE Texture segmentation; LEP; LBP; Deep learning; Semi-supervised learning
ID HOUGH TRANSFORM; ALGORITHM; REPRESENTATION; LINES
AB This paper introduces the usage of semi-supervised learning to obtain competitive detection accuracy of measuring drilled holes on composite parts with very limited noisy training data. An improved texture segmentation algorithm based on local binary patterns algorithm is proposed, named local exponential patterns. The algorithm divides the image texture into nine levels, of which the highest level of texture is selected for contour extraction. An ellipse fitting method is used to fit the target contours and vote for the candidate ellipses. The regions inside the candidate ellipses are taken as the semi-supervised semantic label for images. A new loss named round loss is proposed, and a superior circle segmentation model was trained by learning from incompletely annotated data. To verify the effectiveness of the method, experiments were conducted with the drilled holes on the composite parts. The results show that the proposed semi-supervised deep learning approach is exceedingly suitable for circle detection of holes with different texture information commonly found in robotic drilling. Massive data labeling can be completely avoided with proposed method. The measurement accuracy can reach 0.03 mm, which can meet the visual measurement requirements of the circular holes on composite parts in the robotic drilling system.
C1 [Li, Guanhua; Yang, Shuang; Cao, Siming; Zhu, Weidong; Ke, Yinglin] Zhejiang Univ, Sch Mech Engn, State Key Lab Fluid Power & Mechatron Syst, Hangzhou 310027, Peoples R China.
   [Li, Guanhua; Yang, Shuang; Cao, Siming] Zhejiang Univ, Sch Mech Engn, Key Lab Adv Mfg Technol Zhejiang Prov, Hangzhou 310027, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Zhu, WD (corresponding author), Zhejiang Univ, Sch Mech Engn, State Key Lab Fluid Power & Mechatron Syst, Hangzhou 310027, Peoples R China.
EM wdzhu@zju.edu.cn
RI Cao, Siming/JEZ-4871-2023; Cao, Siming/HPE-6308-2023
FU National Natural Science Foundation of China [51675479, 51205352,
   51575479, 51521064]; Special scientific research for civil aircraft
   [MJZ-G-2011-07]
FX This research is supported by National Natural Science Foundation of
   China (No.51675479, 51205352, 51575479, 51521064) and Special scientific
   research for civil aircraft (NO.MJZ-G-2011-07).
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Akinlar C, 2013, PATTERN RECOGN, V46, P725, DOI 10.1016/j.patcog.2012.09.020
   Amirolad A, 2016, VISUAL COMPUT, V32, P1633, DOI 10.1007/s00371-016-1220-5
   Ayala-Ramirez V, 2006, PATTERN RECOGN LETT, V27, P652, DOI 10.1016/j.patrec.2005.10.003
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bieniek A, 2000, PATTERN RECOGN, V33, P907, DOI 10.1016/S0031-3203(99)00154-5
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen SL, 2017, PATTERN RECOGN, V68, P82, DOI 10.1016/j.patcog.2017.03.007
   Chen TC, 2001, COMPUT VIS IMAGE UND, V83, P172, DOI 10.1006/cviu.2001.0923
   Chung KL, 2007, APPL MATH COMPUT, V190, P132, DOI 10.1016/j.amc.2007.01.012
   Chung KL, 2012, PATTERN RECOGN, V45, P252, DOI 10.1016/j.patcog.2011.07.004
   Cuevas E, 2012, INFORM SCIENCES, V182, P40, DOI 10.1016/j.ins.2010.12.024
   DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Galloway MM., 1975, COMPUTER GRAPHICS IM, V4, P172, DOI DOI 10.1016/S0146-664X(75)80008-6
   Gou JP, 2019, EXPERT SYST APPL, V133, P9, DOI 10.1016/j.eswa.2019.05.009
   HAN JH, 1994, PATTERN RECOGN LETT, V15, P649, DOI 10.1016/0167-8655(94)90068-X
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   HASSNER M, 1980, COMPUT VISION GRAPH, V12, P357, DOI 10.1016/0146-664X(80)90019-2
   Hu P, 2019, IEEE T IMAGE PROCESS, V28, P5352, DOI 10.1109/TIP.2019.2913511
   Hu P, 2018, KNOWL-BASED SYST, V149, P34, DOI 10.1016/j.knosys.2018.02.008
   Long J., 2014, 79 ARXIV, V79
   Lopez L.Sanchez., 2010, Local Binary Patterns applied to Face Detection and Recognition
   Luo LK, 2019, VISUAL COMPUT, V35, P1869, DOI 10.1007/s00371-018-1580-0
   Maletic JI, 2005, KYBERNETES, V33, P809
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   Martínez J, 2013, VISUAL COMPUT, V29, P467, DOI 10.1007/s00371-013-0781-9
   Mei B, 2015, PATTERN ANAL APPL, V18, P695, DOI 10.1007/s10044-014-0394-6
   Nakanishi M, 2000, MACH VISION APPL, V12, P59, DOI 10.1007/s001380050125
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Ojala T., 2001, Advances in Pattern Recognition - ICAPR 2001. Second International Conference. Proceedings (Lecture Notes in Computer Science Vol.2013), P397
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Peng X, 2018, IEEE T IMAGE PROCESS, V27, P5076, DOI 10.1109/TIP.2018.2848470
   Pham S., 1992, Visual Computer, V9, P1, DOI 10.1007/BF01901025
   Pietikäinen M, 2000, PATTERN RECOGN, V33, P43, DOI 10.1016/S0031-3203(99)00032-1
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shaked D, 1996, COMPUT VIS IMAGE UND, V63, P512, DOI 10.1006/cviu.1996.0038
   Smith JR, 1996, INT CONF ACOUST SPEE, P2239, DOI 10.1109/ICASSP.1996.545867
   XU L, 1990, PATTERN RECOGN LETT, V11, P331, DOI 10.1016/0167-8655(90)90042-Z
NR 39
TC 9
Z9 9
U1 3
U2 32
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2021
VL 37
IS 3
BP 433
EP 445
DI 10.1007/s00371-020-01812-w
EA FEB 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QY5QT
UT WOS:000516005300001
DA 2024-07-18
ER

PT J
AU Li, K
   Jin, Y
   Akram, MW
   Han, RZ
   Chen, JW
AF Li, Kuan
   Jin, Yi
   Akram, Muhammad Waqar
   Han, Ruize
   Chen, Jiongwei
TI Facial expression recognition with convolutional neural networks via a
   new face cropping and rotation strategy
SO VISUAL COMPUTER
LA English
DT Article
DE Face cropping; Facial expression recognition; Convolutional neural
   network; Computer vision
ID CLASSIFICATION
AB With the recent development and application of human-computer interaction systems, facial expression recognition (FER) has become a popular research area. The recognition of facial expression is a difficult problem for existing machine learning and deep learning models because that the images can vary in brightness, background, pose, etc. Deep learning methods also require the support of big data. It does not perform well when the database is small. Feature extraction is very important for FER, even a simple algorithm can be very effective if the extracted features are sufficient to be separable. However, deep learning methods automatically extract features so that some useless features can interfere with useful features. For these reasons, FER is still a challenging problem in computer vision. In this paper, with the aim of coping with few data and extracting only useful features from image, we propose new face cropping and rotation strategies and simplification of the convolutional neural network (CNN) to make data more abundant and only useful facial features can be extracted. Experiments to evaluate the proposed method were performed on the CK+ and JAFFE databases. High average recognition accuracies of 97.38% and 97.18% were obtained for 7-class experiments on the CK+ and JAFFE databases, respectively. A study of the impact of each proposed data processing method and CNN simplification is also presented. The proposed method is competitive with existing methods in terms of training time, testing time, and recognition accuracy.
C1 [Li, Kuan; Jin, Yi; Akram, Muhammad Waqar; Chen, Jiongwei] Univ Sci & Technol China, Dept Precis Machinery & Precis Instrumentat, 96 Jinzhai Rd, Hefei 230026, Anhui, Peoples R China.
   [Han, Ruize] Tianjin Univ, Sch Comp Sci & Technol, 135 Yaguan Rd, Tianjin 300350, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Tianjin University
RP Jin, Y (corresponding author), Univ Sci & Technol China, Dept Precis Machinery & Precis Instrumentat, 96 Jinzhai Rd, Hefei 230026, Anhui, Peoples R China.
EM jinyi08@ustc.edu.cn
RI Han, Ruize/AAX-2958-2021; Akram, Muhammad Waqar/T-7150-2018; Jin,
   Yi/ABG-3022-2022
OI Han, Ruize/0000-0002-6587-8936; Akram, Muhammad
   Waqar/0000-0001-5559-043X; 
FU National Natural Science Foundation of China [51605464]; National Basic
   Research Program of China (973 Program) [2014CB049500]; Research on the
   Major Scientific Instrument of National Natural Science Foundation of
   China [61727809]
FX This research was sponsored by the National Natural Science Foundation
   of China (Grant No. 51605464), National Basic Research Program of China
   (973 Program) (2014CB049500) and Research on the Major Scientific
   Instrument of National Natural Science Foundation of China (61727809).
CR [Anonymous], 2014, PROCEDIA IEEE COMPUT, DOI DOI 10.1109/CVPR.2014.233
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   De la Torre Fernando, 2015, IEEE Int Conf Autom Face Gesture Recognit Workshops, V1, DOI 10.1109/FG.2015.7163082
   Ekman P., 1978, Consulting Psychologists
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Gogic I, 2020, VISUAL COMPUT, V36, P97, DOI 10.1007/s00371-018-1585-8
   Goh KM, 2020, VISUAL COMPUT, V36, P445, DOI 10.1007/s00371-018-1607-6
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Jin H, 2019, VISUAL COMPUT, V35, P535, DOI 10.1007/s00371-018-1482-1
   JONES JP, 1987, J NEUROPHYSIOL, V58, P1233, DOI 10.1152/jn.1987.58.6.1233
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu MY, 2015, NEUROCOMPUTING, V159, P126, DOI 10.1016/j.neucom.2015.02.011
   Lopes AT, 2017, PATTERN RECOGN, V61, P610, DOI 10.1016/j.patcog.2016.07.026
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Lyons MJ, 1999, IEEE T PATTERN ANAL, V21, P1357, DOI 10.1109/34.817413
   Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3
   Mayya V, 2016, 2016 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS AND INFORMATICS (ICACCI), P699, DOI 10.1109/ICACCI.2016.7732128
   Mayya V, 2016, PROCEDIA COMPUT SCI, V93, P453, DOI 10.1016/j.procs.2016.07.233
   Mehrabian A., 2008, Communication theory, V6, P193, DOI [10.4324/9781315080918, DOI 10.4324/9781315080918]
   Mohammadi MR, 2014, J VIS COMMUN IMAGE R, V25, P1082, DOI 10.1016/j.jvcir.2014.03.006
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Owusu E, 2014, APPL INTELL, V40, P536, DOI 10.1007/s10489-013-0478-9
   Pu XR, 2015, NEUROCOMPUTING, V168, P1173, DOI 10.1016/j.neucom.2015.05.005
   Rashid M, 2013, VISUAL COMPUT, V29, P1269, DOI 10.1007/s00371-012-0768-y
   Rivera AR, 2013, IEEE T IMAGE PROCESS, V22, P1740, DOI 10.1109/TIP.2012.2235848
   Shan CF, 2009, IMAGE VISION COMPUT, V27, P803, DOI 10.1016/j.imavis.2008.08.005
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sutskever I., 2013, INT C MACHINE LEARNI, P1139
   Uddin MZ, 2017, COMPUT ELECTR ENG, V63, P114, DOI 10.1016/j.compeleceng.2017.04.019
   Wen GH, 2017, COGN COMPUT, V9, P597, DOI 10.1007/s12559-017-9472-6
   Yang P, 2007, PROC CVPR IEEE, P688
   Yu ZB, 2018, VISUAL COMPUT, V34, P1691, DOI 10.1007/s00371-017-1443-0
   Zeng NY, 2018, NEUROCOMPUTING, V273, P643, DOI 10.1016/j.neucom.2017.08.043
   Zhang KH, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P534, DOI 10.1109/ACPR.2015.7486560
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhao JF, 2018, VISUAL COMPUT, V34, P1461, DOI 10.1007/s00371-018-1477-y
NR 39
TC 78
Z9 83
U1 2
U2 60
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2020
VL 36
IS 2
BP 391
EP 404
DI 10.1007/s00371-019-01627-4
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ2TH
UT WOS:000511910300012
DA 2024-07-18
ER

PT J
AU Li, H
   Zhang, SH
   Kong, WH
AF Li, He
   Zhang, Shihui
   Kong, Weihang
TI Bilateral counting network for single-image object counting
SO VISUAL COMPUTER
LA English
DT Article
DE Object counting; Bilateral counting network; Multi-scale feature;
   Different-level context
AB This paper proposes a novel bilateral counting network to estimate the accurate and robust counting result for single-image object counting task. The proposed network is composed of two main components: the concentrated dilated pyramid module and dual-context extraction path. The concentrated dilated pyramid module extracts the multi-scale feature from the image to address the scale variant issue in object counting task via a pyramid structure and also uses a shortcut concentration to facilitate the back-propagation of the gradient so as to improve the counting performance. And the dual-context extraction path obtains different-level context related to the object counting task through convoluting and down-sampling the image different times. The concentrated dilated pyramid module and the dual-context extraction path are integrated to boost the final counting result. Extensive experiments on vehicle counting and crowd counting datasets including TRANCOS, Mall, Shanghaitech_A and WorldExpo'10 demonstrate the feasibility and effectiveness for the object counting task.
C1 [Li, He; Zhang, Shihui; Kong, Weihang] Yanshan Univ, Sch Informat Sci & Engn, Qinhuangdao 066004, Hebei, Peoples R China.
   [Zhang, Shihui] Key Lab Comp Virtual Technol & Syst Integrat Hebe, Qinhuangdao 066004, Hebei, Peoples R China.
C3 Yanshan University
RP Zhang, SH (corresponding author), Yanshan Univ, Sch Informat Sci & Engn, Qinhuangdao 066004, Hebei, Peoples R China.; Zhang, SH (corresponding author), Key Lab Comp Virtual Technol & Syst Integrat Hebe, Qinhuangdao 066004, Hebei, Peoples R China.
EM lihe@stumail.ysu.edu.cn; sshhzz@ysu.edu.cn; whkong@ysu.edu.cn
RI Zhang, Shihui/KFB-3255-2024; Zhang, Shihui/HHS-1779-2022
FU National Natural Science Foundation of China [61379065]; Natural Science
   Foundation of Hebei province in China [F2019203285, 2019203526]; China
   Postdoctoral Science Foundation [2018M631763]; Yanshan University
   Doctoral Foundation [BL18010]
FX This work was supported partly by the National Natural Science
   Foundation of China (No. 61379065), the Natural Science Foundation of
   Hebei province in China (Nos. F2019203285; 2019203526), the Project
   funded by China Postdoctoral Science Foundation (No. 2018M631763) and
   Yanshan University Doctoral Foundation (BL18010)
CR Arteta C, 2016, LECT NOTES COMPUT SC, V9911, P483, DOI 10.1007/978-3-319-46478-7_30
   Boominathan L, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P640, DOI 10.1145/2964284.2967300
   Chen JC, 2016, INT CONF BIOMETR THE
   Chen K., 2012, P BMVC, P1
   Choi JS, 2017, LECT NOTES ELECTR EN, V421, P712, DOI 10.1007/978-981-10-3023-9_109
   Fan CS, 2015, FRONT ARTIF INTEL AP, V274, P1479, DOI 10.3233/978-1-61499-484-8-1479
   Fiaschi L, 2012, INT C PATT RECOG, P2685
   Guerrero-Gómez-Olmedo R, 2015, LECT NOTES COMPUT SC, V9117, P423, DOI 10.1007/978-3-319-19390-8_48
   Idrees H, 2013, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2013.329
   Kingma D. P., 2014, arXiv
   Kumagai S, 2018, MACH VISION APPL, V29, P1119, DOI 10.1007/s00138-018-0955-6
   Lempitsky V., 2010, P ADV NEUR INF PROC, V23, P1, DOI DOI 10.5555/2997189.2997337
   Liu LB, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P849
   Luo HL, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8122367
   Marsden M, 2017, PROCEEDINGS OF THE 12TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2017), VOL 5, P27, DOI 10.5220/0006097300270033
   Mukherjee S, 2015, VISUAL COMPUT, V31, P1405, DOI 10.1007/s00371-014-1022-6
   Oñoro-Rubio D, 2016, LECT NOTES COMPUT SC, V9911, P615, DOI 10.1007/978-3-319-46478-7_38
   Ranjan V, 2018, LECT NOTES COMPUT SC, V11211, P278, DOI 10.1007/978-3-030-01234-2_17
   Rao AS, 2015, VISUAL COMPUT, V31, P1533, DOI 10.1007/s00371-014-1032-4
   Sam DB, 2017, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2017.429
   Sheng BY, 2018, IEEE T CIRC SYST VID, V28, P1788, DOI 10.1109/TCSVT.2016.2637379
   Sindagi V. A., 2017, 2017 14 IEEE INT C A, P1
   Sindagi VA, 2018, PATTERN RECOGN LETT, V107, P3, DOI 10.1016/j.patrec.2017.07.007
   Sossa H, 2003, PROCEEDINGS OF THE FOURTH MEXICAN INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE (ENC 2003), P216, DOI 10.1109/ENC.2003.1232897
   Spampinato C, 2008, VISAPP 2008: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 2, P514
   Pham VQ, 2015, IEEE I CONF COMP VIS, P3253, DOI 10.1109/ICCV.2015.372
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Wang C, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1299, DOI 10.1145/2733373.28063370-12345-67-8/90/01
   Xu B, 2018, IEEE ACM T COMPUT BI, V15, P1797, DOI 10.1109/TCBB.2016.2578337
   Yao H, 2017, ARXIV171009757
   Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
NR 32
TC 9
Z9 9
U1 1
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2020
VL 36
IS 8
BP 1693
EP 1704
DI 10.1007/s00371-019-01769-5
EA OCT 2019
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ML7DA
UT WOS:000492944100001
DA 2024-07-18
ER

PT J
AU Koyama, Y
   Goto, M
AF Koyama, Yuki
   Goto, Masataka
TI Precomputed optimal one-hop motion transition for responsive character
   animation
SO VISUAL COMPUTER
LA English
DT Article
DE Motion transition; Character animation; Motion graphs
AB Characters in interactive 3D applications are often animated by creating transitions from one motion clip to another in response to user input. It is not trivial, however, to achieve quick, natural-looking transitions between two arbitrary motion clips, especially when the two motions are dissimilar. To tackle this problem, we present a simple framework called optimal one-hop motion transition, which creates quick, natural-looking transitions on the fly without requiring careful manual specifications. The key ideas are (1) to insert a short intermediate motion clip, called a hop, between the source and destination motion clips, and (2) to select such a hop motion clip and its temporal alignment in an optimal way by solving a search problem. In the search problem, our framework tries to balance the naturalness of the resulting transitions and the responsiveness to user input. This search can be precomputed and the results can be stored in a lookup table, making the runtime cost to play an optimal transition negligible. We demonstrate that our framework is easily integrated into a widely used game engine, and that it greatly improves the quality of transitions in practical scenarios.
C1 [Koyama, Yuki; Goto, Masataka] Natl Inst Adv Ind Sci & Technol, Cent 2, 1-1-1 Umezono, Tsukuba, Ibaraki 3058568, Japan.
C3 National Institute of Advanced Industrial Science & Technology (AIST)
RP Koyama, Y (corresponding author), Natl Inst Adv Ind Sci & Technol, Cent 2, 1-1-1 Umezono, Tsukuba, Ibaraki 3058568, Japan.
EM koyama.y@aist.go.jp; m.goto@aist.go.jp
FU JST ACCEL Grant, Japan [JPMJAC1602]
FX This work was supported in part by JST ACCEL Grant Number JPMJAC1602,
   Japan.
CR Arikan O, 2002, ACM T GRAPHIC, V21, P483, DOI 10.1145/566570.566606
   Arikan Okan., 2005, SCA 2005: Proceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P59
   Chai JX, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239459
   Chaudhuri Siddhartha, 2013, P 26 ANN ACM S USER, P193, DOI [DOI 10.1145/2501988.2502008, 10.1145/2501988.2502008]
   Clavet S, 2016, GDC 16
   Egbert C, 2010, LECT NOTES COMPUT SC, V6169, P138, DOI 10.1007/978-3-642-14061-7_14
   Epic Games Inc, 2019, STAT MACH
   Garces E, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601131
   Gleicher Michael., 2003, I3D 2003: Proceedings of the 2003 Symposium on Interactive 3D Graphics, P181
   Holden D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073663
   Holden D, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925975
   Ikemoto L, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P145
   Ikemoto Leslie, 2006, P 2006 S INT 3D GRAP, P49
   Iwamoto S, 2017, GDC 17
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Kovar L., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P214
   Kovar Lucas., 2002, SCA 2002: Proceedings of the 2002 ACM SIG-GRAPH/Eurographics Symposium on Computer Animation, P97
   Koyama Y, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173735
   Koyama Yuki, 2014, PROC UIST, P65, DOI [DOI 10.1145/2642918.26473862, 10.1145/2642918.26473862]
   Lau M., 2005, Proceedings of the 2005 ACM SIGGRAPH/Eurographics symposium on Computer animation, SCA '05, P271
   Lau Manfred., 2006, P ACM SIGGRAPHEUROGR, P299
   Laursen L. F., 2016, PARASITOLOGY, P1, DOI [DOI 10.2312/PG.20161326, 10.2312/pg.20161326]
   Lee JH, 2002, ACM T GRAPHIC, V21, P491
   Lee Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866160
   Levine S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185524
   Liu TQ, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766898
   Min JY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366172
   Min JY, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640452
   Mizuguchi Mark, 2001, EURO 2001, V2, DOI [10.2312/egs.20011039, DOI 10.2312/EGS.20011039]
   Mukai T, 2005, ACM T GRAPHIC, V24, P1062, DOI 10.1145/1073204.1073313
   Pullen K, 2002, ACM T GRAPHIC, V21, P501
   Reitsma PSA, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1289603.1289609
   Rose C., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P147, DOI 10.1145/237170.237229
   Safonova A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239557
   Unity Technologies, 2018, UN STAT MACH BAS, V2018. 3-002P
   Unity Technologies, 2018, UN AN TRANS, V2018. 3-002
   WANG J, 2001, ACM T GRAPHIC, V27, p1:1
   Zadziuk K, 2016, GDC 16
NR 38
TC 3
Z9 3
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 1131
EP 1142
DI 10.1007/s00371-019-01693-8
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200029
OA hybrid
DA 2024-07-18
ER

PT J
AU Liu, YJ
   Yu, D
   Chen, XM
   Li, ZM
   Fan, JP
AF Liu, Yujie
   Yu, Deng
   Chen, Xiaoming
   Li, Zongmin
   Fan, Jianping
TI TOP-SIFT: the selected SIFT descriptor based on dictionary learning
SO VISUAL COMPUTER
LA English
DT Article
DE SIFT descriptor selection; Dictionary learning; Sparse coding; Feature
   compression
AB The large amount of SIFT descriptors in an image and the high dimensionality of SIFT descriptor have made problems for the large-scale image database in terms of speed and scalability. In this paper, we present a descriptor selection algorithm based on dictionary learning to remove the redundant features and reserve only a small set of features, which we refer to as TOP-SIFTs. During the experiment, we discovered the inner relativity between the problem of descriptor selection and dictionary learning in sparse representation, and then turned our problem into dictionary learning. We designed a new dictionary learning method to adapt our problem and employed the simulated annealing algorithm to obtain the optimal solution. During the process of learning, we added the sparsity constraint and spatial distribution characteristic of SIFT points. And lastly selected the small representative feature set with good spatial distribution. Compared with the earlier methods, our method is neither relying on the database nor losing important information, and the experiments have shown that our algorithm can save memory space a lot and increase time efficiency while maintaining the accuracy as well.
C1 [Liu, Yujie; Yu, Deng; Chen, Xiaoming; Li, Zongmin] China Univ Petr, Coll Comp & Commun Engn, Qingdao, Shandong, Peoples R China.
   [Fan, Jianping] Univ N Carolina, Dept Comp Sci, Charlotte, NC USA.
C3 China University of Petroleum; University of North Carolina; University
   of North Carolina Charlotte
RP Yu, D (corresponding author), China Univ Petr, Coll Comp & Commun Engn, Qingdao, Shandong, Peoples R China.
EM liuyujie@upc.edu.cn; dengyucn@gmail.com
RI chen, xia/GYR-3948-2022; chen, xia/GXM-5435-2022; chen,
   xin/IQW-3432-2023; Chen, Xiao/JBJ-7561-2023; chen, xi/GXH-3653-2022
OI Chen, Xiao/0000-0002-9797-8384; YU, Deng/0000-0003-2343-3688
FU National Natural Science Foundation of China [61379106, 61379082,
   61227802]; Shandong Provincial Natural Science Foundation [ZR2013FM036,
   ZR2015FM011, ZR2015FM022]
FX This work is partly supported by National Natural Science Foundation of
   China (Grant Nos. 61379106, 61379082, 61227802) and the Shandong
   Provincial Natural Science Foundation (Grant Nos. ZR2013FM036,
   ZR2015FM011, ZR2015FM022).
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], 2013, LARGE SCALE DENSE 3D
   Bao SY, 2013, PROC CVPR IEEE, P1264, DOI 10.1109/CVPR.2013.167
   Brown M, 2005, PROC CVPR IEEE, P510
   Dash M, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P115, DOI 10.1109/ICDM.2002.1183893
   Dean T, 2013, PROC CVPR IEEE, P1814, DOI 10.1109/CVPR.2013.237
   Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100
   Engan K, 1999, ISCAS '99: PROCEEDINGS OF THE 1999 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 4, P1, DOI 10.1109/ISCAS.1999.779928
   Foo J., 2007, P 18 C AUSTRALASIAN, P63
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Johnson M. K., 2006, TECHNICAL REPORT
   KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671
   Knopp J, 2010, LECT NOTES COMPUT SC, V6311, P748, DOI 10.1007/978-3-642-15549-9_54
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Lee YJ, 2009, INT J COMPUT VISION, V85, P143, DOI 10.1007/s11263-009-0252-y
   Li Y, 2016, VISUAL COMPUT, V32, P1525, DOI 10.1007/s00371-015-1137-4
   Liu YJ, 2015, 2015 1ST IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM), P236, DOI 10.1109/BigMM.2015.34
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Nister David, 2006, CVPR
   Nowak E, 2006, LECT NOTES COMPUT SC, V3954, P490
   Sadeghi MA, 2008, VISAPP 2008: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P275
   Turcot Panu, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P2109, DOI 10.1109/ICCVW.2009.5457541
   Xu W, 2010, PROC CVPR IEEE, P263, DOI 10.1109/CVPR.2010.5540202
   Yang CL, 2013, PATTERN RECOGN, V46, P948, DOI 10.1016/j.patcog.2012.07.011
   Yasseen Z, 2017, VISUAL COMPUT, V33, P565, DOI 10.1007/s00371-016-1328-7
   Zhan J, 2015, VISUAL COMPUT, V31, P575, DOI 10.1007/s00371-014-0984-8
   Zhou N, 2014, IEEE T PATTERN ANAL, V36, P715, DOI 10.1109/TPAMI.2013.189
NR 29
TC 14
Z9 14
U1 3
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2019
VL 35
IS 5
BP 667
EP 677
DI 10.1007/s00371-018-1502-1
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HZ0IT
UT WOS:000468524900005
DA 2024-07-18
ER

PT J
AU Wang, R
   Liang, Y
   Xu, JW
   He, ZH
AF Wang, Rui
   Liang, Ying
   Xu, Jing Wen
   He, Zhi Hai
TI Cascading classifier with discriminative multi-features for a specific
   3D object real-time detection
SO VISUAL COMPUTER
LA English
DT Article
DE Specific 3D object detection; Candidate extraction; Candidate region
   recognition; Discriminative multi-features; Cascaded classifiers
ID SCALE; GRADIENTS
AB Real-time specific 3D object detection plays an important role in intelligent service robots and intelligent surveillance fields. Compared to most existing approaches, which use simple template-matching methods, we present a novel discriminative learning-based method referred to as B-CST (BING - Colour + Shape + Texture) to detect a specific 3D object from a video in real time. Instead of the sliding-window technique, an original candidate extraction strategy is proposed, and that a new cascade classifier for recognition is also developed. In the candidate extraction stage, the rapid and high-quality objectness measure, binarised normed gradients, is modified to highlight the target candidate regions as well as to suppress undesirable background regions. In the recognition stage, each candidate region is then verified and further classified into different categories, which are denoted as positive, including multi-view images of target, or negative. The designed cascade classifiers conduct the recognition with discriminative multiple features, i.e. the novel dominant colour histogram, the histogram of oriented gradients and the original Gabor-CS-LTP feature, which is the centre-symmetric local ternary pattern of a special Gabor magnitude mapping. We evaluate our proposed method on our challenging new dataset consisting of 5 objects and two well-known public datasets and then compare it with other detection techniques for a single 3D object. A comparative study shows that our B-CST method is efficient in both high-quality detection results and detection speed, which can achieve the real-time processing requirements of video sequences (approximately 23fps).
C1 [Wang, Rui; Liang, Ying; Xu, Jing Wen] Beihang Univ, Sch Instrumentat Sci & Optoelect Engn, Lab Precis Optomechatron Technol, 37 Xueyuan Rd, Beijing 100191, Peoples R China.
   [He, Zhi Hai] Univ Missouri, Dept Elect Engn & Comp Sci, Columbia, MO 65203 USA.
C3 Beihang University; University of Missouri System; University of
   Missouri Columbia
RP Wang, R (corresponding author), Beihang Univ, Sch Instrumentat Sci & Optoelect Engn, Lab Precis Optomechatron Technol, 37 Xueyuan Rd, Beijing 100191, Peoples R China.
EM wangr@buaa.edu.cn
RI xu, jingcheng/HJZ-3124-2023; xu, jing/GRR-8698-2022
FU National Natural Science Foundation of China [61673039]
FX The authors thank the anonymous reviewers for their assistance. This
   work was supported by a grant from the National Natural Science
   Foundation of China (61673039).
CR [Anonymous], 2012, BMVC
   Arthur D, 2015, P 18 ACM SIAM S DISC, V11, P1027
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Darom T, 2012, IEEE T IMAGE PROCESS, V21, P2758, DOI 10.1109/TIP.2012.2183142
   Feng C., 2014, J HEFEI U TECHNOL NA, P1456
   Guo Y, 2018, VISUAL COMPUT, V34, P1325, DOI 10.1007/s00371-017-1416-3
   Guo YL, 2016, INT J COMPUT VISION, V116, P66, DOI 10.1007/s11263-015-0824-y
   Guo YL, 2014, IEEE T PATTERN ANAL, V36, P2270, DOI 10.1109/TPAMI.2014.2316828
   Guo YL, 2013, INT J COMPUT VISION, V105, P63, DOI 10.1007/s11263-013-0627-y
   Hao-Yuan Kuo, 2014, 2014 IEEE International Conference on Automation Science and Engineering (CASE), P1264, DOI 10.1109/CoASE.2014.6899489
   Lai K, 2011, IEEE INT CONF ROBOT, P1817
   Li T, 2013, VISUAL COMPUT, V30, P5969
   Liu L, 2015, IEEE IMAGE PROC, P2319, DOI 10.1109/ICIP.2015.7351216
   Liu WF, 2006, INT C PATT RECOG, P536
   Özuysal M, 2009, PROC CVPR IEEE, P778, DOI 10.1109/CVPRW.2009.5206633
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Pepik B, 2015, IEEE T PATTERN ANAL, V37, P2232, DOI 10.1109/TPAMI.2015.2408347
   Rios-Cabrera R, 2013, IEEE I CONF COMP VIS, P2048, DOI 10.1109/ICCV.2013.256
   Rios-Cabrera R, 2014, COMPUT VIS IMAGE UND, V120, P103, DOI 10.1016/j.cviu.2013.12.008
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   Rui W., 2016, 2016 INT C INF SYST
   Strum P, 2012, IEEE T PATTERN ANAL, V34, P876, DOI [10.1109/TPAMI.2011.206, DOI 10.1109/TPAMI.2011.206]
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Tang YL, 2016, VISUAL COMPUT, V32, P111, DOI 10.1007/s00371-014-1059-6
   Tejani A, 2014, LECT NOTES COMPUT SC, V8694, P462, DOI 10.1007/978-3-319-10599-4_30
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang R, 2016, VISUAL COMPUT, V32, P1379, DOI 10.1007/s00371-015-1206-8
   Yang HC, 2016, J ALGORITHMS COMPUT, V10, P187, DOI 10.1177/1748301816649073
   Zhang C., 2011, Multiple-instance pruning for learning efficient cascade detectors, Patent No. [US 8,010,471 B2, 8010471]
   Zheng Y., 2010, COMPUTER VISION PATT
NR 33
TC 5
Z9 5
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2019
VL 35
IS 3
BP 399
EP 414
DI 10.1007/s00371-018-1472-3
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HP0MR
UT WOS:000461360600008
DA 2024-07-18
ER

PT J
AU Li, YQ
   Wang, C
   Zhao, JY
   Yuan, QS
AF Li, Yuqi
   Wang, Chong
   Zhao, Jieyu
   Yuan, Qingshu
TI Efficient spectral reconstruction using a trichromatic camera via sample
   optimization
SO VISUAL COMPUTER
LA English
DT Article
DE Spectral reconstruction; Sample optimization; Multispectral images
ID REFLECTANCE SPECTRA; SELECTION; RECOVERY
AB Training-based multispectral reconstruction can effectively recover spectral reflectance of captured objects using a trichromatic camera. However, existing methods are based on synthesized data, and the sizes of training sample set (e.g., multispectral images, reflectance targets) are usually large. In this paper, we present a spectral reconstruction approach using real measured data. To improve the efficiency and accuracy of spectral reconstruction, we propose a volume maximization method for sample optimization without any prior knowledge of light and cameras. We use heuristic global search algorithms to optimize samples and give an efficient spectral reconstruction method which is suitable for sparse sampling. Experimental results show that the proposed sample selection method outperforms other existing methods in terms of both spectral and colorimetric reconstruction errors. Moreover, the proposed reconstruction method achieves higher efficiency and accuracy due to lower sample redundancy.
C1 [Li, Yuqi; Wang, Chong; Zhao, Jieyu] Ningbo Univ, Coll Informat Sci & Engn, 818 Fenghua Rd, Ningbo 315211, Zhejiang, Peoples R China.
   [Li, Yuqi] Zhejiang Univ, Coll Comp Sci & Technol, 38 Zheda Rd, Ningbo 310000, Zhejiang, Peoples R China.
   [Yuan, Qingshu] Hangzhou Normal Univ, Hangzhou Inst Serv Engn, Hangzhou 310000, Zhejiang, Peoples R China.
C3 Ningbo University; Zhejiang University; Hangzhou Normal University
RP Li, YQ (corresponding author), Ningbo Univ, Coll Informat Sci & Engn, 818 Fenghua Rd, Ningbo 315211, Zhejiang, Peoples R China.; Li, YQ (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, 38 Zheda Rd, Ningbo 310000, Zhejiang, Peoples R China.
EM liyuqi1@nbu.edu.cn; wangchong@nbu.edu.cn; zhaojieyu@nbu.edu.cn;
   yuanqs@gmail.com
RI Wang, Chong/IRZ-7328-2023
OI Li, Yuqi/0000-0003-0228-242X
FU National Natural Science Foundation of China [61602268, 61603202,
   61571247]; National Natural Science Foundation of Zhejiang Province
   [LY13F020050, LZ16F030001]; K.C.Wong Magna Fund in Ningbo University
FX The work in this paper has been supported by the National Natural
   Science Foundation of China (Grant Nos. 61602268, 61603202, 61571247),
   the National Natural Science Foundation of Zhejiang Province (Nos.
   LY13F020050, LZ16F030001), and the K.C.Wong Magna Fund in Ningbo
   University.
CR Agahian F, 2008, COLOR RES APPL, V33, P360, DOI 10.1002/col.20431
   Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Alvarez-Cortes S, 2016, COMPUT GRAPH FORUM, V35, P166, DOI 10.1111/cgf.12717
   [Anonymous], 1999, INTRO GENETIC ALGORI, DOI DOI 10.1016/50898-1221(96)90227-8.FIFTH
   Arad B, 2016, LECT NOTES COMPUT SC, V9911, P19, DOI 10.1007/978-3-319-46478-7_2
   Brill M.H., 2002, Color Res. Appl, V27, P304, DOI [10.1002/col.10073, DOI 10.1002/COL.10073, DOI 10.1002/C0L.10073]
   Brown M, 2011, PROC CVPR IEEE, P177, DOI 10.1109/CVPR.2011.5995637
   Carroll R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964938
   Cheung V, 2006, J IMAGING SCI TECHN, V50, P481, DOI [10.2352/J.ImagingSci.Technol.(2006)50:5(481), 10.2352/J.lmagingSci.Technol.(2006)50:5(481)]
   Çivril A, 2009, THEOR COMPUT SCI, V410, P4801, DOI 10.1016/j.tcs.2009.06.018
   Fu Y, 2016, PROC CVPR IEEE, P3727, DOI 10.1109/CVPR.2016.405
   Han S, 2014, INT J COMPUT VISION, V110, P172, DOI 10.1007/s11263-013-0687-z
   Heikkinen V, 2007, J OPT SOC AM A, V24, P2673, DOI 10.1364/JOSAA.24.002673
   Heikkinen V, 2016, J OPT SOC AM A, V33, P1095, DOI 10.1364/JOSAA.33.001095
   IIZUKA S, 2016, ACM T GRAPHIC, V35, DOI DOI 10.1145/2897824.2925974
   Jiang J., CAMERA SPECTRAL SENS
   Kalantari N. K., 2017, ACM Trans. Graph., V36, DOI DOI 10.1145/3072959.3073609
   Kim S, 2017, IEEE T PATTERN ANAL, V39, P1712, DOI 10.1109/TPAMI.2016.2615619
   Lan YX, 2013, VISUAL COMPUT, V29, P773, DOI 10.1007/s00371-013-0829-x
   Lee MH, 2012, OPT LETT, V37, P1937, DOI 10.1364/OL.37.001937
   Li YQ, 2015, COMPUT GRAPH FORUM, V34, P337, DOI 10.1111/cgf.12564
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Mansouri A, 2008, COLOR RES APPL, V33, P485, DOI 10.1002/col.20442
   Mohammadi M, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P59
   Nalbach O, 2017, COMPUT GRAPH FORUM, V36, P409, DOI 10.1111/cgf.13136
   Nguyen RMH, 2014, LECT NOTES COMPUT SC, V8695, P186, DOI 10.1007/978-3-319-10584-0_13
   Oh SW, 2016, PROC CVPR IEEE, P2461, DOI 10.1109/CVPR.2016.270
   Park IC, 2007, PR IEEE COMP DESIGN, P1, DOI 10.1109/ICCD.2007.4601872
   PARKKINEN JPS, 1989, J OPT SOC AM A, V6, P318, DOI 10.1364/JOSAA.6.000318
   Pati YC, 1993, SIGN SYST COMP 1993, P40, DOI DOI 10.1109/ACSSC.1993.342465
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Shen HL, 2008, APPL OPTICS, V47, P2494, DOI 10.1364/AO.47.002494
   Shen HL, 2014, APPL OPTICS, V53, P634, DOI 10.1364/AO.53.000634
   *U JOENS COL GROUP, SPECTR DAT
   Wagadarikar A, 2008, APPL OPTICS, V47, pB44, DOI 10.1364/AO.47.000B44
   Yasuma F, 2010, IEEE T IMAGE PROCESS, V19, P2241, DOI 10.1109/TIP.2010.2046811
   Zhang LH, 2016, LASER PHYS LETT, V13, DOI 10.1088/1612-2011/13/9/095201
   Zhang Q., 2014, INT J IND SYSTEMS EN, V18, P185
   Zhang WF, 2011, OPT LETT, V36, P3933, DOI 10.1364/OL.36.003933
NR 39
TC 11
Z9 11
U1 1
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2018
VL 34
IS 12
BP 1773
EP 1783
DI 10.1007/s00371-017-1469-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GY3WM
UT WOS:000448487400013
DA 2024-07-18
ER

PT J
AU Li, F
   Xiong, Y
AF Li, Fan
   Xiong, Yin
TI Automatic identification of butterfly species based on HoMSC and GLCMoIB
SO VISUAL COMPUTER
LA English
DT Article
DE Automatic identification; Butterfly species; Histograms of multi-scale
   curvature; Gray-level co-occurrence matrix of image blocks
ID ARTIFICIAL NEURAL-NETWORK; MULTIFOCUS IMAGE FUSION; EXTREME
   LEARNING-MACHINE; TEXTURE FEATURES; VISION SYSTEM; TAXONOMISTS; SCHEME;
   COLOR
AB Automatic identification is an efficient technology for the identification of butterfly species and pest control. As the major agriculture and forest pest, butterflies can be accurately classified based on the taxonomic characters. However, such identification can be only achieved by a few insect experts with years of experience. In the study, the shape and texture of butterflies were investigated for the automatic identification of butterfly species in the digital images: Histograms of multi-scale curvature (HoMSC) and gray-level co-occurrence matrix of image blocks (GLCMoIB) were used to describe the shape and texture of butterfly wings, respectively. To get an accurate identification result, a weight-based k-nearest neighbor classifier was designed. In addition, 750 images of 50 butterfly species were used for the identification test. The accuracy rate of this automatic identification method reached 98%. The result suggested that the HoMSC and GLCMoIB features can be efficient for the identification of butterfly species.
C1 [Li, Fan] Kunming Univ Sci & Technol, Fac Informat Engn & Automat, Kunming, Yunnan, Peoples R China.
   [Xiong, Yin] Kunming Univ Sci & Technol, Fac Life Sci & Technol, Kunming, Yunnan, Peoples R China.
C3 Kunming University of Science & Technology; Kunming University of
   Science & Technology
RP Li, F (corresponding author), Kunming Univ Sci & Technol, Fac Informat Engn & Automat, Kunming, Yunnan, Peoples R China.
EM 478263823@qq.com
RI Xiong, Yin/ABY-4186-2022; Xiong, Yin/AAQ-6850-2020; 李, 凡/GZN-1131-2022
OI Xiong, Yin/0000-0001-6540-8267; Li, Fan/0000-0002-3883-0275
CR [Anonymous], 2002, THESIS
   Arbuckle T., 2002, P 15 INT S INF ENV P, P425
   Ertugrul OF., 2015, INT J BIOMED DATA MI, V1, P16
   Gaston KJ, 2004, PHILOS T R SOC B, V359, P655, DOI 10.1098/rstb.2003.1442
   GASTON KJ, 1992, NATURE, V356, P281, DOI 10.1038/356281a0
   Hopkins GW, 2002, ANIM CONSERV, V5, P245, DOI 10.1017/S1367943002002299
   Huang KY, 2007, COMPUT ELECTRON AGR, V57, P3, DOI 10.1016/j.compag.2007.01.015
   Jianwei Z, 2006, AUTOMATIC IDENTIFICA
   Kang SH, 2014, J ASIA-PAC ENTOMOL, V17, P143, DOI 10.1016/j.aspen.2013.12.004
   Kang SH, 2012, J ASIA-PAC ENTOMOL, V15, P437, DOI 10.1016/j.aspen.2012.05.005
   Kaya Y, 2013, TEM J, V2, P13
   Kaya Y, 2015, APPL SOFT COMPUT, V28, P132, DOI 10.1016/j.asoc.2014.11.046
   Kaya Y, 2014, J EXP THEOR ARTIF IN, V26, P267, DOI 10.1080/0952813X.2013.861875
   Kaya Y, 2014, VISUAL COMPUT, V30, P71, DOI 10.1007/s00371-013-0782-8
   Kayci L, 2014, ZOOL MIDDLE EAST, V60, P57, DOI 10.1080/09397140.2014.892340
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3_36
   Li HF, 2017, SIGNAL PROCESS, V138, P71, DOI 10.1016/j.sigpro.2017.03.008
   Li HF, 2016, SIGNAL PROCESS, V128, P474, DOI 10.1016/j.sigpro.2016.05.015
   Li HF, 2016, INFORM SCIENCES, V349, P25, DOI 10.1016/j.ins.2016.02.030
   Li HF, 2016, NEUROCOMPUTING, V171, P138, DOI 10.1016/j.neucom.2015.06.035
   Li HF, 2013, MACH VISION APPL, V24, P1167, DOI 10.1007/s00138-013-0502-4
   Manay S, 2006, IEEE T PATTERN ANAL, V28, P1602, DOI 10.1109/TPAMI.2006.208
   Wang JN, 2012, BIOSYST ENG, V111, P24, DOI 10.1016/j.biosystemseng.2011.10.003
   Weeks PJD, 1999, IMAGE VISION COMPUT, V17, P861, DOI 10.1016/S0262-8856(98)00161-9
   Weeks PJD, 1999, J APPL ENTOMOL, V123, P1, DOI 10.1046/j.1439-0418.1999.00307.x
   Yao Q, 2012, J INTEGR AGR, V11, P978, DOI 10.1016/S2095-3119(12)60089-6
   Yao Z, 1999, MONOGRAPHIA RHOPALOC
NR 27
TC 19
Z9 22
U1 2
U2 37
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2018
VL 34
IS 11
BP 1525
EP 1533
DI 10.1007/s00371-017-1426-1
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GV9ZR
UT WOS:000446521700005
DA 2024-07-18
ER

PT J
AU Chamasemani, FF
   Affendey, LS
   Mustapha, N
   Khalid, F
AF Chamasemani, Fereshteh Falah
   Affendey, Lilly Suriani
   Mustapha, Norwati
   Khalid, Fatimah
TI Video abstraction using density-based clustering algorithm
SO VISUAL COMPUTER
LA English
DT Article
DE Density-based clustering; Video abstraction; Static video summary; Video
   summarization; Video analysis
ID REPRESENTATION
AB The exponential growth in the number of surveillance videos makes the search and retrieval of their contents an extensive, time-consuming, and tedious task. Video abstraction is a general solution to alleviate this problem by generating a short and concise version of the original video. The existing abstraction approaches have commonly relied on global characteristics of visual content and neglected the local details of video frames. This paper presents an enhanced video abstraction approach called Density-based Surveillance video abstraction (DbSva) to generate a static short-length video. The novelty of DbSva is (a) to integrate the advantages of both the global and local features of video contents by fusion and (b) to employ the DENsity-based CLUstEring algorithm (DENCLUE) to significantly improve the quality of abstract videos. Utilizing fusion and the DENCLUE algorithm resulted in the extraction of more informative parts of the videos and increased the robustness of the proposed approach to handle large-scale and noisy videos with no further tuning of the input parameters. A number of qualitative and quantitative experiments support the effectiveness of the proposed approach in generating higher-quality abstract videos compared to the other approaches.
C1 [Chamasemani, Fereshteh Falah; Affendey, Lilly Suriani; Mustapha, Norwati; Khalid, Fatimah] Univ Putra Malaysia, Fac Comp Sci & Informat Technol, Serdang, Selangor, Malaysia.
C3 Universiti Putra Malaysia
RP Chamasemani, FF (corresponding author), Univ Putra Malaysia, Fac Comp Sci & Informat Technol, Serdang, Selangor, Malaysia.
EM F_Falah@hotmail.com
RI Chamasemani, Fereshteh Falah/B-8376-2019
OI Khalid, Fatimah/0000-0002-5791-065X
CR Ajmal M, 2012, LECT NOTES COMPUT SC, V7594, P1, DOI 10.1007/978-3-642-33564-8_1
   Almeida J, 2012, PATTERN RECOGN LETT, V33, P397, DOI 10.1016/j.patrec.2011.08.007
   [Anonymous], INT C PERV COMP
   Arivazhagan S, 2003, PATTERN RECOGN LETT, V24, P3197, DOI 10.1016/j.patrec.2003.08.005
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Belo L, 2014, PROC INT C TOOLS ART, P822, DOI 10.1109/ICTAI.2014.127
   Cahuina E. J. Y. C., GRAPH PATT IM SIBGRA, P226
   Chamasemani F. F., 2015, RES J APPL SCI ENG T, V10, P1316
   Chamasemani FF, 2015, PROCEEDINGS 5TH IEEE INTERNATIONAL CONFERENCE ON CONTROL SYSTEM, COMPUTING AND ENGINEERING (ICCSCE 2015), P470, DOI 10.1109/ICCSCE.2015.7482231
   Chiang CC, 2015, MULTIMED TOOLS APPL, V74, P2861, DOI 10.1007/s11042-013-1750-z
   Damnjanovic Uros, 2008, 2008 Ninth International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS), P63, DOI 10.1109/WIAMIS.2008.53
   DeMenthon D., 1998, Proceedings ACM Multimedia 98, P211, DOI 10.1145/290747.290773
   Dogra DP, 2016, MULTIMED TOOLS APPL, V75, P6373, DOI 10.1007/s11042-015-2576-7
   Dong P, 2010, LECT NOTES COMPUT SC, V6297, P203, DOI 10.1007/978-3-642-15702-8_19
   Ejaz N, 2012, J VIS COMMUN IMAGE R, V23, P1031, DOI 10.1016/j.jvcir.2012.06.013
   Elkhattabi Z., 2015, INT J COMPUTER ELECT, V100, P784
   Fierro-Radilla AN, 2013, 2013 10TH INTERNATIONAL CONFERENCE ON ELECTRICAL ENGINEERING, COMPUTING SCIENCE AND AUTOMATIC CONTROL (CCE), P233, DOI 10.1109/ICEEE.2013.6676028
   de Avila SEF, 2011, PATTERN RECOGN LETT, V32, P56, DOI 10.1016/j.patrec.2010.08.004
   Furini M, 2010, MULTIMED TOOLS APPL, V46, P47, DOI 10.1007/s11042-009-0307-7
   Guimaraes SJF, 2010, LECT NOTES COMPUT SC, V6419, P46
   Gupta A, 2015, 2015 2ND INTERNATIONAL CONFERENCE ON COMPUTING FOR SUSTAINABLE GLOBAL DEVELOPMENT (INDIACOM), P2097
   Henk M. B., 2007, DATA CENTRIC SYSTEMS
   Hinneburg A., 2003, Knowledge and Information Systems, V5, P387, DOI 10.1007/s10115-003-0086-9
   Hinneburg A., 1998, Proceedings Fourth International Conference on Knowledge Discovery and Data Mining, P58
   Hinneburg A, 2007, LECT NOTES COMPUT SC, V4723, P70
   Jain R., 1991, ART SYSTEMS PERFORMA
   Ji QG, 2013, SIGNAL PROCESS-IMAGE, V28, P241, DOI 10.1016/j.image.2012.11.008
   Ji Z., 2010, SIGN PROC SYST ICSPS
   Kogler M., 2009, P 10 INT WORKSH MULT
   Li J, 2007, 2007 IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P546, DOI 10.1109/AVSS.2007.4425369
   Li XL, 2016, IEEE T IMAGE PROCESS, V25, P740, DOI 10.1109/TIP.2015.2507942
   Ma X, 2010, STUD COMPUT INTELL, V287, P53
   Mahmoud KM, 2013, LECT NOTES COMPUT SC, V8156, P733
   Mei T, 2009, VISUAL COMPUT, V25, P39, DOI 10.1007/s00371-008-0282-4
   Mendi E, 2013, COMPUT ELECTR ENG, V39, P790, DOI 10.1016/j.compeleceng.2012.11.020
   Mukane S. M., 2011, INT J COMPUT APPL, V18, P10
   Pritch Y, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P195, DOI 10.1109/AVSS.2009.53
   Ruger Stefan, 2009, SYNTHESIS LECT INFOR, V1, P1, DOI DOI 10.2200/S00244ED1V01Y200912ICR010
   Sabbar W, 2012, 2012 SECOND INTERNATIONAL CONFERENCE ON INNOVATIVE COMPUTING TECHNOLOGY (INTECH), P190, DOI 10.1109/INTECH.2012.6457809
   Shao H, 2008, PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE FOR YOUNG COMPUTER SCIENTISTS, VOLS 1-5, P753
   Strandmark P., 2010, SURFMEX MATLAB SURF
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Yi Y, 2018, VISUAL COMPUT, V34, P391, DOI 10.1007/s00371-016-1345-6
   Yu XD, 2006, VISUAL COMPUT, V22, P357, DOI 10.1007/s00371-006-0013-7
   Zhang L, 2014, VISUAL COMPUT, V30, P1123, DOI 10.1007/s00371-013-0882-5
   Zhao HL, 2009, VISUAL COMPUT, V25, P973, DOI 10.1007/s00371-008-0308-y
   Zhuang Y., 1998, IM PROC 1998 ICIP 98
NR 48
TC 4
Z9 4
U1 0
U2 34
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2018
VL 34
IS 10
BP 1299
EP 1314
DI 10.1007/s00371-017-1432-3
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GR0KK
UT WOS:000442204400003
DA 2024-07-18
ER

PT J
AU Tsuzuki, MDG
   Sato, AK
   Ueda, EK
   Martins, TD
   Takimoto, RY
   Iwao, Y
   Abe, LI
   Gotoh, T
   Kagei, S
AF Guerra Tsuzuki, Marcos de Sales
   Sato, Andre Kubagawa
   Ueda, Edson Kenji
   Martins, Thiago de Castro
   Takimoto, Rogerio Yugo
   Iwao, Yuma
   Abe, Leonardo Ishida
   Gotoh, Toshiyuki
   Kagei, Seiichiro
TI Propagation-based marching cubes algorithm using open boundary loop
SO VISUAL COMPUTER
LA English
DT Article
DE Marching cubes algorithm; Propagation-based approach; Open boundary
ID ISOSURFACE GENERATION
AB The marching cubes (MC) algorithm is employed to generated triangular meshes for visualizing medical images, sculpture scans and mathematical surfaces. It sequentially traverses cuberille data composed of sampled points of a scalar volumetric data. This paper proposes a propagation-based MC algorithm that uses the open boundary loop concept. The open boundary loop is used to determine adjacent cells for the next iteration of the MC algorithm. After inserting each triangle, the open boundary loop is reevaluated. Simultaneously, it is ensured that all triangles are coherently oriented and there are no holes on the isosurface. Several tests are conducted to determine the performance of the algorithm in comparison with the original MC algorithm. Results from these tests indicate that, for large-scale problems, the proposed algorithm performs better than the original.
C1 [Guerra Tsuzuki, Marcos de Sales; Sato, Andre Kubagawa; Ueda, Edson Kenji; Martins, Thiago de Castro; Takimoto, Rogerio Yugo] Univ Sao Paulo, Escola Politecn, Dept Mechatron & Mech Syst Engn, Computat Geometry Lab, Av Prof Mello Moraes 2231, Sao Paulo, SP, Brazil.
   [Iwao, Yuma; Abe, Leonardo Ishida; Gotoh, Toshiyuki; Kagei, Seiichiro] Yokohama Natl Univ, Hodogaya Ku, 79-1 Tokiwadai, Yokohama, Kanagawa 2408501, Japan.
C3 Universidade de Sao Paulo; Yokohama National University
RP Tsuzuki, MDG (corresponding author), Univ Sao Paulo, Escola Politecn, Dept Mechatron & Mech Syst Engn, Computat Geometry Lab, Av Prof Mello Moraes 2231, Sao Paulo, SP, Brazil.
EM mtsuzuki@usp.br
RI Sato, Andre/AAG-9318-2019; Martins, Thiago C/H-4339-2012; Takimoto,
   Rogerio Y/H-5601-2012; SATO, ANDRE/AAW-8746-2021; Tsuzuki,
   Marcos/E-8137-2012
OI Sato, Andre/0000-0002-3352-1318; Tsuzuki, Marcos/0000-0002-8495-2337;
   Takimoto, Rogerio/0000-0002-5798-6160; Ueda, Edson
   Kenji/0000-0003-1118-7328
CR [Anonymous], 2013, Isosurfaces: geometry, topology, and algorithms
   Bajaj CL, 1996, 1996 SYMPOSIUM ON VOLUME VISUALIZATION, PROCEEDINGS, P39, DOI 10.1109/SVV.1996.558041
   Chernyaev E., 1995, TECH REP
   Desbrun, 2007, P 5 EUROGRAPHICS S G, V7, P39
   Galin E, 2000, GRAPH MODELS, V62, P19, DOI 10.1006/gmod.1999.0514
   Itoh T, 2001, IEEE T VIS COMPUT GR, V7, P32, DOI 10.1109/2945.910819
   Itoh T, 1995, IEEE T VIS COMPUT GR, V1, P319, DOI 10.1109/2945.485619
   Iwao Y., 2012, IFAC P VOLUMES IFAC, V45, P85
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Lewiner T, 2003, J GRAPH TOOLS, V8, P2003
   Lorensen W. E., 1987, COMPUTER GRAPHICS, V21, P163, DOI 10.1145/37401.37422
   Lwao Y, 2014, BIOMED SIGNAL PROCES, V12, P28, DOI 10.1016/j.bspc.2013.10.005
   Meyer M, 2007, IEEE T VIS COMPUT GR, V13, P1704, DOI 10.1109/TVCG.2007.70604
   Newman TS, 2006, COMPUT GRAPH-UK, V30, P854, DOI 10.1016/j.cag.2006.07.021
   Schaefer S, 2007, IEEE T VIS COMPUT GR, V13, P610, DOI 10.1109/TVCG.2007.1012
   Shekhar R, 1996, IEEE VISUAL, P335, DOI 10.1109/VISUAL.1996.568127
   Shen HW, 1995, VISUALIZATION '95 - PROCEEDINGS, P143, DOI 10.1109/VISUAL.1995.480806
   Van Kreveld M., 1997, Proceedings of the Thirteenth Annual Symposium on Computational Geometry, P212, DOI 10.1145/262839.269238
   Weber GH, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P171, DOI 10.1109/VISUAL.2002.1183772
   WILHELMS J, 1992, ACM T GRAPHIC, V11, P201, DOI 10.1145/130881.130882
NR 20
TC 6
Z9 6
U1 1
U2 13
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2018
VL 34
IS 10
BP 1339
EP 1355
DI 10.1007/s00371-017-1417-2
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GR0KK
UT WOS:000442204400006
DA 2024-07-18
ER

PT J
AU Lu, FX
   Zhou, B
   Zhang, Y
   Zhao, QP
AF Lu, Feixiang
   Zhou, Bin
   Zhang, Yu
   Zhao, Qinping
TI Real-time 3D scene reconstruction with dynamically moving object using a
   single depth camera
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Real-time 3D scene reconstruction; Moving object; Single depth camera;
   Joint segmentation and reconstruction; Sequential 6D pose prediction
ID INTEGRATION
AB Online 3D reconstruction of real-world scenes has been attracting increasing interests from both the academia and industry, especially with the consumer-level depth cameras becoming widely available. Recent most online reconstruction systems take live depth data from a moving Kinect camera and incrementally fuse them to a single high-quality 3D model in real time. Although most real-world scenes have static environment, the daily objects in a scene often move dynamically, which are non-trivial to reconstruct especially when the camera is also not still. To solve this problem, we propose a single depth camera-based real-time approach for simultaneous reconstruction of dynamic object and static environment, and provide solutions for its key issues. In particular, we first introduce a robust optimization scheme which takes advantage of raycasted maps to segment moving object and background from the live depth map. The corresponding depth data are then fused to the volumes, respectively. These volumes are raycasted to extract views of the implicit surface which can be used as a consistent reference frame for the next iteration of segmentation and tracking. Particularly, in order to handle fast motion of dynamic object and handheld camera in the fusion stage, we propose a sequential 6D pose prediction method which largely increases the registration robustness and avoids registration failures occurred in conventional methods. Experimental results show that our approach can reconstruct moving object as well as static environment with rich details, and outperform conventional methods in multiple aspects.
C1 [Lu, Feixiang] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Zhou, Bin; Zhang, Yu; Zhao, Qinping] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Sch Comp Sci & Engn, Beijing, Peoples R China.
C3 Beihang University; Beihang University
RP Lu, FX (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
EM flylu@buaa.edu.cn; zhoubin@buaa.edu.cn; octopus@buaa.edu.cn;
   zhaoqp@buaa.edu.cn
RI lu, kai/KBB-4008-2024
OI Lu, Feixiang/0000-0003-3952-4402
FU National Natural Science Foundation of China [61502023, U1736217]
FX This study was funded by National Natural Science Foundation of China
   (Grant Nos. 61502023 and U1736217).
CR [Anonymous], 2016, P SAI INT SYST C
   [Anonymous], 2017, P INT C ROB AUT ICRA
   [Anonymous], 2017, ACM Transactions on Graphics (TOG), DOI DOI 10.1145/3083722
   [Anonymous], 2015, ACM T GRAPHIC
   [Anonymous], 2012, PROC CVPR IEEE
   [Anonymous], 2011, BMVC
   Cao C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462012
   Chao-Hui Shen, 2012, ACM Transactions on Graphics, V31, DOI 10.1145/2366145.2366199
   Chen JW, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461940
   Chen K, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661239
   CHEN Y, 1992, IMAGE VISION COMPUT, V10, P145, DOI 10.1016/0262-8856(92)90066-C
   Dou MS, 2015, PROC CVPR IEEE, P493, DOI 10.1109/CVPR.2015.7298647
   Hernandez C., IEEE 11 INT C COMPUT, P1
   Innmann M, 2016, LECT NOTES COMPUT SC, V9912, P362, DOI 10.1007/978-3-319-46484-8_22
   Izadi S, 2011, P UIST, P559, DOI DOI 10.1145/2047196.2047270
   Kähler O, 2015, IEEE T VIS COMPUT GR, V21, P1241, DOI 10.1109/TVCG.2015.2459891
   Kähler O, 2016, IEEE ROBOT AUTOM LET, V1, P192, DOI 10.1109/LRA.2015.2512958
   Li H, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618521
   Liao M, 2009, IEEE I CONF COMP VIS, P167, DOI 10.1109/ICCV.2009.5459161
   McCormac John, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4628, DOI 10.1109/ICRA.2017.7989538
   Newcombe RA, 2015, PROC CVPR IEEE, P343, DOI 10.1109/CVPR.2015.7298631
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Niessner M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508374
   Roth H, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.112
   Steinbrücker F, 2013, IEEE I CONF COMP VIS, P3264, DOI 10.1109/ICCV.2013.405
   Weiss A, 2011, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2011.6126465
   Whelan T, 2015, ROBOT AUTON SYST, V69, P3, DOI 10.1016/j.robot.2014.08.019
   Whelan T, 2016, INT J ROBOT RES, V35, P1697, DOI 10.1177/0278364916669237
   Xu K, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980224
   Xu K, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818075
   Yu T., 2017, IEEE INT C COMP VIS
   Zollhöfer M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601165
NR 32
TC 11
Z9 14
U1 2
U2 32
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 753
EP 763
DI 10.1007/s00371-018-1540-8
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400002
DA 2024-07-18
ER

PT J
AU Mousse, MA
   Motamed, C
   Ezin, EC
AF Mousse, Mikael A.
   Motamed, Cina
   Ezin, Eugene C.
TI Percentage of human-occupied areas for fall detection from two views
SO VISUAL COMPUTER
LA English
DT Article
DE Fall detection; Multi-cameras; Human behaviors analysis; Video
   surveillance
ID VOXEL PERSON; SYSTEM; SURVEILLANCE; CAMERAS; VIDEO
AB Falls are the major causes of fatal injury for the elderly population. To remedy this, several elderly people monitoring systems with fall detection functionality have been proposed. In this work, we investigate a video-based method of detecting fall incidents from multiple cameras. Our goal is to propose a novel method to detect falls on the floor with a multiple-camera system using the percentage of human-occupied areas. We suggest the use of two relatively orthogonal views to estimate the percentage of the surface of the person which is in contact with the ground according to the foreground information of each camera. These features are computed to differentiate by an automatic manner the lying on floor posture which can be considered as fall to other position such as standing up or sitting. This method is evaluated on a public multi-view fall detection dataset which contains videos of a healthy subject who performed 24 realistic scenarios. These scenarios show 22 fall events and 24 confounding events. The results of our experiments show that our proposed algorithm achieved 95.8 % sensitivity and 100 % specificity with less computational costs than state-of-the-art methods.
C1 [Mousse, Mikael A.; Motamed, Cina] Univ Littoral Cote dOpale, Lab Informat Signal & Image Cote Opale, LISIC, EA 4491, F-62228 Calais, France.
   [Mousse, Mikael A.; Ezin, Eugene C.] Inst Math & Sci Phys, Unite Rech Informat & Sci Appl, Porto Novo, Benin.
C3 Universite du Littoral-Cote-d'Opale; Institute of Mathematical Sciences
   & Physics IMPS
RP Mousse, MA (corresponding author), Univ Littoral Cote dOpale, Lab Informat Signal & Image Cote Opale, LISIC, EA 4491, F-62228 Calais, France.; Mousse, MA (corresponding author), Inst Math & Sci Phys, Unite Rech Informat & Sci Appl, Porto Novo, Benin.
EM mousse@lisic.univ-littoral.fr; motamed@lisic.univ-littoral.fr;
   eugene.ezin@imsp-uac.org
RI Mousse, Mikael A./AAO-6421-2021
OI Mousse, Mikael A./0000-0002-3326-6396
FU Association AS2V; Fondation Jacques De Rette, France
FX This work is partially financially supported by the Association AS2V and
   Fondation Jacques De Rette, France. Mikael A. Mousse is grateful to the
   "Service de Cooperation et d'Action Culturelle de l'Ambassade de France
   au Benin". The authors also appreciate the comments provided by the
   reviewers as these have improved the manuscript.
CR Anderson D, 2009, IEEE T FUZZY SYST, V17, P39, DOI 10.1109/TFUZZ.2008.2004498
   Anderson D, 2009, COMPUT VIS IMAGE UND, V113, P80, DOI 10.1016/j.cviu.2008.07.006
   Auvinet E., 2010, 1350 DIRO
   Auvinet E, 2011, IEEE T INF TECHNOL B, V15, P290, DOI 10.1109/TITB.2010.2087385
   Auvinet E, 2010, LECT NOTES COMPUT SC, V6134, P376, DOI 10.1007/978-3-642-13681-8_44
   Auvinet E, 2008, IEEE ENG MED BIO, P2554, DOI 10.1109/IEMBS.2008.4649721
   Bian Z., 2014, IEEE J BIOMED HLTH I
   Charfi I, 2013, J ELECTRON IMAGING, V22, DOI 10.1117/1.JEI.22.4.041106
   Cucchiara R, 2007, EXPERT SYST, V24, P334, DOI 10.1111/j.1468-0394.2007.00438.x
   Feng WG, 2014, SIGNAL IMAGE VIDEO P, V8, P1129, DOI 10.1007/s11760-014-0645-4
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Friedman SM, 2002, J AM GERIATR SOC, V50, P1329, DOI 10.1046/j.1532-5415.2002.50352.x
   Gillespie L., 2003, COCHRANE DATABASE SY, V3
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   HINDMARSH JJ, 1989, ARCH INTERN MED, V149, P2217, DOI 10.1001/archinte.149.10.2217
   Hung D.H., 2013, IEEJ T ELECT INF SYS, V133
   Hung DH, 2013, 2013 SECOND IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR 2013), P516, DOI 10.1109/ACPR.2013.124
   Kepski M, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, THEORY AND APPLICATIONS (VISAPP 2014), VOL 2, P640
   Liao YT, 2012, PATTERN RECOGN, V45, P24, DOI 10.1016/j.patcog.2011.04.017
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mastorakis G, 2014, J REAL-TIME IMAGE PR, V9, P635, DOI 10.1007/s11554-012-0246-9
   Mirmahboub B, 2013, IEEE T BIO-MED ENG, V60, P427, DOI 10.1109/TBME.2012.2228262
   Mousse MA, 2015, ICIMCO 2015 PROCEEDINGS OF THE 12TH INTERNATIONAL CONFERENCE ON INFORMATICS IN CONTROL, AUTOMATION AND ROBOTICS, VOL. 2, P296
   Mubashir Muhammad., 2012, Neurocomputing
   Rougier C, 2011, LECT NOTES COMPUT SC, V6719, P121, DOI 10.1007/978-3-642-21535-3_16
   Rougier C, 2011, IEEE T CIRC SYST VID, V21, P611, DOI 10.1109/TCSVT.2011.2129370
   Stone EE, 2015, IEEE J BIOMED HEALTH, V19, P290, DOI 10.1109/JBHI.2014.2312180
   Thome N, 2008, IEEE T CIRC SYST VID, V18, P1522, DOI 10.1109/TCSVT.2008.2005606
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Yu XG, 2008, 2008 10TH IEEE INTERNATIONAL CONFERENCE ON E-HEALTH NETWORKING, APPLICATIONS AND SERVICES, P42, DOI 10.1109/HEALTH.2008.4600107
   Zhang Z, 2012, INT C PATT RECOG, P3626
   Zweng A, 2010, LECT NOTES COMPUT SC, V6453, P163
NR 33
TC 16
Z9 16
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2017
VL 33
IS 12
BP 1529
EP 1540
DI 10.1007/s00371-016-1296-y
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FK4JE
UT WOS:000413458600004
DA 2024-07-18
ER

PT J
AU Wu, HY
   Wang, JM
   Zhang, XL
AF Wu, Huiyue
   Wang, Jianmin
   Zhang, Xiaolong
TI Combining hidden Markov model and fuzzy neural network for continuous
   recognition of complex dynamic gestures
SO VISUAL COMPUTER
LA English
DT Article
DE Gesture-based interaction; Complex dynamic gesture; Hidden Markov model;
   Fuzzy neural networks; Threshold model
AB In the design of gesture-based user interfaces, continuously recognizing complex dynamic gestures is a challenging task, because of the high-dimensional information of gestures, ambiguous semantic meanings of gestures, and the presence of unpredictable non-gesture body motions. In this paper, we propose a hybrid model that can leverage the time-series modeling ability of hidden Markov model and the fuzzy inference ability of fuzzy neural network. First, a complex dynamic gesture is decomposed and fed into the hybrid model. The likelihood probability of an observation sequence estimated by the hidden Markov model is used as fuzzy membership degree of the corresponding fuzzy class variable in fuzzy neural network. Next, fuzzy rule modeling and fuzzy inference are performed by fuzzy neural network for gesture classification. To spot key gestures accurately, a threshold model is introduced to calculate the likelihood threshold of an input pattern and provide a reliability measure of whether to accept the pattern as a gesture. Finally, the proposed method is applied to recognize ten user-defined dynamic gestures for controlling interactive digital television in a smart room. Results of our experiment show that the proposed method performed better in terms of spotting reliability and recognition accuracy than conventional gesture recognition methods.
C1 [Wu, Huiyue] Sun Yat Sen Univ, Sch Commun & Design, Guangzhou, Guangdong, Peoples R China.
   [Wang, Jianmin] Tongji Univ, Coll Arts & Media, User Experience Lab, Shanghai, Peoples R China.
   [Zhang, Xiaolong] Penn State Univ, University Pk, PA 16802 USA.
   [Zhang, Xiaolong] Taiyuan Univ Technol, Taiyuan, Shanxi, Peoples R China.
C3 Sun Yat Sen University; Tongji University; Pennsylvania Commonwealth
   System of Higher Education (PCSHE); Pennsylvania State University;
   Pennsylvania State University - University Park; Taiyuan University of
   Technology
RP Wu, HY (corresponding author), Sun Yat Sen Univ, Sch Commun & Design, Guangzhou, Guangdong, Peoples R China.; Zhang, XL (corresponding author), Penn State Univ, University Pk, PA 16802 USA.; Zhang, XL (corresponding author), Taiyuan Univ Technol, Taiyuan, Shanxi, Peoples R China.
EM wuhuiyue@gmail.com; wangjianmin@tongji.edu.cn; xiaolong.zhang@gmail.com
RI ZHANG, XIAOLONG/IZQ-4553-2023; wang, jian/GVS-0711-2022; Jiang,
   Yalin/ITV-2565-2023
OI Jiang, Yalin/0009-0003-3726-8828; Zhang, Xiaolong/0000-0002-6828-4930;
   wang, jianmin/0000-0001-8703-8973
FU National Natural Science Foundation of China [61202344]; Fundamental
   Research Funds for the Central Universities, Sun Yat-Sen University
   [1209119]; Special Project on the Integration of Industry, Education and
   Research of Guangdong Province [2012B091000062]; Fundamental Research
   Funds for the Central Universities, Tongji University [0600219052,
   0600219053]
FX We thank the financial support from the National Natural Science
   Foundation of China, No. 61202344; the Fundamental Research Funds for
   the Central Universities, Sun Yat-Sen University, No. 1209119; Special
   Project on the Integration of Industry, Education and Research of
   Guangdong Province, No. 2012B091000062; the Fundamental Research Funds
   for the Central Universities, Tongji University, Nos. 0600219052,
   0600219053. We would like to express our great appreciation to editor
   and reviewers.
CR Bilal S, 2013, ARTIF INTELL REV, V40, P495, DOI 10.1007/s10462-011-9292-0
   Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878
   Colaco Andrea., 2013, P 26 ANN ACM S USER, P227
   Coupe S, 2009, THESIS
   Feng ZQ, 2010, VISUAL COMPUT, V26, P607, DOI 10.1007/s00371-010-0452-z
   Hilliges O, 2009, UIST 2009: PROCEEDINGS OF THE 22ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P139
   Jacob R. J., 1993, ADV HUMAN COMPUTER I, V4, P151
   Kim IC, 2001, APPL INTELL, V15, P131, DOI 10.1023/A:1011231305559
   Kölsch M, 2004, PROCEEDINGS OF MOBIQUITOUS 2004, P86
   Kulshreshth A, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P1093, DOI 10.1145/2556288.2557122
   LaViola J. J., 2014, ACM SIGGRAPH 2014 Courses on-SIGGRAPH'14, P1
   Lee HK, 1999, IEEE T PATTERN ANAL, V21, P961, DOI 10.1109/34.799904
   LINDE Y, 1980, IEEE T COMMUN, V28, P84, DOI 10.1109/TCOM.1980.1094577
   Mitra S, 2007, IEEE T SYST MAN CY C, V37, P311, DOI 10.1109/TSMCC.2007.893280
   Mo Z.Y., 2005, P 10 INT C INT US IN, P239
   Oz C, 2011, ENG APPL ARTIF INTEL, V24, P1204, DOI 10.1016/j.engappai.2011.06.015
   Pedersoli F, 2014, VISUAL COMPUT, V30, P1107, DOI 10.1007/s00371-014-0921-x
   Peng B, 2011, IEEE T PATTERN ANAL, V33, P1175, DOI 10.1109/TPAMI.2010.199
   Pfeil K., 2013, P 2013 INT C INTELLI, P257, DOI [10.1145/2449396.2449429, DOI 10.1145/2449396.2449429]
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Rautaray SS, 2015, ARTIF INTELL REV, V43, P1, DOI 10.1007/s10462-012-9356-9
   Ren Z, 2013, IEEE T MULTIMEDIA, V15, P1110, DOI 10.1109/TMM.2013.2246148
   Seo HJ, 2011, IEEE T PATTERN ANAL, V33, P867, DOI 10.1109/TPAMI.2010.156
   Sohn MK, 2013, IEEE ICCE, P171, DOI 10.1109/ICCE.2013.6486844
   Song Peng., 2012, P 2012 ACM ANN C HUM, P1297, DOI DOI 10.1145/2207676.2208585
   Stenger B, 2006, IEEE T PATTERN ANAL, V28, P1372, DOI 10.1109/TPAMI.2006.189
   Stergiopoulou E, 2009, ENG APPL ARTIF INTEL, V22, P1141, DOI 10.1016/j.engappai.2009.03.008
   VITERBI AJ, 1967, IEEE T INFORM THEORY, V13, P260, DOI 10.1109/TIT.1967.1054010
   Wang X, 2007, Proceedings of the 2nd International Conference on Reliability of Electrical Products and Electrical Contacts, P235, DOI 10.1145/1216295.1216338
   Wang XS, 2007, PROCEEDINGS OF THE 2007 INTERNATIONAL CONFERENCE ON MANAGEMENT SCIENCE AND ENGINEERING, FINANCE ANALYSIS SECTION, P13
   Wilson AH, 2006, GEOL SOC AM SPEC PAP, P255, DOI 10.1130/2006.2405(14)
   Wobbrock JO, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1083
   Wu HY, 2016, MULTIMED TOOLS APPL, V75, P733, DOI 10.1007/s11042-014-2323-5
   Yang HD, 2007, IEEE T ROBOT, V23, P256, DOI 10.1109/TRO.2006.889491
   Yang HD, 2009, IEEE T PATTERN ANAL, V31, P1264, DOI 10.1109/TPAMI.2008.172
   Zhu Ji-Yu, 2006, Chinese Journal of Computers, V29, P2130
NR 36
TC 12
Z9 12
U1 0
U2 18
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2017
VL 33
IS 10
BP 1265
EP 1278
DI 10.1007/s00371-015-1147-2
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FF8VN
UT WOS:000409296000005
DA 2024-07-18
ER

PT J
AU Yang, BL
   Li, FWB
   Wang, X
   Xu, ML
   Liang, XH
   Jiang, ZY
   Jiang, YH
AF Yang, Bailin
   Li, Frederick W. B.
   Wang, Xun
   Xu, Mingliang
   Liang, Xiaohui
   Jiang, Zhaoyi
   Jiang, Yanhui
TI Visual saliency guided textured model simplification
SO VISUAL COMPUTER
LA English
DT Article
DE Visual saliency; Textured model; Model reduction; Simplification;
   Texture space optimization
ID ATTENTION; ENHANCEMENT; MESHES
AB Mesh geometry can be used to model both object shape and details. If texture maps are involved, it is common to let mesh geometry mainly model object shapes and let the texture maps model the most object details, optimising data size and complexity of an object. To support efficient object rendering and transmission, model simplification can be applied to reduce the modelling data. However, existing methods do not well consider how object features are jointly represented by mesh geometry and texture maps, having problems in identifying and preserving important features for simplified objects. To address this, we propose a visual saliency detection method for simplifying textured 3D models. We produce good simplification results by jointly processing mesh geometry and texture map to produce a unified saliency map for identifying visually important object features. Results show that our method offers a better object rendering quality than existing methods.
C1 [Yang, Bailin; Wang, Xun; Jiang, Zhaoyi] Zhejiang Gongshang Univ, Sch Comp Sci & Informat Engn, Hangzhou, Zhejiang, Peoples R China.
   [Li, Frederick W. B.] Univ Durham, Sch Engn & Comp Sci, Durham, England.
   [Xu, Mingliang] Zhengzhou Univ, Coll Comp Sci, Zhengzhou, Peoples R China.
   [Liang, Xiaohui] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Jiang, Yanhui] Hunan Univ, Sch Business, Changsha, Hunan, Peoples R China.
C3 Zhejiang Gongshang University; Durham University; Zhengzhou University;
   Beihang University; Hunan University
RP Jiang, YH (corresponding author), Hunan Univ, Sch Business, Changsha, Hunan, Peoples R China.
EM ybl@mail.zjgsu.edu.cn; frederick.li@durham.ac.uk; wx@mail.zjgsu.edu.cn;
   iexumingliang@zzu.edu.cn; 07395@buaa.edu.cn; zyjiang@mail.zjgsu.edu.cn;
   huihui0733@hnu.edu.cn
RI Li, Frederick W. B./AAM-6662-2021
OI Li, Frederick W. B./0000-0002-4283-4228; Yang,
   Bailin/0000-0003-1754-5595; liang, xiaohui/0000-0001-6351-2538
FU National High Technology Research and Development Program of China (863
   Program) [2013AA013701]; Zhejiang Province Natural Science Foundation
   [LR12F02001, Z1101340]; National Natural Science Foundation of China
   [61170214, 61472363, 61170098]
FX This work was partly supported by the National High Technology Research
   and Development Program of China (863 Program, Grant No. 2013AA013701),
   Zhejiang Province Natural Science Foundation for Distinguished Young
   Scientists (Grant No. LR12F02001), and National Natural Science
   Foundation of China (Grant Nos. 61170214, 61472363, 61170098), Zhejiang
   Province Natural Science Foundation (Grant No. Z1101340).
CR Bailin Yang, 2010, 2010 Pacific Graphics (PG). Proceedings 18th Pacific Conference on Computer Graphics and Applications, P78, DOI 10.1109/PacificGraphics.2010.18
   Balmelli L, 2002, COMPUT GRAPH FORUM, V21, P411, DOI 10.1111/1467-8659.t01-1-00601
   Bulbul A., 2010, P 7 S APPL PERC GRAP, P81, DOI DOI 10.1145/1836248.1836263
   Cheng I., 2006, SHORT PAPER EUROGRAP, V2006, P1
   Connor CE, 2004, CURR BIOL, V14, pR850, DOI 10.1016/j.cub.2004.09.041
   Corsini M., 2012, Eurographics 2012-State of the ArtReports, P135
   Feixas M, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1462055.1462056
   Fuchs H., 1985, P SIGGRAPH, P109
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   González C, 2013, SIGNAL IMAGE VIDEO P, V7, P479, DOI 10.1007/s11760-013-0450-5
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Hunter A, 2000, IEEE VISUAL, P243, DOI 10.1109/VISUAL.2000.885701
   Iourcha K., 1999, U.S. Patent, Patent No. 5956431
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2001, J ELECTRON IMAGING, V10, P161, DOI 10.1117/1.1333677
   Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855
   Kim Y, 2006, IEEE T VIS COMPUT GR, V12, P925, DOI 10.1109/TVCG.2006.174
   Lavoué G, 2009, ACM T APPL PERCEPT, V5, DOI 10.1145/1462048.1462052
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Leifman G, 2012, PROC CVPR IEEE, P414, DOI 10.1109/CVPR.2012.6247703
   Li FWB, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2000486.2000493
   Lindstrom P., 2000, THESIS
   Liu Yu-Shen., 2007, Proceedings of the 2007 ACM Symposium on Solid and Physical Modeling (SPM), P277, DOI DOI 10.1145/1236246.1236285
   Longhurst P., 2006, P 4 INT C COMPUTER G, P21
   Luebke D, 2001, SPRING EUROGRAP, P223
   Martinez Jonas, 2010, 2010 Pacific Graphics (PG). Proceedings 18th Pacific Conference on Computer Graphics and Applications, P14, DOI 10.1109/PacificGraphics.2010.10
   Menze N., 2011, COMPUT GRAPH FORUM, V29, P2261
   Miao YW, 2011, COMPUT GRAPH-UK, V35, P706, DOI 10.1016/j.cag.2011.03.017
   Miao YW, 2010, VISUAL COMPUT, V26, P433, DOI 10.1007/s00371-010-0458-6
   Montabone S, 2010, IMAGE VISION COMPUT, V28, P391, DOI 10.1016/j.imavis.2009.06.006
   Mozer M.C., 1998, Attention, V9, P341
   Ponomarenko N., 2009, P 4 INT WORKSH VID P, P14
   Qu L.J., 2008, IMAGE VISION COMPUT, V28, P391
   Sander PV, 2001, COMP GRAPH, P409, DOI 10.1145/383259.383307
   Schaefer S, 2006, ACM T GRAPHIC, V25, P533, DOI 10.1145/1141911.1141920
   Song R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2530691
   Strom Jacob, 2005, Proceedings of the ACM SIGGRAPH/EUROGRAPHICS conference on Graphics Hardware, P63, DOI DOI 10.1145/1071866.1071877
   Tang Y, 2005, COMPUTER GRAPHICS INTERNATIONAL 2005, PROCEEDINGS, P169
   TAUBIN G, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P902, DOI 10.1109/ICCV.1995.466840
   Walther D, 2006, NEURAL NETWORKS, V19, P1395, DOI 10.1016/j.neunet.2006.10.001
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Yoon D., 2012, P WORKSHOP SIGGRAPH, P53, DOI DOI 10.1145/2425296.2425305
   Zheng YY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185595
NR 44
TC 8
Z9 11
U1 1
U2 28
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2016
VL 32
IS 11
BP 1415
EP 1432
DI 10.1007/s00371-015-1129-4
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EA2BS
UT WOS:000386397000006
DA 2024-07-18
ER

PT J
AU Sand, M
   Henrich, D
AF Sand, Maximilian
   Henrich, Dominik
TI Incremental reconstruction of planar B-Rep models from multiple point
   clouds
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Boundary representation model; Surface reconstruction; Multi-view
AB Boundary representation (B-Rep) models are surface representations containing geometric and topological information that are used in many fields such as reverse engineering, simulation and optimization. To create a B-Rep model from point clouds of different viewpoints, the data are usually merged into one global point cloud and subsequently a surface model is reconstructed. The drawbacks of this approach are high processing times and the fact that all data must be available at once. We present an online approach for building planar B-Rep models from multiple organized point clouds of different viewpoints. Our procedure consists of two steps: B-Rep reconstruction and B-Rep fusion. For each viewpoint, a partial, incomplete boundary representation model is instantly reconstructed from the depth image. Our incremental fusion algorithm incorporates this partial B-Rep model into a global model built from previous viewpoints, making the model more complete and reducing uncertainty. In contrast to existing approaches, our method can be performed online and a valid B-Rep model of all previous point clouds is always available since our fusion is incremental. This enables an application to terminate the reconstruction if the model contains enough information for a given task. We evaluated our approach with an application in the field of robotics, where a depth sensor is attached to a robot arm that observes its workspace.
C1 [Sand, Maximilian; Henrich, Dominik] Univ Bayreuth, Chair Appl Comp Sci Robot & Embedded Syst 3, Univ Str 30, D-95440 Bayreuth, Germany.
C3 University of Bayreuth
RP Sand, M (corresponding author), Univ Bayreuth, Chair Appl Comp Sci Robot & Embedded Syst 3, Univ Str 30, D-95440 Bayreuth, Germany.
EM maximilian.sand@uni-bayreuth.de
CR Arikan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421642
   Bénière R, 2013, COMPUT AIDED DESIGN, V45, P1382, DOI 10.1016/j.cad.2013.06.004
   Benko P, 2001, COMPUT AIDED DESIGN, V33, P839, DOI 10.1016/S0010-4485(01)00100-2
   Berg M., 2008, COMPUTATIONAL GEOMET, V3rd, DOI DOI 10.1007/978-3-540-77974-2
   Berger Matthew, 2014, Eurographics 2014-State of the Art Reports
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   Denker K, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P151, DOI 10.1109/3DV.2013.28
   Ding D, 2001, IEEE T ROBOTIC AUTOM, V17, P515, DOI 10.1109/70.954765
   Douglas D.H., 1973, Cartographica: The International Journal for Geographic Information and Geovisualization, V10, P112, DOI [https://doi.org/10.3138/FM57-6770-U75U-7727, DOI 10.1002/9780470669488.CH2]
   Garland M., 2001, I3D 01, P49, DOI [DOI 10.1145/364338.364345, 10.1145/364338.364345]
   Hänel ML, 2012, INT J SENS NETW, V12, P25, DOI 10.1504/IJSNET.2012.047713
   Holz D, 2014, ROBOT AUTON SYST, V62, P1282, DOI 10.1016/j.robot.2014.03.013
   Huang Jianbing, 2003, J COMPUT INF SCI ENG, V2, P160
   Jones J. L., 1990, Proceedings 1990 IEEE International Conference on Robotics and Automation (Cat. No.90CH2876-1), P683, DOI 10.1109/ROBOT.1990.126063
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Kobbelt LP, 2001, COMP GRAPH, P57, DOI 10.1145/383259.383265
   Li YY, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964947
   Lorensen W. E., 1987, COMPUTER GRAPHICS, V21, P163, DOI 10.1145/37401.37422
   Ma L., 2013, EUR C MOB ROB ECMR B
   Mantyla M., 1987, INTRO SOLID MODELING
   Schnabel R, 2007, COMPUT GRAPH FORUM, V26, P214, DOI 10.1111/j.1467-8659.2007.01016.x
   Schnabel R, 2009, COMPUT GRAPH FORUM, V28, P503, DOI 10.1111/j.1467-8659.2009.01389.x
   Stamati V., 2010, TR201004 COMP SCI DE
   Vanco M, 2008, COMPUT GRAPH FORUM, V27, P1593, DOI 10.1111/j.1467-8659.2007.01109.x
   Wu JH, 2005, COMPUT GRAPH FORUM, V24, P277, DOI 10.1111/j.1467-8659.2005.00852.x
   Yan DM, 2006, LECT NOTES COMPUT SC, V4077, P73
NR 26
TC 2
Z9 2
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 945
EP 954
DI 10.1007/s00371-016-1247-7
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600026
DA 2024-07-18
ER

PT J
AU Skola, F
   Liarokapis, F
AF Skola, Filip
   Liarokapis, Fotis
TI Examining the effect of body ownership in immersive virtual and
   augmented reality environments
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Human factors; Computer graphics; Virtual reality; Augmented reality
ID RUBBER-HAND ILLUSION; MULTISENSORY INTEGRATION; PREMOTOR CORTEX; BRAIN;
   PERFORMANCE; PERCEPTION; ATTENTION; SEE
AB The traditional rubber hand illusion is a psychological experiment where participants are under the illusion that a rubber hand is part of their own body. This paper examines the use of real, virtual and augmented reality environments for identifying the elements that influence body ownership in healthy participants. Compared to the classical experiment where a plastic rubber hand was used, a realistic 3D representation was chosen to create the same illusion this time in both immersive virtual reality and augmented reality. Experiments were performed on 30 volunteers undergoing testing session composed of three stages. Participants were asked to complete two different questionnaires, one measuring their cognitive workload and another one regarding their experience with the rubber hand illusion. In addition, EEG signals of the individuals were recorded, resulting in 90 electroencephalogram datasets. Results indicate correlations between ownership statements with beta and gamma electroencephalogram bands in premotor cortex activity. Link between higher gamma production in ventral premotor area during the illusion was established in previous studies.
C1 [Skola, Filip; Liarokapis, Fotis] Masaryk Univ, Fac Informat, HCI Lab, Brno, Czech Republic.
C3 Masaryk University Brno
RP Skola, F (corresponding author), Masaryk Univ, Fac Informat, HCI Lab, Brno, Czech Republic.
EM xskola@mail.muni.cz; liarokap@mail.muni.cz
RI Liarokapis, Fotis/AAQ-9498-2021; Liarokapis, Fotis/AAD-4444-2019; Škola,
   Filip/AAG-1888-2020
OI Liarokapis, Fotis/0000-0003-3617-2261; Liarokapis,
   Fotis/0000-0003-3617-2261; Skola, Filip/0000-0003-4690-6687
CR [Anonymous], 2015, CREATE CONNECT UNITY
   [Anonymous], 2015, FORTUS 250 MC
   [Anonymous], 2015, PRODUCTS ENOBIO ENOB
   [Anonymous], 2015, WRAP 1200DXAR THE UL
   [Anonymous], 2015, MARA PLUG IN EEGLAB
   [Anonymous], 2015, MIRROR MED IMAGING S
   Baillet S, 2001, IEEE SIGNAL PROC MAG, V18, P14, DOI 10.1109/79.962275
   BasarEroglu C, 1996, INT J PSYCHOPHYSIOL, V24, P101, DOI 10.1016/S0167-8760(96)00051-7
   Botvinick M, 1998, NATURE, V391, P756, DOI 10.1038/35784
   Brovelli A, 2005, NEUROIMAGE, V28, P154, DOI 10.1016/j.neuroimage.2005.05.045
   Costantini M, 2007, CONSCIOUS COGN, V16, P229, DOI 10.1016/j.concog.2007.01.001
   Delorme A, 2004, J NEUROSCI METH, V134, P9, DOI 10.1016/j.jneumeth.2003.10.009
   Ehrsson HH, 2007, SCIENCE, V317, P1048, DOI 10.1126/science.1142175
   Ehrsson HH, 2005, J NEUROSCI, V25, P10564, DOI 10.1523/JNEUROSCI.0800-05.2005
   Ehrsson HH, 2004, SCIENCE, V305, P875, DOI 10.1126/science.1097011
   FERNANDEZ T, 1995, ELECTROEN CLIN NEURO, V94, P175, DOI 10.1016/0013-4694(94)00262-J
   Fitzgibbon SP, 2004, CLIN NEUROPHYSIOL, V115, P1802, DOI 10.1016/j.clinph.2004.03.009
   GALLAGHER S, 1986, J MIND BEHAV, V7, P541
   Graimann B, 2010, FRONT COLLECT, P1, DOI 10.1007/978-3-642-02091-9_1
   Gray CM, 1999, NEURON, V24, P31, DOI 10.1016/S0896-6273(00)80820-X
   Hart S. G., 2006, P HUM FACT ERG SOC A, V50, P904, DOI DOI 10.1177/154193120605000909
   IJsselsteijn WA, 2006, PRESENCE-TELEOP VIRT, V15, P455, DOI 10.1162/pres.15.4.455
   Kalckert A, 2014, CONSCIOUS COGN, V26, P117, DOI 10.1016/j.concog.2014.02.003
   Kalckert A, 2012, FRONT HUM NEUROSCI, V6, DOI 10.3389/fnhum.2012.00040
   Kammers MPM, 2006, NEUROPSYCHOLOGIA, V44, P2430, DOI 10.1016/j.neuropsychologia.2006.04.009
   Kilteni K, 2015, FRONT HUM NEUROSCI, V9, DOI 10.3389/fnhum.2015.00141
   Lenggenhager B, 2007, SCIENCE, V317, P1096, DOI 10.1126/science.1143439
   Maravita A, 2003, CURR BIOL, V13, pR531, DOI 10.1016/S0960-9822(03)00449-4
   Martin J H., 1991, Principles of Neural Science, V3rd, P777
   Merleau-Ponty Maurice., 2005, PHENOMENOLOGY PERCEP
   Petkova VI, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0003832
   Pfurtscheller G, 2001, P IEEE, V89, P1123, DOI 10.1109/5.939829
   Pulvermuller F, 1997, PROG NEUROBIOL, V52, P427, DOI 10.1016/S0301-0082(97)00023-3
   Ramachandran VS, 1996, P ROY SOC B-BIOL SCI, V263, P377, DOI 10.1098/rspb.1996.0058
   RAY WJ, 1985, SCIENCE, V228, P750, DOI 10.1126/science.3992243
   Ron-Angevin R, 2009, NEUROSCI LETT, V449, P123, DOI 10.1016/j.neulet.2008.10.099
   Sanei S, 2013, EEG Signal Processing
   Slater M, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010564
   Sollfrank T, 2015, FRONT HUM NEUROSCI, V9, DOI 10.3389/fnhum.2015.00463
   Suzuki K, 2013, NEUROPSYCHOLOGIA, V51, P2909, DOI 10.1016/j.neuropsychologia.2013.08.014
   Tan D., 2006, WORKSH CHI 2006
   Tsakiris M, 2005, J EXP PSYCHOL HUMAN, V31, P80, DOI 10.1037/0096-1523.31.1.80
   Zhang J, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.01659
   Zhang Y, 2008, NEUROSCIENCE, V156, P238, DOI 10.1016/j.neuroscience.2008.06.061
NR 44
TC 18
Z9 19
U1 0
U2 30
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 761
EP 770
DI 10.1007/s00371-016-1246-8
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600009
DA 2024-07-18
ER

PT J
AU Luo, GL
   Cordier, F
   Seo, HW
AF Luo, Guoliang
   Cordier, Frederic
   Seo, Hyewon
TI Spatio-temporal segmentation for the similarity measurement of deforming
   meshes
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3DOR Workshop
CY APR 06, 2014
CL Strasbourg, FRANCE
DE Spatio-temporal segmentation; Deforming mesh; Similarity; Sequence
   alignment
AB Although there have been a large body of works on computing the similarity of static shapes, similarity judgments on deforming meshes are not studied well. In this study, we investigate a similarity measurement method for comparing two deforming meshes. Based on the degree of deformation, we first binarily label each triangle within each frame as either 'deformed' or 'rigid', then merge the 'deformed' triangles in both spatial and temporal domains for the segmentation. The segmentation results are encoded in a form of evolving graph, with an aim of obtaining a compact representation of the motion of the mesh. Finally, we formulate the similarity measurement as a sequence matching problem: after clustering similar graphs and assigning each of the graphs with the cluster labels, each deforming mesh is represented with a sequence of labels. Then, we apply a sequence alignment algorithm to compute the locally optimal alignment between the two label sequences, and to compute the similarity by normalizing the alignment score. The experimental results over several datasets show that the similarities of animation data can be captured correctly using our approach. This may be significant, as it solves a problem that cannot be handled by current approaches.
C1 [Luo, Guoliang] Jiangxi Normal Univ, MIMLab, Nanchang, Peoples R China.
   [Cordier, Frederic] Univ Haute Alsace, LMIA, EA 3993, Mulhouse, France.
   [Seo, Hyewon] Univ Strasbourg, ICube, UMR 7357, CNRS, Strasbourg, France.
C3 Jiangxi Normal University; Universites de Strasbourg Etablissements
   Associes; Universite de Haute-Alsace (UHA); Universites de Strasbourg
   Etablissements Associes; Universite de Strasbourg; Centre National de la
   Recherche Scientifique (CNRS); CNRS - Institute for Engineering &
   Systems Sciences (INSIS)
RP Luo, GL (corresponding author), Jiangxi Normal Univ, MIMLab, Nanchang, Peoples R China.
EM 28guoliang.luo@gmail.com
NR 0
TC 4
Z9 5
U1 1
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2016
VL 32
IS 2
BP 243
EP 256
DI 10.1007/s00371-015-1178-8
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DF9FU
UT WOS:000371667200008
OA Green Published
DA 2024-07-18
ER

PT J
AU Buades, JM
   Gumbau, J
   Chover, M
AF Maria Buades, Jose
   Gumbau, Jesus
   Chover, Miguel
TI Separable soft shadow mapping
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3DOR Workshop
CY APR 06, 2014
CL Strasbourg, FRANCE
DE Rendering; Real time; Soft shadows
AB We propose an efficient technique for rendering visually plausible real-time soft shadows in screen space. First, we propose a novel blocker estimation technique based on a separable filter. Second, our technique performs a separable Gaussian blur in screen space over the hard shadows produced by the standard shadow mapping technique. Although blurring the hard shadows with a separable filter was done before in the literature using bilateral filtering, we use an alternative approach that minimizes artifacts. Since separated calculation is not possible for all cases, we provide data reutilization criteria based on two user-defined error thresholds called alpha and beta. As a consequence of using separable approaches for both stages of the light visibility estimation, our technique is able to improve rendering performance, especially when high-resolution shadow maps and filtering kernels are used.
C1 [Maria Buades, Jose] Univ Illes Balears, Palma De Mallorca, Spain.
   [Gumbau, Jesus] Univ Jaume 1, Castellon De La Plana, Spain.
   [Chover, Miguel] Univ Jaume 1, Comp Sci, Dept Comp Languages & Syst, Castellon De La Plana, Spain.
C3 Universitat de les Illes Balears; Universitat Jaume I; Universitat Jaume
   I
RP Gumbau, J (corresponding author), Univ Jaume 1, Castellon De La Plana, Spain.
EM josemaria.buades@uib.es; jgumbau@uji.es; chover@uji.es
RI Chover Sellés, Miguel/P-9933-2018; Buades Rubio, Jose Maria/G-4232-2015
OI Chover Sellés, Miguel/0000-0002-0525-7038; Buades Rubio, Jose
   Maria/0000-0002-6137-9558
NR 0
TC 2
Z9 2
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2016
VL 32
IS 2
BP 167
EP 178
DI 10.1007/s00371-015-1062-6
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DF9FU
UT WOS:000371667200003
DA 2024-07-18
ER

PT J
AU Bouali, F
   Devaux, S
   Venturini, G
AF Bouali, Fatma
   Devaux, Sebastien
   Venturini, Gilles
TI Visual mining of time series using a tubular visualization
SO VISUAL COMPUTER
LA English
DT Article
DE Visual data mining; Time series; 3D interactive visualizations; User
   evaluation
ID DATABASES
AB In this paper, we study the visual mining of time series, and we contribute to the study and evaluation of 3D tubular visualizations. We describe the state of the art in the visual mining of time-dependent data, and we concentrate on visualizations that use a tubular shape to represent data. After analyzing the motivations for studying such a representation, we present an extended tubular visualization. We propose new visual encodings of the time and data, new interactions for knowledge discovery, and the use of rearrangement clustering. We show how this visualization can be used in several real-world domains and that it can address large datasets. We present a comparative user study. We conclude with the advantages and the drawbacks of our method (especially the tubular shape).
C1 [Bouali, Fatma] Univ Lille 2, IUT, 25-27 Rue Marechal Foch, F-59100 Roubaix, France.
   [Devaux, Sebastien] Airbus Def & Space Space Syst, Simulat TSEOC12, Les Mureaux, France.
   [Venturini, Gilles] Univ Tours, Comp Sci Lab, 64 Ave Jean Portalis, F-37200 Tours, France.
C3 Universite de Lille; Airbus; Airbus France S.A.S.; Universite de Tours
RP Venturini, G (corresponding author), Univ Tours, Comp Sci Lab, 64 Ave Jean Portalis, F-37200 Tours, France.
EM fatma.bouali@univ-lille2.fr; sebastien.devaux@astrium.eads.net;
   venturini@univ-tours.fr
CR Aigner W, 2012, COMPUT GRAPH FORUM, V31, P995, DOI 10.1111/j.1467-8659.2012.03092.x
   Aigner W, 2011, HUM-COMPUT INT-SPRIN, P1, DOI 10.1007/978-0-85729-079-3
   Aigner W, 2007, COMPUT GRAPH-UK, V31, P401, DOI 10.1016/j.cag.2007.01.030
   Andrienko N., 2011, INT CART C ICC
   Ankerst M, 1998, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION - PROCEEDINGS, P52, DOI 10.1109/INFVIS.1998.729559
   ANKERST M, 1996, P IEEE VIS 96 HOT TO
   Ankerst M, 2001, P ACM SIGKDD WORKSH
   [Anonymous], 2012, QUEUE, DOI DOI 10.1145/2133416.2146416
   ANTUNES C, 2001, KDD WORKSH TEMP DAT
   Aris A, 2005, LECT NOTES COMPUT SC, V3585, P835, DOI 10.1007/11555261_66
   Beardsley T., 1999, PROFILE HUMANS UNITE
   Bertin Jacques., 1977, GRAPHIQUE TRAITEMENT
   Carlis J. V., 1998, 11th Annual Symposium on User Interface Software and Technology. UIST. Proceedings of the ACM Symposium, P29, DOI 10.1145/288392.288399
   Cleveland WS, 1993, Visualizing data
   Craig P., 2012, Proceedings of the 2012 16th International Conference on Information Visualisation (IV), P68, DOI 10.1109/IV.2012.22
   Daassi C., 2000, P BDA00 C, P24
   Debregeas A., 1998, Proceedings Fourth International Conference on Knowledge Discovery and Data Mining, P179
   Francis B., 2003, CASE STUD VIS SOC SC, V30
   Hackstadt S. T., 1994, THESIS
   Hao MC, 2012, INFORM VISUAL, V11, P71, DOI 10.1177/1473871611430769
   Hofmann H, 2012, IEEE T VIS COMPUT GR, V18, P2441, DOI 10.1109/TVCG.2012.230
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504
   Kandogan Eser., 2000, P IEEE INFORM VISUAL, P4
   Keim DA, 1995, VISUALIZATION '95 - PROCEEDINGS, P279, DOI 10.1109/VISUAL.1995.485140
   Keim DA, 1996, IEEE T KNOWL DATA EN, V8, P923, DOI 10.1109/69.553159
   KEIM DA, 2010, MASTERING INFORMATIO
   Kincaid R., 2006, Proceedings of the working conference on Advanced visual interfaces, P404
   Krstajic M, 2011, IEEE T VIS COMPUT GR, V17, P2432, DOI 10.1109/TVCG.2011.179
   Lin J., 2004, Proceedings of the Thirtieth international conference on Very large data bases-Volume, P1269, DOI DOI 10.5555/1316689.1316811
   Mackinlay J. D., 1991, Human Factors in Computing Systems. Reaching Through Technology. CHI '91. Conference Proceedings, P173, DOI 10.1145/108844.108870
   MCCORMICK WT, 1972, OPER RES, V20, P993, DOI 10.1287/opre.20.5.993
   McLachlan P, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P1483
   Minard C.J, 1861, CARTE FIGURATIVE PER, P1812
   Mitchell K, 1997, VISUALIZATION SCI CO
   Müller W, 2003, PROCEEDINGS OF THE 2003 WINTER SIMULATION CONFERENCE, VOLS 1 AND 2, P737, DOI 10.1109/WSC.2003.1261490
   NEKRASOVSKI D, 2006, P SIGCHI C HUM FACT, P11
   Oelke D, 2011, COMPUT GRAPH FORUM, V30, P871, DOI 10.1111/j.1467-8659.2011.01936.x
   Shmueli G, 2006, DECIS SUPPORT SYST, V42, P1521, DOI 10.1016/j.dss.2006.01.001
   Shneiderman B, 1996, IEEE SYMPOSIUM ON VISUAL LANGUAGES, PROCEEDINGS, P336, DOI 10.1109/VL.1996.545307
   Suntinger M, 2008, IEEE COMPUT GRAPH, V28, P46, DOI 10.1109/MCG.2008.97
   Sureau F, 2008, LNCS, P555
   Therón R, 2006, LECT NOTES COMPUT SC, V4073, P70
   Tominski Christian, 2008, P ANN SIGRAD C SPEC, P53
   Van Wijk J. J., 1999, Proceedings 1999 IEEE Symposium on Information Visualization (InfoVis'99), P4, DOI 10.1109/INFVIS.1999.801851
   Wattenberg M, 2002, INFOVIS 2002: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2002, P110, DOI 10.1109/INFVIS.2002.1173155
   Weber M, 2001, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2001, PROCEEDINGS, P7, DOI 10.1109/infvis.2001.963273
   Wong P. C., 1994, Scientific Visualization, Overviews, Methodologies, and Techniques, P3
NR 47
TC 4
Z9 7
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2016
VL 32
IS 1
BP 15
EP 30
DI 10.1007/s00371-014-1052-0
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DF9FQ
UT WOS:000371666800003
DA 2024-07-18
ER

PT J
AU Tang, YL
   Tong, RF
   Tang, M
   Zhang, Y
AF Tang, Yanlong
   Tong, Ruofeng
   Tang, Min
   Zhang, Yun
TI Depth incorporating with color improves salient object detection
SO VISUAL COMPUTER
LA English
DT Article
DE Salient object detection; Depth information; Color information; Stereo
   images
ID VISUAL-ATTENTION; DRIVEN; MODEL
AB Detecting salient objects in challenging images attracts increasing attention as many applications require more robust method to deal with complex images from the Internet. Prior methods produce poor saliency maps in challenging cases mainly due to the complex patterns in the background and internal color edges in the foreground. The former problem may introduce noises into saliency maps and the later forms the difficulty in determining object boundaries. Observing that depth map can supply layering information and more reliable boundary, we improve salient object detection by integrating two features: color information and depth information which are calculated from stereo images. The two features collaborate in a two-stage framework. In the object location stage, depth mainly helps to produce a noise-filtered salient patch, which indicates the location of the object. In the object boundary inference stage, boundary information is encoded in a graph using both depth and color information, and then we employ the random walk to infer more reliable boundaries and obtain the final saliency map. We also build a data set containing 100+ stereo pairs to test the effectiveness of our method. Experiments show that our depth-plus-color based method significantly improves salient object detection compared with previous color-based methods.
C1 [Tang, Yanlong; Tong, Ruofeng; Tang, Min] Zhejiang Univ, Inst Artificial Intelligence, State Key Lab CAD&CG, Hangzhou 310027, Zhejiang, Peoples R China.
   [Zhang, Yun] Zhejiang Univ Media & Commun, Zhejiang Inst Radio & TV Technol, Hangzhou 310018, Zhejiang, Peoples R China.
C3 Zhejiang University; Communication University of Zhejiang
RP Tang, YL (corresponding author), Zhejiang Univ, Inst Artificial Intelligence, State Key Lab CAD&CG, Hangzhou 310027, Zhejiang, Peoples R China.
EM yanlongtang@gmail.com; trf@zju.edu.cn; tang_m@zju.edu.cn;
   zhangyun_zju@zju.edu.cn
RI Tang, Min/KOC-3090-2024
FU National High-Tech Research and Development Program of China
   [2013AA013903]; National Basic Research Program of China [2011CB302205];
   People Programme (Marie Curie Actions) of the European Union's Seventh
   Framework Programme [612627]; Zhejiang Provincial Natural Science
   Foundation of China [LY14F020050]
FX We thank Prof. Jianjun Zhang and Jian Chang from Bournemouth University
   for their valuable discussion. We also thank all anonymous reviewers for
   their valuable comments. This work was supported by National High-Tech
   Research and Development Program of China (No. 2013AA013903), National
   Basic Research Program of China (No. 2011CB302205), the People Programme
   (Marie Curie Actions) of the European Union's Seventh Framework
   Programme FP7/2007-2013/ under REA Grant agreement No. [612627]-
   'AniNex', Zhejiang Provincial Natural Science Foundation of China (No.
   LY14F020050).
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   Backer G, 2001, IEEE T PATTERN ANAL, V23, P1415, DOI 10.1109/34.977565
   Borji A, 2010, IMAGE VISION COMPUT, V28, P1130, DOI 10.1016/j.imavis.2009.10.006
   Bruce N., 2006, P ADV NEUR INF PROC, P155
   Chang KY, 2011, IEEE I CONF COMP VIS, P914, DOI 10.1109/ICCV.2011.6126333
   Chen TY, 2009, BMC BIOINFORMATICS, V10, DOI 10.1186/1471-2105-10-24
   Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193
   Cheng MM, 2014, VISUAL COMPUT, V30, P443, DOI 10.1007/s00371-013-0867-4
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Ciptadi A., 2013, BRIT MACHINE VISION, P1
   Desingh K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.98
   Donoser M, 2009, IEEE I CONF COMP VIS, P817, DOI 10.1109/ICCV.2009.5459296
   Du SP, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508387
   Du SP, 2013, IEEE T VIS COMPUT GR, V19, P1288, DOI 10.1109/TVCG.2013.14
   Fang Y., 2013, VISUAL COMMUNICATION, P1
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Garcia-Diaz A, 2009, LECT NOTES COMPUT SC, V5807, P343
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   Han JW, 2006, IEEE T CIRC SYST VID, V16, P141, DOI 10.1109/TCSVT.2005.859028
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hou X., 2008, Advances in Neural Information Processing Systems, V5, P7
   Hou X., 2007, IEEE C COMP VIS PATT, V1, P1, DOI DOI 10.1109/CVPR.2007.383267
   Hu SM, 2013, VISUAL COMPUT, V29, P393, DOI 10.1007/s00371-013-0792-6
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang HZ, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.110
   Ko BC, 2006, J OPT SOC AM A, V23, P2462, DOI 10.1364/JOSAA.23.002462
   Lang CY, 2012, LECT NOTES COMPUT SC, V7573, P101, DOI 10.1007/978-3-642-33709-3_8
   Li HL, 2011, IEEE T IMAGE PROCESS, V20, P3365, DOI 10.1109/TIP.2011.2156803
   Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359
   Liu H, 2012, VISUAL COMPUT, V28, P279, DOI 10.1007/s00371-011-0638-z
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu Z, 2010, IEEE IMAGE PROC, P253, DOI 10.1109/ICIP.2010.5652613
   Margolin R, 2013, PROC CVPR IEEE, P1139, DOI 10.1109/CVPR.2013.151
   Margolin R, 2013, VISUAL COMPUT, V29, P381, DOI 10.1007/s00371-012-0740-x
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Mu T. J., 2014, VISUAL COMPUT, P1
   Mu TJ, 2014, VISUAL COMPUT, V30, P833, DOI 10.1007/s00371-014-0961-2
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Rosenholtz R, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/1870076.1870080
   Rutishauser U., 2004, P IEEE COMP SOC C CO, DOI DOI 10.1109/CVPR.2004.1315142
   Seo HJ, 2009, J VISION, V9, DOI 10.1167/9.12.15
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Shi Y, 2014, HORTIC RES-ENGLAND, V1, DOI 10.1038/hortres.2014.6
   Siagian C, 2009, IEEE T ROBOT, V25, P861, DOI 10.1109/TRO.2009.2022424
   Smith BM, 2009, PROC CVPR IEEE, P485, DOI 10.1109/CVPRW.2009.5206793
   Tong RF, 2013, IEEE T VIS COMPUT GR, V19, P1375, DOI 10.1109/TVCG.2012.319
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zhang JM, 2013, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2013.26
NR 54
TC 21
Z9 24
U1 1
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2016
VL 32
IS 1
BP 111
EP 121
DI 10.1007/s00371-014-1059-6
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DF9FQ
UT WOS:000371666800010
DA 2024-07-18
ER

PT J
AU Kot, M
   Nagahashi, H
   Szymczak, P
AF Kot, Maciej
   Nagahashi, Hiroshi
   Szymczak, Piotr
TI Elastic moduli of simple mass spring models
SO VISUAL COMPUTER
LA English
DT Article
DE Mass spring model; Soft body deformation; Physically based modeling
ID DEFORMABLE MODELS
AB Mass spring models (MSMs) are a popular choice for representation of soft bodies in computer graphics and virtual reality applications. In this paper, we investigate physical properties of the simplest MSMs composed of mass points and linear springs. The nodes are either placed on a cubic lattice or positioned randomly within the system. We calculate the elastic moduli for such models and relate the results to other studies. We show that there is a well-defined relationship between the geometric characteristics of the MSM systems and physical properties of the modeled materials. It is also demonstrated that these models exhibit a proper convergence to a unique solution upon mesh refinement and thus can represent elastic materials with a high precision.
C1 [Kot, Maciej; Nagahashi, Hiroshi] Tokyo Inst Technol, Imaging Sci & Engn Lab, Tokyo 152, Japan.
   [Szymczak, Piotr] Univ Warsaw, Inst Theoret Phys, Fac Phys, Warsaw, Poland.
C3 Tokyo Institute of Technology; University of Warsaw
RP Kot, M (corresponding author), Tokyo Inst Technol, Imaging Sci & Engn Lab, Tokyo 152, Japan.
EM eustachy@gmail.com
RI Szymczak, Piotr/A-9268-2008
FU JSPS KAKENHI [24300035]; National Science Centre (Poland)
   [2012/07/E/ST3/01734]
FX M.K. and H.N. acknowledge the support of JSPS KAKENHI (Grant Number
   24300035). P.S. acknowledges the support of the National Science Centre
   (Poland) under research Grant No. 2012/07/E/ST3/01734.
CR [Anonymous], ACM T GRAPH
   [Anonymous], 1984, Elasticity Theory
   Baudet V., 2007, P SURG
   Bower A.F., 2009, APPL MECH SOLIDS
   Delingette H, 2008, IEEE T VIS COMPUT GR, V14, P329, DOI 10.1109/TVCG.2007.70431
   Flannery B. P., 1992, NUMERICAL RECIPES C, DOI DOI 10.2277/052143064X
   HARDY RJ, 1982, J CHEM PHYS, V76, P622, DOI 10.1063/1.442714
   Kot M., 2011, MULTIMED VIRTUAL ENV, V110, P201
   Ladd AJC, 1997, PHYSICA A, V240, P349, DOI 10.1016/S0378-4371(97)00158-1
   LAKES R, 1991, J MATER SCI, V26, P2287, DOI 10.1007/BF01130170
   Levine JA, 2014, P ACM SIGGRAPH EUR S
   Lloyd BA, 2007, IEEE T VIS COMPUT GR, V13, P1081, DOI 10.1109/TVCG.2007.1055
   Love A.E.H., 1944, A treatise on the mathematical theory of elasticity, Vfourth
   Meier U, 2005, COMPUT METH PROG BIO, V77, P183, DOI 10.1016/j.cmpb.2004.11.002
   Nealen A, 2006, COMPUT GRAPH FORUM, V25, P809, DOI 10.1111/j.1467-8659.2006.01000.x
   OSTOJA-STARZEWSKI M., 2002, Appl. Mech. Rev., V55, P35, DOI [10.1115/1.1432990, DOI 10.1115/1.1432990]
   San-Vicente G, 2012, IEEE T VIS COMPUT GR, V18, P228, DOI 10.1109/TVCG.2011.32
   Torquato S, 2002, RANDOM HETEROGENEOUS, P1, DOI [10.1007/978-1-4757-6355-3, DOI 10.1007/978-1-4757-6355-3, 10.1007/978-1-4757-6355-3_1]
   Van Gelder A., 1998, Journal of Graphics Tools, V3, P21, DOI 10.1080/10867651.1998.10487490
   Zimmerman JA, 2004, MODEL SIMUL MATER SC, V12, pS319, DOI 10.1088/0965-0393/12/4/S03
NR 20
TC 51
Z9 55
U1 1
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2015
VL 31
IS 10
BP 1339
EP 1350
DI 10.1007/s00371-014-1015-5
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CQ7CG
UT WOS:000360759800004
DA 2024-07-18
ER

PT J
AU Getto, R
   Kuijper, A
   von Landesberger, T
AF Getto, Roman
   Kuijper, Arjan
   von Landesberger, Tatiana
TI Extended surface distance for local evaluation of 3D medical image
   segmentations
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Evaluation; Distance measure; 3D medical image segmentation;
   Segmentation quality; Local distance; Mesh distance
AB The evaluation of 3D medical image segmentation quality requires a reliable detailed comparison of a reference segmentation with an automatic segmentation. It should be able to measure the quality accurately and, thus, to reveal problematic regions. While several (global) measures, providing a single quality value, are available, the only widely used local measure is the Surface Distance (i.e., point-to-surface distance). This measure, however, has significant drawbacks such as asymmetry and underestimation in distant and differently formed regions. Other available measures have limited suitability for 3D medical segmentation evaluation. We present a more reliable distance measure for assessing and analyzing local differences between automatic and reference (i.e., ground truth) 3D segmentations. We identify and overcome Surface Distance drawbacks, esp. in regions with larger dissimilarities. We evaluated our approach on four real medical image datasets. The results indicate that our measure provides more accurate local distance values.
C1 [Getto, Roman; Kuijper, Arjan; von Landesberger, Tatiana] Tech Univ Darmstadt, Darmstadt, Germany.
   [Kuijper, Arjan] Fraunhofer IGD, Darmstadt, Germany.
C3 Technical University of Darmstadt
RP Getto, R (corresponding author), Tech Univ Darmstadt, Darmstadt, Germany.
EM roman.getto@gris.tu-darmstadt.de; arjan.kuijper@gris.tu-darmstadt.de;
   tatiana.von.landesberger@gris.tu-darmstadt.de
RI Kuijper, Arjan/A-7814-2012
OI Kuijper, Arjan/0000-0002-6413-0061; von Landesberger,
   Tatiana/0000-0002-5279-1444
FU DFG within an SPP 1335 project
FX The work has been partially supported by DFG within an SPP 1335 project.
   The authors are grateful to Prof. Georg Sakas and Dr. Meike Becker for
   the data provision, support with the project and helpful comments on the
   paper draft.
CR [Anonymous], P SPIE MED IMAGING
   [Anonymous], P MED IM COMP COMP A
   [Anonymous], P TECHN REP PRINC VI
   [Anonymous], MICCAI
   [Anonymous], ACM T GRAPH
   [Anonymous], P 3 D DIG IM MOD
   [Anonymous], VIS COMPUT
   [Anonymous], COMPUT GRAPH
   [Anonymous], P IJCAI
   [Anonymous], 1997, THESIS CARNEGIE MELL
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], 8 IEEE INT C COMP VI
   Cates J, 2006, P MICCAI 06, V1, P90
   Chalana V, 1997, IEEE T MED IMAGING, V16, P642, DOI 10.1109/42.640755
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   Guéziec A, 2001, IEEE T VIS COMPUT GR, V7, P47, DOI 10.1109/2945.910820
   Heimann T, 2009, IEEE T MED IMAGING, V28, P1251, DOI 10.1109/TMI.2009.2013851
   Heimann T, 2009, MED IMAGE ANAL, V13, P543, DOI 10.1016/j.media.2009.05.004
   Kirschner M, 2011, LECT NOTES COMPUT SC, V6892, P492, DOI 10.1007/978-3-642-23629-7_60
   Kraevoy V, 2004, ACM T GRAPHIC, V23, P861, DOI 10.1145/1015706.1015811
   Li H, 2008, COMPUT GRAPH FORUM, V27, P1421, DOI 10.1111/j.1467-8659.2008.01282.x
   Li HZ, 2007, PROC WRLD ACAD SCI E, V1, P1
   Lipman Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1805964.1805971
   Martinek M, 2015, VISUAL COMPUT, V31, P223, DOI 10.1007/s00371-014-1040-4
   Pizer SM, 2003, INT J COMPUT VISION, V55, P85, DOI 10.1023/A:1026313132218
   Sahillioglu Y, 2011, COMPUT GRAPH FORUM, V30, P1461, DOI 10.1111/j.1467-8659.2011.02020.x
   Strecha C., 2008, P COMPUTER VISION PA, P1
   Udupa JK, 2006, COMPUT MED IMAG GRAP, V30, P75, DOI 10.1016/j.compmedimag.2005.12.001
   van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P1681, DOI 10.1111/j.1467-8659.2011.01884.x
   Wu H, 2015, VISUAL COMPUT, V31, P367, DOI 10.1007/s00371-014-0931-8
   Zhang H, 2008, COMPUT GRAPH FORUM, V27, P1431, DOI 10.1111/j.1467-8659.2008.01283.x
   Zou KH, 2004, ACAD RADIOL, V11, P178, DOI 10.1016/S1076-6332(03)00671-8
NR 32
TC 3
Z9 3
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 989
EP 999
DI 10.1007/s00371-015-1113-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500024
DA 2024-07-18
ER

PT J
AU Kim, S
   Kim, YJ
AF Kim, SeongKi
   Kim, Young J.
TI GPGPU-Perf: efficient, interval-based DVFS algorithm for mobile GPGPU
   applications
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE DVFS; GPGPU; Mobile device; OpenCL; OpenGL ES
AB Although general purpose computations on graphics processing unit (GPGPU) technologies are available even on GPUs, their performance has been seriously affected by the underlying dynamic voltage and frequency scaling (DVFS) mechanism of GPU. In order to save the energy, eventually prolonging the battery life, the DVFS adjusts the GPU's frequency according to the past utilization. When the GPU processes graphic tasks only, it is enough to process them within a fixed time (typically 30-60 frames per second), so the DVFS parameters can be conservatively set. However, in GPGPU case, the GPU should process them at much higher rates depending on applications. Although a modification of DVFS parameters may improve the GPGPU performance, the energy efficiency is sacrificed, and the performance of graphic tasks is affected, as these parameters are shared by both graphic and GPGPU tasks. In order to improve the GPGPU performance without influencing the graphic performance, we devise the new GPGPU-Perf algorithm that adjusts the DVFS parameters such as thresholds and an interval. The new algorithm controls the frequency more intelligently for mobile GPGPU applications, and thus the performance over energy increases by 1.44 times with no influences on graphic tasks and any modifications of GPGPU algorithms. To the best of our knowledge, this paper is the first work that proposes a GPU-DVFS algorithm for GPGPU applications.
C1 [Kim, SeongKi; Kim, Young J.] Ewha Womans Univ, Dept Comp Sci & Engn, Seoul, South Korea.
C3 Ewha Womans University
RP Kim, YJ (corresponding author), Ewha Womans Univ, Dept Comp Sci & Engn, Seoul, South Korea.
EM skkim9226@gmail.com; kimy@ewha.ac.kr
RI Kim, SeongKi/K-2774-2019
OI Kim, SeongKi/0000-0002-2664-3632; Kim, Young J./0000-0003-2159-4832
FU NRF in Korea [2012R1A2A2A01046246, 2012R1A2A2A06047007,
   2014K1A3A1A17073365]; MCST/KOCCA in the CT RD program [R2014060011]
FX This work was supported in part by NRF in Korea (2012R1A2A2A01046246,
   2012R1A2A2A06047007, 2014K1A3A1A17073365) and MCST/KOCCA in the CT R&D
   program 2014 (R2014060011). Young J. Kim is the corresponding author.
CR Altantsetseg E, 2013, VISUAL COMPUT, V29, P617, DOI 10.1007/s00371-013-0800-x
   [Anonymous], 2013, P INT S COMP ARCH IS
   [Anonymous], 2014, OPENCL C SPEC VERS 2
   Bakhoda A, 2009, INT SYM PERFORM ANAL, P163, DOI 10.1109/ISPASS.2009.4919648
   Boyer M., 2013, THESIS U VIRGINIA VI
   Chang B, 2014, VISUAL COMPUT, V30, P201, DOI 10.1007/s00371-013-0796-2
   Che SA, 2009, I S WORKL CHAR PROC, P44, DOI 10.1109/IISWC.2009.5306797
   Choi K, 2004, ISLPED '04: PROCEEDINGS OF THE 2004 INTERNATIONAL SYMPOSIUM ON LOW POWER ELECTRONICS AND DESIGN, P174, DOI 10.1145/1013235.1013282
   Huang MC, 2010, VISUAL COMPUT, V26, P943, DOI 10.1007/s00371-010-0435-0
   Kai Ma, 2012, 2012 41st International Conference on Parallel Processing (ICPP 2012), P48, DOI 10.1109/ICPP.2012.31
   Liu FC, 2014, IEEE T VIS COMPUT GR, V20, P714, DOI 10.1109/TVCG.2013.268
   Liu F, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866180
   Mochocki BC, 2006, DES AUT CON, P592, DOI 10.1109/DAC.2006.229296
   Orgerie AC, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2532637
   Pallipadi V., 2006, Proceedings of the Linux Symposium, V2, P215
   Rister B., 2013, P IEEE INT C AC SPEE
   Shen J., P TECHN REP PDS 2011
   Xinxin M., 2013, P WORKSH POW AW COMP
NR 18
TC 3
Z9 4
U1 1
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 1045
EP 1054
DI 10.1007/s00371-015-1111-1
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500029
DA 2024-07-18
ER

PT J
AU Splechtna, R
   Elshehaly, M
   Gracanin, D
   Duras, M
   Bühler, K
   Matkovic, K
AF Splechtna, Rainer
   Elshehaly, Mai
   Gracanin, Denis
   Duras, Mario
   Buehler, Katja
   Matkovic, Kresimir
TI Interactive interaction plot Supporting parameter space exploration in a
   design phase
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Interactive visual analysis; Interaction plot; Main effects plot; Design
   of experiments
ID VISUALIZATION; SENSITIVITY; UNCERTAINTY; QUALITY
AB Design of experiments (DOE) is the study of how to vary control parameters to efficiently design and evaluate experiments. Main effects plot and interaction plot are two data views often used to explore differences between mean values and interactions between the DOE parameters but they are mostly limited to two parameters. We propose a new data view, interactive interaction plot, that supports exploration and analysis of high-dimensional interactions between parameters. The data view is integrated within a coordinated multiple views system. We describe the new data view using an Olympic medals data set. We also describe a case study dealing with initial selection of hybrid vehicle components. Very positive feedback from automotive domain experts demonstrates the usefulness of the newly proposed approach.
C1 [Splechtna, Rainer; Buehler, Katja; Matkovic, Kresimir] VRVis Res Ctr, Vienna, Austria.
   [Elshehaly, Mai] Virginia Tech, Ctr Human Comp Interact, Blacksburg, VA USA.
   [Gracanin, Denis] Virginia Tech, Dept Comp Sci, Blacksburg, VA USA.
   [Duras, Mario] AVL AST Doo, Zagreb, Croatia.
C3 Virginia Polytechnic Institute & State University; Virginia Polytechnic
   Institute & State University
RP Matkovic, K (corresponding author), VRVis Res Ctr, Vienna, Austria.
EM matkovic@vrvis.at
RI Matkovic, Kresimir/AAT-8896-2021
OI Buhler, Katja/0000-0002-0362-7998; Elshehaly, Mai/0000-0002-5867-6121;
   Matkovic, Kresimir/0000-0001-9406-8943; Gracanin,
   Denis/0000-0001-6831-2818
FU National Institute of Mental Health [R21MH100268]
FX We thank Goran Todorovic from AVL for numerous fruitful discussions.
   Part of this work was done in the scope of the K1 program at the VRVis
   Research Center. Part of this work was supported by a grant from the
   National Institute of Mental Health (R21MH100268).
CR [Anonymous], 2015, SPORTS REFERENCE OLY
   [Anonymous], 2007, International Series in Operations Research & Management Science
   Bachthaler S, 2008, IEEE T VIS COMPUT GR, V14, P1428, DOI 10.1109/TVCG.2008.119
   Berger W, 2011, COMPUT GRAPH FORUM, V30, P911, DOI 10.1111/j.1467-8659.2011.01940.x
   Booshehrian M, 2012, COMPUT GRAPH FORUM, V31, P1235, DOI 10.1111/j.1467-8659.2012.03116.x
   Box GE., 2005, Statistics for Experimenters: Design, Innovation, and Discovery, V2nd ed.
   CHAMBERS J, 1990, COMPSTAT 1990 : PROCEEDINGS IN COMPUTATIONAL STATISTICS, P317
   Demir I, 2013, COMPUT GRAPH FORUM, V32, P21, DOI 10.1111/cgf.12089
   Demir I, 2014, IEEE T VIS COMPUT GR, V20, P2694, DOI 10.1109/TVCG.2014.2346448
   Eriksson L., 2008, Design of Experiments-Principles and Applications
   Ghorbani R, 2010, IEEE T VEH TECHNOL, V59, P2016, DOI 10.1109/TVT.2010.2041563
   Heinrich J, 2009, IEEE T VIS COMPUT GR, V15, P1531, DOI 10.1109/TVCG.2009.131
   Konyha Z, 2006, IEEE T VIS COMPUT GR, V12, P1373, DOI 10.1109/TVCG.2006.99
   Lee T., 2010, SAE TECHNICAL PAPERS
   Liu SX, 2014, VISUAL COMPUT, V30, P1373, DOI 10.1007/s00371-013-0892-3
   Matkovic K, 2008, IEEE T VIS COMPUT GR, V14, P1699, DOI 10.1109/TVCG.2008.145
   Montgomery D., 2013, DESIGN ANAL EXPT, V8
   Padua L, 2014, COMPUT GRAPH-UK, V41, P99, DOI 10.1016/j.cag.2014.02.004
   Park GJ, 2007, Analytic methods for design practice, P309
   Phadke M.N., 2012, IS T SPIE ELECT IMAG
   Potter K, 2009, INT CONF DAT MIN WOR, P233, DOI 10.1109/ICDMW.2009.55
   Roberts JC, 2007, CMV 2007: FIFTH INTERNATIONAL CONFERENCE ON COORDINATED & MULTIPLE VIEWS IN EXPLORATORY VISUALIZATION, PROCEEDINGS, P61, DOI 10.1109/CMV.2007.20
   Sanyal J, 2010, IEEE T VIS COMPUT GR, V16, P1421, DOI 10.1109/TVCG.2010.181
   Shaffer CA, 1998, VISUALIZATION '98, PROCEEDINGS, P491, DOI 10.1109/VISUAL.1998.745351
   Simpson A., 2009, DEV HEAVY DUTY HYBRI
   Stork A, 2008, VISUAL COMPUT, V24, P947, DOI 10.1007/s00371-008-0274-4
   Tweedie L., 1996, Human Factors in Computing Systems. Common Ground. CHI 96 Conference Proceedings, P406, DOI 10.1145/238386.238587
   Waser J, 2010, IEEE T VIS COMPUT GR, V16, P1458, DOI 10.1109/TVCG.2010.223
   Yu-Hsuan Chan, 2010, 2010 Proceedings of IEEE Symposium on Visual Analytics Science and Technology (VAST 2010), P43, DOI 10.1109/VAST.2010.5652460
NR 29
TC 7
Z9 7
U1 3
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 1055
EP 1065
DI 10.1007/s00371-015-1095-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500030
DA 2024-07-18
ER

PT J
AU Kansal, R
   Kumar, S
AF Kansal, Ruchin
   Kumar, Subodh
TI A vectorization framework for constant and linear gradient filled
   regions
SO VISUAL COMPUTER
LA English
DT Article
DE Image vectorization; Gradient reconstruction; Region detection
AB Linear gradients are commonly applied in non-photographic artwork for shading and other artistic effects. It is sometimes necessary to generate a vector graphics form of raster images comprising such artwork with the expectation to obtain a simple output and plug it into a traditional workflow, to be further edited and arranged. Many such workflows support only linear gradients and our goal is to generate a standard vector form of the image that can fit such workflow. This vectorization process should be automatic with minimal user intervention. We present a simple image vectorization algorithm that detects regions of linear gradient in potentially noisy images and reconstructs the vector definition on the basis of that information. It uses a novel interval gradient optimization scheme to derive large regions of uniform gradient. We also demonstrate the technique on noisy and hand-drawn portraits.
C1 [Kansal, Ruchin; Kumar, Subodh] IIT, Delhi, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Delhi
RP Kansal, R (corresponding author), IIT, Delhi, India.
EM rkansal@adobe.com; subodh@cse.iitd.ac.in
RI Kumar, Subodh/JYP-8905-2024
FU department of Science and Technology, government of India
FX The images used in Sect. 4.6 have been taken from ARDECO [15] website.
   Other images have been taken from various open content provider
   websites. This research was supported in part by the department of
   Science and Technology, government of India.
CR Adobe Systems Inc, 2010, ADOB ILL CS
   [Anonymous], P 6 INT C OPT TECHN
   [Anonymous], 1992, R. woods digital image processing
   [Anonymous], P IEEE C COMP VIS PA
   Barla P., 2012, IMAGE VIDEO BASED AR
   Barrett WA, 2002, ACM T GRAPHIC, V21, P777, DOI 10.1145/566570.566651
   Bradski G., 2000, Opencv. Dr. Dobb's journal of software tools
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Dori D, 1999, IEEE T PATTERN ANAL, V21, P202, DOI 10.1109/34.754586
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Hori O., 1993, Proceedings of the Second International Conference on Document Analysis and Recognition (Cat. No.93TH0578-5), P353, DOI 10.1109/ICDAR.1993.395716
   Inkscape, 2010, OP SOURC LIN WIND SC
   Jeschke S, 2011, COMPUT GRAPH FORUM, V30, P523, DOI 10.1111/j.1467-8659.2011.01877.x
   Kansal R, 2013, WSCG 2013, COMMUNICATION PAPERS PROCEEDINGS, P220
   Kuo-Chin Fan, 1995, Proceedings of the Third International Conference on Document Analysis and Recognition, P627, DOI 10.1109/ICDAR.1995.601974
   Lai YK, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531391
   Lecot G., 2006, P EUR S REND
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Orzan A., 2008, P ACM SIGGRAPH
   Selinger P., 2003, Potrace: A polygon-based tracing algorithm
   Sertl S, 2006, J GLOBAL OPTIM, V34, P569, DOI 10.1007/s10898-005-4384-5
   Sezgin M, 2004, J ELECTRON IMAGING, V13, P146, DOI 10.1117/1.1631315
   Stockman George, 2001, Computer Vision
   Sun J., 2007, P ACM SIGGRAPH 2007
   SVG working group, SVG FORM VECT GRAPH
   Tamura H., 1978, P 4 INT JOINT C PATT
   University of Ballarat, GANSO LIB OPT FUNCT
   Xia T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618461
   Zhang SH, 2009, IEEE T VIS COMPUT GR, V15, P618, DOI 10.1109/TVCG.2009.9
NR 29
TC 3
Z9 4
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2015
VL 31
IS 5
BP 717
EP 732
DI 10.1007/s00371-014-0997-3
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CH2CF
UT WOS:000353831400016
DA 2024-07-18
ER

PT J
AU Apostolakis, KC
   Daras, P
AF Apostolakis, Konstantinos C.
   Daras, Petros
TI A framework for implicit human-centered image tagging inspired by
   attributed affect
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computational Visual Media Conference
CY 2013
CL Geneva, SWITZERLAND
DE Implicit human centered tagging; Affective computing; Gaze tracking
ID RECOGNITION; EXTRACTION
AB In this paper, a framework for implicit human-centered tagging is presented. The proposed framework draws its inspiration from the psychologically established process of attribution. The latter strives to explain affect-related changes observed during an individual's participation in an emotional episode, by bestowing the corresponding affect changing properties on a selected perceived stimulus. Our framework tries to reverse-engineer this attribution process. By monitoring the annotator's focus of attention through gaze-tracking, we identify the stimulus attributed as the cause for the observed change in core affect. The latter is analyzed from the user's facial expressions. Experimental results attained by a lightweight, cost-efficient application based on the proposed framework show promising accuracy in both the assessment of topical relevance and direct annotation scenarios. These results are especially encouraging given the fact that the behavioral analyzers used to obtain user affective response and eye gaze lack the level of sophistication and high cost usually encountered in the related literature.
C1 [Apostolakis, Konstantinos C.; Daras, Petros] Ctr Res & Technol Hellas, Inst Informat Technol, Thessaloniki 57001, Greece.
C3 Centre for Research & Technology Hellas
RP Apostolakis, KC (corresponding author), Ctr Res & Technol Hellas, Inst Informat Technol, Thessaloniki 57001, Greece.
EM kapostol@iti.gr; daras@iti.gr
RI Daras, Petros/F-5284-2012
OI Daras, Petros/0000-0003-3814-6710; Apostolakis,
   Konstantinos/0000-0002-4609-4079
FU European Community [ICT-2011-7-287723]
FX The research leading to this work has received funding from the European
   Community's Seventh Framework Programme (FP7/2007-2013) under grant
   agreement no. ICT-2011-7-287723 (REVERIE project).
CR [Anonymous], 2004, Passive driver gaze tracking with active appearance models
   Arapakis I., 2009, Proceedings of the 17th ACM International Conference on Multimedia, P461, DOI DOI 10.1145/1631272.1631336
   Arapakis I, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P371
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Buscher G, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P67, DOI 10.1145/1571941.1571955
   Chen D., 2008, COMPUTING TECHNIQUES, P39
   Chen JX, 2008, PROCEEDINGS OF THE EYE TRACKING RESEARCH AND APPLICATIONS SYMPOSIUM (ETRA 2008), P189, DOI 10.1145/1344471.1344518
   Cootes T.F., TALKING FACE VIDEO
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Ekman P, 1978, FACIAL ACTION CODING
   Hajimirza SN, 2012, IEEE T MULTIMEDIA, V14, P805, DOI 10.1109/TMM.2012.2186792
   Huang CL, 1997, J VIS COMMUN IMAGE R, V8, P278, DOI 10.1006/jvci.1997.0359
   Jesorsky O, 2001, LECT NOTES COMPUT SC, V2091, P90
   Jiao J., 2010, P 2 INT WORKSH SOC S, P59
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Lee EC, 2009, MACH VISION APPL, V20, P319, DOI 10.1007/s00138-008-0129-z
   LIM MY, 2007, P DOCT CONS 2 INT C, P147
   Marks TK, 2010, IEEE T PATTERN ANAL, V32, P348, DOI 10.1109/TPAMI.2008.278
   Martinez Bedard B., 2008, PHILOS THESES, V42
   Milborrow S., 2010, Pattern Recognition Association of South Africa
   Nordstrom M.M., 2004, DTU INFORM BUILDING, V321
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Pogalin E, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P57
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Russell JA, 2003, PSYCHOL REV, V110, P145, DOI 10.1037/0033-295X.110.1.145
   Sadeghi M., 2009, SPIE MED IMAGING
   Salojärvi J, 2005, LECT NOTES COMPUT SC, V3696, P513, DOI 10.1007/11550822_80
   Shan MK, 2009, EXPERT SYST APPL, V36, P7666, DOI 10.1016/j.eswa.2008.09.042
   Simon D, 2008, PAIN, V135, P55, DOI 10.1016/j.pain.2007.05.008
   Smith C, 1997, PSYCHOL FACIAL EXPRE, V229
   Soleymani M, 2012, IEEE SYS MAN CYBERN, P3304, DOI 10.1109/ICSMC.2012.6378301
   Soleymani M, 2012, IEEE T AFFECT COMPUT, V3, P211, DOI 10.1109/T-AFFC.2011.37
   Soleymani M, 2012, IEEE T AFFECT COMPUT, V3, P42, DOI 10.1109/T-AFFC.2011.25
   Strupp S, 2008, LECT NOTES ARTIF INT, V5243, P356, DOI 10.1007/978-3-540-85845-4_44
   Talbot J., 2006, IMPLEMENTING GRABUT
   Terissi LD, 2010, J UNIVERS COMPUT SCI, V16, P903
   Tkalcic M, 2013, IEEE T MULTIMEDIA, V15, P391, DOI 10.1109/TMM.2012.2229970
   Vinciarelli A, 2009, IEEE INT CON MULTI, P1428, DOI 10.1109/ICME.2009.5202770
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Vrochidis S., 2011, Proceedings of the 1st ACM international conference on multimedia retrieval, P43
   Yik M, 2011, EMOTION, V11, P705, DOI 10.1037/a0023980
   Yik M, 2009, J PERS ASSESS, V91, P416, DOI 10.1080/00223890903087596
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zhu J, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P131, DOI 10.1109/AFGR.2002.1004144
NR 45
TC 2
Z9 2
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2014
VL 30
IS 10
BP 1093
EP 1106
DI 10.1007/s00371-013-0903-4
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AP6NF
UT WOS:000342193800004
DA 2024-07-18
ER

PT J
AU Paul, PP
   Gavrilova, M
   Klimenko, S
AF Paul, Padma Polash
   Gavrilova, Marina
   Klimenko, Stanislav
TI Situation awareness of cancelable biometric system
SO VISUAL COMPUTER
LA English
DT Article
DE Biometric fusion; Situation awareness; Cancelable biometrics security;
   Multimodal cancelable biometrics; Cross-folding; Cancelable fusion
ID EIGENFACES; SECURE
AB Recently, cancelable biometrics emerged as one of the highly effective methods of template protection. The concept behind the cancelable biometrics or cancelability is a transformation of a biometric data or extracted feature into an alternative form, which cannot be used by the imposter or intruder easily, and can be revoked if compromised. In this paper, we present a novel architecture for template generation in the context of situation awareness system in real and virtual applications. We develop a novel cancelable biometric template generation algorithm utilizing random biometric fusion, random projection and selection. Proposed random cross-folding method generate cancelable biometric template from multiple biometric traits. We further validate the performance of the proposed algorithm using a virtual multimodal face and ear database.
C1 [Paul, Padma Polash; Gavrilova, Marina] 2500 Univ Calgary, Dept Comp Sci, Calgary, AB, Canada.
   [Klimenko, Stanislav] ICPT, Moscow, Russia.
   [Klimenko, Stanislav] MIPT SU, Moscow, Russia.
C3 Moscow Institute of Physics & Technology
RP Paul, PP (corresponding author), 2500 Univ Calgary, Dept Comp Sci, Calgary, AB, Canada.
EM pppaul@ucalgary.ca; marina@cpsc.ucalgary.ca;
   stanislav.klimenko@gmail.com
RI Gavrilova, Marina/AHD-3605-2022
OI Gavrilova, Marina/0000-0002-5338-1834
FU NSERC; URGC Cybersecurity Seed Grant; RFBR grant [11-07-00329]
FX Authors would like to acknowledge the support of NSERC, URGC
   Cybersecurity Seed Grant, and RFBR grant 11-07-00329 for partial funding
   of this project.
CR Achlioptas D, 2011, ACM SIGACT SIGMOD SI
   Adler A, 2003, CCECE 2003: CANADIAN CONFERENCE ON ELECTRICAL AND COMPUTER ENGINEERING, VOLS 1-3, PROCEEDINGS, P1163
   Ahmadian K, 2013, VISUAL COMPUT, V29, P123, DOI 10.1007/s00371-012-0741-9
   [Anonymous], 2012, INT J SOFTW SCI COMP, DOI DOI 10.4018/jssci.2012070102
   [Anonymous], IEEE INT S INF THEOR
   [Anonymous], 2003, Handbook of fingerprint recognition
   [Anonymous], 2007, HDB BIOMETRICS HDB B
   [Anonymous], 2006, Handbook of Multibiometrics
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Down MP., 2004, Inf Syst Control J, V4, P53
   Eriksson A., 2007, EUR C SPEECH TECHN R
   Feng YC, 2010, IEEE T INF FOREN SEC, V5, P103, DOI 10.1109/TIFS.2009.2038760
   Gavrilova M., 2011, INT J INFORM TECHNOL, V11, P18
   Gavrilova ML, 2009, INT J BIOMETRICS, V1, P329, DOI 10.1504/IJBM.2009.024277
   Goh A., 2003, 7 IFIP TC6 TC11 C CO
   Jain AK, 2008, EURASIP J ADV SIG PR, DOI 10.1155/2008/579416
   Jin ATB, 2004, PATTERN RECOGN, V37, P2245, DOI 10.1016/j.patcog.2004.04.011
   Johnson W. B., 1984, INT C MOD AN PROB
   Juels A, 1999, P 6 ACM C COMP COMM
   Kirillov I.A., 2012, P INT C CYB WORLDS
   Matsumoto T., 2002, P SPIE
   Matsumoto T, 2004, P S CRYPT INF SEC IW
   Monwar MM, 2013, SIGNAL IMAGE VIDEO P, V7, P137, DOI 10.1007/s11760-011-0226-8
   Monwar MM, 2009, IEEE T SYST MAN CY B, V39, P867, DOI 10.1109/TSMCB.2008.2009071
   Paul P. P., 2012, Proceedings of the 2012 11th IEEE International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC), P43, DOI 10.1109/ICCI-CC.2012.6311208
   Phillips PJ, 1998, IMAGE VISION COMPUT, V16, P295, DOI 10.1016/S0262-8856(97)00070-X
   Pillai JK, 2010, INT CONF ACOUST SPEE, P1838, DOI 10.1109/ICASSP.2010.5495383
   Ratha NK, 2007, IEEE T PATTERN ANAL, V29, P561, DOI 10.1109/TPAMI.2007.1004
   Ross A, 2007, IEEE T PATTERN ANAL, V29, P544, DOI 10.1109/TPAMI.2007.1018
   Samaria F. S., 1994, Proceedings of the Second IEEE Workshop on Applications of Computer Vision (Cat. No.94TH06742), P138, DOI 10.1109/ACV.1994.341300
   Sanderson C, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P997, DOI 10.1109/ICIP.2002.1039143
   Soliverez C. E., 1985, Revista Mexicana de Fisica, V31, P743
   Tina Y., 2011, IEEE INT C COGN INF
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Wang Y, 2012, INT J COGN INFORM NA, V6, P1, DOI 10.4018/jcini.2012100101
   Zhichun M., 2003, 4 CHIN C BIOM REC
NR 36
TC 18
Z9 19
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2014
VL 30
IS 9
BP 1059
EP 1067
DI 10.1007/s00371-013-0907-0
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AO5GP
UT WOS:000341372200009
DA 2024-07-18
ER

PT J
AU Bu, SH
   Han, PC
   Liu, ZB
   Li, K
   Han, JW
AF Bu, Shuhui
   Han, Pengcheng
   Liu, Zhenbao
   Li, Ke
   Han, Junwei
TI Shift-invariant ring feature for 3D shape
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE 3D shape descriptor; Sparse coding; Shape correspondence; Shape
   retrieval
ID RETRIEVAL
AB In this paper, we present a shift-invariant ring feature for 3D shape, which can encode multiple low-level descriptors and provide high-discriminative representation of local region for 3D shape. First, several iso-geodesic rings are created at equal intervals, and then low-level descriptors on the sampling rings are used to represent the property of a feature point. In order to boost the descriptive capability of raw descriptors, we formulate the unsupervised basis learning into an L1-penalized optimization problem, which uses convolution operation to address the rotation ambiguity of descriptors resulting from different starting points in rings. In the following extraction procedure of high-level feature, we use the learned bases to calculate the sparse coefficients by solving the optimization problem. Furthermore, to make the coefficients irrelevant with the sequential order in ring, we use Fourier transform to achieve circular-shift invariant ring feature. Experiments on 3D shape correspondence and retrieval demonstrate the satisfactory performance of the proposed intrinsic feature.
C1 [Bu, Shuhui; Han, Pengcheng; Liu, Zhenbao; Han, Junwei] Northwestern Polytech Univ, Xian 710072, Peoples R China.
   [Li, Ke] Informat Engn Univ, Zhengzhou, Peoples R China.
C3 Northwestern Polytechnical University; PLA Information Engineering
   University
RP Liu, ZB (corresponding author), Northwestern Polytech Univ, Xian 710072, Peoples R China.
EM bushuhui@nwpu.edu.cn; liuzhenbao@nwpu.edu.cn
RI Li, Ke/HZM-6170-2023
OI Li, Ke/0000-0002-7873-1554
FU National Natural Science Foundation of China [61202185, 61003137,
   41201390]; Northwestern Polytechnical University Basic Research Fund
   [JC201202, JC201220]; Fundamental Research Funds for the Central
   Universities; Shaanxi Natural Science Fund [2012JQ8037]; Open Project
   Program of the State Key Lab of CAD&CG, Zhejiang University [A1306]
FX This work was supported partly by grants from National Natural Science
   Foundation of China (61202185, 61003137, 41201390), Northwestern
   Polytechnical University Basic Research Fund (JC201202, JC201220), the
   Fundamental Research Funds for the Central Universities, Shaanxi Natural
   Science Fund (2012JQ8037), and Open Project Program of the State Key Lab
   of CAD&CG (A1306), Zhejiang University.
CR Agathos A, 2010, VISUAL COMPUT, V26, P1301, DOI 10.1007/s00371-010-0523-1
   [Anonymous], 2012, ARXIV12065241
   Ben-Chen M., 2008, Proceedings of the 1st Eurographics Conference on 3D Object Retrieval, P1
   Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   Bronstein MM, 2010, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2010.5539838
   Castellani U, 2008, COMPUT GRAPH FORUM, V27, P643, DOI 10.1111/j.1467-8659.2008.01162.x
   Darom T, 2012, IEEE T IMAGE PROCESS, V21, P2758, DOI 10.1109/TIP.2012.2183142
   Del Bimbo A, 2006, ACM T MULTIM COMPUT, V2, P20
   Dubrovina A., 2010, P S 3D DAT PROC, V2
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Hu JX, 2009, VISUAL COMPUT, V25, P667, DOI 10.1007/s00371-009-0340-6
   Jain V, 2007, COMPUT AIDED DESIGN, V39, P398, DOI 10.1016/j.cad.2007.02.009
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kazhdan M., 2003, Symposium on Geometry Processing, P156
   Kim VG, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964974
   Kimmel R, 1998, P NATL ACAD SCI USA, V95, P8431, DOI 10.1073/pnas.95.15.8431
   Knopp J, 2010, LECT NOTES COMPUT SC, V6316, P589, DOI 10.1007/978-3-642-15567-3_43
   Kokkinos I, 2012, PROC CVPR IEEE, P159, DOI 10.1109/CVPR.2012.6247671
   Kovnatsky A., 2012, EUR WORKSH 3D OBJ RE, P1
   Laga H., 2011, EUR WORKSH 3D OBJ RE
   Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482
   Lian ZH, 2013, PATTERN RECOGN, V46, P449, DOI 10.1016/j.patcog.2012.07.014
   Liu ZB, 2013, J COMPUT SCI TECH-CH, V28, P836, DOI 10.1007/s11390-013-1382-9
   López-Sastre RJ, 2013, COMPUT GRAPH-UK, V37, P473, DOI 10.1016/j.cag.2013.04.003
   Mykhalchuk V, 2013, COMPUT GRAPH-UK, V37, P539, DOI 10.1016/j.cag.2013.04.005
   Ng, 2007, ADV NEURAL INF PROCE, P801
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Peyré G, 2006, INT J COMPUT VISION, V69, P145, DOI 10.1007/s11263-006-6859-3
   Sfikas K, 2012, VISUAL COMPUT, V28, P943, DOI 10.1007/s00371-012-0714-z
   Shapira L, 2010, INT J COMPUT VISION, V89, P309, DOI 10.1007/s11263-009-0279-0
   Shapira L, 2008, VISUAL COMPUT, V24, P249, DOI 10.1007/s00371-007-0197-5
   Shen Y.R., 2003, PRINCIPLES NONLINEAR, P1
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Siddiqi K, 2008, MACH VISION APPL, V19, P261, DOI 10.1007/s00138-007-0097-8
   Sipiran I, 2013, COMPUT GRAPH-UK, V37, P460, DOI 10.1016/j.cag.2013.04.002
   Smith E, 2005, NEURAL COMPUT, V17, P19, DOI 10.1162/0899766052530839
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0
   Tevs A, 2011, COMPUT GRAPH FORUM, V30, P543, DOI 10.1111/j.1467-8659.2011.01879.x
   Wu HY, 2010, PROC CVPR IEEE, P438, DOI 10.1109/CVPR.2010.5540180
NR 41
TC 10
Z9 10
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 867
EP 876
DI 10.1007/s00371-014-0970-1
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700028
DA 2024-07-18
ER

PT J
AU Zhong, M
   Qin, H
AF Zhong, Ming
   Qin, Hong
TI Sparse approximation of 3D shapes via spectral graph wavelets
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Sparse approximation; Spectral graph wavelets; Mesh compression
ID COMPRESSION
AB This paper investigates the compressive representation of 3D meshes and articulates a novel sparse approximation method for 3D shapes based on spectral graph wavelets. The originality of this paper is centering on the first attempt of exploiting spectral graph wavelets in the sparse representation for 3D shape geometry. Conventional spectral mesh compression employs the eigenfunctions of mesh Laplacian as shape bases. The Laplacian eigenbases, generalizing the Fourier bases from Euclidean domain to manifold, exhibit global support and are neither efficient nor precise in representing local geometry. To ameliorate, we advocate an innovative approach to 3D mesh compression using spectral graph wavelets as dictionary to encode mesh geometry. In contrast to Laplacian eigenbases, the spectral graph wavelets are locally defined at individual vertices and can better capture local shape information in a more accurate way. Nonetheless, the multiscale spectral graph wavelets form a redundant dictionary as shape bases, therefore we formulate the compression of 3D shape as a sparse approximation problem that can be readily handled by powerful algorithms such as orthogonal matching pursuit. Various experiments demonstrate that our method is superior to the existing spectral mesh compression methods.
C1 [Zhong, Ming; Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook
RP Zhong, M (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM mzhong@cs.stonybrook.edu; qin@cs.stonybrook.edu
FU National Science Foundation of USA [IIS-0949467, IIS-1047715,
   IIS-1049448]; National Natural Science Foundation of China [61190120,
   61190121, 61190125]; Div Of Information & Intelligent Systems; Direct
   For Computer & Info Scie & Enginr [1047715] Funding Source: National
   Science Foundation
FX This research is supported in part by National Science Foundation of USA
   (Grant Nos. IIS-0949467, IIS-1047715, and IIS-1049448), and National
   Natural Science Foundation of China (Grant Nos. 61190120, 61190121,
   61190125).
CR Antoine JP, 2010, APPL COMPUT HARMON A, V28, P189, DOI 10.1016/j.acha.2009.10.002
   Coifman RR, 2006, APPL COMPUT HARMON A, V21, P53, DOI 10.1016/j.acha.2006.04.004
   Donoho DL, 2012, IEEE T INFORM THEORY, V58, P1094, DOI 10.1109/TIT.2011.2173241
   Gavish Matan, 2010, P 27 INT C INT C MAC, P367
   Gumhold S., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P133, DOI 10.1145/280814.280836
   Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Karni Z., 2001, Graphics Interface, V1, P1
   Karypis G, 1998, SIAM J SCI COMPUT, V20, P359, DOI 10.1137/S1064827595287997
   Li CY, 2013, VISUAL COMPUT, V29, P513, DOI 10.1007/s00371-013-0815-3
   Mahadevan S., 2007, Proceedings of the 24th International Conference on Machine Learning, ICML '07, P585
   MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082
   PATI YC, 1993, CONFERENCE RECORD OF THE TWENTY-SEVENTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P40, DOI 10.1109/ACSSC.1993.342465
   Ram I, 2012, IEEE SIGNAL PROC LET, V19, P291, DOI 10.1109/LSP.2012.2190983
   Rossignac J, 1999, IEEE T VIS COMPUT GR, V5, P47, DOI 10.1109/2945.764870
   Skodras A, 2001, IEEE SIGNAL PROC MAG, V18, P36, DOI 10.1109/79.952804
   Tropp JA, 2006, SIGNAL PROCESS, V86, P572, DOI 10.1016/j.sigpro.2005.05.030
   Vallet B, 2008, COMPUT GRAPH FORUM, V27, P251, DOI 10.1111/j.1467-8659.2008.01122.x
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wardetzky M., 2007, P EUROGRAPHICS S GEO, P33, DOI [DOI 10.2312/SGP/SGP07/033-037, 10.2312/SGP/SGP07/033-037]
   Zhang H, 2010, COMPUT GRAPH FORUM, V29, P1865, DOI 10.1111/j.1467-8659.2010.01655.x
NR 21
TC 11
Z9 13
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 751
EP 761
DI 10.1007/s00371-014-0971-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700018
DA 2024-07-18
ER

PT J
AU Curtis, S
   Zafar, B
   Gutub, A
   Manocha, D
AF Curtis, Sean
   Zafar, Basim
   Gutub, Adnan
   Manocha, Dinesh
TI Right of way
SO VISUAL COMPUTER
LA English
DT Article
DE Pedestrian dynamics; Crowd simulation; Multiagent models; Asymmetric
   responses
ID PEDESTRIAN FLOW-THROUGH; SOCIAL FORCE MODEL; SIMULATION; DYNAMICS
AB Pedestrian models typically represent interactions between agents in a symmetric fashion. In general, these symmetric relationships are valid for a large number of crowd simulation scenarios. However, there are many cases in which symmetric responses between agents are inappropriate, leading to unrealistic behavior or undesirable simulation artifacts. We present a novel formulation, called right of way, which provides a well-disciplined mechanism for modeling asymmetric relationships between pedestrians. Right of way is a general principle, which can be applied to different types of pedestrian models. We illustrate this by applying right of way to three different pedestrian models (two based on social forces and one based on velocity obstacles) and show its impact in multiple scenarios. Particularly, we show how it enables simulation of the complex relationships exhibited by pilgrims performing the Islamic religious ritual, the Tawaf.
C1 [Curtis, Sean; Manocha, Dinesh] Univ N Carolina, Chapel Hill, NC 27515 USA.
   [Zafar, Basim] Umm Al Qura Univ, Hajj Res Inst, Mecca, Saudi Arabia.
   [Gutub, Adnan] Umm al Qura Univ, Ctr Res Excellence Hajj & Omrah HajjCoRE, Mecca, Saudi Arabia.
C3 University of North Carolina; University of North Carolina Chapel Hill;
   Umm Al Qura University; Umm Al Qura University
RP Curtis, S (corresponding author), Univ N Carolina, Chapel Hill, NC 27515 USA.
EM seanc@cs.unc.edu; Bjzafar@uqu.edu.sa
RI Gutub, Adnan Abdul-Aziz/O-1240-2016
OI Gutub, Adnan Abdul-Aziz/0000-0003-0923-202X; Manocha,
   Dinesh/0000-0001-7047-9801
FU Directorate For Engineering; Div Of Civil, Mechanical, & Manufact Inn
   [1000579] Funding Source: National Science Foundation
CR [Anonymous], 2009, INT S ROB RES
   Blue V., 1999, TRANSPORT RES REC, V1678, P135, DOI DOI 10.3141/1678-17
   Blue VJ, 1998, TRANSPORT RES REC, P29, DOI 10.3141/1644-04
   Burstedde C, 2001, PHYSICA A, V295, P507, DOI 10.1016/S0378-4371(01)00141-8
   Chraibi M, 2011, NETW HETEROG MEDIA, V6, P425, DOI 10.3934/nhm.2011.6.425
   Chraibi M, 2010, PHYS REV E, V82, DOI 10.1103/PhysRevE.82.046111
   Curtis S, 2011, 1 IEEE WORKSH MOD SI
   Durupinar F, 2011, IEEE COMPUT GRAPH, V31, P22, DOI 10.1109/MCG.2009.105
   Fiorini P, 1998, INT J ROBOT RES, V17, P760, DOI 10.1177/027836499801700706
   Funge J, 1999, COMP GRAPH, P29, DOI 10.1145/311535.311538
   Guy S.J., 2011, P 2011 ACM SIGGRAPH, P43, DOI [10.1145/2019406.2019413, DOI 10.1145/2019406.2019413]
   Guy S.J., 2010, S COMP AN ACM NEW YO
   Guy S.J., 2010, P 9 INT C AUTONOMOUS, V2, P575
   Helbing D, 2000, NATURE, V407, P487, DOI 10.1038/35035023
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   Hirai K., 1975, Proceedings of the 1975 International Conference on Cybernetics and Society, P409
   Johansson A., 2007, Adv. Complex Syst, V10, P271, DOI [10.1142/S0219525907001355, DOI 10.1142/S0219525907001355]
   Ju E, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866162
   Karamouzas I, 2009, LECT NOTES COMPUT SC, V5884, P41, DOI 10.1007/978-3-642-10347-6_4
   Kretz T, 2006, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2006/10/P10001
   Lee KH, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P109
   Moussaïd M, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010047
   Muller K., 1981, THESIS TH O VONGUERI
   Nagai R, 2006, PHYSICA A, V367, P449, DOI 10.1016/j.physa.2005.11.031
   Narain R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618468
   Ondrej J., 2010, P SIGGRAPH, P123
   Patil S., 2010, IEEE T VISUALIZATION, P244
   Pelechano N., 2007, SCA07
   Pettre J., 2009, Proceedings of the 2009 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '09, P189, DOI DOI 10.1145/1599470.1599495
   Plaue M., 2011, P 2011 ISPRS C PHOT
   Reynolds Craig W., 1987, SIGGRAPH
   Schadschneider A, 2002, PEDESTRIAN AND EVACUATION DYNAMICS, P75
   Seyfried A, 2009, TRANSPORT SCI, V43, P395, DOI 10.1287/trsc.1090.0263
   Treuille A, 2006, ACM T GRAPHIC, V25, P1160, DOI 10.1145/1141911.1142008
   Ulicny B, 2002, COMPUT GRAPH FORUM, V21, P767, DOI 10.1111/1467-8659.00634
   Yamamoto K, 2007, PHYSICA A, V379, P654, DOI 10.1016/j.physa.2007.02.040
   YEH H., 2008, SCA 08 P 2005 ACM SI, P39
   Yu QX, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P119
   Yu WJ, 2005, PHYS REV E, V72, DOI 10.1103/PhysRevE.72.026112
   Zafar B., 2011, ANAL MATAF FAMADAN 1
   Zanlungo F, 2011, EPL-EUROPHYS LETT, V93, DOI 10.1209/0295-5075/93/68005
   Zhang J., 2011, J STAT MECH, V2011
   Zhang J., 2012, J STAT MECH, V2012
NR 43
TC 40
Z9 43
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2013
VL 29
IS 12
BP 1277
EP 1292
DI 10.1007/s00371-012-0769-x
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 251LB
UT WOS:000326929200005
DA 2024-07-18
ER

PT J
AU Bin Masood, T
   Thomas, DM
   Natarajan, V
AF Bin Masood, Talha
   Thomas, Dilip Mathew
   Natarajan, Vijay
TI Scalar field visualization via extraction of symmetric structures
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Scalar field visualization; Symmetry detection; Query based exploration
AB Identifying symmetry in scalar fields is a recent area of research in scientific visualization and computer graphics communities. Symmetry detection techniques based on abstract representations of the scalar field use only limited geometric information in their analysis. Hence they may not be suited for applications that study the geometric properties of the regions in the domain. On the other hand, methods that accumulate local evidence of symmetry through a voting procedure have been successfully used for detecting geometric symmetry in shapes. We extend such a technique to scalar fields and use it to detect geometrically symmetric regions in synthetic as well as real-world datasets. Identifying symmetry in the scalar field can significantly improve visualization and interactive exploration of the data. We demonstrate different applications of the symmetry detection method to scientific visualization: query-based exploration of scalar fields, linked selection in symmetric regions for interactive visualization, and classification of geometrically symmetric regions and its application to anomaly detection.
C1 [Bin Masood, Talha; Thomas, Dilip Mathew] Indian Inst Sci, Dept Comp Sci & Automat, Bangalore 560012, Karnataka, India.
   [Natarajan, Vijay] Indian Inst Sci, Dept Comp Sci & Automat, Supercomp Educ & Res Ctr, Bangalore 560012, Karnataka, India.
C3 Indian Institute of Science (IISC) - Bangalore; Indian Institute of
   Science (IISC) - Bangalore
RP Bin Masood, T (corresponding author), Indian Inst Sci, Dept Comp Sci & Automat, Bangalore 560012, Karnataka, India.
EM tbmasood@csa.iisc.ernet.in; dilip@csa.iisc.ernet.in;
   vijayn@csa.iisc.ernet.in
OI Thomas, Dilip/0000-0002-1988-2678; Bin Masood, Talha/0000-0001-5352-1086
FU Department of Science and Technology, India [SR/S3/EECE/0086/2012]
FX This work was supported by a grant from Department of Science and
   Technology, India (SR/S3/EECE/0086/2012).
CR Alliez P, 2003, ACM T GRAPHIC, V22, P485, DOI 10.1145/882262.882296
   [Anonymous], 1996, P AAAI INT C KNOWL D
   Carr H, 2003, COMP GEOM-THEOR APPL, V24, P75, DOI 10.1016/S0925-7721(02)00093-7
   Chazal F, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1021
   Cohen-steiner D., 2002, P S COMP GEOM, P312
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Correa CD, 2011, IEEE T VIS COMPUT GR, V17, P1842, DOI 10.1109/TVCG.2011.244
   Gyulassy A, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P535
   Hong Y, 2008, COMPUT GRAPH-UK, V32, P41, DOI 10.1016/j.cag.2007.12.003
   Kazhdan M, 2004, ALGORITHMICA, V38, P201, DOI 10.1007/s00453-003-1050-5
   Kerber J., 2011, VISION MODELING VISU
   Kerber J., 2010, VISION MODELING VISU
   Lorensen W. E., 1987, COMPUTER GRAPHICS, V21, P163, DOI 10.1145/37401.37422
   Mitra N. J., 2012, P EUROGRAPHICS STATE, P29
   Mitra NJ, 2006, ACM T GRAPHIC, V25, P560, DOI 10.1145/1141911.1141924
   Thomas DM, 2011, IEEE T VIS COMPUT GR, V17, P2035, DOI 10.1109/TVCG.2011.236
NR 16
TC 5
Z9 5
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 761
EP 771
DI 10.1007/s00371-013-0828-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400029
DA 2024-07-18
ER

PT J
AU Wu, JZ
   Zheng, CW
   Hu, XH
   Xu, FJ
AF Wu, Jiaze
   Zheng, Changwen
   Hu, Xiaohui
   Xu, Fanjiang
TI Rendering realistic spectral bokeh due to lens stops and aberrations
SO VISUAL COMPUTER
LA English
DT Article
DE Bokeh effect; Lens stops; Lens aberrations; Dispersive lens model;
   Spectral rendering
AB Creating bokeh effect in synthesized images can improve photorealism and emphasize interesting subjects. Therefore, we present a novel method for rendering realistic bokeh effects, especially chromatic effects, which are absent for existing methods. This new method refers to two key techniques: an accurate dispersive lens model and an efficient spectral rendering scheme. This lens model is implemented based on optical data of real lenses and considers wavelength dependency of physical lenses by introducing a sequential dispersive ray tracing algorithm inside this model. This spectral rendering scheme is proposed to support rendering of lens dispersion and integration between this new model and bidirectional ray tracing. The rendering experiments demonstrate that our method is able to simulate realistic spectral bokeh effects caused by lens stops and aberrations, especially chromatic aberration, and feature high rendering efficiency.
C1 [Wu, Jiaze; Zheng, Changwen; Hu, Xiaohui; Xu, Fanjiang] Chinese Acad Sci, Inst Software, Natl Key Lab Integrated Informat Syst Technol, Beijing, Peoples R China.
   [Wu, Jiaze] Chinese Acad Sci, Grad Univ, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS
RP Wu, JZ (corresponding author), Chinese Acad Sci, Inst Software, Natl Key Lab Integrated Informat Syst Technol, Beijing, Peoples R China.
EM wujiaze05@gmail.com
FU National High-Tech Research and Development Plan of China [2009AA01Z303]
FX We thank the LuxRender community and anonymous provider for the chess
   and bubble scenes. This work was partly supported by the National
   High-Tech Research and Development Plan of China (Grant No.
   2009AA01Z303).
CR Ang T., 2002, DICT PHOTOGRAPHY DIG
   [Anonymous], EUR 2002 EUR ASS SEP
   Born M., 2013, PRINCIPLES OPTICS
   Buhler J., 2002, ACM SIGGRAPH 2002 conference abstracts and applications, P142
   Evans GF, 1999, PROC GRAPH INTERF, P42
   Fischer RE, 2008, P SOC PHOTO-OPT INS, V7068, DOI 10.1117/12.796840
   Hachisuka T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360632
   Haeberli P., 1990, Computer Graphics, V24, P309, DOI 10.1145/97880.97913
   Kass M., 2006, TECHNICAL REPORT
   Kelemen C, 2002, COMPUT GRAPH FORUM, V21, P531, DOI 10.1111/1467-8659.t01-1-00703
   Kodama K., 2006, ACM SIGGRAPH 2006 Research posters, P77
   Kolb C., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P317, DOI 10.1145/218380.218463
   Kosloff T.J., 2009, GI 09, P39
   Kraus M., 2007, COMPUTER GRAPHICS FO, V26
   Laikin M., 2001, Lens Design, V3rd
   Lanman D., 2008, P INT S COMP AESTH G, P102
   Lee S, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778802
   Lee S, 2008, COMPUT GRAPH FORUM, V27, P1955, DOI 10.1111/j.1467-8659.2008.01344.x
   Lee Sungkil, 2009, ACM T GRAPHIC, V28, P1
   Merklinger H. M., 1997, PHOTO TECHN, V18, P37
   Overbeck RS, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618486
   Parker SG, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778803
   Potmesil M., 1981, Computer Graphics, V15, P297, DOI 10.1145/965161.806818
   Riguer G., 2003, ShaderX2: Shader Programming Tips and Tricks with DirectX, V9, P529
   Smith W., 1992, MODERN LENS DESIGN
   Soler C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1516522.1516529
   SUN Y, 2000, INT C COL GRAPH IM P
   Sun YL, 2001, VISUAL COMPUT, V17, P429, DOI 10.1007/s003710100116
   Thomas S.W., 1986, VISUAL COMPUT, V2, P3
   van Walree P., CHROMATIC ABERRATION
   van Walree Paul., Vignetting
   Veach E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P65, DOI 10.1145/258734.258775
   Veach E., 1998, ROBUST MONTE CARLO M
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wikipedia, BOK
   Wikipedia, DISP OPT
   Wu JZ, 2010, VISUAL COMPUT, V26, P555, DOI 10.1007/s00371-010-0459-5
   Ying Yuan, 1988, Visual Computer, V4, P259, DOI 10.1007/BF01901281
   ZEMAX, ZEM SOFTW OPT SYST D
NR 39
TC 17
Z9 21
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2013
VL 29
IS 1
BP 41
EP 52
DI 10.1007/s00371-012-0673-4
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 064HB
UT WOS:000313063400004
DA 2024-07-18
ER

PT J
AU Huang, H
   Fu, TN
   Li, CF
AF Huang, Hua
   Fu, Tian-Nan
   Li, Chen-Feng
TI Painterly rendering with content-dependent natural paint strokes
SO VISUAL COMPUTER
LA English
DT Article
DE Non-photorealistic rendering; Painterly rendering; Stroke; Lighting
   imitation
ID FIELD DESIGN
AB We present a new painterly rendering method that simulates artists' content-dependent painting process and the natural variation of hand-painted strokes. First, a new stroke layout strategy is proposed to enhance the contrast between large and small paint strokes, which is an important characteristic of hand-painted paintings. Specifically, the input image is partitioned into nonuniform grids according to its importance map, and determined by the grid size, an individually constructed paint stroke is applied in each grid. Second, an anisotropic digital brush is designed to simulate a real paint brush. In particular, each bristle of the digital brush has an individual color, so that strokes rendered by the new brush can have multiple colors and naturally varied textures. Finally, we present a novel method to add lighting effects to the canvas. This lighting imitation method is robust and very easy to implement, and it can significantly improve the quality of rendering. Comparing with traditional painterly rendering approaches, the new method simulates more closely the real painting procedure, and our experimental results show that it can produce vivid paintings with fewer artifacts.
C1 [Huang, Hua; Fu, Tian-Nan] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
   [Li, Chen-Feng] Swansea Univ, Sch Engn, Swansea, W Glam, Wales.
C3 Xi'an Jiaotong University; Swansea University
RP Huang, H (corresponding author), Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
EM huanghua@mail.xjtu.edu.cn; ftnmail@gmail.com; c.f.li@swansea.ac.uk
RI Li, Mengqi/AAG-6804-2021; Huang, Hua/M-9684-2013; Li,
   Chenfeng/D-4034-2014; Li, Chenfeng/AFQ-6554-2022
OI Huang, Hua/0000-0003-2587-1702; Li, Chenfeng/0000-0003-0441-211X
FU National Natural Science Foundation of China [60970068]; Program for New
   Century Excellent Talents in University [NCET-09-0635]; Royal Society
   through the Royal Society/NSFC [JP100987]
FX This work was supported in part by grants from the National Natural
   Science Foundation of China (No. 60970068) and Program for New Century
   Excellent Talents in University under Project NCET-09-0635. The author
   would also like to thank the support from the Royal Society through the
   Royal Society/NSFC International Joint Project (JP100987).
CR BORS AG, 2001, ONL S EL ENG DSP ALG, V1
   Curtis C. J., 1997, Proceedings of the 24th annual conference on Computer graphics and interactive techniques, P421
   GOOCH B, 2002, P 2 INT S NONPH AN R
   HAEBERLI P, 1990, COMPUT GRAPH, V24, P214
   Hays J., 2004, PROC NPAR 01, P113
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Hertzmann A., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P453, DOI 10.1145/280814.280951
   Hertzmann A., 2002, NPAR, P91, DOI 10.1145/508530.508546
   Hertzmann A., 2000, NPAR, P7
   HUANG H, 2010, P COMP GRAPH INT COM
   Huang H, 2010, COMPUT GRAPH FORUM, V29, P2055, DOI 10.1111/j.1467-8659.2010.01792.x
   Huang H, 2010, VISUAL COMPUT, V26, P933, DOI 10.1007/s00371-010-0498-y
   KAGAYA M, 2010, IEEE T VIS COMPUT GR
   Lee H, 2009, COMPUT GRAPH FORUM, V28, P1207, DOI 10.1111/j.1467-8659.2009.01498.x
   Lin L., 2010, Proc. NPAR '10, P73, DOI DOI 10.1145/1809939.1809948
   Litwinowicz P., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P407, DOI 10.1145/258734.258893
   OLSEN S, 2005, P GRAPH INT 2005 CAN, P247
   Streit L, 1998, COMPUT GRAPH FORUM, V17, pC207, DOI 10.1111/1467-8659.00268
   Wang B, 2004, IEEE T VIS COMPUT GR, V10, P266, DOI 10.1109/TVCG.2004.1272726
   WINKENBACH G, 1994, P 21 ANN C COMP GRAP, P100
   Zeng K, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640445
   Zhang E, 2007, IEEE T VIS COMPUT GR, V13, P94, DOI 10.1109/TVCG.2007.16
   Zhang E, 2006, ACM T GRAPHIC, V25, P1294, DOI 10.1145/1183287.1183290
   ZHANG S, 2009, ONLINE VIDEO STREAM
   Zhang SH, 2009, SCI CHINA SER F, V52, P162, DOI 10.1007/s11432-009-0035-7
   Zhao M., 2010, PROC INT S NONPHOTOR, P99, DOI DOI 10.1145/1809939.1809951
NR 26
TC 23
Z9 29
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2011
VL 27
IS 9
BP 861
EP 871
DI 10.1007/s00371-011-0596-5
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 807TK
UT WOS:000293922400006
DA 2024-07-18
ER

PT J
AU Lee, H
   Dikici, Ç
   Lavoué, G
   Dupont, F
AF Lee, Ho
   Dikici, Cagatay
   Lavoue, Guillaume
   Dupont, Florent
TI Joint reversible watermarking and progressive compression of 3D meshes
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Reversible watermarking; 3D-mesh; Compression
ID ROBUST
AB A new reversible 3D mesh watermarking scheme is proposed in conjunction with progressive compression. Progressive 3D mesh compression permits a progressive refinement of the model from a coarse to a fine representation by using different levels of detail (LoDs). A reversible watermark is embedded into all refinement levels such that (1) the refinement levels are copyright protected, and (2) an authorized user is able to reconstruct the original 3D model after watermark extraction, hence reversible. The progressive compression considers a connectivity-driven algorithm to choose the vertices that are to be refined for each LoD. The proposed watermarking algorithm modifies the geometry information of these vertices based on histogram bin shifting technique. An authorized user can extract the watermark in each LoD and recover the original 3D mesh, while an unauthorized user which has access to the decompression algorithm can only reconstruct a distorted version of the 3D model. Experimental results show that the proposed method is robust to several attack scenarios while maintaining a good compression ratio.
C1 [Lee, Ho; Dupont, Florent] Univ Lyon 1, CNRS, LIRIS, UMR5205, F-69622 Lyon, France.
   [Dikici, Cagatay; Lavoue, Guillaume] Univ Lyon, CNRS, INSA Lyon, LIRIS,UMR5205, F-69621 Villeurbanne, France.
C3 Institut National des Sciences Appliquees de Lyon - INSA Lyon; Centre
   National de la Recherche Scientifique (CNRS); Universite Claude Bernard
   Lyon 1; Centre National de la Recherche Scientifique (CNRS); Institut
   National des Sciences Appliquees de Lyon - INSA Lyon
RP Lee, H (corresponding author), Univ Lyon 1, CNRS, LIRIS, UMR5205, F-69622 Lyon, France.
EM ho.lee@liris.cnrs.fr; cagatay.dikici@liris.cnrs.fr;
   guillaume.lavoue@liris.cnrs.fr; florent.dupont@liris.cnrs.fr
OI Dupont, Florent/0000-0001-6611-4420
CR Alliez P, 2001, COMP GRAPH, P195, DOI 10.1145/383259.383281
   Cheng HW, 2010, 2010 14TH INTERNATIONAL WORKSHOP ON COMPUTATIONAL ELECTRONICS (IWCE 2010), P331
   Cho JW, 2007, IEEE T SIGNAL PROCES, V55, P142, DOI 10.1109/TSP.2006.882111
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   COHENOR D, 1999, VIS 99 C P, P67
   De Vleeschouwer C, 2003, IEEE T MULTIMEDIA, V5, P97, DOI 10.1109/TMM.2003.809729
   Gandoin PM, 2002, ACM T GRAPHIC, V21, P372, DOI 10.1145/566570.566591
   GOUDIA D, 2011, VISUAL INFORM PROCES, V2, P7882
   Hoppe H., 1996, Proc. ACM SIGGRAPH, P99
   Konstantinides JM, 2009, IEEE T MULTIMEDIA, V11, P23, DOI 10.1109/TMM.2008.2008913
   Lee H., 2009, P VIS MOD VIS WORKSH, P73
   Liu Y, 2008, MM&SEC'08: PROCEEDINGS OF THE MULTIMEDIA & SECURITY WORKSHOP 2008, P43
   LU Z, 2008, P 6 INT WORKSH DIG W, P233
   Ohbuchi R, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P261, DOI 10.1145/266180.266377
   Pajarola R, 2000, IEEE T VIS COMPUT GR, V6, P79, DOI 10.1109/2945.841122
   Peng JL, 2005, J VIS COMMUN IMAGE R, V16, P688, DOI 10.1016/j.jvcir.2005.03.001
   Peng JL, 2010, COMPUT GRAPH FORUM, V29, P2029, DOI 10.1111/j.1467-8659.2010.01789.x
   Praun E, 1999, COMP GRAPH, P49, DOI 10.1145/311535.311540
   TAUBIN G., 1998, ACM SIGGRAPH, P123
   Valette S, 2009, COMPUT GRAPH FORUM, V28, P1301, DOI 10.1111/j.1467-8659.2009.01507.x
   Wang K, 2011, COMPUT GRAPH-UK, V35, P1, DOI 10.1016/j.cag.2010.09.010
   Wang K, 2008, IEEE T MULTIMEDIA, V10, P1513, DOI 10.1109/TMM.2008.2007350
   WANG Y, 2009, IEEE T VIS COMPUT GR, V15, P1
   WILLEMS FMJ, 2003, P DIMACS SERIES DISC, V66, P61
   Wu HT, 2008, 2008 IEEE 10TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, VOLS 1 AND 2, P801
   YANG E, 2008, P 43 ALL C COMM CONT
   Zafeiriou S, 2005, IEEE T VIS COMPUT GR, V11, P596, DOI 10.1109/TVCG.2005.71
NR 27
TC 18
Z9 20
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 781
EP 792
DI 10.1007/s00371-011-0586-7
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600036
DA 2024-07-18
ER

PT J
AU Lv, P
   Zhang, MM
   Xu, ML
   Li, HS
   Zhu, PY
   Pan, ZG
AF Lv, Pei
   Zhang, Mingmin
   Xu, Mingliang
   Li, Huansen
   Zhu, Pengyu
   Pan, Zhigeng
TI Biomechanics-based reaching optimization
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Reaching; Biomechanics; Shoulder complex model; Arm-reachable workspace;
   Optimization
ID INVERSE KINEMATICS; SHOULDER COMPLEX; BALANCE RECOVERY; MOTION; JOINT;
   ANIMATION; ELEVATION; MODEL; ARM
AB This paper presents a Biomechanics-based Lifelike Reaching Controller (BLRC) to generate lifelike reaching motion. The BLRC employs various reaching strategies borrowed from biomechanics to guarantee the naturalness of reaching motion and expands the reachable space to enrich the flexibility of human behavior. We exploit the arm-reachable workspace to guide the motion sampling, and construct different low-dimensional space for each reaching strategy by PCA to reduce the search space, so as to make BLRC fast deal with huge mocap data set. Moreover, we also use the optimization method in these low-dimensional spaces to further speed up the convergence of motion synthesis with the help of the accurate starting point in data space during the search process. We demonstrate the power of the BLRC with more lifelike and complex reaching motion.
C1 [Lv, Pei; Zhang, Mingmin; Xu, Mingliang; Li, Huansen; Zhu, Pengyu; Pan, Zhigeng] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Zhejiang, Peoples R China.
   [Zhang, Mingmin] Zhejiang Univ, Comp & Engn Dept, Hangzhou, Zhejiang, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Zhang, MM (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Zhejiang, Peoples R China.
EM lvpei@cad.zju.edu.cn; zmm@cad.zju.edu.cn; develop_game@yahoo.com.cn;
   lihuansen@cad.zju.edu.cn; zhupengyu@cad.zju.edu.cn; zhigengpan@gmail.com
RI Zhang, Miao/JXY-8985-2024; zhang, mm/IWV-4201-2023; 吕, 培/A-9726-2009
OI Pan, Zhi-geng/0000-0003-0717-5850
CR Alexa M, 2000, COMPUT GRAPH FORUM, V19, pC411, DOI 10.1111/1467-8659.00433
   BAKER DR, 1988, INT J ROBOT RES, V7, P3, DOI 10.1177/027836498800700201
   Bey MJ, 2006, J BIOMECH ENG-T ASME, V128, P604, DOI 10.1115/1.2206199
   Bishop C. M., 1995, NEURAL NETWORKS PATT
   Chai JX, 2005, ACM T GRAPHIC, V24, P686, DOI 10.1145/1073204.1073248
   Doorenbosch CAM, 2001, J ORTHOP SPORT PHYS, V31, P133, DOI 10.2519/jospt.2001.31.3.133
   DVIR Z, 1978, J BIOMECH, V11, P219, DOI 10.1016/0021-9290(78)90047-7
   Grochow K, 2004, ACM T GRAPHIC, V23, P522, DOI 10.1145/1015706.1015755
   Hong QY, 2010, COMPUT GRAPH FORUM, V29, P537, DOI 10.1111/j.1467-8659.2009.01623.x
   Hsiao ET, 1999, J BIOMECH, V32, P1099, DOI 10.1016/S0021-9290(99)00104-9
   Huang WJ, 2010, LECT NOTES COMPUT SC, V6459, P36
   Iqbal K, 2000, J BIOMECH, V33, P1619, DOI 10.1016/S0021-9290(00)00129-9
   Kallmann M, 2003, COMPUT GRAPH FORUM, V22, P313, DOI 10.1111/1467-8659.00678
   Klopcar N, 2007, J BIOMECH, V40, P86, DOI 10.1016/j.jbiomech.2005.11.010
   Klopcar N, 2006, CLIN BIOMECH, V21, pS20, DOI 10.1016/j.clinbiomech.2005.09.009
   Komura T, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P266, DOI 10.1109/CGI.2003.1214480
   Lee SH, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1559755.1559756
   Maurel W, 2000, COMPUT GRAPH-UK, V24, P203, DOI 10.1016/S0097-8493(99)00155-7
   Min JY, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640452
   Mukai T, 2005, ACM T GRAPHIC, V24, P1062, DOI 10.1145/1073204.1073313
   Raunhardt D, 2009, VISUAL COMPUT, V25, P509, DOI 10.1007/s00371-009-0336-2
   Rose CF, 2001, COMPUT GRAPH FORUM, V20, pC239
   Runge CF, 1999, GAIT POSTURE, V10, P161, DOI 10.1016/S0966-6362(99)00032-6
   Safonova A, 2004, ACM T GRAPHIC, V23, P514, DOI 10.1145/1015706.1015754
   Tolani D, 2000, GRAPH MODELS, V62, P353, DOI 10.1006/gmod.2000.0528
   VANDERVEEGER H, 2003, BEHAV RES METH INSTR, V35, P440
   WAMPLER CW, 1986, IEEE T SYST MAN CYB, V16, P93, DOI 10.1109/TSMC.1986.289285
   WHITNEY DE, 1969, IEEE T MAN MACHINE, VMM10, P47, DOI 10.1109/TMMS.1969.299896
   Ye YT, 2010, COMPUT GRAPH FORUM, V29, P555, DOI 10.1111/j.1467-8659.2009.01625.x
   ZHAO JM, 1994, ACM T GRAPHIC, V13, P313, DOI 10.1145/195826.195827
   Zohdy M. A., 1989, Proceedings of the 1989 American Control Conference (Cat. No.89CH2772-2), P999
NR 31
TC 3
Z9 5
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 613
EP 621
DI 10.1007/s00371-011-0568-9
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600020
DA 2024-07-18
ER

PT J
AU Zhang, GJ
   Zhu, DM
   Qiu, XJ
   Wang, ZQ
AF Zhang, Guijuan
   Zhu, Dengming
   Qiu, Xianjie
   Wang, Zhaoqi
TI Skeleton-based control of fluid animation
SO VISUAL COMPUTER
LA English
DT Article
DE Fluid animation; Fluid control; Skeleton-based control
AB We present a skeleton-based control method for fluid animation. Our method is designed to provide an easy and intuitive control approach while producing visually plausible fluid behavior. In our method, users are allowed to control animated fluid with skeleton keyframes. Expected results are then obtained by driving fluid towards a sequence of targets specified in these keyframes. In order to solve for an optimal driving solution, we propose a keyframe matching model based on the transportation principle. Moreover, to ensure that the fluid actors move as rigid bodies while preserving liquid properties during animation, we introduce an approach of driving solid-like liquid motion. Finally, we embed the skeleton-based control method into the standard fluid animation, and apply it to control fluid actors' motion as well as liquid shape deformation. Experimental results show that our method can generate natural-looking interesting fluid behavior with little additional cost.
C1 [Zhang, Guijuan; Zhu, Dengming; Qiu, Xianjie; Wang, Zhaoqi] Chinese Acad Sci, Grad Univ, Inst Comp Technol, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   University of Chinese Academy of Sciences, CAS
RP Zhang, GJ (corresponding author), Chinese Acad Sci, Grad Univ, Inst Comp Technol, Beijing, Peoples R China.
EM zhangguijuan@ict.ac.cn; mdzhu@ict.ac.cn; qxj@ict.ac.cn; zqwang@ict.ac.cn
RI Li, Chun/KBC-9591-2024
FU NSFC [60703019, U0935003]; National Key Technology RD Program
   [2008BAI50B07, 2009BAK43B38]; National High Technology Research and
   Development Program [2009AA01Z312]
FX The authors wish to thank Dr. Jinzhu Gao for proofreading this paper and
   Guoxian Gao for rendering the final images. We would also like to thank
   the anonymous reviewers for their suggestions. This work is funded by
   NSFC Grant No. 60703019, National Key Technology R&D Program
   2008BAI50B07 and 2009BAK43B38, National High Technology Research and
   Development Program of 2009AA01Z312, and NSFC-Guangdong Joint Fund
   (U0935003).
CR ANGELIDIS A, 2006, SCA 06, P25
   [Anonymous], 2015, Introduction to Operations Research
   [Anonymous], 2004, COMPUTER ANIMATION 2, DOI DOI 10.1145/1028523.1028549
   Carlson M., 2004, SIGGRAPH '04, P377
   Cornea ND, 2005, VISUAL COMPUT, V21, P945, DOI 10.1007/s00371-005-0308-0
   Cornea ND, 2007, IEEE T VIS COMPUT GR, V13, P530, DOI 10.1109/TVCG.2007.1002
   Dantzig G. B., 2003, Linear Programming 2: Theory and Extensions
   Enright D, 2002, ACM T GRAPHIC, V21, P736, DOI [10.1145/566570.566581, 10.1145/566570.566645]
   FATTAL R, 2004, GRAPH 04, P441
   Foster N, 1997, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P178, DOI 10.1109/CGI.1997.601299
   Foster N, 2001, COMP GRAPH, P23, DOI 10.1145/383259.383261
   Kim Y., 2006, P 2006 ACM SIGGRAPHE, P33
   Lamorlette A, 2002, ACM T GRAPHIC, V21, P729, DOI 10.1145/566570.566644
   Losasso F, 2008, IEEE T VIS COMPUT GR, V14, P797, DOI 10.1109/TVCG.2008.37
   McNamara A, 2004, ACM T GRAPHIC, V23, P449, DOI 10.1145/1015706.1015744
   Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154
   Pharr P, 2004, PHYS BASED RENDERING
   PIGHIN F, 2004, SCA 04, P223
   REINFELD N.V., 1958, Mathematical programming
   Schpok J., 2005, P 2005 ACM SIGGRAPHE, P97, DOI [10.1145/1073368.1073381, DOI 10.1145/1073368.1073381]
   SELLE A, 2005, SIGGRAPH 05 ACM SIGG, P910, DOI DOI 10.1145/1186822.1073282
   Sethian JA, 1996, P NATL ACAD SCI USA, V93, P1591, DOI 10.1073/pnas.93.4.1591
   Shi Lin., 2005, Proceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '05, P229, DOI DOI 10.1145/1073368.1073401
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   THUREY N, 2006, SCA 06, P7
   Treuille A, 2003, ACM T GRAPHIC, V22, P716, DOI 10.1145/882262.882337
   Yoshizawa S, 2007, COMPUT GRAPH FORUM, V26, P255, DOI 10.1111/j.1467-8659.2007.01047.x
NR 27
TC 11
Z9 15
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2011
VL 27
IS 3
BP 199
EP 210
DI 10.1007/s00371-010-0526-y
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 722QS
UT WOS:000287450000003
DA 2024-07-18
ER

PT J
AU Schmid, J
   Guitián, JAI
   Gobbetti, E
   Magnenat-Thalmann, N
AF Schmid, Jerome
   Guitian, Jose A. Iglesias
   Gobbetti, Enrico
   Magnenat-Thalmann, Nadia
TI A GPU framework for parallel segmentation of volumetric images using
   discrete deformable models
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3DAH Conference
CY MAY 23-24, 2010
CL Chania, GREECE
DE Simulation and modeling; GPU programming; Segmentation
ID INTERACTIVE SEGMENTATION; MEDICAL IMAGES; CT IMAGES; RECONSTRUCTION;
   REGISTRATION; BONE; FLOW; MR
AB Despite the ability of current GPU processors to treat heavy parallel computation tasks, its use for solving medical image segmentation problems is still not fully exploited and remains challenging. A lot of difficulties may arise related to, for example, the different image modalities, noise and artifacts of source images, or the shape and appearance variability of the structures to segment. Motivated by practical problems of image segmentation in the medical field, we present in this paper a GPU framework based on explicit discrete deformable models, implemented over the NVidia CUDA architecture, aimed for the segmentation of volumetric images. The framework supports the segmentation in parallel of different volumetric structures as well as interaction during the segmentation process and real-time visualization of the intermediate results. Promising results in terms of accuracy and speed on a real segmentation experiment have demonstrated the usability of the system.
C1 [Schmid, Jerome; Magnenat-Thalmann, Nadia] Univ Geneva, MIRALab, CH-1227 Battelle, Carouge, Switzerland.
   [Guitian, Jose A. Iglesias; Gobbetti, Enrico] CRS4 Visual Comp Grp, I-09010 Pula, Italy.
   [Magnenat-Thalmann, Nadia] Nanyang Technol Univ, Inst Media Innovat, Singapore, Singapore.
C3 University of Geneva; Nanyang Technological University
RP Schmid, J (corresponding author), Univ Geneva, MIRALab, CH-1227 Battelle, Carouge, Switzerland.
EM schmid@miralab.ch
RI Thalmann, Nadia/AAK-5195-2021; Gobbetti, Enrico/O-2188-2015; Guitian,
   Jose A. Iglesias/A-9718-2017
OI Thalmann, Nadia/0000-0002-1459-5960; Gobbetti,
   Enrico/0000-0003-0831-2458; Guitian, Jose A.
   Iglesias/0000-0002-0817-1010; Schmid, Jerome/0000-0003-2464-8971
CR [Anonymous], P BMVC
   [Anonymous], P BILDVERARBEITUNG M
   [Anonymous], NVIDIA CUDA PROGR GU
   [Anonymous], 2007, MICCAI 2007 Workshop Proceedings: 3D Segmentation in the Clinic-A Grand Challenge
   Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043
   Cates JE, 2004, MED IMAGE ANAL, V8, P217, DOI 10.1016/j.media.2004.06.022
   Cootes T. F., 1993, Information Processing in Medical Imaging. 13th International Conference, IPMI '93 Proceedings, P33, DOI 10.1007/BFb0013779
   Crassin C., 2009, P 2009 S INT 3D GRAP, P15, DOI [10.1145/1507149.1507152, DOI 10.1145/1507149.1507152]
   Delingette H, 1999, INT J COMPUT VISION, V32, P111, DOI 10.1023/A:1008157432188
   Georgii J, 2005, SIMUL MODEL PRACT TH, V13, P693, DOI 10.1016/j.simpat.2005.08.004
   Gilles B, 2010, MED IMAGE ANAL, V14, P291, DOI 10.1016/j.media.2010.01.006
   Gobbetti E, 2008, VISUAL COMPUT, V24, P797, DOI 10.1007/s00371-008-0261-9
   Göddeke D, 2007, PARALLEL COMPUT, V33, P685, DOI 10.1016/j.parco.2007.09.002
   He ZY, 2006, LECT NOTES COMPUT SC, V4291, P191
   Holden M, 1999, LECT NOTES COMPUT SC, V1613, P472
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Kienel E, 2009, P EUR, P89
   Lahabar S., 2009, P IEEE INT S PARALLE, P1
   Lefohn AE, 2003, LECT NOTES COMPUT SC, V2878, P564
   Liu JY, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531375
   Lorigo LM, 1998, LECT NOTES COMPUT SC, V1496, P1195, DOI 10.1007/BFb0056309
   Mosegaard J, 2005, STUD HEALTH TECHNOL, V111, P342
   Muyan-Özçelik P, 2008, INTERNATIONAL CONFERENCE ON COMPUTATIONAL SCIENCES AND ITS APPLICATIONS, PROCEEDINGS, P223, DOI 10.1109/ICCSA.2008.22
   Noe KO, 2008, ACTA ONCOL, V47, P1286, DOI 10.1080/02841860802258760
   Olabarriaga SD, 2001, MED IMAGE ANAL, V5, P127, DOI 10.1016/S1361-8415(00)00041-4
   Pan W, 2009, P EUR, P93
   RODRIGUEZNAVARR.J, 2006, P WORKSH VIRT REAL I, P1
   SANTNER J, 2009, P BMVC
   Schmid J, 2011, INT J COMPUT ASS RAD, V6, P47, DOI 10.1007/s11548-010-0474-z
   Schmid J, 2009, LECT NOTES COMPUT SC, V5903, P13, DOI 10.1007/978-3-642-10470-1_2
   Schmid J, 2008, LECT NOTES COMPUT SC, V5241, P119, DOI 10.1007/978-3-540-85988-8_15
   Sebastian TB, 2003, MED IMAGE ANAL, V7, P21, DOI 10.1016/S1361-8415(02)00065-8
   Sharp GC, 2007, PHYS MED BIOL, V52, P5771, DOI 10.1088/0031-9155/52/19/003
   SHERBONDY A, 2003, P IEEE VIS I003, P23
   Snel JG, 2002, IEEE T MED IMAGING, V21, P888, DOI 10.1109/TMI.2002.803127
   Stoev SL, 2000, IEEE VISUAL, P21, DOI 10.1109/VISUAL.2000.885672
   Strzodka R, 2004, COMPUTING, V73, P373, DOI 10.1007/s00607-004-0087-x
   Tejada E, 2005, SIMUL MODEL PRACT TH, V13, P703, DOI 10.1016/j.simpat.2005.08.005
   Tessmann M, 2008, P EUR WORKSH VIS COM, P117
   UDUPA JK, 1982, COMPUT VISION GRAPH, V18, P213, DOI 10.1016/0146-664X(82)90033-8
   UMEYAMA S, 1991, IEEE T PATTERN ANAL, V13, P376, DOI 10.1109/34.88573
   VERLET L, 1967, PHYS REV, V159, P98, DOI 10.1103/PhysRev.159.98
   Walters JP, 2009, INT PARALL DISTRIB P, P1010
NR 43
TC 11
Z9 14
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2011
VL 27
IS 2
SI SI
BP 85
EP 95
DI 10.1007/s00371-010-0532-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 722QR
UT WOS:000287449900002
OA Green Published
DA 2024-07-18
ER

PT J
AU Cleju, I
   Saupe, D
AF Cleju, Ioan
   Saupe, Dietmar
TI Evaluation of texture registration by epipolar geometry
SO VISUAL COMPUTER
LA English
DT Article
DE Texture registration; Epipolar geometry; Epipolar distances;
   Experimental evaluation; Mutual information
ID OPTIMIZATION CRITERIA; MOTION
AB In the process of digitizing the geometry and appearance of 3D objects, texture registration is a necessary step that solves the 2D-3D mapping between the 2D texture images and the 3D geometric model. For evaluation of texture registration with ground truth, accurate datasets can be obtained with a complex setup consisting of calibrated geometry and texture capture devices. We do not have any knowledge of such evaluation performed; current evaluations reflect, at their best, the precision achieved by the algorithms, but fail to identify a possible bias. We propose a new evaluation measure based on the epipolar geometry of texture image pairs, with the advantage that the ground truth can be extracted solely from the texture images, independent of the 3D acquisition. We developed a noise model suitable to our purpose and analysed three distance measures based on epipolar geometry, well known in the computer vision community, to find new theoretical and experimental results. Finally, using the proposed framework, we evaluated a texture registration algorithm based on mutual information and found that its accuracy was under half-pixel.
C1 [Cleju, Ioan] Oxford Metr Grp YottaDCL, Royal Leamington Spa CV32 4LY, England.
   [Saupe, Dietmar] Univ Konstanz, Dept Comp & Informat Sci, Constance, Germany.
C3 University of Konstanz
RP Cleju, I (corresponding author), Oxford Metr Grp YottaDCL, Yotta House,8 Hamilton Terrace, Royal Leamington Spa CV32 4LY, England.
EM ioan.cleju@yottadcl.com; dietmar.saupe@uni-konstanz.de
FU DFG Research Training Group [1042]
FX Work supported by the DFG Research Training Group 1042
CR Bouguet J-Y., 2009, Camera calibration toolbox
   Clarkson MJ, 2001, IEEE T PATTERN ANAL, V23, P1266, DOI 10.1109/34.969117
   CLEJU I, 2008, THESIS U KONSTANZ
   Cleju I, 2007, LECT NOTES COMPUT SC, V4713, P517
   Faugeras O., 2001, The geometry of multiple images: the laws that govern the formation of multiple images of a scene and some of their applications
   Hartley R., 2000, MULTIPLE VIEW GEOMET, V1
   Hartley RI, 1997, COMPUT VIS IMAGE UND, V68, P146, DOI 10.1006/cviu.1997.0547
   Leedan Y, 2000, INT J COMPUT VISION, V37, P127, DOI 10.1023/A:1008185619375
   Lensch HPA, 2001, GRAPH MODELS, V63, P245, DOI 10.1006/gmod.2001.0554
   Ma Y, 2001, INT J COMPUT VISION, V44, P219, DOI 10.1023/A:1012276232049
   Neugebauer PJ, 1999, COMPUT GRAPH FORUM, V18, pC245
   Seitz S.M., 2006, 2006 IEEE COMP SOC C, V1, P519, DOI https://doi.org/10.1109/CVPR.2006.19
   Torr PHS, 1997, INT J COMPUT VISION, V24, P271, DOI 10.1023/A:1007927408552
   Troccoli A, 2007, MACH VISION APPL, V18, P65, DOI 10.1007/s00138-006-0048-9
   van de Kraats EB, 2005, IEEE T MED IMAGING, V24, P1177, DOI 10.1109/TMI.2005.853240
   Zhang Zhengyou., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, V1, P0, DOI DOI 10.1109/ICCV.1999.791289
   Zhang ZY, 1998, INT J COMPUT VISION, V27, P161, DOI 10.1023/A:1007941100561
   Zhang ZY, 1998, IEEE T PATTERN ANAL, V20, P717, DOI 10.1109/34.689302
   2009, 3D SURFACE ACQUISITI
NR 19
TC 2
Z9 2
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2010
VL 26
IS 11
BP 1407
EP 1420
DI 10.1007/s00371-010-0427-0
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 653UV
UT WOS:000282123700007
DA 2024-07-18
ER

PT J
AU Bucksch, A
   Lindenbergh, R
   Menenti, M
AF Bucksch, Alexander
   Lindenbergh, Roderik
   Menenti, Massimo
TI SkelTre Robust skeleton extraction from imperfect point clouds
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 2nd Eurographics Workshop on 3D Object Retrieval
CY MAR 29, 2009
CL Munich, GERMANY
DE Skeletonization; Point cloud; Laser scanning
ID REEB GRAPHS
AB Terrestrial laser scanners capture 3D geometry of real world objects as a point cloud. This paper reports on a new algorithm developed for the skeletonization of a laser scanner point cloud. The skeletonization algorithm proposed in this paper consists of three steps: (i) extraction of a graph from an octree organization, (ii) reduction of the graph to a skeleton, and (iii) embedding of the skeleton into the point cloud. For these three steps, only one input parameter is required. The results are validated on laser scanner point clouds representing 2 classes of objects; first on botanic trees as a special application and secondly on popular arbitrary objects. The presented skeleton found its first application in obtaining botanic tree parameters like length and diameter of branches and is presented here in a new, generalized version. Its definition as Reeb Graph, proofs the usefulness of the skeleton for applications like shape analysis. In this paper we show that the resulting skeleton contains the Reeb Graph and investigate the practically relevant parameters: centeredness and topological correctness. The robustness of this skeletonization method against undersampling, varying point density and systematic errors of the point cloud is demonstrated on real data examples.
C1 [Bucksch, Alexander; Lindenbergh, Roderik] Delft Univ Technol, Remote Sensing Dept, NL-2629 HS Delft, Netherlands.
C3 Delft University of Technology
RP Bucksch, A (corresponding author), Delft Univ Technol, Remote Sensing Dept, Kluyverweg 1, NL-2629 HS Delft, Netherlands.
EM a.k.bucksch@tudelft.nl; r.c.lindenbergh@tudelft.nl; m.menenti@tudelft.nl
RI Bucksch, Alexander/A-6102-2009
OI Bucksch, Alexander/0000-0002-1071-5355; Menenti,
   Massimo/0000-0001-9176-4556; Lindenbergh, Roderik/0000-0001-8655-5266
CR Amenta N, 2001, COMP GEOM-THEOR APPL, V19, P127, DOI 10.1016/S0925-7721(01)00017-7
   [Anonymous], 1963, MORSE THEORY AM 51, DOI [10.1515/9781400881802, DOI 10.1515/9781400881802]
   [Anonymous], 2003, Computational discrete mathematics: combinatorics and graph theory with Mathematica, DOI DOI 10.1017/CBO9781139164849
   ARTHUR D, 2005, WORST CASE COMPLEXIT, P698
   Blum H., 1967, MODELS PERCEPTION SP, P362, DOI DOI 10.1142/S0218654308001154
   Bucksch A., 2009, Eurographics Workshop on 3D Object Retrieval, P13, DOI DOI 10.2312/3DOR/3DOR09/013-020
   Bucksch A, 2008, ISPRS J PHOTOGRAMM, V63, P115, DOI 10.1016/j.isprsjprs.2007.10.004
   CHEN HH, 1988, COMPUT VISION GRAPH, V43, P409, DOI 10.1016/0734-189X(88)90092-8
   Cole-McLaughlin K, 2004, DISCRETE COMPUT GEOM, V32, P231, DOI 10.1007/s00454-004-1122-6
   Cornea ND, 2007, IEEE T VIS COMPUT GR, V13, P530, DOI 10.1109/TVCG.2007.1002
   Côté JF, 2009, REMOTE SENS ENVIRON, V113, P1067, DOI 10.1016/j.rse.2009.01.017
   Dey TK, 2006, S GEOMETRY PROCESSIN, P143
   Forman R, 1998, ADV MATH, V134, P90, DOI 10.1006/aima.1997.1650
   GORTE B, 2006, P 3DGEOINFO 2006
   Gorte B., 2004, International Archives of Photogrammetry and Remote Sensing, V35, P929
   Musser D.R., 1996, STL tutorial and reference guide
   Palagyi K., 2001, Information Processing in Medical Imaging. 17th International Conference, IPMI 2001. Proceedings (Lecture Notes in Computer Science Vol.2082), P409
   Pascucci V, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239509
   REEB G, 1946, CR HEBD ACAD SCI, V222, P847
   SAITO T, 1994, PATTERN RECOGN, V27, P1551, DOI 10.1016/0031-3203(94)90133-3
   SERRA J, 1982, IMAGE ANAL MATHEMATI
   Shan J., 2008, Topographic Laser Ranging and Scanning: Principles and Processing, V1st
   SHINAGAWA Y, 1991, IEEE COMPUT GRAPH, V11, P66, DOI 10.1109/38.90568
   Verroust A, 2000, VISUAL COMPUT, V16, P15, DOI 10.1007/PL00007210
   Xu H, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1289603.1289610
   YAN DM, 2009, P 11 IEEE INT C COMP
   Zhou Y, 1998, VISUAL COMPUT, V14, P303, DOI 10.1007/s003710050142
NR 27
TC 96
Z9 107
U1 1
U2 40
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2010
VL 26
IS 10
BP 1283
EP 1300
DI 10.1007/s00371-010-0520-4
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 649SI
UT WOS:000281793200005
DA 2024-07-18
ER

PT J
AU Feng, ZQ
   Zhang, MM
   Pan, ZG
   Yang, B
   Xu, T
   Tang, HK
   Li, Y
AF Feng, Zhiquan
   Zhang, Minming
   Pan, Zhigeng
   Yang, Bo
   Xu, Tao
   Tang, Haokui
   Li, Yi
TI 3D-freehand-pose initialization based on operator's cognitive behavioral
   models
SO VISUAL COMPUTER
LA English
DT Article
DE 3D Freehand pose model; Features extraction; Initialization;
   Visualization; Cognitive behavioral models
ID HUMAN-COMPUTER INTERACTION
AB Tracking, recognition and interaction based on 3D freehand are a part of our virtual assembly system, in which monocular camera is used to input online freehand videos and the hand pose tracker requires a reliable initial pose in the first frame. A novel approach to initializing 3D pose and position of freehand is put forward in this paper visualization of 3D hand model and modeling the operators' cognitive behaviors. Our approach is composed of three phases: hand posture recognition, coarse-tuning and fine-tuning. The operator moves his/her hand onto the to meet the needs of our virtual assembly system. The main contribution of this paper is that the three core techniques are for the first time integrated together, including human-computer interaction (HCI) in the process of initializing, projection of the 3D hand model in the period of coarse-tuning time. Then, the computer repeatedly fine-tunes the 3D hand model until the projection of the 3D hand model is completely superimposed onto the operator's hand image. We focus on exploring and modeling cognitive behavior of operator's hand upon which we design our initialization algorithm. Our research shows that cognitive behavioral models are not only beneficial to reducing cognitive loads for operators, because it makes the computers cater for the changes of the operators' hand poses, but also helpful to address high dimensionality of articulated 3D hand model. Our experimental results also show that the approach presented in this paper is easier, more pleasurable and satisfactory experience for the operators. Our initialization system has successfully been applied to our 3D freehand tracking system and a simulation virtual assembly system.
C1 [Feng, Zhiquan] Univ Jinan, Sch Informat Sci & Engn, Jinan 250022, Shandong, Peoples R China.
   [Zhang, Minming; Pan, Zhigeng] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Zhejiang, Peoples R China.
   [Yang, Bo; Xu, Tao; Tang, Haokui; Li, Yi] Univ Jinan, Prov Key Lab Network Based Intelligent Comp, Jinan 250022, Shandong, Peoples R China.
C3 University of Jinan; Zhejiang University; University of Jinan
RP Feng, ZQ (corresponding author), Univ Jinan, Sch Informat Sci & Engn, Jinan 250022, Shandong, Peoples R China.
EM ise_fengzq@ujn.edu.cn; zmm@cad.zju.edu.cn; zhigengpan@gmail.com;
   yangbo@ujn.edu.cn; ise_xut@ujn.edu.cn; ise_tanghk@ujn.edu.cn;
   liyi@ujn.edu.cn
RI Zhang, Minming/K-8132-2018
OI Pan, Zhi-geng/0000-0003-0717-5850
FU NSFC [60773109, 60973093]; Natural Science Foundation of Shandong
   Province [Y2007G39, 2006G03]; Natural Science Foundation for
   Distinguished Youth Scholar of Shandong Province [JQ200820]; Science and
   Technology Plan of Shandong Province Education Department [J07YJ18]
FX This paper is supported by NSFC (No. 60773109), NSFC (No. 60973093),
   Natural Science Foundation of Shandong Province (Y2007G39), Natural
   Science Foundation for Distinguished Youth Scholar of Shandong Province
   (No. JQ200820), Key Project of Natural Science Foundation of Shandong
   Province (2006G03), Science and Technology Plan of Shandong Province
   Education Department (J07YJ18).
CR Anderson JR, 1995, J LEARN SCI, V4, P167, DOI 10.1207/s15327809jls0402_2
   [Anonymous], 1996, ACM Trans. Comput.-Hum. Interact., DOI 10.1145/235833.236054
   Athitsos V, 2003, PROC CVPR IEEE, P432
   Bray M, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P675, DOI 10.1109/AFGR.2004.1301612
   CARD SK, 1993, PSYCHOL HUMAN COMPUT
   CHAPANIS A., 1965, MAN MACHINE ENG
   Cleveland WS, 1993, Visualizing data
   CUI J, 2004, THESIS TSINGHUA U BE
   Erol A., 2005, P IEEE WORKSH VIS HU, P75, DOI [DOI 10.1109/CVPR.2005.395, 10.1109/CVPR.2005.395]
   Erol A, 2007, COMPUT VIS IMAGE UND, V108, P52, DOI 10.1016/j.cviu.2006.10.012
   Feng ZQ, 2008, ISCSCT 2008: INTERNATIONAL SYMPOSIUM ON COMPUTER SCIENCE AND COMPUTATIONAL TECHNOLOGY, VOL 2, PROCEEDINGS, P762, DOI 10.1109/ISCSCT.2008.298
   GORDON NJ, 1993, IEE PROC-F, V140, P107, DOI 10.1049/ip-f-2.1993.0015
   Jaimes A, 2007, COMPUT VIS IMAGE UND, V108, P116, DOI 10.1016/j.cviu.2006.10.019
   John B.E., 1996, ACM T COMPUT-HUM INT, V3, P287
   John BE, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P161
   JULIER SJ, 1997, PROCEDURE AEROSENSE, P82
   Karray F, 2008, INT J SMART SENS INT, V1, P137, DOI 10.21307/ijssis-2017-283
   LaGorce Martinde, 2008, IEEE P IEEE C COMP V, P1
   NORMAN D, 1986, OPERATOR CTR DESIGN
   Picard R. W., 1997, AFFECTIVE COMPUTING
   Ritter F. E., 2000, ACM Transactions on Computer-Human Interaction, V7, P141, DOI 10.1145/353485.353486
   ROSALES R, 2002, THESIS BOSTON U BOST
   Shimada N, 2001, IEEE ICCV WORKSHOP ON RECOGNITION, ANALYSIS AND TRACKING OF FACES AND GESTURES IN REAL-TIME SYSTEMS, PROCEEDINGS, P23, DOI 10.1109/RATFG.2001.938906
   Stefan A, 2008, P 1 ACM INT C PERVAS, P1, DOI [10.1145/1389586.1389595, DOI 10.1145/1389586.1389595]
   Stenger B, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1063
   TANG ZS, 1999, VISUALIZATION 3D DAT
   Te'eni D., 2007, Human Computer Interaction: Developing effective organizational information systems
   Tomasi C, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1441
   Triesch J, 2001, IEEE T PATTERN ANAL, V23, P1449, DOI 10.1109/34.977568
NR 29
TC 15
Z9 19
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 607
EP 617
DI 10.1007/s00371-010-0452-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800021
DA 2024-07-18
ER

PT J
AU Lai, YC
   Chenney, S
   Liu, F
   Niu, YZ
   Fan, SH
AF Lai, Yu-Chi
   Chenney, Stephen
   Liu, Feng
   Niu, Yuzhen
   Fan, Shaohua
TI Animation rendering with Population Monte Carlo image-plane sampler
SO VISUAL COMPUTER
LA English
DT Article
DE Ray-tracing; PMC; Monte Carlo; Global illumination
AB Except for the first frame, a population Monte Carlo image plane (PMC-IP) sampler renders with a start-up kernel function learned from previous results by using motion analysis techniques in the vision community to explore the temporal coherence existing among kernel functions. The predicted kernel function can shift part of the uniformly distributed samples from regions with low visual variance to regions with high visual variance at the start-up iteration and reduce the temporal noise by considering the temporal relation of sample distributions among frames. In the following iterations, the PMC-IP sampler adapts the kernel function to select pixels for refinement according to a perceptually-weighted variance criterion. Our results improve the rendering efficiency by a factor between 2 to 5 over existing techniques in single frame rendering. The rendered animations are perceptually more pleasant.
C1 [Fan, Shaohua] Suzhou Univ, Financial Engn Res Ctr, Suzhou 215006, Peoples R China.
   [Fan, Shaohua] NYU, Courant Inst Math Sci, New York, NY USA.
   [Niu, Yuzhen] Shandong Univ, Jinan 250100, Peoples R China.
   [Liu, Feng] Univ Wisconsin, Madison, WI 53706 USA.
   [Chenney, Stephen] Emergent Game Technol, Chapel Hill, NC USA.
   [Lai, Yu-Chi] Natl Taiwan Univ Sci & Technol, Taipei, Taiwan.
C3 New York University; Shandong University; University of Wisconsin
   System; University of Wisconsin Madison; National Taiwan University of
   Science & Technology
RP Fan, SH (corresponding author), Suzhou Univ, Financial Engn Res Ctr, Suzhou 215006, Peoples R China.
EM yu-chi@mail.ntust.edu.tw; schenney@gmail.com; fliu@cs.wisc.edu;
   yuzhen@cs.wisc.edu; shaohua@cs.wisc.edu
OI Lai, Yu-Chi/0000-0001-8578-3101
FU NSC, Taiwan [99-2218-E-011-005-]
FX Y.-C. Lai funded by: NSC 99-2218-E-011-005-, Taiwan.
CR Baker S., 2007, IEEE ICCV
   Bolin M. R., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P299, DOI 10.1145/280814.280924
   Dayal A., 2005, Eurographics Symposium on Rendering, P265
   DIPPE MAZ, 1985, SIGGRAPH 85, P69
   DOUC R, 2005, 20052006 U PAR DAUPH
   Farrugia JP, 2004, COMPUT GRAPH FORUM, V23, P605, DOI 10.1111/j.1467-8659.2004.00792.x
   FERWERDA JA, 1996, SIGGRAPH 96, P249
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   GHOSH A., 2006, Eurographics Symposium on Rendering, P115
   Glassner A. S., 1995, Principles of Digital Image Synthesis
   HESTERBERG T, 1995, TECHNOMETRICS, V37, P185, DOI 10.2307/1269620
   KIRK D, 1991, COMP GRAPH, V25, P153, DOI 10.1145/127719.122735
   LAI Y, 2007, EUR S REND, P287
   LAI YC, 2008, ISVC 08, P614
   Lee M. E., 1985, Computer Graphics, V19, P61, DOI 10.1145/325165.325179
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lucas B. D., 1981, P IJCAI, P674
   Mantiuk R, 2004, IEEE SYS MAN CYBERN, P2763
   MITCHELL DP, 1987, SIGGRAPH 87, P65
   Painter J., 1989, Computer Graphics, V23, P281, DOI 10.1145/74334.74362
   Pharr M., 2004, Physically Based Rendering: From Theory to Implementation
   PURGATHOFER W, 1986, P EUR 86, P145
   Ramasubramanian M, 1999, COMP GRAPH, P73, DOI 10.1145/311535.311543
   Rigau J, 2003, PROC GRAPH INTERF, P149
   RIGAU J, 2002, P CGI 02 JUL, P439
   SCHLICK C, 1991, P 2 EUR WORKSH REND, P21
   STOKES WA, 2004, SIGGRAPH 04, P742
   SZELISKI R, 2006, MSRTR200492
   Tamstorf R., 1997, Rendering Techniques '97. Proceedings of the Eurographics Workshop. Eurographics, P285
NR 29
TC 1
Z9 1
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 543
EP 553
DI 10.1007/s00371-010-0503-5
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800015
DA 2024-07-18
ER

PT J
AU Park, JY
   Park, JH
AF Park, Jung-Yong
   Park, Jong-Hee
TI A graph-based representation of game scenarios; methodology for
   minimizing anomalies in computer game
SO VISUAL COMPUTER
LA English
DT Article
DE Game design; Game anomaly; Graph; Event type; Game analysis; Scenario;
   Simulation
ID DESIGN
AB When we design the computer games, new concepts are hard to communicate because the development of the new computer games is far less formalized compared with other software. However, the creative processing additionally has the benefit if the new concepts of the computer games are widely used and discussed. We focus on understanding the ideas to be shared between the game designer and the game programmer in the game design. The research defines a graph-based representation of game scenarios is in terms of Event graph, State graph, and Action graph forms in order to minimize anomalies of game flow design. This method in our research can reduce unexpected and undesirable game situations. Also it reduces the mismatched information between the game designer and the game programmer in game-playing. Compared with the existing method such as storyboard, flowchart, and SSM, we demonstrate the viability of our game design methodology.
C1 [Park, Jong-Hee] Kyungpook Natl Univ, Sch Elect & Elect Engn, Taegu, South Korea.
   [Park, Jung-Yong] Daegu Univ, Sch Elect Engn, Gyongsan, South Korea.
C3 Kyungpook National University; Daegu University
RP Park, JH (corresponding author), Kyungpook Natl Univ, Sch Elect & Elect Engn, Taegu, South Korea.
EM jypark@daegu.ac.kr; lulu99@daum.net
CR Bethke E., 2003, GAME DEV PRODUCTION
   Gold J., 2004, Object-oriented game development
   Kanev K, 1998, COMPUT GRAPH, V22, P281, DOI 10.1016/S0097-8493(98)00038-7
   LaMothe A., 2002, Tricks of the Windows game programming gurus, V2nd
   Lewinski JS., 2000, DEV GUIDE COMPUTER G
   Manninen T, 2002, PERS UBIQUIT COMPUT, V6, P390, DOI 10.1007/s007790200044
   McConnell S, 2001, IEEE SOFTWARE, V18, P5, DOI 10.1109/MS.2001.903148
   NATKIN S, 2004, INT J INTELL GAMES S, V3
   Onder B., 2002, GAME DESIGN PERSPECT
   Park JY, 2009, INT J SOFTW ENG KNOW, V19, P913, DOI 10.1142/S0218194009004453
   PARK JY, 2007, J KOREA MULTIMEDIA S, V10, P483
   PARK JY, 2007, J KOREA GAME SOC, V7, P95
   Penton Ron., 2003, Data Structures for Game Programmers" -
   Rouse R., 2001, Game Design Theory and Practice
   Santelices RA, 2001, SOFTWARE PRACT EXPER, V31, P1091, DOI 10.1002/spe.403
   Siang A. C., 2004, INT J INTELL GAMES S, V3
   Taylor MJ, 2007, SYST RES BEHAV SCI, V24, P359, DOI 10.1002/sres.805
   Taylor M. J., 2006, ACM COMPUT ENTERTAIN, V4
   Tenzer J, 2004, PROC INT CONF SOFTW, P75, DOI 10.1109/ICSE.2004.1317428
NR 19
TC 2
Z9 2
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 595
EP 605
DI 10.1007/s00371-010-0482-6
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800020
DA 2024-07-18
ER

PT J
AU Roca, J
   Moya, V
   Gonzalez, C
   Escandell, V
   Murciego, A
   Fernandez, A
   Espasa, R
AF Roca, Jordi
   Moya, Victor
   Gonzalez, Carlos
   Escandell, Vicente
   Murciego, Albert
   Fernandez, Agustin
   Espasa, Roger
TI A SIMD-efficient 14 instruction shader program for high-throughput
   microtriangle rasterization
SO VISUAL COMPUTER
LA English
DT Article
DE Microtriangle rasterization; GPU rendering; Shader performance
AB This paper shows that breaking the barrier of 1 triangle/clock rasterization rate for microtriangles in modern GPU architectures in an efficient way is possible. The fixed throughput of the special purpose culling and triangle setup stages of the classic pipeline limits the GPU scalability to rasterize many triangles in parallel when these cover very few pixels. In contrast, the shader core counts and increasing GFLOPs in modern GPUs clearly suggests parallelizing this computation entirely across multiple shader threads, making use of the powerful wide-ALU instructions. In this paper, we present a very efficient SIMD-like rasterization code targeted at very small triangles that scales very well with the number of shader cores and has higher performance than traditional edge equation based algorithms. We have extended the ATTILA GPU shader ISA (del Barrioet al. in IEEE International Symposium on Performance Analysis of Systems and Software, pp. 231-241, 2006) with two fixed point instructions to meet the rasterization precision requirement. This paper also introduces a novel subpixel Bounding Box size optimization that adjusts the bounds much more finely, which is critical for small triangles, and doubles the 2x2-pixel stamp test efficiency. The proposed shader rasterization program can run on top of the original pixel shader program in such a way that selected fragments are rasterized, attribute interpolated and pixel shaded in the same pass. Our results show that our technique yields better performance than a classic rasterizer at 8 or more shader cores, with speedups as high as 4x for 16 shader cores.
C1 [Roca, Jordi; Moya, Victor; Gonzalez, Carlos; Escandell, Vicente; Murciego, Albert; Fernandez, Agustin] UPC, Comp Architecture Dept, Barcelona, Spain.
   [Espasa, Roger] Intel Barcelona, Barcelona, Spain.
C3 Universitat Politecnica de Catalunya; Intel Corporation
RP Roca, J (corresponding author), UPC, Comp Architecture Dept, Barcelona, Spain.
EM jroca@ac.upc.edu; vmoya@ac.upc.edu; cgonzale@ac.upc.edu;
   vicente@ac.upc.edu; albertm@ac.upc.edu; agustin@ac.upc.edu;
   roger.espasa@intel.com
FU Ministry of Education and Science, Spain; European Union
   [TIN2007-60625]; Intel
FX The authors thank to the anonymous reviewers for their helpful comments.
   This work is supported by the Ministry of Education and Science and of
   Spain and the European Union (FEDER funds) under Contract TIN2007-60625
   and by Intel. For additional information about the authors, visit the
   Attila Research Group web site https://attila.ac.upc.edu/.
CR [Anonymous], 2018, Real-Time Rendering
   [Anonymous], RASTERIZATION LARRAB
   [Anonymous], 2003, Computer Architecture
   *ARB, 2002, ARB FRAGM PROGR SPEC
   *BEYOND3D, 2010, BEYOND3D GRAPH HARDW
   COOK HL, 1988, REYES IMAGE RENDERIN, P28
   DUDASH B, 2008, TESSELATION DISPLACE
   Eldridge M, 2000, COMP GRAPH, P443, DOI 10.1145/344779.344981
   Ewins JP, 1998, IEEE T VIS COMPUT GR, V4, P317, DOI 10.1109/2945.765326
   FATAHALIAN K, 2009, HPG 09, P59
   GREENE N, 1993, SIGGRAPH 93, P231
   KUN Z, 2009, ACM T GRAPH
   LICEAKANE B, 2007, GLSL CTR CENTROID SH
   Low K., 2002, PERSPECTIVE CORRECT
   MCCOOL MD, 2001, HWWS 01, P65
   MCCORMACK J, 2000, HWWS 00, P15
   MITCHELL J, 2004, REAL TIME SHADING CO
   Del Barrio VM, 2006, INT SYM PERFORM ANAL, P231
   OLANO M, 1997, HWWS 97, P89
   PINEDA J, 1988, SIGGRAPH 88, P17
   ZHANG H, 1997, S INT 3D GRAPH, P103
NR 21
TC 3
Z9 6
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 707
EP 719
DI 10.1007/s00371-010-0492-4
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800031
DA 2024-07-18
ER

PT J
AU Tao, YB
   Lin, H
   Bao, HJ
   Dong, F
   Clapworthy, G
AF Tao, Yubo
   Lin, Hai
   Bao, Hujun
   Dong, Feng
   Clapworthy, Gordon
TI Feature enhancement by volumetric unsharp masking
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Local contrast enhancement; Volumetric unsharp masking; Volume
   visualization
ID AMBIENT OCCLUSION
AB Feature enhancement is important for the interpretation of complex structures and the detection of local details in volume visualization. We present a simple and effective method, volumetric unsharp masking, to enhance local contrast of features. In general, unsharp masking is an operation that adds the scaled high-frequency part of the signal to itself. The signal in this paper is the radiance at each sample point in the ray-casting based volume rendering, and the radiance depends on both transfer functions and lighting. Our volumetric unsharp masking modulates the radiance by adding back the scaled difference between the radiance and the smoothed radiance. This local color modulation does not change the shape of features due to the same opacity, but it does enhance local contrast of structures in a unified manner. We implemented volumetric unsharp masking at interactive frame rates based on current GPU features, and performed experiments on various volume data sets to validate this local contrast enhancement. The results showed that volumetric unsharp masking reveals more local details and improves depth perception.
C1 [Tao, Yubo; Lin, Hai; Bao, Hujun] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310003, Zhejiang, Peoples R China.
   [Dong, Feng; Clapworthy, Gordon] Univ Bedfordshire, Dept Comp & Informat Syst, Luton, Beds, England.
C3 Zhejiang University; University of Bedfordshire
RP Lin, H (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310003, Zhejiang, Peoples R China.
EM taoyubo@cad.zju.edu.cn; lin@cad.zju.edu.cn; bao@cad.zju.edu.cn;
   Feng.Dong@beds.ac.uk; Gordon.Clapworthy@beds.ac.uk
CR Badamchizadeh M. A., 2004, Proceedings. Third International Conference on Image and Graphics, P27
   Behrens U, 1998, IEEE SYMPOSIUM ON VOLUME VISUALIZATION, P39, DOI 10.1109/SVV.1998.729583
   Bruckner S., Proceedings of the Seventh Joint Eurographics / IEEE VGTC Conference on Visualization, ser. EUROVIS'05. Aire-la-Ville, Switzerland, Switzerland: Eurographics Association, P69, DOI DOI 10.2312/VISSYM/EUROVIS05/069-076
   Bruckner S, 2007, IEEE T VIS COMPUT GR, V13, P1344, DOI 10.1109/TVCG.2007.70555
   Cignoni P, 2005, COMPUT GRAPH-UK, V29, P125, DOI 10.1016/j.cag.2004.11.012
   Engel K, 2006, Real-Time Volume Graphics
   Engel Klaus, 2001, P ACM SIGGRAPH EUROG, P9, DOI [DOI 10.1145/383507.383515, 10.1145/383507.383515]
   Kindlmann G, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P513, DOI 10.1109/VISUAL.2003.1250414
   Kindlmann G, 1998, IEEE SYMPOSIUM ON VOLUME VISUALIZATION, P79, DOI 10.1109/SVV.1998.729588
   KINDLMANN G, 2008, ACM SIGGRAPH 2008 CO
   Kniss J, 2003, IEEE T VIS COMPUT GR, V9, P150, DOI 10.1109/TVCG.2003.1196003
   LEVOY M, 1988, IEEE COMPUT GRAPH, V8, P29, DOI 10.1109/38.511
   Luft T, 2006, ACM T GRAPHIC, V25, P1206, DOI 10.1145/1141911.1142016
   MARCHESIN S, 2007, P IEEE EUR INT S VOL, P41
   MAX N, 1995, IEEE T VIS COMPUT GR, V1, P99, DOI 10.1109/2945.468400
   Rheingans P, 2001, IEEE T VIS COMPUT GR, V7, P253, DOI 10.1109/2945.942693
   Ritschel T., 2008, ACM T GRAPHIC, V27, P1
   Ropinski T, 2008, COMPUT GRAPH FORUM, V27, P567, DOI 10.1111/j.1467-8659.2008.01154.x
   Rusinkiewicz S, 2006, ACM T GRAPHIC, V25, P1199, DOI 10.1145/1141911.1142015
   Stewart AJ, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P355, DOI 10.1109/VISUAL.2003.1250394
   Svakhine NA, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P347, DOI 10.1109/PCCGA.2003.1238276
   Tarini M, 2006, IEEE T VIS COMPUT GR, V12, P1237, DOI 10.1109/TVCG.2006.115
   Viola I, 2005, IEEE T VIS COMPUT GR, V11, P408, DOI 10.1109/TVCG.2005.62
   Wittenbrink CM, 1998, IEEE SYMPOSIUM ON VOLUME VISUALIZATION, P135, DOI 10.1109/SVV.1998.729595
NR 24
TC 9
Z9 13
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 581
EP 588
DI 10.1007/s00371-009-0328-2
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300023
DA 2024-07-18
ER

PT J
AU Amditis, A
   Karaseitanidis, I
   Bimpas, M
   Blach, R
AF Amditis, Angelos
   Karaseitanidis, Ioannis
   Bimpas, Matthaios
   Blach, Roland
TI Future scenarios of mixed reality: the INTUITION roadmap scenarios
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 4th INTUITION International Conference and Workshop
CY OCT 04-05, 2007
CL Athens, GREECE
DE Virtual environments; Augmented reality; Mixed reality; Technology
   roadmap; Technology scenarios
AB INTUITION is a Network of Excellence that aims to integrate the European research efforts on the scientific and technological field of Virtual and Mixed Reality. To perform that, a series of activities have taken place in order to gather knowledge regarding actors and research profiles, projects and research results, products and patents. Having a clear view of research needs and technology trends the Network has envisioned the research goals that need to be pursued within the years to come. The starting point is a set of visionary scenarios which set out the picture for the technological and scientific advances that need to take place. Within this paper a set of indicative scenarios on a higher and descriptive level are provided and the way they contribute to the roadmap definition is explained. With this report we want to share these scenarios and our initial thoughts to stimulate a broader discussion and invite people from all relevant backgrounds to enter the knowledge creation process. The paper is a collective production of the INTUITION Consortium.
C1 [Amditis, Angelos; Karaseitanidis, Ioannis; Bimpas, Matthaios] Inst Commun & Comp Syst, Athens, Greece.
   [Blach, Roland] Fraunhofer IAO, CC Virtual Environm, D-70569 Stuttgart, Germany.
RP Karaseitanidis, I (corresponding author), Inst Commun & Comp Syst, 9 Iroon Politechniou St, Athens, Greece.
EM a.amditis@iccs.gr; gkara@iccs.gr; mbibas@iccs.gr;
   Roland.Blach@iao.fhg.de
RI Amditis, Angelos/AAV-1541-2020
OI Amditis, Angelos/0000-0002-4089-1990
CR AMDITIS A, 2007, MMI INTERAKTIV, V12, P43
   [Anonymous], 2001, The wiki way: Quick collaboration on the web
   BLACH R, 2006, INTUITION DELIVERABL
   GROENVELD P, 1997, RES TECHNOL MANAG, V40
   HEPP M, 2006, P 1 WORKSH SEMWIKI20
   PHAAL C, 2004, INT J TECHNOL INTEL, V1
NR 6
TC 0
Z9 0
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2008
VL 24
IS 11
BP 935
EP 940
DI 10.1007/s00371-008-0293-1
PG 6
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 359BQ
UT WOS:000259961600002
DA 2024-07-18
ER

PT J
AU Vanacken, L
   Raymaekers, C
   Coninx, K
AF Vanacken, Lode
   Raymaekers, Chris
   Coninx, Karin
TI Automatic speech grammar generation during conceptual modelling of
   virtual environments
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 4th INTUITION International Conference and Workshop
CY OCT 04-05, 2007
CL Athens, GREECE
DE User interfaces & interaction techniques; Speech interfaces; Conceptual
   modelling
AB Speech interfaces are becoming more and more popular as a means to interact with virtual environments but the development and integration of these interfaces is usually still ad hoc, especially the speech grammar creation of the speech interface is a process commonly performed by hand. In this paper, we introduce an approach to automatically generate a speech grammar which is generated using semantic information. The semantic information is represented through ontologies and gathered from the conceptual modelling phase of the virtual environment application. The utterances of the user will be resolved using queries onto these ontologies such that the meaning of the utterance can be resolved. For validation purposes we augmented a city park designer with our approach. Informal tests validate our approach, because they reveal that users mainly use words represented in the semantic data, and therefore also words which are incorporated in the automatically generated speech grammar.
C1 [Vanacken, Lode; Raymaekers, Chris; Coninx, Karin] Hasselt Univ tUL IBBT, Expertise Ctr Digital Media, B-3590 Diepenbeek, Belgium.
C3 Hasselt University
RP Vanacken, L (corresponding author), Hasselt Univ tUL IBBT, Expertise Ctr Digital Media, Wetenschapspk 2, B-3590 Diepenbeek, Belgium.
EM lode.vanacken@uhasselt.be; chris.raymaekers@uhasselt.be;
   karin.coninx@uhasselt.be
CR [Anonymous], 2005, P 7 INT C MULTIMODAL
   [Anonymous], 1997, Proceedings of the fifth conference on Applied natural language processing - ANLC'97, P20
   [Anonymous], 2008, SPARQL QUERY LANGUAG
   CERNAK M, 2002, COMMAND SPEECH INTER
   COHEN PR, 1992, P ACM S US INT SOFTW, P143
   CONINX K, 2006, VIRTUAL CONCEPT 2006
   Conti G, 2006, COMPUT GRAPH-UK, V30, P619, DOI 10.1016/j.cag.2006.03.004
   Corradini A., 2002, P INT CLASS WORKSH N
   CUPPENS E, 2005, FUTURE USER INTERFAC
   GOUBRAN RA, 1996, ICASSP 96, P3545
   Irawati S., 2005, P 2005 INT C AUGM TE, P35, DOI [10.1145/1152399.1152407, DOI 10.1145/1152399.1152407]
   IRAWATI S, 2006, ACM INT C VIRT REAL, P129
   Kaiser E., 2003, ICMI 03, P12
   MARTINEZ JI, 2004, THESIS U MURCIA
   McGlashan S., 1995, P 2 INT WORKSH MIL A
   Muller J, 1998, INT CONF ACOUST SPEE, P3757, DOI 10.1109/ICASSP.1998.679701
   Otto K.A., 2005, WORKSH SEM VIRT ENV, P35
   Pfeiffer T, 2004, P IEEE VIRT REAL ANN, P35, DOI 10.1109/VR.2004.1310053
   Sharma R, 2000, IEEE COMPUT GRAPH, V20, P29, DOI 10.1109/38.824531
   2008, OWL WEB ONTOLOGY LAN
   2008, RESOURCE DESCRIPTION
NR 21
TC 0
Z9 0
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2008
VL 24
IS 11
BP 955
EP 961
DI 10.1007/s00371-008-0276-2
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 359BQ
UT WOS:000259961600005
DA 2024-07-18
ER

PT J
AU Vuçini, E
   Möller, T
   Gröller, ME
AF Vucini, Erald
   Moeller, Torsten
   Groeller, M. Eduard
TI Efficient reconstruction from non-uniform point sets
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE non-uniform reconstruction; variational approximation; B-splines; 3D
   object modeling
AB We propose a method for non-uniform reconstruction of 3D scalar data. Typically, radial basis functions, trigonometric polynomials or shift-invariant functions are used in the functional approximation of 3D data. We adopt a variational approach for the reconstruction and rendering of 3D data. The principle idea is based on data fitting via thin-plate splines. An approximation by B-splines offers more compact support for fast reconstruction. We adopt this method for large datasets by introducing a block-based reconstruction approach. This makes the method practical for large datasets. Our reconstruction will be smooth across blocks. We give reconstruction measurements as error estimations based on different parameter settings and also an insight on the computational effort. We show that the block size used in reconstruction has a negligible effect on the reconstruction error. Finally we show rendering results to emphasize the quality of this 3D reconstruction technique.
C1 [Vucini, Erald; Groeller, M. Eduard] Vienna Univ Technol, Inst Comp Graph & Algorithms, A-1040 Vienna, Austria.
   [Moeller, Torsten] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
C3 Technische Universitat Wien; Simon Fraser University
RP Vuçini, E (corresponding author), Vienna Univ Technol, Inst Comp Graph & Algorithms, Karlspl 13, A-1040 Vienna, Austria.
EM vucini@cg.tuwien.ac.at; torsten@sfu.ca; groeller@cg.tuwien.ac.at
RI Gröller, Eduard/AAH-2111-2020
OI Gröller, Eduard/0000-0002-8569-4149; Moller, Torsten/0000-0003-1192-0710
CR Aldroubi A, 2001, SIAM REV, V43, P585, DOI 10.1137/S0036144501386986
   ALDROUBI A, 2002, P INT C COMP HARM AN, V1, P1
   Alexander M, 2001, INTERNETWEEK, P21
   [Anonymous], [No title captured], DOI DOI 10.1145/99308.99316
   Arigovindan M, 2005, IEEE T IMAGE PROCESS, V14, P450, DOI 10.1109/TIP.2004.841203
   ARIGOVINDAN M, 2005, THESIS ECOLE POLYTEC
   Bruckner S, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P671
   Buhmann M.D., 2003, C MO AP C M, V12, DOI 10.1017/CBO9780511543241
   Chen BQ, 2001, IEEE VISUAL, P45, DOI 10.1109/VISUAL.2001.964492
   Haber J, 2001, IEEE VISUAL, P341
   JANG Y, 2004, EUR IEEE VGTC S VIS, DOI DOI 10.2312/VISSYM/VISSYM04/035-044
   Jang Y, 2006, COMPUT GRAPH FORUM, V25, P587, DOI 10.1111/j.1467-8659.2006.00978.x
   Juba D., 2007, MODELLING RENDERING
   KAHLER R, 2007, P IEEE EG INT S VOL, P49
   Lee S, 1997, IEEE T VIS COMPUT GR, V3, P228, DOI 10.1109/2945.620490
   Ljung P., 2006, P EUR IEEE VGTC S VI, P259
   Muigg P, 2007, IEEE T VIS COMPUT GR, V13, P1592, DOI 10.1109/TVCG.2007.70588
   Neophytou N., 2006, P EGIEEE S VISUALIZA, P13, DOI DOI 10.2312/VISSYM/EUROVIS06/013-020
   Petrosian A.A., 2001, WAVELETS SIGNAL IMAG
   Rossl Christian., 2004, PROC, P71
   Silva CT, 1996, 1996 SYMPOSIUM ON VOLUME VISUALIZATION, PROCEEDINGS, P15, DOI 10.1109/SVV.1996.558038
   Thevenaz P., 2000, Handbook of Medical Imaging, Processing and Analysis
   Tuncer TE, 2007, IEEE T SIGNAL PROCES, V55, P530, DOI 10.1109/TSP.2006.885692
   Vetterli Martin, 1995, Wavelets and Subband Coding
   Weiler M, 2004, IEEE SYMPOSIUM ON VOLUME VISUALIZATION AND GRAPHICS 2004, PROCEEDINGS, P71
   Welsh T, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P425, DOI 10.1109/VISUAL.2003.1250403
   Zwicker M, 2001, COMP GRAPH, P371, DOI 10.1145/383259.383300
NR 27
TC 8
Z9 12
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 555
EP 563
DI 10.1007/s00371-008-0236-x
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800011
DA 2024-07-18
ER

PT J
AU Wang, JN
   Oliveira, MM
   Zhang, HT
   Kaufman, AE
AF Wang, Jianning
   Oliveira, Manuel M.
   Zhang, Haitao
   Kaufman, Arie E.
TI Reconstructing regular meshes from points
SO VISUAL COMPUTER
LA English
DT Article
DE surface reconstruction; regular mesh; point parameterization; cut
   handling
AB We propose an algorithm for reconstructing regular meshes from unorganized point clouds. At first, a nearly isometric point parameterization is computed using only the location of the points. A mesh, composed of nearly equilateral triangles, is later created using a regular sampling pattern. This approach produces meshes with high visual quality and suitable for use with applications such as finite element analysis, which tend to impose strong constraints on the regularity of the input mesh. Geometric properties, such as local connectivity and surface features, are identified directly from the points and are stored independent of the resulting mesh. This decoupling preserves most details and allows more flexibility for meshing. The resulting parameterization supports several direct applications, such as texturing and bump mapping. In addition, novel boundary identification and cut parameterization algorithms are proposed to overcome the difficulties caused by cuts, non-closed surfaces and possible self-overlapping parameter patches. We demonstrate the effectiveness of our approach by reconstructing regular meshes from real datasets, such as a human colon obtained from CT scan and objects digitized using laser scanners.
C1 [Wang, Jianning; Zhang, Haitao; Kaufman, Arie E.] SUNY Stony Brook, CVC, Stony Brook, NY 11790 USA.
   [Oliveira, Manuel M.] Univ Fed Rio Grande do Sul, Inst Informat, BR-90046900 Porto Alegre, RS, Brazil.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook; Universidade Federal do Rio Grande do Sul
RP Wang, JN (corresponding author), SUNY Stony Brook, CVC, Stony Brook, NY 11790 USA.
EM wangjianning@gmail.com
RI Zhang, Haitao/IUP-7507-2023; Menezes de Oliveira Neto,
   Manuel/H-1508-2011
OI Menezes de Oliveira Neto, Manuel/0000-0003-4957-9984
CR Alliez P, 2003, ACM T GRAPHIC, V22, P485, DOI 10.1145/882262.882296
   Alliez P, 2002, ACM T GRAPHIC, V21, P347, DOI 10.1145/566570.566588
   Amenta N, 2004, ACM T GRAPHIC, V23, P264, DOI 10.1145/1015706.1015713
   [Anonymous], 2001, P IMR 2001 NEWP BEAC
   [Anonymous], 1986, Curve and Surface Fitting: An Introduction
   Bernardini F, 1999, IEEE T VIS COMPUT GR, V5, P349, DOI 10.1109/2945.817351
   Botsch M., 2001, Vision, Modeling, and Visualization 2001. Proceedings, P283
   BUNSEN O, 1997, SIMVIS, P66
   Desbrun M, 2002, COMPUT GRAPH FORUM, V21, P209, DOI 10.1111/1467-8659.00580
   Floater MS, 2001, COMPUT AIDED GEOM D, V18, P77, DOI 10.1016/S0167-8396(01)00013-9
   GOTSMAN C, 2003, SIGGRAPH 03, P358
   GU X, 2002, SIGGRAPH 02, P355, DOI DOI 10.1145/566570.566589
   Hartley R, 2000, MULTIPLE VIEW GEOMET
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   HORMANN K, 2003, CURVE SURFACE DESIGN, P215
   Jin M, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P267, DOI 10.1109/VISUAL.2004.75
   LOSASSO F, 2003, EUR S GEOM PROC, P138
   Mencl R, 1998, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P298, DOI 10.1109/CGI.1998.694281
   Ohtake Y, 2003, ACM T GRAPHIC, V22, P463, DOI 10.1145/882262.882293
   Pauly M, 2003, COMPUT GRAPH FORUM, V22, P281, DOI 10.1111/1467-8659.00675
   Praun E, 2003, ACM T GRAPHIC, V22, P340, DOI 10.1145/882262.882274
   Rusinkiewicz S, 2002, ACM T GRAPHIC, V21, P438, DOI 10.1145/566570.566600
   Scheidegger CarlosE., 2005, Proceedings of the 3rd Eurographics symposium on Geometry processing, P63
   Sharf A, 2004, ACM T GRAPHIC, V23, P878, DOI 10.1145/1015706.1015814
   Sorkine O, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P355, DOI 10.1109/VISUAL.2002.1183795
   Tewari G, 2006, COMPUT GRAPH-UK, V30, P917, DOI 10.1016/j.cag.2006.08.019
   Wang HN, 2007, IMAGE VISION COMPUT, V25, P103, DOI 10.1016/j.imavis.2005.12.006
   Xia CY, 2006, IEEE T KNOWL DATA EN, V18, P289, DOI 10.1109/TKDE.2006.38
   Zhang HT, 2004, COMPUT GRAPH FORUM, V23, P715, DOI 10.1111/j.1467-8659.2004.00804.x
   Zwicker M, 2002, ACM T GRAPHIC, V21, P322, DOI 10.1145/566570.566584
NR 30
TC 1
Z9 1
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2008
VL 24
IS 5
BP 361
EP 371
DI 10.1007/s00371-007-0194-8
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 279NZ
UT WOS:000254363200005
DA 2024-07-18
ER

PT J
AU Chaudhuri, P
   Kalra, P
   Banerjee, S
AF Chaudhuri, Parag
   Kalra, Prern
   Banerjee, Subhashis
TI Reusing view-dependent animation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE view-dependent character animation; stylized animation; animation reuse
AB In this paper we present techniques for reusing view-dependent animation. First, we provide a framework for representing view-dependent animations. We formulate the concept of a view space, which is the space formed by the key views and their associated character poses. Tracing a path on the view space generates the corresponding view-dependent animation in real time. We then demonstrate that the framework can be used to synthesize new stylized animations by reusing view-dependent animations. We present three types of novel reuse techniques. In the first we show how to animate multiple characters from the same view space. Next, we show how to animate multiple characters from multiple view spaces. We use this technique to animate a crowd of characters. Finally, we draw inspiration from cubist paintings and create their view-dependent analogues by using different cameras to control different body parts of the same character.
C1 Univ Geneva, Ctr Univ Informat, MIRA Lab, CH-1211 Geneva 4, Switzerland.
   Indian Inst Technol, Dept Comp Sci & Engn, Delhi, India.
C3 University of Geneva; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) - Delhi
RP Chaudhuri, P (corresponding author), Univ Geneva, Ctr Univ Informat, MIRA Lab, CH-1211 Geneva 4, Switzerland.
EM paraa@miralab.unige.ch; pkalra@cse.iitd.emet.in; suban@cse.iitd.emet.in
CR Agrawala M, 2000, SPRING COMP SCI, P125
   [Anonymous], 1996, P 23 ANN C COMP GRAP
   Arikan O, 2003, ACM T GRAPHIC, V22, P402, DOI 10.1145/882262.882284
   Bregler C, 2002, ACM T GRAPHIC, V21, P399, DOI 10.1145/566570.566595
   Chaudhuri P, 2004, COMPUT GRAPH FORUM, V23, P411, DOI 10.1111/j.1467-8659.2004.00772.x
   CHAUDHURI P, 2004, P IND C VIS GRAPH IM, P95
   Coleman Patrick., 2004, P 3 INT S NONPH AN R, P129
   GLASSNER AS, 2000, MSRTR200005 MICR RES
   Hays J., 2004, PROC NPAR 01, P113
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   MARTIN D, 2000, P 1 INT S NONPH AN R, P75
   Rademacher P, 1999, COMP GRAPH, P439, DOI 10.1145/311535.311612
   Singh K, 2002, PROC GRAPH INTERF, P17
NR 13
TC 3
Z9 4
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 707
EP 719
DI 10.1007/s00371-007-0130-y
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600011
DA 2024-07-18
ER

PT J
AU Liu, CX
   Guo, YW
   Pan, L
   Peng, QS
   Zhang, FY
AF Liu, Chunxiao
   Guo, Yanwen
   Pan, Liang
   Peng, Qunsheng
   Zhang, Fuyan
TI Image completion based on views of large displacement
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE image completion; large displacement view; image stitching; texture
   synthesis
AB This paper presents an algorithm for image completion based on the views of large displacement. A distinct from most existing image completion methods, which exploit only the target image's own information to complete the damaged regions, our algorithm makes full use of a large displacement view (LDV) of the same scene, which introduces enough information to resolve the original ill-posed problem. To eliminate any perspective distortion during the warping of the LDV image, we first decompose the target image and the LDV one into several corresponding planar scene regions (PSRs) and transform the candidate PSRs on the LDV image onto their correspondences on the target image. Then using the transformed PSRs, we develop a new image repairing algorithm, coupled with graph cut based image stitching, texture synthesis based image inpainting, and image fusion based hole filling, to complete the missing regions seamlessly. Finally, the ghost effect between the repaired region and its surroundings is eliminated by Poisson image blending. Our algorithm effectively preserves the structure information on the missing area of the target image and produces a repaired result comparable to its original appearance. Experiments show the effectiveness of our method.
C1 Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
   Nanjing Univ, Natl Lab Novel Software Technol, Nanjing 210093, Peoples R China.
C3 Zhejiang University; Nanjing University
RP Liu, CX (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
EM liuchunxiao@cad.zju.edu.cn; ywguo@cad.zju.edu.cn
CR [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P IEEE COMP SOC C CO
   Ballester C, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P10, DOI 10.1109/ICCV.2001.937493
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Bertalmio M., 2001, PROC CVPR IEEE, V1, P1, DOI DOI 10.1109/CVPR.2001.990497
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Chan TF, 2005, COMMUN PUR APPL MATH, V58, P579, DOI 10.1002/cpa.20075
   COLLIS B, 2004, IEE ELECT SYSTEMS SO, V2, P22
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Drori I, 2003, ACM T GRAPHIC, V22, P303, DOI 10.1145/882262.882267
   Fadili M.J., 2005, Proceedings of the 2005 IEEE International Conference on Image Processing, V2, P61
   Hartley R, 2000, MULTIPLE VIEW GEOMET
   Jia JY, 2006, IEEE T PATTERN ANAL, V28, P832, DOI 10.1109/TPAMI.2006.108
   Jia JY, 2003, PROC CVPR IEEE, P643
   Jia YT, 2005, VISUAL COMPUT, V21, P601, DOI 10.1007/s00371-005-0313-3
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Levin A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P305
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Pavic D, 2006, VISUAL COMPUT, V22, P671, DOI 10.1007/s00371-006-0050-2
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Shen Jianhong., 2003, SIAM NEWS, V36, P1
   Shih TK, 2005, THIRD INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY AND APPLICATIONS, VOL 1, PROCEEDINGS, P15
   Sun J, 2005, ACM T GRAPHIC, V24, P861, DOI 10.1145/1073204.1073274
   Szeliski R., 2006, Image alignment and stitching: A tutorial
   Wexler Y, 2004, PROC CVPR IEEE, P120
NR 26
TC 9
Z9 10
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 833
EP 841
DI 10.1007/s00371-007-0137-4
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600023
DA 2024-07-18
ER

PT J
AU Liu, YJ
   Zhou, QY
   Hu, SM
AF Liu, Yong-Jin
   Zhou, Qian-Yi
   Hu, Shi-Min
TI Handling degenerate cases in exact geodesic computation on triangle
   meshes
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE exact geodesic computation; degenerate cases; robustness
AB The computation of exact geodesics on triangle meshes is a widely used operation in computer-aided design and computer graphics. Practical algorithms for computing such exact geodesics have been recently proposed by Surazhsky et al. [5]. By applying these geometric algorithms to real-world data, degenerate cases frequently appear. In this paper we classify and enumerate all the degenerate cases in a systematic way. Based on the classification, we present solutions to handle all the degenerate cases consistently and correctly. The common users may find the present techniques useful when they implement a robust code of computing exact geodesic paths on meshes.
C1 Tsinghua Univ, Dept Commun Sci & Technol, Beijing, Peoples R China.
C3 Tsinghua University
RP Liu, YJ (corresponding author), Tsinghua Univ, Dept Commun Sci & Technol, Beijing, Peoples R China.
EM liuyongjin@tsinghua.edu.cn; zqy@mails.tsinghua.edu.cn;
   shimin@tsinghua.edu.cn
RI Liu, Yong/GWQ-6163-2022; Hu, Shi-Min/AAW-1952-2020
CR EDELSBRUNNER H, 1990, ACM T GRAPHIC, V9, P66, DOI 10.1145/77635.77639
   Hoffmann C.M., 2001, J COMPUT INF SCI ENG, V1, P143
   MITCHELL JSB, 1987, SIAM J COMPUT, V16, P647, DOI 10.1137/0216045
   Sugihara K, 2000, ALGORITHMICA, V27, P5, DOI 10.1007/s004530010002
   Surazhsky V, 2005, ACM T GRAPHIC, V24, P553, DOI 10.1145/1073204.1073228
   ZACHARY J, 1998, INTRO SCI PROGRAMMIN
NR 6
TC 27
Z9 32
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 661
EP 668
DI 10.1007/s00371-007-0136-5
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600006
DA 2024-07-18
ER

PT J
AU Dinerstein, J
   Egbert, PK
   Cline, D
AF Dinerstein, Jonathan
   Egbert, Parris K.
   Cline, David
TI Enhancing computer graphics through machine learning: a survey
SO VISUAL COMPUTER
LA English
DT Review
DE computer graphics and animation; machine learning; generative data
   models; scattered data interpolation; survey
ID HIDDEN MARKOV-MODELS; HUMAN MOTION; ANIMATION; PARAMETERIZATION;
   RECONSTRUCTION; AVATARS; CAPTURE; ACTORS
AB Machine learning has experienced explosive growth in the last few decades, achieving sufficient maturity to provide effective tools for sundry scientific and engineering fields. Machine learning provides a firm theoretical foundation upon which to build techniques that leverage existing data to extract interesting information or to synthesize more data.
   In this paper we survey the uses of machine learning methods and concepts in recent computer graphics techniques. Many graphics techniques are data-driven; however, few graphics papers explicitly leverage the machine learning literature to underpin, validate, and develop their proposed methods. This survey provides novel insights by casting many existing computer graphics techniques into a common learning framework. This not only illuminates how these techniques are related, but also reveals possible ways in which they may be improved. We also use our analysis to propose several directions for future work.
C1 Brigham Young Univ, Dept Comp Sci, Adv 3D Comp Graph Lab, Provo, UT 84602 USA.
C3 Brigham Young University
EM jondinerstein@yahoo.com; egbert@cs.byu.edu; clinedav@gmail.com
CR Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311
   Allen B, 2002, ACM T GRAPHIC, V21, P612, DOI 10.1145/566570.566626
   [Anonymous], 2002, Neural Information Processing Systems
   [Anonymous], CINEFEX
   [Anonymous], P 25 ANN C COMP GRAP
   Arikan O, 2003, ACM T GRAPHIC, V22, P402, DOI 10.1145/882262.882284
   Arikan O, 2002, ACM T GRAPHIC, V21, P483, DOI 10.1145/566570.566606
   Auslander J, 1995, ACM T GRAPHIC, V14, P311, DOI 10.1145/225294.225295
   Badler NI, 1999, COMMUN ACM, V42, P64, DOI 10.1145/310930.310975
   Barhak J, 2001, IEEE T VIS COMPUT GR, V7, P1, DOI 10.1109/2945.910817
   BERGEN DE, 1998, P 4 INT WORKSH MULT
   Bishop C. M., 1995, NEURAL NETWORKS PATT
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   BLUMBERG B, 2002, P 29 ANN C COMP GRAP, P417
   BOWDEN R, 2000, P COMP VIS PATT REC
   Brand M, 1999, COMP GRAPH, P21, DOI 10.1145/311535.311537
   BRAND M, 2000, P SIGGRAPH 2000 ACM, P399
   Buehler C, 2001, COMP GRAPH, P425, DOI 10.1145/383259.383309
   BURKE R, 2001, P COMP GAM DEV C
   CAO Y, 2003, P ACM SIGGRAPH EUR S, P225
   Cao Yong., 2004, SCA 2004: Proceedings of the 2004 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P345
   Cheng Y, 2002, BIOMACROMOLECULES, V3, P456, DOI 10.1021/bm0156227
   Christensen P. H., 1999, Journal of Graphics Tools, V4, P1, DOI 10.1080/10867651.1999.10487505
   Conde T, 2003, LECT NOTES ARTIF INT, V2792, P175
   Cordier F, 2005, COMPUT GRAPH FORUM, V24, P173, DOI 10.1111/j.1467-8659.2005.00841.x
   Davis L., 1991, Handbook of Genetic Algorithms
   Dinerstein J, 2005, ACM T GRAPHIC, V24, P262, DOI 10.1145/1061347.1061352
   Dinerstein J, 2005, COMPUT INTELL-US, V21, P90, DOI 10.1111/j.0824-7935.2005.00266.x
   DINERSTEIN J, 2002, P COMP GRAPH ART INT, P137
   DINERSTEIN J, 2003, J GEOM GRAPH, V7, P221
   DINERSTEIN J, 2004, P COMP AN SOC AG CAS, P231
   Dinerstein J., 2001, J GRAPH TOOLS, V6, P1
   Dinerstein J, 2004, COMPUT ANIMAT VIRT W, V15, P95, DOI 10.1002/cav.8
   Dorigo M, 1999, ARTIF LIFE, V5, P137, DOI 10.1162/106454699568728
   Dumont R, 2003, ACM T GRAPHIC, V22, P152, DOI 10.1145/636886.636888
   Ezzat T, 2002, ACM T GRAPHIC, V21, P388, DOI 10.1145/566570.566594
   FALOUTSOS P, 2001, P ACM SIGGRAPH, P39
   Garland M., 1997, PROC 24 C COMPUTER G, P209, DOI DOI 10.1145/258734.258849
   Gleicher M., 2003, P 2003 S INTERACTIVE, P181
   Go J., 2004, S COMPUTER ANIMATION, P9
   Gritz L., 1997, Genetic Programming 1997 Proceedings of the Second Annual Conference, P139
   GRZESZCZUK R, 1995, P SIGGRAPH 95, P63
   Haro A, 2002, INT C PATT RECOG, P487, DOI 10.1109/ICPR.2002.1044771
   Hase K, 2003, J VISUAL COMP ANIMAT, V14, P73, DOI 10.1002/vis.306
   Haykin S., 1994, NEURAL NETWORKS COMP
   Hays J., 2004, PROC NPAR 01, P113
   Hearst MA, 1998, IEEE INTELL SYST APP, V13, P18, DOI 10.1109/5254.708428
   Heidrich W, 1999, COMP GRAPH, P171, DOI 10.1145/311535.311554
   Hertzmann A, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P22, DOI 10.1109/PCCGA.2003.1238242
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Hirsch M. W., 1997, Graduate Texts in Mathematics, V33
   HOFFMANN M, 2002, P 5 INT C COMP GRAPH, P103
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Isla D., 2001, Proceedings of ICJAI, P1051
   James DL, 2003, ACM T GRAPHIC, V22, P879, DOI 10.1145/882262.882359
   Jensen HW., 2001, REALISTIC IMAGE SYNT, DOI [10.1201/9780429294907, DOI 10.1201/9780429294907]
   JENSEN HW, 1998, P SIGGRAPH 98, P311, DOI DOI 10.1145/280814.280925
   Jeong WK, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P404, DOI 10.1109/PCCGA.2003.1238284
   Johnson M.P., 1999, CHI'99 Proceedings, P152
   Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301
   KAUTZ J, 1999, P 10 EUR WORKSH REND, P281
   Kautz J., 2000, HWWS'00: Proceedings of the ACM SIGGRAPH/EUROGRAPHICS Workshop on Graphics Hardware, (New York, NY, USA), P51
   Kim SB, 2000, NUTR CANCER, V37, P41, DOI 10.1207/S15327914NC3701_5
   Kohonen, 1989, SELF ORG ASS MEMORY
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Kovar L, 2004, ACM T GRAPHIC, V23, P559, DOI 10.1145/1015706.1015760
   KOZA JR, 1994, STAT COMPUT, V4, P87, DOI 10.1007/BF00175355
   KROGH A, 1994, J MOL BIOL, V235, P1501, DOI 10.1006/jmbi.1994.1104
   KUFFNER JJ, 1999, P IEEE INT C COMP AN
   Laird J. E., 2001, Proceedings of the Fifth International Conference on Autonomous Agents, P385, DOI 10.1145/375735.376343
   Latta L, 2002, ACM T GRAPHIC, V21, P509, DOI 10.1145/566570.566610
   Lee Jehee., 2004, SCA 2004: Proceedings of the 2004 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P79
   Lee JH, 2002, ACM T GRAPHIC, V21, P491
   Lee JR, 2004, COMPUT ANIMAT VIRT W, V15, P327, DOI 10.1002/cav.36
   Li Y, 2002, ACM T GRAPHIC, V21, P465
   LIM IS, 1999, P EUR WORKSH AN SIM
   Liu JM, 2002, AUTON AGENT MULTI-AG, V5, P397, DOI 10.1023/A:1019656327617
   LV F, 2004, P PERF EV TRACK SURV
   Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343
   MCCLAVE J, 2004, STATISTICS
   McCool MD, 2001, COMP GRAPH, P171, DOI 10.1145/383259.383276
   Metoyer RA, 2003, COMP ANIM CONF PROC, P149, DOI 10.1109/CASA.2003.1199318
   Millar RJ, 1999, COMPUT GRAPH-UK, V23, P127, DOI 10.1016/S0097-8493(98)00121-6
   MITCHELL T, 1989, ANNU REV COMPUT SCI, V4, P417
   Multon F, 1999, J VISUAL COMP ANIMAT, V10, P39, DOI 10.1002/(SICI)1099-1778(199901/03)10:1<39::AID-VIS195>3.0.CO;2-2
   NGO JT, 1993, P SIGGRAPH 93, P343
   NOSER H, 1995, COMPUT GRAPH, V19, P7, DOI 10.1016/0097-8493(94)00117-H
   Parent Rick., 2002, COMPUTER ANIMATION
   Pighin Frederic., 2004, SCA'04: Proceedings ofthe 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation, P223, DOI 10.1145/1028523.1028552
   Pina A, 2000, COMPUT GRAPH-UK, V24, P297, DOI 10.1016/S0097-8493(99)00165-X
   PINA A, 2003, P 3 IASTED INT C VIS, P25
   PINA A, 2002, P S SMART GRAPH, P17
   Pullen K., 2002, ACM Transactions on Graphics, V21, P501, DOI 10.1145/566570.566608
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Reissell LM, 2001, COMPUT GRAPH FORUM, V20, pC339
   Rose C, 1998, IEEE COMPUT GRAPH, V18, P32, DOI 10.1109/38.708559
   Russell S., 2016, Artificial intelligence a modern approach
   Safonova A, 2004, ACM T GRAPHIC, V23, P514, DOI 10.1145/1015706.1015754
   SALESIN D, 2003, NEUR INF PROC SYST N
   Sand P, 2003, ACM T GRAPHIC, V22, P578, DOI 10.1145/882262.882310
   Schödl A, 2000, COMP GRAPH, P489, DOI 10.1145/344779.345012
   SCHODL A, 2001, P NEUR INF PROC SYST, P1002
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   SEMET Y, 2004, P GEN EV C GECCO 04
   Seo Hyewon., 2003, Proceedings of the 2003 Symposium on Interactive 3D Graphics, P19, DOI [10.1145/641480.641487, DOI 10.1145/641480.641487]
   Sims K., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P15, DOI 10.1145/192161.192167
   SINGH G, 2004, RECENT DEV BIOL INSP, pCH6
   Sonka M., 2014, Image processing, analysis, and machine vision
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   SYKORA D, 2004, P 3 INT S NONPH AN R, P121
   Terzopoulos D., 1999, Communications of the ACM, V42, P32, DOI 10.1145/310930.310966
   TOMLINSON B, 2002, 1 GSFC JPL WORKSH RA
   VANDEPANNE M, 1994, P 5 EUR WORKSH SIM A
   VANDEPANNE M, 1993, P SIGGRAPH 1993
   Vasilescu MAO, 2004, ACM T GRAPHIC, V23, P336, DOI 10.1145/1015706.1015725
   Vrady L., 1999, J GEOMETRY GRAPHICS, V3, P177
   Welsh T, 2002, ACM T GRAPHIC, V21, P277, DOI 10.1145/566570.566576
   Wilson DR, 2000, COMPUT INTELL-US, V16, P1, DOI 10.1111/0824-7935.00103
   Yannakakis G. N., 2003, P 11 MED C CONTR AUT
   YOON B, 2000, P AAAI, P249
   ZHAO T, 2004, P COMP VIS PATT REC
NR 121
TC 3
Z9 7
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2007
VL 23
IS 1
BP 25
EP 43
DI 10.1007/s00371-006-0085-4
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 113RB
UT WOS:000242613900004
DA 2024-07-18
ER

PT J
AU Lai, YK
   Hu, SM
   Martin, RR
AF Lai, Yu-Kun
   Hu, Shi-Min
   Martin, Ralph R.
TI Surface mosaics
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE surface mosaics; particle optimization; Manhattan metric; overlapping
   local parameterizations
AB This paper considers the problem of placing mosaic tiles on a surface to produce a surface mosaic. We assume that the user specifies a mesh model, the size of the tiles and the amount of grout, and, optionally, a few control vectors at key locations on the surface indicating the preferred tile orientation at these points. From these inputs, we place equal-sized rectangular tiles over the mesh such as to almost cover it, with controlled orientation. The alignment of the tiles follows a vector field which is interpolated over the surface from the control vectors and also forced into alignment with any sharp creases, open boundaries, and boundaries between regions of different colors. Our method efficiently solves the problem by posing it as globally optimizing a spring-like energy in the Manhattan metric, using overlapping local parameterizations. We demonstrate the effectiveness of our algorithm with various examples.
C1 Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   Cardiff Univ, Cardiff, Wales.
C3 Tsinghua University; Cardiff University
RP Hu, SM (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
EM laiyk03@mails.tsinghua.edu.cn; shimin@tsinghua.edu.cn; ralph@cs.cf.ac.uk
RI Martin, Ralph R/D-2366-2010; Lai, Yu-Kun/D-2343-2010; Hu,
   Shi-Min/AAW-1952-2020
OI Martin, Ralph/0000-0002-8495-8536
CR Alliez P, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P49
   Alliez P, 2003, ACM T GRAPHIC, V22, P485, DOI 10.1145/882262.882296
   Botsch M, 2001, COMPUT GRAPH FORUM, V20, pC402, DOI 10.1111/1467-8659.00533
   Cohen-Steiner D., 2003, SCG'03: Proceedings ofthe nineteenth annual symposium on Computational geometry, P312
   Elber G, 2003, VISUAL COMPUT, V19, P67, DOI 10.1007/s00371-002-0175-x
   FINKELSTEIN A, 1998, P 7 INT C EL PUBL, P11
   Floater MS, 2005, MATH VIS, P157, DOI 10.1007/3-540-26808-1_9
   Hausner A, 2001, COMP GRAPH, P573, DOI 10.1145/383259.383327
   Kaplan CS, 2000, COMP GRAPH, P499, DOI 10.1145/344779.345022
   Kim J, 2002, ACM T GRAPHIC, V21, P657
   KLEIN AW, 2002, 2 INT S NON PHOT REN, P21
   Marinov M, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P207, DOI 10.1109/PCCGA.2004.1348351
   MOUT D, ANN LIB APPROXIMATE
   RAY N, 2006, IN PRESS ACM T GRAPH
   Silvers R., 1997, Photomosaics
   Surazhsky V, 2003, P EUR S GEOM PROC, P17
   SZELISKI R, 1992, COMP GRAPH, V26, P185, DOI 10.1145/142920.134037
   Turk G, 2001, COMP GRAPH, P347, DOI 10.1145/383259.383297
   Wei LY, 2001, COMP GRAPH, P355, DOI 10.1145/383259.383298
   Witkin A.P., 1994, P 21 ANN C COMPUTER, P269, DOI [10.1145/1198555.1198656, DOI 10.1145/1198555.1198656, DOI 10.1145/192161.192227]
   Yang Y.-L., 2006, P 4 EUR S GEOM PROC, P223
NR 21
TC 20
Z9 25
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 604
EP 611
DI 10.1007/s00371-006-0047-x
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000003
DA 2024-07-18
ER

PT J
AU Li, XL
   Yang, J
   Zhu, YM
AF Li, Xiaoliang
   Yang, Jie
   Zhu, Yuemin
TI Combining estimators for Monte Carlo volume rendering with shading
SO VISUAL COMPUTER
LA English
DT Article
DE volume rendering; Monte Carlo method; visualization; computer graphics
AB Monte Carlo volume rendering (MCVR), which allows for generating X-ray like images, offers significant advantages over existing volume rendering techniques in terms of time and memory complexity. The classical shading for MCVR, however, results in a low quality image. In this paper, a so-called estimators combining technique is presented that estimates a realistic rendering using the Lambertian reflection illumination model. Such a technique is particularly adapted to changes in viewing and lighting, and provides high quality shading images exhibiting strong shape and spatial relationship of objects.
C1 Shanghai Jiao Tong Univ, Inst Image Proc & Pattern Recognit, Shanghai 200240, Peoples R China.
   CREATIS, CNRS, Res Unit 5515, F-69621 Villeurbanne, France.
   CREATIS, INSERM, U630, F-69621 Villeurbanne, France.
C3 Shanghai Jiao Tong University; Institut National de la Sante et de la
   Recherche Medicale (Inserm); Institut National des Sciences Appliquees
   de Lyon - INSA Lyon; Centre National de la Recherche Scientifique
   (CNRS); Institut National des Sciences Appliquees de Lyon - INSA Lyon;
   Institut National de la Sante et de la Recherche Medicale (Inserm)
RP Li, XL (corresponding author), Shanghai Jiao Tong Univ, Inst Image Proc & Pattern Recognit, Shanghai 200240, Peoples R China.
EM leomore@sjtu.edu.cn; jieyang@sjtu.edu.cn;
   Yue-Min.Zhu@creatis.insa-lyon.fr
RI Yang, Jie/JCD-9867-2023; Zhu, Yuemin/K-7292-2014
OI Zhu, Yuemin/0000-0001-6814-1449
CR Cabral B., 1994, P 1994 S VOLUME VISU, P91, DOI DOI 10.1145/197938.197972
   Chiueh TC, 1997, VISUALIZATION '97 - PROCEEDINGS, P329, DOI 10.1109/VISUAL.1997.663900
   Cséfalvi B, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P449, DOI 10.1109/VISUAL.2003.1250406
   Drebin R. A., 1988, Computer Graphics, V22, P65, DOI 10.1145/378456.378484
   Entezari A, 2002, IEEE/ACM SIGGRAPH SYMPOSIUM ON VOLUME VISUALIZATION AND GRAPHICS 2002, PROCEEDINGS, P131, DOI 10.1109/SWG.2002.1226519
   Kaufman A., 2005, VISUALIZATION HDB, V1st, P127, DOI DOI 10.1016/B978-012387582-2/50009-5
   LACROUTE P, 1994, INT C COMP GRAPH INT, P451
   LEVOY M, 1988, IEEE COMPUT GRAPH, V8, P29, DOI 10.1109/38.511
   Levoy M., 1992, Proceedings. Graphics Interface '92, P61
   Ma KL, 2003, COMPUT GRAPH-UK, V27, P725, DOI 10.1016/S0097-8493(03)00146-8
   MALZBENDER T, 1993, ACM T GRAPHIC, V12, P233, DOI 10.1145/169711.169705
   Sobol I.M., 1994, A Primer for the Monte Carlo Method, DOI DOI 10.1201/9781315136448
   Sweeney Jon., 2002, PROC S DATA VISUALIS, P95
   Totsuka T., 1993, Computer Graphics Proceedings, P271, DOI 10.1145/166117.166152
   Veach E., 1995, P 22 ANN C COMP GRAP, P419, DOI [DOI 10.1145/218380.218498, 10.1145/218380.218498]
   WESTOVER L, 2000, ACM COMPUT GRAPH, V24, P367
NR 16
TC 0
Z9 0
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2006
VL 22
IS 7
BP 468
EP 477
DI 10.1007/s00371-006-0016-4
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 074VG
UT WOS:000239839300003
DA 2024-07-18
ER

PT J
AU Jeong, K
   Ni, A
   Lee, S
   Markosian, L
AF Jeong, K
   Ni, A
   Lee, S
   Markosian, L
TI Detail control in line drawings of 3D meshes
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE nonphotorealistic rendering; line drawing; level-of-detail; progressive
   mesh
AB We address the problem of rendering a 3D mesh in the style of a line drawing, in which little or no shading is used and instead shape cues are provided by silhouettes and suggestive contours. Our specific goal is to depict shape features at a chosen scale. For example, when mesh triangles project into the image plane at subpixel sizes, both suggestive contours and silhouettes may form dense networks that convey shape poorly. The solution we propose is to convert the input mesh to a multiresolution representation (specifically, a progressive mesh), then view-dependently refine or coarsen the mesh to control the size of its triangles in image space. We thereby control the scale of shape features that are depicted via silhouettes and suggestive contours. We propose a novel refinement criterion that achieves this goal and address the problem of maintaining temporal coherence of silhouette and suggestive contours when extracting them from a changing mesh.
C1 POSTECH, Dept Comp Sci & Engn, Pohang 790784, South Korea.
   Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA.
C3 Pohang University of Science & Technology (POSTECH); University of
   Michigan System; University of Michigan
RP POSTECH, Dept Comp Sci & Engn, Pohang 790784, South Korea.
EM misterq@postech.ac.kr; alexni@umich.edu; leesy@postech.ac.kr;
   sapo@umich.edu
CR [Anonymous], S 3D DAT PROC VIS TR
   DeCarlo D, 2003, ACM T GRAPHIC, V22, P848, DOI 10.1145/882262.882354
   DeCarlo Doug., 2004, P INT S NONPHOTOREAL, P15, DOI DOI 10.1145/987657.987661
   Deussen O, 2000, COMP GRAPH, P13, DOI 10.1145/344779.344792
   GRABLI S., 2004, P PAC GRAPH
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Hoppe H., 1997, P SIGGRAPH 97, P189, DOI DOI 10.1145/258734.258843
   Isenberg T, 2003, IEEE COMPUT GRAPH, V23, P28, DOI 10.1109/MCG.2003.1210862
   Kirsanov D., 2003, Symposium on Geometry Processing, P102
   Kowalski MA, 1999, COMP GRAPH, P433, DOI 10.1145/311535.311607
   Lake C., 2000, Proceedings of the first international symposium on Non-photorealistic animation and rendering-NPAR'00, P13, DOI 10.1145/340916.3409185[27]M.S.
   Markosian L., 2000, NPAR, P59
   NI A, 2005, MULTISCALE LINE DRAW
   PRAUN E, 2001, P SIGGRAPH 2001, P579
   RUSINKIEWICZ S, 2004, REAL TIME SUGGESTIVE
   WEBB M, 2002, NPAR 2002, P53
   WILSON B, 2004, NPAR 2004
   Winkenbach G., 1994, Proceedings of the 21st Annual Conference on Computer Graphics and Interactive Techniques, P91
   WINKENBACH G, 1996, P SIGGRAPH 96, P469
NR 19
TC 5
Z9 7
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 698
EP 706
DI 10.1007/s00371-005-0323-1
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400022
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Nguyen, MX
   Yuan, XR
   Chen, BQ
AF Nguyen, MX
   Yuan, XR
   Chen, BQ
TI Geometry completion and detail generation by texture synthesis
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE detail preservation; geometry image and mesh editing; partial
   differential equation; surface completion
AB We present a novel method for patching holes in polygonal meshes and synthesizing surfaces with details based on existing geometry. The most novel feature of our proposed method is that we transform the 3D geometry synthesis problem into a 2D domain by parameterizing surfaces and solve this problem in that domain. We then derive local geometry gradient images that encode intrinsic local geometry properties, which are invariant to object translation and rotation. The 3D geometry of holes is then reconstructed from synthesized local gradient images. This method can be extended to execute other mesh editing operations such as geometry detail transfer or synthesis. The resulting major benefits of performing geometry synthesis in 2D are more flexible and robust control, better leveraging of the wealth of current 2D image completion methods, and greater efficiency.
C1 Univ Minnesota Twin Cities, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
C3 University of Minnesota System; University of Minnesota Twin Cities
RP Univ Minnesota Twin Cities, Dept Comp Sci & Engn, 200 Union St SE, Minneapolis, MN 55455 USA.
EM mnguyen@cs.umn.edu; xyuan@cs.umn.edu; baoquan@cs.umn.edu
RI Yuan, Xiaoru/E-1798-2013
OI Yuan, Xiaoru/0000-0002-7233-980X
CR ALIEZ P, 2002, SIGGRAPH 02, P347
   [Anonymous], 2004, S GEOM PROC
   Ashikhmin M., 2001, P 2001 S INT 3D GRAP, P217, DOI DOI 10.1145/364338.364405
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Davis J, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P428, DOI 10.1109/TDPVT.2002.1024098
   de Berg M., 2000, COMPUTATIONAL GEOMET
   Desbrun M, 2002, COMPUT GRAPH FORUM, V21, P209, DOI 10.1111/1467-8659.00580
   do Carmo MP., 1992, RIEMANNIAN GEOMETRY
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   FATTAL R, 2002, SIGGRAPH 02, P249
   Funkhouser T, 2004, ACM T GRAPHIC, V23, P652, DOI 10.1145/1015706.1015775
   Gu X., 2003, P 2003 EUROGRAPHICSA, P127
   Gu X., 2002, ACM Transactions on Graphics (TOG), P355
   Gu XF, 2004, IEEE T MED IMAGING, V23, P949, DOI 10.1109/TMI.2004.831226
   Haeberli P., 1990, SIG GRAPH, P207
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Jin M, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P267, DOI 10.1109/VISUAL.2004.75
   Ju T, 2004, ACM T GRAPHIC, V23, P888, DOI 10.1145/1015706.1015815
   LAI YK, 2005, SPM 05, P15
   Lévy B, 2003, ACM T GRAPHIC, V22, P364, DOI 10.1145/882262.882277
   Lévy B, 2002, ACM T GRAPHIC, V21, P362, DOI 10.1145/566570.566590
   Liang L, 2001, ACM T GRAPHIC, V20, P127, DOI 10.1145/501786.501787
   LIEPA P, 2003, SGP 03
   Ostromoukhov V, 2004, ACM T GRAPHIC, V23, P488, DOI 10.1145/1015706.1015750
   Pfeifle R, 1996, PROC GRAPH INTERF, P186
   Saad Yousef., 2003, Iterative Methods for Sparse Linear Systems
   Sharf A, 2004, ACM T GRAPHIC, V23, P878, DOI 10.1145/1015706.1015814
   Shewchuk JR, 2002, COMP GEOM-THEOR APPL, V22, P21, DOI 10.1016/S0925-7721(01)00047-5
   TAUBIN G, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P902, DOI 10.1109/ICCV.1995.466840
   Verdera J, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 2, PROCEEDINGS, P903
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
NR 31
TC 14
Z9 22
U1 1
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 669
EP 678
DI 10.1007/s00371-005-0315-1
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400019
DA 2024-07-18
ER

PT J
AU Sadgal, M
   El Fazziki, A
   Ouahman, AA
AF Sadgal, M
   El Fazziki, A
   Ouahman, AA
TI Aerial image processing and object recognition
SO VISUAL COMPUTER
LA English
DT Article
DE categorization; recognition; cooperation; modeling; aerial image
ID NETWORK; SHAPE
AB The processing of images representing natural scenes requires substantial elaboration at all levels: preprocessing, segmentation, recognition, and interpretation. These steps unmistakably influence the result quality of a vision system, so it must be endowed with some capabilities. We present here the vision problem in terms of internal organizatio and information management. The object is represented on a scale of categories and the task of the recognition algorithms is to find the most detailed category according to information extracted from the image. All tasks operate on one level. On this principle, we propose a model for the internal representation of a vision system, which tries to generalize the recognition of objects using categorization and cooperation.
C1 Fac Sci Semlalia Marrakech, Dept Informat, Bd Moulay Abdellah, Morocco.
   Fac Sci Semlalia Marrakech, Dept Phys, Bd Moulay Abdellah, Morocco.
C3 Cadi Ayyad University of Marrakech; Cadi Ayyad University of Marrakech
RP Fac Sci Semlalia Marrakech, Dept Informat, BP 2390, Bd Moulay Abdellah, Morocco.
EM sadgal@ucam.ac.ma
RI abdelaziz, elfazziki/Y-1693-2019; Sadgal, mohamed/IAS-0281-2023
OI El Fazziki, Abdelaziz/0000-0002-0302-234X; Sadgal,
   Mohamed/0000-0002-1087-0988
CR Airault S., 1995, Traitement du Signal, V12, P189
   AYACHE N, 1986, IEEE T PATTERN ANAL, V8, P44, DOI 10.1109/TPAMI.1986.4767751
   Basri R, 1996, INT J COMPUT VISION, V19, P147, DOI 10.1007/BF00055802
   BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115
   Comaniciu D., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1197, DOI 10.1109/ICCV.1999.790416
   DERICHE R, 1990, IEEE T PATTERN ANAL, V12, P78, DOI 10.1109/34.41386
   DERRODE S, 2002, SPIES INT S REM SENS, P22
   EDELMAN S, 1992, VISION RES, V32, P2385, DOI 10.1016/0042-6989(92)90102-O
   EDELMAN S, 1997, VISUAL RECOGNITION C
   Edelman Shimon., 1999, REPRESENTATION RECOG
   ELKHARKI O, 1999, 1 RENC SYST EL INF S
   ELKHARKI O, 2000, INT ARCH PHOTOGRAMM, P179
   ELKHARKI O, 2002, J MAGHREBIEN PHYS, V2
   ELKHARKI O, 2002, ASME J
   Erickson MA, 1998, J EXP PSYCHOL GEN, V127, P107, DOI 10.1037/0096-3445.127.2.107
   HUMMEL JE, 1992, PSYCHOL REV, V99, P480, DOI 10.1037/0033-295X.99.3.480
   LOPES A, 1999, INFORMATION PROCESSI
   Mel BW, 1997, NEURAL COMPUT, V9, P777, DOI 10.1162/neco.1997.9.4.777
   POGGIO T, 1990, NATURE, V343, P263, DOI 10.1038/343263a0
   ROSCH E, 1976, COGNITIVE PSYCHOL, V8, P382, DOI 10.1016/0010-0285(76)90013-X
   Rosch E., 1978, COGNITION CATEGORIZA, P27, DOI DOI 10.1016/B978-1-4832-1446-7.50028-5
   SADGAL M, 2001, TELECOM 2001 2 JFMMA
   SADGAL M, 2000, MCSEAI 2000 FES MOR
   Tarr MJ, 1998, NAT NEUROSCI, V1, P275, DOI 10.1038/1089
   TOUZI R, 1988, THESIS U P SABATIER
   Tupin F, 1998, IEEE T GEOSCI REMOTE, V36, P434, DOI 10.1109/36.662728
   WALTER V, 1998, IAPRS, V32, P545
   WIDROW B, 1990, P IEEE, V78, P1415, DOI 10.1109/5.58323
   WROBEL B, 1987, ACT ONZ C GRETSI NIC
NR 29
TC 3
Z9 3
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2005
VL 21
IS 1-2
BP 118
EP 123
DI 10.1007/s00371-004-0275-x
PG 6
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 911OG
UT WOS:000228013300009
DA 2024-07-18
ER

PT J
AU Simionescu, PA
   Beale, D
AF Simionescu, PA
   Beale, D
TI Visualization of hypersurfaces and multivariable (objective) functions
   by partial global optimization
SO VISUAL COMPUTER
LA English
DT Article
DE hypergeometry; hypersurface; visualization; optimization; design studies
ID DESIGN OPTIMIZATION
AB Hypersurfaces of the type z=F(x(1),...,x(n)), where F are single-valued functions of n real variables, cannot be visualized directly due to our inability to perceive dimensions higher than three. However, by projecting them down to two or three dimensions many of their properties can be revealed. In this paper a method to generate such projections is proposed, requiring successive global minimizations and maximizations of the function with respect to n-1 or n-2 variables. A number of examples are given to show the usefulness of the method, particularly for optimization problems where there is a direct interest in the minimum or maximum domains of objective functions.
C1 Univ Tulsa, Dept Mech Engn, Tulsa, OK 74104 USA.
   Auburn Univ, Dept Mech Engn, Auburn, AL 36849 USA.
C3 University of Tulsa; Auburn University System; Auburn University
RP Univ Tulsa, Dept Mech Engn, 600 S Coll Ave, Tulsa, OK 74104 USA.
RI Simionescu, Petru A./AAR-4982-2020
OI Simionescu, Petru A./0000-0003-0181-7277
CR Ackley D., 1987, A Connectionist Machine for Genetic Hillclimbing
   [Anonymous], 1989, Numerical recipes in Fortran: the art of scientific computing
   [Anonymous], 2002, Algorithms for minimization without derivatives
   [Anonymous], STAT IMAGE PROCESSIN
   ARABEYRE J., 1969, TRANSPORT SCI, V3, P140
   ASIMOV D, 1985, SIAM J SCI STAT COMP, V6, P128, DOI 10.1137/0906011
   BACK T, 1997, EVOLUTIONARY ALGORIT
   Bajaj C. L., 1990, Computer Graphics, V24, P117, DOI 10.1145/91394.91428
   Banchoff T.F., 1986, STAT IMAGE PROCESSIN, P187
   Bhaniramka P, 2000, IEEE VISUAL, P267, DOI 10.1109/VISUAL.2000.885704
   Chen JX, 2001, COMPUT SCI ENG, V3, P110, DOI 10.1109/5992.947115
   CORNELL J, 2002, EXPT MIXTURES
   Craven M. W., 1992, International Journal on Artificial Intelligence Tools (Architectures, Languages, Algorithms), V1, P399, DOI 10.1142/S0218213092000260
   ENCARNACAO JL, 1990, COMPUTER AIDED DESIG
   Feiner S., 1990, Computer Graphics, V24, P37, DOI 10.1145/91394.91412
   FEINER S, 1990, P ACM S US INT SOFTW, P76
   Grune L., 1999, Computing and Visualization in Science, V1, P221, DOI 10.1007/s007910050020
   HAIMES YY, 1971, IEEE T SYST MAN CYB, VSMC1, P296
   HANSEN P, 1989, J MECH TRANSM-T ASME, V111, P345, DOI 10.1115/1.3259005
   Hinton G. E., 1984, BOLTZMANN MACHINES C
   INSELBERG A, 1990, P 1 IEEE C VIS, V1, P361
   Inselberg A, 1985, VISUAL COMPUT, V1, P69, DOI 10.1007/BF01898350
   Jones C., 1996, Visualization and Optimization
   JONES CV, 1998, INTERACTIVE T OR MS
   Keiper J., 1994, ORSA Journal on Computing, V6, P273, DOI 10.1287/ijoc.6.3.273
   Messac A, 2000, ENG OPTIMIZ, V32, P721, DOI 10.1080/03052150008941319
   MIHALKO WM, 1991, ORTHOP TODAY, V11, P3
   NAPEL S, 1992, RADIOLOGY, V185, P607, DOI 10.1148/radiology.185.2.1410382
   NIELSON GM, 1991, IEEE COMPUT GRAPH, V11, P47, DOI 10.1109/38.79453
   NOLL AM, 1967, COMMUN ACM, V10, P469, DOI 10.1145/363534.363544
   Papalambros P. Y., 2000, Principles of Optimal Design: Modeling and Computation
   Parkinson AR, 2002, STRUCT MULTIDISCIP O, V23, P127, DOI 10.1007/s00158-002-0172-8
   PUDMENZKY A, 1998, AUSTR C NEUR NETW AC
   ROSENBROCK HH, 1960, COMPUT J, V3, P175, DOI 10.1093/comjnl/3.3.175
   Rossnick SLG., 1986, Proceedings of the IEEE computers in cardiology conference, P193
   Shaffer CA, 1998, VISUALIZATION '98, PROCEEDINGS, P491, DOI 10.1109/VISUAL.1998.745351
   STREETER M, 2001, P INT SOC OPTICAL EN, V8, P234
   van Wijk J. J., 1993, Proceedings Visualization '93. (Cat. No.93CH3354-8), P119, DOI 10.1109/VISUAL.1993.398859
   WEGENKITTL R, 1997, P IEEE VIS C PHOEN A, P291
   WEJCHERT J, 1990, ADV NEURAL INFORMATI, P465
   Winer EH, 2002, STRUCT MULTIDISCIP O, V23, P412, DOI 10.1007/s00158-002-0203-5
   Winer EH, 2001, STRUCT MULTIDISCIP O, V22, P219, DOI 10.1007/s001580100139
   *WOLFR RES, 2004, MATH WORLD
   Wright H, 2000, IEEE VISUAL, P291, DOI 10.1109/VISUAL.2000.885707
   Yao X, 1999, IEEE T EVOLUT COMPUT, V3, P82, DOI 10.1109/4235.771163
NR 45
TC 11
Z9 13
U1 1
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2004
VL 20
IS 10
BP 665
EP 681
DI 10.1007/s00371-004-0260-4
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 876TH
UT WOS:000225520000005
DA 2024-07-18
ER

PT J
AU Dekkers, D
   van Overveld, K
   Golsteijn, R
AF Dekkers, D
   van Overveld, K
   Golsteijn, R
TI Combining CSG modeling with soft blending using Lipschitz-based implicit
   surfaces
SO VISUAL COMPUTER
LA English
DT Article
DE implicit surface; smooth blending; Lipschitz condition
AB In this paper a general method is given for combining CSG modeling with soft blending using implicit surfaces. A class of various blending functions sharing some desirable properties like differentiability and intuitive blend control are given. The functions defining the CSG objects satisfy the Lipschitz condition that gives the possibility of fast root finding but can also prove useful in the field of collision detection and adaptive triangulation.
C1 Eindhoven Univ Technol, NL-5600 MB Eindhoven, Netherlands.
C3 Eindhoven University of Technology
RP Dekkers, D (corresponding author), Eindhoven Univ Technol, NL-5600 MB Eindhoven, Netherlands.
CR [Anonymous], 1984, COMPUT AIDED GEOM D
   BLINN J, 1982, ACM T GRAPHIC, V1, P234
   BLOOMENTHAL J, 1988, P8900106 XER PARC
   Hart JC, 1996, VISUAL COMPUT, V12, P527, DOI 10.1007/s003710050084
   PASKO A, 1995, VISUAL COMPUT, V11, P429, DOI 10.1007/BF02464333
   RICCI A, 1973, COMPUT J, V16, P157, DOI 10.1093/comjnl/16.2.157
   WYVILL B, 1990, COMPUTER ANIMATION T
   Wyvill G., 1986, Visual Computer, V2, P227, DOI 10.1007/BF01900346
NR 8
TC 4
Z9 7
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING STREET, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2004
VL 20
IS 6
BP 380
EP 391
DI 10.1007/s00371-002-0198-3
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 846DU
UT WOS:000223296100003
DA 2024-07-18
ER

PT J
AU Krahnstoever, N
   Lorenz, C
AF Krahnstoever, N
   Lorenz, C
TI Computing curvature-adaptive surface triangulations of three-dimensional
   image data
SO VISUAL COMPUTER
LA English
DT Article
DE surface triangulation; surface meshes; curvature adaptation; Voronoi
   graph; Delaunay triangulation
ID OBJECT RECONSTRUCTION
AB An adaptive surface triangulation algorithm for binary 3D image data is presented. The proposed method is robust and efficient and yields a high-quality surface-mesh description. The main idea of the algorithm is based on selecting a subset of the object voxel surface elements as the vertices of the triangulation and to obtain a discrete approximation of the Voronoi graph on the object surface to generate its dual, the surface Delaunay triangulation. The presented approach incorporates local object shape into the early stages of the algorithm, yielding an elegant method for obtaining shape-adaptive triangular meshes. One of the main advantages of the presented method over previous approaches is that no intermediate surface representation other than the trivial voxel representation provided by the input data is necessary. The method is presented in detail and tested on synthetic as well as real images.
C1 Gen Elect Global Res, Visualizat & Comp Vis, Niskayuna, NY 12309 USA.
   Philips Res Labs, Div Tech Syst, D-22335 Hamburg, Germany.
C3 General Electric; Philips; Philips Research
RP Gen Elect Global Res, Visualizat & Comp Vis, 1 Res Circle, Niskayuna, NY 12309 USA.
EM nils.krahnstoever@research.ge.com; cristian.lorenz@philips.com
CR [Anonymous], P 3 D DIG IM MOD
   BAKER TJ, 1991, INT J NUMER METH ENG, V32, P665, DOI 10.1002/nme.1620320404
   BAKER TJ, 1998, P 6 INT C NUM GRID G, P337
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   Davis MH, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pB627
   Delingette H, 1999, INT J COMPUT VISION, V32, P111, DOI 10.1023/A:1008157432188
   Delingette H, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P311, DOI 10.1109/ICCV.1998.710736
   Eck M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P325, DOI 10.1145/237170.237271
   Eck M, 1995, P 22 ANN C COMP GRAP, P173, DOI DOI 10.1145/218380.218440
   Fleischmann Peter., 1997, In_Proceedings_of_the 6th_International_Meshing_Roundtable, P267
   Hartmann E, 1998, VISUAL COMPUT, V14, P95, DOI 10.1007/s003710050126
   HECKBERT P, 1995, CMUCS95194
   Hilton A, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL II, P381, DOI 10.1109/ICIP.1996.560840
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   IMIELINSKA C, 1998, P 2 US C NAT LIB MED
   Johnson AE, 1998, IMAGE VISION COMPUT, V16, P635, DOI 10.1016/S0262-8856(98)00074-2
   Kobbelt LP, 1999, COMPUT GRAPH FORUM, V18, pC119, DOI 10.1111/1467-8659.00333
   Krahnstöver N, 1999, PROC SPIE, V3661, P620, DOI 10.1117/12.348618
   LI A, 1994, PATTERN RECOGN, V27, P727, DOI 10.1016/0031-3203(94)90050-7
   Lin CC, 1997, VISUAL COMPUT, V13, P342, DOI 10.1007/s003710050108
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Lorenz C, 2000, COMPUT VIS IMAGE UND, V77, P175, DOI 10.1006/cviu.1999.0814
   Lotjonen J, 1998, IEEE T MAGN, V34, P2228, DOI 10.1109/20.703860
   Morris DD, 2000, PROC CVPR IEEE, P332, DOI 10.1109/CVPR.2000.855837
   NING P, 1993, IEEE COMPUT GRAPH, V13, P33, DOI 10.1109/38.252552
   Oblonsek C, 1998, COMPUT VIS IMAGE UND, V69, P185, DOI 10.1006/cviu.1997.0584
   PAPPAS T, 1989, MOEBIUS STRIP KLEIN
   SCHROEDER WJ, 1992, COMP GRAPH, V26, P65, DOI 10.1145/142920.134010
   STOKELY EM, 1992, IEEE T PATTERN ANAL, V14, P833, DOI 10.1109/34.149594
   TARBOX GH, 1995, COMPUT VIS IMAGE UND, V61, P84, DOI 10.1006/cviu.1995.1007
   TURK G, 1992, COMP GRAPH, V26, P55, DOI 10.1145/142920.134008
   WAGNER M, 1997, NEUROIMAGE, V5, pS389
   WAGNER M, 1998, THESIS TU HAMBURG HA
   Wood ZJ, 2000, IEEE VISUAL, P275, DOI 10.1109/VISUAL.2000.885705
   YUAN JS, 1993, IEEE T MAGN, V29, P1906, DOI 10.1109/20.250780
NR 35
TC 10
Z9 13
U1 1
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2004
VL 20
IS 1
BP 17
EP 36
DI 10.1007/s00371-003-0223-1
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 809SA
UT WOS:000220655200002
DA 2024-07-18
ER

PT J
AU Lodha, SK
   Roskin, KM
   Renteria, JC
AF Lodha, SK
   Roskin, KM
   Renteria, JC
TI Hierarchical topology-preserving simplification of terrains
SO VISUAL COMPUTER
LA English
DT Article
DE simplification; topology; terrains; error metrics; hierarchical
   clustering
AB We present an algorithm for simplifying terrain data that preserves topology. We use a decimation algorithm that simplifies the given data set using hierarchical clustering. Topology constraints, along with local error metrics, are used to ensure topology-preserving simplification and to compute precise error bounds in the simplified data. The earth's mover distance is used as a global metric to compute the degradation in topology as the simplification proceeds. Experiments with both analytic and real terrain data are presented. Results indicate that one can obtain significant simplification with low errors without losing topology information.
C1 Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95064 USA.
C3 University of California System; University of California Santa Cruz
RP Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95064 USA.
EM lodha@cse.ucsc.edu; krish@cse.ucsc.edu; renteria@cse.ucsc.edu
CR Bajaj CL, 1998, COMPUT GRAPH-UK, V22, P3, DOI 10.1016/S0097-8493(97)00079-4
   BAJAJ CL, 1999, P VIS 98, P51
   De Leeuw W., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P349, DOI 10.1109/VISUAL.1999.809907
   FOWLER RJ, 1979, P SIGGRAPH 79, P199
   Gerstner T, 2000, ACCURACY 2000, PROCEEDINGS, P245
   Gerstner T, 2000, IEEE VISUAL, P259, DOI 10.1109/VISUAL.2000.885703
   GERSTNER T, 1999, LECT NOTES EARTH SCI, V78, P75, DOI DOI 10.1007/BFB0009720
   HECKBERT PS, 1997, SURVEY SURFACE APPRO
   HELMAN JL, 1991, IEEE COMPUT GRAPH, V11, P36, DOI 10.1109/38.79452
   INTERRANTE V, 1995, IEEE VISUALIZATION, P52
   Lavin YM, 1998, VISUALIZATION '98, PROCEEDINGS, P103, DOI 10.1109/VISUAL.1998.745291
   Lodha SK, 2000, IEEE VISUAL, P343, DOI 10.1109/VISUAL.2000.885714
   MCCORMACK JE, 1993, INT J GEOGR INF SYST, V7, P263, DOI 10.1080/02693799308901956
   PUPPO E, 1997, EUR 97 TUT NOT EUR A
   RUBNER Y, 1998, STANCSTN9886
   Telea A., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P35, DOI 10.1109/VISUAL.1999.809865
   Tricoche X, 2000, IEEE VISUAL, P359, DOI 10.1109/VISUAL.2000.885716
   WILHELMS J, 1994, ACM SIGGR S VOL VIS, P27
NR 18
TC 2
Z9 2
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2003
VL 19
IS 7-8
BP 493
EP 504
DI 10.1007/s00371-003-0214-2
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 749YL
UT WOS:000186957600006
DA 2024-07-18
ER

PT J
AU Alexa, M
AF Alexa, M
TI Differential coordinates for local mesh morphing and deformation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3rd International Conference on Shape Modeling and Applications (SMI
   2001)
CY MAY 07-11, 2001
CL GENOA, ITALY
SP Consiglio Nazl Ricerche
DE meshes; morphing; local frames; free form deformation
ID SHAPE; METAMORPHOSIS
AB Mesh vertices are usually represented with absolute coordinates. In some applications, this leads to problems for local operations because of global misalignment. We investigate the idea of describing mesh geometry in a differential way. These differential coordinates describe local properties of the geometry rather than the absolute position in space. The main application discussed here is the insertion of shape features from one mesh into another, given the meshes have the same connectivity. We regard this as local control overmesh morphing. Differential coordinates also prove useful for free-form modeling of meshes.
C1 Tech Univ Darmstadt, GRIS, D-64283 Darmstadt, Germany.
C3 Technical University of Darmstadt
RP Tech Univ Darmstadt, GRIS, Rundetrumstr 6, D-64283 Darmstadt, Germany.
EM alexa@gris.informatik.tu-darmstadt.de
OI Alexa, Marc/0000-0002-9854-8466
CR Alexa M, 2002, COMPUT GRAPH FORUM, V21, P173, DOI 10.1111/1467-8659.00575
   Alexa M, 2000, COMP GRAPH, P157, DOI 10.1145/344779.344859
   Alexa M, 2000, VISUAL COMPUT, V16, P26, DOI 10.1007/PL00007211
   [Anonymous], 1989, J HOPKINS SERIES MAT
   Bao HJ, 1998, COMPUT GRAPH FORUM, V17, pC23, DOI 10.1111/1467-8659.00250
   Carmel E, 1997, VISUAL COMPUT, V13, P465, DOI 10.1007/s003710050118
   Cohen-Or D, 1998, ACM T GRAPHIC, V17, P116, DOI 10.1145/274363.274366
   DeCarlo D, 1996, PROC GRAPH INTERF, P194
   Eck M, 1995, P 22 ANN C COMP GRAP, P173, DOI DOI 10.1145/218380.218440
   Floater MS, 1997, COMPUT AIDED GEOM D, V14, P231, DOI 10.1016/S0167-8396(96)00031-3
   Floater MS, 1999, J COMPUT APPL MATH, V101, P117, DOI 10.1016/S0377-0427(98)00202-7
   Gregory A, 1998, COMP ANIM CONF PROC, P64, DOI 10.1109/CA.1998.681909
   Guskov I, 1999, COMP GRAPH, P325, DOI 10.1145/311535.311577
   Kanai T, 1998, VISUAL COMPUT, V14, P166, DOI 10.1007/s003710050132
   Kanai T, 1999, PROC GRAPH INTERF, P148
   KANAI T, IEEE COMPUT GRAPH AP, V20, P62
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   KENT JR, 1992, COMP GRAPH, V26, P47, DOI 10.1145/142920.134007
   KOBBELT L, 1998, P SIGGRAPH 98, P105, DOI DOI 10.1145/280814.280831
   Lazarus F, 1997, J VISUAL COMP ANIMAT, V8, P131, DOI 10.1002/(SICI)1099-1778(199707)8:3<131::AID-VIS156>3.0.CO;2-K
   Lee A. W., 1998, Proceedings of the 25th annual conference on Computer graphics and interactive techniques, P95, DOI DOI 10.1145/280814.280828
   Lee AWF, 1999, COMP GRAPH, P343, DOI 10.1145/311535.311586
   LEE SY, 1995, P SIGGRAPH 95, P439
   Lounsbery M, 1997, ACM T GRAPHIC, V16, P34, DOI 10.1145/237748.237750
   Praun E, 2001, COMP GRAPH, P179, DOI 10.1145/383259.383277
   Sederberg T.W., 1993, em Proceedings of the 20th Annual Conference on Computer Graphics and Interactive Techniques, P15, DOI DOI 10.1145/166117.166118.ENDERE
   SHAPIRA M, 1995, IEEE COMPUT GRAPH, V15, P44, DOI 10.1109/38.365005
   Shapiro A, 1998, VISUAL COMPUT, V14, P429, DOI 10.1007/s003710050153
   Sun YM, 1997, J VISUAL COMP ANIMAT, V8, P81, DOI 10.1002/(SICI)1099-1778(199703)8:2<81::AID-VIS163>3.0.CO;2-W
   Taubin G., 1995, P 22 ANN C COMP GRAP, P351, DOI DOI 10.1145/218380.218473
   Tutte W.T., 1963, Proc. London Math. Soc., V13, P743, DOI DOI 10.1112/PLMS/S3-13.1.743
   Zöckler M, 2000, VISUAL COMPUT, V16, P241, DOI 10.1007/PL00013396
   ZORIN D, 1997, P SIGGRAPH 97, P259
NR 33
TC 158
Z9 192
U1 1
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2003
VL 19
IS 2-3
BP 105
EP 114
DI 10.1007/s00371-002-0180-0
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 691AU
UT WOS:000183583300004
DA 2024-07-18
ER

PT J
AU Ma, QX
   Wang, LL
   Ke, W
   Im, SK
AF Ma, Qixiang
   Wang, Lili
   Ke, Wei
   Im, Sio-Kei
TI SMigraPH: a perceptually retained method for passive haptics-based
   migration of MR indoor scenes
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Mixed reality; Scene migration; Scene generation; Passive haptics;
   Optimization
AB To enhance users' immersion in the mixed reality (MR) cross-scene environment, it is imperative to make geometric modifications to arbitrary multi-scale virtual scenes, including adjustments to layout and size, based on the appearance of diverse real-world spaces. Numerous studies have been conducted on the layout arrangement of pure virtual scenes; however, they often neglect the issue of incongruity between virtual and real environments. Our objective is to mitigate the incongruity between virtual and real scenes in MR, establish a rational layout and size for any virtual scene within an enclosed indoor environment, and leverage tangible real objects to achieve multi-class passive haptic feedback. To achieve these goals, we propose SMigraPH, a perceptually retained indoor scene migration method with passive haptics in MR. Firstly, we propose a scene abstraction technique for constructing mathematical representations of both virtual and real scenes, capturing geometric information and topological relationships, while providing a mapping strategy from the virtual to the real domain. Subsequently, we develop an optimization framework called v2rSA that integrates rationality, relationship preservation, haptic reuse, and scale fitting constraints in order to iteratively generate final layouts for virtual scenes. Finally, we render scenarios on optical see-through MR head-mounted displays (HMDs) to enable users to engage in realistic scene exploration and interaction with haptic feedback. We have conducted experiments and a user study on our proposed method, which demonstrates significant improvements in surface registration accuracy, haptic interaction efficiency, and fidelity compared to the state-of-the-art indoor scene layout arrangement method MakeItHome as well as the random placement approach RandomIn. The results of our approach closely resemble those achieved through manual placement using the Human method.
C1 [Ma, Qixiang; Wang, Lili] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100083, Peoples R China.
   [Ke, Wei; Im, Sio-Kei] Macao Polytech Univ, Fac Appl Sci, Macau 999078, Peoples R China.
C3 Beihang University; Macao Polytechnic University
RP Wang, LL (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100083, Peoples R China.
EM sycamore_ma@buaa.edu.cn; wanglily@buaa.edu.cn; wke@mpu.edu.mo;
   marcusim@mpu.edu.mo
RI Ke, Wei/JXM-8153-2024
OI IM, SIO KEI/0000-0002-5599-4300; Ke, Wei/0000-0003-0952-0961
FU the National Natural Science Foundation of China [61932003]; National
   Natural Science Foundation of China [Z221100007722004]; Beijing Science
   and Technology Plan Project [2019YFC1521102]; National Key R D plan
FX We sincerely thank the reviewers for their constructive suggestions and
   comments. This work is supported by the National Natural Science
   Foundation of China through Project 61932003, by Beijing Science and
   Technology Plan Project Z221100007722004, and by National Key R &D plan
   2019YFC1521102.
CR Ali W., 2018, P EUR C COMP VIS ECC, P1, DOI DOI 10.1007/978-3-030-11015-454
   Azmandian M, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P1968, DOI 10.1145/2858036.2858226
   Bermejo C, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3465396
   Butt MA, 1998, IEEE T IMAGE PROCESS, V7, P1477, DOI 10.1109/83.718487
   CHIB S, 1995, AM STAT, V49, P327, DOI 10.2307/2684568
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Dong K., 2022, Vis. Comput., P1
   Dong ZC, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3470847
   Du K.L., 2016, Search and Optimization by Metaheuristics: Techniques and Algorithms Inspired by Nature, P29, DOI [DOI 10.1007/978-3-319-41192-7_2, 10.1007/978-3-319-41192-7_2]
   Fisher M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366154
   Geyer C.J., 1992, Stat. Sci, V7, P473, DOI DOI 10.1214/SS/1177011137
   Gwak J. Y., 2020, P COMP VIS ECCV 20 4, P297, DOI DOI 10.48550/ARXIV.2006.12356
   Insko B.E., 2001, Passive Haptics Significantly Enhances Virtual Environ- ments
   Jang S, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P3706, DOI 10.1145/2858036.285826
   Jin S, 2019, COMPUT GRAPH FORUM, V38, P733, DOI 10.1111/cgf.13875
   Kermani ZS, 2016, COMPUT GRAPH FORUM, V35, P197, DOI 10.1111/cgf.12976
   Lari Z, 2011, INT ARCH PHOTOGRAMM, V38-5, P103
   Lee W, 2005, INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P106
   Li MY, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3303766
   Liu J., 2022, Vis. Comput., P1
   Liu Z, 2021, Arxiv, DOI arXiv:2104.00678
   Matthews BJ, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P19, DOI [10.1109/vr.2019.8797974, 10.1109/VR.2019.8797974]
   Merrell P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964982
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Pearson K, 1905, NATURE, V72, P294, DOI 10.1038/072294b0
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Qi SY, 2018, PROC CVPR IEEE, P5899, DOI 10.1109/CVPR.2018.00618
   Rinne H., 2008, WEIBULL DISTRIBUTION, DOI [10.1201/9781420087444, DOI 10.1201/9781420087444]
   Rusu R. B., 2011, 2011 IEEE INT C ROBO, P1
   Salazar SV, 2020, IEEE T HAPTICS, V13, P167, DOI 10.1109/TOH.2020.2967389
   Schnabel R, 2007, COMPUT GRAPH FORUM, V26, P214, DOI 10.1111/j.1467-8659.2007.01016.x
   Song YN, 2023, VISUAL COMPUT, V39, P1109, DOI 10.1007/s00371-021-02391-0
   Spelmezan D, 2016, IEEE HAPTICS SYM, P98, DOI 10.1109/HAPTICS.2016.7463162
   Sun YL, 2020, VISUAL COMPUT, V36, P2407, DOI 10.1007/s00371-020-01892-8
   Talton JO, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944851
   Tang KK, 2023, VISUAL COMPUT, V39, P5577, DOI 10.1007/s00371-022-02682-0
   Ungureanu D, 2020, Arxiv, DOI [arXiv:2008.11239, DOI 10.48550/ARXIV.2008.11239]
   Wang K, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322941
   Wang K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201362
   Wang LL, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P651, DOI [10.1109/VRW50115.2020.00-99, 10.1109/VRW50115.2020.00176]
   Xu K, 2002, PROC GRAPH INTERF, P25
   Yeh YT, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185552
   Yu LF, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964981
   Zenner A, 2017, IEEE T VIS COMPUT GR, V23, P1312, DOI 10.1109/TVCG.2017.2656978
   Zhang SH, 2019, J COMPUT SCI TECH-CH, V34, P594, DOI 10.1007/s11390-019-1929-5
   Zhou B., 2014, Adv Neural Inf Proces Syst27
NR 47
TC 0
Z9 0
U1 6
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 30
PY 2023
DI 10.1007/s00371-023-03220-2
EA DEC 2023
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DM6X5
UT WOS:001132511100002
DA 2024-07-18
ER

PT J
AU Akhunov, R
   Kolb, A
AF Akhunov, Rustam
   Kolb, Andreas
TI Decoupled Boundary Handling in SPH
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Smoother Particles Hydrodynamics; Boundary Handling; Decoupled SPH; SPH;
   Decoupling
AB Particle-based boundary representations are frequently used in smoothed particle hydrodynamics (SPH) due to their simple integration into fluid solvers. Commonly, incompressible fluid solvers estimate the current density and corresponding forces in case the current density exceeds the rest density to push fluid particles apart. Close to the boundary, the calculation of the fluid particles' density involves both neighboring fluid and neighboring boundary particles, yielding an overestimation of density, and, subsequently, wrong pressure forces and wrong velocities leading to the disturbed fluid particles' behavior in the vicinity of the boundary. In this paper, we present a detailed explanation of this disturbed fluid particle behavior, which is mainly due to the combined or coupled handling of the fluid-fluid particle and the fluid-boundary particle interaction. We propose the decoupled handling of both interaction types, leading to two densities for a given fluid particle, i.e., fluid-induced density and boundary-induced density. In our approach, we alternately apply the corresponding fluid-induced and boundary-induced forces during pressure estimation. This separation avoids force overestimation and reduces unintended fluid dynamics near the boundary, as well as a inconsistent fluid-boundary distance across different fluid amounts and different particle-based boundary handling methods. We compare our method with two regular state-of-the-art methods in different experiments and show how our method handles detailed boundary shapes.
C1 [Akhunov, Rustam; Kolb, Andreas] Univ Siegen, D-57076 Siegen, Germany.
C3 Universitat Siegen
RP Akhunov, R (corresponding author), Univ Siegen, D-57076 Siegen, Germany.
EM rustam.akhunov@uni-siegen.de; andreas.kolb@uni-siegen.de
RI Kolb, Andreas/A-2067-2012
FU Deutsche Forschungsgemeinschaft
FX No Statement Available
CR Akhunov R, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2138
   Akinci N, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508395
   Akinci N, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185558
   Band S, 2018, COMPUT GRAPH-UK, V76, P37, DOI 10.1016/j.cag.2018.08.001
   Band S, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3180486
   Becker M, 2009, IEEE T VIS COMPUT GR, V15, P493, DOI 10.1109/TVCG.2008.107
   Bell N., 2005, P 2005 ACM SIGGRAPH, P77, DOI DOI 10.1145/1073368.1073379
   Bender J., 2019, P 12 ACM SIGGRAPH C, P1
   Bodin K, 2012, IEEE T VIS COMPUT GR, V18, P516, DOI 10.1109/TVCG.2011.29
   Gissler C, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3284980
   Harada T., 2007, P 23 SPRING C COMPUT, P191
   He J, 2010, VISUAL COMPUT, V26, P243, DOI 10.1007/s00371-010-0426-1
   Huber M, 2015, COMPUT GRAPH FORUM, V34, P14, DOI 10.1111/cgf.12455
   Ihmsen M, 2014, IEEE T VIS COMPUT GR, V20, P426, DOI 10.1109/TVCG.2013.105
   Koschier D, 2020, Arxiv, DOI arXiv:2009.06944
   Koschier D, 2022, COMPUT GRAPH FORUM, V41, P737, DOI 10.1111/cgf.14508
   Koschier D, 2017, ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION (SCA 2017), DOI 10.1145/3099564.3099565
   Losasso F, 2008, IEEE T VIS COMPUT GR, V14, P797, DOI 10.1109/TVCG.2008.37
   Monaghan JJ, 2005, REP PROG PHYS, V68, P1703, DOI 10.1088/0034-4885/68/8/R01
   MONAGHAN JJ, 1994, J COMPUT PHYS, V110, P399, DOI 10.1006/jcph.1994.1034
   Muller M., 2004, P 2004 ACM SIGGRAPHE, P141, DOI [DOI 10.1145/1028523.1028542, 10.1145/1028523.1028542, 10]
   Winchenbach R., 2018, THESIS U SIEGEN
   Winchenbach R, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417829
NR 23
TC 0
Z9 0
U1 3
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 22
PY 2023
DI 10.1007/s00371-023-03212-2
EA DEC 2023
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DC1N7
UT WOS:001129743300003
OA Green Submitted, hybrid
DA 2024-07-18
ER

PT J
AU Huang, XQ
   Huang, DT
   Huang, Q
   Huang, CX
   Chen, FY
   Xu, ZJ
AF Huang, Xiaoqian
   Huang, Detian
   Huang, Qin
   Huang, Caixia
   Chen, Feiyang
   Xu, Zhengjun
TI Dtsr: detail-enhanced transformer for image super-resolution
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Super-resolution; Global contextual feature; Cross locally-enhanced
   self-attention; Channel-wise-spatial locally-enhanced self-attention;
   Bidirectional fusion
AB Recently, Transformer has achieved remarkable success in image super-resolution (SR). However, most existing methods adopt the window-based self-attention to balance performance and computational overhead, which prevents Transformer from capturing global information and modeling long-range dependencies at the early layers, resulting in missing certain crucial high-frequency components. To address the above issue, we propose a novel detail-enhanced Transformer for image super-resolution (DTSR) that exploits gradient information to learn high-frequency components and perceives more abundant global contextual features. Specifically, the proposed DTSR consists of a super-resolution branch and a gradient branch, with the former used to capture texture information and the latter used to gather edge and structure information to compensate for the missing high-frequency components in the former. Furthermore, to aggregate the generated features from both branches, we propose a channel-wise-spatial locally-enhanced self-attention layer to model local and global dependencies, and then further incorporate it to construct a cross locally-enhanced self-attention block to extract local features within a patch as well as global contextual features between patches in a single-channel feature map. Finally, the bidirectional fusion strategy enables the fused features to complement each branch to further promote the feature representation. Extensive experiments validate that our DTSR significantly outperforms state-of-the-art methods in terms of both visual quality and quantitative metrics.
C1 [Huang, Xiaoqian] Huaqiao Univ, Coll Informat Sci & Engn, Xiamen 361021, Peoples R China.
   [Huang, Detian; Chen, Feiyang; Xu, Zhengjun] Huaqiao Univ, Coll Engn, Quanzhou 362021, Peoples R China.
   [Huang, Qin] Gaozhou Peoples Hosp, Changpo Branch, Maoming 525200, Peoples R China.
   [Huang, Caixia] Gaozhou Maternal & Child Hlth Care Hosp, Maoming 525200, Peoples R China.
C3 Huaqiao University; Huaqiao University
RP Huang, DT (corresponding author), Huaqiao Univ, Coll Engn, Quanzhou 362021, Peoples R China.
EM huangdetian@hqu.edu.cn
OI Detian, Huang/0000-0002-8542-3728
FU National Key R&D Program of China [2021YFE0205400]; National Natural
   Science Foundation of China [61901183, 61976098]; Natural Science
   Foundation of Fujian Province [2023J01140]; Key Project of Quanzhou
   Science and Technology Plan [2023C007R]; Fundamental Research Funds for
   the Central Universities [ZQN-921]; Collaborative Innovation Platform
   Project of Fujian Province [2021FX03]; Key Science and Technology
   Project of Xiamen City [3502Z20231005]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2021YFE0205400, in part by the National Natural Science
   Foundation of China under Grant 61901183 and Grant 61976098, in part by
   the Natural Science Foundation of Fujian Province under Grant
   2023J01140, in part by the Key Project of Quanzhou Science and
   Technology Plan under Grant 2023C007R, in part by the Fundamental
   Research Funds for the Central Universities under Grant ZQN-921, in part
   by the Collaborative Innovation Platform Project of Fujian Province
   under Grant 2021FX03, and in part by the Key Science and Technology
   Project of Xiamen City under Grant 3502Z20231005.
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Anoosheh A, 2019, IEEE INT CONF ROBOT, P5958, DOI [10.1109/ICRA.2019.8794387, 10.1109/icra.2019.8794387]
   Barnsley MF., 1998, Fractals Everywhere, P1
   Ben Niu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P191, DOI 10.1007/978-3-030-58610-2_12
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Cao JZ, 2023, Arxiv, DOI [arXiv:2106.06847, DOI 10.48550/ARXIV.2106.06847]
   Chen B., 2011, P 28 ANN M CHINESE M, P1
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Fattal R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239546, 10.1145/1276377.1276496]
   Fritsche M, 2019, IEEE INT CONF COMP V, P3599, DOI 10.1109/ICCVW.2019.00445
   Gao ZJ, 2022, IEEE ACCESS, V10, P17760, DOI 10.1109/ACCESS.2022.3147493
   Gu JJ, 2021, PROC CVPR IEEE, P9195, DOI 10.1109/CVPR46437.2021.00908
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   Gu SH, 2012, INT C PATT RECOG, P3128
   Guo Y, 2020, Arxiv, DOI arXiv:2003.07018
   Han K, 2023, IEEE T PATTERN ANAL, V45, P87, DOI 10.1109/TPAMI.2022.3152247
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Isobe Takashi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P645, DOI 10.1007/978-3-030-58610-2_38
   Ji XZ, 2020, IEEE COMPUT SOC CONF, P1914, DOI 10.1109/CVPRW50498.2020.00241
   Jia XX, 2019, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2019.00621
   Jiang K, 2019, IEEE T GEOSCI REMOTE, V57, P5799, DOI 10.1109/TGRS.2019.2902431
   Kim J., 2016, P IEEE C COMP VIS PA, P1
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Lai W.S., 2017, P IEEECVF C COMPUTER
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li KC, 2022, Arxiv, DOI arXiv:2201.09450
   Li WB, 2022, Arxiv, DOI arXiv:2112.10175
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Liang JY, 2022, Arxiv, DOI arXiv:2201.12288
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lin Hezheng, 2021, arXiv
   Lin Z., 2022, arXiv
   Liu Z, 2021, Arxiv, DOI [arXiv:2103.14030, DOI 10.48550/ARXIV.2103.14030]
   Lundberg SM, 2017, ADV NEUR IN, V30
   Ma C, 2020, Arxiv, DOI arXiv:2003.13081
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Mei YQ, 2020, Arxiv, DOI arXiv:2004.13824
   Mei YQ, 2021, PROC CVPR IEEE, P3516, DOI 10.1109/CVPR46437.2021.00352
   Mei YQ, 2020, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR42600.2020.00573
   Paszke A, 2019, ADV NEUR IN, V32
   Shi WZ, 2013, LECT NOTES COMPUT SC, V8151, P9, DOI 10.1007/978-3-642-40760-4_2
   Shrikumar A, 2017, PR MACH LEARN RES, V70
   Simonyan K, 2014, Arxiv, DOI arXiv:1312.6034
   Sun J, 2011, IEEE T IMAGE PROCESS, V20, P1529, DOI 10.1109/TIP.2010.2095871
   Sundararajan M, 2017, PR MACH LEARN RES, V70
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Springenberg JT, 2015, Arxiv, DOI [arXiv:1412.6806, DOI 10.48550/ARXIV.1412.6806]
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang X., 2018, EUR C COMP VIS WORKS, V2018, P701
   Wang ZD, 2021, Arxiv, DOI arXiv:2106.03106
   Xiao T, 2021, ADV NEUR IN, V34
   Yan Q, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2414877
   Yang WH, 2017, IEEE T IMAGE PROCESS, V26, P5895, DOI 10.1109/TIP.2017.2750403
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang K, 2022, IEEE T PATTERN ANAL, V44, P6360, DOI 10.1109/TPAMI.2021.3088914
   Zhang K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4771, DOI 10.1109/ICCV48922.2021.00475
   Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang X., 2022, arXiv
   Zhang Yang, 2019, INT C LEARN REPR
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhou S., 2020, P ADV NEUR INF PROC, V33, P3499
   Zhu Y, 2015, PROC CVPR IEEE, P5417, DOI 10.1109/CVPR.2015.7299180
   Zou WWW, 2012, IEEE T IMAGE PROCESS, V21, P327, DOI 10.1109/TIP.2011.2162423
NR 78
TC 0
Z9 0
U1 13
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 20
PY 2023
DI 10.1007/s00371-023-03200-6
EA DEC 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CT0D3
UT WOS:001127364400001
DA 2024-07-18
ER

PT J
AU Ma, Y
   Zhu, HQ
   Hou, T
   Chen, N
   Huang, H
AF Ma, Yuan
   Zhu, Hongqing
   Hou, Tong
   Chen, Ning
   Huang, Hui
TI DCLR-SF: distribution consistent label refinement and lighten similarity
   network fusion for multi-source domain-adaptive person re-identification
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Person re-identification; Multi-source domain adaptation; Pseudo-label
   refinement; Density peaks clustering; Similarity network fusion
ID UNCERTAINTY
AB Unsupervised domain-adaptive person re-identification is challenging and has received much attention in recent years. Some previous works generate pseudo-labels by clustering algorithms to further optimize the model. But pseudo-labels inevitably contain noise due to the domain shift and the imperfection of the clustering methods. This article attempts to solve this problem by proposing two solutions: cross-supervision and distribution consistent label refinement (DCLR) schemes. For the former, we use labeled multi-source domain data to enhance the complementarity of different branches in the network and then exchange pseudo-labels for suppressing the negative effects of noise. For the latter, the proposed DCLR enlarges the pairwise distance of different identity features in a coarse cluster and then uses density peaks clustering technology to refine. This directly improves the quality of pseudo-labels. In addition, we design an adaptation learning-based memory bank to improve the consistency of attributes across domains and reduce the domain shift. This study also proposes the lighten similarity network fusion for person re-identification tasks to fuse the query results of multiple branches in the network. Extensive experiments demonstrate the superiority of our method on multiple person re-identification datasets.
C1 [Ma, Yuan; Zhu, Hongqing; Hou, Tong; Chen, Ning] East China Univ Sci & Technol, Sch Informat Sci & Engn, Shanghai 200237, Peoples R China.
   [Huang, Hui] Shanghai Normal Univ, Coll Informat & Elect Engn, Shanghai 201418, Peoples R China.
C3 East China University of Science & Technology; Shanghai Normal
   University
RP Zhu, HQ (corresponding author), East China Univ Sci & Technol, Sch Informat Sci & Engn, Shanghai 200237, Peoples R China.
EM hqzhu@ecust.edu.cn
OI Zhu, Hongqing/0000-0002-2122-7066
FU National Nature Science Foundation of China;  [61872143];  [61771196]
FX The authors would like to thank the anonymous reviewers and the
   associate editor for their insightful comments that significantly
   improved the quality of this paper. This work was supported by the
   National Nature Science Foundation of China under Grant 61872143,
   61771196.
CR Bai ZC, 2021, PROC CVPR IEEE, P12909, DOI 10.1109/CVPR46437.2021.01272
   Bao LQ, 2019, IEEE COMPUT SOC CONF, P1496, DOI 10.1109/CVPRW.2019.00191
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Cheng D, 2022, IEEE T IMAGE PROCESS, V31, P3334, DOI 10.1109/TIP.2022.3169693
   Dai YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11844, DOI 10.1109/ICCV48922.2021.01165
   Dai YX, 2021, IEEE T IMAGE PROCESS, V30, P7815, DOI 10.1109/TIP.2021.3104169
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   Feng H, 2021, IEEE T IMAGE PROCESS, V30, P2898, DOI 10.1109/TIP.2021.3056212
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ge Y., 2020, P NIPS, V33, P11309
   Ge YX, 2020, Arxiv, DOI [arXiv:2001.01526, 10.48550/arXiv.2001.01526]
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Han J, 2022, AAAI CONF ARTIF INTE, P790
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He T, 2022, AAAI CONF ARTIF INTE, P879
   Jia ZQ, 2023, VISUAL COMPUT, V39, P1205, DOI 10.1007/s00371-022-02398-1
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Layne R, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.24
   Lei JJ, 2023, IEEE T IND INFORM, V19, P7377, DOI 10.1109/TII.2022.3210589
   Li ZY, 2022, AAAI CONF ARTIF INTE, P1527
   Lin S, 2018, Arxiv, DOI arXiv:1807.01440
   Liu JW, 2019, PROC CVPR IEEE, P7195, DOI 10.1109/CVPR.2019.00737
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Meng Z, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5949, DOI 10.1109/ICASSP.2018.8461682
   Qin DF, 2011, PROC CVPR IEEE, P777, DOI 10.1109/CVPR.2011.5995373
   Rodriguez A, 2014, SCIENCE, V344, P1492, DOI 10.1126/science.1242072
   Shen YT, 2018, LECT NOTES COMPUT SC, V11219, P508, DOI 10.1007/978-3-030-01267-0_30
   Sun J, 2021, IEEE T IMAGE PROCESS, V30, P2935, DOI 10.1109/TIP.2021.3056889
   Tang YZ, 2020, IEEE T IMAGE PROCESS, V29, P5641, DOI 10.1109/TIP.2020.2985545
   Tarvainen A, 2017, ADV NEUR IN, V30
   Wang B, 2014, NAT METHODS, V11, P333, DOI [10.1038/nmeth.2810, 10.1038/NMETH.2810]
   Wang GA, 2020, PROC CVPR IEEE, P6448, DOI 10.1109/CVPR42600.2020.00648
   Wang H, 2020, IEEE ACCESS, V8, P83265, DOI 10.1109/ACCESS.2020.2991440
   Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242
   Wang WH, 2022, IEEE T IMAGE PROCESS, V31, P1532, DOI 10.1109/TIP.2022.3140614
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wei ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P225, DOI 10.1109/ICCV48922.2021.00029
   Wei ZY, 2022, IEEE T NEUR NET LEAR, V33, P4676, DOI 10.1109/TNNLS.2021.3059713
   Wu S, 2019, PROC CVPR IEEE, P10083, DOI [10.1109/CVPR.2019.00666, 10.1109/CVPR.2019.01033]
   Wu YM, 2020, IEEE T IMAGE PROCESS, V29, P8821, DOI 10.1109/TIP.2020.3001693
   Yan YC, 2019, PROC CVPR IEEE, P2153, DOI 10.1109/CVPR.2019.00226
   Yang Zou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P87, DOI 10.1007/978-3-030-58536-5_6
   Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904
   Zhai Y, 2020, COMPUTER VISION ECCV, P594, DOI DOI 10.1007/978-3-030-58571-6_35
   Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454
   Zheng DY, 2022, PATTERN RECOGN, V132, DOI 10.1016/j.patcog.2022.108941
   Zheng KC, 2021, PROC CVPR IEEE, P5306, DOI 10.1109/CVPR46437.2021.00527
   Zheng KC, 2021, AAAI CONF ARTIF INTE, V35, P3538
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhou H, 2023, INT J MACH LEARN CYB, V14, P1951, DOI 10.1007/s13042-022-01739-9
NR 54
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 16
PY 2023
DI 10.1007/s00371-023-03208-y
EA DEC 2023
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CZ9U8
UT WOS:001129178100001
DA 2024-07-18
ER

PT J
AU Chang, WC
   Pham, V
AF Chang, Wen-Chung
   Pham, Van-Toan
TI PFRNet: 3-D partial-to-full point cloud registration network for
   arbitrary pose matching
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Convolutional neural networks; Deep learning; Partial point cloud;
   Point-wise correspondence; 3-D registration
ID ROBUST; ALGORITHM; CONSENSUS; RANSAC
AB 3-D point cloud registration algorithms have been commonly studied and effectively applied to object pose estimation. Due to limited field of view of a 3-D camera, only partial point cloud of the observed object can be grabbed at each frame. Thus, the registration problem between partial and full point clouds remains challenging due to missing data and arbitrary pose matching. This paper proposes a seemingly novel partial-to-full registration network (PFRNet) based on establishing point-wise correspondences with a full range of uncertainty. Specifically, an effective descriptor is developed to generate distance histograms capturing systematically geometric information for each point to release difficulty in training when considering a full range of uncertainty. Then, a compensation network is proposed to adjust the histogram descriptor extracted from the partial point cloud by learning differences caused by missing data. Next, these two descriptors are input to a shared local feature extractor to generate per-point learned features. Besides, in order to establish corresponding point pairs, a deep network is applied to estimate the outlier and annealing parameters. Finally, the proposed architecture adopts a differentiable singular value decomposition module to output the rigid transformation. Experimental results show that our PFRNet achieves high precision, outperforming baseline methods while maintaining fast estimation on both synthetic ModelNet40 and realistic S3DIS data sets.
C1 [Chang, Wen-Chung; Pham, Van-Toan] Natl Taipei Univ Technol, Dept Elect Engn, Taipei Tech Box 2125, Taipei 10608, Taiwan.
C3 National Taipei University of Technology
RP Chang, WC (corresponding author), Natl Taipei Univ Technol, Dept Elect Engn, Taipei Tech Box 2125, Taipei 10608, Taiwan.
EM wchang@ntut.edu.tw
RI Chang, Wen-Chung/I-4025-2013
OI Chang, Wen-Chung/0000-0002-2472-1772
FU Ministry of Science and Technology, Taiwan
FX No Statement Available
CR Aiger D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360684
   [Anonymous], Fast global registration
   [Anonymous], Global registration
   Aoki Y, 2019, PROC CVPR IEEE, P7156, DOI 10.1109/CVPR.2019.00733
   Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Ben-Shabat Yizhak, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P20, DOI 10.1007/978-3-030-58452-8_2
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bouaziz S, 2013, COMPUT GRAPH FORUM, V32, P113, DOI 10.1111/cgf.12178
   Chang WC., 2019, Vis. Comput, V36, P1
   Chang WC, 2021, 2021 THE 9TH INTERNATIONAL CONFERENCE ON CONTROL, MECHATRONICS AND AUTOMATION (ICCMA 2021), P218, DOI 10.1109/ICCMA54375.2021.9646215
   Chang WC, 2020, INT CONF UBIQ ROBOT, P124, DOI 10.1109/UR49135.2020.9144767
   Chang WC, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9163273
   Chang WC, 2018, ICCMA 2018: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON CONTROL, MECHATRONICS AND AUTOMATION, P159, DOI 10.1145/3284516.3284525
   Chen J, 2013, OPT LASER TECHNOL, V45, P414, DOI 10.1016/j.optlastec.2012.06.015
   Chetverikov D, 2005, IMAGE VISION COMPUT, V23, P299, DOI 10.1016/j.imavis.2004.05.007
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Elbaz G, 2017, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2017.265
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fu KX, 2023, IEEE T PATTERN ANAL, V45, P6183, DOI [10.1109/TPAMI.2022.3204713, 10.1109/CVPR46437.2021.00878]
   GOLUB GH, 1970, NUMER MATH, V14, P403, DOI 10.1007/BF02163027
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   He BW, 2013, OPT LASER TECHNOL, V46, P53, DOI 10.1016/j.optlastec.2012.04.027
   Hong-Seok P, 2014, PROCEDIA ENGINEER, V69, P931, DOI 10.1016/j.proeng.2014.03.072
   Hosoki D, 2019, INT C CONTR AUTOMAT, P1037, DOI [10.23919/iccas47443.2019.8971537, 10.23919/ICCAS47443.2019.8971537]
   Jiang J, 2009, NEUROCOMPUTING, V72, P3839, DOI 10.1016/j.neucom.2009.05.013
   Kamencay P, 2019, 2019 42ND INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS AND SIGNAL PROCESSING (TSP), P517, DOI [10.1109/tsp.2019.8769057, 10.1109/TSP.2019.8769057]
   Kingma D. P., 2014, arXiv
   Kurobe A, 2020, IEEE ROBOT AUTOM LET, V5, P3960, DOI 10.1109/LRA.2020.2970946
   Liu HB, 2019, IEEE ACCESS, V7, P73637, DOI 10.1109/ACCESS.2019.2919989
   Lu WX, 2019, Arxiv, DOI arXiv:1905.04153
   Mavridis P, 2015, COMPUT AIDED GEOM D, V35-36, P16, DOI 10.1016/j.cagd.2015.03.022
   Mellado N, 2014, COMPUT GRAPH FORUM, V33, P205, DOI 10.1111/cgf.12446
   Niedfeldt PC, 2016, IEEE T AUTOMAT CONTR, V61, P456, DOI 10.1109/TAC.2015.2437518
   Nistér D, 2005, MACH VISION APPL, V16, P321, DOI 10.1007/s00138-005-0006-y
   Oztireli AC, 2008, VISUAL COMPUT, V24, P679, DOI 10.1007/s00371-008-0248-6
   Pan L, 2021, arXiv, DOI [DOI 10.48550/ARXIV.2111.15606, 10.48550/ARXIV.2111.15606]
   Paszke A, 2019, ADV NEUR IN, V32
   Phillips JM, 2007, 3DIM 2007: SIXTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P427
   Quan SW, 2020, IEEE T GEOSCI REMOTE, V58, P7380, DOI 10.1109/TGRS.2020.2982221
   Quan SW, 2018, INFORM SCIENCES, V444, P153, DOI 10.1016/j.ins.2018.02.070
   Rusu RB, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3384, DOI 10.1109/IROS.2008.4650967
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Sarode V, 2019, Arxiv, DOI arXiv:1908.07906
   Shi XY, 2019, PROCEDIA COMPUT SCI, V147, P181, DOI 10.1016/j.procs.2019.01.219
   Tang KK, 2017, LECT NOTES COMPUT SC, V10115, P311, DOI 10.1007/978-3-319-54193-8_20
   Torr PHS, 2003, IEEE T PATTERN ANAL, V25, P354, DOI 10.1109/TPAMI.2003.1182098
   Wang C, 2018, IEEE PHOTONICS J, V10, DOI 10.1109/JPHOT.2018.2876689
   Wang HY, 2021, P A I C C AUT ROBOT, P280, DOI 10.1109/ICCAR52225.2021.9463502
   Wang X, 2020, IEEE ACCESS, V8, P40692, DOI 10.1109/ACCESS.2020.2976132
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wang Yue, 2019, Adv. Neural Inf. Process. Syst., P8814
   Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI [10.1109/CSTIC.2018.8369274, 10.1007/s11263-019-01198-w]
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xie ZX, 2010, IMAGE VISION COMPUT, V28, P563, DOI 10.1016/j.imavis.2009.09.006
   Xu J, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19194093
   Xu M, 2012, IET COMPUT VIS, V6, P324, DOI 10.1049/iet-cvi.2010.0223
   Xu YQ, 2022, PATTERN RECOGN, V121, DOI 10.1016/j.patcog.2021.108210
   Yang JL, 2013, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2013.184
   Yang JQ, 2016, INFORM SCIENCES, V346, P163, DOI 10.1016/j.ins.2016.01.095
   Ying SH, 2009, IEEE T AUTOM SCI ENG, V6, P559, DOI 10.1109/TASE.2009.2021337
   Zhang WY, 2019, 2019 3RD INTERNATIONAL SYMPOSIUM ON AUTONOMOUS SYSTEMS (ISAS 2019), P65, DOI [10.1109/ISASS.2019.8757773, 10.1109/isass.2019.8757773]
   Zhou QY, 2016, LECT NOTES COMPUT SC, V9906, P766, DOI 10.1007/978-3-319-46475-6_47
   Zhu JH, 2019, OPT LASER TECHNOL, V110, P202, DOI 10.1016/j.optlastec.2018.07.072
   Zi Jian Yew, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11821, DOI 10.1109/CVPR42600.2020.01184
NR 65
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 14
PY 2023
DI 10.1007/s00371-023-03209-x
EA DEC 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CG2L2
UT WOS:001124032100001
DA 2024-07-18
ER

PT J
AU Liang, XD
   Lin, T
AF Liang, Xudong
   Lin, Tao
TI Sketch2Wireframe: an automatic framework for transforming hand-drawn
   sketches to digital wireframes in UI design
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE User interface; Hand-drawn sketch; Digital wireframe; Object detection;
   Layout optimization
AB Transforming hand-drawn sketches into digital wireframes is a pivotal yet time-intensive step in the early stages of User Interface (UI) design. Despite advancements in technology aiming to streamline this task, challenges arise from the multi-scale variations of UI elements and inherent imperfections in the geometric information of sketches. We introduce Sketch2Wireframe, a tool designed to swiftly transform these sketches into refined digital wireframes. The process unfolds in two primary phases: UI element detection and UI layout optimization. First, we propose an object detection model named Attention-based Multi-Scale Region-based Convolutional Neural Network (AMS-RCNN). This model efficiently detects multi-scale elements in sketches by leveraging multi-scale convolution and an attention mechanism, ensuring rich feature extraction for prediction. We then implement a hybrid layout optimization method, which initially utilizes heuristic rules to recognize hierarchy and element organization from sketches, followed by an Integer Programming (IP) approach to globally optimize the layout in compliance with design guidelines. Experimental findings suggest that the AMS-RCNN outperforms baseline models, evidencing a 2.2% mAP increase in element detection. Moreover, our hybrid layout optimization method considerably outperforms the heuristic methods in terms of the perceived quality of the layouts.
C1 [Liang, Xudong; Lin, Tao] Sichuan Univ, Coll Comp Sci, Chengdu 610065, Sichuan, Peoples R China.
C3 Sichuan University
RP Lin, T (corresponding author), Sichuan Univ, Coll Comp Sci, Chengdu 610065, Sichuan, Peoples R China.
EM lintao@scu.edu.cn
OI Liang, Xudong/0009-0003-2816-5237
CR Abdelhamid AA, 2020, IET SOFTW, V14, P816, DOI 10.1049/iet-sen.2019.0378
   [Anonymous], 2013, computing research repository. abs/1312.4400
   [Anonymous], 2018, Material Design Guidelines
   Bao LF, 2015, 2015 IEEE/ACM 37TH IEEE INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING, VOL 2, P673, DOI 10.1109/ICSE.2015.220
   Bertin J., 1983, SEMIOLOGY GRAPHICS D
   Bunian Sara, 2021, P 2021 CHI C HUM FAC, P1, DOI [DOI 10.1145/3411764.3445762, 10.1145/3411764.3445762]
   Camba JD, 2022, COMPUT AIDED DESIGN, V150, DOI 10.1016/j.cad.2022.103283
   Cao B, 2019, IEEE T NEUR NET LEAR, V30, P1731, DOI 10.1109/TNNLS.2018.2872675
   Chen JS, 2020, PROCEEDINGS OF THE 28TH ACM JOINT MEETING ON EUROPEAN SOFTWARE ENGINEERING CONFERENCE AND SYMPOSIUM ON THE FOUNDATIONS OF SOFTWARE ENGINEERING (ESEC/FSE '20), P1202, DOI 10.1145/3368089.3409691
   Dayama NR, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376553
   Dayama NR, 2021, IUI '21 - 26TH INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P70, DOI 10.1145/3397481.3450652
   Fichou D., 2020, CLEF2020 WORKING NOT
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2020, IEEE T PATTERN ANAL, V42, P2011, DOI 10.1109/TPAMI.2019.2913372
   Huang F, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300334
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lupton Ellen., 2010, Thinking with Type, 2nd revised and expanded edition: A Critical Guide for Designers, Writers, Editors, Students
   Manandhar D, 2021, PROC CVPR IEEE, P15804, DOI 10.1109/CVPR46437.2021.01555
   Moran K, 2020, IEEE T SOFTWARE ENG, V46, P196, DOI 10.1109/TSE.2018.2844788
   Pandian VPS, 2020, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES COMPANION (IUI'20), P81, DOI 10.1145/3379336.3381497
   Qian J, 2020, PROC INT CONF SOFTW, P297, DOI 10.1145/3377811.3380431
   Reisfeld E, 2023, VISUAL COMPUT, V39, P2811, DOI 10.1007/s00371-022-02494-2
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Swearngin A, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376593
   Tanner K., 2019, DESIGN THINKING RES, P249, DOI [https://doi.org/10.1007/978-3-319-97082-0_13, DOI 10.1007/978-3-319-97082-0_13]
   Nguyen TA, 2015, IEEE INT CONF AUTOM, P248, DOI 10.1109/ASE.2015.32
   Wimmer Christoph, 2020, OzCHI '20: Proceedings of the 32nd Australian Conference on Human-Computer Interaction, P538, DOI 10.1145/3441000.3441015
   Xu P., 2014, P 27 ANN ACM S US IN, P243, DOI DOI 10.1145/2642918.2647398
   Yu F., 2015, Comput. Res. Repos. abs/1511.07122
   Zeidler C., 2012, P INT C NZ ACMS SPEC, P72, DOI DOI 10.1145/2379256.2379268
   Zhang H, 2023, LECT NOTES COMPUT SC, V13843, P541, DOI 10.1007/978-3-031-26313-2_33
   Zhang MJ, 2020, IEEE T IMAGE PROCESS, V29, P1507, DOI 10.1109/TIP.2019.2942514
   Zhang MJ, 2020, IEEE T NEUR NET LEAR, V31, P2623, DOI 10.1109/TNNLS.2019.2933590
   Zhang MJ, 2020, IEEE T CYBERNETICS, V50, P2701, DOI 10.1109/TCYB.2019.2924589
   Zhang MJ, 2019, IEEE T NEUR NET LEAR, V30, P3109, DOI 10.1109/TNNLS.2018.2890017
NR 40
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 13
PY 2023
DI 10.1007/s00371-023-03188-z
EA DEC 2023
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CF3G6
UT WOS:001123791100001
DA 2024-07-18
ER

PT J
AU Qiao, RJ
   Cai, CT
   Meng, HY
   Wu, KJ
   Wang, F
   Zhao, J
AF Qiao, Rengjie
   Cai, Chengtao
   Meng, Haiyang
   Wu, Kejun
   Wang, Feng
   Zhao, Jie
TI Convex hull regression strategy for people detection on top-view fisheye
   images
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE People detection; Fisheye image; Convex hull; Multi-point representation
ID OBJECT
AB Due to the severe distortion of the fisheye image, the rectangular bounding box contain a lot of invalid information. So the multi-point representation method are emerging. However, it will fail in some extreme cases especially when the center of the object is not in the instance. In this work, we propose a Convex Hull Regression Strategy for people detection on top-view fisheye images. It replaces the instance with its convex hull to solve the above challenging issue and can be pre-trained on regular datasets without additional processing. In addition, the mosaic and mixup data augmentation methods that perform well under rectangular boxes are applied to our representation. Finally, we improve the label assignment and propose a more reasonable loss function called PDIoU loss so as to focus on the overall IoU between ground truth polygon and predicted polygon. Experimental results demonstrate that our method outperforms state-of-the-art algorithms. Source code is available at https://github.com/xiaoxuebajie/CHRS.
C1 [Qiao, Rengjie; Cai, Chengtao; Wang, Feng; Zhao, Jie] Harbin Engn Univ, Dept Intelligent Control & Engn, Harbin 150006, Peoples R China.
   [Meng, Haiyang] Aerosp Control Technol Inst, Shanghai 201109, Peoples R China.
   [Wu, Kejun] Nanyang Technol Univ, Nanyang 639798, Singapore.
C3 Harbin Engineering University; Nanyang Technological University
RP Cai, CT (corresponding author), Harbin Engn Univ, Dept Intelligent Control & Engn, Harbin 150006, Peoples R China.
EM qiaorenjie@hrbeu.edu.cn; caichengtao@hrbeu.edu.cn;
   menghaiyang2016@163.com; kejun.wu@ntu.edu.sg; wfeng@hrbeu.edu.cn;
   jiezhao@hrbeu.edu.cn
RI Qiao, Renjie/JCO-9357-2023
OI Qiao, Renjie/0000-0001-7580-0553
FU This work was supported by the National Natural Science Foundation of
   China(No.52171332) and the Natural Science Foundation of Heilongjiang
   Province of China(No.ZD2022F001). [52171332]; National Natural Science
   Foundation of China; Natural Science Foundation of Heilongjiang Province
   of China
FX This work was supported by the National Natural Science Foundation of
   China(No.52171332) and the Natural Science Foundation of Heilongjiang
   Province of China(No.ZD2022F001).
CR Arsenali B, 2019, IEEE INT CONF COMP V, P2373, DOI 10.1109/ICCVW.2019.00291
   Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Chiang A, HUMAN DETECTION IN FISH-EYE IMAGES USING HOG-BASED DETECTORS OVER ROTATED WINDOWS An-Ti Chiang and Yao Wang Department of Electrical and Computer Engineering
   Chiang SH, 2021, IMAGE VISION COMPUT, V105, DOI 10.1016/j.imavis.2020.104069
   Deng LY, 2020, IEEE T INTELL TRANSP, V21, P4350, DOI 10.1109/TITS.2019.2939832
   Dollár P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479
   Dong WB, 2021, IEEE T IMAGE PROCESS, V30, P2193, DOI 10.1109/TIP.2021.3050673
   Duan ZH, 2020, IEEE COMPUT SOC CONF, P2700, DOI 10.1109/CVPRW50498.2020.00326
   Enze Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12190, DOI 10.1109/CVPR42600.2020.01221
   Ge Z., 2021, ARXIV, DOI DOI 10.48550/ARXIV.2107.08430
   Haggui O, 2024, VISUAL COMPUT, V40, P407, DOI 10.1007/s00371-023-02790-5
   Haggui O, 2021, IEEE INT WORKSH MULT, DOI 10.1109/MMSP53017.2021.9733674
   Krams O, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Li SY, 2019, 2019 16TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), DOI [10.1109/avss.2019.8909877, 10.1109/IRCE.2019.00008]
   Li TW, 2020, IEEE ACCESS, V8, P71739, DOI 10.1109/ACCESS.2020.2987868
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Nurnberg R., 2013, Calculating the area and centroid of a polygon, P3
   Playout C., 2021, Adaptable Deformable Convolutions for Semantic Segmentation of Fisheye Images in Autonomous Driving Systems
   Minh QN, 2021, 2021 17TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS 2021), DOI 10.1109/AVSS52988.2021.9663768
   Rashed H, 2021, IEEE WINT CONF APPL, P2271, DOI 10.1109/WACV48630.2021.00232
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Seidel R, 2019, PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISAPP), VOL 5, P474, DOI 10.5220/0007388404740481
   Shifeng Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9756, DOI 10.1109/CVPR42600.2020.00978
   Tamura M, 2019, IEEE WINT CONF APPL, P1989, DOI 10.1109/WACV.2019.00216
   Tian Z, 2022, IEEE T PATTERN ANAL, V44, P1922, DOI 10.1109/TPAMI.2020.3032166
   Wei X, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103715
   Wei X, 2022, NEUROCOMPUTING, V504, P99, DOI 10.1016/j.neucom.2022.06.116
   Wu KJ, 2023, OPT EXPRESS, V31, P11659, DOI 10.1364/OE.482141
   Xu X, 2022, IEEE INT C INT ROBOT, P9911, DOI 10.1109/IROS47612.2022.9981891
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
   Zhu Benjin, 2020, Autoassign: Differentiable label assignment for dense object detection
NR 35
TC 0
Z9 0
U1 5
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 16
PY 2023
DI 10.1007/s00371-023-03137-w
EA NOV 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Y0QD3
UT WOS:001102391900001
DA 2024-07-18
ER

PT J
AU Kaur, P
   Malhi, A
   Pannu, H
AF Kaur, Parminder
   Malhi, Avleen
   Pannu, Husanbir
TI Annotate and retrieve in vivo images using hybrid self-organizing map
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Self-organizing map; Hebbian learning; Oja rule; Cross-modal retrieval;
   Gastrointestinal endoscopy
ID RECOGNITION
AB Multimodal retrieval has gained much attention lately due to its effectiveness over uni-modal retrieval. For instance, visual features often under-constrain the description of an image in content-based retrieval; however, another modality, such as collateral text, can be introduced to abridge the semantic gap and make the retrieval process more efficient. This article proposes the application of cross-modal fusion and retrieval on real in vivo gastrointestinal images and linguistic cues, as the visual features alone are insufficient for image description and to assist gastroenterologists. So, a cross-modal information retrieval approach has been proposed to retrieve related images given text and vice versa while handling the heterogeneity gap issue among the modalities. The technique comprises two stages: (1) individual modality feature learning; and (2) fusion of two trained networks. In the first stage, two self-organizing maps (SOMs) are trained separately using images and texts, which are clustered in the respective SOMs based on their similarity. In the second (fusion) stage, the trained SOMs are integrated using an associative network to enable cross-modal retrieval. The underlying learning techniques of the associative network include Hebbian learning and Oja learning (Improved Hebbian learning). The introduced framework can annotate images with keywords and illustrate keywords with images, and it can also be extended to incorporate more diverse modalities. Extensive experimentation has been performed on real gastrointestinal images obtained from a known gastroenterologist that have collateral keywords with each image. The obtained results proved the efficacy of the algorithm and its significance in aiding gastroenterologists in quick and pertinent decision making.
C1 [Kaur, Parminder; Pannu, Husanbir] Thapar Inst Engn & Technol, CSED, Patiala, India.
   [Kaur, Parminder; Malhi, Avleen; Pannu, Husanbir] Univ Durham, Dept Comp Sci, Durham, England.
   [Malhi, Avleen] Bournemouth Univ, Dept Comp & Informat, Poole, England.
C3 Thapar Institute of Engineering & Technology; Durham University;
   Bournemouth University
RP Kaur, P (corresponding author), Thapar Inst Engn & Technol, CSED, Patiala, India.; Kaur, P (corresponding author), Univ Durham, Dept Comp Sci, Durham, England.
EM pkaur60_phd18@thapar.edu; amalhi@bournemouth.ac.uk; hspannu@thapar.edu
RI Kaur, Parminder/IZD-9508-2023
OI Kaur, Parminder/0000-0002-1306-3137
FU The authors are thankful to (a) Dr. Sunil Arya, Gastroenterologist at
   Leela Bhawan Patiala amp; Dr. G.S. Sidhu at Max Hospital Mohali, India,
   for the dataset and technical feedback; and (b) Professor Khurshid
   Ahmad, Trinity College Dublin, Ireland, for ad
FX The authors are thankful to (a) Dr. Sunil Arya, Gastroenterologist at
   Leela Bhawan Patiala & Dr. G.S. Sidhu at Max Hospital Mohali, India, for
   the dataset and technical feedback; and (b) Professor Khurshid Ahmad,
   Trinity College Dublin, Ireland, for advice and research direction.
CR Aggarwal A, 2016, APPL ARTIF INTELL, V30, P429, DOI 10.1080/08839514.2016.1185859
   Aggarwal H, 2018, IEEE T COGN DEV SYST, V10, P397, DOI 10.1109/TCDS.2017.2658674
   Ahmad K, 2005, NINTH INTERNATIONAL CONFERENCE ON INFORMATION VISUALISATION, PROCEEDINGS, P268, DOI 10.1109/IV.2005.141
   Amato G, 2019, LECT NOTES COMPUT SC, V11751, P324, DOI 10.1007/978-3-030-30642-7_29
   Ametefe DS, 2023, VISUAL COMPUT, V39, P1703, DOI 10.1007/s00371-022-02437-x
   Arulmozhi P, 2021, VISUAL COMPUT, V37, P2391, DOI 10.1007/s00371-020-01993-4
   Bayoudh K, 2022, VISUAL COMPUT, V38, P2939, DOI 10.1007/s00371-021-02166-7
   Bekhouche SE, 2020, MULTIMED TOOLS APPL, V79, P26605, DOI 10.1007/s11042-020-09278-7
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Cheng Q., 2022, IEEE Trans. Circ. Syst. Video Technol., DOI [10.1109/TCSVT.2022.3182549, DOI 10.1109/TCSVT.2022.3182549]
   Chowdhuri S, 2019, IEEE WINT CONF APPL, P1496, DOI 10.1109/WACV.2019.00164
   Curtindale LM, 2019, J EXP CHILD PSYCHOL, V178, P283, DOI 10.1016/j.jecp.2018.10.006
   Dutta A., 2016, VGG image annotator (VIA)
   Dutta A, 2018, MULTIMED TOOLS APPL, V77, P31991, DOI 10.1007/s11042-018-6247-3
   Fredo ARJ, 2019, COMPOS PART B-ENG, V168, P77, DOI 10.1016/j.compositesb.2018.12.064
   Gabor A., 2019, Telemedicine Technologies, P1
   Ghorbanzadeh O, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11020196
   Guo Y., 2021, US Patent, Patent No. [11,195,313, 11195313]
   Gupta A, 2021, COMPUT BIOL MED, V138, DOI 10.1016/j.compbiomed.2021.104920
   HEBB D. O., 1949
   Ibrahim Rowaida Khalil, 2021, 2021 International Conference on Advanced Computer Applications (ACA), P28, DOI 10.1109/ACA52198.2021.9626822
   Jehangir S, 2021, MEHRAN UNIV RES J EN, V40, P152, DOI 10.22581/muet1982.2101.14
   John LJ, 2013, J PHARMACOL PHARMACO, V4, P86, DOI 10.4103/0976-500X.110870
   Karthik K, 2021, VISUAL COMPUT, V37, P1837, DOI 10.1007/s00371-020-01941-2
   Kaski S, 1998, NEUROCOMPUTING, V21, P101, DOI 10.1016/S0925-2312(98)00039-3
   Kaur P, 2022, KNOWL-BASED SYST, V239, DOI 10.1016/j.knosys.2021.108014
   Kaur P, 2021, COMPUT SCI REV, V39, DOI 10.1016/j.cosrev.2020.100336
   Kaur P, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3331167
   KOHONEN T, 1982, BIOL CYBERN, V43, P59, DOI 10.1007/BF00337288
   Kohonen T, 2013, NEURAL NETWORKS, V37, P52, DOI 10.1016/j.neunet.2012.09.018
   Laaksonen J, 2002, IEEE T NEURAL NETWOR, V13, P841, DOI 10.1109/TNN.2002.1021885
   Li WR, 2023, IEEE T CIRC SYST VID, V33, P3516, DOI 10.1109/TCSVT.2022.3233042
   Li Z, 2022, ELECTR POW SYST RES, V207, DOI 10.1016/j.epsr.2022.107854
   Liu ZP, 2023, ECOL INDIC, V154, DOI 10.1016/j.ecolind.2023.110697
   Ma J., 2023, VISUAL COMPUT, P1
   Mehmood Z, 2018, APPL INTELL, V48, P166, DOI 10.1007/s10489-017-0957-5
   Moradi Mehdi, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P300, DOI 10.1007/978-3-319-46723-8_35
   Nicholson AA, 2019, PSYCHOL MED, V49, P2049, DOI 10.1017/S0033291718002866
   OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687
   Pacella M, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/5139574
   Paek S., 1999, Acm Sigir, V99, P15
   Palazzo S, 2021, IEEE T PATTERN ANAL, V43, P3833, DOI 10.1109/TPAMI.2020.2995909
   Pannu HS, 2020, MULTIMED TOOLS APPL, V79, P21941, DOI 10.1007/s11042-020-08905-7
   Rangarajan AK, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-59108-x
   Rankovic N, 2023, J PERS MED, V13, DOI 10.3390/jpm13071032
   Rezende E, 2018, INFORM TECHNOLOGYNEW, P51, DOI [10.1007/978-3-319-77028-4_9, DOI 10.1007/978-3-319-77028-4_9]
   Shah B., 2023, Vis. Comput., P1
   Sharma V, 2023, VISUAL COMPUT, V39, P6503, DOI 10.1007/s00371-022-02742-5
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Slave AR, 2023, LANDSCAPE URBAN PLAN, V231, DOI 10.1016/j.landurbplan.2022.104641
   Soltani MM, 2016, AUTOMAT CONSTR, V62, P14, DOI 10.1016/j.autcon.2015.10.002
   TEAGUE MR, 1980, J OPT SOC AM, V70, P920, DOI 10.1364/JOSA.70.000920
   Tulsulkar G, 2021, VISUAL COMPUT, V37, P3019, DOI 10.1007/s00371-021-02242-y
   Viitaniemi V., 2005, Keyword-detection approach to automatic image annotation
   Vlaovic ZD, 2023, J CLEAN PROD, V412, DOI 10.1016/j.jclepro.2023.137351
   Wang YF, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P307, DOI 10.1145/2647868.2654901
   Wen KY, 2022, Arxiv, DOI arXiv:2207.00733
   Xie L, 2016, MULTIMED TOOLS APPL, V75, P9185, DOI 10.1007/s11042-016-3432-0
   Xie ZW, 2022, IEEE T SERV COMPUT, V15, P3304, DOI 10.1109/TSC.2021.3098834
   Xu X, 2017, STUD COMPUT INTELL, V672, P165, DOI 10.1007/978-3-319-46245-5_10
   Yang HY, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-95240-y
   Yu LT, 2016, IEEE T IMAGE PROCESS, V26, P5689, DOI 10.1109/TIP.2016.2614136
   Zang Y., 2023, Vis. Comput, V10, P1
   Zernike F, 1934, PHYSICA, V1, P689
   Zhang DS, 2012, PATTERN RECOGN, V45, P346, DOI 10.1016/j.patcog.2011.05.013
   Zhang W, 2011, EXPERT SYST APPL, V38, P2758, DOI 10.1016/j.eswa.2010.08.066
   Zhou G, 2023, FRONT COMPUT NEUROSC, V17, DOI 10.3389/fncom.2023.1079483
   Zhou TH, 2015, SUSTAINABILITY-BASEL, V7, P6303, DOI 10.3390/su7056303
   Zhuang HM, 2023, VISUAL COMPUT, V39, P2207, DOI 10.1007/s00371-021-02322-z
NR 69
TC 0
Z9 0
U1 6
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 31
PY 2023
DI 10.1007/s00371-023-03126-z
EA OCT 2023
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W3JS7
UT WOS:001090630900001
OA hybrid
DA 2024-07-18
ER

PT J
AU Angeli, A
   Stacchio, L
   Donatiello, L
   Giacchè, A
   Marfia, G
AF Angeli, Alessia
   Stacchio, Lorenzo
   Donatiello, Lorenzo
   Giacche, Alessandro
   Marfia, Gustavo
TI Making paper labels smart for augmented wine recognition
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Augmented reality; Augmented wine recognition; Ubiquitous computing;
   Artificial intelligence; Optical character recognition
ID VISUALIZATION
AB An invisible layer of knowledge is progressively growing with the emergence of situated visualizations and reality-based information retrieval systems. In essence, digital content will overlap with real-world entities, eventually providing insights into the surrounding environment and useful information for the user. The implementation of such a vision may appear close, but many subtle details separate us from its fulfillment. This kind of implementation, as the overlap between rendered virtual annotations and the camera's real-world view, requires different computer vision paradigms for object recognition and tracking which often require high computing power and large-scale datasets of images. Nevertheless, these resources are not always available, and in some specific domains, the lack of an appropriate reference dataset could be disruptive for a considered task. In this particular scenario, we here consider the problem of wine recognition to support an augmented reading of their labels. In fact, images of wine bottle labels may not be available as wineries periodically change their designs, product information regulations may vary, and specific bottles may be rare, making the label recognition process hard or even impossible. In this work, we present augmented wine recognition, an augmented reality system that exploits optical character recognition paradigms to interpret and exploit the text within a wine label, without requiring any reference image. Our experiments show that such a framework can overcome the limitations posed by image retrieval-based systems while exhibiting a comparable performance.
C1 [Angeli, Alessia; Donatiello, Lorenzo; Giacche, Alessandro] Univ Bologna, Dept Comp Sci & Engn, Bologna, Italy.
   [Stacchio, Lorenzo] Univ Bologna, Dept Life Qual Studies, Rimini, Italy.
   [Marfia, Gustavo] Univ Bologna, Dept Arts, Bologna, Italy.
C3 University of Bologna; University of Bologna; University of Bologna
RP Marfia, G (corresponding author), Univ Bologna, Dept Arts, Bologna, Italy.
EM gustavo.marfia@unibo.it
OI MARFIA, GUSTAVO/0000-0003-3058-8004; Stacchio,
   Lorenzo/0000-0002-9341-7651
FU This work was supported by the University of Bologna with the Alma
   Attrezzature 2017 grant, by AEFFE S.p.a., the Golinelli Foundation with
   the funding of two Ph.D. scholarships, and by ImageLine S.r.l. for
   providing the wine textual dataset.; University of Bologna; AEFFE
   S.p.a.; Golinelli Foundation
FX This work was supported by the University of Bologna with the Alma
   Attrezzature 2017 grant, by AEFFE S.p.a., the Golinelli Foundation with
   the funding of two Ph.D. scholarships, and by ImageLine S.r.l. for
   providing the wine textual dataset.
CR Abualigah L, 2022, EXPERT SYST APPL, V191, DOI 10.1016/j.eswa.2021.116158
   Abualigah L, 2021, COMPUT IND ENG, V157, DOI 10.1016/j.cie.2021.107250
   Abualigah L, 2021, COMPUT METHOD APPL M, V376, DOI 10.1016/j.cma.2020.113609
   Agushaka JO, 2022, COMPUT METHOD APPL M, V391, DOI 10.1016/j.cma.2022.114570
   Alston JM, 2021, ITAL ECON J, V7, P219, DOI 10.1007/s40797-021-00145-4
   AlvarezMarquez J.O., 2017, MENSCH COMP 2017 WOR
   [Anonymous], 2007, Choices. The Magazine of Food, Farm, DOI DOI 10.2307/CHOICES.22.1.0031
   [Anonymous], 2021, Easy Ocr: JadedAI
   [Anonymous], 2021, TinEye: WineEngine is image recognition for the beverage industry
   Baek J, 2019, IEEE I CONF COMP VIS, P4714, DOI 10.1109/ICCV.2019.00481
   Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959
   Bansal R, 2016, PROCEEDINGS OF THE 5TH INTERNATIONAL CONFERENCE ON SYSTEM MODELING & ADVANCEMENT IN RESEARCH TRENDS (SMART-2016), P63, DOI 10.1109/SYSMART.2016.7894491
   Bayu MZ, 2013, PROC TECH, V11, P396, DOI 10.1016/j.protcy.2013.12.208
   Bressa N, 2022, IEEE T VIS COMPUT GR, V28, P107, DOI 10.1109/TVCG.2021.3114835
   Breuel TM, 2017, PROC INT CONF DOC, P11, DOI 10.1109/ICDAR.2017.12
   Büschel W, 2018, CHIIR'18: PROCEEDINGS OF THE 2018 CONFERENCE ON HUMAN INFORMATION INTERACTION & RETRIEVAL, P171, DOI 10.1145/3176349.3176384
   Cakic S., 2020, V2020, P2020, DOI DOI 10.1109/IT48810.2020.9070558
   Camera di Commercio Molise, 2016, Guida etichettature vino
   Charters S., 1999, Journal of Wine Research, V10, P183, DOI 10.1080/09571269908718177
   Ezugwu AE, 2022, NEURAL COMPUT APPL, V34, P20017, DOI 10.1007/s00521-022-07530-9
   FEDERDOC, 2021, I VINI ITALIANI A DENOMINAZIONE D'ORIGINE 2020
   Fino Michele A., 2013, Questione di Etichetta
   FITZMAURICE GW, 1993, COMMUN ACM, V36, P39, DOI 10.1145/159544.159566
   Follmann Patrick, 2019, Pattern Recognition. 40th German Conference, GCPR 2018. Proceedings: Lecture Notes in Computer Science (LNCS 11269), P363, DOI 10.1007/978-3-030-12939-2_25
   Gebru T., 2022, Mobile wine label recognition
   Glasbey CA, 1998, J APPL STAT, V25, P155, DOI 10.1080/02664769823151
   Graves A., 2006, P 23 INT C MACHINE L, P369
   Gundimeda Venugopal, 2019, First International Conference on Artificial Intelligence and Cognitive Computing (AICC 2018). Advances in Intelligent Systems and Computing (AISC 815), P199, DOI 10.1007/978-981-13-1580-0_20
   Haugstvedt AC, 2012, INT SYM MIX AUGMENT, P247, DOI 10.1109/ISMAR.2012.6402563
   Henchoz N, 2019, ADJUNCT PROCEEDINGS OF THE 2019 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT 2019), P98, DOI 10.1109/ISMAR-Adjunct.2019.00040
   Hinz O, 2011, J MANAGE INFORM SYST, V27, P43, DOI 10.2753/MIS0742-1222270402
   Hu B, 2020, IEEE ACCESS, V8, P19336, DOI 10.1109/ACCESS.2020.2967090
   정종문, 2011, [The Journal of the Korea Contents Association, 한국콘텐츠학회 논문지], V11, P125
   Inseop Na, 2014, International Journal of Contents, V10, P1, DOI 10.5392/IJoC.2014.10.4.001
   Kirillov A, 2023, Arxiv, DOI arXiv:2304.02643
   LEVENSHT.VI, 1965, DOKL AKAD NAUK SSSR+, V163, P845
   Lin MY, 2020, I C CONT AUTOMAT ROB, P957, DOI [10.1109/ICARCV50220.2020.9305489, 10.1109/icarcv50220.2020.9305489]
   livingwinelabels: livingwinelabels, 2021, About us
   Martins NC, 2022, MULTIMED TOOLS APPL, V81, P14749, DOI 10.1007/s11042-021-10971-4
   Orsini A., 2017, Augmented Reality Enhanced Cooking with Microsoft Hololens
   Oyelade ON, 2022, IEEE ACCESS, V10, P16150, DOI 10.1109/ACCESS.2022.3147821
   Penco L, 2021, J MANAG GOV, V25, P1179, DOI 10.1007/s10997-020-09526-w
   Portinari Vittorio, 2016, Elementi di Legislazione Vitivinicola: le norme per l'etichettatura e la tracciabilita dei vini
   PTC, 2022, Vivino and Vuforia's Image Recognition Solution Make a Great Pairing
   Rejeb Abderahman, 2021, Journal of Foodservice Business Research, V24, P415, DOI [10.1080/15378020.2020.1859973, 10.1080/15378020.2021.1883214]
   Salim Nareen O. M., 2021, Journal of Physics: Conference Series, DOI 10.1088/1742-6596/1963/1/012014
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   Singh Amarjot, 2012, International Journal of Machine Learning and Computing, V2, P314, DOI [10.7763/IJMLC.2012.V2.137, DOI 10.7763/IJMLC.2012.V2.137]
   Smelyakov K., 2021, CEUR WORKSHOP PROC, P154
   Stacchio L., 2022, IN PRESS, P1
   Stacchio L, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P643, DOI 10.1109/VRW52623.2021.00204
   Styliaras G.D., 2021, Digital, V1, P216, DOI [10.3390/digital1040016, DOI 10.3390/DIGITAL1040016]
   Tscheu Frances., 2016, Information and Communication Technologies in Tourism 2016, P607, DOI [https://doi.org/10.1007/978-3-319-28231-2_44, DOI 10.1007/978-3-319-28231-2_44]
   Vivino: Vivino, 2021, About us
   Vrigkas M., 2021, SHS WEB C EDP SCI, V102
   Vuforia, 2022, Vuforia SDK
   Wick C, 2018, Arxiv, DOI arXiv:1807.02004
   Wu MY, 2015, 2015 38TH INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS AND SIGNAL PROCESSING (TSP), P568, DOI 10.1109/TSP.2015.7296327
   Xiaoqing Li, 2019, Intelligent Computing Theories and Application. 15th International Conference, ICIC 2019. Proceedings: Lecture Notes in Computer Science (LNCS 11643), P250, DOI 10.1007/978-3-030-26763-6_24
   Yuka: Yuka, 2021, About us
   Zhan FN, 2019, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2019.00216
   Zhu LL, 2021, CURR RES FOOD SCI, V4, P233, DOI 10.1016/j.crfs.2021.03.009
NR 62
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 27
PY 2023
DI 10.1007/s00371-023-03119-y
EA OCT 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W0BN1
UT WOS:001088370100001
OA hybrid
DA 2024-07-18
ER

PT J
AU Manu, CM
   Sreeni, KG
AF Manu, Chippy M.
   Sreeni, K. G.
TI GANID: a novel generative adversarial network for image dehazing
SO VISUAL COMPUTER
LA English
DT Article
DE Image dehazing; Generative adversarial network (GAN); Dilated
   convolution; Deep learning
ID QUALITY ASSESSMENT; VISIBILITY; FOG
AB This paper presents a novel algorithm to dehaze a given hazy input image using a generative adversarial network (GAN). The proposed GAN structure uses a Feature Residual Dense combined Network (FRDN) as a generator and a Markovian discriminator (PatchGAN) with additional layers as the discriminator. FRDN is capable of extracting contextual information using Feature Module (FM) in conjunction with the Residual Dense Module, and IMCU enhances collaborative learning which enhances its performance. The inclusion of proposed reality and visibility loss functions along with L1 loss improves the scene visibility and realness of the dehazed image. The network is trained with images from the benchmark datasets-RESIDE and NTIRE 2021. The proposed technique's performance is evaluated using various metrics such as PSNR, SSIM, FSIM, FADE, NIQE, and BRISQUE. An average PSNR of 25.701, 32.52, and 33.96 has been obtained with the Indoor Training Set, Synthetic Objective Testing Set indoor, and SOTS outdoor images, respectively. The experimental results reveal that the suggested method's performance is better compared to other state-of-the-art techniques.
C1 [Manu, Chippy M.; Sreeni, K. G.] APJA KTU, Dept Elect & Commun, Comp Vis Lab, Coll Engn, Thiruvananthapuram, India.
C3 College of Engineering, Trivandrum
RP Manu, CM (corresponding author), APJA KTU, Dept Elect & Commun, Comp Vis Lab, Coll Engn, Thiruvananthapuram, India.
EM chippyaicte@gmail.com; sreenikg79@gmail.com
CR Amaranageswarao G, 2022, VISUAL COMPUT, V38, P31, DOI 10.1007/s00371-020-01998-z
   Ancuti CO, 2021, IEEE COMPUT SOC CONF, P627, DOI 10.1109/CVPRW53098.2021.00074
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bai HR, 2022, IEEE T IMAGE PROCESS, V31, P1217, DOI 10.1109/TIP.2022.3140609
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Dong Y, 2020, AAAI CONF ARTIF INTE, V34, P10729
   Engin D, 2018, IEEE COMPUT SOC CONF, P938, DOI 10.1109/CVPRW.2018.00127
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Golts A, 2020, IEEE T IMAGE PROCESS, V29, P2692, DOI 10.1109/TIP.2019.2952032
   Goodfellow I. J., 2014, ARXIV
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hong M, 2020, PROC CVPR IEEE, P3459, DOI 10.1109/CVPR42600.2020.00352
   Hsieh PW, 2022, SIGNAL PROCESS, V192, DOI 10.1016/j.sigpro.2021.108396
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Lee C.-Y., 2014, AISTATS
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li B., 2017, AAAI
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li RD, 2018, PROC CVPR IEEE, P8202, DOI 10.1109/CVPR.2018.00856
   Liu W, 2020, IEEE T IMAGE PROCESS, V29, P7819, DOI 10.1109/TIP.2020.3007844
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Liu Z., 2018, COMPUTER VISION PATT
   Ma TS, 2021, VISUAL COMPUT, V37, P925, DOI 10.1007/s00371-020-01843-3
   Mehta A, 2020, IEEE COMPUT SOC CONF, P846, DOI 10.1109/CVPRW50498.2020.00114
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Qu YY, 2019, PROC CVPR IEEE, P8152, DOI 10.1109/CVPR.2019.00835
   Radford A., 2015, ARXIV
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Soma P, 2022, VISUAL COMPUT, V38, P2569, DOI 10.1007/s00371-021-02132-3
   Sun ZY, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3478457
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Tan RT, 2007, 2007 IEEE INTELLIGENT VEHICLES SYMPOSIUM, VOLS 1-3, P435
   van Oldenborgh GJ, 2010, ATMOS CHEM PHYS, V10, P4597, DOI 10.5194/acp-10-4597-2010
   Wang C, 2021, VISUAL COMPUT, V37, P1851, DOI 10.1007/s00371-020-01944-z
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wenlong Z., 2019, IEEE T PATTERN ANAL, V1, P1
   Yang F, 2022, VISUAL COMPUT, V38, P1579, DOI 10.1007/s00371-021-02089-3
   Yang XT, 2018, AAAI CONF ARTIF INTE, P7485
   Yifan L., 2021, Vis. Comput, V37, P09
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhao SY, 2020, IEEE T IMAGE PROCESS, V29, P6947, DOI 10.1109/TIP.2020.2995264
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
   Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 56
TC 6
Z9 7
U1 7
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 3923
EP 3936
DI 10.1007/s00371-022-02536-9
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:001060095400007
DA 2024-07-18
ER

PT J
AU Yang, AP
   Wei, ZH
   Wang, JB
   Cao, JL
   Ji, Z
   Pang, YW
AF Yang, Aiping
   Wei, Zihao
   Wang, Jinbin
   Cao, Jiale
   Ji, Zhong
   Pang, Yanwei
TI Multi-feature self-attention super-resolution network
SO VISUAL COMPUTER
LA English
DT Article
DE Convolutional neural network; Super-resolution; Multi-scale;
   Self-attention; Backward fusion
ID IMAGE SUPERRESOLUTION
AB In recent years, single-image super-resolution (SISR) methods based on the attention mechanism have been widely explored and achieved remarkable performances. However, most existing networks only explore channel correlations or the spatial long-distance dependences in a single scale while ignoring the mutual guidance of multi-scale information, resulting in the loss of high-frequency information in the reconstructed image. To address this issue, we propose a multi-feature self-attention super-resolution network (MFSN) to embed multi-scale encoding information into the attention mechanism. Specifically, the network consists of a shallow feature extraction subnetwork, a multi-feature alignment subnetwork (MFAN) and a reconstruction subnetwork. The MFAN is composed of an adjacent feature alignment residual block (AFAB) and a dense backward fusion block (DBFB), where AFAB explores multi-scale encoding information using the low-resolution space statistics with larger receptive fields to weight and align the original-scale feature map, so as to extract more discriminative high-frequency features adaptively. Meanwhile, the contrast-aware channel attention module adopts contrast pooling that is more suitable for low-level computer vision tasks to realize the adaptive selection of channel feature in the AFAB. Structurally, the DBFB adopts the backward fusion mechanism to fuse the output of each AFAB, from the deep layer to the shallow layer to make full use of the hierarchical features. Experimental results demonstrate the superiority of our MFSN network in terms of both quantitative metrics and visual quality.
C1 [Yang, Aiping; Wei, Zihao; Wang, Jinbin; Cao, Jiale; Ji, Zhong; Pang, Yanwei] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Yang, Aiping] Shanghai Artificial Intelligence Lab, Shanghai, Peoples R China.
C3 Tianjin University
RP Wang, JB (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM wjb@tju.edu.cn
RI Wang, Jinbin/HPC-5396-2023
OI Wang, Jinbin/0000-0001-8432-7235
FU National Key Research and Development Program of China [2022ZD0160400];
   National Natural Science Foundation of China [62071323, 62176178]
FX This work was supported by the National Key Research and Development
   Program of China (Grant No. 2022ZD0160400) and the National Natural
   Science Foundation of China (Grant Nos. 62071323 and 62176178). We
   gratefully acknowledge the support from Shanghai Artificial Intelligence
   Laboratory.
CR Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dodgson NA, 1997, IEEE T IMAGE PROCESS, V6, P1322, DOI 10.1109/83.623195
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Esmaeilzehi A, 2021, IEEE T COMPUT IMAG, V7, P409, DOI 10.1109/TCI.2021.3070522
   Greenspan H, 2009, COMPUT J, V52, P43, DOI 10.1093/comjnl/bxm075
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang-Jiang Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10093, DOI 10.1109/CVPR42600.2020.01011
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li B, 2020, IEEE T IMAGE PROCESS, V29, P8368, DOI 10.1109/TIP.2020.3014953
   Li JC, 2018, LECT NOTES COMPUT SC, V11212, P527, DOI 10.1007/978-3-030-01237-3_32
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Luo X., 2020, LNCS, P272, DOI DOI 10.1007/978-3-030-58542-617
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mei YQ, 2021, PROC CVPR IEEE, P3516, DOI 10.1109/CVPR46437.2021.00352
   Muqeet Abdul, 2020, Proceedings of the 16th European Conference on Computer Vision - ECCV 2020 Workshops. Lecture Notes in Computer Science (LNCS 12537), P103, DOI 10.1007/978-3-030-67070-2_6
   Niu B., 2020, EUR C COMP VIS, P191, DOI [10.1007/978-3-030-58610-2_47, DOI 10.1007/978-3-030-58610-2_12]
   Pfster H., 2021, P IEEE CVF INT C COM, P4278
   Sajjadi Mehdi S. M., 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P4501, DOI 10.1109/ICCV.2017.481
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Sun L., 2022, ARXIV
   Swaminathan A, 2008, IEEE T INF FOREN SEC, V3, P101, DOI 10.1109/TIFS.2007.916010
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tang H, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108792
   Tian C., 2022, arXiv
   Tian CW, 2023, PATTERN RECOGN, V134, DOI 10.1016/j.patcog.2022.109050
   Tian Chunwei, 2024, IEEE Trans Neural Netw Learn Syst, V35, P6507, DOI 10.1109/TNNLS.2022.3210433
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Wang DM, 2021, INT SYMP INERT SENSO, DOI 10.1109/INERTIAL51137.2021.9430471
   Wang LG, 2021, PROC CVPR IEEE, P4915, DOI 10.1109/CVPR46437.2021.00488
   Wang Z., 2017, Digital Video image quality and perceptual coding, P225, DOI DOI 10.1201/9781420027822-7
   Ward C.M., 2017, APPL DIGITAL IMAGE P, V10396, P19
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang DY, 2021, IEEE T GEOSCI REMOTE, V59, P5183, DOI 10.1109/TGRS.2020.3009918
   Zhang M, 2022, MICROMACHINES-BASEL, V13, DOI 10.3390/mi13010054
   Zhang Q, 2023, CAAI T INTELL TECHNO, V8, P331, DOI 10.1049/cit2.12110
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhao H, 2020, COMPUTER VISION ECCV, P56, DOI DOI 10.1007/978-3-030-67070-23
   Zhu FY, 2019, IEEE INT CONF COMP V, P2453, DOI 10.1109/ICCVW.2019.00300
NR 56
TC 1
Z9 1
U1 14
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3473
EP 3486
DI 10.1007/s00371-023-03046-y
EA AUG 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001049919400001
DA 2024-07-18
ER

PT J
AU Wang, JX
   Zou, YS
   Wu, HL
AF Wang, Jianxin
   Zou, Yongsong
   Wu, Honglin
TI Image super-resolution method based on attention aggregation hierarchy
   feature
SO VISUAL COMPUTER
LA English
DT Article
DE Super-resolution; Hierarchical features; Shift operation; Attention
   mechanism
ID NETWORK
AB Recently, single-image super-resolution (SISR) based on convolutional neural networks (CNNs) has encountered challenges, including the presence of numerous network parameters, limited receptive field, and the inability to capture global context information. In order to address these issues, we propose an image super-resolution method based on attention aggregation hierarchy feature (AHSR), which improves the performance of the super-resolution (SR) network through the optimization of convolutional operations and the integration of effective attention modules. AHSR first uses a high-frequency filter to bypass the rich low-frequency information, allowing the main network to focus on learning the high-frequency information. In order to aggregate spatial information within the image, expand the receptive field, and extract local structural features more effectively, we propose the utilization of the shift operation with zero parameters and zero triggers instead of spatial convolution. Additionally, we introduce a multi-Dconv head transposed attention module to improve the aggregation of cross-hierarchical feature information. This approach allows us to obtain enhanced features that incorporate contextual information. Extensive experimental results show that compared to other advanced SR models, the proposed AHSR method can better recover image details with fewer model parameters and less computational complexity.
C1 [Wang, Jianxin; Wu, Honglin] Changsha Univ Sci & Technol, Sch Comp & Commun Engn, Changsha 410114, Peoples R China.
   [Wang, Jianxin; Zou, Yongsong] Changsha Univ Sci & Technol, Sch Hydraul & Environm Engn, Changsha 410114, Peoples R China.
C3 Changsha University of Science & Technology; Changsha University of
   Science & Technology
RP Wu, HL (corresponding author), Changsha Univ Sci & Technol, Sch Comp & Commun Engn, Changsha 410114, Peoples R China.
EM wjx@csust.edu.cn; zys@stu.csust.edu.cn; honglinwu@csust.edu.cn
RI Wang, Jianxin/V-2800-2018
OI wu, honglin/0000-0002-9586-2927
FU Scientific Research Fund of Hunan Provincial Education Department
   [22C0171, 21B0329]; Traffic Science and Technology Project of Hunan
   Province [202042]; Changsha Municipal Natural Science Foundation
   [kq2208236]
FX AcknowledgementsThis work was supported by Scientific Research Fund of
   Hunan Provincial Education Department (Grant Nos. 22C0171 and 21B0329),
   the Traffic Science and Technology Project of Hunan Province (Grant No.
   202042), and in part by Changsha Municipal Natural Science Foundation
   (kq2208236).
CR Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chen H., 2021, arXiv
   Chen YT, 2024, VISUAL COMPUT, V40, P489, DOI 10.1007/s00371-023-02795-0
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Farsiu S, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 2, PROCEEDINGS, P291, DOI 10.1109/ICIP.2003.1246674
   Fritsche M, 2019, IEEE INT CONF COMP V, P3599, DOI 10.1109/ICCVW.2019.00445
   Han CL, 2022, EXPERT SYST APPL, V198, DOI 10.1016/j.eswa.2022.116764
   Hendrycks D., 2016, ARXIV
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Huang WB, 2023, IEEE T MOBILE COMPUT, V22, P5064, DOI 10.1109/TMC.2022.3174816
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D. P., 2014, arXiv
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li F, 2019, NEUROCOMPUTING, V358, P285, DOI 10.1016/j.neucom.2019.05.042
   Li JC, 2018, LECT NOTES COMPUT SC, V11212, P527, DOI 10.1007/978-3-030-01237-3_32
   LI Z, 2022, P IEEECVF C COMPUTER, P833
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   LIU D., 2018, ADV NEURAL INF PROCE, V31, P1673, DOI [DOI 10.48550/ARXIV.1806.02919, 10.48550/arXiv.1806.02919]
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Lu ZS, 2022, IEEE COMPUT SOC CONF, P456, DOI 10.1109/CVPRW56347.2022.00061
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Park SC, 2003, IEEE SIGNAL PROC MAG, V20, P21, DOI 10.1109/MSP.2003.1203207
   Qiu YJ, 2019, IEEE I CONF COMP VIS, P4179, DOI 10.1109/ICCV.2019.00428
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Tang Y, 2022, IEEE T EM TOP COMP I, V6, P1167, DOI 10.1109/TETCI.2021.3136642
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Wang J, 2021, KSII T INTERNET INF, V15, P4065, DOI 10.3837/tiis.2021.11.011
   Wang W, 2019, INT J COMPUT INT SYS, V12, P1592, DOI 10.2991/ijcis.d.191209.001
   Wang XM, 2019, NEUROCOMPUTING, V364, P269, DOI 10.1016/j.neucom.2019.06.078
   Wu BC, 2018, PROC CVPR IEEE, P9127, DOI 10.1109/CVPR.2018.00951
   Yang CY, 2011, LECT NOTES COMPUT SC, V6494, P497, DOI 10.1007/978-3-642-19318-7_39
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang JM, 2021, J AMB INTEL HUM COMP, V12, P8427, DOI 10.1007/s12652-020-02572-0
   Zhang L, 2006, IEEE T IMAGE PROCESS, V15, P2226, DOI 10.1109/TIP.2006.877407
   Zhang X., 2022, ARXIV
   Zhang Y., 2019, arXiv
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhao H, 2020, COMPUTER VISION ECCV, P56, DOI DOI 10.1007/978-3-030-67070-23
   Zhou D., 2022, VISUAL COMPUT, P1
NR 44
TC 3
Z9 3
U1 4
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2655
EP 2666
DI 10.1007/s00371-023-02968-x
EA JUL 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001032577400001
DA 2024-07-18
ER

PT J
AU Wang, RS
   Duan, YF
   Li, YK
   Zheng, DS
   Liu, XH
   Lam, CT
   Tan, T
AF Wang, Rongsheng
   Duan, Yaofei
   Li, Yukun
   Zheng, Dashun
   Liu, Xiaohong
   Lam, Chan Tong
   Tan, Tao
TI PCTMF-Net: heart sound classification with parallel CNNs-transformer and
   second-order spectral analysis
SO VISUAL COMPUTER
LA English
DT Article
DE Classification of heart sound; Heart sound signal; Higher-order
   spectrum; Parallel convolution and transformer
ID FEATURES; DIAGNOSIS
AB Heart disease is a common condition worldwide and has become one of the leading causes of death worldwide. The electrocardiogram (PCG) is a safe, painless, and non-invasive test that captures bioacoustic information reflecting the function of the heart by capturing the acoustic signal of the patient's heart. Nowadays, based on biosignal processing and artificial intelligence technologies, automated heart sound classification is playing an increasingly important role in clinical applications. In this paper, we propose a new parallel CNNs-transformer network with multi-scale feature context aggregation (PCTMF-Net). It combines the advantages of CNNs and transformer to achieve efficient heart sound classification. In PCTMF-Net, firstly, the heart tone signal features are extracted using the second-order spectral analysis, and a transformer-based MHTE-4 (multi-head transformer encoder with four attention heads) is designed to encode and aggregate the contextual information, and then, two CNNs feature extractors are designed in parallel with MHTE-4 to capture the hierarchical features. Finally, the feature vectors obtained from CNNs and MHTE-4 through feature fusion in PCTMF-Net will be fed into the fully connected layer for predicting the classification results of heart sounds. In addition, we perform validation based on two publicly available mutually exclusive heart sound datasets and conduct extensive experiments and comparisons of existing algorithms under different metrics. The experimental results show that our proposed method achieves 99.36% accuracy on the Yaseen dataset and 93% accuracy on the PhysioNet dataset. It surpasses current algorithms in terms of accuracy, recall and F1-score metrics. The aim of this study is to apply these new techniques and methods to improve the diagnostic accuracy and validity of heart disease for clinical use.
C1 [Wang, Rongsheng; Duan, Yaofei; Li, Yukun; Zheng, Dashun; Lam, Chan Tong; Tan, Tao] Macao Polytech Univ, Fac Appl Sci, Rua Luis Gonzaga Gomes, Taipa 999078, Macao, Peoples R China.
   [Liu, Xiaohong] Shanghai Jiao Tong Univ, Fac John Hopcroft Ctr, Shanghai 200240, Peoples R China.
C3 Macao Polytechnic University; Shanghai Jiao Tong University
RP Tan, T (corresponding author), Macao Polytech Univ, Fac Appl Sci, Rua Luis Gonzaga Gomes, Taipa 999078, Macao, Peoples R China.
EM taotanjs@gmail.com
RI Li, yukun/HZJ-2455-2023; Wang, Rongsheng/KFB-5384-2024; Wang,
   Rongsheng/AGM-5221-2022
OI xie, hui/0000-0002-8490-1695; Wang, Rongsheng/0000-0003-2390-5999
FU Macao Polytechnic University [RP/FCA-15/2022]
FX This work is supported by Macao Polytechnic University Grant
   (RP/FCA-15/2022).
CR Abbas Q, 2022, DIAGNOSTICS, V12, DOI 10.3390/diagnostics12123109
   Alquran H, 2019, NEURAL NETW WORLD, V29, P207, DOI 10.14311/NNW.2019.29.014
   [Anonymous], 2017, TECH J
   Bozkurt B, 2018, COMPUT BIOL MED, V100, P132, DOI 10.1016/j.compbiomed.2018.06.026
   Clifford GD, 2017, PHYSIOL MEAS, V38, pE10, DOI 10.1088/1361-6579/aa7ec8
   Deng MQ, 2020, NEURAL NETWORKS, V130, P22, DOI 10.1016/j.neunet.2020.06.015
   Durak L, 2003, IEEE T SIGNAL PROCES, V51, P1231, DOI 10.1109/TSP.2003.810293
   Fatmawati T. Y., 2021, Proceedings of the 1st International Conference on Electronics, Biomedical Engineering, and Health Informatics. ICEBEHI 2020. Lecture Notes in Electrical Engineering (LNEE 746), P593, DOI 10.1007/978-981-33-6926-9_52
   Hayes SN, 2018, CIRCULATION, V137, pE523, DOI 10.1161/CIR.0000000000000564
   Ismail S., 2018, EURASIP Journal on Advances in Signal Processing, V2018, P1
   Jagannathan R, 2019, CURR DIABETES REP, V19, DOI 10.1007/s11892-019-1161-2
   Krishnan PT, 2020, PHYS ENG SCI MED, V43, P505, DOI 10.1007/s13246-020-00851-w
   Kumar D, 2006, 2006 28TH ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING IN MEDICINE AND BIOLOGY SOCIETY, VOLS 1-15, P4862
   Latif S, 2018, IEEE SENS J, V18, P9393, DOI 10.1109/JSEN.2018.2870759
   Li LN, 2017, CHIN AUTOM CONGR, P7380, DOI 10.1109/CAC.2017.8244111
   Liu CY, 2016, PHYSIOL MEAS, V37, P2181, DOI 10.1088/0967-3334/37/12/2181
   Maknickas V, 2017, PHYSIOL MEAS, V38, P1671, DOI 10.1088/1361-6579/aa7841
   Nilanon T, 2016, COMPUT CARDIOL CONF, V43, P585
   Oliveira J, 2021, IEEE ENG MED BIO, P286, DOI 10.1109/EMBC46164.2021.9630559
   Potes C, 2016, COMPUT CARDIOL CONF, V43, P621
   Schmidt SE, 2015, IEEE T BIO-MED ENG, V62, P2611, DOI 10.1109/TBME.2015.2432129
   Sh-Hussain H, 2017, J MED IMAG HEALTH IN, V7, P755, DOI 10.1166/jmihi.2017.2079
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Stasis AC, 2003, ENG MED BIOL SOC ANN, P354
   Thomae C, 2016, COMPUT CARDIOL CONF, V43, P625
   Tschannen M, 2016, COMPUT CARDIOL CONF, V43, P565
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang P, 2007, ANN BIOMED ENG, V35, P367, DOI 10.1007/s10439-006-9232-3
   Yaseen, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8122344
   Zabihi M, 2016, COMPUT CARDIOL CONF, V43, P613, DOI 10.22489/cinc.2016.180-213
   Zeng Y., 2022, J COMMUN MED PUB HLT, V3, P1
   Zhang D., 2019, Fundamentals of Image Data Mining, Texts in Computer, P35, DOI [DOI 10.1007/978-3-030-17989-2_3, 10.1007/978-3-030-17989-23]
   Zheng YN, 2015, COMPUT METH PROG BIO, V122, P372, DOI 10.1016/j.cmpb.2015.09.001
NR 34
TC 2
Z9 2
U1 13
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3811
EP 3822
DI 10.1007/s00371-023-03031-5
EA JUL 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001032577400002
OA Bronze
DA 2024-07-18
ER

PT J
AU Li, SF
   Wu, FZ
   Fan, YQ
   Song, X
   Dong, WM
AF Li, Sifei
   Wu, Fuzhang
   Fan, Yuqing
   Song, Xue
   Dong, Weiming
TI PLDGAN: portrait line drawing generation with prior knowledge and
   conditioning target
SO VISUAL COMPUTER
LA English
DT Article
DE Portrait line drawings; Artistic stylization; Conditional generative
   adversarial networks
ID IMAGE TRANSLATION; EDGE-DETECTION; RESOLUTION
AB Line drawing, a form of minimalist art, is highly abstract and expressive with practical use in conveying 3D shapes and indicating object occlusion. Generating line drawings from photographs is a challenging task that requires the compression of rich texture information into sparse geometric elements, such as lines, curves, and circles, without compromising semantic information. Furthermore, a portrait line drawing should include a full human silhouette and important semantic lines of scenes while avoiding messy lines. To address those challenges, we propose a novel method for generating portrait line drawings, named PLDGAN (Portrait Line Drawing Generative Adversarial Network), which utilizes prior knowledge of pose and semantic segmentation information. We also design a conditioning target and adjust the content loss to the original target loss. To train our PLDGAN, we collect a new dataset containing pairwise portrait images and professional portrait line drawings. Our experiments show that our proposed method achieves state-of-the-art performance and can generate high-quality portrait line drawings.
C1 [Li, Sifei; Dong, Weiming] Chinese Acad Sci, Inst Automat, MAIS, Beijing, Peoples R China.
   [Li, Sifei; Dong, Weiming] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Wu, Fuzhang; Fan, Yuqing] Chinese Acad Sci, Inst Software, Beijing, Peoples R China.
   [Song, Xue] Zhengzhou Univ, Zhengzhou, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Chinese Academy of Sciences; Institute of Software, CAS; Zhengzhou
   University
RP Wu, FZ (corresponding author), Chinese Acad Sci, Inst Software, Beijing, Peoples R China.
EM lisifei2022@ia.ac.cn; wfz275@126.com; fanyuqing@iscas.ac.cn;
   202022592017673@gs.zzu.edu.cn; weiming.dong@ia.ac.cn
RI CAO, ying/KFA-2972-2024; xie, jing/KDO-9486-2024; YANG,
   DAN/KCL-5217-2024; DONG, Weiming/AAG-7678-2020; Liu, Chang/KGL-6678-2024
OI DONG, Weiming/0000-0001-6502-145X; 
FU National Key Ramp;D Program of China [2020AAA0106200]; National
   Natural~Science Foundation of China [61832016, U20B2070]; Beijing
   Natural Science Foundation [L221013]
FX This work was supported in part by National Key R&D Program of China
   under no. 2020AAA0106200, by National Natural & nbsp;Science Foundation
   of China under nos. 61832016 and U20B2070, andin part by Beijing Natural
   Science Foundation under no. L221013.
CR [Anonymous], 1983, Finding Edges and Lines in Images
   Bhunia AK, 2022, LECT NOTES COMPUT SC, V13677, P338, DOI 10.1007/978-3-031-19790-1_21
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chan C, 2022, PROC CVPR IEEE, P7905, DOI 10.1109/CVPR52688.2022.00776
   Chen YJ, 2022, LECT NOTES COMPUT SC, V13676, P440, DOI 10.1007/978-3-031-19787-1_25
   DAUGMAN JG, 1985, J OPT SOC AM A, V2, P1160, DOI 10.1364/JOSAA.2.001160
   Deng YY, 2021, AAAI CONF ARTIF INTE, V35, P1210
   Deng Y, 2022, IEEE I C VI COM I PR, DOI 10.1109/VCIP56404.2022.10008827
   Dollár P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Huang JL, 2022, IEEE T MULTIMEDIA, V24, P1435, DOI 10.1109/TMM.2021.3065230
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Karras Tero, 2020, IEEE C COMP VIS PATT
   Kingma D. P., 2014, arXiv
   Li MT, 2019, IEEE WINT CONF APPL, P1403, DOI 10.1109/WACV.2019.00154
   Liu F, 2019, PROC CVPR IEEE, P5823, DOI 10.1109/CVPR.2019.00598
   Mairal J, 2008, LECT NOTES COMPUT SC, V5304, P43, DOI 10.1007/978-3-540-88690-7_4
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Radford A, 2021, PR MACH LEARN RES, V139
   Ribeiro Leo Sampaio Ferraz, 2020, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, P14153
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saharia C., 2022, 36 C NEURAL INFORM P
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Soria X, 2023, PATTERN RECOGN, V139, DOI 10.1016/j.patcog.2023.109461
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinker Y, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530068
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Xiaofeng R., 2012, ADV NEURAL INFORM PR
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Yi R, 2023, IEEE T PATTERN ANAL, V45, P905, DOI 10.1109/TPAMI.2022.3147570
   Yu J, 2021, IEEE T CYBERNETICS, V51, P4350, DOI 10.1109/TCYB.2020.2972944
   Zhan FN, 2022, LECT NOTES COMPUT SC, V13676, P224, DOI 10.1007/978-3-031-19787-1_13
   Zhang XQ, 2023, IEEE T IND INFORM, V19, P3144, DOI 10.1109/TII.2022.3160705
   Zhang Y., 2023, IEEECVF C COMPUTER V
   Zhang Yuxin, 2022, ACM SIGGRAPH 2022 C
   Zhou X., 2021, IEEE CVF C COMP VIS, p11,465
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu MR, 2019, IEEE T NEUR NET LEAR, V30, P3096, DOI 10.1109/TNNLS.2018.2890018
NR 39
TC 3
Z9 3
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3507
EP 3518
DI 10.1007/s00371-023-02956-1
EA JUN 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001019834000001
DA 2024-07-18
ER

PT J
AU Sivri, TT
   Sahillioglu, Y
AF Sivri, Talya Tumer
   Sahillioglu, Yusuf
TI A data-centric unsupervised 3D mesh segmentation method
SO VISUAL COMPUTER
LA English
DT Article
DE 3D mesh segmentation; Unsupervised learning; Embedding; Node2vec;
   K-Means; Geodesic distance
ID GRAPH
AB In this paper, a novel data-centric approach is proposed for solving the 3D mesh segmentation problem. The method uses node2vec, a semi-supervised learning algorithm, to create vector embedding representations for each node in a 3D mesh graph. This makes the mesh data more compact and easier to process which is important for reducing computation costs. K-Means clustering is then used to cluster each node according to their node embedding information. This data-centric approach is more computationally efficient than other complex models such as CNN and RNN. The main contribution of this study is the development of a data-centric AI framework that combines node2vec embedding, machine learning, and deep learning techniques. The use of cosine similarity is also adapted to compare and evaluate the trained node embedding vectors with different hyperparameters. Additionally, a new algorithm is developed to determine the optimal cluster number using geodesic distance on the 3D mesh. Overall, this approach provides competitive results compared to existing mesh segmentation methods.
C1 [Sivri, Talya Tumer] METU, Multimedia Informat Dept, Ankara, Turkiye.
   [Sahillioglu, Yusuf] METU, Comp Engn Dept, Ankara, Turkiye.
C3 Middle East Technical University; Middle East Technical University
RP Sahillioglu, Y (corresponding author), METU, Comp Engn Dept, Ankara, Turkiye.
EM tumer.talya@metu.edu.tr; ys@ceng.metu.edu.tr
FU TUBITAK [EEEAG-119E572]
FX AcknowledgementsThis work was supported by TUBITAK under the project
   EEEAG-119E572.
CR Abbasi A, 2019, VISUAL COMPUT, V35, P271, DOI 10.1007/s00371-018-1586-7
   Ahmed A., 2013, P 22 INT C WORLD WID, P37, DOI [10.1145/2488388.2488393, DOI 10.1145/2488388.2488393]
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Cao S., 2015, P 24 ACM INT C INFOR, P891
   Cao SS, 2016, AAAI CONF ARTIF INTE, P1145
   Chen H., 2017, ARXIV
   Dong Qiujie, 2024, IEEE Trans Vis Comput Graph, V30, P4349, DOI 10.1109/TVCG.2023.3259044
   Fey M., PYG
   Fouss F, 2007, IEEE T KNOWL DATA EN, V19, P355, DOI 10.1109/TKDE.2007.46
   Goyal P, 2018, KNOWL-BASED SYST, V151, P78, DOI 10.1016/j.knosys.2018.03.022
   Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754
   Hanocka R, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3267347
   Hogg M., 2021, PYGEODESIC
   Jiao X, 2023, COMPUT AIDED DESIGN, V160, DOI 10.1016/j.cad.2023.103512
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   Khattab D, 2017, ADV INTELL SYST, V533, P598, DOI 10.1007/978-3-319-48308-5_57
   Kingma D. P., 2014, arXiv
   Kipf T. N., 2016, ARXIV161107308, V1050, P21
   Kipf TN, 2016, ARXIV
   Lahav A, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417806
   Lai YK, 2008, SPM 2008: PROCEEDINGS OF THE ACM SOLID AND PHYSICAL MODELING SYMPOSIUM, P183
   Luo D., 2011, Proceedings of the 28th International Conference on Machine Learning, P553
   Lv JJ, 2012, COMPUT GRAPH FORUM, V31, P2241, DOI 10.1111/j.1467-8659.2012.03217.x
   MacQueen J., 1967, P 5 BERK S MATH STAT, P281
   Mikolov Tomas, 2013, EFFICIENT ESTIMATION
   Newman MEJ, 2005, SOC NETWORKS, V27, P39, DOI 10.1016/j.socnet.2004.11.009
   Perozzi B., 2016, ARXIV
   Perozzi B, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P701, DOI 10.1145/2623330.2623732
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Sever O. I., 2020, THESIS MIDDLE E TU
   Shu ZY, 2022, COMPUT AIDED DESIGN, V145, DOI 10.1016/j.cad.2021.103181
   Shu ZY, 2020, IEEE T VIS COMPUT GR, V26, P2671, DOI 10.1109/TVCG.2019.2892076
   Sidi O, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024160
   Verdecchia R, 2022, 2022 INTERNATIONAL CONFERENCE ON ICT FOR SUSTAINABILITY (ICT4S 2022), P35, DOI 10.1109/ICT4S55073.2022.00015
   Wang DX, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1225, DOI 10.1145/2939672.2939753
   Wang YH, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366184
   Wu ZZ, 2013, COMPUT GRAPH-UK, V37, P628, DOI 10.1016/j.cag.2013.05.015
NR 41
TC 0
Z9 0
U1 3
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2237
EP 2249
DI 10.1007/s00371-023-02913-y
EA JUN 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001005826400002
DA 2024-07-18
ER

PT J
AU delacalle, FJ
   García, DF
   Usamentiaga, R
   Nuño, P
   Magadán, L
AF delacalle, F. J.
   Garcia, D. F.
   Usamentiaga, R.
   Nuno, P.
   Magadan, L.
TI Hierarchical registration method for surface quality inspection of long
   products
SO VISUAL COMPUTER
LA English
DT Article
DE Computer vision; ICP; Surface inspection; Point cloud registration
AB Manufacturing industry often uses 3D scanning technologies to inspect their products. Some of these techniques produce a point cloud that represent a section of the manufactured product. The clouds must be aligned to the model of the product in order to check its quality. Current registration methods are usually affected by dimensional problems or volumetric anomalies. This paper proposes a new method for the registration process aligning the cloud to the model in several steps. The first step is the state-of-the-art method. The second step uses the information acquired in the first one to perform a fine registration in order to not being affected by dimensional defects or little miss alignments in the previous step providing a huge improvement in the measurement of surface defects. In this paper, several techniques are proposed in order to provide a set of tools that the final users can tune to fit their needs. The whole procedure of registration can be run in real-time conditions using the sampling and caching strategies proposed. The methods proposed are tested over more than 10,000 profiles of a rail proving they successfully align the cloud and the model providing better results in the measurement of surface defects.
C1 [delacalle, F. J.; Garcia, D. F.; Usamentiaga, R.; Nuno, P.; Magadan, L.] Univ Oviedo, Comp Sci & Engn, Gijon 33205, Asturias, Spain.
C3 University of Oviedo
RP delacalle, FJ (corresponding author), Univ Oviedo, Comp Sci & Engn, Gijon 33205, Asturias, Spain.
EM delacalle@uniovi.es; dfgarcia@uniovi.es.es; rusamentiaga@uniovi.es;
   nunopelayo@uniovi.es; magadanluis@uniovi.es
RI Nuño, Pelayo/KIL-6014-2024; Usamentiaga, Ruben/D-9649-2013
OI Nuño, Pelayo/0000-0002-8599-3077; Usamentiaga,
   Ruben/0000-0003-0551-3203; de la Calle Herrero, Francisco
   Javier/0000-0002-5546-2005
FU Spanish State Plan for Scientific and Technical Research and Innovation
   [MCIU-22PID2021-124383OB-I00];  [2021-2023]
FX This work was funded by the project MCIU-22PID2021-124383OB-I00 of the
   Spanish State Plan for Scientific and Technical Research and Innovation
   2021-2023.
CR [Anonymous], 2001, Standards Australia: 1085.1-2002 Railway track material. Part 1 steel rails
   [Anonymous], 2017, 1367412011A12017 INT
   Bernal PM, 2017, IEEE T IND APPL, V53, P4116, DOI 10.1109/TIA.2017.2676092
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   delaCalle FJ, 2020, J REAL-TIME IMAGE PR, V17, P967, DOI 10.1007/s11554-018-0844-2
   delaCalle FJ, 2021, IEEE T IND APPL, V57, P2994, DOI 10.1109/TIA.2021.3059605
   Garrett T, 2016, INT C PATT RECOG, P3085, DOI 10.1109/ICPR.2016.7900108
   Gautier Q., 2014, Real-time 3d reconstruction for fpgas: a case study for evaluating the performance, area, and programmability trade-offs of the altera opencl sdk, DOI [10.13140/RG.2.1.4950.4168, DOI 10.13140/RG.2.1.4950.4168]
   Herrero FJD, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20072142
   Hu L, 2020, VISUAL COMPUT, V36, P669, DOI 10.1007/s00371-019-01648-z
   Ikeda O, 2008, IEEE WORK APP COMP, P153
   Kehtarnavaz N., 2006, Real-Time Image and Video Processing: From Research to Reality, DOI 10.2200/S00021ED1V01Y200604IVM005
   Li YF, 2022, IEEE T INTELL TRANSP, V23, P20098, DOI 10.1109/TITS.2022.3177860
   Molleda J., 2015, IEEE IND APPL SOC AN, V2015, P1
   Santur Y., 2017, NEW RAIL INSPECTION, P1, DOI [10.1109/IDAP.2017.8090245, DOI 10.1109/IDAP.2017.8090245]
   Secil S, 2014, 2014 IEEE INTERNATIONAL SYMPOSIUM ON ROBOTICS AND MANUFACTURING AUTOMATION (ROMA), P160, DOI 10.1109/ROMA.2014.7295880
   Sharif MM, 2022, AUTOMAT CONSTR, V133, DOI 10.1016/j.autcon.2021.103998
   Sharifzadeh S, 2018, MECHATRONICS, V51, P59, DOI 10.1016/j.mechatronics.2018.03.001
   Usamentiaga R, 2018, IEEE T IND APPL, V54, P2955, DOI 10.1109/TIA.2018.2795562
   Xiong ZM, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17081791
   Yang JL, 2016, IEEE T PATTERN ANAL, V38, P2241, DOI 10.1109/TPAMI.2015.2513405
   Yang LJ, 2023, VISUAL COMPUT, V39, P367, DOI 10.1007/s00371-021-02335-8
   Ye C, 2018, ENG STRUCT, V173, P530, DOI 10.1016/j.engstruct.2018.06.094
   Zeng A, 2017, PROC CVPR IEEE, P199, DOI 10.1109/CVPR.2017.29
   Zhang JY, 2022, IEEE T PATTERN ANAL, V44, P3450, DOI 10.1109/TPAMI.2021.3054619
NR 25
TC 0
Z9 0
U1 4
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 1179
EP 1192
DI 10.1007/s00371-023-02839-5
EA APR 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000969690500001
OA hybrid
DA 2024-07-18
ER

PT J
AU Li, CL
   Zhang, Y
   Li, HD
   Zhou, Y
AF Li, Chunlai
   Zhang, Yan
   Li, Haodong
   Zhou, Yang
TI Visual image encryption scheme based on inter-intra-block scrambling and
   weighted diffusion
SO VISUAL COMPUTER
LA English
DT Article
DE Inter-intra-block scrambling; Weighted diffusion; Robust hyper-chaos;
   Visual encryption
ID ALGORITHM; COMPRESSION
AB This paper presents an image encryption scheme with data and appearance security, by adopting inter-intra-block scrambling and weighted diffusion. The 2Drobust hyper-chaotic mapwith flexible geometric distribution and rich hyper-chaotic parameter space is employed to generate the key stream for encryption, by considering the characteristics of plaintext. The plain image is first preprocessed by Huffman coding for getting compressed image. Then, the compressed image is divided into four sub-blocks and is further permuted by the designed inter-intra-block scrambling scheme, which can improve the scrambling effect by making the pixel far away from the original adjacent pixels. After that, a weighted diffusion method strongly related to plaintext and key stream is introduced to diffuse the shuffled image to obtain the noise-like cipher image. And in pursuit of higher security, the meaningless noise-like image is embedded into host image to create the visually meaningful cipher image. A series of experiment tests and analyses are carried out to further demonstrate the excellent performances of the encryption scheme.
C1 [Li, Chunlai; Zhang, Yan; Li, Haodong; Zhou, Yang] Xiangtan Univ, Sch Comp Sci, Xiangtan 411105, Peoples R China.
   [Li, Chunlai; Zhang, Yan; Li, Haodong; Zhou, Yang] Xiangtan Univ, Sch Cyberspace Sci, Xiangtan 411105, Peoples R China.
   [Li, Haodong; Zhou, Yang] Hunan Inst Sci & Technol, Key Lab Hunan Prov Informat Photon & Freespace Op, Yueyang 414006, Peoples R China.
C3 Xiangtan University; Xiangtan University; Hunan Institute of Science &
   Technology
RP Li, CL (corresponding author), Xiangtan Univ, Sch Comp Sci, Xiangtan 411105, Peoples R China.; Li, CL (corresponding author), Xiangtan Univ, Sch Cyberspace Sci, Xiangtan 411105, Peoples R China.
EM lichunlai33@126.com
OI LI, ChunLai/0000-0001-6329-7719
FU Science and Technology Program of Hunan Province [2019TP1014]; Science
   and Research Creative Team of Hunan Institute of Science and Technology
   [2019-TD-10]
FX This study was funded by Science and Technology Program of Hunan
   Province (Grant No. 2019TP1014); Science and Research Creative Team of
   Hunan Institute of Science and Technology (Grant Number 2019-TD-10).
CR Annaby MH, 2018, OPT LASER ENG, V103, P9, DOI 10.1016/j.optlaseng.2017.11.005
   Arroyo D, 2008, CHAOS, V18, DOI 10.1063/1.2959102
   Bao L, 2015, INFORM SCIENCES, V324, P197, DOI 10.1016/j.ins.2015.06.049
   Bhanot R, 2015, INT J SECUR APPL, V9, P289, DOI 10.14257/ijsia.2015.9.4.27
   Borges EP, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.254103
   Brindha M, 2016, APPL SOFT COMPUT, V40, P379, DOI 10.1016/j.asoc.2015.09.055
   Cao C, 2018, SIGNAL PROCESS, V143, P122, DOI 10.1016/j.sigpro.2017.08.020
   Chai XL, 2017, SIGNAL PROCESS, V134, P35, DOI 10.1016/j.sigpro.2016.11.016
   Chen GR, 2004, CHAOS SOLITON FRACT, V21, P749, DOI 10.1016/j.chaos.2003.12.022
   Chen SJ, 2020, IEEE SIGNAL PROC LET, V27, P1680, DOI 10.1109/LSP.2020.3025128
   Fang PF, 2023, VISUAL COMPUT, V39, P1975, DOI 10.1007/s00371-022-02459-5
   Feng W, 2019, EUR PHYS J-SPEC TOP, V228, P1951, DOI 10.1140/epjst/e2019-800209-3
   Fu C, 2011, OPT COMMUN, V284, P5415, DOI 10.1016/j.optcom.2011.08.013
   Hanis S, 2018, MULTIMED TOOLS APPL, V77, P6897, DOI 10.1007/s11042-017-4606-0
   Hanis S, 2019, NONLINEAR DYNAM, V95, P421, DOI 10.1007/s11071-018-4573-7
   Hu J, 2022, IEEE T CIRC SYST VID, V32, P1089, DOI 10.1109/TCSVT.2021.3074259
   Huo DM, 2021, OPT COMMUN, V492, DOI 10.1016/j.optcom.2021.126976
   Kanso A, 2017, OPT LASER ENG, V90, P196, DOI 10.1016/j.optlaseng.2016.10.009
   Kerckhoffs Auguste., 1978, La cryptographie militaire
   Kumar R, 2019, OPT LASER ENG, V120, P118, DOI 10.1016/j.optlaseng.2019.03.024
   LEVINS R, 1966, AM SCI, V54, P421
   Li CQ, 2022, IEEE T COMPUT, V71, P364, DOI 10.1109/TC.2021.3051387
   Li CL, 2021, MULTIMED TOOLS APPL, V80, P18479, DOI 10.1007/s11042-021-10631-7
   Li CL, 2019, AEU-INT J ELECTRON C, V110, DOI 10.1016/j.aeue.2019.152861
   Li CL, 2018, OPTIK, V171, P277, DOI 10.1016/j.ijleo.2018.06.029
   Li CL, 2021, EUR PHYS J-SPEC TOP, V230, P1959, DOI 10.1140/epjs/s11734-021-00182-1
   Li HD, 2023, NONLINEAR DYNAM, V111, P2895, DOI 10.1007/s11071-022-07955-w
   Liao Q, 2021, PHYS REV A, V103, DOI 10.1103/PhysRevA.103.032410
   Liao X, 2022, IEEE T DEPEND SECURE, V19, P897, DOI 10.1109/TDSC.2020.3004708
   Liu HJ, 2020, INT J BIFURCAT CHAOS, V30, DOI 10.1142/S0218127420501734
   Liu WH, 2016, OPT LASER ENG, V84, P26, DOI 10.1016/j.optlaseng.2016.03.019
   Luo YL, 2020, OPT LASER ENG, V124, DOI 10.1016/j.optlaseng.2019.105836
   Ma YL, 2020, J INF SECUR APPL, V54, DOI 10.1016/j.jisa.2020.102566
   Mou J, 2021, MOBILE NETW APPL, V26, P1849, DOI 10.1007/s11036-019-01293-9
   Pak C, 2019, MULTIMED TOOLS APPL, V78, P12027, DOI 10.1007/s11042-018-6739-1
   Peng F, 2021, IEEE T CIRC SYST VID, V31, P4538, DOI 10.1109/TCSVT.2021.3052468
   Ponuma R, 2019, MULTIMED TOOLS APPL, V78, P25707, DOI 10.1007/s11042-019-07808-6
   Tan F, 2022, NEUROCOMPUTING, V493, P129, DOI 10.1016/j.neucom.2022.04.054
   Wang XY, 2019, NONLINEAR DYNAM, V96, P161, DOI 10.1007/s11071-019-04781-5
   Wang XY, 2018, IEEE ACCESS, V6, P23733, DOI 10.1109/ACCESS.2018.2805847
   Wu Y, 2011, Cyber J Multidiscip J Sci Technol J Sel Areas Telecommun (JSAT), V1, P31
   Wu Y, 2014, INFORM SCIENCES, V264, P317, DOI 10.1016/j.ins.2013.11.027
   Wu Y, 2012, J ELECTRON IMAGING, V21, DOI 10.1117/1.JEI.21.1.013014
   Xu J, 2022, VISUAL COMPUT, V38, P1509, DOI 10.1007/s00371-021-02085-7
   Yang FF, 2020, OPT LASER ENG, V129, DOI 10.1016/j.optlaseng.2020.106031
   Yang FF, 2020, SIGNAL PROCESS, V169, DOI 10.1016/j.sigpro.2019.107373
   Yang YG, 2021, INFORM SCIENCES, V562, P304, DOI 10.1016/j.ins.2021.01.041
   Yao H, 2021, INFORM SCIENCES, V563, P130, DOI 10.1016/j.ins.2021.02.015
   Ye GD., 2021, SECUR COMMUN NETW, V2021, P1
   Ye GD, 2010, PATTERN RECOGN LETT, V31, P347, DOI 10.1016/j.patrec.2009.11.008
   Zhou MJ, 2020, SIGNAL PROCESS, V171, DOI 10.1016/j.sigpro.2020.107484
   Zhou Y, 2021, NONLINEAR DYNAM, V103, P2043, DOI 10.1007/s11071-021-06206-8
   Zhou YC, 2014, SIGNAL PROCESS, V97, P172, DOI 10.1016/j.sigpro.2013.10.034
   Zhou YC, 2013, SIGNAL PROCESS, V93, P3039, DOI 10.1016/j.sigpro.2013.04.021
   Zhu HG, 2019, IEEE ACCESS, V7, P14081, DOI 10.1109/ACCESS.2019.2893538
   Zhu LY, 2020, SIGNAL PROCESS, V175, DOI 10.1016/j.sigpro.2020.107629
NR 56
TC 11
Z9 11
U1 16
U2 53
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 731
EP 746
DI 10.1007/s00371-023-02812-2
EA MAR 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000946890400001
DA 2024-07-18
ER

PT J
AU Zou, KF
   Faisan, S
   Heitz, F
   Epain, M
   Croisille, P
   Fanton, L
   Valette, S
AF Zou, Kaifeng
   Faisan, Sylvain
   Heitz, Fabrice
   Epain, Marie
   Croisille, Pierre
   Fanton, Laurent
   Valette, Sebastien
TI Disentangled representations: towards interpretation of sex
   determination from hip bone
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Pattern recognition and classification; Shape analysis; Neural Network;
   Bone
AB Neural network-based classification methods are often criticized for their lack of interpretability and explainability. By highlighting the regions of the input image that contribute the most to the decision, saliency maps have become a popular method to make neural networks interpretable. In medical imaging, they are particularly well-suited for explaining neural networks in the context of abnormality localization. Nevertheless, they seem less suitable for classification problems in which the features that allow distinguishing classes are spatially correlated and scattered. We propose here a novel paradigm based on Disentangled Variational Auto-Encoders. Instead of seeking to understand what the neural network has learned or how prediction is done, we seek to reveal class differences. This is achieved by transforming the sample from a given class into the "same" sample but belonging to another class, thus paving the way to easier interpretation of class differences. Our experiments in the context of automatic sex determination from hip bones show that the obtained results are consistent with expert knowledge. Moreover, the proposed approach enables us to confirm or question the choice of the classifier or eventually to doubt it.
C1 [Zou, Kaifeng; Faisan, Sylvain; Heitz, Fabrice] Univ Strasbourg, CNRS, ICube, BP 10413, 300 Bd Sebastien Brant, F-67412 Illkirch Graffenstaden, France.
   [Epain, Marie; Fanton, Laurent] Hosp Civils Lyon, 3 Quai Celestins, F-69002 Lyon, France.
   [Croisille, Pierre] Univ Hosp St Etienne, 25 Bd Pasteur, F-42100 St Etienne, France.
   [Croisille, Pierre; Fanton, Laurent; Valette, Sebastien] Univ Lyon, Univ Claude Bernard Lyon 1, CREATIS, INSA Lyon,CNRS,Inserm,U1294,UMR 5220, F-69621 Lyon, France.
C3 Universites de Strasbourg Etablissements Associes; Universite de
   Strasbourg; Centre National de la Recherche Scientifique (CNRS); CHU
   Lyon; CHU de St Etienne; Centre National de la Recherche Scientifique
   (CNRS); CNRS - Institute for Engineering & Systems Sciences (INSIS);
   Institut National des Sciences Appliquees de Lyon - INSA Lyon; Institut
   National de la Sante et de la Recherche Medicale (Inserm); Universite
   Claude Bernard Lyon 1
RP Valette, S (corresponding author), Univ Lyon, Univ Claude Bernard Lyon 1, CREATIS, INSA Lyon,CNRS,Inserm,U1294,UMR 5220, F-69621 Lyon, France.
EM sebastien.valette@creatis.insa-lyon.fr
RI Fanton, Laurent/AAM-4170-2020; Croisille, Pierre/H-4928-2014; Fanton,
   Laurent/K-5476-2014; Valette, Sebastien/H-4195-2014
OI Croisille, Pierre/0000-0003-4019-3460; Fanton,
   Laurent/0000-0002-8849-1187; Valette, Sebastien/0000-0001-7549-4808
FU French National Research Agency (ANR) [ANR-19-CE45-0015]; Agence
   Nationale de la Recherche (ANR) [ANR-19-CE45-0015] Funding Source:
   Agence Nationale de la Recherche (ANR)
FX This work was funded by the TOPACS ANR-19-CE45-0015 project of the
   French National Research Agency (ANR).
CR Adebayo J, 2018, ADV NEUR IN, V31
   Agier R, 2020, MED IMAGE ANAL, V59, DOI 10.1016/j.media.2019.101564
   Arun N. T., 2020, MED IMAGING DEEP LEA
   Azizi V, 2022, VISUAL COMPUT, V38, P2785, DOI 10.1007/s00371-021-02155-w
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bruzek J, 2017, AM J PHYS ANTHROPOL, V164, P440, DOI 10.1002/ajpa.23282
   Chen Ricky T. Q., 2018, Advances in neural information processing systems, V31, DOI DOI 10.5555/3327757.3327764
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Coelho JD, 2019, FORENSIC SCI INT, V302, DOI 10.1016/j.forsciint.2019.109873
   Eitel F, 2020, LECT NOTES COMPUT SC, V11797, P3, DOI 10.1007/978-3-030-33850-3_1
   Erhan Dumitru, 2009, VISUALIZING HIGHER L, V1341, P1
   Fong R, 2019, IEEE I CONF COMP VIS, P2950, DOI 10.1109/ICCV.2019.00304
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Joy T., 2020, INT C LEARNING REPRE
   Ju YX, 2021, VISUAL COMPUT, V37, P2907, DOI 10.1007/s00371-021-02198-z
   Kingma DP, 2014, ADV NEUR IN, V27
   Kingma Diederik P, 2014, INT C LEARNING REPRE
   Komar DebraA., 2008, Forensic Anthropology: Contemporary Theory and Practice
   Lample G, 2017, ADV NEUR IN, V30
   Li YY, 2023, VISUAL COMPUT, V39, P2223, DOI 10.1007/s00371-021-02328-7
   Liu XP, 2022, VISUAL COMPUT, V38, P669, DOI 10.1007/s00371-020-02042-w
   Locatello F, 2019, PR MACH LEARN RES, V97
   Maaloe L, 2016, PR MACH LEARN RES, V48
   Murail P., 2005, Bulletins et Memoires de la Societe d'Anthropologie de Paris, V17, P167, DOI [10.4000/bmsap.1157, DOI 10.4000/BMSAP.1157]
   Nguyen A., 2019, Understanding Neural Networks via Feature Visualization: A Survey, P55
   Nikita E, 2020, INT J LEGAL MED, V134, P1213, DOI 10.1007/s00414-019-02148-4
   Nozawa N, 2022, VISUAL COMPUT, V38, P1317, DOI 10.1007/s00371-020-02024-y
   Perarnau G., 2016, NIPS WORKSHOP ADVERS
   Phaphuangwittayakul A, 2023, VISUAL COMPUT, V39, P4015, DOI 10.1007/s00371-022-02566-3
   Ran Liu, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12265), P257, DOI 10.1007/978-3-030-59722-1_25
   Ranjan A, 2018, LECT NOTES COMPUT SC, V11207, P725, DOI 10.1007/978-3-030-01219-9_43
   Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778
   Ruiz A., 2019, LEARNING DISENTANGLE
   Rybkin O., 2021, P INT C MACH LEARN I, V139, P9179
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Shen W, 2017, PROC CVPR IEEE, P1225, DOI 10.1109/CVPR.2017.135
   Siddharth N, 2017, ADV NEUR IN, V30
   Simonyan Karen, 2014, WORKSH P INT C LEARN
   Smilkov D., 2017, WORKSHOP VISUALIZATI
   Wang Q, 2020, VISUAL COMPUT, V36, P141, DOI 10.1007/s00371-018-1594-7
   Wang S, 2022, VISUAL COMPUT, V38, P2539, DOI 10.1007/s00371-021-02129-y
   Wen JH, 2020, VISUAL COMPUT, V36, P1385, DOI 10.1007/s00371-019-01738-y
   Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Yin ZX, 2023, VISUAL COMPUT, V39, P4423, DOI 10.1007/s00371-022-02599-8
   Yoshikawa T, 2022, VISUAL COMPUT, V38, P3121, DOI 10.1007/s00371-022-02538-7
   Young K, 2019, LECT NOTES COMPUTER
   Zhang YZ, 2022, VISUAL COMPUT, V38, P1209, DOI 10.1007/s00371-021-02142-1
   Zhao QY, 2019, LECT NOTES COMPUT SC, V11765, P823, DOI 10.1007/978-3-030-32245-8_91
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zou K., 2022, IEEE INT C IMAGE PRO
NR 52
TC 1
Z9 1
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 JAN 4
PY 2023
DI 10.1007/s00371-022-02755-0
EA JAN 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7O1BA
UT WOS:000907765600001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liang, MY
   Zhang, QN
   Wang, GG
   Xu, N
   Wang, L
   Liu, HS
   Zhang, CL
AF Liang, Meiyan
   Zhang, Qiannan
   Wang, Guogang
   Xu, Na
   Wang, Lin
   Liu, Haishun
   Zhang, Cunlin
TI Multi-scale self-attention generative adversarial network for pathology
   image restoration
SO VISUAL COMPUTER
LA English
DT Article
DE Multi-scale; Self-attention; Generative adversarial network;
   Pathological image restoration
AB High-quality histopathology images are significant for accurate diagnosis and symptomatic treatment. However, local cross-contamination or missing data are common phenomena due to many factors, such as the superposition of foreign bodies and improper operations in obtaining and processing pathological digital images. The interpretation of such images is time-consuming, laborious, and inaccurate. Thus, it is necessary to improve diagnosis accuracy by reconstructing pathological images. However, corrupted image restoration is a challenging task, especially for pathological images. Therefore, we propose a multi-scale self-attention generative adversarial network (MSSA GAN) to restore colon tissue pathological images. The MSSA GAN uses a self-attention mechanism in the generator to efficiently learn the correlations between the corrupted and uncorrupted areas at multiple scales. After jointly optimizing the loss function and understanding the semantic features of pathology images, the network guides the generator in these scales to generate restored pathological images with precise details. The results demonstrated that the proposed method could obtain pixel-level photorealism for histopathology images. Parameters such as RMSE, PSNR, and SSIM of the restored image reached 2.094, 41.96 dB, and 0.9979, respectively. Qualitative and quantitative comparisons with other restoration approaches illustrate the superior performance of the improved algorithm for pathological image restoration.
C1 [Liang, Meiyan; Zhang, Qiannan; Wang, Guogang; Xu, Na] Shanxi Univ, Sch Phys & Elect Engn, Taiyuan 030006, Peoples R China.
   [Wang, Lin] Shanxi Med Univ, Shanxi Bethune Hosp, Tongji Shanxi Hosp, Shanxi Acad Med Sci,Hosp 3, Taiyuan 030032, Peoples R China.
   [Wang, Lin] Huazhong Univ Sci & Technol, Tongji Hosp, Tongji Med Coll, Wuhan 430030, Peoples R China.
   [Liu, Haishun; Zhang, Cunlin] Capital Normal Univ, Beijing Key Lab Terahertz Spect & Imaging, Key Lab Terahertz, Minist Educ,Optoelect, Beijing 100048, Peoples R China.
C3 Shanxi University; Shanxi Medical University; Huazhong University of
   Science & Technology; Capital Normal University
RP Liang, MY; Wang, GG (corresponding author), Shanxi Univ, Sch Phys & Elect Engn, Taiyuan 030006, Peoples R China.
EM meiyanliang@sxu.edu.cn; kingguogang@sxu.edu.cn
RI zhang, cl/JDW-6549-2023; Liang, Meiyan/HPH-6710-2023; LU,
   LU/JEZ-4760-2023
FU National Natural Science Foundation of China [11804209]; Natural Science
   Foundation of Shanxi Province [201901D111031]; Scientific and
   Technological Innovation Programs of Higher Education Institutions in
   Shanxi [2019 L0064]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 11804209, Natural Science Foundation of Shanxi
   Province under Grant 201901D211173, Scientific and Technological
   Innovation Programs of Higher Education Institutions in Shanxi under
   Grant 2019 L0064, and Natural Science Foundation of Shanxi Province
   under Grant 201901D111031.
CR Armanious K, 2019, INT CONF ACOUST SPEE, P3267, DOI 10.1109/icassp.2019.8682677
   Armanious K, 2020, IEEE IMAGE PROC, P3005, DOI [10.1109/icip40778.2020.9191207, 10.1109/ICIP40778.2020.9191207]
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Belli D, 2018, ARXIV
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Bi L, 2017, LECT NOTES COMPUT SC, V10555, P43, DOI 10.1007/978-3-319-67564-0_5
   Chan TF, 2001, J VIS COMMUN IMAGE R, V12, P436, DOI 10.1006/jvci.2001.0487
   Chang YL, 2019, IEEE COMPUT SOC CONF, P1785, DOI 10.1109/CVPRW.2019.00229
   Chen YZ, 2019, NEURAL PROCESS LETT, V49, P1355, DOI 10.1007/s11063-018-9877-6
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Deng Kangle, 2021, arXiv
   Dolhansky B, 2018, PROC CVPR IEEE, P7902, DOI 10.1109/CVPR.2018.00824
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hertz A, 2019, PROC CVPR IEEE, P6851, DOI 10.1109/CVPR.2019.00702
   Hui Z, 2020, ARXIV
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Jin KH, 2015, IEEE T IMAGE PROCESS, V24, P3498, DOI 10.1109/TIP.2015.2446943
   Kaushik H, 2021, IEEE ACCESS, V9, P108276, DOI 10.1109/ACCESS.2021.3101142
   Lei N, 2020, ENGINEERING-PRC, V6, P361, DOI 10.1016/j.eng.2019.09.010
   Li H., 2015, IEEE T CYBERN, V14, p4398 4411
   Li J., 2018, P MACHINE LEARNING R, V80, P3005
   Li X, 2020, INT INFORM TECHNOLOG
   Liang MY, 2020, IEEE ACCESS, V8, P208969, DOI 10.1109/ACCESS.2020.3038764
   Liao L, 2019, IEEE ACCESS, V7, P36921, DOI 10.1109/ACCESS.2019.2905268
   Liu F., 2020, P 28 INT C COMP LING, P3586
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu HY, 2019, IEEE I CONF COMP VIS, P4169, DOI 10.1109/ICCV.2019.00427
   Liu PJ, 2018, IEEE COMPUT SOC CONF, P886, DOI 10.1109/CVPRW.2018.00121
   Liu XF, 2021, LECT NOTES COMPUT SC, V12658, P80, DOI 10.1007/978-3-030-72084-1_8
   Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155
   Nazeri K., 2019, ARXIV
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Pimkin A, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9206625
   Qin Z, 2021, DISPLAYS, V69, DOI 10.1016/j.displa.2021.102028
   Ruzic T, 2015, IEEE T IMAGE PROCESS, V24, P444, DOI 10.1109/TIP.2014.2372479
   Quan TM, 2018, IEEE T MED IMAGING, V37, P1488, DOI 10.1109/TMI.2018.2820120
   Uddin SMN, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20113204
   Wei YQ, 2016, SIGNAL IMAGE VIDEO P, V10, P911, DOI 10.1007/s11760-015-0840-y
   Xue HY, 2017, IEEE T IMAGE PROCESS, V26, P4311, DOI 10.1109/TIP.2017.2718183
   Yang C, 2017, PROC CVPR IEEE, P4076, DOI 10.1109/CVPR.2017.434
   Yang G, 2018, IEEE T MED IMAGING, V37, P1310, DOI 10.1109/TMI.2017.2785879
   Yang YZ, 2022, VISUAL COMPUT, V38, P2647, DOI 10.1007/s00371-021-02143-0
   Ying H, 2017, PROCEDIA COMPUT SCI, V107, P796, DOI 10.1016/j.procs.2017.03.175
   Yu F., 2015, ARXIV
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zeng YH, 2019, PROC CVPR IEEE, P1486, DOI 10.1109/CVPR.2019.00158
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhao L, 2020, PROC CVPR IEEE, P5740, DOI 10.1109/CVPR42600.2020.00578
   Zheng CX, 2019, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2019.00153
NR 50
TC 5
Z9 5
U1 2
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 4305
EP 4321
DI 10.1007/s00371-022-02592-1
EA AUG 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000841133400001
DA 2024-07-18
ER

PT J
AU Ji, CJ
   Chen, YD
   Yang, ZX
   Wu, EH
AF Ji, Chuanjun
   Chen, Yadang
   Yang, Zhi-Xin
   Wu, Enhua
TI Spatio-temporal compression for semi-supervised video object
   segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Video object segmentation; External memory; Spatial-temporal redundancy;
   Memory reading
AB In this paper, we explore the spatial-temporal redundancy in video object segmentation (VOS) under semi-supervised context with the purpose to improve the computational efficiency. Recently, memory-based methods have attracted great attention for their excellent performance. These methods involve first constructing an external memory to store the target object information in the history frames and then selecting the information that is beneficial for modeling the target object by memory reading. However, such methods are inefficient and unable to achieve both high accuracy and high efficiency, due to the large amount of redundant information in memory. Moreover, they periodically sample historical frames and add them to memory; this operation may lose important information from dynamic frames with incremental object changing or aggravate temporal redundancy from static frames with no object changing. To address these problems, we propose an efficient semi-supervised VOS approach via spatio-temporal compression (termed as STCVOS). Specifically, we first adopt a temporally varying sensor to adaptively filter static frames with no target objects evolutions and trigger memory to update with frames containing noticeable variations. Furthermore, we propose a spatially compressed memory to absorb features with varied pixels and remove outdated features, which considerably reduces information redundancy. More importantly, we introduce an efficient memory reader to perform memory reading with less footprint and computational overhead. Experimental results indicate that STCVOS performs well against state-of-the-art methods on the DAVIS 2017 and YouTube-VOS datasets, with a J&F overall score of 82.0% and 79.7%, respectively. Meanwhile, STCVOS achieves a high inference speed of approximately 30 FPS.
C1 [Ji, Chuanjun; Chen, Yadang] Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing 210044, Peoples R China.
   [Ji, Chuanjun; Chen, Yadang] Nanjing Univ Informat Sci & Technol, Engn Res Ctr Digital Forens, Minist Educ, Nanjing 210044, Peoples R China.
   [Yang, Zhi-Xin] Univ Macau, State Key Lab Internet Things Smart City, Macau 999078, Peoples R China.
   [Yang, Zhi-Xin] Univ Macau, Dept Electromech Engn, Macau 999078, Peoples R China.
   [Wu, Enhua] Univ Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100190, Peoples R China.
C3 Nanjing University of Information Science & Technology; Nanjing
   University of Information Science & Technology; University of Macau;
   University of Macau; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS
RP Chen, YD (corresponding author), Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing 210044, Peoples R China.; Chen, YD (corresponding author), Nanjing Univ Informat Sci & Technol, Engn Res Ctr Digital Forens, Minist Educ, Nanjing 210044, Peoples R China.
EM cyd4511632@126.com
RI Yang, Zhi-Xin/AAV-1335-2020; Liu, xuefeng/IUP-1483-2023
OI Yang, Zhi-Xin/0000-0001-9151-7758; 
FU National Natural Science Foundation of China [62072449, 61802197,
   61632003]; Science and Technology Development Fund, Macau SAR
   [0018/2019/AKP, SKL-IOTSC(UM)-2021-2023]; Guangdong Science and
   Technology Department [2018B030324002]; Zhuhai Science and Technology
   Innovation Bureau Zhuhai-Hong Kong-Macau Special Cooperation Project
   [ZH22017002200001PWC]
FX This work was partially supported by the National Natural Science
   Foundation of China (Grant Nos.62072449,61802197,61632003) and is also
   funded in part by the Science and Technology Development Fund, Macau SAR
   (Grant no.0018/2019/AKP and SKL-IOTSC(UM)-2021-2023), in part by the
   Guangdong Science and Technology Department (Grant no.2018B030324002),
   in part by the Zhuhai Science and Technology Innovation Bureau
   Zhuhai-Hong Kong-Macau Special Cooperation Project (Grant no.
   ZH22017002200001PWC)
CR Bao LC, 2018, PROC CVPR IEEE, P5977, DOI 10.1109/CVPR.2018.00626
   Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565
   Chen YH, 2018, PROC CVPR IEEE, P1189, DOI 10.1109/CVPR.2018.00130
   Child Rewon, 2019, Generating long sequences with sparse transformers
   Cho S, 2022, IEEE WINT CONF APPL, P1453, DOI 10.1109/WACV51458.2022.00152
   Cunningham P, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3459665
   Gökstorp SGE, 2022, VISUAL COMPUT, V38, P2033, DOI 10.1007/s00371-021-02264-6
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu L, 2021, PROC CVPR IEEE, P4142, DOI 10.1109/CVPR46437.2021.00413
   Hu Y., 2018, ARXIV
   Huang ZH, 2022, VISUAL COMPUT, V38, P2739, DOI 10.1007/s00371-021-02150-1
   Johnander J, 2019, PROC CVPR IEEE, P8945, DOI 10.1109/CVPR.2019.00916
   Katharopoulos A, 2020, PR MACH LEARN RES, V119
   Khoreva A, 2019, INT J COMPUT VISION, V127, P1175, DOI 10.1007/s11263-019-01164-6
   Kitaev Nikita, 2020, INT C LEARN REPR
   Li R., 2020, ARXIV PREPRINT ARXIV
   Li X, 2018, ARXIV
   Li Y, ARXIV
   Li YR, 2019, VISUAL COMPUT, V35, P1143, DOI 10.1007/s00371-019-01692-9
   Liang Y., 2020, Adv. Neural Inf. Proces. Syst., V33, P3430
   Liu WD, 2021, IEEE T CIRC SYST VID, V31, P1607, DOI 10.1109/TCSVT.2020.3010293
   Lu X, ARXIV
   Luiten J., 2018, ARXIV
   Maninis KK, 2019, IEEE T PATTERN ANAL, V41, P1515, DOI 10.1109/TPAMI.2018.2838670
   Oh SW, 2019, IEEE I CONF COMP VIS, P9225, DOI 10.1109/iccv.2019.00932
   Oh SW, 2018, PROC CVPR IEEE, P7376, DOI 10.1109/CVPR.2018.00770
   Park H, 2021, PROC CVPR IEEE, P8401, DOI 10.1109/CVPR46437.2021.00830
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Perazzi F, 2017, PROC CVPR IEEE, P3491, DOI 10.1109/CVPR.2017.372
   Pont-Tuset J., 2017, ARXIV
   Seong H., ARXIV
   Seong H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12869, DOI 10.1109/ICCV48922.2021.01265
   Shen Z., 2020, EFFICIENT ATTENTION
   Tsai YH, 2016, PROC CVPR IEEE, P3899, DOI 10.1109/CVPR.2016.423
   Tschiedel M, 2022, VISUAL COMPUT, V38, P2635, DOI 10.1007/s00371-021-02138-x
   Voigtlaender P., 2017, ARXIV
   Voigtlaender P, 2019, PROC CVPR IEEE, P9473, DOI 10.1109/CVPR.2019.00971
   Wang HC, 2021, PROC CVPR IEEE, P1296, DOI 10.1109/CVPR46437.2021.00135
   Wang ZQ, 2019, IEEE I CONF COMP VIS, P3977, DOI 10.1109/ICCV.2019.00408
   Xiao HX, 2018, PROC CVPR IEEE, P1140, DOI 10.1109/CVPR.2018.00125
   Xie HZ, 2021, PROC CVPR IEEE, P1286, DOI 10.1109/CVPR46437.2021.00134
   Xu D, 2022, VISUAL COMPUT, V38, P4251, DOI 10.1007/s00371-021-02292-2
   Xu N., 2018, ARXIV
   Yang LJ, 2018, PROC CVPR IEEE, P6499, DOI 10.1109/CVPR.2018.00680
   Yang Ze, arXiv
   Zhang YZ, 2020, PROC CVPR IEEE, P6947, DOI 10.1109/CVPR42600.2020.00698
NR 46
TC 0
Z9 0
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4929
EP 4942
DI 10.1007/s00371-022-02638-4
EA AUG 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000840290600004
DA 2024-07-18
ER

PT J
AU Zhang, Q
   Ge, YL
   Zhang, C
   Bi, HB
AF Zhang, Qiao
   Ge, Yanliang
   Zhang, Cong
   Bi, Hongbo
TI TPRNet: camouflaged object detection via transformer-induced progressive
   refinement network
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Camouflaged object detection; Transformer; Progressive
   refinement
AB Camouflaged object detection (COD) is a challenging task which aims to detect objects similar to the surrounding environment. In this paper, we propose a transformer-induced progressive refinement network (TPRNet) to solve challenging COD tasks. Specifically, our network includes a Transformer-induced Progressive Refinement Module (TPRM) and a Semantic-Spatial Interaction Enhancement Module (SIEM). In TPRM, high-level features with rich semantic information are integrated through transformers as prior guidance, and then, it is sent to the refinement concurrency unit (RCU), and the accurately positioned feature area is obtained through a progressive refinement strategy. In SIEM, we perform feature interaction to localizedaccurate semantic features and low-level features to obtain rich fine-grained clues and increase the symbolic power of boundary features. Extensive experiments on four widely used benchmark datasets (i.e., CAMO, CHAMELEON, COD10K, and NC4K) demonstrate that our TPRNet is an effective COD model and outperforms state-of-the-art models. The code is available https://github.com/zhangyiao970914/TPRNet.
C1 [Zhang, Qiao; Ge, Yanliang; Zhang, Cong; Bi, Hongbo] Northeast Petr Univ, Sch Elect Informat Engn, Daqing 163000, Peoples R China.
C3 Northeast Petroleum University
RP Zhang, C; Bi, HB (corresponding author), Northeast Petr Univ, Sch Elect Informat Engn, Daqing 163000, Peoples R China.
EM qiaozhang_dongyou@163.com; 15804593399@139.com; congzhang98@126.com;
   bhbdq@126.com
OI Bi, Hongbo/0000-0003-2442-330X
FU AnHui Province Key Laboratory of Infrared and Low-Temperature Plasma
   [IRKL2022KF07]
FX The paper is supported by AnHui Province Key Laboratory of Infrared and
   Low-Temperature Plasma under No. IRKL2022KF07.
CR Amit SNKB, 2016, INT GEOSCI REMOTE SE, P5189, DOI 10.1109/IGARSS.2016.7730352
   Ba J. L., 2016, LAYER NORMALIZATION, DOI DOI 10.48550/ARXIV.1607.06450
   Bi HB, 2022, IEEE T CIRC SYST VID, V32, P5708, DOI 10.1109/TCSVT.2021.3124952
   Bi HB, 2021, VISUAL COMPUT, V37, P911, DOI 10.1007/s00371-020-01842-4
   Chen Q, 2021, PROC CVPR IEEE, P13034, DOI 10.1109/CVPR46437.2021.01284
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Cui Y., 2022, WACV, P3411, DOI 10.1109/WACV51458
   Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26
   Dong B., 2021, ARXIV PREPRINT ARXIV, V1
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P6024, DOI 10.1109/TPAMI.2021.3085766
   Fan DP, 2020, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR42600.2020.00285
   Fan DP, 2020, IEEE T MED IMAGING, V39, P2626, DOI 10.1109/TMI.2020.2996645
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Ji GP, 2022, PATTERN RECOGN, V123, DOI 10.1016/j.patcog.2021.108414
   Kingma D. P., 2014, arXiv
   Le XY, 2020, NEUROCOMPUTING, V408, P112, DOI 10.1016/j.neucom.2019.09.107
   Li AX, 2021, PROC CVPR IEEE, P10066, DOI 10.1109/CVPR46437.2021.00994
   Liu DF, 2021, PROC CVPR IEEE, P9811, DOI 10.1109/CVPR46437.2021.00969
   Liu DF, 2020, NEUROCOMPUTING, V409, P1, DOI 10.1016/j.neucom.2020.05.027
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2012, IEEE T IMAGE PROCESS, V21, P4204, DOI 10.1109/TIP.2012.2200492
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Mei HY, 2021, PROC CVPR IEEE, P8768, DOI 10.1109/CVPR46437.2021.00866
   Pan Y., 2011, Mod Appl Sci, V5, P152, DOI DOI 10.5539/MAS.V5N4P152
   Paszke A, 2019, ADV NEUR IN, V32
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196
   Sengottuvelan P., 2008, 2008 1st International Conference on Emerging Trends in Engineering and Technology (ICETET), P6, DOI 10.1109/ICETET.2008.232
   Skurowski P., 2018, Animal camouflage analysis: Chameleon database
   Sun Y., ARXIV PREPRINT ARXIV
   Le TN, 2019, COMPUT VIS IMAGE UND, V184, P45, DOI 10.1016/j.cviu.2019.04.006
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang D, 2021, VISUAL COMPUT, V37, P1101, DOI 10.1007/s00371-020-01855-z
   Wang K, 2022, IEEE T IND ELECTRON, V69, P5364, DOI 10.1109/TIE.2021.3078379
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang WX, 2021, LECT NOTES COMPUT SC, V12901, P109, DOI 10.1007/978-3-030-87193-2_11
   Wang XY, 2022, VISUAL COMPUT, V38, P3831, DOI 10.1007/s00371-021-02224-0
   WEI J, 2020, AAAI CONF ARTIF INTE, V34
   Wu YH, 2021, IEEE T IMAGE PROCESS, V30, P3113, DOI 10.1109/TIP.2021.3058783
   Wu Z, 2019, IEEE I CONF COMP VIS, P7263, DOI 10.1109/ICCV.2019.00736
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xiao HG, 2023, VISUAL COMPUT, V39, P2291, DOI 10.1007/s00371-022-02414-4
   Yan JN, 2021, IEEE ACCESS, V9, P43290, DOI 10.1109/ACCESS.2021.3064443
   Yang F, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4126, DOI 10.1109/ICCV48922.2021.00411
   Yin JQ, 2011, PROCEDIA ENGINEER, V15, DOI 10.1016/j.proeng.2011.08.412
   Youwei P., 2022, ARXIV PREPRINT ARXIV
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zhai Q, 2021, PROC CVPR IEEE, P12992, DOI 10.1109/CVPR46437.2021.01280
   Zhang XQ, 2021, VISUAL COMPUT, V37, P1089, DOI 10.1007/s00371-020-01854-0
   Zhang YB, 2023, VISUAL COMPUT, V39, P1283, DOI 10.1007/s00371-022-02404-6
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhuge MC, 2022, PATTERN RECOGN, V127, DOI 10.1016/j.patcog.2022.108644
NR 58
TC 15
Z9 15
U1 6
U2 37
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4593
EP 4607
DI 10.1007/s00371-022-02611-1
EA JUL 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000826816700001
DA 2024-07-18
ER

PT J
AU Xie, Q
   Hu, JP
   Wang, XC
   Zhang, DC
   Qin, H
AF Xie, Qi
   Hu, Jianping
   Wang, Xiaochao
   Zhang, Daochang
   Qin, Hong
TI Novel and fast EMD-based image fusion via morphological filter
SO VISUAL COMPUTER
LA English
DT Article
DE Image fusion; Empirical mode decomposition; Morphological filter;
   Intrinsic mode functions
ID EMPIRICAL MODE DECOMPOSITION; TRANSFORM; FRAMEWORK
AB This paper presents a novel and fast EMD-based (empirical mode decomposition-based) image fusion approach via morphological filter. Firstly, we develop a multi-channel bidimensional EMD method based on morphological filter to conduct image fusion. It uses the morphological expansion and erosion filters to compute the upper and lower envelopes of a multi-channel image in the sifting processing, and can decompose the input source images into several intrinsic mode functions (IMFs) with different scales and a residue. It significantly improves the computation efficiency of EMD for multi-channel images while maintaining the decomposition quality. Secondly, we adopt a patch-based fusion strategy with overlapping partition to fuse the IMFs and residue instead of the pixel-based fusion way usually used in EMD-based image fusion, where an energy-based maximum selection rule is designed to fuse the IMFs, and the feature information extracted by IMFs is used as a guide to merge the residue. Such strategy can extract the salient information of the source images well and can also reduce the spatial artifacts introduced by the noisy characteristics of the pixel-wise maps. A large number of comparative experiments on the fusion of several commonly used image data sets with multi-focus and multi-modal images, show that our newly proposed fusion method can obtain much better results than the existing EMD-based image fusion approaches. Furthermore, it is very competitive with the state-of-the-art image fusion methods in visualization, objective metrics, and time performance. The code of the proposed method can be downloaded from: https://github.com/neepuhjp/MFMBEMD-ImageFusion.
C1 [Xie, Qi] Jilin Univ, Sch Math, Changchun 130012, Peoples R China.
   [Xie, Qi; Hu, Jianping; Zhang, Daochang] Northeast Elect Power Univ, Sch Sci, Jilin 132012, Jilin, Peoples R China.
   [Wang, Xiaochao] Tiangong Univ, Sch Math Sci, Tianjin 300387, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 Jilin University; Northeast Electric Power University; Tiangong
   University; State University of New York (SUNY) System; State University
   of New York (SUNY) Stony Brook
RP Hu, JP (corresponding author), Northeast Elect Power Univ, Sch Sci, Jilin 132012, Jilin, Peoples R China.; Wang, XC (corresponding author), Tiangong Univ, Sch Math Sci, Tianjin 300387, Peoples R China.
EM xiegi_19820302@126.com; neduhjp307@163.com; wangxiaochao18@163.com;
   daochangzhang@126.com; qin@cs.stonybrook.edu
RI Hu, Jianping/IWM-3698-2023
OI Hu, Jianping/0000-0001-8675-4564
FU National Science Foundation of USA [IIS-1812606, IIS-1715985]; National
   Natural Science Foundation of China [61672149, 61532002, 61602344,
   61802279]; Open Project Program of the State Key Lab of CAD &CG,
   Zhejiang University [A2105]
FX We would like to thank the anonymous reviewers for their helpful
   comments. This work is supported in part by National Science Foundation
   of USA (IIS-1812606, IIS-1715985); National Natural Science Foundation
   of China (No. 61672149, 61532002, 61602344, 61802279); the Open Project
   Program of the State Key Lab of CAD &CG(No. A2105), Zhejiang
   University.dy
CR Al-Baddai S, 2016, INFORM SCIENCES, V348, P305, DOI 10.1016/j.ins.2016.01.089
   [Anonymous], 2010, P 2010 SPRING SIM MU
   Bhuiyan SMA, 2012, J ELECTRON IMAGING, V21, DOI 10.1117/1.JEI.21.3.033019
   Bhuiyan SMA, 2008, EURASIP J ADV SIG PR, DOI 10.1155/2008/728356
   Goshtasby AA, 2007, INFORM FUSION, V8, P114, DOI 10.1016/j.inffus.2006.04.001
   Haghighat MBA, 2011, COMPUT ELECTR ENG, V37, P744, DOI 10.1016/j.compeleceng.2011.07.012
   Han Y, 2013, INFORM FUSION, V14, P127, DOI 10.1016/j.inffus.2011.08.002
   Hu JP, 2016, COMPUT AIDED GEOM D, V43, P95, DOI 10.1016/j.cagd.2016.02.011
   Hu JP, 2014, GRAPH MODELS, V76, P340, DOI 10.1016/j.gmod.2014.03.006
   Hu K, 2021, VISUAL COMPUT, V37, P2841, DOI 10.1007/s00371-021-02168-5
   Huang NE, 1998, P ROY SOC A-MATH PHY, V454, P903, DOI 10.1098/rspa.1998.0193
   Lai R, 2019, IEEE ACCESS, V7, P114385, DOI 10.1109/ACCESS.2019.2935006
   Lewis JJ, 2007, INFORM FUSION, V8, P119, DOI 10.1016/j.inffus.2005.09.006
   LI H, 1995, GRAPH MODEL IM PROC, V57, P235, DOI 10.1006/gmip.1995.1022
   Li HF, 2013, OPTIK, V124, P40, DOI 10.1016/j.ijleo.2011.11.088
   Li H, 2020, KNOWL-BASED SYST, V204, DOI 10.1016/j.knosys.2020.106182
   Li XS, 2015, OPT ENG, V54, DOI 10.1117/1.OE.54.7.073115
   Liu Y, 2020, INFORM FUSION, V64, P71, DOI 10.1016/j.inffus.2020.06.013
   Liu Y, 2018, INFORM FUSION, V42, P158, DOI 10.1016/j.inffus.2017.10.007
   Liu Y, 2015, INFORM FUSION, V24, P147, DOI 10.1016/j.inffus.2014.09.004
   Ma JY, 2019, INFORM FUSION, V45, P153, DOI 10.1016/j.inffus.2018.02.004
   Ma JL, 2019, NEUROCOMPUTING, V335, P9, DOI 10.1016/j.neucom.2019.01.048
   Mandic DP, 2013, IEEE SIGNAL PROC MAG, V30, P74, DOI 10.1109/MSP.2013.2267931
   Naidu V., 2011, 2011 INT C IM INF PR, P1
   Nencini F, 2007, INFORM FUSION, V8, P143, DOI 10.1016/j.inffus.2006.02.001
   Nunes JC, 2003, IMAGE VISION COMPUT, V21, P1019, DOI 10.1016/S0262-8856(03)00094-5
   Pan JJ, 2016, DIGIT SIGNAL PROCESS, V50, P61, DOI 10.1016/j.dsp.2015.12.003
   Qin XQ, 2017, INFRARED PHYS TECHN, V85, P251, DOI 10.1016/j.infrared.2017.07.009
   Rehman N, 2010, P ROY SOC A-MATH PHY, V466, P1291, DOI 10.1098/rspa.2009.0502
   Rehman NU, 2015, SENSORS-BASEL, V15, P10923, DOI 10.3390/s150510923
   Sufyan A, 2022, INT J IMAG SYST TECH, V32, P324, DOI 10.1002/ima.22649
   Trusiak M, 2014, OPT LASER ENG, V52, P230, DOI 10.1016/j.optlaseng.2013.06.003
   Wang P, 2018, INT J DISTRIB SENS N, V14, DOI 10.1177/1550147718818755
   Wang XC, 2020, VISUAL COMPUT, V36, P2201, DOI 10.1007/s00371-020-01909-2
   Wang XC, 2018, COMPUT AIDED GEOM D, V59, P1, DOI 10.1016/j.cagd.2017.11.002
   Xia YL, 2019, IEEE ACCESS, V7, P114261, DOI 10.1109/ACCESS.2019.2936030
   Xydeas CS, 2000, ELECTRON LETT, V36, P308, DOI 10.1049/el:20000267
   Yang C, 2008, INFORM FUSION, V9, P156, DOI 10.1016/j.inffus.2006.09.001
   Yeh MH, 2012, SIGNAL PROCESS, V92, P523, DOI 10.1016/j.sigpro.2011.08.019
   Zhan K, 2019, J ELECTRON IMAGING, V28, DOI 10.1117/1.JEI.28.2.023027
   Zhang Y, 2020, INFORM FUSION, V54, P99, DOI 10.1016/j.inffus.2019.07.011
   Zhu P, 2021, MULTIMED TOOLS APPL, V80, P4455, DOI 10.1007/s11042-020-09860-z
   Zhu ZQ, 2019, IEEE ACCESS, V7, P20811, DOI 10.1109/ACCESS.2019.2898111
NR 43
TC 6
Z9 6
U1 1
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 4249
EP 4265
DI 10.1007/s00371-022-02588-x
EA JUL 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000825226300003
DA 2024-07-18
ER

PT J
AU Yu, ZX
   Oliveira, MM
   Aliaga, DG
AF Yu, Zixun
   Oliveira, Manuel M.
   Aliaga, Daniel G.
TI Preemptive text warping to prevent appearance of motion blur
SO VISUAL COMPUTER
LA English
DT Article
DE Image processing; Motion deblurring; Visual system
AB We present a preemptive image-based method to reduce motion blurring. Motion blur appears when there is a relative motion between the scene and the viewer/camera. A preemptive method pre-filters the content before being displayed in order to mitigate the occurrence of motion blur. Our experiments and user study have shown that such preemptive methods are fundamentally subject to producing visual artifacts as a consequence of unavoidable ringing or intensity oscillations. Frequency-domain analysis shows that the energy weakened at certain frequencies leads to those artifacts. We present a method to process alphanumeric content so that it has lower energy on frequencies eliminated by a given motion blur kernel. Our processed image, when motion blurred, will have a sharper appearance and less artifacts as compared to various alternative approaches. We demonstrate the effectiveness of the proposed technique with simulated and real-world experiments as well as user feedback. Our results show that our approach yields content robust to motion blur while still being perceptually similar to the original text.
C1 [Yu, Zixun; Aliaga, Daniel G.] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
   [Oliveira, Manuel M.] Univ Fed Rio Grande do Sul, Inst Informat, Porto Alegre, RS, Brazil.
C3 Purdue University System; Purdue University; Universidade Federal do Rio
   Grande do Sul
RP Yu, ZX (corresponding author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
EM yu645@purdue.edu; oliveira@inf.ufrgs.br; aliaga@cs.purdue.edu
RI Oliveira, Manuel M/H-1508-2011
FU CNPq-Brazil [312975/2018-0]; CAPES [001]; NSF [1835739, 1816514,
   2107096]; Div Of Information & Intelligent Systems; Direct For Computer
   & Info Scie & Enginr [2107096] Funding Source: National Science
   Foundation; Div Of Information & Intelligent Systems; Direct For
   Computer & Info Scie & Enginr [1816514] Funding Source: National Science
   Foundation
FX This work is funded by CNPq-Brazil (312975/2018-0), CAPES Finance Code
   001, NSF #1816514, NSF #2107096, and NSF #1835739.
CR Aksit K, 2020, OPT EXPRESS, V28, P2107, DOI 10.1364/OE.380858
   Alonso M, 2008, INVERSE PROBL SCI EN, V16, P957, DOI 10.1080/17415970802082823
   [Anonymous], 2006, P IEEE C COMP VIS PA
   Banham MR, 1997, IEEE SIGNAL PROC MAG, V14, P24, DOI 10.1109/79.581363
   Campisi P., 2007, Blindimagedeconvolution:theoryandapplications
   Cho H. M., 2010, P IEEE INT C COMP PH, P1
   Cornsweet T., 2012, Visual Perception
   Debevec Paul E, 2008, ACM SIGGRAPH 2008 CL, P1, DOI DOI 10.1145/1401132.1401174
   DIDYK P, 2010, ACM SIGGRAPH 2010 PA, P1
   Eboli T., ARXIV PREPRINT ARXIV
   Fortunato HE, 2014, VISUAL COMPUT, V30, P661, DOI 10.1007/s00371-014-0966-x
   Fortunato HE, 2012, COMPUT GRAPH FORUM, V31, P459, DOI 10.1111/j.1467-8659.2012.03025.x
   Gastal ESL, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073670
   Har-Noy S, 2008, IEEE T IMAGE PROCESS, V17, P117, DOI 10.1109/TIP.2007.914152
   Huang FC, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601122
   Huang FC, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366204
   Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897
   Lee H, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3203202
   Levin A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360670
   Lin S, 2004, PROC CVPR IEEE, P938
   LUCY LB, 1974, ASTRON J, V79, P745, DOI 10.1086/111605
   Masia B, 2013, COMPUT GRAPH-UK, V37, P1012, DOI 10.1016/j.cag.2013.10.003
   Montalto C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2717307
   Nehab D, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392392
   Oyamada Y, 2007, PROC CVPR IEEE, P3520
   Pamplona VF, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185577
   Peli E, 2009, INT J ARTIF INTELL T, V18, P415, DOI 10.1142/S0218213009000214
   Raskar R, 2006, ACM T GRAPHIC, V25, P795, DOI 10.1145/1141911.1141957
   Ritschel Tobias., 2008, ACM Trans. Graph. (Proc. SIGGRAPH 2008), V27, P1
   Robinson E.A., 1967, GEOPHYS PROSPECT, V15, P311
   Shamos MI, 1976, 17 ANN S FDN COMP SC, P208
   Shin K.H., 2006, SID S DIGEST TECHNIC, V37
   Shutterstock, DOT MATRIX LETT IMAG
   Tikhonov A. N., 1977, SOLUTIONS ILL POSED
   Xia T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618461
   Xu XY, 2017, IEEE I CONF COMP VIS, P251, DOI 10.1109/ICCV.2017.36
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang SH, 2018, PROC CVPR IEEE, P6586, DOI 10.1109/CVPR.2018.00689
NR 38
TC 0
Z9 0
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3391
EP 3403
DI 10.1007/s00371-022-02545-8
EA JUN 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000817830300001
OA Bronze
DA 2024-07-18
ER

PT J
AU Szucs, G
AF Szucs, Gabor
TI Multiclass classification by Min-Max ECOC with Hamming distance
   optimization
SO VISUAL COMPUTER
LA English
DT Article
DE Binary classifier; Deep neural network; ECOC; Hamming distance; Ensemble
   learning; Multiclass classification
ID FUSION; DECOMPOSITION; BINARY; RECOGNITION; PREDICTION; ENSEMBLES
AB Two questions often arise in the field of the ensemble in multiclass classification problems, (i) how to combine base classifiers and (ii) how to design possible binary classifiers. Error-correcting output codes (ECOC) methods answer these questions, but they focused on only the general goodness of the classifier. The main purpose of our research was to strengthen the bottleneck of the ensemble method, i.e., to minimize the largest values of two types of error ratios in the deep neural network-based classifier. The research was theoretical and experimental, the proposed Min-Max ECOC method suggests a theoretically proven optimal solution, which was verified by experiments on image datasets. The optimal solution was based on the maximization of the lowest value in the Hamming matrix coming from the ECOC matrix. The largest ECOC matrix, the so-called full matrix is always a Min-Max ECOC matrix, but smaller matrices generally do not reach the optimal Hamming distance value, and a recursive construction algorithm was proposed to get closer to it. It is not easy to calculate optimal values for large ECOC matrices, but an interval with upper and lower limits was constructed by two theorems, and they were proved. Convolutional Neural Networks with Min-Max ECOC matrix were tested on four real datasets and compared with OVA (one versus all) and variants of ECOC methods in terms of known and two new indicators. The experimental results show that the suggested method surpasses the others, thus our method is promising in the ensemble learning literature.
C1 [Szucs, Gabor] Budapest Univ Technol & Econ, Dept Telecommun & Media Informat, Magyar Tudosok Krt 2, H-1117 Budapest, Hungary.
C3 Budapest University of Technology & Economics
RP Szucs, G (corresponding author), Budapest Univ Technol & Econ, Dept Telecommun & Media Informat, Magyar Tudosok Krt 2, H-1117 Budapest, Hungary.
EM szucs@tmit.bme.hu
OI Szucs, Gabor/0000-0002-5781-1088
FU Budapest University of Technology and Economics
FX Open access funding provided by Budapest University of Technology and
   Economics.
CR Ahmed SAA, 2021, IEEE ACCESS, V9, P86083, DOI 10.1109/ACCESS.2021.3088717
   Allwein E. L., 2001, Journal of Machine Learning Research, V1, P113, DOI 10.1162/15324430152733133
   Alshdaifat E.A., 2015, INT C INN TECHN APPL, P43
   Alvear-Sandoval RF, 2019, INFORM FUSION, V52, P106, DOI 10.1016/j.inffus.2018.12.005
   Bagheri MA, 2014, INT C PATT RECOG, P1254, DOI 10.1109/ICPR.2014.225
   Bora MB, 2020, PROCEDIA COMPUT SCI, V167, P2403, DOI 10.1016/j.procs.2020.03.293
   Chaladze G., 2017, Linnaeus 5 dataset for machine learning
   Chen SG, 2017, INT J MACH LEARN CYB, V8, P1731, DOI 10.1007/s13042-016-0554-7
   Cheng YS, 2019, IEEE ACCESS, V7, P145235, DOI 10.1109/ACCESS.2019.2946198
   D'Ambrosio R, 2013, LECT NOTES COMPUT SC, V8156, P682
   Dietterich T. G., 1995, Journal of Artificial Intelligence Research, V2, P263
   Dong XB, 2020, FRONT COMPUT SCI-CHI, V14, P241, DOI 10.1007/s11704-019-8208-z
   Gao X, 2021, ENG APPL ARTIF INTEL, V97, DOI 10.1016/j.engappai.2020.104034
   García-Pedrajas N, 2011, INFORM FUSION, V12, P111, DOI 10.1016/j.inffus.2010.06.010
   Gogic I, 2020, VISUAL COMPUT, V36, P97, DOI 10.1007/s00371-018-1585-8
   Guo H, 2022, VISUAL COMPUT, V38, P3897, DOI 10.1007/s00371-021-02230-2
   Hastie T, 1998, ANN STAT, V26, P451
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He Q, 2020, INFORM FUSION, V55, P207, DOI 10.1016/j.inffus.2019.09.001
   Klimo M, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11083563
   Krawczyk B, 2015, PATTERN RECOGN, V48, P3969, DOI 10.1016/j.patcog.2015.06.001
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Lachaize M, 2018, INT J APPROX REASON, V103, P303, DOI 10.1016/j.ijar.2018.10.008
   Lei L, 2021, COMPUT INTEL NEUROSC, V2021, DOI 10.1155/2021/5583031
   Liu KH, 2016, INFORM SCIENCES, V349, P102, DOI 10.1016/j.ins.2016.02.028
   Manikandan J, 2013, INT J MACH LEARN CYB, V4, P347, DOI 10.1007/s13042-012-0102-z
   Mehra N., 2013, INT J COMPUTER SCI I, V4, P572
   Nazari S, 2019, INT J MACH LEARN CYB, V10, P433, DOI 10.1007/s13042-017-0723-3
   Neill J.O., 2019, ARXIV PREPRINT ARXIV
   Rocha A, 2014, IEEE T NEUR NET LEAR, V25, P289, DOI 10.1109/TNNLS.2013.2274735
   Scheidegger F, 2021, VISUAL COMPUT, V37, P1593, DOI 10.1007/s00371-020-01922-5
   Shiraishi Y, 2011, NEUROCOMPUTING, V74, P680, DOI 10.1016/j.neucom.2010.09.004
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sobczak S, 2021, VISUAL COMPUT, V37, P423, DOI 10.1007/s00371-019-01782-8
   Stallkamp J, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1453, DOI 10.1109/IJCNN.2011.6033395
   Sun J, 2021, INFORM SCIENCES, V559, P153, DOI 10.1016/j.ins.2021.01.059
   Thakkar A, 2021, INFORM FUSION, V65, P95, DOI 10.1016/j.inffus.2020.08.019
   Xiao H., 2017, ARXIV170807747
   Yan JJ, 2020, KNOWL-BASED SYST, V198, DOI 10.1016/j.knosys.2020.105922
   Ye XN, 2020, INFORM SCIENCES, V537, P485, DOI 10.1016/j.ins.2020.05.088
   Zadrozny B., 2002, P 8 ACM SIGKDD INT C, P694, DOI [10.1145/775047.775151, DOI 10.1145/775047.775151]
   Zhang HX, 2021, VISUAL COMPUT, V37, P2433, DOI 10.1007/s00371-020-01997-0
   Zhou JT, 2019, MACH LEARN, V108, P809, DOI 10.1007/s10994-019-05786-2
   Zhou LG, 2017, INFORM FUSION, V36, P80, DOI 10.1016/j.inffus.2016.11.009
   Zou JY, 2021, INFORM SCIENCES, V571, P1, DOI 10.1016/j.ins.2021.04.038
NR 45
TC 3
Z9 3
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 3949
EP 3961
DI 10.1007/s00371-022-02540-z
EA JUN 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000817073900003
OA hybrid
DA 2024-07-18
ER

PT J
AU Liang, H
   Dong, XH
AF Liang, Hui
   Dong, Xiaohang
TI Enhancing cognitive ability through a VR serious game training model
   mixing Piaget's epistemological methodology and Lumosity concept
SO VISUAL COMPUTER
LA English
DT Article
DE Cognitive impairment; Serious game training; Children's cognitive
   ability; Piaget's cognitive development theory; Gamification
AB Recent studies have shown that children's cognitive function can be improved through relevant training. In order to improve the cognitive impairment in children, we use Piaget's cognitive development theory and the design concept of the Lumosity brain training platform to develop a training model for enhancing the cognitive abilities among children. In the game design process, according to Piaget's cognitive development theory, we proposed different game elements in an attempt to avoid repetition of trainers in terms of gameplay, logical scenes and task elements. Finally, we recruited 50 volunteers to participate in the test, and it was shown by the test results that children's cognitive ability has been improved, and the cognitive recovery of children has also been enhanced under the cognitive training model we developed. However, the duration of cognitive improvement in children remains to be explored further, and some issues need to be addressed in future work.
C1 [Liang, Hui] Zhengzhou Univ Light Ind, Zhengzhou, Peoples R China.
   [Liang, Hui] Bournemouth Univ, Natl Ctr Comp Animat, Poole, Dorset, England.
   [Dong, Xiaohang] Zhengzhou Univ Light Ind, Comp Technol, Zhengzhou, Peoples R China.
C3 Zhengzhou University of Light Industry; Bournemouth University;
   Zhengzhou University of Light Industry
RP Liang, H (corresponding author), Zhengzhou Univ Light Ind, Zhengzhou, Peoples R China.; Liang, H (corresponding author), Bournemouth Univ, Natl Ctr Comp Animat, Poole, Dorset, England.; Dong, XH (corresponding author), Zhengzhou Univ Light Ind, Comp Technol, Zhengzhou, Peoples R China.
EM hliang@zzuli.edu.cn; xiaohangdong@email.zzuli.edu.cn
OI Dong, Xiaohang/0000-0001-7969-3134
FU Scientific and Technological Project in Henan Province [222102210030];
   major science and technology program in Henan Province [211110110500]
FX The research leading to these results has received funding from the
   Scientific and Technological Project in Henan Province (222102210030)
   and the "Jie Bang Gua Shuai" project of the major science and technology
   program in Henan Province (211110110500).
CR Anguera JA, 2013, NATURE, V501, P97, DOI 10.1038/nature12486
   Bainbridge K, 2018, J COGN ENHANCE, V2, P43, DOI 10.1007/s41465-017-0040-5
   Barrouillet P, 2015, DEV REV, V38, P1, DOI 10.1016/j.dr.2015.07.004
   Bower E.M., 1976, AM J ORTHOPSYCHIAT, V46, P729, DOI [10.1037/h0098761, DOI 10.1037/H0098761]
   Cai Y., 2021, VR SERIOUS GAMES MEE
   Chen YL, 2016, ASIA-PAC EDUC RES, V25, P637, DOI 10.1007/s40299-016-0293-2
   Fraporti TT, 2019, PROG NEURO-PSYCHOPH, V93, P214, DOI 10.1016/j.pnpbp.2019.03.021
   Gandy KC, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0239040
   Goh Z.H.J., 2021, IPAD SERIOUS GAME AI, P157, DOI [10.1007/978-981-33-6942-9_10, DOI 10.1007/978-981-33-6942-9_10]
   Govindan K, 2014, INT J PROD ECON, V147, P555, DOI 10.1016/j.ijpe.2013.08.018
   Hardy JL, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0134467
   Ho W, 2008, EUR J OPER RES, V186, P211, DOI 10.1016/j.ejor.2007.01.004
   Jaeggi SM, 2008, P NATL ACAD SCI USA, V105, P6829, DOI 10.1073/pnas.0801268105
   Jiang K, 2020, ACTA PSYCHOL SIN, V52, P1017, DOI 10.3724/SP.J.1041.2020.01017
   Kpolovie P., 2012, JAMES LUMOSITY TRAIN, V2, P186
   Kueider AM, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040588
   Legault I, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00323
   Liu W., 2021, SERIOUS GAME DESIGN, P97, DOI [10.1007/978-981-33-6942-9_6, DOI 10.1007/978-981-33-6942-9_6]
   Lu D., 2021, VIRTUAL PINK DOLPHIN, P77, DOI [10.1007/978-981-33-6942-9_5, DOI 10.1007/978-981-33-6942-9_5]
   Majerus S., 2020, NEUROCOGNITIVE DEV D, V174, P357, DOI [10.1016/B978-0-444-64148-9.00026-0, DOI 10.1016/B978-0-444-64148-9.00026-0]
   Malhi P, 2018, INDIAN J PEDIATR, V85, P498, DOI 10.1007/s12098-018-2613-4
   Martin F, 2020, ETR&D-EDUC TECH RES, V68, P1613, DOI 10.1007/s11423-020-09812-2
   Min-Sheng Chen, 2019, Proceedings of the 20th Congress of the International Ergonomics Association (IEA 2018). Volume IX: Aging, Gender and Work, Anthropometry, Ergonomics for Children and Educational Environments. Advances in Intelligent Systems and Computing (AISC 826), P639, DOI 10.1007/978-3-319-96065-4_67
   Pape K, 2011, BMC PUBLIC HEALTH, V11, DOI 10.1186/1471-2458-11-718
   Pasqualotto A, 2022, NAT HUM BEHAV, V6, P545, DOI 10.1038/s41562-021-01254-x
   Saleh M., 2020, TECHNIUM SOC SCI J, V2, P37, DOI [10.47577/tssj.v2i1.54, DOI 10.47577/TSSJ.V2I1.54]
   Sergienko EA, 2008, PSIKHOL ZH, V29, P34
   Shute VJ, 2015, COMPUT EDUC, V80, P58, DOI 10.1016/j.compedu.2014.08.013
   Teoh S., 2021, LEARNING TAKE SHOWER, P3, DOI [10.1007/978-981-33-6942-9_2, DOI 10.1007/978-981-33-6942-9_2]
   Wang YM, 2008, EUR J OPER RES, V186, P735, DOI 10.1016/j.ejor.2007.01.050
   Zdybel D, 2021, THINK SKILLS CREAT, V41, DOI 10.1016/j.tsc.2021.100855
   Zhang Q., 2021, LEARNING CROSS ROADS, P63, DOI [10.1007/978-981-33-6942-9_4, DOI 10.1007/978-981-33-6942-9_4]
   Zhang Y.W., 2018, ED TEACHING FORUM
   Zickefoose S, 2013, BRAIN INJURY, V27, P707, DOI 10.3109/02699052.2013.775484
NR 34
TC 3
Z9 3
U1 19
U2 71
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3487
EP 3498
DI 10.1007/s00371-022-02552-9
EA JUN 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000815551100001
OA Bronze
DA 2024-07-18
ER

PT J
AU Yang, Y
   Zhang, JL
   Liu, C
   Zhang, HW
   Li, X
AF Yang Yan
   Zhang Jinlong
   Liu Ce
   Zhang Haowen
   Li Xiang
TI Visibility restoration of haze and dust image using color correction and
   composite channel prior
SO VISUAL COMPUTER
LA English
DT Article
DE Image dehazing; Visibility restoration; Color correction; Composite
   channel prior; Mid-channel atmospheric light
ID REMOVAL
AB Visibility restoration of images under haze and dust weather is essential in computer vision tasks. In this work, an algorithm for image visibility restoration based on color correction and composite channel prior (CCP) is proposed. First, the color of a dust image is corrected by color compensation and white balance for blue and red channels. Haze and dust images are effectively distinguished by channel differences. Secondly, the composite channels are defined by simple multiplication and subtraction, and the composite channels of haze image and clear image have a very close pixel distribution. Then, according to atmospheric imaging rules, haze is the main factor that causes brightness difference of each composite channel. To eliminate the brightness difference, an adaptive gamma correction function based on haze density is proposed. In addition, as another important parameter of image restoration task, mean inequality and morphological operations are used to obtain more accurate mid-channel atmospheric light. Finally, a practical transmission map and high-quality clear image are obtained by atmospheric scattering models. Experimental results show that the proposed method is usable and practical. Our results have rich details and edges, especially for dust images.
C1 [Yang Yan; Zhang Jinlong; Liu Ce; Zhang Haowen; Li Xiang] Lanzhou Jiaotong Univ, Sch Elect & Informat Engn, Lanzhou, Gansu, Peoples R China.
C3 Lanzhou Jiaotong University
RP Yang, Y (corresponding author), Lanzhou Jiaotong Univ, Sch Elect & Informat Engn, Lanzhou, Gansu, Peoples R China.
EM yangyantd@mail.lzjtu.cn
RI Zhang, Jing/ISA-6627-2023; Yu, ZH/KBC-6889-2024; zhang,
   jin/IXD-9872-2023; zhang, jin/GXV-9154-2022; zhang, haowen/JNE-7429-2023
OI Zhang, Jing/0009-0003-5039-5688; Yang, Yan/0000-0001-5338-0762
CR Ancuti CO, 2018, IEEE T IMAGE PROCESS, V27, P379, DOI 10.1109/TIP.2017.2759252
   Bi GL, 2017, IEEE PHOTONICS J, V9, DOI 10.1109/JPHOT.2017.2726107
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Kim JY, 2001, IEEE T CIRC SYST VID, V11, P475, DOI 10.1109/76.915354
   Li B., 2017, IEEE INT C COMP VIS
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li CY, 2020, IEEE T MULTIMEDIA, V22, P704, DOI 10.1109/TMM.2019.2933334
   Li RD, 2020, IEEE T IMAGE PROCESS, V29, P6523, DOI 10.1109/TIP.2020.2991509
   Liu RS, 2019, IEEE T NEUR NET LEAR, V30, P2973, DOI 10.1109/TNNLS.2018.2862631
   McCartney E. J., 1976, Optics of the atmosphere. Scattering by molecules and particles
   Mei W, 2019, IEEE INT CONF ELECTR, P579, DOI [10.1109/ICEIEC.2019.8784493, 10.1109/iceiec.2019.8784493]
   Meng G., 2013, IEEE INT C COMPUTER
   Min XK, 2019, IEEE T MULTIMEDIA, V21, P2319, DOI 10.1109/TMM.2019.2902097
   Narasimhan S.G., 2000, IEEE COMPUTER SOC C
   Qian W, 2020, 2020 5TH INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATION SYSTEMS (ICCCS 2020), P329, DOI [10.1109/ICCCS49078.2020.9118601, 10.1109/icccs49078.2020.9118601]
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Rahman, 1997, COMP MULTISCALE RETI, P16
   Rahman ZU, 2004, J ELECTRON IMAGING, V13, P100, DOI 10.1117/1.1636183
   Ren W., 2016, EUROPEAN C COMPUTER
   Sun W, 2015, COMPUT ELECTR ENG, V46, P371, DOI 10.1016/j.compeleceng.2015.02.009
   Tan R.T., 2008, 2008 IEEE COMPUTER S, P2426
   Wang WC, 2017, IEEE-CAA J AUTOMATIC, V4, P410, DOI 10.1109/JAS.2017.7510532
   Wang WC, 2017, IEEE T MULTIMEDIA, V19, P1142, DOI 10.1109/TMM.2017.2652069
   Xiao CX, 2012, VISUAL COMPUT, V28, P713, DOI 10.1007/s00371-012-0679-y
   Xu Y, 2016, 2016 IEEE CIC INT C, P1
   Yang F, 2022, VISUAL COMPUT, V38, P1579, DOI 10.1007/s00371-021-02089-3
   Yang Y, 2020, IEEE SIGNAL PROC LET, V27, P1405, DOI 10.1109/LSP.2020.3013741
   Yang Y, 2020, MULTIDIM SYST SIGN P, V31, P619, DOI 10.1007/s11045-019-00678-z
   Yeh CH, 2013, OPT EXPRESS, V21, P27127, DOI 10.1364/OE.21.027127
   Zhang GF, 2011, IEEE T PATTERN ANAL, V33, P603, DOI 10.1109/TPAMI.2010.115
   Zhang SD, 2020, VISUAL COMPUT, V36, P305, DOI 10.1007/s00371-018-1612-9
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
   Zhu ZQ, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3024335
NR 35
TC 3
Z9 3
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2795
EP 2809
DI 10.1007/s00371-022-02493-3
EA MAY 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000792533800004
DA 2024-07-18
ER

PT J
AU Reisfeld, E
   Sharf, A
AF Reisfeld, Eyal
   Sharf, Andrei
TI OneSketch: learning high-level shape features from simple sketches
SO VISUAL COMPUTER
LA English
DT Article
DE Sketch-based retrieval; Partial representations; High-level shape
   features
AB Humans use simple sketches to convey complex concepts and abstract ideas in a concise way. Just a few abstract pencil strokes can carry a large amount of semantic information that can be used as meaningful representation for many applications. In this work, we explore the power of simple human strokes denoted to capture high-level 2D shape semantics. For this purpose, we introduce OneSketch, a crowd-sourced dataset of abstract one-line sketches depicting high-level 2D object features. To construct the dataset, we formulate a human sketching task with the goal of differentiating between objects with a single minimal stroke. While humans are rather successful at depicting high-level shape semantics and abstraction, we investigate the ability of deep neural networks to convey such traits. We introduce a neural network which learns meaningful shape features from our OneSketch dataset. Essentially, the model learns sketch-to-shape relations and encodes them in an embedding space which reveals distinctive shape features. We show that our network is applicable for differentiating and retrieving 2D objects using very simple one-stroke sketches with good accuracy.
C1 [Reisfeld, Eyal; Sharf, Andrei] Ben Gurion Univ Negev, Beer Sheva, Israel.
C3 Ben Gurion University
RP Sharf, A (corresponding author), Ben Gurion Univ Negev, Beer Sheva, Israel.
EM asharf@cs.bgu.ac.il
RI Sharf, Andrei/F-1370-2012
OI Sharf, Andrei/0000-0002-3963-4508
CR Bell S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766959
   Berger I, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461964
   Bhunia Ayan Kumar, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9776, DOI 10.1109/CVPR42600.2020.00980
   Bui T, 2018, COMPUT GRAPH-UK, V71, P77, DOI 10.1016/j.cag.2017.12.006
   Cao X., 2013, 2013 IEEE INT C COMP
   Cao Y., 2011, CVPR 2011
   Chang Angel X., 2015, arXiv
   Chen SY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459760
   Chen SY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392386
   Chopra S., 2005, 2005 IEEE COMPUTER S, V1
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185540
   Eitz M, 2011, IEEE T VIS COMPUT GR, V17, P1624, DOI 10.1109/TVCG.2010.266
   Ghosh Arnab, 2019, P IEEE CVF INT C COM
   Ha David, 2018, INT C LEARN REPR
   Hadsell R., 2006, 2006 IEEE COMP SOC C, V2
   Hu R, 2013, COMPUT VIS IMAGE UND, V117, P790, DOI 10.1016/j.cviu.2013.02.005
   Kato T., 1992, 1992 P 11 IAPR INT C
   Lamb A., 2020, P IEEE CVF WINT C AP
   Li Y., 2014, BMVC 2014PROCEEDINGS
   Li Yong, 2015, Advanced Technology of Electrical Engineering and Energy, V34, P1
   Limpaecher A, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462016
   Lin H., 2020, P IEEECVF C COMPUTER
   Lin T.-Y., 2015, 2015 IEEE C COMPUTER
   Liu F, 2019, PROC CVPR IEEE, P5823, DOI 10.1109/CVPR.2019.00598
   Muhammad U.R., 2018, P IEEE C COMPUTER VI
   Muhammad UR, 2019, IEEE I CONF COMP VIS, P71, DOI 10.1109/ICCV.2019.00016
   Pang K., 2017, BMVC, P112
   Ribeiro L.S.F., 2020, P IEEE CVF C COMP VI
   Saavedra J.M., 2015, P BRIT MACH VIS C BM
   Sain A., 2020, CROSS MODAL HIERARCH
   Sangkloy P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925954
   Sarvadevabhatla RK, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P271, DOI 10.1145/2733373.2806230
   Shrivastava Abhinav., 2011, P 2011 SIGGRAPH ASIA
   Song J., 2017, P IEEE INT C COMPUTE
   Szegedy Christian, 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.308
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang F., 2015, 2015 IEEE C COMPUTER
   Wang J., 2014, 2014 IEEE C COMPUTER
   Xu XM, 2021, IEEE T VIS COMPUT GR, V27, P178, DOI 10.1109/TVCG.2019.2930512
   Yu Q., 2016, P IEEE C COMPUTER VI
NR 42
TC 4
Z9 3
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2811
EP 2822
DI 10.1007/s00371-022-02494-2
EA MAY 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000791651600001
DA 2024-07-18
ER

PT J
AU Ruhela, R
   Gupta, B
   Lamba, SS
AF Ruhela, Riya
   Gupta, Bhupendra
   Lamba, Subir Singh
TI An efficient approach for texture smoothing by adaptive joint bilateral
   filtering
SO VISUAL COMPUTER
LA English
DT Article
DE Guidance image; Joint bilateral filter; Structure component; Scale map;
   Texture component
ID TOTAL VARIATION MINIMIZATION; IMAGE DECOMPOSITION; PHOTOGRAPHY;
   ENHANCEMENT; FLASH
AB Image decomposition into its structure and texture components is widely used in various image processing and computer vision applications. It is challenging to extract the structure component from an image having intricate texture since it is difficult to extract the structure from the texture that shares similar color intensity or scale. The aim of this work is to smooth the texture component from the image without affecting the significant image structures and to serve the purpose a structure- aware adaptive joint bilateral texture filtering has been employed. Main contribution in this paper is the designing of the guidance image, used in joint bilateral filtering for texture smoothing. To obtain high efficiency by using the proposed method, authors designed a scale map, which provides the size of the spatial kernel at each pixel using the characteristics of the structure and texture components. The experimental section demonstrates the supremacy of the proposed method over the state-of-the-art methods.
C1 [Ruhela, Riya; Gupta, Bhupendra; Lamba, Subir Singh] PDPM Indian Inst Informat Technol Design & Mfg, Dept Nat Sci, Jabalpur 482005, MP, India.
C3 Indian Institute of Information Technology Design & Manufacturing,
   Jabalpur
RP Ruhela, R (corresponding author), PDPM Indian Inst Informat Technol Design & Mfg, Dept Nat Sci, Jabalpur 482005, MP, India.
EM riyashrama41@gmail.com; gupta.bhupendra@gmail.com; subirs@iiitdmj.ac.in
OI Ruhela, Dr. Riya/0000-0002-5175-3913
CR [Anonymous], 1998, Anisotropic diffusion in image processing
   Aujol JF, 2006, INT J COMPUT VISION, V67, P111, DOI 10.1007/s11263-006-4331-z
   Bao P, 2005, IEEE T PATTERN ANAL, V27, P1485, DOI 10.1109/TPAMI.2005.173
   Black MJ, 1998, IEEE T IMAGE PROCESS, V7, P421, DOI 10.1109/83.661192
   Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743
   Cho H, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601188
   Du H, 2016, VISUAL COMPUT, V32, P1537, DOI 10.1007/s00371-015-1138-3
   Eisemann E, 2004, ACM T GRAPHIC, V23, P673, DOI 10.1145/1015706.1015778
   Fan YR, 2017, J FRANKLIN I, V354, P3170, DOI 10.1016/j.jfranklin.2017.01.037
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Fattal R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1477926.1477933
   Gao X, 2020, IEEE T IMAGE PROCESS, V29, P7537, DOI 10.1109/TIP.2020.3004043
   Gavaskar RG, 2019, IEEE T IMAGE PROCESS, V28, P779, DOI 10.1109/TIP.2018.2871597
   Ghosh S, 2020, IEEE T CIRC SYST VID, V30, P2015, DOI 10.1109/TCSVT.2019.2916589
   Ghosh S, 2018, IEEE SIGNAL PROC LET, V25, P1555, DOI 10.1109/LSP.2018.2866949
   Ghosh S, 2016, IEEE SIGNAL PROC LET, V23, P570, DOI 10.1109/LSP.2016.2539982
   Gunturk BK, 2011, IEEE T IMAGE PROCESS, V20, P2690, DOI 10.1109/TIP.2011.2126585
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   Karacan L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508403
   Kim Y, 2019, IEEE T IMAGE PROCESS, V28, P2692, DOI 10.1109/TIP.2018.2889531
   Lamba S.S., 2022, DIGIT SIGNAL PROCESS, V123, P103
   Lin TH, 2016, COMPUT GRAPH FORUM, V35, P57, DOI 10.1111/cgf.13003
   Liu C., 2021, SIGNAL PROCESS-IMAGE
   Mei KQ, 2020, IEEE T IMAGE PROCESS, V29, P2845, DOI 10.1109/TIP.2019.2953361
   Meyer Y., 2001, Memoirs of the American Mathematical Society
   Ono S, 2017, IEEE T IMAGE PROCESS, V26, P1554, DOI 10.1109/TIP.2017.2651392
   Paris S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964963
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Riya, 2021, COMPUT MATH APPL, V93, P106, DOI 10.1016/j.camwa.2021.03.029
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Song CF, 2017, I C VIRTUAL REALITY, P191, DOI 10.1109/ICVRV.2017.00046
   Su Z, 2013, VISUAL COMPUT, V29, P1011, DOI 10.1007/s00371-012-0753-5
   Su Z, 2013, IEEE T MULTIMEDIA, V15, P535, DOI 10.1109/TMM.2012.2237025
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Vese LA, 2003, J SCI COMPUT, V19, P553, DOI 10.1023/A:1025384832106
   Vese LA, 2004, J MATH IMAGING VIS, V20, P7, DOI 10.1023/B:JMIV.0000011316.54027.6a
   Xiao F, 2020, IEEE T EMERG TOP COM, V8, P752, DOI [10.1109/JPHOT.2018.2827165, 10.1109/TETC.2018.2790080]
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Zhang Q, 2014, LECT NOTES COMPUT SC, V8691, P815, DOI 10.1007/978-3-319-10578-9_53
   Zhang ZY, 2021, SIGNAL PROCESS-IMAGE, V96, DOI 10.1016/j.image.2021.116308
   Zhano M, 2008, IEEE T IMAGE PROCESS, V17, P2324, DOI 10.1109/TIP.2008.2006658
   Zhu LF, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366146
NR 44
TC 5
Z9 5
U1 3
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 2035
EP 2049
DI 10.1007/s00371-022-02462-w
EA APR 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000781250600001
DA 2024-07-18
ER

PT J
AU Ince, IF
   Bulut, F
   Kilic, I
   Yildirim, ME
   Ince, OF
AF Ince, Ibrahim Furkan
   Bulut, Faruk
   Kilic, Ilker
   Yildirim, Mustafa Eren
   Ince, Omer Faruk
TI Low dynamic range discrete cosine transform (LDR-DCT) for
   high-performance JPEG image compression
SO VISUAL COMPUTER
LA English
DT Article
DE Low dynamic range; Wavelet transform; Round-off error; Loss of
   information; Quantization factors; Lossless image compression
ID APPROXIMATE DCT; ALGORITHM; EFFICIENT
AB In mathematical theory, the discrete cosine transform (DCT) is a lossless orthogonal transformation method which means it outputs exactly the same values of the input after the inverse transformation. However, this is impossible in today's technology due to the limited capacity of processors in which the maximum value that a number can take is 264 - 1 (20-digit number) in a 64-bit register. Since the DCT employs the floating values higher than this precision, there occurs a round-off error which causes a particular loss of information after the inverse transformation. For this reason, the dynamic range of the DCT coefficients should be reduced so that fewer precision digits are employed in the DCT calculations, thereby the round-off error and loss of information are minimized. In this study, conventional DCT equations are improved both in forward and inverse transformation for the sake of high-performance JPEG image compression. The proposed method reduces the dynamic range of the DCT coefficients and provides a low dynamic range DCT (LDR-DCT) by weighting the DCT coefficients with respect to the frequency level. The effectiveness of the proposed LDR-DCT method is experimented mainly by observing the inter-correlation between the compression ratio and the peak signal-to-noise ratio (PSNR) values which is defined as the compression performance (CP). An extensive experimental benchmarking study is done using the publicly available KODAK image dataset in both grayscale and RGB color spaces, separately. According to the experimental results, the average compression performance (CP) is increased up to about 26% in grayscale images and about 17% in RGB images when the quantization factors (21-121) are employed in the quantization process. Additionally, it is observed that there is an average increment in the compression performance (CP) up to about 8% in grayscale images and about 7% in RGB images when the standard IrfanView quantization tables (quality level of 40 to the quality level of 90) are applied. On the other hand, in the absence of quantization when either the quantization factor of 1 or the standard IrfanView quantization table with the quality level of 100 is applied, it is also observed that there is an average increment in the PSNR value up to about 15% in grayscale images and about 33% in RGB images with respect to the average PSNR values of 24 images in the KODAK image dataset. Therefore, though the proposed LDR-DCT method without quantization does not change the compression ratio, it improves the quality of the output obtained after the inverse transform dramatically. In other words, the conventional DCT method should be replaced by the proposed LDR-DCT method in certain areas where compression is not required. Besides, the study claims that the proposed LDR-DCT method can provide at least the same JPEG image quality as the conventional DCT method with much higher compression ratios if the quantization tables are redesigned accordingly.
C1 [Ince, Ibrahim Furkan; Yildirim, Mustafa Eren] Kyungsung Univ, Dept Elect Engn, Busan, South Korea.
   [Ince, Ibrahim Furkan] Nisantasi Univ, Dept Digital Game Design, Istanbul, Turkey.
   [Bulut, Faruk] Istanbul Arel Univ, Dept Comp Engn, Istanbul, Turkey.
   [Kilic, Ilker] Pamukkale Univ, Dept Elect & Elect Engn, Denizli, Turkey.
   [Yildirim, Mustafa Eren] Bahcesehir Univ, Dept Elect & Elect Engn, Istanbul, Turkey.
   [Ince, Omer Faruk] Korea Inst Sci & Technol, Ctr Intelligent & Interact Robot, Seoul 02792, South Korea.
   [Ince, Omer Faruk] Nisantasi Univ, Dept Mech Engn, Istanbul, Turkey.
C3 Kyungsung University; Istanbul Nisantasi University; Istanbul Arel
   University; Pamukkale University; Bahcesehir University; Korea Institute
   of Science & Technology (KIST); Istanbul Nisantasi University
RP Bulut, F (corresponding author), Istanbul Arel Univ, Dept Comp Engn, Istanbul, Turkey.
EM furkanince@ks.ac.kr; farukbulut@arel.edu.tr; ilkerk@pau.edu.tr;
   mustafaeren.yildirim@eng.bau.edu.tr; 023967@kist.re.kr
RI Ince, Ibrahim Furkan/L-8862-2017; BULUT, Faruk/P-6693-2017; Yildirim,
   Mustafa Eren/G-3445-2017
OI Ince, Ibrahim Furkan/0000-0003-1570-875X; BULUT,
   Faruk/0000-0003-2960-8725; South, Set/0000-0002-3881-3168; Yildirim,
   Mustafa Eren/0000-0002-0662-2770; KILIC, ILKER/0000-0003-3978-4829;
   INCE, Omer Faruk/0000-0002-8165-8335
CR Alkachouh Z, 2000, IEEE T IMAGE PROCESS, V9, P729, DOI 10.1109/83.841948
   Almurib HAF, 2018, IEEE T COMPUT, V67, P149, DOI 10.1109/TC.2017.2731770
   Alshehri SA, 2021, MULTIMED TOOLS APPL, V80, P367, DOI 10.1007/s11042-020-09038-7
   An D., 2020, WIREL COMMUN MOBILE
   Baig MH, 2017, COMPUT VIS IMAGE UND, V164, P111, DOI 10.1016/j.cviu.2017.01.010
   Bharadwaj NA, 2021, 2021 INTERNATIONAL CONFERENCE ON EMERGING SMART COMPUTING AND INFORMATICS (ESCI), P110, DOI 10.1109/ESCI50559.2021.9396904
   Brahimi N, 2020, MULTIMED TOOLS APPL, V79, P7615, DOI 10.1007/s11042-019-08325-2
   Bulut F, 2022, VISUAL COMPUT, V38, P2239, DOI 10.1007/s00371-021-02281-5
   Cai CL, 2020, IEEE T IMAGE PROCESS, V29, P3442, DOI 10.1109/TIP.2019.2960869
   CCITT Recommendation, 1992, T 81 INT TEL UN INF
   Cintra RJ, 2011, IEEE SIGNAL PROC LET, V18, P579, DOI 10.1109/LSP.2011.2163394
   Dagher I, 2018, INT J IMAG SYST TECH, V28, P274, DOI 10.1002/ima.22286
   Dai JY, 2021, QUANTUM INF PROCESS, V20, DOI 10.1007/s11128-021-03187-w
   Gupta N., 2020, EAI END T ENERGY WEB, DOI 10.4108/eai.13-7-2018.163976
   Haweel RT, 2016, J VIS COMMUN IMAGE R, V40, P357, DOI 10.1016/j.jvcir.2016.07.003
   Haweel TI, 2001, SIGNAL PROCESS, V81, P2309, DOI 10.1016/S0165-1684(01)00106-2
   Hussain AJ, 2018, NEUROCOMPUTING, V300, P44, DOI 10.1016/j.neucom.2018.02.094
   Ince IF, 2019, ELECTRONICS-SWITZ, V8, DOI 10.3390/electronics8090936
   ITU, 1992, ITU T RECOMM T
   Jiang F, 2018, IEEE T CIRC SYST VID, V28, P3007, DOI 10.1109/TCSVT.2017.2734838
   Jridi M, 2015, IEEE T CIRCUITS-I, V62, P449, DOI 10.1109/TCSI.2014.2360763
   Khalaf W, 2019, ALGORITHMS, V12, DOI 10.3390/a12120255
   Khan S, 2019, MEAS CONTROL-UK, V52, P1532, DOI 10.1177/0020294019877508
   Li J, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/749172
   Li M, 2021, IEEE T PATTERN ANAL, V43, P3446, DOI 10.1109/TPAMI.2020.2983926
   Lin TJ, 2013, J SYST SOFTWARE, V86, P604, DOI 10.1016/j.jss.2012.10.922
   Liu D., 2018, MULTIMEDIA MODELING, P5
   Liu SZ, 2002, IEEE T CIRC SYST VID, V12, P1139, DOI 10.1109/TCSVT.2002.806819
   Messaoudi A, 2019, SIGNAL IMAGE VIDEO P, V13, P1441, DOI 10.1007/s11760-019-01492-7
   Mishra D, 2021, IEEE T CIRC SYST VID, V31, P1452, DOI 10.1109/TCSVT.2020.3010627
   Mukherjee J, 2008, IEEE T IMAGE PROCESS, V17, P1783, DOI 10.1109/TIP.2008.2002826
   Oliveira RS, 2019, MULTIDIM SYST SIGN P, V30, P1363, DOI 10.1007/s11045-018-0601-5
   Othman S, 2021, ADV ELECTR COMPUT EN, V21, P91, DOI 10.4316/AECE.2021.01010
   Pang CY, 2019, INFORM SCIENCES, V473, P121, DOI 10.1016/j.ins.2018.08.067
   Saha M, 2020, LECT NOTE NETW SYST, V79, P185, DOI 10.1007/978-981-32-9453-0_19
   Santos L, 2012, IEEE J-STARS, V5, P451, DOI 10.1109/JSTARS.2011.2173906
   Siddeq MM, 2020, SENS IMAGING, V21, DOI 10.1007/s11220-020-00302-6
   Song HS, 2009, IEEE SIGNAL PROC LET, V16, P410, DOI 10.1109/LSP.2009.2016010
   Strang G, 1999, SIAM REV, V41, P135, DOI 10.1137/S0036144598336745
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tang JS, 2004, DIGIT SIGNAL PROCESS, V14, P218, DOI 10.1016/j.dsp.2003.06.001
   Thayammal S., 2014, P INT C EL COMM SYST, P1, DOI DOI 10.1109/ECS.2014.6892610
   Touil DE, 2021, MULTIMED TOOLS APPL, V80, P9547, DOI 10.1007/s11042-020-09754-0
   Vyas A, 2018, SIGNALS COMMUN TECHN, P3, DOI 10.1007/978-981-10-7272-7_1
   Wang YH, 2017, IEEE T IMAGE PROCESS, V26, P3360, DOI 10.1109/TIP.2017.2678798
   Zhao C., 2019, PROC IEEE INT C COMM, P1
NR 46
TC 9
Z9 9
U1 4
U2 35
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2022
VL 38
IS 5
BP 1845
EP 1870
DI 10.1007/s00371-022-02418-0
EA MAR 2022
PG 26
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0M9JP
UT WOS:000766099400002
DA 2024-07-18
ER

PT J
AU Wan, KF
   Wang, JM
   Li, B
   Chen, DQ
   Tian, LY
AF Wan, Kaifang
   Wang, Jianmei
   Li, Bo
   Chen, Daqing
   Tian, Linyu
TI Object feature selection under high-dimension and few-shot data based on
   three-way decision
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT ACIAT202
CY 2022
CL ELECTR NETWORK
DE Three-way decision; Few shot; Feature extraction; Feature selection
ID ALGORITHM; SIMILARITY; RELIEF
AB A feature selection method based on three-way decision is proposed to solve the problem of target recognition under high-dimension and few-shot data. The main research results of this paper include the following aspects: (1) aiming at the uncertainty of existing algorithm caused by uncertain data in high-dimension and few-shot data, based on the traditional filtering feature selection ReliefF algorithm, the algorithm is improved, according to the three-way decision thought, the single threshold of original algorithm is extended into two thresholds, to increase the rate of fault tolerance of the original algorithm, avoid features is too much or too little to eliminate. (2) To solve the problem of "dimension disaster" of high-dimension and few-shot data, the features are divided into three domains according to two thresholds and feature weights, and then the features in the boundary domain are further selected by combining the wrapper feature selection idea to reduce the dimension and obtain the optimal feature subset. Finally, the experimental results show that the recognition accuracy and F1 have been improved. Experiment 1 proved the fault tolerance rate and stability of the proposed algorithm, and Experiment 2 verified the advantages of the proposed algorithm in small sample conditions.
C1 [Wan, Kaifang; Wang, Jianmei; Li, Bo] Northwestern Polytech Univ, Sch Elect & Informat, Xian 710072, Peoples R China.
   [Chen, Daqing] London South Bank Univ, Sch Engn, London SE1 0AA, England.
   [Tian, Linyu] AVIC Xian Aeronaut Comp Tech Res Inst, Xian 710068, Peoples R China.
C3 Northwestern Polytechnical University; London South Bank University;
   Aviation Industry Corporation of China (AVIC)
RP Li, B (corresponding author), Northwestern Polytech Univ, Sch Elect & Informat, Xian 710072, Peoples R China.
EM wankaifang@nwpu.edu.cn; 1344617960@qq.com; libo803@nwpu.edu.cn;
   tly19960919@163.com
RI Wan, Kaifang/AAL-8395-2020; Li, bo/IWL-9318-2023; Wang,
   Jianmei/HHZ-0585-2022; Li, Bo/M-2341-2019; Chen, Daqing/AAD-8630-2019;
   li, xiaomin/KCX-9845-2024
OI Li, Bo/0000-0002-1415-4444; 
FU National Natural Science Foundation of China [61573285, 62003267]; Open
   Fund of Key Laboratory of Data Link Technology of China Electronics
   Technology Group Corporation [CLDL-20182101]; Natural Science Foundation
   of Shaanxi Province [2020JQ-220]
FX The authors would like to acknowledge National Natural Science
   Foundation of China (Grant No.61573285, No.62003267), Open Fund of Key
   Laboratory of Data Link Technology of China Electronics Technology Group
   Corporation (Grant No. CLDL-20182101) and Natural Science Foundation of
   Shaanxi Province (Grant No. 2020JQ-220) to provide fund for conducting
   experiments.
CR Ai JQ, 2019, IEEE T GEOSCI REMOTE, V57, P10070, DOI 10.1109/TGRS.2019.2931308
   [Anonymous], 1995, STORAGE RETRIEVAL IM, DOI DOI 10.1117/12.205308
   Biglari M, 2020, INT J ENG-IRAN, V33, P213, DOI 10.5829/ije.2020.33.02b.05
   Chen YJ, 2021, IEEE T IMAGE PROCESS, V30, P4008, DOI 10.1109/TIP.2021.3068645
   Cheng G, 2017, P IEEE, V105, P1865, DOI 10.1109/JPROC.2017.2675998
   Eiras-Franco C, 2021, INT J INTELL SYST, V36, P6161, DOI 10.1002/int.22546
   Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Hong Jiguang, 1984, Acta Automatica Sinica, V10, P22
   Hu J, 2016, KNOWL-BASED SYST, V94, P13, DOI 10.1016/j.knosys.2015.10.025
   Jiang SY, 2016, INFORM PROCESS LETT, V116, P203, DOI 10.1016/j.ipl.2015.07.005
   Kononenko I, 1997, APPL INTELL, V7, P39, DOI 10.1023/A:1008280620621
   Li C., 2013, RES FEATURE SELECTIO
   Li X-X., 2020, ANHUI AGR SCI B, V26, P85
   Li Y., 2008, J CHANGCHUN U TECHNO, V29, P78
   [刘丽 LIU li], 2009, [中国图象图形学报, Journal of Image and Graphics], V14, P622
   Luukka P, 2011, EXPERT SYST APPL, V38, P4600, DOI 10.1016/j.eswa.2010.09.133
   NIBLACK W, 1993, P SOC PHOTO-OPT INS, V1908, P173
   Ponce H., 2021, J. Artif. Intell. Technol, V1, P131, DOI [10.37965/jait.2021.0006, DOI 10.37965/JAIT.2021.0006]
   Shuchun Y., 2020, COMPUTER APPL SOFTW, V37, P247
   Silui C., 2006, COMPUT ENG APPL, V21, P159
   Smith A.R., 1978, P 5 ANN C COMP GRAPH, V12, P12, DOI [10.1145/965139.807361, DOI 10.1145/965139.807361]
   Tu ZG, 2019, IEEE T CIRC SYST VID, V29, P1423, DOI 10.1109/TCSVT.2018.2830102
   Tu ZG, 2019, IEEE T IMAGE PROCESS, V28, P2799, DOI 10.1109/TIP.2018.2890749
   Tu ZG, 2018, PATTERN RECOGN, V79, P32, DOI 10.1016/j.patcog.2018.01.020
   Wang HB, 2021, INFORMATION, V12, DOI 10.3390/info12060228
   Wang Z., 2011, J JISHOU U NAT SCI, V5, P43
   Xu X., 2018, COMPUT RES DEV, V55, P229
   Yang X, 2021, J ARTIFICIAL INTELLI, V1, P51, DOI [DOI 10.37965/JAIT.2020.0051, 10.37965/jait.2020.0051]
   Yao YY, 2021, APPL INTELL, V51, P6298, DOI 10.1007/s10489-020-02142-z
   Yao YY, 2021, GRANULAR COMPUT, V6, P133, DOI 10.1007/s41066-020-00211-9
   Zhang ZH, 2017, PATTERN RECOGN LETT, V87, P139, DOI 10.1016/j.patrec.2016.08.005
   Zheng JY, 2019, SYST SCI CONTROL ENG, V7, P304, DOI 10.1080/21642583.2019.1661312
   Zhou Y., 2007, COMPUT ENG APPL, V43, P171
NR 34
TC 0
Z9 0
U1 4
U2 39
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2023
VL 39
IS 6
SI SI
BP 2261
EP 2275
DI 10.1007/s00371-022-02411-7
EA FEB 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA J1YY4
UT WOS:000756390700001
DA 2024-07-18
ER

PT J
AU Hua, XY
   Wang, MH
   Su, BN
   Liu, X
AF Hua, Xiyao
   Wang, Minghui
   Su, Boni
   Liu, Xia
TI Hierarchical feature fusion network for light field spatial
   super-resolution
SO VISUAL COMPUTER
LA English
DT Article
DE Light field; Super-resolution; Hierarchical feature; Residual
   spatial-angular separable convolution
AB Light field (LF) spatial super-resolution (SR) aims to restore a high-resolution LF image from a degraded low-resolution one. However, due to the complexity of high-dimensional LF images, the existing LF spatial SR methods failed to fully incorporate the correlation between sub-aperture images of the LF. To mitigate this problem, we propose a hierarchical feature fusion network (LF-HFNet) for LF spatial SR with two novel components, namely feature interaction module and residual spatial and channel attention block. By cascading several residual spatial-angular separable convolution blocks with concatenation connections, the former can fully utilize the hierarchical and complementary information between SAIs. And the latter can adaptively rescale the feature responses for emphasizing informative features. Experimental results on both synthetic and real-world LF datasets demonstrate that the proposed method outperforms other state-of-the-art methods with higher PSNR/SSIM.
C1 [Hua, Xiyao; Wang, Minghui; Liu, Xia] Sichuan Univ, Coll Comp Sci, Chengdu, Peoples R China.
   [Su, Boni] Sichuan Univ Arts & Sci, Coll Intelligent Mfg, Dazhou, Peoples R China.
C3 Sichuan University; Sichuan University of Arts & Science
RP Wang, MH (corresponding author), Sichuan Univ, Coll Comp Sci, Chengdu, Peoples R China.
EM wangminghui_lx@163.com
FU Science and Technology Plan Project of Sichuan Province [2021YFG0350];
   National Key Research and Development Program of China [2016YFB0700802];
   Innovative Youth Fund Program of the State Oceanic Administration of
   China [2015001]; Sichuan Dazhou Intelligent Manufacturing Industry
   Technology Research Institute [ZNZZ21]
FX This work was supported in part by the Science and Technology Plan
   Project of Sichuan Province under Grant 2021YFG0350, in part by the
   National Key Research and Development Program of China underGrant
   2016YFB0700802, in part by the Innovative Youth Fund Program of the
   State Oceanic Administration of China under Grant 2015001 and in part by
   Sichuan Dazhou Intelligent Manufacturing Industry Technology Research
   Institute under Grant ZNZZ21.
CR Alain M, 2018, IEEE IMAGE PROC, P2501, DOI 10.1109/ICIP.2018.8451162
   [Anonymous], 2016, NIPS
   BiBi S, 2020, MATHEMATICS-BASEL, V8, DOI 10.3390/math8060967
   Bibi SMA, 2019, IEEE ACCESS, V7, P165779, DOI 10.1109/ACCESS.2019.2953496
   Bishop TE, 2012, IEEE T PATTERN ANAL, V34, P972, DOI 10.1109/TPAMI.2011.168
   Cheng Z, 2021, PROC CVPR IEEE, P10005, DOI 10.1109/CVPR46437.2021.00988
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Egiazarian K, 2015, EUR SIGNAL PR CONF, P2849, DOI 10.1109/EUSIPCO.2015.7362905
   Honauer K, 2017, LECT NOTES COMPUT SC, V10113, P19, DOI 10.1007/978-3-319-54187-7_2
   Jiating Jin, 2020, 2020 IEEE 5th International Conference on Image, Vision and Computing (ICIVC), P193, DOI 10.1109/ICIVC50857.2020.9177480
   Jin J, 2020, AAAI CONF ARTIF INTE, V34, P11141
   Jin J, 2020, PROC CVPR IEEE, P2257, DOI 10.1109/CVPR42600.2020.00233
   Kalantari NK, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980251
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Maqsood S, 2020, MATH PROBL ENG, V2020, DOI 10.1155/2020/4036434
   Meng N, 2020, AAAI CONF ARTIF INTE, V34, P11757
   Mitra Kaushik, 2012, 2012 IEEE COMPUTER S, P22
   Raj A.S., 2016, Stanford lytro light field archive
   Rossi M, 2018, IEEE T IMAGE PROCESS, V27, P4207, DOI 10.1109/TIP.2018.2828983
   Shi JL, 2019, IEEE T IMAGE PROCESS, V28, P5867, DOI 10.1109/TIP.2019.2923323
   Shi WL, 2021, VISUAL COMPUT, V37, P1569, DOI 10.1007/s00371-020-01903-8
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shin C., 2018, EPINET FULLY CONVOLU
   Sun Q., 2020, ARXIV PREPRINT ARXIV
   Usman M, 2020, J ADV MECH DES SYST, V14, DOI 10.1299/jamdsm.2020jamdsm0048
   Wang YQ, 2021, IEEE T IMAGE PROCESS, V30, P1057, DOI 10.1109/TIP.2020.3042059
   Wang YL, 2018, IEEE T IMAGE PROCESS, V27, P4274, DOI 10.1109/TIP.2018.2834819
   Wanner S, 2014, IEEE T PATTERN ANAL, V36, P606, DOI 10.1109/TPAMI.2013.147
   Yeung HWF, 2019, IEEE T IMAGE PROCESS, V28, P2319, DOI 10.1109/TIP.2018.2885236
   Yingqian Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P290, DOI 10.1007/978-3-030-58592-1_18
   Yoon Y, 2017, IEEE SIGNAL PROC LET, V24, P848, DOI 10.1109/LSP.2017.2669333
   Yoon Y, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P57, DOI 10.1109/ICCVW.2015.17
   Yuan Y, 2018, IEEE SIGNAL PROC LET, V25, P1359, DOI 10.1109/LSP.2018.2856619
   Yücer K, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2876504
   Zhang J, 2020, IEEE T IMAGE PROCESS, V29, P4421, DOI 10.1109/TIP.2020.2970529
   Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344
   Zhang SF, 2019, PROC CVPR IEEE, P919, DOI 10.1109/CVPR.2019.00101
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
NR 41
TC 1
Z9 1
U1 1
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 267
EP 279
DI 10.1007/s00371-021-02327-8
EA FEB 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000750684900001
DA 2024-07-18
ER

PT J
AU Zhang, SD
   Zhang, JT
   He, FZ
   Hou, N
AF Zhang, Shengdong
   Zhang, Jiaoting
   He, Fazhi
   Hou, Neng
TI DRDDN: dense residual and dilated dehazing network
SO VISUAL COMPUTER
LA English
DT Article
DE Dense residual learning; Dehazing; Dilated densely connected block
ID RESTORATION; IMAGES
AB Recently, deep convolutional neural networks (CNNs) have made great achievements in image restoration. However, there exists a large space to improve the performance of CNN-based dehazing model. In this paper, we design a fully end-to-end dehazing network, which can be called as dense residual and dilated dehazing network (DRDDN), for single image dehazing. In detail, a dilated densely connected block is designed to fully exploit multi-scale features through an adaptive learning process. The receptive field of the network is enlarged by dilation convolution without losing spatial information. Furthermore, we use deep residual to propagate the low-level features to high-level layers. Therefore, our model can fully exploit both low-level and high-level features for dehazing. Experiments on benchmark datasets and real-world hazy images show that the proposed DRDDN achieves favorable performance against the state-of-the-art methods.
C1 [Zhang, Shengdong] Shaoxing Univ, Comp Sci Engn Dept, Shaoxing 312000, Peoples R China.
   [He, Fazhi] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
   [Zhang, Jiaoting] Civil Aviat Univ China, Coll Elect Informat & Automat, Tianjin 300300, Peoples R China.
   [Hou, Neng] Yangtze Univ, Sch Comp Sci, Jingzhou 434023, Peoples R China.
C3 Shaoxing University; Wuhan University; Civil Aviation University of
   China; Yangtze University
RP He, FZ (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
EM fzhe@whu.edu.cn
RI He, Fazhi/Q-3691-2018
FU National Key Research and Develop Program of China [2017YFB0503004]
FX Funding was provided by National Key Research and Develop Program of
   China (Grant No. 2017YFB0503004).
CR Ancuti CO, 2019, IEEE IMAGE PROC, P1014, DOI [10.1109/icip.2019.8803046, 10.1109/ICIP.2019.8803046]
   Ancuti CO, 2018, IEEE COMPUT SOC CONF, P867, DOI 10.1109/CVPRW.2018.00119
   Ancuti C, 2016, IEEE IMAGE PROC, P2226, DOI 10.1109/ICIP.2016.7532754
   [Anonymous], CoRR abs/1511.07122
   [Anonymous], 2014, P INT C LEARN REPR
   Berman D, 2017, IEEE INT CONF COMPUT, P115
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Bianco S, 2019, IEEE COMPUT SOC CONF, P1927, DOI 10.1109/CVPRW.2019.00244
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen C, 2016, LECT NOTES COMPUT SC, V9906, P576, DOI 10.1007/978-3-319-46475-6_36
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Chen ZY, 2021, PROC CVPR IEEE, P7176, DOI 10.1109/CVPR46437.2021.00710
   Dudhane A, 2019, IEEE COMPUT SOC CONF, P2014, DOI 10.1109/CVPRW.2019.00253
   Fahim MANI, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10070817
   Fan CJ, 2022, IEEE T INTELL TRANSP, V23, P13559, DOI 10.1109/TITS.2021.3125737
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Guo TT, 2019, IEEE COMPUT SOC CONF, P2131, DOI 10.1109/CVPRW.2019.00266
   Hautiere Nicolas, 2008, Image Analysis & Stereology, V27, P87, DOI 10.5566/ias.v27.p87-95
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2009, PROC CVPR IEEE, P1956, DOI [10.1109/CVPRW.2009.5206515, 10.1109/CVPR.2009.5206515]
   He Z., 2019, IEEE T MULTIMED, VPP, P1
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li LRH, 2020, IEEE T IMAGE PROCESS, V29, P2766, DOI 10.1109/TIP.2019.2952690
   Li Q, 2020, KNOWL-BASED SYST, V194, DOI 10.1016/j.knosys.2020.105488
   Li SY, 2021, INT J COMPUT VISION, V129, P1301, DOI 10.1007/s11263-020-01416-w
   Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Morales P, 2019, IEEE COMPUT SOC CONF, P2078, DOI 10.1109/CVPRW.2019.00260
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Ren W., 2018, NEURIPS, P297
   Ren WQ, 2022, IEEE T PATTERN ANAL, V44, P3974, DOI 10.1109/TPAMI.2021.3061604
   Ren WQ, 2020, INT J COMPUT VISION, V128, P240, DOI 10.1007/s11263-019-01235-8
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P1895, DOI 10.1109/TIP.2018.2876178
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Saini M, 2012, IEEE T MULTIMEDIA, V14, P555, DOI 10.1109/TMM.2012.2186957
   Schechner YY, 2001, PROC CVPR IEEE, P325
   Shwartz S., 2006, 2006 IEEE COMP SOC C, V2, P1984, DOI DOI 10.1109/CVPR.2006.71
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Sulami M, 2014, IEEE INT CONF COMPUT
   Tan X, 2021, IEEE T IMAGE PROCESS, V30, P9085, DOI 10.1109/TIP.2021.3122004
   Tan X, 2018, IEEE INT CON MULTI, DOI 10.1109/INTMAG.2018.8508427
   Tang KT, 2014, PROC CVPR IEEE, P2995, DOI 10.1109/CVPR.2014.383
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463
   Vadrevu P, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI 10.14722/ndss.2017.23100
   Yan YY, 2019, IEEE T INF FOREN SEC, V14, P5, DOI 10.1109/TIFS.2018.2834155
   Yang D, 2018, LECT NOTES COMPUT SC, V11211, P729, DOI 10.1007/978-3-030-01234-2_43
   Yu F, 2017, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2017.75
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang JA, 2022, IEEE T CYBERNETICS, V52, P11187, DOI 10.1109/TCYB.2021.3070310
   Zhang S., 2018, ICME FEED NET
   Zhang S., 2017, PAC RIM C MULT, P315
   Zhang S. L., 2016, arXiv
   Zhang SD, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P123, DOI 10.1145/3038912.3052661
   Zhang SD, 2023, IEEE T CYBERNETICS, V53, P454, DOI 10.1109/TCYB.2021.3124231
   Zhang SD, 2020, NEUROCOMPUTING, V410, P363, DOI 10.1016/j.neucom.2020.06.041
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhang SD, 2020, VISUAL COMPUT, V36, P305, DOI 10.1007/s00371-018-1612-9
   Zhang XQ, 2020, COMPUT VIS IMAGE UND, V197, DOI 10.1016/j.cviu.2020.103003
   Zhang YF, 2017, IEEE IMAGE PROC, P3205, DOI 10.1109/ICIP.2017.8296874
   Zheng ZR, 2021, PROC CVPR IEEE, P16180, DOI 10.1109/CVPR46437.2021.01592
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
   Zhu YY, 2018, NEUROCOMPUTING, V275, P499, DOI 10.1016/j.neucom.2017.08.055
NR 71
TC 10
Z9 11
U1 8
U2 36
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 953
EP 969
DI 10.1007/s00371-021-02377-y
EA JAN 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000741976300002
DA 2024-07-18
ER

PT J
AU Isumi, R
   Yamamoto, K
   Noma, T
AF Isumi, Ryoma
   Yamamoto, Kunio
   Noma, Tsukasa
TI Color2Hatch: conversion of color to hatching for low-cost printing
SO VISUAL COMPUTER
LA English
DT Article
DE Decolorization; Color to hatching; Vector image processing; Low-cost
   black and white printing
ID DECOLORIZATION; IMAGES
AB In this paper, we propose Color2Hatch, a decolorization method for business/presentation graphics. In Color2Hatch, each region represented as a closed path and uniformly colored in scalable vector graphics (SVG) is converted to a region hatched in black and white. From the characteristics of business graphics, the hatching patterns are designed to represent mainly the hue in the region; additionally, lightness and saturation can also be reflected. To discriminate subtle differences between colors, attached short line segments, zigzag lines, and wave lines are used in hatching by analogy to a clock. Compared with the existing decolorization methods, for example, grayscale conversion and texturing, our method is superior in the discrimination of regions, suitable for low-cost black and white printing that meets real-world needs.
C1 [Isumi, Ryoma] Kyushu Inst Technol, Grad Sch Comp Sci & Syst Engn, Iizuka, Fukuoka 8208502, Japan.
   [Yamamoto, Kunio; Noma, Tsukasa] Kyushu Inst Technol, Sch Comp Sci & Syst Engn, Dept Artificial Intelligence, Iizuka, Fukuoka 8208502, Japan.
C3 Kyushu Institute of Technology; Kyushu Institute of Technology
RP Noma, T (corresponding author), Kyushu Inst Technol, Sch Comp Sci & Syst Engn, Dept Artificial Intelligence, Iizuka, Fukuoka 8208502, Japan.
EM noma@ai.kyutech.ac.jp
OI Noma, Tsukasa/0000-0002-6583-3957
CR Adobe Systems Incorporated, 1999, POSTSCRIPT LANG REF
   [Anonymous], 2017, SCALABLE VECTOR GRAP
   Bai Y., 2000, P SPIE, V4300, pVI, DOI [10.1117/12.410819, DOI 10.1117/12.410819]
   Bala R, 2004, P SOC PHOTO-OPT INS, V5293, P196, DOI 10.1117/12.532192
   Borgo R., 2013, STATE ART REPORTS, DOI [10.2312/conf/EG2013/stars/039-063, DOI 10.2312/CONF/EG2013/STARS/039-063]
   Cui M, 2010, VISUAL COMPUT, V26, P1349, DOI 10.1007/s00371-009-0412-7
   de Queiroz RL, 2006, IEEE T IMAGE PROCESS, V15, P1464, DOI 10.1109/TIP.2006.871181
   Economic Research Association, 2021, PRINT PRIC DAT 2021
   Gooch AA, 2005, ACM T GRAPHIC, V24, P634, DOI 10.1145/1073204.1073241
   Grundland M, 2007, PATTERN RECOGN, V40, P2891, DOI 10.1016/j.patcog.2006.11.003
   Guss H, 2016, P BRIDGES, V2016, P493
   Hara T, 2001, J IMAG SOC JPN, V40, P231
   Harrington S.J., 1994, COLOR HARD COPY GRAP, V2171, P305
   Harrington S.J., 1992, US Patent, Patent No. [5,153,576, 5153576]
   Institute ANS., 1979, LIN CONV LETT, V14, p2M
   Ji ZP, 2016, VISUAL COMPUT, V32, P1621, DOI 10.1007/s00371-015-1145-4
   Kuhn GR, 2008, VISUAL COMPUT, V24, P505, DOI 10.1007/s00371-008-0231-2
   Liu QG, 2019, VISUAL COMPUT, V35, P205, DOI 10.1007/s00371-017-1464-8
   Makita T., 2001, J IMAG SOC JPN, V41, P358
   Pickett R. M., 1988, Proceedings of the 1988 IEEE International Conference on Systems, Man, and Cybernetics (IEEE Cat. No.88CH2556-9), P514
   Rasche K, 2005, COMPUT GRAPH FORUM, V24, P423, DOI 10.1111/j.1467-8659.2005.00867.x
   Roetling P. G., 1981, Proceedings of the SPIE - The International Society for Optical Engineering, V310, P133, DOI 10.1117/12.932860
   Saito T., 1990, Computer Graphics, V24, P197, DOI 10.1145/97880.97901
   Salisbury M. P., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P401, DOI 10.1145/258734.258890
   Smith K, 2008, COMPUT GRAPH FORUM, V27, P193, DOI 10.1111/j.1467-8659.2008.01116.x
   The Imaging Society of Japan, 2018, INKJ REV VERS
   Wang L., 2013, P SPIE, V8652
   Wang W, 2018, IEEE T IMAGE PROCESS, V27, P5464, DOI 10.1109/TIP.2018.2855424
   Webb Matthew., 2002, Proceedings of the 2nd international symposium on Non- photorealistic animation and rendering, NPAR '02, P53, DOI DOI 10.1145/508530.508540
   Wu JL, 2012, VISUAL COMPUT, V28, P723, DOI 10.1007/s00371-012-0683-2
   Zhang XL, 2018, VISUAL COMPUT, V34, P1099, DOI 10.1007/s00371-018-1524-8
   Zhao HL, 2018, COMPUT GRAPH-UK, V70, P251, DOI 10.1016/j.cag.2017.07.009
   Zhu W, 2014, VISUAL COMPUT, V30, P299, DOI 10.1007/s00371-013-0854-9
NR 33
TC 3
Z9 3
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2021
VL 37
IS 12
SI SI
BP 3103
EP 3113
DI 10.1007/s00371-021-02268-2
EA AUG 2021
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP1DP
UT WOS:000683641300001
OA hybrid
DA 2024-07-18
ER

PT J
AU Xu, JW
   Cao, W
   Liu, B
   Jiang, KY
AF Xu, Jianwen
   Cao, Wei
   Liu, Bin
   Jiang, Kaiyong
TI Object restoration based on extrinsic reflective symmetry plane
   detection
SO VISUAL COMPUTER
LA English
DT Article
DE Object restoration; Reflective symmetry plane; Iterative closest point;
   Boolean operation
ID 3D; SHAPE; COMPLETION; CLASSIFICATION
AB Object restoration is applied in multitudinous fields. It is common that the models that need to be restored have extrinsic reflective symmetry features (ERSF), the detections of which are often applied to their restorations. However, most of these symmetry detection approaches have limitations, especially for the incompletion models with a greater area of missing part and much more complex and obvious features. Therefore, this research proposes a novel object restoration method (ORM-ERSPD) based on extrinsic reflective symmetry plane (ERSP) detection, which can be divided into two steps: extrinsic reflective symmetry plane detection (ERSPD) and object restoration (OR). During ERSPD, the reflected mesh can be computed based on an initial ERSP and aligned to the selected mesh by applying the iterative closest point algorithm. Thus, the reflective middle point set can be obtained between the selected mesh and the aligned reflected mesh to fit for the final ERSP. In OR, the selected mesh is first mirrored, aligned and deformed to the missing part whose boundary is reflected on the complete part for selecting and expanding the mesh. Then, the Boolean operation between the reflected mesh and the input mesh is conducted. Finally, the proposed ORM-ERSPD is applied to a set of incompletion models with global and local ERSF. The results of this research demonstrate that ORM-ERSPD can extract ERSP effectively and robustly and, thus, complete OR successfully.
C1 [Xu, Jianwen; Cao, Wei; Liu, Bin; Jiang, Kaiyong] Huaqiao Univ, Xiamen Key Lab Digital Vis Measurement, Fujian Prov Key Lab Special Energy Mfg, Xiamen 361021, Peoples R China.
C3 Huaqiao University
RP Jiang, KY (corresponding author), Huaqiao Univ, Xiamen Key Lab Digital Vis Measurement, Fujian Prov Key Lab Special Energy Mfg, Xiamen 361021, Peoples R China.
EM jiangky@hqu.edu.cn
RI Xu, Jianwen/AAQ-8600-2021; liu, bin/E-5431-2016
OI Xu, Jianwen/0000-0002-8789-745X; 
FU National Natural Science Foundation of China [51575196]; Pilot Project
   of Fujian Province [2020H0015]
FX This research was supported by the National Natural Science Foundation
   of China (Grant No. 51575196) and the Pilot Project of Fujian Province
   (Grant No. 2020H0015).
CR Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311
   Avetisyan A, 2019, IEEE I CONF COMP VIS, P2551, DOI 10.1109/ICCV.2019.00264
   Bischoff S, 2005, ACM T GRAPHIC, V24, P1332, DOI 10.1145/1095878.1095883
   Canul-Ku M, 2019, IEEE ACCESS, V7, P3298, DOI 10.1109/ACCESS.2018.2886791
   Chen CY, 2008, IEEE T VIS COMPUT GR, V14, P200, DOI 10.1109/TVCG.2007.70625
   Cheng L, 2018, IEEE J-STARS, V11, P285, DOI 10.1109/JSTARS.2017.2752765
   Foti S, 2020, LECT NOTES COMPUT SC, V12443, P198, DOI 10.1007/978-3-030-60365-6_19
   Gregor R., 2014, GCH EUROGRAPHICS, P135
   Guo XY, 2018, VISUAL COMPUT, V34, P93, DOI 10.1007/s00371-016-1316-y
   Harary G, 2014, COMPUT GRAPH FORUM, V33, P45, DOI 10.1111/cgf.12430
   Harary G, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2532548
   Hruda L., 2017, P CESCG 2017 21 CTR
   Hu T, 2019, IEEE INT CONF COMP V, P4114, DOI 10.1109/ICCVW.2019.00506
   Huang H, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366198
   Ji PL, 2019, MULTIMED TOOLS APPL, V78, P35471, DOI 10.1007/s11042-019-08043-9
   Ju T, 2004, ACM T GRAPHIC, V23, P888, DOI 10.1145/1015706.1015815
   Kakarala R, 2013, PROC CVPR IEEE, P249, DOI 10.1109/CVPR.2013.39
   Kazhdan Michael., 2004, COMPUT GRAPH FORUM, P115, DOI DOI 10.1145/1057432.1057448
   Kermi A, 2018, IET IMAGE PROCESS, V12, P1964, DOI 10.1049/iet-ipr.2017.1124
   Kobbelt L, 2020, IEEE T VIS COMPUT GR, V1, P1, DOI [10.1109/TVCG.2020.3010247, DOI 10.1109/TVCG.2020.3010247]
   Korman S, 2015, COMPUT GRAPH FORUM, V34, P2, DOI 10.1111/cgf.12454
   Kouichi M, 2017, P COMP GRAPH INT C
   Liepa Peter, 2003, Proceedings of Eurographics/ACM SIGGRAPH Symposium on Geometry Processing, P200, DOI DOI 10.2312/SGP/SGP03/200-206
   Lipman Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778840
   Lo KY, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618503
   Mao DH, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11050703
   Martinek M, 2015, VISUAL COMPUT, V31, P223, DOI 10.1007/s00371-014-1040-4
   Martinet A, 2006, ACM T GRAPHIC, V25, P439, DOI 10.1145/1138450.1138462
   Mavridis P, 2015, COMPUT GRAPH FORUM, V34, P13, DOI 10.1111/cgf.12741
   Mitra NJ, 2006, ACM T GRAPHIC, V25, P560, DOI 10.1145/1141911.1141924
   Nagar R, 2017, IEEE INT CONF COMP V, P1764, DOI 10.1109/ICCVW.2017.208
   Nealen A., 2006, P 4 INT C COMP GRAPH, P381, DOI DOI 10.1145/1174429.1174494
   Papaioannou G, 2017, ACM J COMPUT CULT HE, V10, DOI 10.1145/3009905
   Passalis G, 2007, VISUAL COMPUT, V23, P5, DOI 10.1007/s00371-006-0037-z
   Pauly M., 2005, Proceedings of Eurographics/ACM SIGGRAPH Symposium on Geometry Processing, P23
   Podolak J, 2006, ACM T GRAPHIC, V25, P549, DOI 10.1145/1141911.1141923
   Dang QV, 2014, 2014 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN CONTROL AND AUTOMATION (CICA), P176
   Reniers D, 2008, VISUAL COMPUT, V24, P383, DOI 10.1007/s00371-008-0220-5
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Schindler K., 2020, ARXIV200800096
   Sfikas K, 2018, COMPUT GRAPH-UK, V71, P208, DOI 10.1016/j.cag.2017.12.001
   Sfikas K, 2014, VISUAL COMPUT, V30, P1261, DOI 10.1007/s00371-014-0935-4
   Shi Yifei, 2020, ACM Transactions on Graphics (TOG), V39, P8
   Siang CV, 2019, 2019 IEEE CONFERENCE ON GRAPHICS AND MEDIA (GAME), P7, DOI [10.1109/GAME47560.2019.8980511, 10.1109/game47560.2019.8980511]
   Sipiran I, 2014, COMPUT GRAPH FORUM, V33, P131, DOI 10.1111/cgf.12481
   Speciale P, 2016, LECT NOTES COMPUT SC, V9912, P313, DOI 10.1007/978-3-319-46484-8_19
   Stutz D, 2020, INT J COMPUT VISION, V128, P1162, DOI 10.1007/s11263-018-1126-y
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Teo CL, 2015, IEEE I CONF COMP VIS, P1644, DOI 10.1109/ICCV.2015.192
   Thrun S, 2005, IEEE I CONF COMP VIS, P1824
   Wu YJ, 2003, LECT NOTES COMPUT SC, V2688, P515
   Xiao CX, 2007, VISUAL COMPUT, V23, P433, DOI 10.1007/s00371-007-0115-x
   Yasseen Z, 2017, VISUAL COMPUT, V33, P565, DOI 10.1007/s00371-016-1328-7
NR 53
TC 3
Z9 3
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3627
EP 3642
DI 10.1007/s00371-021-02192-5
EA JUL 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000679625200004
DA 2024-07-18
ER

PT J
AU Takahashi, S
   Sakurai, D
   Sasaki, M
   Miyamura, HN
   Sanada, Y
AF Takahashi, Shigeo
   Sakurai, Daisuke
   Sasaki, Miyuki
   Miyamura, Hiroko N.
   Sanada, Yukihisa
TI Visual analysis of geospatial multivariate data for investigating
   radioactive deposition processes
SO VISUAL COMPUTER
LA English
DT Article
DE Fukushima Daiichi nuclear power plant accident; Topographic analysis;
   Deposition processes; Continuous scatterplots
ID FIBER SURFACES; VISUALIZATION; EXPLORATION
AB The Fukushima nuclear accident of 2011 raised awareness of the importance of radioactive deposition processes, especially for proposing aerosol measures against possible air pollution. However, identifying these types of processes is often difficult due to complicated terrains. This paper presents an application study for identifying radioactive deposition processes by taking advantage of visual interaction with topographic data. The idea is to visually investigate the correspondence of the spatial positions to the air dose rate along with relevant attributes. This is accomplished by composing scatterplots of pairwise attributes, onto which we project terrain areas to interactively find specific patterns of such attributes. We applied our approach to the analysis of air dose rate distribution data around the Fukushima nuclear plant after the accident. Our visualization technique clearly distinguished contamination areas derived from different deposition processes and thus is useful for elucidation of the deposition process.
C1 [Takahashi, Shigeo] Univ Aizu, Dept Comp Sci & Engn, Aizu Wakamatsu, Fukushima 9658580, Japan.
   [Sakurai, Daisuke] Kyushu Univ, Pan Omics Data Driven Innovat Res Ctr, Res Inst Informat Technol, Kasuga, Fukuoka 8168580, Japan.
   [Sasaki, Miyuki; Sanada, Yukihisa] Japan Atom Energy Agcy, Sect Fukushima Res & Dev, Minami Soma 9750036, Japan.
   [Miyamura, Hiroko N.] Japan Atom Energy Agcy, Ctr Computat Sci & E Syst, Tokyo 1000011, Japan.
   [Miyamura, Hiroko N.] Japan Atom Energy Agcy, Nucl Human Resource Dev Ctr, Tokyo 1000011, Japan.
C3 University of Aizu; Kyushu University; Japan Atomic Energy Agency; Japan
   Atomic Energy Agency; Japan Atomic Energy Agency
RP Takahashi, S (corresponding author), Univ Aizu, Dept Comp Sci & Engn, Aizu Wakamatsu, Fukushima 9658580, Japan.
EM takahashis@acm.org; d.sakurai@ieee.org; sasaki.miyuki@jaea.go.jp;
   miyamura.hiroko@jaea.go.jp; sanada.yukihisa@jaea.go.jp
RI Sanada, Yukihisa/AAZ-9052-2020
OI Sanada, Yukihisa/0000-0002-5388-2444; Sakurai,
   Daisuke/0000-0002-0464-8619
FU JSPS KAKENHI [19H04120, 16H02825, 20K19809]; Grants-in-Aid for
   Scientific Research [20K19809, 19H04120, 16H02825] Funding Source: KAKEN
FX This work was partially supported by JSPS KAKENHI (Grant Numbers
   19H04120, 16H02825, and 20K19809) and is partially based on discussions
   at the 2021 IMI Joint Use Research Program Workshop (I) "Fiber Topology
   Meets Applications 2."
CR Andrienko G, 2010, COMPUT GRAPH FORUM, V29, P913, DOI 10.1111/j.1467-8659.2009.01664.x
   Bachthaler S, 2008, IEEE T VIS COMPUT GR, V14, P1428, DOI 10.1109/TVCG.2008.119
   Beecham R, 2016, COMPUT GRAPH FORUM, V35, P241, DOI 10.1111/cgf.12900
   Butkiewicz T, 2008, IEEE T VIS COMPUT GR, V14, P1165, DOI 10.1109/TVCG.2008.149
   Carr H, 2015, COMPUT GRAPH FORUM, V34, P241, DOI 10.1111/cgf.12636
   Carr H, 2014, IEEE T VIS COMPUT GR, V20, P1100, DOI 10.1109/TVCG.2013.269
   Chattopadhyay A, 2016, COMP GEOM-THEOR APPL, V58, P1, DOI 10.1016/j.comgeo.2016.05.006
   Duke D, 2012, IEEE T VIS COMPUT GR, V18, P2033, DOI 10.1109/TVCG.2012.287
   Edelsbrunner H., 2002, Foundations of Computational Mathematics, P37, DOI [DOI 10.1017/CBO9781139106962.003, 10.1017/CBO9781139106962.003, DOI 10.1017/CBO9781139106962.003.2]
   Edelsbrunner H, 2008, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SGG'08), P242, DOI 10.1145/1377676.1377720
   He Liu, 2011, 2011 IEEE Conference on Visual Analytics Science and Technology, P171, DOI 10.1109/VAST.2011.6102455
   Klacansky P, 2017, IEEE T VIS COMPUT GR, V23, P1782, DOI 10.1109/TVCG.2016.2570215
   Krüger R, 2013, COMPUT GRAPH FORUM, V32, P451, DOI 10.1111/cgf.12132
   Lehmann DJ, 2010, IEEE T VIS COMPUT GR, V16, P1291, DOI 10.1109/TVCG.2010.146
   Maciejewski R, 2010, IEEE T VIS COMPUT GR, V16, P205, DOI 10.1109/TVCG.2009.100
   Meulemans W, 2017, IEEE T VIS COMPUT GR, V23, P381, DOI 10.1109/TVCG.2016.2598542
   Saeki O, 2004, LECT NOTES MATH, V1854, P1
   Sakurai D, 2016, IEEE T VIS COMPUT GR, V22, P945, DOI 10.1109/TVCG.2015.2467433
   Sanada Y, 2019, J ENVIRON RADIOACTIV, V210, DOI 10.1016/j.jenvrad.2018.09.014
   Sanada Y, 2018, SCI TOTAL ENVIRON, V618, P881, DOI 10.1016/j.scitotenv.2017.08.246
   Sanada Y, 2017, BUNSEKI KAGAKU, V66, P149, DOI 10.2116/bunsekikagaku.66.149
   Sanada Y, 2014, EXPLOR GEOPHYS, V45, P3, DOI 10.1071/EG13004
   Sanada Yukihisa., 2014, Progress in Nuclear Science and Technology, V4, P76, DOI [10.15669/pnst.4.76, DOI 10.15669/PNST.4.76]
   Sopan A, 2012, GOV INFORM Q, V29, P223, DOI 10.1016/j.giq.2011.10.002
   Tatu A, 2012, IEEE CONF VIS ANAL, P63, DOI 10.1109/VAST.2012.6400488
   Tierny J, 2017, IEEE T VIS COMPUT GR, V23, P960, DOI 10.1109/TVCG.2016.2599017
   Tominski C, 2017, COMPUT GRAPH FORUM, V36, P173, DOI 10.1111/cgf.12871
   Turkay C, 2014, IEEE T VIS COMPUT GR, V20, P2033, DOI 10.1109/TVCG.2014.2346265
   Turkay C, 2012, IEEE T VIS COMPUT GR, V18, P2621, DOI 10.1109/TVCG.2012.256
   Watanabe K, 2015, IEEE PAC VIS SYMP, P287, DOI 10.1109/PACIFICVIS.2015.7156389
   Yasuda Y., 2017, WATER SCI, V61, P102, DOI [10.20820/suirikagaku.61.5_102, DOI 10.20820/SUIRIKAGAKU.61.5_102]
   Yates A, 2014, COMPUT GRAPH FORUM, V33, P301, DOI 10.1111/cgf.12386
   Yuan XR, 2013, IEEE T VIS COMPUT GR, V19, P2625, DOI 10.1109/TVCG.2013.150
NR 33
TC 0
Z9 0
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2021
VL 37
IS 12
SI SI
BP 3039
EP 3050
DI 10.1007/s00371-021-02248-6
EA JUL 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP1DP
UT WOS:000675047800001
OA hybrid, Green Published
DA 2024-07-18
ER

PT J
AU Nishimura, S
AF Nishimura, Satoshi
TI Grid-induced bounding volume hierarchy for ray tracing dynamic scenes
SO VISUAL COMPUTER
LA English
DT Article
DE Ray tracing; Bounding volume hierarchy; Hierarchical grids;
   Agglomerative clustering
ID BVH CONSTRUCTION
AB This paper proposes a novel method for accelerating ray tracing of animated scenes in which objects are moved, added, or deleted. The method uses two trees with different structures. The first tree is a hierarchical grid tree that is easily generatable from the primitive list and modifiable according to its change. The second tree is a bounding volume hierarchy (BVH) tree, constructed from the first tree with intra-node agglomerative clustering. When the scene is modified, the first tree is updated, and then, the second tree is partially rebuilt for accessed nodes in the first tree. The resulting BVH tree is deterioration-free; it always coincides with the tree generated by frame-wise rebuilding. For applications where a small portion of the scene is modified between frames, the tree formation cost of the proposed method is much lower than frame-wise rebuilding. Experiments showed that the quality of BVH trees generated by this method is comparable to those created by the binned surface area heuristic approach, and that the tree formation speed is faster not only for dynamic scenes but also for static scenes.
C1 [Nishimura, Satoshi] Univ Aizu, Sch Comp Sci & Engn, Aizu Wakamatsu, Fukushima 9658580, Japan.
C3 University of Aizu
RP Nishimura, S (corresponding author), Univ Aizu, Sch Comp Sci & Engn, Aizu Wakamatsu, Fukushima 9658580, Japan.
EM nisim@u-aizu.ac.jp
CR Amanatides J., 1987, EUROGRAPHICS, P3, DOI DOI 10.2312/EGTP.19871000
   [Anonymous], 2007, P 18 EUROGRAPHICS C
   Bittner J, 2015, COMPUT GRAPH-UK, V47, P135, DOI 10.1016/j.cag.2014.12.001
   Domingues Leonardo., 2015, Proceedings of High-Performance Graphics, P13
   FUJIMOTO A, 1986, IEEE COMPUT GRAPH, V6, P16, DOI 10.1109/MCG.1986.276715
   Ganestam P., 2015, J COMPUTER GRAPHICS, V4, P23
   Garanzha K, 2011, VISUAL COMPUT, V27, P697, DOI 10.1007/s00371-011-0593-8
   GOLDSMITH J, 1987, IEEE COMPUT GRAPH, V7, P14, DOI 10.1109/MCG.1987.276983
   Jevans D., 1989, Proceedings. Graphics Interface'89, P164
   Karras Tero., 2013, Proceedings of the 5th High-Performance Graphics Conference, P89, DOI DOI 10.1145/2492045.2492055
   Kopta D., 2012, P ACM SIGGRAPH S INT, P197, DOI DOI 10.1145/2159616.2159649
   Larsson T, 2006, COMPUT GRAPH-UK, V30, P450, DOI 10.1016/j.cag.2006.02.011
   Lauterbach C, 2009, COMPUT GRAPH FORUM, V28, P375, DOI 10.1111/j.1467-8659.2009.01377.x
   Lauterbach C, 2006, RT 06: IEEE SYMPOSIUM ON INTERACTIVE RAY TRACING 2006, PROCEEDINGS, P39
   Lext J, 2001, IEEE COMPUT GRAPH, V21, P22, DOI 10.1109/38.909012
   Lext J., 2001, EUROGRAPHICS 2001 SH
   Meister D, 2018, IEEE T VIS COMPUT GR, V24, P1345, DOI 10.1109/TVCG.2017.2669983
   Pantaleoni J., 2010, P C HIGH PERFORMANCE, P87
   Parker SG, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778803
   Reinhard E, 2000, SPRING COMP SCI, P299
   Wald I, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1186644.1186650
   Wald I, 2007, RT07: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2007, P33, DOI 10.1109/RT.2007.4342588
   Wald I, 2006, RT 06: IEEE SYMPOSIUM ON INTERACTIVE RAY TRACING 2006, PROCEEDINGS, P61
   Wald I, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601199
   Wald I, 2009, COMPUT GRAPH FORUM, V28, P1691, DOI 10.1111/j.1467-8659.2008.01313.x
   Walter B, 2008, RT08: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2008, PROCEEDINGS, P81, DOI 10.1109/RT.2008.4634626
   Yang X, 2016, COMPUT ANIMAT VIRT W, V27, P340, DOI 10.1002/cav.1717
NR 27
TC 0
Z9 0
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2021
VL 37
IS 12
SI SI
BP 2965
EP 2974
DI 10.1007/s00371-021-02205-3
EA JUL 2021
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP1DP
UT WOS:000668881000002
DA 2024-07-18
ER

PT J
AU Ma, C
   Sun, ZX
AF Ma, Chen
   Sun, Zhengxing
TI Multilayered stitch generating for random-needle embroidery
SO VISUAL COMPUTER
LA English
DT Article
DE Non-photorealistic rendering; Color dithering; Local texture growth;
   Stitch modeling
AB Random-needle embroidery is a kind of traditional Chinese embroidery where artists reproduce the original color image using multilayered intersecting stitches and overlapped deliberately. In other words, artists embroider intersecting stitches layer by layer to depict true color subjects. To create this embroidery art, the major challenge lies on evenly distribution of intersecting stitches to prevent the canvas in embroidery areas from being exposed. Meanwhile, intersecting stitches in each layer should be distributed as evenly as possible so that each layer of intersecting stitch can be quantified as a segment color to synthesize the target segment color. To tackle these challenges, we introduce local texture growth mechanism from example-based discrete element texture synthesis to the application of random-needle embroidery art stylization and propose a new local growth where a uniformity metric is designed to keep intersecting stitches nearly evenly distributed in each layer. In addition, we adopt a color dithering model to specify a color for each layer of monochromatic intersecting stitches at microscopic scale. We apply our method on a rich variety of images, and visually appealing random-needle embroidery images can be obtained in all cases.
C1 [Ma, Chen; Sun, Zhengxing] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China.
C3 Nanjing University
RP Sun, ZX (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China.
EM hnyjrh@163.com; szx@nju.edu.cn
RI Sun, Zhengxing/A-7411-2011
FU National Natural Science Foundation of China [42075139, 42077232,
   61272219]; National High Technology Research and Development Program of
   China [2007AA01Z334]; Science and technology program of Jiangsu Province
   [BE2020082, BE2010072, BE2011058, BY2012190]; China Postdoctoral Science
   Foundation [2017M621700]; Innovation Fund of State Key Laboratory for
   Novel Software Technology [ZZKT2018A09]
FX This work was supported by the National Natural Science Foundation of
   China (No. 42075139, 42077232, 61272219), the National High Technology
   Research and Development Program of China (No. 2007AA01Z334), the
   Science and technology program of Jiangsu Province (No. BE2020082,
   BE2010072, BE2011058, BY2012190), the China Postdoctoral Science
   Foundation (No. 2017M621700) and Innovation Fund of State Key Laboratory
   for Novel Software Technology (No. ZZKT2018A09).
CR Alves dos Passos Vladimir, 2010, 2010 Pacific Graphics (PG). Proceedings 18th Pacific Conference on Computer Graphics and Applications, P109, DOI 10.1109/PacificGraphics.2010.22
   Barla P, 2006, COMPUT GRAPH FORUM, V25, P663, DOI 10.1111/j.1467-8659.2006.00986.x
   Chen X., 2012, Proceedings of Graphics Interface 2012, P131, DOI DOI 10.5555/2305276.2305299
   Cui DL, 2017, COMPUT ANIMAT VIRT W, V28, DOI 10.1002/cav.1725
   Davison T, 2019, COMPUT GRAPH-UK, V78, P23, DOI 10.1016/j.cag.2018.10.016
   Derouet-Jourdan A, 2019, COMPUT GRAPH FORUM, V38, P255, DOI 10.1111/cgf.13635
   Hans, 2013, ACM T GRAPHIC, V23
   Huang H, 2011, VISUAL COMPUT, V27, P861, DOI 10.1007/s00371-011-0596-5
   Ijiri T, 2008, COMPUT GRAPH FORUM, V27, P429, DOI 10.1111/j.1467-8659.2008.01140.x
   Kang H, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P43
   Kang HW, 2006, VISUAL COMPUT, V22, P814, DOI 10.1007/s00371-006-0066-7
   Kyprianidis JE, 2013, IEEE T VIS COMPUT GR, V19, P866, DOI 10.1109/TVCG.2012.160
   Landes PE, 2013, COMPUT GRAPH FORUM, V32, P67, DOI 10.1111/cgf.12152
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Lo YH, 2019, COMPUT GRAPH FORUM, V38, P265, DOI 10.1111/cgf.13636
   Loi H, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2983617
   Ma C, 2019, MULTIMED TOOLS APPL, V78, P34065, DOI 10.1007/s11042-019-08053-7
   Ma C, 2018, LECT NOTES COMPUT SC, V11166, P672, DOI 10.1007/978-3-030-00764-5_62
   Ma CY, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461921
   Ma CY, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964957
   Maharik R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964995
   Mccann J., 2017, P S NONPH AN REND NP
   Roveri R, 2015, COMPUT GRAPH FORUM, V34, P39, DOI 10.1111/cgf.12695
   Shen QQ, 2017, LECT NOTES COMPUT SC, V10133, P233, DOI 10.1007/978-3-319-51814-5_20
   Tresset P, 2013, COMPUT GRAPH-UK, V37, P348, DOI 10.1016/j.cag.2013.01.012
   Vaxman A, 2016, COMPUT GRAPH FORUM, V35, P545, DOI 10.1111/cgf.12864
   Wong FJ, 2013, VISUAL COMPUT, V29, P729, DOI 10.1007/s00371-013-0809-1
   Xu XM, 2017, IEEE T VIS COMPUT GR, V23, P1910, DOI 10.1109/TVCG.2016.2569084
   Xu XM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778789
   Yang KW, 2018, MULTIMED TOOLS APPL, V77, P12259, DOI 10.1007/s11042-017-4882-8
   Yang KW, 2018, LECT NOTES COMPUT SC, V10704, P479, DOI 10.1007/978-3-319-73603-7_39
   Yang W., 2016, P 33 COMP GRAPH INT, P9
   Yuan W., 2018, MULT C MULT C SEOUL
   Zeng K, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640445
   Zhou J, 2014, J ZHEJIANG U-SCI C, V15, P729, DOI 10.1631/jzus.C1400099
NR 36
TC 3
Z9 3
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3667
EP 3679
DI 10.1007/s00371-021-02195-2
EA JUN 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000668025400001
DA 2024-07-18
ER

PT J
AU Ichinose, K
   Zhao, X
   Fujishiro, I
   Toyoura, M
   Kashiwagi, K
   Go, K
   Mao, X
AF Ichinose, Keisuke
   Zhao, Xi
   Fujishiro, Issei
   Toyoura, Masahiro
   Kashiwagi, Kenji
   Go, Kentaro
   Mao, Xiaoyang
TI Enhancing edge indicator for visual field loss compensation for
   homonymous hemianopia patients
SO VISUAL COMPUTER
LA English
DT Article
DE Visual field defect; Homonymous hemianopia; Out-of-visual-field
   information compensation; Edge Indicator
ID FLICKER
AB Homonymous hemianopia (HH) is a visual field defect in which the same side of both eyes is blind. In this paper, we propose an out-of-visual-field information compensation system for HH patients using an optical see-through head-mounted display. In the proposed system, in order to reduce the obstruction in the remaining visual field, indicators are placed at the boundary between the remaining visual field and the defective part to notify the HH patient about the events occurring at the blinded side of the visual field. Two different kinds of indicators, namely brightness-based and flicker-based, are designed to visualize the spatial information of the events. We conducted experiments with simulated HH participants to verify the effectiveness of the proposed system and compare the two designed indicators, as well as an existing method that makes use of the overlay overview window. The experimental results proved that the proposed system can react more intuitively and is faster than the existing method in the part near the central visual field. Moreover, it can alleviate the obstruction problem of the existing method.
C1 [Ichinose, Keisuke] Univ Yamanashi, Grad Sch Engn, Kofu, Yamanashi 4008511, Japan.
   [Zhao, Xi] Keio Univ, Ctr Informat & Comp Sci, Yokohama, Kanagawa 2238522, Japan.
   [Fujishiro, Issei] Keio Univ, Dept Informat & Comp Sci, Yokohama, Kanagawa 2238522, Japan.
   [Toyoura, Masahiro; Go, Kentaro; Mao, Xiaoyang] Univ Yamanashi, Dept Comp Sci & Engn, Kofu, Yamanashi 4008511, Japan.
   [Kashiwagi, Kenji] Univ Yamanashi, Dept Ophthalmol, Chuo 4093898, Japan.
   [Fujishiro, Issei; Mao, Xiaoyang] Hangzhou Dianzi Univ, Sch Comp Sci, 1156 2nd St, Hangzhou 310018, Peoples R China.
C3 University of Yamanashi; Keio University; Keio University; University of
   Yamanashi; University of Yamanashi; Hangzhou Dianzi University
RP Mao, X (corresponding author), Univ Yamanashi, Dept Comp Sci & Engn, Kofu, Yamanashi 4008511, Japan.; Mao, X (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci, 1156 2nd St, Hangzhou 310018, Peoples R China.
EM g20tk003@yamanashi.ac.jp; xzhao@keio.jp; fuji@ics.keio.ac.jp;
   mtoyoura@yamanashi.ac.jp; kenjik@yamanashi.ac.jp; go@yamanashi.ac.jp;
   mao@yamanashi.ac.jp
OI Toyoura, Masahiro/0000-0002-5897-7573
FU JSPS [17H00738, 20K20408]; Grants-in-Aid for Scientific Research
   [20K20408, 17H00738] Funding Source: KAKEN
FX This work is supported by the JSPS Grants-in-Aid for Scientific Research
   (Grant Nos. 17H00738 and 20K20408).
CR Bowers AR, 2004, OPHTHAL PHYSL OPT, V24, P296, DOI 10.1111/j.1475-1313.2004.00228.x
   DUSZYNSKI LR, 1955, AM J OPHTHALMOL, V39, P876, DOI 10.1016/0002-9394(55)90182-3
   Gilhotra JS, 2002, STROKE, V33, P2417, DOI 10.1161/01.STR.0000037647.10414.d2
   Goodlaw E., 1983, OPTOM MONTHLY, V74, P363
   Goodwin D, 2014, CLIN OPHTHALMOL, V8, P1919, DOI 10.2147/OPTH.S59452
   Hart S. G., 2006, P HUM FACT ERG SOC A, V50, P904, DOI DOI 10.1177/154193120605000909
   Ichinose K, 2020, 2020 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2020), P79, DOI 10.1109/CW49994.2020.00019
   Niforatos E, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS (ISWC 17), P114, DOI 10.1145/3123021.3123052
   Peli E, 2000, OPTOMETRY VISION SCI, V77, P453, DOI 10.1097/00006324-200009000-00006
   Qian L, 2018, IEEE T VIS COMPUT GR, V24, P2936, DOI 10.1109/TVCG.2018.2868559
   Sayed AM, 2020, AM J OPHTHALMOL, V210, P125, DOI 10.1016/j.ajo.2019.10.006
   Song JH, 2009, J NEUROPHYSIOL, V102, P2681, DOI 10.1152/jn.91352.2008
   Tseng Hung-Yu, 2015, P 17 INT C HUM COMP, P323, DOI DOI 10.1145/2785830.2785831
   TYLER CW, 1987, J OPT SOC AM A, V4, P1612, DOI 10.1364/JOSAA.4.001612
   Vargas-Martín F, 2002, OPTOMETRY VISION SCI, V79, P715, DOI 10.1097/00006324-200211000-00009
   Waldin N, 2017, COMPUT GRAPH FORUM, V36, P467, DOI 10.1111/cgf.13141
   Younis O., 2017, 14 INT MULT SYST SIG
   Zhao X, 2020, COMPUT GRAPH-UK, V89, P59, DOI 10.1016/j.cag.2020.04.001
NR 18
TC 0
Z9 0
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2022
VL 38
IS 4
SI SI
BP 1263
EP 1273
DI 10.1007/s00371-021-02156-9
EA MAY 2021
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0D9DX
UT WOS:000648497900001
DA 2024-07-18
ER

PT J
AU Hu, M
   Ge, P
   Wang, XH
   Lin, H
   Ren, FJ
AF Hu, Min
   Ge, Peng
   Wang, Xiaohua
   Lin, Hui
   Ren, Fuji
TI A spatio-temporal integrated model based on local and global features
   for video expression recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Video expression recognition; Local and global features; Attention
   mechanism; Feature recalibration; Network integration
ID NETWORK; SCALE
AB Facial expressions can be represented largely by the dynamic variations of important facial expression parts, i.e., eyebrows, eyes, nose, and mouth. The features of these parts are regarded as local features. However, facial global information is also useful for recognition because it is a necessary complement to local features. In this paper, a spatio-temporal integrated model that jointly learns local and global features is proposed for video expression recognition. Firstly, to capture the action of facial key units, a spatio-temporal attention part-gradient-based hierarchical bidirectional recurrent neural network (spatio-temporal attention PGHRNN) is constructed. It can capture the dynamic variations of gradients around facial landmark points. In addition, a new kind of spatial attention mechanism is introduced to recalibrate the features of facial various parts adaptively. Secondly, to complement the local features extracted by the spatio-temporal attention PGHRNN, a squeeze-and-excitation residual network of 50 layers with long short-term memory network (SE-ResNet-50-LSTM) is used as a global feature extractor and classifier. Finally, to integrate the local and global features and improve the performance of facial expression recognition, a joint adaptive fine-tuning method (JAFTM) is proposed to combine the two networks, which can adaptively adjust the network weights. Extensive experiments demonstrate that our proposed model can achieve a superior recognition accuracy of 98.95% on CK + for 7-class facial expressions and 85.40% on MMI database, which outperforms other state-of-the-art methods.
C1 [Hu, Min; Ge, Peng; Wang, Xiaohua] Hefei Univ Technol, Key Lab Knowledge Engn Big Data, Minist Educ, Hefei 230601, Peoples R China.
   [Hu, Min; Ge, Peng; Wang, Xiaohua; Ren, Fuji] Hefei Univ Technol, Sch Comp & Informat, Anhui Prov Key Lab Affect Comp & Adv Intelligent, Hefei 230601, Peoples R China.
   [Lin, Hui] Hefei Univ Technol, Sch Elect Sci & Applicat Phys, Hefei 230601, Peoples R China.
   [Ren, Fuji] Univ Tokushima, Grad Sch Adv Technol & Sci, Tokushima 7708502, Japan.
C3 Hefei University of Technology; Hefei University of Technology; Hefei
   University of Technology; Tokushima University
RP Ge, P (corresponding author), Hefei Univ Technol, Key Lab Knowledge Engn Big Data, Minist Educ, Hefei 230601, Peoples R China.; Ge, P (corresponding author), Hefei Univ Technol, Sch Comp & Informat, Anhui Prov Key Lab Affect Comp & Adv Intelligent, Hefei 230601, Peoples R China.
EM gepeng18@mail.hfut.edu.cn; xh_wang@hfut.edu.cn
RI wang, xiao/HGB-7081-2022; wang, xiao/HZI-9156-2023; wang,
   xu/IAN-4886-2023
OI wang, xiao/0000-0002-4088-3341; 
FU National Natural Science Foundation of China [61672202, 61673156]; State
   Key Program of NSFC-Shenzhen Joint Foundation [U1613217]; Fundamental
   Research Funds for the Central Universities of China [PA2020GDSK0061,
   PA2019GDPK0076]
FX This research has been partially supported by National Natural Science
   Foundation of China (Grant No. 61672202, 61673156), State Key Program of
   NSFC-Shenzhen Joint Foundation (Grant No. U1613217) and the Fundamental
   Research Funds for the Central Universities of China (Grant No.
   PA2020GDSK0061, PA2019GDPK0076).
CR Agarwal S, 2018, VISUAL COMPUT, V34, P177, DOI 10.1007/s00371-016-1323-z
   Agrawal A, 2020, VISUAL COMPUT, V36, P405, DOI 10.1007/s00371-019-01630-9
   Akamatsu S, 1998, P 3 INT C AUT FAC GE, P14, DOI DOI 10.5281/ZENODO.3451524
   [Anonymous], 2017, ADV NEURAL INFORM PR
   [Anonymous], 2017, IEEE T AFFECTIVE COM
   Baddar W.J., 2019, IEEE T AFFECT COMPUT
   Calvo RA, 2010, IEEE T AFFECT COMPUT, V1, P18, DOI 10.1109/T-AFFC.2010.1
   Chen JK, 2018, IEEE T AFFECT COMPUT, V9, P38, DOI 10.1109/TAFFC.2016.2593719
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Danelakis A, 2016, VISUAL COMPUT, V32, P1001, DOI 10.1007/s00371-016-1243-y
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fan Y, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P445, DOI 10.1145/2993148.2997632
   Grites T., 2008, Academic advising: a comprehensive handbook, P1, DOI [10.1109/AFGR.2008.4813379, DOI 10.1109/AFGR.2008.4813379]
   Happy SL, 2015, IEEE T AFFECT COMPUT, V6, P1, DOI 10.1109/TAFFC.2014.2386334
   Hasani B, 2017, IEEE COMPUT SOC CONF, P2278, DOI 10.1109/CVPRW.2017.282
   Hasani B, 2017, IEEE INT CONF AUTOMA, P790, DOI 10.1109/FG.2017.99
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang D.-Y., 2017, P 19 ACM INT C MULT, P577, DOI DOI 10.1145/3136755.3143012
   Huang K, 2020, LECT NOTES COMPUT SC, V11962, P161, DOI 10.1007/978-3-030-37734-2_14
   Jeni LA, 2014, LECT NOTES COMPUT SC, V8692, P135, DOI 10.1007/978-3-319-10593-2_10
   Jung H, 2015, IEEE I CONF COMP VIS, P2983, DOI 10.1109/ICCV.2015.341
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Kuo CM, 2018, IEEE COMPUT SOC CONF, P2202, DOI 10.1109/CVPRW.2018.00286
   Levi G, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P503, DOI 10.1145/2823327.2823333
   Li K, 2020, VISUAL COMPUT, V36, P391, DOI 10.1007/s00371-019-01627-4
   Liang DD, 2020, VISUAL COMPUT, V36, P499, DOI 10.1007/s00371-019-01636-3
   Liu MY, 2014, PROC CVPR IEEE, P1749, DOI 10.1109/CVPR.2014.226
   Liu Q., 2020, IEEE T AFFECTIVE COM
   Liu XQ, 2020, VISUAL COMPUT, V36, P1635, DOI 10.1007/s00371-019-01759-7
   Liu YY, 2018, PATTERN RECOGN, V84, P251, DOI 10.1016/j.patcog.2018.07.016
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu Q, 2019, IEEE ACCESS, V7, P137420, DOI 10.1109/ACCESS.2019.2943235
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Luong M.-T., 2015, ARXIV PREPRINT ARXIV
   Meng LL, 2019, IEEE INT CONF COMP V, P1513, DOI 10.1109/ICCVW.2019.00189
   Moore S, 2011, COMPUT VIS IMAGE UND, V115, P541, DOI 10.1016/j.cviu.2010.12.001
   Ofodile I., 2017, AUTOMATIC RECOGNITIO
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Perveen N, 2020, IEEE T IMAGE PROCESS, V29, P8316, DOI 10.1109/TIP.2020.3011846
   Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6
   Qiao ZN, 2021, INT C PATT RECOG, P7521, DOI 10.1109/ICPR48806.2021.9412235
   Rashid M, 2013, VISUAL COMPUT, V29, P1269, DOI 10.1007/s00371-012-0768-y
   Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667
   Ruder S., 2016, ARXIV
   Shi ZS, 2020, IEEE ACCESS, V8, P16785, DOI 10.1109/ACCESS.2020.2968024
   Sikka Karan, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P18, DOI 10.1109/CVPRW.2015.7301350
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Valstar MF, 2010, LREC 2010 - SEVENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, pJ65
   van der Maaten L. J. P., 2008, Journal of Machine Learning Research, V9, P2579, DOI DOI 10.1007/S10479-011-0841-3
   Wang K, 2020, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR42600.2020.00693
   Wang ZH, 2013, PROC CVPR IEEE, P3422, DOI 10.1109/CVPR.2013.439
   Yan JW, 2018, NEUROCOMPUTING, V309, P27, DOI 10.1016/j.neucom.2018.03.068
   Yu ZB, 2018, VISUAL COMPUT, V34, P1691, DOI 10.1007/s00371-017-1443-0
   Zhang KH, 2017, IEEE T IMAGE PROCESS, V26, P4193, DOI 10.1109/TIP.2017.2689999
   Zhang X, 2015, MACH VISION APPL, V26, P467, DOI 10.1007/s00138-015-0677-y
   Zhao GY, 2011, IMAGE VISION COMPUT, V29, P607, DOI 10.1016/j.imavis.2011.07.002
   Zhao XY, 2016, LECT NOTES COMPUT SC, V9906, P425, DOI 10.1007/978-3-319-46475-6_27
   Zheng WM, 2009, IEEE I CONF COMP VIS, P1901, DOI 10.1109/ICCV.2009.5459421
   Zhong L, 2012, PROC CVPR IEEE, P2562, DOI 10.1109/CVPR.2012.6247974
   Zhou JZ, 2020, IEEE IMAGE PROC, P1961, DOI 10.1109/ICIP40778.2020.9191181
   Zhu S, 2020, STOCH ENV RES RISK A, V34, P1313, DOI 10.1007/s00477-020-01766-4
NR 62
TC 5
Z9 6
U1 1
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2022
VL 38
IS 8
BP 2617
EP 2634
DI 10.1007/s00371-021-02136-z
EA MAY 2021
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3B2OE
UT WOS:000646973000001
DA 2024-07-18
ER

PT J
AU Sabih, M
   Vishwakarma, DK
AF Sabih, Mohammad
   Vishwakarma, Dinesh Kumar
TI Crowd anomaly detection with LSTMs using optical features and domain
   knowledge for improved inferring
SO VISUAL COMPUTER
LA English
DT Article
DE Optical flow; Bidirectional LSTM; Regularizers; CNN; Domain knowledge
AB With the increasing population, the probability of occurrence of different kinds of crowd anomalies gets frequent. Blockage on roads, the lighting condition, and the uneven movement of humans and vehicles makes it a tough and challenging problem. The paper proposes the combined use of a convolutional neural network and bidirectional LSTM to solve the task. CNN helps extract frame-level features of the optical flow over the video evaluated by the Lucas Kanade algorithm. A novel approach of improving the predicted class with the domain knowledge of datasets is also performed. The proposed methodology is tested on the crowd anomaly dataset's benchmark datasets, namely UCSD Ped-1 and UCSD Ped-2, and it outperforms various other existing state-of-the-art methods.
C1 [Sabih, Mohammad; Vishwakarma, Dinesh Kumar] Delhi Technol Univ, Biometr Res Lab, Dept Informat Technol, Delhi 110042, India.
C3 Delhi Technological University
RP Vishwakarma, DK (corresponding author), Delhi Technol Univ, Biometr Res Lab, Dept Informat Technol, Delhi 110042, India.
EM thegenius411@gmail.com; dinesh@dtu.ac.in
RI VISHWAKARMA, DINESH/ABK-7887-2022; VISHWAKARMA, DINESH KUMAR/L-3815-2018
OI VISHWAKARMA, DINESH KUMAR/0000-0002-1026-0047
CR Chu WQ, 2019, IEEE T MULTIMEDIA, V21, P246, DOI 10.1109/TMM.2018.2846411
   Cong Y, 2013, IEEE T INF FOREN SEC, V8, P1590, DOI 10.1109/TIFS.2013.2272243
   Dhole H, 2019, INT CONF COMPUT
   Feng YC, 2017, NEUROCOMPUTING, V219, P548, DOI 10.1016/j.neucom.2016.09.063
   Hettiarachchi Anuruddha L., 2014, 7th International Conference on Information and Automation for Sustainability (ICIAfS), P1, DOI 10.1109/ICIAFS.2014.7069590
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Khan MUK, 2019, IEEE T INF FOREN SEC, V14, P541, DOI 10.1109/TIFS.2018.2856189
   Li W, 2014, IEEE IPCCC
   Ojha N, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON INVENTIVE SYSTEMS AND CONTROL (ICISC 2018), P646, DOI 10.1109/ICISC.2018.8398878
   Piciarelli C., 2008, IEEE Transactions on Circuits and Systems for Video Technology
   Revathi AR, 2017, SIGNAL IMAGE VIDEO P, V11, P291, DOI 10.1007/s11760-016-0935-0
   Sabokrou Mohammad, 2017, IEEE T IMAGE PROCESS
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Singh K, 2020, NEUROCOMPUTING, V371, P188, DOI 10.1016/j.neucom.2019.08.059
   TUNG F, 2011, IMAGE VISION COMPUT
   Xu D, 2017, COMPUT VIS IMAGE UND, V156, P117, DOI 10.1016/j.cviu.2016.10.010
   Xu D, 2014, NEUROCOMPUTING, V143, P144, DOI 10.1016/j.neucom.2014.06.011
   Yang ZH, 2020, NEURAL COMPUT APPL, V32, P11597, DOI 10.1007/s00521-019-04647-2
   Zhou JT, 2019, IEEE T INF FOREN SEC, V14, P2537, DOI 10.1109/TIFS.2019.2900907
   ZHUANG N, 2017, IEEE INT SYM MULTIM, V2017, P61
NR 20
TC 13
Z9 14
U1 2
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2022
VL 38
IS 5
BP 1719
EP 1730
DI 10.1007/s00371-021-02100-x
EA APR 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0M9JP
UT WOS:000636212300001
DA 2024-07-18
ER

PT J
AU Dong, K
   Gao, SS
   Xin, SQ
   Zhou, YF
AF Dong, Kun
   Gao, Shanshan
   Xin, Shiqing
   Zhou, Yuanfeng
TI Probability driven approach for point cloud registration of indoor scene
SO VISUAL COMPUTER
LA English
DT Article
DE Point cloud registration; Probabilistic method; Indoor scene; Distance
   matrix; Difference matrix
ID ICP
AB Point cloud registration is a crucial step of localization and mapping with mobile robots or in object modeling pipelines. In this paper, we present a novel probability driven algorithm for point cloud registration of the indoor scene based on RGB-D images. Firstly, we extract the key points in RGB-D images and map the key points to 3D space as preprocessing. Then, we build the distance matrix and the difference matrix for each point cloud, respectively in scalarization and vectorization, to encode the structural proximity. And establish the corresponding point set by computing the matching probabilities. At last, we solve the transform matrix that aligns the source point cloud to the target point cloud. The entire registration framework consists of two phases: coarse registration based on the distance matrix (in scalarization) and fine registration based on the difference matrix (in vectorization). The two-phase registration strategy is able to greatly reduce the influence of inherent noise. Experiments demonstrate that our method outperforms in registration accuracy than the state-of-the-art methods. Furthermore, our method is more efficient than existing methods in computing speed because we utilize the location relationship between key points instead of point features. The source code is provided at our project website .
C1 [Dong, Kun; Zhou, Yuanfeng] Shandong Univ, Sch Software, Jinan, Peoples R China.
   [Gao, Shanshan] Shandong Univ Finance & Econ, Sch Comp Sci & Technol, Jinan, Peoples R China.
   [Xin, Shiqing] Shandong Univ, Sch Comp Sci & Technol, Qingdao, Peoples R China.
C3 Shandong University; Shandong University of Finance & Economics;
   Shandong University
RP Zhou, YF (corresponding author), Shandong Univ, Sch Software, Jinan, Peoples R China.
EM dongkunnukgnod@163.com; yfzhou@sdu.edu.cn
RI liu, xinyi/KFB-4466-2024
FU NSFC-Zhejiang Joint Fund of the Integration of Informatization and
   Industrialization [U1909210]; National Natural Science Foundation of
   China (NSFC) [61772312]; Key Research and Development Project of
   Shandong Province [2017GGX10110, 2019GGX101007]
FX This work was supported by the the NSFC-Zhejiang Joint Fund of the
   Integration of Informatization and Industrialization (U1909210), the
   National Natural Science Foundation of China (NSFC) (61772312), the Key
   Research and Development Project of Shandong Province (2017GGX10110,
   2019GGX101007).
CR Aiger D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360684
   Altantsetseg E, 2018, VISUAL COMPUT, V34, P1021, DOI 10.1007/s00371-018-1534-6
   [Anonymous], 2015, PROC CVPR IEEE
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bobenrieth C., 2017, P COMP GRAPH INT C Y, V27, P1
   Campbell D, 2015, IEEE I CONF COMP VIS, P4292, DOI 10.1109/ICCV.2015.488
   Chang WC, 2020, VISUAL COMPUT, V36, P593, DOI 10.1007/s00371-019-01642-5
   Das MP, 2018, IEEE INT C INT ROBOT, P6357, DOI 10.1109/IROS.2018.8594318
   Eckart B., 2018, ABS180702587 CORR
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gao QH, 2017, LECT NOTES COMPUT SC, V10345, P11, DOI 10.1007/978-3-319-65849-0_2
   Gao W, 2019, PROC CVPR IEEE, P11087, DOI 10.1109/CVPR.2019.01135
   Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806
   Handa A, 2014, IEEE INT CONF ROBOT, P1524, DOI 10.1109/ICRA.2014.6907054
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   Horaud R, 2011, IEEE T PATTERN ANAL, V33, P587, DOI 10.1109/TPAMI.2010.94
   Hu L, 2020, VISUAL COMPUT, V36, P669, DOI 10.1007/s00371-019-01648-z
   Muñoz FLI, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON MULTISENSOR FUSION AND INTEGRATION FOR INTELLIGENT SYSTEMS (MFI), P517
   Jian B, 2005, IEEE I CONF COMP VIS, P1246
   Jian B, 2011, IEEE T PATTERN ANAL, V33, P1633, DOI 10.1109/TPAMI.2010.223
   Lan Z., 2019, P IEEE CVF C COMP VI, P9690
   Le HM, 2019, PROC CVPR IEEE, P124, DOI 10.1109/CVPR.2019.00021
   Lê-Huu DK, 2017, PROC CVPR IEEE, P4914, DOI 10.1109/CVPR.2017.522
   Li M, 2020, VISUAL COMPUT, V36, P1725, DOI 10.1007/s00371-019-01771-x
   Li Z, 2017, IEEE INT SYM BROADB, P111
   Ma GL, 2017, DESTECH TRANS COMP, P253
   Mellado N, 2014, COMPUT GRAPH FORUM, V33, P205, DOI 10.1111/cgf.12446
   Nassar A, 2018, IEEE COMPUT SOC CONF, P1594, DOI 10.1109/CVPRW.2018.00201
   Park J, 2017, IEEE I CONF COMP VIS, P143, DOI 10.1109/ICCV.2017.25
   Point SIC, 2013, COMPUT GRAPH FORUM, V32, P113
   Ren B, 2019, J COMPUT SCI TECH-CH, V34, P581, DOI 10.1007/s11390-019-1928-6
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Servos J, 2017, ROBOT AUTON SYST, V87, P247, DOI 10.1016/j.robot.2016.10.016
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Stoyanov T, 2012, IEEE INT CONF ROBOT, P5196, DOI 10.1109/ICRA.2012.6224717
   Tazir ML, 2018, ROBOT AUTON SYST, V108, P66, DOI 10.1016/j.robot.2018.07.003
   Wang K, 2020, VISUAL COMPUT, V2020, P1432
   Xiao JX, 2013, IEEE I CONF COMP VIS, P1625, DOI 10.1109/ICCV.2013.458
   Yang JL, 2016, IEEE T PATTERN ANAL, V38, P2241, DOI 10.1109/TPAMI.2015.2513405
   Yang SC, 2016, IEEE INT CONF ROBOT, P2183, DOI 10.1109/ICRA.2016.7487368
   Yang X, 2018, IEEE T CYBERNETICS, V48, P1432, DOI 10.1109/TCYB.2017.2697968
   Yasir R, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P230, DOI 10.1109/CRV.2018.00040
   Zhou QY, 2016, LECT NOTES COMPUT SC, V9906, P766, DOI 10.1007/978-3-319-46475-6_47
   Zhou QY, 2015, PROC CVPR IEEE, P632, DOI 10.1109/CVPR.2015.7298662
NR 45
TC 8
Z9 8
U1 8
U2 32
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2022
VL 38
IS 1
BP 51
EP 63
DI 10.1007/s00371-020-01999-y
EA OCT 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YW6DE
UT WOS:000582814800002
DA 2024-07-18
ER

PT J
AU Ströter, D
   Mueller-Roemer, JS
   Stork, A
   Fellner, DW
AF Stroeter, Daniel
   Mueller-Roemer, Johannes S.
   Stork, Andre
   Fellner, Dieter W.
TI OLBVH: octree linear bounding volume hierarchy for volumetric meshes
SO VISUAL COMPUTER
LA English
DT Article
DE Bounding volume hierarchy; GPGPU; Volumetric meshes; Direct volume
   rendering; Intersection detection; Slicing
AB We present a novel bounding volume hierarchy for GPU-accelerated direct volume rendering (DVR) as well as volumetric mesh slicing and inside-outside intersection testing. Our novel octree-based data structure is laid out linearly in memory using space filling Morton curves. As our new data structure results in tightly fitting bounding volumes, boundary markers can be associated with nodes in the hierarchy. These markers can be used to speed up all three use cases that we examine. In addition, our data structure is memory-efficient, reducing memory consumption by up to 75%. Tree depth and memory consumption can be controlled using a parameterized heuristic during construction. This allows for significantly shorter construction times compared to the state of the art. For GPU-accelerated DVR, we achieve performance gain of 8.4x-13x. For 3D printing, we present an efficient conservative slicingmethod that results in a 3x-25xspeedup when using our data structure. Furthermore, we improve volumetric mesh intersection testing speed by 5x-52x.
C1 [Stroeter, Daniel; Mueller-Roemer, Johannes S.] Tech Univ Darmstadt, D-64277 Darmstadt, Germany.
   [Mueller-Roemer, Johannes S.; Stork, Andre; Fellner, Dieter W.] Fraunhofer IGD, Darmstadt, Germany.
   [Stork, Andre; Fellner, Dieter W.] Tech Univ Darmstadt, Comp Sci, Darmstadt, Germany.
   [Fellner, Dieter W.] Graz Univ Technol, Graz, Austria.
C3 Technical University of Darmstadt; Technical University of Darmstadt;
   Graz University of Technology
RP Ströter, D (corresponding author), Tech Univ Darmstadt, D-64277 Darmstadt, Germany.
EM daniel.stroeter@gris.tu-darmstadt.de
OI Stroter, Daniel/0000-0002-2672-7377; Mueller-Roemer, Johannes
   Sebastian/0000-0002-0712-0457; Fellner, Dieter W./0000-0001-7756-0901
FU Projekt DEAL; Qu4lity EU project - Horizon 2020 Framework Programme of
   the European Union [825030]
FX Open Access funding provided by Projekt DEAL. This work was supported by
   the Qu4lity EU project which is co-funded by the Horizon 2020 Framework
   Programme of the European Union under Grant Agreement No. 825030. The
   Fusion and Jets meshes are courtesy of the university of Utah.
CR Alliez P, 2005, ACM T GRAPHIC, V24, P617, DOI 10.1145/1073204.1073238
   Altenhofen Christian, 2018, SOLID FREEFORM FABRI, V29, P1675
   Apetrei Ciprian., 2014, Computer Graphics and Visual Computing (CGVC)
   Bell N., 2015, THRUST 1 8 1
   Binder N, 2016, EUR ACM SIGGRAPH S H
   Cheng Siu-Wing, 2012, Delaunay mesh generation
   Doyle MJ, 2018, IEEE T MULTI-SCALE C, V4, P83, DOI 10.1109/TMSCS.2017.2695338
   Garanzha K., 2011, P ACM SIGGRAPH S HIG, P59, DOI DOI 10.1145/2018323.2018333
   Garcia A, 2014, P 13 ACM SIGGRAPH IN, P151, DOI DOI 10.1145/2670473.2670488
   Gu F., 2018, VISION MODELING VISU, DOI [10.2312/vmv.20181257, DOI 10.2312/VMV.20181257]
   Karras T., 2012, P 4 ACM SIGGRAPH EUR, P33, DOI [10.2312/EGGH/HPG12/033-037, DOI 10.2312/EGGH/HPG12/033-037]
   Lauterbach C, 2009, COMPUT GRAPH FORUM, V28, P375, DOI 10.1111/j.1467-8659.2009.01377.x
   Merill D., 2018, CUB 1 8 0
   Morrical N, 2019, 2019 IEEE VISUALIZATION CONFERENCE (VIS), P256, DOI 10.1109/visual.2019.8933539
   Morton G. M., 1966, COMPUTER ORIENTED GE
   Mueller-Roemer JS, 2018, COMPUT GRAPH FORUM, V37, P443, DOI 10.1111/cgf.13581
   Mueller-Roemer JS, 2017, COMPUT GRAPH FORUM, V36, P59, DOI 10.1111/cgf.13245
   Murguia L. R. Sergio, 2013, GPU PRO 4 ADV RENDER, P319
   NVIDIA, 2020, NVIDIA OPTIX RAY TRA
   Pantaleoni J., 2010, P C HIGH PERFORMANCE, P87
   Stich M., 2018, INTRO NVIDIA RTX DIR
   Vaidyanathan Karthik., 2019, Proceedings of the Conference on High-Performance Graphics (HPG'19), P1, DOI [10.2312/hpg.20191190, DOI 10.2312/HPG.20191190]
   Viitanen T, 2018, P ACM COMPUT GRAPH I, V1, P1, DOI [10.1145/3233309, DOI 10.1145/3233309]
   Viitanen T, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3132702
   Vinkler M, 2017, HPG '17: PROCEEDINGS OF HIGH PERFORMANCE GRAPHICS, DOI 10.1145/3105762.3105782
   Wald I., 2019, HighPerformance Graphics Short Papers, DOI [DOI 10.2312/HPG.20191189, 10.2312/hpg.20191189]
   Wald I, 2007, IEEE T VIS COMPUT GR, V13, P1727, DOI 10.1109/TVCG.2007.70566
   Zellmann S, 2019, IEEE PAC VIS SYMP, P222, DOI 10.1109/PacificVis.2019.00033
   Zhou K, 2011, IEEE T VIS COMPUT GR, V17, P669, DOI 10.1109/TVCG.2010.75
NR 29
TC 13
Z9 13
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2327
EP 2340
DI 10.1007/s00371-020-01886-6
EA JUL 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000545934000002
OA hybrid, Green Published
DA 2024-07-18
ER

PT J
AU Bahce, CG
   Bayazit, U
AF Bahce, Canan Gulbak
   Bayazit, Ulug
TI Compression of geometry videos by 3D-SPECK wavelet coder
SO VISUAL COMPUTER
LA English
DT Article
DE Geometry image; Geometry video; Compression; Wavelet transform;
   Postprocessing
ID PARAMETRIZATION; EFFICIENT
AB A geometry video is formed as a sequence of geometry images where each frame is a remeshed form of a frame of an animated mesh sequence. For efficiently coding geometry videos by exploiting temporal as well spatial correlation at multiple scales, this paper proposes the 3D-SPECK algorithm which has been successfully applied to the coding of volumetric medical image data and hyperspectral image data in the past. The paper also puts forward several postprocessing operations on the reconstructed surfaces that compensate for the visual artifacts appearing in the form of undulations due to the loss of high-frequency wavelet coefficients, cracks near geometry image boundaries due to vertex coordinate quantization errors and serrations due to regular or quad splitting triangulation of local regions of large anisotropic geometric stretch. Experimental results on several animated mesh sequences demonstrate the superiority of the subjective and objective coding performances of the newly proposed approach to those of the commonly recognized animated mesh sequence coding approaches at low and medium coding rates.
C1 [Bahce, Canan Gulbak; Bayazit, Ulug] Istanbul Tech Univ, Dept Comp Engn, Istanbul, Turkey.
C3 Istanbul Technical University
RP Bahce, CG (corresponding author), Istanbul Tech Univ, Dept Comp Engn, Istanbul, Turkey.
EM gulbakcanan@itu.edu.tr; ulugbayazit@itu.edu.tr
RI Bayazit, Ulug/ABB-2362-2020
OI Bayazit, Ulug/0000-0001-6556-4104
CR Alexa M, 2000, COMPUT GRAPH FORUM, V19, pC411, DOI 10.1111/1467-8659.00433
   Bayazit U, 2019, GRAPH MODELS, V106, DOI 10.1016/j.gmod.2019.101045
   Boulfani-Cuisinaud Y, 2007, IEEE IMAGE PROC, P217
   Briceno H.M., 2003, Dans SCA '03, P136
   *CASTR DDII, 2019, PYNTCL
   Chew BS, 2011, IEEE T BROADCAST, V57, P636, DOI 10.1109/TBC.2011.2151590
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Cohen-Steiner D, 2004, VISUAL COMPUT, V20, P4, DOI 10.1007/s00371-003-0217-z
   COLLINS G, 2005, VISION VIDEO GRAPHIC
   Digne J, 2011, COMPUT GRAPH FORUM, V30, P1630, DOI 10.1111/j.1467-8659.2011.01848.x
   Floater MS, 1997, COMPUT AIDED GEOM D, V14, P231, DOI 10.1016/S0167-8396(96)00031-3
   GU X., 2004, COMMUN INF SYST, V3, P171
   GU X, 2003, S GEOM PROC, P127
   Gu X., 2006, P 2006 ACM S SOLID P, P129
   Gu XF, 2002, ACM T GRAPHIC, V21, P355
   Guskov I., 2004, Proc. 2004 ACM SIG- GRAPH/Eurographics Symp. Comput. Animation (SCA '04), P183
   Habe Hitoshi, 2004, P PICT COD S, P301
   Hajizadeh M, 2020, VISUAL COMPUT, V36, P649, DOI 10.1007/s00371-019-01645-2
   Hou JH, 2015, IEEE T CIRC SYST VID, V25, P51, DOI 10.1109/TCSVT.2014.2329376
   Hou JH, 2014, IEEE T CIRC SYST VID, V24, P1541, DOI 10.1109/TCSVT.2014.2313890
   Hou JH, 2013, IEEE T CIRC SYST VID, V23, P1537, DOI 10.1109/TCSVT.2013.2248971
   Ibarria L., 2003, Dans SCA '03, P126
   ISLAM A, 1998, VISUAL COMMUNICATION
   Karni Z, 2004, COMPUT GRAPH-UK, V28, P25, DOI 10.1016/j.cag.2003.10.002
   Karpinsky N, 2013, OPT LASER ENG, V51, P620, DOI 10.1016/j.optlaseng.2012.12.021
   Karpinsky N, 2012, OPT LASER ENG, V50, P280, DOI 10.1016/j.optlaseng.2011.08.002
   Mamou K, 2008, IEEE IMAGE PROC, P2676, DOI 10.1109/ICIP.2008.4712345
   Mamou K, 2006, COMPUT ANIMAT VIRT W, V17, P337, DOI 10.1002/cav.137
   Mamou K, 2009, COMPUT ANIMAT VIRT W, V20, P343, DOI 10.1002/cav.319
   Marpe D, 2003, IEEE T CIRC SYST VID, V13, P620, DOI 10.1109/TCSVT.2003.815173
   MEKURIA R, 2015, MPEG REFERENCE SOFTW
   Meyer M., 2002, Visualization and mathematics, V3, P34
   Müller K, 2006, IEEE IMAGE PROC, P533, DOI 10.1109/ICIP.2006.312394
   Payan F, 2007, COMPUT GRAPH-UK, V31, P77, DOI 10.1016/j.cag.2006.09.009
   Pearlman WA, 2004, IEEE T CIRC SYST VID, V14, P1219, DOI 10.1109/TCSVT.2004.835150
   Peercy M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P303, DOI 10.1145/258734.258873
   Pereira F., 2002, IMSC Press multimedia series
   Praun E, 2003, ACM T GRAPHIC, V22, P340, DOI 10.1145/882262.882274
   Quynh D.T., 2011, P 19 ACM INT C MULTI, P383
   RAU C, 2011, OPENGI EASY PARAMETE
   RICARD J, 2019, VIDEO CODEC BASED PO
   Sander PedroV., 2002, EGRW 02, P87
   Sander PV, 2001, COMP GRAPH, P409, DOI 10.1145/383259.383307
   Sattler Mirko, 2005, P ACM SIGGRAPH EUR S, P209
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Stefanoski Nikolce, 2007, Proceedings 2007 IEEE International Conference on Image Processing, ICIP 2007, P109
   Sumner R.W., 2004, Mesh data from deformation transfer for triangle meshes
   Tang X, 2006, HYPERSPECTRAL DATA COMPRESSION, P273, DOI 10.1007/0-387-28600-4_10
   TAUBIN G, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P852, DOI 10.1109/ICCV.1995.466848
   Vása L, 2014, COMPUT GRAPH FORUM, V33, P145, DOI 10.1111/cgf.12304
   VASA L, 2015, GEOMETRIC LAPLACIAN
   Vlasic D., 2008, Articulated mesh animation from multi-view silhouettes
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   WITTEN IH, 1987, COMMUN ACM, V30, P520, DOI 10.1145/214762.214771
   Xia J., 2010, P 18 ACM INT C MULT, P591
   Xia JZ, 2012, IEEE T CIRC SYST VID, V22, P77, DOI 10.1109/TCSVT.2011.2158337
   Xu JZ, 2016, IEEE T CIRC SYST VID, V26, P50, DOI 10.1109/TCSVT.2015.2478706
   Yoshizawa S, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P200
NR 58
TC 4
Z9 4
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2021
VL 37
IS 5
BP 973
EP 991
DI 10.1007/s00371-020-01847-z
EA MAY 2020
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RV2QF
UT WOS:000531773100001
DA 2024-07-18
ER

PT J
AU Li, HY
   Sun, ZX
AF Li, Hongyan
   Sun, Zhengxing
TI A structural-constraint 3D point clouds segmentation adversarial method
SO VISUAL COMPUTER
LA English
DT Article
DE 3D point clouds; Segmentation and labeling; Adversarial learning;
   Learned structural loss
ID SHAPE SEGMENTATION; CO-SEGMENTATION; NETWORK
AB Point cloud segmentation is a key task of shape analysis with various applications. Existing segmentation methods usually apply a single segmentation network to compute point-wise loss for network training. The point-wise loss, which will lose some structure information and resulting in non-consistency labeling. To deal with this problem, we propose a segmentation adversarial framework. Compared with the existing segmentation methods, the proposed method extends the single segmentation network to a complex network by adding a discrimination network, which can compensate adversarial structural loss and enforce spatial label consistency. The discrimination network is added to the single segmentation network, without increasing any computational burden in testing process as it is not required during inference. Moreover, a structural-constraint point cloud segmentation adversarial network is proposed. Especially, in feature-encoding process, we capture the fix-scale neighborhood of each point and concern the relationship between point pairs to provide better local context. Additionally, the proposed method realizes the global constraint through condition setting and the consistency of prediction label by using learnable loss function. Experimental results show the proposed method can produce more reasonable and consistency segmentation and labeling results.
C1 [Li, Hongyan; Sun, Zhengxing] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.
C3 Nanjing University
RP Sun, ZX (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.
EM lhynju@163.com; njumagic@nju.edu.cn
RI Sun, Zhengxing/A-7411-2011
FU National High Technology Research and Development Program of China
   [2007AA01Z334]; National Natural Science Foundation of China [61321491,
   61272219]; Innovation Fund of State Key Laboratory for Novel Software
   Technology [ZZKT2013A12, ZZKT2016A11]; Program for New Century Excellent
   Talents in University of China [NCET-04-04605]; Nanjing University
   Innovation and Creative Program for Ph.D. candidate [2016013]
FX This work is supported by National High Technology Research and
   Development Program of China (No. 2007AA01Z334); National Natural
   Science Foundation of China (Nos. 61321491, 61272219); Innovation Fund
   of State Key Laboratory for Novel Software Technology (Nos. ZZKT2013A12,
   ZZKT2016A11); Program for New Century Excellent Talents in University of
   China (No. NCET-04-04605); Nanjing University Innovation and Creative
   Program for Ph.D. candidate (No. 2016013).
CR [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   [Anonymous], 2016, NIPS Workshop on Adversarial Training
   Arjovsky M., 2017, ARXIV170107875
   Chen HH, 2019, COMPUT AIDED DESIGN, V115, P122, DOI 10.1016/j.cad.2019.05.036
   Chen XB, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531379
   Cole F, 2008, ACM T GRAPHIC, V27, DOI [10.1145/1360612.1360657, 10.1145/1360612.1360687]
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Ding SY, 2019, DESTECH TRANS SOC
   Fan YL, 2018, VISUAL COMPUT, V34, P659, DOI 10.1007/s00371-017-1405-6
   Gonçalves GR, 2018, SIBGRAPI, P110, DOI 10.1109/SIBGRAPI.2018.00021
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo K, 2015, ACM T GRAPHIC, V35, DOI 10.1145/2835487
   Huang QG, 2018, PROC CVPR IEEE, P2626, DOI 10.1109/CVPR.2018.00278
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kalogerakis E, 2017, PROC CVPR IEEE, P6630, DOI 10.1109/CVPR.2017.702
   Kalogerakis E, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778839
   Le T, 2018, PROC CVPR IEEE, P9204, DOI 10.1109/CVPR.2018.00959
   Le T, 2017, COMPUT GRAPH-UK, V66, P103, DOI 10.1016/j.cag.2017.05.011
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li HY, 2018, LECT NOTES COMPUT SC, V11165, P570, DOI 10.1007/978-3-030-00767-6_53
   Li HY, 2019, INT CONF ACOUST SPEE, P4025, DOI [10.1109/ICASSP.2019.8683628, 10.1109/icassp.2019.8683628]
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Liu JC, 2019, VISUAL COMPUT, V35, P909, DOI 10.1007/s00371-019-01679-6
   Luo P, 2013, VISUAL COMPUT, V29, P587, DOI 10.1007/s00371-013-0824-2
   Maas AL., 2013, P ICML WORKSHOP DEEP, V28, P1
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Pan RJ, 2016, VISUAL COMPUT, V32, P601, DOI 10.1007/s00371-015-1076-0
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Radford A., 2015, ARXIV
   Shapira L, 2008, VISUAL COMPUT, V24, P249, DOI 10.1007/s00371-007-0197-5
   Shen YR, 2018, PROC CVPR IEEE, P4548, DOI 10.1109/CVPR.2018.00478
   Shu ZY, 2016, COMPUT AIDED GEOM D, V43, P39, DOI 10.1016/j.cagd.2016.02.015
   Souly N, 2017, IEEE I CONF COMP VIS, P5689, DOI 10.1109/ICCV.2017.606
   Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608
   Wang PY, 2018, COMPUT GRAPH-UK, V70, P128, DOI 10.1016/j.cag.2017.07.030
   Wang XG, 2018, VISUAL COMPUT, V34, P997, DOI 10.1007/s00371-018-1538-2
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wei MQ, 2019, IEEE T VIS COMPUT GR, V25, P2910, DOI 10.1109/TVCG.2018.2865363
   WU J, 2016, C NEUR INF PROC SYST, P82
   Xu HT, 2017, IEEE I CONF COMP VIS, P2717, DOI 10.1109/ICCV.2017.294
   Yi L, 2017, PROC CVPR IEEE, P6584, DOI 10.1109/CVPR.2017.697
   Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238
   YU F, 2019, IEEE C COMP VIS PATT
   Zhang E, 2005, ACM T GRAPHIC, V24, P1, DOI 10.1145/1037957.1037958
NR 48
TC 4
Z9 4
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2021
VL 37
IS 2
BP 325
EP 340
DI 10.1007/s00371-020-01801-z
EA FEB 2020
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QR4KJ
UT WOS:000510373600001
DA 2024-07-18
ER

PT J
AU Liu, JX
   Xia, YY
   Tang, Z
AF Liu, Jixin
   Xia, Yinyun
   Tang, Zheng
TI Privacy-preserving video fall detection using visual shielding
   information
SO VISUAL COMPUTER
LA English
DT Article
DE Fall detection; Privacy protection; Compressed sensing; Video
   surveillance; Feature extraction; Sparse recognition
ID SYSTEM
AB In recent years, fall detection, especially for the elderly living alone at home, is a challenge in the field of computer vision and pattern recognition. However, there is a concern of loss of privacy in intelligent visual surveillance. In order to solve the contradiction between security surveillance and privacy protection in vision-based fall detection methods, we propose a concept named visual shielding, which can be applied to eliminate visual information but not reduce monitoring function. In the preprocessing, the multilayer compressed sensing is used to compress video frames to achieve visual shielding effect. Then, object region is separated from shielded videos by the low-rank and sparse decomposition theory, based on which to extract motion trajectory features of the object via the dense trajectory algorithm. Finally, the fall detection issue is transformed into the sparse recognition problem of the signal. Experimental results on three public fall databases show that the specificity and sensitivity of the proposed method can be maintained at an ideal level, which has the dual advantages of high privacy protection and high recognition accuracy.
C1 [Liu, Jixin; Xia, Yinyun; Tang, Zheng] Nanjing Univ Posts & Telecommun, Engn Res Ctr Wideband Wireless Commun Technol, Minist Educ, Nanjing 210003, Peoples R China.
C3 Nanjing University of Posts & Telecommunications
RP Liu, JX (corresponding author), Nanjing Univ Posts & Telecommun, Engn Res Ctr Wideband Wireless Commun Technol, Minist Educ, Nanjing 210003, Peoples R China.
EM jessonlew@hotmail.com
RI LIU, Jixin/AHC-0596-2022
FU Provincial Natural Science Foundation of the Science and Technology
   Bureau of Jiangsu Province [BK20180088]; China Postdoctoral Science
   Foundation [2019M651916]; Scientific Research Foundation of Nanjing
   University of Posts and Telecommunications [NY218066]; Postgraduate
   Research & Practice Innovation Program of Jiangsu Province [KYCX18_0919]
FX This work was supported by funds from the Provincial Natural Science
   Foundation of the Science and Technology Bureau of Jiangsu Province
   (Grant No. BK20180088), the China Postdoctoral Science Foundation (Grant
   No. 2019M651916), the Scientific Research Foundation of Nanjing
   University of Posts and Telecommunications (Grant No. NY218066) and the
   Postgraduate Research & Practice Innovation Program of Jiangsu Province
   (Grant No. KYCX18_0919).
CR Ambrose AF, 2013, MATURITAS, V75, P51, DOI 10.1016/j.maturitas.2013.02.009
   Anderson Derek, 2006, Conf Proc IEEE Eng Med Biol Soc, V2006, P6388
   [Anonymous], 2011, P 28 INT C MACHINE L
   [Anonymous], 2009, IEEE INT C AUT FAC G
   [Anonymous], 2017, P 2017 9 INT C INF T, DOI DOI 10.1109/ICITEED.2017.8250441
   Auvinet E., 2010, Multiple cameras fall dataset
   Auvinet E, 2011, IEEE T INF TECHNOL B, V15, P290, DOI 10.1109/TITB.2010.2087385
   Bartlett MS, 2012, IEEE T NEURAL NETWOR, V13, P1450
   Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979
   Candès EJ, 2008, CR MATH, V346, P589, DOI 10.1016/j.crma.2008.03.014
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Charfi I, 2012, 8TH INTERNATIONAL CONFERENCE ON SIGNAL IMAGE TECHNOLOGY & INTERNET BASED SYSTEMS (SITIS 2012), P218, DOI 10.1109/SITIS.2012.155
   Chua JL, 2015, SIGNAL IMAGE VIDEO P, V9, P623, DOI 10.1007/s11760-013-0493-7
   Dafeng W, 2016, ELECT DES ENG, V24, P1126
   Debard Glen, 2012, Outdoor and Large-Scale Real-World Scene Analysis. 15th International Workshop on Theoretical Foundations of Computer Vision. Revised Selected Papers, P356, DOI 10.1007/978-3-642-34091-8_16
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Foroughi Homa, 2008, 2008 11th International Conference on Computer and Information Technology (ICCIT), P219, DOI 10.1109/ICCITECHN.2008.4803020
   Gan L, 2007, PROCEEDINGS OF THE 2007 15TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING, P403
   Igual R, 2013, BIOMED ENG ONLINE, V12, DOI 10.1186/1475-925X-12-66
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kwolek B, 2015, NEUROCOMPUTING, V168, P637, DOI 10.1016/j.neucom.2015.05.061
   Kwolek B, 2014, COMPUT METH PROG BIO, V117, P489, DOI 10.1016/j.cmpb.2014.09.005
   Lee T, 2005, J TELEMED TELECARE, V11, P194, DOI 10.1258/1357633054068946
   Li Y, 2012, IEEE T BIO-MED ENG, V59, P1291, DOI 10.1109/TBME.2012.2186449
   Liu JX, 2017, IET SIGNAL PROCESS, V11, P115, DOI 10.1049/iet-spr.2016.0026
   Liu JX, 2016, NEUROCOMPUTING, V196, P70, DOI 10.1016/j.neucom.2016.01.092
   Mastorakis G, 2014, J REAL-TIME IMAGE PR, V9, P635, DOI 10.1007/s11554-012-0246-9
   Mirmahboub B, 2013, IEEE T BIO-MED ENG, V60, P427, DOI 10.1109/TBME.2012.2228262
   Mousse MA, 2017, VISUAL COMPUT, V33, P1529, DOI 10.1007/s00371-016-1296-y
   Mubashir M, 2013, NEUROCOMPUTING, V100, P144, DOI 10.1016/j.neucom.2011.09.037
   Noury N, 2008, IRBM, V29, P340, DOI 10.1016/j.irbm.2008.08.002
   Patel VM, 2012, IEEE T INF FOREN SEC, V7, P954, DOI 10.1109/TIFS.2012.2189205
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Qian HM, 2008, 2008 10TH INTERNATIONAL CONFERENCE ON CONTROL AUTOMATION ROBOTICS & VISION: ICARV 2008, VOLS 1-4, P1567, DOI 10.1109/ICARCV.2008.4795758
   Rougier C, 2006, P 1 INT WORKSH VID P
   Rougier C, 2011, IEEE T CIRC SYST VID, V21, P611, DOI 10.1109/TCSVT.2011.2129370
   Shoaib M., 2010, 2010 Fourth Pacific-Rim Symposium on Image and Video Technology (PSIVT), P52, DOI 10.1109/PSIVT.2010.16
   Tao J., 2005, Proc. Fifth International Conference on Information, P1590, DOI DOI 10.1109/ICICS.2005.1689327
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang SK, 2016, MULTIMED TOOLS APPL, V75, P11603, DOI 10.1007/s11042-015-2698-y
   Weinland D, 2011, COMPUT VIS IMAGE UND, V115, P224, DOI 10.1016/j.cviu.2010.10.002
   Xue ZC, 2019, VISUAL COMPUT, V35, P1549, DOI 10.1007/s00371-018-1555-1
   Zerrouki N, 2018, MULTIMED TOOLS APPL, V77, P6405, DOI 10.1007/s11042-017-4549-5
   Zha ZY, 2018, VISUAL COMPUT, V34, P117, DOI 10.1007/s00371-016-1318-9
   Zhang Z, 2015, 8TH ACM INTERNATIONAL CONFERENCE ON PERVASIVE TECHNOLOGIES RELATED TO ASSISTIVE ENVIRONMENTS (PETRA 2015), DOI 10.1145/2769493.2769540
NR 45
TC 14
Z9 15
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2021
VL 37
IS 2
BP 359
EP 370
DI 10.1007/s00371-020-01804-w
EA JAN 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QR4KJ
UT WOS:000509275300001
DA 2024-07-18
ER

PT J
AU Raikwar, SC
   Tapaswi, S
AF Raikwar, Suresh Chandra
   Tapaswi, Shashikala
TI Tight lower bound on transmission for single image dehazing
SO VISUAL COMPUTER
LA English
DT Article
DE Atmospheric scattering; Defogging; Dehazing; Fog; Haze; Optimization;
   Restoration; Transmission
ID CONTRAST ENHANCEMENT; MODEL; VISIBILITY; SCATTERING; FRAMEWORK; SYSTEM
AB Effective functioning of outdoor vision systems depends upon the quality of input. Varying effects of light create different weather conditions (like raining, snowfall, haze, mist, fog, and cloud) due to optical properties of light and physical existence of different size particles in the atmosphere. Thus, outdoor images and videos captured in adverse environmental conditions have poor visibility due to scattering of light by atmospheric particles. Visibility restoration (dehazing) of degraded (hazy) images is critical for the useful performance of outdoor vision systems. Most of the existing methods of image dehazing considered atmospheric scattering model (ASM) to improve the visibility of hazy images or videos. According to ASM, the visual quality of dehazed image depends upon accurate estimation of transmission. Existing methods presented different priors with strong assumptions to estimate transmission. The proposed method introduces a tight lower bound on transmission. However, the accuracy of the proposed tight lower bound depends upon minimum color channel of haze-free image. Therefore, a prior is proposed to estimate the minimum color channel of the haze-free image. Furthermore, a blind assessment metric is proposed to evaluate the dehazing methods. Restored and matching corner points of the hazy and haze-free image are used to compute the proposed blind assessment metric. Obtained results are compared with renowned dehazing methods by qualitative and quantitative analysis to prove the efficacy of the proposed method.
C1 [Raikwar, Suresh Chandra; Tapaswi, Shashikala] ABV IIITM, Gwalior, Madhya Pradesh, India.
C3 ABV-Indian Institute of Information Technology & Management, Gwalior
RP Raikwar, SC (corresponding author), ABV IIITM, Gwalior, Madhya Pradesh, India.
EM sureshc@iiitm.ac.in
RI Raikwar, Suresh Chandra/AAO-5844-2021; TAPASWI, SHASHIKALA/AGZ-7714-2022
OI Raikwar, Suresh Chandra/0000-0001-5139-217X; 
CR [Anonymous], 2012, ECCV
   [Anonymous], 2003, The SSIM index for image quality assessment
   [Anonymous], 2016, IEEE C COMP VIS PATT
   Bui TM, 2018, IEEE T IMAGE PROCESS, V27, P999, DOI 10.1109/TIP.2017.2771158
   Choi K., 2017, 2017 ICASSP IEEE INT, P1, DOI DOI 10.1109/ITSC.2017.8317710
   Das Dawn D, 2016, VISUAL COMPUT, V32, P289, DOI 10.1007/s00371-015-1066-2
   Feng Y, 2016, COMM COM INF SC, V662, P345, DOI 10.1007/978-981-10-3002-4_29
   Hautière N, 2006, MACH VISION APPL, V17, P8, DOI [10.1007/s00138-005-0011-1, 10.1007/s00138-006-0011-9]
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Jha DK, 2016, IET COMPUT VIS, V10, P331, DOI 10.1049/iet-cvi.2014.0449
   Khmag A, 2018, VISUAL COMPUT, V34, P675, DOI 10.1007/s00371-017-1406-5
   Kim JH, 2013, J VIS COMMUN IMAGE R, V24, P410, DOI 10.1016/j.jvcir.2013.02.004
   Kim JY, 2001, IEEE T CIRC SYST VID, V11, P475, DOI 10.1109/76.915354
   Kim TK, 1998, IEEE T CONSUM ELECTR, V44, P82, DOI 10.1109/30.663733
   Li YJ, 2018, MOBILE NETW APPL, V23, P352, DOI 10.1007/s11036-017-0933-7
   Li YJ, 2016, COMPUT ELECTR ENG, V54, P68, DOI 10.1016/j.compeleceng.2016.08.008
   Li YA, 2016, NEUROCOMPUTING, V182, P221, DOI 10.1016/j.neucom.2015.12.032
   Ling ZG, 2017, NEUROCOMPUTING, V224, P82, DOI 10.1016/j.neucom.2016.10.050
   Ling ZG, 2016, VISUAL COMPUT, V32, P653, DOI 10.1007/s00371-015-1081-3
   Lu HM, 2018, MOBILE NETW APPL, V23, P368, DOI 10.1007/s11036-017-0932-8
   Lu HM, 2016, IEEE IMAGE PROC, P1998, DOI 10.1109/ICIP.2016.7532708
   Lu HM, 2016, MULTIMED TOOLS APPL, V75, P17081, DOI 10.1007/s11042-015-2977-7
   Lu HM, 2015, J OPT SOC AM A, V32, P886, DOI 10.1364/JOSAA.32.000886
   Ma K., 2015, P IEEE INT C IM PROC
   Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Narasimhan Srinivasa G, 2004, THESIS
   Nayar S. K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P820, DOI 10.1109/ICCV.1999.790306
   Nayar SK, 2003, P IEEE WORKSH COL PH
   Raikwar SC, 2018, MULTIMED TOOLS APPL, V77, P19719, DOI 10.1007/s11042-017-5398-y
   Schechner YY, 2001, PROC CVPR IEEE, P325
   Serikawa S, 2014, COMPUT ELECTR ENG, V40, P41, DOI 10.1016/j.compeleceng.2013.10.016
   Shwartz S., 2006, 2006 IEEE COMP SOC C, V2, P1984, DOI DOI 10.1109/CVPR.2006.71
   Stark JA, 2000, IEEE T IMAGE PROCESS, V9, P889, DOI 10.1109/83.841534
   Tan KK, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P788, DOI 10.1109/ICIP.2000.899827
   Tan R T, 2008, P IEEE C COMP VIS PA, P24
   Tang KT, 2014, PROC CVPR IEEE, P2995, DOI 10.1109/CVPR.2014.383
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Tarel JP, 2010, IEEE INT VEH SYM, P478, DOI 10.1109/IVS.2010.5548128
   Wang R, 2016, SIGNAL PROCESS, V127, P24, DOI 10.1016/j.sigpro.2016.02.003
   Wang WC, 2017, IEEE T MULTIMEDIA, V19, P1142, DOI 10.1109/TMM.2017.2652069
   Wang WC, 2017, NEUROCOMPUTING, V238, P365, DOI 10.1016/j.neucom.2017.01.075
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiao CX, 2012, VISUAL COMPUT, V28, P713, DOI 10.1007/s00371-012-0679-y
   Xu Y, 2016, IEEE ACCESS, V4, P165, DOI 10.1109/ACCESS.2015.2511558
   Yuan H, 2017, IEEE ACCESS, V5, P1735, DOI 10.1109/ACCESS.2017.2660302
   Zhang YQ, 2012, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2012-220
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 51
TC 10
Z9 10
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2020
VL 36
IS 1
BP 191
EP 209
DI 10.1007/s00371-018-1596-5
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ3OJ
UT WOS:000511966800015
DA 2024-07-18
ER

PT J
AU Zhou, F
   Hu, Y
   Shen, XK
AF Zhou, Feng
   Hu, Yong
   Shen, Xukun
TI MSANet: multimodal self-augmentation and adversarial network for RGB-D
   object recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Object recognition; Adversarial network; Multimodal
AB This paper researches on the problem of object recognition using RGB-D data. Although deep convolutional neural networks have so far made progress in this area, they are still suffering a lot from lack of large-scale manually labeled RGB-D data. Labeling large-scale RGB-D dataset is a time-consuming and boring task. More importantly, such large-scale datasets often exist a long tail, and those hard positive examples of the tail can hardly be recognized. To solve these problems, we propose a multimodal self-augmentation and adversarial network (MSANet) for RGB-D object recognition, which can augment the data effectively at two levels while keeping the annotations. Toward the first level, series of transformations are leveraged to generate class-agnostic examples for each instance, which supports the training of our MSANet. Toward the second level, an adversarial network is proposed to generate class-specific hard positive examples while learning to classify them correctly to further improve the performance of our MSANet. Via the above schemes, the proposed approach wins the best results on several available RGB-D object recognition datasets, e.g., our experimental results indicate a 1.5% accuracy boost on benchmark Washington RGB-D object dataset compared with the current state of the art.
C1 [Zhou, Feng; Shen, Xukun] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Hu, Yong] Beihang Univ, Sch New Media Art & Design, Beijing, Peoples R China.
C3 Beihang University; Beihang University
RP Shen, XK (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
EM zhoufeng@buaa.edu.cn; huyong@buaa.edu.cn; xkshen@buaa.edu.cn
RI ARSLAN, Okan/AAA-3232-2020
FU MS-RA CCRP Funding [FY16-RES-THEME-039]
FX This work is supported in part by MS-RA CCRP Funding FY16-RES-THEME-039.
   The authors thank all the anonymous reviewers for their very helpful
   comments to improve this paper.
CR [Anonymous], 2013, INT C MACH LEARN PML
   [Anonymous], 2017, C COMP VIS PATT REC
   [Anonymous], 2013, UNSUPERVISED FEATURE
   [Anonymous], 2015, ARXIV151106343
   [Anonymous], 2014, P AS C COMP VIS
   Bai J, 2014, LECT NOTES COMPUT SC, V8588, P235, DOI 10.1007/978-3-319-09333-8_25
   Blum M, 2012, IEEE INT CONF ROBOT, P1298, DOI 10.1109/ICRA.2012.6225188
   Bo L., 2011, Neural Information Processing Systems, P2115
   Bo LF, 2011, IEEE INT C INT ROBOT, P821, DOI 10.1109/IROS.2011.6048717
   Browatzki B., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1189, DOI 10.1109/ICCVW.2011.6130385
   Cheng Y., 2016, INT JOINT C ART INT, P3345
   Cheng YH, 2015, IEEE I CONF COMP VIS, P145, DOI 10.1109/ICCV.2015.25
   Cheng YH, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P135, DOI 10.1109/3DV.2015.23
   Cheng YH, 2015, COMPUT VIS IMAGE UND, V139, P149, DOI 10.1016/j.cviu.2015.05.007
   Cheng YH, 2014, INT C PATT RECOG, P2377, DOI 10.1109/ICPR.2014.412
   CIREAN DC, 2011, ARXIV11020183
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Denton Emily, 2015, Advances in Neural Information Processing Systems
   Dosovitskiy A, 2016, IEEE T PATTERN ANAL, V38, P1734, DOI 10.1109/TPAMI.2015.2496141
   Eitel A, 2015, IEEE INT C INT ROBOT, P681, DOI 10.1109/IROS.2015.7353446
   Goodfellow I., 2016, NIPS 2016 TUTORIAL G
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang GL, 2017, IEEE ICC
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai K, 2011, IEEE INT CONF ROBOT, P1817
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin S, 2016, VISUAL COMPUT, V32, P681, DOI 10.1007/s00371-016-1245-9
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Radford A., 2015, ARXIV
   Rasool S, 2016, VISUAL COMPUT, V32, P1311, DOI 10.1007/s00371-016-1224-1
   Redondo-Cabrera C, 2012, PROC CVPR IEEE, P3458, DOI 10.1109/CVPR.2012.6248087
   Salimans T., 2016, P 30 C NEUR INF PROC, P2234
   Schwarz M, 2015, IEEE INT CONF ROBOT, P1329, DOI 10.1109/ICRA.2015.7139363
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Socher R., 2012, NIPS, V3, P8
   Song XB, 2014, VISUAL COMPUT, V30, P855, DOI 10.1007/s00371-014-0965-y
   Srivastava RK., 2015, P 28 INT C NEURAL IN, P2377, DOI DOI 10.48550/ARXIV.1507.06228
   Tang YL, 2016, VISUAL COMPUT, V32, P111, DOI 10.1007/s00371-014-1059-6
   Wang AR, 2015, IEEE I CONF COMP VIS, P1125, DOI 10.1109/ICCV.2015.134
   Wang XL, 2015, IEEE I CONF COMP VIS, P2794, DOI 10.1109/ICCV.2015.320
   Wei YC, 2017, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR.2017.687
   Yang YL, 2011, PROC IEEE MICR ELECT, P79, DOI 10.1109/MEMSYS.2011.5734366
   Yu LT, 2017, AAAI CONF ARTIF INTE, P2852
NR 50
TC 10
Z9 11
U1 4
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2019
VL 35
IS 11
SI SI
BP 1583
EP 1594
DI 10.1007/s00371-018-1559-x
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JD5IZ
UT WOS:000490018000007
DA 2024-07-18
ER

PT J
AU Sadeghi, B
   Jamshidi, K
   Vafaei, A
   Monadjemi, SA
AF Sadeghi, Bahman
   Jamshidi, Kamal
   Vafaei, Abbas
   Monadjemi, S. Amirhassan
TI A local image descriptor based on radial and angular gradient intensity
   histogram for blurred image matching
SO VISUAL COMPUTER
LA English
DT Article
DE Blurred image matching; Image descriptor; Scale invariant; Rotation
   invariant
ID FEATURES; SCALE
AB Image rotation and scale change can significantly degrade the efficiency of local descriptors in blurred image matching. Conventional local image descriptors often only employ the rectangular gradient information of detected region around each interest point. Due to unwanted errors estimated for scale and dominant orientation, the performance of these local descriptors is severely degraded when applied to blurred images. To solve this problem, we propose a novel descriptor called radial and angular gradient intensity histogram (RAGIH) which jointly utilizes gradient and intensity features. In this local descriptor, feature vectors are extracted from two concentric circular regions around each key point and using angular and radial gradients in a specific local coordinate system reduces the estimation errors. Extensive experiments on challenging Oxford dataset demonstrate the favorable performance of our descriptor compared to state-of-the-art approaches.
C1 [Sadeghi, Bahman; Jamshidi, Kamal; Vafaei, Abbas; Monadjemi, S. Amirhassan] Univ Isfahan, Fac Comp Engn, Esfahan, Iran.
C3 University of Isfahan
RP Jamshidi, K (corresponding author), Univ Isfahan, Fac Comp Engn, Esfahan, Iran.
EM B.Sadeghi@eng.ui.ac.ir; Jamshidi@eng.ui.ac.ir;
   Abbas_Vafaei@eng.ui.ac.ir; Monadjemi@eng.ui.ac.ir
CR Agarwal S, 2009, IEEE I CONF COMP VIS, P72, DOI 10.1109/ICCV.2009.5459148
   Agarwal S, 2010, COMPUTER, V43, P40, DOI 10.1109/MC.2010.175
   Aguilera C, 2012, SENSORS-BASEL, V12, P12661, DOI 10.3390/s120912661
   [Anonymous], 2004, IMAGEDATASET
   [Anonymous], 2004, O R SITE
   [Anonymous], IEEE COMP SOC C COMP
   [Anonymous], DATASETS
   [Anonymous], 2003, ZUBUD DATASET ZURICH
   Baumberg A., 2000, IEEE COMP SOC C COMP
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Berretti S, 2011, VISUAL COMPUT, V27, P1021, DOI 10.1007/s00371-011-0611-x
   Desai A., 2014, USING AFFINE FEATURE
   Frahm JM, 2010, LECT NOTES COMPUT SC, V6314, P368, DOI 10.1007/978-3-642-15561-1_27
   FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Heikkilä M, 2009, PATTERN RECOGN, V42, P425, DOI 10.1016/j.patcog.2008.08.014
   Kang TK, 2015, PATTERN RECOGN, V48, P670, DOI 10.1016/j.patcog.2014.06.022
   Ke Y, 2004, PROC CVPR IEEE, P506
   KOENDERINK JJ, 1987, BIOL CYBERN, V55, P367, DOI 10.1007/BF00318371
   Lacheheb H, 2017, MULTIMED TOOLS APPL, V76, P6333, DOI 10.1007/s11042-015-3167-3
   Lazebnik S., 2005, COMPUTER VISION PATT
   Li CL, 2010, VISUAL COMPUT, V26, P227, DOI 10.1007/s00371-009-0400-y
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Schaffalitzky F, 2002, LECT NOTES COMPUT SC, V2350, P414
   Shang J, 2017, VISUAL COMPUT, V33, P221, DOI 10.1007/s00371-015-1179-7
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Szeliski R, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000009
   Tamrakar D, 2016, MULTIMED TOOLS APPL, V75, P5777, DOI 10.1007/s11042-015-2541-5
   Ting G., 2013, INFRARED IMAGING APP
   Tola E, 2008, PROC CVPR IEEE, P2578
   Tuytelaars T, 2004, INT J COMPUT VISION, V59, P61, DOI 10.1023/B:VISI.0000020671.28016.e8
   Wang ZH, 2011, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2011.6126294
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
   Zhang MM, 2014, OPTIK, V125, P1469, DOI 10.1016/j.ijleo.2013.09.007
   Zheng M., 2014, IEEE NAV CONTR C CHI
   Zhou W, 2014, IEEE SIGNAL PROC LET, V21, P339, DOI 10.1109/LSP.2013.2294458
NR 40
TC 6
Z9 6
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2019
VL 35
IS 10
BP 1373
EP 1391
DI 10.1007/s00371-018-01616-z
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IU7PN
UT WOS:000483775900004
DA 2024-07-18
ER

PT J
AU Sharma, RP
   Dey, S
AF Sharma, Ram Prakash
   Dey, Somnath
TI Fingerprint liveness detection using local quality features
SO VISUAL COMPUTER
LA English
DT Article
DE Biometrics; Presentation attack detection; Fingerprint liveness; Quality
   features
ID CLASSIFICATION; PERSPIRATION; RECOGNITION; IRIS
AB Fingerprint-based recognition is widely deployed in different domains. However, current recognition systems are vulnerable to presentation attack. Presentation attack utilizes an artificial replica of a fingerprint to deceive the sensors. In such scenarios, fingerprint liveness detection is required to ensure the actual presence of a live fingerprint. In this paper, we propose a static software-based approach using quality features to detect the liveness in a fingerprint image. The proposed method extracts eight sensor-independent quality features from the detailed ridge-valley structure of a fingerprint at the local level to form a 13-dimensional feature vector. Sequential Forward Floating Selection and Random Forest Feature Selection are used to select the optimal feature set from the created feature vector. To classify fake and live fingerprints, we have used support vector machine, random forest, and gradient boosted tree classifiers. The proposed method is tested on a publically available database of LivDet 2009 competition. The experimental results demonstrate that the least average classification error of 5.3% is achieved on LivDet 2009 database, exhibiting supremacy of the proposed method over current state-of-the-art approaches. Additionally, we have analyzed the importance of individual features on LivDet 2009 database, and effectiveness of the best-performing features is evaluated on LivDet 2011, 2013, and 2015 databases. The obtained results depict that the proposed approach is able to perform well irrespective of the different sensors and materials used in these databases. Further, the proposed method utilizes a single fingerprint image. This characteristic makes our method more user-friendly, faster, and less intrusive.
C1 [Sharma, Ram Prakash; Dey, Somnath] Indian Inst Technol Indore, Discipline Comp Sci & Engn, Indore, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Indore
RP Sharma, RP (corresponding author), Indian Inst Technol Indore, Discipline Comp Sci & Engn, Indore, India.
EM er.ramsharma28@gmail.com; somnathd@iiti.ac.in
RI Sharma, Ram Prakash/IAN-9085-2023
OI Sharma, Ram Prakash/0000-0002-1851-2325; Sharma, Ram
   Prakash/0000-0002-3364-2240
FU Science and Engineering Research Board (SERB) [ECR/2017/000027]
FX This research is supported by the Science and Engineering Research Board
   (SERB) Grant Number ECR/2017/000027.
CR Abhyankar A, 2006, IEEE IMAGE PROC, P321, DOI 10.1109/ICIP.2006.313158
   Abhyankar A, 2009, PATTERN RECOGN, V42, P452, DOI 10.1016/j.patcog.2008.06.012
   [Anonymous], 2015, Biometrics Theory, Applications and Systems (BTAS), 2015 IEEE 7th International Conference on
   [Anonymous], P SPIE BIOM TECHN HU
   [Anonymous], OPT ENG
   [Anonymous], 2013, C BIOMETRICS ICB 201
   [Anonymous], 2010, TECH REP
   [Anonymous], P SPIE BIOM TECHN HU
   [Anonymous], 2011, P INT C HAND BAS BIO
   [Anonymous], 2012, Sens. Biosens. MEMS Technol. Appl.
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Choi H, 2007, PROC WRLD ACAD SCI E, V22, P157
   Choi H, 2008, PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON NANOCHANNELS, MICROCHANNELS, AND MINICHANNELS, PTS A AND B, P1005
   Chu YJ, 2019, VISUAL COMPUT, V35, P239, DOI 10.1007/s00371-017-1468-4
   DeCann B, 2009, LECT NOTES COMPUT SC, V5558, P627, DOI 10.1007/978-3-642-01793-3_64
   Derakhshani R, 2003, PATTERN RECOGN, V36, P383, DOI 10.1016/S0031-3203(02)00038-9
   Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451
   Galbally J., 2009, 2009 First IEEE International Conference on Biometrics, Identity and Security (BIdS), P1
   Galbally J, 2014, IEEE T IMAGE PROCESS, V23, P710, DOI 10.1109/TIP.2013.2292332
   Galbally J, 2012, FUTURE GENER COMP SY, V28, P311, DOI 10.1016/j.future.2010.11.024
   Ghiani L, 2017, IMAGE VISION COMPUT, V58, P110, DOI 10.1016/j.imavis.2016.07.002
   Ghiani L, 2013, INT CONF BIOMETR, DOI 10.1109/ICB.2013.6613027
   Ghiani L, 2012, INT C PATT RECOG, P537
   Ghiani L, 2012, LECT NOTES COMPUT SC, V7378, P210, DOI 10.1007/978-3-642-31567-1_21
   Gottschlich C, 2014, P IEEE INT JOINT C B, P1
   Huang QG, 2015, PATTERN RECOGN LETT, V60-61, P1, DOI 10.1016/j.patrec.2015.03.015
   Jia J, 2007, LECT NOTES COMPUT SC, V4642, P309
   Kim W, 2017, IEEE SIGNAL PROC LET, V24, P51, DOI 10.1109/LSP.2016.2636158
   Lee HS, 2009, LECT NOTES COMPUT SC, V5707, P318
   Li CC, 2015, VISUAL COMPUT, V31, P1419, DOI 10.1007/s00371-014-1023-5
   Lim E, 2004, IEEE IMAGE PROC, P1241
   Maltoni D., 2009, HDB FINGERPRINT RECO, DOI 10.1007/978-1-84882-254-2
   Manivanan N, 2010, ELECTRON LETT, V46, P1268, DOI 10.1049/el.2010.1549
   Marasco E, 2012, PATTERN RECOGN LETT, V33, P1148, DOI 10.1016/j.patrec.2012.01.009
   Marcialis Gian Luca, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1289, DOI 10.1109/ICPR.2010.321
   Marcialis GL, 2009, LECT NOTES COMPUT SC, V5716, P12, DOI 10.1007/978-3-642-04146-4_4
   Moon YS, 2005, ELECTRON LETT, V41, P1112, DOI 10.1049/el:20052577
   Nikam Shankar Bhausaheb, 2008, 2008 1st International Conference on Emerging Trends in Engineering and Technology (ICETET), P675, DOI 10.1109/ICETET.2008.134
   Nikam SB, 2008, I C COMP GRAPH IM VI, P217, DOI 10.1109/CGIV.2008.9
   Nikam SB, 2010, SIGNAL IMAGE VIDEO P, V4, P75, DOI 10.1007/s11760-008-0098-8
   Nikam SB, 2009, NEUROCOMPUTING, V72, P2491, DOI 10.1016/j.neucom.2008.11.003
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Olsen Martin Aastrup, 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P158, DOI 10.1109/ICB.2012.6199802
   Olsen MA, 2016, IET BIOMETRICS, V5, P47, DOI 10.1049/iet-bmt.2014.0055
   PUDIL P, 1994, PATTERN RECOGN LETT, V15, P1119, DOI 10.1016/0167-8655(94)90127-9
   Smola AJ, 1999, ADV NEUR IN, V11, P585
   Tabassi E, 2005, IEEE IMAGE PROC, P1833
   Tan B, 2008, J ELECTRON IMAGING, V17, DOI 10.1117/1.2885133
   Wang ZF, 2014, VISUAL COMPUT, V30, P359, DOI 10.1007/s00371-013-0861-x
   Xia ZH, 2017, SIGNAL IMAGE VIDEO P, V11, P381, DOI 10.1007/s11760-016-0936-z
   Yambay D., 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P208, DOI 10.1109/ICB.2012.6199810
   Yuan CS, 2016, CHINA COMMUN, V13, P60, DOI 10.1109/CC.2016.7559076
   Zhang YL, 2014, LECT NOTES COMPUT SC, V8833, P191, DOI 10.1007/978-3-319-12484-1_21
NR 55
TC 27
Z9 27
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2019
VL 35
IS 10
BP 1393
EP 1410
DI 10.1007/s00371-018-01618-x
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IU7PN
UT WOS:000483775900005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Luciano, L
   Ben Hamza, A
AF Luciano, Lorenzo
   Ben Hamza, A.
TI Deep similarity network fusion for 3D shape classification
SO VISUAL COMPUTER
LA English
DT Article
DE Shape classification; Deep learning; Geodesic moments; Similarity
   network fusion; Graph networks
ID SIGNATURE
AB In this paper, we introduce a deep similarity network fusion framework for 3D shape classification using a graph convolutional neural network, which is an efficient and scalable deep learning model for graph-structured data. The proposed approach coalesces the geometrical discriminative power of geodesic moments and similarity network fusion in an effort to design a simple, yet discriminative shape descriptor. This geometric shape descriptor is then fed into a graph convolutional neural network to learn a deep feature representation of a 3D shape. We validate the predictive power of our method on ModelNet shape benchmarks, demonstrating that the proposed framework yields significant performance gains compared to state-of-the-art approaches.
C1 [Luciano, Lorenzo] Amazon Web Serv, Boston, MA USA.
   [Ben Hamza, A.] Concordia Univ, Concordia Inst Informat Syst Engn, Montreal, PQ, Canada.
C3 Amazon.com; Concordia University - Canada
RP Ben Hamza, A (corresponding author), Concordia Univ, Concordia Inst Informat Syst Engn, Montreal, PQ, Canada.
EM hamza@ciise.concordia.ca
RI Hamza, Abdessamad Ben/G-4571-2013
OI Ben Hamza, Abdessamad/0000-0002-3778-8167
FU NSERC [N00929]
FX This work was supported in part by and NSERC Discovery Grant Number
   N00929.
CR [Anonymous], 2015, ARXIV150106297
   [Anonymous], 2016, P EUR WORKSH 3D OBJ
   [Anonymous], P CVPR
   [Anonymous], 2017, P EUR WORKSH 3D OBJ
   [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   [Anonymous], 2017, P ICIP
   [Anonymous], P CVPR
   [Anonymous], 2017, P CVPR
   [Anonymous], 2016, P CVPR
   [Anonymous], COMPUT GRAPH FORUM
   [Anonymous], 2018, P CVPR
   [Anonymous], P EUR C 3D OBJ RETR
   Chaudhari AJ, 2014, PHYS MED BIOL, V59, P961, DOI 10.1088/0031-9155/59/4/961
   Chen XJ, 2017, LECT NOTES COMPUT SC, V10628, P3, DOI 10.1007/978-3-319-71147-8_1
   Eslami SMA, 2014, INT J COMPUT VISION, V107, P155, DOI 10.1007/s11263-013-0669-1
   Fang Y, 2015, PROC CVPR IEEE, P2319, DOI 10.1109/CVPR.2015.7298845
   Hechtlinger Y., 2017, Machine Learning
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Leng B, 2019, IEEE T VIS COMPUT GR, V25, P2896, DOI 10.1109/TVCG.2018.2865317
   Li CY, 2013, VISUAL COMPUT, V29, P513, DOI 10.1007/s00371-013-0815-3
   Luciano L, 2018, PATTERN RECOGN LETT, V105, P182, DOI 10.1016/j.patrec.2017.05.011
   Peng JW, 2018, BOUND VALUE PROBL, DOI 10.1186/s13661-017-0922-6
   Pickup D, 2016, INT J COMPUT VISION, V120, P169, DOI 10.1007/s11263-016-0903-8
   Shen W, 2013, PATTERN RECOGN, V46, P539, DOI 10.1016/j.patcog.2012.07.023
   Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802
   Sinha A, 2016, LECT NOTES COMPUT SC, V9910, P223, DOI 10.1007/978-3-319-46466-4_14
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang B, 2014, NAT METHODS, V11, P333, DOI [10.1038/nmeth.2810, 10.1038/NMETH.2810]
   Wu J, 2016, 2016 2ND INTERNATIONAL CONFERENCE ON SOCIAL, EDUCATION AND MANAGEMENT ENGINEERING (SEME 2016), P82
   Xu X, 2016, INT C PATT RECOG, P3506, DOI 10.1109/ICPR.2016.7900177
   Ye JB, 2016, VISUAL COMPUT, V32, P553, DOI 10.1007/s00371-015-1071-5
   Zhu ZT, 2016, NEUROCOMPUTING, V204, P41, DOI 10.1016/j.neucom.2015.08.127
NR 35
TC 12
Z9 12
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 1171
EP 1180
DI 10.1007/s00371-019-01668-9
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200032
OA Bronze
DA 2024-07-18
ER

PT J
AU Zhang, HY
   Wu, CL
   Deng, JS
   Liu, Z
   Yang, YN
AF Zhang, Huayan
   Wu, Chunlin
   Deng, Jiansong
   Liu, Zheng
   Yang, Yuning
TI A new two-stage mesh surface segmentation method
SO VISUAL COMPUTER
LA English
DT Article
DE Mesh surface segmentation; Piecewise constant function space; Spectral
   analysis; Mumford-Shah model; Laplacian matrix; ADMM; Augmented
   Lagrangian
ID IMAGE SEGMENTATION; MUMFORD; ALGORITHM; SIMPLEX; POINT
AB Partitioning a mesh surface into several semantic components is a fundamental task in geometry processing. This paper presents a new stable and effective segmentation method, which contains two stages. The first stage is a spectral clustering procedure, while the second stage is a variational refining procedure. For spectral clustering, we construct a new Laplacian matrix which reflects more semantic information than classical Laplacian matrices. By this new Laplacian, we introduce a simple and fast spectral clustering method, which gives quite satisfying segmentation results for most surfaces and provides a good initialization for the second stage. In the second stage, we propose a variational refining procedure by a new discretization of the classical non-convex Mumford-Shah model. The variational problem is solved by efficient iterative algorithms based on alternating minimization and alternating direction method of multipliers (ADMM). The first stage provides a good initialization for the second stage, while the second stage refines the result of the first stage well. Experiments demonstrated that our method is very stable and effective compared to existing approaches. It outperforms competitive segmentation methods when evaluated on the Princeton Segmentation Benchmark.
C1 [Zhang, Huayan] Tianjin Polytech Univ, Sch Comp Sci & Software Engn, Tianjin, Peoples R China.
   [Wu, Chunlin] Nankai Univ, Sch Math Sci, Tianjin, Peoples R China.
   [Deng, Jiansong] Univ Sci & Technol China, Sch Math Sci, Hefei, Anhui, Peoples R China.
   [Liu, Zheng] China Univ Geosci, Dept Informat Engn, Beijing, Peoples R China.
   [Yang, Yuning] South China Normal Univ, Sch Math, Guangzhou, Guangdong, Peoples R China.
C3 Tiangong University; Nankai University; Chinese Academy of Sciences;
   University of Science & Technology of China, CAS; China University of
   Geosciences; South China Normal University
RP Wu, CL (corresponding author), Nankai Univ, Sch Math Sci, Tianjin, Peoples R China.
EM zhanghuayan@tjpu.edu.cn; wucl@nankai.edu.cn; dengjs@ustc.edu.cn
RI Deng, Jiansong/F-1869-2010; chen, minghui/KFR-8832-2024; li,
   fangyu/KCY-0521-2024; Wu, Chunlin/H-1171-2017
OI li, fangyu/0009-0009-8303-9157; 
FU NSF of China [11626169, 11301289, 11371341, 61602341, 11531013]
FX We would like to thank Pengfei Xu, Noa Fish, Evangelos Kalogerakis for
   providing their segmentation data of [3,17] and [18,19], and the
   Princeton Segmentation Benchmark [9]. This work was supported by the NSF
   of China (Nos. 11626169, 11301289, 11371341, 61602341 and 11531013).
CR [Anonymous], 2007, Computer-Aided Design Applications, DOI DOI 10.1080/16864360.2007.10738515
   [Anonymous], 1989, GRADUATE TEXTS MATH
   Attene M, 2006, VISUAL COMPUT, V22, P181, DOI 10.1007/s00371-006-0375-x
   Au OKC, 2012, IEEE T VIS COMPUT GR, V18, P1125, DOI 10.1109/TVCG.2011.131
   Benhabiles H, 2010, VISUAL COMPUT, V26, P1451, DOI 10.1007/s00371-010-0494-2
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   CHAMBOLLE A, 1995, SIAM J APPL MATH, V55, P827, DOI 10.1137/S0036139993257132
   Chan TF, 2006, SIAM J APPL MATH, V66, P1632, DOI 10.1137/040615286
   Chen J, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618492
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   Cole F, 2008, ACM T GRAPHIC, V27, DOI [10.1145/1360612.1360657, 10.1145/1360612.1360687]
   Delaunoy A, 2009, IEEE I CONF COMP VIS, P662, DOI 10.1109/ICCV.2009.5459174
   Garland M., 2001, I3D 01, P49, DOI [DOI 10.1145/364338.364345, 10.1145/364338.364345]
   Gelfand Natasha, 2004, Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing, P214
   Guo K, 2015, ACM T GRAPHIC, V35, DOI 10.1145/2835487
   HOFFMAN DD, 1984, COGNITION, V18, P65, DOI 10.1016/0010-0277(84)90022-2
   Kalogerakis E., 2017, CVPR, P1
   Kalogerakis E, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778839
   Katz S, 2005, VISUAL COMPUT, V21, P649, DOI 10.1007/s00371-005-0344-9
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   Lai RJ, 2011, COMPUT VIS IMAGE UND, V115, P1647, DOI 10.1016/j.cviu.2011.05.011
   Lai YK, 2008, SPM 2008: PROCEEDINGS OF THE ACM SOLID AND PHYSICAL MODELING SYMPOSIUM, P183
   Lavoué G, 2005, COMPUT AIDED DESIGN, V37, P975, DOI 10.1016/j.cad.2004.09.001
   Lellmann J, 2011, SIAM J IMAGING SCI, V4, P1049, DOI 10.1137/100805844
   Lellmann J, 2009, LECT NOTES COMPUT SC, V5567, P150, DOI 10.1007/978-3-642-02256-2_13
   Lie J, 2006, IEEE T IMAGE PROCESS, V15, P1171, DOI 10.1109/TIP.2005.863956
   Liu R, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P298
   Liu ZB, 2013, COMPUT GRAPH-UK, V37, P553, DOI 10.1016/j.cag.2013.05.021
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Mangan AP, 1999, IEEE T VIS COMPUT GR, V5, P308, DOI 10.1109/2945.817348
   MICHELOT C, 1986, J OPTIMIZ THEORY APP, V50, P195, DOI 10.1007/BF00938486
   MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503
   Ng MK, 2010, SIAM J SCI COMPUT, V32, P2710, DOI 10.1137/090774823
   Shamir A, 2008, COMPUT GRAPH FORUM, V27, P1539, DOI 10.1111/j.1467-8659.2007.01103.x
   Shapira L, 2008, VISUAL COMPUT, V24, P249, DOI 10.1007/s00371-007-0197-5
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   van Kaick O, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2611811
   Vese LA, 2002, INT J COMPUT VISION, V50, P271, DOI 10.1023/A:1020874308076
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Wang PS, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818068
   Wu CL, 2012, J SCI COMPUT, V50, P145, DOI 10.1007/s10915-011-9477-3
   Wu CL, 2010, SIAM J IMAGING SCI, V3, P300, DOI 10.1137/090767558
   Wu CL, 2010, IEEE T VIS COMPUT GR, V16, P647, DOI 10.1109/TVCG.2009.103
   Wu JH, 2005, COMPUT GRAPH FORUM, V24, P277, DOI 10.1111/j.1467-8659.2005.00852.x
   Xin SQ, 2012, IEEE T VIS COMPUT GR, V18, P879, DOI 10.1109/TVCG.2011.119
   Yan DM, 2012, COMPUT AIDED DESIGN, V44, P1072, DOI 10.1016/j.cad.2012.04.005
   Yang JF, 2010, IEEE J-STSP, V4, P288, DOI 10.1109/JSTSP.2010.2042333
   Zhang H, 2010, COMPUT GRAPH FORUM, V29, P1865, DOI 10.1111/j.1467-8659.2010.01655.x
   Zhang HY, 2015, IEEE T VIS COMPUT GR, V21, P873, DOI 10.1109/TVCG.2015.2398432
   Zhang JY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2167076.2167079
   Zhang JY, 2011, IEEE T VIS COMPUT GR, V17, P357, DOI 10.1109/TVCG.2010.57
   Zhang WY, 2015, COMPUT GRAPH FORUM, V34, P23, DOI 10.1111/cgf.12742
   Zheng YY, 2012, IEEE T VIS COMPUT GR, V18, P1304, DOI 10.1109/TVCG.2011.140
NR 54
TC 8
Z9 9
U1 0
U2 24
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2018
VL 34
IS 11
BP 1597
EP 1615
DI 10.1007/s00371-017-1434-1
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GV9ZR
UT WOS:000446521700010
DA 2024-07-18
ER

PT J
AU Kim, JH
   Kim, W
   Lee, J
AF Kim, Jong-Hyun
   Kim, Wook
   Lee, Jung
TI Physics-inspired approach to realistic and stable water spray with
   narrowband air particles
SO VISUAL COMPUTER
LA English
DT Article
DE Water spray; Vortex effects; Narrowband air particle; Particle-based
   fluids
ID SIMULATION; SMOKE; TURBULENCE; FLOWS
AB We propose an efficient and physics-inspired method for producing water spray effects by modeling air particles within a narrowband of the water surface in particle-based water simulation. In the real world, water and air continuously interact with each other around free surfaces, and this phenomenon is commonly observed in waterfalls or in rough sea waves. Due to the small volume of water spray, the interfaces between water and air become vague, and the interactions between water and air lead to strong vortex phenomena. To express these phenomena, we propose the generation of narrowband air cells in particle-based water simulations and the expression of water spray effects by creating and evolving air particles in narrowband air cells. We guarantee the robustness of the simulation by solving the drifting problem that occurs when the number of adjacent air particles is insufficient. Experiments convincingly demonstrate that the proposed approach is efficient and easy to use while delivering high-quality results. We produce efficient water spray effects from coarse simulation as an independent post-process that can be applied to most particle-based fluid solvers.
C1 [Kim, Jong-Hyun] Kangnam Univ, Dept Software Applicat, Yongin, South Korea.
   [Kim, Wook] Korea Univ, Seoul, South Korea.
   [Lee, Jung] Hallym Univ, Dept Convergence Software, Chunchon, South Korea.
C3 Kangnam University; Korea University; Hallym University
RP Lee, J (corresponding author), Hallym Univ, Dept Convergence Software, Chunchon, South Korea.
EM gogogoscv@gmail.com; marverous@naver.com; airjung@hallym.ac.kr
FU Hallym University Research Fund [HRF-201609-008]; Institute for
   Information & communications Technology Promotion (IITP) grant - Korea
   government (MSIP) [IITP-2016-R7518-16-1028]; Basic Science Research
   Program through the National Research Foundation of Korea (NRF) -
   Ministry of Education, Science and Technology [NRF-2013R1A1A2011602]
FX This research was supported by a Hallym University Research Fund
   (HRF-201609-008), Institute for Information & communications Technology
   Promotion (IITP) grant funded by the Korea government (MSIP)
   (IITP-2016-R7518-16-1028), and Basic Science Research Program through
   the National Research Foundation of Korea (NRF) funded by the Ministry
   of Education, Science and Technology (NRF-2013R1A1A2011602).
CR Angelidis Alexis., 2006, S COMPUTER ANIMATION, P25
   [Anonymous], ACM SIGGRAPH ASIA
   [Anonymous], 2008, STRUCTURALLY ORDERED
   [Anonymous], 2005, Proceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, DOI DOI 10.1145/1073368.1073380
   Becker M, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P209
   Cleary P. W., 2007, ACM SIGGRAPH
   Colagrossi A, 2003, J COMPUT PHYS, V191, P448, DOI 10.1016/S0021-9991(03)00324-3
   Dobashi Y, 2008, COMPUT GRAPH FORUM, V27, P477, DOI 10.1111/j.1467-8659.2008.01145.x
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   He SF, 2011, COMPUT ANIMAT VIRT W, V22, P107, DOI 10.1002/cav.408
   He XW, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2682630
   Hong J. M., 2007, ACM SIGGRAPH
   Ihmsen M, 2012, VISUAL COMPUT, V28, P669, DOI 10.1007/s00371-012-0697-9
   Kim D., 2009, ACM SIGGRAPH ASIA
   Kim D, 2012, IEEE T VIS COMPUT GR, V18, P1488, DOI 10.1109/TVCG.2011.264
   Klingner BM, 2006, ACM T GRAPHIC, V25, P820, DOI 10.1145/1141911.1141961
   Lee HY, 2009, VISUAL COMPUT, V25, P707, DOI 10.1007/s00371-009-0338-0
   Losasso F, 2004, ACM T GRAPHIC, V23, P457, DOI 10.1145/1015706.1015745
   Macklin M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461984
   Mercier O, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818115
   Mihalef Viorel., 2006, S COMPUTER ANIMATION, P317
   Morris JP, 2000, INT J NUMER METH FL, V33, P333, DOI 10.1002/1097-0363(20000615)33:3<333::AID-FLD11>3.0.CO;2-7
   Muller M, 2005, P 2005 ACM SIGGRAPH, P237, DOI DOI 10.1145/1073368.1073402
   Nielsen M. B., 2013, ACM T GRAPHIC, V32
   Ott F., 2003, ARXIVPHYS0303112
   Park S. I., 2005, Computer Animation, Conference Proceedings, P261, DOI [DOI 10.1145/1073368.1073406, 10.1145/1073368.1073406]
   Pfaff T., 2010, ACM SIGGRAPH AS 2010
   Pfaff T, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185608
   Prakash M, 2015, COMPUT GRAPH-UK, V53, P118, DOI 10.1016/j.cag.2015.08.010
   Ren B, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2645703
   Schechter H., 2008, Symposium on Computer animation, P1
   Schechter H, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185557
   Selle A, 2005, ACM T GRAPHIC, V24, P910, DOI 10.1145/1073204.1073282
   Solenthaler B., 2008, P 2008 ACM SIGGRAPH, P211, DOI 10.2312/SCA/SCA08/211-218
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Taekwon Jang, 2011, International Journal of Virtual Reality, V10, P21
   Tartakovsky AM, 2005, J COMPUT PHYS, V207, P610, DOI 10.1016/j.jcp.2005.02.001
   Vines M, 2014, IEEE T VIS COMPUT GR, V20, P303, DOI 10.1109/TVCG.2013.95
   Yan X, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925897
   Yang B, 2014, COMPUT ANIMAT VIRT W, V25, P467, DOI 10.1002/cav.1585
   Yang T, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818117
NR 41
TC 3
Z9 3
U1 3
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2018
VL 34
IS 4
BP 461
EP 471
DI 10.1007/s00371-017-1353-1
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FY6BJ
UT WOS:000426924400002
DA 2024-07-18
ER

PT J
AU de Macedo, DV
   Rodrigues, MAF
AF de Macedo, Daniel Valente
   Formico Rodrigues, Maria Andreia
TI Real-time dynamic reflections for realistic rendering of 3D scenes
SO VISUAL COMPUTER
LA English
DT Article
DE Real-time dynamic reflections; Realistic rendering; 3D scenes
AB Visual effects, such as real-time dynamic reflections, are fundamental for realistic rendering of 3D scenes and walkthrough animations containing multiple moving objects, since they provide the correct identification of their relative distance and of their material properties, generating a closer perception of reality. Most rendering algorithms that generate realistic effects are quite expensive, such as ray tracing, usually used in offline rendering. This paper presents a solution for generating reflections, that we have developed with a real-time hybrid algorithm for rendering rigid objects in realistic dynamic scenes. The algorithm combines rasterization, the screen space reflection (SSR) technique, with pure GPU-ray tracing algorithm through deferred rendering pipeline, doing SSR per pixel and creating a mask with failed pixels to apply ray tracing for those pixels instead. The results demonstrate a significant improvement in performance with a very little perceptual loss in quality of our hybrid algorithm, when compared to the full ray tracing solution. In terms of FPS results, our hybrid solution remains positioned (most of the time) in between the SSR and the pure ray tracing's methods, during the walkthrough. Besides, it scales quite well for realistic dynamic scenes with 3D rigid objects.
C1 [de Macedo, Daniel Valente; Formico Rodrigues, Maria Andreia] Univ Fortaleza UNIFOR, PPGIA, Av Washington Soares 1321,Bloco J,Sala 30, Fortaleza, Ceara, Brazil.
C3 Universidade Fortaleza
RP Rodrigues, MAF (corresponding author), Univ Fortaleza UNIFOR, PPGIA, Av Washington Soares 1321,Bloco J,Sala 30, Fortaleza, Ceara, Brazil.
EM andreia.formico@gmail.com
RI Oliveira, Beatriz/KGL-8895-2024; Rodrigues, Maria/KFR-3007-2024
FU CAPES [157.257/2012-6]; CNPq [481.326/2013-8]
FX Daniel Valente de Macedo and Maria Andreia Formico Rodrigues are
   supported by CAPES and CNPq, under grants No. 157.257/2012-6 and
   481.326/2013-8, respectively, and would like to thank for their
   financial support. In addition, we are also grateful to the referees for
   providing insightful comments and suggestions to improve the manuscript.
CR Andrade P, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, THEORY AND APPLICATIONS (VISAPP 2014), VOL 3, P12
   [Anonymous], 2014, Journal of Computer Graphics Techniques (JCGT)
   [Anonymous], 2010, THESIS
   Bikker J, 2007, RT07: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2007, P1
   BLINN JF, 1976, COMMUN ACM, V19, P542, DOI 10.1145/965143.563322
   Carr NathanA., 2002, P ACM SIGGRAPHEUROGR, P37
   Crytek, 2010, SPONZ MOD
   Games E., 2015, UNREAL ENGINE, V4
   Ganestam P, 2015, VISUAL COMPUT, V31, P1395, DOI 10.1007/s00371-014-1021-7
   GREENE N, 1986, IEEE COMPUT GRAPH, V6, P21, DOI 10.1109/MCG.1986.276658
   Gregory Jason., 2009, GAME ENGINE ARCHITEC
   Johnsson M., 2012, THESIS
   Kim Y, 2016, VISUAL COMPUT, V32, P801, DOI 10.1007/s00371-016-1251-y
   Macedo D. V., 2016, REAL TIME DYNAMIC RE
   Macedo D. V., 2015, P 14 BRAZ S COMP GAM, P9
   Mara M., 2013, LIGHTING DEEP G BUFF
   Miller G, 1984, SIGGRAPH 84 COURSE N, P1
   NVidia: Optix, 2015, OPT
   Parker SG, 2013, COMMUN ACM, V56, P93, DOI [10.1145/2447976.2447997, 10.1145/2447876.2447997]
   Pohl D., 2014, WOLFENSTEIN RAY TRAC
   Popescu V, 2006, ACM T GRAPHIC, V25, P375, DOI 10.1145/1138450.1138460
   Purcell TJ, 2002, ACM T GRAPHIC, V21, P703, DOI 10.1145/566570.566640
   Stockman George, 2001, Computer Vision
   Wald I, 2009, COMPUT GRAPH FORUM, V28, P1691, DOI 10.1111/j.1467-8659.2008.01313.x
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   WHITTED T, 1980, COMMUN ACM, V23, P343, DOI 10.1145/358876.358882
   Widmer S., 2015, P 7 C HIGH PERF GRAP, P67
NR 27
TC 5
Z9 5
U1 1
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2018
VL 34
IS 3
BP 337
EP 346
DI 10.1007/s00371-016-1335-8
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FW2CZ
UT WOS:000425110900003
DA 2024-07-18
ER

PT J
AU Lee, T
   Kang, D
   Kwon, T
AF Lee, Taekhee
   Kang, Daeun
   Kwon, Taesoo
TI Motion normalization method based on an inverted pendulum model for
   clustering
SO VISUAL COMPUTER
LA English
DT Article
DE Physics-based method; Motion capture and reuse; Motion clustering
ID RETRIEVAL; WALKING
AB In many creative industries, such as the animation, movie, and game industries, artists often make good use of motion data to create their works by retrieving a particular motion from motion-capture data and reusing it. A large database of human motion is difficult to use unless the motion data are organized according to the type of motion. Although there have been many results for clustering motion capture data, many variations in the motion data complicate the clustering of data by making one type of motion numerically similar to other types of motions. To improve the motion clustering performance, we present a novel physically based motion normalization method that reduces ambiguous elements of motions, so that motions that have different semantics can be differentiated. The normalized motion data generated by our method can be used as input to existing clustering algorithms and improves the results.
C1 [Lee, Taekhee] FXGear, Seoul, South Korea.
   [Kang, Daeun; Kwon, Taesoo] Hanyang Univ, Seoul, South Korea.
C3 Hanyang University
RP Kwon, T (corresponding author), Hanyang Univ, Seoul, South Korea.
EM watersp@gmail.com; teddysiah@gmail.com; taesoo@hanyang.ac.kr
FU research fund of Hanyang University [HY-201200000000616]
FX This work was supported by the research fund of Hanyang University
   (HY-201200000000616).
CR Agrawal S., 2013, P 2013 ACM SIGGRAPH
   [Anonymous], 2009, P 2009 ACM SIGGRAPH
   [Anonymous], 2009, Proceedings of the 2009 Symposium on Interactive 3D Graphics and Games, I3D'09, DOI DOI 10.1145/1507149.1507181
   [Anonymous], 2013, Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, I3D'13
   Barbic J, 2004, PROC GRAPH INTERF, P185
   Barnachon M, 2014, PATTERN RECOGN, V47, P238, DOI 10.1016/j.patcog.2013.06.020
   Beacco A, 2015, COMPUT GRAPH-UK, V47, P105, DOI 10.1016/j.cag.2014.12.004
   Beaudoin P., 2008, P 2008 ACM SIGGRAPHE, P117
   Choi MG, 2012, COMPUT GRAPH FORUM, V31, P2057, DOI 10.1111/j.1467-8659.2012.03198.x
   Chung SK, 1999, COMP ANIM CONF PROC, P4, DOI 10.1109/CA.1999.781194
   Coros S., ACM T GRAPH, V27
   Feng YF, 2015, IEEE T CYBERNETICS, V45, P2693, DOI 10.1109/TCYB.2014.2381659
   Gao Y, 2005, LECT NOTES COMPUT SC, V3784, P95
   GIRARD M, 1987, IEEE COMPUT GRAPH, V7, P39, DOI 10.1109/MCG.1987.276895
   Heck R, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P129
   Hodgins J. K., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P71, DOI 10.1145/218380.218414
   Hou JH, 2015, IEEE T VIS COMPUT GR, V21, P848, DOI 10.1109/TVCG.2015.2403328
   Jenkins O.C., 2003, AUTON AGENT MULTI-AG, P225
   Kajita S, 2003, IEEE INT CONF ROBOT, P1620, DOI 10.1109/robot.2003.1241826
   Kovar L, 2004, ACM T GRAPHIC, V23, P559, DOI 10.1145/1015706.1015760
   Kovar L., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P214
   Kron Taesoo., 2005, EUROGRAPHICSACM SIGG, P29, DOI DOI 10.1145/1073368.1073373
   Kwon T, 2008, IEEE T VIS COMPUT GR, V14, P707, DOI 10.1109/TVCG.2008.22
   Kwon Taesoo, 2010, P 2010 ACM SIGGRAPHE, P129
   Lasa de, 2010, ACM Trans. Graph., V29, P1
   Lee T, 2015, VISUAL COMPUT, V31, P873, DOI 10.1007/s00371-015-1103-1
   Liu CK, 2002, ACM T GRAPHIC, V21, P408, DOI 10.1145/566570.566596
   López-Méndez A, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.49
   Muller Meinard., 2006, P ACM SIGGRAPHEUROGR, P137
   Min JY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366172
   Min JY, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640452
   Mukai T, 2005, ACM T GRAPHIC, V24, P1062, DOI 10.1145/1073204.1073313
   Müller M, 2005, ACM T GRAPHIC, V24, P677, DOI 10.1145/1073204.1073247
   Park SI., 2002, Proceedings of the 2002 ACM SIGGRAPH/Eurographics Symposium on Computer Animation. SCA2, P105, DOI DOI 10.1145/545261.545279
   Rose C, 1998, IEEE COMPUT GRAPH, V18, P32, DOI 10.1109/38.708559
   Shin H.J., 2006, Proceedings of ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P291
   Shin HJ, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P194
   Singh S, 2011, COMPUT ANIMAT VIRT W, V22, P151, DOI 10.1002/cav.403
   Sugihara T, 2009, IEEE INT CONF ROBOT, P669
   van Basten B.J. H., 2011, Graphics Interface, P9
   vandePanne M, 1997, COMPUT GRAPH FORUM, V16, P211, DOI 10.1111/1467-8659.00181
   Wang Y., 2007, P IEEE INT C COMPUTE, P1, DOI [10.1109/CVPR.2007.383505, DOI 10.1109/CVPR.2007.383505, DOI 10.1016/J.EJ0R.2007.01.050]
   Wang Z, 2016, SIGNAL PROCESS, V120, P691, DOI 10.1016/j.sigpro.2014.11.015
   Wu Chun-Chih, 2010, P 2010 ACM SIGGRAPH, P113
   Zhou F, 2013, IEEE T PATTERN ANAL, V35, P582, DOI 10.1109/TPAMI.2012.137
NR 45
TC 0
Z9 0
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2018
VL 34
IS 1
BP 29
EP 40
DI 10.1007/s00371-016-1308-y
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FR5XF
UT WOS:000419139200005
DA 2024-07-18
ER

PT J
AU Wang, YY
   Liu, RS
   Song, XL
   Su, ZX
AF Wang, Yiyang
   Liu, Risheng
   Song, Xiaoliang
   Su, Zhixun
TI A nonlocal model with regression predictor for saliency detection and
   extension
SO VISUAL COMPUTER
LA English
DT Article
DE Nonlocal L-0; Regression predictor; Saliency detection; Interactive
   segmentation
ID IMAGE
AB Estimating salient object regions automatically has enhanced many computer vision applications in recent years. By observing the intrinsic sparsity of saliency map, we propose a graph-based nonlocal (NL) minimization framework to extract its sparse geometric structure. Our experimental results demonstrate that our method with artificially designed control map yields a significant improvement compared with the state-of-the-art saliency detection methods on four publicly available data sets. These saliency maps are further used for content-aware image resizing and unsupervised matting to test their uniformity. Moreover, we propose to learn the control map adaptively from training data. This strategy totally differs from the previously designed one, which is verified to be effective on image-classified data set. NL with data-driven strategy is extended to interactive segmentation task and is affirmed to be better-performed than other advanced interactive approaches.
C1 [Wang, Yiyang; Song, Xiaoliang; Su, Zhixun] Dalian Univ Technol, Sch Math Sci, Dalian 116024, Peoples R China.
   [Liu, Risheng] Dalian Univ Technol, Sch Software Technol, Dalian 116024, Peoples R China.
   [Liu, Risheng] Dalian Univ Technol, Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian 116024, Peoples R China.
C3 Dalian University of Technology; Dalian University of Technology; Dalian
   University of Technology
RP Liu, RS (corresponding author), Dalian Univ Technol, Sch Software Technol, Dalian 116024, Peoples R China.; Liu, RS (corresponding author), Dalian Univ Technol, Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian 116024, Peoples R China.
EM yywerica@gmail.com; rsliu@dlut.edu.cn; ericsong507@gmail.com;
   zxsu@dlut.edu.cn
OI SONG, XIAOLIANG/0000-0002-4965-6874; Wang, Yiyang/0000-0002-8924-3468
FU National Natural Science Foundation of China [61300086, 61432003,
   61173103, 61572099, 61320106008, 91230103]; Fundamental Research Funds
   for the Central Universities [DUT15QY15]; Hong Kong Scholar Program
   [XJ2015008]; National Science and Technology Major Project
   [2013ZX04005021, 2014ZX04001011]
FX Risheng Liu was supported by the National Natural Science Foundation of
   China (Nos. 61300086, 61432003), the Fundamental Research Funds for the
   Central Universities (DUT15QY15), and the Hong Kong Scholar Program (No.
   XJ2015008). Zhixun Su was supported by the National Natural Science
   Foundation of China (Nos. 61173103, 61572099, 61320106008, 91230103) and
   National Science and Technology Major Project (Nos. 2013ZX04005021,
   2014ZX04001011).
CR Achanta R, 2008, LECT NOTES COMPUT SC, V5008, P66
   Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], ICIP
   [Anonymous], ACML
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Bao C., 2014, CVPR
   Borji Ali, 2019, [Computational Visual Media, 计算可视媒体], V5, P117
   Bougleux S, 2007, LECT NOTES COMPUT SC, V4485, P128
   Boyd S., 2011, FOUND TRENDS MACH LE, V3, P1, DOI DOI 10.1561/2200000016
   Bresson X., 2009, SHORT NOTE NONLOCAL
   Cao XC, 2014, IEEE T IMAGE PROCESS, V23, P4175, DOI 10.1109/TIP.2014.2332399
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chang KY, 2011, IEEE I CONF COMP VIS, P914, DOI 10.1109/ICCV.2011.6126333
   Chen QF, 2012, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2012.6247760
   Chen TC, 2009, PROC EUR SOLID-STATE, P1
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Chia AYS, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024190
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Einhäuser W, 2003, EUR J NEUROSCI, V17, P1089, DOI 10.1046/j.1460-9568.2003.02508.x
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Ge DD, 2011, MATH PROGRAM, V129, P285, DOI 10.1007/s10107-011-0470-2
   Gilboa G, 2008, MULTISCALE MODEL SIM, V7, P1005, DOI 10.1137/070698592
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   Hou X., 2007, IEEE C COMP VIS PATT, V1, P1, DOI DOI 10.1109/CVPR.2007.383267
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jia Y., 2013, ICCV
   Jiang BW, 2013, IEEE I CONF COMP VIS, P1665, DOI 10.1109/ICCV.2013.209
   Jiang HZ, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.110
   Jiang P, 2013, IEEE I CONF COMP VIS, P1976, DOI 10.1109/ICCV.2013.248
   Jiang ZL, 2013, PROC CVPR IEEE, P2043, DOI 10.1109/CVPR.2013.266
   Kanan C., 2010, CVPR
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Lin Z., 2011, ADV NEURAL INFORM PR, V24, P612, DOI DOI 10.1007/S11263-013-0611-6
   Liu Risheng, 2014, P IEEE C COMP VIS PA
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Ma Y.F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   Mai Long., 2013, CVPR
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Nesterov Yurii, 1994, SIAM Studies in Applied Mathematics, DOI [DOI 10.1137/1.9781611970791, 10.1137/1.9781611970791]
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Ochs P, 2014, IEEE T PATTERN ANAL, V36, P1187, DOI 10.1109/TPAMI.2013.242
   Pan Jin-shan, 2014, CVPR
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Scharfenberger C, 2013, PROC CVPR IEEE, P979, DOI 10.1109/CVPR.2013.131
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Stalder S., 2013, ACCV
   Tsai D, 2012, INT J COMPUT VISION, V100, P190, DOI 10.1007/s11263-011-0512-5
   Wang Y., 2014, ACCV
   Wang Y.-X., 2016, AAAI
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Xie YL, 2011, IEEE IMAGE PROC, P645, DOI 10.1109/ICIP.2011.6116634
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang C, 2013, IEEE SIGNAL PROC LET, V20, P637, DOI 10.1109/LSP.2013.2260737
   Yuan G., 2015, CVPR
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zheng YJ, 2009, IEEE I CONF COMP VIS, P889, DOI 10.1109/ICCV.2009.5459326
NR 65
TC 1
Z9 1
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2017
VL 33
IS 11
BP 1467
EP 1482
DI 10.1007/s00371-016-1292-2
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FJ0TP
UT WOS:000412423100009
DA 2024-07-18
ER

PT J
AU Cornel, D
   Tobler, RF
   Sakai, H
   Luksch, C
   Wimmer, M
AF Cornel, Daniel
   Tobler, Robert F.
   Sakai, Hiroyuki
   Luksch, Christian
   Wimmer, Michael
TI Forced Random Sampling: fast generation of importance-guided blue-noise
   samples
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th Conference on Computer Graphics International (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Blue noise; Importance sampling; Ordered dithering
ID WANG TILES
AB In computer graphics, stochastic sampling is frequently used to efficiently approximate complex functions and integrals. The error of approximation can be reduced by distributing samples according to an importance function, but cannot be eliminated completely. To avoid visible artifacts, sample distributions are sought to be random, but spatially uniform, which is called blue-noise sampling. The generation of unbiased, importance-guided blue-noise samples is expensive and not feasible for real-time applications. Sampling algorithms for these applications focus on runtime performance at the cost of having weak blue-noise properties. Blue-noise distributions have also been proposed for digital halftoning in the form of precomputed dither matrices. Ordered dithering with such matrices allows to distribute dots with blue-noise properties according to a grayscale image. By the nature of ordered dithering, this process can be parallelized easily. We introduce a novel sampling method called forced random sampling that is based on forced random dithering, a variant of ordered dithering with blue noise. By shifting the main computational effort into the generation of a precomputed dither matrix, our sampling method runs efficiently on GPUs and allows real-time importance sampling with blue noise for a finite number of samples. We demonstrate the quality of our method in two different rendering applications.
C1 [Cornel, Daniel; Tobler, Robert F.] VRVis Res Ctr, Vienna, Austria.
   [Luksch, Christian] VRVis Res Ctr, Semant Modeling & Acquisit Grp, Vienna, Austria.
   [Sakai, Hiroyuki] TU Wien, Inst Comp Graph & Algorithms, Rendering & Modeling Grp, Vienna, Austria.
   [Wimmer, Michael] TU Wien, Inst Comp Graph & Algorithms, Vienna, Austria.
C3 Technische Universitat Wien; Technische Universitat Wien
RP Cornel, D (corresponding author), VRVis Res Ctr, Vienna, Austria.
EM cornel@vrvis.at
OI Wimmer, Michael/0000-0002-9370-2663; Cornel, Daniel/0000-0002-2481-6720
FU BMVIT; BMWFW; City of Vienna (ZIT) within the scope of COMET Competence
   Centers for Excellent Technologies; Austrian Science Fund (FWF) [P
   27974]
FX The competence center VRVis is funded by BMVIT, BMWFW and City of Vienna
   (ZIT) within the scope of COMET Competence Centers for Excellent
   Technologies. The program COMET is managed by FFG. Hiroyuki Sakai is
   partly supported by the Austrian Science Fund (FWF), project no. P
   27974. We thank Christoph Weinzierl-Heigl for providing us access to his
   implementation of reflective shadow mapping.
CR Abe Y., 2001, ISCAS 2001. The 2001 IEEE International Symposium on Circuits and Systems (Cat. No.01CH37196), P517, DOI 10.1109/ISCAS.2001.921121
   Ahmed AGM, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980218
   [Anonymous], SIGGRAPH ASIA SKETCH
   Balzer M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531392
   Bayer B. E, 1973, IEEE INT C COMM
   Bowers J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866188
   Clarberg P, 2005, ACM T GRAPHIC, V24, P1166, DOI 10.1145/1073204.1073328
   Clarberg P, 2008, COMPUT GRAPH FORUM, V27, P681, DOI 10.1111/j.1467-8659.2008.01166.x
   Cline D., 2006, EUROGRAPHICS WORKSHO, P103
   COOK RL, 1986, ACM T GRAPHIC, V5, P51, DOI 10.1145/7529.8927
   de Goes F, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366190
   Georgiev I., 2016, ACM SIGGRAPH 2016 TA
   Gjol M., 2016, GAM DEV C EUR 2016
   Hiller S., 2001, Vision, Modeling, and Visualization 2001. Proceedings, P265
   Huang HD, 2007, VRST 2007: ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, PROCEEDINGS, P149
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Kopf J, 2006, ACM T GRAPHIC, V25, P509, DOI 10.1145/1141911.1141916
   Lagae A, 2005, ACM T GRAPHIC, V24, P1442, DOI 10.1145/1095878.1095888
   Lagae A, 2008, COMPUT GRAPH FORUM, V27, P114, DOI 10.1111/j.1467-8659.2007.01100.x
   Lagae A, 2006, ACM T GRAPHIC, V25, P1442, DOI 10.1145/1183287.1183296
   McCool M., 1992, Proceedings. Graphics Interface '92, P94
   MITSA T, 1992, J OPT SOC AM A, V9, P1920, DOI 10.1364/JOSAA.9.001920
   Newbern J, 1997, P SOC PHOTO-OPT INS, V3016, P441, DOI 10.1117/12.274542
   Ostromoukhov V, 2004, ACM T GRAPHIC, V23, P488, DOI 10.1145/1015706.1015750
   Ostromoukhov V, 2007, ACM T GRAPHIC, V26, P1
   PURGATHOFER W, 1994, IEEE IMAGE PROC, P1032, DOI 10.1109/ICIP.1994.413512
   Schmaltz C, 2010, COMPUT GRAPH FORUM, V29, P2313, DOI 10.1111/j.1467-8659.2010.01716.x
   Shade J., 2002, 021207 UI WASH DEP C
   ULICHNEY R, 1993, P SOC PHOTO-OPT INS, V1913, P332, DOI 10.1117/12.152707
   Ulichney R., 1987, DIGITAL HALFTONING
   ULICHNEY RA, 1988, P IEEE, V76, P56, DOI 10.1109/5.3288
   Wachtel F, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601107
   Wei L. Y., 2011, PUBLIC SVN REPOSITOR
   Wei LY, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360619
   Wei XL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966398
   Weinzierl-Heigl C., 2013, P 17 CENTR EUR SEM C
   YELLOTT JI, 1983, SCIENCE, V221, P382, DOI 10.1126/science.6867716
   Yuksel C, 2015, COMPUT GRAPH FORUM, V34, P25, DOI 10.1111/cgf.12538
NR 38
TC 0
Z9 0
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 833
EP 843
DI 10.1007/s00371-017-1392-7
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800015
DA 2024-07-18
ER

PT J
AU Hu, SJ
   Zhang, ZY
   Xie, HR
   Igarashi, T
AF Hu, Shaojun
   Zhang, Zhiyi
   Xie, Haoran
   Igarashi, Takeo
TI Data-driven modeling and animation of outdoor trees through interactive
   approach
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th Conference on Computer Graphics International (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Tree; Modeling; Animation; Data-driven; Interactive; Video
ID REAL-TIME ANIMATION; WIND
AB Computer animation of trees has widespread applications in the fields of film production, video games and virtual reality. Physics-based methods are feasible solutions to achieve good approximations of tree movements. However, realistically animating a specific tree in the real world remains a challenge since physics-based methods rely on dynamic properties that are difficult to measure. In this paper, we present a low-cost interactive approach to model and animate outdoor trees from photographs and videos, which can be captured using a smartphone or handheld camera. An interactive editing approach is proposed to reconstruct detailed branches from photographs by considering an epipolar constraint. To track the motions of branches and leaves, a semi-automatic tracking method is presented to allow the user to interactively correct mis-tracked features. Then, the physical parameters of branches and leaves are estimated using a fast Fourier transform, and these properties are applied to a simplified physics-based model to generate animations of trees with various external forces. We compare the animation results with reference videos on several examples and demonstrate that our approach can achieve realistic tree animation.
C1 [Hu, Shaojun; Zhang, Zhiyi] Northwest A&F Univ, Coll Informat Engn, Xianyang, Peoples R China.
   [Xie, Haoran; Igarashi, Takeo] Univ Tokyo, Grad Sch Informat Sci & Technol, Tokyo, Japan.
C3 Northwest A&F University - China; University of Tokyo
RP Hu, SJ (corresponding author), Northwest A&F Univ, Coll Informat Engn, Xianyang, Peoples R China.
EM hsj@nwsuaf.edu.cn; zhangzhiyi@nwafu.edu.cn; xiehr@acm.org; takeo@acm.org
RI Igarashi, Takeo/ITT-5921-2023; Li, Fan/JRY-4017-2023; zhang,
   ZY/HJH-6535-2023; Li, Nan/IXD-8260-2023; Xie, Haoran/V-6397-2019; Xie,
   Haoran/ADP-8087-2022
OI Xie, Haoran/0000-0002-6926-3082; Xie, Haoran/0000-0002-6926-3082; HU,
   Shaojun/0000-0002-4686-7633
FU NSFC [61303124]; National 863 Plan [2013AA102304 02]; NSBR Plan of
   Shaanxi [2015JQ6250]; Grants-in-Aid for Scientific Research [17H06574]
   Funding Source: KAKEN
FX We thank Hironori Yoshida, Seung-tak Noh and the anonymous reviewers.
   This work was supported by NSFC [61303124], National 863 Plan
   [2013AA102304 02] and NSBR Plan of Shaanxi [2015JQ6250].
CR Akagi Y, 2006, COMPUT GRAPH-UK, V30, P529, DOI 10.1016/j.cag.2006.03.017
   Bouguet J-Y, 1999, Pyramidal implementation of the Lucas Kanade feature tracker
   Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191
   Deussen O, 2005, Digital Design of Nature: Computer Generated Plants and Organics
   Diener J, 2009, COMPUT GRAPH FORUM, V28, P533, DOI 10.1111/j.1467-8659.2009.01393.x
   Diener Julien., 2006, Proceedings of the Eurographics Symposium on Computer Animation, P187
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Habel R, 2009, COMPUT GRAPH FORUM, V28, P523, DOI 10.1111/j.1467-8659.2009.01391.x
   Hu SJ, 2012, VISUAL COMPUT, V28, P859, DOI 10.1007/s00371-012-0694-z
   James KR, 2006, AM J BOT, V93, P1522, DOI 10.3732/ajb.93.10.1522
   Li C, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024161
   Livny Y, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964948
   Long J, 2015, VISUAL COMPUT, V31, P325, DOI 10.1007/s00371-014-0927-4
   Olmos BA, 2010, EARTHQ ENG STRUCT D, V39, P1671, DOI 10.1002/eqe.1010
   Ota S, 2004, VISUAL COMPUT, V20, P613, DOI 10.1007/s00371-004-0266-y
   Pirk S., 2014, ACM T GRAPHIC, V33, P1
   Pirk S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366188
   Py C, 2006, J FLUID MECH, V568, P425, DOI 10.1017/S0022112006002667
   Quan L, 2006, ACM T GRAPHIC, V25, P599, DOI 10.1145/1141911.1141929
   Sakaguchi T., 1999, VRST'99. Proceedings of the ACM Symposium on Virtual Reality Software and Technology, P139, DOI 10.1145/323663.323685
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Shinya M., 1992, Computer Graphics Forum, V11, pC119, DOI 10.1111/1467-8659.1130119
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Stam J, 1997, COMPUT GRAPH FORUM, V16, pC159, DOI 10.1111/1467-8659.00152
   Sun M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P96
   Tan P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239538
   Wang B., 2015, ACM T GRAPHIC, V34, P1
NR 27
TC 7
Z9 9
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 1017
EP 1027
DI 10.1007/s00371-017-1377-6
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800031
DA 2024-07-18
ER

PT J
AU Liu, CX
   Shao, H
   Wu, M
   Zhou, YG
   Shao, YQ
   Wang, X
AF Liu, Chunxiao
   Shao, Huan
   Wu, Min
   Zhou, Yanggang
   Shao, Yaqi
   Wang, Xun
TI Multi-scale inherent variation features-based texture filtering
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th International Conference on Computer Graphics (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Texture smoothing; Structure detection; Multi-scale analysis; Inherent
   variation; Guided filtering
AB In order to smooth the multi-scale texture with strong gradient while maintaining the weak structure which is challenging for the existing texture filtering methods, we put forward a novel algorithm based on multi-scale inherent variation features. Based on statistical analysis of various kinds of structure/texture pixels, six-dimensional discriminating features are found and first extracted from the multi-scale inherent variation curve, which demonstrate superiority in recognizing the structure pixels. Then, a preliminary structure prediction map can be obtained with a structure/texture classification model, which is generated by the cross-validation-based SVM training process. Next, we design a post-processing-based fine structure detection scheme to deal with the defects in the structure prediction map with three main steps successively, i.e., removing the mistaken texture pixels with thinning and outlier rejection, retrieving the missed structure pixels with breakpoints connection, and repositioning the structure pixels with structure correction. Finally, we propose a structure guided adaptive image smoothing method to smooth texture while preserving structure without halo effect. Experimental results show that our algorithm works better than the state-of-the-art methods in the preservation of weak structure as well as the suppression of texture with strong gradient or varying scales.
C1 [Liu, Chunxiao; Shao, Huan; Wu, Min; Zhou, Yanggang; Shao, Yaqi; Wang, Xun] Zhejiang Gongshang Univ, Sch Comp Sci & Informat Engn, Hangzhou 310018, Zhejiang, Peoples R China.
C3 Zhejiang Gongshang University
RP Liu, CX (corresponding author), Zhejiang Gongshang Univ, Sch Comp Sci & Informat Engn, Hangzhou 310018, Zhejiang, Peoples R China.
EM cxliu@mail.zjgsu.edu.cn
RI Liu, Chunxiao/D-7975-2016
OI Liu, Chunxiao/0000-0002-0645-8771
FU Zhejiang Provincial Natural Science Foundation of China [LY14F020004];
   National Natural Science Foundation of China [61003188, 61379075,
   U1609215]; Talent Young Foundation of Zhejiang Gongshang University
   [QZ13-9]; National Key Technology RD Program [2014BAK14B01]; Zhejiang
   Provincial Commonweal Technology Applied Research Projects of China
   [2015C33071]; Zhejiang Provincial Research Center of Intelligent
   Transportation Engineering and Technology [2015ERCITZJ-KF1]
FX This work is supported by the Zhejiang Provincial Natural Science
   Foundation of China under Grant No. LY14F020004, the National Natural
   Science Foundation of China under Grant Nos. 61003188, 61379075 and
   U1609215, the Talent Young Foundation of Zhejiang Gongshang University
   under Grant No. QZ13-9, the National Key Technology R&D Program under
   Grant No. 2014BAK14B01, the Zhejiang Provincial Commonweal Technology
   Applied Research Projects of China under Grant No. 2015C33071 and the
   Zhejiang Provincial Research Center of Intelligent Transportation
   Engineering and Technology under Grant No. 2015ERCITZJ-KF1.
CR Baek J., 2010, ACM T GRAPHIC, V29, P81
   Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Cho H, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601188
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Dollár P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   Ham B, 2015, PROC CVPR IEEE, P4823, DOI 10.1109/CVPR.2015.7299115
   Hua M, 2014, PROC CVPR IEEE, pCP1, DOI 10.1109/CVPR.2014.363
   Karacan L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508403
   Li XY, 2013, IEEE T IMAGE PROCESS, V22, P1915, DOI 10.1109/TIP.2013.2237922
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Subr K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618493
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wang Sha, 2014, Infrared and Laser Engineering, V43, P2000
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xu L, 2015, PR MACH LEARN RES, V37, P1669
   Xu L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366158
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Yang QX, 2010, PROC CVPR IEEE, P1775, DOI 10.1109/CVPR.2010.5539847
   [余丽红 YU Li-hong], 2009, [中国图象图形学报, Journal of Image and Graphics], V14, P1950
   Zhang Q, 2014, LECT NOTES COMPUT SC, V8691, P815, DOI 10.1007/978-3-319-10578-9_53
NR 24
TC 3
Z9 3
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 769
EP 778
DI 10.1007/s00371-017-1380-y
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800009
DA 2024-07-18
ER

PT J
AU Liu, L
   Liu, XH
   Sheng, B
   Chen, YY
   Wu, EH
AF Liu, Le
   Liu, Xuehui
   Sheng, Bin
   Chen, Yanyun
   Wu, Enhua
TI Incremental collision-free feathering for animated surfaces
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th Conference on Computer Graphics International (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Feathering; Feather coat; Collision detection; Collision resolution;
   Animation
AB We present a system for efficiently dressing animated 3D models with feathers. While there have been several works on generating static feathers, few methods can incrementally adjust the feathers when the model is animated. Our system makes several important improvements to achieve the goal. We first propose a simple yet effective algorithm to sample the roots based on the orientation field, as the distribution of feather roots has a great impact on the final look. To enable reordering the roots during animation, we analyze and improve the definition of the growth priority. Finally, we propose a method to incrementally adjust minimal feathers to collision-free for each frame of animation. Our system is easy to implement, and the results show that it is efficient and robust.
C1 [Liu, Le; Liu, Xuehui; Chen, Yanyun; Wu, Enhua] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing, Peoples R China.
   [Liu, Le; Liu, Xuehui; Chen, Yanyun; Wu, Enhua] Univ Chinese Acad Sci, Beijing, Peoples R China.
   [Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Wu, Enhua] Univ Macau, Zhuhai, Peoples R China.
   [Wu, Enhua] UM Zhuhai Res Inst, Zhuhai, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS; Shanghai
   Jiao Tong University; University of Macau
RP Liu, L (corresponding author), Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing, Peoples R China.; Liu, L (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.
EM liul@ios.ac.cn; ehwu@umac.mo
FU NSFC [61632003, 61672502, 61572316]; Macao FDCT [068/2015/A2,
   136/2014/A3]; Univ. of Macau [MYRG2014-00139-FST]
FX We would like to thank the anonymous reviewers for their insightful
   comments and suggestions to improve the quality of the paper. This
   research is supported by NSFC (61632003, 61672502, 61572316), Macao FDCT
   Grants (068/2015/A2, 136/2014/A3) and Univ. of Macau Grant
   (MYRG2014-00139-FST).
CR ALPERN B, 1990, PROCEEDINGS OF THE FIRST ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P32
   Bangay S, 2007, AFRIGRAPH 2007: 5TH INTERNATIONAL CONFERENCE ON VIRTUAL REALITY, COMPUTER GRAPHICS, VISUALIZATION AND INTERACTION IN AFRICA, P169
   Bowers J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866188
   Chen YY, 2002, ACM T GRAPHIC, V21, P630, DOI 10.1145/566570.566628
   Franco CG, 2001, XIV BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, PROCEEDINGS, P381, DOI 10.1109/SIBGRAPI.2001.963087
   Hadap S, 2001, COMPUT GRAPH FORUM, V20, pC329, DOI 10.1111/1467-8659.00525
   Jacobson A., 2014, LIBIGL SIMPLE C GEOM
   Kaufman D., 2002, ACM SIGGRAPH 2002 CO
   Knöppel F, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2767000
   Liu L, 2015, COMPUT GRAPH FORUM, V34, P279, DOI 10.1111/cgf.12766
   MarchettiSpaccamela A, 1996, INFORM PROCESS LETT, V59, P53, DOI 10.1016/0020-0190(96)00075-0
   Pearce D.J., 2007, J EXP ALGORITHMICS, V11, P1
   Peyrot J.L., 2013, FEATURE PRESERVING D, P9
   Streit L, 2002, COMPUT GRAPH FORUM, V21, P565, DOI 10.1111/1467-8659.t01-1-00707
   Surazhsky V, 2005, ACM T GRAPHIC, V24, P553, DOI 10.1145/1073204.1073228
   Weber AJ, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1516522.1516523
   Xin SQ, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1559755.1559761
   Xu Y, 2012, COMPUT GRAPH-UK, V36, P232, DOI 10.1016/j.cag.2012.02.005
NR 18
TC 0
Z9 0
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 883
EP 890
DI 10.1007/s00371-017-1369-6
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800019
DA 2024-07-18
ER

PT J
AU Yuan, HL
   Zheng, CW
AF Yuan, Hongliang
   Zheng, Changwen
TI Adaptive rendering based on a weighted mixed-order estimator
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th Conference on Computer Graphics International (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Monte Carlo ray tracing; Adaptive rendering; Nadaraya-Watson estimator;
   Robust local linear estimator
ID REGRESSION
AB In this paper, we propose a novel adaptive rendering method to robustly handle noise artifacts and outliers of Monte Carlo ray tracing by combining the Nadaraya-Watson and robust local linear estimators while efficiently preserving fine details. Our method first constructs a sparse robust local linear estimator in feature space (normal,texture,etc.), while also removing spike noise. Then, we utilize the Nadaraya-Watson estimator to filter the outlier-free image. We generate the final image by interpolating the values of each estimator at each pixel with weights that are inversely proportional to the estimated mean squared errors. Lastly, we distribute additional samples to the regions with higher estimated mean squared errors if sampling budget remains. We have demonstrated that our estimator outperforms previous methods visually and numerically.
C1 [Yuan, Hongliang; Zheng, Changwen] Chinese Acad Sci, Inst Software, Sci & Technol Integrated Informat Syst Lab, Beijing, Peoples R China.
   [Yuan, Hongliang] Univ Chinese Acad Sci, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS
RP Yuan, HL (corresponding author), Chinese Acad Sci, Inst Software, Sci & Technol Integrated Informat Syst Lab, Beijing, Peoples R China.; Yuan, HL (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.
EM 11488336@qq.com; changwen@iscas.ac.cn
OI yuan, hong liang/0000-0002-0255-0255
CR [Anonymous], 2016, PHYS BASED RENDERING
   [Anonymous], BIOGEOSCIENCES
   Bitterli B, 2016, COMPUT GRAPH FORUM, V35, P107, DOI 10.1111/cgf.12954
   Chang JHR, 2015, PROC CVPR IEEE, P10, DOI 10.1109/CVPR.2015.7298595
   CLEVELAND WS, 1979, J AM STAT ASSOC, V74, P829, DOI 10.2307/2286407
   DeCoro C, 2010, COMPUT GRAPH FORUM, V29, P2119, DOI 10.1111/j.1467-8659.2010.01799.x
   Delbracio M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2532708
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Kalantari NK, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766977
   Liu Y., 2017, VIS COMPUT
   Moon B, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925936
   Moon B, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766992
   Moon B, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2641762
   Nadaraya E.A., 1964, Theory of Probability and Its Applications, V61, P405, DOI [10.1137/1109020, DOI 10.1137/1109020]
   Rousselle F, 2013, COMPUT GRAPH FORUM, V32, P121, DOI 10.1111/cgf.12219
   Rousselle F, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366214
NR 16
TC 0
Z9 0
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 695
EP 704
DI 10.1007/s00371-017-1381-x
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800002
DA 2024-07-18
ER

PT J
AU Kapoor, A
   Biswas, KK
   Hanmandlu, M
AF Kapoor, Aditi
   Biswas, K. K.
   Hanmandlu, M.
TI An evolutionary learning based fuzzy theoretic approach for salient
   object detection
SO VISUAL COMPUTER
LA English
DT Article
DE Salient; Fuzzy; Genetic algorithm; Color features
ID VISUAL SALIENCY; ATTENTION
AB Human attention tends to get focused on the most prominent components of a scene which are in sharp contrast with the background. These are termed as salient regions. The human brain perceives an object to be salient based on various features like the relative intensity, spread of the region, color contrast with the background, size and position within an image. Since these features vary widely, no crisp thresholds can be specified for an automatic salient region detector. In this paper we present a rule based system which uses a set of fuzzy features to mark out the salient region in an image. A genetic algorithm based evolutionary system is used to learn the rules from the training images. Extensive comparisons with the state-of-the-art methods in terms of precision, recall and F-measure are made on two different publicly available datasets to prove the effectiveness of this approach. The application of the proposed salient object detection approach is shown in non-photorealistic rendering, perception based image compression and context aware retargeting applications with promising results.
C1 [Kapoor, Aditi] Indian Inst Technol Delhi, Sch Informat Technol, New Delhi 110016, India.
   [Biswas, K. K.] Indian Inst Technol Delhi, Dept Comp Sci & Engn, New Delhi 110016, India.
   [Hanmandlu, M.] Indian Inst Technol Delhi, Dept Elect Engn, New Delhi 110016, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Delhi; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) - Delhi; Indian Institute
   of Technology System (IIT System); Indian Institute of Technology (IIT)
   - Delhi
RP Kapoor, A (corresponding author), Indian Inst Technol Delhi, Sch Informat Technol, New Delhi 110016, India.
EM aditikapoor9585@gmail.com; kkb@cse.iitd.ernet.in; mhmandlu@ee.iitd.ac.in
OI Kapoor, Aditi/0000-0001-8215-9549
CR Achanta R, 2008, LECT NOTES COMPUT SC, V5008, P66
   Achanta R, 2010, IEEE IMAGE PROC, P2653, DOI 10.1109/ICIP.2010.5652636
   Achanta R, 2009, IEEE IMAGE PROC, P1005, DOI 10.1109/ICIP.2009.5413815
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], P 7 IND C COMP VIS G
   [Anonymous], 2013, ICCV
   [Anonymous], 2013, CVPR
   [Anonymous], 2010, CVPR
   [Anonymous], 1998, IEEE TPAMI
   [Anonymous], 2007, PROC IEEE C COMPUT V, DOI 10.1109/CVPR.2007.383267
   [Anonymous], 1994, Mathematica Journal, DOI DOI 10.1016/0165-1684(90
   [Anonymous], 2013, CVPR
   [Anonymous], 2013, CVPR
   Aytekin Ç, 2014, INT C PATT RECOG, P112, DOI 10.1109/ICPR.2014.29
   Borji Ali., 2015, CoRR
   Bruce N., 2006, P ADV NEUR INF PROC, P155
   Bruce NDB, 2005, NEUROCOMPUTING, V65, P125, DOI 10.1016/j.neucom.2004.10.065
   Bruce NDB, 2009, J VISION, V9, DOI 10.1167/9.3.5
   Chang KY, 2011, IEEE I CONF COMP VIS, P914, DOI 10.1109/ICCV.2011.6126333
   Chen LQ, 2003, MULTIMEDIA SYST, V9, P353, DOI 10.1007/s00530-003-0105-4
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193
   Cheng MM, 2014, VISUAL COMPUT, V30, P443, DOI 10.1007/s00371-013-0867-4
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   DeCarlo D, 2002, ACM T GRAPHIC, V21, P769, DOI 10.1145/566570.566650
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Duan LJ, 2011, PROC CVPR IEEE, P473, DOI 10.1109/CVPR.2011.5995676
   Erdem E, 2013, J VISION, V13, DOI 10.1167/13.4.11
   Feng J, 2011, IEEE I CONF COMP VIS, P1028, DOI 10.1109/ICCV.2011.6126348
   Fu K., 2012, 2012 IEEE 11 AS C CO
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Goferman S, 2010, PROC CVPR IEEE, P2376, DOI 10.1109/CVPR.2010.5539929
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Heckbert P., 1982, COMPUT GRAPH, V16, P207
   Hou XD, 2012, IEEE T PATTERN ANAL, V34, P194, DOI 10.1109/TPAMI.2011.146
   Jang J.S.R., 1997, NEUROFUZZY SOFT COMP, P37
   Jiang H., 2011, BMVC, V3
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Kanan C, 2010, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2010.5539947
   Kapoor A., 2012, ICVGIP
   Li N., 2014, CVPR
   Li X., 2013, ICCV
   Liu Tie, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383047
   Ma M., 2003, P 11 ACM INT C MULT
   MacEvoy B, 2005, CIELAB A B PLANE
   Marchesotti L, 2009, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2009.5459467
   Margolin R, 2013, VISUAL COMPUT, V29, P381, DOI 10.1007/s00371-012-0740-x
   Mei Rong, 2006, 2006 International Symposium on Intelligent Signal Processing and Communications (IEEE Cat. No.06EX1444), P817
   Murray N, 2011, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2011.5995506
   Nilsson M., 2007, ICASSP, P1222
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Rahtu E., 2010, ECCV
   Seo HJ, 2009, J VISION, V9, DOI 10.1167/9.12.15
   Tamara L., 2009, INT VIS WORKSH CVPR
   Tavakoli HR, 2011, LECT NOTES COMPUT SC, V6688, P666, DOI 10.1007/978-3-642-21227-7_62
   Wandell B. A, 1995, Foundations of vision
   Wei L.S., 2009, P SOC PHOTO-OPT INS, V7495
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Xie YL, 2013, IEEE T IMAGE PROCESS, V22, P1689, DOI 10.1109/TIP.2012.2216276
   Xu M., 2014, VIS COMPUT
   Yan Q., 2013, IEEE C COMP VIS PATT
   Yang C, 2013, IEEE SIGNAL PROC LET, V20, P637, DOI 10.1109/LSP.2013.2260737
   Yang MH, 2000, ADV NEUR IN, V12, P862
   Yu SX, 2009, LECT NOTES COMPUT SC, V5875, P157, DOI 10.1007/978-3-642-10331-5_15
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zhang JM, 2013, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2013.26
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
   Zhou L, 2014, SIGNAL PROCESS-IMAGE, V29, P434, DOI 10.1016/j.image.2014.01.001
NR 68
TC 8
Z9 8
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2017
VL 33
IS 5
BP 665
EP 685
DI 10.1007/s00371-016-1216-1
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ER4KB
UT WOS:000398767300010
DA 2024-07-18
ER

PT J
AU Siltanen, S
AF Siltanen, Sanni
TI Diminished reality for augmented reality interior design
SO VISUAL COMPUTER
LA English
DT Article
DE Diminished reality; Inpainting; Augmented reality; Interior design; AR
ID ROOM; RECONSTRUCTION
AB A modular real-time diminished reality pipeline for indoor applications is presented. The pipeline includes a novel inpainting method which requires no prior information of the textures behind the object to be diminished. The inpainting method operates on rectified images and adapts to scene illumination. In typically challenging illumination situations, the method produces more realistic results in indoor scenes than previous approaches. Modularity enables using alternative implementations in different stages and adapting the pipeline for different applications. Finally, practical solutions to problems occurring in diminished reality applications, for example interior design, are discussed.
C1 [Siltanen, Sanni] VTT Tech Res Ctr Finland Ltd, Espoo, Finland.
   [Siltanen, Sanni] VTT Tech Res Ctr Finland Ltd, POB 1000, Espoo 02044, Finland.
C3 VTT Technical Research Center Finland; VTT Technical Research Center
   Finland
RP Siltanen, S (corresponding author), VTT Tech Res Ctr Finland Ltd, POB 1000, Espoo 02044, Finland.
EM sanni.siltanen@vtt.fi
CR [Anonymous], 1968, P 1968 ACM NAT C
   [Anonymous], 2014, RGBD MAPPING USING D
   Bao SY, 2014, IEEE WINT CONF APPL, P690, DOI 10.1109/WACV.2014.6836035
   Bhatewara N, 2013, 2013 4TH IEEE INTERNATIONAL CONFERENCE ON COMPUTER & COMMUNICATION TECHNOLOGY (ICCCT), P60, DOI 10.1109/ICCCT.2013.6749604
   Boun Vinh Lu, 2010, 2010 9th IEEE International Symposium on Mixed and Augmented Reality (ISMAR). Science & Technology Papers, P109, DOI 10.1109/ISMAR.2010.5643558
   Criminisi A, 2003, PROC CVPR IEEE, P721
   Dou MS, 2014, 2014 IEEE VIRTUAL REALITY (VR), P39, DOI 10.1109/VR.2014.6802048
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Hartley R, 2000, MULTIPLE VIEW GEOMET
   Herling J., 2010, 2010 9th IEEE International Symposium on Mixed and Augmented Reality (ISMAR). Science & Technology Papers, P207, DOI 10.1109/ISMAR.2010.5643572
   Herling J, 2014, IEEE T VIS COMPUT GR, V20, P866, DOI 10.1109/TVCG.2014.2298016
   Herling J, 2012, INT SYM MIX AUGMENT, P141, DOI 10.1109/ISMAR.2012.6402551
   Jarusirisawad Songkran, 2010, Progress in Informatics, P11, DOI 10.2201/NiiPi.2010.7.3
   Kawai Norihiko, 2014, Virtual, Augmented and Mixed Reality. Designing and Developing Virtual and Augmented Environments. 6th International Conference, VAMR 2014, Held as Part of HCI International 2014. Proceedings: LNCS 8525, P363, DOI 10.1007/978-3-319-07458-0_34
   Kawai N, 2012, INT SYM MIX AUGMENT, P293, DOI 10.1109/ISMAR.2012.6402580
   Kholgade N, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601209
   Korkalo O., 2010, 2010 9th IEEE International Symposium on Mixed and Augmented Reality (ISMAR). Science & Technology Papers, P247, DOI 10.1109/ISMAR.2010.5643590
   Li ZW, 2013, INT SYM MIX AUGMENT, P11, DOI 10.1109/ISMAR.2013.6671759
   Liu HW, 2014, J VIS COMMUN IMAGE R, V25, P709, DOI 10.1016/j.jvcir.2013.03.012
   Mann Steve, 2002, PRESENCE TELEOPERATO
   Mura C, 2014, COMPUT GRAPH-UK, V44, P20, DOI 10.1016/j.cag.2014.07.005
   Pintore G, 2014, VISUAL COMPUT, V30, P707, DOI 10.1007/s00371-014-0947-0
   Siltanen Sanni, 2006, 2006 IEEE/ACM International Symposium on Mixed and Augmented Reality, P253, DOI 10.1109/ISMAR.2006.297831
   Siltanen S., 2013, INT J ARTS SCI, V6, P547
   Siltanen S, 2014, INT SYM MIX AUGMENT, P371
   Simon G, 2011, VISUAL COMPUT, V27, P827, DOI 10.1007/s00371-011-0556-0
   Yang MD, 2013, AUTOMAT CONSTR, V33, P48, DOI 10.1016/j.autcon.2012.09.017
   Zou CQ, 2015, COMPUT GRAPH-UK, V46, P130, DOI 10.1016/j.cag.2014.09.031
NR 28
TC 29
Z9 35
U1 1
U2 31
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2017
VL 33
IS 2
BP 193
EP 208
DI 10.1007/s00371-015-1174-z
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI2TF
UT WOS:000392340400007
DA 2024-07-18
ER

PT J
AU Ji, ZP
   Fang, ME
   Wang, YG
   Ma, WY
AF Ji, Zhongping
   Fang, Mei-e
   Wang, Yigang
   Ma, Weiyin
TI Efficient decolorization preserving dominant distinctions
SO VISUAL COMPUTER
LA English
DT Article
DE Decolorization; Chromatic orientation; Difference of Gaussians;
   Luminance filter
ID COLOR; CONVERSION; IMAGES
AB Representing color images in grayscale has practical and theoretical importance. Current color-to-gray transformations seldom ensure both quality and efficiency simultaneously in practice. In this paper, we present an efficient global mapping from color to gray while preserving visually dominant features of color images. Our color-to-gray transformation is based on a variant of traditional Difference of Gaussians band-pass filter, which is called luminance filter. The band-pass filter usually has high responses on regions with discriminative colors from their surroundings for certain band. The grayscale is derived from the luminance passing a series of band-pass filters. Our method is linear in the number of pixels, simple to implement and computationally efficient, making it suitable for high resolution images. Experimental results show that our method produces convincing results for a large number of natural and synthetic images.
C1 [Ji, Zhongping; Fang, Mei-e; Wang, Yigang] Hangzhou Dianzi Univ, Sch Comp Sci Technol, Hangzhou, Zhejiang, Peoples R China.
   [Ma, Weiyin] City Univ Hong Kong, Dept Mech & Biomed Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Hangzhou Dianzi University; City University of Hong Kong
RP Ji, ZP (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci Technol, Hangzhou, Zhejiang, Peoples R China.
EM jzp@hdu.edu.cn; fme@hdu.edu.cn; wangyg@cad.zju.edu.cn;
   mewma@cityu.edu.hk
RI Fang, Meie/IYS-4458-2023; MA, Weiyin/K-9155-2015
OI Fang, Meie/0000-0003-4292-8889; MA, Weiyin/0000-0001-9760-7789
FU National Natural Science Foundation of China [61202278, 61272032];
   Research Grants Council of Hong Kong SAR [CityU 118512]; City University
   of Hong Kong [SRG 7004072]
FX This work was partially supported by the National Natural Science
   Foundation of China (61202278, 61272032) and Research Grants Council of
   Hong Kong SAR (CityU 118512), City University of Hong Kong (SRG
   7004072).
CR [Anonymous], 2007, PROC 3 EUR C COMPUTA
   [Anonymous], 2010, P AS C COMP VIS
   Bala R, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P82
   Cadík M, 2008, COMPUT GRAPH FORUM, V27, P1745
   Cui M, 2010, VISUAL COMPUT, V26, P1349, DOI 10.1007/s00371-009-0412-7
   Gooch AA, 2005, ACM T GRAPHIC, V24, P634, DOI 10.1145/1073204.1073241
   Gooch B, 2004, ACM T GRAPHIC, V23, P27, DOI 10.1145/966131.966133
   Grundland M, 2007, PATTERN RECOGN, V40, P2891, DOI 10.1016/j.patcog.2006.11.003
   Kang H, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P43
   Kim Y, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618507
   Kuhn GR, 2008, VISUAL COMPUT, V24, P505, DOI 10.1007/s00371-008-0231-2
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu CW, 2014, INT J COMPUT VISION, V110, P222, DOI 10.1007/s11263-014-0732-6
   MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020
   Metzger F., 2006, Laws of seeing
   Nand NR, 2012, P IEEE INT FREQ CONT
   Rasche K, 2005, COMPUT GRAPH FORUM, V24, P423, DOI 10.1111/j.1467-8659.2005.00867.x
   Smith K, 2008, COMPUT GRAPH FORUM, V27, P193, DOI 10.1111/j.1467-8659.2008.01116.x
   Wu JL, 2012, VISUAL COMPUT, V28, P723, DOI 10.1007/s00371-012-0683-2
   Zhao Y, 2010, LECT NOTES COMPUT SC, V6454, P747, DOI 10.1007/978-3-642-17274-8_73
   Zhu W, 2014, VISUAL COMPUT, V30, P299, DOI 10.1007/s00371-013-0854-9
NR 21
TC 12
Z9 12
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2016
VL 32
IS 12
BP 1621
EP 1631
DI 10.1007/s00371-015-1145-4
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EB3NH
UT WOS:000387271200010
DA 2024-07-18
ER

PT J
AU Li, Y
   Peng, ZL
   Liang, DP
   Chang, HY
   Cai, ZQ
AF Li, Ya
   Peng, Zhanglin
   Liang, Depeng
   Chang, Huiyou
   Cai, Zhaoquan
TI Facial age estimation by using stacked feature composition and selection
SO VISUAL COMPUTER
LA English
DT Article
DE Age estimation; Features composition; Biological inspired features;
   Hierarchical architecture; Deep learning
ID BIOLOGICALLY INSPIRED FEATURES; RECOGNITION; IMAGE; MANIFOLD
AB In this paper we propose a novel hierarchical feature composition and selection model used in facial age estimation. In recent years, hierarchical architectures have been shown to outperform the flat structure on a variety of visual modeling tasks and has drawn a lot of attention. In our hierarchical architecture, we use biological inspired features as primitive features, then alternatively select and composite newer features. Firstly, we select features in a boosting way and then weightily combine adjacent selected features. The whole process of feature selection and combination is called a boosting layer. We then stack multiple boosting layers into a hierarchical model. In each boosting layer, a number of weak classifiers comprise the selected features, and their combination weights are inversely proportional against the training errors of weak classifiers. In this way, features of a high layer are more descriptive and with higher semantics, while features of a lower layer include more physical details. We expect that this kind of structural features will be more expressive and objective and hence perform better and with higher efficiency in facial age estimation. Our experimental results on two aging face databases MORPH and FG-NET have shown significant reduction on mean absolute error of age estimation compared with other state-of-the-art methods.
C1 [Li, Ya; Peng, Zhanglin] Sun Yat Sen Univ, Sch Informat Sci & Technol, Guangzhou, Guangdong, Peoples R China.
   [Li, Ya] Guangzhou Univ, Sch Comp Sci & Educ Software, Guangzhou, Guangdong, Peoples R China.
   [Chang, Huiyou] Sun Yat Sen Univ, Sch Software, Guangzhou, Guangdong, Peoples R China.
   [Liang, Depeng] Sun Yat Sen Univ, Sch Adv Comp, Guangzhou, Guangdong, Peoples R China.
   [Cai, Zhaoquan] Huizhou Univ, Network Ctr, Huizhou, Peoples R China.
C3 Sun Yat Sen University; Guangzhou University; Sun Yat Sen University;
   Sun Yat Sen University; Huizhou University
RP Chang, HY (corresponding author), Sun Yat Sen Univ, Sch Software, Guangzhou, Guangdong, Peoples R China.
EM liya@gzhu.edu.cn; z.l.peng1990@gmail.com; liangdepeng@gmail.com;
   isschy@mail.sysu.edu.cn; caizhaoquan@139.com
FU National High Technology Research and Development Program of China
   [2013AA013801]; Science and Technology Planning Project of Guangdong
   Province [2013B010406005]; Guangdong Natural Science Foundation
   [S2013040012570, S2013010013432]; National Natural Science Foundation of
   China [61170193, 61370185]
FX This research was supported by the National High Technology Research and
   Development Program of China (No. 2013AA013801), the Science and
   Technology Planning Project of Guangdong Province (No. 2013B010406005),
   the Guangdong Natural Science Foundation (No. S2013040012570, No.
   S2013010013432), and the National Natural Science Foundation of China
   (No. 61170193, No. 61370185). The authors would like to thank the
   reviewers for their comments and suggestions.
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], ACM INT C MULT
   [Anonymous], INT J COMPUT ELECT E
   [Anonymous], 2007, P IEEE 11 INT C COMP
   [Anonymous], 2014, ARXIV14043606
   Berbar MA, 2014, VISUAL COMPUT, V30, P19, DOI 10.1007/s00371-013-0774-8
   Chang KY, 2011, PROC CVPR IEEE, P585, DOI 10.1109/CVPR.2011.5995437
   Chenjing Yan, 2014, Advances in Multimedia Information Processing - PCM 2014. 15th Pacific-Rim Conference on Multimedia. Proceedings: LNCS 8879, P211, DOI 10.1007/978-3-319-13168-9_22
   Chi J, 2014, VISUAL COMPUT, V30, P649, DOI 10.1007/s00371-014-0960-3
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Fu Y, 2008, IEEE T MULTIMEDIA, V10, P578, DOI 10.1109/TMM.2008.921847
   Fu Y, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1383
   Fu Y, 2010, IEEE T PATTERN ANAL, V32, P1955, DOI 10.1109/TPAMI.2010.36
   Geng X, 2007, IEEE T PATTERN ANAL, V29, P2234, DOI 10.1109/TPAMI.2007.70733
   Geng X, 2014, INT C PATT RECOG, P4465, DOI 10.1109/ICPR.2014.764
   Geng X, 2013, IEEE T PATTERN ANAL, V35, P2401, DOI 10.1109/TPAMI.2013.51
   Guo G., 2010, IEEE COMP SOC C COMP, P71, DOI DOI 10.1109/CVPRW.2010.5543609
   Guo GD, 2008, IEEE T IMAGE PROCESS, V17, P1178, DOI 10.1109/TIP.2008.924280
   Guo GD, 2014, PROC CVPR IEEE, P4257, DOI 10.1109/CVPR.2014.542
   Guo GD, 2011, PROC CVPR IEEE, P657, DOI 10.1109/CVPR.2011.5995404
   Guo GD, 2009, IEEE I CONF COMP VIS, P1986, DOI 10.1109/ICCV.2009.5459438
   Guo GD, 2009, PROC CVPR IEEE, P112, DOI 10.1109/CVPRW.2009.5206681
   Guo Guodong., 2008, IEEE Workshop on Applications of Computer Vision, V51, P61801
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang KQ, 2011, IEEE T SYST MAN CY B, V41, P307, DOI 10.1109/TSMCB.2009.2037923
   Kan M, 2014, PROC CVPR IEEE, P1883, DOI 10.1109/CVPR.2014.243
   Lanitis A, 2002, IEEE T PATTERN ANAL, V24, P442, DOI 10.1109/34.993553
   Lin L, 2015, IEEE T PATTERN ANAL, V37, P959, DOI 10.1109/TPAMI.2014.2359888
   Lin L, 2009, PATTERN RECOGN, V42, P1297, DOI 10.1016/j.patcog.2008.10.033
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma BP, 2013, NEUROCOMPUTING, V115, P1, DOI 10.1016/j.neucom.2012.11.005
   Meyers E, 2008, INT J COMPUT VISION, V76, P93, DOI 10.1007/s11263-007-0058-8
   Mutch J, 2008, INT J COMPUT VISION, V80, P45, DOI 10.1007/s11263-007-0118-0
   Nguyen DT, 2014, FUTURE INFORM TECHNO, P433
   Ni B., 2009, Proceedings of the 17th ACM international conference on Multimedia, P85
   Ni BB, 2011, IEEE T MULTIMEDIA, V13, P1217, DOI 10.1109/TMM.2011.2167317
   Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341
   Serre T, 2007, IEEE T PATTERN ANAL, V29, P411, DOI 10.1109/TPAMI.2007.56
   Song DJ, 2010, IEEE T IMAGE PROCESS, V19, P174, DOI 10.1109/TIP.2009.2032939
   Su Y, 2010, INT CONF ACOUST SPEE, P1270, DOI 10.1109/ICASSP.2010.5495414
   Sun Y, 2014, ADV NEUR IN, V27
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Suo JL, 2011, IEEE T SYST MAN CY A, V41, P226, DOI 10.1109/TSMCA.2010.2064304
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Torralba A, 2004, PROC CVPR IEEE, P762
   Weng R, 2013, 2013 10 IEEE INT C W, P1
   Yan S, 2008, PR IEEE COMP DESIGN, P142, DOI 10.1109/ICCD.2008.4751853
   Yang X., 2014, MULTIMEDIA EXPO ICME, P1
   Yuan JW, 2009, COGNITION EMOTION, V23, P1221, DOI 10.1080/02699930802416453
   Zhang YZ, 2013, IEEE I CONF COMP VIS, P2416, DOI 10.1109/ICCV.2013.300
   Zhang Y, 2014, INT CONF SYST SCI EN, P7, DOI 10.1109/ICSSE.2014.6887894
   Zhang Y, 2010, PROC CVPR IEEE, P2622, DOI 10.1109/CVPR.2010.5539975
NR 53
TC 17
Z9 18
U1 0
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2016
VL 32
IS 12
BP 1525
EP 1536
DI 10.1007/s00371-015-1137-4
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EB3NH
UT WOS:000387271200003
DA 2024-07-18
ER

PT J
AU Li, HL
   Toyoura, M
   Shimizu, K
   Yang, W
   Mao, XY
AF Li, Honglin
   Toyoura, Masahiro
   Shimizu, Kazumi
   Yang, Wei
   Mao, Xiaoyang
TI Retrieval of clothing images based on relevance feedback with focus on
   collar designs
SO VISUAL COMPUTER
LA English
DT Article
DE Content-based clothing image retrieval; Collar design; Feature
   extraction; Saliencymap; SIFT; Relevance feedback; Optimum-path forest
AB The content-based image retrieval methods are developed to help people find what they desire based on preferred images instead of linguistic information. This paper focuses on capturing the image features representing details of the collar designs, which is important for people to choose clothing. The quality of the feature extraction methods is important for the queries. This paper presents several new methods for the collar-design feature extraction. A prototype of clothing image retrieval system based on relevance feedback approach and optimum-path forest algorithm is also developed to improve the query results and allows users to find clothing image of more preferred design. A series of experiments are conducted to test the qualities of the feature extraction methods and validate the effectiveness and efficiency of the RF-OPF prototype from multiple aspects. The evaluation scores of initial query results are used to test the qualities of the feature extraction methods. The average scores of all RF steps, the average numbers of RF iterations taken before achieving desired results and the score transition of RF iterations are used to validate the effectiveness and efficiency of the proposed RF-OPF prototype.
C1 [Li, Honglin] Univ Yamanashi, Comp Sci, Yamanashi, Japan.
   [Toyoura, Masahiro; Mao, Xiaoyang] Univ Yamanashi, Interdisciplinary Grad Sch, Yamanashi, Japan.
   [Shimizu, Kazumi; Yang, Wei] Univ Yamanashi, Yamanashi, Japan.
C3 University of Yamanashi; University of Yamanashi; University of
   Yamanashi
RP Mao, XY (corresponding author), Univ Yamanashi, Interdisciplinary Grad Sch, Yamanashi, Japan.
EM mao@yamanashi.ac.jp
OI Toyoura, Masahiro/0000-0002-5897-7573; mao, xiaoyang/0000-0001-9531-3197
FU JSPS KAKENHI [16H05867, 25280037]; Grants-in-Aid for Scientific Research
   [25280037, 16H05867] Funding Source: KAKEN
FX We would like to thank Prof Takami Yamamoto for her valuable comments.
   This work was partially supported by JSPS KAKENHI (16H05867, 25280037).
CR [Anonymous], P 11 AS C COMP VIS
   [Anonymous], TECH REP
   [Anonymous], 2007, PROC IEEE C COMPUT V, DOI 10.1109/CVPR.2007.383267
   da Silva A. T., 2010, J WSCG, V18, P73
   da Silva AT, 2011, PATTERN RECOGN, V44, P2971, DOI 10.1016/j.patcog.2011.04.026
   FANG J-J., 2003, International Journal of Clothing Science and Technology, V15, P88, DOI 10.1108/09556220310470088
   Fei-Fei L., 2007, P IEEE COMP SOC C CO
   Hauswiesner S, 2013, IEEE T VIS COMPUT GR, V19, P1552, DOI 10.1109/TVCG.2013.67
   Hsu Esther, 2011, EE368 STANF U DEP EL
   Kondo S., 2014, SKIRT IMAGE RETRIEVA, P11
   Liu S, 2012, PROC CVPR IEEE, P3330, DOI 10.1109/CVPR.2012.6248071
   Liu YJ, 2010, COMPUT IND, V61, P576, DOI 10.1016/j.compind.2010.03.007
   Nister David, 2006, CVPR
   Onuma J., 2001, COORDINATE TECHNIQUE
   Papa JP, 2009, INT J IMAG SYST TECH, V19, P120, DOI 10.1002/ima.20188
   Pundir N., 2007, FASHION TECHOLOGY TO
   Qiu G, 2002, PATTERN RECOGN, V35, P1675, DOI 10.1016/S0031-3203(01)00162-5
   Shimizu K, 2015, 2015 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P137, DOI 10.1109/CW.2015.70
   Tekawa M, 2010, LECT NOTES COMPUT SC, V6444, P598, DOI 10.1007/978-3-642-17534-3_74
   Veltkamp Remco C., 2000, UUCS200034
   Wang L., 2008, J PANZHIHUA U, V2, P87
   Zhang X., 2014, ACM SIGGRAPH
NR 22
TC 11
Z9 12
U1 2
U2 18
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2016
VL 32
IS 10
BP 1351
EP 1363
DI 10.1007/s00371-016-1232-1
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EA2BQ
UT WOS:000386396800012
DA 2024-07-18
ER

PT J
AU Liu, YS
   Su, ZX
   Cao, JJ
   Wang, H
AF Liu, Yusong
   Su, Zhixun
   Cao, Junjie
   Wang, Hui
TI Harmonic mean normalized Laplace-Beltrami spectral descriptor
SO VISUAL COMPUTER
LA English
DT Article
DE 3D shape retrieval; Normalized spectral descriptor; Non-rigid;
   Multi-level
AB This paper proposes a framework based on harmonic mean normalized Laplace-Beltrami spectral descriptor. A series of experiments show that the harmonic mean normalization has better performance for non-rigid 3D retrieval, and it is robust to holes, local scaling, noise and sampling. To better distinguish shapes with fine or rough details, weighting method and fusion method are also employed. Weighting method reduces the negative impact of high-frequency information, and fusion method combines multi-level spectral information in both low and high frequencies. Our approach has better performance than other state-of-the-art methods on both retrieval accuracy and time consumption for stretched non-rigid 3D shapes.
C1 [Liu, Yusong; Su, Zhixun; Cao, Junjie] Dalian Univ Technol, Sch Math Sci, Dalian 116024, Peoples R China.
   [Liu, Yusong] China Univ Petr Huadong, Sch Sci, Qingdao 266555, Peoples R China.
   [Wang, Hui] Shijiazhuang Tiedao Univ, Sch Informat Sci & Technol, Shijiazhuang 050043, Peoples R China.
C3 Dalian University of Technology; China University of Petroleum;
   Shijiazhuang Tiedao University
RP Su, ZX (corresponding author), Dalian Univ Technol, Sch Math Sci, Dalian 116024, Peoples R China.
EM ysliu1758@163.com; zxsu@dlut.edu.cn; jjcao1231@gmail.com;
   wanghui19841109@gmail.com
FU National Natural Science Foundation of China [61173103, 61572099,
   61320106008, 91230103, 61363048, 61262050, 61402300]; National Science
   and Technology Major Project [2013ZX04005021, 2014ZX04001011]; Natural
   Science Foundation of Hebei Province [F2014210127]; Projects for
   Introduction of Overseas Scholars of Hebei Province; Funds for Excellent
   Young Scholar of Shijiazhuang Tiedao University
FX This work was partially supported by the National Natural Science
   Foundation of China (61173103, 61572099, 61320106008, 91230103,
   61363048, 61262050, 61402300), National Science and Technology Major
   Project (2013ZX04005021, 2014ZX04001011), the Natural Science Foundation
   of Hebei Province (F2014210127), the Funded Projects for Introduction of
   Overseas Scholars of Hebei Province, and the Funds for Excellent Young
   Scholar of Shijiazhuang Tiedao University.
CR [Anonymous], 2011, SCALE SPACE VARIATIO
   [Anonymous], 2009, P ACM INT C IM VID R, DOI DOI 10.1145/1646396.1646430
   [Anonymous], 2011, PROC EUROGRAPHICS 20, P79, DOI DOI 10.2312/3DOR/3DOR11/079-088
   Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Berger, 1987, GEOMETRY, VI
   Bronstein A., 2010, PROC WORKSHOP 3D OBJ, P1
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   Bronstein MM, 2011, IEEE T PATTERN ANAL, V33, P1065, DOI 10.1109/TPAMI.2010.210
   Bronstein MM, 2010, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2010.5539838
   Bu SH, 2014, VISUAL COMPUT, V30, P867, DOI 10.1007/s00371-014-0970-1
   Carcassoni M, 2003, PATTERN RECOGN, V36, P193, DOI 10.1016/S0031-3203(02)00054-7
   Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006
   Fouss F, 2007, IEEE T KNOWL DATA EN, V19, P355, DOI 10.1109/TKDE.2007.46
   Lavoue G., 2011, Eurographics Conference on 3D Object Retrieval, P41, DOI DOI 10.2312/3DOR/3DOR11/041-048
   Levy B, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P66
   Li B, 2014, MULTIMED TOOLS APPL, V72, P1531, DOI 10.1007/s11042-013-1464-2
   Li CY, 2013, VISUAL COMPUT, V29, P513, DOI 10.1007/s00371-013-0815-3
   Lian ZH, 2010, INT J COMPUT VISION, V89, P130, DOI 10.1007/s11263-009-0295-0
   Liang Z., 2010, IEEE International Conference on Communications ICC, P1
   Lipman Y., 2010, ACM T GRAPHIC, V29, P483
   Marini S., 2010, P 3DOR, P31, DOI DOI 10.2312/3DOR/3DOR10/031-038
   Meyer N, 2003, VISUALIZATION AND MATHEMATICS III, P35
   Raviv D, 2015, SIAM J IMAGING SCI, V8, P403, DOI 10.1137/140987675
   Raviv D, 2015, INT J COMPUT VISION, V111, P1, DOI 10.1007/s11263-014-0728-2
   Raviv D, 2014, J MATH IMAGING VIS, V50, P144, DOI 10.1007/s10851-013-0467-y
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Reuter M., 2005, P 2005 ACM S SOLID P, P101, DOI DOI 10.1145/1060244.1060256
   Rustamov Raif M, 2007, P S GEOM PROC, V257, P225
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Smeets D, 2010, LECT NOTES COMPUT SC, V6169, P162, DOI 10.1007/978-3-642-14061-7_16
   Smeets D, 2009, LECT NOTES COMPUT SC, V5702, P757, DOI 10.1007/978-3-642-03767-2_92
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Wu HY, 2010, PROC CVPR IEEE, P438, DOI 10.1109/CVPR.2010.5540180
   Yi Fang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2145, DOI 10.1109/CVPR.2011.5995695
   Zaharescu A, 2009, PROC CVPR IEEE, P373, DOI 10.1109/CVPRW.2009.5206748
   Zhouhui Lian, 2010, Proceedings of the Shape Modeling International (SMI 2010), P25, DOI 10.1109/SMI.2010.20
NR 36
TC 1
Z9 3
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2016
VL 32
IS 9
BP 1097
EP 1108
DI 10.1007/s00371-015-1172-1
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DU4DE
UT WOS:000382161400004
DA 2024-07-18
ER

PT J
AU Berger, B
   Wolter, FE
   Vais, A
AF Berger, Benjamin
   Wolter, Franz-Erich
   Vais, Alexander
TI Colocalization structures and eigenvalue spectra for colour image
   comparison
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Laplace; Eigenvalue; Fingerprint; Image retrieval; Image comparison;
   Colour images
AB Eigenvalue spectra of the Laplace-Beltrami operator have successfully been employed as fingerprints for shape and image comparison. Especially notable in this context is the work of Peinecke on Laplace spectrum fingerprinting for image data. Recently, new research on greyscale images by Berger et al. introduces the idea of attributing individual eigenfunctions to image parts and describes a mechanism for controlling their localisation. These parts are separated by sufficiently strong variations of grey value, giving the originally global fingerprint a semi-local character. This paper provides an approach to extend this idea to colour images so that not only gradients of brightness but also gradients of hue or chroma lead to localisation of eigenfunctions. This is accomplished by generalising the eigenfunctions to -valued functions and mapping the colours to symmetric -matrices. The resulting matrix field is then used to modify the Laplacian. Finally, we present a distance function for comparing eigenvalue-based fingerprints that makes use of eigenfunction colocalization information.
C1 [Berger, Benjamin; Wolter, Franz-Erich; Vais, Alexander] Leibniz Univ Hannover, D-30167 Hannover, Germany.
C3 Leibniz University Hannover
RP Berger, B (corresponding author), Leibniz Univ Hannover, D-30167 Hannover, Germany.
EM bberger@welfenlab.de
RI Berger, Benjamin/HJA-7645-2022; Wolter, Franz-Erich/AAV-3008-2020;
   Wolter, Franz-Erich/JAC-5956-2023
OI Berger, Benjamin/0000-0003-2654-2898; Wolter,
   Franz-Erich/0000-0002-2293-5494; Wolter, Franz-Erich/0000-0002-2293-5494
CR [Anonymous], 2011, J MACH LEARN TECHNOL
   Berger B, 2015, VISUAL COMPUT, V31, P205, DOI 10.1007/s00371-014-1038-y
   Jinkerson R. A., 1993, SCIENCE, V9, P88
   KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671
   Peinecke N, 2007, COMPUT AIDED DESIGN, V39, P460, DOI 10.1016/j.cad.2007.01.014
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Reuter M., 2005, P 2005 ACM S SOLID P, P101, DOI DOI 10.1145/1060244.1060256
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Wolter F.-E., 2012, 7 WELF LAB
   Wolter FE, 2000, COMPUTER GRAPHICS INTERNATIONAL 2000, PROCEEDINGS, P137, DOI 10.1109/CGI.2000.852329
NR 10
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 1057
EP 1067
DI 10.1007/s00371-016-1260-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600036
DA 2024-07-18
ER

PT J
AU Chen, YY
   Chien, SY
AF Chen, Yen-Yu
   Chien, Shao-Yi
TI Lighting-driven voxels for memory-efficient computation of indirect
   illumination
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Indirect illumination; Voxel cone tracing; Ray tracing
ID GLOBAL ILLUMINATION
AB Several recently proposed voxel-based global illumination algorithms rely on the use of reflective shadow maps (RSMs) to interactively compute indirect illumination. However, RSMs do not scale well with the number of light sources because of their high memory consumption when rendering. Observing that, in most cases only a fraction of the voxels really contribute to single-bounce indirect illumination, in this paper we propose the use of lighting-driven voxels (LDVs), which are constructed from a subset of voxels, to reduce the memory burden. They are used in conjunction with a voxel-based global illumination algorithm that enables the interactive indirect illumination of dynamic scenes. We evaluate the memory usage, query performance, and construction speed for various voxel resolutions. Empirically, rendering with LDVs consumes an order of magnitude less memory than rendering with RSMs. Further, it achieves a higher performance for radiance queries when multiple light sources are used. Moreover, we integrated our method into voxel ray tracing and voxel cone tracing. For each of algorithm, we achieve an interactive performance that significantly reduces memory with respect to the reference solution.
C1 [Chen, Yen-Yu; Chien, Shao-Yi] Natl Taiwan Univ, Grad Inst Elect Engn, Dept Elect Engn, Taipei, Taiwan.
C3 National Taiwan University
RP Chen, YY (corresponding author), Natl Taiwan Univ, Grad Inst Elect Engn, Dept Elect Engn, Taipei, Taiwan.
EM yychen@media.ee.ntu.edu.tw
OI Chien, Shao-Yi/0000-0002-0634-6294
CR [Anonymous], 2018, Real-Time Rendering
   Barák T, 2013, COMPUT GRAPH FORUM, V32, P87, DOI 10.1111/cgf.12154
   Crassin C., 2009, P 2009 S INT 3D GRAP, P15, DOI [10.1145/1507149.1507152, DOI 10.1145/1507149.1507152]
   Crassin C, 2012, OPENGL INSIGHTS, P303
   Crassin C, 2011, COMPUT GRAPH FORUM, V30, P1921, DOI 10.1111/j.1467-8659.2011.02063.x
   Dachsbacher C., 2005, Proc. Symp. Interactive Graph. and Games, P203, DOI DOI 10.1145/1053427.1053460
   Dachsbacher C., 2006, Proc. Symp. Interactive 3D Graph. and Games, Redwood City, P93, DOI DOI 10.1145/1111411.1111428
   Dachsbacher C, 2014, COMPUT GRAPH FORUM, V33, P88, DOI 10.1111/cgf.12256
   HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.1093/biomet/57.1.97
   Kämpe V, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462024
   Kaplanyan A, 2009, ACM SIGGRAPH 2009 AD
   Kaplanyan A., 2010, Proceedings of the 2010 ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, I3D'10, P99, DOI [10.1145/1730804.1730821, 10.1145/1730804.1730821.24, DOI 10.1145/1730804.1730821.24]
   Laine S., 2010, Proceedings of the Symposium on Interactive 3D Graphics and Games, P55, DOI DOI 10.1145/1730804.1730814
   Nichols G., 2009, P 2009 S INTERACTIVE, P83, DOI DOI 10.1145/1507149.1507162
   Nichols G, 2010, IEEE T VIS COMPUT GR, V16, P729, DOI 10.1109/TVCG.2009.97
   Nichols G, 2009, COMPUT GRAPH FORUM, V28, P1141, DOI 10.1111/j.1467-8659.2009.01491.x
   Ritschel T., 2008, ACM T GRAPHIC, V27, P129
   Ritschel T, 2012, COMPUT GRAPH FORUM, V31, P160, DOI 10.1111/j.1467-8659.2012.02093.x
   Ritschel T, 2011, COMPUT GRAPH FORUM, V30, P2258, DOI 10.1111/j.1467-8659.2011.01998.x
   Sugihara M., 2014, P HIGH PERF GRAPH HP, P117, DOI 10.2312/hpg.20141100
   Tabellion E, 2004, ACM T GRAPHIC, V23, P469, DOI 10.1145/1015706.1015748
   Thiedemann Sinje., 2011, Symposium on Interactive 3D Graphics and Games, I3D '11, P103
NR 22
TC 3
Z9 4
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 781
EP 789
DI 10.1007/s00371-016-1235-y
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600011
DA 2024-07-18
ER

PT J
AU Gagnon, J
   Dagenais, F
   Paquette, E
AF Gagnon, Jonathan
   Dagenais, Francois
   Paquette, Eric
TI Dynamic lapped texture for fluid simulations
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Fluid animation; Texture synthesis
ID IMAGE
AB We present a new approach for texturing fluids. Particle trackers are scattered on the surface of the fluid, and used to track deformations and topological changes. For every frame of the animation, the trackers are advected and rotated coherently with the flow of the fluid. Receiver polygons are identified on the fluid surface and are used to transfer uv coordinates, while ensuring a controllable amount of texture distortion. The density of the trackers is adjusted when constructing a texture atlas used for rendering. Trackers that remain unused when filling the atlas are safely removed, while texels of the atlas without any corresponding tracker identify areas where new trackers will be added. Together with our patch layering approach, this tracker creation and removal process reduces popping artifacts. Both the input (fluid surface mesh and velocity field) and the output (texture atlas) of our approach make it easy to integrate into a typical production pipeline. We tested our approach on several types of fluid simulation scenarios, including splashes, rotational flows, and viscous fluids. The resulting animations of textured fluids are free from temporal artifacts and popping, and show a limited amount of distortion, blurring, and discontinuity.
C1 [Gagnon, Jonathan; Dagenais, Francois; Paquette, Eric] Ecole Technol Super, Multimedia Lab, Montreal, PQ, Canada.
   [Gagnon, Jonathan] Mokko Studio, Res & Dev, Montreal, PQ, Canada.
C3 University of Quebec; Ecole de Technologie Superieure - Canada
RP Gagnon, J (corresponding author), Ecole Technol Super, Multimedia Lab, Montreal, PQ, Canada.; Gagnon, J (corresponding author), Mokko Studio, Res & Dev, Montreal, PQ, Canada.
EM jonathangagnon@gmail.com
OI Paquette, Eric/0000-0001-9236-647X
CR [Anonymous], P ACM SIGGRAPH EUR S
   Bojsen-Hansen M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185549
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Jamriska O, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766983
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Kwatra V, 2007, IEEE T VIS COMPUT GR, V13, P939, DOI 10.1109/TVCG.2007.1044
   Mihalef V, 2007, COMPUT GRAPH FORUM, V26, P457, DOI 10.1111/j.1467-8659.2007.01068.x
   Narain R., 2007, EGSR 07
   Neyret F., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P147
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Praun E, 2000, COMP GRAPH, P465, DOI 10.1145/344779.344987
   Soler C, 2002, ACM T GRAPHIC, V21, P673, DOI 10.1145/566570.566635
   Turk G, 2001, COMP GRAPH, P347, DOI 10.1145/383259.383297
   Wei LY, 2001, COMP GRAPH, P355, DOI 10.1145/383259.383298
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   Wu Q, 2004, ACM T GRAPHIC, V23, P364, DOI 10.1145/1015706.1015730
   Yu QZ, 2011, IEEE T VIS COMPUT GR, V17, P1612, DOI 10.1109/TVCG.2010.263
   Yu QZ, 2009, COMPUT GRAPH FORUM, V28, P239, DOI 10.1111/j.1467-8659.2009.01363.x
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
   Zwicker M, 2001, COMP GRAPH, P371, DOI 10.1145/383259.383300
NR 22
TC 6
Z9 6
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 901
EP 909
DI 10.1007/s00371-016-1262-8
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600022
DA 2024-07-18
ER

PT J
AU Huang, LC
   Yang, RY
AF Huang, Luchen
   Yang, Ruoyu
TI Automatic alignment for virtual fitting using 3D garment stretching and
   human body relocation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Three-dimensional garment model; Automatic alignment; Penetration
   detection; Mesh deformation
ID SIMULATION
AB In recent years, researches on virtual fitting are mostly based on the data of garment pieces through Computer Aided Design. However, three-dimensional garment models are more intuitive, integrated and easy to access and manage. Therefore, we propose and design a method to automatically align virtual humans with three-dimensional garment models. Firstly, we basically align two models through three-dimensional geometric calculation and postural alteration of virtual humans. Then, we detect the penetration phenomena and define the points of the garment model penetrating into the human model as penetration points. Based on the penetration detection, we use Poisson mesh deformation to adjust the penetration points by imitating the adjustment process of dragging clothes. Finally, the results obtained from alignment can be directly used for physical-based simulation. Experimental results show that the method we propose can achieve good effects of alignment, and has good versatility and practicality.
C1 [Huang, Luchen] Nanjing Univ, Dept Comp Sci & Technol, Xianlin Ave 163, Nanjing, Jiangsu, Peoples R China.
   [Yang, Ruoyu] Nanjing Univ, Natl Key Lab Novel Software Technol, Xianlin Ave 163, Nanjing, Jiangsu, Peoples R China.
C3 Nanjing University; Nanjing University
RP Yang, RY (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Xianlin Ave 163, Nanjing, Jiangsu, Peoples R China.
EM yangry@nju.edu.cn
CR [Anonymous], 2012, ACM T GRAPHICS TOG
   Divivier A., 2008, INFORM SPEKTRUM, V27, P504
   Durupinar F, 2007, IEEE INT CONF INF VI, P862, DOI 10.1109/IV.2007.17
   Fuhrmann A, 2003, COMPUT GRAPH-UK, V27, P71, DOI 10.1016/S0097-8493(02)00245-5
   GROSS C, 2003, P 19 SPRING C COMP G, P99
   Guan P., 2012, ACM T GRAPHIC, V31, P13
   JIANG Y, 2008, P 2008 9 INT C COMP, P765
   Liu JD, 1996, VISUAL COMPUT, V12, P234
   Song WH, 2005, VISUAL COMPUT, V21, P139, DOI 10.1007/s00371-004-0277-8
   Tong YY, 2003, ACM T GRAPHIC, V22, P445, DOI 10.1145/882262.882290
   Yu YZ, 2004, ACM T GRAPHIC, V23, P644, DOI 10.1145/1015706.1015774
   Zhong YQ, 2010, TEXT RES J, V80, P904, DOI 10.1177/0040517509349790
   Zhong YQ, 2009, TEXT RES J, V79, P792, DOI 10.1177/0040517508090779
NR 13
TC 11
Z9 12
U1 0
U2 19
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 705
EP 715
DI 10.1007/s00371-016-1236-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600004
DA 2024-07-18
ER

PT J
AU Rojas, F
   Tarnogol, F
   Yang, HS
AF Rojas, Francisco
   Tarnogol, Fernando
   Yang, Hyun S.
TI Dynamic social formations of pedestrian groups navigating and using
   public transportation in a virtual city
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT International Conference on Cyberworlds (CW)
CY OCT 06-08, 2014
CL Santander, SPAIN
SP IEEE Comp Soc, Univ Cantabria, Comp Graph & Geometr Modeling Grp, Toho Univ, Fac Sci, Dept Informat Sci, European Assoc Comp Graph, Int Federat Informat Proc, Workgroup 5 10 Comp Graph & Virtual Worlds, Univ Cantabria, Dept Appl Math & Computatl Sci, Vice Rector Res & Knowledge Transfer, Municipal Santander, Reg Govt Cantabria, Spanish Minist Econ & Competitiveness, Cantabria Campus Int, Int Federat Informat Proc, Tech Comm 5 Informat Technol Applicat
DE Crowd simulation; Social group formations and queuing; Pal social
   gestures; Ray casting; Slot-locking; Public transportation
AB Most prior crowd simulations do not have groups of people moving in a social manner. In our work, we use a two-level steering system based on two classes: group agent and pedestrian agent. By interpolating the current and desired slot positions of the group agent according to formation templates, dynamic social group formations can be achieved and can also adapt to the width of passageways using our robust and optimized ray casting technique. Based on this interpolation approach, slot-locking keeps subgroups in a group shoulder-to-shoulder regardless of the current formation assuming sufficient surrounding space exists. At times pal social gestures between adjacent members may occur. We also introduce the social FIFO queue to be used in situations such as waiting for the bus. In the subway scene, we describe a seating strategy for passengers entering the subway car and being aware of your presence. In an immersive evaluation using an Oculus DK2 head-mounted display, participants validated the realism of dynamic social group behavior for navigation and making use of public transportation.
C1 [Rojas, Francisco; Yang, Hyun S.] KAIST AI & Media Lab, Daejeon, South Korea.
   [Tarnogol, Fernando] PsyTech LLC, Buenos Aires, DF, Argentina.
RP Rojas, F (corresponding author), KAIST AI & Media Lab, Daejeon, South Korea.
EM frojas@kaist.ac.kr; fernando@psychologicaltechnologies.com;
   hsyang@kaist.ac.kr
RI Yang, Hyun Seung/C-1984-2011; Alidadi, Mehdi/HJZ-0235-2023
OI Alidadi, Mehdi/0000-0001-5183-7829
CR COLEMAN JS, 1961, SOCIOMETRY, V24, P36, DOI 10.2307/2785927
   Ennis C, 2012, COMPUT ANIMAT VIRT W, V23, P321, DOI 10.1002/cav.1453
   Federici ML, 2012, LECT NOTES COMPUT SC, V7495, P699, DOI 10.1007/978-3-642-33350-7_72
   Hocevar Rafael, 2012, Intelligent Virtual Agents. Proceedings 12th International Conference, IVA 2012, P370, DOI 10.1007/978-3-642-33197-8_38
   Hostetler T., 2002, P EUR IR WORKSH, P7
   Hsu D, 2002, INT J ROBOT RES, V21, P233, DOI 10.1177/027836402320556421
   James J, 1953, AM SOCIOL REV, V18, P569, DOI 10.2307/2087444
   Kamphuis A., 2004, SCA '04: Proceedings of the 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation, P19
   Karamouzas I, 2012, IEEE T VIS COMPUT GR, V18, P394, DOI 10.1109/TVCG.2011.133
   Kavraki LE, 1996, IEEE T ROBOTIC AUTOM, V12, P566, DOI 10.1109/70.508439
   Kendon Adam, 1990, CUP Archive, P153
   LaValle SM, 2001, INT J ROBOT RES, V20, P378, DOI 10.1177/02783640122067453
   Loscos C, 2003, THEORY AND PRACTICE OF COMPUTER GRAPHICS, PROCEEDINGS, P122
   Moussaïd M, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010047
   Musse SR, 2001, IEEE T VIS COMPUT GR, V7, P152, DOI 10.1109/2945.928167
   MUSSE SR, 2000, THESIS EPFL LAUSANNE
   Niederberger C, 2003, COMPUT GRAPH FORUM, V22, P323, DOI 10.1111/1467-8659.00679
   Pedica C, 2008, LECT NOTES COMPUT SC, V5208, P104
   Pedica C, 2010, LECT NOTES ARTIF INT, V6356, P336, DOI 10.1007/978-3-642-15892-6_35
   Pedica C, 2010, APPL ARTIF INTELL, V24, P575, DOI 10.1080/08839514.2010.492165
   Pelechano Nuria., 2008, Virtual Crowds: Methods, Simulation, and Control
   Peters C, 2009, IEEE COMPUT GRAPH, V29, P54, DOI 10.1109/MCG.2009.69
   Qiu FS, 2010, SIMUL MODEL PRACT TH, V18, P190, DOI 10.1016/j.simpat.2009.10.005
   Rojas F.A., 2013, P COMP AN SOC AG CAS
   Rojas F. A., 2013, P 12 ACM SIGGRAPH IN, P31, DOI DOI 10.1145/2534329.2534336
   Scheflen A., 1976, Human territories: How we behave in space-time
   THALMANN D., 2012, Crowd Simulation, V2nd
NR 27
TC 6
Z9 6
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2016
VL 32
IS 3
BP 335
EP 345
DI 10.1007/s00371-015-1187-7
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DF9FK
UT WOS:000371666200007
DA 2024-07-18
ER

PT J
AU Barut, O
   Haciomeroglu, M
AF Barut, Oner
   Haciomeroglu, Murat
TI Real-time collision-free linear trajectory generation on GPU for crowd
   simulations
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Crowd simulation; Crowd navigation; Ambient crowd
ID COMPONENT; ALGORITHM
AB Crowd simulations are mostly employed to compose a background in the current scene. For such ambient crowds, it might be unnecessary to perform complex steering calculations. In this study, a steering-free crowd simulation which eliminates the computational cost arising from expensive steering maneuvers is presented. For this purpose, agents are assigned a linear trajectory that is guaranteed to be collision free before entering the simulation. These trajectories are calculated using readily available rendering pipeline of the GPU. To that end, existing agents' bounding discs are rendered in a spatio-temporal manner as each one forms a straight 3D tube and a projection from a selected initial position is captured. Using the blank areas (holes) in this image, it is possible to determine a suitable constant velocity (a goal position and a speed). In experiments, we not only assess three different methods to choose one of the candidate solutions, but also compare our approach with an existing work. Test results reveal that our technique gives better results in both populating an empty environment with agents quicker and reaching a higher maximum number of agents than the existing method.
C1 [Barut, Oner] Hacettepe Univ, Dept Comp Engn, Ankara, Turkey.
   [Haciomeroglu, Murat] Gazi Univ, Dept Comp Engn, Ankara, Turkey.
C3 Hacettepe University; Gazi University
RP Barut, O (corresponding author), Hacettepe Univ, Dept Comp Engn, Ankara, Turkey.
EM onerbarut@cs.hacettepe.edu.tr
RI Barut, Oner/K-3319-2018
OI Barut, Oner/0000-0003-3442-1586
CR Ahn J, 2006, COMPUT ANIMAT VIRT W, V17, P155, DOI 10.1002/cav.119
   [Anonymous], 2003, Level of detail for 3D graphics
   [Anonymous], 2009, P 2009 S INTERACTIVE, DOI DOI 10.1145/1507149.1507184
   [Anonymous], S ROB RES
   [Anonymous], 2015, ATOMIC COUNTER
   Barut O., 2014, CASA 2014
   Chang F, 2004, COMPUT VIS IMAGE UND, V93, P206, DOI 10.1016/j.cviu.2003.09.002
   Cormen Thomas H., 2001, INTRO ALGORITHMS
   Di Stefano L., 1999, Proceedings 10th International Conference on Image Analysis and Processing, P322, DOI 10.1109/ICIAP.1999.797615
   DILLENCOURT MB, 1992, J ACM, V39, P253, DOI 10.1145/128749.128750
   Dobbyn S., 2005, ACM SIGGRAPH 2005 S, P95, DOI DOI 10.1145/1053427.1053443
   Haciomeroglu M, 2013, COMPUT GRAPH-UK, V37, P862, DOI 10.1016/j.cag.2013.05.006
   Haralick R. M., 1981, Real-Time/Parallel Computing. Image Analysis. Proceedings of Part of the Japan-United States Seminar on Research Towards Real-Time Parallel Image Analysis and Recognition, P11
   Hawick KA, 2010, PARALLEL COMPUT, V36, P655, DOI 10.1016/j.parco.2010.07.002
   Kulpa R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024172
   Kuwahara M, 2012, J VISION, V12, DOI 10.1167/12.12.4
   LUMIA R, 1983, COMPUT VISION GRAPH, V22, P287, DOI 10.1016/0734-189X(83)90071-3
   Magnenat-Thalmann N, 2005, VISUAL COMPUT, V21, P997, DOI 10.1007/s00371-005-0363-6
   Narain R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618468
   Pelechano N, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P99
   Reynolds C. W., 1999, P GAM DEV C, P763
   ROSENFELD A, 1970, J ACM, V17, P146, DOI 10.1145/321556.321570
   Soh Y., 2013, INT J SIGNAL PROCESS, V1, P130
   Stava O., 2010, GPU COMPUTING GEMS E, P569
   Suzuki K, 2003, COMPUT VIS IMAGE UND, V89, P1, DOI 10.1016/S1077-3142(02)00030-9
   Treuille A, 2006, ACM T GRAPHIC, V25, P1160, DOI 10.1145/1141911.1142008
   Wu KS, 2009, PATTERN ANAL APPL, V12, P117, DOI 10.1007/s10044-008-0109-y
NR 27
TC 5
Z9 5
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 843
EP 852
DI 10.1007/s00371-015-1105-z
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500010
DA 2024-07-18
ER

PT J
AU Kaya, Y
   Kayci, L
AF Kaya, Yilmaz
   Kayci, Lokman
TI Application of artificial neural network for automatic detection of
   butterfly species using color and texture features
SO VISUAL COMPUTER
LA English
DT Article
DE Butterfly identification; Expert system; GLCM; LBP; Artificial neural
   network; Texture analysis
ID IMAGE CLASSIFICATION
AB Butterflies can be classified by their outer morphological qualities, genital characteristics that can be obtained using various chemical substances and methods which are carried out manually by preparing genital slides through some certain processes or molecular techniques which is a very expensive method. In this study, a new method which is based on artificial neural networks (ANN) and an image processing technique was used for identification of butterfly species as an alternative to conventional diagnostic methods. Five texture and three color features obtained from 140 butterfly images were used for identification of species. Texture features were obtained by using the average of gray level co-occurrence matrix (GLCM) with different angles and distances. The accuracy of the purposed butterfly classification method has reached 92.85 %. These findings suggested that the texture and color features can be useful for identification of butterfly species.
C1 [Kaya, Yilmaz] Siirt Univ, Fac Engn & Architecture, TR-56100 Siirt, Turkey.
   [Kayci, Lokman] Siirt Univ, Fac Sci & Art, Dept Biol, TR-56100 Siirt, Turkey.
C3 Siirt University; Siirt University
RP Kaya, Y (corresponding author), Siirt Univ, Fac Engn & Architecture, TR-56100 Siirt, Turkey.
EM yilmazkaya1977@gmail.com; kaycilokman@gmail.com
RI KAYA, Yılmaz/C-3822-2017
OI kayci, lokman/0000-0003-4372-5717
CR Bishop C. M., 1995, NEURAL NETWORKS PATT
   Bucinski A, 2007, REP PRACT ONCOL RADI, V12, P9, DOI 10.1016/S1507-1367(10)60036-3
   Carbonell F., 1998, Linneana Belgica, V16, P263
   Carbonell F., 1993, Linneana Belgica, V14, P89
   Dutta S, 2012, PRECIS ENG, V36, P458, DOI 10.1016/j.precisioneng.2012.02.004
   Fausett L., 1994, FUNDAMENTALS NEURAL
   Guo BF, 2008, PATTERN RECOGN, V41, P1653, DOI 10.1016/j.patcog.2007.11.007
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Hebert PDN, 2005, SYST BIOL, V54, P852, DOI 10.1080/10635150500354886
   Hesselbarth G, 1995, TAGFALTER TURKEI
   Huang KY, 2007, COMPUT ELECTRON AGR, V57, P3, DOI 10.1016/j.compag.2007.01.015
   Kaya Y, 2010, WORLD APPL SCI J, V9, P561
   Kayci L, 2007, PRIAMUS S, V6, P1
   Liu GH, 2008, PATTERN RECOGN, V41, P3521, DOI 10.1016/j.patcog.2008.06.010
   Manivannan K, 2012, J HAZARD MATER, V223, P94, DOI 10.1016/j.jhazmat.2012.04.056
   Moshou D, 2004, COMPUT ELECTRON AGR, V44, P173, DOI 10.1016/j.compag.2004.04.003
   Ondimu SN, 2008, COMPUT ELECTRON AGR, V63, P2, DOI 10.1016/j.compag.2008.01.007
   Park SB, 2004, PATTERN RECOGN LETT, V25, P287, DOI 10.1016/j.patrec.2003.10.015
   Schmid C., 2005, Handbook of Pattern Recognition and Computer Vision, VThird
   Shi YJ, 2012, PROCEDIA ENGINEER, V29, P3281, DOI 10.1016/j.proeng.2012.01.480
   Skala Pavel, 2003, Linneana Belgica, V19, P41
   SKLANSKY J, 1978, IEEE T SYST MAN CYB, V8, P237, DOI 10.1109/TSMC.1978.4309944
   Tolman T., 1997, Butterflies of Britain and Europe, P320
   Wang J., 2011, BIOSYST ENG, V3, P24
   Wen CL, 2009, BIOSYST ENG, V104, P299, DOI 10.1016/j.biosystemseng.2009.07.002
   Xian GM, 2010, EXPERT SYST APPL, V37, P6737, DOI 10.1016/j.eswa.2010.02.067
   Yao Q, 2012, J INTEGR AGR, V11, P978, DOI 10.1016/S2095-3119(12)60089-6
NR 27
TC 59
Z9 62
U1 0
U2 38
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2014
VL 30
IS 1
BP 71
EP 79
DI 10.1007/s00371-013-0782-8
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 291AR
UT WOS:000329800500006
DA 2024-07-18
ER

PT J
AU Liu, X
   Shi, ZP
   Shi, ZZ
AF Liu, Xi
   Shi, Zhi-Ping
   Shi, Zhong-Zhi
TI A co-boost framework for learning object categories from Google Images
   with 1st and 2nd order features
SO VISUAL COMPUTER
LA English
DT Article
DE CoBoost learning; Co-training; 1st and 2nd order features; Google Images
ID RECOGNITION
AB Conventional object recognition techniques rely heavily on manually annotated image datasets to achieve good performances. However, collecting high quality datasets is really laborious. The image search engines such as Google Images seem to provide quantities of object images. Unfortunately, a large portion of the search images are irrelevant. In this paper, we propose a semi-supervised framework for learning visual categories from Google Images. We exploit a co-training algorithm, the CoBoost algorithm, and integrate it with two kinds of features, the 1st and 2nd order features, which define bag of words representation and spatial relationship between local features, respectively. We create two boosting classifiers based on the 1st and 2nd order features in the training, during which one classifier provides labels for the other. The 2nd order features are generated dynamically rather than extracted exhaustively to avoid high computation. An active learning technique is also introduced to further improve the performance. Experimental results show that the object models learned from Google Images by our method are competitive with the state-of-the-art unsupervised approaches and some supervised techniques on the standard benchmark datasets.
C1 [Liu, Xi; Shi, Zhi-Ping; Shi, Zhong-Zhi] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS
RP Liu, X (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM xiliu1985@gmail.com; shizp@ics.ict.ac.cn; shizz@ics.ict.ac.cn
FU National Basic Research Priorities Programme [2007CB311004]; National
   Science and Technology Support Plan [2006BAC08B06]; National Science
   Foundation of China [60775035, 60903141, 60933004, 60970088]
FX This work is supported by the National Basic Research Priorities
   Programme (No. 2007CB311004), National Science and Technology Support
   Plan (No. 2006BAC08B06), and National Science Foundation of China (No.
   60775035, No. 60903141, No. 60933004, No. 60970088).
CR [Anonymous], 2008, 2008 2 INT S SYSTEMS
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Bennett KP, 1999, ADV NEUR IN, V11, P368
   Berg TamaraL., 2006, CVPR, DOI DOI 10.1109/CVPR.2006.57
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Chen T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618470
   Cohen I, 2004, IEEE T PATTERN ANAL, V26, P1553, DOI 10.1109/TPAMI.2004.127
   Collins M., 1999, P JOINT SIGDAT C EMP, P189
   Fergus R, 2005, IEEE I CONF COMP VIS, P1816
   Fergus R, 2004, LECT NOTES COMPUT SC, V3021, P242
   Fergus R, 2003, PROC CVPR IEEE, P264
   Fergus R, 2010, P IEEE, V98, P1453, DOI 10.1109/JPROC.2010.2048990
   Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P148
   Gang Wang., 2008, Computer Vision and Pattern Recognition, IEEE Computer Society Conference on, P1
   Lafferty J. D., 2003, P INT C MACH LEARN, P912, DOI DOI 10.5555/3041838.3041953
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Leibe Bastian, 2004, WORKSH STAT LEARN CO
   Leistner C., 2008, IEEE COMPUTER VISION, P1, DOI DOI 10.1109/CVPR.2008.4587629
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li LJ, 2010, INT J COMPUT VISION, V88, P147, DOI 10.1007/s11263-009-0265-6
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Nowak E, 2006, LECT NOTES COMPUT SC, V3954, P490
   Opelt A, 2004, LECT NOTES COMPUT SC, V3022, P71
   Savarese S., 2006, P 2006 IEEE COMPUTER, V2, P2033, DOI DOI 10.1109/CVPR.2006.102
   Schroff F, 2007, IEEE I CONF COMP VIS, P2120
   Shen LL, 2006, PATTERN RECOGN LETT, V27, P1758, DOI 10.1016/j.patrec.2006.02.005
   Sivic J, 2005, IEEE I CONF COMP VIS, P370
   Vijayanarasimhan S., 2008, Proc. CVPR, P1, DOI DOI 10.1109/CVPR.2008.4587632
   Wang G, 2009, PROC CVPR IEEE, P1367, DOI 10.1109/CVPRW.2009.5206816
   Wang J, 2009, PROC CVPR IEEE, P1390, DOI 10.1109/CVPRW.2009.5206729
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zhou ZH, 2004, LECT NOTES COMPUT SC, V3201, P525
   Zhu Xiaojin Jerry, 2005, TECHNICAL REPORT
NR 36
TC 1
Z9 1
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2014
VL 30
IS 1
BP 5
EP 17
DI 10.1007/s00371-012-0772-2
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 291AR
UT WOS:000329800500001
DA 2024-07-18
ER

PT J
AU Lai, DB
   Sourin, A
AF Lai, Danbo
   Sourin, Alexei
TI Interactive free-form shape modeling in cyberworlds
SO VISUAL COMPUTER
LA English
DT Article
DE Shape modeling; Implicit functions; Web visualization
ID REPRESENTATION
AB When procedural models based on implicit functions are used for defining complex shapes, the final model may become slow for rendering. We propose an algorithm for accelerating such rendering for free-form shape modeling where some initial shape is gradually modified by other implicitly-defined shapes with relatively smaller sizes compared to the final shape. The algorithm then adds additional functions to the final function script, which makes the rendering of the whole shape faster. The resulting accelerated function scripts can be then rendered on any suitable rendering platform that we illustrate by using function-based extension of VRML/X3D and POV-Ray.
C1 [Lai, Danbo; Sourin, Alexei] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Sourin, A (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
EM la0001bo@e.ntu.edu.sg; assourin@ntu.edu.sg
RI Sourin, Alexei/A-3701-2011
OI Sourin, Alexei/0000-0003-4051-2927
FU Ministry of Education of Singapore [MOE2011-T2-1-006]; Russian
   Foundation for Basic Research [12-07-00157-a]; Institute for Media
   Innovation, Nanyang Technological University, Singapore
FX This project is supported by the Ministry of Education of Singapore
   Grant MOE2011-T2-1-006 "Collaborative Haptic Modeling for Orthopaedic
   Surgery Training in Cyberspace" and partially by the Russian Foundation
   for Basic Research grant 12-07-00157-a "Virtual Modeling of Minimally
   Invasive Surgery Operations." The authors would also like to acknowledge
   the Ph.D. grant from the Institute for Media Innovation, Nanyang
   Technological University, Singapore.
CR BAJAJ CL, 1995, ACM T GRAPHIC, V14, P103, DOI 10.1145/221659.221662
   Barbier A., 2004, J WSCG, V12, P35
   Cani MP, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P82, DOI 10.1109/SMA.2001.923378
   Ferley E, 2001, GRAPH MODELS, V63, P459, DOI 10.1006/gmod.2001.0558
   Ferley E, 2000, VISUAL COMPUT, V16, P469, DOI 10.1007/PL00007216
   Frisken SF, 2000, COMP GRAPH, P249, DOI 10.1145/344779.344899
   Galin E, 2000, GRAPH MODELS, V62, P19, DOI 10.1006/gmod.1999.0514
   GALYEAN TA, 1991, COMP GRAPH, V25, P267, DOI 10.1145/127719.122747
   Kalra D., 1989, P SIGGRAPH 89, P297
   Lai D., 2011, P 10 ACM SIGGRAPH IN, P519
   Levinski K, 2007, COMPUT GRAPH-UK, V31, P66, DOI 10.1016/j.cag.2006.09.008
   Liu Q, 2006, COMPUT GRAPH-UK, V30, P629, DOI 10.1016/j.cag.2006.03.006
   PASKO A, 1995, VISUAL COMPUT, V11, P429, DOI 10.1007/BF02464333
   Sourin A, 2001, VISUAL COMPUT, V17, P258, DOI 10.1007/s003710100109
   Wei L, 2011, VISUAL COMPUT, V27, P321, DOI 10.1007/s00371-011-0548-0
   Wyvill B, 1999, COMPUT GRAPH FORUM, V18, P149, DOI 10.1111/1467-8659.00365
NR 16
TC 1
Z9 1
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2013
VL 29
IS 10
SI SI
BP 1027
EP 1037
DI 10.1007/s00371-013-0835-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 227LV
UT WOS:000325115400005
DA 2024-07-18
ER

PT J
AU Oshita, M
   Seki, T
   Yamanaka, R
   Nakatsuka, Y
   Iwatsuki, M
AF Oshita, Masaki
   Seki, Takeshi
   Yamanaka, Reiko
   Nakatsuka, Yukiko
   Iwatsuki, Masami
TI Easy-to-use authoring system for Noh (Japanese traditional) dance
   animation and its evaluation
SO VISUAL COMPUTER
LA English
DT Article
DE Animation system; Traditional dance; Motion synthesis; Motion
   composition; Motion capture
AB Noh is a genre of Japanese traditional theater, a kind of musical drama. Similar to other dance forms, Noh dance (shimai) can also be divided into small, discrete units of motion (shosa). Therefore, if we have a set of motion clips of motion units (shosa), we can synthesize Noh dance animation by composing them in a sequence based on the Noh dance notation (katatsuke). However, it is difficult for researchers and learners of Noh dance to utilize existing animation systems to create such animations. The purpose of this research is to develop an easy-to-use authoring system for Noh dance animation. In this paper, we introduce the design, implementation, and evaluation of our system. To solve the problems of existing animation systems, we employ our smart motion synthesis technique to compose motion units automatically. We improved the motion synthesis method by enhancing the algorithms for detecting body orientation and constraints between the foot and ground to handle Noh dance motions correctly. We classify motion units as either pattern units, which are specific forms of motion, represented as shot motion clips, or locomotion units, generated on the fly to denote movement towards a specific position or direction. To handle locomotion-type motion units, we implemented a module to generate walking motion based on a given path. We created several Noh dance animations using this system, which was evaluated through a series of experiments. We also conducted a user test to determine the usefulness of our system for learners of Noh dance.
C1 [Oshita, Masaki] Kyushu Inst Technol, Iizuka, Fukuoka 8208502, Japan.
   [Seki, Takeshi; Yamanaka, Reiko; Nakatsuka, Yukiko; Iwatsuki, Masami] Hosei Univ, Chiyoda Ku, Tokyo 1028160, Japan.
C3 Kyushu Institute of Technology; Hosei University
RP Oshita, M (corresponding author), Kyushu Inst Technol, 680-4 Kawazu, Iizuka, Fukuoka 8208502, Japan.
EM oshita@ces.kyutech.ac.jp; takeshi.seki.67@adm.hosei.ac.jp;
   yamanaka@hosei.ac.jp; yukiko.nakatsuka.58@hosei.ac.jp;
   iwatsuki@hosei.ac.jp
FU Program for Promoting Methodological Innovation in Humanities and Social
   Sciences by Cross-Disciplinary Fusing; Japan Society for the Promotion
   of Science (JSPS) [24500238]; Grants-in-Aid for Scientific Research
   [24500238] Funding Source: KAKEN
FX This work was supported in part by the Program for Promoting
   Methodological Innovation in Humanities and Social Sciences by
   Cross-Disciplinary Fusing as well as Grants-in-Aid for Scientific
   Research (24500238) from the Japan Society for the Promotion of Science
   (JSPS). We would like to thank Masaki Umano, a professional Noh
   performer, for his invaluable help in acquiring motion capture data and
   evaluating our prototype system. We would also like to thank Steven G.
   Nelson for his insightful comments during the writing of this paper.
CR [Anonymous], 2006, REV NATL CTR DIGITIZ
   Bethe Monica, 1982, Dance Analysis, V1
   Bethe Monica, 1982, DANCE NO THEATER, V2
   Bethe Monica, 1982, DANCE NO THEATER, V3
   Brazell Karen., 1998, TRADITIONAL JAPANESE
   Calvert T, 2005, IEEE COMPUT GRAPH, V25, P6, DOI 10.1109/MCG.2005.33
   Choensawat Worawat, 2011, P INT C CULT COMP 20, P167
   Guest Ann Hutchinson, 1987, LABANOTATION SYSTEM
   Ikemoto L, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P145
   Leiter Samuel., 2006, HIST DICT JAPANESE T
   Lockwood N, 2011, P 2011 ACM SIGGRAPH, P267
   M S., 2004, SCA'04: Proceedings of the 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation, P325
   Majkowska A, 2006, P 2006 ACM SIGGRAPH, P309, DOI DOI 10.1145/1218064.1218106
   Oshita M, 2005, COMPUT GRAPH-UK, V29, P931, DOI 10.1016/j.cag.2005.09.010
   Oshita M, 2012, PROCEEDINGS OF THE 2012 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P45, DOI 10.1109/CW.2012.14
   Oshita M, 2008, COMPUT GRAPH FORUM, V27, P1909, DOI 10.1111/j.1467-8659.2008.01339.x
   ROSE C, 1995, P SIGGRAPH 95, P147
   Sang I. P., 2002, P ACM SIGGRAPH S COM, P105
   Shiratori T, 2006, COMPUT GRAPH FORUM, V25, P449, DOI 10.1111/j.1467-8659.2006.00964.x
   Shum HPH, 2009, COMPUT ANIMAT VIRT W, V20, P385, DOI 10.1002/cav.315
   Soga A, 2007, VISUAL COMPUT, V23, P309, DOI 10.1007/s00371-007-0110-2
   Soga A, 2009, 2009 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P171, DOI 10.1109/CW.2009.37
   Wang J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330512
   Yokomichi Morio, 1992, IWANAMI LECT SERIES
   Zhang Mingmin, 2011, P INT C VIRT REAL CO, P327
NR 25
TC 4
Z9 4
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2013
VL 29
IS 10
SI SI
BP 1077
EP 1091
DI 10.1007/s00371-013-0839-8
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 227LV
UT WOS:000325115400009
DA 2024-07-18
ER

PT J
AU Fadaifard, H
   Wolberg, G
AF Fadaifard, Hadi
   Wolberg, George
TI Image warping for retargeting garments among arbitrary poses
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Shape deformation; Image warping; Moving least squares;
   As-rigid-as-possible shape manipulation
AB We address the problem of warping 2D images of garments onto target mannequins of arbitrary poses. The motivation for this work is to enable an online shopper to drag and drop selected articles of clothing onto a single mannequin to configure and visualize outfits. Such a capability requires each garment to be available in a pose that is consistent with the target mannequin. A 2D deformation system is proposed, which enables a designer to quickly deform images of clothing onto a target shape with both fine and coarse controls over the deformation. This system has retargeted thousands of images for retailers to establish virtual dressing rooms for their online customers.
C1 [Fadaifard, Hadi] Brainstorm Technol LLC, New York, NY USA.
   [Wolberg, George] CUNY, City Coll New York, New York, NY 10021 USA.
C3 City University of New York (CUNY) System; City College of New York
   (CUNY)
RP Wolberg, G (corresponding author), CUNY, City Coll New York, New York, NY 10021 USA.
EM fadaifard@brainstormLLC.com; wolberg@cs.ccny.cuny.edu
CR Alexa M., 2000, SIGGRAPH 00
   [Anonymous], ACM T GRAPH
   Cuno A., 2007, Proceedings of the 27th Computer Graphics International Conference, P115
   Igarashi T, 2005, ACM T GRAPHIC, V24, P1134, DOI 10.1145/1073204.1073323
   Karni Z., 2009, SGP 09
   Levin D, 1998, MATH COMPUT, V67, P1517, DOI 10.1090/S0025-5718-98-00974-0
   Magnenat-Thalmann N, 2011, COMPUTER AIDED DESIG, V8, P163
   MCLAIN DH, 1974, COMPUT J, V17, P318, DOI 10.1093/comjnl/17.4.318
   Mokhtarian F., 2003, CURVATURE SCALE SPAC
   Schaefer S, 2006, ACM T GRAPHIC, V25, P533, DOI 10.1145/1141911.1141920
   Sorkine O., 2007, SGP 07
   Weber O, 2009, COMPUT GRAPH FORUM, V28, P587, DOI 10.1111/j.1467-8659.2009.01399.x
   Weng YL, 2006, VISUAL COMPUT, V22, P653, DOI 10.1007/s00371-006-0054-y
NR 13
TC 3
Z9 3
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 525
EP 534
DI 10.1007/s00371-013-0816-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400007
DA 2024-07-18
ER

PT J
AU Yuksel, KA
   Yucebilgin, A
   Balcisoy, S
   Ercil, A
AF Yuksel, Kamer Ali
   Yucebilgin, Alp
   Balcisoy, Selim
   Ercil, Aytul
TI Real-time feature-based image morphing for memory-efficient impostor
   rendering and animation on GPU
SO VISUAL COMPUTER
LA English
DT Article
DE Crowd simulation; Impostor rendering; Image and view morphing; Graphical
   processing unit
ID VIRTUAL HUMANS
AB Real-time rendering of large animated crowds consisting of thousands of virtual humans is important for several applications including simulations, games, and interactive walkthroughs but cannot be performed using complex polygonal models at interactive frame rates. For that reason, methods using large numbers of precomputed image-based representations, called impostors, have been proposed. These methods take advantage of existing programmable graphics hardware to compensate for computational expense while maintaining visual fidelity. Thanks to these methods, the number of different virtual humans rendered in real time is no longer restricted by computational power but by texture memory consumed for the variety and discretization of their animations. This work proposes a resource-efficient impostor rendering methodology that employs image morphing techniques to reduce memory consumption while preserving perceptual quality, thus allowing higher diversity or resolution of the rendered crowds. Results of the experiments indicated that the proposed method, in comparison with conventional impostor rendering techniques, can obtain 38 % smoother animations or 87 % better appearance quality by reducing the number of key-frames required for preserving the animation quality via resynthesizing them with up to 92 % similarity on real time.
C1 [Yuksel, Kamer Ali; Balcisoy, Selim; Ercil, Aytul] Sabanci Univ, Fac Engn & Nat Sci, Istanbul, Turkey.
   [Yucebilgin, Alp] Univ Calif Santa Barbara, Dept Comp Sci, Eyes Lab 4, Santa Barbara, CA 93106 USA.
   [Yuksel, Kamer Ali] Sabanci Univ, Comp Sci & Engn Program, Istanbul, Turkey.
   [Yucebilgin, Alp] Univ Calif Santa Barbara, Comp Sci Program, Santa Barbara, CA 93106 USA.
C3 Sabanci University; University of California System; University of
   California Santa Barbara; Sabanci University; University of California
   System; University of California Santa Barbara
RP Yuksel, KA (corresponding author), Sabanci Univ, Fac Engn & Nat Sci, Istanbul, Turkey.
EM kamer@sabanciuniv.edu; alp@cs.ucsb.edu; balcisoy@sabanciuniv.edu;
   aytulercil@sabanciuniv.edu
RI , Selim/Y-3196-2019
OI Balcisoy, Selim/0000-0002-6495-7341
FU TUBITAK under VIPSAFE [109E134]
FX This work has been partially supported by TUBITAK under project 109E134,
   VIPSAFE.
CR Aubel A, 2000, IEEE T CIRC SYST VID, V10, P207, DOI 10.1109/76.825720
   BEIER T, 1992, COMP GRAPH, V26, P35, DOI 10.1145/142920.134003
   Dobbyn S., 2005, ACM SIGGRAPH 2005 S, P95, DOI DOI 10.1145/1053427.1053443
   Gao PS, 1998, VISUAL COMPUT, V14, P390, DOI 10.1007/s003710050150
   Hamill J, 2005, COMPUT GRAPH FORUM, V24, P623, DOI 10.1111/j.1467-8659.2005.00887.x
   Karungaru S, 2007, INT J INNOV COMPUT I, V3, P247
   Kavan L, 2008, I3D 2008: SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P149
   Lister W., 2010, COMPUT GRAPH FORUM
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Millan E., 2006, P 4 INT C COMPUTER G, P49, DOI DOI 10.1145/1174429.1174436
   Reynolds C.W., 2006, SIGGRAPH symposium on Videogames, P113, DOI [DOI 10.1145/1183316.1183333, 10.1145/1183316.1183333]
   Schaufler G, 1996, COMPUT GRAPH FORUM, V15, pC227, DOI 10.1111/1467-8659.1530227
   Seitz S. M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P21, DOI 10.1145/237170.237196
   Shade J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P75, DOI 10.1145/237170.237209
   Stich T, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/1870076.1870079
   Tecchia F, 2002, COMPUT GRAPH FORUM, V21, P753, DOI 10.1111/1467-8659.00633
   Tecchia F, 2000, SPRING COMP SCI, P83
   Thalmann D, 2009, 2009 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P1, DOI 10.1109/CW.2009.23
   Treuille A, 2006, ACM T GRAPHIC, V25, P1160, DOI 10.1145/1141911.1142008
   Wong T.Y., 2005, DIG IM COMP TECHN AP, P19
   Yuksel K.A., 2001, INT C CYBERWORLDS CW, P197
   Zamith M., 2009, 16 INT C SYST SIGN I, P1
NR 22
TC 1
Z9 1
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2013
VL 29
IS 2
SI SI
BP 131
EP 140
DI 10.1007/s00371-012-0718-8
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 072FM
UT WOS:000313654700005
DA 2024-07-18
ER

PT J
AU Sfikas, K
   Theoharis, T
   Pratikakis, I
AF Sfikas, Konstantinos
   Theoharis, Theoharis
   Pratikakis, Ioannis
TI Non-rigid 3D object retrieval using topological information guided by
   conformal factors
SO VISUAL COMPUTER
LA English
DT Article
DE 3D object retrieval; Non-rigid 3D objects; Graph matching
ID SHAPE RETRIEVAL; REEB GRAPHS; SURFACES
AB Combining the properties of conformal geometry and graph-based topological information, a non-rigid 3D object retrieval methodology is proposed, which is both robust and efficient in terms of retrieval accuracy and computation speed. While graph-based methods are robust to non-rigid object deformations, they require intensive computation which can be reduced by the use of appropriate representations, addressed through geometry-based methods. In this respect, we present a 3D object retrieval methodology, which combines the above advantages in a unified manner. Furthermore, we propose a string matching strategy for the comparison of graphs which describe 3D objects.
C1 [Sfikas, Konstantinos; Theoharis, Theoharis] Univ Athens, Comp Graph Lab, Dept Informat & Telecommun, Athens, Greece.
   [Theoharis, Theoharis] NTNU, IDI, Trondheim, Norway.
   [Pratikakis, Ioannis] Democritus Univ Thrace, Dept Elect & Comp Engn, GR-67100 Xanthi, Greece.
C3 National & Kapodistrian University of Athens; Norwegian University of
   Science & Technology (NTNU); Democritus University of Thrace
RP Sfikas, K (corresponding author), Univ Athens, Comp Graph Lab, Dept Informat & Telecommun, Athens, Greece.
EM ksfikas@di.uoa.gr; theotheo@di.uoa.gr; ipratika@ee.duth.gr
RI Theoharis, Theoharis/AAN-2555-2020; PRATIKAKIS, IOANNIS/AAD-3387-2019
OI PRATIKAKIS, IOANNIS/0000-0002-4124-3688; Sfikas,
   Konstantinos/0000-0002-9173-4557
FU European Union (European Social Fund-ESF); Greek national funds through
   the Operational Program "Education and Lifelong Learning" of the
   National Strategic Reference Framework (NSRF)-Research Funding Program:
   THALIS
FX This research has been co-financed by the European Union (European
   Social Fund-ESF) and Greek national funds through the Operational
   Program "Education and Lifelong Learning" of the National Strategic
   Reference Framework (NSRF)-Research Funding Program: THALIS.
CR Agathos A, 2010, VISUAL COMPUT, V26, P1301, DOI 10.1007/s00371-010-0523-1
   [Anonymous], IEEE COMP SOC C COMP
   [Anonymous], P IEEE SHAP MOD INT
   [Anonymous], 3D SHAPE BASED RETRI
   [Anonymous], EUR WORKSH 3D OBJ RE, DOI DOI 10.2312/3DOR/3DOR08/009-016
   [Anonymous], COMPUTER VISION MAT
   [Anonymous], 3DOR 11
   [Anonymous], 2011, PROC EUROGRAPHICS 20, P79, DOI DOI 10.2312/3DOR/3DOR11/079-088
   BECKMAN FS, 1953, P AM MATH SOC, V4, P810, DOI 10.2307/2032415
   Ben Hamza A, 2003, LECT NOTES COMPUT SC, V2886, P378
   Ben-Chen M., 2008, Proceedings of the 1st Eurographics Conference on 3D Object Retrieval, P1
   Biasotti S, 2008, THEOR COMPUT SCI, V392, P5, DOI 10.1016/j.tcs.2007.10.018
   Biasotti S, 2010, COMPUT GRAPH-UK, V34, P252, DOI 10.1016/j.cag.2010.03.013
   Bronstein AM, 2006, SIAM J SCI COMPUT, V28, P1812, DOI 10.1137/050639296
   Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1
   Bronstein AM, 2010, INT J COMPUT VISION, V89, P266, DOI 10.1007/s11263-009-0301-6
   Carlsson G., 2004, P 2004 EUR ACM SIGGR, P124, DOI DOI 10.1145/1057432.1057449
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Cole-McLaughlin K, 2004, DISCRETE COMPUT GEOM, V32, P231, DOI 10.1007/s00454-004-1122-6
   de Berg M., 1993, ESA '93: Proceedings of the First Annual European Symposium on Algorithms, P121
   Del Bimbo A, 2006, ACM T MULTIM COMPUT, V2, P20
   Dey TK, 2003, LECT NOTES COMPUT SC, V2748, P25
   Elad A, 2003, IEEE T PATTERN ANAL, V25, P1285, DOI 10.1109/TPAMI.2003.1233902
   FLOYD RW, 1962, COMMUN ACM, V5, P345, DOI 10.1145/367766.368168
   Gal R, 2007, IEEE T VIS COMPUT GR, V13, P261, DOI 10.1109/TVCG.2007.45
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Jain V, 2007, COMPUT AIDED DESIGN, V39, P398, DOI 10.1016/j.cad.2007.02.009
   Järvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418
   Kazhdan M., 2003, Symposium on Geometry Processing, P156
   Kim DH, 2004, LECT NOTES COMPUT SC, V3332, P238
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Lian Z., 2010, Eurographics Workshop on 3D Object Retrieval, V10, P101, DOI [10.2312/3DOR/3DOR10/101-108, 10.1109/CVPR.2014.491, DOI 10.2312/3DOR/3DOR10/101-108]
   Mademlis A, 2009, PATTERN RECOGN, V42, P2447, DOI 10.1016/j.patcog.2009.04.024
   MEYER M., 2002, P VISMATH C, P1
   Papadakis P, 2010, INT J COMPUT VISION, V89, P177, DOI 10.1007/s11263-009-0281-6
   Peyré G, 2006, INT J COMPUT VISION, V69, P145, DOI 10.1007/s11263-006-6859-3
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Rustamov Raif M, 2007, P S GEOM PROC, V257, P225
   Sebastian T, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P755, DOI 10.1109/ICCV.2001.937602
   Sfikas K, 2011, INT J COMPUT VISION, V91, P262, DOI 10.1007/s11263-010-0395-x
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   SHINAGAWA Y, 1991, IEEE COMPUT GRAPH, V11, P66, DOI 10.1109/38.90568
   Siddiqi K, 1999, INT J COMPUT VISION, V35, P13, DOI 10.1023/A:1008102926703
   Sundar H, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P130, DOI 10.1109/smi.2003.1199609
   Takahashi S., 1997, Proceedings. Fourth Symposium on Solid Modeling and Applications, P97, DOI 10.1145/267734.267760
   Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0
   Tarasov S. P., 1998, Proceedings of the Fourteenth Annual Symposium on Computational Geometry, P68, DOI 10.1145/276884.276892
   Tierny J, 2009, COMPUT GRAPH FORUM, V28, P41, DOI 10.1111/j.1467-8659.2008.01190.x
   Tung T., 2005, International Journal of Shape Modeling, V11, P91, DOI 10.1142/S0218654305000748
   Xiang P, 2007, LECT NOTES COMPUT SC, V4488, P25
   Zhang J, 2005, LECT NOTES COMPUT SC, V3757, P285, DOI 10.1007/11585978_19
NR 51
TC 21
Z9 25
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2012
VL 28
IS 9
BP 943
EP 955
DI 10.1007/s00371-012-0714-z
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 985QX
UT WOS:000307284700005
DA 2024-07-18
ER

PT J
AU Vidal, V
   Wolf, C
   Dupont, F
AF Vidal, Vincent
   Wolf, Christian
   Dupont, Florent
TI Combinatorial mesh optimization
SO VISUAL COMPUTER
LA English
DT Article
DE Triangular meshes; Mesh optimization; Discrete optimization; Graph cuts
ID ENERGY MINIMIZATION
AB A new mesh optimization framework for 3D triangular surface meshes is presented, which formulates the task as an energy minimization problem in the same spirit as in Hoppe et al. (SIGGRAPH'93: Proceedings of the 20th Annual Conference on Computer Graphics and Interactive Techniques, 1993). The desired mesh properties are controlled through a global energy function including data attached terms measuring the fidelity to the original mesh, shape potentials favoring high quality triangles, and connectivity as well as budget terms controlling the sampling density. The optimization algorithm modifies mesh connectivity as well as the vertex positions. Solutions for the vertex repositioning step are obtained by a discrete graph cut algorithm examining global combinations of local candidates.
   Results on various 3D meshes compare favorably to recent state-of-the-art algorithms. Applications consist in optimizing triangular meshes and in simplifying meshes, while maintaining high mesh quality. Targeted areas are the improvement of the accuracy of numerical simulations, the convergence of numerical schemes, improvements of mesh rendering (normal field smoothness) or improvements of the geometric prediction in mesh compression techniques.
C1 [Vidal, Vincent; Wolf, Christian] Univ Lyon, CNRS, UMR 5205, INSA Lyon,LIRIS, F-69621 Villeurbanne, France.
   [Dupont, Florent] Univ Lyon 1, CNRS, UMR 5205, LIRIS, F-69622 Villeurbanne, France.
C3 Centre National de la Recherche Scientifique (CNRS); Institut National
   des Sciences Appliquees de Lyon - INSA Lyon; Centre National de la
   Recherche Scientifique (CNRS); Institut National des Sciences Appliquees
   de Lyon - INSA Lyon; Universite Claude Bernard Lyon 1
RP Wolf, C (corresponding author), Univ Lyon, CNRS, UMR 5205, INSA Lyon,LIRIS, F-69621 Villeurbanne, France.
EM vincent.vidal@liris.cnrs.fr; christian.wolf@liris.cnrs.fr;
   florent.dupont@liris.cnrs.fr
OI Dupont, Florent/0000-0001-6611-4420
FU French National Research Agency (ANR) through the MDCO [ANR-07-MDCO-015]
FX This work has been supported by the French National Research Agency
   (ANR) through the MDCO program (project MADRAS No. ANR-07-MDCO-015).
CR Alliez P, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P49
   Alliez P., 2008, RECENT ADV REMESHING
   [Anonymous], 2003, PROC 19 ANN S COMPUT, DOI DOI 10.1145/777792.777839
   [Anonymous], 2007, CVPR, DOI DOI 10.1109/CVPR.2007.383203
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   Duda R., 1973, Pattern Classification and Scene Analysis
   GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596
   Guskov I., 2002, PROC S COMP GEOM, P264
   Hoppe H., 1993, SIGGRAPH 93
   Jiao X., 2002, PROC 8 INT C NUMER G, P705
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Kolmogorov V, 2007, IEEE T PATTERN ANAL, V29, P1274, DOI 10.1109/TPAMI.2007.1031
   Lempitsky V., 2008, Proceedings of the 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'08), P1
   Liu LG, 2007, COMPUT AIDED DESIGN, V39, P772, DOI 10.1016/j.cad.2007.03.004
   Nealen A., 2006, P 4 INT C COMP GRAPH, P381, DOI DOI 10.1145/1174429.1174494
   Pearl J., 1988, PROBABILISTIC REASON
   Peyré G, 2006, INT J COMPUT VISION, V69, P145, DOI 10.1007/s11263-006-6859-3
   Pottmann H, 2003, VISUALIZATION MATH, VIII, P223
   Rother C, 2005, PROC CVPR IEEE, P589
   Shewchuk JR, 2002, P 11 INT MESH ROUNDT, P115
   Sifri O., 2003, IMR, P189
   Surazhsky V., 2003, Symposium on Geometry Processing, P20
   SURAZHSKY V., 2003, P 12 INT MESHING ROU, P215
   Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844
   Valette S, 2004, COMPUT GRAPH FORUM, V23, P381, DOI 10.1111/j.1467-8659.2004.00769.x
   Valette S, 2008, IEEE T VIS COMPUT GR, V14, P369, DOI 10.1109/TVCG.2007.70430
   Wang WP, 2006, ACM T GRAPHIC, V25, P214, DOI 10.1145/1138450.1138453
   Winkler T, 2008, VISUAL COMPUT, V24, P775, DOI 10.1007/s00371-008-0259-3
NR 30
TC 11
Z9 11
U1 1
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2012
VL 28
IS 5
BP 511
EP 525
DI 10.1007/s00371-011-0649-9
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 926EJ
UT WOS:000302813800007
DA 2024-07-18
ER

PT J
AU Wang, LL
   Yang, Z
   Ma, ZQ
   Zhao, QP
AF Wang, Lili
   Yang, Zheng
   Ma, Zhiqiang
   Zhao, Qinping
TI Approximating global illumination on mesostructure surfaces with height
   gradient maps
SO VISUAL COMPUTER
LA English
DT Article
DE Real-time rendering; Realistic rendering; Global illumination; Image
   space; Ambient occlusion
AB Rendering global illumination for objects with mesostructure surfaces is a time-consuming task, and cannot presently be applied to interactive graphics. This paper presents a real-time rendering method based on a mesostructure height gradient map (MHGM) to exhibit lighting effects on meso-scale details in dynamic environments. We approximate global illumination using a lighting model including three components: incident ambient light, direct light and single bounce indirect light. MHGM is introduced to create local apex sets, which would help us to compute the three components adaptively. Our approach runs entirely on the graphics hardware, and uses deferred shading and the graphics pipeline to accelerate computation. We achieve high quality results which can render meso-scale details with approximate global illumination even for low-resolution geometric models. Moreover, our approach fully supports dynamic scenes and deformable objects.
C1 [Wang, Lili; Yang, Zheng; Ma, Zhiqiang; Zhao, Qinping] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
C3 Beihang University
RP Wang, LL (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
EM wanglily@buaa.edu.cn
RI wang, lili/HJP-8047-2023; Ma, qiang zhi/HZK-3874-2023
OI Ma, qiang zhi/0000-0003-0302-6604
FU National Natural Science Foundation of China [60842001]; Macao Science
   and Technology Development Fund [043/2009/A2]; Beijing Science
   Technology Star Plans and Technology Star Plans [2009B09]
FX We would like to thank Meng Fankun and Wang Shuo for their help in
   mapping textures on experimental models. This work is the part of
   Project 60842001 supported by National Natural Science Foundation of
   China, Project 043/2009/A2 funded by Macao Science and Technology
   Development Fund, and is also supported by Beijing Science Technology
   Star Plans and Technology Star Plans (No. 2009B09).
CR [Anonymous], 2009, P S INT 3D GRAPH GAM, DOI 10.1145/1507149.1507161.5,7
   Bavoil L., 2008, Tech. rep
   Blinn J., 1978, Proceedings of the 5th annual conference on Computer graphics and interactive techniques-SIGGRAPH'78, V12, P286
   Chen YY, 2004, ACM T GRAPHIC, V23, P343, DOI 10.1145/1015706.1015726
   Cook R. L., 1984, Computers & Graphics, V18, P223
   DIMITROV R, 2008, I3D 08 P 2008 S INT, P1
   Dimitrov R., 2008, ACM SIGGRAPH 2008 TA, P1, DOI DOI 10.1145/1401032.1401061
   Hegeman K., 2006, Proceedings of the 2006 symposium on Interactive 3D graphics and games, P87, DOI [10.1145/1111411.1111427.4, DOI 10.1145/1111411.1111427.4]
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Kaneko T., 2001, Proceedings of ICAT, V2001, P205
   Kontkanen Janne, 2005, P 2005 S INT 3D GRAP, P41, DOI 10.1145/1053427.1053434
   Nowrouzezahrai D., 2009, COMPUT GRAPH FORUM, V21, P665
   Oliveira MM, 2000, COMP GRAPH, P359, DOI 10.1145/344779.344947
   Shanmugam P, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P73
   Sloan PP, 2003, ACM T GRAPHIC, V22, P370, DOI 10.1145/882262.882279
   Snyder J, 2008, COMPUT GRAPH FORUM, V27, P1275, DOI 10.1111/j.1467-8659.2008.01266.x
   Song Y, 2005, VISUAL COMPUT, V21, P774, DOI 10.1007/s00371-005-0320-4
   Suykens F, 2003, COMPUT GRAPH FORUM, V22, P463, DOI 10.1111/1467-8659.00694
   Tatarchuk N., 2006, Proceedings of the 2006 symposium on Interactive 3D graphics and games, P63
   Tong X, 2002, ACM T GRAPHIC, V21, P665, DOI 10.1145/566570.566634
   Wang LF, 2003, ACM T GRAPHIC, V22, P334, DOI 10.1145/882262.882272
   Zhukov S., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P45
NR 22
TC 1
Z9 1
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2012
VL 28
IS 4
BP 329
EP 339
DI 10.1007/s00371-011-0618-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 926EE
UT WOS:000302813300001
DA 2024-07-18
ER

PT J
AU O'Neill, GT
   Lee, WS
   Beaulé, P
AF O'Neill, Gabriel Telles
   Lee, Won-Sook
   Beaule, Paul
TI Segmentation of cam-type femurs from CT scans
SO VISUAL COMPUTER
LA English
DT Article
DE Segmentation; Femur; Femoral-acetabular impingements; Hip; Morphological
   snake
ID ACTIVE CONTOUR MODELS; AUTOMATED SEGMENTATION; SHAPE MODELS; SNAKES
AB We introduce a new way to accurately segment cam-type pathological femurs from pelvic CT scans. The femur is a difficult target for segmentation due to its proximity to the acetabulum, irregular shape and the varying thickness of its hardened outer shell. In addition, the pathological femurs with femoral-acetabular impingements have a non-standard shape, which increases segmentation difficulty. We overcome these difficulties by (a) dividing the femur into two rounds of segmentation-one for the femur head and another for the body-(b) pre-processing the CT scan to reduce anatomical sources of error (c) two modes of segmentation-a rough estimation of a contour and another for fine contours. Segmentations of the CT volume are performed iteratively, on a slice-by-slice basis and contours are extracted using the morphological snake algorithm. Our methodology was designed to require little initialization from the user and to deftly handle the large variation in femur shapes, most notably from deformations attributed to cam femoral-acetabular impingements. Our efforts are to provide physicians with a new tool that creates patient-specific and high-quality 3D femur models while requiring much less time and effort. Femur models segmented with our method had an average volume overlap error of 2.71 +/- 0.44% and symmetric surface distance of 0.28 +/- 0.04 mm compared to ground truth models.
C1 [Lee, Won-Sook; Beaule, Paul] Univ Ottawa, Div Orthopaed Surg, Fac Med, Ottawa, ON K1N 6N5, Canada.
C3 University of Ottawa
RP Lee, WS (corresponding author), Univ Ottawa, Div Orthopaed Surg, Fac Med, 800 King Edward Ave, Ottawa, ON K1N 6N5, Canada.
EM wslee@uottawa.ca
OI Beaule, Paul E./0000-0001-7667-9994
CR Alvarez L., 2010, IEEE C COMP VIS PATT
   Alvarez L.B.-N, 2010, REAL TIME MORPHOLOGI
   Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043
   Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291
   Chen SQ, 2009, IEEE I CONF COMP VIS, P763, DOI 10.1109/ICCV.2009.5459290
   COHEN LD, 1991, CVGIP-IMAG UNDERSTAN, V53, P211, DOI 10.1016/1049-9660(91)90028-N
   COHEN LD, 1993, IEEE T PATTERN ANAL, V15, P1131, DOI 10.1109/34.244675
   Cootes T. F., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P484, DOI 10.1007/BFb0054760
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cremers D, 2000, LECT NOTES COMPUT SC, V1888, P164
   Cremers D, 2006, INT J COMPUT VISION, V69, P335, DOI 10.1007/s11263-006-7533-5
   Fernandes LAF, 2008, PATTERN RECOGN, V41, P299, DOI 10.1016/j.patcog.2007.04.003
   Field D., 2008, Field's lower limb anatomy, palpation, and surface markings
   Gilles B., 2006, Medical Image Computing and Computer Assisted Intervention, International Conference on, P289
   He L, 2008, IMAGE VISION COMPUT, V26, P141, DOI 10.1016/j.imavis.2007.07.010
   Hossain M, 2008, CURR ORTHOPAED, V22, P300, DOI 10.1016/j.cuor.2008.07.011
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Kaus MR, 2003, IEEE T MED IMAGING, V22, P1005, DOI 10.1109/TMI.2003.815864
   Magnenat-Thalmann N., 2003, Biomedizinische Technik/Biomedical Engineering, V48, P20
   McInerney T, 2000, MED IMAGE ANAL, V4, P73, DOI 10.1016/S1361-8415(00)00008-6
   Rizzo D.C., 2001, Delmar's Fundamentals of Anatomy Physiology
   Schmid J, 2011, MED IMAGE ANAL, V15, P155, DOI 10.1016/j.media.2010.09.001
   SHEROUSE GW, 1990, INT J RADIAT ONCOL, V18, P651, DOI 10.1016/0360-3016(90)90074-T
   Song WW, 2007, 2007 IEEE/ICME INTERNATIONAL CONFERENCE ON COMPLEX MEDICAL ENGINEERING, VOLS 1-4, P586, DOI 10.1109/ICCME.2007.4381803
   Stegmaier S, 2005, VOLUME GRAPHICS 2005, P187
   Tannast M, 2007, AM J ROENTGENOL, V188, P1540, DOI 10.2214/AJR.06.0921
   Vese LA, 2002, INT J COMPUT VISION, V50, P271, DOI 10.1023/A:1020874308076
   Xu CY, 1998, IEEE T IMAGE PROCESS, V7, P359, DOI 10.1109/83.661186
   Yokota F, 2009, LECT NOTES COMPUT SC, V5762, P811, DOI 10.1007/978-3-642-04271-3_98
   Zoroofi RA, 2003, IEEE T INF TECHNOL B, V7, P329, DOI 10.1109/TITB.2003.813791
NR 30
TC 8
Z9 8
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2012
VL 28
IS 2
BP 205
EP 218
DI 10.1007/s00371-011-0636-1
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 881SW
UT WOS:000299510100007
DA 2024-07-18
ER

PT J
AU Sipiran, I
   Bustos, B
AF Sipiran, Ivan
   Bustos, Benjamin
TI Harris 3D: a robust extension of the Harris operator for interest point
   detection on 3D meshes
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3rd Eurographics Workshop on 3D Object Retrieval
CY MAY 02, 2010
CL Norrkoping, SWEDEN
DE 3D interest points detection; Local features; Harris operator
ID SCALE; FEATURES; SPACE
AB With the increasing amount of 3D data and the ability of capture devices to produce low-cost multimedia data, the capability to select relevant information has become an interesting research field. In 3D objects, the aim is to detect a few salient structures which can be used, instead of the whole object, for applications like object registration, retrieval, and mesh simplification. In this paper, we present an interest points detector for 3D objects based on Harris operator, which has been used with good results in computer vision applications. We propose an adaptive technique to determine the neighborhood of a vertex, over which the Harris response on that vertex is calculated. Our method is robust to several transformations, which can be seen in the high repeatability values obtained using the SHREC feature detection and description benchmark. In addition, we show that Harris 3D outperforms the results obtained by recent effective techniques such as Heat Kernel Signatures.
C1 [Sipiran, Ivan; Bustos, Benjamin] Univ Chile, PRISMA Res Grp, Dept Comp Sci, Santiago, Chile.
C3 Universidad de Chile
RP Sipiran, I (corresponding author), Univ Chile, PRISMA Res Grp, Dept Comp Sci, Santiago, Chile.
EM isipiran@dcc.uchile.cl; bebustos@dcc.uchile.cl
RI Sipiran, Ivan/GRR-8629-2022; Sipiran, Ivan/AAL-7603-2020; Bustos,
   Benjamin/G-1170-2010
OI Sipiran, Ivan/0000-0002-8766-3581; Bustos, Benjamin/0000-0002-3955-361X
CR [Anonymous], INT J COMPUT VIS SPE
   [Anonymous], 2010, P EUR WORKSH 3D OBJ
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   Castellani U, 2008, COMPUT GRAPH FORUM, V27, P643, DOI 10.1111/j.1467-8659.2008.01162.x
   Garland M., 1997, PROC 24 C COMPUTER G, P209, DOI DOI 10.1145/258734.258849
   Gelfand N., 2005, P 3 EUR S GEOM PROC, V2, P5
   Glomb P, 2009, ADV INTEL SOFT COMPU, V57, P103
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   Ho HT, 2009, IET COMPUT VIS, V3, P201, DOI 10.1049/iet-cvi.2009.0044
   Hu JX, 2009, VISUAL COMPUT, V25, P667, DOI 10.1007/s00371-009-0340-6
   Hua J, 2008, IEEE T VIS COMPUT GR, V14, P1643, DOI 10.1109/TVCG.2008.134
   Katz S, 2005, VISUAL COMPUT, V21, P649, DOI 10.1007/s00371-005-0344-9
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Laptev I., 2007, INT C COMPUT VIS, P1
   Lee CH, 2005, P ACM SIGGRAPH, P659
   Liu Y., 2006, Computer Vision and Pattern Recognition, P2025, DOI DOI 10.1109/CVPR.2006.278
   Loog M, 2010, IEEE T PATTERN ANAL, V32, P1141, DOI 10.1109/TPAMI.2010.53
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Novatnack J., 2007, PROC INT C COMPUT VI, P1
   Pratikakis I., 2010, EUR WORKSH 3D OBJ RE, P7, DOI DOI 10.2312/3DOR/3DOR10/007-014
   Schmid C, 2000, INT J COMPUT VISION, V37, P151, DOI 10.1023/A:1008199403446
   Shilane P, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P108
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Tierny J, 2008, VISUAL COMPUT, V24, P155, DOI 10.1007/s00371-007-0181-0
   Zaharescu A., 2009, P IEEE C COMP VIS PA
   Zou GY, 2008, COMPUT ANIMAT VIRT W, V19, P399, DOI 10.1002/cav.244
   Zou GY, 2009, IEEE T VIS COMPUT GR, V15, P1193, DOI 10.1109/TVCG.2009.159
NR 28
TC 284
Z9 326
U1 9
U2 44
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2011
VL 27
IS 11
SI SI
BP 963
EP 976
DI 10.1007/s00371-011-0610-y
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 831MW
UT WOS:000295736400003
DA 2024-07-18
ER

PT J
AU Chiang, CH
   Jong, BS
   Lin, TW
AF Chiang, Chien-Hsing
   Jong, Bin-Shyan
   Lin, Tsong-Wuu
TI A robust feature-preserving semi-regular remeshing method for triangular
   meshes
SO VISUAL COMPUTER
LA English
DT Article
DE Semi-regular remeshing; Feature preservation; Mesh regularity
AB Benefited from the hierarchical representations, 3D models generated by semi-regular remeshing algorithms based on either global parameterization or normal displacement have more advantages for digital geometry processing applications than the ones produced from traditional isotropic remeshing algorithms. Nevertheless, while original models have sharp features or multiple self-intersecting surfaces, it is still a challenge for previous algorithms to produce a semi-regular mesh with sharp features preservation as well as high mesh regularity. Therefore, this study proposes a robust semi-regular remeshing algorithm that uses a two-step surface segmentation scheme to build the high quality base mesh, as well as the regional relationship between the original surface and subdivision domain surface. Using the regional relationship, the proposed algorithm substantially enhances the accuracy and robustness of the backward projection process of subdivision vertices based on normal displacement. Furthermore, the mesh regularity of remeshed models is improved by the quadric mesh relaxation scheme. The experimental results demonstrate the capabilities of the proposed algorithm's semi-regular remeshing to preserve geometric features and have good triangle aspect ratio.
C1 [Chiang, Chien-Hsing] Chung Yuan Christian Univ, Dept Elect Engn, Chungli, Taiwan.
   [Jong, Bin-Shyan] Chung Yuan Christian Univ, Dept Informat & Comp Engn, Chungli, Taiwan.
   [Lin, Tsong-Wuu] Soochow Univ, Dept Sci & Informat Management, SCU CIS, Taipei 100, Taiwan.
C3 Chung Yuan Christian University; Chung Yuan Christian University;
   Soochow University
RP Chiang, CH (corresponding author), Chung Yuan Christian Univ, Dept Elect Engn, 22 Pu Jen, Chungli, Taiwan.
EM hikki@cg.ice.cycu.edu.tw; bsjong@ice.cycu.edu.tw; twlin@csim.scu.edu.tw
CR Alliez P, 2005, GRAPH MODELS, V67, P204, DOI 10.1016/j.gmod.2004.06.007
   Alliez P, 2008, MATH VIS, P53, DOI 10.1007/978-3-540-33265-7_2
   Attene M, 2006, VISUAL COMPUT, V22, P181, DOI 10.1007/s00371-006-0375-x
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   Friedel I, 2004, ACM T GRAPHIC, V23, P1061, DOI 10.1145/1027411.1027418
   Fu Y, 2009, COMPUT AIDED GEOM D, V26, P711, DOI 10.1016/j.cagd.2009.03.007
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Guskov I, 2000, COMP GRAPH, P95, DOI 10.1145/344779.344831
   Guskov I, 2007, GRAPH MODELS, V69, P1, DOI 10.1016/j.gmod.2006.05.001
   HOPPE H, 1994, SIGGRAPH 94 C P, P295
   Jong BS, 2010, VISUAL COMPUT, V26, P121, DOI 10.1007/s00371-009-0392-7
   KHODAKOVSKY A, 2003, SIGGRAPH 03, P350
   Lavoué G, 2007, COMPUT GRAPH FORUM, V26, P1, DOI 10.1111/j.1467-8659.2007.00930.x
   Lavoué G, 2009, COMPUT GRAPH-UK, V33, P151, DOI 10.1016/j.cag.2009.01.004
   Lee A, 2000, COMP GRAPH, P85, DOI 10.1145/344779.344829
   Lee A. W., 1998, Proceedings of the 25th annual conference on Computer graphics and interactive techniques, P95, DOI DOI 10.1145/280814.280828
   Lévy B, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778856
   Ling RT, 2008, COMPUT GRAPH FORUM, V27, P1383, DOI 10.1111/j.1467-8659.2008.01278.x
   Liu LG, 2007, COMPUT AIDED DESIGN, V39, P772, DOI 10.1016/j.cad.2007.03.004
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Loop C, 1987, THESIS U UTAH
   Marinov M, 2005, GRAPH MODELS, V67, P452, DOI 10.1016/j.gmod.2005.01.003
   Marinov M, 2005, COMPUT GRAPH FORUM, V24, P479, DOI 10.1111/j.1467-8659.2005.00873.x
   Nealen A., 2006, P 4 INT C COMP GRAPH, P381, DOI DOI 10.1145/1174429.1174494
   Ohtake Y., 2000, Proceedings Geometric Modeling and Processing 2000. Theory and Applications, P229, DOI 10.1109/GMAP.2000.838255
   Pietroni N, 2010, IEEE T VIS COMPUT GR, V16, P621, DOI 10.1109/TVCG.2009.96
   Schnabel R, 2007, COMPUT GRAPH FORUM, V26, P214, DOI 10.1111/j.1467-8659.2007.01016.x
   Schnabel R, 2009, COMPUT GRAPH FORUM, V28, P503, DOI 10.1111/j.1467-8659.2009.01389.x
   Shewchuk JR, 2008, DISCRETE COMPUT GEOM, V39, P580, DOI 10.1007/s00454-008-9060-3
   Surazhsky V., 2003, Symposium on Geometry Processing, P20
   Valette S, 2008, IEEE T VIS COMPUT GR, V14, P369, DOI 10.1109/TVCG.2007.70430
   Vorsatz J, 2001, COMPUT GRAPH FORUM, V20, pC393, DOI 10.1111/1467-8659.00532
   Vorsatz J., 2003, Proceedings of the Eighth ACM Symposium on Solid Modeling and Applications, P167, DOI DOI 10.1145/781606.781633
   Wicke M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778786
   Wood ZJ, 2000, IEEE VISUAL, P275, DOI 10.1109/VISUAL.2000.885705
   Wu JH, 2005, COMPUT GRAPH FORUM, V24, P277, DOI 10.1111/j.1467-8659.2005.00852.x
   Yan DM, 2009, COMPUT GRAPH FORUM, V28, P1445, DOI 10.1111/j.1467-8659.2009.01521.x
   YUE W, 2007, SPM 07, P23
NR 39
TC 9
Z9 9
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2011
VL 27
IS 9
BP 811
EP 825
DI 10.1007/s00371-011-0555-1
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 807TK
UT WOS:000293922400002
DA 2024-07-18
ER

PT J
AU Zhang, JW
   Li, L
   Zhang, Y
   Yang, GQ
   Cao, XC
   Sun, JZ
AF Zhang, Jiawan
   Li, Liang
   Zhang, Yi
   Yang, Guoqiang
   Cao, Xiaochun
   Sun, Jizhou
TI Video dehazing with spatial and temporal coherence
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Video dehazing; Markov random field; Image dehazing; Video enhancement
ID FLOW
AB This paper describes a new framework for video dehazing, the process of restoring the visibility of the videos taken under foggy scenes. The framework builds upon techniques in single image dehazing, optical flow estimation and Markov random field. It aims at improving the temporal and spatial coherence of the dehazed video. In this framework, we first extract the transmission map frame-by-frame using guided filter, then estimate the forward and backward optical flow between two neighboring frames to find the matched pixels. The flow fields are used to help us building an MRF model on the transmission map to improve the spatial and temporal coherence of the transmission. The proposed algorithm is verified in both real and synthetic videos. The results demonstrate that our algorithm can preserve the spatial and temporal coherence well. With more coherent transmission map, we get better refocusing effect. We also apply our framework on improving the video coherence on the application of video denoising.
C1 [Zhang, Jiawan; Li, Liang; Yang, Guoqiang; Cao, Xiaochun; Sun, Jizhou] Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
   [Zhang, Jiawan; Li, Liang; Zhang, Yi; Yang, Guoqiang; Cao, Xiaochun; Sun, Jizhou] Tianjin Univ, MTS Lab, Tianjin 300072, Peoples R China.
   [Zhang, Jiawan; Zhang, Yi] Tianjin Univ, Sch Software, Tianjin 300072, Peoples R China.
   [Zhang, Jiawan; Zhang, Yi] Tianjin Univ, Sch Comp Software, Tianjin 300072, Peoples R China.
C3 Tianjin University; Tianjin University; Tianjin University; Tianjin
   University
RP Li, L (corresponding author), Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
EM allen.liliang@gmail.com
OI Zhang, Jiawan/0000-0002-0667-6744
CR ANCUTI CO, 2010, ACM SIGGRAPH ASIA 20
   BAKER S, 2007, DATABASE EVALUATION
   Black MJ, 1996, COMPUT VIS IMAGE UND, V63, P75, DOI 10.1006/cviu.1996.0006
   Bolz J, 2003, ACM T GRAPHIC, V22, P917, DOI 10.1145/882262.882364
   Chen J., 2007, IEEE C COMP VIS PATT
   Chuang YY, 2002, ACM T GRAPHIC, V21, P243, DOI 10.1145/566570.566572
   Dong XM, 2010, IEEE IMAGE PROC, P3593, DOI 10.1109/ICIP.2010.5651965
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Haussecker HW, 2001, IEEE T PATTERN ANAL, V23, P661, DOI 10.1109/34.927465
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   He KM, 2009, PROC CVPR IEEE, P1956, DOI [10.1109/CVPRW.2009.5206515, 10.1109/CVPR.2009.5206515]
   Hu HG, 2010, IEEE C EVOL COMPUTAT
   Kopf J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409069
   Koschmieder H., 1924, Beitraege Phys. Atmosp., P33
   LIN SY, 2007, J COMPUT SCI TECHNOL, V22
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   PREETHAM AJ, 1999, ACM SIGGRAPH
   SCHECHNER YY, 2001, P IEEE COMP SOC C CO, V1, pI325
   Shwartz S., 2006, 2006 IEEE COMP SOC C, V2, P1984, DOI DOI 10.1109/CVPR.2006.71
   Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Villegas P, 2004, IEEE T IMAGE PROCESS, V13, P1092, DOI 10.1109/TIP.2004.828433
   Xingyong Lv, 2010, 2010 Pacific Graphics (PG). Proceedings 18th Pacific Conference on Computer Graphics and Applications, P62, DOI 10.1109/PacificGraphics.2010.16
   Zhang GF, 2008, PROC CVPR IEEE, P1189
   Zhang JW, 2010, VISUAL COMPUT, V26, P761, DOI 10.1007/s00371-010-0444-z
NR 26
TC 50
Z9 58
U1 2
U2 38
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 749
EP 757
DI 10.1007/s00371-011-0569-8
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600033
DA 2024-07-18
ER

PT J
AU Zhong, F
   Qin, XY
   Peng, QS
AF Zhong, Fan
   Qin, Xueying
   Peng, Qunsheng
TI Robust image segmentation against complex color distribution
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Segmentation; Likelihoods; Color distribution; Learning
AB Color distribution is the most effective cue that is widely adopted in previous interactive image segmentation methods. However, it also may introduce additional errors in some situations, for example, when the foreground and background have similar colors. To address this problem, this paper proposes a novel method to learn the segmentation likelihoods. The proposed method is designed for high reliability, for which purpose it may choose to discard some unreliable likelihoods that may cause segmentation error. The reliability of likelihoods is estimated in a few Expectation-Maximization iterations. In each iteration, a novel multi-class transductive learning algorithm, namely, the Constrained Mapping, is proposed to learn likelihoods and identify unreliable likelihoods simultaneously. The resulting likelihoods then can be used as the input of any segmentation methods to improve their robustness. Experiments show that the proposed method is an effective way to improve both segmentation quality and efficiency, especially when the input image has complex color distribution.
C1 [Zhong, Fan; Qin, Xueying] Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Peoples R China.
   [Peng, Qunsheng] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310003, Zhejiang, Peoples R China.
C3 Shandong University; Zhejiang University
RP Qin, XY (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Peoples R China.
EM zhongfan@cad.zju.edu.cn; xyqin@cad.zju.edu.cn; peng@cad.zju.edu.cn
RI Qin, Xueying/AAM-8775-2021
OI Qin, Xueying/0000-0003-0057-295X
CR [Anonymous], INT C MACH LEARN ICM
   [Anonymous], 2001, Interactive graph cuts for optimal boundary & region segmentation of objects in nd images, DOI DOI 10.1109/ICCV.2001.937505
   [Anonymous], 1984, CARUS MATH MONOGR
   Bai XF, 2007, IEEE IC COMP COM NET, P1
   Belkin M, 2004, MACH LEARN, V56, P209, DOI 10.1023/B:MACH.0000033120.25363.1e
   Blake A, 2004, LECT NOTES COMPUT SC, V3021, P428
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Criminisi A, 2008, LECT NOTES COMPUT SC, V5302, P99, DOI 10.1007/978-3-540-88682-2_9
   GANDER W, 1989, LINEAR ALGEBRA APPL, V114, P815, DOI 10.1016/0024-3795(89)90494-1
   Grady L, 2005, PROC CVPR IEEE, P763
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   Gulshan V, 2010, PROC CVPR IEEE, P3129, DOI 10.1109/CVPR.2010.5540073
   Joachims T., 2003, P 20 INT C MACH LEAR, V20, P290, DOI DOI 10.1145/2612669.2612699
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Price BL, 2010, PROC CVPR IEEE, P3161, DOI 10.1109/CVPR.2010.5540079
   Rother C., 2004, ACM Transactions on Graphics (SIGGRAPH), V23, P309
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Sinop AK, 2007, IEEE I CONF COMP VIS, P1016, DOI 10.1109/iccv.2007.4408927
   Sun J, 2006, LECT NOTES COMPUT SC, V3952, P628
   Vicente S., 2008, PROC IEEE C COMPUT V, P1, DOI DOI 10.1109/CVPR.2008.4587440
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Zheng Y., 2008, P COMPUTER VISION PA, P1
NR 23
TC 5
Z9 11
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 707
EP 716
DI 10.1007/s00371-011-0588-5
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600029
DA 2024-07-18
ER

PT J
AU Srijuntongsiri, G
AF Srijuntongsiri, Gun
TI An iterative/subdivision hybrid algorithm for curve/curve intersection
SO VISUAL COMPUTER
LA English
DT Article
DE Curve/curve intersection; Newton's method; Subdivision method
AB The problem of finding all intersections between two space curves is one of the fundamental problems in computer-aided geometric design and computational geometry. This article proposes a new iterative/subdivision hybrid algorithm for this problem. We use a test based on Kantorovich's theorem to detect the starting point from which Newton's method converges quadratically and a subdivision scheme to exclude certain regions that do not contain any intersections. Our algorithm is guaranteed to detect all intersections in the domain for nondegenerate and non-ill-posed cases.
C1 Thammasat Univ, Sirindhorn Int Inst Technol, Pathum Thani 12000, Thailand.
C3 Thammasat University
RP Srijuntongsiri, G (corresponding author), Thammasat Univ, Sirindhorn Int Inst Technol, Pathum Thani 12000, Thailand.
EM gun@siit.tu.ac.th
OI Srijuntongsiri, Gun/0000-0001-6791-4411
CR DEUFLHARD P, 1979, SIAM J NUMER ANAL, V16, P1, DOI 10.1137/0716001
   Farouki R. T., 1987, Computer-Aided Geometric Design, V4, P191, DOI 10.1016/0167-8396(87)90012-4
   Farouki RT, 2001, NUMER ALGORITHMS, V27, P35, DOI 10.1023/A:1016621116240
   Farouki RT, 1996, MATH COMPUT, V65, P1553, DOI 10.1090/S0025-5718-96-00759-4
   Hawat RN, 1999, MATH ENG IND, V7, P269
   Heath M., 2005, Scientific Computing: An Introductory Survey
   Hoschek J., 1993, FUNDAMENTALS COMPUTE
   Houghton E. G., 1985, Computer-Aided Geometric Design, V2, P173, DOI 10.1016/0167-8396(85)90022-6
   Hu CY, 1996, COMPUT AIDED DESIGN, V28, P495, DOI 10.1016/0010-4485(95)00063-1
   KANTOROVICH LV, 1948, DOKL AKAD NAUK SSSR+, V59, P1237
   LIMAIEM A, 1995, COMPUT GRAPH, V19, P391, DOI 10.1016/0097-8493(95)00009-2
   MAEKAWA T, 1993, COMPUT AIDED GEOM D, V10, P407, DOI 10.1016/0167-8396(93)90020-4
   Maekawa T., 1994, Visual Computer, V10, P216, DOI 10.1007/BF01901288
   MEGIDDO N, 1984, J ACM, V31, P114, DOI 10.1145/2422.322418
   Mortenson M., 1985, Geometric Modeling
   PATRIKALAKIS NM, 1993, IEEE COMPUT GRAPH, V13, P89, DOI 10.1109/38.180122
   Rubin S. M., 1980, Computer Graphics, V14, P110, DOI 10.1145/965105.807479
   SEDERBERG TW, 1990, COMPUT AIDED DESIGN, V22, P538, DOI 10.1016/0010-4485(90)90039-F
   SRIJUNTONGSIRI G, 2010, ECTI CON 2010, P1241
   SRIJUNTONGSIRI G, 2009, PARALLEL PROCESSING
   Srijuntongsiri G, 2008, SIAM J SCI COMPUT, V30, P1064, DOI 10.1137/060668043
   WHITTED T, 1980, COMMUN ACM, V23, P343, DOI 10.1145/358876.358882
NR 22
TC 2
Z9 2
U1 1
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2011
VL 27
IS 5
BP 365
EP 371
DI 10.1007/s00371-010-0543-x
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 745ZI
UT WOS:000289209800003
DA 2024-07-18
ER

PT J
AU Orozco, H
   Ramos, F
   Ramos, M
   Thalmann, D
AF Orozco, Hector
   Ramos, Felix
   Ramos, Marco
   Thalmann, Daniel
TI An action selection process to simulate the human behavior in virtual
   humans with real personality
SO VISUAL COMPUTER
LA English
DT Article
DE Human behavior simulation; Virtual human; Personality; Affective state;
   MMPI
ID MODEL
AB In this paper, we present an action selection mechanism to simulate the human behavior in realistic and believable virtual humans according to their personality and affective state taking into account their beliefs, desires, intentions, and the level of intensity of the events they perceive from their environment.
C1 [Orozco, Hector; Ramos, Felix] Ctr Invest & Estud Avanzados, Inst Politecn Nacl, Unidad Guadalajara, Zapopan 45015, Jalisco, Mexico.
   [Ramos, Marco] Univ Autonoma Estado Mexico, Mexico City, DF, Mexico.
   [Thalmann, Daniel] Nanyang Technol Univ, Inst Media Innovat, Singapore, Singapore.
C3 Instituto Politecnico Nacional - Mexico; Universidad Autonoma del Estado
   de Mexico; Nanyang Technological University
RP Orozco, H (corresponding author), Ctr Invest & Estud Avanzados, Inst Politecn Nacl, Unidad Guadalajara, Av Cient 1145, Zapopan 45015, Jalisco, Mexico.
EM horozco@gdl.cinvestav.mx; mramos@univ-tlse1.fr;
   danielthalmann@ntu.edu.sg
RI Thalmann, Daniel/A-4347-2008; Thalmann, Daniel/AAL-1097-2020; Ramos
   Corchado, Marco Antonio/C-6045-2016
OI Thalmann, Daniel/0000-0002-0451-7491; Orozco Aguirre, Hector
   Rafael/0000-0002-2169-4254; Ramos Corchado, Marco
   Antonio/0000-0003-3982-6988
FU CoECyT-Jal [2008-05-97094]; CONACYT [203060]
FX This research was partially supported by CoECyT-Jal project no.
   2008-05-97094, while author Hector Orozco was supported by CONACYT grant
   no. 203060.
CR Abu Maria K, 2007, INFORM SOFTWARE TECH, V49, P695, DOI 10.1016/j.infsof.2006.08.002
   [Anonymous], 2005, INT JOINT C AUTONOMO, DOI DOI 10.1145/1082473.1082478
   Bates J., 1992, MODELLING AUTONOMOUS, P55, DOI [DOI 10.1007/3-540-58266-5_4, DOI 10.1007/3-540-58266-5]
   Bosse T., 2007, P 8 INT C COGN MOD I, V187, P192
   Egges A, 2004, COMPUT ANIMAT VIRT W, V15, P1, DOI 10.1002/cav.3
   Ekman P., 1994, Moods, emotions, and traits
   El-Nasr MS, 2000, AUTON AGENT MULTI-AG, V3, P219, DOI 10.1023/A:1010030809960
   GARCIAROJAS A, 2008, AAMAS 08, P143
   Ghasem-Aghaee N, 2007, COMPUT HUM BEHAV, V23, P2983, DOI 10.1016/j.chb.2006.08.012
   Hong J., 2008, RATIONAL EMOTIONAL A
   Imbert R, 2005, LECT NOTES COMPUT SC, V3805, P63, DOI 10.1007/11590361_7
   JIANG H, 1997, P 6 INT JOINT C AUT, P1
   Kasap Z, 2009, IEEE COMPUT GRAPH, V29, P20, DOI 10.1109/MCG.2009.26
   KOWALSKI R, 1986, NEW GENERAT COMPUT, V4, P67, DOI 10.1007/BF03037383
   LAIRD J, 2003, P 2003 C BEH REP MOD
   Liu Z, 2005, LECT NOTES COMPUT SC, V3784, P629
   Liu Z, 2008, PROCEEDINGS OF 2008 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-7, P2712, DOI 10.1109/ICMLC.2008.4620867
   Martínez-Miranda J, 2005, COMPUT HUM BEHAV, V21, P323, DOI 10.1016/j.chb.2004.02.010
   McCauley L., 2000, Affective Interactions. Towards a New Generation of Computer Interfaces (Lecture Notes in Artificial Intelligence Vol.1814), P107
   MCCRAE RR, 1992, J PERS, V60, P175, DOI 10.1111/j.1467-6494.1992.tb00970.x
   MURAKAMI Y, 2005, AAAI C ART INT P 20, V1, P127
   Ören TI, 2004, PROCEEDINGS OF THE 2004 WINTER SIMULATION CONFERENCE, VOLS 1 AND 2, P801
   OROZCO H, 2010, P 11 COMP GRAPH INT
   Ortony A., 1988, COGNITIVE STRUCTURE
   Poznanski M, 2005, J EXP THEOR ARTIF IN, V17, P221, DOI 10.1080/09528130500112478
   Rizzo P., 1997, P AAAI FALL S SOC IN
   SCHMIDT B, 2002, P 3 WORKSH AG BAS SI
   Sellbom M, 2005, J PERS ASSESS, V85, P179, DOI 10.1207/s15327752jpa8502_10
   Tellegen A., 2003, MMPI-2 Restructured Clinical (RC) scales: Development, validation, and interpretation
NR 29
TC 9
Z9 9
U1 0
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2011
VL 27
IS 4
SI SI
BP 275
EP 285
DI 10.1007/s00371-011-0549-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 739IC
UT WOS:000288707700004
DA 2024-07-18
ER

PT J
AU Sandholm, A
   Schwartz, C
   Pronost, N
   de Zee, M
   Voigt, M
   Thalmann, D
AF Sandholm, Anders
   Schwartz, Cedric
   Pronost, Nicolas
   de Zee, Mark
   Voigt, Michael
   Thalmann, Daniel
TI Evaluation of a geometry-based knee joint compared to a planar knee
   joint
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3DAH Conference
CY MAY 23-24, 2010
CL Chania, GREECE
DE Knee joint; Inverse kinematics and dynamics; Joint reaction; Computed
   muscular control; OrthoLoad; Validation; Musculoskeletal model
ID DYNAMIC SIMULATIONS; LOWER-EXTREMITY; CONTACT FORCES; HUMAN GAIT;
   IN-VIVO; MODEL; MOVEMENT; WALKING; DESIGN; MOTION
AB Today neuromuscular simulations are used in several fields, such as diagnostics and planing of surgery, to get a deeper understanding of the musculoskeletal system. During the last year, new models and datasets have been presented which can provide us with more in-depth simulations and results. The same kind of development has occurred in the field of studying the human knee joint using complex three dimensional finite element models and simulations. In the field of musculoskeletal simulations, no such knee joints can be used. Instead the most common knee joint description is an idealized knee joint with limited accuracy or a planar knee joint which only describes the knee motion in a plane. In this paper, a new knee joint based on both equations and geometry is introduced and compared to a common clinical planar knee joint. The two kinematical models are analyzed using a gait motion, and are evaluated using the muscle activation and joint reaction forces which are compared to in-vivo measured forces. We show that we are able to predict the lateral, anterior and longitudinal moments, and that we are able to predict better knee and hip joint reaction forces.
C1 [Sandholm, Anders; Pronost, Nicolas; Thalmann, Daniel] Ecole Polytech Fed Lausanne, Virtual Real Lab, Lausanne, Switzerland.
   [Schwartz, Cedric; de Zee, Mark; Voigt, Michael] Aalborg Univ, Ctr Sensory Motor Interact, Aalborg, Denmark.
   [Pronost, Nicolas] Univ Utrecht, Games & Virtual Worlds Res Grp, Utrecht, Netherlands.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne; Aalborg University; Utrecht University
RP Sandholm, A (corresponding author), Ecole Polytech Fed Lausanne, Virtual Real Lab, Lausanne, Switzerland.
EM anders.sandholm@epfl.ch
RI Thalmann, Daniel/AAL-1097-2020; Thalmann, Daniel/A-4347-2008
OI Thalmann, Daniel/0000-0002-0451-7491; PRONOST,
   NICOLAS/0000-0003-4499-509X; Schwartz, Cedric/0000-0002-7307-7821; de
   Zee, Mark/0000-0003-0584-271X
CR Allaire S, 2007, P ANN INT IEEE EMBS, P5087, DOI 10.1109/IEMBS.2007.4353484
   Anderson FC, 2001, J BIOMECH, V34, P153, DOI 10.1016/S0021-9290(00)00155-X
   Arnold EM, 2010, ANN BIOMED ENG, V38, P269, DOI 10.1007/s10439-009-9852-5
   Bergmann G, 2001, J BIOMECH, V34, P859, DOI 10.1016/S0021-9290(01)00040-9
   Castagno P., 1995, Gait Posture, V3, P87, DOI DOI 10.1016/0966-6362(95)93466-P
   D'Lima DD, 2007, J BIOMECH, V40, pS11, DOI 10.1016/j.jbiomech.2007.03.004
   Damsgaard M, 2006, SIMUL MODEL PRACT TH, V14, P1100, DOI 10.1016/j.simpat.2006.09.001
   De Groote F, 2008, J BIOMECH, V41, P3390, DOI 10.1016/j.jbiomech.2008.09.035
   Delp SL, 2007, IEEE T BIO-MED ENG, V54, P1940, DOI 10.1109/TBME.2007.901024
   DELP SL, 1990, IEEE T BIO-MED ENG, V37, P757, DOI 10.1109/10.102791
   Ellis BJ, 2006, J ORTHOP RES, V24, P800, DOI 10.1002/jor.20102
   Fernandez JW, 2005, BIOMECH MODEL MECHAN, V4, P20, DOI 10.1007/s10237-005-0072-0
   Glitsch U, 1997, J BIOMECH, V30, P1123, DOI 10.1016/S0021-9290(97)00089-4
   Heinlein B, 2009, CLIN BIOMECH, V24, P315, DOI 10.1016/j.clinbiomech.2009.01.011
   HORSMAN K, 2007, THESIS U TWENTE NETH
   KADABA MP, 1989, J ORTHOP RES, V7, P849, DOI 10.1002/jor.1100070611
   Kalman R E., 1960, J BASIC ENG, V82, P35, DOI DOI 10.1115/1.3662552
   KUROSAWA H, 1985, J BIOMECH, V18, P487, DOI 10.1016/0021-9290(85)90663-3
   Kutzner I, 2010, J BIOMECH, V43, P2164, DOI 10.1016/j.jbiomech.2010.03.046
   Liu MQ, 2008, J BIOMECH, V41, P3243, DOI 10.1016/j.jbiomech.2008.07.031
   Moro-Oka TA, 2008, J ORTHOP RES, V26, P428, DOI 10.1002/jor.20488
   Morrison J B, 1969, Biomed Eng, V4, P573
   Pioletti DP, 1998, J BIOMECH, V31, P753, DOI 10.1016/S0021-9290(98)00077-3
   Ramaniraka NA, 2007, CLIN BIOMECH, V22, P336, DOI 10.1016/j.clinbiomech.2006.10.006
   Sandholm A, 2009, LECT NOTES COMPUT SC, V5903, P110, DOI 10.1007/978-3-642-10470-1_10
   Schmid J, 2009, RECENT ADVANCES IN THE 3D PHYSIOLOGICAL HUMAN, P3, DOI 10.1007/978-1-84882-565-9_1
   Schmid J, 2008, LECT NOTES COMPUT SC, V5241, P119, DOI 10.1007/978-3-540-85988-8_15
   Taylor WR, 2004, J ORTHOP RES, V22, P625, DOI 10.1016/j.orthres.2003.09.003
   Thelen DG, 2006, J BIOMECH, V39, P1107, DOI 10.1016/j.jbiomech.2005.02.010
   WALKER PS, 1988, J BIOMECH, V21, P965, DOI 10.1016/0021-9290(88)90135-2
   Weiss JA, 1996, COMPUT METHOD APPL M, V135, P107, DOI 10.1016/0045-7825(96)01035-3
   YAMAGUCHI GT, 1989, J BIOMECH, V22, P1, DOI 10.1016/0021-9290(89)90179-6
NR 32
TC 23
Z9 27
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2011
VL 27
IS 2
SI SI
BP 161
EP 171
DI 10.1007/s00371-010-0538-7
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 722QR
UT WOS:000287449900009
DA 2024-07-18
ER

PT J
AU Feng, BL
   Cao, JA
   Bao, XG
   Bao, L
   Zhang, YD
   Lin, SX
   Yun, XC
AF Feng, Bailan
   Cao, Juan
   Bao, Xiuguo
   Bao, Lei
   Zhang, Yongdong
   Lin, Shouxun
   Yun, Xiaochun
TI Graph-based multi-space semantic correlation propagation for video
   retrieval
SO VISUAL COMPUTER
LA English
DT Article
DE Concept-based video retrieval; Concept selection and fusion; Multi-space
   integration and propagation; Manifold ranking
ID DETECTORS; GAP
AB By introducing the concept detection results to the retrieval process, concept-based video retrieval (CBVR) has been successfully used for semantic content-based video retrieval application. However, how to select and fuse the appropriate concepts for a specific query is still an important but difficult issue. In this paper, we propose a novel and effective concept selection method, named graph-based multi-space semantic correlation propagation (GMSSCP), to explore the relationship between the user query and concepts for video retrieval application. Compared with traditional methods, GMSSCP makes use of a manifold-ranking algorithm to collectively explore the multi-layered relationships between the query and concepts, and the expansion result is more robust to noises. Parallel to this, GMSSCP has a query-adapting property, which can enhance the process of concept correlation propagation and selection with strong pertinence of query cues. Furthermore, it can dynamically update the unified propagation graph by flexibly introducing the multi-modal query cues as additional nodes, and is not only effective for automatic retrieval but also appropriate for the interactive case. Encouraging experimental results on TRECVID datasets demonstrate the effectiveness of GMSSCP over the state-of-the-art concept selection methods. Moreover, we also apply it to the interactive retrieval system-VideoMap and gain an excellent performance and user experience.
C1 [Feng, Bailan; Cao, Juan; Bao, Lei; Zhang, Yongdong; Lin, Shouxun] Chinese Acad Sci, Inst Comp Technol, Lab Adv Comp Res, Beijing 100190, Peoples R China.
   [Feng, Bailan; Bao, Lei] Chinese Acad Sci, Grad Univ, Beijing 100039, Peoples R China.
   [Bao, Xiuguo; Yun, Xiaochun] Coordinat Ctr China, Natl Comp Network Emergency Response Tech, Beijing 100029, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Feng, BL (corresponding author), Chinese Acad Sci, Inst Comp Technol, Lab Adv Comp Res, Beijing 100190, Peoples R China.
EM fengbailan@ict.ac.cn
FU National Basic Research Program of China (973 Program) [2007CB311100];
   National High Technology and Research Development Program of China (863
   Program) [2007AA01Z416]; National Nature Science Foundation of China
   [6087165, 60802028, 60902090]; Beijing New Star Project on Science
   Technology [2007B071]; Beijing Municipal Education Commission
FX This work was supported by National Basic Research Program of China (973
   Program, 2007CB311100), National High Technology and Research
   Development Program of China (863 Program, 2007AA01Z416), National
   Nature Science Foundation of China (6087165, 60802028, and 60902090),
   Beijing New Star Project on Science & Technology (2007B071), and the
   Co-building Program of Beijing Municipal Education Commission. The
   authors would like to thank Xiufeng Hua and Liang Ma for their supports
   with the system UI design.
CR [Anonymous], 2006, ACM INT C MULTIMEDIA, DOI [10.1145/1180639.1180727, DOI 10.1145/1180639.1180727]
   [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], 2007, PROC INT C MULTIMEDI, DOI DOI 10.1145/1291233.1291447
   [Anonymous], 2007, CIVR '07
   [Anonymous], 2007, P 15 ACM INT C MULT, DOI DOI 10.1145/1291233.1291430
   [Anonymous], 2007, COLUMBIA U BASELINE
   Aslam J. A., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P601, DOI 10.1145/1148170.1148275
   Cao J., 2009, P ACM INT C IM VID R
   Cao J., 2008, P TRECVID WORKSH GAI
   Cao J., 2009, P ACM INT C MULT BEI, P19
   Chong-Wah N., 2001, INT J IMAGE GRAPH, V1, P445
   Fellbaum C., 1998, WORDNET ELECT LEXICA, DOI DOI 10.7551/MITPRESS/7287.001.0001
   Haubold A, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1761, DOI 10.1109/ICME.2006.262892
   Hauptmann A, 2007, IEEE T MULTIMEDIA, V9, P958, DOI 10.1109/TMM.2007.900150
   He J., P 12 ANN ACM INT C M, P9, DOI 10.1145/1027527.1027531
   Li Xirong., 2007, P 6 ACM INT C IMAGE, P603
   Naphade M.R., 2005, A light scale concept ontology for multimedia understanding for trecvid 2005
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Natsev Apostol., 2007, MULTIMEDIA 07, P991
   Neo SY, 2006, LECT NOTES COMPUT SC, V4071, P143
   Ngo C.W., 2008, P TRECVID WORKSH GAI
   Over P., 2008, P TRECVID WORKSH GAI
   Qi G.J., 2007, P 15 ACM INT C MULTI, P17, DOI DOI 10.1145/1291233.1291245
   Rasiwasia N, 2007, IEEE T MULTIMEDIA, V9, P923, DOI 10.1109/TMM.2007.900138
   Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014
   Snoek CGM, 2008, IEEE MULTIMEDIA, V15, P86, DOI 10.1109/MMUL.2008.21
   Snoek CGM, 2007, IEEE T MULTIMEDIA, V9, P975, DOI 10.1109/TMM.2007.900156
   Tang JH, 2009, IEEE T SYST MAN CY B, V39, P409, DOI 10.1109/TSMCB.2008.2006045
   Tang Jinhui., 2009, Proceedings of ACM international conference on Multimedia, P223, DOI DOI 10.1145/1631272.1631305
   Tesic J., 2007, CIVR '07: Proceedings of the 6th ACM international conference on Image and video retrieval, P595
   Wan Hualin, 2003, Journal of Computer Aided Design & Computer Graphics, V15, P195
   Wang Dong., 2007, the 15th ACM International Conference on Multimedia, P285
   Wei X.Y., 2008, P ACM INT C MULT VAN, P26
   Wei XY, 2008, IEEE T MULTIMEDIA, V10, P1085, DOI 10.1109/TMM.2008.2001382
   Xiang Sean Zhou, 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P570, DOI 10.1109/ICIP.1999.822959
   Yuan X., 2006, Proceedings of the 14th annual ACM international conference on Multimedia, P623
   Zhou DY, 2004, ADV NEUR IN, V16, P169
NR 37
TC 11
Z9 12
U1 1
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2011
VL 27
IS 1
BP 21
EP 34
DI 10.1007/s00371-010-0510-6
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 700YI
UT WOS:000285781100002
DA 2024-07-18
ER

PT J
AU Rustamov, RM
AF Rustamov, Raif M.
TI A versatile framework for shape description
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 2nd Eurographics Workshop on 3D Object Retrieval
CY MAR 29, 2009
CL Munich, GERMANY
DE 3D shape retrieval; Numerical shape descriptors; Barycentric
   coordinates; Laplace-Beltrami eigenfunctions; Diffusion wavelets
ID MODEL
AB We present a shape description framework that generates a multitude of shape descriptors through a variety of design and continuum of parameter choices. Our parameter is a surface mesh, referred to as the template, which is supplied at run time, and allows generating different shape descriptors for the same model. Our framework extracts a numerical shape descriptor by computing a selected function on the model mesh, mapping (transferring) it to the template, expanding the mapped function in terms of a basis on the template, and collecting the expansion coefficients into a vector. We investigate possible design choices for the steps in the framework, and introduce novel approaches that provide further freedom in generating a multitude of previously unknown descriptors. We show that our approach is a generalization of the way some of the existing numerical descriptors are defined, and that for appropriate template choices one is able to reproduce some of the well-known descriptors. Finally, we show empirically that design and parameter choices have non-trivial effects on the descriptor's performance, and that better retrieval results can be obtained by combining descriptors obtained via different templates.
C1 Drew Univ, Madison, NJ 07940 USA.
C3 Drew University
RP Rustamov, RM (corresponding author), Drew Univ, 36 Madison Ave, Madison, NJ 07940 USA.
EM rrustamov@drew.edu
CR [Anonymous], 2000, P SPRING C COMP GRAP
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Biasotti S, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1391729.1391731
   BRONSTEIN A, INT J COMPUT VIS
   Bronstein AM, 2006, P NATL ACAD SCI USA, V103, P1168, DOI 10.1073/pnas.0508601103
   Bustos B, 2005, ACM COMPUT SURV, V37, P345, DOI 10.1145/1118890.1118893
   Chazal F., 2009, COMPUTER GRAPHICS FO
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Coifman RR, 2006, APPL COMPUT HARMON A, V21, P53, DOI 10.1016/j.acha.2006.04.004
   Coifman RR, 2005, P NATL ACAD SCI USA, V102, P7426, DOI 10.1073/pnas.0500334102
   Floater MS, 2005, COMPUT AIDED GEOM D, V22, P623, DOI 10.1016/j.cagd.2005.06.004
   Floater MS, 2003, COMPUT AIDED GEOM D, V20, P19, DOI 10.1016/S0167-8396(03)00002-5
   FROSINI P, 2007, EUROGRAPHICS 2007 TU, P1025
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Geng X, 2007, LECT NOTES COMPUT SC, V4551, P587
   Gibbs J.W., 1898, NATURE, V59, P200, DOI DOI 10.1038/059200B0
   Gibbs J. W., 1899, Nature, V59, P606, DOI DOI 10.1038/059606A0
   GIORGI D, 2007, 9O7 IMATICNRGE
   GORDON WJ, 1978, MATH COMPUT
   Heczko M., 2002, DATENBANK SPEKTRUM, V2, P54
   Iyer N, 2005, COMPUT AIDED DESIGN, V37, P509, DOI 10.1016/j.cad.2004.07.002
   Jain V., 2006, SHAPE MODELING INT
   Jolliffe I. T., 2002, PRINCIPAL COMPONENT
   JU T, 2005, TOG SIGGRAPH, P561
   Kazhdan M., 2003, Symposium on geometry processing, V6
   Laga H, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P75
   MADEMLIS A, 2008, INTEMEDIA WORKSH HYP
   MADEMLIS A, 2008, WORKSH 3D OBJ RETR A
   Mémoli F, 2005, FOUND COMPUT MATH, V5, P313, DOI 10.1007/s10208-004-0145-y
   Memoli F., 2007, EUR S POINT BAS GRAP, P81, DOI DOI 10.2312/SPBG/SPBG07/081-090
   Memoli Facundo., 2004, SGP'04: Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing, P32, DOI DOI 10.1145/1057432.1057436
   MEYER M, 2002, P VISUAL MATH
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Paquet E, 1999, IMAGE VISION COMPUT, V17, P157, DOI 10.1016/S0262-8856(98)00119-X
   RUSTAMOV RM, 2009, 3DOR, P1
   RUSTAMOV RM, 2009, P 13 IMA INT C MATH, P307
   Saupe D., 2001, Pattern Recognition. 23rd DAGM Symposium. Proceedings (Lecture Notes in Computer Science Vol.2191), P392
   Shepard D., 1968, P 1968 23 ACM NAT C, P517, DOI DOI 10.1145/800186.810616
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0
   TAUBIN G, 1995, SIGGRAPH 95 C P, P351, DOI [DOI 10.1145/218380.218473, 10.1145/218380.218473]
   Vallet B., 2008, COMPUTER GRAPHICS FO
NR 42
TC 3
Z9 3
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2010
VL 26
IS 10
BP 1245
EP 1256
DI 10.1007/s00371-010-0518-y
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 649SI
UT WOS:000281793200002
DA 2024-07-18
ER

PT J
AU Tang, W
   Lagadec, P
   Gould, D
   Wan, TR
   Zhai, JH
   How, T
AF Tang, Wen
   Lagadec, Pierre
   Gould, Derek
   Wan, Tao Ruan
   Zhai, Jianhua
   How, Thien
TI A realistic elastic rod model for real-time simulation of minimally
   invasive vascular interventions
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference (CGI 2010)
CY JUN 08-12, 2010
CL Nanyang Technol Univ, Singapore, SINGAPORE
HO Nanyang Technol Univ
DE Surgical simulation and training; Interventional radiology;
   Physically-based computer animation; Discrete elastic rod model; Medical
   education
ID SKILLS
AB Simulating intrinsic deformation behaviors of guidewire and catheters for interventional radiology (IR) procedures, such as minimally invasive vascular interventions is a challenging task. Especially real-time simulations for interactive training systems require not only the accuracy of guidewire manipulations, but also the efficiency of computations. The insertion of guidewires and catheters is an essential task for IR procedures and the success of these procedures depends on the accurate navigation of guidewires in complex 3D blood vessel structures to a clinical target, whilst avoiding complications or mistakes of damaging vital tissues and blood vessel walls. In this paper, a novel elastic model for modeling guidewires is presented and evaluated. Our interactive guidewire simulator models the medical instrument as thin flexible elastic rods with arbitrary cross sections, treating the centerline as dynamic and the deformation as quasi-static. Constraints are used to enforce inextensibility of guidewires, providing an efficient computation for bending and twisting modes of the physically-based simulation model. We demonstrate the effectiveness of the new model with a number of simulation examples.
C1 [Wan, Tao Ruan] Univ Bradford, Sch Informat & Media, Bradford BD7 1DP, W Yorkshire, England.
   [Tang, Wen; Lagadec, Pierre] Univ Teesside, Sch Comp, Middlesbrough, Cleveland, England.
   [Gould, Derek; Zhai, Jianhua; How, Thien] Royal Liverpool Univ Hosp, Dept Radiol, Liverpool, Merseyside, England.
C3 University of Bradford; University of Teesside; Royal Liverpool &
   Broadgreen University Hospitals NHS Trust; Royal Liverpool University
   Hospital; University of Liverpool
RP Wan, TR (corresponding author), Univ Bradford, Sch Informat & Media, Bradford BD7 1DP, W Yorkshire, England.
EM w.tang@tees.ac.uk; pierre.lagadec@gmail.com; dgould@liverpool.ac.uk;
   t.wan@bradford.ac.uk; J.Zhai1@liverpool.ac.uk; thienhow@liverpool.ac.uk
CR Alderliesten T, 2007, IEEE T BIO-MED ENG, V54, P29, DOI 10.1109/TBME.2006.886659
   [Anonymous], THESIS UTRECHT U
   Bergou M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360662
   Bertails F, 2006, ACM T GRAPHIC, V25, P1180, DOI 10.1145/1141911.1142012
   Bertails F, 2009, COMPUT GRAPH FORUM, V28, P417, DOI 10.1111/j.1467-8659.2009.01381.x
   Botsch M, 2008, IEEE T VIS COMPUT GR, V14, P213, DOI 10.1109/TVCG.2007.1054
   Çavusoglu MC, 1999, IEEE T ROBOTIC AUTOM, V15, P728, DOI 10.1109/70.782027
   Cotin S, 2005, LECT NOTES COMPUT SC, V3750, P534, DOI 10.1007/11566489_66
   Cotin S, 2000, Stud Health Technol Inform, V70, P59
   Duindam V, 2008, IEEE INT CONF ROBOT, P2483, DOI 10.1109/ROBOT.2008.4543586
   FALK RS, 1995, SIAM J NUMER ANAL, V32, P1185, DOI 10.1137/0732055
   Hughes T. J. R., 2000, FINITE ELEMENT METHO
   LENOIR J, 2002, MODELLING SIMULATION, V12, P102
   Lin CC, 2005, SIAM J APPL MATH, V65, P720, DOI 10.1137/S0036139903431713
   Luboz V, 2008, LECT NOTES COMPUT SC, V5104, P215, DOI 10.1007/978-3-540-70521-5_25
   LUNDERQUIST A, 1995, CARDIOVASC INTER RAD, V18, P209
   NOWINSKI WL, 2001, MIAR, P87
   Ogata N, 1997, Interv Neuroradiol, V3, P65
   PAI DK, 2004, COMPUT GRAPH FORUM, V21, P3
   SCHRODER J, 1993, CARDIOVASC INTER RAD, V16, P93, DOI 10.1007/BF02602986
   Seymour NE, 2002, ANN SURG, V236, P458, DOI 10.1097/00000658-200210000-00008
   Spillmann J, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P63
   TANG W, 2009, P THEOR PRACT COMP G
   van der Heijden GHM, 2003, INT J MECH SCI, V45, P161, DOI 10.1016/S0020-7403(02)00183-2
   WEBSTER RJ, 2004, 9 INT S EXP ROB JUN
NR 25
TC 44
Z9 56
U1 1
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2010
VL 26
IS 9
BP 1157
EP 1165
DI 10.1007/s00371-010-0442-1
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 635JD
UT WOS:000280650500002
DA 2024-07-18
ER

PT J
AU Gui, Y
   Ma, LZ
AF Gui, Yan
   Ma, Lizhuang
TI Periodic pattern of texture analysis and synthesis based on texels
   distribution
SO VISUAL COMPUTER
LA English
DT Article
DE Texture analysis; Texture synthesis; Distribution of texels
ID IMAGE
AB Recently, sample-based texture synthesis techniques have drawn significant attention from researchers. These existing approaches mainly use the Markov Random Field (MRF) or texture features as texture model to analyze the local properties of sample textures. Indeed, human perception is sensitive to structure and periodicity. In this paper, we perform texture synthesis by taking into account the distribution of texels. Given a sample texture, the analysis procedure consists in segmenting texture into individual texels, and detecting each texel in order to analyze their neighborhood relationships by constructing connectivity. Then the synthesis process consists in reproducing a new large texture directly on a user-specified canvas by recomposing segmented texels, which synthesizes two-dimensional texel arrangements based on the previously constructed neighborhood relationships of texels. Results show that the proposed method is successful in generating textures visually indistinguishable to the sample textures. Moreover, the method especially deals with the near-regular textures, which well preserves underlying structural regularity.
C1 [Gui, Yan; Ma, Lizhuang] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200030, Peoples R China.
C3 Shanghai Jiao Tong University
RP Gui, Y (corresponding author), Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200030, Peoples R China.
EM guiyan122@sjtu.edu.cn
FU 863 Program of China [2009AA01Z334]; 973 Program of China [2006CB303105]
FX This work is supported by 863 Program of China (No. 2009AA01Z334), 973
   Program of China (No. 2006CB303105)
CR [Anonymous], 1995, SIGGRAPH
   Ashikhmin M., 2001, P 2001 S INT 3D GRAP, P217, DOI DOI 10.1145/364338.364405
   Barla P, 2006, COMPUT GRAPH FORUM, V25, P663, DOI 10.1111/j.1467-8659.2006.00986.x
   Cohen MF, 2003, ACM T GRAPHIC, V22, P287, DOI 10.1145/882262.882265
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Dischler JM, 2006, VISUAL COMPUT, V22, P926, DOI 10.1007/s00371-006-0077-4
   Dischler JM, 2002, COMPUT GRAPH FORUM, V21, P401, DOI 10.1111/1467-8659.t01-1-00600
   EFROS A, 2001, SIGGRAPH 01
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Harrison P, 2001, W S C G ' 2001, VOLS I & II, CONFERENCE PROCEEDINGS, P190
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Ijiri T, 2008, COMPUT GRAPH FORUM, V27, P429, DOI 10.1111/j.1467-8659.2008.01140.x
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Lefebvre S, 2005, ACM T GRAPHIC, V24, P777, DOI 10.1145/1073204.1073261
   Lefebvre S, 2006, ACM T GRAPHIC, V25, P541, DOI 10.1145/1141911.1141921
   LEUNG TK, 1996, ECCV, V1, P546
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Liang L, 2001, ACM T GRAPHIC, V20, P127, DOI 10.1145/501786.501787
   LIU Y, 2002, INT J COMPUT VISION, V62, P1
   Liu YX, 2004, ACM T GRAPHIC, V23, P368, DOI 10.1145/1015706.1015731
   Liu YX, 2004, IEEE T PATTERN ANAL, V26, P354, DOI 10.1109/TPAMI.2004.1262332
   ORCHARD MT, 1991, IEEE T SIGNAL PROCES, V39, P2677, DOI 10.1109/78.107417
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   Wu Q, 2004, ACM T GRAPHIC, V23, P364, DOI 10.1145/1015706.1015730
   ZHANG J, 2003, SIGGRAPH 03, P295
NR 29
TC 5
Z9 10
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 951
EP 964
DI 10.1007/s00371-010-0470-x
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800055
DA 2024-07-18
ER

PT J
AU Lee, H
   Han, S
AF Lee, Hyokwang
   Han, Soonhung
TI Solving the Shallow Water equations using 2D SPH particles for
   interactive applications
SO VISUAL COMPUTER
LA English
DT Article
DE Physics-based modeling; Shallow Water equations; SPH; Real-time
   simulation; GPU
ID SIMULATION; FLUID; ANIMATION; EFFICIENT; BODIES
AB In this paper, we introduce a 2D particle-based approach to achieve realistic water surface behaviors for interactive applications. We formulate 2D particle-based Shallow Water equations using the Smoothed Particle Hydrodynamics. Particles defined with specific amount of water volume interplay with each other, which generates the horizon flow and the water surface motion. By the application of the particle-based Lagrangian framework to the 2D Shallow Water simulation, our method allows the water particles to move freely without being confined to a grid. The motion of the particles can represent global flow with dynamic waves covering a large area while avoiding extensive 3D fluid dynamics computation. The 2D particle-based Shallow Water equations are straightforward and computed fast with the GPU-based implementation. Experiments on a standard hardware demonstrate the performance of our approach which is running on the GPU, and the results show a realistic motion of the water surface at interactive rates.
C1 [Lee, Hyokwang; Han, Soonhung] Korea Adv Inst Sci & Technol, Dept Mech Engn, Taejon, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST)
RP Lee, H (corresponding author), Korea Adv Inst Sci & Technol, Dept Mech Engn, 335 Gwahak Ro, Taejon, South Korea.
EM adpc9@kaist.ac.kr
RI han, soonhung/AAA-5745-2021; Han, Soonhung/C-2030-2011
OI han, soonhung/0000-0001-5676-8121; 
FU Ministry of Education, Science and Technology [R31-2008-000-10045-0]
FX This research was supported by WCU (World Class University) program
   through the National Research Foundation of Korea funded by the Ministry
   of Education, Science and Technology (R31-2008-000-10045-0).
CR AMADA T, 2004, ACM WORKSH GEN PURP, V41, P42
   Ata R, 2005, INT J NUMER METH FL, V47, P139, DOI 10.1002/fld.801
   Carlson M, 2004, ACM T GRAPHIC, V23, P377, DOI 10.1145/1015706.1015733
   Chen JX, 1997, IEEE COMPUT GRAPH, V17, P52, DOI 10.1109/38.586018
   CHEN JX, 1995, GRAPH MODEL IM PROC, V57, P107, DOI 10.1006/gmip.1995.1012
   Clavet S., 2005, SCA '05, P219, DOI DOI 10.1145/1073368.1073400
   Cords H, 2007, GRAPHITE 2007: 5TH INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES IN AUSTRALASIA AND SOUTHERN ASIA, PROCEEDINGS, P265
   DELEFFE M, 2008, ICHD 08
   Enright D, 2002, ACM T GRAPHIC, V21, P736, DOI [10.1145/566570.566581, 10.1145/566570.566645]
   Foster N, 2001, COMP GRAPH, P23, DOI 10.1145/383259.383261
   Fournier A., 1986, Computer Graphics, V20, P75, DOI 10.1145/15886.15894
   GOMEZ M, 2000, GAME PROGRAMMING GEM, P187
   GREEN S, 2007, NVIDIA CUDA SDK V2 2
   HINSINGER D, 2002, S COMP AN, P161
   Irving G, 2006, ACM T GRAPHIC, V25, P805, DOI 10.1145/1141911.1141959
   Kass M., 1990, Computer Graphics, V24, P49, DOI 10.1145/97880.97884
   Layton AT, 2002, VISUAL COMPUT, V18, P41, DOI 10.1007/s003710100131
   Liu GR., 2003, SMOOTHED PARTICLE HY, DOI 10.1142/5340
   Liu MB, 2002, SHOCK WAVES, V12, P181, DOI 10.1007/s00193-002-0163-0
   Losasso F, 2006, IEEE T VIS COMPUT GR, V12, P343, DOI 10.1109/TVCG.2006.51
   Losasso F, 2004, ACM T GRAPHIC, V23, P457, DOI 10.1145/1015706.1015745
   Losasso F, 2008, IEEE T VIS COMPUT GR, V14, P797, DOI 10.1109/TVCG.2008.37
   Mitchell JasonL., 2005, Real-time synthesis and rendering of ocean water
   MONAGHAN JJ, 1992, ANNU REV ASTRON ASTR, V30, P543, DOI 10.1146/annurev.aa.30.090192.002551
   Muller M., 2003, SCA, P154
   Muller M, 2005, P 2005 ACM SIGGRAPH, P237, DOI DOI 10.1145/1073368.1073402
   OBrien J.F., 1995, Proceedings of the Computer Animation, P198
   Paiva A, 2009, COMPUT AIDED DESIGN, V41, P306, DOI 10.1016/j.cad.2008.10.004
   Peachey D. R., 1986, Computer Graphics, V20, P65, DOI 10.1145/15886.15893
   Premoze S, 2003, COMPUT GRAPH FORUM, V22, P401, DOI 10.1111/1467-8659.00687
   Rodriguez-Paz M, 2005, COMPUT STRUCT, V83, P1396, DOI 10.1016/j.compstruc.2004.11.025
   Satish N, 2009, INT PARALL DISTRIB P, P257
   Schneider J., 2001, Vision, Modeling, and Visualization 2001. Proceedings, P211
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Takahashi T, 2003, COMPUT GRAPH FORUM, V22, P391, DOI 10.1111/1467-8659.00686
   TESSENDORF J, 2004, GAME PROGRAMMING GEM, P265
   Thürey N, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P39, DOI 10.1109/PG.2007.33
   THUREY N, 2006, SCA 2006, P157
   TSO PY, 1987, ACM T GRAPHIC, V6, P191, DOI 10.1145/35068.35070
   Yuksel C, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239550
NR 40
TC 20
Z9 30
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 865
EP 872
DI 10.1007/s00371-010-0439-9
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800046
DA 2024-07-18
ER

PT J
AU Ohmori, K
   Kunii, TL
AF Ohmori, Kenji
   Kunii, Tosiyasu L.
TI Designing and modeling cyberworlds using the incrementally modular
   abstraction hierarchy based on homotopy theory
SO VISUAL COMPUTER
LA English
DT Article
DE Homotopy theory; Software engineering; Cyberworlds; Top-down and
   bottom-up design; Homotopy lifting/extension properties; Pi-calculus;
   Event driven; Infinite state machine
AB For designing and modeling complicated and sophisticated systems such as cyberworlds, their mathematical foundation is critical. To realize it, two important properties called the homotopy lifting property (HLP) and homotopy extension property (HEP) are applied for designing and modeling a system in a bottom-up way and a top-down way, respectively. In this paper, an enterprise system and a real-time embedded system are considered as important socially emerging cases of cyberworlds, where the pi-calculus processes for describing these behaviors formally, a Petri net for explaining process interactions, and XMOS XC programs are modeled and designed by our approach. The spaces in both properties are specified by the incrementally modular abstraction hierarchy by climbing down the abstraction hierarchy from the most abstract homotopy level to the most specific view level, while keeping invariants such as homotopy equivalence and topological equivalence.
C1 [Ohmori, Kenji] Hosei Univ, Koganei, Tokyo 1848584, Japan.
   [Kunii, Tosiyasu L.] Univ Tokyo, Morpho Inc, Bunkyo Ku, Tokyo 1130033, Japan.
C3 Hosei University; University of Tokyo
RP Ohmori, K (corresponding author), Hosei Univ, 3-7-2 Kajinocho, Koganei, Tokyo 1848584, Japan.
EM ohmori@hosei.ac.jp; kunii@ieee.org
CR [Anonymous], 1999, Communicating and Mobile Systems: The Calculus
   Havey Michael., 2005, ESSENTIAL BUSINESS P
   HENNESSY M, 2001, DISTRIBUTED PICALCUL
   Kunii TL, 2005, IEICE T INF SYST, VE88D, P790, DOI 10.1093/ietisy/e88-d.5.790
   Kunii TL, 2006, VISUAL COMPUT, V22, P949, DOI 10.1007/s00371-006-0040-4
   May D, 2007, CONCUR SYST ENGN SER, V65, P21
   Ohmori K, 2007, ICEIS 2007: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON ENTERPRISE INFORMATION SYSTEMS, P437
   Ohmori K, 2007, 2007 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P100, DOI 10.1109/CW.2007.19
   Ohmori K, 2006, 2006 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P216, DOI 10.1109/CW.2006.14
   Ohmori K, 2009, 2009 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P80, DOI 10.1109/CW.2009.20
   Ohmori K, 2008, 2008 FIRST IEEE INTERNATIONAL CONFERENCE ON UBI-MEDIA COMPUTING AND WORKSHOPS, PROCEEDINGS, P69, DOI 10.1109/UMEDIA.2008.4570868
   Ohmori K, 2008, PROCEEDINGS OF THE 2008 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P363, DOI 10.1109/CW.2008.74
   PIERCE BC, 1997, 476 CSCI IND U, P1
   SANGIORGI D, 1999, PICALCULUS THEORY MO
   Sieradski A.J., 1992, INTRO TOPOLOGY HOMOT
   Spanier E., 1966, Algebraic Topology
NR 16
TC 9
Z9 9
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2010
VL 26
IS 5
BP 297
EP 309
DI 10.1007/s00371-010-0420-7
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 587GS
UT WOS:000276978800002
DA 2024-07-18
ER

PT J
AU Coll, N
   Madern, N
   Sellarès, JA
AF Coll, Narcis
   Madern, Narcis
   Sellares, J. Antoni
TI Good-visibility maps visualization
SO VISUAL COMPUTER
LA English
DT Article
DE Visibility; Good-visibility; Visualization; Graphics hardware
ID COMPUTATION
AB Given a set V of viewpoints and a set S of obstacles in an environmental space, the good-visibility depth of a point q in relation to V and S is a measure of how deep or central q is with respect to the points in V that see q while minding the obstacles of S. The good-visibility map determined by V and S is the subdivision of the environmental space in good-visibility regions where all points have the same fixed good-visibility depth. In this paper we present algorithms for computing and efficiently visualizing, using graphics hardware capabilities, good-visibility maps in the plane as well as on triangulated terrains, where the obstacles are the terrain faces. Finally, we present experimental results obtained with the implementation of our algorithms.
RP Madern, N (corresponding author), Edifici P-4,Campus Montilivi, Girona 17071, Spain.
EM nmadern@ima.udg.edu; coll@ima.udg.edu; sellares@ima.udg.edu
RI Sellares, J. Antoni/K-5001-2014
OI Sellares, J. Antoni/0000-0002-8764-7178; Coll,
   Narcis/0000-0001-8111-9158
FU Spanish Ministerio de Educacion y Ciencia [TIN2007-67982-C02-02]
FX The support of the Spanish Ministerio de Educacion y Ciencia under grant
   TIN2007-67982-C02-02 is gratefully acknowledged.
CR Abellanas M., 2005, International Conference on Numerical Analysis and Applied Mathematics 2005 ICNAAM 2005, P35
   ABELLANAS M, 2004, ACTA 4 JORNADAS MATE, P239
   ABELLANAS M, 2007, P 23 EUR WORKSH COMP, P61
   Abellanas M, 2007, LECT NOTES COMPUT SC, V4705, P1
   Aichholzer O., 2009, Proc. 25th European Workshop on Computational Geometry EuroCG '09, P167
   [Anonymous], GEN PURPOSE COMPUTAT
   ASANO T, 2000, VISIBILITY PLANE HDB
   BALABAN IJ, 1995, SCG 95 P 11 ANN S CO, P211
   Ben-Moshe B, 2008, GEOINFORMATICA, V12, P21, DOI 10.1007/s10707-006-0017-5
   CANALES S, 2004, THESIS U POLITECNICA
   Coll N, 2007, INT J GEOGR INF SCI, V21, P1115, DOI 10.1080/13658810701300097
   COLL N, 2008, P COMP GRAPH INT C C, P286
   FISHER I, 2006, P 18 ANN CAN C COMP
   Ghosh S.K., 2007, Visibility Algorithms in the Plane
   Katz M. J., 1992, Computational Geometry: Theory and Applications, V2, P223, DOI 10.1016/0925-7721(92)90024-M
   Krishnan S, 2002, SIAM PROC S, P558
   Krishnan S, 2006, DIMACS SER DISCRET M, V72, P223
   Miller K, 2003, STAT COMPUT, V13, P153, DOI 10.1023/A:1023208625954
   MILLER K, 2003, FAST IMPLEMENTATION, P153
   OKAMOTO Y, 2008, SWAT 08, P77
   ORourke J., 2004, Handbook of Discrete and Computational Geometry, Vsecond, P643, DOI [10.1201/9781420035315.ch28, DOI 10.1201/9781420035315.CH28]
   Owens JD, 2007, COMPUT GRAPH FORUM, V26, P80, DOI 10.1111/j.1467-8659.2007.01012.x
   REIF JH, 1988, SCG 88, P193
   Wang YC, 2008, IEEE T MOBILE COMPUT, V7, P262, DOI 10.1109/TMC.2007.70708
NR 24
TC 2
Z9 2
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2010
VL 26
IS 2
BP 109
EP 120
DI 10.1007/s00371-009-0380-y
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 543RE
UT WOS:000273595200003
DA 2024-07-18
ER

PT J
AU Li, Q
   Yin, WT
   Deng, ZG
AF Li, Qing
   Yin, Wotao
   Deng, Zhigang
TI Image-based face illumination transferring using logarithmic total
   variation models
SO VISUAL COMPUTER
LA English
DT Article
DE Illumination transferring; Face relighting; Logarithmic total variation
   model; Radial basis functions; Face decomposition
AB In this paper, we present a novel image-based technique that transfers illumination from a source face image to a target face image based on the Logarithmic Total Variation (LTV) model. Our method does not require any prior information regarding the lighting conditions or the 3D geometries of the underlying faces. We first use a Radial Basis Functions (RBFs)-based deformation technique to align key facial features of the reference 2D face with those of the target face. Then, we employ the LTV model to factorize each of the two aligned face images to an illumination-dependent component and an illumination-invariant component. Finally, illumination transferring is achieved by replacing the illumination-dependent component of the target face by that of the reference face. We tested our technique on numerous grayscale and color face images from various face datasets including the Yale face Database, as well as the application of illumination-preserved face coloring.
C1 [Li, Qing; Deng, Zhigang] Univ Houston, Dept Comp Sci, Comp Graph & Interact Media Lab, Houston, TX 77204 USA.
   [Yin, Wotao] Rice Univ, Dept Computat & Appl Math, Houston, TX USA.
C3 University of Houston System; University of Houston; Rice University
RP Deng, ZG (corresponding author), Univ Houston, Dept Comp Sci, Comp Graph & Interact Media Lab, Houston, TX 77204 USA.
EM zdeng@cs.uh.edu
RI Yin, Wotao/A-5472-2011
OI Yin, Wotao/0000-0001-6697-9731; Deng, Zhigang/0000-0002-0452-8676; Deng,
   Zhigang/0000-0003-2571-5865
FU University of Houston; Texas Norman Hackerman Advanced Research Program
   [003652-0058-2007]; Dean of Engineering of Rice University; Division Of
   Mathematical Sciences; Direct For Mathematical & Physical Scien
   [0748839] Funding Source: National Science Foundation
FX The authors would like to thank Baez Jose, Yun Chang, Qin Gu, Tanasai
   Sucontphunt at the Computer Graphics and Interactive Media (CGIM) Lab at
   the University of Houston for their data acquisition help. This research
   was partially supported by a University of Houston faculty startup fund
   (Dr. Zhigang Deng) and the Texas Norman Hackerman Advanced Research
   Program (project number: 003652-0058-2007). W. Yin was supported by an
   internal faculty grant from the Dean of Engineering of Rice University.
CR ARAD N, 1994, CVGIP-GRAPH MODEL IM, V56, P161, DOI 10.1006/cgip.1994.1015
   Chen T, 2006, IEEE T PATTERN ANAL, V28, P1519, DOI 10.1109/TPAMI.2006.195
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Deng Z., 2006, P 2006 S INT 3D GRAP, P43, DOI DOI 10.1145/1111411.1111419]
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   GOLDFARB D, 2007, TR0709 CAAM RIC U
   Hua G, 2007, IEEE I CONF COMP VIS, P229
   Lewis JP, 2000, COMP GRAPH, P165, DOI 10.1145/344779.344862
   Liu ZC, 2001, COMP GRAPH, P271
   Marschner SR, 1997, FIFTH COLOR IMAGING CONFERENCE: COLOR SCIENCE, SYSTEMS, AND APPLICATIONS, P262
   Martinez A., 1998, AR FACE DATABASE
   Nikolova M, 2004, J MATH IMAGING VIS, V20, P99, DOI 10.1023/B:JMIV.0000011920.58935.9c
   Noh JY, 2001, COMP GRAPH, P277, DOI 10.1145/383259.383290
   NOH JY, 2000, VRST 2000, P166, DOI DOI 10.1145/502390.502422
   Peers P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239503
   Stein GP, 1999, IEEE T PATTERN ANAL, V21, P244, DOI 10.1109/34.754590
   STOSCHEK A, 2000, P IEEE COMP SOC C CO, V1, P582
   WEN Z, 2003, COMP VIS PATT REC P, P157
   Yin W, 2007, MULTISCALE MODEL SIM, V6, P190, DOI 10.1137/060663027
   Zhou Y, 2003, PROC CVPR IEEE, P109
NR 20
TC 20
Z9 29
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2010
VL 26
IS 1
BP 41
EP 49
DI 10.1007/s00371-009-0375-8
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 526WD
UT WOS:000272325300004
DA 2024-07-18
ER

PT J
AU Zhang, GF
   Hua, W
   Qin, XY
   Shao, YL
   Bao, HJ
AF Zhang, Guofeng
   Hua, Wei
   Qin, Xueying
   Shao, Yuanlong
   Bao, Hujun
TI Video stabilization based on a 3D perspective camera model
SO VISUAL COMPUTER
LA English
DT Article
DE Video stabilization; Structure from motion; Optimization; View warping;
   Warping error
AB This paper presents a novel approach to stabilize video sequences based on a 3D perspective camera model. Compared to previous methods which are based on simplified models, our stabilization system can work in situations where significant depth variations exist in the scenes and the camera undergoes large translational movement. We formulate the stabilization problem as a quadratic cost function on smoothness and similarity constraints. This allows us to precisely control the smoothness by solving a sparse linear system of equations. By taking advantage of the sparseness, our optimization process is very efficient. Instead of recovering dense depths, we use approximate geometry representation and analyze the resulting warping errors. We show that by appropriately constraining warping error, visually plausible results can be achieved even using planar structures. A variety of experiments have been implemented, which demonstrates the robustness and efficiency of our approach.
C1 [Zhang, Guofeng; Hua, Wei; Qin, Xueying; Shao, Yuanlong; Bao, Hujun] Zhejiang Univ, State Lab CAD&CG, Hangzhou, Zhejiang, Peoples R China.
   [Qin, Xueying] Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Peoples R China.
C3 Zhejiang University; Shandong University
RP Qin, XY (corresponding author), Zhejiang Univ, State Lab CAD&CG, Hangzhou, Zhejiang, Peoples R China.
EM zhangguofeng@cad.zju.edu.cn; huawei@cad.zju.edu.cn;
   xyqin@cad.zju.edu.cn; shaoyuanlong@cad.zju.edu.cn; bao@cad.zju.edu.cn
RI Qin, Xueying/AAM-8775-2021; shao, yuanlong/J-4897-2017
OI Qin, Xueying/0000-0003-0057-295X; 
FU NSF of China [60633070]; 973 program of China [2009CB320802]; 863
   program of China [2007AA01Z326]
FX We would like to thank the anonymous reviewers for their constructive
   comments to improve the paper, and also thank Prof. Xiaofei He and Wei
   Chen for their enormous help in revising this paper. This work is
   supported by NSF of China (No. 60633070), 973 program of China ( No.
   2009CB320802), and 863 program of China No. 2007AA01Z326).
CR Barrett R, 1994, TEMPLATES SOLUTION L, DOI DOI 10.1137/1.9781611971538
   BHAT P, 2007, RENDERING TECHNIQUES, P327
   Buehler C, 2001, PROC CVPR IEEE, P609
   DAVIS LS, 1994, P ARPA IM UND WORKSH, P435
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fitzgibbon AW, 2003, INT S VIDEO COMP, V5, P18
   HANSEN M, 1994, P ARPA IM UND WORKSH, P457
   Hartley R, 2000, MULTIPLE VIEW GEOMET
   Litvin A, 2003, PROC SPIE, V5022, P663, DOI 10.1117/12.476436
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mark W. R., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P7, DOI 10.1145/253284.253292
   Matsushita Y, 2005, PROC CVPR IEEE, P50
   Morimoto C, 1998, INT CONF ACOUST SPEE, P2789, DOI 10.1109/ICASSP.1998.678102
   MORIMOTO C, 1997, CVPR, P660, DOI DOI 10.1109/CVPR.1997.609396
   Nistér D, 2005, MACH VISION APPL, V16, P321, DOI 10.1007/s00138-005-0006-y
   PILU M, 2004, CVPR, V1, P625
   Pollefeys M, 2004, INT J COMPUT VISION, V59, P207, DOI 10.1023/B:VISI.0000025798.50602.3a
   Shade J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P231, DOI 10.1145/280814.280882
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298
   Yao YS, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pA191
   Zhang G., 2007, CVPR
   Zhang ZY, 1998, INT J COMPUT VISION, V27, P161, DOI 10.1023/A:1007941100561
   Zhu Z., 1998, IEEE INT C INTELLIGE, P329
NR 24
TC 50
Z9 78
U1 2
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2009
VL 25
IS 11
BP 997
EP 1008
DI 10.1007/s00371-009-0310-z
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 508EQ
UT WOS:000270911700003
DA 2024-07-18
ER

PT J
AU Eler, DM
   Nakazaki, MY
   Paulovich, FV
   Santos, DP
   Andery, GF
   Oliveira, MCF
   Neto, JB
   Minghim, R
AF Eler, Danilo M.
   Nakazaki, Marcel Y.
   Paulovich, Fernando V.
   Santos, Davi P.
   Andery, Gabriel F.
   Oliveira, Maria Cristina F.
   Batista Neto, Joao
   Minghim, Rosane
TI Visual analysis of image collections
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 21st Brazilian Symposium on Computer Graphics and Image Processing
CY OCT 12-15, 2008
CL Campo Grande, BRAZIL
SP Brazilian Comput Soc
DE Visual data mining; Image analysis; Biomedical imaging and visualization
ID PROJECTION; VISUALIZATION; DISTANCE
AB Multidimensional Visualization techniques are invaluable tools for analysis of structured and unstructured data with variable dimensionality. This paper introduces PEx-Image-Projection Explorer for Images-a tool aimed at supporting analysis of image collections. The tool supports a methodology that employs interactive visualizations to aid user-driven feature detection and classification tasks, thus offering improved analysis and exploration capabilities. The visual mappings employ similarity-based multidimensional projections and point placement to layout the data on a plane for visual exploration. In addition to its application to image databases, we also illustrate how the proposed approach can be successfully employed in simultaneous analysis of different data types, such as text and images, offering a common visual representation for data expressed in different modalities.
C1 [Eler, Danilo M.; Nakazaki, Marcel Y.; Paulovich, Fernando V.; Santos, Davi P.; Andery, Gabriel F.; Oliveira, Maria Cristina F.; Batista Neto, Joao; Minghim, Rosane] Univ Sao Paulo, Inst Ciencias Matemat & Computacao, Sao Carlos, SP, Brazil.
C3 Universidade de Sao Paulo
RP Minghim, R (corresponding author), Univ Sao Paulo, Inst Ciencias Matemat & Computacao, Av Trabalhador Sao Carlense 400, Sao Carlos, SP, Brazil.
EM rminghim@icmc.usp.br
RI Minghim, Rosane/JBR-8763-2023; Macc, Inct/K-3440-2013; Ferreira de
   Oliveira, Maria Cristina/D-9257-2011; Pereira-Santos,
   Davi/IAO-7054-2023; Oliveira, Maria/ISB-2741-2023; Eler, Danilo
   Medeiros/M-3320-2019; Paulovich, Fernando/G-1329-2010; Minghim,
   Rosane/E-5703-2011
OI Ferreira de Oliveira, Maria Cristina/0000-0002-4729-5104; Eler, Danilo
   Medeiros/0000-0002-9493-145X; Paulovich, Fernando/0000-0002-2316-760X;
   Minghim, Rosane/0000-0002-4799-8774
CR ALTSCHUL SF, 1990, J MOL BIOL, V215, P403, DOI 10.1016/S0022-2836(05)80360-2
   [Anonymous], P ANN INT ACM SIGIR
   Castellano G, 2000, NEUROCOMPUTING, V31, P1, DOI 10.1016/S0925-2312(99)00146-0
   CHEN C, 2000, 16 IFIP WORLD COMP C, P206
   Cilibrasi R, 2005, IEEE T INFORM THEORY, V51, P1523, DOI 10.1109/TIT.2005.844059
   Cuadros AM, 2007, IEEE CONF VIS ANAL, P99, DOI 10.1109/VAST.2007.4389002
   da Silva LA, 2007, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON INTELLIGENT SYSTEMS DESIGN AND APPLICATIONS, P353, DOI 10.1109/ISDA.2007.100
   Eler DM, 2008, IEEE INT CONF INF VI, P246, DOI 10.1109/IV.2008.39
   Garson G.D., 1991, AI EXPERT, V6, P46, DOI [DOI 10.5555/129449.129452, 10.5555/129449.129452]
   Goldberger J, 2006, IEEE T IMAGE PROCESS, V15, P449, DOI 10.1109/TIP.2005.860593
   HEIJS A, 2007, P 5 INT C COORD MULT, P76, DOI DOI 10.1109/CMV.2007.19
   HUANG K, 2006, P ICIP, P2141
   Jain A, 1997, IEEE T PATTERN ANAL, V19, P153, DOI 10.1109/34.574797
   Kim KI, 2002, IEEE T PATTERN ANAL, V24, P1542, DOI 10.1109/TPAMI.2002.1046177
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803
   Minghim R, 2006, PROC SPIE, V6060, DOI 10.1117/12.650880
   Moghaddam B, 2004, INT J COMPUT VISION, V56, P109, DOI 10.1023/B:VISI.0000004834.62090.74
   Nath R, 1997, COMPUT OPER RES, V24, P767, DOI 10.1016/S0305-0548(96)00088-3
   Paulovich FV, 2007, SIBGRAPI, P27, DOI 10.1109/SIBGRAPI.2007.21
   Paulovich FV, 2008, IEEE T VIS COMPUT GR, V14, P564, DOI 10.1109/TVCG.2007.70443
   Phillips D C, 1970, Biochem Soc Symp, V30, P11
   Pietal MJ, 2007, BIOINFORMATICS, V23, P1429, DOI 10.1093/bioinformatics/btm124
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Santos DP, 2007, SIBGRAPI, P253, DOI 10.1109/SIBGRAPI.2007.17
   Schvaneveldt R.W. )., 1990, Pathfinder Associative Networks: Studies in Knowledge Organization
   Telles GP, 2007, COMPUT GRAPH-UK, V31, P327, DOI 10.1016/j.cag.2007.01.024
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Tuceryan M., 1998, HDB PATTERN RECOGNIT, V2nd
   Venkatesh YV, 2003, PATTERN RECOGN, V36, P2161, DOI 10.1016/S0031-3203(03)00013-X
   Yang L, 2004, IEEE T PATTERN ANAL, V26, P1243, DOI 10.1109/TPAMI.2004.66
NR 30
TC 28
Z9 28
U1 3
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2009
VL 25
IS 10
SI SI
BP 923
EP 937
DI 10.1007/s00371-009-0368-7
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 500TP
UT WOS:000270328200004
DA 2024-07-18
ER

PT J
AU Agus, M
   Bettio, F
   Giachetti, A
   Gobbetti, E
   Guitián, JAI
   Marton, F
   Nilsson, J
   Pintore, G
AF Agus, Marco
   Bettio, Fabio
   Giachetti, Andrea
   Gobbetti, Enrico
   Guitian, Jose Antonio Iglesias
   Marton, Fabio
   Nilsson, Jonas
   Pintore, Giovanni
TI An interactive 3D medical visualization system based on a light field
   display
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Workshop on 3D Physiological Human
CY DEC, 2008
CL Zermatt, SWITZERLAND
DE 3D displays; Volume rendering; Medical data visualization
ID VOLUMETRIC 3-DIMENSIONAL DISPLAY; AUTOSTEREOSCOPIC DISPLAY; STEREO
AB We present a prototype medical data visualization system exploiting a light field display and custom direct volume rendering techniques to enhance understanding of massive volumetric data, such as CT, MRI, and PET scans. The system can be integrated with standard medical image archives and extends the capabilities of current radiology workstations by supporting real-time rendering of volumes of potentially unlimited size on light field displays generating dynamic observer-independent light fields. The system allows multiple untracked naked-eye users in a sufficiently large interaction area to coherently perceive rendered volumes as real objects, with stereo and motion parallax cues. In this way, an effective collaborative analysis of volumetric data can be achieved. Evaluation tests demonstrate the usefulness of the generated depth cues and the improved performance in understanding complex spatial structures with respect to standard techniques.
C1 [Agus, Marco; Bettio, Fabio; Giachetti, Andrea; Gobbetti, Enrico; Guitian, Jose Antonio Iglesias; Marton, Fabio; Nilsson, Jonas; Pintore, Giovanni] CRS4 Visual Comp Grp, I-09010 Pula, Italy.
RP Gobbetti, E (corresponding author), CRS4 Visual Comp Grp, Sardegna Ric Edificio 1,CP 25, I-09010 Pula, Italy.
EM gobbetti@crs4.it
RI Agus, Marco/AAM-5898-2020; Pintore, Giovanni/AFV-0023-2022; Giachetti,
   Andrea/AAD-8247-2020; Gobbetti, Enrico/O-2188-2015; Guitian, Jose A.
   Iglesias/A-9718-2017; Marton, Fabio/KBC-4179-2024
OI Pintore, Giovanni/0000-0001-8944-1045; Gobbetti,
   Enrico/0000-0003-0831-2458; Guitian, Jose A.
   Iglesias/0000-0002-0817-1010; Giachetti, Andrea/0000-0002-7523-6806;
   Agus, Marco/0000-0003-2752-3525; Marton, Fabio/0000-0001-8611-1921
CR Agus M, 2008, COMPUT GRAPH FORUM, V27, P231, DOI 10.1111/j.1467-8659.2008.01120.x
   Bettio F, 2008, COMPUT GRAPH-UK, V32, P55, DOI 10.1016/j.cag.2007.11.002
   BOUCHENY C, 2007, P ACM APGV, P83
   Bruckner S, 2007, COMPUT GRAPH FORUM, V26, P715, DOI 10.1111/j.1467-8659.2007.01095.x
   Cossairt OS, 2007, APPL OPTICS, V46, P1244, DOI 10.1364/AO.46.001244
   Dodgson N. A., 2000, Journal of the Society for Information Display, V8, P169, DOI 10.1889/1.1828713
   Dodgson NA, 1996, APPL OPTICS, V35, P1705, DOI 10.1364/AO.35.001705
   Favalora G, 2001, P SOC PHOTO-OPT INS, V4297, P227, DOI 10.1117/12.430821
   Favalora GE, 2005, COMPUTER, V38, P37, DOI 10.1109/MC.2005.276
   Forgacs T., 2005, Eurographics (Short Presentations), P109, DOI [10.2312/egs.20051036, DOI 10.2312/EGS.20051036]
   Gobbetti E, 2008, VISUAL COMPUT, V24, P797, DOI 10.1007/s00371-008-0261-9
   Huebschman ML, 2003, OPT EXPRESS, V11, P437, DOI 10.1364/OE.11.000437
   Jones A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276427
   Kersten MA, 2006, IEEE T VIS COMPUT GR, V12, P1117, DOI 10.1109/TVCG.2006.139
   McKay S, 2000, P SOC PHOTO-OPT INS, V3957, P198, DOI 10.1117/12.384443
   Mora B, 2004, COMPUT GRAPH FORUM, V23, P489, DOI 10.1111/j.1467-8659.2004.00780.x
   NAPOLI J, 2008, RAD THERAPY PLANNING
   RAAP GB, 2007, CARDIOVASC ULTRASOUN, V5
   RELKE I, 2005, P SPIE, V5664
   Roberts JW, 2000, P SOC PHOTO-OPT INS, V3957, P128, DOI 10.1117/12.384436
   Stanley M, 2000, PROC SPIE, V3956, P13, DOI 10.1117/12.379994
   STHILLAIRE P, 1995, P 5 SPIE S DISPL HOL, P374
   vanBerkel G, 1996, P SOC PHOTO-OPT INS, V2653, P32, DOI 10.1117/12.237437
   Ware C, 1996, ACM T GRAPHIC, V15, P121, DOI 10.1145/234972.234975
   Woodgate GJ, 2000, P SOC PHOTO-OPT INS, V3957, P153, DOI 10.1117/12.384438
   Yang R, 2008, IEEE T VIS COMPUT GR, V14, P84, DOI 10.1109/70410
NR 26
TC 17
Z9 18
U1 2
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2009
VL 25
IS 9
BP 883
EP 893
DI 10.1007/s00371-009-0311-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 478KA
UT WOS:000268585100008
DA 2024-07-18
ER

PT J
AU Miandji, E
   Moghadam, MHS
   Samavati, FF
   Emadi, M
AF Miandji, E.
   Moghadam, M. H. Sargazi
   Samavati, Faramarz F.
   Emadi, M.
TI Real-time multi-band synthesis of ocean water with new iterative
   up-sampling technique
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Real-time rendering; Wave synthesis; Image up-sampling; Programmable
   graphics hardware
AB Adapting natural phenomena rendering for real-time applications has become a common practice in computer graphics. We propose a GPU-based multi-band method for optimized synthesis of "far from coast" ocean waves using an empirical Fourier domain model. Instead of performing two independent syntheses for low- and high-band frequencies of ocean waves, we perform only low-band synthesis and employ results to reproduce high frequency details of ocean surface by an optimized iterative up-sampling stage. Our experimental results show that this approach greatly improves the performance of original multi-band synthesis while maintaining image quality.
C1 [Miandji, E.] Sharif Univ Technol, VR Lab, Tehran, Iran.
   [Samavati, Faramarz F.] Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
C3 Sharif University of Technology; University of Calgary
RP Miandji, E (corresponding author), Sharif Univ Technol, VR Lab, Tehran, Iran.
EM E.Miandji@GMail.com; mhadi_sargazim@ee.sharif.edu; samavati@ucalgary.ca;
   m_emadi@ee.sharif.edu
RI Miandji, Ehsan/GPP-6894-2022; Miandji, Ehsan/ACN-7116-2022
OI Miandji, Ehsan/0000-0002-4435-6784; 
CR BENEZRA M, 2007, IEEE INT C COMP VIS
   CHAN TF, 2007, IMS I MATH SCI LECT
   COOLEY JW, 1965, MATH COMPUT, V19, P297, DOI 10.2307/2003354
   FENG L, 2008, CVPR 2008
   Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956
   Fournier A., 1986, Computer Graphics, V20, P75, DOI 10.1145/15886.15894
   JENSEN LS, 2001, GAM DEV C EUR
   KRYACHKO Y, 2005, GPU GEMS, V2
   LANZA S, 2004, ANIMATION DISPLAY WA
   Levin A., 2006, ADV NEURAL INFORM PR
   MARVASTI FA, 1989, IEEE T ACOUST SPEECH, V37, P1617, DOI 10.1109/29.35407
   MASTIN GA, 1987, IEEE COMPUT GRAPH, V7, P16, DOI 10.1109/MCG.1987.276961
   MESSING KS, 2008, Patent No. 1947603
   Mitchell JasonL., 2005, Real-time synthesis and rendering of ocean water
   MITCHELL JL, 2004, ADV IMAGE PROCESSING
   PATHAK B, 2004, Patent No. 6747630
   PIOTR W, 2008, P ICCS, P721
   Tessendorf J., 1999, SIGGRAPH 1999
   THON S, 2000, P INT C COMP GRAPH
NR 19
TC 5
Z9 10
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 697
EP 705
DI 10.1007/s00371-009-0352-2
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300035
DA 2024-07-18
ER

PT J
AU Yuksel, C
   Keyser, J
AF Yuksel, Cem
   Keyser, John
TI Fast real-time caustics from height fields
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Caustics; Real-time water rendering; GPU algorithms; Height fields
AB Caustics are crucial in water rendering, yet they are often neglected in real-time applications due to the demanding computational requirements of the general-purpose caustics computation methods. In this paper we present a two-pass algorithm for caustics computation that is extremely fast and produces high-quality results. Our algorithm is targeted for commonly used height field representations of water and a planar caustic-receiving surface. The underlying theory of our approach is presented along with implementation details and pseudo codes.
C1 [Yuksel, Cem; Keyser, John] Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX USA.
C3 Texas A&M University System; Texas A&M University College Station
RP Yuksel, C (corresponding author), Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX USA.
EM cem@cemyuksel.com; keyser@cs.tamu.edu
OI Keyser, John/0000-0002-4829-9975
CR [Anonymous], P ACM S INT 3D GRAPH
   Arvo J., 1986, ACM SIGGRAPH 86 COUR
   ERNST M, 2005, GI 05, P87
   GUARDADO J, 2004, GPU GEMS PROGRAMMING, pCH2
   INAKAGE M, 1986, ACM SIGGRAPH 86 COUR
   Iwasaki K, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P374, DOI 10.1109/PCCGA.2001.962894
   Jensen HW., 2001, REALISTIC IMAGE SYNT, DOI [10.1201/9780429294907, DOI 10.1201/9780429294907]
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   MITCHELL D, 1992, P SIGGRAPH 92, P283
   NISHITA T, 1994, P SIGGRAPH 94, P373
   Shah MA, 2007, IEEE T VIS COMPUT GR, V13, P272, DOI 10.1109/TVCG.2007.32
   STAM J, 1996, P SIGGRAPH 96, P150
   Szirmay-Kalos L, 2005, COMPUT GRAPH FORUM, V24, P695, DOI 10.1111/j.1467-8659.2005.0m894.x
   WATT M, 1990, P SIGGRAPH 90, P377
   Wyman Chris., 2006, I3D 06, P153
   [No title captured]
NR 16
TC 8
Z9 8
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 559
EP 564
DI 10.1007/s00371-009-0350-4
PG 6
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300020
DA 2024-07-18
ER

PT J
AU Pietroni, N
   Ganovelli, F
   Cignoni, P
   Scopigno, R
AF Pietroni, Nico
   Ganovelli, Fabio
   Cignoni, Paolo
   Scopigno, Roberto
TI Splitting cubes: a fast and robust technique for virtual cutting
SO VISUAL COMPUTER
LA English
DT Article
DE Physically based modeling; Three-dimensional graphics and realism;
   Animation
AB This paper presents the splitting cubes, a fast and robust technique for performing interactive virtual cutting on deformable objects.
   The technique relies on two ideas. The first one is to embed the deformable object in a regular grid, to apply the deformation function to the grid nodes and to interpolate the deformation inside each cell from its 8 nodes. The second idea is to produce a tessellation for the boundary of the object on the base of the intersections of such boundary with the edges of the grid. Please note that the boundary can be expressed in any way; for example it can be a triangle mesh, an implicit or a parametric surface. The only requirement is that the intersection between the boundary and the grid edges can be computed. This paper shows how the interpolation of the deformation inside the cells can be used to produce discontinuities in the deformation function, and the intersections of the cut surface can be used to visually show the cuts on the object.
   The splitting cubes is essentially a tessellation algorithm for growing, deformable surface, and it can be applied to any method for animating deformable objects. In this paper the case of the mesh-free methods (MMs) is considered: in this context, we described a practical GPU friendly method, that we named the extended visibility criterion, to introduce discontinuities of the deformation.
C1 [Pietroni, Nico; Ganovelli, Fabio; Cignoni, Paolo; Scopigno, Roberto] ISTI CNR Pisa, Visual Comp Lab, Pisa, Italy.
   [Pietroni, Nico] Endocas Ctr Comp Assisted Surg Pisa, Pisa, Italy.
C3 Consiglio Nazionale delle Ricerche (CNR); Istituto di Scienza e
   Tecnologie dell'Informazione "Alessandro Faedo" (ISTI-CNR)
RP Pietroni, N (corresponding author), ISTI CNR Pisa, Visual Comp Lab, Pisa, Italy.
EM nico.pietroni@isti.cnr.it; fabio.ganovelli@isti.cnr.it;
   paolo.cignoni@isti.cnr.it; roberto.scopigno@isti.cnr.it
RI scopigno, roberto/AAH-7645-2020; Cignoni, Paolo/B-7192-2012
OI Cignoni, Paolo/0000-0002-2686-8567; PIETRONI, NICO/0000-0002-8271-2102
CR Belytschko T, 1996, J COMPUT APPL MATH, V74, P111, DOI 10.1016/0377-0427(96)00020-9
   BELYTSCHKO T, 1994, INT J NUMER METH ENG, V37, P229, DOI 10.1002/nme.1620370205
   Bielser D, 2000, EIGHTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P116, DOI 10.1109/PCCGA.2000.883933
   Bielser D, 1999, COMPUT GRAPH FORUM, V18, pC31, DOI 10.1111/1467-8659.00325
   Bruyns CD, 2001, COMPUT GRAPH-UK, V25, P635, DOI 10.1016/S0097-8493(01)00092-9
   Delingette H, 1999, COMP ANIM CONF PROC, P70, DOI 10.1109/CA.1999.781200
   Ganovelli F, 2000, COMPUT GRAPH FORUM, V19, pC271, DOI 10.1111/1467-8659.00419
   GANOVELLI F, 2001, EUROGRAPHICS SHORT P
   Garland M., 1999, THESIS CARNEGIE MELL
   Ju T, 2002, ACM T GRAPHIC, V21, P339
   Kobbelt LP, 2001, COMP GRAPH, P57, DOI 10.1145/383259.383265
   Li SZ, 2004, ITCC 2004: INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: CODING AND COMPUTING, VOL 2, PROCEEDINGS, P288, DOI 10.1109/ITCC.2004.1286648
   LIM YJ, 2004, HAPTICS, P295
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Molino N, 2004, ACM T GRAPHIC, V23, P385, DOI 10.1145/1015706.1015734
   MULLER M, 2004, P ACM SIGGRAPH EUROG
   NIENHUY HW, 2003, THESIS UTRECHT U
   O'Brien JF, 2002, ACM T GRAPHIC, V21, P291, DOI 10.1145/566570.566579
   O'Brien JF, 1999, COMP GRAPH, P137, DOI 10.1145/311535.311550
   Organ D, 1996, COMPUT MECH, V18, P225
   Pauly M, 2005, ACM T GRAPHIC, V24, P957, DOI 10.1145/1073204.1073296
   Pauly M, 2003, ACM T GRAPHIC, V22, P641, DOI 10.1145/882262.882319
   Sifakis E, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P73
   STEINEMANN D, 2006, IEEE VR 2006
   STEINEMANN D, 2006, EUR SIGGRAPH S COMP
   TANAKA A, 1998, VRAIS 98, P71
   *VISUALCOMPUTING L, 2005, ID INT DEF OBJ LIB
   Wyvill G., 1986, Visual Computer, V2, P227, DOI 10.1007/BF01900346
NR 28
TC 31
Z9 37
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2009
VL 25
IS 3
BP 227
EP 239
DI 10.1007/s00371-008-0216-1
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 403RH
UT WOS:000263099200003
DA 2024-07-18
ER

PT J
AU Xiao, CX
   Zheng, WT
   Miao, YW
   Zhao, Y
   Peng, QS
AF Xiao, Chunxia
   Zheng, Wenting
   Miao, Yongwei
   Zhao, Yong
   Peng, Qunsheng
TI A unified method for appearance and geometry completion of point set
   surfaces
SO VISUAL COMPUTER
LA English
DT Article
DE texture synthesis; texture completion; geometry completion; geometric
   detail
AB This paper presents a novel approach for appearance and geometry completion over point-sampled geometry. Based on the result of surface clustering and a given texture sample, we define a global texture energy function on the point set surface for direct texture synthesis. The color texture completion is performed by minimizing a constrained global energy using the existing surface texture on the surface as the input texture sample. We convert the problem of context-based geometry completion into a task of texture completion on the surface. The geometric detail is then peeled and converted into a piece of signed gray-scale texture on the base surface of the point set surface. We fill the holes on the base surface by smoothed extrapolation and the geometric details over these patches are reconstructed by a process of gray-scale texture completion. Experiments show that our method is flexible, efficient and easy to implement. It provides a practical texture synthesis and geometry completion tool for 3D point set surfaces.
C1 Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
   Wuhan Univ, Comp Sch, Wuhan 430072, Peoples R China.
C3 Zhejiang University; Wuhan University
RP Xiao, CX (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
EM cxxiao@cad.zju.edu.cn; wtzheng@cad.zju.edu.cn; miaoyw@cad.zju.edu.cn;
   zhaoyong@cad.zju.edu.cn; peng@cad.zju.edu.cn
RI zhang, jt/JVE-1333-2024; Miao, Yongwei/ABH-1238-2021
OI Miao, Yongwei/0000-0002-5479-9060
CR Alexa M, 2003, WSCG'2003, VOL 11, NO 1, CONFERENCE PROCEEDINGS, P27
   [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], 2004, Proceedings of Eurographics Symposium on Point-Based Graphics 2004
   [Anonymous], 2005, P ACM S SOL PHYS MOD
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   Clarenz U, 2004, COMPUT AIDED GEOM D, V21, P427, DOI 10.1016/j.cagd.2004.02.004
   Davis J, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P428, DOI 10.1109/TDPVT.2002.1024098
   Dellaert F, 2005, PROC CVPR IEEE, P619
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Hoppe Hugues, 1992, P SIGGRAPH 92, P71, DOI DOI 10.1145/133994.134011
   Ju T, 2004, ACM T GRAPHIC, V23, P888, DOI 10.1145/1015706.1015815
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Lévy B, 2003, ACM T GRAPHIC, V22, P364, DOI 10.1145/882262.882277
   MAGDA S, 2003, P 14 EUR WORKSH REND, P582
   NGUYEN MX, 2005, P PAC GRAPH, P23
   Ohtake Y, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P153
   Park S, 2005, IEEE I CONF COMP VIS, P1260
   Pauly M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P163, DOI 10.1109/VISUAL.2002.1183771
   Pauly M., 2005, Proceedings of Eurographics/ACM SIGGRAPH Symposium on Geometry Processing, P23
   Savchenko V, 2002, ADVANCES IN MODELLING, ANIMATION AND RENDERING, P139
   Sharf A, 2004, ACM T GRAPHIC, V23, P878, DOI 10.1145/1015706.1015814
   Soler C, 2002, ACM T GRAPHIC, V21, P673, DOI 10.1145/566570.566635
   Sun J, 2005, ACM T GRAPHIC, V24, P861, DOI 10.1145/1073204.1073274
   Turk G, 2001, COMP GRAPH, P347, DOI 10.1145/383259.383297
   Verdera J, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 2, PROCEEDINGS, P903
   Wei LY, 2001, COMP GRAPH, P355, DOI 10.1145/383259.383298
   Xiao CX, 2006, VISUAL COMPUT, V22, P210, DOI 10.1007/s00371-006-0377-8
   Ying LI, 2001, SPRING EUROGRAP, P301
   ZELINKA S, 2003, P 14 EUR S REND, P673
   ZHANG J, 2003, P SIGGRAPH, P295
   Zhou K, 2005, ACM T GRAPHIC, V24, P1148, DOI 10.1145/1073204.1073325
   Zigelman G, 2002, IEEE T VIS COMPUT GR, V8, P198, DOI 10.1109/2945.998671
NR 32
TC 22
Z9 26
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2007
VL 23
IS 6
BP 433
EP 443
DI 10.1007/s00371-007-0115-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 165BC
UT WOS:000246277800005
DA 2024-07-18
ER

PT J
AU Wu, EH
   Zhu, HB
   Liu, XH
   Liu, YQ
AF Wu, Enhua
   Zhu, Hongbin
   Liu, Xuehui
   Liu, Youquan
TI Simulation and interaction of fluid dynamics
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT International Conference on Cyberworlds (CW 2006)
CY NOV 28-29, 2006
CL Lausanne, SWITZERLAND
SP EPFL, VRlab
DE physically based simulation; graphics processing unit; fluid; mixture;
   interaction
AB In the fluid simulation, the fluids and their surroundings may greatly change properties such as shape and temperature simultaneously, and different surroundings would characterize different interactions, which would change the shape and motion of the fluids in different ways. On the other hand, interactions among fluid mixtures of different kinds would generate more comprehensive behavior. To investigate the interaction behavior in physically based simulation of fluids, it is of importance to build physically correct models to represent the varying interactions between fluids and the environments, as well as interactions among the mixtures. In this paper, we will make a simple review of the interactions, and focus on those most interesting to us, and model them with various physical solutions. In particular, more detail will be given on the simulation of miscible and immiscible binary mixtures. In some of the methods, it is advantageous to be taken with the graphics processing unit (GPU) to achieve real-time computation for middle-scale simulation.
C1 Univ Macau, FST, Dept Comp & Informat Sci, Macau, Peoples R China.
   Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100864, Peoples R China.
   Chinese Acad Sci, Grad Sch, Beijing 100864, Peoples R China.
   Rensselaer Polytech Inst, Dept Mech Aerosp & Nucl Engn, New York, NY USA.
C3 University of Macau; Chinese Academy of Sciences; Institute of Software,
   CAS; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS; Rensselaer Polytechnic Institute
RP Wu, EH (corresponding author), Univ Macau, FST, Dept Comp & Informat Sci, Macau, Peoples R China.
EM ehwu@umac.mo; zhuhb@ios.ac.cn; lxh@ios.ac.cn; liuy10@rpi.edu
RI Liu, Youquan/C-1628-2012; Lin, Fan/JZT-1441-2024
OI Lin, Fan/0000-0002-7330-3833
CR Carlson M, 2004, ACM T GRAPHIC, V23, P377, DOI 10.1145/1015706.1015733
   Dorsey Julie., 1996, Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques, P411, DOI [10.1145/237170. 237280, DOI 10.1145/237170.237280]
   Enright D, 2002, ACM T GRAPHIC, V21, P736, DOI [10.1145/566570.566581, 10.1145/566570.566645]
   FAN Z, 2005, ACM SIGGRAPH EUR S C, P245
   Feldman BE, 2005, ACM T GRAPHIC, V24, P904, DOI 10.1145/1073204.1073281
   FELDMAN BE, 2005, P S COMP AN 2005, P255
   Foster N, 1996, GRAPH MODEL IM PROC, V58, P471, DOI 10.1006/gmip.1996.0039
   FOSTER N, 2001, P ACM SIGGRAPH, P15
   Guendelman E, 2005, ACM T GRAPHIC, V24, P973, DOI 10.1145/1073204.1073299
   HONG JM, 2005, P ACM SIGGRAPH 2005, P915
   Klingner BM, 2006, ACM T GRAPHIC, V25, P820, DOI 10.1145/1141911.1141961
   Li W., 2005, GPU GEMS 2 CHAPTER 4, P747
   Liu Y, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P247
   Liu Youquan, 2005, Journal of Computer Aided Design & Computer Graphics, V17, P2581
   Liu YQ, 2005, VISUAL COMPUT, V21, P727, DOI 10.1007/s00371-005-0314-2
   Losasso F, 2006, ACM T GRAPHIC, V25, P812, DOI 10.1145/1141911.1141960
   Mao H, 2006, PROC GRAPH INTERF, P49
   Mizuno R, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P440, DOI 10.1109/PCCGA.2003.1238291
   Muller M, 2005, P 2005 ACM SIGGRAPH, P237, DOI DOI 10.1145/1073368.1073402
   Muller M., 2003, Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on Computer animation, P154
   Premoze S, 2003, COMPUT GRAPH FORUM, V22, P401, DOI 10.1111/1467-8659.00687
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   THUREY N, 2004, WORKSH VIS MOD VIS V, P199
   Wang HM, 2005, ACM T GRAPHIC, V24, P921, DOI 10.1145/1073204.1073284
   WEI X, 2003, ACM SIGGRAPH EUR S C, P75
   Wei XM, 2004, IEEE T VIS COMPUT GR, V10, P719, DOI 10.1109/TVCG.2004.48
   Wu EH, 2004, COMPUT ANIMAT VIRT W, V15, P139, DOI 10.1002/cav.16
   ZHU HB, 2006, SIMULATION MISCIBLE
   Zhu HB, 2006, COMPUT ANIMAT VIRT W, V17, P403, DOI 10.1002/cav.143
NR 29
TC 3
Z9 4
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2007
VL 23
IS 5
BP 299
EP 308
DI 10.1007/s00371-007-0106-y
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 154JS
UT WOS:000245503600002
DA 2024-07-18
ER

PT J
AU Correia, N
   Romero, L
AF Correia, Nuno
   Romero, Luis
TI Storing user experiences in mixed reality using hypermedia
SO VISUAL COMPUTER
LA English
DT Article
DE replay user experience; mixed reality; hypermedia
AB This paper describes HyperMem, a system to store and replay user experiences in mixed environments. The experience is stored as a set of hypermedia nodes and links, with the information that was displayed along with the video of the real world that was navigated. It uses a generic hypermedia model, implemented as software components, to handle mixed reality environments. This model includes components for storing and replaying experiences and integrating them in the overall set of hypermedia graphs that can be accessed by a given user. The paper presents the goals of the system, the underlying hypermedia model, the application scenarios, and the architecture and tools for replaying and repurposing stored information.
C1 Univ Nova Lisboa, Interact Multimedia Grp, CITI DI, Lisbon, Portugal.
C3 Universidade Nova de Lisboa
RP Correia, N (corresponding author), Univ Nova Lisboa, Interact Multimedia Grp, CITI DI, Lisbon, Portugal.
EM nmc@di.fct.unl.pt; lmcr@di.fct.unl.pt
RI Correia, Natália T. T./D-6699-2013; FCTUNL, CITI/G-6714-2011; Correia,
   Nuno/D-2298-2010
OI Correia, Nuno/0000-0002-8704-6698; Romero, Luis/0000-0003-1316-2474
CR Aizawa K., 2004, Pervasive 2004 workshop on memory and sharing experiences, P15
   Azuma R, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.963459
   Azuma RT, 1997, PRESENCE-VIRTUAL AUG, V6, P355, DOI 10.1162/pres.1997.6.4.355
   Baus J., 2002, IUI 02. 2002 International Conference on Intelligent User Interfaces, P15
   Billaud C, 2001, SCI ALIMENT, V21, P3, DOI 10.3166/sda.21.3-26
   BJORK S, 2001, INTERACT 2001 IFIP T
   Bushnell N, 1996, COMMUN ACM, V39, P31, DOI 10.1145/232014.232025
   Cheverst K, 2002, COMMUN ACM, V45, P47, DOI 10.1145/506218.506244
   De Bra P., 1999, Proceedings of the Tenth ACM Conference on Hypertext and Hypermedia : Returning to our Diverse Roots: Returning to our Diverse Roots, Darmstadt, Germany, P147
   DURI S, 2001, P 1 INT WORKSH MOB C, P20
   FITZGIBBON A, GRAND CHALL COMP RES
   Gemmell J., 2002, P 10 ACM INT C MULTI, P235, DOI DOI 10.1145/641007.641053
   GRAHAM J, 2003, ACM MULTIMEDIA 2003
   GRONBAEK K, 1992, ECHT C P, P191
   GRONBAEK P, 2002, HYPERTEXT 2002, P117
   HALASZ F, 1994, COMMUN ACM, V37, P30, DOI 10.1145/175235.175237
   HALL T, 2001, C VIRT REAL ARCH CUL, P91
   HARDMAN L, 1994, COMMUN ACM, V37, P50, DOI 10.1145/175235.175239
   Korkea-aho M., 2000, CONTEXT AWARE APPL S
   LIN N, 2004, PERV 2004 WORKSH MEM, P31
   LONG S, 1996, INT C MOB COMP NETW, P97
   Pea R, 2004, IEEE MULTIMEDIA, V11, P54, DOI 10.1109/MMUL.2004.1261108
   Piekarski W, 2002, COMMUN ACM, V45, P36, DOI 10.1145/502269.502291
   Romero L., 2004, COMPUT ENTERTAIN, V2, P12
   ROMERO L, 2003, HYP 2003 C NOTT UK A, P2
   Sawhney N., 1996, Seventh ACM Conference on Hypertext. Hypertext '96, P1, DOI 10.1145/234828.234829
   Starner T., 2000, Proceedings of the Fifth International Conference on Intelligent User Interfaces (New Orleans, P256, DOI DOI 10.1145/325737.325864
   SZALAVARI Z, 1998, ACM S VIRT REAL SOFT, P195
   Tamura H, 2001, IEEE COMPUT GRAPH, V21, P64, DOI 10.1109/38.963462
   Thomas B, 2000, FOURTH INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, DIGEST OF PAPERS, P139, DOI 10.1109/ISWC.2000.888480
   TRUONG K, 1999, 12 ANN ACM S US INT, P121
   WEAL M, 2003, HYP 2003 C NOTT UK A, P20
NR 32
TC 3
Z9 5
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2006
VL 22
IS 12
BP 991
EP 1001
DI 10.1007/s00371-006-0039-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 120HE
UT WOS:000243074300005
DA 2024-07-18
ER

PT J
AU Huang, J
   Shi, XH
   Liu, XG
   Zhou, K
   Guo, BN
   Bao, HJ
AF Huang, Jin
   Shi, Xiaohan
   Liu, Xinguo
   Zhou, Kun
   Guo, Baining
   Bao, Hujun
TI Geometrically based potential energy for simulating deformable objects
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE Laplacian; simulation; deformation
ID MESH DEFORMATION
AB This paper presents a fast and stable technique for simulating deformable objects. Unlike in previous physically based methods, our potential energy of deformation is purely geometrically based. It is defined as the L-2 norm of the change of the differential coordinates. A key feature of this energy formulation is that the corresponding stiffness matrix is approximately constant, which enables fast and stable implicit integration and large deformations. Our algorithm can simulate various effects including solid, thin shell and plasticity. We also adopt two schemes to accelerate the simulation process: dimensionality reduction in frequency domain and adaptive rotation computation in spatial domain.
C1 Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
   Microsoft Res Asia, Graph Grp, Beijing, Peoples R China.
C3 Zhejiang University; Microsoft; Microsoft Research Asia
RP Liu, XG (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
EM hj@cad.zju.edu.cn; shixiaohan@cad.zju.edu.cn; xgliu@cad.zju.edu.cn;
   kunzhou@microsoft.com; bainguo@microsoft.com; bao@cad.zju.edu.cn
RI Zhou, Kun/ABF-4071-2020; Zhou, Kun/AAH-9290-2019
OI Zhou, Kun/0000-0003-2320-3655; 
CR Alexa M, 2003, VISUAL COMPUT, V19, P105, DOI 10.1007/s00371-002-0180-0
   [Anonymous], 1997, TR9719 MITS EL RES L
   [Anonymous], 1989, P 16 ANN C COMP GRAP
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Barbic J, 2005, ACM T GRAPHIC, V24, P982, DOI 10.1145/1073204.1073300
   BroNielsen M, 1996, COMPUT GRAPH FORUM, V15, pC57
   Capell S, 2002, ACM T GRAPHIC, V21, P586, DOI 10.1145/566570.566622
   Capell S., 2002, P 2002 ACM SIGGRAPHE, P41
   Choi KJ, 2002, ACM T GRAPHIC, V21, P604, DOI 10.1145/566570.566624
   Choi MG, 2005, IEEE T VIS COMPUT GR, V11, P91
   Cotin S, 1999, IEEE T VIS COMPUT GR, V5, P62, DOI 10.1109/2945.764872
   Debunne G, 2001, COMP GRAPH, P31, DOI 10.1145/383259.383262
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Grinspun E., 2002, P 29 ANN C COMP GRAP, P281
   Hauser KK, 2003, PROC GRAPH INTERF, P247
   HUANG J, 2005, P 2005 ACM S SOL PHY, P221
   Huang J, 2006, ACM T GRAPH, V25
   Huang J, 2006, COMPUT ANIMAT VIRT W, V17, P383, DOI 10.1002/cav.141
   James DL, 1999, COMP GRAPH, P65, DOI 10.1145/311535.311542
   Lipman Y, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P181, DOI 10.1109/SMI.2004.1314505
   METAXAS D, 1992, COMP GRAPH, V26, P309, DOI 10.1145/142920.134085
   MIAO L, 2005, EUR S POINT BAS GRAP, P63
   Müller M, 2005, ACM T GRAPHIC, V24, P471, DOI 10.1145/1073204.1073216
   Müller M, 2004, PROC GRAPH INTERF, P239
   Muller M., 2002, P 2002 ACM SIGGRAPHE, P49, DOI DOI 10.1145/545261.545269
   Nealen A., 2005, Eurographics 2005 state of the art report
   OBRIEN JF, 2002, SIGGRAPH 02, P291
   Sorkine O., 2004, P 2004 EUR ACM SIGGR, P179
   TERZOPOULOS D, 1988, IEEE COMPUT GRAPH, V8, P41, DOI 10.1109/38.20317
   Wu XL, 2001, COMPUT GRAPH FORUM, V20, pC349
   Yu YZ, 2004, ACM T GRAPHIC, V23, P644, DOI 10.1145/1015706.1015774
   Zhou K, 2005, ACM T GRAPHIC, V24, P496, DOI 10.1145/1073204.1073219
NR 32
TC 11
Z9 13
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 740
EP 748
DI 10.1007/s00371-006-0058-7
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000017
DA 2024-07-18
ER

PT J
AU Waschbüsch, M
   Würmlin, S
   Gross, M
AF Waschbuesch, Michael
   Wuermlin, Stephan
   Gross, Markus
TI Interactive 3D video editing
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE 3D video; video editing; video processing; point-based graphics; graph
   cuts
AB We present a generic and versatile framework for interactive editing of 3D video footage. Our framework combines the advantages of conventional 2D video editing with the power of more advanced, depth-enhanced 3D video streams. Our editor takes 3D video as input and writes both 2D or 3D video formats as output. Its underlying core data structure is a novel 4D spatio-temporal representation which we call the video hypervolume. Conceptually, the processing loop comprises three fundamental operators: slicing, selection, and editing. The slicing operator allows users to visualize arbitrary hyperslices from the 4D data set. The selection operator labels subsets of the footage for spatio-temporal editing. This operator includes a 4D graph-cut based algorithm for object selection. The actual editing operators include cut & paste, affine transformations, and compositing with other media, such as images and 2D video. For high-quality rendering, we employ EWA splatting with view-dependent texturing and boundary matting. We demonstrate the applicability of our methods to post-production of 3D video.
C1 ETH, Comp Graph Lab, Zurich, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; ETH Zurich
RP Waschbüsch, M (corresponding author), ETH, Comp Graph Lab, Zurich, Switzerland.
EM waschbuesch@inf.ethz.ch; wuermlin@inf.ethz.ch; grossm@inf.ethz.ch
OI Gross, Markus/0009-0003-9324-779X
CR [Anonymous], 1978, P 5 ANN C COMPUTER G, P270
   Bajaj CL, 1998, IEEE SYMPOSIUM ON VOLUME VISUALIZATION, P95, DOI 10.1109/SVV.1998.729590
   Bennett E P., 2003, Proceedings o fACM Multimedia, P177
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Buehler C, 2001, COMP GRAPH, P425, DOI 10.1145/383259.383309
   Carranza J, 2003, ACM T GRAPHIC, V22, P569, DOI 10.1145/882262.882309
   Cheung GKM, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P373, DOI 10.1109/TDPVT.2004.1335262
   Fels S., 1999, Proc. ofthe Workshop on New Paradigms for Interactive Visualization and Manipulation (NPIVM), P78
   KLEIN AW, 2002, P ACM SIGGRAPH S COM
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Li Y, 2005, ACM T GRAPHIC, V24, P595, DOI 10.1145/1073204.1073234
   Matusik W, 2000, COMP GRAPH, P369, DOI 10.1145/344779.344951
   NIEVERGELT J, 1984, ACM T DATABASE SYST, V9, P38, DOI 10.1145/348.318586
   PASKO A, 2002, GRAPHICAL MODELS, V64
   Pauly M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P163, DOI 10.1109/VISUAL.2002.1183771
   Pauly M, 2001, COMP GRAPH, P379, DOI 10.1145/383259.383301
   Pfister H, 2000, COMP GRAPH, P335, DOI 10.1145/344779.344936
   Rander P, 1997, VISUALIZATION '97 - PROCEEDINGS, P277, DOI 10.1109/VISUAL.1997.663893
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Sadlo F., 2005, Point-Based Graphics 2005 (IEEE Cat. No. 05EX1159), P89, DOI 10.1109/PBG.2005.194069
   Snavely N., 2006, Proceedings of the 4th international symposium on Non-photorealistic animation and rendering, P63, DOI DOI 10.1145/1124728.1124739
   THEOBALT C, 2005, MPII20054004
   Vedula S, 2005, IEEE T PATTERN ANAL, V27, P475, DOI 10.1109/TPAMI.2005.63
   Wang J, 2005, ACM T GRAPHIC, V24, P585, DOI 10.1145/1073204.1073233
   Waschbüsch M, 2005, VISUAL COMPUT, V21, P629, DOI 10.1007/s00371-005-0346-7
   Weyrich T., 2004, SPBG 04 S POINT BASE, DOI [10.2312/SPBG/SPBG04/085-094, DOI 10.2312/SPBG/SPBG04/085-094]
   Woodring J, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P417, DOI 10.1109/VISUAL.2003.1250402
   Würmlin S, 2004, COMPUT GRAPH-UK, V28, P3, DOI 10.1016/j.cag.2003.10.015
   Würmlin S, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P325, DOI 10.1109/PCCGA.2002.1167876
   Zitnick CL, 2004, ACM T GRAPHIC, V23, P600, DOI 10.1145/1015706.1015766
   Zwicker M, 2002, ACM T GRAPHIC, V21, P322, DOI 10.1145/566570.566584
NR 31
TC 6
Z9 10
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 631
EP 641
DI 10.1007/s00371-006-0053-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000006
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Strengert, M
   Klein, T
   Botchen, R
   Stegmaier, S
   Chen, M
   Ertl, T
AF Strengert, Magnus
   Klein, Thomas
   Botchen, Ralf
   Stegmaier, Simon
   Chen, Min
   Ertl, Thomas
TI Spectral volume rendering using GPU-based raycasting
SO VISUAL COMPUTER
LA English
DT Article
DE GPU-raycasting; spectral volume rendering; Kubelka-Munk theory
AB Traditional volume rendering does not incorporate a number of optical properties that are typically observed for semi-transparent materials, such as glass or water, in the real world. Therefore, we have extended GPU-based raycasting to spectral volume rendering based on the Kubelka-Munk theory for light propagation in parallel colorant layers of a turbid medium. This allows us to demonstrate the effects of selective absorption and dispersion in refractive materials, by generating volume renderings using real physical optical properties. We show that this extended volume rendering technique can be easily incorporated into a flexible framework for GPU-based volume raycasting. Our implementation shows a promising performance for a number of real data sets. In particular, we obtain up to 100 times the performance of a comparable CPU implementation.
C1 Univ Stuttgart, Vis & Modeling Grp, D-70569 Stuttgart, Germany.
   Univ Coll Swansea, Dept Comp Sci, Swansea SA2 8PP, W Glam, Wales.
C3 University of Stuttgart; Swansea University
RP Strengert, M (corresponding author), Univ Stuttgart, Vis & Modeling Grp, Universitatsstr 38, D-70569 Stuttgart, Germany.
EM strengert@vis.informatik.uni-stuttgart.de;
   klein@vis.informatik.uni-stuttgart.de;
   botchen@vis.informatik.uni-stuttgart.de;
   stegmaier@vis.informatik.uni-stuttgart.de; m.chen@swansea.ac.uk;
   ertl@vis.informatik.uni-stuttgart.de
CR Abdul-Rahman A, 2005, COMPUT GRAPH FORUM, V24, P413, DOI 10.1111/j.1467-8659.2005.00866.x
   [Anonymous], TR93027 U N CAR CHAP
   Baxter William., 2004, Proceedings of the 3rd international symposium on Non-photorealistic animation and rendering, P45
   Bergner S, 2005, IEEE T VIS COMPUT GR, V11, P207, DOI 10.1109/TVCG.2005.19
   Bergner S, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P101, DOI 10.1109/VISUAL.2002.1183763
   Boada I, 2001, VISUAL COMPUT, V17, P185, DOI 10.1007/PL00013406
   Cabral B., 1994, P 1994 S VOLUME VISU, P91, DOI DOI 10.1145/197938.197972
   Curtis C. J., 1997, Proceedings of the 24th annual conference on Computer graphics and interactive techniques, P421
   DORSEY J, 1996, P SIGGRAPH 96, P387
   ENGEL K, 2001, EUR SIGGRAPH WORKSH, P9
   Glassner A. S., 1995, Principles of Digital Image Synthesis
   GLASSNER AS, 1989, IEEE COMPUT GRAPH, V9, P95, DOI 10.1109/38.31468
   Guthe S, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P53, DOI 10.1109/VISUAL.2002.1183757
   HAASE CS, 1992, ACM T GRAPHIC, V11, P305, DOI 10.1145/146443.146452
   HADWIGER M, 2005, P EUR, P303
   Hall R., 1989, ILLUMINATION COLOR C
   HALL RA, 1983, IEEE COMPUT GRAPH, V3, P10, DOI 10.1109/MCG.1983.263292
   Johnson GM, 1999, IEEE COMPUT GRAPH, V19, P47, DOI 10.1109/38.773963
   Klein T, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P223
   Kniss J, 2002, IEEE T VIS COMPUT GR, V8, P270, DOI 10.1109/TVCG.2002.1021579
   Kniss J, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P109, DOI 10.1109/VISUAL.2002.1183764
   Krüger J, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P287, DOI 10.1109/VISUAL.2003.1250384
   KUBELKA P, 1954, J OPT SOC AM, V44, P330, DOI 10.1364/JOSA.44.000330
   Kubelka P., 1931, Z TECH PHYS, V12, P259, DOI DOI 10.4236/MSCE.2014.28004
   LaMar E., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P355, DOI 10.1109/VISUAL.1999.809908
   Li W, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P317, DOI 10.1109/VISUAL.2003.1250388
   MAX N, 1995, IEEE T VIS COMPUT GR, V1, P99, DOI 10.1109/2945.468400
   MEYER GW, 1988, COMPUT VISION GRAPH, V41, P57, DOI 10.1016/0734-189X(88)90117-X
   Noordmans HJ, 2000, IEEE T VIS COMPUT GR, V6, P196, DOI 10.1109/2945.879782
   *NVIDIA CORP, 2004, NVIDIA SDK 8 0
   PEERCY MS, 1993, ACM SIGGRAPH COMPUTE, V27, P191
   REZK-SALAMA C., 2000, EGSIGGRAPH WORKSHOP, P109, DOI DOI 10.1145/346876.348238
   Rodgman D, 2003, VISION, MODELING, AND VISUALIZATION 2003, P355
   Rodgman D, 2001, SPRING EUROGRAP, P3
   ROTTGER S, 2003, P EG IEEE TCVG S VIS, P231
   Rougeron G, 1998, COMPUT GRAPH FORUM, V17, P3, DOI 10.1111/1467-8659.00212
   Rudolf D, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P163, DOI 10.1109/PCCGA.2003.1238258
   SCHLICK C, 1994, COMPUT GRAPH FORUM, V13, P121, DOI 10.1111/1467-8659.1320121
   Schuster A, 1905, ASTROPHYS J, V21, P1, DOI 10.1086/141186
   Stegmaier S, 2005, VOLUME GRAPHICS 2005, P187
   Weiler M, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P333, DOI 10.1109/VISUAL.2003.1250390
   Westermann R, 1998, SIGGRAPH 98, P169, DOI DOI 10.1145/280814.280860
   Wyszecki G., 1982, Color science: Concepts and methods, quantitative data and formulae
NR 43
TC 16
Z9 21
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2006
VL 22
IS 8
BP 550
EP 561
DI 10.1007/s00371-006-0028-0
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 088TD
UT WOS:000240833100004
DA 2024-07-18
ER

PT J
AU Chen, YS
   Ip, HHS
AF Chen, Yisong
   Ip, Horace H. S.
TI Single view metrology of wide-angle lens images
SO VISUAL COMPUTER
LA English
DT Article
DE lens distortion correction; Euclidean measurement; error analysis;
   single view metrology
ID CAMERA CALIBRATION; DISTORTION
AB This paper presents a framework and the associated algorithms to perform 3D scene analysis from a single image with lens distortion. Previous work focuses on either making 3D measurements under the assumption of one or more ideal pinhole cameras or correcting the lens distortion up to a projective transformation with no additional metric analyses. In this work, we bridge the gap between these two areas of work by incorporating metric constraints into lens distortion correction to achieve metric calibration. Lens distortion parameters, especially the lens distortion center, can be precisely recovered with this approach. Subsequent 3D measurements can be made from the corrected image to recover scene structures. In addition, we propose an algorithm based on hybrid backward and forward covariance propagation to yield a quantitative analysis of the confidence of the results. Experimental results show that our approach simultaneously performs image correction and 3D scene analysis.
C1 City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
   City Univ Hong Kong, Ctr Innovat Applicat Internet & Multimedia Techno, Hong Kong, Hong Kong, Peoples R China.
C3 City University of Hong Kong; City University of Hong Kong
RP Ip, HHS (corresponding author), City Univ Hong Kong, Ctr Innovat Applicat Internet & Multimedia Techno, Hong Kong, Hong Kong, Peoples R China.
EM cship@cityu.edu.hk
OI IP, Ho Shing Horace/0000-0002-1509-9002
CR Ahmed M, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P157, DOI 10.1109/ICIP.2001.958448
   Altunbasak Y, 2003, IEEE T IMAGE PROCESS, V12, P395, DOI 10.1109/TIP.2003.809012
   [Anonymous], 2002, NUMERICAL RECIPES C
   Bräuer-Burchardt C, 2001, IEEE IMAGE PROC, P225, DOI 10.1109/ICIP.2001.958994
   BRAUERBURCHARDT C, 2000, DAGM S, P187
   Claus D, 2005, PROC CVPR IEEE, P213
   Criminisi A, 2000, INT J COMPUT VISION, V40, P123, DOI 10.1023/A:1026598000963
   DEVERNAY F, 1995, P SOC PHOTO-OPT INS, V2567, P62, DOI 10.1117/12.218487
   Devernay F, 2001, MACH VISION APPL, V13, P14, DOI 10.1007/PL00013269
   El-Melegy MT, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P554
   Faugeras O., 1993, Three-dimensional computer vision: a geometric viewpoint
   Fitzgibbon AW, 2001, PROC CVPR IEEE, P125
   Gersho A., 2003, Vector Quantization and Signal Compression
   Geyer C, 2003, VISUAL COMPUT, V19, P405, DOI 10.1007/s00371-003-0204-4
   Grossberg MD, 2005, INT J COMPUT VISION, V61, P119, DOI 10.1023/B:VISI.0000043754.56350.10
   Guillou E, 2000, VISUAL COMPUT, V16, P396, DOI 10.1007/PL00013394
   Hartley RI, 2005, IEEE I CONF COMP VIS, P1834
   Hartley Richard., 2000, Computer Vision
   LI H, 2005, EASY NONITERATIVE ME
   Liebowitz D, 1998, PROC CVPR IEEE, P482, DOI 10.1109/CVPR.1998.698649
   Micusík B, 2003, PROC CVPR IEEE, P485
   Nielsen F, 2005, VISUAL COMPUT, V21, P92, DOI 10.1007/s00371-004-0273-z
   Nielsen F., 2005, Visual Computing: Geometry, Graphics, and Vision
   Prescott B, 1997, GRAPH MODEL IM PROC, V59, P39, DOI 10.1006/gmip.1996.0407
   Sawhney HS, 1999, IEEE T PATTERN ANAL, V21, P235, DOI 10.1109/34.754589
   SHAH S, 1994, IEEE INT CONF ROBOT, P3422, DOI 10.1109/ROBOT.1994.351044
   SHAH S, 1994, P IEEE INT C IMAGE P, V2, P740
   SHOHAM Y, 1988, IEEE T ACOUST SPEECH, V36, P1445, DOI 10.1109/29.90373
   Stein GP, 1997, PROC CVPR IEEE, P602, DOI 10.1109/CVPR.1997.609387
   STURM P, 2004, P ECCV, V2, P1
   Swaminathan R, 2000, IEEE T PATTERN ANAL, V22, P1172, DOI 10.1109/34.879797
   Swaminathan R, 2003, PROC CVPR IEEE, P594
   Thirthala S, 2005, PROC CVPR IEEE, P321
   TRIGGS T, 2000, VISION ALGORITHMS TH, P298
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109
   ZHANG Z, 1996, P INT C PATT REC, V1, P407
NR 36
TC 4
Z9 4
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2006
VL 22
IS 7
BP 445
EP 455
DI 10.1007/s00371-006-0014-6
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 074VG
UT WOS:000239839300001
DA 2024-07-18
ER

PT J
AU Wen, GJ
   Wang, ZQ
   Xia, SH
   Zhu, DM
AF Wen, Gaojin
   Wang, Zhaoqi
   Xia, Shihong
   Zhu, Dengming
TI Least-squares fitting of multiple <i>M</i>-dimensional point sets
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference (CGI 2005)
CY JUN 22-24, 2005
CL Stony Brook, NY
SP IEEE Comp Soc, VGTC, ACM SIGGRAPH
DE absolute orientation; least-squares fitting; singular value
   decomposition; motion capture
ID CLOSED-FORM SOLUTION; ABSOLUTE ORIENTATION; QUATERNIONS; PARAMETERS
AB Based on the classic absolute orientation technique, a new method for least-squares fitting of multiple point sets in m-dimensional space is proposed, analyzed and extended to a weighted form in this paper. This method generates a fixed point set from k corresponding original m-dimensional point sets and minimizes the mean squared error between the fixed point set and these k point sets under the similarity transformation. Experiments and interesting applications are presented to show its efficiency and accuracy.
C1 Chinese Acad Sci, Inst Comp Technol, Beijing 100080, Peoples R China.
   Chinese Acad Sci, Grad Sch, Beijing 100080, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Wen, GJ (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing 100080, Peoples R China.
EM gjwen@ict.ac.cn; zqwang@ict.ac.cn; xsh@ict.ca.cn; mdzhu@ict.ac.cn
OI Wen, Gaojin/0000-0002-8681-7147
CR ALIAS, MOTIONBUILDER
   [Anonymous], P S COMP AN SAN DIEG
   ARUN KS, 1987, IEEE T PATTERN ANAL, V9, P699, DOI 10.1109/TPAMI.1987.4767965
   GLEICHER M, 1999, P ACM SIGGRAPH COMP, P51
   GORYN D, 1995, IEEE T PATTERN ANAL, V17, P1219, DOI 10.1109/34.476514
   Hill A., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P461, DOI 10.1109/ICPR.1996.546069
   HORN BKP, 1988, J OPT SOC AM A, V5, P1127, DOI 10.1364/JOSAA.5.001127
   HORN BKP, 1987, J OPT SOC AM A, V4, P629, DOI 10.1364/JOSAA.4.000629
   Izani M, 2004, IEEE INFOR VIS, P849, DOI 10.1109/IV.2004.1320239
   JEFF MP, SURVEY ABSOLUTE ORIT
   Krishnan S., 2005, Proc. Symposium on Geometry Processing, P187
   LORUSSO A, 1997, J MACH VIS APPL, V9, P272
   OSWAL HL, 1968, PHOTOGRAMM ENG, V34, P1079
   SANSO F, 1973, PHOTOGRAMMETRIA, V29, P203, DOI 10.1016/0031-8663(73)90002-1
   Schut G.H., 1960, Photogrammetria, V16, P34
   Thompson EH., 1958, PHOTOGRAMMETRIA, V4, P163
   UMEYAMA S, 1991, IEEE T PATTERN ANAL, V13, P376, DOI 10.1109/34.88573
   WALKER MW, 1991, CVGIP-IMAG UNDERSTAN, V54, P358, DOI 10.1016/1049-9660(91)90036-O
   Williams J, 2001, COMPUT VIS IMAGE UND, V81, P117, DOI 10.1006/cviu.2000.0884
   ZORDAN VB, 2002, SCA 02, P89
NR 20
TC 17
Z9 21
U1 1
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2006
VL 22
IS 6
BP 387
EP 398
DI 10.1007/s00371-006-0022-6
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 053BL
UT WOS:000238278500003
DA 2024-07-18
ER

PT J
AU Zheng, JJ
   Zhang, JJ
   Zhou, HJ
   Shen, LG
AF Zheng, JJ
   Zhang, JJ
   Zhou, HJ
   Shen, LG
TI Smooth spline surface generation over meshes of irregular topology
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE spline surface; subdivision; irregular mesh; surface generation
ID CONTROL POINT SURFACES
AB An efficient method for generating a smooth spline surface over an irregular mesh is presented in this paper. Similar to the methods proposed by [1, 2, 3, 4], this method generates a generalised bi-quadratic B-spline surface and achieves C-1 smoothness. However, the rules to construct the control points for the proposed spline surfaces are much simpler and easier to follow. The construction process consists of two steps: subdividing the initial mesh once using the Catmull-Clark [5] subdivision rules and generating a collection of smoothly connected surface patches using the resultant mesh. As most of the final mesh is quadrilateral apart from the neighbourhood of the extraordinary points, most of the surface patches are regular quadratic B-splines. The neighbourhood of the extraordinary points is covered by quadratic Zheng-Ball patches [6].
C1 Univ Sci & Technol China, Dept Precis Machinery & Precis Instrumentat, Hefei 230026, Peoples R China.
   Bournemouth Univ, NCCA, Poole BH12 5BB, Dorset, England.
   Univ Sci & Technol China, NSRL, Hefei 230029, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Bournemouth University; Chinese Academy of Sciences;
   University of Science & Technology of China, CAS
RP Univ Sci & Technol China, Dept Precis Machinery & Precis Instrumentat, Hefei 230026, Peoples R China.
EM jjzheng@ustc.edu.cn
CR Ball AA, 2001, COMPUT AIDED GEOM D, V18, P135, DOI 10.1016/S0167-8396(01)00020-6
   CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   DOO D, 1978, COMPUT AIDED DESIGN, V10, P356, DOI 10.1016/0010-4485(78)90111-2
   Lin J, 1998, J MATER PROCESS TECH, V80-1, P613, DOI 10.1016/S0924-0136(98)00214-3
   LOOP C, 1994, COMPUT AIDED GEOM D, V11, P303, DOI [10.1016/0167-8396(94)90005-1, 10.1145/192161.192238]
   Loop C., 1990, Computer Graphics, V24, P347, DOI 10.1145/97880.97917
   Navau JC, 2000, COMPUT AIDED GEOM D, V17, P643, DOI 10.1016/S0167-8396(00)00020-0
   PETERS J, 1993, COMPUT AIDED GEOM D, V10, P347, DOI 10.1016/0167-8396(93)90046-6
   Peters J, 2000, COMP GRAPH, P255, DOI 10.1145/344779.344908
   PETERS J., 1994, DESIGNING FAIR CURVE, P277
   Sabin M. A., 1983, Eurographics '83. Proceedings of the International Conference and Exhibition, P57
   Stam J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P395, DOI 10.1145/280814.280945
   Zheng JJ, 1997, COMPUT AIDED GEOM D, V14, P807, DOI 10.1016/S0167-8396(97)00007-1
   Zheng JJ, 2002, LECT NOTES COMPUT SC, V2330, P239
   Zheng JJ, 2001, COMPUT AIDED GEOM D, V18, P129, DOI 10.1016/S0167-8396(01)00019-X
NR 15
TC 8
Z9 8
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 858
EP 864
DI 10.1007/s00371-005-0345-8
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400039
DA 2024-07-18
ER

PT J
AU Jia, JY
   Tang, K
   Joneja, A
AF Jia, JY
   Tang, K
   Joneja, A
TI Biconic subdivision of surfaces of revolution and its applications in
   intersection problems
SO VISUAL COMPUTER
LA English
DT Article
DE quadric decomposition; bi-conic arc fitting; surfaces of revolution;
   revolute quadrics; intersection
ID APPROXIMATION; INTERPOLATION; CONVEXITY; QUADRICS; CURVES; G(1)
AB This paper presents a novel method for the subdivision of surfaces of revolution. We develop a new technique for approximating the genertrix by a series of pairs of conic sections. By using an error estimate based on convex combination, an efficient least-squares approach is proposed that yields near-optimal fitting. The resulting surface approximation is shown to be more efficient than other tessellation methods in terms of the number of fitting segments. This in turn allows us to implement efficient and robust algorithms for such surfaces. In particular, novel intersection techniques based on the proposed subdivision method are introduced for the two most fundamental types of intersections - line/surface and surface/surface intersections. The experimental results show that our method outperforms conventional methods significantly in both computing time and memory cost.
C1 Hong Kong Univ Sci & Technol, Dept Mech Engn, Kowloon, Hong Kong, Peoples R China.
   Hong Kong Univ Sci & Technol, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
   Hong Kong Univ Sci & Technol, Dept IEEM, Kowloon, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology; Hong Kong University of
   Science & Technology; Hong Kong University of Science & Technology
RP Hong Kong Univ Sci & Technol, Dept Mech Engn, Kowloon, Hong Kong, Peoples R China.
EM csjyjia@ust.hk; mektang@ust.hk; joneja@ust.hk
RI Tang, Kai/ABA-9642-2021
OI Tang, Kai/0000-0002-5184-2086; Joneja, Ajay/0000-0002-6797-1253
CR Baciu G, 2001, COMPUTER GRAPHICS INTERNATIONAL 2001, PROCEEDINGS, P215, DOI 10.1109/CGI.2001.934677
   BALLARD DH, 1981, COMMUN ACM, V24, P310, DOI 10.1145/358645.358661
   Bangert C, 1999, COMPUT AIDED GEOM D, V16, P529, DOI 10.1016/S0167-8396(99)00013-8
   Bangert C, 1999, COMPUT AIDED GEOM D, V16, P497, DOI 10.1016/S0167-8396(98)00047-8
   BIDASARIA HB, 1990, P 18 ANN ACM COMP SC, P427
   BOEHM W, 1991, BEZIER PATCHES QUADR, P1
   BURGER P, 1989, P COMP GRAPH INT
   Carnicer J. M., 1992, Computer-Aided Geometric Design, V9, P279, DOI 10.1016/0167-8396(92)90035-N
   CHAPRA SC, 2003, NUMERICAL METHODS EZ
   Cychosz J. M., 1992, GRAPHICS GEMS, P275
   Dahmen W., 1989, Mathematical Methods in Computer Aided Geometric Design, P181
   DEGEN WLF, 1993, COMPUT AIDED GEOM D, V10, P293, DOI 10.1016/0167-8396(93)90043-3
   DUPONT L, 2003, P ACM S COMP GEOM SA
   DUPONT L, 2001, WORKSH UNC GEOM COMP
   FARIN G, 1989, ACM T GRAPHIC, V8, P89, DOI 10.1145/62054.62056
   Farouki RT, 2001, COMPUT AIDED GEOM D, V18, P639, DOI 10.1016/S0167-8396(01)00058-9
   Floater MS, 1997, COMPUT AIDED GEOM D, V14, P135, DOI 10.1016/S0167-8396(96)00025-8
   Floater MS, 1997, COMPUT AIDED GEOM D, V14, P383, DOI 10.1016/S0167-8396(97)00003-4
   FOURMENTIN F, 1997, MATH SURFACES, V7, P363
   GLASSNER RS, 1989, INTRO RAY TRACKING
   GOLDMAN RN, 1983, IEEE COMPUT GRAPH, V3, P68, DOI 10.1109/MCG.1983.263025
   Guo B., 1993, Visual Computer, V9, P267, DOI 10.1007/BF01908449
   Heo HS, 2001, GRAPH MODELS, V63, P228, DOI 10.1006/gmod.2001.0553
   Heo HS, 1999, COMPUT AIDED DESIGN, V31, P33, DOI 10.1016/S0010-4485(98)00078-5
   JIA J, 2002, P IV02 INF VIS LOND, P119
   KAJIYA JT, 1983, COMPUT GRAPH, V17, P517
   KIM MS, 2000, P RIK S GEOM PROC IN, P1
   LAPORTE H, 1995, COMPUT GRAPH, V19, P251, DOI 10.1016/0097-8493(94)00151-N
   LAPORTE N, 1993, EUR WORKSH COMPUT GR
   LEVIN J, 1976, COMMUN ACM, V19, P555, DOI 10.1145/360349.360355
   LEVIN JZ, 1979, COMPUT VISION GRAPH, V11, P73, DOI 10.1016/0146-664X(79)90077-7
   MEEK DS, 1992, COMPUT AIDED DESIGN, V24, P301, DOI 10.1016/0010-4485(92)90047-E
   OBRADOVIC R, 2000, FACTA U SERIES ARCHI, V2, P117
   PATRIKALAKIS N, 2001, HDB COMPUTER AIDED D
   PAVLIDIS T, 1983, ACM T GRAPHIC, V2, P1, DOI 10.1145/357314.357315
   PIEGL L, 1989, COMPUT AIDED DESIGN, V21, P201, DOI 10.1016/0010-4485(89)90045-6
   PIEGL L, 1987, IEEE COMPUT GRAPH, V7, P45, DOI [10.1109/MCG.1987.276964, 10.1109/MCG.1987.276871]
   PIEGL LA, 2001, CAD, V34, P807
   POTTMANN H, 1991, ACM T GRAPHIC, V10, P366, DOI 10.1145/116913.116916
   Powell M. J. D., 1977, ACM Transactions on Mathematical Software, V3, P316, DOI 10.1145/355759.355761
   SCHABACK R, 1993, CONSTR APPROX, V9, P373, DOI 10.1007/BF01204647
   Sederberg T. W., 1985, Computer-Aided Geometric Design, V2, P53, DOI 10.1016/0167-8396(85)90007-X
   Tu CH, 2002, GEOMETRIC MODELING AND PROCESSING: THEORY AND APPLICATIONS, PROCEEDINGS, P23, DOI 10.1109/GMAP.2002.1027493
   Wang WP, 2003, COMPUT AIDED GEOM D, V20, P401, DOI 10.1016/S0167-8396(03)00081-5
   Wang WP, 2002, GRAPH MODELS, V64, P335, DOI [10.1016/S1077-3169(02)00018-7, 10.1016/S1524-0703(02)00018-7]
   WANG WP, 2001, HDB COMPUTER AIDED G
   WIJK JJV, 1984, ACM T GRAPHIC, V3, P223
   WILF I, 1993, COMPUT AIDED DESIGN, V25, P633, DOI 10.1016/0010-4485(93)90018-J
   WILLEMANS K, 1994, J COMPUT APPL MATH, V56, P263, DOI 10.1016/0377-0427(94)90082-5
   Yang XN, 2002, COMPUT AIDED DESIGN, V34, P1037, DOI 10.1016/S0010-4485(01)00164-6
   Yong JH, 2000, COMPUT AIDED DESIGN, V32, P253, DOI 10.1016/S0010-4485(99)00100-1
NR 51
TC 3
Z9 3
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2004
VL 20
IS 7
BP 457
EP 478
DI 10.1007/s00371-004-0252-4
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 863IZ
UT WOS:000224556200003
DA 2024-07-18
ER

PT J
AU Baerlocher, P
   Boulic, R
AF Baerlocher, P
   Boulic, R
TI An inverse kinematics architecture enforcing an arbitrary number of
   strict priority levels
SO VISUAL COMPUTER
LA English
DT Article
DE articulated figures; synergistic posture control; inverse kinematics;
   conflicting tasks; priority strategy
ID MOTION; MANIPULATORS
AB An efficient inverse kinematics solver is a key element in applications targeting the on-line or off-line postural control of complex articulated figures. In the present paper we progressively describe the strategic components of a very general and robust inverse kinematics architecture. We then present an efficient recursive algorithm enforcing an arbitrary number of strict priorities to arbitrate the fulfillment of conflicting constraints. Due to its local nature, the moderate cost of the solution allows this architecture to run within an interactive environment. The algorithm is illustrated on the postural control of complex articulated figures.
C1 NEKO Entertainment, F-93100 Montreuil, France.
   Ecole Polytech Fed Lausanne, VRlab, CH-1015 Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP NEKO Entertainment, 12-16 Rue Vincennes, F-93100 Montreuil, France.
EM pbaerlocher@e-neko.com; ronan.boulic@epfl.ch
RI BOULIC, RONAN/A-9108-2008
OI BOULIC, RONAN/0000-0001-9176-6877
CR BADLER NI, 1987, IEEE COMPUT GRAPH, V7, P28, DOI 10.1109/MCG.1987.276894
   Baerlocher P, 1998, 1998 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS - PROCEEDINGS, VOLS 1-3, P323, DOI 10.1109/IROS.1998.724639
   BAERLOCHER P, 2001, THESIS SWISS FEDERAL
   Bindiganavale R, 1998, LECT NOTES ARTIF INT, V1537, P70
   Boulic R, 1996, COMPUT GRAPH, V20, P693, DOI 10.1016/S0097-8493(96)00043-X
   Boulic R, 1997, 8TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS, 1997 PROCEEDINGS - ICAR'97, P589, DOI 10.1109/ICAR.1997.620242
   Choi KJ, 2000, J VISUAL COMP ANIMAT, V11, P223, DOI 10.1002/1099-1778(200012)11:5<223::AID-VIS236>3.0.CO;2-5
   CLINE RE, 1964, J SOC IND APPL MATH, V12, P588, DOI 10.1137/0112050
   Girard M., 1985, Computer Graphics, V19, P263, DOI 10.1145/325165.325244
   Gleicher M, 2001, GRAPH MODELS, V63, P107, DOI 10.1006/gmod.2001.0549
   Hanafusa H., 1981, IFAC, 8th Triennal World Congress, V4, P1927
   HODGINS JK, 1997, P SIGGRAPH 97, P153
   Korein JamesU., 1985, GEOMETRIC INVESTIGAT
   LEE J, 1999, P SIGGRAPH 99 LOS AN
   LIEGEOIS A, 1977, IEEE T SYST MAN CYB, V7, P868
   MACIEJEWSKI AA, 1985, INT J ROBOT RES, V4, P109, DOI 10.1177/027836498500400308
   MACIEJEWSKI AA, 1990, IEEE COMPUT GRAPH, V10, P63, DOI 10.1109/38.55154
   MACIEJEWSKI AA, 1988, J ROBOTIC SYST, V5, P527, DOI 10.1002/rob.4620050603
   MONZANI JS, 2000, P EUR 2000 INT SWITZ
   NAKAMURA Y, 1986, J DYN SYST-T ASME, V108, P163, DOI 10.1115/1.3143764
   PHILLIPS CB, 1991, COMP GRAPH, V25, P359, DOI 10.1145/127719.122756
   POPOVIC Z, 1999, P SIGGRAPH 99 LOS AN
   Press W H, 1992, NUMERICAL RECIPES C, P59
   Siciliano B., 1991, 91 ICAR. Fifth International Conference on Advanced Robotics. Robots in Unstructured Environments (Cat. No.91TH0376-4), P1211, DOI 10.1109/ICAR.1991.240390
   TAK S, 2000, P EUR 2000 COMP GRAP, V19
   Tolani D, 2000, GRAPH MODELS, V62, P353, DOI 10.1006/gmod.2000.0528
   Watt A., 1992, ADV ANIMATION RENDER
   Welman C, 1993, THESIS S FRASER U
   Yamane K, 2003, IEEE T VIS COMPUT GR, V9, P352, DOI 10.1109/TVCG.2003.1207443
   ZHAO JM, 1994, ACM T GRAPHIC, V13, P313, DOI 10.1145/195826.195827
NR 30
TC 167
Z9 181
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2004
VL 20
IS 6
BP 402
EP 417
DI 10.1007/s00371-004-0244-4
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 846DU
UT WOS:000223296100005
DA 2024-07-18
ER

PT J
AU Szymczak, A
AF Szymczak, A
TI Optimized Edgebreaker encoding for large and regular triangle meshes
SO VISUAL COMPUTER
LA English
DT Article
DE mesh; compression; entropy; context
ID CONNECTIVITY; COMPRESSION
AB We present a technique aiming to improve the compression of the Edgebreaker CLERS string for large and regular meshes, where regularity is understood as the compactness of the distribution of vertex degrees. Our algorithm uses a specially designed context-based coding to compress the CLERS sequence. It is exceptionally simple to implement and can easily be incorporated into any existing Edgebreaker implementation which uses the Spirale Reversi algorithm for decompression. Even for irregular meshes, it does not carry considerable overhead when compared to the original Edgebreaker encoding. Experimental results show that our procedure is very fast (600 000 triangles per second on a PIII 650 MHz for decompression) and leads to compression rates which are, in most cases, superior to those previously reported for large meshes of high regularity.
C1 Georgia Inst Technol, GVU Ctr, Atlanta, GA 30332 USA.
C3 University System of Georgia; Georgia Institute of Technology
RP Szymczak, A (corresponding author), Georgia Inst Technol, GVU Ctr, Atlanta, GA 30332 USA.
EM andrzej@cc.gatech.edu
CR ALLIEZ P, 2001, EUROGRAPHICS, P480
   Deering M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P13, DOI 10.1145/218380.218391
   Gumhold S., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P133, DOI 10.1145/280814.280836
   GUMHOLD S, 2003, WSI20011
   GUMHOLD S, 1999, ERL WORKSH 99 VIS MO
   Isenburg M, 2001, COMP GEOM-THEOR APPL, V20, P39, DOI 10.1016/S0925-7721(01)00034-7
   Khodavosky A, 2002, GRAPH MODELS, V64, P147, DOI 10.1006/gmod.2002.0575
   KING D, 1999, P 11 CAN C COMP GEOM, P146
   LEE H, 2002, EUR 02 C P, P383
   Rossignac J, 1999, COMP GEOM-THEOR APPL, V14, P119, DOI 10.1016/S0925-7721(99)00028-0
   ROSSIGNAC J, 1999, IEEE T VISUAL COMPUT, V5
   Schindler M, 1998, IEEE DATA COMPR CONF, P572, DOI 10.1109/DCC.1998.672314
   Szymczak A, 2002, IEEE DATA COMPR CONF, P472, DOI 10.1109/DCC.2002.1000015
   Szymczak A, 2001, COMP GEOM-THEOR APPL, V20, P53, DOI 10.1016/S0925-7721(01)00035-9
   Taubin G, 1998, ACM T GRAPHIC, V17, P84, DOI 10.1145/274363.274365
   Touma C, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P26
   TUTTE WT, 1962, CANADIAN J MATH, V14, P21, DOI 10.4153/CJM-1962-002-9
NR 17
TC 6
Z9 6
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2003
VL 19
IS 4
BP 271
EP 278
DI 10.1007/s00371-003-0208-0
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 713KV
UT WOS:000184857800005
DA 2024-07-18
ER

PT J
AU Wu, XJ
   Jiang, X
   Dong, LG
AF Wu, Xianjun
   Jiang, Xian
   Dong, Ligang
TI Gated weighted normative feature fusion for multispectral object
   detection
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Multispectral object detection; Feature fusion; Cross-modality;
   Autonomous vehicles
ID CNN
AB Multispectral image pairs can provide independent and complementary information to more comprehensively describe detection targets, thereby improving the robustness and reliability of object detectors. The performance of an object detector depends on how cross-modality features are extracted and fused. To exploit the different modalities fully, we propose a lightweight yet effective cross-modality feature fusion approach named gated weighted normative feature fusion. In the feature extraction stage, our proposed dual-input backbone network can extract richer and more useful features. In the feature fusion stage, the fusion module can eliminate redundant features, dynamically weigh the importance of two image features, and further normalize fused features. Experiments and ablation studies on several publicly available datasets demonstrate the effectiveness of our method. Our proposed method achieved better performance in terms of mAP50 with 80.3%, mAP with 41.8%, and mAP50 with 98.0%, mAP with 68.0% on the FLIR and LLVIP datasets, respectively. In particular, the inference speed of our method is twice as fast as the current state-of-the-art method.
C1 [Wu, Xianjun; Jiang, Xian; Dong, Ligang] Zhejiang Gongshang Univ, Sussex Artificial Intelligence Inst, Sch Informat & Elect Engn, Xuezheng St, Hangzhou 310018, Zhejiang, Peoples R China.
C3 Zhejiang Gongshang University
RP Dong, LG (corresponding author), Zhejiang Gongshang Univ, Sussex Artificial Intelligence Inst, Sch Informat & Elect Engn, Xuezheng St, Hangzhou 310018, Zhejiang, Peoples R China.
EM 21020090097@pop.zjgsu.edu.cn; jiangxian@zjgsu.edu.cn;
   donglg@zjgsu.edu.cn
FU National Natural Science Foundation of China [62272308]; National
   Natural Science Foundation of China [2013E10012]; New Network Standards
   and Application Technology Key Laboratory Project of Zhejiang Province
   [1120XJ0622034]; General Research Project of Zhejiang Province
FX This work was supported by the National Natural Science Foundation of
   China (Grant No. 62272308), New Network Standards and Application
   Technology Key Laboratory Project of Zhejiang Province (Grant No.
   2013E10012) and General Research Project of Zhejiang Province (Grant No.
   1120XJ0622034).
CR Bilal M, 2017, IEEE T CIRC SYST VID, V27, P2260, DOI 10.1109/TCSVT.2016.2581660
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Cao ZW, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21124184
   Chaoxu Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12592, DOI 10.1109/CVPR42600.2020.01261
   Chen Y, 2018, IET COMPUT VIS, V12, P1179, DOI 10.1049/iet-cvi.2018.5315
   Cui YM, 2022, Arxiv, DOI arXiv:2207.05252
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Esfahanian M, 2013, J ACOUST SOC AM, V134, pEL105, DOI 10.1121/1.4811162
   F.Team, FREE Teledyne FLIR Thermal Dataset for Algorithm Training
   Fang Q., 2021, arXiv
   Faster R., 2015, Adv Neural Inform Process Syst, V9199, P2969239
   Guan DY, 2019, INFORM FUSION, V50, P148, DOI 10.1016/j.inffus.2018.11.017
   Hou YL, 2018, INFRARED PHYS TECHN, V94, P69, DOI 10.1016/j.infrared.2018.08.029
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jia XY, 2021, IEEE INT CONF COMP V, P3489, DOI 10.1109/ICCVW54120.2021.00389
   Jin X, 2017, INFRARED PHYS TECHN, V85, P478, DOI 10.1016/j.infrared.2017.07.010
   Jocher G, YOLO ULTRALYTICS
   Jocher Glenn, 2021, Zenodo, DOI 10.5281/ZENODO.4418161
   Li CY, 2019, PATTERN RECOGN, V85, P161, DOI 10.1016/j.patcog.2018.08.005
   Li SY, 2024, Arxiv, DOI arXiv:2211.03295
   Liu DF, 2021, AAAI CONF ARTIF INTE, V35, P6101
   Qingyun F., 2021, arXiv
   Redmon J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1804.02767
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Song XR, 2021, ALEX ENG J, V60, P73, DOI 10.1016/j.aej.2020.05.035
   Tan M., 2020, P IEEECVF C COMPUTER, P10781, DOI [10.48550/arXiv.1911.09070, DOI 10.1109/CVPR42600.2020.01079]
   Uzkent B, 2020, IEEE WINT CONF APPL, P1813, DOI 10.1109/WACV45572.2020.9093447
   Vaswani A, 2017, ADV NEUR IN, V30
   Wagner J., 2016, EUR S ART NEUR NETW
   Yan LQ, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2220, DOI 10.1109/ICASSP39728.2021.9414517
   Yang XX, 2022, 2022 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA 2022), P2920, DOI 10.1109/ICRA46639.2022.9811999
   Yang ZH, 2018, IEEE INT VEH SYM, P179, DOI 10.1109/IVS.2018.8500642
   Yun JS, 2022, MATHEMATICS-BASEL, V10, DOI 10.3390/math10213966
   Zhang H, 2021, IEEE WINT CONF APPL, P72, DOI 10.1109/WACV48630.2021.00012
   Zhang H, 2020, IEEE IMAGE PROC, P276, DOI [10.1109/ICIP40778.2020.9191080, 10.1109/icip40778.2020.9191080]
   Zhang SS, 2016, PROC CVPR IEEE, P1259, DOI 10.1109/CVPR.2016.141
   Zhang XW, 2011, J INFRARED MILLIM W, V30, P354, DOI 10.3724/SP.J.1010.2011.00354
   Zhou H, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13183656
NR 38
TC 0
Z9 0
U1 5
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 12
PY 2023
DI 10.1007/s00371-023-03173-6
EA DEC 2023
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CF3G0
UT WOS:001123790500002
DA 2024-07-18
ER

PT J
AU Ye, ZX
   Zhang, HY
   Li, X
   Zhang, Q
AF Ye, Zixun
   Zhang, Hongying
   Li, Xue
   Zhang, Qin
TI DeMaskGAN: a de-masking generative adversarial network guided by
   semantic segmentation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Masked face recognition; Masked face reconstruction; Generating
   adversarial network
AB To address the problem of reduced face recognition accuracy in masked scenarios, this paper proposes a masked face reconstruction algorithm DeMaskGAN, which uses the Transformer Reconstruction Head (TRH) to restore the masked face features, and uses the Transformer Segmentation Head as an aid so that the TRH focuses on the masked face region and reconstructs the face to an unmasked state while maintaining the identity information. To improve the model performance, identity consistency, key point consistency, and perceptual consistency supervision mechanisms for faces are proposed to assist in training the model, and data augmentation methods are used to generate Mask-FFHQ datasets adapted to the mask-obscured face segmentation and reconstruction tasks, the experimental results show that the reconstructed face images enable the face recognition algorithm MobileFaceNet to achieve an AUC metric of 0.9743, which is 0.039 better than the direct use of MobileFaceNet to recognize masked faces.
C1 [Ye, Zixun; Zhang, Hongying; Li, Xue; Zhang, Qin] Southwest Univ Sci & Technol, Sch Informat Engn, Mianyang 621010, Peoples R China.
C3 Southwest University of Science & Technology - China
RP Zhang, HY (corresponding author), Southwest Univ Sci & Technol, Sch Informat Engn, Mianyang 621010, Peoples R China.
EM yeahzixun@foxmail.com; zhywyd@163.com; lixue_1428@163.com;
   zhangqin645564@foxmail.com
RI Wang, Huiyan/JXW-9178-2024; Yang, Yifan/JTV-1487-2023; Liu,
   Yuxuan/JVO-7759-2024
FU National Natural Science Foundation of China [61872304]; National
   Natural Science Foundation of China [61872304]
FX This research was supported by the National Natural Science Foundation
   of China (grant No. 61872304).
CR Arjovsky M, 2017, PR MACH LEARN RES, V70
   Chen S, 2018, LECT NOTES COMPUT SC, V10996, P428, DOI 10.1007/978-3-319-97909-0_46
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Din NU, 2020, IEEE ACCESS, V8, P44276, DOI 10.1109/ACCESS.2020.2977386
   Feng T, 2021, IEEE INT CONF COMP V, P1492, DOI 10.1109/ICCVW54120.2021.00173
   Hensel M, 2017, ADV NEUR IN, V30
   Huang BJ, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3569943
   Huang BJ, 2023, PATTERN RECOGN, V135, DOI 10.1016/j.patcog.2022.109142
   Huang BJ, 2023, IEEE T NEUR NET LEAR, V34, P10875, DOI 10.1109/TNNLS.2022.3171604
   Huang G.B., 2008, WORKSH FAC REAL LIF
   Gulrajani I, 2017, ADV NEUR IN, V30
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Khan MKJ, 2019, ELECTRONICS-SWITZ, V8, DOI 10.3390/electronics8101115
   Lee Y.-H., 2020, EUR C COMP VIS
   Li YD, 2021, APPL INTELL, V51, P3012, DOI 10.1007/s10489-020-02100-9
   Lin Q, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5092, DOI 10.1145/3474085.3475559
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lugaresi C., 2019, 3 WORKSHOP COMPUTER
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Nazeri K., 2019, arXiv
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Qiu HB, 2022, IEEE T PATTERN ANAL, V44, P6939, DOI 10.1109/TPAMI.2021.3098962
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song L., 2018, NAT C ART INT
   Tam SY, 2021, FRONT PUBLIC HEALTH, V9, DOI 10.3389/fpubh.2021.665708
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang M, 2021, NEUROCOMPUTING, V429, P215, DOI 10.1016/j.neucom.2020.10.081
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yin X., 2023, 2023 IEEE 17 INT C A, P1, DOI [10.1109/FG57933.2023.10042570, DOI 10.1109/FG57933.2023.10042570]
   Zhang H, 2022, IEEE COMPUT SOC CONF, P2735, DOI 10.1109/CVPRW56347.2022.00309
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhang YX, 2022, PATTERN RECOGN, V126, DOI 10.1016/j.patcog.2022.108522
NR 35
TC 0
Z9 0
U1 1
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 6
PY 2023
DI 10.1007/s00371-023-03125-0
EA NOV 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X2OZ7
UT WOS:001096914400001
DA 2024-07-18
ER

PT J
AU Jia, Q
   Chen, XY
   Wang, Y
   Fan, X
   Ling, HB
   Latecki, LJ
AF Jia, Qi
   Chen, Xinyu
   Wang, Yi
   Fan, Xin
   Ling, Haibin
   Latecki, Longin Jan
TI A rotation robust shape transformer for cartoon character recognition
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Shape representation and learning; Transformer; Positional encoding;
   Cartoon character recognition
ID BAG
AB Recognizing cartoon characters accurately is important for animators to design and create cartoon scenarios by utilizing existing cartoon materials. Current deep learning approaches are sensitive to image rotation and heavily rely on rich textures that rarely exist in cartoon figures. In order to address this problem, the focus of our work is on the distinct nature of shapes, which mostly encodes the geometric structure of contours, rendering more discriminative and robust features than textures. We propose a rotation robust shape transformer for cartoon character recognition. As the filters in deep learning hardly detect discriminative gradient information in cartoon figures, we leverage multi-scale shape context (SC) to obtain the geometry of contour sampling points other than differences in gray level. Further, we propose a rotation-invariant positional encoding to depict the geometric relations of local shape features. The contributions of the different scales of SC templates are learned by attention-based transformer encoder. The obtained network is able to learn shape information effectively from cartoon contours only. The simplistic design attains surprisingly nearly 100% recognition accuracy, which beats both handcrafted and deep learning methods on the proposed challenging Cartoon dataset and traditional datasets. In particular, we gain 86.19% recognition accuracy on rotation test set, rendering an overwhelming superiority of 58.30 percentage higher than the state-of-the-art methods. Moreover, we develop an online cartoon character recognition application for animation scenarios.
C1 [Jia, Qi; Wang, Yi; Fan, Xin] Dalian Univ Technol, Int Sch Informat Sci & Engn, Dalian, Peoples R China.
   [Chen, Xinyu] Dalian Univ Technol, Sch Software Engn, Dalian, Peoples R China.
   [Ling, Haibin] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
   [Latecki, Longin Jan] Temple Univ, Dept Comp & Informat Sci, Philadelphia, PA USA.
C3 Dalian University of Technology; Dalian University of Technology; State
   University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook; Pennsylvania Commonwealth System of Higher Education
   (PCSHE); Temple University
RP Fan, X (corresponding author), Dalian Univ Technol, Int Sch Informat Sci & Engn, Dalian, Peoples R China.
EM xin.fan@dlut.edu.cn
FU This work was supported in part by the Natural Science Foundation of
   China under Grant 62272083 and Grant 61876030, in part by the Liaoning
   Provincial Natural Science Foundation under Grant 2022-MS-128, in part
   by the Fundamental Research Funds for the Cen [62272083, 61876030];
   Natural Science Foundation of China [2022-MS-128]; Liaoning Provincial
   Natural Science Foundation [DUT23YG109]; Fundamental Research Funds for
   the Central Universities [I IS-1814745]; U.S. National Science
   Foundation
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 62272083 and Grant 61876030, in part by the Liaoning
   Provincial Natural Science Foundation under Grant 2022-MS-128, in part
   by the Fundamental Research Funds for the Central Universities
   DUT23YG109, and in part by the U.S. National Science Foundation under
   Grant I IS-1814745.
CR Bai X, 2009, IEEE I CONF COMP VIS, P575, DOI 10.1109/ICCV.2009.5459188
   Belongie S, 2001, ADV NEUR IN, V13, P831
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chen SL, 2017, PATTERN RECOGN, V68, P82, DOI 10.1016/j.patcog.2017.03.007
   Chu XX, 2021, Arxiv, DOI [arXiv:2102.10882, DOI 10.48550/ARXIV.2102.10882]
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai GX, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P670
   Dai ZH, 2019, Arxiv, DOI [arXiv:1901.02860, DOI 10.48550/ARXIV.1901.02860]
   Dosovitskiy Alexey, 2020, ABS201011929 CORR
   Gehring J, 2017, PR MACH LEARN RES, V70
   Geirhos R, 2019, Arxiv, DOI arXiv:1811.12231
   Haseyama M., 2003, P 2003 INT C IM PROC, V3, pIII
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu RX, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2013.2286330
   Hu RX, 2012, PATTERN RECOGN, V45, P3222, DOI 10.1016/j.patcog.2012.02.020
   Jia Q, 2016, PATTERN RECOGN, V52, P358, DOI 10.1016/j.patcog.2015.11.003
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI [10.14722/ndss.2017.23457, 10.1016/j.patcog.2017.05.015]
   Li F.-F., 2003, Caltech101 image dataset
   Li Y, 2021, Arxiv, DOI arXiv:2107.06532
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41
   Micusik B, 2017, INT J COMPUT VISION, V124, P65, DOI 10.1007/s11263-016-0971-9
   Miyagi R, 2017, ASIAPAC SIGN INFO PR, P320, DOI 10.1109/APSIPA.2017.8282044
   Rios E.A., 2021, arXiv
   Ritter S, 2017, PR MACH LEARN RES, V70
   Sabour S, 2017, ADV NEUR IN, V30
   Sarvadevabhatla R. K., 2016, ACM MM, P247
   Shaw P., 2018, P 2018 NAACL, V2, P464, DOI [DOI 10.18653/V1/N18-2074, 10.18653/v1/N18-2074]
   Shekar BH, 2015, PERCEPTION AND MACHINE INTELLIGENCE, 2015, P46, DOI 10.1145/2708463.2709062
   Shen W, 2018, PATTERN RECOGN LETT, V106, P33, DOI 10.1016/j.patrec.2018.02.024
   Simard PY, 2003, PROC INT CONF DOC, P958
   Sirin Y, 2017, MULTIMED TOOLS APPL, V76, P7823, DOI 10.1007/s11042-016-3422-2
   Soderkvist O., 2001, COMPUTER VISION CLAS
   Srinivas A, 2021, PROC CVPR IEEE, P16514, DOI 10.1109/CVPR46437.2021.01625
   Sutskever Ilya, 2011, P 28 INT C MACH LEAR, P1017
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JW, 2012, PATTERN RECOGN LETT, V33, P134, DOI 10.1016/j.patrec.2011.09.042
   Wang TQ, 2018, AAAI CONF ARTIF INTE, P2540
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang XY, 2017, J NEUROSCI, V37, P4705, DOI 10.1523/JNEUROSCI.3622-16.2017
   Wang XG, 2014, PATTERN RECOGN, V47, P2116, DOI 10.1016/j.patcog.2013.12.008
   Xu P, 2018, PROC CVPR IEEE, P8090, DOI 10.1109/CVPR.2018.00844
   Yu J, 2012, IEEE T SYST MAN CY B, V42, P1413, DOI 10.1109/TSMCB.2012.2192108
   Yu Q, 2016, PROC CVPR IEEE, P799, DOI 10.1109/CVPR.2016.93
   Yu Q, 2017, INT J COMPUT VISION, V122, P411, DOI 10.1007/s11263-016-0932-3
NR 47
TC 1
Z9 1
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 27
PY 2023
DI 10.1007/s00371-023-03123-2
EA OCT 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W0BN1
UT WOS:001088370100002
DA 2024-07-18
ER

PT J
AU Li, CX
   Guan, YY
   Yang, SS
   Li, YH
AF Li, Cuixia
   Guan, Yuyin
   Yang, Shanshan
   Li, Yinghao
TI A dynamic learning framework integrating attention mechanism for point
   cloud registration
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Point cloud registration; Dynamic feature extraction; Edge convolution;
   Graph attention network; Offset-attention
ID HISTOGRAMS
AB To improve the low accuracy problem of existing point cloud registration algorithms attributed to deficient point cloud geometric features, we proposed a new point cloud registration network inspired by dynamic feature extraction and the graph attention mechanism. The model uses the dynamic graph edge convolution neural network to characterize the multi-level semantics of the point cloud at first, then uses a feature fusion module based on attention mechanism to fuse the representation information, and finally uses the singular value decomposition (SVD) method to generate the transformation matrix. The experimental verification was carried out on the ModelNet40, ShapeNet Part datasets, and the local industrial part dataset. Experiment results show that our model gets competitive registration performance compared with other advanced models on three datasets. When tested on the untrained data class and the noisy circumstances, our model gets lower average registration errors than compared models. It shows that our framework has not only the characteristics of high registration accuracy and generalization ability but also strong robustness.
C1 [Li, Cuixia; Guan, Yuyin; Yang, Shanshan; Li, Yinghao] Zhengzhou Univ, Sch Cyber Sci & Engn, Zhengzhou 450002, Peoples R China.
C3 Zhengzhou University
RP Li, YH (corresponding author), Zhengzhou Univ, Sch Cyber Sci & Engn, Zhengzhou 450002, Peoples R China.
EM yinghaoli@zzu.edu.cn
FU This study was supported by the National Natural Science Foundation of
   China under Grant Nos. 62206252. And this work was supported in part by
   the National Key Ramp;D Program (2020YFB1712401, 2018YFB1701400), Major
   Science and Technology Project in Henan [62206252]; National Natural
   Science Foundation of China [2020YFB1712401, 2018YFB1701400]; National
   Key Ramp;D Program [201300210500]; Major Science and Technology Project
   in Henan Province [23A520015]; Key scientific research projects of
   colleges and universities in Henan Province
FX This study was supported by the National Natural Science Foundation of
   China under Grant Nos. 62206252. And this work was supported in part by
   the National Key R&D Program (2020YFB1712401, 2018YFB1701400), Major
   Science and Technology Project in Henan Province (201300210500), Key
   scientific research projects of colleges and universities in Henan
   Province(23A520015).
CR Aiger D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360684
   Ao S, 2021, PROC CVPR IEEE, P11748, DOI 10.1109/CVPR46437.2021.01158
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Biber P, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P2743, DOI 10.1109/iros.2003.1249285
   Chen MY, 2019, OPT LASER ENG, V122, P170, DOI 10.1016/j.optlaseng.2019.06.011
   Choy C, 2020, PROC CVPR IEEE, P2511, DOI 10.1109/CVPR42600.2020.00259
   Deng HW, 2019, PROC CVPR IEEE, P3239, DOI 10.1109/CVPR.2019.00336
   Frome A, 2004, LECT NOTES COMPUT SC, V3023, P224
   Ginzburg D, 2022, IEEE IMAGE PROC, P71, DOI 10.1109/ICIP46576.2022.9897800
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Huang SY, 2021, PROC CVPR IEEE, P4265, DOI 10.1109/CVPR46437.2021.00425
   Huang XS, 2021, Arxiv, DOI arXiv:2103.02690
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Li CX, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12031741
   Li J., 2020, P 16 EUR C COMP VIS, Vvol 12369, P378, DOI 10.1007/978-3-030-58586-023
   Lin GC, 2021, COMPUT ELECTRON AGR, V184, DOI 10.1016/j.compag.2021.106107
   Lu WX, 2019, IEEE I CONF COMP VIS, P12, DOI 10.1109/ICCV.2019.00010
   Meng QH, 2022, IEEE T PATTERN ANAL, V44, P4454, DOI 10.1109/TPAMI.2021.3063611
   Pais GD, 2020, PROC CVPR IEEE, P7191, DOI 10.1109/CVPR42600.2020.00722
   Qin Z, 2022, PROC CVPR IEEE, P11133, DOI 10.1109/CVPR52688.2022.01086
   Que Y, 2023, ENG STRUCT, V277, DOI 10.1016/j.engstruct.2022.115406
   Rusu RB, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3384, DOI 10.1109/IROS.2008.4650967
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Salti S, 2014, COMPUT VIS IMAGE UND, V125, P251, DOI 10.1016/j.cviu.2014.04.011
   Sarode V, 2019, Arxiv, DOI arXiv:1908.07906
   Si TZ, 2023, IEEE T MULTIMEDIA, V25, P4323, DOI 10.1109/TMM.2022.3174414
   Tang W, 2022, IEEE T IMAGE PROCESS, V31, P5134, DOI 10.1109/TIP.2022.3193288
   Tang YC, 2023, ENG STRUCT, V274, DOI 10.1016/j.engstruct.2022.115158
   Tao WY, 2024, VISUAL COMPUT, V40, P2615, DOI 10.1007/s00371-023-02942-7
   Vaswani Ashish, 2017, Advances in Neural Information Processing Systems, V2017
   Wang LJ, 2019, Arxiv, DOI arXiv:1904.01428
   Wang Y., 2019, ArXiv, Vabs/1910.12240, DOI [10.48550/arXiv.1910.12240, DOI 10.48550/ARXIV.1910.12240]
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wentao Yuan, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P733, DOI 10.1007/978-3-030-58558-7_43
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Yan ZH, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356573
   Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238
   Yin JB, 2022, LECT NOTES COMPUT SC, V13698, P727, DOI 10.1007/978-3-031-19839-7_42
   Yin JB, 2022, LECT NOTES COMPUT SC, V13699, P17, DOI 10.1007/978-3-031-19842-7_2
   Yin JB, 2023, IEEE T PATTERN ANAL, V45, P9822, DOI 10.1109/TPAMI.2021.3125981
   Zhang J, 2023, FRONT COMPUT SCI-CHI, V17, DOI 10.1007/s11704-022-1523-9
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zi Jian Yew, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11821, DOI 10.1109/CVPR42600.2020.01184
NR 44
TC 1
Z9 1
U1 4
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 20
PY 2023
DI 10.1007/s00371-023-03118-z
EA OCT 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA U9HU2
UT WOS:001087848800003
DA 2024-07-18
ER

PT J
AU Xi, MY
   Yan, H
AF Xi, Mengyuan
   Yan, Hua
TI Lightweight multi-scale network with attention for accurate and
   efficient crowd counting
SO VISUAL COMPUTER
LA English
DT Article
DE Crowd counting; Feature fusion; Lightweight network; Multi-scale;
   Normalized union loss
AB Crowd counting is a significant task in computer vision, which aims to estimate the total number of people appeared in images or videos. However, it is still very challenging due to the huge scale variation and uneven density distributions in dense scenes. Moreover, although many works have been presented to tackle these issues, these methods always have a large number of parameters and high computation complexity, which leads to a limitation to the wide applications in edge devices. In this work, we propose a lightweight method for accurate and efficient crowd counting, called lightweight multi-scale network with attention. It is mainly composed of four parts: lightweight extractor, multi-scale features extraction module (MFEM), attention-based fusion module (ABFM), and efficient density map regressor. We design the MFEM and ABFM delicately to obtain rich scale representations, which is significantly beneficial for improving the counting accuracy. Moreover, the normalized union loss function is proposed to balance contribution of samples with diverse density distributions. Extensive experiments carried out on six mainstream crowd datasets demonstrate that our proposed method achieves superior performance to the other state-of-the-art methods with a small model size and low computational cost.
C1 [Xi, Mengyuan; Yan, Hua] Sichuan Univ, Coll Elect & Informat Engn, Chengdu 610065, Sichuan, Peoples R China.
C3 Sichuan University
RP Yan, H (corresponding author), Sichuan Univ, Coll Elect & Informat Engn, Chengdu 610065, Sichuan, Peoples R China.
EM yanhua@scu.edu.cn
OI Yan, Hua/0000-0001-9231-3175
NR 0
TC 1
Z9 1
U1 4
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4553
EP 4566
DI 10.1007/s00371-023-03099-z
EA SEP 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001071597500001
DA 2024-07-18
ER

PT J
AU Sarupuri, B
   Kulpa, R
   Aristidou, A
   Multon, F
AF Sarupuri, Bhuvaneswari
   Kulpa, Richard
   Aristidou, Andreas
   Multon, Franck
TI Dancing in virtual reality as an inclusive platform for social and
   physical fitness activities: a survey
SO VISUAL COMPUTER
LA English
DT Article
DE Human-centered computing; Virtual reality; Dancing; Social platforms;
   OpenAI
AB Virtual reality (VR) has recently seen significant development in interaction with computers and the visualization of information. More and more people are using virtual and immersive technologies in their daily lives, especially for entertainment, fitness, and socializing purposes. This paper presents a qualitative evaluation of a large sample of users using a VR platform for dancing (N = 292); we study the users' motivations, experiences, and requirements for using VR as an inclusive platform for dancing, mainly as a social or physical activity. We used an artificial intelligence platform (OpenAI) to extract categories or clusters of responses automatically. We organized the data into six user motivation categories: fun, fitness, social activity, pandemic, escape from reality, and professional activities. Our results indicate that dancing in virtual reality is a different experience than in the real world, and there is a clear distinction in the user's motivations for using VR platforms for dancing. Our survey results suggest that VR is a tool that can positively impact physical and mental well-being through dancing. These findings complement the related work, help in identifying the use cases, and can be used to assist future improvements of VR dance applications.
C1 [Sarupuri, Bhuvaneswari; Kulpa, Richard; Multon, Franck] Univ Rennes, CNRS, INRIA, IRISA, Rennes, France.
   [Sarupuri, Bhuvaneswari; Kulpa, Richard; Multon, Franck] Univ Rennes, M2S, Rennes, France.
   [Aristidou, Andreas] Univ Cyprus, CY-1678 Nicosia, Cyprus.
C3 Inria; Centre National de la Recherche Scientifique (CNRS); Universite
   de Rennes; Universite de Rennes; University of Cyprus
RP Aristidou, A (corresponding author), Univ Cyprus, CY-1678 Nicosia, Cyprus.
EM bhuvaneswari.sarupuri@univ-rennes2.fr; richard.kulpa@univ-rennes2.fr;
   a.aristidou@ieee.org; franck.multon@irisa.fr
OI Aristidou, Andreas/0000-0001-7754-0791
FU JPICH Call on Cultural heritage [P2P/JPICH_DH/0417/0052]; Agence
   National pour la Recherche; Cyprus Research amp;amp; Innovation
   Foundation;  [Projet-ANR-17-JPCH-0004]
FX The work was funded as part of the JPICH Call on Cultural heritage.
   Funding agencies are the Agence National pour la Recherche
   (Projet-ANR-17-JPCH-0004), and the Cyprus Research & Innovation
   Foundation (with protocol number P2P/JPICH_DH/0417/0052).
CR Al-Yahmady H. H., 2013, INT INTERDISCIPLINAR, V2, P181, DOI [DOI 10.12816/0002914, https://doi.org/10.12816/0002914]
   [Anonymous], 2011, P 19 ACM INT C MULTI, DOI DOI 10.1145/2072298.2072412
   [Anonymous], 2021, DANCEVIRTUAL
   Aristidou A, 2021, P EG WORKSH GRAPH CU, DOI [10.2312/gch.20211405, DOI 10.2312/GCH.20211405]
   Aristidou A, 2023, IEEE T VIS COMPUT GR, V29, P3519, DOI 10.1109/TVCG.2022.3163676
   Aristidou A, 2019, ACM J COMPUT CULT HE, V12, DOI 10.1145/3344383
   Aristidou A, 2015, ACM J COMPUT CULT HE, V8, DOI 10.1145/2755566
   Aristidou Andreas., 2014, Cyprus Computer Society Journal, V25, P42
   Arndt S, 2018, PROCEEDINGS OF THE 1ST INTERNATIONAL WORKSHOP ON MULTIMEDIA CONTENT ANALYSIS IN SPORTS (MMSPORTS'18), P45, DOI 10.1145/3265845.3265848
   Benford S., 1995, Human Factors in Computing Systems. CHI'95 Conference Proceedings, P242
   Bideau B, 2010, IEEE COMPUT GRAPH, V30, P14, DOI 10.1109/MCG.2009.134
   Blacking John., 1983, DANCE RES, V1, P89, DOI [10.2307/1290805, DOI 10.2307/1290805]
   Blackwell Lindsay, 2019, Proceedings of the ACM on Human-Computer Interaction, V3, DOI 10.1145/3359202
   Brown T., 2020, P ADV NEUR INF PROC, V33, P1877
   Burdisso SG, 2021, J COMPUT SCI TECHNOL, V21, P1, DOI 10.24215/16666038.21.e01
   Cannavò A, 2018, PROCEEDINGS OF THE 4TH INTERNATIONAL CONFERENCE ON VIRTUAL REALITY (ICVR 2018), P26, DOI 10.1145/3198910.3198917
   Chan JCP, 2011, IEEE T LEARN TECHNOL, V4, P187, DOI 10.1109/TLT.2010.27
   Clay A, 2014, INT SYM MIX AUGMENT, P21, DOI 10.1109/ISMAR-AMH.2014.6935434
   Daily SB, 2014, PROCEEDINGS OF THE 45TH ACM TECHNICAL SYMPOSIUM ON COMPUTER SCIENCE EDUCATION (SIGCSE'14), P91, DOI 10.1145/2538862.2538917
   dos Santos ADP, 2017, PROCEEDINGS OF THE 25TH CONFERENCE ON USER MODELING, ADAPTATION AND PERSONALIZATION (UMAP'17), P183, DOI 10.1145/3079628.3079673
   Foster PP, 2013, FRONT AGING NEUROSCI, V5, DOI 10.3389/fnagi.2013.00004
   Freeman G, 2020, CHI'20: EXTENDED ABSTRACTS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3334480.3382923
   Friedman B, 1996, ACM T INFORM SYST, V14, P330, DOI 10.1145/230538.230561
   Herring MP, 2010, ARCH INTERN MED, V170, P321, DOI 10.1001/archinternmed.2009.530
   Howard D, 2020, J MED INTERNET RES, V22, DOI 10.2196/15371
   Jianxing S., 2021, ADV INTELLIGENT SYST
   Jonas M, 2019, CHI PLAY'19: EXTENDED ABSTRACTS OF THE ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P437, DOI 10.1145/3341215.3356271
   Kassing G., 2020, Dance teaching methods and curriculum design: comprehensive K-12 dance education
   Kico I., 2020, STRATEGIC INNOVATIVE, P51, DOI [10.1007/978-3-030-36126-6_7, DOI 10.1007/978-3-030-36126-6_7]
   Kyan M, 2015, ACM T INTEL SYST TEC, V6, DOI 10.1145/2735951
   Leonhardt M, 2021, INT J ENV RES PUB HE, V18, DOI 10.3390/ijerph18116085
   Lin L, 2017, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2017), DOI 10.1145/3119881.3119884
   Lin T., 2021, P ACM C HUM FACT COM, DOI DOI 10.1145/3411764.3445649
   Maloney Divine, 2020, VRST '20: 26th ACM Symposium on Virtual Reality Software and Technology, DOI 10.1145/3385956.3418967
   Maloney Divine, 2020, CHI PLAY '20: Proceedings of the Annual Symposium on Computer-Human Interaction in Play, P510, DOI 10.1145/3410404.3414266
   Maraz A, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0122866
   Martins D, 2022, LECT NOTE NETW SYST, V469, P535, DOI 10.1007/978-3-031-04819-7_51
   Maselli A, 2013, FRONT HUM NEUROSCI, V7, DOI 10.3389/fnhum.2013.00083
   McVeigh-Schultz J, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300794
   McVeigh-Schultz J, 2018, DIS 2018: COMPANION PUBLICATION OF THE 2018 DESIGNING INTERACTIVE SYSTEMS CONFERENCE, P289
   Menin A, 2018, IEEE COMPUT GRAPH, V38, P57, DOI 10.1109/MCG.2018.021951633
   Merom D, 2016, FRONT AGING NEUROSCI, V8, DOI 10.3389/fnagi.2016.00026
   Merom D, 2013, BMC PUBLIC HEALTH, V13, DOI 10.1186/1471-2458-13-477
   Minaee S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3439726
   Montes J., 2019, P 9 LAT AM C HUM COM
   Mueller F, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2651
   Murcia C.Q., 2010, ARTS HEALTH, V2, P149, DOI DOI 10.1080/17533010903488582
   Özcimder K, 2016, P AMER CONTR CONF, P6465, DOI 10.1109/ACC.2016.7526687
   Parmar D, 2016, P IEEE VIRT REAL ANN, P131, DOI 10.1109/VR.2016.7504696
   Radford A, Improving language understanding by generative pre-training
   Radford A., 2019, LANGUAGE MODELS ARE
   Rammstedt B, 2007, J RES PERS, V41, P203, DOI 10.1016/j.jrp.2006.02.001
   Rzeszewski M., 2020, ROZWOJ REGIONALNY PO, VRegionalna, P57, DOI DOI 10.14746/RRPR.2020.51.06
   Santos MEC, 2014, IEEE T LEARN TECHNOL, V7, P38, DOI 10.1109/TLT.2013.37
   Senecal S, 2020, MULTIMED TOOLS APPL, V79, P24621, DOI 10.1007/s11042-020-09192-y
   Shi MY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3407659
   Siani A, 2021, HEALTH TECHNOL-GER, V11, P425, DOI 10.1007/s12553-021-00528-8
   Sikora C, 2017, INT CONF AFFECT, P548, DOI 10.1109/ACII.2017.8273653
   Slater M, 2008, FRONT HUM NEUROSCI, V2, DOI 10.3389/neuro.09.006.2008
   Smith S., 2018, INT J PERF ART DIGIT, V14, P199, DOI [10.1080/14794713.2018.1509256, DOI 10.1080/14794713.2018.1509256]
   Stavrakis E., 2012, Proceedings, P404, DOI 10.1007/978-3-642-34234-9_41
   Stock-Williams C., 2021, WHAT MAKES EFFECTIVE
   Sykownik P, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P546, DOI 10.1109/VR50410.2021.00079
   Takala TM, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P859, DOI [10.1109/VRW50115.2020.00282, 10.1109/VRW50115.2020.00283]
   Theodorou L, 2016, MOCO'16: PROCEEDINGS OF THE 3RD INTERNATIONAL SYMPOSIUM ON MOVEMENT AND COMPUTING, DOI 10.1145/2948910.2948928
   Weiss W., 1976, PUBLIC OPIN QUART, V40, P132, DOI [10.1086/268277, DOI 10.1086/268277]
   Whyatt CP, 2015, PROCEEDINGS OF THE 4TH INTERNATIONAL CONFERENCE ON MOVEMENT AND COMPUTING (MOCO'17), DOI 10.1145/3077981.3078055
   Wikipedia, 2022, DANC WIK FREE ENC
   Wu Zhen, 2021, Journal of Physics: Conference Series, V1828, DOI 10.1088/1742-6596/1828/1/012097
   Yang Z., 2006, Proceedings of the 14th annual ACM international conference on Multimedia, MULTIMEDIA '06, P723, DOI [10.1145/1180639.1180793, DOI 10.1145/1180639.1180793]
   Zacharatos H., 2013, P MOT GAM MIG 13, P39, DOI [10.1145/2522628.2522651, DOI 10.1145/2522628]
NR 71
TC 2
Z9 2
U1 11
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4055
EP 4070
AR s00371-023-03068-6
DI 10.1007/s00371-023-03068-6
EA SEP 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001066001600001
OA Green Published, hybrid
DA 2024-07-18
ER

PT J
AU Luo, Q
   Shao, J
   Dang, WL
   Geng, L
   Zheng, HY
   Liu, C
AF Luo, Qian
   Shao, Jie
   Dang, Wanli
   Geng, Long
   Zheng, Huaiyu
   Liu, Chang
TI An efficient multi-scale channel attention network for person
   re-identification
SO VISUAL COMPUTER
LA English
DT Article
DE Person re-identification; Convolutional neural network; Attention
   mechanism
AB At present, occlusion and similar appearance pose serious challenges to the task of person re-identification. In this work, we propose an efficient multi-scale channel attention network (EMCA) to learn robust and more discriminative features to solve these problems. Specifically, we designed a novel cross-channel attention module (CCAM) in EMCA and placed it after different layers in the backbone. The CCAM includes local cross-channel interaction (LCI) and channel weight integration (CWI). LCI focuses on both the maximum pooling features and the average pooling features to generate channel weights through convolutional layers, respectively. CWI combines the two channel weights to generate richer and more discriminant channel weights. Experiments on four popular person Re-ID datasets (Market-1501, DukeMTMC-ReID, CUHK-03 (detected) and MSMT17) show that the performance of our EMCA is consistently significantly superior to the existing state-of-the-art methods.
C1 [Luo, Qian; Shao, Jie] Xihua Univ, Chengdu 610039, Peoples R China.
   [Luo, Qian; Dang, Wanli; Geng, Long; Zheng, Huaiyu; Liu, Chang] Second Res Inst Civil Aviat Adm China, Chengdu 610041, Peoples R China.
C3 Xihua University
RP Luo, Q (corresponding author), Xihua Univ, Chengdu 610039, Peoples R China.; Luo, Q (corresponding author), Second Res Inst Civil Aviat Adm China, Chengdu 610041, Peoples R China.
EM luoqian@caacetc.com; shaojie@stu.xhu.edu.cn; dangwanli@caacsri.com;
   genglong@caacsri.com; zhenghuaiyu@caacsri.com; liuchang@caacsri.com
FU NNSFC; CAAC [U2133211]; National Natural Science Foundation of China
   [62203452]
FX This document is the results of the research project funded by the NNSFC
   and CAAC (U2133211) and the Young Scientists Fund of the National
   Natural Science Foundation of China (62203452).
CR Bolle RM, 2005, FOURTH IEEE WORKSHOP ON AUTOMATIC IDENTIFICATION ADVANCED TECHNOLOGIES, PROCEEDINGS, P15, DOI 10.1109/AUTOID.2005.48
   Chen GY, 2021, IEEE T IMAGE PROCESS, V30, P7663, DOI 10.1109/TIP.2021.3107211
   Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844
   Chen XM, 2021, IEEE T IMAGE PROCESS, V30, P1935, DOI 10.1109/TIP.2021.3049943
   Chen YF, 2022, PATTERN RECOGN, V126, DOI 10.1016/j.patcog.2022.108567
   Chen YC, 2018, IEEE T PATTERN ANAL, V40, P392, DOI 10.1109/TPAMI.2017.2666805
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Gao S, 2020, P IEEE CVF C COMP VI, P11744
   Gong YC, 2020, IEEE ACCESS, V8, P203700, DOI 10.1109/ACCESS.2020.3036985
   Gu XQ, 2022, PROC CVPR IEEE, P1050, DOI 10.1109/CVPR52688.2022.00113
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Lai SQ, 2021, IEEE INT CONF COMP V, P4133, DOI 10.1109/ICCVW54120.2021.00461
   Li S, 2020, INT J COMPUT VISION, V128, P2936, DOI 10.1007/s11263-020-01349-4
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li YL, 2021, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR46437.2021.00292
   Li ZY, 2021, COMPUT VIS IMAGE UND, V205, DOI 10.1016/j.cviu.2021.103172
   Lian SC, 2021, IEEE T CIRC SYST VID, V31, P3140, DOI 10.1109/TCSVT.2020.3037179
   Liao SC, 2022, PROC CVPR IEEE, P7349, DOI 10.1109/CVPR52688.2022.00721
   Lingxiao He, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P357, DOI 10.1007/978-3-030-58604-1_22
   Liu YH, 2021, IEEE T IMAGE PROCESS, V30, P2060, DOI 10.1109/TIP.2021.3050839
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Martinel N, 2020, IEEE T IMAGE PROCESS, V29, P7306, DOI 10.1109/TIP.2020.3000904
   Pervaiz N, 2023, VISUAL COMPUT, V39, P4087, DOI 10.1007/s00371-022-02577-0
   Pu N., 2021, P IEEE CVF C COMP VI, P7901
   Rao YM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1005, DOI 10.1109/ICCV48922.2021.00106
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Shizhen Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P647, DOI 10.1007/978-3-030-58539-6_39
   Somers V, 2023, IEEE WINT CONF APPL, P1613, DOI 10.1109/WACV56688.2023.00166
   Sun J, 2021, PATTERN RECOGN, V116, DOI 10.1016/j.patcog.2021.107937
   Wang K, 2021, IEEE T IMAGE PROCESS, V30, P3405, DOI 10.1109/TIP.2021.3060909
   Wang PY, 2021, IEEE T IMAGE PROCESS, V30, P2908, DOI 10.1109/TIP.2021.3055952
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang ZK, 2022, PROC CVPR IEEE, P4744, DOI 10.1109/CVPR52688.2022.00471
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu D, 2021, IEEE T EM TOP COMP I, V5, P70, DOI 10.1109/TETCI.2020.3034606
   Yan C., 2021, P IEEECVF INT C COMP, P10943
   Yang J., 2021, P IEEE INT C COMP VI, P11885
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yin J, 2020, INT J COMPUT VISION, V128, P1654, DOI 10.1007/s11263-019-01259-0
   Zahra A., 2022, ARXIV
   Zhang AG, 2021, PROC CVPR IEEE, P598, DOI 10.1109/CVPR46437.2021.00066
   Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325
   Zhang ZZ, 2020, IEEE T IMAGE PROCESS, V29, P7104, DOI 10.1109/TIP.2020.2998931
   Zhang Z, 2021, PATTERN RECOGN, V120, DOI 10.1016/j.patcog.2021.108155
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhong YJ, 2021, IEEE T IMAGE PROCESS, V30, P8384, DOI 10.1109/TIP.2021.3113183
   Zhou KY, 2022, IEEE T PATTERN ANAL, V44, P5056, DOI 10.1109/TPAMI.2021.3069237
   Zhou QQ, 2020, IEEE T IMAGE PROCESS, V29, P7578, DOI 10.1109/TIP.2020.3004267
NR 52
TC 2
Z9 2
U1 7
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3515
EP 3527
DI 10.1007/s00371-023-03049-9
EA AUG 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001060100700001
OA hybrid
DA 2024-07-18
ER

PT J
AU Li, YZ
   Luo, F
   Xiao, CX
AF Li, Yuanzhen
   Luo, Fei
   Xiao, Chunxia
TI Monocular human depth estimation with 3D motion flow and surface normals
SO VISUAL COMPUTER
LA English
DT Article
DE Human depth estimation; 3D motion flow; Texture copy; Surface normal;
   Geometric consistency; Pseudo-label
AB We propose a novel monocular human depth estimation method using video sequences as training data. We jointly train the depth and 3D motion flow networks with photometric and 3D geometric consistency constraints. Instead of depth ground truth, we take the surface normal as the pseudo-label to supervise the depth network learning. The estimated depth may exist texture copy artifact when the clothes on the human body have patterns and text marks (non-dominant color). Thus, we also propose an approach to alleviate the texture copy problem by estimating and adjusting the color of non-dominant color areas. Extensive experiments on public datasets and the Internet have been conducted. The comparison results prove that our method can produce competitive human depth estimation and has better generalization ability than state-of-the-art methods.
C1 [Li, Yuanzhen; Luo, Fei; Xiao, Chunxia] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Hubei, Peoples R China.
C3 Wuhan University
RP Luo, F; Xiao, CX (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Hubei, Peoples R China.
EM yuanzhen@whu.edu.cn; luofei@whu.edu.cn; cxxiao@whu.edu.cn
RI Luo, Fei/HSB-5388-2023
OI Luo, Fei/0000-0002-2450-6159
FU NSFC [61972298]; Bingtuan Science and Technology Program [2019BC008];
   CAAI-Huawei MindSpore Open Fund
FX This work is partially supported by NSFC (No. 61972298), Bingtuan
   Science and Technology Program (No. 2019BC008), and CAAI-Huawei
   MindSpore Open Fund.
CR Aleotti F., 2021, CVPR, P206
   Alldieck T, 2019, IEEE I CONF COMP VIS, P2293, DOI 10.1109/ICCV.2019.00238
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], About us
   ARUN KS, 1987, IEEE T PATTERN ANAL, V9, P699, DOI 10.1109/TPAMI.1987.4767965
   Bian XW, 2022, COMPUT VIS MEDIA, V8, P273, DOI 10.1007/s41095-021-0242-8
   Chen ZP, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2700, DOI 10.1145/3503161.3548074
   Feng Qiao, 2022, NEURIPS
   Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Habermann M, 2020, PROC CVPR IEEE, P5051, DOI 10.1109/CVPR42600.2020.00510
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Huguet F, 2007, IEEE I CONF COMP VIS, P1342, DOI 10.1109/iccv.2007.4409000
   Jafarian Y, 2021, PROC CVPR IEEE, P12748, DOI 10.1109/CVPR46437.2021.01256
   Junhwa Hur, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7394, DOI 10.1109/CVPR42600.2020.00742
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kingma D. P., 2014, arXiv
   Krishna K, 1999, IEEE T SYST MAN CY B, V29, P433, DOI 10.1109/3477.764879
   Kuznietsov Y, 2017, PROC CVPR IEEE, P2215, DOI 10.1109/CVPR.2017.238
   Lähner Z, 2018, LECT NOTES COMPUT SC, V11208, P698, DOI 10.1007/978-3-030-01225-0_41
   Lazova V, 2019, INT CONF 3D VISION, P643, DOI 10.1109/3DV.2019.00076
   Li YZ, 2022, COMPUT VIS MEDIA, V8, P631, DOI 10.1007/s41095-022-0279-3
   Li YZ, 2021, VISUAL COMPUT, V37, P2567, DOI 10.1007/s00371-021-02206-2
   Li ZQ, 2021, PROC CVPR IEEE, P6494, DOI 10.1109/CVPR46437.2021.00643
   Li ZQ, 2019, PROC CVPR IEEE, P4516, DOI 10.1109/CVPR.2019.00465
   Liang XD, 2019, IEEE T PATTERN ANAL, V41, P871, DOI 10.1109/TPAMI.2018.2820063
   Liu XY, 2019, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2019.00062
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Luo F, 2023, VIS INFORM, V7, P66, DOI 10.1016/j.visinf.2022.12.002
   Luo F, 2021, LECT NOTES COMPUT SC, V13002, P54, DOI 10.1007/978-3-030-89029-2_4
   Mildenhall Ben, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P405, DOI 10.1007/978-3-030-58452-8_24
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   nghiaho, US
   Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123
   Petrovai A, 2022, PROC CVPR IEEE, P1568, DOI 10.1109/CVPR52688.2022.00163
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saito S, 2020, PROC CVPR IEEE, P81, DOI 10.1109/CVPR42600.2020.00016
   Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239
   Schuster R, 2018, IEEE WINT CONF APPL, P1056, DOI 10.1109/WACV.2018.00121
   She DY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1837, DOI 10.1145/3503161.3548325
   Tang SC, 2019, IEEE I CONF COMP VIS, P7749, DOI 10.1109/ICCV.2019.00784
   Teed Z, 2021, PROC CVPR IEEE, P8371, DOI 10.1109/CVPR46437.2021.00827
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedula S., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P722, DOI 10.1109/ICCV.1999.790293
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZR, 2020, IEEE WINT CONF APPL, P91, DOI 10.1109/WACV45572.2020.9093302
   Wei Y, 2021, PROC CVPR IEEE, P6950, DOI 10.1109/CVPR46437.2021.00688
   Xiu YL, 2022, PROC CVPR IEEE, P13286, DOI 10.1109/CVPR52688.2022.01294
   Yang GS, 2021, PROC CVPR IEEE, P1266, DOI 10.1109/CVPR46437.2021.00132
   Yu T, 2021, PROC CVPR IEEE, P5742, DOI 10.1109/CVPR46437.2021.00569
   Zhang F, 2021, PROC CVPR IEEE, P4965, DOI 10.1109/CVPR46437.2021.00493
   Zhang W., 2020, P EUR C COMP VIS, P512, DOI 10.1007/978-3-030-58595-2_31
   Zhang X, 2021, PROC CVPR IEEE, P3435, DOI 10.1109/CVPR46437.2021.00344
   Zhang ZT, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459871
   Zheng ZR, 2022, IEEE T PATTERN ANAL, V44, P3170, DOI 10.1109/TPAMI.2021.3050505
   Zheng ZR, 2019, IEEE I CONF COMP VIS, P7738, DOI 10.1109/ICCV.2019.00783
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
NR 60
TC 0
Z9 0
U1 6
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3701
EP 3713
DI 10.1007/s00371-023-02995-8
EA JUL 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001035766800001
OA Bronze
DA 2024-07-18
ER

PT J
AU Drobnitzky, M
   Friederich, J
   Egger, B
   Zschech, P
AF Drobnitzky, Moritz
   Friederich, Jonas
   Egger, Bernhard
   Zschech, Patrick
TI Survey and systematization of 3D object detection models and methods
SO VISUAL COMPUTER
LA English
DT Article
DE 3D object detection; 6-DoF; Tracking; Survey
ID POINT-CLOUD; R-CNN; 2D; SEGMENTATION; FEATURES
AB Strong demand for autonomous vehicles and the wide availability of 3D sensors are continuously fueling the proposal of novel methods for 3D object detection. In this paper, we provide a comprehensive survey of recent developments from 2012-2021 in 3D object detection covering the full pipeline from input data, over data representation and feature extraction to the actual detection modules. We introduce fundamental concepts, focus on a broad range of different approaches that have emerged over the past decade, and propose a systematization that provides a practical framework for comparing these approaches with the goal of guiding future development, evaluation, and application activities. Specifically, our survey and systematization of 3D object detection models and methods can help researchers and practitioners to get a quick overview of the field by decomposing 3DOD solutions into more manageable pieces.
C1 [Drobnitzky, Moritz; Zschech, Patrick] Tech Univ Dresden, Munchner Pl 3, D-01187 Dresden, Germany.
   [Friederich, Jonas] Univ Southern Denmark, Maersk McKinney Moller Inst, Campusvej 55, DK-5230 Odense, Denmark.
   [Egger, Bernhard; Zschech, Patrick] Friedrich Alexander Univ Erlangen Nurnberg, Schlosspl 4, D-91054 Erlangen, Germany.
C3 Technische Universitat Dresden; University of Southern Denmark;
   University of Erlangen Nuremberg
RP Zschech, P (corresponding author), Tech Univ Dresden, Munchner Pl 3, D-01187 Dresden, Germany.; Zschech, P (corresponding author), Friedrich Alexander Univ Erlangen Nurnberg, Schlosspl 4, D-91054 Erlangen, Germany.
EM patrick.zschech@fau.de
RI Zschech, Patrick/AAG-6517-2019; Egger, Bernhard/AAE-5389-2019
OI Zschech, Patrick/0000-0002-1105-8086; Egger,
   Bernhard/0000-0002-4736-2397; Friederich, Jonas/0000-0001-9034-5907
FU Federal Ministry of Education and Research (BMBF), Germany [01IS22080];
   Projekt DEAL
FX PZ acknowledges funding from the Federal Ministry of Education and
   Research (BMBF), Germany, within the project "White-Box-AI" (grant
   number 01IS22080).Open Access funding enabled and organized by Projekt
   DEAL.
CR Ahmadyan A, 2021, PROC CVPR IEEE, P7818, DOI 10.1109/CVPR46437.2021.00773
   Ali W, 2019, LECT NOTES COMPUT SC, V11131, P716, DOI 10.1007/978-3-030-11015-4_54
   Amirkhani A, 2023, VISUAL COMPUT, V39, P5293, DOI 10.1007/s00371-022-02660-6
   [Anonymous], 2014, Arxiv
   [Anonymous], 2018, 2018 IEEE 4 INT FORU, DOI [DOI 10.1109/RTSI.2018.8548514, 10.1109/RTSI.2018.8548514]
   Aprile WA, 2008, VISUAL COMPUT, V24, P941, DOI 10.1007/s00371-008-0278-0
   Arnold E, 2019, IEEE T INTELL TRANSP, V20, P3782, DOI 10.1109/TITS.2019.2892405
   Barabanau I, 2020, PROCEEDINGS OF THE 15TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 5: VISAPP, P652, DOI 10.5220/0009102506520659
   Bayoudh K, 2022, VISUAL COMPUT, V38, P2939, DOI 10.1007/s00371-021-02166-7
   Bello SA, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12111729
   Beltrán J, 2018, IEEE INT C INTELL TR, P3517, DOI 10.1109/ITSC.2018.8569311
   Bishop Christopher M., 2006, Pattern Recognition and Machine Learning, V4
   Brazil G, 2019, IEEE I CONF COMP VIS, P9286, DOI 10.1109/ICCV.2019.00938
   Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164
   Chabot F, 2017, PROC CVPR IEEE, P1827, DOI 10.1109/CVPR.2017.198
   Chen GC, 2022, VISUAL COMPUT, V38, P1051, DOI 10.1007/s00371-021-02067-9
   Chen Q, 2019, INT CON DISTR COMP S, P514, DOI 10.1109/ICDCS.2019.00058
   Chen X., ADV NEURAL INFORM PR, V28, P424
   Chen XZ, 2018, IEEE T PATTERN ANAL, V40, P1259, DOI 10.1109/TPAMI.2017.2706685
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Chen XZ, 2016, PROC CVPR IEEE, P2147, DOI 10.1109/CVPR.2016.236
   Chen YL, 2019, IEEE I CONF COMP VIS, P9774, DOI [10.1109/iccv.2019.00987, 10.1109/ICCV.2019.00987]
   Cheng ZY, 2022, LECT NOTES COMPUT SC, V13698, P514, DOI 10.1007/978-3-031-19839-7_30
   Chenhang He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11870, DOI 10.1109/CVPR42600.2020.01189
   Crivellaro A, 2015, IEEE I CONF COMP VIS, P4391, DOI 10.1109/ICCV.2015.499
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Davies ER, 2012, COMPUTER AND MACHINE VISION: THEORY, ALGORITHMS, PRACTICALITIES, 4TH EDITION, P1
   de La Garanderie GP, 2018, LECT NOTES COMPUT SC, V11217, P812, DOI 10.1007/978-3-030-01261-8_48
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng JJ, 2021, AAAI CONF ARTIF INTE, V35, P1201
   Deng Z, 2017, PROC CVPR IEEE, P398, DOI 10.1109/CVPR.2017.50
   Du XX, 2018, IEEE INT CONF ROBOT, P3194
   Engelcke Martin, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P1355, DOI 10.1109/ICRA.2017.7989161
   Ferguson M, 2019, IEEE WINT CONF APPL, P1357, DOI 10.1109/WACV.2019.00149
   Fernandes D, 2021, INFORM FUSION, V68, P161, DOI 10.1016/j.inffus.2020.11.002
   Fidler S., 2012, NEURIPS, P611
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Friederich J., 2020, P 15 INT C WIRTSCHAF, P1699, DOI DOI 10.30844/WI_2020_R2-FRIEDRICH
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Giancola S, 2018, SPRINGERBRIEF COMPUT, P1, DOI 10.1007/978-3-319-91761-0
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Graham B., 2015, P BRIT MACH VIS C 20, P1501, DOI [10.5244/c.29.150, DOI 10.5244/C.29.150]
   Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961
   Griffiths D, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11121499
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Gupta I, 2019, LECT NOTES COMPUT SC, V11133, P626, DOI 10.1007/978-3-030-11021-5_39
   Gustafsson FK, 2021, IEEE COMPUT SOC CONF, P2849, DOI 10.1109/CVPRW53098.2021.00320
   He RT, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (IEEE ROBIO 2017), P1527, DOI 10.1109/ROBIO.2017.8324634
   Hinterstoisser S, 2011, IEEE I CONF COMP VIS, P858, DOI 10.1109/ICCV.2011.6126326
   Huang SY, 2018, ADV NEUR IN, V31
   Huang Y, 2020, COMPANION OF THE 2020 IEEE 20TH INTERNATIONAL CONFERENCE ON SOFTWARE QUALITY, RELIABILITY, AND SECURITY (QRS-C 2020), P221, DOI 10.1109/QRS-C51114.2020.00045
   Janiesch C, 2021, ELECTRON MARK, V31, P685, DOI 10.1007/s12525-021-00475-2
   Ji CF, 2023, VISUAL COMPUT, V39, P4543, DOI 10.1007/s00371-022-02607-x
   Jorgensen E., 2019, ARXIV
   Kehl W, 2016, LECT NOTES COMPUT SC, V9907, P205, DOI 10.1007/978-3-319-46487-9_13
   Kim JU, 2017, 2017 IEEE THIRD INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2017), P303, DOI 10.1109/BigMM.2017.59
   KITTI, 2021, KITT 3DOD BENCHM
   Ku J, 2018, IEEE INT C INT ROBOT, P5750, DOI 10.1109/IROS.2018.8594049
   Ku J, 2019, PROC CVPR IEEE, P11859, DOI 10.1109/CVPR.2019.01214
   Kuang HW, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20030704
   Lahoud J, 2017, IEEE I CONF COMP VIS, P4632, DOI 10.1109/ICCV.2017.495
   Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298
   Lefsky MA, 2002, BIOSCIENCE, V52, P19, DOI 10.1641/0006-3568(2002)052[0019:LRSFES]2.0.CO;2
   Lehner J., 2019, ARXIV
   Leibe B, 2008, INT J COMPUT VISION, V77, P259, DOI 10.1007/s11263-007-0095-3
   Li B, 2016, ROBOTICS SCI SYSTEMS, V12, DOI [DOI 10.15607/RSS.2016.XII.042, 10.15607/RSS.2016.XII.042]
   Li B, 2017, IEEE INT C INT ROBOT, P1513, DOI 10.1109/IROS.2017.8205955
   Li BY, 2019, PROC CVPR IEEE, P1019, DOI 10.1109/CVPR.2019.00111
   Li J, 2020, ARXIV
   Li ML, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19061434
   Li PL, 2019, PROC CVPR IEEE, P7636, DOI 10.1109/CVPR.2019.00783
   Li S, 2019, IEEE I CONF COMP VIS, P6608, DOI 10.1109/ICCV.2019.00671
   Li X., 2019, ARXIV
   Liang J, 2022, IEEE-CAA J AUTOMATIC, V9, P1083, DOI 10.1109/JAS.2022.105632
   Liang M, 2019, PROC CVPR IEEE, P7337, DOI 10.1109/CVPR.2019.00752
   Liang M, 2018, LECT NOTES COMPUT SC, V11220, P663, DOI 10.1007/978-3-030-01270-0_39
   Liang Z., 2020, ARXIV
   Liu DF, 2021, AAAI CONF ARTIF INTE, V35, P6101
   Liu J, 2018, CHIN CONT DECIS CONF, P4862, DOI 10.1109/CCDC.2018.8407973
   Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
   Liu LJ, 2019, PROC CVPR IEEE, P1057, DOI 10.1109/CVPR.2019.00115
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu WP, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19194188
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu HH, 2019, INT CONF ACOUST SPEE, P1992, DOI 10.1109/ICASSP.2019.8682746
   Luo QH, 2020, NEUROCOMPUTING, V378, P364, DOI 10.1016/j.neucom.2019.10.025
   Ma XZ, 2019, IEEE I CONF COMP VIS, P6850, DOI 10.1109/ICCV.2019.00695
   Mao JG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3144, DOI 10.1109/ICCV48922.2021.00315
   Meyer GP, 2019, IEEE COMPUT SOC CONF, P1230, DOI 10.1109/CVPRW.2019.00162
   Meyer GP, 2019, PROC CVPR IEEE, P12669, DOI 10.1109/CVPR.2019.01296
   Misra I, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2886, DOI 10.1109/ICCV48922.2021.00290
   Mousavian A, 2017, PROC CVPR IEEE, P5632, DOI 10.1109/CVPR.2017.597
   Naiden A, 2019, IEEE IMAGE PROC, P61, DOI [10.1109/ICIP.2019.8803397, 10.1109/icip.2019.8803397]
   Otepka J, 2013, ISPRS INT J GEO-INF, V2, P1038, DOI 10.3390/ijgi2041038
   Pamplona Jose, 2019, Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications. 23rd Iberoamerican Congress, CIARP 2018. Proceedings: Lecture Notes in Computer Science (LNCS 11401), P513, DOI 10.1007/978-3-030-13469-3_60
   Pan XR, 2021, PROC CVPR IEEE, P7459, DOI 10.1109/CVPR46437.2021.00738
   Pang S, 2020, IEEE INT C INT ROBOT, P10386, DOI 10.1109/IROS45743.2020.9341791
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qi CR, 2020, PROC CVPR IEEE, P4403, DOI 10.1109/CVPR42600.2020.00446
   Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Qin ZY, 2019, PROC CVPR IEEE, P7607, DOI 10.1109/CVPR.2019.00780
   Qin ZY, 2019, AAAI CONF ARTIF INTE, P8851
   Rahman MM, 2019, INFORM SCIENCES, V476, P147, DOI 10.1016/j.ins.2018.09.040
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren YZ, 2018, J VIS COMMUN IMAGE R, V55, P131, DOI 10.1016/j.jvcir.2018.05.019
   Ren ZL, 2018, PROC CVPR IEEE, P937, DOI 10.1109/CVPR.2018.00104
   Ren ZL, 2020, IEEE T PATTERN ANAL, V42, P2670, DOI 10.1109/TPAMI.2019.2923201
   Ren Z, 2016, PROC CVPR IEEE, P1525, DOI 10.1109/CVPR.2016.169
   Roddick T., 2018, ARXIV
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Sager C., 2021, CAD 21 P, P319, DOI [10.14733/cadconfP.2021.319-323, DOI 10.14733/CADCONFP.2021.319-323]
   Sager C, 2021, J BUS ANAL, V4, P91, DOI 10.1080/2573234X.2021.1908861
   Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054
   Shen XK, 2020, IEEE WINT CONF APPL, P1687, DOI 10.1109/WACV45572.2020.9093276
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Shi SS, 2021, IEEE T PATTERN ANAL, V43, P2647, DOI 10.1109/TPAMI.2020.2977026
   Shi WJ, 2020, PROC CVPR IEEE, P1708, DOI 10.1109/CVPR42600.2020.00178
   Shin K, 2019, IEEE INT VEH SYM, P2510, DOI 10.1109/IVS.2019.8813895
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Simon M, 2019, LECT NOTES COMPUT SC, V11129, P197, DOI 10.1007/978-3-030-11009-3_11
   Simon M, 2019, IEEE COMPUT SOC CONF, P1190, DOI 10.1109/CVPRW.2019.00158
   Simonelli A, 2019, IEEE I CONF COMP VIS, P1991, DOI 10.1109/ICCV.2019.00208
   Sindagi VA, 2019, IEEE INT CONF ROBOT, P7276, DOI [10.1109/ICRA.2019.8794195, 10.1109/icra.2019.8794195]
   Song SR, 2016, PROC CVPR IEEE, P808, DOI 10.1109/CVPR.2016.94
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Song SR, 2014, LECT NOTES COMPUT SC, V8694, P634, DOI 10.1007/978-3-319-10599-4_41
   Srivastava S, 2019, IEEE INT C INT ROBOT, P4504, DOI [10.1109/iros40897.2019.8967624, 10.1109/IROS40897.2019.8967624]
   Sun H, 2018, IEEE INT C INT ROBOT, P8331, DOI 10.1109/IROS.2018.8593837
   Sun P, 2020, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR42600.2020.00252
   Tang YS, 2019, IEEE I CONF COMP VIS, P1931, DOI 10.1109/ICCV.2019.00202
   Teng Z, 2014, IEEE INT CONF ROBOT, P5473, DOI 10.1109/ICRA.2014.6907664
   Tengteng Huang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P35, DOI 10.1007/978-3-030-58555-6_3
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang DL, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI
   Wang G., 2020, ARXIV
   Wang L, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19040893
   Wang Y, 2020, ARXIV
   Wang ZP, 2018, IEEE INT CON MULTI
   Wang ZX, 2019, IEEE INT C INT ROBOT, P1742, DOI [10.1109/IROS40897.2019.8968513, 10.1109/iros40897.2019.8968513]
   Weng XS, 2019, IEEE INT CONF COMP V, P857, DOI 10.1109/ICCVW.2019.00114
   Wu P, 2023, VISUAL COMPUT, V39, P2425, DOI 10.1007/s00371-022-02672-2
   Xu B, 2018, PROC CVPR IEEE, P2345, DOI 10.1109/CVPR.2018.00249
   Xu DF, 2018, PROC CVPR IEEE, P244, DOI 10.1109/CVPR.2018.00033
   Yamazaki T, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P3111, DOI 10.1109/ICASSP.2018.8461677
   Yan YS, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18051646
   Yang B, 2018, PROC CVPR IEEE, P7652, DOI 10.1109/CVPR.2018.00798
   Yang Bin, 2018, C ROBOT LEARNING, P146
   Yang Ze, arXiv
   Yang ZT, 2019, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2019.00204
   Yi Liu, 2018, 2018 2nd IEEE Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC). Proceedings, P462, DOI 10.1109/IMCEC.2018.8469573
   Yoo JH, 2020, SELECTED PAPERS FROM THE NINETEENTH BIENNIAL IEEE CONFERENCE ON ELECTROMAGNETIC FIELD COMPUTATION (IEEE CEFC 2020), DOI [10.1109/CEFC46938.2020.9451336, 10.1007/978-3-030-58583-9_43]
   Yuanyi Zhong, 2020, 2020 IEEE Winter Conference on Applications of Computer Vision (WACV). Proceedings, P1275, DOI 10.1109/WACV45572.2020.9093498
   Zeng YM, 2018, IEEE ROBOT AUTOM LET, V3, P3434, DOI 10.1109/LRA.2018.2852843
   Zetong Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11037, DOI 10.1109/CVPR42600.2020.01105
   Zhang H, 2022, IEEE COMPUT SOC CONF, P2735, DOI 10.1109/CVPRW56347.2022.00309
   Zhao Zhong-Qiu, 2019, IEEE Trans Neural Netw Learn Syst, V30, P3212, DOI 10.1109/TNNLS.2018.2876865
   Zheng W., 2020, ARXIV
   Zhou DF, 2020, PROC CVPR IEEE, P1836, DOI 10.1109/CVPR42600.2020.00191
   Zhou J., 2019, IEEE WCNC, P1, DOI [DOI 10.1109/wcnc.2019.8885523, DOI 10.1109/CISP-BMEI48845.2019.8965844]
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
   Zia MZ, 2015, INT J COMPUT VISION, V112, P188, DOI 10.1007/s11263-014-0780-y
NR 170
TC 3
Z9 3
U1 9
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1867
EP 1913
DI 10.1007/s00371-023-02891-1
EA JUL 2023
PG 47
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:001026397800007
OA hybrid
DA 2024-07-18
ER

PT J
AU Ye, M
   Yang, SE
   He, YJ
   Peng, ZJ
AF Ye, Meng
   Yang, Shi'en
   He, Yujun
   Peng, Zhangjun
TI A grayscale image enhancement algorithm based on dense residual and
   attention mechanism
SO VISUAL COMPUTER
LA English
DT Article
DE Low-light grayscale image; Image enhancement; Deep learning; Dense
   residual; Attention mechanism
ID RETINEX
AB Deep learning shows great potential in low-light image enhancement, which can improve image brightness and contrast while keeping image natural. However, due to the lack of prior knowledge extracted manually or excessive amplification of noise, these methods result in poor quality of enhanced images. To solve these challenging problems, this paper proposes a dual branch grayscale image enhancement network based on dense residual and attention mechanism. Low-light grayscale image is used as input. Firstly, features that fuse deep and shallow information are extracted through dense residual convolution branch network; secondly, texture features are extracted through U-Net branch network combined with attention mechanism; then the extracted features are integrated, and finally luminance is adjusted by a brightness adjustment module to output an enhanced grayscale image. In addition, a joint loss function is designed to measure the network training loss from brightness, texture, contrast and noise. A large number of quantitative and qualitative experiments on LOL and VE-LOL datasets show that the proposed method improves Peak Signal to Noise Ratio Index and Structural Similarity Index by 19.65-59.76% and 5.61-85.53%, respectively, compared with EnlightenGAN, KinD++, RUAS, LLFlow, etc. The proposed method is superior to the most famous methods, thanks to the deep feature extraction and fusion capability of dense residual convolutional network and texture extraction capability of U-Net network combined with attention mechanism.
C1 [Ye, Meng; Yang, Shi'en; He, Yujun; Peng, Zhangjun] Southwest Univ Sci & Technol, Sch Comp Sci & Technol, Mianyang 621000, Sichuan, Peoples R China.
C3 Southwest University of Science & Technology - China
RP Yang, SE (corresponding author), Southwest Univ Sci & Technol, Sch Comp Sci & Technol, Mianyang 621000, Sichuan, Peoples R China.
EM yemeng@mails.swust.edu.cn; yangshien00@163.com; heyujun@heyujun.com;
   pzj1@swust.edu.cn
OI ye, meng/0000-0002-3338-152X; peng, zhangjun/0000-0003-0270-9681
CR Aldin SS, 2024, VISUAL COMPUT, V40, P319, DOI 10.1007/s00371-023-02784-3
   Campos Carlos, 2021, IEEE Transactions on Robotics, V37, P1874, DOI 10.1109/TRO.2021.3075644
   [陈龙 Chen Long], 2020, [激光与红外, Laser and Infrared], V50, P1020
   Dai Shaosheng, 2010, Semiconductor Optoelectronics, V31, P161
   [丁畅 Ding Chang], 2017, [电子学报, Acta Electronica Sinica], V45, P1165
   Du NT, 2022, NEURAL PROCESS LETT, V54, P4769, DOI 10.1007/s11063-022-10832-7
   Fan MH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2317, DOI 10.1145/3394171.3413757
   Guo C., 2020, ZERO REFERENCE DEEP, P1780
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Jain A. K., 1989, Fundamentals of Digital Image Processing
   Jeon JJ, 2022, SIGNAL PROCESS, V196, DOI 10.1016/j.sigpro.2022.108523
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Kong XY, 2021, IEEE SIGNAL PROC LET, V28, P1540, DOI 10.1109/LSP.2021.3096160
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P9396, DOI 10.1109/TPAMI.2021.3126387
   Liu JY, 2021, INT J COMPUT VISION, V129, P1153, DOI 10.1007/s11263-020-01418-8
   Liu K, 2021, MULTIMED TOOLS APPL, V80, P19421, DOI 10.1007/s11042-021-10740-3
   Liu RS, 2021, PROC CVPR IEEE, P10556, DOI 10.1109/CVPR46437.2021.01042
   Liu SX, 2021, ENTROPY-SWITZ, V23, DOI 10.3390/e23060746
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma QT, 2023, IEEE T MULTIMEDIA, V25, P5580, DOI 10.1109/TMM.2022.3194993
   Ma TS, 2021, VISUAL COMPUT, V37, P925, DOI 10.1007/s00371-020-01843-3
   Qian SY, 2022, APPL INTELL, V52, P1770, DOI 10.1007/s10489-021-02466-4
   Rahman Z, 2021, VISUAL COMPUT, V37, P865, DOI 10.1007/s00371-020-01838-0
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Voulodimos A, 2018, COMPUT INTEL NEUROSC, V2018, DOI 10.1155/2018/7068349
   Wang CX, 2021, VISUAL COMPUT, V37, P77, DOI 10.1007/s00371-020-01888-4
   Wang LW, 2020, IEEE T IMAGE PROCESS, V29, P7984, DOI 10.1109/TIP.2020.3008396
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang XW, 2018, SIGNAL IMAGE VIDEO P, V12, P685, DOI 10.1007/s11760-017-1208-2
   Wang YF, 2022, AAAI CONF ARTIF INTE, P2604
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Wei C, 2018, ARXIV
   Yang Y, 2021, LASER OPTOELECTRON P, V58, DOI 10.3788/LOP202158.0610011
   Zhang YH, 2021, INT J COMPUT VISION, V129, P1013, DOI 10.1007/s11263-020-01407-x
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
NR 43
TC 0
Z9 0
U1 1
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1983
EP 1995
DI 10.1007/s00371-023-02896-w
EA JUN 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:001019061000002
DA 2024-07-18
ER

PT J
AU Zhang, YH
   Wang, HX
   Yang, G
   Zhang, JH
   Gong, CJ
   Wang, YT
AF Zhang, Yunhua
   Wang, Hangxu
   Yang, Gang
   Zhang, Jianhao
   Gong, Congjin
   Wang, Yutao
TI CSNet: a ConvNeXt-based Siamese network for RGB-D salient object
   detection
SO VISUAL COMPUTER
LA English
DT Article
DE Salient object detection; Siamese network; ConvNeXt; RGB-D SOD;
   Multi-modality
AB Global contexts are critical to locating salient objects for salient object detection (SOD). However, the convolution operation in CNNs has a local receptive field, which cannot capture long-distance global information. Recent studies have shown that modernized CNN models with large kernel convolution, such as ConvNeXt, can effectively extend the receptive fields. Based on it, this paper explores the potential of large kernel CNN for SOD task. Inspired by the common information between RGB and depth images in salient objects, we propose a ConvNeXt-based Siamese network with shared weight parameters. This structural design can effectively reduce the number of parameters without sacrificing performance. Furthermore, a depth information preprocessing module is proposed to minimize the impact of low-quality depth images on predicted saliency maps. For cross-modal feature interaction, a dynamic fusion module is designed to enhance cross-modal complementarity dynamically. Extensive experiments and evaluation results on six benchmark datasets demonstrate the outstanding performance of the proposed method against 14 state-of-the-art RGB-D methods. Our code will be released at .
C1 [Zhang, Yunhua; Wang, Hangxu; Yang, Gang; Zhang, Jianhao; Gong, Congjin; Wang, Yutao] Northeastern Univ, Shenyang 110819, Peoples R China.
   [Zhang, Yunhua] DUT Artificial Intelligence Inst, Dalian 116024, Peoples R China.
C3 Northeastern University - China
RP Yang, G (corresponding author), Northeastern Univ, Shenyang 110819, Peoples R China.
EM yanggang@mail.neu.edu.cn
RI Zhang, Jianhao/ABB-8327-2020
FU National Natural Science Foundation of China [62076058]
FX AcknowledgementsThis work is supported by the National Natural Science
   Foundation of China (Grant Number 62076058).
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen Q, 2021, AAAI CONF ARTIF INTE, V35, P1063
   Chen SH, 2023, VISUAL COMPUT, V39, P1437, DOI 10.1007/s00371-022-02421-5
   Chen ZY, 2021, IEEE T IMAGE PROCESS, V30, P7012, DOI 10.1109/TIP.2020.3028289
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Cheng MM, 2017, J COMPUT SCI TECH-CH, V32, P110, DOI 10.1007/s11390-017-1681-7
   Chongyi Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P225, DOI 10.1007/978-3-030-58598-3_14
   Ding XH, 2022, PROC CVPR IEEE, P11953, DOI 10.1109/CVPR52688.2022.01166
   Donoser M, 2009, IEEE I CONF COMP VIS, P817, DOI 10.1109/ICCV.2009.5459296
   Du C., 2021, ARXIV
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fu KR, 2022, IEEE T PATTERN ANAL, V44, P5541, DOI 10.1109/TPAMI.2021.3073689
   Gao Y, 2023, VISUAL COMPUT, V39, P3979, DOI 10.1007/s00371-022-02543-w
   Guo JY, 2022, PROC CVPR IEEE, P12165, DOI 10.1109/CVPR52688.2022.01186
   Han Q., 2021, ARXIV
   Hassanien MA, 2022, DIAGNOSTICS, V12, DOI 10.3390/diagnostics12051053
   Huang HM, 2020, INT CONF ACOUST SPEE, P1055, DOI [10.1109/ICASSP40776.2020.9053405, 10.1109/icassp40776.2020.9053405]
   Ji W, 2021, PROC CVPR IEEE, P9466, DOI 10.1109/CVPR46437.2021.00935
   Jin WD, 2021, IEEE T IMAGE PROCESS, V30, P3376, DOI 10.1109/TIP.2021.3060167
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li GY, 2021, IEEE T IMAGE PROCESS, V30, P3528, DOI 10.1109/TIP.2021.3062689
   Li GY, 2020, IEEE T IMAGE PROCESS, V29, P4873, DOI 10.1109/TIP.2020.2976689
   Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359
   Liang PP, 2016, IEEE SIGNAL PROC LET, V23, P949, DOI 10.1109/LSP.2016.2556706
   Liu C., 2022, ARXIV
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4702, DOI 10.1109/ICCV48922.2021.00468
   Liu S., 2022, ARXIV
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Y, 2021, IEEE T IMAGE PROCESS, V30, P3804, DOI 10.1109/TIP.2021.3065239
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZY, 2023, VISUAL COMPUT, V39, P2881, DOI 10.1007/s00371-022-02499-x
   Liu ZY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4481, DOI 10.1145/3474085.3475601
   Liu ZY, 2022, IEEE T CIRC SYST VID, V32, P4486, DOI 10.1109/TCSVT.2021.3127149
   Liu ZY, 2021, VISUAL COMPUT, V37, P529, DOI 10.1007/s00371-020-01821-9
   Liu ZY, 2019, NEUROCOMPUTING, V363, P46, DOI 10.1016/j.neucom.2019.07.012
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Loshchilov I., 2018, arXiv
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Nian Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13753, DOI 10.1109/CVPR42600.2020.01377
   Niu YZ, 2020, IEEE T IMAGE PROCESS, V29, P9496, DOI 10.1109/TIP.2020.3028170
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Pang YW, 2023, IEEE T IMAGE PROCESS, V32, P892, DOI 10.1109/TIP.2023.3234702
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Peng X., 2022, BALANCED MULTIMODAL
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Ren JQ, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301391
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun J, 2023, VISUAL COMPUT, V39, P4391, DOI 10.1007/s00371-022-02597-w
   Sun P, 2021, PROC CVPR IEEE, P1407, DOI 10.1109/CVPR46437.2021.00146
   Touvron H., 2020, P INT C MACH LEARN, P10347
   Wang J, 2022, VISUAL COMPUT, V38, P1803, DOI 10.1007/s00371-021-02106-5
   Wang NN, 2019, IEEE ACCESS, V7, P55277, DOI 10.1109/ACCESS.2019.2913107
   Wang WG, 2018, IEEE T CIRC SYST VID, V28, P1727, DOI 10.1109/TCSVT.2017.2701279
   Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8
   Wang YN, 2021, VISUAL COMPUT, V37, P1467, DOI 10.1007/s00371-020-01882-w
   Wu Y.-H., 2021, ARXIV
   Xiaoqi Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P646, DOI 10.1007/978-3-030-58542-6_39
   Yongri Piao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9057, DOI 10.1109/CVPR42600.2020.00908
   You Chenghui, 2022, 2022 IEEE International Conference on Visual Communications and Image Processing (VCIP), P1, DOI 10.1109/VCIP56404.2022.10008798
   Youwei Pang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P235, DOI 10.1007/978-3-030-58595-2_15
   Yu F., 2015, ARXIV
   Zhang Hongfei, 2022, Journal of Physics: Conference Series, V2289, DOI 10.1088/1742-6596/2289/1/012022
   Zhang LH, 2020, IEEE T IMAGE PROCESS, V29, P3534, DOI 10.1109/TIP.2019.2962688
   Zhang M, 2020, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR42600.2020.00353
   Zhang N, 2022, IEEE T IMAGE PROCESS, V31, P4556, DOI 10.1109/TIP.2022.3185550
   Zhang Z, 2021, IEEE T IMAGE PROCESS, V30, P1949, DOI 10.1109/TIP.2021.3049959
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460
   Zhou BY, 2021, LECT NOTES COMPUT SC, V13020, P397, DOI 10.1007/978-3-030-88007-1_33
   Zhu JY, 2015, IEEE T PATTERN ANAL, V37, P862, DOI 10.1109/TPAMI.2014.2353617
NR 76
TC 2
Z9 2
U1 9
U2 32
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1805
EP 1823
DI 10.1007/s00371-023-02887-x
EA MAY 2023
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000992757300001
DA 2024-07-18
ER

PT J
AU Deng, MF
   Zhao, HL
   Gao, M
AF Deng, Mingfang
   Zhao, Huailin
   Gao, Ming
TI CLFormer: a unified transformer-based framework for weakly supervised
   crowd counting and localization
SO VISUAL COMPUTER
LA English
DT Article
DE Shunted Transformer; Weakly supervised learning; Crowd counting; Crowd
   localization
ID SCALE
AB Recent progress in crowd counting and localization methods mainly relies on expensive point-level annotations and convolutional neural networks with limited receptive filed, which hinders their applications in complex real-world scenes. To this end, we present CLFormer, a Transformer-based weakly supervised crowd counting and localization framework. The model extracts global information from the input image using a Transformer and then passes the extracted features to both a regression branch for crowd counting and a localization branch for localization. Initial proposals are produced by the localization branch and filtered via score maps generated from the extracted features, and their centers are used as pseudo-point-level annotations. Through staggered training of the two branches, the quality of pseudo-point-level annotations is improved, and the final localization maps are generated. Experiments on four benchmark datasets (i.e., ShanghaiTech, UCF-QNRF, JHUCROWD++, and NWPU-Crowd) demonstrate that CLFormer obtains better counting performance than weakly supervised and fully supervised counting networks and comparable localization performance to fully supervised localization networks.
C1 [Deng, Mingfang; Zhao, Huailin; Gao, Ming] Shanghai Inst Technol, Sch Elect & Elect Engn, Shanghai 201418, Peoples R China.
C3 Shanghai Institute of Technology
RP Zhao, HL (corresponding author), Shanghai Inst Technol, Sch Elect & Elect Engn, Shanghai 201418, Peoples R China.
EM dengmingfang2021@163.com; tyoukr@163.com; gaoming_one@163.com
RI Zhao, Huailin/H-6597-2018
OI Mingfang, Deng/0000-0002-5744-9605
CR Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen, 2021, ARXIV
   Chen XQ, 2021, LECT NOTES COMPUT SC, V13019, P203, DOI 10.1007/978-3-030-88004-0_17
   Chu XX, 2021, ADV NEUR IN
   Dosovitskiy Alexey, 2020, ABS201011929 CORR
   Duan Z., 2021, IEEE Trans. Geosci. Remote Sens, V60, P1
   Gao J., ARXIV
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33
   Khan SD, 2021, VISUAL COMPUT, V37, P2127, DOI 10.1007/s00371-020-01974-7
   Lai QX, 2022, IEEE T IMAGE PROCESS, V31, P3111, DOI 10.1109/TIP.2022.3158064
   Laradji IH, 2020, IEEE IMAGE PROC, P2316, DOI 10.1109/ICIP40778.2020.9191122
   Lei YJ, 2021, PATTERN RECOGN, V109, DOI 10.1016/j.patcog.2020.107616
   Li B, 2023, VISUAL COMPUT, V39, P2671, DOI 10.1007/s00371-022-02485-3
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Li Z., 2022, VISUAL COMPUT, P1
   Liang DK, 2023, IEEE T MULTIMEDIA, V25, P6040, DOI 10.1109/TMM.2022.3203870
   Liang DK, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3445-y
   Lin H., 2021, ARXIV
   Lin H, 2022, PROC CVPR IEEE, P19596, DOI 10.1109/CVPR52688.2022.01901
   Liu CX, 2019, PROC CVPR IEEE, P82, DOI 10.1109/CVPR.2019.00017
   Liu N, 2019, PROC CVPR IEEE, P3220, DOI 10.1109/CVPR.2019.00334
   Liu Y., 2022, IEEE T PATTERN ANAL
   Liu YT, 2019, PROC CVPR IEEE, P6462, DOI 10.1109/CVPR.2019.00663
   Ma ZH, 2021, AAAI CONF ARTIF INTE, V35, P2319
   Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624
   Ren S., 2022, P IEEE CVF C COMP VI, P10853
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sajid U, 2021, IEEE INT CONF COMP V, P2249, DOI 10.1109/ICCVW54120.2021.00254
   Sam DB, 2021, IEEE T PATTERN ANAL, V43, P2739, DOI 10.1109/TPAMI.2020.2974830
   Sam DB, 2017, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2017.429
   Shi ML, 2019, 2019 1ST INTERNATIONAL CONFERENCE ON INDUSTRIAL ARTIFICIAL INTELLIGENCE (IAI 2019), DOI 10.1109/iciai.2019.8850794
   Sindagi VA, 2022, IEEE T PATTERN ANAL, V44, P2594, DOI 10.1109/TPAMI.2020.3035969
   Song QY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3345, DOI 10.1109/ICCV48922.2021.00335
   Topkaya IS, 2014, 2014 11TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P313, DOI 10.1109/AVSS.2014.6918687
   Tripathi G, 2019, VISUAL COMPUT, V35, P753, DOI 10.1007/s00371-018-1499-5
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Q, 2021, IEEE T PATTERN ANAL, V43, P2141, DOI 10.1109/TPAMI.2020.3013269
   Wang SZ, 2022, AAAI CONF ARTIF INTE, P2522
   Wang SZ, 2020, NEUROCOMPUTING, V404, P227, DOI 10.1016/j.neucom.2020.04.139
   Wang SY, 2022, IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, V60, DOI [10.1109/TGRS.2021.3057721, DOI 10.1109/TGRS.2021.3057721]
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang YB, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3465455
   Xie YJ, 2020, IEEE IMAGE PROC, P1531, DOI 10.1109/ICIP40778.2020.9191086
   Xu CF, 2022, INT J COMPUT VISION, V130, P405, DOI 10.1007/s11263-021-01542-z
   Yang YF, 2020, PROC CVPR IEEE, P4373, DOI 10.1109/CVPR42600.2020.00443
   Yifan Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P1, DOI 10.1007/978-3-030-58598-3_1
   Zhang DW, 2022, IEEE T PATTERN ANAL, V44, P5866, DOI 10.1109/TPAMI.2021.3074313
   ZHANG L, 2023, ACM SIGGRAPH 2020 EM, V39, P73, DOI DOI 10.1007/S00371-021-02313-0
   Zhang MJ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P6019, DOI 10.1145/3503161.3547919
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
   Zhou TF, 2023, MED IMAGE ANAL, V83, DOI 10.1016/j.media.2022.102599
   Zhou TF, 2022, IEEE T IMAGE PROCESS, V31, P799, DOI 10.1109/TIP.2021.3132834
   Zhou TF, 2020, IEEE T IMAGE PROCESS, V29, P8326, DOI 10.1109/TIP.2020.3013162
NR 54
TC 4
Z9 4
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 1053
EP 1067
DI 10.1007/s00371-023-02831-z
EA APR 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000968179700001
DA 2024-07-18
ER

PT J
AU Chen, QQ
   Yao, JF
   Long, JY
AF Chen, Qingqing
   Yao, Junfeng
   Long, Junyi
TI Similar image matching via global topology consensus
SO VISUAL COMPUTER
LA English
DT Article
DE Feature matching; Outlier removal; Global topology; Similar image
   matching
AB Recovering three-dimensional structure from images is one of the important researches in computer vision. The quality of feature matching is one of the keys to obtaining more accurate results. However, as different objects or different surfaces of objects have similar images with the same elements and different typography, the camera pose estimation will be wrong and the task will fail. This paper proposes a new mismatch elimination algorithm based on global topology consistency. We first formulate the matching task as a mathematical model based on the global constraints, then convert the feature matching into grid matching, calculate the confidence of the grids according to the changes in the angle and displacement between correspondence grid vectors, and remove the mismatches with low confidence. The experiments have demonstrated that our proposed method performs better than the state-of-the-art feature matching methods to accomplish outlier match rejection in the task of similar image matching and could be helped to obtain the correct camera pose to reconstruct more complete and more accurate object models.
C1 [Chen, Qingqing] Putian Univ, Coll Mechatron & Informat Engn, Putian, Fujian, Peoples R China.
   [Yao, Junfeng] Xiamen Univ, Sch Film, Xiamen, Fujian, Peoples R China.
   [Long, Junyi] Putian Univ, Coll New Engn Ind, Putian, Fujian, Peoples R China.
C3 Putian University; Xiamen University; Putian University
RP Chen, QQ (corresponding author), Putian Univ, Coll Mechatron & Informat Engn, Putian, Fujian, Peoples R China.
EM sharry778@163.com
RI Yao, Junfeng/ABE-6440-2020; chen, qingqing/GXV-4580-2022
OI chen, qingqing/0000-0002-7196-2376
FU Natural Science Foundation of China [62072388]; Collaborative Project
   Foundation of Fuzhou-XiamenQuanzhou Innovation Zone [3502ZCQXT202001];
   Industry Guidance Project Foundation of Science technology Bureau of
   Fujian province in 2020 [2020H0047]; Natural Science Foundation of
   Science Technology Bureau of Fujian province in 2019 [2019J01601];
   Creation Fund project of Science Technology Bureau of Fujian province in
   2019 [2019C0021]; Middle Youth Education Project of Fujian Province in
   2019 [JAT190596]; Fujian Sunshine Charity Foundation
FX This work was supported by the Natural Science Foundation of China (No.
   62072388), the Collaborative Project Foundation of Fuzhou-XiamenQuanzhou
   Innovation Zone(No.3502ZCQXT202001), the Industry Guidance Project
   Foundation of Science technology Bureau of Fujian province in 2020
   (No.2020H0047), the Natural Science Foundation of Science Technology
   Bureau of Fujian province in 2019 (No.2019J01601), the Creation Fund
   project of Science Technology Bureau of Fujian province in
   2019(No.2019C0021), the Middle Youth Education Project of Fujian
   Province in 2019 (No. JAT190596), and Fujian Sunshine Charity
   Foundation.
CR Aanæs H, 2016, INT J COMPUT VISION, V120, P153, DOI 10.1007/s11263-016-0902-9
   [Anonymous], 2015, Openmvs: Open Multiple View Stereovision
   Barath D, 2018, PROC CVPR IEEE, P6733, DOI 10.1109/CVPR.2018.00704
   Bay H., 2008, COMPUT VIS IMAGE UND, V10, P346, DOI DOI 10.1016/j.cviu.2007.09.014
   Bian JW, 2017, PROC CVPR IEEE, P2828, DOI 10.1109/CVPR.2017.302
   Brachmann E, 2019, IEEE I CONF COMP VIS, P4321, DOI 10.1109/ICCV.2019.00442
   Cao SY, 2020, IEEE T IMAGE PROCESS, V29, P5147, DOI 10.1109/TIP.2020.2980972
   Changchang Wu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3057, DOI 10.1109/CVPR.2011.5995552
   Chen ST, 2021, VISUAL COMPUT, V37, P411, DOI 10.1007/s00371-020-01811-x
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fuhrmann Simon, 2014, P WORKSH GRAPH CULT, P11, DOI [10.1016/j.cag.2015.09.003, DOI 10.2312/GCH.20141299]
   Gamba J., 2020, RADAR SIGNAL PROCESS, V105, P121
   Lazaridis G, 2006, IEEE T IMAGE PROCESS, V15, P2343, DOI 10.1109/TIP.2006.877346
   Li XR, 2010, INT J COMPUT VISION, V89, P1, DOI 10.1007/s11263-010-0318-x
   Liang L, 2020, NEUROCOMPUTING, V371, P1, DOI 10.1016/j.neucom.2019.06.101
   Liao QF, 2021, IEEE T PATTERN ANAL, V43, P3229, DOI 10.1109/TPAMI.2020.2978477
   Lin WY, 2018, IEEE T PATTERN ANAL, V40, P34, DOI 10.1109/TPAMI.2017.2652468
   Lipman Y, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2602142
   Liu C., 2022, Vis. Comput., P1
   Liu DF, 2022, VISUAL COMPUT, V38, P1689, DOI 10.1007/s00371-021-02098-2
   Liu Y, 2020, VISUAL COMPUT, V36, P827, DOI 10.1007/s00371-019-01656-z
   Liu YZ, 2021, NEUROCOMPUTING, V421, P273
   Loeckx D, 2010, IEEE T MED IMAGING, V29, P19, DOI 10.1109/TMI.2009.2021843
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma JY, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01359-2
   Ma JY, 2019, INT J COMPUT VISION, V127, P512, DOI [10.1109/TMAG.2017.2763198, 10.1007/s11263-018-1117-z]
   Ma JY, 2017, AAAI CONF ARTIF INTE, P4218
   Ma JY, 2014, IEEE T IMAGE PROCESS, V23, P1706, DOI 10.1109/TIP.2014.2307478
   Moulon P, 2017, LECT NOTES COMPUT SC, V10214, P60, DOI 10.1007/978-3-319-56414-2_5
   Moulon P, 2013, IEEE I CONF COMP VIS, P3248, DOI 10.1109/ICCV.2013.403
   Myatt D. R., 2002, Electronic Proceedings of the 13th British Machine Vision Conference, P458
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Schönberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31
   Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832
   Wang G, 2021, PATTERN RECOGN, V117, DOI 10.1016/j.patcog.2021.107986
   Wang G, 2016, NEUROCOMPUTING, V216, P393, DOI 10.1016/j.neucom.2016.08.009
   Wu CC, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P127, DOI 10.1109/3DV.2013.25
   Zhao Y, 2020, IEEE T ROBOT, V36, P657, DOI 10.1109/TRO.2020.2964138
   Zhu WQ, 2016, 2016 INTERNATIONAL CONFERENCE ON ROBOTS & INTELLIGENT SYSTEM (ICRIS), P94, DOI 10.1109/ICRIS.2016.19
NR 40
TC 0
Z9 0
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 937
EP 952
DI 10.1007/s00371-023-02824-y
EA MAR 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000960829200001
OA hybrid
DA 2024-07-18
ER

PT J
AU Hou, WQ
   Jing, HC
AF Hou, Wenqing
   Jing, Huicheng
TI RC-YOLOv5s: for tile surface defect detection
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Defect detection; Tiles; RC-YOLOv5s; Attention mechanism
AB To solve the problems of complex surface texture of magnetic tile, cumbersome process of traditional detection algorithm and low detection accuracy, this paper proposes a deep learning-based detection model: RC-YOLOv5s. The model incorporates two new structures: Res-Head and Drop-CA, where Res-Head enhances the feature fusion and information exchange between different layer structures and Drop-CA alleviates the case that the model pays too much attention to the defect target and reduces the missed detection rate of the model. Compared with YOLOv5s, the detection accuracy of the proposed model is improved by 1.83%, the missing rate is reduced from 1.673 to 0.372%, and the average detection frame rate of a single image reaches 41.67, which meets the requirements of real-time and accuracy of tile surface detection.
C1 [Hou, Wenqing; Jing, Huicheng] North China Univ Sci & Technol, Tangshan 063000, Peoples R China.
C3 North China University of Science & Technology
RP Jing, HC (corresponding author), North China Univ Sci & Technol, Tangshan 063000, Peoples R China.
EM Houwenqing123@163.com; Jinghc606@163.com
OI Hou, Wenqing/0000-0002-3597-7669
CR Bhatt PM, 2021, J COMPUT INF SCI ENG, V21, DOI 10.1115/1.4049535
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Chen ZY, 2023, VISUAL COMPUT, V39, P3995, DOI 10.1007/s00371-022-02554-7
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Guo ZX, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22093467
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu B, 2020, IEEE ACCESS, V8, P108335, DOI 10.1109/ACCESS.2020.3001349
   Hu H., 2019, Electromech. Eng., V36, P117
   Hu XL, 2021, COMPUT ELECTRON AGR, V185, DOI 10.1016/j.compag.2021.106135
   Huang YB, 2020, VISUAL COMPUT, V36, P85, DOI 10.1007/s00371-018-1588-5
   Jing JF, 2020, J ENG FIBER FABR, V15, DOI 10.1177/1558925020908268
   Larsson G, 2017, Arxiv, DOI [arXiv:1605.07648, DOI 10.48550/ARXIV.1605.07648]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Loshchilov I, 2019, Arxiv, DOI arXiv:1711.05101
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Minh M.T, 2022, ZENODO
   Paszke A, 2019, ADV NEUR IN, V32
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Tan M., 2020, P IEEECVF C COMPUTER, P10781, DOI [10.48550/arXiv.1911.09070, DOI 10.1109/CVPR42600.2020.01079]
   Torrey L., 2010, IGI Global, P242, DOI 10.4018/978-1-60566-766-9.CH011
   Wang J., 2021, arXiv
   Yang JF, 2017, IEEE I CONF COMP VIS, P1753, DOI 10.1109/ICCV.2017.193
NR 28
TC 10
Z9 10
U1 18
U2 61
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 459
EP 470
DI 10.1007/s00371-023-02793-2
EA MAR 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000941910100001
DA 2024-07-18
ER

PT J
AU Muhammad, ZUD
   Huang, ZJ
   Gu, NJ
   Muhammad, U
AF Muhammad, Zaka-Ud-Din
   Huang, Zhangjin
   Gu, Naijie
   Muhammad, Usman
TI DCANet: deep context attention network for automatic polyp segmentation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Deep learning; Medical image segmentation; Polyp segmentation;
   Colorectal cancer; Multi-attention
ID VALIDATION
AB Automatic and accurate polyp segmentation is significant for diagnosis and treatment of colorectal cancer. This is a challenging task due to the polyp's shape and size diversity. Recently, various deep convolutional neural networks have been developed for polyp segmentation. However, most state-of-the-art methods have suffered from a poor performance in the segmentation of smaller, fiat, or noisy polyp objects. In the paper, we propose a novel Deep Context Attention Network (DCANet) for accurate polyp segmentation based on an encoder-decoder framework. ResNet34 is adopted as the encoder, and five functional modules are introduced to improve the performance. Specifically, the improved local context attention module (LCAM) and global context module (GCM) are exploited to efficiently extract the local multi-scale and global multi-receptive-field context information, respectively. Then, an enhanced feature fusion module (FFM) is devised to effectively select and aggregate context features through spatial-channel attention. Finally, equipped with elaborately designed multi-attention modules (MAM), new decoder and supervision blocks are developed to accurately predict polyp regions via powerful channel-spatial-channel attention. Extensive experiments are conducted on the Kvasir-SEG and EndoScene benchmark datasets. The results demonstrate that the proposed network achieves superior performance compared to other state-of-the-art models. The source code will be available at https://github.com/ZAKAUDD/DCANet.
C1 [Muhammad, Zaka-Ud-Din; Huang, Zhangjin; Gu, Naijie] Univ Sci & Technol China, Sch Comp Sci & Technol, Hefei 230027, Peoples R China.
   [Muhammad, Zaka-Ud-Din; Huang, Zhangjin; Gu, Naijie] Anhui Prov Key Lab Software Comp & Commun, Hefei 230027, Peoples R China.
   [Huang, Zhangjin] USTC, Deqing Alpha Innovat Inst, Huzhou 313299, Peoples R China.
   [Muhammad, Usman] Univ Sci & Technol China, Dept Automat, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS; Chinese Academy of Sciences; University of
   Science & Technology of China, CAS
RP Huang, ZJ (corresponding author), Univ Sci & Technol China, Sch Comp Sci & Technol, Hefei 230027, Peoples R China.; Huang, ZJ (corresponding author), Anhui Prov Key Lab Software Comp & Commun, Hefei 230027, Peoples R China.; Huang, ZJ (corresponding author), USTC, Deqing Alpha Innovat Inst, Huzhou 313299, Peoples R China.
EM zaka494@mail.ustc.edu.cn; zhuang@ustc.edu.cn; gunj@ustc.edu.cn;
   usmanraza212@mail.ustc.edu.cn
RI Huang, Zhangjin/I-7929-2014; ZAKA-UD-DIN, MUHAMMAD/ITR-8490-2023
FU National Natural Science Foundation of China [71991464/71991460,
   61877056]
FX This work was supported in part by the National Natural Science
   Foundation of China (Nos. 71991464/71991460 and 61877056).
CR Ahuja S, 2021, APPL INTELL, V51, P571, DOI 10.1007/s10489-020-01826-w
   Akbari M, 2018, IEEE ENG MED BIO, P69, DOI 10.1109/EMBC.2018.8512197
   Angelica B., 2021, Colorectal cancer: symptoms, treatment, risk factors and more
   Arnold M, 2017, GUT, V66, P683, DOI 10.1136/gutjnl-2015-310912
   Asif M, 2021, APPL INTELL, V51, P1959, DOI 10.1007/s10489-020-01923-w
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bernal J, 2012, PATTERN RECOGN, V45, P3166, DOI 10.1016/j.patcog.2012.03.002
   Bernal J, 2017, IEEE T MED IMAGING, V36, P1231, DOI 10.1109/TMI.2017.2664042
   Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007
   Brandao P, 2017, PROC SPIE, V10134, DOI 10.1117/12.2254361
   Bychkov D, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-21758-3
   Chao P, 2019, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2019.00365
   Choudhury A, 2022, APPL INTELL, V52, P7339, DOI 10.1007/s10489-021-02688-6
   Cicek Ozgun, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P424, DOI 10.1007/978-3-319-46723-8_49
   Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26
   Dong B, 2024, Arxiv, DOI [arXiv:2108.06932, DOI 10.48550/ARXIV.2108.06932]
   Fang YQ, 2019, LECT NOTES COMPUT SC, V11764, P302, DOI 10.1007/978-3-030-32239-7_34
   Gloria R, 2020, What is colorectal cancer?, V06
   Guo YB, 2020, J IMAGING, V6, DOI 10.3390/jimaging6070069
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J., 2020, P IEEE C COMP VIS PA
   Huang C.-H., 2021, ARXIV, DOI DOI 10.48550/ARXIV.2101.07172
   Huang HM, 2020, INT CONF ACOUST SPEE, P1055, DOI [10.1109/ICASSP40776.2020.9053405, 10.1109/icassp40776.2020.9053405]
   Jha D, 2021, IEEE ACCESS, V9, P40496, DOI 10.1109/ACCESS.2021.3063716
   Jha D, 2020, COMP MED SY, P558, DOI 10.1109/CBMS49503.2020.00111
   Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37
   Jha D, 2019, IEEE INT SYM MULTIM, P225, DOI 10.1109/ISM46123.2019.00049
   Ji GP, 2021, LECT NOTES COMPUT SC, V12901, P142, DOI 10.1007/978-3-030-87193-2_14
   Jiafu Zhong, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P285, DOI 10.1007/978-3-030-59725-2_28
   Jorge B., 2018, P 32 CARS C
   Karkanis SA, 2003, IEEE T INF TECHNOL B, V7, P141, DOI 10.1109/TITB.2003.813794
   Kim T, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2167, DOI 10.1145/3474085.3475375
   Lai HL, 2023, VISUAL COMPUT, V39, P1453, DOI 10.1007/s00371-022-02422-4
   Leufkens AM, 2012, ENDOSCOPY, V44, P470, DOI 10.1055/s-0031-1291666
   Li QL, 2017, 2017 10TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI)
   Li S, 2021, arXiv, DOI 10.48550/arXiv.2105.09511
   Li XM, 2018, IEEE T MED IMAGING, V37, P2663, DOI 10.1109/TMI.2018.2845918
   Mahmud T, 2021, COMPUT BIOL MED, V128, DOI 10.1016/j.compbiomed.2020.104119
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Min Min., 2019, Scient. Rep, V9, P1
   Mori Y, 2018, ANN INTERN MED, V169, P357, DOI 10.7326/M18-0249
   Murugesan B, 2019, IEEE ENG MED BIO, P7223, DOI [10.1109/EMBC.2019.8857339, 10.1109/embc.2019.8857339]
   Park SY, 2012, IEEE T BIO-MED ENG, V59, P1408, DOI 10.1109/TBME.2012.2188397
   Pogorelov K, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P112, DOI 10.1145/3083187.3083189
   Pogorelov K, 2017, MULTIMED TOOLS APPL, V76, P22493, DOI 10.1007/s11042-017-4989-y
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruifei Zhang, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P253, DOI 10.1007/978-3-030-59725-2_25
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shen YT, 2021, LECT NOTES COMPUT SC, V12901, P559, DOI 10.1007/978-3-030-87193-2_53
   Shin Y, 2018, IEEE ACCESS, V6, P56007, DOI 10.1109/ACCESS.2018.2872717
   Silva J, 2014, INT J COMPUT ASS RAD, V9, P283, DOI 10.1007/s11548-013-0926-3
   Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997
   Tan-Cong N., 2021, INT C MED IM COMP CO, P633
   Tomar Nikhil Kumar, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12668), P307, DOI 10.1007/978-3-030-68793-9_23
   Tomar NK, 2023, IEEE T NEUR NET LEAR, V34, P9375, DOI 10.1109/TNNLS.2022.3159394
   Vázquez D, 2017, J HEALTHC ENG, V2017, DOI 10.1155/2017/4037190
   Wang P., 2022, Vis Comput, P1
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Y, 2015, COMPUT METH PROG BIO, V120, P164, DOI 10.1016/j.cmpb.2015.04.002
   Wei J, 2021, LECT NOTES COMPUT SC, V12901, P699, DOI 10.1007/978-3-030-87193-2_66
   Wickstrom K, 2020, MED IMAGE ANAL, V60, DOI 10.1016/j.media.2019.101619
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu HS, 2021, AAAI CONF ARTIF INTE, V35, P2916
   Xiao X, 2018, 2018 NINTH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY IN MEDICINE AND EDUCATION (ITME 2018), P327, DOI 10.1109/ITME.2018.00080
   Yeung M, 2021, COMPUT BIOL MED, V137, DOI 10.1016/j.compbiomed.2021.104815
   Yu Wang, 2019, Pattern Recognition and Computer Vision. Second Chinese Conference, PRCV 2019. Proceedings. Lecture Notes in Computer Science (LNCS 11858), P41, DOI 10.1007/978-3-030-31723-2_4
   Zhang W, 2021, APPL INTELL, V51, P7533, DOI 10.1007/s10489-021-02242-4
   Zhang YD, 2021, Arxiv, DOI arXiv:2102.08005
   Zhang YD, 2021, LECT NOTES COMPUT SC, V12901, P14, DOI 10.1007/978-3-030-87193-2_2
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
   Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1
   Zhuang HM, 2023, VISUAL COMPUT, V39, P2207, DOI 10.1007/s00371-021-02322-z
NR 72
TC 1
Z9 1
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2022 NOV 28
PY 2022
DI 10.1007/s00371-077-07677-x
EA NOV 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 6N2XK
UT WOS:000889421700001
DA 2024-07-18
ER

PT J
AU Sruthi, CJ
   Lijiya, A
AF Sruthi, C. J.
   Lijiya, A.
TI Double-handed dynamic gesture recognition using contour-based hand
   tracking and maximum mean probability ensembling (MMPE) for Indian Sign
   Language
SO VISUAL COMPUTER
LA English
DT Article
DE Double handed gestures; Dynamic gestures; Hand tracking; Sign language
   recognition; Indian Sign Language; Key-frame extraction
ID FRAMEWORK
AB The ability to communicate in verbal language is one of the greatest gifts of humankind. The people who do not have this ability feel isolated and struggle to convey their part in society. Sign language or gesture communication is the only method they can rely upon, but most of our community cannot understand this language without the help of a translator. The paper presents a dynamic Indian Sign Language recognition system without complicated sensors or costly devices to sense the movements of the hands. The paper proposes a problem-specific contour-based hand tracking algorithm that can track both hands simultaneously, solving the ambiguity caused by merging the hands while gesturing. The paper also proposes a maximum mean probability ensembling that combines the classification probabilities of three different classification models for better accuracy. The proposed model recognizes the double-handed dynamic gestures with an accuracy of 89.83%. The paper discusses the performance of scale-invariant feature transform, tracked image feature and their combination feature for dynamic gesture classification, and tests the discriminating power of different classifiers on these features. The support vector machine classifier showed the best performance.
C1 [Sruthi, C. J.; Lijiya, A.] Natl Inst Technol Calicut, Dept Comp Sci & Engn, Kozhikode, Kerala, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Calicut
RP Sruthi, CJ (corresponding author), Natl Inst Technol Calicut, Dept Comp Sci & Engn, Kozhikode, Kerala, India.
EM stuthicj.edu@gmail.com; lijiya@nitc.ac.in
RI C J, Sruthi/GQB-2201-2022
OI C J, Sruthi/0000-0002-2354-2174; A, Lijiya/0000-0002-8310-1362
CR Abdelnasser H, 2015, IEEE CONF COMPUT, P17, DOI 10.1109/INFCOMW.2015.7179321
   Abreu JG, 2016, SYMP VIRTUAL AUGMENT, P64, DOI 10.1109/SVR.2016.21
   Al Farid F, 2022, J IMAGING, V8, DOI 10.3390/jimaging8060153
   Al-Hammadi M, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22124558
   Al-Hammadi M, 2020, IEEE ACCESS, V8, P192527, DOI 10.1109/ACCESS.2020.3032140
   Aly S, 2020, IEEE ACCESS, V8, P83199, DOI 10.1109/ACCESS.2020.2990699
   Athira PK, 2022, J KING SAUD UNIV-COM, V34, P771, DOI 10.1016/j.jksuci.2019.05.002
   Bantupalli K, 2018, IEEE INT CONF BIG DA, P4896, DOI 10.1109/BigData.2018.8622141
   Bastos ILO, 2015, SIBGRAPI, P305, DOI 10.1109/SIBGRAPI.2015.26
   Bhaumik G, 2022, VISUAL COMPUT, V38, P3853, DOI 10.1007/s00371-021-02225-z
   Bukhari J., 2015, Int J u- e-Serv Sci Technol, V8, P131, DOI [10.14257/ijunesst.2015.8.1.12, DOI 10.14257/IJUNESST.2015.8.1.12]
   Cheok MJ, 2019, INT J MACH LEARN CYB, V10, P131, DOI 10.1007/s13042-017-0705-5
   FALVO V, 2020, PROC FRONT EDUC CONF
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Gadekallu TR, 2022, COMPUT ELECTR ENG, V100, DOI 10.1016/j.compeleceng.2022.107836
   Gurbuz SZ, 2021, IEEE SENS J, V21, P3763, DOI 10.1109/JSEN.2020.3022376
   Hartanto R, 2014, 2014 6TH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY AND ELECTRICAL ENGINEERING (ICITEE), P120
   Imran J, 2020, VISUAL COMPUT, V36, P1233, DOI 10.1007/s00371-019-01725-3
   Jain R, 2022, VISUAL COMPUT, V38, P1957, DOI 10.1007/s00371-021-02259-3
   Johnson RJ, 2016, SIGN LANG STUD, V16, P473, DOI 10.1353/sls.2016.0016
   Karami A, 2011, EXPERT SYST APPL, V38, P2661, DOI 10.1016/j.eswa.2010.08.056
   Kishore PVV, 2018, IEEE SENS J, V18, P3327, DOI 10.1109/JSEN.2018.2810449
   Kumar A, 2016, 2016 3rd International Conference on Recent Advances in Information Technology (RAIT), P422, DOI 10.1109/RAIT.2016.7507939
   Kumar DA, 2018, MULTIMED TOOLS APPL, V77, P32063, DOI 10.1007/s11042-018-6199-7
   Kumar P, 2017, NEUROCOMPUTING, V259, P21, DOI 10.1016/j.neucom.2016.08.132
   Kumar P, 2017, PATTERN RECOGN LETT, V86, P1, DOI 10.1016/j.patrec.2016.12.004
   Liao YQ, 2019, IEEE ACCESS, V7, P38044, DOI 10.1109/ACCESS.2019.2904749
   Lindeberg T., 2012, SCHOLARPEDIA, V7, P10491, DOI [10.4249/scholarpedia.10491, DOI 10.4249/SCHOLARPEDIA.10491]
   Liu T, 2016, IEEE IMAGE PROC, P2871, DOI 10.1109/ICIP.2016.7532884
   Liu Yun, 2009, Proceedings of the 2009 Second International Workshop on Computer Science and Engineering (WCSE 2009), P72, DOI 10.1109/WCSE.2009.769
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   Lucas B., 1981, P INT JOINT C ART IN, P674, DOI DOI 10.1364/J0SAA.19.002142
   Mandke K., 2019, Deaf education beyond the western world: Context, challenges, and prospects, perspectives on deafness, P261, DOI [https://doi.org/10.1093/oso/9780190880514.003.0014, DOI 10.1093/OSO/9780190880514.003.0014]
   Mittal A, 2019, IEEE SENS J, V19, P7056, DOI 10.1109/JSEN.2019.2909837
   Oliveira V., 2009, Skin Detection using HSV color space, P1
   Oudah M, 2020, J IMAGING, V6, DOI 10.3390/jimaging6080073
   Pan TY, 2016, 2016 IEEE SECOND INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM), P64, DOI 10.1109/BigMM.2016.44
   Patil Sandeep Baburao, 2017, Journal of the Institution of Engineers (India): Series B (Electrical, Electronics & Telecommunication and Computer Engineering), V98, P19, DOI 10.1007/s40031-016-0250-8
   Rastgoo R, 2020, EXPERT SYST APPL, V150, DOI 10.1016/j.eswa.2020.113336
   Rekha J., 2011, Proceedings of the 2011 International Conference on Image Processing, Computer Vision, & Pattern Recognition (IPCV 2011), P80
   Rekha J., 2011, 2011 3rd International Conference on Trendz in Information Sciences & Computing (TISC), P30, DOI 10.1109/TISC.2011.6169079
   Sahoo JP, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22030706
   Sethi A., 2012, IJCSET, V2, P1203
   Sharma K, 2015, 2ND INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND INTEGRATED NETWORKS (SPIN) 2015, P730, DOI 10.1109/SPIN.2015.7095389
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Shurong L., 2015, International Journal of Signal Processing, Image Processing and Pattern Recognition, V8, P135, DOI [10.14257/ijsip.2015.8.12.14, DOI 10.14257/IJSIP.2015.8.12.14]
   Sruthi C., 2020, C INT SYST, P369
   Sruthi C. J., 2019, 2019 International Conference on Communication and Signal Processing (ICCSP), P0596, DOI 10.1109/ICCSP.2019.8698006
   Tariq M, 2012, 2012 15TH INTERNATIONAL MULTITOPIC CONFERENCE (INMIC), P17, DOI 10.1109/INMIC.2012.6511463
   Tripathi K, 2015, 2015 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS AND INFORMATICS (ICACCI), P2211, DOI 10.1109/ICACCI.2015.7275945
   Venugopalan A, 2021, EXPERT SYST APPL, V185, DOI 10.1016/j.eswa.2021.115601
   Wang S, 2023, VISUAL COMPUT, V39, P4487, DOI 10.1007/s00371-022-02602-2
   Yasen M, 2019, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.218
NR 53
TC 2
Z9 2
U1 5
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6183
EP 6203
DI 10.1007/s00371-022-02720-x
EA NOV 2022
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000883235000002
DA 2024-07-18
ER

PT J
AU Yin, ZX
   Xia, KW
   Wang, SJ
   He, ZP
   Zhang, JN
   Zu, BK
AF Yin, Zhixian
   Xia, Kewen
   Wang, Sijie
   He, Ziping
   Zhang, Jiangnan
   Zu, Baokai
TI Unpaired low-dose CT denoising via an improved cycle-consistent
   adversarial network with attention ensemble
SO VISUAL COMPUTER
LA English
DT Article
DE Image denoising; Cycle-consistent adversarial network; Low-dose computed
   tomography; UNet; Attention gates
ID IMAGE-RECONSTRUCTION; COMPUTED-TOMOGRAPHY; NOISE-REDUCTION
AB Many deep learning-based approaches have been authenticated well performed for low-dose computed tomography (LDCT) image postprocessing. Unfortunately, most of them highly depend on well-paired datasets, which are difficult to acquire in clinical practice. Therefore, we propose an improved cycle-consistent adversarial networks (CycleGAN) to improve the quality of LDCT images. We employ a UNet-based network with attention gates ensembled as the generator, which could adaptively stress salient features which is useful for the denoising task. By doing so, the proposed network could enable the decoder to acquire available semantic features from the encoder with emphasis, thereby improving its performance. Then, perceptual loss found on the visual geometry group (VGG) is drawn into the cycle consistency loss to elevate the visual effect of denoised images to that of standard-dose computed tomography images as far as possible. Moreover, we raise an ameliorative adversarial loss based on the least square loss. In particular, the Lipschitz constraint is added to the objective function of the discriminator, while total variation is added to that of the generator, to further enhance the denoising capability of the network. The proposed method is trained and tested on a public dataset named 'Lung-PET-CT-Dx' and a real clinical dataset. Results show that the proposed method outperforms the comparative methods and even performs comparably results to that of an approach based on paired datasets in terms of quantitative scores and visual sense.
C1 [Yin, Zhixian; Xia, Kewen; Wang, Sijie; He, Ziping; Zhang, Jiangnan] Hebei Univ Technol, Sch Elect & Informat Engn, Tianjin 300401, Peoples R China.
   [Zu, Baokai] Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
C3 Hebei University of Technology; Beijing University of Technology
RP Xia, KW (corresponding author), Hebei Univ Technol, Sch Elect & Informat Engn, Tianjin 300401, Peoples R China.
EM zhixian.yin@hotmail.com; kwxia@hebut.edu.cn;
   201821902025@stu.hebut.edu.cn; 201811901003@stu.hebut.edu.cn;
   201911901007@stu.hebut.edu.cn; bzu@bjut.edu.cn
OI Wang, Sijie/0000-0002-2912-0773
FU National Natural Science Foundation of China [U1813222, 42075129]; Hebei
   Province Natural Science Foundation [E2021202179]; Key Research and
   Development Project from Hebei Province [19210404D, 20351802D,
   21351803D]; Beijing Natural Science Foundation [4214062]; Other
   Commissions Project of Beijing [Q6025001202001]
FX This study was funded by the National Natural Science Foundation of
   China (grant number NO. U1813222, NO. 42075129); Hebei Province Natural
   Science Foundation (grant number NO. E2021202179); Key Research and
   Development Project from Hebei Province (grant number NO. 19210404D, NO.
   20351802D, NO. 21351803D); Beijing Natural Science Foundation (grant
   number NO. 4214062); and theOther Commissions Project of Beijing (grant
   number NO. Q6025001202001).
CR Bayoudh K, 2022, VISUAL COMPUT, V38, P2939, DOI 10.1007/s00371-021-02166-7
   Brenner DJ, 2007, NEW ENGL J MED, V357, P2277, DOI 10.1056/NEJMra072149
   Chen H., 2017, 2017 IEEE 14 INT S B
   Chen H, 2017, IEEE T MED IMAGING, V36, P2524, DOI 10.1109/TMI.2017.2715284
   Chen M, 2021, NEUROCOMPUTING, V452, P510, DOI 10.1016/j.neucom.2020.10.004
   Chen Y, 2013, PHYS MED BIOL, V58, P5803, DOI 10.1088/0031-9155/58/16/5803
   Cui XY, 2016, IEEE T NUCL SCI, V63, P1860, DOI 10.1109/TNS.2016.2565604
   Gao YF, 2020, MED PHYS, V47, P5032, DOI 10.1002/mp.14449
   Góes DA, 2020, DIGIT SIGNAL PROCESS, V106, DOI 10.1016/j.dsp.2020.102835
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu J, 2021, MED IMAGE ANAL, V74, DOI 10.1016/j.media.2021.102209
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   Gulrajani I, 2017, ARXIV
   He YW, 2020, MED BIOL ENG COMPUT, V58, P2621, DOI 10.1007/s11517-020-02246-1
   Hong SG, 2016, COMPUT METH PROG BIO, V123, P129, DOI 10.1016/j.cmpb.2015.10.004
   Huang YC, 2021, COMPUT METH PROG BIO, V208, DOI 10.1016/j.cmpb.2021.106271
   Huang ZX, 2021, NEUROCOMPUTING, V428, P104, DOI 10.1016/j.neucom.2020.10.077
   Huang ZY, 2020, IEEE T COMPUT IMAG, V6, P1203, DOI 10.1109/TCI.2020.3012928
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jang S, 2020, IEEE T MED IMAGING, V39, P1636, DOI 10.1109/TMI.2019.2953974
   Jiang XB, 2021, VISUAL COMPUT, V37, P2419, DOI 10.1007/s00371-020-01996-1
   Jin Y, 2022, Vis. Comput., P1
   Kachelriess M, 2001, MED PHYS, V28, P475, DOI 10.1118/1.1358303
   Kang E, 2019, MED PHYS, V46, P550, DOI 10.1002/mp.13284
   Li M, 2020, IEEE T MED IMAGING, V39, P2289, DOI 10.1109/TMI.2020.2968472
   Li SZ, 2022, BIOMED SIGNAL PROCES, V75, DOI 10.1016/j.bspc.2022.103543
   Li TF, 2004, IEEE T NUCL SCI, V51, P2505, DOI 10.1109/TNS.2004.834824
   Li ZH, 2021, COMPUT MATH METHOD M, V2021, DOI 10.1155/2021/2973108
   Lyu QH, 2018, MED PHYS, V45, P2603, DOI 10.1002/mp.12916
   Ma YJ, 2021, NUCL SCI TECH, V32, DOI 10.1007/s41365-021-00874-2
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Messerli-Odermatt O, 2020, ACAD RADIOL, V27, P644, DOI 10.1016/j.acra.2019.07.017
   NAIDICH DP, 1990, RADIOLOGY, V175, P729, DOI 10.1148/radiology.175.3.2343122
   Oktay O., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1804.03999
   Pathak Y, 2018, MOD PHYS LETT B, V32, DOI 10.1142/S0217984918503001
   Qiu DF, 2021, COMPUT METH PROG BIO, V200, DOI 10.1016/j.cmpb.2021.105934
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   [史再峰 Shi Zaifeng], 2021, [天津大学学报. 自然科学与工程技术版, Journal of Tianjin University], V54, P899
   Sidky EY, 2008, PHYS MED BIOL, V53, P4777, DOI 10.1088/0031-9155/53/17/021
   SMITH PR, 1973, J PHYS A-MATH GEN, V6, P361, DOI 10.1088/0305-4470/6/3/011
   Tang C, 2019, COMPUT MATH METHOD M, V2019, DOI 10.1155/2019/8639825
   Ulyanov Dmitry, 2016, arXiv
   Wang J, 2006, IEEE T MED IMAGING, V25, P1272, DOI 10.1109/TMI.2006.882141
   Wang SJ, 2020, IET SIGNAL PROCESS, V14, P269, DOI 10.1049/iet-spr.2019.0365
   Wang W, 2020, IEEE T COMPUT IMAG, V6, P1548, DOI 10.1109/TCI.2020.3039385
   Wang YM, 2015, OPTIK, V126, P4949, DOI 10.1016/j.ijleo.2015.09.128
   Wolterink JM, 2017, IEEE T MED IMAGING, V36, P2536, DOI 10.1109/TMI.2017.2708987
   Yahya AA, 2020, MULTIMED TOOLS APPL, V79, P20391, DOI 10.1007/s11042-020-08815-8
   Yang QS, 2018, IEEE T MED IMAGING, V37, P1348, DOI 10.1109/TMI.2018.2827462
   Yang Z., 2021, 2021 INT C WIRELESS
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Yin ZX, 2021, SYMMETRY-BASEL, V13, DOI 10.3390/sym13010126
   You CY, 2020, IEEE T MED IMAGING, V39, P188, DOI 10.1109/TMI.2019.2922960
   Yu HC, 2017, IEEE IMAGE PROC, P3944, DOI 10.1109/ICIP.2017.8297022
   Yu L., 2008, P SPIE INT SOC OPT E, V6913
   Yuan HL, 2018, VISUAL COMPUT, V34, P551, DOI 10.1007/s00371-017-1360-2
   Zeng D, 2015, IEEE T NUCL SCI, V62, P2226, DOI 10.1109/TNS.2015.2467219
   Zhang Q, 2013, OPTIK, V124, P2811, DOI 10.1016/j.ijleo.2012.08.045
   Zhang T, 2020, MED PHYS, V47, P2222, DOI 10.1002/mp.14058
   Zhang YK, 2010, IEEE T NUCL SCI, V57, P2587, DOI 10.1109/TNS.2010.2060356
   Zhang YG, 2018, ACTA OPT SIN, V38, DOI 10.3788/A0S201838.0410003
   Zhao TT, 2019, MED PHYS, V46, P190, DOI 10.1002/mp.13252
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu SQ, 2020, ACTA OPT SIN, V40, DOI 10.3788/AOS202040.2210002
   Zhuang HM, 2023, VISUAL COMPUT, V39, P2207, DOI 10.1007/s00371-021-02322-z
NR 65
TC 4
Z9 4
U1 5
U2 50
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4423
EP 4444
DI 10.1007/s00371-022-02599-8
EA AUG 2022
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000836364100001
DA 2024-07-18
ER

PT J
AU Wang, S
   Zhang, SH
   Zhang, XW
   Geng, QJ
AF Wang, Shi
   Zhang, Shihui
   Zhang, Xiaowei
   Geng, Qingjia
TI A two-branch hand gesture recognition approach combining atrous
   convolution and attention mechanism
SO VISUAL COMPUTER
LA English
DT Article
DE Hand gesture recognition; Hand gesture segmentation; Multi-scale module;
   Multi-scale attention mechanism; Atrous convolution
AB Hand gesture recognition is an important research field in computer vision. To effectively solve the problem of low hand gesture recognition accuracy, we propose two modules by using atrous convolution in this paper. One is Multi-Scale Fusion (MSF) module. The other is Light-Weight Multi-Scale (LWMS) module. The MSF module can be used for extracting multi-scale features at different receptive fields. The LWMS module can be considered as a kind of enhanced and expanded convolutional operation. Based on the two modules, a Hand Gesture Recognition Approach called HGRA is designed. HGRA is a hand gesture recognition approach which is based on an end-to-end CNN-based framework with two branches. One branch uses the U-Net combined with Multi-Scale Attention module to perform hand gesture segmentation in order to separate hand gestures from complex backgrounds. Then the segmentation result is used for extracting shape features. The other branch extracts visual features, such as appearance and color. The shape and the visual features obtained by the two branches are integrated to perform hand gesture recognition. Experimental results on the OUHANDS and HGR1 gesture datasets show that the proposed method has competitive performance both in hand gesture segmentation and recognition.
C1 [Wang, Shi; Zhang, Shihui; Zhang, Xiaowei] Yanshan Univ, Sch Informat Sci & Engn, Qinhuangdao 066004, Hebei, Peoples R China.
   [Zhang, Shihui; Zhang, Xiaowei] Key Lab Comp Virtual Technol & Syst Integrat Hebe, Qinhuangdao 066004, Hebei, Peoples R China.
   [Wang, Shi; Geng, Qingjia] Hebei Normal Univ Sci & Technol, Qinhuangdao 066004, Hebei, Peoples R China.
C3 Yanshan University; Hebei Normal University of Science & Technology
RP Wang, S (corresponding author), Yanshan Univ, Sch Informat Sci & Engn, Qinhuangdao 066004, Hebei, Peoples R China.; Wang, S (corresponding author), Hebei Normal Univ Sci & Technol, Qinhuangdao 066004, Hebei, Peoples R China.
EM wangshi2665@hevttc.edu.cn; sshhzz@ysu.edu.cn
RI Zhang, Shihui/HHS-1779-2022; Zhang, Shihui/KFB-3255-2024; zhang,
   xiaowei/GQH-5387-2022
FU National Natural Science Foundation of China [61379065]; Natural Science
   Foundation of Hebei province in China [F2019203285]
FX This work was supported partly by the National Natural Science
   Foundation of China[grant numbers 61379065]; the Natural Science
   Foundation of Hebei province in China [grant numbers F2019203285].
CR Alani AA, 2018, 2018 4TH INTERNATIONAL CONFERENCE ON INFORMATION MANAGEMENT (ICIM2018), P5, DOI 10.1109/INFOMAN.2018.8392660
   [Anonymous], HGR1
   Bhaumik G, 2022, VISUAL COMPUT, V38, P3853, DOI 10.1007/s00371-021-02225-z
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Chen M., 2021, COMPUTER MODERNIZATI, P20
   Cheok MJ, 2019, INT J MACH LEARN CYB, V10, P131, DOI 10.1007/s13042-017-0705-5
   Dadashzadeh A, 2019, IET COMPUT VIS, V13, P700, DOI 10.1049/iet-cvi.2018.5796
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Islam MZ, 2019, 2019 JOINT 8TH INTERNATIONAL CONFERENCE ON INFORMATICS, ELECTRONICS & VISION (ICIEV) AND 2019 3RD INTERNATIONAL CONFERENCE ON IMAGING, VISION & PATTERN RECOGNITION (ICIVPR) WITH INTERNATIONAL CONFERENCE ON ACTIVITY AND BEHAVIOR COMPUTING (ABC), P324, DOI [10.1109/iciev.2019.8858563, 10.1109/ICIEV.2019.8858563]
   Kawulok M, 2013, IEEE IMAGE PROC, P3720, DOI 10.1109/ICIP.2013.6738767
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   MATILAINEN M, 2016, INT CONF IMAG PROC, DOI DOI 10.1109/IPTA.2016.7821025
   Mittal A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.75
   Molchanov Pavlo, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P1, DOI 10.1109/CVPRW.2015.7301342
   Ong EJ, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P889
   Radosavovic Ilija, 2020, P IEEE CVF C COMP VI, P10428
   Rastgoo R, 2021, EXPERT SYST APPL, V164, DOI 10.1016/j.eswa.2020.113794
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Tan M., 2021, ARXIV PREPRINT
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Xu Kun, 2021, 2021 International Conference on Communications, Information System and Computer Engineering (CISCE), P447, DOI 10.1109/CISCE52179.2021.9445897
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao S, 2018, CHIN CONT DECIS CONF, P5966, DOI 10.1109/CCDC.2018.8408176
   Zhu XL, 2016, IEEE WINT CONF APPL
   Zunair H, 2021, COMPUT BIOL MED, V136, DOI 10.1016/j.compbiomed.2021.104699
NR 32
TC 8
Z9 8
U1 7
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4487
EP 4500
DI 10.1007/s00371-022-02602-2
EA JUL 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000825226300002
DA 2024-07-18
ER

PT J
AU Liu, SJ
   Wang, HB
   Hu, L
   Li, QS
   Liu, XR
AF Liu, Shengjun
   Wang, Haibo
   Hu, Ling
   Li, Qinsong
   Liu, Xinru
TI Incremental functional maps for accurate and smooth shape correspondence
SO VISUAL COMPUTER
LA English
DT Article
DE Shape correspondence; Functional maps; Multiscale spectral manifold
   wavelets
AB Incorporating multiscale spectral manifold wavelets preservation into the functional map framework for shape correspondence achieves great results in terms of both efficiency and effectiveness. However, fixing the dimension of the spectral embedding strategy in iterations of optimization is troublesome, such as missing high-frequency information when the dimension is small or getting trapped in local minima at a high dimension. In this paper, we present a simple and efficient method for refining correspondences from low frequency to high frequency with a theoretical guarantee. We formulate a strong constraint where the multiscale spectral manifold wavelets should be preserved at each scale correspondingly in the case of the arbitrary dimension of spectral embeddings. To solve the formula, we deduce two relaxed optimization subproblems and propose an incremental alternating iterative algorithm between the spatial and spectral domains via spectral up-sampling and filtering, computing the functional maps and pointwise maps in turn. Our results demonstrate that our method is robust to noisy initialization and scalable with regard to shape resolutions. The deformable shape correspondence benchmark experiments show our method produces more accurate and smoother results than state of the arts.
C1 [Liu, Shengjun; Wang, Haibo; Li, Qinsong; Liu, Xinru] Cent South Univ, Inst Engn Modeling & Sci Comp, Changsha, Peoples R China.
   [Hu, Ling] Hunan First Normal Univ, Sch Math & Stat, Changsha, Peoples R China.
   [Liu, Shengjun] Cent South Univ, State Key Lab High Performance Mfg Complex, Changsha, Peoples R China.
C3 Central South University; Hunan First Normal University; Central South
   University
RP Li, QS (corresponding author), Cent South Univ, Inst Engn Modeling & Sci Comp, Changsha, Peoples R China.
EM shjliu.cg@csu.edu.cn; wang_haibo2017@163.com; huling.cg@foxmail.com;
   qinsli.cg@foxmail.com; liuxinru@csu.edu.cn
RI Liu, Xinru/KEH-2341-2024; Hu, Ling/AAA-5764-2020
OI Wang, Haibo/0000-0002-6612-7528; Li, Qinsong/0000-0002-6795-8956
FU Natural Science Foundation of China [62172447, 61876191]; Hunan
   Provincial Natural Science Foundation of China [2021JJ30172]; Open
   Project Program of the National Laboratory of Pattern Recognition (NLPR)
   [202200025]
FX This work was supported by the Natural Science Foundation of China (No.
   62172447, 61876191), Hunan Provincial Natural Science Foundation of
   China (No. 2021JJ30172), and the Open Project Program of the National
   Laboratory of Pattern Recognition (NLPR) (No. 202200025).
CR Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Bertsekas D., 1998, OPTIM NEURAL COMPUT, V8
   Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491
   Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1
   Donati Nicolas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8589, DOI 10.1109/CVPR42600.2020.00862
   Dym N, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130826
   Eisenberger M., 2020, ADV NEURAL INFORM PR, P491
   Ezuz D, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3202660
   Ezuz D, 2017, COMPUT GRAPH FORUM, V36, P165, DOI 10.1111/cgf.13254
   Gao M., 2021, C COMPUTER VISION PA, P193
   Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005
   Hou TB, 2012, GRAPH MODELS, V74, P221, DOI 10.1016/j.gmod.2012.04.010
   Hu L., 2021, C COMPUTER VISION PA, P545
   Huang QX, 2008, COMPUT GRAPH FORUM, V27, P1449, DOI 10.1111/j.1467-8659.2008.01285.x
   Huang RQ, 2020, COMPUT GRAPH FORUM, V39, P265, DOI 10.1111/cgf.14084
   Jain Varun., 2007, INT J SHAPE MODELING, V13, P101, DOI DOI 10.1142/S0218654307000968
   Kim VG, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964974
   Lahner Z., 2016, EUR WORKSH 3D OBJ RE
   Leonardi N, 2013, IEEE T SIGNAL PROCES, V61, P3357, DOI 10.1109/TSP.2013.2259825
   Li QS, 2021, COMPUT GRAPH FORUM, V40, P81, DOI 10.1111/cgf.14120
   Litman R, 2014, IEEE T PATTERN ANAL, V36, P171, DOI 10.1109/TPAMI.2013.148
   Maron H, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925913
   Melzi S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356524
   Meyer N, 2003, VISUALIZATION AND MATHEMATICS III, P35
   Nogneng D, 2018, COMPUT GRAPH FORUM, V37, P179, DOI 10.1111/cgf.13352
   Nogneng D, 2017, COMPUT GRAPH FORUM, V36, P259, DOI 10.1111/cgf.13124
   Ovsjanikov M., 2017, ACM SIGGRAPH COURSES
   Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526
   Pai G, 2021, PROC CVPR IEEE, P384, DOI 10.1109/CVPR46437.2021.00045
   Ren J, 2021, COMPUT GRAPH FORUM, V40, P81, DOI 10.1111/cgf.14359
   Ren J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275040
   Rodolà E, 2017, COMPUT GRAPH FORUM, V36, P700, DOI 10.1111/cgf.13160
   Rodolà E, 2017, COMPUT GRAPH FORUM, V36, P222, DOI 10.1111/cgf.12797
   Rui Xiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9510, DOI 10.1109/CVPR42600.2020.00953
   Sahillioglu Y, 2020, VISUAL COMPUT, V36, P1705, DOI 10.1007/s00371-019-01760-0
   Salti S, 2014, COMPUT VIS IMAGE UND, V125, P251, DOI 10.1016/j.cviu.2014.04.011
   Schonsheck SC, 2021, J SCI COMPUT, V86, DOI 10.1007/s10915-020-01390-y
   Shtern Alon, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P499, DOI 10.1109/3DV.2014.24
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Vestner M, 2017, INT CONF 3D VISION, P517, DOI 10.1109/3DV.2017.00065
   Vestner M, 2017, PROC CVPR IEEE, P6681, DOI 10.1109/CVPR.2017.707
   Wang YQ, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392443
   Xiang R, 2021, PROC CVPR IEEE, P15925, DOI 10.1109/CVPR46437.2021.01567
NR 44
TC 0
Z9 0
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3313
EP 3325
DI 10.1007/s00371-022-02553-8
EA JUL 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000820940700001
DA 2024-07-18
ER

PT J
AU Fu, YP
   Chen, QQ
   Zhao, HF
AF Fu, Yanping
   Chen, Qiaoqiao
   Zhao, Haifeng
TI CGFNet: cross-guided fusion network for RGB-thermal semantic
   segmentation CGI PaperID: 105
SO VISUAL COMPUTER
LA English
DT Article
DE Semantic segmentation; RGB-thermal segmentation; Multimodal fusion
ID MOTION REMOVAL; D SLAM
AB Semantic segmentation is a basic task in computer vision, which is widely used in various fields such as autonomous driving, detection, augmented reality and so on. Recent advances in deep learning have achieved commendable results in the semantic segmentation for visible (RGB) images. However, the performance of these methods will decline precipitously in dark or visually degraded environments. To overcome this problem, thermal images are introduced into semantic segmentation tasks because they can be captured in all-weather. To make full use of the two modal information, we propose a novel cross-guided fusion attention network for RGB-T semantic segmentation, which uses an attention mechanism to extract the weights of two modalities and guide each other. Then we extract the global information and add it to the decoding process. In addition, we propose a dual decoder to decode the features of different modalities. The two decoders will predict three prediction maps. We use these three prediction maps to train our network jointly. We conducted experiments on two public datasets, and the experimental results demonstrate the effectiveness of the proposed method.
C1 [Fu, Yanping; Chen, Qiaoqiao; Zhao, Haifeng] Anhui Univ, Sch Comp Sci & Technol, Anhui Prov Key Lab Multimodal Cognit Computat, Hefei, Peoples R China.
C3 Anhui University
RP Zhao, HF (corresponding author), Anhui Univ, Sch Comp Sci & Technol, Anhui Prov Key Lab Multimodal Cognit Computat, Hefei, Peoples R China.
EM ypfu@ahu.edu.cn; senith@163.com
RI zhao, haifeng/JNX-7170-2023
OI Fu, Yanping/0000-0002-4191-4779
FU National Natural Science Foundation of China [61876002, 62076005]; Anhui
   Natural Science Foundation Anhui Energy Internet Joint Fund
   [2008085UD07]; Anhui Provincial University Collaborative Innovation
   Project [GXXT-2021-030]; Anhui Provincial Key Research and Development
   Project [202104a07020029]
FX This work was supported in part by the National Natural Science
   Foundation of China (No.61876002, No.62076005), Anhui Natural Science
   Foundation Anhui Energy Internet Joint Fund (No.2008085UD07), Anhui
   Provincial University Collaborative Innovation Project (No.
   GXXT-2021-030) and Anhui Provincial Key Research and Development Project
   (No.202104a07020029).
CR Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bojarski M., 2016, ARXIV PREPRINT ARXIV
   Chen CY, 2015, IEEE I CONF COMP VIS, P2722, DOI 10.1109/ICCV.2015.312
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen S., ARXIV PREPRINT ARXIV
   Cheng JY, 2017, 2017 18TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS (ICAR), P589, DOI 10.1109/ICAR.2017.8023671
   Deng FQ, 2021, IEEE INT C INT ROBOT, P4467, DOI 10.1109/IROS51168.2021.9636084
   Fridman L., 2017, ARXIV PREPRINT ARXIV
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Ha Q, 2017, IEEE INT C INT ROBOT, P5108, DOI 10.1109/IROS.2017.8206396
   Hazirbas C, 2017, LECT NOTES COMPUT SC, V10111, P213, DOI 10.1007/978-3-319-54181-5_14
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hu XX, 2019, IEEE IMAGE PROC, P1440, DOI [10.1109/ICIP.2019.8803025, 10.1109/icip.2019.8803025]
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Jianbo Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P1, DOI 10.1007/978-3-030-58574-7_1
   Li XH, 2015, IEEE INT VEH SYM, P1160, DOI 10.1109/IVS.2015.7225840
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lyu Y, 2020, ELECTRON LETT, V56, P920, DOI 10.1049/el.2020.1635
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Park SJ, 2017, IEEE I CONF COMP VIS, P4990, DOI 10.1109/ICCV.2017.533
   Pohlen T, 2017, PROC CVPR IEEE, P3309, DOI 10.1109/CVPR.2017.353
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shivakumar SS, 2020, IEEE INT CONF ROBOT, P9441, DOI [10.1109/icra40945.2020.9196831, 10.1109/ICRA40945.2020.9196831]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun Y, 2020, IEEE T INF FOREN SEC, V15, P3295, DOI 10.1109/TIFS.2020.2986879
   Sun YX, 2019, IEEE ROBOT AUTOM LET, V4, P2576, DOI 10.1109/LRA.2019.2904733
   Sun YX, 2018, ROBOT AUTON SYST, V108, P115, DOI 10.1016/j.robot.2018.07.002
   Sun YX, 2017, ROBOT AUTON SYST, V89, P110, DOI 10.1016/j.robot.2016.11.012
   Tu ZZ, 2021, IEEE T IMAGE PROCESS, V30, P5678, DOI 10.1109/TIP.2021.3087412
   Wang PQ, 2018, IEEE WINT CONF APPL, P1451, DOI 10.1109/WACV.2018.00163
   Xiaokang Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P561, DOI 10.1007/978-3-030-58621-8_33
   Yu CQ, 2018, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR.2018.00199
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang Q, 2021, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR46437.2021.00266
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou W., 2021, IEEE ACCESS, V99, P1, DOI [10.1109/ACCESS.2021.3051398, DOI 10.1109/ACCESS.2021.3051398]
   Zhou W., 2021, ARXIV PREPRINT ARXIV
   Zhou WJ, 2021, IEEE T IMAGE PROCESS, V30, P7790, DOI 10.1109/TIP.2021.3109518
NR 38
TC 12
Z9 12
U1 5
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3243
EP 3252
DI 10.1007/s00371-022-02559-2
EA JUL 2022
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000819705400001
DA 2024-07-18
ER

PT J
AU Kim, MG
   Ryu, J
   Son, J
   Han, J
AF Kim, Myoung Gon
   Ryu, JiSeok
   Son, Jaemin
   Han, JungHyun
TI Virtual object sizes for efficient and convenient mid-air manipulation
SO VISUAL COMPUTER
LA English
DT Article
DE Mid-air manipulation; Virtual reality; Human-computer interaction
AB It has been taken for granted that the sizes of virtual objects affect the efficiency and convenience of mid-air manipulation in immersive virtual environments. If a virtual object is too small or too large, for example, manipulating it becomes a difficult task. Nevertheless, the virtual object sizes that are optimal and convenient have rarely been studied. In this paper, we select a virtual object with many distinct geometric features and conduct user studies via docking tasks. Through the user studies, the optimal and convenient sizes for mid-air manipulation are estimated. In order to verify the results, a proxy-based manipulation method is designed and implemented, where the proxy is created with the estimated optimal size. The test based on the method shows that the optimal-size proxy enables users to manipulate efficiently virtual objects and the estimated range of convenient sizes is also preferred by the users.
C1 [Kim, Myoung Gon; Ryu, JiSeok; Son, Jaemin; Han, JungHyun] Korea Univ, Dept Comp Sci & Engn, Seoul, South Korea.
C3 Korea University
RP Han, J (corresponding author), Korea Univ, Dept Comp Sci & Engn, Seoul, South Korea.
EM m_gon_kim@korea.ac.kr; roadmageb@korea.ac.kr; jasson31@korea.ac.kr;
   jhan@korea.ac.kr
FU Ministry of Science and ICT, Korea [IITP-2022-2020-0-01819];
   ITRC(Information Technology Research Center) [IITP-2022-2020-0-01460]; 
   [2020-0-00861]
FX This research was supported by the Ministry of Science and ICT, Korea,
   under the ICT Creative Consilience Program (IITP-2022-2020-0-01819),
   ITRC(Information Technology Research Center) Support Program
   (IITP-2022-2020-0-01460) and the grant No. 2020-0-00861, which are all
   supervised by the IITP (Institute for Information & Communications
   Technology Planning & Evaluation).
CR Argelaguet F, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P269, DOI 10.1145/2993369.2993391
   Beever L, 2020, IEEE CONF COMPU INTE, P136, DOI 10.1109/CoG47356.2020.9231769
   Bowman D. A., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P35, DOI 10.1145/253284.253301
   Elvezio C., 2017, ACM SIGGRAPH 2017 VR Village, SIGGRAPH '17, P1
   Feng J., 2015, Proceedings of the 3rd ACM Symposium on Spatial User Interaction - SUI'15, P2, DOI [DOI 10.1145/2788940.2788942, 10.1145/2788940, DOI 10.1145/2788940]
   Frees S, 2005, P IEEE VIRT REAL ANN, P99
   Frees S, 2007, ACM T COMPUT-HUM INT, V14, DOI 10.1145/1229855.1229857
   Gloumeau PC, 2021, IEEE T VIS COMPUT GR, V27, P2488, DOI 10.1109/TVCG.2020.2987834
   Hartman Neal, 2020, MIG '20: Motion, Interaction and Games, DOI 10.1145/3424636.3426908
   Homan, 1997, SMARTSCENE IMMERSIVE
   Jerald J, 2013, 2013 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P197
   Kim T, 2014, IEEE COMPUT GRAPH, V34, P31
   Krekhov A, 2018, PROCEEDINGS OF THE 2018 ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY (CHI PLAY 2018), P243, DOI 10.1145/3242671.3242704
   LaViola Joseph J., 2017, 3D User interfaces: theory and practice
   Martin-Gomez A, 2020, INT SYM MIX AUGMENT, P207, DOI [10.1109/ISMAR50242.2020.00044, 10.1109/1SMA1R50242.2020.00044]
   Masliah M. R., 2000, CHI 2000 Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2000. The Future is Here, P25, DOI 10.1145/332040.332403
   Medeiros D., 2013, SBC J INTERACT SYST, V4, P30, DOI [10.5753/jis.2013.633, DOI 10.5753/JIS.2013.633]
   Mendes D, 2019, COMPUT GRAPH FORUM, V38, P21, DOI 10.1111/cgf.13390
   Mendes D, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P261, DOI 10.1145/2993369.2993396
   Mendes D, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P3, DOI 10.1109/3DUI.2014.6798833
   Mine M. R., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P19, DOI 10.1145/258734.258747
   Mine M, 2015, COMPUT GRAPH-UK, V48, P84, DOI 10.1016/j.cag.2015.02.004
   Oda O, 2015, UIST'15: PROCEEDINGS OF THE 28TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P405, DOI 10.1145/2807442.2807497
   Oda O, 2012, INT SYM MIX AUGMENT, P207, DOI 10.1109/ISMAR.2012.6402558
   Pierce J. S., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P39, DOI 10.1145/253284.253303
   Pierce J. S., 1999, Proceedings 1999 Symposium on Interactive 3D Graphics, P141, DOI 10.1145/300523.300540
   Pohl H., 2021, P 2021 CHI C HUMAN F, P1
   Pouke M., 2021, FRONT VIRTUAL REALIT, V2, P48
   Poupyrev I., 1998, P ACM CHI, V98
   Poupyrev I., 1996, P 9 ANN ACM S USER I, P79, DOI [DOI 10.1145/237091.237102, 10.1145/237091.237102]
   Rodrigues V., 2015, AT ENC PORT COMP GRA, P100
   Song Peng., 2012, P 2012 ACM ANN C HUM, P1297, DOI DOI 10.1145/2207676.2208585
   Stoakley R., 1995, P SIGCHI C HUM FACT, P265, DOI [10.1145/223904.223938, DOI 10.1145/223904.223938]
   Wang J, 2015, COMPUT GRAPH-UK, V48, P71, DOI 10.1016/j.cag.2015.02.007
   Wang M, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P235, DOI 10.1109/VR50410.2021.00045
   Wang R., 2011, Proceedings of the 24th annual ACM symposium on User interface software and technology. UIST '11, P549
   Whitlock M, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P41, DOI 10.1109/VR.2018.8446381
   Wilkes C., 2008, Proceedings of the 2008 ACM Symposium on Virtual Reality Software and Technology, P23, DOI DOI 10.1145/1450579.1450585
   Xia HJ, 2018, UIST 2018: PROCEEDINGS OF THE 31ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P853, DOI 10.1145/3242587.3242597
   Yang YL, 2021, IEEE T VIS COMPUT GR, V27, P1214, DOI 10.1109/TVCG.2020.3030427
   Yu K, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P392, DOI 10.1109/VR50410.2021.00062
   Zeelenberg R, 2015, BEHAV RES METHODS, V47, P127, DOI 10.3758/s13428-014-0476-9
   Zingg T., 1935, BEITRAG SCHOTTERANAL
NR 43
TC 2
Z9 2
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3463
EP 3474
DI 10.1007/s00371-022-02555-6
EA JUL 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000819705400003
PM 35791413
OA Bronze, Green Published
DA 2024-07-18
ER

PT J
AU Jin, Y
   Li, ZX
   Zhu, DM
   Shi, M
   Wang, ZQ
AF Jin, Yue
   Li, Zhaoxin
   Zhu, Dengming
   Shi, Min
   Wang, Zhaoqi
TI Automatic and real-time green screen keying
SO VISUAL COMPUTER
LA English
DT Article
DE Green screen keying; Green spill removal; Green screen dataset;
   Real-time
AB Green screen keying has always been an essential and fundamental part of film and television special effects. In the actual shooting process, captured green screen images vary significantly due to the comprehensive influence of lighting, shooting angle, green cloth material, characters, etc. In order to obtain visually pleasing effects, traditional methods usually require professionals to adjust the corresponding parameters with lots of workload for different images, which is inefficient. Meanwhile, there are tedious steps in dealing with the green spill problem. In this paper, we propose a deep learning-based green screen keying method which is automatic, effective, and real-time. Firstly, we create a green screen dataset that contains not only alpha and foreground maps but also samples with green spill phenomenon and a large number of distinct green screen backgrounds. Secondly, we propose an end-to-end network that can automatically tackle green screen keying and green spill removal problems. Our method only takes a single image as input without user interaction and estimates alpha and foreground simultaneously. Extensive experiments clearly demonstrate the superiority of our proposed method. Moreover, our method achieves approximately 75fps on 720P videos (1280 x 720) and 25fps on 1080P videos (1920 x 1080), which can be considered real-time for many applications.
C1 [Jin, Yue; Li, Zhaoxin; Zhu, Dengming; Wang, Zhaoqi] Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China.
   [Shi, Min] North China Elect Power Univ, Sch Control & Comp Engn, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   North China Electric Power University
RP Li, ZX (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China.
EM jinyue2020@ict.ac.cn; cszli@hotmail.com; mdzhu@ict.ac.cn;
   shi_min@ncepu.edu.cn; zqwang@ict.ac.cn
OI Li, Zhaoxin/0000-0001-5450-8131
FU National Key Research and Development Program of China [2020YFB1710400];
   National Natural Science Foundation of China [62172392, 61702482];
   Scientic Research Instrument and Equipment Development Project of
   Chinese Academy of Sciences [YJKYYQ20190055]
FX This work was supported by National Key Research and Development Program
   of China (2020YFB1710400), National Natural Science Foundation of China
   under Grants (Nos. 62172392 and 61702482) and Scientic Research
   Instrument and Equipment Development Project of Chinese Academy of
   Sciences (YJKYYQ20190055).
CR Aksoy Y, 2017, PROC CVPR IEEE, P228, DOI 10.1109/CVPR.2017.32
   Aksoy Y, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2907940
   [Anonymous], 2019, P IEEECVF INT C COMP
   [Anonymous], 2008, ART SCI DIGITAL COMP
   Aximmetry, ALL IN ON VIRT STUD
   Chen QF, 2013, IEEE T PATTERN ANAL, V35, P2175, DOI 10.1109/TPAMI.2013.18
   Chen Q, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P618, DOI 10.1145/3240508.3240610
   Chuang YY, 2001, PROC CVPR IEEE, P264
   Forte Marco, 2020, arXiv
   Gastal ESL, 2010, COMPUT GRAPH FORUM, V29, P575, DOI 10.1111/j.1467-8659.2009.01627.x
   Goel A., 2021, IAMALPHA INSTANT ADA
   Grady L, 2005, PROCEEDINGS OF THE FIFTH IASTED INTERNATIONAL CONFERENCE ON VISUALIZATION, IMAGING, AND IMAGE PROCESSING, P423
   Hou QQ, 2019, IEEE I CONF COMP VIS, P4129, DOI 10.1109/ICCV.2019.00423
   Jiang W., 2021, ARXIV211200510
   Johnson J, 2016, IEEE T IMAGE PROCESS, V25, P3032, DOI 10.1109/TIP.2016.2555705
   Kaiming He, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2049, DOI 10.1109/CVPR.2011.5995495
   Ke Z., 2020, ARXIV 201111961
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li HX, 2021, SYMMETRY-BASEL, V13, DOI 10.3390/sym13081454
   Li JZZ, 2022, INT J COMPUT VISION, V130, P246, DOI 10.1007/s11263-021-01541-0
   Li Jizhizi, 2021, ARXIV210707235
   Lin S., 2021, IEEE CVF C COMP VIS, P8762
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Liu QL, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P526, DOI 10.1145/3474085.3475203
   Mishima Y., 1994, US Patent, Patent No. [5,355,174, 5355174]
   Qiao Y, 2020, P IEEE CVF C COMP VI, P13676
   Rhemann C, 2009, PROC CVPR IEEE, P1826, DOI 10.1109/CVPRW.2009.5206503
   Sengupta S, 2020, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR42600.2020.00236
   Smith A. R., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P259, DOI 10.1145/237170.237263
   Sun J, 2004, ACM T GRAPHIC, V23, P315, DOI 10.1145/1015706.1015721
   Tang JW, 2019, PROC CVPR IEEE, P3050, DOI 10.1109/CVPR.2019.00317
   Vlahos P., 1999, U.S. Patent, Patent No. 5907315
   Xu N, 2017, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.2017.41
   Yu QH, 2021, PROC CVPR IEEE, P1154, DOI 10.1109/CVPR46437.2021.00121
   Zhang YK, 2019, PROC CVPR IEEE, P7461, DOI 10.1109/CVPR.2019.00765
   US
NR 36
TC 0
Z9 1
U1 2
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3135
EP 3147
DI 10.1007/s00371-022-02542-x
EA JUN 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000813592000002
OA Bronze
DA 2024-07-18
ER

PT J
AU Parvaz, R
AF Parvaz, Reza
TI Point spread function estimation for blind image deblurring problems
   based on framelet transform
SO VISUAL COMPUTER
LA English
DT Article
DE Blind deblurring; Framelet; PSF estimation; Natural image; Fractional
   calculation
ID DECONVOLUTION; MINIMIZATION; RESTORATION
AB One of the most important issues in image processing is the approximation of the image that has been lost due to the blurring process. These types of matters are divided into non-blind and blind problems. The second type of problem is more complex in terms of calculations than the first problem due to the unknown of original image and point spread function estimation. In the present paper, an algorithm based on coarse-to-fine iterative by l(0)-alpha l(1) regularization and framelet transform is introduced to approximate the spread function estimation. Framelet transfer improves the restored kernel due to the decomposition of the kernel to different frequencies. Also, in the proposed model, a fraction gradient operator is used instead of the ordinary gradient operator. The proposed method is investigated on different kinds of images such as text, face and natural. The output of the proposed method reflects the effectiveness of the proposed algorithm in restoring images from blind problems.
C1 [Parvaz, Reza] Univ Mohaghegh Ardabili, Dept Math, Ardebil 5619911367, Iran.
C3 University of Mohaghegh Ardabili
RP Parvaz, R (corresponding author), Univ Mohaghegh Ardabili, Dept Math, Ardebil 5619911367, Iran.
EM reza.parvaz@yahoo.com
CR Beck A, 2017, MOS-SIAM SER OPTIMIZ, P1, DOI 10.1137/1.9781611974997
   Cafagna D, 2007, IEE IND ELECTRON M, V1, P35, DOI 10.1109/MIE.2007.901479
   Cai JF, 2009, MULTISCALE MODEL SIM, V8, P337, DOI 10.1137/090753504
   Cai JF, 2009, SIAM J IMAGING SCI, V2, P226, DOI 10.1137/080733371
   Campisi P., 2017, BLIND IMAGE DECONVOL
   Chai AW, 2007, NUMER MATH, V106, P529, DOI 10.1007/s00211-007-0075-0
   Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491
   Christiansen M, 2008, SIAM J SCI COMPUT, V30, P855, DOI 10.1137/060671413
   Daubechies I, 2003, APPL COMPUT HARMON A, V14, P1, DOI 10.1016/S1063-5203(02)00511-0
   Dong JH, 2024, IEEE T PATTERN ANAL, V46, P1664, DOI 10.1109/TPAMI.2021.3128560
   Estatico C, 2017, NUMER MATH, V137, P895, DOI 10.1007/s00211-017-0893-7
   Fairag F, 2020, NUMER ALGORITHMS, V85, P931, DOI 10.1007/s11075-019-00845-0
   Feng Q, 2019, VISUAL COMPUT, V35, P1081, DOI 10.1007/s00371-019-01697-4
   Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956
   Goldstein T, 2009, SIAM J IMAGING SCI, V2, P323, DOI 10.1137/080725891
   Guo L, 2021, APPL MATH COMPUT, V404, DOI 10.1016/j.amc.2021.126224
   Hansen P.C., 2006, DEBLURRING IMAGES MA
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hirsch M, 2011, IEEE I CONF COMP VIS, P463, DOI 10.1109/ICCV.2011.6126276
   Hu DD, 2023, VISUAL COMPUT, V39, P281, DOI 10.1007/s00371-021-02329-6
   Hu Z, 2014, PROC CVPR IEEE, P3382, DOI 10.1109/CVPR.2014.432
   Khan A, 2021, VISUAL COMPUT, V37, P1661, DOI 10.1007/s00371-020-01930-5
   Köhler R, 2012, LECT NOTES COMPUT SC, V7578, P27, DOI 10.1007/978-3-642-33786-4_3
   Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521
   Lai WS, 2016, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2016.188
   Levin A, 2009, PROC CVPR IEEE, P1964, DOI 10.1109/CVPRW.2009.5206815
   Li J, 2016, J VIS COMMUN IMAGE R, V40, P14, DOI 10.1016/j.jvcir.2016.06.003
   Liu G, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0122562
   Liu JJ, 2020, J COMPUT APPL MATH, V378, DOI 10.1016/j.cam.2020.112934
   Liu JJ, 2020, INVERSE PROBL, V36, DOI 10.1088/1361-6420/ab6df0
   Liu RT, 2008, IEEE IMAGE PROC, P505, DOI 10.1109/ICIP.2008.4711802
   Lou YF, 2018, J SCI COMPUT, V74, P767, DOI 10.1007/s10915-017-0463-2
   Lou YF, 2015, ADV INTELL SYST, V359, P169, DOI 10.1007/978-3-319-18161-5_15
   Mainardi F, 2018, MATHEMATICS-BASEL, V6, DOI 10.3390/math6090145
   Pan JS, 2017, IEEE T PATTERN ANAL, V39, P342, DOI 10.1109/TPAMI.2016.2551244
   Pan JS, 2016, PROC CVPR IEEE, P1628, DOI 10.1109/CVPR.2016.180
   Pan JS, 2014, PROC CVPR IEEE, P2901, DOI 10.1109/CVPR.2014.371
   Parvaz R., ARXIV PREPRINT ARXIV
   Rajagopalan A., 2014, Motion Deblurring: Algorithms and Systems, DOI DOI 10.1017/CBO9781107360181
   Repetti A, 2015, IEEE SIGNAL PROC LET, V22, P539, DOI 10.1109/LSP.2014.2362861
   Ron A, 1997, J FUNCT ANAL, V148, P408, DOI 10.1006/jfan.1996.3079
   Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672
   Shi MZ, 2016, SIGNAL PROCESS, V126, P65, DOI 10.1016/j.sigpro.2015.11.022
   Sridevi G, 2019, CIRC SYST SIGNAL PR, V38, P3802, DOI 10.1007/s00034-019-01029-w
   Volcker A., 2021, THESIS HANDELSHOYSKO
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Wen F, 2021, IEEE T CIRC SYST VID, V31, P2923, DOI 10.1109/TCSVT.2020.3034137
   Whyte O, 2014, INT J COMPUT VISION, V110, P185, DOI 10.1007/s11263-014-0727-3
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157
   Yang X. J., 2019, General Fractional Derivatives: Theory, Methods and Applications
   Yuan Q, 2020, VISUAL COMPUT, V36, P1591, DOI 10.1007/s00371-019-01762-y
   Zhang FJ, 2018, MULTIMED TOOLS APPL, V77, P26239, DOI 10.1007/s11042-018-5847-2
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhao CP, 2020, IEEE ACCESS, V8, P22072, DOI 10.1109/ACCESS.2020.2969675
   Zhou C, 2021, IEEE INT CONF COMP V, P1155, DOI 10.1109/ICCVW54120.2021.00135
NR 58
TC 3
Z9 3
U1 1
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2653
EP 2669
DI 10.1007/s00371-022-02484-4
EA APR 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000784861800001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Miao, Q
   Xu, C
   Li, F
   Zuo, WM
   Meng, ZP
AF Miao, Qing
   Xu, Chao
   Li, Feng
   Zuo, Wangmeng
   Meng, Zhaopeng
TI Delayed rectification of discriminative correlation filters for visual
   tracking
SO VISUAL COMPUTER
LA English
DT Article
DE Visual tracking; Appearance model; Discriminative correlation filters
AB Discriminative correlation filters (DCF) have demonstrated competitive tracking performance in recent years. In these approaches, DCF methods only learn the appearance models with the historical tracking results, thus have the risks of drifting the targets due to the unforeseen target appearances in the future. In this paper, we present a novel tracking framework which rectifies the DCF models in the current frame with the potential future target appearances. To achieve this, the tracking model is updated with time-delay strategies and the model learning in each frame consists of two strategies: an exploration module and an exploitation module. The exploration module aims at discovering the potential target appearances in the near future, while the exploitation module further combines the future target appearances with the historical tracking results to learn more robust DCF models. To validate the proposed method, we integrate it into two state-of-the-art DCF trackers, i.e., spatially regularized discriminative correlation filters decontamination and efficient convolution operators, and also conduct extensive experiments on three tracking benchmarks: OTB-2015, Temple-Color and LaSOT. The results show that by incorporating with the proposed framework, the modified DCF methods can leverage the future target appearances for learning more robust models and are also superior to the baseline methods. In addition, they can also achieve competitive performance against the state-of-the-art methods on several datasets.
C1 [Miao, Qing; Xu, Chao; Meng, Zhaopeng] Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China.
   [Li, Feng; Zuo, Wangmeng] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin, Peoples R China.
C3 Tianjin University; Harbin Institute of Technology
RP Xu, C (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China.
EM csqmiao@tju.edu.cn; xuchao@tju.edu.cn; fengli_hit@hotmail.com;
   wmzuo@hit.edu.cn; mengzp@tju.edu.cn
RI Zuo, Wangmeng/B-3701-2008
FU National key R&D program of China [2018YFB1701701]; National Natural
   Science Foundation of China [U19A2073]
FX This work was supported by the National key R&D program of China under
   Grant No. 2018YFB1701701, and the National Natural Science Foundation of
   China under Grant No. U19A2073.
CR [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.465
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2018, LECT NOTES COMPUT SC, V11206, P493, DOI 10.1007/978-3-030-01216-8_30
   Bibi A, 2016, LECT NOTES COMPUT SC, V9910, P419, DOI 10.1007/978-3-319-46466-4_25
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Chen Z., 2015, ARXIV PREPRINT ARXIV
   Choi J, 2018, PROC CVPR IEEE, P479, DOI 10.1109/CVPR.2018.00057
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2016, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2016.159
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Fan H, 2017, IEEE I CONF COMP VIS, P5487, DOI 10.1109/ICCV.2017.585
   Fazl-Ersi E, 2019, VISUAL COMPUT, V35, P1447, DOI 10.1007/s00371-018-1510-1
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   Galoogahi HK, 2013, IEEE I CONF COMP VIS, P3072, DOI 10.1109/ICCV.2013.381
   Gladh S, 2016, INT C PATT RECOG, P1243, DOI 10.1109/ICPR.2016.7899807
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hongyi Su, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9225, P1, DOI 10.1007/978-3-319-22180-9_1
   Huang Z., 2017, P IEEE C COMP VIS PA, P4021, DOI DOI 10.1109/CVPR.2017.510
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li F, 2017, IEEE INT CONF COMP V, P2001, DOI 10.1109/ICCVW.2017.234
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905
   Liu T, 2015, PROC CVPR IEEE, P4902, DOI 10.1109/CVPR.2015.7299124
   Lukezic A, 2019, LECT NOTES COMPUT SC, V11362, P595, DOI 10.1007/978-3-030-20890-5_38
   Lukezic A, 2018, IEEE T CYBERNETICS, V48, P1849, DOI 10.1109/TCYB.2017.2716101
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177
   Mbelwa, 2019, VISUAL COMPUT, P1
   Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466
   Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Yang, 2021, VISUAL COMPUT, P1
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang TZ, 2017, PROC CVPR IEEE, P4819, DOI [10.1109/CVPR.2017.512, 10.1109/ICCV.2017.469]
   Zhang YH, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18113937
   Zhao DW, 2019, INFORM SCIENCES, V470, P78, DOI 10.1016/j.ins.2018.08.053
   Zuo WM, 2019, IEEE T PATTERN ANAL, V41, P1158, DOI 10.1109/TPAMI.2018.2829180
NR 47
TC 2
Z9 2
U1 4
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1237
EP 1250
DI 10.1007/s00371-022-02401-9
EA MAR 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000768629000001
DA 2024-07-18
ER

PT J
AU Weiherer, M
   Eigenberger, A
   Egger, B
   Brébant, V
   Prantl, L
   Palm, C
AF Weiherer, Maximilian
   Eigenberger, Andreas
   Egger, Bernhard
   Brebant, Vanessa
   Prantl, Lukas
   Palm, Christoph
TI Learning the shape of female breasts: an open-access 3D statistical
   shape model of the female breast built from 110 breast scans
SO VISUAL COMPUTER
LA English
DT Article
DE Statistical shape model; Non-rigid surface registration; Breast imaging;
   Surgical outcome simulation; Breast reconstruction surgery
ID SIMULATION; RECONSTRUCTION; DEFORMATIONS; SPACE
AB We present the Regensburg Breast Shape Model (RBSM)-a 3D statistical shape model of the female breast built from 110 breast scans acquired in a standing position, and the first publicly available. Together with the model, a fully automated, pairwise surface registration pipeline used to establish dense correspondence among 3D breast scans is introduced. Our method is computationally efficient and requires only four landmarks to guide the registration process. A major challenge when modeling female breasts from surface-only 3D breast scans is the non-separability of breast and thorax. In order to weaken the strong coupling between breast and surrounding areas, we propose to minimize the variance outside the breast region as much as possible. To achieve this goal, a novel concept called breast probability masks (BPMs) is introduced. A BPM assigns probabilities to each point of a 3D breast scan, telling how likely it is that a particular point belongs to the breast area. During registration, we use BPMs to align the template to the target as accurately as possible inside the breast region and only roughly outside. This simple yet effective strategy significantly reduces the unwanted variance outside the breast region, leading to better statistical shape models in which breast shapes are quite well decoupled from the thorax. The RBSM is thus able to produce a variety of different breast shapes as independently as possible from the shape of the thorax. Our systematic experimental evaluation reveals a generalization ability of 0.17mm and a specificity of 2.8mm. To underline the expressiveness of the proposed model, we finally demonstrate in two showcase applications how the RBSM can be used for surgical outcome simulation and the prediction of a missing breast from the remaining one. Our model is available at https://www.rbsm.re-mic.de/.
C1 [Weiherer, Maximilian; Palm, Christoph] Ostbayer TH Regensburg OTH Regensburg, Regensburg Med Image Comp ReMIC, Regensburg, Germany.
   [Weiherer, Maximilian; Egger, Bernhard] Friedrich Alexander Univ Erlangen Nurnberg FAU, Chair Visual Comp, Erlangen, Germany.
   [Eigenberger, Andreas] OTH Regensburg, Fac Mech Engn, Regensburg, Germany.
   [Eigenberger, Andreas; Brebant, Vanessa; Prantl, Lukas] Univ Hosp Regensburg, Univ Ctr Plast Aesthet Hand & Reconstruct Surg, Regensburg, Germany.
   [Palm, Christoph] OTH Regensburg, Regensburg Ctr Biomed Engn RCBE, Regensburg, Germany.
   [Palm, Christoph] Regensburg Univ, Regensburg, Germany.
C3 University of Erlangen Nuremberg; University of Regensburg; University
   of Regensburg
RP Palm, C (corresponding author), Ostbayer TH Regensburg OTH Regensburg, Regensburg Med Image Comp ReMIC, Regensburg, Germany.; Palm, C (corresponding author), OTH Regensburg, Regensburg Ctr Biomed Engn RCBE, Regensburg, Germany.; Palm, C (corresponding author), Regensburg Univ, Regensburg, Germany.
EM christoph.palm@oth-regensburg.de
RI Egger, Bernhard/AAE-5389-2019; Palm, Christoph/F-4943-2014; Prantl,
   Lukas/AAM-1848-2021
OI Egger, Bernhard/0000-0002-4736-2397; Palm,
   Christoph/0000-0001-9468-2871; Prantl, Lukas/0000-0003-2454-2499
FU Projekt DEAL
FX Open Access funding enabled and organized by Projekt DEAL.
CR Albrecht T, 2013, MED IMAGE ANAL, V17, P959, DOI 10.1016/j.media.2013.05.010
   Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311
   Ambellan F, 2019, ADV EXP MED BIOL, V1156, P67, DOI 10.1007/978-3-030-19385-0_5
   Amberg B, 2007, IEEE I CONF COMP VIS, P1326
   Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Balaniuk R, 2003, LECT NOTES COMPUT SC, V2673, P48
   Balaniuk Remis, 2006, ANN 8 S VIRTUAL REAL, P387
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Booth J, 2016, PROC CVPR IEEE, P5543, DOI 10.1109/CVPR.2016.598
   Botsch M., 2010, Polygon Mesh Processing, DOI DOI 10.1201/B10688
   Chao I, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778775
   Ciechomski PD, 2012, J MED INTERNET RES, V14, DOI 10.2196/jmir.1903
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Costa IF, 2001, IEEE INT CONF ROBOT, P2337, DOI 10.1109/ROBOT.2001.932971
   Cotin S, 2000, VISUAL COMPUT, V16, P437, DOI 10.1007/PL00007215
   Cruz N., 2015, PLAST RECONSTR SURG, V136, P161, DOI DOI 10.1097/01.PRS.0000472486.29688.27
   Dai H, 2020, INT J COMPUT VISION, V128, P547, DOI 10.1007/s11263-019-01260-7
   Du S., 2007, IMAGE PROCESSING, V5,, P193, DOI DOI 10.1109/ICIP.2007.4379798
   Egger B, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3395208
   Gallego C, 2011, PROC SPIE, V7962, DOI 10.1117/12.877712
   Gallo G, 2010, BIOSIGNALS 2010: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON BIO-INSPIRED SYSTEMS AND SIGNAL PROCESSING, P163
   Georgii J, 2014, IEEE J BIOMED HEALTH, V18, P907, DOI 10.1109/JBHI.2013.2285308
   Gerig T, 2018, IEEE INT CONF AUTOMA, P75, DOI 10.1109/FG.2018.00021
   Göpper MW, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0233586
   GOWER JC, 1975, PSYCHOMETRIKA, V40, P33, DOI 10.1007/BF02291478
   Hartmann R, 2020, AESTHET PLAST SURG, V44, P1980, DOI 10.1007/s00266-020-01749-4
   HORN BKP, 1988, J OPT SOC AM A, V5, P1127, DOI 10.1364/JOSAA.5.001127
   Hwang K, 2015, ARCH PLAST SURG-APS, V42, P226, DOI 10.5999/aps.2015.42.2.226
   Jiang T, 2017, VISUAL COMPUT, V33, P891, DOI 10.1007/s00371-017-1390-9
   Kim Y, 2008, COMPUT ANIMAT VIRT W, V19, P515, DOI 10.1002/cav.237
   Lacher RM, 2019, MED IMAGE ANAL, V53, P11, DOI 10.1016/j.media.2019.01.003
   Levi Z, 2015, IEEE T VIS COMPUT GR, V21, P264, DOI 10.1109/TVCG.2014.2359463
   Lotter L, 2021, ARCH GYNECOL OBSTET, V303, P515, DOI 10.1007/s00404-020-05837-3
   Luthi M., 2012, INSIGHT J
   Lüthi M, 2018, IEEE T PATTERN ANAL, V40, P1860, DOI 10.1109/TPAMI.2017.2739743
   Mazier A, 2021, J BIOMECH, V128, DOI 10.1016/j.jbiomech.2021.110645
   Ordas S, 2007, PROC SPIE, V6511, DOI 10.1117/12.708879
   Panozzo D., 2010, 2 INT WORKSHOP COMPU, P9
   Paulsen R., 2002, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2002. 5th International Conference. Proceedings, Part II (Lecture Notes in Computer Science Vol.2489), P373
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Roose L, 2006, LECT NOTES COMPUT SC, V4072, P197
   Ruiz G, 2018, MED IMAGE ANAL, V47, P164, DOI 10.1016/j.media.2018.04.007
   Seo H, 2007, COMPUT ANIMAT VIRT W, V18, P141, DOI 10.1002/cav.169
   Sorkine O, 2007, S GEOM PROC, V4, P109, DOI [10.1145/1073204.1073323, DOI 10.1145/1073204.1073323]
   Sorkine-Hornung O., 2017, COMPUTING, V1, P1
   Styner MA, 2003, LECT NOTES COMPUT SC, V2732, P63
   Su Z., 2011, THESIS U COLL LONDON
   Sumner RW, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239531
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Sung H, 2021, CA-CANCER J CLIN, V71, P209, DOI 10.3322/caac.21660
   Vorstenbosch J, 2017, AESTHET PLAST SURG, V41, P481, DOI 10.1007/s00266-017-0830-2
   Wang Z, 2016, FRONT INFORM TECH EL, V17, P501, DOI 10.1631/FITEE.1500184
   Williams C, 2003, LECT NOTES COMPUT SC, V2878, P9
   Wilms M, 2017, MED IMAGE ANAL, V38, P17, DOI 10.1016/j.media.2017.02.003
   Yamazaki S, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P191, DOI 10.1109/3DV.2013.33
   Yang JL, 2016, IEEE T PATTERN ANAL, V38, P2241, DOI 10.1109/TPAMI.2015.2513405
   Yoshiyasu Y, 2014, COMPUT GRAPH FORUM, V33, P257, DOI 10.1111/cgf.12451
   Zinsser T., 2005, INT C PATT REC IM PR, P116
NR 60
TC 2
Z9 2
U1 3
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1597
EP 1616
DI 10.1007/s00371-022-02431-3
EA MAR 2022
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000765009500001
OA hybrid, Green Submitted
DA 2024-07-18
ER

PT J
AU Fei, KX
   Wang, C
   Zhang, JX
   Liu, YZ
   Xie, X
   Tu, ZG
AF Fei, Kexin
   Wang, Chao
   Zhang, Jiaxu
   Liu, Yuanzhong
   Xie, Xing
   Tu, Zhigang
TI Flow-pose Net: an effective two-stream network for fall detection
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT ACIAT202
CY 2022
CL ELECTR NETWORK
DE Fall detection; Flow-pose Net; Human pose; Optical flow; Simultaneous
   motion and appearance feature extraction
ID SURVEILLANCE; REPRESENTATIONS
AB Aging society gives rise to the need of fall detection for the elderly. The interference of the environmental noise and the loss of motion information causing fall detection still challenging. In this work, we present a novel two-stream network, called Flow-pose Net (FP-Net), which integrates the optical flow and human pose information to achieve robust and accurate fall detection in videos. Specifically, we use a human pose estimation model to detect the joints of the human body and design a GCN-based network to learn the body appearance feature from human pose. For motion feature extraction, we estimate optical flow from raw videos and utilize a CNN-based network to learn rich motion feature. Finally, the appearance feature and the motion feature are concatenated and then fed into a classifier to perform the classification of fall. To the best of our knowledge, we are the first to combine the optical flow and the human pose to simultaneously extract motion and appearance features for fall detection. Extensive experiments are conducted on two popular datasets URFD and Le2i, and the results show that our FP-Net achieves the state-of-the-art performance and has high robustness.
C1 [Fei, Kexin; Wang, Chao; Zhang, Jiaxu; Liu, Yuanzhong; Tu, Zhigang] Wuhan Univ, State Key Lab Informat Engn Surveying Mapping & R, Wuhan, Peoples R China.
   [Xie, Xing] Anyang Normal Univ, Dept Comp & Informat Engn, Anyang, Peoples R China.
C3 Wuhan University; Anyang Normal University
RP Wang, C (corresponding author), Wuhan Univ, State Key Lab Informat Engn Surveying Mapping & R, Wuhan, Peoples R China.
EM c.wang@whu.edu.cn; tuzhigang@whu.edu.cn
RI Tu, Zhigang/AAG-2255-2020; hu, xin/KHT-2406-2024
OI Tu, Zhigang/0000-0001-5003-2260; Liu, Yuanzhong/0000-0002-8852-3222
FU National Natural Science Foundation of China [62106177]; Central
   University Basic Research Fund of China [2042020KF0016]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 62106177. It was also supported by the Central
   University Basic Research Fund of China (No.2042020KF0016). The
   numerical calculation was supported by the supercomputing system in the
   Super-computing Center of Wuhan University.
CR Adhikari K, 2020, PROC SPIE, V11433, DOI 10.1117/12.2556540
   Adhikari K, 2017, PROCEEDINGS OF THE FIFTEENTH IAPR INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS - MVA2017, P81, DOI 10.23919/MVA.2017.7986795
   Anishchenko Lesya, 2018, 2018 Ural Symposium on Biomedical Engineering, Radioelectronics and Information Technology (USBEREIT). Proceedings, P99, DOI 10.1109/USBEREIT.2018.8384560
   Aziz O, 2011, IEEE T NEUR SYS REH, V19, P670, DOI 10.1109/TNSRE.2011.2162250
   Bayoudh K, 2022, VISUAL COMPUT, V38, P2939, DOI 10.1007/s00371-021-02166-7
   Beddiar DR, 2020, MULTIMED TOOLS APPL, V79, P30509, DOI 10.1007/s11042-020-09004-3
   Berlin SJ, 2021, J AMB INTEL HUM COMP, DOI 10.1007/s12652-021-03250-5
   Bhandari S, 2017, IEEE GLOB CONF CONSU
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Charfi I, 2014, ROBUST SPATIOORAL DE
   Charfi I, 2013, J ELECTRON IMAGING, V22, DOI 10.1117/1.JEI.22.4.041106
   Chen YJ, 2021, IEEE T IMAGE PROCESS, V30, P4008, DOI 10.1109/TIP.2021.3068645
   Ciabattoni L, 2018, 2018 ZOOMING INNOVATION IN CONSUMER TECHNOLOGIES CONFERENCE (ZINC), P130, DOI 10.1109/ZINC.2018.8448970
   Cotechini V, 2019, DATA BRIEF, V23, DOI 10.1016/j.dib.2019.103839
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Du XT, 2019, VISUAL COMPUT, V35, P1703, DOI 10.1007/s00371-018-1591-x
   Ezatzadeh S, 2017, 2017 9TH INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE TECHNOLOGY (IKT 2017), P93, DOI 10.1109/IKT.2017.8258624
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Feng WG, 2014, SIGNAL IMAGE VIDEO P, V8, P1129, DOI 10.1007/s11760-014-0645-4
   Foroughi Homa, 2008, 2008 11th International Conference on Computer and Information Technology (ICCIT), P219, DOI 10.1109/ICCITECHN.2008.4803020
   Gammulle H, 2017, IEEE WINT CONF APPL, P177, DOI 10.1109/WACV.2017.27
   Gao WB, 2021, APPL SOFT COMPUT, V111, DOI 10.1016/j.asoc.2021.107728
   Geertsema EE, 2019, J BIOMECH, V88, P25, DOI 10.1016/j.jbiomech.2019.03.007
   Huang WB, 2021, IEEE J BIOMED HEALTH, V25, P3834, DOI 10.1109/JBHI.2021.3092396
   Huang WB, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3091990
   Huang ZY, 2018, 2018 4TH INTERNATIONAL CONFERENCE ON UNIVERSAL VILLAGE (IEEE UV 2018)
   Kasturi S, 2017, C HUM SYST INTERACT, P144, DOI 10.1109/HSI.2017.8005016
   Khan SS, 2017, MED ENG PHYS, V39, P12, DOI 10.1016/j.medengphy.2016.10.014
   Kong YQ, 2019, J VIS COMMUN IMAGE R, V59, P215, DOI 10.1016/j.jvcir.2019.01.024
   Kwolek B, 2014, COMPUT METH PROG BIO, V117, P489, DOI 10.1016/j.cmpb.2014.09.005
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Liu JX, 2021, VISUAL COMPUT, V37, P359, DOI 10.1007/s00371-020-01804-w
   Lu J., 2020, J ZHENGZHOU U MED ED, V55, P662
   Lu N, 2019, IEEE J BIOMED HEALTH, V23, P314, DOI 10.1109/JBHI.2018.2808281
   [马露 Ma Lu], 2019, [计算机科学, Computer Science], V46, P106
   Mastorakis G, 2014, J REAL-TIME IMAGE PR, V9, P635, DOI 10.1007/s11554-012-0246-9
   Miaou SG, 2006, 1ST TRANSDISCIPLINARY CONFERENCE ON DISTRIBUTED DIAGNOSIS AND HOME HEALTHCARE, CONFERENCE PROCEEDINGS, P39, DOI 10.1109/DDHH.2006.1624792
   Mousse MA, 2017, VISUAL COMPUT, V33, P1529, DOI 10.1007/s00371-016-1296-y
   Núñez-Marcos A, 2017, WIREL COMMUN MOB COM, DOI 10.1155/2017/9474806
   Peng YF, 2019, PROCEDIA COMPUT SCI, V147, P271, DOI 10.1016/j.procs.2019.01.253
   Rougier C, 2011, IEEE T CIRC SYST VID, V21, P611, DOI 10.1109/TCSVT.2011.2129370
   Sabatini AM, 2016, IEEE T NEUR SYS REH, V24, P774, DOI 10.1109/TNSRE.2015.2460373
   Shen B. G., 2014, J COMPUT APPL, V34, P264
   Shen B. G., 2014, J COMPUT APPL, V34, P223
   [申代友 Shen Daiyou], 2019, [中国医学物理学杂志, Chinese Journal of Medical Physics], V36, P223
   Simonyan K., 2014, CORR
   Singla N., 2014, MOTION DETECTION BAS, DOI DOI 10.1109/CVPR.2016.213
   Sucerquia A, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17010198
   Tang Y, 2021, IEEE SENS J, V21, P581, DOI 10.1109/JSEN.2020.3015521
   Teed Z., 2020, IEEE INT C COMP VIS
   Teng Q, 2021, IEEE SENS J, V21, P18063, DOI 10.1109/JSEN.2021.3085360
   Tu ZG, 2019, IEEE T CIRC SYST VID, V29, P1423, DOI 10.1109/TCSVT.2018.2830102
   Tu ZG, 2019, SIGNAL PROCESS-IMAGE, V72, P9, DOI 10.1016/j.image.2018.12.002
   Tu ZG, 2019, IEEE T IMAGE PROCESS, V28, P2799, DOI 10.1109/TIP.2018.2890749
   Tu ZG, 2018, PATTERN RECOGN, V79, P32, DOI 10.1016/j.patcog.2018.01.020
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Yajai A, 2017, J VIS COMMUN IMAGE R, V49, P257, DOI 10.1016/j.jvcir.2017.08.008
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yuan Z., 2017, J HENAN NORM UNIV, V45, P96, DOI [10.16366/j.cnki.1000-2367.2017.03.014, DOI 10.16366/j.cnki.1000-2367.2017.03.014]
   Ze L., 2021, VIDEO SWIM TRANSFORM
   Zerrouki N, 2018, MULTIMED TOOLS APPL, V77, P6405, DOI 10.1007/s11042-017-4549-5
NR 62
TC 8
Z9 8
U1 6
U2 41
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2023
VL 39
IS 6
SI SI
BP 2305
EP 2320
DI 10.1007/s00371-022-02416-2
EA FEB 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA J1YY4
UT WOS:000757764500001
DA 2024-07-18
ER

PT J
AU Gao, X
   Zhang, Y
   Wang, H
   Sun, YJ
   Zhao, F
   Zhang, XF
AF Gao, Xin
   Zhang, Yan
   Wang, Hua
   Sun, Yujuan
   Zhao, Feng
   Zhang, Xiaofeng
TI A modified fuzzy clustering algorithm based on dynamic relatedness model
   for image segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Fuzzy clustering; Image segmentation; Dynamic relatedness model;
   non-local information
ID C-MEANS; LOCAL INFORMATION; NONLOCAL INFORMATION
AB Accurate segmentation is the basis of object detection, computer vision and other fields. However, the complexity of images, together with the existence of noise and other image artifacts, makes image segmentation still a bottleneck. In this paper, a dynamic relatedness model is presented and an improved fuzzy clustering algorithm is proposed. Compared with traditional algorithms, the relatedness model is measured in the process of image segmentation, and can avoid the effect of inaccurate features in noisy images. With the help of the proposed relatedness, more accurate information can be adopted to enhance the results. Simulated experiments on various images demonstrate that the proposed algorithm can achieve satisfying results and is insensitive to noise of different types.
C1 [Gao, Xin; Zhang, Yan; Wang, Hua; Sun, Yujuan; Zhang, Xiaofeng] Ludong Univ, Sch Informat & Elect Engn, Yantai 264025, Peoples R China.
   [Wang, Hua; Zhao, Feng; Zhang, Xiaofeng] Shandong Technol & Business Univ, Shandong Future Intelligent Financial Engn Lab, Yantai 264005, Peoples R China.
C3 Ludong University; Shandong Technology & Business University
RP Zhang, XF (corresponding author), Ludong Univ, Sch Informat & Elect Engn, Yantai 264025, Peoples R China.; Zhang, XF (corresponding author), Shandong Technol & Business Univ, Shandong Future Intelligent Financial Engn Lab, Yantai 264005, Peoples R China.
EM gao_xin_private@163.com; 15684177618@163.com; hwa229@163.com;
   syj_anne@163.com; zhaofeng1016@126.com; iamzxf@126.com
RI Gao, Xin/D-5487-2013
OI Zhang, Xiaofeng/0000-0001-8978-1058
FU NSF of China [61873117, 61903172, 62007017, 62106091, 62176140,
   62171209]
FX This research was funded by NSF of China under Granted Number 61873117,
   61903172, 62007017, 62106091, 62176140 and 62171209. The authors also
   gratefully acknowledge the reviewers' helpful comments and suggestions,
   which will improve the presentation significantly.
CR Aboutabit N, 2021, VISUAL COMPUT, V37, P1545, DOI 10.1007/s00371-020-01896-4
   Ahmed MN, 2002, IEEE T MED IMAGING, V21, P193, DOI 10.1109/42.996338
   Benaichouche AN, 2013, DIGIT SIGNAL PROCESS, V23, P1390, DOI 10.1016/j.dsp.2013.07.005
   BESSER H, 1990, LIBR TRENDS, V38, P787
   Bhandari AK, 2014, EXPERT SYST APPL, V41, P3538, DOI 10.1016/j.eswa.2013.10.059
   Cai WL, 2007, PATTERN RECOGN, V40, P825, DOI 10.1016/j.patcog.2006.07.011
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen SC, 2004, IEEE T SYST MAN CY B, V34, P1907, DOI 10.1109/TSMCB.2004.831165
   Cocosco C.A., 1997, NeuroImage, V5
   Codella NCF, 2018, I S BIOMED IMAGING, P168, DOI 10.1109/ISBI.2018.8363547
   Dunn J. C., 1973, Journal of Cybernetics, V3, P32, DOI 10.1080/01969727308546046
   Gharieb RR, 2014, CAIRO INT BIOM ENG, P47, DOI 10.1109/CIBEC.2014.7020912
   Gong MG, 2013, IEEE T IMAGE PROCESS, V22, P573, DOI 10.1109/TIP.2012.2219547
   Gong MG, 2012, IEEE T IMAGE PROCESS, V21, P2141, DOI 10.1109/TIP.2011.2170702
   Hu WB, 2021, INFORM SCIENCES, V568, P199, DOI 10.1016/j.ins.2021.03.066
   Jaffar MA, 2009, 2009 THIRD INTERNATIONAL CONFERENCE ON ELECTRICAL ENGINEERING, P136
   Krinidis S, 2010, IEEE T IMAGE PROCESS, V19, P1328, DOI 10.1109/TIP.2010.2040763
   Kumawat A, 2022, VISUAL COMPUT, V38, P3681, DOI 10.1007/s00371-021-02196-1
   Liu XX, 2020, COMPUT VIS MEDIA, V6, P467, DOI 10.1007/s41095-020-0181-9
   Ma DY, 2021, IEEE T IMAGE PROCESS, V30, P1825, DOI 10.1109/TIP.2020.3045640
   Pham DL, 2001, COMP MED SY, P127, DOI 10.1109/CBMS.2001.941709
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roy S, 2008, I S BIOMED IMAGING, P452, DOI 10.1109/ISBI.2008.4541030
   Szilágyi L, 2003, P ANN INT IEEE EMBS, V25, P724, DOI 10.1109/IEMBS.2003.1279866
   Tripathy BK., 2014, IEEE INT C COMPUT IN, V2014, P1, DOI [10.1109/ICCIC.2014.7238446, DOI 10.1109/ICCIC.2014.7238446]
   Tschandl P, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.161
   Wu CM, 2020, APPL SOFT COMPUT, V94, DOI 10.1016/j.asoc.2020.106468
   Xiao CX, 2013, VISUAL COMPUT, V29, P27, DOI 10.1007/s00371-012-0672-5
   Yingdi Guo, 2015, WSEAS Transactions on Computers, V14, P369
   Yu X, 2021, INT J INTELL SYST, V36, P757, DOI 10.1002/int.22319
   Zhang F, 2020, COMPUT VIS MEDIA, V6, P417, DOI 10.1007/s41095-020-0186-4
   Zhang XF, 2021, COMPUT VIS MEDIA, V7, P513, DOI 10.1007/s41095-021-0239-3
   Zhang XF, 2021, INFORM SCIENCES, V550, P129, DOI 10.1016/j.ins.2020.10.039
   Zhang XF, 2017, MULTIMED TOOLS APPL, V76, P7869, DOI 10.1007/s11042-016-3399-x
   Zhang YX, 2021, VISUAL COMPUT, V37, P1061, DOI 10.1007/s00371-020-01852-2
   Zhao F, 2011, SIGNAL PROCESS, V91, P988, DOI 10.1016/j.sigpro.2010.10.001
   Zheng YH, 2015, J INTELL FUZZY SYST, V28, P961, DOI 10.3233/IFS-141378
NR 37
TC 7
Z9 7
U1 1
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1583
EP 1596
DI 10.1007/s00371-022-02430-4
EA FEB 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000754455000002
DA 2024-07-18
ER

PT J
AU Huang, Y
   Yang, CC
   Shi, Y
   Chen, H
   Yan, WZ
   Chen, ZY
AF Huang, Yan
   Yang, Chuanchuan
   Shi, Yu
   Chen, Hao
   Yan, Weizhen
   Chen, Zhangyuan
TI PLGP: point cloud inpainting by patch-based local geometric propagating
SO VISUAL COMPUTER
LA English
DT Article
DE Point cloud; Hole inpainting; Local geometry; Patch searching; Patch
   propagating
ID FILLING HOLES; COMPLETION
AB The booming of LiDAR technologies has made the point cloud become a prevailing data format for 3D object representation. However, point cloud usually exhibits holes of data loss mainly due to occurrence of noise, occlusion or the surface material of the object, which is a serious problem affects the target expression of point cloud. Point cloud inpainting is the key solution for holes problem. In this paper, we propose a patch-based local geometric propagating (PLGP) method to automatically fill the lost data obtained by three-dimensional scanning. Different from typical methods transforming the point cloud into range image to conduct the hole-detection or filling the missing region with a whole best match, this work tends to detect the hole directly in 3D space and inpaint it by iteratively searching for the context with local similarity and making it propagate appropriately along the occlusion's local geometric structure. The experimental results with comparisons demonstrate its competitive effectiveness with a F-score as high as 0.89 and a 23.45 dB average gain in GPSNR with consumed time reduced by up to 60%.
C1 [Huang, Yan; Yang, Chuanchuan; Chen, Zhangyuan] Peking Univ, Dept Elect, State Key Lab Adv Opt Commun Syst & Networks, Beijing 100871, Peoples R China.
   [Shi, Yu] Peking Univ, Dept Elect, Beijing 100871, Peoples R China.
   [Chen, Hao; Yan, Weizhen] Sense Future Technol Co Ltd, Beijing 100025, Peoples R China.
C3 Peking University; Peking University
RP Yang, CC (corresponding author), Peking Univ, Dept Elect, State Key Lab Adv Opt Commun Syst & Networks, Beijing 100871, Peoples R China.
EM huang_yan@pku.edu.cn; yangchuanchuan@pku.edu.cn; shiyu@pku.edu.cn;
   chenhao@sensefuture.cn; yanweizhen@sensefuture.cn; chenzhy@pku.edu.cn
RI SHI, YH/HLG-1159-2023; Chen, Zhangyuan/ADX-1000-2022; SHI,
   YAN/HNI-1042-2023
FU National Key R&D Program of China [2019YFB1802904]
FX This study was funded by National Key R&D Program of China (Grant
   2019YFB1802904). National Key R&D Program of China, 2019YFB1802904,
   Chuanchuan Yang.
CR [Anonymous], 2005, S GEOM PROC EUR ASS
   Aranjuelo N, 2020, INT WORKSH SOFT COMP, P813
   Cai ZP, 2015, IEEE GEOSCI REMOTE S, V12, P2272, DOI 10.1109/LGRS.2015.2466811
   Curless B., The Stanford 3D scanning repository
   Dinesh C, 2018, IEEE SIGNAL PROC LET, V25, P878, DOI 10.1109/LSP.2018.2831621
   Doria D., 2012, 2012 IEEE COMP SOC C, P65
   Fedorov V, 2015, IMAGE PROCESS ON LIN, V5, P362, DOI 10.5201/ipol.2015.136
   Fu ZQ, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102861
   Fu ZQ, 2021, IEEE T MULTIMEDIA, V23, P3022, DOI 10.1109/TMM.2021.3068606
   Fu ZQ, 2018, IEEE IMAGE PROC, P2137, DOI 10.1109/ICIP.2018.8451550
   Goyal P, 2020, INT J DATA SCI ANAL, V10, P25, DOI 10.1007/s41060-020-00208-2
   Guo XY, 2018, VISUAL COMPUT, V34, P93, DOI 10.1007/s00371-016-1316-y
   Hu W, 2019, IEEE T IMAGE PROCESS, V28, P4087, DOI 10.1109/TIP.2019.2906554
   Lai PJ, 2016, IEEE GLOB CONF SIG, P1218, DOI 10.1109/GlobalSIP.2016.7906035
   Li XH, 2014, IEEE T GEOSCI REMOTE, V52, P7086, DOI 10.1109/TGRS.2014.2307354
   Quinsat Y, 2015, INT J ADV MANUF TECH, V81, P411, DOI 10.1007/s00170-015-7185-0
   Sahay Pratyush, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P1, DOI 10.1109/CVPRW.2015.7301388
   Sun H, 2020, IEEE T CIRCUITS-II, V67, P1644, DOI 10.1109/TCSII.2020.3013758
   Tachella J, 2019, 2019 IEEE 8TH INTERNATIONAL WORKSHOP ON COMPUTATIONAL ADVANCES IN MULTI-SENSOR ADAPTIVE PROCESSING (CAMSAP 2019), P206, DOI [10.1109/CAMSAP45676.2019.9022496, 10.1109/camsap45676.2019.9022496]
   Tian D, 2017, IEEE IMAGE PROC, P3460, DOI 10.1109/ICIP.2017.8296925
   Wang HN, 2007, IMAGE VISION COMPUT, V25, P103, DOI 10.1016/j.imavis.2005.12.006
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
   Xiao CX, 2007, VISUAL COMPUT, V23, P433, DOI 10.1007/s00371-007-0115-x
NR 23
TC 3
Z9 3
U1 1
U2 27
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2023
VL 39
IS 2
BP 723
EP 732
DI 10.1007/s00371-021-02370-5
EA JAN 2022
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8T2TV
UT WOS:000746282000001
DA 2024-07-18
ER

PT J
AU Wang, XH
   Xu, M
   Hu, M
   Ren, FJ
AF Wang, Xiaohua
   Xu, Ming
   Hu, Min
   Ren, Fuji
TI A multi-scale framework based on jigsaw patches and focused label
   smoothing for bone age assessment
SO VISUAL COMPUTER
LA English
DT Article
DE Hand and wrist radiographs; Puzzles; Bone age assessment
ID CHILDREN; CARPAL
AB The clinical assessment of bone age is critical. Because it allows the study of the endocrine, genetic, and growth disorders in children's growth. However, clinical assessment of bone age is time-consuming and labor-intensive and is susceptible to observer error. Fully automated bone age assessment (BAA) systems can be a good solution to this problem, assisting or even replacing specialists in bone age assessment. Most of the existing fully automated BAA systems use more discriminative areas (carpal bones, finger bones, etc.) from the whole image as local information input. However, extracting multiple local regions is still complex and time-consuming. Selective extraction of regions is not objective enough and can lose some useful global information. In this paper, we propose a fully automated end-to-end BAA system that requires no additional annotation and no extraction of regions of interest. Specifically, we use a puzzle generator to generate skeletal puzzles containing information at different scales. By inputting smaller-scale skeletal puzzles, the network is forced to mining local fine-grained information first, then input larger-scale skeletal puzzles to obtain coarse-grained information, and finally learn the intact picture to obtain global information. BAA task is suboptimal as a general classification or regression task using single-valued labels due to the high similarity between hand images of similar ages. We propose a new label processing method called focused label smoothing for the BAA task and combine it with expectation regression to obtain a more robust age estimate. We perform adequate experiments on the public dataset from the 2017 Pediatric Bone Age Challenge organized by the Radiological Society of North America and we achieve great experimental performance compared to the method without manual annotation and competitive results with the method using additional manual annotation.
C1 [Wang, Xiaohua; Xu, Ming; Hu, Min] Hefei Univ Technol, Key Lab Knowledge Engn Big Data, Minist Educ, Hefei 230601, Peoples R China.
   [Wang, Xiaohua; Xu, Ming; Hu, Min; Ren, Fuji] Hefei Univ Technol, Sch Comp & Informat, Anhui Prov Key Lab Affect Comp & Adv Intelligent, Hefei 230601, Peoples R China.
   [Ren, Fuji] Univ Tokushima, Grad Sch Adv Technol & Sci, Tokushima 7708502, Japan.
C3 Hefei University of Technology; Hefei University of Technology;
   Tokushima University
RP Xu, M; Hu, M (corresponding author), Hefei Univ Technol, Key Lab Knowledge Engn Big Data, Minist Educ, Hefei 230601, Peoples R China.; Xu, M; Hu, M (corresponding author), Hefei Univ Technol, Sch Comp & Informat, Anhui Prov Key Lab Affect Comp & Adv Intelligent, Hefei 230601, Peoples R China.
EM 2019170958@mail.hfut.edu.cn; jsjxhumin@hfut.edu.cn
RI wang, xiao/HZI-9156-2023; wang, xiao/HGB-7081-2022; wang,
   xu/IAN-4886-2023
OI wang, xiao/0000-0002-4088-3341; Xu, Ming/0000-0001-6839-1886
FU National Natural Science Foundation of China [61672202]; State Key
   Program of NSFC-Shenzhen Joint Foundation [U1613217]; Fundamental
   Research Funds for the Central Universities of China [PA2019GDPK0076,
   PA2020GDSK0060]
FX This research has been partially supported by the National Natural
   Science Foundation of China (Grant No. 61672202), the State Key Program
   of NSFC-Shenzhen Joint Foundation (Grant No. U1613217), and the
   Fundamental Research Funds for the Central Universities of China (Grant
   Nos. PA2019GDPK0076 and PA2020GDSK0060).
CR ALBANESE A, 1995, CLIN ENDOCRINOL, V43, P105, DOI 10.1111/j.1365-2265.1995.tb01899.x
   [Anonymous], 2017, PEDIAT BONE AGE ASSE
   [Anonymous], 2017, Machine learning and the future of radiology: how we won the 2017 RSNA ML challenge
   Bejnordi BE, 2017, JAMA-J AM MED ASSOC, V318, P2199, DOI 10.1001/jama.2017.14585
   Bull RK, 1999, ARCH DIS CHILD, V81, P172, DOI 10.1136/adc.81.2.172
   Carty H, 2002, ASSESSMENT SKELETAL
   Chen C., 2020, ATTENTIONGUIDED DISC
   Chen X, 2018, IEEE INT WORKSH MULT
   De Sanctis Vincenzo, 2014, Indian J Endocrinol Metab, V18, pS63, DOI 10.4103/2230-8210.145076
   Escobar M, 2019, LECT NOTES COMPUT SC, V11769, P531, DOI 10.1007/978-3-030-32226-7_59
   Frangi AF, 1998, LECT NOTES COMPUT SC, V1496, P130, DOI 10.1007/BFb0056195
   Garn StanleyM., 1959, American journal of human genetics, V11, P282
   Geng X, 2013, IEEE T PATTERN ANAL, V35, P2401, DOI 10.1109/TPAMI.2013.51
   Gertych A, 2007, COMPUT MED IMAG GRAP, V31, P322, DOI 10.1016/j.compmedimag.2007.02.012
   Giordano D, 2010, IEEE T INSTRUM MEAS, V59, P2539, DOI 10.1109/TIM.2010.2058210
   Goldstein H., 2001, ASSESSMENT SKELETAL
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Halabi SS, 2019, RADIOLOGY, V290, P498, DOI 10.1148/radiol.2018180736
   Hsieh CW, 2007, MED BIOL ENG COMPUT, V45, P283, DOI 10.1007/s11517-006-0155-9
   Huo ZW, 2016, IEEE COMPUT SOC CONF, P722, DOI 10.1109/CVPRW.2016.95
   Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005
   Müller R, 2019, ADV NEUR IN, V32
   Pietka E, 2003, COMPUT MED IMAG GRAP, V27, P217, DOI 10.1016/S0895-6111(02)00076-9
   Pietka E, 2001, IEEE T MED IMAGING, V20, P715, DOI 10.1109/42.938240
   POZNANSKI AK, 1978, RADIOLOGY, V129, P661, DOI 10.1148/129.3.661
   Ren XH, 2019, IEEE J BIOMED HEALTH, V23, P2030, DOI 10.1109/JBHI.2018.2876916
   Ruoyi Du, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P153, DOI 10.1007/978-3-030-58565-5_10
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Son SJ, 2019, IEEE ACCESS, V7, P33346, DOI 10.1109/ACCESS.2019.2903131
   Spampinato C, 2017, MED IMAGE ANAL, V36, P41, DOI 10.1016/j.media.2016.10.010
   Tanner JM, 1983, Assessment of Skeletal Maturity and Prediction of Adult Height (TW2 Method), V2nd
   Thodberg HH, 2009, IEEE T MED IMAGING, V28, P52, DOI 10.1109/TMI.2008.926067
   Tong C, 2018, J MED SYST, V42, DOI 10.1007/s10916-018-1091-6
   Zhang JF, 2021, IEEE ACCESS, V9, P40308, DOI 10.1109/ACCESS.2021.3063803
NR 34
TC 4
Z9 4
U1 1
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 1015
EP 1025
DI 10.1007/s00371-021-02381-2
EA JAN 2022
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000745362200002
DA 2024-07-18
ER

PT J
AU Paul, A
AF Paul, Abhisek
TI Adaptive tri-plateau limit tri-histogram equalization algorithm for
   digital image enhancement
SO VISUAL COMPUTER
LA English
DT Article
DE Histogram equalization; Plateau limit; Average information content;
   Contrast enhancement
ID SIMILARITY; RETRIEVAL; SCHEME
AB Histogram equalization (HE) is one of the most important techniques for contrast enhancement of digital images. Conventional HE methods persuade excessive enhancement, unnatural artifacts and brightness transform resulting abnormal and unusual appearance. To solve such problems, a novel tri-plateau limit-oriented tri-histogram equalization technique is suggested for digital image enhancement, where histogram of the input image is initially separated in three sub-histograms using separation threshold parameters. Next, plateau limit criteria for sub-histograms are formulated using the average of the mean and the median of each sub-histogram, and subsequently, a redistributed parameter is calculated and merged with each sub-histogram to restrict over-enhancement. Finally, modified sub-histograms are equalized separately and the enhanced image is produced by incorporating the images accomplished by the transformation function. Experimental results demonstrate that the proposed technique efficiently enhances the contrast of images, while visual quality assessments and quantitative measures, like average information content (AIC), feature similarity index measure (FSIM), multi scale structural similarity index measure (MS-SSIM), visual saliency-induced index (VSI), and gradient magnitude similarity deviation (GMSD) effectively validate the superiority of the proposed algorithm with respect to the other traditional state-of-the-art HE methods.
C1 [Paul, Abhisek] Natl Inst Technol, Dept Comp Sci & Engn, Agartala, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Agartala
RP Paul, A (corresponding author), Natl Inst Technol, Dept Comp Sci & Engn, Agartala, India.
EM abhisekpaul13@gmail.com
RI Paul, Abhisek/U-4659-2019
CR Acharya UK, 2020, OPTIK, V224, DOI 10.1016/j.ijleo.2020.165760
   Anwar S, 2020, ARAB J SCI ENG, V45, P11103, DOI 10.1007/s13369-020-04983-9
   Aquino-Morínigo PB, 2017, SIGNAL IMAGE VIDEO P, V11, P857, DOI 10.1007/s11760-016-1032-0
   Bhandari AK, 2020, J AMB INTEL HUM COMP, V11, P1605, DOI 10.1007/s12652-019-01258-6
   Pineda IAB, 2019, SIGNAL IMAGE VIDEO P, V13, P843, DOI 10.1007/s11760-019-01420-9
   Bulut F, 2022, VISUAL COMPUT, V38, P2239, DOI 10.1007/s00371-021-02281-5
   Cao LC, 2020, MED BIOL ENG COMPUT, V58, P483, DOI 10.1007/s11517-019-02106-7
   Chang SD, 2006, APPL OPTICS, V45, P5967, DOI 10.1364/AO.45.005967
   Chen LG, 2020, VISUAL COMPUT, V36, P2017, DOI 10.1007/s00371-020-01950-1
   Chen SD, 2003, IEEE T CONSUM ELECTR, V49, P1301, DOI 10.1109/TCE.2003.1261233
   Chen SD, 2003, IEEE T CONSUM ELECTR, V49, P1310, DOI 10.1109/TCE.2003.1261234
   Chen ZH, 2020, VISUAL COMPUT, V36, P2189, DOI 10.1007/s00371-020-01929-y
   Das B, 2022, VISUAL COMPUT, V38, P179, DOI 10.1007/s00371-020-02010-4
   Dawood H, 2020, MULTIMED TOOLS APPL, V79, P4483, DOI 10.1007/s11042-019-7520-9
   Franzen Rich, Kodak lossless true color image suite
   Gao GX, 2021, OPTIK, V226, DOI 10.1016/j.ijleo.2020.165659
   Gonzalez R. C., 2002, DIGITAL IMAGE PROCES
   Huang ZH, 2021, OPTIK, V226, DOI 10.1016/j.ijleo.2020.165877
   Iqbal K, 2016, NEUROCOMPUTING, V174, P413, DOI 10.1016/j.neucom.2015.03.120
   Jin C, 2020, MULTIMED TOOLS APPL, V79, P20947, DOI 10.1007/s11042-020-08871-0
   Joshi P, 2020, VISUAL COMPUT, V36, P71, DOI 10.1007/s00371-018-1587-6
   Kandhway P, 2019, IET IMAGE PROCESS, V13, P1658, DOI 10.1049/iet-ipr.2019.0111
   Kim YT, 1997, IEEE T CONSUM ELECTR, V43, P1, DOI 10.1109/30.580378
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Liu XW, 2019, VISUAL COMPUT, V35, P1883, DOI 10.1007/s00371-018-1581-z
   Loh YP, 2019, SIGNAL PROCESS-IMAGE, V74, P175, DOI 10.1016/j.image.2019.02.001
   Caballero RDM, 2019, INFRARED PHYS TECHN, V99, P257, DOI 10.1016/j.infrared.2019.03.016
   Moriyama D, 2020, OPT REV, V27, P352, DOI 10.1007/s10043-020-00602-y
   Ooi CH, 2010, IEEE T CONSUM ELECTR, V56, P2543, DOI 10.1109/TCE.2010.5681139
   Ooi CH, 2009, IEEE T CONSUM ELECTR, V55, P2072, DOI 10.1109/TCE.2009.5373771
   Paul A, 2021, OPTIK, V247, DOI 10.1016/j.ijleo.2021.167887
   Paul A, 2020, APPL OPTICS, V59, P9032, DOI 10.1364/AO.395848
   Paul A, 2018, IET IMAGE PROCESS, V12, P1617, DOI 10.1049/iet-ipr.2017.1088
   Ping-Hsien Lin, 2009, 2009 IEEE 13th International Symposium on Consumer Electronics (ISCE), P387, DOI 10.1109/ISCE.2009.5156973
   Qadar MA, 2015, OPTIK, V126, P5890, DOI 10.1016/j.ijleo.2015.08.278
   Raikwar SC, 2020, VISUAL COMPUT, V36, P191, DOI 10.1007/s00371-018-1596-5
   Gandhi CR, 2021, J AMB INTEL HUM COMP, DOI 10.1007/s12652-021-03132-w
   Rong Z, 2015, OPTIK, V126, P5665, DOI 10.1016/j.ijleo.2015.08.169
   Shannon C. E., 1948, BELL SYST TECH J, V27, P379, DOI DOI 10.1002/J.1538-7305.1948.TB01338.X
   Sidar I., 2020, US Patent App, Patent No. [16/685,299, 16685299]
   Siddiqi AA, 2019, BIOMED ENG-APP BAS C, V31, DOI 10.4015/S1016237219500388
   Signal and image processing institute of USc university of southern california, USC SIPI IMAGE DATAB
   Sim KS, 2007, PATTERN RECOGN LETT, V28, P1209, DOI 10.1016/j.patrec.2007.02.003
   Simi VR, 2020, APPL SOFT COMPUT, V93, DOI 10.1016/j.asoc.2020.106364
   Singh H, 2019, MULTIMED TOOLS APPL, V78, P20431, DOI 10.1007/s11042-019-7383-0
   Sirajuddeen CK, 2020, SIGNAL IMAGE VIDEO P, V14, P9, DOI 10.1007/s11760-019-01516-2
   Srinivas K, 2020, IEEE T CIRC SYST VID, V30, P4663, DOI 10.1109/TCSVT.2019.2960861
   Tang Y., CIRCUIT SYST SIG PRO, P1
   Ulutas G, 2021, MULTIMED TOOLS APPL, V80, P15067, DOI 10.1007/s11042-020-10426-2
   Wang P, 2021, MULTIMED TOOLS APPL, V80, P17705, DOI 10.1007/s11042-021-10607-7
   Wang Y, 1999, IEEE T CONSUM ELECTR, V45, P68, DOI 10.1109/30.754419
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yuan Q, 2020, VISUAL COMPUT, V36, P1591, DOI 10.1007/s00371-019-01762-y
   Zarie M, 2019, IET IMAGE PROCESS, V13, P1081, DOI 10.1049/iet-ipr.2018.5395
   Zarie M, 2018, OPTIK, V175, P126, DOI 10.1016/j.ijleo.2018.08.082
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang L, 2019, VISUAL COMPUT, V35, P1091, DOI 10.1007/s00371-019-01685-8
   Zhang SD, 2020, VISUAL COMPUT, V36, P305, DOI 10.1007/s00371-018-1612-9
   Zhang S, 2017, NEUROCOMPUTING, V245, P1, DOI 10.1016/j.neucom.2017.03.029
NR 61
TC 17
Z9 19
U1 10
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 297
EP 318
DI 10.1007/s00371-021-02330-z
EA NOV 2021
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000713568700001
DA 2024-07-18
ER

PT J
AU Jain, R
   Karsh, RK
   Barbhuiya, A
AF Jain, Rahul
   Karsh, Ram Kumar
   Barbhuiya, Abul Abbas
TI Encoded motion image-based dynamic hand gesture recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Dynamic hand gesture; Human-computer interaction; Dynamic imaging;
   2D-CNN
ID HUMAN-COMPUTER INTERACTION; DATASET
AB Dynamic hand gesture recognition is a crucial need in a smart human-computer interaction (HCI) system. Dynamic imaging has been recently introduced as a gesture description paradigm for simultaneously capturing spatial, temporal, and structural information from the depth video. However, existing techniques based on dynamic images cannot differentiate gesture movements that follow the same path but in opposite directions, for example, "moving a hand down" versus "moving a hand up." To solve the issue, we have proposed an approach in which a gesture depth video is converted into a single image called an encoded motion image (EMI). The EMI has been given to a modified pre-trained 2D-CNN(two-dimensional convolutional neural network) based on VGG-19 to classify gestures present in the depth video. The experiments were carried out on two datasets: a multi-modal large-scale EgoGesture and MSR Gesture 3D datasets. For the EgoGesture dataset, the proposed method achieved an accuracy of 90.63%. Such a result provides state-of-the-art accuracy when employing this large-scale dataset of 83 classes and the 2D-CNN approach. For the MSR Gesture 3D dataset, the proposed method accuracy is 99.24%, which outperforms the state-of-the-art methods. This work also highlights the recognition accuracy and precision of each gesture. Instead of high-end systems like GPU, the experiments are conducted using a web-based data science environment called Kaggle to demonstrate the work's economic efficiency.
C1 [Jain, Rahul; Karsh, Ram Kumar; Barbhuiya, Abul Abbas] Natl Inst Technol, Elect & Commun Engn Dept, Silchar 788010, Assam, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Silchar
RP Jain, R (corresponding author), Natl Inst Technol, Elect & Commun Engn Dept, Silchar 788010, Assam, India.
EM rahul_jain_rs@ece.nits.ac.in
OI Karsh, Ram/0000-0002-2341-341X; Jain, Rahul/0000-0003-4894-3721
CR Al-Shamayleh AS, 2018, MULTIMED TOOLS APPL, V77, P28121, DOI 10.1007/s11042-018-5971-z
   Asadi-Aghbolaghi M, 2017, IEEE INT CONF AUTOMA, P476, DOI 10.1109/FG.2017.150
   Azad R, 2019, IEEE T CIRC SYST VID, V29, P1729, DOI 10.1109/TCSVT.2018.2855416
   Barbhuiya AA, 2021, MULTIMED TOOLS APPL, V80, P3051, DOI 10.1007/s11042-020-09829-y
   Barros P, 2014, IEEE-RAS INT C HUMAN, P646, DOI 10.1109/HUMANOIDS.2014.7041431
   Cao CQ, 2017, IEEE I CONF COMP VIS, P3783, DOI 10.1109/ICCV.2017.406
   Chang CC, 2006, J INF SCI ENG, V22, P1047
   Chen C, 2015, IEEE WINT CONF APPL, P1092, DOI 10.1109/WACV.2015.150
   Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174
   dos Santos CC, 2020, NEUROCOMPUTING, V400, P238, DOI 10.1016/j.neucom.2020.03.038
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Duan JL, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3131343
   Elboushaki A, 2020, EXPERT SYST APPL, V139, DOI 10.1016/j.eswa.2019.112829
   Guyon I, 2014, MACH VISION APPL, V25, P1929, DOI 10.1007/s00138-014-0596-3
   Hasan H, 2014, NEURAL COMPUT APPL, V25, P251, DOI 10.1007/s00521-013-1481-0
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Koehn P., 1994, Combining genetic algorithms and neural networks: the encoding problem
   Kopuklu Okan, 2020, IEEE Transactions on Biometrics, Behavior, and Identity Science, V2, P85, DOI 10.1109/TBIOM.2020.2968216
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kurakin A, 2012, EUR SIGNAL PR CONF, P1975
   Li ZF, 2019, MULTIMED TOOLS APPL, V78, P19587, DOI 10.1007/s11042-019-7356-3
   Lin M., 2013, ARXIV13124400
   Liu Z, 2016, IMAGE VISION COMPUT, V55, P93, DOI 10.1016/j.imavis.2016.04.004
   Mitra S, 2007, IEEE T SYST MAN CY C, V37, P311, DOI 10.1109/TSMCC.2007.893280
   Molchanov P., 2016, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, P4207, DOI DOI 10.1109/CVPR.2016.456
   Narayana P, 2018, PROC CVPR IEEE, P5235, DOI 10.1109/CVPR.2018.00549
   O'Mahony N, 2020, ADV INTELL SYST COMP, V943, P128, DOI 10.1007/978-3-030-17795-9_10
   Pavlovic VI, 1997, IEEE T PATTERN ANAL, V19, P677, DOI 10.1109/34.598226
   Simonyan K, 2014, ADV NEUR IN, V27
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy Christian, 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.308
   Wang PC, 2018, COMPUT VIS IMAGE UND, V171, P118, DOI 10.1016/j.cviu.2018.04.007
   Wang PC, 2018, IEEE T MULTIMEDIA, V20, P1051, DOI 10.1109/TMM.2018.2818329
   Wang PC, 2016, INT C PATT RECOG, P13, DOI 10.1109/ICPR.2016.7899600
   Wang PC, 2016, INT C PATT RECOG, P7, DOI 10.1109/ICPR.2016.7899599
   Yang R, 2015, LECT NOTES COMPUT SC, V9007, P37, DOI 10.1007/978-3-319-16814-2_3
   Yang X., 2012, P 20 ACM INT C MULT, P1057, DOI DOI 10.1145/2393347.2396382
   Zhang YF, 2018, IEEE T MULTIMEDIA, V20, P1038, DOI 10.1109/TMM.2018.2808769
   Zhang Z, 2017, IEEE INT CONF AUTOMA, P238, DOI 10.1109/FG.2017.38
NR 41
TC 12
Z9 12
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2022
VL 38
IS 6
BP 1957
EP 1974
DI 10.1007/s00371-021-02259-3
EA AUG 2021
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1C6YN
UT WOS:000682810300001
DA 2024-07-18
ER

PT J
AU Mata-Mendoza, D
   Cedillo-Hernandez, M
   Garcia-Ugalde, F
   Cedillo-Hernandez, A
   Nakano-Miyatake, M
   Perez-Meana, H
AF Mata-Mendoza, David
   Cedillo-Hernandez, Manuel
   Garcia-Ugalde, Francisco
   Cedillo-Hernandez, Antonio
   Nakano-Miyatake, Mariko
   Perez-Meana, Hector
TI Secured telemedicine of medical imaging based on dual robust
   watermarking
SO VISUAL COMPUTER
LA English
DT Article
DE Digital watermarking; Information security; DICOM imaging;
   Authentication; Detachment avoidance
ID MULTIPLE WATERMARKING; VISIBLE WATERMARKING; DIGITAL WATERMARKING;
   INFORMATION; IMAGES; DELIVERY; SCHEME; NOISE
AB Medical information management has progressed in the last few years because of the advances in information technologies. Nowadays, it is possible to share medical images among specialists geographically distant to interpret, discuss, and get improved diagnostics. However, any alteration of transmitted image metadata may lead to issues related to information security, such as detachment and authentication. Detachment refers to link the data of an electronic patient record to an incorrect medical image, while authentication aims to identify the image source. These security problems are critical as they may cause the loss of sensitive data or wrong medical diagnoses. Digital watermarking is an emerging technique that faces these security problems as it allows to embed the metadata directly into the medical image. This paper proposes a hybrid and robust watermarking technique to prevent detachment and authenticate medical images. The quantization index modulation algorithm under dither modulation in conjunction with forwarding error correction is used to embed relevant metadata as a robust-imperceptible watermarking to avoid detachment. The visible-imperceptible watermarking paradigm, whose use is an innovation in medical images, is applied to insert a second watermark in the spatial domain to perform authentication. The experimental results show the contribution of the proposed scheme and its efficiency regarding robustness and imperceptibility.
C1 [Mata-Mendoza, David; Cedillo-Hernandez, Manuel; Nakano-Miyatake, Mariko; Perez-Meana, Hector] Inst Politecn Nacl SEPI ESIME Culhuacan, Av Santa Ana 1000 Culhuacan CTM, Cdmx 04440, Mexico.
   [Garcia-Ugalde, Francisco] Univ Nacl Autonoma Mexico, Fac Ingn, CDMX 04510, Mexico.
   [Cedillo-Hernandez, Antonio] Tecnol Monterrey, Escuela Ingn & Ciencias, Av Eugenio Garza Sada 2501, Monterrey 64849, NL, Mexico.
C3 Universidad Nacional Autonoma de Mexico; Tecnologico de Monterrey
RP Cedillo-Hernandez, M (corresponding author), Inst Politecn Nacl SEPI ESIME Culhuacan, Av Santa Ana 1000 Culhuacan CTM, Cdmx 04440, Mexico.
EM mcedillohdz@hotmail.com
RI Perez-Meana, Hector/N-1624-2019; Nakano, Mariko/N-4075-2019;
   Cedillo-Hernandez, Antonio/IYJ-5019-2023; Nakano, Mariko/O-2954-2017;
   Cedillo-Hernandez, Manuel/R-2154-2018
OI Perez-Meana, Hector/0000-0002-7786-2050; Cedillo-Hernandez,
   Antonio/0000-0003-3420-6851; Nakano, Mariko/0000-0003-1346-7825;
   Cedillo-Hernandez, Manuel/0000-0002-9149-9841; Garcia-Ugalde, Francisco
   Javier/0000-0002-1140-2190
FU Instituto Politecnico Nacional (IPN); DGAPA in the Universidad Nacional
   Autonoma de Mexico (UNAM) [PAPIIT IT-101119]; Instituto Mexicano del
   Seguro Social (IMSS) of Mexico; Consejo Nacional de Ciencia y Tecnologia
   de Mexico (CONACYT)
FX Authors thank the Instituto Politecnico Nacional (IPN), the PAPIIT
   IT-101119 project research from DGAPA in the Universidad Nacional
   Autonoma de Mexico (UNAM), the Instituto Mexicano del Seguro Social
   (IMSS) of Mexico as well as the Consejo Nacional de Ciencia y Tecnologia
   de Mexico (CONACYT) by the support provided during the realization of
   this research.
CR Aherrahrou N, 2015, BIOMED ENG ONLINE, V14, DOI 10.1186/s12938-015-0101-x
   Anand A, 2020, COMPUT COMMUN, V152, P72, DOI 10.1016/j.comcom.2020.01.038
   [Anonymous], 1972, NASATRR396
   Ahmadi SBB, 2021, VISUAL COMPUT, V37, P385, DOI 10.1007/s00371-020-01808-6
   Barni M., 2004, WATERMARKING SYSTEMS, P23, DOI [10.1201/9780203913512, DOI 10.1201/9780203913512]
   Bosselaers A, 1997, DR DOBBS J, V22, P24
   Cedillo-Hernandez M, 2020, BIOMED SIGNAL PROCES, V56, DOI 10.1016/j.bspc.2019.101695
   Cedillo-Hernandez M, 2015, SIGNAL IMAGE VIDEO P, V9, P1163, DOI 10.1007/s11760-013-0555-x
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Chen B, 2001, IEEE T INFORM THEORY, V47, P1423, DOI 10.1109/18.923725
   Chuang SC, 2007, IEEE IMAGE PROC, P1389
   Coatrieux G, 2008, STUD HEALTH TECHNOL, V136, P667
   Colin RR, 2008, IEICE T INF SYST, VE91D, P862, DOI 10.1093/ietisy/e91-d.3.862
   Das S, 2012, J MED SYST, V36, P3339, DOI 10.1007/s10916-012-9827-1
   Dobbertin H., 1996, Fast Software Encryption. Third International Workshop Proceedings, P71
   El Tokhy MS, 2021, COMPUT ELECTR ENG, V89, DOI 10.1016/j.compeleceng.2020.106932
   Gangadhar Y, 2018, BIOMED SIGNAL PROCES, V43, P31, DOI 10.1016/j.bspc.2018.02.007
   Geetha R., 2020, Journal of Medical Engineering & Technology, V44, P55, DOI 10.1080/03091902.2020.1718223
   Gong ZT, 2022, VISUAL COMPUT, V38, P707, DOI 10.1007/s00371-020-02045-7
   Haddad S, 2020, IEEE T INF FOREN SEC, V15, P2556, DOI 10.1109/TIFS.2020.2972159
   Hernandez MC, 2015, IEICE T INF SYST, VE98D, P1702, DOI 10.1587/transinf.2015EDL8016
   Huang CH, 2009, IEEE T INF FOREN SEC, V4, P193, DOI 10.1109/TIFS.2009.2020778
   Kahlessenane F, 2021, J AMB INTEL HUM COMP, V12, P2931, DOI 10.1007/s12652-020-02450-9
   Kalaivani K, 2016, BRAZ ARCH BIOL TECHN, V59, DOI 10.1590/1678-4324-2016161070
   Lin PY, 2014, J SYST SOFTWARE, V95, P194, DOI 10.1016/j.jss.2014.04.038
   Liu DC, 2021, VISUAL COMPUT, V37, P2355, DOI 10.1007/s00371-020-01991-6
   Liu J, 2020, IEEE ACCESS, V8, P93939, DOI 10.1109/ACCESS.2020.2995015
   Medixant, 2020, RADIANT DICOM VIEW S
   Mousavi SM, 2017, MULTIMED TOOLS APPL, V76, P10313, DOI 10.1007/s11042-016-3622-9
   Mousavi SM, 2014, J DIGIT IMAGING, V27, P714, DOI 10.1007/s10278-014-9700-5
   National Electrical Manufacturers Association (NEMA), 2020, DICOM SEC
   Nuñez-Ramirez D, 2020, IEEE LAT AM T, V18, P1398, DOI 10.1109/TLA.2020.9111675
   Pei SC, 2015, IEEE T MULTIMEDIA, V17, P128, DOI 10.1109/TMM.2014.2368255
   Qasim AF, 2018, COMPUT SCI REV, V27, P45, DOI 10.1016/j.cosrev.2017.11.003
   Schneier B., 2015, APPL CRYPTOGRAPHY, VSecond
   Selvi TM, 2022, VISUAL COMPUT, V38, P385, DOI 10.1007/s00371-020-02021-1
   Sharma A, 2017, WIRELESS PERS COMMUN, V92, P1611, DOI 10.1007/s11277-016-3625-x
   Sharma A, 2015, PROCEDIA COMPUT SCI, V70, P778, DOI 10.1016/j.procs.2015.10.117
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Singh AK, 2016, MULTIMED TOOLS APPL, V75, P8381, DOI 10.1007/s11042-015-2754-7
   Sklar B., 2003, DIGITAL COMMUNICATIO, V2nd
   Su GD, 2020, IEEE ACCESS, V8, P160840, DOI 10.1109/ACCESS.2020.3019832
   Swaraja K, 2020, BIOMED SIGNAL PROCES, V55, DOI 10.1016/j.bspc.2019.101665
   Swaraja K, 2018, MULTIMED TOOLS APPL, V77, P28249, DOI 10.1007/s11042-018-6020-7
   Tang CW, 2003, IEEE T SIGNAL PROCES, V51, P950, DOI 10.1109/TSP.2003.809367
   Thakkar FN, 2017, MULTIMED TOOLS APPL, V76, P3669, DOI 10.1007/s11042-016-3928-7
   Juarez-Sandoval OU, 2018, MULTIMED TOOLS APPL, V77, P26601, DOI 10.1007/s11042-018-5881-0
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   WATSON AB, 1993, P SOC PHOTO-OPT INS, V1913, P202, DOI 10.1117/12.152694
   Yuan ZH, 2021, VISUAL COMPUT, V37, P1867, DOI 10.1007/s00371-020-01945-y
NR 50
TC 22
Z9 22
U1 3
U2 27
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2022
VL 38
IS 6
BP 2073
EP 2090
DI 10.1007/s00371-021-02267-3
EA AUG 2021
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1C6YN
UT WOS:000680330600001
DA 2024-07-18
ER

PT J
AU Amirkhani, A
   Karimi, MP
AF Amirkhani, Abdollah
   Karimi, Mohammad Parsa
TI Adversarial defenses for object detectors based on Gabor convolutional
   layers
SO VISUAL COMPUTER
LA English
DT Article
DE Machine vision; Object detection; Adversarial attack; Robust detector
ID ATTACKS
AB Despite their many advantages and positive features, the deep neural networks are extremely vulnerable against adversarial attacks. This drawback has substantially reduced the adversarial accuracy of the visual object detectors. To make these object detectors robust to adversarial attacks, a new Gabor filter-based method has been proposed in this paper. This method has then been applied on the YOLOv3 with different backbones, the SSD with different input sizes and on the FRCNN; and thus, six robust object detector models have been presented. In order to evaluate the efficacy of the models, they have been subjected to adversarial training via three types of targeted attacks (TOG-fabrication, TOG-vanishing, and TOG-mislabeling) and three types of untargeted random attacks (DAG, RAP, and UEA). The best average accuracy (49.6%) was achieved by the YOLOv3-d model, and for the PASCAL VOC dataset. This is far superior to the best performance and accuracy and obtained in previous works (25.4%). Empirical results show that, while the presented approach improves the adversarial accuracy of the object detector models, it does not affect the performance of these models on clean data.
C1 [Amirkhani, Abdollah; Karimi, Mohammad Parsa] Iran Univ Sci & Technol, Sch Automot Engn, Tehran 1684613114, Iran.
C3 Iran University Science & Technology
RP Amirkhani, A (corresponding author), Iran Univ Sci & Technol, Sch Automot Engn, Tehran 1684613114, Iran.
EM amirkhani@iust.ac.ir
RI Amirkhani, Abdollah/C-6743-2019
OI Amirkhani, Abdollah/0000-0001-6891-4528
CR Akhtar N, 2018, IEEE ACCESS, V6, P14410, DOI 10.1109/ACCESS.2018.2807385
   Alekseev A., 2019, 2019 INT C ENG TEL, P1, DOI [DOI 10.1109/ent47717.2019.9030571, 10.1109/EnT47717.2019.9030571]
   Aprilpyone M, 2019, IEEE ACCESS, V7, P177932, DOI 10.1109/ACCESS.2019.2958358
   Arnab A, 2018, PROC CVPR IEEE, P888, DOI 10.1109/CVPR.2018.00099
   Arora S, 2022, VISUAL COMPUT, V38, P2461, DOI 10.1007/s00371-021-02123-4
   Bansal A, 2018, IEEE COMPUT SOC CONF, P10, DOI 10.1109/CVPRW.2018.00009
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Cho S., 2020, 2020 INT JOINT C NEU, P1
   Chow K.H., 2020, ADVERSARIAL OBJECTNE
   Chow KH, 2020, SECURITY CRYPTOLOGY, V12309, P460, DOI 10.1007/978-3-030-59013-0_23
   Goswami G, 2019, INT J COMPUT VISION, V127, P719, DOI 10.1007/s11263-019-01160-w
   Guo Q., 2019, P EUR C COMP VIS ECC
   Kamboj A, 2022, VISUAL COMPUT, V38, P2383, DOI 10.1007/s00371-021-02119-0
   Karan, 2020, 2020 International Conference on Electronics and Sustainable Communication Systems (ICESC). Proceedings, P45, DOI 10.1109/ICESC48915.2020.9155891
   Kong T, 2020, IEEE T IMAGE PROCESS, V29, P7389, DOI 10.1109/TIP.2020.3002345
   Lee Mark, 2019, PHYS ADVERSARIAL PAT
   Li D., PATTERN RECOGN, V110, P2021
   Li HF, 2020, IEEE T CYBERNETICS, V50, P4835, DOI 10.1109/TCYB.2019.2914099
   Li Y., 2018, British Machine Vision Conference, P231
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu ZY, 2020, VISUAL COMPUT, V36, P1823, DOI 10.1007/s00371-019-01778-4
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Naseer M, 2019, IEEE WINT CONF APPL, P1300, DOI 10.1109/WACV.2019.00143
   Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36
   Perez Juan C., 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P450, DOI 10.1007/978-3-030-58545-7_26
   Rahmadwati G.N., 2011, First IEEE International Conference on Healthcare Informatics, Imaging and Systems Biology, P48
   Ramalli Alessandro, 2017, 2017 IEEE International Ultrasonics Symposium (IUS), DOI 10.1109/ULTSYM.2017.8091860
   Ros AS, 2018, AAAI CONF ARTIF INTE, P1660
   Sarwar SS, 2017, I SYMPOS LOW POWER E
   Song D., 2018, 12 WORKSH OFF TECHN
   Wang YJ, 2021, INFORM SCIENCES, V556, P459, DOI 10.1016/j.ins.2020.08.087
   Wang YJ, 2020, J NETW COMPUT APPL, V161, DOI 10.1016/j.jnca.2020.102634
   Wei XX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P954
   Wu F, 2019, IEEE INT C NETW SENS, P363, DOI [10.1109/ICNSC.2019.8743246, 10.1109/icnsc.2019.8743246]
   Xie CH, 2017, IEEE I CONF COMP VIS, P1378, DOI 10.1109/ICCV.2017.153
   Yahya Z, 2020, IEEE ACCESS, V8, P33855, DOI 10.1109/ACCESS.2020.2974525
   Zhang HC, 2019, IEEE I CONF COMP VIS, P421, DOI 10.1109/ICCV.2019.00051
NR 38
TC 5
Z9 5
U1 1
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2022
VL 38
IS 6
BP 1929
EP 1944
DI 10.1007/s00371-021-02256-6
EA JUL 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1C6YN
UT WOS:000677248300001
DA 2024-07-18
ER

PT J
AU Dai, WT
   Erdt, M
   Sourin, A
AF Dai, Wenting
   Erdt, Marius
   Sourin, Alexei
TI Detection and segmentation of image anomalies based on unsupervised
   defect reparation
SO VISUAL COMPUTER
LA English
DT Article
DE Anomaly detection; Anomaly segmentation; Defect reparation; Image
   resynthesis
AB Anomaly detection is a challenging task in the field of data analysis, especially when it comes to unsupervised pixel-level segmentation of anomalies in images. In this paper, we present a novel multi-stage image resynthesis framework for detecting and segmenting image anomalies. In contrast to existing reconstruction-based approaches, our method is based on repairing suspicious regions of defective images so that the defects can be localized in the residual map between inputs and the repaired outputs. To avoid the reconstruction artifacts caused by defects, we propose to generate each pixel of the image by its context in the first coarse reconstruction stage. Then, while excluding all safe pixels, our method repairs suspicious regions that have large deviations to the original input image in subsequent stages. After several iterations, the defects will be detected in the final residual map. The experimental results show that we achieved better performances than the state-of-the-art benchmarks using the publicly available MVTec dataset as well as a real-world equipment surface dataset. In addition, the method also demonstrates an excellent capability of repairing defects in abnormal samples.
C1 [Dai, Wenting; Sourin, Alexei] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
   [Erdt, Marius] Nanyang Technol Univ, Fraunhofer Res Ctr, Singapore, Singapore.
C3 Nanyang Technological University; Nanyang Technological University
RP Dai, WT (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
EM daiw0004@e.ntu.edu.sg; marius.erdt@fraunhofer.sg; assourin@ntu.edu.sg
RI Wenting, Dai/KGM-7881-2024
OI Wenting, Dai/0000-0001-7033-2332
CR Akcay S, 2019, LECT NOTES COMPUT SC, V11363, P622, DOI 10.1007/978-3-030-20893-6_39
   Alrashdi I, 2019, 2019 IEEE 9TH ANNUAL COMPUTING AND COMMUNICATION WORKSHOP AND CONFERENCE (CCWC), P305, DOI 10.1109/CCWC.2019.8666450
   Bergmann P., 2018, ARXIV180702011 CORR
   Bergmann P, 2019, PROC CVPR IEEE, P9584, DOI 10.1109/CVPR.2019.00982
   Bottger T., 2016, Pattern Recognition and Image Analysis, V26, P88
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hariharan A, 2020, ADV INTELL SYST COMP, V1130, P705, DOI 10.1007/978-3-030-39442-4_52
   Hu GH, 2020, TEXT RES J, V90, P247, DOI 10.1177/0040517519862880
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Mei S, 2018, IEEE T INSTRUM MEAS, V67, P1266, DOI 10.1109/TIM.2018.2795178
   Mujeeb A, 2019, ADV ENG INFORM, V42, DOI 10.1016/j.aei.2019.100933
   Napoletano P, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18010209
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Schlegl T, 2017, LECT NOTES COMPUT SC, V10265, P146, DOI 10.1007/978-3-319-59050-9_12
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Steger C., 2001, Pattern Recognition. 23rd DAGM Symposium. Proceedings (Lecture Notes in Computer Science Vol.2191), P148
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wenqian Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8639, DOI 10.1109/CVPR42600.2020.00867
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zenati H., 2018, ARXIV180206222 CORR
   Zhao ZX, 2018, LECT NOTES ARTIF INT, V11013, P473, DOI 10.1007/978-3-319-97310-4_54
NR 22
TC 8
Z9 8
U1 7
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2021
VL 37
IS 12
SI SI
BP 3093
EP 3102
DI 10.1007/s00371-021-02257-5
EA JUL 2021
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP1DP
UT WOS:000677248300002
DA 2024-07-18
ER

PT J
AU Bhaumik, G
   Verma, M
   Govil, MC
   Vipparthi, SK
AF Bhaumik, Gopa
   Verma, Monu
   Govil, Mahesh Chandra
   Vipparthi, Santosh Kumar
TI ExtriDeNet: an intensive feature extrication deep network for hand
   gesture recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Hand gesture recognition; ExtriDeNet; Deep learning; Convolutional
   neural network
AB In this paper, a lightweighted Intensive Feature Extrication Deep Network (ExtriDeNet) is proposed for precise hand gesture recognition (HGR). ExtriDeNet primarily consists of two blocks: Intensive Feature Fusion Block (IFFB) and Intensive Feature Assimilation Block (IFAB). IFFB incorporates two different scaled filters 3 x 3 and 5 x 5 to capture contextual features of hands, while IFAB is designed by embedding influential features of IFFB with two extreme minute and high-level feature responses from two receptive fields generated by employing 1 x 1 and 7 x 7 sized filters, respectively. The combination of multiscaled filters enriches the network with the most significant features and enhances the learnability of the network. Thus, the proposed ExtriDeNet efficiently defines the distinctive features of different hand gesture classes and achieves high performance as compared to state-of-the-art HGR approaches. The performance of the proposed network is evaluated on the standard datasets: MUGD, Finger Spelling, OUHands, NUS-I, NUS-II and HGR1 for both subject-dependent and subject-independent scheme.
C1 [Bhaumik, Gopa; Govil, Mahesh Chandra] Natl Inst Technol, CSE Dept, Sikkim, India.
   [Verma, Monu; Vipparthi, Santosh Kumar] Malaviya Natl Inst Technol, CSE Dept, Jaipur, Rajasthan, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Sikkim; National Institute of Technology (NIT System);
   Malaviya National Institute of Technology Jaipur
RP Bhaumik, G (corresponding author), Natl Inst Technol, CSE Dept, Sikkim, India.
EM gopa.bhaumik09@nitsikkim.ac.in
RI Vipparthi, Santosh Kumar/AAV-8694-2020
OI Vipparthi, Santosh Kumar/0000-0002-5672-3537; , Gopa/0000-0002-3481-717X
CR Andersson K., 2019, JOINT 2019 8 INT C I
   Barczak A, 2011, A new 2D static hand gesture colour image dataset for ASL gestures
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Bhuvaneshwari C, 2019, MATER TODAY-PROC
   Côté-Allard U, 2019, IEEE T NEUR SYS REH, V27, P760, DOI 10.1109/TNSRE.2019.2896269
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Imran J, 2020, VISUAL COMPUT, V36, P1233, DOI 10.1007/s00371-019-01725-3
   Lindeberg T., 2012, SCHOLARPEDIA, V7, P10491, DOI [10.4249/scholarpedia.10491, DOI 10.4249/SCHOLARPEDIA.10491]
   Liu JQ, 2019, IEEE IMAGE PROC, P375, DOI [10.1109/ICIP.2019.8802970, 10.1109/icip.2019.8802970]
   MATILAINEN M, 2016, INT CONF IMAG PROC, DOI DOI 10.1109/IPTA.2016.7821025
   Nicolas Pugeault R.B, ASL FINGER SPELLING
   Ozcan T, 2019, NEURAL COMPUT APPL, V31, P8955, DOI 10.1007/s00521-019-04427-y
   Pedersoli F, 2014, VISUAL COMPUT, V30, P1107, DOI 10.1007/s00371-014-0921-x
   Pinto RF, 2019, J ELECTR COMPUT ENG, V2019, DOI 10.1155/2019/4167890
   Pisharady PK, 2013, INT J COMPUT VISION, V101, P403, DOI 10.1007/s11263-012-0560-5
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shu XB, 2022, IEEE T PATTERN ANAL, V44, P3300, DOI 10.1109/TPAMI.2021.3050918
   Shu XB, 2021, IEEE T NEUR NET LEAR, V32, P663, DOI 10.1109/TNNLS.2020.2978942
   Shu XB, 2021, IEEE T PATTERN ANAL, V43, P1110, DOI 10.1109/TPAMI.2019.2942030
   Shu XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P35, DOI 10.1145/2733373.2806216
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy Christian, 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.308
   Tamiru NK, 2022, VISUAL COMPUT, V38, P1703, DOI 10.1007/s00371-021-02099-1
   Tang J., 2016, Intl. Test Conf, P1
   Trong-Nguyen Nguyen, 2015, Journal of Automation and Control Engineering, V3, P40, DOI 10.12720/joace.3.1.40-45
   Varun Kollipara Sai, 2019, 2019 International Conference on Communication and Signal Processing (ICCSP), P0592, DOI 10.1109/ICCSP.2019.8697980
   Vipparthi S.K, 2020, 2020 INT C COMM SIGN
   Zhan F, 2019, 2019 IEEE 20TH INTERNATIONAL CONFERENCE ON INFORMATION REUSE AND INTEGRATION FOR DATA SCIENCE (IRI 2019), P295, DOI 10.1109/IRI.2019.00054
   Zhang W, 2020, VISUAL COMPUT, V36, P2433, DOI 10.1007/s00371-020-01955-w
NR 30
TC 19
Z9 19
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3853
EP 3866
DI 10.1007/s00371-021-02225-z
EA JUL 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000670161500002
DA 2024-07-18
ER

PT J
AU Guan, XY
   Luo, LK
   Li, HL
   Wang, H
   Liu, C
   Wang, S
   Jin, XG
AF Guan, Xinyang
   Luo, Likang
   Li, Honglin
   Wang, He
   Liu, Chen
   Wang, Su
   Jin, Xiaogang
TI Automatic embroidery texture synthesis for garment design and online
   display
SO VISUAL COMPUTER
LA English
DT Article
DE Embroidery; Non-photorealistic rendering; Image-based artistic rendering
AB We introduce an automatic texture synthesis-based framework to convert an arbitrary input image into embroidery style art for garment design and online display. Given an input image and some reference textures, we first extract key embroidery regions from the input image using image segmentation. Each segmented region is single-colored and labeled with a stitch style automatically. We then fill these regions with embroidery reference textures via a stitch-style-based texture synthesis method. For each region, our approach maintains color similarity before and after synthesis, along with stitch style consistency. Compared to existing approaches, our method is able to generate digital embroidery patterns with faithful details automatically. Moreover, it can accept diverse input images effectively, enabling a fast preview of the embroidery patterns synthesized on digital garments interactively, and therefore accelerating the workflow from design to production. We validate our method through extensive experimentation and comparison.
C1 [Guan, Xinyang; Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Peoples R China.
   [Luo, Likang; Liu, Chen] Zhejiang Linctex Digital Technol Co Ltd, Hangzhou, Peoples R China.
   [Li, Honglin] Quanzhou Med Coll, Quanzhou, Peoples R China.
   [Wang, He] Univ Leeds, Sch Comp, Leeds, W Yorkshire, England.
   [Wang, Su] Peking Univ, Sch Software & Microelect, Beijing, Peoples R China.
C3 Zhejiang University; University of Leeds; Peking University
RP Jin, XG (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Peoples R China.
EM jin@cad.zju.edu.cn
RI Wang, He/ABD-8303-2021
OI Wang, He/0000-0002-2281-5679; Jin, Xiaogang/0000-0001-7339-2920
FU National Key R&D Program of China [2017YFB1002600]; National Natural
   Science Foundation of China [61732015]; Ningbo Major Special Projects of
   the "Science and Technology Innovation 2025" [2020Z007]; Key Research
   and Development Program of Zhejiang Province [2018C01090]
FX We thank the anonymous reviewers for their constructive comments.
   Xiaogang Jin was supported by the National Key R&D Program of China
   (Grant No. 2017YFB1002600), the National Natural Science Foundation of
   China (Grant No. 61732015), the Ningbo Major Special Projects of the
   "Science and Technology Innovation 2025" (Grant No. 2020Z007), and the
   Key Research and Development Program of Zhejiang Province (Grant No.
   2018C01090).
CR Barnes C, 2010, LECT NOTES COMPUT SC, V6313, P29
   Barnes Connelly, 2017, [Computational Visual Media, 计算可视媒体], V3, P3
   Barnes C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766934
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Chang HW, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766978
   Chen TC, 2017, AGEING SOC, V37, P1798, DOI 10.1017/S0144686X16000623
   Chen X., 2012, Proceedings of Graphics Interface 2012, P131, DOI DOI 10.5555/2305276.2305299
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Cui DL, 2017, COMPUT ANIMAT VIRT W, V28, DOI 10.1002/cav.1725
   Delong A, 2012, INT J COMPUT VISION, V96, P1, DOI 10.1007/s11263-011-0437-z
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Epshtein B, 2010, PROC CVPR IEEE, P2963, DOI 10.1109/CVPR.2010.5540041
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jing YH, 2021, IEEE T CYBERNETICS, V51, P568, DOI 10.1109/TCYB.2019.2904768
   Kyprianidis JE, 2013, IEEE T VIS COMPUT GR, V19, P866, DOI 10.1109/TVCG.2012.160
   Li Xueting, 2018, ARXIV180804537
   Ma C, 2019, MULTIMED TOOLS APPL, V78, P34065, DOI 10.1007/s11042-019-08053-7
   Men YF, 2018, PROC CVPR IEEE, P6353, DOI 10.1109/CVPR.2018.00665
   Qian WH, 2020, INT J PATTERN RECOGN, V34, DOI 10.1142/S0218001420590454
   Qian WH, 2019, MULTIMED TOOLS APPL, V78, P995, DOI 10.1007/s11042-018-6002-9
   Shen QQ, 2017, LECT NOTES COMPUT SC, V10133, P233, DOI 10.1007/978-3-319-51814-5_20
   Yang K., 2012, P 5 INT S VIS INF CO, P87
   Yang KW, 2018, MULTIMED TOOLS APPL, V77, P12259, DOI 10.1007/s11042-017-4882-8
   Yang S, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1688, DOI 10.1145/3240508.3240580
   Yang S, 2017, PROC CVPR IEEE, P2886, DOI 10.1109/CVPR.2017.308
   ZHANG TY, 1984, COMMUN ACM, V27, P236, DOI 10.1145/357994.358023
   Zhao YD, 2015, IEEE T VIS COMPUT GR, V21, P229, DOI 10.1109/TVCG.2014.2355221
   Zhou Y, 2017, COMPUT GRAPH FORUM, V36, P199, DOI 10.1111/cgf.13119
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 33
TC 5
Z9 6
U1 6
U2 32
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2553
EP 2565
DI 10.1007/s00371-021-02216-0
EA JUL 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000670161500001
DA 2024-07-18
ER

PT J
AU Morikawa, C
   Kobayashi, M
   Satoh, M
   Kuroda, Y
   Inomata, T
   Matsuo, H
   Miura, T
   Hilaga, M
AF Morikawa, Chamin
   Kobayashi, Michihiro
   Satoh, Masaki
   Kuroda, Yasuhiro
   Inomata, Teppei
   Matsuo, Hitoshi
   Miura, Takeshi
   Hilaga, Masaki
TI Image and video processing on mobile devices: a survey
SO VISUAL COMPUTER
LA English
DT Article
DE Mobile devices; Computer vision; Image processing
AB Image processing and computer vision on mobile devices have a wide range of applications such as digital image enhancement and augmented reality. While images acquired by cameras on mobile devices can be processed with generic image processing algorithms, there are numerous constraints and external issues that call for customized algorithms for such devices. In this paper, we survey mobile image processing and computer vision applications while highlighting these constraints and explaining how the algorithms have been modified/adapted to meet accuracy and performance demands. We hope that this paper will be a useful resource for researchers who intend to apply image processing and computer vision algorithms to real-world scenarios and applications that involve mobile devices.
C1 [Morikawa, Chamin; Kobayashi, Michihiro; Satoh, Masaki; Kuroda, Yasuhiro; Inomata, Teppei; Matsuo, Hitoshi; Miura, Takeshi; Hilaga, Masaki] Morpho Inc, Chiyoda Ku, Tokyo 1010065, Japan.
RP Morikawa, C (corresponding author), Morpho Inc, Chiyoda Ku, Tokyo 1010065, Japan.
EM c-morikawa@morphoinc.com; m-kobayashi@morphoinc.com;
   m-satoh@morphoinc.com; y-kuroda@morphoinc.com; t-inomata@morphoinc.com;
   h-matsuo@morphoinc.com; tmiura@morphoinc.com; m-hilaga@morphoinc.com
CR Adams MD, 2002, IEEE SENS J, V2, P2, DOI 10.1109/7361.987055
   Angulu R, 2018, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-018-0278-6
   [Anonymous], 2017, INSIDER MAGAZINE POP
   [Anonymous], 2013, DIGITALTRENDS COMPLE
   [Anonymous], Microsoft - Windows Blogs: Kinect for Windows SDK 1.8 Available With Expanded Capabilities"
   [Anonymous], 2019, APPLE INSIDER HANDS
   [Anonymous], 2020, HIGHER INTELLECT VIN
   [Anonymous], 2009, INT C COMP PHOT
   Apple Inc, 2020, APPL UNV ALL NEW IPA
   ARM Inc, 2017, ARM COMP LIB
   Ba J. L., 2016, LAYER NORMALIZATION, DOI DOI 10.48550/ARXIV.1607.06450
   Bartczak B, 2009, LECT NOTES COMPUT SC, V5876, P228, DOI 10.1007/978-3-642-10520-3_21
   Bebeselea-Sterp Elena., 2017, International Journal of Advanced Computer Science and Applications, V8, DOI [DOI 10.14569/IJACSA.2017.081144, 10.14569/IJACSA.2017.081144]
   Bleiweiss A., 2010, 2010 IEEE 12th International Workshop on Multimedia Signal Processing (MMSP), P116, DOI 10.1109/MMSP.2010.5662004
   Bochkovskiy A., 2020, PREPRINT
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dean J., 2015, NIPS DEEP LEARNING R
   Debevec Paul E, 2008, ACM SIGGRAPH 2008 CL, P1, DOI DOI 10.1145/1401132.1401174
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   DeVries T, 2017, PREPRINT
   Dew P., 1988, DIGITALTRENDS COMPLE, V22, P346, DOI 10.1145/54852.378545
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Eigen D., 2014, ADV NEUR IN
   Endo K, 2007, INT J JPN SOCIOL, V16, P115, DOI 10.1111/j.1475-6781.2007.00103.x
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fairchild MD, 2007, FIFTEENTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, AND APPLICATIONS, FINAL PROGRAM AND PROCEEDINGS, P233
   Felzenszwalb P, 2008, PROC CVPR IEEE, P1984
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Georgescu MD, 2003, 2003 IEEE PACIFIC RIM CONFERENCE ON COMMUNICATIONS, COMPUTERS, AND SIGNAL PROCESSING, VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P638
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Gonzalez R.C., 2018, Digital Image Processing
   Goyal B, 2020, INFORM FUSION, V55, P220, DOI 10.1016/j.inffus.2019.09.003
   Han S., 2016, ARXIV151000149
   Han S, 2015, ADV NEUR IN, V28
   Hasinoff SW, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980254
   Haskell B., 2004, Portable electronics product design and development
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hemayed EE, 2003, IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P351, DOI 10.1109/AVSS.2003.1217942
   Hjelmås E, 2001, COMPUT VIS IMAGE UND, V83, P236, DOI 10.1006/cviu.2001.0921
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hubara I, 2018, J MACH LEARN RES, V18
   Inc. G., 2018, TENSORFLOW LITE ML M
   Intel Inc, 2019, BEG GUID DEPTH
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Kartynnik Yury., 2019, Real-time Facial Surface Geometry from Monocular Video on Mobile GPUs, arXiv
   Kavya N. P., 2012, INT J COMPUTER APPL, V58, P31, DOI 10.5120/9304-3526
   Khabarlak K., 2021, FAST FACIAL LANDMARK
   Khairy M, 2019, J PARALLEL DISTR COM, V127, P65, DOI 10.1016/j.jpdc.2018.11.012
   Kim Y.D, 2016, P 4 INT C LEARN REPR
   Kirillov A, 2019, PROC CVPR IEEE, P9396, DOI 10.1109/CVPR.2019.00963
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar J, 2015, PROCEDIA COMPUT SCI, V58, P486, DOI 10.1016/j.procs.2015.08.011
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lipkin L., 1970, LECT NOTES COMPUT SC, V169, P166, DOI [10.1126/science.169.3941.166, DOI 10.1007/978-3-319-46448-0_2]
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Z., Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mahjourian R, 2018, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2018.00594
   Milani S, 2016, SIGNAL PROCESS-IMAGE, V41, P28, DOI 10.1016/j.image.2015.11.008
   Morpho Inc, 2020, MORPH TECHN
   Morpho Inc, 2018, FEAT TECHN SOFTNEURO
   Morpho Inc, 2020, FEAT TECHN SEM FILT
   Nair Rahul, 2013, Time-Of-Flight and Depth Imaging. Sensors, Algorithms and Applications. Dagstuhl 2012 Seminar on Time-of-Flight Imaging and GCPR 2013 Workshop on Imaging New Modalities: LNCS 8200, P105, DOI 10.1007/978-3-642-44964-2_6
   Narioka K, 2018, IEEE INT VEH SYM, P132, DOI 10.1109/IVS.2018.8500397
   Neumann J.V., 1945, 1 DRAFT REPORT EDVAC
   Ng R, 2005, RES REPORT, DOI DOI 10.1145/3097571
   Park J, 2019, ASIAPAC SIGN INFO PR, P1970, DOI [10.1109/apsipaasc47483.2019.9023270, 10.1109/APSIPAASC47483.2019.9023270]
   Poulose M., 2013, Int. J. Comp. Appl. Technol. Res., V3, P286
   PratapSingh Mahendra., 2014, International Journal of Computer Applications, V90, P34, DOI [DOI 10.5120/15564-4339, 10.5120/15564-4339]
   Qualcomm Inc, 2020, MOB AI ON DEV AL QUA
   Qualcomm Inc, 2016, QUALC NEUR PROC SDK
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shinagawa Y, 1998, IEEE T PATTERN ANAL, V20, P994, DOI 10.1109/34.713364
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tan M., 2020, INT C MACH LEARN, DOI DOI 10.48550/ARXIV.1905.11946
   Tao A., 2020, Arxiv
   Tico M., 2007, IMAGE VISION COMPUT, V1, pI, DOI [10.1109/ICASSP.2007.365970, DOI 10.1016/J.IMAVIS.2006.02.026]
   van Ouwerkerk JD, 2006, IMAGE VISION COMPUT, V24, P1039, DOI 10.1016/j.imavis.2006.02.026
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wan L, 2013, P 30 INT C MACH LEAR, P1058, DOI DOI 10.5555/3042817.3043055
   Wang X., 2020, ARXIV200310152 CORR
   Wikipedia, 2015, DULM MAGN
   Xu Y, 2019, IEEE I CONF COMP VIS, P2811, DOI 10.1109/ICCV.2019.00290
   Yun-Ta T., 2020, BLOG POSTGOOGLE BLOG
   Zhang H., 2018, P ICLR 2018
   Zhang L, 2017, IEEE T CIRC SYST VID, V27, P225, DOI 10.1109/TCSVT.2015.2501941
   Zhang YQ, 2016, J SENSORS, V2016, DOI 10.1155/2016/2802343
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zoph B., 2017, ICLR, P1, DOI DOI 10.1109/ICAIIC48513.2020.9065031
NR 106
TC 22
Z9 22
U1 3
U2 43
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2021
VL 37
IS 12
SI SI
BP 2931
EP 2949
DI 10.1007/s00371-021-02200-8
EA JUN 2021
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP1DP
UT WOS:000663983600001
PM 34177023
OA Bronze, Green Published
DA 2024-07-18
ER

PT J
AU Wang, SB
   Xiang, N
   Xia, Y
   You, LH
   Zhang, JJ
AF Wang, Shuangbu
   Xiang, Nan
   Xia, Yu
   You, Lihua
   Zhang, Jianjun
TI Real-time surface manipulation with <i>C</i><SUP>1</SUP> continuity
   through simple and efficient physics-based deformations
SO VISUAL COMPUTER
LA English
DT Article
DE Surface manipulation; Physics-based deformations; Partial differential
   equation; User interface; Mapping
ID FREE-FORM DEFORMATION; NURBS; ANIMATION; SHAPE
AB We present a novel but simple physics-based method to interactively manipulate surface shapes of 3D models with C1 continuity in real time. A fourth-order partial differential equation involving a sculpting force originating from elastic bending of thin plates is proposed to define physics-based deformations and achieve C1 continuity at the boundary of deformation regions. In order to obtain real-time physics-based surface manipulation, we construct a mapping relationship between a deformation region in a 3D coordinate space and a unit circle on a 2D parametric plane, formulate corresponding C1 continuous boundary conditions for the unit circle, and obtain a simple analytical solution to describe the physics-based deformation in the unit circle caused by a sculpting force. After that, the obtained physics-based deformation is mapped back to the 3D coordinate space, and added to the original surface to create a new surface shape with C1 continuity at the boundary of the deformation region. We also develop an interactive user interface as a plug-in of the 3D modelling software package Maya to achieve real-time surface manipulation. The effectiveness, easiness, real-time performance, and better realism of our proposed method is demonstrated by testing surface deformations on several 3D models and comparing with other methods and ground-truth deformations.
C1 [Wang, Shuangbu; Xiang, Nan; Xia, Yu; You, Lihua; Zhang, Jianjun] Bournemouth Univ, Natl Ctr Comp Animat, Poole, Dorset, England.
C3 Bournemouth University
RP Xia, Y (corresponding author), Bournemouth Univ, Natl Ctr Comp Animat, Poole, Dorset, England.
EM yxia@bournemouth.ac.uk
FU European Union [778035]; Marie Curie Actions (MSCA) [778035] Funding
   Source: Marie Curie Actions (MSCA)
FX This research is supported by the PDE-GIR project which has received
   funding from the European Union's Horizon 2020 research and innovation
   programme under the Marie Skodowska-Curie grant agreement No. 778035.
CR Alexa M, 2003, VISUAL COMPUT, V19, P105, DOI 10.1007/s00371-002-0180-0
   [Anonymous], 1959, THEORY PLATES SHELLS
   Barr A.H., 1987, READINGS COMPUTER VI, VVolume 1, P661
   BLOOR MIG, 1990, COMPUT AIDED DESIGN, V22, P202, DOI 10.1016/0010-4485(90)90049-I
   BLOOR MIG, 1994, COMPUT GRAPH, V18, P161, DOI 10.1016/0097-8493(94)90090-6
   Brown JM, 1998, COMPUT METHOD APPL M, V158, P221, DOI 10.1016/S0045-7825(98)00252-7
   CELNIKER G, 1991, COMP GRAPH, V25, P257, DOI 10.1145/127719.122746
   Coquillart S., 1990, Computer Graphics, V24, P187, DOI 10.1145/97880.97900
   Du HX, 2005, GRAPH MODELS, V67, P43, DOI 10.1016/j.gmod.2004.06.002
   Du HX, 2000, EIGHTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P213, DOI 10.1109/PCCGA.2000.883943
   EDELSBRUNNER H, 1983, IEEE T INFORM THEORY, V29, P551, DOI 10.1109/TIT.1983.1056714
   Fong C., 2015, ARXIV PREPRINT ARXIV
   GUDUKBAY U, 1994, COMPUT AIDED DESIGN, V26, P868, DOI 10.1016/0010-4485(94)90051-5
   Hirota G, 2000, COMPUT AIDED DESIGN, V32, P499, DOI 10.1016/S0010-4485(00)00038-5
   HSU WM, 1992, COMP GRAPH, V26, P177, DOI 10.1145/142920.134036
   Kalra P., 1992, Computer Graphics Forum, V11, pC59, DOI 10.1111/1467-8659.1130059
   LAMOUSIN HJ, 1994, IEEE COMPUT GRAPH, V14, P59, DOI 10.1109/38.329096
   Lipman Y, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P181, DOI 10.1109/SMI.2004.1314505
   LOWE TW, 1990, COMPUT AIDED DESIGN, V22, P655, DOI 10.1016/0010-4485(90)90012-2
   Mancewicz Joe., 2014, Proceedings of the Fourth Symposium on Digital Production. DigiPro'14, P7, DOI DOI 10.1145/2633374.2633376
   McDonnell KT, 2007, VISUAL COMPUT, V23, P285, DOI 10.1007/s00371-007-0096-9
   Nealen A, 2006, COMPUT GRAPH FORUM, V25, P809, DOI 10.1111/j.1467-8659.2006.01000.x
   Piegl L., 2012, The NURBS book
   QIN H, 1995, COMPUT AIDED DESIGN, V27, P111, DOI 10.1016/0010-4485(95)92151-H
   Qin H, 1997, COMPUT AIDED GEOM D, V14, P325, DOI 10.1016/S0167-8396(96)00062-3
   Sederberg T. W., 1986, Computer Graphics, V20, P151, DOI 10.1145/15886.15903
   Sheffer A, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000011
   Song WH, 2005, VISUAL COMPUT, V21, P139, DOI 10.1007/s00371-004-0277-8
   Sorkine O., 2004, P 2004 EUR ACM SIGGR, P179
   Terzopoulos D., 1988, Computer Graphics, V22, P269, DOI 10.1145/378456.378522
   TERZOPOULOS D, 1994, ACM T GRAPHIC, V13, P103, DOI 10.1145/176579.176580
   Terzopoulos D., 1988, Visual Computer, V4, P306, DOI 10.1007/BF01908877
   Terzopoulos D., 1987, COMPUT GRAPH, P205, DOI DOI 10.1145/37402.37427
   Ugail H, 1999, ACM T GRAPHIC, V18, P195, DOI 10.1145/318009.318078
   Vassilev TI, 1997, COMPUT GRAPH FORUM, V16, P191, DOI 10.1111/1467-8659.00179
   Wang SB, 2019, OPTIM ENG, V20, P907, DOI 10.1007/s11081-019-09425-6
   You LH, 2003, IEEE T SYST MAN CY B, V33, P616, DOI 10.1109/TSMCB.2003.814283
   You LH, 2020, MULTIMED TOOLS APPL, V79, P23161, DOI 10.1007/s11042-020-09060-9
   Zhang HJ, 2002, COMPUT GRAPH-UK, V26, P89, DOI 10.1016/S0097-8493(01)00160-1
   Zhang JianJ, 2006, GEOMETRIC MODELING I, P15
   Zhang JJ, 2004, COMPUT GRAPH FORUM, V23, P311, DOI 10.1111/j.1467-8659.2004.00762.x
   Zhang YZ, 2020, COMPUT GRAPH-UK, V89, P167, DOI 10.1016/j.cag.2020.05.013
   Zheng JM, 2003, INT J ADV MANUF TECH, V22, P54, DOI 10.1007/s00170-002-1442-8
   Zhou K, 2005, ACM T GRAPHIC, V24, P496, DOI 10.1145/1073204.1073219
NR 44
TC 4
Z9 4
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2741
EP 2753
DI 10.1007/s00371-021-02169-4
EA JUN 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000662812100001
OA hybrid
DA 2024-07-18
ER

PT J
AU Bayoudh, K
   Knani, R
   Hamdaoui, F
   Mtibaa, A
AF Bayoudh, Khaled
   Knani, Raja
   Hamdaoui, Faycal
   Mtibaa, Abdellatif
TI A survey on deep multimodal learning for computer vision: advances,
   trends, applications, and datasets
SO VISUAL COMPUTER
LA English
DT Article
DE Applications; Computer vision; Datasets; Deep learning; Sensory
   modalities; Multimodal learning
ID NEURAL-NETWORKS; IMAGE; FUSION; ALGORITHM; PERCEPTION; GRADIENT; INDOOR;
   MODELS
AB The research progress in multimodal learning has grown rapidly over the last decade in several areas, especially in computer vision. The growing potential of multimodal data streams and deep learning algorithms has contributed to the increasing universality of deep multimodal learning. This involves the development of models capable of processing and analyzing the multimodal information uniformly. Unstructured real-world data can inherently take many forms, also known as modalities, often including visual and textual content. Extracting relevant patterns from this kind of data is still a motivating goal for researchers in deep learning. In this paper, we seek to improve the understanding of key concepts and algorithms of deep multimodal learning for the computer vision community by exploring how to generate deep models that consider the integration and combination of heterogeneous visual cues across sensory modalities. In particular, we summarize six perspectives from the current literature on deep multimodal learning, namely: multimodal data representation, multimodal fusion (i.e., both traditional and deep learning-based schemes), multitask learning, multimodal alignment, multimodal transfer learning, and zero-shot learning. We also survey current multimodal applications and present a collection of benchmark datasets for solving problems in various vision domains. Finally, we highlight the limitations and challenges of deep multimodal learning and provide insights and directions for future research.
C1 [Bayoudh, Khaled; Mtibaa, Abdellatif] Univ Monastir, Fac Sci Monastir FSM, Lab Elect & Microelect LR99ES30, Elect Dept,Natl Engn Sch Monastir ENIM, Monastir, Tunisia.
   [Knani, Raja] Univ Monastir, Fac Sci Monastir FSM, Phys Dept, Lab Elect & Microelect LR99ES30, Monastir, Tunisia.
   [Hamdaoui, Faycal] Univ Monastir, Natl Engn Sch Monastir, Lab Control Elect Syst & Environm LASEE, Elect Dept,Natl Engn Sch Monastir ENIM, Monastir, Tunisia.
C3 Universite de Monastir; Universite de Monastir; Universite de Monastir
RP Bayoudh, K (corresponding author), Univ Monastir, Fac Sci Monastir FSM, Lab Elect & Microelect LR99ES30, Elect Dept,Natl Engn Sch Monastir ENIM, Monastir, Tunisia.
EM khaled.isimm@gmail.com; knani.raja@gmail.com; faycel_hamdaoui@yahoo.fr;
   abdellatif.mtibaa@enim.rnu.tn
RI Hamdaoui, Fayçal/FCM-9090-2022; Hamdaoui, Fayçal/KAL-8389-2024; Bayoudh,
   Khaled/AAZ-7157-2020
OI Hamdaoui, Fayçal/0000-0002-7278-898X; Hamdaoui,
   Fayçal/0000-0002-7278-898X; Bayoudh, Khaled/0000-0002-1148-4800; MTIBAA,
   Abdellatif/0000-0001-5180-9975
CR Abavisani M, 2019, PROC CVPR IEEE, P1165, DOI 10.1109/CVPR.2019.00126
   Abbass MY, 2021, VISUAL COMPUT, V37, P993, DOI 10.1007/s00371-020-01848-y
   Abbass MY, 2021, VISUAL COMPUT, V37, P831, DOI 10.1007/s00371-020-01833-5
   Abdulnabi AH, 2018, IEEE T MULTIMEDIA, V20, P1656, DOI 10.1109/TMM.2017.2774007
   Aiolli F, 2015, NEUROCOMPUTING, V169, P215, DOI 10.1016/j.neucom.2014.11.078
   Ammour B, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9010085
   Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   [Anonymous], 2015, 2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA), Paris, DOI 10.1109/iisa.2015.7387997
   [Anonymous], 2016, ARXIV160702565
   [Anonymous], 2020, DEEPARTS TURN YOUR P
   [Anonymous], PAMI
   [Anonymous], COMPUT VIS PATTERN R
   [Anonymous], 2018, BRIT MACH VIS C
   Asvadi A, 2018, PATTERN RECOGN LETT, V115, P20, DOI 10.1016/j.patrec.2017.09.038
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Bank D., 2020, arXiv preprint arXiv:2003.05991, DOI DOI 10.48550/ARXIV.2003.05991
   Bashiri FS, 2019, J IMAGING, V5, DOI 10.3390/jimaging5010005
   Basly H, 2022, VISUAL COMPUT, V38, P993, DOI 10.1007/s00371-021-02064-y
   Bayoudh K, 2020, PHYS ENG SCI MED, V43, P1415, DOI 10.1007/s13246-020-00957-1
   Bayoudh K, 2021, APPL INTELL, V51, P124, DOI 10.1007/s10489-020-01801-5
   Bayoudh Khaled., 2017, From Machine Learning To Deep Learning, V1st ed.
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Benson AM, 2017, ROUTL RES SPORT CULT, P1
   Bhatt G, 2019, PATTERN RECOGN, V95, P12, DOI 10.1016/j.patcog.2019.05.032
   Bilbao Imanol, 2017, 2017 Eighth International Conference on Intelligent Computing and Information Systems (ICICIS). Proceedings, P173, DOI 10.1109/INTELCIS.2017.8260032
   Boxiao Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10867, DOI 10.1109/CVPR42600.2020.01088
   Bucci S., 2018, ARXIV180711697
   BUTLER D, 2012, LECT NOTES COMPUT SC, V2012, P611
   CADENE E, 2019, PROC CVPR IEEE, P1989
   Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164
   Cai ZW, 2021, IEEE T PATTERN ANAL, V43, P1483, DOI 10.1109/TPAMI.2019.2956516
   Cangea, 2019, ARXIV190804950
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Chandar S, 2016, NEURAL COMPUT, V28, P257, DOI 10.1162/NECO_a_00801
   Chen CQ, 2019, PROC CVPR IEEE, P627, DOI 10.1109/CVPR.2019.00072
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Choi Sungjoon., 2016, CoRR
   Ciaparrone G, 2020, NEUROCOMPUTING, V381, P61, DOI 10.1016/j.neucom.2019.11.023
   Couprie C., 2014, The Journal of Machine Learning Research
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   Dapogny A, 2020, IEEE INT CONF AUTOMA, P192, DOI 10.1109/FG47880.2020.00038
   Dargan S, 2020, EXPERT SYST APPL, V143, DOI 10.1016/j.eswa.2019.113114
   Das A, 2018, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2018.00008
   Dilawari A, 2019, IEEE ACCESS, V7, P29253, DOI 10.1109/ACCESS.2019.2902507
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Djuric Nemanja, 2020, arXiv
   Duan GY, 2011, PHYSCS PROC, V22, P471, DOI 10.1016/j.phpro.2011.11.073
   Duong L, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL) AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (IJCNLP), VOL 2, P845
   Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54
   Engilberge M, 2018, PROC CVPR IEEE, P3984, DOI 10.1109/CVPR.2018.00419
   Escalera S, 2016, J MACH LEARN RES, V17
   Fan CY, 2019, PROC CVPR IEEE, P1999, DOI 10.1109/CVPR.2019.00210
   Fan X, 2022, VISUAL COMPUT, V38, P279, DOI 10.1007/s00371-020-02015-z
   Biten AF, 2019, PROC CVPR IEEE, P12458, DOI 10.1109/CVPR.2019.01275
   Gao ML, 2019, IEEE ACCESS, V7, P43110, DOI 10.1109/ACCESS.2019.2907071
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Ghahramani Z, 1997, MACH LEARN, V29, P245, DOI 10.1023/A:1007425814087
   Girshick R., 2015, IEEE I CONF COMP VIS, DOI [DOI 10.1109/ICCV.2015.169, 10.1109/ICCV.2015.169]
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Gönen M, 2011, J MACH LEARN RES, V12, P2211
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Grigorescu S, 2020, J FIELD ROBOT, V37, P362, DOI 10.1002/rob.21918
   Guan SY, 2018, CHIN J MECH ENG-EN, V31, DOI 10.1186/s10033-018-0275-9
   Guclu O, 2020, VISUAL COMPUT, V36, P1271, DOI 10.1007/s00371-019-01720-8
   Guo WZ, 2019, IEEE ACCESS, V7, P63373, DOI 10.1109/ACCESS.2019.2916887
   Guo YM, 2018, INT J MULTIMED INF R, V7, P87, DOI 10.1007/s13735-017-0141-z
   Guo YM, 2016, NEUROCOMPUTING, V187, P27, DOI 10.1016/j.neucom.2015.09.116
   Handa A., 2015, Scenenet: Understanding real world indoor scenes with synthetic data. arXiv
   Hao FS, 2019, IEEE I CONF COMP VIS, P8459, DOI 10.1109/ICCV.2019.00855
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   HASCOET T, 2019, J IMAGE VIDEO PROC, V2019, P13, DOI DOI 10.1186/S13640-018-0371-X
   Hatcher WG, 2018, IEEE ACCESS, V6, P24411, DOI 10.1109/ACCESS.2018.2830661
   He G., 2020, P CVPR WORKSH, P964
   He K., 2018, ARXIV170306870
   He M, 2020, VISUAL COMPUT, V36, P1053, DOI 10.1007/s00371-019-01714-6
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hori C, 2017, IEEE I CONF COMP VIS, P4203, DOI 10.1109/ICCV.2017.450
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Horry MJ, 2020, IEEE ACCESS, V8, P149808, DOI 10.1109/ACCESS.2020.3016780
   Hosseinabad SH, 2021, VISUAL COMPUT, V37, P119, DOI 10.1007/s00371-019-01786-4
   Huang X, 2021, VISUAL COMPUT, V37, P95, DOI 10.1007/s00371-020-01982-7
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang Y, 2016, Adv Inform Managemen, P1519, DOI 10.1109/IMCEC.2016.7867471
   Isola P., 2018, P IEEE C COMPUTER VI
   Jang Y, 2019, INT J COMPUT VISION, V127, P1385, DOI 10.1007/s11263-019-01189-x
   Jia XK, 2022, VISUAL COMPUT, V38, P963, DOI 10.1007/s00371-021-02061-1
   Jiao LC, 2019, IEEE ACCESS, V7, P128837, DOI 10.1109/ACCESS.2019.2939201
   Jin Xin, 2020, ARXIV200612009
   Jing, 2018, ARXIV170504058
   Kandylakis Z, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11040446
   Kay W., 2017, ARXIV170506950
   Khan A, 2020, ARTIF INTELL REV, V53, P5455, DOI 10.1007/s10462-020-09825-6
   Kim JC, 2020, IEEE ACCESS, V8, P104933, DOI 10.1109/ACCESS.2020.2997255
   Koo JH, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18093040
   Krantz, 2020, ARXIV200402857
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar Y, 2020, AAAI CONF ARTIF INTE, V34, P2645
   Lafferty John, 2001, INT C MACH LEARN ICM
   Lahat D, 2015, P IEEE, V103, P1449, DOI 10.1109/JPROC.2015.2460697
   Lai K, 2011, IEEE INT CONF ROBOT, P1817
   Laina I, 2019, IEEE I CONF COMP VIS, P7413, DOI 10.1109/ICCV.2019.00751
   Lan XY, 2020, PATTERN RECOGN LETT, V130, P12, DOI 10.1016/j.patrec.2018.10.002
   Landi Federico, 2020, ARXIV191112377
   Lawrence S, 2000, IEEE IJCNN, P114, DOI 10.1109/IJCNN.2000.857823
   Le, 2020, ARXIV200210698
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee JA, 2019, IEEE I CONF COMP VIS, P5076, DOI 10.1109/ICCV.2019.00518
   Li JX, 2017, PROCEDIA COMPUT SCI, V107, P135, DOI 10.1016/j.procs.2017.03.069
   Li K, 2019, IEEE I CONF COMP VIS, P3582, DOI 10.1109/ICCV.2019.00368
   Li S, 2018, PROC CVPR IEEE, P5457, DOI 10.1109/CVPR.2018.00572
   Li XL, 2018, MULTIMED TOOLS APPL, V77, P29847, DOI 10.1007/s11042-018-5856-1
   Lian GY, 2020, VISUAL COMPUT, V36, P799, DOI 10.1007/s00371-019-01661-2
   Libovicky J, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 2, P196, DOI 10.18653/v1/P17-2031
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2015, PROC CVPR IEEE, P5007, DOI 10.1109/CVPR.2015.7299135
   LINDE Y, 1980, IEEE T COMMUN, V28, P84, DOI 10.1109/TCOM.1980.1094577
   Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
   Liu S, 2020, OPT ENG, V59, DOI 10.1117/1.OE.59.5.053103
   Liu SY, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19204494
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu XX, 2019, VISUAL COMPUT, V35, P445, DOI 10.1007/s00371-018-1566-y
   Liu YN, 2016, SIGNAL PROCESS, V120, P761, DOI 10.1016/j.sigpro.2015.01.001
   Liu Y, 2019, IEEE I CONF COMP VIS, P6697, DOI 10.1109/ICCV.2019.00680
   Liu Y, 2020, IEEE T IMAGE PROCESS, V29, P4788, DOI 10.1109/TIP.2020.2975980
   Liu ZY, 2021, VISUAL COMPUT, V37, P529, DOI 10.1007/s00371-020-01821-9
   Liu ZW, 2019, PROC CVPR IEEE, P3462, DOI 10.1109/CVPR.2019.00358
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Longteng Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10324, DOI 10.1109/CVPR42600.2020.01034
   Luo QH, 2020, NEUROCOMPUTING, V378, P364, DOI 10.1016/j.neucom.2019.10.025
   Ma L, 2015, IEEE I CONF COMP VIS, P2623, DOI 10.1109/ICCV.2015.301
   Ma TS, 2021, VISUAL COMPUT, V37, P925, DOI 10.1007/s00371-020-01843-3
   Madhuranga D, 2021, VISUAL COMPUT, V37, P1263, DOI 10.1007/s00371-020-01864-y
   Mahmud T, 2021, IEEE SENS J, V21, P1715, DOI 10.1109/JSEN.2020.3015781
   Marechal Catherine, 2019, High-Performance Modelling and Simulation for Big Data Applications: Selected Results of the COST Action IC1406 cHiPSet. Lecture Notes in Computer Science (LNCS 11400), P307, DOI 10.1007/978-3-030-16272-6_11
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Metzger A, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-50835-4
   Miao QG, 2017, IEEE INT CONF COMP V, P3047, DOI 10.1109/ICCVW.2017.360
   Michael YB., 2019, MULTIMODAL SCENE UND
   Mohanapriya D., 2020, COGNITIVE APPROACH C, P65
   Morvant E, 2014, LECT NOTES COMPUT SC, V8621, P153, DOI 10.1007/978-3-662-44415-3_16
   Mur- Artal Raul, 2017, IEEE T ROBOT, DOI [DOI 10.1109/TR0.2017.2705103, DOI 10.1109/TRO.2017.2705103]
   Namin ST, 2015, IEEE I CONF COMP VIS, P1188, DOI 10.1109/ICCV.2015.141
   Nguyen K, 2019, PROC CVPR IEEE, P12519, DOI 10.1109/CVPR.2019.01281
   Nishida N, 2016, LECT NOTES COMPUT SC, V9431, P682, DOI 10.1007/978-3-319-29451-3_54
   Nowlan S. J., 1995, Advances in Neural Information Processing Systems 7, P901
   Ophoff T, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19040866
   Palaskar S, 2020, COMPUT SPEECH LANG, V64, DOI 10.1016/j.csl.2020.101093
   Parida KK, 2020, IEEE WINT CONF APPL, P3240, DOI [10.1109/wacv45572.2020.9093438, 10.1109/WACV45572.2020.9093438]
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Peri D, 2019, IEEE IMAGE PROC, P295, DOI [10.1109/ICIP.2019.8802922, 10.1109/icip.2019.8802922]
   Pouyanfar S, 2018, ACM COMPUT SURV, V51, DOI 10.1145/3150226
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Radu V., 2018, PROC ACM INTERACT MO, V1, P1
   Ramachandram D, 2017, IEEE SIGNAL PROC MAG, V34, P96, DOI 10.1109/MSP.2017.2738401
   Ramanath, 2020, ARXIV190402874, V4, P37
   Rangesh A, 2019, IEEE T INTELL VEHICL, V4, P588, DOI 10.1109/TIV.2019.2938110
   Rao R., 2020, CVPR, P956
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rohith G, 2021, VISUAL COMPUT, V37, P1965, DOI 10.1007/s00371-020-01957-8
   ROSENBLATT F, 1960, P IRE, V48, P301, DOI 10.1109/JRPROC.1960.287598
   Ruder S., 2018, ARXIV170508142
   Ruder S, 2017, arXiv preprint arXiv:1706.05098
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Salakhutdinov R., 2009, ARTIFICIAL INTELLIGE, V5, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Sangkloy P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925954
   Sano A, 2019, IEEE J BIOMED HEALTH, V23, P1607, DOI 10.1109/JBHI.2018.2867619
   Santosh KC, 2020, J MED SYST, V44, DOI 10.1007/s10916-020-01562-1
   Savian S., 2020, Deep Biometrics, P257
   Scupáková K, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-38914-y
   Shamwell EJ, 2020, IEEE T PATTERN ANAL, V42, P2478, DOI 10.1109/TPAMI.2019.2909895
   Shao L, 2015, IEEE T NEUR NET LEAR, V26, P1019, DOI 10.1109/TNNLS.2014.2330900
   Sheela KG, 2013, MATH PROBL ENG, V2013, DOI 10.1155/2013/425740
   Shi F, 2021, IEEE REV BIOMED ENG, V14, P4, DOI 10.1109/RBME.2020.2987975
   Shu Y., 2018, INTERNET MULTIMEDIA, P75
   Silberman N., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P601, DOI 10.1109/ICCVW.2011.6130298
   Simon M, 2019, IEEE COMPUT SOC CONF, P1190, DOI 10.1109/CVPRW.2019.00158
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh A, 2014, IEEE INT CONF ROBOT, P509, DOI 10.1109/ICRA.2014.6906903
   Singh R, 2020, MULTIMEDIA SYST, V26, P313, DOI 10.1007/s00530-019-00645-5
   Singh Vikas., 2019, Computational Intelligence: Theories, Applications and Future Directions-Volume, VI, P53
   Song HT, 2021, VISUAL COMPUT, V37, P2285, DOI 10.1007/s00371-020-01986-3
   Spinello L, 2011, IEEE INT C INT ROBOT, P3838, DOI 10.1109/IROS.2011.6048835
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stone J. V., 2004, Independent Component Analysis: A Tutorial Introduction, P129
   Thoma M., 2016, SURVEY SEMANTIC SEGM
   Tombari F., 2011, P IEEE RSJ INT C INT, P4857
   Toriya H, 2019, INT GEOSCI REMOTE SE, P923, DOI [10.1109/igarss.2019.8898605, 10.1109/IGARSS.2019.8898605]
   Tran Du, 2017, ARXIV170805038
   Tu SQ, 2020, PRECIS AGRIC, V21, P1072, DOI 10.1007/s11119-020-09709-3
   Turkoglu MO, 2019, AAAI CONF ARTIF INTE, P8901
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Vaezi Joze Hamid Reza, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13286, DOI 10.1109/CVPR42600.2020.01330
   Van Brummelen J, 2018, TRANSPORT RES C-EMER, V89, P384, DOI 10.1016/j.trc.2018.02.012
   Van DerMalsburg C, 1986, BRAIN THEORY, P245, DOI DOI 10.1007/978-3-642-70911-1_20
   Van Gael Jurgen., 2008, NIPS 2008, Advances in Neural Information Processing Systems, V21, P1697
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Vo N, 2019, PROC CVPR IEEE, P6432, DOI 10.1109/CVPR.2019.00660
   WANG A, 2014, LECT NOTES COMPUT SC, P453
   Wang BR, 2019, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2019.00273
   Wang L, 2020, VISUAL COMPUT, V36, P317, DOI 10.1007/s00371-018-1609-4
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang M., 2019, ARXIV180406655
   Wang W, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3293318
   Wang WF, 2020, IEEE ACCESS, V8, P36776, DOI 10.1109/ACCESS.2020.2975640
   Wang XF, 2020, IEEE COMMUN SURV TUT, V22, P869, DOI 10.1109/COMST.2020.2970550
   Wang X, 2019, PROC CVPR IEEE, P3622, DOI 10.1109/CVPR.2019.00679
   Wang X, 2017, PROC CVPR IEEE, P7178, DOI 10.1109/CVPR.2017.759
   Wang ZJ, 2020, IEEE ACCESS, V8, P2847, DOI 10.1109/ACCESS.2019.2962554
   Waymo, 2020, Waymo safety report Online
   Wei LX, 2021, VISUAL COMPUT, V37, P133, DOI 10.1007/s00371-019-01787-3
   Wen HW, 2017, PATTERN RECOGN, V63, P601, DOI 10.1016/j.patcog.2016.09.039
   Wu MY, 2019, INT CONF ACOUST SPEE, P830, DOI 10.1109/ICASSP.2019.8682377
   Wu X, 2019, ARXIV190803673
   Xi PC, 2020, VISUAL COMPUT, V36, P1869, DOI 10.1007/s00371-019-01775-7
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xiao Yi, 2019, ARXIV190603199
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Xu Y, 2019, LECT NOTES COMPUT SC, V11837, P193, DOI 10.1007/978-3-030-32962-4_18
   Yan PK, 2018, LECT NOTES COMPUT SC, V11046, P197, DOI 10.1007/978-3-030-00919-9_23
   Yang R., 2019, 2019 IEEE INT C IMAG, P3975, DOI DOI 10.1109/ICIP.2019.8803528
   Yang XH, 2021, IEEE T KNOWL DATA EN, V33, P2349, DOI 10.1109/TKDE.2019.2958342
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yu LC, 2019, PROC CVPR IEEE, P6302, DOI 10.1109/CVPR.2019.00647
   Zarbakhsh P, 2020, VISUAL COMPUT, V36, P951, DOI 10.1007/s00371-019-01705-7
   Zhang L, 2010, WIREL NETW MOB COMMU, P1
   Zhang S., 2019, Proceeding of 2nd China Symposium on Cognitive Computing and Hybrid Intelligence (CCHI), P280, DOI [10.1109/CCHI.2019.8901921, DOI 10.1109/CCHI.2019.8901921]
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhang SD, 2020, VISUAL COMPUT, V36, P305, DOI 10.1007/s00371-018-1612-9
   Zhang WW, 2019, IEEE I CONF COMP VIS, P2365, DOI 10.1109/ICCV.2019.00245
   Zhang XQ, 2021, VISUAL COMPUT, V37, P1089, DOI 10.1007/s00371-020-01854-0
   Zhang YB, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P890
   Zhang YL, 2019, IEEE I CONF COMP VIS, P5942, DOI 10.1109/ICCV.2019.00604
   Zhang ZH, 2021, VISUAL COMPUT, V37, P1731, DOI 10.1007/s00371-020-01934-1
   Zhao DX, 2019, NEUROCOMPUTING, V329, P476, DOI 10.1016/j.neucom.2018.11.004
   Zhou T, 2021, COMPUT VIS MEDIA, V7, P37, DOI 10.1007/s41095-020-0199-z
   Zou CH, 2019, INT J COMPUT VISION, V127, P143, DOI 10.1007/s11263-018-1133-z
NR 242
TC 115
Z9 121
U1 59
U2 268
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2022
VL 38
IS 8
BP 2939
EP 2970
DI 10.1007/s00371-021-02166-7
EA JUN 2021
PG 32
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3B2OE
UT WOS:000659773400001
PM 34131356
OA Bronze, Green Published
DA 2024-07-18
ER

PT J
AU Hirasawa, N
   Kanai, T
   Ando, R
AF Hirasawa, Naoyuki
   Kanai, Takashi
   Ando, Ryoichi
TI A flux-interpolated advection scheme for fluid simulation
SO VISUAL COMPUTER
LA English
DT Article
DE Computer graphics; Fluid simulation; Conservative advection; Predictive
   diffusion correction; Flux interpolation
ID SMOKE
AB We propose a new advection scheme for fluid simulation that improves both conservation and numerical diffusion. Our work differs from previous works in that we re-formulate interpolation as cell-face flux of a vector field instantly constructed from a scalar field, rather than a per-point evaluation at back-traced positions. Our novel interpolation method enables excellent preservation of conservative quantities since the sum of flux exactly counteracts on cell faces, which eventually evaluates the boundary-flux of the whole domain. Our method can be implemented as a plug-and-play extension (or a temporary scratchpad) to the conventional semi-Lagrangian scheme; hence, our method naturally inherits all the benefits of semi-Lagrangian schemes and can be seamlessly integrated with existing fluid simulation pipelines together with other (black-boxed) solver components. We conducted numerical experiments to verify the accuracy of our scheme (conservation and the improved numerical diffusion) and compared its qualitative results with state-of-the-art advection schemes that are in heavy use in the production environment such as MacCormack and the WENO6 interpolation.
C1 [Hirasawa, Naoyuki; Kanai, Takashi] Univ Tokyo, Grad Sch Arts & Sci, Meguro Ku, Tokyo 1538902, Japan.
   [Ando, Ryoichi] Natl Inst Informat, Chiyoda Ku, Tokyo 1018430, Japan.
C3 University of Tokyo; Research Organization of Information & Systems
   (ROIS); National Institute of Informatics (NII) - Japan
RP Hirasawa, N (corresponding author), Univ Tokyo, Grad Sch Arts & Sci, Meguro Ku, Tokyo 1538902, Japan.
EM hirasawa@g.ecc.u-tokyo.ac.jp; kanait@acm.org; rand@nii.ac.jp
OI Hirasawa, Naoyuki/0000-0003-2100-2147; Ando, Ryoichi/0000-0002-7899-9091
FU JSPS [18K18060]; Grants-in-Aid for Scientific Research [18K18060]
   Funding Source: KAKEN
FX This study is supported by JSPS Grant-in-Aid for Young Scientists
   (18K18060).
CR Adams B, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276437, 10.1145/1239451.1239499]
   Ando R, 2012, IEEE T VIS COMPUT GR, V18, P1202, DOI 10.1109/TVCG.2012.87
   [Anonymous], 2008, P 2008 ACM SIGGRAPHE
   Blender, 2020, BLEND 3D MOD REND PA
   Bridson R., 2015, Fluid simulation for computer graphics
   Brochu T., 2006, THESIS U BRIT COLUMB
   Brochu T, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778784
   Chentanez Nuttapong., 2012, Proc of sca, P245
   CHORIN AJ, 1968, MATH COMPUT, V22, P745, DOI 10.2307/2004575
   Chuyuan Fu, 2017, ACM Transactions on Graphics, V36, DOI 10.1145/3130800.3130878
   Demidov D, 2020, SOFTW IMPACTS, V6, DOI 10.1016/j.simpa.2020.100037
   Dinev D, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3153420
   Elcott S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1189762.1189766
   Ewing RE, 2001, J COMPUT APPL MATH, V128, P423, DOI 10.1016/S0377-0427(00)00522-7
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Fei Y, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073630
   Hachisuka Toshiya, 2005, ACM SIGGRAPH 2005 PO
   Heo N, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866198
   Ihmsen Markus, 2014, P 35 ANN C EUR ASS C, DOI [10.2312/egst.20141034, DOI 10.2312/EGST.20141034]
   Iske A, 2004, NUMER METH PART D E, V20, P388, DOI 10.1002/num.10100
   Jeschke S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2714572
   Jiang C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766996
   Jiang Chenfanfu, 2016, ACM SIGGRAPH 2016 CO, P1
   Kim B., 2005, P 1 EUROGRAPHICS C N, DOI DOI 10.2312/NPH/NPH05/051-056
   Kim B, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239549
   Kim C, 2016, ASME FLUID ENG DIV
   Kim D, 2008, COMPUT GRAPH FORUM, V27, P467, DOI 10.1111/j.1467-8659.2008.01144.x
   Kim T., 2008, ACM SIGGRAPH 2008 PA
   Kugelstadt Tassilo, 2019, IEEE COMPUT ARCHIT L, V01, P1
   Kwatra N, 2010, P 2010 ACM SIGGRAPH
   Lentine Michael., 2011, Proceedings of the 2011 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '11, P91, DOI [10.1145/2019406.2019419, DOI 10.1145/2019406.2019419]
   Li GS, 2008, COMPUT GRAPH FORUM, V27, P727, DOI 10.1111/j.1467-8659.2008.01201.x
   Li W, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392400
   Mullen P, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239517, 10.1145/1276377.1276459]
   Mullen P, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531344
   Narain R, 2019, P ACM COMPUT GRAPH, V2, DOI 10.1145/3340257
   Pfaff T, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185608
   Qiu JM, 2011, J COMPUT PHYS, V230, P863, DOI 10.1016/j.jcp.2010.04.037
   Qu ZY, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322945
   Rasmussen N, 2003, ACM T GRAPHIC, V22, P703, DOI 10.1145/882262.882335
   Ren B, 2016, VISUAL COMPUT, V32, P523, DOI 10.1007/s00371-015-1086-y
   Sato Takahiro, 2018, Computational Visual Media, V4, P223, DOI 10.1007/s41095-018-0117-9
   Schechter H., 2008, P 2008 ACM SIGGRAPH
   Selle A, 2005, ACM T GRAPHIC, V24, P910, DOI 10.1145/1073204.1073282
   Selle A, 2008, J SCI COMPUT, V35, P350, DOI 10.1007/s10915-007-9166-4
   Shinar, 2019, ACM SIGGRAPH 2019 CO
   SHU C.- W., 1999, HIGH ORDER METHODS C, V9, P439
   Söderström A, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1857907.1857912
   Solenthaler B, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964976
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Su J, 2013, IEEE T VIS COMPUT GR, V19, P189, DOI 10.1109/TVCG.2012.132
   Sussman M, 2009, SIAM J SCI COMPUT, V31, P2447, DOI 10.1137/080732122
   Takahashi T, 2019, COMPUT GRAPH FORUM, V38, P49, DOI 10.1111/cgf.13618
   Tao M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356543
   Taylor GI, 1937, PROC R SOC LON SER-A, V158, P0499, DOI 10.1098/rspa.1937.0036
   Tessendorf J., 2011, P COMP GRAPH INT WOR
   Weissmann S, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778852
   Wojtan C, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360646
   Wojtan Chris., 2011, ACM SIGGRAPH 2011 CO
   Zehnder J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201324
   Zhang M., 2014, P 13 ACM SIGGRAPH IN, P71
   Zhang XX, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766982
   Zhu B, 2010, COMPUT GRAPH FORUM, V29, P2207, DOI 10.1111/j.1467-8659.2010.01809.x
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
NR 64
TC 0
Z9 0
U1 2
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2607
EP 2618
DI 10.1007/s00371-021-02187-2
EA JUN 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000659773400002
DA 2024-07-18
ER

PT J
AU Pintus, R
   Villanueva, AJ
   Zorcolo, A
   Hadwiger, M
   Gobbetti, E
AF Pintus, Ruggero
   Jaspe Villanueva, Alberto
   Zorcolo, Antonio
   Hadwiger, Markus
   Gobbetti, Enrico
TI A practical and efficient model for intensity calibration of multi-light
   image collections
SO VISUAL COMPUTER
LA English
DT Article
DE Multi-light image collections; Calibration; Light intensity; Shape;
   Material modeling; Cultural heritage
ID UNCALIBRATED PHOTOMETRIC STEREO
AB We present a novel practical and efficient mathematical formulation for light intensity calibration of multi-light image collections (MLICs). Inspired by existing and orthogonal calibration methods, we design a hybrid solution that leverages their strengths while overcoming most of their weaknesses. We combine the rationale of approaches based on fixed analytical models with the interpolation scheme of image domain methods. This allows us to minimize the final residual error in light intensity estimation, without imposing an overly constraining illuminant type. Unlike previous approaches, the proposed calibration strategy proved to be simpler, more efficient and versatile, and extremely adaptable in different setup scenarios. We conduct an extensive analysis and validation of our new light model compared to several state-of-the-art techniques, and we show how the proposed solution provides a more reliable outcomes in terms of accuracy and precision, and a more stable calibration across different light positions/orientations, and with a more general light form factor.
C1 [Pintus, Ruggero; Zorcolo, Antonio; Gobbetti, Enrico] Visual & Data Intens Comp, CRS4, Cagliari, Italy.
   [Jaspe Villanueva, Alberto; Hadwiger, Markus] KAUST, Visual Comp, Thuwal, Saudi Arabia.
C3 King Abdullah University of Science & Technology
RP Pintus, R (corresponding author), Visual & Data Intens Comp, CRS4, Cagliari, Italy.
EM ruggero.pintus@crs4.it; alberto.jaspe@kaust.edu.sa;
   antonio.zorcolo@crs4.it; markus.hadwiger@kaust.edu.sa;
   enrico.gobbetti@crs4.it
RI Pintus, Ruggero/AAX-5719-2020
OI Pintus, Ruggero/0000-0003-1786-7068; Jaspe-Villanueva,
   Alberto/0000-0003-3899-308X; Hadwiger, Markus/0000-0003-1239-4871
FU Sardinian Regional Authorities under grant VIGECLAB; KAUST
FX This research was partially supported by Sardinian Regional Authorities
   under grant VIGECLAB. A. Jaspe Villanueva and M. Hadwiger acknowledge
   the support of KAUST. We thank Fabio Marton for providing the test
   object.
CR Alldrin NG, 2007, PROC CVPR IEEE, P1822
   Angelopoulou ME, 2014, MACH VISION APPL, V25, P1317, DOI 10.1007/s00138-014-0609-2
   [Anonymous], 2013, VMV
   [Anonymous], 2014, BMVC
   Barsky S, 2003, IEEE T PATTERN ANAL, V25, P1239, DOI 10.1109/TPAMI.2003.1233898
   Basri R, 2007, INT J COMPUT VISION, V72, P239, DOI 10.1007/s11263-006-8815-7
   Chen GY, 2019, PROC CVPR IEEE, P8731, DOI 10.1109/CVPR.2019.00894
   Chen GY, 2018, LECT NOTES COMPUT SC, V11213, P3, DOI 10.1007/978-3-030-01240-3_1
   CHI, 2020, CULT HER IM WEBS
   Ciortan I. M., 2016, P GCH, DOI [10.2312/gch.20161396, DOI 10.2312/GCH.20161396]
   Dong Y, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661283
   Gardner A, 2003, ACM T GRAPHIC, V22, P749, DOI 10.1145/882262.882342
   Georgiev GT, 2008, PROC SPIE, V7081, DOI 10.1117/12.795931
   Giachetti A, 2018, COMPUT VIS IMAGE UND, V168, P118, DOI 10.1016/j.cviu.2017.05.014
   Habel R., 2010, Proc. ACM SIGGRAPH Symp. Interactive 3D Graph. and Games, Washington, P189, DOI DOI 10.1145/1730804.1730835
   Huang X, 2015, 2015 DIGITAL HERITAGE INTERNATIONAL CONGRESS, VOL 1: DIGITIZATION & ACQUISITION, COMPUTER GRAPHICS & INTERACTION, P215, DOI 10.1109/DigitalHeritage.2015.7413874
   Ikeuchi K., 1979, 539 AI MIT
   Jiuai Sun, 2013, International Journal of Computer Theory and Engineering, V5, P14, DOI 10.7763/IJCTE.2013.V5.638
   Jung J, 2015, PROC CVPR IEEE, P4521, DOI 10.1109/CVPR.2015.7299082
   Koppal SJ, 2007, IEEE I CONF COMP VIS, P566
   Li B, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P1991
   Lu F, 2013, PROC CVPR IEEE, P1490, DOI 10.1109/CVPR.2013.196
   Ma L, 2019, OPT EXPRESS, V27, P4024, DOI 10.1364/OE.27.004024
   Mecca R, 2014, SIAM J IMAGING SCI, V7, P2732, DOI 10.1137/140968100
   Migita T, 2008, LECT NOTES COMPUT SC, V5304, P412, DOI 10.1007/978-3-540-88690-7_31
   Nimier-David M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356498
   Papadhimitri T, 2014, INT J COMPUT VISION, V107, P139, DOI 10.1007/s11263-013-0665-5
   Pintus R, 2019, COMPUT GRAPH FORUM, V38, P909, DOI 10.1111/cgf.13732
   Pintus R., 2016, P STAG, P143
   Quéau Y, 2018, J MATH IMAGING VIS, V60, P313, DOI 10.1007/s10851-017-0761-1
   Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317
   Sato I, 2007, IEEE I CONF COMP VIS, P1493
   Wong KYK, 2008, LECT NOTES COMPUT SC, V5302, P631, DOI 10.1007/978-3-540-88682-2_48
   WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479
   Xie LM, 2015, OPT LASER ENG, V64, P42, DOI 10.1016/j.optlaseng.2014.07.006
NR 35
TC 1
Z9 1
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2755
EP 2767
DI 10.1007/s00371-021-02172-9
EA JUN 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000658151600001
DA 2024-07-18
ER

PT J
AU Zhang, YZ
   Ong, CC
   Zheng, JM
   Lie, ST
   Guo, ZD
AF Zhang, Yuzhe
   Ong, Chan Chi
   Zheng, Jianmin
   Lie, Seng-Tjhen
   Guo, Zhendong
TI Generative design of decorative architectural parts
SO VISUAL COMPUTER
LA English
DT Article
DE Decorative architectural parts; Generative design; 3D modelling; Machine
   learning; Evolution principle
AB This paper presents a method for generative design of decorative architectural parts such as corbel, moulding and panel, which usually have clear structure and aesthetic details. The method is composed of two components: offline learning and online generation. The offline learning trains a 2D CurveInfoGAN and a 3D VoxelVAE that learn the feature representations of the parts in a dataset. The online generation proceeds with an evolution procedure that evolves to product new generation of part components by selecting, crossing over and mutating features, followed by a feature-driven deformation that synthesizes the 3D mesh representation of new models. Built upon these technical components, a generative design tool is developed, which allows the user to input a decorative architectural model as a reference and then generates a set of new models that are "more of the same" as the reference and meanwhile exhibit some "surprising" elements. The experiments demonstrate the effectiveness of the method and also showcase the use of classic geometric modelling and advanced machine learning techniques in modelling of architectural parts.
C1 [Zhang, Yuzhe; Ong, Chan Chi; Zheng, Jianmin; Guo, Zhendong] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
   [Lie, Seng-Tjhen] Nanyang Technol Univ, Sch Civil & Environm Engn, Singapore, Singapore.
C3 Nanyang Technological University; Nanyang Technological University
RP Zheng, JM (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
EM zhang.yz@ntu.edu.sg; c170117@e.ntu.edu.sg; asjmzheng@ntu.edu.sg;
   cstlie@ntu.edu.sg; zhendong.guo@ntu.edu.sg
RI Lie, Seng Tjhen/A-3766-2011; Zheng, Jianmin/A-3717-2011
OI Zheng, Jianmin/0000-0002-5062-6226
FU Ministry of Education, Singapore [2017-T2-1-076]
FX This work is supported by the Ministry of Education, Singapore, under
   its MoE Tier-2 Grant (2017-T2-1-076).
CR Chen W., 2019, AIAA SCITECH FOR
   Chen W, 2019, J MECH DESIGN, V141, DOI 10.1115/1.4044076
   Chen Xi, 2016, Advances in Neural Information Processing Systems (NIPS), V29
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   Cohen-Or D, 2016, VISUAL COMPUT, V32, P7, DOI 10.1007/s00371-015-1193-9
   Funkhouser T, 2004, ACM T GRAPHIC, V23, P652, DOI 10.1145/1015706.1015775
   Gao L, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356488
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Jianmin Zheng, 1992, Computer-Aided Geometric Design, V9, P321, DOI 10.1016/0167-8396(92)90027-M
   Kalogerakis E, 2012, ACM T GRAPHIC, V31, DOI [10.1145/2077341.2077342, 10.1145/2185520.2185551]
   Kingma D. P., 2013, ARXIV13126114
   Koch K., 2003, ARCHITECTURAL PATTER
   Li J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073637
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qi CR, 2017, ADV NEUR IN, V30
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Sung M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130821
   Tan QY, 2018, PROC CVPR IEEE, P5841, DOI 10.1109/CVPR.2018.00612
   Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230
   Wang H, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275025
   Wu JJ, 2016, ADV NEUR IN, V29
   Wu ZJ, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322956
   Xu K, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185553
   Yu YZ, 2004, ACM T GRAPHIC, V23, P644, DOI 10.1145/1015706.1015774
   [张英杰 Zhang Yingjie], 2020, [仪表技术与传感器, Intstrument Technique and Sensor], P9
NR 26
TC 4
Z9 4
U1 7
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2022
VL 38
IS 4
SI SI
BP 1209
EP 1225
DI 10.1007/s00371-021-02142-1
EA MAY 2021
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0D9DX
UT WOS:000652519300001
DA 2024-07-18
ER

PT J
AU Miura, KT
   Gobithaasan, RU
   Salvi, P
   Wang, D
   Sekine, T
   Usuki, S
   Inoguchi, JI
   Kajiwara, K
AF Miura, Kenjiro T.
   Gobithaasan, R. U.
   Salvi, Peter
   Wang, Dan
   Sekine, Tadatoshi
   Usuki, Shin
   Inoguchi, Jun-ichi
   Kajiwara, Kenji
TI εκ-Curves: controlled local curvature extrema
SO VISUAL COMPUTER
LA English
DT Article
DE Interpolatory curves; Curvature continuity; Control of curvature
   magnitude; Degree elevation
AB The kappa-curve is a recently published interpolating spline which consists of quadratic Bezier segments passing through input points at the loci of local curvature extrema. We extend this representation to control the magnitudes of local maximum curvature in a new scheme called extended- or is an element of kappa-curves. kappa-curves have been implemented as the curvature tool in Adobe Illustrator (R) and Photoshop (R) and are highly valued by professional designers. However, because of the limited degrees of freedom of quadratic Bezier curves, it provides no control over the curvature distribution. We propose new methods that enable the modification of local curvature at the interpolation points by degree elevation of the Bernstein basis as well as application of generalized trigonometric basis functions. By using is an element of kappa-curves, designers acquire much more ability to produce a variety of expressions, as illustrated by our examples.
C1 [Miura, Kenjiro T.; Wang, Dan; Sekine, Tadatoshi; Usuki, Shin] Shizuoka Univ, Grad Sch Sci & Technol, Shizuoka, Japan.
   [Gobithaasan, R. U.] Univ Malaysia Terengganu, Fac Ocean Engn Technol & Informat, Terengganu, Malaysia.
   [Salvi, Peter] Budapest Univ Technol & Econ, Dept Control Engn & Informat Technol, Budapest, Hungary.
   [Inoguchi, Jun-ichi] Univ Tsukuba, Inst Math, Tsukuba, Ibaraki, Japan.
   [Kajiwara, Kenji] Kyushu Univ, Inst Math Ind, Fukuoka, Fukuoka, Japan.
C3 Shizuoka University; Universiti Malaysia Terengganu; Budapest University
   of Technology & Economics; University of Tsukuba; Kyushu University
RP Miura, KT (corresponding author), Shizuoka Univ, Grad Sch Sci & Technol, Shizuoka, Japan.
EM miura.kenjiro@shizuoka.ac.jp; gr@umt.edu.my; salvi@iit.bme.hu;
   wang.dan.18@shizuoka.ac.jp; sekine.tadatoshi@shizuoka.ac.jp;
   usuki@shizuoka.ac.jp; inoguchi@math.tsukuba.ac.jp;
   kaji@imi.kyushu-u.ac.jp
RI Gobithaasan, R. U./H-8891-2018; Salvi, Peter/H-1918-2012
OI Gobithaasan, R. U./0000-0003-3077-8772; Miura,
   Kenjiro/0000-0001-9326-3130; Usuki, Shin/0000-0002-4363-6346; Kajiwara,
   Kenji/0000-0002-0543-9384; Dan, Wang/0000-0003-1906-4998; Salvi,
   Peter/0000-0003-2456-2051
FU JST CREST [JPMJCR1911]; JSPS [19H02048, 26630038]; Solutions and
   Foundation Integrated Research Program; ImPACT Program of the Council
   for Science, Technology and Innovation; Hungarian Scientific Research
   Fund (OTKA) [124727]; 2016, 2018 and 2019 IMI Joint Use Program
   Short-term Joint Research "Differential Geometry and Discrete
   Differential Geometry for Industrial Design"; Grants-in-Aid for
   Scientific Research [19H02048, 21H01226, 26630038] Funding Source: KAKEN
FX This work was supported by JST CREST (No. JPMJCR1911); JSPS Grant-in-Aid
   for Scientific Research (B, No. 19H02048); JSPS Grant-in-Aid for
   Challenging Exploratory Research (No. 26630038); Solutions and
   Foundation Integrated Research Program; ImPACT Program of the Council
   for Science, Technology and Innovation; and the Hungarian Scientific
   Research Fund (OTKA, No. 124727). The authors acknowledge the support by
   2016, 2018 and 2019 IMI Joint Use Program Short-term Joint Research
   "Differential Geometry and Discrete Differential Geometry for Industrial
   Design" (September 2016, September 2018 and September 2019). The second
   author acknowledges University Malaysia Terengganu for approving
   sabbatical leave which was utilized to work on emerging researches,
   including this work.
CR ATTNEAVE F, 1954, PSYCHOL REV, V61, P183, DOI 10.1037/h0054663
   BARSKY BA, 1983, ACM T GRAPHIC, V2, P109, DOI 10.1145/357318.357321
   Chelius C., 2016, USING CURVATURE TOOL
   Chen ZG, 2019, COMPUT AIDED DESIGN, V114, P155, DOI 10.1016/j.cad.2019.05.010
   Djudjic D, 2017, Photoshop CC officially gets curvature pen tool and other improvements
   Do Carmo M., 1976, Differential Geometry of Curves and Surfaces
   Farin G., 2001, MORGAN KAUFMANN SERI, V5th
   Gobithaasan R.U., CURVATURE EXTREMA CO
   Han XA, 2009, APPL MATH LETT, V22, P226, DOI 10.1016/j.aml.2008.03.015
   Jamaludin M., 2001, P 4 INT C CAD CG, P161
   Levien R., 2009, Comput-Aided Des. Appl., V6, P91, DOI DOI 10.3722/CADAPS.2009.91-102
   Liu LG, 2008, COMPUT GRAPH FORUM, V27, P1495, DOI 10.1111/j.1467-8659.2008.01290.x
   Miura Kenjiro T., 2018, Computer-Aided Design and Applications, V15, P256, DOI 10.1080/16864360.2017.1375677
   Miura K.T., 2021, ARXIV210108138
   Miura K.T., 2013, Comput-Aided Des. Appl., V10, P1021
   Miura K. T., 2006, Computer-Aided Design and Applications, V3, P457
   Miura KT, 2020, J COMPUT DES ENG, V7, P155, DOI 10.1093/jcde/qwaa014
   Norman JF, 2001, PERCEPTION, V30, P1285, DOI 10.1068/p3272
   SAID HB, 1989, ACM T GRAPHIC, V8, P360, DOI 10.1145/77269.77275
   Salvi P., 2020, IMPLEMENTATION CURVE
   Sorkine O, 2007, S GEOM PROC, V4, P109, DOI [10.1145/1073204.1073323, DOI 10.1145/1073204.1073323]
   Usman M, 2020, J ADV MECH DES SYST, V14, DOI 10.1299/jamdsm.2020jamdsm0048
   Walton DJ, 2001, J COMPUT APPL MATH, V134, P69, DOI 10.1016/S0377-0427(00)00529-X
   Wang D., 2020, Comput-Aided Des. Appl., V18, P399
   Yan ZP, 2019, COMPUT AIDED GEOM D, V72, P98, DOI 10.1016/j.cagd.2019.06.002
   Yan ZP, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073692
   Yuksel C, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3400301
   Zhang G., 2019, QUASICUBIC TRIGONOME
   Zhang JW, 1996, COMPUT AIDED GEOM D, V13, P199, DOI 10.1016/0167-8396(95)00022-4
   Zhu YP, 2015, J COMPUT APPL MATH, V273, P160, DOI 10.1016/j.cam.2014.06.014
   Ziatdinov R., 2016, SCI VISUALIZATION, V8, P168
NR 31
TC 10
Z9 10
U1 1
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2022
VL 38
IS 8
BP 2723
EP 2738
DI 10.1007/s00371-021-02149-8
EA MAY 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3B2OE
UT WOS:000651680800001
OA Green Published, hybrid
DA 2024-07-18
ER

PT J
AU Zhang, KF
   Wang, XM
   Xu, T
   Du, YJ
   Huang, ZX
AF Zhang, Kaifang
   Wang, Xiaoming
   Xu, Tao
   Du, Yajun
   Huang, Zengxi
TI Generalization-error-bound-based discriminative dictionary learning
SO VISUAL COMPUTER
LA English
DT Article
DE Discriminative dictionary learning; Collaborative representation;
   Support vector machine; Generalization error bound
ID K-SVD; IMAGE
AB Support vector guided dictionary learning, as a discriminative dictionary learning method combining with support vector machine (SVM), embodies the margin maximization principle and achieves good generalization performances in many practical applications. However, this method ignores the key fact that the generalization performance of the SVM classifier depends not only on the margin between two classes of training samples, but also on the radius of the smallest sphere covering them. In the paper, we propose a novel method called generalization-error-bound-based discriminative dictionary learning (GEBDDL). The basic insight of GEBDDL is that the coding vectors, which are used to build the SVM classifier, are not fixed during the learning process. As a result, the radius of the smallest sphere changes with the learned coding vectors. The key feature of GEBDDL is that it explicitly incorporates the radius-margin-bound, which is directly related to the upper bound of the leave-one-out error of SVM, into its objective function to guide learning the dictionary and the coding vectors, and building the SVM classifier. In the paper, we first elaborate our motivation and propose the optimization model and then discuss how to solve it in detail. Further, we explore how to approximate the radius of the smallest sphere in our methodology. This can enhance the computational efficiency by bypassing the quadratic programming problem of computing the radius, while yielding a close performance to GEBDDL. Finally, the comprehensive experiments are conducted on several benchmark datasets, and the results demonstrate the superiority of the proposed methods over the other competing methods.
C1 [Zhang, Kaifang; Wang, Xiaoming; Xu, Tao; Du, Yajun; Huang, Zengxi] XiHua Univ, Sch Comp & Software Engn, Chengdu 610039, Peoples R China.
   [Zhang, Kaifang; Wang, Xiaoming; Xu, Tao; Du, Yajun; Huang, Zengxi] XiHua Univ, Robot Res Ctr, Chengdu 610039, Peoples R China.
C3 Xihua University; Xihua University
RP Zhang, KF (corresponding author), XiHua Univ, Sch Comp & Software Engn, Chengdu 610039, Peoples R China.; Zhang, KF (corresponding author), XiHua Univ, Robot Res Ctr, Chengdu 610039, Peoples R China.
EM devinzhang1994@163.com; wangxmxh@aliyun.com
OI zhang, kaifang/0000-0001-5196-6532
FU National Natural Science Foundation of China [61602390]; Innovation Fund
   of Postgraduate, Xihua University [ycjj2019085]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61602390; and in part by the Innovation
   Fund of Postgraduate, Xihua University under Grant ycjj2019085.
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], 2011, COLUMBIA OBJECT IMAG
   [Anonymous], 2017, IEEE T INDUST INFORM
   Benavente R, 1998, 24 COMP VIS CTR
   Boyd S., 2011, FOUND TRENDS MACH LE, V3, P1, DOI DOI 10.1561/2200000016
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   Cai SJ, 2014, LECT NOTES COMPUT SC, V8692, P624, DOI 10.1007/978-3-319-10593-2_41
   Chapelle O, 2007, NEURAL COMPUT, V19, P1155, DOI 10.1162/neco.2007.19.5.1155
   Cherkassky V, 1997, IEEE Trans Neural Netw, V8, P1564, DOI 10.1109/TNN.1997.641482
   Deng WH, 2018, IEEE T PATTERN ANAL, V40, P2513, DOI 10.1109/TPAMI.2017.2757923
   Dianat, 2020, VISUAL COMPUT, V23, P1, DOI [10.1007/s00791-020-00320-7, DOI 10.1007/S00791-020-00320-7]
   Do H., 2013, PROC INT C MACH LEAR, P169
   Do H, 2009, LECT NOTES ARTIF INT, V5781, P315, DOI 10.1007/978-3-642-04180-8_38
   Foody GM, 2004, REMOTE SENS ENVIRON, V93, P107, DOI 10.1016/j.rse.2004.06.017
   Fotiadou K, 2019, IEEE T GEOSCI REMOTE, V57, P2777, DOI 10.1109/TGRS.2018.2877124
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Jiang JJ, 2014, J SIGNAL PROCESS SYS, V75, P245, DOI 10.1007/s11265-013-0804-9
   Jiang ZL, 2013, IEEE T PATTERN ANAL, V35, P2651, DOI 10.1109/TPAMI.2013.88
   Lee H., 2007, C ADV INF PROC SYST
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li HF, 2018, PATTERN RECOGN, V79, P130, DOI 10.1016/j.patcog.2018.02.005
   Li ZM, 2017, IEEE T NEUR NET LEAR, V28, P278, DOI 10.1109/TNNLS.2015.2508025
   Li ZX, 2020, INT J PROD RES, V58, P3220, DOI 10.1080/00207543.2019.1630774
   Lin YQ, 2011, PROC CVPR IEEE, P1689, DOI 10.1109/CVPR.2011.5995477
   Liu Q, 2016, VISUAL COMPUT, V32, P535, DOI 10.1007/s00371-015-1087-x
   Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388
   Mairal J, 2008, PROC CVPR IEEE, P2415
   Mairal J, 2012, IEEE T PATTERN ANAL, V34, P791, DOI 10.1109/TPAMI.2011.156
   Samaria F. S., 1994, Proceedings of the Second IEEE Workshop on Applications of Computer Vision (Cat. No.94TH06742), P138, DOI 10.1109/ACV.1994.341300
   Vapnik V, 2000, NEURAL COMPUT, V12, P2013, DOI 10.1162/089976600300015042
   Wang DH, 2014, PATTERN RECOGN, V47, P885, DOI 10.1016/j.patcog.2013.08.004
   Wen ZD, 2017, IEEE T CYBERNETICS, V47, P3758, DOI 10.1109/TCYB.2016.2581861
   Wright J, 2008, IEEE INT CONF AUTOMA, P942
   Xu Y, 2016, PATTERN RECOGN, V54, P68, DOI 10.1016/j.patcog.2015.12.017
   Xu Yunhong, 2016, 2016 13th International Conference on Service Systems and Service Management (ICSSSM), P1, DOI 10.1109/ICSSSM.2016.7538568
   Yang M, 2014, INT J COMPUT VISION, V109, P209, DOI 10.1007/s11263-014-0722-8
   Zelin, IEEE ACCESS, P1
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277
   Zhang Q.Z.Q., 2010, PROC CVPR IEEE, DOI [10.1109/CVPR.2010.5539989, DOI 10.1109/CVPR.2010.5539989]
   Zheng M, 2011, IEEE T IMAGE PROCESS, V20, P1327, DOI 10.1109/TIP.2010.2090535
NR 40
TC 1
Z9 1
U1 2
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2022
VL 38
IS 8
BP 2853
EP 2869
DI 10.1007/s00371-021-02160-z
EA MAY 2021
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3B2OE
UT WOS:000651332600002
DA 2024-07-18
ER

PT J
AU Beulah, A
   Sharmila, TS
   Pramod, VK
AF Beulah, A.
   Sharmila, T. Sree
   Pramod, V. K.
TI Degenerative disc disease diagnosis from lumbar MR images using hybrid
   features
SO VISUAL COMPUTER
LA English
DT Article
DE Disc degeneration; Expectation-Maximization; Gabor features; Invariant
   moments; Low back pain; Lumbar spine; Magnetic Resonance Imaging;
   Support Vector Machine
AB Disc degeneration is a common type of lumbar disc disease. Disc degeneration leads to low back pain, and it is caused due to injury in Intervertebral Disc (IVD). An automatic diagnostic system to diagnose degenerative discs from T2-weighted sagittal MR image is proposed. A fully automated Expectation-Maximization (EM)-based new IVD segmentation is proposed to segment the lumbar IVD from mid-sagittal MR image. Then, a hybrid of basic intensity, invariant moments, Gabor features are extracted from segmented IVDs. The IVDs are classified as degenerative or non-degenerative using Support Vector Machine (SVM) classifier. The proposed system is trained, tested and evaluated for 93 clinical sagittal MR images of 93 patients. The optimized hyperparameters are estimated. The proposed model is tested and validated for the dataset and obtained an accuracy of 92.47%. The patient-based analysis was performed and obtained an accuracy of 92.86%. The performance analysis of the proposed model with other classifiers like k-NN, decision tree, Linear Discriminant Analysis (LDA) and Feedforward neural network is also analyzed. This proposed method outperforms when compared with state-of-the-art methods. This system can be used as a second opinion in diagnosing degenerative discs.
C1 [Beulah, A.] Sri Sivasubramaniya Nadar Coll Engn, Dept Comp Sci & Engn, Chennai, Tamil Nadu, India.
   [Sharmila, T. Sree] Sri Sivasubramaniya Nadar Coll Engn, Dept Informat Technol, Chennai, Tamil Nadu, India.
   [Pramod, V. K.] Trivandrum Med Coll, Dept Orthopaed, Thiruvananthapuram, Kerala, India.
C3 SSN College of Engineering; SSN College of Engineering
RP Beulah, A (corresponding author), Sri Sivasubramaniya Nadar Coll Engn, Dept Comp Sci & Engn, Chennai, Tamil Nadu, India.
EM beulaharul@ssn.edu.in; sreesharmilat@ssn.edu.in; pvalsalam@gmail.com
RI T., Sree Sharmila/ABC-3930-2021; Beulah, A./Y-4433-2019
OI T., Sree Sharmila/0000-0001-5744-9739; T, Sree
   Sharmila/0009-0009-1736-2669
CR Akkurt, 2011 INT S INN INT S, P490
   Alomari RS, 2009, I S BIOMED IMAGING, P546, DOI 10.1109/ISBI.2009.5193105
   [Anonymous], 1994, R DIG IM PROC METH
   [Anonymous], 2012, ADV ORTHOPEDICS, V2012
   Baratloo A, 2015, EMERGENCY, V3, P48
   Barreiro MD, 2014, ISSNIP BIOSIG BIOROB, P140
   BEN P, 2011, LECT NOTES COMPUT SC, P221
   Beulah A, 2018, MULTIMED TOOLS APPL, V77, P27215, DOI 10.1007/s11042-018-5914-8
   Beulah A, 2016, PROCEEDINGS OF THE 2016 2ND INTERNATIONAL CONFERENCE ON APPLIED AND THEORETICAL COMPUTING AND COMMUNICATION TECHNOLOGY (ICATCCT), P293, DOI 10.1109/ICATCCT.2016.7912011
   Beulah A., 2019, INT C EM CURR TRENDS, P1226
   Bigos S., 1996, J MANUAL MANIPULATIV, V4, P99, DOI [DOI 10.1179/JMT.1996.4.3.99, 10.1179/jmt.1996.4.3, DOI 10.1179/JMT.1996.4.3]
   Castro-Mateos I, 2016, EUR SPINE J, V25, P2721, DOI 10.1007/s00586-016-4654-6
   Chou R, 2007, ANN INTERN MED, V147, P478, DOI 10.7326/0003-4819-147-7-200710020-00006
   Chow DHK, 2017, MUSCULOSKEL SCI PRAC, V29, P78, DOI 10.1016/j.msksp.2017.03.007
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Doi K, 2007, COMPUT MED IMAG GRAP, V31, P198, DOI 10.1016/j.compmedimag.2007.02.002
   HARALICK RM, 1979, P IEEE, V67, P786, DOI 10.1109/PROC.1979.11328
   Hearst MA, 1998, IEEE INTELL SYST APP, V13, P18, DOI 10.1109/5254.708428
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692
   Hurwitz EL, 2018, EUR SPINE J, V27, P796, DOI 10.1007/s00586-017-5432-9
   Kim KI, 2002, IEEE T PATTERN ANAL, V24, P1542, DOI 10.1109/TPAMI.2002.1046177
   Mahdy LN, 2018, ISA T, V81, P244, DOI 10.1016/j.isatra.2018.07.006
   McHugh ML, 2012, BIOCHEM MEDICA, V22, P276, DOI 10.11613/bm.2012.031
   Mercimek M, 2005, SADHANA-ACAD P ENG S, V30, P765, DOI 10.1007/BF02716709
   Neubert A, 2013, J AM MED INFORM ASSN, V20, P1082, DOI 10.1136/amiajnl-2012-001547
   Oktay AB, 2014, COMPUT MED IMAG GRAP, V38, P613, DOI 10.1016/j.compmedimag.2014.04.006
   Platen D., 2018, GERMAN J SPORTS MED, V69
   Qin FB, 2019, IEEE INT CONF ROBOT, P9821, DOI 10.1109/icra.2019.8794122
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   TERTTI M, 1991, SPINE, V16, P629, DOI 10.1097/00007632-199106000-00006
   Unal Y., 2011, 6 INT ADV TECHN S IA, P16
   Unal Y, 2015, APPL SOFT COMPUT, V33, P65, DOI 10.1016/j.asoc.2015.04.031
   Wintz Paul C., 1977, DIGITAL IMAGE PROCES, V13, P451
   Yang P, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3339474
   Zaidi H, 2011, MED PHYS, V38, P5667, DOI 10.1118/1.3633909
NR 35
TC 6
Z9 6
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2022
VL 38
IS 8
BP 2771
EP 2783
DI 10.1007/s00371-021-02154-x
EA MAY 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3B2OE
UT WOS:000648357500001
DA 2024-07-18
ER

PT J
AU Wang, S
   Zou, YN
   Min, WD
   Wu, JS
   Xiong, X
AF Wang, Shuai
   Zou, Yanni
   Min, Weidong
   Wu, Jiansheng
   Xiong, Xin
TI Multi-view face generation via unpaired images
SO VISUAL COMPUTER
LA English
DT Article
DE Multi-view face generation; Generative adversarial net; Adversarial
   autoencoder; Semi-supervised learning
AB Multi-view face generation from a single image is an essential and challenging problem. Most of the existing methods need to use paired images when training models. However, collecting and labeling large-scale paired face images could lead to high labor and time cost. In order to address this problem, multi-view face generation via unpaired images is proposed in this paper. To avoid using paired data, the encoder and discriminator are trained, so that the high-level abstract features of the identity and view of the input image are learned by the encoder, and then, these low-dimensional data are input into the generator, so that the realistic face image can be reconstructed by the training generator and discriminator. During testing, multiple one-hot vectors representing the view are imposed to the identity representation and the generator is employed to map them to high-dimensional data, respectively, which can generate multi-view images while preserving the identity features. Furthermore, to reduce the number of used labels, semi-supervised learning is used in the model. The experimental results show that our method can produce photo-realistic multi-view face images with a small number of view labels, and makes a useful exploration for the synthesis of face images via unpaired data and very few labels.
C1 [Wang, Shuai; Zou, Yanni; Wu, Jiansheng; Xiong, Xin] Nanchang Univ, Sch Informat Engn, Nanchang 330031, Jiangxi, Peoples R China.
   [Min, Weidong] Nanchang Univ, Sch Software, Nanchang 330047, Jiangxi, Peoples R China.
   [Min, Weidong] Jiangxi Key Lab Smart City, Nanchang 330047, Jiangxi, Peoples R China.
C3 Nanchang University; Nanchang University
RP Min, WD (corresponding author), Nanchang Univ, Sch Software, Nanchang 330047, Jiangxi, Peoples R China.; Min, WD (corresponding author), Jiangxi Key Lab Smart City, Nanchang 330047, Jiangxi, Peoples R China.
EM minweidong@ncu.edu.cn
RI Min, Weidong/D-4585-2017
OI Xiong, Xin/0000-0003-2998-6494
FU National Natural Science Foundation of China [62076117, 61762061,
   61862044]; Natural Science Foundation of Jiangxi Province, China
   [20161ACB20004]; Jiangxi Key Laboratory of Smart City [20192BCD40002]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 62076117, Grant 61762061 and 61862044, the Natural
   Science Foundation of Jiangxi Province, China, under Grant
   20161ACB20004, and Jiangxi Key Laboratory of Smart City under Grant No.
   20192BCD40002.
CR al, 2016, INT C LEARN REPR ICL
   al, 2018, IEEE C COMP VIS PATT
   [Anonymous], ANN C NEUR INF PROC
   [Anonymous], 2018, PATTERN RECOGN LETT
   [Anonymous], 2016, BRIT MACH VIS C BMVC
   [Anonymous], 2014, ANN C NEUR INF PROC, P2672
   [Anonymous], 2018, INT C LEARN REPR ICL
   [Anonymous], IEEE J EM SEL TOP C
   [Anonymous], 2020, RECURRENT CYCLE CONS
   Bao JM, 2017, IEEE I CONF COMP VIS, P2764, DOI 10.1109/ICCV.2017.299
   Bas A., 2017, LECT NOTES COMPUT SC, P377, DOI DOI 10.1007/978-3-319-54427-4_28
   Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983
   Blanz V., 2002, ACM SIGGRAPH, P187
   Booth J, 2018, INT J COMPUT VISION, V126, P233, DOI 10.1007/s11263-017-1009-7
   Cao, ARXIV PREPRINT ARXIV
   Denton E., ARXIV PREPRINT ARXIV
   Denton Vighnesh, 2017, ANN C NEUR INF PROC
   Fu C, ARXIV190312003
   Gourier, P POINT 2004 WORKSH, P17
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Hassner T, 2015, PROC CVPR IEEE, P4295, DOI 10.1109/CVPR.2015.7299058
   Hensel M, 2017, ADV NEUR IN, V30
   Huang GL, 2017, IEEE ICC
   Huang RJ, 2015, VISUAL COMPUT, V31, P1683, DOI 10.1007/s00371-014-1049-8
   Huang R, 2017, IEEE I CONF COMP VIS, P2458, DOI 10.1109/ICCV.2017.267
   Infogan, 2016, ANN C NEUR INF PROC
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Jiang L, 2018, IEEE T IMAGE PROCESS, V27, P4756, DOI 10.1109/TIP.2018.2845697
   Kingma D. P., 2014, arXiv
   Li SX, 2012, LECT NOTES COMPUT SC, V7572, P102, DOI 10.1007/978-3-642-33718-5_8
   Liu HJ, 2020, VISUAL COMPUT, V36, P2105, DOI 10.1007/s00371-020-01913-6
   Ma MY, 2016, VISUAL COMPUT, V32, P1223, DOI 10.1007/s00371-015-1158-z
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Odena A, ARXIV PREPRINT ARXIV
   Odena A, 2017, PR MACH LEARN RES, V70
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Pinho E, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8081213
   PNVR K., 2020, IEEE C COMP VIS PATT
   Sagonas C, 2017, INT J COMPUT VISION, V122, P270, DOI 10.1007/s11263-016-0920-7
   Sagonas C, 2015, IEEE I CONF COMP VIS, P3871, DOI 10.1109/ICCV.2015.441
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Springenberg, INT C LEARN REPR ICL
   Tang YL, 2019, VISUAL COMPUT, V35, P783, DOI 10.1007/s00371-019-01695-6
   Tewariet, 2020, IEEE C COMP VIS PATT
   Tian Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P942
   Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141
   Viola P, 2002, ADV NEUR IN, V14, P1311
   Yang J., 2015, Advances in Neural Information Processing Systems, P1099
   Yim J, 2015, PROC CVPR IEEE, P676, DOI 10.1109/CVPR.2015.7298667
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
   Zhu XY, 2015, PROC CVPR IEEE, P787, DOI 10.1109/CVPR.2015.7298679
   Zhu Zhenyao, 2014, ADV NEURAL INFORM PR, DOI DOI 10.5555/2968826.2968851
   2017, I C NETWORK PROTOCOL
NR 53
TC 7
Z9 7
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2022
VL 38
IS 7
BP 2539
EP 2554
DI 10.1007/s00371-021-02129-y
EA APR 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2B0WL
UT WOS:000639803700001
DA 2024-07-18
ER

PT J
AU Lin, CW
   Hong, SD
AF Lin, Chih-Wei
   Hong, Sidi
TI High-order histogram-based local clustering patterns in polar coordinate
   for facial recognition and retrieval
SO VISUAL COMPUTER
LA English
DT Article
DE Facial recognition; Facial retrieval; Local descriptor; Local Clustering
   Patterns (LCP); Histogram-based; Polar Coordinate
ID FACE-RECOGNITION; DISCRIMINANT-ANALYSIS; DESCRIPTOR; PCA;
   REPRESENTATION; SCALE
AB Local feature patterns are conspicuous and are widely used in computer vision, especially in face recognition and retrieval. However, a statistical descriptor that can be used in various scenarios and effectively present the detailed local discrimination information of face images is a challenging and exploring task even if deep learning technology is widelyspread. In this study, we propose a novel local pattern descriptor called the Local Clustering Pattern (LCP) in high-order derivative space for facial recognition and retrieval. Unlike prior methods, LCP exploits the concept of clustering to analyze the relationship of intra- and inter-classes of the referenced pixel and its adjacent pixels to encode the local descriptor for facial recognition. There are three tasks (1) Local Clustering Pattern (LCP), (2) Clustering Coding Scheme, (3) High-order Local Clustering Pattern. To generate local clustering pattern, the local derivative variations with multi-direction are considered and that are integrated on rectangular coordinate system with the pairwise combinatorial direction. Moreover, to generate the discriminative local pattern, the features of local derivative variations are transformed from the rectangular coordinate system into the polar coordinate system to generate the characteristics of magnitude (m) and orientation (theta). Then, we shift and project the features (m and theta), which are scattered in the four quadrants of polar coordinate system, into the first quadrant of polar coordinates to strengthen the relationship of intra- and inter-classes of the referenced pixel and its adjacent pixels. To encode the local pattern, we consider the spatial relationship between reference and its adjacent pixels and fuse the clustering algorithm into the coding scheme by utilizing the relationship of intra- and inter-classes in a local patch. In addition, we extend the LCP from low- into high-order derivative space to extract the detailed and abundant information for facial description. LCP efficiently encodes the feature of a local region that is discriminative the inter-classes and robust the intra-class of the related pixels to describe a face image.
   This study has three main contributions: (1) we generate the novel features with magnitude (m) and orientation (theta) based on the pairs of the derivative variations to describe the characteristics of each pixel, (2) we shift and project the features from four quadrants of polar coordinate system into the first quadrant of polar coordinates to strengthen the relationship of intra- and inter-classes between pixels in a local patch, (3) we exploit the concept of clustering, which considers the relationship of intra- and inter-classes of the referenced pixel and its adjacent pixels, to encode the local descriptor in a polar coordinate system for facial recognition and retrieval. Experimental results show that LCP outperforms the existing descriptors (LBP, ELBP LDP, LTrP, LVP, LDZP, LGHP) on six public datasets (ORL, Extend Yale B, CAS PEAL, and LFW, CMU-PIE and FERET) for both face recognition and retrieval tasks. Moreover, we further compare the proposed facial descriptor with the popular deep convolutional neural networks to demonstrate the discrimination of the extracted features and applicability of our approach.
C1 [Lin, Chih-Wei; Hong, Sidi] Fujian Agr & Forestry Univ, Coll Comp & Informat Sci, Fuzhou 350002, Peoples R China.
   [Lin, Chih-Wei] Fujian Agr & Forestry Univ, Coll Forestry, Fuzhou 350002, Peoples R China.
   [Lin, Chih-Wei] Fujian Agr & Forestry Univ, Forestry Postdoctoral Stn, Fuzhou 350002, Peoples R China.
C3 Fujian Agriculture & Forestry University; Fujian Agriculture & Forestry
   University; Fujian Agriculture & Forestry University
RP Lin, CW (corresponding author), Fujian Agr & Forestry Univ, Coll Comp & Informat Sci, Fuzhou 350002, Peoples R China.; Lin, CW (corresponding author), Fujian Agr & Forestry Univ, Coll Forestry, Fuzhou 350002, Peoples R China.; Lin, CW (corresponding author), Fujian Agr & Forestry Univ, Forestry Postdoctoral Stn, Fuzhou 350002, Peoples R China.
EM cwlin@fafu.edu.cn
FU China Postdoctoral Science Foundation [2018M632565]; Channel
   Postdoctoral Exchange Funding Scheme; Youth Program of Humanities and
   Social Sciences Foundation, Ministry of Education of China [18YJCZH093]
FX This research was funded by the China Postdoctoral Science Foundation
   under Grant 2018M632565; the Channel Postdoctoral Exchange Funding
   Scheme; and the Youth Program of Humanities and Social Sciences
   Foundation, Ministry of Education of China under Grant 18YJCZH093.
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   BRUNELLI R, 1993, IEEE T PATTERN ANAL, V15, P1042, DOI 10.1109/34.254061
   Chakraborty S, 2018, IEEE T CIRC SYST VID, V28, P171, DOI 10.1109/TCSVT.2016.2603535
   Chintalapati S, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND COMPUTING RESEARCH (ICCIC), P541
   Davarzani R, 2015, SIGNAL PROCESS, V111, P274, DOI 10.1016/j.sigpro.2014.11.005
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ekenel HK, 2010, COMPUT VIS IMAGE UND, V114, P596, DOI 10.1016/j.cviu.2009.06.009
   Etemad K, 1997, J OPT SOC AM A, V14, P1724, DOI 10.1364/JOSAA.14.001724
   Fan KC, 2014, IEEE T IMAGE PROCESS, V23, P2877, DOI 10.1109/TIP.2014.2321495
   Gao W, 2008, IEEE T SYST MAN CY A, V38, P149, DOI 10.1109/TSMCA.2007.909557
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Heisele B, 2003, COMPUT VIS IMAGE UND, V91, P6, DOI 10.1016/S1077-3142(03)00073-0
   Heisele B, 2007, INT J COMPUT VISION, V74, P167, DOI 10.1007/s11263-006-0006-z
   Huang D, 2011, IEEE T SYST MAN CY C, V41, P765, DOI 10.1109/TSMCC.2011.2118750
   Huang G. B., 2008, WORKSH FAC REAL LIF
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Hung TY, 2014, IEEE IMAGE PROC, P239, DOI 10.1109/ICIP.2014.7025047
   Ijiri Y., 2006, Proceedings of the 7th International Conference on Mobile Data Management, MDM '06, P49, DOI DOI 10.1109/MDM.2006.138
   Kamgar-Parsi B, 2011, IEEE T PATTERN ANAL, V33, P1925, DOI 10.1109/TPAMI.2011.68
   Kim KI, 2002, IEEE SIGNAL PROC LET, V9, P40, DOI 10.1109/97.991133
   Kong H, 2005, NEURAL NETWORKS, V18, P585, DOI 10.1016/j.neunet.2005.06.041
   Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92
   Lee SH, 2012, IEEE T IMAGE PROCESS, V21, P2347, DOI 10.1109/TIP.2011.2181526
   Liu L, 2012, IMAGE VISION COMPUT, V30, P86, DOI 10.1016/j.imavis.2012.01.001
   Lu JW, 2003, IEEE T NEURAL NETWOR, V14, P117, DOI 10.1109/TNN.2002.806629
   Lu KY., 2015, PACIFIC RIM S IMAGE, P656
   Martìnez AM, 2001, IEEE T PATTERN ANAL, V23, P228, DOI 10.1109/34.908974
   Murala S, 2012, IEEE T IMAGE PROCESS, V21, P2874, DOI 10.1109/TIP.2012.2188809
   Noushath S, 2006, NEUROCOMPUTING, V69, P1711, DOI 10.1016/j.neucom.2006.01.012
   Noushath S, 2006, PATTERN RECOGN, V39, P1396, DOI 10.1016/j.patcog.2006.01.018
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Patel K, 2016, IEEE T INF FOREN SEC, V11, P2268, DOI 10.1109/TIFS.2016.2578288
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Roy SK, 2018, PATTERN RECOGN LETT, V108, P23, DOI 10.1016/j.patrec.2018.02.027
   Samaria F. S., 1994, Proceedings of the Second IEEE Workshop on Applications of Computer Vision (Cat. No.94TH06742), P138, DOI 10.1109/ACV.1994.341300
   Sim T, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P53, DOI 10.1109/AFGR.2002.1004130
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   TURK MA, 1991, 1991 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, P586
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Xie XD, 2006, IEEE T IMAGE PROCESS, V15, P2481, DOI 10.1109/TIP.2006.877435
   Yin Z, 2014, 3DTV C TRUE VIS CAPT, P1
   Zhang BC, 2010, IEEE T IMAGE PROCESS, V19, P533, DOI 10.1109/TIP.2009.2035882
   Zhang DQ, 2005, NEUROCOMPUTING, V69, P224, DOI 10.1016/j.neucom.2005.06.004
NR 45
TC 2
Z9 2
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2022
VL 38
IS 5
BP 1741
EP 1758
DI 10.1007/s00371-021-02102-9
EA MAR 2021
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0M9JP
UT WOS:000634616600001
DA 2024-07-18
ER

PT J
AU Farooq, MU
   Saad, MNM
   Khan, SD
AF Farooq, Muhammad Umer
   Saad, Mohamad Naufal M.
   Khan, Sultan Daud
TI Motion-shape-based deep learning approach for divergence behavior
   detection in high-density crowd
SO VISUAL COMPUTER
LA English
DT Article
DE FTLE; Divergence; Anomaly; Motion; Blob; Shape; Localization
ID DYNAMICS
AB We propose a novel method of abnormal crowd behavior detection in surveillance videos. Mainly, our work focuses on detecting crowd divergence behavior that can lead to serious disasters like a stampede. We introduce a notion of physically capturing motion in the form of images and classify crowd behavior using a convolution neural network (CNN) trained on motion-shape images (MSIs). First, the optical flow (OPF) is computed, and finite-time Lyapunov exponent (FTLE) field is obtained by integrating OPF. Lagrangian coherent structure (LCS) in the FTLE field represents crowd-dominant motion. A ridge extraction scheme is proposed for the conversion of LCS-to-grayscale MSIs. Lastly, a supervised training approach is utilized with CNN to predict normal or divergence behavior for any unknown image. We test our method on six real-world low- as well as high-density crowd datasets and compare performance with state-of-the-art methods. Experimental results show that our method is not only robust for any type of scene but also outperform existing state-of-the-art methods in terms of accuracy. We also propose a divergence localization method that not only identifies divergence starting (source) points but also comes with a new feature of generating a 'localization mask' around the diverging crowd showing the size of divergence. Finally, we also introduce two new datasets containing videos of crowd normal and divergence behaviors at the high density.
C1 [Farooq, Muhammad Umer; Saad, Mohamad Naufal M.] Univ Teknol PETRONAS, Ctr Intelligent Signal & Imaging Res CISIR, Bandar Seri Iskandar 32610, Perak Darul Rid, Malaysia.
   [Khan, Sultan Daud] Natl Univ Technol, Dept Comp Sci, Islamabad, Pakistan.
C3 Universiti Teknologi Petronas
RP Farooq, MU (corresponding author), Univ Teknol PETRONAS, Ctr Intelligent Signal & Imaging Res CISIR, Bandar Seri Iskandar 32610, Perak Darul Rid, Malaysia.
EM m.umer.farooq82@gmail.com
RI Saad, Naufal m/N-2871-2013; Khan, Sultan Daud/J-7563-2019
OI Khan, Sultan Daud/0000-0002-7406-8441; Farooq, Muhammad
   Umer/0000-0002-2865-9953
FU Center for Intelligent Signal for Imaging Research (CISIR)
   [3920089787/30.10.2017]
FX We thank Dr. Yasir Salih for sharing high-density crowd dataset and
   valuable suggestions on motion estimation at high-density crowd. The
   MassMotion crowd simulation software is supported by the Center for
   Intelligent Signal for Imaging Research (CISIR) under PO Number
   3920089787/30.10.2017.
CR Ahmed F, 2015, IEEE I CONF COMP VIS, P1850, DOI 10.1109/ICCV.2015.215
   Ali S, 2007, PROC CVPR IEEE, P65
   [Anonymous], 2009, DATASET
   [Anonymous], CROWD DAT
   Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   Batty M, 2003, INT J GEOGR INF SCI, V17, P673, DOI 10.1080/1365881031000135474
   Benabbas Y, 2011, EURASIP J IMAGE VIDE, DOI 10.1155/2011/163682
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Chen CY, 2015, IEEE SENS J, V15, P2431, DOI 10.1109/JSEN.2014.2381260
   Chen DY, 2011, J VIS COMMUN IMAGE R, V22, P178, DOI 10.1016/j.jvcir.2010.12.004
   Cheriyadat AM, 2008, IEEE J-STSP, V2, P568, DOI 10.1109/JSTSP.2008.2001306
   Cong Y, 2013, IEEE T INF FOREN SEC, V8, P1590, DOI 10.1109/TIFS.2013.2272243
   Cong Y, 2011, PROC CVPR IEEE, P1807, DOI 10.1109/CVPR.2011.5995434
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Direkoglu C, 2020, IEEE ACCESS, V8, P80408, DOI 10.1109/ACCESS.2020.2990355
   Dong YH, 2017, CHIN J TRAUMATOL, V20, P240, DOI 10.1016/j.cjtee.2016.08.005
   Fortun Denis, 2015, Computer Vision and Image Understanding, V134, P1, DOI 10.1016/j.cviu.2015.02.008
   Helbing D, 2012, EPJ DATA SCI, V1, DOI 10.1140/epjds7
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hu M, 2008, INT C PATT RECOG, P9
   Hu WM, 2006, IEEE T PATTERN ANAL, V28, P1450, DOI 10.1109/TPAMI.2006.176
   Huang SN, 2018, MATH PROBL ENG, V2018, DOI 10.1155/2018/6323942
   Illiyas FT, 2013, INT J DISAST RISK RE, V5, P10, DOI 10.1016/j.ijdrr.2013.09.003
   Johansson A, 2008, ADV COMPLEX SYST, V11, P497, DOI 10.1142/S0219525908001854
   Kratz L, 2012, IEEE T PATTERN ANAL, V34, P987, DOI 10.1109/TPAMI.2011.173
   Kratz L, 2009, PROC CVPR IEEE, P1446, DOI 10.1109/CVPRW.2009.5206771
   Krausz B, 2012, COMPUT VIS IMAGE UND, V116, P307, DOI 10.1016/j.cviu.2011.08.006
   Lawal IA, 2017, IEEE T CIRC SYST VID, V27, P2395, DOI 10.1109/TCSVT.2016.2580401
   Li XH, 2016, IEEE J-STARS, V9, P3629, DOI 10.1109/JSTARS.2016.2533547
   Lipinski D, 2010, CHAOS, V20, DOI 10.1063/1.3270049
   Lucas B., 1981, Proc. DARPA Image Understanding Workshop, P121
   Mehran R, 2010, LECT NOTES COMPUT SC, V6313, P439
   Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641
   Peikert, SEMSEGORG
   Pereira EM, 2016, J VIS COMMUN IMAGE R, V40, P265, DOI 10.1016/j.jvcir.2016.06.020
   Ravanbakhsh M, 2018, IEEE WINT CONF APPL, P1689, DOI 10.1109/WACV.2018.00188
   Ravanbakhsh M, 2017, IEEE IMAGE PROC, P1577, DOI 10.1109/ICIP.2017.8296547
   Shadden SC, 2005, PHYSICA D, V212, P271, DOI 10.1016/j.physd.2005.10.007
   Solmaz B, 2012, IEEE T PATTERN ANAL, V34, P2064, DOI 10.1109/TPAMI.2012.123
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang XF, 2014, J COMPUT, V9, P1144, DOI 10.4304/jcp.9.5.1144-1149
   Wang XF, 2014, OPTIK, V125, P924, DOI 10.1016/j.ijleo.2013.07.166
   Wu SD, 2010, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2010.5539882
   Wu S, 2017, INT J COMPUT VISION, V123, P499, DOI 10.1007/s11263-017-1005-y
   Wu S, 2014, IEEE T CIRC SYST VID, V24, P85, DOI 10.1109/TCSVT.2013.2276151
   Wu YP, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P361, DOI 10.1145/2733373.2806227
   Xu J, 2017, LECT NOTES COMPUT SC, V10116, P79, DOI 10.1007/978-3-319-54407-6_6
   Zeiler MD, 2013, INT CONF ACOUST SPEE, P3517, DOI 10.1109/ICASSP.2013.6638312
   Zitouni MS, 2016, NEUROCOMPUTING, V186, P139, DOI 10.1016/j.neucom.2015.12.070
NR 49
TC 15
Z9 15
U1 0
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2022
VL 38
IS 5
BP 1553
EP 1577
DI 10.1007/s00371-021-02088-4
EA FEB 2021
PG 25
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0M9JP
UT WOS:000621743400001
DA 2024-07-18
ER

PT J
AU Zhu, ZY
   Toyoura, M
   Fujishiro, I
   Go, K
   Kashiwagi, K
   Mao, XY
AF Zhu, Zhenyang
   Toyoura, Masahiro
   Fujishiro, Issei
   Go, Kentaro
   Kashiwagi, Kenji
   Mao, Xiaoyang
TI LineM: assessing metamorphopsia symptom using line manipulation task
SO VISUAL COMPUTER
LA English
DT Article
DE Metamorphopsia; Computer-based assessment; Line manipulation; Distortion
   compensation
AB Metamorphopsia patients experience distortions in their views. The number of metamorphopsia patients is arising with the increase in the elderly population. For early detection of metamorphopsia, examination approaches have been proposed. However, none of the existing methods can be employed to precisely assess the perceptions of patients. We propose LineM as a novel method to precisely measure the distorted views of metamorphopsia patients based on interactive line manipulation task involving the patients. To explore the potential of LineM, a case study which uses the measured results from LineM to visualize the distorted views of metamorphopsia patients is presented. Furthermore, a novel technique for compensating the distorted views of individual metamorphopsia patients using the results from LineM is also presented. Considering that elderly individuals account for the majority of metamorphopsia patients, an intuitive and user-friendly prototype system has been implemented. The effectiveness and usability of LineM and the compensation method were demonstrated via a qualitative and subjective experiment that involved metamorphopsia patients and elderly participants.
C1 [Zhu, Zhenyang] Univ Yamanashi, Grad Sch Engn, Kofu, Yamanashi 4008511, Japan.
   [Toyoura, Masahiro; Go, Kentaro; Mao, Xiaoyang] Univ Yamanashi, Dept Comp Sci & Engn, Kofu, Yamanashi 4008511, Japan.
   [Fujishiro, Issei] Keio Univ, Dept Informat & Comp Sci, Yokohama, Kanagawa 2238522, Japan.
   [Kashiwagi, Kenji] Univ Yamanashi, Dept Ophthalmol, Chuo Ku, Kofu, Yamanashi 4093898, Japan.
   [Mao, Xiaoyang] Hangzhou Dianzi Univ, Sch Comp Sci, 1156 2nd St, Hangzhou 310018, Peoples R China.
C3 University of Yamanashi; University of Yamanashi; Keio University;
   University of Yamanashi; Hangzhou Dianzi University
RP Mao, XY (corresponding author), Univ Yamanashi, Dept Comp Sci & Engn, Kofu, Yamanashi 4008511, Japan.; Mao, XY (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci, 1156 2nd St, Hangzhou 310018, Peoples R China.
EM zhuyamanashi2016@gmail.com; mtoyoura@yamanashi.ac.jp;
   fuji@ics.keio.ac.jp; go@yamanashi.ac.jp; kenjik@yamanashi.ac.jp;
   mao@yamanashi.ac.jp
RI Zhu, Zhenyang/AAZ-6197-2021; Mao, Xiaoyang/AAG-1294-2020
OI Zhu, Zhenyang/0000-0003-1023-3193; Mao, Xiaoyang/0000-0001-5010-6952;
   Toyoura, Masahiro/0000-0002-5897-7573
FU JSPS [17H00738, 19H05472, 20J15406]; Grants-in-Aid for Scientific
   Research [20J15406, 20K20408] Funding Source: KAKEN
FX This work is supported by JSPS Grants-in-Aid for Scientific Research
   (Grant Nos. 17H00738, 19H05472, and 20J15406). We would like to thank
   all patients and participants who helped evaluate LineM. Their valuable
   comments contributed to the design and improvement of the system.
   Hiromichi Ichige contributed to the implementation of the system.
CR AMSLER M, 1947, OPHTHALMOLOGICA, V114, P248, DOI 10.1159/000300476
   Friedman DS, 2004, ARCH OPHTHALMOL-CHIC, V122, P564
   Ichige H, 2019, 2019 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P187, DOI 10.1109/CW.2019.00038
   Loewenstein A, 2003, OPHTHALMOLOGY, V110, P966, DOI 10.1016/S0161-6420(03)00074-5
   Loewenstein A, 2010, RETINA-J RET VIT DIS, V30, P1058, DOI 10.1097/IAE.0b013e3181d1a75e
   Matsumoto C, 2003, INVEST OPHTH VIS SCI, V44, P4012, DOI 10.1167/iovs.03-0117
   Wang M, 2008, ACTA OPHTHALMOL, V86, P126, DOI 10.1111/j.1600-0420.2007.00889.x
   Wang YZ, 2013, INVEST OPHTH VIS SCI, V54, P5497, DOI 10.1167/iovs.13-12037
   Wang YZ, 2002, INVEST OPHTH VIS SCI, V43, P2055
   Wiecek E, 2015, INVEST OPHTH VIS SCI, V56, P494, DOI 10.1167/iovs.14-15394
   Wong WL, 2014, LANCET GLOB HEALTH, V2, pE106, DOI 10.1016/S2214-109X(13)70145-1
   Xu KY, 2018, CAN J OPHTHALMOL, V53, P168, DOI 10.1016/j.jcjo.2017.08.006
NR 12
TC 1
Z9 1
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2022
VL 38
IS 5
BP 1607
EP 1617
DI 10.1007/s00371-021-02091-9
EA FEB 2021
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0M9JP
UT WOS:000621265300001
DA 2024-07-18
ER

PT J
AU Banerjee, A
   Das, N
   Santosh, KC
AF Banerjee, Arnab
   Das, Nibaran
   Santosh, K. C.
TI Weber local descriptor for image analysis and recognition: a survey
SO VISUAL COMPUTER
LA English
DT Article
DE Weber local descriptor; Image analysis and recognition; Computer vision
   and pattern recognition
ID TEXTURE CLASSIFICATION; OBJECT DETECTION; BINARY PATTERN; MODEL;
   REPRESENTATION; FEATURES; SCALE
AB Weber local descriptor (WLD) is applied for addressing the challenges in image/pattern problems, especially in computer vision and pattern recognition domains. In this paper, we review literature on theories and applications of WLD. Using WLD, we address the different challenges of image analysis and recognition features with respect to illumination changes, contrast differences, and geometrical transformations like rotation, scaling, translation, and mirroring. Further, the role of the classifiers and experimental protocols used in the different applications are discussed. Applications include texture classification, medical imaging, agricultural safety, fingerprint analysis, forgery analysis, and face recognition.
C1 [Banerjee, Arnab; Das, Nibaran] Jadavpur Univ, Kolkata 700032, W Bengal, India.
   [Santosh, K. C.] Univ South Dakota, Vermillion, SD 57069 USA.
C3 Jadavpur University; University of South Dakota
RP Santosh, KC (corresponding author), Univ South Dakota, Vermillion, SD 57069 USA.
EM arnab.jdvu@yahoo.in; nibaran@gmail.com; santosh.kc@ieee.org
RI Santosh, KC/H-1363-2012
OI Santosh, KC/0000-0003-4176-0236; BANERJEE, ARNAB/0000-0002-3864-2784
FU SERB, GOI [SB/S3/EECE/054/2016]
FX Authors are thankful to CMATER LAB, CSE Dept., Jadavpur university for
   getting the well and good infrastructure during this work. The first
   author is also grateful to Dr. B. C. Roy Polytechnic, Durgapur for the
   overall support during the progress of this work. The work is partially
   supported by the SERB, GOI via project no SB/S3/EECE/054/2016. Also, the
   authors would like to acknowledge Sk Md Obaidullah for his initial
   discussion.
CR Agrawal D.G., 2014, INT J ENG RES APPL, V4, P502
   Alhussein M, 2016, CLUSTER COMPUT, V19, P99, DOI 10.1007/s10586-016-0535-3
   Amerini I, 2011, IEEE T INF FOREN SEC, V6, P1099, DOI 10.1109/TIFS.2011.2129512
   Banerjee A, 2017, COMM COM INF SC, V709, P277, DOI 10.1007/978-981-10-4859-3_26
   Bereta M, 2013, PATTERN RECOGN, V46, P2634, DOI 10.1016/j.patcog.2013.03.010
   Bhatt HS, 2012, IEEE T INF FOREN SEC, V7, P1522, DOI 10.1109/TIFS.2012.2204252
   Bhatt Himanshu S., 2010, 2010 Fourth IEEE International Conference on Biometrics: Theory, Applications and Systems (BTAS), P1, DOI DOI 10.1109/BTAS.2010.5634507
   Bolme DS, 2009, PROC CVPR IEEE, P2105, DOI 10.1109/CVPRW.2009.5206701
   Bourdev L, 2005, PROC CVPR IEEE, P236, DOI 10.1109/cvpr.2005.310
   Brodatz P., 1999, Textures: a photographic album for artists and designers
   Caputo B, 2005, IEEE I CONF COMP VIS, P1597, DOI 10.1109/iccv.2005.54
   Chen J, 2010, IEEE T PATTERN ANAL, V32, P1705, DOI 10.1109/TPAMI.2009.155
   Chen M, 2009, IEEE IMAGE PROC, P289, DOI 10.1109/ICIP.2009.5413511
   Dalal N., 2005, PROC IEEE COMPUT SOC, V1, P886
   Dawood H., 2012, Proceedings of the 2012 IEEE International Conference on Computer Science and Automation Engineering (CSAE 2012), P203, DOI 10.1109/CSAE.2012.6272939
   Dawood H, 2014, LECT NOTES ARTIF INT, V8467, P684, DOI 10.1007/978-3-319-07173-2_58
   DOWNTON F, 1966, BIOMETRIKA, V53, P129, DOI 10.1093/biomet/53.1-2.129
   Gaber T, 2016, COMPUT ELECTRON AGR, V122, P55, DOI 10.1016/j.compag.2015.12.022
   Galbally J, 2012, FUTURE GENER COMP SY, V28, P311, DOI 10.1016/j.future.2010.11.024
   Gong DY, 2011, 2011 FIRST ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR), P589, DOI 10.1109/ACPR.2011.6166675
   Gragnaniello Diego, 2013, Proceedings of the 2013 IEEE Workshop on Biometric Measurements and Systems for Security and Medical Applications (BIOMS), P46, DOI 10.1109/BIOMS.2013.6656148
   Graham Daniel B, 1998, CHARACTERISING VIRTU, P446
   Hadid A, 2004, PROC CVPR IEEE, P797
   Han XH, 2015, IEEE T CYBERNETICS, V45, P1180, DOI 10.1109/TCYB.2014.2346793
   Heath M, 1998, COMP IMAG VIS, V13, P457
   Hommel S., 2011, 2011 Proceedings of IEEE 12th International Symposium on Computational Intelligence and Informatics (CINTI 2011), P189, DOI 10.1109/CINTI.2011.6108497
   Huang C, 2007, IEEE I CONF COMP VIS, P177
   Hussain Muhammad, 2013, IEEE EUROCON 2013, P1570, DOI 10.1109/EUROCON.2013.6625186
   Hussain M., 2012, 2012 International Conference on Systems, Signals and Image Processing (IWSSIP), P288
   Hussain M, 2015, INT J ARTIF INTELL T, V24, DOI 10.1142/S0218213015400163
   Hussain M, 2014, 2014 IEEE INTERNATIONAL SYMPOSIUM ON INNOVATIONS IN INTELLIGENT SYSTEMS AND APPLICATIONS (INISTA 2014), P197, DOI 10.1109/INISTA.2014.6873618
   Hussain M, 2012, 8TH INTERNATIONAL CONFERENCE ON SIGNAL IMAGE TECHNOLOGY & INTERNET BASED SYSTEMS (SITIS 2012), P85, DOI 10.1109/SITIS.2012.24
   Jabid T, 2010, IEEE ICCE
   Kanade T., 2000, P 4 IEEE INT C AUT F, P46, DOI [10.1109/AFGR.2000.840611, DOI 10.1109/AFGR.2000.840611]
   Kao WC, 2010, PATTERN RECOGN, V43, P1736, DOI 10.1016/j.patcog.2009.11.016
   Klare B., 2010, P BIOM TECHN HUM ID, V7667
   Klare BF, 2011, IEEE T PATTERN ANAL, V33, P639, DOI 10.1109/TPAMI.2010.180
   Kohli N., 2012, 2012 IEEE Fifth International Conference On Biometrics: Theory, Applications And Systems (BTAS 2012), P245, DOI 10.1109/BTAS.2012.6374584
   Krasnogor N, 2005, IEEE T EVOLUT COMPUT, V9, P474, DOI 10.1109/TEVC.2005.850260
   Lan RS, 2017, IEEE T CIRC SYST VID, V27, P261, DOI 10.1109/TCSVT.2015.2492839
   Lazebnik S, 2003, PROC CVPR IEEE, P319
   Li J, 2015, LECT NOTES COMPUT SC, V9008, P541, DOI 10.1007/978-3-319-16628-5_39
   Li ST, 2013, NEUROCOMPUTING, V122, P272, DOI 10.1016/j.neucom.2013.05.038
   Lin YY, 2004, LECT NOTES COMPUT SC, V3021, P402
   Liu F, 2013, NEUROCOMPUTING, V120, P325, DOI 10.1016/j.neucom.2012.06.061
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lyons MJ, 1999, IEEE T PATTERN ANAL, V21, P1357, DOI 10.1109/34.817413
   Marasco E, 2012, PATTERN RECOGN LETT, V33, P1148, DOI 10.1016/j.patrec.2012.01.009
   Marcialis GL, 2009, LECT NOTES COMPUT SC, V5716, P12, DOI 10.1007/978-3-642-04146-4_4
   Martinez A., 1998, AR FACE DATABASE
   Muhammad G., 2012, 2012 International Conference on Systems, Signals and Image Processing (IWSSIP), P421
   Nefian A. V., 1999, Georgia tech face database
   Ng T.T., 2004, ADVENT Technical Report
   Ojala T, 2002, INT C PATT RECOG, P701, DOI 10.1109/ICPR.2002.1044854
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Ojansivu V, 2008, LECT NOTES COMPUT SC, V5099, P236, DOI 10.1007/978-3-540-69905-7_27
   Ojansivu V, 2008, INT C PATT RECOG, P3596
   ORL, 1992, ORL DATASET OLIVETTI
   Pal Anabik, 2013, Pattern Recognition and Machine Intelligence. 5th International Conference, PReMI 2013. Proceedings: LNCS 8251, P355, DOI 10.1007/978-3-642-45062-4_48
   Perner P, 1998, INT C PATT RECOG, P1677, DOI 10.1109/ICPR.1998.712043
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Roth V, 2000, ADV NEUR IN, V12, P568
   Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647
   Saadat S, 2015, J FORENSIC SCI, V60, P1451, DOI 10.1111/1556-4029.12853
   Schneiderman H, 2000, PROC CVPR IEEE, P746, DOI 10.1109/CVPR.2000.855895
   Shekar B.H., 2015, 2015 IEEE INT C SIGN, DOI 10.1109/spices.2015.7091559
   Sun SN, 2013, MATH PROBL ENG, V2013, DOI 10.1155/2013/920597
   Sun YJ, 2010, IEEE T PATTERN ANAL, V32, P1610, DOI 10.1109/TPAMI.2009.190
   Tharwat A, 2018, FISH RES, V204, P324, DOI 10.1016/j.fishres.2018.03.008
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Walia E, 2014, SIGNAL IMAGE VIDEO P, V8, P859, DOI 10.1007/s11760-012-0312-6
   Wang W, 2010, IEEE IMAGE PROC, P2101, DOI 10.1109/ICIP.2010.5652660
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P1955, DOI 10.1109/TPAMI.2008.222
   Wang XH, 2013, 2013 IEEE/SICE INTERNATIONAL SYMPOSIUM ON SYSTEM INTEGRATION (SII), P227, DOI 10.1109/SII.2013.6776664
   Xia S., 2011, Proceedings of the 22nd International Joint Conference on Artificial Intelligence, P2539, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-422
   Xie ZJ, 2011, PROCEEDINGS OF 2011 INTERNATIONAL CONFERENCE ON PUBLIC ADMINISTRATION (7TH), VOL II, P427
   Xie ZH, 2012, LECT NOTES COMPUT SC, V7202, P365, DOI 10.1007/978-3-642-31919-8_47
   Xin YQ, 2007, PATTERN RECOGN, V40, P3740, DOI 10.1016/j.patcog.2007.05.004
   Xu XK, 2013, IEEE IMAGE PROC, P4422, DOI 10.1109/ICIP.2013.6738911
   Yambay D., 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P208, DOI 10.1109/ICB.2012.6199810
   Yao Y, 2007, 2007 5TH INTERNATIONAL CONFERENCE ON MICROWAVE AND MILLIMETER WAVE TECHNOLOGY PROCEEDINGS, P7
   Yu KM, 2013, PATTERN RECOGN, V46, P2144, DOI 10.1016/j.patcog.2013.01.032
   Zhang WC, 2005, IEEE I CONF COMP VIS, P786
   Zhang Z, 2015, KNOWL-BASED SYST, V84, P78, DOI 10.1016/j.knosys.2015.04.003
   Zhao XD, 2011, LECT NOTES COMPUT SC, V6526, P12, DOI 10.1007/978-3-642-18405-5_2
   Zhou X., 2011, ACM Multimedia, P953
NR 87
TC 11
Z9 12
U1 3
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2022
VL 38
IS 1
BP 321
EP 343
DI 10.1007/s00371-020-02017-x
EA NOV 2020
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YW6DE
UT WOS:000592624500003
DA 2024-07-18
ER

PT J
AU Fan, X
   Jiang, W
   Luo, H
   Mao, WJ
AF Fan, Xing
   Jiang, Wei
   Luo, Hao
   Mao, Weijie
TI Modality-transfer generative adversarial network and dual-level unified
   latent representation for visible thermal Person re-identification
SO VISUAL COMPUTER
LA English
DT Article
DE Visible thermal person re-identification; Cross-modality;
   Modality-transfer; Feature embedding
AB Visible thermal person re-identification, also known as RGB-infrared person re-identification, is an emerging cross-modality searching problem that identifies the same person from different modalities. To solve this problem, it is necessary to know what a person looks like in different modalities. Images of the same person at the same time from the same camera view in both modalities should be captured, so that similarities and differences could be discovered. However, existing datasets do not completely satisfy those requirements. Thus, a modality-transfer generative adversarial network is proposed to generate a cross-modality counterpart for a source image in the target modality, obtaining paired images for the same person. Given that query images are from one modality and gallery images are from another modality, it is necessary to produce a unified representation for both modalities so cross-modality matching could be performed. In this study, a novel dual-level unified latent representation is proposed for visible thermal person re-identification task, including an image-level patch fusion strategy and a feature-level hierarchical granularity triplet loss, producing a more general and robust unified feature embedding. Extensive experiments on both the SYSU-MM01 dataset (with visible and near-infrared images) and the RegDB dataset (with visible and far-infrared images) demonstrate the efficiency and generality of the proposed method, which achieves state-of-the-art performance. The code will be publicly released.
C1 [Fan, Xing; Jiang, Wei; Luo, Hao; Mao, Weijie] Zhejiang Univ, State Key Lab Ind Control Technol, Hangzhou 310027, Peoples R China.
C3 Zhejiang University
RP Jiang, W (corresponding author), Zhejiang Univ, State Key Lab Ind Control Technol, Hangzhou 310027, Peoples R China.
EM jiangwei_zju@zju.edu.cn
RI jiang, wei/J-6317-2018; Fan, Xing/AAY-6418-2020
OI jiang, wei/0000-0002-9240-5851; Fan, Xing/0000-0003-2622-2210
FU National Natural Science Foundation of China [61633019]; Public Projects
   of Zhejiang Province [LGF18F030002]; Science Foundation of Chinese
   Aerospace Industry [JCKY2018204B053]
FX This work is supported by the National Natural Science Foundation of
   China (No. 61633019), the Public Projects of Zhejiang Province (No.
   LGF18F030002), and the Science Foundation of Chinese Aerospace Industry
   (JCKY2018204B053).
CR [Anonymous], 2017, PERSON TRANSFER GAN
   Bai X, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107036
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Dai PY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P677
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Ding SY, 2015, PATTERN RECOGN, V48, P2993, DOI 10.1016/j.patcog.2015.04.005
   Fan X., 2018, ARXIV E PRINTS
   Ge Y., 2018, ADV NEURAL INFORM PR
   Giachetti A, 2016, VISUAL COMPUT, V32, P693, DOI 10.1007/s00371-016-1234-z
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu SQ, 2017, IEEE INT SYMP ELEC
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Hao Y, 2019, AAAI CONF ARTIF INTE, P8385
   Hermans Alexander, 2017, ARXIV170307737
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hong K, 2016, VISUAL COMPUT, V32, P1369, DOI 10.1007/s00371-015-1164-1
   Hou XN, 2016, VISUAL COMPUT, V32, P479, DOI 10.1007/s00371-015-1079-x
   Huo YF, 2017, IEEE INT SYMP ELEC
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Jaderberg Max, 2015, ADV NEURAL INFORM PR, P8
   Kabbai L, 2019, VISUAL COMPUT, V35, P679, DOI 10.1007/s00371-018-1503-0
   Khamis S, 2015, LECT NOTES COMPUT SC, V8927, P134, DOI 10.1007/978-3-319-16199-0_10
   Li J, 2021, VISUAL COMPUT, V37, P619, DOI 10.1007/s00371-020-01828-2
   Liu JY, 2018, PROC CVPR IEEE, P3233, DOI 10.1109/CVPR.2018.00341
   Luo H., 2019, P IEEE CVF C COMP VI
   Nguyen DT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030605
   Qian XL, 2018, LECT NOTES COMPUT SC, V11213, P661, DOI 10.1007/978-3-030-01240-3_40
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Sun YH, 2017, IEEE ICC
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Varior RR, 2016, LECT NOTES COMPUT SC, V9911, P135, DOI 10.1007/978-3-319-46478-7_9
   Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8_48
   Wang GA, 2019, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2019.00372
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang Z, 2019, IEEE CONF COMPU INTE
   Wei LH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P420, DOI 10.1145/3123266.3123279
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Xia B, 2019, IEEE I CONF COMP VIS, P3759, DOI 10.1109/ICCV.2019.00386
   Ye M, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1092
   Ye M, 2018, AAAI CONF ARTIF INTE, P7501
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Zhang Xiangyu, 2017, CoRRabs/1711.08184
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zheng L, 2015, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2015.7298783
   Zheng Liang, 2017, P IEEE C COMP VIS PA, P3346, DOI DOI 10.1109/CVPR.2017.357
   Zheng Liang, 2016, arXiv preprint arXiv
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
NR 52
TC 17
Z9 18
U1 2
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2022
VL 38
IS 1
BP 279
EP 294
DI 10.1007/s00371-020-02015-z
EA NOV 2020
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YW6DE
UT WOS:000591967500001
DA 2024-07-18
ER

PT J
AU Mhala, NC
   Pais, AR
AF Mhala, Nikhil C.
   Pais, Alwyn R.
TI A secure visual secret sharing (VSS) scheme with CNN-based image
   enhancement for underwater images
SO VISUAL COMPUTER
LA English
DT Article
DE Visual secret sharing (VSS); Visual cryptography; Super-resolution;
   Underwater images; Discrete cosine transform (DCT)
ID GRAY-SCALE; CRYPTOGRAPHY
AB Nowadays, underwater images are being used to identify various important resources like objects, minerals, and valuable metals. Due to the wide availability of the Internet, we can transmit underwater images over a network. As underwater images contain important information, there is a need to transmit them securely over a network. Visual secret sharing (VSS) scheme is a cryptographic technique, which is used to transmit visual information over insecure networks. Recently proposed randomized VSS (RVSS) scheme recovers secret image (SI) with a self-similarity index (SSIM) of 60-80%. But, RVSS is suitable for general images, whereas underwater images are more complex than general images. In this paper, we propose a VSS scheme using super-resolution for sharing underwater images. Additionally, we have removed blocking artifacts from the reconstructed SI using convolution neural network (CNN)-based architecture. The proposed CNN-based architecture uses a residue image as a cue to improve the visual quality of the SI. The experimental results show that the proposed VSS scheme can reconstruct SI with almost 86-99% SSIM.
C1 [Mhala, Nikhil C.; Pais, Alwyn R.] Natl Inst Technol Karnataka, Dept Comp Sci & Engn, Informat Secur Res Lab, Mangalore, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Karnataka
RP Mhala, NC (corresponding author), Natl Inst Technol Karnataka, Dept Comp Sci & Engn, Informat Secur Res Lab, Mangalore, India.
EM mhala.nikhil@gmail.com
RI Mhala, Nikhil/P-8806-2019
OI Pais, Alwyn/0000-0003-4571-4608; MHALA, NIKHIL/0000-0003-0223-6335
CR [Anonymous], 2018, IEEE Trans. Neural Netw. Learn. Syst., DOI DOI 10.1109/TNNLS.2018.2798162
   Ateniese G, 1996, INFORM COMPUT, V129, P86, DOI 10.1006/inco.1996.0076
   Ateniese G, 2001, THEOR COMPUT SCI, V250, P143, DOI 10.1016/S0304-3975(99)00127-9
   Bertsekas D. P., 1999, NONLINEAR PROGRAMMIN
   Chang CC, 2002, INFORM SCIENCES, V141, P123, DOI 10.1016/S0020-0255(01)00194-3
   Chen SK, 2009, OPT ENG, V48, DOI 10.1117/1.3262345
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Duarte A, 2016, OCEANS-IEEE
   Eskicioglu AM, 1995, IEEE T COMMUN, V43, P2959, DOI 10.1109/26.477498
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fang WP, 2008, PATTERN RECOGN, V41, P1410, DOI 10.1016/j.patcog.2007.09.004
   Fu K., 2020, P IEEE CVF C COMP VI, P3052
   Fu KR, 2019, NEUROCOMPUTING, V356, P69, DOI 10.1016/j.neucom.2019.04.062
   Glasby GP, 2000, SCIENCE, V289, P551, DOI 10.1126/science.289.5479.551
   Gujjunoori S., 2013, BUSYEMBED HVS BASED
   Gujjunoori S, 2013, J INF SECUR APPL, V18, P157, DOI 10.1016/j.istr.2013.01.002
   Halfar J, 2007, SCIENCE, V316, P987, DOI 10.1126/science.1138289
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou YC, 2013, INFORM SCIENCES, V233, P290, DOI 10.1016/j.ins.2013.01.006
   Hou YC, 2011, IEEE T CIRC SYST VID, V21, P1760, DOI 10.1109/TCSVT.2011.2106291
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mhala NC, 2019, SIGNAL PROCESS, V162, P253, DOI 10.1016/j.sigpro.2019.04.023
   Mhala NC, 2019, INT CONF COMMUN SYST, P823, DOI [10.1109/comsnets.2019.8711327, 10.1109/COMSNETS.2019.8711327]
   Mhala NC, 2018, IET IMAGE PROCESS, V12, P422, DOI 10.1049/iet-ipr.2017.0759
   Naor M, 1995, Advances in cryptographyEurocrypt'94. Vis lecture notes in computer science, V950, P1, DOI [DOI 10.1007/BFB0053419, 10.1007/BFb0053419, DOI 10.1007/978-1-4939-9484-7_1]
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Shivani S, 2017, MULTIMED TOOLS APPL, P1
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Wang RZ, 2009, IEEE SIGNAL PROC LET, V16, P659, DOI 10.1109/LSP.2009.2021334
   Wang RZ, 2007, 2 INT C INN COMP INF, P283
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZM, 2009, IEEE T INF FOREN SEC, V4, P383, DOI 10.1109/TIFS.2009.2024721
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhou Z, 2006, IEEE T IMAGE PROCESS, V15, P2441, DOI 10.1109/TIP.2006.875249
NR 38
TC 14
Z9 14
U1 1
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2021
VL 37
IS 8
BP 2097
EP 2111
DI 10.1007/s00371-020-01972-9
EA SEP 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TT5FX
UT WOS:000569242200001
DA 2024-07-18
ER

PT J
AU Fischer, R
   Dittmann, P
   Weller, R
   Zachmann, G
AF Fischer, Roland
   Dittmann, Philipp
   Weller, Rene
   Zachmann, Gabriel
TI AutoBiomes: procedural generation of multi-biome landscapes
SO VISUAL COMPUTER
LA English
DT Article
DE Procedural content generation; Terrain generation; Virtual worlds;
   Biomes; Climate simulation; Digital elevation models
AB Advances in computer technology and increasing usage of computer graphics in a broad field of applications lead to rapidly rising demands regarding size and detail of virtual landscapes. Manually creating huge, realistic looking terrains and populating them densely with assets is an expensive and laborious task. In consequence, (semi-)automatic procedural terrain generation is a popular method to reduce the amount of manual work. However, such methods are usually highly specialized for certain terrain types and especially the procedural generation of landscapes composed of different biomes is a scarcely explored topic. We present a novel system, called AutoBiomes, which is capable of efficiently creating vast terrains with plausible biome distributions and therefore different spatial characteristics. The main idea is to combine several synthetic procedural terrain generation techniques with digital elevation models (DEMs) and a simplified climate simulation. Moreover, we include an easy-to-use asset placement component which creates complex multi-object distributions. Our system relies on a pipeline approach with a major focus on usability. Our results show that our system allows the fast creation of realistic looking terrains.
C1 [Fischer, Roland; Dittmann, Philipp] Univ Bremen, Dept Comp Graph & Virtual Real, Bremen, Germany.
   [Weller, Rene] Univ Bremen, Comp Graph Grp, Bremen, Germany.
   [Zachmann, Gabriel] Univ Bremen, Comp Graph Visual Comp & Virtual Real, Bremen, Germany.
C3 University of Bremen; University of Bremen; University of Bremen
RP Fischer, R (corresponding author), Univ Bremen, Dept Comp Graph & Virtual Real, Bremen, Germany.
EM rfischer@cs.uni-bremen.de; dittmann@cs.uni-bremen.de;
   weller@informatik.uni-bremen.de; zach@cs.uni-bremen.de
RI Zachmann, Gabriel/AAI-9685-2020
OI Zachmann, Gabriel/0000-0001-8155-1127
FU Projekt DEAL
FX Open Access funding provided by Projekt DEAL.
CR Amato Alba, 2017, GAME DYNAMICS, P15, DOI DOI 10.1007/978-3-319-53088-8_2
   [Anonymous], 2017, ARXIV170703383
   Deussen O., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P275, DOI 10.1145/280814.280898
   Galin E., 2019, COMPUTER GRAPHICS FO
   Gamito MN, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640451
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Génevaux JD, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461996
   Guérin E, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130804
   Hendrikx M, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2422956.2422957
   Hyttinen T., 2017, THESIS
   Ihmsen M., 2014, Eurographics 2014-State of the Art Reports
   Jako B., 2011, EUROGRAPHICS 2011 SH
   Jones T. R., 2006, Journal of Graphics Tools, V11, P27
   Lane B, 2002, PROC GRAPH INTERF, P69
   Musgrave F. K., 1989, Computer Graphics, V23, P41, DOI 10.1145/74334.74337
   Peck J, 2016, FASTNOISE SIMD
   Perlin K, 2002, ACM T GRAPHIC, V21, P681, DOI 10.1145/566570.566636
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   Radford A., 2015, ARXIV
   Smelik RM, 2014, COMPUT GRAPH FORUM, V33, P31, DOI 10.1111/cgf.12276
   Stam J., 2003, P GAM DEV C
   WHITTAKER R H, 1975, P385
   Wulff-Jensen A, 2018, L N INST COMP SCI SO, V229, P85, DOI 10.1007/978-3-319-76908-0_9
   Zhou H, 2007, IEEE T VIS COMPUT GR, V13, P834, DOI 10.1109/TVCG.2007.1027
NR 24
TC 5
Z9 5
U1 1
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2263
EP 2272
DI 10.1007/s00371-020-01920-7
EA JUL 2020
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000552157800001
OA hybrid
DA 2024-07-18
ER

EF