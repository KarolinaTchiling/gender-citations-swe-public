FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Zhang, YB
   Xu, L
   Xiang, HB
   Kong, HH
   Bi, JH
   Han, C
AF Zhang, Yubo
   Xu, Lei
   Xiang, Haibin
   Kong, Haihua
   Bi, Junhao
   Han, Chao
TI LKSMN: Large Kernel Spatial Modulation Network for Lightweight Image
   Super-Resolution
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image super-resolution; Self-attention; Convolutional modulation;
   Large-kernel convolution decomposition
AB Although Vits-based networks have achieved stunning results in image super-resolution, their self-attention (SA) modeling in the unidimension greatly limits the reconstruction performance. In addition, the high consumption of resources for SA limits its application scenarios. In this study, we explore the working mechanism of SA and redesign its key structures to retain powerful modeling capabilities while reducing resource consumption. Further, we propose large kernel spatial modulation network (LKSMN); it can leverage the complementary strengths of attention from spatial and channel dimensions to mine a fuller range of potential correlations. Specifically, three effective designs were included in LKSMN. First, we propose multi-scale spatial modulation attention (MSMA) based on convolutional modulation (CM) and large-kernel convolution decomposition (LKCD). Instead of generating feature-relevance scores via queries and keys in the SA, MSMA uses LKCD to act directly on the input features to produce convolutional features that imitate relevance scores matrix. This process reduces the computational and storage overhead of the SA while retaining its ability to robustly model long-range dependent correlations. Second, we introduce multi-dconv head transposed attention (MDTA) as an attention modeling scheme in the channel dimension, which complements the advantages of our MSMA to model pixel interactions in both dimensions simultaneously. Final, we propose a multi-level feature aggregation module (MLFA) for aggregating the feature information extracted from different depth modules located in the network, to avoid the problem of shallow feature information disappearance. Extensive experiments demonstrate that our proposed method can achieve competitive results with a small network scale (e.g., 26.33dB@Urban100 x\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\times $$\end{document} 4 with only 253K parameters). The code is available at https://figshare.com/articles/software/LKSMN_Large_Kernel_Spatial_Modulation_Network_for_Lightweight_Image_Super-Resolution/25603893
C1 [Zhang, Yubo; Xu, Lei; Xiang, Haibin; Kong, Haihua; Bi, Junhao; Han, Chao] Northeast Petr Univ, Sch Elect Informat Engn, Daqing 163000, Peoples R China.
C3 Northeast Petroleum University
RP Zhang, YB (corresponding author), Northeast Petr Univ, Sch Elect Informat Engn, Daqing 163000, Peoples R China.
EM zhangyubo@nepu.edu.cn; xl442242883@163.com
FU Heilongjiang Province, China [1507202202]
FX This work was supported by the basic research projects of under graduate
   universities affiliated with Heilongjiang Province, China (1507202202).
   DAS:The datasets analyzed during the current study are available in the
   public dataset. And the different algorithms' results performed in
   datasets during the current study are available from the public paper or
   the corresponding author on reasonable request.
CR Bastidas AA, 2019, IEEE COMPUT SOC CONF, P881, DOI 10.1109/CVPRW.2019.00117
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Chen XY, 2023, PROC CVPR IEEE, P22367, DOI 10.1109/CVPR52729.2023.02142
   Conde M.V., 2022, EUR C COMP VIS, P669
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Ding XH, 2022, PROC CVPR IEEE, P11953, DOI 10.1109/CVPR52688.2022.01166
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong WS, 2011, IEEE T IMAGE PROCESS, V20, P1838, DOI 10.1109/TIP.2011.2108306
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Fan HQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6804, DOI 10.1109/ICCV48922.2021.00675
   Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075
   Gu JJ, 2021, PROC CVPR IEEE, P9195, DOI 10.1109/CVPR46437.2021.00908
   Guo MH, 2023, COMPUT VIS MEDIA, V9, P733, DOI 10.1007/s41095-023-0364-2
   He K., 2015, ICCV, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hou Q., 2024, IEEE T PATTERN ANAL
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Huang S, 2023, VISUAL COMPUT, V39, P3647, DOI 10.1007/s00371-023-02938-3
   Jingwei Xin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P91, DOI 10.1007/978-3-030-58548-8_6
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D. P., 2014, arXiv
   Kong XT, 2021, PROC CVPR IEEE, P12011, DOI 10.1109/CVPR46437.2021.01184
   Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304
   Li JC, 2018, LECT NOTES COMPUT SC, V11212, P527, DOI 10.1007/978-3-030-01237-3_32
   Li W., 2020, Advances in Neural Information Processing Systems, V33, P20343
   Li X., 2023, P IEEE CVF INT C COM, P12792
   Li ZY, 2022, IEEE COMPUT SOC CONF, P832, DOI 10.1109/CVPRW56347.2022.00099
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu D, 2018, ADV NEUR IN, V31
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Loshchilov I., 2016, arXiv
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Mei YQ, 2021, PROC CVPR IEEE, P3516, DOI 10.1109/CVPR46437.2021.00352
   Mei YQ, 2020, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR42600.2020.00573
   Srinivas A, 2021, PROC CVPR IEEE, P16514, DOI 10.1109/CVPR46437.2021.01625
   Sun L., 2023, P IEEE CVF INT C COM, P13190
   Sun L., 2022, Advances in Neural Information Processing Systems, V35, P17314
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Wan C., 2024, P IEEE CVF C COMP VI, P6246
   Wang H, 2023, PROC CVPR IEEE, P22378, DOI 10.1109/CVPR52729.2023.02143
   Wang L, 2021, J VIS COMMUN IMAGE R, V80, DOI 10.1016/j.jvcir.2021.103300
   Wang LG, 2021, PROC CVPR IEEE, P4915, DOI 10.1109/CVPR46437.2021.00488
   Wang SL, 2012, PROC CVPR IEEE, P2216, DOI 10.1109/CVPR.2012.6247930
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xiaotong Luo, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P272, DOI 10.1007/978-3-030-58542-6_17
   Xie C., 2023, P IEEE CVF C COMP VI, P1283
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xin JW, 2023, INT J COMPUT VISION, V131, P1808, DOI 10.1007/s11263-023-01789-8
   Xin JW, 2022, IEEE T NEUR NET LEAR, V33, P707, DOI 10.1109/TNNLS.2020.3028688
   Yoo J, 2023, IEEE WINT CONF APPL, P4945, DOI 10.1109/WACV56688.2023.00493
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zeyde R., 2012, INT C CURV SURF, P711
   Zhang K., 2017, Proceedings of the IEEE 2017 Conference on Computer Vision and Pattern Recognition, DOI [DOI 10.1109/CVPR.2017.300, 10.1109/CVPR.2017.300]
   Zhang KB, 2012, IEEE T IMAGE PROCESS, V21, P4544, DOI 10.1109/TIP.2012.2208977
   Zhang L, 2006, IEEE T IMAGE PROCESS, V15, P2226, DOI 10.1109/TIP.2006.877407
   Zhang XD, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4034, DOI 10.1145/3474085.3475291
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhao H., 2020, EUROPEAN C COMPUTER, P56, DOI DOI 10.1007/978-3-030-67070-23
   ZHOu L., 2022, EUR C COMP VIS, V13802, P256, DOI [DOI 10.1007/978-3-031-25063-7_16, 10.1007/978-3-031-25063-7_16]
   Zhou Y, 2023, IEEE T NEUR NET LEAR, V34, P7719, DOI 10.1109/TNNLS.2022.3146004
NR 71
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 25
PY 2024
DI 10.1007/s00371-024-03562-5
EA JUL 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZN6T2
UT WOS:001276024300001
DA 2024-08-05
ER

PT J
AU Yamada, R
   Tsuji, T
   Hiramitsu, T
   Seki, H
   Nishimura, T
   Suzuki, Y
   Watanabe, T
AF Yamada, Ryunosuke
   Tsuji, Tokuo
   Hiramitsu, Tatsuhiro
   Seki, Hiroaki
   Nishimura, Toshihiro
   Suzuki, Yosuke
   Watanabe, Tetsuyou
TI Fast and precise approximation of Minkowski sum of two rotational
   ellipsoids with a superellipsoid
SO VISUAL COMPUTER
LA English
DT Article
DE Minkowski sum; Ellipsoid; Superellipsoid; Collision detection; Obstacle
   avoidance
ID GAS FLUIDIZATION
AB In this paper, we propose a fast and accurate method for approximating the Minkowski sum of two rotational ellipsoids with a superellipsoid. The Minkowski sum is used in a variety of applications such as robot motion planning and particle flow simulation requiring collision detection. Many of them are computed based on Minkowski sum, whose accuracy and processing time depend on how the mesh is created. We approximate Minkowski sum with a superellipsoid function. The superellipsoid has various shapes with the value of the exponents, and the computation of the parameters including the exponents is an algebraic computation with the time complexity O ( 1 ) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\varvec{O}(1)$$\end{document} . As a result, approximation error is small and computation is fast in all combinations of rotational ellipsoids. In addition, collision is quickly detected by solving the inequality of superellipsoid.
C1 [Yamada, Ryunosuke; Tsuji, Tokuo; Hiramitsu, Tatsuhiro; Seki, Hiroaki; Nishimura, Toshihiro; Suzuki, Yosuke; Watanabe, Tetsuyou] Kanazawa Univ, Kanazawa, Ishikawa, Japan.
C3 Kanazawa University
RP Yamada, R; Tsuji, T (corresponding author), Kanazawa Univ, Kanazawa, Ishikawa, Japan.
EM noboriryuu0623@stu.kanazawa-u.ac.jp; tokuo-tsuji@se.kanazawa-u.ac.jp;
   thiramitsu@se.kanazawa-u.ac.jp; hseki@se.kanazawa-u.ac.jp;
   tnishimura@se.kanazawa-u.ac.jp; suzuki@se.kanazawa-u.ac.jp;
   twata@se.kanazawa-u.ac.jp
RI ; Watanabe, Tetsuyou/L-3532-2015; Tsuji, Tokuo/I-5112-2017
OI Nishimura, Toshihiro/0000-0003-2858-6368; Watanabe,
   Tetsuyou/0000-0003-2549-1435; Tsuji, Tokuo/0000-0002-1573-1928;
   Hiramitsu, Tatsuhiro/0000-0002-1053-583X; Seki,
   Hiroaki/0000-0003-1588-4825
FU Kanazawa University
FX No Statement Available
CR Ammoun S, 2009, INT C INTELL COMP CO, P417, DOI 10.1109/ICCP.2009.5284727
   Best A, 2016, IEEE INT CONF ROBOT, P298, DOI 10.1109/ICRA.2016.7487148
   Brito B, 2019, IEEE ROBOT AUTOM LET, V4, P4459, DOI 10.1109/LRA.2019.2929976
   Chan CC, 2020, IEEE ACCESS, V8, P80120, DOI 10.1109/ACCESS.2020.2988654
   Chirikjian GS, 2021, INT J MATH, V32, DOI 10.1142/S0129167X21400097
   Eberly D., 1999, DISTANCE POINT TRIAN
   Gan JQ, 2016, AICHE J, V62, P62, DOI 10.1002/aic.15050
   Halder A, 2018, IEEE DECIS CONTR P, P4040, DOI 10.1109/CDC.2018.8619508
   Hwang KS, 2003, IEEE T IND ELECTRON, V50, P385, DOI 10.1109/TIE.2003.809406
   Ilin DN, 2016, MATEC WEB CONF, V80, DOI 10.1051/matecconf/20168002004
   Kurzhanskii A.B., 1997, Ellipsoidal Calculus for Estimation and Control
   Qin XL, 2019, COMPUT OPTIM APPL, V74, P821, DOI 10.1007/s10589-019-00124-7
   Ruan SP, 2022, COMPUT AIDED DESIGN, V143, DOI 10.1016/j.cad.2021.103133
   Tsuji T, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P1830, DOI 10.1109/IROS.2009.5354501
   Yan Y, 2015, GEOMETRIAE DEDICATA, V177, P103, DOI 10.1007/s10711-014-9981-3
   Zhou ZY, 2011, CHEM ENG SCI, V66, P6128, DOI 10.1016/j.ces.2011.08.041
NR 16
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4609
EP 4621
DI 10.1007/s00371-024-03445-9
EA JUN 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001248641200001
OA hybrid
DA 2024-08-05
ER

PT J
AU Zhang, G
   Geng, Y
   Gong, ZG
AF Zhang, Gang
   Geng, Yang
   Gong, Zhao G.
TI A comprehensive review of deep learning approaches for group activity
   analysis
SO VISUAL COMPUTER
LA English
DT Review; Early Access
DE Group activity analysis; Group detection; Group feature extraction;
   Group activity recognition
ID TRACKING; MODELS
AB The study of group activity analysis has garnered significant attention. Group activity offers a unique perspective on the relationships between individuals, providing insights that individual and crowd activities may not reveal. This paper aims to contribute to the existing body of knowledge by providing a comprehensive review of the methods employed in utilizing deep learning for the analysis of group activity. The review encompasses an overview of various methodologies for group detection, segmentation, feature extraction, and description. Additionally, it delves into the classification, recognition, and prediction of group activities, including crowd trajectory prediction. The representation of crowd activity patterns and labels, along with an exploration of datasets for crowd activity analysis, is also included. Ultimately, the paper concludes with a discussion of potential future research directions in the field. By offering a comprehensive review of the advancements in group activity analysis through the lens of deep learning, this paper aims to provide researchers with a better understanding of the field's progress, thereby contributing to the continued development of this area of study.
C1 [Zhang, Gang] Shenyang Univ Technol, Sch Software, Shenyang, Peoples R China.
   [Geng, Yang] Shenyang Univ Technol, Sch Informat Sci & Engn, Shenyang, Peoples R China.
   [Gong, Zhao G.] Shenyang Univ Technol, Sci & Technol Res Inst, Shenyang, Peoples R China.
C3 Shenyang University of Technology; Shenyang University of Technology;
   Shenyang University of Technology
RP Zhang, G (corresponding author), Shenyang Univ Technol, Sch Software, Shenyang, Peoples R China.
EM zhang_gang_1973@yahoo.com
FU Applied Basic Research Pro-gram of Liaoning Provincial Department of
   Science and Technology of China [2022JH2/101300245]; Basic scientific
   research projects of Liaoning Provincial Department of Education of
   China [LJKZ0146]
FX This work was supported by the Applied Basic Research Pro-gram of
   Liaoning Provincial Department of Science and Technology of China (Grant
   Number: 2022JH2/101300245) and the Basic scientific research projects of
   Liaoning Provincial Department of Education of China (Grant Number:
   LJKZ0146)
CR Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110
   Allain P., 2012, P INT WORKSH PATT RE, P1
   [Anonymous], UNUSUAL CROWD ACTIVI
   Aouaidjia K, 2021, IEEE T SYST MAN CY-S, V51, P2774, DOI 10.1109/TSMC.2019.2916896
   Artacho B, 2020, PROC CVPR IEEE, P7033, DOI 10.1109/CVPR42600.2020.00706
   Bae I, 2022, LECT NOTES COMPUT SC, V13682, P270, DOI 10.1007/978-3-031-20047-2_16
   Ballerini M, 2008, P NATL ACAD SCI USA, V105, P1232, DOI 10.1073/pnas.0711437105
   Bansal N, 2004, MACH LEARN, V56, P89, DOI 10.1023/B:MACH.0000033116.57574.95
   Bendali-Braham M, 2021, MACH LEARN APPL, V4, DOI 10.1016/j.mlwa.2021.100023
   Nievas EB, 2011, LECT NOTES COMPUT SC, V6855, P332, DOI 10.1007/978-3-642-23678-5_39
   Berndt DJ., 1994, P KDD WORKSH SEATTL, P359, DOI DOI 10.5555/3000850.3000887
   Bertasius G, 2021, PR MACH LEARN RES, V139
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chai LY, 2022, IEEE T PATTERN ANAL, V44, P2856, DOI 10.1109/TPAMI.2020.3043372
   Cheng K, 2020, PROC CVPR IEEE, P180, DOI 10.1109/CVPR42600.2020.00026
   Choi W., 2009, P IEEE INT C COMP VI, P1
   Cunjun Yu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P507, DOI 10.1007/978-3-030-58610-2_30
   Dehghan A, 2018, IEEE T PATTERN ANAL, V40, P568, DOI 10.1109/TPAMI.2017.2687462
   Dendorfer P., 2020, arXiv
   [邓海刚 Deng Haigang], 2022, [电子学报, Acta Electronica Sinica], V50, P2018
   Deng ZW, 2016, PROC CVPR IEEE, P4772, DOI 10.1109/CVPR.2016.516
   Dupont C, 2017, IEEE COMPUT SOC CONF, P2184, DOI 10.1109/CVPRW.2017.271
   Duta I., 2021, P ADV NEURAL INFORM, P7111
   Ehsanpour Mahsa, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P177, DOI 10.1007/978-3-030-58545-7_11
   Ehsanpour M, 2022, PROC CVPR IEEE, P20951, DOI 10.1109/CVPR52688.2022.02031
   Fan HQ, 2016, Arxiv, DOI [arXiv:1612.00603, 10.48550/arXiv.1612.00603 1612.00603]
   Borja-Borja LF, 2017, LECT NOTES COMPUT SC, V10306, P294, DOI 10.1007/978-3-319-59147-6_26
   Gao Z, 2019, IEEE INTERNET THINGS, V6, P9280, DOI 10.1109/JIOT.2019.2911669
   Ge WN, 2012, IEEE T PATTERN ANAL, V34, P1003, DOI 10.1109/TPAMI.2011.176
   GRANGER CWJ, 1969, ECONOMETRICA, V37, P424, DOI 10.2307/1912791
   Graves A, 2014, Arxiv, DOI [arXiv:1308.0850, DOI 10.48550/ARXIV.1308.0850]
   Gupta A, 2018, PROC CVPR IEEE, P2255, DOI 10.1109/CVPR.2018.00240
   Hamilton WL, 2017, ADV NEUR IN, V30
   Han MF, 2022, PROC CVPR IEEE, P2980, DOI 10.1109/CVPR52688.2022.00300
   Han R, 2022, LECT NOTES COMPUT SC, V13664, P244, DOI 10.1007/978-3-031-19772-7_15
   Hassner T., 2012, Computer Vision and Pattern Recognition Workshops (CVPRW), 2012 IEEE Computer Society Conference on, P1, DOI [10.1109/CVPRW.2012.6239348, DOI 10.1109/CVPRW.2012.6239348]
   Heeseung Kwon, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P345, DOI 10.1007/978-3-030-58517-4_21
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI [10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166]
   Ibrahim MS, 2016, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2016.217
   Kamel A, 2021, IEEE T MULTIMEDIA, V23, P1330, DOI 10.1109/TMM.2020.2999181
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kim D, 2022, PROC CVPR IEEE, P20051, DOI 10.1109/CVPR52688.2022.01945
   Kok VJ, 2016, NEUROCOMPUTING, V177, P342, DOI 10.1016/j.neucom.2015.11.021
   Koniusz P, 2022, IEEE T PATTERN ANAL, V44, P648, DOI 10.1109/TPAMI.2021.3107160
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Li JC, 2022, LECT NOTES COMPUT SC, V13695, P142, DOI 10.1007/978-3-031-19833-5_9
   Li J, 2019, IEEE T MULTIMEDIA, V21, P2531, DOI 10.1109/TMM.2019.2908350
   Li SC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13648, DOI 10.1109/ICCV48922.2021.01341
   Li T, 2015, IEEE T CIRC SYST VID, V25, P367, DOI 10.1109/TCSVT.2014.2358029
   Li XL, 2020, IEEE T IMAGE PROCESS, V29, P5571, DOI 10.1109/TIP.2020.2985284
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Martín-Martín R, 2023, IEEE T PATTERN ANAL, V45, P6748, DOI 10.1109/TPAMI.2021.3070543
   Mei L, 2019, IEEE INT CONF COMP V, P1222, DOI 10.1109/ICCVW.2019.00155
   Min Kyle, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P283, DOI 10.1007/978-3-030-58568-6_17
   Mohamed Abduallah, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14412, DOI 10.1109/CVPR42600.2020.01443
   Moussaïd M, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010047
   Niknejad M, 2015, IEEE T IMAGE PROCESS, V24, P3624, DOI 10.1109/TIP.2015.2447836
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Patino L, 2017, IEEE COMPUT SOC CONF, P2126, DOI 10.1109/CVPRW.2017.264
   Peng XJ, 2016, LECT NOTES COMPUT SC, V9908, P744, DOI 10.1007/978-3-319-46493-0_45
   Qi C. R., 2016, CoRR, DOI DOI 10.48550/ARXIV.1612.00593
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shao J, 2015, PROC CVPR IEEE, P4657, DOI 10.1109/CVPR.2015.7299097
   Shao J, 2014, PROC CVPR IEEE, P2227, DOI 10.1109/CVPR.2014.285
   Sharif Md Haidar, 2022, arXiv
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Shu TM, 2017, PROC CVPR IEEE, P4255, DOI 10.1109/CVPR.2017.453
   Shu XB, 2021, IEEE T NEUR NET LEAR, V32, P663, DOI 10.1109/TNNLS.2020.2978942
   Simonyan K, 2014, ADV NEUR IN, V27
   Solera F, 2016, IEEE T PATTERN ANAL, V38, P995, DOI 10.1109/TPAMI.2015.2470658
   Su R, 2021, IEEE T PATTERN ANAL, V43, P4477, DOI 10.1109/TPAMI.2020.2997860
   Sun JK, 2022, IEEE T KNOWL DATA EN, V34, P2348, DOI 10.1109/TKDE.2020.3008774
   Tamura M, 2022, LECT NOTES COMPUT SC, V13664, P19, DOI 10.1007/978-3-031-19772-7_2
   Tang JH, 2022, IEEE T PATTERN ANAL, V44, P636, DOI 10.1109/TPAMI.2019.2928540
   Tripathi G, 2019, VISUAL COMPUT, V35, P753, DOI 10.1007/s00371-018-1499-5
   Turner J.C., 1982, SOCIAL IDENTITY INTE, DOI DOI 10.1016/J.VACCINE.2011.08.002
   Tzelepi M, 2021, IEEE T EM TOP COMP I, V5, P191, DOI 10.1109/TETCI.2019.2897815
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Q, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01365-4
   Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839
   Wang Q, 2020, IEEE T PATTERN ANAL, V42, P46, DOI 10.1109/TPAMI.2018.2875002
   Wang XY, 2020, PROC CVPR IEEE, P3265, DOI 10.1109/CVPR42600.2020.00333
   Waqar S, 2022, MULTIMED TOOLS APPL, V81, P43947, DOI 10.1007/s11042-022-13227-x
   Wu Jian-Chao, 2023, Journal of Software, P964, DOI 10.13328/j.cnki.jos.006693
   Wu JC, 2019, PROC CVPR IEEE, P9956, DOI 10.1109/CVPR.2019.01020
   Wu LF, 2021, INT J AUTOM COMPUT, V18, P334, DOI 10.1007/s11633-020-1258-8
   Yan R, 2022, IEEE T NEUR NET LEAR, V33, P7574, DOI 10.1109/TNNLS.2021.3085567
   Yan R, 2023, IEEE T PATTERN ANAL, V45, P6955, DOI 10.1109/TPAMI.2020.3034233
   Yuan HJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7456, DOI 10.1109/ICCV48922.2021.00738
   Zeng RH, 2022, IEEE T PATTERN ANAL, V44, P6209, DOI 10.1109/TPAMI.2021.3090167
   Zhan XH, 2018, LECT NOTES COMPUT SC, V11213, P576, DOI 10.1007/978-3-030-01240-3_35
   Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684
   Zhang D, 2020, PROC CVPR IEEE, P3881, DOI 10.1109/CVPR42600.2020.00394
   Zhang Q, 2022, PROC CVPR IEEE, P7502, DOI 10.1109/CVPR52688.2022.00736
   Zhao T, 2023, IEEE T PATTERN ANAL, V45, P3019, DOI 10.1109/TPAMI.2022.3178957
   Zhou Rui, 2022, 2022 International Conference on Robotics and Automation (ICRA), P805, DOI 10.1109/ICRA46639.2022.9811585
   Zhu X., 2021, ICLR, P1, DOI DOI 10.48550/ARXIV.2010.04159
NR 98
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 30
PY 2024
DI 10.1007/s00371-024-03479-z
EA MAY 2024
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO8V9
UT WOS:001235494300003
DA 2024-08-05
ER

PT J
AU Ji, JZ
   Zhao, RF
   Lei, ML
AF Ji, Junzhong
   Zhao, Runfeng
   Lei, Minglong
TI Latent diffusion transformer for point cloud generation
SO VISUAL COMPUTER
LA English
DT Article
DE 3D; Point cloud generation; Diffusion model; Transformers
AB Diffusion models have been successfully applied to point cloud generation tasks recently. The main notion is using a forward process to progressively add noises into point clouds and then use a reverse process to generate point clouds by denoising these noises. However, since point cloud data is high-dimensional and exhibits complex structures, it is challenging to adequately capture the surface distribution of point clouds. Moreover, point cloud generation methods often resort to sampling methods and local operations to extract features, which inevitably ignores the global structures and overall shapes of point clouds. To address these limitations, we propose a latent diffusion model based on Transformers for point cloud generation. Instead of directly building a diffusion process based on the points, we first propose a latent compressor to convert original point clouds into a set of latent tokens before feeding them into diffusion models. Converting point clouds as latent tokens not only improves expressiveness, but also exhibits better flexibility since they can adapt to various downstream tasks. We carefully design the latent compressor based on an attention-based auto-encoder architecture to capture global structures in point clouds. Then, we propose to use transformers as the backbones of the latent diffusion module to maintain global structures. The powerful feature extraction ability of transformers guarantees the high quality and smoothness of generated point clouds. Experiments show that our method achieves superior performance in both unconditional generation on ShapeNet and multi-modal point cloud completion on ShapeNet-ViPC. Our code and samples are publicly available at https://github.com/Negai-98/LDT.
C1 [Ji, Junzhong; Zhao, Runfeng; Lei, Minglong] Beijing Univ Technol, Beijing Municipal Key Lab Multimedia & Intelligent, Beijing 100124, Peoples R China.
   [Ji, Junzhong; Zhao, Runfeng; Lei, Minglong] Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
C3 Beijing University of Technology; Beijing University of Technology
RP Lei, ML (corresponding author), Beijing Univ Technol, Beijing Municipal Key Lab Multimedia & Intelligent, Beijing 100124, Peoples R China.; Lei, ML (corresponding author), Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
EM jjz01@bjut.edu.cn; zhaottyxx@emails.bjut.edu.cn; leiml@bjut.edu.cn
FU National Natural Science Foundation of China [4222020]; Beijing Natural
   Science Foundation [62241601]; National Natural Science Foundation of
   China
FX This work was supported in part by the Beijing Natural Science
   Foundation (Grant No. 4222020), and in part by the National Natural
   Science Foundation of China (Grant No. 62241601).
CR Aiello E., 2022, ADV NEURAL INF PROCE, V35, P37349
   Cai Ruojin, 2020, EUR C COMP VIS, P364, DOI [DOI 10.1007/978-3-030-58580-822, DOI 10.1007/978-3-030-58580-8_22]
   Chai S, 2023, PROC CVPR IEEE, P18349, DOI 10.1109/CVPR52729.2023.01760
   Chang A.X., 2015, ArXiv
   Chang HW, 2022, PROC CVPR IEEE, P11305, DOI 10.1109/CVPR52688.2022.01103
   Chen R, 2019, IEEE I CONF COMP VIS, P1538, DOI 10.1109/ICCV.2019.00162
   Chen ZH, 2023, IEEE T PATTERN ANAL, V45, P13489, DOI 10.1109/TPAMI.2023.3293885
   Chen ZH, 2021, VISUAL COMPUT, V37, P2657, DOI 10.1007/s00371-021-02199-y
   Cheng AC, 2022, LECT NOTES COMPUT SC, V13663, P89, DOI 10.1007/978-3-031-20062-5_6
   Cho J., 2023, DALL EVAL PROBING RE, P3020, DOI [10.1109/ICCV51070.2023.00283, DOI 10.1109/ICCV51070.2023.00283]
   Dhariwal P, 2021, ADV NEUR IN, V34
   Dinh L., 2015, Nice: Non-linear independent components estimation
   Dosovitskiy A., 2021, ICLR
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Harvey W, 2022, ADV NEURAL INF PROCE, P27953
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Ho J, 2022, J MACH LEARN RES, V23, P1
   Huang R., 2022, IJCAI, P4157
   Huang RJ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2595, DOI 10.1145/3503161.3547855
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Kim H., 2020, Advances in Neural Information Processing Systems, P16388
   Kim J, 2021, PROC CVPR IEEE, P15054, DOI 10.1109/CVPR46437.2021.01481
   Klokov R., 2020, EUROPEAN C COMPUTER, V12368, P694
   Lai X, 2022, PROC CVPR IEEE, P8490, DOI 10.1109/CVPR52688.2022.00831
   Lee J, 2019, PR MACH LEARN RES, V97
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu Q, 2022, VISUAL COMPUT, V38, P3341, DOI 10.1007/s00371-022-02550-x
   Liu ZJ, 2019, ADV NEUR IN, V32
   Luo ST, 2021, PROC CVPR IEEE, P2836, DOI 10.1109/CVPR46437.2021.00286
   Lyu Z., 2022, INT C LEARNING REPRE
   Ma BR, 2022, PROC CVPR IEEE, P6305, DOI 10.1109/CVPR52688.2022.00621
   Peebles W, 2023, IEEE I CONF COMP VIS, P4172, DOI 10.1109/ICCV51070.2023.00387
   Peng Songyou, 2021, NEURIPS, V34, P13032
   Qin ZX, 2021, VISUAL COMPUT, V37, P2195, DOI 10.1007/s00371-020-01979-2
   Ramasinghe S, 2020, IEEE INT C INT ROBOT, P8169, DOI 10.1109/IROS45743.2020.9341265
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Ruan LD, 2023, PROC CVPR IEEE, P10219, DOI 10.1109/CVPR52729.2023.00985
   Sheng B, 2022, IEEE T CYBERNETICS, V52, P6662, DOI 10.1109/TCYB.2021.3079311
   Shu DW, 2019, IEEE I CONF COMP VIS, P3858, DOI 10.1109/ICCV.2019.00396
   Tchapmi LP, 2019, PROC CVPR IEEE, P383, DOI 10.1109/CVPR.2019.00047
   Vahdat A, 2021, 35 C NEURAL INFORM P, V34
   Vahdat Arash, 2022, P 36 INT C NEURAL IN, V35, P10021
   Wu JJ, 2016, ADV NEUR IN, V29
   Xiang M, 2023, APPL INTELL, V53, P14971, DOI 10.1007/s10489-022-04219-3
   Xiang P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5479, DOI 10.1109/ICCV48922.2021.00545
   Xu FY, 2023, APPL INTELL, V53, P2362, DOI 10.1007/s10489-022-03576-3
   Xu QG, 2022, PROC CVPR IEEE, P5428, DOI 10.1109/CVPR52688.2022.00536
   Yang GD, 2019, IEEE I CONF COMP VIS, P4540, DOI 10.1109/ICCV.2019.00464
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yu XM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12478, DOI 10.1109/ICCV48922.2021.01227
   Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088
   Zhang BW, 2022, PROC CVPR IEEE, P11294, DOI 10.1109/CVPR52688.2022.01102
   Zhang XC, 2021, PROC CVPR IEEE, P15885, DOI 10.1109/CVPR46437.2021.01563
   Zhou LQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5806, DOI 10.1109/ICCV48922.2021.00577
   Zhu Z, 2024, IEEE T VIS COMPUT GR, V30, P3545, DOI 10.1109/TVCG.2023.3236061
NR 56
TC 0
Z9 0
U1 19
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 3903
EP 3917
DI 10.1007/s00371-024-03396-1
EA APR 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001206047800002
DA 2024-08-05
ER

PT J
AU Huang, LJ
   Kang, WC
   Chen, GK
   Zhang, Q
   Zhang, JW
AF Huang, Liangjun
   Kang, Wencan
   Chen, Guangkai
   Zhang, Qing
   Zhang, Jianwei
TI Light-sensitive and adaptive fusion network for RGB-T crowd counting
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE RGB-T image; Crowd counting; Light-sensitive; Cross-modal fusion
ID PEOPLE; IMAGE
AB Mainstream RGB-T crowd counting methods use cross-modal complementary information to improve the counting accuracy. However, most of them neglect the effect of lighting variation on cross-modal data fusion. In this paper, we propose a Light-sensitive and Adaptive Fusion Network (LAFNet) for RGB-T crowd counting. Specifically, we present a Modality-specific Feature Extraction Module (MFEM) that fuses the lighting information, and a Light-sensitive and Adaptive Fusion Module (LAFM) that adjusts the fusion strategies of different modalities according to the lighting conditions of the input crowd images. Moreover, we propose an Improved Multi-scale Extraction Module (IMEM) to extract and fuse multi-modal at different scales. We evaluate our method on the RGBT-CC dataset and the experiment results show the validity of the model and its effectiveness in various scenarios.
C1 [Huang, Liangjun; Kang, Wencan; Chen, Guangkai; Zhang, Qing] Shanghai Inst Technol, Sch Comp Sci & Informat Engn, Shanghai 201418, Peoples R China.
   [Zhang, Jianwei] Univ Hamburg, Dept Informat, D-20354 Hamburg, Germany.
C3 Shanghai Institute of Technology; University of Hamburg
RP Huang, LJ (corresponding author), Shanghai Inst Technol, Sch Comp Sci & Informat Engn, Shanghai 201418, Peoples R China.
EM gene_huang@sit.edu.cn; kwencan0v0@163.com; winniefu@stellaacc.com;
   zhangqing@sit.edu.cn; zhang@informatik.uni-hamburg.de
FU Natural Science Foundation of Shanghai Municipality
FX No Statement Available
CR Alaska YA, 2017, TRAVEL MED INFECT DI, V15, P67, DOI 10.1016/j.tmaid.2016.09.002
   Basalamah S, 2019, IEEE ACCESS, V7, P71576, DOI 10.1109/ACCESS.2019.2918650
   Bondi E, 2014, 2014 11TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P337, DOI 10.1109/AVSS.2014.6918691
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chan AB, 2009, IEEE I CONF COMP VIS, P545, DOI 10.1109/ICCV.2009.5459191
   Chen DL, 2019, COMM COM INF SC, V1143, P135, DOI 10.1007/978-3-030-36802-9_16
   Chen XQ, 2021, LECT NOTES COMPUT SC, V13019, P203, DOI 10.1007/978-3-030-88004-0_17
   Chen ZH, 2023, IEEE T PATTERN ANAL, V45, P13489, DOI 10.1109/TPAMI.2023.3293885
   Deng MF, 2024, VISUAL COMPUT, V40, P1053, DOI 10.1007/s00371-023-02831-z
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Duan Z., 2021, IEEE Trans. Geosci. Remote Sens, V60, P1
   Fan ZZ, 2022, NEUROCOMPUTING, V472, P224, DOI 10.1016/j.neucom.2021.02.103
   Fu HY, 2012, IEEE IMAGE PROC, P2685, DOI 10.1109/ICIP.2012.6467452
   Gao GS, 2020, Arxiv, DOI [arXiv:2003.12783, DOI 10.48550/ARXIV.2003.12783]
   Guerrero-Gómez-Olmedo R, 2015, LECT NOTES COMPUT SC, V9117, P423, DOI 10.1007/978-3-319-19390-8_48
   Hashemzadeh M, 2016, INFORM SCIENCES, V345, P199, DOI 10.1016/j.ins.2016.01.060
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hwang S, 2015, PROC CVPR IEEE, P1037, DOI 10.1109/CVPR.2015.7298706
   Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33
   Idrees H, 2013, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2013.329
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Kang D, 2019, IEEE T CIRC SYST VID, V29, P1408, DOI 10.1109/TCSVT.2018.2837153
   Khan SD, 2021, INT J COMPUT INT SYS, V14, DOI 10.1007/s44196-021-00016-x
   Khan SD, 2021, ARAB J SCI ENG, V46, P3051, DOI 10.1007/s13369-020-04990-w
   Khan SD, 2021, VISUAL COMPUT, V37, P2127, DOI 10.1007/s00371-020-01974-7
   Li H, 2023, IEEE T IND INFORM, V19, P306, DOI 10.1109/TII.2022.3171352
   Li SF, 2023, SIGNAL IMAGE VIDEO P, V17, P601, DOI 10.1007/s11760-022-02266-4
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Lian DZ, 2019, PROC CVPR IEEE, P1821, DOI 10.1109/CVPR.2019.00192
   Lin H, 2022, PROC CVPR IEEE, P19596, DOI 10.1109/CVPR52688.2022.01901
   Lin T.-Y., 2017, PROC CVPR IEEE, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu LB, 2021, PROC CVPR IEEE, P4821, DOI 10.1109/CVPR46437.2021.00479
   Liu LB, 2021, IEEE T INTELL TRANSP, V22, P7169, DOI 10.1109/TITS.2020.3002718
   Lowe D. G., 1999, Computer vision, V2, P1150, DOI [10.1109/ICCV.1999.790410, DOI 10.1109/ICCV.1999.790410]
   Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624
   Sam DB, 2021, IEEE T PATTERN ANAL, V43, P2739, DOI 10.1109/TPAMI.2020.2974830
   Shao YH, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20195550
   She JT, 2020, J MED VIROL, V92, P747, DOI 10.1002/jmv.25807
   Shen Z, 2018, PROC CVPR IEEE, P5245, DOI 10.1109/CVPR.2018.00550
   Sheng B, 2022, IEEE T CYBERNETICS, V52, P6662, DOI 10.1109/TCYB.2021.3079311
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tang HH, 2022, IEEE INT SYMP CIRC S, P3299, DOI 10.1109/ISCAS48785.2022.9937583
   Thissen M, 2023, Arxiv, DOI arXiv:2304.06401
   Tripathi G, 2019, VISUAL COMPUT, V35, P753, DOI 10.1007/s00371-018-1499-5
   Wang B., 2020, ADV NEURAL INF PROCE, P1595, DOI DOI 10.48550/ARXIV.2009.13077
   Wang SZ, 2020, NEUROCOMPUTING, V404, P227, DOI 10.1016/j.neucom.2020.04.139
   Wu B, 2005, IEEE I CONF COMP VIS, P90
   Wu Z., 2022, 2022 IEEE INT C MULT, P1
   Xie YJ, 2020, IEEE IMAGE PROC, P1531, DOI 10.1109/ICIP40778.2020.9191086
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Yang SD, 2019, IEEE INT CONF COMP V, P4521, DOI 10.1109/ICCVW.2019.00553
   Youwei Pang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P235, DOI 10.1007/978-3-030-58595-2_15
   Yu RZ, 2019, LECT NOTES ARTIF INT, V11671, P523, DOI 10.1007/978-3-030-29911-8_40
   Yu T, 2018, PROC CVPR IEEE, P7287, DOI 10.1109/CVPR.2018.00761
   Yu Y, 2023, APPL INTELL, V53, P22602, DOI 10.1007/s10489-023-04721-2
   Zeng X, 2020, EXPERT SYST APPL, V141, DOI 10.1016/j.eswa.2019.112977
   Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684
   Zhang J., 2020, P IEEE CVF C COMP VI, P8582, DOI DOI 10.1109/CVPR42600.2020.00861
   Zhang Q, 2019, PROC CVPR IEEE, P8289, DOI 10.1109/CVPR.2019.00849
   Zhang SH, 2017, PROC CVPR IEEE, P4264, DOI 10.1109/CVPR.2017.454
   Zhang SH, 2021, EXPERT SYST APPL, V180, DOI 10.1016/j.eswa.2021.115071
   Zhang XC, 2012, 2012 IEEE NINTH INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL-BASED SURVEILLANCE (AVSS), P215, DOI 10.1109/AVSS.2012.82
   Zhang Yang, 2022, Proceedings of the 11th International Conference on Computer Engineering and Networks. Lecture Notes in Electrical Engineering (808), P90, DOI 10.1007/978-981-16-6554-7_10
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
   Zhou WJ, 2022, NEUROCOMPUTING, V490, P347, DOI 10.1016/j.neucom.2021.11.100
   Zhou WJ, 2022, IEEE T EM TOP COMP I, V6, P957, DOI 10.1109/TETCI.2021.3118043
   Zhou WJ, 2022, IEEE T MULTIMEDIA, V24, P2192, DOI 10.1109/TMM.2021.3077767
   Zhu AC, 2022, IEEE T INTELL TRANSP, V23, P8090, DOI 10.1109/TITS.2021.3075859
NR 69
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 21
PY 2024
DI 10.1007/s00371-024-03388-1
EA APR 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OF8M6
UT WOS:001205943800001
DA 2024-08-05
ER

PT J
AU Jing, QY
   Zhang, P
   Zhang, W
   Lei, WM
AF Jing, Qingyang
   Zhang, Peng
   Zhang, Wei
   Lei, Weimin
TI An improved target tracking method based on extraction of corner points
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Adaptive corners; Generalized Hough algorithm; KCF; Target tracking
AB Kernel correlation filter (KCF) algorithm is famous for fast tracking speed and has been used widely, while it is susceptible under some challenging scenes. Aiming at the problem of limited applicable scenes of KCF, an improved target tracking method based on extraction of corner points is proposed. Eight neighborhood template is used to filter corner points, and adaptive threshold approach is introduced to minimize the influence of noise on corner extraction. Moreover, owing to appropriate quantity and strong robustness, adaptive corners can reduce the influence of incomplete extraction on edge points while enhancing extraction speed. Subsequently, histogram of oriented gradient (HOG) and color name (CN) features are extracted, and targets are segmented into blocks to solve the problem that standard KCF is likely to lose targets when they have deformation and scale variation. Under occlusion, learning rate parameter is adjusted adaptively ensuring accuracy of model updating. To get rid of drift phenomenon when targets have fast motion, multiple targets are correlated, and the final center position of each target is achieved according to the contour depicted by generalized Hough algorithm. Experimental results on the dataset and actual scenes demonstrate that our proposed method improves the EAO from 0.299 to 0.505, and improves overall precision of diverse attributes from 0.781 to 0.787.
C1 [Jing, Qingyang; Zhang, Peng; Zhang, Wei; Lei, Weimin] Northeastern Univ, Sch Comp Sci & Engn, Shenyang 110169, Liaoning, Peoples R China.
   [Zhang, Peng] Shenyang Er Yi San Elect Technol Co Ltd, Shenyang 110027, Liaoning, Peoples R China.
C3 Northeastern University - China
RP Zhang, W (corresponding author), Northeastern Univ, Sch Comp Sci & Engn, Shenyang 110169, Liaoning, Peoples R China.
EM 2379212283@qq.com; 1910609@stu.neu.edu.cn; zhangwei1@mail.neu.edu.cn;
   leiweimin@mail.neu.edu.cn
FU Fundamental Research Funds for the Central Universities of China;
   National Key Research and Development Program of China [N2216010]; 
   [2022JH1/10400025];  [2018YFB1702000]
FX This work was supported by the 'Jie Bang Gua Shuai' Science and
   Technology Major Project of Liaoning Province in 2022
   (No.2022JH1/10400025), the Fundamental Research Funds for the Central
   Universities of China (No. N2216010) and the National Key Research and
   Development Program of China (No. 2018YFB1702000).
CR Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Choi J, 2017, PROC CVPR IEEE, P4828, DOI 10.1109/CVPR.2017.513
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Daniyan A, 2018, IEEE T SIGNAL PROCES, V66, P6076, DOI 10.1109/TSP.2018.2873537
   Dong XP, 2023, IEEE T PATTERN ANAL, V45, P8049, DOI 10.1109/TPAMI.2022.3230064
   Dong XP, 2021, IEEE T PATTERN ANAL, V43, P1515, DOI 10.1109/TPAMI.2019.2956703
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Dong XW, 2017, IEEE T AUTOMAT CONTR, V62, P3658, DOI 10.1109/TAC.2017.2673411
   Du CJ, 2022, VISUAL COMPUT, V38, P1883, DOI 10.1007/s00371-021-02247-7
   Du SD, 2022, IEEE T COMPUT SOC SY, V9, P18, DOI 10.1109/TCSS.2021.3093298
   Geninatti SR, 2020, IEEE T CIRC SYST VID, V30, P2932, DOI 10.1109/TCSVT.2019.2934499
   Guan MY, 2021, IEEE T MULTIMEDIA, V23, P3841, DOI 10.1109/TMM.2020.3032043
   Guo DQ, 2022, J VIS COMMUN IMAGE R, V84, DOI 10.1016/j.jvcir.2022.103484
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   Han QH, 2018, DIGIT SIGNAL PROCESS, V78, P136, DOI 10.1016/j.dsp.2018.03.007
   He Z, 2019, PROC CVPR IEEE, P1318, DOI 10.1109/CVPR.2019.00141
   Huang B, 2020, IEEE T MULTIMEDIA, V22, P2820, DOI 10.1109/TMM.2020.2965482
   Huo WX, 2022, MULTIMED TOOLS APPL, V81, P20917, DOI 10.1007/s11042-022-12669-7
   Jasani BA, 2018, IEEE T CIRC SYST VID, V28, P3516, DOI 10.1109/TCSVT.2017.2757998
   Li TC, 2023, SIGNAL PROCESS, V205, DOI 10.1016/j.sigpro.2022.108883
   Li Y, 2016, IEEE SIGNAL PROC LET, V23, P1136, DOI 10.1109/LSP.2016.2582783
   Masoumi-Ganjgah F, 2017, DIGIT SIGNAL PROCESS, V69, P154, DOI 10.1016/j.dsp.2017.06.007
   Mueller M, 2017, PROC CVPR IEEE, P1387, DOI 10.1109/CVPR.2017.152
   Song YB, 2017, IEEE I CONF COMP VIS, P2574, DOI 10.1109/ICCV.2017.279
   Tang M, 2018, PROC CVPR IEEE, P4874, DOI 10.1109/CVPR.2018.00512
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Wang MM, 2017, PROC CVPR IEEE, P4800, DOI 10.1109/CVPR.2017.510
   Yang K., 2021, CoRR abs/2104.07303
   Yang MW, 2022, VISUAL COMPUT, V38, P625, DOI 10.1007/s00371-020-02038-6
   Yin JB, 2020, PROC CVPR IEEE, P6767, DOI 10.1109/CVPR42600.2020.00680
   Zhang SQ, 2021, IEEE T MULTIMEDIA, V23, P859, DOI 10.1109/TMM.2020.2990089
   Zhang SL, 2020, IEEE T MULTIMEDIA, V22, P1998, DOI 10.1109/TMM.2019.2952252
   Zhang W, 2023, SIGNAL PROCESS, V202, DOI 10.1016/j.sigpro.2022.108741
   Zhong C, 2022, IEEE J-STARS, V15, P1481, DOI 10.1109/JSTARS.2022.3146035
   Zolfaghari M, 2022, VISUAL COMPUT, V38, P849, DOI 10.1007/s00371-020-02055-5
NR 35
TC 0
Z9 0
U1 9
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 30
PY 2024
DI 10.1007/s00371-024-03283-9
EA MAR 2024
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MO9V7
UT WOS:001194690600001
DA 2024-08-05
ER

PT J
AU Zhang, XZ
   Chen, K
   Johan, H
   Erdt, M
AF Zhang, Xingzi
   Chen, Kan
   Johan, Henry
   Erdt, Marius
TI <i>SLOD2+WIN</i>: semantics-aware addition and LoD of 3D window details
   for LoD2 CityGML models with textures
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Window; Semantics; LoD2 CityGML with textures; LoD
ID GENERATION
AB In many urban planning and visualization applications, it is crucial to have 3D window details. However, the process of acquiring and reconstructing them can be challenging. Therefore, in many 3D city models, buildings often lack 3D windows. Instead, their building facades are usually represented using 2D planar textures (i.e., LoD2 CityGML models with textures). To generate a 3D facade from 2D images, current methods often need to carefully design various grammars to achieve desired results, which can be tedious. A useful property of building, the window semantics information, is also lacking. The main contribution of this paper is proposing a semantics-aware method for addition and LoD control of 3D window details to LoD2 CityGML models with textures (namely SLOD2+WIN). A deep learning-based two-level window-pane detection is introduced, and the detection results are then processed and adjusted to generate 3D windows and add to the building models. Unlike other methods, the semantics (i.e., frames and panes) are considered for adding window details. We also propose and integrate a LoD scheme for 3D windows following the same concept as the LoD in CityGML. The tedious efforts of reconstruction or grammar creation can be reduced in our method. Only the information present in the texture itself is extracted, and the shape and pattern information of windows are obtained and adjusted from the detection results in an efficient and unsupervised manner to achieve neat window parsing. Specifically, clustering-based window/pane alignment, neatness-based window image voting, grid-based symmetry, thickness filtering, and fitting-based window-top modeling are proposed. To demonstrate the effectiveness and usefulness of SLOD2+WIN, experiments on several datasets of 3D cities are conducted to add 3D window details with the novel LoD schema, illustrative applications in architectural visualization and urban planning are also showcased. This paper extended its conference version.
C1 [Zhang, Xingzi] Fraunhofer Singapore, Singapore, Singapore.
   [Chen, Kan] Singapore Inst Technol, Singapore, Singapore.
   [Johan, Henry] Nanyang Technol Univ, Nanyang, Singapore.
   [Erdt, Marius] Dyson Singapore, Singapore, Singapore.
C3 Singapore Institute of Technology; Nanyang Technological University
RP Chen, K (corresponding author), Singapore Inst Technol, Singapore, Singapore.
EM kan.chen@singaporetech.edu.sg
FU National Research Foundation Singapore
FX The authors thank the reviewers for their constructive comments. The
   authors also thank Mr. Ajin Joseph for providing the local building
   models (texture data: Google, Maxar Technologies) used in the examples.
CR Bacharidis K, 2020, ISPRS INT J GEO-INF, V9, DOI 10.3390/ijgi9050322
   Becker Susanne., 2007, Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci, V36, P7
   Bhatt Manush, 2020, PEARC '20: Practice and Experience in Advanced Research Computing, P132, DOI 10.1145/3311790.3396670
   Buyukdemircioglu M, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12132128
   Cao J, 2017, BUILD ENVIRON, V123, P601, DOI 10.1016/j.buildenv.2017.07.018
   Chou CC, 2018, ADV ENG INFORM, V38, P538, DOI 10.1016/j.aei.2018.09.003
   DEFAYS D, 1977, COMPUT J, V20, P364, DOI 10.1093/comjnl/20.4.364
   Demir I, 2016, INT CONF 3D VISION, P194, DOI 10.1109/3DV.2016.28
   Du JT, 2018, BUILD ENVIRON, V145, P243, DOI 10.1016/j.buildenv.2018.09.029
   Fryskowska A, 2018, J CULT HERIT, V34, P95, DOI 10.1016/j.culher.2018.04.003
   Fuhrmann Simon, 2014, P EUR WORKSH GRAPH C, P11, DOI DOI 10.2312/GCH.20141299
   Gadde R, 2018, IEEE T PATTERN ANAL, V40, P1273, DOI 10.1109/TPAMI.2017.2696526
   Hensel S., 2019, ISPRSAnnals Photogram Remote Sens Spatial Inf Sci, V4, P37, DOI [10.5194/isprs-annals-IV-2-W5-37-2019, DOI 10.5194/ISPRSANNALSIV-2-W5-37-2019]
   Hu H, 2020, Arxiv, DOI arXiv:2002.08549
   Inc T., 2022, 3D Design Software | 3D Modeling on the Web | SketchUp
   Jiang B, 2022, NEURAL NETWORKS, V153, P204, DOI 10.1016/j.neunet.2022.05.024
   Kolbe T.H., 2005, Geo-information for disaster management, P883, DOI [DOI 10.1007/3-540-27468-5_63, 10.1007/3-540-27468-5_63, DOI 10.1007/3-540-27468-563]
   Koutsourakis P, 2009, IEEE I CONF COMP VIS, P1795, DOI 10.1109/ICCV.2009.5459400
   Li XL, 2020, BUILD ENVIRON, V186, DOI 10.1016/j.buildenv.2020.107353
   Lin H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461969
   Liu HT, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2301
   Malihi S, 2016, INT ARCH PHOTOGRAMM, V41, P71, DOI 10.5194/isprsarchives-XLI-B3-71-2016
   Malihi S, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10081320
   Nan LL, 2015, COMPUT GRAPH FORUM, V34, P217, DOI 10.1111/cgf.12554
   Nishida G, 2018, COMPUT GRAPH FORUM, V37, P415, DOI 10.1111/cgf.13372
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Riemenschneider H, 2014, LECT NOTES COMPUT SC, V8693, P516, DOI 10.1007/978-3-319-10602-1_34
   Rocha G, 2020, HERITAGE-BASEL, V3, P47, DOI 10.3390/heritage3010004
   Roy AM, 2023, ADV ENG INFORM, V56, DOI 10.1016/j.aei.2023.102007
   Ruano S., 2019, arXiv
   Stoter J, 2020, COMPUT ENVIRON URBAN, V80, DOI 10.1016/j.compenvurbsys.2019.101424
   The Alliancefor SustainableEnergy L, 2022, OpenStudio
   van den Brink L, 2013, T GIS, V17, P920, DOI 10.1111/tgis.12026
   Wang QD, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8090737
   Wen XD, 2019, ISPRS INT J GEO-INF, V8, DOI 10.3390/ijgi8030135
   Wichmann A., 2018, Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci, V42, P1191, DOI [DOI 10.5194/ISPRS-ARCHIVES-XLII-2-1191-2018, 10.5194/isprs-archives-XLII-2-1191-2018]
   Xu B, 2020, Arxiv, DOI arXiv:2005.09223
   Zhang Xingzi, 2022, 2022 International Conference on Cyberworlds (CW), P63, DOI 10.1109/CW55638.2022.00018
   Zhang XZ, 2019, PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (GRAPP), VOL 1, P294, DOI 10.5220/0007507802940301
NR 39
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 15
PY 2024
DI 10.1007/s00371-024-03304-7
EA MAR 2024
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KZ2R0
UT WOS:001183730500001
DA 2024-08-05
ER

PT J
AU Chen, AQ
   Li, M
   Gao, Y
AF Chen, Anqi
   Li, Ming
   Gao, Yang
TI Mem-Box: VR sandbox for adaptive working memory evaluation and training
   using physiological signals
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Working memory; Virtual reality; Cognitive rehabilitation
ID HEART-RATE-VARIABILITY; CAPACITY; ATTENTION; STRESS
AB Working memory is crucial for higher cognitive functions in humans and is a focus in cognitive rehabilitation. Compared to conventional working memory training methods, VR-based training provides a more immersive experience with realistic scenarios, offering enhanced transferability to daily life. However, existing VR-based training methods often focus on basic cognitive tasks, underutilize VR's realism, and rely heavily on subjective assessment methods. In this paper, we introduce a VR Sandbox for working memory training and evaluation, MEM-Box, which simulates everyday life scenarios and routines and adaptively adjusts task difficulty based on user performance. We conducted a training experiment utilizing the MEM-Box and compared it with a control group undergoing PC-based training. The results of the Stroop test indicate that both groups demonstrated improvements in working memory abilities, with MEM-Box training showing greater efficacy. Physiological data confirmed the effectiveness of the MEM-Box, as we observed lower HRV and SDNN. Furthermore, the results of the frequency-domain analysis indicate higher sympathetic nervous system activity (LFpower and LF/HF) during MEM-Box training, which is related to the higher sense of presence in VR. These metrics pave the way for building adaptive VR systems based on physiological data.
C1 [Chen, Anqi; Li, Ming; Gao, Yang] Beihang Univ, Beijing, Peoples R China.
C3 Beihang University
RP Gao, Y (corresponding author), Beihang Univ, Beijing, Peoples R China.
EM chenaq0626@buaa.edu.cn; minglee@buaa.edu.cn; gaoyangvr@buaa.edu.cn
FU National Key R &D Program of China [2023YFC3604500]; Beijing Science and
   Technology Plan Project [Z221100007722001, Z231100005923039]
FX This work was supported by the National Key R &D Program of China
   (2023YFC3604500), and the Beijing Science and Technology Plan Project
   (No. Z221100007722001, Z231100005923039).
CR Acerbi G, 2017, LECT NOTES ELECTR EN, V426, P31, DOI 10.1007/978-3-319-54283-6_3
   BADDELEY A, 1992, SCIENCE, V255, P556, DOI 10.1126/science.1736359
   Beloe P, 2020, DEVELOPMENTAL SCI, V23, DOI 10.1111/desc.12831
   Benedek M, 2010, PSYCHOPHYSIOLOGY, V47, P647, DOI 10.1111/j.1469-8986.2009.00972.x
   Berger EM., 2020, SSRN Electronic Journal, DOI DOI 10.2139/SSRN.3622985
   Boujut A., 2020, Alzheimers Dementia, V16, DOI [10.1002/alz.045254, DOI 10.1002/ALZ.045254]
   Bouker J, 2013, 2013 10TH INTERNATIONAL CONFERENCE AND EXPO ON EMERGING TECHNOLOGIES FOR A SMARTER WORLD (CEWIT)
   Escamilla JC, 2020, BRAIN SCI, V10, DOI 10.3390/brainsci10080552
   Colflesh GJH, 2007, PSYCHON B REV, V14, P699, DOI 10.3758/BF03196824
   Dey A, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P220, DOI [10.1109/vr.2019.8797840, 10.1109/VR.2019.8797840]
   Doniger Glen M, 2018, Alzheimers Dement (N Y), V4, P118, DOI 10.1016/j.trci.2018.02.005
   Gupta K, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P756, DOI [10.1109/VR46266.2020.1581313729558, 10.1109/VR46266.2020.000-5]
   Han KJ, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11177843
   HART S G, 1988, P139
   Hjortskov N, 2004, EUR J APPL PHYSIOL, V92, P84, DOI 10.1007/s00421-004-1055-z
   Hutchison KA, 2011, J EXP PSYCHOL LEARN, V37, P851, DOI 10.1037/a0023437
   Jaeggi SM, 2003, NEUROIMAGE, V19, P210, DOI 10.1016/S1053-8119(03)00098-3
   Jones G, 2015, COGNITION, V144, P1, DOI 10.1016/j.cognition.2015.07.009
   Kalantari S, 2022, INNOV AGING, V6, DOI 10.1093/geroni/igac015
   Kane MJ, 2003, J EXP PSYCHOL GEN, V132, P47, DOI 10.1037/0096-3445.132.1.47
   Kennedy R.S., 1993, Int. J. Aviat. Psy, P203
   Lefebvre CD, 2005, CLIN NEUROPHYSIOL, V116, P1665, DOI 10.1016/j.clinph.2005.03.015
   Levin MF, 2011, EXPERT REV NEUROTHER, V11, P153, DOI [10.1586/ern.10.201, 10.1586/ERN.10.201]
   Luong T, 2022, IEEE T VIS COMPUT GR, V28, P5154, DOI 10.1109/TVCG.2021.3110459
   Lusta Majdi, 2023, Engineering Psychology and Cognitive Ergonomics: 20th International Conference, EPCE 2023, Held as Part of the 25th HCI International Conference, HCII 2023, Proceedings. Lecture Notes in Computer Science, Lecture Notes in Artificial Intelligence (14017), P120, DOI 10.1007/978-3-031-35392-5_9
   McDuff D, 2014, IEEE ENG MED BIO, P2957, DOI 10.1109/EMBC.2014.6944243
   McDuff DJ, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P4000, DOI 10.1145/2858036.2858247
   MILLER GA, 1956, PSYCHOL REV, V63, P81, DOI 10.1037/h0043158
   Parong J, 2020, APPL COGNITIVE PSYCH, V34, P29, DOI 10.1002/acp.3582
   Putze F, 2019, IEEE ENG MED BIO, P3103, DOI [10.1109/embc.2019.8856386, 10.1109/EMBC.2019.8856386]
   SAUL JP, 1990, AM J PHYSIOL, V258, pH713, DOI 10.1152/ajpheart.1990.258.3.H713
   SAYERS BM, 1973, ERGONOMICS, V16, P17
   Stroop JR, 1935, J EXP PSYCHOL, V18, P643, DOI 10.1037/h0054651
   Teixeira-Santos AC, 2019, NEUROSCI BIOBEHAV R, V103, P163, DOI 10.1016/j.neubiorev.2019.05.009
   Tharion E, 2009, NATL MED J INDIA, V22, P63
   Visnovcova Z, 2014, PHYSIOL MEAS, V35, P1319, DOI 10.1088/0967-3334/35/7/1319
   Witmer BG, 2005, PRESENCE-TELEOP VIRT, V14, P298, DOI 10.1162/105474605323384654
   Wu WQ, 2019, IEEE J BIOMED HEALTH, V23, P703, DOI 10.1109/JBHI.2018.2832069
   Xiao Y, 2023, IEEE Trans Vis Comput Gr, P1
NR 39
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 25
PY 2024
DI 10.1007/s00371-024-03539-4
EA JUN 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WH6W8
UT WOS:001254028100001
DA 2024-08-05
ER

PT J
AU Liu, CF
   Gu, FJ
AF Liu, Caifeng
   Gu, Fangjie
TI Differential motion attention network for efficient action recognition
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Action recognition; Temporal reasoning; Differential motion attention;
   Efficiency
ID NEURAL-NETWORKS
AB Despite the great progresses achieved by commonly-used 3D CNNs and two-stream methods in action recognition, they cause heavy computational burden which are inefficient and even infeasible in real-world scenarios. In this paper, we propose differential motion attention network (DMANet) to specially highlight human dynamics toward efficient action recognition. First, we argue that consecutive frames contain redundant static features and construct a low computational unit for discriminative motion extraction to highlight the human action trajectories across consecutive frames. Second, as not all spatial regions in images play an equal role in depicting human actions, we propose an adaptive protocol to dynamically emphasize informative spatial regions. As an end-to-end lightweight framework, our DMANet outperforms costly 3D CNNs and two-stream methods by 2.3% with only 0.23x\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\times $$\end{document} computations and other efficient methods by 1.6% on Something-Something v1 dataset. Experimental results on two temporal-related datasets and the large-scale scene-related Kinetics-400 dataset prove the efficacy of DMANet. In-depth ablations further give both quantitative and qualitative support on its effects.
C1 [Liu, Caifeng; Gu, Fangjie] Dalian Univ Technol, Sch Econ & Management, Dalian 116000, Peoples R China.
C3 Dalian University of Technology
RP Gu, FJ (corresponding author), Dalian Univ Technol, Sch Econ & Management, Dalian 116000, Peoples R China.
EM iucaifeng@dlou.edu.cn; gufangjie@dlou.edu.cn
CR Alfasly S, 2024, IEEE T NEUR NET LEAR, V35, P2496, DOI 10.1109/TNNLS.2022.3190367
   Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen ZH, 2023, IEEE T PATTERN ANAL, V45, P13489, DOI 10.1109/TPAMI.2023.3293885
   Crasto N, 2019, PROC CVPR IEEE, P7874, DOI 10.1109/CVPR.2019.00807
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fan Q., 2022, P INT C LEARN REPR, P1
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Gao LQ, 2023, VISUAL COMPUT, V39, P3417, DOI 10.1007/s00371-023-02979-8
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heeseung Kwon, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P345, DOI 10.1007/978-3-030-58517-4_21
   Hu J., 2018, SQUEEZE AND EXCITATI, P7132
   Hu L., 2023, P 37 AAAI C ART INT, P854, DOI 10.1609/aaai.v37i1.25164
   Hu LY, 2023, PROC CVPR IEEE, P2529, DOI 10.1109/CVPR52729.2023.00249
   Hu LY, 2023, EXPERT SYST APPL, V232, DOI 10.1016/j.eswa.2023.120683
   Huang Z., 2022, P INT C LEARN REPR
   Jiang BY, 2019, IEEE I CONF COMP VIS, P2000, DOI 10.1109/ICCV.2019.00209
   Kim B, 2022, LECT NOTES COMPUT SC, V13664, P209, DOI 10.1007/978-3-031-19772-7_13
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Lee S, 2021, PROC CVPR IEEE, P3053, DOI 10.1109/CVPR46437.2021.00307
   Li MS, 2022, IEEE T PATTERN ANAL, V44, P3316, DOI 10.1109/TPAMI.2021.3053765
   Li XH, 2020, PROC CVPR IEEE, P1089, DOI 10.1109/CVPR42600.2020.00117
   Li XY, 2022, IEEE WINT CONF APPL, P827, DOI 10.1109/WACV51458.2022.00090
   Li Y, 2020, Arxiv, DOI arXiv:2004.01398
   Lim LA, 2018, PATTERN RECOGN LETT, V112, P256, DOI 10.1016/j.patrec.2018.08.002
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Lin T.-Y., 2017, PROC CVPR IEEE, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin X., 2021, IEEE Trans. Multimed.
   Liu CF, 2021, MULTIMED TOOLS APPL, V80, P7313, DOI 10.1007/s11042-020-09643-6
   Liu Z, 2022, PROC CVPR IEEE, P3192, DOI 10.1109/CVPR52688.2022.00320
   Liu ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13688, DOI 10.1109/ICCV48922.2021.01345
   Liu ZY, 2020, AAAI CONF ARTIF INTE, V34, P11669
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Lu TW, 2023, NEURAL COMPUT APPL, DOI 10.1007/s00521-023-09069-9
   Lu XK, 2022, IEEE T PATTERN ANAL, V44, P7885, DOI 10.1109/TPAMI.2021.3115815
   Lu XK, 2019, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2019.00374
   Luo CX, 2019, IEEE I CONF COMP VIS, P5511, DOI 10.1109/ICCV.2019.00561
   Paszke A, 2019, ADV NEUR IN, V32
   Patrick M, 2021, ADV NEUR IN, V34
   Qin ZY, 2023, IEEE T IMAGE PROCESS, V32, P6543, DOI 10.1109/TIP.2023.3328485
   Qin ZY, 2023, IEEE-CAA J AUTOMATIC, V10, P1192, DOI 10.1109/JAS.2023.123456
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Shao H, 2020, AAAI CONF ARTIF INTE, V34, P11966
   Sheng B, 2022, IEEE T CYBERNETICS, V52, P6662, DOI 10.1109/TCYB.2021.3079311
   Simonyan K, 2014, ADV NEUR IN, V27
   Singh B, 2018, Arxiv, DOI arXiv:1805.09300
   Su Y, 2021, AD HOC NETW, V113, DOI 10.1016/j.adhoc.2020.102380
   Su Y, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3180420
   Sudhakaran S, 2020, PROC CVPR IEEE, P1099, DOI 10.1109/CVPR42600.2020.00118
   Sun SY, 2018, PROC CVPR IEEE, P1390, DOI 10.1109/CVPR.2018.00151
   Tran D, 2019, IEEE I CONF COMP VIS, P5551, DOI 10.1109/ICCV.2019.00565
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang H, 2020, PROC CVPR IEEE, P349, DOI 10.1109/CVPR42600.2020.00043
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang HB, 2024, IEEE T NEUR NET LEAR, V35, P10121, DOI 10.1109/TNNLS.2023.3239033
   Wang HB, 2023, IEEE T MULTIMEDIA, V25, P6629, DOI 10.1109/TMM.2022.3212270
   Wang HB, 2021, IEEE T MULTIMEDIA, V23, P3828, DOI 10.1109/TMM.2020.3032023
   Wang LM, 2021, PROC CVPR IEEE, P1895, DOI 10.1109/CVPR46437.2021.00193
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang QJ, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3289798
   Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang ZW, 2021, PROC CVPR IEEE, P13209, DOI 10.1109/CVPR46437.2021.01301
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu Peng, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P105, DOI 10.1145/3581783.3611978
   Xie Saining, 2017, ABS171204851 CORR
   Yu FS, 2016, Arxiv, DOI arXiv:1511.07122
   Zhang SW, 2020, Arxiv, DOI arXiv:2002.07442
   Zhao Y, 2018, ADV NEUR IN, V31
   Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou JQ, 2023, IEEE T MULTIMEDIA, V25, P5192, DOI 10.1109/TMM.2022.3189253
   Zhu XQ, 2019, IEEE I CONF COMP VIS, P3493, DOI 10.1109/ICCV.2019.00359
NR 77
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 13
PY 2024
DI 10.1007/s00371-024-03478-0
EA JUN 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UF3N2
UT WOS:001246607000003
DA 2024-08-05
ER

PT J
AU Liu, Y
   Li, XY
   Liu, Y
   Zhong, W
AF Liu, Yong
   Li, Xingyuan
   Liu, Yong
   Zhong, Wei
TI SimpliFusion: a simplified infrared and visible image fusion network
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image fusion; Transformer; Low-level vision; Image processing
ID MULTISCALE TRANSFORM; ARCHITECTURE; FRAMEWORK; ENSEMBLE; FOCUS; NEST
AB This paper introduces SimpliFusion, a network designed for the fusion of infrared and visible images, leveraging a simplified transformer architecture. SimpliFusion is engineered to adeptly handle both long-range and short-range information, facilitating a more effective integration of infrared and visible data. The core of SimpliFusion lies in its innovative use of a streamlined transformer model, which simplifies the traditional complexities associated with transformer networks while maintaining high efficiency and accuracy in image fusion tasks. The network architecture of SimpliFusion incorporates specialized attention mechanisms that are adept at capturing and integrating diverse spatial and temporal features from both infrared and visible spectra. This includes an intra-domain fusion unit based on self-attention for processing within each spectral domain, and an inter-domain fusion unit based on cross-attention for bridging and integrating information across the infrared and visible domains. These units are specifically designed to exploit the long-range dependencies characteristic of infrared data and the detailed textural information prevalent in visible images. Extensive experiments conducted on a range of multi-modal image fusion scenarios, including both multi-modal image fusion and object detection, demonstrate the superiority of SimpliFusion.
C1 [Liu, Yong; Li, Xingyuan; Liu, Yong] Dalian Univ Technol, Sch Software Technol, Dalian, Peoples R China.
   [Zhong, Wei] Dalian Univ Technol, Int Sch Informat Sci & Engn, Dalian, Peoples R China.
C3 Dalian University of Technology; Dalian University of Technology
RP Zhong, W (corresponding author), Dalian Univ Technol, Int Sch Informat Sci & Engn, Dalian, Peoples R China.
EM zhongwei@dlut.edu.cn
FU National Natural Science Foundation of China [61906029, 62027826]
FX This work was partially supported by the National Natural Science
   Foundation of China (No.61906029), and the National Natural Science
   Foundation of China (No.62027826).
CR Aslantas V, 2015, AEU-INT J ELECTRON C, V69, P160, DOI 10.1016/j.aeue.2015.09.004
   Bavirisetti DP, 2017, 2017 20TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P701
   Bayoudh K, 2022, VISUAL COMPUT, V38, P2939, DOI 10.1007/s00371-021-02166-7
   Bhatnagar G, 2013, IEEE T MULTIMEDIA, V15, P1014, DOI 10.1109/TMM.2013.2244870
   Bian PC, 2021, LECT NOTES COMPUT SC, V13021, P287, DOI 10.1007/978-3-030-88010-1_24
   Bian PC, 2021, IEEE IMAGE PROC, P1794, DOI 10.1109/ICIP42928.2021.9506532
   Carion N., 2020, EUR C COMP VIS, P213
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen J, 2020, INFORM SCIENCES, V508, P64, DOI 10.1016/j.ins.2019.08.066
   Chen YT, 2024, VISUAL COMPUT, V40, P489, DOI 10.1007/s00371-023-02795-0
   Cheng ZZ, 2015, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2015.55
   Cui GM, 2015, OPT COMMUN, V341, P199, DOI 10.1016/j.optcom.2014.12.032
   Cvejic N, 2007, IEEE SENS J, V7, P743, DOI 10.1109/JSEN.2007.894926
   da Cunha AL, 2006, IEEE T IMAGE PROCESS, V15, P3089, DOI 10.1109/TIP.2006.877507
   De S., 2020, Adv. Neural Inf. Process. Syst, P19964
   Dong YH, 2021, PR MACH LEARN RES, V139
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Eskicioglu AM, 1995, IEEE T COMMUN, V43, P2959, DOI 10.1109/26.477498
   Fu Y, 2022, Arxiv, DOI arXiv:2107.13967
   Gan W, 2015, INFRARED PHYS TECHN, V72, P37, DOI 10.1016/j.infrared.2015.07.003
   He BBY, 2023, Arxiv, DOI arXiv:2302.10322
   He KJ, 2018, NEUROCOMPUTING, V320, P157, DOI 10.1016/j.neucom.2018.09.018
   Hu HM, 2017, IEEE T MULTIMEDIA, V19, P2706, DOI 10.1109/TMM.2017.2711422
   Huang J, 2020, IEEE ACCESS, V8, P55145, DOI 10.1109/ACCESS.2020.2982016
   Huang W, 2015, SENSORS-BASEL, V15, P2041, DOI 10.3390/s150102041
   Jiang ZY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3783, DOI 10.1145/3503161.3547966
   Li H, 2016, INFRARED PHYS TECHN, V74, P28, DOI 10.1016/j.infrared.2015.11.002
   Li H, 2021, INFORM FUSION, V73, P72, DOI 10.1016/j.inffus.2021.02.023
   Li H, 2020, IEEE T INSTRUM MEAS, V69, P9645, DOI 10.1109/TIM.2020.3005230
   Li H, 2020, IEEE T IMAGE PROCESS, V29, P4733, DOI 10.1109/TIP.2020.2975984
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Li JP, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3164136
   Liao GB, 2022, IEEE T CIRC SYST VID, V32, P7646, DOI 10.1109/TCSVT.2022.3184840
   Liu CH, 2017, INFRARED PHYS TECHN, V83, P94, DOI 10.1016/j.infrared.2017.04.018
   Liu Decheng, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P4647, DOI 10.1145/3581783.3612355
   Liu DC, 2024, IEEE T INF FOREN SEC, V19, P735, DOI 10.1109/TIFS.2023.3327666
   Liu DC, 2024, IEEE T MULTIMEDIA, V26, P2894, DOI 10.1109/TMM.2023.3304913
   Liu DC, 2022, IEEE T NEUR NET LEAR, V33, P5611, DOI 10.1109/TNNLS.2021.3071119
   Liu JY, 2023, IEEE I CONF COMP VIS, P8081, DOI 10.1109/ICCV51070.2023.00745
   Liu JY, 2022, PROC CVPR IEEE, P5792, DOI 10.1109/CVPR52688.2022.00571
   Liu JY, 2022, IEEE SIGNAL PROC LET, V29, P1614, DOI 10.1109/LSP.2022.3180672
   Liu JY, 2022, IEEE T CIRC SYST VID, V32, P105, DOI 10.1109/TCSVT.2021.3056725
   Liu JY, 2021, IEEE SIGNAL PROC LET, V28, P1818, DOI 10.1109/LSP.2021.3109818
   Liu RS, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1600, DOI 10.1145/3474085.3475299
   Liu RS, 2022, IEEE T PATTERN ANAL, V44, P7688, DOI 10.1109/TPAMI.2021.3115825
   Liu RS, 2021, IEEE T IMAGE PROCESS, V30, P1261, DOI 10.1109/TIP.2020.3043125
   Liu Y, 2016, IEEE SIGNAL PROC LET, V23, P1882, DOI 10.1109/LSP.2016.2618776
   Liu Y, 2015, INFORM FUSION, V24, P147, DOI 10.1016/j.inffus.2014.09.004
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu XK, 2022, IEEE T PATTERN ANAL, V44, P7885, DOI 10.1109/TPAMI.2021.3115815
   Lu XK, 2019, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2019.00374
   Ma JY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3038013
   Ma JY, 2022, IEEE-CAA J AUTOMATIC, V9, P1200, DOI 10.1109/JAS.2022.105686
   Ma JY, 2020, IEEE T IMAGE PROCESS, V29, P4980, DOI 10.1109/TIP.2020.2977573
   Ma JY, 2020, INFORM FUSION, V54, P85, DOI 10.1016/j.inffus.2019.07.005
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JY, 2016, INFORM FUSION, V31, P100, DOI 10.1016/j.inffus.2016.02.001
   Meng FJ, 2016, NEUROCOMPUTING, V177, P1, DOI 10.1016/j.neucom.2015.10.080
   Mou J, 2013, 2013 6TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), VOLS 1-3, P1046, DOI 10.1109/CISP.2013.6745210
   Noci L., 2023, arXiv
   Noci Lorenzo, 2022, ADV NEURAL INFORM PR, V35, P27198
   Qin Z, 2024, Adv. Neural Inf. Process. Syst., V36
   Qin ZY, 2023, AAAI CONF ARTIF INTE, P2110
   Qu LH, 2022, AAAI CONF ARTIF INTE, P2126
   Qu Xiao-Bo, 2008, Acta Automatica Sinica, V34, P1508, DOI 10.3724/SP.J.1004.2008.01508
   Rao Dongyu, 2023, IEEE Trans Image Process, VPP, DOI 10.1109/TIP.2023.3273451
   Roberts JW, 2008, J APPL REMOTE SENS, V2, DOI 10.1117/1.2945910
   Sheng B, 2022, IEEE T CYBERNETICS, V52, P6662, DOI 10.1109/TCYB.2021.3079311
   Tang LF, 2022, INFORM FUSION, V83, P79, DOI 10.1016/j.inffus.2022.03.007
   Toet A, 2017, DATA BRIEF, V15, P249, DOI 10.1016/j.dib.2017.09.038
   Touvron H, 2023, Arxiv, DOI [arXiv:2302.13971, DOI 10.48550/ARXIV.2302.13971]
   Vaswani A, 2017, ADV NEUR IN, V30
   Vibashan VS, 2022, IEEE IMAGE PROC, P3566, DOI 10.1109/ICIP46576.2022.9897280
   Wang B., 2021, GPT-J-6B: A 6 billion parameter autoregressive language model
   Wang XC, 2017, LECT NOTES COMPUT SC, V10361, P420, DOI 10.1007/978-3-319-63309-1_39
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu Peng, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P105, DOI 10.1145/3581783.3611978
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Xu H, 2021, IEEE T COMPUT IMAG, V7, P824, DOI 10.1109/TCI.2021.3100986
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Xu H, 2020, IEEE T IMAGE PROCESS, V29, P7203, DOI 10.1109/TIP.2020.2999855
   Xu HH, 2021, IEEE J-STARS, V14, P8823, DOI 10.1109/JSTARS.2021.3108233
   Yadav SP, 2020, MED BIOL ENG COMPUT, V58, P669, DOI 10.1007/s11517-020-02136-6
   Yang B, 2016, INT J WAVELETS MULTI, V14, DOI 10.1142/S0219691316500247
   Zhang B., 2019, arXiv
   Zhang H, 2021, IEEE T COMPUT IMAG, V7, P1134, DOI 10.1109/TCI.2021.3119954
   Zhang H, 2020, AAAI CONF ARTIF INTE, V34, P12797
   Zhang Q, 2018, INFORM FUSION, V40, P57, DOI 10.1016/j.inffus.2017.05.006
   Zhang Y, 2020, INFORM FUSION, V54, P99, DOI 10.1016/j.inffus.2019.07.011
   Zhao F, 2021, INFORM FUSION, V76, P189, DOI 10.1016/j.inffus.2021.06.002
   Zhao WD, 2018, IEEE T MULTIMEDIA, V20, P866, DOI 10.1109/TMM.2017.2760100
   Zhao ZX, 2023, PROC CVPR IEEE, P5906, DOI 10.1109/CVPR52729.2023.00572
   Zhao ZX, 2023, Arxiv, DOI arXiv:2303.06840
   Zhao ZX, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P970
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
NR 96
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 29
PY 2024
DI 10.1007/s00371-024-03423-1
EA MAY 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL2G6
UT WOS:001234536800005
DA 2024-08-05
ER

PT J
AU Liu, WX
   Jia, XM
   Ju, YH
   Ju, YK
   Jiang, K
   Wu, SF
   Zhong, L
   Zhong, X
AF Liu, Wenxuan
   Jia, Xuemei
   Ju, Yihao
   Ju, Yakun
   Jiang, Kui
   Wu, Shifeng
   Zhong, Luo
   Zhong, Xian
TI Fragrant: frequency-auxiliary guided relational attention network for
   low-light action recognition
SO VISUAL COMPUTER
LA English
DT Review; Early Access
DE Low-light action recognition; Frequency domain analysis; Relational
   attention mechanism; Motion representation; Action boundary
ID DARK
AB Video action recognition aims to classify actions within sequences of video frames, which has important applications in computer vision fields. Existing methods have shown proficiency in well-lit environments but experience a drop in efficiency under low-light conditions. This decline is due to the challenge of extracting relevant information from dark, noisy images. Furthermore, simply introducing enhancement networks as preprocessing will lead to an increase in both parameters and computational burden for the video. To address this dilemma, this paper presents a novel frequency-based method, FRequency-Auxiliary Guided Relational Attention NeTwork (FRAGRANT), designed specifically for low-light action recognition. Its distinctive features can be summarized as: (1) a novel Frequency-Auxiliary Module that focuses on informative object regions, characterizing action and motion while effectively suppressing noise; (2) a sophisticated Relational Attention Module that enhances motion representation by modeling the local s between position neighbors, thereby more efficiently resolving issues, such as fuzzy boundaries. Comprehensive testing demonstrates that FRAGRANT outperforms existing methods, achieving state-of-the-art results on various standard low-light action recognition benchmarks.
C1 [Liu, Wenxuan; Zhong, Luo; Zhong, Xian] Wuhan Univ Technol, Sch Comp Sci & Artificial Intelligence, Hubei Key Lab Transportat Internet Things, Wuhan 430070, Peoples R China.
   [Jia, Xuemei; Wu, Shifeng] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
   [Ju, Yihao] Wuhan Traff Management Bur, Wuhan 430024, Peoples R China.
   [Ju, Yakun; Zhong, Xian] Nanyang Technol Univ, Sch Elect & Elect Engn, Rapid Rich Object Search Lab, Singapore 639798, Singapore.
   [Jiang, Kui] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
C3 Wuhan University of Technology; Wuhan University; Nanyang Technological
   University; Harbin Institute of Technology
RP Zhong, X (corresponding author), Wuhan Univ Technol, Sch Comp Sci & Artificial Intelligence, Hubei Key Lab Transportat Internet Things, Wuhan 430070, Peoples R China.; Jia, XM (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.; Zhong, X (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Rapid Rich Object Search Lab, Singapore 639798, Singapore.
EM lwxfight@whut.edu.cn; jiaxuemeiL@whu.edu.cn; jyh0812@163.com;
   kelvin.yakun.ju@gmail.com; jiangkui@hit.edu.cn; wudy@whut.edu.cn;
   zhongluo@whut.edu.cn; zhongx@whut.edu.cn
FU Fundamental Research Funds for the Central Universities [62271361,
   52271366]; National Natural Science Foundation of China
   [WHUTIOT2023-002]; Fundamental Research Funds for the Central
   Universities
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62271361 and 52271366, and the
   Fundamental Research Funds for the Central Universities under Grant
   WHUTIOT2023-002. The numerical calculations in this paper have been done
   on the supercomputing system in the Supercomputing Center of Wuhan
   University.
CR Alfasly S, 2023, NEUROCOMPUTING, V516, P231, DOI 10.1016/j.neucom.2022.10.037
   Aouaidjia K, 2021, IEEE T SYST MAN CY-S, V51, P2774, DOI 10.1109/TSMC.2019.2916896
   Bertasius G, 2021, PR MACH LEARN RES, V139
   BUIJS HL, 1974, IEEE T ACOUST SPEECH, VAS22, P420, DOI 10.1109/TASSP.1974.1162620
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen X, 2022, IEEE T MULTIMEDIA, V24, P288, DOI 10.1109/TMM.2021.3050069
   de la Riva M, 2019, IEEE INT CONF COMP V, P1337, DOI 10.1109/ICCVW.2019.00169
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Gao CQ, 2016, NEUROCOMPUTING, V212, P36, DOI 10.1016/j.neucom.2016.05.094
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Gowda SN, 2021, AAAI CONF ARTIF INTE, V35, P1451
   Guo S, 2023, VISUAL COMPUT, V39, P1363, DOI 10.1007/s00371-022-02412-6
   Hao SJ, 2020, IEEE T MULTIMEDIA, V22, P3025, DOI 10.1109/TMM.2020.2969790
   Hira S, 2021, IEEE COMPUT SOC CONF, P853, DOI 10.1109/CVPRW53098.2021.00095
   Hu MS, 2022, PROC CVPR IEEE, P3564, DOI 10.1109/CVPR52688.2022.00356
   Huang SH, 2024, IEEE J BIOMED HEALTH, V28, P2416, DOI 10.1109/JBHI.2024.3363080
   Huang WX, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3565886
   Jiang HY, 2019, IEEE I CONF COMP VIS, P7323, DOI 10.1109/ICCV.2019.00742
   Jiang K, 2022, AAAI CONF ARTIF INTE, P1078
   Jiang ZL, 2017, IEEE COMPUT SOC CONF, P309, DOI 10.1109/CVPRW.2017.44
   Kim M., 2021, ADV NEURAL INFORM PR, V34
   Kingma D. P., 2014, arXiv
   Kothandaraman D, 2022, LECT NOTES COMPUT SC, V13697, P657, DOI 10.1007/978-3-031-19836-6_37
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Lavin A, 2016, PROC CVPR IEEE, P4013, DOI 10.1109/CVPR.2016.435
   Li D, 2021, PROC CVPR IEEE, P12316, DOI 10.1109/CVPR46437.2021.01214
   Li JP, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2039, DOI 10.1145/3394171.3413641
   Li K., 2022, P INT C LEARN REPR
   Li K., 2021, P INT C LEARN REPR, P1
   Li KC, 2023, IEEE I CONF COMP VIS, P1632, DOI 10.1109/ICCV51070.2023.00157
   Li KP, 2022, AAAI CONF ARTIF INTE, P1341
   Li Y, 2020, PROC CVPR IEEE, P906, DOI 10.1109/CVPR42600.2020.00099
   Lin J, 2022, IEEE T PATTERN ANAL, V44, P2760, DOI 10.1109/TPAMI.2020.3029799
   Liu JY, 2021, INT J COMPUT VISION, V129, P1153, DOI 10.1007/s11263-020-01418-8
   Liu WX, 2023, IEEE T IMAGE PROCESS, V32, P2719, DOI 10.1109/TIP.2023.3273459
   Long X, 2022, IEEE T PATTERN ANAL, V44, P2140, DOI 10.1109/TPAMI.2020.3029554
   Luo HN, 2022, IEEE T CIRC SYST VID, V32, P3073, DOI 10.1109/TCSVT.2021.3100842
   Lv F., 2018, Proc. BMVC, V220, P4
   Munsif M, 2024, HUM-CENT COMPUT INFO, V14, DOI 10.22967/HCIS.2024.14.004
   Pan Z., 2022, Adv. Neural Inf. Process. Syst.
   PIZER SM, 1987, COMPUT VISION GRAPH, V39, P355, DOI 10.1016/S0734-189X(87)80186-X
   Rasheed H, 2023, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR52729.2023.00633
   Shchekotov I, 2022, INTERSPEECH, P1188, DOI 10.21437/Interspeech.2022-603
   Sheng B, 2022, IEEE T CYBERNETICS, V52, P6662, DOI 10.1109/TCYB.2021.3079311
   Sheng B, 2020, IEEE T CIRC SYST VID, V30, P955, DOI 10.1109/TCSVT.2019.2901629
   Sheng B, 2020, IEEE T VIS COMPUT GR, V26, P1332, DOI 10.1109/TVCG.2018.2869326
   Sheng XX, 2023, IEEE T CIRC SYST VID, V33, P977, DOI 10.1109/TCSVT.2022.3207518
   TANCIK M, 2020, ADV NEUR IN, V33, pNI266
   Tang C, 2022, IEEE T PATTERN ANAL, V44, P955, DOI 10.1109/TPAMI.2020.3014629
   Tian CW, 2024, INFORM FUSION, V102, DOI 10.1016/j.inffus.2023.102043
   Tian Y., 2021, P PAC RIM INT C ART, P236
   Tu ZG, 2023, IEEE T IMAGE PROCESS, V32, P3507, DOI 10.1109/TIP.2023.3286254
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang MM, 2023, IEEE T PATTERN ANAL, V45, P3347, DOI 10.1109/TPAMI.2022.3173658
   Wang T., 2023, P AAAI C ART INT
   Xinyu Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P275, DOI 10.1007/978-3-030-58539-6_17
   Xu K, 2020, PROC CVPR IEEE, P1737, DOI 10.1109/CVPR42600.2020.00181
   Xu L., 2021, P PAC RIM INT C ART, P321
   Yang CY, 2020, PROC CVPR IEEE, P588, DOI 10.1109/CVPR42600.2020.00067
   Yang T., 2023, P INT C LEARN REPR
   Zeng JB, 2023, Arxiv, DOI arXiv:2308.15345
   Zhang B, 2023, INFORM FUSION, V97, DOI 10.1016/j.inffus.2023.101822
   Zhang F, 2021, PROC CVPR IEEE, P4965, DOI 10.1109/CVPR46437.2021.00493
   Zhong Xian, 2022, P INT JOINT C ART IN, P1743, DOI [10.24963/ijcai.2022/243, DOI 10.24963/IJCAI.2022/243]
NR 65
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 14
PY 2024
DI 10.1007/s00371-024-03427-x
EA MAY 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QQ7N8
UT WOS:001222401900003
DA 2024-08-05
ER

PT J
AU Jin, YX
   Gui, F
   Chen, MH
   Chen, X
   Li, HX
   Zhang, JF
AF Jin, Yixiao
   Gui, Fu
   Chen, Minghao
   Chen, Xiang
   Li, Haoxuan
   Zhang, Jingfa
TI Deep learning-driven automated quality assessment of ultra-widefield
   optical coherence tomography angiography images for diabetic retinopathy
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Deep learning; Image quality assessment (IQA); Transfer learning;
   Diabetic retinopathy; Optical coherence tomography angiography (OCTA);
   Ultra-widefield optical coherence tomography angiography (UW-OCTA)
ID ARTIFICIAL-INTELLIGENCE; VESSEL SEGMENTATION; MOTION CORRECTION
AB Image quality assessment (IQA) of fundus images constitutes a foundational step in automated disease analysis. This process is pivotal in supporting the automation of screening, diagnosis, follow-up, and related academic research for diabetic retinopathy (DR). This study introduced a deep learning-based approach for IQA of ultra-widefield optical coherence tomography angiography (UW-OCTA) images of patients with DR. Given the novelty of ultra-widefield technology, its limited prevalence, the high costs associated with equipment and operational training, and concerns regarding ethics and patient privacy, UW-OCTA datasets are notably scarce. To address this, we initially pre-train a vision transformer (ViT) model on a dataset comprising 6 mm x 6 mm OCTA images, enabling the model to acquire a fundamental understanding of OCTA image characteristics and quality indicators. Subsequent fine-tuning on 12 mm x 12 mm UW-OCTA images aims to enhance accuracy in quality assessment. This transfer learning strategy leverages the generic features learned during pre-training and adjusts the model to evaluate UW-OCTA image quality effectively. Experimental results demonstrate that our proposed method achieves superior performance compared to ResNet18, ResNet34, and ResNet50, with an AUC of 0.9026 and a Kappa value of 0.7310. Additionally, ablation studies, including the omission of pre-training on 6 mm x 6 mm OCTA images and the substitution of the backbone network with the ViT base version, resulted in varying degrees of decline in AUC and Kappa values, confirming the efficacy of each module within our methodology.
C1 [Jin, Yixiao; Chen, Minghao; Zhang, Jingfa] Shanghai Jiao Tong Univ, Sch Med, Shanghai Gen Hosp, Dept Ophthalmol, 100 Hai Ning Rd, Shanghai 200080, Peoples R China.
   [Jin, Yixiao; Chen, Minghao; Zhang, Jingfa] Shanghai Engn Ctr Precise Diag & Treatment Eye Dis, Natl Clin Res Ctr Eye Dis, Shanghai Key Lab Ocular Fundus Dis,Shanghai Key Cl, Shanghai Engn Ctr Visual Sci & Photomed,Shanghai C, Shanghai, Peoples R China.
   [Gui, Fu] Nanchang Univ, Dept Ophthalmol, Affiliated Hosp 2, Nanchang, Jiangxi, Peoples R China.
   [Chen, Xiang] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Li, Haoxuan] Shanghai Univ Sport, Sch Exercise & Hlth, Sports Engn, 650 Qingyuan Ring Rd, Shanghai 200438, Peoples R China.
C3 Shanghai Jiao Tong University; Nanchang University; Shanghai Jiao Tong
   University; Shanghai University of Sport
RP Zhang, JF (corresponding author), Shanghai Jiao Tong Univ, Sch Med, Shanghai Gen Hosp, Dept Ophthalmol, 100 Hai Ning Rd, Shanghai 200080, Peoples R China.; Zhang, JF (corresponding author), Shanghai Engn Ctr Precise Diag & Treatment Eye Dis, Natl Clin Res Ctr Eye Dis, Shanghai Key Lab Ocular Fundus Dis,Shanghai Key Cl, Shanghai Engn Ctr Visual Sci & Photomed,Shanghai C, Shanghai, Peoples R China.; Li, HX (corresponding author), Shanghai Univ Sport, Sch Exercise & Hlth, Sports Engn, 650 Qingyuan Ring Rd, Shanghai 200438, Peoples R China.
EM 2311519002@sus.edu.cn; 13917311571@139.com
RI Zhang, Jingfa/JFS-5047-2023
OI Zhang, Jingfa/0000-0003-0601-4342
CR Almasi R, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-18206-8
   Berchuck SI, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-54653-6
   Brock A, 2019, Arxiv, DOI arXiv:1809.11096
   Camino A, 2016, BIOMED OPT EXPRESS, V7, P3905, DOI 10.1364/BOE.7.003905
   Chefer H, 2021, PROC CVPR IEEE, P782, DOI 10.1109/CVPR46437.2021.00084
   Cheung N, 2010, LANCET, V376, P124, DOI 10.1016/S0140-6736(09)62124-3
   Cui Y, 2021, BRIT J OPHTHALMOL, V105, P577, DOI 10.1136/bjophthalmol-2020-316245
   Dai L, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-23458-5
   De Fauw J, 2018, NAT MED, V24, P1342, DOI 10.1038/s41591-018-0107-6
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Falavarjani KG, 2017, BRIT J OPHTHALMOL, V101, P564, DOI 10.1136/bjophthalmol-2016-309104
   Fenner BJ, 2018, BRIT J OPHTHALMOL, V102, P509, DOI 10.1136/bjophthalmol-2017-310700
   Gao SS, 2016, INVEST OPHTH VIS SCI, V57, pOCT27, DOI 10.1167/iovs.15-19043
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hormel TT, 2021, PROG RETIN EYE RES, V85, DOI 10.1016/j.preteyeres.2021.100965
   Hwang DK, 2019, THERANOSTICS, V9, P232, DOI 10.7150/thno.28447
   Jia YL, 2012, OPT EXPRESS, V20, P4710, DOI 10.1364/OE.20.004710
   Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244
   Kraus MF, 2014, BIOMED OPT EXPRESS, V5, P2591, DOI 10.1364/BOE.5.002591
   Kreitner L., 2024, IEEE Transactions on Medical Imaging, P1
   Le D, 2020, TRANSL VIS SCI TECHN, V9, DOI 10.1167/tvst.9.2.35
   Lee R, 2015, EYE VISION, V2, DOI 10.1186/s40662-015-0026-2
   Li F, 2020, NPJ DIGIT MED, V3, DOI 10.1038/s41746-020-00329-9
   Liu H., 2022, arXiv
   Liu YH, 2022, IEEE T MED IMAGING, V41, P3686, DOI 10.1109/TMI.2022.3193029
   Ma YH, 2021, IEEE T MED IMAGING, V40, P928, DOI 10.1109/TMI.2020.3042802
   Niederleithner M, 2023, IEEE T MED IMAGING, V42, P1009, DOI 10.1109/TMI.2022.3222638
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Qian B., 2022, Patterns, V2024
   Rakocz N, 2021, NPJ DIGIT MED, V4, DOI 10.1038/s41746-021-00411-w
   Ramesh A, 2021, Arxiv, DOI arXiv:2102.12092
   Ran AR, 2019, NEUROPHOTONICS, V6, DOI 10.1117/1.NPh.6.4.041110
   Sheng B, 2024, SCI BULL, V69, P583, DOI 10.1016/j.scib.2024.01.004
   Spaide RF, 2018, PROG RETIN EYE RES, V64, P1, DOI 10.1016/j.preteyeres.2017.11.003
   Spaide RF, 2015, RETINA-J RET VIT DIS, V35, P2163, DOI 10.1097/IAE.0000000000000765
   Teo ZL, 2021, OPHTHALMOLOGY, V128, P1580, DOI 10.1016/j.ophtha.2021.04.027
   Wang SZ, 2016, IEEE T MED IMAGING, V35, P1046, DOI 10.1109/TMI.2015.2506902
   Wong TY, 2020, OPHTHALMOLOGICA, V243, P9, DOI 10.1159/000502387
   Wong TY, 2016, JAMA-J AM MED ASSOC, V316, P2366, DOI 10.1001/jama.2016.17563
   Xu XY, 2023, IEEE T MED IMAGING, V42, P481, DOI 10.1109/TMI.2022.3214291
   Yang DW, 2023, JAMA OPHTHALMOL, V141, P641, DOI 10.1001/jamaophthalmol.2023.1821
   Yim J, 2020, NAT MED, V26, P892, DOI 10.1038/s41591-020-0867-7
   Yoo TK, 2022, COMPUT METH PROG BIO, V219, DOI 10.1016/j.cmpb.2022.106735
   Yosinski J, 2014, Arxiv, DOI [arXiv:1411.1792, 10.48550/arXiv.1411.1792]
NR 45
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 10
PY 2024
DI 10.1007/s00371-024-03383-6
EA MAY 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QJ2H9
UT WOS:001220437700002
OA hybrid
DA 2024-08-05
ER

PT J
AU Xiong, KH
   Qing, LB
   Li, LD
   Guo, L
   Peng, YH
AF Xiong, Kunhong
   Qing, Linbo
   Li, Lindong
   Guo, Li
   Peng, Yonghong
TI Facial expression recognition based on local-global information
   reasoning and spatial distribution of landmark features
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Facial expression recognition; Local-global information; Spatial
   distribution feature of landmarks; Augmentation loss
ID ATTENTION; NETWORK
AB In the field of facial expression recognition (FER), two main trends point to the data-driven FER and feature-driven FER exist. The former focused on the data problems (e.g., sample imbalance and multimodal fusion), while the latter explored the facial expression features. As the feature-driven FER is more important than the data-driven FER, for deeper mining of facial features, we propose an expression recognition model based on Local-Global information Reasoning and Landmark Spatial Distributions. Particularly to reason local-global information, multiple attention mechanisms with the modified residual module are designed for the Res18-LG module. In addition, taking the spatial topology of facial landmarks into account, a topological relationship graph of landmarks and a two-layer graph neural network are introduced to extract spatial distribution features. Finally, the experiment results on FERPlus and RAF-DB datasets demonstrate that our model outperforms the state-of-the-art methods.
C1 [Xiong, Kunhong; Qing, Linbo; Li, Lindong; Guo, Li; Peng, Yonghong] Sichuan Univ, Coll Elect & Informat Engn, Chengdu 610065, Peoples R China.
C3 Sichuan University
RP Qing, LB (corresponding author), Sichuan Univ, Coll Elect & Informat Engn, Chengdu 610065, Peoples R China.
EM qing_lb@scu.edu.cn
RI Xiong, Kunhong/KYP-8411-2024
FU National Natural Science Foundation of China [2023YFS0195]; Sichuan
   Science and Technology Program
FX This work was supported by the Sichuan Science and Technology Program
   under Grant 2023YFS0195.
CR Abdi H, 2010, WIRES COMPUT STAT, V2, P433, DOI 10.1002/wics.101
   Ayeche F, 2021, TRAIT SIGNAL, V38, P1575, DOI 10.18280/ts.380602
   Barsoum E, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P279, DOI 10.1145/2993148.2993165
   Brody S., 2021, Int. Conf. on Learning Representations
   Chattopadhyay J., 2018, INT C COMPUTATIONAL, P1181
   Chen J, 2021, NEURAL COMPUT APPL, V33, P8669, DOI 10.1007/s00521-020-05616-w
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Farzaneh AH, 2021, IEEE WINT CONF APPL, P2401, DOI 10.1109/WACV48630.2021.00245
   Gong WJ, 2022, NEURAL COMPUT APPL, V34, P10175, DOI 10.1007/s00521-022-07016-8
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   Hasani B, 2017, IEEE COMPUT SOC CONF, P2278, DOI 10.1109/CVPRW.2017.282
   [洪惠群 Hong Huiqun], 2022, [计算机科学与探索, Journal of Frontiers of Computer Science & Technology], V16, P1764
   Hu M, 2022, VOLUNTAS, V33, P308, DOI 10.1007/s11266-020-00305-7
   Huang QH, 2021, INFORM SCIENCES, V580, P35, DOI 10.1016/j.ins.2021.08.043
   Huang Y., 2021, ADV NEURAL INFORM PR
   Huang YB, 2021, IEEE INT CONF COMP V, P3602, DOI 10.1109/ICCVW54120.2021.00403
   Kaya M, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11091066
   Kim S, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22103729
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Kollias D, 2023, PROC CVPR IEEE, P5589, DOI 10.1109/CVPR52729.2023.00541
   Li H., 2021, arXiv
   Li HH, 2023, VISUAL COMPUT, V39, P4709, DOI 10.1007/s00371-022-02619-7
   Li J, 2020, NEUROCOMPUTING, V411, P340, DOI 10.1016/j.neucom.2020.06.014
   Li S, 2022, IEEE T AFFECT COMPUT, V13, P1195, DOI 10.1109/TAFFC.2020.2981446
   Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277
   Liang XC, 2023, VISUAL COMPUT, V39, P2277, DOI 10.1007/s00371-022-02413-5
   Liu C, 2023, INFORM SCIENCES, V619, P781, DOI 10.1016/j.ins.2022.11.068
   Liu Y, 2023, IEEE T AFFECT COMPUT, V14, P2657, DOI 10.1109/TAFFC.2022.3215918
   Lo L, 2022, IEEE T MULTIMEDIA, V24, P4275, DOI 10.1109/TMM.2022.3197365
   Ma FY, 2023, IEEE T AFFECT COMPUT, V14, P1236, DOI 10.1109/TAFFC.2021.3122146
   Mittal Trisha, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14222, DOI 10.1109/CVPR42600.2020.01424
   Ngwe JL, 2023, Arxiv, DOI arXiv:2306.09626
   Pecoraro R, 2022, INFORMATION, V13, DOI 10.3390/info13090419
   Poria S, 2019, Arxiv, DOI arXiv:1810.02508
   Romero A., 2018, INT C LEARN REPR, P2920, DOI DOI 10.48550/ARXIV.1710.10903
   Ruan DL, 2021, PROC CVPR IEEE, P7656, DOI 10.1109/CVPR46437.2021.00757
   Saurav S, 2022, VISUAL COMPUT, V38, P1083, DOI 10.1007/s00371-021-02069-7
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Sebe N, 2005, PROC SPIE, V5670, P56, DOI 10.1117/12.600746
   She JH, 2021, PROC CVPR IEEE, P6244, DOI 10.1109/CVPR46437.2021.00618
   Shi J., 2021, arXiv
   Shi JQ, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21010205
   Sun B, 2018, NEURAL NETWORKS, V105, P36, DOI 10.1016/j.neunet.2017.11.021
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang K, 2020, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR42600.2020.00693
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Wang XP, 2023, IEEE T AFFECT COMPUT, V14, P878, DOI 10.1109/TAFFC.2021.3100352
   Wang ZN, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107694
   Wasi AT, 2023, Arxiv, DOI [arXiv:2305.01486, DOI 10.48550/ARXIV.2305.01486]
   Wen ZY, 2023, BIOMIMETICS-BASEL, V8, DOI 10.3390/biomimetics8020199
   Wolf K, 2015, DIALOGUES CLIN NEURO, V17, P457
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Wu SH, 2021, J AMB INTEL HUM COMP, DOI 10.1007/s12652-021-03113-z
   Xi Y, 2023, VISUAL COMPUT, V39, P5001, DOI 10.1007/s00371-022-02642-8
   Xia H., 2023, Vis. Comput, V40, P1
   Xue FL, 2023, IEEE T AFFECT COMPUT, V14, P3244, DOI 10.1109/TAFFC.2022.3226473
   Xue FL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3581, DOI 10.1109/ICCV48922.2021.00358
   Yao LS, 2022, WIRELESS PERS COMMUN, V125, P1483, DOI 10.1007/s11277-022-09616-y
   Ye JY, 2023, IEEE J BIOMED HEALTH, V27, P3698, DOI 10.1109/JBHI.2023.3260816
   Yu MJ, 2020, PATTERN RECOGN LETT, V131, P166, DOI 10.1016/j.patrec.2020.01.016
   Yu WM, 2022, PATTERN RECOGN, V123, DOI 10.1016/j.patcog.2021.108401
   Zhang HF, 2020, IEEE ACCESS, V8, P37976, DOI 10.1109/ACCESS.2020.2975913
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang YH, 2021, ADV NEUR IN, V34
   Zhao ZQ, 2021, AAAI CONF ARTIF INTE, V35, P3510
   Zheng C, 2023, IEEE INT CONF COMP V, P3138, DOI 10.1109/ICCVW60793.2023.00339
NR 66
TC 0
Z9 0
U1 9
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 6
PY 2024
DI 10.1007/s00371-024-03345-y
EA APR 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NA4P6
UT WOS:001197708000002
DA 2024-08-05
ER

PT J
AU Liu, YX
   Jiang, T
   Si, PJ
   Zhu, SD
   Yan, CG
   Wang, S
   Yin, HB
AF Liu, Yixiu
   Jiang, Tao
   Si, Pengju
   Zhu, Shangdong
   Yan, Chenggang
   Wang, Shuai
   Yin, Haibing
TI Unpaired semantic neural person image synthesis
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Pose-guided person image synthesis; Appearance-shape decoupling network;
   Semantic mirror orientation adjustment module; Appearance inversion and
   semantic consistency constraint
AB Pose-guided person image synthesis is a challenging task that aims to generate photo-realistic images of a person with the same appearance as a source image but the same pose as a target image. Existing methods often suffer from noticeable artifacts due to the omission of multi-view information, and the requirement for paired source-target images in certain methods during training further limits the application of the models. To address these issues, we present a semantic neural person image synthesis framework, named SNPIS, which leverages neural radiance fields (NeRF) to synthesize high-fidelity human images of arbitrary poses from multi-view source images and target semantic maps. First, we introduce the semantic mirror orientation adjustment that forces sampling points to focus on the human body, effectively suppressing background interference and enhancing human details. Then we devise a NeRF-based appearance-shape decoupling generative adversarial network, which separates appearance and shape of a shared volume generated from multi-view source images and corresponding semantic maps. Finally, we use the obtained decoupled generator to synthesize human images guided by the target semantic maps, employing appearance inversion, and optimizing pose reconstruction with semantic consistency constraint. Experimental results show that our approach not only outperforms existing unpaired pose-guided person image synthesis methods, but also competes with many paired methods.
C1 [Liu, Yixiu; Jiang, Tao; Yan, Chenggang; Wang, Shuai; Yin, Haibing] Hangzhou Dianzi Univ, Hangzhou 310018, Peoples R China.
   [Si, Pengju] Henan Univ Sci & Technol, Luoyang 471023, Peoples R China.
   [Zhu, Shangdong] Northeastern Univ, Shenyang 110169, Liaoning, Peoples R China.
   [Wang, Shuai; Yin, Haibing] Hangzhou Dianzi Univ, Lishui Inst, Lishui 323000, Peoples R China.
C3 Hangzhou Dianzi University; Henan University of Science & Technology;
   Northeastern University - China; Hangzhou Dianzi University
RP Si, PJ (corresponding author), Henan Univ Sci & Technol, Luoyang 471023, Peoples R China.
EM sipengju@haust.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Bhunia AK, 2023, PROC CVPR IEEE, P5968, DOI 10.1109/CVPR52729.2023.00578
   Chan ER, 2021, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR46437.2021.00574
   Chen AP, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3470848
   Chen YD, 2022, LECT NOTES COMPUT SC, V13674, P730, DOI 10.1007/978-3-031-19781-9_42
   Cheong SY, 2023, Arxiv, DOI arXiv:2304.08870
   Esser P, 2018, PROC CVPR IEEE, P8857, DOI 10.1109/CVPR.2018.00923
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Grigorev A, 2021, PROC CVPR IEEE, P5147, DOI 10.1109/CVPR46437.2021.00511
   Hao Tang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P717, DOI 10.1007/978-3-030-58595-2_43
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Kajiya J. T., 1984, Computers & Graphics, V18, P165
   Karras T, 2018, Arxiv, DOI [arXiv:1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Kocabas M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11107, DOI 10.1109/ICCV48922.2021.01094
   Li NN, 2023, Arxiv, DOI arXiv:2210.01887
   Li RZ, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2047
   Lin K, 2021, IEEE T CIRC SYST VID, V31, P1066, DOI 10.1109/TCSVT.2020.2995122
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu L., 2021, ACM T GRAPHIC, V40, P1
   Liu Steven, 2021, P IEEECVF INT C COMP, P5773
   Liu XQ, 2019, ADV NEUR IN, V32
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Lv ZY, 2021, PROC CVPR IEEE, P10801, DOI 10.1109/CVPR46437.2021.01066
   Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018
   Ma TX, 2021, PROC CVPR IEEE, P13617, DOI 10.1109/CVPR46437.2021.01341
   Men YF, 2020, PROC CVPR IEEE, P5083, DOI 10.1109/CVPR42600.2020.00513
   Mescheder L, 2018, PR MACH LEARN RES, V80
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Mu JT, 2023, Arxiv, DOI arXiv:2304.14401
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Peng SD, 2022, Arxiv, DOI arXiv:2203.08133
   Peng SD, 2021, PROC CVPR IEEE, P9050, DOI 10.1109/CVPR46437.2021.00894
   Perez E, 2018, AAAI CONF ARTIF INTE, P3942
   Pumarola A, 2018, PROC CVPR IEEE, P8620, DOI 10.1109/CVPR.2018.00899
   Qin WH, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2092
   Raj Amit, 2021, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P3721, DOI 10.1109/CVPR46437.2021.00372
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Sheng B, 2022, IEEE T CYBERNETICS, V52, P6662, DOI 10.1109/TCYB.2021.3079311
   Sitzmann V., 2020, Advances in neural information processing systems, V33, P7462, DOI DOI 10.48550/ARXIV.2006.09661
   Song SJ, 2019, PROC CVPR IEEE, P2352, DOI 10.1109/CVPR.2019.00246
   Su SY, 2021, ADV NEUR IN, V34
   Sun JX, 2022, PROC CVPR IEEE, P7662, DOI 10.1109/CVPR52688.2022.00752
   Sun LB, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2187
   Thies J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323035
   Trivedi H, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2169
   Tseng WC, 2022, IEEE INT CONF ROBOT, P8454, DOI 10.1109/ICRA46639.2022.9812272
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Y., 2021, P IEEE CVF INT C COM, p13 749
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZJ, 2022, PROC CVPR IEEE, P7693, DOI 10.1109/CVPR52688.2022.00755
   Weng CY, 2022, PROC CVPR IEEE, P16189, DOI 10.1109/CVPR52688.2022.01573
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Zhang JS, 2021, PROC CVPR IEEE, P7978, DOI 10.1109/CVPR46437.2021.00789
   Zhang PZ, 2022, PROC CVPR IEEE, P7703, DOI 10.1109/CVPR52688.2022.00756
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhu PH, 2020, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR42600.2020.00515
   Zhu Z, 2019, PROC CVPR IEEE, P2342, DOI 10.1109/CVPR.2019.00245
NR 62
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 2
PY 2024
DI 10.1007/s00371-024-03331-4
EA APR 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MT0M2
UT WOS:001195768300001
DA 2024-08-05
ER

PT J
AU Zhu, LH
   Zhao, Y
   Fang, YX
   Wang, JX
AF Zhu, Liuhao
   Zhao, Yi
   Fang, Yixiang
   Wang, Junxiang
TI A novel robust digital image watermarking scheme based on attention
   U-Net plus plus structure
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Digital image watermarking; Attention mechanism; U-Net plus plus; The
   quadratic nonlinear growth loss weight
ID UNET
AB With the advancement of the internet, digital image watermarking techniques have found widespread application across various domains, including copyright protection and information security. However, traditional digital image watermarking techniques are susceptible to geometric distortions due to their limited feature extraction capabilities and reliance on manually designed watermark embedding algorithms. Recently, deep neural network-based digital watermarking has emerged as a promising approach due to its powerful nonlinear fitting ability, which has high robustness against various distortions, especially against geometric distortions. Most existing deep neural network-based digital watermarking frameworks employ U-Net style encoders, which may inadequately extract image features and exploit the correlation between secret messages and image pixels. Consequently, this results in a sub-optimal balance between visual quality and robustness. To overcome these limitations, a novel encoder called Attention U-Net++ that merges the advantages of U-Net++ and Attention U-Net is proposed. By incorporating the attention mechanism into the U-Net++ architecture, our proposed encoder effectively extracts image features and finds optimal pixel space for embedding messages, enhancing visual quality and robustness. Furthermore, a quadratic nonlinear growth loss weight setting based on the WGAN style discriminator is devised to enhance performance. Experimental results demonstrate that our proposed method achieves superior visual quality and robustness compared to the state-of-the-art schemes.
C1 [Zhu, Liuhao; Zhao, Yi; Fang, Yixiang; Wang, Junxiang] Jingdezhen Ceram Univ, Sch Mech & Elect Engn, Jingdezhen 333403, Jiangxi, Peoples R China.
C3 Jingdezhen Ceramic Institute
RP Zhao, Y (corresponding author), Jingdezhen Ceram Univ, Sch Mech & Elect Engn, Jingdezhen 333403, Jiangxi, Peoples R China.
EM zhliuhao@163.com; zhaoyi@jcu.edu.cn; fangyixiang@jci.edu.cn;
   wjx851113851113@163.com
OI zhao, yi/0009-0000-4345-1651
FU National Natural Science Foundation of China [62062044, 61762054];
   National Natural Science Foundation of China
FX This work was supported by the National Natural Science Foundation of
   China (Grant No. 62062044, 61762054).
CR Zhang KA, 2019, Arxiv, DOI arXiv:1901.03892
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Eltoukhy MM, 2023, ALEX ENG J, V78, P517, DOI 10.1016/j.aej.2023.07.068
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Hai T, 2014, J APPL RES TECHNOL, V12, P122, DOI 10.1016/S1665-6423(14)71612-8
   Hosny KM, 2022, MULTIMED TOOLS APPL, V81, P24347, DOI 10.1007/s11042-022-12282-8
   Hosny KM, 2022, MULTIMED TOOLS APPL, V81, P17185, DOI 10.1007/s11042-022-12037-5
   Hosny KM, 2021, BIOMED SIGNAL PROCES, V70, DOI 10.1016/j.bspc.2021.103007
   Hosny KM, 2021, IEEE ACCESS, V9, P47425, DOI 10.1109/ACCESS.2021.3068211
   Jia J, 2022, IEEE T CYBERNETICS, V52, P7094, DOI 10.1109/TCYB.2020.3037208
   Jiaohua Qin, 2010, Information Technology Journal, V9, P1725, DOI 10.3923/itj.2010.1725.1738
   Jin QG, 2020, FRONT BIOENG BIOTECH, V8, DOI 10.3389/fbioe.2020.605132
   Kandi H, 2017, COMPUT SECUR, V65, P247, DOI 10.1016/j.cose.2016.11.016
   Khafaga DS, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22155612
   Kingma D. P., 2014, arXiv
   Kumar S, 2021, MULTIMED TOOLS APPL, V80, P9315, DOI 10.1007/s11042-020-09943-x
   Liu YN, 2020, SIGNAL PROCESS-IMAGE, V88, DOI 10.1016/j.image.2020.115946
   Magdy M, 2022, IEEE ACCESS, V10, P38821, DOI 10.1109/ACCESS.2022.3165813
   Mun SM, 2017, Arxiv, DOI arXiv:1704.03248
   Oktay O, 2018, Arxiv, DOI [arXiv:1804.03999, DOI 10.48550/ARXIV.1804.03999]
   Ray A, 2020, INT J MULTIMED INF R, V9, P249, DOI 10.1007/s13735-020-00197-9
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sadreazami H, 2019, IEEE T CIRCUITS-II, V66, P151, DOI 10.1109/TCSII.2018.2846547
   Shi HC, 2018, LECT NOTES COMPUT SC, V10735, P534, DOI 10.1007/978-3-319-77380-3_51
   Tancik M, 2020, PROC CVPR IEEE, P2114, DOI 10.1109/CVPR42600.2020.00219
   Thomas E, 2021, IEEE J BIOMED HEALTH, V25, P1724, DOI 10.1109/JBHI.2020.3024188
   Tian HW, 2013, IEEE T CYBERNETICS, V43, P2190, DOI 10.1109/TCYB.2013.2245415
   Vaidya SP, 2023, VISUAL COMPUT, V39, P2245, DOI 10.1007/s00371-022-02406-4
   Wang XY, 2022, SIGNAL PROCESS, V192, DOI 10.1016/j.sigpro.2021.108371
   Wang XY, 2021, J VIS COMMUN IMAGE R, V77, DOI 10.1016/j.jvcir.2021.103123
   Wang XY, 2021, MULTIMED TOOLS APPL, V80, P27717, DOI 10.1007/s11042-021-11056-y
   Wang XY, 2020, SIGNAL PROCESS-IMAGE, V88, DOI 10.1016/j.image.2020.115972
   Wang XY, 2019, J VIS COMMUN IMAGE R, V62, P309, DOI 10.1016/j.jvcir.2019.05.012
   Wengrowski E, 2019, PROC CVPR IEEE, P1515, DOI 10.1109/CVPR.2019.00161
   Xiang SJ, 2008, IEEE T CIRC SYST VID, V18, P777, DOI 10.1109/TCSVT.2008.918843
   Xiyang Luo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13545, DOI 10.1109/CVPR42600.2020.01356
   Yuan ZH, 2021, VISUAL COMPUT, V37, P1867, DOI 10.1007/s00371-020-01945-y
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang XT, 2021, INT J INTELL SYST, V36, P4321, DOI 10.1002/int.22461
   Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1
   Zhu JR, 2018, LECT NOTES COMPUT SC, V11219, P682, DOI 10.1007/978-3-030-01267-0_40
   Zotin Alexandr, 2020, Procedia Computer Science, V176, P1633, DOI 10.1016/j.procs.2020.09.187
NR 42
TC 0
Z9 0
U1 14
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 8
PY 2024
DI 10.1007/s00371-024-03271-z
EA FEB 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD1M4
UT WOS:001157463900001
DA 2024-08-05
ER

PT J
AU Lei, WH
   Qin, HW
   Hou, XY
   Chen, HR
AF Lei, Weihao
   Qin, Huawang
   Hou, Xiaoyang
   Chen, Haoran
TI A two-stage model for spatial downscaling of daily precipitation data
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Multi-scale feature; Deep learning; Spatial downscaling; Swin
   Transformer; Focal frequency loss
ID NETWORK
AB Providing reliable and accurate high-resolution meteorological data is very significant to guide the rapid response to extreme weather conditions. However, due to the constraints of computing power and simulation time, the spatial resolution of existing all global climate models is low, and it is unable to provide meteorological data with more precise resolution at local scale. In this research, a deep learning downscaling model called two-stage multi-scale feature extraction network (TSMFN) is proposed. By combining ERA5 reanalysis data and terrain data, spatial downscaling of global precipitation measurement mission precipitation data is carried out. Specifically, in the first stage of the network, several multi-scale residual Inception blocks are used to extract multi-scale features of low-resolution precipitation data; several residual-based residual multi-scale cross blocks are used to fully excavate multi-scale features after the fusion of multiple data. In the second stage of the TSMFN, the output feature map after the fusion of the previous stage is fused with the high-resolution monthly average precipitation data, and several progressive multistage Swin Transformer blocks are constructed to overcome the problems that the reconstructed image of a general convolutional neural network is smooth and cannot reflect the real spatial distribution of precipitation. Finally, a hybrid loss function combining L1\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$L_{1}$$\end{document} loss function and focal frequency loss function is proposed to alleviate the ill-posedness of the downscaling. By comparing the proposed algorithm with some advanced deep learning downscaling algorithms, the results of the experiment show that the TSMFN model is significantly better in terms of generated image quality and multiple evaluation indexes, and the model has stronger generalization ability.
C1 [Lei, Weihao; Qin, Huawang] Nanjing Univ Informat Sci & Technol, Sch Elect & Informat Engn, Nanjing 210044, Peoples R China.
   [Hou, Xiaoyang] Zhejiang Ocean Univ, Sch Marine Sci & Technol, Zhoushan 316022, Peoples R China.
   [Chen, Haoran] Nanjing Univ Informat Sci & Technol, Sch Automat, Nanjing 210044, Peoples R China.
C3 Nanjing University of Information Science & Technology; Zhejiang Ocean
   University; Nanjing University of Information Science & Technology
RP Qin, HW (corresponding author), Nanjing Univ Informat Sci & Technol, Sch Elect & Informat Engn, Nanjing 210044, Peoples R China.
EM 15212871917@163.com; qin_h_w@163.com; 3289493714@qq.com;
   781241941@qq.com
CR Accarino G, 2021, AI-BASEL, V2, P600, DOI 10.3390/ai2040036
   Cai R., 2021, arXiv preprint arXiv, p2111.10800
   Chen H., 2022, Front. Earth Sci, V10, P756
   Chen J, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12062935
   Cheng JX, 2020, IEEE ACCESS, V8, P39623, DOI 10.1109/ACCESS.2020.2974785
   [程文聪 Cheng Wencong], 2020, [热带气象学报, Journal of Tropical Meteorology], V36, P307
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Fang FM, 2020, IEEE T IMAGE PROCESS, V29, P4656, DOI 10.1109/TIP.2020.2973769
   Fang JS, 2022, IEEE COMPUT SOC CONF, P1102, DOI 10.1109/CVPRW56347.2022.00119
   Harilal N, 2021, IEEE ACCESS, V9, P25208, DOI 10.1109/ACCESS.2021.3057500
   Hersbach H, 2020, Q J ROY METEOR SOC, V146, P1999, DOI 10.1002/qj.3803
   Höhlein K, 2020, METEOROL APPL, V27, DOI 10.1002/met.1961
   Ji YQ, 2021, FRONT EARTH SC-SWITZ, V8, DOI 10.3389/feart.2020.598025
   Jiang LM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13899, DOI 10.1109/ICCV48922.2021.01366
   Karl TR, 2003, SCIENCE, V302, P1719, DOI 10.1126/science.1090228
   Kidd C., 2020, Advances in Global Change Research, VVolume 67, P343
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim J, 2022, SUSTAINABILITY-BASEL, V14, DOI 10.3390/su14084719
   Kumar B, 2023, EARTH SCI INFORM, DOI 10.1007/s12145-023-00970-4
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Li JC, 2021, IEEE T CIRC SYST VID, V31, P2547, DOI 10.1109/TCSVT.2020.3027732
   Li JC, 2018, LECT NOTES COMPUT SC, V11212, P527, DOI 10.1007/978-3-030-01237-3_32
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu YM, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P3145, DOI 10.1145/3394486.3403366
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu ZY, 2022, SIGNAL IMAGE VIDEO P, V16, P1143, DOI 10.1007/s11760-021-02063-5
   Maggioni V, 2016, J HYDROMETEOROL, V17, P1101, DOI 10.1175/JHM-D-15-0190.1
   Mei YQ, 2021, PROC CVPR IEEE, P3516, DOI 10.1109/CVPR46437.2021.00352
   Misra V, 2003, J CLIMATE, V16, P103, DOI 10.1175/1520-0442(2003)016<0103:DDOSSO>2.0.CO;2
   Sathianarayanan M, 2023, INT ARCH PHOTOGRAMM, P327, DOI 10.5194/isprs-archives-XLVIII-4-W6-2022-327-2023
   Sha YK, 2020, J APPL METEOROL CLIM, V59, P2075, DOI 10.1175/JAMC-D-20-0058.1
   Sharma SCM, 2022, ENVIRON DATA SCI, V1, DOI 10.1017/eds.2022.23
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Singh M, 2023, COMPUT URBAN SCI, V3, DOI 10.1007/s43762-023-00096-9
   Sulaiman NA., 2020, Int. J, DOI [10.30534/ijatcse/2020/9191.42020, DOI 10.30534/IJATCSE/2020/9191.42020]
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Vandal T, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1663, DOI 10.1145/3097983.3098004
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JL, 2021, GEOSCI MODEL DEV, V14, P6355, DOI 10.5194/gmd-14-6355-2021
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wu YC, 2022, ADV METEOROL, V2022, DOI 10.1155/2022/3140872
   Xiang L, 2022, ATMOSPHERE-BASEL, V13, DOI 10.3390/atmos13040511
   [于法川 Yu Fachuan], 2022, [地球信息科学学报, Journal of Geo-Information Science], V24, P750
   [张慕琪 Zhang Muqi], 2022, [北京大学学报. 自然科学版, Acta Scientiarum Naturalium Universitatis Pekinensis], V58, P221
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhou KH, 2022, ADV ATMOS SCI, V39, P1472, DOI 10.1007/s00376-021-1207-7
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 48
TC 0
Z9 0
U1 12
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 31
PY 2024
DI 10.1007/s00371-023-03236-8
EA JAN 2024
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN2G6
UT WOS:001153277300001
DA 2024-08-05
ER

PT J
AU Saklani, A
   Tiwari, S
   Pannu, HS
AF Saklani, Avantika
   Tiwari, Shailendra
   Pannu, H. S.
TI Deep attentive multimodal learning for food information enhancement via
   early-stage heterogeneous fusion
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Feature fusion; Deep learning; Food categorization; Image and text
   fusion; Multimodal machine learning
AB In contrast to single-modal content, multimodal data can offer greater insight into food statistics more vividly and effectively. But traditional food classification system focuses on individual modality. It is thus futile as the massive amount of data are emerging on a daily basis which has latterly attracted researchers in this field. Moreover, there are very few available multimodal Indian food datasets. On studying these findings, we build a novel multimodal food analysis model based on deep attentive multimodal fusion network (DAMFN) for lingual and visual integration. The model includes three stages: functional feature extraction, early-stage fusion and feature classification. In functional feature extraction, deep features from the individual modalities are abstracted. Then an early-stage fusion is applied that leverages the deep correlation between the modalities. Lastly, the fused features are provided to the classification system for the final decision in the feature classification phase. We further developed a dataset having Indian food images with their related caption for the experimental purpose. In addition to this, the proposed approach is also evaluated on a large-scale dataset called UPMC Food 101, having 90,704 instances. The experimental results demonstrate that the proposed DAMFN outperforms several state-of-the-art techniques of multimodal food classification methods as well as the individual modality systems.
C1 [Saklani, Avantika; Tiwari, Shailendra; Pannu, H. S.] Thapar Inst Engn & Technol, Comp Sci & Technol, Patiala 147004, India.
C3 Thapar Institute of Engineering & Technology
RP Saklani, A (corresponding author), Thapar Inst Engn & Technol, Comp Sci & Technol, Patiala 147004, India.
EM asaklani_phd20@thapar.edu; shailendra@thapar.edu; hspannu@thapar.edu
FU Coordinator of TIET-Tel Aviv University Joint Food Security Centre of
   Excellence
FX The authors are thankful to Dr Moushumi Ghosh, Coordinator of TIET-Tel
   Aviv University Joint Food Security Centre of Excellence, for her
   invaluable contribution in validating the dataset used in the research.
   The meticulous review and analysis of the dataset helped to ensure its
   accuracy and reliability important for the research.
CR Abavisani M, 2020, IEEE IMAGE PROC, P773, DOI [10.1109/ICIP40778.2020.9191317, 10.1109/icip40778.2020.9191317]
   Ahn YY, 2011, SCI REP-UK, V1, DOI 10.1038/srep00196
   Arslan Berker, 2022, IEEE Transactions on Artificial Intelligence, V3, P238, DOI 10.1109/TAI.2021.3108126
   Bahador N, 2021, JMIR MHEALTH UHEALTH, V9, DOI 10.2196/21926
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Bharti SK, 2022, WIREL COMMUN MOB COM, V2022, DOI 10.1155/2022/1653696
   Bruni Elia., 2011, Proceedings of the GEMS 2011 Workshop on GEo- metrical Models of Natural Language Semantics, P22
   Chen JJ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P32, DOI 10.1145/2964284.2964315
   Dela Comble A., 2022, arXiv
   Fakhrou A, 2021, MULTIMED TOOLS APPL, V80, P33011, DOI 10.1007/s11042-021-11329-6
   Gallo I., 2018, Digital image computing: techniques and applications (DICTA), P1
   Gao W, 2020, IEEE MULTIMEDIA, V27, P28, DOI 10.1109/MMUL.2020.3012675
   Hao HY, 2020, INT J DISAST RISK RE, V51, DOI 10.1016/j.ijdrr.2020.101760
   Huang FR, 2019, KNOWL-BASED SYST, V167, P26, DOI 10.1016/j.knosys.2019.01.019
   Josephine V. L. Helen, 2021, IOP Conference Series: Materials Science and Engineering, V1131, DOI 10.1088/1757-899X/1131/1/012007
   Kiela D, 2020, Arxiv, DOI arXiv:1909.02950
   Kiela D, 2018, AAAI CONF ARTIF INTE, P5198
   Kumar RD, 2021, J SUPERCOMPUT, V77, P8172, DOI 10.1007/s11227-021-03622-w
   Kumari R, 2021, EXPERT SYST APPL, V184, DOI 10.1016/j.eswa.2021.115412
   Lasne K.S., 2021, ITM WEB C
   Liang T, 2022, PROC CVPR IEEE, P15471, DOI 10.1109/CVPR52688.2022.01505
   Liu CX, 2021, IEEE T CIRC SYST VID, V31, P2480, DOI 10.1109/TCSVT.2020.3020079
   Liu YH, 2019, Arxiv, DOI [arXiv:1907.11692, DOI 10.48550/ARXIV.1907.11692]
   Ma MM, 2022, PROC CVPR IEEE, P18156, DOI 10.1109/CVPR52688.2022.01764
   Malekzadeh M, 2021, 2021 IEEE 12TH ANNUAL UBIQUITOUS COMPUTING, ELECTRONICS & MOBILE COMMUNICATION CONFERENCE (UEMCON), P84, DOI 10.1109/UEMCON53757.2021.9666633
   Min WQ, 2017, IEEE T MULTIMEDIA, V19, P1100, DOI 10.1109/TMM.2016.2639382
   Narayana P., 2019, arXiv
   Nawaz S, 2019, IEEE SENSOR LETT, V3, DOI 10.1109/LSENS.2018.2880790
   News A., YouTube mistakenly flags Notre Dame Cathedral fire videos as 9/11 conspiracy abcnews.go.com
   Rajapaksha P, 2021, IEEE ACCESS, V9, P154704, DOI 10.1109/ACCESS.2021.3128742
   Sajadmanesh S, 2017, WWW'17 COMPANION: PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P1013, DOI 10.1145/3041021.3055137
   Segura-Bedmar I, 2022, INFORMATION, V13, DOI 10.3390/info13060284
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Subhi MA, 2019, IEEE ACCESS, V7, P35370, DOI 10.1109/ACCESS.2019.2904519
   Thakkar Vignesh, 2018, 2018 Fifth International Conference on Emerging Applications of Information Technology (EAIT). Proceedings, DOI 10.1109/EAIT.2018.8470438
   VijayaKumari G., 2022, Global Transit. Proc, V3, P225, DOI [10.1016/j.gltp.2022.03.027, DOI 10.1016/J.GLTP.2022.03.027]
   Wang X, 2015, IEEE INT CONF MULTI
   Wiegand M., 2019, P 14 C EUR CHAPT ASS, P673
   WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1
   Yang S, 2010, PROC CVPR IEEE, P2249, DOI 10.1109/CVPR.2010.5539907
   Yang XC, 2021, IEEE T MULTIMEDIA, V23, P4014, DOI 10.1109/TMM.2020.3035277
   Yu Q., 2016, Technical report
NR 42
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 3
PY 2024
DI 10.1007/s00371-024-03546-5
EA JUL 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XK1T9
UT WOS:001261494700003
DA 2024-08-05
ER

PT J
AU Wei, XC
   Pu, GY
   Huo, YC
   Bao, HJ
   Wang, R
AF Wei, Xuchen
   Pu, GuiYang
   Huo, Yuchi
   Bao, Hujun
   Wang, Rui
TI Refined tri-directional path tracing with generated light portal
SO VISUAL COMPUTER
LA English
DT Article
DE Rendering; Monte Carlo path tracing; Light portal
AB The rendering efficiency of Monte Carlo path tracing often depends on the ease of path construction. For scenes with particularly complex visibility, e.g. where the camera and light sources are placed in separate rooms connected by narrow doorways or windows, it is difficult to construct valid paths using traditional path tracing algorithms such as unidirectional path tracing or bidirectional path tracing. Light portal is a class of methods that assist in sampling direct light paths based on prior knowledge of the scene. It usually requires additional manual editing and labelling by the artist or renderer user. Tri-directional path tracing is a sophisticated path tracing algorithm that combines bidirectional path tracing and light portals sampling, but the original work lacks sufficient analysis to demonstrate its effectiveness. In this paper, we propose an automatic light portal generation algorithm based on spatial radiosity analysis that mitigates the cost of manual operations for complex scenes. We also further analyse and improve the light portal-based tri-directional path tracing rendering algorithm, giving a detailed analysis of path construction strategies, algorithm complexity, and the unbiasedness of the Monte Carlo estimation. The experimental results show that our algorithm can accurately locate the light portals with low computational cost and effectively improve the rendering performance of complex scenes.
C1 [Wei, Xuchen; Huo, Yuchi; Bao, Hujun; Wang, Rui] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Peoples R China.
   [Pu, GuiYang] China Mobile Hangzhou Informat Technol Co Ltd, Hangzhou, Peoples R China.
C3 Zhejiang University
RP Wang, R (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Peoples R China.
EM ruiwang@zju.edu.cn
FU National Key R&D Program of China [2023YFF0905102]; Key R&D Program of
   Zhejiang Province [2023C01039]
FX The work was partially supported by National Key R&D Program of China
   (No. 2023YFF0905102), Key R&D Program of Zhejiang Province (No.
   2023C01039).
CR Airey J.M., 1990, THESIS U N CAROLINA
   Aliaga DG, 1997, VISUALIZATION '97 - PROCEEDINGS, P355, DOI 10.1109/VISUAL.1997.663903
   Anderson L, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073704
   [Anonymous], 2001, Realistic Image Synthesis Using Photon Mapping
   Autodesk,Inc, 2024, ARNOLD NODE REFERENC
   Bitterli B., 2016, RENDERING RESOURCES
   Bitterli B, 2015, COMPUT GRAPH FORUM, V34, P13, DOI 10.1111/cgf.12674
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Georgiev I, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366211
   Goldberg A.V., 1986, P 18 ANN ACM S THEOR
   Goral C. M., 1984, Computers & Graphics, V18, P213
   Gritz L., 2010, ACM SIGGRAPH 2010 TA, DOI 10.1145/1837026.1837070
   Hachisuka T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409083
   Jakob W, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185554
   JONES CB, 1971, COMPUT J, V14, P232, DOI 10.1093/comjnl/14.3.232
   Kajiya J. T., 1986, Computer Graphics, V20, P143, DOI 10.1145/15886.15902
   khronos, 2020, KHRONOS RAY TRACING
   Lafortune E. P., 1993, EDUGRAPHICS '93. First International Conference on Graphics Education. COMPUGRAPHICS '93. Third International Conference on Computational Graphics and Visualization Techniques. Combined Proceedings, P145
   Lin DQ, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530158
   Lowe N, 2005, IEEE T VIS COMPUT GR, V11, P81, DOI 10.1109/TVCG.2005.1
   Luan FJ, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392382
   Luebke D., 1995, Proceedings 1995 Symposium on Interactive 3D Graphics, P105, DOI 10.1145/199404.199422
   Müller T, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459812
   Müller T, 2017, COMPUT GRAPH FORUM, V36, P91, DOI 10.1111/cgf.13227
   Nguyen P., 2014, THESIS
   Ogaki S, 2020, P ACM COMPUT GRAPH, V3, DOI 10.1145/3406176
   Otsu H., 2020, P VIS MOD VIS
   Pixar Animation Studios, 2021, RENDERMAN 24 DOCUMEN
   Rath A, 2023, PROCEEDINGS OF SIGGRAPH 2023 CONFERENCE PAPERS, SIGGRAPH 2023, DOI 10.1145/3588432.3591543
   Rath A, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392441
   Ruppert L, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392421
   TELLER SJ, 1991, COMP GRAPH, V25, P61, DOI 10.1145/127719.122725
   Veach E., 1995, Photorealistic Rendering Techniques, P145
   Veach E., 2023, METROPOLIS LIGHT TRA, DOI [10.1145/3596711.3596744, DOI 10.1145/3596711.3596744]
   Veach E., 1998, ROBUST MONTE CARLO M, pAAI9837162
NR 35
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 5079
EP 5089
DI 10.1007/s00371-024-03464-6
EA JUN 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001254028100002
DA 2024-08-05
ER

PT J
AU Zhu, LC
   Wang, XW
   Zhong, WD
   Li, M
AF Zhu, Liangchao
   Wang, Xuwei
   Zhong, Weidong
   Li, Ming
TI Learning microstructure-property mapping via label-free 3D convolutional
   neural network
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Microstructure-property mapping; Label-free; 3D Convolutional neural
   network; Homogenization
ID TOPOLOGY OPTIMIZATION; HOMOGENIZATION; DESIGN; FRAMEWORK;
   RECONSTRUCTION; LINKAGES
AB Predicting the physical property of a class of microstructures is crucial in material design, structural simulation, and design. Property prediction may be conducted millions of times in these studies and is better derived instantly for computational efficiency. This issue is addressed in this study via building a mapping from a 3D microstructure to its effective material property, or called structure-property mapping, using a 3D convolutional neural network (CNN). Unlike the direct approach using labeled simulation data, the mapping is based on the physical knowledge of the structure-property relationship determined by its underlying PDE equations. The knowledge is embedded in the loss function of the CNN framework, which is designed and tested under several different formulations to improve its training convergence. Ultimately, the derived structure-property mapping can instantly predict the associated material property for a given microstructure and has far better generalization ability than the data-labeled approach, as demonstrated via numerical examples.
C1 [Zhu, Liangchao; Wang, Xuwei; Zhong, Weidong; Li, Ming] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
C3 Zhejiang University
RP Li, M (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
EM liming@cad.zju.edu.cn
FU National Key Research and Development Program [2020YFC2201303]; National
   Key Research and Development Program of China [62372401]; NSF of China
FX We would like to thank all the anonymous reviewers for their valuable
   comments and suggestions. The work described in this paper is partially
   supported by the National Key Research and Development Program of China
   (No. 2020YFC2201303), and the NSF of China (No. 62372401).
CR Andreassen E, 2014, COMP MATER SCI, V83, P488, DOI 10.1016/j.commatsci.2013.09.006
   Beck C., 2018, arXiv
   BENDSOE MP, 1988, COMPUT METHOD APPL M, V71, P197, DOI 10.1016/0045-7825(88)90086-2
   Bensoussan A., 1978, Asymptotic analysis for periodic structures
   Berg J, 2018, NEUROCOMPUTING, V317, P28, DOI 10.1016/j.neucom.2018.06.056
   Bessa MA, 2017, COMPUT METHOD APPL M, V320, P633, DOI 10.1016/j.cma.2017.03.037
   Cecen A, 2018, ACTA MATER, V146, P76, DOI 10.1016/j.actamat.2017.11.053
   Chen DS, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766889
   Chen J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201386
   Cheng L., 2015, SOLID FREEFORM FABRI, P10
   Conde-Rodríguez F, 2017, VISUAL COMPUT, V33, P17, DOI 10.1007/s00371-015-1149-0
   Dong GY, 2019, J ENG MATER-T ASME, V141, DOI 10.1115/1.4040555
   Dong LT, 2013, CMC-COMPUT MATER CON, V37, P1
   Fang Z, 2005, COMPUT AIDED DESIGN, V37, P65, DOI 10.1016/j.cad.2004.04.002
   Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45
   Garmestani H, 2001, J MECH PHYS SOLIDS, V49, P589, DOI 10.1016/S0022-5096(00)00040-5
   Golovin IS, 2003, J ALLOY COMPD, V355, P2, DOI 10.1016/S0925-8388(03)00241-X
   Groen JP, 2019, COMPUT METHOD APPL M, V349, P722, DOI 10.1016/j.cma.2019.02.031
   Groen JP, 2018, INT J NUMER METH ENG, V113, P1148, DOI 10.1002/nme.5575
   Gu JX, 2018, PATTERN RECOGN, V77, P354, DOI 10.1016/j.patcog.2017.10.013
   Holdstein Y, 2008, VISUAL COMPUT, V24, P295, DOI 10.1007/s00371-007-0202-z
   Kharevych L, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1531326.1531357, 10.1145/15313261531357]
   Kroner E., 1986, Modelling small deformations of polycrystals, P229
   Le BA, 2015, INT J NUMER METH ENG, V104, P1061, DOI 10.1002/nme.4953
   Lefik M, 2009, COMPUT METHOD APPL M, V198, P1785, DOI 10.1016/j.cma.2008.12.036
   Liu XC, 2016, COMPUT AIDED DESIGN, V78, P71, DOI 10.1016/j.cad.2016.05.017
   Liu Y, 2013, COMPUT AIDED DESIGN, V45, P65, DOI 10.1016/j.cad.2012.03.007
   Liu ZL, 2019, COMPUT METHOD APPL M, V345, P1138, DOI 10.1016/j.cma.2018.09.020
   Liu ZL, 2018, COMPUT METHOD APPL M, V330, P547, DOI 10.1016/j.cma.2017.11.005
   Michel JC, 1999, COMPUT METHOD APPL M, V172, P109, DOI 10.1016/S0045-7825(98)00227-8
   Smith LN, 2018, Arxiv, DOI [arXiv:1803.09820, DOI 10.48550/ARXIV.1803.09820]
   Nesme M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531358
   Peng H, 2022, ADDIT MANUF, V60, DOI 10.1016/j.addma.2022.103237
   Raissi M, 2019, J COMPUT PHYS, V378, P686, DOI 10.1016/j.jcp.2018.10.045
   Ronneberger O, 2015, Arxiv, DOI arXiv:1505.04597
   Sanchez-Palencia E., 1980, Non-homogeneous media and vibration theory
   SIGMUND O, 1994, INT J SOLIDS STRUCT, V31, P2313, DOI 10.1016/0020-7683(94)90154-6
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun BT, 2023, ADDIT MANUF, V72, DOI 10.1016/j.addma.2023.103626
   Tan ST, 1999, VISUAL COMPUT, V15, P90, DOI 10.1007/s003710050164
   Torquato S., 2002, Interdisciplinary Applied Mathematics, V55, pB62, DOI 10.1115/1.1483342
   Torres R, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982414
   White DA, 2019, COMPUT METHOD APPL M, V346, P1118, DOI 10.1016/j.cma.2018.09.007
   Wu J, 2021, IEEE T VIS COMPUT GR, V27, P43, DOI 10.1109/TVCG.2019.2938946
   Xia L., 2013, Comput. Methods Mater. Sci, V13, P219
   Xia L, 2015, COMPUT METHOD APPL M, V286, P147, DOI 10.1016/j.cma.2014.12.018
   Xu HY, 2015, J MECH DESIGN, V137, DOI 10.1115/1.4029768
   Yang N, 2014, COMPUT AIDED DESIGN, V56, P11, DOI 10.1016/j.cad.2014.06.006
   Yang X., 2021, INT DES ENG TECHN C, V85376
   Yang ZJ, 2018, COMP MATER SCI, V151, P278, DOI 10.1016/j.commatsci.2018.05.014
   Yoo DJ, 2011, INT J PRECIS ENG MAN, V12, P61, DOI 10.1007/s12541-011-0008-9
   Zhang Y, 2017, INT CONF ACOUST SPEE, P4845, DOI 10.1109/ICASSP.2017.7953077
   Zhao DY, 2022, VISUAL COMPUT, V38, P1065, DOI 10.1007/s00371-021-02068-8
   Zhu LC, 2019, VIS INFORM, V3, P69, DOI 10.1016/j.visinf.2019.07.002
   Zhu YH, 2019, J COMPUT PHYS, V394, P56, DOI 10.1016/j.jcp.2019.05.024
NR 55
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 24
PY 2024
DI 10.1007/s00371-024-03411-5
EA MAY 2024
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL4M0
UT WOS:001234594700004
DA 2024-08-05
ER

PT J
AU Chen, YC
   Chen, B
   Xian, WZ
   Wang, JJ
   Huang, Y
   Chen, M
AF Chen, Yichi
   Chen, Bin
   Xian, Weizhi
   Wang, Junjie
   Huang, Yao
   Chen, Min
TI LGFDR: local and global feature denoising reconstruction for
   unsupervised anomaly detection
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Unsupervised learning; Anomaly detection; Anomaly localization; Local
   and global anomalies; Feature denoising reconstruction
AB Unsupervised anomaly detection is a challenging task in many visual inspection scenarios and has attracted significant attention. Anomalies are typically related to local low-level features or require global semantic information to be detected. However, most of the existing methods fail to strike a balance between local and global features and thus lack versatility and practicality. To address this issue, we propose local and global feature denoising reconstruction (LGFDR). The proposed method can implicitly learn the latent distribution of local and global features for normal images via a dual-tower reconstruction network. Next, a selective reconstruction head (SRH) is designed to adaptively fuse the information from local and global reconstructions. Moreover, adding noise to the features proves a simple and general operation that can further enhance the generalization of reconstruction networks. On the MVTec AD benchmark, LGFDR achieves 98.8% and 65.3% of pixel-level AUROC and AP for anomaly localization and 99.3% of image-level AUROC for anomaly detection, respectively. In addition, a real-world metal plate surface defect detection project is adopted to validate LGFDR. Both the public and the practical experimental results show the effectiveness of our proposed approach. The code will be available at https://github.com/Karma1628/work-1.
C1 [Chen, Yichi; Chen, Min] Chinese Acad Sci, Chengdu Inst Comp Applicat, Chengdu 610041, Sichuan, Peoples R China.
   [Chen, Yichi; Chen, Bin; Chen, Min] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 100049, Peoples R China.
   [Chen, Bin; Wang, Junjie] HIT, Int Inst Art Intelligence, Shenzhen 518000, Guangdong, Peoples R China.
   [Chen, Bin; Xian, Weizhi] Harbin Inst Technol, Chongqing Res Inst, Chongqing 401120, Peoples R China.
   [Huang, Yao] Shanghai Spaceflight Precis Machinery Inst, Dept Test & Inspect, Shanghai 610101, Peoples R China.
C3 Chinese Academy of Sciences; Chengdu Institute of Computer Application,
   CAS; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS; Harbin Institute of Technology
RP Chen, B (corresponding author), Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 100049, Peoples R China.; Chen, B (corresponding author), HIT, Int Inst Art Intelligence, Shenzhen 518000, Guangdong, Peoples R China.; Chen, B (corresponding author), Harbin Inst Technol, Chongqing Res Inst, Chongqing 401120, Peoples R China.
EM chenyichi21@mails.ucas.ac.cn; chenbin2020@hit.edu.cn; wasxxwz@163.com;
   jjwanghz@stu.hit.edu.cn; yhuang96@163.com; susie1@cuit.edu.cn
RI Chen, Bin/IQV-6112-2023
OI Chen, Bin/0000-0002-3979-021X; Xian, Weizhi/0000-0001-5137-3542
FU Natural Science Foundation of Chongqing; Science and Technology Project
   of Shenzhen [GXWD-20220811170603002];  [CSTB2022NSCQ-MSX0922]
FX This research is supported by the Natural Science Foundation of
   Chongqing under Grant (CSTB2022NSCQ-MSX0922) and the Science and
   Technology Project of Shenzhen under Grant (GXWD-20220811170603002).
CR Akçay S, 2019, IEEE IJCNN, DOI 10.1109/ijcnn.2019.8851808
   Bergmann P, 2019, PROC CVPR IEEE, P9584, DOI 10.1109/CVPR.2019.00982
   Chen LY, 2022, NEURAL NETWORKS, V147, P53, DOI 10.1016/j.neunet.2021.12.008
   Cohen N, 2021, Arxiv, DOI arXiv:2005.02357
   Defard Thomas, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12664), P475, DOI 10.1007/978-3-030-68799-1_35
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Gauss YFA, 2019, IEEE IJCNN, DOI 10.1109/ijcnn.2019.8851829
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Lei JR, 2023, PROC CVPR IEEE, P14143, DOI 10.1109/CVPR52729.2023.01359
   Liang YF, 2023, IEEE T IMAGE PROCESS, V32, P4327, DOI 10.1109/TIP.2023.3293772
   Liu JQ, 2023, Arxiv, DOI arXiv:2301.11514
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZK, 2023, PROC CVPR IEEE, P20402, DOI 10.1109/CVPR52729.2023.01954
   Luth C.T., 2023, BVM WORKSH, P246
   Roth K, 2022, PROC CVPR IEEE, P14298, DOI 10.1109/CVPR52688.2022.01392
   Rudolph M, 2021, IEEE WINT CONF APPL, P1906, DOI 10.1109/WACV48630.2021.00195
   Shi Y, 2021, NEUROCOMPUTING, V424, P9, DOI 10.1016/j.neucom.2020.11.018
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song JW, 2021, Arxiv, DOI arXiv:2110.03396
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tao X, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3196436
   Tao X, 2022, IEEE T IND INFORM, V18, P7707, DOI 10.1109/TII.2022.3142326
   Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yang LX, 2021, PR MACH LEARN RES, V139
   You Zhiyuan, 2022, INT C NEURAL INFORM, P298
   Yu J., 2021, arXiv
   Zagoruyko S, 2017, Arxiv, DOI arXiv:1605.07146
   Zavrtanik V, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8310, DOI 10.1109/ICCV48922.2021.00822
   Zavrtanik V, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107706
NR 34
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 12
PY 2024
DI 10.1007/s00371-024-03281-x
EA MAY 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QJ3K5
UT WOS:001220466300002
DA 2024-08-05
ER

PT J
AU Zhou, ZZ
   Huo, YJ
   Huang, GH
   Zeng, A
   Chen, XH
   Huang, L
   Li, ZN
AF Zhou, Zhizhen
   Huo, Yejing
   Huang, Guoheng
   Zeng, An
   Chen, Xuhang
   Huang, Lian
   Li, Zinuo
TI QEAN: quaternion-enhanced attention network for visual dance generation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Dance generation; Multi-modal task; Quaternion network; Time-series
   prediction task; Animation generation task
AB The study of music-generated dance is a novel and challenging image generation task. It aims to input a piece of music and seed motions, then generate natural dance movements for the subsequent music. Transformer-based methods face challenges in time-series prediction tasks related to human movements and music due to their struggle in capturing the nonlinear relationship and temporal aspects. This can lead to issues like joint deformation, role deviation, floating, and inconsistencies in dance movements generated in response to the music. In this paper, we propose a quaternion-enhanced attention network for visual dance synthesis from a quaternion perspective, which consists of a spin position embedding (SPE) module and a quaternion rotary attention (QRA) module. First, SPE embeds position information into self-attention in a rotational manner, leading to better learning of features of movement sequences and audio sequences and improved understanding of the connection between music and dance. Second, QRA represents and fuses 3D motion features and audio features in the form of a series of quaternions, enabling the model to better learn the temporal coordination of music and dance under the complex temporal cycle conditions of dance generation. Finally, we conducted experiments on the dataset AIST++, and the results show that our approach achieves better and more robust performance in generating accurate, high-quality dance movements. Our source code and dataset can be available from https://github.com/MarasyZZ/QEAN and https://google.github.io/aistplusplus_dataset, respectively.
C1 [Zhou, Zhizhen; Huo, Yejing; Huang, Guoheng; Zeng, An] Guangdong Univ Technol, Guangzhou, Guangdong, Peoples R China.
   [Chen, Xuhang] Huizhou Univ, Huizhou, Guangdong, Peoples R China.
   [Huang, Lian] Guangdong Mech & Elect Coll, Guangzhou, Guangdong, Peoples R China.
   [Li, Zinuo] Univ Western Australia, Perth, WA 6009, Australia.
C3 Guangdong University of Technology; Huizhou University; University of
   Western Australia
RP Huang, GH (corresponding author), Guangdong Univ Technol, Guangzhou, Guangdong, Peoples R China.; Chen, XH (corresponding author), Huizhou Univ, Huizhou, Guangdong, Peoples R China.
EM 3121005415@mail2.gdut.edu.cn; 2112305121@mail2.gdut.edu.cn;
   kevinwong@gdut.edu.cn; zengan@gdut.edu.cn; xuhangc@hzu.edu.cn;
   mrhuanglian@gmail.com; zinuo.li@research.uwa.edu.au
RI Chen, Xuhang/JPW-8742-2023
OI Chen, Xuhang/0000-0001-6000-3914
FU Key Areas Research and Development Program of Guangzhou; National
   Natural Science Foundation [92267107]; Science and Technology Planning
   Project of Guangdong [2021B0101220006]; Science and Technology Projects
   in Guangzhou [202201011706]; Science and technology research in key
   areas in Foshan [2020001006832]; Key Area Research and Development
   Program of Guangdong Province [2018B010109007, 2019B010153002]; Science
   and technology projects of Guangzhou [202007040006]; Guangdong
   Provincial Key Laboratory of Cyber-Physical System [2020B1212060069]; 
   [2023B01J0029]
FX This work was supported in part by National Natural Science Foundation
   under Grant 92267107, the Science and Technology Planning Project of
   Guangdong under Grant 2021B0101220006, Science and Technology Projects
   in Guangzhou under Grant 202201011706, Key Areas Research and
   Development Program of Guangzhou under Grant 2023B01J0029, Science and
   technology research in key areas in Foshan under Grant 2020001006832,
   Key Area Research and Development Program of Guangdong Province under
   Grant 2018B010109007 and 2019B010153002, Science and technology projects
   of Guangzhou under Grant 202007040006, and Guangdong Provincial Key
   Laboratory of Cyber-Physical System under Grant 2020B1212060069.
CR Aristidou A, 2018, VISUAL COMPUT, V34, P1725, DOI 10.1007/s00371-017-1452-z
   Bai ZW, 2021, J INTERNET TECHNOL, V22, P913, DOI 10.53106/160792642021072204018
   Bengio S, 2015, Arxiv, DOI arXiv:1506.03099
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Du Z., 2021, ANN M ASS COMP LING
   Ghosh P, 2017, INT CONF 3D VISION, P458, DOI 10.1109/3DV.2017.00059
   Ginosar S, 2019, PROC CVPR IEEE, P3492, DOI 10.1109/CVPR.2019.00361
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Greenwood D, 2017, INTERSPEECH, P3991, DOI 10.21437/Interspeech.2017-894
   Hensel M, 2017, ADV NEUR IN, V30
   Holden D, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925975
   Huang Ruozi, 2020, INT C LEARN REPR
   Huang YH, 2022, INT CONF ACOUST SPEE, P4858, DOI 10.1109/ICASSP43922.2022.9747838
   Kundu JN, 2020, IEEE WINT CONF APPL, P2713, DOI 10.1109/WACV45572.2020.9093627
   Li Jiaman, 2020, arXiv
   Li L., 2021, arXiv
   Li RL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13381, DOI 10.1109/ICCV48922.2021.01315
   Li SY, 2022, PROC CVPR IEEE, P11040, DOI 10.1109/CVPR52688.2022.01077
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ma W., 2023, The Visual Computer
   McFee Brian, 2015, SCIPY
   Onuma Kensuke, 2008, Eurographics
   Pavllo D, 2020, INT J COMPUT VISION, V128, P855, DOI 10.1007/s11263-019-01245-6
   Qiu HB, 2019, IEEE I CONF COMP VIS, P4341, DOI 10.1109/ICCV.2019.00444
   Sheng B, 2022, IEEE T CYBERNETICS, V52, P6662, DOI 10.1109/TCYB.2021.3079311
   Siciliano M, 2017, J BUS FINANC LIBR, V22, P81, DOI 10.1080/08963568.2017.1285747
   Su JL, 2023, Arxiv, DOI arXiv:2104.09864
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Tendulkar P., 2020, arXiv
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Tsuchida S., 2019, INT SOC MUSIC INFORM
   Vaswani A, 2017, ADV NEUR IN, V30
   Wu CF, 2023, Arxiv, DOI arXiv:2303.04671
   Wu Z., 2023, EasyPhoto: your smart AI photo generator
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Yang Y, 2023, TRANS-FORM-ACAO, V46, P315, DOI 10.1590/0101-3173.2023.v46n4.p315
   Yu QH, 2023, Arxiv, DOI arXiv:2308.02487
   Zhang X., 2021, arXiv
   Zheng ZW, 2023, IEEE T CIRC SYST VID, V33, P2102, DOI 10.1109/TCSVT.2022.3223150
   Zhu Y., 2022, arXiv
NR 41
TC 0
Z9 0
U1 7
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 14
PY 2024
DI 10.1007/s00371-024-03376-5
EA APR 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NQ2Q6
UT WOS:001201858000002
DA 2024-08-05
ER

PT J
AU Lu, XB
   Liu, F
   Rong, Y
   Chen, YX
   Xiong, SW
AF Lu, Xiongbo
   Liu, Feng
   Rong, Yi
   Chen, Yaxiong
   Xiong, Shengwu
TI MakeupDiffuse: a double image-controlled diffusion model for exquisite
   makeup transfer
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Makeup transfer; Diffusion model; Controllable generation; Teacher
   module
ID STYLE TRANSFER
AB Makeup transfer is a challenging task, involving the transfer of a reference makeup style onto the source face while preserving the original appearance. Current GAN-based methods, representing makeup styles through reduced-dimensional matrices, often generate smooth high-frequency attributes and imprecise images. Additionally, these models are difficult to train and prone to model collapse problems. This paper introduces MakeupDiffuse, a diffusion-based model adapting a foundational diffusion model pre-trained on large-scale image datasets for makeup transfer. Fine-grained makeup transfer is achieved through a novel double image controller, controlling the identity process and adjusting the makeup style. To address the lack of paired data, we include another pre-trained makeup transfer network as a teacher module to supervise model training. Due to the flexible architecture, our model efficiently trains and generates faces with high perceptual quality and the expected makeup style. Unlike subjective evaluations based on user studies, we propose a new objective and comprehensive quantitative metric: Makeup Transfer Score, improving the current evaluation system. Extensive experiments demonstrate our approach's ability to generate natural makeup faces with exquisite details, achieving state-of-the-art performance.
C1 [Lu, Xiongbo; Liu, Feng; Rong, Yi; Chen, Yaxiong] Wuhan Univ Technol, Sch Comp & Artificial Intelligence, 122 Luoshi Rd, Wuhan 430070, Hubei, Peoples R China.
   [Xiong, Shengwu] Shanghai Artificial Intelligence Lab, Shanghai 200232, Peoples R China.
   [Xiong, Shengwu] Wuhan Univ Technol, Sanya Sci & Educ Innovat Pk, Sanya 572000, Peoples R China.
   [Xiong, Shengwu] Wuhan Huaxia Inst Technol, Sch Informat Engn, Wuhan 430223, Peoples R China.
C3 Wuhan University of Technology; Wuhan University of Technology; Wuhan
   Huaxia University of Technology
RP Xiong, SW (corresponding author), Shanghai Artificial Intelligence Lab, Shanghai 200232, Peoples R China.; Xiong, SW (corresponding author), Wuhan Univ Technol, Sanya Sci & Educ Innovat Pk, Sanya 572000, Peoples R China.; Xiong, SW (corresponding author), Wuhan Huaxia Inst Technol, Sch Informat Engn, Wuhan 430223, Peoples R China.
EM luxiongbo@whut.edu.cn; liuf@whut.edu.cn; yrong@whut.edu.cn;
   chenyaxiong@whut.edu.cn; xiongsw@whut.edu.cn
FU Innovative Research Group Project of the National Natural Science
   Foundation of China
FX No Statement Available
CR Afifi M, 2021, PROC CVPR IEEE, P7937, DOI 10.1109/CVPR46437.2021.00785
   Chang HW, 2018, PROC CVPR IEEE, P40, DOI 10.1109/CVPR.2018.00012
   Deng H, 2021, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR46437.2021.00648
   Dhariwal P, 2021, ADV NEUR IN, V34
   Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gu Q, 2019, IEEE I CONF COMP VIS, P10480, DOI 10.1109/ICCV.2019.01058
   Guo D, 2009, PROC CVPR IEEE, P73, DOI 10.1109/CVPRW.2009.5206833
   Hertz A., 2023, P INT C LEARN REPR I
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Ho J, 2022, J MACH LEARN RES, V23, P1
   Huang ZK, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P652
   Jiang WT, 2020, PROC CVPR IEEE, P5193, DOI 10.1109/CVPR42600.2020.00524
   Kim G, 2022, PROC CVPR IEEE, P2416, DOI 10.1109/CVPR52688.2022.00246
   Kwon G., 2023, P INT C LEARN REPR I
   Lan JY, 2023, VISUAL COMPUT, V39, P6167, DOI 10.1007/s00371-022-02719-4
   Li C, 2015, PROC CVPR IEEE, P4621, DOI 10.1109/CVPR.2015.7299093
   Li HY, 2022, NEUROCOMPUTING, V479, P47, DOI 10.1016/j.neucom.2022.01.029
   Li TT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P645, DOI 10.1145/3240508.3240618
   Liu DC, 2023, Arxiv, DOI arXiv:2312.11285
   Liu DC, 2024, IEEE T INF FOREN SEC, V19, P735, DOI 10.1109/TIFS.2023.3327666
   Liu DC, 2021, PATTERN RECOGN, V109, DOI 10.1016/j.patcog.2020.107579
   Liu W., 2023, Vis. Comput., P1
   Lugmayr A, 2022, PROC CVPR IEEE, P11451, DOI 10.1109/CVPR52688.2022.01117
   Lyu YM, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3601, DOI 10.1145/3474085.3475531
   Meng C., 2022, P INT C LEARN REPR I, P1
   Nitzan Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417826
   Organisciak D, 2021, INT C PATT RECOG, P6011, DOI 10.1109/ICPR48806.2021.9412604
   Radford A, 2021, PR MACH LEARN RES, V139
   Ramesh A., 2022, Hierarchical text-conditional image generation with clip latents, DOI 10.48550/arXiv.2204.06125
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Ronneberger P., 2015, MEDICAL IMAGE COMPUT, P234, DOI [10.1007/978-3-319-24574-4_28, DOI 10.1007/978-3-319-24574-4_28]
   Ruiz N, 2023, PROC CVPR IEEE, P22500, DOI 10.1109/CVPR52729.2023.02155
   Saharia C., 2022, P ACM SIGGRAPH, P1
   Saharia C., 2022, ADV NEURAL INF PROCE, V35, P36479
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sohl-Dickstein J, 2015, PR MACH LEARN RES, V37, P2256
   Song Jiaming, 2021, P INT C LEARN REPR I
   Song Y., 2021, P INT C LEARN REPR I
   Sun ZY, 2022, AAAI CONF ARTIF INTE, P2325
   Nguyen T, 2021, PROC CVPR IEEE, P13300, DOI 10.1109/CVPR46437.2021.01310
   Tiwari H, 2023, VISUAL COMPUT, V39, P6521, DOI 10.1007/s00371-022-02746-1
   Voynov A, 2023, PROCEEDINGS OF SIGGRAPH 2023 CONFERENCE PAPERS, SIGGRAPH 2023, DOI 10.1145/3588432.3591560
   Wang JY, 2023, AAAI CONF ARTIF INTE, P2555
   Xiang JF, 2022, LECT NOTES COMPUT SC, V13682, P719, DOI 10.1007/978-3-031-20047-2_41
   Yang CY, 2022, LECT NOTES COMPUT SC, V13676, P737, DOI 10.1007/978-3-031-19787-1_42
   Zhang H., 2019, DISENTANGLED MAKEUP
   Zhang LM, 2023, IEEE I CONF COMP VIS, P3813, DOI 10.1109/ICCV51070.2023.00355
   Zhu A, 2020, IEEE T IMAGE PROCESS, V29, P6932, DOI 10.1109/TIP.2020.2995062
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 50
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 18
PY 2024
DI 10.1007/s00371-024-03317-2
EA MAR 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LJ9G4
UT WOS:001186543400002
DA 2024-08-05
ER

PT J
AU Chen, M
   Jin, F
   Lu, Q
   Yu, QH
   Chen, W
   Li, X
AF Chen, Mei
   Jin, Fan
   Lu, Qiang
   Yu, Quanhao
   Chen, Wei
   Li, Xin
TI Small-scale block defect detection of fabric surface based on SCG-NET
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Fabric defect detection; Small-scale defect detection; Attention
   mechanism; Multi-scale learning; Lightweighting
ID INSPECTION
AB In contrast to the common small target detection problems, it is more difficult to locate and identify the small surface defects of fabric due to its own texture and complex background interference. Therefore, this paper proposes an effective detector for small-scale block defects on fabric surface by taking advantage of the backbone which integrates the Coordinate Attention module to enhance the acquisition of small-scale block defect location information. The FPN + PAN multi-scale detection structure is adopted to effectively integrate the feature information between different levels and deal with the multi-scale problem of defects. In the Neck section, a small target detection layer is set to expand the receptive field to prevent the loss of small-scale defect feature information. Moreover, we propose to use the GhostBottleneck module instead of the ordinary downsampling process to eliminate redundant convolutional calculations to improve the detection speed. The experimental results show that the optimal detection results of 0.56 and 0.842 are achieved in the detection recall and accuracy of the public fabric dataset; compared with other detectors, the result of small-scale defect detection rate is reduced by at least 2.7%, and the detection process meets the real-time requirement of automatic defect detection, which verifies the effectiveness of our method. Code and data are available at: https://github.com/VIMLab-hfut/SCG-NET.
C1 [Chen, Mei; Jin, Fan; Lu, Qiang; Yu, Quanhao; Chen, Wei; Li, Xin] Hefei Univ Technol, Hefei, Peoples R China.
C3 Hefei University of Technology
RP Li, X (corresponding author), Hefei Univ Technol, Hefei, Peoples R China.
EM chenmei@hfut.edu.cn; 2016217914@mail.hfut.edu.cn; luqiang@hfut.edu.cn;
   2020170377@mail.hfut.edu.cn; chenwei@hfut.edu.cn; lixin@hfut.edu.cn
FU the National Natural Science Foundation of China [201904d07020010];
   National Natural Science Foundation of China [IMIPY2021022]; Scientific
   and Technological Achievement Cultivation Project of Intelligent
   Manufacturing Research Institute of Hefei University of Technology
FX This work was supported in part by the National Natural Science
   Foundation of China (201904d07020010) and the Scientific and
   Technological Achievement Cultivation Project of Intelligent
   Manufacturing Research Institute of Hefei University of Technology
   (IMIPY2021022).
CR Abouelela A, 2005, PATTERN RECOGN LETT, V26, P1435, DOI 10.1016/j.patrec.2004.11.016
   [Anonymous], 2014, About Us
   Banumathi P., 2015, RES J APPL SCI ENG T, V9, P272, DOI [https://doi.org/10.19026/rjaset.9.1404, DOI 10.19026/RJASET.9.1404]
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Campbell JG, 1998, OPT ENG, V37, P2536, DOI 10.1117/1.601692
   Chen MQ, 2022, COMPUT IND, V134, DOI 10.1016/j.compind.2021.103551
   Chen Z., 2020, PROC EUR C COMPUT VI, P195, DOI DOI 10.1007/978-3-030-58558-7_12
   Chi-Ho Chan, 1999, Conference Record of the 1999 IEEE Industry Applications Conference. Thirty-Forth IAS Annual Meeting (Cat. No.99CH36370), P1743, DOI 10.1109/IAS.1999.805975
   dot, About us
   Ghiasi G, 2021, PROC CVPR IEEE, P2917, DOI 10.1109/CVPR46437.2021.00294
   Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165
   Han Y, 2007, IMAGE VISION COMPUT, V25, P1239, DOI 10.1016/j.imavis.2006.07.028
   Hanbay K, 2015, SIG PROCESS COMMUN, P735, DOI 10.1109/SIU.2015.7129932
   He D, 2019, OPT LASER ENG, V117, P40, DOI 10.1016/j.optlaseng.2019.01.011
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Jia L, 2017, J FRANKLIN I, V354, P5694, DOI 10.1016/j.jfranklin.2017.05.035
   Jia L, 2017, NEUROCOMPUTING, V238, P84, DOI 10.1016/j.neucom.2017.01.039
   Jing JF, 2017, OPT ENG, V56, DOI 10.1117/1.OE.56.9.093104
   Jocher G., 2023, YOLO by Ultralytics
   Jocher Glenn, 2022, Zenodo, DOI 10.5281/ZENODO.3908559
   Jun X, 2021, TEXT RES J, V91, P130, DOI 10.1177/0040517520935984
   Karlekar VV, 2015, 1ST INTERNATIONAL CONFERENCE ON COMPUTING COMMUNICATION CONTROL AND AUTOMATION ICCUBEA 2015, P712, DOI 10.1109/ICCUBEA.2015.145
   Kumar A, 2003, PATTERN RECOGN, V36, P1645, DOI 10.1016/S0031-3203(03)00005-0
   Lin T.-Y., 2017, PROC CVPR IEEE, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Liu GH, 2021, VISUAL COMPUT, V37, P515, DOI 10.1007/s00371-020-01820-w
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu Z., 2018, 9 INT C GRAPHIC IMAG, V10615
   Liu ZF, 2021, DISPLAYS, V68, DOI 10.1016/j.displa.2021.102008
   Ngan HYT, 2011, IMAGE VISION COMPUT, V29, P442, DOI 10.1016/j.imavis.2011.02.002
   Pengcheng Li, 2021, 2021 11th International Conference on Information Science and Technology (ICIST), P531, DOI 10.1109/ICIST52614.2021.9440580
   Qiao SY, 2021, PROC CVPR IEEE, P10208, DOI 10.1109/CVPR46437.2021.01008
   Sari L, 2014, INT C PATT RECOG, P1639, DOI 10.1109/ICPR.2014.290
   Sun PZ, 2021, PROC CVPR IEEE, P14449, DOI 10.1109/CVPR46437.2021.01422
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   tianchi.aliyun, Tianchi cup public tile dataset
   tianchi.aliyun, Tianchi cup public fabric dataset
   Wang CY, 2021, PROC CVPR IEEE, P13024, DOI 10.1109/CVPR46437.2021.01283
   Wang JW, 2022, Arxiv, DOI arXiv:2110.13389
   Wang JW, 2021, INT C PATT RECOG, P3791, DOI 10.1109/ICPR48806.2021.9413340
   Wang JW, 2021, IEEE T GEOSCI REMOTE, V59, P4307, DOI 10.1109/TGRS.2020.3010051
   Yang C., 2021, arXiv
   Yang Z, 2019, IEEE I CONF COMP VIS, P9656, DOI 10.1109/ICCV.2019.00975
   Yapi D, 2018, IEEE T AUTOM SCI ENG, V15, P1014, DOI 10.1109/TASE.2017.2696748
   Zhang GJ, 2019, IEEE T GEOSCI REMOTE, V57, P10015, DOI 10.1109/TGRS.2019.2930982
   Zhang S., 2020, P IEEE CVF C COMP VI, P9759
   Zhou XY, 2019, Arxiv, DOI arXiv:1904.07850
   Zhou XY, 2017, 2017 2ND IEEE INTERNATIONAL CONFERENCE ON INTEGRATED CIRCUITS AND MICROSYSTEMS (ICICM), P213, DOI 10.1109/ICAM.2017.8242171
   Zhu XZ, 2021, Arxiv, DOI [arXiv:2010.04159, 10.48550/arXiv.2010.04159]
NR 51
TC 0
Z9 0
U1 22
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 1
PY 2024
DI 10.1007/s00371-024-03289-3
EA MAR 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JF9E9
UT WOS:001171862800002
DA 2024-08-05
ER

PT J
AU Gong, Y
   Wu, P
   Xu, RJ
   Zhang, XM
   Wang, T
   Li, X
AF Gong, Yu
   Wu, Peng
   Xu, Renjie
   Zhang, Xiaoming
   Wang, Tao
   Li, Xuan
TI TripleFormer: improving transformer-based image classification method
   using multiple self-attention inputs
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image classification; Convolution neural networks; Visual transformer;
   Deep learning
AB Transformer network structures have significantly improved the performance of computer vision (CV) tasks. However, due to the restriction of taking high-dimensional tokens as model inputs and the singularity of tokenization method, it is computationally intensive and easily deteriorates local fine-grained features when recognizing images based on Transformer architecture. In this paper, we incorporate a novel triple self-attention mechanism as a single encoder block and integrate it with the Transformer structure to introduce a new model, namely TripleFormer. Firstly, modeling information inside the window rather than the entire image, we present two sequence inputs from orthogonal perspectives, which are named Patch Attention In Spatial (PAIS) and Patch Attention In Channel (PAIC) for capturing more detailed features. We further partition the multi-channel of one feature map along spatial dimensions and compute attention belonging to same channels. In this way, the proposed encoder block incorporates local feature extraction and long-range visual dependencies to boost the feature learning capability. Finally, experiments on ImageNet-1K and CIFAR100 datasets exhibit the superiority of our proposed models as compared to other methods in terms of lower FLOPs and complexity while maintaining similar accuracy. In addition, our models demonstrate competitive performance on small-scale datasets in comparison to other pure Transformer models.
C1 [Gong, Yu; Wu, Peng; Li, Xuan] Zhejiang Sci Tech Univ, Sch Informat Sci & Engn, Hangzhou 310018, Zhejiang, Peoples R China.
   [Xu, Renjie] Army Acad Armored Forces, Performance & Training Ctr, Beijing 100072, Peoples R China.
   [Zhang, Xiaoming; Wang, Tao] Army Acad Armored Forces, Dept Vehicle Engn, Beijing 100072, Peoples R China.
C3 Zhejiang Sci-Tech University
RP Wu, P (corresponding author), Zhejiang Sci Tech Univ, Sch Informat Sci & Engn, Hangzhou 310018, Zhejiang, Peoples R China.; Xu, RJ (corresponding author), Army Acad Armored Forces, Performance & Training Ctr, Beijing 100072, Peoples R China.; Zhang, XM (corresponding author), Army Acad Armored Forces, Dept Vehicle Engn, Beijing 100072, Peoples R China.
EM yugong_zstu@163.com; wupeng@zstu.edu.cn; 1728217581@qq.com;
   13661366835@139.com; 3387340132@qq.com
FU Natural Science Foundation of Zhejiang Province
FX No Statement Available
CR Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Carion N., 2020, EUR C COMP VIS, P213
   Chu XX, 2021, Arxiv, DOI arXiv:2102.10882
   Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Ding MY, 2022, LECT NOTES COMPUT SC, V13684, P74, DOI 10.1007/978-3-031-20053-3_5
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Girshick R., 2014, C COMPUTER VISION PA, DOI 10.1109/CVPR.2014.81
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Han K, 2021, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.2103.00112
   Hassani A, 2022, Arxiv, DOI [arXiv:2104.05704, 10.48550/arXiv.2104.05704]
   Hassani A, 2022, Arxiv, DOI arXiv:2204.07143
   He K., CVPR 2016
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Kingma D. P., 2014, arXiv
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Li YW, 2021, Arxiv, DOI arXiv:2104.05707
   Lin H., 2022, 2022 IEEE INT C MULT, P1
   Lin T.-Y., 2017, PROC CVPR IEEE, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Liu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4702, DOI 10.1109/ICCV48922.2021.00468
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Loshchilov I, 2017, Sgdr: Stochastic gradient descent with warm restarts, DOI [10.48550/arXiv.1608.03983, DOI 10.48550/ARXIV.1608.03983]
   Lu XK, 2022, IEEE T PATTERN ANAL, V44, P7885, DOI 10.1109/TPAMI.2021.3115815
   Lu XK, 2022, IEEE T PATTERN ANAL, V44, P2228, DOI 10.1109/TPAMI.2020.3040258
   Lu XK, 2019, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2019.00374
   Mehta S, 2022, Separable self-attention for mobile vision transformers
   Mehta S, 2022, Arxiv, DOI arXiv:2110.02178
   Pan JT, 2022, LECT NOTES COMPUT SC, V13671, P294, DOI 10.1007/978-3-031-20083-0_18
   Peng ZL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P357, DOI 10.1109/ICCV48922.2021.00042
   Radosavovic Ilija, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10425, DOI 10.1109/CVPR42600.2020.01044
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Tang H, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108792
   Tolstikhin I, 2021, ADV NEUR IN, V34
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wadekar SN, 2022, Arxiv, DOI [arXiv:2209.15159, DOI 10.48550/ARXIV.2209.15159]
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Xu WJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9961, DOI 10.1109/ICCV48922.2021.00983
   Yao ZY, 2021, Arxiv, DOI arXiv:2104.01318
   Yu FS, 2016, Arxiv, DOI arXiv:1511.07122
   Yuan K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P559, DOI 10.1109/ICCV48922.2021.00062
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang HY, 2018, Arxiv, DOI arXiv:1710.09412
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou XY, 2019, Arxiv, DOI arXiv:1904.07850
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 58
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 1
PY 2024
DI 10.1007/s00371-024-03294-6
EA MAR 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JF9E9
UT WOS:001171862800001
DA 2024-08-05
ER

PT J
AU Adolf, J
   Kán, PT
   Feuchtner, T
   Adolfová, B
   Dolezal, J
   Lhotská, L
AF Adolf, Jindrich
   Kan, Peter
   Feuchtner, Tiare
   Adolfova, Barbora
   Dolezal, Jaromir
   Lhotska, Lenka
TI Offistretch: camera-based real-time feedback for daily stretching
   exercises
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Exergame; Serious game; Body tracking; Pose estimation; Preventive
   rehabilitation; Human-computer interaction
AB In this paper, we present OffiStretch, a camera-based system for optimal stretching guidance at home or in the workplace. It consists of a vision-based method for real-time assessment of the user's body pose to provide visual feedback as interactive guidance during stretching exercises. Our method compares the users' actual pose with a pre-trained target pose to assess the quality of stretching for a number of different exercises. We utilize angular and spatial pose features to perform this comparison for each individual exercise. The result of this pose assessment is presented to the user as real-time visual feedback on an "augmented mirror" display. As our method relies simply on a single RGB camera, it can be easily utilized in everyday training scenarios. We validate our method in a user study, comparing users' performance and motivation in stretching when receiving audio-visual guidance on a TV screen both with and without our live feedback. While participants performed equally well in both conditions, feedback boosted their motivation to perform the exercises, highlighting its potential for increasing users' well-being. Moreover, our results suggest that participants preferred stretching exercises with our live feedback over the condition without the feedback. Finally, an expert evaluation with professional physiotherapists reveals that further work must target improvements of the feedback to ensure correct guidance during stretching.
C1 [Adolf, Jindrich; Dolezal, Jaromir; Lhotska, Lenka] Czech Tech Univ, Prague, Czech Republic.
   [Kan, Peter] TU Wien, Inst Visual Comp & Human Ctr Technol, Vienna, Austria.
   [Kan, Peter; Feuchtner, Tiare] Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.
   [Feuchtner, Tiare] Univ Konstanz, Human Comp Interact Grp, Constance, Germany.
   [Adolfova, Barbora] Acad Performing Arts Prague, Prague, Czech Republic.
C3 Czech Technical University Prague; Technische Universitat Wien; Aarhus
   University; University of Konstanz; Academy of Performing Arts Prague
RP Adolf, J (corresponding author), Czech Tech Univ, Prague, Czech Republic.
EM jindrich.adolf@cvut.cz
OI Feuchtner, Tiare/0000-0002-9922-5538
FU Intelligent Health Promotion Service System of the Technology Agency of
   the Czech Republic - Innovation Fund Denmark [TM03000048]
FX We would like to thank the physiotherapy specialists Matya & scaron;
   Turna, Michaela S & yacute;korova, and Tereza Skalova who helped with
   defining the stretching exercises and analyzing our user study results.
   We also thank the colleagues who helped design and conduct our user
   study, namely Tessa Goineau, Melina Monlouis and Eduarda Costa. Last but
   not least, we would like to thank all volunteers who participated in our
   survey and study. This research is supported by project No. TM03000048
   Intelligent Health Promotion Service System of the Technology Agency of
   the Czech Republic, partially funded by the Innovation Fund Denmark
   (MADE Fast project).
CR Ahmadyan A, 2020, Arxiv, DOI [arXiv:2006.13194, DOI 10.48550/ARXIV.2006.13194]
   Anderson F., 2013, Proceedings of the 26th annual ACM symposium on User interface software and technology, DOI [10.1145/2501988.2502045, DOI 10.1145/2501988.2502045]
   Andrade A, 2019, CYBERPSYCH BEH SOC N, V22, P724, DOI 10.1089/cyber.2019.0341
   Badiola-Bengoa A, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21185996
   Bailenson J, 2008, MEDIA PSYCHOL, V11, P354, DOI 10.1080/15213260802285214
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Chua PT, 2003, P IEEE VIRT REAL ANN, P87
   Colyer SL, 2018, SPORTS MED-OPEN, V4, DOI 10.1186/s40798-018-0139-y
   Daneshmandi Hadi, 2017, J Lifestyle Med, V7, P69, DOI 10.15280/jlm.2017.7.2.69
   Dittakavi B, 2022, IEEE COMPUT SOC CONF, P3539, DOI 10.1109/CVPRW56347.2022.00398
   Elsayed H, 2021, PROCEEDINGS OF THE 2021 ACM DESIGNING INTERACTIVE SYSTEMS CONFERENCE (DIS 2021), P1046, DOI 10.1145/3461778.3462026
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Fieraru M, 2021, PROC CVPR IEEE, P9914, DOI 10.1109/CVPR46437.2021.00979
   Filippeschi A, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17061257
   Gerling K., 2012, Conference on Human Factors in Computing Systems-Proceedings, P1873, DOI DOI 10.1145/2207676.2208324
   González-González CS, 2018, INT J INTERACT MULTI, V5, P46, DOI 10.9781/ijimai.2018.03.005
   Hayes G, 2019, J ADOLESCENT HEALTH, V65, P446, DOI 10.1016/j.jadohealth.2019.03.013
   Helten T., 2013, 2013 IEEE INT C COMP
   Hoang TN, 2016, PROCEEDINGS OF THE NORDICHI '16: THE 9TH NORDIC CONFERENCE ON HUMAN-COMPUTER INTERACTION - GAME CHANGING DESIGN, DOI 10.1145/2971485.2971521
   Jarrahi MH, 2018, PERS UBIQUIT COMPUT, V22, P433, DOI 10.1007/s00779-017-1099-9
   Kanase Rahul Ravikant, 2021, ITM Web of Conferences, V40, DOI 10.1051/itmconf/20214003031
   Kotowski SE, 2022, WORK, V71, P319, DOI 10.3233/WOR-211052
   Kyan M, 2015, ACM T INTEL SYST TEC, V6, DOI 10.1145/2735951
   Laranjo L, 2021, BRIT J SPORT MED, V55, DOI 10.1136/bjsports-2020-102892
   Lee IM, 2012, LANCET, V380, P219, DOI 10.1016/S0140-6736(12)61031-9
   Losilla F, 2019, ICSOFT: PROCEEDINGS OF THE 14TH INTERNATIONAL CONFERENCE ON SOFTWARE TECHNOLOGIES, P643, DOI 10.5220/0007798906430648
   Noury L., 2020, Journal of sports sciences, V39, P1
   Pacheco TBF, 2020, SYST REV-LONDON, V9, DOI 10.1186/s13643-020-01421-7
   Park HS, 2021, MULTIMED TOOLS APPL, V80, P31159, DOI 10.1007/s11042-020-10148-5
   Park JH, 2020, KOREAN J FAM MED, V41, P365, DOI 10.4082/kjfm.20.0165
   Rice M., 2011, P 2011 ACM SIGGRAPH, P17, DOI [10.1145/2018556.2018560, DOI 10.1145/2018556.2018560]
   Saldana J, 2013, The coding manual for qualitative researchers, DOI DOI 10.1017/CBO9781107415324.004
   Soares VN, 2021, ARCH GERONTOL GERIAT, V97, DOI 10.1016/j.archger.2021.104485
   Staiano AE, 2019, CYBERPSYCHOLOGY, V13, DOI 10.5817/CP2019-3-7
   Stamm Oskar, 2020, JMIR Mhealth Uhealth, V8, pe19608, DOI 10.2196/19608
   Yang Y, 2013, COMPUT EDUC, V61, P1, DOI 10.1016/j.compedu.2012.09.006
   Zhou Q., 2022, DESIGNING INTERACTIV
NR 37
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 28
PY 2024
DI 10.1007/s00371-024-03450-y
EA MAY 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SJ9P4
UT WOS:001234206700004
OA hybrid
DA 2024-08-05
ER

PT J
AU Wang, MS
   Mei, Y
   Yang, LC
   Tian, B
   Wu, KJ
AF Wang, Mengsi
   Mei, Yuan
   Yang, Lichun
   Tian, Bin
   Wu, Kaijun
TI SDR: stepwise deep rectangling model for stitched images
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image rectangling; Image stitching; Convolutional neural network;
   Stepwise warping
AB The image rectangling task aims to solve the problem of boundary irregularity in the stitched image without reducing the wide-field-of-view content information of the stitched image. Existing image rectangling methods are either limited by application scenarios, or have a little incomplete phenomenon in the rectangular boundary. To this end, we propose a stepwise deep rectangling model based on the idea of stepwise regression for the general image rectangling task. Considering the influence of factors such as luminosity differences in various regions of the image, we introduce a shallow feature encoder to eliminate the influence of such factors on mesh prediction. At the same time, we embed the mask information into the encoded image to constrain the network to learn a rectangular image with complete boundary. Subsequently, we perform mesh cumulative regression prediction based on the multi-level features extracted by the feature extractor. Experimental results show that the proposed method performs well in a variety of stitched image rectangling scenarios, exhibiting state-of-the-art performance in both qualitative and quantitative comparisons.
C1 [Wang, Mengsi; Mei, Yuan; Tian, Bin; Wu, Kaijun] Lanzhou Jiaotong Univ, Sch Elect & Informat Engn, Lanzhou 730070, Peoples R China.
   [Yang, Lichun] Lanzhou Jiaotong Univ, Key Lab Opt Elect Technol & Intelligent Control, Minist Educ, Lanzhou 730070, Peoples R China.
C3 Lanzhou Jiaotong University; Lanzhou Jiaotong University
RP Wu, KJ (corresponding author), Lanzhou Jiaotong Univ, Sch Elect & Informat Engn, Lanzhou 730070, Peoples R China.
EM wkj@mail.lzjtu.cn
OI wang, mengsi/0000-0001-5888-090X
FU the Key Talent Project of Gansu Province [23JRRA860]; Natural Science
   Foundation Key Project of Gansu Province; Key Talent Project of Gansu
   Province [2023YFSH0043, 2023YFDZ0043]; Inner Mongolia Key R &D and
   Achievement Transformation Project [ZDYF2304]; Key Research and
   Development Project of Lanzhou Jiaotong University
FX This work was supported by the Natural Science Foundation Key Project of
   Gansu Province (No. 23JRRA860), the Key Talent Project of Gansu
   Province, the Inner Mongolia Key R &D and Achievement Transformation
   Project (Nos. 2023YFSH0043, 2023YFDZ0043) and the Key Research and
   Development Project of Lanzhou Jiaotong University (No. ZDYF2304)
CR [Anonymous], 2010, P 17 ACM S VIRTUAL R
   Anzid H, 2023, VISUAL COMPUT, V39, P1667, DOI 10.1007/s00371-022-02435-z
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Ballester C, 2001, IEEE T IMAGE PROCESS, V10, P1200, DOI 10.1109/83.935036
   Cai N, 2017, VISUAL COMPUT, V33, P249, DOI 10.1007/s00371-015-1190-z
   Chan TF, 2001, J VIS COMMUN IMAGE R, V12, P436, DOI 10.1006/jvci.2001.0487
   Chang CH, 2014, PROC CVPR IEEE, P3254, DOI 10.1109/CVPR.2014.422
   Chang CH, 2012, PROC CVPR IEEE, P1075, DOI 10.1109/CVPR.2012.6247786
   Chen J, 2022, VISUAL COMPUT, V38, P3191, DOI 10.1007/s00371-022-02564-5
   Chen YS, 2016, LECT NOTES COMPUT SC, V9909, P186, DOI 10.1007/978-3-319-46454-1_12
   Chen YT, 2021, VISUAL COMPUT, V37, P1691, DOI 10.1007/s00371-020-01932-3
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   DeTone D, 2016, Arxiv, DOI arXiv:1606.03798
   Dong WM, 2012, J COMPUT SCI TECH-CH, V27, P121, DOI 10.1007/s11390-012-1211-6
   Gaddam VR, 2016, IEEE T MULTIMEDIA, V18, P1819, DOI 10.1109/TMM.2016.2586304
   Gao JL, 2023, VISUAL COMPUT, DOI 10.1007/s00371-023-03065-9
   Garg A, 2023, VISUAL COMPUT, V39, P2683, DOI 10.1007/s00371-022-02486-2
   Guo YW, 2009, IEEE T MULTIMEDIA, V11, P856, DOI 10.1109/TMM.2009.2021781
   Han DF, 2010, VISUAL COMPUT, V26, P749, DOI 10.1007/s00371-010-0480-8
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462004
   Hensel M, 2017, ADV NEUR IN, V30
   Hossein-Nejad Z, 2022, VISUAL COMPUT, V38, P1991, DOI 10.1007/s00371-021-02261-9
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Jin Y, 2010, VISUAL COMPUT, V26, P769, DOI 10.1007/s00371-010-0472-8
   Jirong Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P653, DOI 10.1007/978-3-030-58452-8_38
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kajiura N, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1755, DOI 10.1145/3394171.3413857
   Karni Z, 2009, COMPUT GRAPH FORUM, V28, P1257, DOI 10.1111/j.1467-8659.2009.01503.x
   Kopf J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366150
   Krishnakumar K, 2020, VISUAL COMPUT, V36, P1837, DOI 10.1007/s00371-019-01780-w
   Li DB, 2019, IEEE T IMAGE PROCESS, V28, P5105, DOI 10.1109/TIP.2019.2914360
   Li DP, 2015, PROC CVPR IEEE, P213, DOI 10.1109/CVPR.2015.7298617
   Li J, 2018, IEEE T MULTIMEDIA, V20, P1672, DOI 10.1109/TMM.2017.2777461
   Liao TL, 2023, Arxiv, DOI arXiv:2202.06276
   Lin KM, 2016, LECT NOTES COMPUT SC, V9907, P370, DOI 10.1007/978-3-319-46487-9_23
   Lin SS, 2013, IEEE T MULTIMEDIA, V15, P359, DOI 10.1109/TMM.2012.2228475
   Liu YJ, 2019, VISUAL COMPUT, V35, P667, DOI 10.1007/s00371-018-1502-1
   Lu P, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P120, DOI 10.1145/3394171.3413824
   Lu P, 2021, IEEE T MULTIMEDIA, V23, P3618, DOI 10.1109/TMM.2020.3029882
   Mastan ID, 2020, IEEE WINT CONF APPL, P2355, DOI [10.1109/wacv45572.2020.9093637, 10.1109/WACV45572.2020.9093637]
   Nie L, 2021, Arxiv, DOI [arXiv:2107.02524, DOI 10.1109/TCSVT.2021.3125736]
   Nie L, 2022, PROC CVPR IEEE, P5730, DOI 10.1109/CVPR52688.2022.00565
   Nie L, 2021, IEEE T IMAGE PROCESS, V30, P6184, DOI 10.1109/TIP.2021.3092828
   Nie L, 2020, J VIS COMMUN IMAGE R, V73, DOI 10.1016/j.jvcir.2020.102950
   Noh H., 2012, P 20 ACM INT C MULTI, P709, DOI DOI 10.1145/2393347.2396293
   Oliveira SAF, 2016, EXPERT SYST APPL, V44, P332, DOI 10.1016/j.eswa.2015.09.015
   Shocher A, 2019, IEEE I CONF COMP VIS, P4491, DOI 10.1109/ICCV.2019.00459
   Simakov D, 2008, PROC CVPR IEEE, P3887
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Suvorov R, 2022, IEEE WINT CONF APPL, P3172, DOI 10.1109/WACV51458.2022.00323
   Tan WM, 2020, IEEE T MULTIMEDIA, V22, P1730, DOI 10.1109/TMM.2019.2959925
   Teterwak P, 2019, IEEE I CONF COMP VIS, P10520, DOI 10.1109/ICCV.2019.01062
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
   Xu JJ, 2024, VISUAL COMPUT, V40, P87, DOI 10.1007/s00371-022-02767-w
   Yan ZY, 2018, LECT NOTES COMPUT SC, V11218, P3, DOI 10.1007/978-3-030-01264-9_1
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Zhang F, 2014, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2014.423
   Zhang JC, 2023, VISUAL COMPUT, V39, P4915, DOI 10.1007/s00371-022-02637-5
   Zhang JD, 2024, VISUAL COMPUT, V40, P427, DOI 10.1007/s00371-023-02791-4
   Zhang Y, 2021, IEEE T VIS COMPUT GR, V27, P3198, DOI 10.1109/TVCG.2020.2965097
   Zhou Y, 2021, IEEE T CIRC SYST VID, V31, P126, DOI 10.1109/TCSVT.2020.2977943
   Zhu LL, 2016, INT CONF ACOUST SPEE, P1706, DOI 10.1109/ICASSP.2016.7471968
   Zhu LL, 2016, 30 IEEE C VISUAL COM
NR 64
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 6
PY 2024
DI 10.1007/s00371-024-03407-1
EA MAY 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PM5S0
UT WOS:001214514300001
DA 2024-08-05
ER

PT J
AU Zheng, LH
   Xu, WR
   Miao, ZJ
   Qiu, XX
   Gong, SS
AF Zheng, Lihuan
   Xu, Wanru
   Miao, Zhenjiang
   Qiu, Xinxiu
   Gong, Shanshan
TI RESTHT: relation-enhanced spatial-temporal hierarchical transformer for
   video captioning
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Video captioning; Spatial-temporal modeling; Transformer; GCN
AB The video captioning task is generating description sentences by learning semantic information. It has a wide range of applications in areas such as video retrieval, automatic generation of subtitles and blind assistance. Visual semantic information plays a decisive role in video captioning. However, traditional methods are relatively rough for video feature modeling, failing to harness local and global features to understand temporal and spatial relationships. In this paper, we propose a video captioning model based on the Transformer and GCN network called "Relation-Enhanced Spatial-Temporal Hierarchical Transformer" (RESTHT). To address the above issues, we present a spatial-temporal hierarchical network framework to jointly model local and global features in terms of both time and space. For temporal modeling, our model learns the direct interactions between diverse video features and sentence features in the temporal sequence via the pre-trained GPT2, and the global feature construction encourages it to capture essential and relevant information. For spatial modeling, we use self-attention and GCN networks to learn the spatial relationship from appearance and motion perspectives jointly. Through spatial-temporal modeling, our method can comprehend the global time-space relationships of complex events in videos and catch the interaction between different objects to generate more accurate descriptions applicable to universal video captioning tasks. We conducted experiments on two widely used datasets, and especially in the MSVD dataset, our model improves the score of CIDEr by 6.1 compared to the baseline and excels present methods by 13. The results verify that our model can fully model the temporal and spatial relationship and outperforms other related models.
C1 [Zheng, Lihuan; Xu, Wanru; Miao, Zhenjiang; Qiu, Xinxiu; Gong, Shanshan] Beijing Jiaotong Univ, Inst Informat Sci, 3 Shangyuancun, Beijing 100044, Peoples R China.
C3 Beijing Jiaotong University
RP Zheng, LH (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, 3 Shangyuancun, Beijing 100044, Peoples R China.
EM 15801561689@163.com
FU Fundamental Research Funds for the Central Universities [4242028];
   Beijing Natural Science Foundation [62006015, 61971446]; NSFC; CEFLA
   Audio-Video Restoration and Evaluation Key Lab ofMinistry of Culture and
   Tourism
FX This work is supported by Beijing Natural Science Foundation 4242028,
   the NSFC 62006015, 61971446, and CEFLA Audio-Video Restoration and
   Evaluation Key Lab ofMinistry of Culture and Tourism.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Banerjee S., 2005, P ACL WORKSHOP INTRI, P65, DOI DOI 10.3115/1626355.1626389
   Bertasius G, 2021, PR MACH LEARN RES, V139
   Boxiao Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10867, DOI 10.1109/CVPR42600.2020.01088
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen David, 2011, ACL
   Chen HR, 2020, FRONT ROBOT AI, V7, DOI 10.3389/frobt.2020.475767
   Deng JC, 2022, IEEE T CIRC SYST VID, V32, P880, DOI 10.1109/TCSVT.2021.3063423
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Du XT, 2019, VISUAL COMPUT, V35, P1703, DOI 10.1007/s00371-018-1591-x
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu YS, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P774, DOI 10.1145/3343031.3351072
   Jiang B, 2022, NEURAL NETWORKS, V153, P204, DOI 10.1016/j.neunet.2022.05.024
   Kingma D. P., 2014, arXiv
   Lei J, 2020, Arxiv, DOI arXiv:2005.05402
   Li ZK, 2021, IEEE-ACM T AUDIO SPE, V29, P2476, DOI 10.1109/TASLP.2021.3065823
   Lin C.-Y., 2004, ANN M ASS COMP LING, P74
   Luo HS, 2020, Arxiv, DOI arXiv:2002.06353
   Pan PB, 2016, PROC CVPR IEEE, P1029, DOI 10.1109/CVPR.2016.117
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Peng YQ, 2022, VISUAL COMPUT, V38, P4267, DOI 10.1007/s00371-021-02294-0
   Radford A., 2019, OpenAI blog, V1, P9
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Roy AM, 2023, ADV ENG INFORM, V56, DOI 10.1016/j.aei.2023.102007
   Ryu H, 2021, AAAI CONF ARTIF INTE, V35, P2514
   Shaoxiang Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P333, DOI 10.1007/978-3-030-58548-8_20
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun B, 2023, VISUAL COMPUT, V39, P9, DOI 10.1007/s00371-021-02309-w
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan GC, 2020, Arxiv, DOI [arXiv:2007.09049, DOI 10.24963/IJCAI.2020/104]
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang LX, 2022, IEEE T CIRC SYST VID, V32, P4751, DOI 10.1109/TCSVT.2021.3131721
   Wang YK, 2022, PROC CVPR IEEE, P12176, DOI 10.1109/CVPR52688.2022.01187
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Zhang JM, 2023, PROC CVPR IEEE, P1136, DOI 10.1109/CVPR52729.2023.00116
   Zhang JC, 2019, PROC CVPR IEEE, P8319, DOI 10.1109/CVPR.2019.00852
   Zhang ZQ, 2021, PROC CVPR IEEE, P9832, DOI 10.1109/CVPR46437.2021.00971
   Ziqi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13275, DOI 10.1109/CVPR42600.2020.01329
NR 46
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 18
PY 2024
DI 10.1007/s00371-024-03350-1
EA APR 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC4R2
UT WOS:001205052300001
DA 2024-08-05
ER

PT J
AU Singh, RP
   Singh, LD
AF Singh, Rohit Pratap
   Singh, Laiphrakpam Dolendro
TI Dyhand: dynamic hand gesture recognition using BiLSTM and soft attention
   methods
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Dynamic hand gesture; BiLSTM; Attention; Human-computer interaction;
   Skeleton-based gesture recognition; Deep learning
ID NEURAL-NETWORKS; SKELETON; DEPTH
AB Hand gesture recognition is an essential task in computer vision. It is the most intuitive and natural medium for communication when dealing with computers. Recently, with the advent of innovative technologies and high performing computer systems, there has been a surge in the research of Gesture Recognition. Traditional approaches to modelling skeletons are typically based on hand-crafted components or traversal algorithms, leading to limited expressive capacity and generalisation challenges. In this work, we present a novel dynamic skeleton model based on BiLSTM and soft attention named DyHand that mitigates the challenges of intra-class and inter-class variability of gesture classes to a great extent. The comparison of our model with state-of-the-art approaches on the two benchmark data sets with various data augmentation techniques is reported. The proposed approach yields the best results, achieving 97.14 and 96.42% recognition accuracy in the 14 and 28 gesture categories, respectively, for the DHG-14/28 data set and comparable recognition accuracy of 93.98% on 14 gesture classes and 87.86% on 28 gesture classes, respectively, in case of SHREC'17 data set.
C1 [Singh, Rohit Pratap; Singh, Laiphrakpam Dolendro] Natl Inst Technol Silchar, Dept Comp Sci & Engn, Silchar 788010, Assam, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Silchar
RP Singh, RP (corresponding author), Natl Inst Technol Silchar, Dept Comp Sci & Engn, Silchar 788010, Assam, India.
EM rohitkako@gmail.com; ldsingh.cse@gmail.com
OI Singh, Rohit Pratap/0000-0002-6495-3586
CR Biswas K. K., 2011, 2011 5th International Conference on Automation, Robotics and Applications (ICARA 2011), P100, DOI 10.1109/ICARA.2011.6144864
   Boulahia SY, 2017, INT CONF IMAG PROC
   Boulahia SY, 2016, INT C PATT RECOG, P985, DOI 10.1109/ICPR.2016.7899764
   Caputo FM, 2018, COMPUT GRAPH-UK, V73, P17, DOI 10.1016/j.cag.2018.02.009
   Chen XH, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19020239
   Chen XH, 2017, IEEE IMAGE PROC, P2881, DOI 10.1109/ICIP.2017.8296809
   Chen YX, 2019, Arxiv, DOI [arXiv:1907.08871, 10.48550/arXiv.1907.08871]
   Cho KYHY, 2014, Arxiv, DOI [arXiv:1406.1078, 10.48550/arXiv.1406.1078]
   Cippitelli E, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/4351435
   Corbetta M, 2002, NAT REV NEUROSCI, V3, P201, DOI 10.1038/nrn755
   De Smedt Q., 2016, P IEEE C COMPUTER VI, P1
   De Smedt Q, 2019, COMPUT VIS IMAGE UND, V181, P60, DOI 10.1016/j.cviu.2019.01.008
   DeSmedt Q., 2017, PhD thesis
   Devanne M, 2015, IEEE T CYBERNETICS, V45, P1340, DOI 10.1109/TCYB.2014.2350774
   Devineau G, 2018, IEEE INT CONF AUTOMA, P106, DOI 10.1109/FG.2018.00025
   Evangelidis G, 2014, INT C PATT RECOG, P4513, DOI 10.1109/ICPR.2014.772
   Freeman William T, 1995, P INT WORKSH AUT FAC, V12, P296
   Garcia-Hernando G, 2018, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2018.00050
   Glorot X., 2010, P 13 INT C ART INT S, P249
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Hou J., 2018, Proceedings of the European Conference on Computer Vision (ECCV), P0
   Ionescu B, 2005, EURASIP J APPL SIG P, V2005, P2101, DOI 10.1155/ASP.2005.2101
   Jain R, 2022, VISUAL COMPUT, V38, P1957, DOI 10.1007/s00371-021-02259-3
   Kingma D. P., 2014, arXiv
   Lai K, 2018, INT C PATT RECOG, P3451, DOI 10.1109/ICPR.2018.8545718
   Laurent T., 2016, arXiv
   Liu JB, 2020, PROC CVPR IEEE, P5750, DOI 10.1109/CVPR42600.2020.00579
   Liu JF, 2024, IEEE T MULTIMEDIA, V26, P811, DOI 10.1109/TMM.2023.3271811
   Liu J, 2018, IEEE T IMAGE PROCESS, V27, P1586, DOI 10.1109/TIP.2017.2785279
   Luo JJ, 2013, IEEE I CONF COMP VIS, P1809, DOI 10.1109/ICCV.2013.227
   Ma CY, 2018, VISUAL COMPUT, V34, P1053, DOI 10.1007/s00371-018-1556-0
   Maghoumi M, 2020, LECT NOTES COMPUT SC, V11844, P16, DOI 10.1007/978-3-030-33720-9_2
   Mahmud H, 2024, VISUAL COMPUT, V40, P11, DOI 10.1007/s00371-022-02762-1
   Marin G, 2016, MULTIMED TOOLS APPL, V75, P14991, DOI 10.1007/s11042-015-2451-6
   Miah ASM, 2023, IEEE ACCESS, V11, P4703, DOI 10.1109/ACCESS.2023.3235368
   Molchanov Pavlo, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P1, DOI 10.1109/CVPRW.2015.7301342
   Neverova N, 2016, IEEE T PATTERN ANAL, V38, P1692, DOI 10.1109/TPAMI.2015.2461544
   Neverova N, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P484, DOI 10.1109/ICCVW.2013.69
   Thang ND, 2011, APPL INTELL, V35, P163, DOI 10.1007/s10489-009-0209-4
   Nguyen XS, 2019, PROC CVPR IEEE, P12028, DOI 10.1109/CVPR.2019.01231
   Núñez JC, 2018, PATTERN RECOGN, V76, P80, DOI 10.1016/j.patcog.2017.10.033
   Oberweger M, 2016, Arxiv, DOI arXiv:1502.06807
   Oberweger M, 2017, IEEE INT CONF COMP V, P585, DOI 10.1109/ICCVW.2017.75
   Ohn-Bar E, 2013, IEEE COMPUT SOC CONF, P465, DOI 10.1109/CVPRW.2013.76
   Oord A.v.d., 2016, arXiv, DOI DOI 10.48550/ARXIV.1609.03499
   Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98
   Quentin De Smedt J-PV, 2017, SHREC 17 TRACK 3D HA
   Reddy KS, 2011, COMM COM INF SC, V198, P346
   Singh A, 2022, MULTIMEDIA SYST, V28, P195, DOI 10.1007/s00530-021-00816-3
   Song JH, 2022, IEEE T CIRC SYST VID, V32, P6227, DOI 10.1109/TCSVT.2022.3165069
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tai D, 2020, KSII T INTERNET INF, V14, P3924, DOI 10.3837/tiis.2020.09.020
   Vaswani A, 2017, ADV NEUR IN, V30
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang C, 2014, INT WORK COGNIT INFO
   Wang C, 2015, IEEE T MULTIMEDIA, V17, P29, DOI 10.1109/TMM.2014.2374357
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang S, 2023, VISUAL COMPUT, V39, P4487, DOI 10.1007/s00371-022-02602-2
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang H, 2022, IEEE T IMAGE PROCESS, V31, P164, DOI 10.1109/TIP.2021.3129117
   Yang X., 2012, 2012 IEEE computer society conference on computer vision and pattern recognition workshops, P14, DOI [10.1109/CVPRW.2012.6239232, DOI 10.1109/CVPRW.2012.6239232]
   Zhang W, 2020, VISUAL COMPUT, V36, P2433, DOI 10.1007/s00371-020-01955-w
   Zhang XK, 2016, PROC CVPR IEEE, P4498, DOI 10.1109/CVPR.2016.487
NR 65
TC 0
Z9 0
U1 10
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 18
PY 2024
DI 10.1007/s00371-024-03307-4
EA MAR 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LJ9G4
UT WOS:001186543400003
DA 2024-08-05
ER

PT J
AU Rai, A
AF Rai, Arpit
TI Learning geometric invariants through neural networks
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Computer vision; Shape recognition; Geometric invariants; Image
   recognition
AB Convolution neural networks have become a fundamental model for solving various computer vision tasks. However, these operations are only invariant to translations of objects and their performance suffer under rotation and other affine transformations. This work proposes a novel neural network that leverages geometric invariants, including curvature, higher-order differentials of curves extracted from object boundaries at multiple scales, and the relative orientations of edges. These features are invariant to affine transformation and can improve the robustness of shape recognition in neural networks. Our experiments on the smallNORB dataset with a 2-layer network operating over these geometric invariants outperforms a 3-layer convolutional network by 9.69% while being more robust to affine transformations, even when trained without any data augmentations. Notably, our network exhibits a mere 6% degradation in test accuracy when test images are rotated by 40 degrees\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$<^>{\circ }$$\end{document}, in contrast to significant drops of 51.7 and 69% observed in VGG networks and convolution networks, respectively, under the same transformations. Additionally, our models show superior robustness than invariant feature descriptors such as the SIFT-based bag-of-words classifier, and its rotation invariant extension, the RIFT descriptor that suffer drops of 35 and 14.1% respectively, under similar image transformations. Our experimental results further show improved robustness against scale and shear transformations. Furthermore, the multi-scale extension of our geometric invariant network, that extracts curve differentials of higher orders, show enhanced robustness to scaling and shearing transformations.
C1 [Rai, Arpit] DPDGroup UK, DPDDepot, Broadwell Rd, Birmingham B69 4DA, Scotland.
RP Rai, A (corresponding author), DPDGroup UK, DPDDepot, Broadwell Rd, Birmingham B69 4DA, Scotland.
EM arpit.manu6@gmail.com
OI Rai, Arpit/0000-0002-2884-1122
FU University of Edinburgh as a parent institution
FX The author would like to thank the University of Edinburgh as a parent
   institution and resources and the GPU provided by the Google
   Colaboratory team used for training and testing the models.DAS:All the
   datasets that are analyzed and used in our experiments are freely
   available for public use and research in the following repositories
   https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/, and as a Mendeley
   format at https://data.mendeley.com/datasets/55xv4y25rs and can be cited
   as: Rai, Arpit (2022), "smallNORB", Mendeley Data, V1, doi:
   10.17632/55xv4y25rs.1. The different transformations that were applied
   to the datasets during the experiments are part of the Tensorflow, the
   tensorflow image processing, and the tensorflow-addons library.
CR Ahmed MAGES, 2020, EMERG MICROBES INFEC, V9, P868, DOI 10.1080/22221751.2020.1754133
   Albawi S, 2017, I C ENG TECHNOL
   Belongie S., 2000, Advances in neural information processing systems, V13
   Cohen TS, 2016, PR MACH LEARN RES, V48
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Geirhos R., 2019, INT C LEARN REPR ICL
   Hinton N., 2012, Improving neural networks by preventing co-adaptation of feature detectors
   KANOPOULOS N, 1988, IEEE J SOLID-ST CIRC, V23, P358, DOI 10.1109/4.996
   Koushik J, 2016, Arxiv, DOI arXiv:1605.09081
   Krizhevsky A., 2012, Learning multiple layers of features from tiny images, DOI DOI 10.1145/3079856.3080246
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lazebnik S., 2004, BRIT MACH VIS C BMVC, P779, DOI [DOI 10.5244/C.18.98, 10.5244/C.18.98]
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MARR D, 1978, PROC R SOC SER B-BIO, V200, P269, DOI 10.1098/rspb.1978.0020
   Mokhtarian F., 2003, Curvature scale space representation: theory, applications, and MPEG-7 Standardization, P215
   Mukhopadhyay P, 2015, PATTERN RECOGN, V48, P993, DOI 10.1016/j.patcog.2014.08.027
   Nair V., 2010, Proceedings of the 27th International Conference on Machine Learning (ICML-10), P807
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
NR 18
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 22
PY 2024
DI 10.1007/s00371-024-03398-z
EA JUL 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZF7P7
UT WOS:001273949500002
DA 2024-08-05
ER

PT J
AU Fang, H
   Weng, DD
   Tian, ZY
   Ma, Y
AF Fang, Hui
   Weng, Dongdong
   Tian, Zeyu
   Ma, Yin
TI Manitalk: manipulable talking head generation from single image in the
   wild
SO VISUAL COMPUTER
LA English
DT Article
DE Facial animation; Expression manipulation; Gaze manipulation; Neural
   network
AB Generating talking head videos through a face image and a piece of speech audio has gained widespread interest. Existing talking face synthesis methods typically lack the ability to generate manipulable facial details and pupils, which is desirable for producing stylized facial expressions. We present ManiTalk, the first manipulable audio-driven talking head generation system. Our system consists of three stages. In the first stage, the proposed Exp Generator and Pose Generator generate synchronized talking landmarks and presentation-style head poses. In the second stage, we parameterize the positions of eyebrows, eyelids, and pupils, enabling personalized and straightforward manipulation of facial details. In the last stage, we introduce SFWNet to warp facial images based on the landmark motions. Additional driving sketches are input to generate more precise expressions. Extensive quantitative and qualitative evaluations, along with user studies, demonstrate that the system can accurately manipulate facial details and achieve excellent lip synchronization. Our system achieves state-of-the-art performance in terms of identity preservation and video quality. Code is available at https://github.com/shanzhajuan/ManiTalk.
C1 [Fang, Hui; Tian, Zeyu] Beijing Inst Technol, Sch Opt & Photon, Beijing Engn Res Ctr Mixed Real & Adv Display, Beijing, Peoples R China.
   [Weng, Dongdong] Beijing Inst Technol, Zhengzhou Res Inst, Beijing Engn Res Ctr Mixed Real & Adv Display, Sch Opt & Photon, Zhengzhou, Peoples R China.
   [Ma, Yin] Ningxia Baofeng Grp Co Ltd, Yinchuan, Peoples R China.
C3 Beijing Institute of Technology; Beijing Institute of Technology
RP Weng, DD (corresponding author), Beijing Inst Technol, Zhengzhou Res Inst, Beijing Engn Res Ctr Mixed Real & Adv Display, Sch Opt & Photon, Zhengzhou, Peoples R China.
EM FangHui72@163.com; crgj@bit.edu.cn; tianty97@163.com; mayin98@163.com
FU National Key Research and Development Program of China [2022YFF0902303];
   National Key R &D Program of China [Z221100007722002]; Beijing Municipal
   Science & Technology Commission and Administrative Commission of
   Zhongguancun Science Park [62072036]; National Natural Science
   Foundation of China
FX This work was supported by the National Key R &D Program of China (No.
   2022YFF0902303) and the Beijing Municipal Science & Technology
   Commission and Administrative Commission of Zhongguancun Science Park
   (No. Z221100007722002) and the National Natural Science Foundation of
   China (No. 62072036).
CR Baevski A, 2020, Advances in neural information processing systems, V33, P12449, DOI 10.5555/3495724.3496768
   Baltrusaitis Tadas, 2018, 2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018), P59, DOI 10.1109/FG.2018.00019
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Cao HW, 2014, IEEE T AFFECT COMPUT, V5, P377, DOI 10.1109/TAFFC.2014.2336244
   Chatziagapi Aggelina, 2023, 2023 IEEE 17th International Conference on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG57933.2023.10042567
   Cheng K, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555399
   Chenxu Z., 2023, ARXIV
   Cudeiro D, 2019, PROC CVPR IEEE, P10093, DOI 10.1109/CVPR.2019.01034
   Deng H, 2021, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR46437.2021.00648
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Doukas MC, 2023, IEEE T PATTERN ANAL, V45, P9743, DOI 10.1109/TPAMI.2023.3253243
   Eskimez SE, 2021, IEEE T MULTIMEDIA, V24, P3480, DOI 10.1109/TMM.2021.3099900
   Fan YR, 2022, PROC CVPR IEEE, P18749, DOI 10.1109/CVPR52688.2022.01821
   Ganin Y, 2016, LECT NOTES COMPUT SC, V9906, P311, DOI 10.1007/978-3-319-46475-6_20
   He Z, 2019, IEEE I CONF COMP VIS, P6931, DOI 10.1109/ICCV.2019.00703
   Ji Xinya, 2022, ACM SIGGRAPH 2022 C, P1
   Karras T, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073658
   Lahiri A, 2021, PROC CVPR IEEE, P2754, DOI 10.1109/CVPR46437.2021.00278
   Lu YX, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480484
   Lugaresi C., 2019, ARXIV
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Nagrani A., 2017, ARXIV
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   Prajwal KR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P484, DOI 10.1145/3394171.3413532
   Ruzzi A, 2023, PROC CVPR IEEE, P9676, DOI 10.1109/CVPR52729.2023.00933
   Siarohin A, 2021, PROC CVPR IEEE, P13648, DOI 10.1109/CVPR46437.2021.01344
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song LS, 2022, IEEE T INF FOREN SEC, V17, P585, DOI 10.1109/TIFS.2022.3146783
   Suwajanakorn S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073640
   Thies Justus, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P716, DOI 10.1007/978-3-030-58517-4_42
   Van den Oord A., 2016, CoRR, P125, DOI DOI 10.1109/ICASSP.2009.4960364
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang S, 2021, ARXIV
   Wang SZ, 2022, AAAI CONF ARTIF INTE, P2531
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang XT, 2021, PROC CVPR IEEE, P9164, DOI 10.1109/CVPR46437.2021.00905
   Wen X, 2020, IEEE T VIS COMPUT GR, V26, P3457, DOI 10.1109/TVCG.2020.3023573
   Wolf L, 2010, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2010.5540133
   Yi R., 2020, arXiv
   Yu Y, 2020, PROC CVPR IEEE, P7312, DOI 10.1109/CVPR42600.2020.00734
   Zhang CX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3847, DOI 10.1109/ICCV48922.2021.00384
   Zhang CX, 2023, IEEE T VIS COMPUT GR, V29, P1438, DOI 10.1109/TVCG.2021.3117484
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang WX, 2023, PROC CVPR IEEE, P8652, DOI 10.1109/CVPR52729.2023.00836
   Zhang YH, 2022, INT CONF ACOUST SPEE, P4848, DOI 10.1109/ICASSP43922.2022.9747284
   Zhang ZM, 2021, PROC CVPR IEEE, P3660, DOI 10.1109/CVPR46437.2021.00366
   Zhao J, 2022, PROC CVPR IEEE, P3647, DOI 10.1109/CVPR52688.2022.00364
   Zhou Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417774
NR 48
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4913
EP 4925
DI 10.1007/s00371-024-03490-4
EA JUN 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001242314900004
DA 2024-08-05
ER

PT J
AU Yokota, S
   Fujishiro, I
AF Yokota, Soma
   Fujishiro, Issei
TI Visual simulation of opal using bond percolation throughthe weighted
   Voronoi diagram and the Ewald construction
SO VISUAL COMPUTER
LA English
DT Article
DE Gemstone; Structural color; Colloidal crystal; Voronoi diagram;
   Computational crystal modeling
ID SCATTERING; MODEL
AB Opal, a renowned gemstone, exhibits an optical phenomenon known as "play of color," which is classified as a kind of structural color. This unique property makes opal a highly valuable material from the natural science, industry, and humanity perspectives. Opal has a complex internal structure called colloidal polycrystalline, and thus, representations of opal remain unestablished in computer graphics. In this study, to approximate the complex internal structure of opal, we imitate its formation process using the three-dimensional additively weighted Voronoi tessellation and percolation model. Further, we apply path tracing to compute the diffraction patterns of visible light inside opal by utilizing the Ewald construction employed in X-ray diffraction theory. Finally, we achieve a visually plausible output through spectral rendering.
C1 [Yokota, Soma; Fujishiro, Issei] Keio Univ, Yokohama, Japan.
C3 Keio University
RP Yokota, S; Fujishiro, I (corresponding author), Keio Univ, Yokohama, Japan.
EM sy415@keio.jp; ifujishiro@keio.jp
OI Fujishiro, Issei/0000-0002-8898-730X
FU Japan Science and Technology Agency [21H04916]; JSPS KAKENHI
   [JPMJSP2123]; JST SPRING
FX This work has been supported in part by JSPS KAKENHI under the
   Grant-in-Aid for Scientific Research (A) No. 21H04916 and JST SPRING No.
   JPMJSP2123.
CR [Anonymous], 2001, Realistic Image Synthesis Using Photon Mapping
   [Anonymous], 1992, Graphics Gems III (IBM Version), DOI DOI 10.1016/B978-0-08-050755-2.50036-1
   AURENHAMMER F, 1991, COMPUT SURV, V23, P345, DOI 10.1145/116873.116880
   Bao GY, 2023, J MATER CHEM C, V11, P3513, DOI 10.1039/d2tc05499j
   Belcour L, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073620
   Benamira A, 2021, COMPUT GRAPH FORUM, V40, P163, DOI 10.1111/cgf.14349
   Broadbent S., 1957, Math. Proc. Camb. Philos. Soc., V53, P629
   Cuypers T, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2231816.2231820
   Dhillon DS, 2014, COMPUT GRAPH FORUM, V33, P177, DOI 10.1111/cgf.12425
   Dijkstra M, 2021, NAT MATER, V20, P762, DOI 10.1038/s41563-021-01014-2
   Duminil-Copin H., 2018, P INT C MATH, P2829
   Ewald PP, 1921, ANN PHYS-BERLIN, V64, P253
   Fascione Luca., 2017, ACM SIGGRAPH 2017 Courses
   Filin S., 2002, AUST GEMMOL, V21, P278
   Frisvad JR, 2011, J OPT SOC AM A, V28, P2436, DOI 10.1364/JOSAA.28.002436
   Gaillou E, 2008, ORE GEOL REV, V34, P113, DOI 10.1016/j.oregeorev.2007.07.004
   Gao WH, 2016, J NANOPART RES, V18, DOI 10.1007/s11051-016-3691-8
   Gondek J. S., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P213, DOI 10.1145/192161.192202
   Guillén I, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417782
   Guo Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480543
   Guy S, 2004, ACM T GRAPHIC, V23, P231, DOI 10.1145/1015706.1015708
   HE XD, 1991, COMP GRAPH, V25, P175, DOI 10.1145/127719.122738
   Holzschuch N, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073621
   Huang WJ, 2022, AM J CHINESE MED, V50, P1963, DOI 10.1142/S0192415X22500847
   Hui KC, 2007, COMPUT GRAPH FORUM, V26, P24, DOI 10.1111/j.1467-8659.2007.00936.x
   Icart I, 1999, SPRING EUROGRAP, P261
   Imura M., 2003, DIGITAL IMAGE COMPUT, P349
   Jakob W, 2019, COMPUT GRAPH FORUM, V38, P147, DOI 10.1111/cgf.13626
   Jensen HenrikWann., 1996, Rendering Techniques '96, P21
   JONES JB, 1964, NATURE, V204, P994
   Kim JH, 2017, PROC SPIE, V10345, DOI 10.1117/12.2269172
   Kittel C., 2004, Introduction to solid state physics, Veighth
   Kneiphof T, 2019, COMPUT GRAPH FORUM, V38, P77, DOI 10.1111/cgf.13772
   McOrist GD, 1997, J RADIOANAL NUCL CH, V223, P9, DOI 10.1007/BF02223356
   Meng J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766949
   Meng J, 2015, COMPUT GRAPH FORUM, V34, P31, DOI 10.1111/cgf.12676
   Müller T, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982429
   Müller T, 2017, COMPUT GRAPH FORUM, V36, P91, DOI 10.1111/cgf.13227
   Nagata N, 1997, IEEE T VIS COMPUT GR, V3, P307, DOI 10.1109/2945.646234
   Otsu H, 2018, COMPUT GRAPH FORUM, V37, P370, DOI 10.1111/cgf.13332
   Perlin K, 2002, ACM T GRAPHIC, V21, P681, DOI 10.1145/566570.566636
   Pimpinelli A, 2014, J PHYS CHEM LETT, V5, P995, DOI 10.1021/jz500282t
   Poly Haven, 2020, US
   Rayleigh L., 1899, Lond. Edinb. Dublin Philos. Mag. J. Sci., V47, P375, DOI [10.1080/14786449908621276, DOI 10.1080/14786449908621276]
   Rohrer GS, 2011, J MATER SCI, V46, P5881, DOI 10.1007/s10853-011-5677-3
   Simoni M, 2010, GEMS GEMOL, V46, P114, DOI 10.5741/GEMS.46.2.114
   Smits B., 1999, Journal of Graphics Tools, V4, P11, DOI 10.1080/10867651.1999.10487511
   Smits BE., 1992, NEWTONS COLORS SIMUL, P185, DOI DOI 10.1007/978
   Soulié R, 2007, COMPUT GRAPH FORUM, V26, P66, DOI 10.1111/j.1467-8659.2007.00949.x
   Stam J, 1999, COMP GRAPH, P101, DOI 10.1145/311535.311546
   Sun YL, 2006, ACM T GRAPHIC, V25, P100, DOI 10.1145/1122501.1122506
   Thomas S.W., 1986, VISUAL COMPUT, V2, P3
   Toisoul A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3012001
   Weidlich A., 2008, P 24 SPRING C COMP G, P51
   Weidlich A., 2009, P GRAPHICS INTERFACE, P79
   Weidlich A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330517
   Wilkie A, 2014, COMPUT GRAPH FORUM, V33, P123, DOI 10.1111/cgf.12419
   WOLFF LB, 1990, IEEE COMPUT GRAPH, V10, P44, DOI 10.1109/38.62695
   Wu FK, 2013, GRAPH MODELS, V75, P318, DOI 10.1016/j.gmod.2013.07.004
   Xia MQ, 2023, COMPUT GRAPH FORUM, V42, DOI 10.1111/cgf.14893
   Yokoi S., 1986, Visual Computer, V2, P307, DOI 10.1007/BF02020431
NR 61
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 5005
EP 5016
DI 10.1007/s00371-024-03504-1
EA JUN 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001242155100005
DA 2024-08-05
ER

PT J
AU Sun, WY
   Zhang, JD
   Liu, YT
AF Sun, Wenyue
   Zhang, Jindong
   Liu, Yitong
TI Adversarial-based refinement dual-branch network for semi-supervised
   salient object detection of strip steel surface defects
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Salient object detection; Adversarial learning; Semi-supervised
   learning; Surface defects
AB The detection of surface defects on strip steel poses significant challenges due to factors such as background noise and interference. Moreover, the lack of labeled defect data in practical scenarios makes it difficult to effectively differentiate defects using computer vision and machine-learning methods. To tackle these issues, we propose a novel semi-supervised saliency detection method called adversarial-based refinement dual-branch network (ARDNet). We introduce an adversarial learning mechanism to ARDNet, which leverages unlabeled data through a generator network and a discriminator network. The generator network's encoder employs multiple convolutional branches to extract multi-level features, while the multi-scale integration refinement module (MIRM) integrates semantic features from these branches and refines edge details. Subsequently, the decoder fuses deep features into the saliency map and utilizes a dual-branch structure to minimize interference between labeled and unlabeled inputs. Lastly, the discriminator network provides additional supervision to the generator network by distinguishing between the predicted probability maps generated by the generator and the ground truth segmentation distributions. Experiments on the publicly available dataset show that our approach surpasses other competitive methods in terms of filtering background noise, adapting to multi-scale defect sizes, and preserving defect details. By effectively utilizing unlabeled images, our method enhances segmentation accuracy, accurately locates defect positions, and successfully segments the defects.
C1 [Sun, Wenyue; Zhang, Jindong; Liu, Yitong] Jilin Univ, Coll Comp Sci & Technol, Changchun 130012, Peoples R China.
   [Zhang, Jindong] Jilin Univ, Key Lab Symbol Computat & Knowledge Engn, Minist Educ, Changchun 130012, Peoples R China.
C3 Jilin University; Jilin University
RP Zhang, JD (corresponding author), Jilin Univ, Coll Comp Sci & Technol, Changchun 130012, Peoples R China.; Zhang, JD (corresponding author), Jilin Univ, Key Lab Symbol Computat & Knowledge Engn, Minist Educ, Changchun 130012, Peoples R China.
EM zhangjindong_100@163.com
FU the Korea Foundation for Advanced Studies' International Scholar
   Exchange Fellowship; Korea Foundation for Advanced Studies'
   International Scholar Exchange Fellowship [2021DQ0009]; Fundamental
   Research Funds for the Chongqing Research Institute Jilin University;
   Fundamental Research Funds for the Central Universities
FX This study was supported by the Korea Foundation for Advanced Studies'
   International Scholar Exchange Fellowship for the academic year of
   2017-2018, the Fundamental Research Funds for the Chongqing Research
   Institute Jilin University (2021DQ0009), and the Fundamental Research
   Funds for the Central Universities.
CR Bao YQ, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3083561
   Chen SH, 2020, IEEE T IMAGE PROCESS, V29, P3763, DOI 10.1109/TIP.2020.2965989
   Cui WQ, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3290965
   Ding T, 2022, MEASUREMENT, V199, DOI 10.1016/j.measurement.2022.111429
   Dong GZ, 2022, MATH BIOSCI ENG, V19, P8786, DOI 10.3934/mbe.2022408
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Feng Q, 2024, VISUAL COMPUT, V40, P3633, DOI 10.1007/s00371-023-03056-w
   Fidon L, 2018, LECT NOTES COMPUT SC, V10670, P64, DOI 10.1007/978-3-319-75238-9_6
   Gao SY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3656, DOI 10.1145/3503161.3547912
   Gao Y, 2023, VISUAL COMPUT, V39, P3979, DOI 10.1007/s00371-022-02543-w
   Han CL, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3191653
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jiang BW, 2013, IEEE I CONF COMP VIS, P1665, DOI 10.1109/ICCV.2013.209
   Jiang X, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3149097
   Kingma D. P., 2015, P INT C LEARN REPR, P1
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Li MJ, 2022, COMPUT ELECTR ENG, V102, DOI 10.1016/j.compeleceng.2022.108208
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu M., 2022, arXiv
   Liu ZY, 2023, VISUAL COMPUT, V39, P2881, DOI 10.1007/s00371-022-02499-x
   Luo QW, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3165270
   Lv YQ, 2022, PATTERN RECOGN, V123, DOI 10.1016/j.patcog.2021.108364
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Nguyen D., 2019, NIPS 19, P204
   Pang Y, 2023, VISUAL COMPUT, V39, P1959, DOI 10.1007/s00371-022-02458-6
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Song GR, 2020, IEEE T INSTRUM MEAS, V69, P9709, DOI 10.1109/TIM.2020.3002277
   Song GR, 2020, OPT LASER ENG, V128, DOI 10.1016/j.optlaseng.2019.106000
   Sun XL, 2017, PATTERN RECOGN, V66, P253, DOI 10.1016/j.patcog.2017.01.012
   Wan B, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3250302
   Wang CJ, 2020, IEEE T IND INFORM, V16, P2667, DOI 10.1109/TII.2019.2945362
   Wang FL, 2016, J CENT SOUTH UNIV, V23, P1123, DOI 10.1007/s11771-016-0362-y
   Wang YF, 2022, PROC CVPR IEEE, P11717, DOI 10.1109/CVPR52688.2022.01143
   Wenfeng Luo, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P784, DOI 10.1007/978-3-030-58558-7_46
   Wu FP, 2022, CHIN J MECH ENG-EN, V35, DOI 10.1186/s10033-022-00726-z
   Wu YH, 2022, IEEE T IMAGE PROCESS, V31, P3125, DOI 10.1109/TIP.2022.3164550
   Xia CX, 2022, VISUAL COMPUT, V38, P3059, DOI 10.1007/s00371-022-02561-8
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Xu XS, 2023, VISUAL COMPUT, V39, P6097, DOI 10.1007/s00371-022-02715-8
   Zhang D., 2020, Adv. Neural Info. Process. Syst., V33, P12236
   Zhang JD, 2024, VISUAL COMPUT, V40, P427, DOI 10.1007/s00371-023-02791-4
   Zhang JD, 2023, SIGNAL IMAGE VIDEO P, V17, P1153, DOI 10.1007/s11760-022-02322-z
   Zhang JD, 2020, MULTIMED TOOLS APPL, V79, P23367, DOI 10.1007/s11042-020-09152-6
   Zhang XY, 2024, VISUAL COMPUT, V40, P2713, DOI 10.1007/s00371-023-02980-1
   Zhao SM, 2020, COMPUT ANIMAT VIRT W, V31, DOI 10.1002/cav.1954
   Zhong X, 2021, J VIS COMMUN IMAGE R, V78, DOI 10.1016/j.jvcir.2021.103138
   Zhou HJ, 2023, PROC CVPR IEEE, P7257, DOI 10.1109/CVPR52729.2023.00701
   Zhou SY, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8091628
   Zhou XF, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2021.3132082
   Zhou XF, 2021, IEEE ACCESS, V9, P149465, DOI 10.1109/ACCESS.2021.3124814
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 56
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 21
PY 2024
DI 10.1007/s00371-024-03442-y
EA MAY 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RO3K3
UT WOS:001228562800004
DA 2024-08-05
ER

PT J
AU Hu, YQ
   Zhang, JQ
   Bai, L
   Li, J
   Li, B
   Zang, Y
   Hu, WJ
AF Hu, Yuanqi
   Zhang, Jianqi
   Bai, Ling
   Li, Jing
   Li, Bing
   Zang, Ying
   Hu, Wenjun
TI From sketch to reality: precision-friendly 3D generation technology
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE 3D Reconstruction; Sketch; Graphics
AB With the rapid development of computer graphics and 3D computer vision, the field of reconstructing 3D geometric shapes from sketches has quickly seen an influx of innovative methodologies. However, the majority of current methods primarily rely on the associative memory of matching images to models during their training stages, failing to fully grasp the real structure of the three-dimensional objects presented in the images. These approaches often lead to significant discrepancies between the reconstructed objects and the expected models. For instance, some methods often lose important geometric details when dealing with objects that have complex topological structures, resulting in a significant reduction in the visual consistency between the reconstructed models and the original images. Moreover, existing technologies also demonstrate clear limitations in theoretically capturing the subtle symmetry and local features of three-dimensional objects. To address this issue, we propose a two-phase framework in this work to more accurately reflect the objects in the input images in the reconstructed models. Initially, we employ an encoder-decoder structure to generate an implicit signed distance field (SDF) representing the 3D shapes. Subsequently, we carry out a comprehensive optimization of the decoder from the first phase. This process includes two main steps: firstly, utilizing differentiable rendering techniques to render the mesh model derived from the distance field, ensuring its consistency with the input images; secondly, combining the symmetry of the 3D shapes with innovative regularization loss to further refine the decoder, aiming to reduce the discrepancies between the images and the 3D shapes. Compared to similar research, our method not only demonstrates superior performance in reconstructing 3D shapes from sketches but also offers new perspectives and solutions for the optimization of 3D shapes. This work signifies an important advancement in understanding and reconstructing the transition process from sketches to precise 3D models. Code: https://github.com/Hyuq1/Sketch2Reality.git
C1 [Hu, Yuanqi; Zhang, Jianqi; Bai, Ling; Li, Jing; Li, Bing; Zang, Ying; Hu, Wenjun] Huzhou Univ, Sch Informat Engn, Huzhou 313000, Peoples R China.
C3 Huzhou University
RP Zang, Y; Hu, WJ (corresponding author), Huzhou Univ, Sch Informat Engn, Huzhou 313000, Peoples R China.
EM h18156381513@163.com; zhangjianq5@outlook.com; 19816903126@163.com;
   15953293525@163.com; bingli@zjhu.edu.cn; 02750@zjhu.edu.cn;
   hoowenjun@foxmail.com
FU This paper is supported by the Public Welfare Research Program of Huzhou
   Science and Technology Bureau
FX No Statement Available
CR Al-Jebrni AH, 2023, VISUAL COMPUT, V39, P3675, DOI 10.1007/s00371-023-02984-x
   Bonnici A, 2019, AI EDAM, V33, P370, DOI 10.1017/S0890060419000349
   Camba JD, 2022, COMPUT AIDED DESIGN, V150, DOI 10.1016/j.cad.2022.103283
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Chen Tianxiang, 2023, ICASSP 2023 2023 IEE, P1
   Chen ZH, 2023, IEEE T PATTERN ANAL, V45, P13489, DOI 10.1109/TPAMI.2023.3293885
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   El Sayed NAM, 2011, COMPUT EDUC, V56, P1045, DOI 10.1016/j.compedu.2010.10.019
   Gadelha M, 2019, IEEE I CONF COMP VIS, P22, DOI 10.1109/ICCV.2019.00011
   Giunchi D., 2018, P JOINT S COMP AESTH, P1
   Guo HB, 2021, IEEE T CYBERNETICS, V51, P2735, DOI 10.1109/TCYB.2019.2934823
   Hu X., 2018, BMVC, P230
   Huang HB, 2017, IEEE T VIS COMPUT GR, V23, P2003, DOI 10.1109/TVCG.2016.2597830
   Huang Q, 2015, PROC NAECON IEEE NAT, P1, DOI 10.1109/NAECON.2015.7443030
   Kar A, 2017, ADV NEUR IN, V30
   Kato H, 2018, PROC CVPR IEEE, P3907, DOI 10.1109/CVPR.2018.00411
   Lee J, 2021, PROC CVPR IEEE, P11407, DOI 10.1109/CVPR46437.2021.01125
   Leung B, 2022, IEEE COMPUT SOC CONF, P4089, DOI 10.1109/CVPRW56347.2022.00453
   Li CJ, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417807
   Lin Chen-Hsuan, 2020, P ADV NEURAL INFORM, V33, P11453
   Liu S., 2019, Adv Neural Inf Process Syst, V32
   Liu SC, 2019, IEEE I CONF COMP VIS, P7707, DOI 10.1109/ICCV.2019.00780
   Nazir A, 2021, IEEE T BIO-MED ENG, V68, P2540, DOI 10.1109/TBME.2021.3050310
   Nie WZ, 2021, IEEE T MULTIMEDIA, V23, P1962, DOI 10.1109/TMM.2020.3006371
   Niemeyer M, 2020, PROC CVPR IEEE, P3501, DOI 10.1109/CVPR42600.2020.00356
   Nishida G, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925951
   Olsen L, 2009, COMPUT GRAPH-UK, V33, P85, DOI 10.1016/j.cag.2008.09.013
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Plumed R, 2022, COMPUT GRAPH-UK, V102, P349, DOI 10.1016/j.cag.2021.10.013
   Sangkloy P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925954
   Sinha S, 2023, PROC CVPR IEEE, P21349, DOI 10.1109/CVPR52729.2023.02045
   Sra M, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P191, DOI 10.1145/2993369.2993372
   Tanaka M., 2020, Comput. Aided Design Appl, V17, P1168, DOI [10.14733/cadaps.2020.1168-1176, DOI 10.14733/CADAPS.2020.1168-1176]
   Wang F, 2015, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2015.7298797
   Wang M, 2020, COMPUT VIS MEDIA, V6, P3, DOI 10.1007/s41095-020-0162-z
   Chang AX, 2015, Arxiv, DOI [arXiv:1512.03012, DOI 10.48550/ARXIV.1512.03012]
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Yu A, 2021, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR46437.2021.00455
   Zang Ying, 2023, 2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC), P1537, DOI 10.1109/SMC53992.2023.10393936
   Zhang SH, 2021, PROC CVPR IEEE, P6008, DOI 10.1109/CVPR46437.2021.00595
   Zhou Y, 2023, IEEE T NEUR NET LEAR, V34, P7719, DOI 10.1109/TNNLS.2022.3146004
NR 42
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 16
PY 2024
DI 10.1007/s00371-024-03425-z
EA MAY 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RH2Q0
UT WOS:001226712900001
DA 2024-08-05
ER

PT J
AU Zhu, QG
   Cen, Q
   Wang, YX
   Chen, WD
   Liu, S
AF Zhu, QiGuang
   Cen, Qiang
   Wang, YuXin
   Chen, WeiDong
   Liu, Shuo
TI An underwater target recognition algorithm incorporating improved
   attention mechanism and downsampling
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Underwater target detection; Channel attention; Feature fusion; Focal
   loss function
AB To address the issue of low accuracy in recognizing underwater targets due to dense and blurred targets in underwater target detection, we propose a joint improved attention mechanism and downsampling network for underwater target detection. Firstly, to address the issue of dense targets, we introduce an improved channel attention module. This module enhances attention to spatial dimension information, highlights the saliency of feature maps of different channels and improves the detection ability of dense targets. Secondly, to address the issue of blurred underwater targets, we introduce a down-sampling module that combines same-layer connections and cross-layer skipping. This module reduces information loss caused by convolutional down-sampling and integrates features from different layers more fully. By improving the feature expression of the underwater image, the network's detection accuracy for underwater blurred targets is further enhanced. Finally, the study introduces the focus loss function to address the imbalance of positive and negative samples. This function dynamically reduces the weight of easy-to-distinguish samples during training and prioritizes difficult-to-distinguish samples. Experimental results demonstrate a 2.71% increase in average accuracy of the improved algorithm on the DUO dataset. Additionally, the calculation amount is reduced by 9.1 GFLOPs, and the parameter amount is reduced by 5.44 M. Code:https://figshare.com/articles/dataset/improved-yolov5/25375129. Dataset:https://figshare.com/articles/dataset/DUO_zip/25370527.
C1 [Zhu, QiGuang; Cen, Qiang; Wang, YuXin; Chen, WeiDong; Liu, Shuo] Yanshan Univ, Sch Informat Sci & Engn, Hebei St, Qinhuangdao 066004, Hebei, Peoples R China.
   [Zhu, QiGuang; Chen, WeiDong; Liu, Shuo] Key Lab Special Fiber & Fiber Sensor Hebei Prov, Hebei St, Qinhuangdao 066004, Hebei, Peoples R China.
C3 Yanshan University
RP Cen, Q (corresponding author), Yanshan Univ, Sch Informat Sci & Engn, Hebei St, Qinhuangdao 066004, Hebei, Peoples R China.
EM zhu7880@ysu.edu.cn; cenqiang@stumail.ysu.edu.cn;
   wangyuxin@stumail.ysu.edu.cn; wdchen@ysu.edu.cn; liushuo@ysu.edu.cn
FU National Natural Science Foundation of China;  [61773333];  [62273296]
FX This work is supported by the National Natural Science Foundation of
   China under Grants 61773333 and 62273296.
CR Chen ZH, 2023, IEEE T PATTERN ANAL, V45, P13489, DOI 10.1109/TPAMI.2023.3293885
   Cheng BW, 2018, LECT NOTES COMPUT SC, V11219, P473, DOI 10.1007/978-3-030-01267-0_28
   Cui YM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8118, DOI 10.1109/ICCV48922.2021.00803
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fu CP, 2023, NEUROCOMPUTING, V517, P243, DOI 10.1016/j.neucom.2022.10.039
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hong JS, 2020, Arxiv, DOI [arXiv:2007.08097, DOI 10.48550/ARXIV.2007.08097]
   Hou WQ, 2024, VISUAL COMPUT, V40, P459, DOI 10.1007/s00371-023-02793-2
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Khan A, 2024, IEEE ACCESS, V12, P12618, DOI 10.1109/ACCESS.2024.3353688
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Li XB, 2024, VISUAL COMPUT, V40, P1299, DOI 10.1007/s00371-023-02849-3
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu CW, 2021, IEEE INT CONF MULTI, DOI 10.1109/ICMEW53276.2021.9455997
   Liu CW, 2022, IEEE T CIRC SYST VID, V32, P2831, DOI 10.1109/TCSVT.2021.3100059
   Liu DF, 2021, PROC CVPR IEEE, P9811, DOI 10.1109/CVPR46437.2021.00969
   Liu DF, 2021, AAAI CONF ARTIF INTE, V35, P6101
   Liu H, 2015, CHIN J OCEANOL LIMN, V33, P114, DOI 10.1007/s00343-015-4080-3
   Qiao X, 2017, COMPUT ELECTRON AGR, V135, P134, DOI 10.1016/j.compag.2017.02.008
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sun X, 2018, NEUROCOMPUTING, V275, P897, DOI 10.1016/j.neucom.2017.09.044
   Sun Y, 2023, J MAR SCI ENG, V11, DOI 10.3390/jmse11061178
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang N, 2023, NEUROCOMPUTING, V532, P1, DOI 10.1016/j.neucom.2023.02.018
   Wang W., 2023, P INT C LEARN REPR, P1
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Yang YY, 2023, J OCEAN U CHINA, V22, P665, DOI 10.1007/s11802-023-5296-z
   Zeng LC, 2021, ENG APPL ARTIF INTEL, V100, DOI 10.1016/j.engappai.2021.104190
   Zhu Q., 2024, Duo.zip. figshare.dataset, DOI [10.6084/m9.figshare.25370527.v1, DOI 10.6084/M9.FIGSHARE.25370527.V1]
   Zhu Q., 2024, improved-yolov5. figshare. dataset, DOI [10.6084/m9.figshare.25375129.v1, DOI 10.6084/M9.FIGSHARE.25375129.V1]
NR 32
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 13
PY 2024
DI 10.1007/s00371-024-03437-9
EA MAY 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QL1A9
UT WOS:001220925900001
DA 2024-08-05
ER

PT J
AU Alcover-Couso, R
   Sanmiguel, JC
   Escudero-Viñolo, M
   Carballeira, P
AF Alcover-Couso, Roberto
   Sanmiguel, Juan C.
   Escudero-Vinolo, Marcos
   Carballeira, Pablo
TI Per-class curriculum for Unsupervised Domain Adaptation in semantic
   segmentation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Semantic Segmentation; Unsupervised Domain Adaptation; Curriculum
   learning; Synthetic data
AB Accurate training of deep neural networks for semantic segmentation requires a large number of pixel-level annotations of real images, which are expensive to generate or not even available. In this context, Unsupervised Domain Adaptation (UDA) can transfer knowledge from unlimited synthetic annotations to unlabeled real images of a given domain. UDA methods are composed of an initial training stage with labeled synthetic data followed by a second stage for feature alignment between labeled synthetic and unlabeled real data. In this paper, we propose a novel approach for UDA focusing the initial training stage, which leads to increased performance after adaptation. We introduce a curriculum strategy where each semantic class is learned progressively. Thereby, better features are obtained for the second stage. This curriculum is based on: (1) a class-scoring function to determine the difficulty of each semantic class, (2) a strategy for incremental learning based on scoring and pacing functions that limits the required training time unlike standard curriculum-based training and (3) a training loss to operate at class level. We extensively evaluate our approach as the first stage of several state-of-the-art UDA methods for semantic segmentation. Our results demonstrate significant performance enhancements across all methods: improvements of up to 10% for entropy-based techniques and 8% for adversarial methods. These findings underscore the dependency of UDA on the accuracy of the initial training. The implementation is available at https://github.com/vpulab/PCCL.
C1 [Alcover-Couso, Roberto; Sanmiguel, Juan C.; Escudero-Vinolo, Marcos; Carballeira, Pablo] Univ Autonoma Madrid UAM, Video Proc & Understanding Lab, Madrid 28049, Spain.
RP Alcover-Couso, R (corresponding author), Univ Autonoma Madrid UAM, Video Proc & Understanding Lab, Madrid 28049, Spain.
EM roberto.alcover@uam.es; juancarlos.sanmiguel@uam.es;
   marcos.escudero@uam.es; pablo.carballeira@uam.es
RI ; Escudero-Vinolo, Marcos/C-3601-2014
OI Alcover-Couso, Roberto/0000-0001-9609-4416; Escudero-Vinolo,
   Marcos/0000-0002-9156-3428
FU Universidad Autnoma de Madrid [TED2021-131643A-I00,
   PID2021-125051OB-I00]; Spanish Government
FX This work has been partially supported by the Spanish Government through
   its TED2021-131643A-I00 (HVD) and the PID2021-125051OB-I00 (SEGA-CV)
   projects.
CR Barbato F, 2024, VISUAL COMPUT, V40, P811, DOI 10.1007/s00371-023-02818-w
   Bayoudh K, 2022, VISUAL COMPUT, V38, P2939, DOI 10.1007/s00371-021-02166-7
   Bengio J., 2009, Proceedings of the 26th Annual International Conference on Machine Learning, P41
   Biasetton M, 2019, IEEE COMPUT SOC CONF, P1211, DOI 10.1109/CVPRW.2019.00160
   Borse S, 2021, PROC CVPR IEEE, P5897, DOI 10.1109/CVPR46437.2021.00584
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen MH, 2019, IEEE I CONF COMP VIS, P2090, DOI 10.1109/ICCV.2019.00218
   Cheng H, 2019, PROC CVPR IEEE, P4743, DOI 10.1109/CVPR.2019.00488
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Escudero-Viñolo M, 2022, IEEE IMAGE PROC, P1476, DOI 10.1109/ICIP46576.2022.9897273
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Feng Q, 2024, VISUAL COMPUT, V40, P3633, DOI 10.1007/s00371-023-03056-w
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gomaa WH., 2013, Int. J. Comput. Appl., V68, P13, DOI DOI 10.5120/11638-7118
   Gong C, 2016, IEEE T IMAGE PROCESS, V25, P3249, DOI 10.1109/TIP.2016.2563981
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guo Y., 2023, Fundam. Res., DOI [10.1016/j.fmre.2023.06.006, DOI 10.1016/J.FMRE.2023.06.006]
   Hacohen G., 2019, INT C MACHINE LEARN, P2535, DOI DOI 10.48550/ARXIV.1904.03626
   Haoran Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P642, DOI 10.1007/978-3-030-58568-6_38
   Hinterstoisser S, 2019, IEEE INT CONF COMP V, P2787, DOI 10.1109/ICCVW.2019.00340
   Hoffman J., 2016, arXiv
   Hoyer L, 2022, LECT NOTES COMPUT SC, V13690, P372, DOI 10.1007/978-3-031-20056-4_22
   Huang YF, 2024, VISUAL COMPUT, V40, P3427, DOI 10.1007/s00371-023-03043-1
   Ionescu RT, 2016, PROC CVPR IEEE, P2157, DOI 10.1109/CVPR.2016.237
   Jiang L, 2015, AAAI CONF ARTIF INTE, P2694
   Lee S, 2022, PROC CVPR IEEE, P9926, DOI 10.1109/CVPR52688.2022.00970
   Liu HJ, 2020, VISUAL COMPUT, V36, P2105, DOI 10.1007/s00371-020-01913-6
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo X, 2023, NEURAL NETWORKS, V157, P216, DOI 10.1016/j.neunet.2022.10.015
   Ma F, 2017, PR MACH LEARN RES, V70
   Park S, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22072623
   Peng D, 2022, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR52688.2022.00262
   Pentina A, 2015, PROC CVPR IEEE, P5492, DOI 10.1109/CVPR.2015.7299188
   Qin W, 2020, IEEE ACCESS, V8, P25990, DOI 10.1109/ACCESS.2020.2970726
   Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7
   Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352
   Sankaranarayanan S, 2018, PROC CVPR IEEE, P3752, DOI 10.1109/CVPR.2018.00395
   Shen W, 2023, IEEE T PATTERN ANAL, V45, P9284, DOI [10.1109/TPAMI.2023.3246102, 10.1109/ICASSP49357.2023.10096003]
   Sinha S., 2020, Advances in Neural Information Processing Systems, P21653, DOI DOI 10.5555/3495724.3497541
   Soviany P, 2022, INT J COMPUT VISION, V130, P1526, DOI 10.1007/s11263-022-01611-x
   Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780
   Tsvetkov Y, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P130
   Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262
   Wang Q, 2019, IEEE T IMAGE PROCESS, V28, P4376, DOI 10.1109/TIP.2019.2910667
   Wang ZJ, 2023, COMPUT VIS IMAGE UND, V234, DOI 10.1016/j.cviu.2023.103743
   Zhang Y, 2020, IEEE T PATTERN ANAL, V42, P1823, DOI 10.1109/TPAMI.2019.2903401
   Zhao XY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10603, DOI 10.1109/ICCV48922.2021.01045
   Zhao YY, 2022, LECT NOTES COMPUT SC, V13688, P535, DOI 10.1007/978-3-031-19815-1_31
   Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI [10.1007/978-3-030-01219-9_, 10.1007/978-3-030-01219-9_18]
   Zou Y, 2019, IEEE I CONF COMP VIS, P5981, DOI 10.1109/ICCV.2019.00608
NR 51
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 15
PY 2024
DI 10.1007/s00371-024-03373-8
EA APR 2024
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OB0Q4
UT WOS:001204686400001
OA hybrid
DA 2024-08-05
ER

PT J
AU Iraji, MS
   Tanha, J
   Balafar, MA
   Feizi-Derakhshi, MR
AF Iraji, Mohammad Saber
   Tanha, Jafar
   Balafar, Mohammad-Ali
   Feizi-Derakhshi, Mohammad-Reza
TI Image classification with consistency-regularized bad semi-supervised
   generative adversarial networks: a visual data analysis and synthesis
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Visual synthesis; Informative fake images; Low-confidence images; Bad
   generative adversarial network; Semi-supervised classification
ID SEGMENTATION
AB Semi-supervised learning, which entails training a model with manually labeled images and pseudo-labels for unlabeled images, has garnered considerable attention for its potential to improve image classification performance. Nevertheless, incorrect decision boundaries of classifiers and wrong pseudo-labels for beneficial unlabeled images below the confidence threshold increase the generalization error in semi-supervised learning. This study proposes a novel framework for semi-supervised learning termed consistency-regularized bad generative adversarial network (CRBSGAN) through a new loss function. The proposed model comprises a discriminator, a bad generator, and a classifier that employs data augmentation and consistency regularization. Local augmentation is created to compensate for data scarcity and boost bad generators. Moreover, label consistency regularization is considered for bad fake images, real labeled images, unlabeled images, and latent space for the discriminator and bad generator. In the adversarial game between the discriminator and the bad generator, feature space is better captured under these conditions. Furthermore, local consistency regularization for good-augmented images applied to the classifier strengthens the bad generator in the generator-classifier adversarial game. The consistency-regularized bad generator produces informative fake images similar to the support vectors located near the correct classification boundary. In addition, the pseudo-label error is reduced for low-confidence unlabeled images used in training. The proposed method reduces the state-of-the-art error rate from 6.44 to 4.02 on CIFAR-10, 2.06 to 1.56 on MNIST, and 6.07 to 3.26 on SVHN using 4000, 3000, and 500 labeled training images, respectively. Furthermore, it achieves a reduction in the error rate on the CINIC-10 dataset from 19.38 to 15.32 and on the STL-10 dataset from 27 to 16.34 when utilizing 1000 and 500 labeled images per class, respectively. Experimental results and visual synthesis indicate that the CRBSGAN algorithm is more efficient than the methods proposed in previous works. The source code is available at https://github.com/ms-iraji/CRBSGAN_NE arrow.
C1 [Iraji, Mohammad Saber; Tanha, Jafar; Balafar, Mohammad-Ali; Feizi-Derakhshi, Mohammad-Reza] Univ Tabriz, Fac Elect & Comp Engn, Dept Comp Engn, Tabriz, Iran.
C3 University of Tabriz
RP Tanha, J (corresponding author), Univ Tabriz, Fac Elect & Comp Engn, Dept Comp Engn, Tabriz, Iran.
EM iraji.ms@gmail.com; tanha@tabrizu.ac.ir; Balafarila@tabrizu.ac.ir;
   mfeizi@tabrizu.ac.ir
RI Feizi Derakhshi, Mohammad Reza/AAK-1687-2020
OI Feizi Derakhshi, Mohammad Reza/0000-0002-8548-976X
CR Arantes RB, 2022, IMAGE VISION COMPUT, V117, DOI 10.1016/j.imavis.2021.104338
   Bao JM, 2017, IEEE I CONF COMP VIS, P2764, DOI 10.1109/ICCV.2017.299
   Berthelot D., 2020, INT C LEARN REPR, P1
   Chang JH, 2022, KNOWL-BASED SYST, V249, DOI 10.1016/j.knosys.2022.108837
   Chen JM, 2021, NEUROCOMPUTING, V453, P731, DOI 10.1016/j.neucom.2020.06.133
   Chen ZX, 2020, Arxiv, DOI arXiv:2007.03844
   Coates A., 2011, INT C ARTIFICIAL INT
   Dai ZH, 2017, 31 ANN C NEURAL INFO, V30
   Darlow L.N., 2018, CINIC-10 is not ImageNet or CIFAR-10
   Deng WX, 2021, PATTERN RECOGN LETT, V152, P398, DOI 10.1016/j.patrec.2021.10.009
   Deng ZJ, 2017, ADV NEUR IN, V30
   Ding WP, 2021, INFORM SCIENCES, V578, P559, DOI 10.1016/j.ins.2021.07.059
   Dong JH, 2019, ADV NEUR IN, V32
   Emadi M, 2021, INFORM PROCESS MANAG, V58, DOI 10.1016/j.ipm.2020.102444
   Ertugrul E, 2020, COMPUT ANIMAT VIRT W, V31, DOI 10.1002/cav.1959
   Feng W, 2019, KNOWL-BASED SYST, V182, DOI 10.1016/j.knosys.2019.07.016
   Gan Y, 2023, NEURAL COMPUT APPL, V35, P6197, DOI 10.1007/s00521-022-08002-w
   Gan YL, 2022, KNOWL-BASED SYST, V245, DOI 10.1016/j.knosys.2022.108602
   Gangwar A, 2023, NEUROCOMPUTING, V528, P200, DOI 10.1016/j.neucom.2023.01.027
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gu XW, 2020, INFORM SCIENCES, V535, P204, DOI 10.1016/j.ins.2020.05.018
   Gu XW, 2018, APPL SOFT COMPUT, V68, P53, DOI 10.1016/j.asoc.2018.03.032
   He R, 2020, EXPERT SYST APPL, V150, DOI 10.1016/j.eswa.2020.113244
   Huo XY, 2022, PATTERN RECOGN, V129, DOI 10.1016/j.patcog.2022.108727
   Jian CX, 2021, ENG APPL ARTIF INTEL, V104, DOI 10.1016/j.engappai.2021.104365
   Jiang JW, 2020, GRAPH MODELS, V111, DOI 10.1016/j.gmod.2020.101077
   Jiang Y., 2021, P 35 C NEUR INF PROC, P14745
   Ke ZH, 2019, IEEE I CONF COMP VIS, P6727, DOI 10.1109/ICCV.2019.00683
   Kim D, 2019, INFORM SCIENCES, V477, P15, DOI 10.1016/j.ins.2018.10.006
   Krizhevsky A., 2012, Learning multiple layers of features from tiny images, DOI DOI 10.1145/3079856.3080246
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee Dong-Hyun, 2013, P ICML WORKSH CHALL
   Li BT, 2021, EXPERT SYST APPL, V165, DOI 10.1016/j.eswa.2020.113957
   Li CX, 2022, IEEE T PATTERN ANAL, V44, P9629, DOI 10.1109/TPAMI.2021.3127558
   Li CJ, 2017, ADV NEUR IN, V30
   Li W, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107646
   Li XZ, 2023, NEUROCOMPUTING, V547, DOI 10.1016/j.neucom.2023.126352
   Li YC, 2021, NEUROCOMPUTING, V435, P26, DOI 10.1016/j.neucom.2020.12.114
   Liu L, 2021, PATTERN RECOGN, V120, DOI 10.1016/j.patcog.2021.108140
   Liu Y, 2020, PROC CVPR IEEE, P5719, DOI 10.1109/CVPR42600.2020.00576
   Liu Z, 2023, KNOWL-BASED SYST, V259, DOI 10.1016/j.knosys.2022.110053
   Lu LY, 2023, BIOMED SIGNAL PROCES, V79, DOI 10.1016/j.bspc.2022.104203
   Mao JW, 2022, COMPUT BIOL MED, V147, DOI 10.1016/j.compbiomed.2022.105729
   Mayer C, 2021, COMPUT VIS IMAGE UND, V202, DOI 10.1016/j.cviu.2020.103109
   Meel P, 2021, EXPERT SYST APPL, V177, DOI 10.1016/j.eswa.2021.115002
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Netzer Y., 2011, Advances in neural information processing systems
   Qin YM, 2023, VISUAL COMPUT, V39, P3597, DOI 10.1007/s00371-023-02922-x
   Qiu SL, 2022, NEUROCOMPUTING, V492, P278, DOI 10.1016/j.neucom.2022.04.020
   Ranzato M, 2007, PROC CVPR IEEE, P1429
   Rasmus A, 2015, ADV NEUR IN, V28
   Ren Q, 2022, EXPERT SYST APPL, V202, DOI 10.1016/j.eswa.2022.117278
   Rifai S, 2011, NIPS
   Rubin M, 2019, MED IMAGE ANAL, V57, P176, DOI 10.1016/j.media.2019.06.014
   Salakhutdinov R., 2007, AISTATS, V2, P412
   Salimans T, 2016, ADV NEUR IN, V29
   Sheng B, 2018, COMPUT AIDED GEOM D, V62, P133, DOI 10.1016/j.cagd.2018.03.021
   Sohn K., 2020, Advances in Neural Information Processing Systems, P596
   Springenberg J. T., 2016, ICML
   Tarvainen A, 2017, ADV NEUR IN, V30
   Verma V, 2022, NEURAL NETWORKS, V145, P90, DOI 10.1016/j.neunet.2021.10.008
   Wang J, 2022, NEUROCOMPUTING, V514, P162, DOI 10.1016/j.neucom.2022.09.004
   Wang L, 2022, VISUAL COMPUT, V38, P2009, DOI 10.1007/s00371-021-02262-8
   Wang RQ, 2023, PATTERN RECOGN, V133, DOI 10.1016/j.patcog.2022.108987
   Wei X, 2021, NEURAL NETWORKS, V133, P166, DOI 10.1016/j.neunet.2020.10.018
   Weston J., 2008, P 25 INT C MACH LEAR
   Wu Y.-H., 2022, IEEE Trans. Pattern Anal. Mach. Intell.
   Wu YH, 2022, IEEE T PATTERN ANAL, V44, P10261, DOI 10.1109/TPAMI.2021.3134684
   Xiao H, 2022, NEUROCOMPUTING, V508, P36, DOI 10.1016/j.neucom.2022.08.052
   Xiu Y, 2022, WIREL COMMUN MOB COM, V2022, DOI 10.1155/2022/5323327
   Xu H, 2023, KNOWL-BASED SYST, V260, DOI 10.1016/j.knosys.2022.110166
   Yang M, 2023, EXPERT SYST APPL, V213, DOI 10.1016/j.eswa.2022.118873
   Yang M, 2023, PATTERN RECOGN, V140, DOI 10.1016/j.patcog.2023.109521
   Yang S., 2023, INT C ART INT STAT
   Yu K, 2020, MEASUREMENT, V165, DOI 10.1016/j.measurement.2020.107987
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang BW, 2021, 35 C NEURAL INFORM P, V34
   Zhang Huan, 2020, P INT C LEARN REPR
   Zhang YK, 2021, SIGNAL PROCESS, V183, DOI 10.1016/j.sigpro.2021.108030
   Zhao Z., 2021, P AAAI C ART INT
   Zoppi T, 2021, IEEE ACCESS, V9, P150579, DOI 10.1109/ACCESS.2021.3125920
NR 81
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 6
PY 2024
DI 10.1007/s00371-024-03360-z
EA APR 2024
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NA4P6
UT WOS:001197708000001
DA 2024-08-05
ER

PT J
AU Eldosoky, MA
   Li, JP
   Ul Haq, A
   Zeng, FY
   Xu, M
   Khan, S
   Khan, I
AF Eldosoky, Mahmoud A.
   Li, Jian Ping
   Ul Haq, Amin
   Zeng, Fanyu
   Xu, Mao
   Khan, Shakir
   Khan, Inayat
TI WallNet: Hierarchical Visual Attention-Based Model for Putty Bulge
   Terminal Points Detection
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Autonomous construction robots; Putty bulge detection; Hierarchical
   visual attention; Attention features fusion
AB Construction robots have conquered the indoor and outdoor building decoration fields, aiming to automatically accomplish the manually performed tasks efficiently, thus reducing the dependence on human labor and saving time. Fixing the putty on walls is a labor-intensive and slow process, so incorporating construction robots into such a task is significant. While fixing the putty on walls, putty bulges emerge in various positions within the working space. To successfully realize this task, the robots must autonomously determine the putty bulge positions within the working area. Integrating visual attention mechanisms into convolutional neural networks has been proven to enhance their feature extraction capability. We proposed a deep learning model for regressing the putty bulge terminal points spatial positions. Two novel visual attention modules were proposed and precisely integrated into the model's backbone. For enhancing the extraction of semantic features and better formulating the channel dependency, a residual channel attention module (RCAM) was proposed. A lightweight spatial attention module (LSAM) was proposed to maximize the weights of significant spatial information so the model can localize the bulge terminal points more accurately. The features generated by the attention modules at multiple scales were fused by a proposed attention feature fusion module (AFFM) to accomplish the putty bulge terminal points regression task. Our experiments proved that fusing the hierarchical feature maps extracted by the proposed attention modules is significantly better than the traditional learning scheme that directly propagates the feature maps throughout the network architecture.
C1 [Eldosoky, Mahmoud A.; Li, Jian Ping; Ul Haq, Amin] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Eldosoky, Mahmoud A.] Benha Univ, Benha Fac Engn, Dept Elect Engn, Banha 13511, Egypt.
   [Zeng, Fanyu] Nanjing Univ Posts & Telecommun, Sch Commun & Informat Engn, Nanjing 210003, Peoples R China.
   [Xu, Mao] Qingdao Univ, Sch Automat, Qingdao 266071, Peoples R China.
   [Khan, Shakir] Imam Mohammad Ibn Saud Islamic Univ IMSIU, Coll Comp & Informat Sci, Riyadh 11432, Saudi Arabia.
   [Khan, Inayat] Univ Engn & Technol, Dept Comp Sci Engn, Mardan 23200, Pakistan.
C3 University of Electronic Science & Technology of China; Egyptian
   Knowledge Bank (EKB); Benha University; Nanjing University of Posts &
   Telecommunications; Qingdao University; Imam Mohammad Ibn Saud Islamic
   University (IMSIU)
RP Li, JP (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
EM jpli2222@uestc.edu.cn
RI Khan, Dr Shakir/O-8721-2014
OI Khan, Dr Shakir/0000-0002-7925-9191; A. Eldosoky,
   Mahmoud/0000-0001-6894-1200
FU National Natural Science Foundation of China
FX No Statement Available
CR Al-Huda Z, 2023, ENG APPL ARTIF INTEL, V122, DOI 10.1016/j.engappai.2023.106142
   Asadi E, 2018, IEEE ROBOT AUTOM MAG, V25, P82, DOI 10.1109/MRA.2018.2816972
   Cai YQ, 2024, VISUAL COMPUT, V40, P169, DOI 10.1007/s00371-023-02773-6
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Eldosoky MA, 2022, IEEE ACCESS, V10, P73945, DOI 10.1109/ACCESS.2022.3190404
   Fu YP, 2022, VISUAL COMPUT, V38, P3243, DOI 10.1007/s00371-022-02559-2
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hou WQ, 2024, VISUAL COMPUT, V40, P459, DOI 10.1007/s00371-023-02793-2
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jing P, 2023, IEEE ACCESS, V11, P919, DOI 10.1109/ACCESS.2022.3233072
   Jocher Glenn, 2022, ultralytics/ yolov5: v7.0-yolov5 sota realtime instance segmentation
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li ZX, 2023, VISUAL COMPUT, V39, P1045, DOI 10.1007/s00371-021-02383-0
   Liu HJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3763, DOI 10.1109/ICCV48922.2021.00376
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (IEEE-ROBIO 2021), P1034, DOI 10.1109/ROBIO54168.2021.9739274
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Pang J, 2022, SIGNAL IMAGE VIDEO P, V16, P911, DOI 10.1007/s11760-021-02034-w
   Paszke A, 2019, ADV NEUR IN, V32
   Ren DY, 2024, VISUAL COMPUT, V40, P5155, DOI 10.1007/s00371-023-02907-w
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shamsabadi EA, 2022, AUTOMAT CONSTR, V140, DOI 10.1016/j.autcon.2022.104316
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wan HF, 2021, ADV MATER SCI ENG, V2021, DOI 10.1155/2021/5520515
   Wang GH, 2023, VISUAL COMPUT, V39, P2969, DOI 10.1007/s00371-022-02503-4
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang T, 2023, KNOWL-BASED SYST, V271, DOI 10.1016/j.knosys.2023.110541
   Wang WJ, 2021, KSCE J CIV ENG, V25, P4495
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yan RJ, 2019, IEEE T AUTOM SCI ENG, V16, P506, DOI 10.1109/TASE.2018.2829927
   Yang H, 2023, VISUAL COMPUT, V39, P6711, DOI 10.1007/s00371-022-02758-x
   Yang QN, 2020, AUTOMAT CONSTR, V116, DOI 10.1016/j.autcon.2020.103199
   Zhang T, 2023, ELECTRON LETT, V59, DOI 10.1049/ell2.12687
NR 37
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 18
PY 2024
DI 10.1007/s00371-024-03312-7
EA MAR 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LJ9G4
UT WOS:001186543400001
DA 2024-08-05
ER

PT J
AU Liang, MY
   Jiang, X
   Cao, J
   Li, B
   Wang, L
   Chen, QH
   Zhang, CL
   Zhao, YJ
AF Liang, Meiyan
   Jiang, Xing
   Cao, Jie
   Li, Bo
   Wang, Lin
   Chen, Qinghui
   Zhang, Cunlin
   Zhao, Yuejin
TI CAF-AHGCN: context-aware attention fusion adaptive hypergraph
   convolutional network for human-interpretable prediction of gigapixel
   whole-slide image
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Context-aware Information; Computational pathology; Adaptive hypergraph
   convolution network; Interpretability; Whole-slide image
ID INSTANCE; TRANSFORMER
AB Predicting labels of gigapixel whole-slide images (WSIs) and localizing regions of interest (ROIs) with high precision are of great interest in computational pathology. The existing methods are mainly based on multi-instance learning (MIL) approach and its variants. However, such algorithms primarily treat the instances of slides as independent samples, which cannot effectively describe the relationship between instances in WSI. This would cause only a subset of high-score instances can be located, which is not applicable in clinical scenarios. Context-aware attention fusion adaptive hypergraph convolution network (CAF-AHGCN) is proposed to adaptively establish the local and global topology of cropped image patches spatially arranged in the slides. The framework combines hypergraph embedding representation with attention-based MIL pooling aggregation in a hierarchical feature fusion manner, which fully preserves high-order spatial structural correlations of the patches to make a slide-level prediction. Here, an adaptive hypergraph convolutional network is designed to dynamically adjust the correlation strength between the hypergraph nodes as the model goes deeper. Poly-1 loss and residual connection are also applied to prevent over-smoothing and improve the generalization ability of deep CAF-AHGCN model. We verified the superiority of CAF-AHGCN on two datasets, CAMELYON16 and TCGA-NSCLC. The results showed that the ACC, AUC and F1 score predicted by our model outperform other state-of-the-art algorithms on both datasets. The heat maps obtained by CAF-AHGCN are highly consistent with the pixel-wise annotated label of the WSI. The results show that CAF-AHGCN not only achieves high-accuracy label prediction for WSI, but also provides patch-wise human-interpretable features in ROI localization heatmaps. The outstanding performance of CAF-AHGCN framework provides a new perspective for future clinical applications of computer-aided diagnosis and intelligent systems.
C1 [Liang, Meiyan; Jiang, Xing; Chen, Qinghui] Shanxi Univ, Sch Phys & Elect Engn, Taiyuan 030006, Peoples R China.
   [Liang, Meiyan] ShanxiBeike Biotechnol Co Ltd, Taiyuan 030032, Peoples R China.
   [Cao, Jie; Zhao, Yuejin] Beijing Inst Technol, Sch Opt & Photon, Beijing 100081, Peoples R China.
   [Wang, Lin] Shanxi Med Univ, Shanxi Bethune Hosp, Tongji Shanxi Hosp, Shanxi Acad Med Sci,Hosp 3,Dept Pathol, Taiyuan 030032, Peoples R China.
   [Li, Bo] Shanxi Rongjun Hosp, Dept Rehabil Treatment, Taiyuan 030000, Peoples R China.
   [Zhang, Cunlin] Capital Normal Univ, Beijing Key Lab Terahertz Spect & Imaging, Key Lab Terahertz Optoelect, Minist Educ, Beijing 100048, Peoples R China.
C3 Shanxi University; Beijing Institute of Technology; Shanxi Medical
   University; Capital Normal University
RP Liang, MY (corresponding author), Shanxi Univ, Sch Phys & Elect Engn, Taiyuan 030006, Peoples R China.; Liang, MY (corresponding author), ShanxiBeike Biotechnol Co Ltd, Taiyuan 030032, Peoples R China.; Cao, J (corresponding author), Beijing Inst Technol, Sch Opt & Photon, Beijing 100081, Peoples R China.; Wang, L (corresponding author), Shanxi Med Univ, Shanxi Bethune Hosp, Tongji Shanxi Hosp, Shanxi Acad Med Sci,Hosp 3,Dept Pathol, Taiyuan 030032, Peoples R China.
EM meiyanliang@sxu.edu.cn; caojie@bit.edu.cn; 13613411069@139.com
RI chen, xi/GXH-3653-2022
FU Natural Science Foundation of Shanxi Province [202203021222015]; Natural
   Science Foundation of Shanxi Province [11804209]; National Natural
   Science Foundation of China [2023-010]; Shanxi Scholarship Council of
   China
FX This work is supported in part by Natural Science Foundation of Shanxi
   Province under Grant 202303021211014 and 20210302123411, National
   Natural Science Foundation of China under Grant 11804209, Research
   Project Supported by Shanxi Scholarship Council of China 2023-010, the
   Natural Science Foundation of Shanxi Province under Grant
   202203021222015.
CR Abel R, 2019, Arxiv, DOI arXiv:1911.06892
   Ahmad N, 2022, VISUAL COMPUT, V38, P2751, DOI 10.1007/s00371-021-02153-y
   Ahmedt-Aristizabal D, 2022, COMPUT MED IMAG GRAP, V95, DOI 10.1016/j.compmedimag.2021.102027
   Bai S, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107637
   Bontempo G., 2023, INT C IM AN PROC, P1
   Campanella G, 2019, NAT MED, V25, P1301, DOI 10.1038/s41591-019-0508-1
   Chen RJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3995, DOI 10.1109/ICCV48922.2021.00398
   Chen RJ, 2021, LECT NOTES COMPUT SC, V12908, P339, DOI 10.1007/978-3-030-87237-3_33
   Cheng S., 2021, Nature Communications, V12, P1, DOI [DOI 10.1038/S41467-021-25296-X, 10.21203/rs.3.rs-377187/v1]
   Deng RN, 2022, LECT NOTES COMPUT SC, V13594, P24, DOI 10.1007/978-3-031-18814-5_3
   Gao Y, 2022, IEEE T PATTERN ANAL, V44, P2548, DOI 10.1109/TPAMI.2020.3039374
   Giraldo JH, 2022, IEEE IMAGE PROC, P16, DOI 10.1109/ICIP46576.2022.9897774
   Guan YH, 2022, PROC CVPR IEEE, P18791, DOI 10.1109/CVPR52688.2022.01825
   Gurcan Metin N, 2009, IEEE Rev Biomed Eng, V2, P147, DOI 10.1109/RBME.2009.2034865
   Hou WT, 2022, AAAI CONF ARTIF INTE, P933
   Hsu WW, 2021, Arxiv, DOI arXiv:2108.02656
   Huang J, 2021, IEEE IMAGE PROC, P3657, DOI 10.1109/ICIP42928.2021.9506153
   Ianni JD, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-59985-2
   Ilse M, 2018, PR MACH LEARN RES, V80
   Jaume G, 2021, PROC CVPR IEEE, P8102, DOI 10.1109/CVPR46437.2021.00801
   Jialun Wu, 2021, 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), P2104, DOI 10.1109/BIBM52615.2021.9669870
   Kanavati F, 2021, TECHNOL CANCER RES T, V20, DOI 10.1177/15330338211027901
   Kanavati F, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-66333-x
   Leng ZQ, 2022, Arxiv, DOI [arXiv:2204.12511, DOI 10.48550/ARXIV.2204.12511]
   Lerousseau Marvin, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12265), P470, DOI 10.1007/978-3-030-59722-1_45
   Li B, 2021, PROC CVPR IEEE, P14313, DOI 10.1109/CVPR46437.2021.01409
   Li H, 2021, LECT NOTES COMPUT SC, V12908, P206, DOI 10.1007/978-3-030-87237-3_20
   Li WJ, 2019, LECT NOTES COMPUT SC, V11764, P532, DOI 10.1007/978-3-030-32239-7_59
   Li XY, 2024, VISUAL COMPUT, V40, P5717, DOI 10.1007/s00371-023-03131-2
   Li Z, 2021, IEEE J BIOMED HEALTH, V25, P429, DOI 10.1109/JBHI.2020.3039741
   Liang MY, 2023, COMPUT METH PROG BIO, V229, DOI 10.1016/j.cmpb.2022.107268
   Lu MY, 2021, NAT BIOMED ENG, V5, P555, DOI 10.1038/s41551-020-00682-w
   Lu WQ, 2020, IEEE COMPUT SOC CONF, P1049, DOI 10.1109/CVPRW50498.2020.00138
   Marini N, 2022, NPJ DIGIT MED, V5, DOI 10.1038/s41746-022-00635-4
   Marini N, 2021, PR MACH LEARN RES, V156, P170
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Qu LH, 2022, LECT NOTES COMPUT SC, V13432, P24, DOI 10.1007/978-3-031-16434-7_3
   Sankarapandian S., P IEEE CVF INT C COM, P629
   Shao ZC, 2021, ADV NEUR IN
   Sureka M, 2020, IEEE INT C BIOINF BI, P331, DOI 10.1109/BIBE50027.2020.00060
   Thandiackal K, 2022, Arxiv, DOI [arXiv:2204.12454, DOI 10.48550/ARXIV.2204.12454]
   Wang SJ, 2019, MED IMAGE ANAL, V58, DOI 10.1016/j.media.2019.101549
   Wang XL, 2019, INT CONF MEASURE, P1, DOI [10.1109/ICMIC48233.2019.9068567, 10.1109/TCYB.2019.2935141]
   Xiang TG, 2022, IEEE T MED IMAGING, V41, P2180, DOI 10.1109/TMI.2022.3157983
   Xu G, 2019, IEEE I CONF COMP VIS, P10681, DOI 10.1109/ICCV.2019.01078
   Zhang HR, 2022, PROC CVPR IEEE, P18780, DOI 10.1109/CVPR52688.2022.01824
   Zhao Y, 2022, LECT NOTES COMPUT SC, V13432, P66, DOI 10.1007/978-3-031-16434-7_7
   Zhao Y, 2020, PROC CVPR IEEE, P4836, DOI 10.1109/CVPR42600.2020.00489
   Zheng Y., 2022, arXiv
   Zheng YS, 2022, LECT NOTES COMPUT SC, V13432, P283, DOI 10.1007/978-3-031-16434-7_28
   Zhou YN, 2019, IEEE INT CONF COMP V, P388, DOI 10.1109/ICCVW.2019.00050
NR 51
TC 0
Z9 0
U1 6
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 13
PY 2024
DI 10.1007/s00371-024-03269-7
EA FEB 2024
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HO6Q9
UT WOS:001160490100003
DA 2024-08-05
ER

PT J
AU Saleh, AR
   Momeni, HR
AF Saleh, Ahmad Reza
   Momeni, Hamid Reza
TI An improved iterative closest point algorithm based on the particle
   filter and K-means clustering for fine model matching
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Matching algorithm; Iterative closest point; Particle filter; Global
   optimization; K-means clustering; Measurement uncertainty
ID REGISTRATION; ROBUST
AB The rigid matching of two geometric clouds is vital in the computer vision and its intelligent applications, such as computational geometry, robotics, shape modelling, surface reconstruction and mapping, and many other fields. The variants of the iterative closest point algorithm were employed as the most noticeable matching algorithm. In traditional ICP algorithms applications for symmetrical geometry matching, the initial uncertainty and the multiple local minima of the distance function adversely affect the alignment process, which leads to weak performance, such as incorrect correspondence, narrow convergence region, and instability. In this study, the novel algorithm fused the ICP algorithm, particle filter and K-means clustering to correctly estimate the transformation ICP parameters. Further guide to initial values of parameters and their covariance obtained by k-means clustering. Then, a particle filter was implemented to estimate accurate values and perform global optimization. In the introduced PF-ICP algorithm, the alignment parameters: rotation angles, scale factor, and translation, were defined as particles elements optimized using a sequential importance resampling (SIR) particle filter. The proposed algorithm was implemented on a medical robot FPGA board and applied to "three symmetrical models" and "noisy and poor datasets." The calculated variances and estimated parameters were compared with four modified ICP methods. The results show a significantly increasing accuracy and convergence region with an acceptable speed for the practical conditions.
C1 [Saleh, Ahmad Reza; Momeni, Hamid Reza] Tarbiat Modares Univ, Fac Elect & Comp Engn, POB 14115-111, Tehran, Iran.
C3 Tarbiat Modares University
RP Momeni, HR (corresponding author), Tarbiat Modares Univ, Fac Elect & Comp Engn, POB 14115-111, Tehran, Iran.
EM ahmadreza_saleh@modares.ac.ir; Research552@gmail.com
CR Ameer M, 2022, MATHEMATICS-BASEL, V10, DOI 10.3390/math10071045
   Barrie Wetherill G., 1986, Regression Analysis with Applications, DOI [10.1007/978-94-009-4105-2, DOI 10.1007/978-94-009-4105-2]
   Bengtsson O, 2003, ROBOT AUTON SYST, V44, P29, DOI 10.1016/S0921-8890(03)00008-3
   Bergström P, 2017, NUMER ALGORITHMS, V74, P755, DOI 10.1007/s11075-016-0170-3
   Bergström P, 2016, COMPUT OPTIM APPL, V63, P543, DOI 10.1007/s10589-015-9771-3
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bouaziz S, 2013, COMPUT GRAPH FORUM, V32, P113, DOI 10.1111/cgf.12178
   Breux Y, 2022, INT J ROBOT RES, V41, P875, DOI 10.1177/02783649221101418
   Censi A, 2007, IEEE INT CONF ROBOT, P3167, DOI 10.1109/ROBOT.2007.363961
   Combès B, 2020, COMPUT VIS IMAGE UND, V191, DOI 10.1016/j.cviu.2019.102854
   Dong JM, 2014, NEUROCOMPUTING, V140, P67, DOI 10.1016/j.neucom.2014.03.035
   Eggert DW, 1997, MACH VISION APPL, V9, P272, DOI 10.1007/s001380050048
   Favre K, 2021, IEEE SYS MAN CYBERN, P2018, DOI 10.1109/SMC52423.2021.9658727
   Fox D, 1999, SIXTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-99)/ELEVENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE (IAAI-99), P343
   Guo Y, 2022, NEUROCOMPUTING, V508, P225, DOI 10.1016/j.neucom.2022.08.047
   Gustafsson F, 2010, IEEE AERO EL SYS MAG, V25, P53, DOI 10.1109/MAES.2010.5546308
   Hu L, 2020, VISUAL COMPUT, V36, P669, DOI 10.1007/s00371-019-01648-z
   Junrui Zhao, 2021, Proceedings of the 2021 IEEE International Conference on Power Electronics, Computer Applications (ICPECA), P70, DOI 10.1109/ICPECA51329.2021.9362557
   Kamgar-Parsi B., 1999, Unmanned Ground Vehicle Technology, V3693
   Li JY, 2022, ISPRS J PHOTOGRAMM, V185, P219, DOI 10.1016/j.isprsjprs.2022.01.019
   Li M, 2020, VISUAL COMPUT, V36, P1725, DOI 10.1007/s00371-019-01771-x
   Liu JS, 2001, STAT ENG IN, P225
   Majeed A, 2020, MATHEMATICS-BASEL, V8, DOI 10.3390/math8081246
   Maken FA, 2022, IEEE ROBOT AUTOM LET, V7, P1063, DOI 10.1109/LRA.2021.3137503
   Maken FA, 2020, IEEE INT CONF ROBOT, P8602, DOI [10.1109/icra40945.2020.9197085, 10.1109/ICRA40945.2020.9197085]
   Manoj PS, 2015, 2015 14th IAPR International Conference on Machine Vision Applications (MVA), P526, DOI 10.1109/MVA.2015.7153246
   Masuda T., 1996, P 13 INT C PATT REC
   Meng Y, 2011, VISUAL COMPUT, V27, P543, DOI 10.1007/s00371-011-0580-0
   Muñoz FII, 2018, ADV ROBOTICS, V32, P161, DOI 10.1080/01691864.2018.1434013
   Nguyen MX, 2005, VISUAL COMPUT, V21, P669, DOI 10.1007/s00371-005-0315-1
   Nüchter A, 2007, 3DIM 2007: SIXTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P419
   Oomori S, 2016, ARTIF LIFE ROBOT, V21, P149, DOI 10.1007/s10015-016-0265-x
   Oztireli AC, 2008, VISUAL COMPUT, V24, P679, DOI 10.1007/s00371-008-0248-6
   Röwekämper J, 2012, IEEE INT C INT ROBOT, P3158, DOI 10.1109/IROS.2012.6385988
   Saleem W, 2007, VISUAL COMPUT, V23, P381, DOI 10.1007/s00371-006-0094-3
   Segal A., 2009, P ROB SCI SYST SEATT, P435, DOI DOI 10.15607/RSS.2009.V.021
   Song YN, 2023, VISUAL COMPUT, V39, P1109, DOI 10.1007/s00371-021-02391-0
   Speekenbrink M, 2016, J MATH PSYCHOL, V73, P140, DOI 10.1016/j.jmp.2016.05.006
   Turk G., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P311, DOI 10.1145/192161.192241
   Walker HF, 2011, SIAM J NUMER ANAL, V49, P1715, DOI 10.1137/10078356X
   Wang X, 2020, IEEE ACCESS, V8, P40692, DOI 10.1109/ACCESS.2020.2976132
   Xiao J, 2020, J NAVIGATION, V73, P56, DOI 10.1017/S0373463319000535
   Xiao J, 2018, J NAVIGATION, V71, P649, DOI 10.1017/S0373463317000844
   Ying WJ, 2022, COGN COMPUT SYST, V4, P20, DOI 10.1049/ccs2.12040
   Yue XF, 2022, APPL INTELL, V52, P12569, DOI 10.1007/s10489-022-03201-3
   Zhang JY, 2022, IEEE T PATTERN ANAL, V44, P3450, DOI 10.1109/TPAMI.2021.3054619
NR 46
TC 0
Z9 0
U1 10
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 22
PY 2024
DI 10.1007/s00371-023-03195-0
EA JAN 2024
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GJ3D1
UT WOS:001152252500002
DA 2024-08-05
ER

PT J
AU Alvarado, E
   Argudo, O
   Rohmer, D
   Cani, MP
   Pelechano, N
AF Alvarado, Eduardo
   Argudo, Oscar
   Rohmer, Damien
   Cani, Marie-Paule
   Pelechano, Nuria
TI TRAIL: Simulating the impact of human locomotion on natural landscapes
SO VISUAL COMPUTER
LA English
DT Review
DE Character animation; Natural phenomena
ID VEGETATION; CHARACTERS; KINEMATICS; MOTION
AB Human and animal presence in natural landscapes is initially revealed by the immediate impact of their locomotion, from footprints to crushed grass. In this work, we present an approach to model the effects of virtual characters on natural terrains, focusing on the impact of human locomotion. We introduce a lightweight solution to compute accurate foot placement on uneven ground and infer dynamic foot pressure from kinematic animation data and the mass of the character. A ground and vegetation model enables us to effectively simulate the local impact of locomotion on soft soils and plants over time, resulting in the formation of visible paths. As our results show, we can parameterize various soil materials and vegetation types validated with real-world data. Our method can be used to significantly increase the realism of populated natural landscapes and the sense of presence in virtual applications and games.
C1 [Alvarado, Eduardo] MPI Informat, Saarland Informat Campus, Saarbrucken, Germany.
   [Argudo, Oscar; Pelechano, Nuria] Univ Politecn Cataluna, Barcelona, Spain.
   [Rohmer, Damien; Cani, Marie-Paule] Inst Polytech Paris, Ecole Polytech, LIX, CNRS, Palaiseau, France.
C3 Max Planck Society; Universitat Politecnica de Catalunya; Institut
   Polytechnique de Paris; Ecole Polytechnique; Centre National de la
   Recherche Scientifique (CNRS)
RP Alvarado, E (corresponding author), MPI Informat, Saarland Informat Campus, Saarbrucken, Germany.
EM ealvarad@mpi-inf.mpg.de
OI Argudo, Oscar/0000-0003-3943-1839
FU Max Planck Institute for Informatics (2) [TED2021-129761B-I00,
   MCIN/AEI/10.13039/501100011033]; European Union [860768]
FX This work is part of the project SENDA (TED2021-129761B-I00), funded by
   MCIN/AEI/10.13039/501100011033 and the European Union
   "NextGenerationEU"/PRTR. It is also part of the European Union's Horizon
   2020 research and innovation programme CLIPE, under the Marie Sk &
   lstrok;odowska-Curie grant agreement No. 860768.
CR Alvarado E, 2022, COMPUT GRAPH FORUM, V41, P169, DOI 10.1111/cgf.14633
   Alvarado E, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.801856
   Aristidou A, 2018, COMPUT GRAPH FORUM, V37, P35, DOI 10.1111/cgf.13310
   Bermudez L, 2018, ACM SIGGRAPH CONFERENCE ON MOTION, INTERACTION, AND GAMES (MIG 2018), DOI 10.1145/3274247.3274515
   Boulic R, 1996, COMPUT GRAPH, V20, P693, DOI 10.1016/S0097-8493(96)00043-X
   Clavet S., 2016, GAM DEV C GDC
   COLE DN, 1995, J APPL ECOL, V32, P203, DOI 10.2307/2404429
   COLE DN, 1995, J APPL ECOL, V32, P215, DOI 10.2307/2404430
   Cordonnier Guillaume, 2017, ACM Transactions on Graphics, V36, DOI 10.1145/3072959.3073667
   Cordonnier G, 2018, COMPUT GRAPH FORUM, V37, P497, DOI 10.1111/cgf.13379
   Cordonnier G, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592422
   da Silva DB, 2017, COMPUT GRAPH FORUM, V36, P499, DOI 10.1111/cgf.13096
   Gain J, 2017, COMPUT GRAPH FORUM, V36, P63, DOI 10.1111/cgf.13107
   Galin E, 2019, COMPUT GRAPH FORUM, V38, P553, DOI 10.1111/cgf.13657
   Garcia A.L., 2012, ACM SIGGRAPH 2012 CO
   Gerling B, 2017, GEOPHYS RES LETT, V44, P11088, DOI 10.1002/2017GL075110
   Hodgins J. K., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P71, DOI 10.1145/218380.218414
   Holden D, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392440
   Holden D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073663
   Johansen R.S., 2009, THESIS AARHUS U I IN
   Karim A.A., 2012, WORKSH VIRT REAL INT, DOI [10.2312/PE/vriphys, DOI 10.2312/PE/VRIPHYS]
   Karim AA, 2013, COMPUT ANIMAT VIRT W, V24, P3, DOI 10.1002/cav.1467
   Klár G, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925906
   Kry PG, 2012, COMPUT GRAPH-UK, V36, P904, DOI 10.1016/j.cag.2012.08.010
   Kwiatkowski A, 2022, COMPUT GRAPH FORUM, V41, P613, DOI 10.1111/cgf.14504
   Kwon T, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392432
   Lee J, 1999, COMP GRAPH, P39
   Lentine M, 2011, IEEE T VIS COMPUT GR, V17, P682, DOI 10.1109/TVCG.2010.108
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Marion JL, 2017, J ENVIRON MANAGE, V189, P46, DOI 10.1016/j.jenvman.2016.11.074
   Mourot L, 2022, COMPUT GRAPH FORUM, V41, P122, DOI 10.1111/cgf.14426
   Müller M, 2007, J VIS COMMUN IMAGE R, V18, P109, DOI 10.1016/j.jvcir.2007.01.005
   Paliard C., 2021, EUROGRAPHICS 2021 SH, DOI [10.2312/egs.20211019, DOI 10.2312/EGS.20211019]
   Paris A, 2019, COMPUT GRAPH FORUM, V38, P47, DOI 10.1111/cgf.13815
   Raibert M. H., 1991, Computer Graphics, V25, P349, DOI 10.1145/127719.122755
   Shahabpoor E, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17092085
   Wannop JW, 2014, GAIT POSTURE, V40, P118, DOI 10.1016/j.gaitpost.2014.03.004
   Witkin A., 1988, Computer Graphics, V22, P159, DOI 10.1145/378456.378507
NR 38
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 5029
EP 5041
DI 10.1007/s00371-024-03506-z
EA JUN 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001242155100007
OA hybrid
DA 2024-08-05
ER

PT J
AU Qian, B
   Chen, H
   Xu, YP
   Wen, Y
   Li, HT
   Xie, Y
   Feng, DD
   Kim, JM
   Bi, L
   Xu, X
   He, XG
   Sheng, B
AF Qian, Bo
   Chen, Hao
   Xu, Yupeng
   Wen, Yang
   Li, Huating
   Xie, Yuan
   Feng, David Dagan
   Kim, Jinman
   Bi, Lei
   Xu, Xun
   He, Xiangui
   Sheng, Bin
TI Deep contour attention learning for scleral deformation from OCT images
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Convolutional network; Deep learning; Scleral segmentation; Attention
   mechanism
ID OPTICAL COHERENCE TOMOGRAPHY; SWEPT-SOURCE; PATHOLOGICAL MYOPIA; RETINAL
   LAYER; SEGMENTATION; EYES; COMPLICATIONS; THICKNESS; POSTERIOR; GLAUCOMA
AB Swept-source optical coherence tomography (SS-OCT) is widely used to diagnose high myopia due to its advantage in imaging the ocular anatomic structures. Although the scleral deformation provides information on the risk of the high myopia, further validation of these highly promising findings in clinical studies has been limited by the current semi-automated software, which requires human input, and the automatic analysis of the scleral structure is quite challenging due to the ambiguous boundaries. To address these challenges, we propose a deep contour attention network (DCANet) for automatic segmentation of scleral deformation structure. Specifically, we design a scale-aware attention feature fusion module to achieve cross-scale feature fusion, which can facilitate the network to learn complementary information from multi-scale features. In addition, we develop a pyramid feature enhancement module to allow the network to learn global contextual features through the combination of receptive field and attention mechanism, and we also propose a boundary heatmap label to enrich boundary information. We evaluate the performance of the proposed method on two in-house SS-OCT datasets. In addition to the multiple metrics that are used for evaluating the segmentation performance, including Jaccard similarity coefficient, dice similarity coefficient and boundary distance error, we also propose length similarity coefficient and angle similarity coefficient to evaluate the length estimation and angle estimation, respectively. The experimental results show that our method can effectively improve the segmentation performance, and our DCANet achieves the overall best performance on two datasets compared with other state-of-the-art networks. Our findings motivate the development of clinically applicable deep learning systems for the prediction of high myopia progression on the basis of the scleral phenotypes from SS-OCT images.
C1 [Qian, Bo; Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Chen, Hao] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
   [Xu, Yupeng; Xu, Xun; He, Xiangui] Tongii Univ, Dept Clin Res, Shanghai Vis Hlth Ctr,Shanghai Eye Hosp, Shanghai Eye Dis Prevent & Treatment Ctr,Sch Med, Shanghai 200030, Peoples R China.
   [Xu, Yupeng; Xu, Xun; He, Xiangui] Shanghai Children Myopia Inst, Shanghai, Peoples R China.
   [Xu, Yupeng; Xu, Xun; He, Xiangui] Shanghai Jiao Tong Univ, Natl Clin Res Ctr Eye Dis,Shanghai Gen Hosp,Dept O, Shanghai Key Lab Ocular Fundus Dis,Sch Med, Engn Ctr Visual Sci & Photomed,Shanghai Ctr Eye Sh, Shanghai, Peoples R China.
   [Wen, Yang] Shenzhen Univ, Coll Elect & Informat Engn, Guangdong Key Lab Intelligent Informat Proc, Shenzhen, Peoples R China.
   [Li, Huating] Shanghai Jiao Tong Univ, Affiliated Peoples Hosp 6, Shanghai, Peoples R China.
   [Xie, Yuan] East China Normal Univ, Sch Comp Sci & Technol, Shanghai, Peoples R China.
   [Feng, David Dagan; Kim, Jinman] Univ Sydney, Sch Comp Sci, Sydney, Australia.
   [Bi, Lei] Shanghai Jiao Tong Univ, Inst Translat Med, Shanghai, Peoples R China.
C3 Shanghai Jiao Tong University; Hong Kong University of Science &
   Technology; Tongji University; Shanghai Jiao Tong University; Shenzhen
   University; Shanghai Jiao Tong University; East China Normal University;
   University of Sydney; Shanghai Jiao Tong University
RP Sheng, B (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.; He, XG (corresponding author), Tongii Univ, Dept Clin Res, Shanghai Vis Hlth Ctr,Shanghai Eye Hosp, Shanghai Eye Dis Prevent & Treatment Ctr,Sch Med, Shanghai 200030, Peoples R China.; He, XG (corresponding author), Shanghai Children Myopia Inst, Shanghai, Peoples R China.; He, XG (corresponding author), Shanghai Jiao Tong Univ, Natl Clin Res Ctr Eye Dis,Shanghai Gen Hosp,Dept O, Shanghai Key Lab Ocular Fundus Dis,Sch Med, Engn Ctr Visual Sci & Photomed,Shanghai Ctr Eye Sh, Shanghai, Peoples R China.
EM xianhezi@163.com; shengbin@sjtu.edu.cn
RI Kim, Jin Man/HJO-8987-2023
FU Basic and Applied Basic Research Foundation of Guangdong Province
   [62272298, 62077037, 82201246, 82273648, 82171100, 62301330, 62101346];
   National Natural Science Foundation of China [2021YFC2702100,
   2021YFC2702104]; Key R &D Program of Ministry of Science and Technology
   [22XD1422900]; Excellent Academic Leader of Shanghai Science and
   Technology Commission [2022XD032]; Talent Program of Shanghai Municipal
   Health and Health Commission [2022A1515110101, 20231121103807001,
   2021A1515011702]; Guangdong Basic and Applied Basic Research Foundation
   [20231121103807001]; Stable Support Plan for Shenzhen Higher Education
   Institutions
FX This work was supported by National Natural Science Foundation of
   China(62272298, 62077037, 82201246, 82273648, 82171100, 62301330,
   62101346); Key R &D Program of Ministry of Science and Technology
   (2021YFC2702100, 2021YFC2702104); Excellent Academic Leader of Shanghai
   Science and Technology Commission (22XD1422900); Talent Program of
   Shanghai Municipal Health and Health Commission (2022XD032); Guangdong
   Basic and Applied Basic Research Foundation (2022A1515110101,
   20231121103807001, 2021A1515011702); the Stable Support Plan for
   Shenzhen Higher Education Institutions (20231121103807001).
CR Ahmad N, 2022, VISUAL COMPUT, V38, P2751, DOI 10.1007/s00371-021-02153-y
   Ang M, 2019, BRIT J OPHTHALMOL, V103, P855, DOI 10.1136/bjophthalmol-2018-312866
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bhattacharyya D, 2023, VISUAL COMPUT, V39, P5245, DOI 10.1007/s00371-022-02657-1
   Bi L, 2018, VISUAL COMPUT, V34, P1043, DOI 10.1007/s00371-018-1519-5
   Bogunovic H, 2014, IEEE T MED IMAGING, V33, P2242, DOI 10.1109/TMI.2014.2336246
   Chen H, 2018, NEUROIMAGE, V170, P446, DOI 10.1016/j.neuroimage.2017.04.041
   Chen H, 2017, MED IMAGE ANAL, V36, P135, DOI 10.1016/j.media.2016.11.004
   Chen M, 2017, LECT NOTES COMPUT SC, V10554, P177, DOI 10.1007/978-3-319-67561-9_20
   Chen XJ, 2012, INVEST OPHTH VIS SCI, V53, P8042, DOI 10.1167/iovs.12-10083
   Chen XJ, 2012, IEEE T MED IMAGING, V31, P1521, DOI 10.1109/TMI.2012.2191302
   Chiu SJ, 2010, OPT EXPRESS, V18, P19413, DOI 10.1364/OE.18.019413
   Choma MA, 2003, OPT EXPRESS, V11, P2183, DOI 10.1364/OE.11.002183
   Dai L, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-23458-5
   Danesh H, 2014, COMPUT MATH METHOD M, V2014, DOI 10.1155/2014/479268
   Devalla SK, 2018, BIOMED OPT EXPRESS, V9, P3244, DOI 10.1364/BOE.9.003244
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Ghosh S, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3329784
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu GY, 2020, INVEST OPHTH VIS SCI, V61, DOI 10.1167/iovs.61.4.46
   Huazhu Fu, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P132, DOI 10.1007/978-3-319-46723-8_16
   Ikuno Y, 2017, RETINA-J RET VIT DIS, V37, P2347, DOI 10.1097/IAE.0000000000001489
   Imran A, 2021, VISUAL COMPUT, V37, P2407, DOI 10.1007/s00371-020-01994-3
   Jiang M, 2022, VISUAL COMPUT, V38, P2473, DOI 10.1007/s00371-021-02124-3
   Jiang XB, 2021, VISUAL COMPUT, V37, P2419, DOI 10.1007/s00371-020-01996-1
   Kajic V, 2012, BIOMED OPT EXPRESS, V3, P86, DOI 10.1364/BOE.3.000086
   Kingma D. P., 2014, arXiv
   Kong LX, 2023, VISUAL COMPUT, V39, P5527, DOI 10.1007/s00371-022-02678-w
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li Y, 2021, VISUAL COMPUT, ppp1
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu HQ, 2013, IEEE ENG MED BIO, P5869, DOI 10.1109/EMBC.2013.6610887
   McBrien NA, 2003, PROG RETIN EYE RES, V22, P307, DOI 10.1016/S1350-9462(02)00063-0
   Ohno-Matsui K, 2012, INVEST OPHTH VIS SCI, V53, P7290, DOI 10.1167/iovs.12-10371
   Ohno-Matsui K, 2011, INVEST OPHTH VIS SCI, V52, P9644, DOI 10.1167/iovs.11-8597
   Oktay O, 2018, Arxiv, DOI [arXiv:1804.03999, DOI 10.48550/ARXIV.1804.03999]
   Park HYL, 2014, AM J OPHTHALMOL, V157, P876, DOI 10.1016/j.ajo.2014.01.007
   Park HYL, 2014, AM J OPHTHALMOL, V157, P550, DOI 10.1016/j.ajo.2013.11.008
   Querques G, 2015, AM J OPHTHALMOL, V160, P759, DOI 10.1016/j.ajo.2015.07.017
   Ran AR, 2019, LANCET DIGIT HEALTH, V1, pE172, DOI 10.1016/S2589-7500(19)30085-8
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roy AG, 2018, LECT NOTES COMPUT SC, V11070, P421, DOI 10.1007/978-3-030-00928-1_48
   Roy AG, 2017, BIOMED OPT EXPRESS, V8, P3627, DOI 10.1364/BOE.8.003627
   Saw SM, 2005, OPHTHAL PHYSL OPT, V25, P381, DOI 10.1111/j.1475-1313.2005.00298.x
   Shi F, 2015, IEEE T MED IMAGING, V34, P441, DOI 10.1109/TMI.2014.2359980
   Sui XD, 2017, NEUROCOMPUTING, V237, P332, DOI 10.1016/j.neucom.2017.01.023
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang FY, 2021, DIABETES CARE, V44, P2078, DOI 10.2337/dc20-3064
   Tian J, 2012, IEEE ENG MED BIO, P5360, DOI 10.1109/EMBC.2012.6347205
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang X, 2020, IEEE J BIOMED HEALTH, V24, P3431, DOI 10.1109/JBHI.2020.2983730
   Wang X, 2020, MED IMAGE ANAL, V63, DOI 10.1016/j.media.2020.101695
   Wang X, 2019, LECT NOTES COMPUT SC, V11764, P39, DOI 10.1007/978-3-030-32239-7_5
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xi PC, 2020, VISUAL COMPUT, V36, P1869, DOI 10.1007/s00371-019-01775-7
   Xiao HG, 2023, VISUAL COMPUT, V39, P2291, DOI 10.1007/s00371-022-02414-4
   Yanwu Xu, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9902, P441, DOI 10.1007/978-3-319-46726-9_51
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Zhang L, 2017, I S BIOMED IMAGING, P406, DOI 10.1109/ISBI.2017.7950548
NR 59
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 25
PY 2024
DI 10.1007/s00371-024-03401-7
EA MAY 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SJ3E7
UT WOS:001234039700002
DA 2024-08-05
ER

PT J
AU Yan, DP
   Ding, GY
   Huang, KX
   Huang, TY
AF Yan, Dapeng
   Ding, Gangyi
   Huang, Kexiang
   Huang, Tianyu
TI Generating natural pedestrian crowds by learning real crowd trajectories
   through a transformer-based GAN
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Crowd simulation; GAN; Pedestrian motion; Transformer
ID NEURAL-NETWORK; BEHAVIOR
AB Traditional methods for constructing crowd simulations often have shortcomings in terms of realism, and data-driven methods are an effective approach to enhancing the visual realism of crowd simulation. However, existing work mainly constructs crowd simulations through prediction-based approaches based on deep learning or by fitting the parameters of traditional methods, which limits the expressiveness of the model. In response to these limitations, this paper introduces a method capable of generating realistic pedestrian crowds. This approach uses a Generative Adversarial Network, complemented by a transformer module, to learn behavioral patterns from actual crowd trajectories. We use a transformer module to extract trajectory features of the crowd, then convert the spatial relationships between individuals into sequences using a special data processing mechanism, and use the transformer module to extract social features of the crowd, while guiding the movement of each individual with their target direction. During training, we simultaneously learn from real crowd data and simulation data resolving collisions by traditional methods, to enhance the collision avoidance behavior of virtual crowds while maintaining the movement patterns of real crowds, resulting in more general collision avoidance behavior. The crowds generated by the model are not limited to specific scenarios and show generalization capabilities. Compared to other models, our method shows better performance on publicly available large-scale pedestrian datasets after training. Our code is publicly available at https://github.com/ydp91/NPCGAN.
C1 [Yan, Dapeng; Ding, Gangyi; Huang, Kexiang; Huang, Tianyu] Beijing Inst Technol, Key Lab Digital Performance & Simulat Technol, Zhong Guan Cun South St, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology
RP Huang, TY (corresponding author), Beijing Inst Technol, Key Lab Digital Performance & Simulat Technol, Zhong Guan Cun South St, Beijing 100081, Peoples R China.
EM yandapeng@bit.edu.cn; huangtianyu@bit.edu.cn
RI Yan, Dapeng/HTT-0751-2023
OI Yan, Dapeng/0000-0003-2559-9841
FU National Key Research and Development Program of China; 
   [2020YFC2007200]
FX This research has been supported by the National Key Research and
   Development Program of China (No. 2020YFC2007200).
CR Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110
   Amirian J, 2019, PROCEEDINGS OF THE 32ND INTERNATIONAL CONFERENCE ON COMPUTER ANIMATION AND SOCIAL AGENTS (CASA 2019), P7, DOI 10.1145/3328756.3328769
   Amirian J, 2019, IEEE COMPUT SOC CONF, P2964, DOI 10.1109/CVPRW.2019.00359
   [Anonymous], 2009, P 2009 S INTERACTIVE, DOI DOI 10.1145/1507149.1507184
   Charalambous P, 2014, COMPUT GRAPH FORUM, V33, P95, DOI 10.1111/cgf.12403
   Charalambous P, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592459
   Colombo RM, 2009, NONLINEAR ANAL-REAL, V10, P2716, DOI 10.1016/j.nonrwa.2008.08.002
   Curtis S., 2012, Proceedings of the ACM SIGGRAPH symposium on interactive 3D graphics and games. ACM, P15
   Fiorini P, 1998, INT J ROBOT RES, V17, P760, DOI 10.1177/027836499801700706
   Giuliari F, 2021, INT C PATT RECOG, P10335, DOI 10.1109/ICPR48806.2021.9412190
   Golas A., 2014, ACM SIGGRAPH 2014 TA
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gupta A, 2018, PROC CVPR IEEE, P2255, DOI 10.1109/CVPR.2018.00240
   Helbing D, 2005, TRANSPORT SCI, V39, P1, DOI 10.1287/trsc.1040.0108
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   Hu KD, 2023, IEEE T VIS COMPUT GR, V29, P2036, DOI 10.1109/TVCG.2021.3139031
   Huang L, 2021, IEEE ACCESS, V9, P50846, DOI 10.1109/ACCESS.2021.3069134
   Jordao K., 2015, P 8 ACM SIGGRAPH C M, P167
   Kanyuk P., 2016, Simulating Heterogeneous Crowds with Interactive Behaviors, V217
   Kim S, 2016, P IEEE VIRT REAL ANN, P29, DOI 10.1109/VR.2016.7504685
   Kojima T, 2021, EACL 2021: THE 16TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: PROCEEDINGS OF THE STUDENT RESEARCH WORKSHOP, P175
   LEE S, 2010, ACM T GRAPHIC, V29, P1, DOI DOI 10.1145/1778765.1778859
   Lerner A, 2007, COMPUT GRAPH FORUM, V26, P655, DOI 10.1111/j.1467-8659.2007.01089.x
   Li YQ, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2067
   Lu GH, 2016, ACM T MODEL COMPUT S, V26, DOI 10.1145/2885496
   Luo LB, 2018, COMPUT GRAPH FORUM, V37, P375, DOI 10.1111/cgf.13303
   Lv ZZ, 2022, INT J INTELL SYST, V37, P4417, DOI 10.1002/int.22724
   Ma Y, 2016, IEEE T INTELL TRANSP, V17, P3159, DOI 10.1109/TITS.2016.2542843
   Mohamed A, 2022, LECT NOTES COMPUT SC, V13682, P463, DOI 10.1007/978-3-031-20047-2_27
   Narain R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618468
   Panayiotou Andreas, 2022, SIGGRAPH22 Conference Proceeding: Special Interest Group on Computer Graphics and Interactive Techniques Conference Proceedings, DOI 10.1145/3528233.3530712
   Park J., 2022, P IEEECVF C COMPUTER, P8983
   Pellegrini S, 2009, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2009.5459260
   Sadeghian A, 2019, PROC CVPR IEEE, P1349, DOI 10.1109/CVPR.2019.00144
   Sheng B, 2022, IEEE T CYBERNETICS, V52, P6662, DOI 10.1109/TCYB.2021.3079311
   Silva A.R.D., 2009, Comput. Entertain., V7, P1, DOI DOI 10.1145/1658866.1658870
   Snape J., 2010, 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2010), P4584, DOI 10.1109/IROS.2010.5652073
   Snape J, 2011, IEEE T ROBOT, V27, P696, DOI 10.1109/TRO.2011.2120810
   Song X, 2018, PHYSICA A, V509, P827, DOI 10.1016/j.physa.2018.06.045
   Ting SP, 2008, COMPUT ANIMAT VIRT W, V19, P505, DOI 10.1002/cav.262
   Tsai TY, 2018, COMPUT ANIMAT VIRT W, V29, DOI 10.1002/cav.1765
   van den Berg J, 2008, IEEE INT CONF ROBOT, P1928, DOI 10.1109/ROBOT.2008.4543489
   van den Berg J, 2011, SPRINGER TRAC ADV RO, V70, P3
   Vaswani A, 2017, ADV NEUR IN, V30
   Wei X, 2018, NEUROCOMPUTING, V310, P125, DOI 10.1016/j.neucom.2018.05.022
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Yao ZZ, 2020, NEUROCOMPUTING, V404, P173, DOI 10.1016/j.neucom.2020.04.141
   Yu S., 2022, arXiv
   Yuan Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9793, DOI 10.1109/ICCV48922.2021.00967
   Zhang GZ, 2022, PROCEEDINGS OF THE 28TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, KDD 2022, P2439, DOI 10.1145/3534678.3539440
   Zhang J., 2022, P 30 INT C ADV GEOGR
   Zhang JW, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2114
   Zhao M, 2018, COMPUT GRAPH FORUM, V37, P184, DOI 10.1111/cgf.13259
   Zhao MB, 2013, IEEE ACM DIS SIM, P125, DOI 10.1109/DS-RT.2013.21
   Zhong JH, 2022, ACM T MODEL COMPUT S, V32, DOI 10.1145/3481299
NR 55
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 29
PY 2024
DI 10.1007/s00371-024-03385-4
EA APR 2024
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PA0M6
UT WOS:001211242100001
DA 2024-08-05
ER

PT J
AU Yang, C
   Yang, M
   Li, HY
   Jiang, LL
   Suo, X
   Mao, LJ
   Meng, WL
   Li, Z
AF Yang, Chao
   Yang, Meng
   Li, Hongyu
   Jiang, Linlu
   Suo, Xiang
   Mao, Lijuan
   Meng, Weiliang
   Li, Zhen
TI A survey on soccer player detection and tracking with videos
SO VISUAL COMPUTER
LA English
DT Review; Early Access
DE Detection; Multi-object tracking; Soccer player tracking
ID MEAN SHIFT; CAMERA; SYSTEM
AB Soccer is a popular sport, and there is a growing need for automated analysis of soccer videos, while the detection and tracking of the players is the indispensable prerequisite. In this paper, we first introduce and classify multi-object tracking and then present two mostly used multi-object tracking methods, DeepSort and TrackFormer. When multi-object tracking is applied to soccer scenarios, some preprocessing and post-processing are generally performed, with preprocessing including processing of the video, such as splicing and background removing, and post-processing including further applications, such as player mapping for a 2D stadium. By directly employing the two methods above, we test the real scene and train TrackFormer to get further results. Meanwhile, in order to facilitate researchers who are interested in multi-object tracking as well as in the direction of player tracking, recent advances in preprocessing and processing methods for soccer player tracking are given and future research directions are suggested.
C1 [Yang, Chao; Yang, Meng; Li, Hongyu; Jiang, Linlu] Beijing Forestry Univ, Sch Informat Sci & Technol, Beijing, Peoples R China.
   [Yang, Meng] Natl Forestry & Grassland Adm, Engn Res Ctr Forestry Oriented Intelligent Proc, Beijing 100083, Peoples R China.
   [Suo, Xiang; Mao, Lijuan; Li, Zhen] Shanghai Univ Sport, Sch Phys Educ, Shanghai, Peoples R China.
   [Meng, Weiliang] Chinese Acad Sci, Inst Automat, Beijing, Peoples R China.
C3 Beijing Forestry University; Shanghai University of Sport; Chinese
   Academy of Sciences; Institute of Automation, CAS
RP Yang, M (corresponding author), Beijing Forestry Univ, Sch Informat Sci & Technol, Beijing, Peoples R China.; Yang, M (corresponding author), Natl Forestry & Grassland Adm, Engn Res Ctr Forestry Oriented Intelligent Proc, Beijing 100083, Peoples R China.; Li, Z (corresponding author), Shanghai Univ Sport, Sch Phys Educ, Shanghai, Peoples R China.
EM yangmeng@bjfu.edu.cn; maolijuan@sus.edu.cn; lizhen@sus.edu.cn
RI Li, Hongyu/KGL-4818-2024
OI Li, Hongyu/0000-0002-4588-9929; YANG, Meng/0000-0001-6439-2873; Li,
   Hongyu/0009-0008-2866-303X; SUO, Xiang/0009-0000-5899-0703
FU National Natural Science Foundation of China; Open Project Program of
   State Key Laboratory of Virtual Reality Technology and Systems, Beihang
   University [VRLAB2023B01];  [62077037];  [62376231,62365014]; 
   [U22B2034];  [62262043];  [62171321];  [62162044];  [62162414]
FX This work was supported in part by the National Natural Science
   Foundation of China (62077037, 62376231,62365014, U22B2034, 62262043,
   62171321, 62162044 and 62162414) and in part by the Open Project Program
   of State Key Laboratory of Virtual Reality Technology and Systems,
   Beihang University (No. VRLAB2023B01).
CR Aharon N, 2022, Arxiv, DOI [arXiv:2206.14651, DOI 10.48550/ARXIV.2206.14651]
   Bar -Shalom Y., 1990, Tracking and data association
   Baysal S, 2016, IEEE T CIRC SYST VID, V26, P1350, DOI 10.1109/TCSVT.2015.2455713
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Ben Shitrit H, 2014, IEEE T PATTERN ANAL, V36, P1614, DOI 10.1109/TPAMI.2013.210
   Ben Shitrit H, 2011, IEEE I CONF COMP VIS, P137, DOI 10.1109/ICCV.2011.6126235
   BenShitrit H., 2013, Technical report
   Bishop G., 2001, An introduction to the kalman filter, V8, P41
   Bu J, 2011, IEEE INT CON MULTI
   Cai JR, 2022, PROC CVPR IEEE, P8080, DOI 10.1109/CVPR52688.2022.00792
   Carreira-Perpi¤an MA, 2015, Arxiv, DOI [arXiv:1503.00687, 10.48550/arXiv.1503.00687]
   CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568
   Cherkassky V, 2004, NEURAL NETWORKS, V17, P113, DOI 10.1016/S0893-6080(03)00169-2
   Chu P, 2023, IEEE WINT CONF APPL, P4859, DOI 10.1109/WACV56688.2023.00485
   Chu YJ, 2022, IEEE COMPUT SOC CONF, P3522, DOI 10.1109/CVPRW56347.2022.00396
   Cioppa A, 2022, IEEE COMPUT SOC CONF, P3490, DOI 10.1109/CVPRW56347.2022.00393
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Comaniciu D., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1197, DOI 10.1109/ICCV.1999.790416
   Cuevas C, 2020, MULTIMED TOOLS APPL, V79, P29685, DOI 10.1007/s11042-020-09409-0
   Cuevas C, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107278
   Duh D.J., 2013, Intelligent Technologies and Engineering Systems, P123
   Fashing M, 2005, IEEE T PATTERN ANAL, V27, P471, DOI 10.1109/TPAMI.2005.59
   Feng N, 2020, MULTIMED TOOLS APPL, V79, P28971, DOI 10.1007/s11042-020-09414-3
   Girbau A., 2022, arXiv
   He X, 2022, SOFT COMPUT, V26, P10971, DOI 10.1007/s00500-022-07295-2
   Herrmann M, 2014, AASRI PROC, V8, P30, DOI 10.1016/j.aasri.2014.08.006
   Homayounfar N, 2017, PROC CVPR IEEE, P4012, DOI 10.1109/CVPR.2017.427
   Hurault Samuel, 2020, MMSports '20: Proceedings of the 3rd International Workshop on Multimedia Content Analysis in Sports, P9, DOI 10.1145/3422844.3423054
   Hyun J, 2023, IEEE WINT CONF APPL, P4839, DOI 10.1109/WACV56688.2023.00483
   Isard M., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P343, DOI 10.1007/BFb0015549
   Kamble PR, 2019, OPTO-ELECTRON REV, V27, P58, DOI 10.1016/j.opelre.2019.02.003
   Kim JY, 2009, I C COMP GRAPH IM VI, P367, DOI 10.1109/CGIV.2009.87
   Kim W, 2019, J VIS COMMUN IMAGE R, V65, DOI 10.1016/j.jvcir.2019.102683
   Kim W, 2018, MULTIMEDIA SYST, V24, P611, DOI 10.1007/s00530-018-0586-9
   Komorowski J, 2020, Arxiv, DOI arXiv:1912.05445
   Kristan M, 2009, COMPUT VIS IMAGE UND, V113, P598, DOI 10.1016/j.cviu.2008.01.009
   Lee J, 2020, I C INF COMM TECH CO, P1161, DOI 10.1109/ICTC49870.2020.9289223
   Li HP, 2012, INT CONF ACOUST SPEE, P1001, DOI 10.1109/ICASSP.2012.6288054
   Li JX, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22155863
   Li M, 2005, CLIN LINGUIST PHONET, V19, P545, DOI 10.1080/02699200500113616
   Li PH, 2003, IMAGE VISION COMPUT, V21, P111, DOI 10.1016/S0262-8856(02)00133-6
   Li Q, 2015, 2015 8TH INTERNATIONAL CONFERENCE ON INTELLIGENT NETWORKS AND INTELLIGENT SYSTEMS (ICINIS), P74, DOI 10.1109/ICINIS.2015.35
   Liu JC, 2013, PROC CVPR IEEE, P1830, DOI 10.1109/CVPR.2013.239
   Liu QK, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P530
   Liu S, 2022, PROC CVPR IEEE, P8866, DOI 10.1109/CVPR52688.2022.00867
   Liu ZL, 2023, Arxiv, DOI arXiv:2306.05238
   Lu K., 2017, arXiv
   Lu WL, 2013, IEEE T PATTERN ANAL, V35, P1704, DOI 10.1109/TPAMI.2012.242
   Luiten J, 2021, INT J COMPUT VISION, V129, P548, DOI 10.1007/s11263-020-01375-2
   Maggiolino G, 2023, Arxiv, DOI arXiv:2302.11813
   Manafifard M, 2017, COMPUT VIS IMAGE UND, V159, P19, DOI 10.1016/j.cviu.2017.02.002
   Martín R, 2014, MULTIMED TOOLS APPL, V73, P1617, DOI 10.1007/s11042-013-1659-6
   Meinhardt T, 2022, PROC CVPR IEEE, P8834, DOI 10.1109/CVPR52688.2022.00864
   MEINHOLD RJ, 1983, AM STAT, V37, P123, DOI 10.2307/2685871
   Misu T, 2002, INT C PATT RECOG, P556, DOI 10.1109/ICPR.2002.1044792
   Morais E., 2012, 2012 XXV SIBGRAPI - Conference on Graphics, Patterns and Images (SIBGRAPI 2012), P174, DOI 10.1109/SIBGRAPI.2012.32
   Morais E, 2014, PATTERN RECOGN LETT, V39, P21, DOI 10.1016/j.patrec.2013.09.007
   Mori G, 2005, IEEE T PATTERN ANAL, V27, P1832, DOI 10.1109/TPAMI.2005.220
   Naik BT, 2022, IEEE ACCESS, V10, P32494, DOI 10.1109/ACCESS.2022.3161441
   Najafzadeh N, 2015, 2015 INTERNATIONAL SYMPOSIUM ON ARTIFICIAL INTELLIGENCE AND SIGNAL PROCESSING (AISP), P310, DOI 10.1109/AISP.2015.7123503
   Najeeb Huda Dheyauldeen, 2020, 2020 International Conference on Computer Science and Software Engineering (CSASE). Proceedings, P78, DOI 10.1109/CSASE48920.2020.9142058
   Najeeb H. D., 2021, Muthanna J. Pure Sci., V8, P1, DOI [DOI 10.52113/2/08.01.2021/1-13, 10.52113/2/08.01.2021/1-13]
   Pang JM, 2021, PROC CVPR IEEE, P164, DOI 10.1109/CVPR46437.2021.00023
   Sabirin H, 2015, IEICE T INF SYST, VE98D, P1580, DOI 10.1587/transinf.2014EDP7313
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Scott A, 2022, IEEE COMPUT SOC CONF, P3568, DOI 10.1109/CVPRW56347.2022.00401
   Sha L., 2020, P IEEE CVF C COMP VI, P13627
   STREIT RL, 1994, P SOC PHOTO-OPT INS, V2235, P394, DOI 10.1117/12.179066
   Sverrisson S, 2019, LECT NOTES COMPUT SC, V11482, P399, DOI 10.1007/978-3-030-20205-7_33
   Theiner J., 2022, P IEEE CVF WINT C AP, P823
   Vandeghen R, 2022, IEEE COMPUT SOC CONF, P3480, DOI 10.1109/CVPRW56347.2022.00392
   Veltkamp RC, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P188, DOI 10.1109/SMA.2001.923389
   Vishwanathan S, 2002, IEEE IJCNN, P2393, DOI 10.1109/IJCNN.2002.1007516
   Wang HF, 2005, PROCEEDINGS OF THE 2005 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS AND BRAIN, VOLS 1-3, P279
   Wang Q, 2021, PROC CVPR IEEE, P3875, DOI 10.1109/CVPR46437.2021.00387
   Wang YX, 2021, Arxiv, DOI arXiv:2006.13164
   Welch G. F., 2021, Computer Vision: A ReferenceGuide, P1, DOI [10.1007/978-3-030-03243-2_716-1, DOI 10.1007/978-3-030-03243-2_716-1]
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Xing JL, 2011, IEEE T IMAGE PROCESS, V20, P1652, DOI 10.1109/TIP.2010.2102045
   Xuefeng Bai, 2011, 2011 Seventh International Conference on Intelligent Information Hiding and Multimedia Signal Processing, P356, DOI 10.1109/IIHMSP.2011.84
   Yang Y, 2017, J VIS COMMUN IMAGE R, V46, P81, DOI 10.1016/j.jvcir.2017.03.008
   Yu JQ, 2018, IEEE 1ST CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2018), P418, DOI 10.1109/MIPR.2018.00090
   Yunhao D., 2023, IEEE Trans. Multimed, V5, P55, DOI [10.1109/TMM.2023.3240881, DOI 10.1109/TMM.2023.3240881]
   Zhang RH, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107260
   Zhang YF, 2022, LECT NOTES COMPUT SC, V13682, P1, DOI 10.1007/978-3-031-20047-2_1
   Zhang YF, 2021, INT J COMPUT VISION, V129, P3069, DOI 10.1007/s11263-021-01513-4
   Zheng B, 2022, MOB INF SYST, V2022, DOI 10.1155/2022/8090871
NR 87
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 8
PY 2024
DI 10.1007/s00371-024-03367-6
EA APR 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ND1M4
UT WOS:001198420700001
DA 2024-08-05
ER

PT J
AU Shen, IC
   Su, LW
   Wu, YT
   Chen, BY
AF Shen, I. -Chao
   Su, Li-Wen
   Wu, Yu-Ting
   Chen, Bing-Yu
TI StylePart: image-based shape part manipulation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image editing; Image generative model; 3D shape generative model; Latent
   vector mapping function
AB Direct part-level manipulation of man-made shapes in an image is desired given its simplicity. However, it is not intuitive given the existing manually created cuboid and cylinder controllers. To tackle this problem, we present StylePart, a framework that enables direct shape manipulation of an image by leveraging generative models of both images and 3D shapes. Our key contribution is a shape-consistent latent mapping function that connects the image generative latent space and the 3D man-made shape attribute latent space. Our method "forwardly maps" the image content to its corresponding 3D shape attributes, where the shape part can be easily manipulated. The attribute codes of the manipulated 3D shape are then "backwardly mapped" to the image latent code to obtain the final manipulated image. By using both forward and backward mapping, an user can edit the image directly without resorting to any 3D workflow. We demonstrate our approach through various manipulation tasks, including part replacement, part resizing, and shape orientation manipulation, and evaluate its effectiveness through extensive ablation studies.
C1 [Shen, I. -Chao] Univ Tokyo, Dept Comp Sci, Tokyo, Japan.
   [Su, Li-Wen; Chen, Bing-Yu] Natl Taiwan Univ, Dept Comp Sci, Taipei, Taiwan.
   [Wu, Yu-Ting] Natl Taipei Univ Educ, Comp Sci Dept, Taipei, Taiwan.
C3 University of Tokyo; National Taiwan University; National Taipei
   University of Education
RP Shen, IC (corresponding author), Univ Tokyo, Dept Comp Sci, Tokyo, Japan.
EM ichaoshen@g.ecc.u-tokyo.ac.jp; susan31213@gmail.com;
   yutingwu@mail.ntpu.edu.tw; robin@ntu.edu.tw
FU National Research Foundation Singapore [JP23K16921]; JSPS
   [NSTC111-2634-F-002-022, 111-2221-E-002-145-MY3, 111-3111-E-002-002,
   112-2218-E-002-029, 111-2222-E-305-001-MY2]; National Science and
   Technology Council [NTU112L900902]; National Taiwan University
FX We thank the anonymous reviewers for their valuable comments. This work
   was supported in part by JSPS Grant-in-Aid JP23K16921, and National
   Science and Technology Council, under Grant, NSTC111-2634-F-002-022,
   111-2221-E-002-145-MY3, 111-3111-E-002-002, 112-2218-E-002-029, and
   111-2222-E-305-001-MY2 and National Taiwan University (NTU112L900902).
CR Abdal R., 2020, P IEEECVF C COMPUTER, P8296
   Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Alaluf Yuval, 2021, arXiv
   Chen AP, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3470848
   Chen T, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508378
   Creswell A, 2019, IEEE T NEUR NET LEAR, V30, P1967, DOI 10.1109/TNNLS.2018.2875194
   Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Feng Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459936
   Gal R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531339
   Gao L, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356488
   Garon M, 2019, PROC CVPR IEEE, P6901, DOI 10.1109/CVPR.2019.00707
   GeorgiaGkioxari J.J., 2019, P IEEE INT C COMPUTE
   Goetschalckx L, 2019, Arxiv, DOI arXiv:1906.10112
   Gu JJ, 2020, PROC CVPR IEEE, P3009, DOI 10.1109/CVPR42600.2020.00308
   Guan SY, 2020, Arxiv, DOI arXiv:2007.01758
   Härkönen E, 2020, Arxiv, DOI arXiv:2004.02546
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hin TCL., 2021, Comput. Gr. Forum, DOI [10.1111/cgf.14188, DOI 10.1111/CGF.14188]
   Hu R., 2022, ACM Trans. Gr. Proc. SIGGRAPH, V39, P113
   Jahanian A., 2019, arXiv
   Jahanian Ali, 2020, INT C LEARN REPR
   Jiang CK, 2019, IEEE ICC
   Jiapeng Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P592, DOI 10.1007/978-3-030-58520-4_35
   Jie Lei, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P447, DOI 10.1007/978-3-030-58589-1_27
   Karras T., 2020, P IEEE CVF C COMP VI, P8110, DOI [10.1109/cvpr42600.2020.00813, DOI 10.1109/CVPR42600.2020.00813]
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kholgade Natasha, 2014, ACM Transactions on Graphics, V33, DOI 10.1145/2601097.2601209
   Kingma D. P., 2014, arXiv
   Lehtinen J., 2020, PROC NEURIPS, V33, P12104
   Ling H., 2021, Advances in Neural Information Processing Systems (NIPS), P16331
   Liu GL, 2017, IEEE I CONF COMP VIS, P2280, DOI 10.1109/ICCV.2017.248
   Liu Zongdai, 2020, P IEEECVF C COMPUTER, P11336
   Mao AH, 2020, Arxiv, DOI arXiv:2003.03551
   Minyoung Huh, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P17, DOI 10.1007/978-3-030-58536-5_2
   Mo KC, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356527
   Paszke A, 2019, ADV NEUR IN, V32
   Plumerault A, 2020, Arxiv, DOI [arXiv:2001.10238, 10.48550/arXiv.2001.10238]
   Shen Y., 2020, P IEEECVF C COMPUTER, P9243
   Shen Yujun, 2021, CVPR
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Song C, 2020, PROC CVPR IEEE, P428, DOI 10.1109/CVPR42600.2020.00051
   Spingarn-Eliezer N., 2021, arXiv
   SUZUKI S, 1985, COMPUT VISION GRAPH, V30, P32, DOI 10.1016/0734-189X(85)90016-7
   Tewari A, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417803
   Tewari A, 2020, PROC CVPR IEEE, P6141, DOI 10.1109/CVPR42600.2020.00618
   Tewari A, 2017, IEEE INT CONF COMP V, P1274, DOI 10.1109/ICCVW.2017.153
   Tov O, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459838
   Wang B., 2021, arXiv
   Wang C, 2022, Arxiv, DOI arXiv:2104.11228
   Wu J., 2017, Advances in Neural Information Processing Systems (NeurIPS)
   Wu ZZ, 2021, PROC CVPR IEEE, P12858, DOI 10.1109/CVPR46437.2021.01267
   Yan XC, 2016, ADV NEUR IN, V29
   Yu A, 2021, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR46437.2021.00455
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zheng YY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185595
   Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36
   Zhu YJ, 2021, PROC CVPR IEEE, P12829, DOI 10.1109/CVPR46437.2021.01264
NR 58
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 2
PY 2024
DI 10.1007/s00371-024-03310-9
EA APR 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MT0M2
UT WOS:001195768300002
OA Green Submitted, hybrid
DA 2024-08-05
ER

PT J
AU Lei, XM
   Wen, XW
   Li, Z
AF Lei, Xuemei
   Wen, Xiaowei
   Li, Zheng
TI A multi-target cow face detection model in complex scenes
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Convolutional neural network; Cow face detection; Multi-target; Complex
   scenes
ID IDENTIFICATION; RECOGNITION
AB The development of intelligent agriculture has accelerated the automation and scale of cattle farming. Recognizing cattle faces as a significant biological characteristic is crucial for accurate reproduction and health tracking. We propose a multi-target cow face detection model (MT-CF-DM) in complex scenes based on the YOLOv7 framework. The backbone of our proposal model consists of the GhostNet and the CBAM (convolutional block attention module) attention mechanism to improve the model's perception of various scales and features. Additionally, we propose three novel modules of the neck network: SPPFCSPC (Spatial Pyramid Pooling-Fast Cross-Stage Partial Connection), a novel BiFPN, and C2f (CSP Bottleneck with 2 convolutions). SPPFCSPC is to reduce the number of model parameters. New BiFPN is to utilize feature information and improve the model's detection capability. C2f is to maintain the network's lightweight nature and obtain more comprehensive gradient flow data. Moreover, a method for establishing cow face datasets is proposed based on the MT-CF-DM. During the testing phase, our model combines with a cropping module to simultaneously acquire a dataset for cow faces. The experimental results show the parameter of the proposal model is 15.8 M, the model size is 22.3 M, GFLOPs are 25.8, FPS is 97.6, mAP is 98.5%, precision is 98.8%, recall is 97.3%. Compared to YOLOv7, our proposed model reduces parameters by 56.8% and has a 48.3 increase in FPS. The model demonstrates high accuracy and small number of parameters for detecting cow faces in complex environments, suitable for edge computing system applications.
C1 [Lei, Xuemei; Wen, Xiaowei] Inner Mongolia Univ, Sch Elect Informat Engn, Hohhot 010020, Inner Mongolia, Peoples R China.
   [Li, Zheng] Inner Mongolia Univ Sci & Technol, Sch Informat Engn, Baotou 014000, Inner Mongolia, Peoples R China.
C3 Inner Mongolia University; Inner Mongolia University of Science &
   Technology
RP Lei, XM (corresponding author), Inner Mongolia Univ, Sch Elect Informat Engn, Hohhot 010020, Inner Mongolia, Peoples R China.
EM ndlxm@imu.edu.cn; 17648244024@163.com; lizheng289@163.com
CR Chen LJ, 2023, ENG STRUCT, V275, DOI 10.1016/j.engstruct.2022.115291
   Chen XN, 2022, J SYST ARCHITECT, V124, DOI 10.1016/j.sysarc.2022.102394
   Cheng M, 2022, COMPUT ELECTRON AGR, V198, DOI 10.1016/j.compag.2022.107010
   Daum T, 2022, AGR SYST, V196, DOI 10.1016/j.agsy.2021.103353
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Gu ZS, 2023, COMPUT ELECTRON AGR, V212, DOI 10.1016/j.compag.2023.108143
   Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165
   Hao WL, 2023, EXPERT SYST APPL, V230, DOI 10.1016/j.eswa.2023.120551
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Hongyu Wang, 2020, Journal of Physics: Conference Series, V1453, DOI 10.1088/1742-6596/1453/1/012054
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hwang JW, 2023, APPL ACOUST, V211, DOI 10.1016/j.apacoust.2023.109478
   Jiang M, 2020, COMPUT ELECTRON AGR, V177, DOI 10.1016/j.compag.2020.105706
   Li CY, 2022, Arxiv, DOI [arXiv:2209.02976, DOI 10.48550/ARXIV.2209.02976]
   Li H., 2022, Slim-neck by GSConv: A Better Design Paradigm of Detector Architectures for Autonomous Vehicles
   Li J, 2023, COMPUT ELECTRON AGR, V211, DOI 10.1016/j.compag.2023.107955
   Li R, 2023, APPL ENERG, V348, DOI 10.1016/j.apenergy.2023.121529
   Li XP, 2023, COMPUT ELECTRON AGR, V205, DOI 10.1016/j.compag.2023.107651
   Li Z, 2022, COMPUT ELECTRON AGR, V195, DOI 10.1016/j.compag.2022.106848
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZS, 2021, CMC-COMPUT MATER CON, V66, P457, DOI 10.32604/cmc.2020.012165
   Ma C., 2023, Inf. Process. Agric, P1, DOI [10.1016/j.inpa.2023.03.004, DOI 10.1016/J.INPA.2023.03.004]
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Mahmud MS, 2021, COMPUT ELECTRON AGR, V187, DOI 10.1016/j.compag.2021.106313
   Noor A, 2020, COMPUT ELECTRON AGR, V175, DOI 10.1016/j.compag.2020.105528
   O'Neill CJ, 2017, BIOSYST ENG, V162, P140, DOI 10.1016/j.biosystemseng.2017.07.007
   Parikoglou I, 2022, AGR ECON-BLACKWELL, V53, P109, DOI 10.1111/agec.12729
   Piña R, 2023, SMART AGR TECHNOL, V5, DOI 10.1016/j.atech.2023.100235
   Qiao YL, 2023, COMPUT ELECTRON AGR, V204, DOI 10.1016/j.compag.2022.107579
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shaikh TA, 2022, COMPUT ELECTRON AGR, V198, DOI 10.1016/j.compag.2022.107119
   Sun SN, 2013, NEUROCOMPUTING, V120, P310, DOI 10.1016/j.neucom.2012.08.068
   Tan R., 2020, P IEEE CVF C COMP VI, DOI [10.1109/CVPR42600.2020.01079, DOI 10.1109/CVPR42600.2020.01079]
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang SY, 2023, ENG APPL ARTIF INTEL, V117, DOI 10.1016/j.engappai.2022.105504
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2022, ECOL INFORM, V72, DOI 10.1016/j.ecoinf.2022.101886
   WOO S, 2018, P EUR C COMP VIS ECC, DOI DOI 10.1007/978-3-030-01234-2_1
   Wu L, 2021, AGRONOMY-BASEL, V11, DOI 10.3390/agronomy11030476
   Xiao JX, 2022, COMPUT ELECTRON AGR, V194, DOI 10.1016/j.compag.2022.106738
   Xu BB, 2022, COMPUT ELECTRON AGR, V193, DOI 10.1016/j.compag.2021.106675
   Xu LJ, 2023, COMPUT ELECTRON AGR, V205, DOI 10.1016/j.compag.2022.107590
   Yan HW, 2020, INMATEH-AGRIC ENG, V61, P97, DOI 10.35633/inmateh-61-11
   Yang WJ, 2023, COMPUT ELECTRON AGR, V211, DOI 10.1016/j.compag.2023.108006
   Yokoi K, 2023, PARKINSONISM RELAT D, V113, DOI 10.1016/j.parkreldis.2023.105411
   Yuan CC, 2023, CROP PROT, V172, DOI 10.1016/j.cropro.2023.106342
   Zhang F, 2022, AGRICULTURE-BASEL, V12, DOI 10.3390/agriculture12101668
   Zhang P, 2022, COMPUT ELECTRON AGR, V203, DOI 10.1016/j.compag.2022.107491
   Zhang Q, 2022, RESOUR CONSERV RECY, V181, DOI 10.1016/j.resconrec.2022.106235
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zheng ZY, 2023, COMPUT ELECTRON AGR, V209, DOI 10.1016/j.compag.2023.107857
NR 59
TC 0
Z9 0
U1 18
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 8
PY 2024
DI 10.1007/s00371-024-03301-w
EA MAR 2024
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KA8J0
UT WOS:001177328200002
DA 2024-08-05
ER

PT J
AU Yang, Q
   Weng, DD
   Liu, Y
AF Yang, Qing
   Weng, Dongdong
   Liu, Yue
TI Utilizing periodic feature-enhanced neural-field modeling for the
   photorealistic representation of human head avatars
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Implicit model; Neural fields; 3D model; Neural representations
AB This paper introduces a groundbreaking neural representation technique known as periodic feature-enhanced neural-field modeling (PNM) tailored for 3D models. PNM has been meticulously crafted to proficiently capture the intricate surface geometry of 4D human avatars, yielding a multitude of applications across various domains, including remote conferencing, livestream marketing, short video webcasting, VR/AR/XR applications, and the video game and movie industry. While current neural modeling approaches excel in representing either low-frequency or high-frequency surface details, they often fall short when simultaneously addressing both aspects, leading to subpar overall model quality. To overcome this limitation, PNM leverages position encoding and periodic activation, harnessing its Fourier properties to deliver enhanced representation of high-frequency data, all the while preserving smooth, noise-free surfaces. Our experiments substantiate PNM's superiority, surpassing state-of-the-art methods in terms of both quantitative and qualitative model reconstruction quality and the portrayal of high-frequency geometry details. Finally, we apply PNM in the digitization pipeline for 4D human avatars and Metaverse applications, demonstrating its remarkable visual performance in dynamic scenarios.
C1 [Yang, Qing; Weng, Dongdong; Liu, Yue] Beijing Inst Technol, Beijing Engn Res Ctr Mixed Real & Adv Display, Sch Opt & Photon, Beijing 100081, Peoples R China.
   [Yang, Qing] Intel China Ltd, Beijing, Peoples R China.
C3 Beijing Institute of Technology; Intel Corporation
RP Weng, DD (corresponding author), Beijing Inst Technol, Beijing Engn Res Ctr Mixed Real & Adv Display, Sch Opt & Photon, Beijing 100081, Peoples R China.
EM charles.q.yang@gmail.com; crgj@bit.edu.cn; liuyue@bit.edu.cn
FU Strategic research and consulting project of Chinese Academy of
   Engineering
FX No Statement Available
CR [Anonymous], 2023, 3D model website
   Atzmon M, 2020, PROC CVPR IEEE, P2562, DOI 10.1109/CVPR42600.2020.00264
   Barron JT, 2022, PROC CVPR IEEE, P5460, DOI 10.1109/CVPR52688.2022.00539
   Barron JT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5835, DOI 10.1109/ICCV48922.2021.00580
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Chan ER, 2022, PROC CVPR IEEE, P16102, DOI 10.1109/CVPR52688.2022.01565
   Egger B, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3395208
   Gafni G, 2021, PROC CVPR IEEE, P8645, DOI 10.1109/CVPR46437.2021.00854
   Gao X, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555501
   Grassal PW, 2022, PROC CVPR IEEE, P18632, DOI 10.1109/CVPR52688.2022.01810
   Gropp A, 2020, Arxiv, DOI arXiv:2002.10099
   Hong Y, 2022, PROC CVPR IEEE, P20342, DOI 10.1109/CVPR52688.2022.01973
   Lee JY, 2022, Arxiv, DOI arXiv:2212.00914
   Li TY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130813
   Liu AJ, 2021, IEEE INT CONF COMP V, P814, DOI 10.1109/ICCVW54120.2021.00096
   Martin-Brualla R, 2021, PROC CVPR IEEE, P7206, DOI 10.1109/CVPR46437.2021.00713
   Mehta I, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14194, DOI 10.1109/ICCV48922.2021.01395
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123
   Pumarola A, 2021, PROC CVPR IEEE, P10313, DOI 10.1109/CVPR46437.2021.01018
   Rebain D, 2021, Arxiv, DOI [arXiv:2106.03804, 10.48550/arXiv.2106.03804, DOI 10.48550/ARXIV.2106.03804]
   Sitzmann V, 2020, Adv. Neural. Inf. Process. Syst, V33, P7462
   Sitzmann V., 2021, Adv. Neural. Inf. Process. Syst, V34, P1931319325
   Tewari A, 2022, COMPUT GRAPH FORUM, V41, P703, DOI 10.1111/cgf.14507
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Xu YL, 2023, Arxiv, DOI arXiv:2211.13206
   Yang Qing, 2022, 2022 IEEE 24th Int Conf on High Performance Computing & Communications; 8th Int Conf on Data Science & Systems; 20th Int Conf on Smart City; 8th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys), P2227, DOI 10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00329
   Yang Q., 2020, P IEEECVF C COMPUTER, P644, DOI 10.1109/cvprw50498.2020.00330
   Yang Q, 2021, INT C PATT RECOG, P1376, DOI 10.1109/ICPR48806.2021.9413046
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zheng YF, 2022, Arxiv, DOI arXiv:2212.08377
   Zheng YF, 2022, PROC CVPR IEEE, P13535, DOI 10.1109/CVPR52688.2022.01318
   Zielonka W, 2023, Arxiv, DOI arXiv:2211.12499
NR 35
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 28
PY 2024
DI 10.1007/s00371-024-03299-1
EA FEB 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JF9E4
UT WOS:001171862300001
DA 2024-08-05
ER

PT J
AU Shao, MW
   Bao, ZY
   Liu, WH
   Qiao, YJ
   Wan, YC
AF Shao, Mingwen
   Bao, Zhiyuan
   Liu, Weihan
   Qiao, Yuanjian
   Wan, Yecong
TI Frequency domain-enhanced transformer for single image deraining
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image deraining; Frequency domain; Self-attention; Dual
   domain-complemented
ID RAINDROP REMOVAL; ADVERSARIAL; ATTENTION; NETWORK; MODEL
AB Since Transformers show a strong capability of building long-range dependencies, the relevant methods are extensively employed for image deraining tasks. However, the intrinsic limitations of Transformers, including costly computational complexity and insufficient ability to capture high-frequency components of the image, hinder the the utilization of Transformers in high-resolution images and lead to the unsatisfactory recovery of local edges and textures. To overcome these limitations, we propose an simple but effective Frequency Domain Enhanced Transformer (FDEFormer) for the image deraining. Firstly, drawing inspiration from the convolution theorem, we devise an efficient approach called frequency domain enhanced multi-head self-attention. The proposed approach replaces traditional matrix multiplication with element-wise product operations in the frequency domain, leading to a substantial reduction in computational complexity. Secondly, the existing spatial domain Transformers-based methods only focus on low-frequency features but pay less attention to high-frequency components, which can adversely affect the quality of the reconstructed images. Therefore, to integrate the content of different frequency levels, we propose a dual domain-complemented feed-forward network. Besides, we further present an attention feature fusion module to facilitate a more effective fusion of features across different layers. Extensive experiments on several datasets demonstrate that our FDEFormer performs favorably against state-of-the-art methods while taking acceptable computational costs. The source code and pre-trained models are available at https://github.com/bobozy1999/FDEFormer.
C1 [Shao, Mingwen] Quanzhou Vocat & Tech Univ, Natl Sci Digital Ind Coll, Jinjiang 362000, Peoples R China.
   [Shao, Mingwen; Bao, Zhiyuan; Liu, Weihan; Qiao, Yuanjian; Wan, Yecong] China Univ Petr East China, Coll Comp Sci & Technol, Qingdao 266000, Shandong, Peoples R China.
C3 China University of Petroleum
RP Shao, MW (corresponding author), Quanzhou Vocat & Tech Univ, Natl Sci Digital Ind Coll, Jinjiang 362000, Peoples R China.; Shao, MW (corresponding author), China Univ Petr East China, Coll Comp Sci & Technol, Qingdao 266000, Shandong, Peoples R China.
EM smw278@126.com; zhiyuanbao@s.upc.edu.cn; liudaneng110@126.com;
   yjqiao@s.upc.edu.cn; yecongwan@gmail.com
FU National Key Research and development Program of China [2021YFA1000102];
   National Natural Science Foundation of China [62376285, 62272375,
   61673396]; Natural Science Foundation of Shandong Province, China
   [ZR2022MF260]
FX The authors are very indebted to the anonymous referees for their
   critical comments and suggestions for the improvement of this paper.
   This work was supported by National Key Research and development Program
   of China (2021YFA1000102), and in part by the grants from the National
   Natural Science Foundation of China (Nos. 62376285, 62272375, 61673396),
   Natural Science Foundation of Shandong Province, China (No.
   ZR2022MF260).
CR Blackledge J.M., 2005, Digital Image Processing: Mathematical and Computational Methods, P44
   Cao JM, 2022, IEEE T IMAGE PROCESS, V31, P3726, DOI 10.1109/TIP.2022.3175432
   Chen DY, 2014, IEEE T CIRC SYST VID, V24, P1430, DOI 10.1109/TCSVT.2014.2308627
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen HY, 2023, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR52729.2023.00169
   Chen LY, 2021, IEEE COMPUT SOC CONF, P182, DOI 10.1109/CVPRW53098.2021.00027
   Chen MM, 2023, VISUAL COMPUT, V39, P3727, DOI 10.1007/s00371-023-02947-2
   Chen X, 2022, PROC CVPR IEEE, P2007, DOI 10.1109/CVPR52688.2022.00206
   Deng LJ, 2018, APPL MATH MODEL, V59, P662, DOI 10.1016/j.apm.2018.03.001
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Du YJ, 2020, IEEE T IMAGE PROCESS, V29, P6288, DOI 10.1109/TIP.2020.2990606
   Fu XY, 2023, IEEE T PATTERN ANAL, V45, P9534, DOI 10.1109/TPAMI.2023.3241756
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802
   Huang HB, 2021, PROC CVPR IEEE, P7728, DOI 10.1109/CVPR46437.2021.00764
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang TX, 2019, IEEE T IMAGE PROCESS, V28, P2089, DOI 10.1109/TIP.2018.2880512
   Kamgar-Parsi B, 1999, IEEE T IMAGE PROCESS, V8, P1467, DOI 10.1109/83.791975
   Kingma D.P., 2014, arXiv
   Kong LS, 2023, PROC CVPR IEEE, P5886, DOI 10.1109/CVPR52729.2023.00570
   Kui Jiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8343, DOI 10.1109/CVPR42600.2020.00837
   Li BY, 2022, PROC CVPR IEEE, P17431, DOI 10.1109/CVPR52688.2022.01693
   Li RT, 2019, PROC CVPR IEEE, P1633, DOI 10.1109/CVPR.2019.00173
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Li YW, 2023, PROC CVPR IEEE, P18278, DOI 10.1109/CVPR52729.2023.01753
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Lin X, 2021, IEEE T MULTIMEDIA, V23, P664, DOI 10.1109/TMM.2020.2987703
   Liu X, 2019, PROC CVPR IEEE, P7000, DOI 10.1109/CVPR.2019.00717
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Loshchilov I, 2017, Sgdr: Stochastic gradient descent with warm restarts, DOI [10.48550/arXiv.1608.03983, DOI 10.48550/ARXIV.1608.03983]
   Luo Y, 2022, VISUAL COMPUT, V38, P3109, DOI 10.1007/s00371-022-02567-2
   Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388
   Peng JY, 2020, PATTERN RECOGN LETT, V131, P121, DOI 10.1016/j.patrec.2019.12.012
   Purohit K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2289, DOI 10.1109/ICCV48922.2021.00231
   Qian R, 2018, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR.2018.00263
   Quan RJ, 2021, PROC CVPR IEEE, P9143, DOI 10.1109/CVPR46437.2021.00903
   Quan YH, 2019, IEEE I CONF COMP VIS, P2463, DOI 10.1109/ICCV.2019.00255
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Shao MW, 2021, IEEE T IMAGE PROCESS, V30, P4828, DOI 10.1109/TIP.2021.3076283
   Shao MW, 2023, KNOWL-BASED SYST, V263, DOI 10.1016/j.knosys.2023.110306
   Sun H, 2019, IEEE INT C INT ROBOT, P962, DOI [10.1109/iros40897.2019.8967644, 10.1109/IROS40897.2019.8967644]
   Tu ZZ, 2022, PROC CVPR IEEE, P5759, DOI 10.1109/CVPR52688.2022.00568
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan YC, 2023, Arxiv, DOI arXiv:2305.09996
   Wan YC, 2023, PATTERN ANAL APPL, V26, P1527, DOI 10.1007/s10044-023-01184-6
   Wan YC, 2022, KNOWL-BASED SYST, V252, DOI 10.1016/j.knosys.2022.109244
   Wang W, 2022, IEEE T IMAGE PROCESS, V31, P1949, DOI 10.1109/TIP.2022.3146017
   Wang YL, 2017, IEEE T IMAGE PROCESS, V26, P3936, DOI 10.1109/TIP.2017.2708502
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Wei W, 2019, PROC CVPR IEEE, P3872, DOI 10.1109/CVPR.2019.00400
   Xianhui Zheng, 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P258, DOI 10.1007/978-3-642-42051-1_33
   Xiao J, 2023, IEEE T PATTERN ANAL, V45, P12978, DOI 10.1109/TPAMI.2022.3183612
   Xu, 2012, 2012 IEEE INT C COMP, V2, P304
   Yang H, 2023, VISUAL COMPUT, V39, P3887, DOI 10.1007/s00371-022-02533-y
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Yasarla R, 2020, PROC CVPR IEEE, P2723, DOI 10.1109/CVPR42600.2020.00280
   Yasarla R, 2019, PROC CVPR IEEE, P8397, DOI 10.1109/CVPR.2019.00860
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhu L, 2017, IEEE I CONF COMP VIS, P2545, DOI 10.1109/ICCV.2017.276
NR 65
TC 0
Z9 0
U1 8
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 14
PY 2024
DI 10.1007/s00371-023-03252-8
EA FEB 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HR9E7
UT WOS:001161343100001
DA 2024-08-05
ER

PT J
AU Liu, H
   Li, L
   Yu, N
   Ma, K
   Peng, T
   Hu, XR
AF Liu, Hong
   Li, Li
   Yu, Neng
   Ma, Kai
   Peng, Tao
   Hu, Xinrong
TI Outfit compatibility model using fully connected self-adjusting graph
   neural network
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Outfit compatibility model; Graph neural networks; Representation
   learning
AB Outfit compatibility modeling is an increasingly important task that has garnered much attention from researchers, and graph neural network (GNN)-based methods have become the mainstream approach to address this task. Despite significant progress achieved by existing research, most of them have overlooked the importance of low-order connectivity in graph data and the relationship between individual fashion items and overall outfit style. To address this issue, we propose an outfit compatibility modeling scheme based on outfit-level relationships. This scheme consists of two key components: FCSA-GNN and GOR. The former consists of multiple layers of Fashion Items Relationship Propagation, where the weights of the graph edges are adapted based on the Category Co-occurrence Matrix. This approach avoids potential imbalances in the quantities of different fashion item categories within the dataset. Subsequently, by connecting the outputs of GNN at each layer, it explores the relationships between fashion item visual signals at different levels. The latter integrates the overall outfits style and further enhances the model's ability to learn a comprehensive representation of the overall outfits through the proposed outfit-level relationships. Experimental results on two real datasets, Polyvore outfit-ND and Polyvore outfit-D, demonstrate that our method outperforms existing state-of-the-art methods when considering only visual information.
C1 [Liu, Hong; Peng, Tao; Hu, Xinrong] Wuhan Text Univ, Dept Comp Sci, 1 Sunshine Ave, Wuhan 430200, Hubei, Peoples R China.
   [Li, Li; Yu, Neng; Ma, Kai; Hu, Xinrong] Wuhan Text Univ, Dept Artificial Intelligence, 1 Sunshine Ave, Wuhan 430200, Hubei, Peoples R China.
C3 Wuhan Textile University; Wuhan Textile University
RP Li, L (corresponding author), Wuhan Text Univ, Dept Artificial Intelligence, 1 Sunshine Ave, Wuhan 430200, Hubei, Peoples R China.
EM liuhongthethati@gmail.com; lli@wtu.edu.cn; 2403696169@qq.com;
   2549954580@qq.com; pt@wtu.edu.cn; hxr@wtu.edu.cn
RI Ma, kai/KSL-8338-2024
OI Ma, kai/0009-0004-3748-2549; li, li/0000-0002-2027-1145
FU National Natural Science Foundation of China
FX No Statement Available
CR Amin MS, 2023, VISUAL COMPUT, V39, P3851, DOI 10.1007/s00371-022-02520-3
   Brody S, 2022, Arxiv, DOI arXiv:2105.14491
   Cucurull G, 2019, PROC CVPR IEEE, P12609, DOI 10.1109/CVPR.2019.01290
   Cui ZY, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P307, DOI 10.1145/3308558.3313444
   Defferrard M, 2016, ADV NEUR IN, V29
   Deldjoo Y, 2023, Arxiv, DOI arXiv:2202.02757
   Dong X., 2022, IEEE Transact. Neural Netw. Learn. Syst.
   Guan WL, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2299, DOI 10.1145/3474085.3475392
   Han XJ, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P785, DOI 10.1145/3331184.3331245
   Han XT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1078, DOI 10.1145/3123266.3123394
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jing P., 2023, IEEE Transact. Multimed.
   Kaicheng P., 2021, PROC IEEECVF C COMPU, P3894
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li QM, 2018, AAAI CONF ARTIF INTE, P3538
   Li XC, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P159, DOI 10.1145/3397271.3401080
   Li Z, 2022, MATHEMATICS-BASEL, V10, DOI 10.3390/math10203913
   Lin YL, 2020, PROC CVPR IEEE, P3308, DOI 10.1109/CVPR42600.2020.00337
   Liu WW, 2020, CIKM '20: PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, P925, DOI 10.1145/3340531.3412332
   Liu X, 2021, IEEE T MULTIMEDIA, V23, P2894, DOI 10.1109/TMM.2020.3018021
   McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755
   Saranya MS, 2023, VISUAL COMPUT, V39, P4195, DOI 10.1007/s00371-022-02584-1
   Sarkar R, 2022, IEEE COMPUT SOC CONF, P2262, DOI 10.1109/CVPRW56347.2022.00249
   Shajini M, 2022, VISUAL COMPUT, V38, P3551, DOI 10.1007/s00371-021-02178-3
   Song X., 2021, IEEE Transact. Multimed.
   Song XM, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P320, DOI 10.1145/3343031.3350956
   Song XM, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P753, DOI 10.1145/3123266.3123314
   Su TY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4073, DOI 10.1145/3474085.3475537
   Tan RB, 2019, IEEE I CONF COMP VIS, P10372, DOI 10.1109/ICCV.2019.01047
   Vasileva MI, 2018, LECT NOTES COMPUT SC, V11220, P405, DOI 10.1007/978-3-030-01270-0_24
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HW, 2022, EXPERT SYST APPL, V202, DOI 10.1016/j.eswa.2022.117114
   Wang XD, 2019, SIGBIOMED WORKSHOP ON BIOMEDICAL NATURAL LANGUAGE PROCESSING (BIONLP 2019), P165, DOI 10.1145/3331184.3331267
   Wijesinghe A., 2022, INT C LEARN REPR
   Xiao L, 2022, IEEE IMAGE PROC, P2431, DOI 10.1109/ICIP46576.2022.9897313
   Yang XW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2636, DOI 10.1145/3394171.3413936
   Zhan HJ, 2022, IEEE T MULTIMEDIA, V24, P819, DOI 10.1109/TMM.2021.3059514
   Zhang BX, 2020, IEEE T VIS COMPUT GR, V26, P2546, DOI 10.1109/TVCG.2019.2894627
   Zhang JY, 2022, Arxiv, DOI arXiv:2203.10453
NR 39
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 22
PY 2024
DI 10.1007/s00371-023-03238-6
EA JAN 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GJ3D1
UT WOS:001152252500004
DA 2024-08-05
ER

PT J
AU Lin, YY
   Dai, J
   Pan, JJ
   Zhou, F
   Bai, JX
AF Lin, Yuanyuan
   Dai, Ju
   Pan, Junjun
   Zhou, Feng
   Bai, Junxuan
TI Free editing of Shape and Texture with Deformable Net for 3D Caricature
   Generation
SO VISUAL COMPUTER
LA English
DT Article
DE 3D exaggerated caricature face; Texture modeling; Shape reconstruction;
   Latent code
ID TRANSLATION
AB 2D caricature editing has shown superior performance. However, 3D exaggerated caricature face (ECF) modeling with flexible shape and texture editing capabilities is far from achieving satisfactory high-quality results. This paper aims to model shape and texture variations of 3D caricatures in a learnable parameter space. To achieve this goal, we propose a novel framework for highly controllable editing of 3D caricatures. Our model mainly consists of the texture and shape hyper-networks, texture and shape Sirens, and a projection module. Specifically, two hyper-networks take the texture and shape latent codes as inputs to learn the compact parameter spaces of the two Siren modules. The texture and shape Sirens are leveraged to model the deformation variations of textural styles and geometric shapes. We further incorporate precise control of the camera parameters in the projection module to enhance the quality of generated ECF results. Our method allows flexible editing online and swapping textural features between 3D caricatures. For this purpose, we contribute a 3D caricature face dataset with textures for training and testing. Experiments and user evaluations demonstrate that our method is capable of generating diverse high-fidelity caricatures and achieves better editing capabilities than state-of-the-art methods.
C1 [Lin, Yuanyuan; Pan, Junjun] Beihang Univ, Beijing, Peoples R China.
   [Dai, Ju] Peng Cheng Lab, Shenzhen, Peoples R China.
   [Zhou, Feng] North China Univ Technol, Beijing, Peoples R China.
   [Bai, Junxuan] Capital Univ Phys Educ & Sports, Inst Artificial Intelligence Sports, Beijing, Peoples R China.
   [Bai, Junxuan] Emerging Interdisciplinary Platform Med & Engn Spo, Beijing, Peoples R China.
C3 Beihang University; Peng Cheng Laboratory; North China University of
   Technology; CAPITAL UNIVERSITY OF PHYSICAL EDUCATION AND SPORTS
RP Pan, JJ (corresponding author), Beihang Univ, Beijing, Peoples R China.; Dai, J (corresponding author), Peng Cheng Lab, Shenzhen, Peoples R China.
EM daij@pcl.ac.cn; pan_junjun@buaa.edu.cn
FU National Natural Science Fundation of China [2022ZD0115902]; National
   Natural Science Foundation of China [62102208]; Beijing Natural Science
   Foundation [4232023]; Young Elite Scientists Sponsorship Program by BAST
   [BYESS2023382]; Beijing Emerging Interdisciplinary Platform for Medicine
   and Engineering in Sports [VRLAB2024C06]
FX This research is supported by National Key
   R&\documentclass[12pt]{minimal} \usepackage{amsmath}
   \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb}
   \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek}
   \setlength{\oddsidemargin}{-69pt} \begin{document}$$ { \&
   }$$\end{document}D Program of China (No. 2022ZD0115902), National
   Natural Science Foundation of China (No. 62102208), Beijing Natural
   Science Foundation (No. 4232023), Young Elite Scientists Sponsorship
   Program by BAST (No. BYESS2023382), Beijing Emerging Interdisciplinary
   Platform for Medicine and Engineering in Sports (EIPMES), the Open
   Project Program of State Key Laboratory of Virtual Reality Technology
   and Systems, Beihang University (No. VRLAB2024C06).
CR Akleman E., 2000, P VIS, V1, P2000
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Cai HR, 2021, GRAPH MODELS, V115, DOI 10.1016/j.gmod.2021.101103
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Danecek R, 2022, PROC CVPR IEEE, P20279, DOI 10.1109/CVPR52688.2022.01967
   Deng Y, 2021, PROC CVPR IEEE, P10281, DOI 10.1109/CVPR46437.2021.01015
   Ding YH, 2021, INT C PATT RECOG, P4520, DOI 10.1109/ICPR48806.2021.9412569
   Feng Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459936
   Galanakis S, 2023, IEEE WINT CONF APPL, P3525, DOI 10.1109/WACV56688.2023.00353
   Garg J, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1101, DOI 10.1145/3240508.3240658
   Gatys L., 2016, CoRR, V16, P326, DOI 10.1167/16.12.326
   Gong JL, 2020, IEEE WINT CONF APPL, P349, DOI 10.1109/WACV45572.2020.9093543
   Han XG, 2020, IEEE T VIS COMPUT GR, V26, P2349, DOI 10.1109/TVCG.2018.2886007
   Han XG, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073629
   Hou HD, 2021, IEEE T IMAGE PROCESS, V30, P8644, DOI 10.1109/TIP.2021.3118984
   Huang Meijia, 2021, PACIFIC GRAPHICS SHO
   Huo J., 2017, BRIT MACH VIS C, P223
   Jang W, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459860
   Kaidi Cao, 2018, ACM Transactions on Graphics, V37, DOI 10.1145/3272127.3275046
   Karras T., 2020, P IEEE CVF C COMP VI, P8110, DOI [10.1109/cvpr42600.2020.00813, DOI 10.1109/CVPR42600.2020.00813]
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Karras T, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073658
   Kim J., 2020, INT C LEARN REPR, P1
   Laishram Lamyanba, 2023, Frontiers of Computer Vision: 29th International Workshop, IW-FCV 2023, Revised Selected Papers. Communications in Computer and Information Science (1857), P71, DOI 10.1007/978-981-99-4914-4_6
   Lee S., 2022, ACM SIGGRAPH C P, P1
   Li WB, 2020, NEURAL NETWORKS, V132, P66, DOI 10.1016/j.neunet.2020.08.011
   Liu YC, 2022, LECT NOTES COMPUT SC, V13675, P107, DOI 10.1007/978-3-031-19784-0_7
   O'Toole AJ, 1999, VISION RES, V39, P3145, DOI 10.1016/S0042-6989(99)00034-6
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Pinkney J.N., 2020, ARXIV
   Qiu YD, 2021, PROC CVPR IEEE, P10231, DOI 10.1109/CVPR46437.2021.01010
   Shen YJ, 2022, IEEE T PATTERN ANAL, V44, P2004, DOI 10.1109/TPAMI.2020.3034267
   Shi YC, 2019, PROC CVPR IEEE, P10754, DOI 10.1109/CVPR.2019.01102
   Wallraven C., 1999, DAGM S, P405
   Wang WY, 2019, PROC CVPR IEEE, P1038, DOI 10.1109/CVPR.2019.00113
   Wu QY, 2018, PROC CVPR IEEE, P7336, DOI 10.1109/CVPR.2018.00766
   Yang S, 2022, PROC CVPR IEEE, P7683, DOI 10.1109/CVPR52688.2022.00754
   Ye ZP, 2023, IEEE T VIS COMPUT GR, V29, P2203, DOI 10.1109/TVCG.2021.3126659
   Zhao XZ, 2023, FRONT NEUROSCI-SWITZ, V17, DOI 10.3389/fnins.2023.1136416
   Zheng Z., 2022, ARXIV
   Zheng ZQ, 2019, NEUROCOMPUTING, V355, P71, DOI 10.1016/j.neucom.2019.04.032
   Zhou P., 2021, ARXIV
NR 43
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4675
EP 4687
DI 10.1007/s00371-024-03461-9
EA MAY 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001230084100001
DA 2024-08-05
ER

PT J
AU Fu, PB
   Xiao, GY
   Yang, HR
AF Fu, Pengbin
   Xiao, Ganyun
   Yang, Huirong
TI SATD: syntax-aware handwritten mathematical expression recognition based
   on tree-structured transformer decoder
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Offline handwritten mathematical expression recognition; Transformer;
   Coverage attention; Tree decoder; Attention
AB The complex two-dimensional structure poses huge challenges for handwritten mathematical expression recognition (HMER). Many researchers process the LaTeX sequence into a tree structure and then design tree decoders based on RNN to address this issue. However, RNNs have problems with long-term dependency due to their structural characteristics. Although Transformers solve the long-term dependency problem, tree decoders based on Transformers are rarely used for HMER because the attention coverage is significantly insufficient when the distance between parent and child nodes is large in tree structures. In this paper, we propose a novel offline HMER model SATD incorporating a tree decoder based on Transformer to learn the implicit structural relationships in LaTeX strings. Moreover, to address the issue of distant parent-child nodes, we introduce a multi-scale attention aggregation module to refine attention weights using contextual information with different receptive fields. Experiments on CROHME2014/2016/2019 and HME100K datasets demonstrate performance improvements, achieving accuracy rates of 63.45%/60.42%/61.05% on the CROHME 2014/2016/2019 test sets. The source code https://github.com/EnderXiao/SATD/ of this work will be publicly available.
C1 [Fu, Pengbin; Xiao, Ganyun; Yang, Huirong] Beijing Univ Technol, Fac Informat Technol, Xidawang Rd, Beijing 100124, Peoples R China.
C3 Beijing University of Technology
RP Yang, HR (corresponding author), Beijing Univ Technol, Fac Informat Technol, Xidawang Rd, Beijing 100124, Peoples R China.
EM fupengbin@bjut.edu.cn; ender@emails.bjut.edu.cn; yanghuirong@bjut.edu.cn
CR Ahmad W., Long Papers, V1, P1389
   Altan A, 2021, APPL SOFT COMPUT, V100, DOI 10.1016/j.asoc.2020.106996
   Alvarez-Melis D., 2016, INT C LEARN REPR
   Bao H., 2021, arXiv preprint arXiv:2106.08254, DOI DOI 10.48550/ARXIV.2106.08254
   BENGIO Y, 1993, 1993 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOLS 1-3, P1183, DOI 10.1109/ICNN.1993.298725
   Bian XB, 2022, AAAI CONF ARTIF INTE, P113
   Chakraborty S, 2022, IEEE T SOFTWARE ENG, V48, P1385, DOI 10.1109/TSE.2020.3020502
   Chen Z, 2023, Arxiv, DOI [arXiv:2205.08534, DOI 10.48550/ARXIV.2205.08534]
   Coquenet D, 2023, IEEE T PATTERN ANAL, V45, P508, DOI 10.1109/TPAMI.2022.3144899
   Dai ZH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2978
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Ding HS, 2021, LECT NOTES COMPUT SC, V12822, P602, DOI 10.1007/978-3-030-86331-9_39
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Freitag Markus, 2017, P 1 WORKSHOP NEURAL, P56, DOI [10.18653/v1/W17-3207, DOI 10.18653/V1/W17-3207]
   Harer J, 2019, Arxiv, DOI arXiv:1908.00449
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Kam-Fai Chan, 2000, International Journal on Document Analysis and Recognition, V3, P3, DOI 10.1007/PL00013549
   Kolen J., 2001, A field guide to dynamical recurrent neural networks, DOI 10.1109/9780470544037.ch14
   Li F, 2024, VISUAL COMPUT, V40, P1439, DOI 10.1007/s00371-023-02859-1
   Li LY, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2190
   Li Z, 2024, PATTERN RECOGN, V149, DOI 10.1016/j.patcog.2023.110220
   Li Z, 2020, INT CONF FRONT HAND, P175, DOI 10.1109/ICFHR2020.2020.00041
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Lin ZH, 2022, LECT NOTES COMPUT SC, V13639, P213, DOI 10.1007/978-3-031-21648-0_15
   Mahdavi Mahshad, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1533, DOI 10.1109/ICDAR.2019.00247
   Mouchère H, 2016, INT CONF FRONT HAND, P607, DOI [10.1109/ICFHR.2016.0116, 10.1109/ICFHR.2016.108]
   Paszke A, 2019, ADV NEUR IN, V32
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Sinwar D, 2021, INT J DOC ANAL RECOG, V24, P97, DOI 10.1007/s10032-021-00365-5
   Sun ZY, 2020, AAAI CONF ARTIF INTE, V34, P8984
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang JM, 2024, PATTERN RECOGN, V148, DOI 10.1016/j.patcog.2023.110155
   Thakur U, 2024, INT J DOC ANAL RECOG, V27, P147, DOI 10.1007/s10032-023-00451-w
   Tu ZP, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P76
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang WH, 2023, PROC CVPR IEEE, P19175, DOI 10.1109/CVPR52729.2023.01838
   Wang Yau-Shian, 2019, arXiv
   Wang YN, 2018, ADV NEUR IN, V31
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xuwen Zhang, 2020, Journal of Physics: Conference Series, V1453, DOI 10.1088/1742-6596/1453/1/012004
   Yu JH, 2022, Arxiv, DOI arXiv:2205.01917
   Yuan Y, 2022, PROC CVPR IEEE, P4543, DOI 10.1109/CVPR52688.2022.00451
   Zhang JS, 2021, IEEE T MULTIMEDIA, V23, P2471, DOI 10.1109/TMM.2020.3011316
   Zhang JS, 2018, INT C PATT RECOG, P2245, DOI 10.1109/ICPR.2018.8546031
   Zhang JS, 2017, PATTERN RECOGN, V71, P196, DOI 10.1016/j.patcog.2017.06.017
   Zhang Jianshu, 2020, INT C MACHINE LEARNI, P11076
   Zhao WQ, 2022, LECT NOTES COMPUT SC, V13688, P392, DOI 10.1007/978-3-031-19815-1_23
   Zhao WQ, 2021, LECT NOTES COMPUT SC, V12822, P570, DOI 10.1007/978-3-030-86331-9_37
NR 52
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 3
PY 2024
DI 10.1007/s00371-024-03372-9
EA MAY 2024
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PN0Z5
UT WOS:001214652000002
DA 2024-08-05
ER

PT J
AU Ma, P
   He, XY
   Chen, YY
   Liu, Y
AF Ma, Ping
   He, Xinyi
   Chen, Yiyang
   Liu, Yuan
TI ISOD: improved small object detection based on extended scale feature
   pyramid network
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Deep learning; Feature pyramid network; Small object detection;
   Reflective vest; Attention mechanism
ID SYSTEMS
AB Rapid and accurate target detection is one of the inevitable requirements of intelligent construction site. To meet the speed requirements and improve detection accuracy, an improved small object detection (ISOD) network is proposed. The network utilizes an efficient channel attention mechanism to extract features in the backbone and combines the proposed extended scale feature pyramid network to simplify calculations and create additional high-resolution pyramid layers to improve the ability of detecting small targets. To verify the effectiveness of ISOD, experiments are conducted using the proposed Reflective Vest Scene Dataset and Tsinghua-Tencent 100K, achieving 0.425 and 0.635 mAP@0.5-\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$-$$\end{document}0.95, respectively, exceeding the SOTA YOLOv7 model, demonstrating its excellent small target detection capability and scalability.
C1 [Ma, Ping; He, Xinyi; Liu, Yuan] Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, 1800 Lihu Ave, Wuxi 214122, Jiangsu, Peoples R China.
   [Chen, Yiyang] Soochow Univ, Sch Mech & Elect Engn, 8 Jixue Rd, Suzhou 215137, Jiangsu, Peoples R China.
C3 Jiangnan University; Soochow University - China
RP Liu, Y (corresponding author), Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, 1800 Lihu Ave, Wuxi 214122, Jiangsu, Peoples R China.; Chen, YY (corresponding author), Soochow Univ, Sch Mech & Elect Engn, 8 Jixue Rd, Suzhou 215137, Jiangsu, Peoples R China.
EM pingma@jiangnan.edu.cn; 6213115011@stu.jiangnan.edu.cn;
   yychen90@suda.edu.cn; lyuan1800@jiangnan.edu.cn
RI Chen, Yiyang/T-3492-2019
OI Chen, Yiyang/0000-0001-9960-9040
FU National Natural Science Foundation of China [62103293]; Fundamental
   Research Funds for the Central Universities [JUSRP121071]; Natural
   Science Foundation of Jiangsu Province [BK20210709]; Suzhou Municipal
   Science and Technology Bureau [SYG202138]; Taihu Light Basic Research
   Project on Scientific and Technological Breakthroughs of Wuxi City
   [K20221006]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62103293, in part by the Fundamental
   Research Funds for the Central Universities under Grant JUSRP121071, in
   part by the Natural Science Foundation of Jiangsu Province under Grant
   BK20210709, in part by the Suzhou Municipal Science and Technology
   Bureau under Grant SYG202138, and in part by the 'Taihu Light' Basic
   Research Project on Scientific and Technological Breakthroughs of Wuxi
   City under Grant K20221006.
CR Amelio A, 2023, EXPERT SYST APPL, V215, DOI 10.1016/j.eswa.2022.119391
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen HT, 2022, IEEE T NEUR NET LEAR, V33, P5694, DOI 10.1109/TNNLS.2021.3071292
   Chen Y., 2023, Optik
   Chen YY, 2020, KNOWL-BASED SYST, V190, DOI 10.1016/j.knosys.2020.105479
   Fan SX, 2022, COMPUT ELECTRON AGR, V193, DOI 10.1016/j.compag.2022.106715
   Ge P., 2024, J. Visual Commun. Image Represent
   Ge PQ, 2022, EXPERT SYST APPL, V210, DOI 10.1016/j.eswa.2022.118493
   Ge PQ, 2022, PATTERN RECOGN LETT, V158, P71, DOI 10.1016/j.patrec.2022.04.025
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He K., 2017, P IEEE INT C COMP VI, P2961
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Hu XL, 2021, COMPUT ELECTRON AGR, V185, DOI 10.1016/j.compag.2021.106135
   Huang L, 2021, CONCURR COMP-PRACT E, V33, DOI 10.1002/cpe.6234
   Ioffe Sergey, 2015, INT C MACHINE LEARNI, V37, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li X., 2023, Visual Comput.
   Lian YF, 2024, VISUAL COMPUT, V40, P1395, DOI 10.1007/s00371-023-02857-3
   Liang X, 2019, COMPUT-AIDED CIV INF, V34, P415, DOI 10.1111/mice.12425
   Lin M, 2014, Arxiv, DOI arXiv:1312.4400
   Lin T.-Y., 2017, PROC CVPR IEEE, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Ma P, 2023, INT J CONTROL AUTOM, V21, P1828, DOI 10.1007/s12555-022-0080-1
   Ma P, 2021, INT J ADAPT CONTROL, V35, P1898, DOI 10.1002/acs.3302
   Mou XA, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23052710
   Redmon J., 2018, CoRR
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan R., 2020, P IEEE CVF C COMP VI, DOI [10.1109/CVPR42600.2020.01079, DOI 10.1109/CVPR42600.2020.01079]
   Tian G., 2022, Appl. Intell., P1
   Tian Z, 2022, IEEE T PATTERN ANAL, V44, P1922, DOI 10.1109/TPAMI.2020.3032166
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang GA, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3241981
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang SK, 2022, VISUAL COMPUT, V38, P3073, DOI 10.1007/s00371-022-02560-9
   Yang CX, 2023, ENG APPL ARTIF INTEL, V123, DOI 10.1016/j.engappai.2023.106472
   Yu Z, 2022, INFORM PROCESS MANAG, V59, DOI 10.1016/j.ipm.2022.102868
   Zeng S, 2024, VISUAL COMPUT, V40, P1787, DOI 10.1007/s00371-023-02886-y
   Zhang MY, 2020, SAFETY SCI, V126, DOI 10.1016/j.ssci.2020.104658
   Zhao L, 2022, SUSTAINABILITY-BASEL, V14, DOI 10.3390/su14094930
   Zhao Z., 2024, Reliab. Eng. Syst. Saf.
   Zhu XZ, 2021, Arxiv, DOI [arXiv:2010.04159, 10.48550/arXiv.2010.04159]
   Zhu Z, 2016, PROC CVPR IEEE, P2110, DOI 10.1109/CVPR.2016.232
NR 55
TC 2
Z9 2
U1 10
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 28
PY 2024
DI 10.1007/s00371-024-03341-2
EA MAR 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MO8O5
UT WOS:001194656100001
DA 2024-08-05
ER

PT J
AU Nagar, R
AF Nagar, Rajendra
TI Robust extrinsic symmetry estimation in 3D point clouds
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Reflection symmetry; Point clouds; Statistical estimation; Optimization;
   Heat kernel signatures
ID SHAPE; OPTIMIZATION
AB Detecting the reflection symmetry plane of an object represented by a 3D point cloud is a fundamental problem in 3D computer vision and geometry processing due to its various applications, such as compression, object detection, robotic grasping, 3D surface reconstruction, etc. Several approaches exist to solve this problem for clean 3D point clouds. However, it is a challenging problem to solve in the presence of outliers and missing parts. The existing methods try to overcome this challenge primarily by voting-based techniques but do not work efficiently. In this work, we proposed a statistical estimator-based approach for the plane of reflection symmetry that is robust to outliers and missing parts. We pose the problem of finding the optimal estimator for the reflection symmetry as an optimization problem on a 2-sphere that quickly converges to the global solution for an approximate initialization. We further adapt the heat kernel signature for symmetry invariant matching of mirror symmetric points. This approach helps us to decouple the chicken-and-egg problem of finding the optimal symmetry plane and correspondences between the reflective symmetric points. The proposed approach achieves comparable mean ground-truth error and 4.5% increment in the F-score as compared to the state-of-the-art approaches on the benchmark dataset.
C1 [Nagar, Rajendra] Indian Inst Technol Jodhpur, Dept Elect Engn, Jodhpur 342030, Rajasthan, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Jodhpur
RP Nagar, R (corresponding author), Indian Inst Technol Jodhpur, Dept Elect Engn, Jodhpur 342030, Rajasthan, India.
EM rn@iitj.ac.in
FU SERB, Government of India, through the Start-up Research Grant (SRG)
   scheme
FX This work was funded by the SERB, Government of India, through the
   Start-up Research Grant (SRG) scheme.
CR Abbasi A, 2019, VISUAL COMPUT, V35, P271, DOI 10.1007/s00371-018-1586-7
   Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1
   Basu A, 1998, BIOMETRIKA, V85, P549, DOI 10.1093/biomet/85.3.549
   Belkin M, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1031
   Berner A, 2008, Volume Graphics, P1
   Boumal N, 2014, J MACH LEARN RES, V15, P1455
   Cicconet M, 2017, IEEE INT CONF COMP V, P1759, DOI 10.1109/ICCVW.2017.207
   Cohen A, 2012, PROC CVPR IEEE, P1514, DOI 10.1109/CVPR.2012.6247841
   Combes B., 2008, IEEE C COMPUTER VISI, P1
   Cosmo Luca, 2016, Proc. 3DOR, V2, P12
   Ecins A, 2017, IEEE INT CONF COMP V, P1779, DOI 10.1109/ICCVW.2017.210
   Fletcher R., 1987, Practical Methods of Optimization, DOI [10.1002/9781118723203, DOI 10.1002/9781118723203]
   Funk C, 2017, IEEE INT CONF COMP V, P1692, DOI 10.1109/ICCVW.2017.198
   Funk C, 2016, PROC CVPR IEEE, P5165, DOI 10.1109/CVPR.2016.558
   Gao L, 2021, IEEE T VIS COMPUT GR, V27, P3007, DOI 10.1109/TVCG.2020.3003823
   Gnutti A, 2021, IEEE T IMAGE PROCESS, V30, P5708, DOI 10.1109/TIP.2021.3085202
   Hauagge DC, 2012, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2012.6247677
   Hauberg S, 2014, PROC CVPR IEEE, P3810, DOI 10.1109/CVPR.2014.481
   Hodan Tomas, 2020, P IEEECVF C COMPUTER, P11703
   Hruda L, 2022, VISUAL COMPUT, V38, P555, DOI 10.1007/s00371-020-02034-w
   Hruda L, 2020, LECT NOTES COMPUT SC, V12141, P509, DOI 10.1007/978-3-030-50426-7_38
   Hu L, 2021, J MATH IMAGING VIS, V63, P689, DOI 10.1007/s10851-021-01024-4
   Jian B, 2011, IEEE T PATTERN ANAL, V33, P1633, DOI 10.1109/TPAMI.2010.223
   Kazhdan M, 2002, LECT NOTES COMPUT SC, V2351, P642
   Kim VG, 2010, COMPUT GRAPH FORUM, V29, P1689, DOI 10.1111/j.1467-8659.2010.01778.x
   Korman S, 2015, COMPUT GRAPH FORUM, V34, P2, DOI 10.1111/cgf.12454
   Koser Kevin, 2011, Pattern Recognition. Proceedings 33rd DAGM Symposium, P266, DOI 10.1007/978-3-642-23123-0_27
   Lasowski R, 2009, IEEE I CONF COMP VIS, P963, DOI 10.1109/ICCV.2009.5459356
   Li B, 2016, GRAPH MODELS, V83, P2, DOI 10.1016/j.gmod.2015.09.003
   Li QS, 2021, COMPUT GRAPH FORUM, V40, P81, DOI 10.1111/cgf.14120
   Li R-W, 2023, P IEEE CVF INT C COM, P14543
   Lipman Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778840
   Liu TQ, 2012, COMPUT GRAPH FORUM, V31, P1607, DOI 10.1111/j.1467-8659.2012.03166.x
   Liu Y, 2012, IEEE T VIS COMPUT GR, V18, P1693, DOI 10.1109/TVCG.2011.152
   Lu YX, 2009, FOUND TRENDS COMPUT, V5, P1, DOI 10.1561/0600000008
   Lukác M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073661
   Ma JY, 2013, PROC CVPR IEEE, P2147, DOI 10.1109/CVPR.2013.279
   Mancas M, 2005, INT CONF ACOUST SPEE, P725
   Martinet A, 2006, ACM T GRAPHIC, V25, P439, DOI 10.1145/1138450.1138462
   Melzi S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356524
   Miglani A., 2013, 2013 4 NATL C COMPUT, P1
   Mitra NJ, 2006, ACM T GRAPHIC, V25, P560, DOI 10.1145/1141911.1141924
   Mitra NJ, 2013, COMPUT GRAPH FORUM, V32, P1, DOI 10.1111/cgf.12010
   Mitra S., 2004, P 2004 IEEE COMPUTER, V2, pII
   Nagar R, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107483
   Nagar R, 2019, IEEE T SIGNAL PROCES, V67, P1582, DOI 10.1109/TSP.2019.2893835
   Nagar R, 2018, LECT NOTES COMPUT SC, V11205, P433, DOI 10.1007/978-3-030-01246-5_26
   Ovsjanikov M, 2008, COMPUT GRAPH FORUM, V27, P1341, DOI 10.1111/j.1467-8659.2008.01273.x
   Podolak J, 2006, ACM T GRAPHIC, V25, P549, DOI 10.1145/1141911.1141923
   Qiao YL, 2019, Arxiv, DOI arXiv:1911.00189
   Ren J, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417800
   Sahillioglu Y, 2013, COMPUT GRAPH FORUM, V32, P177, DOI 10.1111/cgf.12007
   Sahillioglu Y, 2020, VISUAL COMPUT, V36, P1705, DOI 10.1007/s00371-019-01760-0
   Seo A., 2021, P IEEECVF INT C COMP, P1285
   Sharp N, 2020, COMPUT GRAPH FORUM, V39, P69, DOI 10.1111/cgf.14069
   Shi YF, 2023, IEEE T PATTERN ANAL, V45, P4882, DOI 10.1109/TPAMI.2022.3186876
   Shi YF, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417775
   Shi ZY, 2016, COMPUT GRAPH FORUM, V35, P217, DOI 10.1111/cgf.12978
   Sipiran I., 2023, EUROGRAPHICS WORKSHO, P17
   Sipiran I, 2014, COMPUT GRAPH FORUM, V33, P131, DOI 10.1111/cgf.12481
   Speciale P, 2016, LECT NOTES COMPUT SC, V9912, P313, DOI 10.1007/978-3-319-46484-8_19
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Sung M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818094
   Tan VYF, 2013, IEEE T PATTERN ANAL, V35, P1592, DOI 10.1109/TPAMI.2012.240
   Thomas DM, 2014, IEEE T VIS COMPUT GR, V20, P2427, DOI 10.1109/TVCG.2014.2346332
   Thrun S, 2005, IEEE I CONF COMP VIS, P1824
   Tyler C. W., 2003, HUMAN SYMMETRY PERCE
   van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P1681, DOI 10.1111/j.1467-8659.2011.01884.x
   Wang H, 2017, COMPUT GRAPH FORUM, V36, P51, DOI 10.1111/cgf.13271
   Wang WC, 2019, COMPUT GRAPH FORUM, V38, P617, DOI 10.1111/cgf.13865
   Wang Y., 2019, P IEEECVF C COMPUTER, P6231
   Wang YQ, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392443
   Wu SZ, 2020, PROC CVPR IEEE, P1, DOI 10.1109/CVPR42600.2020.00008
   Chang AX, 2015, Arxiv, DOI [arXiv:1512.03012, DOI 10.48550/ARXIV.1512.03012]
   Xu K., 2009, ACM SIGGRAPH ASIA 20, P1
   Xu K, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366200
   Yu Zhong, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P689, DOI 10.1109/ICCVW.2009.5457637
   ZABRODSKY H, 1995, IEEE T PATTERN ANAL, V17, P1154, DOI 10.1109/34.476508
   Zhang ZX, 2023, IEEE I CONF COMP VIS, P8862, DOI 10.1109/ICCV51070.2023.00817
   Zhou YC, 2021, PROC CVPR IEEE, P15935, DOI 10.1109/CVPR46437.2021.01568
NR 80
TC 0
Z9 0
U1 8
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 15
PY 2024
DI 10.1007/s00371-024-03313-6
EA MAR 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LB0L1
UT WOS:001184198700002
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Tiwari, AK
   Sharma, GK
AF Tiwari, Alok Kumar
   Sharma, G. K.
TI FS-3DSSN: an efficient few-shot learning for single-stage 3D object
   detection on point clouds
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Point cloud; 3D object detection; Few-shot learning; Single-stage
   detector; Autonomous driving
AB The current 3D object detectionmethods have achieved promising results for conventional tasks to detect frequently occurring objects like cars, pedestrians and cyclists. However, they require many annotated boundary boxes and class labels for training, which is very expensive and hard to obtain. Nevertheless, detecting infrequent occurring objects, such as police vehicles, is also essential for autonomous driving to be successful. Therefore, we explore the potential of few-shot learning to handle this challenge of detecting infrequent categories. The current 3D object detectors do not have the necessary architecture to support this type of learning. Thus, this paper presents a new method termed few-shot single-stage network for 3D object detection (FS-3DSSN) to predict infrequent categories of objects. FS-3DSSN uses a class-incremental few-shot learning approach to detect infrequent categories without compromising the detection accuracy of frequent categories. It consists of twomodules: (i) a single-stage network architecture for 3D object detection (3DSSN) using deformable convolutions to detect small objects and (ii) a class-incremental-based meta-learning module to learn and predict infrequent class categories. 3DSSN obtained 84.53 mAP(3D) on the KITTI car category and 73.4 NDS on the nuScenes dataset, outperforming previous state of the art. Further, the result of FS-3DSSN on nuScenes is also encouraging for detecting infrequent categories while maintaining accuracy in frequent classes.
C1 [Tiwari, Alok Kumar; Sharma, G. K.] ABV Indian Inst Informat Technol & Management, Dept Informat Technol, Gwalior, India.
C3 ABV-Indian Institute of Information Technology & Management, Gwalior
RP Tiwari, AK (corresponding author), ABV Indian Inst Informat Technol & Management, Dept Informat Technol, Gwalior, India.
EM alok.tiwari243@gmail.com; gksharma@iiitm.ac.in
RI Tiwari, Alok/KUF-0770-2024
OI Tiwari, Alok/0000-0001-8679-3926
CR Alaba SY, 2023, IEEE SENS J, V23, P3378, DOI 10.1109/JSEN.2023.3235830
   Antonelli S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3519022
   Bai XY, 2022, PROC CVPR IEEE, P1080, DOI 10.1109/CVPR52688.2022.00116
   Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164
   Cao JieCheng, 2024, IEEE Transactions on Artificial Intelligence, P254, DOI 10.1109/TAI.2023.3237787
   Chen C, 2022, AAAI CONF ARTIF INTE, P221
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Chen YL, 2019, IEEE I CONF COMP VIS, P9774, DOI [10.1109/iccv.2019.00987, 10.1109/ICCV.2019.00987]
   Cheng M, 2022, IEEE T CIRC SYST VID, V32, P2158, DOI 10.1109/TCSVT.2021.3088545
   Chenhang He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11870, DOI 10.1109/CVPR42600.2020.01189
   Cheraghian A, 2021, PROC CVPR IEEE, P2534, DOI 10.1109/CVPR46437.2021.00256
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Drobnitzky M., 2023, The Visual Computer, V11, P1
   Du L, 2022, IEEE T PATTERN ANAL, V44, P8097, DOI 10.1109/TPAMI.2021.3104172
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Han GX, 2022, AAAI CONF ARTIF INTE, P780
   Huang G, 2023, IEEE T PATTERN ANAL, V45, P4071, DOI 10.1109/TPAMI.2022.3199617
   Huang Z., 2023, Vis. Comput, V10, P1
   Jiang W, 2021, IEEE T CIRC SYST VID, V31, P1091, DOI 10.1109/TCSVT.2020.2995754
   Kang BY, 2019, IEEE I CONF COMP VIS, P8419, DOI 10.1109/ICCV.2019.00851
   Koh J, 2022, Arxiv, DOI arXiv:2212.00442
   Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298
   Lin T.-Y., 2017, PROC CVPR IEEE, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu ZY, 2020, VISUAL COMPUT, V36, P1823, DOI 10.1007/s00371-019-01778-4
   Lu Y, 2023, IEEE T CYBERNETICS, V53, P514, DOI 10.1109/TCYB.2022.3149825
   Ning KL, 2023, IEEE T INTELL TRANSP, V24, P3223, DOI 10.1109/TITS.2022.3225880
   Qi CR, 2017, ADV NEUR IN, V30
   Qi Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P68, DOI 10.1007/978-3-030-58589-1_5
   Qian R, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108796
   Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054
   Shi GS, 2022, LECT NOTES COMPUT SC, V13670, P35, DOI 10.1007/978-3-031-20080-9_3
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Shi SS, 2021, IEEE T PATTERN ANAL, V43, P2647, DOI 10.1109/TPAMI.2020.2977026
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang K, 2023, IEEE T INTELL VEHICL, V8, P1699, DOI 10.1109/TIV.2022.3213796
   Wei LX, 2021, VISUAL COMPUT, V37, P133, DOI 10.1007/s00371-019-01787-3
   Wu P, 2023, VISUAL COMPUT, V39, P2425, DOI 10.1007/s00371-022-02672-2
   Wu XW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1679, DOI 10.1145/3394171.3413832
   Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337
   Yin TW, 2021, PROC CVPR IEEE, P11779, DOI 10.1109/CVPR46437.2021.01161
   Yu CB, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3174483
   Yuan S., 2022, AS C COMP VIS, P1761
   Zetong Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11037, DOI 10.1109/CVPR42600.2020.01105
   Zhao S., 2023, arXiv
   Zheng W, 2021, PROC CVPR IEEE, P14489, DOI 10.1109/CVPR46437.2021.01426
   Zhou DF, 2019, INT CONF 3D VISION, P85, DOI 10.1109/3DV.2019.00019
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
NR 49
TC 0
Z9 0
U1 16
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 18
PY 2024
DI 10.1007/s00371-023-03228-8
EA JAN 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FE3U7
UT WOS:001144055000001
DA 2024-08-05
ER

PT J
AU Yu, JW
   Peng, KY
   Zhang, LW
   Xie, W
AF Yu, Jinwei
   Peng, Kaiyu
   Zhang, Langwen
   Xie, Wei
TI Image encryption algorithm based on DNA network and hyperchaotic system
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image encryption; Hyper-chaotic system; DNA operation; DNA network;
   Triploid mutation
ID MAP
AB This paper presents a novel deoxyribonucleic acid (DNA) network encryption method combined with a hyperchaotic system for image encryption. The DNA network encryption algorithm contains 12 different DNA operations, which overcomes the limitations of existing DNA encryption schemes that rely on limited fixed DNA operation types. Firstly, secure hash algorithm 256-bit (SHA-256) is employed to generate secret keys associated with the plaintext. Secondly, the hyperchaotic system is employed to derive key streams with high plaintext sensitivity. Thirdly, the DNA encoding, DNA network encryption and decoding are performed sequentially to encrypt the plaintext image using the keystreams. Finally, an additional row-column disruption is executed to strengthen the resistance of the encryption scheme to attacks. Security analysis reveals that the information entropy of our cipher image reaches 7.9975 and the average pixel correlation coefficients are below 0.01, implying that our encryption algorithm has excellent security.
C1 [Yu, Jinwei; Peng, Kaiyu; Zhang, Langwen; Xie, Wei] South China Univ Technol, Sch Automat Sci & Engn, Guangzhou, Guangdong, Peoples R China.
   [Xie, Wei] South China Univ Technol, Guangdong Prov Key Lab Tech & Equipment Macromol A, Guangzhou, Guangdong, Peoples R China.
   [Xie, Wei] South China Univ Technol, Key Lab Autonomous Syst & Networked Control, Minist Educ, Guangzhou, Guangdong, Peoples R China.
   [Xie, Wei] South China Univ Technol, Guangdong Engn Technol Res Ctr Unmanned Aerial Veh, Guangzhou, Guangdong, Peoples R China.
C3 South China University of Technology; South China University of
   Technology; South China University of Technology; South China University
   of Technology
RP Xie, W (corresponding author), South China Univ Technol, Sch Automat Sci & Engn, Guangzhou, Guangdong, Peoples R China.; Xie, W (corresponding author), South China Univ Technol, Guangdong Prov Key Lab Tech & Equipment Macromol A, Guangzhou, Guangdong, Peoples R China.; Xie, W (corresponding author), South China Univ Technol, Key Lab Autonomous Syst & Networked Control, Minist Educ, Guangzhou, Guangdong, Peoples R China.; Xie, W (corresponding author), South China Univ Technol, Guangdong Engn Technol Res Ctr Unmanned Aerial Veh, Guangzhou, Guangdong, Peoples R China.
EM weixie@scut.edu.cn
RI CAO, ying/KFA-2972-2024; xie, jing/KDO-9486-2024; Liu,
   Chang/KGL-6678-2024; YANG, DAN/KCL-5217-2024
OI Yu, Jinwei/0000-0002-9969-2697
FU Key-Area Research and Development Program of Guangdong Province; 
   [2018B010108001]
FX This work is supported by Key-Area Research and Development Program of
   Guangdong Province under the grant 2018B010108001.
CR Abualigah L, 2021, COMPUT IND ENG, V157, DOI 10.1016/j.cie.2021.107250
   Agushaka JO, 2022, COMPUT METHOD APPL M, V391, DOI 10.1016/j.cma.2022.114570
   Chai XL, 2022, NONLINEAR DYNAM, V108, P2671, DOI 10.1007/s11071-022-07328-3
   Chai XL, 2019, SIGNAL PROCESS, V155, P44, DOI 10.1016/j.sigpro.2018.09.029
   Cun QQ, 2023, VISUAL COMPUT, V39, P6589, DOI 10.1007/s00371-022-02750-5
   Dong WL, 2021, OPT COMMUN, V499, DOI 10.1016/j.optcom.2021.127211
   Dou YQ, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10062187
   Duan CF, 2022, OPT LASER ENG, V150, DOI 10.1016/j.optlaseng.2021.106881
   Elsaid SA, 2023, MULTIMED TOOLS APPL, V82, P1995, DOI 10.1007/s11042-022-12641-5
   Ezugwu AE, 2022, NEURAL COMPUT APPL, V34, P20017, DOI 10.1007/s00521-022-07530-9
   Fang PF, 2023, VISUAL COMPUT, V39, P1975, DOI 10.1007/s00371-022-02459-5
   Hu T, 2017, SIGNAL PROCESS, V134, P234, DOI 10.1016/j.sigpro.2016.12.008
   Huang ZW, 2022, OPT LASER TECHNOL, V149, DOI 10.1016/j.optlastec.2022.107879
   Jain A, 2016, MULTIMED TOOLS APPL, V75, P5455, DOI 10.1007/s11042-015-2515-7
   Kaur G, 2022, VISUAL COMPUT, V38, P1027, DOI 10.1007/s00371-021-02066-w
   Khalil N, 2021, OPT LASER TECHNOL, V143, DOI 10.1016/j.optlastec.2021.107326
   Li CL, 2024, VISUAL COMPUT, V40, P731, DOI 10.1007/s00371-023-02812-2
   Li X, 2016, OPTIK, V127, P2558, DOI 10.1016/j.ijleo.2015.11.221
   Liu WH, 2017, INT J BIFURCAT CHAOS, V27, DOI 10.1142/S0218127417501711
   Lone MA, 2023, NONLINEAR DYNAM, V111, P5919, DOI 10.1007/s11071-022-07995-2
   Matthews R., 1989, Cryptologia, V13, P29, DOI DOI 10.1080/0161-118991863745
   Niyat AY, 2017, OPT LASER ENG, V90, P225, DOI [10.1016/j.optlaseng.2016.10:019, 10.1016/j.optlaseng.2016.10.019]
   Rehman AU, 2018, OPTIK, V159, P348, DOI 10.1016/j.ijleo.2018.01.064
   Sabir S, 2021, J INF SECUR APPL, V63, DOI 10.1016/j.jisa.2021.103005
   Signing VRF, 2022, CHAOS SOLITON FRACT, V155, DOI 10.1016/j.chaos.2021.111777
   Solak E, 2010, OPT COMMUN, V283, P232, DOI 10.1016/j.optcom.2009.09.070
   Talhaoui MZ, 2021, VISUAL COMPUT, V37, P541, DOI 10.1007/s00371-020-01822-8
   Wang QY, 2022, J INF SECUR APPL, V70, DOI 10.1016/j.jisa.2022.103340
   Wang XY, 2023, VISUAL COMPUT, V39, P3123, DOI 10.1007/s00371-022-02517-y
   Wang XY, 2021, CHAOS SOLITON FRACT, V147, DOI 10.1016/j.chaos.2021.110970
   Wang XY, 2020, OPT LASER ENG, V125, DOI 10.1016/j.optlaseng.2019.105851
   Wang XY, 2020, INFORM SCIENCES, V507, P16, DOI 10.1016/j.ins.2019.08.041
   Wang XY, 2012, SIGNAL PROCESS, V92, P1101, DOI 10.1016/j.sigpro.2011.10.023
   WATSON JD, 1953, NATURE, V171, P737, DOI 10.1038/171737a0
   Wen WY, 2020, NONLINEAR DYNAM, V99, P1587, DOI 10.1007/s11071-019-05378-8
   Wu JH, 2018, SIGNAL PROCESS, V153, P11, DOI 10.1016/j.sigpro.2018.06.008
   Wu XJ, 2018, SIGNAL PROCESS, V148, P272, DOI 10.1016/j.sigpro.2018.02.028
   Xu J, 2022, VISUAL COMPUT, V38, P1509, DOI 10.1007/s00371-021-02085-7
   Xu QY, 2019, OPT LASER ENG, V121, P203, DOI 10.1016/j.optlaseng.2019.04.011
   Yang CX, 2022, NONLINEAR DYNAM, V109, P2103, DOI 10.1007/s11071-022-07534-z
   Yildirim M, 2022, CHAOS SOLITON FRACT, V155, DOI 10.1016/j.chaos.2021.111631
   Yu JW, 2022, CHAOS SOLITON FRACT, V162, DOI 10.1016/j.chaos.2022.112456
   Zarei A, 2016, APPL MATH COMPUT, V291, P323, DOI 10.1016/j.amc.2016.07.023
   Zhang Q, 2013, OPTIK, V124, P6276, DOI 10.1016/j.ijleo.2013.05.009
   Zhou GM, 2015, NEUROCOMPUTING, V169, P150, DOI 10.1016/j.neucom.2014.11.095
   Zhou KL, 2020, OPT LASER TECHNOL, V121, DOI 10.1016/j.optlastec.2019.105769
NR 46
TC 2
Z9 2
U1 41
U2 51
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 2
PY 2024
DI 10.1007/s00371-023-03219-9
EA JAN 2024
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DN8R7
UT WOS:001132825200001
DA 2024-08-05
ER

PT J
AU Wirth, T
   Rak, A
   von Buelow, M
   Knauthe, V
   Kuijper, A
   Fellner, DW
AF Wirth, Tristan
   Rak, Arne
   von Buelow, Max
   Knauthe, Volker
   Kuijper, Arjan
   Fellner, Dieter W.
TI NeRF-FF: a plug-in method to mitigate defocus blur for runtime optimized
   neural radiance fields
SO VISUAL COMPUTER
LA English
DT Article
DE Novel View Synthesis; Neural Radiance Fields; Image Restoration
AB Neural radiance fields (NeRFs) have revolutionized novel view synthesis, leading to an unprecedented level of realism in rendered images. However, the reconstruction quality of NeRFs suffers significantly from out-of-focus regions in the input images. We propose NeRF-FF, a plug-in strategy that estimates image masks based on Focus Frustums (FFs), i.e., the visible volume in the scene space that is in-focus. NeRF-FF enables a subsequently trained NeRF model to omit out-of-focus image regions during the training process. Existing methods to mitigate the effects of defocus blurred input images often leverage dynamic ray generation. This makes them incompatible with the static ray assumptions employed by runtime-performance-optimized NeRF variants, such as Instant-NGP, leading to high training times. Our experiments show that NeRF-FF outperforms state-of-the-art approaches regarding training time by two orders of magnitude-reducing it to under 1 min on end-consumer hardware-while maintaining comparable visual quality.
C1 [Wirth, Tristan; Rak, Arne; von Buelow, Max; Knauthe, Volker; Kuijper, Arjan; Fellner, Dieter W.] Tech Univ Darmstadt, Darmstadt, Germany.
   [Kuijper, Arjan; Fellner, Dieter W.] Fraunhofer IGD, Darmstadt, Germany.
   [Fellner, Dieter W.] Graz Univ Technol, Graz, Austria.
C3 Technical University of Darmstadt; Graz University of Technology
RP Wirth, T (corresponding author), Tech Univ Darmstadt, Darmstadt, Germany.
EM tristan.wirth@gris.tu-darmstadt.de
OI Kuijper, Arjan/0000-0002-6413-0061; von Buelow, Max/0000-0002-0036-319X
FU Projekt DEAL
FX Open Access funding enabled and organized by Projekt DEAL
CR Barron JT, 2022, PROC CVPR IEEE, P5460, DOI 10.1109/CVPR52688.2022.00539
   Barron JT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5835, DOI 10.1109/ICCV48922.2021.00580
   Chen AP, 2022, LECT NOTES COMPUT SC, V13692, P333, DOI 10.1007/978-3-031-19824-3_20
   Chen JK, 2023, PROC CVPR IEEE, P12439, DOI 10.1109/CVPR52729.2023.01197
   Chen XY, 2022, PROC CVPR IEEE, P12933, DOI 10.1109/CVPR52688.2022.01260
   Chen ZQ, 2023, PROC CVPR IEEE, P16569, DOI 10.1109/CVPR52729.2023.01590
   Dai P, 2023, PROC CVPR IEEE, P154, DOI 10.1109/CVPR52729.2023.00023
   Deng KL, 2022, PROC CVPR IEEE, P12872, DOI 10.1109/CVPR52688.2022.01254
   Fridovich-Keil S, 2022, PROC CVPR IEEE, P5491, DOI 10.1109/CVPR52688.2022.00542
   Huang X, 2022, PROC CVPR IEEE, P18377, DOI 10.1109/CVPR52688.2022.01785
   Jambon C, 2023, P ACM COMPUT GRAPH, V6, DOI 10.1145/3585499
   Jiang SY, 2023, PROC CVPR IEEE, P12543, DOI 10.1109/CVPR52729.2023.01207
   Jun-Seong K, 2022, LECT NOTES COMPUT SC, V13692, P384, DOI 10.1007/978-3-031-19824-3_23
   Kajiya J. T., 1984, Computers & Graphics, V18, P165
   Karaali A, 2018, IEEE T IMAGE PROCESS, V27, P1126, DOI 10.1109/TIP.2017.2771563
   Kerbl B, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592433
   Lazova V., 2023, P IEEE CVF WINT C AP, P4340, DOI DOI 10.48550/ARXIV.2204.10850
   Lee D., 2023, IEEE CVF ICCV C P, P17639
   Lee D, 2023, PROC CVPR IEEE, P12386, DOI 10.1109/CVPR52729.2023.01192
   Li TY, 2022, PROC CVPR IEEE, P5511, DOI 10.1109/CVPR52688.2022.00544
   Lin CH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5721, DOI 10.1109/ICCV48922.2021.00569
   Liu Steven, 2021, P IEEECVF INT C COMP, P5773
   Lombardi S, 2021, ACM T GRAPHIC, V40, DOI [10.1145/3476576.3476608, 10.1145/3450626.3459863]
   Ma L, 2022, PROC CVPR IEEE, P12851, DOI 10.1109/CVPR52688.2022.01252
   Martin-Brualla R, 2021, PROC CVPR IEEE, P7206, DOI 10.1109/CVPR46437.2021.00713
   Meng Q, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6331, DOI 10.1109/ICCV48922.2021.00629
   Mildenhall B, 2020, P ECCV, DOI DOI 10.1007/978-3-030-58452-8
   Mildenhall B, 2022, PROC CVPR IEEE, P16169, DOI 10.1109/CVPR52688.2022.01571
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Munkberg J, 2022, PROC CVPR IEEE, P8270, DOI 10.1109/CVPR52688.2022.00810
   Niemeyer M, 2022, PROC CVPR IEEE, P5470, DOI 10.1109/CVPR52688.2022.00540
   Niemeyer M, 2021, PROC CVPR IEEE, P11448, DOI 10.1109/CVPR46437.2021.01129
   Park K., 2021, arXiv
   Park K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5845, DOI 10.1109/ICCV48922.2021.00581
   Peng C, 2023, AAAI CONF ARTIF INTE, P2029
   Peng SD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14294, DOI 10.1109/ICCV48922.2021.01405
   Pumarola A, 2021, PROC CVPR IEEE, P10313, DOI 10.1109/CVPR46437.2021.01018
   Qi Y., 2023, IEEE CVF ICCV C P, P13254
   Rebain D, 2022, PROC CVPR IEEE, P1548, DOI 10.1109/CVPR52688.2022.00161
   Reiser C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14315, DOI 10.1109/ICCV48922.2021.01407
   Rematas K, 2022, PROC CVPR IEEE, P12922, DOI 10.1109/CVPR52688.2022.01259
   Schwarz K., 2020, ADV NEURAL INFORM PR, V33, P20154, DOI DOI 10.48550/ARXIV.2007.02442
   Son H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2622, DOI 10.1109/ICCV48922.2021.00264
   Srinivasan PP, 2021, PROC CVPR IEEE, P7491, DOI 10.1109/CVPR46437.2021.00741
   Su S.-Y., 2021, ADV NEURAL INF PROCE, V34, P12278
   Sun C, 2022, PROC CVPR IEEE, P5449, DOI 10.1109/CVPR52688.2022.00538
   Tancik M, 2023, PROCEEDINGS OF SIGGRAPH 2023 CONFERENCE PAPERS, SIGGRAPH 2023, DOI 10.1145/3588432.3591516
   Tancik M, 2022, PROC CVPR IEEE, P8238, DOI 10.1109/CVPR52688.2022.00807
   Tretschk E, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12939, DOI 10.1109/ICCV48922.2021.01272
   von Buelow M, 2022, ACM J COMPUT CULT HE, V15, DOI 10.1145/3500924
   Wang L, 2022, PROC CVPR IEEE, P13514, DOI 10.1109/CVPR52688.2022.01316
   Wang P, 2023, PROC CVPR IEEE, P4170, DOI 10.1109/CVPR52729.2023.00406
   Wang Z, 2021, ARXIV
   Wang ZA, 2023, PROC CVPR IEEE, P8370, DOI 10.1109/CVPR52729.2023.00809
   Warburg F., 2023, ARXIV
   Wirth T, 2023, COMPUT GRAPH FORUM, DOI 10.1111/cgf.14977
   Wu ZJ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1718, DOI 10.1145/3503161.3548088
   Yang BB, 2022, LECT NOTES COMPUT SC, V13676, P597, DOI 10.1007/978-3-031-19787-1_34
   Yu A, 2021, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR46437.2021.00455
   Yu A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5732, DOI 10.1109/ICCV48922.2021.00570
   Yuan YJ, 2022, PROC CVPR IEEE, P18332, DOI 10.1109/CVPR52688.2022.01781
   Zeng C, 2023, PROCEEDINGS OF SIGGRAPH 2023 CONFERENCE PAPERS, SIGGRAPH 2023, DOI 10.1145/3588432.3591482
   Zhang JZ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459676
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang XM, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480496
   Zhao FQ, 2022, PROC CVPR IEEE, P7733, DOI 10.1109/CVPR52688.2022.00759
NR 66
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 5043
EP 5055
DI 10.1007/s00371-024-03507-y
EA JUL 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001259414100004
OA hybrid
DA 2024-08-05
ER

PT J
AU Li, YX
   Liu, YZ
   Chen, R
   Li, H
   Zhao, N
AF Li, Yixi
   Liu, Yanzhe
   Chen, Rong
   Li, Hui
   Zhao, Na
TI Point cloud upsampling via a coarse-to-fine network with
   transformer-encoder
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Point cloud upsampling; Coarse-to-fine network; Transformer; Shuffle
AB Point clouds provide a common geometric representation for burgeoning 3D graphics and vision tasks. To deal with the sparse, noisy and non-uniform output of most 3D data acquisition devices, this paper presents a novel coarse-to-fine learning framework that incorporates the Transformer-encoder and positional feature fusion. Its long-range dependencies with sensitive positional information allow robust feature embedding and fusion of points, especially noising elements and non-regular outliers. The proposed network consists of a Coarse Points Generator and a Points Offsets Refiner. The generator embodies a multi-feature Transformer-encoder and an EdgeConv-based feature reshaping to infer the coarse but dense upsampling point sets, whereas the refiner further learns the positions of upsampled points based on multi-feature fusion strategy that can adaptively adjust the fused features' weights of coarse points and points offsets. Extensive qualitative and quantitative results on both synthetic and real-scanned datasets demonstrate the superiority of our method over the state-of-the-arts. Our code is publicly available at https://github.com/Superlyxi/CFT-PU.
C1 [Li, Yixi; Liu, Yanzhe; Chen, Rong; Li, Hui; Zhao, Na] Dalian Maritime Univ, Coll Informat Sci & Technol, Linghai Rd, Dalian 116026, Liaoning, Peoples R China.
C3 Dalian Maritime University
RP Chen, R; Li, H (corresponding author), Dalian Maritime Univ, Coll Informat Sci & Technol, Linghai Rd, Dalian 116026, Liaoning, Peoples R China.
EM superlyxi@163.com; liuyanzhe@dlmu.edu.cn; rchen@dlmu.edu.cn;
   li_hui@dlmu.edu.cn; zna@dlmu.edu.cn
FU National Natural Science Foundation of China [62002039, 61672122];
   National Natural Science Foundation of China [36330603]; Fundamental
   Research Funds for the Central Universities
FX This work is supported by the National Natural Science Foundation of
   China (No.62002039, No. 61672122), and the Fundamental Research Funds
   for the Central Universities (No.36330603).
CR Cai G, 2023, VISUAL COMPUT, V39, P2781, DOI 10.1007/s00371-022-02492-4
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Ding DD, 2021, IEEE T CIRC SYST VID, V31, P4661, DOI 10.1109/TCSVT.2021.3099106
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Du Hang, 2022, P AS C COMP VIS, P586
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   He Y, 2023, PROC CVPR IEEE, P5354, DOI 10.1109/CVPR52729.2023.00518
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Li RH, 2021, PROC CVPR IEEE, P344, DOI 10.1109/CVPR46437.2021.00041
   Li RH, 2019, IEEE I CONF COMP VIS, P7202, DOI 10.1109/ICCV.2019.00730
   Liu H, 2022, IEEE T IMAGE PROCESS, V31, P7389, DOI 10.1109/TIP.2022.3222918
   Long C, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2191, DOI 10.1145/3503161.3547777
   Qian GC, 2021, PROC CVPR IEEE, P11678, DOI 10.1109/CVPR46437.2021.01151
   Qiu Shi, 2022, P AS C COMP VIS, P2475
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang YF, 2019, PROC CVPR IEEE, P5951, DOI 10.1109/CVPR.2019.00611
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Chang AX, 2015, Arxiv, DOI [arXiv:1512.03012, DOI 10.48550/ARXIV.1512.03012]
   Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295
   Yue Qian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P752, DOI 10.1007/978-3-030-58529-7_44
   Zhang PP, 2021, IEEE T CIRC SYST VID, V31, P4673, DOI 10.1109/TCSVT.2021.3100134
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhao TM, 2023, PATTERN RECOGN, V143, DOI 10.1016/j.patcog.2023.109796
NR 28
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 21
PY 2024
DI 10.1007/s00371-024-03535-8
EA JUN 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UW9J6
UT WOS:001251216700001
DA 2024-08-05
ER

PT J
AU Li, SY
   Liu, ZH
   Gao, MJ
   Bai, Y
   Yin, HZ
AF Li, Shiyu
   Liu, Zehao
   Gao, Meijing
   Bai, Yang
   Yin, Haozheng
TI MDSCN: multiscale depthwise separable convolutional network for
   underwater graphics restoration
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Computer graphics; Underwater image restoration; Optimizer; Multiscale
   convolution; Depth separable convolution
AB Underwater imaging techniques have been a focus of research for computer vision. Underwater imaging frequently encounters challenges for poor image quality and slow restoration speed, thereby hindering human underwater exploration endeavors. To enhance the quality and improve the real-time performance of underwater image restoration, the paper proposes a lightweight underwater color image restoration network based on multiscale depthwise separable convolution. First, the algorithm tackles the problems of difficult convergence and slow training by improving the AdamW optimizer. Then, we propose a multiscale depthwise separable convolution module with RGB channel, which allows efficient extraction of image features based on the underwater light propagation properties. The MDSCN can effectively improve the processing speed and recovery effect of underwater images. Through experimentation and analysis, our algorithm outperforms traditional image processing methods and recent deep learning approaches in terms of visual effects and objective evaluation metrics. Furthermore, our algorithm also has a better performs than existing deep learning methods in processing speed, which demonstrates excellent generalizability and practicality. The research in the article is highly informative for the field of underwater computer vision. The dataset, training weights files and codes are publicly available https://gitee.com/raining-li/underwater-image-processing/tree/master.
C1 [Li, Shiyu; Liu, Zehao; Bai, Yang; Yin, Haozheng] Yanshan Univ, Coll Informat Sci & Engn, Key Lab Special Fiber & Fiber Sensor Hebei Prov, Qinhuangdao 066004, Hebei, Peoples R China.
   [Gao, Meijing] Beijing Inst Technol, Coll Informat & Elect, Beijing 100081, Peoples R China.
   [Gao, Meijing] Beijing Inst Technol, Tangshan Res Inst, Tangshan 063000, Peoples R China.
C3 Yanshan University; Beijing Institute of Technology; Beijing Institute
   of Technology
RP Gao, MJ (corresponding author), Beijing Inst Technol, Coll Informat & Elect, Beijing 100081, Peoples R China.; Gao, MJ (corresponding author), Beijing Inst Technol, Tangshan Res Inst, Tangshan 063000, Peoples R China.
EM gaomeijing@126.com
FU Hebei Graduate Innovation Funding Project of China
FX No Statement Available
CR Chen BY, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13040731
   Chen RZ, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3134762
   [丛润民 Cong Runmin], 2020, [信号处理, Journal of Signal Processing], V36, P1377
   Drews P, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P825, DOI 10.1109/ICCVW.2013.113
   Elfwing S, 2018, NEURAL NETWORKS, V107, P3, DOI 10.1016/j.neunet.2017.12.012
   Fabbri C, 2018, IEEE INT CONF ROBOT, P7159
   Garcia R, 2002, OCEANS 2002 MTS/IEEE CONFERENCE & EXHIBITION, VOLS 1-4, CONFERENCE PROCEEDINGS, P1018
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hu K, 2022, J MAR SCI ENG, V10, DOI 10.3390/jmse10020241
   Islam MJ, 2020, Arxiv, DOI arXiv:2002.01155
   Jiang B, 2015, J REAL-TIME IMAGE PR, V10, P239, DOI 10.1007/s11554-014-0399-9
   Jiang ZY, 2022, IEEE T CIRC SYST VID, V32, P6584, DOI 10.1109/TCSVT.2022.3174817
   Li CY, 2018, IEEE SIGNAL PROC LET, V25, P323, DOI 10.1109/LSP.2018.2792050
   Li J, 2018, IEEE ROBOT AUTOM LET, V3, P387, DOI 10.1109/LRA.2017.2730363
   Li N, 2018, IEEE ACCESS, V6, P54241, DOI 10.1109/ACCESS.2018.2870854
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu RS, 2022, IEEE T IMAGE PROCESS, V31, P4922, DOI 10.1109/TIP.2022.3190209
   Loshchilov I, 2019, Arxiv, DOI [arXiv:1711.05101, 10.48550/arXiv.1711.05101, DOI 10.48550/ARXIV.1711.05101]
   Perez J, 2017, LECT NOTES COMPUT SC, V10338, P183, DOI 10.1007/978-3-319-59773-7_19
   Qi Q, 2022, IEEE T IMAGE PROCESS, V31, P6816, DOI 10.1109/TIP.2022.3216208
   Ruder S, 2017, Arxiv, DOI arXiv:1609.04747
   Sharma P, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3511021
   Song W, 2018, LECT NOTES COMPUT SC, V11164, P678, DOI 10.1007/978-3-030-00776-8_62
   Wang N, 2021, Arxiv, DOI arXiv:1912.10269
   Wang Y, 2017, IEEE IMAGE PROC, P1382, DOI 10.1109/ICIP.2017.8296508
   Xia M, 2021, INT J REMOTE SENS, V42, P2022, DOI 10.1080/01431161.2020.1849852
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Yong HW, 2020, Arxiv, DOI arXiv:2004.01461
   Zhang MH, 2019, ADV NEUR IN, V32
   Zhou Y, 2023, IEEE T NEUR NET LEAR, V34, P7719, DOI 10.1109/TNNLS.2022.3146004
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zuiderveld K., 1994, Graphics gems, DOI 10.1016/b978-0-12-336156-1.50061-6
   US
NR 33
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 17
PY 2024
DI 10.1007/s00371-024-03519-8
EA JUN 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UN1G1
UT WOS:001248641200002
DA 2024-08-05
ER

PT J
AU Hartley, M
   Mellado, N
   Fiorio, C
   Faraj, N
AF Hartley, Marc
   Mellado, Nicolas
   Fiorio, Christophe
   Faraj, Noura
TI Flexible terrain erosion
SO VISUAL COMPUTER
LA English
DT Article
DE Procedural modeling; Terrain morphing; Natural phenomena; Erosion
   processes
ID GENERATION; SIMULATION
AB In this paper, we present a novel particle-based method for simulating erosion on various terrain representations, including height fields, voxel grids, material layers, and implicit terrains. Our approach breaks down erosion into two key processes-terrain alteration and material transport-allowing for flexibility in simulation. We utilize independent particles governed by basic particle physics principles, enabling efficient parallel computation. For increased precision, a vector field can adjust particle speed, adaptable for realistic fluid simulations or user-defined control. We address material alteration in 3D terrains with a set of equations applicable across diverse models, requiring only per-particle specifications for size, density, coefficient of restitution, and sediment capacity. Our modular algorithm is versatile for real-time and offline use, suitable for both 2.5D and 3D terrains.
C1 [Hartley, Marc; Fiorio, Christophe; Faraj, Noura] Univ Montpellier, LIRMM, CNRS, Montpellier, France.
   [Mellado, Nicolas] Univ Toulouse, IRIT, CNRS, Toulouse, France.
C3 Centre National de la Recherche Scientifique (CNRS); Universite
   Paul-Valery; Universite Perpignan Via Domitia; Universite de
   Montpellier; Centre National de la Recherche Scientifique (CNRS);
   Universite de Toulouse; Universite Toulouse III - Paul Sabatier;
   Universite Federale Toulouse Midi-Pyrenees (ComUE); Institut National
   Polytechnique de Toulouse
RP Hartley, M (corresponding author), Univ Montpellier, LIRMM, CNRS, Montpellier, France.
EM marc.hartley@umontpellier.fr; nicolas.mellado@irit.fr;
   christophe.fiorio@lirmm.fr; noura.faraj@umontpellier.fr
FU Agence Nationale de la Recherche
FX No Statement Available
CR [Anonymous], 2009, Proceedings of the 2009 symposium on Interactive 3D graphics and games-I3D'09, page, DOI DOI 10.1145/1507149.1507155
   Argudo O, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417855
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Becher M., 2017, P ACM SIGGRAPH S INT, DOI DOI 10.1145/3023368.3023383
   Benes B, 2006, COMPUT ANIMAT VIRT W, V17, P99, DOI 10.1002/cav.77
   Benes B, 2001, SPRING CONFERENCE ON COMPUTER GRAPHICS, PROCEEDINGS, P80, DOI 10.1109/SCCG.2001.945341
   Caretto L.S., 1973, Proceedings of the Third International Conference on Numerical Methods in Fluid Mechanics, VII, P60, DOI [10.1007/bfb0112677, 10.1007/BFb0112677, DOI 10.1007/BFB0112677]
   Cordonnier Guillaume, 2017, ACM Transactions on Graphics, V36, DOI 10.1145/3072959.3073667
   Cordonnier G, 2018, COMPUT GRAPH FORUM, V37, P497, DOI 10.1111/cgf.13379
   Cordonnier G, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592422
   Cordonnier G, 2018, IEEE T VIS COMPUT GR, V24, P1756, DOI 10.1109/TVCG.2017.2689022
   Cordonnier G, 2016, COMPUT GRAPH FORUM, V35, P165, DOI 10.1111/cgf.12820
   Dey R, 2018, ENTERTAIN COMPUT, V27, P128, DOI 10.1016/j.entcom.2018.04.003
   Eisemann Elmar., 2008, Proceedings of Graphics Interface, P73
   Emilien A, 2015, COMPUT GRAPH FORUM, V34, P22, DOI 10.1111/cgf.12515
   Galin E, 2019, COMPUT GRAPH FORUM, V38, P553, DOI 10.1111/cgf.13657
   Guérin E, 2022, COMPUT GRAPH FORUM, V41, P85, DOI 10.1111/cgf.14460
   Guérin E, 2016, COMPUT GRAPH FORUM, V35, P177, DOI 10.1111/cgf.12821
   Hong QQ, 2013, 2013 6TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), VOLS 1-3, P686, DOI 10.1109/CISP.2013.6745253
   Ito T, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P244, DOI 10.1109/CGI.2003.1214475
   Jones BD, 2017, ENG COMPUTATION, V34, P1204, DOI 10.1108/EC-02-2016-0052
   Jones MD, 2010, IEEE T VIS COMPUT GR, V16, P81, DOI 10.1109/TVCG.2009.39
   KAUFMAN A, 1993, COMPUTER, V26, P51, DOI 10.1109/MC.1993.274942
   Koschier D, 2022, COMPUT GRAPH FORUM, V41, P737, DOI 10.1111/cgf.14508
   Kristof P, 2009, COMPUT GRAPH FORUM, V28, P219, DOI 10.1111/j.1467-8659.2009.01361.x
   Lengyel E., 2010, VOXEL BASED TERRAIN, P148
   Mei X, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P47, DOI 10.1109/PG.2007.15
   Musgrave F. K., 1989, Computer Graphics, V23, P41, DOI 10.1145/74334.74337
   Neidhold B., 2005, P EUR WORKSH NAT PHE, P25, DOI [DOI 10.2312/NPH/NPH05/025-032, 10.2312/NPH/NPH05/025-032doi.org/10.2312/NPH/NPH05/025-032, DOI 10.2312/NPH/NPH05/025-032DOI.ORG/10.2312/NPH/NPH05/025-032]
   O'Brien J. F., 1995, Proceedings. Computer Animation '95, P198, DOI 10.1109/CA.1995.393532
   Olsen J., 2004, Realtime procedural terrain generation, P20
   Onoue K, 2000, EIGHTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P427, DOI 10.1109/PCCGA.2000.883978
   Paris A, 2021, COMPUT GRAPH FORUM, V40, P277, DOI 10.1111/cgf.14420
   Paris A, 2019, COMPUT GRAPH FORUM, V38, P47, DOI 10.1111/cgf.13815
   Paris A, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3342765
   Peytavie A, 2009, COMPUT GRAPH FORUM, V28, P457, DOI 10.1111/j.1467-8659.2009.01385.x
   RANZ WE, 1960, AICHE J, V6, P124, DOI 10.1002/aic.690060123
   RICHARDSON JF, 1954, CHEM ENG SCI, V3, P65, DOI 10.1016/0009-2509(54)85015-9
   Rigaudiere D., 2000, METHODS
   Roa T., 2004, PROCEEDINGS, P17
   Roose D., 2011, DYNAMIC REFINEMENT F
   Roudier P., 1993, Computer Graphics Forum, V12, P375, DOI 10.1111/1467-8659.1230375
   Schott H., 2023, LARGE SCALE TERRAIN
   Smelik R.M., 2009, P CASA WORKSH 3D ADV
   Stachniak S., 2005, Computer Graphics and Artificial Intelligence, V1, P64
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Stam J, 2003, ACM T GRAPHIC, V22, P724, DOI 10.1145/882262.882338
   Stokes G.G., 2009, On the Effect of the Internal Friction of Fluids on the Motion of Pendulums, P1, DOI [10.1017/CBO9780511702266.002, DOI 10.1017/CBO9780511702266.002]
   SWOPE WC, 1982, J CHEM PHYS, V76, P637, DOI 10.1063/1.442716
   Tychonievich LA, 2010, VISUAL COMPUT, V26, P1485, DOI 10.1007/s00371-010-0506-2
   VERLET L, 1967, PHYS REV, V159, P98, DOI 10.1103/PhysRev.159.98
   Wojtan Christopher., 2007, Eurographics Workshop on Natural Phenomena, P15, DOI DOI 10.2312/NPH/NPH07/015-022
   Yan P, 2020, COMPUT GEOTECH, V122, DOI 10.1016/j.compgeo.2020.103511
NR 53
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4593
EP 4607
DI 10.1007/s00371-024-03444-w
EA JUN 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001242155100001
DA 2024-08-05
ER

PT J
AU Duan, X
   Cao, Y
   Zhang, RJ
   Wang, X
   Li, P
AF Duan, Xin
   Cao, Yu
   Zhang, Renjie
   Wang, Xin
   Li, Ping
TI Shadow-aware image colorization
SO VISUAL COMPUTER
LA English
DT Article
DE Colorization; Shadow detection; Transformer
ID COLOR
AB Significant advancements have been made in colorization in recent years, especially with the introduction of deep learning technology. However, challenges remain in accurately colorizing images under certain lighting conditions, such as shadow. Shadows often cause distortions and inaccuracies in object recognition and visual data interpretation, impacting the reliability and effectiveness of colorization techniques. These problems often lead to unsaturated colors in shadowed images and incorrect colorization of shadows as objects. Our research proposes the first shadow-aware image colorization method, addressing two key challenges that previous studies have overlooked: integrating shadow information with general semantic understanding and preserving saturated colors while accurately colorizing shadow areas. To tackle these challenges, we develop a dual-branch shadow-aware colorization network. Additionally, we introduce our shadow-aware block, an innovative mechanism that seamlessly integrates shadow-specific information into the colorization process, distinguishing between shadow and non-shadow areas. This research significantly improves the accuracy and realism of image colorization, particularly in shadow scenarios, thereby enhancing the practical application of colorization in real-world scenarios.
C1 [Duan, Xin; Zhang, Renjie; Wang, Xin; Li, Ping] Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong, Peoples R China.
   [Cao, Yu] Hong Kong Polytech Univ, Sch Fash & Text, Kowloon, Hong Kong, Peoples R China.
   [Li, Ping] Hong Kong Polytech Univ, Sch Design, Kowloon, Hong Kong, Peoples R China.
C3 Hong Kong Polytechnic University; Hong Kong Polytechnic University; Hong
   Kong Polytechnic University
RP Li, P (corresponding author), Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong, Peoples R China.; Li, P (corresponding author), Hong Kong Polytech Univ, Sch Design, Kowloon, Hong Kong, Peoples R China.
EM hizuka.duan@connect.polyu.hk; yu-daniel.cao@connect.polyu.hk;
   renjie.zhang@connect.polyu.hk; xin1025.wang@connect.polyu.hk;
   p.li@polyu.edu.hk
OI Cao, Yu (Daniel)/0000-0002-9761-0723
FU Hong Kong Polytechnic University;  [P0048387];  [P0042740];  [P0044520];
    [P0043906];  [P0049586];  [P0050657]
FX The authors would like to thank the editors and anonymous reviewers for
   their insightful comments and suggestions. This work was supported by
   The Hong Kong Polytechnic University under Grants P0048387, P0042740,
   P0044520, P0043906, P0049586, and P0050657.
CR Akita K, 2020, COMPUT GRAPH FORUM, V39, P601, DOI 10.1111/cgf.14171
   [Anonymous], 2016, ACM T GRAPHIC, DOI DOI 10.1145/2897824.2925974
   Assarsson U, 2003, ACM T GRAPHIC, V22, P511, DOI 10.1145/882262.882300
   Baba M., 2004, P ACM SIGGRAPH, P60
   Bahng H, 2018, LECT NOTES COMPUT SC, V11216, P443, DOI 10.1007/978-3-030-01258-8_27
   Caesar H, 2018, PROC CVPR IEEE, P1209, DOI 10.1109/CVPR.2018.00132
   Cao Yu, 2024, IEEE Trans Vis Comput Graph, VPP, DOI 10.1109/TVCG.2024.3357568
   Cao Y, 2023, IEEE INT CON MULTI, P1637, DOI 10.1109/ICME55011.2023.00282
   Charpiat G, 2008, LECT NOTES COMPUT SC, V5304, P126, DOI 10.1007/978-3-540-88690-7_10
   Chen JB, 2018, PROC CVPR IEEE, P8721, DOI 10.1109/CVPR.2018.00909
   Chen ZH, 2021, PROC CVPR IEEE, P2714, DOI 10.1109/CVPR46437.2021.00274
   Chen ZH, 2020, PROC CVPR IEEE, P5610, DOI 10.1109/CVPR42600.2020.00565
   Cheng ZZ, 2015, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2015.55
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deshpande A, 2017, PROC CVPR IEEE, P2877, DOI 10.1109/CVPR.2017.307
   Deshpande A, 2015, IEEE I CONF COMP VIS, P567, DOI 10.1109/ICCV.2015.72
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Fang FM, 2020, IEEE T VIS COMPUT GR, V26, P2931, DOI 10.1109/TVCG.2019.2908363
   Glorot X., 2010, P 13 INT C ART INT S, P249
   Gupta Raj Kumar, 2012, P 20 ACM INT C MULT, P369, DOI DOI 10.1145/2393347.2393402
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He MM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201365
   Hou L, 2021, IEEE T PATTERN ANAL, V43, P1337, DOI 10.1109/TPAMI.2019.2948011
   Hou YZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1673, DOI 10.1145/3474085.3475310
   Hu XW, 2021, IEEE T IMAGE PROCESS, V30, P1925, DOI 10.1109/TIP.2021.3049331
   Huang ZT, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555471
   Ironi Revital, 2005, RENDERING TECHNIQUES, V29, P201, DOI DOI 10.2312/EGWR/EGSR05/201-210
   Jheng-Wei Su, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7965, DOI 10.1109/CVPR42600.2020.00799
   Ji XZ, 2022, LECT NOTES COMPUT SC, V13676, P20, DOI 10.1007/978-3-031-19787-1_2
   Kang XY, 2023, IEEE I CONF COMP VIS, P328, DOI 10.1109/ICCV51070.2023.00037
   Kim E., 2021, IEEE INT C COMP VIS, p14,647
   Kim G, 2022, LECT NOTES COMPUT SC, V13667, P350, DOI 10.1007/978-3-031-20071-7_21
   Kumar MD, 2022, INT J WATER RESOUR D, V38, P306, DOI 10.1080/07900627.2021.1899900
   Larsson G, 2016, LECT NOTES COMPUT SC, V9908, P577, DOI 10.1007/978-3-319-46493-0_35
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Liu L., 2023, IEEE C COMP VIS PATT, p10,449
   Liu XP, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409105
   Lu X, 2022, PROC CVPR IEEE, P3106, DOI 10.1109/CVPR52688.2022.00312
   Messaoud S, 2018, LECT NOTES COMPUT SC, V11210, P603, DOI 10.1007/978-3-030-01231-1_37
   Qu YG, 2006, ACM T GRAPHIC, V25, P1214, DOI 10.1145/1141911.1142017
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sykora D, 2009, COMPUT GRAPH FORUM, V28, P599, DOI 10.1111/j.1467-8659.2009.01400.x
   Vasluianu Florin-Alexandru, 2023, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P1826, DOI 10.1109/CVPRW59228.2023.00181
   Wang JF, 2018, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR.2018.00192
   Wang Y, 2022, LECT NOTES COMPUT SC, V13675, P271, DOI 10.1007/978-3-031-19784-0_16
   Welsh T, 2002, ACM T GRAPHIC, V21, P277, DOI 10.1145/566570.566576
   Weng SC, 2022, LECT NOTES COMPUT SC, V13667, P1, DOI 10.1007/978-3-031-20071-7_1
   Wu YZ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14357, DOI 10.1109/ICCV48922.2021.01411
   Xia MH, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555432
   Xiao CF, 2020, COMPUT GRAPH FORUM, V39, P20, DOI 10.1111/cgf.13659
   Xiao Y, 2022, IEEE T VIS COMPUT GR, V28, P1557, DOI 10.1109/TVCG.2020.3021510
   Xie EZ, 2021, ADV NEUR IN, V34
   Xie XY, 2023, ASIA-PAC EDUC RES, V32, P439, DOI 10.1007/s40299-022-00665-2
   Zhang LM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275090
   Zhang R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073703
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhao JJ, 2020, INT J COMPUT VISION, V128, P818, DOI 10.1007/s11263-019-01271-4
   Zheng QL, 2019, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2019.00531
   Zhu J, 2010, ASIA S PACIF DES AUT, P220
   Zou CQ, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356561
NR 60
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4969
EP 4979
DI 10.1007/s00371-024-03500-5
EA JUN 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001238606000001
OA hybrid
DA 2024-08-05
ER

PT J
AU Sun, LB
   Li, YF
   Qin, WH
AF Sun, Libo
   Li, Yifan
   Qin, Wenhu
TI PEPillar: a point-enhanced pillar network for efficient 3D object
   detection in autonomous driving
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE 3D object detection; Feature fusion; Point clouds; Pillar
AB Pillar-based 3D object detection methods outperform traditional point-based and voxel-based methods in terms of speed. However, most of recent methods in this category use simple aggregation techniques to construct pillar feature maps, which leads to a significant loss of raw point cloud detail and a decrease in detection accuracy. Given the critical demand for both rapid response and high precision in autonomous driving, we introduce PEPillar, an innovative 3D object detection method that adopts point cloud data fusion. Concretely, we firstly use the Point-Enhanced Pillar module to learn pillar and keypoints features from the input data. Then attention mechanism is employed to seamlessly integrate features from multiple sources, which improves the model's ability to detect various objects and demonstrates robustness in complex scenarios. Benefiting from the simplicity of pillar representation, PEPillar can use established 2D convolutional neural networks to solve the challenges in backbone network redesign. The Multi-Receptive Field Neck is introduced to enhance the detection accuracy of smaller objects. Additionally, we design the model into a faster single-stage and a more precise two-stage format to meet various requirements. The results of the evaluation indicate a 5.14% improvement of our method compared to the baseline model in the moderately difficult car detection task, achieving levels comparable to state-of-the-art methods that use point and voxel representations.
C1 [Sun, Libo; Li, Yifan; Qin, Wenhu] Southeast Univ, Sch Instrument Sci & Engn, Nanjing 210096, Peoples R China.
C3 Southeast University - China
RP Sun, LB (corresponding author), Southeast Univ, Sch Instrument Sci & Engn, Nanjing 210096, Peoples R China.
EM sunlibo@seu.edu.cn; 220223373@seu.edu.cn; qinwenhu@seu.edu.cn
FU the Key R&D Program of Jiangsu Province [BE2023010-3]; Key R &D Program
   of Jiangsu Province [CX(23)3120]; Jiangsu modern agricultural industry
   single technology research and development project; Advanced Computing
   and Intelligent Engineering (National Level) Laboratory Fund
FX This work was supported by the Key R &D Program of Jiangsu Province
   under Grant BE2023010-3, Jiangsu modern agricultural industry single
   technology research and development project under Grant CX(23)3120 and
   Advanced Computing and Intelligent Engineering (National Level)
   Laboratory Fund.
CR Bahdanau D, 2016, Arxiv, DOI arXiv:1409.0473
   Beltrán J, 2018, IEEE INT C INTELL TR, P3517, DOI 10.1109/ITSC.2018.8569311
   Chen ZD, 2023, IEEE T PATTERN ANAL, V45, P5158, DOI 10.1109/TPAMI.2022.3195759
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Deng JJ, 2021, AAAI CONF ARTIF INTE, V35, P1201
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Eldar Y, 1997, IEEE T IMAGE PROCESS, V6, P1305, DOI 10.1109/83.623193
   Engelcke Martin, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P1355, DOI 10.1109/ICRA.2017.7989161
   Fan L, 2022, PROC CVPR IEEE, P8448, DOI 10.1109/CVPR52688.2022.00827
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Graham B, 2017, Arxiv, DOI arXiv:1706.01307
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang ZC, 2023, PATTERN RECOGN LETT, V168, P138, DOI 10.1016/j.patrec.2023.03.016
   Kingma D. P., 2014, arXiv
   Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298
   Le DT, 2023, IEEE ROBOT AUTOM LET, V8, P1159, DOI 10.1109/LRA.2022.3233234
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Mao JG, 2023, INT J COMPUT VISION, V131, P1909, DOI 10.1007/s11263-023-01790-1
   Pan XR, 2021, PROC CVPR IEEE, P7459, DOI 10.1109/CVPR46437.2021.00738
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054
   Shen Y, 2023, Arxiv, DOI arXiv:2303.02000
   Shi GS, 2022, Arxiv, DOI arXiv:2205.07403
   Shi SS, 2023, INT J COMPUT VISION, V131, P531, DOI 10.1007/s11263-022-01710-9
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Shi SS, 2021, IEEE T PATTERN ANAL, V43, P2647, DOI 10.1109/TPAMI.2020.2977026
   Shi WJ, 2020, PROC CVPR IEEE, P1708, DOI 10.1109/CVPR42600.2020.00178
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Team O. D., 2020, OPENPCDET OPEN SOURC
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Yue, 2020, ECCV
   Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337
   Yang B, 2018, PROC CVPR IEEE, P7652, DOI 10.1109/CVPR.2018.00798
   Yang ZT, 2019, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2019.00204
   Yin TW, 2021, PROC CVPR IEEE, P11779, DOI 10.1109/CVPR46437.2021.01161
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zheng YZ, 2023, IEEE T CIRC SYST VID, V33, P1671, DOI 10.1109/TCSVT.2022.3212987
   Zhou SF, 2023, Arxiv, DOI arXiv:2302.02367
   Zhou XY, 2019, Arxiv, DOI arXiv:1904.07850
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
   Zhou Yin, 2020, CORL
NR 48
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 30
PY 2024
DI 10.1007/s00371-024-03481-5
EA MAY 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO8V9
UT WOS:001235494300005
DA 2024-08-05
ER

PT J
AU Liu, QP
   Lu, ZW
   Gao, RX
   Bu, XH
   Hanajima, N
AF Liu, Qunpo
   Lu, Zhiwei
   Gao, Ruxin
   Bu, Xuhui
   Hanajima, Naohiko
TI SimpleMask: parameter link and efficient instance segmentation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Instance segmentation; ResNet50; Visual attention network; U-Net;
   Parameter-link loss
ID IMAGE
AB To resolve the problem that the segmentation result of the full convolutional neural network in the Mask R-CNN model is not fine enough, and that the number of loss function hyperparameters is too large, leadings to the time and resource consumption of parameter adjustment, we propose a parameter link and efficient instance segmentation model in this paper. Aiming at the problem that the Mask R-CNN model does not pay attention to sample features, the method of fusing the visual attention network in the ResNet50 backbone network is adopted to achieve self-adaptation and long-range correlation in self-attention, so that the model can precisely recognize the target location and effectively detect and segment the target. The U-Net network is introduced into the segmentation, and the image is processed by stepwise upsampling and downsampling, so that the network segmentation accuracy for the pixel mask is more accurate. Considering the parameter tuning problem of the instance segmentation task, a parameter link loss is recommended to simplify the complexity of model training parameter tuning and further enhance the detection and segmentation performance of the model. We conduct extensive experiments on three extensive baselines, i.e., MiniCOCO, Cityscapes and PASCAL VOC2012, to assess the validity of our model. The experimental findings demonstrate that (1) in the MiniCOCO dataset, a box AP of 35.1 and a mask AP of 32.0 are obtained. Compared with the most advanced mask2former algorithm, the box AP and mask AP are 1.7 and 2.2 higher, respectively. (2) The AP value on Cityscapes is 38.1. In comparison with alternative instance segmentation models, the mAP of each category has been greatly improved. (3) The generalization experiment of our model on the PASCAL VOC2012 dataset shows that the box mAP and mask mAP are 75.5 and 63.6, respectively, which are improved by 3.9 and 1.9, respectively, when contrasting with the Mask R-CNN model. Our model has significant advantages in both detection and segmentation. The code will be available at https://gitee.com/zhiweilu111/simple-mask/tree/master.
C1 [Liu, Qunpo; Lu, Zhiwei; Gao, Ruxin; Bu, Xuhui] Henan Polytech Univ, Sch Elect Engn & Automat, Jiaozuo, Henan, Peoples R China.
   [Liu, Qunpo; Gao, Ruxin; Bu, Xuhui; Hanajima, Naohiko] Henan Int Joint Lab Direct Drive & Control Intelli, Jiaozuo, Henan, Peoples R China.
   [Hanajima, Naohiko] Muroran Inst Technol, Coll Informat & Syst, 27-1 Mizumoto cho, Muroran, Hokkaido 0508585, Japan.
C3 Henan Polytechnic University; Muroran Institute of Technology
RP Liu, QP (corresponding author), Henan Polytech Univ, Sch Elect Engn & Automat, Jiaozuo, Henan, Peoples R China.; Liu, QP (corresponding author), Henan Int Joint Lab Direct Drive & Control Intelli, Jiaozuo, Henan, Peoples R China.
EM lqpny@hpu.edu.cn
FU The National Natural Science Foundation of China [U1804147]; National
   Natural Science Foundation of China [20IRTSTHN019]; Innovative
   Scientists and Technicians Team of Henan Provincial High Education
   [212102210508]; Science and Technology Project of Henan Province
FX This work is partially supported by the National Natural Science
   Foundation of China (No. U1804147), Innovative Scientists and
   Technicians Team of Henan Provincial High Education (20IRTSTHN019),
   Science and Technology Project of Henan Province (No.212102210508).
CR Acuna D, 2018, PROC CVPR IEEE, P859, DOI 10.1109/CVPR.2018.00096
   Ahn J, 2019, PROC CVPR IEEE, P2204, DOI 10.1109/CVPR.2019.00231
   Arnab A, 2017, PROC CVPR IEEE, P879, DOI 10.1109/CVPR.2017.100
   Arun Aditya, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P254, DOI 10.1007/978-3-030-58604-1_16
   Bai M, 2017, PROC CVPR IEEE, P2858, DOI 10.1109/CVPR.2017.305
   Bolya D, 2022, IEEE T PATTERN ANAL, V44, P1108, DOI 10.1109/TPAMI.2020.3014297
   Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925
   Cai ZW, 2021, IEEE T PATTERN ANAL, V43, P1483, DOI 10.1109/TPAMI.2019.2956516
   Chen H., 2020, CVPR, DOI DOI 10.1109/CVPR42600.2020.00860
   Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Chen KA, 2021, IEEE T PATTERN ANAL, V43, P3782, DOI 10.1109/TPAMI.2020.2991457
   Chen XL, 2019, IEEE I CONF COMP VIS, P2061, DOI 10.1109/ICCV.2019.00215
   Cheng BW, 2022, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR52688.2022.00135
   Cholakkal H, 2019, PROC CVPR IEEE, P12389, DOI 10.1109/CVPR.2019.01268
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343
   Dai JF, 2016, LECT NOTES COMPUT SC, V9910, P534, DOI 10.1007/978-3-319-46466-4_32
   Dong Bo, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P2131, DOI 10.1145/3581783.3612185
   Dou WH, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2100
   Du XZ, 2021, Arxiv, DOI arXiv:2107.00057
   Duan KW, 2021, Arxiv, DOI arXiv:2104.04899
   Enze Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12190, DOI 10.1109/CVPR42600.2020.01221
   Fang YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6890, DOI 10.1109/ICCV48922.2021.00683
   Gao Y, 2023, VISUAL COMPUT, V39, P1137, DOI 10.1007/s00371-021-02393-y
   Ge WF, 2019, IEEE I CONF COMP VIS, P3344, DOI 10.1109/ICCV.2019.00344
   Ghiasi G, 2021, PROC CVPR IEEE, P2917, DOI 10.1109/CVPR46437.2021.00294
   Guo MH, 2022, Arxiv, DOI arXiv:2202.09741
   Guo RH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7137, DOI 10.1109/ICCV48922.2021.00707
   Hayder Z, 2017, PROC CVPR IEEE, P587, DOI 10.1109/CVPR.2017.70
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hsu CC, 2019, ADV NEUR IN, V32
   Hu J, 2021, Arxiv, DOI [arXiv:2105.00637, 10.48550/arXiv.2105.00637]
   Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657
   Hwang J, 2021, IEEE WINT CONF APPL, P1019, DOI 10.1109/WACV48630.2021.00106
   Khoreva A, 2017, PROC CVPR IEEE, P1665, DOI 10.1109/CVPR.2017.181
   Kim H.Y., 2018, CoRR abs/1810.10327
   Kirillov A., 2020, P IEEE CVF C COMP VI, P9799, DOI DOI 10.1109/CVPR42600.2020.00982
   Kirillov A, 2017, PROC CVPR IEEE, P7322, DOI 10.1109/CVPR.2017.774
   Lee J, 2021, PROC CVPR IEEE, P2643
   Lee Y., 2020, CVPR, P13906
   Li F, 2023, PROC CVPR IEEE, P3041, DOI 10.1109/CVPR52729.2023.00297
   Li XM, 2022, VISUAL COMPUT, V38, P3027, DOI 10.1007/s00371-021-02174-7
   Liao SS, 2019, INT CONF ACOUST SPEE, P1917, DOI 10.1109/ICASSP.2019.8682309
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu S, 2017, IEEE I CONF COMP VIS, P3516, DOI 10.1109/ICCV.2017.378
   Liu YD, 2018, LECT NOTES COMPUT SC, V11207, P708, DOI 10.1007/978-3-030-01219-9_42
   Liu Y, 2022, IEEE T PATTERN ANAL, V44, P1415, DOI 10.1109/TPAMI.2020.3023152
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Minaee S, 2022, IEEE T PATTERN ANAL, V44, P3523, DOI 10.1109/TPAMI.2021.3059968
   Mottaghi R, 2014, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2014.119
   Neven D, 2019, PROC CVPR IEEE, P8829, DOI 10.1109/CVPR.2019.00904
   Oksuz K, 2020, P ADV NEUR INF PROC, V33, P15534
   Oksuz K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2989, DOI 10.1109/ICCV48922.2021.00300
   Oksuz K, 2021, IEEE T PATTERN ANAL, V43, P3388, DOI 10.1109/TPAMI.2020.2981890
   Pei JL, 2022, LECT NOTES COMPUT SC, V13678, P19, DOI 10.1007/978-3-031-19797-0_2
   Pei JL, 2023, IEEE T MULTIMEDIA, V25, P1964, DOI 10.1109/TMM.2022.3141891
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rufeng Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10223, DOI 10.1109/CVPR42600.2020.01024
   Samet Nermin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P406, DOI 10.1007/978-3-030-58595-2_25
   Sofiiuk K, 2019, IEEE I CONF COMP VIS, P7354, DOI 10.1109/ICCV.2019.00745
   Sun YQ, 2020, IEEE ACCESS, V8, P24135, DOI 10.1109/ACCESS.2020.2969480
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tianheng Cheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P660, DOI 10.1007/978-3-030-58568-6_39
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Vaswani A, 2017, ADV NEUR IN, V30
   Vuola AO, 2019, I S BIOMED IMAGING, P208, DOI [10.1109/ISBI.2019.8759574, 10.1109/isbi.2019.8759574]
   Wang SR, 2020, AAAI CONF ARTIF INTE, V34, P12208
   Wang X., 2020, INT C NEURAL INF PRO, V33, P17721
   Wu HS, 2021, COMPUT ANIMAT VIRT W, V32, DOI 10.1002/cav.2023
   Xinlong Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P649, DOI 10.1007/978-3-030-58523-5_38
   Xiong YW, 2019, PROC CVPR IEEE, P8810, DOI 10.1109/CVPR.2019.00902
   Yan G, 2024, VISUAL COMPUT, V40, P2203, DOI 10.1007/s00371-023-02911-0
   Yang Z, 2023, VISUAL COMPUT, V39, P3937, DOI 10.1007/s00371-022-02537-8
   Ying H, 2019, Arxiv, DOI arXiv:1912.01954
   Zha HF, 2021, COMPUT ANIMAT VIRT W, V32, DOI 10.1002/cav.2022
   Zhang G, 2021, PROC CVPR IEEE, P6857, DOI 10.1109/CVPR46437.2021.00679
   Zhi Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P282, DOI 10.1007/978-3-030-58452-8_17
   Zhou YZ, 2018, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2018.00399
   Zhu Y, 2019, PROC CVPR IEEE, P3111, DOI 10.1109/CVPR.2019.00323
NR 84
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 24
PY 2024
DI 10.1007/s00371-024-03451-x
EA MAY 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL4M0
UT WOS:001234594700003
DA 2024-08-05
ER

PT J
AU Estrada, JG
   Prasolova-Forland, E
   Kjeksrud, S
   Themelis, C
   Lindqvist, P
   Kvam, K
   Midthun, O
   Sverre, K
   Hokstad, LM
   Mohamed, SK
   Grassini, S
   Ricci, S
AF Estrada, Jose Garcia
   Prasolova-Forland, Ekaterina
   Kjeksrud, Stian
   Themelis, Chryssa
   Lindqvist, Petter
   Kvam, Kristine
   Midthun, Ole
   Sverre, Knut
   Hokstad, Leif Martin
   Mohamed, Soud Khalifa
   Grassini, Simone
   Ricci, Serena
TI Military education in extended reality (XR): learning troublesome
   knowledge through immersive experiential application
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Extended reality; XR; Embodiment; Military education; Threat-based
   approach; Empathy; Threshold concepts
ID EMPATHY
AB Extended reality (XR) applications for education are usually associated with motivation, engagement, knowledge and experiences that are difficult to achieve in the real world. One example of such knowledge in military education is the threat-based approach to protection of civilians in armed conflicts. The Norwegian Defence University College (NDUC) educates officers on the threat-based approach. This approach challenges conventional military practice, introducing new ways of thinking about what military forces can and cannot do to protect civilians from violence. Today, militaries are expected to protect civilians from perpetrators that target them as part of their warfare, expanding their responsibilities under International Humanitarian Law. This expansion of tasks represents "troublesome knowledge" for many military officers and demands a deeper understanding of the rationales and strategies driving perpetrators' targeting of civilians. To overcome this pedagogical challenge, we developed an XR-supported educational program combining immersive experiences and pedagogical approaches, including combined 360 degrees-videos and embodiment, dialogue with virtual humans and collaborative VR-landscapes to encourage immersive peer-to-peer learning. The results are encouraging, suggesting high levels of acceptance by learners of the threat-based concept, an effect on stimulating critical discussion and a positive reaction to XR-supported learning.
C1 [Estrada, Jose Garcia] NTNU UAS Tech Wien, Vienna, Austria.
   [Prasolova-Forland, Ekaterina; Themelis, Chryssa; Hokstad, Leif Martin; Mohamed, Soud Khalifa] Norwegian Univ Sci & Technol NTNU, Trondheim, Norway.
   [Kjeksrud, Stian; Lindqvist, Petter] Norwegian Def Univ Coll NDUC, Oslo, Norway.
   [Kvam, Kristine; Midthun, Ole] Fynd Real, Hamar, Norway.
   [Sverre, Knut] TRY, Oslo, Norway.
   [Grassini, Simone] Univ Bergen, NTNU, Bergen, Norway.
   [Ricci, Serena] Univ Genoa, NTNU, Genoa, Italy.
C3 Norwegian University of Science & Technology (NTNU); University of
   Bergen; University of Genoa
RP Estrada, JG (corresponding author), NTNU UAS Tech Wien, Vienna, Austria.
EM jose.garcia@technikum-wien.at; ekaterip@ntnu.no; skjeksrud@mil.no;
   chrysoula.themeli@ntnu.no; phfl@online.no;
   kristine.kvam@fyndreality.com; ole.midthun@gmail.com; knut@try.no;
   leif.Hokstad@ntnu.no; soud.k.mohamed@ntnu.no; simone.grassini@uib.no;
   Serena.Ricci@unige.it
OI RICCI, SERENA/0000-0002-9042-6341
FU NDUC; Norwegian Ministry of Defence
FX The authors would like to thank the participants of the study as well as
   NDUC and Norwegian Ministry of Defence for providing funding and
   organizational support. Many thanks to Syver Lauritzen for providing
   input to the technical part of the paper. Special thanks to Rune Solberg
   for practical organization of the trial and Guido Makransky for
   providing valuable inputs to the design of the questionnaire as well as
   to Mel Slater for insightful feedback on the implementation of
   embodiment in this project.
CR Ahir K., 2020, Augmented Human Research, V5, P7, DOI 10.1007/s41133-019-0025-2
   [Anonymous], 2022, Forsvaret: armed forces in numbers
   [Anonymous], 2011, Researchomatic: Empathy in the military
   Baceviciute S, 2022, J COMPUT ASSIST LEAR, V38, P470, DOI 10.1111/jcal.12630
   Banakou D, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.00917
   BANDURA A, 1989, DEV PSYCHOL, V25, P729, DOI 10.1037/0012-1649.25.5.729
   Bangor A, 2008, INT J HUM-COMPUT INT, V24, P574, DOI 10.1080/10447310802205776
   Beadle AW, 2018, GLOB INST, P100
   Beadle AW., 2015, International Military Operations in the 21st Century: Global Trends and the Future of Intervention. Cass Military Studies
   Braithwaite JJ., 2013, Psychophysiology, V49, P1017, DOI DOI 10.1111/J.1469-8986.2012.01384.X
   Cameron CD, 2018, SOC PERSONAL PSYCHOL, V12, DOI 10.1111/spc3.12418
   Chen YC, 2011, J PROF ISS ENG ED PR, V137, P267, DOI 10.1061/(ASCE)EI.1943-5541.0000078
   Corriette B, 2023, 2023 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS, VRW, P187, DOI 10.1109/VRW58643.2023.00046
   Cousin G., 2006, PLANET, V17, P4, DOI [DOI 10.11120/PLAN.2006.00170004, 10.11120/plan.2006.00170004]
   Crilly N, 2015, DESIGN STUD, V38, P54, DOI 10.1016/j.destud.2015.01.002
   Davies S, 2022, J PEACE RES, V59, P593, DOI 10.1177/00223433221108428
   Decety Jean, 2004, Behav Cogn Neurosci Rev, V3, P71, DOI 10.1177/1534582304267187
   Decety J, 2014, TRENDS COGN SCI, V18, P337, DOI 10.1016/j.tics.2014.04.008
   Estrada Jose Garcia, 2022, 2022 International Conference on Cyberworlds (CW), P195, DOI 10.1109/CW55638.2022.00047
   Evans C., 2015, Engaged student learning: High impact strategies to enhance student achievement
   Fernandez Ana Alipass, 2023, 2023 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct), P667, DOI 10.1109/ISMAR-Adjunct60411.2023.00143
   Ferreira JMM, 2020, IEEE GLOB ENG EDUC C, P913, DOI [10.1109/educon45650.2020.9125346, 10.1109/EDUCON45650.2020.9125346]
   Garcia Estrada J.F., 2023, P 29 ACM S VIRT REAL, P1
   Girardi R, 2019, SYMP VIRTUAL AUGMENT, P25, DOI 10.1109/SVR.2019.00021
   Greco A, 2017, IEEE SENS J, V17, P716, DOI 10.1109/JSEN.2016.2623677
   Greenwood AT, 2016, AMB INTELL SMART ENV, V21, P249, DOI 10.3233/978-1-61499-690-3-249
   Herrera F, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0204494
   Kay J., 2020, Radical uncertainty: decision-making beyond the numbers
   Khooshabeh P, 2017, P IEEE VIRT REAL ANN, P333, DOI 10.1109/VR.2017.7892312
   Kim H, 2021, FRONT PSYCHIATRY, V12, DOI 10.3389/fpsyt.2021.614539
   Kjeksrud S., 2023, USING FORCE PROTECT, DOI [10.1093/oso/9780192857101.001.0001, DOI 10.1093/OSO/9780192857101.001.0001]
   Kjeksrud S., 2016, Protecting civilians from violence: a threat-based approach to protection of civilians in un peace operations
   Kukulska-Hulme A., 2021, Innovating Pedagogy 2021
   Liu Y, 2018, BEHAV BRAIN RES, V341, P50, DOI 10.1016/j.bbr.2017.12.021
   Makransky G, 2022, J COMPUT ASSIST LEAR, V38, P1127, DOI 10.1111/jcal.12670
   Mangina E., 2021, The IEEE global initiative on ethics of extended reality (XR) report-extended reality (XR) ethics in education, P1
   Mao CC, 2017, ICIET'17: PROCEEDINGS OF THE 5TH INTERNATIONAL CONFERENCE ON INFORMATION AND EDUCATION TECHNOLOGY, P73, DOI 10.1145/3029387.3029418
   NATO, 2016, NATO Policy for the Protection of Civilians
   Neumann DL, 2015, MEASURES OF PERSONALITY AND SOCIAL PSYCHOLOGICAL CONSTRUCTS, P257, DOI 10.1016/B978-0-12-386915-9.00010-3
   Neyret S, 2020, FRONT ROBOT AI, V7, DOI 10.3389/frobt.2020.00031
   Palmas F, 2020, IEEE INT CONF ADV LE, P322, DOI 10.1109/ICALT49669.2020.00103
   Palmer K., 2018, Harv. Bus. Rev, V8, P1
   Plechata Adela, 2022, EXPERIENCING HERD IM
   Rahma ON, 2022, J MED SIGNALS SENS, V12, P155, DOI 10.4103/jmss.JMSS_78_20
   Regal G, 2023, MULTIMODAL TECHNOLOG, V7, DOI 10.3390/mti7090088
   Rogers J., 2018, Technical report
   Shen LJ, 2010, WESTERN J COMM, V74, P504, DOI 10.1080/10570314.2010.512278
   Slater M, 2016, FRONT ROBOT AI, V3, DOI 10.3389/frobt.2016.00074
   Sonkin D, 2013, PARTN ABUSE, V4, P287, DOI 10.1891/1946-6560.4.2.287
   Statistics Norway, 2022, Population
   STE, 2021, Synthetic Training Environment Cross-Functional Team
   Steven Lonard, 2023, Procedia Computer Science, P892, DOI 10.1016/j.procs.2023.10.596
   Stevens J., 2014, P 2014 SUMM SIM MULT, P1
   Surface E., 2007, technical report no. 2007010602
   Turner V., 1990, By means of performance: intercultural studies of theatre and ritual, V9
   Tzu S., 2008, Strategic Studies, P63
   U. DPO, 2019, The protection of civilians in United Nations peacekeeping
   Ukrainian Military Center, 2023, Ukrainian NLAW and MANPADS operators are being trained using interactive shooting ranges
   UNHCR, 2022, Global trends in forced displacement 2021
   USArmy, 2012, army doctrine reference publication (adrp) 6-22 army leadership
   Xie B, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.645153
   Yang Tao, 2020, 2020 IEEE 2nd International Conference on Computer Science and Educational Informatization (CSEI). Proceedings, P150, DOI 10.1109/CSEI50228.2020.9142479
   Zaki J, 2015, CURR DIR PSYCHOL SCI, V24, P471, DOI 10.1177/0963721415599978
   Zaki J, 2014, PSYCHOL BULL, V140, P1608, DOI 10.1037/a0037679
NR 64
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 17
PY 2024
DI 10.1007/s00371-024-03339-w
EA APR 2024
PG 30
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NY9U8
UT WOS:001204140500004
OA hybrid
DA 2024-08-05
ER

PT J
AU Wei, XY
   Dong, YM
   Liu, Q
   Wang, L
   Lou, LT
AF Wei, Xiyu
   Dong, Yanmei
   Liu, Qin
   Wang, Lei
   Lou, Liantang
TI Robust corner detection in continuous space
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Corner detection; Continuous space; Line segment; Radial distortion;
   Continuous representation
ID ACCURATE JUNCTION DETECTION; SCALE-SPACE
AB Corner detection is important in image analysis and understanding, but most existing corner detectors are sensitive to image quality, lens radial distortion, and illumination. In this paper, we propose a corner detector for robust corner detection in continuous space. We use the open string theory to construct the continuous representation of an image. Defining a corner as the intersection of two or more curve edges or straight line edges, we design a corner response function for corner determination. In detail, for each integer point, we construct multiple grayscale-parallelograms by any two directed line segments of that point, and the corner response function is based on these grayscale-parallelograms. Finally, a point with a high response value is detected as a corner. Experimental results on conventional images, wide-angle images, and fisheye images show that the proposed method obtains state-of-the-art performance on conventional images and achieves superior performance on wide-angle images and fisheye images, even under weak lighting and low-quality conditions.
C1 [Wei, Xiyu; Dong, Yanmei; Liu, Qin; Wang, Lei] Guangxi Univ Sci & Technol, Tus Coll Digit, Key Lab Intelligent Informat Proc & Graph Proc, Liuzhou 545006, Peoples R China.
   [Wei, Xiyu; Lou, Liantang] South Cent Minzu Univ, Sch Math & Stat, Wuhan 430074, Peoples R China.
C3 Guangxi University of Science & Technology; South Central Minzu
   University
RP Dong, YM (corresponding author), Guangxi Univ Sci & Technol, Tus Coll Digit, Key Lab Intelligent Informat Proc & Graph Proc, Liuzhou 545006, Peoples R China.
EM weixiyucicy@foxmail.com; dongyanmei@gxust.edu.cn; liuqinqdszxy@qq.com;
   leiwang1027@126.com; louliantang@163.com
FU Guangxi University of Science and Technology [20Z39]; Project for
   Enhancing Young and Middle-aged Teacher's Research Basis Ability in
   Colleges of Guangxi [2024KY0358]
FX This work was supported by the Doctoral Fund of Guangxi University of
   Science and Technology (No. 20Z39) and the Project for Enhancing Young
   and Middle-aged Teacher's Research Basis Ability in Colleges of Guangxi
   (No. 2024KY0358).
CR [Anonymous], 1979, P 6 INT JOINT C ART
   Awrangjeb M, 2008, IEEE T MULTIMEDIA, V10, P1059, DOI 10.1109/TMM.2008.2001384
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Burschka D, 2005, MED IMAGE ANAL, V9, P413, DOI 10.1016/j.media.2005.05.005
   Castle R, 2007, IEEE INT CONF ROBOT, P4102, DOI 10.1109/ROBOT.2007.364109
   Chen ST, 2016, NEUROCOMPUTING, V173, P434, DOI 10.1016/j.neucom.2015.01.102
   DERICHE R, 1993, INT J COMPUT VISION, V10, P101, DOI 10.1007/BF01420733
   DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060
   Dong YM, 2022, MULTIMED TOOLS APPL, V81, P18417, DOI 10.1007/s11042-022-12236-0
   Dong YM, 2022, INT J SOC ROBOT, V14, P733, DOI 10.1007/s12369-020-00744-8
   Elias R, 2012, IEEE T IMAGE PROCESS, V21, P2109, DOI 10.1109/TIP.2011.2175738
   Fan B, 2023, IEEE T MULTIMEDIA, V25, P1713, DOI 10.1109/TMM.2022.3154165
   Fan B, 2019, IEEE T IMAGE PROCESS, V28, P4774, DOI 10.1109/TIP.2019.2909640
   Hansen P., 2007, IEEE 11 INT C COMP V
   Harris C, 1988, A combined corner and edge detector, P147, DOI [10.5244/C.2.23.23.1-23.6, DOI 10.5244/C.2.23]
   He XC, 2008, OPT ENG, V47, DOI 10.1117/1.2931681
   Huo ZQ, 2020, PATTERN RECOGN LETT, V135, P1, DOI 10.1016/j.patrec.2020.03.027
   Jia YD, 2023, IEEE T MOBILE COMPUT, V22, P2458, DOI 10.1109/TMC.2021.3112559
   Jian CF, 2019, IET IMAGE PROCESS, V13, P991, DOI 10.1049/iet-ipr.2018.5959
   Jiang Y, 2023, VISUAL COMPUT, V39, P733, DOI 10.1007/s00371-021-02371-4
   Jin DY, 2020, IEEE ACCESS, V8, P75884, DOI 10.1109/ACCESS.2020.2989640
   Jing JF, 2023, IEEE T PATTERN ANAL, V45, P4694, DOI 10.1109/TPAMI.2022.3201185
   Kenney CS, 2005, PROC CVPR IEEE, P191
   Liu CJ, 2023, IEEE ROBOT AUTOM LET, V8, P3366, DOI 10.1109/LRA.2023.3268015
   Liu JD, 2021, PATTERN RECOGN IMAGE, V31, P221, DOI 10.1134/S1054661821020115
   Liu QX, 2022, OPTIK, V249, DOI 10.1016/j.ijleo.2021.168306
   Lou L., 2015, INT S MULT IM PROC P
   Lourenço M, 2012, IEEE T ROBOT, V28, P752, DOI 10.1109/TRO.2012.2184952
   Lourenço M, 2010, IEEE INT CONF ROBOT, P1028, DOI 10.1109/ROBOT.2010.5509282
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MEHROTRA R, 1990, PATTERN RECOGN, V23, P1223, DOI 10.1016/0031-3203(90)90118-5
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Pan X, 2021, BIOMED SIGNAL PROCES, V63, DOI 10.1016/j.bspc.2020.102112
   Rosten E, 2005, IEEE I CONF COMP VIS, P1508
   Rosten E, 2010, IEEE T PATTERN ANAL, V32, P105, DOI 10.1109/TPAMI.2008.275
   Ruzon MA, 2001, IEEE T PATTERN ANAL, V23, P1281, DOI 10.1109/34.969118
   Santellani E., 2023, P IEEECVF INT C COMP, P9728
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Sinzinger ED, 2008, PATTERN RECOGN, V41, P494, DOI 10.1016/j.patcog.2007.06.032
   Smith SM, 1997, INT J COMPUT VISION, V23, P45, DOI 10.1023/A:1007963824710
   Pham TA, 2014, PATTERN RECOGN, V47, P282, DOI 10.1016/j.patcog.2013.06.027
   Wan CL, 2023, VISUAL COMPUT, V39, P2499, DOI 10.1007/s00371-022-02474-6
   Wang SA, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3195277
   Xia GS, 2014, INT J COMPUT VISION, V106, P31, DOI 10.1007/s11263-013-0640-1
   Zhang WC, 2015, PATTERN RECOGN, V48, P2785, DOI 10.1016/j.patcog.2015.03.021
   Zhang WC, 2014, IET IMAGE PROCESS, V8, P639, DOI 10.1049/iet-ipr.2013.0641
   Zhang WC, 2023, IEEE T PATTERN ANAL, V45, P9883, DOI 10.1109/TPAMI.2023.3240129
   Zhang WC, 2020, INT J COMPUT VISION, V128, P438, DOI 10.1007/s11263-019-01257-2
   Zhang WC, 2021, IEEE T PATTERN ANAL, V43, P1213, DOI 10.1109/TPAMI.2019.2949302
   Zhang WC, 2019, IEEE T IMAGE PROCESS, V28, P4444, DOI 10.1109/TIP.2019.2910655
   Zhang XH, 2015, IEEE T PATTERN ANAL, V37, P2207, DOI 10.1109/TPAMI.2015.2396074
   Zhao SS, 2023, INT J COMPUT VISION, V131, P2908, DOI 10.1007/s11263-023-01837-3
   Zhong BJ, 2007, IEEE T PATTERN ANAL, V29, P508, DOI 10.1109/TPAMI.2007.50
   Zhou CJ, 2021, J INF PROCESS SYST, V17, P124, DOI 10.3745/JIPS.02.0153
NR 54
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 17
PY 2024
DI 10.1007/s00371-024-03362-x
EA APR 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NY9U8
UT WOS:001204140500002
DA 2024-08-05
ER

PT J
AU Chai, YH
   Gong, JY
   Tan, X
   Xu, JC
   Xie, Y
   Ma, LZ
AF Chai, Yuanhao
   Gong, Jingyu
   Tan, Xin
   Xu, Jiachen
   Xie, Yuan
   Ma, Lizhuang
TI Learnable scene prior for point cloud semantic segmentation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE 3D vision; Point cloud analysis; Semantic segmentation; Scene
   comprehension
ID OBJECT RECOGNITION; IMAGES
AB In this paper, we propose a Geo-SceneEncoder framework to handle point cloud scene semantic segmentation, including a SceneEncoder to learn a scene prior, an advanced geometric kernel to learn geometry information from the point cloud, and a region similarity loss to refine segmentation results. In semantic segmentation, global information plays a pivotal role, while most recent works ignore the importance and usually fail to fully use it. Specifically, they do not explicitly extract meaningful global information and simply use global features in the concatenation. In this paper, we propose a SceneEncoder module to give scene-aware guidance to final segmentation results. This module learns to predict a scene descriptor that represents the categories existing in the scene and uses it to filter out categories not belonging to this scene directly. Additionally, to better use geometry information in the point cloud, we propose an advanced version of kernel correlation to extract geometric features at various scales. Then, we design a region similarity loss to alleviate segmentation noise in the local region. This loss propagates distinguishing features to their neighbors with the same label, enhancing the distinguishing ability of point-wise features. We integrate our methods into several prevailing networks and conduct comprehensive experiments on benchmark datasets ScanNet, S3DIS, and ShapeNet. Results show that our methods greatly improve the performance of baselines and outperform many state-of-the-art competitors. The source code is available at https://github.com/azuki-miho/GeoSceneEncoder.
C1 [Chai, Yuanhao] Washington Univ St Louis, McKelvey Sch Engn, St Louis, MO 63130 USA.
   [Tan, Xin; Xie, Yuan; Ma, Lizhuang] East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China.
   [Gong, Jingyu; Xu, Jiachen; Ma, Lizhuang] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
C3 Washington University (WUSTL); East China Normal University; Shanghai
   Jiao Tong University
RP Gong, JY (corresponding author), Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
EM gongjingyu@sjtu.edu.cn
FU National Natural Science Foundation of China [61972157, 72192821,
   62106268]; Shanghai Municipal Science and Technology Major Project
   [2021SHZDZX0102]
FX This work is partially supported by National Natural Science Foundation
   of China (Nos. 61972157, 72192821, 62106268), Shanghai Municipal Science
   and Technology Major Project (2021SHZDZX0102).
CR Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Atzmon M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201301
   Chen H, 2007, PATTERN RECOGN LETT, V28, P1252, DOI 10.1016/j.patrec.2007.02.009
   Choy C, 2019, IEEE I CONF COMP VIS, P8957, DOI 10.1109/ICCV.2019.00905
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Deng HW, 2018, LECT NOTES COMPUT SC, V11209, P620, DOI 10.1007/978-3-030-01228-1_37
   Du ZJ, 2024, IEEE T NEUR NET LEAR, V35, P4798, DOI 10.1109/TNNLS.2022.3155282
   Ertugrul E, 2020, COMPUT ANIMAT VIRT W, V31, DOI 10.1002/cav.1959
   Gong JY, 2021, PROC CVPR IEEE, P11668, DOI 10.1109/CVPR46437.2021.01150
   Gong JY, 2022, COMPUT VIS MEDIA, V8, P303, DOI 10.1007/s41095-021-0244-6
   Gong JY, 2021, AAAI CONF ARTIF INTE, V35, P1424
   Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961
   Gupta S, 2013, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2013.79
   Han ZZ, 2019, IEEE T CYBERNETICS, V49, P481, DOI 10.1109/TCYB.2017.2778764
   Hu Q., 2020, IEEE CVF C COMP VIS
   Huan Lei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11608, DOI 10.1109/CVPR42600.2020.01163
   Huang JW, 2019, PROC CVPR IEEE, P4435, DOI 10.1109/CVPR.2019.00457
   Huang QG, 2018, PROC CVPR IEEE, P2626, DOI 10.1109/CVPR.2018.00278
   Jiang J., 2023, IEEE Transactions on Multimedia, DOI [10.1109/TMM.2023.3314973, DOI 10.1109/TMM.2023.3314973]
   Jiang JC, 2024, Arxiv, DOI arXiv:2401.02610
   Jiang JC, 2024, VISUAL COMPUT, V40, P5169, DOI 10.1007/s00371-023-02921-y
   Jiang L, 2019, IEEE I CONF COMP VIS, P10432, DOI 10.1109/ICCV.2019.01053
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kirillov A., 2020, P IEEE CVF C COMP VI, P9799, DOI DOI 10.1109/CVPR42600.2020.00982
   Le T, 2018, PROC CVPR IEEE, P9204, DOI 10.1109/CVPR.2018.00959
   Lei H, 2021, IEEE T PATTERN ANAL, V43, P3664, DOI 10.1109/TPAMI.2020.2983410
   Li HF, 2021, IEEE T INF FOREN SEC, V16, P1480, DOI 10.1109/TIFS.2020.3036800
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Li YY, 2018, ADV NEUR IN, V31
   Lin YQ, 2020, PROC CVPR IEEE, P4292, DOI 10.1109/CVPR42600.2020.00435
   Liu JX, 2019, IEEE I CONF COMP VIS, P7545, DOI 10.1109/ICCV.2019.00764
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Qi FL, 2024, IEEE T IND INFORM, V20, P6631, DOI 10.1109/TII.2024.3352232
   Qin YM, 2023, VISUAL COMPUT, V39, P3597, DOI 10.1007/s00371-023-02922-x
   Rao YM, 2019, PROC CVPR IEEE, P452, DOI 10.1109/CVPR.2019.00054
   Schult J., 2020, IEEE CVF C COMP VIS
   Shen YR, 2018, PROC CVPR IEEE, P4548, DOI 10.1109/CVPR.2018.00478
   Sheng B, 2021, IEEE T CYBERNETICS, V51, P1463, DOI 10.1109/TCYB.2020.2988792
   Sheng B, 2018, COMPUT AIDED GEOM D, V62, P133, DOI 10.1016/j.cagd.2018.03.021
   Sheng B, 2018, GRAPH MODELS, V97, P1, DOI 10.1016/j.gmod.2018.03.001
   Shi WJ, 2022, IEEE T CIRC SYST VID, V32, P183, DOI 10.1109/TCSVT.2021.3056726
   Song ZJ, 2022, IEEE T CIRC SYST VID, V32, P4599, DOI 10.1109/TCSVT.2021.3132047
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Tan X, 2023, IEEE T PATTERN ANAL, V45, P15328, DOI 10.1109/TPAMI.2023.3319470
   Tan X, 2021, IEEE T IMAGE PROCESS, V30, P9085, DOI 10.1109/TIP.2021.3122004
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Tsin Y, 2004, LECT NOTES COMPUT SC, V3023, P558
   Wang X, 2019, ADV NEUR IN, V32
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Chang AX, 2015, Arxiv, DOI [arXiv:1512.03012, DOI 10.48550/ARXIV.1512.03012]
   Xu JC, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P601
   Xu QG, 2020, PROC CVPR IEEE, P5660, DOI 10.1109/CVPR42600.2020.00570
   Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6
   Yan X, 2020, PROC CVPR IEEE, P5588, DOI 10.1109/CVPR42600.2020.00563
   Yang JC, 2019, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2019.00344
   Zeng A, 2017, PROC CVPR IEEE, P199, DOI 10.1109/CVPR.2017.29
   Zhang JZ, 2020, PROC CVPR IEEE, P4533, DOI 10.1109/CVPR42600.2020.00459
   Zhang ZY, 2019, IEEE I CONF COMP VIS, P1607, DOI 10.1109/ICCV.2019.00169
   Zhao HS, 2019, PROC CVPR IEEE, P5550, DOI 10.1109/CVPR.2019.00571
   Zhao L, 2020, AAAI CONF ARTIF INTE, V34, P12951
   Zhou H., 2020, IEEE CVF C COMP VIS
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
NR 64
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 8
PY 2024
DI 10.1007/s00371-024-03344-z
EA APR 2024
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NB3I4
UT WOS:001197937700001
DA 2024-08-05
ER

PT J
AU Flores, M
   Valiente, D
   Peidró, A
   Reinoso, O
   Payá, L
AF Flores, Maria
   Valiente, David
   Peidro, Adrian
   Reinoso, Oscar
   Paya, Luis
TI Generating a full spherical view by modeling the relation between two
   fisheye images
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Full spherical view; Dual fisheye images; Image stitching; Fisheye
   projection
AB Full spherical views provide advantages in many applications that use visual information. Dual back-to-back fisheye cameras are receiving much attention to obtain this type of view. However, obtaining a high-quality full spherical view is very challenging. In this paper, we propose a correction step that models the relation between the pixels of the pair of fisheye images in polar coordinates. This correction is implemented during the mapping from the unit sphere to the fisheye image using the equidistant fisheye projection. The objective is that the projections of the same point in the pair of images have the same position on the unit sphere after the correction. In this way, they will also have the same position on the equirectangular coordinate system. Consequently, the discontinuity between the spherical views for blending is minimized. Throughout the manuscript, we show that the angular polar coordinates of the same scene point in the fisheye images are related by a sine function and the radial distance coordinates by a linear function. Also, we propose employing a polynomial as a geometric transformation between the pair of spherical views during the image alignment since the relationship between the matching points of pairs of spherical views is not linear, especially in the top/bottom regions. Quantitative evaluations demonstrate that using the correction step improves the quality of the full spherical view, i.e. IQ MS-SSIM, up to 7%. Similarly, using a polynomial improves the IQ MS-SSIM up to 6.29% with respect to using an affine matrix.
C1 [Flores, Maria; Valiente, David; Peidro, Adrian; Reinoso, Oscar; Paya, Luis] Miguel Hernandez Univ, Inst Engn Res, Ave Univ,S, Elche 03202, Alicante, Spain.
C3 Universidad Miguel Hernandez de Elche
RP Flores, M (corresponding author), Miguel Hernandez Univ, Inst Engn Res, Ave Univ,S, Elche 03202, Alicante, Spain.
EM m.flores@umh.es; dvaliente@umh.es; apeidro@umh.es; o.reinoso@umh.es;
   lpaya@umh.es
RI Paya, Luis/I-5178-2015; Reinoso, Oscar/L-2236-2014
OI Paya, Luis/0000-0002-3045-4316; Flores Tenza, Maria/0000-0003-1117-0868;
   Reinoso, Oscar/0000-0002-1065-8944; Valiente, David/0000-0002-2245-0542
FU Conselleria de Innovacin, Universidades, Ciencia y Sociedad Digital,
   Generalitat Valenciana [TED2021-130901B-I00,
   MCIN/AEI/10.13039/501100011033]; European Union [ACIF/2020/141];
   Generalitat Valenciana; Fondo Social Europeo (FSE)
FX This work is part of the project TED2021-130901B-I00 funded by
   MCIN/AEI/10.13039/501100011033 and by the European Union
   "NextGenerationEU"/PRTR, of the project PROMETEO/2021/075 funded by
   Generalitat Valenciana, and of the grant ACIF/2020/141 funded by
   Generalitat Valenciana and Fondo Social Europeo (FSE).
CR Anand S., 2020, A Guide for Machine Vision in Quality Control, DOI [10.1201/9781003002826, DOI 10.1201/9781003002826]
   ARVC, Laboratorio de Automatizacion Robotica y Vision por Computador (ARVC)-UMH
   Benseddik HE, 2020, INT J ROBOT RES, V39, P1037, DOI 10.1177/0278364920915248
   Bo-Hong Lin, 2020, 2020 International Conference on Pervasive Artificial Intelligence (ICPAI), P194, DOI 10.1109/ICPAI51961.2020.00043
   Cabrera J.J., 2022, SN COMPUT SCI, V3, P271, DOI [DOI 10.1007/S42979-022-01127-8, 10.1007/s42979-022-01127-8]
   Cai YG, 2022, IEEE T CIRC SYST VID, V32, P6400, DOI 10.1109/TCSVT.2022.3165878
   Cebollada S, 2022, ARTIF INTELL REV, V55, P2847, DOI 10.1007/s10462-021-10076-2
   Cebollada S., 2022, Informatics in Control, Automation and Robotics, P226, DOI [10.1007/978-3-030, DOI 10.1007/978-3-030]
   Colonnese S., 2018, 2018 7 EUROPEAN WORK, P1, DOI [10.1109/EUVIP.2018.8611639, DOI 10.1109/EUVIP.2018.8611639]
   Delmas S, 2021, IEEE/SICE I S SYS IN, P505, DOI 10.1109/IEEECONF49454.2021.9382766
   Duan HY, 2023, IEEE J-STSP, V17, P1150, DOI 10.1109/JSTSP.2023.3250956
   Flores M, 2022, ENG APPL ARTIF INTEL, V107, DOI 10.1016/j.engappai.2021.104539
   Garmin, VIRB 360
   Ghosh D, 2016, J VIS COMMUN IMAGE R, V34, P1, DOI 10.1016/j.jvcir.2015.10.014
   Gledhill D, 2003, COMPUT GRAPH-UK, V27, P435, DOI 10.1016/S0097-8493(03)00038-4
   Ho T, 2017, IEEE IMAGE PROC, P51, DOI 10.1109/ICIP.2017.8296241
   Ho T, 2017, INT CONF ACOUST SPEE, P2172, DOI 10.1109/ICASSP.2017.7952541
   Krams O, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Lee SH, 2018, CLUSTER COMPUT, V21, P1175, DOI 10.1007/s10586-017-0930-4
   Li J, 2020, IEEE J-STSP, V14, P209, DOI 10.1109/JSTSP.2019.2953950
   Li J, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2360, DOI 10.1145/3343031.3350973
   Lo IC, 2022, IEEE T IMAGE PROCESS, V31, P251, DOI 10.1109/TIP.2021.3130531
   Lo IC, 2018, IEEE IMAGE PROC, P3164, DOI 10.1109/ICIP.2018.8451333
   Morbidi F, 2023, IEEE ROBOT AUTOM MAG, V30, P24, DOI 10.1109/MRA.2022.3178965
   Ni GY, 2017, 2017 10TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI)
   Prados R, 2014, SPRINGERBRIEF COMPUT, P35, DOI 10.1007/978-3-319-05558-9_3
   Rana A, 2019, INT CONF ACOUST SPEE, P2012, DOI 10.1109/ICASSP.2019.8683318
   Román V, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21103327
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Samsung, 2017, Gear 360
   Saura-Herreros M, 2021, VISUAL COMPUT, V37, P2809, DOI 10.1007/s00371-021-02239-7
   Scaramuzza D, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P5695, DOI 10.1109/IROS.2006.282372
   Souza T, 2018, SIBGRAPI, P313, DOI 10.1109/SIBGRAPI.2018.00047
   Szeliski R, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000009
   theta360, Ricoh: Producto | RICOH THETA S
   Tian CZ, 2021, J VIS COMMUN IMAGE R, V81, DOI 10.1016/j.jvcir.2021.103324
   Ha VKL, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10010369
   Wang T, 2019, CONF TECHNOL APPL, DOI 10.1109/taai48200.2019.8959887
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei L., 2019, Virtual Reality & Intelligent Hardware, V1, P55, DOI [10.3724/SP.J.2096-5796.2018.0008, DOI 10.3724/SP.J.2096-5796.2018.0008]
   Xue L, 2021, OPTIK, V238, DOI 10.1016/j.ijleo.2021.166520
   Yang L, 2023, Arxiv, DOI [arXiv:2307.08252, DOI 10.48550/ARXIV.2307.08252]
   Zhang JD, 2024, VISUAL COMPUT, V40, P427, DOI 10.1007/s00371-023-02791-4
   Zhang JD, 2019, MULTIMED TOOLS APPL, V78, P27663, DOI 10.1007/s11042-019-07890-w
   Zhang WX, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22020470
   Zhang Y, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21030705
   Zhang ZC, 2016, IEEE INT CONF ROBOT, P801, DOI 10.1109/ICRA.2016.7487210
NR 48
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 29
PY 2024
DI 10.1007/s00371-024-03293-7
EA FEB 2024
PG 26
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JF9E8
UT WOS:001171862700001
OA hybrid
DA 2024-08-05
ER

PT J
AU Wu, Y
   Cai, CT
   Yeo, CK
   Wu, KJ
AF Wu, Yue
   Cai, Chengtao
   Yeo, Chai Kiat
   Wu, Kejun
TI Target-aware pooling combining global contexts for aerial tracking
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Transformer; UAVs; Deep CNN network; Target-aware pooling; Global
   contexts
AB The UAVs captured targets are relatively small when compared with the ordinary surveillance cameras. Thus, a strong discriminative ability is required for aerial trackers to accurately locate small targets, especially in challenging scenes, including similar objects, occlusion and size change. Since most existing aerial trackers do not perform satisfactorily, in this paper, we design a tracker called TAP-GC that utilizes a weight-sharing deep CNN network to extract the multi-scale template and test features. We then construct a target-aware pooling module in the template branch, allowing the tracker to pay more attention to the target-related information. Thereafter, we directly fuse the template and test features through a transformer which is able to make full use of the global context, enabling the tracker to discriminate the target more accurately. Extensive experiments on well-known aerial tracking benchmarks, UAV123, UAV123@10fps and DTB70, show that our tracker outperforms a number of state-of-the-art trackers. In addition, when evaluating TAP-GC on OTB100, a tracking benchmark captured by ordinary cameras, it also achieves leading tracking performance. TAP-GC can achieve about 70 fps speed for real-time UAV tracking.
C1 [Wu, Yue; Cai, Chengtao] Harbin Engn Univ, Sch Intelligent Sci & Engn, Harbin 150001, Peoples R China.
   [Cai, Chengtao] Harbin Engn Univ, Key Lab Intelligent Technol & Applicat Marine Equi, Minist Educ, Harbin 150001, Peoples R China.
   [Yeo, Chai Kiat] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Wu, Kejun] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Harbin Engineering University; Harbin Engineering University; Nanyang
   Technological University; Nanyang Technological University
RP Cai, CT (corresponding author), Harbin Engn Univ, Sch Intelligent Sci & Engn, Harbin 150001, Peoples R China.; Cai, CT (corresponding author), Harbin Engn Univ, Key Lab Intelligent Technol & Applicat Marine Equi, Minist Educ, Harbin 150001, Peoples R China.
EM wuyue1116@hrbeu.edu.cn; caichengtao@hrbeu.edu.cn; asckyeo@ntu.edu.sg;
   kejun.wu@ntu.edu.sg
FU Key Projects of Heilongjiang Provincial Natural Science Foundation; 
   [ZD2022F001]
FX This paper was supported by the Key Projects of Heilongjiang Provincial
   Natural Science Foundation (Grant No. ZD2022F001).
CR Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Cao ZA, 2022, PROC CVPR IEEE, P14778, DOI 10.1109/CVPR52688.2022.01438
   Cao ZA, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15437, DOI 10.1109/ICCV48922.2021.01517
   Carion N., 2020, EUR C COMP VIS, P213
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Chu J, 2018, IEEE ACCESS, V6, P19959, DOI 10.1109/ACCESS.2018.2815149
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Fan H, 2021, INT J COMPUT VISION, V129, P439, DOI 10.1007/s11263-020-01387-y
   Fan JW, 2023, VISUAL COMPUT, V39, P319, DOI 10.1007/s00371-021-02331-y
   Fu CH, 2021, IEEE INT CONF ROBOT, P510, DOI 10.1109/ICRA48506.2021.9560756
   Glorot X., 2011, P 14 INT C ART INT S, V15, P315, DOI DOI 10.1002/ECS2.1832
   Han GX, 2022, PROC CVPR IEEE, P5311, DOI 10.1109/CVPR52688.2022.00525
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Huang K, 2022, PROC INT C TOOLS ART, P644, DOI 10.1109/ICTAI56018.2022.00100
   Huang K, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12083931
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Huang ZY, 2019, IEEE I CONF COMP VIS, P2891, DOI 10.1109/ICCV.2019.00298
   Huang Ziyuan, 2022, ICLR
   Jaderberg M., 2015, Adv. Neural Inf. Process. Syst, V28, P66
   Jiang BR, 2018, LECT NOTES COMPUT SC, V11218, P816, DOI 10.1007/978-3-030-01264-9_48
   Junos MH, 2022, VISUAL COMPUT, V38, P2341, DOI 10.1007/s00371-021-02116-3
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li SM, 2020, IEEE ACCESS, V8, P122772, DOI 10.1109/ACCESS.2020.3007261
   Li SY, 2017, AAAI CONF ARTIF INTE, P4140
   Lin Fangjian, 2022, P AS C COMP VIS, P2663
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lüscher C, 2019, INTERSPEECH, P231, DOI 10.21437/Interspeech.2019-1780
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Real E, 2017, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR.2017.789
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Vaswani A, 2017, ADV NEUR IN, V30
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yan B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10428, DOI 10.1109/ICCV48922.2021.01028
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Yiming Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11920, DOI 10.1109/CVPR42600.2020.01194
   Yu J, 2016, P 24 ACM INT C MULT, P516, DOI DOI 10.1145/2964284.2967274
   Yuan Y, 2020, EURASIP J IMAGE VIDE, V2020, DOI 10.1186/s13640-020-0496-6
   Zhang Q, 2023, VISUAL COMPUT, V39, P4593, DOI 10.1007/s00371-022-02611-1
   Zhang YQ, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20041010
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
NR 45
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 29
PY 2024
DI 10.1007/s00371-024-03282-w
EA FEB 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JF9E8
UT WOS:001171862700002
DA 2024-08-05
ER

PT J
AU Gilal, NU
   Qaraqe, M
   Schneider, J
   Agus, M
AF Gilal, Nauman Ullah
   Qaraqe, Marwa
   Schneider, Jens
   Agus, Marco
TI Autocleandeepfood: auto-cleaning and data balancing transfer learning
   for regional gastronomy food computing
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Food computing; Web-scrapping; Traditional cuisines; MENA food data set;
   Noisy labels; Transfer learning; Auto-cleaning; Data imbalance
AB Food computing has emerged as a promising research field, employing artificial intelligence, deep learning, and data science methodologies to enhance various stages of food production pipelines. To this end, the food computing community has compiled a variety of data sets and developed various deep-learning architectures to perform automatic classification. However, automated food classification presents a significant challenge, particularly when it comes to local and regional cuisines, which are often underrepresented in available public-domain data sets. Nevertheless, obtaining high-quality, well-labeled, and well-balanced real-world labeled images is challenging since manual data curation requires significant human effort and is time-consuming. In contrast, the web has a potentially unlimited source of food data but tapping into this resource has a good chance of corrupted and wrongly labeled images. In addition, the uneven distribution among food categories may lead to data imbalance problems. All these issues make it challenging to create clean data sets for food from web data. To address this issue, we present AutoCleanDeepFood, a novel end-to-end food computing framework for regional gastronomy that contains the following components: (i) a fully automated pre-processing pipeline for custom data sets creation related to specific regional gastronomy, (ii) a transfer learning-based training paradigm to filter out noisy labels through loss ranking, incorporating a Russian Roulette probabilistic approach to mitigate data imbalance problems, and (iii) a method for deploying the resulting model on smartphones for real-time inferences. We assess the performance of our framework on a real-world noisy public domain data set, ETH Food-101, and two novel web-collected datasets, MENA-150 and Pizza-Styles. We demonstrate the filtering capabilities of our proposed method through embedding visualization of the feature space using the t-SNE dimension reduction scheme. Our filtering scheme is efficient and effectively improves accuracy in all cases, boosting performance by 0.96, 0.71, and 1.29% on MENA-150, ETH Food-101, and Pizza-Styles, respectively.
C1 [Gilal, Nauman Ullah; Qaraqe, Marwa; Schneider, Jens; Agus, Marco] Hamad Bin Khalifa Univ, Qatar Fdn, Coll Sci & Engn, Div Informat & Comp Technol, Doha, Qatar.
C3 Qatar Foundation (QF); Hamad Bin Khalifa University-Qatar
RP Agus, M (corresponding author), Hamad Bin Khalifa Univ, Qatar Fdn, Coll Sci & Engn, Div Informat & Comp Technol, Doha, Qatar.
EM magus@hbku.edu.qa
FU Hamad bin Khalifa University
FX No Statement AvailableDAS:The data presented in this study are openly
   available
   at:https://drive.google.com/file/d/1VlKb7pgST10ny88fBkrMBO4Fs6_5XJfM/vie
   w.
CR Aguilar E, 2018, IEEE T MULTIMEDIA, V20, P3266, DOI 10.1109/TMM.2018.2831627
   Albert P., 2022, P IEEE CVF WINT C AP, P392
   Ando Y, 2019, MADIMA'19: PROCEEDINGS OF THE 5TH INTERNATIONAL WORKSHOP ON MULTIMEDIA ASSISTED DIETARY MANAGEMENT, P76, DOI 10.1145/3347448.3357172
   Arefeen M.A., 2020, IEEE Trans. Syst.
   Blom G., 1996, Math Mag, V69, P293, DOI [DOI 10.2307/2690538, 10.1080/0025570X.1996.11996458, DOI 10.1080/0025570X.1996.11996458]
   Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29
   Chen JJ, 2021, IEEE T IMAGE PROCESS, V30, P1514, DOI 10.1109/TIP.2020.3045639
   Chen JJ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P32, DOI 10.1145/2964284.2964315
   Chen XL, 2015, IEEE I CONF COMP VIS, P1431, DOI 10.1109/ICCV.2015.168
   Ciocca G, 2020, IEEE ACCESS, V8, P32003, DOI 10.1109/ACCESS.2020.2973704
   Ciocca G, 2015, LECT NOTES COMPUT SC, V9281, P334, DOI 10.1007/978-3-319-23222-5_41
   Dinic R, 2017, PROCEEDINGS OF THE 19TH INTERNATIONAL CONFERENCE ON HUMAN-COMPUTER INTERACTION WITH MOBILE DEVICES AND SERVICES (MOBILEHCI '17), DOI 10.1145/3098279.3125434
   Elreedy D, 2019, INFORM SCIENCES, V505, P32, DOI 10.1016/j.ins.2019.07.070
   Farinella GM, 2015, LECT NOTES COMPUT SC, V8927, P584, DOI 10.1007/978-3-319-16199-0_41
   Freitas CNC, 2020, SIBGRAPI, P234, DOI 10.1109/SIBGRAPI51738.2020.00039
   Gilal N., 2023, Multimedia Tools Appl, V83, P1
   Gilal N.U., 2021, SMART TOOLS APPS GRA
   Gilal N.U., 2023, J. Image Graph., V11
   Goncalves D, 2020, Inf. Process. Agric., V8, P560
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   He JP, 2021, IEEE INT CONF COMP V, P2337, DOI 10.1109/ICCVW54120.2021.00265
   Hu P, 2021, PROC CVPR IEEE, P5399, DOI 10.1109/CVPR46437.2021.00536
   Jiang LD, 2020, IEEE ACCESS, V8, P47477, DOI 10.1109/ACCESS.2020.2973625
   Kaur H, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3343440
   Kaur P, 2019, Arxiv, DOI arXiv:1907.06167
   Kawano Y, 2015, MULTIMED TOOLS APPL, V74, P5263, DOI 10.1007/s11042-014-2000-8
   Kazi A, 2022, MULTIMED TOOLS APPL, V81, P7611, DOI 10.1007/s11042-022-12150-5
   Lam MB, 2020, IEEE ACCESS, V8, P88360, DOI 10.1109/ACCESS.2020.2993053
   Latif Ghazanfar, 2020, ICCTA '20: Proceedings of the 2020 6th International Conference on Computer and Technology Applications, P17, DOI 10.1145/3397125.3397154
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee GG, 2019, TENCON IEEE REGION, P802, DOI [10.1109/tencon.2019.8929715, 10.1109/TENCON.2019.8929715]
   Li W, 2017, Arxiv, DOI arXiv:1708.02862
   Lin WC, 2017, INFORM SCIENCES, V409, P17, DOI 10.1016/j.ins.2017.05.008
   Lu J, 2018, PR MACH LEARN RES, V80
   Lyu Y, 2020, Arxiv, DOI arXiv:1905.10045
   Ma PH, 2022, FOOD CHEM, V373, DOI 10.1016/j.foodchem.2021.130994
   Medus LD, 2021, FOOD CONTROL, V125, DOI 10.1016/j.foodcont.2021.107962
   Min WQ, 2021, Arxiv, DOI arXiv:2103.16107
   Min WQ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P393, DOI 10.1145/3394171.3414031
   Min WQ, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3329168
   Nguyen D. T., 2019, Self: Learning to filter noisy labels with self-ensembling
   Nguyen HT, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108470
   Okamoto Kaimu, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12665), P647, DOI 10.1007/978-3-030-68821-9_51
   Pan LL, 2017, 2017 IEEE 3RD INTERNATIONAL CONFERENCE ON COLLABORATION AND INTERNET COMPUTING (CIC), P181, DOI 10.1109/CIC.2017.00033
   Poply Parth, 2020, SPML 2020: Proceedings of the 2020 3rd International Conference on Signal Processing and Machine Learning, P73, DOI 10.1145/3432291.3432295
   Ramdani Assyifa, 2020, 2020 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT). Proceedings, P91, DOI 10.1109/IAICT50021.2020.9172024
   Runyu Mao, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12665), P571, DOI 10.1007/978-3-030-68821-9_47
   Sadler CR, 2021, TRENDS FOOD SCI TECH, V112, P149, DOI 10.1016/j.tifs.2021.02.059
   Sarda Ekta, 2021, Proceedings of International Conference on Computational Intelligence and Data Engineering. ICCIDE 2020. Lecture Notes on Data Engineering and Communications Technologies (LNDECT 56), P435, DOI 10.1007/978-981-15-8767-2_36
   Shao Z., 2021, P 3 WORKSHOP AIXFOOD, P19, DOI 10.1145/3475725.3483625
   Shao ZM, 2019, IEEE INT CONF BIG DA, P5186, DOI 10.1109/BigData47090.2019.9006165
   Shen J., 2023, Mach. Learn, V113, P1
   Siddiqi R, 2019, ICDLT 2019: 2019 3RD INTERNATIONAL CONFERENCE ON DEEP LEARNING TECHNOLOGIES, P91, DOI 10.1145/3342999.3343002
   Simonetti L, 2012, J EUR STUD, V42, P168, DOI 10.1177/0047244112436908
   Song H, 2023, IEEE T NEUR NET LEAR, V34, P8135, DOI 10.1109/TNNLS.2022.3152527
   Subhi MA, 2018, IEEE EMBS CONF BIO, P284, DOI 10.1109/IECBES.2018.8626720
   Sun JN, 2019, PROCEEDINGS OF MVA 2019 16TH INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS (MVA), DOI 10.23919/mva.2019.8757886
   Tahir GA, 2021, HEALTHCARE-BASEL, V9, DOI 10.3390/healthcare9121676
   Tan MX, 2019, PR MACH LEARN RES, V97
   Temdee P, 2017, 2017 GLOBAL WIRELESS SUMMIT (GWS), P132, DOI 10.1109/GWS.2017.8300490
   Tsai CF, 2019, INFORM SCIENCES, V477, P47, DOI 10.1016/j.ins.2018.10.029
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wei HX, 2021, ADV NEUR IN, V34
   Xie XY, 2021, KNOWL-BASED SYST, V213, DOI 10.1016/j.knosys.2020.106689
   Yang JK, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P83, DOI 10.1145/3394171.3413952
   Yao YZ, 2021, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR46437.2021.00515
   Yen SJ, 2006, LECT NOTES CONTR INF, V344, P731
   Zhang Y., 2021, P IEEE CVF INT C COM, P15065
   Zhao H, 2021, IEEE WINT CONF APPL, P1710, DOI 10.1109/WACV48630.2021.00175
   Zheng GQ, 2021, AAAI CONF ARTIF INTE, V35, P11053
   Zhidong Shen, 2020, Procedia Computer Science, V174, P448, DOI 10.1016/j.procs.2020.06.113
   Zhou X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P72, DOI 10.1109/ICCV48922.2021.00014
NR 72
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 9
PY 2024
DI 10.1007/s00371-024-03560-7
EA JUL 2024
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YK4D7
UT WOS:001268360600001
OA hybrid
DA 2024-08-05
ER

PT J
AU Xi, RT
   Lyu, J
   Sun, K
   Ma, T
AF Xi, Runtao
   Lyu, Jiahao
   Sun, Kang
   Ma, Tian
TI Learning kernel parameter lookup tables to implement adaptive bilateral
   filtering
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Bilateral filter; Image smoothing; Lookup table; Deep Learning
AB Bilateral filtering is a widely used image smoothing filter that preserves image edges while also smoothing texture. In previous research, the focus of improving the bilateral filter has primarily been on constructing an adaptive range kernel. However, recent research has shown that even slight noise perturbations can prevent the bilateral filter from effectively preserving image edges. To address this issue, we employ a neural network to learn the kernel parameters that can effectively counteract noise perturbations. Additionally, to enhance the adaptability of the learned kernel parameters to the local edge features of the image, we utilize the edge-sensitive indexing method to construct kernel parameter lookup tables (LUTs). During testing, we determine the appropriate spatial kernel and range kernel parameters for each pixel using a lookup table and interpolation. This allows us to effectively smooth the image in the presence of noise perturbation. In this paper, we conducted comparative experiments on several datasets to verify that the proposed method outperforms existing bilateral filtering methods in preserving image structure, removing image texture, and resisting slight noise perturbations. The code is available at https://github.com/FightingSrain/AdaBFLUT.
C1 [Xi, Runtao; Sun, Kang] CCTEG Changzhou Res Inst, Changzhou 213000, Jiangsu, Peoples R China.
   [Xi, Runtao; Sun, Kang] Tiandi Changzhou Automat Co Ltd, Changzhou 213000, Jiangsu, Peoples R China.
   [Lyu, Jiahao] Xian Univ Technol, Sch Comp Sci & Engn, Xian 710600, Shaanxi, Peoples R China.
   [Ma, Tian] Xian Univ Sci & Technol, Coll Comp Sci & Technol, Xian 710600, Shaanxi, Peoples R China.
C3 Xi'an University of Technology; Xi'an University of Science & Technology
RP Xi, RT (corresponding author), CCTEG Changzhou Res Inst, Changzhou 213000, Jiangsu, Peoples R China.; Xi, RT (corresponding author), Tiandi Changzhou Automat Co Ltd, Changzhou 213000, Jiangsu, Peoples R China.
EM jusadw@163.com; jhlyu25@stu.xaut.edu.cn; xdlysk@live.cn;
   matian@xust.edu.cn
FU Key Technology Research on Safety and Intelligent Comprehensive Control
   in Electrolytic Aluminium Industry; National Natural Science Foundation
   of China [62101432, 62102309]; Shaanxi Natural Science Fundamental
   Research Program Project [2022JM-508]
FX This work was supported by the Key Technology Research on Safety and
   Intelligent Comprehensive Control in Electrolytic Aluminium Industry
   (Grant No. 2023TY2006), the National Natural Science Foundation of China
   (Grant No. 62101432 and 62102309), and in part by Shaanxi Natural
   Science Fundamental Research Program Project (No. 2022JM-508).
CR Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Chaudhury KN, 2016, IEEE T IMAGE PROCESS, V25, P2519, DOI 10.1109/TIP.2016.2548363
   Chaudhury KN, 2011, IEEE T IMAGE PROCESS, V20, P3376, DOI 10.1109/TIP.2011.2159234
   Chen BH, 2022, INT CONF ACOUST SPEE, P1675, DOI 10.1109/ICASSP43922.2022.9746631
   Chen BH, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2020.3048488
   Chen BH, 2020, IEEE SIGNAL PROC LET, V27, P1670, DOI 10.1109/LSP.2020.3024990
   Chen ZH, 2023, IEEE T PATTERN ANAL, V45, P13489, DOI 10.1109/TPAMI.2023.3293885
   Cheng ZZ, 2015, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2015.55
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Gavaskar RG, 2019, IEEE T IMAGE PROCESS, V28, P779, DOI 10.1109/TIP.2018.2871597
   Ghosh S, 2018, IEEE SIGNAL PROC LET, V25, P1555, DOI 10.1109/LSP.2018.2866949
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Jo Y, 2021, PROC CVPR IEEE, P691, DOI 10.1109/CVPR46437.2021.00075
   Kingma D. P., 2014, arXiv
   Li JC, 2023, PROC CVPR IEEE, P5866, DOI 10.1109/CVPR52729.2023.00568
   Li JC, 2022, LECT NOTES COMPUT SC, V13678, P238, DOI 10.1007/978-3-031-19797-0_14
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu CX, 2023, IEEE T IMAGE PROCESS, V32, P4742, DOI 10.1109/TIP.2023.3290849
   Liu GD, 2023, IEEE I CONF COMP VIS, P12183, DOI 10.1109/ICCV51070.2023.01122
   Ma C, 2022, LECT NOTES COMPUT SC, V13677, P305, DOI 10.1007/978-3-031-19790-1_19
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Sheng B, 2022, IEEE T CYBERNETICS, V52, P6662, DOI 10.1109/TCYB.2021.3079311
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Tsubokawa T, 2024, MULTIMED TOOLS APPL, V83, P26131, DOI 10.1007/s11042-023-16405-7
   Wei D., 2022, J. Mine Autom., V48, P19, DOI [10.13272/j.issn.1671-251x.2021110069, DOI 10.13272/J.ISSN.1671-251X.2021110069]
   Wu HK, 2018, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2018.00197
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Yang CQ, 2022, LECT NOTES COMPUT SC, V13678, P201, DOI 10.1007/978-3-031-19797-0_12
   Yang CQ, 2022, PROC CVPR IEEE, P17501, DOI 10.1109/CVPR52688.2022.01700
   Yang QX, 2009, PROC CVPR IEEE, P557, DOI 10.1109/CVPRW.2009.5206542
   Zeng H, 2022, IEEE T PATTERN ANAL, V44, P2058, DOI 10.1109/TPAMI.2020.3026740
   Zeyde R., 2012, INT C CURV SURF, P711
   Zhang FY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P6493, DOI 10.1145/3503161.3547879
   [张雷 Zhang Lei], 2023, [工矿自动化, Industry and Mine Automation], V49, P45
   Zhong ZW, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3253472
NR 43
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 1
PY 2024
DI 10.1007/s00371-024-03553-6
EA JUL 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XC2G8
UT WOS:001259414100002
DA 2024-08-05
ER

PT J
AU Li, W
   Li, BW
   Wang, JQ
   Meng, WL
   Zhang, JG
   Zhang, XP
AF Li, Wei
   Li, Bowen
   Wang, Jingqi
   Meng, Weiliang
   Zhang, Jiguang
   Zhang, Xiaopeng
TI ROMOT: Referring-expression-comprehension open-set multi-object tracking
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Open-set; Referring expression comprehension; Detection; Tracking
AB Traditional multi-object tracking is limited to tracking a predefined set of categories, whereas open-vocabulary tracking expands its capabilities to track novel categories. In this paper, we propose ROMOT (referring-expression-comprehension open-set multi-object tracking), which not only tracks objects from novel categories not included in the training data, but also enables tracking based on referring expression comprehension (REC). REC describes targets solely by their attributes, such as "the person running at the front" or "the bird flying in the air rather than on the ground," making it particularly relevant for real-world multi-object tracking scenarios. Our ROMOT achieves this by harnessing the exceptional capabilities of a visual language model and employing multi-stage cross-modal attention to handle tracking for novel categories and REC tasks. Integrating RSM (reconstruction similarity metric) and OCM (observation-centric momentum) in our ROMOT eliminates the need for task-specific training, addressing the challenge of insufficient datasets. Our ROMOT enhances efficiency and adaptability in handling tracking requirements without relying on extensive tracking training data.
C1 [Li, Wei; Li, Bowen; Meng, Weiliang; Zhang, Jiguang; Zhang, Xiaopeng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Li, Wei; Li, Bowen; Wang, Jingqi; Meng, Weiliang; Zhang, Jiguang; Zhang, Xiaopeng] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing, Peoples R China.
   [Wang, Jingqi] Cent China Normal Univ, Wuhan, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; Institute of Automation, CAS; Central
   China Normal University
RP Meng, WL (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.; Meng, WL (corresponding author), Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing, Peoples R China.
EM weiliang.meng@ia.ac.cn; xiaopeng.zhang@ia.ac.cn
FU National Natural Science Foundation of China; Beijing Natural Science
   Foundation [L231013, JQ23014];  [U21A20515];  [62376271];  [62071157]; 
   [62171321];  [62162044];  [62365014];  [52175493]
FX This work was supported by Beijing Natural Science Foundation (Nos.
   L231013, JQ23014), National Natural Science Foundation of China (Nos.
   U21A20515, 62376271, 62071157, 62171321, 62162044, 62365014 and
   52175493).
CR Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103
   Cao JK, 2023, PROC CVPR IEEE, P9686, DOI 10.1109/CVPR52729.2023.00934
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen Zhenfang, 2020, P IEEE CVF C COMP VI, P10086
   Dave Achal, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P436, DOI 10.1007/978-3-030-58558-7_26
   Dave A, 2019, IEEE INT CONF COMP V, P1493, DOI 10.1109/ICCVW.2019.00187
   Dendorfer P, 2021, INT J COMPUT VISION, V129, P845, DOI 10.1007/s11263-020-01393-0
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Fischer T, 2023, IEEE T PATTERN ANAL, V45, P15380, DOI 10.1109/TPAMI.2023.3301975
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gupta A, 2019, PROC CVPR IEEE, P5351, DOI 10.1109/CVPR.2019.00550
   Kirillov A, 2023, Arxiv, DOI arXiv:2304.02643
   Li F, 2022, PROC CVPR IEEE, P13609, DOI 10.1109/CVPR52688.2022.01325
   Li LH, 2022, PROC CVPR IEEE, P10955, DOI 10.1109/CVPR52688.2022.01069
   Li SY, 2023, PROC CVPR IEEE, P5567, DOI 10.1109/CVPR52729.2023.00539
   Li SY, 2022, LECT NOTES COMPUT SC, V13682, P498, DOI 10.1007/978-3-031-20047-2_29
   Li W., 2023, SIGGRAPH ASIA 2023 P, P1
   Liu S., 2022, arXiv
   Liu SL, 2023, Arxiv, DOI arXiv:2303.05499
   Liu Y, 2022, PROC CVPR IEEE, P19023, DOI 10.1109/CVPR52688.2022.01846
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luiten J, 2021, INT J COMPUT VISION, V129, P548, DOI 10.1007/s11263-020-01375-2
   Ma F, 2022, PROC CVPR IEEE, P8771, DOI 10.1109/CVPR52688.2022.00858
   Meinhardt T, 2022, PROC CVPR IEEE, P8834, DOI 10.1109/CVPR52688.2022.00864
   Miao PH, 2024, Arxiv, DOI arXiv:2204.09957
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Subramanian S., 2022, arXiv
   Wang Z., 2021, Advances in Neural Information Processing Systems, V34, P726
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Wu DM, 2023, PROC CVPR IEEE, P14633, DOI 10.1109/CVPR52729.2023.01406
   Zhang H, 2023, IEEE I CONF COMP VIS, P1020, DOI 10.1109/ICCV51070.2023.00100
   Zhang H, 2022, Arxiv, DOI [arXiv:2203.03605, DOI 10.48550/ARXIV.2203.03605, 10.48550/arXiv.2203.03605]
   Zhang YF, 2022, LECT NOTES COMPUT SC, V13682, P1, DOI 10.1007/978-3-031-20047-2_1
   Zhang YF, 2021, INT J COMPUT VISION, V129, P3069, DOI 10.1007/s11263-021-01513-4
   Zhong YW, 2022, PROC CVPR IEEE, P16772, DOI 10.1109/CVPR52688.2022.01629
   Zhongdao Wang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P107, DOI 10.1007/978-3-030-58621-8_7
NR 37
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 19
PY 2024
DI 10.1007/s00371-024-03544-7
EA JUN 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UT2Y8
UT WOS:001250256000001
DA 2024-08-05
ER

PT J
AU Jiang, MX
   Shen, LQ
   Hu, XY
   Hu, M
   An, P
   Tian, T
AF Jiang, Mingxing
   Shen, Liquan
   Hu, Xiangyu
   Hu, Min
   An, Ping
   Tian, Tao
TI meTMQI: multi-task and exposure-prior learning for Tone-Mapped Quality
   Index
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Tone-mapped image (TMI); Image quality assessment (IQA); Convolutional
   neural network (CNN); Exposure-prior; Multi-task
ID FREE-ENERGY PRINCIPLE; STATISTICS
AB With limited dynamic range in consumer-level photographs and electronic displays, high dynamic range images can be rendered as the standard dynamic range image by tone mapping algorithms. To quantify the distortion of tone-mapped images (TMIs), different descriptors or network architectures are explored for quality-aware measurement. Although promising performances have been achieved, they are still limited by three problems: (1) The exposure attribute has not been accurately measured; (2) many deep learning methods, only with the extremely limited reliable ground truth data without the exploitation of quality ranking between different TMIs, are difficult to achieve effective backpropagation; and (3) these methods only use single-task learning (i.e., predict quality score), which ignore the promotion of related tasks. In this paper, we propose a Tone-Mapped Quality Index via multi-task and exposure-prior learning, called meTMQI. We first use quality-discriminative TMI pairs which can be obtained in large quantities to train a purpose-designed Siamese network. Second, multi-task learning is employed to predict quality score, rank score pairs, and classify quality levels, aiming to enhance the predict accuracy in terms of quality. Then, we propose an exposure-prior generation module, which helps to measure the exposure attribute for joint optimization. Experiments are executed on the benchmark datasets, and the results show the effectiveness and generalization ability with other image quality assessment models.
C1 [Jiang, Mingxing; Tian, Tao] Chaohu Univ, Sch Informat Engn & Commun, Hefei 238024, Peoples R China.
   [Jiang, Mingxing; Shen, Liquan; Hu, Xiangyu; An, Ping] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
   [Hu, Min] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230602, Peoples R China.
C3 Chaohu University; Shanghai University; Hefei University of Technology
RP Shen, LQ (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
EM jsslq@163.com
FU National Natural Science Foundation of China [61931022, 61671282,
   62301087, 62176084]; National Natural Science Foundation of China (NSFC)
FX This research was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grant Nos. 61931022, 61671282,
   62301087, and 62176084.
CR [Anonymous], 2012, document Rec. ITU-R BT.500-11
   [Anonymous], 2012, ITU-R BT.500-13
   Avcibas I, 2002, J ELECTRON IMAGING, V11, P206, DOI 10.1117/1.1455011
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Cao XD, 2021, J ELECTRON IMAGING, V30, DOI 10.1117/1.JEI.30.4.043020
   Cao Z., 2007, P 24 INT C MACH LEAR, P129
   Chen PF, 2019, PATTERN RECOGN, V89, P108, DOI 10.1016/j.patcog.2019.01.010
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Friston KJ, 2006, J PHYSIOL-PARIS, V100, P70, DOI 10.1016/j.jphysparis.2006.10.001
   Friston KJ, 2010, NAT REV NEUROSCI, V11, P127, DOI 10.1038/nrn2787
   Gao F, 2015, IEEE T NEUR NET LEAR, V26, P2275, DOI 10.1109/TNNLS.2014.2377181
   Golestaneh SA, 2022, IEEE WINT CONF APPL, P3989, DOI 10.1109/WACV51458.2022.00404
   Gu J, 2019, AAAI CONF ARTIF INTE, P8336
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Guo D, 2010, PROC CVPR IEEE, P515, DOI 10.1109/CVPR.2010.5540170
   He Q, 2018, IEEE INT CONF MULTI
   Hu XY, 2023, IEEE T MULTIMEDIA, V25, P4814, DOI 10.1109/TMM.2022.3183404
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ioffe Sergey, 2015, INT C MACHINE LEARNI, V37, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Jang H, 2020, IEEE ACCESS, V8, P38554, DOI 10.1109/ACCESS.2020.2975857
   Jiang MX, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3185322
   Jiang MX, 2021, IEEE T CONSUM ELECTR, V67, P350, DOI 10.1109/TCE.2021.3130176
   Jiang MX, 2020, IEEE T CONSUM ELECTR, V66, P153, DOI 10.1109/TCE.2020.2985742
   Jung C, 2013, J ELECTRON IMAGING, V22, DOI 10.1117/1.JEI.22.3.033001
   Kang L, 2015, IEEE IMAGE PROC, P2791, DOI 10.1109/ICIP.2015.7351311
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kim J, 2017, IEEE J-STSP, V11, P206, DOI 10.1109/JSTSP.2016.2639328
   Kundu D, 2017, IEEE T IMAGE PROCESS, V26, P4725, DOI 10.1109/TIP.2017.2713945
   Kundu D, 2017, IEEE T IMAGE PROCESS, V26, P2957, DOI 10.1109/TIP.2017.2685941
   Li XW, 2019, J ELECTRON IMAGING, V28, DOI 10.1117/1.JEI.28.2.023008
   Lin HH, 2018, Arxiv, DOI arXiv:1803.08489
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Liu YL, 2020, PROC CVPR IEEE, P1648, DOI 10.1109/CVPR42600.2020.00172
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P3951, DOI 10.1109/TIP.2017.2708503
   Mahmoudpour S, 2020, IEEE T MULTIMEDIA, V22, P1939, DOI 10.1109/TMM.2019.2950570
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Ou FZ, 2022, IEEE T MULTIMEDIA, V24, P4197, DOI 10.1109/TMM.2021.3114551
   Ponomarenko Nikolay, 2013, 2013 4th European Workshop on Visual Information Processing (EUVIP), P106
   Reinhard E., 2010, High dynamic range imaging: Acquisition, display, and image-based lighting
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sheikh H. R., 2003, Live image quality assessment database
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song Y, 2017, IEEE IMAGE PROC, P1012, DOI 10.1109/ICIP.2017.8296434
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tie-Yan Liu, 2009, Foundations and Trends in Information Retrieval, V3, P225, DOI 10.1561/1500000016
   Varga D, 2018, IEEE INT CON MULTI
   Wang L, 2020, ASIAPAC SIGN INFO PR, P1096
   Wang XJ, 2021, IEEE T MULTIMEDIA, V23, P692, DOI 10.1109/TMM.2020.2986583
   Wang ZX, 2020, PROC CVPR IEEE, P1817, DOI 10.1109/CVPR42600.2020.00189
   Wu J, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.3.033025
   Wu QB, 2017, IEEE T MULTIMEDIA, V19, P2490, DOI 10.1109/TMM.2017.2700206
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Jiang XH, 2020, NEUROCOMPUTING, V386, P30, DOI 10.1016/j.neucom.2019.12.027
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Yeganeh H, 2013, IEEE T IMAGE PROCESS, V22, P657, DOI 10.1109/TIP.2012.2221725
   Yue GH, 2023, IEEE T MULTIMEDIA, V25, P6499, DOI 10.1109/TMM.2022.3209889
   Yue GH, 2020, IEEE T IND INFORM, V16, P1764, DOI 10.1109/TII.2019.2927527
   Yue GH, 2019, IEEE T IND ELECTRON, V66, P3784, DOI 10.1109/TIE.2018.2851984
   Yue GH, 2018, IEEE T IND ELECTRON, V65, P2525, DOI 10.1109/TIE.2017.2739708
   Zerman E., 2018, P HUM VIS EL IM, V2018, P1
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang Q., 2017, US Patent, Patent No. 9734567
   Zhu H., 2020, P IEEECVF C COMPUTER, P14143
NR 69
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 11
PY 2024
DI 10.1007/s00371-024-03441-z
EA JUN 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TY4Z7
UT WOS:001244819600001
DA 2024-08-05
ER

PT J
AU Meduri, D
   Sharma, M
   Natarajan, V
AF Meduri, Dhruv
   Sharma, Mohit
   Natarajan, Vijay
TI Jacobi set simplification for tracking topological features in
   time-varying scalar fields
SO VISUAL COMPUTER
LA English
DT Article
DE Time-varying fields; Visualization; Jacobi set; Critical points;
   Topological simplification
ID FIBER SURFACES; VISUALIZATION; EXTRACTION; GRAPHS
AB The Jacobi set of a bivariate scalar field is the set of points where the gradients of the two constituent scalar fields align with each other. It captures the regions of topological changes in the bivariate field. The Jacobi set is a bivariate analog of critical points, and may correspond to features of interest. In the specific case of time-varying fields and when one of the scalar fields is time, the Jacobi set corresponds to temporal tracks of critical points, and serves as a feature-tracking graph. The Jacobi set of a bivariate field or a time-varying scalar field is complex, resulting in cluttered visualizations that are difficult to analyze. This paper addresses the problem of Jacobi set simplification. Specifically, we use the time-varying scalar field scenario to introduce a method that computes a reduced Jacobi set. The method is based on a stability measure called robustness that was originally developed for vector fields and helps capture the structural stability of critical points. We also present a mathematical analysis for the method, and describe an implementation for 2D time-varying scalar fields. Applications to both synthetic and real-world datasets demonstrate the effectiveness of the method for tracking features.
C1 [Meduri, Dhruv] Univ Utah, Salt Lake City, UT 84112 USA.
   [Sharma, Mohit; Natarajan, Vijay] Indian Inst Sci, Bangalore, India.
C3 Utah System of Higher Education; University of Utah; Indian Institute of
   Science (IISC) - Bangalore
RP Meduri, D (corresponding author), Univ Utah, Salt Lake City, UT 84112 USA.
EM u1471195@utah.edu; mohitsharma@iisc.ac.in; vijayn@iisc.ac.in
FU SERB [CRG/2021/005278]; Alexander von Humboldt Foundation; Berlin MATH+
   under the Visiting Scholar program
FX This work is partially supported by a grant from SERB, Govt. of India
   (CRG/2021/005278). VN acknowledges support from the Alexander von
   Humboldt Foundation, and Berlin MATH+ under the Visiting Scholar
   program. Part of this work was completed when VN was a guest Professor
   at the Zuse Institute Berlin.
CR Artamonova IV, 2017, J PHYS CONF SER, V798, DOI 10.1088/1742-6596/798/1/012040
   Ayachit U., 2015, THE PARAVIEW GUIDE
   Bachthaler S, 2008, IEEE T VIS COMPUT GR, V14, P1428, DOI 10.1109/TVCG.2008.119
   Bhatia H, 2015, COMP GEOM-THEOR APPL, V48, P311, DOI 10.1016/j.comgeo.2014.10.009
   Blecha C, 2019, IEEE PAC VIS SYMP, P189, DOI 10.1109/PacificVis.2019.00030
   Bremer PT, 2007, J PHYS CONF SER, V78, DOI 10.1088/1742-6596/78/1/012007
   Bujack R., 2020, STATE ART TIME DEPEN
   Carr H, 2015, COMPUT GRAPH FORUM, V34, P241, DOI 10.1111/cgf.12636
   Carr H, 2014, IEEE T VIS COMPUT GR, V20, P1100, DOI 10.1109/TVCG.2013.269
   Chattopadhyay A., 2014, P EUROVIS SHORT PAP
   Chazal F., 2011, COMPUTING ROBUSTNESS
   Conway J.B., 1973, Functions of One Complex Variable
   Copernicus Marine Service, 2024, US
   Delfinado CecilJose A., 1993, SCG 93, P232, DOI DOI 10.1145/160985.161140
   Doleisch H., 2004, IEEE VIS CONT
   Edelsbrunner H, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P275, DOI 10.1109/VISUAL.2004.68
   Edelsbrunner H, 2002, DISCRETE COMPUT GEOM, V28, P511, DOI 10.1007/s00454-002-2885-2
   EDELSBRUNNER H, 1990, ACM T GRAPHIC, V9, P66, DOI 10.1145/77635.77639
   Edelsbrunner H., 2002, Foundations of Computational Mathematics, P37, DOI DOI 10.1017/CBO9781139106962.003.2
   Edelsbrunner H., 2022, Computational Topology: An Introduction
   Edelsbrunner H, 2008, COMP GEOM-THEOR APPL, V41, P149, DOI 10.1016/j.comgeo.2007.11.001
   Edelsbrunner H, 2011, FOUND COMPUT MATH, V11, P345, DOI 10.1007/s10208-011-9090-8
   Edelsbrunner H, 2008, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SGG'08), P242, DOI 10.1145/1377676.1377720
   Günther T, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073684
   Hansen CD., 2014, SCI VISUALIZATION UN
   Helland-Hansen B., 1916, FORH SKAND NATURF MO, V16, P357
   Huettenberger L, 2014, IEEE T VIS COMPUT GR, V20, P2684, DOI 10.1109/TVCG.2014.2346447
   Klötzl D, 2022, 2022 IEEE WORKSHOP ON TOPOLOGICAL DATA ANALYSIS AND VISUALIZATION (TOPOINVIS 2022), P39, DOI 10.1109/TopoInVis57755.2022.00011
   Klötzl D, 2022, VISUAL COMPUT, V38, P3435, DOI 10.1007/s00371-022-02557-4
   Lukasczyk J, 2021, IEEE T VIS COMPUT GR, V27, P572, DOI 10.1109/TVCG.2020.3030353
   Lukasczyk J, 2020, IEEE T VIS COMPUT GR, V26, P249, DOI 10.1109/TVCG.2019.2934368
   Lukasczyk J, 2017, COMPUT GRAPH FORUM, V36, P13, DOI 10.1111/cgf.13164
   Makela K., 2020, ARXIV
   Nagaraj S, 2011, IEEE T VIS COMPUT GR, V17, P182, DOI 10.1109/TVCG.2010.64
   Norgard G, 2013, COMPUT AIDED GEOM D, V30, P597, DOI 10.1016/j.cagd.2012.03.015
   Popinet S., 2004, ClusterWorld, V2, P7
   Post FH, 2003, COMPUT GRAPH FORUM, V22, P775, DOI 10.1111/j.1467-8659.2003.00723.x
   Raith F, 2019, IEEE T VIS COMPUT GR, V25, P1122, DOI 10.1109/TVCG.2018.2864846
   Reininghaus J, 2012, IEEE T VIS COMPUT GR, V18, P1563, DOI 10.1109/TVCG.2011.269
   Saikia H, 2017, COMPUT GRAPH FORUM, V36, P1, DOI 10.1111/cgf.13163
   Sharma M, 2024, IEEE T VIS COMPUT GR, V30, P3532, DOI 10.1109/TVCG.2023.3237768
   Sharma M, 2022, 2022 IEEE WORKSHOP ON TOPOLOGICAL DATA ANALYSIS AND VISUALIZATION (TOPOINVIS 2022), P49, DOI 10.1109/TopoInVis57755.2022.00012
   Sharma M, 2021, 2021 IEEE VISUALIZATION CONFERENCE - SHORT PAPERS (VIS 2021), P96, DOI 10.1109/VIS49827.2021.9623300
   Skraba P, 2015, IEEE T VIS COMPUT GR, V21, P930, DOI 10.1109/TVCG.2015.2440250
   Soler M, 2018, SYMP LARG DATA ANAL, P23, DOI 10.1109/LDAV.2018.8739196
   Suthambhara N, 2011, MATH VIS, P91
   Theisel H., 2003, Data Visualisation 2003. Joint Eurographics/IEEE TCVG. Symposium on Visualization, P141
   Tierny J, 2018, IEEE T VIS COMPUT GR, V24, P832, DOI 10.1109/TVCG.2017.2743938
   Tierny J, 2017, IEEE T VIS COMPUT GR, V23, P960, DOI 10.1109/TVCG.2016.2599017
   Tricoche X, 2002, COMPUT GRAPH-UK, V26, P249, DOI 10.1016/S0097-8493(02)00056-0
   Weinkauf T, 2011, IEEE T VIS COMPUT GR, V17, P770, DOI 10.1109/TVCG.2010.93
   Yan L, 2021, COMPUT GRAPH FORUM, V40, P599, DOI 10.1111/cgf.14331
NR 52
TC 2
Z9 2
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4843
EP 4855
DI 10.1007/s00371-024-03484-2
EA JUN 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001242155100006
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Xu, H
   Wu, YQ
   Tang, XJ
   Zhang, J
   Zhang, Y
   Zhang, ZB
   Li, C
   Jin, XG
AF Xu, Hao
   Wu, Yiqian
   Tang, Xiangjun
   Zhang, Jing
   Zhang, Yang
   Zhang, Zhebin
   Li, Chen
   Jin, Xiaogang
TI FusionDeformer: text-guided mesh deformation using diffusion models
SO VISUAL COMPUTER
LA English
DT Article
DE Diffusion model; Mesh deformation; Score Distillation Sampling
AB Mesh deformation has a wide range of applications, including character creation, geometry modelling, deforming animation, and morphing. Recently, mesh deformation methods based on CLIP models demonstrated the ability to perform automatic text-guided mesh deformation. However, using 2D guidance to deform a 3D mesh attempts to solve an ill-posed problem and leads to distortion and unsmoothness, which cannot be eliminated by CLIP-based methods because they focus on semantic-aware features and cannot identify these artefacts. To this end, we propose FusionDeformer, a novel automatic text-guided mesh deformation method that leverages diffusion models. The deformation is achieved by Score Distillation Sampling, which minimizes the KL-divergence between the distribution of rendered deformed mesh and the text-conditioned distribution. To alleviate the intrinsic ill-posed problem, we incorporate two approaches into our framework. The first approach involves combining multiple orthogonal views into a single image, providing robust deformation while avoiding the need for additional memory. The second approach incorporates a new regularization to address the unsmooth artefacts. Our experimental results show that the proposed method can generate high-quality, smoothly deformed meshes that align precisely with the input text description while preserving the topological relationships. Additionally, our method offers a text2morphing approach to animation design, enabling common users to produce special effects animation.
C1 [Xu, Hao; Wu, Yiqian; Tang, Xiangjun; Zhang, Jing; Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Peoples R China.
   [Zhang, Yang] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
   [Zhang, Zhebin; Li, Chen] OPPO US Res Ctr, Seattle, WA USA.
C3 Zhejiang University; Zhejiang University
RP Jin, XG (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Peoples R China.
EM jin@cad.zju.edu.cn
FU Key Research and Development Program of Zhejiang Province [2023C01047];
   Key R &D Program of Zhejiang
FX This work was supported by Key R &D Program of Zhejiang (No.
   2023C01047).
CR Aigerman N, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530141
   Bailey SW, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392397
   Barron JT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5835, DOI 10.1109/ICCV48922.2021.00580
   CAO Y., 2023, ARXIV
   Chen R., 2023, P IEEE CVF INT C COM, V22, P246
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   Gadelha M, 2017, INT CONF 3D VISION, P402, DOI 10.1109/3DV.2017.00053
   Gal R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531339
   Gao W, 2023, PROCEEDINGS OF SIGGRAPH 2023 CONFERENCE PAPERS, SIGGRAPH 2023, DOI 10.1145/3588432.3591552
   Han X., 2024, ADV NEURAL INFORM PR, V36
   Hanocka R, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3267347
   Henzler P, 2019, IEEE I CONF COMP VIS, P9983, DOI 10.1109/ICCV.2019.01008
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Huang QX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5795, DOI 10.1109/ICCV48922.2021.00576
   Huang Shuo, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P5734, DOI 10.1145/3581783.3612022
   Huang Yangyi, 2024, INT C 3D VIS 3DV
   Jacobson A., 2013, THESIS ETH ZURICH
   Jain A, 2022, PROC CVPR IEEE, P857, DOI 10.1109/CVPR52688.2022.00094
   Jakab T, 2021, PROC CVPR IEEE, P12778, DOI 10.1109/CVPR46437.2021.01259
   Kanazawa A, 2018, LECT NOTES COMPUT SC, V11219, P386, DOI 10.1007/978-3-030-01267-0_23
   Khalid NM, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555392
   Kim B, 2023, IEEE I CONF COMP VIS, P15919, DOI 10.1109/ICCV51070.2023.01463
   Kraevoy V, 2004, ACM T GRAPHIC, V23, P861, DOI 10.1145/1015706.1015811
   Laine S, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417861
   Li Wenzhe, 2023, ARXIV
   Lin CH, 2023, PROC CVPR IEEE, P300, DOI 10.1109/CVPR52729.2023.00037
   Luo ST, 2021, PROC CVPR IEEE, P2836, DOI 10.1109/CVPR46437.2021.00286
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Michel O, 2022, PROC CVPR IEEE, P13482, DOI 10.1109/CVPR52688.2022.01313
   Mildenhall B, 2020, P ECCV, DOI DOI 10.1007/978-3-030-58452-8
   Mo KC, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356527
   Nichol P., 2021, Improved denoising diffusion probabilistic models, P8162, DOI DOI 10.48550/ARXIV.2102.09672
   Poole Ben, 2023, 11 INT C LEARN REPR
   Radford A, 2021, PR MACH LEARN RES, V139
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Romero Cristian, 2021, ACM Transactions on Graphics, V40, DOI 10.1145/3476576.3476703
   Saharia C., 2022, ADV NEURAL INF PROCE, V36, P479
   Sanghi A, 2022, PROC CVPR IEEE, P18582, DOI 10.1109/CVPR52688.2022.01805
   Shi Y., 2023, Arxiv
   Sorkine O., 2009, EUROGRAPHICS TUTORIA, P11
   Stan G.B.M., 2023, ARXIV
   Tan QY, 2018, PROC CVPR IEEE, P5841, DOI 10.1109/CVPR.2018.00612
   Tang J., 2023, P IEEE CVF INT C COM, V17, P739
   Tsalicoglou Christina, 2023, arXiv
   von Platen P., 2022, Diffusers: State-of-the-art diffusion models
   Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4
   Worchel M, 2022, PROC CVPR IEEE, P6177, DOI 10.1109/CVPR52688.2022.00609
   Yang GD, 2019, IEEE I CONF COMP VIS, P4540, DOI 10.1109/ICCV.2019.00464
   Yariv L, 2021, ADV NEUR IN
   Yifan W, 2020, PROC CVPR IEEE, P72, DOI 10.1109/CVPR42600.2020.00015
   Yumer ME, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766908
   Zhao M., 2023, ARXIV
   Zheng ML, 2021, PROC CVPR IEEE, P5928, DOI 10.1109/CVPR46437.2021.00587
NR 53
TC 1
Z9 1
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4701
EP 4712
DI 10.1007/s00371-024-03463-7
EA MAY 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001234039700001
DA 2024-08-05
ER

PT J
AU Yang, GM
   Ding, YF
   Fang, XJ
   Zhang, J
   Chu, Y
AF Yang, Gaoming
   Ding, Yifeng
   Fang, Xianjin
   Zhang, Ji
   Chu, Yan
TI Fast face swapping with high-fidelity lightweight generator assisted by
   online knowledge distillation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Face swapping; Online knowledge distillation; Identity-irrelevant
   similarity loss; High-fidelity lightweight generator
ID IMAGE
AB Advanced face swapping approaches have achieved high-fidelity results. However, the success of most methods hinges on heavy parameters and high-computational costs. With the popularity of real-time face swapping, these factors have become obstacles restricting their swap speed and application. To overcome these challenges, we propose a high-fidelity lightweight generator (HFLG) for face swapping, which is a compressed version of the existing network Simple Swap and consists of its 1/4 channels. Moreover, to stabilize the learning of HFLG, we introduce feature map-based online knowledge distillation into our training process and improve the teacher-student architecture. Specifically, we first enhance our teacher generator to provide more efficient guidance. It minimizes the loss of details on the lower face. In addition, a new identity-irrelevant similarity loss is proposed to improve the preservation of non-facial regions in the teacher generator results. Furthermore, HFLG uses an extended identity injection module to inject identity more efficiently. It gradually learns face swapping by imitating the feature maps and outputs of the teacher generator online. Extensive experiments on faces in the wild demonstrate that our method achieves comparable results with other methods while having fewer parameters, lower computations, and faster inference speed. The code is available at https://github.com/EifelTing/HFLFS.
C1 [Yang, Gaoming; Ding, Yifeng; Fang, Xianjin] Anhui Univ Sci & Technol, Sch Comp Sci & Engn, Huainan, Peoples R China.
   [Zhang, Ji] Univ Southern Queensland, Sch Math Phys & Comp, Toowoomba, Qld, Australia.
   [Chu, Yan] Harbin Engn Univ, Coll Comp Sci & Technol, Harbin, Peoples R China.
C3 Anhui University of Science & Technology; University of Southern
   Queensland; Harbin Engineering University
RP Ding, YF (corresponding author), Anhui Univ Sci & Technol, Sch Comp Sci & Engn, Huainan, Peoples R China.
EM gmyang@aust.edu.cn; yfding@aust.edu.cn; xjfang@aust.edu.cn;
   Ji.Zhang@unisq.edu.au; chuyan@hrbeu.edu.cn
FU Anhui Natural Science Foundation of China; Academic Funding Project for
   Top Talents in University Disciplines [gxbjZD2021050]; Anhui Provincial
   Higher Education Institutions Scientific Research Project
   [2022AH040113]; Anhui University of Science and Technology [2023,
   2023cx2136];  [2308085MF218]
FX This work was supported in part by the Anhui Natural Science Foundation
   of China under grant number 2308085MF218, in part by the Academic
   Funding Project for Top Talents in University Disciplines under grant
   number gxbjZD2021050, in part by the Anhui Provincial Higher Education
   Institutions Scientific Research Project under grant number
   2022AH040113, and in part by the Anhui University of Science and
   Technology 2023 Graduate Innovation Fund Project under grant number
   2023cx2136.
CR Andrew Brock J.D., 2019, P INT C LEARN REPR I
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chen PG, 2021, PROC CVPR IEEE, P5006, DOI 10.1109/CVPR46437.2021.00497
   Chen RW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2003, DOI 10.1145/3394171.3413630
   De Brabandere B, 2016, ADV NEUR IN, V29
   DeepFakes, about us
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Heo B, 2019, IEEE I CONF COMP VIS, P1921, DOI 10.1109/ICCV.2019.00201
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hu T, 2023, PROC CVPR IEEE, P20351, DOI 10.1109/CVPR52729.2023.01949
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Ioffe Sergey, 2015, INT C MACHINE LEARNI, V37, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Gulrajani I, 2017, ADV NEUR IN, V30
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Langner O, 2010, COGNITION EMOTION, V24, P1377, DOI 10.1080/02699930903485076
   Li LZ, 2020, PROC CVPR IEEE, P5073, DOI 10.1109/CVPR42600.2020.00512
   Li MY, 2022, IEEE T PATTERN ANAL, V44, P9331, DOI 10.1109/TPAMI.2021.3126742
   Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065
   Liu ZA, 2023, PROC CVPR IEEE, P8578, DOI 10.1109/CVPR52729.2023.00829
   Nirkin Y, 2023, IEEE T PATTERN ANAL, V45, P560, DOI 10.1109/TPAMI.2022.3155571
   Nirkin Y, 2019, IEEE I CONF COMP VIS, P7183, DOI 10.1109/ICCV.2019.00728
   Nirkin Y, 2018, IEEE INT CONF AUTOMA, P98, DOI 10.1109/FG.2018.00024
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Ren YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6773, DOI 10.1109/ICCV48922.2021.00672
   Rössler A, 2019, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2019.00009
   Rosberg F, 2023, IEEE WINT CONF APPL, P3443, DOI 10.1109/WACV56688.2023.00345
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Ruiz N, 2018, IEEE COMPUT SOC CONF, P2155, DOI 10.1109/CVPRW.2018.00281
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tero Karras T.A, 2018, P INT C LEARN REPR, P26
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang Yuhan, 2021, P 30 INT JOINT C ART, P1136, DOI DOI 10.24963/IJCAI.2021/157
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu YY, 2022, PROC CVPR IEEE, P7632, DOI 10.1109/CVPR52688.2022.00749
   Xu ZL, 2022, AAAI CONF ARTIF INTE, P2973
   Xu ZL, 2021, AAAI CONF ARTIF INTE, V35, P3083
   Yuan G, 2023, Arxiv, DOI arXiv:2306.05356
   Zhang LF, 2022, PROC CVPR IEEE, P12454, DOI 10.1109/CVPR52688.2022.01214
   Zhu YH, 2021, PROC CVPR IEEE, P4832, DOI 10.1109/CVPR46437.2021.00480
   zllrunning, 2019, face-parsing.pytorch
NR 49
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 14
PY 2024
DI 10.1007/s00371-024-03414-2
EA MAY 2024
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QQ7N8
UT WOS:001222401900001
DA 2024-08-05
ER

PT J
AU Wang, X
   Feng, J
   Ding, JJ
   Gao, J
AF Wang, Xin
   Feng, Jin
   Ding, Jiajia
   Gao, Jun
TI Light field salient object detection based on discrete viewpoint
   selection and multi-feature fusion
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Light field image; Multi-view image; Depth information;
   Disparity-relatedness; Spatial feature
ID GEOMETRY; NETWORK; MODEL
AB Light field imaging has been widely acknowledged for its ability to capture both spatial and angular information of a scene, which can improve the performance of salient object detection (SOD) in complex environments. Existing approaches based on refocused images mainly explore the spatial features of different focus areas, while methods based on multi-view images are plagued by limitations such as data redundancy and high computational costs. In this study, we introduce a novel discrete viewpoint selection scheme to mitigate data redundancy. We also leverage the geometric characteristics of light field multi-view images to design a disparity extraction module that extracts disparity-relatedness between the selected viewpoints. Additionally, we construct a multi-feature fusion-feedback module to achieve mutual fusion of multiple features including spatial, edge, and depth for more accurate SOD. To validate our approach, we compare it with 12 existing methods on three datasets, and our results demonstrate a balance between multi-view image redundancy and model performance. Our method accurately locates salient objects even in challenging scenarios such as multiple objects and complex backgrounds, thereby achieving high-precision SOD.
C1 [Wang, Xin; Feng, Jin; Ding, Jiajia; Gao, Jun] Hefei Univ Technol, Sch Comp & Informat, Hefei 230601, Anhui, Peoples R China.
   [Wang, Xin] Intelligent Interconnected Syst Lab Anhui Prov, Hefei 230601, Anhui, Peoples R China.
C3 Hefei University of Technology
RP Wang, X (corresponding author), Hefei Univ Technol, Sch Comp & Informat, Hefei 230601, Anhui, Peoples R China.; Wang, X (corresponding author), Intelligent Interconnected Syst Lab Anhui Prov, Hefei 230601, Anhui, Peoples R China.
EM wangxin@hfut.edu.cn; 2020111053@mail.hfut.edu.cn;
   jiajiading@mail.hfut.edu.cn; gaojun@hfut.edu.cn
FU National Natural Science Foundation of China [62171178, 61801161,
   61971177]; National Natural Science Foundation of China [1908085QF282];
   Natural Science Foundation of Anhui Province [JZ2020HGTB0048];
   Fundamental Research Funds for the Central Universities
FX This work was supported by the National Natural Science Foundation of
   China (62171178, 61801161, and 61971177), the Natural Science Foundation
   of Anhui Province (1908085QF282) and the Fundamental Research Funds for
   the Central Universities (JZ2020HGTB0048).
CR Arya R, 2017, APPL INTELL, V46, P254, DOI 10.1007/s10489-016-0819-6
   Bo Li L. T., 2021, P IEEE CVF INT C COM, P3580
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Cheng MM, 2022, IEEE T PATTERN ANAL, V44, P8006, DOI 10.1109/TPAMI.2021.3107956
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fu KR, 2022, IEEE T PATTERN ANAL, V44, P5541, DOI 10.1109/TPAMI.2021.3073689
   Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312
   Gao W., 2023, IEEE Trans. Pattern Anal. Mach. Intell.
   Gao W, 2022, IEEE T CIRC SYST VID, V32, P2091, DOI 10.1109/TCSVT.2021.3082939
   Guanyu Z., 2022, Appl. Intell., P1
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong S, 2015, PR MACH LEARN RES, V37, P597
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang Y, 2022, NEUROCOMPUTING, V491, P78, DOI 10.1016/j.neucom.2022.03.056
   Jing D, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1692, DOI 10.1145/3474085.3475312
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Li GB, 2018, PROC CVPR IEEE, P3243, DOI 10.1109/CVPR.2018.00342
   Li NY, 2015, PROC CVPR IEEE, P5216, DOI 10.1109/CVPR.2015.7299158
   Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359
   Liu GH, 2014, PROCEEDINGS OF 2013 INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND CLOUD COMPUTING COMPANION (ISCC-C), P728, DOI 10.1109/ISCC-C.2013.21
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Máttyus G, 2017, IEEE I CONF COMP VIS, P3458, DOI 10.1109/ICCV.2017.372
   Ng R., 2005, Comput. Sci. Tech. Rep. (CSTR), V2, P1, DOI DOI 10.1145/3097571
   Piao YR, 2020, Arxiv, DOI [arXiv:2012.15124, DOI 10.48550/ARXIV.2012.15124]
   Piao YR, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P904
   Piao YR, 2020, AAAI CONF ARTIF INTE, V34, P11865
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606
   Ronneberger P., 2015, MEDICAL IMAGE COMPUT, P234, DOI [10.1007/978-3-319-24574-4_28, DOI 10.1007/978-3-319-24574-4_28]
   Shin C, 2018, PROC CVPR IEEE, P4748, DOI 10.1109/CVPR.2018.00499
   Wang TT, 2019, IEEE I CONF COMP VIS, P8837, DOI 10.1109/ICCV.2019.00893
   Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330
   Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433
   Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154
   Wang ZC, 2016, APPL INTELL, V45, P1, DOI 10.1007/s10489-015-0739-x
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Wenguan Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3395, DOI 10.1109/CVPR.2015.7298961
   Wu RM, 2019, PROC CVPR IEEE, P8142, DOI 10.1109/CVPR.2019.00834
   Wu Z, 2019, IEEE I CONF COMP VIS, P7263, DOI 10.1109/ICCV.2019.00736
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Zhang HS, 2022, IEEE T PATTERN ANAL, V44, P3577, DOI 10.1109/TPAMI.2021.3059783
   Zhang J, 2022, IEEE T PATTERN ANAL, V44, P5761, DOI 10.1109/TPAMI.2021.3073564
   Zhang J, 2020, IEEE T IMAGE PROCESS, V29, P4421, DOI 10.1109/TIP.2020.2970529
   Zhang J, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3107956
   Zhang J, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2212
   Zhang M., Advances in Neural Information Processing Systems, V32, P898
   Zhang M, 2020, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR42600.2020.00353
   Zhang M, 2020, IEEE T IMAGE PROCESS, V29, P6276, DOI 10.1109/TIP.2020.2990341
   Zhang QD, 2021, IEEE T IMAGE PROCESS, V30, P7578, DOI 10.1109/TIP.2021.3108018
   Zhang QD, 2021, IEEE T CIRC SYST VID, V31, P1849, DOI 10.1109/TCSVT.2020.3013119
   Zhang ZY, 2016, PROC CVPR IEEE, P669, DOI 10.1109/CVPR.2016.79
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
NR 57
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 20
PY 2024
DI 10.1007/s00371-024-03375-6
EA APR 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OD9B8
UT WOS:001205429200001
DA 2024-08-05
ER

PT J
AU Takao, S
AF Takao, Shunsuke
TI Underwater image sharpening and color correction via dataset based on
   revised underwater image formation model
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Underwater image dataset; Underwater image enhancement; MCMC; Style
   transfer
ID ENHANCEMENT; SPACE; WATER
AB Underwater images bring about substantial information to many tasks regarding marine science or coastal engineering. Meanwhile, enhancement of serious underwater image degradation like wavelength-dependent color distortion or decreased contrast is essential in practical applications. Although deep learning-based underwater image enhancement methods have increasingly been developed, construction of a large-scale underwater image dataset is still a remaining issue. Currently, expensive cost and the difficulty of measurement disturb collection of real data. On the other hand, alternatively employed synthetic underwater images based on simplified physical model or generative adversarial network may deviate from real data. In order to reduce the domain gap between real and synthetic underwater images, we generate underwater images based on physically revised underwater image formation model. By reformulating the model as Monte Carlo integration in statistical physics, we avoid variable multiplication and enable the calculation. The constructed dataset is shown to include diverse degradation and be closer to real images as well. Subsequently, underwater image color correction is tackled via exemplar-based style transfer to cope with diverse color cast. Finally, simply designed image sharpening algorithm combining discrete wavelet transform and Laplacian pyramid is proposed to improve the visibility. The proposed scheme mainly achieves superior or competitive performance compared to other latest methods.
C1 [Takao, Shunsuke] Port & Airport Res Inst, Yokosuka, Japan.
C3 Port & Airport Research Institute
RP Takao, S (corresponding author), Port & Airport Res Inst, Yokosuka, Japan.
EM takao.s.work@gmail.com
OI Takao, Shunsuke/0000-0003-0938-3178
CR Akkaynak D, 2019, PROC CVPR IEEE, P1682, DOI 10.1109/CVPR.2019.00178
   Akkaynak D, 2018, PROC CVPR IEEE, P6723, DOI 10.1109/CVPR.2018.00703
   Akkaynak D, 2017, PROC CVPR IEEE, P568, DOI 10.1109/CVPR.2017.68
   Ancuti CO, 2018, IEEE T IMAGE PROCESS, V27, P379, DOI 10.1109/TIP.2017.2759252
   Anwar S, 2020, SIGNAL PROCESS-IMAGE, V89, DOI 10.1016/j.image.2020.115978
   Aubry M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629645
   Bojanowski P, 2018, PR MACH LEARN RES, V80
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Caflisch R. E., 1998, Acta Numerica, V7, P1, DOI 10.1017/S0962492900002804
   Cao B, 2023, IEEE I CONF COMP VIS, P23498, DOI 10.1109/ICCV51070.2023.02153
   Cao B, 2023, INT J COMPUT VISION, V131, P1995, DOI 10.1007/s11263-023-01791-0
   Chen YW, 2022, IEEE ACCESS, V10, P90523, DOI 10.1109/ACCESS.2022.3201555
   Chiang JY, 2012, IEEE T IMAGE PROCESS, V21, P1756, DOI 10.1109/TIP.2011.2179666
   Cho W, 2019, PROC CVPR IEEE, P10631, DOI 10.1109/CVPR.2019.01089
   Demirel H, 2011, IEEE T IMAGE PROCESS, V20, P1458, DOI 10.1109/TIP.2010.2087767
   Fabbri C, 2018, IEEE INT CONF ROBOT, P7159
   Fu XP, 2023, MULTIMED TOOLS APPL, V82, P32941, DOI 10.1007/s11042-023-14871-7
   Fu XY, 2014, IEEE IMAGE PROC, P4572, DOI 10.1109/ICIP.2014.7025927
   Fu ZQ, 2022, LECT NOTES COMPUT SC, V13678, P465, DOI 10.1007/978-3-031-19797-0_27
   Gao WS, 2010, INT CONF COMP SCI, P67, DOI 10.1109/ICCSIT.2010.5563693
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gu Jinjin, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P633, DOI 10.1007/978-3-030-58621-8_37
   HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.1093/biomet/57.1.97
   Huang SR, 2023, PROC CVPR IEEE, P18145, DOI 10.1109/CVPR52729.2023.01740
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Hunt RWG, 2011, WILEY-ISTE, P41
   Islam MJ, 2020, ROBOTICS: SCIENCE AND SYSTEMS XVI
   Islam MJ, 2020, IEEE ROBOT AUTOM LET, V5, P3227, DOI 10.1109/LRA.2020.2974710
   Jerlov N. G., 1976, MARINE OPTICS
   Jiang J, 2013, IEEE WORK APP COMP, P168, DOI 10.1109/WACV.2013.6475015
   Kingma D. P., 2014, arXiv
   Koschmieder H., 1924, BEITR PHYS FREIEN AT, P33, DOI DOI 10.1007/978-3-663-04661-5_2
   Li CY, 2021, IEEE T IMAGE PROCESS, V30, P4985, DOI 10.1109/TIP.2021.3076367
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li CY, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107038
   Liu K, 2021, OPT EXPRESS, V29, P28307, DOI 10.1364/OE.428626
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   MUplavikar P., 2019, IEEE C COMP VIS PATT
   Ouyang T, 2024, VISUAL COMPUT, DOI 10.1007/s00371-023-03215-z
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Pascale D., 2006, The BabelColor Company, V6
   PELI E, 1990, J OPT SOC AM A, V7, P2032, DOI 10.1364/JOSAA.7.002032
   Peng LT, 2023, IEEE T IMAGE PROCESS, V32, P3066, DOI 10.1109/TIP.2023.3276332
   Schechner YY, 2005, IEEE J OCEANIC ENG, V30, P570, DOI 10.1109/JOE.2005.850871
   Schettini R, 2010, EURASIP J ADV SIG PR, DOI 10.1155/2010/746052
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Solonenko MG, 2015, APPL OPTICS, V54, P5392, DOI 10.1364/AO.54.005392
   Takao S., 2022, EUR C COMP VIS WORKS, P721
   Wang W, 2022, IEEE T PATTERN ANAL, V44, P5472, DOI 10.1109/TPAMI.2021.3072422
   Wang Y, 2019, IEEE ACCESS, V7, P140233, DOI 10.1109/ACCESS.2019.2932130
   Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI [10.1007/s11263-019-01198-w, 10.1109/CSTIC.2018.8369274]
   Xue QQ, 2024, VISUAL COMPUT, V40, P5475, DOI 10.1007/s00371-023-03117-0
   Yang M, 2015, IEEE T IMAGE PROCESS, V24, P6062, DOI 10.1109/TIP.2015.2491020
   Yoo J, 2019, IEEE I CONF COMP VIS, P9035, DOI 10.1109/ICCV.2019.00913
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhou JC, 2022, OPT EXPRESS, V30, P17290, DOI 10.1364/OE.450858
   Zhou Y, 2020, Arxiv, DOI arXiv:2002.09315
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhuang PX, 2022, IEEE T IMAGE PROCESS, V31, P5442, DOI 10.1109/TIP.2022.3196546
NR 60
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 13
PY 2024
DI 10.1007/s00371-024-03377-4
EA APR 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NO7T5
UT WOS:001201464900001
DA 2024-08-05
ER

PT J
AU Rahimpour, SM
   Kazemi, M
   Moallem, P
   Safayani, M
AF Rahimpour, Seyed Mohammad
   Kazemi, Mohammad
   Moallem, Payman
   Safayani, Mehran
TI Video anomaly detection based on attention and efficient spatio-temporal
   feature extraction
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Video anomaly detection; Attention; Transfer learning; LSTM
   auto-encoders
AB An anomaly is a pattern, behavior, or event that does not frequently happen in an environment. Video anomaly detection has always been a challenging task. Home security, public area monitoring, and quality control in production lines are only a few applications of video anomaly detection. The spatio-temporal nature of the videos, the lack of an exact definition for anomalies, and the inefficiencies of feature extraction for videos are examples of the challenges that researchers face in video anomaly detection. To find a solution to these challenges, we propose a method that uses parallel deep structures to extract informative features from the videos. The method consists of different units including an attention unit, frame sampling units, spatial and temporal feature extractors, and thresholding. Using these units, we propose a video anomaly detection that aggregates the results of four parallel structures. Aggregating the results brings generality and flexibility to the algorithm. The proposed method achieves satisfying results for four popular video anomaly detection benchmarks.
C1 [Rahimpour, Seyed Mohammad; Kazemi, Mohammad; Moallem, Payman] Univ Isfahan, Dept Elect Engn, Esfahan, Iran.
   [Safayani, Mehran] Isfahan Univ Technol, Dept Elect & Comp Engn, Esfahan 8415683111, Iran.
C3 University of Isfahan; Isfahan University of Technology
RP Kazemi, M (corresponding author), Univ Isfahan, Dept Elect Engn, Esfahan, Iran.
EM m.kazemi@eng.ui.ac.ir
CR Acsintoae A, 2022, PROC CVPR IEEE, P20111, DOI 10.1109/CVPR52688.2022.01951
   Asad M, 2022, APPL INTELL, V52, P1126, DOI 10.1007/s10489-021-02356-9
   Aslam N, 2024, VISUAL COMPUT, V40, P1729, DOI 10.1007/s00371-023-02882-2
   Chandrakala S, 2023, ARTIF INTELL REV, V56, P3319, DOI 10.1007/s10462-022-10258-6
   Chang YP, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108213
   Chaurasia Rajeev Kumar, 2023, International Journal of Information Technology, P1569, DOI 10.1007/s41870-023-01193-y
   Chu WQ, 2019, IEEE T MULTIMEDIA, V21, P246, DOI 10.1109/TMM.2018.2846411
   cse.cuhk, About us
   Desai Padmashree, 2022, Journal of Physics: Conference Series, V2161, DOI 10.1088/1742-6596/2161/1/012024
   Fu Z., 2005, IEEE INT C IMAGE PRO, V2, pII, DOI [10.1109/ICIP.2005.1530127, DOI 10.1109/ICIP.2005.1530127]
   Gayal BS, 2023, MULTIMED TOOLS APPL, V82, P28895, DOI 10.1007/s11042-023-14917-w
   github, About Us
   Gong D, 2019, IEEE I CONF COMP VIS, P1705, DOI 10.1109/ICCV.2019.00179
   Hu JT, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19194145
   keras.io, About us
   Lai YD, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102894
   Li H, 2012, IET SIGNAL PROCESS, V6, P521, DOI 10.1049/iet-spr.2011.0074
   Lin ZH, 2020, AAAI CONF ARTIF INTE, V34, P11531
   Liu WR, 2023, PROC CVPR IEEE, P12147, DOI 10.1109/CVPR52729.2023.01169
   Luo WX, 2021, IEEE T PATTERN ANAL, V43, P1070, DOI 10.1109/TPAMI.2019.2944377
   Luo WX, 2017, IEEE INT CON MULTI, P439, DOI 10.1109/ICME.2017.8019325
   Mukherjee S, 2019, INT CONF ACOUST SPEE, P2027, DOI [10.1109/ICASSP.2019.8682158, 10.1109/icassp.2019.8682158]
   Nayak R, 2021, IMAGE VISION COMPUT, V106, DOI 10.1016/j.imavis.2020.104078
   Piciarelli C, 2008, IEEE T CIRC SYST VID, V18, P1544, DOI 10.1109/TCSVT.2008.2005599
   Raja R, 2023, MULTIMED TOOLS APPL, V82, P12635, DOI 10.1007/s11042-022-13954-1
   Sabih M, 2022, VISUAL COMPUT, V38, P1719, DOI 10.1007/s00371-021-02100-x
   Sabokrou M, 2016, ELECTRON LETT, V52, P1122, DOI 10.1049/el.2016.0440
   Sabokrou M, 2018, COMPUT VIS IMAGE UND, V172, P88, DOI 10.1016/j.cviu.2018.02.006
   Sabokrou M, 2017, IEEE T IMAGE PROCESS, V26, P1992, DOI 10.1109/TIP.2017.2670780
   Sadr AV, 2023, NEURAL COMPUT APPL, V35, P1157, DOI 10.1007/s00521-021-05839-5
   Shi XJ, 2015, Arxiv, DOI [arXiv:1506.04214, DOI 10.48550/ARXIV.1506.04214, 10.48550/arXiv.1506.04214]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh D, 2019, IEEE T INTELL TRANSP, V20, P879, DOI 10.1109/TITS.2018.2835308
   Sun JY, 2018, IEEE ACCESS, V6, P33353, DOI 10.1109/ACCESS.2018.2848210
   svcl.ucsd, About us
   svip-lab, About us
   Wang T, 2019, IEEE T INF FOREN SEC, V14, P1390, DOI 10.1109/TIFS.2018.2878538
   Wang XZ, 2022, IEEE T NEUR NET LEAR, V33, P2301, DOI 10.1109/TNNLS.2021.3083152
   Wang ZM, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2463, DOI 10.1145/3394171.3413529
   Yadav D., 2023, Comput. Vis. Mach. Intell. Proc. CVMI, V2022, P489, DOI [10.1007/978-981-19-7867-8_39, DOI 10.1007/978-981-19-7867-8_39]
   Ye MC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1805, DOI 10.1145/3343031.3350899
   Yu Q, 2021, APPL INTELL, V51, P3241, DOI 10.1007/s10489-020-01944-5
   Yunpeng Chang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P329, DOI 10.1007/978-3-030-58555-6_20
   Zhang JX, 2022, IEEE T CIRC SYST VID, V32, P8646, DOI 10.1109/TCSVT.2022.3193574
   Zhang Y, 2021, IEEE T CIRC SYST VID, V31, P3694, DOI 10.1109/TCSVT.2020.3039798
   Zhong YH, 2022, IEEE T CIRC SYST VID, V32, P8285, DOI 10.1109/TCSVT.2022.3190539
NR 46
TC 0
Z9 0
U1 11
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 4
PY 2024
DI 10.1007/s00371-024-03361-y
EA APR 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MX3A6
UT WOS:001196884200002
DA 2024-08-05
ER

PT J
AU Zhang, H
   Jia, DL
   Ma, H
AF Zhang, Heng
   Jia, Dongli
   Ma, Hui
TI Rainy day image semantic segmentation based on two-stage progressive
   network
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Deep learning; Semantic segmentation; Progressive network; End-to-end
   network; Attention mechanism
AB Semantic segmentation plays a crucial role in the fields of computer vision and computer graphics, with extensive applications in various practical scenarios. Significant progress has been made in semantic segmentation tasks using deep learning-based methods. However, most existing semantic segmentation algorithms focus on good weather conditions, and they face challenges in terms of accuracy and robustness when applied to rainy scenes due to factors such as raindrops, haze, and lighting variations. To address this issue, this paper proposes a rainy-day semantic segmentation method based on a two-stage progressive network. The proposed method consists of two modules: a rain removal module responsible for eliminating raindrops and haze from the input rainy images and restoring the basic structural information of the images, and a segmentation module that performs pixel-level semantic prediction on the rain-removed images. Specifically, the rain removal module introduces two progressive units with shared weights to gradually achieve rain removal. The segmentation module adopts an encoder-decoder architecture, utilizing down-sampling and deep asynchronous bottleneck units for encoding. It also introduces a dual attention-guided fusion module to aggregate channel attention information and spatial attention information, guiding the multiscale feature fusion process in the decoder. Experimental results demonstrate that this method effectively mitigates the influence of rain streaks on semantic segmentation, thereby improving segmentation performance and achieving more accurate and robust semantic segmentation results in rainy conditions. We will provide the code and datasets on https://github.com/zhang152267/TSPN.
C1 [Zhang, Heng; Jia, Dongli; Ma, Hui] Hebei Univ Engn, Sch Informat & Elect Engn, Handan, Peoples R China.
C3 Hebei University of Engineering
RP Jia, DL (corresponding author), Hebei Univ Engn, Sch Informat & Elect Engn, Handan, Peoples R China.
EM jiadongli@hebeu.edu.cn
OI Jia, Dongli/0000-0002-4368-5826
CR Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Brostow GJ, 2008, LECT NOTES COMPUT SC, V5302, P44, DOI 10.1007/978-3-540-88682-2_5
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng ZM, 2022, VISUAL COMPUT, V38, P749, DOI 10.1007/s00371-021-02075-9
   Elhassan MAM, 2024, Arxiv, DOI arXiv:2206.07298
   Elhassan MAM, 2021, EXPERT SYST APPL, V183, DOI 10.1016/j.eswa.2021.115090
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Gao GW, 2023, IEEE T MULTIMEDIA, V25, P3273, DOI 10.1109/TMM.2022.3157995
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gould S, 2009, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2009.5459211
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   Hu P, 2021, IEEE ROBOT AUTOM LET, V6, P263, DOI 10.1109/LRA.2020.3039744
   Huang YF, 2024, VISUAL COMPUT, V40, P3427, DOI 10.1007/s00371-023-03043-1
   Jiang K, 2021, IEEE T IMAGE PROCESS, V30, P7404, DOI 10.1109/TIP.2021.3102504
   Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057
   Kui Jiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8343, DOI 10.1109/CVPR42600.2020.00837
   Li G, 2019, Arxiv, DOI arXiv:1907.11357
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299
   Liang TJ, 2023, KSII T INTERNET INF, V17, P1996, DOI 10.3837/tiis.2023.08.002
   Liang TJ, 2022, J ADV TRANSPORT, V2022, DOI 10.1155/2022/3825532
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu MX, 2022, IEEE T INTELL TRANSP, V23, P3522, DOI 10.1109/TITS.2020.3037727
   Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388
   Pan HH, 2023, IEEE T INTELL TRANSP, V24, P3448, DOI 10.1109/TITS.2022.3228042
   Paszke A, 2016, Arxiv, DOI arXiv:1606.02147
   Paszke A, 2019, ADV NEUR IN, V32
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Romera E, 2018, IEEE T INTELL TRANSP, V19, P263, DOI 10.1109/TITS.2017.2750080
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shi XJ, 2015, ADV NEUR IN, V28
   Shotton J, 2008, PROC CVPR IEEE, P1245
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang Y, 2019, IEEE IMAGE PROC, P1860, DOI [10.1109/icip.2019.8803154, 10.1109/ICIP.2019.8803154]
   Wang Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1833, DOI 10.1145/3343031.3350945
   Xie E., 2021, ADV NEURAL INF PROCE, V34, P12077
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zhang H, 2023, SIGNAL IMAGE VIDEO P, V17, P4171, DOI 10.1007/s11760-023-02649-1
   Zheng S, 2022, IEEE WINT C APPL COM, P52, DOI 10.1109/WACVW54805.2022.00011
   Zhu XZ, 2019, IEEE I CONF COMP VIS, P6687, DOI 10.1109/ICCV.2019.00679
NR 45
TC 0
Z9 0
U1 10
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 26
PY 2024
DI 10.1007/s00371-024-03287-5
EA FEB 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JA9S5
UT WOS:001170555600001
DA 2024-08-05
ER

PT J
AU Zhang, ZB
   Zhang, JD
AF Zhang, Zhibo
   Zhang, Jindong
TI Parallel multi-image encryption based on cross-plane DNA manipulation
   and a novel 2D chaotic system
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Multi-image encryption; DNA coding; Chaotic system; Image security
ID IMAGE; ALGORITHM; MAP; NETWORKS
AB In this paper, we propose a novel parallel multi-image encryption algorithm based on cross-plane DNA operations. Firstly, a two-dimensional chaotic system, 2D-SCIM, is constructed. Secondly, for a set of images, whether they are color images, grayscale images, or their combinations, we perform bit-plane decomposition according to the channels without limitations on quantity and arrangement order. Subsequently, the low and high bit planes are paired and encoded into DNA planes using chaotic sequences. Next, the DNA planes undergo two rounds of cross-plane diffusion and cross-plane cyclic shifting in parallel threads, where the chaotic sequence controls the encoding, direction, order, and operations of diffusion and cyclic shifting. Finally, the original image structure is used to combine the bit planes and obtain the encrypted image set. Security analyses, including key sensitivity, histograms, correlations, information entropy, differential attacks, noise attacks, and encryption speed, are conducted on the algorithm. Experimental results demonstrate that the proposed image encryption algorithm effectively withstands various attacks.
C1 [Zhang, Zhibo; Zhang, Jindong] Jilin Univ, Coll Comp Sci & Technol, Changchun 130012, Peoples R China.
   [Zhang, Jindong] Jilin Univ, Key Lab Symbol Computat & Knowledge Engn, Minist Educ, Changchun 130012, Peoples R China.
C3 Jilin University; Jilin University
RP Zhang, JD (corresponding author), Jilin Univ, Coll Comp Sci & Technol, Changchun 130012, Peoples R China.; Zhang, JD (corresponding author), Jilin Univ, Key Lab Symbol Computat & Knowledge Engn, Minist Educ, Changchun 130012, Peoples R China.
EM zhangjindong_100@163.com
RI Zhang, Zhibo/HPE-5550-2023
CR Alvarez G, 2006, INT J BIFURCAT CHAOS, V16, P2129, DOI 10.1142/S0218127406015970
   Ben Farah MA, 2020, NONLINEAR DYNAM, V99, P3041, DOI 10.1007/s11071-019-05413-8
   Ben Farah MA, 2020, OPT LASER TECHNOL, V121, DOI 10.1016/j.optlastec.2019.105777
   BRIGGS K, 1990, PHYS LETT A, V151, P27, DOI 10.1016/0375-9601(90)90841-B
   Chai XL, 2023, IEEE INTERNET THINGS, V10, P7380, DOI 10.1109/JIOT.2022.3228781
   Chai XL, 2022, NONLINEAR DYNAM, V108, P2671, DOI 10.1007/s11071-022-07328-3
   Chai XL, 2019, MULTIMED TOOLS APPL, V78, P35419, DOI 10.1007/s11042-019-08168-x
   Chen C, 2020, SIGNAL PROCESS, V168, DOI 10.1016/j.sigpro.2019.107340
   Diaconu AV, 2016, INFORM SCIENCES, V355, P314, DOI 10.1016/j.ins.2015.10.027
   Gan ZH, 2019, NEURAL COMPUT APPL, V31, P7111, DOI 10.1007/s00521-018-3541-y
   Gao L, 2013, J NEURAL ENG, V10, DOI 10.1088/1741-2560/10/3/036023
   Gao XY, 2022, J KING SAUD UNIV-COM, V34, P1535, DOI 10.1016/j.jksuci.2022.01.017
   Gao XY, 2022, NONLINEAR DYNAM, V108, P613, DOI 10.1007/s11071-021-07192-7
   Gao XY, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-94748-7
   Ghazanfaripour H, 2020, OPT LASER TECHNOL, V131, DOI 10.1016/j.optlastec.2020.106339
   GRASSBERGER P, 1983, PHYS REV A, V28, P2591, DOI 10.1103/PhysRevA.28.2591
   Guesmi R, 2021, MULTIMED TOOLS APPL, V80, P1925, DOI 10.1007/s11042-020-09672-1
   Hua ZY, 2021, NONLINEAR DYNAM, V104, P4505, DOI 10.1007/s11071-021-06472-6
   Hua ZY, 2021, INFORM SCIENCES, V546, P1063, DOI 10.1016/j.ins.2020.09.032
   Hua ZY, 2019, INFORM SCIENCES, V480, P403, DOI 10.1016/j.ins.2018.12.048
   Hua ZY, 2018, SIGNAL PROCESS, V149, P148, DOI 10.1016/j.sigpro.2018.03.010
   Hua ZY, 2016, INFORM SCIENCES, V339, P237, DOI 10.1016/j.ins.2016.01.017
   Hua ZY, 2015, INFORM SCIENCES, V297, P80, DOI 10.1016/j.ins.2014.11.018
   Huang ZJ, 2020, OPT LASER ENG, V124, DOI 10.1016/j.optlaseng.2019.105821
   Kadir A, 2014, OPTIK, V125, P1671, DOI 10.1016/j.ijleo.2013.09.040
   Kaur G, 2022, J KING SAUD UNIV-COM, V34, P5883, DOI 10.1016/j.jksuci.2021.03.007
   Kumar M, 2019, MULTIMED TOOLS APPL, V78, P28025, DOI 10.1007/s11042-019-07893-7
   Li SL, 2018, ENTROPY-SWITZ, V20, DOI 10.3390/e20060463
   Li XJ, 2021, OPT LASER TECHNOL, V140, DOI 10.1016/j.optlastec.2021.107074
   Li YB, 2015, OPT LASER ENG, V72, P18, DOI 10.1016/j.optlaseng.2015.03.027
   Liao XF, 2010, SIGNAL PROCESS, V90, P2714, DOI 10.1016/j.sigpro.2010.03.022
   Liu Y, 2020, MULTIMED TOOLS APPL, V79, P21579, DOI 10.1007/s11042-020-08880-z
   Luo YL, 2020, OPT LASER ENG, V124, DOI 10.1016/j.optlaseng.2019.105836
   Ping P, 2018, NEUROCOMPUTING, V283, P53, DOI 10.1016/j.neucom.2017.12.048
   sipi.usc.edu, US
   Song W, 2023, MATH COMPUT SIMULAT, V204, P71, DOI 10.1016/j.matcom.2022.07.029
   Wang SM, 2022, OPT LASER TECHNOL, V148, DOI 10.1016/j.optlastec.2021.107753
   Wang XY, 2023, MATHEMATICS-BASEL, V11, DOI 10.3390/math11030567
   Wang XY, 2020, OPT LASER ENG, V125, DOI 10.1016/j.optlaseng.2019.105851
   Wang XY, 2020, INFORM SCIENCES, V507, P16, DOI 10.1016/j.ins.2019.08.041
   Wu XJ, 2018, MULTIMED TOOLS APPL, V77, P12349, DOI 10.1007/s11042-017-4885-5
   Wu Y., 2011, Journal of Selected Areas in Telecommunications (JSAT), V1, P31
   Xu L, 2016, OPT LASER ENG, V78, P17, DOI 10.1016/j.optlaseng.2015.09.007
   Yang ZL, 2022, OPT LASER ENG, V152, DOI 10.1016/j.optlaseng.2022.106969
   Yildirim M, 2021, OPTIK, V237, DOI 10.1016/j.ijleo.2021.166728
   Zhu Y, 2023, MATHEMATICS-BASEL, V11, DOI 10.3390/math11030767
NR 46
TC 1
Z9 1
U1 60
U2 60
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 7
PY 2024
DI 10.1007/s00371-023-03259-1
EA FEB 2024
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HB6E8
UT WOS:001157060800002
DA 2024-08-05
ER

PT J
AU Mobini, M
   Faraji, MR
AF Mobini, Mobina
   Faraji, Mohammad Reza
TI Multi-scale gradient wavelet-based image quality assessment
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Full-reference; Perceptual image quality assessment; Wavelet function;
   Image gradient; Change of color saturation
ID SIMILARITY INDEX
AB The quality of images is always at stake due to the abundant compression, transferring, and processes done on them. Each of which brings about its own type of degradation to the images. In this paper, we present a full-reference image quality assessment method, called the multi-scale gradient wavelet (MSGW) method, to evaluate the quality of images that are under various types of distortions. The proposed MSGW method, which is based on a set of gradient filters that operate on the approximation sub-band of the output of the wavelet function on images, presents a novel approach to evaluating the quality of images. More specifically, we introduce an approach to detect the change of color saturation distortion type together with a new strategy to deal with this challenging distortion. The method is then combined with the HaarPSI method to further improve performance results. Our extensive experiments on five commonly used databases show that the proposed image quality assessment methods are superior to the state-of-the-art full-reference IQA methods, indicating they are more consistent with the subjective assessments.
C1 [Mobini, Mobina; Faraji, Mohammad Reza] Inst Adv Studies Basic Sci IASBS, Dept Comp Sci & Informat Technol, Zanjan 45137, Iran.
C3 Institute for Advanced Studies in Basic Sciences (IASBS)
RP Mobini, M (corresponding author), Inst Adv Studies Basic Sci IASBS, Dept Comp Sci & Informat Technol, Zanjan 45137, Iran.
EM m.mobini@iasbs.ac.ir
OI Mobini, Mobina/0000-0002-0847-1478
CR Abu Layek M, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11030296
   Ahmed N, 2021, MULTIMED TOOLS APPL, V80, P15677, DOI 10.1007/s11042-020-10286-w
   Amirshahi S.A., 2021, Deep learning in image quality assessment: past, present, and what lies ahead, V2021, P1
   [Anonymous], 2013, International Scholarly Research Notices
   Athar S, 2019, IEEE ACCESS, V7, P140030, DOI 10.1109/ACCESS.2019.2943319
   Bae SH, 2016, IEEE T IMAGE PROCESS, V25, P2392, DOI 10.1109/TIP.2016.2545863
   Chakraborty S., 2020, An Advanced Approach to Detect Edges of Digital Images for Image Segmentation, P90
   Chandler D.M., 2014, SPIE, V9014
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Chebbi Emna, 2014, Journal of Computer Science, V10, P353, DOI 10.3844/jcssp.2014.353.360
   Chen CM, 2023, EURASIP J IMAGE VIDE, V2023, DOI 10.1186/s13640-023-00611-2
   Gao F, 2016, SIGNAL PROCESS, V124, P210, DOI 10.1016/j.sigpro.2015.08.012
   Giron-Sierra J.M., 2016, Digital Signal Processing with Matlab Examples, Volume 2: Decomposition, Recovery, Data-Based Actions
   Ji JY, 2023, VISUAL COMPUT, V39, P443, DOI 10.1007/s00371-021-02340-x
   Larson E C, 2009, Categorical Image Quality (CSIQ) database [EB/OL]
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Le Callet P., 2005, Subjective quality assessment irccyn/ivc database
   Leisti T, 2022, FRONT PSYCHOL, V13, DOI 10.3389/fpsyg.2022.867874
   Li SN, 2011, IEEE T MULTIMEDIA, V13, P935, DOI 10.1109/TMM.2011.2152382
   Lin J.Y., 2013, Visual-saliency-enhanced image quality assessment indices, P1
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Liu TJ, 2015, IEEE IMAGE PROC, P3155, DOI 10.1109/ICIP.2015.7351385
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P3951, DOI 10.1109/TIP.2017.2708503
   Mittal Ajay, 2012, Autonomous and Intelligent Systems. Proceedings Third International Conference, AIS 2012, P250, DOI 10.1007/978-3-642-31368-4_30
   Nafchi HZ, 2016, IEEE ACCESS, V4, P5579, DOI 10.1109/ACCESS.2016.2604042
   Ponomarenko N, 2008, 2008 IEEE 10TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, VOLS 1 AND 2, P407
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Ponomarenko N, 2013, LECT NOTES COMPUT SC, V8192, P402, DOI 10.1007/978-3-319-02895-8_36
   Reisenhofer R, 2018, SIGNAL PROCESS-IMAGE, V61, P33, DOI 10.1016/j.image.2017.11.001
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shi CY, 2020, IEEE ACCESS, V8, P97310, DOI 10.1109/ACCESS.2020.2995420
   Shi ZF, 2018, SIGNAL PROCESS, V145, P99, DOI 10.1016/j.sigpro.2017.11.015
   Sonka M., 2014, Image processing, analysis and machine vision
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Temel D, 2019, SIGNAL PROCESS-IMAGE, V70, P37, DOI 10.1016/j.image.2018.09.005
   Tong YB, 2010, J IMAGING SCI TECHN, V54, DOI 10.2352/J.ImagingSci.Technol.2010.54.3.030503
   Varga D, 2022, SIGNALS-BASEL, V3, P483, DOI 10.3390/signals3030028
   Varga D, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11040559
   Wang RF, 2019, IEEE ACCESS, V7, P5285, DOI 10.1109/ACCESS.2018.2889992
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   Watson A. B., 1993, DCC '93. Data Compression Conference (Cat. No.93TH0536-3), P178, DOI 10.1109/DCC.1993.253132
   Xue WF, 2013, IEEE I CONF COMP VIS, P705, DOI 10.1109/ICCV.2013.93
   Yang JF, 2020, IEEE ACCESS, V8, P179702, DOI 10.1109/ACCESS.2020.3028282
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhan YB, 2017, IEEE T MULTIMEDIA, V19, P1837, DOI 10.1109/TMM.2017.2689923
   Zhang B, 2017, INT CONF ACOUST SPEE, P1253, DOI 10.1109/ICASSP.2017.7952357
   Zhang F, 2020, APPL SOFT COMPUT, V87, DOI 10.1016/j.asoc.2019.105987
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang L, 2010, IEEE IMAGE PROC, P321, DOI 10.1109/ICIP.2010.5649275
   Zhang XD, 2013, IEEE SIGNAL PROC LET, V20, P319, DOI 10.1109/LSP.2013.2244081
   Zhou F, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0116312
NR 56
TC 0
Z9 0
U1 7
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 5
PY 2024
DI 10.1007/s00371-024-03267-9
EA FEB 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GY9N9
UT WOS:001156357600002
DA 2024-08-05
ER

PT J
AU Murthy, JS
   Siddesh, GM
AF Murthy, Jamuna S.
   Siddesh, G. M.
TI A smart video analytical framework for sarcasm detection using novel
   adaptive fusion network and SarcasNet-99 model
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Sarcasm; TedX; Twitter; LeakyReLU; BERT
AB Sarcasm is often related to something that has created a mass confusion among the general uninformed public. It is always associated with a mockery tone or trenchancy facial expression or weird language. Existing literatures that are profound in the field of sarcasm detection mainly focused on text-based input with sarcastic comments or facial expression-based analysis, i.e., image input. But both text and image input are not sufficient to analyze the underlying sarcasm behind the scene. This kind of analysis can also be misleading sometimes as the emotional expression can change with social circumstances (i.e., audio tone) over time. Hence to address these challenges, "A Smart Video Analytical framework for Sarcasm Detection using Deep Learning" is introduced where sarcasm detection is done by considering video modality. Proposed model extracts three important features from the video, i.e., text using proposed Enhanced-BERT, image using ImageNet and audio using Librosa. After extraction, each modality is addressed individually and is finally fused using proposed adaptive early fusion approach. The final task prediction of classification is done using novel deep neural network called "SarcasNet-99" to detect sarcasm in video over distributed framework called Apache Storm. TedX and GIF Reply datasets are used for model training and testing with around 10,000 + video clips. When compared against existing state-of-the-art techniques such as AlexNet, DenseNet, SqueezeNet and ResNet, the proposed model predicted accuracy 99.005% with LeakyReLU activation function.
C1 [Murthy, Jamuna S.] Visvesvaraya Technol Univ, Ramaiah Inst Technol, Dept Comp Sci & Engn, Bengaluru, India.
   [Siddesh, G. M.] Visvesvaraya Technol Univ, Ramaiah Inst Technol, Dept Artificial Intelligence & Data Sci, Bengaluru, India.
C3 Ramaiah Institute of Technology; Visvesvaraya Technological University;
   Ramaiah Institute of Technology; Visvesvaraya Technological University
RP Murthy, JS (corresponding author), Visvesvaraya Technol Univ, Ramaiah Inst Technol, Dept Comp Sci & Engn, Bengaluru, India.
EM jamunamurthy.s@gmail.com; siddeshgm14@gmail.com
FU Ramaiah Institute of Technology (MSRIT) [Bangalore-560054]; Visvesvaraya
   Technological University [Belagavi-590018]
FX This research was supported by Ramaiah Institute of Technology (MSRIT),
   Bangalore-560054 and Visvesvaraya Technological University, Jnana
   Sangama, Belagavi-590018.
CR Ahuja R, 2022, ARAB J SCI ENG, V47, P9379, DOI 10.1007/s13369-021-06193-3
   Bedi M., 2021, IEEE T AFFECT COMPUT
   Bhat Aruna, 2022, 2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE), P1981, DOI 10.1109/ICACITE53722.2022.9823869
   Chatterjee S, 2023, SOFT COMPUT, V27, P5621, DOI 10.1007/s00500-023-08045-8
   Ding N, 2022, MULTIMED TOOLS APPL, V81, P8597, DOI 10.1007/s11042-022-12122-9
   Dutta Poulami, 2022, 2022 6th International Conference on Computing Methodologies and Communication (ICCMC), P207, DOI 10.1109/ICCMC53470.2022.9753981
   Garcia-Diaz J., 2022, P 16 INT WORKSH SEM, P742
   Godara J, 2021, BEHAV NEUROL, V2021, DOI 10.1155/2021/9731519
   Juyal Prachi, 2022, 2022 3rd International Conference on Smart Electronics and Communication (ICOSEC), P1198, DOI 10.1109/ICOSEC54921.2022.9951988
   Kamal A, 2022, COGN COMPUT, V14, P91, DOI 10.1007/s12559-021-09821-0
   Khan S, 2022, IEEE ACCESS, V10, P7881, DOI 10.1109/ACCESS.2022.3143799
   Li LLY, 2020, Arxiv, DOI arXiv:2010.06671
   Liang B, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P1767
   Liang B, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4707, DOI 10.1145/3474085.3475190
   Liu H, 2022, Arxiv, DOI arXiv:2210.03501
   Moores B., 2022, arXiv
   Muaad AY, 2022, COMPUT INTEL NEUROSC, V2022, DOI 10.1155/2022/7937667
   Rahma A., 2023, IEEE Access, V8, P24
   Ray A., 2022, arXiv, DOI [10.48550/arXiv.2206.02119, DOI 10.48550/ARXIV.2206.02119]
   Sharma DK, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11182844
   Vinoth D, 2022, J SUPERCOMPUT, V78, P10575, DOI 10.1007/s11227-022-04312-x
   Yao F., 2021, IEEE Trans. Neural Netw. Learn. Syst, V24, P31
   Zhang YZ, 2023, ACM T INTERNET TECHN, V23, DOI 10.1145/3533430
   Zhao X, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9533800
   Zuhri AT., 2022, J. Elem. School Educ, V1, P41
NR 25
TC 0
Z9 0
U1 7
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 22
PY 2024
DI 10.1007/s00371-023-03224-y
EA JAN 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GJ3D1
UT WOS:001152252500006
DA 2024-08-05
ER

PT J
AU Zhu, JX
   Shao, M
   Sun, LB
   Xia, SY
AF Zhu, Jiaxuan
   Shao, Ming
   Sun, Libo
   Xia, Siyu
TI ACL-SAR: model agnostic adversarial contrastive learning for robust
   skeleton-based action recognition
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Skeleton-based action recognition; Adversarial training; Contrastive
   learning
AB Human skeleton data have been widely explored in action recognition and the human-computer interface recently, thanks to off-the-shelf motion sensors and cameras. With the widespread usage of deep models on human skeleton data, their vulnerabilities under adversarial attacks have raised increasing security concerns. Although there are several works focusing on attack strategies, fewer efforts are put into defense against adversaries in skeleton-based action recognition, which is nontrivial. In addition, labels required in adversarial learning are another pain in adversarial training-based defense. This paper proposes a robust model agnostic adversarial contrastive learning framework for this task. First, we introduce an adversarial contrastive learning framework for skeleton-based action recognition (ACL-SAR). Second, the nature of cross-view skeleton data enables cross-view adversarial contrastive learning (CV-ACL-SAR) as a further improvement. Third, adversarial attack and defense strategies are investigated, including alternate instance-wise attacks and options in adversarial training. To validate the effectiveness of our method, we conducted extensive experiments on the NTU-RGB+D and HDM05 datasets. The results show that our defense strategies are not only robust to various adversarial attacks but can also maintain generalization.
C1 [Zhu, Jiaxuan; Xia, Siyu] Southeast Univ, Key Lab Measurement & Control CSE, Minist Educ, Nanjing, Peoples R China.
   [Shao, Ming] Univ Massachusetts, Comp & Informat Sci Dept, N Dartmouth, MA USA.
   [Sun, Libo] Southeast Univ, Sch Instrument Sci & Engn, Nanjing, Peoples R China.
C3 Southeast University - China; University of Massachusetts System;
   University Massachusetts Dartmouth; Southeast University - China
RP Xia, SY (corresponding author), Southeast Univ, Key Lab Measurement & Control CSE, Minist Educ, Nanjing, Peoples R China.
EM xsy@seu.edu.cn
FU Key R&D Program of Jiangsu Province, China [2144772]; National Science
   Foundation [BE2023010-3]; Jiangsu Province Industry Foresight and Key
   Core Technology Key Projects; Key Laboratory of Intelligent Processing
   Technology for Digital Music (Zhejiang Conservatory of Music)
   [2023DMKLB003]; Ministry of Culture and Tourism [HC-CN-20221107001]; ZTE
   Industry-University-Institute Cooperation Funds
FX This work was supported in part by the National Science Foundation under
   Grant No. 2144772, Jiangsu Province Industry Foresight and Key Core
   Technology Key Projects under Grant No. BE2023010-3, Key Laboratory of
   Intelligent Processing Technology for Digital Music (Zhejiang
   Conservatory of Music), Ministry of Culture and Tourism under Grant
   2023DMKLB003 and ZTE Industry-University-Institute Cooperation Funds
   under Grant No. HC-CN-20221107001.DAS:No datasets were generated or
   analysed during the current study.
CR [Anonymous], IEEE Trans Neural Netw Learn Syst
   Barsoum E, 2018, IEEE COMPUT SOC CONF, P1499, DOI 10.1109/CVPRW.2018.00191
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Chen TL, 2020, PROC CVPR IEEE, P696, DOI 10.1109/CVPR42600.2020.00078
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Cheng K, 2020, PROC CVPR IEEE, P180, DOI 10.1109/CVPR42600.2020.00026
   Cho K, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, THEORY AND APPLICATIONS (VISAPP 2014), VOL 2, P122
   Cohen Gilad, 2020, P IEEE CVF C COMP VI, P14453
   Diao YF, 2021, PROC CVPR IEEE, P7593, DOI 10.1109/CVPR46437.2021.00751
   Du Y, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P579, DOI 10.1109/ACPR.2015.7486569
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Feinman R, 2017, Arxiv, DOI arXiv:1703.00410
   Guan SN, 2023, Arxiv, DOI arXiv:2302.12007
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Ho CH, 2020, Arxiv, DOI arXiv:2010.12050
   Goodfellow IJ, 2015, Arxiv, DOI [arXiv:1412.6572, DOI 10.48550/ARXIV.1412.6572]
   Jiang Z., 2020, NEURIPS
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kim M., Advances in Neural Information Processing Systems, V33
   Kun Su, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9628, DOI 10.1109/CVPR42600.2020.00965
   Lea C, 2017, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2017.113
   Lee I, 2017, IEEE I CONF COMP VIS, P1012, DOI 10.1109/ICCV.2017.115
   Li B, 2018, MULTIMED TOOLS APPL, V77, P22901, DOI 10.1007/s11042-018-5642-0
   Li C., 2017, 2017 IEEE INT C MULT, P585
   Li JY, 2020, Arxiv, DOI arXiv:2006.06911
   Li JY, 2020, Arxiv, DOI arXiv:2012.01740
   Li LG, 2021, PROC CVPR IEEE, P4739, DOI 10.1109/CVPR46437.2021.00471
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Liao FZ, 2018, PROC CVPR IEEE, P1778, DOI 10.1109/CVPR.2018.00191
   Lin LL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2490, DOI 10.1145/3394171.3413548
   Liu H, 2017, Arxiv, DOI [arXiv:1705.08106, DOI 10.48550/ARXIV.1705.08106]
   Liu J, 2018, IEEE T PATTERN ANAL, V40, P3007, DOI 10.1109/TPAMI.2017.2771306
   Liu YP, 2017, Arxiv, DOI arXiv:1611.02770
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Lu JJ, 2017, IEEE I CONF COMP VIS, P446, DOI 10.1109/ICCV.2017.56
   Lu Z., 2023, P IEEE CVF INT C COM, P4597
   Madry A, 2019, Arxiv, DOI arXiv:1706.06083
   Moosavi-Dezfooli SM, 2017, PROC CVPR IEEE, P86, DOI 10.1109/CVPR.2017.17
   Muller M., 2007, DOCUMENTATION MOCAP
   Mustafa A, 2020, IEEE T IMAGE PROCESS, V29, P1711, DOI 10.1109/TIP.2019.2940533
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Papernot N, 2017, PROCEEDINGS OF THE 2017 ACM ASIA CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (ASIA CCS'17), P506, DOI 10.1145/3052973.3053009
   Rao HC, 2021, INFORM SCIENCES, V569, P90, DOI 10.1016/j.ins.2021.04.023
   Shafahi A, 2019, Arxiv, DOI [arXiv:1904.12843, 10.48550/arXiv.1904.12843]
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Shu X., IEEE Trans. Pattern Anal. Mach. Intell
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Su JW, 2019, IEEE T EVOLUT COMPUT, V23, P828, DOI 10.1109/TEVC.2019.2890858
   Sun G, 2022, IEEE INTERNET THINGS, V9, P11365, DOI 10.1109/JIOT.2021.3128646
   Szegedy C, 2014, Arxiv, DOI arXiv:1312.6199
   Tanaka N, 2022, AAAI CONF ARTIF INTE, P2335
   TramŠr F, 2020, Arxiv, DOI arXiv:1705.07204
   Tu Z., IEEE Trans. Multimedia
   Uesato J, 2019, Arxiv, DOI arXiv:1905.13725
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vemulapalli R, 2016, PROC CVPR IEEE, P4471, DOI 10.1109/CVPR.2016.484
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang H, 2021, PROC CVPR IEEE, P14651, DOI 10.1109/CVPR46437.2021.01442
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xiao Chaowei, 2018, arXiv
   Xu B., IEEE Trans. Neural Netw. Learn. Syst
   Xu BQ, 2022, IEEE T IMAGE PROCESS, V31, P3852, DOI 10.1109/TIP.2022.3175605
   Xu Binqian, 2023, arXiv
   Xu S., IEEE Trans. Multimedia
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yao H., 2021, 2021 IEEE INT C MULT, P1
   Zaremba W, 2015, Arxiv, DOI [arXiv:1409.2329, DOI 10.48550/ARXIV.1409.2329]
   Zhang, 2020, INT C MACHINE LEARNI, P11278
   Zhang HY, 2019, PR MACH LEARN RES, V97
   Zhang JX, 2022, IEEE T CIRC SYST VID, V32, P8646, DOI 10.1109/TCSVT.2022.3193574
   Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24
   Zheng NG, 2018, AAAI CONF ARTIF INTE, P2644
   Zhou HY, 2023, PROC CVPR IEEE, P10608, DOI 10.1109/CVPR52729.2023.01022
   Zhou QF, 2020, SECURITY CRYPTOLOGY, V12309, P146, DOI 10.1007/978-3-030-59013-0_8
   Zhu KJ, 2020, IEEE T MULTIMEDIA, V22, P2977, DOI 10.1109/TMM.2019.2962304
   Zhu WH, 2016, PROC INT CONF ANTI, P1, DOI 10.1109/ICASID.2016.7873885
NR 80
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 11
PY 2024
DI 10.1007/s00371-024-03548-3
EA JUL 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YN8V1
UT WOS:001269267100001
DA 2024-08-05
ER

PT J
AU Elsayed, M
   Reda, M
   Mashaly, AS
   Amein, AS
AF Elsayed, Mohamed
   Reda, Mohamed
   Mashaly, Ahmed S.
   Amein, Ahmed S.
TI LERFNet: an enlarged effective receptive field backbone network for
   enhancing visual drone detection
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Computer vision; Deep learning; Drone detection; Effective receptive
   field; YOLOv6
AB Recently, the world has witnessed a great increase in drone applications and missions. Drones must be detected quickly, effectively, and precisely when they are being handled illegally. Vision-based anti-drone systems provide an efficient performance compared to radar- and acoustic-based systems. The effectiveness of drone detection is affected by a number of issues, including the drone's small size, conflicts with other objects, and noisy backgrounds. This paper employs enlarging the effective receptive field (ERF) of feature maps generated from the YOLOv6 backbone. First, RepLKNet is used as the backbone of YOLOv6, which deploys large kernels with depth-wise convolution. Then, to get beyond RepLKNet's large inference time, a novel LERFNet is implemented. LERFNet uses dilated convolution in addition to large kernels to enlarge the ERF and overcome each other's problems. The linear spatial-channel attention module (LAM) is used to give more attention to the most informative pixels and high feature channels. LERFNet produces output feature maps with a large ERF and high shape bias to enhance the detection of various drone sizes in complex scenes. The RepLKNet and LERFNet backbones for Tiny-YOLOv6, Tiny-YOLOv6, YOLOv5s, and Tiny-YOLOv7 are compared. In comparison to the aforementioned techniques, the suggested model's results show a greater balance between accuracy and speed. LERFNet increases the MAP by 2.8%\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$2.8\%$$\end{document}, while significantly reducing the GFLOPs and parameter numbers when compared to the original backbone of YOLOv6.
C1 [Elsayed, Mohamed; Reda, Mohamed; Mashaly, Ahmed S.] Mil Tech Coll, Cairo 11765, Egypt.
   [Amein, Ahmed S.] October6 Univ, Giza 12572, Egypt.
C3 Military Technical College
RP Elsayed, M (corresponding author), Mil Tech Coll, Cairo 11765, Egypt.
EM mohamed1987.1988@gmail.com; mohamedredaismail@mtc.edu.eg;
   mashaly@mtc.edu.eg; ahmed.saleh.csis@o6u.edu.eg
RI Amein, Ahmed S./N-1356-2013; Elsayed, Mohamed Ahmed/HKN-7284-2023;
   ELSAYED, Mohamed/X-5097-2019
OI Amein, Ahmed S./0000-0002-7675-2140; Elsayed, Mohamed
   Ahmed/0000-0002-2133-9762; ELSAYED, Mohamed/0000-0002-1443-8519
FU Science, Technology &Innovation Funding Authority (STDF); Egyptian
   Knowledge Bank (EKB)
FX Open access funding provided by The Science, Technology &Innovation
   Funding Authority (STDF) in cooperation with The Egyptian Knowledge Bank
   (EKB)
CR Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Chen CR, 2019, IEEE INT CONF COMP V, P100, DOI 10.1109/ICCVW.2019.00018
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, 10.48550/arXiv.1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding XH, 2022, PROC CVPR IEEE, P11953, DOI 10.1109/CVPR52688.2022.01166
   Ding XH, 2021, PROC CVPR IEEE, P13728, DOI 10.1109/CVPR46437.2021.01352
   Elsayed M., 2021, 2021 10 INT C INT CO, P57
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Feng CJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3490, DOI 10.1109/ICCV48922.2021.00349
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Ge Z, 2021, PROC CVPR IEEE, P303, DOI 10.1109/CVPR46437.2021.00037
   Gevorgyan Z, 2022, Arxiv, DOI arXiv:2205.12740
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Jocher G., 2020, Zenodo
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Li CY, 2022, Arxiv, DOI [arXiv:2209.02976, DOI 10.48550/ARXIV.2209.02976]
   Li XF, 2020, ACTA METALL SIN-ENGL, V33, P759, DOI 10.1007/s40195-020-01039-7
   Li Yingwei, 2023, IAENG International Journal of Computer Science, P759
   Li YL, 2024, VISUAL COMPUT, V40, P4505, DOI 10.1007/s00371-023-03095-3
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu HS, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21103374
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu YC, 2021, VISUAL COMPUT, V37, P1769, DOI 10.1007/s00371-020-01937-y
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Nair V., 2010, ICML, P807
   Redmon J., 2018, CoRR
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Tan R., 2020, P IEEE CVF C COMP VI, DOI [10.1109/CVPR42600.2020.01079, DOI 10.1109/CVPR42600.2020.01079]
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang CY, 2022, Arxiv, DOI [arXiv:2207.02696, DOI 10.48550/ARXIV.2207.02696]
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Zeng S, 2024, VISUAL COMPUT, V40, P1787, DOI 10.1007/s00371-023-02886-y
   Zhang HY, 2021, PROC CVPR IEEE, P8510, DOI 10.1109/CVPR46437.2021.00841
   Zhang HY, 2018, Arxiv, DOI arXiv:1710.09412
   Zhao J, 2022, IEEE T INTELL TRANSP, V23, P25323, DOI 10.1109/TITS.2022.3177627
   Zhao ZQ, 2019, IEEE T NEUR NET LEAR, V30, P3212, DOI 10.1109/TNNLS.2018.2876865
NR 43
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 1
PY 2024
DI 10.1007/s00371-024-03527-8
EA JUL 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XC2G8
UT WOS:001259414100005
OA hybrid
DA 2024-08-05
ER

PT J
AU Zhang, XH
   Xiong, SW
   Sun, ZY
   Xiang, JW
AF Zhang, Xiaohong
   Xiong, Shengwu
   Sun, Zhaoyang
   Xiang, Jianwen
TI Semi-hard constraint augmentation of triplet learning to improve image
   corruption classification
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Data augmentation; Common corruptions; Image classification; Triplet
   learning
ID COMMON CORRUPTIONS; BENCHMARKING; ROBUSTNESS
AB When facing the challenge of image distribution shift and natural corruptions, most of data augmentation methods only consider the diversity of training image to enlarge the data quantity, the hardness quality of augmented training image has not been explored. In this paper, we propose the semi-hard constraint augmentation (ShCA) of triplet learning method to improve the image corruption classification. First, the semi-hard positive and negative samples are adaptively generated with the online instance distance ranking. The hardness of augmented image is manipulated by the amplitude disturbance term of selected information images, this forms the relative narrower margin distance between positive and negative images without model collapse. Second, the semi-hard triplet learning can promote the subtle discriminative ability of reserved phase features in the smaller optimization space, which fine structure features can make the model corruption robustness. Compared with other state-of-art methods on the four public clean and corrupted image classification datasets, our proposed ShCA method gets the better performance of image corruption classification, especially for the corrupted images with noise, blur, weather and digital categories. The codes are available at https://github.com/zhangxhxh/ShCA.
C1 [Zhang, Xiaohong; Xiong, Shengwu; Sun, Zhaoyang; Xiang, Jianwen] Wuhan Univ Technol, Sch Comp Sci & Artificial Intelligence, Wuhan 430070, Peoples R China.
C3 Wuhan University of Technology
RP Xiang, JW (corresponding author), Wuhan Univ Technol, Sch Comp Sci & Artificial Intelligence, Wuhan 430070, Peoples R China.
EM jwxiang@whut.edu.cn
RI Zhang, Xiaohong/A-3060-2015
CR Benz P, 2021, IEEE WINT CONF APPL, P494, DOI 10.1109/WACV48630.2021.00054
   Chen GY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P448, DOI 10.1109/ICCV48922.2021.00051
   Chen J., 2023, Association for the Advancement of Artificial Intelligence, P7033
   Chen J, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2050
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chrabaszcz Patryk, 2017, A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets
   Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   DeVries T, 2017, Arxiv, DOI arXiv:1708.04552
   Dong YP, 2023, PROC CVPR IEEE, P1022, DOI 10.1109/CVPR52729.2023.00105
   Dwibedi D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9568, DOI 10.1109/ICCV48922.2021.00945
   Geirhos R., 2019, INT C LEARN REPR ICL
   Gong CY, 2021, PROC CVPR IEEE, P1055, DOI 10.1109/CVPR46437.2021.00111
   Lopes RG, 2019, Arxiv, DOI arXiv:1906.02611
   Guo Y, 2023, PROC CVPR IEEE, P4108, DOI 10.1109/CVPR52729.2023.00400
   Hendrycks D, 2018, P INT C LEARN REPR
   Hendrycks D, 2022, PROC CVPR IEEE, P16762, DOI 10.1109/CVPR52688.2022.01628
   Hendrycks Dan, 2020, 8 INT C LEARN REPR I
   Hong Xuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P126, DOI 10.1007/978-3-030-58568-6_8
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kalantidis Yannis, 2020, ADV NEURAL INFORM PR
   Kar OF, 2022, PROC CVPR IEEE, P18941, DOI 10.1109/CVPR52688.2022.01839
   Krizhevsky A., 2012, Learning multiple layers of features from tiny images, DOI DOI 10.1145/3079856.3080246
   Lee Hae Beom, 2020, INT C LEARN REPR
   Lee JH, 2020, IEEE COMPUT SOC CONF, P3264, DOI 10.1109/CVPRW50498.2020.00386
   Lee S, 2023, PROC CVPR IEEE, P11776, DOI 10.1109/CVPR52729.2023.01133
   Li PD, 2023, IEEE T CIRC SYST VID, V33, P1952, DOI 10.1109/TCSVT.2022.3213680
   Li X., 2023, Knowl. Based Syst
   Lim J, 2022, PROC CVPR IEEE, P212, DOI 10.1109/CVPR52688.2022.00031
   Liu C, 2024, Arxiv, DOI arXiv:2302.14302
   Liu X., 2022, IEEE INT C MULT EXP, P1
   Modas A, 2022, LECT NOTES COMPUT SC, V13685, P623, DOI 10.1007/978-3-031-19806-9_36
   Pan Z., 2022, Comput. Animat. Virtual Worlds, V33
   Park C., 2022, Adv. Neural Inform. Process. Syst.
   Pei YT, 2018, LECT NOTES COMPUT SC, V11214, P697, DOI 10.1007/978-3-030-01249-6_42
   Qian Q, 2019, IEEE I CONF COMP VIS, P6459, DOI 10.1109/ICCV.2019.00655
   Ren XH, 2022, PATTERN RECOGN, V131, DOI 10.1016/j.patcog.2022.108864
   Rommel C., 2022, Advances in Neural Information Processing Systems
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shi D., 2020, P INT C MACH LEARN, P8828
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Sohn K, 2016, ADV NEUR IN, V29
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Sun J, 2022, LECT NOTES COMPUT SC, V13664, P654, DOI 10.1007/978-3-031-19772-7_38
   Sun MJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7506, DOI 10.1109/ICCV48922.2021.00743
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang H., 2022, Adv. Neural. Inf. Process. Syst, V35, P13456
   Wang SX, 2023, Arxiv, DOI [arXiv:2305.06024, DOI arXiv:2305.06024.v2]
   Wang X, 2023, IEEE T PATTERN ANAL, V45, P5549, DOI 10.1109/TPAMI.2022.3203630
   Wang X, 2019, PROC CVPR IEEE, P5017, DOI 10.1109/CVPR.2019.00516
   Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309
   Xu LL, 2022, Arxiv, DOI arXiv:2206.00212
   Xu M, 2023, PATTERN RECOGN, V137, DOI 10.1016/j.patcog.2023.109347
   Xu QW, 2023, PATTERN RECOGN, V139, DOI 10.1016/j.patcog.2023.109474
   Xu QW, 2021, PROC CVPR IEEE, P14378, DOI 10.1109/CVPR46437.2021.01415
   Yang Long, 2022, Advances in Neural Information Processing Systems (NeurIPS)
   Yu H, 2021, IEEE T IMAGE PROCESS, V30, P8955, DOI 10.1109/TIP.2021.3121150
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang H., 2018, INT C LEARNING REPRE
   Zhang YL, 2022, LECT NOTES COMPUT SC, V13432, P242, DOI 10.1007/978-3-031-16434-7_24
   Zhao BC, 2022, LECT NOTES COMPUT SC, V13668, P163, DOI 10.1007/978-3-031-20074-8_10
   Zhao YR, 2018, LECT NOTES COMPUT SC, V11213, P508, DOI 10.1007/978-3-030-01240-3_31
   Zheng WZ, 2021, IEEE T PATTERN ANAL, V43, P3214, DOI 10.1109/TPAMI.2020.2980231
   zhou man, 2022, ADV NEURAL INFORM PR, V35, P22995
NR 66
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 18
PY 2024
DI 10.1007/s00371-024-03514-z
EA JUN 2024
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UQ5T1
UT WOS:001249543600003
DA 2024-08-05
ER

PT J
AU Li, XF
   Quan, HY
AF Li, Xiafan
   Quan, Hongyan
TI MVPCL: multi-view prototype consistency learning for semi-supervised
   medical image segmentation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Multi-view; Consistency regularization; Prototype fusion;
   Semi-supervised segmentation; Cross-view attention
ID TRANSFORMER; NET
AB Semi-supervised learning (SSL) methods show their powerful performance in dealing with the issue of data shortage in the field of medical image segmentation. However, most existing SSL methods neither fully utilize the multi-view information of medical images nor generate high-quality pseudo-labels to expand the training set. In this study, we propose a multi-view prototype consistency learning (MVPCL) framework for semi-supervised medical image segmentation. Specifically, the multi-view encoder and cross-view attention mechanism are employed to obtain high-level latent features of the original volumes, and the consistency constraints on the predictions of multi-view inputs enhance the performance of the networks. Moreover, the fused prototype representations are learned from the entire dataset by adopting an entropy-based uncertainty map. Eventually, the consistency constraints on prototype-based predictions result in a more representative prototype for each class, thereby optimizing the embedding space distribution. Substantial experimental results on three public benchmark datasets, including LiTS, LA, and ACDC, demonstrate the efficacy of the proposed method compared to the existing approaches. The source code is available at https://github.com/lixiafan/MVPCL-master.
C1 [Li, Xiafan; Quan, Hongyan] East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China.
C3 East China Normal University
RP Quan, HY (corresponding author), East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China.
EM 51255901065@stu.ecnu.edu.cn; hyquan@cs.ecnu.edu.cn
CR Al-Jebrni AH, 2023, VISUAL COMPUT, V39, P3675, DOI 10.1007/s00371-023-02984-x
   Bai YH, 2023, PROC CVPR IEEE, P11514, DOI 10.1109/CVPR52729.2023.01108
   Bernard O, 2018, IEEE T MED IMAGING, V37, P2514, DOI 10.1109/TMI.2018.2837502
   Bilic P, 2023, MED IMAGE ANAL, V84, DOI 10.1016/j.media.2022.102680
   Chaitanya K, 2023, MED IMAGE ANAL, V87, DOI 10.1016/j.media.2023.102792
   Cheema MN, 2021, IEEE T IND INFORM, V17, P7991, DOI 10.1109/TII.2021.3064369
   Chen J., 2021, TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation
   Chen JK, 2023, IEEE T MED IMAGING, V42, P594, DOI 10.1109/TMI.2022.3213372
   Dong N., 2018, BMVC, V3, P1
   Gao SB, 2023, LECT NOTES COMPUT SC, V14220, P98, DOI 10.1007/978-3-031-43907-0_10
   Gao YH, 2021, LECT NOTES COMPUT SC, V12903, P61, DOI 10.1007/978-3-030-87199-4_6
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   Ji YF, 2021, LECT NOTES COMPUT SC, V12901, P326, DOI 10.1007/978-3-030-87193-2_31
   Li G, 2021, PROC CVPR IEEE, P8330, DOI 10.1109/CVPR46437.2021.00823
   Li JJ, 2024, IEEE T MED IMAGING, V43, P64, DOI 10.1109/TMI.2023.3289859
   Li YY, 2023, VISUAL COMPUT, V39, P2223, DOI 10.1007/s00371-021-02328-7
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Lu WJ, 2023, LECT NOTES COMPUT SC, V14223, P662, DOI 10.1007/978-3-031-43901-8_63
   Luo XD, 2021, AAAI CONF ARTIF INTE, V35, P8801
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Nazir A, 2020, IEEE T IMAGE PROCESS, V29, P7192, DOI 10.1109/TIP.2020.2999854
   Oliver A, 2018, ADV NEUR IN, V31
   Qian LD, 2023, VISUAL COMPUT, V39, P5953, DOI 10.1007/s00371-022-02705-w
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Samuli L., 2017, INT C LEARN REPR ICL, V4, P6
   Shuailin Li, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12261), P552, DOI 10.1007/978-3-030-59710-8_54
   Tarvainen A., 2017, P 31 ANN C NEUR INF, P1195, DOI DOI 10.1137/0330046
   Wang D, 2021, VISUAL COMPUT, V37, P1101, DOI 10.1007/s00371-020-01855-z
   Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929
   Wang YQ, 2023, LECT NOTES COMPUT SC, V14222, P486, DOI 10.1007/978-3-031-43898-1_47
   Wang YC, 2022, PROC CVPR IEEE, P4238, DOI 10.1109/CVPR52688.2022.00421
   Wu LS, 2023, IEEE T PATTERN ANAL, V45, P8827, DOI 10.1109/TPAMI.2022.3233584
   Wu YC, 2021, LECT NOTES COMPUT SC, V12902, P297, DOI 10.1007/978-3-030-87196-3_28
   Xiong ZH, 2021, MED IMAGE ANAL, V67, DOI 10.1016/j.media.2020.101832
   Xu Z, 2022, IEEE J BIOMED HEALTH, V26, P3174, DOI 10.1109/JBHI.2022.3162043
   Yizhe Zhang, 2017, Medical Image Computing and Computer Assisted Intervention  MICCAI 2017. 20th International Conference. Proceedings: LNCS 10435, P408, DOI 10.1007/978-3-319-66179-7_47
   Yu LQ, 2019, LECT NOTES COMPUT SC, V11765, P605, DOI 10.1007/978-3-030-32245-8_67
NR 40
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 29
PY 2024
DI 10.1007/s00371-024-03497-x
EA MAY 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL2G6
UT WOS:001234536800001
DA 2024-08-05
ER

PT J
AU Du, PS
   Wang, X
   Zheng, Q
   Wang, X
   Li, WG
   Xu, X
AF Du, Pengshu
   Wang, Xiao
   Zheng, Qi
   Wang, Xi
   Li, Weigang
   Xu, Xin
TI Glare countering and exploiting via dual stream network for nighttime
   vehicle detection
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Nighttime vehicle detection; Glare; FLL; FGF; YOLOv5
AB Nighttime vehicle detection is a challenging problem due to the interferences from night-specific low light and glare. The low-light enhancement and attention mechanism have been applied to address low-light interference, while glare interference is usually addressed by introducing additional infrared devices. Infrared devices suppress the glare while suppressing key vehicle attributes, significantly impacting vehicle detection. In this paper, we proposed a novel dual-branch network for processing only RGB images with glare in nighttime vehicle detection. On the one hand, we eliminate the effect of glare as much as possible with the foreground light leveling module (FLL). On the other hand, we also take notice of the glare itself, with the foreground glare filtering module (FGF), for more efficient and accurate vehicle detection. Our algorithm uses only RGB images, does not introduce additional information, and achieves comparable performance to methods based on infrared images. Experimental results show that our algorithm improves the average detection accuracy on the VD-NUS-G dataset by 4.7% compared to the YOLOv5 baseline.
C1 [Du, Pengshu; Li, Weigang] Wuhan Univ Sci & Technol, Hubei Prov Key Lab Intelligent Informat Proc & Rea, Wuhan, Peoples R China.
   [Wang, Xiao; Xu, Xin] Wuhan Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan, Peoples R China.
   [Zheng, Qi] Xidian Univ, Hangzhou 311231, Peoples R China.
   [Wang, Xi] Hubei Huazhong Elect Power Technol Dev Co Ltd, Wuhan 430070, Peoples R China.
C3 Wuhan University of Science & Technology; Wuhan University of Science &
   Technology; Xidian University
RP Wang, X (corresponding author), Wuhan Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan, Peoples R China.
EM wangxiao2021@wust.edu.cn
FU National Natural Science Foundation of China [62302351, 62376201];
   National Nature Science Foundation of China [2022CFB018]; Nature Science
   Foundation of Hubei Province [ZNXX2022001]; Hubei Province Key
   Laboratory of Intelligent Information Processing and Real-time
   Industrial System (Wuhan University of Science and Technology)
FX This work is supported by National Nature Science Foundation of China
   (62302351, 62376201), Nature Science Foundation of Hubei Province
   (2022CFB018), and Hubei Province Key Laboratory of Intelligent
   Information Processing and Real-time Industrial System (Wuhan University
   of Science and Technology) (ZNXX2022001).
CR Alkentar B., 2021, J. Eng., V27, P19, DOI [DOI 10.31026/J.ENG.2021.08.02, 10.31026/j.eng.2021.08.02.23J, DOI 10.31026/J.ENG.2021.08.02.23J]
   Arunmozhi A, 2018, INT CONF ELECTRO INF, P362, DOI 10.1109/EIT.2018.8500159
   Bhargava S, 2022, Arxiv, DOI arXiv:2209.09808
   Cao JW, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20164646
   Chen XW, 2020, COMPLEXITY, V2020, DOI 10.1155/2020/8842297
   Chen ZC, 2022, MEASUREMENT, V201, DOI 10.1016/j.measurement.2022.111655
   Chen ZH, 2023, IEEE T PATTERN ANAL, V45, P13489, DOI 10.1109/TPAMI.2023.3293885
   Deng ZK, 2023, COMPUT VIS MEDIA, V9, P3, DOI 10.1007/s41095-022-0275-7
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gao Lei, 2008, Journal of Beijing University of Aeronautics and Astronautics, V34, P1113
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Ghosh R, 2021, MULTIMED TOOLS APPL, V80, P25985, DOI 10.1007/s11042-021-10954-5
   Guan LM, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9030451
   Hazoor A, 2021, TRANSPORT RES REC, V2675, P1573, DOI 10.1177/03611981211008885
   Hou J., 2024, Advances in Neural Information Processing Systems, V36
   Huang S, 2021, Journal of Physics: Conference Series, V1883
   Huang YT, 2021, 2021 IEEE 6TH INTERNATIONAL CONFERENCE ON SMART CLOUD (SMARTCLOUD 2021), P1, DOI 10.1109/SmartCloud52277.2021.00008
   Ibarra-Arenado M, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17050975
   Jiang ZC, 2020, Arxiv, DOI arXiv:2011.04244
   Jin YY, 2022, LECT NOTES COMPUT SC, V13697, P404, DOI 10.1007/978-3-031-19836-6_23
   Juric D, 2014, INT CONF CONNECT VEH, P655, DOI 10.1109/ICCVE.2014.7297630
   Kennerley M, 2023, PROC CVPR IEEE, P11484, DOI 10.1109/CVPR52729.2023.01105
   Kuang HL, 2017, IEEE T INTELL TRANSP, V18, P927, DOI 10.1109/TITS.2016.2598192
   Lim T.-Y., 2019, MACH LEARN AUT DRIV, V2
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Mahto P, 2020, Transp. Modes J, V11
   Miao Y, 2020, CHIN AUTOM CONGR, P6617, DOI 10.1109/CAC51589.2020.9326819
   Mu KN, 2016, OPTIK, V127, P4794, DOI 10.1016/j.ijleo.2016.01.017
   Neumann L, 2019, LECT NOTES COMPUT SC, V11361, P691, DOI 10.1007/978-3-030-20887-5_43
   O'Malley R, 2010, IEEE T INTELL TRANSP, V11, P453, DOI 10.1109/TITS.2010.2045375
   Parvin S., 2021, J. Comput. Commun, V9, P29, DOI [10.4236/jcc.2021.93003, DOI 10.4236/JCC.2021.93003]
   Qian K, 2021, PROC CVPR IEEE, P444, DOI 10.1109/CVPR46437.2021.00051
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rouf M.A., 2023, Embedded Selforganising Syst., V10, P4
   Russell A., 2012, 2012 International Symposium on Communications and Information Technologies (ISCIT), P620, DOI 10.1109/ISCIT.2012.6380975
   Satzoda RK, 2019, IEEE T INTELL TRANSP, V20, P4297, DOI 10.1109/TITS.2016.2614545
   Schamm T, 2010, IEEE INT VEH SYM, P418, DOI 10.1109/IVS.2010.5548013
   Shao H. X., 2012, ADV MAT RES, V546, P721, DOI DOI 10.4028/WWW.SCIENTIFIC.NET/AMR.546-547.721
   Shazwani AN, 2016, 2016 6TH IEEE INTERNATIONAL CONFERENCE ON CONTROL SYSTEM, COMPUTING AND ENGINEERING (ICCSCE), P407, DOI 10.1109/ICCSCE.2016.7893608
   Sinha D, 2019, 2019 IEEE 10TH ANNUAL UBIQUITOUS COMPUTING, ELECTRONICS & MOBILE COMMUNICATION CONFERENCE (UEMCON), P280, DOI 10.1109/uemcon47517.2019.8993089
   Sun YM, 2022, IEEE T CIRC SYST VID, V32, P6700, DOI 10.1109/TCSVT.2022.3168279
   Tan R., 2020, P IEEE CVF C COMP VI, DOI [10.1109/CVPR42600.2020.01079, DOI 10.1109/CVPR42600.2020.01079]
   Tao HJ, 2020, J REAL-TIME IMAGE PR, V17, P745, DOI 10.1007/s11554-019-00856-z
   Teoh SS, 2012, MACH VISION APPL, V23, P831, DOI 10.1007/s00138-011-0355-7
   Wang H, 2019, J SENSORS, V2019, DOI 10.1155/2019/8473980
   Wang X, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15174310
   Wu Youyu, 2023, 2023 3rd International Conference on Neural Networks, Information and Communication Engineering (NNICE), P266, DOI 10.1109/NNICE58320.2023.10105722
   Yan G, 2016, OPTIK, V127, P7941, DOI 10.1016/j.ijleo.2016.05.092
   Yang Xian-feng, 2014, Computer Engineering, V40, P210, DOI 10.3969/j.issn.1000-3428.2014.09.042
   Zhang Ting, 2022, J Voice, DOI 10.1016/j.jvoice.2022.08.029
   Zhao Wangyu, 2019, Geomatics and Information Science of Wuhan University, V44, P1832, DOI 10.13203/j.whugis20180146
NR 53
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 21
PY 2024
DI 10.1007/s00371-024-03433-z
EA MAY 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RO3K3
UT WOS:001228562800002
DA 2024-08-05
ER

PT J
AU Zhou, Y
   Chen, X
   Li, TY
   Lin, SQ
   Sheng, B
   Liu, RH
   Dai, RP
AF Zhou, Yan
   Chen, Xiang
   Li, Tingyao
   Lin, Shiqun
   Sheng, Bin
   Liu, Ruhan
   Dai, Rongping
TI GAMNet: a gated attention mechanism network for grading myopic traction
   maculopathy in OCT images
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Optical coherence tomography (OCT); Myopic traction maculopathy (MTM);
   Deep learning; Vision transformer; Gated attention mechanism
ID OPTICAL COHERENCE TOMOGRAPHY
AB Myopic traction maculopathy (MTM) is a retinal disease caused by tractional forces on the macula, serving as a major contributor to irreversible visual impairment in highly myopic eyes. Given the clinical diversity of MTM, its classification is crucial for providing customized management and care decisions. Current methods primarily fall into two categories: Traditional approaches use machine learning for feature extraction, while deep learning methods employ data-driven training of classification models. However, due to the spatial similarity of optical coherence tomography (OCT) images, data imbalance, and coarse-grained partitioning, the MTM classification task remains challenging. Thus, we propose a gated attention mechanism network (GAMNet) to automatically grade MTM using OCT images. GAMNet adopts a ResNet-34 backbone with a selective fusion of temporal and channel attention modules to construct the basic architecture. For improved feature extraction ability, GAMNet combines a gated attention mechanism following a dual attention module. To address the robustness issues associated with data imbalance, our model utilizes a combination of data augmentation techniques and a weighted loss function. Our model was trained and evaluated by a dataset containing 26,616 OCT images collected from 2499 eyes. Comparing with six baseline models, experimental results indicate that the GAMNet successfully achieves the intended objectives, with an accuracy of 93.3%, an F1 score of 90.0%, and a recall rate of 89.7%, outperforming existing MTM classification methods.
C1 [Chen, Xiang; Li, Tingyao; Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Zhou, Yan; Lin, Shiqun; Dai, Rongping] Chinese Acad Med Sci & Peking Union Med Coll, Peking Union Med Coll Hosp, Dept Ophthalmol, Beijing, Peoples R China.
   [Zhou, Yan; Lin, Shiqun; Dai, Rongping] Chinese Acad Med Sci, Beijing, Peoples R China.
   [Zhou, Yan; Lin, Shiqun; Dai, Rongping] Chinese Acad Med Sci, Key Lab Ocular Fundus Dis, Beijing, Peoples R China.
   [Liu, Ruhan] Cent South Univ, Furong Lab, Changsha, Hunan, Peoples R China.
C3 Shanghai Jiao Tong University; Chinese Academy of Medical Sciences -
   Peking Union Medical College; Peking Union Medical College Hospital;
   Peking Union Medical College; Chinese Academy of Medical Sciences -
   Peking Union Medical College; Chinese Academy of Medical Sciences -
   Peking Union Medical College; Central South University; Furong
   Laboratory
RP Dai, RP (corresponding author), Chinese Acad Med Sci & Peking Union Med Coll, Peking Union Med Coll Hosp, Dept Ophthalmol, Beijing, Peoples R China.; Dai, RP (corresponding author), Chinese Acad Med Sci, Beijing, Peoples R China.; Dai, RP (corresponding author), Chinese Acad Med Sci, Key Lab Ocular Fundus Dis, Beijing, Peoples R China.; Liu, RH (corresponding author), Cent South Univ, Furong Lab, Changsha, Hunan, Peoples R China.
EM zhouyan416@163.com; 1961764891@qq.com; shiqun-lin@163.com;
   shengbin@cs.sjtu.edu.cn; 223101@csu.edu.cn; derricka@sina.com
FU Non-profit Central Research Institute Fund of Chinese Academy of Medical
   Sciences
FX No Statement Available
CR Abbood SH, 2022, IEEE ACCESS, V10, P73079, DOI 10.1109/ACCESS.2022.3189374
   Abusalim Samah, 2022, 2022 International Conference on Digital Transformation and Intelligence (ICDI), P117, DOI 10.1109/ICDI57181.2022.10007158
   [Anonymous], 2014, Computer Vision and Pattern Recognition
   Bak C, 2018, IEEE T MULTIMEDIA, V20, P1688, DOI 10.1109/TMM.2017.2777665
   Benhamou N, 2002, AM J OPHTHALMOL, V133, P794, DOI 10.1016/S0002-9394(02)01394-6
   Binczyk F, 2021, TRANSL LUNG CANCER R, V10, P1186, DOI 10.21037/tlcr-20-708
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Cheng YH, 2017, PROC CVPR IEEE, P1475, DOI 10.1109/CVPR.2017.161
   Chu X, 2017, PROC CVPR IEEE, P5669, DOI 10.1109/CVPR.2017.601
   Dolgin E, 2015, NATURE, V519, P276, DOI 10.1038/519276a
   Dosovitskiy A., 2020, Comput. Sci, V6, P66
   Gulshan V, 2016, JAMA-J AM MED ASSOC, V316, P2402, DOI 10.1001/jama.2016.17216
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang XR, 2023, MED PHYS, V50, P5398, DOI 10.1002/mp.16623
   Li YZ, 2023, Cloud Compu and Big, P467, DOI 10.1109/ICCCBDA56900.2023.10154755
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu YC, 2021, Arxiv, DOI arXiv:2111.12419
   Mnih V, 2014, ADV NEUR IN, V27
   Ng DSC, 2016, EYE, V30, P901, DOI 10.1038/eye.2016.47
   Panozzo G, 2004, ARCH OPHTHALMOL-CHIC, V122, P1455, DOI 10.1001/archopht.122.10.1455
   Rahman SA, 2021, BRIEF BIOINFORM, V22, P1767, DOI 10.1093/bib/bbaa021
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren Yi, 2023, 2023 8th International Conference on Intelligent Computing and Signal Processing (ICSP), P1174, DOI 10.1109/ICSP58490.2023.10248857
   Sayanagi K, 2010, RETINA-J RET VIT DIS, V30, P623, DOI 10.1097/IAE.0b013e3181ca4e7c
   Schmidt-Erfurth U, 2018, PROG RETIN EYE RES, V67, P1, DOI 10.1016/j.preteyeres.2018.07.004
   Shimada N, 2013, AM J OPHTHALMOL, V156, P948, DOI 10.1016/j.ajo.2013.06.031
   Simonyan K., 2014, C TRACK P
   Sogawa T, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0227240
   Wang SW, 2021, PHOTODIAGN PHOTODYN, V33, DOI 10.1016/j.pdpdt.2021.102208
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu ZF, 2019, PATTERN RECOGN, V90, P119, DOI 10.1016/j.patcog.2019.01.006
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yana Luo, 2021, 2021 International Conference on Computer Network, Electronic and Automation (ICCNEA), P121, DOI 10.1109/ICCNEA53019.2021.00036
   Yang N, 2022, 2022 3 INT C INF SCI, P271, DOI 10.1109/ISPDS56360.2022.9874134
   Zhang Xinyi., 2018, P BMVC
   Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320
   Zou Menghan, 2023, 2023 8th International Conference on Intelligent Computing and Signal Processing (ICSP), P1011, DOI 10.1109/ICSP58490.2023.10248544
NR 37
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 8
PY 2024
DI 10.1007/s00371-024-03386-3
EA MAY 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QD6V0
UT WOS:001218988200001
DA 2024-08-05
ER

PT J
AU Guan, YN
   Liao, SJ
   Yang, WY
AF Guan, Ya'nan
   Liao, Shujiao
   Yang, Wenyuan
TI AParC-DETR: Accelerate DETR training by introducing Adaptive
   Position-aware Circular Convolution
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Computer vision; Object detection; DEtection TRansformer; Convolution;
   Granular computing
AB Detection Transformer (DETR) is a more concise detection paradigm that eliminates artificial designs and interventions. However, it is difficult for previous DETR models to obtain local sensitive locations when processing images, which leads to slow convergence during training. In this article, we introduce the Adaptive Position-Aware Circular Convolution DEtection TRansformer (AParC-DETR), which has a global receptive field and can perceive sensitive local features, improving the model's adaptability while limiting the increase in computation. Groups of particles are sampled from the 3D space to encode the content vector. The content vector and positional vector are updated through Multi-Head Self-Attention with Boundary Information. The channel and spatial features are mixed through Adaptive Position-Aware Circular Convolution Global Mixing to obtain the mixed feature matrix. An Adaptive Gating Channel Mixing (AGCM) mechanism with a gate control branch is incorporated to improve adaptability while limiting computational costs. Position-Aware Spatial Mixing (PASM) extends the receptive field to the global level with lower computational cost, using instance kernels and position embedding strategies. The category and bounding box decouple the output detection results to avoid the interference of mutual coupling. In object detection on MS COCO, AParC-DETR achieves an AP of 44.2 after 12 epochs of training, which improves to 45.8 after 36 epochs. Moreover, ablation studies are performed on Cityscapes dataset to analyze AParC-DETR's computational efficiency. With 119 G FLOPs and 136 M reference counts, AParC-DETR attains 23 FPS. The codes are upload on https://github.com/Guanyn/AParC-DETR.git.
C1 [Guan, Ya'nan; Liao, Shujiao; Yang, Wenyuan] Minnan Normal Univ, Fujian Key Lab Granular Comp & Applicat, Zhangzhou 363000, Fujian, Peoples R China.
   [Guan, Ya'nan; Liao, Shujiao; Yang, Wenyuan] Minnan Normal Univ, Sch Math & Stat, Zhangzhou 363000, Fujian, Peoples R China.
C3 MinNan Normal University; MinNan Normal University
RP Liao, SJ (corresponding author), Minnan Normal Univ, Fujian Key Lab Granular Comp & Applicat, Zhangzhou 363000, Fujian, Peoples R China.; Liao, SJ (corresponding author), Minnan Normal Univ, Sch Math & Stat, Zhangzhou 363000, Fujian, Peoples R China.
EM guanyn12@163.com; sjliao2011@163.com; yangwy@xmu.edu.cnm
FU National Natural Science Foundation of China [12101289, 11871259,
   62076221]; National Natural Science Foundation of China [2022J01891];
   Natural Science Foundation of Fujian Province; Institute of
   Meteorological Big Data-Digital Fujian; Fujian Key Laboratory of Data
   Science and Statistics (Minnan Normal University), China
FX This study is supported by various sources, including the National
   Natural Science Foundation of China under Grant Nos. 12101289, 11871259,
   and 62076221, the Natural Science Foundation of Fujian Province under
   Grant No. 2022J01891, the Institute of Meteorological Big Data-Digital
   Fujian, and the Fujian Key Laboratory of Data Science and Statistics
   (Minnan Normal University), China.
CR Beal J., 2020, arXiv, DOI DOI 10.48550/ARXIV.2012.09958
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen FY, 2023, PROC CVPR IEEE, P23756, DOI 10.1109/CVPR52729.2023.02275
   Chen H., 2024, PeLK: Parameter-efficient Large Kernel ConvNets with Peripheral Convolution
   Chen JR, 2023, PROC CVPR IEEE, P12021, DOI 10.1109/CVPR52729.2023.01157
   Chen SF, 2023, IEEE T PATTERN ANAL, V45, P14284, DOI 10.1109/TPAMI.2023.3303397
   Chen X., 2022, arXiv
   Cheng BW, 2022, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR52688.2022.00135
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai XY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2968, DOI 10.1109/ICCV48922.2021.00298
   Ding X., 2021, arXiv
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Gao P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3601, DOI 10.1109/ICCV48922.2021.00360
   Gao ZT, 2022, PROC CVPR IEEE, P5354, DOI 10.1109/CVPR52688.2022.00529
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Guo JY, 2022, PROC CVPR IEEE, P816, DOI 10.1109/CVPR52688.2022.00090
   Han K, 2021, ADV NEUR IN
   Han Qiu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P549, DOI 10.1007/978-3-030-58452-8_32
   Hassan H, 2022, COMPUT BIOL MED, V141, DOI 10.1016/j.compbiomed.2021.105123
   Hongkai Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P260, DOI 10.1007/978-3-030-58555-6_16
   Hou Q., 2022, arXiv
   Howard S., 2024, CoordGate: Efficiently Computing Spatially-Varying Convolutions in Convolutional Neural Networks
   Ige T, 2023, LECT NOTE NETW SYST, V597, P656, DOI 10.1007/978-3-031-21438-7_54
   Jocher G., YOLO by Ultralytics
   Kollias A., 2022, COMPUTER VISION ECCV, P677, DOI DOI 10.1007/978-3-031-25082-8_46
   Li B, 2023, VISUAL COMPUT, V39, P2671, DOI 10.1007/s00371-022-02485-3
   Li F, 2023, PROC CVPR IEEE, P3041, DOI 10.1109/CVPR52729.2023.00297
   Li F, 2022, PROC CVPR IEEE, P13609, DOI 10.1109/CVPR52688.2022.01325
   Liang XC, 2023, VISUAL COMPUT, V39, P2277, DOI 10.1007/s00371-022-02413-5
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Hong, 2021, ADV NEURAL INFORM PR, V34
   Liu JH, 2022, LECT NOTES COMPUT SC, V13681, P33, DOI 10.1007/978-3-031-19803-8_3
   Liu S., 2022, arXiv
   Liu Z, 2022, PROC CVPR IEEE, P11999, DOI 10.1109/CVPR52688.2022.01170
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Melas-Kyriazi L, 2021, Arxiv, DOI arXiv:2105.02723
   Meng DP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3631, DOI 10.1109/ICCV48922.2021.00363
   Pan XR, 2022, PROC CVPR IEEE, P805, DOI 10.1109/CVPR52688.2022.00089
   Redmon J., 2018, CoRR
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Roh B, 2022, Arxiv, DOI arXiv:2111.14330
   Sun PZ, 2021, PROC CVPR IEEE, P14449, DOI 10.1109/CVPR46437.2021.01422
   Sun ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3591, DOI 10.1109/ICCV48922.2021.00359
   Tang W, 2023, IEEE T MULTIMEDIA, V25, P5413, DOI 10.1109/TMM.2022.3192661
   Tang W, 2022, IEEE T IMAGE PROCESS, V31, P5134, DOI 10.1109/TIP.2022.3193288
   Tolstikhin I, 2021, ADV NEUR IN, V34
   Tu ZZ, 2022, PROC CVPR IEEE, P5759, DOI 10.1109/CVPR52688.2022.00568
   Wang T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4641, DOI 10.1109/ICCV48922.2021.00462
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang YM, 2022, AAAI CONF ARTIF INTE, P2567
   Wu Qihang, 2023, 2023 IEEE 3rd International Conference on Power, Electronics and Computer Applications (ICPECA), P789, DOI 10.1109/ICPECA56706.2023.10076095
   Xiong Y., 2024, Efficient deformable convnets: Rethinking dynamic and sparse operator for vision applications
   Yang T, 2023, Arxiv, DOI arXiv:2210.04020
   Yao DZ, 2024, VISUAL COMPUT, V40, P2589, DOI 10.1007/s00371-023-02939-2
   Yao ZY, 2021, Arxiv, DOI arXiv:2104.01318
   Zhang GJ, 2022, Arxiv, DOI arXiv:2207.14172
   Zhang GJ, 2022, PROC CVPR IEEE, P939, DOI 10.1109/CVPR52688.2022.00102
   Zhang H, 2022, Arxiv, DOI [arXiv:2203.03605, DOI 10.48550/ARXIV.2203.03605, 10.48550/arXiv.2203.03605]
   Zhang HK, 2022, LECT NOTES COMPUT SC, V13686, P613, DOI 10.1007/978-3-031-19809-0_35
   Zhang Q, 2023, VISUAL COMPUT, V39, P4593, DOI 10.1007/s00371-022-02611-1
   Zhang X., 2023, AKConv: Convolutional Kernel with Arbitrary Sampled Shapes and Arbitrary Number of Parameters
   Zheng MH, 2021, Arxiv, DOI [arXiv:2011.09315, 10.48550/arXiv.2011.09315]
   Zhou DQ, 2021, Arxiv, DOI arXiv:2103.11886
   Zhou PW, 2023, VISUAL COMPUT, V39, P3235, DOI 10.1007/s00371-023-02966-z
   Zhu BJ, 2020, Arxiv, DOI arXiv:2007.03496
   Zhu XZ, 2021, Arxiv, DOI [arXiv:2010.04159, 10.48550/arXiv.2010.04159]
NR 70
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 7
PY 2024
DI 10.1007/s00371-024-03422-2
EA MAY 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QD2K1
UT WOS:001218872900002
DA 2024-08-05
ER

PT J
AU Khokhar, FA
   Shah, JH
   Saleem, R
   Masood, A
AF Khokhar, Fahad Ahmed
   Shah, Jamal Hussain
   Saleem, Rabia
   Masood, Anum
TI Harnessing deep learning for faster water quality assessment:
   identifying bacterial contaminants in real time
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Optimized Fast Detection; Water Contamination Assessment; YOLOv5;
   Fine-Tuning; Rapid Detection; Bacterial Detection; Deep learning
ID IDENTIFICATION; NETWORK
AB Water is essential for human survival. Humans can live without food for a few days but without water, a person can barely survive for 3-5 days. Various parts of the world, particularly under-developed countries, have regions where clean water is scarce, and humans living in such conditions have no access to clean water. Our solution provides information on whether water is contaminated or not. Moreover, it overcomes the delay time in getting the result of water contamination using traditional methods of up to 5-6 hrs. Our proposed method detects the colonies of the bacteria that are taken from the water sample (after gram staining) and then classifies the type of bacteria to whom it belongs and how much quantity of each bacterium causes infection to the human body. Bacteria detection is performed by a novel deep learning-based model with user-specified parameters. To improve our ability to detect dangerous bacteria including E. coli, yeast, and particles, we perform tests using datasets from a variety of researchers. On the test benchmark, the fine-tuned proposed model achieves 84.56% accuracy and provides the level of contamination in water.
C1 [Khokhar, Fahad Ahmed] Univ Florence, Dept Math & Informat, Florence, Italy.
   [Shah, Jamal Hussain] COMSATS Univ Islamabad, Dept Comp Sci, Wah Cantt, Pakistan.
   [Saleem, Rabia] Govt Coll Univ, Dept Comp Sci, Faisalabad, Pakistan.
   [Masood, Anum] Norwegian Univ Sci & Technol, Dept Circulat & Med Imaging, Trondheim, Norway.
C3 University of Florence; COMSATS University Islamabad (CUI); Government
   College University Faisalabad; Norwegian University of Science &
   Technology (NTNU)
RP Masood, A (corresponding author), Norwegian Univ Sci & Technol, Dept Circulat & Med Imaging, Trondheim, Norway.
EM anum.masood@ntnu.no
FU NTNU Norwegian University of Science and Technology (incl St. Olavs
   Hospital - Trondheim University Hospital) [7794]; HEC- National Research
   Program for Universities of Pakistan
FX The authors would like to thank HEC- National Research Program for
   Universities of Pakistan (ProjectNumber : 7794).
CR Aghalari Z, 2020, GLOBALIZATION HEALTH, V16, DOI 10.1186/s12992-020-0546-y
   [Anonymous], 2013, WORLD HLTH REPORT 20
   Bridge JW, 2010, B WORLD HEALTH ORGAN, V88, P873, DOI 10.2471/BLT.09.072512
   Centers for Disease Control and Prevention (CDC) and Water, 2017, Healthy and others: Current waterborne disease burden data & gaps
   Chandnani G., 2022, Groundw. Sustain. Dev
   Chayadevi ML, 2013, ADV INTELL SYST, V174, P1091
   Cheema MN, 2019, IEEE T BIO-MED ENG, V66, P2641, DOI 10.1109/TBME.2019.2894123
   Deng L, 2022, IEEE J BIOMED HEALTH, V26, P369, DOI 10.1109/JBHI.2021.3113700
   Engineers C., 2010, Long term 2 enhanced surface water treatment rule toolbox guidance manual
   Galar A, 2012, EUR J CLIN MICROBIOL, V31, P2445, DOI 10.1007/s10096-012-1588-8
   Gopinath SCB, 2014, BIOSENS BIOELECTRON, V60, P332, DOI 10.1016/j.bios.2014.04.014
   Hong Men, 2008, 2008 International Conference on Computer Science and Software Engineering (CSSE 2008), P830, DOI 10.1109/CSSE.2008.485
   Hosang J, 2017, PROC CVPR IEEE, P6469, DOI 10.1109/CVPR.2017.685
   Khayyat MM, 2023, ALEX ENG J, V75, P407, DOI 10.1016/j.aej.2023.05.082
   KhoKhar FA, 2022, COMPUT ELECTR ENG, V99, DOI 10.1016/j.compeleceng.2022.107818
   Li Xiaojuan, 2009, WSEAS Transactions on Computers, V8, P237
   Lili Xu, 2010, 2010 2nd International Conference on Education Technology and Computer (ICETC 2010), P389, DOI 10.1109/ICETC.2010.5529223
   Marston HD, 2016, JAMA-J AM MED ASSOC, V316, P1193, DOI 10.1001/jama.2016.11764
   Mortier T, 2021, COMPUT STRUCT BIOTEC, V19, P6157, DOI 10.1016/j.csbj.2021.11.004
   Mosleh MAA, 2012, BMC BIOINFORMATICS, V13, DOI 10.1186/1471-2105-13-S17-S25
   Nazir A, 2020, IEEE T IMAGE PROCESS, V29, P7192, DOI 10.1109/TIP.2020.2999854
   Nazir A, 2020, J BIOMED INFORM, V106, DOI 10.1016/j.jbi.2020.103430
   Nnachi RC, 2022, ENVIRON INT, V166, DOI 10.1016/j.envint.2022.107357
   Noble Rachel T., 2005, Journal of Water and Health, V3, P381
   Organization W.H., 2019, Water, sanitation, hygiene and health: a primer for health professionals
   Osman M. K., 2010, Proceedings 10th International Conference on Intelligent Systems Design and Applications (ISDA 2010), P1229, DOI 10.1109/ISDA.2010.5687018
   Papa F, 2023, SURV GEOPHYS, V44, P43, DOI 10.1007/s10712-022-09700-9
   Ray SS, 2016, RSC ADV, V6, P85495, DOI 10.1039/c6ra14952a
   Rhoads DD, 2012, INT J MOL SCI, V13, P2535, DOI 10.3390/ijms13032535
   Sandgren A, 2009, PLOS MED, V6, P132, DOI 10.1371/journal.pmed.1000002
   Tamiev D, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0241200
   Trattner S, 2004, IEEE T MED IMAGING, V23, P807, DOI 10.1109/TMI.2004.827481
   Visitsattaponge S, 2024, IEEE ACCESS, V12, P15609, DOI 10.1109/ACCESS.2024.3358671
   Yasmine G, 2023, INT WIREL COMMUN, P1579, DOI 10.1109/IWCMC58020.2023.10182423
NR 34
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 26
PY 2024
DI 10.1007/s00371-024-03382-7
EA APR 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ1J9
UT WOS:001208646400001
OA hybrid
DA 2024-08-05
ER

PT J
AU Yuan, JQ
   Fan, MT
   Liu, ZY
   Han, TX
   Kuang, ZZ
   Pan, CH
   Ding, JJ
AF Yuan, Junqing
   Fan, Mengting
   Liu, Zhenyang
   Han, Tongxuan
   Kuang, Zhenzhong
   Pan, Chihao
   Ding, Jiajun
TI Collaborative neural radiance fields for novel view synthesis
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Novel view synthesis; Collaborative strategy; Multiple models; Neural
   radiance field
AB Neural radiance fields (NeRF) synthesize realistic novel views by estimating point attributes (density and color), followed by the volume rendering method. However, accurately predicting the arbitrary point attributes poses a challenge for the single NeRF-based model. Such limitation directly impacts the quality of novel view synthesis. To address this problem, a collaborative strategy with multiple NeRF-based models is proposed. This strategy is the first to introduce a multi-model cascaded architecture into NeRF for achieving high-quality novel view synthesis. Its purpose is to utilize a cascading architecture in space for the progressive enhancement of point attribute accuracy. The cascading architecture includes point adjustment and snapshots fusion. Specifically, point adjustment leverages a pretrained NeRF-based model to predict the initial density and color of each point in space. This step affords an initial rendering of target scene. Then, these initial density and color of points are directly transferred to the subsequent NeRF-based model. This process guides the subsequent NeRF-based model to focus on the refinement of initial point attributes and synthesize more realistic novel views. Finally, snapshots fusion fuses outputs (referred as snapshots) from multiple parallel subsequent NeRF-based models to synthesize the ultimate high-quality novel views. The proposed strategy is tested with a range of established NeRF-based methods, such as NeRF, Instant-NGP, and TensoRF. Experimental data for this research are sourced from the realistic 360 synthetic dataset and the LLFF dataset. Results indicate that the proposed collaborative strategy with established NeRF-based methods can improve the quality of novel view synthesis, surpassing the corresponding single model. Our project page is available at https://github.com/ZhenyangLiu/Collaborative-Neural-Radiance-Fields-for-Novel-View-Synthesis.
C1 [Yuan, Junqing; Liu, Zhenyang; Han, Tongxuan] Zhejiang Univ Technol, Coll Sci, Hangzhou 310023, Peoples R China.
   [Fan, Mengting; Kuang, Zhenzhong; Pan, Chihao; Ding, Jiajun] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou 310018, Peoples R China.
   [Liu, Zhenyang] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
   [Ding, Jiajun] Zhejiang Zhongcai Pipes Sci &Technol Co Ltd, Xinchang 312500, Peoples R China.
C3 Zhejiang University of Technology; Hangzhou Dianzi University; Fudan
   University
RP Liu, ZY (corresponding author), Zhejiang Univ Technol, Coll Sci, Hangzhou 310023, Peoples R China.; Liu, ZY (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
EM yjqzjhz@163.com; fmt@hdu.edu.cn; lzyzjhz@163.com;
   kreeze11152003@163.com; zzkuang@hdu.edu.cn; stephpanch@gmail.com;
   djj@hdu.edu.cn
FU Natural Science Foundation of China; National Undergraduate Training
   Program for Innovation and Entrepreneurship [202310336014]; Zhejiang
   Provincial Natural Science Foundation of China [LY22F020028]; National
   Natural Science Foundation of China [U21B2040];  [62206082]
FX The authors acknowledge the financial supported by the Natural Science
   Foundation of China (Grant No. 62206082), National Undergraduate
   Training Program for Innovation and Entrepreneurship (Grant No.
   202310336014), Zhejiang Provincial Natural Science Foundation of China
   (Grant No. LY22F020028) and the National Natural Science Foundation of
   China (Grant No. U21B2040).
CR Avidan S, 1997, PROC CVPR IEEE, P1034, DOI 10.1109/CVPR.1997.609457
   Barron JT, 2022, PROC CVPR IEEE, P5460, DOI 10.1109/CVPR52688.2022.00539
   Barron JT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5835, DOI 10.1109/ICCV48922.2021.00580
   Chen AP, 2022, LECT NOTES COMPUT SC, V13692, P333, DOI 10.1007/978-3-031-19824-3_20
   Chen AP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14104, DOI 10.1109/ICCV48922.2021.01386
   Ding JJ, 2018, SIGNAL PROCESS, V152, P69, DOI 10.1016/j.sigpro.2018.05.012
   Ertugrul E, 2020, COMPUT ANIMAT VIRT W, V31, DOI 10.1002/cav.1959
   Garbin SJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14326, DOI 10.1109/ICCV48922.2021.01408
   Ge LH, 2019, IEEE T PATTERN ANAL, V41, P956, DOI 10.1109/TPAMI.2018.2827052
   Ji MQ, 2017, IEEE I CONF COMP VIS, P2326, DOI 10.1109/ICCV.2017.253
   Jiang JW, 2020, GRAPH MODELS, V111, DOI 10.1016/j.gmod.2020.101077
   Kazemy A, 2022, IEEE T NEUR NET LEAR, V33, P952, DOI 10.1109/TNNLS.2020.3030638
   Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954
   Luvizon DC, 2023, PATTERN RECOGN, V142, DOI 10.1016/j.patcog.2023.109714
   Martin-Brualla R, 2021, PROC CVPR IEEE, P7206, DOI 10.1109/CVPR46437.2021.00713
   Miao H., 2021, P IEEECVF INT C COMP, P15631
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Morales A, 2023, PATTERN RECOGN, V139, DOI 10.1016/j.patcog.2023.109367
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Park K, 2021, Arxiv, DOI arXiv:2106.13228
   Park K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5845, DOI 10.1109/ICCV48922.2021.00581
   Pumarola A, 2021, PROC CVPR IEEE, P10313, DOI 10.1109/CVPR46437.2021.01018
   Qin Y., 2023, Vis. Comput, V1, P1
   Reiser C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14315, DOI 10.1109/ICCV48922.2021.01407
   Schwarz K., 2020, ADV NEURAL INFORM PR, V33, P20154, DOI DOI 10.48550/ARXIV.2007.02442
   Sheng B, 2021, IEEE T CYBERNETICS, V51, P1463, DOI 10.1109/TCYB.2020.2988792
   Sheng B, 2018, COMPUT AIDED GEOM D, V62, P133, DOI 10.1016/j.cagd.2018.03.021
   Sheng B, 2018, GRAPH MODELS, V97, P1, DOI 10.1016/j.gmod.2018.03.001
   Singha T, 2023, PATTERN RECOGN, V140, DOI 10.1016/j.patcog.2023.109557
   Taherkhani A, 2018, IEEE T NEUR NET LEAR, V29, P5394, DOI 10.1109/TNNLS.2018.2797801
   Tancik M, 2022, PROC CVPR IEEE, P8238, DOI 10.1109/CVPR52688.2022.00807
   Wang C, 2022, Arxiv, DOI arXiv:2112.01759
   Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4
   Wang QQ, 2021, PROC CVPR IEEE, P4688, DOI 10.1109/CVPR46437.2021.00466
   Xie Z., 2023, PROC IEEE INT C COMP, P18024
   Xiong DP, 2018, INFORM SCIENCES, V454, P328, DOI 10.1016/j.ins.2018.04.075
   Xu QG, 2022, PROC CVPR IEEE, P5428, DOI 10.1109/CVPR52688.2022.00536
   Yang GW, 2023, COMPUT VIS MEDIA, V9, P401, DOI 10.1007/s41095-022-0327-z
   Yang Guo-Wei, 2022, IEEE Trans. Vis. Comput. Graph., P1
   Yu A, 2021, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR46437.2021.00455
   Zhang K, 2020, Arxiv, DOI arXiv:2010.07492
NR 41
TC 0
Z9 0
U1 7
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 12
PY 2024
DI 10.1007/s00371-024-03379-2
EA APR 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NN5Y8
UT WOS:001201155300002
DA 2024-08-05
ER

PT J
AU Wang, LH
   Li, J
   Guo, SW
   Han, SK
AF Wang, Luhan
   Li, Jun
   Guo, Shangwei
   Han, Shaokun
TI A cascaded graph convolutional network for point cloud completion
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE 3D point cloud; Point cloud completion; Deep learning; Graph
   convolutional network
ID SHAPE
AB Point cloud completion represents a complex task that entails predicting the complete geometry of a 3D shape from a set representation of the partial observational points. This paper presents a novel point cloud completion method, called Cascaded Graph Convolutional Completion Network (CGCN). Our method leverages a cascaded encoder-decoder architecture to predict the missing points from the input, the partial points. These predicted missing points are then concatenated with the input points to form the complete shape. Our architecture consists of two main modules: a Multi-Level Feature Extraction Encoder (MFE) and a Folding-Refinement Decoder (FRD). The MFE is composed of the Edge Feature Extraction Module (EFE) and the Global Feature Extraction Module (GFE). The encoder initiates by using several EFEs to secure multi-level local features through the execution of graph convolutional operations on each point. It then employs a GFE to extract global features from the final level features. Consequently, the MFE aggregates features containing local information into global features, offering an output that carries both local and global features. Our FRD contains both a folding block and several refinement blocks. Initially, the folding block morphs 2D grids with features output by the MFE to predict the initial missing points. The decoder then iteratively applies a series of refinement blocks to refine these initial missing points and eventually obtain the output of the predicted missing points. The FRD merges multi-level features and global features to deliver the shape of the predicted missing part. To achieve the complete shape, we integrate the predicted missing part, derived from the FRD and the input points. We then sample this unified point cloud to secure the final output. The effectiveness and competitiveness of our model are validated through experiments on the ShapeNet dataset, using Chamfer Distance (CD) as metrics.
C1 [Wang, Luhan; Li, Jun; Guo, Shangwei; Han, Shaokun] Beijing Key Lab Precis Optoelect Measurement Instr, Beijing 100081, Peoples R China.
   [Wang, Luhan; Li, Jun; Guo, Shangwei; Han, Shaokun] Sch Opt & Photon, Beijing 100081, Peoples R China.
RP Han, SK (corresponding author), Beijing Key Lab Precis Optoelect Measurement Instr, Beijing 100081, Peoples R China.; Han, SK (corresponding author), Sch Opt & Photon, Beijing 100081, Peoples R China.
EM 3120210613@bit.edu.cn; junl@bit.edu.cn; 3120195346@bit.edu.cn;
   skhan@bit.edu.cn
CR Achlioptas P, 2018, PR MACH LEARN RES, V80
   Bai SY, 2021, ANN TRANSL MED, V9, DOI 10.21037/atm-21-1572
   Chang YK, 2021, NEUROCOMPUTING, V460, P266, DOI 10.1016/j.neucom.2021.06.080
   Chen L., 2023, The Visual Computer
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Duan YQ, 2019, PROC CVPR IEEE, P949, DOI 10.1109/CVPR.2019.00104
   Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54
   Fan L, 2017, IEEE ICC
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Gadelha M., 2018, P EUROPEAN C COMPUTE
   Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961
   Guo S., 2021, arXiv
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Han XG, 2017, IEEE I CONF COMP VIS, P85, DOI 10.1109/ICCV.2017.19
   He XW, 2018, PROC CVPR IEEE, P1945, DOI 10.1109/CVPR.2018.00208
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang Z., 2020, P IEEECVF C COMPUTER
   Joseph-Rivlin M, 2019, IEEE INT CONF COMP V, P4085, DOI 10.1109/ICCVW.2019.00503
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Le T, 2018, PROC CVPR IEEE, P9204, DOI 10.1109/CVPR.2018.00959
   Li J, 2022, NEUROCOMPUTING, V491, P1, DOI 10.1016/j.neucom.2022.03.060
   Li RH, 2019, IEEE I CONF COMP VIS, P7202, DOI 10.1109/ICCV.2019.00730
   Lin C.-H., 2018, P AAAI C ARTIFFCIAL, V32, DOI [10.1609/aaai.v32i1.12278, DOI 10.1609/AAAI.V32I1.12278]
   Lin HX, 2019, IEEE INT CON MULTI, P326, DOI 10.1109/ICME.2019.00064
   Liu H, 2024, VISUAL COMPUT, V40, P971, DOI 10.1007/s00371-023-02826-w
   Liu J., 2019, P IEEECVF INT C COMP
   Liu MH, 2020, AAAI CONF ARTIF INTE, V34, P11596
   Liu Q, 2022, VISUAL COMPUT, V38, P3341, DOI 10.1007/s00371-022-02550-x
   Liu X., 2020, SPU-net: self-supervised point cloud upsampling by coarseto-fine reconstruction with self-projection optimization
   Lu D., 2022, Transformers in 3D point clouds: a survey
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Mendoza A, 2020, Arxiv, DOI arXiv:2010.04278
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Nguyen D.T., 2016, P IEEE C COMPUTER VI
   Nie Yinyu, 2020, ADV NEURAL INFORM PR, V33, P16119
   Pan L, 2020, IEEE ROBOT AUTOM LET, V5, P4392, DOI 10.1109/LRA.2020.2994483
   Qi CR, 2017, ADV NEUR IN, V30
   Qiu Q., 2021, Remote Sens, V13
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Sarmad M, 2019, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR.2019.00605
   Shen YR, 2018, PROC CVPR IEEE, P4548, DOI 10.1109/CVPR.2018.00478
   Shi JQ, 2021, IEEE ROBOT AUTOM LET, V6, P7081, DOI 10.1109/LRA.2021.3097081
   Simonovsky M, 2017, PROC CVPR IEEE, P29, DOI 10.1109/CVPR.2017.11
   Soltani AA, 2017, PROC CVPR IEEE, P2511, DOI 10.1109/CVPR.2017.269
   Song Y., 2023, The Visual Computer
   Steder B, 2011, IEEE INT CONF ROBOT, P2601, DOI 10.1109/ICRA.2011.5980187
   Stutz D, 2018, PROC CVPR IEEE, P1955, DOI 10.1109/CVPR.2018.00209
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Sun X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P980, DOI 10.1145/3343031.3351042
   Sun Y., 2020, The Visual Computer, V36, P10
   Sung M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818094
   Tchapmi L.P., 2019, P IEEECVF C COMPUTER
   Thrun S, 2005, IEEE I CONF COMP VIS, P1824
   Varley J, 2017, IEEE INT C INT ROBOT, P2442, DOI 10.1109/IROS.2017.8206060
   Velikovi P., 2017, Graph Attention Networks
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wei MQ, 2023, IEEE T PATTERN ANAL, V45, P9374, DOI 10.1109/TPAMI.2023.3238516
   Wei X, 2020, PROC CVPR IEEE, P1847, DOI 10.1109/CVPR42600.2020.00192
   Wen X, 2020, PROC CVPR IEEE, P1936, DOI 10.1109/CVPR42600.2020.00201
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Chang AX, 2015, Arxiv, DOI [arXiv:1512.03012, DOI 10.48550/ARXIV.1512.03012]
   Xiang P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5479, DOI 10.1109/ICCV48922.2021.00545
   Xie Haozhe, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P365, DOI 10.1007/978-3-030-58545-7_21
   Xu QG, 2020, PROC CVPR IEEE, P5660, DOI 10.1109/CVPR42600.2020.00570
   Yan X, 2020, PROC CVPR IEEE, P5588, DOI 10.1109/CVPR42600.2020.00563
   Yang JC, 2019, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2019.00344
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yang Z, 2019, IEEE I CONF COMP VIS, P7504, DOI 10.1109/ICCV.2019.00760
   Yang ZL, 2020, Arxiv, DOI [arXiv:1906.08237, DOI 10.48550/ARXIV.1906.08237]
   Yu J., 2021, 3D medical point transformer: introducing convolution to attention networks for medical point cloud analysis
   Yu T, 2018, PROC CVPR IEEE, P186, DOI 10.1109/CVPR.2018.00027
   Yu XM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12478, DOI 10.1109/ICCV48922.2021.01227
   Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088
   Zhai R., 2022, ISPRS Int. J. Geo-Inf., V11
   Zhang K., 2019, arXiv, DOI DOI 10.48550/ARXIV.1904.10014
   Zhang W., 2023, IEEE Trans. Vis. Comput. Graph.
   Zhao H., 2019, P IEEECVF C COMPUTER
   Zhou C., 2021, arXiv
   Zhou HR, 2022, Arxiv, DOI arXiv:2207.10315
NR 79
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 9
PY 2024
DI 10.1007/s00371-024-03354-x
EA APR 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NG1R9
UT WOS:001199213300001
DA 2024-08-05
ER

PT J
AU Lu, SL
   Fung, S
   Pan, W
   Wickramasinghe, N
   Lu, XQ
AF Lu, Shenglin
   Fung, Sheldon
   Pan, Wei
   Wickramasinghe, Nilmini
   Lu, Xuequan
TI Veintr: robust end-to-end full-hand vein identification with transformer
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Palm vein identification; Biometrics security; Transformer; 0000; 1111
ID RECOGNITION; PALMPRINT; FUSION
AB Hand vein identification stands out to be an increasingly popular approach for biometric identification due to its distinctiveness and convenience. While state-of-the-art techniques are able to achieve good performance, they share two common drawbacks: (1) complex preprocessing procedures, e.g., vein enhancement and Region of Interest (ROI) extraction, and (2) vein information loss due to hand ROI partition. To address these issues, we propose VeinTr, an end-to-end full-hand vein identification approach. In particular, our VeinTr consists of three components: a local feature extractor, a lightweight transformer, and a global feature decoder. We first obtain local features via convolution-based ResNet-like blocks. Then the attention mechanism is employed to aggregate global features from local features, which can be then decoded as global hand vein features. Finally, a global feature decoder is applied to generate robust hand features. By doing so, VeinTr is capable of directly extracting robust hand vein features from raw hand vein images. We evaluate our method on CASIA, TPV, and PLUSVein hand vein datasets. Experimental results show that our approach outperforms the state-of-the-art methods and has strong inter-dataset generalization abilities.
C1 [Lu, Shenglin; Pan, Wei] OPT MV, Dongguan, Peoples R China.
   [Fung, Sheldon; Wickramasinghe, Nilmini; Lu, Xuequan] La Trobe Univ, Melbourne, Vic, Australia.
C3 La Trobe University
RP Lu, XQ (corresponding author), La Trobe Univ, Melbourne, Vic, Australia.
EM b.lu@latrobe.edu.au
OI wickramasinghe, nilmini/0000-0002-1314-8843
FU  [3.2501.11.47];  [3.6267.01]
FX This work is supported in part by the investigator fund (3.2501.11.47)
   and the industry fund (3.6267.01).
CR Akbar AF, 2016, 2016 4TH INTERNATIONAL CONFERENCE ON INFORMATION AND COMMUNICATION TECHNOLOGY (ICOICT)
   Ananthi G, 2022, VISUAL COMPUT, V38, P1901, DOI 10.1007/s00371-021-02253-9
   Beining Huang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1269, DOI 10.1109/ICPR.2010.316
   Bhilare S, 2018, MACH VISION APPL, V29, P1269, DOI 10.1007/s00138-018-0959-2
   Chai TT, 2023, VISUAL COMPUT, V39, P4029, DOI 10.1007/s00371-022-02571-6
   Chen YY, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3468873
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Feng D., 2020, ICONIP, P316
   Fronitasari D, 2017, 2017 15TH INTERNATIONAL CONFERENCE ON QUALITY IN RESEARCH (QIR) - INTERNATIONAL SYMPOSIUM ON ELECTRICAL AND COMPUTER ENGINEERING, P18, DOI 10.1109/QIR.2017.8168444
   Fung Sheldon, 2021, 2021 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN52387.2021.9534089
   Genovese A, 2019, IEEE INT CONF COMP, P166, DOI 10.1109/civemsa45640.2019.9071620
   Gumaei A, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18051575
   Hao Y, 2008, IEEE IMAGE PROC, P281, DOI 10.1109/ICIP.2008.4711746
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ito K, 2015, INT CONF BIOMETR, P334, DOI 10.1109/ICB.2015.7139058
   Jing YP, 2023, COMPUT VIS MEDIA, V9, P657, DOI 10.1007/s41095-022-0317-1
   Johnson J, 2024, VISUAL COMPUT, V40, P3217, DOI 10.1007/s00371-023-03023-5
   Kang WX, 2014, IEEE T INF FOREN SEC, V9, P1974, DOI 10.1109/TIFS.2014.2361020
   Kang WX, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0097548
   Kauba C, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19225014
   Kolberg J, 2023, IEEE WINT CONF APPL, P653, DOI 10.1109/WACVW58289.2023.00072
   Li Xueyan, 2007, 2007 1st International Conference on Bioinformatics and Biomedical Engineering, P612, DOI 10.1109/ICBBE.2007.160
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Mirmohamadsadeghi L, 2014, IET BIOMETRICS, V3, P198, DOI 10.1049/iet-bmt.2013.0041
   Öztürk HI, 2022, IEEE COMPUT SOC CONF, P1626, DOI 10.1109/CVPRW56347.2022.00169
   Perwira DY, 2014, 2014 INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY SYSTEMS AND INNOVATION (ICITSI), P99, DOI 10.1109/ICITSI.2014.7048245
   Piciucco E, 2018, IET BIOMETRICS, V7, P439, DOI 10.1049/iet-bmt.2017.0192
   Pratiwi AY, 2016, 2016 4TH INTERNATIONAL CONFERENCE ON INFORMATION AND COMMUNICATION TECHNOLOGY (ICOICT)
   Qin HF, 2017, IEEE T INF FOREN SEC, V12, P1816, DOI 10.1109/TIFS.2017.2689724
   Rahul RC, 2015, 2015 INTERNATIONAL CONFERENCE ON COMPUTING AND NETWORK COMMUNICATIONS (COCONET), P793, DOI 10.1109/CoCoNet.2015.7411280
   Revaud J, 2016, INT J COMPUT VISION, V120, P300, DOI 10.1007/s11263-016-0908-3
   Thapar D, 2019, 2019 5TH IEEE INTERNATIONAL CONFERENCE ON IDENTITY, SECURITY, AND BEHAVIOR ANALYSIS (ISBA 2019), DOI 10.1109/isba.2019.8778623
   Vaswani A, 2017, ADV NEUR IN, V30
   Wirayuda TAB, 2015, 5TH INTERNATIONAL CONFERENCE ON ELECTRICAL ENGINEERING AND INFORMATICS 2015, P350, DOI 10.1109/ICEEI.2015.7352525
   Wu KS, 2013, J SYST SOFTWARE, V86, P2870, DOI 10.1016/j.jss.2013.06.065
   Wu W, 2020, IET BIOMETRICS, V9, P1, DOI 10.1049/iet-bmt.2019.0034
   Wu W, 2019, IET BIOMETRICS, V8, P206, DOI 10.1049/iet-bmt.2018.5027
   Xie CH, 2019, PATTERN RECOGN LETT, V119, P148, DOI 10.1016/j.patrec.2017.12.001
   Zeng S, 2016, Media, V2, P357
   Zhang L, 2018, SYMMETRY-BASEL, V10, DOI 10.3390/sym10040078
   Zhang Yufei, 2023, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P1011, DOI 10.1109/CVPRW59228.2023.00108
   Zhou YB, 2011, IEEE T INF FOREN SEC, V6, P1259, DOI 10.1109/TIFS.2011.2158423
NR 43
TC 0
Z9 0
U1 7
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 20
PY 2024
DI 10.1007/s00371-024-03286-6
EA MAR 2024
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LX3V2
UT WOS:001190081000002
DA 2024-08-05
ER

PT J
AU An, HY
   Jia, RS
AF An, Heng-Yu
   Jia, Rui-Sheng
TI Self-supervised facial expression recognition with fine-grained feature
   selection
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Facial expression recognition; Self-supervised learning; Multi-level
   feature combine; Feature selection
ID REPRESENTATION
AB Facial expression recognition (FER) holds significant practical implications in real-world scenarios such as human-computer interaction, fatigue driving detection, and learning engagement analysis. Nonetheless, acquiring large-scale and high-quality annotated facial expression datasets is profoundly challenging due to the inherent ambiguity of facial images and concerns over privacy. Consequently, this paper introduces a self-supervised facial expression recognition method based on mask image modeling. This method can learn multi-level facial feature representations without expensive labels and achieves commendable facial expression recognition performance through further fine-grained feature selection. Specifically, we propose the multi-level feature selector (MFS). The MFS comprises two pivotal components: the multi-level feature combiner and the feature selector. During the pre-training stage, the multi-level feature combiner is employed to integrate multi-level features, effectively addressing the vision transformer's deficiencies in capturing high-frequency facial semantics. Subsequently, in the fine-tuning stage, the feature selector can automatically differentiate highly discriminative regions, extracting fine-grained features. Subsequently, we use graph convolutional networks to further mine the latent connections among fine-grained features, ultimately deriving an integrated feature with enhanced discriminative capabilities. Through such fine-grained facial feature selection, we can mitigate performance degradation induced by inter-class similarities and intra-class variations. Experimental results on the RAF-DB, AffectNet, and FER + datasets demonstrate that our approach significantly outperforms other self-supervised methods in recognition performance and closely approaches the state-of-the-art methods in supervised learning. The code is available at https://github.com/Greysahy/MFS.
C1 [An, Heng-Yu; Jia, Rui-Sheng] Shandong Univ Sci & Technol, Coll Comp Sci & Engn, Qingdao 266590, Peoples R China.
C3 Shandong University of Science & Technology
RP Jia, RS (corresponding author), Shandong Univ Sci & Technol, Coll Comp Sci & Engn, Qingdao 266590, Peoples R China.
EM jrs716@163.com
FU Humanities and Social Science Fund of the Ministry of Education of the
   People's Republic of China
FX No Statement Available
CR Barsoum E, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P279, DOI 10.1145/2993148.2993165
   Cai J, 2018, IEEE INT CONF AUTOMA, P302, DOI 10.1109/FG.2018.00051
   Cai Z, 2023, PROC CVPR IEEE, P1493, DOI 10.1109/CVPR52729.2023.00150
   Chen J., 2014, INT WORKSHOPS ELECT, P884
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen XL, 2020, Arxiv, DOI arXiv:2003.04297
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549
   Ekman P., 1978, Facial action coding system: A technique for the measurement of facial movement
   Esmaeili V, 2022, IET IMAGE PROCESS, V16, P3814, DOI 10.1049/ipr2.12596
   Esmaeili V, 2020, MULTIMED TOOLS APPL, V79, P20221, DOI 10.1007/s11042-020-08737-5
   Farzaneh AH, 2021, IEEE WINT CONF APPL, P2401, DOI 10.1109/WACV48630.2021.00245
   Grill J.B., 2020, P 34 INT C NEUR INF, V33, P21271, DOI [10.48550/arXiv.2006.07733, DOI 10.48550/ARXIV.2006.07733]
   Happy SL, 2015, IEEE T AFFECT COMPUT, V6, P1, DOI 10.1109/TAFFC.2014.2386334
   Hasani B, 2022, IEEE T AFFECT COMPUT, V13, P1023, DOI 10.1109/TAFFC.2020.2986440
   He H., 2020, P IEEE CVF C COMP VI, P9729
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   Li HY, 2022, PROC CVPR IEEE, P4156, DOI 10.1109/CVPR52688.2022.00413
   Li HY, 2022, IEEE T IMAGE PROCESS, V31, P4637, DOI 10.1109/TIP.2022.3186536
   Li HY, 2021, NEUROCOMPUTING, V432, P159, DOI 10.1016/j.neucom.2020.12.076
   Li HY, 2021, IEEE T IMAGE PROCESS, V30, P2016, DOI 10.1109/TIP.2021.3049955
   Li HH, 2023, VISUAL COMPUT, V39, P4709, DOI 10.1007/s00371-022-02619-7
   Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277
   Li Y, 2018, INT C PATT RECOG, P2209, DOI 10.1109/ICPR.2018.8545853
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P2439, DOI 10.1109/TIP.2018.2886767
   Ma BW, 2022, Arxiv, DOI [arXiv:2210.15878, 10.48550/arXiv.2210.15878]
   Mao JW, 2023, Arxiv, DOI [arXiv:2301.12149, DOI 10.48550/ARXIV.2301.12149]
   Fernandez PDM, 2019, IEEE COMPUT SOC CONF, P837, DOI 10.1109/CVPRW.2019.00112
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Park N., 2022, arXiv, DOI DOI 10.48550/ARXIV.2202.06709
   Roy Shuvendu, 2021, ICMI '21: Proceedings of the 2021 International Conference on Multimodal Interaction, P253, DOI 10.1145/3462244.3479955
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shi JG, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2094
   Shu Y., 2022, PREPRINT, DOI [10.48550/arXiv.2210.03853, DOI 10.48550/ARXIV.2210.03853]
   Sun Licai, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P6110, DOI 10.1145/3581783.3612365
   Wang K, 2020, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR42600.2020.00693
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Wen ZY, 2023, BIOMIMETICS-BASEL, V8, DOI 10.3390/biomimetics8020199
   Xia HY, 2024, VISUAL COMPUT, V40, P2035, DOI 10.1007/s00371-023-02900-3
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Xue FL, 2023, IEEE T AFFECT COMPUT, V14, P3244, DOI 10.1109/TAFFC.2022.3226473
   Zeng D, 2022, PROC CVPR IEEE, P20259, DOI 10.1109/CVPR52688.2022.01965
   Zhao Shuwen, 2018, BMVC, V12, P317
   Zhao ZQ, 2021, AAAI CONF ARTIF INTE, V35, P3510
   Zhao ZQ, 2021, IEEE T IMAGE PROCESS, V30, P6544, DOI 10.1109/TIP.2021.3093397
   Zheng C, 2023, IEEE INT CONF COMP V, P3138, DOI 10.1109/ICCVW60793.2023.00339
NR 45
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 17
PY 2024
DI 10.1007/s00371-024-03322-5
EA MAR 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LI2J4
UT WOS:001186092000002
DA 2024-08-05
ER

PT J
AU Zhu, JL
   Wang, H
   Hogg, D
   Kelly, T
AF Zhu, Jialin
   Wang, He
   Hogg, David
   Kelly, Tom
TI Learning to sculpt neural cityscapes
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Terrain mesh; Deep learning; Mesh deformation; Linear programming
AB We introduce a system that learns to sculpt 3D models of massive urban environments. The majority of humans live their lives in urban environments, using detailed virtual models for applications as diverse as virtual worlds, special effects, and urban planning. Generating such 3D models from exemplars manually is time-consuming, while 3D deep learning approaches have high memory costs. In this paper, we present a technique for training 2D neural networks to repeatedly sculpt a plane into a large-scale 3D urban environment. An initial coarse depth map is created by a GAN model, from which we refine 3D normal and depth using an image translation network regularized by a linear system. The networks are trained using real-world data to allow generative synthesis of meshes at scale. We exploit sculpting from multiple viewpoints to generate a highly detailed, concave, and water-tight 3D mesh. We show cityscapes at scales of 100x1600\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$100 \times 1600$$\end{document} meters with more than 2 million triangles, and demonstrate that our results are objectively and subjectively similar to our exemplars.
C1 [Zhu, Jialin; Hogg, David] Univ Leeds, Sch Comp, Woodhouse Lane, Leeds LS2 9JT, England.
   [Wang, He] UCL, Dept Comp Sci, Gower St, London WC1E 6BT, England.
C3 University of Leeds; University of London; University College London
RP Zhu, JL (corresponding author), Univ Leeds, Sch Comp, Woodhouse Lane, Leeds LS2 9JT, England.
EM misaliet@outlook.com
FU University of Leed
FX No Statement AvailableDAS:The Netherland dataset for evaluation can be
   found at https://3dbag.nl/en/viewer. Due to copyright reasons, we cannot
   release the English town dataset. However, the way to obtain it can be
   known by contacting the corresponding author.
CR Alliez P, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P49
   autodesk, Mudbox by Autodesk
   Baillard C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P559, DOI 10.1109/CVPR.1999.784966
   Biljecki F, 2017, COMPUT ENVIRON URBAN, V64, P1, DOI 10.1016/j.compenvurbsys.2017.01.001
   Biljecki F, 2016, COMPUT ENVIRON URBAN, V59, P25, DOI 10.1016/j.compenvurbsys.2016.04.005
   Brenner C, 2016, 24TH ACM SIGSPATIAL INTERNATIONAL CONFERENCE ON ADVANCES IN GEOGRAPHIC INFORMATION SYSTEMS (ACM SIGSPATIAL GIS 2016), DOI 10.1145/2996913.2996990
   Centin M., 2015, Rameshcleaner: conservative fixing of triangular meshes
   Chan ER, 2022, PROC CVPR IEEE, P16102, DOI 10.1109/CVPR52688.2022.01565
   Chen Q., 2020, P IEEE CVF C COMP VI, P270
   Chen ZQ, 2020, PROC CVPR IEEE, P42, DOI 10.1109/CVPR42600.2020.00012
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   Community B.O., 2018, Blender-a 3D Modelling and Rendering Package
   Darabi S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185578
   Demir Ilke, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P456, DOI 10.1109/3DV.2014.31
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Fang H., 2020, CVPR 2020 IEEE C COM
   Fang T, 2013, IEEE T VIS COMPUT GR, V19, P1720, DOI 10.1109/TVCG.2013.68
   Femiani J, 2018, Arxiv, DOI arXiv:1805.08634
   Feng QJ, 2021, IEEE INT CONF ROBOT, P5208, DOI 10.1109/ICRA48506.2021.9561337
   Fruhstuck A, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322993
   Galin E, 2019, COMPUT GRAPH FORUM, V38, P553, DOI 10.1111/cgf.13657
   Gao Jun, 2022, NEURIPS
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Georgiou Y, 2021, INT CONF 3D VISION, P1034, DOI 10.1109/3DV53792.2021.00111
   Gröger G, 2012, ISPRS J PHOTOGRAMM, V71, P12, DOI 10.1016/j.isprsjprs.2012.04.004
   Groueix T, 2018, Arxiv, DOI arXiv:1802.05384
   Guérin E, 2022, COMPUT GRAPH FORUM, V41, P85, DOI 10.1111/cgf.14460
   Guérin E, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130804
   Guo X, 2020, NEUROCOMPUTING, V394, P127, DOI 10.1016/j.neucom.2019.01.115
   Gurobi Optimization LLC, 2023, GUROBI OPTIMIZER REF
   Haala N., 1998, IAPRS, V32, P339
   Haala N, 2010, ISPRS J PHOTOGRAMM, V65, P570, DOI 10.1016/j.isprsjprs.2010.09.006
   Hao ZK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14052, DOI 10.1109/ICCV48922.2021.01381
   Heusel M., 2017, NeurIPS, P6629
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2020, Arxiv, DOI [arXiv:2006.06676, 10.48550/arXiv.2006.06676, DOI 10.48550/ARXIV.2006.06676]
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Kelly G., 2007, P GDTW 2007 5 ANN C, P8
   Kelly T, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275065
   Kozinski M, 2015, PROC CVPR IEEE, P2820, DOI 10.1109/CVPR.2015.7298899
   Kratt J, 2015, COMPUT GRAPH FORUM, V34, P361, DOI 10.1111/cgf.12566
   Lafarge F, 2010, IEEE T PATTERN ANAL, V32, P135, DOI 10.1109/TPAMI.2008.281
   Lafarge F, 2012, INT J COMPUT VISION, V99, P69, DOI 10.1007/s11263-012-0517-8
   Laycock R.G., 2003, P 8 ACM S SOLID MODE, P346
   Liang L, 2001, ACM T GRAPHIC, V20, P127, DOI 10.1145/501786.501787
   Liao YY, 2018, PROC CVPR IEEE, P2916, DOI 10.1109/CVPR.2018.00308
   Lin CH, 2023, Arxiv, DOI arXiv:2301.09637
   Lin H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461969
   Lipp M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601197
   Liu R, 2018, ADV NEUR IN, V31
   Lorensen H. E., 1987, Proc. SIGGRAPH, V21, P163, DOI 10.1145/37401.37422
   Martinovic A, 2013, PROC CVPR IEEE, P201, DOI 10.1109/CVPR.2013.33
   maxon, ZBrush by Maxon
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Mildenhall B, 2020, P ECCV, DOI DOI 10.1007/978-3-030-58452-8
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Monszpart A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766995
   Morreale L., 2022, IEEE CVPR, P19333
   Müller P, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276484, 10.1145/1239451.1239536]
   Müller P, 2006, ACM T GRAPHIC, V25, P614, DOI 10.1145/1141911.1141931
   Musialski P, 2013, COMPUT GRAPH FORUM, V32, P146, DOI 10.1111/cgf.12077
   Nan LL, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778830
   Nash Charlie, 2020, ICML
   Nehab D, 2005, ACM T GRAPHIC, V24, P536, DOI 10.1145/1073204.1073226
   Nishida G, 2018, COMPUT GRAPH FORUM, V37, P415, DOI 10.1111/cgf.13372
   Paris A, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3342765
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Perche S., 2023, Computer Graphics Forum, V42, P14936
   Peters R, 2022, PHOTOGRAMM ENG REM S, V88, P165, DOI 10.14358/PERS.21-00032R2
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Sayed M, 2022, Arxiv, DOI arXiv:2208.14743
   Schwarz M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766956
   Shen T., 2021, Advances in Neural Information Processing Systems, P6087
   Skorokhodov I, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14124, DOI 10.1109/ICCV48922.2021.01388
   Smith E, 2018, ADV NEUR IN, V31
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Soltani AA, 2017, PROC CVPR IEEE, P2511, DOI 10.1109/CVPR.2017.269
   St'ava O, 2010, COMPUT GRAPH FORUM, V29, P665, DOI 10.1111/j.1467-8659.2009.01636.x
   Sun X, 2020, COMPUT AIDED GEOM D, V80, DOI 10.1016/j.cagd.2020.101862
   Taesung Park, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P319, DOI 10.1007/978-3-030-58545-7_19
   Teboul O, 2013, IEEE T PATTERN ANAL, V35, P1744, DOI 10.1109/TPAMI.2012.252
   Teller S., 1998, IMAGE UNDERSTANDING, P455
   van den Oord A, 2017, ADV NEUR IN, V30
   Wang JK, 2022, PR MACH LEARN RES
   Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4
   Wang TC, 2018, Arxiv, DOI arXiv:1808.06601
   Wangdi K, 2021, ASIA PAC POLICY STUD, V8, P176, DOI 10.1002/app5.315
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   Wen C, 2019, IEEE I CONF COMP VIS, P1042, DOI 10.1109/ICCV.2019.00113
   Wonka P, 2003, ACM T GRAPHIC, V22, P669, DOI 10.1145/882262.882324
   Wu FZ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601162
   Wu JJ, 2016, ADV NEUR IN, V29
   Wu J, 2017, ADV NEUR IN, V30
   Wu ZN, 2024, Arxiv, DOI arXiv:2401.17053
   Xiangli YB, 2022, LECT NOTES COMPUT SC, V13692, P106, DOI 10.1007/978-3-031-19824-3_7
   Xie HZ, 2024, Arxiv, DOI arXiv:2309.00610
   Zafeirouli K, 2019, INT ICE CONF ENG, DOI 10.1109/ice.2019.8792573
   Zhang J, 2022, GRAPH MODELS, V119, DOI 10.1016/j.gmod.2021.101122
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang YF, 2022, ISPRS J PHOTOGRAMM, V189, P143, DOI 10.1016/j.isprsjprs.2022.04.028
   Zhou Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201285
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
   Zhu JL, 2021, COMPUT GRAPH FORUM, V40, P193, DOI 10.1111/cgf.14413
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu LJ, 2018, LECT NOTES COMPUT SC, V11215, P640, DOI 10.1007/978-3-030-01252-6_38
NR 113
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 12
PY 2024
DI 10.1007/s00371-024-03528-7
EA JUL 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YR3A8
UT WOS:001270158900001
OA hybrid
DA 2024-08-05
ER

PT J
AU Ben Salah, K
   Othmani, M
   Fourati, J
   Kherallah, M
AF Ben Salah, Khawla
   Othmani, Mohamed
   Fourati, Jihen
   Kherallah, Monji
TI Advancing spatial mapping for satellite image road segmentation with
   multi-head attention
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Roads; Segmentation; Aerial image; Modified MAP-UNet; Masks
AB Remote sensing imaging is an interesting field, particularly in road areas. Road segmentation has become crucial in several areas, such as transportation network optimization, urban planning, and image analysis. We proposed in this study an upgraded mixed-scale UNet network (MAP-UNet) with a multi-head attention mechanism to identify and delineate road networks within aerial images. This upgraded model identifies and delineates road networks within aerial images. Modified MAP-UNet aims to enhance the efficiency of road segmentation through the integration of multi-scale features and attention mechanisms. We performed a comparison using the most recent methods. Our proposed approach achieves recall (76.18%), precision (80.30%), and IoU (63.00%) threshold overtime on the DeepGlobe dataset.
C1 [Ben Salah, Khawla] Natl Engn Sch, Comp Sci ATES Adv Technol Environm & Smart City, Sfax, Tunisia.
   [Othmani, Mohamed] Univ Gafsa, Comp Sci ATES Adv Technol Environm & Smart City, Gafsa, Tunisia.
   [Kherallah, Monji] Univ Sfax, Phys ATES Adv Technol Environm & Smart City, Sfax, Tunisia.
   [Fourati, Jihen] Natl Engn Sch, Comp Sci, Sfax, Tunisia.
C3 Universite de Tunis-El-Manar; Ecole Nationale d'Ingenieurs de Tunis
   (ENIT); Universite de Sfax; Universite de Gafsa; Universite de Sfax;
   Universite de Sfax; Universite de Tunis-El-Manar; Ecole Nationale
   d'Ingenieurs de Tunis (ENIT)
RP Ben Salah, K (corresponding author), Natl Engn Sch, Comp Sci ATES Adv Technol Environm & Smart City, Sfax, Tunisia.
EM khawlabensalah8@gmail.com; med.othmani@gmail.com;
   jihen.fourati@enis.u-sfax.tn; monji.kherallah@fss.usf.tn
CR Abdollahi A, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12091444
   Azooz HJ, 2024, LECT NOTES COMPUT SC, V14403, P366, DOI 10.1007/978-981-97-0376-0_28
   Bastani F, 2018, PROC CVPR IEEE, P4720, DOI 10.1109/CVPR.2018.00496
   Batra A, 2019, PROC CVPR IEEE, P10377, DOI 10.1109/CVPR.2019.01063
   Ben Salah K., 2022, INT C COMP COLL INT, P554
   Ben Salah K, 2022, VISUAL COMPUT, V38, P1833, DOI 10.1007/s00371-021-02108-3
   Bosurgi G, 2023, SUSTAINABILITY-BASEL, V15, DOI 10.3390/su15021334
   Chaurasia A, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Demir I, 2018, IEEE COMPUT SOC CONF, P172, DOI 10.1109/CVPRW.2018.00031
   Dosovitskiy A., 2016, Advances in Neural Information Processing Systems, P658
   Fourati Jihen, 2023, Advances in Computational Collective Intelligence: 15th International Conference, ICCCI 2023, Proceedings. Communications in Computer and Information Science (1864), P123, DOI 10.1007/978-3-031-41774-0_10
   Fourati J, 2022, SIGNAL IMAGE VIDEO P, V16, P2175, DOI 10.1007/s11760-022-02180-9
   Guennich A., 2022, INT C INT SYST DES A, P181
   Guo YM, 2018, INT J MULTIMED INF R, V7, P87, DOI 10.1007/s13735-017-0141-z
   He H, 2019, J APPL REMOTE SENS, V13, DOI 10.1117/1.JRS.13.034510
   Hyeonsoo Lee, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12261), P14, DOI 10.1007/978-3-030-59710-8_2
   Junjun Ma, 2020, Journal of Physics: Conference Series, V1544, DOI 10.1088/1742-6596/1544/1/012101
   Lu XY, 2021, ISPRS J PHOTOGRAMM, V175, P340, DOI 10.1016/j.isprsjprs.2021.03.008
   Malarvizhi K, 2016, PROC TECH, V24, P1835, DOI 10.1016/j.protcy.2016.05.231
   Marin D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6588, DOI 10.1109/ICCV48922.2021.00654
   Máttyus G, 2017, IEEE I CONF COMP VIS, P3458, DOI 10.1109/ICCV.2017.372
   Mattyus G, 2018, PROC CVPR IEEE, P8024, DOI 10.1109/CVPR.2018.00837
   Mosinska A, 2018, PROC CVPR IEEE, P3136, DOI 10.1109/CVPR.2018.00331
   Qi XQ, 2020, IEEE ACCESS, V8, P146627, DOI 10.1109/ACCESS.2020.3015587
   Salah K.B., 2023, 2023 INT C CYB CW SO, P46, DOI [10.1109/CW58918.2023.00017, DOI 10.1109/CW58918.2023.00017]
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Tang M, 2018, LECT NOTES COMPUT SC, V11220, P524, DOI 10.1007/978-3-030-01270-0_31
   Telli M., 2022, INT C INT SYST DES A, P250
   Telli M, 2023, MULTIMED TOOLS APPL, DOI 10.1007/s11042-023-17358-7
   Wei Y, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3061213
   Yang YF, 2021, IEEE ACCESS, V9, P114070, DOI 10.1109/ACCESS.2021.3104605
   Yu SY, 2021, AAAI CONF ARTIF INTE, V35, P3234
   Zhou MT, 2022, ISPRS J PHOTOGRAMM, V193, P234, DOI 10.1016/j.isprsjprs.2022.09.005
NR 33
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 14
PY 2024
DI 10.1007/s00371-024-03431-1
EA MAY 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QQ7N8
UT WOS:001222401900002
DA 2024-08-05
ER

PT J
AU Wang, L
   Tang, XS
   Hao, KR
AF Wang, Lei
   Tang, Xue-song
   Hao, Kuangrong
TI GFPE-ViT: vision transformer with geometric-fractal-based position
   encoding
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Vision transformers; Hilbert curve; Fractal geometric; Patch embedding;
   Image classification
AB In recent years, transformers have become a significant tool in computer vision, revolutionizing fundamental tasks. This paper focuses on the mapping strategy employed during patch embedding, explicitly addressing the transition from a two-dimensional (2D) to a one-dimensional (1D) representation. Existing methods typically divide images into tokens and perform sequential mapping in a top-to-bottom, left-to-right manner. However, these approaches suffer from a notable loss of local information due to the inherent discontinuity in the mapping process. To address these limitations, we propose a novel solution called Geometric-Fractals-based Position Encoding for Vision Transformer (GFPE-ViT). Our method capitalizes on the self-similarity and periodicity observed in fractal curves to enhance the proximity of local information. By integrating the fractalized order into the position encoding, considering the fractal periodicity, we effectively improve the simplicity and efficacy of the mapping strategy. Extensive experimental evaluations highlight the superiority of our approach over existing methods. GFPE-ViT achieves higher accuracy and faster convergence, underscoring its effectiveness in enhancing the mapping strategy for image patching tasks. Results show that GFPE-ViT performs 0.38% higher than the vanilla ViT in terms of top-1 accuracy on CIFAR-100 after pre-training with ImageNet-21k and achieves a 50% reduction in convergence time compared to vanilla ViT on small-scale datasets. By leveraging the self-similarity and periodicity in fractal curves, our proposed method opens new avenues for improving local information proximity in computer vision tasks.
C1 [Wang, Lei; Tang, Xue-song; Hao, Kuangrong] Donghua Univ, Coll Informat Sci & Technol, 2999 Renmin North Rd, Shanghai 201620, Peoples R China.
C3 Donghua University
RP Tang, XS (corresponding author), Donghua Univ, Coll Informat Sci & Technol, 2999 Renmin North Rd, Shanghai 201620, Peoples R China.
EM tangxs@dhu.edu.cn
CR Ai LM, 2024, VISUAL COMPUT, V40, P1453, DOI 10.1007/s00371-023-02860-8
   Alrayes N, 2021, SENS BIO-SENS RES, V31, DOI 10.1016/j.sbsr.2020.100395
   [Anonymous], 1890, Math. Ann.
   Belavadi P, 2022, LECT NOTES COMPUT SC, V13480, P270, DOI 10.1007/978-3-031-14463-9_18
   Brown T. B., 2020, P 34 INT C NEURAL IN, P1
   Cai G, 2023, VISUAL COMPUT, V39, P2781, DOI 10.1007/s00371-022-02492-4
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen M, 2020, PR MACH LEARN RES, V119
   Chen W, 2022, LECT NOTES COMPUT SC, V13662, P730, DOI 10.1007/978-3-031-20086-1_42
   Chu XX, 2021, Arxiv, DOI arXiv:2102.10882
   Corcoran T, 2018, Arxiv, DOI arXiv:1802.02532
   d'Ascoli S, 2021, PR MACH LEARN RES, V139, DOI 10.1088/1742-5468/ac9830
   Demirtas M., 2022, 2022 INN INT SYST AP, P1
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Gehring J, 2017, PR MACH LEARN RES, V70
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Hilbert D, 1935, STETIGE ABBILDUNG LI, P1
   Ho JAT, 2019, Arxiv, DOI arXiv:1912.12180
   Kaggle, About us
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Krizhevsky A, 2009, CIFAR-10 dataset
   Kurek J, 2018, B POL ACAD SCI-TECH, V66, DOI 10.24425/bpas.2018.125930
   Li YH, 2022, LECT NOTES COMPUT SC, V13669, P280, DOI 10.1007/978-3-031-20077-9_17
   Liang XC, 2023, VISUAL COMPUT, V39, P2277, DOI 10.1007/s00371-022-02413-5
   Liu YW, 2023, J MOL GRAPH MODEL, V118, DOI 10.1016/j.jmgm.2022.108344
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Loshchilov I, 2017, Sgdr: Stochastic gradient descent with warm restarts, DOI [10.48550/arXiv.1608.03983, DOI 10.48550/ARXIV.1608.03983]
   Loshchilov I, 2019, Arxiv, DOI [arXiv:1711.05101, 10.48550/arXiv.1711.05101, DOI 10.48550/ARXIV.1711.05101]
   Moreno J, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19061442
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Pan X., 2022, Adv. Neural. Inf. Process. Syst, V35, P22895
   Pan XR, 2022, PROC CVPR IEEE, P805, DOI 10.1109/CVPR52688.2022.00089
   Pan ZZ, 2022, AAAI CONF ARTIF INTE, P2035
   Paszke A, 2019, ADV NEUR IN, V32
   Peng ZH, 2022, J INTELL INF SYST, V58, P513, DOI 10.1007/s10844-021-00671-8
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren B, 2023, PROC CVPR IEEE, P20382, DOI 10.1109/CVPR52729.2023.01952
   Ridnik T, 2021, Arxiv, DOI [arXiv:2104.10972, 10.48550/arXiv.2104.10972, DOI 10.48550/ARXIV.2104.10972]
   Ronen T., 2023, IEEE C COMP VIS PATT, P4612
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Shahna K, 2020, APPL SOFT COMPUT, V90, DOI 10.1016/j.asoc.2020.106162
   Shaw P, 2018, Arxiv, DOI [arXiv:1803.02155, 10.48550/arXiv.1803.02155]
   Simoncelli EP, 2001, ANNU REV NEUROSCI, V24, P1193, DOI 10.1146/annurev.neuro.24.1.1193
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan MX, 2019, PR MACH LEARN RES, V97
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wang HY, 2021, PROC CVPR IEEE, P5459, DOI 10.1109/CVPR46437.2021.00542
   Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8
   Wang XY, 2023, CLUSTER COMPUT, V26, P2011, DOI 10.1007/s10586-022-03723-y
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xia ZF, 2022, PROC CVPR IEEE, P4784, DOI 10.1109/CVPR52688.2022.00475
   Xiao T, 2021, ADV NEUR IN, V34
   Xie J., 2022, SPIE, V12342, P78
   Yuan K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P559, DOI 10.1109/ICCV48922.2021.00062
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang HY, 2018, Arxiv, DOI arXiv:1710.09412
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang X., 2023, Vis. Comput., P1
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhu XZ, 2021, Arxiv, DOI [arXiv:2010.04159, 10.48550/arXiv.2010.04159]
NR 70
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 17
PY 2024
DI 10.1007/s00371-024-03381-8
EA APR 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NY9U8
UT WOS:001204140500005
DA 2024-08-05
ER

PT J
AU Geng, X
   Gao, JX
   Zhang, YH
   Wang, R
AF Geng, Xu
   Gao, Jinxiong
   Zhang, Yonghui
   Wang, Rong
TI A dual-branch feature fusion neural network for fish image fine-grained
   recognition
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Fine-grained visual categorization; Fish categorization; Feature fusion;
   Coarse to fine
ID CLASSIFICATION
AB The recognition of fish species holds significant importance in aquaculture and marine biology. However, it is a challenging problem due to the high similarity among intra-genus species. Existing recognition methods primarily seek prominent features of the species. However, we believe that the diverse levels of similarity between a species and other species can also function as implicit characteristics for that specific species. Based on this perspective, we propose a dual-branch fusion network for fine-grained fish species recognition utilizing inter-species similarity. This approach consists of a backbone network and two branches for coarse- and fine-grained recognition. In the coarse-grained branch, we designed a guidance matrix and species similarity labels to facilitate the generation of species similarity information. In the fine-grained branch, features from the backbone network are fused with similarity information to achieve precise recognition. Finally, fine-tuning the neural network through loss functions. We conduct experimental validation on three publicly available fish datasets, yielding excellent accuracy outcomes. Code is available at https://github.com/xingxing317/fish_classification.
C1 [Geng, Xu; Gao, Jinxiong; Zhang, Yonghui; Wang, Rong] Hainan Univ, Sch Informat & Commun Engn, 58 Renmin Ave, Haikou 570228, Peoples R China.
C3 Hainan University
RP Zhang, YH (corresponding author), Hainan Univ, Sch Informat & Commun Engn, 58 Renmin Ave, Haikou 570228, Peoples R China.
EM yhzhang@hainanu.edu.cn
FU Key Research and Development Project of Hainan Province
FX No Statement Available
CR Alsmadi MK, 2022, J KING SAUD UNIV-COM, V34, P1625, DOI 10.1016/j.jksuci.2020.07.005
   Boulais Oce<prime>ane, 2021, P FGVC8 8 WORKSH FIN, V25
   Branson S, 2014, Arxiv, DOI arXiv:1406.2952
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Chen ZH, 2023, IEEE T PATTERN ANAL, V45, P13489, DOI 10.1109/TPAMI.2023.3293885
   Deep BV, 2019, 2019 6TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND INTEGRATED NETWORKS (SPIN), P665, DOI [10.1109/SPIN.2019.8711657, 10.1109/spin.2019.8711657]
   Deng WJ, 2022, IEEE T IMAGE PROCESS, V31, P4186, DOI 10.1109/TIP.2022.3181492
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Gao JX, 2024, EXPERT SYST APPL, V237, DOI 10.1016/j.eswa.2023.121688
   Ge WF, 2019, PROC CVPR IEEE, P3029, DOI 10.1109/CVPR.2019.00315
   Ge Z., 2016, P IEEE WINT C APPL C, P1, DOI [10.1109/WACV.2016.7477700, DOI 10.1109/WACV.2016.7477700]
   Guo ZH, 2023, IEEE T PATTERN ANAL, V45, P12960, DOI 10.1109/TPAMI.2022.3207091
   Han JW, 2022, IEEE T PATTERN ANAL, V44, P579, DOI 10.1109/TPAMI.2019.2933510
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jemal I., 2020, P 2020 IEEE ACS 17 I, P1
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Knausgård KM, 2022, APPL INTELL, V52, P6988, DOI 10.1007/s10489-020-02154-9
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Li J, 2023, REV AQUACULT, V15, P409, DOI 10.1111/raq.12726
   Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu CB, 2020, AAAI CONF ARTIF INTE, V34, P11555
   Liu M, 2022, IEEE T IMAGE PROCESS, V31, P748, DOI 10.1109/TIP.2021.3135477
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Si GZ, 2023, FRONT MAR SCI, V10, DOI 10.3389/fmars.2023.1174347
   Siddiqui SA, 2018, ICES J MAR SCI, V75, P374, DOI 10.1093/icesjms/fsx109
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914
   Wang J, 2022, Arxiv, DOI arXiv:2107.02341
   Wang YM, 2018, PROC CVPR IEEE, P4148, DOI 10.1109/CVPR.2018.00436
   Wang Y, 2023, APPL INTELL, V53, P19115, DOI 10.1007/s10489-023-04465-z
   Wei XS, 2022, IEEE T PATTERN ANAL, V44, P8927, DOI 10.1109/TPAMI.2021.3126648
   Wei XS, 2018, PATTERN RECOGN, V76, P704, DOI 10.1016/j.patcog.2017.10.002
   Wen JY, 2024, IEEE T MULTIMEDIA, V26, P944, DOI 10.1109/TMM.2023.3273924
   Yan YC, 2020, NEUROCOMPUTING, V396, P254, DOI 10.1016/j.neucom.2018.07.100
   Yang Z, 2018, LECT NOTES COMPUT SC, V11218, P438, DOI 10.1007/978-3-030-01264-9_26
   Yu CJ, 2018, LECT NOTES COMPUT SC, V11220, P595, DOI 10.1007/978-3-030-01270-0_35
   Zhang JJ, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2036
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zheng HL, 2019, PROC CVPR IEEE, P5007, DOI 10.1109/CVPR.2019.00515
   Zheng HL, 2020, IEEE T IMAGE PROCESS, V29, P476, DOI 10.1109/TIP.2019.2921876
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhihui Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9746, DOI 10.1109/CVPR42600.2020.00977
   Zhuang PQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1301, DOI 10.1145/3240508.3240616
NR 49
TC 0
Z9 0
U1 6
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 10
PY 2024
DI 10.1007/s00371-024-03366-7
EA APR 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NK5U4
UT WOS:001200368700001
DA 2024-08-05
ER

PT J
AU Li, Z
   Yan, KX
   Zhou, DM
   Wang, CC
   Quan, JR
AF Li, Zhen
   Yan, Kaixiang
   Zhou, Dongming
   Wang, Changcheng
   Quan, Jiarui
TI A novel highland and freshwater-circumstance dataset: advancing
   underwater image enhancement
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Underwater image enhancement; Unsupervised; Inland lakes
AB As an important underlying visual processing task, underwater image enhancement techniques have received a lot of attention from researchers due to their importance in marine engineering and lake ecosystem optimization. However, various underwater image enhancement algorithms have been proposed to be evaluated mainly with marine water body datasets, and it is not clear whether these algorithms can be performed on datasets collected from inland lakes in the freshwaters. To bridge this gap, for the first time, we construct an underwater image dataset for highland and freshwater-circumstances (HFUI) using 1000 real images to complement the underwater image datasets. In addition, we propose an unsupervised underwater image enhancement algorithm (HUFI-Net) specifically for this dataset to correct the sharpness and color of the images. This algorithm, as well as current advanced underwater image enhancement algorithms, was investigated qualitatively and quantitatively using this dataset to evaluate the effectiveness and limitations of various algorithms and to provide novel ideas for future underwater image enhancement research. We also further validate the generalization and effectiveness of the algorithm on the underwater image datasets of the UIEB and the RUIE.
C1 [Li, Zhen; Yan, Kaixiang; Zhou, Dongming; Wang, Changcheng; Quan, Jiarui] Yunnan Univ, Sch Informat Sci & Engn, Kunming 650000, Yunnan, Peoples R China.
C3 Yunnan University
RP Zhou, DM (corresponding author), Yunnan Univ, Sch Informat Sci & Engn, Kunming 650000, Yunnan, Peoples R China.
EM lizhen@mail.ynu.edu.cn; yankx@mail.ynu.edu.cn; zhoudm@ynu.edu.cn;
   changcheng@mail.ynu.edu.cn; yunquanjr@mail.ynu.edu.cn
FU National Natural Science Foundation of China under Grants
FX No Statement Available
CR Carlevaris-Bianco N., OCEANS 2010 MTSIEEE, P1
   Chang L, 2023, ISPRS J PHOTOGRAMM, V196, P415, DOI 10.1016/j.isprsjprs.2023.01.007
   Chao L., 2010 2 INT C COMPUTE, V2, P2
   Chen XL, 2021, Arxiv, DOI arXiv:2101.00991
   Cheng L., 2023, IEEE Access
   Choi S., 2023, IEEE Access
   Drews P, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P825, DOI 10.1109/ICCVW.2013.113
   Duarte A., OCEANS 2016, P1
   Fu ZQ, 2022, LECT NOTES COMPUT SC, V13678, P465, DOI 10.1007/978-3-031-19797-0_27
   Fu ZQ, 2022, SIGNAL PROCESS-IMAGE, V102, DOI 10.1016/j.image.2021.116622
   Han J., 2021 IEEE INT GEOSCI, P2385
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hou WL, 2012, APPL OPTICS, V51, P2678, DOI 10.1364/AO.51.002678
   Huang DM, 2018, LECT NOTES COMPUT SC, V10704, P453, DOI 10.1007/978-3-319-73603-7_37
   HUMMEL R, 1977, COMPUT VISION GRAPH, V6, P184, DOI 10.1016/S0146-664X(77)80011-7
   Iqbal Kashif, 2007, IAENG International Journal of Computer Science, V34, P239
   Islam MJ, 2020, IEEE ROBOT AUTOM LET, V5, P3227, DOI 10.1109/LRA.2020.2974710
   Itasaka T., 2023, IEEE Access
   Jian MW, 2017, IEEE INT CON MULTI, P1297, DOI 10.1109/ICME.2017.8019324
   Kingma D. P., 2014, arXiv
   Lai Y, 2023, VISUAL COMPUT, V39, P4133, DOI 10.1007/s00371-022-02580-5
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li CY, 2017, PATTERN RECOGN LETT, V94, P62, DOI 10.1016/j.patrec.2017.05.023
   Li Z, 2023, IEEE ACCESS, V11, P16042, DOI 10.1109/ACCESS.2023.3245150
   Lin RJ, 2022, VISUAL COMPUT, V38, P4419, DOI 10.1007/s00371-021-02305-0
   Liu RS, 2019, Arxiv, DOI arXiv:1901.05320
   Liu RS, 2020, IEEE T CIRC SYST VID, V30, P4861, DOI 10.1109/TCSVT.2019.2963772
   LIU YC, 1995, IEEE T CONSUM ELECTR, V41, P460, DOI 10.1109/30.468045
   Naik A, 2021, AAAI CONF ARTIF INTE, V35, P15853
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Peng YT, 2017, IEEE T IMAGE PROCESS, V26, P1579, DOI 10.1109/TIP.2017.2663846
   Pizer S. M., 1990, Proceedings of the First Conference on Visualization in Biomedical Computing (Cat. No.90TH0311-1), P337, DOI 10.1109/VBC.1990.109340
   Qiao NZ, 2023, VISUAL COMPUT, V39, P3029, DOI 10.1007/s00371-022-02510-5
   Qiu R, 2023, VISUAL COMPUT, V39, P2933, DOI 10.1007/s00371-022-02501-6
   Sharma P., 2021, PREPRINT
   Shen X., 2023, Vis. Comput., P1
   Song W, 2018, LECT NOTES COMPUT SC, V11164, P678, DOI 10.1007/978-3-030-00776-8_62
   Speagle J.S., arXiv
   Van de Weijer J, 2007, IEEE T IMAGE PROCESS, V16, P2207, DOI 10.1109/TIP.2007.901808
   Wen JJ, 2023, Arxiv, DOI arXiv:2302.08269
   Yae S, 2023, IEEE ACCESS, V11, P29175, DOI 10.1109/ACCESS.2023.3243173
   Yang M, 2015, IEEE T IMAGE PROCESS, V24, P6062, DOI 10.1109/TIP.2015.2491020
   Zhang X, 2023, APPL INTELL, V53, P3255, DOI 10.1007/s10489-022-03628-8
   Zhang Z., 2023, Vis. Comput., P1
   Zhang ZW, 2023, Arxiv, DOI arXiv:2303.06543
   Zhuang PQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1301, DOI 10.1145/3240508.3240616
NR 46
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 1
PY 2024
DI 10.1007/s00371-024-03285-7
EA APR 2024
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MP7C0
UT WOS:001194882700001
DA 2024-08-05
ER

PT J
AU Qin, XJ
   Li, XY
   Li, MJ
   Zheng, HB
   Xu, XG
AF Qin, Xujia
   Li, Xinyu
   Li, Mengjia
   Zheng, Hongbo
   Xu, Xiaogang
TI Self-supervised single-image 3D face reconstruction method based on
   attention mechanism and attribute refinement
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE 3D face reconstruction; Attention mechanism; Self-supervised; Attribute
   refinement; Deep learning
ID SHAPE
AB Single-view 3D face reconstruction refers to recovering 3D information of a face, such as shape and texture, from a single image. With the wide application of deep learning in the image field, there have been a number of researches using this method to learn the 3D shape and texture of a face from image information. In this paper, we propose a self-supervised single-image 3D face reconstruction method based on the attention mechanism and attribute refinement, which incorporates the attention mechanism in the network structural model, allowing feature extraction to fuse the information of the channel domain and the spatial domain to enhance the feature extraction capability. Joint 2D image-level supervision and supervision between 3D attributes can better learn the 3D model of the face. In this paper, on the basis of using the traditional 2D image supervision, we design a variety of loss functions by combining the cyclic consistency, interpolation consistency, and landmark consistency to realize the 3D attribute level supervision. In order to strengthen the ability to characterize the details of the face, this paper proposes an attribute refinement network to enhance the ability of the model to reconstruct the details and make the reconstruction results more realistic. Based on the symmetry of the face, this paper constructs a deep learning network model to decouple the 3D information directly from the image, and finally realizes unsupervised 3D face reconstruction from a single image.
C1 [Qin, Xujia; Li, Xinyu; Li, Mengjia; Zheng, Hongbo] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
   [Xu, Xiaogang] Zhejiang Lab, Inst Artificial Intelligence, Hangzhou, Peoples R China.
   [Xu, Xiaogang] Zhejiang Gongshang Univ, Coll Comp & Informat Engn, Hangzhou, Peoples R China.
C3 Zhejiang University of Technology; Zhejiang Laboratory; Zhejiang
   Gongshang University
RP Qin, XJ; Zheng, HB (corresponding author), Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
EM qxj@zjut.edu.cn; zhb@zjut.edu.cn
FU National Natural Science Foundation of China [61702455, 61672462];
   National Natural Science Foundation of China [LY20F020025]; Natural
   Science Foundation of Zhejiang province, China
FX This work was supported in part by the National Natural Science
   Foundation of China (Grant No. 61702455, 61672462) and the Natural
   Science Foundation of Zhejiang province, China (Grant No. LY20F020025)
CR Bengio J., 2009, Proceedings of the 26th Annual International Conference on Machine Learning, P41
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Booth J, 2018, INT J COMPUT VISION, V126, P233, DOI 10.1007/s11263-017-1009-7
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Cao ZW, 2022, LECT NOTES COMPUT SC, V13672, P737, DOI 10.1007/978-3-031-19775-8_43
   Cao ZW, 2021, IEEE WINT CONF APPL, P1187, DOI 10.1109/WACV48630.2021.00123
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   Deng ZR, 2023, VISUAL COMPUT, V39, P5547, DOI 10.1007/s00371-022-02679-9
   Faugeras O., 2001, The Geometry of Multiple Images, DOI [10.7551/mitpress/3259.001.0001, DOI 10.7551/MITPRESS/3259.001.0001]
   Feng Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459936
   Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gecer B, 2022, IEEE T PATTERN ANAL, V44, P4879, DOI 10.1109/TPAMI.2021.3084524
   Gecer B, 2019, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2019.00125
   Guo YD, 2021, IEEE T IMAGE PROCESS, V30, P3815, DOI 10.1109/TIP.2021.3065798
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu T, 2021, PROC CVPR IEEE, P5998, DOI 10.1109/CVPR46437.2021.00594
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Jackson AS, 2017, IEEE I CONF COMP VIS, P1031, DOI 10.1109/ICCV.2017.117
   Jaderberg M, 2015, ADV NEUR IN, V28
   Ju YJ, 2022, IEEE WINT CONF APPL, P1173, DOI 10.1109/WACV51458.2022.00124
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kato H, 2018, PROC CVPR IEEE, P3907, DOI 10.1109/CVPR.2018.00411
   Kemelmacher-Shlizerman I, 2011, IEEE T PATTERN ANAL, V33, P394, DOI 10.1109/TPAMI.2010.63
   Kulkarni N, 2019, IEEE I CONF COMP VIS, P2202, DOI 10.1109/ICCV.2019.00229
   Lattas A, 2020, PROC CVPR IEEE, P757, DOI 10.1109/CVPR42600.2020.00084
   Li CL, 2023, PROC CVPR IEEE, P372, DOI 10.1109/CVPR52729.2023.00044
   Liu DF, 2021, AAAI CONF ARTIF INTE, V35, P6101
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Martinez A. M., 1998, THE AR FACE DATABASE
   MUKHERJEE DP, 1995, PHILOS T R SOC A, V351, P77, DOI 10.1098/rsta.1995.0026
   Navaneet KL, 2020, PROC CVPR IEEE, P1129, DOI 10.1109/CVPR42600.2020.00121
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Richardson E, 2017, PROC CVPR IEEE, P5553, DOI 10.1109/CVPR.2017.589
   Salvi A, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9206776
   Tewari A, 2019, PROC CVPR IEEE, P10804, DOI 10.1109/CVPR.2019.01107
   Tewari A, 2017, IEEE I CONF COMP VIS, P3735, DOI 10.1109/ICCV.2017.401
   Tran L, 2021, IEEE T PATTERN ANAL, V43, P157, DOI 10.1109/TPAMI.2019.2927975
   Tu XG, 2021, IEEE T MULTIMEDIA, V23, P1160, DOI 10.1109/TMM.2020.2993962
   Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu SZ, 2020, PROC CVPR IEEE, P1, DOI 10.1109/CVPR42600.2020.00008
   Yin X, 2017, IEEE I CONF COMP VIS, P4010, DOI 10.1109/ICCV.2017.430
   Zeng XX, 2019, IEEE I CONF COMP VIS, P2315, DOI 10.1109/ICCV.2019.00240
   Zhang Zhenyu, 2021, P IEEECVF C COMPUTER, P14214
   Zhou YX, 2019, PROC CVPR IEEE, P1097, DOI 10.1109/CVPR.2019.00119
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
   Zhu XZ, 2019, IEEE I CONF COMP VIS, P6687, DOI 10.1109/ICCV.2019.00679
   Zou HY, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10202539
NR 53
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 22
PY 2024
DI 10.1007/s00371-024-03319-0
EA MAR 2024
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LX7Z6
UT WOS:001190189400002
DA 2024-08-05
ER

PT J
AU Khouya, YA
   Oussous, MA
   Jakimi, A
   Ghorbel, F
AF Khouya, Youssef Ait
   Oussous, Mohammed Ait
   Jakimi, Abdeslam
   Ghorbel, Faouzi
TI Stable and invertible invariants description for gray-level images based
   on Radon transform
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Completeness; Stable; Invertible; Radon transform; Analytical
   Fourier-Mellin transform
ID FOURIER DESCRIPTORS; COMPLETE SET; RECOGNITION; MOMENTS; RECONSTRUCTION;
   SHAPES; AFFINE
AB In a large number of applications, different types of descriptors have been implemented to identify and recognize textured objects in grayscale images. Their classification must be carried out independently of their position, orientation and scale. The property of completeness of descriptors which guarantees their uniqueness for a given shape is also a sought-after property. It is known in the literature that such a property is difficult to obtain. It was possible to achieve it in rare cases for planar curves using for instance Fourier descriptors or the curvature. In grayscale images, we know at least three cases of complete descriptors: Those based on Zernike moments, those computed from the analytical Fourier-Mellin transform or obtained from the complex moments. To our current knowledge, in the case of curved surfaces and 3D volume images, there are yet no complete descriptors invariant to the 3D rigid motions. The property of invertibility of invariants, introduced recently and which implies completeness, allows the reconstruction of the object shape up to a similarity. The two sets of descriptors that we propose here verify on the one hand the invariance and the invertibility and on the other hand, the notion of stability which was introduced to guarantee the fact that the descriptors vary slightly during small variations in the shape. Their construction based on the Radon transform allows a certain robustness with respect to noise. In this article, we rigorously demonstrate the properties of invariance, invertibility and convergence for the two sets of proposed invariants. To evaluate the stability and robustness with respect to noise, experimental studies are carried out on different well-known datasets, namely Kimia 99 and MPEG7. We introduced our own face dataset, which we named FSTEF for further evaluation. On the other hand, several types and levels of noise were added to test the robustness to noise. Therefore, the effectiveness of the suggested sets of invariants are demonstrated by the different studies proposed in the present work.
C1 [Khouya, Youssef Ait; Oussous, Mohammed Ait; Jakimi, Abdeslam] Moulay Ismail Univ, Fac Sci & Tech, Dept Comp Sci, Software Engn & Informat Syst Engn Team, BP 509, Errachidia 52000, Morocco.
   [Ghorbel, Faouzi] Natl Sch Comp Sci, CRISTAL Lab, GRIFT Res Grp, Campus Univ 2010, Manouba 2011, Tunisia.
C3 Moulay Ismail University of Meknes; Universite de la Manouba
RP Khouya, YA (corresponding author), Moulay Ismail Univ, Fac Sci & Tech, Dept Comp Sci, Software Engn & Informat Syst Engn Team, BP 509, Errachidia 52000, Morocco.
EM youssefaitkhouya@gmail.com; aitoussous.med@gmail.com; ajakimi@yahoo.fr;
   faouzi.ghorbel@gmail.com
OI Abdeslam, Jakimi/0000-0003-1425-3474; Ghorbel,
   Faouzi/0000-0002-6364-1089
CR Abbasi S, 1999, LECT NOTES COMPUT SC, V1682, P435
   Aftab S, 2022, MULTIMED TOOLS APPL, V81, P42325, DOI 10.1007/s11042-022-13536-1
   Alalwan N, 2021, ALEX ENG J, V60, P1231, DOI 10.1016/j.aej.2020.10.046
   [Anonymous], 2016, Revised Selected Papers
   Bayoudh K, 2022, VISUAL COMPUT, V38, P2939, DOI 10.1007/s00371-021-02166-7
   Ben Gamra M, 2021, IMAGE VISION COMPUT, V114, DOI 10.1016/j.imavis.2021.104282
   Benkhlifa A., 2019, A normalized generalized curvature scale space for 2D contour representation, DOI [10.1007/978-3-030-19816-9_13, DOI 10.1007/978-3-030-19816-9_13]
   BenKhlifa A, 2019, SIGNAL PROCESS-IMAGE, V75, P32, DOI 10.1016/j.image.2019.03.009
   Brosch T, 2013, LECT NOTES COMPUT SC, V8150, P633, DOI 10.1007/978-3-642-40763-5_78
   Bryner D, 2014, IEEE T PATTERN ANAL, V36, P998, DOI 10.1109/TPAMI.2013.199
   Bryner D, 2012, PROC CVPR IEEE, P390, DOI 10.1109/CVPR.2012.6247700
   Cecotti H, 2020, INT J MACH LEARN CYB, V11, P1839, DOI 10.1007/s13042-020-01075-w
   Chong CW, 2003, INT J PATTERN RECOGN, V17, P1011, DOI 10.1142/S0218001403002769
   Chong CW, 2004, PATTERN RECOGN, V37, P119, DOI 10.1016/j.patcog.2003.06.003
   CRIMMINS TR, 1982, IEEE T SYST MAN CYB, V12, P848, DOI 10.1109/TSMC.1982.4308918
   Deans S.R., 2007, The Radon Transform and Some of Its Applications
   Derrode S, 2001, COMPUT VIS IMAGE UND, V83, P57, DOI 10.1006/cviu.2001.0922
   Dong B, 2016, I C COMM SOFTW NET, P581, DOI 10.1109/ICCSN.2016.7586590
   Elghoul S, 2021, SIGNAL PROCESS-IMAGE, V90, DOI 10.1016/j.image.2020.116058
   Galigekere RR, 2000, OPT ENG, V39, P1088, DOI 10.1117/1.602471
   Ghorbel E, 2022, IEEE T IMAGE PROCESS, V31, P5788, DOI 10.1109/TIP.2022.3199105
   Ghorbel F, 2006, PATTERN RECOGN LETT, V27, P1361, DOI 10.1016/j.patrec.2006.01.001
   Ghorbel F, 1998, ANN TELECOMMUN, V53, P242
   GHORBEL F, 1994, PATTERN RECOGN LETT, V15, P1043, DOI 10.1016/0167-8655(94)90037-X
   Gupta S, 2021, VISUAL COMPUT, V37, P447, DOI 10.1007/s00371-020-01814-8
   Hoang TV, 2012, PATTERN RECOGN, V45, P271, DOI 10.1016/j.patcog.2011.06.020
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692
   Huang ZH, 1996, IEEE T IMAGE PROCESS, V5, P1473, DOI 10.1109/83.536895
   Jafari-Khouzani K, 2005, IEEE T IMAGE PROCESS, V14, P783, DOI 10.1109/TIP.2005.847302
   Jafari-Khouzani K, 2005, IEEE T PATTERN ANAL, V27, P1004, DOI 10.1109/TPAMI.2005.126
   Jemal MA, 2024, MULTIMED TOOLS APPL, V83, P25901, DOI 10.1007/s11042-023-16482-8
   Kaur P, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3331167
   Khouya Youssef Ait, 2019, International Journal of Advanced Intelligence Paradigms, V14, P140
   Kuang LW, 2020, IEEE T CLOUD COMPUT, V8, P363, DOI 10.1109/TCC.2015.2511766
   Lao H, 2022, IET IMAGE PROCESS, V16, P3948, DOI 10.1049/ipr2.12605
   Latecki LJ, 2000, PROC CVPR IEEE, P424, DOI 10.1109/CVPR.2000.855850
   Li D, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2021.3137558
   Li H, 2015, PATTERN RECOGN, V48, P3895, DOI 10.1016/j.patcog.2015.06.002
   Li PX, 2018, PATTERN RECOGN, V76, P323, DOI 10.1016/j.patcog.2017.11.007
   Li ST, 2019, IEEE T GEOSCI REMOTE, V57, P6690, DOI 10.1109/TGRS.2019.2907932
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mokhtarian F., 2013, Computational Imaging and Vision
   Mokhtarian F., 1996, BRIT MACH VIS C
   Mukundan R, 2001, IEEE T IMAGE PROCESS, V10, P1357, DOI 10.1109/83.941859
   Otter DW, 2021, IEEE T NEUR NET LEAR, V32, P604, DOI 10.1109/TNNLS.2020.2979670
   Qian XL, 2023, IEEE T MULTIMEDIA, V25, P1810, DOI 10.1109/TMM.2022.3167805
   Scheidegger F, 2021, VISUAL COMPUT, V37, P1593, DOI 10.1007/s00371-020-01922-5
   Sebastian TB, 2004, IEEE T PATTERN ANAL, V26, P550, DOI 10.1109/TPAMI.2004.1273924
   SHENG YL, 1994, J OPT SOC AM A, V11, P1748, DOI 10.1364/JOSAA.11.001748
   Smach F, 2008, J MATH IMAGING VIS, V30, P43, DOI 10.1007/s10851-007-0036-3
   Tabbone S, 2006, COMPUT VIS IMAGE UND, V102, P42, DOI 10.1016/j.cviu.2005.06.005
   TEAGUE MR, 1980, J OPT SOC AM, V70, P920, DOI 10.1364/JOSA.70.000920
   TEH CH, 1988, IEEE T PATTERN ANAL, V10, P496, DOI 10.1109/34.3913
   Tigistu T, 2021, MULTIMED TOOLS APPL, V80, P36143, DOI 10.1007/s11042-021-11397-8
   Villamizar M, 2019, VISUAL COMPUT, V35, P349, DOI 10.1007/s00371-018-01617-y
   Wang B, 2016, IEEE T IMAGE PROCESS, V25, P5635, DOI 10.1109/TIP.2016.2609816
   Wang JJ, 2018, J MANUF SYST, V48, P144, DOI 10.1016/j.jmsy.2018.01.003
   Wang X, 2007, PATTERN RECOGN, V40, P3503, DOI 10.1016/j.patcog.2007.04.020
   Wu XW, 2020, NEUROCOMPUTING, V396, P39, DOI 10.1016/j.neucom.2020.01.085
   Xiao B, 2015, PATTERN RECOGN, V48, P2772, DOI 10.1016/j.patcog.2015.04.007
   Xu D, 2008, PATTERN RECOGN, V41, P240, DOI 10.1016/j.patcog.2007.05.001
   Yang CZ, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107809
   Yang JW, 2020, IEEE T IMAGE PROCESS, V29, P4114, DOI 10.1109/TIP.2020.2967578
   Zhang DS, 2002, SIGNAL PROCESS-IMAGE, V17, P825, DOI 10.1016/S0923-5965(02)00084-X
   Zhang H, 2010, IMAGE VISION COMPUT, V28, P38, DOI 10.1016/j.imavis.2009.04.004
   Zhang QC, 2018, INFORM FUSION, V42, P146, DOI 10.1016/j.inffus.2017.10.006
   Zhang ZW, 2022, IEEE T KNOWL DATA EN, V34, P249, DOI 10.1109/TKDE.2020.2981333
   Zhang ZX, 2018, Arxiv, DOI arXiv:1705.10874
NR 68
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 19
PY 2024
DI 10.1007/s00371-024-03311-8
EA MAR 2024
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LN2Z4
UT WOS:001187430700002
DA 2024-08-05
ER

PT J
AU Xu, HF
   Tang, YK
   He, JZ
   Zhang, ZQ
AF Xu, Hongfeng
   Tang, Yueke
   He, Jiezhou
   Zhang, Zhongqiong
TI AMSFANet: attention-based multiscale small face aware restoration method
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Generative adversarial network; Small face super-resolution; Attention
   mechanism; Multiscale features
ID SUPERRESOLUTION; NETWORK
AB Deep learning has achieved remarkable performance in various fields, including face recognition. However, recognizing small-sized face images remains a challenge in this domain. The limited number of pixels in small face images makes it difficult to extract facial features, leading to decreased accuracy of face recognition systems. Furthermore, small face images often suffer from low resolution and poor image quality, which further complicates the recognition process. To address this issue, this paper proposes a novel method for low-resolution face restoration by transforming it into a mapping problem from low-resolution small face images to high-resolution face images. We introduce an attention-based multiscale small face aware network (AMSFANet) for low-resolution face restoration. The proposed method is based on a super-resolution generative adversarial network (SRGAN) with improved loss constraints using the wasserstein distance and gradient penalty strategy to enhance the model's robustness during training. We also propose an attention-based multiscale residual module to replace the traditional residual structure, which strengthens the generator's focus on faces, reduces the impact of complex backgrounds on face restoration, and improves the final image's facial clarity, making it effective for subsequent face recognition. Experimental results demonstrate that the proposed method effectively improves the quality of low-resolution face images and enhances subsequent face recognition accuracy.
C1 [Xu, Hongfeng; Tang, Yueke] Guizhou Normal Univ, Sch Econ & Management, Guiyang 550025, Guizhou, Peoples R China.
   [Xu, Hongfeng] Xiamen Univ, Sch Informat, Xiamen 361005, Peoples R China.
   [He, Jiezhou] Xiamen Univ, Inst Artificial Intelligence, Xiamen 361005, Peoples R China.
   [Zhang, Zhongqiong] Anshun Univ, Sch Math & Comp Sci, Anshun 561000, Guizhou, Peoples R China.
C3 Guizhou Normal University; Xiamen University; Xiamen University; Anshun
   University
RP Xu, HF (corresponding author), Guizhou Normal Univ, Sch Econ & Management, Guiyang 550025, Guizhou, Peoples R China.; Xu, HF (corresponding author), Xiamen Univ, Sch Informat, Xiamen 361005, Peoples R China.
EM homny@sina.com
RI Xu, HF/KOD-6035-2024
OI he, jiezhou/0000-0002-3428-3665; Xu, Hongfeng/0000-0002-7635-8485
FU National Natural Science Foundation of China
FX No Statement Available
CR Aakerberg A, 2022, IET IMAGE PROCESS, V16, P442, DOI 10.1049/ipr2.12359
   Akyol A, 2012, PATTERN RECOGN, V45, P4103, DOI 10.1016/j.patcog.2012.05.018
   Anwarul S, 2020, LECT NOTES ELECTR EN, V597, P495, DOI 10.1007/978-3-030-29407-6_36
   Arjovsky M., 2017, arXiv, DOI DOI 10.48550/ARXIV.1701.04862
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Avci D, 2023, BIOCYBERN BIOMED ENG, V43, P58, DOI 10.1016/j.bbe.2022.12.001
   Baker S., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P83, DOI 10.1109/AFGR.2000.840616
   Bashir SMA, 2021, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.621
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng JK, 2020, PROC CVPR IEEE, P5202, DOI 10.1109/CVPR42600.2020.00525
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Gao GW, 2023, IEEE T IMAGE PROCESS, V32, P1978, DOI 10.1109/TIP.2023.3261747
   Gulrajani I., 2017, Adv. Neural Inf. Process. Syst, V30, P5
   Guo GD, 2019, COMPUT VIS IMAGE UND, V189, DOI 10.1016/j.cviu.2019.102805
   Hensel M, 2017, ADV NEUR IN, V30
   Huang H, 2010, PATTERN RECOGN, V43, P2532, DOI 10.1016/j.patcog.2010.02.007
   Jiang JJ, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3485132
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li LY, 2021, VISUAL COMPUT, V37, P2855, DOI 10.1007/s00371-021-02236-w
   Liu JP, 2021, IEEE T CYBERNETICS, V51, P839, DOI 10.1109/TCYB.2020.2977537
   Liu Li, 2008, 2008 9th International Conference on Signal Processing (ICSP 2008), P1153, DOI 10.1109/ICOSP.2008.4697334
   Liu ZS, 2021, IEEE T IMAGE PROCESS, V30, P4157, DOI 10.1109/TIP.2021.3069554
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lu X., 2023, VIS COMP, P1
   Lv X, 2022, NEUROCOMPUTING, V489, P98, DOI 10.1016/j.neucom.2022.02.042
   Malakshan SR, 2023, IEEE ACCESS, V11, P11238, DOI 10.1109/ACCESS.2023.3241606
   Nowozin S, 2016, ADV NEUR IN, V29
   Rakotonirina NC, 2020, INT CONF ACOUST SPEE, P3637, DOI 10.1109/ICASSP40776.2020.9054071
   Tang YG, 2022, PATTERN RECOGN LETT, V163, P32, DOI 10.1016/j.patrec.2022.09.012
   Vb SK., 2020, Intell. Syst. Comput. Technol, V37, P3
   Wang CJ, 2018, INT C PATT RECOG, P1554, DOI 10.1109/ICPR.2018.8545814
   Wang CY, 2023, PROC CVPR IEEE, P22356, DOI 10.1109/CVPR52729.2023.02141
   Wang F, 2023, VISUAL COMPUT, V39, P5069, DOI 10.1007/s00371-022-02646-4
   Wang ZF, 2014, VISUAL COMPUT, V30, P359, DOI 10.1007/s00371-013-0861-x
   Xiu J., 2022, Vis. Comput, V30, P1
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Yang X, 2022, VISUAL COMPUT, V38, P4307, DOI 10.1007/s00371-021-02297-x
   You CY, 2020, IEEE T MED IMAGING, V39, P188, DOI 10.1109/TMI.2019.2922960
   Zeng K., 2023, ICASSP 2023 2023 IEE, P1
   Zhang WL, 2019, IEEE I CONF COMP VIS, P3096, DOI 10.1109/ICCV.2019.00319
NR 42
TC 0
Z9 0
U1 7
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 28
PY 2024
DI 10.1007/s00371-024-03302-9
EA FEB 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JF9E4
UT WOS:001171862300002
DA 2024-08-05
ER

PT J
AU Wen, JY
   Zhuang, YS
   Deng, JY
AF Wen, Jiayan
   Zhuang, Yuansheng
   Deng, Junyi
TI EDM: a enhanced diffusion models for image restoration in complex scenes
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Diffusion model; Image restoration; Self-attention; Complex scenes;
   Residual module
AB At presently, diffusion model has achieved state-of-the-art performance by modeling the image synthesis process through a series of denoising network applications. Image restoration (IR) is to improve the subjective image quality corrupted by various kinds of degradation unlike image synthesis. However, IR for complex scenes such as worksite images is greatly challenging in the low-level vision field due to complicated environmental factors. To solve this problem, we propose a enhanced diffusion models for image restoration in complex scenes (EDM). It improves the authenticity and representation ability for the generation process, while effectively handles complex backgrounds and diverse object types. EDM has three main contributions: (1) Its framework adopts a Mish-based residual module, which enhances the ability to learn complex patterns of images, and allows for the presence of negative gradients to reduce overfitting risks during model training. (2) It employs a mixed-head self-attention mechanism, which augments the correlation among input elements at each time step, and maintains a better balance between capturing the global structural information and local detailed textures of the image. (3) This study evaluates EDM on a self-built dataset specifically tailored for worksite image restoration, named "Workplace," and was compared with results from another two public datasets named Places2 and Rain100H. Furthermore, the achievement of experiments on these datasets not only demonstrates EDM's application value in a specific domain, but also its potential and versatility in broader image restoration tasks. Code, dataset and models are available at: https://github.com/Zhuangvictor0/EDM-A-Enhanced-Diffusion-Models-for-Image-Restoration-in-Complex-Scenes
C1 [Wen, Jiayan; Zhuang, Yuansheng] Guangxi Univ Sci & Technol, Sch Automat, Liuzhou 545006, Guangxi, Peoples R China.
   [Wen, Jiayan; Zhuang, Yuansheng] Guangxi Univ Sci & Technol, Res Ctr Intelligent Cooperat & Cross applicat, Liuzhou 545616, Guangxi, Peoples R China.
   [Deng, Junyi] Guangxi Univ Sci & Technol, Sch Comp Sci & Technol, Liuzhou 545006, Guangxi, Peoples R China.
C3 Guangxi University of Science & Technology; Guangxi University of
   Science & Technology; Guangxi University of Science & Technology
RP Deng, JY (corresponding author), Guangxi Univ Sci & Technol, Sch Comp Sci & Technol, Liuzhou 545006, Guangxi, Peoples R China.
EM wenjiayan2012@126.com; 18770609145@163.com; dengjunyi@vip.sina.com
FU National Natural Science Foundation of China; Guangxi Science and
   Technology Planning Project [Gui Ke AD21220161]; Major Science and
   Technology Projects in Guangxi [Guike AA22068064];
   Industry-University-Research Innovation Fund for Chinese Universities
   [2021ZYA08002]; Guangxi Key Research and Development Project [Gui Ke
   AB23075164]; Basic Ability Improvement Project for Young and Middle-Aged
   University Teachers of Guangxi [2021KY0364]; Doctoral Fund Project of
   Guangxi University of Science and Technology [Xiao Ke Bo 23Z08]; 
   [61963006]
FX This paper was supported in part by the National Natural Science
   Foundation of China under Grant 61963006, Guangxi Science and Technology
   Planning Project under Grant Gui Ke AD21220161, Major Science and
   Technology Projects in Guangxi under Grant Guike AA22068064, the
   Industry-University-Research Innovation Fund for Chinese Universities
   under Grant 2021ZYA08002, Guangxi Key Research and Development Project
   under Grant Gui Ke AB23075164, the Basic Ability Improvement Project for
   Young and Middle-Aged University Teachers of Guangxi under Grant
   2021KY0364 and the Doctoral Fund Project of Guangxi University of
   Science and Technology under Grant Xiao Ke Bo 23Z08. (Corresponding
   author: Junyi Deng.)DAS:No datasets were generated or analysed during
   the current study.
CR Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Bettahar S, 2008, IMAGE VISION COMPUT, V26, P1481, DOI 10.1016/j.imavis.2008.02.010
   Binczak S, 2010, IMAGE VISION COMPUT, V28, P914, DOI 10.1016/j.imavis.2009.11.008
   Chen YT, 2023, J VIS COMMUN IMAGE R, V91, DOI 10.1016/j.jvcir.2023.103776
   Cordonnier JB, 2021, Arxiv, DOI arXiv:2006.16362
   Ding KY, 2022, IEEE T PATTERN ANAL, V44, P2567, DOI 10.1109/TPAMI.2020.3045810
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Elfwing S, 2018, NEURAL NETWORKS, V107, P3, DOI 10.1016/j.neunet.2017.12.012
   Farnia F., 2020, P 37 INT C MACH LEAR, P3029
   Guo HB, 2021, IEEE T CYBERNETICS, V51, P2735, DOI 10.1109/TCYB.2019.2934823
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D, 2020, Arxiv, DOI arXiv:1606.08415
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Huang JH, 2022, APPL INTELL, V52, P14693, DOI 10.1007/s10489-021-03092-w
   Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2
   Lee W, 2022, PROC CVPR IEEE, P17704, DOI 10.1109/CVPR52688.2022.01720
   Li WB, 2022, PROC CVPR IEEE, P10748, DOI 10.1109/CVPR52688.2022.01049
   Lin H., 2022, 2022 IEEE INT C MULT, P1
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Lugmayr A, 2022, PROC CVPR IEEE, P11451, DOI 10.1109/CVPR52688.2022.01117
   Misra D, 2020, Arxiv, DOI [arXiv:1908.08681, 10.48550/arXiv.1908.08681]
   Nair N. G., 2023, P IEEE CVF WINT C AP, P3434
   Nair V., 2010, ICML, P807
   Nichol A, 2022, Arxiv, DOI arXiv:2112.10741
   Nwankpa C, 2018, Arxiv, DOI [arXiv:1811.03378, DOI 10.48550/ARXIV.1811.03378]
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Poole B., 2022, arXiv
   Radford A, 2021, PR MACH LEARN RES, V139
   Ramesh A., 2022, arXiv:2204.06125, V7
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Saharia C, 2022, Arxiv, DOI [arXiv:2205.11487, DOI 10.48550/ARXIV.2205.11487]
   Sheng B, 2020, IEEE T VIS COMPUT GR, V26, P1332, DOI 10.1109/TVCG.2018.2869326
   Song JM, 2022, Arxiv, DOI [arXiv:2010.02502, DOI 10.48550/ARXIV.2010.02502]
   Suin M, 2020, PROC CVPR IEEE, P3603, DOI 10.1109/CVPR42600.2020.00366
   Sutedy Muhammad Fariz, 2022, 2022 5th International Seminar on Research of Information Technology and Intelligent Systems (ISRITI), P440, DOI 10.1109/ISRITI56927.2022.10052908
   Suvorov R, 2022, IEEE WINT CONF APPL, P3172, DOI 10.1109/WACV51458.2022.00323
   VARMA Rohan., 2018, Picking Loss Functions - A comparison between MSE, Cross Entropy, and Hinge Loss
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang XH, 2023, VISUAL COMPUT, V39, P1351, DOI 10.1007/s00371-022-02410-8
   Wang YH, 2022, Arxiv, DOI arXiv:2212.00490
   Xia B, 2023, Arxiv, DOI arXiv:2303.09472
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Yang T., 2023, arXiv
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Zeng YH, 2023, IEEE T VIS COMPUT GR, V29, P3266, DOI 10.1109/TVCG.2022.3156949
   Zhang J, 2018, PROC CVPR IEEE, P1828, DOI 10.1109/CVPR.2018.00196
   Zhang K., 2017, Proceedings of the IEEE 2017 Conference on Computer Vision and Pattern Recognition, DOI [DOI 10.1109/CVPR.2017.300, 10.1109/CVPR.2017.300]
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhou B., 2015, Places2: A large-scale database for scene understanding
NR 55
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 24
PY 2024
DI 10.1007/s00371-024-03549-2
EA JUL 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZL4D2
UT WOS:001275432300001
DA 2024-08-05
ER

PT J
AU Li, QW
   Shao, MW
   Liu, FK
   Qiao, YJ
   Hu, ZY
AF Li, Qiwang
   Shao, Mingwen
   Liu, Fukang
   Qiao, Yuanjian
   Hu, Zhiyong
TI Contrastive local constraint for irregular image reconstruction and
   editability
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Generative adversarial networks; GAN inversion; Contrastive learning;
   Image reconstruction
AB GAN inversion aims to invert a real image back into the latent space of a pre-trained GAN model, showing great potential in image reconstruction and editing. However, existing methods suffer from a loss of detail and texture when inverting irregular images with complex structural features. This is because they only focus on global reconstruction while ignoring local constraints. In this work, we propose a contrastive local constraint inversion framework, named CLC, to improve reconstruction on irregular images without decreasing the editability. Specifically, we introduce patch-wise multilayer contrastive learning (MCL) to strengthen the local correspondence in the iterative correction of the global reconstruction process, aiming to reconstruct detailed information of the target image. Meanwhile, to further refine this local correspondence between the target and reconstructed images, we push the network to learn more distinguishing representations by using hard negative samples obtained from a negative sample generator. This allows the model to further pull in the distance of locally corresponding positions in feature space, achieving better detail and texture reconstruction. Extensive experiments on irregular images demonstrate that our CLC exhibits considerable performance, significantly improving the reconstruction and editing quality of irregular images. Code is available at https://github.com/harmonic-lqw/CLC.
C1 [Li, Qiwang; Shao, Mingwen; Liu, Fukang; Qiao, Yuanjian] China Univ Petr East China, Qingdao Inst Software, Coll Comp Sci & Technol, Qingdao 266580, Shandong, Peoples R China.
   [Hu, Zhiyong] China Univ Petr East China, Coll Control Sci & Engn, Qingdao 266000, Peoples R China.
C3 China University of Petroleum; China University of Petroleum
RP Shao, MW (corresponding author), China Univ Petr East China, Qingdao Inst Software, Coll Comp Sci & Technol, Qingdao 266580, Shandong, Peoples R China.
EM lqwharmonic@163.com; smw278@126.com; upccaishu@163.com;
   yjqiao@s.upc.edu.cn; panghuyouxiang@163.com
FU National Key Research and Development Program of China [2021YFA1000102];
   National Natural Science Foundation of China [62376285, 62272375,
   61673396]; Natural Science Foundation of Shandong Province, China
   [ZR2022MF260]
FX The authors are very indebted to the anonymous referees for their
   critical comments and suggestions for the improvement of this paper.
   This work was supported by National Key Research and Development Program
   of China (2021YFA1000102), and in part by the grants from the National
   Natural Science Foundation of China (Nos. 62376285, 62272375, 61673396),
   Natural Science Foundation of Shandong Province, China (No.
   ZR2022MF260).
CR Abdal R., 2020, P IEEECVF C COMPUTER, P8296
   Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Alaluf Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6691, DOI 10.1109/ICCV48922.2021.00664
   Bachman P, 2019, ADV NEUR IN, V32
   Bau D, 2020, P NATL ACAD SCI USA, V117, P30071, DOI 10.1073/pnas.1907375117
   Brock A, 2019, Arxiv, DOI arXiv:1809.11096
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen ZH, 2023, IEEE T PATTERN ANAL, V45, P13489, DOI 10.1109/TPAMI.2023.3293885
   Cheng ZZ, 2015, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2015.55
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guan SY, 2020, Arxiv, DOI arXiv:2007.01758
   Guo ZH, 2023, VISUAL COMPUT, DOI 10.1007/s00371-023-02810-4
   Harkonen E, 2020, C NEUR INF PROC SYST
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Hu XQ, 2022, PROC CVPR IEEE, P11327, DOI 10.1109/CVPR52688.2022.01105
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Jiapeng Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P592, DOI 10.1007/978-3-030-58520-4_35
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2018, Arxiv, DOI [arXiv:1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kwon M, 2022, Arxiv, DOI arXiv:2210.10960
   Lehtinen J., 2020, PROC NEURIPS, V33, P12104
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu LY, 2021, Arxiv, DOI arXiv:1908.03265
   Ma FC, 2018, ADV NEUR IN, V31
   Park T., 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-719, 10.1007/978-3-030-58545-719, DOI 10.1007/978-3-030-58545-7_19]
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Shen Y., 2020, P IEEECVF C COMPUTER, P9243
   Sheng B, 2022, IEEE T CYBERNETICS, V52, P6662, DOI 10.1109/TCYB.2021.3079311
   Tov O, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459838
   Tsai YH, 2017, PROC CVPR IEEE, P2799, DOI 10.1109/CVPR.2017.299
   Voynov A., 2020, INT C MACHINE LEARNI, P9786
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang WL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14000, DOI 10.1109/ICCV48922.2021.01376
   Wang XL, 2021, PROC CVPR IEEE, P3023, DOI 10.1109/CVPR46437.2021.00304
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei TY, 2022, IEEE T IMAGE PROCESS, V31, P3267, DOI 10.1109/TIP.2022.3167305
   Wu ZZ, 2021, PROC CVPR IEEE, P12858, DOI 10.1109/CVPR46437.2021.01267
   Xia WH, 2023, IEEE T PATTERN ANAL, V45, P3121, DOI 10.1109/TPAMI.2022.3181070
   Xia WH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1782, DOI 10.1145/3394171.3413868
   Xie ZD, 2021, PROC CVPR IEEE, P16679, DOI 10.1109/CVPR46437.2021.01641
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Yu FS, 2016, Arxiv, DOI arXiv:1506.03365
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang MH, 2019, ADV NEUR IN, V32
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36
NR 55
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 18
PY 2024
DI 10.1007/s00371-024-03523-y
EA JUN 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UQ5T1
UT WOS:001249543600001
DA 2024-08-05
ER

PT J
AU Fang, X
   Gao, X
   Li, BF
   Zhai, F
   Qin, Y
   Meng, ZH
   Lu, JS
   Xiao, C
AF Fang, Xiao
   Gao, Xin
   Li, Baofeng
   Zhai, Feng
   Qin, Yu
   Meng, Zhihang
   Lu, Jiansheng
   Xiao, Chun
TI A non-uniform low-light image enhancement method with multi-scale
   attention transformer and luminance consistency loss
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Low-light image enhancement; Non-uniform illumination; Unsupervised
   learning; Multi-scale attention network; Consistency loop training;
   Luminance consistency loss
ID HISTOGRAM EQUALIZATION; RETINEX
AB Low-light image enhancement aims to improve the perception of images collected in dim environments and provide high-quality data support for image recognition tasks. When dealing with photographs captured under non-uniform illumination, existing methods cannot adaptively extract the differentiated luminance information, which will easily cause overexposure and underexposure. From the perspective of unsupervised learning, we propose a multi-scale attention Transformer named MSATr, which sufficiently extracts local and global features for light balance to improve the visual quality. Specifically, we present a multi-scale window division scheme, which uses exponential sequences to adjust the window size of each layer. Within different-sized windows, the self-attention computation can be refined, ensuring the pixel-level feature processing capability of the model. For feature interaction across windows, a global transformer branch is constructed to provide comprehensive brightness perception and alleviate exposure problems. Furthermore, we propose a loop training strategy, in which diverse images generated by weighted mixing and a luminance consistency loss are used to effectively improve the model's generalization ability. Extensive experiments on several benchmark datasets quantitatively and qualitatively prove that our MSATr is superior to state-of-the-art low-light image enhancement methods. The enhanced images have more natural brightness and outstanding details. The code is released at https://github.com/fang001021/MSATr.
C1 [Fang, Xiao; Gao, Xin; Meng, Zhihang] Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing 100876, Peoples R China.
   [Li, Baofeng; Zhai, Feng; Qin, Yu] China Elect Power Res Inst Co Ltd, Beijing 100192, Peoples R China.
   [Zhai, Feng] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Lu, Jiansheng; Xiao, Chun] State Grid Shanxi Mkt Serv Ctr, Taiyuan 030032, Peoples R China.
C3 Beijing University of Posts & Telecommunications; Tianjin University
RP Gao, X (corresponding author), Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing 100876, Peoples R China.
EM fang@bupt.edu.cn; xlhhh74@bupt.edu.cn; libaofeng@epri.sgcc.com.cn;
   zhaifeng@epri.sgcc.com.cn; qinyuqqwwe@126.com; mengzhihang@bupt.edu.cn;
   lujiansheng@sx.sgcc.com.cn; tyutxiaochun@163.com
FU Science & Technology Project of State Grid Corporation of China;
   High-Performance Computing Platform of BUPT;  [5400-202355230A-1-1-ZN]
FX The authors would like to thank their colleagues from the machine
   learning group for discussions on this paper. This work was supported by
   Science & Technology Project of State Grid Corporation of China
   (5400-202355230A-1-1-ZN) and the High-Performance Computing Platform of
   BUPT.
CR Abdullah-Al-Wadud M, 2007, IEEE T CONSUM ELECTR, V53, P593, DOI 10.1109/TCE.2007.381734
   Aboah A., 2023, P IEEE C COMP VIS PA, P5349
   An J, 2021, PROC CVPR IEEE, P862, DOI 10.1109/CVPR46437.2021.00092
   Cambria E, 2014, IEEE COMPUT INTELL M, V9, P48, DOI 10.1109/MCI.2014.2307227
   Chen ZH, 2021, VISUAL COMPUT, V37, P2657, DOI 10.1007/s00371-021-02199-y
   Cui XL, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2129
   Deng YY, 2022, PROC CVPR IEEE, P11316, DOI 10.1109/CVPR52688.2022.01104
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fan SJ, 2023, ENG APPL ARTIF INTEL, V117, DOI 10.1016/j.engappai.2022.105632
   Fu Y, 2022, KNOWL-BASED SYST, V240, DOI 10.1016/j.knosys.2021.108010
   Goodfellow Ian, 2014, PROC ADV NEURAL IN, V27
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2023, INT J COMPUT VISION, V131, P48, DOI 10.1007/s11263-022-01667-9
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Huang YJ, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2051
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Lee C, 2013, IEEE T IMAGE PROCESS, V22, P5372, DOI 10.1109/TIP.2013.2284059
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Li LY, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2190
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Loh YP, 2019, COMPUT VIS IMAGE UND, V178, P30, DOI 10.1016/j.cviu.2018.10.010
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Lv F., 2018, Proc. BMVC, V220, P4
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Ma L, 2022, PROC CVPR IEEE, P5627, DOI 10.1109/CVPR52688.2022.00555
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Pizer S. M., 1990, Proceedings of the First Conference on Visualization in Biomedical Computing (Cat. No.90TH0311-1), P337, DOI 10.1109/VBC.1990.109340
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Safaaldin S., 2023, Vis. Comput., P1
   Sheng B, 2022, IEEE T CYBERNETICS, V52, P6662, DOI 10.1109/TCYB.2021.3079311
   Tian CW, 2020, KNOWL-BASED SYST, V205, DOI 10.1016/j.knosys.2020.106235
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang Y, 1999, IEEE T CONSUM ELECTR, V45, P68, DOI 10.1109/30.754419
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Wei C, 2018, Arxiv, DOI arXiv:1808.04560
   Wu WH, 2022, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR52688.2022.00581
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang YX, 2023, IEEE T IMAGE PROCESS, V32, P1498, DOI 10.1109/TIP.2023.3243853
   Pan ZY, 2020, NEUROCOMPUTING, V386, P147, DOI 10.1016/j.neucom.2019.12.093
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 50
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 29
PY 2024
DI 10.1007/s00371-024-03452-w
EA MAY 2024
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL2G6
UT WOS:001234536800003
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Li, HB
   Guo, AD
   Li, YQ
AF Li, Haibin
   Guo, Aodi
   Li, Yaqian
TI CCMA: CapsNet for audio-video sentiment analysis using cross-modal
   attention
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Sentiment analysis; Audio-video bimodal; Positional embedding; Capsule
   network; Cross-modal fusion
ID FUSION
AB Multimodal sentiment analysis is a challenging research area that aims to investigate the use of complementary multimodal information to analyze the sentiment tendencies of a character. In order to effectively fuse multimodal heterogeneous data from different information sources, the current advanced models have developed a variety of fusion strategies mainly based on text modality, and research in the field of audio-visual bimodal fusion is relatively scarce. Therefore, in this paper, we propose a framework for sentiment analysis based on audio and video bimodality, CCMA. Initially, we preprocess the raw data and retain modality-specific temporal information through positional embedding. On the one hand, in order to solve the issue of modal contribution unbalance, we use capsule network and 1D convolution at the video modality side and audio modality side, respectively, to better represent the modal features. On the other hand, we believe that inter-modal explicit interaction is the best way to fuse cross-modal information, and design a cross-modal attentional interaction module for explicit interaction of modal information to enhance the fusion quality. Experiments on two popular sentiment analysis datasets RAVDESS and CMU-MOSEI show that the accuracy of our model performs better that the competing methods, which illustrates the effectiveness of our method.
C1 [Li, Haibin; Guo, Aodi; Li, Yaqian] Yanshan Univ, Key Lab Ind Comp Control Engn Hebei Prov, Qinhuangdao 066004, Peoples R China.
C3 Yanshan University
RP Guo, AD (corresponding author), Yanshan Univ, Key Lab Ind Comp Control Engn Hebei Prov, Qinhuangdao 066004, Peoples R China.
EM fsfst2009@foxmail.com
FU National Natural Science Foundation of China [62106214]; National Nature
   Science Foundation of China
FX This work was supported by the National Nature Science Foundation of
   China under Grant 62106214.
CR Cai J, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P443, DOI 10.1109/MIPR.2019.00089
   Chumachenko K, 2022, INT C PATT RECOG, P2822, DOI 10.1109/ICPR56361.2022.9956592
   Dang CN, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21165666
   Gandhi A, 2023, INFORM FUSION, V91, P424, DOI 10.1016/j.inffus.2022.09.025
   Ghosal D, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P3454
   Gu DH, 2021, IEEE ACCESS, V9, P157329, DOI 10.1109/ACCESS.2021.3126782
   Guo PN, 2022, LECT NOTES COMPUT SC, V13605, P315, DOI 10.1007/978-3-031-20500-2_26
   Hazarika D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1122, DOI 10.1145/3394171.3413678
   Hsu YL, 2020, IEEE T AFFECT COMPUT, V11, P85, DOI 10.1109/TAFFC.2017.2781732
   Huang J, 2020, INT CONF ACOUST SPEE, P3507, DOI [10.1109/icassp40776.2020.9053762, 10.1109/ICASSP40776.2020.9053762]
   Kim D, 2022, NEUROCOMPUTING, V506, P168, DOI 10.1016/j.neucom.2022.07.035
   Krishna DN, 2020, INTERSPEECH, P4243, DOI 10.21437/Interspeech.2020-1190
   Kumar P, 2024, MULTIMED TOOLS APPL, V83, P28373, DOI 10.1007/s11042-023-16443-1
   Langlet C, 2016, KNOWL-BASED SYST, V106, P116, DOI 10.1016/j.knosys.2016.05.038
   Le HD, 2023, IEEE ACCESS, V11, P14742, DOI 10.1109/ACCESS.2023.3244390
   Lee CM, 2005, IEEE T SPEECH AUDI P, V13, P293, DOI 10.1109/TSA.2004.838534
   Lin H, 2023, INFORM PROCESS MANAG, V60, DOI 10.1016/j.ipm.2022.103229
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Lin YB, 2019, INT CONF ACOUST SPEE, P2002, DOI 10.1109/icassp.2019.8683226
   Liu JX, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P6339, DOI 10.1109/ICASSP39728.2021.9413608
   Livingstone SR, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0196391
   Luna-Jiménez C, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21227665
   Madhu S., 2018, Int. J. Sci. Res. Comput. Sci. Eng, V6, P34, DOI DOI 10.26438/IJSRCSE/V6I4.3436
   Majumder N, 2018, KNOWL-BASED SYST, V161, P124, DOI 10.1016/j.knosys.2018.07.041
   Middya AI, 2022, KNOWL-BASED SYST, V244, DOI 10.1016/j.knosys.2022.108580
   Noroozi F, 2021, IEEE T AFFECT COMPUT, V12, P505, DOI 10.1109/TAFFC.2018.2874986
   Paraskevopoulos G, 2022, INT CONF ACOUST SPEE, P4573, DOI 10.1109/ICASSP43922.2022.9746418
   Pozzi FA, 2017, SENTIMENT ANALYSIS IN SOCIAL NETWORKS, P1, DOI 10.1016/B978-0-12-804412-4.00001-2
   Sabour S, 2017, ADV NEUR IN, V30
   Shahin I, 2022, EXPERT SYST APPL, V188, DOI 10.1016/j.eswa.2021.116080
   Sheng B, 2022, IEEE T CYBERNETICS, V52, P6662, DOI 10.1109/TCYB.2021.3079311
   Tang J., 2021, Long Papers, V1, P5301, DOI [10.18653/v1/2021.acl, DOI 10.18653/V1/2021.ACL]
   Tsai YHH, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P1823, DOI 10.18653/v1/2020.emnlp-main.143
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang D, 2023, PATTERN RECOGN, V136, DOI 10.1016/j.patcog.2022.109259
   Wang F, 2023, COGN COMPUT, V15, P289, DOI 10.1007/s12559-022-10073-9
   Wang YA, 2022, IEEE ACCESS, V10, P51315, DOI 10.1109/ACCESS.2022.3174215
   Wang ZL, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P2514, DOI 10.1145/3366423.3380000
   Wei Han, 2021, ICMI '21: Proceedings of the 2021 International Conference on Multimodal Interaction, P6, DOI 10.1145/3462244.3479919
   Wei QL, 2023, IEEE T BROADCAST, V69, P10, DOI 10.1109/TBC.2022.3215245
   Wu JF, 2022, IEEE-ACM T AUDIO SPE, V30, P1815, DOI 10.1109/TASLP.2022.3178236
   Xing FZ, 2018, ARTIF INTELL REV, V50, P49, DOI 10.1007/s10462-017-9588-9
   Yang B, 2022, IEEE-ACM T AUDIO SPE, V30, P2015, DOI 10.1109/TASLP.2022.3178204
   Yang B, 2022, NEUROCOMPUTING, V467, P130, DOI 10.1016/j.neucom.2021.09.041
   Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236
   Zeng ZH, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P126
   Zhang Y, 2022, AAAI CONF ARTIF INTE, P9100
   Zhu LA, 2023, INFORM FUSION, V95, P306, DOI 10.1016/j.inffus.2023.02.028
NR 49
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 21
PY 2024
DI 10.1007/s00371-024-03453-9
EA MAY 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RO3K3
UT WOS:001228562800003
DA 2024-08-05
ER

PT J
AU Ali, SG
   Zhang, CX
   Guan, ZY
   Chen, TL
   Wu, Q
   Li, P
   Yang, P
   Ghazanfar, Z
   Jung, YHY
   Chen, YT
   Sheng, B
   Tham, YC
   Wang, XN
   Wen, Y
AF Ali, Saba Ghazanfar
   Zhang, Chenxi
   Guan, Zhouyu
   Chen, Tingli
   Wu, Qiang
   Li, Ping
   Yang, Po
   Ghazanfar, Zainab
   Jung, Younhyun
   Chen, Yuting
   Sheng, Bin
   Tham, Yih-Chung
   Wang, Xiangning
   Wen, Yang
TI AI-enhanced digital technologies for myopia management: advancements,
   challenges, and future prospects
SO VISUAL COMPUTER
LA English
DT Review
DE Digital healthcare; Myopia; Digital devices; Artificial intelligence; AI
   health monitoring devices; Deep learning
ID PARTIAL COHERENCE INTERFEROMETRY; TIME SPENT OUTDOORS;
   ARTIFICIAL-INTELLIGENCE; REFRACTIVE ERROR; DIABETIC-RETINOPATHY;
   CHILDREN; REPEATABILITY; ATROPINE; BIOMETRY; LENGTH
AB Myopia, a prevalent refractive error leading to visual impairment, presents a substantial global health challenge. Current preventive measures, including pharmaceutical interventions and optical corrections, require significant resources and a dedicated workforce. Digital healthcare technology, encompassing artificial intelligence (AI) and telemedicine, offers promising solutions by delivering scalable, portable, and dependable approaches. Inspired by the success of deep learning systems in ophthalmology, these digital health technologies can revolutionize myopia management, enhancing automation, scalability, and remote monitoring. In this comprehensive review, we explore the potential applications of digital healthcare technology in measuring, preventing, controlling, and managing myopia. We highlight specific focus areas, emphasize the advancements and benefits of digital healthcare, and discuss the challenges associated with the clinical implementation of digital devices and AI applications in myopia management. Realizing the full potential of digital technologies in myopia management necessitates addressing access barriers, seamless integration with digital healthcare systems, validation of their effectiveness, and fostering user engagement. While adoption challenges persist, harnessing digital healthcare solutions can lead to improved vision, reduced myopia-related difficulties, and overall enhanced eye health. As this field advances, it holds a promising future for individuals affected by myopia.
C1 [Ali, Saba Ghazanfar; Chen, Yuting; Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Zhang, Chenxi] Chinese Acad Med Sci, Peking Union Med Coll Hosp, Dept Ophthalmol, Beijing, Peoples R China.
   [Guan, Zhouyu; Wu, Qiang; Wang, Xiangning] Shanghai Jiao Tong Univ, Shanghai Peoples Hosp 6, Sch Med, Shanghai 200233, Peoples R China.
   [Chen, Tingli] Huadong Sanat Hosp, Wuxi, Jiangsu, Peoples R China.
   [Li, Ping] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
   [Yang, Po] Univ Sheffield, Dept Comp Sci, Sheffield, England.
   [Ghazanfar, Zainab; Jung, Younhyun] Gachon Univ, Dept Software, Seonamsi, South Korea.
   [Tham, Yih-Chung] Singapore Eye Res Inst, Singapore Natl Eye Ctr, Singapore, Singapore.
   [Wen, Yang] Shenzhen Univ, Coll Elect & Informat Engn, Guangdong Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
C3 Shanghai Jiao Tong University; Chinese Academy of Medical Sciences -
   Peking Union Medical College; Peking Union Medical College Hospital;
   Shanghai Jiao Tong University; Hong Kong Polytechnic University;
   University of Sheffield; Gachon University; National University of
   Singapore; Singapore National Eye Center; Shenzhen University
RP Wang, XN (corresponding author), Shanghai Jiao Tong Univ, Shanghai Peoples Hosp 6, Sch Med, Shanghai 200233, Peoples R China.; Wen, Y (corresponding author), Shenzhen Univ, Coll Elect & Informat Engn, Guangdong Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
EM saba_ali@sjtu.edu.cn; zhangcx_pumch@163.com; chentingli1028@163.com;
   p.li@polyu.edu.hk; Po.Yang@sheffield.ac.uk; younhyun.jung@gachon.ac.kr;
   thamyc@nus.edu.sg; xnwang21@163.com; wen_yang@szu.edu.cn
RI Guan, Zhouyu/JFS-9545-2023
OI Guan, Zhouyu/0009-0008-5102-0067
FU National Science Foundation of China
FX No Statement Available
CR Abràmoff MD, 2018, NPJ DIGIT MED, V1, DOI 10.1038/s41746-018-0040-6
   Ali SG, 2022, IET IMAGE PROCESS, V16, P2171, DOI 10.1049/ipr2.12481
   Ali SG, 2021, MULTIMED TOOLS APPL, V80, P35105, DOI 10.1007/s11042-020-09303-9
   Cai XB, 2019, EXP EYE RES, V188, DOI 10.1016/j.exer.2019.107778
   Cao K, 2020, OPHTHALMIC RES, V63, P97, DOI 10.1159/000501937
   Cao YP, 2020, MEDICINE, V99, DOI 10.1097/MD.0000000000017992
   Chan B, 2006, CLIN EXP OPTOM, V89, P160, DOI 10.1111/j.1444-0938.2006.00029.x
   Chan HS, 2023, DIGIT HEALTH, V9, DOI 10.1177/20552076231176638
   Chia A, 2012, OPHTHALMOLOGY, V119, P347, DOI 10.1016/j.ophtha.2011.07.031
   Cruysberg LPJ, 2010, BRIT J OPHTHALMOL, V94, P106, DOI 10.1136/bjo.2009.161729
   De Fauw J, 2018, NAT MED, V24, P1342, DOI 10.1038/s41591-018-0107-6
   Dias R, 2019, GENOME MED, V11, DOI 10.1186/s13073-019-0689-8
   Flaxman SR, 2017, LANCET GLOB HEALTH, V5, pE1221, DOI 10.1016/S2214-109X(17)30393-5
   Foo LL, 2021, CURR OPIN OPHTHALMOL, V32, P413, DOI 10.1097/ICU.0000000000000791
   Fricke TR, 2018, BRIT J OPHTHALMOL, V102, P855, DOI 10.1136/bjophthalmol-2017-311266
   Garcia-Nonoal Z, 2024, VISUAL COMPUT, V40, P245, DOI 10.1007/s00371-023-02778-1
   Ghazala FR, 2021, EYE, V35, P1780, DOI 10.1038/s41433-020-1085-8
   Gulshan V, 2016, JAMA-J AM MED ASSOC, V316, P2402, DOI 10.1001/jama.2016.17216
   Gunasekeran DV, 2021, NPJ DIGIT MED, V4, DOI 10.1038/s41746-021-00412-9
   Gwiazda J, 2003, INVEST OPHTH VIS SCI, V44, P1492, DOI 10.1167/iovs.02-0816
   Haigis W, 2000, GRAEF ARCH CLIN EXP, V238, P765, DOI 10.1007/s004170000188
   He MG, 2015, JAMA-J AM MED ASSOC, V314, P1142, DOI 10.1001/jama.2015.10803
   HITZENBERGER CK, 1991, INVEST OPHTH VIS SCI, V32, P616
   Holden BA, 2016, OPHTHALMOLOGY, V123, P1036, DOI 10.1016/j.ophtha.2016.01.006
   Holden Brien A, 2015, Community Eye Health, V28, P35
   Hollander JE, 2020, NEW ENGL J MED, V382, P1679, DOI 10.1056/NEJMp2003539
   Huang JH, 2017, BRIT J OPHTHALMOL, V101, P493, DOI 10.1136/bjophthalmol-2016-308352
   Huang JH, 2016, OPHTHALMOLOGY, V123, P697, DOI 10.1016/j.ophtha.2015.11.010
   Hussin HM, 2006, EYE, V20, P1021, DOI 10.1038/sj.eye.6702069
   Joachimsen L, 2019, OPHTHALMOL THER, V8, P427, DOI 10.1007/s40123-019-0194-6
   Kavitha C., 2023, 2023 Eighth International Conference on Science Technology Engineering and Mathematics (ICONSTEM), P1, DOI 10.1109/ICONSTEM56934.2023.10142830
   Kido T, 2018, BMC MED GENOMICS, V11, DOI 10.1186/s12920-018-0322-5
   Kunert KS, 2016, J CATARACT REFR SURG, V42, P76, DOI 10.1016/j.jcrs.2015.07.039
   Li FF, 2019, ASIA-PAC J OPHTHALMO, V8, P360, DOI 10.1097/APO.0000000000000256
   Li I, 2022, COMPUT SCI REV, V46, DOI 10.1016/j.cosrev.2022.100511
   Li Y, 2023, TAIWAN J OPHTHALMOL, V13, P142, DOI 10.4103/tjo.TJO-D-23-00032
   Li Y, 2023, BRIT J OPHTHALMOL, V107, P600, DOI 10.1136/bjophthalmol-2021-320926
   Li ZW, 2019, ANN TRANSL MED, V7, DOI 10.21037/atm.2019.11.28
   Lin Z, 2016, GRAEF ARCH CLIN EXP, V254, P2247, DOI 10.1007/s00417-016-3440-9
   Matheny ME, 2020, JAMA-J AM MED ASSOC, V323, P509, DOI 10.1001/jama.2019.21579
   McAlinden C, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0128929
   Milea D, 2020, NEW ENGL J MED, V382, P1687, DOI 10.1056/NEJMoa1917130
   Mrochen M, 2020, INVEST OPHTH VIS SCI, V61
   Ohno-Matsui K, 2015, AM J OPHTHALMOL, V159, P877, DOI 10.1016/j.ajo.2015.01.022
   Olsen T, 2007, ACTA OPHTHALMOL SCAN, V85, P361, DOI 10.1111/j.1600-0420.2006.00847.x
   Orel Erol, 2023, J Med Internet Res, V25, pe39736, DOI 10.2196/39736
   Pineles SL, 2017, OPHTHALMOLOGY, V124, P1857, DOI 10.1016/j.ophtha.2017.05.032
   Reiswich Andreas, 2019, Stud Health Technol Inform, V260, P73
   Richter GM, 2017, JAMA OPHTHALMOL, V135, P724, DOI 10.1001/jamaophthalmol.2017.1176
   Rose KA, 2008, OPHTHALMOLOGY, V115, P1279, DOI 10.1016/j.ophtha.2007.12.019
   Sacchi M, 2019, ACTA OPHTHALMOL, V97, pE1136, DOI 10.1111/aos.14166
   Santodomingo-Rubido J, 2002, BRIT J OPHTHALMOL, V86, P458, DOI 10.1136/bjo.86.4.458
   Saw SM, 2005, BRIT J OPHTHALMOL, V89, P1489, DOI 10.1136/bjo.2005.071118
   Shafiq S, 2021, 2021 47TH EUROMICRO CONFERENCE ON SOFTWARE ENGINEERING AND ADVANCED APPLICATIONS (SEAA 2021), P99, DOI 10.1109/SEAA53835.2021.00022
   Shammas HJ, 2016, J CATARACT REFR SURG, V42, P50, DOI 10.1016/j.jcrs.2015.07.042
   Sherwin JC, 2012, OPHTHALMOLOGY, V119, P2141, DOI 10.1016/j.ophtha.2012.04.020
   Shorey S, 2019, J MED INTERNET RES, V21, DOI 10.2196/14658
   Sogawa T, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0227240
   Steinmetz JD, 2021, LANCET GLOB HEALTH, V9, pE144, DOI [10.1016/S2214-109X(20)30489-7, 10.1016/S2214-109X(20)30425-3]
   Sun Han, 2022, Cyber Security Intelligence and Analytics: The 4th International Conference on Cyber Security Intelligence and Analytics (CSIA 2022). Lecture Notes on Data Engineering and Communications Technologies (123), P1051, DOI 10.1007/978-3-030-96908-0_130
   Sun JT, 2018, J OPHTHALMOL, V2018, DOI 10.1155/2018/9781987
   Sundaram S.S., 2022, ARXIV
   Tan TE, 2021, LANCET DIGIT HEALTH, V3, pE317, DOI 10.1016/S2589-7500(21)00055-8
   Tedja MS, 2019, INVEST OPHTH VIS SCI, V60, pM89, DOI 10.1167/iovs.18-25965
   Tedja MS, 2018, NAT GENET, V50, P834, DOI 10.1038/s41588-018-0127-7
   Tham YC, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-30004-9
   Ting DSW, 2017, JAMA-J AM MED ASSOC, V318, P2211, DOI 10.1001/jama.2017.18152
   Tyagi S, 2023, VISUAL COMPUT, V39, P813, DOI 10.1007/s00371-021-02347-4
   van Schendel K., 2022, DESIGNING VIRTUAL RE
   Varadarajan AV, 2018, INVEST OPHTH VIS SCI, V59, P2861, DOI 10.1167/iovs.18-23887
   Verhoeven VJM, 2013, NAT GENET, V45, P314, DOI 10.1038/ng.2554
   Verkicharla PK, 2017, TRANSL VIS SCI TECHN, V6, DOI 10.1167/tvst.6.3.20
   Waller M, 2018, CURR ALLERGY ASTHM R, V18, DOI 10.1007/s11882-018-0808-4
   Wen LB, 2016, INVEST OPHTH VIS SCI, V57
   Whitelaw S, 2020, LANCET DIGIT HEALTH, V2, pE435, DOI 10.1016/S2589-7500(20)30142-4
   Wojtkowski M, 2002, J BIOMED OPT, V7, P457, DOI 10.1117/1.1482379
   Wu XH, 2019, LANCET, V394, P22
   Xie ZH, 2020, BMC OPHTHALMOL, V20, DOI 10.1186/s12886-020-01410-3
   Xu J, 2019, HUM GENET, V138, P109, DOI 10.1007/s00439-019-01970-5
   Yam JC, 2020, ACTA OPHTHALMOL, V98, pE639, DOI 10.1111/aos.14350
   Yang YH, 2020, ANN TRANSL MED, V8, DOI 10.21037/atm.2019.12.39
   Zhang CY, 2019, J EMERG TECHNOL ACCO, V16, P69, DOI 10.2308/jeta-52653
   Zhang CC, 2022, FRONT MED-LAUSANNE, V9, DOI 10.3389/fmed.2022.840498
   Zhang JZ, 2024, GRAEF ARCH CLIN EXP, V262, P3, DOI 10.1007/s00417-023-06101-5
   Zhang JZ, 2023, FRONT CELL DEV BIOL, V11, DOI 10.3389/fcell.2023.1124005
NR 85
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 3871
EP 3887
DI 10.1007/s00371-024-03391-6
EA APR 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001211242100003
DA 2024-08-05
ER

PT J
AU Luo, SZ
   Xu, JX
   Dingliana, J
   Wei, MQ
   Han, L
   He, LW
   Pan, JH
AF Luo, Shengzhou
   Xu, Jingxing
   Dingliana, John
   Wei, Mingqiang
   Han, Lu
   He, Lewei
   Pan, Jiahui
TI Twinenet: coupling features for synthesizing volume rendered images via
   convolutional encoder-decoders and multilayer perceptrons
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Neural volume rendering; Transfer functions; Convolutional
   encoder-decoders; Multilayer perceptrons; Positional encoding
ID VISUALIZATION; NETWORKS; CNN
AB Volume visualization plays a crucial role in both academia and industry, as volumetric data is extensively utilized in fields such as medicine, geosciences, and engineering. Addressing the complexities of volume rendering, neural rendering has emerged as a potential solution, facilitating the production of high-quality volume rendered images. In this paper, we propose TwineNet, a neural network architecture specifically designed for volume rendering. TwineNet combines features extracted from volume data, transfer functions, and viewpoints by utilizing twining skip connections across multiple feature layers. Building upon the TwineNet architecture, we introduce two neural networks, VolTFNet and PosTFNet, which leverage convolutional encoder-decoders and multilayer perceptrons to synthesize volume rendered images with novel transfer functions and viewpoints. Our experimental findings demonstrate the superiority of our models compared to state-of-the-art approaches in generating high-quality volume rendered images with novel transfer functions and viewpoints. This research contributes to advancing the field of volume rendering and showcases the potential of neural rendering techniques in scientific visualization.
C1 [Luo, Shengzhou] Guangdong Univ Educ, Sch Comp Sci, Guangzhou, Peoples R China.
   [Luo, Shengzhou; Xu, Jingxing; He, Lewei; Pan, Jiahui] South China Normal Univ, Sch Software, Foshan, Peoples R China.
   [Dingliana, John] Trinity Coll Dublin, Sch Comp Sci & Stat, Dublin, Ireland.
   [Wei, Mingqiang] Nanjing Univ Aeronaut & Astronaut, Sch Comp Sci & Technol, Nanjing, Peoples R China.
   [Han, Lu] Guangzhou Coll Technol & Business, Sch Engn, Guangzhou, Peoples R China.
   [Pan, Jiahui] Pazhou Lab, Guangzhou, Peoples R China.
C3 South China Normal University; Trinity College Dublin; Nanjing
   University of Aeronautics & Astronautics; Pazhou Lab
RP Luo, SZ (corresponding author), Guangdong Univ Educ, Sch Comp Sci, Guangzhou, Peoples R China.; Luo, SZ; He, LW; Pan, JH (corresponding author), South China Normal Univ, Sch Software, Foshan, Peoples R China.; Pan, JH (corresponding author), Pazhou Lab, Guangzhou, Peoples R China.
EM luos@m.scnu.edu.cn; xujingxing@m.scnu.edu.cn; John.Dingliana@tcd.ie;
   mingqiang.wei@gmail.com; hanlu@gzgs.edu.cn; helewei@m.scnu.edu.cn;
   panjiahui@m.scnu.edu.cn
FU Basic and Applied Basic Research Foundation of Guangdong Province
   [2020A1515110245]; Basic and Applied Basic Research Foundation of
   Guangdong Province
FX We would like to express our gratitude to the Open Scientific
   Visualization Datasets [60] for providing us with the teapot, nucleon,
   tooth, and foot data sets. We also acknowledge the Computer-Assisted
   Paleoanthropology group and the Visualization and Multimedia Lab at
   University of Zurich for the acquisition of the hazelnut data set [61].
   Our sincere thanks go to the respective owners for their generosity in
   sharing these data sets. This work was supported by the Basic and
   Applied Basic Research Foundation of Guangdong Province
   (2020A1515110245).
CR [Anonymous], 2017, Open SciVis Datasets
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bauer AC, 2016, COMPUT GRAPH FORUM, V35, P577, DOI 10.1111/cgf.12930
   Bauer D, 2023, IEEE T VIS COMPUT GR, V29, P515, DOI 10.1109/TVCG.2022.3209498
   Benbarka N, 2022, IEEE WINT CONF APPL, P2283, DOI 10.1109/WACV51458.2022.00234
   Berger M, 2019, IEEE T VIS COMPUT GR, V25, P1636, DOI 10.1109/TVCG.2018.2816059
   Chan ER, 2021, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR46437.2021.00574
   Chen JW, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3539225
   Chen K, 2022, IEEE T INTELL TRANSP, V23, P20046, DOI 10.1109/TITS.2022.3170874
   Devkota S, 2022, LECT NOTES COMPUT SC, V13598, P103, DOI 10.1007/978-3-031-20713-6_8
   Engel D, 2021, IEEE T VIS COMPUT GR, V27, P1268, DOI 10.1109/TVCG.2020.3030344
   Han J, 2023, IEEE T VIS COMPUT GR, V29, P4951, DOI 10.1109/TVCG.2022.3197203
   Han J, 2022, VIS INFORM, V6, P62, DOI 10.1016/j.visinf.2022.04.004
   Han J, 2022, IEEE T VIS COMPUT GR, V28, P2445, DOI 10.1109/TVCG.2020.3032123
   Han J, 2022, IEEE T VIS COMPUT GR, V28, P270, DOI 10.1109/TVCG.2021.3114815
   Han J, 2020, IEEE T VIS COMPUT GR, V26, P205, DOI 10.1109/TVCG.2019.2934255
   He WB, 2020, IEEE T VIS COMPUT GR, V26, P23, DOI 10.1109/TVCG.2019.2934312
   Hertz A., 2021, Advances in Neural Information Processing Systems, P8820
   Hong F, 2019, IEEE PAC VIS SYMP, P282, DOI 10.1109/PacificVis.2019.00041
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Ji YZ, 2021, INFORM SCIENCES, V546, P835, DOI 10.1016/j.ins.2020.09.003
   Jin Y, 2023, VISUAL COMPUT, V39, P4819, DOI 10.1007/s00371-022-02630-y
   Kim S, 2021, IEEE ACCESS, V9, P124281, DOI 10.1109/ACCESS.2021.3100429
   Landgraf Z, 2022, PR MACH LEARN RES
   Li Y, 2021, 35 C NEURAL INFORM P, V34
   Lin ZK, 2023, VISUAL COMPUT, V39, P597, DOI 10.1007/s00371-021-02360-7
   Lopes DS, 2018, VISUAL COMPUT, V34, P1713, DOI 10.1007/s00371-017-1448-8
   Luo S., 2017, P 33 SPRING C COMP G, DOI [10.1145/3154353.3154357, DOI 10.1145/3154353.3154357]
   Ma KL, 2009, IEEE COMPUT GRAPH, V29, P14, DOI 10.1109/MCG.2009.120
   Maas A.L., 2013, ICML WORK DEEP LEARN, V28
   Mai L, 2022, PROC CVPR IEEE, P10728, DOI 10.1109/CVPR52688.2022.01047
   Mehta I, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14194, DOI 10.1109/ICCV48922.2021.01395
   Meronen L., 2021, Advances in Neural Information Processing Systems, P1673
   Meyer-Spradow J, 2009, IEEE COMPUT GRAPH, V29, P6, DOI 10.1109/MCG.2009.130
   Mildenhall Ben, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P405, DOI 10.1007/978-3-030-58452-8_24
   Niemeyer M, 2020, PROC CVPR IEEE, P3501, DOI 10.1109/CVPR42600.2020.00356
   Partin L, 2023, J COMPUT PHYS, V472, DOI 10.1016/j.jcp.2022.111666
   Qiu JX, 2024, VISUAL COMPUT, V40, P1485, DOI 10.1007/s00371-023-02863-5
   Rahaman N, 2019, PR MACH LEARN RES, V97
   Ramasinghe S, 2022, LECT NOTES COMPUT SC, V13693, P142, DOI 10.1007/978-3-031-19827-4_9
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruiz M, 2011, IEEE T VIS COMPUT GR, V17, P1932, DOI 10.1109/TVCG.2011.173
   Selvaraj A, 2023, NEURAL PROCESS LETT, V55, P1605, DOI 10.1007/s11063-022-10953-z
   Shi N, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3309993
   Sitzmann V., 2020, Advances in neural information processing systems, V33, P7462, DOI DOI 10.48550/ARXIV.2006.09661
   Sitzmann V, 2019, ADV NEUR IN, V32
   Strengert M, 2006, VISUAL COMPUT, V22, P550, DOI 10.1007/s00371-006-0028-0
   Tancik M., 2020, ADV NEURAL INFORM PR, V33, P7537, DOI DOI 10.48550/ARXIV.2006.10739
   Tancik M, 2021, PROC CVPR IEEE, P2845, DOI 10.1109/CVPR46437.2021.00287
   Tewari A, 2022, COMPUT GRAPH FORUM, V41, P703, DOI 10.1111/cgf.14507
   Thu NP, 2018, ADV NEUR IN, V31
   VMML of the University of Zurich, 2013, Research Datasets
   Wang CL, 2023, IEEE T VIS COMPUT GR, V29, P3714, DOI 10.1109/TVCG.2022.3167896
   Wang QW, 2022, IEEE T VIS COMPUT GR, V28, P5134, DOI 10.1109/TVCG.2021.3106142
   Wang YF, 2021, IEEE T VIS COMPUT GR, V27, P1301, DOI 10.1109/TVCG.2020.3030374
   Weiss S, 2022, COMPUT GRAPH FORUM, V41, P196, DOI 10.1111/cgf.14578
   Weiss S, 2022, IEEE T VIS COMPUT GR, V28, P562, DOI 10.1109/TVCG.2021.3114769
   Weiss S, 2022, IEEE T VIS COMPUT GR, V28, P2654, DOI 10.1109/TVCG.2020.3039340
   Wu LW, 2022, PROC CVPR IEEE, P16179, DOI 10.1109/CVPR52688.2022.01572
   Xu R, 2021, PROC CVPR IEEE, P13564, DOI 10.1109/CVPR46437.2021.01336
   Yang CH, 2019, J VISUAL-JAPAN, V22, P991, DOI 10.1007/s12650-019-00583-4
   Yariv L., 2021, ADV NEURAL INFORM PR, P4805, DOI DOI 10.48550/ARXIV.2106.12052
   Zheng JQ, 2022, LECT NOTES COMPUT SC, V13687, P144, DOI 10.1007/978-3-031-19812-0_9
NR 63
TC 1
Z9 1
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 12
PY 2024
DI 10.1007/s00371-024-03368-5
EA APR 2024
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NN5Y8
UT WOS:001201155300001
DA 2024-08-05
ER

PT J
AU Qin, LD
   Lin, CY
   Huang, SJ
   Yang, SR
   Zhao, Y
AF Qin, Leidong
   Lin, Chunyu
   Huang, Shujuan
   Yang, Shangrong
   Zhao, Yao
TI Camera calibration for the surround-view system: a benchmark and dataset
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Surround-view system; Advanced driver assistance system (ADAS);
   Automatic extrinsic calibration
AB Surround-view system (SVS) is widely used in the advanced driver assistance system (ADAS). SVS uses four fish-eye lenses to monitor real-time scenes around the vehicle. However, accurate intrinsic and extrinsic parameter estimation is required for the proper functioning of the system. At present, the intrinsic calibration can be pipeline by utilizing checkerboard algorithm, while extrinsic calibration is still immature. Therefore, we proposed a specific calibration pipeline to estimate extrinsic parameters robustly. This scheme takes a driving sequence of four cameras as input. It firstly utilizes lane line to roughly estimate each camera pose. Considering the environmental condition differences in each camera, we separately select strategies from two methods to accurately estimate the extrinsic parameters. To achieve accurate estimates for both front and rear camera, we proposed a method that mutually iterating line detection and pose estimation. As for bilateral camera, we iteratively adjust the camera pose and position by minimizing texture and edge error between ground projections of adjacent cameras. After estimating the extrinsic parameters, the surround-view image can be synthesized by homography-based transformation. The proposed pipeline can robustly estimate the four SVS camera extrinsic parameters in real driving environments. In addition, to evaluate the proposed scheme, we build a surround-view fish-eye dataset, which contains 40 videos with 32,000 frames, acquired from different real traffic scenarios. All the frames in each video are manually labeled with lane annotation, with its GT extrinsic parameters. Moreover, this surround-view dataset could be used by other researchers to evaluate their performance. The dataset will be available soon.
C1 [Qin, Leidong; Lin, Chunyu; Huang, Shujuan; Yang, Shangrong; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing Key Lab Adv Informat Sci & Network, Beijing 100044, Peoples R China.
C3 Beijing Jiaotong University
RP Lin, CY (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing Key Lab Adv Informat Sci & Network, Beijing 100044, Peoples R China.
EM 21120295@bjtu.edu.cn; cylin@bjtu.edu.cn; shujuanhuang@bjtu.edu.cn;
   sr_yang@bjtu.edu.cn; yzhao@bjtu.edu.cn
RI Lin, Chunyu/AAI-5185-2021; Yang, Shangrong/JXN-8480-2024
OI Lin, Chunyu/0000-0003-2847-0349; 
FU National Natural Science Foundation of China [62172032, 62372036];
   National Natural Science Foundation of China
FX This work was supported by the National Natural Science Foundation of
   China (No.62172032, No. 62372036)
CR Baftiu I, 2016, 2016 IEEE INTERNATIONAL SYMPOSIUM ON ROBOTICS AND INTELLIGENT SENSORS (IRIS), P190, DOI 10.1109/IRIS.2016.8066089
   Chen Y, 2023, IEEE T MULTIMEDIA, V25, P1622, DOI 10.1109/TMM.2022.3144889
   Chen YM, 2023, VISUAL COMPUT, V39, P581, DOI 10.1007/s00371-021-02358-1
   Choi K, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18092956
   Dubská M, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.90
   Gao Y, 2018, IEEE T INTELL TRANSP, V19, P320, DOI 10.1109/TITS.2017.2750087
   Haris M, 2023, VISUAL COMPUT, V39, P519, DOI 10.1007/s00371-021-02353-6
   Hedi A., 2012, IFAC P, P120
   Hou YN, 2019, IEEE I CONF COMP VIS, P1013, DOI 10.1109/ICCV.2019.00110
   Huval B, 2015, Arxiv, DOI [arXiv:1504.01716, DOI 10.48550/ARXIV.1504.01716]
   Lee S, 2017, IEEE I CONF COMP VIS, P1965, DOI 10.1109/ICCV.2017.215
   Li ZQ, 2022, LECT NOTES COMPUT SC, V13669, P1, DOI 10.1007/978-3-031-20077-9_1
   Liu X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P383, DOI 10.1145/3343031.3350885
   Lourakis MIA, 2010, LECT NOTES COMPUT SC, V6312, P43, DOI 10.1007/978-3-642-15552-9_4
   Ma C, 2010, THIRD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING: WKDD 2010, PROCEEDINGS, P200, DOI 10.1109/WKDD.2010.118
   Ma Y, 2021, SYMMETRY-BASEL, V13, DOI 10.3390/sym13010128
   Maya P, 2020, INT ARAB CONF INF TE
   More J. J., 1978, Proceedings of the Biennial Conference on numerical analysis, P105
   Natroshvili K, 2017, IEEE INT VEH SYM, P82, DOI 10.1109/IVS.2017.7995702
   Pan X., 2018, arXiv
   Punagin A., 2020, Int. J. Res. Appl. Sci. Eng. Technol, V8, P2994, DOI DOI 10.22214/IJRASET.2020.5502
   Tabelini L, 2021, INT C PATT RECOG, P6150, DOI 10.1109/ICPR48806.2021.9412265
   Teo TY, 2021, MULTIMED TOOLS APPL, V80, P2063, DOI 10.1007/s11042-020-09819-0
   Ueshiba T., 2002, Sensors, V8, P4
   Wang J, 2014, 2014 IEEE 17TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P35, DOI 10.1109/ITSC.2014.6957662
   Yogamani S, 2019, IEEE I CONF COMP VIS, P9307, DOI 10.1109/ICCV.2019.00940
   Yuenan Hou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12483, DOI 10.1109/CVPR42600.2020.01250
   Zequn Qin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P276, DOI 10.1007/978-3-030-58586-0_17
   Zhang T., 2020, 2020 IEEE INT C MULT, P1
   Zhang TJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3153, DOI 10.1145/3474085.3475461
   Zhao K, 2014, 2014 IEEE 17TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P1490, DOI 10.1109/ITSC.2014.6957643
NR 31
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 16
PY 2024
DI 10.1007/s00371-024-03275-9
EA FEB 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8T6
UT WOS:001163171400001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Shi, YQ
   Liu, D
   Zhang, LG
   Xia, XZ
   Sun, JG
AF Shi, Yiqi
   Liu, Duo
   Zhang, Liguo
   Xia, Xuezhi
   Sun, Jianguo
TI MaCo: efficient unsupervised low-light image enhancement via
   illumination-based magnitude control
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Low-light image enhancement (LLIE); Unsupervised learning; Magnitude
   control; High computational efficiency
ID RETINEX; REPRESENTATION; GAP
AB This paper presents a novel low-light image enhancement (LLIE) method based on magnitude control (MaCo). Our method establishes a relationship between the low-light image and illumination, using pixel intensity and image brightness. Exploiting this relationship, MaCo enhances pixels with varying magnitudes, achieving pixel-wise LLIE. This yields high-quality enhanced images without local overexposure. We also introduce a set of carefully formulated unsupervised loss functions to enable training using only low-light images. Concretely, our method first trains a lightweight deep network, Low-res Coefficient Estimation Network (LCE-Net), to estimate the feature map in low-resolution space. Next, the High-res Illumination Estimation Module (HIE Module) is proposed to perform bilateral-grid-based upsampling for obtaining the high-res illumination. The illumination is finally utilized to light up the low-light image, yielding the enhanced image. MaCo is efficient and consumes fewer resources, as most computations occur in low resolution and the network is lightweight. Current LLIE datasets are mostly synthesized or generated through altered camera settings. These datasets are often limited in accurately representing real-world situations. To address this problem, we create a new dataset, IOLD, containing 572 images captured under real low- and normal-light conditions. In particular, the potential advantage of MaCo for face detection in the dark is also discussed. Extensive qualitative and quantitative experiments demonstrate that our method performs favorably against state-of-the-art methods in terms of effectiveness and efficiency. The IOLD dataset will be made publicly available at https://drive.google.com/drive/folders/1VAkuj9gheZ4aPEhBZ5MszHF6zpbrE_9_?usp=sharing.
C1 [Shi, Yiqi; Liu, Duo; Zhang, Liguo; Xia, Xuezhi; Sun, Jianguo] Harbin Engn Univ, Sch Comp Sci & Technol, Harbin 150006, Heilongjiang, Peoples R China.
   [Sun, Jianguo] Xidian Univ, Hangzhou Inst Technol, Sch Comp Sci & Technol, Hangzhou 311231, Zhejiang, Peoples R China.
C3 Harbin Engineering University; Xidian University
RP Sun, JG (corresponding author), Harbin Engn Univ, Sch Comp Sci & Technol, Harbin 150006, Heilongjiang, Peoples R China.; Sun, JG (corresponding author), Xidian Univ, Hangzhou Inst Technol, Sch Comp Sci & Technol, Hangzhou 311231, Zhejiang, Peoples R China.
EM shiyiqi@hrbeu.edu.cn; liu_duo@hrbeu.edu.cn; zhangliguo@hrbeu.edu.cn;
   xiaxuezhi709@gmail.com; sunjianguo@hrbeu.edu.cn
RI LIGUO, ZHANG/AAC-8765-2021
FU National Key Research and Development Program of China
FX No Statement Available
CR Awad M, 2020, IEEE T COMPUT IMAG, V6, P408, DOI 10.1109/TCI.2019.2956873
   Barron J.T., 2015, P IEEE C COMP VIS PA
   Barron JT, 2016, LECT NOTES COMPUT SC, V9907, P617, DOI 10.1007/978-3-319-46487-9_38
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Coltuc D, 2006, IEEE T IMAGE PROCESS, V15, P1143, DOI 10.1109/TIP.2005.864170
   Fan QN, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275081
   Fu Y, 2022, KNOWL-BASED SYST, V240, DOI 10.1016/j.knosys.2021.108010
   Fu Z., 2022, P IEEE CVF C COMP VI
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo H., 2021, 2021 INT JOINT C NEU, P1
   Guo HB, 2021, IEEE T CYBERNETICS, V51, P2735, DOI 10.1109/TCYB.2019.2934823
   Guo S, 2023, VISUAL COMPUT, V39, P1363, DOI 10.1007/s00371-022-02412-6
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hai J, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103712
   Hao SJ, 2020, IEEE T MULTIMEDIA, V22, P3025, DOI 10.1109/TMM.2020.2969790
   Hu WH, 2022, VISUAL COMPUT, V38, P3731, DOI 10.1007/s00371-021-02210-6
   Huang HF, 2022, IEEE T IMAGE PROCESS, V31, P1391, DOI 10.1109/TIP.2022.3140610
   Ibrahim H, 2007, IEEE T CONSUM ELECTR, V53, P1752, DOI 10.1109/TCE.2007.4429280
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Jin YX, 2021, IEEE T NEUR NET LEAR, V32, P2330, DOI 10.1109/TNNLS.2020.3004634
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Ke JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5128, DOI 10.1109/ICCV48922.2021.00510
   Land E.H., 1965, CIBA FDN S COLOUR VI
   LAND EH, 1986, P NATL ACAD SCI USA, V83, P3078, DOI 10.1073/pnas.83.10.3078
   Lee C, 2013, IEEE T IMAGE PROCESS, V22, P5372, DOI 10.1109/TIP.2013.2284059
   Li CY, 2022, Arxiv, DOI arXiv:2207.14273
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P4225, DOI 10.1109/TPAMI.2021.3063604
   Li HX, 2021, IEEE T IMAGE PROCESS, V30, P8526, DOI 10.1109/TIP.2021.3117061
   Li J, 2019, PROC CVPR IEEE, P5055, DOI 10.1109/CVPR.2019.00520
   Li JJ, 2021, IEEE T CIRC SYST VID, V31, P4227, DOI 10.1109/TCSVT.2021.3049940
   Li P, 2022, IEEE T NEUR NET LEAR, V33, P5346, DOI 10.1109/TNNLS.2021.3070463
   Lin QWN, 2022, INFORM SCIENCES, V608, P1401, DOI 10.1016/j.ins.2022.07.051
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Lin YH, 2022, IEEE T IMAGE PROCESS, V31, P4897, DOI 10.1109/TIP.2022.3189805
   Liu RS, 2021, PROC CVPR IEEE, P10556, DOI 10.1109/CVPR46437.2021.01042
   Loh YP, 2019, COMPUT VIS IMAGE UND, V178, P30, DOI 10.1016/j.cviu.2018.10.010
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Ma L, 2022, PROC CVPR IEEE, P5627, DOI 10.1109/CVPR52688.2022.00555
   Mertens T, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P382, DOI 10.1109/PG.2007.17
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Rahman Z, 2021, VISUAL COMPUT, V37, P865, DOI 10.1007/s00371-020-01838-0
   Sheng B, 2020, IEEE T CIRC SYST VID, V30, P955, DOI 10.1109/TCSVT.2019.2901629
   Sheng B, 2020, IEEE T VIS COMPUT GR, V26, P1332, DOI 10.1109/TVCG.2018.2869326
   Song XJ, 2023, VISUAL COMPUT, V39, P489, DOI 10.1007/s00371-021-02343-8
   Stark JA, 2000, IEEE T IMAGE PROCESS, V9, P889, DOI 10.1109/83.841534
   Tan X, 2021, IEEE T IMAGE PROCESS, V30, P9085, DOI 10.1109/TIP.2021.3122004
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei C, 2018, Arxiv, DOI arXiv:1808.04560
   Wen Y, 2021, IEEE T IMAGE PROCESS, V30, P6142, DOI 10.1109/TIP.2021.3092814
   Wu WH, 2022, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR52688.2022.00581
   Xu J, 2020, IEEE T IMAGE PROCESS, V29, P5022, DOI 10.1109/TIP.2020.2974060
   Xu XG, 2022, PROC CVPR IEEE, P17693, DOI 10.1109/CVPR52688.2022.01719
   Yang S, 2016, PROC CVPR IEEE, P5525, DOI 10.1109/CVPR.2016.596
   Yang WH, 2021, IEEE T IMAGE PROCESS, V30, P3461, DOI 10.1109/TIP.2021.3062184
   Yang WH, 2021, IEEE T IMAGE PROCESS, V30, P2072, DOI 10.1109/TIP.2021.3050850
   Yu NN, 2023, VISUAL COMPUT, V39, P1251, DOI 10.1007/s00371-022-02402-8
   Yuan L., 2012, COMPUTER VISION ECCV
   Yuan Y., 2019, arXiv
   Zhang F., 2021, arXiv
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang Y., 2020, arXiv
   Zhang YH, 2021, INT J COMPUT VISION, V129, P1013, DOI 10.1007/s11263-020-01407-x
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhang Y, 2022, IEEE T IMAGE PROCESS, V31, P759, DOI 10.1109/TIP.2021.3135473
   Zhang Z, 2022, PROC CVPR IEEE, P1889, DOI 10.1109/CVPR52688.2022.00194
   Zhao ZJ, 2022, IEEE T CIRC SYST VID, V32, P1076, DOI 10.1109/TCSVT.2021.3073371
NR 71
TC 0
Z9 0
U1 14
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 9
PY 2024
DI 10.1007/s00371-023-03249-3
EA FEB 2024
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HL7C8
UT WOS:001159716200001
DA 2024-08-05
ER

PT J
AU Darmawan, JT
   Sigalingging, XK
   Faisal, M
   Leu, JS
   Ratnasari, NRP
AF Darmawan, Jeremie Theddy
   Sigalingging, Xanno Kharis
   Faisal, Muhamad
   Leu, Jenq-Shiou
   Ratnasari, Nanda Rizqia Pradana
TI Neural network-based small cursor detection for embedded assistive
   technology
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Assistive device testbed; Machine learning; Embedded systems; Small
   object detection; Human-computer interface
AB Assistive technology (AT) is invaluable to people with special educational needs and disabilities, enabling them to interact with computers more efficiently. It is able to improve the interactivity between humans and computers for learning purposes. However, there is a lack of accessibility and interactivity with touchscreen devices and visual content that AT users encounter. These issues are critical in providing much-needed assistance and enhancing the daily lives of disabled individuals. Therefore, we propose an AT framework based on neural networks using embedded systems for improving the accessibility and interactivity of AT users. The proposed framework utilizes the Jetson Nano and is mainly used with a speech-to-intent neural network to process speech and move cursors. When improved with cursor object detection, the framework could obtain the location of the cursors in external displays and move the cursors of other devices. Since cursor datasets are very limited and not many detection models are up for the task, we investigated the use of Slicing Aided Hyper Inference (SAHI) pipeline along with two fine-tuned models, Fully Convoluted One-Stage (FCOS) and Task-aligned One-stage Object Detection (TOOD), to identify the minimum data required for these models to work optimally. With less than 120 annotated images and a data multiplier of 5 and 30, both models were able to achieve similar to 52 and similar to 60mAP, respectively. These results were comparable to performance on other small object detection datasets. In addition, we also present a working proof-of-concept for our proposed embedded assistive technology framework.
C1 [Darmawan, Jeremie Theddy; Sigalingging, Xanno Kharis; Faisal, Muhamad; Leu, Jenq-Shiou] Natl Taiwan Univ Sci & Technol, Dept Elect & Comp Engn, 43,Sect 4,Keelung Rd, Taipei City 106, Taiwan.
   [Darmawan, Jeremie Theddy; Ratnasari, Nanda Rizqia Pradana] Indonesia Int Inst Life Sci, Sch Life Sci, Dept Bioinformat, Jl Pulomas Barat Kav 88,RT4-RW9,Kayu Putih, Jakarta 13210, Indonesia.
   [Darmawan, Jeremie Theddy] Natl Univ Singapore, Yong Loo Lin Sch Med, Dept Biochem, C-O MD7,Level 2,8 Med Dr, Singapore 117597, Singapore.
C3 National Taiwan University of Science & Technology; Indonesia
   International Institute for Life-Sciences (i3L); National University of
   Singapore
RP Darmawan, JT; Leu, JS (corresponding author), Natl Taiwan Univ Sci & Technol, Dept Elect & Comp Engn, 43,Sect 4,Keelung Rd, Taipei City 106, Taiwan.; Darmawan, JT; Ratnasari, NRP (corresponding author), Indonesia Int Inst Life Sci, Sch Life Sci, Dept Bioinformat, Jl Pulomas Barat Kav 88,RT4-RW9,Kayu Putih, Jakarta 13210, Indonesia.; Darmawan, JT (corresponding author), Natl Univ Singapore, Yong Loo Lin Sch Med, Dept Biochem, C-O MD7,Level 2,8 Med Dr, Singapore 117597, Singapore.
EM jeremie.darmawan@student.i3l.ac.id; jsleu@mail.ntust.edu.tw;
   nanda.ratnasari@i3l.ac.id
OI Leu, Jenq-Shiou/0000-0001-7197-9912; Darmawan, Jeremie
   Theddy/0000-0001-9726-3613
CR Adadi A, 2021, J BIG DATA-GER, V8, DOI 10.1186/s40537-021-00419-9
   Akyon FC, 2022, IEEE IMAGE PROC, P966, DOI 10.1109/ICIP46576.2022.9897990
   Anusuya MA., 2010, Proc. IEEE, V64, P501, DOI [10.48550/arxiv.1001.2267, DOI 10.48550/ARXIV.1001.2267]
   Borg J, 2015, DISABIL REHABIL-ASSI, V10, P301, DOI 10.3109/17483107.2014.974221
   Cai CJ, 2020, J MED CHEM, V63, P8683, DOI 10.1021/acs.jmedchem.9b02147
   Caputo A.C., 2014, Digital Video Surveillance and Security, P25, DOI [10.1016/B978-0-12-420042-5.00002-2, DOI 10.1016/B978-0-12-420042-5.00002-2]
   Das P., 2015, J. Appl. Fundam. Sci, V1, P2395
   Desai V., 2020, INT J INNOV SCI RES, V5, P267, DOI DOI 10.38124/IJISRT20JUL342
   Faisal M, 2023, IEEE ACCESS, V11, P62281, DOI 10.1109/ACCESS.2023.3286935
   Feng CJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3490, DOI 10.1109/ICCV48922.2021.00349
   Fernandez-Batanero JM., 2022, Educ. Technol. Res. Dev, DOI [10.1007/S11423-022-10127-7/FIGURES/10, DOI 10.1007/S11423-022-10127-7/FIGURES/10]
   Fu C, 2017, arXiv
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   github, Picovoice: Picovoice/rhino: On-device speech-to-intent engine powered by deep learning
   Han T., 2022, IEEE INT C MACH LEAR, P6, DOI DOI 10.1109/MLISE57402.2022.00009
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Hsieh HL, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1006168
   Hwang B, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11172783
   Karki J, 2023, DISABIL REHABIL-ASSI, V18, P8, DOI 10.1080/17483107.2021.1892843
   Kim MG, 2022, INT J ENV RES PUB HE, V19, DOI 10.3390/ijerph19095126
   Kober N., 1991, WHAT WE KNOW MATH TE
   Lalor J.P., 2017, arXiv, DOI DOI 10.48550/ARXIV.1702.08563
   Lam D., 2018, arXiv
   Larson E, 2018, IEEE INT WORK C SO, P225, DOI 10.1109/SCAM.2018.00034
   Li YS, 2007, TECHNOL SOC, V29, P361, DOI 10.1016/j.techsoc.2007.04.004
   Liscio E, 2021, FORENSIC SCI INT, V320, DOI 10.1016/j.forsciint.2021.110690
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo BF, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2083
   Manjusha L., 2022, Journal of Positive School Psychology, V6, P606
   McNicholl A, 2021, DISABIL REHABIL-ASSI, V16, P130, DOI 10.1080/17483107.2019.1642395
   Merbler J., 1999, PREV SCH FAIL, V43, P113, DOI 10.1080/10459889909603311
   Olakanmi OA, 2020, ETR&D-EDUC TECH RES, V68, P1711, DOI 10.1007/s11423-020-09795-0
   Padilla R, 2020, INT CONF SYST SIGNAL, P237, DOI [10.1109/IWSSIP48289.2020.9145130, 10.1109/iwssip48289.2020.9145130]
   Peter P., 2006, ACM SIGACCESS Access. Comput, DOI [10.1145/1127564.1127571, DOI 10.1145/1127564.1127571]
   Qin YL, 2019, IEEE T MED IMAGING, V38, P2569, DOI 10.1109/TMI.2019.2905841
   Quek F., 2016, Handbook of Science and Technology Convergence, P973, DOI [10.1007/978-3-319-07052-0_25, DOI 10.1007/978-3-319-07052-0_25]
   Rajmond Jano, 2009, 2009 15th International Symposium for Design and Technology of Electronics Packages (SIITME 2009), P289, DOI 10.1109/SIITME.2009.5407358
   Ravuri S, 2015, 2015 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P368, DOI 10.1109/ASRU.2015.7404818
   Ren SQ, 2016, Arxiv, DOI [arXiv:1506.01497, DOI 10.1109/TPAMI.2016.2577031]
   Schroeder M.R., 1999, The Speech Signal, P105, DOI 10.1007/978-3-662-03861-1_7
   Shi GM, 2022, FRONT POLIT SCI, V4, DOI 10.3389/fpos.2022.859272
   Shin DJ, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12083734
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Sivin-Kachala J., 1994, REPORT EFFECTIVENESS
   Sweigart Al, Welcome to PyAutoGUI's documentation!-PyAutoGUI documentation
   Sze S., 2004, SOC INF TECHN TEACH, P4959
   The kernel development community, Linux USB HID gadget driver-The Linux Kernel documentation
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   Wen CJ, 2022, FRONT PLANT SCI, V13, DOI 10.3389/fpls.2022.973985
   Xia H., 2023, Vis. Comput, DOI [10.1007/S00371-023-02900-3/METRICS, DOI 10.1007/S00371-023-02900-3/METRICS]
   Xie X., 2018, 9 INT C GRAPH IM PRO, P236
   Xin FF, 2024, VISUAL COMPUT, V40, P393, DOI 10.1007/s00371-023-02789-y
   Xin ZM, 2024, VISUAL COMPUT, V40, P2347, DOI 10.1007/s00371-023-02920-z
   Zeng S, 2024, VISUAL COMPUT, V40, P1787, DOI 10.1007/s00371-023-02886-y
   Zhu PF, 2018, Arxiv, DOI [arXiv:1804.07437, DOI 10.48550/ARXIV.1804.07437]
   Zhu PF, 2022, IEEE T PATTERN ANAL, V44, P7380, DOI 10.1109/TPAMI.2021.3119563
NR 59
TC 1
Z9 1
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 25
PY 2024
DI 10.1007/s00371-023-03246-6
EA JAN 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GC2K0
UT WOS:001150397600002
DA 2024-08-05
ER

PT J
AU Gutiérrez-Fernández, A
   Fernández-Llamas, C
   Vázquez-Casares, AM
   Mauriz, E
   Riego-del-Castillo, V
   John, NW
AF Gutierrez-Fernandez, Alexis
   Fernandez-Llamas, Camino
   Vazquez-Casares, Ana M.
   Mauriz, Elba
   Riego-del-Castillo, Virginia
   John, Nigel W.
TI Immersive haptic simulation for training nurses in emergency medical
   procedures
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Human-computer interaction; Haptic device; Virtual reality; Medical
   skills training
AB The use of haptic simulation for emergency procedures in nursing training presents a viable, versatile and affordable alternative to traditional mannequin environments. In this paper, an evaluation is performed in a virtual environment with a head-mounted display and haptic devices, and also with a mannequin. We focus on a chest decompression, a life-saving invasive procedure used for trauma-associated cardiopulmonary resuscitation (and other causes) that every emergency physician and/or nurse needs to master. Participants' heart rate and blood pressure were monitored to measure their stress level. In addition, the NASA Task Load Index questionnaire was used. The results show the approved usability of the VR environment and that it provides a higher level of immersion compared to the mannequin, with no statistically significant difference in terms of cognitive load, although the use of VR is perceived as a more difficult task. We can conclude that the use of haptic-enabled virtual reality simulators has the potential to provide an experience as stressful as the real one while training in a safe and controlled environment.
C1 [Gutierrez-Fernandez, Alexis; Fernandez-Llamas, Camino; Riego-del-Castillo, Virginia] Univ Leon, Sch Ind Comp & Aeroespace Engn, Leon, Spain.
   [Vazquez-Casares, Ana M.; Mauriz, Elba] Univ Leon, Fac Hlth Sci, Leon, Spain.
   [John, Nigel W.] Univ Chester, Med Graph Lab, Chester, England.
C3 Universidad de Leon; Universidad de Leon; University of Chester
RP Gutiérrez-Fernández, A (corresponding author), Univ Leon, Sch Ind Comp & Aeroespace Engn, Leon, Spain.
EM agutf@unileon.es; cferll@unileon.es; emaugr@unileon.es;
   amvazc@unileon.es; vriec@unileon.es; nigel.john@chester.ac.uk
RI Llamas, Camino Fernández/G-2805-2016; Llamas, Camino
   Fernández/AAC-4269-2020; vázquez-casares, ana/JBR-8754-2023; Mauriz,
   Elba/E-2095-2017; vazquez-casares, ana/J-5651-2013; Gutierrez-Fernandez,
   Alexis/N-2365-2018
OI Llamas, Camino Fernández/0000-0002-8705-4786; Llamas, Camino
   Fernández/0000-0002-8705-4786; Mauriz, Elba/0000-0002-6459-8107; Riego
   del Castillo, Virginia/0000-0003-3389-0786; vazquez-casares,
   ana/0000-0002-0360-5262; Gutierrez-Fernandez, Alexis/0000-0002-3173-3720
FU Universidad de Len
FX No Statement Available
CR Artero PMA, 2023, MEDICINE, V102, DOI 10.1097/MD.0000000000032736
   Barsom EZ, 2020, BRIT J EDUC TECHNOL, V51, P2050, DOI 10.1111/bjet.13025
   Bello F., 2009, EUROGRAPHICS 2009 ME, P5
   Boyle MJ, 2012, WORLD J EMERG MED, V3, P265, DOI [10.5847/wjem.j.1920-8642.2012.04.005, 10.5847/wjem.j.issn.1920-8642.2012.04.005]
   Buttussi Fabio, 2020, J Biomed Inform, V111, P103590, DOI 10.1016/j.jbi.2020.103590
   Coles TR, 2011, IEEE T HAPTICS, V4, P51, DOI 10.1109/ToH.2010.19
   Cowan A, 2021, J ENDOUROL, V35, P1571, DOI 10.1089/end.2020.1037
   Dominjon L, 2005, World Haptics Conference: First Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virutual Environment and Teleoperator Systems, Proceedings, P639
   Efendi D, 2023, BMC NURS, V22, DOI 10.1186/s12912-023-01312-x
   Esteban G, 2015, INT J ENG EDUC, V31, P726
   Gutierrez-Fernandez Alexis, 2022, 2022 International Conference on Cyberworlds (CW), P79, DOI 10.1109/CW55638.2022.00020
   Gutierrez-Fernandez Alexis, 2023, Zenodo, DOI 10.5281/ZENODO.7540444
   HART S G, 1988, P139
   Isbilir E, 2019, FRONT HUM NEUROSCI, V13, DOI 10.3389/fnhum.2019.00375
   Johannessen E, 2020, COMPUT HUM BEHAV, V111, DOI 10.1016/j.chb.2020.106393
   Li ZX, 2020, INTERACT COMPUT, V32, P17, DOI 10.1093/iwcomp/iwaa002
   Martens MAG, 2019, J PSYCHOPHARMACOL, V33, P1264, DOI 10.1177/0269881119860156
   Massie T. H., 1994, P S HAPT INT VIRT EN, P295
   Mauriz E, 2021, INT J ENV RES PUB HE, V18, DOI 10.3390/ijerph18105448
   Miles M, 2014, Qualitative data analysis
   Rees N., 2020, J Paramed Pract, V12, P478, DOI [10.12968/jpar.2020.12.12.478., DOI 10.12968/JPAR.2020.12.12.478]
   Reyes Lourdes R., 2022, 2022 IEEE Haptics Symposium (HAPTICS), DOI 10.1109/HAPTICS52432.2022.9765597
   Sarac M, 2021, 2021 IEEE WORLD HAPTICS CONFERENCE (WHC), P873, DOI 10.1109/WHC49131.2021.9517260
   Schoomnaker RE, 2006, IEEE SYS MAN CYBERN, P2464, DOI 10.1109/ICSMC.2006.385233
   Wilson KL, 2013, MIL MED, V178, P981, DOI 10.7205/MILMED-D-13-00074
   Yu P, 2020, COMPUT ANIMAT VIRT W, V31, DOI 10.1002/cav.1940
   Zhenxing Li, 2020, Haptics: Science, Technology, Applications. 12th International Conference, EuroHaptics 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12272), P158, DOI 10.1007/978-3-030-58147-3_18
NR 27
TC 0
Z9 0
U1 8
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 24
PY 2024
DI 10.1007/s00371-023-03227-9
EA JAN 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FS4C1
UT WOS:001147821000002
OA Green Published, hybrid
DA 2024-08-05
ER

PT J
AU Xu, LY
   Yuan, QN
   Sun, Y
   Gao, QY
AF Xu, Liangyao
   Yuan, Qingni
   Sun, Yu
   Gao, Qingyang
TI Image neural style transfer combining global and local optimization
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Deep neural networks; Style transfer; Global optimization; Local
   optimization
AB In order to avoid the shortcomings of a single optimization method, improve the effect of style transfer, and control the occurrence of artifacts, this paper proposes a neural style transfer method combining global and local optimization. In the calculation of local loss, the content mask and style mask are used to the matching process of the image patches to preserve the style details and reduce the mismatching of the image. The global loss function is calculated by Gram matrix, and the mask of the content feature map is added to the feature map of the synthetic image. The effect of mask data on the image is controlled by hyperparameters, and the Laplacian operator is introduced for structural refinement to better preserve the structural integrity of the stylized image. Experimental results show that this method can extend the scope of application of style transfer, can be effectively used for different images, and effectively control artifacts. The data for our approach are publicly available at https://github.com/xlyusegithub/styletransfer.git.
C1 [Xu, Liangyao; Yuan, Qingni; Sun, Yu; Gao, Qingyang] Guizhou Univ, Key Lab Adv Mfg Technol, Minist Educ, Guiyang 550025, Peoples R China.
   [Yuan, Qingni] Guizhou Univ, State Key Lab Publ Big Data, Guiyang, Peoples R China.
   [Yuan, Qingni] Guizhou Univ, Sch Mech Engn, Guiyang, Peoples R China.
C3 Guizhou University; Guizhou University; Guizhou University
RP Yuan, QN (corresponding author), Guizhou Univ, Key Lab Adv Mfg Technol, Minist Educ, Guiyang 550025, Peoples R China.; Yuan, QN (corresponding author), Guizhou Univ, State Key Lab Publ Big Data, Guiyang, Peoples R China.; Yuan, QN (corresponding author), Guizhou Univ, Sch Mech Engn, Guiyang, Peoples R China.
EM xly72666@163.com; qnyuan@gzu.edu.cn
RI gao, qingyang/GYV-0632-2022
OI gao, qingyang/0009-0005-7739-3900
FU National Natural Science Foundation of China; Department of Science and
   Technology of Guizhou Province [[2022] G140, [2022] K024, [2023] G094,
   [2023] G125]; Laboratory Open Project of Guizhou University
   [SYSKF2023-089]; Bureau of Science and Technology of Guiyang [[2022]
   2-3];  [52065010];  [52165063]
FX This work was supported in part by the National Natural Science
   Foundation of China (Project No.52065010 and No.52165063), Department of
   Science and Technology of Guizhou Province (Project No. [2022] G140; No.
   [2022] K024; [2023] G094; [2023] G125), Laboratory Open Project of
   Guizhou University (SYSKF2023-089), Bureau of Science and Technology of
   Guiyang (Project No. [2022] 2-3).
CR Gatys LA, 2015, Arxiv, DOI [arXiv:1508.06576, 10.1167/16.12.326, DOI 10.1167/16.12.326]
   Chen XY, 2019, IEEE T IMAGE PROCESS, V28, P546, DOI 10.1109/TIP.2018.2869695
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Dong S, 2022, MATH PROBL ENG, V2022, DOI 10.1155/2022/8918722
   Gatys L., 2015, arXiv:1505.07376, V28, P262
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Hanmin Ye, 2021, 2021 IEEE 2nd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA), P1171, DOI 10.1109/ICIBA52610.2021.9687957
   Hong S, 2023, INT J DIGIT EARTH, V16, P1491, DOI 10.1080/17538947.2023.2202422
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Champandard AJ, 2016, Arxiv, DOI arXiv:1603.01768
   Jin X, 2022, SECUR COMMUN NETW, V2022, DOI 10.1155/2022/5087129
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim S, 2021, IEEE ACCESS, V9, P7930, DOI 10.1109/ACCESS.2021.3049637
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Lan JY, 2023, VISUAL COMPUT, V39, P6167, DOI 10.1007/s00371-022-02719-4
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43
   Li HX, 2021, IEEE T IMAGE PROCESS, V30, P8526, DOI 10.1109/TIP.2021.3117061
   Li SH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1716, DOI 10.1145/3123266.3123425
   Li YJ, 2017, PROC CVPR IEEE, P266, DOI 10.1109/CVPR.2017.36
   Li YM, 2024, VISUAL COMPUT, V40, P2265, DOI 10.1007/s00371-023-02915-w
   Liao YS, 2022, IEEE T IMAGE PROCESS, V31, P1911, DOI 10.1109/TIP.2022.3149237
   Lin ZJ, 2021, IEEE ACCESS, V9, P54518, DOI 10.1109/ACCESS.2021.3054969
   Liu Y, 2023, VISUAL COMPUT, V39, P3299, DOI 10.1007/s00371-023-02941-8
   Luan FJ, 2017, PROC CVPR IEEE, P6997, DOI 10.1109/CVPR.2017.740
   Ma Z., 2022, IEEE Trans. Neural Networks Learn. Syst.
   Ma ZQ, 2020, NEUROCOMPUTING, V406, P135, DOI 10.1016/j.neucom.2020.04.027
   Park DY, 2019, PROC CVPR IEEE, P5873, DOI 10.1109/CVPR.2019.00603
   Prince SJD., 2012, COMPUTER VISION MODE, DOI DOI 10.1017/CBO9780511996504
   Tang ZY, 2023, Cloud Compu and Big, P560, DOI 10.1109/ICCCBDA56900.2023.10154714
   Wang FB, 2024, VISUAL COMPUT, V40, P761, DOI 10.1007/s00371-023-02814-0
   Wang H, 2020, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR42600.2020.00193
   Wang ZZ, 2020, IET COMPUT VIS, V14, P575, DOI 10.1049/iet-cvi.2019.0844
   Wu ChangMing, 2022, 2022 7th International Conference on Intelligent Computing and Signal Processing (ICSP), P1127, DOI 10.1109/ICSP54964.2022.9778781
   Yang RR, 2019, IEEE COMPUT SOC CONF, P1769, DOI 10.1109/CVPRW.2019.00227
   Ye HM, 2020, PROCEEDINGS OF 2020 IEEE 4TH INFORMATION TECHNOLOGY, NETWORKING, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (ITNEC 2020), P1635, DOI 10.1109/ITNEC48623.2020.9085127
   Ye WJ, 2022, J VIS COMMUN IMAGE R, V82, DOI 10.1016/j.jvcir.2021.103378
   Ye WJ, 2023, VISUAL COMPUT, V39, P609, DOI 10.1007/s00371-021-02361-6
   Yu JC, 2022, MULTIMED TOOLS APPL, V81, P3915, DOI 10.1007/s11042-021-11694-2
   Yu Y, 2024, VISUAL COMPUT, V40, P3411, DOI 10.1007/s00371-023-03042-2
   Zhang FC, 2022, COMPUT ELECTR ENG, V104, DOI 10.1016/j.compeleceng.2022.108459
   Zhang YL, 2019, IEEE I CONF COMP VIS, P5942, DOI 10.1109/ICCV.2019.00604
   Zhao HH, 2020, VISUAL COMPUT, V36, P1307, DOI 10.1007/s00371-019-01726-2
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu SX, 2020, PROC INT C TOOLS ART, P694, DOI 10.1109/ICTAI50040.2020.00111
NR 45
TC 1
Z9 1
U1 8
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 24
PY 2024
DI 10.1007/s00371-023-03244-8
EA JAN 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FS4C1
UT WOS:001147821000001
DA 2024-08-05
ER

PT J
AU Yu, H
   Cao, J
   Liu, XR
   Chen, ZG
AF Yu, Hang
   Cao, Juan
   Liu, Xiangrong
   Chen, Zhonggui
TI Regularity-constrained point cloud reconstruction of building models via
   global alignment
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Regularity-constrained reconstruction; Primitive segmentation; Nonlinear
   optimization; Geometric design and computation
AB Recovering the shape of an object from a 3D point cloud is a challenging task due to the complex shapes and diverse requirements of different tasks. For buildings in urban scenes, maintaining geometric constraints such as parallelism, perpendicularity, symmetry, and coplanarity is important during the model fitting process. In this paper, we present a regularity-constrained point cloud reconstruction framework that comprises primitive initialization, constraint construction, and global optimization. Our approach first performs normal optimization and plane segmentation on the input point cloud. We then compute the global reference directions to set the target normal for each plane to construct the constraints. Finally, we obtain the model by globally optimizing the position and orientation of the planes while considering the constraints. Our experimental results demonstrate that our proposed algorithm not only strictly enforces geometric constraints but also closely fits the input point cloud. Furthermore, our framework outperforms state-of-the-art methods in terms of shape recovery and constraint maintenance, as demonstrated by comparative evaluations.
C1 [Yu, Hang; Cao, Juan] Xiamen Univ, Sch Math Sci, Xiamen, Peoples R China.
   [Liu, Xiangrong; Chen, Zhonggui] Xiamen Univ, Sch Informat, Xiamen, Peoples R China.
C3 Xiamen University; Xiamen University
RP Chen, ZG (corresponding author), Xiamen Univ, Sch Informat, Xiamen, Peoples R China.
EM chenzhonggui@xmu.edu.cn
OI Chen, Zhonggui/0000-0002-9960-4896
FU National Natural Science Foundation of China [61972327, 62272402];
   National Natural Science Foundation of China [2022YZ040011]; Special
   Fund for Key Program of Science and Technology of Fujian Province
   [2022J01001]; Natural Science Foundation of Fujian Province
   [20720220037]; Fundamental Research Funds for the Central Universities
FX The research was supported by National Natural Science Foundation of
   China (61972327, 62272402), Special Fund for Key Program of Science and
   Technology of Fujian Province (2022YZ040011), Natural Science Foundation
   of Fujian Province (2022J01001), and Fundamental Research Funds for the
   Central Universities (20720220037).
CR Vo AV, 2015, ISPRS J PHOTOGRAMM, V104, P88, DOI 10.1016/j.isprsjprs.2015.01.011
   Bauchet JP, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3376918
   Chen ZY, 2022, ISPRS J PHOTOGRAMM, V194, P58, DOI 10.1016/j.isprsjprs.2022.09.017
   Dalitz C, 2017, IMAGE PROCESS ON LIN, V7, P184, DOI 10.5201/ipol.2017.208
   Derpanis KG, 2010, Overview of the ransac algorithm, V4, P2
   Erler Philipp, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P108, DOI 10.1007/978-3-030-58558-7_7
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Hazewinkel Michiel, 2001, Encyclopedia of Mathematics
   Huang L.L., Applied Mechanics and Materials
   Hulik R, 2014, J VIS COMMUN IMAGE R, V25, P86, DOI 10.1016/j.jvcir.2013.04.001
   Isack H, 2012, INT J COMPUT VISION, V97, P123, DOI 10.1007/s11263-011-0474-7
   Jenke P, 2006, COMPUT GRAPH FORUM, V25, P379, DOI 10.1111/j.1467-8659.2006.00957.x
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Kelly T, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130823
   Khaloo A, 2017, ADV ENG INFORM, V34, P1, DOI 10.1016/j.aei.2017.07.002
   Korman S, 2018, PROC CVPR IEEE, P6693, DOI 10.1109/CVPR.2018.00700
   Li L, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9050433
   Li LX, 2019, PROC CVPR IEEE, P2647, DOI 10.1109/CVPR.2019.00276
   Li YY, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964947
   Lim SP, 2014, ARTIF INTELL REV, V42, P59, DOI 10.1007/s10462-012-9329-z
   Lin YB, 2020, ISPRS J PHOTOGRAMM, V161, P208, DOI 10.1016/j.isprsjprs.2020.01.009
   Madsen K., 2004, METHODS NONLINEAR LE, V2nd
   Mandikal P, 2019, IEEE WINT CONF APPL, P1052, DOI 10.1109/WACV.2019.00117
   Monszpart A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766995
   Musialski P, 2013, COMPUT GRAPH FORUM, V32, P146, DOI 10.1111/cgf.12077
   Nan LL, 2017, IEEE I CONF COMP VIS, P2372, DOI 10.1109/ICCV.2017.258
   Nan LL, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778830
   Oesau S, 2016, COMPUT GRAPH FORUM, V35, P203, DOI 10.1111/cgf.12720
   Schnabel R, 2007, COMPUT GRAPH FORUM, V26, P214, DOI 10.1111/j.1467-8659.2007.01016.x
   Sharma G., 2020, ECCV, P261
   Sheung H., 2009, 2009 SIAMACM JOINT C, P13
   Wang SY, 2020, ISPRS J PHOTOGRAMM, V170, P29, DOI 10.1016/j.isprsjprs.2020.09.004
   Xu R, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555443
   Yan SM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2733, DOI 10.1109/ICCV48922.2021.00275
   Yang M. Y., 2010, INT C MACH CONTR GUI, V8, P95
   Zhang L, 2010, COMPUT GRAPH-UK, V34, P198, DOI 10.1016/j.cag.2010.03.006
NR 37
TC 0
Z9 0
U1 6
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 20
PY 2024
DI 10.1007/s00371-023-03241-x
EA JAN 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FJ2F6
UT WOS:001145320800001
DA 2024-08-05
ER

PT J
AU Zhao, ZJ
   Yang, H
   Liu, PY
   Nie, HT
   Zhang, ZB
   Li, CY
AF Zhao, Zijian
   Yang, Hang
   Liu, Peiyu
   Nie, Haitao
   Zhang, Zhongbo
   Li, Chunyu
TI Defocus blur detection via adaptive cross-level feature fusion and
   refinement
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Defocus blur detection; Homogeneous region; Adaptive cross-level feature
   fusion
AB Convolutional neural networks have achieved competitive performance in defocus blur detection (DBD). However, due to the different receptive fields of different convolutional layers, there are distinct differences in the features generated by these layers, and the complementary information between cross-level features cannot be fully utilized. Besides, there are still challenges to be solved in homogeneous regions. To tackle the above issues, we focus on both homogeneous region dataset augmentation and model design to propose a novel DBD model based on adaptive cross-level feature fusion and refinement. Specifically, in terms of homogeneous region dataset enhancement, a Laplace filter is used to extract the homogeneous region image patch of the training image to realize homogeneous region image augmentation, which improves the robustness of the model for DBD in the homogeneous region; in terms of model design, we propose an adaptive fusion mechanism with self-learning weights and design the adaptive cross-level feature fusion module, which adaptively discriminates between different levels of features and fuses them step-by-step. In addition, we design the cross-level feature refinement module and embed it into the network, which captures the complementary information of the cross-level features, and refines cross-level feature information from coarse to fine in the decoder stage. Experimental results on two commonly used datasets show that the proposed method outperforms 13 state-of-the-art approaches.
C1 [Zhao, Zijian; Yang, Hang; Nie, Haitao; Li, Chunyu] Chinese Acad Sci, Changchun Inst Opt Fine Mech & Phys, Changchun 130033, Peoples R China.
   [Zhao, Zijian] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Liu, Peiyu] AVIC Shenyang Aircraft Design & Res Inst, Shenyang 110087, Peoples R China.
   [Zhang, Zhongbo] Jilin Univ, Changchun 130012, Peoples R China.
C3 Chinese Academy of Sciences; Changchun Institute of Optics, Fine
   Mechanics & Physics, CAS; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; Aviation Industry Corporation of China
   (AVIC); Jilin University
RP Yang, H (corresponding author), Chinese Acad Sci, Changchun Inst Opt Fine Mech & Phys, Changchun 130033, Peoples R China.
EM yanghang@ciomp.ac.cn
RI Li, Chunyu/AAV-1600-2021; Zhao, Zijian/AGD-3257-2022
FU Chinese Academy of Sciences-Youth Innovation Promotion Association;
   Key-Area Research and Development Program of Guangdong Province
   [2019B010155003]; Science & Technology Development Project of Jilin
   Province; Key R D Programs [20210201078GX];  [2020220]
FX This work is supported by the Chinese Academy of Sciences-Youth
   Innovation Promotion Association, Grant Number 2020220, recipient Hang
   Yang; Key-Area Research and Development Program of Guangdong Province
   (Grant No. 2019B010155003); Science & Technology Development Project of
   Jilin Province, Key R &D Programs No. 20210201078GX.
CR Abuolaim Abdullah, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P111, DOI 10.1007/978-3-030-58607-2_7
   Cai RT, 2023, VISUAL COMPUT, V39, P4639, DOI 10.1007/s00371-022-02614-y
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Golestaneh SA, 2017, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2017.71
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jiang ZY, 2022, IEEE T IMAGE PROCESS, V31, P3494, DOI 10.1109/TIP.2022.3171424
   Jiang ZY, 2022, VISUAL COMPUT, DOI 10.1007/s00371-022-02713-w
   Karaali A, 2022, IEEE T IMAGE PROCESS, V31, P1097, DOI 10.1109/TIP.2021.3139243
   Lee J, 2019, PROC CVPR IEEE, P12214, DOI 10.1109/CVPR.2019.01250
   Li JX, 2023, IEEE T IMAGE PROCESS, V32, P1158, DOI 10.1109/TIP.2023.3240856
   Li JX, 2021, IEEE T IMAGE PROCESS, V30, P3748, DOI 10.1109/TIP.2021.3065171
   Li PX, 2018, PATTERN RECOGN, V76, P323, DOI 10.1016/j.patcog.2017.11.007
   Lin XY, 2022, NEUROCOMPUTING, V501, P88, DOI 10.1016/j.neucom.2022.06.023
   Liu RT, 2008, PROC CVPR IEEE, P954
   Liu ZY, 2023, VISUAL COMPUT, V39, P2881, DOI 10.1007/s00371-022-02499-x
   Liu ZY, 2020, VISUAL COMPUT, V36, P1823, DOI 10.1007/s00371-019-01778-4
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P5155, DOI 10.1109/TIP.2018.2847421
   Pang YW, 2016, IEEE T CYBERNETICS, V46, P2220, DOI 10.1109/TCYB.2015.2472478
   Park J, 2017, PROC CVPR IEEE, P2760, DOI 10.1109/CVPR.2017.295
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saad E, 2016, IEEE T IMAGE PROCESS, V25, P3141, DOI 10.1109/TIP.2016.2555702
   Shi JP, 2014, PROC CVPR IEEE, P2965, DOI 10.1109/CVPR.2014.379
   Su Bolan, 2011, P 19 ACM INT C MULT, P1397, DOI [DOI 10.1145/2072298.2072024, DOI 10.5555/1785794.1785825]
   Tang C, 2020, AAAI CONF ARTIF INTE, V34, P12063
   Tang C, 2019, PROC CVPR IEEE, P2695, DOI 10.1109/CVPR.2019.00281
   Tang C, 2016, IEEE SIGNAL PROC LET, V23, P1652, DOI 10.1109/LSP.2016.2611608
   Vu CT, 2012, IEEE T IMAGE PROCESS, V21, P934, DOI 10.1109/TIP.2011.2169974
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Yi X, 2016, IEEE T IMAGE PROCESS, V25, P1626, DOI 10.1109/TIP.2016.2528042
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang KH, 2024, IEEE T CIRC SYST VID, V34, P3755, DOI 10.1109/TCSVT.2023.3319330
   Zhang KH, 2020, PROC CVPR IEEE, P2734, DOI 10.1109/CVPR42600.2020.00281
   Zhang XX, 2016, J VIS COMMUN IMAGE R, V35, P257, DOI 10.1016/j.jvcir.2016.01.002
   Zhang Y, 2013, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2013.145
   Zhao JF, 2013, SIGNAL IMAGE VIDEO P, V7, P1173, DOI 10.1007/s11760-012-0381-6
   Zhao WD, 2021, PROC CVPR IEEE, P6929, DOI 10.1109/CVPR46437.2021.00686
   Zhao WD, 2021, IEEE T IMAGE PROCESS, V30, P5426, DOI 10.1109/TIP.2021.3084101
   Zhao WD, 2019, PROC CVPR IEEE, P8897, DOI 10.1109/CVPR.2019.00911
   Zhao WD, 2018, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2018.00325
   Zhao ZJ, 2022, COMPLEX INTELL SYST, V8, P4265, DOI 10.1007/s40747-022-00711-y
   Zhu X, 2013, IEEE T IMAGE PROCESS, V22, P4879, DOI 10.1109/TIP.2013.2279316
NR 43
TC 1
Z9 1
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 20
PY 2024
DI 10.1007/s00371-023-03229-7
EA JAN 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FK0N7
UT WOS:001145551000001
DA 2024-08-05
ER

PT J
AU Rabich, S
   Stotko, P
   Klein, R
AF Rabich, Saskia
   Stotko, Patrick
   Klein, Reinhard
TI FPO plus plus : efficient encoding and rendering of dynamic neural
   radiance fields by analyzing and enhancing Fourier PlenOctrees
SO VISUAL COMPUTER
LA English
DT Article
DE Neural radiance fields; Dynamic scenes; Real-time rendering; Encoding;
   Fourier transform
AB Fourier PlenOctrees have shown to be an efficient representation for real-time rendering of dynamic neural radiance fields (NeRF). Despite its many advantages, this method suffers from artifacts introduced by the involved compression when combining it with recent state-of-the-art techniques for training the static per-frame NeRF models. In this paper, we perform an in-depth analysis of these artifacts and leverage the resulting insights to propose an improved representation. In particular, we present a novel density encoding that adapts the Fourier-based compression to the characteristics of the transfer function used by the underlying volume rendering procedure and leads to a substantial reduction of artifacts in the dynamic model. We demonstrate the effectiveness of our enhanced Fourier PlenOctrees in the scope of quantitative and qualitative evaluations on synthetic and real-world scenes.
C1 [Rabich, Saskia; Stotko, Patrick; Klein, Reinhard] Univ Bonn, Bonn, Germany.
C3 University of Bonn
RP Rabich, S (corresponding author), Univ Bonn, Bonn, Germany.
EM srabich@cs.uni-bonn.de
FU Rheinische Friedrich-Wilhelms-Universitt Bonn (1040) [01IS22094E
   WEST-AI,]; Federal Ministry of Education and Research; Federal Ministry
   of Education and Research of Germany [KL 1142/11-2, FOR 2535]; DFG
FX This work has been funded by the Federal Ministry of Education and
   Research under grant no. 01IS22094E WEST-AI, by the Federal Ministry of
   Education and Research of Germany and the state of North-Rhine
   Westphalia as part of the Lamarr-Institute for Machine Learning and
   Artificial Intelligence, and additionally by the DFG project KL
   1142/11-2 (DFG Research Unit FOR 2535 Anticipating Human Behavior).
CR Attal B., 2021, PROC ADV NEURAL IN, VVolume 34, P26289
   Barron JT, 2023, IEEE I CONF COMP VIS, P19640, DOI 10.1109/ICCV51070.2023.01804
   Barron JT, 2022, PROC CVPR IEEE, P5460, DOI 10.1109/CVPR52688.2022.00539
   Barron JT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5835, DOI 10.1109/ICCV48922.2021.00580
   Bi Sai, 2020, ECCV, P294
   Cao A, 2023, PROC CVPR IEEE, P130, DOI 10.1109/CVPR52729.2023.00021
   Chen AP, 2022, LECT NOTES COMPUT SC, V13692, P333, DOI 10.1007/978-3-031-19824-3_20
   Chen Z., 2023, IEEE CVF C COMP VIS, p16,569
   CMU Graphics Lab, 2022, CARNEGIE MELLON U CM
   Collet A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766945
   Deng KL, 2022, PROC CVPR IEEE, P12872, DOI 10.1109/CVPR52688.2022.01254
   Du YL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14304, DOI 10.1109/ICCV48922.2021.01406
   Fang JM, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555383
   Fridovich-Keil S, 2023, PROC CVPR IEEE, P12479, DOI 10.1109/CVPR52729.2023.01201
   Fridovich-Keil S, 2022, PROC CVPR IEEE, P5491, DOI 10.1109/CVPR52688.2022.00542
   Gafni G, 2021, PROC CVPR IEEE, P8645, DOI 10.1109/CVPR46437.2021.00854
   Gao C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5692, DOI 10.1109/ICCV48922.2021.00566
   Garbin SJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14326, DOI 10.1109/ICCV48922.2021.01408
   Guo KW, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356571
   Guo Xiang, 2022, P AS C COMP VIS, P3757
   Hedman P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5855, DOI 10.1109/ICCV48922.2021.00582
   Isik M, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592415
   Jena S., 2022, EUR C COMP VIS WORKS
   Kasten Y., 2020, PROC NEURIPS, V33, P2492, DOI 10.48550/arXiv.2003.09852
   Kondo N., 2021, ARXIV
   Kurz A, 2022, LECT NOTES COMPUT SC, V13677, P254, DOI 10.1007/978-3-031-19790-1_16
   Li L., 2022, ADV NEURAL INF PROCE, V35, P13485
   Li TY, 2022, PROC CVPR IEEE, P5511, DOI 10.1109/CVPR52688.2022.00544
   Li ZQ, 2021, PROC CVPR IEEE, P6494, DOI 10.1109/CVPR46437.2021.00643
   Lin CH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5721, DOI 10.1109/ICCV48922.2021.00569
   Lin K.-E., 2021, P IEEE CVF INT C COM, P1749
   Lindell DB, 2021, PROC CVPR IEEE, P14551, DOI 10.1109/CVPR46437.2021.01432
   Liu J.-W., 2022, ADV NEURAL INFORM PR, V35, P36762
   Liu L., 2020, Advances in Neural Information Processing Systems, V33, P15651
   Lombardi S, 2021, ACM T GRAPHIC, V40, DOI [10.1145/3476576.3476608, 10.1145/3450626.3459863]
   Lombardi S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323020
   Ma L, 2022, PROC CVPR IEEE, P12851, DOI 10.1109/CVPR52688.2022.01252
   MakeHuman, 2022, MAKEHUMAN OPEN SOURC
   Meka A, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417814
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Mildenhall B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322980
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Neff T., 2021, COMPUT GRAPH FORUM, V40, P45, DOI [DOI 10.1111/CGF.14340, 10.1111/cgf.14340]
   Niemeyer M, 2020, PROC CVPR IEEE, P3501, DOI 10.1109/CVPR42600.2020.00356
   Park K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5845, DOI 10.1109/ICCV48922.2021.00581
   Peng SD, 2021, PROC CVPR IEEE, P9050, DOI 10.1109/CVPR46437.2021.00894
   Pumarola A, 2021, PROC CVPR IEEE, P10313, DOI 10.1109/CVPR46437.2021.01018
   Rebain D, 2021, PROC CVPR IEEE, P14148, DOI 10.1109/CVPR46437.2021.01393
   Reiser C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14315, DOI 10.1109/ICCV48922.2021.01407
   Shao RZ, 2023, PROC CVPR IEEE, P16632, DOI 10.1109/CVPR52729.2023.01596
   Sitzmann V, 2019, ADV NEUR IN, V32
   Song LC, 2023, IEEE T VIS COMPUT GR, V29, P2732, DOI 10.1109/TVCG.2023.3247082
   Suhail M, 2022, PROC CVPR IEEE, P8259, DOI 10.1109/CVPR52688.2022.00809
   Sun C, 2022, PROC CVPR IEEE, P5449, DOI 10.1109/CVPR52688.2022.00538
   Tancik M, 2022, PROC CVPR IEEE, P8238, DOI 10.1109/CVPR52688.2022.00807
   Tancik M, 2021, PROC CVPR IEEE, P2845, DOI 10.1109/CVPR46437.2021.00287
   Tretschk E, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12939, DOI 10.1109/ICCV48922.2021.01272
   twindom, TWINDOM TWINDOM DATA
   Wang L, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4641, DOI 10.1145/3474085.3475412
   Wang L, 2022, PROC CVPR IEEE, P13514, DOI 10.1109/CVPR52688.2022.01316
   Wang LY, 2023, PROCESS BIOCHEM, V129, P76, DOI 10.1016/j.procbio.2023.03.013
   Wang P, 2023, PROC CVPR IEEE, P4170, DOI 10.1109/CVPR52729.2023.00406
   Wang QQ, 2021, PROC CVPR IEEE, P4688, DOI 10.1109/CVPR46437.2021.00466
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2021, ARXIV
   Weng CY, 2022, PROC CVPR IEEE, P16189, DOI 10.1109/CVPR52688.2022.01573
   Wirth T., 2023, COMPUTER GRAPHICS FO, V42
   Wu L., 2022, P IEEE CVF C COMP VI, P16200
   Wu MY, 2020, PROC CVPR IEEE, P1679, DOI 10.1109/CVPR42600.2020.00175
   Xu H., 2021, ADV NEURAL INFORM PR, V34, P14955
   Yariv L., 2021, ADV NEURAL INFORM PR, P4805, DOI DOI 10.48550/ARXIV.2106.12052
   Yu A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5732, DOI 10.1109/ICCV48922.2021.00570
   Zhang JZ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459676
   Zhang K., 2020, ARXIV
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
NR 75
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4777
EP 4788
DI 10.1007/s00371-024-03475-3
EA JUN 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001251894500002
OA hybrid
DA 2024-08-05
ER

PT J
AU Zhou, J
   Li, YS
   Wang, MJ
   Li, NN
   Li, ZY
   Wang, WX
AF Zhou, Jun
   Li, Yaoshun
   Wang, Mingjie
   Li, Nannan
   Li, Zhiyang
   Wang, Weixiao
TI Robust point cloud normal estimation via multi-level critical point
   aggregation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Point cloud processing; Normal; Estimation; Local feature aggregation;
   Global feature refinement
AB We propose a multi-level critical point aggregation architecture based on a graph attention mechanism for 3D point cloud normal estimation, which can efficiently focus on locally important points during the feature extraction process. Wherein, the local feature aggregation (LFA) module and the global feature refinement (GFR) module are designed to accurately identify critical points which are geometrically closer to tangent plane for surface fitting at both local and global levels. Specifically, the LFA module captures significant local information from neighboring points with strong geometric correlations to the query point in the low-level feature space. The GFR module enhances the exploration of global geometric correlations in the high-level feature space, allowing the network to focus precisely on critical global points. To address indistinguishable features in the low-level space, we implement a stacked LFA structure. This structure transfers essential adjacent information across multiple levels, enabling deep feature aggregation layer by layer. Then the GFR module can leverage robust local geometric information and refines it into comprehensive global features. Our multi-level point-aware architecture improves the stability and accuracy of surface fitting and normal estimation, even in the presence of sharp features, high noise or anisotropic structures. Experimental results demonstrate that our method is competitive and achieves stable performance on both synthetic and real-world datasets. Code is available at https://github.com/CharlesLee96/NormalEstimation.
C1 [Zhou, Jun; Li, Yaoshun; Li, Nannan; Li, Zhiyang] Dalian Maritime Univ, Sch Informat Sci & Technol, Dalian, Peoples R China.
   [Wang, Mingjie] Zhejiang Sci Tech Univ, Dept Math, Hangzhou, Peoples R China.
   [Wang, Weixiao] Univ British Columbia, Sch Engn, Vancouver, BC, Canada.
C3 Dalian Maritime University; Zhejiang Sci-Tech University; University of
   British Columbia
RP Zhou, J (corresponding author), Dalian Maritime Univ, Sch Informat Sci & Technol, Dalian, Peoples R China.
EM zj.9004@gmail.com; lys96@dlmu.edu.cn; mingjiew@zstu.edu.cn;
   nannanli@dlmu.edu.cn; lizy0205@dlmu.edu.cn; wwx0612@mail.ubc.ca
FU Natural Science Foundation of China; China Postdoctoral Science
   Foundation [2021M690501]; Science Foundation of Zhejiang Sci-Tech
   University [22062338-Y]; Beijing Postdoctoral Science Foundation
   [2022-ZZ-069];  [62002040]
FX This research was supported in part by the Natural Science Foundation of
   China under Grants 62002040, in part by China Postdoctoral Science
   Foundation 2021M690501, in part by the Science Foundation of Zhejiang
   Sci-Tech University under Grant Number 22062338-Y and in part by Beijing
   Postdoctoral Science Foundation under Grant Number 2022-ZZ-069.
CR Alliez P., 2007, P 5 EUROGRAPHICS S G, P39
   Ben-Shabat Yizhak, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P20, DOI 10.1007/978-3-030-58452-8_2
   Ben-Shabat Y, 2019, PROC CVPR IEEE, P10104, DOI 10.1109/CVPR.2019.01035
   Boulch A, 2016, COMPUT GRAPH FORUM, V35, P281, DOI 10.1111/cgf.12983
   Boulch A, 2012, COMPUT GRAPH FORUM, V31, P1765, DOI 10.1111/j.1467-8659.2012.03181.x
   Cao JJ, 2022, IEEE T IND ELECTRON, V69, P921, DOI 10.1109/TIE.2021.3053904
   Cazals F, 2005, COMPUT AIDED GEOM D, V22, P121, DOI 10.1016/j.cagd.2004.09.004
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Guennebaud G, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239474, 10.1145/1276377.1276406]
   Guerrero P, 2018, COMPUT GRAPH FORUM, V37, P75, DOI 10.1111/cgf.13343
   Hackel T, 2017, Arxiv, DOI arXiv:1704.03847
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Hua BS, 2016, INT CONF 3D VISION, P92, DOI 10.1109/3DV.2016.18
   Kazhdan M, 2006, S GEOM PROC EUR ASS, DOI [DOI 10.2312/SGP/SGP06/061-070, 10.2312/SGP/SGP06/061-070]
   Lenssen Jan Eric, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11244, DOI 10.1109/CVPR42600.2020.01126
   Levin D, 1998, MATH COMPUT, V67, P1517, DOI 10.1090/S0025-5718-98-00974-0
   Li KQ, 2022, LECT NOTES COMPUT SC, V13692, P651, DOI 10.1007/978-3-031-19824-3_38
   Li Q, 2023, PROC CVPR IEEE, P13591, DOI 10.1109/CVPR52729.2023.01306
   Li Qing, 2022, Advances in Neural Information Processing Systems, V35, P4218
   Li S., 2023, P AAAI C ART INT, V37, P1396
   Mérigot Q, 2011, IEEE T VIS COMPUT GR, V17, P743, DOI 10.1109/TVCG.2010.261
   Mitra NJ, 2004, INT J COMPUT GEOM AP, V14, P261, DOI 10.1142/S0218195904001470
   Paszke A, 2017, PyTorch Tensors Dyn. Neural Netw. Python Strong GPU Accel, V6, P67
   STEWART GW, 1993, SIAM REV, V35, P551, DOI 10.1137/1035134
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Xiu Haoyi, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P2535, DOI 10.1145/3581783.3613762
   Zhang J, 2022, COMPUT AIDED DESIGN, V142, DOI 10.1016/j.cad.2021.103119
   Zhou HR, 2023, IEEE T PATTERN ANAL, V45, P946, DOI 10.1109/TPAMI.2022.3145877
   Zhou J, 2023, COMPUT AIDED DESIGN, V161, DOI 10.1016/j.cad.2023.103533
   Zhou J, 2022, COMPUT AIDED DESIGN, V142, DOI 10.1016/j.cad.2021.103121
   Zhou J, 2020, COMPUT AIDED DESIGN, V129, DOI 10.1016/j.cad.2020.102916
   Zhu Runsong, 2021, P IEEE CVF INT C COM, P6118
NR 33
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 21
PY 2024
DI 10.1007/s00371-024-03532-x
EA JUN 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UY0Q4
UT WOS:001251511100002
DA 2024-08-05
ER

PT J
AU Pan, YQ
   Chen, QH
   Fang, X
AF Pan, Yueqian
   Chen, Qiaohong
   Fang, Xian
TI DAMAF: dual attention network with multi-level adaptive complementary
   fusion for medical image segmentation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Medical image segmentation; Dual attention; Multi-level fusion
   attention; Multi-level skip attention
ID NET
AB Transformers have been widely applied in medical image segmentation due to their ability to establish excellent long-distance dependency through self-attention. However, relying solely on self-attention makes it difficult to effectively extract rich spatial and channel information from adjacent levels. To address this issue, we propose a novel dual attention model based on a multi-level adaptive complementary fusion mechanism, namely DAMAF. We first employ efficient attention and transpose attention to synchronously capture robust spatial and channel cures in a lightweight manner. Then, we design a multi-level fusion attention block to expand the complementarity of features at each level and enrich the contextual information, thereby gradually enhancing the correlation between high-level and low-level features. In addition, we develop a multi-level skip attention block to strengthen the adjacent-level information of the model through mutual fusion, which improves the feature expression ability of the model. Extensive experiments on the Synapse, ACDC, and ISIC-2018 datasets demonstrate that the proposed DAMAF achieves significantly superior results compared to competitors. Our code is publicly available at https://github.com/PanYging/DAMAF.
C1 [Pan, Yueqian; Chen, Qiaohong; Fang, Xian] Zhejiang Sci Tech Univ, Sch Comp Sci & Technol, Hangzhou 310018, Peoples R China.
C3 Zhejiang Sci-Tech University
RP Fang, X (corresponding author), Zhejiang Sci Tech Univ, Sch Comp Sci & Technol, Hangzhou 310018, Peoples R China.
EM yueqian64@gmail.com; chen_lisa@zstu.edu.cn; xianfang@zstu.edu.cn
FU Zhejiang Provincial Natural Science Foundation of China
FX No Statement Available
CR Ali A., 2021, C NEUR INF PROC SYST, V34, P20014
   Ates GC, 2023, Arxiv, DOI arXiv:2303.17696
   Azad Reza, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12535), P251, DOI 10.1007/978-3-030-66415-2_16
   Azad R, 2023, LECT NOTES COMPUT SC, V14277, P83, DOI 10.1007/978-3-031-46005-0_8
   Azad R, 2022, LECT NOTES COMPUT SC, V13583, P377, DOI 10.1007/978-3-031-21014-3_39
   Bernard O, 2018, IEEE T MED IMAGING, V37, P2514, DOI 10.1109/TMI.2018.2837502
   Cao Hu, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13803), P205, DOI 10.1007/978-3-031-25066-8_9
   Chen J., 2021, TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Codella N, 2019, Arxiv, DOI [arXiv:1902.03368, 10.48550/arXiv.1902.03368]
   Dong Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P323, DOI 10.1007/978-3-030-58604-1_20
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Feng SL, 2020, IEEE T MED IMAGING, V39, P3008, DOI 10.1109/TMI.2020.2983721
   Graham B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12239, DOI 10.1109/ICCV48922.2021.01204
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   He AL, 2023, IEEE T MED IMAGING, V42, P2763, DOI 10.1109/TMI.2023.3264513
   Heidari M, 2023, IEEE WINT CONF APPL, P6191, DOI 10.1109/WACV56688.2023.00614
   Hu J., 2018, SQUEEZE AND EXCITATI, P7132
   Huang HM, 2020, INT CONF ACOUST SPEE, P1055, DOI [10.1109/icassp40776.2020.9053405, 10.1109/ICASSP40776.2020.9053405]
   Huang XH, 2023, IEEE T MED IMAGING, V42, P1484, DOI 10.1109/TMI.2022.3230943
   Isensee F, 2021, NAT METHODS, V18, P203, DOI 10.1038/s41592-020-01008-z
   Jha D, 2020, COMP MED SY, P558, DOI 10.1109/CBMS49503.2020.00111
   Karaali Ali, 2022, Pattern Recognition and Artificial Intelligence: Third International Conference, ICPRAI 2022, Proceedings, Part I. Lecture Notes in Computer Science (13363), P198, DOI 10.1007/978-3-031-09037-0_17
   Li JJ, 2024, IEEE T MED IMAGING, V43, P64, DOI 10.1109/TMI.2023.3289859
   Li S, 2021, arXiv
   Lin AL, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3178991
   Lin T.Y., P IEEE C COMPUTER VI, P2117
   Lin X., 2021, IEEE Trans. Multimedia
   Lin XA, 2023, IEEE J BIOMED HEALTH, V27, P3501, DOI 10.1109/JBHI.2023.3266977
   Lin X, 2023, IEEE T MED IMAGING, V42, P2325, DOI 10.1109/TMI.2023.3247814
   Liu Qianying, 2023, ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1, DOI 10.1109/ICASSP49357.2023.10096379
   Liu RH, 2021, IEEE T MED IMAGING, V40, P3446, DOI 10.1109/TMI.2021.3087857
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Nazir A, 2022, IEEE T IMAGE PROCESS, V31, P880, DOI 10.1109/TIP.2021.3136619
   Oktay O., 2022, Medical Imaging with Deep Learning
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruan Jiacheng, 2022, 2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), P1150, DOI 10.1109/BIBM55620.2022.9995040
   Shao ZK, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15082138
   Shen ZR, 2021, IEEE WINT CONF APPL, P3530, DOI 10.1109/WACV48630.2021.00357
   Shit S, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2147
   Shu YC, 2023, IEEE T MULTIMEDIA, V25, P1700, DOI 10.1109/TMM.2022.3154159
   Tan R., 2020, P IEEE CVF C COMP VI, DOI [10.1109/CVPR42600.2020.01079, DOI 10.1109/CVPR42600.2020.01079]
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HY, 2022, INT CONF ACOUST SPEE, P2390, DOI 10.1109/ICASSP43922.2022.9746172
   Wang WX, 2021, LECT NOTES COMPUT SC, V12901, P109, DOI 10.1007/978-3-030-87193-2_11
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu HS, 2022, MED IMAGE ANAL, V76, DOI 10.1016/j.media.2021.102327
   Wu YK, 2023, NONLINEAR DYNAM, V111, P2161, DOI [10.1007/s11071-022-07935-0, 10.1007/s00521-022-07859-1]
   Xie BQ, 2023, IEEE T IMAGE PROCESS, V32, P4407, DOI 10.1109/TIP.2023.3299190
   Xu Guoping, 2021, arXiv
   Yao Chang, 2022, 2022 5th International Conference on Information Communication and Signal Processing (ICICSP), P280, DOI 10.1109/ICICSP55539.2022.10050624
   Yin HT, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3293887
   Yu Q, 2022, IEEE T IMAGE PROCESS, V31, P5893, DOI 10.1109/TIP.2022.3203223
   Zha HF, 2021, COMPUT ANIMAT VIRT W, V32, DOI 10.1002/cav.2022
   Zhang TW, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2022.3189961
   Zhang TW, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14102395
   Zhang TW, 2022, PATTERN RECOGN, V123, DOI 10.1016/j.patcog.2021.108365
   Zhang TW, 2020, ISPRS J PHOTOGRAMM, V167, P123, DOI 10.1016/j.isprsjprs.2020.05.016
   Zhang YD, 2021, LECT NOTES COMPUT SC, V12901, P14, DOI 10.1007/978-3-030-87193-2_2
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
   Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1
   Zhu CZ, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2096
NR 62
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 20
PY 2024
DI 10.1007/s00371-024-03543-8
EA JUN 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UW9Q5
UT WOS:001251223700003
DA 2024-08-05
ER

PT J
AU Liu, QP
   Tang, Q
   Su, B
   Bu, XH
   Hanajima, N
   Wang, ML
AF Liu, Qunpo
   Tang, Qi
   Su, Bo
   Bu, Xuhui
   Hanajima, Naohiko
   Wang, Manli
TI Wire rope damage detection based on a uniform-complementary binary
   pattern with exponentially weighted guide image filtering
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Guided filtering; Local binary pattern; Image enhancement; Defect
   detection
ID INVARIANT TEXTURE CLASSIFICATION; GRAY-SCALE; FUSION
AB In response to the problem of unclear texture structure in steel wire rope images caused by complex and uncertain lighting conditions, resulting in inconsistent LBP feature values for the same structure, this paper proposes a steel wire surface damage recognition method based on exponential weighted guided filtering and complementary binary equivalent patterns. Leveraging the phenomenon of Mach bands in vision, we introduce a guided filtering method based on local exponential weighting to enhance texture details by applying exponential mapping to evaluate pixel differences within local window regions during image filtering. Additionally, we propose complementary binary equivalent pattern descriptors as neighborhood difference symbol information representation operators to reduce feature dimensionality while enhancing the robustness of binary encoding against interference. Experimental results demonstrate that compared to classical guided filtering algorithms, our image enhancement method achieves improvements in PSNR and SSIM mean values by more than 32.5% and 18.5%, respectively, effectively removing noise while preserving image edge structures. Moreover, our algorithm achieves a classification accuracy of 99.3% on the steel wire dataset, with a processing time of only 0.606 s per image.
C1 [Liu, Qunpo; Tang, Qi; Su, Bo; Bu, Xuhui; Wang, Manli] Henan Polytech Univ, Sch Elect Engn & Automat, Jiaozuo, Henan, Peoples R China.
   [Liu, Qunpo; Su, Bo; Bu, Xuhui; Hanajima, Naohiko] Henan Int Joint Lab Direct Dr & Control Intelligen, Jiaozuo, Henan, Peoples R China.
   [Hanajima, Naohiko] Muroran Inst Technol, Coll Informat & Syst, 27-1 Mizumoto Cho, Muroran, Hokkaido 0508585, Japan.
C3 Henan Polytechnic University; Muroran Institute of Technology
RP Liu, QP (corresponding author), Henan Polytech Univ, Sch Elect Engn & Automat, Jiaozuo, Henan, Peoples R China.; Liu, QP (corresponding author), Henan Int Joint Lab Direct Dr & Control Intelligen, Jiaozuo, Henan, Peoples R China.
EM lqpny@hpu.edu.cn; Tangqi556@163.com
FU Innovative Scientists and Technicians Team of Henan Provincial High
   Education; Natural Science Foundation of Henan Province [162300410126];
   National Natural Science Foundation of China [U1804147]; Science and
   Technology Project of Henan Province [212102210508]; Henan Province
   Science and Technology Research Projects [242102220113];  [20IRTSTHN019]
FX The Natural Science Foundation of Henan Province (162300410126). This
   work is partially supported by the National Natural Science Foundation
   of China (U1804147), Science and Technology Project of Henan Province
   (212102210508), Henan Province Science and Technology Research
   Projects(242102220113), and Innovative Scientists and Technicians Team
   of Henan Provincial High Education (20IRTSTHN019).
CR Al Saidi I, 2022, J IMAGING, V8, DOI 10.3390/jimaging8070200
   Arora N, 2024, MULTIMED TOOLS APPL, V83, P9817, DOI 10.1007/s11042-023-15832-w
   Chakraborti T, 2018, IEEE SIGNAL PROC LET, V25, P635, DOI 10.1109/LSP.2018.2817176
   Chang XD, 2023, WEAR, V514, DOI 10.1016/j.wear.2022.204582
   Dabov K, 2009, SPARS 09 SIGNAL PROC
   Nguyen DT, 2010, IEEE IMAGE PROC, P4609, DOI 10.1109/ICIP.2010.5651633
   Hao HQ, 2019, ENG APPL ARTIF INTEL, V79, P34, DOI 10.1016/j.engappai.2018.12.004
   Hao SJ, 2016, SIGNAL PROCESS, V120, P789, DOI 10.1016/j.sigpro.2015.02.017
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Jie T, 2019, INSIGHT, V61, P521, DOI 10.1784/insi.2019.61.9.521
   Karanwal S, 2021, DIGIT SIGNAL PROCESS, V110, DOI 10.1016/j.dsp.2020.102948
   Kinge S, 2023, MATH BIOSCI ENG, V20, P10063, DOI 10.3934/mbe.2023442
   Kou F, 2015, IEEE T IMAGE PROCESS, V24, P4528, DOI 10.1109/TIP.2015.2468183
   Lei GY, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19020388
   Li CC, 2023, TRIBOL INT, V187, DOI 10.1016/j.triboint.2023.108745
   Li X, 2020, J SENSORS, V2020, DOI 10.1155/2020/6419371
   Li ZG, 2015, IEEE T IMAGE PROCESS, V24, P120, DOI 10.1109/TIP.2014.2371234
   Liang ZF, 2014, IEICE ELECTRON EXPR, V11, DOI 10.1587/elex.11.20141002
   Liao JS, 2023, CAN J REMOTE SENS, V49, DOI 10.1080/07038992.2023.2246158
   Liu J, 2023, SUSTAINABILITY-BASEL, V15, DOI 10.3390/su15032347
   Liu QP, 2024, VISUAL COMPUT, V40, P545, DOI 10.1007/s00371-023-02800-6
   Liu SW, 2020, J NONDESTRUCT EVAL, V39, DOI 10.1007/s10921-020-00732-y
   Mathew D, 2023, MULTIMED TOOLS APPL, DOI 10.1007/s11042-023-14869-1
   Mazurek P, 2023, SUSTAINABILITY-BASEL, V15, DOI 10.3390/su15065441
   Neslusan M, 2019, J MAGN MAGN MATER, V484, P179, DOI 10.1016/j.jmmm.2019.04.017
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Ojala T, 2000, LECT NOTES COMPUT SC, V1842, P404
   Pei PP, 2022, LASER OPTOELECTRON P, V59, DOI 10.3788/LOP202259.1210001
   Peng YX, 2023, FRICTION, V11, P763, DOI 10.1007/s40544-022-0665-y
   Qiao YL, 2021, IET IMAGE PROCESS, V15, P2372, DOI 10.1049/ipr2.12220
   Ram I, 2013, INT CONF ACOUST SPEE, P1350, DOI 10.1109/ICASSP.2013.6637871
   Rostami J, 2020, STRUCT HEALTH MONIT, V19, P481, DOI 10.1177/1475921719855915
   Roy SwalpaKumar., 2018, arXiv
   Shi ZL, 2021, IEEE T IMAGE PROCESS, V30, P7472, DOI 10.1109/TIP.2021.3106812
   Song WY, 2021, IEEE GEOSCI REMOTE S, V18, P1069, DOI 10.1109/LGRS.2020.2990711
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Tian J, 2022, ENTROPY-SWITZ, V24, DOI 10.3390/e24070981
   Tian J, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3096288
   Tian J, 2020, INSIGHT, V62, P98, DOI 10.1784/insi.2020.62.2.98
   Ullah A, 2022, MATH PROBL ENG, V2022, DOI 10.1155/2022/8013474
   Wang HY, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3216670
   Wang HY, 2020, IEEE T INSTRUM MEAS, V69, P7437, DOI 10.1109/TIM.2020.2983232
   Wiener N., 1942, Tech. rep.
   Wu GG, 2022, MEAS CONTROL-UK, V55, P308, DOI 10.1177/00202940211065627
   Xia H, 2021, IEEE SENS J, V21, P18497, DOI 10.1109/JSEN.2021.3088158
   Xu YX, 2021, ISPRS INT J GEO-INF, V10, DOI 10.3390/ijgi10100658
   Xue SH, 2020, ENTROPY-SWITZ, V22, DOI 10.3390/e22020209
   Yao HT, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14010127
   Yuan YL, 2022, INT T ELECTR ENERGY, V2022, DOI 10.1155/2022/1066163
   Zhang H, 2023, J BRIDGE ENG, V28, DOI 10.1061/JBENF2.BEENG-6021
   Zhang N, 2023, J SOUND VIB, V567, DOI 10.1016/j.jsv.2023.117910
   Zhang YC, 2023, MEASUREMENT, V218, DOI 10.1016/j.measurement.2023.113184
   Zhang YC, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3284927
NR 55
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 18
PY 2024
DI 10.1007/s00371-024-03538-5
EA JUN 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UQ5T1
UT WOS:001249543600002
DA 2024-08-05
ER

PT J
AU Xiong, JB
   Zou, SN
   Tang, J
   Tjahjadi, T
AF Xiong, Jianbo
   Zou, Shinan
   Tang, Jin
   Tjahjadi, Tardi
TI MCDGait: multimodal co-learning distillation network with
   spatial-temporal graph reasoning for gait recognition in the wild
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Biometrics; Human identification; Gait recognition; Multimodal
   co-learning distillation; Spatial-temporal graph reasoning
AB Gait recognition in the wild has attracted the attention of the academic community. However, existing unimodal algorithms cannot achieve the same performance on in-the-wild datasets as in-the-lab datasets because unimodal data have many limitations in-the-wild environments. Therefore, we propose a multimodal approach combining silhouettes and skeletons and formulate the multimodal gait recognition problem as a multimodal co-learning problem. In particular, we propose a multimodal co-learning distillation network (MCDGait) that integrates two sub-networks processing unimodal data into a single fusion network. Based on the semantic consistency of different modalities and the paradigm of deep mutual learning, the performance of the entire network is continuously improved via the bidirectional knowledge distillation between the sub-networks and fusion network. Inspired by the observation that specific body parts or joints exhibit unique motion characteristics and have linkage with other parts or joints during walking, we propose a spatial-temporal graph reasoning module (ST-GRM). This module represents the parts or joints as graph nodes and the motion linkages between them as edges. By utilizing dynamic graph generator, the module implicitly captures the dynamic changes of the human body. Based on the generated graphs, the independent spatial-temporal linkage feature of each part and the interactive spatial-temporal linkage feature are aggregated simultaneously. Extensive experiments conducted on two in-the-wild datasets demonstrate the state-of-the-art performance of the proposed method. The average rank-1 accuracy on datasets Gait3D and GREW is 50.90% and 58.06%, respectively. The source code can be obtained from https://github.com/BoyeXiong/MCDGait.
C1 [Xiong, Jianbo; Zou, Shinan; Tang, Jin] Cent South Univ, Sch Automation, Changsha, Peoples R China.
   [Tjahjadi, Tardi] Univ Warwick, Sch Engn, Coventry, England.
C3 Central South University; University of Warwick
RP Tang, J (corresponding author), Cent South Univ, Sch Automation, Changsha, Peoples R China.
EM tjin@csu.edu.cn
CR Castro FM, 2020, NEURAL COMPUT APPL, V32, P14173, DOI 10.1007/s00521-020-04811-z
   Chao Fan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14213, DOI 10.1109/CVPR42600.2020.01423
   Chao HQ, 2019, AAAI CONF ARTIF INTE, P8126
   Dong Y., 2024, P AAAI C ART INT, V38, P1600, DOI DOI 10.1609/AAAI.V38I2.27926
   Fan C, 2023, PROC CVPR IEEE, P9707, DOI 10.1109/CVPR52729.2023.00936
   Fan LF, 2019, IEEE I CONF COMP VIS, P5723, DOI 10.1109/ICCV.2019.00582
   Gao LQ, 2023, VISUAL COMPUT, V39, P3417, DOI 10.1007/s00371-023-02979-8
   Han K, 2022, Arxiv, DOI arXiv:2206.00272
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hou JC, 2018, IEEE TETCI, V2, P117, DOI 10.1109/TETCI.2017.2784878
   Huang XH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12889, DOI 10.1109/ICCV48922.2021.01267
   Huang Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14900, DOI 10.1109/ICCV48922.2021.01465
   Jang E, 2017, Arxiv, DOI arXiv:1611.01144
   Kingma D. P., 2014, arXiv
   Kumar P, 2019, IEEE T FUZZY SYST, V27, P956, DOI 10.1109/TFUZZ.2018.2870590
   Li GD, 2023, APPL INTELL, V53, P1535, DOI 10.1007/s10489-022-03543-y
   Li GH, 2019, IEEE I CONF COMP VIS, P9266, DOI 10.1109/ICCV.2019.00936
   Lin BB, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3054, DOI 10.1145/3394171.3413861
   Lin BB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14628, DOI 10.1109/ICCV48922.2021.01438
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu RH, 2023, IEEE T MED IMAGING, V42, P1083, DOI 10.1109/TMI.2022.3223683
   Ma K, 2023, PROC CVPR IEEE, P22076, DOI 10.1109/CVPR52729.2023.02114
   Mao YY, 2022, LECT NOTES COMPUT SC, V13663, P734, DOI 10.1007/978-3-031-20062-5_42
   Marín-Jiménez MJ, 2021, IEEE T INF FOREN SEC, V16, P5452, DOI 10.1109/TIFS.2021.3132579
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Papavasileiou I., 2021, Smart Health, V19, DOI DOI 10.1016/J.SMHL.2020.100162
   Park W, 2019, PROC CVPR IEEE, P3962, DOI 10.1109/CVPR.2019.00409
   Paszke A, 2019, ADV NEUR IN, V32
   Pei YL, 2021, 2021 IEEE 8TH INTERNATIONAL CONFERENCE ON DATA SCIENCE AND ADVANCED ANALYTICS (DSAA), DOI 10.1109/DSAA53316.2021.9564233
   Peng BY, 2019, IEEE I CONF COMP VIS, P5006, DOI 10.1109/ICCV.2019.00511
   Qi SY, 2018, LECT NOTES COMPUT SC, V11213, P407, DOI 10.1007/978-3-030-01240-3_25
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Rahate A, 2022, INFORM FUSION, V81, P203, DOI 10.1016/j.inffus.2021.12.003
   Saihui Hou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P382, DOI 10.1007/978-3-030-58545-7_22
   Seo S, 2020, IEEE ACCESS, V8, P140426, DOI 10.1109/ACCESS.2020.3006563
   Shiraga K, 2016, INT CONF BIOMETR
   Takemura Noriko, 2018, IPSJ Transactions on Computer Vision and Applications, V10, DOI 10.1186/s41074-018-0039-6
   Teepe T, 2021, IEEE IMAGE PROC, P2314, DOI 10.1109/ICIP42928.2021.9506717
   Tian Y., 2020, Contrastive representation distillation
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang WG, 2019, IEEE I CONF COMP VIS, P9235, DOI 10.1109/ICCV.2019.00933
   Wenguan Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8926, DOI 10.1109/CVPR42600.2020.00895
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yu SQ, 2006, INT C PATT RECOG, P441
   Zheng JK, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P6136, DOI 10.1145/3503161.3547897
   Zheng JK, 2022, PROC CVPR IEEE, P20196, DOI 10.1109/CVPR52688.2022.01959
   Zhu HD, 2023, INT CONF BIOMETR THE, DOI 10.1109/IJCB57857.2023.10448634
   Zhu JG, 2021, PROC CVPR IEEE, P9256, DOI 10.1109/CVPR46437.2021.00914
   Zhu Z., 2021, ICCV, p14 789, DOI DOI 10.1109/ICCV48922.2021.01452
NR 51
TC 0
Z9 0
U1 7
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 15
PY 2024
DI 10.1007/s00371-024-03426-y
EA JUN 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UG1B9
UT WOS:001246803800001
DA 2024-08-05
ER

PT J
AU Ma, Y
   Wang, M
   Lu, GY
   Sun, YJ
AF Ma, Ying
   Wang, Meng
   Lu, Guangyun
   Sun, Yajun
TI Multi-label semantic sharing based on graph convolutional network for
   image-to-text retrieval
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image-text matching; Transformer; Multi-label; Graph convolutional
   network
ID REGULARIZATION
AB Cross-modal hashing has attracted widespread attention due to its ability to reduce the complexity of storage and retrieval. However, many existing methods use a symbolic function to map hash codes, which leads to a loss of semantic information when mapping the original features to a low-dimensional space and consequently decreases retrieval accuracy. To address these challenges, we propose a cross-modal hashing method called Multi-Label Semantic Sharing based on Graph Convolutional Network for Image-to-Text Retrieval (MLSS). Specifically, we employ dual transformers to encode multimodal data and utilize CNN to assist in extracting local information from images, thereby enhancing the matching capability between images and text. Additionally, we design a multi-label semantic sharing module based on a graph convolutional network, which learns a unified multi-label classifier and establishes a semantic bridge between the feature representation space and the hashing space for images and text. By leveraging multi-label semantic information to guide feature and hash learning, MLSS generates hash codes that preserve semantic similarity information, leading to a significant improvement in the performance of image-to-text retrieval. Our experiments on three benchmark datasets demonstrate that MLSS outperforms several state-of-the-art cross-modal retrieval methods. Our code can be found at https://github.com/My1new/MLSS.
C1 [Ma, Ying; Sun, Yajun] Guangxi Univ Sci & Technol, Coll Sci, Liuzhou 545000, Peoples R China.
   [Wang, Meng] Guangxi Univ Sci & Technol, Tus Coll Digit, Liuzhou 545000, Peoples R China.
   [Lu, Guangyun] Liuzhou Inst Technol, Coll Informat Sci & Engn, Liuzhou 545000, Peoples R China.
C3 Guangxi University of Science & Technology; Guangxi University of
   Science & Technology
RP Wang, M (corresponding author), Guangxi Univ Sci & Technol, Tus Coll Digit, Liuzhou 545000, Peoples R China.
EM MaYing5w@163.com; mwang007@gxust.edu.cn; 170024422@qq.com;
   2818133456@qq.com
CR Cao Y, 2018, LECT NOTES COMPUT SC, V11205, P207, DOI 10.1007/978-3-030-01246-5_13
   Cao Y, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1445, DOI 10.1145/2939672.2939812
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Chen ZD, 2018, AAAI CONF ARTIF INTE, P274
   Chua TS, 2009, P ACM INT C IM VID R, P1
   Cong Bai, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P525, DOI 10.1145/3372278.3390711
   Dong XF, 2022, IEEE T CIRC SYST VID, V32, P1634, DOI 10.1109/TCSVT.2021.3075242
   Fan L., 2024, Advances in Neural Information Processing Systems, V36
   Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01
   Gan C, 2016, PROC CVPR IEEE, P87, DOI 10.1109/CVPR.2016.17
   Gu W, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P159, DOI 10.1145/3323873.3325045
   Huiskes M.J., 2008, P ACM INT C MULT INF, P39, DOI 10.1145/1460096
   Jiang D, 2023, PROC CVPR IEEE, P2787, DOI 10.1109/CVPR52729.2023.00273
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Kingma D. P., 2014, arXiv
   Kou FF, 2019, IEEE T VEH TECHNOL, V68, P11588, DOI 10.1109/TVT.2018.2890405
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li ZY, 2022, NEUROCOMPUTING, V483, P148, DOI 10.1016/j.neucom.2022.02.007
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu JW, 2021, IEEE T IMAGE PROCESS, V30, P332, DOI 10.1109/TIP.2020.3036735
   Lu X, 2019, SIGNAL PROCESS, V154, P217, DOI 10.1016/j.sigpro.2018.09.007
   Maas A.L., 2013, ICML WORK DEEP LEARN, V28
   Meng M, 2024, IEEE T CIRC SYST VID, V34, P1914, DOI 10.1109/TCSVT.2023.3293104
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Peng YX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3284750
   Pennington J., 2014, P C EMP METH NAT LAN, P1532, DOI DOI 10.3115/V1/D14-1162
   Radford A, 2021, PR MACH LEARN RES, V139
   Ramesh Aditya., 2022, ARXIV PREPRINT ARXIV, V1.2, P3, DOI DOI 10.48550/ARXIV.2204.06125
   Sennrich R, 2016, Arxiv, DOI [arXiv:1508.07909, DOI 10.48550/ARXIV.1508.07909]
   Shi L, 2022, INT J INTELL SYST, V37, P4393, DOI 10.1002/int.22723
   Tu Junfeng, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P453, DOI 10.1145/3503161.3548187
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang D, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3890
   Wang SJ, 2020, IEEE WINT CONF APPL, P1497, DOI 10.1109/WACV45572.2020.9093614
   Wang XZ, 2020, NEUROCOMPUTING, V400, P255, DOI 10.1016/j.neucom.2020.03.019
   Xia DL, 2020, MULTIMED TOOLS APPL, V79, P1339, DOI 10.1007/s11042-019-08238-0
   Xie D, 2020, IEEE T IMAGE PROCESS, V29, P3626, DOI 10.1109/TIP.2020.2963957
   Xu RQ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P982
   Xu X, 2017, IEEE T IMAGE PROCESS, V26, P2494, DOI 10.1109/TIP.2017.2676345
   Zhang R., 2022, PROC IEEECVF C COMPU, P8552
   Zhang X, 2018, LECT NOTES COMPUT SC, V11219, P614, DOI 10.1007/978-3-030-01267-0_36
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 44
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 10
PY 2024
DI 10.1007/s00371-024-03496-y
EA JUN 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TQ3D6
UT WOS:001242677100002
DA 2024-08-05
ER

PT J
AU Ruiz, C Jr
   de Jesús, O
   Serrano, C
   González, A
   Nonell, P
   Metaute, A
   Miralles, D
AF Ruiz Jr, Conrado
   de Jesus, Oscar
   Serrano, Claudia
   Gonzalez, Alejandro
   Nonell, Pau
   Metaute, Arnau
   Miralles, David
TI Bridging realities: training visuo-haptic object recognition models for
   robots using 3D virtual simulations
SO VISUAL COMPUTER
LA English
DT Article
DE Visuo-haptic object recognition; Synthetic datasets; 3D virtual
   simulations; Multimodal machine learning; Robotics
AB This paper proposes an approach for training visuo-haptic object recognition models for robots using synthetic datasets generated by 3D virtual simulations. In robotics, where visual object recognition has witnessed considerable progress due to an abundance of image datasets, the scarcity of diverse haptic samples has resulted in a noticeable gap in research on machine learning incorporating the haptic sense. Our proposed methodology addresses this challenge by utilizing 3D virtual simulations to create realistic synthetic datasets, offering a scalable and cost-effective solution to integrate haptic and visual cues for object recognition seamlessly. Acknowledging the importance of multimodal perception, particularly in robotic applications, our research not only closes the existing gap but envisions a future where intelligent agents possess a holistic understanding of their environment derived from both visual and haptic senses. Our experiments show that synthetic datasets can be used for training object recognition in haptic and visual modes by incorporating noise, performing some preprocessing, data augmentation, or domain adaptation. This work contributes to the advancement of multimodal machine learning toward a more nuanced and comprehensive robotic perception.
C1 [Ruiz Jr, Conrado; de Jesus, Oscar; Serrano, Claudia; Gonzalez, Alejandro; Nonell, Pau; Metaute, Arnau; Miralles, David] La Salle Univ Ramon Llull, Human Environm Res, Barcelona, Spain.
   [Ruiz Jr, Conrado] De La Salle Univ, Manila, Philippines.
C3 Universitat Ramon Llull; De La Salle University
RP Ruiz, C Jr (corresponding author), La Salle Univ Ramon Llull, Human Environm Res, Barcelona, Spain.; Ruiz, C Jr (corresponding author), De La Salle Univ, Manila, Philippines.
EM conrado.ruiz@salle.url.edu; oscar.dejesus@salle.url.edu;
   c.serrano@salle.url.edu; a.gonzalez@salle.url.edu;
   pau.nonell@salle.url.edu; arnau.metaute@salle.url.edu;
   david.miralles@salle.url.edu
OI Ruiz, Conrado Jr./0000-0002-0956-7201
FU Universitat Ramon Llull
FX No Statement Available
CR [Anonymous], UR3E collaborative robot arm that automates almost anything. UR3e collaborative robot arm that automates almost anything
   Chen XB, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531379
   Dong SY, 2017, IEEE INT C INT ROBOT, P137, DOI [10.1109/IROS.2017.8202149, 10.1109/CIIS.2017.29]
   Erickson Z., 2017, SEMISUPERVISED HAPTI
   Ericson C., 2004, REAL TIME COLLISION, DOI [10.1201/b14581, DOI 10.1201/B14581]
   Fanello SR, 2017, ROBOT AUTON SYST, V91, P151, DOI 10.1016/j.robot.2016.10.001
   Fernandes D., 2021, ARXIV
   Fougerolle Y.D., 2007, SPIE, V6356, P206
   Garrofé G, 2021, LECT NOTES COMPUT SC, V13002, P41, DOI 10.1007/978-3-030-89029-2_3
   Gielis J, 2003, AM J BOT, V90, P333, DOI 10.3732/ajb.90.3.333
   Gorges N., 2011, 2011 15th International Conference on Advanced Robotics, P15, DOI 10.1109/ICAR.2011.6088637
   Kulkarni S., 2024, IEEE ROBOT AUTOM LET, V4, P1
   Li B, 2024, PHYS OCCUP THER PEDI, V44, P336, DOI [10.1109/ICASSP49357.2023.10096330, 10.1080/01942638.2023.2248241]
   Luo S, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P3137, DOI 10.1109/IROS.2016.7759485
   Navarro-Guerrero N, 2023, AUTON ROBOT, V47, P377, DOI 10.1007/s10514-023-10091-y
   Nicolau F, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P21, DOI 10.1109/VR51125.2022.00019
   Nikolenko S.I., 2019, ARXIV
   Peter K., 1988, HAPTIC OBJECT RECOGN
   Schraml D, 2019, PROC SPIE, V11144, DOI 10.1117/12.2533485
   Spiers AJ, 2016, IEEE T HAPTICS, V9, P207, DOI 10.1109/TOH.2016.2521378
   Wu B., 2019, ARXIV
   Yuan WZ, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17122762
   Zhang MM, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4931, DOI 10.1109/IROS.2016.7759724
   Zhao Zhong-Qiu, 2019, IEEE Trans Neural Netw Learn Syst, V30, P3212, DOI 10.1109/TNNLS.2018.2876865
NR 24
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4661
EP 4673
DI 10.1007/s00371-024-03455-7
EA MAY 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001234536800004
OA hybrid, Green Submitted
DA 2024-08-05
ER

PT J
AU Li, FM
   Hou, JC
   Liu, S
   Liu, XJ
   Liu, LJ
   Lyu, Q
AF Li, Fumin
   Hou, Jucai
   Liu, Song
   Liu, Xiaojie
   Liu, Lianji
   Lyu, Qing
TI Blast furnace raw material granularity recognition model based on deep
   learning and multimodal fusion of 3D point cloud
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Sinter; Coke; Granularity identification; Deep learning; 3D point cloud
AB Accurate online recognition of raw material particle size is crucial for the green and efficient operation of blast furnaces. In this study, a high-resolution image acquisition system for blast furnace raw materials was established, and multiple batches of samples were collected for sintered ore and coke particle size analysis. To improve the robustness and generalization capability of the model, image enhancement techniques such as translation, scaling, cropping, random rotation, and brightness adjustment were applied to obtain the basic image datasets for sintered ore and coke. Among four mainstream instance segmentation algorithms, YOLACT++ was selected as the base network. To enhance the recognition accuracy of raw material similarities and subtle features, improvements were made to the YOLACT++ backbone network by replacing the original ResNet101 with DetNet59, and a composite loss function was constructed. Based on the enhanced YOLACT++ algorithm, a particle size recognition model was developed, achieving accuracy rates (PA), recall rates (Recall), F1 scores, and mean intersection over union (MIoU) all exceeding 90%. To address issues such as misidentification of sintered ore voids and over-segmentation in the particle size recognition model, an optimized 3D point cloud concavity and convexity segmentation algorithm was employed. By re-inputting the segmented 3D point cloud results into the particle size recognition model, problems such as void misidentification and over-segmentation were effectively resolved, achieving the multimodal fusion of deep learning and 3D point clouds and improving the accuracy and stability of sintered ore particle size recognition.
C1 [Li, Fumin; Hou, Jucai; Liu, Xiaojie; Lyu, Qing] North China Univ Sci & Technol, Coll Met & Energy, Tangshan 063009, Peoples R China.
   [Liu, Song] Coll Artificial Intelligence, Tangshan Coll, Tangshan 063000, Hebei, Peoples R China.
   [Liu, Lianji] Tangshan Iron & Steel Grp Co Ltd, Chief Engn Off, Tangshan 063000, Hebei, Peoples R China.
C3 North China University of Science & Technology
RP Liu, S (corresponding author), Coll Artificial Intelligence, Tangshan Coll, Tangshan 063000, Hebei, Peoples R China.
EM Neversettle0722@163.com
RI Liu, song/HJA-0326-2022
OI Liu, song/0000-0002-9605-4147
FU Tangshan city applied basic research science and technology plan project
   [23560301D]; Hebei Province Innovation Capacity Enhancement Programme
   Project [B202302007]; Tangshan Talent Funding Project; Tangshan Key
   Laboratory of Internet of Things and Mobile Internet New Technology
FX Thanks are give to Hebei Province Innovation Capacity Enhancement
   Programme Project (23560301D), the Tangshan Talent Funding Project
   (B202302007), the Tangshan Key Laboratory of Internet of Things and
   Mobile Internet New Technology.
CR Chen JW, 2022, ENERGY REP, V8, P10125, DOI 10.1016/j.egyr.2022.08.016
   Gao XD, 2022, MATERIALS, V15, DOI 10.3390/ma15176078
   Guo C., 2017, P 2017 NAT ANN C BLA, P439
   Habek GC, 2022, APPL ARTIF INTELL, V36, DOI 10.1080/08839514.2022.2145641
   He YC, 2022, NAT COMMUN, V13, DOI 10.1038/s41467-022-32586-5
   Heydari M, 2016, POWDER TECHNOL, V303, P260, DOI 10.1016/j.powtec.2016.09.020
   Holzinger G, 2022, POWDER TECHNOL, V395, P669, DOI 10.1016/j.powtec.2021.10.005
   Li FM., 2023, Iron Steel Res. J, V2023, P1
   Li ZH, 2023, IEEE T DEPEND SECURE, V20, P606, DOI 10.1109/TDSC.2022.3140899
   Lou SW, 2023, INFORM SCIENCES, V642, DOI 10.1016/j.ins.2023.119176
   Madeleine A., 2023, J. Comput. Des. Eng, V3, P2634
   Mitra T, 2017, MATER MANUF PROCESS, V32, P1179, DOI 10.1080/10426914.2016.1257133
   Mondal DN, 2022, STEEL RES INT, V93, DOI 10.1002/srin.202200035
   Pagnutti G, 2018, IMAGE VISION COMPUT, V70, P21, DOI 10.1016/j.imavis.2017.12.004
   Perpiñán J, 2023, J CLEAN PROD, V405, DOI 10.1016/j.jclepro.2023.137001
   Rahmatmand B, 2023, FUEL, V336, DOI 10.1016/j.fuel.2022.127077
   Wang R., 2017, P 2017 CHIN INT SYST, P505
   Xia H, 2022, DIGIT SIGNAL PROCESS, V121, DOI 10.1016/j.dsp.2021.103311
   [谢博 Xie Bo], 2019, [爆破, Blasting], V36, P43
   Yang XB, 2020, IEEE ACCESS, V8, P151555, DOI 10.1109/ACCESS.2020.3017560
   Yu Z.J., 2018, 3D digital analysis of blasting block based on point cloud identification, V18, P102
   Yuan Y, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9111876
   Zhang L., 2022, Sinter Pelletiz, V47, P15
   Zhang X., 2022, J Chongqing Technol Bus Univ (Nat Sci Edition), V39, P118
   Zhang XT., 2019, Modern Metall, V47, P55
   Zhang ZL, 2012, INT J MIN SCI TECHNO, V22, P739, DOI 10.1016/j.ijmst.2012.08.026
   Zhao F, 2022, INT J INTELL SYST, V37, P10988, DOI 10.1002/int.23030
NR 27
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 25
PY 2024
DI 10.1007/s00371-024-03449-5
EA MAY 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SJ3E7
UT WOS:001234039700003
DA 2024-08-05
ER

PT J
AU Yuan, YT
   Xu, SC
   Lin, SD
AF Yuan, Yuetao
   Xu, Shuchang
   Lin, Shudong
TI Jigsaw puzzle difficulty assessment and analysis of influencing factors
   based on deep learning method
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Jigsaw puzzle; Deep learning; Difficulty recognition
AB Jigsaw puzzle is a casual game that can be used for leisure and stress relief. This paper presents a novel algorithm for quantifying and estimating the time required for users to complete jigsaw puzzle games and providing game difficulty reference for game designers. Firstly, a difficulty quantification model is proposed. Then, based on observation and hypothesis, it is believed that jigsaw puzzle difficulty is related to elements such as texture in the puzzle. Finally, experimental validation demonstrates that jigsaw puzzle difficulty is related to the texture and number of repeated elements in the puzzle. The algorithm is tested on a large amount of jigsaw puzzle game datasets, subsequently verifying its effectiveness and accuracy. The main contribution of this algorithm is to provide a new quantitative evaluation method for jigsaw puzzle game difficulty, which can assist game designers in optimizing game difficulty and enhancing user experience. Our data, code, and model are available at CunHua-YYT/JigsawSort (github.com).
C1 [Yuan, Yuetao; Xu, Shuchang] Hangzhou Normal Univ, Sch Informat Sci & Technol, Hangzhou 311121, Peoples R China.
   [Lin, Shudong] Beijing Dailybread Co Ltd, Beijing 100089, Peoples R China.
C3 Hangzhou Normal University
RP Xu, SC (corresponding author), Hangzhou Normal Univ, Sch Informat Sci & Technol, Hangzhou 311121, Peoples R China.
EM xusc@hznu.edu.cn
FU Beijing Dailybread CO., LTD
FX No Statement Available
CR [Anonymous], About us
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Doersch C, 2017, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2017.226
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Donahue Jeff, 2017, Adversarial Feature Learning
   Feng TL, 2023, IEEE T PATTERN ANAL, V45, P8577, DOI 10.1109/TPAMI.2022.3232328
   Fissler P., 2018, Front. Aging Neurosci, V1, P408085
   Fissler P, 2017, TRIALS, V18, DOI 10.1186/s13063-017-2151-9
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrix M, 2019, IEEE T GAMES, V11, P320, DOI 10.1109/TG.2018.2791019
   Kim D, 2018, IEEE WINT CONF APPL, P793, DOI 10.1109/WACV.2018.00092
   Lee HY, 2017, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2017.79
   Lowe D. G., 1999, Computer vision, V2, P1150, DOI [10.1109/ICCV.1999.790410, DOI 10.1109/ICCV.1999.790410]
   Macqueen J., 1967, P 5 BERKELEY S MATH, V1
   Missura O, 2009, LECT NOTES ARTIF INT, V5808, P197, DOI 10.1007/978-3-642-04747-3_17
   Nef T, 2020, FRONT AGING NEUROSCI, V12, DOI 10.3389/fnagi.2020.00087
   Noroozi M., 2016, P EUROPEAN C COMPUTE
   Noroozi M, 2017, IEEE I CONF COMP VIS, P5899, DOI 10.1109/ICCV.2017.628
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Shaw P., 2018, Self-attention with relative position representations, P464
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sweetser P., 2005, Computers in Entertainment (CIE), V3, P3, DOI DOI 10.1145/1077246.1077253
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Vaswani A, 2017, ADV NEUR IN, V30
   Wojna Z, 2017, PROC INT CONF DOC, P844, DOI 10.1109/ICDAR.2017.143
   Zhang R, 2017, PROC CVPR IEEE, P645, DOI 10.1109/CVPR.2017.76
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 28
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 29
PY 2024
DI 10.1007/s00371-024-03387-2
EA APR 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PA0M6
UT WOS:001211242100002
DA 2024-08-05
ER

PT J
AU Zhang, C
   Da, FP
   Gai, SY
AF Zhang, Can
   Da, Feipeng
   Gai, Shaoyan
TI Point clouds feature frequency domain analysis based on multilayer
   perceptron
SO VISUAL COMPUTER
LA English
DT Review; Early Access
DE Point cloud understanding; Multilayer perceptron; Frequency domain
   analysis; Fourier transform
ID GRAPH CONVOLUTION; NETWORK
AB We propose a method for frequency domain feature analysis of point clouds based on the multilayer perceptron paradigm (MLP), named PFFA-MLP. By leveraging the Fourier transform and the latest MLP as a replacement for the transformer mechanism, PFFA-MLP maps point clouds to the frequency domain using a three-dimensional (3D) discrete Fourier transform. PFFA-MLP employs lightweight stacked DGCNN modules for initial feature extraction on multi-scale point cloud signals. It also includes a frequency domain feature analysis module, based on the MLP, which analyzes and aggregates the extracted multi-scale point cloud features. Point cloud understanding experiments are conducted on the ModelNet40, ScanObjectNN, and ShapeNet Part benchmarks. On ModelNet40, PFFA-MLP achieves an accuracy of 98% compared to PointMLP, while improving the detection speed by 7.2 times. On ScanObjectNN, the accuracy reaches 94% of PointMLP, while the detection speed is improved by 3.2 times. On ShapeNet Part, it achieves 97% of PointMLP. The experimental results demonstrate that PFFA-MLP achieves a favorable trade-off between training and inference speed. This method is applicable in scenarios that require efficient point cloud analysis while maintaining remarkable detection precision.
C1 [Zhang, Can; Da, Feipeng; Gai, Shaoyan] Southeast Univ, Sch Automat, 2 Sipailou St, Nanjing 210096, Jiangsu, Peoples R China.
   [Zhang, Can; Da, Feipeng; Gai, Shaoyan] Southeast Univ, Key Lab Measurement & Control Complex Syst Engn, Minist Educ, 2 Sipailou St, Nanjing 210096, Jiangsu, Peoples R China.
C3 Southeast University - China; Southeast University - China
RP Gai, SY (corresponding author), Southeast Univ, Sch Automat, 2 Sipailou St, Nanjing 210096, Jiangsu, Peoples R China.; Gai, SY (corresponding author), Southeast Univ, Key Lab Measurement & Control Complex Syst Engn, Minist Educ, 2 Sipailou St, Nanjing 210096, Jiangsu, Peoples R China.
EM 230218770@seu.edu.cn; dafp@seu.edu.cn; qxxymm@163.com
FU Special Project on Basic Research of Frontier Leading Technology of
   Jiangsu Province of China [BK20192004C]
FX This work was supported by the Special Project on Basic Research of
   Frontier Leading Technology of Jiangsu Province of China (Grant No.
   BK20192004C).
CR Phan AV, 2018, NEURAL NETWORKS, V108, P533, DOI 10.1016/j.neunet.2018.09.001
   Hua BS, 2018, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2018.00109
   Chen LF, 2023, VISUAL COMPUT, V39, P863, DOI 10.1007/s00371-021-02351-8
   COCHRAN WT, 1967, PR INST ELECTR ELECT, V55, P1664, DOI 10.1109/PROC.1967.5957
   COFFEY TC, 1970, AIAA J, V8, P1790, DOI 10.2514/3.49870
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   DUHAMEL P, 1990, SIGNAL PROCESS, V19, P259, DOI 10.1016/0165-1684(90)90158-U
   Engel N, 2021, IEEE ACCESS, V9, P134826, DOI 10.1109/ACCESS.2021.3116304
   Fei B, 2023, PATTERN RECOGN, V133, DOI 10.1016/j.patcog.2022.109051
   Gothwal H., 2011, J. Biomed. Sci. Eng., V4, P289
   Guo JY, 2022, PROC CVPR IEEE, P816, DOI 10.1109/CVPR52688.2022.00090
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Hamdi A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1, DOI 10.1109/ICCV48922.2021.00007
   Han D, 2021, INT C CONTR AUTOMAT, P342, DOI 10.23919/ICCAS52745.2021.9650008
   Han K, 2023, IEEE T PATTERN ANAL, V45, P87, DOI 10.1109/TPAMI.2022.3152247
   Hou WG, 2012, INVERSE PROBL SCI EN, V20, P287, DOI 10.1080/17415977.2011.603087
   Huang R, 2021, ISPRS J PHOTOGRAMM, V171, P310, DOI 10.1016/j.isprsjprs.2020.11.014
   Jiménez-Martínez J, 2013, WATER RESOUR RES, V49, P3007, DOI 10.1002/wrcr.20260
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Lee-Thorp J, 2022, Arxiv, DOI [arXiv:2105.03824, 10.48550/arXiv.2105.03824, DOI 10.48550/ARXIV.2105.03824]
   Li YY, 2018, ADV NEUR IN, V31
   Lian D., 2021, arXiv
   Lin HJ, 2023, PROC CVPR IEEE, P17682, DOI 10.1109/CVPR52729.2023.01696
   Lin MX, 2022, LECT NOTES COMPUT SC, V13663, P380, DOI 10.1007/978-3-031-20062-5_22
   Liu JM, 2024, IEEE T MULTIMEDIA, V26, P3897, DOI 10.1109/TMM.2023.3317998
   Ma X, 2022, Arxiv, DOI arXiv:2202.07123
   Melas-Kyriazi L, 2021, Arxiv, DOI arXiv:2105.02723
   Mironovova M, 2015, INT CONF FUTURE GEN
   Paul S, 2024, VISUAL COMPUT, V40, P5435, DOI 10.1007/s00371-023-03114-3
   Qi CR, 2017, ADV NEUR IN, V30
   Qin ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P763, DOI 10.1109/ICCV48922.2021.00082
   Qiu S, 2022, IEEE T MULTIMEDIA, V24, P1943, DOI 10.1109/TMM.2021.3074240
   Rozenberszki D, 2022, LECT NOTES COMPUT SC, V13693, P125, DOI 10.1007/978-3-031-19827-4_8
   Shi WJ, 2020, PROC CVPR IEEE, P1708, DOI 10.1109/CVPR42600.2020.00178
   Sifuzzaman M., 2009, J Phys Sci
   Song YA, 2022, J VIS COMMUN IMAGE R, V82, DOI 10.1016/j.jvcir.2021.103411
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Tolstikhin I, 2021, ADV NEUR IN, V34
   Touvron H, 2023, IEEE T PATTERN ANAL, V45, P5314, DOI 10.1109/TPAMI.2022.3206148
   Ulicny M., 2017, P 19 IR MACH VIS IM, P1
   Uy MA, 2019, IEEE I CONF COMP VIS, P1588, DOI 10.1109/ICCV.2019.00167
   Wang H., 2022, P ADV NEUR INF PROC, P29975
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wei MQ, 2023, IEEE T PATTERN ANAL, V45, P9374, DOI 10.1109/TPAMI.2023.3238516
   Wu B, 2023, MULTIMED TOOLS APPL, V82, P35949, DOI 10.1007/s11042-023-14639-z
   Wu Q., 2023, IEEE Trans. Multimedia, early access, DOI [10.1109/TMM.2023.3283881, DOI 10.1109/TMM.2023.3283881]
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Wu X., 2024, CVPR
   Wu Xiaoyang, 2022, NeurIPS
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Chang AX, 2015, Arxiv, DOI [arXiv:1512.03012, DOI 10.48550/ARXIV.1512.03012]
   Xu K, 2020, PROC CVPR IEEE, P1737, DOI 10.1109/CVPR42600.2020.00181
   Yin XYL, 2023, Arxiv, DOI arXiv:2312.13071
   Yu T, 2022, IEEE WINT CONF APPL, P3615, DOI 10.1109/WACV51458.2022.00367
   Yu WH, 2022, PROC CVPR IEEE, P10809, DOI 10.1109/CVPR52688.2022.01055
   Yu XM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12478, DOI 10.1109/ICCV48922.2021.01227
   Ze Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P326, DOI 10.1007/978-3-030-58592-1_20
   Zhang F, 2019, TRAIT SIGNAL, V36, P393, DOI 10.18280/ts.360503
   Zhang S, 2019, J VIS COMMUN IMAGE R, V61, P170, DOI 10.1016/j.jvcir.2019.03.005
   Zhang YF, 2023, INT J COMPUT VISION, V131, P3332, DOI 10.1007/s11263-023-01869-9
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhao HS, 2019, PROC CVPR IEEE, P5550, DOI 10.1109/CVPR.2019.00571
   Zhong YJ, 2022, PROC CVPR IEEE, P4494, DOI 10.1109/CVPR52688.2022.00446
NR 64
TC 0
Z9 0
U1 9
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 17
PY 2024
DI 10.1007/s00371-024-03380-9
EA APR 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NY9U8
UT WOS:001204140500006
DA 2024-08-05
ER

PT J
AU Wu, K
   Zhu, L
   Shi, WH
   Wang, WW
AF Wu, Kun
   Zhu, Lei
   Shi, Weihang
   Wang, Wenwu
TI Automated fabric defect detection using multi-scale fusion MemAE
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Defect detection; Memory-augmented auto-encoder; Multi-scale fusion;
   DeepSVDD
AB Fabric defect detection (FDD) is an essential part of the textile industry. To replace manual visual inspection, computer-vision-based solutions has been widely investigated in both unsupervised or supervised manner during the past few decades. However, precisely locating defective regions, especially for those tiny and inconspicuous ones, is still challenging. To this end, we propose an unsupervised learning approach for FDD via multi-scale memory-augmented auto-encoder (MemAE) fusion. The architecture of the proposed CNN model follows an encode-decode style. On the one hand, we explore the defect-free region reconstruction ability of the shallow CNN layers. On the other hand, the reconstruction ability of defect region is much emphasized in the deep layers. In the training phase of the network, we only use defect-free samples to learn the visual features of normal fabric textural representation. In the testing phase, a defective image is expected to be unpainted during the inference, and we further emphasize the defect regions by computing the pixel-level residual maps between the defective image and its corresponding reconstructed version. Additionally, a coarse-to-fine scheme is proposed to precisely locate the defect region by first introducing KNN and DeepSVDD to coarsely localize the defects, followed by a finer refinement using the obtained residual map. The proposed method is evaluated on two different types of datasets: the Periodic-pattern fabric database and the Yarn-dyed fabric database. The experimental results illustrate that our method consistently outperforms the state-of-the-art methods in terms of quality and robustness in both types of datasets with certain margin.
C1 [Wu, Kun; Zhu, Lei; Shi, Weihang; Wang, Wenwu] Wuhan Univ Sci & Technol, Sch Informat Sci & Engn, Wuhan, Peoples R China.
C3 Wuhan University of Science & Technology
RP Zhu, L (corresponding author), Wuhan Univ Sci & Technol, Sch Informat Sci & Engn, Wuhan, Peoples R China.
EM Athinklo@outlook.com; zhulei@wust.edu.cn; shiweihang1998@outlook.com;
   wangwenwu@wust.edu.cn
RI Zhu, Lei/KUD-1330-2024
OI Zhu, Lei/0000-0001-7001-5775
CR Gong D, 2019, IEEE I CONF COMP VIS, P1705, DOI 10.1109/ICCV.2019.00179
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Hanbay K, 2016, OPTIK, V127, P11960, DOI 10.1016/j.ijleo.2016.09.110
   Hongwei Zhang, 2019, 2019 IEEE 8th Data Driven Control and Learning Systems Conference (DDCLS). Proceedings, P1263, DOI 10.1109/DDCLS.2019.8908944
   Hu GH, 2020, TEXT RES J, V90, P247, DOI 10.1177/0040517519862880
   Jing JF, 2019, COLOR TECHNOL, V135, P213, DOI 10.1111/cote.12394
   Jing JF, 2013, J TEXT I, V104, P18, DOI 10.1080/00405000.2012.692940
   Kang XJ, 2020, IEEE ACCESS, V8, P221808, DOI 10.1109/ACCESS.2020.3041849
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Krizhevsky Alex, 2010, Convolutional deep belief networks on cifar-10
   Kumar A, 2002, IEEE T IND APPL, V38, P425, DOI 10.1109/28.993164
   LeCun Y, 2004, PROC CVPR IEEE, P97
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562
   Li C, 2021, SECUR COMMUN NETW, V2021, DOI 10.1155/2021/9948808
   Li P, 2019, MULTIMED TOOLS APPL, V78, P99, DOI 10.1007/s11042-017-5263-z
   Liu Z., 2019, P CHIN C PATT REC CO, P528
   Lizarraga-Morales RA, 2019, IEEE ACCESS, V7, P18042, DOI 10.1109/ACCESS.2019.2896078
   Mak KL, 2009, IMAGE VISION COMPUT, V27, P1585, DOI 10.1016/j.imavis.2009.03.007
   Mei S, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18041064
   Ngan HYT, 2010, PATTERN RECOGN, V43, P2132, DOI 10.1016/j.patcog.2009.12.001
   Raheja JL, 2013, OPTIK, V124, P6469, DOI 10.1016/j.ijleo.2013.05.004
   Redmon J., 2018, CoRR
   Reiss T, 2021, PROC CVPR IEEE, P2805, DOI 10.1109/CVPR46437.2021.00283
   Ruff L, 2018, PR MACH LEARN RES, V80
   Tong L, 2017, IEEE ACCESS, V5, P5947, DOI 10.1109/ACCESS.2017.2667890
   Rae JW, 2016, Arxiv, DOI arXiv:1610.09027
   Wei B., 2019, Artificial Intelligence on Fashion and Textiles, VVolume 849, P45, DOI [10.1007/978-3-319-99695-06, DOI 10.1007/978-3-319-99695-06]
   Weston J, 2015, Arxiv, DOI arXiv:1410.3916
   Xie HS, 2019, IEEE ACCESS, V7, P182320, DOI 10.1109/ACCESS.2019.2959880
   Zhou J, 2012, 2012 11TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2012), VOL 1, P21, DOI 10.1109/ICMLA.2012.13
   Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1
   Zhu DD, 2015, AUTEX RES J, V15, P226, DOI 10.1515/aut-2015-0001
NR 33
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 14
PY 2024
DI 10.1007/s00371-024-03358-7
EA APR 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NQ2Q6
UT WOS:001201858000001
DA 2024-08-05
ER

PT J
AU Thomas, J
   Raj, ED
AF Thomas, Jeena
   Raj, Ebin Deni
TI Improved image dehazing model with color correction transform-based dark
   channel prior
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Dark channel prior; Dehazing; Gamma correction; White balance; Color
   correction transform
ID NETWORK; ENHANCEMENT; WEATHER
AB The precise identification and positioning of objects require the restoration of hazy images. This paper proposes the implementation of Color Correction Transform based Dark Channel Prior (CCTDCP), a new prior-based dehazing technique. The CCTDCP strategy is intended to consider the brightening impact of haze and blue shading cast of hazy pictures. The proposed technique has introduced white balance color correction transform in order to preserve the originality of the image. This transform is responsible for generating properly color-corrected hazy images employing Euclidean norm and radial basis function. An integration of dark channel prior and gamma correction techniques is incorporated for effectual restoration. The experimental outcomes on benchmark datasets demonstrate superior PSNR and SSIM performance, better visual quality, and lesser execution time of the CCTDCP algorithm over the state-of-the-art techniques. The highest performance achieved using the suggested CCTDCP strategy has a PSNR of 28.3798 dB and SSIM value of 0.8317 on the O-Haze and I-Haze datasets, respectively.
C1 [Thomas, Jeena; Raj, Ebin Deni] Indian Inst Informat Technol, Dept Comp Sci & Engn, Kottayam 686635, Kerala, India.
RP Raj, ED (corresponding author), Indian Inst Informat Technol, Dept Comp Sci & Engn, Kottayam 686635, Kerala, India.
EM jeenaphd2019@iiitkottayam.ac.in; ebindeniraj@iiitkottayam.ac.in
FU DST [DST/WOS-A/ET-7/2021 (G)]; Department of Science and Technology
   (DST), Ministry of Science and Technology, Government of India, through
   Women Scientist Scheme-A (WOS-A)
FX This work is supported by the Department of Science and Technology
   (DST), Ministry of Science and Technology, Government of India, through
   Women Scientist Scheme-A (WOS-A) under sanction order No.
   DST/WOS-A/ET-7/2021 (G) (WISE KIRAN).
CR Afifi M, 2019, PROC CVPR IEEE, P1535, DOI 10.1109/CVPR.2019.00163
   Agrawal SC, 2023, VISUAL COMPUT, V39, P5763, DOI 10.1007/s00371-022-02694-w
   Ancuti CO, 2020, IEEE COMPUT SOC CONF, P1798, DOI 10.1109/CVPRW50498.2020.00230
   Ancuti CO, 2019, IEEE IMAGE PROC, P1014, DOI [10.1109/ICIP.2019.8803046, 10.1109/icip.2019.8803046]
   Ancuti CO, 2018, IEEE COMPUT SOC CONF, P867, DOI 10.1109/CVPRW.2018.00119
   Ancuti CO, 2019, IEEE SIGNAL PROC LET, V26, P1413, DOI 10.1109/LSP.2019.2932189
   Ancuti C, 2018, IEEE COMPUT SOC CONF, P1004, DOI 10.1109/CVPRW.2018.00134
   Ancuti C, 2016, IEEE IMAGE PROC, P2226, DOI 10.1109/ICIP.2016.7532754
   Babu GH, 2020, J VIS COMMUN IMAGE R, V72, DOI 10.1016/j.jveir.2020.102912
   Berman D, 2020, IEEE T PATTERN ANAL, V42, P720, DOI 10.1109/TPAMI.2018.2882478
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Borkar K, 2020, NEUROCOMPUTING, V400, P294, DOI 10.1016/j.neucom.2020.03.027
   Brunet D, 2012, IEEE T IMAGE PROCESS, V21, P1488, DOI 10.1109/TIP.2011.2173206
   Bui TM, 2018, IEEE T IMAGE PROCESS, V27, P999, DOI 10.1109/TIP.2017.2771158
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Chen Y, 2023, Neurocomputing
   Das B., 2022, Vis. Comput., P1
   Dong Y., 2022, IEEE Trans. Cybern.
   Dudhane A, 2019, IEEE COMPUT SOC CONF, P2014, DOI 10.1109/CVPRW.2019.00253
   Ehsan SM, 2021, IEEE ACCESS, V9, P89055, DOI 10.1109/ACCESS.2021.3090078
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Gandelsman Y, 2019, PROC CVPR IEEE, P11018, DOI 10.1109/CVPR.2019.01128
   Golts A, 2020, IEEE T IMAGE PROCESS, V29, P2692, DOI 10.1109/TIP.2019.2952032
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hodges C, 2019, PATTERN RECOGN LETT, V128, P70, DOI 10.1016/j.patrec.2019.08.013
   Hu Q, 2023, VISUAL COMPUT, V39, P997, DOI 10.1007/s00371-021-02380-3
   Huang SC, 2013, IEEE T IMAGE PROCESS, V22, P1032, DOI 10.1109/TIP.2012.2226047
   Jiang YT, 2017, COMPUT VIS IMAGE UND, V165, P17, DOI 10.1016/j.cviu.2017.10.014
   Ju MY, 2021, IEEE T IMAGE PROCESS, V30, P2180, DOI 10.1109/TIP.2021.3050643
   Ju MY, 2020, IEEE T IMAGE PROCESS, V29, P3104, DOI 10.1109/TIP.2019.2957852
   Ju MY, 2017, VISUAL COMPUT, V33, P1613, DOI 10.1007/s00371-016-1305-1
   Juneja A, 2022, ARCH COMPUT METHOD E, V29, P1727, DOI 10.1007/s11831-021-09637-z
   Dhara SK, 2021, IEEE T CIRC SYST VID, V31, P2076, DOI 10.1109/TCSVT.2020.3007850
   Kaplan NH, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103720
   Kaur M, 2020, INFORM SCIENCES, V521, P326, DOI 10.1016/j.ins.2020.02.048
   Khan H, 2020, NEUROCOMPUTING, V381, P141, DOI 10.1016/j.neucom.2019.10.005
   Khan M.J., 2022, Vis. Comput., P1
   Kim SE, 2020, IEEE T IMAGE PROCESS, V29, P1985, DOI 10.1109/TIP.2019.2948279
   Kumar A, 2021, J VIS COMMUN IMAGE R, V78, DOI 10.1016/j.jvcir.2021.103122
   Li BY, 2019, Arxiv, DOI arXiv:1712.04143
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li Jin, 2022, IEEE T MULTIMEDIA
   Li XL, 2023, VISUAL COMPUT, V39, P663, DOI 10.1007/s00371-021-02365-2
   Li Z, 2021, INT C PATT RECOG, P8267, DOI 10.1109/ICPR48806.2021.9412595
   Li ZG, 2021, IEEE T IMAGE PROCESS, V30, P9270, DOI 10.1109/TIP.2021.3123551
   Li ZG, 2018, IEEE T IMAGE PROCESS, V27, P442, DOI 10.1109/TIP.2017.2750418
   Liao YH, 2018, LECT NOTES COMPUT SC, V11164, P469, DOI 10.1007/978-3-030-00776-8_43
   Lin RJ, 2022, VISUAL COMPUT, V38, P4419, DOI 10.1007/s00371-021-02305-0
   Ling P., 2023, IEEE T IMAGE PROCESS
   Ling ZG, 2018, IEEE T MULTIMEDIA, V20, P1699, DOI 10.1109/TMM.2017.2778565
   Liu T., 2021, Vis. Comput., P1
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Liu X, 2016, IET IMAGE PROCESS, V10, P877, DOI 10.1049/iet-ipr.2016.0138
   Liu YF, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3267271
   Lu ZW, 2020, IEEE SIGNAL PROC LET, V27, P665, DOI 10.1109/LSP.2020.2985570
   Luthen J., 2017, Electron. Imaging, V29, P79, DOI [DOI 10.2352/ISSN.2470-1173.2017.12.IQSP-229, 10.2352/ISSN.2470-1173.2017.12.IQSP-229]
   Magnenat-Thalmann N., 1989, Visual Computer, V5, P32, DOI 10.1007/BF01901479
   Manu C.M., 2022, VISUAL COMPUT, P1, DOI DOI 10.1007/s00371-077-07516-9
   McCartney E. J., 1976, Optics of the atmosphere. Scattering by molecules and particles
   Mehra A., 2020, IEEE Trans. Intell. Transport. Syst., P1
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Mondal R., 2020, Math. Morphol.-Theory Appl., V4, P87
   Nair D, 2018, J VIS COMMUN IMAGE R, V50, P9, DOI 10.1016/j.jvcir.2017.11.005
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Nayar S. K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P820, DOI 10.1109/ICCV.1999.790306
   Nnolim UA, 2019, OPTIK, V195, DOI 10.1016/j.ijleo.2019.163111
   Raikwar SC, 2020, IEEE T IMAGE PROCESS, V29, P4832, DOI 10.1109/TIP.2020.2975909
   Rao J, 2023, VISUAL COMPUT, V39, P2111, DOI 10.1007/s00371-022-02468-4
   Ren WQ, 2020, INT J COMPUT VISION, V128, P240, DOI 10.1007/s11263-019-01235-8
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P1895, DOI 10.1109/TIP.2018.2876178
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Salazar-Colores S, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.4.043022
   Santra S, 2018, IEEE T IMAGE PROCESS, V27, P4598, DOI 10.1109/TIP.2018.2841198
   Shin J, 2022, IEEE T MULTIMEDIA, V24, P245, DOI 10.1109/TMM.2021.3050053
   Singh D, 2019, APPL INTELL, V49, P4276, DOI 10.1007/s10489-019-01504-6
   Singh D, 2019, SIGNAL PROCESS-IMAGE, V70, P131, DOI 10.1016/j.image.2018.09.011
   Singh D, 2017, IMAGING SCI J, V65, P282, DOI 10.1080/13682199.2017.1329792
   Song XJ, 2023, VISUAL COMPUT, V39, P489, DOI 10.1007/s00371-021-02343-8
   Song YF, 2018, IEEE T MULTIMEDIA, V20, P1548, DOI 10.1109/TMM.2017.2771472
   Song YC, 2015, CHIN CONT DECIS CONF, P5840, DOI 10.1109/CCDC.2015.7161852
   Sun ZY, 2021, COMPUT VIS IMAGE UND, V203, DOI 10.1016/j.cviu.2020.103133
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Tang QF, 2021, COMPUT VIS IMAGE UND, V202, DOI 10.1016/j.cviu.2020.103086
   Thomas J., 2021, INT C INF PROC, P29
   Wang AN, 2019, IEEE T IMAGE PROCESS, V28, P381, DOI 10.1109/TIP.2018.2868567
   Wang CS, 2020, IEEE ACCESS, V8, P9488, DOI 10.1109/ACCESS.2020.2964271
   Wang S., 2023, Vis. Comput., P1
   Wang W., 2022, Signal Process.: Image Commun, V106
   Wang WC, 2017, IEEE T MULTIMEDIA, V19, P1142, DOI 10.1109/TMM.2017.2652069
   Xiao CX, 2012, VISUAL COMPUT, V28, P713, DOI 10.1007/s00371-012-0679-y
   Xiao JS, 2017, IET IMAGE PROCESS, V11, P1163, DOI 10.1049/iet-ipr.2017.0058
   Yang D, 2018, LECT NOTES COMPUT SC, V11211, P729, DOI 10.1007/978-3-030-01234-2_43
   Yang F, 2022, VISUAL COMPUT, V38, P1579, DOI 10.1007/s00371-021-02089-3
   Yang M, 2024, VISUAL COMPUT, V40, P303, DOI 10.1007/s00371-023-02783-4
   Yang Y, 2021, MULTIMED TOOLS APPL, V80, P15701, DOI 10.1007/s11042-021-10540-9
   Yi WC, 2024, VISUAL COMPUT, V40, P2293, DOI 10.1007/s00371-023-02917-8
   Yuan FN, 2021, PATTERN RECOGN, V119, DOI 10.1016/j.patcog.2021.108076
   Zhang M., 2022, IEEE Transactions on neural networks and learn- ing systems
   Zhang MJ, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3263848
   Zhang MJ, 2022, PROC CVPR IEEE, P867, DOI 10.1109/CVPR52688.2022.00095
   Zhang MJ, 2023, IEEE T CYBERNETICS, V53, P578, DOI 10.1109/TCYB.2022.3163294
   Zhang SD, 2020, NEUROCOMPUTING, V410, P363, DOI 10.1016/j.neucom.2020.06.041
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhang SD, 2020, VISUAL COMPUT, V36, P305, DOI 10.1007/s00371-018-1612-9
   Zhang XQ, 2021, IEEE T CIRC SYST VID, V31, P3025, DOI 10.1109/TCSVT.2020.3035722
   Zhang XQ, 2020, COMPUT VIS IMAGE UND, V197, DOI 10.1016/j.cviu.2020.103003
   Zhao D, 2021, IEEE T CIRC SYST VID, V31, P3037, DOI 10.1109/TCSVT.2020.3036992
   Zhao D, 2019, SIGNAL PROCESS-IMAGE, V74, P253, DOI 10.1016/j.image.2019.02.004
   Zhao SY, 2021, IEEE T IMAGE PROCESS, V30, P3391, DOI 10.1109/TIP.2021.3060873
   Zhu HY, 2021, IEEE T CYBERNETICS, V51, P829, DOI 10.1109/TCYB.2019.2955092
   Zhu HY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1234
   Zhu MZ, 2018, IEEE SIGNAL PROC LET, V25, P174, DOI 10.1109/LSP.2017.2780886
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 117
TC 0
Z9 0
U1 17
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 8
PY 2024
DI 10.1007/s00371-024-03270-0
EA FEB 2024
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD1M4
UT WOS:001157463900002
DA 2024-08-05
ER

PT J
AU Sekar, RR
   Rajkumar, TD
   Anne, KR
AF Sekar, R. Raja
   Rajkumar, T. Dhiliphan
   Anne, Koteswara Rao
TI Deep fake detection using an optimal deep learning model with multi head
   attention-based feature extraction scheme
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Face forgery detection; Viola Jones; Deep fake detection; Convolution
   neural network; Feature learning; Transfer learning; Long short-term
   memory
ID FACE; NETWORKS
AB Face forgery, or deep fake, is a frequently used method to produce fake face images, network pornography, blackmail, and other illegal activities. Researchers developed several detection approaches based on the changing traces presented by deep forgery to limit the damage caused by deep fake methods. They obtain limited performance when evaluating cross-datum scenarios. This paper proposes an optimal deep learning approach with an attention-based feature learning scheme to perform DFD more accurately. The proposed system mainly comprises '5' phases: face detection, preprocessing, texture feature extraction, spatial feature extraction, and classification. The face regions are initially detected from the collected data using the Viola-Jones (VJ) algorithm. Then, preprocessing is carried out, which resizes and normalizes the detected face regions to improve their quality for detection purposes. Next, texture features are learned using the Butterfly Optimized Gabor Filter to get information about the local features of objects in an image. Then, the spatial features are extracted using Residual Network-50 with Multi Head Attention (RN50MHA) to represent the data globally. Finally, classification is done using the Optimal Long Short-Term Memory (OLSTM), which classifies the data as fake or real, in which optimization of network is done using Enhanced Archimedes Optimization Algorithm. The proposed system is evaluated on four benchmark datasets such as Face Forensics + + (FF + +), Deepfake Detection Challenge, Celebrity Deepfake (CDF), and Wild Deepfake. The experimental results show that DFD using OLSTM and RN50MHA achieves a higher inter and intra-dataset detection rate than existing state-of-the-art methods.
C1 [Sekar, R. Raja; Rajkumar, T. Dhiliphan; Anne, Koteswara Rao] Kalasalingam Acad Res & Educ, Dept Comp Sci & Engn, Srivilliputhur 626126, India.
C3 Kalasalingam Academy of Research & Education
RP Sekar, RR (corresponding author), Kalasalingam Acad Res & Educ, Dept Comp Sci & Engn, Srivilliputhur 626126, India.
EM rajasekar.tulip@gmail.com; dhilipanrajkumar@gmail.com; aoanne@gmail.com
FX DAS:The manuscript has no associated data.
CR Abdullakutty F, 2021, INFORM FUSION, V75, P55, DOI 10.1016/j.inffus.2021.04.015
   Arashloo SR, 2021, IEEE T INF FOREN SEC, V16, P4635, DOI 10.1109/TIFS.2021.3111766
   Awotunde JB, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12010087
   Balas V. E., 2022, INTELLIGENT COMPUTIN, P1
   Benlamoudi A, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22103760
   Bonomi M, 2021, J VIS COMMUN IMAGE R, V79, DOI 10.1016/j.jvcir.2021.103239
   Chen HN, 2020, IEEE T INF FOREN SEC, V15, P578, DOI 10.1109/TIFS.2019.2922241
   Dong YS, 2023, NEUROCOMPUTING, V535, P123, DOI 10.1016/j.neucom.2023.03.034
   Hsu CC, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10010370
   Ismail A, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21165413
   kaggle.com, ABOUT US
   Kim E, 2021, IEEE ACCESS, V9, P123493, DOI 10.1109/ACCESS.2021.3110859
   Kumar N., 2021, Advances in Interdisciplinary Engineering, DOI [10.1007/978-981-15-9956-9, DOI 10.1007/978-981-15-9956-9]
   Lai ZM, 2022, SYSTEMS-BASEL, V10, DOI 10.3390/systems10020031
   Liu C., 2023, IEEE Transactions on Dependable and Secure Computing
   Liu Y., 2023, IEEE Trans. Geosci. Remote Sens, V61, P1, DOI DOI 10.1109/TGRS.2023.3334492
   Liu YF, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3298661
   Liu YF, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3133956
   Ma YK, 2020, IEEE ACCESS, V8, P26505, DOI 10.1109/ACCESS.2020.2971224
   Malolan B, 2020, 2020 3RD INTERNATIONAL CONFERENCE ON INFORMATION AND COMPUTER TECHNOLOGIES (ICICT 2020), P289, DOI 10.1109/ICICT50521.2020.00051
   Ming ZH, 2020, J IMAGING, V6, DOI 10.3390/jimaging6120139
   Mitra A, 2020, 2020 6TH IEEE INTERNATIONAL SYMPOSIUM ON SMART ELECTRONIC SYSTEMS (ISES 2020) (FORMERLY INIS), P91, DOI 10.1109/iSES50453.2020.00031
   Mitra S. P., 2021, Social Netw. Comput. Sci., V2, P1, DOI [DOI 10.1007/S42979-021-00495-X, 10.1007/s42979-021-00495-x]
   paperswithcode, ABOUT US
   Qurat-ul-ain, 2021, INT BHURBAN C APPL S, P271, DOI 10.1109/IBCAST51254.2021.9393234
   Rafique R., 2021, 2021 4 INT C COMP IN, P1
   Rana M. S., 2022, IEEE Access
   Sedik A, 2022, NEURAL COMPUT APPL, V34, P1251, DOI 10.1007/s00521-021-06416-6
   Shang ZH, 2021, PATTERN RECOGN, V116, DOI 10.1016/j.patcog.2021.107950
   Tolosana R, 2020, INFORM FUSION, V64, P131, DOI 10.1016/j.inffus.2020.06.014
   Vinolin V, 2021, VISUAL COMPUT, V37, P2369, DOI 10.1007/s00371-020-01992-5
   Wang B, 2022, SCI PROGRAMMING-NETH, V2022, DOI 10.1155/2022/9179998
   Wang H, 2023, IEEE J OCEANIC ENG, V48, P443, DOI 10.1109/JOE.2022.3226202
   Xue ZY, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11244143
   Zhang WG, 2020, ENTROPY-SWITZ, V22, DOI 10.3390/e22020249
   Zhou YB, 2021, IEEE IMAGE PROC, P3597, DOI 10.1109/ICIP42928.2021.9506371
NR 36
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 17
PY 2024
DI 10.1007/s00371-024-03567-0
EA JUL 2024
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YS3B4
UT WOS:001270422500001
DA 2024-08-05
ER

PT J
AU Reimann, M
   Büssemeyer, M
   Buchheim, B
   Semmo, A
   Döllner, J
   Trapp, M
AF Reimann, Max
   Buessemeyer, Martin
   Buchheim, Benito
   Semmo, Amir
   Doellner, Juergen
   Trapp, Matthias
TI Artistic style decomposition for texture and shape editing
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Texture decomposition; Neural style transfer; Geometric abstraction;
   Texture control; Stroke-based rendering
AB While methods for generative image synthesis and example-based stylization produce impressive results, their black-box style representation intertwines shape, texture, and color aspects, limiting precise stylistic control and editing of artistic images. We introduce a novel method for decomposing the style of an artistic image that enables interactive geometric shape abstraction and texture control. We spatially decompose the input image into geometric shapes and an overlaying parametric texture representation, facilitating independent manipulation of color and texture. The parameters in this texture representation, comprising the image's high-frequency details, control painterly attributes in a series of differentiable stylization filters. Shape decomposition is achieved using either segmentation or stroke-based neural rendering techniques. We demonstrate that our shape and texture decoupling enables diverse stylistic edits, including adjustments in shape, stroke, and painterly attributes such as contours and surface relief. Moreover, we demonstrate shape and texture style transfer in the parametric space using both reference images and text prompts and accelerate these by training networks for single- and arbitrary-style parameter prediction.
C1 [Reimann, Max; Buessemeyer, Martin; Buchheim, Benito; Doellner, Juergen] Univ Potsdam, Hasso Plattner Inst Digital Engn, Potsdam, Germany.
   [Semmo, Amir; Trapp, Matthias] Digital Masterpieces GmbH, Potsdam, Germany.
C3 University of Potsdam
RP Reimann, M (corresponding author), Univ Potsdam, Hasso Plattner Inst Digital Engn, Potsdam, Germany.
EM max.reimann@hpi.de
FU Projekt DEAL; German Federal Ministry of Education and Research (BMBF)
   [01IS15041, 01IS19006]; German Federal Ministry for Economic Affairs and
   Climate Action (BMWK) [16KN086401]
FX Open Access funding enabled and organized by Projekt DEAL. This work was
   funded by the German Federal Ministry of Education and Research (BMBF)
   (through Grants 01IS15041-"mdViPro" and 01IS19006-"KI-Labor ITSE"), and
   the German Federal Ministry for Economic Affairs and Climate Action
   (BMWK) (through Grant 16KN086401-"PO-NST").
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Bousseau A., 2006, P 4 INT S NONPH AN R, P141, DOI DOI 10.1145/1124728.1124751
   Bussemeyer M., 2023, P IEEE INT C CYB CW, P1, DOI [10.1109/cw58918.2023.00011, DOI 10.1109/CW58918.2023.00011]
   Chong MJ, 2022, LECT NOTES COMPUT SC, V13676, P128, DOI 10.1007/978-3-031-19787-1_8
   Chung JW, 2024, Arxiv, DOI arXiv:2312.09008
   Dhariwal P, 2021, ADV NEUR IN, V34
   Gal R, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530164
   Gatys LA, 2017, PROC CVPR IEEE, P3730, DOI 10.1109/CVPR.2017.397
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Goodfellow Ian, 2014, PROC ADV NEURAL IN, V27
   Hertzmann A., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P453, DOI 10.1145/280814.280951
   Hertzmann A, 2022, Arxiv, DOI arXiv:2205.01605
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Huang ZW, 2019, IEEE I CONF COMP VIS, P8708, DOI 10.1109/ICCV.2019.00880
   Ihde L., 2022, Journal of WSCG, P99
   Jang W, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459860
   Jing Y., 2018, P ECCV, DOI [10.1007/978-3-030-01261-815, DOI 10.1007/978-3-030-01261-815]
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Jonschkowski R., 2016, End-to-end learnable histogram filters
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim G, 2022, PROC CVPR IEEE, P2416, DOI 10.1109/CVPR52688.2022.00246
   Kingma D. P., 2014, arXiv
   Kolkin N, 2019, PROC CVPR IEEE, P10043, DOI 10.1109/CVPR.2019.01029
   Kwon G, 2022, PROC CVPR IEEE, P18041, DOI 10.1109/CVPR52688.2022.01753
   Kyprianidis JE, 2013, IEEE T VIS COMPUT GR, V19, P866, DOI 10.1109/TVCG.2012.160
   Lin TY, 2015, Arxiv, DOI arXiv:1405.0312
   Liu SH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6578, DOI 10.1109/ICCV48922.2021.00653
   Lötzsch W, 2022, LECT NOTES COMPUT SC, V13677, P135, DOI 10.1007/978-3-031-19790-1_9
   Nichol K., 2016, Kaggle Painter by Numbers (WikiArt)
   Park DY, 2019, PROC CVPR IEEE, P5873, DOI 10.1109/CVPR.2019.00603
   Patashnik O, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2065, DOI 10.1109/ICCV48922.2021.00209
   PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839
   Radford A, 2021, PR MACH LEARN RES, V139
   Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196
   Reimann M, 2022, VISUAL COMPUT, V38, P4019, DOI 10.1007/s00371-022-02518-x
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Saharia C, 2022, Arxiv, DOI [arXiv:2205.11487, DOI 10.48550/ARXIV.2205.11487]
   Semmo A, 2016, COMPUT GRAPH-UK, V55, P157, DOI 10.1016/j.cag.2015.12.001
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh J, 2022, LECT NOTES COMPUT SC, V13674, P678, DOI 10.1007/978-3-031-19781-9_39
   Sohl-Dickstein J, 2015, PR MACH LEARN RES, V37, P2256
   Song Y.-Z., 2008, CAE, P65, DOI DOI 10.2312/COMPAESTH/COMPAESTH08/065-072
   Wang MY, 2014, IEEE T VIS COMPUT GR, V20, P1451, DOI 10.1109/TVCG.2014.2303984
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Winnemoeller H, 2012, COMPUT GRAPH-UK, V36, P740, DOI 10.1016/j.cag.2012.03.004
   Winnemoller H., 2012, Image and Video-Based Artistic Stylisation, P353, DOI [10.1007/978-1-4471-4519-6, DOI 10.1007/978-1-4471-4519-6]
   Zou ZX, 2021, PROC CVPR IEEE, P15684, DOI 10.1109/CVPR46437.2021.01543
NR 49
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 1
PY 2024
DI 10.1007/s00371-024-03521-0
EA JUL 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XC2G8
UT WOS:001259414100003
OA hybrid
DA 2024-08-05
ER

PT J
AU Wu, YB
   Li, WT
   Chen, ZY
   Wen, H
   Cui, ZW
   Zhang, YJ
AF Wu, Yabo
   Li, Wenting
   Chen, Ziyang
   Wen, Hui
   Cui, Zhongwei
   Zhang, Yongjun
TI Distribution-decouple learning network: an innovative approach for
   single image dehazing with spatial and frequency decoupling
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image dehazing; Distribution; Decouple; Frequency
ID VISION
AB Image dehazing methods face challenges in addressing the high coupling between haze and object feature distributions in the spatial and frequency domains. This coupling often results in oversharpening, color distortion, and blurring of details during the dehazing process. To address these issues, we introduce the distribution-decouple module (DDM) and dual-frequency attention mechanism (DFAM). The DDM works effectively in the spatial domain, decoupling haze and object features through a feature decoupler and then uses a two-stream modulator to further reduce the negative impact of haze on the distribution of object features. Simultaneously, the DFAM focuses on decoupling information in the frequency domain, separating high- and low-frequency information and applying attention to different frequency components for frequency calibration. Finally, we introduce a novel dehazing network, the distribution-decouple learning network for single image dehazing with spatial and frequency decoupling (DDLNet). This network integrates DDM and DFAM, effectively addressing the issue of coupled feature distributions in both spatial and frequency domains, thereby enhancing the clarity and fidelity of the dehazed images. Extensive experiments indicate the outperformance of our DDLNet when compared to the state-of-the-art (SOTA) methods, achieving a 1.50 dB increase in PSNR on the SOTS-indoor dataset. Concomitantly, it indicates a 1.26 dB boost on the SOTS-outdoor dataset. Additionally, our method performs significantly well on the nighttime dehazing dataset NHR, achieving a 0.91 dB improvement. Code and trained models are available at https://github.com/aoe-wyb/DDLNet.
C1 [Wu, Yabo; Chen, Ziyang; Wen, Hui; Zhang, Yongjun] Guizhou Univ, Inst Artificial Intelligence, Coll Comp Sci & Technol, State Key Lab Publ Big Data, Guiyang 550025, Guizhou, Peoples R China.
   [Li, Wenting] Guizhou Univ Commerce, Comp & Informat Engn Coll, Guiyang 550014, Guizhou, Peoples R China.
   [Cui, Zhongwei] Guizhou Normal Univ, Sch Math & Big Data, Guiyang 550018, Guizhou, Peoples R China.
C3 Guizhou University; Guizhou University of Commerce; Guizhou Normal
   University
RP Zhang, YJ (corresponding author), Guizhou Univ, Inst Artificial Intelligence, Coll Comp Sci & Technol, State Key Lab Publ Big Data, Guiyang 550025, Guizhou, Peoples R China.
EM gs.wuyb22@gzu.edu.cn; 201520274@gzcc.edu.cn; gs.ziyangchen22@gzu.edu.cn;
   gs.hwen22@gzu.edu.cn; zhongweicui@gznc.edu.cn; zyj6667@126.com
RI Chen, Ziyang/KHX-1531-2024
OI Chen, Ziyang/0000-0002-9361-0240
FU Natural science research project of Guizhou Provincial Department of
   Education, China [QianJiaoJi[2022]029,QianJiaoHeKY[2021]022]
FX This work is supported by the Natural science research project of
   Guizhou Provincial Department of Education, China
   (QianJiaoJi[2022]029,QianJiaoHeKY[2021]022).
CR Ancuti CO, 2020, IEEE COMPUT SOC CONF, P1798, DOI 10.1109/CVPRW50498.2020.00230
   Ancuti CO, 2019, IEEE IMAGE PROC, P1014, DOI [10.1109/ICIP.2019.8803046, 10.1109/icip.2019.8803046]
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Chen YP, 2019, IEEE I CONF COMP VIS, P3434, DOI 10.1109/ICCV.2019.00353
   Cho SJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4621, DOI 10.1109/ICCV48922.2021.00460
   Cui Y., 2024, P AAAI C ART INT
   Cui Y., 2023, P AAAI C ART INT, P479
   Cui Yuning, 2023, P IEEE CVF INT C COM, P13001
   Gui J, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3576918
   Guo CL, 2022, PROC CVPR IEEE, P5802, DOI 10.1109/CVPR52688.2022.00572
   Guo XJ, 2022, INFORM FUSION, V86-87, P146, DOI 10.1016/j.inffus.2022.07.005
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He T, 2019, PROC CVPR IEEE, P558, DOI 10.1109/CVPR.2019.00065
   Kumar Bagesh, 2022, IC3-2022: Proceedings of the 2022 Fourteenth International Conference on Contemporary Computing, P242, DOI 10.1145/3549206.3549252
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li RD, 2023, APPL INTELL, V53, P12437, DOI 10.1007/s10489-022-04158-z
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li Y, 2022, KNOWL-BASED SYST, V254, DOI 10.1016/j.knosys.2022.109579
   Li Y, 2015, IEEE I CONF COMP VIS, P226, DOI 10.1109/ICCV.2015.34
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu KH, 2021, IEEE-CAA J AUTOMATIC, V8, P1428, DOI 10.1109/JAS.2021.1004057
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Liu Y., 2020, P AS C COMP VIS, P1
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Mao XT, 2023, AAAI CONF ARTIF INTE, P1905
   McCartney E. J., 1976, Optics of the atmosphere. Scattering by molecules and particles
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Qin ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P763, DOI 10.1109/ICCV48922.2021.00082
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P1895, DOI 10.1109/TIP.2018.2876178
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song XB, 2023, IEEE T CIRC SYST VID, V33, P575, DOI 10.1109/TCSVT.2022.3207020
   Song YD, 2023, IEEE T IMAGE PROCESS, V32, P1927, DOI 10.1109/TIP.2023.3256763
   Tu ZZ, 2022, PROC CVPR IEEE, P5759, DOI 10.1109/CVPR52688.2022.00568
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang S., 2023, Vis. Comput, V40, P1
   Wang T, 2024, PATTERN RECOGN, V145, DOI 10.1016/j.patcog.2023.109956
   Wu HY, 2021, PROC CVPR IEEE, P10546, DOI 10.1109/CVPR46437.2021.01041
   Xie D., 2023, Expert Syst. Appl, V8, P122427
   Yang F, 2022, VISUAL COMPUT, V38, P1579, DOI 10.1007/s00371-021-02089-3
   Yu H, 2022, LECT NOTES COMPUT SC, V13679, P181, DOI 10.1007/978-3-031-19800-7_11
   Zhang J, 2017, PROC CVPR IEEE, P5226, DOI 10.1109/CVPR.2017.555
   Zhang J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2355, DOI 10.1145/3394171.3413763
   Zhang SD, 2023, IEEE T CYBERNETICS, V53, P454, DOI 10.1109/TCYB.2021.3124231
   Zhang Y., 2023, IEEE Trans. Circuits Syst. Video Technol.
   Zhou M., 2023, INT C MACH LEARN, P42589
   Zhou Y., 2022, IEEE Trans. Neural Netw. Learn. Syst.
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
   Zou WB, 2021, IEEE INT CONF COMP V, P1895, DOI 10.1109/ICCVW54120.2021.00216
NR 56
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 28
PY 2024
DI 10.1007/s00371-024-03556-3
EA JUN 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WU7X3
UT WOS:001257463400003
DA 2024-08-05
ER

PT J
AU Khalilsaraei, SB
   Komar, A
   Zheng, JM
   Augsdörfer, U
AF Barzegar Khalilsaraei, Saeedeh
   Komar, Alexander
   Zheng, Jianmin
   Augsdoerfer, Ursula
TI InceptCurves: curve reconstruction using an inception network
SO VISUAL COMPUTER
LA English
DT Article
DE Machine learning; Curve reconstruction; Subdivision curves; B-splines
ID B-SPLINE CURVE; PARTICLE SWARM OPTIMIZATION; APPROXIMATION
AB Curve reconstruction is a fundamental task in many visual computing applications. In this paper, a data-driven approach for curve reconstruction is proposed. We present an inception layered deep neural network structure, capable of learning simultaneously the number of control points and their positions in order to reconstruct the curve. To train the network, a large set of general synthetic data is generated. The reconstructed uniform B-spline closely approximates any arbitrary input curve, with or without intersections. Because the network predicts the number of control points required for the B-spline reconstruction, redundancy is reduced in the curve representation. We demonstrate our approach on various examples.
C1 [Barzegar Khalilsaraei, Saeedeh; Komar, Alexander; Augsdoerfer, Ursula] Graz Univ Technol, Inst Comp Graph & Knowledge Visualizat, Graz, Austria.
   [Zheng, Jianmin] Nanyang Technol Univ, Coll Comp & Data Sci, Singapore, Singapore.
C3 Graz University of Technology; Nanyang Technological University
RP Khalilsaraei, SB (corresponding author), Graz Univ Technol, Inst Comp Graph & Knowledge Visualizat, Graz, Austria.
EM s.barzegar@cgv.tugraz.at
OI Komar, Alexander/0000-0002-0657-3796
FU Graz University of Technology
FX No Statement Available
CR Achlioptas P, 2018, PR MACH LEARN RES, V80
   Albawi S, 2017, I C ENG TECHNOL
   Chaikin G.M., 1974, Computer graphics and image processing, V3, P346, DOI DOI 10.1016/0146-664X(74)90028-8
   De Boor C., 1968, LEAST SQUARES CUBIC
   Deng CY, 2014, COMPUT AIDED DESIGN, V47, P32, DOI 10.1016/j.cad.2013.08.012
   Ebrahimi A, 2019, IRAN J SCI TECHNOL A, V43, P947, DOI 10.1007/s40995-017-0347-1
   Galvez A, 2008, LECT NOTES COMPUT SC, V5102, P116, DOI 10.1007/978-3-540-69387-1_13
   Gálvez A, 2011, COMPUT AIDED DESIGN, V43, P1683, DOI 10.1016/j.cad.2011.07.010
   Gao J., 2019, ARXIV
   Hoschek J., 1993, Fundamentals of computer aided geometric design
   Ioffe Sergey, 2015, INT C MACHINE LEARNI, V37, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Kingma D. P., 2014, arXiv
   Komar A, 2023, LECT NOTES COMPUT SC, V14361, P343, DOI 10.1007/978-3-031-47969-4_27
   Lapuschkin S, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-08987-4
   Laube P, 2018, INT CONF 3D VISION, P691, DOI 10.1109/3DV.2018.00084
   Laube P, 2018, COMPUT AIDED GEOM D, V62, P104, DOI 10.1016/j.cagd.2018.03.019
   Liu WB, 2017, NEUROCOMPUTING, V234, P11, DOI 10.1016/j.neucom.2016.12.038
   Mandal S., 2021, NUMERICAL MATH ADV A, P1031
   Masci J., 2016, SIGGRAPH ASIA 2016 C, P1, DOI DOI 10.1145/2988458.2988485
   Park H, 2004, COMPUT AIDED GEOM D, V21, P479, DOI 10.1016/j.cagd.2004.03.003
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Piegl L., 1995, The NURBS Book, DOI [10.1007/978-3-642-97385-7, DOI 10.1007/978-3-642-97385-7]
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Sabin M, 2010, GEOM COMPUT, V6, P1, DOI 10.1007/978-3-642-13648-1
   Scholz F, 2021, COMPUT AIDED GEOM D, V85, DOI 10.1016/j.cagd.2021.101977
   Song BY, 2021, APPL SOFT COMPUT, V100, DOI 10.1016/j.asoc.2020.106960
   Sun CL, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12199465
   Szegedy C, 2014, Arxiv, DOI arXiv:1312.6199
   Tong Y., 2021, ASP Trans. Pattern Recogn. Intell. Syst, V1, P32, DOI [10.52810/tpris.2021.100019, DOI 10.52810/TPRIS.2021.100019]
   Valenzuela O, 2013, APPL MATH MODEL, V37, P5851, DOI 10.1016/j.apm.2012.11.002
   Varady Tamas., 2002, Handbook of Computer Aided Geometric Design, P651
   Wang H, 2022, COMMUN MATH STAT, V10, P163, DOI 10.1007/s40304-021-00246-7
   Wen ZP, 2024, COMPUT AIDED DESIGN, V169, DOI 10.1016/j.cad.2023.103668
NR 33
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4805
EP 4815
DI 10.1007/s00371-024-03477-1
EA JUN 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001244596700002
OA hybrid
DA 2024-08-05
ER

PT J
AU Marlali, E
   Balcisoy, S
AF Marlali, Ekin
   Balcisoy, Selim
TI Automated inpainting of Tex Willer comics: adapting manga techniques for
   western comic transformation
SO VISUAL COMPUTER
LA English
DT Article
DE Inpainting; Image processing; CNN
AB Comic inpainting is essential for language localization and adapting comics into animations, involving the removal of speech boxes, textboxes, and sound effects while maintaining artistic integrity. This paper introduces an automated approach leveraging the Manga inpainting method by Xie et al., adapted for western comics like Tex Willer. Tex Willer comics, known for their fusion of Italian art and American Western imagery, pose unique challenges due to their distinct styles. Customized Manga Restoration and Structural Line Extraction codes are employed, along with automated mask generation using deep CNN-based speech balloon detection and segmentation. Our contributions include the development of a precise model tailored to Tex Willer Comics, resulting in visually coherent inpainting results. The paper follows a structured format, covering State-of-the-Art, methodology, results, and conclusions, aiming to streamline the process of western comic inpainting for professionals in the comics and animation industries.
C1 [Marlali, Ekin; Balcisoy, Selim] Sabanci Univ, TR-34956 Istanbul, Turkiye.
C3 Sabanci University
RP Marlali, E (corresponding author), Sabanci Univ, TR-34956 Istanbul, Turkiye.
EM ekinmarlali@sabanciuniv.edu; balcisoy@sabanciuniv.edu
FU Sabancimath; University
FX No Statement Available
CR [Anonymous], 2021, TEX WILLER 001 MANO
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Chu WT, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P417, DOI 10.1145/3078971.3079031
   Dubray D., 2019, 2019 INT C DOC AN RE
   Ho A.K.N., 2012, 2012 10 IAPR INT WOR
   Ito Kota., 2015, P 36 ANN C EUR ASS C
   Kopf J., 2012, ACM T GRAPHIC, V31, P1
   Li CL, 2017, ACM T INFORM SYST, V36, DOI 10.1145/3091108
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu Xueting, 2017, [Computational Visual Media, 计算可视媒体], V3, P61
   Mao Xiangyu., 2015, Comput. Vis. Med, V1, P69, DOI DOI 10.1007/S41095-015-0007-3
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Nazeri Kamyar, 2019, ARXIV190100212
   Nguyen N.-V., 2017, 2017 14 IAPR INT C D
   Ogawa T., 2018, OBJECT DETECTION COM
   Ono N, 2021, PROCEEDINGS OF SIGGRAPH ASIA 2021 TECHNICAL COMMUNICATIONS, DOI 10.1145/3478512.3488607
   Qu Y., 2006, ACM SIGGRAPH 2006 PA, V06
   Qu YG, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409108
   Ren YR, 2019, IEEE I CONF COMP VIS, P181, DOI 10.1109/ICCV.2019.00027
   Rigaud C., 2013, 2013 12 INT C DOC AN
   Sasaki K, 2017, PROC CVPR IEEE, P5768, DOI 10.1109/CVPR.2017.611
   Sykora D., 2004, P 3 INT S NONPH AN R
   Tsubota K, 2019, IEEE INT SYM MULTIM, P212, DOI 10.1109/ISM46123.2019.00046
   Wu H., 2023, IEEE T VISUALIZATION
   Xie M., 2023, MANGA RESCREENING IN
   Xie MS, 2021, PROC CVPR IEEE, P13400, DOI 10.1109/CVPR46437.2021.01320
   Xie MS, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459822
   Xie MS, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417873
   Xu L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366158
   Yao CY, 2017, IEEE T VIS COMPUT GR, V23, P1070, DOI 10.1109/TVCG.2016.2525774
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zeng YH, 2019, PROC CVPR IEEE, P1486, DOI 10.1109/CVPR.2019.00158
   Zhang SH, 2009, IEEE T VIS COMPUT GR, V15, P618, DOI 10.1109/TVCG.2009.9
NR 33
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4585
EP 4592
DI 10.1007/s00371-024-03440-0
EA MAY 2024
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001234206700005
OA hybrid
DA 2024-08-05
ER

PT J
AU Zhu, SY
   Li, YL
   Liu, JZ
   Li, B
AF Zhu, Shuyang
   Li, Youlong
   Liu, Jizhao
   Li, Bin
TI An automatic framework for quadrilateral surface reconstruction with
   partitions from 3D point clouds
SO VISUAL COMPUTER
LA English
DT Article
DE Quadrilateral surface; Surface reconstruction; Surface partition; 3D
   point clouds
AB Currently, three-dimensional (3D) point clouds are widely used in the gaming and film industries. Inspired by the reverse process of polycube-based parametrical mapping, we present an automatic framework that can directly reconstruct a quadrilateral surface with part-based partitions from 3D point clouds and restore their intricate details while remaining resistant to noise. This innovative framework holds potential applications in various fields, including the enhancement of skeleton and rigging processes in computer animation. In our framework, we initially generate a one-dimensional medial skeleton from 3D point clouds. This skeleton guides us in determining the direction mutation and area mutation of 3D point clouds, aiding in the partitioning of the initial quadrilateral surface into meaningful parts. Following this, we evolve our initial quadrilateral surface by moving each vertex toward its normal direction, which is bounded by an unsigned distance field derived from the 3D point clouds. Our framework ultimately generates a high-quality quadrilateral surface that recovers fine details of 3D point clouds with part-based partitions that reflect meaningful partitions of the original object.
C1 [Zhu, Shuyang; Li, Youlong] Lanzhou Univ, Sch Journalism & Commun, Lanzhou, Peoples R China.
   [Liu, Jizhao; Li, Bin] Lanzhou Univ, Sch Informat Sci & Engn, Lanzhou, Peoples R China.
C3 Lanzhou University; Lanzhou University
RP Li, B (corresponding author), Lanzhou Univ, Sch Informat Sci & Engn, Lanzhou, Peoples R China.
EM binli@lzu.edu.cn
OI Li, Bin/0009-0004-6456-8487
FU Natural Science Foundation of Gansu Province
FX No Statement Available
CR Amenta N., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P415, DOI 10.1145/280814.280947
   Amenta Nina., 2001, P 6 ACM S SOLID MODE
   [Anonymous], 2010, P SHAP MOD INT C JUN, DOI DOI 10.1109/SMI.2010.25
   Berretti S, 2009, IMAGE VISION COMPUT, V27, P1540, DOI 10.1016/j.imavis.2009.02.004
   Bukenberger DR, 2021, VISUAL COMPUT, V37, P2725, DOI 10.1007/s00371-021-02183-6
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Hu X, 2018, IEEE T VIS COMPUT GR, V24, P1930, DOI 10.1109/TVCG.2017.2704119
   Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461913
   Ji ZP, 2005, INT C COMP AID DES C, P269
   Jiang XT, 2017, RAPID PROTOTYPING J, V23, P54, DOI 10.1108/RPJ-07-2015-0091
   Kazhdan M., 2006, 4 EUROGRAPHICS S GEO
   Li X., 2001, PROC S INTERACT 3D G, P35
   Lu WY, 2020, COMPUT AIDED GEOM D, V77, DOI 10.1016/j.cagd.2020.101831
   MACKIEWICZ A, 1993, COMPUT GEOSCI, V19, P303, DOI 10.1016/0098-3004(93)90090-R
   Mortara M, 2006, COMPUT GRAPH-UK, V30, P185, DOI 10.1016/j.cag.2006.01.024
   Museth K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487235
   Nielson GM, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P489, DOI 10.1109/VISUAL.2004.28
   Parakkat AD, 2024, COMPUT GRAPH FORUM, DOI 10.1111/cgf.15019
   Rodrigues RSV, 2018, COMPUT GRAPH FORUM, V37, P235, DOI 10.1111/cgf.13323
   ROTH SD, 1982, COMPUT VISION GRAPH, V18, P109, DOI 10.1016/0146-664X(82)90169-1
   Sahillioglu Y, 2010, COMPUT VIS IMAGE UND, V114, P334, DOI 10.1016/j.cviu.2009.12.003
   Sam V., 2012, COMPUT AIDED DESIGN, V9, P869, DOI 10.3722/cadaps.2012.869-879
   Seylan C, 2019, GRAPH MODELS, V105, DOI 10.1016/j.gmod.2019.101041
   Tagliasacchi A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531377
   Tam R., 2003, IEEE VISUALIZ, V105
   Tarini M, 2004, ACM T GRAPHIC, V23, P853, DOI 10.1145/1015706.1015810
   Wei XZ, 2018, IEEE T VIS COMPUT GR, V24, P2799, DOI 10.1109/TVCG.2017.2767047
NR 27
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4725
EP 4736
DI 10.1007/s00371-024-03466-4
EA MAY 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001229255500002
DA 2024-08-05
ER

PT J
AU Fang, J
   Yang, G
   Han, AY
   Liu, XM
   Chen, B
   Wang, C
AF Fang, Juan
   Yang, Guan
   Han, Ayou
   Liu, Xiaoming
   Chen, Bo
   Wang, Chen
TI Zero-shot learning via categorization-relevant disentanglement and
   discriminative samples synthesis
SO VISUAL COMPUTER
LA English
DT Article
DE Zero-shot learning; Disentangled representation; Feature generation
   methods; Semantic consistency
AB Zero-shot learning (ZSL) trains classifiers on seen classes and utilizes class-level semantic attributes to recognize unseen classes. In recent years, feature generation methods have been employed to address the data imbalance issue in ZSL by synthesizing samples of unseen classes. Nevertheless, the current generation methods solely rely on training the generator using seen classes, resulting in synthesized visual features that are inevitably biased toward seen classes and lack discriminative information. In this paper, we propose an effective ZSL method, termed zero-shot learning via categorization-relevant disentanglement and discriminative samples synthesis. To mitigate the category bias problem, we incorporate a domain discriminator and a semantic decoder into the generative network. By maximizing the decision boundary between categories, distinguishing unseen classes and promoting the alignment of synthetic features with their corresponding semantic embeddings, to enhance semantic consistency. In addition, to reduce the interference of redundant information in visual features, we propose a batch recombining-based disentangling method. The original visual features are projected into categorization-irrelevant and categorization-relevant representations through a conditional encoder, and a correlation penalty is employed to ensure the independence between the two component representations. The categorization-irrelevant representation is reorganized and spliced with the categorization-relevant representation, and disentanglement is achieved by calculating the difference between the post-reorganized and pre-reorganized representations. Finally, we train a classifier using the categorization-relevant representations to improve classification accuracy. Experiments are conducted on four publicly available ZSL datasets, and the proposed method achieves superior results, demonstrating its effectiveness in addressing the challenges of ZSL.
C1 [Fang, Juan; Yang, Guan; Han, Ayou; Liu, Xiaoming; Wang, Chen] Zhongyuan Univ Technol, Sch Comp Sci, Zhengzhou 450007, Peoples R China.
   [Fang, Juan; Yang, Guan; Han, Ayou; Liu, Xiaoming] Henan Key Lab Publ Opin Intelligent Anal, Zhengzhou 450007, Peoples R China.
   [Chen, Bo] Shenzhen Univ, Sch Math & Stat, Shenzhen 518000, Peoples R China.
C3 Zhongyuan University of Technology; Shenzhen University
RP Yang, G (corresponding author), Zhongyuan Univ Technol, Sch Comp Sci, Zhengzhou 450007, Peoples R China.; Yang, G (corresponding author), Henan Key Lab Publ Opin Intelligent Anal, Zhengzhou 450007, Peoples R China.
EM 2021107258@zut.edu.cn; yangguan@zut.edu.cn; hanayou2030@gmail.com;
   ming616@zut.edu.cn; chenbo@szu.edu.cn; 2021107267@zut.edu.cn
RI Wang, Chen/KHU-3234-2024
FU Key Research Projects of Higher Education Institutions in Henan; Henan
   Postgraduate Education Reform and Quality Improvement Project
   [YJS2022KC19]; Special Fund Project for Basic Scientific Research of
   Zhongyuan University of Technology;  [23A520022]
FX This work was supported in part by the Key Research Projects of Higher
   Education Institutions in Henan (Project No. 23A520022), Henan
   Postgraduate Education Reform and Quality Improvement Project (Project
   No. YJS2022KC19), and Special Fund Project for Basic Scientific Research
   of Zhongyuan University of Technology (Project No.K2021TD05).
CR Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   Annadani Y, 2018, PROC CVPR IEEE, P7603, DOI 10.1109/CVPR.2018.00793
   [Anonymous], 2016, PROCEEDINGS OF THE I, DOI DOI 10.1109/CVPR.2016.649
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bhagat PK, 2023, VISUAL COMPUT, V39, P2149, DOI 10.1007/s00371-022-02470-w
   Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4
   Chen JM, 2023, IEEE INT CON MULTI, P2231, DOI 10.1109/ICME55011.2023.00381
   Chen SM, 2022, AAAI CONF ARTIF INTE, P330
   Chen SM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P122, DOI 10.1109/ICCV48922.2021.00019
   Chen Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8692, DOI 10.1109/ICCV48922.2021.00859
   Chen Z, 2020, IEEE WINT CONF APPL, P863, DOI [10.1109/wacv45572.2020.9093610, 10.1109/WACV45572.2020.9093610]
   Chou Y.-Y., 2021, INT C LEARNING REPRE
   Deng WX, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1303, DOI 10.1145/3474085.3475579
   Deng WX, 2022, IEEE T MULTIMEDIA, V24, P2407, DOI 10.1109/TMM.2021.3080516
   Feng YG, 2022, PROC CVPR IEEE, P9336, DOI 10.1109/CVPR52688.2022.00913
   Geng YX, 2022, PROCEEDINGS OF THE 28TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, KDD 2022, P443, DOI 10.1145/3534678.3539453
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guan JQ, 2022, VISUAL COMPUT, V38, P3087, DOI 10.1007/s00371-022-02539-6
   Han ZY, 2021, PROC CVPR IEEE, P2371, DOI 10.1109/CVPR46437.2021.00240
   Higgins I., 2017, INT C LEARN REPR
   Gulrajani I, 2017, ADV NEUR IN, V30
   Jiang HJ, 2019, IEEE I CONF COMP VIS, P9764, DOI 10.1109/ICCV.2019.00986
   Keshari Rohit, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13297, DOI 10.1109/CVPR42600.2020.01331
   Kim H, 2018, PR MACH LEARN RES, V80
   Kingma D. P., 2014, arXiv
   Kingma D. P., 2013, ARXIV
   Kong X, 2022, PROC CVPR IEEE, P9296, DOI 10.1109/CVPR52688.2022.00909
   Li JJ, 2019, PROC CVPR IEEE, P7394, DOI 10.1109/CVPR.2019.00758
   Li XY, 2021, AAAI CONF ARTIF INTE, V35, P1966
   Liu JR, 2022, KNOWL-BASED SYST, V236, DOI 10.1016/j.knosys.2021.107780
   Liu Y, 2021, PROC CVPR IEEE, P3793, DOI 10.1109/CVPR46437.2021.00379
   Liu Y, 2020, IEEE COMPUT SOC CONF, P4053, DOI 10.1109/CVPRW50498.2020.00478
   Liu Y, 2019, IEEE I CONF COMP VIS, P6697, DOI 10.1109/ICCV.2019.00680
   Narayan Sanath, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P479, DOI 10.1007/978-3-030-58542-6_29
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Palatucci M., 2009, Advances in neural information processing systems, V22, P1410
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Schönfeld E, 2019, PROC CVPR IEEE, P8239, DOI 10.1109/CVPR.2019.00844
   Su HZ, 2022, PROC CVPR IEEE, P7875, DOI 10.1109/CVPR52688.2022.00773
   Tong B, 2019, PROC CVPR IEEE, P11459, DOI 10.1109/CVPR.2019.01173
   Verma VK, 2018, PROC CVPR IEEE, P4281, DOI 10.1109/CVPR.2018.00450
   Wah C., 2011, The caltech-ucsd birds-200-2011 dataset
   Wang WL, 2021, IEEE WINT CONF APPL, P3470, DOI 10.1109/WACV48630.2021.00351
   Xian YQ, 2019, PROC CVPR IEEE, P10267, DOI 10.1109/CVPR.2019.01052
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768
   Yang G, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app122412642
   Yang M., 2020, ARXIV
   Yang ZQ, 2022, IMAGE VISION COMPUT, V128, DOI 10.1016/j.imavis.2022.104586
   Zhang F., 2019, ICML, P7434
   Zhang HF, 2019, INFORM SCIENCES, V470, P43, DOI 10.1016/j.ins.2018.08.048
   Zhang ZM, 2015, IEEE I CONF COMP VIS, P4166, DOI 10.1109/ICCV.2015.474
   Zhao SJ, 2019, AAAI CONF ARTIF INTE, P5885
   Zhu YZ, 2018, PROC CVPR IEEE, P1004, DOI 10.1109/CVPR.2018.00111
NR 54
TC 0
Z9 0
U1 14
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 3889
EP 3901
DI 10.1007/s00371-024-03393-4
EA APR 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001211242100005
DA 2024-08-05
ER

PT J
AU Yu, T
   Meng, WL
   Wu, ZQ
   Guo, JW
   Zhang, XP
AF Yu, Ting
   Meng, Weiliang
   Wu, Zhongqi
   Guo, Jianwei
   Zhang, Xiaopeng
TI Diff-pcg: diffusion point cloud generation conditioned on continuous
   normalizing flow
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE 3D shape generation; Diffusion model; Continuous normalizing flow; Point
   cloud
AB With the continuous advancement of computer technology and graphic capabilities, the creation of 3D point clouds holds great promise across various fields. However, previous methods in this area are still facing huge challenges, such as complex training setups and limited precision in generating high-quality 3D content. Taking inspiration from the denoising diffusion probabilistic model, we propose Diff-PCG, a Diffusion Point Cloud Generation Conditioned on Continuous Normalizing Flow for 3D generation. Our approach seamlessly combines forward diffusion and reverse processes to produce high-quality 3D point clouds. Moreover, we include a trainable continuous normalizing flow that controls the foundational structure of the point cloud to enhance the representation ability of the encoded information. Extensive experiments validate the efficacy of our approach in generating high-quality 3D point clouds.
C1 [Yu, Ting; Meng, Weiliang; Guo, Jianwei; Zhang, Xiaopeng] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing, Peoples R China.
   [Yu, Ting; Meng, Weiliang; Guo, Jianwei; Zhang, Xiaopeng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Wu, Zhongqi] Chinese Acad Sci, Inst Sci & Dev, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Chinese Academy of Sciences
RP Guo, JW; Zhang, XP (corresponding author), Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing, Peoples R China.; Guo, JW; Zhang, XP (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
EM jianwei.guo@nlpr.ia.ac.cn; xiaopeng.zhang@ia.ac.cn
FU National Natural Science Foundation of China; Guangdong Science and
   Technology Program [2023B1515120026]; Beijing Natural Science Foundation
   [L231013]; Open Project Program of State Key Laboratory of Virtual
   Reality Technology and Systems, Beihang University [VRLAB2023B01]; 
   [U22B2034];  [U21A20515];  [62376271];  [62172416];  [62262043]; 
   [62102414];  [62365014];  [62162044]
FX This work is partially funded by the Guangdong Science and Technology
   Program (2023B1515120026), National Natural Science Foundation of China
   (Nos. U22B2034, U21A20515, 62376271, 62172416, 62262043, 62102414,
   62365014 and 62162044), Beijing Natural Science Foundation (No.
   L231013), and the Open Project Program of State Key Laboratory of
   Virtual Reality Technology and Systems, Beihang University (No.
   VRLAB2023B01).
CR Achlioptas P, 2018, PR MACH LEARN RES, V80
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Chen RTQ, 2018, 32 C NEURAL INFORM P, V31
   Chen ZQ, 2021, PROC CVPR IEEE, P15735, DOI 10.1109/CVPR46437.2021.01548
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   Cheng AC, 2022, LECT NOTES COMPUT SC, V13663, P89, DOI 10.1007/978-3-031-20062-5_6
   Gao J., 2022, ADV NEURAL INFORM PR, P31841
   Groueix T., 2018, Atlasnet: a papier-mache approach to learning 3d surface generation
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Huang WL, 2019, AAAI CONF ARTIF INTE, P8481
   Kazhdan M., 2003, Symposium on Geometry Processing, P156
   Kim J, 2021, PROC CVPR IEEE, P15054, DOI 10.1109/CVPR46437.2021.01481
   Klokov R., 2020, EUROPEAN C COMPUTER, V12368, P694
   Ko WJ, 2022, IEEE INT C INT ROBOT, P544, DOI 10.1109/IROS47612.2022.9981471
   Li CL, 2018, Arxiv, DOI arXiv:1810.05795
   Li P, 2023, PROC CVPR IEEE, P16816, DOI 10.1109/CVPR52729.2023.01613
   Li RH, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459766
   Li SD, 2022, AAAI CONF ARTIF INTE, P1386
   Li Z, 2023, Arxiv, DOI arXiv:2310.16861
   Liao YY, 2020, PROC CVPR IEEE, P5870, DOI 10.1109/CVPR42600.2020.00591
   Litany O, 2018, PROC CVPR IEEE, P1886, DOI 10.1109/CVPR.2018.00202
   Liu Z, 2023, Arxiv, DOI arXiv:2303.08133
   Liu ZJ, 2019, ADV NEUR IN, V32
   Luo ST, 2021, PROC CVPR IEEE, P2836, DOI 10.1109/CVPR46437.2021.00286
   Mo KC, 2019, Arxiv, DOI [arXiv:1908.00575, 10.1145/3355089.3356527, DOI 10.1145/3355089.3356527]
   Nash Charlie, 2020, ICML
   Nichol A, 2022, Arxiv, DOI arXiv:2212.08751
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Qi CR, 2017, ADV NEUR IN, V30
   Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530
   Sanghi A, 2022, Arxiv, DOI arXiv:2211.01427
   Sharma A, 2016, LECT NOTES COMPUT SC, V9915, P236, DOI 10.1007/978-3-319-49409-8_20
   Shu DW, 2019, IEEE I CONF COMP VIS, P3858, DOI 10.1109/ICCV.2019.00396
   Sohl-Dickstein J, 2015, PR MACH LEARN RES, V37, P2256
   Song JM, 2022, Arxiv, DOI [arXiv:2010.02502, DOI 10.48550/ARXIV.2010.02502]
   Song Yang, 2019, Advances in Neural Information Processing Systems, V32
   Sun YB, 2020, IEEE WINT CONF APPL, P61, DOI [10.1109/wacv45572.2020.9093430, 10.1109/WACV45572.2020.9093430]
   Tan QY, 2018, PROC CVPR IEEE, P5841, DOI 10.1109/CVPR.2018.00612
   Uy MA, 2018, PROC CVPR IEEE, P4470, DOI 10.1109/CVPR.2018.00470
   Wang L, 2020, Arxiv, DOI arXiv:2011.14289
   Wu JJ, 2016, ADV NEUR IN, V29
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Chang AX, 2015, Arxiv, DOI [arXiv:1512.03012, DOI 10.48550/ARXIV.1512.03012]
   Yang GD, 2019, IEEE I CONF COMP VIS, P4540, DOI 10.1109/ICCV.2019.00464
   Yang J, 2023, VIS COMPUT IND BIOME, V6, DOI 10.1186/s42492-023-00145-4
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yang Z., 2022, P AS C COMP VIS, P3189
   Zaheer M, 2017, ADV NEUR IN, V30
   Zhang DJ, 2020, INTEGR COMPUT-AID E, V27, P57, DOI 10.3233/ICA-190608
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhou LQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5806, DOI 10.1109/ICCV48922.2021.00577
NR 55
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 8
PY 2024
DI 10.1007/s00371-024-03370-x
EA APR 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ND1M4
UT WOS:001198420700002
DA 2024-08-05
ER

PT J
AU Jia, DY
   Pang, YW
   Cao, JL
   Jing, P
AF Jia, Dayu
   Pang, Yanwei
   Cao, Jiale
   Jing, Pan
TI SSNet: a joint learning network for semantic segmentation and disparity
   estimation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Joint learning; Semantic segmentation; Stereo disparity estimation;
   Transformer
AB Joint learning for semantic segmentation and disparity estimation is adopted to scene parsing for mutual benefit. However, existing joint learning approaches unify the two task briefly which may result in negative feature mixing. In order to solve the problem, a win-win approach Stereo Semantic Network (SSNet) is proposed for pixel-wise scene parsing. SSNet is the first Transformer based end-to-end joint learning model for semantic segmentation and disparity estimation. The main novelty lies in the proposed Transformer Feature Separation Module (TFSM) which is designed to separate features for segmentation prediction and disparity regression according to the characteristics of the two tasks. The segmentation and disparity results are supervised jointly with a weighted summation loss function to improve the performance of both tasks. Experimental results on Cityscapes Dataset and KITTI 2015 Dataset demonstrate that SSNet outperforms state-of-the-art joint learning approaches.
C1 [Jia, Dayu; Pang, Yanwei; Cao, Jiale] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Jia, Dayu] Chinese Acad Sci, Shenyang Inst Automat, State Key Lab Robot, Shenyang 110016, Peoples R China.
   [Jia, Dayu] Chinese Acad Sci, Inst Robot & Intelligent Mfg, Shenyang 110169, Peoples R China.
   [Jing, Pan] Tianjin Univ Technol & Educ, Sch Elect Engn, Tianjin 300222, Peoples R China.
   [Pang, Yanwei; Cao, Jiale] Shanghai Artificial Intelligence Lab, Shanghai 200232, Peoples R China.
C3 Tianjin University; Chinese Academy of Sciences; Shenyang Institute of
   Automation, CAS; Chinese Academy of Sciences; Tianjin University of
   Technology & Education
RP Pang, YW (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.; Pang, YW (corresponding author), Shanghai Artificial Intelligence Lab, Shanghai 200232, Peoples R China.
EM dayu_jia@tju.edu.cn; pyw@tju.edu.cn; connor@tju.edu.cn;
   panjing@tute.edu.cn
FU National Key Research and Development Program of China
FX No Statement Available
CR Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Badrinarayanan V, 2016, Arxiv, DOI [arXiv:1511.00561, DOI 10.1109/TPAMI.2016.2644615]
   Bevandic P, 2021, Arxiv, DOI arXiv:2108.11224
   Cao JL, 2019, PROC CVPR IEEE, P7384, DOI 10.1109/CVPR.2019.00757
   Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567
   Chen LC, 2018, Arxiv, DOI [arXiv:1802.02611, 10.48550/ARXIV.1802.02611]
   Chen LC, 2016, Arxiv, DOI [arXiv:1412.7062, DOI 10.48550/ARXIV.1412.7062, 10.48550/ARXIV.1412.7062]
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, 10.48550/arXiv.1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Ding XH, 2019, IEEE I CONF COMP VIS, P1911, DOI 10.1109/ICCV.2019.00200
   Dong JX, 2022, IEEE T PATTERN ANAL, V44, P8355, DOI 10.1109/TPAMI.2021.3102575
   Dosovitskiy A., 2021, ICLR
   Dvornik N, 2017, IEEE I CONF COMP VIS, P4174, DOI 10.1109/ICCV.2017.447
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Guo XY, 2019, PROC CVPR IEEE, P3268, DOI 10.1109/CVPR.2019.00339
   Hamzah RA, 2016, J SENSORS, V2016, DOI 10.1155/2016/8742920
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI [10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166]
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jiang M, 2022, VISUAL COMPUT, V38, P2473, DOI 10.1007/s00371-021-02124-3
   Kline J, 2020, PROCEEDINGS OF THE 2020 32ND INTERNATIONAL TELETRAFFIC CONGRESS (ITC 32), P1, DOI [10.1109/ITC3249928.2020.00009, 10.1007/978-3-030-58565-5_35]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lambert J, 2021, Arxiv, DOI arXiv:2112.13762
   Li HY, 2022, PROC CVPR IEEE, P4156, DOI 10.1109/CVPR52688.2022.00413
   Li PL, 2019, PROC CVPR IEEE, P7636, DOI 10.1109/CVPR.2019.00783
   Li X, 2022, VISUAL COMPUT, V38, P3881, DOI 10.1007/s00371-021-02228-w
   Li YJ, 2016, LECT NOTES COMPUT SC, V9908, P154, DOI 10.1007/978-3-319-46493-0_10
   Li ZS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6177, DOI 10.1109/ICCV48922.2021.00614
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Lin ZK, 2023, VISUAL COMPUT, V39, P597, DOI 10.1007/s00371-021-02360-7
   Liu XT, 2020, PROC CVPR IEEE, P4846, DOI 10.1109/CVPR42600.2020.00490
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925
   Neuhold G, 2017, IEEE I CONF COMP VIS, P5000, DOI 10.1109/ICCV.2017.534
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Pang YW, 2019, IEEE I CONF COMP VIS, P4229, DOI 10.1109/ICCV.2019.00433
   Paszke A, 2019, ADV NEUR IN, V32
   Qin ZY, 2019, PROC CVPR IEEE, P7607, DOI 10.1109/CVPR.2019.00780
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Strudel R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7242, DOI 10.1109/ICCV48922.2021.00717
   Sun K, 2019, Arxiv, DOI [arXiv:1904.04514, DOI 10.48550/ARXIV.1904.04514]
   Tian L, 2019, VISUAL COMPUT, V35, P1427, DOI 10.1007/s00371-018-01622-1
   Valanarasu JMJ, 2021, LECT NOTES COMPUT SC, V12901, P36, DOI 10.1007/978-3-030-87193-2_4
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Y, 2019, PROC CVPR IEEE, P8063, DOI 10.1109/CVPR.2019.00826
   Xie EZ, 2021, ADV NEUR IN, V34
   Xu HF, 2020, PROC CVPR IEEE, P1956, DOI 10.1109/CVPR42600.2020.00203
   Yang GR, 2018, LECT NOTES COMPUT SC, V11211, P660, DOI 10.1007/978-3-030-01234-2_39
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   You YR, 2020, Arxiv, DOI arXiv:1906.06310
   Yu WB, 2023, IEEE T IMAGE PROCESS, V32, P483, DOI 10.1109/TIP.2022.3229614
   Yuhui Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P173, DOI 10.1007/978-3-030-58539-6_11
   Zeng Y, 2019, IEEE I CONF COMP VIS, P7222, DOI 10.1109/ICCV.2019.00732
   Zhan WJ, 2019, IEEE INT CONF ROBOT, P2946, DOI [10.1109/ICRA.2019.8793573, 10.1109/icra.2019.8793573]
   Zhang FH, 2019, PROC CVPR IEEE, P185, DOI 10.1109/CVPR.2019.00027
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
NR 67
TC 0
Z9 0
U1 7
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 2
PY 2024
DI 10.1007/s00371-024-03336-z
EA APR 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MT0M2
UT WOS:001195768300003
DA 2024-08-05
ER

PT J
AU Qadeer, N
   Shah, JH
   Sharif, M
   Dahan, F
   Khokhar, FA
   Ghazal, R
AF Qadeer, Nauman
   Shah, Jamal Hussain
   Sharif, Muhammad
   Dahan, Fadl
   Khokhar, Fahad Ahmed
   Ghazal, Rubina
TI Multi-camera tracking of mechanically thrown objects for automated
   in-plant logistics by cognitive robots in Industry 4.0
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Smart manufacturing systems; Robotic throw-catch approach; Automated
   in-plant logistics 4.0; Cognitive robots; Multi-camera simulation;
   Encoder-decoder Bi-LSTM
ID PREDICTION
AB Employing cognitive robots, capable of throwing and catching, is a strategy aimed at expediting the logistics process within Industry 4.0's smart manufacturing plants, specifically for the transportation of small-sized manufacturing parts. Since the flight of mechanically thrown objects is inherently unpredictable, it is crucial for the catching robot to observe the initial trajectory with utmost precision and intelligently forecast the final catching position to ensure accurate real-time grasping. This study utilizes multi-camera tracking to monitor mechanically thrown objects. It involves the creation of a 3D simulation that facilitates controlled mechanical throwing of objects within the internal logistics environment of Industry 4.0. The developed simulation empowers users to define the attributes of the thrown object and capture its trajectory using a simulated pinhole camera, which can be positioned at any desired location and orientation within the in-plant logistics environment of flexible manufacturing systems. The simulation facilitated ample experimentation to be conducted for determining the optimal camera positions for accurately observing the 3D interception positions of a flying object based on its apparent size on the camera's sensor plane. Subsequently, a variety of calibrated multi-camera setups were experimented while placing cameras at identified optimal positions. Based on the obtained results, the most effective multi-camera configuration setup is derived. Finally, a training dataset is prepared for 3000 simulated throwing experiments where the initial part of the trajectory consists of observed interception positions, through derived best multi-camera setup, and the final part consists of actual positions. The encoder-decoder Bi-LSTM deep neural network is proposed and trained on this dataset. The trained model outperformed the current state-of-the-art by accurately predicting the final 3D catching point, achieving a mean average error of 5 mm and a root-mean-square error of 7 mm in 200 real-world test experiments.
C1 [Qadeer, Nauman; Shah, Jamal Hussain; Sharif, Muhammad] COMSATS Univ Islamabad, Dept Comp Sci, Wah Cantt, Pakistan.
   [Qadeer, Nauman] Fed Urdu Univ Arts Sci & Technol, Dept Comp Sci, Islamabad, Pakistan.
   [Dahan, Fadl] Prince Sattam Bin Abdulaziz Univ, Dept Management Informat Syst, Al Kharj, Saudi Arabia.
   [Khokhar, Fahad Ahmed] Univ Florence, Dept Math & Informat, Florence, Italy.
   [Ghazal, Rubina] PMAS Arid Agr Univ, Univ Inst Informat Technol, Dept Comp Sci, Rawalpindi, Pakistan.
C3 COMSATS University Islamabad (CUI); Federal Urdu University of Arts
   Science & Technology; Prince Sattam Bin Abdulaziz University; University
   of Florence; Arid Agriculture University
RP Khokhar, FA (corresponding author), Univ Florence, Dept Math & Informat, Florence, Italy.
EM nauman.qadeer@fuuast.edu.pk; jhshah@ciitwah.edu.pk;
   muhammadsharifmalik@yahoo.com; f.naji@psau.edu.sa;
   fahadahmed.khokhar@unifi.it; rubinaghazal@uaar.edu.pk
RI Dahan, Fadl/GQH-4155-2022; Qadeer, Nauman/AAP-4998-2020; Ghazal,
   Rubina/GPF-8954-2022
OI Dahan, Fadl/0000-0002-5975-0696; Qadeer, Nauman/0000-0002-2178-0435;
   Ghazal, Rubina/0000-0003-2911-2148
FU Universit degli Studi di Firenze [7794]; HEC's National Research Program
   for Universities of Pakistan
FX The authors would like to thank HEC's National Research Program for
   Universities of Pakistan to support this research under Project no.
   7794.
CR Alam F., 2007, Effects of spin on tennis ball aerodynamics: an experimental and computational study, P324
   [Anonymous], 2022, World Robotics Report: "All-Time High" with Half a Million Robots Installed in one Year
   Ansari GJ, 2021, IEEE ACCESS, V9, P54923, DOI 10.1109/ACCESS.2021.3071169
   Bäuml B, 2010, IEEE INT C INT ROBOT, P2592, DOI 10.1109/IROS.2010.5651175
   Barteit D.F., 2010, Tracking of thrown objects: catching of mechanically thrown parts for transport in manufacturing
   Barteit D, 2009, IEEE INTL CONF IND I, P680, DOI 10.1109/INDIN.2009.5195885
   Barteit D, 2008, IEEE INTL CONF IND I, P857
   Bauml Berthold, 2011, 2011 11th IEEE-RAS International Conference on Humanoid Robots (Humanoids 2011), P513, DOI 10.1109/Humanoids.2011.6100837
   Black JT., 2020, DEGARMOS MAT PROCESS, V13th Editi
   Chen ZH, 2020, IEEE T CIRC SYST VID, V30, P1410, DOI 10.1109/TCSVT.2019.2902937
   Frank H, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS, VOLS 1-3, P91, DOI 10.1109/ROBIO.2006.340302
   Frank H, 2008, 2008 IEEE CONFERENCE ON ROBOTICS, AUTOMATION, AND MECHATRONICS, VOLS 1 AND 2, P250
   Frank H, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON TECHNOLOGIES FOR PRACTICAL ROBOT APPLICATIONS, P62, DOI 10.1109/TEPRA.2008.4686674
   Gayanov R, 2018, IFAC PAPERSONLINE, V51, P533, DOI 10.1016/j.ifacol.2018.11.271
   Gayanov R, 2017, IEEE INT SYMP SIGNAL, P134, DOI 10.1109/ISSPIT.2017.8388630
   Gomez-Gonzalez S, 2019, ROBOTICS, V8, DOI 10.3390/robotics8040090
   Gomez-Gonzalez S, 2020, IEEE ROBOT AUTOM LET, V5, P970, DOI 10.1109/LRA.2020.2966390
   GomezGonzalez S., 2019, Real time probabilistic models for robot trajectories
   Hartley R, 2003, Multiple view geometry in computer vision, DOI [10.1016/S0143-8166(01)00145-2, DOI 10.1017/CBO9780511811685]
   Lin HI, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20020333
   Lin X., 2021, IEEE Trans. Multimed.
   Maddikunta PKR, 2022, J IND INF INTEGR, V26, DOI 10.1016/j.jii.2021.100257
   Mehta R., 2008, Sports Technol, V1, P7, DOI [10.1080/19346182.2008.9648446, DOI 10.1080/19346182.2008.9648446]
   Mironov K, 2017, 2017 INTERNATIONAL CONFERENCE ON INDUSTRIAL ENGINEERING, APPLICATIONS AND MANUFACTURING (ICIEAM)
   Mironov Konstantin, 2015, IFAC - Papers Online, V48, P28, DOI 10.1016/j.ifacol.2015.09.155
   Mironov K., 2019, Observing and forecasting the trajectory of the thrown body with use of genetic programming
   Mironov K., 2013, Applying neural networks for prediction of flying objects trajectory, V17, P33
   Mironov K, 2016, MED C CONTR AUTOMAT, P512, DOI 10.1109/MED.2016.7536007
   Moller T., 2005, Range Imaging Day Zurich, V7, P3
   Nadeem A., 2021, 2021 MOHAMMAD ALI JI, P1
   Pongratz M, 2012, COMM COM INF SC, V336, P136
   Qadeer N, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22062113
   Shen Y, 2012, COMPUT ANIMAT VIRT W, V23, P179, DOI 10.1002/cav.1465
   Zhu HL, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P111, DOI 10.1145/2911996.2912008
NR 34
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 16
PY 2024
DI 10.1007/s00371-024-03296-4
EA MAR 2024
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LC6A7
UT WOS:001184607700003
OA hybrid
DA 2024-08-05
ER

PT J
AU Yan, J
   Qin, GH
   Sun, MH
   Liang, YH
   Zhang, ZH
   Xu, YH
AF Yan, Jie
   Qin, Guihe
   Sun, Minghui
   Liang, Yanhua
   Zhang, Zhonghan
   Xu, Yinghui
TI A lightweight multi-granularity asymmetric motion mode video frame
   prediction algorithm
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE ConvLSTM; Asymmetric convolution kernel; Lightweight model; Video
   prediction
ID NETWORK
AB Due to the increasing demand for improved accuracy in video prediction tasks, there has been a noticeable trend in deepening network layers and adopting more intricate architectures. Although these approaches can indeed enhance the performance of models, they come at the expense of longer training times and heightened hardware requirements. To tackle this challenge, this study introduces a novel lightweight neural network architecture based on ConvLSTM. The proposed architecture consists of three fundamental components: an asymmetric convolutional kernel (ACK), a fine-grained feature extractor (FFE), and a coarse-grained feature fuser (CFF). The ACK is purposefully designed to specifically enhance motion modeling capabilities. This is achieved by independently establishing motion models in each direction, enabling the expansion of individual motion patterns. Moreover, the proposed framework incorporates the FFE and CFF modules, which effectively extract and integrate spatial texture features at intra- and inter-levels, employing a hierarchical localization technique. This approach enables efficient and lightweight video frame prediction. We conducted extensive performance evaluations of the proposed model on multiple datasets. Remarkably, even with a reduction of 50% in the number of parameters compared to the baseline model, our approach still achieved competitive results when compared with other existing methods.
C1 [Yan, Jie; Qin, Guihe; Sun, Minghui; Liang, Yanhua; Zhang, Zhonghan; Xu, Yinghui] Jilin Univ, Coll Comp Sci & Technol, Qianjin St, Changchun 130012, Jilin, Peoples R China.
   [Yan, Jie; Qin, Guihe; Sun, Minghui; Liang, Yanhua; Zhang, Zhonghan; Xu, Yinghui] Jilin Univ, Key Lab Symbol Computat & Knowledge Engn, Minist Educ, Qianjin St, Changchun 130012, Jilin, Peoples R China.
C3 Jilin University; Jilin University
RP Liang, YH (corresponding author), Jilin Univ, Coll Comp Sci & Technol, Qianjin St, Changchun 130012, Jilin, Peoples R China.; Liang, YH (corresponding author), Jilin Univ, Key Lab Symbol Computat & Knowledge Engn, Minist Educ, Qianjin St, Changchun 130012, Jilin, Peoples R China.
EM yanjie19@mails.jlu.edu.cn; qingh@jlu.edu.cn; smh@jlu.edu.cn;
   yhliang@jlu.edu.cn; zhzhang20@mails.jlu.edu.cn; yhxu20@mails.jlu.edu.cn
CR Blank M, 2005, IEEE I CONF COMP VIS, P1395
   Brock A, 2019, Arxiv, DOI arXiv:1809.11096
   Chen H, 2021, Neural Information Processing Systems
   Chiang TH, 2024, VISUAL COMPUT, V40, P1069, DOI 10.1007/s00371-023-02832-y
   Ding XH, 2019, IEEE I CONF COMP VIS, P1911, DOI 10.1109/ICCV.2019.00200
   Dong XY, 2022, PROC CVPR IEEE, P12114, DOI 10.1109/CVPR52688.2022.01181
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Gökstorp SGE, 2022, VISUAL COMPUT, V38, P2033, DOI 10.1007/s00371-021-02264-6
   Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ho JAT, 2019, Arxiv, DOI arXiv:1912.12180
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Huang G, 2018, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2018.00291
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang X., 2023, Appl. Intell.
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Li YQ, 2021, IEEE SIGNAL PROC LET, V28, P234, DOI 10.1109/LSP.2021.3051845
   Li Z., 2023, IEEE Transactions on Neural Networks and Learning Systems
   Liu Y, 2023, IEEE T PATTERN ANAL, V45, P11624, DOI 10.1109/TPAMI.2023.3284038
   Liu Y, 2022, IEEE T IMAGE PROCESS, V31, P1978, DOI 10.1109/TIP.2022.3147032
   Liu YQ, 2022, IEEE T CIRC SYST VID, V32, P4927, DOI 10.1109/TCSVT.2021.3138431
   Ma NN, 2018, Arxiv, DOI [arXiv:1807.11164, DOI 10.48550/ARXIV.1807.11164]
   Majd M, 2019, APPL INTELL, V49, P2515, DOI 10.1007/s10489-018-1395-8
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Shi XJ, 2017, Arxiv, DOI [arXiv:1706.03458, 10.48550/arXiv.1706.03458, DOI 10.48550/ARXIV.1706.03458]
   Shi XJ, 2015, ADV NEUR IN, V28
   Shibuya E, 2022, VISUAL COMPUT, V38, P3791, DOI 10.1007/s00371-021-02221-3
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang H., 2023, P 31 ACM INT C MULTI
   Tang H, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108792
   Villegas R, 2018, Arxiv, DOI arXiv:1706.08033
   Wang SN, 2020, Arxiv, DOI [arXiv:2006.04768, DOI 10.48550/ARXIV.2006.04768]
   Wang YB, 2018, Arxiv, DOI arXiv:1804.06300
   Wang YB, 2023, IEEE T PATTERN ANAL, V45, P2208, DOI 10.1109/TPAMI.2022.3165153
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yan H, 2023, arXiv
   Yan J, 2022, NEUROCOMPUTING, V494, P160, DOI 10.1016/j.neucom.2022.04.063
   Yuan MX, 2022, APPL INTELL, V52, P5015, DOI 10.1007/s10489-021-02631-9
   Zha ZC, 2023, IEEE T CIRC SYST VID, V33, P3947, DOI 10.1109/TCSVT.2023.3236636
   Zhai XH, 2022, PROC CVPR IEEE, P12094, DOI 10.1109/CVPR52688.2022.01179
   Zhang L., 2018, NEURAL INFORM PROCES
   Zhang Q, 2018, SIGNAL PROCESS, V147, P146, DOI 10.1016/j.sigpro.2018.01.021
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhu YY, 2023, IEEE T IND INFORM, V19, P1248, DOI 10.1109/TII.2022.3179243
NR 51
TC 0
Z9 0
U1 6
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 16
PY 2024
DI 10.1007/s00371-024-03298-2
EA MAR 2024
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LC6A7
UT WOS:001184607700001
DA 2024-08-05
ER

PT J
AU Tong, K
   Wu, YQ
AF Tong, Kang
   Wu, Yiquan
TI I-YOLO: a novel single-stage framework for small object detection
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Small object detection; Channel shuffle; Multi-scale feature learning;
   Feature fusion; Small-PCB; YOLO
ID DEFECT DETECTION; CONTEXT; NETWORK
AB Small object detection is a challenging task in computer vision. We claim that the huge performance gap between the small object detectors and normal sized object detectors stems from two aspects, including the small object dataset and the small object itself. In terms of datasets, we build a large-scale dataset with high image resolution dubbed Small-PCB, in order to promote detection in semiconductor industry. For the small object itself, we utilize multi-scale feature learning and feature fusion strategy to help detect objects. More concretely, we devise two novel components to predict small objects better: re-parameterized module with channel shuffle (RMCS) and multi-scale feature enhanced convolution (MFEC). MFEC aims to split input channels into several parts and applies convolutions with different sizes to each part, and adopt point-by-point convolution to fuse individual channel features. RMCS not only use structural re-parameterization, but also channel shuffle. The usage of channel shuffle can be seen as a fusion of channel features. It strengthens feature information interaction between different channel groups, which bring more informative feature clues. Based on the RMCS and the MFEC, we introduce OIU-RMCS and M-MFEC, respectively. Finally, we build our I-YOLO via integrating these two components into a YOLO-based detector. A large number of qualitative and quantitative results in the experiments indicate that our proposed I-YOLO achieves the state-of-the-art performance on the popular AI-TODv2 and Small-PCB datasets.
C1 [Tong, Kang; Wu, Yiquan] Nanjing Univ Aeronaut & Astronaut, Coll Elect & Informat Engn, Nanjing 211106, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics
RP Wu, YQ (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Elect & Informat Engn, Nanjing 211106, Peoples R China.
EM nuaavision@163.com
CR Bai YC, 2018, LECT NOTES COMPUT SC, V11217, P210, DOI 10.1007/978-3-030-01261-8_13
   Cai ZW, 2021, IEEE T PATTERN ANAL, V43, P1483, DOI 10.1109/TPAMI.2019.2956516
   Cao G., 2017, 9 INT C GRAPHIC IMAG
   Chen C, 2016, ASIAN C COMPUTER VIS, P1
   Chen G, 2022, IEEE T SYST MAN CY-S, V52, P936, DOI 10.1109/TSMC.2020.3005231
   Cheng G, 2016, ISPRS J PHOTOGRAMM, V117, P11, DOI 10.1016/j.isprsjprs.2016.03.014
   Dai XY, 2021, PROC CVPR IEEE, P7369, DOI 10.1109/CVPR46437.2021.00729
   Deng CF, 2022, IEEE T MULTIMEDIA, V24, P1968, DOI 10.1109/TMM.2021.3074273
   Ding RW, 2019, CAAI T INTELL TECHNO, V4, P110, DOI 10.1049/trit.2019.0019
   Feng CJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3490, DOI 10.1109/ICCV48922.2021.00349
   Gong LX, 2023, APPL INTELL, V53, P19449, DOI 10.1007/s10489-023-04544-1
   Gong YQ, 2021, IEEE WINT CONF APPL, P1159, DOI 10.1109/WACV48630.2021.00120
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guo ZB, 2023, VISUAL COMPUT, V39, P4267, DOI 10.1007/s00371-022-02589-w
   He F., 2020, 28 EUROPEAN S ARTIFI
   Hong DF, 2023, REMOTE SENS ENVIRON, V299, DOI 10.1016/j.rse.2023.113856
   Hong DF, 2021, IEEE T GEOSCI REMOTE, V59, P4340, DOI 10.1109/TGRS.2020.3016820
   Hong DF, 2019, IEEE T IMAGE PROCESS, V28, P1923, DOI 10.1109/TIP.2018.2878958
   Hong MB, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3103069
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ji Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1349, DOI 10.1145/3343031.3351064
   Krishna H, 2017, PROCEEDINGS 2017 4TH IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR), P340, DOI 10.1109/ACPR.2017.149
   Lee Y., 2020, IEEE C COMPUTER VISI
   Lee YW, 2019, IEEE COMPUT SOC CONF, P752, DOI 10.1109/CVPRW.2019.00103
   Leng J., 2023, IEEE Trans. Multimed, P1
   Leng JX, 2023, IEEE T CIRC SYST VID, V33, P1320, DOI 10.1109/TCSVT.2022.3210207
   Leng JX, 2021, NEUROCOMPUTING, V433, P287, DOI 10.1016/j.neucom.2020.12.093
   Li CY, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3279834
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Li JA, 2017, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2017.211
   Li X, 2023, IEEE T PATTERN ANAL, V45, P3139, DOI 10.1109/TPAMI.2022.3180392
   Li YH, 2019, IEEE I CONF COMP VIS, P6053, DOI 10.1109/ICCV.2019.00615
   Li Z., 2018, Comput. Res. Reposit., V5
   Lian J, 2020, IEEE T IND INFORM, V16, P1343, DOI 10.1109/TII.2019.2945403
   Liang WJ, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3193175
   Liang X, 2020, IEEE T CIRC SYST VID, V30, P1758, DOI 10.1109/TCSVT.2019.2905881
   Lim JS, 2021, 3RD INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE IN INFORMATION AND COMMUNICATION (IEEE ICAIIC 2021), P181, DOI 10.1109/ICAIIC51459.2021.9415217
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu G, 2021, IMAGE VISION COMPUT, V111, DOI 10.1016/j.imavis.2021.104197
   Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZM, 2020, IEEE COMPUT SOC CONF, P4422, DOI 10.1109/CVPRW50498.2020.00521
   Ma YD, 2023, MACH VISION APPL, V34, DOI 10.1007/s00138-023-01402-5
   Min K, 2022, NEURAL NETWORKS, V155, P439, DOI 10.1016/j.neunet.2022.08.029
   Oliva A, 2007, TRENDS COGN SCI, V11, P520, DOI 10.1016/j.tics.2007.09.009
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sun C, 2021, APPL INTELL, V51, P3311, DOI 10.1007/s10489-020-01949-0
   Sun PZ, 2021, PROC CVPR IEEE, P14449, DOI 10.1109/CVPR46437.2021.01422
   Tian Z, 2022, IEEE T PATTERN ANAL, V44, P1922, DOI 10.1109/TPAMI.2020.3032166
   Tong K., 2021, 10 INT C COMPUTING P
   Tong K, 2023, J VIS COMMUN IMAGE R, V93, DOI 10.1016/j.jvcir.2023.103830
   Tong K, 2022, IMAGE VISION COMPUT, V123, DOI 10.1016/j.imavis.2022.104471
   Ul Amin S., 2023, Comput. Syst. Sci. Eng, V46, P3939, DOI [10.32604/csse.2023.034805, DOI 10.32604/CSSE.2023.034805]
   Üzen H, 2023, VISUAL COMPUT, V39, P1745, DOI 10.1007/s00371-022-02442-0
   Wang HX, 2023, VISUAL COMPUT, V39, P639, DOI 10.1007/s00371-021-02363-4
   Wang JW, 2021, INT C PATT RECOG, P3791, DOI 10.1109/ICPR48806.2021.9413340
   Wang Q, 2021, J VIS COMMUN IMAGE R, V79, DOI 10.1016/j.jvcir.2021.103260
   Wang SY, 2023, ENG APPL ARTIF INTEL, V117, DOI 10.1016/j.engappai.2022.105504
   Wang Z., 2019, 10 INT C IMAGE GRAPH
   Wu X, 2023, IEEE T IMAGE PROCESS, V32, P364, DOI 10.1109/TIP.2022.3228497
   Xu C, 2022, ISPRS J PHOTOGRAMM, V190, P79, DOI 10.1016/j.isprsjprs.2022.06.002
   Yan ZW, 2021, NEURAL PROCESS LETT, V53, P1921, DOI 10.1007/s11063-021-10493-y
   Yang CHY, 2022, PROC CVPR IEEE, P13658, DOI 10.1109/CVPR52688.2022.01330
   Yang S, 2016, PROC CVPR IEEE, P5525, DOI 10.1109/CVPR.2016.596
   Yu X, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3246494
   Yu XH, 2020, IEEE WINT CONF APPL, P1246, DOI 10.1109/WACV45572.2020.9093394
   Zeng NY, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3153997
   Zhang H, 2021, EXPERT SYST APPL, V185, DOI 10.1016/j.eswa.2021.115673
   Zhang S., 2020, IEEE C COMPUTER VISI
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang YQ, 2020, INT J COMPUT VISION, V128, P1810, DOI 10.1007/s11263-020-01301-6
   Zheng QY, 2021, IMAGE VISION COMPUT, V108, DOI 10.1016/j.imavis.2021.104128
   Zhu CC, 2019, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2019.00093
   Zhu Z, 2016, PROC CVPR IEEE, P2110, DOI 10.1109/CVPR.2016.232
NR 79
TC 0
Z9 0
U1 61
U2 61
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 20
PY 2024
DI 10.1007/s00371-024-03284-8
EA FEB 2024
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IU9U1
UT WOS:001168975900001
DA 2024-08-05
ER

PT J
AU Hu, P
   Wang, CJ
   Li, DQ
   Zhao, X
AF Hu, Peng
   Wang, Chenjun
   Li, Dequan
   Zhao, Xin
TI An improved hybridmultiscale fusion algorithm based on NSST for
   infrared-visible images
SO VISUAL COMPUTER
LA English
DT Article
DE Image fusion; Multiscale decomposition; Morphological; Support value
   transform; Shearlet transform
ID PERFORMANCE; TRANSFORM; NETWORK
AB The key to improving the fusion quality of infrared-visible images is effectively extracting and fusing complementary information such as bright-dark information and saliency details. For this purpose, an improved hybrid multiscale fusion algorithm inspired by non-subsampled shearlet transform (NSST) is proposed. In this algorithm, firstly, the support value transform (SVT) is used instead of the non-subsampled pyramid as the frequency separator to decompose an image into a set of high-frequency support value images and one low-frequency approximate background. These support value images mainly contain the saliency details from the source image. And then, the shearlet transform of NSST is retained to further extract the saliency edges from these support value images. Secondly, to extract the bright-dark details from the low-frequency approximate background, a morphological multiscale top-bottom hat decomposition is constructed. Finally, the extracted information is combined by different rules and the fused image is reconstructed by the corresponding inverse transforms. Experimental results have shown the proposed algorithm has obvious advantages in retaining saliency details and improving image contrast over those state-of-the-art algorithms.
C1 [Hu, Peng; Wang, Chenjun; Li, Dequan; Zhao, Xin] Anhui Univ Sci & Technol, State Key Lab Min Response & Disaster Prevent & C, Huainan 232001, Peoples R China.
   [Hu, Peng; Wang, Chenjun; Li, Dequan; Zhao, Xin] Anhui Univ Sci & Technol, Sch Artificial Intelligence, Huainan 232001, Peoples R China.
C3 Anhui University of Science & Technology; Anhui University of Science &
   Technology
RP Hu, P (corresponding author), Anhui Univ Sci & Technol, State Key Lab Min Response & Disaster Prevent & C, Huainan 232001, Peoples R China.; Hu, P (corresponding author), Anhui Univ Sci & Technol, Sch Artificial Intelligence, Huainan 232001, Peoples R China.
EM aust_hp@163.com
FU Natural Science Research Project of Anhui Educational Committee
   [2022AH050801]; University-level key projects of Anhui University of
   science and technology [QNZD2021-02]; Anhui Provincial Natural Science
   Foundation [2208085ME128]; Scientific Research Foundation for Highlevel
   Talents of Anhui University of Science and Technology [13210679];
   Huainan Science and Technology Planning Project [2021005]
FX We sincerely thank the reviewers and editors for carefully checking our
   manuscript and providing many suggestions. This work is supported by the
   Natural Science Research Project of Anhui Educational Committee (No.
   2022AH050801), University-level key projects of Anhui University of
   science and technology (No. QNZD2021-02), Anhui Provincial Natural
   Science Foundation (No. 2208085ME128), Scientific Research Foundation
   for Highlevel Talents of Anhui University of Science and Technology (No.
   13210679), Huainan Science and Technology Planning Project (No.
   2021005).
CR Averbuch A., 2020, arXiv, DOI [10.48550/arXiv.2008.11595, DOI 10.48550/ARXIV.2008.11595]
   Averbuch A, 2021, SIGNAL PROCESS-IMAGE, V97, DOI 10.1016/j.image.2021.116334
   BAMBERGER RH, 1992, IEEE T SIGNAL PROCES, V40, P882, DOI 10.1109/78.127960
   Bavirisetti DP, 2017, 2017 20TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P701
   Candès E, 2006, MULTISCALE MODEL SIM, V5, P861, DOI 10.1137/05064182X
   Chen J, 2020, INFORM SCIENCES, V508, P64, DOI 10.1016/j.ins.2019.08.066
   Cheng BY, 2018, NEUROCOMPUTING, V310, P135, DOI 10.1016/j.neucom.2018.05.028
   Deepika T, 2020, Arxiv, DOI [arXiv:2007.11488, DOI 10.48550/ARXIV.2007.11488]
   Do MN, 2005, IEEE T IMAGE PROCESS, V14, P2091, DOI 10.1109/TIP.2005.859376
   Easley G, 2008, APPL COMPUT HARMON A, V25, P25, DOI 10.1016/j.acha.2007.09.003
   Guanqiu Qi, 2019, International Journal of Simulation and Process Modelling, V14, P559
   Han Y, 2013, INFORM FUSION, V14, P127, DOI 10.1016/j.inffus.2011.08.002
   He GQ, 2019, IEEE GEOSCI REMOTE S, V16, P1796, DOI 10.1109/LGRS.2019.2907721
   Hu P, 2019, INFRARED PHYS TECHN, V102, DOI 10.1016/j.infrared.2019.102977
   Hu P, 2019, INFRARED PHYS TECHN, V102, DOI 10.1016/j.infrared.2019.102994
   Kong Z., 2021, Journal of Physics: Conference Series, V1994
   KUTYNIOK G., 2007, J WAVELET THEORY APP, V1, P1
   Kwon HJ, 2021, CHEMOSENSORS, V9, DOI 10.3390/chemosensors9040075
   Li H, 2016, INFRARED PHYS TECHN, V74, P28, DOI 10.1016/j.infrared.2015.11.002
   Li H, 2022, Arxiv, DOI arXiv:1804.08992
   Li H, 2021, INFORM FUSION, V73, P72, DOI 10.1016/j.inffus.2021.02.023
   Li H, 2019, INFRARED PHYS TECHN, V102, DOI 10.1016/j.infrared.2019.103039
   Li H, 2018, INT C PATT RECOG, P2705, DOI 10.1109/ICPR.2018.8546006
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li ST, 2017, INFORM FUSION, V33, P100, DOI 10.1016/j.inffus.2016.05.004
   Li ST, 2011, INFORM FUSION, V12, P74, DOI 10.1016/j.inffus.2010.03.002
   Liu CH, 2017, INFRARED PHYS TECHN, V83, P94, DOI 10.1016/j.infrared.2017.04.018
   Ma JY, 2020, COMPUT VIS IMAGE UND, V197, DOI 10.1016/j.cviu.2020.103016
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JY, 2019, INFORM FUSION, V45, P153, DOI 10.1016/j.inffus.2018.02.004
   Ma JL, 2017, INFRARED PHYS TECHN, V82, P8, DOI 10.1016/j.infrared.2017.02.005
   Mitianoudis N, 2007, INFORM FUSION, V8, P131, DOI 10.1016/j.inffus.2005.09.001
   Piella G, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P173
   Qu GH, 2002, ELECTRON LETT, V38, P313, DOI 10.1049/el:20020212
   Roberts JW, 2008, J APPL REMOTE SENS, V2, DOI 10.1117/1.2945910
   SERRA J, 1986, COMPUT VISION GRAPH, V35, P283, DOI 10.1016/0734-189X(86)90002-2
   Sun CQ, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9122162
   Tan ZY, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3050551
   Wang XT, 2020, ARAB J SCI ENG, V45, P3245, DOI 10.1007/s13369-020-04351-7
   Xydeas CS, 2000, ELECTRON LETT, V36, P308, DOI 10.1049/el:20000267
   Yan XA, 2019, IEEE ACCESS, V7, P123436, DOI 10.1109/ACCESS.2019.2937751
   Zhao C, 2019, INT J WAVELETS MULTI, V17, DOI 10.1142/S0219691319500450
   Zheng S, 2007, IEEE T IMAGE PROCESS, V16, P1831, DOI 10.1109/TIP.2007.896687
   Zheng ZF, 2019, IEEE ACCESS, V7, P118472, DOI 10.1109/ACCESS.2019.2936295
   Zhu P, 2017, OPT REV, V24, P370, DOI 10.1007/s10043-017-0331-1
NR 45
TC 2
Z9 3
U1 4
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 1245
EP 1259
DI 10.1007/s00371-023-02844-8
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:001151021700050
DA 2024-08-05
ER

PT J
AU Ali, SG
   Wang, XX
   Li, P
   Li, HT
   Yang, P
   Jung, Y
   Qin, J
   Kim, J
   Sheng, B
AF Ali, Saba Ghazanfar
   Wang, Xiaoxia
   Li, Ping
   Li, Huating
   Yang, Po
   Jung, Younhyun
   Qin, Jing
   Kim, Jinman
   Sheng, Bin
TI EGDNet: an efficient glomerular detection network for multiple anomalous
   pathological feature in glomerulonephritis
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Glomerulonephritis; Object detection; Multi-anomalous pathological
   features; Inter-class imbalance; Feature pyramid balancing
ID SEGMENTATION
AB Glomerulonephritis (GN) is a severe kidney disorder in which the tissues in the kidney become inflamed and have problems filtering waste from the blood. Typical approaches for GN diagnosis require a specialist's examination of pathological glomerular features (PGF) in pathology images of a patient. These PGF are primarily analyzed via manual quantitative evaluation, which is a time-consuming, labor-intensive, and error-prone task for doctors. Thus, automatic and accurate detection of PGF is crucial for the efficient diagnosis of GN and other kidney-related diseases. Recent advances in convolutional neural network-based deep learning methods have shown the capability of learning complex structural variants with promising detection results in medical image applications. However, these methods are not directly applicable to glomerular detection due to large spatial and structural variability and inter-class imbalance. Thus, we propose an efficient glomerular detection network (EGDNet) for the first time for seven types of PGF detection. Our EGDNet consists of four modules: (i) a hybrid data augmentation strategy to resolve dataset problems, (ii) an efficient intersection over unit balancing module for uniform sampling of hard and easy samples, (iii) a feature pyramid balancing module to obtain balanced multi-scale features for robust detection, and (iv) balanced L1 regression loss which alleviates the impact of anomalous data for multi-PGF detection. We also formulated a private dataset of seven PGF from an affiliated hospital in Shanghai, China. Experiments on the dataset show that our EGDNet outperforms state-of-the-art methods by achieving superior accuracy of 91.2%\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\%$$\end{document}, 94.9%\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\%$$\end{document}, and 94.2%\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\%$$\end{document} on small, medium, and large pathological features, respectively.
C1 [Ali, Saba Ghazanfar; Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Wang, Xiaoxia] Shanghai Jiao Tong Univ, Tongren Hosp, Sch Med, Dept Nephrol, Shanghai 200336, Peoples R China.
   [Li, Ping] Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong, Peoples R China.
   [Li, Ping] Hong Kong Polytech Univ, Sch Design, Kowloon, Hong Kong, Peoples R China.
   [Li, Huating] Shanghai Jiao Tong Univ, Affiliated Peoples Hosp 6, Shanghai 200233, Peoples R China.
   [Yang, Po] Univ Sheffield, Dept Comp Sci, Sheffield S1 4DP, England.
   [Jung, Younhyun] Gachon Univ, Dept Software, Seongnam Si, South Korea.
   [Qin, Jing] Hong Kong Polytech Univ, Ctr Smart Hlth, Sch Nursing, Kowloon, Hong Kong, Peoples R China.
   [Kim, Jinman] Univ Sydney, Sch Comp Sci, Biomed & Multimedia Informat Technol Res Grp, Sydney, NSW 2006, Australia.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; Hong Kong
   Polytechnic University; Hong Kong Polytechnic University; Shanghai Jiao
   Tong University; University of Sheffield; Gachon University; Hong Kong
   Polytechnic University; University of Sydney
RP Sheng, B (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.; Jung, Y (corresponding author), Gachon Univ, Dept Software, Seongnam Si, South Korea.
EM saba_ali@sjtu.edu.cn; ouyang1985@sjtu.edu.cn; p.li@polyu.edu.hk;
   huarting99@sjtu.edu.cn; po.yang@sheffield.ac.uk;
   younhyun.jung@gachon.ac.kr; harry.qin@polyu.edu.hk;
   jinman.kim@sydney.edu.au; shengbin@cs.sjtu.edu.cn
FU National Natural Science Foundation [82170745, 200032, 221, 200040]
FX This study was supported by the National Natural Science Foundation
   (Grant No. 82170745). We would like to acknowledge Dr. Zhigang Zhang,
   Department of Pathology, School of Basic Medical Sciences, Fudan
   University, Shanghai, 200032, China, and Dr. Sijia Chen, Department of
   Nephrology, Huadong Hospital Affiliated to Fudan University, No. 221
   West Yan'an Road, Shanghai, 200040, People's Republic of China, for
   their generous help in collecting data and clinical setting.DAS:No
   datasets were generated or analyzed during the current study.
CR Bueno G, 2020, DATA BRIEF, V29, DOI 10.1016/j.dib.2020.105314
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Rehem JMC, 2021, PRO BIOMED OPT IMAG, V11603, DOI 10.1117/12.2582201
   Chang D, 2021, Arxiv, DOI arXiv:2111.12982
   Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Chen L.-C., 2018, P EUR C COMP VIS ECC, P801, DOI DOI 10.1007/978-3-030-01234-2_49
   Chen L.C., 2017, IEEE CVPR, P1
   Fang HS, 2019, IEEE I CONF COMP VIS, P682, DOI 10.1109/ICCV.2019.00077
   Gadermayr M, 2019, COMPUT MED IMAG GRAP, V71, P40, DOI 10.1016/j.compmedimag.2018.11.002
   Ghiasi G., 2020, arXiv:2012.07177, P1
   Jiang L, 2021, AM J PATHOL, V191, P1431, DOI 10.1016/j.ajpath.2021.05.004
   Jiang N, 2019, IEEE COMPUT SOC CONF, P1172, DOI 10.1109/CVPRW.2019.00154
   Kato T, 2015, BMC BIOINFORMATICS, V16, DOI 10.1186/s12859-015-0739-1
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Khened M, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-90444-8
   Kisantal M, 2019, Arxiv, DOI arXiv:1902.07296
   Korman S, 2019, KIDNEY INT REP, V4, P955, DOI 10.1016/j.ekir.2019.04.008
   Kumar P, 2018, 2018 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI), P48, DOI 10.1109/SSCI.2018.8628895
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li Y, 2023, IEEE T MULTIMEDIA, V25, P8372, DOI 10.1109/TMM.2023.3236211
   Lin T.-Y., 2017, PROC CVPR IEEE, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu JR, 2021, IEEE T MED IMAGING, V40, P3580, DOI 10.1109/TMI.2021.3091178
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Ma YH, 2021, IEEE T MED IMAGING, V40, P3955, DOI 10.1109/TMI.2021.3101937
   Marée R, 2016, I S BIOMED IMAGING, P1033, DOI 10.1109/ISBI.2016.7493442
   Priego-Torres BM, 2020, EXPERT SYST APPL, V151, DOI 10.1016/j.eswa.2020.113387
   Marsh JN, 2018, IEEE T MED IMAGING, V37, P2718, DOI 10.1109/TMI.2018.2851150
   Olivier A, 2021, PR MACH LEARN RES, V143, P554
   Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091
   Pedraza A, 2017, COMM COM INF SC, V723, P839, DOI 10.1007/978-3-319-60964-5_73
   Qin DA, 2021, IEEE T MED IMAGING, V40, P3820, DOI 10.1109/TMI.2021.3098703
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Roy M, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-020-80610-9
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salvi M, 2021, COMPUT MED IMAG GRAP, V90, DOI 10.1016/j.compmedimag.2021.101930
   Sheehan S, 2019, AM J PATHOL, V189, P1786, DOI 10.1016/j.ajpath.2019.05.019
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Singh B., 2018, Advances in neural information processing systems, P1
   Singh B, 2018, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2018.00377
   Xiu Chen, 2019, 2019 IEEE 5th International Conference on Computer and Communications (ICCC), P1787, DOI 10.1109/ICCC47050.2019.9064409
   Xu Y., 2020, IEEE Trans. Biomed. Eng, V68, P1
   Yang CK, 2022, BIOMED J, V45, P675, DOI 10.1016/j.bj.2021.08.011
   Yu H, 2018, IEEE IJCNN
   Zhang JJ, 2020, INT J ENV RES PUB HE, V17, DOI 10.3390/ijerph17030866
   Zheng Y, 2021, AM J PATHOL, V191, P1442, DOI 10.1016/j.ajpath.2021.05.005
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou Q, 2023, IEEE T MULTIMEDIA, V25, P9254, DOI 10.1109/TMM.2023.3248966
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 50
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 26
PY 2024
DI 10.1007/s00371-024-03570-5
EA JUL 2024
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZR2E9
UT WOS:001276948900001
DA 2024-08-05
ER

PT J
AU Li, Y
   Li, ZM
   Liu, HW
   Wang, Q
AF Li, Ya
   Li, Ziming
   Liu, Huiwang
   Wang, Qing
TI ZMNet: feature fusion and semantic boundary supervision for real-time
   semantic segmentation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Real-time semantic segmentation; Multi-level feature fusion; Attention
   mechanism; Encoder-decoder architecture; Boundary supervision
ID NETWORK
AB Feature fusion module is an essential component of real-time semantic segmentation networks to bridge the semantic gap among different feature layers. However, many networks are inefficient in multi-level feature fusion. In this paper, we propose a simple yet effective decoder that consists of a series of multi-level attention feature fusion modules (MLA-FFMs) aimed at fusing multi-level features in a top-down manner. Specifically, MLA-FFM is a lightweight attention-based module. Therefore, it can not only efficiently fuse features to bridge the semantic gap at different levels, but also be applied to real-time segmentation tasks. In addition, to solve the problem of low accuracy of existing real-time segmentation methods at semantic boundaries, we propose a semantic boundary supervision module (BSM) to improve the accuracy by supervising the prediction of semantic boundaries. Extensive experiments demonstrate that our network achieves a state-of-the-art trade-off between segmentation accuracy and inference speed on both Cityscapes and CamVid datasets. On a single NVIDIA GeForce 1080Ti GPU, our model achieves 77.4% mIoU with a speed of 97.5 FPS on the Cityscapes test dataset, and 74% mIoU with a speed of 156.6 FPS on the CamVid test dataset, which is superior to most state-of-the-art real-time methods.
C1 [Li, Ya; Li, Ziming; Liu, Huiwang] Guangzhou Univ, Sch Comp Sci & Cyber Engn, Guangzhou 510006, Peoples R China.
   [Wang, Qing] Sun Yat Sen Univ, Sch Comp & Engn, Guangzhou 510275, Peoples R China.
C3 Guangzhou University; Sun Yat Sen University
RP Wang, Q (corresponding author), Sun Yat Sen Univ, Sch Comp & Engn, Guangzhou 510275, Peoples R China.
EM liya@gzhu.edu.cn; liziming_maxlee@outlook.com;
   liuhuiwang1025@outlook.com; wangq79@mail.sysu.edu.cn
FU National Natural Science Foundation of China [61906049]; National
   Natural Science Foundation of China [2022JXGG016]; Guangzhou Higher
   Education Teaching Reform Project
FX This work was supported in part by National Natural Science Foundation
   of China (No. 61906049), and in part by Guangzhou Higher Education
   Teaching Reform Project (No. 2022JXGG016).
CR Brostow GJ, 2008, LECT NOTES COMPUT SC, V5302, P44, DOI 10.1007/978-3-540-88682-2_5
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, 10.48550/arXiv.1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai YM, 2021, IEEE WINT CONF APPL, P3559, DOI 10.1109/WACV48630.2021.00360
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fan MY, 2021, PROC CVPR IEEE, P9711, DOI 10.1109/CVPR46437.2021.00959
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong YD, 2021, Arxiv, DOI arXiv:2101.06085
   Hu J., 2018, SQUEEZE AND EXCITATI, P7132
   Huang Y., 2023, Vis. Comput., P1
   Huang ZL, 2022, IEEE T PATTERN ANAL, V44, P550, DOI 10.1109/TPAMI.2021.3062772
   Jiang M, 2022, VISUAL COMPUT, V38, P2473, DOI 10.1007/s00371-021-02124-3
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li HC, 2019, PROC CVPR IEEE, P9514, DOI 10.1109/CVPR.2019.00975
   Li P., 2020, 31 BRIT MACH VIS VIR
   Li Y, 2022, AAAI CONF ARTIF INTE, P1438
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Liu J, 2023, NEUROCOMPUTING, V521, P27, DOI 10.1016/j.neucom.2022.11.084
   Luo D, 2023, NEUROCOMPUTING, V520, P205, DOI 10.1016/j.neucom.2022.11.075
   Orsic M, 2019, PROC CVPR IEEE, P12599, DOI 10.1109/CVPR.2019.01289
   Park J, 2018, Arxiv, DOI [arXiv:1807.06514, 10.48550/arXiv.1807.06514]
   Paszke A, 2016, Arxiv, DOI arXiv:1606.02147
   Poudel R. P. K., 2019, arXiv
   Sheng PP, 2022, NEUROCOMPUTING, V509, P94, DOI 10.1016/j.neucom.2022.08.049
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Sun S., 2023, IEEE Transactions on Image Processing
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Weng X, 2022, IEEE T CIRC SYST VID, V32, P4444, DOI 10.1109/TCSVT.2021.3121680
   Weng X, 2022, IEEE T INTELL TRANSP, V23, P17224, DOI 10.1109/TITS.2022.3150350
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xiangtai Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P775, DOI 10.1007/978-3-030-58452-8_45
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Yin H, 2023, MACH VISION APPL, V34, DOI 10.1007/s00138-023-01373-7
   Yu CQ, 2021, INT J COMPUT VISION, V129, P3051, DOI 10.1007/s11263-021-01515-2
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu F, 2018, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2018.00255
   Zhang H, 2022, IEEE COMPUT SOC CONF, P2735, DOI 10.1109/CVPRW56347.2022.00309
   Zhang WQ, 2022, PROC CVPR IEEE, P12073, DOI 10.1109/CVPR52688.2022.01177
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhao Shilei, 2023, 2023 IEEE International Conference on Multimedia and Expo Workshops (ICMEW), P69, DOI 10.1109/ICMEW59549.2023.00018
   Zhen M., 2020, IEEE C COMPUT VIS PA, P13666, DOI DOI 10.48550/ARXIV.2004.07684
NR 43
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 20
PY 2024
DI 10.1007/s00371-024-03448-6
EA JUN 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UW9Q5
UT WOS:001251223700002
DA 2024-08-05
ER

PT J
AU Thai, A
   Gradus-Pizlo, I
   Pizlo, Z
   Sahin, H
   Gopi, M
AF Thai, Andy
   Gradus-Pizlo, Irmina
   Pizlo, Zygmunt
   Sahin, Hakan
   Gopi, M.
TI Automatic segmentation and implicit surface representation of dynamic
   cardiac data
SO VISUAL COMPUTER
LA English
DT Article
DE Medical imaging; Image segmentation; Shape and surface modeling; Human
   heart
ID ATLASES; IMAGES; VOLUME
AB Segmentation of anatomical structures on 2D images of cardiac exams is necessary for performing 3D volumetric analysis, enabling the computation of parameters for diagnosing cardiovascular disease. In this work, we present robust algorithms to automatically segment cardiac imaging data and generate a volumetric anatomical reconstruction of a patient-specific heart model by propagating active contour output within a patient stack through a self-supervised learning model. Contour initializations are automatically generated, then output segmentations on sparse image slices are transferred and merged across a stack of images within the same heart data set during the segmentation process. We demonstrate whole-heart segmentation and compare the results with ground truth manual annotations. Additionally, we provide a framework to represent segmented heart data in the form of implicit surfaces, allowing interpolation operations to generate intermediary models of heart sections and volumes throughout the cardiac cycle and to estimate ejection fraction.
C1 [Thai, Andy; Gopi, M.] Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92697 USA.
   [Gradus-Pizlo, Irmina] Univ Calif Irvine, Sch Med, Dept Med, Div Cardiol, Irvine, CA USA.
   [Pizlo, Zygmunt] Univ Calif Irvine, Dept Cognit Sci, Irvine, CA USA.
C3 University of California System; University of California Irvine;
   University of California System; University of California Irvine;
   University of California System; University of California Irvine
RP Thai, A (corresponding author), Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92697 USA.
EM andy.thai@uci.edu
OI Thai, Andy/0000-0001-7094-8508
CR Appleton B., 2003, APRS WORKSH DIG IM C
   Bai JW, 2016, PROCEEDINGS OF 2016 5TH INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND NETWORK TECHNOLOGY (ICCSNT), P741, DOI 10.1109/ICCSNT.2016.8070256
   Balakrishnan G., 2018, ARXIV
   Bernard O, 2018, IEEE T MED IMAGING, V37, P2514, DOI 10.1109/TMI.2018.2837502
   Bui I., 2022, VESSEL PLUS, V6, P31, DOI [10.20517/2574-1209.2021.97, DOI 10.20517/2574-1209.2021.97]
   Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043
   Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291
   Chen C, 2020, FRONT CARDIOVASC MED, V7, DOI 10.3389/fcvm.2020.00025
   Chen X, 2019, PROC CVPR IEEE, P11624, DOI 10.1109/CVPR.2019.01190
   COHEN LD, 1991, CVGIP-IMAG UNDERSTAN, V53, P211, DOI 10.1016/1049-9660(91)90028-N
   DICE LR, 1945, ECOLOGY, V26, P297, DOI 10.2307/1932409
   El-Taraboulsi J., 2023, ARTIF INTELL LIFE SC, V4, P100083, DOI [10.1016/j.ailsci.2023.100083, DOI 10.1016/J.AILSCI.2023.100083]
   Iglesias JE, 2015, MED IMAGE ANAL, V24, P205, DOI 10.1016/j.media.2015.06.012
   Fonseca CG, 2011, BIOINFORMATICS, V27, P2288, DOI 10.1093/bioinformatics/btr360
   Frangi AF, 2001, IEEE T MED IMAGING, V20, P2, DOI 10.1109/42.906421
   Gharleghi R, 2022, COMPUT METH PROG BIO, V225, DOI 10.1016/j.cmpb.2022.107015
   Habijan M, 2019, INT CONF SYST SIGNAL, P121, DOI 10.1109/iwssip.2019.8787253
   Habijan M, 2020, CARDIOVASC ENG TECHN, V11, P725, DOI 10.1007/s13239-020-00494-8
   Hemalatha R., 2018, MEDICAL BIOLOGICAL I
   Huang HM, 2020, INT CONF ACOUST SPEE, P1055, DOI [10.1109/icassp40776.2020.9053405, 10.1109/ICASSP40776.2020.9053405]
   Kass M., 1987, International Journal of Computer Vision, V1, P321, DOI 10.1007/BF00133570
   Kikinis R., 2014, 3D SLICER PLATFORM S, V3, P277
   Klein S, 2010, IEEE T MED IMAGING, V29, P196, DOI 10.1109/TMI.2009.2035616
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lederman C, 2010, I S BIOMED IMAGING, P436, DOI 10.1109/ISBI.2010.5490317
   Lee HY, 2010, IEEE T BIO-MED ENG, V57, P905, DOI 10.1109/TBME.2009.2014545
   Leow LJH, 2024, MATHEMATICS-BASEL, V12, DOI 10.3390/math12040616
   Liu ZK, 2021, FRONT ONCOL, V10, DOI 10.3389/fonc.2020.581347
   Lorensen H. E., 1987, Proc. SIGGRAPH, V21, P163, DOI 10.1145/37401.37422
   Márquez-Neila P, 2014, IEEE T PATTERN ANAL, V36, P2, DOI 10.1109/TPAMI.2013.106
   MENET S, 1990, 1990 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS, P194, DOI 10.1109/ICSMC.1990.142091
   Moe-Byrne T, 2022, FRONT DIGIT HEALTH, V4, DOI 10.3389/fdgth.2022.1010779
   MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503
   NAJMAN L, 1994, SIGNAL PROCESS, V38, P99, DOI 10.1016/0165-1684(94)90059-0
   Nugroho PA, 2016, 2016 INTERNATIONAL CONFERENCE ON KNOWLEDGE CREATION AND INTELLIGENT COMPUTING (KCIC), P35, DOI 10.1109/KCIC.2016.7883622
   Perry R., 2009, MIDAS J, V49, DOI [10.54294/g80ruo, DOI 10.54294/G80RUO]
   Plueimpitiwiriyawej C, 2005, IEEE T MED IMAGING, V24, P593, DOI 10.1109/TMI.2005.843740
   Ranjan A, 2018, LECT NOTES COMPUT SC, V11207, P725, DOI 10.1007/978-3-030-01219-9_43
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schreier J, 2020, RADIOTHER ONCOL, V145, P1, DOI 10.1016/j.radonc.2019.11.021
   Sharif Hanan, 2022, 2022 International Conference on IT and Industrial Technologies (ICIT), P01, DOI 10.1109/ICIT56493.2022.9988971
   Soomro S, 2017, COMPUT MATH METHOD M, V2017, DOI 10.1155/2017/8350680
   Sorensen T., 1948, Biol. Skr, V5, P1, DOI DOI 10.1234/12345678
   Suinesiaputra A, 2015, IEEE J BIOMED HEALTH, V19, P1283, DOI 10.1109/JBHI.2014.2370952
   Sullivan C.B., 2019, Journal of Open Source Software, V4, P1450, DOI DOI 10.21105/JOSS.01450
   Tan Q., 2018, P 30 AAAI C ART INT, V32, P1
   TEH CH, 1989, IEEE T PATTERN ANAL, V11, P859, DOI 10.1109/34.31447
   Turk G, 1999, COMP GRAPH, P335, DOI 10.1145/311535.311580
   Bui V, 2020, COMPUT BIOL MED, V125, DOI 10.1016/j.compbiomed.2020.104019
   World Health Organization, 2022, WHO Noncommunicable Diseases Progress Monitor Reports
   Yeung PH, 2021, LECT NOTES COMPUT SC, V12902, P69, DOI 10.1007/978-3-030-87196-3_7
   Zhang Q, 2011, J DIGIT IMAGING, V24, P640, DOI 10.1007/s10278-010-9321-6
   Zhuang XH, 2016, MED IMAGE ANAL, V31, P77, DOI 10.1016/j.media.2016.02.006
NR 53
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4869
EP 4883
DI 10.1007/s00371-024-03486-0
EA JUN 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001244596700004
OA hybrid
DA 2024-08-05
ER

PT J
AU Ruprecht, I
   Michelic, F
   Eggeling, E
   Preiner, R
AF Ruprecht, Irena
   Michelic, Florian
   Eggeling, Eva
   Preiner, Reinhold
TI Adaptive movement behavior for real-time crowd simulation
SO VISUAL COMPUTER
LA English
DT Review
DE Real-time crowd simulation; Agent-based simulation; Physically based
   modeling; Computing methodologies; Real-time simulation; Agent/discrete
   models
AB We present a novel method for modeling adaptive movement behavior in force-based crowd simulations tailored to the requirements of interactive real-time applications. We aim for an efficient, controllable and situation-dependent synthesis of global path planning and local collision avoidance. To achieve this, we address two typical problems that can cause severe performance impacts and undesired motion: local neighbor search and the combination of goal directions with collision avoidance. We employ a direction-aware neighbor set to efficiently select agents for collision avoidance computation and crowd density estimation. We adaptively blend between goal and avoidance directions to utilize the entire spectrum of navigation strategies for goal attainment, ranging from completely goal-oriented to strongly avoidance-centric behaviors. Our method can be combined with established force-based simulation techniques and easily integrates into existing real-time simulation pipelines. Experiments show that our method can improve control over run-time performance as well as the movement behavior of agents compared to plain force-based methods.
C1 [Ruprecht, Irena; Eggeling, Eva] Fraunhofer Austria Res GmbH, Inffeldgasse 16C, A-8010 Graz, Austria.
   [Michelic, Florian; Preiner, Reinhold] Graz Univ Technol, Inffeldgasse 16C, A-8010 Graz, Austria.
C3 Graz University of Technology
RP Ruprecht, I (corresponding author), Fraunhofer Austria Res GmbH, Inffeldgasse 16C, A-8010 Graz, Austria.
EM irena.ruprecht@fraunhofer.at
OI Preiner, Reinhold/0000-0002-5167-1977; Michelic,
   Florian/0009-0004-9937-5757; Ruprecht, Irena/0000-0002-0749-1186
CR Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110
   Best A., 2014, P SCA 14
   Bicho AD, 2012, COMPUT GRAPH-UK, V36, P70, DOI 10.1016/j.cag.2011.12.004
   Boatright CD, 2015, COMPUT ANIMAT VIRT W, V26, P483, DOI 10.1002/cav.1572
   Bruneau J, 2017, COMPUT GRAPH FORUM, V36, P108, DOI 10.1111/cgf.13066
   Chen Q, 2021, COMPUT ANIMAT VIRT W, V32, DOI 10.1002/cav.1977
   Dutra TB, 2017, COMPUT GRAPH FORUM, V36, P337, DOI 10.1111/cgf.13130
   Guy S., 2015, GAME AI PRO 2 COLLEC, P195
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   Hocker M., 2010, EWORK EBUSINESS ARCH, P389, DOI [10.1201/b10527, DOI 10.1201/B10527, DOI 10.1201/B10527-65]
   Karamouzas I, 2014, PHYS REV LETT, V113, DOI 10.1103/PhysRevLett.113.238701
   Karamouzas I, 2009, LECT NOTES COMPUT SC, V5884, P41, DOI 10.1007/978-3-642-10347-6_4
   Lee J, 2018, ACM SIGGRAPH CONFERENCE ON MOTION, INTERACTION, AND GAMES (MIG 2018), DOI 10.1145/3274247.3274510
   Li QR, 2020, CHAOS, V30, DOI 10.1063/1.5132945
   Moussaïd M, 2011, P NATL ACAD SCI USA, V108, P6884, DOI 10.1073/pnas.1016507108
   Narain R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618468
   Ondrej J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778860
   Pelechano N, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P99
   Ren Z, 2017, COMPUT GRAPH FORUM, V36, P45, DOI 10.1111/cgf.12993
   Thalmann D., 2013, Crowd Simulation, V2nd, DOI DOI 10.1007/978-1-4471-4450-2
   van den Berg J, 2008, IEEE INT CONF ROBOT, P1928, DOI 10.1109/ROBOT.2008.4543489
   van den Berg J, 2011, SPRINGER TRAC ADV RO, V70, P3
   van Toll W, 2021, COMPUT GRAPH FORUM, V40, P731, DOI 10.1111/cgf.142664
   Van Toll W., 2019, ACM SIGGRAPH C MOT I
   van Toll W, 2021, COMPUT GRAPH-UK, V98, P306, DOI 10.1016/j.cag.2021.06.005
   van Toll WG, 2012, COMPUT ANIMAT VIRT W, V23, P59, DOI 10.1002/cav.1424
   Warren WH, 2018, CURR DIR PSYCHOL SCI, V27, P232, DOI 10.1177/0963721417746743
   Weiss T, 2019, COMPUT GRAPH-UK, V78, P12, DOI 10.1016/j.cag.2018.10.008
NR 28
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4789
EP 4803
DI 10.1007/s00371-024-03476-2
EA JUN 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001242180100002
DA 2024-08-05
ER

PT J
AU Wang, SL
   Hou, QW
   Li, JA
   Liu, JL
AF Wang, Shilong
   Hou, Qianwen
   Li, Jiaang
   Liu, Jianlei
TI TSID-Net: a two-stage single image dehazing framework with style
   transfer and contrastive knowledge transfer
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Single image dehazing; Style transfer; Comparative knowledge transfer;
   Unsupervised learning
AB Haze-free images have become a prerequisite for many computer vision tasks; therefore, single image dehazing is particularly important in the field of computer vision. However, existing deep learning dehazing methods face two main problems. First, existing dehazing methods are mostly trained based on paired images, but obtaining paired data of the same scene in the real world is challenging, which limits their dehazing performance. Second, most existing dehazing methods are primarily result-driven, which disregards the intermediate process of dehazing, and the rich prior knowledge present in clear and hazy images is not fully utilized, resulting in significant deviations between the dehazed results and the ground truth. Therefore, we propose a novel two-stage single image dehazing network, TSID-Net, to address the above two issues. In the first stage, we consider hazy images as a form of hazy artistic style, while clear images serve as the content information of the artwork. By combining style transfer, we generate high-quality and diverse paired images. This approach significantly mitigates the challenge of acquiring paired data and provides an ample training sample for the second stage. In the second stage, we utilize abundant clear and hazy images to train positive and negative teacher networks with strong robust prior learning capabilities. By combining knowledge transfer, contrastive learning and process-oriented mechanism, we achieve effective knowledge transfer and contrastive knowledge transfer of the intermediate features in the student network. Additionally, we propose a style version bank and incorporate curricular contrastive regularization to achieve dual contrastive learning of both the process and results for student network. Extensive experimental results demonstrate that TSID-Net effectively removes haze and produces visually pleasing results. Code is available at: https://github.com/wsl666/TSID-Net.git.
C1 [Wang, Shilong; Hou, Qianwen; Li, Jiaang; Liu, Jianlei] Qufu Normal Univ, Sch Cyber Sci & Engn, Jining 273165, Peoples R China.
C3 Qufu Normal University
RP Liu, JL (corresponding author), Qufu Normal Univ, Sch Cyber Sci & Engn, Jining 273165, Peoples R China.
EM wangshilong@qfnu.edu.cn; houqianwen@qfnu.edu.cn; lijang2000@qfnu.edu.cn;
   jianleiliu@qfnu.edu.cn
FU National Natural Science Foundation of China; Engineering Project for
   Improving the Innovation Capability of Technology-oriented Small and
   Medium-sized Enterprises [2023TSGC0293];  [61773243]
FX This work is partially supported by the National Natural Science
   Foundation of China (Grant No. 61773243) and the Engineering Project for
   Improving the Innovation Capability of Technology-oriented Small and
   Medium-sized Enterprises(2023TSGC0293).
CR Ancuti CO, 2018, IEEE COMPUT SOC CONF, P867, DOI 10.1109/CVPRW.2018.00119
   Ancuti C, 2018, LECT NOTES COMPUT SC, V11182, P620, DOI 10.1007/978-3-030-01449-0_52
   Bai HR, 2022, IEEE T IMAGE PROCESS, V31, P1217, DOI 10.1109/TIP.2022.3140609
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Cheema MN, 2021, IEEE T IND INFORM, V17, P7991, DOI 10.1109/TII.2021.3064369
   Chen HB, 2021, ADV NEUR IN, V34
   Chen HB, 2021, PROC CVPR IEEE, P872, DOI 10.1109/CVPR46437.2021.00093
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen WT, 2022, PROC CVPR IEEE, P17632, DOI 10.1109/CVPR52688.2022.01713
   Chen ZY, 2021, PROC CVPR IEEE, P7176, DOI 10.1109/CVPR46437.2021.00710
   Chen ZX, 2024, IEEE T IMAGE PROCESS, V33, P1002, DOI 10.1109/TIP.2024.3354108
   Cheng D., 2024, IEEE Trans. Multimed, DOI [10.1109/TMM.2024.3382493, DOI 10.1109/TMM.2024.3382493]
   Engin D, 2018, IEEE COMPUT SOC CONF, P938, DOI 10.1109/CVPRW.2018.00127
   Galdran A, 2018, SIGNAL PROCESS, V149, P135, DOI 10.1016/j.sigpro.2018.03.008
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Grill J.B., 2020, P 34 INT C NEUR INF, V33, P21271, DOI [10.48550/arXiv.2006.07733, DOI 10.48550/ARXIV.2006.07733]
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He Sunan, 2023, P AAAI C ARTIFICIAL, P808, DOI DOI 10.48550/ARXIV.2207.01887
   Henaff OJ, 2020, PR MACH LEARN RES, V119
   Hong M, 2020, PROC CVPR IEEE, P3459, DOI 10.1109/CVPR42600.2020.00352
   Jiang YX, 2023, IEEE I CONF COMP VIS, P7323, DOI 10.1109/ICCV51070.2023.00676
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Karambakhsh A, 2024, IEEE T NEUR NET LEAR, V35, P532, DOI 10.1109/TNNLS.2022.3175775
   Kingma D. P., 2014, arXiv
   Kotovenko D, 2019, PROC CVPR IEEE, P10024, DOI 10.1109/CVPR.2019.01027
   Lan YW, 2023, J ELECTRON IMAGING, V32, DOI 10.1117/1.JEI.32.1.013002
   Lan YW, 2022, FRONT NEUROROBOTICS, V16, DOI 10.3389/fnbot.2022.1036465
   Lan YW, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-19132-5
   Li BY, 2017, Arxiv, DOI arXiv:1707.06543
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2022, PROC CVPR IEEE, P17431, DOI 10.1109/CVPR52688.2022.01693
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li JF, 2023, IEEE T MULTIMEDIA, V25, P3587, DOI 10.1109/TMM.2022.3163554
   Li SS, 2023, IEEE T IMAGE PROCESS, V32, P6558, DOI 10.1109/TIP.2023.3333564
   Li Y., 2022, Advances in Neural Information Processing Systems, V35, P18442
   Li ZH, 2023, IEEE T PATTERN ANAL, V45, P10555, DOI 10.1109/TPAMI.2023.3257546
   Lin X, 2023, IEEE I CONF COMP VIS, P12608, DOI 10.1109/ICCV51070.2023.01162
   Ling PY, 2023, IEEE T IMAGE PROCESS, V32, P3238, DOI 10.1109/TIP.2023.3279980
   Liu J, 2023, IEEE T PATTERN ANAL, V45, P8845, DOI 10.1109/TPAMI.2022.3226276
   Liu P, 2023, VISUAL COMPUT, DOI 10.1007/s00371-023-03177-2
   Liu XH, 2023, IEEE T INTELL TRANSP, V24, P870, DOI 10.1109/TITS.2022.3210455
   Lu M, 2019, IEEE I CONF COMP VIS, P5951, DOI 10.1109/ICCV.2019.00605
   Mallick T, 2014, IEEE SENS J, V14, P1731, DOI 10.1109/JSEN.2014.2309987
   McCartney E. J., 1976, Optics of the atmosphere. Scattering by molecules and particles
   Park DY, 2019, PROC CVPR IEEE, P5873, DOI 10.1109/CVPR.2019.00603
   Park T., 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-719, 10.1007/978-3-030-58545-719, DOI 10.1007/978-3-030-58545-7_19]
   Pernus Martin, 2023, IEEE Trans Image Process, V32, P5893, DOI 10.1109/TIP.2023.3326675
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Qiu YW, 2023, IEEE I CONF COMP VIS, P12756, DOI 10.1109/ICCV51070.2023.01176
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Sanakoyeu A, 2018, LECT NOTES COMPUT SC, V11212, P715, DOI 10.1007/978-3-030-01237-3_43
   Shao YJ, 2020, PROC CVPR IEEE, P2805, DOI 10.1109/CVPR42600.2020.00288
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song XB, 2023, IEEE T IMAGE PROCESS, V32, P1231, DOI 10.1109/TIP.2023.3234701
   Song YD, 2023, IEEE T IMAGE PROCESS, V32, P1927, DOI 10.1109/TIP.2023.3256763
   Su YZ, 2023, PATTERN RECOGN, V142, DOI 10.1016/j.patcog.2023.109700
   Su YZ, 2021, NEUROCOMPUTING, V423, P620, DOI 10.1016/j.neucom.2020.10.061
   Sweeney C, 2019, IEEE INT CONF ROBOT, P796, DOI [10.1109/icra.2019.8793820, 10.1109/ICRA.2019.8793820]
   Torbunov D, 2023, IEEE WINT CONF APPL, P702, DOI 10.1109/WACV56688.2023.00077
   Tran L, 2024, VISUAL COMPUT, DOI 10.1007/s00371-024-03330-5
   Wang N, 2022, CIRC-CARDIOVASC QUAL, V15, P416, DOI 10.1161/CIRCOUTCOMES.121.008552
   Wang N, 2021, AIP ADV, V11, DOI 10.1063/5.0059424
   Wang YZ, 2024, IEEE T IMAGE PROCESS, V33, P1361, DOI 10.1109/TIP.2024.3362153
   Wu HY, 2021, PROC CVPR IEEE, P10546, DOI 10.1109/CVPR46437.2021.01041
   Wu HY, 2020, IEEE COMPUT SOC CONF, P1975, DOI 10.1109/CVPRW50498.2020.00247
   Wu ZJ, 2020, AAAI CONF ARTIF INTE, V34, P12305
   Yi WC, 2024, MULTIMED TOOLS APPL, DOI 10.1007/s11042-024-18502-7
   Yi WC, 2024, EXPERT SYST APPL, V235, DOI 10.1016/j.eswa.2023.121130
   Zhang C, 2022, IEEE T CIRC SYST VID, V32, P4552, DOI 10.1109/TCSVT.2021.3130158
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang YL, 2019, IEEE I CONF COMP VIS, P5942, DOI 10.1109/ICCV.2019.00604
   Zhao SY, 2021, IEEE T IMAGE PROCESS, V30, P3391, DOI 10.1109/TIP.2021.3060873
   Zhao SY, 2020, IEEE T IMAGE PROCESS, V29, P6947, DOI 10.1109/TIP.2020.2995264
   Zheng Y, 2023, PROC CVPR IEEE, P5785, DOI 10.1109/CVPR52729.2023.00560
   Zhou JH, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107529
   Zhou Y, 2023, IEEE T NEUR NET LEAR, V34, P7719, DOI 10.1109/TNNLS.2022.3146004
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 80
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 7
PY 2024
DI 10.1007/s00371-024-03511-2
EA JUN 2024
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TO4C0
UT WOS:001242180100005
DA 2024-08-05
ER

PT J
AU Mohammed, A
   Samundiswary, P
AF Mohammed, Ajmal
   Samundiswary, P.
TI SecMISS: Secured Medical Image Secret Sharing mechanism for smart health
   applications
SO VISUAL COMPUTER
LA English
DT Article
DE Visual Secret Sharing (VSS); Security; Super-resolution; Medical images;
   Smart health
ID GRAY-SCALE; PRIVACY; SCHEME; ISSUES
AB Sustainable smart city initiatives significantly improve the living standards of citizens and bring significant changes in economic, environmental, and social well-being. Sustainable and remote medical services are vital attributes of modern smart health infrastructure. Smart remote health infrastructure uses interconnected devices to gather and exchange health data. In smart health infrastructure, massive quantities of health data are gathered as Personal Health Records (PHR). Clinical images form a major part of the accumulated PHR. As medical images are confidential and sensitive, secure storage and transmission are significant. Hence, different cryptographic techniques are used to enhance medical image security. The conventional cryptographic schemes are complex and generate a single cipher image corresponding to a plain image. The cipher images are stored in a single database. Such storage makes the cryptosystem weak against single-point attacks. This paper discusses a simple, keyless, and distributed secure storage mechanism for medical images called SecMISS. SecMISS uses Random Grid-based Visual Secret Sharing (RGVSS) along with super-resolution for secure encryption with distributed storage and better reconstruction. The security parameters such as SSIM (approximate to\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\approx $$\end{document} 0), correlation of adjacent pixels (approximate to\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\approx $$\end{document} 0), PSNR (<=\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\le $$\end{document} 8 dB), and entropy (= 1) between share and initial image show that SecMISS provides highest level of security. Similarly, different Human Visual System (HVS) parameters between the initial and reconstructed images show an improvement in the reconstruction. Experimental results show that SecMISS outperforms the existing technique in terms of security and reconstruction. Thereby SecMISS can be efficiently used for securing medical image in sustainable smart hospitals and health infrastructures.
C1 [Mohammed, Ajmal; Samundiswary, P.] Pondicherry Univ, Dept Elect Engn, Pondicherry, India.
C3 Pondicherry University
RP Mohammed, A (corresponding author), Pondicherry Univ, Dept Elect Engn, Pondicherry, India.
EM ajmalvm@pondiuni.ac.in; sam.dee@pondiuni.ac.in
RI Mohammed, Ajmal/JFK-9031-2023
OI Mohammed, Ajmal/0000-0001-9510-8310
CR Afandi T.M.K., 2021, Medical images compression and encryption using DCT, arithmetic encoding and chaos-based encryption
   Ahmad S, 2021, MULTIMED TOOLS APPL, V80, P32071, DOI 10.1007/s11042-021-11152-z
   Ajmal Mohammed V. M., 2022, 2022 Third International Conference on Intelligent Computing Instrumentation and Control Technologies (ICICICT), P887, DOI 10.1109/ICICICT54557.2022.9917608
   Aouissaoui I, 2021, IET IMAGE PROCESS, V15, P2770, DOI 10.1049/ipr2.12261
   Camara C, 2015, J BIOMED INFORM, V55, P272, DOI 10.1016/j.jbi.2015.04.007
   Dagher GG, 2018, SUSTAIN CITIES SOC, V39, P283, DOI 10.1016/j.scs.2018.02.014
   Ding Y, 2023, VISUAL COMPUT, V39, P1517, DOI 10.1007/s00371-022-02426-0
   El-Shafai W, 2021, J AMB INTEL HUM COMP, V12, P9007, DOI 10.1007/s12652-020-02597-5
   Elad M, 1997, IEEE T IMAGE PROCESS, V6, P1646, DOI 10.1109/83.650118
   Fang PF, 2023, VISUAL COMPUT, V39, P1975, DOI 10.1007/s00371-022-02459-5
   Geetha BT, 2022, COMPUT INTEL NEUROSC, V2022, DOI 10.1155/2022/2243827
   Girma A., 2018, Information Technology-New Generations
   Hatzivasilis G, 2019, IEEE INT CONF DISTR, P457, DOI 10.1109/DCOSS.2019.00091
   Janani T, 2021, J INF SECUR APPL, V59, DOI 10.1016/j.jisa.2021.102832
   John S., 2023, Meas. Sens., V25
   Kamal ST, 2021, IEEE ACCESS, V9, P37855, DOI 10.1109/ACCESS.2021.3063237
   Kiran P, 2022, MICROPROCESS MICROSY, V91, DOI 10.1016/j.micpro.2022.104546
   Kumar S, 2019, MED BIOL ENG COMPUT, V57, P2517, DOI 10.1007/s11517-019-02037-3
   Lin CH, 2021, IEEE ACCESS, V9, P118624, DOI 10.1109/ACCESS.2021.3107608
   Mamdouh Moustafa, 2020, Proceedings of the International Conference on Artificial Intelligence and Computer Vision (AICV2020). Advances in Intelligent Systems and Computing (AISC 1153), P721, DOI 10.1007/978-3-030-44289-7_67
   Mhala NC, 2021, VISUAL COMPUT, V37, P2097, DOI 10.1007/s00371-020-01972-9
   Mhala NC, 2019, SIGNAL PROCESS, V162, P253, DOI 10.1016/j.sigpro.2019.04.023
   Mhala NC, 2018, IET IMAGE PROCESS, V12, P422, DOI 10.1049/iet-ipr.2017.0759
   Mishra P, 2021, J AMB INTEL HUM COMP, DOI 10.1007/s12652-021-03410-7
   Muhammed A, 2021, MULTIMED TOOLS APPL, V80, P10255, DOI 10.1007/s11042-020-10095-1
   Mukhopadhyay S, 2021, MULTIMED TOOLS APPL, V80, P14495, DOI 10.1007/s11042-020-10424-4
   Murali P, 2023, VISUAL COMPUT, V39, P1057, DOI 10.1007/s00371-021-02384-z
   Naor M., 1995, Advances in Cryptology - EUROCRYPT '94. Workshop on the Theory and Application of Cryptographic Techniques. Proceedings, P1, DOI 10.1007/BFb0053419
   Nazir S, 2020, IEEE ACCESS, V8, P95714, DOI 10.1109/ACCESS.2020.2995572
   Panwar A, 2022, COMPUT INTEL NEUROSC, V2022, DOI 10.1155/2022/3045107
   Rathore MM, 2018, SUSTAIN CITIES SOC, V40, P600, DOI 10.1016/j.scs.2017.12.022
   Samaila MG, 2018, SECUR PRIVACY, V1, DOI 10.1002/spy2.20
   Sarosh P, 2021, SUSTAIN CITIES SOC, V74, DOI 10.1016/j.scs.2021.103129
   Sheng Q., 2023, The Visual Computer, P16
   Sliwa J, 2018, INTELL DAT CENT SYST, P121, DOI 10.1016/B978-0-12-812130-6.00007-X
   Srividhya S, 2016, J VIS COMMUN IMAGE R, V38, P284, DOI 10.1016/j.jvcir.2016.03.012
   Thien CC, 2003, IEEE T CIRC SYST VID, V13, P1161, DOI 10.1109/TCSVT.2003.819176
   Tiwari A, 2023, MULTIMED TOOLS APPL, DOI 10.1007/s11042-023-15878-w
   Wang XW, 2022, SECUR COMMUN NETW, V2022, DOI 10.1155/2022/4260804
   Yan XH, 2019, J INF SECUR APPL, V47, P208, DOI 10.1016/j.jisa.2019.05.008
   Yan XH, 2018, J REAL-TIME IMAGE PR, V14, P61, DOI 10.1007/s11554-015-0540-4
   Yangxiu Fang, 2021, Innovation in Medicine and Healthcare. Proceedings of 9th KES-InMed 2021. Smart Innovation, Systems and Technologies (SIST 242), P61, DOI 10.1007/978-981-16-3013-2_6
   Yin S., 2020, Int. J. Netw. Secur., V22, P419
   Yousef R, 2022, MULTIMEDIA SYST, V28, P881, DOI 10.1007/s00530-021-00884-5
NR 44
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4251
EP 4271
DI 10.1007/s00371-023-03080-w
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001243980400006
DA 2024-08-05
ER

PT J
AU Chen, JW
   Su, W
   Ge, MJ
   He, Y
   Yu, J
AF Chen, Jiawei
   Su, Wen
   Ge, Mengjiao
   He, Ye
   Yu, Jun
TI To-Former: semantic segmentation of transparent object with
   edge-enhanced transformer
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Semantic segmentation; Transparent objects; Edge enhanced; Multi-scale
AB Transparent objects are widely present in our daily environment. The precise semantic segmentation of transparent objects is crucial for enhancing the perception and comprehension of visual scenes in computer vision systems. Unlike conventional objects, transparent objects are difficult to distinguish from the background or other objects due to blending, color changes, and blurred boundaries. Accurate segmentation requires global context, multi-scale edge information, and long-distance pixel dependencies. According to the above facts, we propose a simple and efficient semantic segmentation architecture for transparent object known as transparent object transformer (To-Former). We introduce an edge-enhanced multi-head self-attention mechanism that incorporates multi-scale separable convolution and pooling. It is specially designed to efficiently extract both edge and global features from transparent objects. We propose the convolutional feed-forward network to enhance features at multiple scales to attain a superior feature representation while reducing the computational complexity. The To-Former constructs a reinforced decoder, integrating the feature channel enhancement module. It has the potential to significantly enhance feature channels, thereby improving the feature fusion effect and the segmentation effectiveness of different feature categories. The benchmark experiments show that our To-Former can achieve consistent improvements with fewer parameters over several typical and state-of-the-art segmentation baseline models on challenging public benchmarks. It is even impressive in the boundary positioning of transparent objects in simple and complex scenes. The codes are available at https://github.com/cehn-jiawei/To-Former
C1 [Chen, Jiawei; Su, Wen; Ge, Mengjiao; He, Ye] Zhejiang Sci Tech Univ, Sch Informat Sci & Engn, 2 St,Xiasha High Campus Dist, Hangzhou 310018, Zhejiang, Peoples R China.
   [Yu, Jun] Univ Sci & Technol China, Sch Informat Sci & Technol, 96, JinZhai Rd, Hefei 230026, Anhui, Peoples R China.
C3 Zhejiang Sci-Tech University; Chinese Academy of Sciences; University of
   Science & Technology of China, CAS
RP Su, W (corresponding author), Zhejiang Sci Tech Univ, Sch Informat Sci & Engn, 2 St,Xiasha High Campus Dist, Hangzhou 310018, Zhejiang, Peoples R China.
EM 202220703027@mails.zstu.edu.cn; wensu@zstu.edu.cn;
   202230705132@mails.zstu.edu.cn; 202230705135@mails.zstu.edu.cn;
   harryjun@ustc.edu.cn
OI ge, meng jiao/0009-0003-2822-6378; Su, Wen/0000-0001-6787-4384; chen,
   jia wei/0009-0005-3160-8363
FU National Natural Science Foundation of China
FX No Statement Available
CR Al-Rfou R, 2019, AAAI CONF ARTIF INTE, P3159
   Chen GY, 2018, PROC CVPR IEEE, P9233, DOI 10.1109/CVPR.2018.00962
   Chen GH, 2019, J PHYS CONF SER, V1183, DOI 10.1088/1742-6596/1183/1/012011
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Dong XY, 2022, PROC CVPR IEEE, P12114, DOI 10.1109/CVPR52688.2022.01181
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Enze Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P696, DOI 10.1007/978-3-030-58601-0_41
   Everingham M., 2012, VOC VOC2012 WORKSH I
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Fu L, 2022, arXiv
   Ioffe Sergey, 2015, INT C MACHINE LEARNI, V37, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Kalra Agastya, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8599, DOI 10.1109/CVPR42600.2020.00863
   Lee S., 2022, SIGGRAPH ASIA 2022 P, P1
   Lee Y, 2022, PROC CVPR IEEE, P7277, DOI 10.1109/CVPR52688.2022.00714
   Li YH, 2022, PROC CVPR IEEE, P4794, DOI 10.1109/CVPR52688.2022.00476
   Lin GS, 2016, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2016.348
   Lin X., 2021, IEEE Trans. Multimed.
   Lin ZK, 2023, VISUAL COMPUT, V39, P597, DOI 10.1007/s00371-021-02360-7
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Maaz M., 2022, EUR C COMP VIS, P3, DOI DOI 10.1007/978-3-031-25082-8_1
   Okazawa A, 2019, IEEE INT C INT ROBOT, P4977, DOI [10.1109/IROS40897.2019.8968095, 10.1109/iros40897.2019.8968095]
   Peng HY, 2023, COMPUT GRAPH-UK, V115, P382, DOI 10.1016/j.cag.2023.07.028
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sacha M, 2023, IEEE WINT CONF APPL, P1481, DOI 10.1109/WACV56688.2023.00153
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang D, 2023, COMPUT GRAPH-UK, V116, P317, DOI 10.1016/j.cag.2023.09.001
   Wang H, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2090
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Xie EZ, 2021, ADV NEUR IN, V34
   Xie EZ, 2021, Arxiv, DOI arXiv:2101.08461
   Xu WJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9961, DOI 10.1109/ICCV48922.2021.00983
   Xu YC, 2015, IEEE I CONF COMP VIS, P3442, DOI 10.1109/ICCV.2015.393
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yuan YH, 2021, Arxiv, DOI arXiv:1809.00916
   Zhang JM, 2021, IEEE INT CONF COMP V, P1760, DOI 10.1109/ICCVW54120.2021.00202
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   Zhu LH, 2023, Arxiv, DOI arXiv:2304.01184
NR 50
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 31
PY 2024
DI 10.1007/s00371-024-03494-0
EA MAY 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR2C4
UT WOS:001236102600001
DA 2024-08-05
ER

PT J
AU Xue, LX
   Wang, WH
   Wang, RG
   Yang, J
AF Xue, Lixia
   Wang, Wenhao
   Wang, Ronggui
   Yang, Juan
TI Modular dual-stream visual fusion network for visual question answering
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Visual question answering; Dual-stream; Region features; Grid features;
   Visual fusion
AB Object detection networks' extracted region features have been pivotal in visual question answering (VQA) advancements. However, lacking global context, these features may yield inaccurate answers for questions demanding such information. Conversely, grid features provide detailed global context but falter on questions requiring high-level semantic insights due to their lack of semantic richness. Therefore, this paper proposes an improved attention-based dual-stream visual fusion network (MDVFN), which fuses region features with grid features to obtain global context information, while grid features supplement high-level semantic information. Specifically, we design a visual crossed attention (VCA) module in the attention network, which can interactively fuse two visual features to enhance their performance before guiding attention with the question features. It is worth noting that in order to reduce the semantic noise generated by the interaction of two image features in the visual cross attention (VCA) module, the targeted optimization is carried out. Before fusion, the visual position information is embedded, respectively, and the visual fusion graph is used to constrain the fusion process. Additionally, to combine text information, grid features, and region features, we propose a modality-mixing network. To validate our model, we conducted extensive experiments on the VQA-v2 benchmark dataset and the GQA dataset. These experiments demonstrate that MDVFN outperforms the most advanced methods. For instance, our proposed model achieved accuracies of 72.16% and 72.03% on the VQA-v2 and GQA datasets, respectively.
C1 [Xue, Lixia; Wang, Wenhao; Wang, Ronggui; Yang, Juan] Hefei Univ Technol, Sch Comp & Informat, Hefei 230601, Peoples R China.
C3 Hefei University of Technology
RP Yang, J (corresponding author), Hefei Univ Technol, Sch Comp & Informat, Hefei 230601, Peoples R China.
EM xlxzzm@163.com; wangwenhao135167@163.com; wangrgui@foxmail.com;
   yangjuanA1002@163.com
FU National Key R &D Program of China
FX No Statement Available
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Ben-Younes H, 2019, AAAI CONF ARTIF INTE, P8102
   Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285
   Chen K, 2016, Arxiv, DOI arXiv:1511.05960
   Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059
   Diao HW, 2021, AAAI CONF ARTIF INTE, V35, P1218
   Gao P, 2019, PROC CVPR IEEE, P6632, DOI 10.1109/CVPR.2019.00680
   Goyal Y, 2019, INT J COMPUT VISION, V127, P398, DOI 10.1007/s11263-018-1116-0
   Hu RH, 2019, IEEE I CONF COMP VIS, P10293, DOI 10.1109/ICCV.2019.01039
   Hudson C.D. Drew A., 2019, Neural Information Processing Systems
   Hudson DA, 2019, PROC CVPR IEEE, P6693, DOI 10.1109/CVPR.2019.00686
   Ji Z, 2020, IEEE ACCESS, V8, P38438, DOI 10.1109/ACCESS.2020.2975594
   Jiang Huaizu, 2020, P IEEE CVF C COMP VI, P10267, DOI DOI 10.1109/CVPR42600.2020.01028
   Kim J.-H., 2018, neural information processing systems
   Kim JH, 2016, ADV NEUR IN, V29
   Kingma D. P., 2014, arXiv
   Liang WX, 2020, Arxiv, DOI arXiv:2011.10731
   Lu JS, 2017, Arxiv, DOI [arXiv:1606.00061, 10.48550/arXiv.1606.00061]
   Luo YP, 2021, Arxiv, DOI arXiv:2101.06462
   Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232
   Nguyen BX, 2022, IEEE COMPUT SOC CONF, P4557, DOI 10.1109/CVPRW56347.2022.00502
   Rahman T, 2019, IEEE I CONF COMP VIS, P8907, DOI 10.1109/ICCV.2019.00900
   Ren MY, 2015, ADV NEUR IN, V28
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shen X, 2022, IEICE T INF SYST, VE105D, P785, DOI 10.1587/transinf.2021EDP7189
   Shih KJ, 2016, PROC CVPR IEEE, P4613, DOI 10.1109/CVPR.2016.499
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   X.L.C.H.X.Z.P.Z.L.W.L.H.H.D.L.W.F.C.Y.G.J. XiujunYin, 2020, EUR C COMP VIS
   XIONG P., 2022, arXiv
   Xu ZY, 2023, INFORM PROCESS MANAG, V60, DOI 10.1016/j.ipm.2022.103207
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yu Z, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3743, DOI 10.1145/3394171.3413977
   Yu Z, 2018, IEEE T NEUR NET LEAR, V29, P5947, DOI 10.1109/TNNLS.2018.2817340
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Zhang PC, 2021, PROC CVPR IEEE, P5575, DOI 10.1109/CVPR46437.2021.00553
   Zheng XT, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3079918
   Zhou BL, 2015, Arxiv, DOI arXiv:1512.02167
   Zhou YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2054, DOI 10.1109/ICCV48922.2021.00208
   Zhou YY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1245, DOI 10.1145/3394171.3413998
NR 41
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 28
PY 2024
DI 10.1007/s00371-024-03346-x
EA MAY 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SJ9P4
UT WOS:001234206700002
DA 2024-08-05
ER

PT J
AU Wang, XN
   Guan, ZY
   Qian, B
   Chen, TL
   Wu, Q
AF Wang, Xiang-ning
   Guan, Zhouyu
   Qian, Bo
   Chen, Tingli
   Wu, Qiang
TI A deep learning system for the detection of optic disc
   neovascularization in diabetic retinopathy using optical coherence
   tomography angiography images
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Diabetic retinopathy; Neovascularization of the optic disc; Deep
   learning; Optical coherence tomography angiography; Automated diagnosis
ID FOVEAL AVASCULAR ZONE
AB As one of the major complications of diabetic retinopathy (DR), neovascularization of the optic disc (NVD) is a leading cause of visual impairment and blindness. Early identification and timely treatment of NVD are essential to prevent these risks. In this paper, we develop a deep learning (DL) system to identify, quantify, and visualize NVD from optical coherence tomography angiography (OCTA) images. Two datasets of OCTA images were used in this study to develop and evaluate the DL system: (1) 24,576 OCTA images collected from 96 patients with NVD; (2) 15,360 OCTA images from 60 NVD patients with NVD. The task of the DL system involved the detection of the optic disc boundary, the identification of the NVD regions, and the construction and calculation of 3D images for these regions. The DL system achieved promising results in the detection of the optic disc boundary and the identification of NVD regions. The accuracy of the DL system was significantly better than other DL algorithms and comparable to the performance of retina specialists. Furthermore, the DL system could provide a more intuitive 3D image for visualizing the NVD and its blood flow information.
C1 [Wang, Xiang-ning; Wu, Qiang] Shanghai Jiao Tong Univ, Dept Ophthalmol, Sch Med, Shanghai Peoples Hosp 6, Shanghai, Peoples R China.
   [Guan, Zhouyu] Shanghai Jiao Tong Univ, Shanghai Key Lab Diabet Mellitus, Shanghai Clin Ctr Diabet,Shanghai Peoples Hosp 6, Dept Endocrinol & Metab,Shanghai Diabet Inst,Sch M, Shanghai, Peoples R China.
   [Qian, Bo] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Chen, Tingli] Huadong Sanatorium, Wuxi, Jiangsu, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai
   Jiao Tong University
RP Wu, Q (corresponding author), Shanghai Jiao Tong Univ, Dept Ophthalmol, Sch Med, Shanghai Peoples Hosp 6, Shanghai, Peoples R China.
EM qiang.wu@shsmu.edu.cn
RI Guan, Zhouyu/JFS-9545-2023
OI Guan, Zhouyu/0009-0008-5102-0067
FU College-level Project Fund of Shanghai Sixth People's Hospital
FX No Statement Available
CR Arya M, 2020, RETINA-J RET VIT DIS, V40, P1686, DOI 10.1097/IAE.0000000000002671
   Avery RL, 2006, OPHTHALMOLOGY, V113, P1695, DOI 10.1016/j.ophtha.2006.05.064
   Dodo Y, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-05663-9
   Falavarjani KG, 2017, INVEST OPHTH VIS SCI, V58, P30, DOI 10.1167/iovs.16-20579
   Fu J, 2021, IEEE T NEUR NET LEAR, V32, P2547, DOI 10.1109/TNNLS.2020.3006524
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hirano T, 2018, BRIT J OPHTHALMOL, V102, P1199, DOI 10.1136/bjophthalmol-2017-311358
   Hu ZZ, 2019, GRAEF ARCH CLIN EXP, V257, P1611, DOI 10.1007/s00417-019-04315-0
   Kaizu Y, 2018, GRAEF ARCH CLIN EXP, V256, P2275, DOI 10.1007/s00417-018-4122-6
   Kong Mingui, 2012, Korean J Ophthalmol, V26, P428, DOI 10.3341/kjo.2012.26.6.428
   Lee J, 2016, OPHTHALMOLOGY, V123, P2368, DOI 10.1016/j.ophtha.2016.07.010
   Li YH, 2022, BRIT J OPHTHALMOL, V106, P633, DOI 10.1136/bjophthalmol-2020-317825
   LITTLE HL, 1976, AM J OPHTHALMOL, V82, P675, DOI 10.1016/0002-9394(76)90001-5
   Lu ES, 2022, BRIT J OPHTHALMOL, V106, P534, DOI 10.1136/bjophthalmol-2020-317983
   LU HE, 1986, COMMUN ACM, V29, P239, DOI 10.1145/5666.5670
   Lu YS, 2018, INVEST OPHTH VIS SCI, V59, P2212, DOI 10.1167/iovs.17-23498
   Mansour SE, 2020, CLIN OPHTHALMOL, V14, P653, DOI 10.2147/OPTH.S236637
   Onishi AC, 2018, INVEST OPHTH VIS SCI, V59, P2167, DOI 10.1167/iovs.17-23304
   Parravano M, 2017, AM J OPHTHALMOL, V179, P90, DOI 10.1016/j.ajo.2017.04.021
   RAND LI, 1985, INVEST OPHTH VIS SCI, V26, P983
   Rosen RB, 2019, AM J OPHTHALMOL, V203, P103, DOI 10.1016/j.ajo.2019.01.012
   Spaide RF, 2015, RETINA-J RET VIT DIS, V35, P2163, DOI 10.1097/IAE.0000000000000765
   Ting DSW, 2019, PROG RETIN EYE RES, V72, DOI 10.1016/j.preteyeres.2019.04.003
   Wang H, 2019, BMC OPHTHALMOL, V19, DOI 10.1186/s12886-019-1189-8
   Yasukura S, 2018, INVEST OPHTH VIS SCI, V59, P5893, DOI 10.1167/iovs.18-25108
   You QS, 2020, RETINA-J RET VIT DIS, V40, P891, DOI 10.1097/IAE.0000000000002487
   Zhang X, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-22363-0
NR 27
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 10
PY 2024
DI 10.1007/s00371-024-03418-y
EA MAY 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QJ2H9
UT WOS:001220437700001
DA 2024-08-05
ER

PT J
AU Miao, X
   Li, SJ
   Li, Z
   Xu, WZ
   Yang, N
AF Miao, Xuan
   Li, Shijie
   Li, Zheng
   Xu, Wenzheng
   Yang, Ning
TI Multi-scale gated network for efficient image super-resolution
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Single-image super-resolution; Transformer; Multi-scale filtering;
   Lightweight network
AB Remarkable progress has been made in the field of single-image super-resolution (SISR), with convolutional neural network being widely adopted to achieve state-of-the-art performance. Recently, researchers have been increasingly interested in exploring the application of Transformer in SISR. However, the high computational cost of Transformer poses a challenge to its deployment on mobile devices. To address this issue, we propose a novel lightweight multi-scale gated network (MSGN) by exploring the variant of the Transformer which is built upon its general structure. MSGN utilizes efficient multi-scale gated block (EMGB) as the token mixer for the Transformer. Specifically, EMGB uses multi-scale filtering block and gating mechanism to extract and augment various features at multiple granularities. In addition, the simplified channel attention is used to extract channel global information. Furthermore, an enhanced multi-layer perceptron is employed instead of the MLP layer in Transformer to further improve the performance of the network. Our extensive experimental results demonstrate that MSGN achieves the best performance among the state-of-the-art efficient image SR models while utilizing the least number of parameters and FLOPs.
C1 [Miao, Xuan; Li, Shijie; Li, Zheng; Xu, Wenzheng; Yang, Ning] Sichuan Univ, Coll Comp Sci, 24 South Sect 1,Yihuan Rd, Chengdu 610065, Sichuan, Peoples R China.
C3 Sichuan University
RP Li, Z; Yang, N (corresponding author), Sichuan Univ, Coll Comp Sci, 24 South Sect 1,Yihuan Rd, Chengdu 610065, Sichuan, Peoples R China.
EM mouxuan@stu.scu.edu.cn; shijieli@stu.scu.edu.cn; lizheng@scu.edu.cn;
   wenzheng.xu@scu.edu.cn; yangning@scu.edu.cn
FU National Key Research and Development Program of China [2020YFA0714003];
   National Key Research and Development Program of China [2021YFQ0059];
   Science and Technology Planning Project of Sichuan Province [GJXM92579];
   National Major Project of China
FX This research was supported by National Key Research and Development
   Program of China (Grant No. 2020YFA0714003), Science and Technology
   Planning Project of Sichuan Province (Grant No. 2021YFQ0059), National
   Major Project of China (Grant No. GJXM92579).
CR Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Capel D, 2003, IEEE SIGNAL PROC MAG, V20, P75, DOI 10.1109/MSP.2003.1203211
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen LY, 2022, LECT NOTES COMPUT SC, V13667, P17, DOI 10.1007/978-3-031-20071-7_2
   Chen YT, 2021, MULTIMED TOOLS APPL, V80, P30839, DOI 10.1007/s11042-020-09969-1
   Cho SJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4621, DOI 10.1109/ICCV48922.2021.00460
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Du Z., 2022, P IEEE CVF C COMP VI, P853, DOI DOI 10.48550/ARXIV.2204.08397
   Elfwing S, 2018, NEURAL NETWORKS, V107, P3, DOI 10.1016/j.neunet.2017.12.012
   Farsiu S, 2004, IEEE T IMAGE PROCESS, V13, P1327, DOI 10.1109/TIP.2004.834669
   Hendrycks D, 2020, Arxiv, DOI arXiv:1606.08415
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082
   Ioffe Sergey, 2015, INT C MACHINE LEARNI, V37, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Jie Liu, 2020, Proceedings of the 16th European Conference on Computer Vision - ECCV 2020 Workshops. Lecture Notes in Computer Science (LNCS 12537), P41, DOI 10.1007/978-3-030-67070-2_2
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D. P., 2014, arXiv
   Lai QX, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107139
   Lai W.-S., 2017, P IEEE C COMPUTER VI, P624, DOI [DOI 10.1109/CVPR.2017.618, 10.1109/CVPR.2017.618, 10.1109/cvpr.2017.618]
   Li ZY, 2022, IEEE COMPUT SOC CONF, P832, DOI 10.1109/CVPRW56347.2022.00099
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lu ZS, 2022, IEEE COMPUT SOC CONF, P456, DOI 10.1109/CVPRW56347.2022.00061
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Nair V., 2010, ICML, P807
   Romano Y, 2017, IEEE T COMPUT IMAG, V3, P110, DOI 10.1109/TCI.2016.2629284
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Sun L., 2022, arXiv
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Tolstikhin I, 2021, ADV NEUR IN, V34
   Uiboupin T, 2016, 2016 24TH SIGNAL PROCESSING AND COMMUNICATION APPLICATION CONFERENCE (SIU), P437, DOI 10.1109/SIU.2016.7495771
   Wang GT, 2022, AAAI CONF ARTIF INTE, P2423
   Wang L, 2022, IEEE COMPUT SOC CONF, P816, DOI 10.1109/CVPRW56347.2022.00097
   Wang LG, 2021, PROC CVPR IEEE, P4915, DOI 10.1109/CVPR46437.2021.00488
   Wang PJ, 2022, EARTH-SCI REV, V232, DOI 10.1016/j.earscirev.2022.104110
   Wang Y, 2022, IEEE COMPUT SOC CONF, P776, DOI 10.1109/CVPRW56347.2022.00093
   Xiaotong Luo, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P272, DOI 10.1007/978-3-030-58542-6_17
   Yu WH, 2022, PROC CVPR IEEE, P10809, DOI 10.1109/CVPR52688.2022.01055
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zeyde R., 2012, INT C CURV SURF, P711
   Zhang SX, 2019, IEEE ACCESS, V7, P12319, DOI 10.1109/ACCESS.2018.2871626
   Zhang W, 2023, VISUAL COMPUT, V39, P3519, DOI 10.1007/s00371-023-03021-7
   Zhang XD, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4034, DOI 10.1145/3474085.3475291
   Zhang YH, 2014, IEEE J-STARS, V7, P1271, DOI 10.1109/JSTARS.2014.2305652
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
NR 54
TC 0
Z9 0
U1 11
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 3
PY 2024
DI 10.1007/s00371-024-03410-6
EA MAY 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PN0Z5
UT WOS:001214652000001
DA 2024-08-05
ER

PT J
AU Shen, ZY
   Chen, CY
   Zhang, RP
   Yu, HY
   Li, L
AF Shen, Zhongye
   Chen, Chunyi
   Zhang, Ripei
   Yu, Haiyang
   Li, Ling
TI Perception-JND-driven path tracing for reducing sample budget
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Perception-driven rendering; Just noticeable difference; Monte Carlo
   path tracing; Adaptive sampling
ID GLOBAL ILLUMINATION; IMAGE; NOISE; MODEL; PROFILE
AB Monte Carlo path tracing is a widely used method for generating realistic rendering results in multimedia applications, but often suffers from poor convergence and heavy sampling budget. Insufficient path samples may lead to noisy results. Some noises are hidden in textures, and the human visual system cannot detect them all. Just noticeable difference (JND) quantifies this limitation as a full-reference perception threshold. In rendering, the reference is unavailable and a surrogate is required. This paper proposed a perception-JND-driven path tracing method for reducing sampling budget. We tested and verified the surrogate JND thresholds derived from current rendering results. Then, we introduced difference pooling module and shading restart module to control perceptual convergence. Further, to improve accuracy, we developed the strategy for optimizing sampling steps. Experiments showed that the proposed method outperformed the state-of-the-art method at moderately low sampling levels, offering a lightweight and efficient solution to reducing sample budget while improving visual quality.
C1 [Shen, Zhongye; Chen, Chunyi; Zhang, Ripei; Yu, Haiyang; Li, Ling] Changchun Univ Sci & Technol, Changchun 130022, Jilin, Peoples R China.
C3 Changchun University of Science & Technology
RP Chen, CY (corresponding author), Changchun Univ Sci & Technol, Changchun 130022, Jilin, Peoples R China.
EM chenchunyi@hotmail.com
OI chen, chunyi/0009-0002-7055-0115
FU National Natural Science Foundation of China [U19A2063]; National
   Natural Science Foundation of China [20230201080GX]; Jilin Provincial
   Development Program of Science and Technology
FX This work was supported by the National Natural Science Foundation of
   China under Grant (No. U19A2063) and Jilin Provincial Development
   Program of Science and Technology (20230201080GX).
CR Ahmed AGM, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417881
   AHUMADA AJ, 1992, P SOC PHOTO-OPT INS, V1666, P365
   Andersson P, 2020, P ACM COMPUT GRAPH, V3, DOI 10.1145/3406183
   [Anonymous], 1987, P 14 ANN C COMP GRAP, DOI [10.1145/37401.37410, DOI 10.1145/37401.37410, DOI 10.1145/37402.37410]
   Bae SH, 2014, IEEE T IMAGE PROCESS, V23, P3227, DOI 10.1109/TIP.2014.2327808
   Bolin M. R., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P299, DOI 10.1145/280814.280924
   Buisine J, 2021, ENTROPY-SWITZ, V23, DOI 10.3390/e23010075
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Chou CH, 1995, IEEE T CIRC SYST VID, V5, P467, DOI 10.1109/76.475889
   Constantin J, 2020, NEURAL COMPUT APPL, V32, P427, DOI 10.1007/s00521-018-3941-z
   Constantin J, 2015, NEUROCOMPUTING, V164, P82, DOI 10.1016/j.neucom.2014.10.090
   DALY S, 1992, P SOC PHOTO-OPT INS, V1666, P2, DOI 10.1117/12.135952
   Farrugia JP, 2004, COMPUT GRAPH FORUM, V23, P605, DOI 10.1111/j.1467-8659.2004.00792.x
   Harvey C, 2017, COMPUT GRAPH FORUM, V36, P172, DOI 10.1111/cgf.12793
   He SF, 2020, IEEE T NEUR NET LEAR, V31, P2672, DOI 10.1109/TNNLS.2019.2933439
   Hou XD, 2007, PROC CVPR IEEE, P2280
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang G, 2021, COMPUT GRAPH-UK, V94, P22, DOI 10.1016/j.cag.2020.09.007
   Ki S, 2018, IEEE T IMAGE PROCESS, V27, P3178, DOI 10.1109/TIP.2018.2818439
   Koskela Matias K., 2018, Computational Visual Media, V4, P267, DOI 10.1007/s41095-018-0113-0
   Laparra V, 2017, J OPT SOC AM A, V34, P1511, DOI 10.1364/JOSAA.34.001511
   Li B, 1998, P SOC PHOTO-OPT INS, V3299, P98, DOI 10.1117/12.320101
   Lin WS, 2022, IEEE T MULTIMEDIA, V24, P3706, DOI 10.1109/TMM.2021.3106503
   Longhurst P., 2006, P 4 INT C COMPUTER G, P21, DOI DOI 10.1145/1108590.1108595
   Lu Dong, 2011, 2011 IEEE 10th IVMSP Workshop: Perception and Visual Signal Analysis, P159, DOI 10.1109/IVMSPW.2011.5970372
   Lubin J., 1995, Vision Models for Target Detection and Recognition, P245
   Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935
   Mueller JH, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3446790
   Myszkowski K., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P223
   Parke F. I., 1991, Journal of Visualization and Computer Animation, V2, P44, DOI 10.1002/vis.4340020204
   Pattanaik S. N., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P287, DOI 10.1145/280814.280922
   Pharr M., 2016, Physically based rendering: From theory to implementation
   Prashnani E., 2021, arXiv
   Qu LJ, 2008, IEEE T VIS COMPUT GR, V14, P1015, DOI 10.1109/TVCG.2008.51
   Ren ZX, 2014, IEEE T CIRC SYST VID, V24, P769, DOI 10.1109/TCSVT.2013.2280096
   Renaud C., 2012, INTELLIGENT COMPUTER, P1
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Stern M.K., 2010, The Corsini Encyclopedia of Psychology
   Takouachet N., 2007, P 23 SPRING C COMP G, P55
   Takouachet N, 2017, COMPUT GRAPH-UK, V69, P49, DOI 10.1016/j.cag.2017.09.008
   Tolhurst D.J., 2005, P 2 S APPL PERC GRAP, P135
   Ullah I, 2020, MULTIMED TOOLS APPL, V79, P34605, DOI 10.1007/s11042-020-08849-y
   Wang HK, 2021, IEEE T IMAGE PROCESS, V30, P487, DOI 10.1109/TIP.2020.3037525
   Wang LL, 2022, Arxiv, DOI arXiv:2211.07969
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Wei ZY, 2009, IEEE T CIRC SYST VID, V19, P337, DOI 10.1109/TCSVT.2009.2013518
   Weier M, 2017, COMPUT GRAPH FORUM, V36, P611, DOI 10.1111/cgf.13150
   Weier M, 2016, COMPUT GRAPH FORUM, V35, P289, DOI 10.1111/cgf.13026
   Wu JJ, 2017, IEEE T IMAGE PROCESS, V26, P2682, DOI 10.1109/TIP.2017.2685682
   Wu JJ, 2016, INT CONF ACOUST SPEE, P1581, DOI 10.1109/ICASSP.2016.7471943
   Wu JJ, 2013, IEEE T MULTIMEDIA, V15, P1705, DOI 10.1109/TMM.2013.2268053
   Yang XK, 2005, SIGNAL PROCESS-IMAGE, V20, P662, DOI 10.1016/j.image.2005.04.001
   Zhang AJ, 2019, MULTIMED TOOLS APPL, V78, P33487, DOI 10.1007/s11042-019-08155-2
   Zhang XH, 2005, SIGNAL PROCESS, V85, P795, DOI 10.1016/j.sigpro.2004.12.002
NR 54
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 25
PY 2024
DI 10.1007/s00371-023-03199-w
EA JAN 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GC2K0
UT WOS:001150397600001
DA 2024-08-05
ER

PT J
AU Song, XY
   Han, DZ
   Chen, CQ
   Shen, X
   Wu, HF
AF Song, Xiaoyu
   Han, Dezhi
   Chen, Chongqing
   Shen, Xiang
   Wu, Huafeng
TI Vman: visual-modified attention network for multimodal paradigms
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Transformer; Visual question answering (VQA); Visual grounding (VG);
   Vision-language task (VLT); Dependency modeling
ID LANGUAGE
AB Due to excellent dependency modeling and powerful parallel computing capabilities, Transformer has become the primary research method in vision-language tasks (VLT). However, for multimodal VLT like VQA and VG, which demand high-dependency modeling and heterogeneous modality comprehension, solving the issues of introducing noise, insufficient information interaction, and obtaining more refined visual features during the image self-interaction of conventional Transformers is challenging. Therefore, this paper proposes a universal visual-modified attention network (VMAN) to address these problems. Specifically, VMAN optimizes the attention mechanism in Transformer, introducing a visual-modified attention unit that establishes text-visual correspondence before the self-interaction of image information. Modulating image features with modified units to obtain more refined query features for subsequent interaction, filtering out noise information while enhancing dependency modeling and reasoning capabilities. Furthermore, two modified approaches have been designed: the weighted sum-based approach and the cross-attention-based approach. Finally, we conduct extensive experiments on VMAN across five benchmark datasets for two tasks (VQA, VG). The results indicate that VMAN achieves an accuracy of 70.99%\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\%$$\end{document} on the VQA-v2 and makes a breakthrough of 74.41%\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\%$$\end{document} on the RefCOCOg which involves more complex expressions. The results fully prove the rationality and effectiveness of VMAN. The code is available at https://github.com/79song/VMAN.
C1 [Song, Xiaoyu; Han, Dezhi; Chen, Chongqing; Shen, Xiang] Shanghai Maritime Univ, Coll Informat Engn, 1550 Haigang Ave, Shanghai 201306, Peoples R China.
   [Wu, Huafeng] Shanghai Maritime Univ, Merchant Marine Coll, 1550 Haigang Ave, Shanghai 201306, Peoples R China.
C3 Shanghai Maritime University; Shanghai Maritime University
RP Han, DZ (corresponding author), Shanghai Maritime Univ, Coll Informat Engn, 1550 Haigang Ave, Shanghai 201306, Peoples R China.
EM xysong7990@163.com; dzhan@shmtu.edu.cn; chenchongqing0305@163.com;
   shenxiang1107@163.com; hfwu@shmtu.edu.cn
FU National Natural Science Foundation of China [52331012]; Natural Science
   Foundation of Shanghai [21ZR1426500]; The 2022 Graduate Top Innovative
   Talents Training Program at Shanghai Maritime University [2022YBR005,
   2022YBR014]
FX This work was supported in part by the National Natural Science
   Foundation of China (Grant No. 52331012), the Natural Science Foundation
   of Shanghai under Grant 21ZR1426500, and the 2022 Graduate Top
   Innovative Talents Training Program at Shanghai Maritime University
   under Grant 2022YBR005, 2022YBR014.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Cao J., 2022, IEEE Trans. Neural Netw. Learn. Syst.
   Cao JJ, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2021.3135655
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen CQ, 2024, PATTERN RECOGN, V147, DOI 10.1016/j.patcog.2023.110084
   Chen CQ, 2023, KNOWL-BASED SYST, V275, DOI 10.1016/j.knosys.2023.110706
   Chen CQ, 2022, PATTERN RECOGN, V132, DOI 10.1016/j.patcog.2022.108980
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen L, 2021, AAAI CONF ARTIF INTE, V35, P1036
   Chen X, 2023, PROC CVPR IEEE, P5896, DOI 10.1109/CVPR52729.2023.00571
   Chen ZH, 2023, IEEE T PATTERN ANAL, V45, P13489, DOI 10.1109/TPAMI.2023.3293885
   Deng JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1749, DOI 10.1109/ICCV48922.2021.00179
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Ding HH, 2023, IEEE T PATTERN ANAL, V45, P7900, DOI 10.1109/TPAMI.2022.3217852
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Du Yunhao., 2022, IEEE INT C MULT EXP, P1, DOI [DOI 10.1109/ICME52920.2022.9859880, 10.1109/ICME52920.2022.9859880]
   Gen Luo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10031, DOI 10.1109/CVPR42600.2020.01005
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Guo DL, 2023, IEEE T NEUR NET LEAR, V34, P1023, DOI 10.1109/TNNLS.2021.3104937
   Guo WY, 2021, IEEE T IMAGE PROCESS, V30, P6730, DOI 10.1109/TIP.2021.3097180
   Guo ZH, 2023, APPL INTELL, V53, P586, DOI 10.1007/s10489-022-03559-4
   Hong RC, 2022, IEEE T PATTERN ANAL, V44, P684, DOI 10.1109/TPAMI.2019.2911066
   Huang BB, 2021, PROC CVPR IEEE, P16883, DOI 10.1109/CVPR46437.2021.01661
   Hudson DA, 2019, PROC CVPR IEEE, P6693, DOI 10.1109/CVPR.2019.00686
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Kim JH, 2018, ADV NEUR IN, V31
   Kocon J, 2023, INFORM FUSION, V99, DOI 10.1016/j.inffus.2023.101861
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li G, 2020, AAAI CONF ARTIF INTE, V34, P11336
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Li M., 2021, NEURIPS, P19652
   Liao Yiyi, 2020, CVPR
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu XH, 2019, PROC CVPR IEEE, P1950, DOI 10.1109/CVPR.2019.00205
   Liu YH, 2023, IEEE T MULTIMEDIA, V25, P5344, DOI 10.1109/TMM.2022.3190686
   Mao AH, 2023, IEEE T MULTIMEDIA, V25, P6997, DOI 10.1109/TMM.2022.3216770
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Qi D., 2020, ARXIV200107966
   Radford A, 2021, PR MACH LEARN RES, V139
   Rahman T, 2021, IEEE COMPUT SOC CONF, P1653, DOI 10.1109/CVPRW53098.2021.00181
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shen X, 2024, APPL INTELL, V54, P5062, DOI 10.1007/s10489-024-05437-7
   Shen X, 2023, APPL INTELL, V53, P16706, DOI 10.1007/s10489-022-04355-w
   Shi FY, 2024, IEEE T PATTERN ANAL, V46, P1181, DOI 10.1109/TPAMI.2023.3328185
   Song SS, 2023, I S BIOMED IMAGING, DOI 10.1109/ISBI53787.2023.10230530
   Su Weijie, 2020, 8 INT C LEARN REPR I
   Sun C, 2019, IEEE I CONF COMP VIS, P7463, DOI 10.1109/ICCV.2019.00756
   Vaswani A, 2017, ADV NEUR IN, V30
   Xiaofeng Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P414, DOI 10.1007/978-3-030-58589-1_25
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Yan F, 2022, VISUAL COMPUT, V38, P3097, DOI 10.1007/s00371-022-02524-z
   Yang L, 2022, PROC CVPR IEEE, P9489, DOI 10.1109/CVPR52688.2022.00928
   Yang SB, 2021, IEEE T PATTERN ANAL, V43, P2765, DOI 10.1109/TPAMI.2020.2973983
   Yang ZY, 2019, IEEE I CONF COMP VIS, P4682, DOI 10.1109/ICCV.2019.00478
   Yao HB, 2023, IMAGE VISION COMPUT, V140, DOI 10.1016/j.imavis.2023.104840
   Ye JB, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1702, DOI 10.1145/3474085.3475313
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
   Yu Z., 2019, CoRR abs/1908.04107
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zeng Y, 2022, INT C MACH LEARN ICM, V162, P25994
   Zhengyuan Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P387, DOI 10.1007/978-3-030-58568-6_23
   Zhou LW, 2020, AAAI CONF ARTIF INTE, V34, P13041
   Zhou YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2054, DOI 10.1109/ICCV48922.2021.00208
   Zhou YY, 2023, IEEE T NEUR NET LEAR, V34, P134, DOI 10.1109/TNNLS.2021.3090426
   Zhu CY, 2022, LECT NOTES COMPUT SC, V13695, P598, DOI 10.1007/978-3-031-19833-5_35
   Zhu Jinhua, 2020, 8 INT C LEARN REPR I
NR 70
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 18
PY 2024
DI 10.1007/s00371-024-03563-4
EA JUL 2024
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YW3V4
UT WOS:001271492100001
DA 2024-08-05
ER

PT J
AU Chatterjee, J
   Vega, MT
AF Chatterjee, Jit
   Vega, Maria Torres
TI 3D-Scene-Former: 3D scene generation from a single RGB image using
   Transformers
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE 3D scene understanding; Mesh generation; 3D object detection; 3D pose
   estimation; Transformers
ID RECOGNITION
AB 3D scene generation requires complex hardware setups, such as multiple cameras and depth sensors. To address this challenge, there is a need for generating 3D scenes from a single RGB image by understanding the spatio-contextual information inside a scene. However, generating 3D scenes from a single RGB image represents a formidable undertaking as the depth information is missing. Moreover, we need to generate the scene from various angles and positions, which necessitates extrapolations from the limited information in a single image. Current state-of-the-art techniques hinge on extracting global and local features from the 2D scene and employ a combined estimation strategy to tackle this challenge. However, existing approaches still grapple with accurately estimating 3D parameters, especially due to the strong occlusions in cluttered environments. In this paper, we propose 3D-Scene-Former, a novel solution to generate 3D indoor scenes from a single RGB image and refine the initial estimations using a Transformer network. We evaluated our approach on two well-known datasets benchmarking it against state-of-the-art solutions. Our method outperforms the state-of-the-art in terms of 3D object detection and 3D pose estimation by a margin of 11.37%. 3D-Scene-Former opens new venues for 3D content creation, transforming a single RGB image into realistic 3D scenes through the use of interconnected mesh structures.
C1 [Chatterjee, Jit; Vega, Maria Torres] Katholieke Univ Leuven, Dept Elect Engn ESAT, eMedia Res Lab, B-3000 Leuven, Belgium.
C3 KU Leuven
RP Chatterjee, J (corresponding author), Katholieke Univ Leuven, Dept Elect Engn ESAT, eMedia Res Lab, B-3000 Leuven, Belgium.
EM jit.chatterjee@kuleuven.be; maria.torresvega@kuleuven.be
RI Chatterjee, Jit/JQV-5471-2023
OI Chatterjee, Jit/0000-0002-1760-6573
FX DAS:No datasets were generated or analysed during the current study.
CR Aldoma A, 2012, IEEE ROBOT AUTOM MAG, V19, P80, DOI 10.1109/MRA.2012.2206675
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chatterjee J, 2023, PROCEEDINGS OF THE 2023 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE MEDIA EXPERIENCES, IMX 2023, P398, DOI 10.1145/3573381.3597232
   Dahnert M., 2021, Advances in Neural Information Processing Systems
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Genova K, 2020, PROC CVPR IEEE, P4856, DOI 10.1109/CVPR42600.2020.00491
   Gkioxari G, 2019, IEEE I CONF COMP VIS, P9784, DOI 10.1109/ICCV.2019.00988
   Gümeli C, 2022, PROC CVPR IEEE, P4012, DOI 10.1109/CVPR52688.2022.00399
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu JJ, 2019, IEEE WINT CONF APPL, P1043, DOI 10.1109/WACV.2019.00116
   Huang S., 2019, P 33 INT C NEUR INF, DOI [10.5555/3454287.3455086, DOI 10.5555/3454287.3455086]
   Huang S., 2018, Advances in Neural Information Processing Systems, P206, DOI [10.5555/3326943.3326963, DOI 10.5555/3326943.3326963]
   Huang SY, 2018, LECT NOTES COMPUT SC, V11211, P194, DOI 10.1007/978-3-030-01234-2_12
   Karpathy A, 2013, IEEE INT CONF ROBOT, P2088, DOI 10.1109/ICRA.2013.6630857
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2929, DOI 10.1109/ICCV48922.2021.00294
   Mousavian A, 2017, PROC CVPR IEEE, P5632, DOI 10.1109/CVPR.2017.597
   Nie YY, 2020, PROC CVPR IEEE, P52, DOI 10.1109/CVPR42600.2020.00013
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Rampasek Ladislav, 2022, ADV NEURAL INFORM PR
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rothganger F, 2006, INT J COMPUT VISION, V66, P231, DOI 10.1007/s11263-005-3674-1
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Song SR, 2016, PROC CVPR IEEE, P808, DOI 10.1109/CVPR.2016.94
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Song SR, 2014, LECT NOTES COMPUT SC, V8694, P634, DOI 10.1007/978-3-319-10599-4_41
   Sun XY, 2018, PROC CVPR IEEE, P2974, DOI 10.1109/CVPR.2018.00314
   Tombari Federico, 2010, 2010 Fourth Pacific-Rim Symposium on Image and Video Technology (PSIVT), P349, DOI 10.1109/PSIVT.2010.65
   Vaswani A, 2017, ADV NEUR IN, V30
   Wu Y., 2019, Detectron2
   Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41
   Zhang C, 2021, PROC CVPR IEEE, P8829, DOI 10.1109/CVPR46437.2021.00872
NR 36
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 15
PY 2024
DI 10.1007/s00371-024-03573-2
EA JUL 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YV1H4
UT WOS:001271163900001
DA 2024-08-05
ER

PT J
AU Wang, YL
   Lang, YZ
   Qian, YS
AF Wang, Yi-lun
   Lang, Yi-zheng
   Qian, Yun-sheng
TI Effective multi-scale enhancement fusion method for low-light images
   based on interest-area perception OCTM and "pixel healthiness"
   evaluation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Low-light images; Image enhancement; Multi-scale fusion; Contrast-tone
   mapping
ID CONTRAST ENHANCEMENT; QUALITY ASSESSMENT; JND
AB Low-light images suffer from low contrast and low dynamic range. However, most existing single-frame low-light image enhancement algorithms are not good enough in terms of detail preservation and color expression and often have high algorithmic complexity. In this paper, we propose a single-frame low-light image fusion enhancement algorithm based on multi-scale contrast-tone mapping and "pixel healthiness" evaluation. It can adaptively adjust the exposure level of each region according to the principal component in the image and enhance contrast while preserving color and detail expression with low computational complexity. In particular, to find the most appropriate size of the artificial image sequence and the target enhancement range for each image, we propose a multi-scale parameter determination method based on the principal component analysis of the V-channel histogram to obtain the best enhancement while reducing unnecessary computations. In addition, a new "pixel healthiness" evaluation method based on global illuminance and local contrast is proposed for fast and efficient computation of weights for image fusion. Subjective evaluation and objective metrics show that our algorithm performs better than existing single-frame image algorithms and other fusion-based algorithms in enhancement, contrast, color expression, and detail preservation.
C1 [Wang, Yi-lun; Lang, Yi-zheng; Qian, Yun-sheng] Nanjing Univ Sci & Technol, Sch Elect & Opt Engn, Nanjing 210094, Jiangsu, Peoples R China.
C3 Nanjing University of Science & Technology
RP Qian, YS (corresponding author), Nanjing Univ Sci & Technol, Sch Elect & Opt Engn, Nanjing 210094, Jiangsu, Peoples R China.
EM yshqian@mail.njust.edu.cn
FU National Natural Science Foundation of China [U2141239]
FX National Funding for this study was obtained from the National Natural
   Science Foundation of China (No. U2141239).
CR Agaian SS, 2001, IEEE T IMAGE PROCESS, V10, P367, DOI 10.1109/83.908502
   Ancuti CO, 2018, IEEE T IMAGE PROCESS, V27, P379, DOI 10.1109/TIP.2017.2759252
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Chen SD, 2003, IEEE T CONSUM ELECTR, V49, P1310, DOI 10.1109/TCE.2003.1261234
   Chou CH, 1995, IEEE T CIRC SYST VID, V5, P467, DOI 10.1109/76.475889
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Ghosh S, 2019, IEEE IMAGE PROC, P205, DOI [10.1109/icip.2019.8802986, 10.1109/ICIP.2019.8802986]
   Gu K, 2015, IEEE T CIRC SYST VID, V25, P1480, DOI 10.1109/TCSVT.2014.2372392
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hao SJ, 2020, IEEE T MULTIMEDIA, V22, P3025, DOI 10.1109/TMM.2020.2969790
   Hessel C, 2020, IEEE WINT CONF APPL, P137, DOI 10.1109/WACV45572.2020.9093643
   Huang SC, 2013, IEEE T IMAGE PROCESS, V22, P1032, DOI 10.1109/TIP.2012.2226047
   Kong XY, 2021, IEEE SIGNAL PROC LET, V28, P1540, DOI 10.1109/LSP.2021.3096160
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Lang YZ, 2023, OPT EXPRESS, V31, P14008, DOI 10.1364/OE.485672
   Lang YZ, 2023, J OPT SOC AM A, V40, P1, DOI 10.1364/JOSAA.468876
   Li CY, 2018, PATTERN RECOGN LETT, V104, P15, DOI 10.1016/j.patrec.2018.01.010
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Ma L, 2022, IEEE T NEUR NET LEAR, V33, P5666, DOI 10.1109/TNNLS.2021.3071245
   McCann JJ, 2017, J ELECTRON IMAGING, V26, DOI 10.1117/1.JEI.26.3.031204
   McCann JJ, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.00005
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Paul A, 2023, VISUAL COMPUT, V39, P297, DOI 10.1007/s00371-021-02330-z
   Peng YT, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22010024
   Ren XT, 2020, IEEE T IMAGE PROCESS, V29, P5862, DOI 10.1109/TIP.2020.2984098
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x
   Srinivas K, 2020, IEEE T CIRC SYST VID, V30, P4663, DOI 10.1109/TCSVT.2019.2960861
   Su HN, 2022, IEEE T MULTIMEDIA, V24, P17, DOI 10.1109/TMM.2020.3043106
   Vonikakis V, 2008, IET IMAGE PROCESS, V2, P19, DOI 10.1049/iet-ipr:20070012
   Wang QT, 2020, IEEE T CIRC SYST VID, V30, P2418, DOI 10.1109/TCSVT.2019.2919310
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wei C, 2018, Arxiv, DOI arXiv:1808.04560
   Wu XL, 2011, IEEE T IMAGE PROCESS, V20, P1262, DOI 10.1109/TIP.2010.2092438
   Xu J, 2020, IEEE T IMAGE PROCESS, V29, P5022, DOI 10.1109/TIP.2020.2974060
   Xu YD, 2021, INFORM SCIENCES, V548, P378, DOI 10.1016/j.ins.2020.09.066
   Yan J., 2019, No-reference quality assessment of contrast-distorted images using contrast enhancement, V15, DOI [10.48550/arXiv.1904.08879, DOI 10.48550/ARXIV.1904.08879]
   Ying ZQ, 2017, Arxiv, DOI arXiv:1711.00591
   Zhang H., 2011, 2011 4 INT C IM SIGN, P704
   Zheng CB, 2021, IEEE T CIRC SYST VID, V31, P1425, DOI 10.1109/TCSVT.2020.3009235
   Zhou RF, 2024, SIGNAL IMAGE VIDEO P, V18, P803, DOI 10.1007/s11760-023-02801-x
   Zuiderveld K., 1994, Graphics gems, DOI 10.1016/b978-0-12-336156-1.50061-6
NR 43
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 2
PY 2024
DI 10.1007/s00371-024-03554-5
EA JUL 2024
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XG0S4
UT WOS:001260419000003
DA 2024-08-05
ER

PT J
AU Jiang, JJ
   Chen, ZW
   Lei, FY
   Xu, L
   Huang, JH
   Yuan, XC
AF Jiang, Jianjian
   Chen, Ziwei
   Lei, Fangyuan
   Xu, Long
   Huang, Jiahao
   Yuan, Xiaochen
TI Multi-granularity hypergraph-guided transformer learning framework for
   visual classification
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Hierarchical multi-granularity; Visual classification; Hypergraph neural
   network; Swin transformer
ID NETWORK
AB Fine-grained single-label classification tasks aim to distinguish highly similar categories but often overlook inter-category relationships. Hierarchical multi-granularity visual classification strives to categorize image labels at various hierarchy levels, offering optimize label selection for people. This paper addresses the hierarchical multi-granularity classification problem from two perspectives: (1) effective utilization of labels at different levels and (2) efficient learning to distinguish multi-granularity visual features. To tackle these issues, we propose a novel multi-granularity hypergraph-guided transformer learning framework (MHTL), seamlessly integrating swin transformers and hypergraph neural networks for handling visual classification tasks. Firstly, we employ swin transformer as an image hierarchical feature learning (IHFL) module to capture hierarchical features. Secondly, a feature reassemble (FR) module is applied to rearrange features at different hierarchy levels, creating a spectrum of features from coarse to fine-grained. Thirdly, we propose a feature relationship mining (FRM) module, to unveil the correlation between features at different granularity. Within this module, we introduce a learnable hypergraph modeling method to construct coarse to fine-grained hypergraph structures. Simultaneously, multi-granularity hypergraph neural networks are employed to explore grouping relationships across different granularities, thereby enhancing the learning of semantic feature representations. Finally, we adopt a multi-granularity classifier (MC) to predict hierarchical label probabilities. Experimental results demonstrate that MHTL outperforms other state-of-the-art classification methods across three multi-granularity datasets. The source code and models are released at https://github.com/JJJTF/MHTL.
C1 [Jiang, Jianjian; Chen, Ziwei; Lei, Fangyuan; Xu, Long] Guangdong Polytech Normal Univ, Guangdong Prov Key Lab Intellectual Property & Big, Guangzhou 510665, Peoples R China.
   [Huang, Jiahao; Yuan, Xiaochen] Macao Polytech Univ, Fac Appl Sci, Macau 999078, Peoples R China.
C3 Guangdong Polytechnic Normal University; Macao Polytechnic University
RP Lei, FY (corresponding author), Guangdong Polytech Normal Univ, Guangdong Prov Key Lab Intellectual Property & Big, Guangzhou 510665, Peoples R China.
EM leify@gpnu.edu.cn
FU National Natural Science Foundation of China [U1701266]; Guangdong
   Provincial Key Laboratory Project of Intellectual Property and Big Data
   [2018B030322016]; Special Projects for Key Fields in Higher Education of
   Guangdong [2020ZDZX3077]; Key Discipline Research Capacity Improvement
   Project of Guangdong Province [2022ZDJS013]
FX This work was partly supported by the National Natural Science
   Foundation of China (U1701266), the Guangdong Provincial Key Laboratory
   Project of Intellectual Property and Big Data (2018B030322016), Special
   Projects for Key Fields in Higher Education of Guangdong (2020ZDZX3077),
   Key Discipline Research Capacity Improvement Project of Guangdong
   Province (2022ZDJS013).
CR Al-Jebrni AH, 2023, VISUAL COMPUT, V39, P3675, DOI 10.1007/s00371-023-02984-x
   Bera A, 2022, IEEE T IMAGE PROCESS, V31, P6017, DOI 10.1109/TIP.2022.3205215
   Chang DL, 2021, PROC CVPR IEEE, P11471, DOI 10.1109/CVPR46437.2021.01131
   Cheema MN, 2021, IEEE T IND INFORM, V17, P7991, DOI 10.1109/TII.2021.3064369
   Chen CF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P347, DOI 10.1109/ICCV48922.2021.00041
   Chen HZ, 2024, PATTERN RECOGN, V149, DOI 10.1016/j.patcog.2024.110265
   Chen JZ, 2022, PROC CVPR IEEE, P4848, DOI 10.1109/CVPR52688.2022.00481
   Chen Q, 2021, NEUROCOMPUTING, V459, P408, DOI 10.1016/j.neucom.2021.07.008
   Chen TS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P2023, DOI 10.1145/3240508.3240523
   Dosovitskiy A., 2021, ICLR
   Fan HQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6804, DOI 10.1109/ICCV48922.2021.00675
   Feng YF, 2019, AAAI CONF ARTIF INTE, P3558
   Gao Y, 2023, IEEE T PATTERN ANAL, V45, P3181, DOI 10.1109/TPAMI.2022.3182052
   Geng J., 2022, IEEE Trans. Geosci. Remote Sens, V60, P1
   Han K, 2021, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.2103.00112
   He J, 2022, AAAI CONF ARTIF INTE, P852
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J., 2018, SQUEEZE AND EXCITATI, P7132
   Jiang X., 2024, AAAI, P2570
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Kuang ZZ, 2021, NEUROCOMPUTING, V425, P191, DOI 10.1016/j.neucom.2020.04.085
   Li B., 2023, ACM Trans. Multimedia Comput. Commun. Appl., V19
   Li JJ, 2024, IEEE T MED IMAGING, V43, P64, DOI 10.1109/TMI.2023.3289859
   Li ZW, 2022, IEEE T NEUR NET LEAR, V33, P6999, DOI 10.1109/TNNLS.2021.3084827
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu RH, 2022, PATTERNS, V3, DOI 10.1016/j.patter.2022.100512
   Liu Y, 2022, LECT NOTES COMPUT SC, V13684, P57, DOI 10.1007/978-3-031-20053-3_4
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Maji S, 2013, Arxiv, DOI arXiv:1306.5151
   Miao Z, 2021, IEEE SIGNAL PROC LET, V28, P1983, DOI 10.1109/LSP.2021.3114622
   Nazir A, 2020, IEEE T IMAGE PROCESS, V29, P7192, DOI 10.1109/TIP.2020.2999854
   Shi HY, 2019, IEEE T NEUR NET LEAR, V30, P2963, DOI 10.1109/TNNLS.2018.2869747
   Shu X, 2023, KNOWL-BASED SYST, V273, DOI 10.1016/j.knosys.2023.110599
   Sun GL, 2020, AAAI CONF ARTIF INTE, V34, P12047
   Sun H, 2023, SIGNAL PROCESS-IMAGE, V110, DOI 10.1016/j.image.2022.116885
   Tan MX, 2019, PR MACH LEARN RES, V97
   Vaswani A., 2017, Advances in neural information processing systems, P5998
   Wadhwa G, 2021, IEEE WINT CONF APPL, P3911, DOI 10.1109/WACV48630.2021.00396
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang RZ, 2023, Arxiv, DOI [arXiv:2112.02353, 10.48550/arXiv.2112.02353]
   Wang YX, 2018, IEEE T IMAGE PROCESS, V27, P4437, DOI 10.1109/TIP.2018.2837219
   Wu HY, 2022, LECT NOTES COMPUT SC, V13142, P230, DOI 10.1007/978-3-030-98355-0_20
   Wu XP, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P284, DOI 10.1145/3394171.3414046
   Xiao L., 2020, Medical Imaging 2020: Biomedical Applications in Molecular, Structural, and Functional Imaging, V11317
   Yan ZC, 2015, IEEE I CONF COMP VIS, P2740, DOI 10.1109/ICCV.2015.314
   Yang SY, 2024, VISUAL COMPUT, DOI 10.1007/s00371-023-03226-w
   Zhang S., 2024, ICASSP 2024, P7370
   Zhao YF, 2021, PROC CVPR IEEE, P15074, DOI 10.1109/CVPR46437.2021.01483
   Zhu XQ, 2017, Arxiv, DOI arXiv:1709.09890
   Zhuang PQ, 2020, AAAI CONF ARTIF INTE, V34, P13130
NR 50
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 28
PY 2024
DI 10.1007/s00371-024-03541-w
EA JUN 2024
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WU7X3
UT WOS:001257463400002
DA 2024-08-05
ER

PT J
AU Bhardwaj, M
   Khan, NU
   Baghel, V
AF Bhardwaj, Munish
   Khan, Nafis Uddin
   Baghel, Vikas
TI Road crack detection using pixel classification and intensity-based
   distinctive fuzzy C-means clustering
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE K-Means clustering; Fuzzy C-means clustering; Road crack detection
ID K-MEANS; RECONSTRUCTION; INFORMATION; ALGORITHM; IMAGES
AB Road cracks are quickly becoming one of the world's most serious concerns. It may have an impact on traffic safety and increase the likelihood of road accidents. A significant amount of money is spent each year for road repair and upkeep. This cost can be lowered if the cracks are discovered in good time. However, detection takes longer and is less precise when done manually. Because of ambient noise, intensity in-homogeneity, and low contrast, crack identification is a complex technique for automatic processes. As a result, several techniques have been developed in the past to pinpoint the specific site of the crack. In this research, a novel fuzzy C-means clustering algorithm is proposed that will detect fractures automatically by adding optimal edge pixels utilizing a second-order difference and intensity-based edge and non-edge fuzzy factors. This technique provides information of the intensity of edge and non-edge pixels, allowing it to recognize edges even when the image has little contrast. This method does not necessitate the use of any data set to train the model and no any critical parameter optimization is required. As a result, it can recognize edges or fissures even in novel or previously unknown input pictures of different environments. The experimental results reveal that the unique fuzzy C-means clustering-based segmentation method beats many of the existing methods used for detecting alligator, transverse, and longitudinal fractures from road photos in terms of precession, recall, and F1 score, PSNR, and execution time.
C1 [Bhardwaj, Munish; Baghel, Vikas] Jaypee Univ Informat Technol, Solan, India.
   [Khan, Nafis Uddin] SR Univ, Warangal, India.
C3 Jaypee University of Information Technology
RP Khan, NU (corresponding author), SR Univ, Warangal, India.
EM munish368@gmail.com; nafis.khan@sru.edu.in; vikas.baghel@juit.ac.in
CR Ahmad Abdul Rahim, 2020, 2020 10th IEEE International Conference on Control System, Computing and Engineering (ICCSCE), P153, DOI 10.1109/ICCSCE50387.2020.9204935
   Ahmed NB, 2017, 2017 INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND DIAGNOSIS (ICCAD), P528, DOI 10.1109/CADIAG.2017.8075714
   Ai DH, 2018, IEEE ACCESS, V6, P24452, DOI 10.1109/ACCESS.2018.2829347
   BEZDEK JC, 1984, COMPUT GEOSCI, V10, P191, DOI 10.1016/0098-3004(84)90020-7
   Bhardwaj M., 2022, IEEE INT C PAR DISTR, P547, DOI [10.1109/PDGC56933.2022.10053319, DOI 10.1109/PDGC56933.2022.10053319]
   Bhardwaj Munish, 2022, Digital Image Enhancement Reconstruct, P293
   Cao WM, 2020, IEEE ACCESS, V8, P14531, DOI 10.1109/ACCESS.2020.2966881
   Cha YJ, 2017, COMPUT-AIDED CIV INF, V32, P361, DOI 10.1111/mice.12263
   Chen JD, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3298391
   Cubero-Fernandez A, 2017, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-017-0187-0
   Deng L, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15061530
   Fan R, 2022, IEEE T CYBERNETICS, V52, P5799, DOI 10.1109/TCYB.2021.3060461
   Fan Z, 2018, Arxiv, DOI [arXiv:1802.02208, 10.48550/arXiv.1802.02208]
   Gamage P.T., 2017, Independent Study
   Ghosh S, 2013, INT J ADV COMPUT SC, V4, P35
   github.com, About us
   Gonzalez RC., 2009, Digital Image Processing, DOI [10.1117/1.3115362, DOI 10.1117/1.3115362]
   Hong ZH, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3129607
   Hung MC, 2001, 2001 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P225, DOI 10.1109/ICDM.2001.989523
   Isa NAM, 2009, IEEE T CONSUM ELECTR, V55, P2145, DOI 10.1109/TCE.2009.5373781
   Jing Yang, 2020, IOP Conference Series: Materials Science and Engineering, V782, DOI 10.1088/1757-899X/782/4/042033
   Kaddah W, 2020, VISUAL COMPUT, V36, P1369, DOI 10.1007/s00371-019-01742-2
   Khan NU, 2014, MULTIMED TOOLS APPL, V73, P573, DOI 10.1007/s11042-013-1620-8
   Khumsap Panop, 2018, IEEE INT COMP SCI EN, P1
   Krinidis S, 2010, IEEE T IMAGE PROCESS, V19, P1328, DOI 10.1109/TIP.2010.2040763
   Lei T, 2018, IEEE T FUZZY SYST, V26, P3027, DOI 10.1109/TFUZZ.2018.2796074
   Li HF, 2019, IEEE T INTELL TRANSP, V20, P2025, DOI 10.1109/TITS.2018.2856928
   Liu YH, 2019, NEUROCOMPUTING, V338, P139, DOI 10.1016/j.neucom.2019.01.036
   Liu ZQ, 2019, AUTOMAT CONSTR, V104, P129, DOI 10.1016/j.autcon.2019.04.005
   Madhu A, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13204163
   Mandhare R., 2018, Int. J. Innov. Res. Sci. Eng. Technol, V7, P6518
   Mohan A, 2018, ALEX ENG J, V57, P787, DOI 10.1016/j.aej.2017.01.020
   Mubashshira S, 2020, IEEE REGION 10 SYMP, P1596
   Noga M., 2022, MED IMAGING DEEP LEA
   Ouma YO, 2017, AUTOMAT CONSTR, V83, P196, DOI 10.1016/j.autcon.2017.08.017
   PAL NR, 1995, IEEE T FUZZY SYST, V3, P370, DOI 10.1109/91.413225
   Qi YS, 2022, VISUAL COMPUT, V38, P2499, DOI 10.1007/s00371-021-02126-1
   Qu Z, 2022, IEEE T NEUR NET LEAR, V33, P4890, DOI 10.1109/TNNLS.2021.3062070
   Quan YW, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION (ICMA), P1615, DOI [10.1109/icma.2019.8816422, 10.1109/ICMA.2019.8816422]
   Ren M, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/2647389
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shi Y, 2016, IEEE T INTELL TRANSP, V17, P3434, DOI 10.1109/TITS.2016.2552248
   Singh N.K., 2017, Int. J. Res. Appl. Sci. Eng. Technol, V5, P114, DOI DOI 10.22214/IJRASET.2017.11017
   Wang D., 2022, Computational Intelligence and Neuroscience
   Wang D., 2022, Computational Intelligence and Neuroscience
   Wang QS, 2020, APPL SOFT COMPUT, V92, DOI 10.1016/j.asoc.2020.106318
   Wang WX, 2021, CONSTR BUILD MATER, V271, DOI 10.1016/j.conbuildmat.2020.121885
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yohwan Noh, 2017, 2017 International Conference on Applied System Innovation (ICASI). Proceedings, P877, DOI 10.1109/ICASI.2017.7988574
   Yuan GJ, 2022, IET INTELL TRANSP SY, V16, P782, DOI 10.1049/itr2.12173
   Zhang H, 2018, IEEE J-STARS, V11, P2896, DOI 10.1109/JSTARS.2018.2846603
   Zhang L, 2016, IEEE IMAGE PROC, P3708, DOI 10.1109/ICIP.2016.7533052
   Zhang YM, 2018, IEEE T INTELL TRANSP, V19, P3935, DOI 10.1109/TITS.2018.2791476
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
NR 54
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 22
PY 2024
DI 10.1007/s00371-024-03470-8
EA JUN 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UZ5H2
UT WOS:001251894500001
DA 2024-08-05
ER

PT J
AU Zhao, Q
   Zhang, CX
   Rao, ZB
   Chen, Z
   Wang, ZG
   Lu, K
AF Zhao, Qi
   Zhang, Congxuan
   Rao, Zhibo
   Chen, Zhen
   Wang, Zige
   Lu, Ke
TI GPDF-Net: geometric prior-guided stereo matching with disparity fusion
   refinement
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Stereo matching; Geometric prior guidance; Disparity feature; Efficient
   3D convolution
ID AGGREGATION
AB Stereo matching is a popular topic in the image processing and computer vision fields. Although deep learning-based stereo matching approaches have achieved remarkable performance with respect to both estimation accuracy and computation efficiency, textureless and occluded regions and regions with edge blurring remain significant challenges for most existing stereo matching methods. To address these issues, we propose a geometric prior-guided stereo matching method with disparity fusion refinement, named GPDF-Net, in this paper. First, we exploit a geometric prior guidance module in the feature extraction part to enable features to obtain the global information of cross-view interactions and pay attention to the structural information contained in the input image. Second, we construct cost volume with disparity fusion refinement that introduces disparity-related geometric features to the concatenation volume and suppresses its redundant information to provide a better similarity measure. Third, we explore a method for replacing 3D convolution, making the cost aggregation module lighter and more efficient. Finally, we compare the proposed method with several state-of-the-art approaches on the Scene Flow and KITTI test databases. The experimental results demonstrate that the proposed method achieves competitive performance with respect to both accuracy and robustness, and it produces better results than those of other methods when given blurred edges and textureless and occluded areas. (The code is available at https://github.com/PCwenyue.)
C1 [Zhao, Qi; Zhang, Congxuan; Chen, Zhen; Wang, Zige] Nanchang Hangkong Univ, Sch Measuring & Opt Engn, Nanchang 330063, Peoples R China.
   [Zhang, Congxuan; Rao, Zhibo; Chen, Zhen] Nanchang Hangkong Univ, Key Lab Jiangxi Prov Image Proc & Pattern Recognit, Nanchang 330063, Peoples R China.
   [Lu, Ke] Univ Chinese Acad Sci, Coll Engn Sci, Beijing 100049, Peoples R China.
C3 Nanchang Hangkong University; Nanchang Hangkong University; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Zhang, CX; Chen, Z (corresponding author), Nanchang Hangkong Univ, Sch Measuring & Opt Engn, Nanchang 330063, Peoples R China.; Zhang, CX; Chen, Z (corresponding author), Nanchang Hangkong Univ, Key Lab Jiangxi Prov Image Proc & Pattern Recognit, Nanchang 330063, Peoples R China.
EM zcxdsg@163.com; dr_chenzhen@163.com
OI Zhang, Congxuan/0000-0003-1356-1205
FU Innovation Fund Designated for Graduate Students of Jiangxi Province
FX No Statement Available
CR Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567
   Chang TY, 2023, PROC CVPR IEEE, P9559, DOI 10.1109/CVPR52729.2023.00922
   Chen ZH, 2023, IEEE T PATTERN ANAL, V45, P13489, DOI 10.1109/TPAMI.2023.3293885
   Chen ZY, 2015, IEEE I CONF COMP VIS, P972, DOI 10.1109/ICCV.2015.117
   Cheng M., 2023, P IEEE CVF C COMP VI, P1702
   Cheng XJ, 2020, IEEE T PATTERN ANAL, V42, P2361, DOI 10.1109/TPAMI.2019.2947374
   Chong AX, 2022, NEUROCOMPUTING, V492, P601, DOI 10.1016/j.neucom.2021.12.052
   Da Sie Y, 2019, IEEE VTS VEH TECHNOL, DOI 10.1109/vtcspring.2019.8746289
   de Figueiredo RP., 2021, arXiv, DOI 10.48550/arXiv.2105.12691
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Feihu Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P420, DOI 10.1007/978-3-030-58536-5_25
   Gan YK, 2018, LECT NOTES COMPUT SC, V11207, P232, DOI 10.1007/978-3-030-01219-9_14
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Girshick R., 2015, Proceedings of the IEEE international conference on computer vision, DOI [DOI 10.1109/ICCV.2015.169, 10.1109/ICCV.2015.169]
   Gu XD, 2020, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR42600.2020.00257
   Guo XY, 2019, PROC CVPR IEEE, P3268, DOI 10.1109/CVPR.2019.00339
   He QD, 2023, IEEE T INTELL TRANSP, V24, P152, DOI 10.1109/TITS.2022.3215766
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI [10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166]
   Humenberger M., 2010, 2010 IEEE COMP SOC C
   Ikehata S, 2023, PROC CVPR IEEE, P13198, DOI 10.1109/CVPR52729.2023.01268
   Jing JP, 2023, IEEE I CONF COMP VIS, P3295, DOI 10.1109/ICCV51070.2023.00307
   Kendall A., 2017, 2017 IEEE INT C COMP
   Kerkaou Z, 2020, MULTIMED TOOLS APPL, V79, P27039, DOI 10.1007/s11042-020-09260-3
   Kingma D. P., 2014, arXiv
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Li X, 2022, VISUAL COMPUT, V38, P3881, DOI 10.1007/s00371-021-02228-w
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Lipson L, 2021, INT CONF 3D VISION, P218, DOI 10.1109/3DV53792.2021.00032
   Liu BY, 2022, AAAI CONF ARTIF INTE, P1647
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo WJ, 2016, PROC CVPR IEEE, P5695, DOI 10.1109/CVPR.2016.614
   Matic A, 2021, FRONT NEUROROBOTICS, V15, DOI 10.3389/fnbot.2021.755723
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Menze M, 2015, ISPRS ANN PHOTO REM, VII-3, P427, DOI 10.5194/isprsannals-II-3-W5-427-2015
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3
   Shen ZL, 2021, PROC CVPR IEEE, P13901, DOI 10.1109/CVPR46437.2021.01369
   Smolyanskiy N, 2017, FRONT ROBOT AI, V4, DOI 10.3389/frobt.2017.00011
   Sun JM, 2021, PROC CVPR IEEE, P8918, DOI 10.1109/CVPR46437.2021.00881
   Sun L., 2023, IEEE Trans. Instrum. Meas.
   Tonioni A, 2019, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2019.00028
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Q., 2020, Learning Feature Descriptors Using Camera Pose Supervision, DOI [10.1007/978-3-030-58452-844, DOI 10.1007/978-3-030-58452-844]
   Woodford O, 2009, IEEE T PATTERN ANAL, V31, P2115, DOI 10.1109/TPAMI.2009.131
   Wu BB, 2021, IEEE ROBOT AUTOM LET, V6, P7137, DOI 10.1109/LRA.2021.3097260
   Wu ZY, 2019, IEEE I CONF COMP VIS, P7483, DOI 10.1109/ICCV.2019.00758
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Xu GW, 2023, PROC CVPR IEEE, P21919, DOI 10.1109/CVPR52729.2023.02099
   Xu GW, 2022, PROC CVPR IEEE, P12971, DOI 10.1109/CVPR52688.2022.01264
   Xu H., 2023, IEEE T PATTERN ANAL
   Xu HF, 2020, PROC CVPR IEEE, P1956, DOI 10.1109/CVPR42600.2020.00203
   Yang KL, 2017, J AMB INTEL SMART EN, V9, P743, DOI 10.3233/AIS-170459
   Yang L, 2023, PROC CVPR IEEE, P21108, DOI 10.1109/CVPR52729.2023.02022
   Yu WH, 2023, Arxiv, DOI [arXiv:2303.16900, 10.48550/arXiv.2303.16900]
   Žbontar J, 2016, Arxiv, DOI arXiv:1510.05970
   Zeng K, 2022, IEEE T INTELL TRANSP, V23, P25437, DOI 10.1109/TITS.2021.3134416
   Zhang FH, 2019, PROC CVPR IEEE, P185, DOI 10.1109/CVPR.2019.00027
NR 58
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 4
PY 2024
DI 10.1007/s00371-024-03459-3
EA JUN 2024
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TA7R3
UT WOS:001238606000002
DA 2024-08-05
ER

PT J
AU Wang, LW
   Wang, Z
   Zhao, X
   Tsung, FG
   Zeng, W
AF Wang, Liangwei
   Wang, Zhan
   Zhao, Xi
   Tsung, Fugee
   Zeng, Wei
TI Antarctica storytelling: creating interactive story maps for polar
   regions with graphic-based approach
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Storytelling; Story maps; Map-based visual narrative; Antarctica
ID VISUALIZATION
AB Although story maps have gained popularity for storytelling related to spatial information, existing story maps authoring tools often fall short in delivering diverse narrative forms and struggle to accurately render polar regions due to the limitations of tile-based mapping. In this work, we introduce a graphic-based method to address these challenges, developing a framework specifically designed for creating story maps for polar regions. Our key contribution lies in offering heuristic strategies for story map design, emphasizing their role in effectively visualizing and disseminating polar culture. This paper outlines essential design tasks for story map creation and introduces three pivotal narrative strategies: integration of map and other visual elements, attention cue, and cartographic interaction. Additionally, we emphasize the significance of storyboard design, focusing on aspects such as logical sequencing, temporal order, map scale, and interactive design. To validate the effectiveness of our story map design framework, we develop several story map cases centered around the exploration history of Antarctica. These examples highlight the diversity and interactivity in the story maps produced through our methodology. Finally, we explore the challenges and limitations encountered in the process of creating story maps, and from these observations, we identify prospective areas for further research.
C1 [Wang, Liangwei; Wang, Zhan; Tsung, Fugee; Zeng, Wei] Hong Kong Univ Sci & Technol Guangzhou, Guangzhou, Peoples R China.
   [Tsung, Fugee; Zeng, Wei] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
   [Zhao, Xi] Sun Yat Sen Univ, Zhuhai, Peoples R China.
C3 Hong Kong University of Science & Technology (Guangzhou); Hong Kong
   University of Science & Technology; Sun Yat Sen University
RP Zeng, W (corresponding author), Hong Kong Univ Sci & Technol Guangzhou, Guangzhou, Peoples R China.; Zeng, W (corresponding author), Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
EM weizeng@hkust-gz.edu.cn
RI ; Tsung, Fugee/H-8291-2013
OI Zeng, Wei/0000-0002-5600-8824; Zhao, Xi/0000-0003-2274-891X; Tsung,
   Fugee/0000-0002-0575-8254; Wang, Zhan/0000-0003-3318-6473; Wang,
   Liangwei/0000-0003-3481-3993
FU National Natural Science Foundation of China [62172398]; National
   Natural Science Foundation of China
FX This work is supported partially by National Natural Science Foundation
   of China (62172398).
CR Amini F, 2017, IEEE T VIS COMPUT GR, V23, P501, DOI 10.1109/TVCG.2016.2598647
   Amini F, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1459, DOI 10.1145/2702123.2702431
   Bartalesi V, 2023, INT J DIGIT EARTH, V16, P234, DOI 10.1080/17538947.2023.2168774
   Bostock M, 2011, IEEE T VIS COMPUT GR, V17, P2301, DOI 10.1109/TVCG.2011.185
   Chen SM, 2020, IEEE T VIS COMPUT GR, V26, P2499, DOI 10.1109/TVCG.2018.2889054
   Chotisarn N, 2023, VIS INFORM, V7, P18, DOI 10.1016/j.visinf.2023.01.004
   Denil M., 2016, Cartogr. Perspect., V84, P5, DOI [10.14714/CP84.1374, DOI 10.14714/CP84.1374]
   ESRI, 2024, Arcgis storymaps. ESRI Story Map
   Figueiras A, 2014, IEEE INT CONF INF VI, P46, DOI 10.1109/IV.2014.79
   FORSYTHE WC, 1995, ECOL MODEL, V80, P87, DOI 10.1016/0304-3800(94)00034-F
   Gerrish L., 2021, Medium resolution vector polygons of the antarctic coastline (7.4) data set
   Huang HS, 2011, INT J GEOGR INF SCI, V25, P1561, DOI 10.1080/13658816.2010.532133
   KnightLab N.U.: Storymap.js, 2024, maps that tell stories
   Kontou DM, 2022, REG STUD REG SCI, V9, P320, DOI 10.1080/21681376.2022.2074306
   Lan XY, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445344
   Li WC, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2023), DOI 10.1145/3544548.3581470
   Mapbox, Mapbox Studio
   Mocnik FB, 2018, CARTOGR J, V55, P36, DOI 10.1080/00087041.2017.1304498
   Neumann Andreas., 2012, Springer Handbook of Geographic Information, P273, DOI [10.1007/978-3-319-17885-11485, DOI 10.1007/978-3-319-17885-11485]
   ORSI AH, 1995, DEEP-SEA RES PT I, V42, P641, DOI 10.1016/0967-0637(95)00021-W
   Phillips J, 2012, EARTH-SCI REV, V115, P153, DOI 10.1016/j.earscirev.2012.09.005
   Prestby T, 2024, CARTOGR GEOGR INF SC, V51, P222, DOI 10.1080/15230406.2022.2102077
   Riche N.H., 2018, Data-driven storytelling, DOI DOI 10.1201/9781315281575
   Roth RE, 2021, CARTOGR J, V58, P83, DOI 10.1080/00087041.2019.1633103
   Roth RE, 2013, J SPAT INF SCI, P59, DOI 10.5311/JOSIS.2013.6.105
   Sample JT, 2010, TILE-BASED GEOSPATIAL INFORMATION SYSTEMS: PRINCIPLES AND PRACTICES, P1, DOI 10.1007/978-1-4419-7631-4
   Segel E, 2010, IEEE T VIS COMPUT GR, V16, P1139, DOI 10.1109/TVCG.2010.179
   Shi Yang, 2023, IEEE Trans Vis Comput Graph, V29, P972, DOI 10.1109/TVCG.2022.3209409
   Smith DA, 2016, COMPUT ENVIRON URBAN, V57, P106, DOI 10.1016/j.compenvurbsys.2016.01.002
   Song Z., 2022, Cartogr. Perspect., V100, P10, DOI [10.14714/CP100.1759, DOI 10.14714/CP100.1759]
   Sova R., 2006, P UPA C US STOR, P1
   Speckmann B, 2010, IEEE T VIS COMPUT GR, V16, P881, DOI 10.1109/TVCG.2010.180
   Thöny M, 2018, ISPRS INT J GEO-INF, V7, DOI 10.3390/ijgi7030123
   [田璐 Tian Lu], 2012, [极地研究, Chinese Journal of Polar Research], V24, P284
   Tierney L., 2018, Cartogr. Perspect., V89, P44, DOI [10.14714/CP89.1469, DOI 10.14714/CP89.1469]
   VISVALINGAM M, 1993, CARTOGR J, V30, P46
   Wang L., 2023, P 16 INT S VIS INF C, DOI [10.1145/3615522.3615524, DOI 10.1145/3615522.3615524]
   Wu XJ, 2020, COMPUT GEOSCI-UK, V137, DOI 10.1016/j.cageo.2020.104420
   Zhi Q, 2019, COMPUT GRAPH FORUM, V38, P675, DOI 10.1111/cgf.13719
   Zunino A, 2020, ISPRS INT J GEO-INF, V9, DOI 10.3390/ijgi9100563
NR 40
TC 0
Z9 0
U1 6
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 30
PY 2024
DI 10.1007/s00371-024-03489-x
EA MAY 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO8V9
UT WOS:001235494300002
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Zhao, J
   He, YJ
   Shi, Z
   Qin, J
   Xie, YN
AF Zhao, Jing
   He, Yong-jun
   Shi, Zheng
   Qin, Jian
   Xie, Yi-ning
TI A style-aware network based on multi-task learning for multi-domain
   image normalization
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Multi-domain image style transfer; Style awareness; Multi-task learning;
   Staining standardization
ID STAIN; AUGMENTATION
AB Cervical cell image styles may vary due to factors such as specimen preparation methods and staining schemes. These variations can cause inconsistencies among pathologists and degrade the model performance. Existing staining standardization networks often fail to achieve structure preservation and style approximation. We propose a style-aware network (SA-Net) for multi-domain image normalization to address this issue. SA-Net incorporates a style perception task into the CycleGAN generator to identify different image styles, thus avoiding the need for multiple generators in real-world applications. additionally, We also employ pixel-wise convolutional kernels in the generator to learn only the image style and preserve the image structure. Our experiments demonstrate that SA-Net can effectively enhance the model's generalization ability and outperform the state-of-the-art methods in multi-style standardization.
C1 [Zhao, Jing; Xie, Yi-ning] Northeast Forestry Univ, Mech & Elect Engn, Harbin 150006, Peoples R China.
   [He, Yong-jun] Harbin Inst Technol, Sch Comp Sci, Harbin 150001, Peoples R China.
   [Shi, Zheng; Qin, Jian] Harbin Univ Sci & Technol, Sch Comp Sci & Technol, 52 Xuefu Rd, Harbin 150080, Peoples R China.
C3 Northeast Forestry University - China; Harbin Institute of Technology;
   Harbin University of Science & Technology
RP Xie, YN (corresponding author), Northeast Forestry Univ, Mech & Elect Engn, Harbin 150006, Peoples R China.; He, YJ (corresponding author), Harbin Inst Technol, Sch Comp Sci, Harbin 150001, Peoples R China.
EM Holywit@163.com; yiningxie@nefu.edu.cn
OI xie, yining/0000-0002-0435-4012
FU National funded postdoctoral researcher program
FX No Statement Available
CR Tosta TAA, 2019, COMPUT MED IMAG GRAP, V77, DOI 10.1016/j.compmedimag.2019.101646
   BenTaieb A, 2018, IEEE T MED IMAGING, V37, P792, DOI 10.1109/TMI.2017.2781228
   Chen XH, 2021, COMPUT STRUCT BIOTEC, V19, P3852, DOI 10.1016/j.csbj.2021.06.025
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Cong C, 2022, MED IMAGE ANAL, V82, DOI 10.1016/j.media.2022.102580
   Cong C, 2021, I S BIOMED IMAGING, P1949, DOI 10.1109/ISBI48211.2021.9433860
   Hetz MJ, 2023, Arxiv, DOI [arXiv:2301.09431, 10.48550/arXiv.2301.09431, DOI 10.48550/ARXIV.2301.09431]
   Hoque MZ, 2021, COMPUT MED IMAG GRAP, V90, DOI 10.1016/j.compmedimag.2021.101901
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kang HT, 2024, Arxiv, DOI arXiv:2305.06511
   Lei GY, 2020, NEUROCOMPUTING, V406, P267, DOI 10.1016/j.neucom.2020.04.008
   Liang MY, 2023, VISUAL COMPUT, V39, P4305, DOI 10.1007/s00371-022-02592-1
   Ling Y, 2023, IEEE T MED IMAGING, V42, P3625, DOI 10.1109/TMI.2023.3298361
   Lo YC, 2021, APPL SOFT COMPUT, V98, DOI 10.1016/j.asoc.2020.106822
   Lu N, 2024, VISUAL COMPUT, V40, P4519, DOI 10.1007/s00371-023-03096-2
   Mahapatra Dwarikanath, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12265), P309, DOI 10.1007/978-3-030-59722-1_30
   Mahapatra S, 2023, IEEE T MED IMAGING, V42, P1746, DOI 10.1109/TMI.2023.3238425
   Nanni L, 2021, J IMAGING, V7, DOI 10.3390/jimaging7120254
   Nazki H, 2022, Arxiv, DOI arXiv:2204.09782
   Nishar Harshal, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12265), P330, DOI 10.1007/978-3-030-59722-1_32
   Patil A, 2021, I S BIOMED IMAGING, P1563, DOI 10.1109/ISBI48211.2021.9434121
   Pérez-Bueno F, 2022, COMPUT MED IMAG GRAP, V97, DOI 10.1016/j.compmedimag.2022.102048
   Plass M, 2022, IEEE COMPUT GRAPH, V42, P47, DOI 10.1109/MCG.2022.3197957
   Shaban MT, 2019, I S BIOMED IMAGING, P953, DOI [10.1109/isbi.2019.8759152, 10.1109/ISBI.2019.8759152]
   Shen YQ, 2022, LECT NOTES COMPUT SC, V13432, P212, DOI 10.1007/978-3-031-16434-7_21
   Tellez D, 2019, MED IMAGE ANAL, V58, DOI 10.1016/j.media.2019.101544
   Tosta TAA, 2023, BIOMED SIGNAL PROCES, V85, DOI 10.1016/j.bspc.2023.104978
   Vijh S, 2021, APPL INTELL, V51, P7735, DOI 10.1007/s10489-021-02231-7
   Wagner SJ, 2021, LECT NOTES COMPUT SC, V12908, P257, DOI 10.1007/978-3-030-87237-3_25
   Ye H.L., 2022, PREPRINT
   Zhao BC, 2022, COMPUT ELECTR ENG, V103, DOI 10.1016/j.compeleceng.2022.108304
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 32
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 12
PY 2024
DI 10.1007/s00371-024-03363-w
EA MAY 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QJ3K5
UT WOS:001220466300001
DA 2024-08-05
ER

PT J
AU Zhang, JJ
   Zhou, YB
   Tong, T
   Liu, HJ
   Tian, T
   Hu, XM
   Gao, QQ
   Lin, XY
AF Zhang, Jiajun
   Zhou, Yuanbo
   Tong, Tong
   Liu, Hongjun
   Tian, Tian
   Hu, Xingmei
   Gao, Qinquan
   Lin, Xiaoyong
TI E2-RealSR: efficient and effective real-world super-resolution network
   based on partial degradation modulation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Real-world image super-resolution; Efficient and effective
   super-resolution; Partial degradation modulation; Degradation prediction
AB The goal of efficient and effective real-world image super-resolution (Real-ISR) is to recover the high-resolution image from the given low-resolution image with unknown degradation by limited computation resources. Prior research has attempted to design a fully degradation-adaptive network, where the entire backbone is a nonlinear combination of several sub-networks which can handle different degradation subspaces. However, these methods heavily rely on expensive dynamic convolution operations and are inefficient in super-resolving images of different degradation levels. To address this issue, we propose an efficient and effective real-world image super-resolution network (E2-RealSR) based on partial degradation modulation, which is consisted of a small regression and a lightweight super-resolution network. The former accurately predicts the individual degradation parameters of input images, while the latter only modulates its partial parameters based on the degradation information. The extensive experiments validate that our proposed method is capable of recovering the rich details in real-world images with varying degradation levels. Moreover, our approach also has an advantage in terms of efficiency, compared to state-of-the-art methods. Our method shows improved performance while only using 20% of the parameters and 60% of the FLOPs of DASR. The relevant code is made available on this link as open source.
C1 [Zhang, Jiajun; Zhou, Yuanbo; Tong, Tong; Gao, Qinquan] Fuzhou Univ, Coll Phys & Informat Engn, Fuzhou 350108, Fujian, Peoples R China.
   [Liu, Hongjun; Tian, Tian; Hu, Xingmei] Chongqing Dadukou Dist Arch, Chongqing 400000, Peoples R China.
   [Lin, Xiaoyong] Xiamen Univ Technol, Xiamen 361021, Fujian, Peoples R China.
C3 Fuzhou University; Xiamen University of Technology
RP Gao, QQ (corresponding author), Fuzhou Univ, Coll Phys & Informat Engn, Fuzhou 350108, Fujian, Peoples R China.
EM jjiajunzhang@gmail.com; webbozhou@gmail.com; ttraveltong@gmail.com;
   hongjun_cq@hotmail.com; 69803336@qq.com; 466651993@qq.com;
   gqinquan@fzu.edu.cn; xylin_xm@126.com
FU National Natural Science Foundation of China [62171133]; Artificial
   Intelligence and Economy Integration Platform of Fujian Province; Fujian
   Health Commission [2022ZD01003]
FX This work was supported by National Natural Science Foundation of China
   under Grant 62171133, in part by the Artificial Intelligence and Economy
   Integration Platform of Fujian Province, and the Fujian Health
   Commission under Grant 2022ZD01003.
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Bell-Kligler S, 2019, ADV NEUR IN, V32
   Chen HG, 2022, INFORM FUSION, V79, P124, DOI 10.1016/j.inffus.2021.09.005
   Chen XY, 2022, Arxiv, DOI arXiv:2205.04437
   Chen YT, 2024, VISUAL COMPUT, V40, P489, DOI 10.1007/s00371-023-02795-0
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Fritsche M, 2019, IEEE INT CONF COMP V, P3599, DOI 10.1109/ICCVW.2019.00445
   Gu JJ, 2019, PROC CVPR IEEE, P1604, DOI 10.1109/CVPR.2019.00170
   Hua XY, 2023, VISUAL COMPUT, V39, P267, DOI 10.1007/s00371-021-02327-8
   Huang CK, 2009, OPT COMMUN, V282, P2123, DOI 10.1016/j.optcom.2009.02.044
   Huang Yan, 2020, Advances in Neural Information Processing Systems, V33, P5632, DOI DOI 10.48550/ARXIV.2010.02631
   Kang XD, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3192680
   Kingma D. P., 2014, arXiv
   Kong FY, 2022, IEEE COMPUT SOC CONF, P765, DOI 10.1109/CVPRW56347.2022.00092
   Li X, 2021, AAAI CONF ARTIF INTE, V35, P1975
   Liang J, 2022, LECT NOTES COMPUT SC, V13678, P574, DOI 10.1007/978-3-031-19797-0_33
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu AQ, 2023, VISUAL COMPUT, V39, P3837, DOI 10.1007/s00371-022-02519-w
   Liu AR, 2023, IEEE T PATTERN ANAL, V45, P5461, DOI 10.1109/TPAMI.2022.3203009
   Lu XB, 2024, VISUAL COMPUT, V40, P41, DOI 10.1007/s00371-022-02764-z
   Luo Z., 2022, CVPR, P6063
   Luo ZX, 2021, Arxiv, DOI arXiv:2105.06878
   Luo ZW, 2022, PROC CVPR IEEE, P17621, DOI 10.1109/CVPR52688.2022.01712
   Mahapatra D, 2019, COMPUT MED IMAG GRAP, V71, P30, DOI 10.1016/j.compmedimag.2018.10.005
   Mou C, 2022, LECT NOTES COMPUT SC, V13677, P723, DOI 10.1007/978-3-031-19790-1_43
   Pengxu Wei, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P101, DOI 10.1007/978-3-030-58598-3_7
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Trockman A., 2022, ARXIV
   Wang JX, 2024, VISUAL COMPUT, V40, P2655, DOI 10.1007/s00371-023-02968-x
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang XT, 2021, IEEE INT CONF COMP V, P1905, DOI 10.1109/ICCVW54120.2021.00217
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie LB, 2021, ADV NEUR IN, V34
   Yoo J, 2022, Arxiv, DOI arXiv:2203.07682
   Yu-Syuan Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12493, DOI 10.1109/CVPR42600.2020.01251
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4771, DOI 10.1109/ICCV48922.2021.00475
   Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhou YB, 2020, IEEE COMPUT SOC CONF, P1722, DOI 10.1109/CVPRW50498.2020.00222
NR 43
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 11
PY 2024
DI 10.1007/s00371-024-03279-5
EA MAY 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QJ3J7
UT WOS:001220465500001
DA 2024-08-05
ER

PT J
AU Liu, G
   Wang, JB
   Qian, Y
   Li, YH
AF Liu, Gang
   Wang, Jiebang
   Qian, Yao
   Li, Yonghua
TI Infrared and visible image fusion method based on visual saliency
   objects and fuzzy region attributes
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image fusion; Infrared image; Saliency detection; Fuzzy membership;
   Regional consistency
ID NETWORK; TRANSFORM
AB To enhance the regional feature preserving ability and guaranteed inclusion of complete objects in the final fused images, this paper proposes a new image fusion method based on visual saliency objects and fuzzy region attributes. In this approach, the complementary feature encoders are used to extract the features of infrared and visible images, respectively. In the feature fusion layer, to avoid the loss of target features caused by pixel-level fusion and region rigid division, this paper introduces the fuzzy set theory and salient target detection into the design of the fusion strategy. Following the k-means clustering and the energy scale of the infrared scene, the essential attributes of the areas are established. Meanwhile, a salient object segmentation algorithm based on a convolution neural network is adopted to extract the salient region of the source image. Then, feature maps are fused using fuzzy region memberships and salient feature mapping. Finally, the fused image is reconstructed by a redundant feature decoder. The proposed method focuses on the fusion of color scene images and multi-wavelength infrared (thermal and near-infrared) images. Experiments are performed on public RGB-T and RGB-NIR datasets. Compared with several start-of-the-art methods, the evaluation of visual and objective metrics demonstrates that the proposed fusion method has superior performance and is more in line with human sensory characteristics.
C1 [Liu, Gang; Wang, Jiebang; Qian, Yao; Li, Yonghua] Shanghai Univ Elect Power, Sch Automat Engn, Shanghai 200090, Peoples R China.
C3 Shanghai University of Electric Power
RP Liu, G (corresponding author), Shanghai Univ Elect Power, Sch Automat Engn, Shanghai 200090, Peoples R China.
EM liugang@shiep.edu.cn; jbwang625@gmail.com; qianyao199709@163.com;
   yonghuali1998@gmail.com
OI Liu, Gang/0000-0003-4143-828X
FU National Natural Science Foundation of China [62203224]; National
   Natural Science Foundation of China [22010501300]; Capacity Building
   Plan for some Non-military Universities and Colleges of Shanghai
   Scientific Committee
FX The authors gratefully acknowledge the financial support by The National
   Natural Science Foundation of China (62203224), Capacity Building Plan
   for some Non-military Universities and Colleges of Shanghai Scientific
   Committee (22010501300).
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Baldi P., 2012, P WORKSH UNS TRANSF, P37
   Borji A, 2019, COMPUT VIS MEDIA, V5, P117, DOI 10.1007/s41095-019-0149-9
   Brown M, 2011, PROC CVPR IEEE, P177, DOI 10.1109/CVPR.2011.5995637
   Chen J, 2020, INFORM SCIENCES, V508, P64, DOI 10.1016/j.ins.2019.08.066
   Chen L, 2013, OPT EXPRESS, V21, P5182, DOI 10.1364/OE.21.005182
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Cui GM, 2015, OPT COMMUN, V341, P199, DOI 10.1016/j.optcom.2014.12.032
   da Cunha AL, 2006, IEEE T IMAGE PROCESS, V15, P3089, DOI 10.1109/TIP.2006.877507
   Davis JW, 2007, COMPUT VIS IMAGE UND, V106, P162, DOI 10.1016/j.cviu.2006.06.010
   Fan ZL, 2021, OPT ENG, V60, DOI 10.1117/1.OE.60.12.123102
   Fan ZL, 2016, INFRARED PHYS TECHN, V74, P44, DOI 10.1016/j.infrared.2015.11.006
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Gao C, 2021, IEEE ACCESS, V9, P91462, DOI 10.1109/ACCESS.2021.3090436
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guo ZL, 2018, INT GEOSCI REMOTE SE, P6187, DOI 10.1109/IGARSS.2018.8519049
   Han L, 2019, IEEE ACCESS, V7, P48890, DOI 10.1109/ACCESS.2019.2910572
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Hou RC, 2020, IEEE T COMPUT IMAG, V6, P640, DOI 10.1109/TCI.2020.2965304
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   ino, INO video dataset
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Ji YZ, 2021, INFORM SCIENCES, V546, P835, DOI 10.1016/j.ins.2020.09.003
   Jiang YQ, 2021, INFRARED PHYS TECHN, V117, DOI 10.1016/j.infrared.2021.103842
   Jin HY, 2014, INFRARED PHYS TECHN, V64, P134, DOI 10.1016/j.infrared.2014.02.013
   Lahmyed R, 2019, MULTIMED TOOLS APPL, V78, P15861, DOI 10.1007/s11042-018-6974-5
   Lewis J.J., 2006, The Eden Project multi-sensor data set. The Online Resource for Research in Image Fusion (ImageFusion.org)
   Lewis JJ, 2007, INFORM FUSION, V8, P119, DOI 10.1016/j.inffus.2005.09.006
   Li CL, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.106977
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li HF, 2016, INFRARED PHYS TECHN, V76, P174, DOI 10.1016/j.infrared.2016.02.005
   Li H, 2022, Arxiv, DOI arXiv:1804.08992
   Li H, 2020, IEEE T IMAGE PROCESS, V29, P4733, DOI 10.1109/TIP.2020.2975984
   Li H, 2018, INT C PATT RECOG, P2705, DOI 10.1109/ICPR.2018.8546006
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu G., 2010, P INT C MACH LEARN, P663
   Liu Y, 2017, INFORM FUSION, V36, P191, DOI 10.1016/j.inffus.2016.12.001
   Liu Y, 2016, IEEE SIGNAL PROC LET, V23, P1882, DOI 10.1109/LSP.2016.2618776
   Ma JY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3038013
   Ma JY, 2020, IEEE T IMAGE PROCESS, V29, P4980, DOI 10.1109/TIP.2020.2977573
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JY, 2019, INFORM FUSION, V45, P153, DOI 10.1016/j.inffus.2018.02.004
   Ma JY, 2016, INFORM FUSION, V31, P100, DOI 10.1016/j.inffus.2016.02.001
   Ma JL, 2017, INFRARED PHYS TECHN, V82, P8, DOI 10.1016/j.infrared.2017.02.005
   Ma L., 2021, IEEE T INSTRUM MEAS, V70, P1, DOI [10.1109/TIM.2021.3075747.4J, DOI 10.1109/TIM.2021.3075747.4J, 10.1109/TIM.2021.3075747, DOI 10.1109/TIM.2021.3075747]
   Mao R, 2018, 2018 3RD INTERNATIONAL CONFERENCE ON MECHANICAL, CONTROL AND COMPUTER ENGINEERING (ICMCCE), P568, DOI 10.1109/ICMCCE.2018.00125
   Meher B, 2019, INFORM FUSION, V48, P119, DOI 10.1016/j.inffus.2018.07.010
   Meng FJ, 2016, NEUROCOMPUTING, V177, P1, DOI 10.1016/j.neucom.2015.10.080
   Miles B, 2013, IEEE T BIO-MED ENG, V60, P1841, DOI 10.1109/TBME.2013.2243448
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Niu YF, 2012, MATH PROBL ENG, V2012, DOI 10.1155/2012/275138
   Pang HC, 2012, 2012 5TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), P543, DOI 10.1109/CISP.2012.6469884
   Prabhakar KR, 2017, IEEE I CONF COMP VIS, P4724, DOI 10.1109/ICCV.2017.505
   Rao YJ, 1997, MEAS SCI TECHNOL, V8, P355, DOI 10.1088/0957-0233/8/4/002
   Riley T, 2006, PROC SPIE, V6402, DOI 10.1117/12.689925
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sun CQ, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9122162
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1007/s11263-017-1004-z, 10.1109/ICCV.2015.164]
   Xu Z, 2021, IET COMPUT VIS, V15, P487, DOI 10.1049/cvi2.12046
   Yang B, 2010, IEEE T INSTRUM MEAS, V59, P884, DOI 10.1109/TIM.2009.2026612
   Yun Z., 2006, P 14 ACM INT C MULT
   Zhang DZ, 2021, MATH PROBL ENG, V2021, DOI 10.1155/2021/4209963
   Zhang XY, 2017, J OPT SOC AM A, V34, P1400, DOI 10.1364/JOSAA.34.001400
   Zhang XC, 2019, 2019 22ND INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION 2019), DOI 10.23919/fusion43075.2019.9011253
   Zhao JF, 2013, INFRARED PHYS TECHN, V56, P93, DOI 10.1016/j.infrared.2012.11.003
   Zhao MY, 2016, PATTERN RECOGN, V51, P281, DOI 10.1016/j.patcog.2015.09.008
NR 68
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 3
PY 2024
DI 10.1007/s00371-024-03392-5
EA MAY 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PN0Z5
UT WOS:001214652000003
DA 2024-08-05
ER

PT J
AU Long, DN
   Chen, RR
AF Long, Dingning
   Chen, Rongrong
TI Cognitive capacity and aesthetics: the influence of visual working
   memory on landscape ink painting preference
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Visual working memory; Visual aesthetic; Complexity; Ink paintings;
   Processing fluency
ID COMPLEXITY; ART; BEAUTY; APPRECIATION; INFORMATION
AB The appreciation of art is not solely influenced by the inherent qualities of a work, such as image complexity, but also by external factors related to the observer, such as their visual working memory capacity, which allows for the real-time storage and manipulation of visual information. In this study, we sought to examine the predictive role of both intrinsic and extrinsic factors in aesthetic appreciation of landscape ink paintings. To assess the visual working memory capacity, we utilized the Visual-Object Working Memory task which required participants to sequentially encode, maintain, and determine whether the current visual pattern matched the one presented two trials ago. Following this, participants were presented with pairs of twenty landscape ink paintings and asked to indicate either their preference or perceived complexity for one painting over the other. Additionally, participants rated their aesthetic preference and perceived level of complexity for each painting on 6-point Likert scales. While our sample did not exhibit a bias toward complexity in their evaluation of paintings, we did discover that individuals with a greater visual working memory capacity tend to favor ink paintings with higher image entropy. These results align with the Processing Fluency Theory, suggesting that individuals typically have a more positive reaction to stimuli that are readily and smoothly processed. This research may have implications for the customization of art recommendation algorithms that take into account individual cognitive traits.
C1 [Long, Dingning; Chen, Rongrong] BNU HKBU United Int Coll, Fac Sci & Technol, Appl Psychol Programme, Zhuhai, Peoples R China.
   [Long, Dingning] Beijing Normal Univ, Fac Psychol, Beijing, Peoples R China.
   [Chen, Rongrong] BNU HKBU United Int Coll, Guangdong Prov Key Lab IRADS, Zhuhai, Peoples R China.
C3 Beijing Normal University - Hong Kong Baptist University United
   International College; Beijing Normal University; Beijing Normal
   University - Hong Kong Baptist University United International College
RP Chen, RR (corresponding author), BNU HKBU United Int Coll, Fac Sci & Technol, Appl Psychol Programme, Zhuhai, Peoples R China.; Chen, RR (corresponding author), BNU HKBU United Int Coll, Guangdong Prov Key Lab IRADS, Zhuhai, Peoples R China.
EM Dingningll@outlook.com; rainerrchen@uic.edu.cn
OI Chen, Rongrong/0000-0001-8287-3478; Long, Dingning/0009-0003-1495-9626
FU Guangdong Provincial Key Laboratory IRADS
FX No Statement Available
CR Arnheim, 2020, ART VISUAL PERCEPTIO, DOI [10.1525/9780520351271, DOI 10.1525/9780520351271]
   Bao Y, 2016, FRONT PSYCHOL, V7, DOI 10.3389/fpsyg.2016.01596
   BERLYNE D. E., 1960
   Berlyne D.E., 1971, Aesthetics and Psychobiology
   BORNSTEIN MH, 1975, J AESTHET ART CRITIC, V34, P86, DOI 10.2307/428656
   Chamorro-Premuzic T, 2009, BRIT J PSYCHOL, V100, P501, DOI 10.1348/000712608X366867
   CHIPMAN SF, 1977, J EXP PSYCHOL GEN, V106, P269, DOI 10.1037/0096-3445.106.3.269
   Cleridou K, 2014, EMPIR STUD ARTS, V32, P231, DOI 10.2190/EM.32.2.f
   Cooper PA, 2008, PERCEPTION, V37, P1216, DOI 10.1068/p5865
   Di Dio C, 2007, PLOS ONE, V2, DOI 10.1371/journal.pone.0001201
   Eskine KJ, 2012, EMOTION, V12, P1071, DOI 10.1037/a0027200
   EYSENCK HJ, 1968, J GEN PSYCHOL, V79, P3, DOI 10.1080/00221309.1968.9710447
   Fan ZB, 2022, COMPUT J, V65, P1964, DOI 10.1093/comjnl/bxab035
   Faul F, 2009, BEHAV RES METHODS, V41, P1149, DOI 10.3758/BRM.41.4.1149
   Forsythe A, 2011, BRIT J PSYCHOL, V102, P49, DOI 10.1348/000712610X498958
   Friedenberg J, 2016, ACTA PSYCHOL, V168, P41, DOI 10.1016/j.actpsy.2016.04.007
   Germine L, 2015, CURR BIOL, V25, P2684, DOI 10.1016/j.cub.2015.08.048
   GOMBRICH Ernst., 1992, The Sense of Order: A Study in the Psychology of Decorative Art
   Güçlütürk Y, 2016, FRONT HUM NEUROSCI, V10, DOI 10.3389/fnhum.2016.00112
   HOCHBERG JE, 1957, PSYCHOL REV, V64, P73, DOI 10.1037/h0043738
   Jin K., 2023, P 16 INT S VIS INF C, V21, P1
   Krpan D, 2022, PSYCHOL AESTHET CREA, DOI 10.1037/aca0000511
   Leder H, 2004, BRIT J PSYCHOL, V95, P489, DOI 10.1348/0007126042369811
   LEEUWENBERG EL, 1969, PSYCHOL REV, V76, P216, DOI 10.1037/h0027285
   Little AC, 2015, BRIT J PSYCHOL, V106, P397, DOI 10.1111/bjop.12098
   Luck SJ, 2013, TRENDS COGN SCI, V17, P391, DOI 10.1016/j.tics.2013.06.006
   Marin MM, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0072412
   MARTINDALE C, 1990, AM J PSYCHOL, V103, P53, DOI 10.2307/1423259
   MARTINDALE C, 1988, J EXP PSYCHOL HUMAN, V14, P661, DOI 10.1037/0096-1523.14.4.661
   Masuda T, 2008, PERS SOC PSYCHOL B, V34, P1260, DOI 10.1177/0146167208320555
   Mather George, 2020, Vision (Basel), V4, DOI 10.3390/vision4010010
   Mather G, 2018, ART PERCEPT, V6, P97, DOI 10.1163/22134913-20181092
   Mather G, 2014, ART PERCEPT, V2, P11, DOI 10.1163/22134913-00002018
   Michailidou E, 2008, SIGDOC'08: PROCEEDINGS OF THE 26TH ACM INTERNATIONAL CONFERENCE ON DESIGN OF COMMUNICATION, P215
   Miller CA, 2020, PSYCHOL AESTHET CREA, V14, P237, DOI 10.1037/aca0000241
   National Commission for the Protection of Human Subjects of Biomedical and Behavioral Research, 1979, The Belmont report: Ethical principles and guidelines for the protection of human subjects of research
   Palmer SE, 2013, PSYCHON B REV, V20, P453, DOI 10.3758/s13423-012-0355-2
   Palmer SE, 2013, ANNU REV PSYCHOL, V64, P77, DOI 10.1146/annurev-psych-120710-100504
   Palmer SE, 2010, P NATL ACAD SCI USA, V107, P8877, DOI 10.1073/pnas.0906172107
   Reber R, 2004, PERS SOC PSYCHOL REV, V8, P364, DOI 10.1207/s15327957pspr0804_3
   Reber R., 2012, Processing Fluency, Aesthetic Pleasure, and Culturally Shared Taste
   Schloss KB, 2011, ATTEN PERCEPT PSYCHO, V73, P551, DOI 10.3758/s13414-010-0027-0
   Sherman A, 2015, J EXP PSYCHOL HUMAN, V41, P898, DOI 10.1037/a0039314
   Spehar B, 2003, COMPUT GRAPH-UK, V27, P813, DOI 10.1016/S0097-8493(03)00154-7
   SPERLING G, 1960, PSYCHOL MONOGR, V74, P1, DOI 10.1037/h0093759
   Tatarkiewicz W., 2005, History of Aesthetics: Edited by J. Harrell
   Teng CY, 2019, NAT HUM BEHAV, V3, P827, DOI 10.1038/s41562-019-0640-4
   Vogel EK, 2001, J EXP PSYCHOL HUMAN, V27, P92, DOI 10.1037/0096-1523.27.1.92
   ZAJONC RB, 1968, J PERS SOC PSYCHOL, V9, P1, DOI 10.1037/h0025848
   Zhang K, 2012, LEONARDO, V45, P243, DOI 10.1162/LEON_a_00366
   Zhang WW, 2008, NATURE, V453, P233, DOI 10.1038/nature06860
NR 51
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 3
PY 2024
DI 10.1007/s00371-024-03405-3
EA MAY 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PN0Z5
UT WOS:001214652000004
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Zhu, HW
   Bai, SQ
   Shi, JL
   Wang, CG
   Sun, YH
   Lu, JW
   Shu, X
   Huang, SC
AF Zhu, Haowei
   Bai, Suqin
   Shi, Jinlong
   Wang, Chenggen
   Sun, Yunhan
   Lu, Jiawen
   Shu, Xin
   Huang, Shucheng
TI IOFusion: instance segmentation and optical-flow guided 3D
   reconstruction in dynamic scenes
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE 3D reconstruction; Instance segmentation; Optical flow; Dynamic scene
ID VISUAL SLAM; ENVIRONMENTS; TRACKING
AB To improve the accuracy of camera pose estimation for RGBD-based 3D reconstruction in dynamic scenes, a method based on instance segmentation and optical flow is proposed. Firstly, instance segmentation is used to detect objects, and a semantic map is constructed by removing non-rigid objects. Secondly, motion residual is calculated by optical flow and camera flow to detect dynamic rigid objects, and nonlinear optimization is used to track the dynamic rigid objects extracted from the semantic map. Thirdly, after removing features of non-rigid objects and dynamic rigid ones in each frame, the remaining features are used to optimize the camera pose. Finally, a TSDF model is used to reconstruct the static background, and point clouds are used to reconstruct dynamic rigid objects. Experiments on TUM and Bonn datasets show that the method produces better camera poses than the current state-of-the-art methods in most dynamic scenes. Ablation experiments on Bonn dataset show that retaining features of static rigid objects significantly improves camera pose estimation precision. The annotated datasets and the source code are available at https://github.com/CodingMaplee/IOFusion/tree/main.
C1 [Zhu, Haowei; Bai, Suqin; Shi, Jinlong; Wang, Chenggen; Lu, Jiawen; Shu, Xin; Huang, Shucheng] Jiangsu Univ Sci & Technol, Sch Comp Sci, Changhui Rd, Zhenjiang 212114, Jiangsu, Peoples R China.
   [Bai, Suqin] Civil Aviat Univ China, Civil Aviat Smart Airport Theory & Key Lab, Xinli Rd, Tianjin 300300, Peoples R China.
   [Sun, Yunhan] Nanjing Univ, State Key Lab Novel Software Technol, HuNan Rd, Nanjing 210008, Jiangsu, Peoples R China.
C3 Jiangsu University of Science & Technology; Civil Aviation University of
   China; Nanjing University
RP Bai, SQ (corresponding author), Jiangsu Univ Sci & Technol, Sch Comp Sci, Changhui Rd, Zhenjiang 212114, Jiangsu, Peoples R China.; Bai, SQ (corresponding author), Civil Aviat Univ China, Civil Aviat Smart Airport Theory & Key Lab, Xinli Rd, Tianjin 300300, Peoples R China.
EM zhu_hao_wei@163.com; jsjxy_bsq@just.edu.cn; shi_jinlong@163.com;
   chenggenwang98@gmail.com; sunyh@smail.nju.edu.cn;
   ljw17368503991@163.com; shuxin@just.edu.cn; schuang6@126.com
OI Zhu, Haowei/0000-0003-0229-2935
FU Civil Aviation University of China; Civil Aviation Smart Airport Theory
   and Key Laboratory, Civil Aviation University of China [62276118];
   National Natural Science Foundation of China
FX This work is supported by the Civil Aviation Smart Airport Theory and
   Key Laboratory, Civil Aviation University of China (No.SATS202207) and
   the National Natural Science Foundation of China (No.62276118).
CR Alcantarilla PF, 2012, IEEE INT CONF ROBOT, P1290, DOI 10.1109/ICRA.2012.6224690
   Bescos B, 2018, IEEE ROBOT AUTOM LET, V3, P4076, DOI 10.1109/LRA.2018.2860039
   Bujanca M., 2019, P IEEECVF INT C COMP
   Bujanca M, 2022, IEEE INT C INT ROBOT, P11063, DOI 10.1109/IROS47612.2022.9981591
   BUSSARD RW, 1991, FUSION TECHNOL, V19, P273, DOI 10.13182/FST91-A29364
   Campos C, 2021, IEEE T ROBOT, V37, P1874, DOI 10.1109/TRO.2021.3075644
   Canziani A., 2016, arXiv, DOI DOI 10.48550/ARXIV.1605.07678
   Cheng JY, 2019, ADV ROBOTICS, V33, P576, DOI 10.1080/01691864.2019.1610060
   Cho KYHY, 2014, Arxiv, DOI arXiv:1409.1259
   Collaboration A., 2008, Jinst
   Dai A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3054739
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Gao W, 2019, Arxiv, DOI arXiv:1904.13073
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Grisetti Giorgio, 2011, P IEEE INT C ROB AUT, P9, DOI DOI 10.1109/ICRA.2011.5979949
   Gu JX, 2018, PATTERN RECOGN, V77, P354, DOI 10.1016/j.patcog.2017.10.013
   Guo CZ, 2022, INT CONF ACOUST SPEE, P3818, DOI 10.1109/ICASSP43922.2022.9746993
   Hachiuma R, 2019, Arxiv, DOI arXiv:1907.09127
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Izadi S., 2011, P 24 ANN ACM S USER, P559
   Keller M, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P1, DOI 10.1109/3DV.2013.9
   Li W, 2019, IEEE WINT CONF APPL, P1413, DOI 10.1109/WACV.2019.00155
   Likas A, 2003, PATTERN RECOGN, V36, P451, DOI 10.1016/S0031-3203(02)00060-2
   Lin WB, 2022, PROC CVPR IEEE, P1726, DOI 10.1109/CVPR52688.2022.00178
   Liu X., 2022, Vis. Comput., P1
   Liu YB, 2021, IEEE ACCESS, V9, P106981, DOI 10.1109/ACCESS.2021.3100426
   Lorensen H. E., 1987, Proc. SIGGRAPH, V21, P163, DOI 10.1145/37401.37422
   More J. J., 1978, Proceedings of the Biennial Conference on numerical analysis, P105
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Niessner M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508374
   Ozkan M, 2022, VISUAL COMPUT, V38, P3953, DOI 10.1007/s00371-021-02241-z
   Palazzolo E, 2019, IEEE INT C INT ROBOT, P7855, DOI [10.1109/IROS40897.2019.8967590, 10.1109/iros40897.2019.8967590]
   Redmon J., 2018, CoRR
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rosinol A, 2020, IEEE INT CONF ROBOT, P1689, DOI [10.1109/ICRA40945.2020.9196885, 10.1109/icra40945.2020.9196885]
   Runz Martin, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4471, DOI 10.1109/ICRA.2017.7989518
   Rünz M, 2018, INT SYM MIX AUGMENT, P10, DOI 10.1109/ISMAR.2018.00024
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Schöps T, 2020, IEEE T PATTERN ANAL, V42, P2494, DOI 10.1109/TPAMI.2019.2947048
   Scona R, 2018, IEEE INT CONF ROBOT, P3849, DOI 10.1109/ICRA.2018.8460681
   Strecke M, 2019, IEEE I CONF COMP VIS, P5864, DOI 10.1109/ICCV.2019.00596
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Teed Zachary, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P402, DOI 10.1007/978-3-030-58536-5_24
   Wang KX, 2019, IEEE INT CONF ROBOT, P6919, DOI [10.1109/ICRA.2019.8794101, 10.1109/icra.2019.8794101]
   Wang KK, 2021, VISUAL COMPUT, V37, P603, DOI 10.1007/s00371-020-01826-4
   Wang X., 2020, INT C NEURAL INF PRO, V33, P17721
   Whelan T, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI
   Whelan T, 2016, INT J ROBOT RES, V35, P1697, DOI 10.1177/0278364916669237
   Wong YS, 2021, COMPUT GRAPH FORUM, V40, P511, DOI 10.1111/cgf.142651
   Wu WX, 2022, NEURAL COMPUT APPL, V34, P6011, DOI 10.1007/s00521-021-06764-3
   Xinlong Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P649, DOI 10.1007/978-3-030-58523-5_38
   Xu BB, 2019, IEEE INT CONF ROBOT, P5231, DOI [10.1109/icra.2019.8794371, 10.1109/ICRA.2019.8794371]
   Yan ZX, 2017, IEEE T VIS COMPUT GR, V23, P2389, DOI 10.1109/TVCG.2017.2734458
   Zhou J, 2020, AI OPEN, V1, P57, DOI 10.1016/j.aiopen.2021.01.001
NR 59
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 26
PY 2024
DI 10.1007/s00371-024-03365-8
EA APR 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ1J9
UT WOS:001208646400002
DA 2024-08-05
ER

PT J
AU Liao, Y
   Liu, PY
   Wu, XN
   Pan, ZX
   Zhu, KJ
   Zhou, H
   Liu, JH
   Duan, Q
AF Liao, Yun
   Liu, Peiyu
   Wu, Xuning
   Pan, Zhixuan
   Zhu, Kaijun
   Zhou, Hao
   Liu, Junhui
   Duan, Qing
TI Using scale-equivariant CNN to enhance scale robustness in feature
   matching
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image matching; Feature matching; Scale-equivariance; Transformer
AB Image matching is an important task in computer vision. The detector-free dense matching method is an important research direction of image matching due to its high accuracy and robustness. The classical detector-free image matching methods utilize convolutional neural networks to extract features and then match them. Due to the lack of scale equivariance in CNNs, this method often exhibits poor matching performance when the images to be matched undergo significant scale variations. However, large-scale variations are very common in practical problems. To solve the above problem, we propose SeLFM, a method that combines scale equivariance and the global modeling capability of transformer. The two main advantages of this method are scale-equivariant CNNs can extract scale-equivariant features, while transformer also brings global modeling capability. Experiments prove that this modification improves the performance of the matcher in matching image pairs with large-scale variations and does not affect the general matching performance of the matcher. The code will be open-sourced at this link: https://github.com/LiaoYun0x0/SeLFM/tree/main
C1 [Liao, Yun; Liu, Peiyu; Wu, Xuning; Pan, Zhixuan; Liu, Junhui; Duan, Qing] Yunnan Univ, Natl Pilot Sch Software, Kunming 650106, Yunnan, Peoples R China.
   [Liao, Yun; Zhu, Kaijun; Zhou, Hao] Yunnan Lanyi Network Technol Co, Kunming 650000, Yunnan, Peoples R China.
   [Liao, Yun; Liu, Peiyu; Wu, Xuning; Pan, Zhixuan; Liu, Junhui; Duan, Qing] Yunnan Key Lab Software Engn, Kunming, Yunnan, Peoples R China.
C3 Yunnan University
RP Duan, Q (corresponding author), Yunnan Univ, Natl Pilot Sch Software, Kunming 650106, Yunnan, Peoples R China.; Duan, Q (corresponding author), Yunnan Key Lab Software Engn, Kunming, Yunnan, Peoples R China.
EM 676295641@qq.com; 842845047@qq.com; 414288364@qq.com; yilk123@qq.com;
   xunfeng.zkj@gmail.com; 1083480050@qq.com; hanks@ynu.edu.cn;
   qduan@ynu.edu.cn
FU Open Foundation of Yunnan Key Laboratory of Software Engineering
   [2020SE307]; Scientific Research Foundation of Education Department of
   Yunnan Province [2021J0007]
FX This work was partially supported by the Open Foundation of Yunnan Key
   Laboratory of Software Engineering under Grant No.2020SE307, and
   Scientific Research Foundation of Education Department of Yunnan
   Province under Grant No. 2021J0007.
CR Ai LM, 2024, VISUAL COMPUT, V40, P1453, DOI 10.1007/s00371-023-02860-8
   Balntas V, 2020, IEEE T PATTERN ANAL, V42, P2825, DOI 10.1109/TPAMI.2019.2915233
   Bökman G, 2022, IEEE COMPUT SOC CONF, P5106, DOI 10.1109/CVPRW56347.2022.00559
   Carion N., 2020, EUROPEAN C COMPUTER
   Chen H, 2021, APPL OPTICS, V60, P6264, DOI 10.1364/AO.424280
   Chen HK, 2022, LECT NOTES COMPUT SC, V13692, P20, DOI 10.1007/978-3-031-19824-3_2
   Chen LF, 2023, VISUAL COMPUT, V39, P5229, DOI 10.1007/s00371-022-02656-2
   Chen Y, 2022, AAAI CONF ARTIF INTE, P365
   Chen Z., 2021, MM 21 ACM MULTIMEDIA
   DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060
   Dosovitskiy A., 2021, ICLR
   Dusmanu M, 2019, PROC CVPR IEEE, P8084, DOI 10.1109/CVPR.2019.00828
   Engel J, 2018, IEEE T PATTERN ANAL, V40, P611, DOI 10.1109/TPAMI.2017.2658577
   Ghosh R, 2019, Arxiv, DOI arXiv:1906.03861
   Giang KT, 2023, AAAI CONF ARTIF INTE, P2447
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Jiang W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6187, DOI 10.1109/ICCV48922.2021.00615
   Kanazawa A, 2014, Arxiv, DOI arXiv:1412.5104
   Kondor Risi, 2018, INT C MACHINE LEARNI, P2747
   Li K., 2022, arXiv
   Li N, 2022, VISUAL COMPUT, V38, P2091, DOI 10.1007/s00371-021-02270-8
   Li SW, 2015, IEEE I CONF COMP VIS, P4283, DOI 10.1109/ICCV.2015.487
   Li X., 2020, Advances in Neural Information Processing Systems.
   Li ZQ, 2018, PROC CVPR IEEE, P2041, DOI 10.1109/CVPR.2018.00218
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Lindenberger P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5967, DOI 10.1109/ICCV48922.2021.00593
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo ZX, 2020, PROC CVPR IEEE, P6588, DOI 10.1109/CVPR42600.2020.00662
   Marcos D, 2018, Arxiv, DOI arXiv:1807.11783
   Mishchuk A, 2018, Arxiv, DOI arXiv:1705.10872
   Noh H, 2017, IEEE I CONF COMP VIS, P3476, DOI 10.1109/ICCV.2017.374
   Revaud J, 2019, Arxiv, DOI [arXiv:1906.06195, DOI 10.48550/ARXIV.1906.06195]
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sarlin PE, 2019, Arxiv, DOI arXiv:1812.03506
   Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499
   Sarlin PE, 2019, PROC CVPR IEEE, P12708, DOI 10.1109/CVPR.2019.01300
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Shen ZW, 2024, VISUAL COMPUT, V40, P1327, DOI 10.1007/s00371-023-02851-9
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sosnovik I., 2020, 8 INT C LEARNING REP
   Sun JM, 2021, PROC CVPR IEEE, P8918, DOI 10.1109/CVPR46437.2021.00881
   Sun LB, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2187
   Taira H, 2021, IEEE T PATTERN ANAL, V43, P1293, DOI 10.1109/TPAMI.2019.2952114
   Taira H, 2018, PROC CVPR IEEE, P7199, DOI 10.1109/CVPR.2018.00752
   Tang S., 2022, International Conference on Learning Representations
   Tian YR, 2020, Arxiv, DOI arXiv:2006.10202
   Tian YR, 2020, Arxiv, DOI arXiv:2005.13605
   Tian YR, 2019, PROC CVPR IEEE, P11008, DOI 10.1109/CVPR.2019.01127
   Tyszkiewicz M., 2020, ADV NEURAL INFORM PR
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang CW, 2022, AAAI CONF ARTIF INTE, P2388
   Wang QQ, 2024, Arxiv, DOI arXiv:2004.13324
   Wang Q, 2023, LECT NOTES COMPUT SC, V13843, P256, DOI 10.1007/978-3-031-26313-2_16
   Worrall DE, 2019, C NEURAL INFORM PROC, P7364
   Wu YH, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2078
   Xie E., 2021, ADV NEURAL INF PROCE, V34, P12077
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Xu YC, 2014, Arxiv, DOI arXiv:1411.6369
   Yoon S, 2021, IEEE ROBOT AUTOM LET, V6, P8726, DOI 10.1109/LRA.2021.3111760
   Zhang ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4086, DOI 10.1109/ICCV48922.2021.00407
   Zhou LF, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2088
   Zhou QJ, 2021, PROC CVPR IEEE, P4667, DOI 10.1109/CVPR46437.2021.00464
NR 63
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 25
PY 2024
DI 10.1007/s00371-024-03389-0
EA APR 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OO3N4
UT WOS:001208177700002
DA 2024-08-05
ER

PT J
AU Lubna, A
   Kalady, S
   Lijiya, A
AF Lubna, A.
   Kalady, Saidalavi
   Lijiya, A.
TI Visual question answering on blood smear images using convolutional
   block attention module powered object detection
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Visual question answering; Automated blood cell counting; Deep learning;
   Object detection; Attention mechanism; Disease detection
AB One of the vital characteristics that determine the health condition of a person is the shape and number of the red blood cells, white blood cells and platelets present in one's blood. Any abnormality in these characteristics is an indication of the person suffering from diseases like anaemia, leukaemia or thrombocytosis. The counting of the blood cell is conventionally made by means of microscopic studies with the application of suitable chemical substances in the blood. The conventional methods pose challenges in the analysis in terms of manual labour and are time-consuming and costly tasks requiring highly skilled medical professionals. This paper proposes a novel scheme to analyse the blood sample of an individual by employing a visual question answering (VQA) system, which accepts a blood smear image as input and answers questions pertaining to the sample, viz. amount of blood cells, nature of abnormalities, etc. very quickly without requiring the service of a skilled medical professional. In VQA, the computer generates textual answers to questions about an input image. Solving this difficult problem requires visual understanding, question comprehension and deductive reasoning. The proposed approach exploits a convolutional neural network for question categorisation and an object detector with an attention mechanism for visual comprehension. The experiment has been conducted with two types of attention: (1) convolutional block attention module and (2) squeeze-and-excitation network which facilitates very fast and reliable results. A VQA dataset has been created for this study due to the unavailability of a public dataset, and the proposed system exhibited an accuracy of 94% for numeric response questions/yes or no type questions and has a BLEU score of 0.91. It is also observed that the attention-based object recognition model of the proposed system for counting the blood characteristics has an accuracy of 97%, 100% and 98% for red blood cell count, white blood cell count and platelet count, respectively, which is an improvement of 1%, 0.06% and 1.61% as compared to the state-of-the-art model.
C1 [Lubna, A.; Kalady, Saidalavi; Lijiya, A.] Natl Inst Technol Calicut, Comp Sci & Engn Dept, Kozhikode 673601, Kerala, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Calicut
RP Lubna, A (corresponding author), Natl Inst Technol Calicut, Comp Sci & Engn Dept, Kozhikode 673601, Kerala, India.
EM lubna_p170100cs@nitc.ac.in; said@nitc.ac.in; lijiya@nitc.ac.in
RI A, lubna/KYD-5178-2024
OI A, lubna/0000-0003-2181-4671
CR Acevedo A, 2020, DATA BRIEF, V30, DOI 10.1016/j.dib.2020.105474
   Alam MM, 2019, HEALTHC TECHNOL LETT, V6, P103, DOI 10.1049/htl.2018.5098
   Alomari YM, 2014, COMPUT MATH METHOD M, V2014, DOI 10.1155/2014/979302
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2017, Shenggan: BCCD dataset
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Borji A, 2019, COMPUT VIS MEDIA, V5, P117, DOI 10.1007/s41095-019-0149-9
   Chappuis C, 2022, IEEE COMPUT SOC CONF, P1371, DOI 10.1109/CVPRW56347.2022.00143
   Chaudhary AH, 2019, EAI SPRINGER INNOVAT, P87, DOI 10.1007/978-3-319-96139-2_9
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dvanesh V.D., 2018, 2018 INT C CURRENT T, P1
   Felzenszwalb P., 2008, 2008 IEEE C COMP VIS, DOI [DOI 10.1109/CVPR.2008.4587597, 10.1109/CVPR.2008.4587597]
   Gasmi K, 2022, J SUPERCOMPUT, V78, P15042, DOI 10.1007/s11227-022-04474-8
   Gasmi K, 2022, CYBERNET SYST, V53, P403, DOI 10.1080/01969722.2021.2018543
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   Guo ZH, 2023, VISUAL COMPUT, V39, P5783, DOI 10.1007/s00371-022-02695-9
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hosseinabad SH, 2021, VISUAL COMPUT, V37, P119, DOI 10.1007/s00371-019-01786-4
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Inchur VB, 2020, 2020 5TH IEEE INTERNATIONAL CONFERENCE ON RECENT TRENDS ON ELECTRONICS, INFORMATION, COMMUNICATION & TECHNOLOGY (RTEICT-2020), P21, DOI 10.1109/RTEICT49044.2020.9315603
   Jiang ZF, 2021, OSA CONTINUUM, V4, P323, DOI 10.1364/OSAC.413787
   Jocher G., 2020, etal.: Yolov5
   Khan AU, 2021, PROC CVPR IEEE, P8461, DOI 10.1109/CVPR46437.2021.00836
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Liu SY, 2022, BMC MED IMAGING, V22, DOI 10.1186/s12880-022-00800-x
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Maitra M., 2012, Int. J. Comput. Appl, V53, P13, DOI DOI 10.5120/8505-2274
   Malinowski M, 2015, Arxiv, DOI arXiv:1505.01121
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Patgiri C, 2021, BIOMED SIGNAL PROCES, V68, DOI 10.1016/j.bspc.2021.102745
   Redmon J., 2018, CoRR
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shao XJ, 2022, IET IMAGE PROCESS, V16, P1424, DOI 10.1049/ipr2.12421
   Teney D, 2017, IEEE SIGNAL PROC MAG, V34, P63, DOI 10.1109/MSP.2017.2739826
   Trott A., 2018, C TRACK P
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang W., 2020, P IEEE CVF C COMP VI, P12695, DOI DOI 10.1109/CVPR42600.2020.01271
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu JJ, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108214
   Wu Q., 2022, Advances in Computer Vision and Pattern Recognition, DOI [10.1007/978-981-19-0964-1_12, DOI 10.1007/978-981-19-0964-1_12]
   Yan F, 2022, VISUAL COMPUT, V38, P3097, DOI 10.1007/s00371-022-02524-z
   Yuan ZH, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3173811
   Zhan HY, 2022, NEUROCOMPUTING, V467, P323, DOI 10.1016/j.neucom.2021.10.016
   Zhang Y., 2018, 6 INT C LEARNING REP
NR 47
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 9
PY 2024
DI 10.1007/s00371-024-03359-6
EA APR 2024
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NG1R9
UT WOS:001199213300003
DA 2024-08-05
ER

PT J
AU Guo, XL
   Wei, MQ
AF Guo, Xianglin
   Wei, Mingqiang
TI Shape generation via learning an adaptive multimodal prior
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Shape generation; Variational mixture of posteriors prior; GAN;
   Diversity
AB Significant interest and progress have been drawn to the recent advancements in image creation using deep generative model, but the field of automatic three-dimensional shape creation is largely under-developed and inspires a great deal of research activity across a wide variety of disciplines. We add a new kind of previously named variational mixture of posteriors into the adversarial network using geometric data described as volumetric grids. Our main contribution is the introduction of a new type of prior called variational mixture of posteriors prior into the adversarial network, dubbed VampPrior-3DGAN, in a mathematic principled way. Specifically, we leverage an encoder as a regularizer to penalize missing modes, while introduce a variational mixture of posterior prior as the latent variable distribution of GAN to dynamically and adaptively update its prior distribution. The key intuition behind this architecture is that the latent variables should retain information about the data to minimize the undue impact of the prior assumptions. This seemingly simple modification to the GAN framework is surprisingly effective and results in models which enable diversity in generated samples, although trained with limited data. Realistic 3D objects can be easily generated by sampling the VampPrior-3DGAN's latent probabilistic manifold. For validation, we apply our method on tasks from the fields of three-dimensional volumetric generation, reconstruction from a single RGB image and partial shape completion from a single perspective view, and show that it is on par with or outperforms the state-of-the-art approaches, both quantitatively and qualitatively.
C1 [Guo, Xianglin] Anhui Univ Technol, Coll Comp Sci & Technol, Maanshan 243032, Anhui, Peoples R China.
   [Guo, Xianglin; Wei, Mingqiang] Nanjing Univ Aeronaut & Astronaut, Shenzhen Res Inst, Shenzhen 518038, Guangdong, Peoples R China.
C3 Anhui University of Technology; Nanjing University of Aeronautics &
   Astronautics
RP Wei, MQ (corresponding author), Nanjing Univ Aeronaut & Astronaut, Shenzhen Res Inst, Shenzhen 518038, Guangdong, Peoples R China.
EM x.guo@ahut.edu.cn; mqwei@nuaa.edu.cn
FU Key Project Fund of College Research Program of Anhui Provincial
   Department of Education [2023AH051129]; Shenzhen Science and Technology
   Program [JCYJ20220818103401003, JCYJ20220530172403007]; Guangdong Basic
   and Applied Basic Research Foundation [2022A1515010170]
FX This work was supported by the Key Project Fund of College Research
   Program of Anhui Provincial Department of Education (No. 2023AH051129),
   the Shenzhen Science and Technology Program (No. JCYJ20220818103401003,
   No. JCYJ20220530172403007) and the Guangdong Basic and Applied Basic
   Research Foundation (No. 2022A1515010170).
CR Ben-Yosef M., 2018, arXiv
   Black M.J., 2023, 11 INT C LEARNING RE, P1
   Brock A., 2016, Proc. ICLR
   Chen JK, 2023, PROC CVPR IEEE, P12439, DOI 10.1109/CVPR52729.2023.01197
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Deprelle Theo, 2019, Advances in Neural Information Processing Systems, P7435
   Dilokthanakul Nat, 2016, arXiv, DOI DOI 10.48550/ARXIV.1611.02648
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Gao J., 2022, ADV NEURAL INFORM PR, P31841
   Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Gu SY, 2020, Arxiv, DOI arXiv:2006.16990
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Huang SY, 2023, Arxiv, DOI arXiv:2305.01643
   Insafutdinov E, 2018, ADV NEUR IN, V31
   Kar A, 2015, PROC CVPR IEEE, P1966, DOI 10.1109/CVPR.2015.7298807
   King DB, 2015, ACS SYM SER, V1214, P1
   Kingma D. P., 2014, arXiv
   Li W., 2016, INT C LEARNING REPRE, P1
   Lim KL, 2020, IEEE SIGNAL PROC LET, V27, P231, DOI 10.1109/LSP.2020.2965328
   Lin CH, 2023, PROC CVPR IEEE, P300, DOI 10.1109/CVPR52729.2023.00037
   Luo ST, 2021, PROC CVPR IEEE, P2836, DOI 10.1109/CVPR46437.2021.00286
   Lyu Zhaoyang, 2021, INT C LEARNING REPRE
   Mandikal P, 2019, IEEE WINT CONF APPL, P1052, DOI 10.1109/WACV.2019.00117
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Michalkiewicz M, 2019, Arxiv, DOI arXiv:1901.06802
   Mildenhall Ben, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P405, DOI 10.1007/978-3-030-58452-8_24
   Mittal P, 2022, PROC CVPR IEEE, P306, DOI 10.1109/CVPR52688.2022.00040
   Nichol A, 2022, Arxiv, DOI arXiv:2212.08751
   Pan L, 2021, PROC CVPR IEEE, P8520, DOI 10.1109/CVPR46437.2021.00842
   Pan L, 2020, IEEE ROBOT AUTOM LET, V5, P4392, DOI 10.1109/LRA.2020.2994483
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Poole B., 2022, 11 INT C LEARNING RE, P1
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Ramesh P, 2022, 38 INT C MACHINE LEA
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Saharia C., 2022, ADV NEURAL INF PROCE, V35, P36479
   Sanghi A, 2022, Arxiv, DOI arXiv:2211.01427
   Tang JS, 2023, Arxiv, DOI arXiv:2303.14184
   Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230
   Tchapmi LP, 2019, PROC CVPR IEEE, P383, DOI 10.1109/CVPR.2019.00047
   Tomczak JM, 2018, PR MACH LEARN RES, V84
   Trevithick A, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592460
   Wang Y, 2018, IEEE J-STSP, V12, P1615, DOI 10.1109/JSTSP.2018.2876995
   Wu JJ, 2016, ADV NEUR IN, V29
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Chang AX, 2015, Arxiv, DOI [arXiv:1512.03012, DOI 10.48550/ARXIV.1512.03012]
   Xiang Y, 2014, IEEE WINT CONF APPL, P75, DOI 10.1109/WACV.2014.6836101
   Xiao JX, 2016, INT J COMPUT VISION, V119, P3, DOI 10.1007/s11263-014-0748-y
   Xie H., 2020, EUROPEAN C COMPUTER, P365, DOI DOI 10.1007/978-3-030-58545-721
   Xie HZ, 2019, IEEE I CONF COMP VIS, P2690, DOI 10.1109/ICCV.2019.00278
   Xueting Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P677, DOI 10.1007/978-3-030-58568-6_40
   Yu XM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12478, DOI 10.1109/ICCV48922.2021.01227
   Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088
   Zhou LQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5806, DOI 10.1109/ICCV48922.2021.00577
NR 61
TC 1
Z9 1
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 20
PY 2024
DI 10.1007/s00371-024-03303-8
EA MAR 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LX3V2
UT WOS:001190081000003
DA 2024-08-05
ER

PT J
AU Mulawade, RN
   Garth, C
   Wiebel, A
AF Mulawade, Raju Ningappa
   Garth, Christoph
   Wiebel, Alexander
CA Bergen pCT Collaboration
TI Visual analytics system for understanding DeepRL-based charged particle
   tracking
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Machine learning; Particle physics; Explainability; High-dimensional
   data; Visual analytics
AB In this work, we propose a visual analytics system to analyze deep reinforcement learning (deepRL) models working on the track reconstruction of charged particles in the field of particle physics. The data of these charged particles are in the form of point clouds with high-dimensional features. We use one of the existing post hoc saliency methods of explainable artificial intelligence (XAI) and extend its adaptation to compute saliency attributions for the input data corresponding to the output of the model. Our proposed system helps users to explore these saliency attributions corresponding to the high-dimensional input data of the machine learning model and interpret the decision-making process of the model. In particular, we provide the users with multiple task-oriented components, different types of linked views and interactive tools to analyze the model. We explain how to use the system by outlining a typical user workflow and demonstrate the system's usefulness using several case studies which address specific analysis tasks.
C1 [Mulawade, Raju Ningappa; Wiebel, Alexander] Hsch Worms Univ Appl Sci, Dept Comp Sci, Erenburgerstr 19, D-67549 Worms, Rheinland Pfalz, Germany.
   [Garth, Christoph] RPTU Kaiserslautern Landau, Dept Comp Sci, Gottlieb Daimler Str, D-67663 Kaiserslautern, Rheinland Pfalz, Germany.
RP Mulawade, RN (corresponding author), Hsch Worms Univ Appl Sci, Dept Comp Sci, Erenburgerstr 19, D-67549 Worms, Rheinland Pfalz, Germany.
EM mulawade@hs-worms.de; garth@rptu.de; wiebel@hs-worms.de
OI Wiebel, Alexander/0000-0002-6583-3092; Mulawade, Raju
   Ningappa/0000-0002-0180-8517; Garth, Christoph/0000-0003-1669-8549
FU German federal state of Rhineland-Palatinate (Forschungskolleg SIVERT)
FX This work was partly supported by the German federal state of
   Rhineland-Palatinate (Forschungskolleg SIVERT). We thank all members of
   the Bergen pCT Collaboration for providing consulting and especially
   Tobias Kortus for providing the data and the model for analysis. Members
   of the Bergen pCT Collaboration: Johan Almea, Gergely Gabor
   Barnafoeldic, Rene Bartheld, Viatcheslav Borshchovl, Anthony van den
   Brinkd, Mamdouh Chaara, Viljar Eikelanda, Georgi Genova, Ola Gr &
   oslash;ttvika, Havard Helstrupe, Ralf Keidelk, Chinorat Kobdajm, Shruti
   Mehendalea, Ilker Merice, Odd Harald Odlanda,b, Gabor Pappg, Thomas
   Peitzmannd, Helge Egil Seime Pettersenb, Pierluigi Piersimonia, Attiq Ur
   Rehmana, Matthias Richterh, Dieter Roehricha, Andreas Tefre Samn &
   oslash;ya, Joao Secoi,j, Hesam Shafieea,f, Arnon Songmoolnaka,m, Jarle
   Rambo S & oslash;liea,f, Ganesh Tambavea, Ihor Tymchukl, Kjetil
   Ullalanda, Monika Varga-Kofaragoc, Lennart Volzi,j, Boris Wagnera,
   RenZheng Xiaoa,n, Shiming Yanga, Christoph Gartho, Nicolas R. Gaugerp,
   Steffen Wendzelk, Alexander Wiebelk, Tobias Kortusk, Alexander
   Schillingk, Raju Ningappa Mulawadek, Sebastian Zillienk, Max Aehlep,
   Viktor Leonhardto
CR Alme J, 2020, FRONT PHYS-LAUSANNE, V8, DOI 10.3389/fphy.2020.568243
   Archambault D., 2022, Machine Learning Methods in Visualisation for Big Data, DOI [10.2312/mlvis.20221069, DOI 10.2312/MLVIS.20221069]
   Biscarat C., 2021, EPJ WEB C, V251, P03047, DOI DOI 10.1051/EPJCONF/202125103047
   DeZoort G., 2021, Comput. Softw. Big Sci, V5, P26, DOI DOI 10.1007/S41781-021-00073-Z
   Elabd A, 2022, FRONT BIG DATA, V5, DOI 10.3389/fdata.2022.828666
   Eschbach R., 2022, Vision, Modeling, and Visualization, DOI [10.2312/vmv.20221210, DOI 10.2312/VMV.20221210]
   Espadoto M, 2021, IEEE T VIS COMPUT GR, V27, P2153, DOI 10.1109/TVCG.2019.2944182
   Farrell S, 2018, Arxiv, DOI arXiv:1810.06111
   Farrell S, 2017, EPJ WEB CONF, V150, DOI 10.1051/epjconf/201715000003
   Gupta A, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9206688
   Hohman F, 2020, IEEE T VIS COMPUT GR, V26, P1096, DOI 10.1109/TVCG.2019.2934659
   HoloViz, ABOUT US
   Johnson RP, 2018, REP PROG PHYS, V81, DOI 10.1088/1361-6633/aa8b1d
   Kortus T, 2023, IEEE T PATTERN ANAL, V45, P15820, DOI 10.1109/TPAMI.2023.3305027
   Kortus Tobias, 2022, Zenodo
   Lundberg SM, 2017, ADV NEUR IN, V30
   Mager M, 2016, NUCL INSTRUM METH A, V824, P434, DOI 10.1016/j.nima.2015.09.057
   Matrone F, 2022, IEEE J-STARS, V15, P6571, DOI 10.1109/JSTARS.2022.3195200
   Mulawade Raju Ningappa, 2024, Zenodo, DOI 10.5281/ZENODO.10491504
   Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778
   Rosynski M, 2020, Arxiv, DOI arXiv:2012.01281
   Schwegler M, 2023, ALGORITHMS, V16, DOI 10.3390/a16070316
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Smilkov D, 2017, Arxiv, DOI arXiv:1706.03825
   Sundararajan M, 2017, PR MACH LEARN RES, V70
   Tan H., 2022, P IEEECVF WINTER C A, P2239
   Springenberg JT, 2015, Arxiv, DOI [arXiv:1412.6806, 10.48550/arXiv.1412.6806]
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Xuan XW, 2022, IEEE T VIS COMPUT GR, V28, P2326, DOI 10.1109/TVCG.2022.3165347
   Yuan J, 2021, COMPUT VIS MEDIA, V7, P3, DOI 10.1007/s41095-020-0191-7
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang M, 2020, IEEE T MULTIMEDIA, V22, P1744, DOI 10.1109/TMM.2019.2963592
   Zheng TH, 2019, IEEE I CONF COMP VIS, P1598, DOI 10.1109/ICCV.2019.00168
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 34
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 18
PY 2024
DI 10.1007/s00371-024-03297-3
EA MAR 2024
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LJ9G4
UT WOS:001186543400004
OA hybrid
DA 2024-08-05
ER

PT J
AU Li, HB
   Ling, L
   Li, YQ
   Zhang, WM
AF Li, Haibin
   Ling, Li
   Li, Yaqian
   Zhang, Wenming
TI DFE-Net: detail feature extraction network for small object detection
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Small object; Deep learning; DFE-Net; Extended separable convolution;
   Detail feature enhancement module; Decouple head; SIoU; VisDrone2019;
   TT100K
AB The universal object detector has the problem of low detection accuracy and high missed detection rate when detecting small objects. In this paper, we propose a novel deep learning model called detail feature extraction network (DFE-Net) based on YOLOv5. Firstly, we remove C5 layer from the baseline backbone and introduce the detail feature enhancement module. In this module, we incorporate extended separable convolution to enable the model to effectively capture fine-grained information about small objects. Secondly, we propose a novel decoupled head based on attention. The mixed attention module assigns different weights to the classification and regression tasks in separate branches, effectively improving detection performance. Finally, the SIoU loss function is introduced to speed up the convergence of the model. The experimental results indicate that our proposed DFE-Net, when compared to the baseline, not only exhibits a significantly higher improvement in accuracy on the general small object dataset, but also reduces the parameter count by 57.7%. The mAP50\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$_{50}$$\end{document} and mAP50-95\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$_{50-95}$$\end{document} improve by 3.7% and 2.1% on the VisDrone2019 dataset, and by 5.9% and 5.1% on the TT100K dataset.
C1 [Li, Haibin; Ling, Li; Li, Yaqian; Zhang, Wenming] Yanshan Univ, Key Lab Ind Comp Control Engn Hebei Prov, Qinhuangdao 066004, Hebei, Peoples R China.
   [Li, Haibin; Ling, Li; Li, Yaqian; Zhang, Wenming] Yanshan Univ, Minist Educ, Intelligent Control Syst & Intelligent Equipment, Engn Res Ctr, Qinhuangdao 066004, Hebei, Peoples R China.
C3 Yanshan University; Yanshan University
RP Ling, L (corresponding author), Yanshan Univ, Key Lab Ind Comp Control Engn Hebei Prov, Qinhuangdao 066004, Hebei, Peoples R China.; Ling, L (corresponding author), Yanshan Univ, Minist Educ, Intelligent Control Syst & Intelligent Equipment, Engn Res Ctr, Qinhuangdao 066004, Hebei, Peoples R China.
EM lingli981119@163.com
FU Provincial Key Laboratory Performance Subsidy Project [22567612H];
   National Natural Science Foundation of China [62106214]
FX This work was supported by the Provincial Key Laboratory Performance
   Subsidy Project (22567612H) and the National Natural Science Foundation
   of China (62106214).
CR Bai YC, 2018, LECT NOTES COMPUT SC, V11217, P210, DOI 10.1007/978-3-030-01261-8_13
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chaoxu Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12592, DOI 10.1109/CVPR42600.2020.01261
   Cheng G, 2023, IEEE T PATTERN ANAL, V45, P13467, DOI 10.1109/TPAMI.2023.3290594
   Deng CF, 2022, IEEE T MULTIMEDIA, V24, P1968, DOI 10.1109/TMM.2021.3074273
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Gevorgyan Z, 2022, Arxiv, DOI arXiv:2205.12740
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Hong DF, 2023, REMOTE SENS ENVIRON, V299, DOI 10.1016/j.rse.2023.113856
   Hong DF, 2024, Arxiv, DOI [arXiv:2311.07113, DOI 10.48550/ARXIV.2311.07113]
   Hong DF, 2019, IEEE T IMAGE PROCESS, V28, P1923, DOI 10.1109/TIP.2018.2878958
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu PY, 2017, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2017.166
   Jocher Glenn, 2022, Zenodo, DOI 10.5281/ZENODO.3908559
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Leng JX, 2023, IEEE T CIRC SYST VID, V33, P1320, DOI 10.1109/TCSVT.2022.3210207
   Li C., 2023, IEEE Trans. Geosci. Remote Sens.
   Li CY, 2022, Arxiv, DOI [arXiv:2209.02976, DOI 10.48550/ARXIV.2209.02976]
   Li HC, 2018, Arxiv, DOI [arXiv:1805.10180, 10.48550/arXiv.1805.10180]
   Li JA, 2017, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2017.211
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ma MC, 2023, IEEE T IMAGE PROCESS, V32, P1026, DOI 10.1109/TIP.2022.3232209
   Noh J, 2019, IEEE I CONF COMP VIS, P9724, DOI 10.1109/ICCV.2019.00982
   Qi L, 2021, PROC CVPR IEEE, P14438, DOI 10.1109/CVPR46437.2021.01421
   Ran Q, 2021, IEEE J-STARS, V14, P5786, DOI 10.1109/JSTARS.2021.3079968
   Redmon J., 2018, CoRR
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Tan R., 2020, P IEEE CVF C COMP VI, DOI [10.1109/CVPR42600.2020.01079, DOI 10.1109/CVPR42600.2020.01079]
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang WG, 2019, PROC CVPR IEEE, P5961, DOI 10.1109/CVPR.2019.00612
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yang L., 2023, P IEEE CVF INT C COM, P19961
   Yang X, 2019, IEEE I CONF COMP VIS, P8231, DOI 10.1109/ICCV.2019.00832
   Yuan X., 2023, P IEEE CVF INT C COM, P6317
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang HY, 2021, PROC CVPR IEEE, P8510, DOI 10.1109/CVPR46437.2021.00841
   Zhang HY, 2018, Arxiv, DOI arXiv:1710.09412
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhu PF, 2022, IEEE T PATTERN ANAL, V44, P7380, DOI 10.1109/TPAMI.2021.3119563
   Zhu XZ, 2019, IEEE I CONF COMP VIS, P6687, DOI 10.1109/ICCV.2019.00679
   Zhu YC, 2023, PROC CVPR IEEE, P19723, DOI 10.1109/CVPR52729.2023.01889
   Zhu Z, 2016, PROC CVPR IEEE, P2110, DOI 10.1109/CVPR.2016.232
NR 50
TC 1
Z9 1
U1 25
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 15
PY 2024
DI 10.1007/s00371-024-03277-7
EA FEB 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HV1H4
UT WOS:001162185500001
DA 2024-08-05
ER

PT J
AU Zhou, JH
   Zhao, HW
   Sun, MS
AF Zhou, Jianhang
   Zhao, Hongwei
   Sun, Mingsi
TI SEHSNet: Stage Enhancement and Hierarchical Supervision Network for edge
   detection
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Edge detection; Hierarchical supervision; Multi-levels; Interlayer
   interaction
AB Edge detection is a challenging low-level vision task. After a long period of development, there are already edge detectors whose edge recognition performance exceeds that of humans. However, there are still three problems in the existing modern deep CNN edge detection methods, which can be summarized as low-level under chaos, high-level over suppression, and inter-layer without interaction. For the first problem, we propose a cascade structure of dilated branches with truncated gradient flow, called the low-level proofreading module. A high-level cognition module is used to excavate the global edge cues to solve the over-suppression problem of high level. Considering the limitations of deep supervision, we create a deep hierarchical supervision strategy to facilitate inter-layer communication. We combine all the components to form our network, Stage Enhancement and Hierarchical Supervision Network. It is from the perspective of the deep edge detector, synthesizing and optimizing the advantages of previous works, and creating a new architecture to accommodate edge detection tasks. Our network outperforms the SOTA algorithms on several metrics. For example, when only using BSDS500 dataset for training, we achieved the F-measure ODS of 0.820, breaking the previous best record of 0.808.
C1 [Zhou, Jianhang; Zhao, Hongwei; Sun, Mingsi] Jilin Univ, Coll Comp Sci & Technol, Changchun 130012, Jilin, Peoples R China.
   [Zhou, Jianhang; Zhao, Hongwei] Jilin Univ, Key Lab Symbol Computat & Knowledge Engn, Minist Educ, Changchun 130012, Peoples R China.
   [Sun, Mingsi] Jilin Agr Sci & Technol Coll, Coll Elect & Informat Engn, Jilin 132101, Jilin, Peoples R China.
C3 Jilin University; Jilin University
RP Zhao, HW (corresponding author), Jilin Univ, Coll Comp Sci & Technol, Changchun 130012, Jilin, Peoples R China.; Zhao, HW (corresponding author), Jilin Univ, Key Lab Symbol Computat & Knowledge Engn, Minist Educ, Changchun 130012, Peoples R China.
EM jhzhou22@mails.jlu.edu.cn; 1323360067@qq.com; sunms19@mails.jlu.edu.cn
FU Provincial Science and Technology Innovation Special Fund Project of
   Jilin Province; Natural Science Foundation of Jilin Province
   [20200201037JC]; Fundamental Research Funds for the Central
   Universities;  [20190302026GX]
FX The authors are grateful to the anonymous reviewers for their insightful
   comments which have certainly improved this paper. This research was
   funded by the Provincial Science and Technology Innovation Special Fund
   Project of Jilin Province, Grant Number 20190302026GX, Natural Science
   Foundation of Jilin Province, Grant Number 20200201037JC, and the
   Fundamental Research Funds for the Central Universities, JLU.
CR Aboutabit N, 2021, VISUAL COMPUT, V37, P1545, DOI 10.1007/s00371-020-01896-4
   Andersson M, 2011, VISUAL COMPUT, V27, P665, DOI 10.1007/s00371-011-0560-4
   [Anonymous], 2016, ICLR
   [Anonymous], 2012, Advances in Neural Information Processing Systems, DOI DOI 10.1634/THEONCOLOGIST.8-3-252
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bertasius G, 2015, IEEE I CONF COMP VIS, P504, DOI 10.1109/ICCV.2015.65
   Bertasius G, 2015, PROC CVPR IEEE, P4380, DOI 10.1109/CVPR.2015.7299067
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Cao YJ, 2021, IEEE T MULTIMEDIA, V23, P761, DOI 10.1109/TMM.2020.2987685
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Q, 2021, PROC CVPR IEEE, P13034, DOI 10.1109/CVPR46437.2021.01284
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng RX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P304, DOI 10.1145/3394171.3413750
   Deng RX, 2018, LECT NOTES COMPUT SC, V11210, P570, DOI 10.1007/978-3-030-01231-1_35
   Dollár P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Gaur D, 2023, VISUAL COMPUT, V39, P4293, DOI 10.1007/s00371-022-02591-2
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   Gupta S, 2013, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2013.79
   Hallman S, 2015, PROC CVPR IEEE, P1732, DOI 10.1109/CVPR.2015.7298782
   He JZ, 2019, PROC CVPR IEEE, P3823, DOI 10.1109/CVPR.2019.00395
   Hu Y, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P782
   Huan LX, 2022, IEEE T PATTERN ANAL, V44, P6602, DOI 10.1109/TPAMI.2021.3084197
   Hwang Jyh-Jing., 2015, ICLR
   Isola P, 2014, LECT NOTES COMPUT SC, V8691, P799, DOI 10.1007/978-3-319-10578-9_52
   Kittler J., 1983, Image and Vision Computing, V1, P37, DOI [DOI 10.1016/0262-8856(83)90006-9, 10.1016/0262-8856(83)90006-9]
   Konishi S, 2003, IEEE T PATTERN ANAL, V25, P57, DOI 10.1109/TPAMI.2003.1159946
   Kumawat A, 2022, VISUAL COMPUT, V38, P3681, DOI 10.1007/s00371-021-02196-1
   Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562
   Li ZM, 2018, LECT NOTES COMPUT SC, V11213, P339, DOI [10.1007/978-3-030-01240-3_21, 10.1007/978-3-030-01219-9_23]
   Liao Y, 2017, IEEE INT CON MULTI, P859, DOI 10.1109/ICME.2017.8019358
   Lim JJ, 2013, PROC CVPR IEEE, P3158, DOI 10.1109/CVPR.2013.406
   Lin CA, 2022, APPL INTELL, V52, P11027, DOI 10.1007/s10489-022-03202-2
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Liu Y, 2017, PROC CVPR IEEE, P5872, DOI 10.1109/CVPR.2017.622
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Lu R, 2019, IEEE I CONF COMP VIS, P10342, DOI 10.1109/ICCV.2019.01044
   Ma J., 2023, The Visual Computer, P1
   Maninis KK, 2016, LECT NOTES COMPUT SC, V9905, P580, DOI 10.1007/978-3-319-46448-0_35
   Mély DA, 2016, VISION RES, V120, P93, DOI 10.1016/j.visres.2015.11.007
   Pan SH, 2023, VISUAL COMPUT, V39, P4149, DOI 10.1007/s00371-022-02581-4
   Paszke Adam, 2017, NIPS W
   Pu MY, 2022, PROC CVPR IEEE, P1392, DOI 10.1109/CVPR52688.2022.00146
   Pu MY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6859, DOI 10.1109/ICCV48922.2021.00680
   Shen W, 2015, PROC CVPR IEEE, P3982, DOI 10.1109/CVPR.2015.7299024
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Soria X, 2020, IEEE WINT CONF APPL, P1912, DOI 10.1109/WACV45572.2020.9093290
   Su Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5097, DOI 10.1109/ICCV48922.2021.00507
   Wang CY, 2020, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW50498.2020.00203
   Wang GX, 2019, LECT NOTES COMPUT SC, V11366, P686, DOI 10.1007/978-3-030-20876-9_43
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Wang YP, 2017, PROC CVPR IEEE, P1724, DOI 10.1109/CVPR.2017.187
   Wibisono J. K., 2021, ICME, P1, DOI [10.1109/ICME51207.2021.9428230, DOI 10.1109/ICME51207.2021.9428230]
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1007/s11263-017-1004-z, 10.1109/ICCV.2015.164]
   Xu D, 2017, ADV NEURAL INFORM PR, P3961, DOI 10.48550/arXiv.1801.00524
   Yang F, 2020, IEEE T INTELL TRANSP, V21, P1525, DOI 10.1109/TITS.2019.2910595
   Yang JM, 2016, PROC CVPR IEEE, P193, DOI 10.1109/CVPR.2016.28
   Yang W., 2023, The Visual Computer, P1
   Yu ZD, 2017, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2017.191
   Zhao LJ, 2017, VISUAL COMPUT, V33, P1169, DOI 10.1007/s00371-016-1279-z
   Zhou CX, 2023, Arxiv, DOI arXiv:2303.11828
NR 64
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 15
PY 2024
DI 10.1007/s00371-024-03280-y
EA FEB 2024
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HV1H4
UT WOS:001162185500003
DA 2024-08-05
ER

PT J
AU Johari, K
   Tong, CTZ
   Bhardwaj, R
   Subbaraju, V
   Kim, JJ
   Tan, UX
AF Johari, Kritika
   Tong, Christopher Tay Zi
   Bhardwaj, Rishabh
   Subbaraju, Vigneshwaran
   Kim, Jung-Jae
   Tan, U. -Xuan
TI Manufacturing domain instruction comprehension using synthetic data
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Domain adaptation; Referring expressions comprehension;
   Human-in-the-loop; Human-robot collaboration
AB Referring expression comprehension (REC) system solves a task to localize objects in a given image, based on natural language expression. We propose a novel approach to adapting the pre-trained REC model for the manufacturing domain. Despite significant advances in REC research, current REC datasets fail to recognize objects from specific yet important domains such as manufacturing due to the absence of domain-specific samples during training. Thus, we introduce a synthetic data-based domain adaptation approach for REC. To adapt a REC model to the manufacturing domain, we generated a synthetic REC dataset RefMD that consists of two sub-datasets: (1) dataset for manufacturing object classification, and (2) dataset for REC adaptation to manufacturing. Each dataset serves as one step toward the REC adaptation. Adaptation of the object classification network (visual backbone) is carried out by training ResNet50 on domain-specific labeled data, while the REC adaptation completes with the adaptation of modules in RealGIN altogether. The manufacturing domain-adapted model is further enhanced with the capability to handle ambiguous referring expressions through human-in-the-loop (HITL) interaction. The experiments on 3D-printed manufacturing objects demonstrate that the interactive REC model can accurately comprehend human instructions with an 82% accuracy. This paper introduces an approach to facilitate domain adaptation using solely synthetic data for the specific case of the manufacturing domain. However, the proposed adaptation methodology can be applied to any other domain by following the same synthetic data generation process.
C1 [Johari, Kritika; Tong, Christopher Tay Zi; Bhardwaj, Rishabh; Tan, U. -Xuan] Singapore Univ Technol & Design, Singapore, Singapore.
   [Subbaraju, Vigneshwaran; Kim, Jung-Jae] ASTAR, Singapore, Singapore.
C3 Singapore University of Technology & Design; Agency for Science
   Technology & Research (A*STAR)
RP Johari, K (corresponding author), Singapore Univ Technol & Design, Singapore, Singapore.
EM kritika_johari@mymail.sutd.edu.sg; ctzy818@gmail.com;
   rishabh_bhardwaj@mymail.sutd.edu.sg;
   vigneshwaran_subbaraju@ihpc.a-star.edu.sg; jjkim@i2r.a-star.edu.sg;
   uxuan_tan@sutd.edu.sg
RI Tan, U-Xuan/Y-2253-2019
OI Tan, U-Xuan/0000-0002-5757-1379; Johari, Kritika/0000-0002-6647-0246
FU Agency for Science, Technology and Research
FX No Statement Available
CR Ahmad N, 2022, VISUAL COMPUT, V38, P2751, DOI 10.1007/s00371-021-02153-y
   Ayadi M, 2021, VISUAL COMPUT, V37, P789, DOI 10.1007/s00371-020-01830-8
   Bhagat PK, 2023, VISUAL COMPUT, V39, P2149, DOI 10.1007/s00371-022-02470-w
   Bhardwaj R, 2022, Arxiv, DOI arXiv:2205.11024
   Borkman S., 2021, Unity Perception: Generate Synthetic Data for Computer Vision, DOI DOI 10.1109/ICCVW.2019.00340
   Chen XP, 2018, Arxiv, DOI arXiv:1812.03426
   Chung JY, 2014, Arxiv, DOI [arXiv:1412.3555, DOI 10.48550/ARXIV.1412.3555]
   Deng CR, 2018, PROC CVPR IEEE, P7746, DOI 10.1109/CVPR.2018.00808
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Gorniak P, 2004, J ARTIF INTELL RES, V21, P429, DOI 10.1613/jair.1327
   Hatori J, 2018, IEEE INT CONF ROBOT, P3774
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Escalante HJ, 2010, COMPUT VIS IMAGE UND, V114, P419, DOI 10.1016/j.cviu.2009.03.008
   Johari K, 2021, LECT NOTES ARTIF INT, V13086, P191, DOI 10.1007/978-3-030-90525-5_17
   Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215
   Jurado D, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21041123
   Kazakos I., 2021, arXiv
   Kazemzadeh S., 2014, EMNLP, DOI DOI 10.3115/V1/D14-1086
   Ke-Lin Du, 1999, International Journal of Robotics & Automation, V14, P171
   Kingma D. P., 2014, arXiv
   Kong X., 2023, Vis. Comput, V8, P1
   Latif UK, 2020, VISUAL COMPUT, V36, P1491, DOI 10.1007/s00371-019-01745-z
   Liang H, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P743, DOI 10.1145/2733373.2807972
   Liao Yiyi, 2020, CVPR
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu RT, 2019, PROC CVPR IEEE, P4180, DOI 10.1109/CVPR.2019.00431
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Nagaraja VK, 2016, LECT NOTES COMPUT SC, V9908, P792, DOI 10.1007/978-3-319-46493-0_48
   Prakash AJ, 2023, VISUAL COMPUT, V39, P1765, DOI 10.1007/s00371-022-02443-z
   Pramanick P, 2022, IEEE ROBOT AUTOM LET, V7, P10826, DOI 10.1109/LRA.2022.3195198
   Qiao YY, 2021, IEEE T MULTIMEDIA, V23, P4426, DOI 10.1109/TMM.2020.3042066
   Qin YM, 2023, VISUAL COMPUT, V39, P3597, DOI 10.1007/s00371-023-02922-x
   Reif R, 2008, VISUAL COMPUT, V24, P987, DOI 10.1007/s00371-008-0271-7
   Ren SQ, 2016, Arxiv, DOI [arXiv:1506.01497, DOI 10.1109/TPAMI.2016.2577031]
   Sadhu A, 2019, IEEE I CONF COMP VIS, P4693, DOI 10.1109/ICCV.2019.00479
   Scalise R, 2018, INT J ROBOT RES, V37, P558, DOI 10.1177/0278364918760992
   Shridhar M, 2018, Arxiv, DOI arXiv:1806.03831
   Shridhar M, 2020, INT J ROBOT RES, V39, P217, DOI 10.1177/0278364919897133
   Tang PZ, 2024, VISUAL COMPUT, V40, P2015, DOI 10.1007/s00371-023-02899-7
   Thomason J, 2019, IEEE INT CONF ROBOT, P6934, DOI 10.1109/icra.2019.8794287
   Wood E, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3661, DOI 10.1109/ICCV48922.2021.00366
   Wood E, 2015, IEEE I CONF COMP VIS, P3756, DOI 10.1109/ICCV.2015.428
   Xiang N., 2023, Vis. Comput., P1
   Yang LJ, 2019, IEEE I CONF COMP VIS, P5187, DOI 10.1109/ICCV.2019.00529
   Yang Y., 2022, 2022 IEEE INT C ROBO
   Yu K, 2020, VISUAL COMPUT, V36, P2051, DOI 10.1007/s00371-020-01911-8
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
   Zhou Y., 2019, arXiv
NR 50
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 27
PY 2024
DI 10.1007/s00371-023-03232-y
EA JAN 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FX5D9
UT WOS:001149155600001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Ji, LE
   Qiu, SR
   Xu, Z
   Liu, Y
   Yang, G
AF Ji, Lianen
   Qiu, Shirong
   Xu, Zhi
   Liu, Yue
   Yang, Guang
TI Comparing dimensionality reduction techniques for visual analysis of the
   LSTM hidden activity on multi-dimensional time series modeling
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Visual analytics; Long short-term memory; Dimensionality reduction;
   Multi-dimensional time series
ID ANALYTICS; DYNAMICS
AB Long short-term memory (LSTM) network is widely applied to multi-dimensional time series modeling to solve many real-world problems, and visual analytics plays a crucial role in improving its interpretability. To understand the high-dimensional activations in the hidden layer of the model, the application of dimensionality reduction (DR) techniques is essential. However, the diversity of DR techniques dramatically increases the difficulty of selecting one among them. In this paper, aiming at the applicability of DR techniques for visual analysis of LSTM hidden activity on multi-dimensional time series modeling, we select four representative DR techniques as the comparison objects, including principal component analysis (PCA), multi-dimensional scaling (MDS), t-distributed stochastic neighbor embedding (t-SNE), and uniform manifold approximation and projection (UMAP). The original continuous modeling data and the symbolically processed discrete data are used as knowledge of model learning, which are associated with LSTM hidden layer activity, and the ability of DR techniques to maintain high-dimensional information of the hidden layer activation is compared. According to the model structure of LSTM and the characteristics of modeling data, the controlled experiments were carried out in five typical tasks, namely the quality evaluation of DR, the abstract representation of high and low hidden layers, the association analysis between model and output variable, the importance analysis of input features and the exploration of temporal regularity. Through the complete experimental process and detailed result analysis, we distilled a systematic guidance for analysts to select appropriate and effective DR techniques for visual analytics of LSTM.
C1 [Ji, Lianen; Qiu, Shirong; Xu, Zhi; Liu, Yue; Yang, Guang] China Univ Petr, Dept Comp Sci, Beijing Key Lab Petr Data Min, Beijing, Peoples R China.
C3 China University of Petroleum
RP Ji, LE (corresponding author), China Univ Petr, Dept Comp Sci, Beijing Key Lab Petr Data Min, Beijing, Peoples R China.
EM jilianen@cup.edu.cn; 2017011930@student.cup.edu.cn
OI Ji, Lianen/0000-0003-0503-0009
FU National Natural Science Foundation of China [60873093]; NSFC; Strategic
   Cooperation Technology Projects of CNPC [ZLZX2020-05]; CUPB
FX This work was supported by the NSFC under Grant No. 60873093 and the
   Strategic Cooperation Technology Projects of CNPC and CUPB
   (ZLZX2020-05).
CR Ali M, 2019, VISUAL COMPUT, V35, P1013, DOI 10.1007/s00371-019-01673-y
   Alicioglu G, 2022, COMPUT GRAPH-UK, V102, P502, DOI 10.1016/j.cag.2021.09.002
   Armstrong G, 2022, FRONT BIOINFORM, V2, DOI 10.3389/fbinf.2022.821861
   Ayesha S, 2020, INFORM FUSION, V59, P44, DOI 10.1016/j.inffus.2020.01.005
   Ballester-Ripoll R, 2024, VISUAL COMPUT, V40, P2571, DOI 10.1007/s00371-023-02937-4
   Bäuerle A, 2023, VISUAL COMPUT, V39, P4323, DOI 10.1007/s00371-022-02593-0
   Lipton ZC, 2015, Arxiv, DOI arXiv:1506.00019
   Choo J, 2018, IEEE COMPUT GRAPH, V38, P84, DOI 10.1109/MCG.2018.042731661
   Chu YD, 2020, IEEE T NEUR NET LEAR, V31, P1297, DOI 10.1109/TNNLS.2019.2919676
   Cox MA., 2008, Handbook of Data Visualization, P315, DOI [10.1007/978-3-540-33037-014, DOI 10.1007/978-3-540-33037-0_14, 10.1007/978-3-540-33037-0_14]
   De Lorenzo A, 2019, PROCEEDINGS OF THE 2019 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE COMPANION (GECCCO'19 COMPANION), P1864, DOI 10.1145/3319619.3326868
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   Espadoto M, 2021, IEEE T VIS COMPUT GR, V27, P2153, DOI 10.1109/TVCG.2019.2944182
   Gabella M, 2020, Arxiv, DOI [arXiv:1902.08160, 10.48550/arXiv.1902.08160, DOI 10.48550/ARXIV.1902.08160]
   Gracia A, 2014, INFORM SCIENCES, V270, P1, DOI 10.1016/j.ins.2014.02.068
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Gunning D, 2019, AI MAG, V40, P44, DOI 10.1609/aimag.v40i2.2850
   Hohman F, 2019, IEEE T VIS COMPUT GR, V25, P2674, DOI 10.1109/TVCG.2018.2843369
   Holzinger A, 2021, COMM COM INF SC, V1524, P427, DOI 10.1007/978-3-030-93736-2_33
   Holzinger A, 2022, INFORM FUSION, V79, P263, DOI 10.1016/j.inffus.2021.10.007
   Jain R, 2023, MULTIMED TOOLS APPL, V82, P22613, DOI 10.1007/s11042-023-14432-y
   [纪连恩 Ji Lian'en], 2021, [计算机辅助设计与图形学学报, Journal of Computer-Aided Design & Computer Graphics], V33, P1876
   Jia WK, 2022, COMPLEX INTELL SYST, V8, P2663, DOI 10.1007/s40747-021-00637-x
   Karo I.M.K., 2017, 2017 2 INT C INF COM, P1, DOI [10.1109/IAC.2017.8280572, DOI 10.1109/IAC.2017.8280572]
   Kindlmann G, 2014, IEEE T VIS COMPUT GR, V20, P2181, DOI 10.1109/TVCG.2014.2346325
   La Rosa B, 2023, COMPUT GRAPH FORUM, V42, P319, DOI 10.1111/cgf.14733
   Lin J, 2007, DATA MIN KNOWL DISC, V15, P107, DOI 10.1007/s10618-007-0064-z
   Liu SS, 2017, IEEE T VIS COMPUT GR, V23, P1249, DOI 10.1109/TVCG.2016.2640960
   Martins RM, 2014, COMPUT GRAPH-UK, V41, P26, DOI 10.1016/j.cag.2014.01.006
   McInnes L, 2020, Arxiv, DOI [arXiv:1802.03426, 10.21105/joss.00861]
   Natsukawa H, 2021, IEEE T VIS COMPUT GR, V27, P506, DOI 10.1109/TVCG.2020.3028956
   Paulovich FV, 2008, IEEE T VIS COMPUT GR, V14, P564, DOI 10.1109/TVCG.2007.70443
   Ras G, 2022, J ARTIF INTELL RES, V73, P329
   Rauber PE, 2017, IEEE T VIS COMPUT GR, V23, P101, DOI 10.1109/TVCG.2016.2598838
   Shen QM, 2020, IEEE PAC VIS SYMP, P61, DOI 10.1109/PacificVis48177.2020.2785
   Strobelt H, 2018, IEEE T VIS COMPUT GR, V24, P667, DOI 10.1109/TVCG.2017.2744158
   Tang ZY, 2017, INT CONF ACOUST SPEE, P2736, DOI 10.1109/ICASSP.2017.7952654
   van der Maaten L., 2007, J. Mach. Learn. Res., V10
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9
   Xia JZ, 2022, IEEE T VIS COMPUT GR, V28, P529, DOI 10.1109/TVCG.2021.3114694
   Zahavy T, 2016, PR MACH LEARN RES, V48
   Zhao Y, 2019, IEEE T VIS COMPUT GR, V25, P12, DOI 10.1109/TVCG.2018.2865020
   Zhou HY, 2021, AAAI CONF ARTIF INTE, V35, P11106
NR 45
TC 0
Z9 0
U1 9
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 22
PY 2024
DI 10.1007/s00371-023-03235-9
EA JAN 2024
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GJ3D1
UT WOS:001152252500007
DA 2024-08-05
ER

PT J
AU Li, HH
   Yuan, X
   Xu, CL
   Zhang, R
   Liu, XY
   Liu, LQ
AF Li, Huihui
   Yuan, Xu
   Xu, Chunlin
   Zhang, Rui
   Liu, Xiaoyong
   Liu, Lianqi
TI Complexity aware center loss for facial expression recognition
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Facial expression recognition; Complexity-aware; Center loss; Deep
   metric learning
ID REPRESENTATION
AB Deep metric-based center loss has been widely used to enhance inter-class separability and intra-class compactness of network features and achieved promising results in facial expression recognition (FER) recently. However, existing center loss does not take the complexity of expression samples into consideration, which deteriorates the representativeness of the generated center vectors resulting in suboptimal performance. To solve this problem, we propose a novel complexity aware center loss for FER. Specifically, a multi-category division module is firstly devised to distinguish simple samples and difficult samples for each category based on the entropy value of sample prediction results. Then, an exact representative center module is employed on simple samples to generate a more representative center vector for each category by encouraging greater differences between different categories. Finally, an adaptive distance adjustment module is proposed to reduce the interference of difficult samples in the model learning process to further improve the accuracy of FER by maintaining a suitable distance between difficult samples and their corresponding center vector. Extensive experimental results on two benchmark datasets demonstrate the effectiveness, universality and superiority of our methods. The code will be available at https://github.com/sanjiaobo/CACL.
C1 [Li, Huihui; Yuan, Xu; Xu, Chunlin; Zhang, Rui] Guangdong Polytech Normal Univ, Sch Comp Sci, Zhongshan Ave, Guangzhou 510665, Guangdong, Peoples R China.
   [Liu, Xiaoyong] Guangdong Polytech Normal Univ, Sch Data Sci & Engn, Heyuan 517583, Peoples R China.
   [Liu, Xiaoyong] Guangdong Polytech Normal Univ, Acad Heyuan, Heyuan 517099, Peoples R China.
   [Liu, Lianqi] Guangzhou Kangning Hosp, Guangzhou 510555, Peoples R China.
   [Liu, Lianqi] Collaborat Innovat Ctr Civil Affairs Guangzhou, Guangzhou 510315, Peoples R China.
C3 Guangdong Polytechnic Normal University; Guangdong Polytechnic Normal
   University; Guangdong Polytechnic Normal University
RP Xu, CL; Zhang, R (corresponding author), Guangdong Polytech Normal Univ, Sch Comp Sci, Zhongshan Ave, Guangzhou 510665, Guangdong, Peoples R China.
EM lihh@gpnu.edu.cn; 939943578@qq.com; xuchunlin@gpnu.edu.cn;
   ruhand@gpnu.edu.cn; 35643506@qq.com; lcliulqi@163.com
FU National Natural Science Foundation of China; Guangdong Basic and
   Applied Basic Research Foundation [2023A1515010939]; Project of
   Education Department of Guangdong Province [2022KTSCX068, 2021ZDZX1079];
   Guangzhou Science and Technology Planning Project [2023A04J0364];
   Ministry of education of Humanities and Social Science project
   [18JDGC012]; Guangdong Science and Technology Project [KTP20210197,
   2017A040403068]; Guangzhou Science and Technology Plan Project
   [2023B04J0092];  [62006049];  [62172113]
FX This study was supported by National Natural Science Foundation of China
   (Grant Nos. 62006049 and 62172113), Guangdong Basic and Applied Basic
   Research Foundation (Grant No. 2023A1515010939), Project of Education
   Department of Guangdong Province (Grant Nos. 2022KTSCX068 and
   2021ZDZX1079), Guangzhou Science and Technology Planning Project (Grant
   No. 2023A04J0364), The Ministry of education of Humanities and Social
   Science project (Grant No. 18JDGC012), Guangdong Science and Technology
   Project (Grant Nos. KTP20210197 and 2017A040403068), Guangzhou Science
   and Technology Plan Project (Grant No. 2023B04J0092).
CR Al-Jebrni AH, 2023, VISUAL COMPUT, V39, P3675, DOI 10.1007/s00371-023-02984-x
   Arazo E, 2019, PR MACH LEARN RES, V97
   Benitez-Quiroz CF, 2016, PROC CVPR IEEE, P5562, DOI 10.1109/CVPR.2016.600
   Chang J, 2020, PROC CVPR IEEE, P5709, DOI 10.1109/CVPR42600.2020.00575
   Chen ZH, 2021, VISUAL COMPUT, V37, P2657, DOI 10.1007/s00371-021-02199-y
   Farzaneh AH, 2021, IEEE WINT CONF APPL, P2401, DOI 10.1109/WACV48630.2021.00245
   Farzaneh AH, 2020, IEEE COMPUT SOC CONF, P1631, DOI 10.1109/CVPRW50498.2020.00211
   Gao HX, 2023, NEURAL NETWORKS, V158, P228, DOI 10.1016/j.neunet.2022.11.025
   Gera D, 2022, Arxiv, DOI arXiv:2208.10221
   Gera D, 2021, IEEE INT CONF COMP V, P3578, DOI 10.1109/ICCVW54120.2021.00399
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Hu ZJ, 2022, IEEE T COMPUT SOC SY, V9, P1345, DOI 10.1109/TCSS.2021.3127561
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Lang JJ, 2022, PATTERN RECOGN LETT, V163, P17, DOI 10.1016/j.patrec.2022.09.007
   Le N, 2023, IEEE WINT CONF APPL, P6077, DOI 10.1109/WACV56688.2023.00603
   Li H., 2023, IEEE Trans. Affect. Comput., P1
   Li H., 2022, Vis. Comput, V39, P1
   Li HY, 2022, PROC CVPR IEEE, P4156, DOI 10.1109/CVPR52688.2022.00413
   Li HY, 2022, IEEE T IMAGE PROCESS, V31, P4637, DOI 10.1109/TIP.2022.3186536
   Li HY, 2021, NEUROCOMPUTING, V432, P159, DOI 10.1016/j.neucom.2020.12.076
   Li HY, 2021, IEEE T IMAGE PROCESS, V30, P2016, DOI 10.1109/TIP.2021.3049955
   Li S, 2019, IEEE T IMAGE PROCESS, V28, P356, DOI 10.1109/TIP.2018.2868382
   Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277
   Li YJ, 2019, PR MACH LEARN RES, V101, P897
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu HW, 2022, IEEE T CIRC SYST VID, V32, P6253, DOI 10.1109/TCSVT.2022.3165321
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Lu J, 2018, PR MACH LEARN RES, V80
   Mao JW, 2023, Arxiv, DOI [arXiv:2301.12149, DOI 10.48550/ARXIV.2301.12149]
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Ren MY, 2018, PR MACH LEARN RES, V80
   She JH, 2021, PROC CVPR IEEE, P6244, DOI 10.1109/CVPR46437.2021.00618
   Tang Y, 2021, IEEE T IMAGE PROCESS, V30, P444, DOI 10.1109/TIP.2020.3037467
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang K, 2020, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR42600.2020.00693
   Wang XY, 2019, IEEE I CONF COMP VIS, P6970, DOI 10.1109/ICCV.2019.00707
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wen ZY, 2023, BIOMIMETICS-BASEL, V8, DOI 10.3390/biomimetics8020199
   Xia HY, 2024, VISUAL COMPUT, V40, P2035, DOI 10.1007/s00371-023-02900-3
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Zhang X, 2023, Arxiv, DOI arXiv:2305.17895
   Zhang YH, 2021, ADV NEUR IN, V34
   Zhang YH, 2022, LECT NOTES COMPUT SC, V13686, P418, DOI 10.1007/978-3-031-19809-0_24
   Zheng C, 2023, Arxiv, DOI arXiv:2204.04083
NR 44
TC 0
Z9 0
U1 15
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 9
PY 2024
DI 10.1007/s00371-023-03221-1
EA JAN 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EJ0I1
UT WOS:001138436600001
DA 2024-08-05
ER

PT J
AU Zhao, X
   Chen, YH
   Yang, CZ
   Fang, LC
AF Zhao, Xin
   Chen, Yinhuang
   Yang, Chengzhuan
   Fang, Lincong
TI FuseNet: a multi-modal feature fusion network for 3D shape
   classification
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE 3D shape classification; Multi-view; Point cloud; Feature fusion
ID CONVOLUTIONAL NEURAL-NETWORKS; MODEL
AB Recently, the primary focus of research in 3D shape classification has been on point cloud and multi-view methods. However, the multi-view approaches inevitably lose the structural information of 3D shapes due to the camera angle limitation. The point cloud methods use a neural network to maximize the pooling of all points to obtain a global feature, resulting in the loss of local detailed information. The disadvantages of multi-view and point cloud methods affect the performance of 3D shape classification. This paper proposes a novel FuseNet model, which integrates multi-view and point cloud information and significantly improves the accuracy of 3D model classification. First, we propose a multi-view and point cloud part to obtain the raw features of different convolution layers of multi-view and point clouds. Second, we adopt a multi-view pooling method for feature fusion of multiple views to integrate features of different convolution layers more effectively, and we propose an attention-based multi-view and point cloud fusion block for integrating features of point cloud and multiple views. Finally, we extensively tested our method on three benchmark datasets: the ModelNet10, ModelNet40, and ShapeNet Core55. Our method's experimental results demonstrate superior or comparable classification performance to previously established state-of-the-art techniques for 3D shape classification.
C1 [Zhao, Xin; Chen, Yinhuang; Yang, Chengzhuan] Zhejiang Normal Univ, Sch Comp Sci & Technol, 688 Yingbin Rd, Jinhua 321004, Zhejiang, Peoples R China.
   [Fang, Lincong] Zhejiang Univ Finance & Econ, Sch Informat Management & Artifcial Intelligence, 18 Xueyuan St, Hangzhou 310018, Zhejiang, Peoples R China.
C3 Zhejiang Normal University; Zhejiang University of Finance & Economics
RP Yang, CZ (corresponding author), Zhejiang Normal Univ, Sch Comp Sci & Technol, 688 Yingbin Rd, Jinhua 321004, Zhejiang, Peoples R China.
EM czyang@zjnu.edu.cn; lincongfang@zufe.edu.cn
FU National Natural Science Foundation of China [62106227]; China
   Postdoctoral Science Foundation [2023M743132]; A "Teacher Professional
   Development Project" for Domestic Visiting Scholars [FX2023007]
FX We would like to thank the anonymous reviewers for their helpful
   suggestions. This work was supported by the National Natural Science
   Foundation of China (Grant No. 62106227), the China Postdoctoral Science
   Foundation (Grant No. 2023M743132), and the "Teacher Professional
   Development Project" for Domestic Visiting Scholars in 2023 (Project No.
   FX2023007).
CR Bai S, 2017, IEEE T MULTIMEDIA, V19, P1257, DOI 10.1109/TMM.2017.2652071
   Bai S, 2016, PROC CVPR IEEE, P5023, DOI 10.1109/CVPR.2016.543
   Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22
   Chen LF, 2023, VISUAL COMPUT, V39, P863, DOI 10.1007/s00371-021-02351-8
   Chen XZ, 2018, NEUROCOMPUTING, V316, P144, DOI 10.1016/j.neucom.2018.07.061
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Fang Y., 2022, Multimed. Syst., P1
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Furuya T., 2016, BMVC, V7, P8
   Goyal A., 2021, INT C MACHINE LEARNI, P3809
   Hagbi N, 2011, IEEE T VIS COMPUT GR, V17, P1369, DOI 10.1109/TVCG.2010.241
   Hamdi A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1, DOI 10.1109/ICCV48922.2021.00007
   Han ZZ, 2019, AAAI CONF ARTIF INTE, P8376
   Han ZZ, 2019, IEEE T IMAGE PROCESS, V28, P3986, DOI 10.1109/TIP.2019.2904460
   Han ZZ, 2019, IEEE T IMAGE PROCESS, V28, P658, DOI 10.1109/TIP.2018.2868426
   Hassan R, 2023, NEUROCOMPUTING, V526, P96, DOI 10.1016/j.neucom.2023.01.026
   Hegde V., 2016, arXiv
   Hu J., 2018, SQUEEZE AND EXCITATI, P7132
   Huang Xiaoting, 2022, 2022 IEEE 22nd International Conference on Communication Technology (ICCT), P1682, DOI 10.1109/ICCT56141.2022.10072638
   Kanezaki A, 2018, PROC CVPR IEEE, P5010, DOI 10.1109/CVPR.2018.00526
   Khan SH, 2019, PROC CVPR IEEE, P9731, DOI 10.1109/CVPR.2019.00997
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Kumawat S, 2019, PROC CVPR IEEE, P4898, DOI 10.1109/CVPR.2019.00504
   Li B, 2013, MULTIMED TOOLS APPL, V62, P821, DOI 10.1007/s11042-011-0873-3
   Li JL, 2023, NEUROCOMPUTING, V529, P166, DOI 10.1016/j.neucom.2023.01.094
   Li YZ, 2018, ADV NEUR IN, V31
   Lin T.-Y., 2017, PROC CVPR IEEE, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu AA, 2023, MULTIMEDIA SYST, V29, P1995, DOI 10.1007/s00530-023-01086-x
   Liu H, 2024, VISUAL COMPUT, V40, P971, DOI 10.1007/s00371-023-02826-w
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Ma C, 2019, IEEE T MULTIMEDIA, V21, P1169, DOI 10.1109/TMM.2018.2875512
   Ma X, 2022, Arxiv, DOI arXiv:2202.07123
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Meng HY, 2019, IEEE I CONF COMP VIS, P8499, DOI 10.1109/ICCV.2019.00859
   Mitra NJ, 2006, ACM T GRAPHIC, V25, P560, DOI 10.1145/1141911.1141924
   Pylvanainen T., 2010, S 3D DAT PROC VIS TR, V737, P738
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Richards-Rissetto H., 2012, 2012 18th International Conference on Virtual Systems and Multimedia (VSMM 2012). Proceedings, P331, DOI 10.1109/VSMM.2012.6365942
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Savva M., 2016, P EUR WORKSH 3D OBJ, V10
   Schnabel R, 2007, COMPUT GRAPH FORUM, V26, P214, DOI 10.1111/j.1467-8659.2007.01016.x
   Sfikas K, 2018, COMPUT GRAPH-UK, V71, P208, DOI 10.1016/j.cag.2017.12.001
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Sun HY, 2023, Arxiv, DOI arXiv:2305.00161
   Wang C, 2019, Arxiv, DOI arXiv:1906.01592
   Wang LQ, 2023, MACH INTELL RES, V20, P872, DOI 10.1007/s11633-023-1430-z
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wei X, 2020, PROC CVPR IEEE, P1847, DOI 10.1109/CVPR42600.2020.00192
   Wu CZ, 2023, PROC CVPR IEEE, P5333, DOI 10.1109/CVPR52729.2023.00516
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xu RC, 2023, APPL INTELL, V53, P7741, DOI 10.1007/s10489-022-03949-8
   Yavartanoo M, 2019, LECT NOTES COMPUT SC, V11365, P691, DOI 10.1007/978-3-030-20873-8_44
   You HX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1310, DOI 10.1145/3240508.3240702
   Zhang ZY, 2023, APPL INTELL, V53, P19060, DOI 10.1007/s10489-023-04498-4
   Zhang ZZ, 2018, IEEE T IMAGE PROCESS, V27, P5957, DOI 10.1109/TIP.2018.2862625
   Zhao YX, 2021, INT C PATT RECOG, P134, DOI 10.1109/ICPR48806.2021.9413135
   Zhi SF, 2018, COMPUT GRAPH-UK, V71, P199, DOI 10.1016/j.cag.2017.10.007
NR 59
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 26
PY 2024
DI 10.1007/s00371-024-03581-2
EA JUL 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZR2E9
UT WOS:001276948900002
DA 2024-08-05
ER

PT J
AU Song, W
   Yang, KL
AF Song, Wei
   Yang, Kaili
TI Dual adaptive local semantic alignment for few-shot fine-grained
   classification
SO VISUAL COMPUTER
LA English
DT Review; Early Access
DE Few-shot learning; Semantic details; Local feature alignment;
   Fine-grained image classification
ID NETWORK; SYSTEM
AB Few-shot fine-grained classification (FS-FGC) aims to learn discriminative semantic details (e.g., beaks and wings) with few labeled samples to precisely recognize novel classes. However, existing feature alignment methods mainly use a support set to align the query sample, which may lead to incorrect alignment of local semantic due to interference from background and non-target objects. In addition, these methods do not take into account the discrepancy of semantic information among channels. To address the above issues, we propose an effective dual adaptive local semantic alignment approach, which is composed of the channel semantic alignment module (CSAM) and the spatial semantic alignment module (SSAM). Specifically, CSAM adaptively generates channel weights to highlight discriminative information based on two sub-modules, namely the class-aware attention module and the target-aware attention module. CAM emphasizes the discriminative semantic details of each category in the support set and TAM enhances the target object region of the query image. On the basis of this, SSAM promotes effective alignment of semantically relevant local regions through a spatial bidirectional alignment strategy. Combining two adaptive modules to better capture fine-grained semantic contextual information along two dimensions, channel and spatial improves the accuracy and robustness of FS-FGC. Experimental results on three widely used fine-grained classification datasets demonstrate excellent performance that has significant competitive advantages over current mainstream methods. Codes are available at: https://github.com/kellyagya/DALSA.
C1 [Song, Wei; Yang, Kaili] Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, Jiangsu Prov Engn Lab Pattern Recognit & Computat, Wuxi 214122, Peoples R China.
C3 Jiangnan University
RP Song, W (corresponding author), Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, Jiangsu Prov Engn Lab Pattern Recognit & Computat, Wuxi 214122, Peoples R China.
EM songwei@jiangnan.edu.cn; 6223152009@stu.jiangnan.edu.cn
FX DAS:No datasets were generated or analysed during the current study.
CR Cao SY, 2022, INT J MACH LEARN CYB, V13, P2273, DOI 10.1007/s13042-022-01522-w
   Chen WY, 2020, Arxiv, DOI arXiv:1904.04232
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Chen ZH, 2023, IEEE T PATTERN ANAL, V45, P13489, DOI 10.1109/TPAMI.2023.3293885
   Chi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12200, DOI 10.1109/CVPR42600.2020.01222
   Dai L, 2024, NAT MED, DOI 10.1038/s41591-023-02702-z
   Dai L, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-23458-5
   Doersch C., 2020, NeurIPS, V33, P21981
   Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459
   Guo C, 2023, VISUAL COMPUT, V39, P2597, DOI 10.1007/s00371-022-02481-7
   Guo YR, 2022, IEEE T IMAGE PROCESS, V31, P4543, DOI 10.1109/TIP.2022.3184813
   Han-Jia Ye, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8805, DOI 10.1109/CVPR42600.2020.00883
   Hariharan B, 2017, IEEE I CONF COMP VIS, P3037, DOI 10.1109/ICCV.2017.328
   Hou RB, 2019, ADV NEUR IN, V32
   Hu WJ, 2020, IEEE ACCESS, V8, P115287, DOI 10.1109/ACCESS.2020.3001237
   Huang HX, 2022, IEEE T CIRC SYST VID, V32, P853, DOI 10.1109/TCSVT.2021.3065693
   Huang HX, 2021, IEEE T MULTIMEDIA, V23, P1666, DOI 10.1109/TMM.2020.3001510
   Jia JF, 2024, ENG APPL ARTIF INTEL, V127, DOI 10.1016/j.engappai.2023.107296
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Kai Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13467, DOI 10.1109/CVPR42600.2020.01348
   Kang D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8802, DOI 10.1109/ICCV48922.2021.00870
   Khosla A., 2011, CVPR WORKSH, V2
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Lee K, 2019, PROC CVPR IEEE, P10649, DOI 10.1109/CVPR.2019.01091
   Lee S, 2022, PROC CVPR IEEE, P5321, DOI 10.1109/CVPR52688.2022.00526
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Li M, 2022, VISUAL COMPUT, V38, P811, DOI 10.1007/s00371-020-02052-8
   Li WB, 2019, PROC CVPR IEEE, P7253, DOI 10.1109/CVPR.2019.00743
   Li XX, 2021, IEEE T IMAGE PROCESS, V30, P1318, DOI 10.1109/TIP.2020.3043128
   Li YF, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3235747
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu Y, 2022, VISUAL COMPUT, V38, P3377, DOI 10.1007/s00371-022-02551-w
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Nazir A, 2020, IEEE T IMAGE PROCESS, V29, P7192, DOI 10.1109/TIP.2020.2999854
   Qian B, 2024, PATTERNS, V5, DOI 10.1016/j.patter.2024.100929
   Qin Y, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12152480
   Simon C, 2020, PROC CVPR IEEE, P4135, DOI 10.1109/CVPR42600.2020.00419
   Snell J, 2017, ADV NEUR IN, V30
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Tang H, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108792
   Tian S, 2021, IEEE IMAGE PROC, P2478, DOI 10.1109/ICIP42928.2021.9506685
   Vinyals O., 2016, Advances in neural information processing systems, V29
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang Y, 2019, Arxiv, DOI arXiv:1911.04623
   Wei XS, 2019, IEEE T IMAGE PROCESS, V28, P6116, DOI 10.1109/TIP.2019.2924811
   Wertheimer D, 2021, PROC CVPR IEEE, P8008, DOI 10.1109/CVPR46437.2021.00792
   Wu J., 2023, P AAAI C ART INT WAS, P2821
   Wu YK, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P107, DOI 10.1145/3474085.3475532
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Xu CM, 2021, PROC CVPR IEEE, P5178, DOI 10.1109/CVPR46437.2021.00514
   Zagoruyko S, 2017, Arxiv, DOI [arXiv:1612.03928, DOI 10.48550/ARXIV.1612.03928]
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zeng QT, 2024, VISUAL COMPUT, V40, P153, DOI 10.1007/s00371-023-02772-7
   Zha ZC, 2023, IEEE T CIRC SYST VID, V33, P3947, DOI 10.1109/TCSVT.2023.3236636
   Zhang RX, 2018, ADV NEUR IN, V31
   Zhu YH, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1090
   Zhu YS, 2020, VISUAL COMPUT, V36, P1771, DOI 10.1007/s00371-019-01770-y
NR 57
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 22
PY 2024
DI 10.1007/s00371-024-03576-z
EA JUL 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZC5Q9
UT WOS:001273109100001
DA 2024-08-05
ER

PT J
AU Elanwar, R
   Betke, M
AF Elanwar, Randa
   Betke, Margrit
TI Generative adversarial networks for handwriting image generation: a
   review
SO VISUAL COMPUTER
LA English
DT Review; Early Access
DE Handwriting synthesis; Generative adversarial networks; Text-image
   generation
AB Handwriting synthesis, the task of automatically generating realistic images of handwritten text, has gained increasing attention in recent years, both as a challenge in itself, as well as a task that supports handwriting recognition research. The latter task is to synthesize large image datasets that can then be used to train deep learning models to recognize handwritten text without the need for human-provided annotations. While early attempts at developing handwriting generators yielded limited results [1], more recent works involving generative models of deep neural network architectures have been shown able to produce realistic imitations of human handwriting [2-19]. In this review, we focus on one of the most prevalent and successful architectures in the field of handwriting synthesis, the generative adversarial network (GAN). We describe the capabilities, architecture specifics, and performance of the GAN-based models that have been introduced to the literature since 2019 [2-14]. These models can generate random handwriting styles, imitate reference styles, and produce realistic images of arbitrary text that was not in the training lexicon. The generated images have been shown to contribute to improving handwriting recognition results when augmenting the training samples of recognition models with synthetic images. The synthetic images were often hard to expose as non-real, even by human examiners, but also could be implausible or style-limited. The review includes a discussion of the characteristics of the GAN architecture in comparison with other paradigms in the image-generation domain and highlights the remaining challenges for handwriting synthesis.
C1 [Elanwar, Randa] Elect Res Inst, Comp & Syst Dept, Cairo, Egypt.
   [Betke, Margrit] Boston Univ, Dept Comp Sci, Boston, MA USA.
C3 Egyptian Knowledge Bank (EKB); Electronics Research Institute (ERI);
   Boston University
RP Elanwar, R (corresponding author), Elect Res Inst, Comp & Syst Dept, Cairo, Egypt.
EM randa.elanwar@eri.sci.eg; betke@bu.edu
FU Science, Technology &Innovation Funding Authority (STDF); Egyptian
   Knowledge Bank (EKB)
FX Open access funding provided by The Science, Technology &Innovation
   Funding Authority (STDF) in cooperation with The Egyptian Knowledge Bank
   (EKB).
CR Aksan E, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173779
   Alonso Eloi, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P481, DOI 10.1109/ICDAR.2019.00083
   Betker J., 2023, Improving image generation with better captions
   Bhunia A.K., 2021, IEEE INT C COMP VIS, P1086
   Bińkowski M, 2021, Arxiv, DOI [arXiv:1801.01401, 10.48550/arXiv.1801.0140132, 10.48550/arXiv.1801.01401]
   Chang B, 2018, IEEE WINT CONF APPL, P199, DOI 10.1109/WACV.2018.00028
   Chang Chun Chieh, 2023, Document Analysis and Recognition - ICDAR 2023 Workshops.. Lecture Notes in Computer Science (14194), P285, DOI 10.1007/978-3-031-41501-2_20
   Chlap P, 2021, J MED IMAG RADIAT ON, V65, P545, DOI 10.1111/1754-9485.13261
   Dai G, 2023, PROC CVPR IEEE, P5977, DOI 10.1109/CVPR52729.2023.00579
   Davis B, 2020, Arxiv, DOI arXiv:2009.00678
   de Vries H, 2017, ADV NEUR IN, V30
   Dhariwal P, 2021, ADV NEUR IN, V34
   Dilipkumar D., 2017, Generative Adversarial Image Refinement for Handwriting Recognition
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Elanwar R.I., 2013, 2 INT C NEW PAR EL I, P1
   Fogel S, 2020, PROC CVPR IEEE, P4323, DOI 10.1109/CVPR42600.2020.00438
   Gan J, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3550070
   Gan J, 2021, AAAI CONF ARTIF INTE, V35, P7484
   Gers FA, 1999, IEE CONF PUBL, P850, DOI [10.1049/cp:19991218, 10.1162/089976600300015015]
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Graves A, 2006, ICML, P369, DOI DOI 10.1145/1143844.1143891
   Graves A, 2014, Arxiv, DOI [arXiv:1308.0850, DOI 10.48550/ARXIV.1308.0850]
   Grosicki Emmanuele, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P1398, DOI 10.1109/ICDAR.2009.184
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Ho J., 2020, C NEUR INF PROC SYST, P2256
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Lim JH, 2017, Arxiv, DOI [arXiv:1705.02894, 10.48550/arXiv.1705.02894, DOI 10.48550/ARXIV.1705.02894]
   Jain S, 2022, J INTELL MANUF, V33, P1007, DOI 10.1007/s10845-020-01710-x
   Ji B, 2020, Arxiv, DOI arXiv:1907.11845
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kang L, 2022, IEEE T PATTERN ANAL, V44, P8846, DOI 10.1109/TPAMI.2021.3122572
   Kang Lei, 2020, LNCS, P273, DOI DOI 10.1007/978-3-030-58592-117
   Karras T., 2018, P INT C LEARN REPR, P4401
   Khrulkov V, 2018, PR MACH LEARN RES, V80
   Kleber F, 2013, PROC INT CONF DOC, P560, DOI 10.1109/ICDAR.2013.117
   Lian Z, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3213767
   Liang W, 2023, PROCESSES, V11, DOI 10.3390/pr11123284
   Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065
   Liu XY, 2021, IEEE SIGNAL PROC LET, V28, P1838, DOI 10.1109/LSP.2021.3109541
   Luo CJ, 2023, IEEE T NEUR NET LEAR, V34, P8503, DOI 10.1109/TNNLS.2022.3151477
   Marti U.-V., 2002, International Journal on Document Analysis and Recognition, V5, P39, DOI 10.1007/s100320200071
   Mirza M., 2014, ARXIV
   Nichol P., 2021, Improved denoising diffusion probabilistic models, P8162, DOI DOI 10.48550/ARXIV.2102.09672
   Nikolaidou Konstantina, 2023, Document Analysis and Recognition - ICDAR 2023: 17th International Conference, Proceedings. Lecture Notes in Computer Science (14188), P384, DOI 10.1007/978-3-031-41679-8_22
   Odena A, 2017, PR MACH LEARN RES, V70
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Pippi V, 2023, PROC CVPR IEEE, P22458, DOI 10.1109/CVPR52729.2023.02151
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Salimans T, 2016, ADV NEUR IN, V29
   Saxena D, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3446374
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   Shmelkov K, 2018, LECT NOTES COMPUT SC, V11206, P218, DOI 10.1007/978-3-030-01216-8_14
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sohl-Dickstein J, 2015, PR MACH LEARN RES, V37, P2256
   Souibgui M.A., 2022, IEEE WINTER C APPL C, P935
   Tolosana R, 2021, AAAI CONF ARTIF INTE, V35, P600
   Tong A, 2014, 2014 11TH IAPR INTERNATIONAL WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS (DAS 2014), P81, DOI 10.1109/DAS.2014.43
   Waheed A, 2020, IEEE ACCESS, V8, P91916, DOI 10.1109/ACCESS.2020.2994762
   Wang Heng, 2023, Document Analysis and Recognition - ICDAR 2023: 17th International Conference, Proceedings. Lecture Notes in Computer Science (14190), P302, DOI 10.1007/978-3-031-41685-9_19
   Wang YM, 2022, INT C PATT RECOG, P1457, DOI 10.1109/ICPR56361.2022.9956551
   Wen C, 2021, IEEE WINT CONF APPL, P3881, DOI 10.1109/WACV48630.2021.00393
   Xu M, 2023, PATTERN RECOGN, V137, DOI 10.1016/j.patcog.2023.109347
   Yang S., 2023, Image Data Augmentation for Deep Learning: A Survey
   Zdenek Jan, 2023, Document Analysis and Recognition - ICDAR 2023: 17th International Conference, Proceedings. Lecture Notes in Computer Science (14188), P313, DOI 10.1007/978-3-031-41679-8_18
   Zdenek J, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5655, DOI 10.1145/3474085.3475713
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhu JY, 2017, ADV NEUR IN, V30
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu YZ, 2023, PROC CVPR IEEE, P14235, DOI 10.1109/CVPR52729.2023.01368
NR 70
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 2
PY 2024
DI 10.1007/s00371-024-03534-9
EA JUL 2024
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XG0S4
UT WOS:001260419000002
OA hybrid
DA 2024-08-05
ER

PT J
AU Suo, X
   Tang, WD
   Mao, LJ
   Li, Z
AF Suo, Xiang
   Tang, Weidi
   Mao, Lijuan
   Li, Zhen
TI Digital human and embodied intelligence for sports science:
   advancements, opportunities and prospects
SO VISUAL COMPUTER
LA English
DT Review; Early Access
DE Motion capture; Digital humans; Wearable sensors; Sports applications;
   Pose estimation
ID MOTION CAPTURE SYSTEMS; OF-THE-ART; ACCURACY; SENSORS
AB This paper presents a comprehensive review of state-of-the-art motion capture techniques for digital human modeling in sports, including traditional optical motion capture systems, wearable sensor capture systems, computer vision capture systems, and fusion motion capture systems. The review explores the strengths, limitations, and applications of each technique in the context of sports science, such as performance analysis, technique optimization, injury prevention, and interactive training. The paper highlights the significance of accurate and comprehensive motion data acquisition for creating high-fidelity digital human models that can replicate an athlete's movements and biomechanics. However, several challenges and limitations are identified, such as limited capture volume, marker occlusion, accuracy limitations, lack of diverse datasets, and computational complexity. To address these challenges, the paper emphasizes the need for collaborative efforts from researchers and practitioners across various disciplines. By bridging theory and practice and identifying application-specific challenges and solutions, this review aims to facilitate cross-disciplinary collaboration and guide future research and development efforts in harnessing the power of digital human technology for sports science advancement, ultimately unlocking new possibilities for athlete performance optimization and health.
C1 [Suo, Xiang; Tang, Weidi; Mao, Lijuan; Li, Zhen] Shanghai Univ Sport, Shanghai 200438, Peoples R China.
C3 Shanghai University of Sport
RP Li, Z (corresponding author), Shanghai Univ Sport, Shanghai 200438, Peoples R China.
EM xiang_suo@sus.edu.cn; weidi_tang@sus.edu.cn; maolijuan@sus.edu.cn;
   lizhen@sus.edu.cn
OI SUO, Xiang/0009-0000-5899-0703
FU Research and Innovation Grant for Graduate Students, Shanghai University
   of Sport
FX No Statement Available
CR Abdel-Malek K., 2016, Digital human method and simulation for predicting musculoskeletal injuries
   Al-Jebrni AH, 2023, VISUAL COMPUT, V39, P3675, DOI 10.1007/s00371-023-02984-x
   Alldieck T, 2018, Arxiv, DOI [arXiv:1808.01338, 10.48550/arXiv.1808.01338, DOI 10.48550/ARXIV.1808.01338]
   Aouaidjia K, 2021, IEEE T SYST MAN CY-S, V51, P2774, DOI 10.1109/TSMC.2019.2916896
   Armitano-Lago C, 2022, FRONT SPORTS ACT LIV, V3, DOI 10.3389/fspor.2021.809898
   Ates HC, 2022, NAT REV MATER, V7, P887, DOI 10.1038/s41578-022-00460-x
   Aughey RJ, 2022, SPORTS ENG, V25, DOI 10.1007/s12283-021-00365-y
   Aurand AM, 2017, J BIOMECH, V58, P237, DOI 10.1016/j.jbiomech.2017.05.006
   Avogaro A, 2023, FRONT COMP SCI-SWITZ, V5, DOI 10.3389/fcomp.2023.1153160
   Baumgartner T, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-41142-0
   Benjaminse A., 2020, ISBS Proc. Arch, V38, P752
   Bulearca M., 2010, Global Business Management Research, V2, P237
   Capasa L, 2022, J THEOR APPL EL COMM, V17, P686, DOI 10.3390/jtaer17020036
   Chao Edmund Y S, 2007, J Orthop Surg Res, V2, P2, DOI 10.1186/1749-799X-2-2
   Chatzopoulos D, 2017, IEEE ACCESS, V5, P6917, DOI 10.1109/ACCESS.2017.2698164
   Cobos M, 2022, EURASIP J AUDIO SPEE, V2022, DOI 10.1186/s13636-022-00242-x
   Colyer SL, 2018, SPORTS MED-OPEN, V4, DOI 10.1186/s40798-018-0139-y
   Corazza S, 2010, INT J COMPUT VISION, V87, P156, DOI 10.1007/s11263-009-0284-3
   Das K, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-49360-2
   Dawei Liang, 2019, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V3, DOI 10.1145/3314404
   Debevec P, 2000, COMP GRAPH, P145, DOI 10.1145/344779.344855
   Desmarais Y, 2021, Arxiv, DOI arXiv:2010.06449
   Di Raimondo G, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23094484
   Duan CP, 2023, APPL SCI-BASEL, V13, DOI 10.3390/app13137611
   García LC, 2023, J CLIN MED, V12, DOI 10.3390/jcm12247539
   Gilbert A, 2019, INT J COMPUT VISION, V127, P381, DOI 10.1007/s11263-018-1118-y
   Goebert C., 2020, Sports Innov. J, V1, P134, DOI [10.18060/24227, DOI 10.18060/24227]
   Gu Y, 2017, IEEE INTERNET THINGS, V4, P2326, DOI 10.1109/JIOT.2017.2754578
   Gurbuz SZ, 2022, 2022 IEEE-EMBS INTERNATIONAL CONFERENCE ON BIOMEDICAL AND HEALTH INFORMATICS (BHI) JOINTLY ORGANISED WITH THE IEEE-EMBS INTERNATIONAL CONFERENCE ON WEARABLE AND IMPLANTABLE BODY SENSOR NETWORKS (BSN'22), DOI 10.1109/BHI56158.2022.9926892
   Gurchiek RD, 2019, J APPL BIOMECH, V35, P164, DOI 10.1123/jab.2018-0107
   Habermann M, 2021, Arxiv, DOI [arXiv:2107.02407, 10.48550/arXiv.2107.02407, DOI 10.48550/ARXIV.2107.02407]
   Haratian R, 2022, SENS IMAGING, V23, DOI 10.1007/s11220-022-00394-2
   He QQ, 2024, CHIN J MECH ENG-EN, V37, DOI 10.1186/s10033-024-00998-7
   Hiemann A, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21093214
   Hohmuth R, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23031060
   Hribernik M, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22083006
   Hu PP, 2022, IEEE T MULTIMEDIA, V24, P2139, DOI 10.1109/TMM.2021.3076340
   Huang S, 2023, Arxiv, DOI [arXiv:2306.06669, DOI 10.48550/ARXIV.2306.06669, 10.48550/arXiv.2306.06669]
   Imsdahl SI, 2020, J ORTHOP RES, V38, P450, DOI 10.1002/jor.24464
   Joo H, 2018, Arxiv, DOI [arXiv:1801.01615, 10.48550/arXiv.1801.01615, DOI 10.48550/ARXIV.1801.01615]
   Kanko MR, 2021, J BIOMECH, V127, DOI 10.1016/j.jbiomech.2021.110665
   Karambakhsh A, 2024, IEEE T NEUR NET LEAR, V35, P532, DOI 10.1109/TNNLS.2022.3175775
   Khan F, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20082272
   Kim A., 2024, Data Sci. Manag, DOI [10.1016/j.dsm.2024.01.002, DOI 10.1016/J.DSM.2024.01.002]
   Koga H, 2010, AM J SPORT MED, V38, P2218, DOI 10.1177/0363546510373570
   Lam WWT, 2023, J NEUROENG REHABIL, V20, DOI 10.1186/s12984-023-01186-9
   Lavikainen J, 2023, ANN BIOMED ENG, V51, P2479, DOI 10.1007/s10439-023-03278-y
   Lei Q, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19194129
   Li Haifu, 2021, Journal of Physics: Conference Series, DOI 10.1088/1742-6596/1992/3/032047
   Li W, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21103496
   Li XX, 2015, J GEODESY, V89, P607, DOI 10.1007/s00190-015-0802-8
   Li XY, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11091068
   Liang H, 2014, IEEE T MULTIMEDIA, V16, P1241, DOI 10.1109/TMM.2014.2306177
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu W, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3524497
   Liu X, 2021, FRONT COMP SCI-SWITZ, V3, DOI 10.3389/fcomp.2021.661676
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Lorenz EA, 2024, J NEUROENG REHABIL, V21, DOI 10.1186/s12984-023-01294-6
   Lugrís U, 2024, MULTIBODY SYST DYN, V60, P3, DOI 10.1007/s11044-023-09938-0
   Malus J, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21051830
   Manakitsa N, 2024, TECHNOLOGIES, V12, DOI 10.3390/technologies12020015
   Marshall B, 2023, VIRTUAL REAL-LONDON, V27, P2397, DOI 10.1007/s10055-023-00807-x
   Mathieu E, 2023, J NEUROENG REHABIL, V20, DOI 10.1186/s12984-023-01253-1
   Mcclintock FA, 2024, MED ENG PHYS, V126, DOI 10.1016/j.medengphy.2024.104146
   Merriaux P, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17071591
   Morais JE, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22103677
   Musse SR, 2001, IEEE T VIS COMPUT GR, V7, P152, DOI 10.1109/2945.928167
   Naik BT, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12094429
   Nakano N, 2020, FRONT SPORTS ACT LIV, V2, DOI 10.3389/fspor.2020.00050
   Nasr A, 2024, COMPUT METHOD BIOMEC, V27, P306, DOI 10.1080/10255842.2023.2184747
   Naumann A., 2007, MMI Interaktiv
   Nazir A, 2021, IEEE T BIO-MED ENG, V68, P2540, DOI 10.1109/TBME.2021.3050310
   Nguyen KD, 2011, IEEE-ASME T MECH, V16, P213, DOI 10.1109/TMECH.2009.2039222
   Nozawa T, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1341, DOI [10.1109/VR.2019.8797717, 10.1109/vr.2019.8797717]
   Okada Y., 2023, Virtual ski training system that allows beginners to acquire ski skills based on physical and visual feedbacks, P1268, DOI [10.1109/IROS55552.2023.10342020, DOI 10.1109/IROS55552.2023.10342020]
   Pagnon D, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21196530
   Peters M, 2019, ADV INTELL SYST, V876, P885, DOI 10.1007/978-3-030-02053-8_134
   Phutane U, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21041208
   Pinheiro GD, 2022, FRONT SPORTS ACT LIV, V4, DOI 10.3389/fspor.2022.818556
   Pons-Moll G, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766993
   Pueo B, 2017, RETOS, P241
   Qian B, 2024, PATTERNS, V5, DOI 10.1016/j.patter.2024.100929
   Redkar S., 2017, Int. Robot. Autom. J, DOI [10.15406/iratj.2017.03.00080, DOI 10.15406/IRATJ.2017.03.00080]
   Reinschmidt C, 1997, J BIOMECH, V30, P729, DOI 10.1016/S0021-9290(97)00001-8
   Ren L, 2010, J BIOMECH, V43, P194, DOI 10.1016/j.jbiomech.2009.09.027
   Retscher G, 2019, APPL GEOMAT, V11, P187, DOI 10.1007/s12518-018-00252-5
   Roupa I, 2022, ARCH COMPUT METHOD E, V29, P4915, DOI 10.1007/s11831-022-09757-0
   Samkari E, 2023, MACH LEARN KNOW EXTR, V5, P1612, DOI 10.3390/make5040081
   Sawan Nedal, 2020, EBEE 2020: 2020 2nd International Conference on E-Business and E-commerce Engineering, P55, DOI 10.1145/3446922.3446932
   Seth A, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1006223
   Shaikh MB, 2024, NEURAL COMPUT APPL, DOI 10.1007/s00521-023-09186-5
   Shan WK, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3446, DOI 10.1145/3474085.3475504
   Sharif SV, 2023, RELIAB ENG SYST SAFE, V235, DOI 10.1016/j.ress.2023.109219
   Sheng BY, 2020, IEEE INTERNET THINGS, V7, P3592, DOI 10.1109/JIOT.2020.2973272
   Shimizu K, 2021, IEEE ENG MED BIO, P7178, DOI 10.1109/EMBC46164.2021.9629854
   Sinha AK, 2023, INT J SOC ROBOT, DOI 10.1007/s12369-023-00999-x
   Smith AC, 2016, J APPL BIOMECH, V32, P23, DOI 10.1123/jab.2015-0032
   Stancin S, 2013, SENSORS-BASEL, V13, P7505, DOI 10.3390/s130607505
   Stelzer A, 2004, IEEE T MICROW THEORY, V52, P2664, DOI 10.1109/TMTT.2004.838281
   Strojny P., 2023, COMPUT ED X REAL, V2
   Su BB, 2023, FRONT NEUROROBOTICS, V17, DOI 10.3389/fnbot.2023.1244417
   Tang W, 2023, MACH VISION APPL, V34, DOI 10.1007/s00138-023-01474-3
   Tanier M., Bleach. Rep
   Telfer S., 2022, Musculoskeletal Modeling of the Foot and Ankle, P387, DOI [10.1016/B978-0-12-815449-6.00021-4, DOI 10.1016/B978-0-12-815449-6.00021-4]
   Thewlis D, 2013, J APPL BIOMECH, V29, P112, DOI 10.1123/jab.29.1.112
   Tian LM, 2023, COMPLEX INTELL SYST, V9, P865, DOI 10.1007/s40747-022-00837-z
   Tonkin EL, 2023, SCI DATA, V10, DOI 10.1038/s41597-023-02017-1
   Topley M, 2020, J BIOMECH, V106, DOI 10.1016/j.jbiomech.2020.109820
   Torvinen P, 2024, BIOENGINEERING-BASEL, V11, DOI 10.3390/bioengineering11020136
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Toshpulatov M, 2022, J SUPERCOMPUT, V78, P7616, DOI 10.1007/s11227-021-04184-7
   Tretschk E, 2023, COMPUT GRAPH FORUM, V42, P485, DOI 10.1111/cgf.14774
   Trivedi U, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23083912
   Umek Anton, 2022, Personal and Ubiquitous Computing, V26, P1023, DOI 10.1007/s00779-020-01486-0
   Urtasun R, 2004, COMPUT GRAPH FORUM, V23, P799, DOI 10.1111/j.1467-8659.2004.00809.x
   van der Kruk E, 2018, EUR J SPORT SCI, V18, P806, DOI 10.1080/17461391.2018.1463397
   Veloso A., 2006, Biomechanics modeling of human musculoskeleial system using adams multibody dynamics package, P401
   Vlasic D., 2009, ACM SIGGRAPH ASIA 20, P1, DOI [10.1145/1661412.1618520, DOI 10.1145/1661412.1618520]
   Willwacher S, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-58352-5
   Wittmann F, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19061312
   Xiao L, 2023, J CLOUD COMPUT-ADV S, V12, DOI 10.1186/s13677-023-00552-1
   Xie K, 2022, Arxiv, DOI [arXiv:2109.09913, 10.48550/arXiv.2109.09913, DOI 10.48550/ARXIV.2109.09913]
   Xu LY, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16091443
   Yang Jungang, 2024, Meas.: Sens, V33, DOI [10.1016/j.measen.2024.101104, DOI 10.1016/J.MEASEN.2024.101104]
   Yin LQ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1739, DOI 10.1145/3503161.3547766
   Yung-Che Li, 2021, 2021 IEEE International Conference on Robotics, Automation and Artificial Intelligence (RAAI), P6, DOI 10.1109/RAAI52226.2021.9507807
   Zhang ZJ, 2024, VISUAL COMPUT, V40, P5123, DOI 10.1007/s00371-023-02862-6
   Zhang ZJ, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2120
   Zhu H, 2019, Arxiv, DOI [arXiv:1904.10506, 10.48550/arXiv.1904.10506, DOI 10.48550/ARXIV.1904.10506]
   Zhuang HC, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11219982
   Zuo CH, 2024, Arxiv, DOI [arXiv:2312.05473, 10.48550/arXiv.2312.05473, DOI 10.48550/ARXIV.2312.05473]
NR 131
TC 1
Z9 1
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 21
PY 2024
DI 10.1007/s00371-024-03547-4
EA JUN 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UY0Q4
UT WOS:001251511100003
DA 2024-08-05
ER

PT J
AU Qian, B
   Wang, XN
   Guan, ZY
   Yang, DW
   Ran, AR
   Li, TY
   Wang, ZY
   Wen, Y
   Shu, XM
   Xie, JY
   Liu, SC
   Xing, GY
   Silva-Rodriguez, J
   Kobbi, R
   Li, P
   Chen, TL
   Bi, L
   Kim, J
   Jia, WP
   Li, HT
   Qin, J
   Zhang, P
   Cheng, CY
   Heng, PA
   Wong, TY
   Cheung, CY
   Tham, YC
   Thalmann, NM
   Sheng, B
AF Qian, Bo
   Wang, Xiangning
   Guan, Zhouyu
   Yang, Dawei
   Ran, Anran
   Li, Tingyao
   Wang, Zheyuan
   Wen, Yang
   Shu, Xinming
   Xie, Jinyang
   Liu, Shichang
   Xing, Guanyu
   Silva-Rodriguez, Julio
   Kobbi, Riadh
   Li, Ping
   Chen, Tingli
   Bi, Lei
   Kim, Jinman
   Jia, Weiping
   Li, Huating
   Qin, Jing
   Zhang, Ping
   Cheng, Ching-Yu
   Heng, Pheng-Ann
   Wong, Tien Yin
   Cheung, Carol Y.
   Tham, Yih-Chung
   Thalmann, Nadia Magnenat
   Sheng, Bin
TI HRDC challenge: a public benchmark for hypertension and hypertensive
   retinopathy classification from fundus images
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Hypertension; Hypertensive; Retinopathy; Deep learning
ID SEGMENTATION
AB Hypertensive retinopathy (HR) can potentially lead to vision loss if left untreated. Early screening and treatment are critical in reducing the risk of vision loss. The computer-aided diagnostic system presents an opportunity to improve the efficiency and reliability of HR screening and diagnosis, particularly given the shortage of specialized medical professionals and the challenges faced by primary care physicians in making precise diagnoses. A notable barrier to the development of such diagnostic algorithms is the lack of publicly available benchmarks and datasets. To address these issues, we organized a challenge named "HRDC-Hypertensive Retinopathy Diagnosis Challenge" in conjunction with the Computer Graphics International (CGI) 2023 conference. The challenge provided a fundus image dataset for two clinical tasks: hypertension classification and HR classification, with each task containing 1000 images. This paper presents a concise summary and analysis of the submitted methods and results for the two challenge tasks. For hypertension classification, the best performing algorithm achieved a Kappa score of 0.3819, an F1 score of 0.6337, and a specificity of 0.8472. For HR classification, the best performing algorithm achieved a Kappa score of 0.4154, an F1 score of 0.6122, and a specificity of 0.8444. We also explored an ensemble approach to the top-ranking methods, which further improved performance beyond the individual best performing algorithm for each task. The challenge results show that there is room for further optimization of these methods, but the insights and methodologies derived from this challenge provide valuable directions for developing more precise and reliable classification models for hypertension and HR.
C1 [Qian, Bo; Li, Tingyao; Wang, Zheyuan; Shu, Xinming; Xie, Jinyang; Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Wang, Xiangning] Shanghai Jiao Tong Univ, Dept Ophthalmol, Shanghai Peoples Hosp 6, Sch Med, Shanghai, Peoples R China.
   [Guan, Zhouyu; Jia, Weiping; Li, Huating] Shanghai Jiao Tong Univ, Dept Endocrinol & Metab, Peoples Hosp 6, Shanghai, Peoples R China.
   [Yang, Dawei; Ran, Anran; Cheung, Carol Y.] Chinese Univ Hong Kong, Dept Ophthalmol & Visual Sci, Hong Kong, Peoples R China.
   [Wen, Yang] Shenzhen Univ, Sch Elect & Informat Engn, Shenzhen, Peoples R China.
   [Liu, Shichang; Xing, Guanyu] Sichuan Univ, Chengdu, Peoples R China.
   [Silva-Rodriguez, Julio] ETS, Montreal, PQ, Canada.
   [Kobbi, Riadh] DIAGNOS Inc, Quebec City, PQ, Canada.
   [Li, Ping] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
   [Chen, Tingli] Huadong Sanat, Wuxi, Jiangsu, Peoples R China.
   [Bi, Lei] Shanghai Jiao Tong Univ, Inst Translat Med, Shanghai, Peoples R China.
   [Kim, Jinman] Univ Sydney, Sch Comp Sci, Sydney, Australia.
   [Qin, Jing] Hong Kong Polytech Univ, Ctr Smart Hlth, Sch Nursing, Hong Kong, Peoples R China.
   [Zhang, Ping] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH USA.
   [Cheng, Ching-Yu; Tham, Yih-Chung] Singapore Natl Eye Ctr, Singapore Eye Res Inst, Singapore, Singapore.
   [Heng, Pheng-Ann] Chinese Univ Hong Kong, Inst Med Intelligence & XR, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
   [Wong, Tien Yin] Tsinghua Univ, Tsinghua Med, Beijing, Peoples R China.
   [Thalmann, Nadia Magnenat] Univ Geneva, MIRALab, CUI, Geneva, Switzerland.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai
   Jiao Tong University; Chinese University of Hong Kong; Shenzhen
   University; Sichuan University; University of Quebec; Ecole de
   Technologie Superieure - Canada; Hong Kong Polytechnic University;
   Shanghai Jiao Tong University; University of Sydney; Hong Kong
   Polytechnic University; University System of Ohio; Ohio State
   University; Singapore National Eye Center; National University of
   Singapore; Chinese University of Hong Kong; Tsinghua University;
   University of Geneva
RP Sheng, B (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.; Chen, TL (corresponding author), Huadong Sanat, Wuxi, Jiangsu, Peoples R China.
EM chentingli1028@163.com; shengbin@sjtu.edu.cn
OI Guan, Zhouyu/0009-0008-5102-0067
FU National Key R & D Program of China [2022YFC2502802]; National Natural
   Science Foundation of China [62272298, 8238810007]; Beijing Natural
   Science Foundation [IS23096]; College-level Project Fund of Shanghai
   Sixth People's Hospital [ynlc201909]; Interdisciplinary Program of
   Shanghai Jiao Tong University [YG2022QN089]; Clinical Special Program of
   Shanghai Municipal Health Commission [20224044]; Chronic Disease Health
   Management and Comprehensive Intervention Based on Big Data Application;
   Research on Health Management Strategy and Application of Elderly
   Population [GWVI-11.1-28]
FX This work was supported by National Key R & D Program of China
   (2022YFC2502802), National Natural Science Foundation of China (62272298
   and 8238810007), Beijing Natural Science Foundation (IS23096), the
   College-level Project Fund of Shanghai Sixth People's Hospital
   (ynlc201909), the Interdisciplinary Program of Shanghai Jiao Tong
   University (YG2022QN089), Clinical Special Program of Shanghai Municipal
   Health Commission (20224044), Chronic Disease Health Management and
   Comprehensive Intervention Based on Big Data Application (GWVI-8), and
   Research on Health Management Strategy and Application of Elderly
   Population (GWVI-11.1-28).
CR Abbas Q, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21206936
   Abbas Q, 2020, MULTIMED TOOLS APPL, V79, P31595, DOI 10.1007/s11042-020-09630-x
   Akbar S, 2018, ARTIF INTELL MED, V90, P15, DOI 10.1016/j.artmed.2018.06.004
   Arsalan M, 2022, J PERS MED, V12, DOI 10.3390/jpm12010007
   Badawi SA, 2022, J DIGIT IMAGING, V35, P281, DOI 10.1007/s10278-021-00545-z
   Cavallari M, 2015, BIOMED RES INT, V2015, DOI 10.1155/2015/752957
   Cheung CY, 2022, NAT REV DIS PRIMERS, V8, DOI 10.1038/s41572-022-00342-0
   Chhajer B., 2014, High Blood Pressure
   Dai L, 2024, NAT MED, DOI 10.1038/s41591-023-02702-z
   Dai L, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-23458-5
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fang HH, 2022, IEEE T MED IMAGING, V41, P2828, DOI 10.1109/TMI.2022.3172773
   Fu YP, 2022, VISUAL COMPUT, V38, P3243, DOI 10.1007/s00371-022-02559-2
   Ganaie MA, 2022, Arxiv, DOI arXiv:2104.02395
   Guan Z., 2023, Cell Rep. Med.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Holm S, 2017, J MED IMAGING, V4, DOI 10.1117/1.JMI.4.1.014503
   Kanavati F, 2021, PR MACH LEARN RES, V143, P338
   Kauppi T., 2007, BMVC, V1
   Khened M, 2019, MED IMAGE ANAL, V51, P21, DOI 10.1016/j.media.2018.10.004
   Kumar A., 2022, arXiv
   Li XJ, 2020, VISUAL COMPUT, V36, P39, DOI 10.1007/s00371-018-1582-y
   Li YY, 2023, VISUAL COMPUT, V39, P2223, DOI 10.1007/s00371-021-02328-7
   Liu RH, 2022, PATTERNS, V3, DOI 10.1016/j.patter.2022.100512
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Loshchilov I, 2019, Arxiv, DOI [arXiv:1711.05101, 10.48550/arXiv.1711.05101, DOI 10.48550/ARXIV.1711.05101]
   Nagpal D, 2021, PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON INVENTIVE COMPUTATION TECHNOLOGIES (ICICT 2021), P924, DOI 10.1109/ICICT50816.2021.9358746
   Organization W.H., 1996, Hypertension control: report of a WHO Expert Committee
   Orlando JI, 2020, MED IMAGE ANAL, V59, DOI 10.1016/j.media.2019.101570
   Pavao A., 2022, Codalab competitions: an open source platform to organize scientific challenges
   Poplin R, 2018, NAT BIOMED ENG, V2, P158, DOI 10.1038/s41551-018-0195-0
   Porwal P, 2020, MED IMAGE ANAL, V59, DOI 10.1016/j.media.2019.101561
   Qian B, 2024, PATTERNS, V5, DOI 10.1016/j.patter.2024.100929
   Radford A, 2021, PR MACH LEARN RES, V139
   Rajagopalan S, 2018, J AM COLL CARDIOL, V72, P2054, DOI 10.1016/j.jacc.2018.07.099
   Sajid MZ, 2023, DIAGNOSTICS, V13, DOI 10.3390/diagnostics13081439
   Shajini M, 2022, VISUAL COMPUT, V38, P3551, DOI 10.1007/s00371-021-02178-3
   Sheng B, 2024, SCI BULL, V69, P583, DOI 10.1016/j.scib.2024.01.004
   Silva-Rodriguez J, 2023, Arxiv, DOI arXiv:2308.07898
   Staal J, 2004, IEEE T MED IMAGING, V23, P501, DOI 10.1109/TMI.2004.825627
   Suman S, 2023, COMPUT METH PROG BIO, V240, DOI 10.1016/j.cmpb.2023.107627
   Tsukikawa M, 2020, CLIN OPTOM, V12, P67, DOI 10.2147/OPTO.S183492
   Wiesenfarth M, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-82017-6
   Wong TY, 2004, NEW ENGL J MED, V351, P2310, DOI 10.1056/NEJMra032865
   Wu XW, 2020, NEUROCOMPUTING, V396, P39, DOI 10.1016/j.neucom.2020.01.085
   Xie FY, 2017, IEEE T MED IMAGING, V36, P849, DOI 10.1109/TMI.2016.2633551
   Zhang L, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0233166
   Zhou B, 2021, LANCET, V398, P957, DOI 10.1016/S0140-6736(21)01330-1
   Zhu CZ, 2017, COMPUT MED IMAG GRAP, V55, P68, DOI 10.1016/j.compmedimag.2016.05.004
NR 49
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 8
PY 2024
DI 10.1007/s00371-024-03384-5
EA JUN 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TO4B7
UT WOS:001242179800001
DA 2024-08-05
ER

PT J
AU Skala, V
AF Skala, Vaclav
TI A new fully projective O(lg N) line convex polygon intersection
   algorithm
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Intersection computation; Projective space; Convex polygon;
   Computational complexity; Line clipping; Duality
ID EFFICIENT ALGORITHM; COMPUTATION; PRINCIPLE
AB Intersecting algorithms, especially line clipping in E 2 \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$${E<^>{2}}$$\end{document} and E 3 \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$${E<^>{3}}$$\end{document} in computer graphics, have been studied for a long time. Many different algorithms have been developed. The simplest case is a line clipping by a convex polygon in E 2 \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$${E<^>{2}}$$\end{document} with O(N) computational complexity and with known polygon edges orientation. This contribution presents a new algorithm for a line clipping by a convex polygon in E 2 \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$<^>{2}$$\end{document} with O(lg N) complexity, which is based on the point-in-half plane test. The proposed algorithm does not require prior knowledge of the polygon edge orientation. The vertices of the convex polygon and the clipped line can be given in projective space using homogeneous coordinates. The algorithm uses vector-vector operations for efficient implementation with SSE or AVX vector-vector instructions or on GPUs. It is simple and robust.
C1 [Skala, Vaclav] Univ West Bohemia, Dept Comp Sci & Engn, Univerzitni 8, CZ-30100 Plzen, Czech Republic.
C3 University of West Bohemia Pilsen
RP Skala, V (corresponding author), Univ West Bohemia, Dept Comp Sci & Engn, Univerzitni 8, CZ-30100 Plzen, Czech Republic.
EM skala@kiv.zcu.cz
RI Skala, Vaclav/F-9141-2011
OI Skala, Vaclav/0000-0001-8886-4281
FU University of West Bohemia
FX The author thanks colleagues and students at the University of West
   Bohemia in Pilsen for their comments and recommendations, as they
   stimulated this work and suggestions proposed, to anonymous reviewers
   for hints, constructive suggestions and critical comments. Thanks also
   to colleagues at the Shandong University in Jinan and Zhejiang
   University in Hangzhou, China, for recent discussions and sharing their
   knowledge. Thanks also belong to several authors of recently published
   relevant papers [22] for sharing their views and hints provided.
CR Agoston M.K., 2005, COMPUTER GRAPHICS GE, DOI DOI 10.1007/B138805
   Agoston MK., 2005, Computer Graphics and Geometric Modelling: Mathematics, DOI [10.1007/b138899, DOI 10.1007/B138899]
   Angel E., 2011, Interactive Computer Graphics: A Top-Down Approach with Shader-Based OpenGL, V6
   [Anonymous], 2018, REAL TIME RENDERING
   [Anonymous], 2010, Computer Graphics with Open GL
   [Anonymous], 1990, Computer Graphics: Principles and Practice
   AROKIASAMY A, 1989, COMPUT GRAPH, V13, P99, DOI 10.1016/0097-8493(89)90045-9
   Bourke P., 1997, Determining if a point lies on the interior of a polygon
   Bui DH, 1998, VISUAL COMPUT, V14, P31, DOI 10.1007/s003710050121
   Cohen D., 1969, Technical report
   Comninos P., 2005, Mathematical and Computer Programming Techniques for Computer Graphics, DOI [10.1007/978-1-84628-292-8, DOI 10.1007/978-1-84628-292-8]
   Coxeter HSM., 1992, The Real Projective Plane
   CYRUS M, 1978, COMPUT GRAPH, V3, P23, DOI 10.1016/0097-8493(78)90021-3
   Eberly DH., 2003, Game Physics, DOI [10.1201/9781482282801, DOI 10.1201/9781482282801]
   Ferguson RS., 2013, Practical Algorithms for 3D Computer Graphics, V2, DOI [10.1201/b16333, DOI 10.1201/B16333]
   Franklin W.R., 1994, PNPOLY-Point Inclusion in Polygon Test
   Govil-Pai S., 2005, Principles of Computer Graphics: Theory and Practice Using OpenGL and Maya
   Haines E., 1994, Point in polygon strategies, P24, DOI [10.1016/B978-0-12-336156-1.50013-6, DOI 10.1016/B978-0-12-336156-1.50013-6]
   Hill FS., 2006, Computer Graphics Using OpenGL, V3
   Hughes J.F., 2014, Computer graphics: principles and practice, V3rd ed.
   Johnson M., 1996, Mathematics Today, P138
   Lengyel E., 2011, Mathematics for 3D Game Programming and Computer Graphics
   LIANG YD, 1984, ACM T GRAPHIC, V3, P1, DOI 10.1145/357332.357333
   Ochilbek R, 2018, INT CONF ELECT COMP
   Pharr M., 2016, Physically based rendering: From theory to implementation
   Rappoport A., 1991, Visual Computer, V7, P19, DOI 10.1007/BF01994114
   Salomon D., 2006, Transformations and Projections in Computer Graphics, DOI [10.1007/978-1-4612-1504-23, DOI 10.1007/978-1-4612-1504-23]
   Salomon D., 2011, The Computer Graphics Manual, P1, DOI DOI 10.1007/978-0-85729-886-7
   Salomon D., 1999, Computer Graphics and Geometric Modeling, V1, DOI [10.1007/978-1-4612-1504-2, DOI 10.1007/978-1-4612-1504-2]
   Schneider P.J., 2003, Geometric tools for computer graphics, P1, DOI [10.1016/B978-1-55860-594-7.50025-4, DOI 10.1016/B978-1-55860-594-7.50025-4]
   SHIMRAT M, 1962, COMMUN ACM, V5, P434, DOI 10.1145/368637.368653
   Shirley P., 2009, Fundamentals of Computer Graphics, V3, DOI [10.1201/9781439865521, DOI 10.1201/9781439865521]
   SKALA V, 1990, CG INTERNATIONAL 90, P255
   Skala Vaclav, 2021, WSEAS Transactions on Systems, P320, DOI 10.37394/23202.2021.20.36
   Skala V, 2020, INT J MATH COMPUT SC, V15, P769
   Skala V, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P270, DOI 10.1109/CGI.2004.1309220
   Skala Vaclav, 2012, WSEAS Transactions on Computers, V11, P275
   SKALA V, 1993, COMPUT GRAPH-UK, V17, P417, DOI 10.1016/0097-8493(93)90030-D
   Skala Vaclav, 2010, 4th International Conference on Applied Mathematics, Simulation, Modelling (ASM 2010), P66
   Skala V., 1989, EUROGRAPHICS '89. Proceedings of the European Computer Graphics Conference, P355
   Skala Vaclav, 2010, WSEAS Transactions on Mathematics, V9, P407
   Skala V, 2001, COMPUTER GRAPHICS INTERNATIONAL 2001, PROCEEDINGS, P167, DOI 10.1109/CGI.2001.934671
   SKALA V, 1994, COMPUT GRAPH, V18, P517, DOI 10.1016/0097-8493(94)90064-7
   Skala V, 1996, COMPUT GRAPH, V20, P523, DOI 10.1016/0097-8493(96)00024-6
   Skala V, 1996, COMPUT GRAPH FORUM, V15, P61, DOI 10.1111/1467-8659.1510061
   Skala V, 1997, COMPUT GRAPH, V21, P209, DOI 10.1016/S0097-8493(96)00084-2
   Skala V., 1996, Mach. Graph. Vis., V5, P483
   Skala V., 2014, Commun. Comput. Inform. Sci, V434, P642, DOI [10.1007/978-3-319-07857-1113, DOI 10.1007/978-3-319-07857-1113]
   Skala V, 2006, INT J IMAGE GRAPH, V6, P625, DOI 10.1142/S0219467806002422
   Skala V, 2008, COMPUT GRAPH-UK, V32, P120, DOI 10.1016/j.cag.2007.09.007
   Skala V, 2023, INFORMATICA-LITHUAN, V34, P169, DOI 10.15388/23-INFOR508
   Skala V, 2021, LECT NOTES COMPUT SC, V12953, P16, DOI 10.1007/978-3-030-86976-2_2
   Skala V, 2020, ANN MATH INFORM, V52, P199, DOI 10.33039/ami.2020.05.001
   Skala V, 2017, AIP CONF PROC, V1863, DOI 10.1063/1.4992684
   Skala V, 2016, AIP CONF PROC, V1738, DOI 10.1063/1.4952270
   Skala V, 2015, AIP CONF PROC, V1648, DOI 10.1063/1.4913058
   Skala V, 2008, INT J IMAGE GRAPH, V8, P615, DOI 10.1142/S021946780800326X
   Theoharis T., 2008, Graphics and Visualization: Principles & Algorithms, V1st, DOI [10.1201/b10676, DOI 10.1201/B10676]
   Thomas A., 2008, Integrated Graphic and Computer Modelling, V1, DOI [10.1007/978-1-84800-179-4, DOI 10.1007/978-1-84800-179-4]
   Vince J., 2010, Introduction to the Mathematics for Computer Graphics, V3, DOI [10.1007/978-1-4471-6290-2, DOI 10.1007/978-1-4471-6290-2]
   Vince JA., 2008, Geometric Algebra for Computer Graphics, V1, DOI [10.1007/978-1-84628-997-2, DOI 10.1007/978-1-84628-997-2]
   Vince J, 2009, GEOMETRIC ALGEBRA: AN ALGEBRAIC SYSTEM FOR COMPUTER GAMES AND ANIMATION, P1
   Watt A., 1990, Fundamentals of Three-Dimensional Computer Graphics
   Weiler K., 1977, ACM SIGGRAPH COMPUTE, V11, P214, DOI DOI 10.1145/965141.563896
   Wikipedia contributors, 2023, Streaming SIMD Extensions-Wikipedia, The Free Encyclopedia
   Wikipedia contributors, 2024, Advanced Vector Extensions-Wikipedia, The Free Encyclopedia
NR 66
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 6
PY 2024
DI 10.1007/s00371-024-03413-3
EA JUN 2024
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TO4Y3
UT WOS:001242202400002
OA hybrid
DA 2024-08-05
ER

PT J
AU Yuan, ZW
   Tang, PX
   Sang, XG
   Zhang, F
   Zhang, ZQ
AF Yuan, Zhengwu
   Tang, Peixian
   Sang, Xinguang
   Zhang, Fan
   Zhang, Zheqi
TI Visionary: vision-aware enhancement with reminding scenes generated by
   captions via multimodal transformer for embodied referring expression
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Embodied artificial intelligence; Vision-and-language navigation;
   Cross-modal learning; Image captioning; Text-to-image generation
ID LANGUAGE
AB Embodied referring expression (REVERIE) is a challenging task that requires an embodied agent to autonomously navigate in unseen environment and locate the target object specified in the given instruction. One main challenge in this task is the scarcity of data, leading to low generalization ability and poor performance of the agent in unseen environments. To address these issues, we propose VISIONARY (VISION-Aware enhancement with Reminding scenes generated by captions), which leverages advanced pre-trained models as a source of common sense and generates additional valuable information from both linguistic and visual aspects based on the embodied agent's visual input to guide the navigation decision-making process. Specifically, the reminding scene generation mechanism is proposed to describe the observed scene in detail and generate corresponding reminding scenes, which can effectively enrich the input and serve as a supplement to the training data. Additionally, the caption-aware module and the adaptive fusion module are proposed to, respectively, inject the generated scene description and reminding scene into the model, thereby enhancing the navigation efficiency and generalization ability of the agent. Extensive experiments conducted on the REVERIE benchmark demonstrate the effectiveness of our proposed methods, achieving improvements of 2.34 and 2.32% on the key metrics SPL and RGSPL, respectively, in unseen environments compared to the previous state-of-the-art method. The code is available at https://github.com/tpxbps/visionary.
C1 [Yuan, Zhengwu; Tang, Peixian; Sang, Xinguang; Zhang, Fan; Zhang, Zheqi] Chongqing Univ Posts & Telecommun, Sch Comp Sci & Technol, Chongqing 400065, Peoples R China.
   [Yuan, Zhengwu] Minist Culture & Tourism, Key Lab Tourism Multisource Data Percept & Decis, Chongqing 400065, Peoples R China.
C3 Chongqing University of Posts & Telecommunications
RP Tang, PX (corresponding author), Chongqing Univ Posts & Telecommun, Sch Comp Sci & Technol, Chongqing 400065, Peoples R China.
EM yuanzw@cqupt.edu.cn; s220232052@stu.cqupt.edu.cn;
   D210201020@stu.cqupt.edu.cn; s220201130@stu.cqupt.edu.cn;
   1692835485@qq.com
FU Chongqing Municipal Education Commission
FX No Statement Available
CR Anderson P, 2018, Arxiv, DOI arXiv:1807.06757
   Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   Auer S, 2007, LECT NOTES COMPUT SC, V4825, P722, DOI 10.1007/978-3-540-76298-0_52
   Brown T. B., 2020, P 34 INT C NEURAL IN, P1
   Chang AE, 2017, Arxiv, DOI arXiv:1709.06158
   Chen H, 2019, PROC CVPR IEEE, P12530, DOI 10.1109/CVPR.2019.01282
   Chen J., 2023, COMPUTER GRAPHICS IN, P455
   Chen S., 2021, ADV NEURAL INF PROCE, V34, P5834
   Chen SZ, 2022, LECT NOTES COMPUT SC, V13699, P638, DOI 10.1007/978-3-031-19842-7_37
   Chen SZ, 2022, PROC CVPR IEEE, P16516, DOI 10.1109/CVPR52688.2022.01604
   Dai J, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2072
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Feng YCJ, 2024, IEEE T VIS COMPUT GR, V30, P295, DOI 10.1109/TVCG.2023.3327168
   Fengda Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10009, DOI 10.1109/CVPR42600.2020.01003
   Fried D, 2018, ADV NEUR IN, V31
   Gao C, 2024, IEEE T PATTERN ANAL, V46, P994, DOI 10.1109/TPAMI.2023.3326851
   Gao C, 2021, PROC CVPR IEEE, P3063, DOI 10.1109/CVPR46437.2021.00308
   Gu J, 2022, arXiv
   Guhur P.-L., 2021, ICCV, P1634, DOI 10.1109/ICCV48922.2021.00166
   Hong YC, 2021, Arxiv, DOI arXiv:2011.13922
   Huang HS, 2019, IEEE I CONF COMP VIS, P7403, DOI 10.1109/ICCV.2019.00750
   Jain V, 2019, Arxiv, DOI arXiv:1905.12255
   Koh Jing Yu, 2021, P IEEE CVF INT C COM, P14738
   Kojima T, 2022, ADV NEUR IN
   Kolve E, 2022, Arxiv, DOI arXiv:1712.05474
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Ku ALXD, 2020, Arxiv, DOI arXiv:2010.07954
   Li CL, 2022, Arxiv, DOI arXiv:2205.12005
   Li HY, 2022, PROC CVPR IEEE, P4156, DOI 10.1109/CVPR52688.2022.00413
   Li HY, 2022, IEEE T IMAGE PROCESS, V31, P4637, DOI 10.1109/TIP.2022.3186536
   Li JL, 2023, Arxiv, DOI arXiv:2305.19195
   Li JL, 2022, PROC CVPR IEEE, P15386, DOI 10.1109/CVPR52688.2022.01497
   Li JN, 2023, Arxiv, DOI arXiv:2301.12597
   Li M., 2023, P AAAI C ARTIFICIAL, V37, P1386
   Li XY, 2023, PROC CVPR IEEE, P2583, DOI 10.1109/CVPR52729.2023.00254
   Lin X., 2021, Eapt: efficient attention pyramid transformer for image processing
   Lin XR, 2021, PROC CVPR IEEE, P7032, DOI 10.1109/CVPR46437.2021.00696
   Liu C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1624, DOI 10.1109/ICCV48922.2021.00167
   Loshchilov I, 2019, Arxiv, DOI [arXiv:1711.05101, 10.48550/arXiv.1711.05101, DOI 10.48550/ARXIV.1711.05101]
   Nichol A, 2022, Arxiv, DOI arXiv:2112.10741
   Padmakumar A, 2022, AAAI CONF ARTIF INTE, P2017
   Qi MS, 2019, PROC CVPR IEEE, P5232, DOI 10.1109/CVPR.2019.00538
   Radford A, 2021, PR MACH LEARN RES, V139
   Ramakrishnan S. K., 2021, arXiv
   Ramesh Aditya., 2022, ARXIV PREPRINT ARXIV, V1.2, P3, DOI DOI 10.48550/ARXIV.2204.06125
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Ross S., 2011, INT C ARTIFICIAL INT, P627, DOI DOI 10.48550/ARXIV.1011.0686
   Saharia C., 2022, ADV NEURAL INF PROCE, V35, P36479
   Savva M, 2019, IEEE I CONF COMP VIS, P9338, DOI 10.1109/ICCV.2019.00943
   Speer R, 2017, AAAI CONF ARTIF INTE, P4444
   Stefanini M, 2023, IEEE T PATTERN ANAL, V45, P539, DOI 10.1109/TPAMI.2022.3148210
   Su JL, 2024, NEUROCOMPUTING, V568, DOI 10.1016/j.neucom.2023.127063
   Tan H, 2019, Arxiv, DOI arXiv:1908.07490
   Tan H, 2019, Arxiv, DOI arXiv:1904.04195
   Thomason J., 2020, PMLR, P394
   Tsu-Jui Fu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P71, DOI 10.1007/978-3-030-58539-6_5
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Z., 2023, P IEEECVF INT C COMP, P12009
   Wei R, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2155
   Xia F, 2018, PROC CVPR IEEE, P9068, DOI 10.1109/CVPR.2018.00945
   Xin Wang, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P6622, DOI 10.1109/CVPR.2019.00679
   Yuankai Qi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9979, DOI 10.1109/CVPR42600.2020.01000
   Zareian Alireza, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P606, DOI 10.1007/978-3-030-58592-1_36
   Zhao M, 2021, 16TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (EACL 2021), P1302
   Zhu FD, 2021, PROC CVPR IEEE, P12684, DOI 10.1109/CVPR46437.2021.01250
NR 65
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 30
PY 2024
DI 10.1007/s00371-024-03469-1
EA MAY 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO8V9
UT WOS:001235494300004
DA 2024-08-05
ER

PT J
AU Wang, PJ
   Yang, K
   Yuan, CZ
   Li, HJ
   Tang, W
   Yang, XS
AF Wang, Pengjie
   Yang, Kang
   Yuan, Chengzhi
   Li, Houjie
   Tang, Wen
   Yang, Xiaosong
TI Few-shot anime pose transfer
SO VISUAL COMPUTER
LA English
DT Review
DE Generative adversarial networks; Anime generation; Image generation;
   Video generation; Meta-learning
AB In this paper, we propose a few-shot method for pose transfer of anime characters-given a source image of an anime character and a target pose, we transfer the pose of the target to the source character. Despite recent advances in pose transfer on real people images, these methods typically require large numbers of training images of different person under different poses to achieve reasonable results. However, anime character images are expensive to obtain they are created with a lot of artistic authoring. To address this, we propose a meta-learning framework for few-shot pose transfer, which can well generalize to an unseen character given just a few examples of the character. Further, we propose fusion residual blocks to align the features of the source and target so that the appearance of the source character can be well transferred to the target pose. Experiments show that our method outperforms leading pose transfer methods, especially when the source characters are not in the training set.
C1 [Wang, Pengjie; Yang, Kang] Dalian Minzu Univ, Sch Comp Sci & Engn, Dalian, Peoples R China.
   [Yuan, Chengzhi; Li, Houjie] Dalian Minzu Univ, Sch Informat & Commun Engn, Dalian, Peoples R China.
   [Wang, Pengjie; Tang, Wen; Yang, Xiaosong] Bournemouth Univ, Natl Ctr Comp Animat, Bournemouth, England.
   [Yang, Kang] Chongqing Univ Posts & Telecommun, Sch Comp Sci & Technol, Chongqing, Peoples R China.
C3 Dalian Minzu University; Dalian Minzu University; Bournemouth
   University; Chongqing University of Posts & Telecommunications
RP Yang, XS (corresponding author), Bournemouth Univ, Natl Ctr Comp Animat, Bournemouth, England.
EM xyang@bournemouth.ac.uk
RI Wang, Pengjie/KHE-3288-2024
FU Horizon 2020 [900025]; European Union
FX Results incorporated in this paper have received funding from the
   European Union's Horizon 2020 research and innovation programme under
   the Marie Sk & lstrok;odowska-Curie grant agreement No 900025. Thanks to
   Miaomiao Chen for her carefully editing of the submission. Chenzhi Yuan
   conducted most of the experiments for the submission, while Kang Yang
   handled other experiments.
CR Aberman K., 2019, ARXIV
   AlBahar B, 2019, IEEE I CONF COMP VIS, P9015, DOI 10.1109/ICCV.2019.00911
   Antoniou A., 2018, ARXIV
   Apostolidis E, 2021, IEEE T CIRC SYST VID, V31, P3278, DOI 10.1109/TCSVT.2020.3037883
   Balakrishnan G, 2018, PROC CVPR IEEE, P8340, DOI 10.1109/CVPR.2018.00870
   Branwen G., 2019, DANBOORU2019 LARGE S
   Chan C, 2019, IEEE I CONF COMP VIS, P5932, DOI 10.1109/ICCV.2019.00603
   Chen BY, 2022, IEEE T CIRC SYST VID, V32, P302, DOI 10.1109/TCSVT.2021.3059706
   Esser P, 2018, PROC CVPR IEEE, P8857, DOI 10.1109/CVPR.2018.00923
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Finn C, 2017, PR MACH LEARN RES, V70
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Hao Tang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P717, DOI 10.1007/978-3-030-58595-2_43
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heusel M., 2017, NeurIPS, P6629
   Ioffe Sergey, 2015, INT C MACHINE LEARNI, V37, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jaw DW, 2021, IEEE T CIRC SYST VID, V31, P1342, DOI 10.1109/TCSVT.2020.3003025
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T., 2017, ARXIV
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kingma D. P., 2014, arXiv
   Kingma D. P., 2013, ARXIV
   Li JF, 2019, PROC CVPR IEEE, P10855, DOI 10.1109/CVPR.2019.01112
   Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065
   Liu W, 2019, IEEE I CONF COMP VIS, P5903, DOI 10.1109/ICCV.2019.00600
   Ma LQ, 2017, ADV NEUR IN, V30
   Miyato T., 2018, ARXIV
   Nair V., 2010, Proceedings of the 27th International Conference on Machine Learning (ICML-10), P807
   Ren YR, 2020, IEEE T IMAGE PROCESS, V29, P8622, DOI 10.1109/TIP.2020.3018224
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Uchida Y, 2018, P EUR C COMP VIS ECC
   van den Oord A., 2016, INT C MACHINE LEARNI, P1747
   Wang M, 2019, PROC CVPR IEEE, P1495, DOI 10.1109/CVPR.2019.00159
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiu Y., 2018, ARXIV
   Xu SX, 2021, IEEE T CIRC SYST VID, V31, P1308, DOI 10.1109/TCSVT.2020.3001267
   Yuan MK, 2020, IEEE T CIRC SYST VID, V30, P4258, DOI 10.1109/TCSVT.2019.2953753
   Zakharov E, 2019, IEEE I CONF COMP VIS, P9458, DOI 10.1109/ICCV.2019.00955
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407
   Zhang JS, 2021, PROC CVPR IEEE, P7978, DOI 10.1109/CVPR46437.2021.00789
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu Z, 2019, PROC CVPR IEEE, P2342, DOI 10.1109/CVPR.2019.00245
NR 45
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4635
EP 4646
DI 10.1007/s00371-024-03447-7
EA MAY 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001234594700002
OA hybrid
DA 2024-08-05
ER

PT J
AU Li, HH
   Zhu, JH
   Wen, GH
   Zhong, HY
AF Li, Huihui
   Zhu, Junhao
   Wen, Guihua
   Zhong, Haoyang
TI Structural self-contrast learning based on adaptive weighted negative
   samples for facial expression recognition
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Facial expression recognition; Deep learning; Earth mover's distance;
   Contrast learning
ID REPRESENTATION; NETWORK
AB Face expression recognition in the wild faces challenges such as small data size, low quality images, and noisy labels. In order to solve these problems, this paper proposes a novel structural self-contrast learning method (SSCL) based on adaptively weighted negative samples. Firstly, two augmented variants are generated for each input sample as positive samples, which should be as similar in structure as possible. Secondly, EMD (Earth's Mover Distance) is used to define the structural similarity between the two augmented samples and extract the features in a self-supervised way. Subsequently, the concept of negative samples is further introduced into SSCL, and the augmented variants of other categories are generated as negative samples, which makes the structural similarity between positive samples increase and between negative samples simultaneously decrease. Finally, an adaptive weighting method for the negative is proposed, which aims to solve the imbalance between positive samples and negative samples, mine difficult negative samples, and reduce their similarity with the input image in an adaptive way. The extensive experiments on benchmark datasets demonstrate that SSCL outperforms the state-of-the-art methods. The cross-dataset evaluation shows the superior generalization capability of the proposed method. The code will be available at https://github.com/zjhzhy/SSCL.
C1 [Li, Huihui; Zhong, Haoyang] Guangdong Polytech Normal Univ, Sch Comp Sci, Zhongshan Ave, Guangzhou 510665, Guangdong, Peoples R China.
   [Zhu, Junhao; Wen, Guihua] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Guangdong, Peoples R China.
C3 Guangdong Polytechnic Normal University; South China University of
   Technology
RP Wen, GH (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Guangdong, Peoples R China.
EM lihh@gpnu.edu.cn; cszjh@mail.scut.edu.cn; crghwen@scut.edu.cn;
   928388463@qq.com
FU Basic and Applied Basic Research Foundation of Guangdong Province
   [2023A1515010939]; Guangdong Basic and Applied Basic Research Foundation
   [62006049, 62176095]; National Natural Science Foundation of China
   [2022KTSCX068]; Project of Education Department of Guangdong Province
   [2020B1111120001]; Guangdong Province Key Area R and D Plan Project
FX The work was supported by Guangdong Basic and Applied Basic Research
   Foundation(Grant No. 2023A1515010939), National Natural Science
   Foundation of China(Grant Nos.62006049, 62176095), Project of Education
   Department of Guangdong Province (Grant No.2022KTSCX068), Guangdong
   Province Key Area R and D Plan Project (Grant No.2020B1111120001).
CR Chen JY, 2022, IEEE T IND INFORM, V18, P16, DOI 10.1109/TII.2021.3075989
   Chen T., 2020, Adv. Neural Inf. Process. Syst., V33, P22243, DOI DOI 10.48550/ARXIV.2006.10029
   Chen TS, 2022, IEEE T PATTERN ANAL, V44, P9887, DOI 10.1109/TPAMI.2021.3131222
   Chen T, 2020, Arxiv, DOI [arXiv:2002.05709, DOI 10.48550/ARXIV.2002.05709]
   Chen ZH, 2023, IEEE T PATTERN ANAL, V45, P13489, DOI 10.1109/TPAMI.2023.3293885
   Chi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12200, DOI 10.1109/CVPR42600.2020.01222
   Chowanda A, 2021, J BIG DATA-GER, V8, DOI 10.1186/s40537-021-00522-x
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Dosovitskiy A, 2014, ADV NEUR IN, V27
   Fang B, 2023, IEEE ACCESS, V11, P45547, DOI 10.1109/ACCESS.2023.3274193
   Fetterman A., 2020, Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning
   Florea C., 2019, BMVC, P104, DOI DOI 10.1109/ECAI50035.2020.9223242
   Fu YJ, 2020, IEEE T IMAGE PROCESS, V29, P6535, DOI 10.1109/TIP.2020.2991510
   Gong WJ, 2022, NEURAL COMPUT APPL, V34, P10175, DOI 10.1007/s00521-022-07016-8
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Jin X, 2021, IEEE T IMAGE PROCESS, V30, P7143, DOI 10.1109/TIP.2021.3101820
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Khosla P., 2020, Adv. Neural Inf. Process. Syst, P18661, DOI DOI 10.48550/ARXIV.2004.11362
   Li HY, 2024, IEEE T AFFECT COMPUT, V15, P173, DOI 10.1109/TAFFC.2023.3263886
   Li HY, 2022, PROC CVPR IEEE, P4156, DOI 10.1109/CVPR52688.2022.00413
   Li HY, 2022, IEEE T IMAGE PROCESS, V31, P4637, DOI 10.1109/TIP.2022.3186536
   Li HY, 2021, NEUROCOMPUTING, V432, P159, DOI 10.1016/j.neucom.2020.12.076
   Li HY, 2021, IEEE T IMAGE PROCESS, V30, P2016, DOI 10.1109/TIP.2021.3049955
   Li HH, 2023, VISUAL COMPUT, V39, P4709, DOI 10.1007/s00371-022-02619-7
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Li M, 2021, IEEE T AFFECT COMPUT, V12, P544, DOI 10.1109/TAFFC.2018.2880201
   Li S, 2022, IEEE T AFFECT COMPUT, V13, P881, DOI 10.1109/TAFFC.2020.2973158
   Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277
   Li Y, 2018, INT C PATT RECOG, P2209, DOI 10.1109/ICPR.2018.8545853
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P2439, DOI 10.1109/TIP.2018.2886767
   Li YF, 2021, AAAI CONF ARTIF INTE, V35, P8547
   Liang H, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2165
   Liang LQ, 2021, IEEE T INF FOREN SEC, V16, P482, DOI 10.1109/TIFS.2020.3007327
   Liao L, 2022, MACH VISION APPL, V33, DOI 10.1007/s00138-022-01288-9
   Lin SS, 2021, IEEE T MULTIMEDIA, V23, P1581, DOI 10.1109/TMM.2020.3001497
   Liu C., 2022, Vis. Comput., P1
   Liu C, 2023, INFORM SCIENCES, V619, P781, DOI 10.1016/j.ins.2022.11.068
   Liu P, 2022, IEEE T CYBERNETICS, V52, P12649, DOI 10.1109/TCYB.2021.3085744
   Lucey P., 2010, 2010 IEEE COMP SOC C, P94, DOI DOI 10.1109/CVPRW.2010.5543262
   Meng ZB, 2017, IEEE INT CONF AUTOMA, P558, DOI 10.1109/FG.2017.140
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Pan BW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P566, DOI 10.1145/3343031.3351049
   Poux D, 2022, IEEE T IMAGE PROCESS, V31, P446, DOI 10.1109/TIP.2021.3129120
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Sharma V, 2020, IEEE INT CONF AUTOMA, P109, DOI 10.1109/FG47880.2020.00011
   She JH, 2021, PROC CVPR IEEE, P6244, DOI 10.1109/CVPR46437.2021.00618
   Sheng B, 2022, IEEE T CYBERNETICS, V52, P6662, DOI 10.1109/TCYB.2021.3079311
   Tang Y, 2021, IEEE T IMAGE PROCESS, V30, P444, DOI 10.1109/TIP.2020.3037467
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   van Engelen JE, 2020, MACH LEARN, V109, P373, DOI 10.1007/s10994-019-05855-6
   Vasudeva K, 2023, MULTIMED TOOLS APPL, V82, P31351, DOI 10.1007/s11042-023-14803-5
   Vo TH, 2020, IEEE ACCESS, V8, P131988, DOI 10.1109/ACCESS.2020.3010018
   Wang C, 2022, IEEE T CIRC SYST VID, V32, P1834, DOI 10.1109/TCSVT.2021.3083326
   Wang K, 2020, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR42600.2020.00693
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Xiao JH, 2023, APPL SOFT COMPUT, V141, DOI 10.1016/j.asoc.2023.110312
   Xie Y, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1255, DOI 10.1145/3394171.3413822
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Zeng D, 2022, PROC CVPR IEEE, P20259, DOI 10.1109/CVPR52688.2022.01965
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang T, 2022, IEEE T KNOWL DATA EN, V34, P544, DOI 10.1109/TKDE.2020.2985365
   Zhang Y., 2024, AAAI
   Zhang Y., 2021, Neural Information Processing Systems
   Zhang Y., 2023, 37 C NEURAL INFORM P
   Zhang YH, 2022, LECT NOTES COMPUT SC, V13686, P418, DOI 10.1007/978-3-031-19809-0_24
   Zhang ZY, 2023, INFORM SCIENCES, V630, P370, DOI 10.1016/j.ins.2023.02.056
   Zhao Shuwen, 2018, BMVC, V12, P317
   Zhao ZQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1553, DOI 10.1145/3474085.3475292
   Zhao ZQ, 2021, AAAI CONF ARTIF INTE, V35, P3510
   Zhao ZQ, 2021, IEEE T IMAGE PROCESS, V30, P6544, DOI 10.1109/TIP.2021.3093397
   Zhou L., 2021, Neural Comput. Appl., P1
   Zhou LY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2964, DOI 10.1145/3394171.3413515
   Zhuang FZ, 2021, P IEEE, V109, P43, DOI 10.1109/JPROC.2020.3004555
NR 77
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 15
PY 2024
DI 10.1007/s00371-024-03349-8
EA APR 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OB0Q4
UT WOS:001204686400002
DA 2024-08-05
ER

PT J
AU Hu, YX
   Wu, PP
   Zhang, B
   Sun, WH
   Gao, YR
   Hao, CX
   Chen, XR
AF Hu, Yanxiang
   Wu, Panpan
   Zhang, Bo
   Sun, Wenhao
   Gao, Yaru
   Hao, Caixia
   Chen, Xinran
TI A new multi-focus image fusion quality assessment method with
   convolutional sparse representation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Multi-focus image fusion; Fusion image quality assessment; Convolutional
   sparse representation; Multi-source joint layering; Feature-level
   analysis
ID INFORMATION MEASURE; PERFORMANCE
AB Assessing image fusion quality purposefully is a challenging task due to the diversities of fused features. In this work, a specific multi-focus image fusion quality assessment method is proposed based on joint image layering and convolutional sparse representation. Specifically, the proposed method includes two stages: Tikhonov regularization optimization-based joint image layering and convolutional sparse representation-based focus similarity comparison. The first stage decomposes the source images and their fusion result jointly into a common base layer and respective detail layers, and then, the second stage compares the focus similarity between these detail layers with their convolutional sparse features. The main novelty of our work is to assess fusion quality with learning features, rather than with those handcrafted low-level patterns. Consequently, our method has higher reliability and feature-level analytical ability. A large number of objective and subjective experiments demonstrate the effectiveness and specificity of the proposed method. Moreover, the applicability of the general blind natural image quality metrics for image fusion was also examined and discussed. Besides experiments, the feature-level characteristics of multi-focus image fusion were also investigated and analyzed with the proposed method. Our analysis reveals some potential laws that could provide new perspectives for fusion algorithm design and improvement.
C1 [Hu, Yanxiang; Wu, Panpan; Zhang, Bo; Sun, Wenhao; Gao, Yaru; Hao, Caixia; Chen, Xinran] Tianjin Normal Univ, Coll Comp & Informat Engn, Tianjin, Peoples R China.
C3 Tianjin Normal University
RP Hu, YX (corresponding author), Tianjin Normal Univ, Coll Comp & Informat Engn, Tianjin, Peoples R China.
EM huyanxiang@tjnu.edu.cn; pwu@tjnu.edu.cn; jxzb@tjnu.edu.cn;
   2110090017@stu.tjnu.edu.cn; gaoyaru@stu.tjnu.edu.cn;
   2210090007@stu.tjnu.edu.cn; 2210090023@stu.tjnu.edu.cn
OI WU, PANPAN/0000-0003-2915-2086
FU National Natural Science Foundation of China
FX The authors would like to thank the anonymous reviewers for their
   valuable comments. We would like to thank Professor Liu Zheng for
   providing fusion quality objective assessment toolbox and Professor O.
   Rockinger for providing image fusion toolbox.
CR Amin-Naji M, 2019, INFORM FUSION, V51, P201, DOI 10.1016/j.inffus.2019.02.003
   Cai RT, 2023, VISUAL COMPUT, V39, P4639, DOI 10.1007/s00371-022-02614-y
   Chen Y, 2009, IMAGE VISION COMPUT, V27, P1421, DOI 10.1016/j.imavis.2007.12.002
   Cvejic N, 2006, ELECTRON LETT, V42, P626, DOI 10.1049/el:20060693
   Cvejic N., 2005, INT J SIGNAL PROCESS, V2, P178
   Duan Z, 2020, IEEE ACCESS, V8, P135284, DOI 10.1109/ACCESS.2020.3010542
   Han Y, 2013, INFORM FUSION, V14, P127, DOI 10.1016/j.inffus.2011.08.002
   Hancheng Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14131, DOI 10.1109/CVPR42600.2020.01415
   Hossny M, 2008, ELECTRON LETT, V44, P1066, DOI 10.1049/el:20081754
   Hu YX, 2022, IET IMAGE PROCESS, V16, P216, DOI 10.1049/ipr2.12345
   Hu YX, 2019, J VIS COMMUN IMAGE R, V61, P225, DOI 10.1016/j.jvcir.2019.04.005
   Wei H, 2007, PATTERN RECOGN LETT, V28, P493, DOI 10.1016/j.patrec.2006.09.005
   Jagtap NS, 2022, VISUAL COMPUT, V38, P4353, DOI 10.1007/s00371-021-02300-5
   Jiang LM, 2022, APPL INTELL, V52, P339, DOI 10.1007/s10489-021-02358-7
   Lai R, 2019, IEEE ACCESS, V7, P114385, DOI 10.1109/ACCESS.2019.2935006
   Lebedeva I, 2023, VISUAL COMPUT, V39, P1095, DOI 10.1007/s00371-021-02387-w
   Li SS, 2008, 2008 INTERNATIONAL CONFERENCE ON AUDIO, LANGUAGE AND IMAGE PROCESSING, VOLS 1 AND 2, PROCEEDINGS, P167, DOI 10.1109/ICALIP.2008.4589989
   Li ST, 2017, INFORM FUSION, V33, P100, DOI 10.1016/j.inffus.2016.05.004
   Liu JY, 2023, VISUAL COMPUT, V39, P4869, DOI 10.1007/s00371-022-02633-9
   Liu Y, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3124058
   Liu Y, 2020, INFORM FUSION, V64, P71, DOI 10.1016/j.inffus.2020.06.013
   Liu Y, 2018, INFORM FUSION, V42, P158, DOI 10.1016/j.inffus.2017.10.007
   Liu Y, 2017, INFORM FUSION, V36, P191, DOI 10.1016/j.inffus.2016.12.001
   Liu Y, 2016, IEEE SIGNAL PROC LET, V23, P1882, DOI 10.1109/LSP.2016.2618776
   Liu Y, 2015, INFORM FUSION, V24, P147, DOI 10.1016/j.inffus.2014.09.004
   Liu Y, 2015, INFORM FUSION, V23, P139, DOI 10.1016/j.inffus.2014.05.004
   Liu Z., Image Fusion Assessment Toolbox
   Liu Z, 2008, COMPUT VIS IMAGE UND, V109, P56, DOI 10.1016/j.cviu.2007.04.003
   Liu Z, 2012, IEEE T PATTERN ANAL, V34, P94, DOI 10.1109/TPAMI.2011.109
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Ma LF, 2023, APPL INTELL, V53, P1452, DOI 10.1007/s10489-022-03658-2
   Min XK, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3470970
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P6054, DOI 10.1109/TIP.2020.2988148
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Min XK, 2017, IEEE T IMAGE PROCESS, V26, P5462, DOI 10.1109/TIP.2017.2735192
   Nozaripour A, 2023, VISUAL COMPUT, V39, P1731, DOI 10.1007/s00371-022-02441-1
   Oliver R., Image Fusion Toolbox
   Piella G, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P173
   Poreddy AKR, 2023, VISUAL COMPUT, V39, P6743, DOI 10.1007/s00371-022-02760-3
   Qu GH, 2002, ELECTRON LETT, V38, P313, DOI 10.1049/el:20020212
   Sang QB, 2024, VISUAL COMPUT, V40, P3183, DOI 10.1007/s00371-023-03019-1
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Tang L, 2020, SIGNAL PROCESS-IMAGE, V85, DOI 10.1016/j.image.2020.115852
   TIKHONOV AN, 1963, DOKL AKAD NAUK SSSR+, V151, P501
   Wang CX, 2023, VISUAL COMPUT, V39, P1121, DOI 10.1007/s00371-021-02392-z
   Wang PW, 2008, INT CONF SIGN PROCES, P965, DOI 10.1109/ICOSP.2008.4697288
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wohlberg B, 2016, IEEE T IMAGE PROCESS, V25, P301, DOI 10.1109/TIP.2015.2495260
   Wu JJ, 2020, IEEE T IMAGE PROCESS, V29, P7414, DOI 10.1109/TIP.2020.3002478
   Xiao B, 2021, IEEE T IMAGE PROCESS, V30, P163, DOI 10.1109/TIP.2020.3033158
   Xie Q, 2023, VISUAL COMPUT, V39, P4249, DOI 10.1007/s00371-022-02588-x
   Xydeas CS, 2000, ELECTRON LETT, V36, P308, DOI 10.1049/el:20000267
   Yin GH, 2022, AAAI CONF ARTIF INTE, P3134
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang H, 2021, INFORM FUSION, V66, P40, DOI 10.1016/j.inffus.2020.08.022
   Zhang Q, 2018, INFORM FUSION, V40, P57, DOI 10.1016/j.inffus.2017.05.006
   Zhang XC, 2022, IEEE T PATTERN ANAL, V44, P4819, DOI 10.1109/TPAMI.2021.3078906
   Zheng HY, 2021, PROC CVPR IEEE, P630, DOI 10.1109/CVPR46437.2021.00069
   Zheng YF, 2007, INFORM FUSION, V8, P177, DOI 10.1016/j.inffus.2005.04.003
NR 59
TC 0
Z9 0
U1 8
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 3
PY 2024
DI 10.1007/s00371-024-03351-0
EA APR 2024
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MU4R5
UT WOS:001196140700001
DA 2024-08-05
ER

PT J
AU González-Toledo, D
   Cuevas-Rodríguez, M
   Molina-Tanco, L
   Reyes-Lecuona, A
AF Gonzalez-Toledo, Daniel
   Cuevas-Rodriguez, Maria
   Molina-Tanco, Luis
   Reyes-Lecuona, Arcadio
TI The spheroidal trackball: generalising the fixed trackball for virtual
   camera navigation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Usability; 3D interaction; 3D user interfaces; Virtual trackball;
   Virtual camera navigation; Ellipsoid; Spheroid
AB Virtual trackball techniques have become a standard in 3D applications, particularly for interfaces with limited degrees of freedom such as touchscreens or mice. The fact that we are used to them does not mean that they cannot be improved upon. Recent research has highlighted the significance of considering users' mental models of a preferred rotation axis, as it can improve performance, perceived usability and perceived workload. Building upon these findings, this paper introduces the spheroidal trackball framework-a novel method for orbiting the virtual camera around elongated objects. The paper presents the mathematical formulation and the evaluation of the technique. The formulation offers enough information to implement the approach. The evaluation shows the advantages of this approach over the fixed spherical trackball for this class of objects, in terms of task performance, usability and perceived workload. This research constitutes an advancement in the refinement of 3D user interaction techniques, opening new avenues of innovation in this still evolving field.
C1 [Gonzalez-Toledo, Daniel; Cuevas-Rodriguez, Maria; Molina-Tanco, Luis; Reyes-Lecuona, Arcadio] Univ Malaga, Telecommun Res Inst TELMA, Malaga, Spain.
C3 Universidad de Malaga
RP Reyes-Lecuona, A (corresponding author), Univ Malaga, Telecommun Res Inst TELMA, Malaga, Spain.
EM dgonzalezt@uma.es; mariacuevas@uma.es; lmtanco@uma.es; areyes@uma.es
OI Reyes-Lecuona, Arcadio/0000-0002-3699-4065; Molina Tanco,
   Luis/0000-0002-0476-0547
FU Universidad de Mlaga
FX No Statement Available
CR [Anonymous], 2019, Sketchfab: Sketchfab-Publish & find 3D models online
   [Anonymous], 2019, Google SketchUp: 3D Design Software 3D Modeling on the Web SketchUp
   Autodesk, 2012, 3ds Max-3d Modelling, Animation and Rendering Software
   Bade R, 2005, LECT NOTES COMPUT SC, V3638, P138
   BERNSTEIN N., 1967
   Berthouze L, 2004, ADAPT BEHAV, V12, P47, DOI 10.1177/105971230401200104
   Besançon L, 2021, COMPUT GRAPH FORUM, V40, P293, DOI 10.1111/cgf.14189
   Blender Foundation, 2015, blender.org-Home of the Blender project-Free and Open 3D Creation Software
   Brooke J., 1996, SUS-a quick and dirty usability scale, DOI [DOI 10.1201/9781498710411-35, DOI 10.1201/9781498710411]
   Chen M., 1988, Computer Graphics, V22, P121, DOI 10.1145/378456.378497
   Cowan G., 1998, Statistical data analysis, DOI [10.1093/oso/9780198501565.001.0001, DOI 10.1093/OSO/9780198501565.001.0001]
   deArquer I., 2001, Gobierno de Espana NTP, V544
   Devin F., Sistema de Escalas de Usabilidad: (sic)que es y para que sirve?
   Gonzalez-Toledo D., 2017, Dynamics of Long-Life Assets: From Technology Adaptation to Upgrading the Business Model, P193, DOI [10.1007/978-3-319-45438-2_11, DOI 10.1007/978-3-319-45438-2_11]
   Gonzalez-Toledo D, 2023, VISUAL COMPUT, V39, P1149, DOI 10.1007/s00371-021-02394-x
   Hart SG., 2006, P HUM FACT ERG SOC A, V50, P904, DOI [10.1177/154193120605000909, DOI 10.1177/154193120605000909]
   Khan Azam., 2005, Proceedings of the 2005 symposium on Interactive 3D graphics and games, I3D '05, P73, DOI DOI 10.1145/1053427.1053439
   Malomo L., 2016, P C SMART TOOLS APPL, P89, DOI [10.2312/stag.20161368, DOI 10.2312/STAG.20161368]
   Marchand É, 2002, VISUAL COMPUT, V18, P1, DOI 10.1007/s003710100122
   OfficialGoogle B., 2016, 3D Warehouse
   Rybicki S., 2016, P GRAPHICS INTERFACE, P93
   Sauro J., 2010, P 28 INT C HUMAN FAC, P2347, DOI [10.1145/1753326.1753679, DOI 10.1145/1753326.1753679]
   Sauro Jeff., 2011, MeasuringU
   SCHMIDT RA, 1975, PSYCHOL REV, V82, P225, DOI 10.1037/h0076770
   SHOEMAKE K, 1992, GRAPH INTER, P151
   Shoemake K., 1994, Graphics Gems IV, P175
   Thornton R.W., 1979, P 6 ANN C COMPUTER G, V13, P102, DOI [10.1145/800249.807430, DOI 10.1145/800249.807430]
   Weisstein E.W., 2020, Evolute-From Wolfram MathWorld
   Weisstein E.W., 2020, Ellipse Evolute-From Wolfram MathWorld
NR 29
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 27
PY 2024
DI 10.1007/s00371-023-03250-w
EA FEB 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JF2H6
UT WOS:001171680100001
OA hybrid
DA 2024-08-05
ER

PT J
AU Huo, YQ
   Qiao, Y
   Liu, YH
AF Huo, Yongqing
   Qiao, Yan
   Liu, Yaohui
TI A deep learning-based steganography method for high dynamic range images
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE HDR images; Image steganography; Deep learning
AB High dynamic range (HDR) images have recently drawn much attention in multimedia community. In this paper, we proposed an HDR image steganography method based on deep learning, which is for HDR images with OpenEXR format. To the best of our knowledge, this is the first steganography method that applies deep learning to HDR image steganography, and the first steganography method that hides images in HDR images. The LDR secret image is hidden in the mantissa of the HDR cover image of the same size through a hidden network, and recovered through an extraction network in the receiver. Experimental results show that the proposed algorithm has advantages in security, robustness and capacity compared with other "hiding image in image" algorithms.
C1 [Huo, Yongqing; Qiao, Yan; Liu, Yaohui] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Huo, YQ (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
EM hyq980132@uestc.edu.cn
CR Bai YQ, 2021, SIGNAL PROCESS-IMAGE, V91, DOI 10.1016/j.image.2020.116084
   Baldi P., 2011, ICML Unsupervised and Transfer Learning, P37
   Baluja S, 2017, ADV NEUR IN, V30
   Chang CC., 2013, J. Electron. Sci. Technol, V01, P22
   Chen F, 2020, IEEE ACCESS, V8, P21966, DOI 10.1109/ACCESS.2020.2969524
   Cheng YM, 2009, IEEE MULTIMEDIA, V16, P70, DOI 10.1109/MMUL.2009.43
   Duan XT, 2021, IETE TECH REV, V38, P172, DOI 10.1080/02564602.2020.1808097
   Duan XT, 2019, IEEE ACCESS, V7, P9314, DOI 10.1109/ACCESS.2019.2891247
   Filler T., 2010, P SPIE C SEC FOR MUL, VII, P501
   Fridrich J, 2009, Steganography in Digital Media: Principles Algorithms and Applications
   Gao XY, 2020, SIGNAL PROCESS, V173, DOI 10.1016/j.sigpro.2020.107579
   Goljan M, 2014, IEEE INT WORKS INFOR, P185, DOI 10.1109/WIFS.2014.7084325
   hdri-skies.com, About us
   hdrihaven.com, About us
   He XY, 2019, MULTIMED TOOLS APPL, V78, P29137, DOI 10.1007/s11042-018-6589-x
   Holub V, 2013, P 1 ACM WORKSH INF H, P59, DOI DOI 10.1145/2482513.2482514
   Holub V, 2014, EURASIP J INF SECUR, DOI 10.1186/1687-417X-2014-1
   Holub V, 2012, IEEE INT WORKS INFOR, P234, DOI 10.1109/WIFS.2012.6412655
   Hu DH, 2018, IEEE ACCESS, V6, P38303, DOI 10.1109/ACCESS.2018.2852771
   Ke Y, 2018, IEEE ACCESS, V6, P73009, DOI 10.1109/ACCESS.2018.2881680
   Li B, 2014, IEEE IMAGE PROC, P4206, DOI 10.1109/ICIP.2014.7025854
   Li MT, 2011, INT J INNOV COMPUT I, V7, P2021
   Lin YT, 2017, IEEE T MULTIMEDIA, V19, P196, DOI 10.1109/TMM.2016.2605499
   Liu LS, 2021, KNOWL-BASED SYST, V223, DOI 10.1016/j.knosys.2021.107022
   Liu YL, 2020, PROC CVPR IEEE, P1648, DOI 10.1109/CVPR42600.2020.00172
   Narwaria M, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.1.010501
   openexr.com, Technical Introduction to OpenEXR
   Pevny T, 2010, LECT NOTES COMPUT SC, V6387, P161, DOI 10.1007/978-3-642-16435-4_13
   Rehman A.U., 2018, End-to-end trained CNN encoder-decoder networks for image steganography
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   scarlet.stanford.edu, About us
   Shi HC, 2018, LECT NOTES COMPUT SC, V10735, P534, DOI 10.1007/978-3-319-77380-3_51
   Subhedar MS., 2021, Iran J. Comput. Sci, V4, P241, DOI [10.1007/s42044-020-00077-9, DOI 10.1007/S42044-020-00077-9]
   Tang W., 2017, IEEE Signal Process. Lett, V66, P1, DOI DOI 10.1109/ICOCN.2017.8121297
   Van TP, 2019, ISCIT 2019: PROCEEDINGS OF 2019 19TH INTERNATIONAL SYMPOSIUM ON COMMUNICATIONS AND INFORMATION TECHNOLOGIES (ISCIT), P410, DOI [10.1109/ISCIT.2019.8905216, 10.1109/iscit.2019.8905216]
   Volkhonskiy D., 2017, Inf. Hiding Multimedia Secur, P201
   Westfeld A., 2001, PROC INFORM HIDIN, P289, DOI [10.1007/3-540-, 10.1007/3-540-45496-921]
   Wu XT, 2019, DIGIT SIGNAL PROCESS, V93, P22, DOI 10.1016/j.dsp.2019.06.016
   Xiao X, 2018, 2018 NINTH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY IN MEDICINE AND EDUCATION (ITME 2018), P327, DOI 10.1109/ITME.2018.00080
   Yang JH, 2020, IEEE T INF FOREN SEC, V15, P839, DOI 10.1109/TIFS.2019.2922229
   Yu CM, 2011, DISPLAYS, V32, P225, DOI 10.1016/j.displa.2011.02.004
   Zhang R., 2019, Multimedia Tools Appl.
   Zhang R, 2019, MULTIMED TOOLS APPL, V78, P8559, DOI 10.1007/s11042-018-6951-z
   Zhou ZL, 2019, IEEE ACCESS, V7, P179891, DOI 10.1109/ACCESS.2019.2955990
   zooid.org, JSteg
NR 45
TC 0
Z9 0
U1 11
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 6
PY 2024
DI 10.1007/s00371-023-03214-0
EA FEB 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HA1I6
UT WOS:001156670000001
DA 2024-08-05
ER

PT J
AU Sattler, F
   Carrillo-Perez, B
   Barnes, S
   Stebner, K
   Stephan, M
   Lux, G
AF Sattler, Felix
   Carrillo-Perez, Borja
   Barnes, Sarah
   Stebner, Karsten
   Stephan, Maurice
   Lux, Gregor
TI Embedded 3D reconstruction of dynamic objects in real time formaritime
   situational awareness pictures
SO VISUAL COMPUTER
LA English
DT Article
DE Situtational awareness; Dynamic 3D reconstruction; Real-time monitoring;
   Maritime safety and security
ID VISION
AB Assessing the security status of maritime infrastructures is a key factor formaritime safety and security. Facilities such as ports and harbors are highly active traffic zones with many different agents and infrastructures present, like containers, trucks or vessels. Conveying security-related information in a concise and easily understandable format can support the decision-making process of stakeholders, such as port authorities, law enforcement agencies and emergency services. In this work, we propose a novel real-time 3D reconstruction framework for enhancing maritime situational awareness pictures by joining temporal 2D video data into a single consistent display. We introduce and verify a pipeline prototype for dynamic 3D reconstruction of maritime objects using a static observer and stereoscopic cameras on an GPU-accelerated embedded device. A simulated dataset of a harbor basin was created and used for real-time processing. Usage of a simulated setup allowed verification against synthetic ground-truth data. The presented pipeline runs entirely on a remote, low-power embedded system with similar to 6 Hz. A Nvidia Jetson Xavier AGX module was used, featuring 512 CUDA-cores, 16 GB memory and an ARMv8 64-bit octa-core CPU.
C1 [Sattler, Felix; Carrillo-Perez, Borja; Barnes, Sarah; Stephan, Maurice] German Aerosp Ctr DLR, Inst Protect Maritime Infrastruct, Maritime Secur Technol, Fischkai 1, D-27572 Bremerhaven, Germany.
   [Stebner, Karsten] German Aerosp Ctr DLR, Inst Opt Sensor Syst, Secur Res & Applicat, Rutherfordstr 2, D-12489 Berlin, Germany.
   [Lux, Gregor] Westphalian Univ Appl Sci, Dept Comp Sci, Neidenburger Str 43, Gelsenkirchen, Nrw, Germany.
C3 Helmholtz Association; German Aerospace Centre (DLR); Helmholtz
   Association; German Aerospace Centre (DLR)
RP Sattler, F (corresponding author), German Aerosp Ctr DLR, Inst Protect Maritime Infrastruct, Maritime Secur Technol, Fischkai 1, D-27572 Bremerhaven, Germany.
EM felix.sattler@dlr.de; borja.carrilloperez@dlr.de; sarah.barnes@dlr.de;
   karsten.stebner@dlr.de; maurice.stephan@dlr.de; gregor.lux@w-hs.de
RI Carrillo-Perez, Borja/JNS-5214-2023
OI /0009-0002-5495-5841; Sattler, Felix/0000-0001-8869-282X
FU German Aerospace Center (Deutsches Zentrum fur Luft-und Raumfahrt e.V.);
   Federal Ministry for Economic Affairs and Climate Action
   (Bundesministerium fur Wissenschaft und Klimaschutz); Projekt DEAL
FX Open Access funding enabled and organized by Projekt DEAL. This research
   was funded by the core funding of the German Aerospace Center (Deutsches
   Zentrum fur Luft-und Raumfahrt e.V.). The funding is granted by the
   Federal Ministry for Economic Affairs and Climate Action
   (Bundesministerium fur Wissenschaft und Klimaschutz, BMWK). It has been
   realized as part of the project "Digital Atlas" in a cooperation between
   the DLR institute for the protection of maritime infrastructures and the
   DLR institute for optical sensor systems.
CR Bârsan IA, 2018, IEEE INT CONF ROBOT, P7510
   Blender Online Community, 2018, Blender-a 3D modelling and rendering package
   Bullinger S., 2020, Ph.D. thesis, DOI [10.5445/KSP/1000105589, DOI 10.5445/KSP/1000105589]
   Carrillo-Perez B, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22072713
   CHEN Y, 1991, 1991 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-3, P2724, DOI 10.1109/ROBOT.1991.132043
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   Engelmann F, 2017, IEEE WINT CONF APPL, P400, DOI 10.1109/WACV.2017.51
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Grinvald M, 2021, IEEE INT CONF ROBOT, P14192, DOI 10.1109/ICRA48506.2021.9560923
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI [10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166]
   Jocher Glenn, 2022, Zenodo
   Karoly Artur I., 2022, 2022 IEEE 20th Jubilee World Symposium on Applied Machine Intelligence and Informatics (SAMI)., P000329, DOI 10.1109/SAMI54271.2022.9780790
   Letourneau JJ, 2018, BIOORG MED CHEM LETT, V28, P756, DOI 10.1016/j.bmcl.2018.01.005
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lucas Bruce D, 1981, IJCAI 81, V81, DOI DOI 10.5555/1623264.1623280
   Merrill D, 2022, CUB v1.16.0
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Niessner M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508374
   Paszke A, 2019, ADV NEUR IN, V32
   Peng SD, 2019, PROC CVPR IEEE, P4556, DOI 10.1109/CVPR.2019.00469
   Qiao SY, 2021, PROC CVPR IEEE, P10208, DOI 10.1109/CVPR46437.2021.01008
   Qiu WC, 2016, LECT NOTES COMPUT SC, V9915, P909, DOI 10.1007/978-3-319-49409-8_75
   Ribeiro M, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22218090
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Simoudis E., 1996, KDD 96 P 2 INT C KNO, P226
   Sirimanne S.N., 2020, Technical Report. Tech. Rep..
   Solano-Carrillo E, 2021, IEEE INT C INTELL TR, P2193, DOI 10.1109/ITSC48978.2021.9564675
   Strecke M, 2019, IEEE I CONF COMP VIS, P5864, DOI 10.1109/ICCV.2019.00596
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sundermeyer M., 2019, ARXIV191101911
   Wang CY, 2021, PROC CVPR IEEE, P13024, DOI 10.1109/CVPR46437.2021.01283
   Whelan T, 2015, INT J ROBOT RES, V34, P598, DOI 10.1177/0278364914551008
   Yang Y, 2021, J MAR SCI ENG, V9, DOI 10.3390/jmse9111281
   Zhang H., 2022, 2022 IEEE INT C UNM, P653, DOI [10.1109/ICUS55513.2022.9986766, DOI 10.1109/ICUS55513.2022.9986766]
NR 35
TC 1
Z9 1
U1 2
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 571
EP 584
DI 10.1007/s00371-023-02802-4
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:001151021700009
OA hybrid
DA 2024-08-05
ER

PT J
AU Zhao, XY
   Wang, Z
   Deng, ZC
   Qin, HD
   Zhu, ZB
AF Zhao, Xiaoyang
   Wang, Zhuo
   Deng, Zhongchao
   Qin, Hongde
   Zhu, Zhongben
TI Transmission-guided multi-feature fusion Dehaze network
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image dehazing; Decoding; Encoding; Feature enhancement
AB Image dehazing is an important direction of low-level visual tasks, and its quality and efficiency directly affect the quality of high-level visual tasks. Therefore, how to quickly and efficiently process hazy images with different thicknesses of fog has become the focus of research. This paper presents a multi-feature fusion embedded image dehazing network based on transmission guidance. Firstly, we propose a transmission graph-guided feature fusion enhanced coding network, which can combine different weight information and show better flexibility for different dehazing information. At the same time, in order to keep more detailed information in the reconstructed image, we propose a decoder network embedded with Mix module, which can not only keep shallow information, but also allow the network to learn the weights of different depth information spontaneously and re-fit the dehazing features. The comparative experiments on RESIDE and Haze4K datasets verify the efficiency and high quality of our algorithm. A series of ablation experiments show that Multi-weight attention feature fusion module (WA) module and Mix module can effectively improve the model performance. The code is released in https://doi.org/10.5281/zenodo.10836919.
C1 [Zhao, Xiaoyang; Deng, Zhongchao; Zhu, Zhongben] Harbin Engn Univ, Qingdao Innovat & Dev Ctr, Qingdao 266000, Peoples R China.
   [Wang, Zhuo; Qin, Hongde] Harbin Engn Univ, Sci & Technol Underwater Vehicle Technol Lab, Harbin 150001, Peoples R China.
C3 Harbin Engineering University; Harbin Engineering University
RP Deng, ZC (corresponding author), Harbin Engn Univ, Qingdao Innovat & Dev Ctr, Qingdao 266000, Peoples R China.
EM zhaoxiaoyang@hrbeu.edu.cn; wangzhuo@hrbeu.edu.cn;
   dengzhongchao@hrbeu.edu.cn; qinhongde@hrbeu.edu.cn;
   zhuzhongben@hrbeu.edu.cn
FU National Natural Science Foundation of China [52025111]
FX This work was funded by the National Natural Science Foundation of China
   under Grants 52025111.
CR Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Fan GD, 2024, IEEE T NEUR NET LEAR, V35, P1598, DOI 10.1109/TNNLS.2022.3184164
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720
   Guo F, 2020, NEUROCOMPUTING, V378, P9, DOI 10.1016/j.neucom.2019.09.094
   Guo XY, 2019, PROC CVPR IEEE, P3268, DOI 10.1109/CVPR.2019.00339
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2009, PROC CVPR IEEE, P1956, DOI [10.1109/CVPRW.2009.5206515, 10.1109/CVPR.2009.5206515]
   Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Liu P, 2023, VISUAL COMPUT, DOI 10.1007/s00371-023-03177-2
   Liu Q, 2018, IEEE T IMAGE PROCESS, V27, P5178, DOI 10.1109/TIP.2018.2849928
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Liu Y, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P50, DOI 10.1145/3474085.3475331
   Lu L., 2023, MixDehazeNet: Mix Structure Block for Image Dehazing Network
   McCartney E. J., 1976, Optics of the atmosphere. Scattering by molecules and particles
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Narasimhan SG., 2008, ACM SIGGRAPH ASIA 2008 courses on-SIGGRAPH Asia 08, P1
   Nayar S. K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P820, DOI 10.1109/ICCV.1999.790306
   Peng YT, 2018, IEEE T IMAGE PROCESS, V27, P2856, DOI 10.1109/TIP.2018.2813092
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Qu YY, 2019, PROC CVPR IEEE, P8152, DOI 10.1109/CVPR.2019.00835
   Redmon J., 2018, YOLOv 3: An Incremental ImprovementC
   Ren WQ, 2020, INT J COMPUT VISION, V128, P240, DOI 10.1007/s11263-019-01235-8
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Salazar-Colores S, 2019, IEEE T IMAGE PROCESS, V28, P2357, DOI 10.1109/TIP.2018.2885490
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Sheng B, 2020, IEEE T CIRC SYST VID, V30, P955, DOI 10.1109/TCSVT.2019.2901629
   Su YZ, 2021, NEUROCOMPUTING, V423, P620, DOI 10.1016/j.neucom.2020.10.061
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Valanarasu JMJ, 2022, PROC CVPR IEEE, P2343, DOI 10.1109/CVPR52688.2022.00239
   Wen Y, 2021, IEEE T IMAGE PROCESS, V30, P6142, DOI 10.1109/TIP.2021.3092814
   Wu HY, 2021, PROC CVPR IEEE, P10546, DOI 10.1109/CVPR46437.2021.01041
   Wu RQ, 2023, PROC CVPR IEEE, P22282, DOI 10.1109/CVPR52729.2023.02134
   Ye T., 2021, Perceiving and Modeling Density is All You Need for Image Dehazing
   Zhang D., 2022, 2022 IEEE INT C MULT, P1
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang YB, 2024, MULTIMED TOOLS APPL, V83, P11367, DOI 10.1007/s11042-023-15844-6
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao SY, 2021, IEEE T IMAGE PROCESS, V30, P3391, DOI 10.1109/TIP.2021.3060873
   Zheng Y, 2023, PROC CVPR IEEE, P5785, DOI 10.1109/CVPR52729.2023.00560
   Zhou Y, 2023, IEEE T NEUR NET LEAR, V34, P7719, DOI 10.1109/TNNLS.2022.3146004
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 52
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 3
PY 2024
DI 10.1007/s00371-024-03533-w
EA JUL 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XK1T9
UT WOS:001261494700001
DA 2024-08-05
ER

PT J
AU Cheekaty, S
   Muneeswari, G
AF Cheekaty, Suresh
   Muneeswari, G.
TI Advancing autism prediction through visual-based AI approaches:
   integrating advanced eye movement analysis and shape recognition with
   Kalman filtering
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE ETSP images; Eye movement classification and prediction; CLAHE; Image
   enhancement; Deep learning
ID SPECTRUM DISORDER; CLASSIFICATION; ENHANCEMENT; TRACKING; SYSTEM
AB In the recent past, the global prevalence of autism spectrum disorder (ASD) has witnessed a remarkable surge, underscoring its significance as a widespread neurodevelopmental disorder affecting children, with an incidence rate of 0.62%. Individuals diagnosed with ASD often grapple with challenges in language acquisition and comprehending verbal communication, compounded by difficulties in nonverbal communication aspects such as gestures and eye contact. Eye movement analysis, a multifaceted field spanning industrial engineering to psychology, offers invaluable insights into human attention and behavior patterns. The present study proposes an economical eye movement analysis system that adroitly integrates Neuro Spectrum Net (NSN) techniques with Kalman filtering, enabling precise eye position estimation. The overarching objective is to enhance deep learning models for early autism detection by leveraging eye-tracking data, a critical consideration given the pivotal role of early intervention in mitigating the disorder's impact. Through the synergistic incorporation of NSN and contrast-limited adaptive histogram equalization for feature extraction, the proposed model exhibits superior scalability and accuracy when compared to existing methodologies, thereby holding promising potential for clinical applications. A comprehensive series of experiments and rigorous evaluations underscore the system's efficacy in eye movement classification and pupil position identification, outperforming traditional Recurrent Neural Network approaches. The dataset utilized in the aforementioned scholarly article is accessible through the Zenodo repository and can be retrieved via the following link: [https://zenodo.org/records/10935303?preview=1].
C1 [Cheekaty, Suresh; Muneeswari, G.] VIT AP Univ, Sch Comp Sci & Engn, Amaravati, Andhra Pradesh, India.
C3 VIT-AP University
RP Muneeswari, G (corresponding author), VIT AP Univ, Sch Comp Sci & Engn, Amaravati, Andhra Pradesh, India.
EM muneeswari.g@vitap.ac.in
FU VIT-AP University, Amaravati
FX This endeavor would not have been possible without the generous support
   from VIT-AP University, Amaravati. Words cannot express my gratitude to
   my Research Supervisor Dr. G Muneeswari, Professor, School of Computer
   Science and Engineering, VIT-AP University, Amaravati, and chair of my
   committee for her invaluable patience and feedback. I also could not
   have undertaken this journey without my defense committee, who
   generously provided knowledge and expertise.
CR Ahmed IA, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11040530
   Akter Tania, 2021, 2021 2nd International Conference on Robotics, Electrical and Signal Processing Techniques (ICREST), P383, DOI 10.1109/ICREST51555.2021.9331152
   Aurangzeb K, 2021, IEEE ACCESS, V9, P47930, DOI 10.1109/ACCESS.2021.3068477
   Carette Romuald, 2018, 2018 Thirteenth International Conference on Digital Information Management (ICDIM), P248, DOI 10.1109/ICDIM.2018.8846967
   Carette R, 2019, HEALTHINF: PROCEEDINGS OF THE 12TH INTERNATIONAL JOINT CONFERENCE ON BIOMEDICAL ENGINEERING SYSTEMS AND TECHNOLOGIES - VOL 5: HEALTHINF, P103, DOI 10.5220/0007402601030112
   Chen ZH, 2023, IEEE T PATTERN ANAL, V45, P13489, DOI 10.1109/TPAMI.2023.3293885
   Constantino JN, 2017, NATURE, V547, P340, DOI 10.1038/nature22999
   Dai L, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-23458-5
   Dawson G, 2008, DEV PSYCHOPATHOL, V20, P775, DOI 10.1017/S0954579408000370
   Elbattah M, 2021, J IMAGING, V7, DOI 10.3390/jimaging7050083
   Elbattah M, 2019, IEEE ENG MED BIO, P1417, DOI 10.1109/EMBC.2019.8856904
   Eslami T, 2019, FRONT NEUROINFORM, V13, DOI 10.3389/fninf.2019.00070
   Falck-Ytter T, 2018, J CHILD PSYCHOL PSYC, V59, P872, DOI 10.1111/jcpp.12863
   Fernandes AS, 2023, IEEE T VIS COMPUT GR, V29, P2269, DOI 10.1109/TVCG.2023.3247058
   Glauser J, 2022, J NEURODEV DISORD, V14, DOI 10.1186/s11689-021-09413-x
   Gredebck G., 2010, Dev. Neuropsychol, V35, P340
   Guillon Q, 2014, NEUROSCI BIOBEHAV R, V42, P279, DOI 10.1016/j.neubiorev.2014.03.013
   Guo HB, 2021, IEEE T CYBERNETICS, V51, P2735, DOI 10.1109/TCYB.2019.2934823
   Guo XL, 2024, VISUAL COMPUT, DOI 10.1007/s00371-024-03303-8
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Klin A, 2006, REV BRAS PSIQUIATR, V28, pS1, DOI 10.1590/S1516-44462006000500001
   Komogortsev OV, 2007, LECT NOTES COMPUT SC, V4552, P679
   Komogortsev OV, 2013, BEHAV RES METHODS, V45, P203, DOI 10.3758/s13428-012-0234-9
   Kumar CDN., 2018, J. Eng. (IOSRJEN), V5, P63
   Kwon MK, 2019, J AM ACAD CHILD PSY, V58, P1004, DOI 10.1016/j.jaac.2018.12.011
   Leigh R.J., 2015, The Neurology of Eye Movements, DOI DOI 10.1093/MED/9780199969289.001.0001
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Lord C, 2006, ARCH GEN PSYCHIAT, V63, P694, DOI 10.1001/archpsyc.63.6.694
   Loth E, 2017, MOL AUTISM, V8, DOI 10.1186/s13229-017-0146-8
   Mohamad M., 2022, Creat. Educ, V13, P3602, DOI [10.4236/ce.2022.1311230, DOI 10.4236/CE.2022.1311230]
   Munadi K, 2020, IEEE ACCESS, V8, P217897, DOI 10.1109/ACCESS.2020.3041867
   Nahiduzzaman M, 2021, IEEE ACCESS, V9, P152261, DOI 10.1109/ACCESS.2021.3125791
   Patil A., 2020, Int. J. Data Sci. Anal, V6, P99, DOI [10.11648/j.ijdsa.20200604.11, DOI 10.11648/J.IJDSA.20200604.11]
   Pizer S. M., 1990, Proceedings of the First Conference on Visualization in Biomedical Computing (Cat. No.90TH0311-1), P337, DOI 10.1109/VBC.1990.109340
   Prelock PA., 2021, The Handbook of Language and Speech Disorders, P129, DOI [10.1002/9781119606987.ch7, DOI 10.1002/9781119606987.CH7]
   Qian B, 2024, PATTERNS, V5, DOI 10.1016/j.patter.2024.100929
   Robles M, 2022, IEEE T VIS COMPUT GR, V28, P2168, DOI 10.1109/TVCG.2022.3150489
   Russell AJ, 2016, AUTISM, V20, P623, DOI 10.1177/1362361315604271
   SAUTER D, 1991, MED BIOL ENG COMPUT, V29, P63, DOI 10.1007/BF02446297
   Sheng B, 2022, IEEE T CYBERNETICS, V52, P6662, DOI 10.1109/TCYB.2021.3079311
   Singh P., 2013, Int. J. Eng. Innov. Technol. (IJEIT), V3, P29
   Sonawane B, 2021, VISUAL COMPUT, V37, P1151, DOI 10.1007/s00371-020-01859-9
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Yu X, 2021, IEEE ACM T COMPUT BI, V18, P94, DOI 10.1109/TCBB.2020.2986544
NR 45
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 26
PY 2024
DI 10.1007/s00371-024-03529-6
EA JUN 2024
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WM6H5
UT WOS:001255322700002
DA 2024-08-05
ER

PT J
AU Liu, SY
   Xu, F
   Wu, CD
   Chi, JN
   Yu, XS
   Wei, LX
   Leng, CJ
AF Liu, Suyi
   Xu, Fang
   Wu, Chengdong
   Chi, Jianning
   Yu, Xiaosheng
   Wei, Longxing
   Leng, Chuanjiang
TI CMT-6D: a lightweight iterative 6DoF pose estimation network based on
   cross-modal Transformer
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE 6D pose estimation; Cross-modal Transformer; Cross-modal key query
   strategy; 3D keypoint selection; Lightweight pose iterative
ID 3D OBJECT DETECTION
AB 6DoF pose estimation has received much attention in recent years. A key challenge is the difficulty of estimating object pose when the target texture is weak. In this work, we present the cross-modal Transformer (CMT-6D), a Transformer-based network suitable for highly accurate workpiece-level object 6D pose estimation from a single RGBD image. Our main insight is to make the surface texture information of RGB images with the geometric feature information of point clouds complement each other through a cross-modal Transformer, enabling accurate estimation of the pose of weakly textured targets. Specifically, the whole framework consists of two parallel Transformer branches, named Point Transformer and Image Transformer. Both parallel transformer networks use a pyramid structured encoder and a multi-layer perceptron structured decoder to extract geometric features of point clouds and texture features of RGB images, respectively. Then, a cross-modal key query strategy is proposed for information exchange between parallel channels. In addition, at the output representation stage, we design a simple and effective 3D keypoint selection algorithm to solve the problem that keypoints are likely to appear in the non-significant region. Finally, to improve the accuracy of attitude estimation and meet real-time requirements, a lightweight pose iterative network based on target feature regression is proposed to correct the initial attitude estimation error. Extensive experiments demonstrate the effectiveness and superiority of our method on LineMOD, Occlusion LineMOD, T-Less, and YCB-Video datasets. We demonstrate that our method can improve the 6D pose estimation performance by comparing with the state-of-the-art. Ablation research and visualization validate the design of CMT-6D.
C1 [Liu, Suyi; Wu, Chengdong; Chi, Jianning; Yu, Xiaosheng; Leng, Chuanjiang] Northeastern Univ, Fac Robot Sci & Engn, Chuangxin Rd, Shenyang 110167, Liaoning, Peoples R China.
   [Xu, Fang] Acad Sinica, Shenyang Siasun Robot Automat Co Ltd, Quanyun Rd, Shenyang 110180, Liaoning, Peoples R China.
   [Wei, Longxing] China Aerosp Sci & Ind Corp, Inst 706, Acad 2, Yongding Rd, Beijing 100049, Peoples R China.
C3 Northeastern University - China; Chinese Academy of Sciences; China
   Aerospace Science & Industry Corporation (CASIC)
RP Liu, SY (corresponding author), Northeastern Univ, Fac Robot Sci & Engn, Chuangxin Rd, Shenyang 110167, Liaoning, Peoples R China.
EM 1300811151@qq.com; xufang@siasun.com; wuchengdong@mail.neu.edu.cn;
   chijianning@mail.neu.edu.cn; yuxiaosheng@mail.neu.edu.cn;
   18846934112@163.com; 1910651@stu.neu.edu.cn
FU Foundation of Ministry of Industry and Information Technology; National
   Natural Science Foundation of China [U20A20197, 62306187]; 
   [TC220H05X-04]
FX This work is supported in part by the National Natural Science
   Foundation of China under Grant nos. U20A20197, 62306187 and the
   Foundation of Ministry of Industry and Information Technology
   TC220H05X-04.
CR ARUN KS, 1987, IEEE T PATTERN ANAL, V9, P699, DOI 10.1109/TPAMI.1987.4767965
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Brachmann E, 2014, LECT NOTES COMPUT SC, V8690, P536, DOI 10.1007/978-3-319-10605-2_35
   Calli B, 2015, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS (ICAR), P510, DOI 10.1109/ICAR.2015.7251504
   Chetverikov D, 2002, INT C PATT RECOG, P545, DOI 10.1109/ICPR.2002.1047997
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Gao G, 2021, IEEE INT CONF ROBOT, P11081, DOI 10.1109/ICRA48506.2021.9561475
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He W., 2020, P IEEE CVF C COMP VI, P11629, DOI [10.1109/CVPR42600.2020.01165, DOI 10.1109/CVPR42600.2020.01165]
   He YS, 2021, PROC CVPR IEEE, P3002, DOI 10.1109/CVPR46437.2021.00302
   Hinterstoisser S., 2012, ACCV, P548
   Hodan T, 2018, LECT NOTES COMPUT SC, V11214, P19, DOI 10.1007/978-3-030-01249-6_2
   Hodan T, 2017, IEEE WINT CONF APPL, P880, DOI 10.1109/WACV.2017.103
   Hodan Tomas, 2020, P IEEECVF C COMPUTER, P11703
   Jiang XK, 2022, PROC CVPR IEEE, P11164, DOI 10.1109/CVPR52688.2022.01089
   Kehl W, 2017, IEEE I CONF COMP VIS, P1530, DOI 10.1109/ICCV.2017.169
   Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6
   Li ZG, 2019, IEEE I CONF COMP VIS, P7677, DOI 10.1109/ICCV.2019.00777
   Liang Y, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9534175
   Lin SF, 2023, IEEE T AUTOM SCI ENG, DOI 10.1109/TASE.2023.3327772
   Lin SF, 2022, IEEE ROBOT AUTOM LET, V7, P6526, DOI 10.1109/LRA.2022.3174261
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin X., 2021, IEEE Trans. Multimedia
   Marchand E, 2016, IEEE T VIS COMPUT GR, V22, P2633, DOI 10.1109/TVCG.2015.2513408
   Park K, 2019, IEEE I CONF COMP VIS, P7667, DOI 10.1109/ICCV.2019.00776
   Peng SD, 2022, IEEE T PATTERN ANAL, V44, P3212, DOI 10.1109/TPAMI.2020.3047388
   Qi CR, 2017, ADV NEUR IN, V30
   Qingyong Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11105, DOI 10.1109/CVPR42600.2020.01112
   Rangesh A, 2020, IEEE T INTELL VEHICL, V5, P449, DOI 10.1109/TIV.2020.2966074
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shi YF, 2021, PROC CVPR IEEE, P15217, DOI 10.1109/CVPR46437.2021.01497
   Song C, 2020, PROC CVPR IEEE, P428, DOI 10.1109/CVPR42600.2020.00051
   Song SR, 2014, LECT NOTES COMPUT SC, V8694, P634, DOI 10.1007/978-3-319-10599-4_41
   Sundermeyer Martin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13913, DOI 10.1109/CVPR42600.2020.01393
   Sundermeyer M, 2018, LECT NOTES COMPUT SC, V11210, P712, DOI 10.1007/978-3-030-01231-1_43
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Tsourounis D, 2022, J IMAGING, V8, DOI 10.3390/jimaging8100256
   Wang C, 2019, PROC CVPR IEEE, P3338, DOI 10.1109/CVPR.2019.00346
   Wang DM, 2022, IEEE T MULTIMEDIA, V24, P4394, DOI 10.1109/TMM.2021.3117092
   Wang X., 2020, INT C NEURAL INF PRO, V33, P17721
   Wanqing Zhao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14122, DOI 10.1109/CVPR42600.2020.01414
   Wu YH, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2078
   Xiang Y, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV
   Xie T, 2022, IEEE ROBOT AUTOM LET, V7, P1840, DOI 10.1109/LRA.2021.3136873
   Xu DF, 2018, PROC CVPR IEEE, P244, DOI 10.1109/CVPR.2018.00033
   Yan X, 2020, PROC CVPR IEEE, P5588, DOI 10.1109/CVPR42600.2020.00563
   Zakharov S, 2019, IEEE I CONF COMP VIS, P1941, DOI 10.1109/ICCV.2019.00203
   Zhang C, 2022, PROC CVPR IEEE, P11789, DOI 10.1109/CVPR52688.2022.01150
   Zhang YQ, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3150568
   Zhou GL, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2022.3222467
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
   Zhu ML, 2014, IEEE INT CONF ROBOT, P3936, DOI 10.1109/ICRA.2014.6907430
   Zou L, 2022, IEEE T IMAGE PROCESS, V31, P6907, DOI 10.1109/TIP.2022.3216980
   Zou L, 2021, COMPUT GRAPH-UK, V97, P139, DOI 10.1016/j.cag.2021.04.018
NR 57
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 17
PY 2024
DI 10.1007/s00371-024-03520-1
EA JUN 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UN1G1
UT WOS:001248641200004
DA 2024-08-05
ER

PT J
AU Wu, J
   Wu, H
   Yuan, GW
AF Wu, Jing
   Wu, Hao
   Yuan, Guowu
TI Detail-aware image denoising via structure preserved network and
   residual diffusion model
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image denoising; Conditional diffusion model; Perceptual quality;
   Detailed texture
AB The rapid development of deep learning has led to significant strides in image denoising research and has achieved advanced denoising performance in terms of distortion metrics. However, most denoising models that construct loss functions based on pixel-by-pixel differences cause phenomena, such as blurred edges or over-smoothing in denoised images, unsatisfactory to human perception. Our approach to addressing this issue involves prioritizing visual perceptual quality and efficiently restoring high-frequency details that may have been lost during the point-by-point denoising process, all the while preserving the overall structure of the image. We introduce a structure preserved network to generate cost-effective initial predictions that are subsequently incorporated into a conditional diffusion model as a constraint that closely aligns with the actual images. This allows us to more accurately estimate the distribution of clean images by diffusing from the residuals. We observe that by maintaining image consistency in the initial prediction, we can use a residual diffusion model with lower complexity and fewer iterations to restore the detailed texture for the smoothed parts, ultimately leading to a denoised image sample that is more consistent with the visual perceptual quality. Our method is superior in matching human perceptual metrics, e.g. FID, and maintains its performance even at high noise levels, enabling the preservation of the sharp edge and texture features of the image, while reducing computational costs and equipment requirements. This not only achieves the objective of denoising but also results in enhanced subjective visual effects.
C1 [Wu, Jing; Wu, Hao; Yuan, Guowu] Yunnan Univ, Sch Informat Sci & Engn, Kunming, Peoples R China.
C3 Yunnan University
RP Wu, H (corresponding author), Yunnan Univ, Sch Informat Sci & Engn, Kunming, Peoples R China.
EM haowu_sise@ynu.edu.cn
RI Yuan, Guowu/HJI-7569-2023
OI Yuan, Guowu/0000-0002-8449-6861
FU National Natural Science Foundation of China [62061049, 12263008];
   Yunnan Provincial Department of Science and Technology-Yunnan University
   Joint Special Project for Double-Class Construction
   [202201BF070001-005]; Practical Innovation Fund Project for Professional
   Degree Graduate Students of Yunnan University [ZC-22221881]
FX This study was funded by the National Natural Science Foundation of
   China (GrantNo.62061049,GrantNo.12263008), the Yunnan Provincial
   Department of Science and Technology-Yunnan University Joint Special
   Project for Double-Class Construction (Grant No.202201BF070001-005) and
   the Practical Innovation Fund Project for Professional Degree Graduate
   Students of Yunnan University (Grant No. ZC-22221881).
CR Abdelhamed A, 2018, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2018.00182
   Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Alsaiari A, 2019, 2019 IEEE 2ND INTERNATIONAL CONFERENCE ON INFORMATION AND COMPUTER TECHNOLOGIES (ICICT), P126, DOI [10.1109/infoct.2019.8710893, 10.1109/INFOCT.2019.8710893]
   Anwar S, 2019, IEEE I CONF COMP VIS, P3155, DOI 10.1109/ICCV.2019.00325
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Blau Y, 2018, PROC CVPR IEEE, P6228, DOI 10.1109/CVPR.2018.00652
   Brock A, 2019, Arxiv, DOI arXiv:1809.11096
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen JW, 2018, PROC CVPR IEEE, P3155, DOI 10.1109/CVPR.2018.00333
   Chen NX, 2020, Arxiv, DOI arXiv:2009.00713
   Chen YJ, 2015, PROC CVPR IEEE, P5261, DOI 10.1109/CVPR.2015.7299163
   Chung H, 2022, PROC CVPR IEEE, P12403, DOI 10.1109/CVPR52688.2022.01209
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Divakar N, 2017, IEEE COMPUT SOC CONF, P1076, DOI 10.1109/CVPRW.2017.145
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Franzen R, 1999, KODAK LOSSLESS TRUE
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181
   Hensel M, 2017, ADV NEUR IN, V30
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Karras T, 2018, Arxiv, DOI [arXiv:1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Liu Hong, 2021, ADV NEURAL INFORM PR, V34
   Loshchilov I, 2017, Sgdr: Stochastic gradient descent with warm restarts, DOI [10.48550/arXiv.1608.03983, DOI 10.48550/ARXIV.1608.03983]
   Loshchilov I, 2019, Arxiv, DOI [arXiv:1711.05101, 10.48550/arXiv.1711.05101, DOI 10.48550/ARXIV.1711.05101]
   Lu C, 2022, Arxiv, DOI arXiv:2206.00927
   Lugmayr Andreas, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P715, DOI 10.1007/978-3-030-58558-7_42
   Lugmayr A, 2021, IEEE COMPUT SOC CONF, P596, DOI 10.1109/CVPRW53098.2021.00072
   Luo ZW, 2023, Arxiv, DOI arXiv:2301.11699
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888
   Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Menon S, 2020, PROC CVPR IEEE, P2434, DOI 10.1109/CVPR42600.2020.00251
   ÖOzdenizci O, 2023, IEEE T PATTERN ANAL, V45, P10346, DOI 10.1109/TPAMI.2023.3238179
   Ohayon G, 2021, IEEE INT CONF COMP V, P1805, DOI 10.1109/ICCVW54120.2021.00207
   Oord A.v.d., 2016, arXiv, DOI DOI 10.48550/ARXIV.1609.03499
   Prakash M., 2020, arXiv, DOI 10.48550/arxiv.2006.06072
   Saharia C, 2023, IEEE T PATTERN ANAL, V45, P4713, DOI 10.1109/TPAMI.2022.3204461
   Schmidt U, 2014, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2014.349
   Sohl-Dickstein J, 2015, PR MACH LEARN RES, V37, P2256
   Song JM, 2022, Arxiv, DOI [arXiv:2010.02502, DOI 10.48550/ARXIV.2010.02502]
   Tian CW, 2020, NEURAL NETWORKS, V124, P117, DOI 10.1016/j.neunet.2019.12.024
   Timofte R., 2017, P IEEE C COMPUTER VI, P114
   Tu ZZ, 2022, PROC CVPR IEEE, P5759, DOI 10.1109/CVPR52688.2022.00568
   Valanarasu JMJ, 2022, LECT NOTES COMPUT SC, V13435, P23, DOI 10.1007/978-3-031-16443-9_3
   van den Oord A, 2016, ADV NEUR IN, V29
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Whang J, 2022, PROC CVPR IEEE, P16272, DOI 10.1109/CVPR52688.2022.01581
   Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zamir SW, 2023, IEEE T PATTERN ANAL, V45, P1934, DOI 10.1109/TPAMI.2022.3167175
   Zamir SW, 2020, PROC CVPR IEEE, P2693, DOI 10.1109/CVPR42600.2020.00277
   Zhang K, 2022, IEEE T PATTERN ANAL, V44, P6360, DOI 10.1109/TPAMI.2021.3088914
   Zhang K, 2023, MACH INTELL RES, V20, P822, DOI 10.1007/s11633-023-1466-0
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang L, 2011, J ELECTRON IMAGING, V20, DOI 10.1117/1.3600632
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zongsheng Yue, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P41, DOI 10.1007/978-3-030-58607-2_3
NR 67
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 18
PY 2024
DI 10.1007/s00371-024-03353-y
EA APR 2024
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC4R2
UT WOS:001205052300002
DA 2024-08-05
ER

PT J
AU Xiong, HY
   Xiang, Y
AF Xiong, Haoyu
   Xiang, Yu
TI Robust gradient aware and reliable entropy minimization for stable
   test-time adaptation in dynamic scenarios
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image classification; Unsupervised domain adaptation; Test-time
   adaptation; Distribution shift
ID NEURAL-NETWORKS
AB Test-time adaptation (TTA) aims to provide neural networks capable of adapting to the target domain distribution using only unlabeled test data. Most existing TTA methods have achieved success under mild conditions, such as independently sampled data from a single or multiple static domains. However, these attempts may fail in dynamic scenarios, where the test data distribution undergoes continuous changes over time. By digging into the failure cases, we find that high-entropy or noisy samples during long-term adaptation may lead to inevitable catastrophic failure. Thus, we propose a Robust Gradient Aware and Reliable entropy minimization approach, called RGAR, to further stabilize TTA from three aspects: (1) Boosting model robustness to distribution shift, we propose a dual-stream perturbation technique that enables two weak-to-strong perturbation views of the student model guided by a common strong view of the mean teacher model; (2) mitigating the impact of high-entropy samples from different scenarios, we present to minimize the reliable samples that take into account both the distribution shift and sample adaptation degree; (3) enabling the model to be insensitive to small perturbations by encouraging model weights to reach flatter minima while focusing on the maximal gradient norm. Extensive experimental results demonstrate the effectiveness of our proposed method, RGAR. We achieve state-of-the-art performance on widely used benchmark datasets, such as CIFAR10C, CIFAR100C, and ImageNet-C. Our source code is available at https://anonymous.4open.science/r/D152/.
C1 [Xiong, Haoyu; Xiang, Yu] Yunnan Normal Univ, Sch Informat Sci & Technol, Kunming, Yunnan, Peoples R China.
C3 Yunnan Normal University
RP Xiang, Y (corresponding author), Yunnan Normal Univ, Sch Informat Sci & Technol, Kunming, Yunnan, Peoples R China.
EM xiangyu@ynnu.edu.cn
RI xiang, yu/KOC-9302-2024
CR Ahn H, 2019, ADV NEUR IN, V32
   Baldi P., 2013, Advances in neural information processing systems, P2814
   Bender EM, 2021, PROCEEDINGS OF THE 2021 ACM CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, FACCT 2021, P610, DOI 10.1145/3442188.3445922
   Chen C, 2020, AAAI CONF ARTIF INTE, V34, P3422
   Chen ZH, 2023, IEEE T PATTERN ANAL, V45, P13489, DOI 10.1109/TPAMI.2023.3293885
   Cheng ZW, 2021, NEURAL COMPUT APPL, V33, P6891, DOI 10.1007/s00521-020-05465-7
   Croce F., 2020, arXiv, DOI 10.48550/arXiv.2010.09670
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957
   Englesson E., 2021, arXiv
   Foret P., 2021, INT C LEARN REPR
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Garg S., 2020, Advances in Neural Information Processing Systems, V33, P3290
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D, 2018, P INT C LEARN REPR
   Hendrycks D, 2019, Arxiv, DOI arXiv:1903.12261
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Huang P., 2022, Adv. Neural Inform. Process. Syst, V35, P10656
   Huang SY, 2017, Arxiv, DOI arXiv:1702.02284
   Kang GL, 2022, IEEE T PATTERN ANAL, V44, P1793, DOI 10.1109/TPAMI.2020.3029948
   Kang GL, 2019, PROC CVPR IEEE, P4888, DOI 10.1109/CVPR.2019.00503
   Kim D, 2022, LECT NOTES COMPUT SC, V13693, P621, DOI 10.1007/978-3-031-19827-4_36
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   Kumar A, 2020, PR MACH LEARN RES, V119
   Laskin M, 2020, PR MACH LEARN RES, V119
   Lee D.-H., 2013, WORKSH CHALL REPR LE, V3, P896
   Lee Y, 2011, ETR&D-EDUC TECH RES, V59, P593, DOI 10.1007/s11423-010-9177-y
   Li H, 2022, LECT NOTES COMPUT SC, V13596, P32, DOI 10.1007/978-3-031-17899-3_4
   Liang J., 2020, International Conference on Machine Learning, P6028
   Liang J, 2023, Arxiv, DOI arXiv:2303.15361
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu YH, 2019, Arxiv, DOI [arXiv:1907.11692, DOI 10.48550/ARXIV.1907.11692]
   Ma WN, 2022, LECT NOTES COMPUT SC, V13433, P313, DOI 10.1007/978-3-031-16437-8_30
   Manli S., 2022, NEURIPS
   McCloskey M., 1989, Psychology of learning and motivation, V24, P165
   Nado Z, 2021, Arxiv, DOI arXiv:2006.10963
   Niu S., 2023, INT C LEARN REPR
   Parisi GI, 2019, NEURAL NETWORKS, V113, P54, DOI 10.1016/j.neunet.2019.01.012
   Qi T., 2022, P ADV NEUR INF PROC, V35, P7852
   Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587
   Rolnick D, 2019, 33 C NEURAL INFORM P, V32
   Sagawa Shiori, 2022, INT C LEARN REPR ICL
   Shin H, 2017, ADV NEUR IN, V30
   Shin I, 2022, PROC CVPR IEEE, P16907, DOI 10.1109/CVPR52688.2022.01642
   Singh A., 2022, AAAIW
   Song Y., 2019, Adv. Neural Inform. Process. Syst, V32, P19
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Su Yongyi, 2022, Advances in Neural Information Processing Systems
   Sun Y., 2020, PMLR, V119, P9229
   Sun Y, 2023, IEEE T IMAGE PROCESS, V32, P1732, DOI 10.1109/TIP.2023.3251025
   Tarvainen A, 2017, ADV NEUR IN, V30
   Wager S, 2013, Advances in neural information processing systems
   Wagner T., 2018, INT C MACH LEARN, P5095
   Wang DQ, 2021, Arxiv, DOI [arXiv:2006.10726, 10.48550/arXiv.2006.10726]
   Wang JD, 2023, IEEE T KNOWL DATA EN, V35, P8052, DOI 10.1109/TKDE.2022.3178128
   Wang Q, 2022, PROC CVPR IEEE, P7191, DOI 10.1109/CVPR52688.2022.00706
   Wang Xin, 2022, 2022 IEEE Smartworld, Ubiquitous Intelligence & Computing, Scalable Computing & Communications, Digital Twin, Privacy Computing, Metaverse, Autonomous & Trusted Vehicles (SmartWorld/UIC/ScalCom/DigitalTwin/PriComp/Meta), P1360, DOI 10.1109/SmartWorld-UIC-ATC-ScalCom-DigitalTwin-PriComp-Metaverse56740.2022.00196
   Wang YS, 2019, IEEE I CONF COMP VIS, P322, DOI 10.1109/ICCV.2019.00041
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu M, 2023, PATTERN RECOGN, V137, DOI 10.1016/j.patcog.2023.109347
   Yang LH, 2023, PROC CVPR IEEE, P7236, DOI 10.1109/CVPR52729.2023.00699
   Yao Huaxiu, 2022, ADV NEURAL INFORM PR, P10309
   Zagoruyko S, 2017, Arxiv, DOI arXiv:1605.07146
   Zhang XX, 2023, PROC CVPR IEEE, P20247, DOI 10.1109/CVPR52729.2023.01939
   Zhang Yi-Fan, 2022, DOMAIN SPECIFIC RISK
   Zhou KY, 2023, IEEE T PATTERN ANAL, V45, P4396, DOI 10.1109/TPAMI.2022.3195549
NR 66
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 1
PY 2024
DI 10.1007/s00371-024-03327-0
EA APR 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MP7C0
UT WOS:001194882700004
DA 2024-08-05
ER

PT J
AU Song, GS
   Gai, SY
   Da, F
AF Song, Gusu
   Gai, Shaoyan
   Da, Feipeng
TI Memory-based gradient-guided progressive propagation network for video
   deblurring
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Video deblurring; Memory network; RNN; Gradient map
ID SUPERRESOLUTION; BLUR
AB Video deblurring is a challenging visual task because it requires handling temporal correlations among frames and dealing with various sources of uncertainty in motion blur. To tackle these challenges and effectively capture the spatiotemporal relationships in video sequences, we propose a memory-based gradient-guided progressive propagation network. Our network combines the memory and deblurring branches to restore sharp frames. The memory branch is responsible for storing blurry-sharp feature pairs to provide valuable information for the deblurring process. Within the deblurring branch, we propose two key modules, the progressive feature propagation (PFP) module and the gradient-guided reconstruction (GGR) module , to extract better features that help recover image details. The PFP module is designed to differentiate and align diverse features across different frames. By grouping and progressively propagating these features, the module integrates video information from multiple scales. The GGR module utilizes structural priors present in gradient features to guide the fusion of features. This module leverages the inherent structural information in the gradient features to enhance the recovery of fine details and reduce geometric distortions in the deblurred frames. Experimental results demonstrate that our proposed method achieves competitive results with state-of-the-art methods, while our model is less complex and requires relatively fewer parameters.
C1 [Song, Gusu; Gai, Shaoyan; Da, Feipeng] Southeast Univ, Sch Automat, Nanjing 210096, Jiangsu, Peoples R China.
   [Song, Gusu; Gai, Shaoyan; Da, Feipeng] Minist Educ, Key Lab Measurement & Control Complex Syst Engn, Nanjing 210096, Jiangsu, Peoples R China.
   [Da, Feipeng] Southeast Univ, Shenzhen Res Inst, Shenzhen 518063, Guangdong, Peoples R China.
C3 Southeast University - China; Southeast University - China
RP Gai, SY; Da, F (corresponding author), Southeast Univ, Sch Automat, Nanjing 210096, Jiangsu, Peoples R China.; Gai, SY; Da, F (corresponding author), Minist Educ, Key Lab Measurement & Control Complex Syst Engn, Nanjing 210096, Jiangsu, Peoples R China.; Da, F (corresponding author), Southeast Univ, Shenzhen Res Inst, Shenzhen 518063, Guangdong, Peoples R China.
EM song_gusu@163.com; qxxymm@163.com; dafp@seu.edu.cn
OI Gai, Shaoyan/0000-0001-5750-4013
FU Special Project on Basic Research of Frontier Leading Technology of
   Jiangsu Province of China [BK20192004C]
FX This work was supported by the Special Project on Basic Research of
   Frontier Leading Technology of Jiangsu Province of China (Grant No.
   BK20192004C).
CR Cao MD, 2023, IEEE T CIRC SYST VID, V33, P160, DOI 10.1109/TCSVT.2022.3201045
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chan KCK, 2021, PROC CVPR IEEE, P4945, DOI 10.1109/CVPR46437.2021.00491
   CHARBONNIER P, 1994, IEEE IMAGE PROC, P168
   Chen J, 2022, VISUAL COMPUT, V38, P3191, DOI 10.1007/s00371-022-02564-5
   Chen LY, 2022, LECT NOTES COMPUT SC, V13667, P17, DOI 10.1007/978-3-031-20071-7_2
   Cheng HK, 2021, ADV NEUR IN, V34
   Cheng Ma, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7766, DOI 10.1109/CVPR42600.2020.00779
   Cheng ZZ, 2023, SIGNAL PROCESS-IMAGE, V113, DOI 10.1016/j.image.2023.116924
   Cho SJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4621, DOI 10.1109/ICCV48922.2021.00460
   Choi YJ, 2022, PATTERN RECOGN LETT, V164, P246, DOI 10.1016/j.patrec.2022.11.014
   Claus M, 2019, IEEE COMPUT SOC CONF, P1843, DOI 10.1109/CVPRW.2019.00235
   Deng JN, 2020, AAAI CONF ARTIF INTE, V34, P10696
   Doersch C., 2020, NeurIPS, V33, P21981
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fang N, 2022, NEUROCOMPUTING, V489, P128, DOI 10.1016/j.neucom.2022.02.067
   Guan ZY, 2021, IEEE T PATTERN ANAL, V43, P949, DOI 10.1109/TPAMI.2019.2944806
   Guo Q, 2021, IEEE T IMAGE PROCESS, V30, P1812, DOI 10.1109/TIP.2020.3045630
   Hao ZY, 2023, COMPUT VIS IMAGE UND, V233, DOI 10.1016/j.cviu.2023.103693
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu DD, 2023, VISUAL COMPUT, V39, P281, DOI 10.1007/s00371-021-02329-6
   Hui TW, 2018, PROC CVPR IEEE, P8981, DOI 10.1109/CVPR.2018.00936
   Javaran TA, 2017, MACH VISION APPL, V28, P431, DOI 10.1007/s00138-017-0824-8
   Ji B, 2022, PROC CVPR IEEE, P1918, DOI 10.1109/CVPR52688.2022.00196
   Jiang BR, 2022, LECT NOTES COMPUT SC, V13678, P663, DOI 10.1007/978-3-031-19797-0_38
   Khan A, 2021, VISUAL COMPUT, V37, P1661, DOI 10.1007/s00371-020-01930-5
   Kim TH, 2018, IEEE T PATTERN ANAL, V40, P2374, DOI 10.1109/TPAMI.2017.2761348
   Kingma D. P., 2014, arXiv
   Li C, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3587468
   Liang JY, 2022, Arxiv, DOI [arXiv:2201.12288, DOI 10.48550/ARXIV.2201.12288]
   Lin J., 2022, P INT C MACHINE LEAR, P13394
   Liu J, 2023, VISUAL COMPUT, V39, P3091, DOI 10.1007/s00371-022-02515-0
   Mao YX, 2023, IEEE T CIRC SYST VID, V33, P172, DOI 10.1109/TCSVT.2022.3202361
   Miller A, 2016, Arxiv, DOI arXiv:1606.03126
   Nah S, 2019, PROC CVPR IEEE, P8094, DOI 10.1109/CVPR.2019.00829
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Oh SW, 2019, IEEE I CONF COMP VIS, P9225, DOI 10.1109/iccv.2019.00932
   Pan JS, 2020, PROC CVPR IEEE, P3040, DOI 10.1109/CVPR42600.2020.00311
   Pan LY, 2020, IEEE T IMAGE PROCESS, V29, P1748, DOI 10.1109/TIP.2019.2945867
   Parvaz R, 2023, VISUAL COMPUT, V39, P2653, DOI 10.1007/s00371-022-02484-4
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Rota C, 2023, ARTIF INTELL REV, V56, P5317, DOI 10.1007/s10462-022-10302-5
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Son H, 2022, COMPUT GRAPH FORUM, V41, P177, DOI 10.1111/cgf.14667
   Son H, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3453720
   Sukhbaatar S, 2015, ADV NEUR IN, V28
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Tassano M, 2020, PROC CVPR IEEE, P1351, DOI 10.1109/CVPR42600.2020.00143
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang CH, 2023, INT J COMPUT VISION, V131, P1840, DOI 10.1007/s11263-023-01793-y
   Wang T, 2021, COMPUT VIS IMAGE UND, V203, DOI 10.1016/j.cviu.2020.103135
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wang YS, 2022, LECT NOTES COMPUT SC, V13679, P413, DOI 10.1007/978-3-031-19800-7_24
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weston J, 2015, Arxiv, DOI arXiv:1410.3916
   Zhan ZQ, 2019, NEUROCOMPUTING, V341, P88, DOI 10.1016/j.neucom.2019.03.009
   Zhang HC, 2022, LECT NOTES COMPUT SC, V13676, P581, DOI 10.1007/978-3-031-19787-1_33
   Zhang KH, 2019, IEEE T IMAGE PROCESS, V28, P291, DOI 10.1109/TIP.2018.2867733
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhang ZC, 2023, VISUAL COMPUT, V39, P1375, DOI 10.1007/s00371-022-02415-3
   Zhao HT, 2021, J VIS COMMUN IMAGE R, V74, DOI 10.1016/j.jvcir.2020.102921
   Zhihang Zhong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P191, DOI 10.1007/978-3-030-58539-6_12
   Zhou SC, 2019, IEEE I CONF COMP VIS, P2482, DOI 10.1109/ICCV.2019.00257
   Zhu C, 2022, AAAI CONF ARTIF INTE, P3598
NR 67
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 6
PY 2024
DI 10.1007/s00371-024-03306-5
EA MAR 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JT6V2
UT WOS:001175462100001
DA 2024-08-05
ER

PT J
AU Hou, BW
   Li, GY
AF Hou, Bowen
   Li, Gongyan
TI PCCFormer: Parallel coupled convolutional transformer for image
   super-resolution
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image super-resolution; Transformer; Convolutional neural network
ID NETWORK
AB In recent years, with the continuous development of deep learning technology, image super-resolution (SR) has witnessed tremendous progress. Transformer as an emerging deep learning model, its application in the field of image SR is also gradually attracted the attention of researchers. In this paper, we propose parallel coupled convolutional transformer (PCCFormer) architecture for image SR. The proposed design uses transformer and convolutional neural network (CNN) as the structure of the network, thus leveraging the complementary benefits. We use parallel attention transformer to improve feature expression ability of the model. We introduce adaptive convolution residual block to effectively extract the image local information. We introduce a novel concurrent module to aggregate enriched features, including long-range features from transformer and local features captured by CNN. Extensive experiments conducted on benchmark datasets illustrate that PCCFormer surpasses existing image SR methods. For example, our PCCFormer achieves the PSNR of 34.01 dB on Urban100 dataset with scale factor 2.
C1 [Hou, Bowen] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Hou, Bowen; Li, Gongyan] Chinese Acad Sci, Inst Microelect, Beijing 100029, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; Institute of Microelectronics, CAS
RP Hou, BW (corresponding author), Univ Chinese Acad Sci, Beijing 100049, Peoples R China.; Hou, BW (corresponding author), Chinese Acad Sci, Inst Microelect, Beijing 100029, Peoples R China.
EM houbowen@ime.ac.cn; ligongyan@ime.ac.cn
FU International Partnership Program of Chinese Academy of Sciences
FX No Statement Available
CR Arefin MR, 2020, IEEE COMPUT SOC CONF, P816, DOI 10.1109/CVPRW50498.2020.00111
   Ben Niu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P191, DOI 10.1007/978-3-030-58610-2_12
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Cao JZ, 2022, LECT NOTES COMPUT SC, V13678, P325, DOI 10.1007/978-3-031-19797-0_19
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen HY, 2021, Arxiv, DOI [arXiv:2104.09497, DOI 10.48550/ARXIV.2104.09497]
   Chen XY, 2023, PROC CVPR IEEE, P22367, DOI 10.1109/CVPR52729.2023.02142
   Chen YP, 2022, PROC CVPR IEEE, P5260, DOI 10.1109/CVPR52688.2022.00520
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Feichtenhofer C, 2017, PROC CVPR IEEE, P7445, DOI 10.1109/CVPR.2017.787
   Gavade A., 2014, NAT C ADV TECHN EL E, V10
   Han Fang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P741, DOI 10.1007/978-3-030-58555-6_44
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XY, 2019, PROC CVPR IEEE, P1732, DOI 10.1109/CVPR.2019.00183
   Hendrycks D, 2020, Arxiv, DOI arXiv:1606.08415
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Irfan MA, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11040464
   Isobe Takashi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P645, DOI 10.1007/978-3-030-58610-2_38
   Jin Z, 2020, IEEE T MULTIMEDIA, V22, P1055, DOI 10.1109/TMM.2019.2938340
   Kim J.-H., 2018, ARXIV PREPRINT ARXIV, V2, P2
   Kim J, 2016, PROC CVPR IEEE, P1646, DOI 10.1109/CVPR.2016.182
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Li WB, 2022, Arxiv, DOI arXiv:2112.10175
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu ZS, 2022, IEEE COMPUT SOC CONF, P456, DOI 10.1109/CVPRW56347.2022.00061
   Maeda S, 2022, LECT NOTES COMPUT SC, V13679, P464, DOI 10.1007/978-3-031-19800-7_27
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mei YQ, 2021, PROC CVPR IEEE, P3516, DOI 10.1109/CVPR46437.2021.00352
   Mei YQ, 2020, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR42600.2020.00573
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shiina K, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-88605-w
   Simonyan K, 2014, ADV NEUR IN, V27
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su JN, 2023, IEEE T PATTERN ANAL, V45, P8453, DOI 10.1109/TPAMI.2022.3229689
   Sun N, 2019, IEEE ACCESS, V7, P186470, DOI 10.1109/ACCESS.2019.2960828
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Timofte R, 2018, IEEE COMPUT SOC CONF, P965, DOI 10.1109/CVPRW.2018.00130
   Vaswani A, 2017, ADV NEUR IN, V30
   Vavilala V., 2020, SPECIAL INTEREST GRO, P1
   Wang ZW, 2015, IEEE I CONF COMP VIS, P370, DOI 10.1109/ICCV.2015.50
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Wen RM, 2022, NEUROCOMPUTING, V504, P240, DOI 10.1016/j.neucom.2022.07.050
   Xia B, 2022, AAAI CONF ARTIF INTE, P2759
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Ying XY, 2020, IEEE SIGNAL PROC LET, V27, P496, DOI 10.1109/LSP.2020.2973813
   Yoo J, 2023, IEEE WINT CONF APPL, P4945, DOI 10.1109/WACV56688.2023.00493
   Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344
   Zhang XD, 2022, LECT NOTES COMPUT SC, V13677, P649, DOI 10.1007/978-3-031-19790-1_39
   Zhang YL, 2021, PROC CVPR IEEE, P13420, DOI 10.1109/CVPR46437.2021.01322
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhou S., 2020, Advances in Neural Information Processing Systems, V33, P3499
   Zhu X, 2017, MATH PROBL ENG, V2017, DOI 10.1155/2017/3259357
   Zou WB, 2022, IEEE COMPUT SOC CONF, P929, DOI 10.1109/CVPRW56347.2022.00107
NR 64
TC 0
Z9 0
U1 16
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 5
PY 2024
DI 10.1007/s00371-023-03257-3
EA FEB 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GY9N9
UT WOS:001156357600001
DA 2024-08-05
ER

PT J
AU Ouyang, Z
   Zhao, HH
   Zhang, YD
   Chen, L
AF Ouyang, Ze
   Zhao, Huihuang
   Zhang, Yudong
   Chen, Long
TI STVDNet: spatio-temporal interactive video de-raining network
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Auto-encoder; Computer vision; De-raining; LSTM; SSIM loss function
AB Video de-raining is of significant importance problem in computer vision as rain streaks adversely affect the visual quality of images and hinder subsequent vision-related tasks. Existing video de-raining methods still face challenges such as black shadows and loss of details. In this paper, we introduced a novel de-raining framework called STVDNet, which effectively solves the issues of black shadows and detail loss after de-raining. STVDNet utilizes a Spatial Detail Feature Extraction Module based on an auto-encoder to capture the spatial characteristics of the video. Additionally, we introduced an innovative interaction between the extracted spatial features and Spatio-Temporal features using LSTM to generate initial de-raining results. Finally, we employed 3D convolution and 2D convolution for the detailed processing of the coarse videos. During the training process, we utilized three loss functions, among which the SSIM loss function was employed to process the generated videos, aiming to enhance their detail structure and color recovery. Through extensive experiments conducted on three public datasets, we demonstrated the superiority of our proposed method over state-of-the-art approaches. We also provide our code and pre-trained models at https://github.com/O-Y-ZONE/STVDNet.git.
C1 [Ouyang, Ze; Zhao, Huihuang; Chen, Long] Hengyang Normal Univ, Sch Comp Sci & Technol, Hengyang, Peoples R China.
   [Zhao, Huihuang] Hunan Univ, Natl Engn Lab Robot Visual Percept & Control Techn, Changsha, Peoples R China.
   [Zhang, Yudong] Univ Leicester, Sch Comp & Math Sci, Univ Rd, Leicester LE1 7RH, England.
C3 Hengyang Normal University; Hunan University; University of Leicester
RP Zhao, HH (corresponding author), Hengyang Normal Univ, Sch Comp Sci & Technol, Hengyang, Peoples R China.; Zhao, HH (corresponding author), Hunan Univ, Natl Engn Lab Robot Visual Percept & Control Techn, Changsha, Peoples R China.
EM betterlife008@163.com
FU National Natural Science Foundation of China [61772179]; Hunan
   Provincial Natural Science Foundation of China [2023JJ50095,
   2022JJ50016]; Science and Technology Innovation Program of Hunan
   Province [2016TP1020]; Industry University Research Innovation
   Foundation of Ministry of Education Science and Technology Development
   Center [2020QT09]; The "14th Five-Year Plan" Key Disciplines and
   Application-oriented Special Disciplines of Hunan Province [351];
   Postgraduate Scientific Research Innovation Project of Hunan Province
   [CX20231265]; Open Research Fund of The State Key Laboratory of
   Multimodal Artificial Intelligence Systems [MAIS-2023-09]
FX This work was supported by the National Natural Science Foundation of
   China (61772179), Hunan Provincial Natural Science Foundation of China
   (2023JJ50095, 2022JJ50016), the Science and Technology Innovation
   Program of Hunan Province (2016TP1020), Industry University Research
   Innovation Foundation of Ministry of Education Science and Technology
   Development Center (2020QT09), The "14th Five-Year Plan" Key Disciplines
   and Application-oriented Special Disciplines of Hunan Province
   (Xiangjiaotong [2022] 351), Postgraduate Scientific Research Innovation
   Project of Hunan Province (CX20231265), and Open Research Fund of The
   State Key Laboratory of Multimodal Artificial Intelligence Systems
   (MAIS-2023-09). DAS:The datasets used in this paper are all public
   datasets, which can be found in the links we provide.
CR Chen J, 2018, PROC CVPR IEEE, P6286, DOI 10.1109/CVPR.2018.00658
   Chen X, 2022, PROC CVPR IEEE, P2007, DOI 10.1109/CVPR52688.2022.00206
   Cui X, 2022, IEEE T CIRC SYST VID, V32, P8327, DOI 10.1109/TCSVT.2022.3190516
   Du YJ, 2020, IEEE T IMAGE PROCESS, V29, P6288, DOI 10.1109/TIP.2020.2990606
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Guo Q, 2021, AAAI CONF ARTIF INTE, V35, P1487
   Jaseena KU, 2021, ENERG CONVERS MANAGE, V234, DOI 10.1016/j.enconman.2021.113944
   Jiang K, 2021, IEEE T CIRC SYST VID, V31, P3981, DOI 10.1109/TCSVT.2020.3044887
   Jiang TX, 2019, IEEE T IMAGE PROCESS, V28, P2089, DOI 10.1109/TIP.2018.2880512
   Kaihao Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P71, DOI 10.1007/978-3-030-58583-9_5
   Kim JH, 2014, IEEE IMAGE PROC, P5432, DOI 10.1109/ICIP.2014.7026099
   Kumari D, 2024, ELECTRONICS-SWITZ, V13, DOI 10.3390/electronics13071229
   Li J., 2022, CHIN C PATT REC COMP, P165
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Liu JY, 2018, PROC CVPR IEEE, P3233, DOI 10.1109/CVPR.2018.00341
   Liu JY, 2019, IEEE T IMAGE PROCESS, V28, P699, DOI 10.1109/TIP.2018.2869722
   Liu T, 2019, IEEE INT CON MULTI, P664, DOI 10.1109/ICME.2019.00120
   Ota J., 2017, SPIE, V10133, P509
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Roy AM, 2022, COMPUT ELECTRON AGR, V193, DOI 10.1016/j.compag.2022.106694
   Wang H, 2021, PROC CVPR IEEE, P14786, DOI 10.1109/CVPR46437.2021.01455
   Wang H, 2020, PROC CVPR IEEE, P3100, DOI 10.1109/CVPR42600.2020.00317
   Wei YY, 2021, IEEE T IMAGE PROCESS, V30, P4788, DOI 10.1109/TIP.2021.3074804
   Wu QB, 2020, IEEE T CIRC SYST VID, V30, P3883, DOI 10.1109/TCSVT.2020.2972566
   Yang WH, 2020, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR42600.2020.00179
   Yang WH, 2019, PROC CVPR IEEE, P1661, DOI 10.1109/CVPR.2019.00176
   Yang WH, 2020, IEEE T PATTERN ANAL, V42, P1377, DOI 10.1109/TPAMI.2019.2895793
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Ye YT, 2022, PROC CVPR IEEE, P5811, DOI 10.1109/CVPR52688.2022.00573
   Yu CF, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2634, DOI 10.1145/3474085.3475441
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zhang K., 2017, Proceedings of the IEEE 2017 Conference on Computer Vision and Pattern Recognition, DOI [DOI 10.1109/CVPR.2017.300, 10.1109/CVPR.2017.300]
   Zhang KH, 2023, IEEE T PATTERN ANAL, V45, P1287, DOI 10.1109/TPAMI.2022.3148707
   Zhang L, 2017, IEEE INT CONF COMP V, P3120, DOI 10.1109/ICCVW.2017.369
   Zhang YL, 2021, IEEE T PATTERN ANAL, V43, P2480, DOI 10.1109/TPAMI.2020.2968521
NR 36
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 20
PY 2024
DI 10.1007/s00371-024-03565-2
EA JUL 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZB1O2
UT WOS:001272741200001
DA 2024-08-05
ER

PT J
AU Terzioglu, B
   Celikcan, U
   Capin, TK
AF Terzioglu, Berkay
   Celikcan, Ufuk
   Capin, Tolga Kurtulus
TI Gaze-contingent adaptation of VR stereo parameters for cybersickness
   prevention
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual reality; Eye tracking; Cyber-sickness; Stereo parameters;
   Gaze-contingent rendering
ID DEPTH-PERCEPTION; SICKNESS
AB Extended exposure to virtual reality displays has been linked to the emergence of cybersickness, characterized by symptoms such as nausea, dizziness, fatigue, and disruptions in eye movements. The main objective of our study is to examine the effects of real-time fine-tuning of stereo parameters and blurriness in virtual reality on the discomfort level of users who are experiencing motion sickness triggered by the display. Our hypothesis proposes that by dynamically correcting the rendering settings, the symptoms of motion sickness can be relieved and the overall VR user experience can be improved. Our methodology commences with a prediction model for the comfort level of the viewer based on their gaze parameters, such as pupil diameter, blink count, gaze position, and fixation duration. We then propose a method to dynamically adapt the stereoscopic rendering parameters by considering the predicted comfort level of the viewer.
C1 [Terzioglu, Berkay; Capin, Tolga Kurtulus] TED Univ, Ankara, Turkiye.
   [Celikcan, Ufuk] Hacettepe Univ, Ankara, Turkiye.
C3 Ted University; Hacettepe University
RP Capin, TK (corresponding author), TED Univ, Ankara, Turkiye.
EM tolga.capin@tedu.edu.tr
FU TED University
FX No Statement Available
CR Avan E, 2022, COMPUT GRAPH-UK, V102, P390, DOI 10.1016/j.cag.2021.09.016
   Bruck S, 2009, I C COMP GRAPH IM VI, P486, DOI 10.1109/CGIV.2009.83
   Carnegie K, 2015, IEEE COMPUT GRAPH, V35, P34, DOI 10.1109/MCG.2015.98
   Cebeci B, 2024, COMPUT GRAPH-UK, V118, P23, DOI 10.1016/j.cag.2023.10.012
   Cebeci B, 2019, COMPUT ANIMAT VIRT W, V30, DOI 10.1002/cav.1893
   Chang E, 2021, J COMPUT DES ENG, V8, P728, DOI 10.1093/jcde/qwab010
   Conti J, 2017, COMPUT GRAPH-UK, V69, P24, DOI 10.1016/j.cag.2017.08.017
   Dennison MS, 2016, DISPLAYS, V44, P42, DOI 10.1016/j.displa.2016.07.002
   Duchowski D. H., 2014, P ACM S APPL PERC, P39, DOI DOI 10.1145/2628257.2628259
   Hirzle T, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445361
   Hoffman DM, 2008, J VISION, V8, DOI 10.1167/8.3.33
   Hussain Razeen, 2020, 2020 IEEE 4th International Conference on Image Processing, Applications and Systems (IPAS), P71, DOI 10.1109/IPAS50080.2020.9334947
   International Telecommunication Union, 2012, SUBJ METH ASS STER 3
   Islam R, 2021, INT SYM MIX AUGMENT, P31, DOI 10.1109/ISMAR52148.2021.00017
   Kennedy R.S., 1993, Int. J. Aviat. Psy, P203
   Kim H., 2013, IEEE T AUTOM SCI ENG, P1, DOI DOI 10.1109/ISR.2013.6695682
   Kim YY, 2008, PRESENCE-TELEOP VIRT, V17, P1, DOI 10.1162/pres.17.1.1
   Kourtesis P, 2023, ARXIV
   LEDGER H, 2013, PLYMOUTH STUD SCI, V6, P206
   Liu BY, 2021, 2021 IFIP NETWORKING CONFERENCE AND WORKSHOPS (IFIP NETWORKING), DOI 10.23919/IFIPNETWORKING52078.2021.9472798
   Lopes Phil, 2020, P 13 ACM SIGGRAPH C, P1, DOI DOI 10.1145/3424636.3426906
   Martin-Gomez A, 2022, IEEE T VIS COMPUT GR, V28, P4156, DOI 10.1109/TVCG.2021.3079849
   McIntosh L, 2012, COMPUT GRAPH FORUM, V31, P1810, DOI 10.1111/j.1467-8659.2012.02097.x
   Nam Y, 2022, CYBERPSYCH BEH SOC N, V25, P135, DOI 10.1089/cyber.2021.0167
   Naqvi SAA, 2013, IEEE ENG MED BIO, P6405, DOI 10.1109/EMBC.2013.6611020
   Ozkan A, 2023, INT J HUM-COMPUT ST, V176, DOI 10.1016/j.ijhcs.2023.103039
   Ozkan A, 2023, DISPLAYS, V78, DOI 10.1016/j.displa.2023.102415
   Saredakis D, 2020, FRONT HUM NEUROSCI, V14, DOI 10.3389/fnhum.2020.00096
   WANN JP, 1995, VISION RES, V35, P2731, DOI 10.1016/0042-6989(95)00018-U
   Wibirama S, 2018, ENTERTAIN COMPUT, V26, P117, DOI 10.1016/j.entcom.2018.02.003
   William J., 2020, J. Sci. Res., V64, P168, DOI [10.37398/JSR.2020.640137, DOI 10.37398/JSR.2020.640137]
NR 31
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 5017
EP 5028
DI 10.1007/s00371-024-03505-0
EA JUN 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001242677100001
OA hybrid
DA 2024-08-05
ER

PT J
AU Saboo, S
   Singha, J
AF Saboo, Shweta
   Singha, Joyeeta
TI Semantic hand gesture integration system using self-co-articulation and
   movement epenthesis detection
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Hand gesture recognition; Movement epenthesis; Self-co-articulation;
   Deep learning; Classifier fusion
ID RECOGNITION
AB Recognizing hand gestures poses a formidable challenge, particularly when dealing with semantic gestures that require disentanglement prior to recognition. This paper addresses the intricate issue of an additional stroke, commonly referred to as 'movement epenthesis stroke,' which emerges between continuous gestures. Our proposed system employs a multifaceted approach to tackle this challenge. Initially, the system extracts color-motion information to facilitate hand detection, subsequently employing a fusion of shape information and a modified Kanade-Lucas-Tomasi (KLT) feature tracker. This integration significantly mitigates the issue of occlusions. The identification of movement epenthesis is accomplished by analyzing the gesture trajectory using a speed profile. Furthermore, self-co-articulation strokes are discerned by leveraging slope-angle information. To enhance the recognition process, a carefully selected set of 40 features is extracted, which are then employed for recognizing the resulting meaningful gestures. These features serve as inputs to various classification models, including support vector machines (SVM), k-nearest neighbors (kNN), and extreme learning machines (ELM). Deep learning algorithms are judiciously deployed to recognize gesture trajectories, thus streamlining the time-consuming feature extraction process. The outcomes of individual classifiers are amalgamated, resulting in a classifier fusion model. This model is enhanced through majority voting and is used in conjunction with cross-validation results. The experimental analysis culminates in an impressive accuracy rate of 98.88% achieved by the classifier fusion model. This achievement surpasses the performance of individual classifiers, underscoring the effectiveness of our proposed methodology.
C1 [Saboo, Shweta; Singha, Joyeeta] LNM Inst Informat Technol, Dept Elect & Commun Engn, Jaipur 302031, India.
   [Saboo, Shweta] Jaipur Engn Coll & Res Ctr, Jaipur, India.
C3 LNM Institute of Information Technology
RP Saboo, S (corresponding author), LNM Inst Informat Technol, Dept Elect & Commun Engn, Jaipur 302031, India.; Saboo, S (corresponding author), Jaipur Engn Coll & Res Ctr, Jaipur, India.
EM shweta.saboo.y18pg@lnmiit.ac.in; joyeeta.singha@lnmiit.ac.in
FU DST; SEED Division [SP/YO/407/2018]
FX This work is supported by DST (Govt. of India) under the SEED Division
   [SP/YO/407/2018].
CR Aggarwal JK, 1999, COMPUT VIS IMAGE UND, V73, P428, DOI 10.1006/cviu.1998.0744
   Alam MS, 2021, IEEE T HUM-MACH SYST, V51, P229, DOI 10.1109/THMS.2021.3066854
   Asaari MSM, 2015, MULTIMED TOOLS APPL, V74, P9231, DOI 10.1007/s11042-014-2078-z
   Avola D, 2023, LECT NOTES COMPUT SC, V14233, P500, DOI 10.1007/978-3-031-43148-7_42
   Bhuyan MK, 2006, CONF CYBERN INTELL S, P748
   Bhuyan MK, 2014, J MULTIMODAL USER IN, V8, P333, DOI 10.1007/s12193-014-0165-0
   Chai D, 1999, IEEE T CIRC SYST VID, V9, P551, DOI 10.1109/76.767122
   Chen MY, 2016, IEEE T HUM-MACH SYST, V46, P403, DOI 10.1109/THMS.2015.2492598
   Cheng H, 2016, IEEE T CIRC SYST VID, V26, P1659, DOI 10.1109/TCSVT.2015.2469551
   Choudhury A., 2021, SN Comput. Sci, V2, P1, DOI [10.1007/s42979-021-00544-5, DOI 10.1007/S42979-021-00544-5]
   Choudhury A, 2017, J INTELL SYST, V26, P471, DOI 10.1515/jisys-2016-0009
   Chuang ZJ, 2006, IEEE T CIRC SYST VID, V16, P1313, DOI 10.1109/TCSVT.2006.883509
   Guo JM, 2011, IEEE IMAGE PROC, P549, DOI 10.1109/ICIP.2011.6116404
   Guo L, 2021, IEEE T HUM-MACH SYST, V51, P300, DOI 10.1109/THMS.2021.3086003
   Imran J, 2020, VISUAL COMPUT, V36, P1233, DOI 10.1007/s00371-019-01725-3
   Kao CY, 2011, PROCEDIA ENGINEER, V15, DOI 10.1016/j.proeng.2011.08.700
   Kapitanov A., 2024, P IEEE CVF WINT C AP, P4572
   Kelly D, 2009, 2009 13TH INTERNATIONAL MACHINE VISION AND IMAGE PROCESSING CONFERENCE, P145, DOI 10.1109/IMVIP.2009.33
   Kolsch M., 2004, Computer Vision and Pattern Recognition Workshop, P158
   Lee HK, 1999, IEEE T PATTERN ANAL, V21, P961, DOI 10.1109/34.799904
   Li GF, 2019, CLUSTER COMPUT, V22, pS2719, DOI 10.1007/s10586-017-1435-x
   Liang RH, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P558, DOI 10.1109/AFGR.1998.671007
   Lin J, 2013, OPTIK, V124, P6795, DOI 10.1016/j.ijleo.2013.05.097
   Mahmud H, 2022, VISUAL COMPUT, V38, P1015, DOI 10.1007/s00371-021-02065-x
   Miah ASM, 2023, IEEE ACCESS, V11, P4703, DOI 10.1109/ACCESS.2023.3235368
   Nadgeri S. M., 2010, Proceedings of the Third International Conference on Emerging Trends in Engineering and Technology (ICETET 2010), P37, DOI 10.1109/ICETET.2010.63
   Hoang NN, 2019, IEEE IMAGE PROC, P539, DOI [10.1109/icip.2019.8803813, 10.1109/ICIP.2019.8803813]
   Quattoni A, 2007, IEEE T PATTERN ANAL, V29, P1848, DOI 10.1109/TPAMI.2007.1124
   Rakun E., 2022, Indones. J. Electr. Eng. Comput. Sci, V26, P1402
   RUBINE D, 1991, COMP GRAPH, V25, P329, DOI 10.1145/127719.122753
   Saboo S, 2022, MULTIMEDIA SYST, V28, P183, DOI 10.1007/s00530-021-00811-8
   Saboo S, 2021, MULTIMED TOOLS APPL, V80, P20579, DOI 10.1007/s11042-021-10669-7
   Shan CF, 2007, PATTERN RECOGN, V40, P1958, DOI 10.1016/j.patcog.2006.12.012
   Sharkey A. J., 2012, Combining artificial neural nets: ensemble and modular multi-net systems
   Signer B., 2011, Technical Report/ETH Zurich, Department of Computer Science, V561
   Singha J, 2017, MULTIMEDIA SYST, V23, P499, DOI 10.1007/s00530-016-0510-0
   Singha J, 2016, IET COMPUT VIS, V10, P143, DOI 10.1049/iet-cvi.2014.0432
   Singha J, 2015, IETE J RES, V61, P597, DOI 10.1080/03772063.2015.1054900
   Sruthi CJ, 2023, VISUAL COMPUT, V39, P6183, DOI 10.1007/s00371-022-02720-x
   Theodoridis  S., 2010, INTRO PATTERN RECOGN
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wu HY, 2017, VISUAL COMPUT, V33, P1265, DOI 10.1007/s00371-015-1147-2
   Xu D, 2015, J INTELL ROBOT SYST, V77, P583, DOI 10.1007/s10846-014-0039-4
   Yang RD, 2010, IEEE T PATTERN ANAL, V32, P462, DOI 10.1109/TPAMI.2009.26
   Zhao L., 2001, Synthesis and acquisition of laban movement analysis qualitative parameters for communicative gestures
NR 45
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 7
PY 2024
DI 10.1007/s00371-024-03394-3
EA MAY 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QD2K1
UT WOS:001218872900003
DA 2024-08-05
ER

PT J
AU Zhou, H
   Yin, JJ
   Yang, YL
   Fang, M
   Li, P
AF Zhou, Hao
   Yin, Junjie
   Yang, Yilun
   Fang, Meie
   Li, Ping
TI Topology-guided accelerated vector field streamline visualization
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Vector field; Flow visualization; Topology-guided; Streamline
   visualization; Adaptive seed placement
AB Streamline visualization is crucial for understanding 3D vector fields, and seed placement is essential for high-quality streamlines. Current algorithms are either uniform, omitting vital information, or slow due to precalculation. We propose three novel techniques to build a topology-guided accelerated vector field streamline visualization framework. First, we convert the tracing process into an iterative one, dynamically generating seed points based on critical region detection and a cooldown mechanic. Second, we introduce "planar critical points" combined with traditional critical points to identify critical regions, and place new seed points according to their critical point types. During this process, we circumvent complex eigenvalue calculation with determining whether the eigenvalues are real, which is enough for our placement strategies. Third, we offer a fast streamline simplification technique based on global distortion to reduce clutter. Based on the proposed streamline visualization framework and various vector field visualization methods, we develop a CUDA-based application tool called AdaptiFlux, which achieves real-time visualization of vector fields more efficiently than those representative visualization tools.
C1 [Zhou, Hao; Yin, Junjie; Yang, Yilun; Fang, Meie] Guangzhou Univ, Metaverse Res Inst, Sch Comp Sci & Cyber Engn, 230 Wai Huan Xi Rd, Guangzhou 510006, Guangdong, Peoples R China.
   [Li, Ping] Hong Kong Polytech Univ, Dept Comp, Hung Hom, Hong Kowloon, 11 Yuk Choi Rd, Hong Kong, Peoples R China.
   [Li, Ping] Hong Kong Polytech Univ, Sch Design, Hung Hom, Hong Kowloon, 11 Yuk Choi Rd, Hong Kong, Peoples R China.
C3 Guangzhou University; Hong Kong Polytechnic University; Hong Kong
   Polytechnic University
RP Fang, M (corresponding author), Guangzhou Univ, Metaverse Res Inst, Sch Comp Sci & Cyber Engn, 230 Wai Huan Xi Rd, Guangzhou 510006, Guangdong, Peoples R China.
EM 2112106278@e.gzhu.edu.cn; yjj@e.gzhu.edu.cn; cnyyl@126.com;
   fme@gzhu.edu.cn; p.li@polyu.edu.hk
OI Fang, Meie/0000-0003-4292-8889
FU National Natural Science Foundation of China
FX No Statement Available
CR Ahrens J., 2005, Visualization handbook, P717, DOI DOI 10.1016/B978-012387582-2/50038-1
   Andrysco N., 2005, A user study contrasting 2d unsteady vector field visualization techniques
   Brambilla A., 2013, VMV, P1
   Bujack R., 2021, Topological Methods in Data Analysis and Visualization VI, P111, DOI [10.1007/978-3-030-83500-2_7, DOI 10.1007/978-3-030-83500-2_7]
   Bujack R, 2020, ENVIRON EARTH SCI, V79, DOI 10.1007/s12665-019-8800-4
   Butcher J., 2007, Scholarpedia, V2, P3147, DOI [DOI 10.4249/SCHOLARPEDIA.3147, 10.4249/scholarpedia.3147]
   Cabral B., 1993, Computer Graphics Proceedings, P263, DOI 10.1145/166117.166151
   Carmo BS, 2004, LECT NOTES COMPUT SC, V3216, P451
   Childs H., 2012, High Performance Visualization: Enabling Extreme-Scale Scientific Insight, P357, DOI [DOI 10.1201/B12985, 10.1201/b12985]
   Engelke W, 2019, COMPUT GRAPH FORUM, V38, P248, DOI 10.1111/cgf.13528
   Hall P., 1993, Visual Computer, V10, P69, DOI 10.1007/BF01901943
   Hu X., 2023, SPIE, P1230
   Krüger J, 2005, IEEE T VIS COMPUT GR, V11, P744, DOI 10.1109/TVCG.2005.87
   Laramee RS, 2008, ENG APPL COMP FLUID, V2, P264, DOI 10.1080/19942060.2008.11015227
   Li C, 2014, I C VIRTUAL REALITY, P43, DOI 10.1109/ICVRV.2014.42
   Liu F, 2022, FRONT EARTH SC-SWITZ, V9, DOI 10.3389/feart.2021.804617
   Longxing Kong, 2014, Applied Mechanics and Materials, V571-572, P676, DOI 10.4028/www.scientific.net/AMM.571-572.676
   Max N, 2005, VISUAL COMPUT, V21, P979, DOI 10.1007/s00371-005-0361-8
   Nouanesengsy B, 2011, IEEE T VIS COMPUT GR, V17, P1785, DOI 10.1109/TVCG.2011.219
   Peng Baoyun, 2016, Journal of Computer Aided Design & Computer Graphics, V28, P464
   Peng Z., 2011, EUROGRAPHICS POSTERS, P13
   Perry A. E., 1975, Advances in Geophysics, V18, P299
   Prager A., 2022, WSCG 2022 FOR, P28, DOI [10.24132/CSRN.3201.5, DOI 10.24132/CSRN.3201.5]
   Pugmire D, 2009, PROCEEDINGS OF THE CONFERENCE ON HIGH PERFORMANCE COMPUTING NETWORKING, STORAGE AND ANALYSIS
   Salzbrunn T., 2009, Flow Structure Based 3D Streamline Placement, P89, DOI [10.1007/978-3-540-88606-8_7, DOI 10.1007/978-3-540-88606-8_7]
   Sane S, 2020, COMPUT GRAPH FORUM, V39, P785, DOI 10.1111/cgf.14036
   Schroeder W., 2006, VISUALIZATION TOOLKI, DOI DOI 10.1016/B978-012387582-2/50003-4
   Skraba P, 2016, IEEE T VIS COMPUT GR, V22, P1683, DOI 10.1109/TVCG.2016.2534538
   Skraba P, 2015, IEEE T VIS COMPUT GR, V21, P930, DOI 10.1109/TVCG.2015.2440250
   Smolik M, 2017, PROCEDIA COMPUT SCI, V108, P2373, DOI 10.1016/j.procs.2017.05.271
   Stalling D., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P249, DOI 10.1145/218380.218448
   Stalling D., 1996, Tech. Rep. SC-96-01, V7, P14195
   Storti Duane., 2015, CUDA ENG INTRO HIGH
   Sundquist A, 2003, IEEE T VIS COMPUT GR, V9, P273, DOI 10.1109/TVCG.2003.1207436
   Tao J, 2013, IEEE T VIS COMPUT GR, V19, P393, DOI 10.1109/TVCG.2012.143
   Telea A., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P35, DOI 10.1109/VISUAL.1999.809865
   Verma V, 2000, IEEE VISUAL, P163, DOI 10.1109/VISUAL.2000.885690
   Wang CL, 2011, ENTROPY-SWITZ, V13, P254, DOI 10.3390/e13010254
   Wischgoll T., 2002, P S DAT VIS 2022, P227
   Xu HX, 2011, VISUAL COMPUT, V27, P441, DOI 10.1007/s00371-011-0577-8
   Xu LJ, 2010, IEEE T VIS COMPUT GR, V16, P1216, DOI 10.1109/TVCG.2010.131
   Yang Y., 2022, Metaverse, V3, P8, DOI [10.54517/met.v3i1.2103, DOI 10.54517/MET.V3I1.2103]
   Ye XH, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P471
   Zharfa M., 2019, Bulletin of the American Physical Society
NR 44
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 16
PY 2024
DI 10.1007/s00371-024-03357-8
EA APR 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW0H5
UT WOS:001203370000002
DA 2024-08-05
ER

PT J
AU Xu, YC
   Han, CW
   Lv, SQ
   Wang, Z
   Wang, M
AF Xu, Yingcheng
   Han, Congwei
   Lv, Shuqi
   Wang, Ze
   Wang, Miao
TI Internal and external transmission encoder-decoder network for
   single-image deraining
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Internal and external transmission; Encoder-decoder network; Image
   deraining
AB The visual quality of images captured in rainy weather is degraded by rain streaks, which may significantly reduce the accuracy of computer vision systems. In this paper, an internal and external transmission encoder-decoder network is proposed for eliminating rain streaks in a single image. Since rain streaks and background structures tend to be spatially long, we apply an internal connected aggregation unit to leverage global information and aggregate features at different scales to help remove rain streaks while restoring scene details. To fully exploit effective features of the encoder network and remove redundant information, we utilize an external connected enhancement unit to obtain effective feature maps for predicting clear outputs. By utilizing the efficient information inside and outside the unit, the proposed method can remove the rain streaks of different sizes and directions in the image and restore clear background structure details. Extensive experimental results on synthetic datasets and natural samples show that our method can achieve better rain removal performance than other advanced methods.
C1 [Xu, Yingcheng; Han, Congwei; Lv, Shuqi; Wang, Ze; Wang, Miao] Shandong Univ Finance & Econ, Sch Comp Sci & Technol, Jinan 250014, Peoples R China.
C3 Shandong University of Finance & Economics
RP Lv, SQ (corresponding author), Shandong Univ Finance & Econ, Sch Comp Sci & Technol, Jinan 250014, Peoples R China.
EM 202115012@mail.sdufe.edu.cn
FU National Natural Science Foundation of China [61972227, 61902217];
   National Natural Science Foundation of China [ZR2020KF015]; Shandong
   Provincial Natural Science Foundation Key Project
FX This work was supported in part by the National Natural Science
   Foundation of China (Nos. 61972227, 61902217) and in part by Shandong
   Provincial Natural Science Foundation Key Project (No. ZR2020KF015).
CR Chen WT, 2022, PROC CVPR IEEE, P17632, DOI 10.1109/CVPR52688.2022.01713
   Chen X, 2021, IEEE COMPUT SOC CONF, P872, DOI 10.1109/CVPRW53098.2021.00097
   Chen YL, 2013, IEEE I CONF COMP VIS, P1968, DOI 10.1109/ICCV.2013.247
   Fan ZW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1751, DOI 10.1145/3240508.3240694
   Fu X., 2021, P THE AAAI C ARTIFIC, P1
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802
   Guo Q.., 2020, arXiv
   Hu XW, 2021, IEEE T IMAGE PROCESS, V30, P1759, DOI 10.1109/TIP.2020.3048625
   Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057
   Kim JH, 2013, IEEE IMAGE PROC, P914, DOI 10.1109/ICIP.2013.6738189
   Kui Jiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8343, DOI 10.1109/CVPR42600.2020.00837
   Li G, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1056, DOI 10.1145/3240508.3240636
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Li Y, 2017, IEEE T IMAGE PROCESS, V26, P3874, DOI 10.1109/TIP.2017.2708841
   Lin CY, 2020, IEEE T IMAGE PROCESS, V29, P9250, DOI 10.1109/TIP.2020.3025402
   Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388
   Ren DW, 2020, IEEE T IMAGE PROCESS, V29, P6852, DOI 10.1109/TIP.2020.2994443
   Sen Deng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14548, DOI 10.1109/CVPR42600.2020.01457
   Wang C, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2517, DOI 10.1145/3394171.3413559
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Yang YZ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1814, DOI 10.1145/3343031.3351149
   Zamir S.W., 2021, P IEEE C COMPUTER VI, P1
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhu Z, 2019, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2019.00068
NR 28
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 20
PY 2024
DI 10.1007/s00371-024-03261-1
EA MAR 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LX3V2
UT WOS:001190081000004
DA 2024-08-05
ER

PT J
AU Yan, TY
   Yin, HJ
AF Yan, Tianyu
   Yin, Hujun
TI High-frequency channel attention and contrastive learning for image
   super-resolution
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image super-resolution; Attention mechanism; Contrastive learning; Deep
   learning
AB Over the last decade, convolutional neural networks (CNNs) have allowed remarkable advances in single image super-resolution (SISR). In general, recovering high-frequency features is crucial for high-performance models. High-frequency features suffer more serious damages than low-frequency features during downscaling, making it hard to recover edges and textures. In this paper, we attempt to guide the network to focus more on high-frequency features in restoration from both channel and spatial perspectives. Specifically, we propose a high-frequency channel attention (HFCA) module and a frequency contrastive learning (FCL) loss to aid the process. For the channel-wise perspective, the HFCA module rescales channels by predicting statistical similarity metrics of the feature maps and their high-frequency components. For the spatial perspective, the FCL loss introduces contrastive learning to train a spatial mask that adaptively assigns high-frequency areas with large scaling factors. We incorporate the proposed HFCA module and FCL loss into an EDSR baseline model to construct the proposed lightweight high-frequency channel contrastive network (HFCCN). Extensive experimental results show that it can yield markedly improved or competitive performances compared to the state-of-the-art networks of similar model parameters.
C1 [Yan, Tianyu; Yin, Hujun] Univ Manchester, Dept Elect & Elect Engn, Oxford Rd, Manchester M13 9PL, England.
C3 University of Manchester
RP Yin, HJ (corresponding author), Univ Manchester, Dept Elect & Elect Engn, Oxford Rd, Manchester M13 9PL, England.
EM hujun.yin@manchester.ac.uk
OI Yin, Hujun/0000-0002-9198-5401
CR Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Anwar S, 2022, IEEE T PATTERN ANAL, V44, P1192, DOI 10.1109/TPAMI.2020.3021088
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen WT, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3188908
   Chen YT, 2024, VISUAL COMPUT, V40, P489, DOI 10.1007/s00371-023-02795-0
   Choi JS, 2017, IEEE COMPUT SOC CONF, P1150, DOI 10.1109/CVPRW.2017.153
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu YT, 2020, IEEE T CIRC SYST VID, V30, P3911, DOI 10.1109/TCSVT.2019.2915238
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Huang YF, 2021, IEEE T IMAGE PROCESS, V30, P2325, DOI 10.1109/TIP.2021.3050856
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Khan A, 2021, VISUAL COMPUT, V37, P1661, DOI 10.1007/s00371-020-01930-5
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D. P., 2014, arXiv
   Kong FY, 2022, IEEE COMPUT SOC CONF, P765, DOI 10.1109/CVPRW56347.2022.00092
   Kong XT, 2021, PROC CVPR IEEE, P12011, DOI 10.1109/CVPR46437.2021.01184
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu AQ, 2023, VISUAL COMPUT, V39, P3837, DOI 10.1007/s00371-022-02519-w
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Liu TC, 2022, INT CONF ACOUST SPEE, P7517, DOI 10.1109/ICASSP43922.2022.9747021
   Magid SA, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4268, DOI [10.1109/ICCV48922.2021.00425, 10.1109/iccv48922.2021.00425]
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Paszke A, 2019, ADV NEUR IN, V32
   Peled S, 2001, MAGN RESON MED, V45, P29, DOI 10.1002/1522-2594(200101)45:1<29::AID-MRM1005>3.0.CO;2-Z
   Qin ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P763, DOI 10.1109/ICCV48922.2021.00082
   Rasti P, 2016, LECT NOTES COMPUT SC, V9756, P175, DOI 10.1007/978-3-319-41778-3_18
   Shi WL, 2021, VISUAL COMPUT, V37, P1569, DOI 10.1007/s00371-020-01903-8
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Thornton MW, 2006, INT J REMOTE SENS, V27, P473, DOI 10.1080/01431160500207088
   Tian CW, 2022, IEEE T SYST MAN CY-S, V52, P3718, DOI 10.1109/TSMC.2021.3069265
   Tian Y., 2020, Contrastive representation distillation
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Wang K, 2022, IEEE COMPUT SOC CONF, P341, DOI 10.1109/CVPRW56347.2022.00049
   Wang Y., 2021, arXiv
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xia B, 2022, Arxiv, DOI arXiv:2201.03794
   Zeyde R., 2012, INT C CURV SURF, P711
   Zhang JM, 2023, MULTIMED TOOLS APPL, DOI 10.1007/s11042-023-15429-3
   Zhang JM, 2022, J AMB INTEL SMART EN, V14, P317, DOI 10.3233/AIS-220038
   Zhang JM, 2022, HUM-CENT COMPUT INFO, V12, DOI 10.22967/HCIS.2022.12.023
   Zhang MJ, 2024, IEEE T NEUR NET LEAR, V35, P1810, DOI 10.1109/TNNLS.2022.3185529
   Zhang MJ, 2023, IEEE T NEUR NET LEAR, V34, P10538, DOI 10.1109/TNNLS.2022.3168540
   Zhang MJ, 2023, IEEE T CYBERNETICS, V53, P578, DOI 10.1109/TCYB.2022.3163294
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhou G., 2021, IEEE T GEOSCI ELECT, V60, P5614115, DOI [DOI 10.1109/TGRS.2021.3128033, https://doi.org/10.1109/TGRS.2021.3128033]
NR 59
TC 0
Z9 0
U1 10
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 29
PY 2024
DI 10.1007/s00371-024-03276-8
EA FEB 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JF9E8
UT WOS:001171862700004
OA hybrid
DA 2024-08-05
ER

PT J
AU Xu, B
   Jin, RS
   Li, JH
   Zhang, B
   Liu, K
AF Xu, Bin
   Jin, Rushi
   Li, Jinhua
   Zhang, Bo
   Liu, Kai
TI Robust and fast QR code images deblurring via local maximum and minimum
   intensity prior
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE QR code; Blind deblurring; Local maximum intensity; Local minimum
   intensity; Gradient sparsity prior
AB QR codes, as information carriers, have been extensively applied in numerous fields. However, relative movement between imaging targets and the camera can lead to severe motion blur, which hinders the recognition of the QR code. Achieving high-quality deblurring while maintaining efficiency is a challenging task in practice. In this paper, we propose a robust and fast deblurring method for QR code images using the local maximum and minimum intensity (LMMI) prior. First, in the process of image blur degradation, a general feature emerges: local maximum intensity tends to decrease, while local minimum intensity increases. This characteristic can be effectively harnessed in the deblurring process of QR code images. Second, the novel algorithm flexibly binarizes the LMMI within the maximum a posteriori (MAP) framework, as opposed to directly applying the semi-quadratic splitting algorithm, resulting in a significant enhancement in computational efficiency. Finally, latent image estimation and kernel estimation are alternately solved within the MAP framework until convergence. Experimental results reveal that the proposed method exhibits considerable improvements in both deblurring effect and computational efficiency.
C1 [Xu, Bin; Jin, Rushi; Li, Jinhua; Zhang, Bo] Sichuan Univ, Sch Mech Engn, No 24 South Sect 1,Yihuan Rd, Chengdu 610065, Sichuan, Peoples R China.
   [Liu, Kai] Sichuan Univ, Coll Elect Engn, No 24 South Sect 1,Yihuan Rd, Chengdu 610065, Sichuan, Peoples R China.
C3 Sichuan University; Sichuan University
RP Zhang, B (corresponding author), Sichuan Univ, Sch Mech Engn, No 24 South Sect 1,Yihuan Rd, Chengdu 610065, Sichuan, Peoples R China.
EM bin_xu@outlook.com; rushi_jin@163.com; jhli@scu.edu.cn;
   bo_zh@scu.edu.cn; kailiu@scu.edu.cn
OI Li, Jinhua/0000-0001-5732-8400
CR Chen L, 2019, PROC CVPR IEEE, P1742, DOI 10.1109/CVPR.2019.00184
   Chen RJ, 2021, MOBILE NETW APPL, V26, P2472, DOI 10.1007/s11036-021-01780-y
   Chen Shanshan, 2022, Nanomanufacturing and Metrology, P423, DOI 10.1007/s41871-022-00130-0
   Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491
   Eqtedaei A, 2024, VISUAL COMPUT, V40, P333, DOI 10.1007/s00371-023-02785-2
   Feng Q, 2019, VISUAL COMPUT, V35, P1081, DOI 10.1007/s00371-019-01697-4
   Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956
   github, About Us
   hlcode, About us
   Hu DD, 2023, VISUAL COMPUT, V39, P281, DOI 10.1007/s00371-021-02329-6
   Jiang XL, 2017, NEUROCOMPUTING, V242, P1, DOI 10.1016/j.neucom.2017.01.080
   Khan A, 2021, VISUAL COMPUT, V37, P1661, DOI 10.1007/s00371-020-01930-5
   Köhler R, 2012, LECT NOTES COMPUT SC, V7578, P27, DOI 10.1007/978-3-642-33786-4_3
   Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521
   Levin A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2657, DOI 10.1109/CVPR.2011.5995308
   Levin A, 2009, PROC CVPR IEEE, P1964, DOI 10.1109/CVPRW.2009.5206815
   Li JN, 2022, NEUROCOMPUTING, V493, P351, DOI 10.1016/j.neucom.2022.04.041
   Liu C., 2023, Visual Comput., P1
   Liu J, 2023, VISUAL COMPUT, V39, P3091, DOI 10.1007/s00371-022-02515-0
   Liu J, 2022, CIRC SYST SIGNAL PR, V41, P1074, DOI 10.1007/s00034-021-01827-1
   Liu NZ, 2018, PATTERN RECOGN LETT, V111, P117, DOI 10.1016/j.patrec.2018.04.036
   Matsukuma Hiraku, 2021, Nanomanufacturing and Metrology, V4, P53, DOI 10.1007/s41871-020-00091-2
   Michaeli T, 2014, LECT NOTES COMPUT SC, V8691, P783, DOI 10.1007/978-3-319-10578-9_51
   Pan JS, 2017, IEEE T PATTERN ANAL, V39, P342, DOI 10.1109/TPAMI.2016.2551244
   Pan JS, 2016, PROC CVPR IEEE, P1628, DOI 10.1109/CVPR.2016.180
   Pan JS, 2014, PROC CVPR IEEE, P2901, DOI 10.1109/CVPR.2014.371
   Rao J., 2020, Int. J. Metrol. Q. Eng., V11
   Ren WQ, 2022, IEEE T PATTERN ANAL, V44, P3974, DOI 10.1109/TPAMI.2021.3061604
   Ren WQ, 2016, IEEE T IMAGE PROCESS, V25, P3426, DOI 10.1109/TIP.2016.2571062
   Shi YF, 2020, OPTIK, V219, DOI 10.1016/j.ijleo.2020.164902
   Sörös G, 2015, ISWC 2015: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, P117, DOI 10.1145/2802083.2808390
   Song Bowen, 2023, Nanomanufacturing and Metrology, DOI 10.1007/s41871-023-00201-w
   Tao Sun, 2011, Proceedings of the 2011 IEEE International Conference on Automation and Logistics (ICAL), P164, DOI 10.1109/ICAL.2011.6024704
   Wang H., 2023, Vis. Comput., P1
   Wen F, 2021, IEEE T CIRC SYST VID, V31, P2923, DOI 10.1109/TCSVT.2020.3034137
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Yan YY, 2017, PROC CVPR IEEE, P6978, DOI 10.1109/CVPR.2017.738
   Yang H, 2019, VISUAL COMPUT, V35, P1627, DOI 10.1007/s00371-018-1562-2
   Yu XY, 2019, IET IMAGE PROCESS, V13, P923, DOI 10.1049/iet-ipr.2018.5792
   Zhang ZC, 2023, VISUAL COMPUT, V39, P1375, DOI 10.1007/s00371-022-02415-3
   Zheng H., 2023, Vis. Comput., P1
   Zhong L, 2013, PROC CVPR IEEE, P612, DOI 10.1109/CVPR.2013.85
   Zhou Y., 2014, A Map-estimation Framework for Blind Deblurring Using High-level Edge Priors
   Zhu W., 2012, New Technology of Library and Information Service
   Zhu Y., 2023, Vis. Comput., P1
NR 46
TC 0
Z9 0
U1 6
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 27
PY 2024
DI 10.1007/s00371-024-03272-y
EA FEB 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JF2H6
UT WOS:001171680100002
DA 2024-08-05
ER

PT J
AU Pan, H
   Gao, BK
   Wang, XF
   Jiang, CL
   Chen, P
AF Pan, Heng
   Gao, Bingkun
   Wang, Xiufang
   Jiang, Chunlei
   Chen, Peng
TI DICNet: achieve low-light image enhancement with image decomposition,
   illumination enhancement, and color restoration
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Retinex; Low-light enhancement; Feature separation processing; Feature
   cross fusion
ID HISTOGRAM EQUALIZATION; CONTRAST; RETINEX; NETWORK
AB Low-light image enhancement (LLIE) is mainly used to restore image degradation caused by environmental noise, lighting effects, and other factors. Despite many relevant works combating environmental interference, LLIE currently still faces multiple limitations, such as noise, unnatural color recovery, and severe loss of details, etc. To effectively overcome these limitations, we propose a DICNet based on the Retinex theory. DICNet consists of three components: image decomposition, illumination enhancement, and color restoration. To avoid the influence of noise during the enhancement process, we use feature maps after the image high-frequency component denoising process to guide image decomposition and suppress noise interference. For illumination enhancement, we propose a feature separation method that considering the influence of different lighting intensities and preserves details. In addition, to address the insufficient high-low-level feature fusion of the U-Net used in color restoration, we design a Feature Cross-Fusion Module and propose a feature fusion connection plug-in to ensure natural and realistic color restoration. Based on a large number of experiments on publicly available datasets, our method outperforms existing state-of-the-art methods in both performance and visual quality.
C1 [Pan, Heng; Gao, Bingkun; Wang, Xiufang; Jiang, Chunlei; Chen, Peng] Northeast Petr Univ, Sch Elect Engn Informat Dept, Daqing 163318, Peoples R China.
C3 Northeast Petroleum University
RP Gao, BK (corresponding author), Northeast Petr Univ, Sch Elect Engn Informat Dept, Daqing 163318, Peoples R China.
EM gaobk@163.com
FU the Natural Science Foundation of Heilongjiang Province [LH2022E024];
   Natural Science Foundation of Heilongjiang Province
FX This work was supported in part by the Natural Science Foundation of
   Heilongjiang Province under Grant LH2022E024.
CR Baldassarre F, 2017, Arxiv, DOI arXiv:1712.03400
   BEGHDADI A, 1989, COMPUT VISION GRAPH, V46, P162, DOI 10.1016/0734-189X(89)90166-7
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Chen SD, 2003, IEEE T CONSUM ELECTR, V49, P1310, DOI 10.1109/TCE.2003.1261234
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dudhane A, 2022, PROC CVPR IEEE, P5749, DOI 10.1109/CVPR52688.2022.00567
   Farid H, 2001, IEEE T IMAGE PROCESS, V10, P1428, DOI 10.1109/83.951529
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Fu ZQ, 2023, PROC CVPR IEEE, P22252, DOI 10.1109/CVPR52729.2023.02131
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo S, 2023, VISUAL COMPUT, V39, P1363, DOI 10.1007/s00371-022-02412-6
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hai J, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103712
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Nguyen H, 2023, IEEE WINT CONF APPL, P1756, DOI 10.1109/WACV56688.2023.00180
   Ibrahim H, 2007, IEEE T CONSUM ELECTR, V53, P1752, DOI 10.1109/TCE.2007.4429280
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Jin YY, 2022, LECT NOTES COMPUT SC, V13697, P404, DOI 10.1007/978-3-031-19836-6_23
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4439, DOI 10.1109/ICCV48922.2021.00442
   Kingma D. P., 2014, arXiv
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P9396, DOI 10.1109/TPAMI.2021.3126387
   Liu RS, 2021, PROC CVPR IEEE, P10556, DOI 10.1109/CVPR46437.2021.01042
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Ma L, 2022, PROC CVPR IEEE, P5627, DOI 10.1109/CVPR52688.2022.00555
   Maas A.L., 2013, P ICML CIT, V30, P3
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   PELI E, 1990, J OPT SOC AM A, V7, P2032, DOI 10.1364/JOSAA.7.002032
   Pisano ED, 1998, J DIGIT IMAGING, V11, P193, DOI 10.1007/BF03178082
   Rahman Z, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P1003, DOI 10.1109/ICIP.1996.560995
   Reza AM, 2004, J VLSI SIG PROC SYST, V38, P35, DOI 10.1023/B:VLSI.0000028532.53893.82
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shen L, 2017, Arxiv, DOI arXiv:1711.02488
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang Chenxi, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P7459, DOI 10.1145/3581783.3611909
   Wang C, 2005, IEEE T CONSUM ELECTR, V51, P1326, DOI 10.1109/TCE.2005.1561863
   Wang CX, 2021, VISUAL COMPUT, V37, P77, DOI 10.1007/s00371-020-01888-4
   Wang HY, 2022, LECT NOTES COMPUT SC, V13678, P343, DOI 10.1007/978-3-031-19797-0_20
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei C, 2018, Arxiv, DOI arXiv:1808.04560
   Wen JY, 2023, Arxiv, DOI arXiv:2308.08197
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu WH, 2022, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR52688.2022.00581
   Yang WH, 2021, IEEE T IMAGE PROCESS, V30, P2072, DOI 10.1109/TIP.2021.3050850
   Yu XY, 2023, VISUAL COMPUT, V39, P4165, DOI 10.1007/s00371-022-02582-3
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhang Z, 2022, PROC CVPR IEEE, P1889, DOI 10.1109/CVPR52688.2022.00194
   Zhou F., 2023, Pattern Recogn
NR 58
TC 1
Z9 1
U1 6
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 22
PY 2024
DI 10.1007/s00371-024-03262-0
EA FEB 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IW7L2
UT WOS:001169439000001
DA 2024-08-05
ER

PT J
AU Yan, YH
   Zhou, MQ
   Zhang, D
   Geng, SL
AF Yan, Yuhuan
   Zhou, Mingquan
   Zhang, Dan
   Geng, Shengling
TI Improved biharmonic kernel signature for 3D non-rigid shape matching and
   retrieval
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Laplace-Beltrami operator; Improved biharmonic kernel signature; Shape
   matching; Shape retrieval
ID GLOBAL POINT SIGNATURE; SPECTRAL DESCRIPTORS; GEOMETRIC FEATURES;
   LAPLACIAN
AB Object retrieval, in particular 3D shape retrieval, recently has many applications such as molecular biology, medical research and computer-aided manufacturing. As the internet and 3D modeling tools have led to an increasingly growth in the number of available 3D models, it becomes necessary to have a proper and efficient representation method to capture the most important information about shapes, which for searching target object more efficiently and accurately. In this paper, we propose a novel 3D shape descriptor, improved biharmonic kernel signature (IBKS). Firstly, based on the Laplace-Beltrami operator, we perform a linear combination of some scaled eigenvalues and eigenfunctions of the biharmonic kernel signature (BKS) to construct invariant metrics about the shape. Secondly, we note that the Gaussian curvature of the vertices remains stable as the shape undergoes pose changes. This is a local property of surface that is isometric invariant and stable at most points under object articulation but is not well coded in the BKS descriptor. Therefore, we use curvature aggregation method to reduce the influence of noise, which enhances the feature separation and makes the descriptor more discriminative. The IBKS descriptor built upon the concept of biharmonic kernel signature, the descriptor inherits several useful properties such as stability of the shape joint region, robustness to isometric, scaling, noise, sampling and topology transformations. Due to the stability and discriminative power of IBKS, we can simply and effectively characterize, identify and analyze non-rigid 3D shapes. We show how our framework can be applied for shape representation, matching and retrieval. To assert our method more stable to non-rigid deformations, we compare our framework with advanced non-rigid signatures on traditional benchmark McGill and SHREC2015 datasets. Experimental results show that our spectrum shape descriptor is more stable and discriminative and is significantly better than other descriptors.
C1 [Yan, Yuhuan; Zhou, Mingquan; Zhang, Dan; Geng, Shengling] State Key Lab Tibetan Intelligent Informat Proc &, Xining 810000, Peoples R China.
   [Zhou, Mingquan; Zhang, Dan; Geng, Shengling] Peoples Govt Qinghai Prov, Acad Plateau Sci & Sustainabil, Xining 810000, Peoples R China.
   [Zhou, Mingquan; Zhang, Dan; Geng, Shengling] Beijing Normal Univ, Xining 810000, Peoples R China.
   [Yan, Yuhuan; Zhou, Mingquan; Zhang, Dan; Geng, Shengling] Qinghai Normal Univ, Sch Comp Sci, Xining 810000, Peoples R China.
C3 Beijing Normal University; Qinghai Normal University
RP Zhang, D (corresponding author), State Key Lab Tibetan Intelligent Informat Proc &, Xining 810000, Peoples R China.; Zhang, D (corresponding author), Peoples Govt Qinghai Prov, Acad Plateau Sci & Sustainabil, Xining 810000, Peoples R China.; Zhang, D (corresponding author), Beijing Normal Univ, Xining 810000, Peoples R China.; Zhang, D (corresponding author), Qinghai Normal Univ, Sch Comp Sci, Xining 810000, Peoples R China.
EM 2314305749@qq.com; mqzhou@bnu.edu.cn; danz@mail.bnu.edu.cn;
   370898534@qq.com
RI Zhang, Dan/AFA-2608-2022
OI Zhang, Dan/0000-0002-7295-4837; Dan, Zhang/0000-0001-5676-0656
FU National Key R D plan; National Nature Science Foundation of China
   [62102213, 62262056]; Natural Science Youth Foundation of Qinghai
   Province [2023-ZJ-947Q]; Major R &D and Transformation Projects in
   Qinghai Province [2021-GX-111];  [2020YFC1523305]
FX This work was partially supported by the National Nature Science
   Foundation of China (No. 62102213, 62262056); National Key R &D plan
   (No. 2020YFC1523305); Natural Science Youth Foundation of Qinghai
   Province (No. 2023-ZJ-947Q); Major R &D and Transformation Projects in
   Qinghai Province (2021-GX-111).
CR [Anonymous], 2006, CVPR
   [Anonymous], 2016, SIGGRAPH ASIA COURSE
   Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   BERARD P, 1994, GEOM FUNCT ANAL, V4, P373, DOI 10.1007/BF01896401
   Biasotti S, 2016, COMPUT GRAPH FORUM, V35, P87, DOI 10.1111/cgf.12734
   Boscaini D, 2016, COMPUT GRAPH FORUM, V35, P431, DOI 10.1111/cgf.12844
   Boscaini D, 2015, COMPUT GRAPH FORUM, V34, P13, DOI 10.1111/cgf.12693
   Boscaini D, 2016, ADV NEUR IN, V29
   Bronstein A., P EUROGRAPHICS WORKS
   Bronstein A., 2010, Comput. Rev, V51, P222
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   Bronstein MM, 2010, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2010.5539838
   Carcassoni M, 2003, PATTERN RECOGN, V36, P193, DOI 10.1016/S0031-3203(02)00054-7
   Chaudhari AJ, 2014, PHYS MED BIOL, V59, P961, DOI 10.1088/0031-9155/59/4/961
   Choukroun Y, 2020, IEEE T VIS COMPUT GR, V26, P1320, DOI 10.1109/TVCG.2018.2867513
   Choulli M, 2017, SEMIGROUP FORUM, V94, P71, DOI 10.1007/s00233-015-9757-6
   Coifman RR, 2005, P NATL ACAD SCI USA, V102, P7426, DOI 10.1073/pnas.0500334102
   Cosmo L, 2022, INT J COMPUT VISION, V130, P1474, DOI 10.1007/s11263-022-01610-y
   Du GG, 2016, I C VIRTUAL REALITY, P144, DOI 10.1109/ICVRV.2016.31
   Fang Y, 2015, PROC CVPR IEEE, P2319, DOI 10.1109/CVPR.2015.7298845
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Han ZZ, 2016, IEEE T IMAGE PROCESS, V25, P5331, DOI 10.1109/TIP.2016.2605920
   Havens TC, 2008, INT C PATT RECOG, P2304, DOI 10.1109/ICPR.2008.4761772
   Hu JX, 2009, VISUAL COMPUT, V25, P667, DOI 10.1007/s00371-009-0340-6
   Jain V, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P118
   Kokkinos I, 2012, PROC CVPR IEEE, P159, DOI 10.1109/CVPR.2012.6247671
   Lavoué G, 2012, VISUAL COMPUT, V28, P931, DOI 10.1007/s00371-012-0724-x
   Li CY, 2013, INT J MULTIMED INF R, V2, P261, DOI 10.1007/s13735-013-0041-9
   Li CY, 2014, MULTIMEDIA SYST, V20, P253, DOI 10.1007/s00530-013-0318-0
   Li CY, 2013, VISUAL COMPUT, V29, P513, DOI 10.1007/s00371-013-0815-3
   Li HS, 2018, INT CONF BIG DATA, P448, DOI 10.1109/BigComp.2018.00072
   Lian Z, 2015, P EUROGRAPHICS WORKS, P257
   Lian ZH, 2013, MACH VISION APPL, V24, P1685, DOI 10.1007/s00138-013-0501-5
   Limberger F., 2015, P BRIT MACHINE VISIO
   Litman R, 2014, IEEE T PATTERN ANAL, V36, P171, DOI 10.1109/TPAMI.2013.148
   Magnet R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3773, DOI 10.1109/ICCV48922.2021.00377
   Masci J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P832, DOI 10.1109/ICCVW.2015.112
   Melzi S, 2018, COMPUT GRAPH FORUM, V37, P20, DOI 10.1111/cgf.13309
   Memon S., 2019, P 2 INT C COMPUTING, P1
   Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576
   Naffouti SE, 2017, SIGNAL PROCESS-IMAGE, V58, P228, DOI 10.1016/j.image.2017.07.005
   Ohbuchi R, 2008, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2008, PROCEEDINGS, P93, DOI 10.1109/SMI.2008.4547955
   Ovsjanikov Maks, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P320, DOI 10.1109/ICCVW.2009.5457682
   Ovsjanikov M, 2008, COMPUT GRAPH FORUM, V27, P1341, DOI 10.1111/j.1467-8659.2008.01273.x
   Patané G, 2016, COMPUT GRAPH FORUM, V35, P599, DOI 10.1111/cgf.12866
   Philbin J., 2007, P IEEE COMPUTER VISI, P1
   Pickup D, 2016, INT J COMPUT VISION, V120, P169, DOI 10.1007/s11263-016-0903-8
   Pickup D., 2015, P 8 EUROGRAPHICS C 3, P1
   Raviv D., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2361, DOI 10.1109/CVPR.2011.5995486
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Reuter M, 2010, INT J COMPUT VISION, V89, P287, DOI 10.1007/s11263-009-0278-1
   Robert C., 2014, Chance, V27, P62, DOI [10.1080/09332480.2014.914768, DOI 10.1080/09332480.2014.914768]
   Rostami R, 2019, COMPUT GRAPH FORUM, V38, P356, DOI 10.1111/cgf.13536
   Rustamov R.M., 2007, P 5 EUR S GEOM PROC, V257, P225, DOI DOI 10.2312/SGP/SGP07/225-233
   Rustamov RM, 2011, COMPUT GRAPH FORUM, V30, P1521, DOI 10.1111/j.1467-8659.2011.02026.x
   Sahillioglu Y, 2011, COMPUT GRAPH FORUM, V30, P1461, DOI 10.1111/j.1467-8659.2011.02020.x
   Sahillioglu Y, 2020, VISUAL COMPUT, V36, P1705, DOI 10.1007/s00371-019-01760-0
   Siddiqi K, 2008, MACH VISION APPL, V19, P261, DOI 10.1007/s00138-007-0097-8
   Smeets D, 2012, PATTERN RECOGN, V45, P2817, DOI 10.1016/j.patcog.2012.01.020
   STRICHARTZ RS, 1983, J FUNCT ANAL, V52, P48, DOI 10.1016/0022-1236(83)90090-3
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   van Gemert JC, 2010, IEEE T PATTERN ANAL, V32, P1271, DOI 10.1109/TPAMI.2009.132
   Wan L, 2015, Pacific Graphics, P25
   Wang JC, 2012, ACTA MATH SCI, V32, P1213
   Wu YQ, 2018, IEEE T SERV COMPUT, V11, P341, DOI 10.1109/TSC.2015.2501981
   Yan Y., 2024, Tools Appl., V83, P8077
   Yan YH, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-29047-4
   Yang JY, 2016, NEUROCOMPUTING, V190, P70, DOI 10.1016/j.neucom.2016.01.032
   Yang YT, 2021, ENGINEERING-PRC, V7, P787, DOI 10.1016/j.eng.2021.03.011
   Ye JB, 2016, VISUAL COMPUT, V32, P553, DOI 10.1007/s00371-015-1071-5
   Zhang D, 2021, LECT NOTES COMPUT SC, V12890, P44, DOI 10.1007/978-3-030-87361-5_4
   Zhang D, 2021, MULTIMED TOOLS APPL, V80, P615, DOI 10.1007/s11042-020-09420-5
   Zhang D, 2021, VISUAL COMPUT, V37, P749, DOI 10.1007/s00371-020-01946-x
   Zhang H., 2005, PROC C VISION MODELI, P429
NR 74
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 12
PY 2024
DI 10.1007/s00371-023-03254-6
EA FEB 2024
PG 27
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HK8R5
UT WOS:001159495100001
DA 2024-08-05
ER

PT J
AU Chen, Z
   Huang, SH
   Lv, H
   Luo, ZX
   Liu, JH
AF Chen, Zhe
   Huang, Shihao
   Lv, Hui
   Luo, Zhixue
   Liu, Jinhao
TI Defect detection in automotive glass based on modified YOLOv5 with
   multi-scale feature fusion and dual lightweight strategy
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Automotive glass; YOLOv5; Lightweighting; Pseudo-labeling; M-SFF
ID CLASSIFICATION; SYSTEM
AB Automotive glass is one of the key components in manufacturing engineering, and the inspection of defects is essentially an important item of quality evaluation. Deep learning has become a promising technology and is very suitable for defect detection. However, a unified method to detect all kinds of defects is very difficult due to the few publicly available image datasets of automotive glass, and it is a challenge to deploy high-precision models in resource-constrained edge devices. Focusing on these problems, defect detection in automotive glass based on modified YOLOv5 with pseudo-tagging and dual lightweight strategy is developed in this paper. First, aiming at the problem that the defect samples are extremely lacking, combined pseudo-labeling and traditional data expansion methods have been explored to effectively increase the number of samples for meeting the requirement of deep learning model training, thus improving the performance of the detection model. Second, double lightweight modules, MobileNetV3, and Ghost module are introduced into the backbone and the neck network of the YOLOv5 model, respectively, for reducing the complexity of the model. In addition, a multi-scale feature fusion (M-SFF) module, which riches the semantic and spatial information of feature maps, is added to the output of the backbone to further improve the detection accuracy of the model. The effectiveness of each innovation module was verified through ablation experiment and horizontal comparative experimentally. Experimental results show that the complexity and deployment difficulty of the improved YOLOv5 model are significantly reduced, the floating-point operations per second (FLOPs) values and weight sizes are significantly smaller than those of other models, and the detection performance is also improved based on the original YOLOv5, an average accuracy increase of 3.9% on average, precision slightly decreased, and recall rates increased by 7.6%. The method is appropriate for applications in the defect inspection of automotive glass.
C1 [Chen, Zhe; Huang, Shihao; Lv, Hui; Luo, Zhixue; Liu, Jinhao] Fujian Key Lab Automot Elect & Elect Drive, Fuzhou 350118, Peoples R China.
   [Chen, Zhe; Huang, Shihao; Lv, Hui; Luo, Zhixue; Liu, Jinhao] Fujian Univ Technol, Sch Elect Elect Engn & Phys, Fuzhou 350118, Peoples R China.
C3 Fujian University of Technology
RP Huang, SH (corresponding author), Fujian Key Lab Automot Elect & Elect Drive, Fuzhou 350118, Peoples R China.; Huang, SH (corresponding author), Fujian Univ Technol, Sch Elect Elect Engn & Phys, Fuzhou 350118, Peoples R China.
EM 951411819@qq.com; haoshihuang@fjut.edu.cn; 1635518661@qq.com;
   1029519663@qq.com; 2221905083@smail.fjut.edu.cn
CR Ali S, 2021, 29TH IEEE CONFERENCE ON SIGNAL PROCESSING AND COMMUNICATIONS APPLICATIONS (SIU 2021), DOI 10.1109/SIU53274.2021.9478027
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Bouguezzi S, 2022, VISUAL COMPUT, V38, P3747, DOI 10.1007/s00371-021-02211-5
   Chen GC, 2022, VISUAL COMPUT, V38, P1051, DOI 10.1007/s00371-021-02067-9
   Chen ZY, 2022, AGRONOMY-BASEL, V12, DOI 10.3390/agronomy12020365
   Cui YM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8118, DOI 10.1109/ICCV48922.2021.00803
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Fu C, 2017, arXiv
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Guo KY, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-07527-3
   Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165
   Le HA, 2023, INT GEOSCI REMOTE SE, P6194, DOI 10.1109/IGARSS52108.2023.10282614
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang ZY, 2021, MATH BIOSCI ENG, V18, P3491, DOI 10.3934/mbe.2021175
   Jian CX, 2017, APPL SOFT COMPUT, V52, P348, DOI 10.1016/j.asoc.2016.10.030
   Jiang JB, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10103621
   Jie Zhao, 2011, Proceedings of the Sixth International Conference on Image and Graphics (ICIG 2011), P642, DOI 10.1109/ICIG.2011.187
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Kang Z, 2020, IEEE ACCESS, V8, P140019, DOI 10.1109/ACCESS.2020.3010496
   Lang XL, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22249897
   Li ZX, 2024, Arxiv, DOI arXiv:1712.00960
   Liu CY, 2021, ENERGIES, V14, DOI 10.3390/en14051426
   Liu DF, 2021, PROC CVPR IEEE, P9811, DOI 10.1109/CVPR46437.2021.00969
   Liu DF, 2021, AAAI CONF ARTIF INTE, V35, P6101
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Ojala T, 2000, LECT NOTES COMPUT SC, V1842, P404
   Redmon J., 2018, CoRR
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2016, Arxiv, DOI [arXiv:1506.01497, DOI 10.1109/TPAMI.2016.2577031]
   Sari F., 2022, Deep learning application in detecting glass defects with color space conversion and adaptive histogram equalization, DOI [10.18280/ts.390238, DOI 10.18280/TS.390238]
   Singh T., 2013, Int. J. Comput. Appl, DOI [10.5120/11650-7152, DOI 10.5120/11650-7152]
   Wan F, 2022, EURASIP J ADV SIG PR, V2022, DOI 10.1186/s13634-022-00931-x
   Wang L, 2023, PROC CVPR IEEE, P5620, DOI 10.1109/CVPR52729.2023.00544
   Wang L, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4324, DOI 10.1145/3474085.3475572
   Wang L, 2019, IEEE I CONF COMP VIS, P8697, DOI 10.1109/ICCV.2019.00879
   Wang L, 2019, IEEE IMAGE PROC, P974, DOI [10.1109/ICIP.2019.8803051, 10.1109/icip.2019.8803051]
   Wang W., 2022, arXiv, DOI [10.22215/etd/2020-14214, DOI 10.22215/ETD/2020-14214]
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xie C, 2022, IET IMAGE PROCESS, V16, P273, DOI 10.1049/ipr2.12364
   Yang J., 2021, Focal self-attention for local-global interactions in vision transformers
   Yousefian-Jazi A, 2014, J PROCESS CONTR, V24, P1015, DOI 10.1016/j.jprocont.2013.12.009
   Yuan ZC, 2018, INT J PRECIS ENG MAN, V19, P801, DOI 10.1007/s12541-018-0096-x
   Zhang DH, 2023, ARTIF INTELL REV, V56, P10651, DOI 10.1007/s10462-023-10438-y
   Zhang XY, 2024, VISUAL COMPUT, V40, P2713, DOI 10.1007/s00371-023-02980-1
   Zhu L, 2023, PROC CVPR IEEE, P10323, DOI 10.1109/CVPR52729.2023.00995
NR 47
TC 1
Z9 1
U1 11
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 17
PY 2024
DI 10.1007/s00371-023-03225-x
EA JAN 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FA4N3
UT WOS:001143010300001
DA 2024-08-05
ER

PT J
AU Winchenbach, R
   Moeller, M
   Kolb, A
AF Winchenbach, Rene
   Moeller, Michael
   Kolb, Andreas
TI Lipschitz-agnostic, efficient and accurate rendering of implicit
   surfaces
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Rendering; Implicit surfaces; Ray casting; Root finding; Chebyshev
   approximation
ID CHEBYSHEV SERIES FORM; COMPUTING REAL ROOTS; INTERVAL; ZEROS
AB In this paper, we propose an accurate and controllable rendering process for implicit surfaces with no or unknown analytic Lipschitz constants. Our process is built upon a ray-casting approach where we construct an adaptive Chebyshev proxy along each ray to perform an accurate intersection test via a robust and multi-stage searching method. By taking into account approximation errors and numerical conditions, our methods comprise several pre-conditioning and post-processing stages to improve the numerical accuracy, which potentially applied recursively. The intersection search is performed by evaluating a QR decomposition on the Chebyshev proxy function, which can be done in a numerically accurate way. Our process achieves comparable accuracy to other techniques that impose more constraints on the surface, e.g., knowledge of Lipschitz constants, and higher accuracy compared to approaches that impose similar constraints as our approach.
C1 [Winchenbach, Rene] Tech Univ Munich, Munich, Germany.
   [Moeller, Michael; Kolb, Andreas] Univ Siegen, Siegen, Germany.
C3 Technical University of Munich; Universitat Siegen
RP Winchenbach, R (corresponding author), Tech Univ Munich, Munich, Germany.
EM rene.winchenbach@gmail.com
RI Kolb, Andreas/A-2067-2012
OI Winchenbach, Rene/0000-0003-2446-9412
FU Technische Universitt Mnchen (1025)
FX No Statement Available
CR Aurentz JL, 2017, ACM T MATH SOFTWARE, V43, DOI 10.1145/2998442
   Aydinlilar M, 2021, COMPUT GRAPH FORUM, V40, P117, DOI 10.1111/cgf.14208
   Battles Z, 2004, SIAM J SCI COMPUT, V25, P1743, DOI 10.1137/S1064827503430126
   Bernstein S., 1918, Math. Ann, V79, P1, DOI [10.1007/BF01457173, DOI 10.1007/BF01457173]
   Blinn J. F., 1982, Computer Graphics, V16, DOI 10.1145/965145.801290
   Bloomenthal J., 1997, INTRO IMPLICIT SURFA
   Boyd J. P., 2000, Chebyshev and Fourier Spectral Methods, V2nd
   Boyd JP, 2007, J COMPUT APPL MATH, V205, P281, DOI 10.1016/j.cam.2006.05.006
   Boyd JP, 2014, OTHER TITL APPL MATH, P1, DOI 10.1137/1.9781611973525
   Boyd JP, 2006, APPL NUMER MATH, V56, P1077, DOI 10.1016/j.apnum.2005.09.007
   Boyd JP, 2006, APPL MATH COMPUT, V174, P1642, DOI 10.1016/j.amc.2005.07.009
   BOYD JP, 1995, J COMPUT PHYS, V118, P1, DOI 10.1006/jcph.1995.1075
   Boyd JP, 2002, SIAM J NUMER ANAL, V40, P1666, DOI 10.1137/S0036142901398325
   Caprani O., 2000, Reliable Computing, V6, P9, DOI 10.1023/A:1009921806032
   Collins G. E., 1982, Computing (Supplementum), P83
   Driscoll T.A., 2014, Chebfun Guide
   Fan J, 2007, DES AUT TEST EUROPE, P1508, DOI 10.1007/BF01386223
   FRASER W, 1966, SIAM REV, V8, P322, DOI 10.1137/1008064
   Fryazinov O, 2010, COMPUT GRAPH-UK, V34, P708, DOI 10.1016/j.cag.2010.07.003
   GalaadTeam U. o. N. S.A., Implicit algebraic surfaces
   Galin E, 2020, COMPUT GRAPH FORUM, V39, P545, DOI 10.1111/cgf.13951
   Génevaux JD, 2015, COMPUT GRAPH FORUM, V34, P198, DOI 10.1111/cgf.12530
   Gomes A.J. P., 2009, Implicit Curves and Surfaces: Mathematics, Data Structures and Algorithms, P145, DOI DOI 10.1007/978-1-84882-406-5
   Gourmel O, 2010, COMPUT GRAPH FORUM, V29, P281, DOI 10.1111/j.1467-8659.2009.01597.x
   Hanrahan P., 1983, Computer Graphics, V17, P83, DOI 10.1145/964967.801136
   Hart JC, 1996, VISUAL COMPUT, V12, P527, DOI 10.1007/s003710050084
   JACOBS SJ, 1990, J COMPUT PHYS, V88, P169, DOI 10.1016/0021-9991(90)90246-W
   Jin XG, 2009, VISUAL COMPUT, V25, P279, DOI 10.1007/s00371-008-0267-3
   Kalra D., 1989, Computer Graphics, V23, P297, DOI 10.1145/74334.74364
   Kanamori Y, 2008, COMPUT GRAPH FORUM, V27, P351, DOI 10.1111/j.1467-8659.2008.01132.x
   Keeter MJ, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392429
   Keinert B., 2014, STAG Smart Tools Graph, V8, P4
   Knoll A, 2009, COMPUT GRAPH FORUM, V28, P26, DOI 10.1111/j.1467-8659.2008.01189.x
   Knoll A, 2007, RT07: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2007, P11, DOI 10.1109/RT.2007.4342585
   Mason J.C., 2002, Chebyshev polynomials, DOI [DOI 10.1201/9781420036114, 10.1201/9781420036114]
   MAVRIPLIS C, 1994, COMPUT METHOD APPL M, V116, P77, DOI 10.1016/S0045-7825(94)80010-3
   MITCHELL DP, 1990, GRAPH INTER, P68
   NISHITA T, 1994, COMPUT GRAPH FORUM, V13, pC271, DOI 10.1111/1467-8659.1330271
   OHARA H, 1968, COMPUT J, V11, P213, DOI 10.1093/comjnl/11.2.213
   Rawat Prashant Singh, 2018, SC18: International Conference for High Performance Computing, Networking, Storage and Analysis. Proceedings, P590, DOI 10.1109/SC.2018.00049
   Runge C., 1901, Z. Math. Phys., V46, P224
   Seyb D, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356502
   Sherstyuk A, 1999, COMPUT GRAPH FORUM, V18, P139, DOI 10.1111/1467-8659.00364
   Singh JM, 2018, SA'18: SIGGRAPH ASIA 2018 TECHNICAL BRIEFS, DOI 10.1145/3283254.3283287
   Singh JM, 2010, IEEE T VIS COMPUT GR, V16, P261, DOI 10.1109/TVCG.2009.41
   Sloan I., 1980, SIAM Rev, V8, P322
   Trefethen LN., 1997, NUMERICAL LINEAR ALG, DOI DOI 10.1137/1.9780898719574
   VANWIJK JJ, 1985, COMPUT GRAPH, V9, P283, DOI 10.1016/0097-8493(85)90055-X
   Weierstrass K., 1885, Sitzungsberichte der Koniglich Preussischen Akad. der Wiss. zu Berlin, V2, P633
   Wikipedia, Implicit algebraic surfaces
NR 50
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 8
PY 2024
DI 10.1007/s00371-023-03216-y
EA JAN 2024
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EG4P9
UT WOS:001137759400001
OA hybrid
DA 2024-08-05
ER

PT J
AU He, XN
   Xia, YK
   Qiao, YS
   Lee, B
   Ye, YH
AF He, Xiaonan
   Xia, Yukun
   Qiao, Yuansong
   Lee, Brian
   Ye, Yuhang
TI Semantic guidance incremental network for efficiency video
   super-resolution
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Video super-resolution; Semantic guidance; Efficiency; Convolutional
   neural network
AB In video streaming, bandwidth constraints significantly affect client-side video quality. Addressing this, deep neural networks offer a promising avenue for implementing video super-resolution (VSR) at the user end, leveraging advancements in modern hardware, including mobile devices. The principal challenge in VSR is the computational intensity involved in processing temporal/spatial video data. Conventional methods, uniformly processing entire scenes, often result in inefficient resource allocation. This is evident in the over-processing of simpler regions and insufficient attention to complex regions, leading to edge artifacts in merged regions. Our innovative approach employs semantic segmentation and spatial frequency-based categorization to divide each video frame into regions of varying complexity: simple, medium, and complex. These are then processed through an efficient incremental model, optimizing computational resources. A key innovation is the sparse temporal/spatial feature transformation layer, which mitigates edge artifacts and ensures seamless integration of regional features, enhancing the naturalness of the super-resolution outcome. Experimental results demonstrate that our method significantly boosts VSR efficiency while maintaining effectiveness. This marks a notable advancement in streaming video technology, optimizing video quality with reduced computational demands. This approach, featuring semantic segmentation, spatial frequency analysis, and an incremental network structure, represents a substantial improvement over traditional VSR methodologies, addressing the core challenges of efficiency and quality in high-resolution video streaming.
C1 [He, Xiaonan; Qiao, Yuansong; Lee, Brian; Ye, Yuhang] Technol Univ Shannon Midlands Midwest, Univ Rd, Athlone N37 HD68, Ireland.
   [Xia, Yukun] Jiangxi Coll Foreign Studies, Nanchang 330099, Jiangxi, Peoples R China.
RP Ye, YH (corresponding author), Technol Univ Shannon Midlands Midwest, Univ Rd, Athlone N37 HD68, Ireland.
EM yuhang.ye@tus.ie
FU IReL Consortium
FX Open Access funding provided by the IReL Consortium.
CR Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304
   Chan K.C., 2022, P IEEE CVF C COMP VI, P5972
   Chan KCK, 2022, PROC CVPR IEEE, P5962, DOI 10.1109/CVPR52688.2022.00588
   Dawood MS, 2021, MATER TODAY-PROC, V46, P3848, DOI 10.1016/j.matpr.2021.02.287
   Gatys LA, 2017, PROC CVPR IEEE, P3730, DOI 10.1109/CVPR.2017.397
   Geng ZC, 2022, PROC CVPR IEEE, P17420, DOI 10.1109/CVPR52688.2022.01692
   Kong XT, 2021, PROC CVPR IEEE, P12011, DOI 10.1109/CVPR46437.2021.01184
   Ledig C, 2014, PROC CVPR IEEE, P3065, DOI 10.1109/CVPR.2014.392
   Lee R, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3469094
   Li G, 2023, PROC CVPR IEEE, P10259, DOI 10.1109/CVPR52729.2023.00989
   Liang Jingyun, 2022, ADV NEURAL INF PROCE, V35, P378
   Liu D, 2017, IEEE I CONF COMP VIS, P2526, DOI 10.1109/ICCV.2017.274
   Liu JM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4611, DOI 10.1109/ICCV48922.2021.00459
   Lu ZH, 2023, Arxiv, DOI arXiv:2305.06524
   Nah S, 2019, IEEE COMPUT SOC CONF, P1996, DOI 10.1109/CVPRW.2019.00251
   Ren WQ, 2017, IEEE I CONF COMP VIS, P1086, DOI 10.1109/ICCV.2017.123
   Tao X, 2017, IEEE I CONF COMP VIS, P4482, DOI 10.1109/ICCV.2017.479
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Wang Hao, 2023, IEEE Trans. Neural Netw. Learn. Syst.
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Wen WL, 2022, IEEE T IMAGE PROCESS, V31, P1761, DOI 10.1109/TIP.2022.3146625
   Xiao ZY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P664, DOI 10.1145/3394171.3413667
   Xu K, 2024, Arxiv, DOI arXiv:2305.00163
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yi P, 2019, IEEE I CONF COMP VIS, P3106, DOI 10.1109/ICCV.2019.00320
   Ying XY, 2020, IEEE SIGNAL PROC LET, V27, P1500, DOI 10.1109/LSP.2020.3013518
   Zhang A., 2023, P IEEE CVF INT C COM, p12,728
   Zhang HC, 2019, IEEE I CONF COMP VIS, P8798, DOI 10.1109/ICCV.2019.00889
   Zhang YH, 2023, INT J COMPUT VISION, V131, P2153, DOI 10.1007/s11263-023-01801-1
   Zhu SZ, 2017, IEEE I CONF COMP VIS, P1689, DOI 10.1109/ICCV.2017.186
NR 31
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 2
PY 2024
DI 10.1007/s00371-024-03488-y
EA JUL 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XG0S4
UT WOS:001260419000001
OA hybrid
DA 2024-08-05
ER

PT J
AU Kita, N
AF Kita, Naoki
TI PackMolds: computational design of packaging molds for thermoforming
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Computational design; Computational fabrication; Thermoforming; Mold
   design
AB We present a novel technique for designing molds suitable for desktop thermoforming, specifically for creating packaging such as blister packs. Our molds, PackMolds, feature neither undercuts nor negative draft angles, facilitating their easy release from thermoformed plastic sheets. In this study, we optimize the geometry of PackMolds to comply with user-specified draft angle constraints. Instead of simulating the traditional thermoforming process, which necessitates time discretization and specifying detailed parameters for both material properties and machine configuration to achieve an accurate simulation result, we formulate our problem as a constrained geometric optimization problem and solve it using a gradient-based solver. Additionally, in contrast to industrial thermoforming, which benefits from advanced tools, desktop thermoforming lacks such sophisticated resources. Therefore, we introduce a suite of assistive tools to enhance the success of desktop thermoforming. Furthermore, we demonstrate its wide applicability by showcasing its use in not only designing blister packs but also in creating double-sided blister packs and model stands.
C1 [Kita, Naoki] Shinshu Univ, Nagano, Japan.
C3 Shinshu University
RP Kita, N (corresponding author), Shinshu Univ, Nagano, Japan.
EM nkita@cs.shinshu-u.ac.jp
CR Alderighi T, 2022, COMPUT GRAPH FORUM, V41, P435, DOI 10.1111/cgf.14581
   Alderighi T, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480555
   Alderighi T, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322981
   Alderighi T, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201381
   [Anonymous], 2020, McNeel: Rhino 7-rhinoceros 3d
   [Anonymous], 2023, McNeel: Rhino 8-rhinoceros 3d
   CGAL, 3d alpha wrapping: user manual
   Edelsbrunner H, 2008, CONTEMP MATH, V453, P257
   Florian J., 1987, Practical Thermoforming: Principles and Applications
   Keinert B, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818131
   Klein P., 2009, Synthesis Lectures on Materials Engineering, V1, P1, DOI DOI 10.2200/S00184ED1V01Y200904MRE001
   Kobbelt LP, 1999, COMPUT GRAPH FORUM, V18, pC119, DOI 10.1111/1467-8659.00333
   Kraft D., 1988, Technical Report DFVLR-FB 88-28
   Malomo L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982397
   mayku.me, Makyu: Formbox
   Mitra NJ, 2013, COMPUT GRAPH FORUM, V32, P1, DOI 10.1111/cgf.12010
   Nakashima K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201341
   Portaneri C, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530152
   Schüller C, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925914
   Sellán S, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417778
   Stein O, 2019, COMPUT GRAPH-UK, V80, P51, DOI 10.1016/j.cag.2019.03.001
   t-sim, Accuform: T-sim
   Throne JL., 1987, Thermoforming
   Valkeneers T, 2019, PROCEEDINGS OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY (UIST 2019), P687, DOI 10.1145/3332165.3347915
   Vollmer J, 1999, COMPUT GRAPH FORUM, V18, pC131, DOI 10.1111/1467-8659.00334
   Zhang YZ, 2017, IEEE T VIS COMPUT GR, V23, P1924, DOI 10.1109/TVCG.2016.2598570
   Zhou QN, 2016, Arxiv, DOI arXiv:1605.04797
NR 27
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 1
PY 2024
DI 10.1007/s00371-024-03462-8
EA JUL 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XC2G8
UT WOS:001259414100006
DA 2024-08-05
ER

PT J
AU Zhou, HP
   Deng, B
   Sun, KL
   Zhang, SX
   Zhang, YQ
AF Zhou, Huaping
   Deng, Bin
   Sun, Kelei
   Zhang, Shunxiang
   Zhang, Yongqi
TI UTE-CrackNet: transformer-guided and edge feature extraction U-shaped
   road crack image segmentation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Road crack segmentation; Semantic segmentation; U-shaped structure;
   Transformer; Edge detection
ID NETWORK
AB Cracks in the road surface can cause significant harm. Road crack detection, segmentation, and immediate repair can help reduce the occurrence of risks. Some methods based on convolutional neural networks still have some problems, such as fuzzy edge information, small receptive fields, and insufficient perception ability of local information. To solve the above problems, this paper offers UTE-CrackNet, a novel road crack segmentation network that attempts to increase the generalization ability and segmentation accuracy of road crack segmentation networks. To begin, our design combines the U-shaped structure that enables the model to learn more features. Given the lack of skip connections, we designed the multi-convolution coordinate attention block to reduce semantic differences in cascaded features and the gated residual attention block to get more local features. Because most fractures have strip characteristics, we propose the transformer edge atlas spatial pyramid pooling module, which innovatively applies the transformer module and edge detection module to the network so that the network can better capture the edge information and context information of the fracture area. In addition, we use focus loss in training to solve the problem of positive and negative sample imbalances. Experiments were conducted on four publicly available road crack segmentation datasets: Rissbilder, GAPS384, CFD, and CrackTree200. The experimental results reveal that the network outperforms the standard road fracture segmentation models. The code and models are publicly available at https://github.com/mushan0929/UTE-crackNet.
C1 [Zhou, Huaping; Deng, Bin; Sun, Kelei; Zhang, Shunxiang; Zhang, Yongqi] Anhui Univ Sci & Technol, Sch Comp Sci & Engn, Huainan 232001, Peoples R China.
   [Zhou, Huaping; Deng, Bin; Sun, Kelei; Zhang, Shunxiang; Zhang, Yongqi] Anhui Univ Sci & Technol, Affiliated Hosp 1, Huainan 232001, Peoples R China.
C3 Anhui University of Science & Technology; Anhui University of Science &
   Technology
RP Deng, B (corresponding author), Anhui Univ Sci & Technol, Sch Comp Sci & Engn, Huainan 232001, Peoples R China.; Deng, B (corresponding author), Anhui Univ Sci & Technol, Affiliated Hosp 1, Huainan 232001, Peoples R China.
EM hpzhou@aust.edu.cn; 2014626737@qq.com; klsun@aust.edu.cn;
   sxzhang@aust.edu.cn; 1297676518@qq.com
FU Key Research and Development Projects in Anhui Province
FX No Statement Available
CR Ai DH, 2023, ENG APPL ARTIF INTEL, V117, DOI 10.1016/j.engappai.2022.105478
   Al-Huda Z, 2023, ENG APPL ARTIF INTEL, V122, DOI 10.1016/j.engappai.2023.106142
   Arbaoui A, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10151772
   Augustauskas R, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20092557
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Cao J., 2021, IEEE Tran. Instrument. Measure., P1
   [曹锦纲 Cao Jingang], 2020, [计算机辅助设计与图形学学报, Journal of Computer-Aided Design & Computer Graphics], V32, P1324
   Chen FC, 2018, IEEE T IND ELECTRON, V65, P4392, DOI 10.1109/TIE.2017.2764844
   Chen L, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278067
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen ZH, 2023, IEEE T PATTERN ANAL, V45, P13489, DOI 10.1109/TPAMI.2023.3293885
   Cheng YW, 2021, KNOWL-BASED SYST, V216, DOI 10.1016/j.knosys.2021.106796
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Dai L., Nat. Commun, DOI 10-1038/s41467021-23458-5
   Dais D, 2021, AUTOMAT CONSTR, V125, DOI 10.1016/j.autcon.2021.103606
   Eisenbach M., 2017, 2017 INT JOINT C NEU
   Feng SL, 2020, IEEE T MED IMAGING, V39, P3008, DOI 10.1109/TMI.2020.2983721
   Gopalakrishnan K, 2017, CONSTR BUILD MATER, V157, P322, DOI 10.1016/j.conbuildmat.2017.09.110
   Huili Zhao, 2010, Proceedings of the 2010 3rd International Congress on Image and Signal Processing (CISP 2010), P964, DOI 10.1109/CISP.2010.5646923
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Jun F., 2022, 2022 INT C COMP ENG
   Li HF, 2019, IEEE T INTELL TRANSP, V20, P2025, DOI 10.1109/TITS.2018.2856928
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Li Y., 2016, ICLR, P1
   Liang S., 2010, ICLEM 2010
   Lin QH, 2023, ENG APPL ARTIF INTEL, V126, DOI 10.1016/j.engappai.2023.106876
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu CAQ, 2022, IEEE T INTELL TRANSP, V23, P15546, DOI 10.1109/TITS.2022.3141827
   Liu YH, 2019, NEUROCOMPUTING, V338, P139, DOI 10.1016/j.neucom.2019.01.036
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu J, 2019, IEEE T KNOWL DATA EN, V31, P2346, DOI 10.1109/TKDE.2018.2876857
   Myeongsuk Pak, 2021, Advances in Computer Science and Ubiquitous Computing. CSA-CUTE 2019. Lecture Notes in Electrical Engineering (LNEE 715), P267, DOI 10.1007/978-981-15-9343-7_36
   Nazir A, 2020, IEEE T IMAGE PROCESS, V29, P7192, DOI 10.1109/TIP.2020.2999854
   Qu Z, 2022, IEEE T INTELL TRANSP, V23, P16038, DOI 10.1109/TITS.2022.3147669
   Qu Z, 2022, IEEE T INTELL TRANSP, V23, P11710, DOI 10.1109/TITS.2021.3106647
   Qu Z, 2022, IEEE T NEUR NET LEAR, V33, P4890, DOI 10.1109/TNNLS.2021.3062070
   Ren YP, 2020, CONSTR BUILD MATER, V234, DOI 10.1016/j.conbuildmat.2019.117367
   Ronneberger O., 2015, P MED IM COMP COMP A, P234, DOI [DOI 10.48550/ARXIV.1505.04597, DOI 10.1007/978-3-319-24574-4_28]
   Shi Y, 2016, IEEE T INTELL TRANSP, V17, P3434, DOI 10.1109/TITS.2016.2552248
   Subirats P, 2006, IEEE IMAGE PROC, P3037, DOI 10.1109/ICIP.2006.313007
   Sun XZ, 2022, IEEE T INTELL TRANSP, V23, P18392, DOI 10.1109/TITS.2022.3158670
   Tang JS, 2013, IEEE SYS MAN CYBERN, P3026, DOI 10.1109/SMC.2013.516
   Valanarasu JMJ, 2022, LECT NOTES COMPUT SC, V13435, P23, DOI 10.1007/978-3-031-16443-9_3
   Wang Z., 2024, Construct Build. Mater., V411
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1007/s11263-017-1004-z, 10.1109/ICCV.2015.164]
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Xing ZK, 2020, KNOWL-BASED SYST, V194, DOI 10.1016/j.knosys.2020.105570
   Yang L, 2023, AUTOMAT CONSTR, V150, DOI 10.1016/j.autcon.2023.104853
   Yu F., 2016, ICLR, P1
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zhong JT, 2022, AUTOMAT CONSTR, V141, DOI 10.1016/j.autcon.2022.104436
   Zou Q, 2019, IEEE T IMAGE PROCESS, V28, P1498, DOI 10.1109/TIP.2018.2878966
NR 53
TC 0
Z9 0
U1 6
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 20
PY 2024
DI 10.1007/s00371-024-03531-y
EA JUN 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UW9Q5
UT WOS:001251223700001
DA 2024-08-05
ER

PT J
AU Zhang, B
   Ma, R
   Cao, Y
   An, P
AF Zhang, Bing
   Ma, Ran
   Cao, Yu
   An, Ping
TI Swin-VEC: Video Swin Transformer-based GAN for video error concealment
   of VVC
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Video error concealment; Video Swin Transformer; Generative adversarial
   network; Spatiotemporal dimensions reconstruction
AB Video error concealment can effectively improve the visual perception quality of videos damaged by packet loss in video transmission or error reception at the decoder. The latest versatile video coding (VVC) standard further improves the compression performance and lacks error recovery mechanism, which makes the VVC bitstream highly sensitive to errors. Most of the existing error concealment algorithms are designed for the video coding standards before VVC and are not applicable to VVC; thus, the research on video error concealment for VVC is urgently needed. In this paper, a novel deep video error concealment model for VVC is proposed, called Swin-VEC. The model innovatively integrates Video Swin Transformer into the generator of generative adversarial network (GAN). Specifically, the generator of the model employs convolutional neural network (CNN) to extract shallow features, and utilizes the Video Swin Transformer to extract deep multi-scale features. Subsequently, the designed dual upsampling modules are used to accomplish the recovery of spatiotemporal dimensions, and combined with CNN to achieve frame reconstruction. Moreover, an augmented dataset BVI-DVC-VVC is constructed for model training and verification. The optimization of the model is realized by adversarial training. Extensive experiments on BVI-DVC-VVC and UCF101 demonstrate the effectiveness and superiority of our proposed model for the video error concealment of VVC.
C1 [Zhang, Bing; Ma, Ran; Cao, Yu; An, Ping] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
   [Zhang, Bing; Ma, Ran; Cao, Yu; An, Ping] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai 200444, Peoples R China.
C3 Shanghai University; Shanghai University
RP Ma, R (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.; Ma, R (corresponding author), Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai 200444, Peoples R China.
EM maran@shu.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Benjak M, 2021, IEEE IMAGE PROC, P2114, DOI 10.1109/ICIP42928.2021.9506399
   Bross B, 2021, IEEE T CIRC SYST VID, V31, P3736, DOI 10.1109/TCSVT.2021.3101953
   Byongsu H, 2019, MULTIMED TOOLS APPL, V78, P2587, DOI 10.1007/s11042-018-6362-1
   Cao Hu, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13803), P205, DOI 10.1007/978-3-031-25066-8_9
   Chang YL, 2019, IEEE I CONF COMP VIS, P9065, DOI 10.1109/ICCV.2019.00916
   Fan CM, 2022, IEEE INT SYMP CIRC S, P2333, DOI 10.1109/ISCAS48785.2022.9937486
   Iqbal R., 2022, Cecnn: a convergent error concealment neural network for videos
   Kazemi M, 2021, MULTIMED TOOLS APPL, V80, P27385, DOI 10.1007/s11042-021-11005-9
   Kazemi M, 2021, MULTIMED TOOLS APPL, V80, P12685, DOI 10.1007/s11042-020-10333-6
   Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244
   Kim TH, 2018, LECT NOTES COMPUT SC, V11207, P111, DOI 10.1007/978-3-030-01219-9_7
   Lee YH, 2017, J SIGNAL PROCESS SYS, V88, P13, DOI 10.1007/s11265-016-1112-y
   Liu J, 2015, IEEE T CIRC SYST VID, V25, P353, DOI 10.1109/TCSVT.2014.2359145
   Liu R, 2021, Arxiv, DOI arXiv:2104.06637
   Liu R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14020, DOI 10.1109/ICCV48922.2021.01378
   Liu Z, 2022, PROC CVPR IEEE, P3192, DOI 10.1109/CVPR52688.2022.00320
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ma D, 2022, IEEE T MULTIMEDIA, V24, P3847, DOI 10.1109/TMM.2021.3108943
   Marvasti-Zadeh SM, 2016, Arxiv, DOI arXiv:1610.07753
   Nazeri K., 2019, arXiv
   Odena A., 2016, Distill, DOI [10.23915/distill.00003.-URL, 10.23915/distill.00003, DOI 10.23915/DISTILL.00003]
   Sabour S, 2017, ADV NEUR IN, V30
   Sankisa A, 2020, SIGNAL IMAGE VIDEO P, V14, P1369, DOI 10.1007/s11760-020-01671-x
   Sankisa A, 2018, IEEE IMAGE PROC, P380, DOI 10.1109/ICIP.2018.8451090
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Vaswani A, 2017, ADV NEUR IN, V30
   Xiang CY, 2019, INT CONF ACOUST SPEE, P1827, DOI 10.1109/icassp.2019.8683622
   Xu JJ, 2018, IEEE IMAGE PROC, P3294, DOI 10.1109/ICIP.2018.8451175
   Yanhong Zeng, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P528, DOI 10.1007/978-3-030-58517-4_31
   Yao DZ, 2024, VISUAL COMPUT, V40, P2589, DOI 10.1007/s00371-023-02939-2
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang KD, 2022, LECT NOTES COMPUT SC, V13678, P74, DOI 10.1007/978-3-031-19797-0_5
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
   Zhu XS, 2023, VISUAL COMPUT, V39, P4721, DOI 10.1007/s00371-022-02620-0
NR 37
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 18
PY 2024
DI 10.1007/s00371-024-03518-9
EA JUN 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UQ5T1
UT WOS:001249543600007
DA 2024-08-05
ER

PT J
AU Oh, G
   Moon, B
AF Oh, Geunwoo
   Moon, Bochang
TI Joint self-attention for denoising Monte Carlo rendering
SO VISUAL COMPUTER
LA English
DT Article
DE Monte Carlo denoising; Monte Carlo rendering; Joint self-attention;
   Transformer-based denoising
ID IMAGE
AB Image-space denoising of rendered images has become a commonly adopted approach since this post-rendering process often drastically reduces required sample counts (thus rendering times) for producing a visually pleasing image without noticeable noise. It is a common practice to conduct such denoising while preserving image details by exploiting auxiliary information (e.g., G-buffers) as well as input colors. However, it is still challenging to devise an ideal denoising framework that fully considers the two inputs with different characteristics, e.g., noisy but complete shading information in the input colors and less noisy but partial shading information in the auxiliary buffers. This paper proposes a transformer-based denoising framework with a new self-attention mechanism that infers a joint self-attention map, i.e., self-similarity in input features, through dual attention scores: one from noisy colors and another from auxiliary buffers. We demonstrate that this separate consideration of the two inputs allows our framework to produce more accurate denoising results than state-of-the-art denoisers for various test scenes.
C1 [Oh, Geunwoo; Moon, Bochang] Gwangju Inst Sci & Technol, Gwangju, South Korea.
C3 Gwangju Institute of Science & Technology (GIST)
RP Moon, B (corresponding author), Gwangju Inst Sci & Technol, Gwangju, South Korea.
EM bmoon@gist.ac.kr
OI Moon, Bochang/0000-0003-3142-0115
FU Institute of Information & communications Technology Planning &
   Evaluation (IITP) grant funded by the Korea government (MSIT);
   NewSee2l035 (Modern Hall)
FX We appreciate the anonymous reviewers for their constructive comments.
   We also appreciate the following authors and artists for each scene:
   Mareck (Bath Room), SlykDrako (Bed Room), thecali (Car2, Spaceship),
   NovaAshbell (Class Room), Wig42 (Dining Room, Staircase, Grey & White
   Room, Modern Living Room), UP3D (Little Lamp), Benedikt Bitterli (Utah
   Teapot, Veach Ajar), Cem Yuksel (Curly Hair), Jay-Artist (Country
   Kitchen, White Room), NewSee2l035 (Modern Hall), and BhaWin (Glass of
   Water).
CR Ba JL, 2016, ARXIV
   Back J., 2022, ACM SIGGRAPH 2022 C
   Back J, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417847
   Bahdanau D, 2016, Arxiv, DOI arXiv:1409.0473
   Bako S, 2017, ACM T GRAPHIC, V36, DOI [10.1145/3072959.3073703, 10.1145/3072959.3073708]
   Balint M, 2023, PROCEEDINGS OF SIGGRAPH 2023 CONFERENCE PAPERS, SIGGRAPH 2023, DOI 10.1145/3588432.3591562
   Bitterli B., 2014, TUNGSTEN RENDERER
   Bitterli B., 2016, RENDERING RESOURCES
   Bitterli B, 2016, COMPUT GRAPH FORUM, V35, P107, DOI 10.1111/cgf.12954
   Chaitanya CRA, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073601
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Cho IY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459876
   Fan HM, 2021, COMPUT GRAPH FORUM, V40, P15, DOI 10.1111/cgf.14338
   Firmino A, 2022, COMPUT GRAPH FORUM, V41, P1, DOI 10.1111/cgf.14454
   Gharbi M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322954
   Gu J, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555496
   Holger D., 2010, P C HIGH PERF GRAPH, P67
   Huo Y, 2021, COMPUT VIS MEDIA, V7, P169, DOI 10.1007/s41095-021-0209-9
   Isik M, 2021, ACM T GRAPHIC, V40, DOI [10.1145/3450626.3459793, 10.1145/3476576.3476580]
   Kajiya J. T., 1986, Computer Graphics, V20, P143, DOI 10.1145/15886.15902
   Kalantari NK, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766977
   Li TM, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366213
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lin WH, 2021, COMPUT GRAPH FORUM, V40, P369, DOI 10.1111/cgf.14194
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Loshchilov I., 2017, INT C LEARNING REPRE
   Loshchilov I., 2018, INT C LEARN REPR
   Meng X., 2020, EUROGRAPHICS S RENDE
   Moon B, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2641762
   Moon B, 2013, COMPUT GRAPH FORUM, V32, P139, DOI 10.1111/cgf.12004
   Munkberg J, 2020, COMPUT GRAPH FORUM, V39, P1, DOI 10.1111/cgf.14049
   Rousselle F, 2013, COMPUT GRAPH FORUM, V32, P121, DOI 10.1111/cgf.12219
   Rousselle F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024193
   Sen P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2167076.2167083
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Vaswani A, 2021, PROC CVPR IEEE, P12889, DOI 10.1109/CVPR46437.2021.01270
   Vaswani A, 2017, ADV NEUR IN, V30
   Vogels T, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201388
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Xu B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356547
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu YF, 2022, COMPUT VIS MEDIA, V8, P33, DOI 10.1007/s41095-021-0247-3
   Yu JQ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480565
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang XY, 2021, COMPUT GRAPH FORUM, V40, P1, DOI 10.1111/cgf.14337
   Zheng SK, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480510
   Zwicker M, 2015, COMPUT GRAPH FORUM, V34, P667, DOI 10.1111/cgf.12592
NR 49
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4623
EP 4634
DI 10.1007/s00371-024-03446-8
EA JUN 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001246607000002
DA 2024-08-05
ER

PT J
AU Tan, YW
   Low, SE
   Chow, J
   Teo, J
   Bhojan, A
AF Tan, Yu Wei
   Low, Siang Ern
   Chow, Jonas
   Teo, Javon
   Bhojan, Anand
TI DHR plus S: distributed hybrid rendering with realistic real-time
   shadows for interactive thin client metaverse and game applications
SO VISUAL COMPUTER
LA English
DT Article
DE Distributed hybrid rendering; Real-time; Ray tracing; Rasterization;
   Soft shadows; Ambient occlusion; Thin client; Interactive; Metaverse;
   Games
AB Distributed hybrid rendering (DHR) is a real-time rendering approach that incorporates cloud-based ray tracing with locally rasterized graphics for interactive thin client metaverse and game applications. With cloud assistance, DHR can generate high-fidelity ray-traced graphics contents remotely and deliver them to thin clients with low graphics capability, including standalone extended reality devices and mobile phones, while maintaining interactive frame rates for users under adverse network conditions. DHR can already achieve the effect of ray-traced hard shadows that form with the occlusion of direct illumination. We enhance the realism of these shadows by softening their edges with the direction of rays traced and approximating the occlusion of indirect illumination by reconstructing ray-traced ambient occlusion with a modified version of spatiotemporal variance-guided filtering. Our technique uses only 20-30% of the bandwidth of remote rendering and is also tolerant of delays of up to 200 ms with only slight distortion to the shadows along object edges.
C1 [Tan, Yu Wei; Low, Siang Ern; Chow, Jonas; Teo, Javon; Bhojan, Anand] Natl Univ Singapore, Sch Comp, Singapore, Singapore.
C3 National University of Singapore
RP Tan, YW (corresponding author), Natl Univ Singapore, Sch Comp, Singapore, Singapore.
EM yuwei@u.nus.edu; e0388996@u.nus.edu; e0544170@u.nus.edu;
   e0725706@u.nus.edu; banand@comp.nus.edu.sg
FU Singapore Ministry of Education Academic Research [T1 251RES2205]
FX This work is supported by the Singapore Ministry of Education Academic
   Research grant T1 251RES2205, "Real-time Distributed Hybrid Rendering
   with 5G Edge Computing for Realistic Graphics in Mobile Games and
   Metaverse Applications".
CR Anand B, 2014, 2014 SEVENTH INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND UBIQUITOUS NETWORKING (ICMU), P14, DOI 10.1109/ICMU.2014.6799051
   Barr-Brisebois C., 2019, RAY TRACING GEMS
   Beck S., 1981, CPU GPU HYBRID REAL, P1
   Le BH, 2019, ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES (I3D 2019), DOI 10.1145/3306131.3317029
   Cantory V, 2023, 2023 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS, VRW, P893, DOI 10.1109/VRW58643.2023.00289
   Chalmers A., 2006, P 4 INT C COMPUTER G, P9, DOI [http://doi.acm.org/10.1145/1174429.1174431, DOI 10.1145/1174429.1174431]
   Cook R. L., 1984, Computers & Graphics, V18, P137
   Datta Sayantan, 2022, SIGGRAPH22 Conference Proceeding: Special Interest Group on Computer Graphics and Interactive Techniques Conference Proceedings, DOI 10.1145/3528233.3530700
   Dhawal S., 2023, P 13 IND C COMP VIS, DOI [10.1145/3571600.3571640, DOI 10.1145/3571600.3571640]
   Fang H., 2022, 2022 INT C CYB, P47, DOI [10.1109/CW55638.2022.00016, DOI 10.1109/CW55638.2022.00016]
   Gautron P, 2022, PROCEEDINGS SIGGRAPH 2022 TALKS, DOI 10.1145/3532836.3536239
   Guo J, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480531
   Jiang HM, 2019, SA'19: SIGGRAPH ASIA 2019 TECHNICAL BRIEFS, P41, DOI 10.1145/3355088.3365154
   Keinert B, 2018, ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES (I3D 2018), DOI 10.1145/3190834.3190847
   Laine S, 2010, COMPUT GRAPH FORUM, V29, P1325, DOI 10.1111/j.1467-8659.2010.01728.x
   Landis H., 2004, Production-ready global illumination
   Lauterbach C., 2009, TR09004 UNC CS
   Li L, 2024, IEEE NETWORK, V38, P137, DOI 10.1109/MNET.133.2200524
   Lin C, 2019, IEEE IMAGE PROC, P529, DOI [10.1109/icip.2019.8803809, 10.1109/ICIP.2019.8803809]
   Liu Dan, 2022, 2022 IEEE Smartworld, Ubiquitous Intelligence & Computing, Scalable Computing & Communications, Digital Twin, Privacy Computing, Metaverse, Autonomous & Trusted Vehicles (SmartWorld/UIC/ScalCom/DigitalTwin/PriComp/Meta), P2366, DOI 10.1109/SmartWorld-UIC-ATC-ScalCom-DigitalTwin-PriComp-Metaverse56740.2022.00332
   Lu E, 2023, INT SYM MIX AUGMENT, P312, DOI 10.1109/ISMAR59233.2023.00046
   Ma BZ, 2022, INT PARALL DISTRIB P, P672, DOI 10.1109/IPDPS53621.2022.00071
   Schied C, 2017, HPG '17: PROCEEDINGS OF HIGH PERFORMANCE GRAPHICS, DOI 10.1145/3105762.3105770
   Shen H, 2023, IEEE INT CONF COMM, P1451, DOI 10.1109/ICCWORKSHOPS57953.2023.10283486
   Sheng YC, 2021, PROC CVPR IEEE, P4378, DOI 10.1109/CVPR46437.2021.00436
   Shi PT, 2022, P ACM COMPUT GRAPH, V5, DOI 10.1145/3522614
   Sun QP, 2024, IEEE T CONSUM ELECTR, V70, P338, DOI 10.1109/TCE.2023.3328051
   Tan Y.W., 2022, P 1 WORKSH INT EXTEN, P51, DOI [10.1145/3552483.3556455, DOI 10.1145/3552483.3556455]
   Tan YW, 2022, VISIGRAPP, P302, DOI 10.5220/0010996200003124
   Tewari A, 2020, COMPUT GRAPH FORUM, V39, P701, DOI 10.1111/cgf.14022
   Tian CW, 2020, NEURAL NETWORKS, V131, P251, DOI 10.1016/j.neunet.2020.07.025
   Vermeer J, 2021, P ACM COMPUT GRAPH, V4, DOI 10.1145/3451268
   Weinrauch A, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592431
   WHITTED T, 1980, COMMUN ACM, V23, P343, DOI 10.1145/358876.358882
   Xiao L, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392376
   Xu Y, 2022, P ACM COMPUT GRAPH, V5, DOI 10.1145/3522617
   Zhang DJ, 2020, IEEE ACCESS, V8, P64434, DOI 10.1109/ACCESS.2020.2984771
   Zhou L, 2023, 2023 IEEE CLOUD SUMMIT, P1, DOI 10.1109/CloudSummit57601.2023.00007
   Zhukov S., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P45
NR 39
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4981
EP 4991
DI 10.1007/s00371-024-03501-4
EA JUN 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001242180100003
OA hybrid
DA 2024-08-05
ER

PT J
AU Nie, YH
   Zhao, XB
   Li, YX
   Lu, QW
   Tao, QC
   Yu, YM
AF Nie, Yihe
   Zhao, Xingbo
   Li, Yongxiang
   Lu, Qianwen
   Tao, Qingchuan
   Yu, Yanmei
TI DEAR: a novel deep-level semantics feature reinforce framework for
   Infrared Small Object Segmentation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Infrared Small Object Segmentation; Multi-Scale Nested; Deep feature
   preservation
ID SMALL TARGET DETECTION; MODEL
AB Infrared Small Object Segmentation (ISOS) faces challenges in isolating small and faint objects from infrared images due to their limited texture details and small spatial presence. Existing deep learning methods have shown promise but often assume that these networks can effectively map small objects to deep semantic features. This mapping, however, may not be accurately learned by the model due to the excessive downsampling, which may cause the loss of high-level semantic representations essential for accurately identifying small objects in infrared images. To address this issue, this study introduces a novel learning paradigm, DEAR, designed to reinforce deep-level features of infrared small objects from three perspectives. Specifically, the Multi-Scale Nested (MSN) module merges shallow object features with deep semantic features through iterative interactions. The Central Difference Convolution (CDC) module enhances semantic contrast between small objects and their backgrounds, minimizing information loss during downsampling. Additionally, the Orthogonal Dynamic Fusion (ODF) module enhances the depth feature representation by emphasizing and fusing the key features of small objects in both channel and spatial dimensions. Experimental evaluations on the NUAA-SIRST and NUDT-SIRST datasets show that DEAR significantly outperforms 18 state-of-the-art methods in ISOS, demonstrating its effectiveness. The code is publicly available online at https://github.com/nieyihe888/DEAR.
C1 [Nie, Yihe; Tao, Qingchuan; Yu, Yanmei] Sichuan Univ, Coll Elect Informat, Chengdu 610065, Peoples R China.
   [Zhao, Xingbo] Alibaba Grp, Hangzhou 310023, Peoples R China.
   [Li, Yongxiang] Sichuan Univ, Coll Comp Sci, Chengdu 610065, Peoples R China.
   [Lu, Qianwen] Univ Tokyo, Grad Sch Interdisciplinary Informat Studies, Tokyo 1638001, Japan.
C3 Sichuan University; Alibaba Group; Sichuan University; University of
   Tokyo
RP Li, YX (corresponding author), Sichuan Univ, Coll Comp Sci, Chengdu 610065, Peoples R China.
EM 742004668@qq.com; zhaoxingbo.zxb@alibaba-inc.com;
   rhythmli.scu@gmail.com; 396416324@qq.com; taoqingchuan@scu.edu.cn;
   yuyanmei@scu.edu.cn
OI Li, Yongxiang/0000-0002-0496-8196
CR Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bai K, 2016, IEEE IMAGE PROC, P181, DOI 10.1109/ICIP.2016.7532343
   Bai XZ, 2010, PATTERN RECOGN, V43, P2145, DOI 10.1016/j.patcog.2009.12.023
   Chen CLP, 2014, IEEE T GEOSCI REMOTE, V52, P574, DOI 10.1109/TGRS.2013.2242477
   Chen LC, 2016, Arxiv, DOI [arXiv:1412.7062, DOI 10.48550/ARXIV.1412.7062, 10.48550/ARXIV.1412.7062]
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, 10.48550/arXiv.1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chu H, 2018, PROC CVPR IEEE, P3002, DOI 10.1109/CVPR.2018.00317
   Dai YM, 2021, IEEE T GEOSCI REMOTE, V59, P9813, DOI 10.1109/TGRS.2020.3044958
   Dai YM, 2021, IEEE WINT CONF APPL, P949, DOI 10.1109/WACV48630.2021.00099
   Dai YM, 2017, IEEE J-STARS, V10, P3752, DOI 10.1109/JSTARS.2017.2700023
   Deng H, 2016, IEEE T GEOSCI REMOTE, V54, P4204, DOI 10.1109/TGRS.2016.2538295
   Deng H, 2016, IEEE T AERO ELEC SYS, V52, P60, DOI 10.1109/TAES.2015.140878
   Deshpande SD, 1999, P SOC PHOTO-OPT INS, V3809, P74, DOI 10.1117/12.364049
   do Nascimento MG, 2019, IEEE I CONF COMP VIS, P5147, DOI 10.1109/ICCV.2019.00525
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Gao CQ, 2013, IEEE T IMAGE PROCESS, V22, P4996, DOI 10.1109/TIP.2013.2281420
   Guo HB, 2021, IEEE T CYBERNETICS, V51, P2735, DOI 10.1109/TCYB.2019.2934823
   Han JH, 2021, IEEE GEOSCI REMOTE S, V18, P1670, DOI 10.1109/LGRS.2020.3004978
   Han JH, 2014, IEEE GEOSCI REMOTE S, V11, P2168, DOI 10.1109/LGRS.2014.2323236
   He X, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2020.3039240
   Huang HM, 2020, INT CONF ACOUST SPEE, P1055, DOI [10.1109/icassp40776.2020.9053405, 10.1109/ICASSP40776.2020.9053405]
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Li BY, 2023, IEEE T IMAGE PROCESS, V32, P1745, DOI 10.1109/TIP.2022.3199107
   Lin T.-Y., 2017, PROC CVPR IEEE, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu S, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14133232
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma TL, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2022.3140432
   Nazir A, 2020, IEEE T IMAGE PROCESS, V29, P7192, DOI 10.1109/TIP.2020.2999854
   Pan PW, 2023, IEEE INT CON MULTI, P2381, DOI 10.1109/ICME55011.2023.00406
   Quan TM, 2021, FRONT COMP SCI-SWITZ, V3, DOI 10.3389/fcomp.2021.613981
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sun Y, 2021, IEEE T GEOSCI REMOTE, V59, P3737, DOI 10.1109/TGRS.2020.3022069
   Teutsch M., 2010, 2010 INT WATERSIDE S, P1
   Tong XZ, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3279253
   Wang KW, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3163410
   Wang ZG, 2023, IEEE T NEUR NET LEAR, V34, P3811, DOI 10.1109/TNNLS.2021.3128968
   Wei YT, 2016, PATTERN RECOGN, V58, P216, DOI 10.1016/j.patcog.2016.04.002
   Wu TH, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3235002
   Wu X, 2023, IEEE T IMAGE PROCESS, V32, P364, DOI 10.1109/TIP.2022.3228497
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Yu FS, 2016, Arxiv, DOI arXiv:1511.07122
   Yu ZT, 2020, PROC CVPR IEEE, P5294, DOI 10.1109/CVPR42600.2020.00534
   Yuan S, 2024, Arxiv, DOI arXiv:2401.15583
   Zeng M, 2006, INFRARED PHYS TECHN, V48, P67, DOI 10.1016/j.infrared.2005.04.006
   Zhang LD, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11040382
   Zhang LD, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10111821
   Zhang MJ, 2022, PROC CVPR IEEE, P867, DOI 10.1109/CVPR52688.2022.00095
   Zhang TF, 2023, IEEE T AERO ELEC SYS, V59, P4250, DOI [10.1109/TAES.2023.3238703, 10.1109/ICASSP49357.2023.10096500]
   Zhang TF, 2021, Arxiv, DOI arXiv:2111.03580
   Zhang Y, 2023, INT J INTELL SYST, V2023, DOI 10.1155/2023/2850370
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
   Zou X., 2024, Advances in Neural Information Processing Systems, V36
   Zuo Z, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14143412
NR 59
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 5
PY 2024
DI 10.1007/s00371-024-03499-9
EA JUN 2024
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TO3D0
UT WOS:001242155100002
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Long, JW
   Zhang, KX
   Zhu, JZ
AF Long, Jianwu
   Zhang, Kaixin
   Zhu, Jiangzhou
TI Edge-aware texture filtering with superpixels constraint
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Texture filtering; Edge-aware; Superpixel; Texture measurement
AB Extracting meaningful structural edges from complex texture images presents a significant challenge. Accurately measuring and differentiating texture information within an image are crucial for efficient texture filtering. While most existing texture filtering methods employ regular rectangular filter windows, the irregularity inherent in textures and structures can limit measurement accuracy, reducing the effectiveness of texture filtering. To address this problem, we propose an edge-aware texture filtering method that integrates superpixels. By employing patch shift, our filter constructs an edge-aware filtering region with superpixels constraint. This region includes pixels with minimal differences and similar texture characteristics. Utilizing the perceptual properties of superpixels for irregular edges enhances texture measurement, thereby improving the quality of texture filtering. Experimental results demonstrate that the proposed method outperforms existing techniques, yielding superior filtering outcomes. The source code is available at: https://github.com/kxZhang1016/EATFS.
C1 [Long, Jianwu; Zhang, Kaixin] Chongqing Univ Technol, Coll Comp Sci & Engn, Chongqing 400054, Peoples R China.
   [Zhu, Jiangzhou] Jilin Univ, Coll Comp Sci & Technol, Changchun 130012, Jilin, Peoples R China.
C3 Chongqing University of Technology; Jilin University
RP Zhang, KX (corresponding author), Chongqing Univ Technol, Coll Comp Sci & Engn, Chongqing 400054, Peoples R China.
EM jwlong@cqut.edu.cn; KxZhang@stu.cqut.edu.cn; zhujz23@mails.jlu.edu.cn
FU Science and Technology Research Program of Chongqing Municipal Education
   Commission; Humanities and Social Sciences Research Program of Chongqing
   Municipal Education Commission [23SKGH263]; Foundation and Frontier
   Research Key Program of Chongqing Science and Technology Commission
   [cstc2015jcyjBX0127]; National Natural Science Foundation of China for
   Young Scientists [61502065]; Funding Achievements of the Action Plan for
   High Quality Development of Graduate Education at Chongqing University
   of Technology [gzlcx20233225];  [KJQN202201148]
FX This work was supported by the Science and Technology Research Program
   of Chongqing Municipal Education Commission (Grant No. KJQN202201148),
   the Humanities and Social Sciences Research Program of Chongqing
   Municipal Education Commission (Grant No. 23SKGH263), the Foundation and
   Frontier Research Key Program of Chongqing Science and Technology
   Commission (Grant No. cstc2015jcyjBX0127), the National Natural Science
   Foundation of China for Young Scientists (Grant No. 61502065), and the
   Funding Achievements of the Action Plan for High Quality Development of
   Graduate Education at Chongqing University of Technology (Grant No.
   gzlcx20233225).
CR Bao LC, 2014, IEEE T IMAGE PROCESS, V23, P555, DOI 10.1109/TIP.2013.2291328
   Cai BL, 2017, IEEE IMAGE PROC, P250, DOI 10.1109/ICIP.2017.8296281
   Cao W, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3113985
   Cho H, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601188
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Ghosh S, 2020, IEEE T CIRC SYST VID, V30, P2015, DOI 10.1109/TCSVT.2019.2916589
   Ham B, 2015, PROC CVPR IEEE, P4823, DOI 10.1109/CVPR.2015.7299115
   Huang JQ, 2023, IEEE T IMAGE PROCESS, V32, P1627, DOI 10.1109/TIP.2023.3247181
   Jeon J, 2016, COMPUT GRAPH FORUM, V35, P77, DOI 10.1111/cgf.13005
   Karacan L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508403
   Lee H, 2017, COMPUT GRAPH FORUM, V36, P262, DOI 10.1111/cgf.12875
   Li MJ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1875, DOI 10.1145/3503161.3547857
   Lin TH, 2016, COMPUT GRAPH FORUM, V35, P57, DOI 10.1111/cgf.13003
   Magnier Baptiste, 2015, Image Analysis. 19th Scandinavian Conference, SCIA 2015. Proceedings: LNCS 9127, P3, DOI 10.1007/978-3-319-19665-7_1
   Porikli F., 2008, 2008 IEEE C COMP VIS, DOI [10.1109/CVPR.2008.4587843, DOI 10.1109/CVPR.2008.4587843]
   Pradhan K, 2024, VISUAL COMPUT, V40, P505, DOI 10.1007/s00371-023-02796-z
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Song CF, 2019, COMPUT GRAPH FORUM, V38, P149, DOI 10.1111/cgf.13824
   Sun BC, 2023, VISUAL COMPUT, V39, P5327, DOI 10.1007/s00371-022-02662-4
   Tomasi C, 1998, P IEEE INT C COMPUTE, P839
   Xu L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366158
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Xu PP, 2018, IEEE T IMAGE PROCESS, V27, P3621, DOI 10.1109/TIP.2018.2820427
   Yang QX, 2016, PROC CVPR IEEE, P4517, DOI 10.1109/CVPR.2016.489
   Yang QX, 2009, PROC CVPR IEEE, P557, DOI 10.1109/CVPRW.2009.5206542
   Yin H, 2019, PROC CVPR IEEE, P8750, DOI 10.1109/CVPR.2019.00896
   Yuan Y, 2021, IEEE T IMAGE PROCESS, V30, P7702, DOI 10.1109/TIP.2021.3108403
   Zang Y, 2015, IEEE T VIS COMPUT GR, V21, P1015, DOI 10.1109/TVCG.2015.2410296
   Zang Y, 2014, IEEE T VIS COMPUT GR, V20, P1253, DOI 10.1109/TVCG.2014.2298017
   Zhang FH, 2015, IEEE I CONF COMP VIS, P361, DOI 10.1109/ICCV.2015.49
   Zhang Q, 2014, LECT NOTES COMPUT SC, V8691, P815, DOI 10.1007/978-3-319-10578-9_53
   Zhu L, 2016, COMPUT GRAPH FORUM, V35, P217, DOI 10.1111/cgf.13019
NR 33
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 7
PY 2024
DI 10.1007/s00371-024-03415-1
EA MAY 2024
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QD2K1
UT WOS:001218872900004
DA 2024-08-05
ER

PT J
AU Zhao, WQ
   Zhu, JL
   Li, P
   Huang, J
   Tang, JW
AF Zhao, Wenqing
   Zhu, Jianlin
   Li, Ping
   Huang, Jin
   Tang, Junwei
TI Attention mechanism-based generative adversarial networks for image
   cartoonization
SO VISUAL COMPUTER
LA English
DT Article
DE Cartoonization; Style transfer; Generative adversarial networks;
   Attention mechanism
AB As a common art form in daily life, cartoon images play an important role in the fields of movie production and science education. However, intelligently generating different style cartoon images from real-world photographs often has a number of problems, which mainly include: (1) The generated images do not have obvious cartoon-style textures; (2) the generated images are prone to structural confusion, color artifacts, and loss of the original image content. Therefore, style transfer and preservation of original content is a great challenge in the field of image cartoonization. In this paper, we propose an attention mechanism-based generating adversarial networks for image cartoonization to address the above problem. The method uses the attention module to perform feature correction on the deep network features extracted from the residual blocks in the generative model, so that it strengthens the cartoon features of the generated image and enhances the ability of the generative model to perceive the cartoon style. At the same time, we also use the attention module to perform feature correction on the features extracted from the convolutional block in the discriminative model, so that it can strengthen the discriminative ability between the generated cartoon image and the real cartoon image while reducing the structural confusion, color artifacts, and loss of the original image content caused by style transfer. Qualitative experiments and quantitative evaluations demonstrate the advantages of our method in terms of style transfer and content preservation, while ablation study validates the role of each module in the method. The code is https://github.com/zwq11/Image-Cartoonization.git.
C1 [Zhao, Wenqing; Huang, Jin; Tang, Junwei] Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan 430200, Peoples R China.
   [Zhu, Jianlin] South Cent Minzu Univ, Dept Comp Sci, Wuhan 430070, Peoples R China.
   [Li, Ping] Hong Kong Polytech Univ, Kowloon, Hong Kong, Peoples R China.
C3 Wuhan Textile University; South Central Minzu University; Hong Kong
   Polytechnic University
RP Huang, J (corresponding author), Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan 430200, Peoples R China.
EM 864862992@qq.com; Jianlin.Zhu@mail.scuec.edu.cn; p.li@polyu.edu.hk;
   derick0320@foxmail.com; jwtang@wtu.edu.cn
CR Carion N., 2020, EUROPEAN C COMPUTER
   Chen Y, 2018, PROC CVPR IEEE, P9465, DOI 10.1109/CVPR.2018.00986
   Chen Y, 2017, IEEE IMAGE PROC, P2010, DOI 10.1109/ICIP.2017.8296634
   Cheng ZZ, 2015, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2015.55
   Cho W, 2019, PROC CVPR IEEE, P10631, DOI 10.1109/CVPR.2019.01089
   Dumoulin V, 2018, Arxiv, DOI arXiv:1603.07285
   Gatys L., 2016, CoRR, V16, P326, DOI 10.1167/16.12.326
   Gatys LA, 2015, ADV NEUR IN, V28
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guan Q A, 2018, ARXIV
   Guo HB, 2021, IEEE T CYBERNETICS, V51, P2735, DOI 10.1109/TCYB.2019.2934823
   Guo J., 2019, IEEE ICC, P1, DOI DOI 10.1109/icc.2019.8761919
   Hensel M, 2017, ADV NEUR IN, V30
   Huang S, 2023, VISUAL COMPUT, V39, P3647, DOI 10.1007/s00371-023-02938-3
   Huang XW, 2018, PROCEEDINGS OF 2018 IEEE INTERNATIONAL CONFERENCE ON INTEGRATED CIRCUITS, TECHNOLOGIES AND APPLICATIONS (ICTA 2018), P172, DOI 10.1109/CICTA.2018.8706048
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jin YX, 2021, IEEE T NEUR NET LEAR, V32, P2330, DOI 10.1109/TNNLS.2020.3004634
   Karacan L., 2016, ARXIV
   Karambakhsh A, 2024, IEEE T NEUR NET LEAR, V35, P532, DOI 10.1109/TNNLS.2022.3175775
   Larochelle H., 2010, Advances in neural information processing systems, P1243
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li HX, 2021, IEEE T IMAGE PROCESS, V30, P8526, DOI 10.1109/TIP.2021.3117061
   Li P, 2022, IEEE T NEUR NET LEAR, V33, P5346, DOI 10.1109/TNNLS.2021.3070463
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu MY, 2017, ADV NEUR IN, V30
   Mnih V., 2010, ADV NEURAL INFORM PR, V27
   Nazir A, 2022, IEEE T IMAGE PROCESS, V31, P880, DOI 10.1109/TIP.2021.3136619
   Park DY, 2019, PROC CVPR IEEE, P5873, DOI 10.1109/CVPR.2019.00603
   Qin YM, 2023, VISUAL COMPUT, V39, P3597, DOI 10.1007/s00371-023-02922-x
   Rosin P., 2012, IMAGE VIDEO BASED AR
   Shu YZ, 2022, IEEE T VIS COMPUT GR, V28, P3376, DOI 10.1109/TVCG.2021.3067201
   Wang F, 2017, IEEE ICC
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xinrui Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8087, DOI 10.1109/CVPR42600.2020.00811
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Yang H, 2020, IEEE IC COMP COM NET, DOI 10.1109/icccn49398.2020.9209630
   Yao Y, 2019, PROC CVPR IEEE, P1467, DOI 10.1109/CVPR.2019.00156
   Zhang HY, 2019, PR MACH LEARN RES, V97
   Zhang LM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275090
   Zhou Y, 2023, IEEE T NEUR NET LEAR, V34, P7719, DOI 10.1109/TNNLS.2022.3146004
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 42
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 3971
EP 3984
DI 10.1007/s00371-024-03404-4
EA APR 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001210792300002
DA 2024-08-05
ER

PT J
AU Tang, ZX
   Shen, HY
   Yu, P
   Zhang, KS
   Chen, JY
AF Tang, Zhixuan
   Shen, Haiyun
   Yu, Peng
   Zhang, Kaisong
   Chen, Jianyu
TI Infrared tracking for accurate localization by capturing global context
   information
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Thermal infrared tracking; Transformer; Convolutional projection; Global
   context
ID NETWORKS
AB The existing model's representation and fusion of features is inadequate in TIR scenarios. Our design of a new infrared tracking algorithm is based on this problem. Specifically, we use the transformer structure for feature extraction in our model, which captures global information and establishes long-distance dependencies between features. Additionally, convolutional projection is adopted to replace linear projection in the basic transformer module, which enriches local information and reduces computational complexity. For feature fusion, we associate the global spatiotemporal information between the template and search feature using the global information association module, which helps us get more accurate target positions. Finally, we utilize a new prediction head to further increase the stability of the prediction box and improve model performance. Our method achieved excellent performance on multiple infrared datasets.
C1 [Tang, Zhixuan; Shen, Haiyun; Yu, Peng; Zhang, Kaisong; Chen, Jianyu] Southwest Petr Univ, Sch Elect Informat, 8 Xindu Ave, Chengdu 610500, Sichuan, Peoples R China.
C3 Southwest Petroleum University
RP Tang, ZX (corresponding author), Southwest Petr Univ, Sch Elect Informat, 8 Xindu Ave, Chengdu 610500, Sichuan, Peoples R China.
EM 1556362667@qq.com
FU Special Project of Science and Technology Strategic Cooperation between
   Nanchong City and Southwest Petroleum University
FX No Statement Available
CR Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen CF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P347, DOI 10.1109/ICCV48922.2021.00041
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Danelljan M., 2014, BRIT MACH VIS C NOTT, DOI DOI 10.5244/C.28.65
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fan CX, 2022, VISUAL COMPUT, V38, P4291, DOI 10.1007/s00371-021-02296-y
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Forsyth A. D, 2017, Shape, contour and grouping in computer vision, P1251
   Fu ZH, 2021, PROC CVPR IEEE, P13769, DOI 10.1109/CVPR46437.2021.01356
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.129, 10.1109/ICCV.2017.128]
   Gao SJ, 2016, 2016 RESEARCH IN ADAPTIVE AND CONVERGENT SYSTEMS, P40, DOI 10.1145/2987386.2987392
   Gu FW, 2023, IEEE T AUTOM SCI ENG, DOI 10.1109/TASE.2023.3319676
   Gu FW, 2023, NEURAL COMPUT APPL, DOI 10.1007/s00521-023-08824-2
   Gundogdu Erhan, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P1, DOI 10.1109/CVPRW.2015.7301290
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Khanafer M, 2020, IEEE INSTRU MEAS MAG, V23, P10, DOI 10.1109/mim.2020.9200875
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li X, 2019, PROC CVPR IEEE, P1369, DOI 10.1109/CVPR.2019.00146
   Li X, 2019, KNOWL-BASED SYST, V166, P71, DOI 10.1016/j.knosys.2018.12.011
   Li YW, 2021, Arxiv, DOI arXiv:2104.05707
   Li Y, 2014, APPL OPTICS, V53, P6518, DOI 10.1364/AO.53.006518
   Liu Q, 2021, IEEE T MULTIMEDIA, V23, P2114, DOI 10.1109/TMM.2020.3008028
   Liu Q, 2024, IEEE T NEUR NET LEAR, V35, P9844, DOI 10.1109/TNNLS.2023.3236895
   Liu Q, 2020, AAAI CONF ARTIF INTE, V34, P11604
   Liu Q, 2020, IEEE T MULTIMEDIA, V22, P666, DOI 10.1109/TMM.2019.2932615
   Liu Q, 2017, KNOWL-BASED SYST, V134, P189, DOI 10.1016/j.knosys.2017.07.032
   Loshchilov I, 2019, Arxiv, DOI [arXiv:1711.05101, 10.48550/arXiv.1711.05101, DOI 10.48550/ARXIV.1711.05101]
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Ojha Shipra., 2015, 2015 International Conference on Pervasive Computing (ICPC), P1, DOI DOI 10.1109/PERVASIVE.2015.7087180
   Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Shirmohammadi S, 2014, IEEE INSTRU MEAS MAG, V17, P41, DOI 10.1109/MIM.2014.6825388
   Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937
   Song YB, 2017, IEEE I CONF COMP VIS, P2574, DOI 10.1109/ICCV.2017.279
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan MJ, 2019, IEEE INTERNET THINGS, V6, P9689, DOI 10.1109/JIOT.2019.2930656
   Wang N, 2021, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR46437.2021.00162
   Wang N, 2019, PROC CVPR IEEE, P1308, DOI 10.1109/CVPR.2019.00140
   Wang N, 2018, PROC CVPR IEEE, P4844, DOI 10.1109/CVPR.2018.00509
   Yang SD, 2022, VISUAL COMPUT, V38, P2107, DOI 10.1007/s00371-021-02271-7
   Yu XG, 2017, PATTERN RECOGN LETT, V100, P152, DOI 10.1016/j.patrec.2017.10.026
   Yue Wu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10183, DOI 10.1109/CVPR42600.2020.01020
   Zhang C, 2024, VISUAL COMPUT, V40, P2961, DOI 10.1007/s00371-023-02997-6
   Zhou DQ, 2021, Arxiv, DOI arXiv:2103.11886
   Zhu XZ, 2021, Arxiv, DOI [arXiv:2010.04159, 10.48550/arXiv.2010.04159]
   Ziang Ma, 2020, Proceedings of the 16th European Conference on Computer Vision (ECCV 2020) Workshops. Lecture Notes in Computer Science (LNCS 12539), P653, DOI 10.1007/978-3-030-68238-5_43
NR 60
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 4
PY 2024
DI 10.1007/s00371-024-03328-z
EA APR 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MX3A6
UT WOS:001196884200001
DA 2024-08-05
ER

PT J
AU Tran, L
   Park, DC
AF Tran, Le-Anh
   Park, Dong-Chul
TI Encoder-decoder networks with guided transmission map for effective
   image dehazing
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image dehazing; Dark channel prior; Spatial pyramid pooling; U-Net;
   Generative adversarial networks
AB A plain-architecture and effective image dehazing scheme, called Encoder-Decoder Network with Guided Transmission Map (EDN-GTM), is proposed in this paper. Nowadays, neural networks are often built based on complex architectures and modules, which inherently prevent them from being efficiently deployed on general mobile platforms that are not integrated with latest deep learning operators. Hence, from a practical point of view, plain-architecture networks would be more appropriate for implementation. To this end, we aim to develop non-sophisticated networks with effective dehazing performance. A vanilla U-Net is adopted as a starting baseline, then extensive analyses have been conducted to derive appropriate training settings and architectural features that can optimize dehazing effectiveness. As a result, several modifications are applied to the baseline such as plugging spatial pyramid pooling to the bottleneck and replacing ReLU activation with Swish activation. Moreover, we found that the transmission feature estimated by Dark Channel Prior (DCP) can be utilized as an additional prior for a generative network to recover appealing haze-free images. Experimental results on various benchmark datasets have shown that the proposed EDN-GTM scheme can achieve state-of-the-art dehazing results as compared to prevailing dehazing methods which are built upon complex architectures. In addition, the proposed EDN-GTM model can be combined with YOLOv4 to witness an improvement in object detection performance in hazy weather conditions. The code of this work is publicly available at https://github.com/tranleanh/edn-gtm.
C1 [Tran, Le-Anh; Park, Dong-Chul] Myongji Univ, Dept Elect Engn, Yongin 17058, South Korea.
C3 Myongji University
RP Park, DC (corresponding author), Myongji Univ, Dept Elect Engn, Yongin 17058, South Korea.
EM parkd@mju.ac.kr
OI Tran, Le-Anh/0000-0002-9380-7166
CR Ancuti CO, 2020, IEEE COMPUT SOC CONF, P1798, DOI 10.1109/CVPRW50498.2020.00230
   Ancuti CO, 2019, IEEE IMAGE PROC, P1014, DOI [10.1109/ICIP.2019.8803046, 10.1109/icip.2019.8803046]
   Ancuti CO, 2018, IEEE COMPUT SOC CONF, P867, DOI 10.1109/CVPRW.2018.00119
   Ancuti C, 2018, LECT NOTES COMPUT SC, V11182, P620, DOI 10.1007/978-3-030-01449-0_52
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Bui TM, 2018, IEEE T IMAGE PROCESS, V27, P999, DOI 10.1109/TIP.2017.2771158
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Chen L.-C., 2018, ECCV, P801, DOI [DOI 10.1007/978-3-030-01234-249, 10.1007/978-3-030-01234-2_49]
   Chiang C.-M., P IEEECVF C COMPUTER
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Dong Y, 2020, AAAI CONF ARTIF INTE, V34, P10729
   Engin D, 2018, IEEE COMPUT SOC CONF, P938, DOI 10.1109/CVPRW.2018.00127
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   Golts A, 2020, IEEE T IMAGE PROCESS, V29, P2692, DOI 10.1109/TIP.2019.2952032
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hong M, 2020, PROC CVPR IEEE, P3459, DOI 10.1109/CVPR42600.2020.00352
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Ju MY, 2021, IEEE T IMAGE PROCESS, V30, P2180, DOI 10.1109/TIP.2021.3050643
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Thanh LT, 2019, ASIA-PAC CONF COMMUN, P36, DOI [10.1109/APCC47188.2019.9026457, 10.1109/apcc47188.2019.9026457]
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li BY, 2021, INT J COMPUT VISION, V129, P1754, DOI 10.1007/s11263-021-01431-5
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Maas A.L., 2013, P ICML CIT, V30, P3
   Mehra A, 2021, IEEE T INTELL TRANSP, V22, P4256, DOI 10.1109/TITS.2020.3013099
   Mehta A, 2020, IEEE COMPUT SOC CONF, P846, DOI 10.1109/CVPRW50498.2020.00114
   Mehta S, 2022, Arxiv, DOI arXiv:2110.02178
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Mish Misra D., 2019, arXiv, V4, P2
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Qu YY, 2019, PROC CVPR IEEE, P8152, DOI 10.1109/CVPR.2019.00835
   Ramachandran P, 2017, Arxiv, DOI arXiv:1710.05941
   Ren WQ, 2020, INT J COMPUT VISION, V128, P240, DOI 10.1007/s11263-019-01235-8
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8
   Salazar-Colores S, 2020, IEEE ACCESS, V8, P208898, DOI 10.1109/ACCESS.2020.3038437
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun P, 2020, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR42600.2020.00252
   Tran Le-Anh, 2022, Procedia Computer Science, P682, DOI 10.1016/j.procs.2022.08.082
   Wang CY, 2020, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW50498.2020.00203
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu HY, 2021, PROC CVPR IEEE, P10546, DOI 10.1109/CVPR46437.2021.01041
   Yang Y, 2022, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR52688.2022.00208
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P1975, DOI 10.1109/TCSVT.2019.2912145
   Zhang H, 2018, IEEE COMPUT SOC CONF, P1015, DOI 10.1109/CVPRW.2018.00135
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 63
TC 1
Z9 1
U1 6
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 1
PY 2024
DI 10.1007/s00371-024-03330-5
EA APR 2024
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MP7C0
UT WOS:001194882700002
DA 2024-08-05
ER

PT J
AU Senior, H
   Slabaugh, G
   Yuan, SX
   Rossi, L
AF Senior, Henry
   Slabaugh, Gregory
   Yuan, Shanxin
   Rossi, Luca
TI Graph neural networks in vision-language image understanding: a survey
SO VISUAL COMPUTER
LA English
DT Review; Early Access
DE Graph neural networks; Image captioning; Visual question answering;
   Image retrieval
ID RETRIEVAL; KNOWLEDGE
AB 2D image understanding is a complex problem within computer vision, but it holds the key to providing human-level scene comprehension. It goes further than identifying the objects in an image, and instead, it attempts to understand the scene. Solutions to this problem form the underpinning of a range of tasks, including image captioning, visual question answering (VQA), and image retrieval. Graphs provide a natural way to represent the relational arrangement between objects in an image, and thus, in recent years graph neural networks (GNNs) have become a standard component of many 2D image understanding pipelines, becoming a core architectural component, especially in the VQA group of tasks. In this survey, we review this rapidly evolving field and we provide a taxonomy of graph types used in 2D image understanding approaches, a comprehensive list of the GNN models used in this domain, and a roadmap of future potential developments. To the best of our knowledge, this is the first comprehensive survey that covers image captioning, visual question answering, and image retrieval techniques that focus on using GNNs as the main part of their architecture.
C1 [Senior, Henry; Slabaugh, Gregory; Yuan, Shanxin] Queen Mary Univ London, Digital Environm Res Inst, New Rd, London E1 1HH, England.
   [Rossi, Luca] Hong Kong Polytech Univ, Dept Elect & Elect Engn, Hung Hom, Hong Kong, Peoples R China.
C3 University of London; Queen Mary University London; Hong Kong
   Polytechnic University
RP Senior, H (corresponding author), Queen Mary Univ London, Digital Environm Res Inst, New Rd, London E1 1HH, England.
EM h.senior@qmul.ac.uk; g.slabaugh@qmul.ac.uk; shanxin.yuan@qmul.ac.uk;
   luca.rossi@polyu.edu.hk
FU Engineering and Physical Sciences Research Council
FX No Statement Available
CR Abu-El-Haifa S, 2019, PR MACH LEARN RES, V97
   Alexander M., 2014, PREPRINT
   An D, 2024, Arxiv, DOI arXiv:2304.03047
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Auer S, 2007, LECT NOTES COMPUT SC, V4825, P722, DOI 10.1007/978-3-540-76298-0_52
   Banerjee S., 2005, P ACL WORKSHOP INTRI, P65, DOI DOI 10.3115/1626355.1626389
   Barbero Federico, 2022, Proceedings of Topological, Algebraic, and Geometric Learning Workshops 2022, volume 196 of Proceedings of Machine Learning Research, P28
   Barlas G, 2021, VISUAL COMPUT, V37, P1309, DOI 10.1007/s00371-020-01867-9
   Battaglia, 2018, ARXIV
   Bayoudh K, 2022, VISUAL COMPUT, V38, P2939, DOI 10.1007/s00371-021-02166-7
   Betker J., 2023, Computer Science
   Bicciato A., 2023, Pattern Recogn, V8, P110210
   Bigham C., 2010, P 23ND ANN ACM S USE, P333, DOI [10.1145/, DOI 10.1145/1866029]
   Bodnar C., 2022, NeurIPS
   Bodnar C, 2021, ADV NEUR IN
   Chamberlain B., 2021, INT C MACH LEARN PML, P1407
   Chamberlain BP, 2022, ARXIV
   Chang XJ, 2023, IEEE T PATTERN ANAL, V45, P1, DOI 10.1109/TPAMI.2021.3137605
   Chaudhuri Ushasi, 2022, IEEE Geoscience and Remote Sensing Letters, V19, DOI 10.1109/LGRS.2021.3056392
   Chaudhuri U, 2019, COMPUT VIS IMAGE UND, V184, P22, DOI 10.1016/j.cviu.2019.04.004
   Chen C., 2022, arXiv
   Chen DL, 2020, AAAI CONF ARTIF INTE, V34, P3438
   Chen SZ, 2021, ADV NEUR IN, V34
   Chen X., 2022, ARXIV
   Cho K., 2014, P 2014 C EMP METH NA, DOI 10.3115/v1/D14-1179
   Clipman SJ, 2022, SCI ADV, V8, DOI 10.1126/sciadv.abf0158
   Conwell C., 2022, arXiv
   Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059
   Cosmo L, 2022, INT J COMPUT VISION, V130, P1474, DOI 10.1007/s11263-022-01610-y
   Cui Z., 2022, Multimed. Tools Appl., V6, P1
   De Marneffe Marie-Catherine, 2008, COLING 2008, P1, DOI DOI 10.3115/1608858.1608859
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng Z., 2020, In NeurIPS, V33, P20660
   Dong XZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2615, DOI 10.1145/3474085.3475439
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Frasca F., 2022, arXiv
   Gao D., 2020, P IEEECVF C COMPUTER, P12746, DOI 10.1109/CVPR42600.2020.01276
   Gao F., 2022, arXiv
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Guo LT, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P765, DOI 10.1145/3343031.3350943
   Guo Z., 2022, Vis. Comput., V6, P1
   Gutierrez C, 2021, COMMUN ACM, V64, P96, DOI [10.1145/3447772, 10.1145/3418294]
   Hamilton WL, 2017, ADV NEUR IN, V30
   Han XT, 2017, IEEE I CONF COMP VIS, P1472, DOI 10.1109/ICCV.2017.163
   He S., 2020, ACCV
   Herdade S, 2019, ADV NEUR IN, V32
   Hirota Y., 2022, FACCT, P1280
   Hossain MZ, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3295748
   Hosseinabad SH, 2021, VISUAL COMPUT, V37, P119, DOI 10.1007/s00371-019-01786-4
   Iwana B.K., 2016, ARXIV
   Jain V, 2022, MULTIMED TOOLS APPL, V81, P35619, DOI 10.1007/s11042-021-11878-w
   Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990
   Kan J., 2021, ICME, P1
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Khademi M, 2020, P 58 ANN M ASS COMP, P7177, DOI DOI 10.18653/V1/2020.ACL-MAIN.643
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Kong K., 2023, INT C MACHINE LEARNI, P17375
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Laina I, 2019, IEEE I CONF COMP VIS, P7413, DOI 10.1109/ICCV.2019.00751
   Li C., 2022, arXiv
   Li CL, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P165, DOI 10.1145/2911451.2911499
   Li G, 2019, IEEE I CONF COMP VIS, P8927, DOI 10.1109/ICCV.2019.00902
   Li H., 2022, ARXIV
   Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041
   Li MJ, 2023, PROC CVPR IEEE, P3334, DOI 10.1109/CVPR52729.2023.00325
   Li MJ, 2022, PROC CVPR IEEE, P20624, DOI 10.1109/CVPR52688.2022.02000
   Li RF, 2020, NEUROCOMPUTING, V396, P92, DOI 10.1016/j.neucom.2020.02.041
   Li YJ, 2017, Arxiv, DOI arXiv:1511.05493
   Liang YY, 2021, INT C PATT RECOG, P3491, DOI 10.1109/ICPR48806.2021.9412891
   Lin C.-Y., 2004, ANN M ASS COMP LING, P74
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu C., 2019, NeurIPS, V32
   Liu H, 2004, BT TECHNOL J, V22, P211, DOI 10.1023/B:BTTJ.0000047600.45421.6d
   Liu LP, 2022, KNOWL-BASED SYST, V237, DOI 10.1016/j.knosys.2021.107650
   Liu XX, 2019, VISUAL COMPUT, V35, P445, DOI 10.1007/s00371-018-1566-y
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZY, 2023, VISUAL COMPUT, V39, P2881, DOI 10.1007/s00371-022-02499-x
   Marino K, 2019, PROC CVPR IEEE, P3190, DOI 10.1109/CVPR.2019.00331
   Misraa A.K., 2020, arXiv
   Monti F, 2018, 2018 IEEE DATA SCIENCE WORKSHOP (DSW), P225, DOI 10.1109/DSW.2018.8439897
   Morris C, 2019, AAAI CONF ARTIF INTE, P4602
   Narasimhan M, 2018, ADV NEUR IN, V31
   Nuthalapati SV, 2021, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, CIKM 2021, P3353, DOI 10.1145/3459637.3482218
   Pan H, 2022, NEUROCOMPUTING, V492, P62, DOI 10.1016/j.neucom.2022.03.071
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pennington J., 2014, P C EMP METH NAT LAN, P1532, DOI DOI 10.3115/V1/D14-1162
   Peter Young M. H., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Pradhan J, 2020, VISUAL COMPUT, V36, P1847, DOI 10.1007/s00371-019-01773-9
   Qin Y, 2020, VISUAL COMPUT, V36, P621, DOI 10.1007/s00371-019-01644-3
   Raffel C, 2020, J MACH LEARN RES, V21
   Ramesh A., 2022, ARXIV
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Romero A., 2018, INT C LEARN REPR, P2920, DOI DOI 10.48550/ARXIV.1710.10903
   Schuster Sebastian, 2015, P 4 WORKSH VIS LANG, P70, DOI DOI 10.18653/V1/W15-2812
   Sharma H, 2021, IMAGE VISION COMPUT, V110, DOI 10.1016/j.imavis.2021.104165
   Shi WJ, 2020, PROC CVPR IEEE, P1708, DOI 10.1109/CVPR42600.2020.00178
   Shopon M, 2021, VISUAL COMPUT, V37, P2713, DOI 10.1007/s00371-021-02245-9
   Singh AK, 2019, IEEE I CONF COMP VIS, P4601, DOI 10.1109/ICCV.2019.00470
   Singh A, 2019, PROC CVPR IEEE, P8309, DOI 10.1109/CVPR.2019.00851
   Song Z., 2021, ICME, P1
   Srinivasan Lakshminarasimhan., 2018, Int. J. Appl. Eng. Res., V13
   Stanovich KE, 2000, BEHAV BRAIN SCI, V23, P645, DOI 10.1017/S0140525X00003435
   Sui Jiahong, 2022, 2022 7th International Conference on Image, Vision and Computing (ICIVC), P480, DOI 10.1109/ICIVC55077.2022.9886239
   Tai KS, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1556
   Tandon N, 2014, AAAI CONF ARTIF INTE, P166
   Tang Z., 2021, IEEE J. Biomed. Health. Inf.
   Teney D, 2017, PROC CVPR IEEE, P3233, DOI 10.1109/CVPR.2017.344
   Thomas J.J., 2020, Deep Learning Techniques and Optimization Strategies in Big Data Analytics, P107
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vrandecic D, 2014, COMMUN ACM, V57, P78, DOI 10.1145/2629489
   Wang HQ, 2021, PROC CVPR IEEE, P8451, DOI 10.1109/CVPR46437.2021.00835
   Wang JB, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107075
   Wang M., 2022, IEEE Trans. Multimed.
   Wang P, 2018, IEEE T PATTERN ANAL, V40, P2413, DOI 10.1109/TPAMI.2017.2754246
   Wang Y., 2023, P IEEECVF INT C COMP, P21582
   Wang ZC, 2022, AAAI CONF ARTIF INTE, P5914
   Wang ZY, 2022, BIOINFORMATICS, V38, P2579, DOI 10.1093/bioinformatics/btac112
   Wei C, 2023, Arxiv, DOI arXiv:2311.00618
   Wu SM, 2017, CSCW'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING, P1180, DOI 10.1145/2998181.2998364
   Wu W., 2023, NEURAL COMPUT APPL, V8, P1
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Xu K, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8350934
   Xu S., 2022, arXiv
   Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41
   Yang SY, 2023, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS, VOL 2, ASPLOS 2023, P103, DOI 10.1145/3575693.3575725
   Yang X, 2023, Arxiv, DOI arXiv:2305.02177
   Yang X, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4181, DOI 10.1145/3394171.3413859
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yao T, 2019, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2019.00271
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yeyun Zou, 2020, 2020 2nd International Conference on Information Technology and Computer Application (ITCA), P289, DOI 10.1109/ITCA52113.2020.00069
   Yi HC, 2022, BRIEF BIOINFORM, V23, DOI 10.1093/bib/bbab340
   Yiwu Zhong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P211, DOI 10.1007/978-3-030-58568-6_13
   Yoon S, 2021, AAAI CONF ARTIF INTE, V35, P10718
   Yu J, 2020, PATTERN RECOGN, V108, DOI 10.1016/j.patcog.2020.107563
   Yusuf AA., 2023, Multimed. Tools Appl., V6, P1
   Yusuf AA, 2022, ARTIF INTELL REV, V55, P6277, DOI 10.1007/s10462-022-10151-2
   Zeng Y., 2022, ARXIV
   Zhang Biao, 2016, ARXIV160507869
   Zhang C., 2019, BMVC, P151, DOI [10.5244/C.33.151, DOI 10.5244/C.33.151]
   Zhang FF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3367, DOI 10.1145/3394171.3413917
   Zhang PC, 2021, PROC CVPR IEEE, P5575, DOI 10.1109/CVPR46437.2021.00553
   Zhang X, 2021, AUTOPHAGY, V17, P1519, DOI 10.1080/15548627.2020.1840796
   Zhang YX, 2020, AAAI CONF ARTIF INTE, V34, P12910
   Zhang ZL, 2020, AAAI CONF ARTIF INTE, V34, P12943
   Zhao D., 2021, P IEEECVF INT C COMP, P14830
   Zhao YS, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P4194, DOI 10.1145/3503161.3548281
   Zhong J., 2022, Vis. Comput., V6, P1
   Zhou DM, 2021, IEEE DATA MINING, P1535, DOI 10.1109/ICDM51629.2021.00201
   Zhou J, 2020, AI OPEN, V1, P57, DOI 10.1016/j.aiopen.2021.01.001
   Zhu FD, 2021, PROC CVPR IEEE, P12684, DOI 10.1109/CVPR46437.2021.01250
   Zhu ZH, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1097
NR 156
TC 0
Z9 0
U1 13
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 29
PY 2024
DI 10.1007/s00371-024-03343-0
EA MAR 2024
PG 26
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MO7Y7
UT WOS:001194640300001
OA Green Submitted, hybrid
DA 2024-08-05
ER

PT J
AU Xiao, SY
   Wang, YF
   Wang, YH
AF Xiao, Shuyu
   Wang, Yongfang
   Wang, Yihan
TI SISIM: statistical information similarity-based point cloud quality
   assessment
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Local binary patterns; Point cloud quality assessment; Statistical
   similarity; Support vector regression
ID VISUAL QUALITY; RECONSTRUCTION; SCALE
AB Due to the widespread use of point cloud, the demand for compression and transmission is more and more prominent. However, this cause various losses to the point cloud. It is necessary for application to evaluate the quality of point cloud. Therefore, we propose a new point cloud quality assessment (PCQA) metric named statistical information similarity (SISIM). First, we preprocess point cloud (PC) by scaling based on density and then project PC into texture maps and geometry maps. In addition, the SISIM based on Natural Scene Statistics (NSS) is proposed as texture features under the premise of proving that the texture maps meet NSS. Furthermore, we propose to extract geometry features based on local binary patterns (LBP) on account of the phenomenon that LBP maps of geometry images vary with different distortions. Finally, we predict the quality of PCs by fusing texture features with geometry features. Experiments show that our proposed method outperforms the state-of-the-art PCQA metrics on three publicly available datasets.
C1 [Xiao, Shuyu; Wang, Yongfang; Wang, Yihan] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
C3 Shanghai University
RP Wang, YF (corresponding author), Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
EM yfw@shu.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Alexiou E., 2019, INT WORK QUAL MULTIM, P1, DOI [DOI 10.1109/qomex.2019.8743277, 10.1109/QoMEX.2019.8743277]
   Alexiou E, 2020, IEEE INT CONF MULTI
   Alexiou E, 2018, IEEE INT CON MULTI
   Alexiou E, 2017, IEEE INT WORKSH MULT
   Bulbul A, 2011, IEEE SIGNAL PROC MAG, V28, P80, DOI 10.1109/MSP.2011.942466
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen H, 2023, J DIGIT IMAGING, V36, P932, DOI 10.1007/s10278-022-00711-x
   Chen JT, 2023, IEEE ROBOT AUTOM LET, V8, P5878, DOI 10.1109/LRA.2023.3301278
   de Queiroz RL, 2017, IEEE T IMAGE PROCESS, V26, P3886, DOI 10.1109/TIP.2017.2707807
   Diniz R, 2022, COMPUT GRAPH-UK, V103, P31, DOI 10.1016/j.cag.2022.01.003
   Diniz R, 2021, IEEE SIGNAL PROC LET, V28, P1150, DOI 10.1109/LSP.2021.3088059
   Diniz R, 2020, IEEE IMAGE PROC, P3443, DOI 10.1109/ICIP40778.2020.9190956
   Diniz R, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123076
   Flynn D., 2020, Mpeg's Pcc Metric Version 0.13.5
   Freitas XG, 2023, VISUAL COMPUT, V39, P1907, DOI 10.1007/s00371-022-02454-w
   He ZY, 2021, IEEE IMAGE PROC, P1444, DOI 10.1109/ICIP42928.2021.9506762
   Javaheri Alireza, 2017, 2017 IEEE International Conference on Multimedia and Expo: Workshops (ICMEW), P1, DOI 10.1109/ICMEW.2017.8026263
   Javaheri A, 2021, IEEE T MULTIMEDIA, V23, P4049, DOI 10.1109/TMM.2020.3037481
   Jia X, 2022, IEEE T IMAGE PROCESS, V31, P6831, DOI 10.1109/TIP.2022.3215024
   Karanwal S, 2021, 2021 IEEE 12TH ANNUAL UBIQUITOUS COMPUTING, ELECTRONICS & MOBILE COMMUNICATION CONFERENCE (UEMCON), P534, DOI 10.1109/UEMCON53757.2021.9666506
   Li CD, 2020, IEEE SENS J, V20, P5597, DOI 10.1109/JSEN.2020.2971521
   Liu Q, 2021, IEEE T CIRC SYST VID, V31, P4645, DOI 10.1109/TCSVT.2021.3100282
   Lu ZA, 2022, IEEE SIGNAL PROC LET, V29, P1804, DOI 10.1109/LSP.2022.3198601
   Meynet G, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123147
   Meynet G, 2019, INT WORK QUAL MULTIM
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Moorthy AK, 2010, INT CONF ACOUST SPEE, P962, DOI 10.1109/ICASSP.2010.5495298
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Pan YX, 2005, IEEE T MULTIMEDIA, V7, P269, DOI 10.1109/TMM.2005.843364
   Pereira F, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P7368, DOI 10.1145/3503161.3546961
   Schwarz S., 2018, ISO/IEC JTC1/SC29/WG11
   SHARIFI K, 1995, IEEE T CIRC SYST VID, V5, P52, DOI 10.1109/76.350779
   Su HL, 2019, IEEE IMAGE PROC, P3182, DOI [10.1109/icip.2019.8803298, 10.1109/ICIP.2019.8803298]
   Tian D, 2017, IEEE IMAGE PROC, P3460, DOI 10.1109/ICIP.2017.8296925
   Torlig EM, 2018, PROC SPIE, V10752, DOI 10.1117/12.2322741
   Viola I, 2020, IEEE SIGNAL PROC LET, V27, P1660, DOI 10.1109/LSP.2020.3024065
   Wang YF, 2019, NEUROCOMPUTING, V332, P298, DOI 10.1016/j.neucom.2018.12.029
   Wu XJ, 2021, IEEE T CIRC SYST VID, V31, P4630, DOI 10.1109/TCSVT.2021.3101484
   Yang Q, 2022, IEEE T PATTERN ANAL, V44, P3015, DOI 10.1109/TPAMI.2020.3047083
   Yang Q, 2021, IEEE T MULTIMEDIA, V23, P3877, DOI 10.1109/TMM.2020.3033117
   Yuan H, 2020, MOBILE NETW APPL, V25, P1863, DOI 10.1007/s11036-020-01570-y
   Zhang J, 2014, 2014 INTERNATIONAL CONFERENCE ON AUDIO, LANGUAGE AND IMAGE PROCESSING (ICALIP), VOLS 1-2, P827, DOI 10.1109/ICALIP.2014.7009910
NR 42
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 28
PY 2024
DI 10.1007/s00371-024-03352-z
EA MAR 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MO7S7
UT WOS:001194634300001
DA 2024-08-05
ER

PT J
AU Yu, Y
   Yang, Y
   Xing, JS
AF Yu, Yue
   Yang, Yue
   Xing, Jingshuo
TI PMGAN: pretrained model-based generative adversarial network for
   text-to-image generation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Pretrained model; Generative adversarial network; Text-to-image
   generation; Feature extraction
AB Text-to-image generation is a challenging task. Although diffusion models can generate high-quality images of complex scenes, they sometimes suffer from a lack of realism. Additionally, there is often a large diversity among images generated from different text with the same semantics. Furthermore, the generation of details is sometimes insufficient. Generative adversarial networks can generate realism images. These images are consistent with the text descriptions. And the networks can generate content-consistent images. In this paper, we argue that generating images that are more consistent with the text descriptions is more important than generating higher-quality images. Therefore, this paper proposes the pretrained model-based generative adversarial network (PMGAN). PMGAN utilizes multiple pre-trained models in both generator and discriminator. Specifically, in the generator, the deep attentional multimodal similarity model text encoder extracts word and sentence embeddings from the input text, and the contrastive language-image pre-training (CLIP) text encoder extracts initial image features from the input text. In the discriminator, a pre-trained CLIP image encoder extracts image features from the input image. The CLIP encoder can map text and images into a common semantic space, which is beneficial to generate high-quality images. Experimental results show that compared to the state-of-the-art methods, PMGAN achieves better scores on both inception score and Frechet inception distance and can produce higher quality images while maintaining greater consistency with text descriptions.
C1 [Yu, Yue; Yang, Yue; Xing, Jingshuo] Beijing Inst Technol, Sch Comp Sci & Technol, 5 Zhongguancun South St, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology
RP Yu, Y (corresponding author), Beijing Inst Technol, Sch Comp Sci & Technol, 5 Zhongguancun South St, Beijing 100081, Peoples R China.
EM yuyueanny@hotmail.com; yue.yang2023@outlook.com; ustb_xing@163.com
FU National Natural Science Foundation of China [61807002]; National
   Natural Science Foundation of China
FX This work was supported by the National Natural Science Foundation of
   China [Grant Numbers 61807002].
CR Chang H., 2023, arXiv
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Ding M., 2021, Advances in Neural Information Processing Systems (NeurIPS-21), V34, P19822
   Ding Ming, 2022, Advances in Neural Information Processing Systems, V35, P16890
   Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268
   Gu SY, 2022, PROC CVPR IEEE, P10686, DOI 10.1109/CVPR52688.2022.01043
   Kang MG, 2023, Arxiv, DOI arXiv:2303.05511
   Li B, 2022, arXiv
   Li XH, 2021, ISPRS J PHOTOGRAMM, V179, P14, DOI 10.1016/j.isprsjprs.2021.07.007
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Nichol A, 2022, Arxiv, DOI arXiv:2112.10741
   Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160
   Radford A, 2021, PR MACH LEARN RES, V139
   Raffel C, 2020, J MACH LEARN RES, V21
   Ramesh A., 2022, Hierarchical text-conditional image generation with clip latents, DOI 10.48550/arXiv.2204.06125
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Reed S, 2016, PR MACH LEARN RES, V48
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saharia C., 2022, ADV NEURAL INF PROCE, V35, P36479
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tao M, 2023, PROC CVPR IEEE, P14214, DOI 10.1109/CVPR52729.2023.01366
   Tao M, 2022, PROC CVPR IEEE, P16494, DOI 10.1109/CVPR52688.2022.01602
   van den Oord A, 2017, ADV NEUR IN, V30
   Vaswani A, 2017, ADV NEUR IN, V30
   Wah C., 2011, The caltech-ucsd birds-200-2011 dataset
   Wang SB, 2023, VISUAL COMPUT, V39, P6085, DOI 10.1007/s00371-022-02714-9
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yu JH, 2022, Arxiv, DOI [arXiv:2206.10789, 10.48550/arXiv.2206.10789]
   Yuan LC, 2019, IEEE ACCESS, V7, P30637, DOI 10.1109/ACCESS.2019.2903543
   Zhang H, 2021, PROC CVPR IEEE, P833, DOI 10.1109/CVPR46437.2021.00089
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang YB, 2023, VISUAL COMPUT, V39, P1283, DOI 10.1007/s00371-022-02404-6
   Zhou YF, 2022, PROC CVPR IEEE, P17886, DOI 10.1109/CVPR52688.2022.01738
   Zhu MF, 2019, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR.2019.00595
NR 37
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 28
PY 2024
DI 10.1007/s00371-024-03326-1
EA MAR 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MO7S7
UT WOS:001194634300004
DA 2024-08-05
ER

PT J
AU Yang, C
   Yang, M
   Li, HY
   Jiang, LL
   Suo, X
   Li, Z
   Meng, WL
   Mao, LJ
AF Yang, Chao
   Yang, Meng
   Li, Hongyu
   Jiang, Linlu
   Suo, Xiang
   Li, Zhen
   Meng, Weiliang
   Mao, Lijuan
TI Soccer player tracking and data correction based on attention with
   full-field videos
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Soccer player tracking; Data correction; Field mapping
ID SYSTEM
AB Tracking soccer players serves as a fundamental prerequisite for the automated analysis of soccer videos. The strides in computer vision technology have paved the way for consistently precise player tracking in soccer videos. However, it's crucial to acknowledge that real-world soccer videos often come with inherent limitations. In response to the practical demand for soccer player tracking, we propose a comprehensive pipeline. For practicality and validity of data analysis, our pipeline includes the main method and the preprocessing process; The main method is to get valid tracking data and to further apply the data, including tracking soccer players based on the attention mechanism, which we call Soccer Player TrackFormer, correction and two-dimensional mapping. The preprocessing process is to handle video concatenation and dataset creation for the sake of making up for the lack of a full-field soccer video dataset. Experiments validate that our method can reliably obtain precise tracking data, significantly augmenting the value of soccer video analysis.
C1 [Yang, Chao; Yang, Meng; Li, Hongyu; Jiang, Linlu] Beijing Forestry Univ, Sch Informat Sci & Technol, Beijing, Peoples R China.
   [Yang, Meng] Natl Forest & Grassland Adm, Engn Res Ctr Forestry Oriented Intelligent Informa, Beijing, Peoples R China.
   [Suo, Xiang; Li, Zhen; Mao, Lijuan] Shanghai Univ Sport, Sch Phys Educ, Shanghai, Peoples R China.
   [Meng, Weiliang] Chinese Acad Sci, Inst Automat, Beijing, Peoples R China.
C3 Beijing Forestry University; Shanghai University of Sport; Chinese
   Academy of Sciences; Institute of Automation, CAS
RP Yang, M (corresponding author), Beijing Forestry Univ, Sch Informat Sci & Technol, Beijing, Peoples R China.; Yang, M (corresponding author), Natl Forest & Grassland Adm, Engn Res Ctr Forestry Oriented Intelligent Informa, Beijing, Peoples R China.; Mao, LJ (corresponding author), Shanghai Univ Sport, Sch Phys Educ, Shanghai, Peoples R China.
EM yangmeng@bjfu.edu.cn; maolijuan@sus.edu.cn
RI Li, Hongyu/KGL-4818-2024
OI Li, Hongyu/0000-0002-4588-9929; SUO, Xiang/0009-0000-5899-0703; Li,
   Hongyu/0009-0008-2866-303X; YANG, Meng/0000-0001-6439-2873
FU National Natural Science Foundation of China; Open Project Program of
   State Key Laboratory of Virtual Reality Technology and Systems, Beihang
   University [VRLAB2023B01];  [62077037];  [62376231,62365014]; 
   [U22B2034];  [62262043];  [62171321];  [62162044];  [62162414]
FX This work was supported in part by the National Natural Science
   Foundation of China (62077037, 62376231,62365014, U22B2034, 62262043,
   62171321, 62162044 and 62162414), and in part by the Open Project
   Program of State Key Laboratory of Virtual Reality Technology and
   Systems, Beihang University (No. VRLAB2023B01).
CR Aharon N, 2022, Arxiv, DOI [arXiv:2206.14651, DOI 10.48550/ARXIV.2206.14651]
   Bar -Shalom Y., 1990, Tracking and data association
   Baysal S, 2016, IEEE T CIRC SYST VID, V26, P1350, DOI 10.1109/TCSVT.2015.2455713
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Ben Shitrit H, 2011, IEEE I CONF COMP VIS, P137, DOI 10.1109/ICCV.2011.6126235
   Cai JR, 2022, PROC CVPR IEEE, P8080, DOI 10.1109/CVPR52688.2022.00792
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Cioppa A, 2022, IEEE COMPUT SOC CONF, P3490, DOI 10.1109/CVPRW56347.2022.00393
   Girbau A., 2022, arXiv
   Homayounfar N, 2017, PROC CVPR IEEE, P4012, DOI 10.1109/CVPR.2017.427
   Hu MC, 2011, IEEE T MULTIMEDIA, V13, P266, DOI 10.1109/TMM.2010.2100373
   Isard M., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P343, DOI 10.1007/BFb0015549
   Li HP, 2012, INT CONF ACOUST SPEE, P1001, DOI 10.1109/ICASSP.2012.6288054
   Li JX, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22155863
   Li M, 2005, CLIN LINGUIST PHONET, V19, P545, DOI 10.1080/02699200500113616
   Li PH, 2003, IMAGE VISION COMPUT, V21, P111, DOI 10.1016/S0262-8856(02)00133-6
   Li Q, 2015, 2015 8TH INTERNATIONAL CONFERENCE ON INTELLIGENT NETWORKS AND INTELLIGENT SYSTEMS (ICINIS), P74, DOI 10.1109/ICINIS.2015.35
   Liu ZL, 2023, Arxiv, DOI arXiv:2306.05238
   Mackowiak S., 2013, Int. J. Electron. Telecomm.
   Maggiolino G, 2023, Arxiv, DOI arXiv:2302.11813
   Manafifard M, 2017, COMPUT VIS IMAGE UND, V159, P19, DOI 10.1016/j.cviu.2017.02.002
   Martín R, 2014, MULTIMED TOOLS APPL, V73, P1617, DOI 10.1007/s11042-013-1659-6
   Meinhardt T, 2022, PROC CVPR IEEE, P8834, DOI 10.1109/CVPR52688.2022.00864
   Morais E., 2012, 2012 XXV SIBGRAPI - Conference on Graphics, Patterns and Images (SIBGRAPI 2012), P174, DOI 10.1109/SIBGRAPI.2012.32
   Morais E, 2014, PATTERN RECOGN LETT, V39, P21, DOI 10.1016/j.patrec.2013.09.007
   Mori G, 2005, IEEE T PATTERN ANAL, V27, P1832, DOI 10.1109/TPAMI.2005.220
   Naik BT, 2022, IEEE ACCESS, V10, P32494, DOI 10.1109/ACCESS.2022.3161441
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Stewart R, 2016, PROC CVPR IEEE, P2325, DOI 10.1109/CVPR.2016.255
   STREIT RL, 1994, P SOC PHOTO-OPT INS, V2235, P394, DOI 10.1117/12.179066
   Veltkamp RC, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P188, DOI 10.1109/SMA.2001.923389
   Wang Y., 2020, arXiv preprint arXiv:2006.13164, V1
   Welch G. F., 2021, Computer Vision: A ReferenceGuide, P1, DOI [10.1007/978-3-030-03243-2_716-1, DOI 10.1007/978-3-030-03243-2_716-1]
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Xing JL, 2011, IEEE T IMAGE PROCESS, V20, P1652, DOI 10.1109/TIP.2010.2102045
   Yang Y, 2017, J VIS COMMUN IMAGE R, V46, P81, DOI 10.1016/j.jvcir.2017.03.008
   Yu JQ, 2018, IEEE 1ST CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2018), P418, DOI 10.1109/MIPR.2018.00090
   Yunhao D., 2023, IEEE Trans. Multim. Strongsort
   Zhang YF, 2022, LECT NOTES COMPUT SC, V13682, P1, DOI 10.1007/978-3-031-20047-2_1
   Zhang YF, 2021, INT J COMPUT VISION, V129, P3069, DOI 10.1007/s11263-021-01513-4
   Zheng B, 2022, MOB INF SYST, V2022, DOI 10.1155/2022/8090871
   Zhu XZ, 2021, Arxiv, DOI [arXiv:2010.04159, 10.48550/arXiv.2010.04159]
NR 42
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 22
PY 2024
DI 10.1007/s00371-024-03300-x
EA MAR 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LX7Z6
UT WOS:001190189400001
DA 2024-08-05
ER

PT J
AU Lone, ZA
   Pais, AR
AF Lone, Zubair Ahmad
   Pais, Alwyn Roshan
TI Salient object detection in HSI using MEV-SFS and saliency optimization
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Salient object detection; Hyperspectral images; Image analysis; Image
   processing
ID QUALITY
AB The existing methods in salient object detection (SOD) in hyperspectral images (HSI) have used different priors like center prior, boundary prior to procure cues to find the salient object. These methods fail, if the salient object is slightly touching the boundary. So, we extrapolate boundary connectivity, a measure to check if the object touches the boundary. The salient object is obtained by using background and foreground cues, which are calculated using boundary connectivity and contrast map, respectively. Also, to reduce the information redundancy and hence time complexity, we select top three most informative bands using different feature selection and feature extraction algorithms. The proposed algorithm is tested on HS-SOD dataset. It is observed that the proposed algorithm performs better than the state-of-the-art techniques in almost all the metrics, such as Precision (0.57), Recall (0.46), f1\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$f_1$$\end{document} score (0.51), CC (0.43), NSS (2.13), and MAE (0.09). In addition, we performed a comparative analysis of four different feature selection (MEV-SFS, OPBS) and feature extraction (PCA, MNF) algorithms in the context of SOD in HSI. We observed that feature selection algorithms are computationally efficient with OPBS and MEV-SFS taking about 7.98 and 8.34 s on average to reduce the feature space, respectively.
C1 [Lone, Zubair Ahmad; Pais, Alwyn Roshan] Natl Inst Technol Karnataka, Dept Comp Sci & Engn, Dakshina Kannada 575025, Karnataka, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Karnataka
RP Lone, ZA (corresponding author), Natl Inst Technol Karnataka, Dept Comp Sci & Engn, Dakshina Kannada 575025, Karnataka, India.
EM faiqzubair@gmail.com; alwyn@nitk.edu.in
CR Abdi H, 2010, WIRES COMPUT STAT, V2, P433, DOI 10.1002/wics.101
   Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Ali M, 2019, VISUAL COMPUT, V35, P1013, DOI 10.1007/s00371-019-01673-y
   Barbin DF, 2012, ANAL CHIM ACTA, V719, P30, DOI 10.1016/j.aca.2012.01.004
   Bayoudh K, 2022, VISUAL COMPUT, V38, P2939, DOI 10.1007/s00371-021-02166-7
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Das DK, 2022, VISUAL COMPUT, V38, P3803, DOI 10.1007/s00371-021-02222-2
   Devi RB, 2021, VISUAL COMPUT, V37, P1207, DOI 10.1007/s00371-020-01862-0
   Fowler JE, 2012, IEEE T IMAGE PROCESS, V21, P184, DOI 10.1109/TIP.2011.2159730
   GREEN AA, 1988, IEEE T GEOSCI REMOTE, V26, P65, DOI 10.1109/36.3001
   Huang C, 2021, AD HOC NETW, V112, DOI 10.1016/j.adhoc.2020.102369
   Huang Z., 2023, Vis. Comput, V10, P1
   Imamoglu NR, 2018, INT WORK QUAL MULTIM, P165
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Le Moan S, 2013, IEEE J-STARS, V6, P2472, DOI 10.1109/JSTARS.2013.2257989
   Liang J, 2018, PATTERN RECOGN, V76, P476, DOI 10.1016/j.patcog.2017.11.024
   Liang J, 2013, IEEE IMAGE PROC, P2393, DOI 10.1109/ICIP.2013.6738493
   Liu Z-Y., 2022, VIS COMP, V2022, P1
   Liu ZY, 2020, VISUAL COMPUT, V36, P1823, DOI 10.1007/s00371-019-01778-4
   Manolakis D, 2014, IEEE SIGNAL PROC MAG, V31, P24, DOI 10.1109/MSP.2013.2278915
   Paoletti ME, 2019, ISPRS J PHOTOGRAMM, V158, P279, DOI 10.1016/j.isprsjprs.2019.09.006
   Piqueras S, 2011, ANAL CHIM ACTA, V705, P182, DOI 10.1016/j.aca.2011.05.020
   Polder G., 2013, A spectral imaging system for detection of botrytis in greenhouses
   Srujana Oruganti Sai, 2020, 2020 Third ISEA Conference on Security and Privacy (ISEA-ISAP), P94, DOI 10.1109/ISEA-ISAP49340.2020.235006
   Wambugu N, 2021, INT J APPL EARTH OBS, V105, DOI 10.1016/j.jag.2021.102603
   Wu YH, 2023, VISUAL COMPUT, V39, P5729, DOI 10.1007/s00371-022-02692-y
   Yan HQ, 2016, INT GEOSCI REMOTE SE, P1560, DOI 10.1109/IGARSS.2016.7729398
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Zhang L, 2018, NEUROCOMPUTING, V291, P215, DOI 10.1016/j.neucom.2018.02.070
   Zhang LP, 2010, NEUROCOMPUTING, V73, P927, DOI 10.1016/j.neucom.2009.09.011
   Zhang WQ, 2018, IEEE T GEOSCI REMOTE, V56, P4318, DOI 10.1109/TGRS.2018.2811046
   Zhao ZQ, 2019, IEEE T NEUR NET LEAR, V30, P3212, DOI 10.1109/TNNLS.2018.2876865
   Zhu L, 2020, IEEE T VIS COMPUT GR, V26, P2471, DOI 10.1109/TVCG.2018.2889055
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 34
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 20
PY 2024
DI 10.1007/s00371-024-03324-3
EA MAR 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LX3V2
UT WOS:001190081000001
DA 2024-08-05
ER

PT J
AU Chen, HY
   Chen, XD
   Wu, W
   Yang, WY
   Mao, XY
AF Chen, Hongyu
   Chen, Xiao-Diao
   Wu, Wen
   Yang, Wenya
   Mao, Xiaoyang
TI Annotate less but perform better: weakly supervised shadow detection via
   label augmentation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image segmentation; Shadow detection; Weakly supervised; Graph
   convolutional network; Uncertainty analysis
AB Shadow detection is essential for scene understanding and image restoration. Existing paradigms for producing shadow detection training data usually rely on densely labeling each image pixel, which will lead to a bottleneck when scaling up the number of images. To tackle this problem, by labeling shadow images with only a few strokes, this paper designs a learning framework for Weakly supervised Shadow Detection, namely WSD. Firstly, it creates two shadow detection datasets with scribble annotations, namely Scr-SBU and Scr-ISTD. Secondly, it proposes an uncertainty-guided label augmentation scheme based on graph convolutional networks, which can propagate the sparse scribble annotations to more reliable regions, and then avoid the model converging to an undesired local minima as intra-class discontinuity. Finally, it introduces a multi-task learning framework to jointly learn for shadow detection and edge detection, which encourages generated shadow maps to be comprehensive and well aligned with shadow boundaries. Experimental results on benchmark datasets demonstrate that our framework even outperforms existing semi-supervised and fully supervised shadow detectors requiring only 2% pixels to be labeled.
C1 [Chen, Hongyu; Chen, Xiao-Diao; Wu, Wen; Yang, Wenya] Hangzhou Dianzi Univ, Sch Comp Sci, Hangzhou, Peoples R China.
   [Chen, Xiao-Diao] Xinchuang Haihe Lab, Tianjin, Peoples R China.
   [Mao, Xiaoyang] Univ Yamanashi, Dept Comp Sci & Engn, Kofu, Japan.
C3 Hangzhou Dianzi University; University of Yamanashi
RP Chen, XD; Wu, W; Yang, WY (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci, Hangzhou, Peoples R China.; Chen, XD (corresponding author), Xinchuang Haihe Lab, Tianjin, Peoples R China.
EM chenhongyu651@gmail.com; xiaodiao@hdu.edu.cn; wuwen.hdu.cs@gmail.com;
   yangwenya@hdu.edu.cn; mao@yamanashi.ac.jp
RI 杨, 文雅/GSE-4273-2022; Mao, Xiaoyang/AAG-1294-2020; WU, Wen/HKW-7234-2023
OI Mao, Xiaoyang/0000-0001-5010-6952; WU, Wen/0000-0003-0919-3948
FU National Science Foundation of China
FX No Statement Available
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Al-Amaren A, 2023, APPL INTELL, V53, P11282, DOI 10.1007/s10489-022-04062-6
   Al-Huda Z, 2023, APPL INTELL, V53, P14527, DOI 10.1007/s10489-022-04212-w
   [Anonymous], 2008, Open Signal Process. J
   Chen XD., 2023, IEEE Trans. Geosci. Remote Sens, V61, P1
   Chen ZH, 2020, PROC CVPR IEEE, P5610, DOI 10.1109/CVPR42600.2020.00565
   Codella N, 2019, Arxiv, DOI [arXiv:1902.03368, 10.48550/arXiv.1902.03368]
   Cucchiara R, 2001, 2001 IEEE INTELLIGENT TRANSPORTATION SYSTEMS - PROCEEDINGS, P334, DOI 10.1109/ITSC.2001.948679
   Ecins A, 2014, IEEE INT CONF COMPUT
   Ge Y., 2023, P AAAI C ARTIFICIAL, V37, P667
   Gulshan V, 2010, PROC CVPR IEEE, P3129, DOI 10.1109/CVPR.2010.5540073
   Guo M.-H., 2022, P ADV NEURAL INFORM, P1
   Guo RQ, 2011, PROC CVPR IEEE
   He R., 2023, AAAI, V37, P781
   Hu XW, 2021, IEEE T IMAGE PROCESS, V30, P1925, DOI 10.1109/TIP.2021.3049331
   Hu XW, 2018, PROC CVPR IEEE, P7454, DOI 10.1109/CVPR.2018.00778
   Huang X, 2011, IEEE I CONF COMP VIS, P898, DOI 10.1109/ICCV.2011.6126331
   Joshi I, 2021, IEEE WINT C APPL COM, P60, DOI 10.1109/WACVW52041.2021.00011
   Junejo IN, 2008, LECT NOTES COMPUT SC, V5302, P318, DOI 10.1007/978-3-540-88682-2_25
   Kang S, 2023, APPL INTELL, V53, P15067, DOI 10.1007/s10489-022-04269-7
   Karsch K, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024191
   Kendall A, 2017, 31 ANN C NEURAL INFO, V30
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Kittler J., 1983, Image and Vision Computing, V1, P37, DOI [DOI 10.1016/0262-8856(83)90006-9, 10.1016/0262-8856(83)90006-9]
   Krahenbuhl P., 2011, NeurIPS, V24
   Lalonde JF, 2012, INT J COMPUT VISION, V98, P123, DOI 10.1007/s11263-011-0501-8
   Lalonde JF, 2010, LECT NOTES COMPUT SC, V6312, P322, DOI 10.1007/978-3-642-15552-9_24
   Le HE, 2018, LECT NOTES COMPUT SC, V11206, P680, DOI 10.1007/978-3-030-01216-8_41
   Liu DF, 2021, AAAI CONF ARTIF INTE, V35, P6101
   Liu Y, 2019, IEEE T PATTERN ANAL, V41, P1939, DOI 10.1109/TPAMI.2018.2878849
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo DH, 2023, APPL INTELL, V53, P24739, DOI 10.1007/s10489-023-04692-4
   Meng QJ, 2019, IEEE T MED IMAGING, V38, P2755, DOI 10.1109/TMI.2019.2913311
   Mikic I, 2000, INT C PATT RECOG, P321, DOI 10.1109/ICPR.2000.905341
   Okabe T, 2009, IEEE I CONF COMP VIS, P1693
   Panagopoulos A, 2009, PROC CVPR IEEE, P651, DOI 10.1109/CVPRW.2009.5206665
   Pei JL, 2022, NEUROCOMPUTING, V507, P332, DOI 10.1016/j.neucom.2022.08.038
   Pu MY, 2022, PROC CVPR IEEE, P1392, DOI 10.1109/CVPR52688.2022.00146
   Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tang M, 2018, PROC CVPR IEEE, P1818, DOI 10.1109/CVPR.2018.00195
   Unal O, 2022, PROC CVPR IEEE, P2687, DOI 10.1109/CVPR52688.2022.00272
   Vicente T.F.Y., 2018, Large-scale weakly-supervised shadow detection
   Vicente TFY, 2018, IEEE T PATTERN ANAL, V40, P682, DOI 10.1109/TPAMI.2017.2691703
   Vincente TFY, 2016, LECT NOTES COMPUT SC, V9910, P816, DOI 10.1007/978-3-319-46466-4_49
   Nguyen V, 2017, IEEE I CONF COMP VIS, P4520, DOI 10.1109/ICCV.2017.483
   Wang JF, 2018, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR.2018.00192
   Wang Y, 2018, PROC CVPR IEEE, P4884, DOI 10.1109/CVPR.2018.00513
   Wu L, 2010, COMPUT VIS IMAGE UND, V114, P915, DOI 10.1016/j.cviu.2010.04.003
   Wu W., 2023, IEEE Transactions on Circuits and Systems for Video Technology, P1
   Wu W, 2023, KNOWL-BASED SYST, V273, DOI 10.1016/j.knosys.2023.110614
   Wu W, 2021, COMPUT GRAPH-UK, V95, P156, DOI 10.1016/j.cag.2021.02.005
   Wu W, 2022, VISUAL COMPUT, V38, P1665, DOI 10.1007/s00371-021-02095-5
   Wu W, 2022, VISUAL COMPUT, V38, P1677, DOI 10.1007/s00371-021-02096-4
   Xie EZ, 2021, ADV NEUR IN, V34
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu B., 2023, IEEE Trans. Circ. Syst. Video Technol., P1
   Yang W., 2023, Appl. Intell., P1
   Ye SQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6423, DOI 10.1109/ICCV48922.2021.00638
   Zhang BF, 2022, IEEE T PATTERN ANAL, V44, P8082, DOI 10.1109/TPAMI.2021.3083269
   Zhang J., 2020, PROC IEEECVF C COMP, P12546
   Zheng QL, 2019, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2019.00531
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhu JJ, 2010, PROC CVPR IEEE, P223, DOI 10.1109/CVPR.2010.5540209
   Zhu L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4682, DOI 10.1109/ICCV48922.2021.00466
   Zhu L, 2018, LECT NOTES COMPUT SC, V11210, P122, DOI 10.1007/978-3-030-01231-1_8
   Zhu YR, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P6717, DOI 10.1145/3503161.3547904
NR 68
TC 0
Z9 0
U1 6
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 16
PY 2024
DI 10.1007/s00371-024-03278-6
EA FEB 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8T6
UT WOS:001163171400002
DA 2024-08-05
ER

PT J
AU Pandey, A
   Kumar, P
AF Pandey, Ajeet
   Kumar, Piyush
TI Residual deep gated recurrent unit-based attention framework for human
   activity recognition by exploiting dilated features
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Dilated convolutional neural network; Gated recurrent unit; Attention
   mechanism; Action recognition; Residual mechanism
ID NETWORK; LSTM
AB Human activity recognition (HAR) in video streams becomes a thriving research area in computer vision and pattern recognition. Activity recognition in actual video is quite demanding due to a lack of data with respect to motion, way or style, and cluttered background. The current HAR approaches primarily apply pre-trained weights of various deep learning (DL) models for the apparent description of frames during the learning phase. It impacts the assessment of feature discrepancies, like the separation between both the temporal and visual cues. To address this issue, a residual deep gated recurrent unit (RD-GRU)-enabled attention framework with a dilated convolutional neural network (DiCNN) is introduced in this article. This approach particularly targets potential information in the input video frame to recognize the distinct activities in the videos. The DiCNN network is used to capture the crucial, unique features. In this network, the skip connection segment is employed with DiCNN to update the information that retains more knowledge than a shallow layer. Moreover, these features are fed into an attention module to capture the added high-level discriminative action associated with patterns and signs. The attention mechanism is followed by an RD-GRU to learn the long video sequences in order to enhance the performance. The performance metrics, namely accuracy, precision, recall, and f1-score, are used to evaluate the performance of the introduced model on four diverse benchmark datasets: UCF11, UCF Sports, JHMDB, and THUMOS. On these datasets it achieves an accuracy of 98.54%, 99.31%, 82.47%, and 95.23%, respectively. This illustrates the validity of the proposed work compared with state-of-the-art (SOTA) methods.
C1 [Pandey, Ajeet; Kumar, Piyush] Natl Inst Technol Patna, Comp Sci & Engn, Patna 800005, Bihar, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Patna
RP Pandey, A (corresponding author), Natl Inst Technol Patna, Comp Sci & Engn, Patna 800005, Bihar, India.
EM ajeetp.ph21.cs@nitp.ac.in; piyush.cs@nitp.ac.in
CR Abdelbaky A, 2021, VISUAL COMPUT, V37, P1821, DOI 10.1007/s00371-020-01940-3
   Ahmad T, 2019, IEEE ACCESS, V7, P121212, DOI 10.1109/ACCESS.2019.2937344
   [Anonymous], 2022, Multimedia Tools and Applications, P1
   Cho S, 2020, IEEE WINT CONF APPL, P624, DOI [10.1109/WACV45572.2020.9093639, 10.1109/wacv45572.2020.9093639]
   Chung JY, 2014, Arxiv, DOI [arXiv:1412.3555, DOI 10.48550/ARXIV.1412.3555]
   da Costa KAP, 2019, COMPUT NETW, V151, P147, DOI 10.1016/j.comnet.2019.01.023
   Dai C, 2020, APPL SOFT COMPUT, V86, DOI 10.1016/j.asoc.2019.105820
   Das Antar A, 2019, 2019 JOINT 8TH INTERNATIONAL CONFERENCE ON INFORMATICS, ELECTRONICS & VISION (ICIEV) AND 2019 3RD INTERNATIONAL CONFERENCE ON IMAGING, VISION & PATTERN RECOGNITION (ICIVPR) WITH INTERNATIONAL CONFERENCE ON ACTIVITY AND BEHAVIOR COMPUTING (ABC), P134, DOI [10.1109/iciev.2019.8858508, 10.1109/ICIEV.2019.8858508]
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Gammulle H, 2017, IEEE WINT CONF APPL, P177, DOI 10.1109/WACV.2017.27
   Gan C., 2019, Knowl.-Based Syst, V188, P1
   Gan CQ, 2020, KNOWL-BASED SYST, V188, DOI 10.1016/j.knosys.2019.06.035
   Garcia-Garcia A, 2018, APPL SOFT COMPUT, V70, P41, DOI 10.1016/j.asoc.2018.05.018
   Gharaee Z, 2017, APPL SOFT COMPUT, V59, P574, DOI 10.1016/j.asoc.2017.06.007
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hejazi SM, 2022, IMAGE VISION COMPUT, V123, DOI 10.1016/j.imavis.2022.104465
   Herath S, 2017, IMAGE VISION COMPUT, V60, P4, DOI 10.1016/j.imavis.2017.01.010
   Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jiang GH, 2021, APPL INTELL, V51, P7043, DOI 10.1007/s10489-021-02195-8
   Keshavarzian A, 2019, FUTURE GENER COMP SY, V101, P14, DOI 10.1016/j.future.2019.06.009
   Khan SU, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9224963
   Khodabandelou G, 2021, IEEE T AUTOM SCI ENG, V18, P495, DOI 10.1109/TASE.2020.3030852
   Kondratyuk D, 2021, PROC CVPR IEEE, P16015, DOI 10.1109/CVPR46437.2021.01576
   Kumar P., 2012, Proceedings of the 2012 1st International Conference on Recent Advances in Information Technology (RAIT 2012), P750, DOI 10.1109/RAIT.2012.6194548
   Kwon H, 2018, PATTERN RECOGN LETT, V112, P161, DOI 10.1016/j.patrec.2018.07.011
   Lee TM, 2019, IEEE T VIS COMPUT GR, V25, P1919, DOI 10.1109/TVCG.2019.2899186
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Li H, 2023, SIGNAL IMAGE VIDEO P, V17, P57, DOI 10.1007/s11760-022-02203-5
   Liu J., 2009, 2009 IEEE C COMPUTER
   Liu QL, 2019, IEEE ACCESS, V7, P82246, DOI 10.1109/ACCESS.2019.2923651
   Ma M, 2018, PATTERN RECOGN, V76, P506, DOI 10.1016/j.patcog.2017.11.026
   Majd M, 2020, NEUROCOMPUTING, V396, P224, DOI 10.1016/j.neucom.2018.10.095
   Malibari AA, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12146848
   Muhammad K, 2021, FUTURE GENER COMP SY, V125, P820, DOI 10.1016/j.future.2021.06.045
   Pandey Ajeet, 2023, Intelligent Data Engineering and Analytics: Proceedings of the 10th International Conference on Frontiers in Intelligent Computing: Theory and Applications (FICTA 2022). Smart Innovation, Systems and Technologies (327), P405, DOI 10.1007/978-981-19-7524-0_36
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Ranasinghe K, 2022, PROC CVPR IEEE, P2864, DOI 10.1109/CVPR52688.2022.00289
   Sahoo SP, 2022, DIGIT SIGNAL PROCESS, V131, DOI 10.1016/j.dsp.2022.103763
   Soomro K., 2014, Advances in Computer Vision and Pattern Recognition, V71, P181
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Tu ZG, 2018, PATTERN RECOGN, V79, P32, DOI 10.1016/j.patcog.2018.01.020
   Ullah A, 2021, APPL SOFT COMPUT, V103, DOI 10.1016/j.asoc.2021.107102
   Ullah A, 2018, IEEE ACCESS, V6, P1155, DOI 10.1109/ACCESS.2017.2778011
   Ullah H., 2022, arXiv
   Vrskova R, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12020931
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wei XS, 2019, IEEE T IMAGE PROCESS, V28, P6116, DOI 10.1109/TIP.2019.2924811
   Wu D, 2017, IEEE IJCNN, P2865, DOI 10.1109/IJCNN.2017.7966210
   Xing Z, 2023, PROC CVPR IEEE, P18816, DOI 10.1109/CVPR52729.2023.01804
   Xu J, 2021, NEUROCOMPUTING, V441, P350, DOI 10.1016/j.neucom.2020.04.150
   Yan S, 2022, PROC CVPR IEEE, P3323, DOI 10.1109/CVPR52688.2022.00333
   Yang W., 2022, Stochastic Analysis, Filtering, and Stochastic Optimization: A Commemorative Volume to Honor Mark HA Davis's Contributions, P431, DOI 10.1007/978-3-030-98519-6_18
   Yeung S, 2018, INT J COMPUT VISION, V126, P375, DOI 10.1007/s11263-017-1013-y
   Zhang CL, 2022, APPL INTELL, V52, P12771, DOI 10.1007/s10489-021-03068-w
   Zhang ZF, 2021, IEEE INTERNET THINGS, V8, P6012, DOI 10.1109/JIOT.2020.3033449
   Zhao Yuerong, 2021, Concurrency and Computation: Practice and Experience, Ve6137
   Zhen P., 2023, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems
   Zhou AH, 2023, MULTIMEDIA SYST, V29, P487, DOI 10.1007/s00530-022-00961-3
   Zhou YZ, 2018, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2018.00054
NR 62
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 6
PY 2024
DI 10.1007/s00371-024-03266-w
EA FEB 2024
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HA1I6
UT WOS:001156670000002
DA 2024-08-05
ER

PT J
AU Wang, XJ
   Guo, JC
   Wang, YD
   He, WR
AF Wang, Xiaojian
   Guo, Jichang
   Wang, Yudong
   He, Wanru
TI Jdlmask: joint defogging learning with boundary refinement for foggy
   scene instance segmentation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Foggy scene instance segmentation; Joint learning; Multi-scale feature
   fusion; Boundary refinement
AB State-of-the-art instance segmentation approaches, such as Mask R-CNN, have exhibited remarkable performance under clear weather conditions. However, their effectiveness is significantly compromised in foggy environments, primarily due to reduced visibility and obscured object details. To address this challenge, we introduce a joint defogging learning with boundary refinement (JDLMask) framework. Unlike conventional strategies that treat image dehazing as a preprocessing step, JDLMask employs a shared structure that enables the joint learning of defogging and instance segmentation. This integrated approach greatly bolsters the model's adaptability to foggy scenarios. Recognizing challenges in feature extraction due to fog interference, we propose a multi-scale feature fusion mask head, based on the encoder-decoder architecture. This component is designed to acquire both local and global information, thereby enhancing the model's feature representation capacity. Furthermore, we integrate a boundary refinement module, which sharpens the model's localization accuracy by focusing on critical boundary details. Addressing the scarcity of datasets tailored, for instance, segmentation in real-world foggy scenes, we have enriched the Foggy Driving dataset with meticulously crafted instance mask annotations and named it the Foggy Driving InstanceSeg. Comprehensive experiments demonstrate JDLMask's superiority. Compared to the baseline Mask R-CNN, JDLMask achieves improvements of 4.4% and 3.8% in mask AP on the Foggy Cityscapes and Cityscapes validation sets, respectively, and a 2.5% gain on the Foggy Driving InstanceSeg dataset.
C1 [Wang, Xiaojian; Guo, Jichang; Wang, Yudong; He, Wanru] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Guo, Jichang; Wang, Yudong] Shanghai Artificial Intelligence Lab, Shanghai 200232, Peoples R China.
C3 Tianjin University
RP Guo, JC (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.; Guo, JC (corresponding author), Shanghai Artificial Intelligence Lab, Shanghai 200232, Peoples R China.
EM xj_wang99@tju.edu.cn; jcguo@tju.edu.cn; yudongwang@tju.edu.cn;
   hewan_ru@tju.edu.cn
RI Wang, Yudong/GQY-6481-2022; Guo, Jichang/GQY-5798-2022
OI Guo, Jichang/0000-0003-3130-1685
FU National Key Research and Development Program of China
FX No Statement Available
CR Bi XL, 2023, IEEE T BIG DATA, V9, P688, DOI 10.1109/TBDATA.2022.3187413
   Bolya Daniel, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P558, DOI 10.1007/978-3-030-58580-8_33
   Cai ZW, 2021, IEEE T PATTERN ANAL, V43, P1483, DOI 10.1109/TPAMI.2019.2956516
   Chen JQ, 2023, VISUAL COMPUT, V39, P5135, DOI 10.1007/s00371-022-02650-8
   Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Gao Y, 2023, VISUAL COMPUT, V39, P1137, DOI 10.1007/s00371-021-02393-y
   Guo CL, 2022, PROC CVPR IEEE, P5802, DOI 10.1109/CVPR52688.2022.00572
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hu Q, 2023, VISUAL COMPUT, V39, P997, DOI 10.1007/s00371-021-02380-3
   Huang SC, 2021, IEEE T PATTERN ANAL, V43, P2623, DOI 10.1109/TPAMI.2020.2977911
   Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657
   Kirillov A., 2020, P IEEE CVF C COMP VI, P9799, DOI DOI 10.1109/CVPR42600.2020.00982
   Lee S, 2022, PROC CVPR IEEE, P18889, DOI 10.1109/CVPR52688.2022.01834
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li JL, 2023, IEEE WINT CONF APPL, P612, DOI 10.1109/WACV56688.2023.00068
   Li Y, 2022, AAAI CONF ARTIF INTE, P1438
   Li ZX, 2024, VISUAL COMPUT, V40, P5357, DOI 10.1007/s00371-023-03109-0
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu WY, 2022, AAAI CONF ARTIF INTE, P1792
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Minaee S, 2022, IEEE T PATTERN ANAL, V44, P3523, DOI 10.1109/TPAMI.2021.3059968
   Narasimhan S., 2008, Contrast restoration of weather degraded images, P1, DOI [10.1145/1508044.1508114, DOI 10.1145/1508044.1508114]
   Nayar S. K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P820, DOI 10.1109/ICCV.1999.790306
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8
   Shen X, 2021, PROC CVPR IEEE, P8716, DOI 10.1109/CVPR46437.2021.00861
   Sobel I., 1973, Pattern Classification and Scene Analysis, P271, DOI DOI 10.2307/2286028
   Sun YX, 2022, IEEE T CIRC SYST VID, V32, P6029, DOI 10.1109/TCSVT.2022.3155182
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tianheng Cheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P660, DOI 10.1007/978-3-030-58568-6_39
   Wang X., 2020, ARXIV
   Xinlong Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P649, DOI 10.1007/978-3-030-58523-5_38
   Yang Z, 2023, VISUAL COMPUT, V39, P3937, DOI 10.1007/s00371-022-02537-8
   Zhang G, 2021, PROC CVPR IEEE, P6857, DOI 10.1109/CVPR46437.2021.00679
   Zhang J, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P984, DOI 10.1145/3240508.3240653
   Zhang S., 2021, P MACHINE LEARNING R, P785
   Zhi Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P282, DOI 10.1007/978-3-030-58452-8_17
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 45
TC 0
Z9 0
U1 20
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 30
PY 2024
DI 10.1007/s00371-023-03230-0
EA JAN 2024
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GI6E5
UT WOS:001152070300001
DA 2024-08-05
ER

PT J
AU Gao, MJ
   Bai, Y
   Xie, YJ
   Zhang, BZ
   Li, SY
   Li, ZL
AF Gao, Meijing
   Bai, Yang
   Xie, Yunjia
   Zhang, Bozhi
   Li, Shiyu
   Li, Zhilong
TI SMC-SRGAN-Lightning super-resolution algorithm based on optical
   micro-scanning thermal microscope image
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Optical micro-scanning thermal microscope image; Super-resolution;
   Generative adversarial network; Deep learning; MobileNet
ID DEEP; NETWORK
AB Due to the low spatial resolution of the existing optical micro-scanning thermal microscope imaging system, the acquired micro-scanning infrared images have inferior image quality and low contrast. Deep learning methods, represented by SRGAN, have shown promising results in super-resolution. However, this method still has artifacts, blurriness, low spatial resolution, and slow reconstruction speed. Therefore, we propose the SMC-SRGAN-Lightning super-resolution algorithm based on optical micro-scanning thermal microscope images in this study. Firstly, we enhance the model's attention to features and improve the details and clarity of the reconstructed images. Removing the BN layer in residual blocks, replacing the ReLU with SMU, and introducing the CBAM to construct the SMC module. Secondly, we incorporate the attention mechanism SEnet into the Bottleneck structure of MobileNetV2. Reducing the channels in the first 1 x 1 convolution layer to 1/4 and creating the SE-MobileNetV2 module. It can enhance the model's focus on essential features, computational efficiency, and accuracy. Finally, to validate the effectiveness of our method, we compare it with four other super-resolution algorithms on public datasets and images obtained from the optical micro-scanning thermal microscope imaging system. Experimental results indicate that our method improves image clarity, preserving details, and textures. Comprehensively considering super-resolved image quality and time costs, our method is superior to other methods.
C1 [Gao, Meijing; Xie, Yunjia] Beijing Inst Technol, Coll Informat & Elect, Beijing 100081, Peoples R China.
   [Bai, Yang; Li, Shiyu; Li, Zhilong] Yanshan Univ, Sch Informat Sci & Engn, Key Lab Special Fiber & Fiber Sensor Hebei Prov, Qinhuangdao 066004, Peoples R China.
   [Gao, Meijing] Beijing Inst Technol, Tangshan Res Inst, Beijing, Peoples R China.
   [Zhang, Bozhi] Beijing Inst Technol, Sch Aerosp Engn, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology; Yanshan University; Beijing Institute
   of Technology; Beijing Institute of Technology
RP Gao, MJ (corresponding author), Beijing Inst Technol, Coll Informat & Elect, Beijing 100081, Peoples R China.; Gao, MJ (corresponding author), Beijing Inst Technol, Tangshan Res Inst, Beijing, Peoples R China.
EM gaomeijing@126.com; 2410086172@qq.com; 1696234762@qq.com;
   zhangbozhi93@126.com; 87837070@qq.com; 741024620@qq.com
FU National Natural Science Foundation of China [61971373]; National Nature
   Science Foundation of China [F2023105001]; Hebei Natural Science
   Foundation
FX This work was supported by National Nature Science Foundation of China
   (61971373) and Hebei Natural Science Foundation (F2023105001).
CR Biswas K, 2022, PROC CVPR IEEE, P784, DOI 10.1109/CVPR52688.2022.00087
   Chen YH, 2020, APPL OPTICS, V59, P6407, DOI 10.1364/AO.396417
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Gao MJ, 2007, 2007 IEEE/ICME INTERNATIONAL CONFERENCE ON COMPLEX MEDICAL ENGINEERING, VOLS 1-4, P1847, DOI 10.1109/ICCME.2007.4382067
   Gao MJ, 2018, INFRARED PHYS TECHN, V95, P46, DOI 10.1016/j.infrared.2018.10.013
   Gao MJ, 2017, INFRARED PHYS TECHN, V83, P252, DOI 10.1016/j.infrared.2017.05.004
   Gao MJ, 2016, INFRARED PHYS TECHN, V76, P661, DOI 10.1016/j.infrared.2016.04.034
   [高美静 Gao Meijing], 2009, [光学学报, Acta Optica Sinica], V29, P2175
   [郭莹 Guo Ying], 2017, [通信学报, Journal on Communications], V38, P142
   Huang YF, 2021, INT J INTELL SYST, V36, P2465, DOI 10.1002/int.22387
   Jing D., 2018, Environ. Chem, V37, P1440
   Katircioglu F, 2019, INFRARED PHYS TECHN, V100, P15, DOI 10.1016/j.infrared.2019.05.004
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li Q, 2021, IEEE ACCESS, V9, P9318, DOI 10.1109/ACCESS.2020.3048956
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   [刘岩 Liu Yan], 2022, [宇航计测技术, Journal of Astronautic Metrology and Measurement], V42, P73
   Meijing G., 2008, SPIE, V6621, P62117
   Ning KQ, 2021, IEEE ACCESS, V9, P85839, DOI 10.1109/ACCESS.2021.3088128
   Passarella LS, 2022, GEOPHYS RES LETT, V49, DOI 10.1029/2021GL097571
   Shao B., 2019, Research on infrared super resolution imaging and small target classification technology
   Staffa E, 2017, VASCULAR, V25, P42, DOI 10.1177/1708538116640444
   Tan A., 2022, VIS COMP, P1
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wickramasinghe CS, 2021, IEEE ACCESS, V9, P40511, DOI 10.1109/ACCESS.2021.3064819
   [岳静静 Yue Jingjing], 2017, [红外技术, Infrared Technology], V39, P973
   Zhang BZ, 2022, INFRARED PHYS TECHN, V127, DOI 10.1016/j.infrared.2022.104404
   Zhang WL, 2019, IEEE I CONF COMP VIS, P3096, DOI 10.1109/ICCV.2019.00319
NR 28
TC 0
Z9 0
U1 7
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 22
PY 2024
DI 10.1007/s00371-023-03247-5
EA JAN 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GJ3D1
UT WOS:001152252500005
DA 2024-08-05
ER

PT J
AU Zhang, YQ
   Huang, MY
   Wang, YY
   Chen, Z
   Huang, YQ
   Xiang, XS
AF Zhang, Yaqin
   Huang, Meiyu
   Wang, Yangyang
   Chen, Zhao
   Huang, Yunqing
   Xiang, Xueshuang
TI Attention-based network for passive non-light-of-sight reconstruction in
   complex scenes
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Non-line-of-sight reconstruction; Attention-based network; Complex
   Scenes
AB Passive non-line-of-sight (NLOS) reconstruction has received considerable success in diverse fields. However, the existing reconstruction methods ignore that complex scenes attenuate object-related information and view object-related information and noise in measured images as equivalent, yielding low-quality recovery. We propose an attention-based encoder-decoder (AED) network to tackle this problem. Specifically, we introduce an attention in the attention (A2B) module that can prune the attention layers to help the network focus on the object-related information in the measured images. In addition, we establish several datasets in complex scenes, including varying ambient light conditions and parameter settings of reconstruction systems, as well as complex hidden objects, to verify the generalization of our method. Experiments on our constructed datasets demonstrate that our methods achieve better recovery performance than existing methods, with more robustness to complex scenes.
C1 [Zhang, Yaqin; Huang, Meiyu; Wang, Yangyang; Xiang, Xueshuang] China Acad Space Technol, Qian Xuesen Lab Space Technol, Beijing 100094, Peoples R China.
   [Zhang, Yaqin; Huang, Yunqing] Xiangtan Univ, Sch Math & Computat Sci, Xiangtan 411105, Peoples R China.
   [Chen, Zhao] Beijing Univ Chem Technol, Coll Math & Phys, Beijing 100029, Peoples R China.
C3 Xiangtan University; Beijing University of Chemical Technology
RP Xiang, XS (corresponding author), China Acad Space Technol, Qian Xuesen Lab Space Technol, Beijing 100094, Peoples R China.; Huang, YQ (corresponding author), Xiangtan Univ, Sch Math & Computat Sci, Xiangtan 411105, Peoples R China.
EM 202131510120@smail.xtu.edu.cn; huangmeiyu2023@163.com; yywanghi@163.com;
   chenzhao2021@buct.edu.cn; huangyq@xtu.edu.cn; xiangxueshuang2023@163.com
RI Huang, Meiyu/S-6410-2018
FU Hunan Provincial Innovation Foundation for Postgraduate [CX20220646];
   Hunan Provincial Innovation Foundation for Postgraduate of FUNDER
   [11971410]; NSFC Project [2020YFA0713500]; China's National Key R D
   Programs
FX This research was funded by Hunan Provincial Innovation Foundation for
   Postgraduate of FUNDER grant number CX20220646. Huang's research was
   partially supported by NSFC Project (11971410) and China's National Key
   R &D Programs (2020YFA0713500).
CR Aistudio, 2020, Hand gesture recognition dataset
   Bouman KL, 2017, IEEE I CONF COMP VIS, P2289, DOI 10.1109/ICCV.2017.249
   Chen HY, 2021, Arxiv, DOI [arXiv:2104.09497, DOI 10.48550/ARXIV.2104.09497]
   Chen WZ, 2019, PROC CVPR IEEE, P3783, DOI 10.1109/CVPR.2019.00695
   Chen XJ, 2023, PHOTONICS-BASEL, V10, DOI 10.3390/photonics10010025
   Geng RX, 2022, IEEE T IMAGE PROCESS, V31, P110, DOI 10.1109/TIP.2021.3128312
   He JH, 2022, OPT EXPRESS, V30, P16758, DOI 10.1364/OE.455803
   Huang C., 2023, SID S DIGEST TECHNIC, V54, P321
   Isogawa M, 2020, PROC CVPR IEEE, P7011, DOI 10.1109/CVPR42600.2020.00704
   Kingma D. P., 2014, arXiv
   Klein J, 2016, SCI REP-UK, V6, DOI 10.1038/srep32491
   Krizhevsky A., 2012, Learning multiple layers of features from tiny images, DOI DOI 10.1145/3079856.3080246
   Kumar Abhishek, 2020, IEEE DataPort, DOI 10.21227/9C9B-3J44
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Liu XC, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-15157-4
   Ma J, 2021, AAAI CONF ARTIF INTE, V35, P8828
   Metzler CA, 2020, OPTICA, V7, P63, DOI 10.1364/OPTICA.374026
   Mu Fangzhou, 2022, IEEE Trans Pattern Anal Mach Intell, VPP, DOI 10.1109/TPAMI.2022.3203383
   Peng JY, 2023, IEEE T PATTERN ANAL, V45, P4180, DOI 10.1109/TPAMI.2022.3200745
   Saunders C, 2021, IEEE INT CONF COMPUT, DOI 10.1109/ICCP51581.2021.9466264
   Saunders C, 2019, NATURE, V565, P472, DOI 10.1038/s41586-018-0868-6
   Seidel SW, 2021, IEEE T COMPUT IMAG, V7, P58, DOI 10.1109/TCI.2020.3037405
   Seidel SW, 2019, IEEE INT CONF COMPUT, DOI 10.1109/iccphot.2019.8747342
   Su X, 2023, COMPUT GRAPH FORUM, DOI 10.1111/cgf.14958
   Sun L, 2019, OPT EXPRESS, V27, P33120, DOI 10.1364/OE.27.033120
   Sun YW, 2022, PHOTONICS-BASEL, V9, DOI 10.3390/photonics9080512
   Sun YW, 2019, OPT EXPRESS, V27, P16032, DOI 10.1364/OE.27.016032
   Tanaka K, 2019, Arxiv, DOI arXiv:1911.12906
   tecperson K., 2017, Sign Language MNIST: Drop-In Replacement for MNIST for Hand Gesture Recognition Tasks
   Tingyi Y., 2019, Acta Optica Sinica, V39, P0711002, DOI [10.3788/AOS201939.0711002, DOI 10.3788/AOS201939.0711002]
   Wang YY, 2021, COMMUN PHYS-UK, V4, DOI 10.1038/s42005-021-00588-2
   Wang ZY, 2023, OPT LASER ENG, V169, DOI 10.1016/j.optlaseng.2023.107701
   Willomitzer F., 2019, Computational Optical Sensing and Imaging, P2
   Wu HZ, 2022, OPT LETT, V47, P5056, DOI 10.1364/OL.471319
   Xiao H, 2017, Arxiv, DOI [arXiv:1708.07747, DOI 10.48550/ARXIV.1708.07747]
   Yedidia AB, 2019, PROC CVPR IEEE, P12223, DOI 10.1109/CVPR.2019.01251
   Zhao H., 2020, EUROPEAN C COMPUTER, P56, DOI DOI 10.1007/978-3-030-67070-23
   Zhou C, 2020, Arxiv, DOI arXiv:2005.00007
   Zhu DY, 2022, ACS PHOTONICS, V9, P2046, DOI 10.1021/acsphotonics.2c00186
   Zhu SY, 2023, PHYS REV APPL, V19, DOI 10.1103/PhysRevApplied.19.034090
NR 40
TC 0
Z9 0
U1 9
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 10
PY 2024
DI 10.1007/s00371-023-03223-z
EA JAN 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EM0J4
UT WOS:001139223000001
DA 2024-08-05
ER

PT J
AU Wu, J
   Nie, WY
   Zheng, Y
   Zuo, G
   Dong, JM
   Wei, SW
AF Wu, Jun
   Nie, Wanyu
   Zheng, Yu
   Zuo, Gan
   Dong, Jiaming
   Wei, Siwei
TI Malleable pruning meets more scaled wide-area of attention model for
   real-time crack detection
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Concrete defect images; Malleable efficient channel pruning; More scaled
   wide-area receptive field; Multi-channel fusion of spatial attention;
   Image segmentation
AB Rapid real-time detection of crack images helps prevent the emergence of more significant potential hazards. However, mature and sophisticated convolutional neural networks are more concerned with images of general everyday objects. These neural networks do not meet the real-time requirements for concrete defect detection for cracks with complex morphology and varying scales. This manuscript proposes a lightweight improvement strategy, which consists mainly of malleable efficient channel pruning, a more scaled wide-area receptive field (MSWR), and multi-channel fusion of spatial attention, referred to as MMM strategies. Firstly, the channel pruning count can intuitively make the general convolutional neural network more lightweight. Secondly, the wider receptive field can fuse multi-scale feature maps and recognize cracks of various scales. Finally, the multi-channel fusion of spatial attention enhances detection performance efficiently, ensuring real-time capability at minimal cost. The experimental results show that the lightweight network improved by the MMM strategy sacrifices no more than 8% in the detection accuracy of defects. In some cases, the detection accuracy is even improved, while the detection speed has a significant advantage. This lightweight strategy improves defect detection and has higher real-time adaptability than mainstream convolutional neural networks. The codes are available at https://github.com/mmm587/MMM.
C1 [Wu, Jun; Nie, Wanyu; Zheng, Yu; Dong, Jiaming] Hubei Univ Technol, Inst Comp Sci, Wuhan 430068, Peoples R China.
   [Zuo, Gan] IBM Consulting, Wuhan 430033, Peoples R China.
   [Wei, Siwei] Wuhan Univ Technol, Wuhan 430070, Peoples R China.
   [Wei, Siwei] CCCC Second Highway Consultants Co Ltd, Wuhan 430056, Peoples R China.
C3 Hubei University of Technology; Wuhan University of Technology
RP Wei, SW (corresponding author), Wuhan Univ Technol, Wuhan 430070, Peoples R China.; Wei, SW (corresponding author), CCCC Second Highway Consultants Co Ltd, Wuhan 430056, Peoples R China.
EM wujun@hbut.edu.cn; 102211169@hbut.edu.cn; 102311343@hbut.edu.cn;
   zuogan@cn.ibm.com; 102111098@hbut.edu.cn; weisiwei2023@gmail.com
OI Nie, Wanyu/0009-0000-9228-8037
FU the National Natural Science Foundation of China [61602161, 61772180];
   National Natural Science Foundation of China [2020BAB012]; Hubei
   Province Science and Technology Support Project [HBUT: 2021046, 21060,
   21066]; Fundamental Research Funds for the Research Fund of Hubei
   University of Technology
FX This work is supported by the National Natural Science Foundation of
   China under Grants 61602161 and 61772180, Hubei Province Science and
   Technology Support Project under Grant 2020BAB012, and the Fundamental
   Research Funds for the Research Fund of Hubei University of Technology
   under Grants HBUT: 2021046, 21060, and 21066.
CR Arya D, 2020, IEEE INT CONF BIG DA, P5533, DOI 10.1109/BigData50022.2020.9377790
   Bianchi E, 2022, J COMPUT CIVIL ENG, V36, DOI 10.1061/(ASCE)CP.1943-5487.0001045
   Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173
   Cardellicchio A, 2023, ENG FAIL ANAL, V149, DOI 10.1016/j.engfailanal.2023.107237
   Chen TI, 2023, IEEE T MULTIMEDIA, V25, P291, DOI 10.1109/TMM.2021.3125195
   Chen ZZ, 2022, PROC CVPR IEEE, P4693, DOI 10.1109/CVPR52688.2022.00466
   Dong HW, 2022, IEEE T IND INFORM, V18, P1801, DOI 10.1109/TII.2021.3090036
   Dorafshan S, 2018, DATA BRIEF, V21, P1664, DOI 10.1016/j.dib.2018.11.015
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Eisenbach M, 2017, IEEE IJCNN, P2039, DOI 10.1109/IJCNN.2017.7966101
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fang HR, 2023, ADV ENG INFORM, V55, DOI 10.1016/j.aei.2023.101890
   Gao Y, 2023, J MATER RES TECHNOL, V25, P273, DOI 10.1016/j.jmrt.2023.05.271
   Gao ZP, 2019, IEEE IJCNN, DOI [10.1109/ijcnn.2019.8851910, 10.1007/978-3-030-10797-0_1]
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hua W, 2023, THEOR APPL FRACT MEC, V124, DOI 10.1016/j.tafmec.2022.103741
   Huang WX, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3565886
   Jocher Glenn, 2022, Zenodo, DOI 10.5281/ZENODO.3908559
   Jocher Glenn., 2023, Ultralytics YOLO
   Kang CJ, 2023, STRUCT HEALTH MONIT, V22, P3320, DOI 10.1177/14759217221150376
   Kang D, 2020, AUTOMAT CONSTR, V118, DOI 10.1016/j.autcon.2020.103291
   Kheradmandi N, 2022, CONSTR BUILD MATER, V321, DOI 10.1016/j.conbuildmat.2021.126162
   Li F, 2023, PROC CVPR IEEE, P18558, DOI 10.1109/CVPR52729.2023.01780
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Li XF, 2023, ROCK MECH ROCK ENG, V56, P5029, DOI 10.1007/s00603-023-03319-x
   Liang Y, 2023, IEEE T NEUR NET LEAR, V34, P6069, DOI 10.1109/TNNLS.2021.3133127
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu YH, 2019, NEUROCOMPUTING, V338, P139, DOI 10.1016/j.neucom.2019.01.036
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2023, AUTOMAT CONSTR, V146, DOI 10.1016/j.autcon.2022.104698
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Que Y, 2023, ENG STRUCT, V277, DOI 10.1016/j.engstruct.2022.115406
   Sheta A.F., 2020, P INT C COMP SYST IT, P214
   Shi Y, 2016, IEEE T INTELL TRANSP, V17, P3434, DOI 10.1109/TITS.2016.2552248
   Tan MX, 2021, PR MACH LEARN RES, V139, P7102
   Tang YC, 2023, ENG STRUCT, V274, DOI 10.1016/j.engstruct.2022.115158
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang CY, 2020, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW50498.2020.00203
   Wu J, 2023, MULTIMED TOOLS APPL, DOI 10.1007/s11042-023-17373-8
   Wu J, 2023, APPL SCI-BASEL, V13, DOI 10.3390/app13031746
   Wu J, 2023, CONNECT SCI, V35, DOI 10.1080/09540091.2022.2155614
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Xu XY, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22031215
   Yan HT, 2023, Arxiv, DOI arXiv:2201.01615
   Yang F, 2020, IEEE T INTELL TRANSP, V21, P1525, DOI 10.1109/TITS.2019.2910595
   Yang ZW, 2023, IEEE T IMAGE PROCESS, V32, P2985, DOI 10.1109/TIP.2023.3277389
   Yao ZY, 2021, Arxiv, DOI arXiv:2104.01318
   Yu CQ, 2021, INT J COMPUT VISION, V129, P3051, DOI 10.1007/s11263-021-01515-2
   Yu Y, 2022, STRUCT HEALTH MONIT, V21, P2244, DOI 10.1177/14759217211053546
   Zhang J, 2023, COMPLEX INTELL SYST, V9, P1639, DOI 10.1007/s40747-022-00876-6
   Zhang XW, 2023, PATTERN RECOGN, V134, DOI 10.1016/j.patcog.2022.109098
   Zhang YF, 2022, NEUROCOMPUTING, V506, P146, DOI 10.1016/j.neucom.2022.07.042
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
   Zhong X, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103719
   Zhong X, 2023, IEEE T MULTIMEDIA, V25, P1979, DOI 10.1109/TMM.2022.3141886
   Zhou Y, 2023, IEEE T NEUR NET LEAR, V34, P7719, DOI 10.1109/TNNLS.2022.3146004
   Zhou Z, 2023, COMPUT-AIDED CIV INF, V38, P2491, DOI 10.1111/mice.13003
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
   Zhu YT, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15030615
   Zou Q, 2012, PATTERN RECOGN LETT, V33, P227, DOI 10.1016/j.patrec.2011.11.004
NR 65
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 17
PY 2024
DI 10.1007/s00371-024-03522-z
EA JUN 2024
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UI3S0
UT WOS:001247393800001
DA 2024-08-05
ER

PT J
AU Maack, RGC
   Raith, F
   Pérez, JF
   Scheuermann, G
   Gillmann, C
AF Maack, Robin G. C.
   Raith, Felix
   Perez, Juan F.
   Scheuermann, Gerik
   Gillmann, Christina
TI A workflow to systematically design uncertainty-aware visual analytics
   applications
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Visual analytics; Uncertainty analysis; Workflow generation
ID VISUALIZATION; PROVENANCE; FRAMEWORK
AB Visual analytics (VA) is a paradigm for insight generation by using visual analysis techniques and automated reasoning by transforming data into hypotheses and visualization to extract new insights. The insights are fed back into the data to enhance it until the desired insight is found. Many applications use this principle to provide meaningful mechanisms to assist decision-makers in achieving their goals. This process can be affected by various uncertainties that can interfere with the user decision-making process. Currently, there are no methodical description and handling tool to include uncertainty in VA systematically. We provide a unified workflow to transform the classic VA cycle into an uncertainty-aware visual analytics (UAVA) cycle consisting of five steps. To prove its usability, three real-world applications represent examples of the UAVA cycle implementation and the described workflow.
C1 [Maack, Robin G. C.] RPTU Kaiserslautern Landau, Kaiserslautern, Germany.
   [Raith, Felix; Scheuermann, Gerik] Univ Leipzig, Leipzig, Germany.
   [Perez, Juan F.] Univ Los Andes, Ctr Optimizat & Appl Probabil COPA, Bogota, Colombia.
   [Gillmann, Christina] Ctr Scalable Data Analyt & Artificial Intelligence, Leipzig, Germany.
C3 Leipzig University; Universidad de los Andes (Colombia)
RP Gillmann, C (corresponding author), Ctr Scalable Data Analyt & Artificial Intelligence, Leipzig, Germany.
EM gillmann@informatik.uni-leipzig.de
FU Federal Ministry of Education and Research of Germany; Saechsische
   Staatsministerium fur Wissenschaft Kultur und Tourismus in the program
   Center of Excellence for AI-research "Center for Scalable Data Analytics
   and Artificial Intelligence Dresden/Leipzig," [ScaDS.AI]; Projekt DEAL
FX The authors acknowledge the financial support by the Federal Ministry of
   Education and Research of Germany and by the Saechsische
   Staatsministerium fur Wissenschaft Kultur und Tourismus in the program
   Center of Excellence for AI-research "Center for Scalable Data Analytics
   and Artificial Intelligence Dresden/Leipzig," project identification
   number: ScaDS.AI. Open Access funding enabled and organized by Projekt
   DEAL.
CR Booth P, 2019, PROCEEDINGS OF THE 52ND ANNUAL HAWAII INTERNATIONAL CONFERENCE ON SYSTEM SCIENCES, P1597
   Bors C., 2019, EUROVIS WORKSH VIS A
   Cai SM, 2019, COMPUTING, V101, P1397, DOI 10.1007/s00607-018-0679-5
   Conlen M, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173712
   Correa C.D., 2009, 2009 IEEE S VIS AN S
   Cui W., 2019, Visual analytics: A comprehensive overview
   Federico P, 2016, BEYOND TIME AND ERRORS: NOVEL EVALUATION METHODS FOR VISUALIZATION, BELIV 2016, P104, DOI 10.1145/2993901.2993915
   Gerrits T, 2019, COMPUT GRAPH FORUM, V38, P325, DOI 10.1111/cgf.13692
   Ghilani C., 2004, Lecture Notes
   Gigerenzer G, 1996, MED DECIS MAKING, V16, P273, DOI 10.1177/0272989X9601600312
   Gillmann C, 2023, IEEE COMPUT GRAPH, V43, P62, DOI 10.1109/MCG.2023.3299297
   Gillmann C, 2021, IEEE COMPUT GRAPH, V41, P90, DOI 10.1109/MCG.2021.3099881
   Gillmann C, 2018, VIS INFORM, V2, P26, DOI 10.1016/j.visint2018.04.004
   Gleicher M, 2023, IEEE COMPUT GRAPH, V43, P111, DOI 10.1109/MCG.2023.3267213
   Görtler J, 2020, IEEE T VIS COMPUT GR, V26, P822, DOI 10.1109/TVCG.2019.2934812
   Griethe H., 2006, SIMVIS
   Hägele D, 2022, IT-INF TECHNOL, V64, P121, DOI 10.1515/itit-2022-0033
   Herschel M, 2017, VLDB J, V26, P881, DOI 10.1007/s00778-017-0486-1
   Huang ZS, 2020, IEEE T VIS COMPUT GR, V26, P2576, DOI 10.1109/TVCG.2019.2892483
   Kahneman D., 2011, THINKING FAST SLOW
   Karami A., 2015, CEUR WORKSHOP P
   Keim D., 2011, P 11 INT C KNOWL MAN
   Keim DA, 2008, LECT NOTES COMPUT SC, V4404, P76, DOI 10.1007/978-3-540-71080-6_6
   Kohlhammer J, 2009, NATO SCI PEACE SECUR, P299, DOI 10.1007/978-90-481-2899-0_23
   Liu SX, 2018, VIS INFORM, V2, P191, DOI 10.1016/j.visinf.2018.12.001
   Maack RGC, 2023, VISUAL COMPUT, V39, P6345, DOI 10.1007/s00371-022-02733-6
   Maack RGC, 2021, COMPUT GRAPH-UK, V98, P293, DOI 10.1016/j.cag.2021.05.011
   MacEachren AM, 2012, IEEE T VIS COMPUT GR, V18, P2496, DOI 10.1109/TVCG.2012.279
   Padilla L., 2022, Computational Statistics in Data Science
   Padilla LM, 2018, COGN RES, V3, DOI 10.1186/s41235-018-0120-9
   Pettersen EF, 2021, PROTEIN SCI, V30, P70, DOI 10.1002/pro.3943
   Piotr R., 2007, iMol Overview
   Porollo A, 2007, BMC BIOINFORMATICS, V8, DOI 10.1186/1471-2105-8-316
   Preston A, 2019, IEEE COMPUT GRAPH, V39, P72, DOI 10.1109/MCG.2019.2918158
   Ragan ED, 2016, IEEE T VIS COMPUT GR, V22, P31, DOI 10.1109/TVCG.2015.2467551
   Sacha D, 2016, IEEE T VIS COMPUT GR, V22, P240, DOI 10.1109/TVCG.2015.2467591
   Senaratne H.V., 2017, Uncertainty-aware visual analytics for spatio-temporal data exploration
   Sperling L., 2022, Uncertainty-aware evaluation of machine learning performance in binary classification tasks
   Varga M., 2016, Building Trust in Information
   Wang JP, 2019, IEEE T VIS COMPUT GR, V25, P2853, DOI 10.1109/TVCG.2018.2853721
   Wang X., 2011, Designing knowledge-assisted visual analytics systems for organizational environments
   Welle F, 2024, TRANSL STROKE RES, V15, P739, DOI 10.1007/s12975-023-01160-6
   Xu K, 2020, COMPUT GRAPH FORUM, V39, P757, DOI 10.1111/cgf.14035
   Yan L, 2020, IEEE T VIS COMPUT GR, V26, P832, DOI 10.1109/TVCG.2019.2934242
NR 44
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 7
PY 2024
DI 10.1007/s00371-024-03435-x
EA JUN 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TO4C0
UT WOS:001242180100001
OA hybrid
DA 2024-08-05
ER

PT J
AU Xiong, YG
   Wang, XK
   Xu, YR
   Zhang, YL
   Chang, J
   Zhang, JJ
   Ban, XJ
AF Xiong, Yuege
   Wang, Xiaokun
   Xu, Yanrui
   Zhang, Yalan
   Chang, Jian
   Zhang, Jianjun
   Ban, Xiaojuan
TI Dual-mechanism surface tension model for SPH-based simulation
SO VISUAL COMPUTER
LA English
DT Article
DE Surface tension; Laplace pressure; Rough surface; Interface stability
AB We present an innovative Lagrangian dual-mechanism model for simulating versatile surface tension phenomena, designed to replicate the intricate interplay of liquids with textured solid surfaces and the emergence of gas bubbles. This model synergistically merges the influence of inter-particle dynamics with global surface curvature, ensuring a harmonious balance between the intricacies of fluid motion and the imperative of surface area reduction. A cornerstone of our methodology is the incorporation of Laplace pressure differentials across fluid boundaries, enhancing interface stability and enabling the depiction of distinctive droplet oscillations driven by fluctuations in kinetic energy. Additionally, our model introduces a dual-scale smoothing kernel, meticulously engineered to resolve the subtle nuances of surface textures. The prowess of our model is exemplified in its ability to simulate superhydrophobic behaviors, underscoring its utility. Integrated within the smoothed particle hydrodynamics framework, our model offers efficient simulation performance, contributing a valuable tool to the field of fluid simulation.
C1 [Xiong, Yuege; Wang, Xiaokun; Xu, Yanrui] Univ Sci & Technol Beijing, Sch Intelligence Sci & Technol, Beijing, Peoples R China.
   [Xu, Yanrui] Univ Groningen, Groningen, Netherlands.
   [Zhang, Yalan; Ban, Xiaojuan] Univ Sci & Technol Beijing, Beijing, Peoples R China.
   [Chang, Jian; Zhang, Jianjun] Bournemouth Univ, Poole, England.
C3 University of Science & Technology Beijing; University of Groningen;
   University of Science & Technology Beijing; Bournemouth University
RP Wang, XK (corresponding author), Univ Sci & Technol Beijing, Sch Intelligence Sci & Technol, Beijing, Peoples R China.
EM banxj@ustb.edu.cn; wangxiaokun@ustb.edu.cn
OI Xu, Yanrui/0000-0002-2154-1178
FU National Key Research and Development Program of China [2022ZD0118001];
   National Key Research and Development Program of China [62376025,
   62332017, U22A2022]; National Natural Science Foundation of China
   [2023A1515030177, 2022A1515110350, 2021A1515012285]; Guangdong Basic and
   Applied Basic Research Foundation
FX This research was funded by the National Key Research and Development
   Program of China (No.2022ZD0118001), the National Natural Science
   Foundation of China (Nos.62376025, 62332017, U22A2022), and the
   Guangdong Basic and Applied Basic Research Foundation
   (Nos.2023A1515030177, 2022A1515110350, 2021A1515012285).
CR Akinci N, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508395
   Batty C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185609
   Becker M, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P209
   Bergou M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778853
   BRACKBILL JU, 1992, J COMPUT PHYS, V100, P335, DOI 10.1016/0021-9991(92)90240-Y
   Cassie ABD, 1944, T FARADAY SOC, V40, P0546, DOI 10.1039/tf9444000546
   Chen JY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459874
   Chen YL, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417859
   Clavet S., 2005, SCA 05 P 2005 ACM SI, P219, DOI DOI 10.1145/1073368.1073400
   Da F, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925899
   Da F, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2767003
   He XW, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2682630
   Hu YM, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356506
   Huber M., 2015, VRIPHYS, P41, DOI DOI 10.2312/VRIPHYS.20151333
   Hyde DAB, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417845
   Ihmsen M, 2014, IEEE T VIS COMPUT GR, V20, P426, DOI 10.1109/TVCG.2013.105
   Ishida S, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392405
   Jeske SR, 2024, ACM T GRAPHIC, V43, DOI 10.1145/3631936
   LUNDGREN TS, 1988, J FLUID MECH, V194, P479, DOI 10.1017/S0022112088003076
   MLLER M., 2003, P 2003 ACM SIGGRAPH, P154
   MONAGHAN JJ, 1992, ANNU REV ASTRON ASTR, V30, P543, DOI 10.1146/annurev.aa.30.090192.002551
   MONAGHAN JJ, 1994, J COMPUT PHYS, V110, P399, DOI 10.1006/jcph.1994.1034
   Morris JP, 2000, INT J NUMER METH FL, V33, P333, DOI 10.1002/1097-0363(20000615)33:3<333::AID-FLD11>3.0.CO;2-7
   Oger G, 2007, J COMPUT PHYS, V225, P1472, DOI 10.1016/j.jcp.2007.01.039
   Plateau JosephAntoine Ferdinand., 1873, STATIQUE EXPRIMENTAL
   Ruan LW, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459862
   Shepard D, 1968, Proceeding of ACM National Conference, P517, DOI [DOI 10.1145/800186.810616, 10.1145/800186.810616]
   Si WX, 2019, COMPUT GRAPH-UK, V81, P1, DOI 10.1016/j.cag.2019.03.015
   Solenthaler B., 2008, ACM SIGGR EUR S COMP
   Wang HQ, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417788
   Wang MD, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459864
   Wang XK, 2017, J COMPUT SCI TECH-CH, V32, P1186, DOI 10.1007/s11390-017-1793-0
   Wang XK, 2024, COMPUT VIS MEDIA, DOI 10.1007/s41095-023-0368-y
   Wendland H., 1995, Advances in Computational Mathematics, V4, P389, DOI 10.1007/BF02123482
   Wenzel RN, 1936, IND ENG CHEM, V28, P988, DOI 10.1021/ie50320a024
   Xing JR, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555476
   Yang T., 2016, P ACM SIGGRAPH EUR S, P57, DOI DOI 10.2312/SCA.20161223
   Yang T, 2017, IEEE T VIS COMPUT GR, V23, P2235, DOI 10.1109/TVCG.2017.2706289
   Zhang MY, 2010, J COMPUT PHYS, V229, P7238, DOI 10.1016/j.jcp.2010.06.010
   Zhang YZ, 2012, IEEE T VIS COMPUT GR, V18, P1281, DOI 10.1109/TVCG.2011.141
   Zhu B, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601201
   Zorilla F, 2020, COMPUTERS, V9, DOI 10.3390/computers9020023
NR 42
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4765
EP 4776
DI 10.1007/s00371-024-03474-4
EA MAY 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001232207700003
DA 2024-08-05
ER

PT J
AU Zhao, X
   Xu, FY
   Liu, Z
AF Zhao, Xun
   Xu, Feiyun
   Liu, Zheng
TI TransDehaze: transformer-enhanced texture attention for end-to-end
   single image dehaze
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Single image dehaze; Transformer; Texture; Power normalization;
   Cross-attention
ID VISION
AB Image dehazing plays a pivotal role in enhancing the performance of computer vision applications by restoring the original colors and textures of images affected by haze. Traditional dehazing methods, primarily based on atmospheric scattering models, often lead to image distortions due to imprecise parameter estimations. The emergence of deep learning has shifted the paradigm toward neural network-based approaches for image dehazing. However, these approaches, largely relying on localized convolution operations, frequently result in the loss or distortion of texture features in the dehazed images. In response to these limitations, this paper presents TransDehaze, an innovative dehazing algorithm that effectively integrates the strengths of Transformer and U-net architectures to better preserve image textures. TransDehaze leverages multiple transformer structures to extract features at various depths, subsequently fusing these features using learnable weights to accurately restore texture details. Additionally, the algorithm is enhanced with power normalization and cross-link techniques to optimize transformer's efficacy. Our comprehensive evaluation involves a range of mainstream dehazing methods and three distinct datasets: NYU-Depth V2, IST, and TOST. The results demonstrate that TransDehaze not only surpasses existing state-of-the-art dehazing algorithms in terms of dehazing quality but also significantly improves detection efficiency. An ablation study is included to highlight the individual contributions of each module within TransDehaze, illustrating its remarkable advancements in image dehazing. Our code is at https://github.com/igoindown/TransDehaze.git.
C1 [Zhao, Xun; Xu, Feiyun] Southeast Univ, Sch Mech Engn, Southeast Univ Rd 2, Nanjing 211189, Jiangsu, Peoples R China.
   [Liu, Zheng] Univ British Columbia, Sch Engn, Kelowna, BC V1V 1V7, Canada.
C3 Southeast University - China; University of British Columbia
RP Xu, FY (corresponding author), Southeast Univ, Sch Mech Engn, Southeast Univ Rd 2, Nanjing 211189, Jiangsu, Peoples R China.
EM seu_zhaoxun@seu.edu.cn; fyxu@seu.edu.cn; zheng.liu@ubc.ca
RI Xu, Feiyun/HHS-4654-2022
OI Xu, Feiyun/0000-0002-2468-2697
FU National Natural Science Foundation of China [51975117]; National
   Natural Science Foundation of China
FX This research is supported by the National Natural Science Foundation of
   China (Grant No. 51975117). The author would appreciate the anonymous
   reviewers and the editor for their valuable comments.
CR Adhikari S., 2019, 2019 ARTIFICIAL INTE
   Brown T., 2020, ADV NEURAL INFORM PR, V33, P1877, DOI DOI 10.48550/ARXIV.2005.14165
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Chen M, 2020, PR MACH LEARN RES, V119
   Chen YP, 2019, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2019.00052
   Cho KYHY, 2014, Arxiv, DOI [arXiv:1406.1078, 10.48550/arXiv.1406.1078]
   CHOI D H, 2008, 2008 16 EUR SIGN PRO, P1
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Frants V, 2023, IEEE T CYBERNETICS, V53, P5448, DOI 10.1109/TCYB.2023.3238640
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Jeong CY, 2023, J REAL-TIME IMAGE PR, V20, DOI 10.1007/s11554-023-01270-2
   Kaplan NH, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103720
   Kim TK, 1998, IEEE T CONSUM ELECTR, V44, P82, DOI 10.1109/30.663733
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Lee CH, 2013, 2013 INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY & INTERNET-BASED SYSTEMS (SITIS), P43, DOI 10.1109/SITIS.2013.19
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li RD, 2018, PROC CVPR IEEE, P8202, DOI 10.1109/CVPR.2018.00856
   Liu YH, 2019, Arxiv, DOI [arXiv:1907.11692, DOI 10.48550/ARXIV.1907.11692]
   Liu Z, 2022, PROC CVPR IEEE, P3192, DOI 10.1109/CVPR52688.2022.00320
   McCartney E. J., 1976, Optics of the atmosphere. Scattering by molecules and particles
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Nnolim U, 2008, IEEE IMTC P, P1738, DOI 10.1109/IMTC.2008.4547325
   Nosaka R, 2011, LECT NOTES COMPUT SC, V7088, P82, DOI 10.1007/978-3-642-25346-1_8
   Park D, 2014, IEEE IMAGE PROC, P4037, DOI 10.1109/ICIP.2014.7025820
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Qu YY, 2019, PROC CVPR IEEE, P8152, DOI 10.1109/CVPR.2019.00835
   Radford A., 2019, OpenAI blog, V1, P9
   Raffel C, 2023, Arxiv, DOI arXiv:1910.10683
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schonfeld E., 2020, P IEEE CVF C COMP VI, P8207
   Senthilkumar KP, 2019, L N COMPUT VIS BIOME, V31, P113, DOI 10.1007/978-3-030-04061-1_11
   Shen S., 2020, INT C MACHINE LEARNI, P8741
   Sherstinsky A, 2020, PHYSICA D, V404, DOI 10.1016/j.physd.2019.132306
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Singh D, 2019, ARCH COMPUT METHOD E, V26, P1395, DOI 10.1007/s11831-018-9294-z
   Stark JA, 2000, IEEE T IMAGE PROCESS, V9, P889, DOI 10.1109/83.841534
   Thummerer A, 2020, PHYS MED BIOL, V65, DOI 10.1088/1361-6560/abb1d6
   Tufail Z, 2019, IET IMAGE PROCESS, V13, P1161, DOI 10.1049/iet-ipr.2018.6485
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinay P, 2023, IEEE WINT CONF APPL, P548, DOI 10.1109/WACVW58289.2023.00061
   Wang Junhu, 2023, International Conference on Neural Computing for Advanced Applications: 4th International Conference, NCAA 2023, Proceedings. Communications in Computer and Information Science (1870), P116, DOI 10.1007/978-981-99-5847-4_9
   Wang J, 2023, Arxiv, DOI [arXiv:2308.10510, DOI 10.1016/J.NEUNET.2024.106281]
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Y., 2022, arXiv
   Yang D, 2018, LECT NOTES COMPUT SC, V11211, P729, DOI 10.1007/978-3-030-01234-2_43
   Yang Yuming, 2023, 2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC), P3783, DOI 10.1109/SMC53992.2023.10394653
   Yu J., 2024, IEEE Trans. Med. Imaging
   Yuan YH, 2021, Arxiv, DOI arXiv:1809.00916
   Zhai DM, 2013, IEEE DATA COMPR CONF, P103, DOI 10.1109/DCC.2013.18
   Zhang JA, 2022, IEEE T CYBERNETICS, V52, P11187, DOI 10.1109/TCYB.2021.3070310
   Zhang SY, 2019, PR MACH LEARN RES, V97
   Zhang ZX, 2018, IEEE GEOSCI REMOTE S, V15, P749, DOI 10.1109/LGRS.2018.2802944
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
   Zhu XZ, 2021, Arxiv, DOI [arXiv:2010.04159, 10.48550/arXiv.2010.04159]
NR 66
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 27
PY 2024
DI 10.1007/s00371-024-03458-4
EA MAY 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SC2Z5
UT WOS:001232207700004
DA 2024-08-05
ER

PT J
AU Zhang, C
   Yu, J
   Wan, RQ
AF Zhang, Chun
   Yu, Jian
   Wan, Ruoqing
TI An enhanced crack segmentation method using implicit classification and
   inference rules in steel bridge
SO VISUAL COMPUTER
LA English
DT Article
DE Crack segmentation; Masked image inference; Deep learning
AB In order to further improve the accuracy of crack segmentation of steel box girder bridges in complex backgrounds, a crack detection method based on information fusion combining implicit feature classification and masked image inference is proposed. Firstly, the crack segmentation model is employed to obtain the preliminary crack detection results from object recognition perspective. Then, the crack segmentation effect is enhanced with classification and inference rules for cracks learned from the training image set. In the novel masked inference module, the morphological correlation of cracks between different patches is automatically learned to provide the crack distribution probability. Meanwhile, the classification rules for cracks and distractors at the patch scale are captured and applied to suppress disturbances in classification module. Through iterative detection, inference and fusion of crack information at pixel scale and patch scale, the integrated segmentation network obtains the final crack identification results. Results from actual bridge images show that the crack detect effect of the enhanced crack detection integrated algorithm is significantly better than many classic single segmentation networks such as Deeplabv3Plus and Transformer. The average IoU, precision and recall of the steel bridge images in complex backgrounds reached 77.66%, 85.76% and 89.33%, respectively. The interference of objects such as handwriting, calibration rulers and welds can be effectively reduced.
C1 [Zhang, Chun; Yu, Jian; Wan, Ruoqing] Nanchang Univ, Sch Infrastructure Engn, Nanchang 330031, Jiangxi, Peoples R China.
C3 Nanchang University
RP Zhang, C (corresponding author), Nanchang Univ, Sch Infrastructure Engn, Nanchang 330031, Jiangxi, Peoples R China.
EM zhangchun@ncu.edu.cn
OI Chun, Zhang/0000-0001-6779-8609
FU National Natural Science Foundation of China [52268050]; National
   Natural Science Foundation of China
FX The research work was supported by the National Natural Science
   Foundation of China (No. 52268050). The authors also would like to thank
   the IPC-SHM 2020 organizing committee of Harbin Institute of Technology
   for providing bridge image data.
CR Augustauskas R, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20092557
   Cha YJ, 2017, COMPUT-AIDED CIV INF, V32, P361, DOI 10.1111/mice.12263
   Chen L, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278067
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng JR, 2018, TENCON IEEE REGION, P0462, DOI 10.1109/TENCON.2018.8650059
   Cheng KY, 2020, MULTIMED TOOLS APPL, V79, P13725, DOI 10.1007/s11042-019-08600-2
   Cui Y., 2021, PROC IEEE INT C COMP, DOI DOI 10.48550/ARXIV.2108.05821
   Dong XB, 2020, FRONT COMPUT SCI-CHI, V14, P241, DOI 10.1007/s11704-019-8208-z
   Dosovitskiy A., 2020, ARXIV, DOI DOI 10.1109/TIP.2022.3148867
   Fan Z., 2018, ARXIV, DOI DOI 10.29007/H4K6
   Gao Z, 2019, IEEE GLOB COMM CONF, DOI 10.1109/globecom38437.2019.9014164
   Guo JM, 2022, IEEE T INTELL TRANSP, V23, P7343, DOI 10.1109/TITS.2021.3069135
   Gupta P, 2022, MULTIMED TOOLS APPL, V81, P40181, DOI 10.1007/s11042-022-13152-z
   He K., 2015, ICCV, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Huang Z., 2017, P IEEE C COMPUTER VI, P4700, DOI DOI 10.1109/CVPR.2017.243
   Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657
   Huili Zhao, 2010, Proceedings of the 2010 3rd International Congress on Image and Signal Processing (CISP 2010), P964, DOI 10.1109/CISP.2010.5646923
   Inoue Y, 2019, IEEE WINT CONF APPL, P686, DOI 10.1109/WACV.2019.00078
   Jenkins MD, 2018, EUR SIGNAL PR CONF, P2120, DOI 10.23919/EUSIPCO.2018.8553280
   Ji G. P., 2023, ARXIV, DOI DOI 10.2307/J.CTVH8R4KJ.4
   Jing-Ming Guo, 2021, 2021 International Conference on System Science and Engineering (ICSSE), P442, DOI 10.1109/ICSSE52999.2021.9538477
   Ke L., 2022, P IEEECVF C COMPUTER, P4412
   Kobayashi T., 2018, AS C COMP VIS DEC, P88, DOI DOI 10.4236/JMP.2014.513121
   König J, 2019, IEEE IMAGE PROC, P1460, DOI [10.1109/ICIP.2019.8803060, 10.1109/icip.2019.8803060]
   KONIG J, 2019, 2019 27 EUROPEAN SIG, P1, DOI DOI 10.23919/EUSIPCO.2019.8902341
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kulathilake KASH, 2023, COMPLEX INTELL SYST, V9, P2713, DOI 10.1007/s40747-021-00405-x
   Kuo WC, 2019, IEEE I CONF COMP VIS, P9206, DOI 10.1109/ICCV.2019.00930
   Lau SLH, 2020, IEEE ACCESS, V8, P114892, DOI 10.1109/ACCESS.2020.3003638
   Li J, 2019, ENG STRUCT, V196, DOI 10.1016/j.engstruct.2019.109304
   Li K, 2023, IEEE T CYBERNETICS, V53, P1051, DOI 10.1109/TCYB.2021.3103885
   Li P, 2015, INT CONF INSTR MEAS, P1716, DOI 10.1109/IMCCC.2015.364
   Lin T.-Y., 2017, PROC CVPR IEEE, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu D., 2021, ARXIV, DOI DOI 10.48550/ARXIV.2103.10284
   Liu DF, 2023, IEEE T IMAGE PROCESS, V32, P2678, DOI 10.1109/TIP.2023.3272826
   Liu KH, 2022, CONSTR BUILD MATER, V317, DOI 10.1016/j.conbuildmat.2021.125917
   Liu YH, 2019, NEUROCOMPUTING, V338, P139, DOI 10.1016/j.neucom.2019.01.036
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Miao J., 2020, ARXIV, DOI DOI 10.48550/ARXIV.2003.13246
   Mosinska A, 2018, PROC CVPR IEEE, P3136, DOI 10.1109/CVPR.2018.00331
   Pan JS, 2021, IEEE T PATTERN ANAL, V43, P2449, DOI 10.1109/TPAMI.2020.2969348
   Pauly L., 2017, 34 INT S AUTOMATION, P479, DOI 10.22260/isarc2017/0066
   Peng X, 2021, CONSTR BUILD MATER, V299, DOI 10.1016/j.conbuildmat.2021.123896
   Qu Z, 2022, IEEE T INTELL TRANSP, V23, P11710, DOI 10.1109/TITS.2021.3106647
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Salehi SSM, 2017, LECT NOTES COMPUT SC, V10541, P379, DOI 10.1007/978-3-319-67389-9_44
   Salman M, 2013, IEEE INT C INTELL TR, P2039, DOI 10.1109/ITSC.2013.6728529
   Simonyan K., 2014, VERY DEEP CONVOLUTIO
   Szegedy C, 2014, Arxiv, DOI arXiv:1312.6199
   Tan M., 2019, INT C MACHINE LEARNI, P6105, DOI DOI 10.48550/ARXIV.1905.11946
   Tan R., 2020, P IEEE CVF C COMP VI, DOI [10.1109/CVPR42600.2020.01079, DOI 10.1109/CVPR42600.2020.01079]
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang KCP, 2007, TRANSPORT RES REC, P73, DOI 10.3141/2024-09
   Wang S, 2020, MACH VISION APPL, V31, DOI 10.1007/s00138-020-01114-0
   Wang W., 2022, 36 C NEUR INF PROC S, DOI DOI 10.48550/ARXIV.2210.00911
   Wang W., 2023, ARXIV, DOI DOI 10.48550/ARXIV.2209.07383
   Wu ZH, 2020, 2020 3RD INTERNATIONAL CONFERENCE ON ROBOTICS, CONTROL AND AUTOMATION ENGINEERING (RCAE 2020), P103, DOI [10.1109/RCAE51546.2020.9294343, 10.1109/rcae51546.2020.9294343]
   Xiang XZ, 2020, IET IMAGE PROCESS, V14, P1580, DOI 10.1049/iet-ipr.2019.0973
   Xie E., 2021, ADV NEURAL INF PROCE, V34, P12077
   Xie Z., 2022, ARXIV, DOI DOI 10.2307/J.CTT13X06F6.10
   Xu HY, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9142867
   Ya S, 2011, J BRIDGE ENG, V16, P492, DOI 10.1061/(ASCE)BE.1943-5592.0000181
   Yoon H., 2016, ENABLING SMART CITY, DOI [10.14359/51686286, DOI 10.14359/51686286]
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zhang KG, 2021, IEEE T INTELL TRANSP, V22, P1306, DOI 10.1109/TITS.2020.2990703
   Zhang L, 2016, IEEE IMAGE PROC, P3708, DOI 10.1109/ICIP.2016.7533052
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zou Q, 2019, IEEE T IMAGE PROCESS, V28, P1498, DOI 10.1109/TIP.2018.2878966
NR 69
TC 0
Z9 0
U1 8
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4001
EP 4021
DI 10.1007/s00371-024-03409-z
EA APR 2024
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001211242100004
DA 2024-08-05
ER

PT J
AU Liu, H
   Li, PX
   Liu, DP
   Zhang, BC
   Ren, JS
   Wang, YC
   Li, HY
   Zhang, JX
   Yang, L
   Liu, B
AF Liu, Hui
   Li, Pengxi
   Liu, Dongpei
   Zhang, Bocheng
   Ren, Jieshu
   Wang, Yichao
   Li, Hongyu
   Zhang, Jianxin
   Yang, Liang
   Liu, Bin
TI A personalized insertion centers preoperative positioning method for
   minimally invasive surgery of cruciate ligament reconstruction
SO VISUAL COMPUTER
LA English
DT Article
DE Cruciate ligament; Ligament injury; Personalized knee modeling;
   Minimally invasive ligament surgery
ID MEDIAL COLLATERAL LIGAMENT; REGISTRATION; STRATEGY; PATIENT; IMAGES
AB In the surgery of knee cruciate ligament repair, how to accurately and personalized obtain cruciate ligament insertion centers is the key issue and a great challenge for surgeons. Current artificial judgment is often with deviation, which increases the risk of surgical failure, complications, and the need for a second surgery. Surgical failure can cause imbalanced force on the knee joint (even limping). In this paper, we propose a personalized preoperative positioning method framework of cruciate ligament insertion centers. We focus on locating the insertion centers and verifying the accuracy of the results. The main steps of the method are as follows. First, we propose a new network model W-UNet to better segment accurate bone regions and small ligament regions from MRI. Second, based on RANSAC algorithm and ICP algorithm, MRI bone model and CT bone model are registered. Third, based on the extracted bone and ligament models, we propose an accurate and personalized method for locating the insertion centers. Fourth, we propose a method based on the principle of calculus, using broken lines instead of curves, to solve the problem of ligament reconstruction blocked by intercondylar protrusions. Finally, the length of cruciate ligament with different flexion angles verifies the accuracy of insertion centers. Preoperative personalized insertion centers preoperative positioning can be performed according to the patient's MRI and CT images. Validation experiments proved the accuracy and robustness of this method. Surgeons can use this framework to accurately obtain personalized preoperative insertion centers location for target patients. This framework provides a reasonable and feasible technical means for locating and marking cruciate ligament insertion centers. It effectively solves the problem of difficult ligament centers localization in clinical surgery and reduces the risk of surgical failure.
C1 [Liu, Hui; Li, Pengxi; Ren, Jieshu; Wang, Yichao; Li, Hongyu; Liu, Bin] Dalian Univ Technol, Int Sch Informat Sci & Engn DUT RUISE, Dalian, Peoples R China.
   [Liu, Hui; Li, Pengxi; Ren, Jieshu; Wang, Yichao; Li, Hongyu; Liu, Bin] Dalian Univ Technol, DUT RU Cores Ctr Adv ICT Act Life, Dalian, Peoples R China.
   [Liu, Dongpei; Zhang, Bocheng; Yang, Liang] Dalian Med Univ, Hosp 2, Dalian, Peoples R China.
   [Zhang, Jianxin] Dalian Minzu Univ, Sch Comp Sci & Engn, Dalian, Peoples R China.
   [Zhang, Jianxin] Dalian Univ, Key Lab Adv Design & Intelligent Comp, Minist Educ, Dalian, Peoples R China.
C3 Dalian University of Technology; Dalian University of Technology; Dalian
   Medical University; Dalian Minzu University; Dalian University
RP Liu, B (corresponding author), Dalian Univ Technol, Int Sch Informat Sci & Engn DUT RUISE, Dalian, Peoples R China.; Liu, B (corresponding author), Dalian Univ Technol, DUT RU Cores Ctr Adv ICT Act Life, Dalian, Peoples R China.; Yang, L (corresponding author), Dalian Med Univ, Hosp 2, Dalian, Peoples R China.
EM yangliang_dmu@sina.com; liubin@dlut.edu.cn
RI Li, Hongyu/KGL-4818-2024
OI Li, Hongyu/0000-0002-4588-9929; Liu, Bin/0000-0002-1072-6601
FU National Natural Science Foundation of China [62372079, 61972440,
   61572101]; Fundamental Research Funds for the Central Universities of
   China [DUT22YG104]; National Natural Science Foundation of Liaoning
   Province of China [2021-YGJC-23]; Scientific Research Project of
   Educational Department of Liaoning Province of China [LZ2020031]; Key
   Research and Development Projects of Liaoning Province of China
   [2021JH2/10300025]
FX This study was supported by the National Natural Science Foundation of
   China (Nos. 62372079, 61972440 and 61572101), the Fundamental Research
   Funds for the Central Universities of China (No. DUT22YG104), the
   National Natural Science Foundation of Liaoning Province of China (No.
   2021-YGJC-23), the Scientific Research Project of Educational Department
   of Liaoning Province of China (No. LZ2020031), and the Key Research and
   Development Projects of Liaoning Province of China (No.
   2021JH2/10300025). The author would like to thank the volunteers for
   providing images.
CR Azad R, 2019, IEEE INT CONF COMP V, P406, DOI 10.1109/ICCVW.2019.00052
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Campanelli V, 2020, COMP M BIO BIO E-IV, V8, P31, DOI 10.1080/21681163.2018.1559101
   Choi HF, 2014, VISUAL COMPUT, V30, P739, DOI 10.1007/s00371-014-0983-9
   Christino MA, 2015, J AM ACAD ORTHOP SUR, V23, P501, DOI 10.5435/JAAOS-D-14-00173
   Dong K, 2022, VISUAL COMPUT, V38, P51, DOI 10.1007/s00371-020-01999-y
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Flannery SW, 2022, J ORTHOP RES, V40, P277, DOI 10.1002/jor.24984
   Flannery SW, 2021, J ORTHOP RES, V39, P831, DOI 10.1002/jor.24926
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Grawe B, 2018, J AM ACAD ORTHOP SUR, V26, pE120, DOI 10.5435/JAAOS-D-16-00028
   Gudodagi R., 2013, INT J ENG COMP SCI, V2, P2033
   Gülabi D, 2014, ACTA ORTHOP TRAUMATO, V48, P231, DOI 10.3944/AOTT.2014.3149
   Hansang Lee, 2014, Journal of KIISE: Software and Applications, V41, P36
   Jacobson KE, 2006, SPORTS MED ARTHROSC, V14, P58, DOI 10.1097/01.jsa.0000212305.47323.58
   Jia H. H., 2010, 2010 4 INT C BIOINF, P2209
   Kohn D, 2000, CHIRURG, V71, P1055, DOI 10.1007/s001040051181
   Krogsgaard MR, 2011, J ELECTROMYOGR KINES, V21, P82, DOI 10.1016/j.jelekin.2010.09.012
   Lee H., 2015, P 11 AS C COMP VIS 2, VII, P305
   Lee H, 2014, COMPUT BIOL MED, V55, P1, DOI 10.1016/j.compbiomed.2014.09.004
   Lin QY, 2016, J MED SYST, V40, DOI 10.1007/s10916-016-0462-0
   Lin QY, 2015, BIOMED OPT EXPRESS, V6, P3287, DOI 10.1364/BOE.6.003287
   Lynch TS, 2015, J AM ACAD ORTHOP SUR, V23, P154, DOI 10.5435/JAAOS-D-14-00005
   Marchant BG, 2010, AM J SPORT MED, V38, P1987, DOI 10.1177/0363546510372797
   Munch DRK, 2019, KNEE SURG SPORT TR A, V27, P2672, DOI 10.1007/s00167-018-5297-4
   Prince MR, 2015, ARTHROSC TEC, V4, pE619, DOI 10.1016/j.eats.2015.06.009
   Rao ZT, 2020, KNEE SURG SPORT TR A, V28, P797, DOI 10.1007/s00167-019-05499-y
   Rasool S, 2013, VISUAL COMPUT, V29, P333, DOI 10.1007/s00371-012-0736-6
   Razali MH, 2019, PROCEEDINGS OF 2019 2ND INTERNATIONAL CONFERENCE ON ELECTRONICS AND ELECTRICAL ENGINEERING TECHNOLOGY (EEET 2019), P118, DOI 10.1145/3362752.3365196
   [容可 Rong Ke], 2014, [医用生物力学, Journal of Medical Biomechanics], V29, P339
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Strudel R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7242, DOI 10.1109/ICCV48922.2021.00717
   Sun XB., 2014, CHINA J ENDOSC, V46, P453
   Tao WY, 2024, VISUAL COMPUT, V40, P2615, DOI 10.1007/s00371-023-02942-7
   Tiamklang T, 2012, COCHRANE DB SYST REV, DOI 10.1002/14651858.CD008413.pub2
   Üzen H, 2023, VISUAL COMPUT, V39, P1745, DOI 10.1007/s00371-022-02442-0
   Vinay NA, 2014, 2014 FIFTH INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING (ICSIP 2014), P142, DOI 10.1109/ICSIP.2014.28
   Wang C, 2022, VISUAL COMPUT, V38, P4279, DOI 10.1007/s00371-021-02295-z
   Weiler A, 2007, ARTHROSCOPY, V23, P164, DOI 10.1016/j.arthro.2006.09.008
   Yang DZ, 2024, VISUAL COMPUT, V40, P1095, DOI 10.1007/s00371-023-02834-w
   Zarychta P, 2014, 2014 PROCEEDINGS OF THE 21ST INTERNATIONAL CONFERENCE ON MIXED DESIGN OF INTEGRATED CIRCUITS & SYSTEMS (MIXDES), P489, DOI 10.1109/MIXDES.2014.6872248
   Zheng JJ, 2015, SIGNAL IMAGE VIDEO P, V9, P1815, DOI 10.1007/s11760-014-0660-5
   Zheng LX, 2021, IEEE T IND ELECTRON, V68, P2368, DOI 10.1109/TIE.2020.2973893
NR 44
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 3937
EP 3960
DI 10.1007/s00371-024-03399-y
EA APR 2024
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001204140500001
DA 2024-08-05
ER

PT J
AU Chen, JG
   Zhang, X
   Ma, LL
   Yang, B
   Zhang, KB
AF Chen, Jinguang
   Zhang, Xin
   Ma, Lili
   Yang, Bo
   Zhang, Kaibing
TI CS-VITON: a realistic virtual try-on network based on clothing region
   alignment and SPM
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Virtual try-on; Image generation; Feature fusion; Deep learning;
   Clothing region alignment; SPM
AB Image-based virtual try-on involves generating an image of a person wearing a given clothing. Existing virtual try-on works suffer from the problem of misaligned regions between the predicted segmentation map and the deformed clothing, and the generation results of try-on are unnatural. To address this issue, we refine the definition of the misaligned regions and propose a high-resolution virtual try-on network called CS-VITON. The network adopts a two-stage strategy. The first stage is called the condition generator, which predicts the target segmentation map while deforming the clothing into shapes that match the human body. A component that measures the difference between the generated segmentation maps and the mask of deformed clothing is added to the loss function of the deep network. The component is well matched with the tasks of this stage, resulting in more reasonable necklines and skin boundaries. The second stage is called the try-on generator, in which the process of generating try-on images is modulated using residual blocks constructed based on style-preserved modulation. The modulation process takes into account the specific contextual style of the image, which improves the realism of the try-on results. Extensive experiments were conducted on a common high-resolution virtual try-on dataset, demonstrating that that our method yields more realistic virtual try-on results. Metrics such as kernel inception distance also showed some improvement. The code will be available soon at https://github.com/xinz626/CS-VITON.
C1 [Chen, Jinguang; Zhang, Xin; Ma, Lili; Yang, Bo; Zhang, Kaibing] Xian Polytech Univ, Sch Comp Sci, Shaanxi Key Lab Clothing Intelligence, Xian 710048, Shaanxi, Peoples R China.
C3 Xi'an Polytechnic University
RP Ma, LL (corresponding author), Xian Polytech Univ, Sch Comp Sci, Shaanxi Key Lab Clothing Intelligence, Xian 710048, Shaanxi, Peoples R China.
EM xacjg@163.com; azure_x2021@163.com; xamll@163.com;
   yangboo@stu.xjtu.edu.cn; zhangkaibing@xpu.edu.cn
FU National Natural Science Foundation of China [61971339]; National
   Natural Science Foundation of China [2023-JC-YB-826]; Natural Science
   Basic Research Program of Shaanxi [22JP028]; Shaanxi Provincial
   Education Department [XT-QC-202309-119287]; Joint Foundation of Shaanxi
   Computer Society [2020100]; Technology Guidance Program of China
   National Textile and Apparel Council
FX This work was supported by the National Natural Science Foundation of
   China (No. 61971339), the Natural Science Basic Research Program of
   Shaanxi (No. 2023-JC-YB-826), the Scientific Research Program Funded by
   Shaanxi Provincial Education Department (No. 22JP028), the Joint
   Foundation of Shaanxi Computer Society and Xi'an Xiangteng
   Microelectronics Technology Co., Ltd (No. XT-QC-202309-119287), and the
   Technology Guidance Program of China National Textile and Apparel
   Council (No. 2020100).
CR Baldrati A, 2023, Arxiv, DOI arXiv:2304.02051
   Bertiche Hugo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P344, DOI 10.1007/978-3-030-58565-5_21
   Bińkowski M, 2021, Arxiv, DOI [arXiv:1801.01401, 10.48550/arXiv.1801.0140132, 10.48550/arXiv.1801.01401]
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen C.-Y., 2023, P IEEECVF INT C COMP, P7513
   Choi S, 2021, PROC CVPR IEEE, P14126, DOI 10.1109/CVPR46437.2021.01391
   Farooqi N., 2019, Life Science Journal, V16, P11
   Gong K, 2018, LECT NOTES COMPUT SC, V11208, P805, DOI 10.1007/978-3-030-01225-0_47
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Han XT, 2019, IEEE I CONF COMP VIS, P10470, DOI 10.1109/ICCV.2019.01057
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Han Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7847, DOI 10.1109/CVPR42600.2020.00787
   Hensel M, 2017, ADV NEUR IN, V30
   Lim JH, 2017, Arxiv, DOI [arXiv:1705.02894, 10.48550/arXiv.1705.02894, DOI 10.48550/ARXIV.1705.02894]
   Jetchev N, 2017, IEEE INT CONF COMP V, P2287, DOI 10.1109/ICCVW.2017.269
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kang M, 2023, PROC CVPR IEEE, P10124, DOI 10.1109/CVPR52729.2023.00976
   Kingma D. P., 2014, arXiv
   Lee S, 2022, LECT NOTES COMPUT SC, V13677, P204, DOI 10.1007/978-3-031-19790-1_13
   Li PK, 2022, IEEE T PATTERN ANAL, V44, P3260, DOI 10.1109/TPAMI.2020.3048039
   Luo WY, 2022, LECT NOTES COMPUT SC, V13677, P561, DOI 10.1007/978-3-031-19790-1_34
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Salimans T, 2016, ADV NEUR IN, V29
   Santesteban I, 2021, PROC CVPR IEEE, P11758, DOI 10.1109/CVPR46437.2021.01159
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh A, 2022, ARAB J SCI ENG, V47, P9801, DOI 10.1007/s13369-021-06348-2
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang W, 2023, IEEE T MULTIMEDIA, V25, P5413, DOI 10.1109/TMM.2022.3192661
   Tang W, 2022, IEEE T IMAGE PROCESS, V31, P5134, DOI 10.1109/TIP.2022.3193288
   Wang BC, 2018, LECT NOTES COMPUT SC, V11217, P607, DOI 10.1007/978-3-030-01261-8_36
   Wang KK, 2023, VISUAL COMPUT, V39, P429, DOI 10.1007/s00371-021-02339-4
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu HR, 2022, INTEGR COMPUT-AID E, V29, P141, DOI 10.3233/ICA-210672
   Xu J, 2023, VISUAL COMPUT, V39, P2005, DOI 10.1007/s00371-022-02460-y
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang SY, 2022, IEEE T IMAGE PROCESS, V31, P5599, DOI 10.1109/TIP.2022.3192989
   Zhao FW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13219, DOI 10.1109/ICCV48922.2021.01299
   Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18
NR 46
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 28
PY 2024
DI 10.1007/s00371-024-03347-w
EA MAR 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MO7S7
UT WOS:001194634300002
DA 2024-08-05
ER

PT J
AU Lyu, L
   Cao, W
   Ren, XH
   Wu, EH
   Yang, ZX
AF Lyu, Luan
   Cao, Wei
   Ren, Xiaohua
   Wu, Enhua
   Yang, Zhi-Xin
TI Efficient odd-even multigrid for pointwise incompressible fluid
   simulation on GPU
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Fluid simulation; Divergence-free; Pointwise incompressibility;
   Multigrid; GPU
AB Fluid simulation is a well-established research field in computer graphics that aims to generate realistic and visually appealing simulations of fluids, including water, smoke, and fire. Divergence is identified as a crucial factor in achieving realistic fluid behavior. This article presents a GPU optimization of the pointwise incompressible fluid simulation technique, which uses pointwise incompressible velocity interpolation to ensure divergence at arbitrary points, rather than grid cells. Pointwise incompressible velocity interpolation at arbitrary points is the curl of velocity potential. The velocity potential can be obtained from the divergence-free velocities by solving a Poisson equation. To enhance the computational efficiency of the simulation, an odd-even geometric multigrid method is introduced to solve Poisson's equation on non-power-of-2 resolution grids. Additionally, to recover the potential on grid cells, a warp-level method is proposed via divergence-free velocities on grid cells. The proposed warp-level potential recovery method offers GPU optimization and enhanced efficiency, in contrast to the sweeping method that only provides CPU optimization for potential recovery. Furthermore, a locally enhanced method is proposed to recover the detailed velocity potential within small domains through the use of the odd-even multigrid. The experiments show that the odd-even multigrid outperforms Weber's multigrid by an average speedup factor of 4.32 across various grid sizes. On the other hand, the warp-level potential recovery method demonstrates notable speedups ranging from approximately 2-6 times for 2D grids and about 1.35-2.64 times for 3D grids when compared to the parallel sweeping method on GPU. The results of these contributions demonstrate remarkable improvements in the efficiency and performance of fluid simulations in computer graphics, harnessing the computational capabilities of modern GPUs to achieve visually appealing and realistic fluid behaviors.
C1 [Lyu, Luan; Yang, Zhi-Xin] Univ Macau, State Key Lab Internet Things Smart City, Macau 999078, Peoples R China.
   [Lyu, Luan; Yang, Zhi-Xin] Univ Macau, Ctr AI & Robot, Macau 999078, Peoples R China.
   [Lyu, Luan; Wu, Enhua; Yang, Zhi-Xin] Univ Macau, Fac Sci & Technol, Macau 999078, Peoples R China.
   [Cao, Wei] China Univ Petr, Coll Comp Sci & Technol, Qingdao 266580, Peoples R China.
   [Ren, Xiaohua] Tencent, Multimedia Res Ctr, Shenzhen 518000, Peoples R China.
   [Wu, Enhua] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100190, Peoples R China.
C3 University of Macau; University of Macau; University of Macau; China
   University of Petroleum; Tencent; Chinese Academy of Sciences; Institute
   of Software, CAS
RP Yang, ZX (corresponding author), Univ Macau, State Key Lab Internet Things Smart City, Macau 999078, Peoples R China.; Yang, ZX (corresponding author), Univ Macau, Ctr AI & Robot, Macau 999078, Peoples R China.; Wu, EH; Yang, ZX (corresponding author), Univ Macau, Fac Sci & Technol, Macau 999078, Peoples R China.; Wu, EH (corresponding author), Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100190, Peoples R China.
EM yb87471@um.edu.mo; yb57412@um.edu.mo; xiaohuasuper@gmail.com;
   ehwu@um.edu.mo; zxyang@um.edu.mo
RI Yang, Zhi-Xin/AAV-1335-2020
OI Yang, Zhi-Xin/0000-0001-9151-7758; Ren, Xiaohua/0000-0002-3196-9880
FU Fund of China University of Ptroleum [0075/2023/AMJ, 0003/2023/RIB1,
   001/2024/SKL]; Science and Technology Development Fund, Macau SAR
   [62332015, 62072449]; NSFC [2020B1515130001]; Guangdong Science and
   Technology Department [ZH2220004002524]; Zhuhai Science and Technology
   Innovation Bureau [2022GH09]; International Science and Technology
   project of Guangzhou Development District; Zhuhai UM Research Institute
   [MYRG2022-00059-FST, MYRG-GRG2023-00237-FST-UMDF]; University of Macau
   [21CX06042A]; Fund of China University of Petroleum
FX This work was funded in part by the Science and Technology Development
   Fund, Macau SAR (Grant no. 0075/2023/AMJ, 0003/2023/RIB1, 001/2024/SKL),
   in part by NSFC (62332015, 62072449), in part by the Guangdong Science
   and Technology Department (Grant no. 2020B1515130001), in part by the
   Zhuhai Science and Technology Innovation Bureau (Grant no.
   ZH2220004002524), in part by the International Science and Technology
   project of Guangzhou Development District (Grant no. 2022GH09), in part
   by Zhuhai UM Research Institute (Grant no. HF-011-2021), in part by the
   University of Macau (Grant No.: MYRG2022-00059-FST,
   MYRG-GRG2023-00237-FST-UMDF), and in part by the Fund of China
   University of Petroleum (Grant No. 21CX06042A).
CR Aanjaneya M, 2019, P ACM COMPUT GRAPH, V2, DOI 10.1145/3340255
   Aanjaneya M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073625
   [Александрова Мария Викторовна Aleksandrova M.], 2010, [Проблемы Дальнего Востока, Problemy Dal'nego Vostoka], P65
   Ando R, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766935
   [Anonymous], 2008, P 2008 ACM SIGGRAPHE
   Biswas Ayan., 2016, EGPGV@ EuroVis, P69
   Bolz J, 2003, ACM T GRAPHIC, V22, P917, DOI 10.1145/882262.882364
   Bridson R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276435, 10.1145/1239451.1239497]
   Chang J, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555498
   Chen ZL, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818066
   Chentanez N., 2011, P 2011 ACM SIGGRAPHE, P83
   Chentanez N, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964977
   Demidov D, 2019, LOBACHEVSKII J MATH, V40, P535, DOI 10.1134/S1995080219050056
   DeWolf I., 2018, Divergence-Free Noise
   Dick C, 2016, IEEE T VIS COMPUT GR, V22, P2480, DOI 10.1109/TVCG.2015.2511734
   Ding XW, 2023, P ACM COMPUT GRAPH, V6, DOI 10.1145/3585511
   Elcott S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1189762.1189766
   Ferstl F, 2014, IEEE T VIS COMPUT GR, V20, P1405, DOI 10.1109/TVCG.2014.2307873
   Harris MarkJ, 2005, SIGGRAPH 05 ACM SIGG, P220
   Jiang C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766996
   Kim T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360649
   Raateland W, 2022, P ACM COMPUT GRAPH, V5, DOI 10.1145/3522608
   Sato S, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3213771
   Sato S, 2015, VISUAL COMPUT, V31, P959, DOI 10.1007/s00371-015-1122-y
   Schechter H., 2008, P 2008 ACM SIGGRAPHE, P1
   Setaluri R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661269
   Shao H, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530109
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Weber D, 2015, COMPUT GRAPH FORUM, V34, P481, DOI 10.1111/cgf.12577
   Wu K, 2018, COMPUT GRAPH FORUM, V37, P157, DOI 10.1111/cgf.13350
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
NR 31
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 13
PY 2024
DI 10.1007/s00371-024-03264-y
EA FEB 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HO6Q9
UT WOS:001160490100001
DA 2024-08-05
ER

PT J
AU Yang, SY
   Jin, Y
   Lei, JS
   Zhang, SP
AF Yang, Shengying
   Jin, Yao
   Lei, Jingsheng
   Zhang, Shuping
TI Multi-directional guidance network for fine-grained visual
   classification
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Fine-grained visual classification; Attention mechanism; Multi-layer
   interaction
AB Fine-grained images have a high confusion among subclasses. The key to this is finding discriminative regions that can be used for classification. The existing methods mainly use attention mechanisms or high-level linguistic information for classification, which only focus on the feature regions with the highest response and neglect other parts, resulting in inadequate capability for feature representation. Classification based on only a single feature part is not reliable. The fusion mechanism can achieve locating several different parts. However, simple feature fusion strategies do not exploit cross-layer information and lack the use of low-level information. To effectively address this limitation, we propose the multi-directional guidance network. Our network starts with a feature and attention guidance module that forces the network to learn detailed feature representations. Second, we propose a multi-layer guidance module that integrates diverse semantic information. In addition, we introduce a multi-way transfer structure to fuse low-level and high-level semantics in a novel way to improve generalization ability of the network. We have conducted extensive experiments on the FGVC benchmark dataset (CUB-200-2011, Stanford Cars and FGVC Aircraft) to demonstrate the superior performance of the method. Our code will be available at https://github.com/syyang2022/MGN.
C1 [Yang, Shengying; Jin, Yao; Lei, Jingsheng] Zhejiang Univ Sci & Technol, Coll Educ Sci & Technol, Hangzhou 310023, Peoples R China.
   [Zhang, Shuping] Xinjiang Inst Technol, Aksu 735400, Peoples R China.
C3 Zhejiang University of Science & Technology; Xinjiang Institute of
   Technology
RP Yang, SY (corresponding author), Zhejiang Univ Sci & Technol, Coll Educ Sci & Technol, Hangzhou 310023, Peoples R China.
EM syyang@zust.edu.cn; jinyao0224@163.com; jshlei@zust.edu.cn;
   2015166@xjit.edu.cn
RI hu, guangchen/KEI-6324-2024; Yang, Shengying/KHV-2740-2024
OI Yang, Shengying/0000-0003-3010-1622
FU National Natural Science Foundation of China [2022D01C349]; Natural
   Science Foundation of Xinjiang Uygur Autonomous Region [Y202352150,
   Y202352263]; Scientific Research Fund of Zhejiang Provincial Education
   Department
FX This work is supported by Natural Science Foundation of Xinjiang Uygur
   Autonomous Region (No. 2022D01C349) and Scientific Research Fund of
   Zhejiang Provincial Education Department (Y202352150, Y202352263).
CR Agushaka JO, 2023, NEURAL COMPUT APPL, V35, P4099, DOI 10.1007/s00521-022-07854-6
   Chang DL, 2021, PROC CVPR IEEE, P11471, DOI 10.1109/CVPR46437.2021.01131
   Chang DL, 2020, IEEE T IMAGE PROCESS, V29, P4683, DOI 10.1109/TIP.2020.2973812
   Chen XS, 2020, PROC CVPR IEEE, P3297, DOI 10.1109/CVPR42600.2020.00336
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Ding Y, 2019, IEEE I CONF COMP VIS, P6598, DOI 10.1109/ICCV.2019.00670
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Gao Y, 2020, AAAI CONF ARTIF INTE, V34, P10818
   Ge WF, 2019, PROC CVPR IEEE, P3029, DOI 10.1109/CVPR.2019.00315
   He J, 2022, AAAI CONF ARTIF INTE, P852
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu G, 2023, ADV ENG INFORM, V57, DOI 10.1016/j.aei.2023.102004
   Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132
   Jiang SQ, 2020, IEEE T IMAGE PROCESS, V29, P265, DOI 10.1109/TIP.2019.2929447
   Kong S, 2017, PROC CVPR IEEE, P7025, DOI 10.1109/CVPR.2017.743
   Kong T, 2016, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2016.98
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Lei JJ, 2021, IEEE T CIRC SYST VID, V31, P2686, DOI 10.1109/TCSVT.2020.3027616
   Li HC, 2018, Arxiv, DOI [arXiv:1805.10180, 10.48550/arXiv.1805.10180]
   Li PH, 2018, PROC CVPR IEEE, P947, DOI 10.1109/CVPR.2018.00105
   Liao Q., 2019, P IEEECVF INT C COMP, P0
   Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775
   Lin T.-Y., 2017, PROC CVPR IEEE, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu CB, 2020, AAAI CONF ARTIF INTE, V34, P11555
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Long J., 2015, Fully convolutional networks for semantic segmentation, P3431
   Luo W, 2019, IEEE I CONF COMP VIS, P8241, DOI 10.1109/ICCV.2019.00833
   Maji S, 2013, Arxiv, DOI arXiv:1306.5151
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shi XR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P601, DOI 10.1145/3394171.3413883
   Song XH, 2017, IEEE T IMAGE PROCESS, V26, P2721, DOI 10.1109/TIP.2017.2686017
   Sun M, 2018, LECT NOTES COMPUT SC, V11220, P834, DOI 10.1007/978-3-030-01270-0_49
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang J, 2022, Arxiv, DOI arXiv:2107.02341
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Yang GF, 2020, FRONT PLANT SCI, V11, DOI 10.3389/fpls.2020.600854
   Yang Z, 2018, LECT NOTES COMPUT SC, V11218, P438, DOI 10.1007/978-3-030-01264-9_26
   Yu CJ, 2018, LECT NOTES COMPUT SC, V11220, P595, DOI 10.1007/978-3-030-01270-0_35
   Zare M, 2023, J BIONIC ENG, V20, P2359, DOI 10.1007/s42235-023-00386-2
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang LB, 2019, IEEE I CONF COMP VIS, P8330, DOI 10.1109/ICCV.2019.00842
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang T, 2021, IEEE I C VI COM I PR, DOI 10.1109/VCIP53242.2021.9675376
   Zheng HL, 2019, PROC CVPR IEEE, P5007, DOI 10.1109/CVPR.2019.00515
   Zheng HL, 2020, IEEE T IMAGE PROCESS, V29, P476, DOI 10.1109/TIP.2019.2921876
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zhuang PQ, 2020, AAAI CONF ARTIF INTE, V34, P13130
NR 49
TC 2
Z9 2
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 29
PY 2024
DI 10.1007/s00371-023-03226-w
EA JAN 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GD0W6
UT WOS:001150619200001
DA 2024-08-05
ER

PT J
AU Han, KX
   You, WT
   Shi, SH
   Sun, LY
AF Han, Kaixin
   You, Weitao
   Shi, Shuhui
   Sun, Lingyun
TI Hearing with the eyes: modulating lyrics typography for music
   visualization
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Typography modulation; Music visualization; Lyrics typography; Graphic
   design
AB In human-computer interaction (HCI), typography was initially used for visual communication, which enhanced visual interest in graphic design. The investigation of how modulating visual elements (e.g., typography) to visualize sound (e.g., voice) has received substantial attention. Musical lyrics typography is a commonly used form of visual communication. However, the mapping of musical features to lyrics typography features is rarely diverse studied. And modulating lyrics typography from musical features by a certain model may not strongly arouse people's perception of auditory and visual connections. In this paper, we first proposed several models of modulating typography from musical features. Then, we investigated which model can modulate lyrics typography well to visualize music. Finally, the experiment results show that the lyrics typography by the mapping of musical features (loudness, pitch, and duration) and typography feature (character size, baseline shift, and character width) can better arouse people's perception of auditory and visual connection. And the lyrics typography modulated by a moderate mapping parameter can evoke people's high visual aesthetic preference. We aspire for our work to offer a novel perspective on music visualization, assisting the hard-of-hearing people to experience musical content.
C1 [Han, Kaixin; You, Weitao; Shi, Shuhui] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
   [You, Weitao] Zhejiang Univ, Alibaba Zhejiang Univ Joint Res Inst Frontier Tech, Hangzhou, Peoples R China.
   [Sun, Lingyun] Zhejiang Univ, China Southern Power Grid Joint Res Ctr AI, Hangzhou, Zhejiang, Peoples R China.
   [Sun, Lingyun] Zhejiang Univ, Zhejiang Singapore Innovat & AI Joint Res Lab, Hangzhou, Zhejiang, Peoples R China.
C3 Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang
   University
RP You, WT (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.; You, WT (corresponding author), Zhejiang Univ, Alibaba Zhejiang Univ Joint Res Inst Frontier Tech, Hangzhou, Peoples R China.
EM Weitao_you@zju.edu.cn
FU Key Technologies Research and Development Program [2022YFB3303301];
   National Key R&D Program of China [62107035, 62006208]; National Natural
   Science Foundation of China
FX This work is supported by the National Key R&D Program of China
   (2022YFB3303301), and the National Natural Science Foundation of China
   (No. 62107035 and No. 62006208).
CR Agrawal C., 2021, P 23 INT ACM SIGACCE, P1
   Aoki T., 2022, P 2022 CHI C HUMAN F, P1
   Bergstrom Tony, 2007, Proceedings Graphics Interface 2007, P297, DOI 10.1145/1268517.1268565
   Cantareira GD, 2016, IEEE T MULTIMEDIA, V18, P2238, DOI 10.1109/TMM.2016.2614226
   Chan WY, 2010, IEEE T VIS COMPUT GR, V16, P161, DOI 10.1109/TVCG.2009.63
   Chew E., 2005, Comput Entertain (CIE), V3, P1
   Ciuha P., 2010, P INT C MULT, P1677
   Castro JCE, 2019, PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON DIGITAL AND INTERACTIVE ARTS (ARTECH 2019), DOI 10.1145/3359852.3359892
   de Lacerda Pataca C., Visualization of Speech Prosody and Emotion in Captions: Accessibility for Deaf and Hard-of-Hearing Users
   De Prisco R, 2017, INFORM VISUAL, V16, P139, DOI 10.1177/1473871616655468
   Deja JA., 2020, CHI Conf Human Factor Comput Syst, V2020, P1
   Eikelenboom M., 2019, Int J Inclus Museum, V12, P51, DOI [10.18848/1835-2014/CGP/v12i03/51-64, DOI 10.18848/1835-2014/CGP/V12I03/51-64]
   Ekstrom M P, 2012, DIGITAL IMAGE PROCES
   Fadillah C., 2019, 2019 INT C SUSTAINAB, P309
   Fassnidge C, 2019, J COGNITIVE NEUROSCI, V31, P922, DOI 10.1162/jocn_a_01395
   Fellows K. S., 2009, Typecast: the voice of typography
   Fonteles JH, 2013, J VISUAL LANG COMPUT, V24, P472, DOI 10.1016/j.jvlc.2013.10.002
   Fujishiro I, 2015, 2015 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P189, DOI 10.1109/CW.2015.65
   Gumulia A., 2011, P 19 ACM INT C MULTI, P949
   Guttman SE, 2005, PSYCHOL SCI, V16, P228, DOI 10.1111/j.0956-7976.2005.00808.x
   Hiraga R, 2002, SECOND INTERNATIONAL CONFERENCE ON WEB DELIVERING OF MUSIC, PROCEEDINGS, P101, DOI 10.1109/WDM.2002.1176199
   Hiraga R., 2002, P 10 ACM INT C MULTI, P239
   Isaacson E.J., 2005, ISMIR, P389
   Jun Y, 2022, J RETAIL CONSUM SERV, V64, DOI 10.1016/j.jretconser.2021.102724
   Kadner F, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445140
   Kato J, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3403, DOI 10.1145/2702123.2702140
   Katz M, 2008, POP MUSIC SOC, V31, P511, DOI 10.1080/03007760802184065
   Kim Jeeeun, 2015, P BOSTON 14 INT C IN, P426, DOI [10.1145/2771839.2771870, DOI 10.1145/2771839.2771870]
   Koch Beth Elynn, 2011, Human emotion response to typographic design
   Lee J., 2011, HCI INT 2011 POSTERS, P303
   Lee S. W., 2015, NIMEpp, P65
   Lehtiniemi A., 2012, Proceedings of the 2012 16th International Conference on Information Visualisation (IV), P143, DOI 10.1109/IV.2012.34
   Lim S., 2014, Future Information Technology, James J, P503, DOI [10.1007/978-3-642-55038-6_79, DOI 10.1007/978-3-642-55038-6_79]
   Lim Sooyeon, 2022, The International Journal of Advanced Culture Technology, V10, P122
   Lima H, 2019, IEEE INT CON INF VIS, P352, DOI 10.1109/IV.2019.00066
   Lima HB, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3461835
   Ludden D., 2015, Psychology Today, V19
   Macas C, 2019, PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON DIGITAL AND INTERACTIVE ARTS (ARTECH 2019), DOI 10.1145/3359852.3359874
   MacDonald J, 2000, PERCEPTION, V29, P1155, DOI 10.1068/p3020
   Machida W, 2011, IEEE INT CONF INF VI, P145, DOI 10.1109/IV.2011.62
   Malandrino D, 2015, IEEE INT CONF INF VI, P56, DOI 10.1109/iV.2015.21
   Mardirossian Arpi, 2007, ISMIR PP
   Matthias W., 2015, INT C SPEECH TECHNOL, P1
   Miniukovich A, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300738
   Mitroo J.B., 1979, ACM SIGGRAPH COMPUTE, V13, P218, DOI DOI 10.1145/965103.807447
   Mol E, 2021, NOR ARCHAEOL REV, V54, P80, DOI 10.1080/00293652.2021.1951830
   Parente J, 2022, LECT NOTES COMPUT SC, P212, DOI [10.1007/978-3-031-03789-4_27, 10.1007/978-3-031-03789-4_14]
   Pataca CD, 2023, IEEE T AFFECT COMPUT, V14, P6, DOI 10.1109/TAFFC.2022.3174721
   Pataca CD, 2020, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, IUI 2020, P139, DOI 10.1145/3377325.3377526
   Pentimalli B, 2023, SENSES SOC, V18, P254, DOI 10.1080/17458927.2023.2232621
   Raden AZM., 2019, Int. J. Sci. Technol. Res, V8, P61
   Rosenkvist A, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1349, DOI [10.1109/VR.2019.8797903, 10.1109/vr.2019.8797903]
   Schlippe T, 2020, 2020 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2020), P196, DOI 10.1109/CW49994.2020.00039
   Snydal J., 2005, CHI 05 EXTENDED ABST, P1805
   So C, 2023, INT J HUM-COMPUT INT, V39, P755, DOI 10.1080/10447318.2022.2049081
   Toledo J., 2005, P 4 WSEAS INT C ELEC, P27
   Turgut ÖP, 2014, PROCD SOC BEHV, V122, P40, DOI 10.1016/j.sbspro.2014.01.1300
   Turgut OP, 2012, PROCD SOC BEHV, V51, P583, DOI 10.1016/j.sbspro.2012.08.209
   Zhiquan Y., 2009, P 27 INT C HUMAN FAC, P3413
NR 59
TC 0
Z9 0
U1 13
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 19
PY 2024
DI 10.1007/s00371-023-03239-5
EA JAN 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FJ1Z0
UT WOS:001145314200001
DA 2024-08-05
ER

PT J
AU Martin, D
   Fandos, A
   Masia, B
   Serrano, A
AF Martin, Daniel
   Fandos, Andres
   Masia, Belen
   Serrano, Ana
TI SAL3D: a model for saliency prediction in 3D meshes
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Saliency; Eye tracking; Attention; 3D meshes
ID VISUAL-ATTENTION; SHIFTS
AB Advances in virtual and augmented reality have increased the demand for immersive and engaging 3D experiences. To create such experiences, it is crucial to understand visual attention in 3D environments, which is typically modeled by means of saliency maps. While attention in 2D images and traditional media has been widely studied, there is still much to explore in 3D settings. In this work, we propose a deep learning-based model for predicting saliency when viewing 3D objects, which is a first step toward understanding and predicting attention in 3D environments. Previous approaches rely solely on low-level geometric cues or unnatural conditions, however, our model is trained on a dataset of real viewing data that we have manually captured, which indeed reflects actual human viewing behavior. Our approach outperforms existing state-of-the-art methods and closely approximates the ground-truth data. Our results demonstrate the effectiveness of our approach in predicting attention in 3D objects, which can pave the way for creating more immersive and engaging 3D experiences.
C1 [Martin, Daniel; Fandos, Andres; Masia, Belen; Serrano, Ana] Univ Zaragoza, I3A, Zaragoza, Spain.
C3 University of Zaragoza
RP Martin, D; Masia, B; Serrano, A (corresponding author), Univ Zaragoza, I3A, Zaragoza, Spain.
EM danims@unizar.es; bmasia@unizar.es; anase@unizar.es
RI Martin, Daniel/KLZ-9356-2024
OI Martin, Daniel/0000-0002-0073-6398; Masia, Belen/0000-0003-0060-7278
FU H2020 European Research Council
FX No Statement Available
CR Assens M, 2017, IEEE INT CONF COMP V, P2331, DOI 10.1109/ICCVW.2017.275
   Assens Marc, 2018, P EUR C COMP VIS ECC, P0
   Berdun EB, 2022, COMPUT GRAPH-UK, V106, P200, DOI 10.1016/j.cag.2022.06.002
   Borji A, 2012, PROC CVPR IEEE, P438, DOI 10.1109/CVPR.2012.6247706
   Castellani U, 2008, COMPUT GRAPH FORUM, V27, P643, DOI 10.1111/j.1467-8659.2008.01162.x
   Chen XY, 2021, PROC CVPR IEEE, P10871, DOI 10.1109/CVPR46437.2021.01073
   Chen XB, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185525
   Dahou Yasser, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12663), P305, DOI 10.1007/978-3-030-68796-0_22
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Hu SF, 2020, NEUROCOMPUTING, V400, P11, DOI 10.1016/j.neucom.2020.02.106
   Huang X, 2015, IEEE I CONF COMP VIS, P262, DOI 10.1109/ICCV.2015.38
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2000, VISION RES, V40, P1489, DOI 10.1016/S0042-6989(99)00163-7
   Jiang L, 2019, AAAI CONF ARTIF INTE, P8521
   Kim Y, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1670671.1670676
   Kingma D.P., 2014, Proc. of ICLR
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Kummerer M, 2021, arXiv
   Lau M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925927
   Lavoué G, 2018, COMPUT GRAPH FORUM, V37, P191, DOI 10.1111/cgf.13353
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Leifman G, 2012, PROC CVPR IEEE, P414, DOI 10.1109/CVPR.2012.6247703
   Li G., 2015, 2015 IEEE C COMP VIS, P06
   Limper Max., 2016, Proceedings of the Thirty-Seventh Annual Conference of the European Association for Computer Graphics: Short Papers, P13, DOI DOI 10.2312/EGSH.20161003
   Liu C., 2022, Vis. Comput., P1
   Martin D., 2020, CVPR WORKSH COMP VIS
   Martin D, 2022, Arxiv, DOI arXiv:2204.09404
   Martin D, 2022, IEEE T VIS COMPUT GR, V28, P2003, DOI 10.1109/TVCG.2022.3150502
   MORGAN MW, 1968, AMER J OPT ARCH AM A, V45, P417
   Nousias S, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102796
   Pan JT, 2016, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2016.71
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599
   Song R, 2021, IEEE T VIS COMPUT GR, V27, P151, DOI 10.1109/TVCG.2019.2928794
   Song R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2530691
   Sun WJ, 2021, IEEE T PATTERN ANAL, V43, P2101, DOI 10.1109/TPAMI.2019.2956930
   Tasse FP, 2015, IEEE I CONF COMP VIS, P163, DOI 10.1109/ICCV.2015.27
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang X, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275094
   Wei N, 2018, IEEE ACCESS, V6, P54536, DOI 10.1109/ACCESS.2018.2872168
   Wu JL, 2013, GRAPH MODELS, V75, P255, DOI 10.1016/j.gmod.2013.05.002
   Xie Z., 2020, Understanding and scheduling weight decay
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
NR 43
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 4
PY 2024
DI 10.1007/s00371-023-03206-0
EA JAN 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EA1V1
UT WOS:001136097700001
OA Green Published, hybrid
DA 2024-08-05
ER

PT J
AU Gallego, NPD
   Ilao, J
   Cordel, MI
   Ruiz, C Jr
AF Gallego, Neil Patrick Del
   Ilao, Joel
   Cordel, Macario, II
   Ruiz Jr, Conrado
TI Training a shadow removal network using only 3D primitive occluders
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Shadow removal; Deep neural network; Synthetic images; Computer
   graphics; Image processing
AB Removing shadows in images is often a necessary pre-processing task for improving the performance of computer vision applications. Deep learning shadow removal approaches require a large-scale dataset that is challenging to gather. To address the issue of limited shadow data, we present a new and cost-effective method of synthetically generating shadows using 3D virtual primitives as occluders. We simulate the shadow generation process in a virtual environment where foreground objects are composed of mapped textures from the Places-365 dataset. We argue that complex shadow regions can be approximated by mixing primitives, analogous to how 3D models in computer graphics can be represented as triangle meshes. We use the proposed synthetic shadow removal dataset, DLSUSynthPlaces-100K, to train a feature-attention-based shadow removal network without explicit domain adaptation or style transfer strategy. The results of this study show that the trained network achieves competitive results with state-of-the-art shadow removal networks that were trained purely on typical SR datasets such as ISTD or SRD. Using a synthetic shadow dataset of only triangular prisms and spheres as occluders produces the best results. Therefore, the synthetic shadow removal dataset can be a viable alternative for future deep-learning shadow removal methods. The source code and dataset can be accessed at this link: https://neildg.github.io/SynthShadowRemoval/.
C1 [Gallego, Neil Patrick Del; Ilao, Joel; Cordel, Macario, II; Ruiz Jr, Conrado] De La Salle Univ, 2401 Taft Ave Malate, Manila 1004, Metro Manila, Philippines.
   [Gallego, Neil Patrick Del] De Le Salle Univ, Graph Animat Multimedia & Entertainment GAME Lab, Manila, Philippines.
   [Ilao, Joel] De La Salle Univ, Ctr Automation Res, Manila, Philippines.
   [Cordel, Macario, II] De La Salle Univ, Dr Andrew L Tan Data Sci Inst, Manila, Philippines.
   [Ruiz Jr, Conrado] La Salle Univ Ramon Llull, Human Environm Res HER Grp, Barcelona 08022, Spain.
C3 De La Salle University; De La Salle University; De La Salle University;
   Universitat Ramon Llull
RP Gallego, NPD (corresponding author), De La Salle Univ, 2401 Taft Ave Malate, Manila 1004, Metro Manila, Philippines.; Gallego, NPD (corresponding author), De Le Salle Univ, Graph Animat Multimedia & Entertainment GAME Lab, Manila, Philippines.
EM neil.delgallego@dlsu.edu.ph; joel.ilao@dlsu.edu.ph;
   macario.cordel@dlsu.edu.ph; conrado.ruiz@salle.url.edu
OI Cordel, Macario II/0000-0001-7270-9236; Ruiz, Conrado
   Jr./0000-0002-0956-7201
FU De La Salle University (DLSU), Department of Science and Technology
   (DOST)
FX We would like to acknowledge De La Salle University (DLSU), Department
   of Science and Technology (DOST), for funding this research. Models were
   trained using the DOST COARE Computing facility. We would also like to
   thank Mr. Gregory Cu and Mr. Fritz Flores for the technical support
   provided to us when training our network using the DLSU College of
   Computer Studies computing clusters.
CR Ahn WJ, 2023, NEUROCOMPUTING, V552, DOI 10.1016/j.neucom.2023.126559
   Akenine-Mller T., 2018, Real-Time Rendering, V4, DOI DOI 10.1201/B22086
   Al-Najdawi N, 2012, PATTERN RECOGN LETT, V33, P752, DOI 10.1016/j.patrec.2011.12.013
   Chang R., 2016, OCEANS 2016, P1
   Chen ZH, 2020, PROC CVPR IEEE, P5610, DOI 10.1109/CVPR42600.2020.00565
   Chen ZP, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2700, DOI 10.1145/3503161.3548074
   Chen ZP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4723, DOI 10.1109/ICCV48922.2021.00470
   Cun XD, 2020, AAAI CONF ARTIF INTE, V34, P10680
   Fu L, 2021, PROC CVPR IEEE, P10566, DOI 10.1109/CVPR46437.2021.01043
   Gao JH, 2022, IEEE COMPUT SOC CONF, P598, DOI 10.1109/CVPRW56347.2022.00075
   Gryka M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2732407
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   Guo R, 2021, INT C PATT RECOG, P5867, DOI 10.1109/ICPR48806.2021.9413254
   Hastings-Trew J., 2021, Reproducing real world light
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D, 2020, Arxiv, DOI [arXiv:1912.02781, DOI 10.48550/ARXIV:1912.02781]
   Hieu Le, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P264, DOI 10.1007/978-3-030-58621-8_16
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Hu XW, 2021, IEEE T IMAGE PROCESS, V30, P1925, DOI 10.1109/TIP.2021.3049331
   Hu XW, 2019, IEEE I CONF COMP VIS, P2472, DOI 10.1109/ICCV.2019.00256
   Hu XW, 2018, PROC CVPR IEEE, P7454, DOI 10.1109/CVPR.2018.00778
   Huang X, 2011, IEEE I CONF COMP VIS, P898, DOI 10.1109/ICCV.2011.6126331
   Inoue N, 2021, IEEE T CIRC SYST VID, V31, P4187, DOI 10.1109/TCSVT.2020.3047977
   Isola P., 2017, P IEEE C COMP VIS PA, P1125, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Jianhua Ye, 2012, 2012 9th International Conference on Fuzzy Systems and Knowledge Discovery, P1859, DOI 10.1109/FSKD.2012.6234020
   Jin YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5007, DOI 10.1109/ICCV48922.2021.00498
   Kingma D. P., 2014, arXiv
   Krähenbühl P, 2018, PROC CVPR IEEE, P2955, DOI 10.1109/CVPR.2018.00312
   Le H, 2022, IEEE T PATTERN ANAL, V44, P9088, DOI 10.1109/TPAMI.2021.3124934
   Le H, 2019, IEEE I CONF COMP VIS, P8577, DOI 10.1109/ICCV.2019.00867
   Li ZY, 2022, J ELECTRON IMAGING, V31, DOI 10.1117/1.JEI.31.2.023028
   Lin JX, 2018, PROC CVPR IEEE, P5524, DOI 10.1109/CVPR.2018.00579
   Liu J., 2023, IEEE Trans. Multimed, V8, P64
   Liu QG, 2011, IEEE T INF FOREN SEC, V6, P1111, DOI 10.1109/TIFS.2011.2139209
   Liu ZH, 2021, PROC CVPR IEEE, P4925, DOI 10.1109/CVPR46437.2021.00489
   Mahmood N, 2019, IEEE I CONF COMP VIS, P5441, DOI 10.1109/ICCV.2019.00554
   Mostafa Y, 2017, CAN J REMOTE SENS, V43, P545, DOI 10.1080/07038992.2017.1384310
   Park T., 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-719, 10.1007/978-3-030-58545-719, DOI 10.1007/978-3-030-58545-7_19]
   Poynton C., 2003, Digital Video and HDTV Algorithms and Interfaces, V1st ed.
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Qu LQ, 2017, PROC CVPR IEEE, P2308, DOI 10.1109/CVPR.2017.248
   Radford A, 2016, Arxiv, DOI [arXiv:1511.06434, DOI 10.48550/ARXIV.1511.06434]
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sanin Andres, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P141, DOI 10.1109/ICPR.2010.43
   Shahtahmassebi A, 2013, CHINESE GEOGR SCI, V23, P403, DOI 10.1007/s11769-013-0613-x
   Shen L, 2015, PROC CVPR IEEE, P2067, DOI 10.1109/CVPR.2015.7298818
   Shirley P., 2009, Fundamentals of Computer Graphics, V3, DOI [10.1201/9781439865521, DOI 10.1201/9781439865521]
   Unity, 2021, Unity engine
   Vasluianu FA, 2021, IEEE COMPUT SOC CONF, P826, DOI 10.1109/CVPRW53098.2021.00092
   Vijayan M, 2019, MULTIMED TOOLS APPL, V78, P7055, DOI 10.1007/s11042-018-6459-6
   Vincente TFY, 2016, LECT NOTES COMPUT SC, V9910, P816, DOI 10.1007/978-3-319-46466-4_49
   Wan J, 2022, LECT NOTES COMPUT SC, V13679, P361, DOI 10.1007/978-3-031-19800-7_21
   Wang JF, 2018, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR.2018.00192
   Wang TY, 2023, IEEE T PATTERN ANAL, V45, P3259, DOI 10.1109/TPAMI.2022.3185628
   Wu W, 2023, IEEE T CIRC SYST VID, V33, P6213, DOI 10.1109/TCSVT.2023.3263903
   Wu W, 2023, KNOWL-BASED SYST, V273, DOI 10.1016/j.knosys.2023.110614
   Wu W, 2021, COMPUT GRAPH-UK, V95, P156, DOI 10.1016/j.cag.2021.02.005
   Wu W, 2022, VISUAL COMPUT, V38, P1665, DOI 10.1007/s00371-021-02095-5
   Wu W, 2022, VISUAL COMPUT, V38, P1677, DOI 10.1007/s00371-021-02096-4
   Wu XT, 2022, COMPUT GRAPH-UK, V104, P152, DOI 10.1016/j.cag.2022.04.003
   Wu Xian-Tao, 2022, The Visual Computer, V2022, P1
   Chang AX, 2015, Arxiv, DOI [arXiv:1512.03012, DOI 10.48550/ARXIV.1512.03012]
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Zhang L, 2020, AAAI CONF ARTIF INTE, V34, P12829
   Zheng QL, 2019, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2019.00531
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu YR, 2022, PROC CVPR IEEE, P5617, DOI 10.1109/CVPR52688.2022.00554
NR 68
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 27
PY 2024
DI 10.1007/s00371-024-03536-7
EA JUN 2024
PG 38
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WS9U2
UT WOS:001256988800001
DA 2024-08-05
ER

PT J
AU Li, QS
   Guo, YY
   Liu, XR
   Hu, L
   Luo, FF
   Liu, SJ
AF Li, Qinsong
   Guo, Yueyu
   Liu, Xinru
   Hu, Ling
   Luo, Feifan
   Liu, Shengjun
TI Deformable shape matching with multiple complex spectral filter operator
   preservation
SO VISUAL COMPUTER
LA English
DT Article
DE 3D shape matching; Functional maps; Complex spectral filter operators
AB The functional maps framework has achieved remarkable success in non-rigid shape matching. However, the traditional functional map representations do not explicitly encode surface orientation, which can easily lead to orientation-reversing correspondence. The complex functional map addresses this issue by linking oriented tangent bundles to favor orientation-preserving correspondence. Nevertheless, the absence of effective restrictions on the complex functional maps hinders them from obtaining high-quality correspondences. To this end, we introduce novel and powerful constraints to determine complex functional maps by incorporating multiple complex spectral filter operator preservation constraints with a rigorous theoretical guarantee. Such constraints encode the surface orientation information and enforce the isometric property of the map. Based on these constraints, we propose a novel and efficient method to obtain orientation-preserving and accurate correspondences across shapes by alternatively updating the functional maps, complex functional maps, and pointwise maps. Extensive experiments demonstrate our significant improvements in correspondence quality and computing efficiency. In addition, our constraints can be easily adapted to other functional maps-based methods to enhance their performance.
C1 [Li, Qinsong] Cent South Univ, Big Data Inst, Changsha, Peoples R China.
   [Guo, Yueyu; Liu, Xinru; Liu, Shengjun] Cent South Univ, Inst Engn Modeling & Sci Comp, Changsha, Peoples R China.
   [Hu, Ling] Hunan First Normal Univ, Sch Math & Stat, Changsha, Peoples R China.
   [Liu, Shengjun] Cent South Univ, State Key Lab High Performance Mfg Complex, Changsha, Peoples R China.
   [Luo, Feifan] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
C3 Central South University; Central South University; Hunan First Normal
   University; Central South University; Zhejiang University
RP Liu, SJ (corresponding author), Cent South Univ, Inst Engn Modeling & Sci Comp, Changsha, Peoples R China.; Liu, SJ (corresponding author), Cent South Univ, State Key Lab High Performance Mfg Complex, Changsha, Peoples R China.; Luo, FF (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
EM qinsli.cg@foxmail.com; gyy956197161@163.com; liuxinru@csu.edu.cn;
   huling.cg@foxmail.com; Luoff@zju.edu.cn; shjliu.cg@csu.edu.cn
FU Natural Science Foundation of China [62172447, 62302530]; Hunan
   Provincial Natural Science Foundation of China [2023JJ40769]; Hunan
   Province Education Department's Key Project [23A0641]; Central South
   University's Fundamental Research Funds [2024ZZTS0770]
FX This work was supported by the Natural Science Foundation of China (Nos.
   62172447, 62302530), the Hunan Provincial Natural Science Foundation of
   China (No. 2023JJ40769), the Hunan Province Education Department's Key
   Project (No.23A0641), and the Central South University's Fundamental
   Research Funds (No.2024ZZTS0770).
CR Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   Attaiki S, 2021, INT CONF 3D VISION, P299, DOI 10.1109/3DV53792.2021.00040
   Azencot O, 2013, COMPUT GRAPH FORUM, V32, P73, DOI 10.1111/cgf.12174
   Bastian L., 2023, ARXIV
   Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491
   Cao D., 2024, ARXIV
   Cao DL, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592107
   Donati Nicolas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8589, DOI 10.1109/CVPR42600.2020.00862
   Donati N, 2022, PROC CVPR IEEE, P732, DOI 10.1109/CVPR52688.2022.00082
   Donati N, 2022, COMPUT GRAPH FORUM, V41, P317, DOI 10.1111/cgf.14437
   Dubrovina A, 2011, ADV DATA SCI ADAPT, V3, P203, DOI 10.1142/S1793536911000829
   Eisenberger M., 2020, 2020 IEEE CVF C COMP, p12,262
   Eisenberger Marvin, 2020, Advances in Neural Information Processing Systems, P10491
   Halimi O, 2019, PROC CVPR IEEE, P4365, DOI 10.1109/CVPR.2019.00450
   Hartwig F, 2023, PROCEEDINGS OF SIGGRAPH 2023 CONFERENCE PAPERS, SIGGRAPH 2023, DOI 10.1145/3588432.3591518
   Houston K., 2015, ARXIV
   Hu L, 2023, GRAPH MODELS, V129, DOI 10.1016/j.gmod.2023.101189
   Hu L, 2021, PROC CVPR IEEE, P14531, DOI 10.1109/CVPR46437.2021.01430
   Huang RQ, 2020, COMPUT GRAPH FORUM, V39, P265, DOI 10.1111/cgf.14084
   Keros AD, 2023, PROCEEDINGS OF SIGGRAPH 2023 CONFERENCE PAPERS, SIGGRAPH 2023, DOI 10.1145/3588432.3591544
   Kim VG, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964974
   Kirgo M, 2021, COMPUT GRAPH FORUM, V40, P165, DOI 10.1111/cgf.14180
   Le T., 2024, ARXIV
   Li Q., 2024, DEEP MSFOP MULTIPLE
   Li QS, 2021, COMPUT GRAPH FORUM, V40, P81, DOI 10.1111/cgf.14120
   Litany O, 2017, IEEE I CONF COMP VIS, P5660, DOI 10.1109/ICCV.2017.603
   Liu SJ, 2022, COMPUT GRAPH FORUM, V41, P51, DOI 10.1111/cgf.14656
   Liu X., 2023, VISUAL COMPUT, P1
   Lorensen H. E., 1987, Proc. SIGGRAPH, V21, P163, DOI 10.1145/37401.37422
   Magnet R., 2024, ARXIV
   Magnet R, 2023, COMPUT GRAPH FORUM, V42, P89, DOI 10.1111/cgf.14746
   Magnet R, 2022, INT CONF 3D VISION, P495, DOI 10.1109/3DV57658.2022.00061
   Melzi S, 2018, COMPUT GRAPH FORUM, V37, P20, DOI 10.1111/cgf.13309
   Melzi S, 2019, PROC CVPR IEEE, P4624, DOI 10.1109/CVPR.2019.00476
   Melzi S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356524
   Melzi Simone, 2019, EUROGRAPHICS WORKSHO
   Mohamed S., 2022, ADV NEURAL INFORM PR, P29336
   Nogneng D, 2017, COMPUT GRAPH FORUM, V36, P259, DOI 10.1111/cgf.13124
   Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526
   Ovsjanikov M, 2011, COMPUT GRAPH FORUM, V30, P1503, DOI 10.1111/j.1467-8659.2011.02024.x
   Panine M, 2022, COMPUT GRAPH FORUM, V41, P394, DOI 10.1111/cgf.14579
   Ren J, 2021, COMPUT GRAPH FORUM, V40, P81, DOI 10.1111/cgf.14359
   Ren J, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417800
   Ren J, 2019, COMPUT GRAPH FORUM, V38, P39, DOI 10.1111/cgf.13788
   Ren J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275040
   Rodolà E, 2017, COMPUT GRAPH FORUM, V36, P700, DOI 10.1111/cgf.13160
   Rodola E., 2017, Computer Graphics Forum, V36, P222, DOI 10.1111/cgf.12797
   Roufosse JM, 2019, IEEE I CONF COMP VIS, P1617, DOI 10.1109/ICCV.2019.00170
   Sahillioglu Y, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3554978
   Sahillioglu Y, 2020, VISUAL COMPUT, V36, P1705, DOI 10.1007/s00371-019-01760-0
   Salti S, 2014, COMPUT VIS IMAGE UND, V125, P251, DOI 10.1016/j.cviu.2014.04.011
   Sharma A., 2020, P 34 INT C NEUR INF
   Sharp N, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3507905
   Sharp N, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3243651
   Solomon J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925903
   Sun M., 2023, 2023 IEEE CVF INT C, P461
   Wang YQ, 2019, PROC CVPR IEEE, P6224, DOI 10.1109/CVPR.2019.00639
   Yazgan M., 2023, IEEE T VIS COMPUT GR, P1
   ZUFFI S, 2017, CVPR, DOI DOI 10.1109/CVPR.2017.586
NR 59
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4885
EP 4898
DI 10.1007/s00371-024-03487-z
EA JUN 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001254028100003
DA 2024-08-05
ER

PT J
AU Zhang, JJ
   Lin, Y
   Zhou, X
   Shi, PR
   Zhu, XQ
   Zeng, D
AF Zhang, Junjie
   Lin, Yi
   Zhou, Xin
   Shi, Pangrong
   Zhu, Xiaoqiang
   Zeng, Dan
TI Precision in pursuit: a multi-consistency joint approach for infrared
   anti-UAV tracking
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Anti-UAV; Visual object tracking; Template updating; Tracking drift;
   Multi-consistency
AB In addition to addressing issues commonly encountered in generic visual object tracking, this task endeavors necessitates coping with the diminutive characteristics of infrared UAV targets, potential discontinuities of targets between adjacent frames due to irregular motion patterns and camera rotations, and the occurrence of drastic appearance deformations. To effectively tackle these challenges, we present a multi-consistency joint approach tailored for infrared anti-UAV tracking. The concept of consistency is realized across three pivotal perspectives: Firstly, by acknowledging the inherent motion consistency of targets, we introduce the Robust Motion Constraint Module (RMCM). This module effectively mitigates tracking drift issues by imposing constraints on the position and size variations of tracked results through an adaptive cost matrix. Secondly, we design the Flexible Spatial Remapping Module (FSRM) grounded in spatial consistency. This module facilitates the identification of correspondences between images in adjacent frames, effectively resolving the substantial target displacement. Lastly, we propose the adaptive template update strategy to address the challenge of significant target deformations. This strategy dynamically selects alternate templates to ensure feature consistency, thereby maintaining stable tracking even in the face of drastic pose changes. The experimental results on benchmark datasets demonstrate that our approach achieves promising advancements compared to state-of-the-art models. Specifically, our approach achieves 69.65 Acc, 80.45 Acc, 65.88 Acc and 50.28 Acc & lowast;\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$Acc<^>{*}$$\end{document} on the 1st anti-UAV test set dataset, 1st anti-UAV test-dev dataset, 2nd anti-UAV test-dev dataset and 3rd anti-UAV validation set dataset, respectively. We will make our code publicly available upon the acceptance of our paper.
C1 [Zhang, Junjie; Lin, Yi; Zhou, Xin; Shi, Pangrong; Zhu, Xiaoqiang; Zeng, Dan] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Key Lab Specialty Fiber Opt, Joint Int Res Lab Specialty Fiber Opt & Adv Commun, Shangda Rd 99, Shanghai 200444, Peoples R China.
C3 Shanghai University
RP Zeng, D (corresponding author), Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Key Lab Specialty Fiber Opt, Joint Int Res Lab Specialty Fiber Opt & Adv Commun, Shangda Rd 99, Shanghai 200444, Peoples R China.
EM junjie_zhang@shu.edu.cn; liny@shu.edu.cn; aqizhoua@shu.edu.cn;
   shipangrong_spr@shu.edu.cn; xqzhu@shu.edu.cn; dzeng@shu.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Abdelpakey MH, 2018, LECT NOTES COMPUT SC, V11241, P463, DOI 10.1007/978-3-030-03801-4_41
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Che AL, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2160
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Chen ZH, 2023, IEEE T PATTERN ANAL, V45, P13489, DOI 10.1109/TPAMI.2023.3293885
   Chen ZH, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2061
   Cheng F, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22103701
   Cheng ZZ, 2015, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2015.55
   Cui YT, 2022, PROC CVPR IEEE, P13598, DOI 10.1109/CVPR52688.2022.01324
   Cui YT, 2021, Arxiv, DOI arXiv:2104.00403
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Fang HZ, 2021, IEEE INT CONF COMP V, P1240, DOI 10.1109/ICCVW54120.2021.00144
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fu ZH, 2021, PROC CVPR IEEE, P13769, DOI 10.1109/CVPR46437.2021.01356
   Gao SY, 2022, LECT NOTES COMPUT SC, V13682, P146, DOI 10.1007/978-3-031-20047-2_9
   Guo C, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2066
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   He AF, 2018, PROC CVPR IEEE, P4834, DOI 10.1109/CVPR.2018.00508
   Huang B, 2021, IEEE INT CONF COMP V, P1204, DOI 10.1109/ICCVW54120.2021.00140
   Huang LH, 2020, AAAI CONF ARTIF INTE, V34, P11037
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Jiang N, 2021, Arxiv, DOI [arXiv:2101.08466, DOI 10.48550/ARXIV.2101.08466]
   Kim M, 2022, LECT NOTES COMPUT SC, V13682, P534, DOI 10.1007/978-3-031-20047-2_31
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Li Y., 2023, P IEEE CVF C COMP VI, P3025
   Lin Liting, 2022, ADV NEUR IN
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Ottoni LTC, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12234859
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sheng B, 2022, IEEE T CYBERNETICS, V52, P6662, DOI 10.1109/TCYB.2021.3079311
   Voigtlaender P, 2020, PROC CVPR IEEE, P6577, DOI 10.1109/CVPR42600.2020.00661
   Wang N, 2021, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR46437.2021.00162
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wang Q, 2018, PROC CVPR IEEE, P4854, DOI 10.1109/CVPR.2018.00510
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xiaoran Shi, 2022, 2022 3rd International Conference on Computer Vision, Image and Deep Learning & International Conference on Computer Engineering and Applications (CVIDL & ICCEA), P986, DOI 10.1109/CVIDLICCEA56201.2022.9824591
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Xu YD, 2020, AAAI CONF ARTIF INTE, V34, P12549
   Yan B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10428, DOI 10.1109/ICCV48922.2021.01028
   Ye BT, 2022, LECT NOTES COMPUT SC, V13682, P341, DOI 10.1007/978-3-031-20047-2_20
   Yu YC, 2020, PROC CVPR IEEE, P6727, DOI 10.1109/CVPR42600.2020.00676
   Yunhua Zhang, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11213), P355, DOI 10.1007/978-3-030-01240-3_22
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhao J, 2022, IEEE T INTELL TRANSP, V23, P25323, DOI 10.1109/TITS.2022.3177627
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
NR 50
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 15
PY 2024
DI 10.1007/s00371-024-03525-w
EA JUN 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UG1B9
UT WOS:001246803800002
DA 2024-08-05
ER

PT J
AU Huang, ZG
   Xue, WL
   Zhou, YX
   Sun, JL
   Wu, YZ
   Yuan, TT
   Chen, SY
AF Huang, Zhigang
   Xue, Wanli
   Zhou, Yuxi
   Sun, Jinlu
   Wu, Yazhou
   Yuan, Tiantian
   Chen, Shengyong
TI Dual-stage temporal perception network for continuous sign language
   recognition
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Continuous sign language recognition; Temporal modeling; Multi-scale
   local temporal relations; Global temporal relations
AB Continuous sign language recognition (CSLR) aims to identify a sequence of glosses from a sign language video with only a sentence-level label provided in a weakly supervised way. In sign language videos, the transitions among actions are naturally fluent, and different glosses or the same gloss correspond to video clips with various temporal scales. Obviously, these factors pose a challenge to the effective extraction of complex temporal information. However, most previous deep learning-based CSLR methods employ a temporal modeling method with a fixed temporal receptive field, which is a simple and effective solution but does not cope well with video clips that have various temporal scales. To relieve this problem, we propose a dual-stage temporal perception module (DTPM) by leveraging the strengths of both temporal convolutions and transformers, which follows a hierarchical structure with dual stages aimed at capturing richer and more comprehensive temporal features. Specifically, each stage for DTPM is cleverly composed of two parts: a multi-scale local temporal module (MS-LTM), followed by a set of global-local temporal modules (GLTMs), where each GLTM can be further decomposed into a global temporal relational module (GTRM) and a local temporal relational module (LTRM). At each stage, an MS-LTM is first employed to model multi-scale local temporal relations and then utilize a set of GLTMs to model global temporal relations and strengthen local temporal relations. We finally aggregate the output features of each stage to form a video feature representation with rich semantic information. Extensive experiments on three CSLR benchmarks, PHOENIX14 (Koller et al. Comput Vis Image Underst 141:108-125, 2015), PHOENIX14-T (Camgoz et al., in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 7784-7793, 2018), and CSL (Huang et al., in: Proceedings of the AAAI conference on artificial intelligence, pp 32, 2018), validate the effectiveness of our proposed method.
C1 [Huang, Zhigang; Xue, Wanli; Zhou, Yuxi; Chen, Shengyong] Tianjin Univ Technol, Sch Comp Sci & Engn, 399 BinShuiXi Rd, Tianjin 300384, Peoples R China.
   [Sun, Jinlu] Tiangong Univ, Sch Elect & Informat Engn, 399 BinShuiXi Rd, Tianjin 300387, Peoples R China.
   [Yuan, Tiantian] Tianjin Univ Technol, Tech Coll Deaf, 391 BinShuiXi Rd, Tianjin 300384, Peoples R China.
   [Xue, Wanli; Wu, Yazhou] Shaanxi Digital Mapping Informat Technol Co Ltd, 10 ZhangBaYi Rd, Xian 710075, Shaanxi, Peoples R China.
C3 Tianjin University of Technology; Tiangong University; Tianjin
   University of Technology
RP Xue, WL; Zhou, YX (corresponding author), Tianjin Univ Technol, Sch Comp Sci & Engn, 399 BinShuiXi Rd, Tianjin 300384, Peoples R China.; Xue, WL (corresponding author), Shaanxi Digital Mapping Informat Technol Co Ltd, 10 ZhangBaYi Rd, Xian 710075, Shaanxi, Peoples R China.
EM xuewanli@email.tjut.edu.cn; yxz@email.tjut.edu.cn
OI Zhou, Yuxi/0000-0001-6875-726X
FU Mission on Nano Science and Technology
FX No Statement Available
CR Abu Farha Y, 2019, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2019.00369
   Adaloglou N, 2022, IEEE T MULTIMEDIA, V24, P1750, DOI 10.1109/TMM.2021.3070438
   Camgoz NC, 2017, IEEE I CONF COMP VIS, P3075, DOI 10.1109/ICCV.2017.332
   Camgoz NC, 2018, PROC CVPR IEEE, P7784, DOI 10.1109/CVPR.2018.00812
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Cui RP, 2019, IEEE T MULTIMEDIA, V21, P1880, DOI 10.1109/TMM.2018.2889563
   Cui RP, 2017, PROC CVPR IEEE, P1610, DOI 10.1109/CVPR.2017.175
   Dai R, 2022, PROC CVPR IEEE, P20009, DOI 10.1109/CVPR52688.2022.01941
   Dai R, 2021, IEEE WINT CONF APPL, P2969, DOI 10.1109/WACV48630.2021.00301
   Dai YM, 2021, IEEE WINT CONF APPL, P3559, DOI 10.1109/WACV48630.2021.00360
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dreuw P, 2008, SIXTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, LREC 2008, P1115
   Forster J, 2014, LREC 2014 - NINTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P1911
   Fu L, 2022, arXiv
   Girdhar R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13485, DOI 10.1109/ICCV48922.2021.01325
   Graves A, 2006, ICML, P369, DOI DOI 10.1145/1143844.1143891
   Guo D, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P744
   Guo LM, 2023, PROC CVPR IEEE, P10771, DOI 10.1109/CVPR52729.2023.01037
   Han K, 2021, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.2103.00112
   Hao AM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11283, DOI 10.1109/ICCV48922.2021.01111
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J., 2018, SQUEEZE AND EXCITATI, P7132
   Hu L., 2022, arXiv
   Hu LY, 2022, LECT NOTES COMPUT SC, V13695, P511, DOI 10.1007/978-3-031-19833-5_30
   Huang J, 2018, AAAI CONF ARTIF INTE, P2257
   Ka Leong Cheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P697, DOI 10.1007/978-3-030-58586-0_41
   Kingma D. P., 2014, arXiv
   Koller O, 2017, PROC CVPR IEEE, P3416, DOI 10.1109/CVPR.2017.364
   Koller O, 2015, COMPUT VIS IMAGE UND, V141, P108, DOI 10.1016/j.cviu.2015.09.013
   Li HB, 2020, INT CONF ACOUST SPEE, P2348, DOI [10.1109/icassp40776.2020.9054316, 10.1109/ICASSP40776.2020.9054316]
   Li SY, 2019, ADV NEUR IN, V32
   Liu YC, 2021, Arxiv, DOI arXiv:2111.12419
   Min YC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11522, DOI 10.1109/ICCV48922.2021.01134
   Ning X, 2024, INFORM FUSION, V102, DOI 10.1016/j.inffus.2023.102033
   Ning X, 2021, IEEE T CIRC SYST VID, V31, P3391, DOI 10.1109/TCSVT.2020.3043026
   Pu JF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1497, DOI 10.1145/3394171.3413931
   Pu JF, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P885
   Pu JF, 2019, PROC CVPR IEEE, P4160, DOI 10.1109/CVPR.2019.00429
   Radford, 2018, OPENAI BLOG
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tian CW, 2024, CAAI T INTELL TECHNO, DOI 10.1049/cit2.12297
   Tian CW, 2024, INFORM FUSION, V102, DOI 10.1016/j.inffus.2023.102043
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang LM, 2021, PROC CVPR IEEE, P1895, DOI 10.1109/CVPR46437.2021.00193
   Wang S, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1483, DOI 10.1145/3240508.3240671
   Wang Y, 2023, Vis. Intell, V1, P25, DOI [10.1007/s44267-023-00027-6, DOI 10.1007/S44267-023-00027-6]
   Wang ZW, 2021, PROC CVPR IEEE, P13209, DOI 10.1109/CVPR46437.2021.01301
   Wei CC, 2021, IEEE T CIRC SYST VID, V31, P1138, DOI 10.1109/TCSVT.2020.2999384
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Xue W., 2023, Vis. Intell, V1, P26, DOI [10.1007/s44267-023-00028-5, DOI 10.1007/S44267-023-00028-5]
   Xue WL, 2024, IEEE T CONSUM ELECTR, V70, P535, DOI 10.1109/TCE.2023.3342163
   Yang CY, 2020, PROC CVPR IEEE, P588, DOI 10.1109/CVPR42600.2020.00067
   Yang T, 2023, Arxiv, DOI arXiv:2210.04020
   Yang ZY, 2019, Arxiv, DOI arXiv:1908.01341
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang ZH, 2019, IEEE IMAGE PROC, P285, DOI [10.1109/icip.2019.8802972, 10.1109/ICIP.2019.8802972]
   Zhao QJ, 2019, AAAI CONF ARTIF INTE, P9259
   Zhao W, 2024, Vis. Intell, V2, P1, DOI [10.1007/s44267-024-00037-y, DOI 10.1007/S44267-024-00037-Y]
   Zhe Niu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P172, DOI 10.1007/978-3-030-58517-4_11
   Zhou H, 2020, AAAI CONF ARTIF INTE, V34, P13009
   Zhou H, 2019, IEEE INT CON MULTI, P1282, DOI 10.1109/ICME.2019.00223
   Zuo RL, 2022, PROC CVPR IEEE, P5121, DOI 10.1109/CVPR52688.2022.00507
NR 66
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 8
PY 2024
DI 10.1007/s00371-024-03516-x
EA JUN 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TO9G8
UT WOS:001242314900002
DA 2024-08-05
ER

PT J
AU Zhu, SY
   Fang, C
   Yu, P
   Zhai, X
   Hao, AM
   Pan, JJ
AF Zhu, Siyan
   Fang, Cheng
   Yu, Peng
   Zhai, Xiao
   Hao, Aimin
   Pan, Junjun
TI Efficient frictional contacts for soft body dynamics via ADMM
SO VISUAL COMPUTER
LA English
DT Article
DE Physics-based animation; Elastic Dynamics; Frictional Contact handling
ID SIMULATION
AB This paper addresses the longstanding challenge of soft body dynamics with frictional contact through a novel combination of Projective Dynamics for elasticity simulation and Alternating Direction Method of Multipliers for frictional contact handling. The approach utilizes parallel local strain projection for deformable bodies and nonlinear Projected Gauss-Seidel for contact with Coulomb friction, consolidated by a pre-factorized global strain propagation step. Integration of contact stabilization, Matchstick anisotropic friction, and Rayleigh damping enhances reliability and usability. Effectiveness, accuracy, and computational efficiency are demonstrated in challenging cases, including multi-layer and persistent contacts. With a CPU-based parallel implementation, our method achieves visually plausible and stable simulation results at an interactive framerate in moderate-scale scenes, showcasing its applicability across various graphics applications.
C1 [Zhu, Siyan; Fang, Cheng; Yu, Peng; Hao, Aimin; Pan, Junjun] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Zhai, Xiao] Weta FX, Wellington, New Zealand.
C3 Beihang University
RP Yu, P; Pan, JJ (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.; Zhai, X (corresponding author), Weta FX, Wellington, New Zealand.
EM yupeng@buaa.edu.cn; zhaixiao43@gmail.com; pan_junjun@buaa.edu.cn
OI Yu, Peng/0000-0002-8652-2744
FU Natural Science Foundation of China; National Key R &D Program of China
   [2023YFC3604505]; Postdoctoral Fellowship Program of CPSF [GZC20233375];
   Beijing Natural Science Foundation [L232065]; CAS Interdisciplinary
   Project [JCTD- 2020-11];  [U20A20195];  [62272017];  [62172437]
FX This work was supported in part by the National Key R &D Program of
   China (no. 2023YFC3604505), Natural Science Foundation of China (no.
   U20A20195, 62272017, 62172437), the Postdoctoral Fellowship Program of
   CPSF under grant number GZC20233375, Beijing Natural Science Foundation
   (L232065), CAS Interdisciplinary Project (JCTD- 2020-11).
CR Agudo A, 2022, IEEE T PATTERN ANAL, V44, P519, DOI 10.1109/TPAMI.2020.3008276
   Andrews S., 2022, ACM SIGGRAPH 2022 CO, P1
   BARAFF D, 1992, COMP GRAPH, V26, P303, DOI 10.1145/142920.134084
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Bertails-Descoubes F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899410
   Bouaziz S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601116
   Boyd N., 2010, FOUND TRENDS MACH LE, V3, P1, DOI [10.1561/2200000016, DOI 10.1561/2200000016]
   Chan SH, 2017, IEEE T COMPUT IMAG, V3, P84, DOI 10.1109/TCI.2016.2629286
   Chen Y.L., 2023, P ACM SIGGRAPHEUROGR, P1
   Daviet G, 2023, PROCEEDINGS OF SIGGRAPH 2023 CONFERENCE PAPERS, SIGGRAPH 2023, DOI 10.1145/3588432.3591551
   Daviet G, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392439
   Deul C, 2016, COMPUT ANIMAT VIRT W, V27, P103, DOI 10.1002/cav.1614
   Erleben K, 2020, COMPUT GRAPH FORUM, V39, P450, DOI 10.1111/cgf.13885
   Fang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322968
   Frâncu M, 2017, COMPUT GRAPH-UK, V69, P12, DOI 10.1016/j.cag.2017.09.004
   Gast TF, 2015, IEEE T VIS COMPUT GR, V21, P1103, DOI 10.1109/TVCG.2015.2459687
   Grigorev A, 2023, PROC CVPR IEEE, P16965, DOI 10.1109/CVPR52729.2023.01627
   Han H, 2022, STOCH ENV RES RISK A, V36, P2153, DOI 10.1007/s00477-021-01993-3
   Lan L, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530069
   Lee J.S., 2023, ARXIV
   Li J., 2018, P 14 WORKSH VIRT REA, P29, DOI [10.2312/vriphys.20181065, DOI 10.2312/VRIPHYS.20181065]
   Li J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201308
   Li M., 2021, ACM T GRAPHIC, V40
   Li MC, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392425
   Ly M, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392396
   Macklin M, 2020, COMPUT GRAPH FORUM, V39, P89, DOI 10.1111/cgf.14104
   Macklin M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3338695
   Macklin M, 2014, ACM T GRAPHIC, V33, DOI [10.1145/280/109/2601152, 10.1145/2601097.2601152]
   Macklin Miles, 2016, P 9 INT C MOTION GAM, P49, DOI 10.1145/2994258.2994272
   Müller M, 2007, J VIS COMMUN IMAGE R, V18, P109, DOI 10.1016/j.jvcir.2007.01.005
   Muller M, 2020, COMPUT GRAPH FORUM, V39, P101, DOI 10.1111/cgf.14105
   Narain R., 2016, S COMPUTER ANIMATION, V1
   Otaduy MA, 2009, COMPUT GRAPH FORUM, V28, P559, DOI 10.1111/j.1467-8659.2009.01396.x
   Peiret A, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355621
   Peng YJ, 2018, ACTA OCEANOL SIN, V37, P1, DOI [10.1007/s13131-018-1188-2, 10.1145/3197517.3201290]
   Servin M., 2006, P SIGRAD C, P22, DOI DOI 10.1111/CGF.12272
   Shi A, 2023, P ACM COMPUT GRAPH, V6, DOI 10.1145/3606934
   Smith Breannan, 2018, ACM Transactions on Graphics, V37, DOI 10.1145/3180491
   Tasora A, 2021, INT J NUMER METH ENG, V122, P4093, DOI 10.1002/nme.6693
   Terzopoulos D., 1987, Computer Graphics, V21, P205, DOI 10.1145/37402.37427
   Tournier M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766969
   Verschoor M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3209887
   Wang HM, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980236
   Zhang JY, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356491
NR 44
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4569
EP 4583
DI 10.1007/s00371-024-03438-8
EA MAY 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001228186500002
DA 2024-08-05
ER

PT J
AU Liu, JJ
   Liu, JL
   Jiang, RX
   Gu, BX
   Chen, YW
   Shen, C
AF Liu, Junjie
   Liu, Junlong
   Jiang, Rongxin
   Gu, Boxuan
   Chen, Yaowu
   Shen, Chen
TI Boosted verification using siamese neural network with DiffBlock
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Static feature; Discriminative information; Verification; Siamese neural
   network; DiffBlock
AB On face recognition, person and vehicle re-identification tasks, different networks and losses have been proposed to learn better features, which further maximizes the decision margin in the feature space. Despite the promising progress having been made, it still remains a challenge to discriminate the different but similar targets while recognizing the same but dissimilar objects, which results from the contradiction between the information retention and the intra-/inter-class distance optimization in the static feature representation methods. The similarity of the static features is insufficient to represent the relationship between diverse images. In this paper, a novel DiffBlock module is proposed to compare the pairwise intermediate features and amplify the difference between the samples. Then SNND (siamese neural network with DiffBlock) is proposed to progressively dig out the discriminative information and judge the relationship between the samples precisely. Extensive experiments on multiple benchmarks for face, person and vehicle verification show that our proposed SNND significantly outperforms previous state-of-the-art methods.
C1 [Liu, Junjie] Zhejiang Univ, Hangzhou, Peoples R China.
   [Gu, Boxuan; Chen, Yaowu] Zhejiang Univ, Embedded Syst Engn Res Ctr, Minist Educ China, Hangzhou, Peoples R China.
   [Jiang, Rongxin] Zhejiang Prov Key Lab Network Multimedia Technol, Hangzhou, Peoples R China.
   [Liu, Junjie; Liu, Junlong; Shen, Chen] Alibaba DAMO Acad, Hangzhou, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Jiang, RX (corresponding author), Zhejiang Prov Key Lab Network Multimedia Technol, Hangzhou, Peoples R China.
EM rongxinj@zju.edu.cn
FU National Key R &D Program of China; Fundamental Research Funds for the
   Central Universities; Alibaba Group through Alibaba Research Intern
   Program;  [2020AAA0103901]
FX This work was supported in part by the National Key R &D Program of
   China under Grant 2020AAA0103901; in part by the Fundamental Research
   Funds for the Central Universities; and in part by Alibaba Group through
   Alibaba Research Intern Program.
CR Bengio Y., 2009, P INT C MACHINE LEAR
   Boutros F, 2022, IEEE COMPUT SOC CONF, P1577, DOI 10.1109/CVPRW56347.2022.00164
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Cheng YT, 2020, INT CONF ACOUST SPEE, P1928, DOI [10.1109/icassp40776.2020.9053328, 10.1109/ICASSP40776.2020.9053328]
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Chung D, 2017, IEEE I CONF COMP VIS, P1992, DOI 10.1109/ICCV.2017.218
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28
   Duta IC, 2021, INT C PATT RECOG, P9415, DOI 10.1109/ICPR48806.2021.9412193
   Furui S, 2012, IEEE SIGNAL PROC MAG, V29, P16, DOI 10.1109/MSP.2012.2209906
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   He A, 2018, P EUROPEAN C COMPUTE
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7
   Huang G.B., 2008, WORKSH FAC REAL LIF
   Huang S, 2023, IEEE T CYBERNETICS, V53, P6173, DOI 10.1109/TCYB.2022.3163707
   Johnson J, 2021, IEEE T BIG DATA, V7, P535, DOI 10.1109/TBDATA.2019.2921572
   Kemelmacher-Shlizerman I, 2016, PROC CVPR IEEE, P4873, DOI 10.1109/CVPR.2016.527
   Kim M, 2022, PROC CVPR IEEE, P18729, DOI 10.1109/CVPR52688.2022.01819
   Krizhevsky A, 2009, CIFAR-10 dataset
   Lin YS, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3469288
   Liu W., 2017, P IEEE C COMPUTER VI
   Liu XC, 2016, IEEE INT CON MULTI
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lou YH, 2019, PROC CVPR IEEE, P3230, DOI 10.1109/CVPR.2019.00335
   McLaughlin N, 2016, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2016.148
   Mery D, 2022, IEEE WINT CONF APPL, P1194, DOI 10.1109/WACV51458.2022.00126
   Moschoglou S, 2017, IEEE COMPUT SOC CONF, P1997, DOI 10.1109/CVPRW.2017.250
   Norouzi M., 2012, P 25 INT C NEURAL IN, V25
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sengupta S., 2016, 2016 IEEE Winter Conference on Applications of Computer Vision (WACV), P1, DOI DOI 10.1109/WACV.2016.7477558
   Shen C, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1942, DOI 10.1145/3123266.3123452
   Shen JB, 2020, IEEE T CYBERNETICS, V50, P3068, DOI 10.1109/TCYB.2019.2936503
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tan M., 2021, P INT C MACHINE LEAR, DOI DOI 10.48550/ARXIV.2104.00298
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang Q, 2018, PROC CVPR IEEE, P4854, DOI 10.1109/CVPR.2018.00510
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Yang Lei, 2020, P IEEECVF C COMPUTER, P13369, DOI DOI 10.1109/CVPR42600.2020.01338
   Yi D, 2014, Arxiv, DOI [arXiv:1411.7923, DOI 10.48550/ARXIV.1411.7923]
   Yu J, 2020, IEEE INT CONF AUTOMA, P882, DOI 10.1109/FG47880.2020.00136
   Yu J, 2020, IEEE INT CONF AUTOMA, P892, DOI 10.1109/FG47880.2020.00127
   Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng M, 2019, PROC CVPR IEEE, P5728, DOI 10.1109/CVPR.2019.00588
   Zheng T., 2018, Technical Report 18-01
   Zheng TY, 2017, Arxiv, DOI arXiv:1708.08197
   Zheng Z, 2019, P IEEE C COMPUTER VI, ppp2138
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541
   Zhu XK, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3531014
NR 59
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 2
PY 2024
DI 10.1007/s00371-024-03318-1
EA APR 2024
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MP7B8
UT WOS:001194882500001
DA 2024-08-05
ER

PT J
AU Wang, XB
   Liu, PF
   Xiang, S
   Weng, YK
   Yao, MH
AF Wang, Xianbao
   Liu, Pengfei
   Xiang, Sheng
   Weng, Yangkai
   Yao, Minghai
TI Search on dual-space: discretization accuracy-based architecture search
   for person re-identification
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Person re-identification; Neural architecture search; Search space;
   Multi-scale feature fusion
AB Network architectures automatically generated for person re-identification (re-ID) using neural architecture search (NAS) algorithms exhibit unique advantages. However, existing NAS algorithms are primarily designed to solve the image classification task, and person re-ID, as a sub-problem of image retrieval, differs significantly from classification. To address this issue, we propose a neural architecture search method that leverages dual space to tackle the problem of person re-identification. The neural network discovered through this approach is named DSSNet. In our approach, the dual-space framework comprises two distinct subspaces, each housing a specialized re-ID module dedicated to extracting crucial pedestrian information. By integrating the knowledge of person re-identification into the neural architecture search process, DSSNet achieves superior performance and robustness in re-ID tasks. Moreover, traditional gradient-based NAS methods associate the intensity of operations with continuous architecture parameters during the search process, leading to network degradation. To enhance the accuracy of the search, we propose a novel architecture selection method based on discretization accuracy. The method optimizes the selection of architectures by considering their performance at a discrete precision level. In addition, we introduce a retrieval loss to guide the architecture in learning the similarity or dissimilarity between two pedestrian objects. Our approach significantly improves the accuracy of the search process without the need for human intervention. Extensive experiments demonstrate that our final architecture outperforms state-of-the-art re-ID models on three benchmark datasets, showcasing its superior performance in the re-ID task.
C1 [Wang, Xianbao; Liu, Pengfei; Xiang, Sheng; Weng, Yangkai; Yao, Minghai] Zhejiang Univ Technol, Coll Informat Engn, Hangzhou 310023, Zhejiang Provin, Peoples R China.
C3 Zhejiang University of Technology
RP Xiang, S (corresponding author), Zhejiang Univ Technol, Coll Informat Engn, Hangzhou 310023, Zhejiang Provin, Peoples R China.
EM xiangsheng@zjut.edu.cn
OI xiang, sheng/0000-0003-0547-7222
FU Innovative Research Group Project of the National Natural Science
   Foundation of China
FX No Statement Available
CR Ahmed E, 2015, PROC CVPR IEEE, P3908, DOI 10.1109/CVPR.2015.7299016
   Baker B, 2017, Arxiv, DOI [arXiv:1611.02167, DOI 10.48550/ARXIV.1611.02167.MODE]
   Chen BH, 2019, IEEE I CONF COMP VIS, P371, DOI 10.1109/ICCV.2019.00046
   Chen X., 2020, PR MACH LEARN RES
   Chen YHS, 2021, PROC CVPR IEEE, P587, DOI 10.1109/CVPR46437.2021.00065
   Dong XY, 2019, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2019.00186
   Fu CY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11803, DOI 10.1109/ICCV48922.2021.01161
   Fu Y, 2019, AAAI CONF ARTIF INTE, P8295
   Gu HY, 2021, NEUROCOMPUTING, V435, P53, DOI 10.1016/j.neucom.2020.12.105
   Guan'an Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P275, DOI 10.1007/978-3-030-58598-3_17
   Guo QB, 2022, PATTERN RECOGN, V126, DOI 10.1016/j.patcog.2021.108448
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou RB, 2022, IEEE T PATTERN ANAL, V44, P4894, DOI 10.1109/TPAMI.2021.3079910
   Hou RB, 2019, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2019.00954
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang BL, 2022, IET IMAGE PROCESS, V16, P2001, DOI 10.1049/ipr2.12465
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jia ZQ, 2023, VISUAL COMPUT, V39, P1205, DOI 10.1007/s00371-022-02398-1
   Lei Z, 2021, ALGORITHMS, V14, DOI 10.3390/a14050137
   Li HJ, 2021, PROC CVPR IEEE, P6725, DOI 10.1109/CVPR46437.2021.00666
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lingxiao He, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P357, DOI 10.1007/978-3-030-58604-1_22
   Liu HX, 2019, Arxiv, DOI [arXiv:1806.09055, 10.48550/arXiv.1806.09055]
   Liu ZM, 2019, IEEE I CONF COMP VIS, P6121, DOI 10.1109/ICCV.2019.00622
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Pham H, 2018, PR MACH LEARN RES, V80
   Qian XL, 2018, LECT NOTES COMPUT SC, V11213, P661, DOI 10.1007/978-3-030-01240-3_40
   Quan RJ, 2019, IEEE I CONF COMP VIS, P3749, DOI 10.1109/ICCV.2019.00385
   Real E, 2017, PR MACH LEARN RES, V70
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Shang Gao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11741, DOI 10.1109/CVPR42600.2020.01176
   Shizhou Zhang, 2019, Pattern Recognition and Computer Vision. Second Chinese Conference, PRCV 2019. Proceedings. Lecture Notes in Computer Science (LNCS 11857), P540, DOI 10.1007/978-3-030-31654-9_46
   Si JL, 2018, PROC CVPR IEEE, P5363, DOI 10.1109/CVPR.2018.00562
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wang GA, 2020, PROC CVPR IEEE, P6448, DOI 10.1109/CVPR42600.2020.00648
   Wang RC, 2021, Arxiv, DOI arXiv:2108.04392
   Wang Y, 2018, PROC CVPR IEEE, P8042, DOI [10.1109/CVPR.2018.00839, 10.1109/CVPR.2018.00736]
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu GL, 2022, PATTERN RECOGN, V121, DOI 10.1016/j.patcog.2021.108239
   Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140
   Xu YH, 2021, IEEE T PATTERN ANAL, V43, P2953, DOI 10.1109/TPAMI.2021.3059510
   Yang WJ, 2019, PROC CVPR IEEE, P1389, DOI 10.1109/CVPR.2019.00148
   Yang X, 2018, IEEE T IMAGE PROCESS, V27, P791, DOI 10.1109/TIP.2017.2765836
   Yao QM, 2020, AAAI CONF ARTIF INTE, V34, P6664
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Zela A, 2020, Arxiv, DOI arXiv:1909.09656
   Zheng L, 2016, Arxiv, DOI arXiv:1610.02984
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng M, 2019, PROC CVPR IEEE, P5728, DOI 10.1109/CVPR.2019.00588
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhou QQ, 2022, IEEE T NEUR NET LEAR, V33, P6627, DOI 10.1109/TNNLS.2021.3082701
   Zoph B, 2017, Arxiv, DOI arXiv:1611.01578
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
NR 58
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 28
PY 2024
DI 10.1007/s00371-024-03308-3
EA MAR 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MO7S7
UT WOS:001194634300003
DA 2024-08-05
ER

PT J
AU Li, Y
   Zhang, W
   Wu, ML
   Zhang, D
   Wang, ZG
   You, CJ
AF Li, Ye
   Zhang, Wu
   Wu, Meiling
   Zhang, Di
   Wang, Zhiguo
   You, Changjiang
TI Multi-keypoints matching network for clothing detection
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Clothing detection; Multi-keypoints matching; Bottom-up
AB Clothing detection is a hot research focus as its application of identifying the specific category of clothing, such as long-sleeved and short-sleeved. Image-based clothing detection requires the model to detect accurate position. At present, the approaches of clothing detection are mainly divided into two categories: one is top-down, which is anchor-based and needs to calculate the intersection over union between the anchor box and the bounding box, but it is limited by the setting of the anchor box and does not perform well when the clothing scale is variable; the other is bottom-up, which uses the feature extraction network to get the keypoints and calculates the position and size of the clothing, but the prediction of the keypoints often has a slight error for it lacks the internal information of the clothing. To address the above issues, we propose a multi-keypoints matching network for clothing detection (MKMnet) based on the bottom-up method. It detects three keypoints (top-left corner point, bottom-right corner point, and center point) to ensure high detecting accuracy. Firstly, we perform corner keypoint matching by calculating the distance between the embedding vectors of different corner points to get the initial bounding box, then we get the final bounding box by matching the center point. The way to get the bounding box by corner point matching makes the model have the ability to detect clothing of any scale and shape, and adding the center point for further verification eliminates a large number of false-positive bounding boxes. The MKMnet proposed in this paper can obtain the bounding boxes accurately through the linear combination of the center point, and improving the accuracy of clothing recognition. The experimental results show that the MKMnet has higher accuracy than existing methods.
C1 [Li, Ye; Zhang, Wu; Wu, Meiling; Zhang, Di; Wang, Zhiguo; You, Changjiang] Univ Elect Sci & Technol China, Chengdu, Peoples R China.
   [Li, Ye] Kashi Inst Elect & Informat Ind, Kashi, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Li, Y (corresponding author), Univ Elect Sci & Technol China, Chengdu, Peoples R China.; Li, Y (corresponding author), Kashi Inst Elect & Informat Ind, Kashi, Peoples R China.
EM liyeye@uestc.edu.cn; 946143870@qq.com; meilingwu9967@163.com;
   uestc_zhd@163.com; zgwang@uestc.edu.cn; cjyou@uestc.edu.cn
RI Wu, Mei-Ling/E-9222-2010
FU Natural Science Foundation of Xinjiang Province
FX No Statement Available
CR Chen HZ, 2012, LECT NOTES COMPUT SC, V7574, P609, DOI 10.1007/978-3-642-33712-3_44
   Chen K., 2019, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, P4974, DOI DOI 10.1109/CVPR.2019.00511
   Chen MH, 2019, IEEE I CONF COMP VIS, P2090, DOI 10.1109/ICCV.2019.00218
   Duan K., 2022, arXiv
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Ge YY, 2019, PROC CVPR IEEE, P5332, DOI 10.1109/CVPR.2019.00548
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ji X, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1654, DOI 10.1145/3123266.3123429
   Kiapour MH, 2015, IEEE I CONF COMP VIS, P3343, DOI 10.1109/ICCV.2015.382
   Kim HJ, 2021, IEEE ACCESS, V9, P11694, DOI 10.1109/ACCESS.2021.3051424
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Liao LZ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1571, DOI 10.1145/3240508.3240646
   Lin T.-Y., 2017, PROC CVPR IEEE, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TH, 2020, Arxiv, DOI arXiv:2005.00419
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Majuran S, 2023, VISUAL COMPUT, V39, P6609, DOI 10.1007/s00371-022-02751-4
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Shajini M, 2021, VISUAL COMPUT, V37, P1517, DOI 10.1007/s00371-020-01885-7
   Sidnev A., 2020, Deepmark++: Centernet-based clothing detection
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Tian Q, 2023, MULTIMED TOOLS APPL, V82, P7383, DOI 10.1007/s11042-022-13424-8
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang WG, 2018, PROC CVPR IEEE, P4271, DOI 10.1109/CVPR.2018.00449
   Yamaguchi K, 2013, IEEE I CONF COMP VIS, P3519, DOI 10.1109/ICCV.2013.437
   Yan SJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P172, DOI 10.1145/3123266.3123276
   Yang W, 2014, PROC CVPR IEEE, P3182, DOI 10.1109/CVPR.2014.407
   Yang Z, 2019, IEEE I CONF COMP VIS, P9656, DOI 10.1109/ICCV.2019.00975
   Zheng S, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1670, DOI 10.1145/3240508.3240652
NR 29
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 25
PY 2024
DI 10.1007/s00371-024-03337-y
EA MAR 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LY8O6
UT WOS:001190466900001
DA 2024-08-05
ER

PT J
AU Li, CL
   Da, F
AF Li, Chunlu
   Da, Feipeng
TI Refined dense face alignment through image matching
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Face alignment; Face reconstruction; Dense geometric supervision;
   Outlier mixup
ID BLIND QUALITY ASSESSMENT; RECOGNITION; RECONSTRUCTION; EXPRESSION
AB Face alignment is the foundation of building 3D avatars for virtue communication in the metaverse, human-computer interaction, AI-generated content, etc., and therefore, it is critical that face deformation is reflected precisely to better convey expression, pose and identity. However, misalignment exists in the currently best methods that fit a face model to a target image and can be easily captured by human perception, thus degrading the reconstruction quality. The main reason is that the widely used metrics for training, including the landmark re-projection loss, pixel-wise loss and perception-level loss, are insufficient to address the misalignment and suffer from ambiguity and local minimums. To address misalignment, we propose an image MAtchinG-driveN dEnse geomeTrIC supervision (MAGNETIC). Specifically, we treat face alignment as a matching problem and establish pixel-wise correspondences between the target and rendered images. Then reconstructed facial points are guided towards their corresponding points on the target image, thus improving reconstruction. Synthesized image pairs are mixed up with face outliers to simulate the target and rendered images with ground-truth pixel-wise correspondences to enable the training of a robust prediction network. Compared with existing methods that turn to 3D scans for dense geometric supervision, our method reaches comparable shape reconstruction results with much lower effort. Experimental results on the NoW testset show that we reach the state-of-the-art among all self-supervised methods and even outperform methods using photo-realistic images. We also achieve comparable results with the state-of-the-art on the benchmark of Feng et al. Codes will be available at: github.com/ChunLLee/ReconstructionFromMatching.
C1 [Li, Chunlu; Da, Feipeng] Southeast Univ, Sch Automat, Sipailou, Nanjing 210096, Jiangsu, Peoples R China.
C3 Southeast University - China
RP Da, F (corresponding author), Southeast Univ, Sch Automat, Sipailou, Nanjing 210096, Jiangsu, Peoples R China.
EM 230179482@seu.edu.cn; dafp@seu.edu.cn
RI Li, Chunlu/JPX-0940-2023
FU Natural Science Research of Jiangsu Higher Education Institutions of
   China [BK20181269]; Natural Science Foundation of Jiangsu Province of
   China [BK20192004C]; Special Project on Basic Research of Frontier
   Leading Technology of Jiangsu Province of China
FX This work is funded by the Natural Science Foundation of Jiangsu
   Province of China (Grant No. BK20181269) and the Special Project on
   Basic Research of Frontier Leading Technology of Jiangsu Province of
   China (Grant No. BK20192004C). We would like to express our sincere
   appreciation to all the sponsors.
CR An Z., 2019, P IEEE CVF C COMP VI
   Baker S, 2007, IEEE I CONF COMP VIS, P588, DOI 10.1109/cvpr.2007.383191
   Bambach S, 2015, IEEE I CONF COMP VIS, P1949, DOI 10.1109/ICCV.2015.226
   Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Chai ZH, 2022, LECT NOTES COMPUT SC, V13668, P74, DOI 10.1007/978-3-031-20074-8_5
   Chhikara P, 2021, IEEE INTERNET THINGS, V8, P6949, DOI 10.1109/JIOT.2020.3037207
   Danecek R, 2022, PROC CVPR IEEE, P20279, DOI 10.1109/CVPR52688.2022.01967
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng JK, 2020, PROC CVPR IEEE, P5202, DOI 10.1109/CVPR42600.2020.00525
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   DeVries T, 2017, Arxiv, DOI arXiv:1708.04552
   Egger B, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3395208
   EKMAN P, 1993, AM PSYCHOL, V48, P384, DOI 10.1037/0003-066X.48.4.384
   Fang Z, 2022, VISUAL COMPUT, V38, P1151, DOI 10.1007/s00371-021-02074-w
   Feng Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459936
   Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33
   Feng Z.H., 2018, Evaluation of dense 3d reconstruction from 2d face images in the wild
   Gan YS, 2024, VISUAL COMPUT, V40, P585, DOI 10.1007/s00371-023-02803-3
   Gerig T, 2018, IEEE INT CONF AUTOMA, P75, DOI 10.1109/FG.2018.00021
   Hongyi Z., 2018, INT C LEARN REPR
   Huang X, 2021, VISUAL COMPUT, V37, P95, DOI 10.1007/s00371-020-01982-7
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Jianzhu Guo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P152, DOI 10.1007/978-3-030-58529-7_10
   Ju YX, 2021, VISUAL COMPUT, V37, P2907, DOI 10.1007/s00371-021-02198-z
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Koizumi Tatsuro, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P690, DOI 10.1007/978-3-030-58536-5_41
   Koujan MR, 2020, PROC CVPR IEEE, P6617, DOI 10.1109/CVPR42600.2020.00665
   Li CL, 2023, Arxiv, DOI arXiv:2106.09614
   Li TY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130813
   Liu Y, 2023, IEEE T PATTERN ANAL, V45, P11624, DOI 10.1109/TPAMI.2023.3284038
   Liu Y, 2022, IEEE T IMAGE PROCESS, V31, P1978, DOI 10.1109/TIP.2022.3147032
   Liu Y, 2020, IEEE T IMAGE PROCESS, V29, P3168, DOI 10.1109/TIP.2019.2957930
   Liu Y, 2019, IEEE T CIRC SYST VID, V29, P2416, DOI 10.1109/TCSVT.2018.2868123
   Liu YJ, 2017, IEEE INT CONF COMP V, P1619, DOI 10.1109/ICCVW.2017.190
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Ma ZY, 2023, PROC CVPR IEEE, P16901, DOI 10.1109/CVPR52729.2023.01621
   Min XK, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3470970
   Min XK, 2019, IEEE T INTELL TRANSP, V20, P2879, DOI 10.1109/TITS.2018.2868771
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P6054, DOI 10.1109/TIP.2020.2988148
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3805, DOI 10.1109/TIP.2020.2966082
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Min XK, 2017, IEEE T IMAGE PROCESS, V26, P5462, DOI 10.1109/TIP.2017.2735192
   Mohaghegh H, 2023, NEUROCOMPUTING, V520, P82, DOI 10.1016/j.neucom.2022.11.048
   Ning X, 2020, IEEE SIGNAL PROC LET, V27, P1944, DOI 10.1109/LSP.2020.3032277
   Onizuka Hayato, 2019, P IEEE CVF INT C COM
   Peng Z, 2023, COMPUT VIS MEDIA, V9, P109, DOI 10.1007/s41095-021-0267-z
   Sanyal S, 2019, PROC CVPR IEEE, P7755, DOI 10.1109/CVPR.2019.00795
   Shang Jiaxiang, 2020, EUROPEAN C COMPUTER, V2360, P53
   Shao ZW, 2020, NEUROCOMPUTING, V396, P477, DOI 10.1016/j.neucom.2018.11.108
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Teed Zachary, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P402, DOI 10.1007/978-3-030-58536-5_24
   Tewari A, 2019, PROC CVPR IEEE, P10804, DOI 10.1109/CVPR.2019.01107
   Tewari A, 2018, PROC CVPR IEEE, P2549, DOI 10.1109/CVPR.2018.00270
   Tewari A, 2017, IEEE I CONF COMP VIS, P3735, DOI 10.1109/ICCV.2017.401
   Trân AT, 2017, PROC CVPR IEEE, P1493, DOI 10.1109/CVPR.2017.163
   Tran L, 2019, PROC CVPR IEEE, P1126, DOI 10.1109/CVPR.2019.00122
   Verma M, 2020, IEEE T IMAGE PROCESS, V29, P1618, DOI 10.1109/TIP.2019.2912358
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wood E, 2022, LECT NOTES COMPUT SC, V13673, P160, DOI 10.1007/978-3-031-19778-9_10
   Wu CY, 2021, INT CONF 3D VISION, P453, DOI 10.1109/3DV53792.2021.00055
   Xu HF, 2022, PROC CVPR IEEE, P8111, DOI 10.1109/CVPR52688.2022.00795
   Yang W., 2023, IEEE Trans. Multimed., V26, P3663
   Yang Y, 2016, NEUROCOMPUTING, V195, P149, DOI 10.1016/j.neucom.2015.08.114
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang L, 2006, IEEE T PATTERN ANAL, V28, P351, DOI 10.1109/TPAMI.2006.53
   Zhang T., 2023, P IEEE CVF INT C COM, P9033
   Zhong YY, 2017, IEEE SIGNAL PROC LET, V24, P1213, DOI 10.1109/LSP.2017.2715076
   Zhou EJ, 2018, LECT NOTES COMPUT SC, V11220, P3, DOI 10.1007/978-3-030-01270-0_1
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
   Zielonka W, 2022, LECT NOTES COMPUT SC, V13673, P250, DOI 10.1007/978-3-031-19778-9_15
NR 74
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 17
PY 2024
DI 10.1007/s00371-024-03316-3
EA MAR 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LI2J4
UT WOS:001186092000001
DA 2024-08-05
ER

PT J
AU Zhao, S
   Gong, ZK
   Zhao, DY
AF Zhao, Shuen
   Gong, Zhikun
   Zhao, Dongyu
TI Traffic signs andmarkings recognition based on lightweight convolutional
   neural network
SO VISUAL COMPUTER
LA English
DT Article
DE Autonomous driving; Traffic signs and markings; Deep learning;
   Lightweight convolutional neural networks
ID ENHANCEMENT
AB Intelligent recognition of traffic signs and markings is an important component of autonomous driving and intelligent transportation systems. It is also an important theoretical basis for autonomous driving path planning. To address the low accuracy and poor real-time performance of the traffic signs and markings detection in complex and multivariate scenes, a lightweight convolutional neural network recognition method in multiple interference scenes is proposed. Firstly, based on Gamma correction and contrast limited histogram equalization algorithm, the traffic signs and markings image under the low illumination condition is enhanced adaptively. Then, the MobileNet-V2 fuse together with DeepLab-V3 + algorithm to segment traffic signs and markings in multiple scenes. Finally, an identification model is established to realize the adaptive recognition of traffic signs and markings, which are based on Lightweight Convolutional Neural Networks (Lw-CNN). Besides, the recognition method proposed in this paper is verified by the CSUST Chinese Traffic Sign Detection Benchmark (CCTSDB). The experimental results show that the Mean Intersection Over Union (MIOU) of the MobileNet-V2 and DeepLab-V3 + algorithm reaches 83.07%, it increased by 25.8% than before image enhancement. The recognition accuracy of algorithm based on lightweight convolutional neural network is 99.92%, higher than the algorithms of MobileNet-V2 and VGG16.
C1 [Zhao, Shuen; Gong, Zhikun; Zhao, Dongyu] Chongqing Jiaotong Univ, Sch Mech & Vehicle Engn, Chongqing 400074, Peoples R China.
C3 Chongqing Jiaotong University
RP Gong, ZK (corresponding author), Chongqing Jiaotong Univ, Sch Mech & Vehicle Engn, Chongqing 400074, Peoples R China.
EM 2496963458@qq.com
OI Zhikun, Gong/0000-0001-5309-0327
FU National Natural Science Foundation of China [52072054]; Special Key
   Project of Chongqing Technology Innovation and Application Development
   [cstc2021jscx-cylh0026]; Open Funding Key Laboratory of Industry and
   Information Technology [2021KFKT01]
FX This work was supported by the National Natural Science Foundation of
   China (52072054), the Special Key Project of Chongqing Technology
   Innovation and Application Development (cstc2021jscx-cylh0026), and the
   Open Funding Key Laboratory of Industry and Information Technology
   (2021KFKT01).
CR Alam A, 2020, INT J INTELL TRANSP, V18, P98, DOI 10.1007/s13177-019-00178-1
   Bar Hillel A, 2014, MACH VISION APPL, V25, P727, DOI 10.1007/s00138-011-0404-2
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Lee S, 2017, IEEE I CONF COMP VIS, P1965, DOI 10.1109/ICCV.2017.215
   Lee S, 2010, IEEE T CONSUM ELECTR, V56, P2636, DOI 10.1109/TCE.2010.5681151
   Li YD, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REAL-TIME COMPUTING AND ROBOTICS (IEEE RCAR), P411, DOI 10.1109/RCAR.2016.7784064
   Liu SG, 2019, IEEE T CONSUM ELECTR, V65, P303, DOI 10.1109/TCE.2019.2893644
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Neven D, 2018, IEEE INT VEH SYM, P286
   Pan XG, 2018, AAAI CONF ARTIF INTE, P7276
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun K, 2019, Arxiv, DOI [arXiv:1904.04514, DOI 10.48550/ARXIV.1904.04514]
   Tang H, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108792
   Tang H, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P610, DOI 10.1145/3394171.3413884
   Tong GF, 2017, C IND ELECT APPL, P2066, DOI 10.1109/ICIEA.2017.8283178
   Wang K, 2022, VISUAL COMPUT, V38, P2329, DOI 10.1007/s00371-021-02115-4
   Wu LX, 2019, J PHYS CONF SER, V1176, DOI 10.1088/1742-6596/1176/3/032045
   [肖进胜 Xiao Jinsheng], 2014, [自动化学报, Acta Automatica Sinica], V40, P697
   You F, 2013, J OPT SOC KOREA, V17, P188, DOI 10.3807/JOSK.2013.17.2.188
   Yu H, 2022, INT CONF ACOUST SPEE, P1456, DOI 10.1109/ICASSP43922.2022.9747597
   Yu NN, 2023, VISUAL COMPUT, V39, P1251, DOI 10.1007/s00371-022-02402-8
   Yu XY, 2023, VISUAL COMPUT, V39, P4165, DOI 10.1007/s00371-022-02582-3
   Zha ZC, 2023, IEEE T CIRC SYST VID, V33, P3947, DOI 10.1109/TCSVT.2023.3236636
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang JM, 2020, IEEE ACCESS, V8, P29742, DOI 10.1109/ACCESS.2020.2972338
   Zhang JM, 2017, ALGORITHMS, V10, DOI 10.3390/a10040127
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
NR 30
TC 3
Z9 3
U1 23
U2 63
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 559
EP 570
DI 10.1007/s00371-023-02801-5
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:001151021700008
DA 2024-08-05
ER

PT J
AU Wang, GQ
   Chen, MS
   Lin, YC
   Tan, XH
   Zhang, CZ
   Yao, WX
   Gao, BH
   Zeng, WD
AF Wang, Guanqiang
   Chen, Mingsong
   Lin, Yongcheng
   Tan, Xianhua
   Zhang, Chizhou
   Yao, Wenxin
   Gao, Baihui
   Zeng, Weidong
TI An efficient parallel fusion structure of distilled and
   transformer-enhanced modules for lightweight image super-resolution
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image processing; Super-resolution; Feature distillation; Parallel
   fusion; Network lightweigh
AB Although the convolution neural network (CNN) and transformer methods have greatly promoted the development of image super-resolution, these two methods have their disadvantages. Making a trade-off between the two methods and effectively integrating their advantages can restore high-frequency information of images with fewer parameters and higher quality. Hence, in this study, a novel dual parallel fusion structure of distilled feature pyramid and serial CNN and transformer (PFDFCT) model is proposed. In one branch, a lightweight serial structure of CNN and transformer is implemented to guarantee the richness of the global features extracted by transformer. In the other branch, an efficient distillation feature pyramid hybrid attention module is designed to efficiently purify the local features extracted by CNN and maintain integrity through cross-fusion. Such a multi-path parallel fusion strategy can ensure the richness and accuracy of features while avoiding the use of complex and long-range structures. The results show that the PFDFCT can reduce the mis-generated stripes and make the reconstructed image clearer for both easy-to-reconstruct and difficult-to-reconstruct targets compared to other advanced methods. Additionally, PFDFCT achieves a remarkable advance in model size and computational cost. Compared to the state-of-the-art (SOTA) model (i.e., efficient long-range attention network) in 2022, PFDFCT reduces parameters and floating point of operations (FLOPs) by more than 20% and 26% under all three scales, while maintaining a similar advanced reconstruction ability. The FLOPs of PFDFCT are as low as 31.8G, 55.3G, and 122.5G under scales of 2, 3 and 4, which are much lower than most current SOTA methods.
C1 [Wang, Guanqiang; Lin, Yongcheng; Tan, Xianhua; Yao, Wenxin; Gao, Baihui; Zeng, Weidong] Cent South Univ, Sch Mech & Elect Engn, Changsha 410083, Peoples R China.
   [Chen, Mingsong; Zhang, Chizhou] Cent South Univ, Light Alloy Res Inst, Changsha 410083, Peoples R China.
   [Wang, Guanqiang; Chen, Mingsong; Lin, Yongcheng; Tan, Xianhua; Yao, Wenxin; Gao, Baihui; Zeng, Weidong] State Key Lab Precis Mfg Extreme Serv Performance, Changsha 410083, Peoples R China.
C3 Central South University; Central South University
RP Lin, YC; Tan, XH (corresponding author), Cent South Univ, Sch Mech & Elect Engn, Changsha 410083, Peoples R China.; Chen, MS; Zhang, CZ (corresponding author), Cent South Univ, Light Alloy Res Inst, Changsha 410083, Peoples R China.; Chen, MS; Lin, YC; Tan, XH (corresponding author), State Key Lab Precis Mfg Extreme Serv Performance, Changsha 410083, Peoples R China.
EM chenms18@csu.edu.cn; yclin@csu.edu.cn; xianhuatan@csu.edu.cn;
   213801003@csu.edu.cn
RI Lin, Yongcheng/HPD-2520-2023
FU National Key Research and Development Program of China [2022YFB3706902];
   Hunan Provincial Natural Science Foundation of China [2022JJ40614]
FX This work was financially supported by the
   NationalKeyResearchandDevelopmentProgram\documentclass[12pt]{minimal}
   \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts}
   \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs}
   \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt}
   \begin{document}$${\text{National}}\;{\text{ Key }}\;{\text{Research
   }}\;{\text{and}}\;{\text{ Development
   }}\;{\text{Program}}$$\end{document} of China under Grant
   2022YFB3706902. Hunan Provincial Natural Science Foundation of China
   under Grant 2022JJ40614.
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Angarano S, 2023, ENG APPL ARTIF INTEL, V123, DOI 10.1016/j.engappai.2023.106407
   Anwar S, 2022, IEEE T PATTERN ANAL, V44, P1192, DOI 10.1109/TPAMI.2020.3021088
   Ben Niu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P191, DOI 10.1007/978-3-030-58610-2_12
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chen H., 2021, P IEEE CVF C COMP VI, P2165
   Chen YT, 2024, VISUAL COMPUT, V40, P489, DOI 10.1007/s00371-023-02795-0
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Fang JS, 2022, IEEE COMPUT SOC CONF, P1102, DOI 10.1109/CVPRW56347.2022.00119
   Gao Q., 2018, AS C COMP VIS, P1103
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Huang S, 2023, VISUAL COMPUT, V39, P3647, DOI 10.1007/s00371-023-02938-3
   Huang ZY, 2021, EXPERT SYST APPL, V169, DOI 10.1016/j.eswa.2020.114450
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Jie Liu, 2020, Proceedings of the 16th European Conference on Computer Vision - ECCV 2020 Workshops. Lecture Notes in Computer Science (LNCS 12537), P41, DOI 10.1007/978-3-030-67070-2_2
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kim JH, 2020, Arxiv, DOI arXiv:1811.12043
   Kingma D. P., 2014, arXiv
   Lan RS, 2021, IEEE T CYBERNETICS, V51, P1443, DOI 10.1109/TCYB.2020.2970104
   Li W., 2020, Advances in Neural Information Processing Systems, V33, P20343
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lin T.-Y., 2017, PROC CVPR IEEE, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Liu YP, 2024, VISUAL COMPUT, V40, P3441, DOI 10.1007/s00371-023-03044-0
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu ZS, 2022, IEEE COMPUT SOC CONF, P456, DOI 10.1109/CVPRW56347.2022.00061
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Niu TZ, 2024, MEASUREMENT, V225, DOI 10.1016/j.measurement.2023.113998
   Park D, 2018, IEEE COMPUT SOC CONF, P995, DOI 10.1109/CVPRW.2018.00133
   Paszke A, 2019, ADV NEUR IN, V32
   Shi WL, 2021, VISUAL COMPUT, V37, P1569, DOI 10.1007/s00371-020-01903-8
   Ullah Z, 2022, ENG APPL ARTIF INTEL, V116, DOI 10.1016/j.engappai.2022.105486
   Wang G., 2024, Displays, V82
   Wang HF, 2022, ENGINEERING-PRC, V18, P143, DOI 10.1016/j.eng.2021.03.023
   Wang HF, 2023, ENGINEERING-PRC, V25, P51, DOI 10.1016/j.eng.2022.04.024
   Wang JX, 2024, VISUAL COMPUT, V40, P2655, DOI 10.1007/s00371-023-02968-x
   Wang L, 2020, PROC CVPR IEEE, P3773, DOI 10.1109/CVPR42600.2020.00383
   Wang Y, 2022, EXPERT SYST APPL, V197, DOI 10.1016/j.eswa.2022.116793
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Xiaotong Luo, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P272, DOI 10.1007/978-3-030-58542-6_17
   Xu BY, 2024, EXPERT SYST, V41, DOI 10.1111/exsy.13092
   Yang AP, 2024, VISUAL COMPUT, V40, P3473, DOI 10.1007/s00371-023-03046-y
   Yang HR, 2023, EXPERT SYST APPL, V226, DOI 10.1016/j.eswa.2023.120159
   Yang X, 2022, EXPERT SYST APPL, V204, DOI 10.1016/j.eswa.2022.117594
   Yang X, 2021, SIGNAL IMAGE VIDEO P, V15, P1397, DOI 10.1007/s11760-021-01870-0
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zeyde R., 2012, INT C CURV SURF, P711
   Zhang XD, 2022, LECT NOTES COMPUT SC, V13677, P649, DOI 10.1007/978-3-031-19790-1_39
   Zhou M, 2020, ENGINEERING-PRC, V6, P275, DOI 10.1016/j.eng.2019.12.014
   Zhou ZT, 2023, DISPLAYS, V76, DOI 10.1016/j.displa.2022.102352
NR 54
TC 1
Z9 1
U1 11
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 22
PY 2024
DI 10.1007/s00371-023-03243-9
EA JAN 2024
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GJ3D1
UT WOS:001152252500001
DA 2024-08-05
ER

PT J
AU Li, LR
   Ding, J
   Cui, H
   Chen, ZQ
   Liao, GS
AF Li, Lirong
   Ding, Jiang
   Cui, Hao
   Chen, Zhiqiang
   Liao, Guisheng
TI LiteMSNet: a lightweight semantic segmentation network with multi-scale
   feature extraction for urban streetscape scenes
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Lightweight network; Urban streetscape scene; Semantic segmentation;
   Dilated convolution; Multi-scale features
AB Semantic segmentation plays a pivotal role in computer scene understanding, but it typically requires a large amount of computing to achieve high performance. To achieve a balance between accuracy and complexity, we propose a lightweight semantic segmentation model, termed LiteMSNet (a Lightweight Semantic Segmentation Network with Multi-Scale Feature Extraction for urban streetscape scenes). In this model, we propose a novel Improved Feature Pyramid Network, which embeds a shuffle attention mechanism followed by a stacked Depth-wise Asymmetric Gating Module. Furthermore, a Multi-scale Dilation Pyramid Module is developed to expand the receptive field and capture multi-scale feature information. Finally, the proposed lightweight model integrates two loss mechanisms, the Cross-Entropy and the Dice Loss functions, which effectively mitigate the issue of data imbalance and gradient saturation. Numerical experimental results on the CamVid dataset demonstrate a remarkable mIoU measurement of 70.85% with less than 5M parameters, accompanied by a real-time inference speed of 66.1 FPS, surpassing the existing methods documented in the literature. The code for this work will be made available at https://github.com/River-ding/LiteMSNet.
C1 [Li, Lirong; Ding, Jiang; Cui, Hao] HuBei Univ Technol, Wuhan 430068, Hubei, Peoples R China.
   [Li, Lirong; Ding, Jiang; Cui, Hao] Hubei Key Lab Efficient Utilizat & Control Energy, Wuhan 430068, Hubei, Peoples R China.
   [Chen, Zhiqiang] Univ Missouri Kansas City, Kansas City, MO 64110 USA.
   [Liao, Guisheng] Xidian Univ, Xian 710126, Shanxi, Peoples R China.
C3 Hubei University of Technology; University of Missouri System;
   University of Missouri Kansas City; Xidian University
RP Ding, J (corresponding author), HuBei Univ Technol, Wuhan 430068, Hubei, Peoples R China.; Ding, J (corresponding author), Hubei Key Lab Efficient Utilizat & Control Energy, Wuhan 430068, Hubei, Peoples R China.
EM rongli@hbut.edu.cn; 102100254@hbut.edu.cn; 102210256@hbut.edu.cn;
   chenzhiq@umkc.edu; liaogs@xidian.edu.cn
FU National Natural Science Foundation of China [62071172]; Development
   Fund of Hubei Provincial Key Laboratory of Efficient Utilization and
   Control of Energy Storage Operation [HBSEES202306]
FX This study was supported by the National Natural Science Foundation of
   China (62071172) and the Development Fund of Hubei Provincial Key
   Laboratory of Efficient Utilization and Control of Energy Storage
   Operation (HBSEES202306).
CR Abu Alhaija H, 2018, INT J COMPUT VISION, V126, P961, DOI 10.1007/s11263-018-1070-x
   Taghanaki SA, 2021, ARTIF INTELL REV, V54, P137, DOI 10.1007/s10462-020-09854-1
   Chen BK, 2019, IEEE T INTELL TRANSP, V20, P137, DOI 10.1109/TITS.2018.2801309
   Chen L.-C., 2018, P EUR C COMP VIS ECC, P801, DOI DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Chung JY, 2014, Arxiv, DOI [arXiv:1412.3555, DOI 10.48550/ARXIV.1412.3555]
   Dong GS, 2021, IEEE T INTELL TRANSP, V22, P3258, DOI 10.1109/TITS.2020.2980426
   Elhassan MAM, 2024, Arxiv, DOI arXiv:2206.07298
   Feng D, 2021, IEEE T INTELL TRANSP, V22, P1341, DOI 10.1109/TITS.2020.2972974
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Guo X, 2020, NEUROCOMPUTING, V394, P127, DOI 10.1016/j.neucom.2019.01.115
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong YD, 2021, Arxiv, DOI arXiv:2101.06085
   Li G, 2019, Arxiv, DOI arXiv:1907.11357
   Li X, 2019, IEEE T INTELL TRANSP, V20, P2072, DOI 10.1109/TITS.2018.2857566
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lin ZK, 2023, VISUAL COMPUT, V39, P597, DOI 10.1007/s00371-021-02360-7
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mehta S, 2018, LECT NOTES COMPUT SC, V11214, P561, DOI 10.1007/978-3-030-01249-6_34
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Poudel R. P. K., 2019, arXiv
   Radosavovic I., 2020, P IEEE CVF C COMP VI, P10425, DOI 10.1109/CVPR42600.2020.01044
   Romera E, 2018, IEEE T INTELL TRANSP, V19, P263, DOI 10.1109/TITS.2017.2750080
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sun K., 2019, arXiv
   Wang H, 2022, IEEE T IND INFORM, V18, P1220, DOI 10.1109/TII.2021.3073128
   Wang K, 2022, VISUAL COMPUT, V38, P2329, DOI 10.1007/s00371-021-02115-4
   Wang Y, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3383-y
   Wu TY, 2021, IEEE T IMAGE PROCESS, V30, P1169, DOI 10.1109/TIP.2020.3042065
   Xiangtai Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P775, DOI 10.1007/978-3-030-58452-8_45
   Xu J., 2023, PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers, DOI [10.1109/CVPR52729.2023.01871, DOI 10.1109/CVPR52729.2023.01871]
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yuan XH, 2021, EXPERT SYST APPL, V169, DOI 10.1016/j.eswa.2020.114417
   Zha HF, 2021, COMPUT ANIMAT VIRT W, V32, DOI 10.1002/cav.2022
   Zhang QL, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2235, DOI 10.1109/ICASSP39728.2021.9414568
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou Q, 2020, APPL SOFT COMPUT, V96, DOI 10.1016/j.asoc.2020.106682
   Zongxin Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11791, DOI 10.1109/CVPR42600.2020.01181
NR 40
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 22
PY 2024
DI 10.1007/s00371-024-03569-y
EA JUL 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZF7P7
UT WOS:001273949500003
DA 2024-08-05
ER

PT J
AU Han, Q
   Chen, JJ
   Min, WD
   Li, JH
   Zhan, LX
   Li, LF
AF Han, Qing
   Chen, Jiongjin
   Min, Weidong
   Li, Jiahao
   Zhan, Lixin
   Li, Longfei
TI DCSG: data complement pseudo-label refinement and self-guided
   pre-training for unsupervised person re-identification
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Person re-identification; Unsupervised learning; Frequent itemset
   mining; Label smoothing
ID NETWORK
AB Existing unsupervised person re-identification (Re-ID) methods use clustering to generate pseudo-labels that are generally noisy, and initializing the model with ImageNet pre-training weights introduces a large domain gap that severely impacts the model's performance. To address the aforementioned issues, we propose the data complement pseudo-label refinement and self-guided pre-training framework, referred to as DCSG. Firstly, our method utilizes image information from multiple augmentation views to complement the source image data, resulting in aggregated information. We employ this aggregated information to design a correlation score that serves as a reliability evaluation for the source features and cluster centroids. By optimizing the pseudo-labels for each sample, we enhance their robustness. Secondly, we propose a pre-training strategy that leverages the potential information within the training process. This strategy involves mining classes with high similarity in the training set to guide model training and facilitate smooth pre-training. Consequently, the model acquires preliminary capabilities to distinguish pedestrian-related features at an early stage of training, thereby reducing the impact of domain gaps arising from ImageNet pre-training weights. Our method demonstrates superior performance on multiple person Re-ID datasets, validating the effectiveness of our proposed approach. Notably, it achieves an mAP metric of 84.3% on the Market1501 dataset, representing a 2.8% improvement compared to the state-of-the-art method. The code is available at https://github.com/duolaJohn/DCSG.git.
C1 [Han, Qing; Chen, Jiongjin; Min, Weidong; Li, Jiahao; Zhan, Lixin; Li, Longfei] Nanchang Univ, Sch Math & Comp Sci, Nanchang 330031, Peoples R China.
   [Han, Qing; Min, Weidong] Nanchang Univ, Inst Metaverse, Nanchang 330031, Peoples R China.
   [Han, Qing; Min, Weidong] Jiangxi Prov Key Lab Virtual Real, Nanchang 330031, Peoples R China.
C3 Nanchang University; Nanchang University
RP Min, WD (corresponding author), Nanchang Univ, Sch Math & Comp Sci, Nanchang 330031, Peoples R China.; Min, WD (corresponding author), Nanchang Univ, Inst Metaverse, Nanchang 330031, Peoples R China.; Min, WD (corresponding author), Jiangxi Prov Key Lab Virtual Real, Nanchang 330031, Peoples R China.
EM hanqing@ncu.edu.cn; 416100210351@email.ncu.edu.cn;
   minweidong@ncu.edu.cn; 359100230012@email.ncu.edu.cn;
   zhanlixin@email.ncu.edu.cn; lilongfei@email.ncu.edu.cn
RI Min, Weidong/D-4585-2017
FU National Natural Science Foundation of China [62166026, 62076117];
   Jiangxi Provincial Key Laboratory of Virtual Reality [2024SSY03151]
FX This work was supported by the National Natural Science Foundation of
   China under Grant No. 62166026 and 62076117 and the Jiangxi Provincial
   Key Laboratory of Virtual Reality under Grant No.2024SSY03151.
CR Aryabarzan N, 2021, EXPERT SYST APPL, V174, DOI 10.1016/j.eswa.2021.114738
   Bashir S, 2020, EXPERT SYST APPL, V143, DOI 10.1016/j.eswa.2019.113046
   Chen H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14940, DOI 10.1109/ICCV48922.2021.01469
   Chen H, 2021, PROC CVPR IEEE, P2004, DOI 10.1109/CVPR46437.2021.00204
   Chen ZH, 2023, IEEE T PATTERN ANAL, V45, P13489, DOI 10.1109/TPAMI.2023.3293885
   Cheng ZZ, 2015, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2015.55
   Cho Y, 2022, PROC CVPR IEEE, P7298, DOI 10.1109/CVPR52688.2022.00716
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fu DP, 2022, PROC CVPR IEEE, P2466, DOI 10.1109/CVPR52688.2022.00251
   Fu DP, 2021, PROC CVPR IEEE, P14745, DOI 10.1109/CVPR46437.2021.01451
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Gan WS, 2021, IEEE T KNOWL DATA EN, V33, P1306, DOI 10.1109/TKDE.2019.2942594
   Ge Y., 2020, Advances in Neural Information Processing Systems, p11 309
   Ge Y., 2020, ICLR
   Han J, 2022, AAAI CONF ARTIF INTE, P790
   Han XM, 2023, IEEE T IMAGE PROCESS, V32, P29, DOI 10.1109/TIP.2022.3224325
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He T, 2022, AAAI CONF ARTIF INTE, P879
   Hikmawati E, 2021, J BIG DATA-GER, V8, DOI 10.1186/s40537-021-00538-3
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Jianing Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P483, DOI 10.1007/978-3-030-58586-0_29
   Kumar S, 2022, J KING SAUD UNIV-COM, V34, P1639, DOI 10.1016/j.jksuci.2019.09.006
   Li HS, 2021, J BIG DATA-GER, V8, DOI 10.1186/s40537-021-00473-3
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Li JP, 2020, IEEE T CYBERNETICS, V50, P3281, DOI [10.1109/TPAMI.2019.2929036, 10.1109/TCYB.2019.2904052]
   Li MK, 2022, IEEE T IMAGE PROCESS, V31, P3606, DOI 10.1109/TIP.2022.3173163
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Lin JCW, 2021, INFORM FUSION, V76, P122, DOI 10.1016/j.inffus.2021.05.011
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu YX, 2023, IEEE T CIRC SYST VID, V33, P326, DOI 10.1109/TCSVT.2022.3200671
   Luna JM, 2019, WIRES DATA MIN KNOWL, V9, DOI 10.1002/widm.1329
   Luna JM, 2018, IEEE T CYBERNETICS, V48, P2851, DOI 10.1109/TCYB.2017.2751081
   Peng WR, 2024, VISUAL COMPUT, V40, P1853, DOI 10.1007/s00371-023-02890-2
   Qin YM, 2023, VISUAL COMPUT, V39, P3597, DOI 10.1007/s00371-023-02922-x
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sheng B, 2022, IEEE T CYBERNETICS, V52, P6662, DOI 10.1109/TCYB.2021.3079311
   Si TZ, 2023, IEEE T MULTIMEDIA, V25, P4323, DOI 10.1109/TMM.2022.3174414
   Simoudis E., 1996, KDD 96 P 2 INT C KNO, P226
   Song XL, 2021, IEEE T MULTIMEDIA, V24, P3229, DOI 10.1109/TMM.2021.3096014
   Sumalatha S, 2020, EXPERT SYST APPL, V141, DOI 10.1016/j.eswa.2019.112967
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wei LH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P420, DOI 10.1145/3123266.3123279
   Wu JMT, 2021, INFORM SCIENCES, V553, P31, DOI 10.1016/j.ins.2020.12.004
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Yang ZZ, 2022, PROC CVPR IEEE, P14278, DOI 10.1109/CVPR52688.2022.01390
   Zhai Y., 2020, IEEE CVF C COMP VIS
   Zhan LX, 2023, INT J APPL EARTH OBS, V118, DOI 10.1016/j.jag.2023.103259
   Zhang PY, 2022, LECT NOTES COMPUT SC, V13674, P215, DOI 10.1007/978-3-031-19781-9_13
   Zhang XY, 2019, IEEE I CONF COMP VIS, P8221, DOI 10.1109/ICCV.2019.00831
   Zheng L, 2015, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2015.7298783
   Zheng Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8351, DOI 10.1109/ICCV48922.2021.00826
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
NR 54
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 1
PY 2024
DI 10.1007/s00371-024-03542-9
EA JUL 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XC2G8
UT WOS:001259414100001
DA 2024-08-05
ER

PT J
AU Mzoughi, H
   Njeh, I
   Benslima, M
   Farhat, N
   Mhiri, C
AF Mzoughi, Hiba
   Njeh, Ines
   Benslima, Mohamed
   Farhat, Nouha
   Mhiri, Chokri
TI Vision transformers (ViT) and deep convolutional neural network
   (D-CNN)-based models for MRI brain primary tumors images
   multi-classification supported by explainable artificial intelligence
   (XAI)
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Vision transformers (ViT); Convolutional neural networks (CNN);
   Pre-trained networks brain MRI; Classification; Deep learning (DL); Deep
   transfer learning (DTL); Explainable artificial intelligence (XAI);
   Grad-CAM
AB The manual classification of primary brain tumors through Magnetic Resonance Imaging (MRI) is considered as a critical task during the clinical routines that requires highly qualified neuroradiologists. Deep Learning (DL)-based computer-aided diagnosis tools are established to support the neurosurgeons' opinion during the diagnosis. However, the black-box nature and the lack of transparency and interpretability of such DL-based models make their implementation, especially in critical and sensitive medical applications, very difficult. The explainable artificial intelligence techniques help to gain clinicians' confidence and to provide explanations about the models' predictions. Typical and existing Convolutional Neural Network (CNN)-based architectures could not capture long-range global information and feature from pathology MRI scans. Recently, Vision Transformer (ViT) networks have been introduced to solve the issue of long-range dependency in CNN-based architecture by introducing a self-attention mechanism to analyze images, allowing the network to capture deep long-range reliance between pixels. The purpose of the proposed study is to provide efficient CAD tool for MRI brain tumor classification. At the same, we aim to enhance the neuroradiologists' confidence when using DL in clinical and medical standards. In this paper, we investigated a deep ViT architecture trained from scratch for the multi-classification task of common primary tumors (gliomas, meningiomas, and pituitary brain tumors), using T1-weighted contrast-enhanced MRI sequences. Several XAI techniques have been adopted: Gradient-weighted Class Activation Mapping (Grad-CAM), Local Interpretable Model-agnostic Explanations (LIME), and SHapley Additive exPlanations (SHAP), to visualize the most significant and distinguishing features related to the model prediction results. A publicly available benchmark dataset has been used for the evaluation task. The comparative study confirms the efficiency of ViT architecture compared to the CNN model using the testing dataset. The test accuracy of 83.37% for the Convolutional Neural Network (CNN) and 91.61% for the Vision Transformer (ViT) indicates that the ViT model outperformed the CNN model in the classification task. Based on the experimental results, we could confirm that the proposed ViT model presents a competitive performance outperforming the multi-classification state-of-the-art models using MRI sequences. Further, the proposed models present an exact and correct interpretation. Thus, we could confirm that the proposed CAD could be established during the clinical diagnosis routines.
C1 [Mzoughi, Hiba] Sfax Univ, Natl Engn Sch Sfax ENIS, Sfax, Tunisia.
   [Njeh, Ines] Gabes Univ, Higher Inst Comp Sci & Multimedia Gabes, Gabes, Tunisia.
   [Benslima, Mohamed] Sfax Univ, Natl Sch Elect & Telecommun Sfax, ATISP Lab, Sfax, Tunisia.
   [Farhat, Nouha; Mhiri, Chokri] Habib Bourguiba Univ Hosp, Dept Neurol, Sfax, Tunisia.
C3 Universite de Sfax; Ecole Nationale dIngenieurs de Sfax (ENIS);
   Universite de Gabes; Universite de Sfax; Universite de Sfax; Hopital
   Habib Bourguiba
RP Mzoughi, H (corresponding author), Sfax Univ, Natl Engn Sch Sfax ENIS, Sfax, Tunisia.
EM hiba.mzoughi@yahoo.fr
CR Aamir M, 2022, COMPUT ELECTR ENG, V101, DOI 10.1016/j.compeleceng.2022.108105
   Abd El Kader I, 2021, BRAIN SCI, V11, DOI 10.3390/brainsci11030352
   Abiwinanda N, 2019, IFMBE PROC, V68, P183, DOI 10.1007/978-981-10-9035-6_33
   Adu Kwabena, 2019, 2019 IEEE International Conference on Robotics and Biomimetics (ROBIO), P942, DOI 10.1109/ROBIO49542.2019.8961610
   Afshar P, 2019, INT CONF ACOUST SPEE, P1368, DOI 10.1109/ICASSP.2019.8683759
   Aloraini M, 2023, APPL SCI-BASEL, V13, DOI 10.3390/app13063680
   Anilkumar B., 2019, Int. J. Eng. Adv. Technol. ISSN, V8, p10
   Asiri AA, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23187913
   Badza MM, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10061999
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Haque M.M., 2023, GANs for Data Augmentation in Healthcare, P157
   Havaei M, 2017, MED IMAGE ANAL, V35, P18, DOI 10.1016/j.media.2016.05.004
   Ismael MR, 2018, INT CONF ELECTRO INF, P252, DOI 10.1109/EIT.2018.8500308
   Ismael SAA, 2020, ARTIF INTELL MED, V102, DOI 10.1016/j.artmed.2019.101779
   Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244
   Kumarakulasinghe NB, 2020, COMP MED SY, P7, DOI 10.1109/CBMS49503.2020.00009
   Nohara Y, 2022, COMPUT METH PROG BIO, V214, DOI 10.1016/j.cmpb.2021.106584
   Sekhar A, 2022, IEEE J BIOMED HEALTH, V26, P983, DOI 10.1109/JBHI.2021.3100758
   Selvaraju R.R., 2016, arXiv, DOI [DOI 10.48550/ARXIV.1611.07450, 10.48550/arXiv.1611.07450]
   Singh J, 2011, J PHARMACOL PHARMACO, V2, P138, DOI 10.4103/0976-500X.81919
   Tummala S, 2022, CURR ONCOL, V29, P7498, DOI 10.3390/curroncol29100590
   Ullah I, 2022, INT J ENERG RES, V46, P15211, DOI 10.1002/er.8219
   Yang G, 2022, INFORM FUSION, V77, P29, DOI 10.1016/j.inffus.2021.07.016
   Zulfiqar F, 2023, BIOMED SIGNAL PROCES, V84, DOI 10.1016/j.bspc.2023.104777
NR 24
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 26
PY 2024
DI 10.1007/s00371-024-03524-x
EA JUN 2024
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WM6H5
UT WOS:001255322700003
DA 2024-08-05
ER

PT J
AU Sacht, L
AF Sacht, Leonardo
TI Bottle cap art via clustering and optimal color assignments
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image approximation; Bottle cap art; Pixel clustering; Plastic recycling
AB We present a new methodology for the problem of approximating an input image with a given set of plastic bottle caps. The first step of our method adaptively discretizes the image into a grid where the caps are going to be placed. The next step reduces the number of colors in this discrete image using K-means color clustering. The last step calculates color assignments performing a minimization that aims to approximate colors and color differences between clustered regions while respecting the given cap colors and quantities. A collection of results showcases the potential of our method to tackle this very constrained problem at a much lower cost than the current state of the art.
C1 [Sacht, Leonardo] Univ Fed Santa Catarina, Dept Matemat, Campus Univ Trindade, BR-88040900 Florianopolis, SC, Brazil.
C3 Universidade Federal de Santa Catarina (UFSC)
RP Sacht, L (corresponding author), Univ Fed Santa Catarina, Dept Matemat, Campus Univ Trindade, BR-88040900 Florianopolis, SC, Brazil.
EM leonardo.sacht@ufsc.br
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Afifi M, 2018, Arxiv, DOI arXiv:1802.01009
   Boonstra M., 2017, Technical report
   Chao CKT, 2021, COMPUT GRAPH FORUM, V40, P87, DOI 10.1111/cgf.14343
   FLOYD RW, 1976, P SID, V17, P75
   Gerstner T, 2013, COMPUT GRAPH-UK, V37, P333, DOI 10.1016/j.cag.2012.12.007
   Gervautz M, 1988, NEW TRENDS COMPUTER, P219
   Heckbert P., 1982, Computer Graphics, V16, P297, DOI 10.1145/965145.801294
   KNUTH DE, 1987, ACM T GRAPHIC, V6, P245, DOI 10.1145/35039.35040
   Kopf J, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508370
   Öztireli AC, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766891
   Sacht L, 2022, COMPUT GRAPH-UK, V107, P277, DOI 10.1016/j.cag.2022.08.004
   VELHO L, 1991, COMP GRAPH, V25, P81, DOI 10.1145/127719.122727
   Velho L, 1997, X BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, PROCEEDINGS, P203, DOI 10.1109/SIGRA.1997.625178
   Weber N, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980239
NR 15
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 12
PY 2024
DI 10.1007/s00371-024-03512-1
EA JUN 2024
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TX6L6
UT WOS:001244596700003
DA 2024-08-05
ER

PT J
AU Xu, JY
   Zhang, CM
   Zhu, WK
   Zhang, HB
   Li, L
   Mao, XY
AF Xu, Jiayi
   Zhang, Chenming
   Zhu, Weikang
   Zhang, Hongbin
   Li, Li
   Mao, Xiaoyang
TI Personalized hairstyle and hair color editing based on multi-feature
   fusion
SO VISUAL COMPUTER
LA English
DT Article
DE Hair editing; StyleGAN; Feature fusion; Modulation
AB In the metaverse era, virtual design of hairstyle becomes very popular for personalized aesthetics. As hair design tasks can be decomposed into hair attribute editing and generation, the development of generative adversarial networks (GANs) has significantly prompted its development. The majority of the existing algorithms focus on transferring the overall hair region from one face to another, which ignore fine control over the color and geometric features. Furthermore, these algorithms may result in unnatural generation results. In this paper, we propose a hair modification framework that learns hairstyle information from a reference face mask and color information from a guidance face image. Firstly, the features of the input face image and reference images are extracted through a group of encoders, and then divided into feature vectors of coarse, medium, and fine levels. Secondly, multi-level feature vectors are fused in the latent space using attention-based modulation modules. Finally, the fused feature vector is passed through a StyleGAN generator to generate face images with specified hairstyle and hair color. Experimental results show that the proposed method can finely simulate the hairstyle transition between long and short hair under the constraint of the reference mask, and can produce realistic fusion effects in the hair-covered regions, such as ears, neck, and forehead. Various hair dyeing effects that adapt to personalized characteristics are demonstrated, as facial features including skin color and hair texture are preserved when transferring the hair color.
C1 [Xu, Jiayi; Zhang, Chenming; Zhang, Hongbin; Li, Li] Hangzhou Dianzi Univ, Hangzhou, Peoples R China.
   [Zhu, Weikang] China Mobile Hangzhou Informat Technol Co Ltd, Hangzhou, Peoples R China.
   [Mao, Xiaoyang] Univ Yamanashi, Yamanashi, Japan.
C3 Hangzhou Dianzi University; University of Yamanashi
RP Mao, XY (corresponding author), Univ Yamanashi, Yamanashi, Japan.
EM xujiayi@hdu.edu.cn; 221050375@hdu.edu.cn; zhuweikangluoyu@163.com;
   zhb@hdu.edu.cn; lili2008@hdu.edu.cn; mao@yamanashi.ac.jp
FU University of Yamanashi
FX No Statement Available
CR [Anonymous], 2023, GENERATIVE FILL AI I
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Harkonen E, 2020, C NEUR INF PROC SYST
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Huang YG, 2020, PROC CVPR IEEE, P5900, DOI 10.1109/CVPR42600.2020.00594
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Karras T., 2020, P IEEE CVF C COMP VI, P8110, DOI [10.1109/cvpr42600.2020.00813, DOI 10.1109/CVPR42600.2020.00813]
   Karras T., 2017, ARXIV
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kim T, 2022, LECT NOTES COMPUT SC, V13677, P188, DOI 10.1007/978-3-031-19790-1_12
   Kong T, 2018, LECT NOTES COMPUT SC, V11209, P172, DOI 10.1007/978-3-030-01228-1_11
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Mirza M., 2014, ARXIV
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Patashnik O, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2065, DOI 10.1109/ICCV48922.2021.00209
   Radford A, 2021, PR MACH LEARN RES, V139
   Saha R, 2021, PROC CVPR IEEE, P1984, DOI 10.1109/CVPR46437.2021.00202
   Shen YJ, 2022, IEEE T PATTERN ANAL, V44, P2004, DOI 10.1109/TPAMI.2020.3034267
   Tan Z., 2020, ARXIV
   Tov O, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459838
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wei TY, 2022, PROC CVPR IEEE, P18051, DOI 10.1109/CVPR52688.2022.01754
   Wright L, 2021, ARXIV
   Wu PW, 2019, IEEE I CONF COMP VIS, P5913, DOI 10.1109/ICCV.2019.00601
   Xiao C., 2021, ARXIV
   Yang H, 2020, IEEE IC COMP COM NET, DOI 10.1109/icccn49398.2020.9209630
   Yu CQ, 2021, INT J COMPUT VISION, V129, P3051, DOI 10.1007/s11263-021-01515-2
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu P., 2021, ARXIV
NR 30
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4751
EP 4763
DI 10.1007/s00371-024-03468-2
EA MAY 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001234536800006
OA hybrid
DA 2024-08-05
ER

PT J
AU Lin, J
   Fan, ZW
   Huang, LP
   Huang, KF
AF Lin, Jie
   Fan, Zongwen
   Huang, Lipai
   Huang, Kaifeng
TI Depth-based adaptable image layer prediction using bidirectional depth
   semantic fusion
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image editing; Image layer prediction; Fusion module; Depth estimation;
   Semantic segmentation
ID SEGMENTATION
AB Image editing is a laborious process that demands considerable effort to achieve visually appealing outcomes. To address this problem, we propose a novel method depth-based adaptable image layer prediction network for multi-object semantic segmentation, localization, and estimation of the relative depths for image layer prediction. The individual objects in an image are available to be extracted and edited without impacting the background or other objects. Our approach depth semantic prediction network utilizes a fusion module to bidirectionally optimize depth estimation as well as semantic segmentation. In addition, refinement network is designed to refine coarse results into fine segmentation. Experimental results demonstrate the salient progress our method has achieved, and that it can preserve spatial hierarchical relationships and avoid category constraints as well. To the best of our knowledge, this paper introduces a novel approach to image layer prediction, which is more adaptive than the editing method focusing on foreground and background extraction.
C1 [Lin, Jie; Fan, Zongwen; Huang, Kaifeng] Huaqiao Univ, Dept Comp Sci & Technol, Xiamen 362021, Peoples R China.
   [Fan, Zongwen] Huaqiao Univ, Xiamen Key Lab Comp Vis & Pattern Recognit, Xiamen 362021, Peoples R China.
   [Huang, Lipai] Texas A&M Univ, Zachry Dept Civil & Environm Engn, Urban Resilience AI Lab, College Stn, TX 77840 USA.
C3 Huaqiao University; Huaqiao University; Texas A&M University System;
   Texas A&M University College Station
RP Lin, J (corresponding author), Huaqiao Univ, Dept Comp Sci & Technol, Xiamen 362021, Peoples R China.
EM 21024083007@stu.hqu.edu.cn
CR Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712
   Borji A, 2019, COMPUT VIS MEDIA, V5, P117, DOI 10.1007/s41095-019-0149-9
   Carroll R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964938
   Chen H, 2017, IEEE INT C INT ROBOT, P4911, DOI 10.1109/IROS.2017.8206370
   Chen LZ, 2021, IEEE T IMAGE PROCESS, V30, P2313, DOI 10.1109/TIP.2021.3049332
   Chen Weifeng, 2016, ADV NEURAL INFORM PR, V29, P730, DOI DOI 10.5555/3157096.3157178
   Chen ZH, 2023, IEEE T PATTERN ANAL, V45, P13489, DOI 10.1109/TPAMI.2023.3293885
   Cheng YH, 2017, PROC CVPR IEEE, P1475, DOI 10.1109/CVPR.2017.161
   Chinchor N., 1993, 5 MESSAGE UNDERSTAND
   Cong RM, 2020, IEEE T CYBERNETICS, V50, P3627, DOI 10.1109/TCYB.2019.2932005
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Gao WS, 2010, INT CONF COMP SCI, P67, DOI 10.1109/ICCSIT.2010.5563693
   Gupta V, 2016, 2016 INTERNATIONAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (ICONSIP)
   Han GY, 2018, IEEE SIGNAL PROC LET, V25, P753, DOI 10.1109/LSP.2018.2820041
   Han K, 2023, IEEE T PATTERN ANAL, V45, P87, DOI 10.1109/TPAMI.2022.3152247
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He L, 2021, NEUROCOMPUTING, V440, P251, DOI 10.1016/j.neucom.2021.01.126
   Hu XX, 2019, IEEE IMAGE PROC, P1440, DOI [10.1109/icip.2019.8803025, 10.1109/ICIP.2019.8803025]
   Jadon S, 2020, 2020 IEEE CONFERENCE ON COMPUTATIONAL INTELLIGENCE IN BIOINFORMATICS AND COMPUTATIONAL BIOLOGY (CIBCB), P115, DOI 10.1109/cibcb48159.2020.9277638
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Koch T, 2020, COMPUT VIS IMAGE UND, V191, DOI 10.1016/j.cviu.2019.102877
   Koyama Y, 2018, COMPUT GRAPH FORUM, V37, P397, DOI 10.1111/cgf.13577
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Lin GS, 2016, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2016.348
   Lin T.-Y., 2017, PROC CVPR IEEE, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin X, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19081795
   Monnier T., 2021, P IEEECVF INT C COMP, P8640
   Mousavian A, 2016, INT CONF 3D VISION, P611, DOI 10.1109/3DV.2016.69
   Nekrasov V, 2019, IEEE INT CONF ROBOT, P7101, DOI [10.1109/icra.2019.8794220, 10.1109/ICRA.2019.8794220]
   Niklaus S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356528
   Shen L, 2008, PROC CVPR IEEE, P2479
   Sheng B, 2022, IEEE T CYBERNETICS, V52, P6662, DOI 10.1109/TCYB.2021.3079311
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Tan JC, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2988229
   Wang LJ, 2020, PROC CVPR IEEE, P538, DOI 10.1109/CVPR42600.2020.00062
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Xu D, 2018, PROC CVPR IEEE, P675, DOI 10.1109/CVPR.2018.00077
   Zhang ZY, 2020, IEEE T PATTERN ANAL, V42, P2608, DOI 10.1109/TPAMI.2019.2926728
   Zhang ZY, 2018, LECT NOTES COMPUT SC, V11214, P238, DOI 10.1007/978-3-030-01249-6_15
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
NR 43
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 28
PY 2024
DI 10.1007/s00371-024-03430-2
EA MAY 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SJ9P4
UT WOS:001234206700006
DA 2024-08-05
ER

PT J
AU Zhao, JK
   Cai, YG
AF Zhao, Jiakun
   Cai, Yige
TI SCAKD: a knowledge distillation framework based on spatial-corner
   attention for infrared and visible image fusion
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Knowledge distillation; Image fusion; Infrared image; Attention
   mechanism.
ID CONVOLUTIONAL NEURAL-NETWORKS
AB Infrared and visible image fusion is a kind of image enhancement technology, aiming at combining the characteristics of infrared image and visible image to synthesize them into an image that contains more information and is more suitable for human and computer perception and recognition. Most of the existing fusion methods focus on improving the fusion effect but ignore the model size and computing cost, which is not conducive to the actual deployment of such models on resource-constrained platforms such as mobile terminals. To solve this problem, we propose a new knowledge distillation framework for infrared and visible image fusion. Based on the spatial and corner attention features, the framework uses the middle-layer feature mapping to construct the distillation loss function and guides the lightweight student model to learn the fusion ability of the heavyweight teacher model. After distillation training, the student model can achieve a similar level of integration with the teacher model. A large number of qualitative and quantitative experiments show that our method has better fusion performance and lower model complexity than the existing methods.
C1 [Zhao, Jiakun; Cai, Yige] Xi An Jiao Tong Univ, Sch Software Engn, Xian 710049, Peoples R China.
C3 Xi'an Jiaotong University
RP Cai, YG (corresponding author), Xi An Jiao Tong Univ, Sch Software Engn, Xian 710049, Peoples R China.
EM zhaojk@xjtu.edu.cn; caiyige@stu.xjtu.edu.cn
FU National Key R &D Program of China;  [2020YFB2008403]
FX This work was supported by the National Key R &D Program of China (Grant
   No: 2020YFB2008403).
CR Ba LJ, 2014, ADV NEUR IN, V27
   Bajammal M, 2022, IEEE T SOFTWARE ENG, V48, P1722, DOI 10.1109/TSE.2020.3032986
   Bavirisetti DP, 2017, 2017 20TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P701
   Deng Liang, 2022, 2022 5th International Conference on Pattern Recognition and Artificial Intelligence (PRAI), P755, DOI 10.1109/PRAI55851.2022.9904229
   Gao QQ, 2019, LECT NOTES COMPUT SC, V11362, P527, DOI 10.1007/978-3-030-20890-5_34
   Gou JP, 2021, INT J COMPUT VISION, V129, P1789, DOI 10.1007/s11263-021-01453-z
   Himeur CE, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3481804
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Jang Y, 2021, IEEE ACCESS, V9, P8278, DOI 10.1109/ACCESS.2021.3049470
   Li GQ, 2021, PATTERN RECOGN, V109, DOI 10.1016/j.patcog.2020.107610
   Li HF, 2020, IEEE T INSTRUM MEAS, V69, P1082, DOI 10.1109/TIM.2019.2912239
   Li H, 2019, INFRARED PHYS TECHN, V102, DOI 10.1016/j.infrared.2019.103039
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li ST, 2017, INFORM FUSION, V33, P100, DOI 10.1016/j.inffus.2016.05.004
   Liu L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8251, DOI 10.1109/ICCV48922.2021.00816
   Ma JY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3038013
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JY, 2019, INFORM FUSION, V45, P153, DOI 10.1016/j.inffus.2018.02.004
   Ma JY, 2016, INFORM FUSION, V31, P100, DOI 10.1016/j.inffus.2016.02.001
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Peng P., 2021, 2021 5 CAA INT C VEH, P1
   Romero A, 2015, Arxiv, DOI arXiv:1412.6550
   Ruan XF, 2021, AAAI CONF ARTIF INTE, V35, P2495
   Saputra MRU, 2019, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2019.00035
   Tan C, 2022, INFORM SCIENCES, V583, P1, DOI 10.1016/j.ins.2021.10.043
   Toet A, 2017, DATA BRIEF, V15, P249, DOI 10.1016/j.dib.2017.09.038
   Wang D, 2022, Arxiv, DOI [arXiv:2205.11876, DOI 10.48550/ARXIV.2205.11876]
   Wang J, 2014, INFRARED PHYS TECHN, V67, P477, DOI 10.1016/j.infrared.2014.09.019
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Yan H, 2019, IEEE T CIRC SYST VID, V29, P80, DOI 10.1109/TCSVT.2017.2772892
   Yang ZD, 2022, PROC CVPR IEEE, P4633, DOI 10.1109/CVPR52688.2022.00460
   Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754
   Zagoruyko S, 2017, Arxiv, DOI [arXiv:1612.03928, DOI 10.48550/ARXIV.1612.03928]
   Zhang F, 2019, PROC CVPR IEEE, P3512, DOI 10.1109/CVPR.2019.00363
   Zhang H, 2021, IEEE T COMPUT IMAG, V7, P1134, DOI 10.1109/TCI.2021.3119954
   Zhang H, 2021, INT J COMPUT VISION, V129, P2761, DOI 10.1007/s11263-021-01501-8
   Zhang XY, 2017, J OPT SOC AM A, V34, P1400, DOI 10.1364/JOSAA.34.001400
   Zhang Y, 2020, INFORM FUSION, V54, P99, DOI 10.1016/j.inffus.2019.07.011
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhao JF, 2017, INFRARED PHYS TECHN, V81, P201, DOI 10.1016/j.infrared.2017.01.012
   Zhao JF, 2014, INFRARED PHYS TECHN, V62, P86, DOI 10.1016/j.infrared.2013.11.008
   Zhu ZQ, 2019, IEEE ACCESS, V7, P20811, DOI 10.1109/ACCESS.2019.2898111
NR 43
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 2
PY 2024
DI 10.1007/s00371-024-03356-9
EA MAY 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PS1T7
UT WOS:001215987700002
DA 2024-08-05
ER

PT J
AU Zang, Y
   Lu, AK
   Li, B
   Hu, WJ
AF Zang, Ying
   Lu, Ankang
   Li, Bing
   Hu, Wenjun
TI Revisiting segmentation-guided denoising student-teacher in anomaly
   detection
SO VISUAL COMPUTER
LA English
DT Article
DE Anomaly detection; Knowledge review; Data augmentation; Feature-level
   perturbation
AB Anomaly detection is a critical issue that needs to be addressed in large-scale industrial manufacturing. DeSTSeg integrates a pre-trained teacher network, a denoising student encoder-decoder, and a segmentation network into a single framework. However, this network faces two main challenges. Firstly, the supervision between the teacher and student networks is only focused on the same layers, lacking additional efficient information exchange, making it difficult for the student to effectively learn knowledge. Secondly, simple image-level perturbations struggle to cope with complex industrial scenes. To address these issues, we propose a high-performance anomaly detection network that combines knowledge review and feature-level perturbation, named KR-FP. To facilitate more efficient learning from the teacher network by the student, we introduce a knowledge review (KR) module to enhance the learning pathways of the teacher-student networks. This module comprises an attention-based fusion (ABF) module and a hierarchical context loss (HCL) module. The ABF module generates multi-scale fused features by integrating features from different levels. Then, the HCL module transfers knowledge into different levels of contextual information for interaction, thereby enhancing learning outcomes. Additionally, due to the limitations of image-level data augmentation methods in complex industrial environments, we introduce a Feature Perturbation (FP) module to enhance the model's ability to adapt to diverse and complex industrial scenes, significantly improving the model's stability and robustness. Experiments on the MVTec AD dataset have proven the effectiveness of our approach, achieving competitive results with a pixel-level average accuracy of 78.0% and an instance-level average accuracy of 78.8%. Source code and pretrained models are available at https://github.com/chongyouxiaolu/KR-FP.
C1 [Zang, Ying; Lu, Ankang; Li, Bing; Hu, Wenjun] Huzhou Univ, Sch Informat Engn, Huzhou 313000, Peoples R China.
C3 Huzhou University
RP Hu, WJ (corresponding author), Huzhou Univ, Sch Informat Engn, Huzhou 313000, Peoples R China.
EM 02750@zjhu.edu.cn; 15213137839@163.com; bingli@zjhu.edu.cn;
   hoowenjun@foxmail.com
FU the Public Welfare Research Program of Huzhou Science and Technology
   Bureau [2022GZ01]; Public Welfare Research Program of Huzhou Science and
   Technology Bureau
FX This paper is supported by the Public Welfare Research Program of Huzhou
   Science and Technology Bureau (2022GZ01).
CR Akcay S, 2019, LECT NOTES COMPUT SC, V11363, P622, DOI 10.1007/978-3-030-20893-6_39
   ANDREWS JTA, 2016, ICML
   Bergmann P., 2018, arXiv
   Bergmann P, 2022, INT J COMPUT VISION, V130, P947, DOI 10.1007/s11263-022-01578-9
   Bergmann P, 2020, PROC CVPR IEEE, P4182, DOI 10.1109/CVPR42600.2020.00424
   Bergmann P, 2019, PROC CVPR IEEE, P9584, DOI 10.1109/CVPR.2019.00982
   Chalapathy R., 2019, ARXIV
   Chen L, 2019, MED IMAGE ANAL, V58, DOI 10.1016/j.media.2019.101539
   Chen PG, 2021, PROC CVPR IEEE, P5006, DOI 10.1109/CVPR46437.2021.00497
   Cohen Niv, 2020, ARXIV
   Defard T., 2020, PATTERN RECOGNITION, V2664, P475, DOI [10. 1007/978-3-030-68799-1_35, DOI 10.1007/978-3-030-68799-1_35]
   Deng X., 2022, P IEEE CVF C COMP VI, P9737
   DeVries T., 2017, ARXIV
   Fucka M., 2023, ARXIV
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Hinton G., 2014, NEURIPS WORKSHOPS
   Hyun J., 2024, WACV, P2052
   Jenni S, 2020, PROC CVPR IEEE, P6407, DOI 10.1109/CVPR42600.2020.00644
   Kingma D. P., 2013, ARXIV
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lang J., 2021, INT C NEURAL INFORM, V13111, P644, DOI [10.1007/978-3-030-92273-3_53, DOI 10.1007/978-3-030-92273-3_53]
   Lee S, 2022, IEEE ACCESS, V10, P78446, DOI 10.1109/ACCESS.2022.3193699
   Li CL, 2021, PROC CVPR IEEE, P9659, DOI 10.1109/CVPR46437.2021.00954
   Li H., 2023, ARXIV
   Liu H., 2023, INT C NEURAL INFORM, P339, DOI [10.1007/978-981-99-8181-6_26, DOI 10.1007/978-981-99-8181-6_26]
   Liu T., 2023, ARXIV
   Liu TH, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3142023
   Liu WR, 2023, PROC CVPR IEEE, P12147, DOI 10.1109/CVPR52729.2023.01169
   Liu ZK, 2023, PROC CVPR IEEE, P20402, DOI 10.1109/CVPR52729.2023.01954
   Roth K, 2022, PROC CVPR IEEE, P14298, DOI 10.1109/CVPR52688.2022.01392
   Ruff L., 2018, Proceedings of the 35th international conference on machine learning, P4393, DOI DOI 10.1109/DSW.2019.8755576
   Salehi M, 2021, PROC CVPR IEEE, P14897, DOI 10.1109/CVPR46437.2021.01466
   Schlegl T, 2017, LECT NOTES COMPUT SC, V10265, P146, DOI 10.1007/978-3-319-59050-9_12
   Seebock P., 2016, ARXIV
   Tokozume Y, 2018, PROC CVPR IEEE, P5486, DOI 10.1109/CVPR.2018.00575
   Towards multimodal disinformation detection by vision-language knowledge interaction, 2024, INF FUS, V102, DOI [10.1016/j.inffus.2023.102037, DOI 10.1016/J.INFFUS.2023.102037]
   Tien TD, 2023, PROC CVPR IEEE, P24511, DOI 10.1109/CVPR52729.2023.02348
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Tung F, 2019, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2019.00145
   Wang Guodong, 2021, ARXIV
   Wang S, 2023, ARXIV
   Yamada Shinji, 2021, ARXIV
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zavrtanik V, 2022, LECT NOTES COMPUT SC, V13691, P539, DOI 10.1007/978-3-031-19821-2_31
   Zavrtanik V, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8310, DOI 10.1109/ICCV48922.2021.00822
   Zavrtanik V, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107706
   Zhai WZ, 2023, IEEE INTERNET THINGS, V10, P18930, DOI 10.1109/JIOT.2023.3268226
   Zhang GS, 2024, IEEE T CONSUM ELECTR, V70, P2018, DOI 10.1109/TCE.2023.3337207
   Zhang X, 2023, PROC CVPR IEEE, P3914, DOI 10.1109/CVPR52729.2023.00381
   Zhe L, 2018, PROC CVPR IEEE, P8290, DOI 10.1109/CVPR.2018.00865
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou YX, 2024, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3344118
   Zou Y, 2022, LECT NOTES COMPUT SC, V13690, P392, DOI 10.1007/978-3-031-20056-4_23
NR 53
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4023
EP 4038
DI 10.1007/s00371-024-03412-4
EA APR 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001210792300001
DA 2024-08-05
ER

PT J
AU Lei, XC
   Chen, ZY
   Yu, ZX
   Jiang, ZT
AF Lei, Xiaochun
   Chen, Zeyu
   Yu, Zhaoxin
   Jiang, Zetao
TI BENet: boundary-enhanced network for real-time semantic segmentation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Semantic segmentation; Deep neural networks; Real-time inference;
   Boundary-enhanced
AB In the realm of real-time semantic segmentation, deep neural networks have demonstrated promising potential. However, current methods face challenges when it comes to accurately segmenting object boundaries and small objects. This limitation is partly attributed to the prevalence of convolutional neural networks, which often involve multiple sequential down-sampling operations, resulting in the loss of fine-grained details. To overcome this drawback, we introduce BENet, a real-time semantic segmentation network with a focus on enhancing object boundaries. The proposed BENet integrates two key components: the boundary extraction module (BEM) and the boundary adaption layer (BAL). The proposed BEM efficiently extracts boundary information, while the BAL guides the network using this information to preserve intricate details during the feature extraction process. Furthermore, to address the challenges associated with poor segmentation of elongated objects, we introduce the strip mixed aggregation pyramid pooling module (SMAPPM). This module employs strip pooling kernels to effectively expand the contextual representation and receptive field of the network, thereby enhancing overall segmentation performance. Our experiments conducted on a single RTX 3090 GPU show that our method achieves an mIoU of 79.4% at a speed of 45.5 FPS on the Cityscapes test set without ImageNet pre-training.
C1 [Lei, Xiaochun; Chen, Zeyu; Yu, Zhaoxin; Jiang, Zetao] Guilin Univ Elect Technol, Sch Comp Sci & Informat Secur, Guilin 541010, Guangxi, Peoples R China.
   [Lei, Xiaochun; Jiang, Zetao] Guilin Univ Elect Technol, Guangxi Key Lab Image & G Intelligent Proc, Guilin 541004, Guangxi, Peoples R China.
C3 Guilin University of Electronic Technology; Guilin University of
   Electronic Technology
RP Jiang, ZT (corresponding author), Guilin Univ Elect Technol, Sch Comp Sci & Informat Secur, Guilin 541010, Guangxi, Peoples R China.; Jiang, ZT (corresponding author), Guilin Univ Elect Technol, Guangxi Key Lab Image & G Intelligent Proc, Guilin 541004, Guangxi, Peoples R China.
EM lxc8125@guet.edu.cn; 2100300312@mails.guet.edu.cn;
   2100301436@mails.guet.edu.cn; zetaojiang@guet.edu.cn
OI Lei, Xiaochun/0000-0002-5805-7283; Chen, Zeyu/0009-0009-2675-1340
FU National Natural Science Foundation of China
FX No Statement Available
CR Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen LC, 2016, Arxiv, DOI [arXiv:1412.7062, DOI 10.48550/ARXIV.1412.7062, 10.48550/ARXIV.1412.7062]
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, 10.48550/arXiv.1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen XR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14165, DOI 10.1109/ICCV48922.2021.01392
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dou WH, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2100
   Fan DP, 2020, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR42600.2020.00285
   Fan MY, 2021, PROC CVPR IEEE, P9711, DOI 10.1109/CVPR46437.2021.00959
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gao R., 2023, P IEEE CVF C COMP VI, P4674
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   He JJ, 2019, PROC CVPR IEEE, P7511, DOI 10.1109/CVPR.2019.00770
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong YD, 2021, Arxiv, DOI arXiv:2101.06085
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   KANOPOULOS N, 1988, IEEE J SOLID-ST CIRC, V23, P358, DOI 10.1109/4.996
   Liang D, 2022, AAAI CONF ARTIF INTE, P1555
   Liang D, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2335, DOI 10.1109/ICASSP39728.2021.9414355
   Liang D, 2021, PATTERN RECOGN, V117, DOI 10.1016/j.patcog.2021.107995
   Lin D, 2019, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2019.00767
   Lin PW, 2020, PROC CVPR IEEE, P4202, DOI 10.1109/CVPR42600.2020.00426
   Lin Y, 2023, LECT NOTES COMPUT SC, V13939, P730, DOI 10.1007/978-3-031-34048-2_56
   Lin Y, 2023, Arxiv, DOI arXiv:2202.08195
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Nirkin Y, 2021, PROC CVPR IEEE, P4060, DOI 10.1109/CVPR46437.2021.00405
   Poudel RPK, 2018, Arxiv, DOI arXiv:1805.04554
   Paszke A, 2016, Arxiv, DOI arXiv:1606.02147
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Peng J., 2022, arXiv
   Poudel R. P. K., 2019, arXiv
   Qi YL, 2023, IEEE I CONF COMP VIS, P6047, DOI 10.1109/ICCV51070.2023.00558
   Romera E, 2018, IEEE T INTELL TRANSP, V19, P263, DOI 10.1109/TITS.2017.2750080
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Si HY, 2019, Arxiv, DOI arXiv:1911.07217
   Takikawa T, 2019, IEEE I CONF COMP VIS, P5228, DOI 10.1109/ICCV.2019.00533
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang Junke, 2022, Advances in Neural Information Processing Systems
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Xiangtai Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P435, DOI 10.1007/978-3-030-58520-4_26
   Xiangtai Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P775, DOI 10.1007/978-3-030-58452-8_45
   Xu JC, 2023, PROC CVPR IEEE, P19529, DOI 10.1109/CVPR52729.2023.01871
   Yan M, 2023, HERIT SCI, V11, DOI 10.1186/s40494-023-00865-z
   Yan M, 2023, CAAI T INTELL TECHNO, V8, P319, DOI 10.1049/cit2.12153
   Yu CQ, 2021, INT J COMPUT VISION, V129, P3051, DOI 10.1007/s11263-021-01515-2
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu FS, 2016, Arxiv, DOI arXiv:1511.07122
   Zha HF, 2021, COMPUT ANIMAT VIRT W, V32, DOI 10.1002/cav.2022
   Zhang YH, 2023, INT J COMPUT VISION, V131, P2153, DOI 10.1007/s11263-023-01801-1
   Zhang YH, 2019, PROC CVPR IEEE, P11633, DOI 10.1109/CVPR.2019.01191
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhu HW, 2022, AAAI CONF ARTIF INTE, P3608
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 56
TC 0
Z9 0
U1 25
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 27
PY 2024
DI 10.1007/s00371-024-03320-7
EA MAR 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MO8T2
UT WOS:001194660900003
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Tran, V
   Liu, SH
   Huang, CE
   Aslam, MS
   Yang, KL
   Li, YH
   Wang, JC
AF Tran, Van Nhiem
   Liu, Shen-Hsuan
   Huang, Chi-En
   Aslam, Muhammad Saqlain
   Yang, Kai-Lin
   Li, Yung-Hui
   Wang, Jia-Ching
TI HAPiCLR: heuristic attention pixel-level contrastive loss representation
   learning for self-supervised pretraining
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Pixel-level contrastive learning; Pixel-level attention; Self-supervised
   learning; Object contextual representation; Visual representation
   learning
AB Recent self-supervised contrastive learning methods are powerful and efficient for robust representation learning, pulling semantic features from different cropping views of the same image while pushing other features away from other images in the embedding vector space. However, model training for contrastive learning is quite inefficient. In the high-dimensional vector space of the images, images can differ from each other in many ways. We address this problem with heuristic attention pixel-level contrastive loss for representation learning (HAPiCLR), a self-supervised joint embedding contrastive framework that operates at the pixel level and makes use of heuristic mask information. HAPiCLR leverages pixel-level information from the object's contextual representation instead of identifying pair-wise differences in instance-level representations. Thus, HAPiCLR enhances contrastive learning objectives without requiring large batch sizes, memory banks, or queues, thereby reducing the memory footprint and the processing needed for large datasets. Furthermore, HAPiCLR loss combined with other contrastive objectives such as SimCLR or MoCo loss produces considerable performance boosts on all downstream tasks, including image classification, object detection, and instance segmentation.
C1 [Tran, Van Nhiem; Wang, Jia-Ching] Natl Cent Univ, Dept Comp Sci & Informat Engn, Taoyuan 32001, Taiwan.
   [Liu, Shen-Hsuan; Huang, Chi-En; Aslam, Muhammad Saqlain; Yang, Kai-Lin; Li, Yung-Hui] Hon Hai Res Inst, AI Res Ctr, Taipei 114699, Taiwan.
C3 National Central University
RP Li, YH (corresponding author), Hon Hai Res Inst, AI Res Ctr, Taipei 114699, Taiwan.
EM yunghui.li@foxconn.com
OI Li, Yung-Hui/0000-0002-0475-3689
CR Bachman P, 2019, ADV NEUR IN, V32
   Bardes A, 2022, Arxiv, DOI arXiv:2105.04906
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Cao Y., 2020, P ADV NEUR INF PROC, V33, P15614
   Caron Mathilde, 2020, Advances in Neural Information Processing Systems, V33, P9912, DOI DOI 10.5555/3495724.3496555
   Chen T., 2020, Adv. Neural Inf. Process. Syst., V33, P22243, DOI DOI 10.48550/ARXIV.2006.10029
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen XL, 2020, Arxiv, DOI arXiv:2003.04297
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Hjelm RD, 2019, Arxiv, DOI arXiv:1808.06670
   Ding J, 2024, IEEE T PATTERN ANAL, V46, P1348, DOI 10.1109/TPAMI.2022.3164911
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Goyal P, 2018, Arxiv, DOI arXiv:1706.02677
   Grill J.B., 2020, P 34 INT C NEUR INF, V33, P21271, DOI [10.48550/arXiv.2006.07733, DOI 10.48550/ARXIV.2006.07733]
   Hadsell R., 2006, Computer vision and pattern recognition, 2006 IEEE computer society conference on, V2, P1735, DOI DOI 10.1109/CVPR.2006.100
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henaff OJ, 2020, PR MACH LEARN RES, V119
   Iizuka S, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925974
   Ioffe Sergey, 2015, INT C MACHINE LEARNI, V37, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Jaiswal A, 2021, TECHNOLOGIES, V9, DOI 10.3390/technologies9010002
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Loshchilov I, 2017, Sgdr: Stochastic gradient descent with warm restarts, DOI [10.48550/arXiv.1608.03983, DOI 10.48550/ARXIV.1608.03983]
   Mahendran A, 2019, LECT NOTES COMPUT SC, V11365, P99, DOI 10.1007/978-3-030-20873-8_7
   Misra I, 2020, PROC CVPR IEEE, P6706, DOI 10.1109/CVPR42600.2020.00674
   Nair V., 2010, ICML, P807
   Putri WR, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22062133
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shiyin Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12231, DOI 10.1109/CVPR42600.2020.01225
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Nguyen TH, 2019, ADV NEUR IN, V32
   Tian Y., 2020, Advances in neural information processing systems, V33, P6827, DOI DOI 10.5555/3495724.3496297
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Van Gansbeke W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10032, DOI 10.1109/ICCV48922.2021.00990
   Tran VN, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22145169
   Wang XL, 2021, PROC CVPR IEEE, P3023, DOI 10.1109/CVPR46437.2021.00304
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xia W, 2023, IEEE T IMAGE PROCESS, V32, P1170, DOI 10.1109/TIP.2023.3240863
   Xie EZ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8372, DOI 10.1109/ICCV48922.2021.00828
   Xie ZD, 2021, PROC CVPR IEEE, P16679, DOI 10.1109/CVPR46437.2021.01641
   Ye Mang, 2019, CVPR, P6210
   Yonglong Tian, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P776, DOI 10.1007/978-3-030-58621-8_45
   You Yang, 2017, ARXIV170803888
   Zbontar J, 2021, PR MACH LEARN RES, V139
   Zhan XH, 2019, PROC CVPR IEEE, P1881, DOI 10.1109/CVPR.2019.00198
   Zhong LY, 2023, IEEE T SIGNAL PROCES, V71, P772, DOI 10.1109/TSP.2023.3254888
   Zhuang CX, 2019, IEEE I CONF COMP VIS, P6001, DOI 10.1109/ICCV.2019.00610
NR 50
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 15
PY 2024
DI 10.1007/s00371-023-03217-x
EA MAR 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LB0L1
UT WOS:001184198700001
DA 2024-08-05
ER

PT J
AU Guo, ZH
   Shao, MW
   Li, SH
AF Guo, Zihao
   Shao, Mingwen
   Li, Shunhang
TI Image-to-image translation using an offset-basedmulti-scale codes GAN
   encoder
SO VISUAL COMPUTER
LA English
DT Article
DE Generative adversarial networks; GAN inversion; Image-to-image
   translation; Super-resolution; Conditional face synthesis
AB Despite the remarkable achievements of generative adversarial networks (GANs) in high-quality image synthesis, applying pre-trained GAN models to image-to-image translation is still challenging. Previous approaches typically map the conditional image into the latent spaces of GANs by per-image optimization or learning a GAN encoder. However, neither of these two methods can ideally perform image-to-image translation tasks. In this work, we propose a novel learning-based framework which can complete common image-to-image translation tasks with high quality in real-time based on pre-trained GANs. Specifically, to mitigate the semantic misalignment between conditional and synthesized images, we propose an offset-based image synthesis method that allows our encoder to use multiple rather than one forward propagation to predict the latent codes. During the multiple forward passes, the final latent codes are adjusted continuously according to the semantic difference between the conditional image and the current synthesized image. To further reduce the loss of details during encoding, we extract multiple latent codes at multiple scales from input instead of a single code to synthesize the image. Moreover, we propose an optional multiple feature maps fusion module that combines our encoder with different generators to implement our multiple latent codes strategies. Finally, we analyze the performance and demonstrate the effectiveness of our framework by comparing it with state-of-the-art works on super-resolution and conditional face synthesis tasks.
C1 [Guo, Zihao; Shao, Mingwen; Li, Shunhang] China Univ Petr, Coll Comp Sci & Technol, Qingdao, Peoples R China.
C3 China University of Petroleum
RP Shao, MW (corresponding author), China Univ Petr, Coll Comp Sci & Technol, Qingdao, Peoples R China.
EM guozh980422@163.com; smw278@126.com; z20070036@s.upc.edu.cn
FU National Key Research and development Program of China [2021YFA1000102];
   National Natural Science Foundation of China [61673396, 61976245];
   Natural Science Foundation of Shandong Province [ZR2022MF260]
FX The authors are very indebted to the anonymous referees for their
   critical comments and suggestions for the improvement of this paper.
   This work was supported by National Key Research and development Program
   of China (2021YFA1000102), and in part by the grants from the National
   Natural Science Foundation of China (Nos. 61673396, 61976245), Natural
   Science Foundation of Shandong Province (No: ZR2022MF260).
CR Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Alaluf Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6691, DOI 10.1109/ICCV48922.2021.00664
   Bai J, 2020, VISUAL COMPUT, V36, P2145, DOI 10.1007/s00371-020-01943-0
   Bau D, 2019, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2019.00460
   Chan KCK, 2021, PROC CVPR IEEE, P14240, DOI 10.1109/CVPR46437.2021.01402
   Chen SY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392386
   Creswell A, 2019, IEEE T NEUR NET LEAR, V30, P1967, DOI 10.1109/TNNLS.2018.2875194
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Fan Y, 2020, INT J MACH LEARN CYB, V11, P2077, DOI 10.1007/s13042-020-01098-3
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gu JJ, 2020, PROC CVPR IEEE, P3009, DOI 10.1109/CVPR42600.2020.00308
   Gui J, 2023, IEEE T KNOWL DATA EN, V35, P3313, DOI 10.1109/TKDE.2021.3130191
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kang HW, 2005, VISUAL COMPUT, V21, P821, DOI 10.1007/s00371-005-0328-9
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2018, Arxiv, DOI [arXiv:1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lan JY, 2023, VISUAL COMPUT, V39, P6167, DOI 10.1007/s00371-022-02719-4
   Li LY, 2021, VISUAL COMPUT, V37, P2855, DOI 10.1007/s00371-021-02236-w
   Li LY, 2022, VISUAL COMPUT, V38, P3577, DOI 10.1007/s00371-021-02188-1
   Lin M, 2014, Arxiv, DOI arXiv:1312.4400
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Ma FC, 2018, ADV NEUR IN, V31
   Menon S, 2020, PROC CVPR IEEE, P2434, DOI 10.1109/CVPR42600.2020.00251
   Mohammadi Pedram, 2014, Majlesi Journal of Electrical Engineering
   Reisfeld E, 2023, VISUAL COMPUT, V39, P2811, DOI 10.1007/s00371-022-02494-2
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Shaham TR, 2021, PROC CVPR IEEE, P14877, DOI 10.1109/CVPR46437.2021.01464
   Shao MW, 2021, KNOWL-BASED SYST, V225, DOI 10.1016/j.knosys.2021.107122
   Shao MW, 2021, KNOWL-BASED SYST, V229, DOI 10.1016/j.knosys.2021.107311
   Simo-Serra E, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925972
   Song HT, 2021, VISUAL COMPUT, V37, P2285, DOI 10.1007/s00371-020-01986-3
   Song XX, 2020, SIGNAL IMAGE VIDEO P, V14, P1217, DOI 10.1007/s11760-020-01660-0
   Venkatanath N, 2015, NATL CONF COMMUN
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P1955, DOI 10.1109/TPAMI.2008.222
   Wright and Less, 2019, Ranger-a synergistic optimizer
   Xia W., 2021, arXiv
   Xiu J, 2023, VISUAL COMPUT, V39, P5883, DOI 10.1007/s00371-022-02701-0
   Yu FS, 2016, Arxiv, DOI arXiv:1506.03365
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36
NR 45
TC 6
Z9 7
U1 2
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 699
EP 715
DI 10.1007/s00371-023-02810-4
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:001151021700017
DA 2024-08-05
ER

PT J
AU Zhou, HR
   Wang, WJ
   Chen, G
   Wang, XL
AF Zhou, Haoran
   Wang, Wenju
   Chen, Gang
   Wang, Xiaolin
TI A point cloud self-learning network based on contrastive learning for
   classification and segmentation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Contrastive learning; Point cloud representation learning; Data
   augmentation
AB In the field of point cloud representation learning, many self-supervised learning methods aim to address the issue of conventional supervised learning methods relying heavily on labeled data. Particularly in recent years, contrastive learning-based methods have gained an increasing popularity. However, most of the current contrastive learning methods solely rely on conventional random augmentation, limiting the effectiveness of representation learning. Moreover, to prevent model collapse, they construct positive and negative sample pairs or explicit clustering centers, which adds complexity to data preprocessing operations. To address these challenges effectively and achieve accurate point cloud classification and segmentation, we propose PointSL, a self-learning network for point clouds based on contrastive learning. PointSL incorporates a learnable point cloud augmentation (LPA) module, which transforms samples with high precision, significantly improving the augmentation effect. To further enhance feature discrimination, PointSL introduces a self-learning process along a refined feature predictor (FFP). This innovative approach leverages the attention mechanism to facilitate mutual feature prediction between pairs of point clouds, thereby continuously improving discriminant performance. Additionally, the network constructed a simple yet effective self-adaptive loss function that optimizes the entire network through gradient feedback. For pretraining, it is beneficial to obtain encoders with a better generalization and a higher accuracy. We evaluate PointSL on benchmark datasets such as ModelNet40, Sydney Urban Objects and ShapeNetPart. Experimental results demonstrate that PointSL outperforms state-of-the-art self-supervised methods and supervised counterparts, achieving exceptional performance in classification and segmentation tasks. Notably, on the Sydney Urban Objects and ModelNet40 datasets, PointSL achieves OA and AA metrics of 80.6%, 69.9%, 94.2% and 91.4%, respectively. On the ShapeNetPart dataset, PointSL achieves Inst.mIoU and Cls.mIoU metrics of 86.3% and 85.1%, respectively.
C1 [Zhou, Haoran; Wang, Wenju; Chen, Gang; Wang, Xiaolin] Univ Shanghai Sci & Technol, Coll Commun & Art Design, Shanghai 200093, Peoples R China.
C3 University of Shanghai for Science & Technology
RP Wang, WJ (corresponding author), Univ Shanghai Sci & Technol, Coll Commun & Art Design, Shanghai 200093, Peoples R China.
EM 203592859@st.usst.edu.cn; wangwenju@usst.edu.cn; 203592861@st.usst.edu;
   203592861@st.usst.edu.cn
RI Sun, Yang/KHY-5117-2024; Wang, Jinlong/KHC-3829-2024; Li,
   Zexi/KFA-6939-2024; Wang, Siying/KHX-1894-2024; zhang,
   yingying/KGM-8162-2024; wang, haoyu/KHY-6295-2024; Zhou,
   Haoran/JPL-5691-2023
OI zhang, yingying/0000-0001-7479-3398; wang, haoyu/0009-0001-2467-5331;
   Zhou, Haoran/0000-0002-3530-0500; Wang, Wenju/0000-0002-8549-4710
FU Natural Science Foundation of Shanghai
FX The authors would like to thank the relevant researchers from Princeton
   University, Stanford University and TTIC for providing ShapeNetCore,
   ShapeNetPart and ModelNet40 datasets, and the relevant researchers from
   the University of Sydney for providing the Sydney Urban Objects dataset.
CR Afham M, 2022, PROC CVPR IEEE, P9892, DOI 10.1109/CVPR52688.2022.00967
   Caron M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9630, DOI 10.1109/ICCV48922.2021.00951
   Caron Mathilde, 2020, Advances in Neural Information Processing Systems, V33, P9912, DOI DOI 10.5555/3495724.3496555
   Chen H, 2022, ROBOT AUTON SYST, V154, DOI 10.1016/j.robot.2022.104124
   Chen S., 2021, BRIT MACHINE VISION
   Chen SH, 2021, IEEE SIGNAL PROC MAG, V38, P68, DOI 10.1109/MSP.2020.2984780
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549
   Choi J, 2021, IEEE INT C INT ROBOT, P3391, DOI 10.1109/IROS51168.2021.9635887
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   DeDeuge M., 2013, AUSTRALASIAN C ROBIT, V2
   Du B, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3133, DOI 10.1145/3474085.3475458
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Geng ZH, 2023, IISE TRANS, V55, P912, DOI 10.1080/24725854.2022.2106389
   Goyal A, 2021, PR MACH LEARN RES, V139
   Grill J.B., 2020, P 34 INT C NEUR INF, V33, P21271, DOI [10.48550/arXiv.2006.07733, DOI 10.48550/ARXIV.2006.07733]
   Gutmann A., 2010, P MACHINE LEARNING R, P297, DOI DOI 10.1145/3292500.3330651
   Hamdi A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1, DOI 10.1109/ICCV48922.2021.00007
   Han ZZ, 2019, AAAI CONF ARTIF INTE, P8376
   Hou J, 2021, PROC CVPR IEEE, P15582, DOI 10.1109/CVPR46437.2021.01533
   Jiang JC, 2024, VISUAL COMPUT, V40, P5169, DOI 10.1007/s00371-023-02921-y
   Li C., 2023, Vis. Comput., P1
   Li RH, 2020, PROC CVPR IEEE, P6377, DOI 10.1109/CVPR42600.2020.00641
   Liu ZJ, 2019, ADV NEUR IN, V32
   Liu ZS, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10196735
   Long FC, 2023, PROC CVPR IEEE, P21824, DOI 10.1109/CVPR52729.2023.02090
   Ma X, 2022, Arxiv, DOI arXiv:2202.07123
   Mohammadi SS, 2021, IEEE IMAGE PROC, P3103, DOI 10.1109/ICIP42928.2021.9506426
   Nguyen V, 2022, REMOTE SENS ENVIRON, V279, DOI 10.1016/j.rse.2022.113115
   Pang Y, 2022, LECT NOTES COMPUT SC, V13662, P604, DOI 10.1007/978-3-031-20086-1_35
   Qi CR, 2017, ADV NEUR IN, V30
   Qian GC, 2021, ADV NEUR IN, V34
   Ran HX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15457, DOI 10.1109/ICCV48922.2021.01519
   Rao Yongming, 2021, P IEEECVF INT C COMP, P3283
   Saining Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P574, DOI 10.1007/978-3-030-58580-8_34
   Sanghi Aditya, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P626, DOI 10.1007/978-3-030-58526-6_37
   Sauder J, 2019, ADV NEUR IN, V32
   Sheshappanavar SV, 2021, IEEE INT CONF COMP V, P2118, DOI 10.1109/ICCVW54120.2021.00240
   Shi Y., 2020, CVPR, P9353
   Singh SA, 2023, EXPERT SYST APPL, V218, DOI 10.1016/j.eswa.2023.119623
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang C, 2019, NEUROCOMPUTING, V323, P139, DOI 10.1016/j.neucom.2018.09.075
   Wang HC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9762, DOI 10.1109/ICCV48922.2021.00964
   Wang WJ, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14091996
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Chang AX, 2015, Arxiv, DOI [arXiv:1512.03012, DOI 10.48550/ARXIV.1512.03012]
   Xiang TG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P895, DOI 10.1109/ICCV48922.2021.00095
   Xu MT, 2021, PROC CVPR IEEE, P3172, DOI 10.1109/CVPR46437.2021.00319
   Yan SM, 2023, Arxiv, DOI [arXiv:2201.00785, 10.48550/arXiv.2201.00785]
   Yang J., 2021, PROC IEEE INT C COMP, P6413
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yu T, 2018, PROC CVPR IEEE, P186, DOI 10.1109/CVPR.2018.00027
   Yu XM, 2022, PROC CVPR IEEE, P19291, DOI 10.1109/CVPR52688.2022.01871
   Yunlu Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P330, DOI 10.1007/978-3-030-58580-8_20
   Zeng YH, 2023, PROC CVPR IEEE, P15244, DOI 10.1109/CVPR52729.2023.01463
   Zhang C., 2021, arXiv
   Zhang J., 2022, The Visual Computer, P1
   Zhang ZW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10232, DOI 10.1109/ICCV48922.2021.01009
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhao YH, 2019, PROC CVPR IEEE, P1009, DOI 10.1109/CVPR.2019.00110
   Zheng YC, 2022, IEEE T INTELL TRANSP, V23, P22312, DOI 10.1109/TITS.2022.3153133
NR 64
TC 0
Z9 0
U1 10
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 23
PY 2024
DI 10.1007/s00371-023-03248-4
EA JAN 2024
PG 25
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FP2V7
UT WOS:001146990400001
DA 2024-08-05
ER

PT J
AU Yadav, NK
   Singh, SK
   Dubey, SR
AF Yadav, Nand Kumar
   Singh, Satish Kumar
   Dubey, Shiv Ram
TI ISA-GAN: inception-based self-attentive encoder-decoder network for face
   synthesis using delineated facial images
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE GAN; Self-attention network; Synthesized loss; Face synthesis;
   Thermal-visible transformation
AB Facial image synthesis using delineated face images is a complex task associated with computer vision. The delineated face image synthesis such as sketch to face generation or thermal to visual face generation using generative adversarial networks (GANs) is a widely accepted task due to its generative capability. The accurate and realistic sample generation by using GAN with an attention network shows a more realistic sample generation. Attention-based network improves the network's learning by prioritizing the learning over a specific region. Motivated by the success of attention mechanism in recent literature, we develop a new inception-based encoder-decoder self-attentive generative adversarial network (ISA-GAN) by incorporating an inception network with self-attention-based learning. The proposed network is embedded with parallel self-attention, which helps to generate high-quality images and converges faster in terms of training epochs. The proposed scenario has been experimented for face synthesis using delineated face images of sketch images over the CUHK dataset. We also test it over thermal to visual face synthesis using WHU-IIP and CVBL-CHILD datasets. The proposed ISA-GAN outperforms the state-of-the-art generative models for face synthesis. The proposed ISA-GAN shows on an average improvement of 9.95%\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$9.95\%$$\end{document} in SSIM score over CUHK dataset while 10.38%,12.58%\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$10.38\%,12.58\% $$\end{document} for WHU-IIP and CVBL-CHILD datasets, respectively.
C1 [Yadav, Nand Kumar; Singh, Satish Kumar; Dubey, Shiv Ram] Indian Inst Informat Technol, Comp Vis & Biometr Lab, Allahabad, UP, India.
   [Yadav, Nand Kumar] Natl Res Univ Higher Sch Econ, Moscow, Russia.
C3 Indian Institute of Information Technology Allahabad; HSE University
   (National Research University Higher School of Economics)
RP Yadav, NK (corresponding author), Indian Inst Informat Technol, Comp Vis & Biometr Lab, Allahabad, UP, India.; Yadav, NK (corresponding author), Natl Res Univ Higher Sch Econ, Moscow, Russia.
EM pis2016004@iiita.ac.in; sk.singh@iiita.ac.in; srdubey@iiita.ac.in
RI Yadav, Nand Kumar/ADA-5765-2022; singh, satish/U-7158-2018; Yadav, Nand
   Kumar/HMP-7145-2023
OI Yadav, Nand Kumar/0000-0001-6251-0223; singh,
   satish/0000-0002-8536-4991; Yadav, Nand Kumar/0000-0001-6251-0223
FU Ministry Of Education, Government of India
FX No Statement Available
CR Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Asif M, 2021, APPL INTELL, V51, P1959, DOI 10.1007/s10489-020-01923-w
   Kancharagunta KB, 2019, Arxiv, DOI arXiv:1901.03554
   Babu KK, 2020, NEUROCOMPUTING, V413, P41, DOI 10.1016/j.neucom.2020.06.104
   Chen HN, 2020, IEEE T INF FOREN SEC, V15, P578, DOI 10.1109/TIFS.2019.2922241
   Cho K., 2014, P 2014 C EMP METH NA, DOI 10.3115/v1/D14-1179
   Dubey SR, 2020, IEEE T NEUR NET LEAR, V31, P4500, DOI 10.1109/TNNLS.2019.2955777
   Fang Z, 2022, VISUAL COMPUT, V38, P1151, DOI 10.1007/s00371-021-02074-w
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Hensel M, 2017, ADV NEUR IN, V30
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kera SB, 2023, VISUAL COMPUT, V39, P2347, DOI 10.1007/s00371-022-02445-x
   Kniaz VV, 2019, LECT NOTES COMPUT SC, V11134, P606, DOI 10.1007/978-3-030-11024-6_46
   Kumar S, 2018, IEEE IND APPLIC SOC
   Lauriola I, 2022, NEUROCOMPUTING, V470, P443, DOI 10.1016/j.neucom.2021.05.103
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lejbolle AR, 2020, IEEE T INF FOREN SEC, V15, P1216, DOI 10.1109/TIFS.2019.2938870
   Leng L, 2014, NEUROCOMPUTING, V131, P377, DOI 10.1016/j.neucom.2013.10.005
   Liang S, 2022, VISUAL COMPUT, V38, P1369, DOI 10.1007/s00371-021-02282-4
   Liao B., 2007, 2 INT C INN COMP INF, P56
   Liu Huajun, 2021, arXiv
   Lu Leng, 2010, 2010 International Conference on Information and Communication Technology Convergence (ICTC), P467, DOI 10.1109/ICTC.2010.5674791
   Mao XJ, 2016, ADV NEUR IN, V29
   Mejjati Youssef Alami, 2018, ADV NEURAL INFORM PR, P3697
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Mousa A, 2023, VISUAL COMPUT, V39, P1295, DOI 10.1007/s00371-022-02405-5
   Peng CL, 2020, IEEE T INF FOREN SEC, V15, P172, DOI 10.1109/TIFS.2019.2916633
   Sajjadi MSM, 2018, ADV NEUR IN, V31
   Serengil SI, 2021, 2021 7TH INTERNATIONAL CONFERENCE ON ENGINEERING AND EMERGING TECHNOLOGIES (ICEET 2021), P863, DOI 10.1109/ICEET53442.2021.9659697
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shen YJ, 2018, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2018.00092
   Simon L, 2019, Arxiv, DOI arXiv:1905.05441
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Souly N, 2017, IEEE I CONF COMP VIS, P5689, DOI 10.1109/ICCV.2017.606
   Sun Q, 2022, VISUAL COMPUT, V38, P1283, DOI 10.1007/s00371-021-02219-x
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang H, 2019, IEEE IJCNN
   Tang H, 2019, PROC CVPR IEEE, P2412, DOI 10.1109/CVPR.2019.00252
   Tang X, 2002, IEEE IMAGE PROC, P257
   Tuzel, 2016, Advances in Neural Information Processing Systems, P469
   Üzen H, 2023, VISUAL COMPUT, V39, P1745, DOI 10.1007/s00371-022-02442-0
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang LD, 2018, IEEE INT CONF AUTOMA, P83, DOI 10.1109/FG.2018.00022
   Wang ZL, 2018, IEEE SIGNAL PROC LET, V25, P1161, DOI 10.1109/LSP.2018.2845692
   Wei Zheng, 2021, IEEE Transactions on Biometrics, Behavior, and Identity Science, V3, P296, DOI 10.1109/TBIOM.2021.3066983
   Yadav NK, 2023, NEURAL COMPUT APPL, V35, P19729, DOI 10.1007/s00521-023-08724-5
   Yadav NK, 2022, APPL INTELL, V52, P12704, DOI 10.1007/s10489-021-03064-0
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Zhang H, 2018, PREPRINT
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhang SC, 2019, IEEE T NEUR NET LEAR, V30, P1419, DOI 10.1109/TNNLS.2018.2869574
   Zhang XY, 2021, APPL INTELL, V51, P2589, DOI 10.1007/s10489-020-01905-y
   Zhao SH, 2022, COMPUT ELECTR ENG, V101, DOI 10.1016/j.compeleceng.2022.108090
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 58
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 22
PY 2024
DI 10.1007/s00371-023-03233-x
EA JAN 2024
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GJ3D1
UT WOS:001152252500003
DA 2024-08-05
ER

PT J
AU Zhang, L
   Li, SF
   Luo, X
   Liu, XR
   Zhang, RX
AF Zhang, Liang
   Li, Shifeng
   Luo, Xi
   Liu, Xiaoru
   Zhang, Ruixuan
TI Video anomaly detection with both normal and anomaly memory modules
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Video anomaly detection; Pseudo-anomaly; Memory module
AB In this paper, we propose a novel framework for video anomaly detection that employs dual memory modules for both normal and anomaly patterns. By maintaining separate memory modules, one for normal patterns and one for anomaly patterns, our approach captures a broader range of video data behaviors. By exploring separate memory modules for normal and anomaly patterns, we begin by generating pseudo-anomalies using a temporal pseudo-anomaly synthesizer. This data is then used to train the anomaly memory module, while normal data trains the normal memory module. To distinguish between normal and anomalous data, we introduce a loss function that computes memory loss between the two memory modules. We enhance the memory modules by incorporating entropy loss and a hard shrinkage rectified linear unit (ReLU). Additionally, we integrate skip connections within our model to ensure the memory module captures comprehensive patterns beyond prototypical representations. Extensive experimentation and analysis on various challenging video anomaly datasets validate the effectiveness of our approach in detecting anomalies. The code for our method is available at https://github.com/SVIL2024/Pseudo-Anomaly-MemAE.
C1 [Zhang, Liang; Li, Shifeng; Luo, Xi; Liu, Xiaoru] BoHai Univ, Coll Informat Sci & Technol, Jin Shan St, Jinzhou 121010, Peoples R China.
   [Zhang, Ruixuan] Hikvision Res Inst, Qian Mo St, Hangzhou 310052, Peoples R China.
C3 Bohai University
RP Li, SF (corresponding author), BoHai Univ, Coll Informat Sci & Technol, Jin Shan St, Jinzhou 121010, Peoples R China.
EM 3399358031@qq.com; limax_2008@outlook.com; lx2023009012@bhu.edu.cn;
   lxr2023009011@bhu.edu.cn; 147432965@qq.com
FU National Natural Science Foundation of China [61402049]; National
   Natural Science Foundation of China [LJKZ1019]; Science and Technology
   Research Project of the Department of Education of Liaoning Province
   [L21BGL002]; Social Science Planning Fund of Liaoning Province
FX This work was jointly supported by the National Natural Science
   Foundation of China (61402049), Science and Technology Research Project
   of the Department of Education of Liaoning Province (LJKZ1019) and
   Social Science Planning Fund of Liaoning Province (L21BGL002).DAS:No
   datasets were generated or analyzed during the current study.
CR Abati D, 2019, PROC CVPR IEEE, P481, DOI 10.1109/CVPR.2019.00057
   Astrid M, 2021, IEEE INT CONF COMP V, P207, DOI 10.1109/ICCVW54120.2021.00028
   Chen YQ, 2001, IEEE IMAGE PROC, P34, DOI 10.1109/ICIP.2001.958946
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fan YX, 2020, COMPUT VIS IMAGE UND, V195, DOI 10.1016/j.cviu.2020.102920
   Georgescu MI, 2022, IEEE T PATTERN ANAL, V44, P4505, DOI 10.1109/TPAMI.2021.3074805
   Golan I., 2018, Advances in neural information processing systems
   Gong D, 2019, IEEE I CONF COMP VIS, P1705, DOI 10.1109/ICCV.2019.00179
   Gong D, 2019, IEEE T IMAGE PROCESS, V28, P1851, DOI 10.1109/TIP.2018.2875352
   Gong D, 2016, PROC CVPR IEEE, P1827, DOI 10.1109/CVPR.2016.202
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Hasan M, 2016, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2016.86
   Huang X., 2023, ICASSP 2023, P1
   Hyunjong Park, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14360, DOI 10.1109/CVPR42600.2020.01438
   Ioffe Sergey, 2015, INT C MACHINE LEARNI, V37, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Ionescu RT, 2017, IEEE I CONF COMP VIS, P2914, DOI 10.1109/ICCV.2017.315
   Jaechul Kim, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2921, DOI 10.1109/CVPRW.2009.5206569
   JasonWeston S.C., 2015, INT C LEARN REPR
   Kiran BR, 2018, J IMAGING, V4, DOI 10.3390/jimaging4020036
   Liu W, 2018, PROC CVPR IEEE, P6536, DOI 10.1109/CVPR.2018.00684
   Liu ZA, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13568, DOI 10.1109/ICCV48922.2021.01333
   Lu CW, 2013, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2013.338
   Lu YW, 2019, 2019 16TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), DOI 10.1109/avss.2019.8909850
   Luo WX, 2017, IEEE I CONF COMP VIS, P341, DOI 10.1109/ICCV.2017.45
   Luo WX, 2017, IEEE INT CON MULTI, P439, DOI 10.1109/ICME.2017.8019325
   Maas A.L., 2013, ICML WORK DEEP LEARN, V28
   Mathieu M., 2016, ICLR
   Munawar Asim, 2017, 2017 IEEE 27 INT WOR, P1, DOI DOI 10.1109/MLSP.2017.8168155
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Ravanbakhsh M, 2017, IEEE IMAGE PROC, P1577, DOI 10.1109/ICIP.2017.8296547
   Ruff L., 2018, Proceedings of the 35th international conference on machine learning, P4393, DOI DOI 10.1109/DSW.2019.8755576
   Sabokrou M, 2016, ELECTRON LETT, V52, P1122, DOI 10.1049/el.2016.0440
   Sabokrou M, 2018, PROC CVPR IEEE, P3379, DOI 10.1109/CVPR.2018.00356
   Santoro A, 2016, International Conference on Machine Learning, P1842
   Sukhbaatar S, 2015, ADV NEUR IN, V28
   Wang S, 2010, INT CONF SIGN PROCES, P1220, DOI 10.1109/ICOSP.2010.5655356
   Ye MC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1805, DOI 10.1145/3343031.3350899
   Yu G, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P583, DOI 10.1145/3394171.3413973
   Yunpeng Chang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P329, DOI 10.1007/978-3-030-58555-6_20
   Zaheer Muhammad Zaigham, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P358, DOI 10.1007/978-3-030-58542-6_22
   Zaigham Zaheer Muhammad, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14171, DOI 10.1109/CVPR42600.2020.01419
   Zhai SF, 2016, PR MACH LEARN RES, V48
   Zhao YR, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1933, DOI 10.1145/3123266.3123451
   Zong B, 2018, INT C LEARN REPR VAN, P1
NR 44
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 22
PY 2024
DI 10.1007/s00371-024-03584-z
EA JUL 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZF7P7
UT WOS:001273949500001
DA 2024-08-05
ER

PT J
AU Thapa, S
   Sarkar, A
AF Thapa, Surendrabikram
   Sarkar, Abhijit
TI A deep dive into enhancing sharing of naturalistic driving data through
   face deidentification
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Video data sharing; Privacy protection; Face swapping algorithms;
   Generative adversarial networks; Multimedia privacy
ID PRIVACY; IDENTIFICATION
AB Human factors research in transportation relies on naturalistic driving studies (NDS) which collect real-world data from drivers on actual roads. NDS data offer valuable insights into driving behavior, styles, habits, and safety-critical events. However, these data often contain personally identifiable information (PII), such as driver face videos, which cannot be publicly shared due to privacy concerns. To address this, our paper introduces a comprehensive framework for deidentifying drivers' face videos, that can facilitate the wide sharing of driver face videos while protecting PII. Leveraging recent advancements in generative adversarial networks (GANs), we explore the efficacy of different face swapping algorithms in preserving essential human factors attributes while anonymizing participants' identities. Most face swapping algorithms are tested in restricted lighting conditions and indoor settings, there is no known study that tested them in adverse and natural situations. We conducted extensive experiments using large-scale outdoor NDS data, evaluating the quantification of errors associated with head, mouth, and eye movements, along with other attributes important for human factors research. Additionally, we performed qualitative assessments of these methods through human evaluators providing valuable insights into the quality and fidelity of the deidentified videos. We propose the utilization of synthetic faces as substitutes for real faces to enhance generalization. Additionally, we created practical guidelines for video deidentification, emphasizing error threshold creation, spot-checking for abrupt metric changes, and mitigation strategies for reidentification risks. Our findings underscore nuanced challenges in balancing data utility and privacy, offering valuable insights into enhancing face video deidentification techniques in NDS scenarios.
C1 [Thapa, Surendrabikram] Virginia Tech, Dept Comp Sci, Blacksburg, VA 24060 USA.
   [Thapa, Surendrabikram; Sarkar, Abhijit] Virginia Tech, Transportat Inst, Blacksburg, VA 24060 USA.
C3 Virginia Polytechnic Institute & State University; Virginia Polytechnic
   Institute & State University
RP Thapa, S (corresponding author), Virginia Tech, Dept Comp Sci, Blacksburg, VA 24060 USA.; Thapa, S (corresponding author), Virginia Tech, Transportat Inst, Blacksburg, VA 24060 USA.
EM surenthapa5803@gmail.com
FU National Surface Transportation Safety Center for Excellence (NSTSCE)
FX This project is partly funded by the National Surface Transportation
   Safety Center for Excellence (NSTSCE).
CR Alemi AA, 2019, Arxiv, DOI arXiv:1612.00410
   Aggarwal A., 2021, Int. J. Inf. Magn. Data Insights, DOI [10.1016/j.jjimei.2020.100004, DOI 10.1016/J.JJIMEI.2020.100004]
   Agrawal P, 2011, IEEE T CIRC SYST VID, V21, P299, DOI 10.1109/TCSVT.2011.2105551
   Akhtar Z, 2023, J IMAGING, V9, DOI 10.3390/jimaging9010018
   [Anonymous], U.S. Department of Health Human Services
   Archana R., 2018, Int. J. Electr. Comput. Eng, V8, P3976
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Asamoah D., 2018, Int. J. Comput. Appl, V181, P6, DOI [DOI 10.5120/IJCA2018917899, 10.5120/ijca2018917899]
   Asperti A., 2023, SN Comput. Sci, V4, P349, DOI [DOI 10.1007/S42979-023-01796-Z, 10.1007/s42979-023-01796-z]
   Boyle M., 2000, CSCW 2000. ACM 2000 Conference on Computer Supported Cooperative Work, P1, DOI 10.1145/358916.358935
   Cai Z., 2024, P AAAI C ART IT, V38, P918
   Chen RW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2003, DOI 10.1145/3394171.3413630
   Chen YC, 2019, PROC CVPR IEEE, P9851, DOI 10.1109/CVPR.2019.01009
   Council for International Organizations of Medical Sciences, 2002, Bull Med Ethics, P17
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   Deb D., 2020, IEEEIAPR INT JOINT, P1, DOI DOI 10.1109/ijcb48548.2020.9304898
   Deshpande Yogesh, 2023, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P5975, DOI 10.1109/CVPRW59228.2023.00636
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Fecher B, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0118053
   Gafni O, 2019, IEEE I CONF COMP VIS, P9377, DOI 10.1109/ICCV.2019.00947
   Gao GG, 2021, PROC CVPR IEEE, P3403, DOI 10.1109/CVPR46437.2021.00341
   Garfinkel S., 2015, De-identification of personal information, P1, DOI [DOI 10.6028/NIST.IR.8053, 10.6028/NIST.IR.8053]
   Ghosh S., 2018, Kernel Smoothing: Principles, Methods and Applications
   Goodfellow Ian J., 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P117, DOI 10.1007/978-3-642-42051-1_16
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Groshev A, 2022, IEEE ACCESS, V10, P83452, DOI 10.1109/ACCESS.2022.3196668
   Gross R., 2009, Protecting privacy in video surveillance, P129
   He JP, 2019, IEEE T AUTOMAT CONTR, V64, P5222, DOI 10.1109/TAC.2019.2910171
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Hsu GS, 2022, PROC CVPR IEEE, P632, DOI 10.1109/CVPR52688.2022.00072
   Huang BJ, 2023, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR52729.2023.00436
   Gulrajani I, 2017, ADV NEUR IN, V30
   Jagalingam P, 2015, AQUAT PR, V4, P133, DOI 10.1016/j.aqpro.2015.02.019
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kaplan B, 2015, CAMB Q HEALTHC ETHIC, V24, P256, DOI 10.1017/S0963180114000589
   Kapp MB, 2006, J CLIN PATHOL, V59, P335, DOI 10.1136/jcp.2005.030957
   Karnowski T., 2021, Technical report
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Kietzmann J, 2020, BUS HORIZONS, V63, P135, DOI 10.1016/j.bushor.2019.11.006
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Kowalczyk S, 2011, ANNU REV INFORM SCI, V45, P247, DOI 10.1002/aris.2011.1440450113
   Kushida CA, 2012, MED CARE, V50, pS82, DOI 10.1097/MLR.0b013e3182585355
   Li JM, 2023, PROC CVPR IEEE, P17142, DOI 10.1109/CVPR52729.2023.01644
   Li LZ, 2020, Arxiv, DOI arXiv:1912.13457
   Masood M, 2023, APPL INTELL, V53, P3974, DOI 10.1007/s10489-022-03766-z
   Medicine I., 2015, Sharing Clinical Trial Data: Maximizing Benefits, Minimizing Risk
   Meridou DT, 2015, IT PROF, V17, P20, DOI 10.1109/MITP.2015.88
   Meyer MN, 2018, ADV METH PRACT PSYCH, V1, P131, DOI 10.1177/2515245917747656
   Mirsky Y, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3425780
   Mokhayeri F, 2020, IEEE WINT CONF APPL, P241, DOI 10.1109/WACV45572.2020.9093275
   Murphy-Chutorian E, 2009, IEEE T PATTERN ANAL, V31, P607, DOI 10.1109/TPAMI.2008.106
   Neubauer T, 2011, INT J MED INFORM, V80, P190, DOI 10.1016/j.ijmedinf.2010.10.016
   Newton EM, 2005, IEEE T KNOWL DATA EN, V17, P232, DOI 10.1109/TKDE.2005.32
   Nirkin Y, 2019, IEEE I CONF COMP VIS, P7183, DOI 10.1109/ICCV.2019.00728
   Peiffer-Smadja N, 2020, NAT MACH INTELL, V2, P293, DOI 10.1038/s42256-020-0181-6
   RemyaRevi K., 2021, 2 INT C NETW ADV COM, P25
   Renza D, 2013, IEEE GEOSCI REMOTE S, V10, P76, DOI 10.1109/LGRS.2012.2193372
   Ribaric S, 2015, IEEE INT CONF AUTOMA
   Sara U., 2019, J. Comput. Commun, V7, P8, DOI [DOI 10.4236/JCC.2019.73002, 10.4236/jcc.2019.73002]
   Sarkar A, 2015, INT CONF BIOMETR THE
   Sarkar A, 2016, INT CONF BIOMETR THE
   Schulz K, 2020, Arxiv, DOI arXiv:2001.00396
   Stockemer D, 2018, PS-POLIT SCI POLIT, V51, P799, DOI 10.1017/S1049096518000926
   Sun RQ, 2021, COMPUT VIS MEDIA, V7, P363, DOI 10.1007/s41095-021-0219-7
   Teng Zhang, 2020, 2020 IEEE 3rd International Conference on Computer and Communication Engineering Technology (CCET), P67, DOI 10.1109/CCET50901.2020.9213159
   Tenopir C, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0021101
   Thapa Surendrabikram, 2022, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, P1509, DOI 10.1177/1071181322661302
   Thapa S., 2023, 2023 IEEE CVF C COMP, P5965, DOI [10.1109/CVPRW59228.2023.00635, DOI 10.1109/CVPRW59228.2023.00635]
   Thapa S., 2023, Face de-identification of drivers from NDS data and its effectiveness in human factors
   Thapa S, 2023, IEEE INT VEH SYM, DOI 10.1109/IV55152.2023.10186646
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Tishby N, 2015, 2015 IEEE INFORMATION THEORY WORKSHOP (ITW)
   Tolba A, 2019, COMPUT NETW, V152, P78, DOI 10.1016/j.comnet.2019.01.038
   Tomashchuk O, 2019, LECT NOTES COMPUT SC, V11711, P63, DOI 10.1007/978-3-030-27813-7_5
   Varghese Renju Rachel, 2021, 2021 International Conference on Data Analytics for Business and Industry (ICDABI), P531, DOI 10.1109/ICDABI53623.2021.9655979
   Victor T., 2015, Analysis of naturalistic driving study data: Safer glances, driver inattention, and crash risk
   Wang C.Y., 2021, P ACM HUM COMP INT, P1
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Weijie S HEN., 2012, SOURCEInternational Journal of Digital Content Technology its Applic, V6, P372
   Xu Y, 2014, APPL MATH INFORM SCI, V8, P1103, DOI 10.12785/amis/080321
   Yadav D, 2019, PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING AND CONTROL SYSTEMS (ICCS), P852, DOI [10.1109/ICCS45141.2019.9065881, 10.1109/iccs45141.2019.9065881]
   Zhou JZ, 2021, IEEE T INF FOREN SEC, V16, P1088, DOI 10.1109/TIFS.2020.3029913
   Zobaed S., 2021, ARTIFICIAL INTELLIGE, P177
NR 84
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 4
PY 2024
DI 10.1007/s00371-024-03552-7
EA JUL 2024
PG 32
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XK1T6
UT WOS:001261494400001
OA hybrid
DA 2024-08-05
ER

PT J
AU Song, C
   Chen, QJ
   Li, FWB
   Jiang, ZY
   Zheng, D
   Shen, YL
   Yang, BL
AF Song, Chao
   Chen, Qingjie
   Li, Frederick W. B.
   Jiang, Zhaoyi
   Zheng, Dong
   Shen, Yuliang
   Yang, Bailin
TI Multi-feature fusion enhanced monocular depth estimation with boundary
   awareness
SO VISUAL COMPUTER
LA English
DT Article
DE Self-supervised monocular depth estimation; Laplacian pyramid residuals;
   Boundary depth; Multi-level semantic
AB Self-supervised monocular depth estimation has opened up exciting possibilities for practical applications, including scene understanding, object detection, and autonomous driving, without the need for expensive depth annotations. However, traditional methods for single-image depth estimation encounter limitations in photometric loss due to a lack of geometric constraints, reliance on pixel-level intensity or color differences, and the assumption of perfect photometric consistency, leading to errors in challenging conditions and resulting in overly smooth depth maps with insufficient capture of object boundaries and depth transitions. To tackle these challenges, we propose MFFENet, which leverages multi-level semantic and boundary-aware features to improve depth estimation accuracy. MFFENet extracts multi-level semantic features using our modified HRFormer approach. These features are then fed into our decoder and enhanced using attention mechanisms to enrich the boundary information generated by Laplacian pyramid residuals. To mitigate the weakening of semantic features during convolution processes, we introduce a feature-enhanced combination strategy. We also integrate the DeconvUp module to improve the restoration of depth map boundaries. We introduce a boundary loss that enforces constraints between object boundaries. We propose an extended evaluation method that utilizes Laplacian pyramid residuals to evaluate boundary depth. Extensive evaluations on the KITTI, Cityscapes, and Make3D datasets demonstrate the superior performance of MFFENet compared to state-of-the-art models in monocular depth estimation.
C1 [Song, Chao; Chen, Qingjie; Jiang, Zhaoyi; Yang, Bailin] Zhejiang Gongshang Univ, Hangzhou, Peoples R China.
   [Li, Frederick W. B.] Univ Durham, Durham, England.
   [Zheng, Dong] UNIUBI Res, Hangzhou, Peoples R China.
   [Shen, Yuliang] Zhejiang Zoland Animat Co Ltd, Hangzhou, Peoples R China.
C3 Zhejiang Gongshang University; Durham University
RP Yang, BL (corresponding author), Zhejiang Gongshang Univ, Hangzhou, Peoples R China.
EM csong@zjsu.edu.cn; chenqingjieslr@gmail.com; frederick.li@durham.ac.uk;
   zyjiang@zjsu.edu.cn; zhengdong@uni-ubi.com; 36232185@qq.com;
   ybl@zjgsu.edu.cn
OI Song, Chao/0000-0002-9415-4929
FU The "Pioneer" and "Leading Goose" R & D Program of Zhejiang Province
   [2023C01150]; National Natural Science Foundation of China [62172366];
   Scientific Key Research Program Project of Hangzhou [2022AIZD0110];
   Zhejiang Provincial Natural Science Foundation of China [LY22F020013]
FX The authors express gratitude for the support received from the
   "Pioneer" and "Leading Goose" R & D Program of Zhejiang Province
   (2023C01150), the National Natural Science Foundation of China (No.
   62172366), Scientific Key Research Program Project of Hangzhou (No.
   2022AIZD0110) and the Zhejiang Provincial Natural Science Foundation of
   China (No. LY22F020013).
CR Bae J., 2023, P AAAI C ART INT WAS, P187
   Chang Shu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P572, DOI 10.1007/978-3-030-58529-7_34
   Chen PY, 2019, PROC CVPR IEEE, P2619, DOI 10.1109/CVPR.2019.00273
   Chen XY, 2023, IEEE WINT CONF APPL, P5765, DOI 10.1109/WACV56688.2023.00573
   Choi J., 2020, 34 C NEUR INF PROC S
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Eigen D, 2014, ADV NEUR IN, V27
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Guizilini V, 2020, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR42600.2020.00256
   Hirschmüller H, 2005, PROC CVPR IEEE, P807, DOI 10.1109/cvpr.2005.56
   Johnston A, 2020, PROC CVPR IEEE, P4755, DOI 10.1109/CVPR42600.2020.00481
   Jung H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12622, DOI 10.1109/ICCV48922.2021.01241
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Kline J, 2020, PROCEEDINGS OF THE 2020 32ND INTERNATIONAL TELETRAFFIC CONGRESS (ITC 32), P1, DOI [10.1109/ITC3249928.2020.00009, 10.1007/978-3-030-58565-5_35]
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Lee Y, 2022, PROC CVPR IEEE, P7277, DOI 10.1109/CVPR52688.2022.00714
   Lyu XY, 2021, AAAI CONF ARTIF INTE, V35, P2294
   Minsoo Song, 2021, IEEE Transactions on Circuits and Systems for Video Technology, V31, P4381, DOI 10.1109/TCSVT.2021.3049869
   Peng R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15540, DOI 10.1109/ICCV48922.2021.01527
   Sun Libo, 2023, IEEE Transactions on Pattern Analysis and Machine Intelligence
   Sun QY, 2022, IEEE T NEUR NET LEAR, V33, P2023, DOI 10.1109/TNNLS.2021.3100895
   Tosi F, 2019, PROC CVPR IEEE, P9791, DOI 10.1109/CVPR.2019.01003
   Wang CY, 2018, PROC CVPR IEEE, P2022, DOI 10.1109/CVPR.2018.00216
   Yang N, 2020, PROC CVPR IEEE, P1278, DOI 10.1109/CVPR42600.2020.00136
   Yuan Y., 2021, P C NEUR INF PROC SY, P7281
   Zhang N, 2023, PROC CVPR IEEE, P18537, DOI 10.1109/CVPR52729.2023.01778
   Zhang YR, 2022, IEEE T IMAGE PROCESS, V31, P3251, DOI 10.1109/TIP.2022.3167307
   Zhao CQ, 2022, INT CONF 3D VISION, P668, DOI 10.1109/3DV57658.2022.00077
   Zhou Hang, 2021, BRIT MACH VIS C BMVC
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
   Zhou ZK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12757, DOI 10.1109/ICCV48922.2021.01254
   Zhu S., 2020, P IEEE CVF C COMP VI, P13116
NR 34
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4955
EP 4967
DI 10.1007/s00371-024-03498-w
EA JUN 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001251894500003
DA 2024-08-05
ER

PT J
AU Yang, J
   Wu, ZL
   Wu, RB
AF Yang, Jun
   Wu, Zilu
   Wu, Renbiao
TI Micro-expression recognition based on contextual transformer networks
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Micro-expression recognition; Contextual transformer; Dual-path network;
   Apex frame
AB Micro-expression is a kind of unintentional facial expression that can reflect the genuine emotion. As a contactless affective computing method, micro-expressions have received attention from psychology and computer vision experts. Due to the characteristics of micro-expressions such as short duration, low intensity, and sparse facial action units, it is a challenging task to learn effective features of micro-expressions from images or videos. To address the above problems, we propose a contextual transformer-based dual-path network. Firstly, the apex frame conveys the most emotional information expressed in micro-expression, so it is taken as the object for feature extraction. Then, a dual-path network is used as the basic feature extraction network to improve the ability of fine feature extraction and exploring new features. Secondly, the contextual transformer module is embedded to achieve the extraction and fusion of global and neighboring local information of micro-expressions. Finally, to solve the problem of sample imbalance, the focal loss function is adopted to improve the classification performance of difficult-to-classify samples by selectively weighting the loss values. The proposed micro-expression recognition model is validated by the leave-one-subject-out cross-validation. Extensive experiments have been conducted with SMIC, CASME II, SAMM, and the fused datasets. Comparing with the baseline method, the unweighted F1 score and unweighted average recall on the fused dataset are improved by 0.1542 and 0.1863, respectively. The experimental results show that the proposed model outperforms the other mainstream models.
C1 [Yang, Jun; Wu, Zilu; Wu, Renbiao] Civil Aviat Univ China, Coll Elect Informat & Automat, Tianjin Key Lab Adv Signal Proc, Tianjin 300300, Peoples R China.
C3 Civil Aviation University of China
RP Wu, RB (corresponding author), Civil Aviat Univ China, Coll Elect Informat & Automat, Tianjin Key Lab Adv Signal Proc, Tianjin 300300, Peoples R China.
EM rbwu@cauc.edu.cn
FU Open Fund of Tianjin Key Lab for Advanced Signal Processing, Civil
   Aviation University of China [2022ASP-TJ03]; Fundamental Research Funds
   for the Central Universities of China [3122023011]
FX This study was funded by "the Open Fund of Tianjin Key Labfor Advanced
   Signal Processing, Civil Aviation University of China"(Grant Number
   2022ASP-TJ03) and "the Fundamental Research Fundsfor the Central
   Universities of China" (Grant Number 3122023011)
CR [Anonymous], 2009, Protecting Airline Passengers in the Age of Terrorism, DOI DOI 10.5040/9798216002246.CH-005
   [Anonymous], 1966, Methods of research in psychotherapy, DOI [10.1007/978-1-4684-6045-2_14, DOI 10.1007/978-1-4684-6045-2_14]
   Ben XY, 2022, IEEE T PATTERN ANAL, V44, P5826, DOI 10.1109/TPAMI.2021.3067464
   Chattopadhay A, 2018, IEEE WINT CONF APPL, P839, DOI 10.1109/WACV.2018.00097
   Chen YP, 2017, ADV NEUR IN, V30
   Davison Adrian K., 2018, IEEE Transactions on Affective Computing, V9, P116, DOI 10.1109/TAFFC.2016.2573832
   EKMAN P, 1969, PSYCHIATR, V32, P88, DOI 10.1080/00332747.1969.11023575
   Franke M., 2009, ELECT TECHNOLOGY ISS, P1
   Gan YS, 2024, VISUAL COMPUT, V40, P585, DOI 10.1007/s00371-023-02803-3
   Gan YS, 2019, SIGNAL PROCESS-IMAGE, V74, P129, DOI 10.1016/j.image.2019.02.005
   Gholamalinezhad H, 2020, Arxiv, DOI arXiv:2009.07485
   Gong W, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3539576
   Goyani Mahesh, 2020, International Journal of Computers and Applications, V42, P360, DOI 10.1080/1206212X.2017.1395134
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang J, 2020, 2020 13TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2020), P163, DOI [10.1109/CISP-BMEI51763.2020.9263671, 10.1109/cisp-bmei51763.2020.9263671]
   Huang XH, 2016, NEUROCOMPUTING, V175, P564, DOI 10.1016/j.neucom.2015.10.096
   Kim DH, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P382, DOI 10.1145/2964284.2967247
   Lei L, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2237, DOI 10.1145/3394171.3413714
   Li XB, 2018, IEEE T AFFECT COMPUT, V9, P563, DOI 10.1109/TAFFC.2017.2667642
   Li XB, 2013, IEEE INT CONF AUTOMA, DOI 10.1109/FG.2013.6553717
   Li YT, 2021, IEEE T IMAGE PROCESS, V30, P249, DOI 10.1109/TIP.2020.3035042
   Li YH, 2023, IEEE T PATTERN ANAL, V45, P1489, DOI 10.1109/TPAMI.2022.3164083
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Liong ST, 2018, SIGNAL PROCESS-IMAGE, V62, P82, DOI 10.1016/j.image.2017.11.006
   Liu YJ, 2016, IEEE T AFFECT COMPUT, V7, P299, DOI 10.1109/TAFFC.2015.2485205
   Martin BC., 2009, The Philosophy of Deception, DOI [10.1093/acprof:oso/9780195327939.001.0001, DOI 10.1093/ACPROF:OSO/9780195327939.001.0001]
   Niu ruihua, 2021, Journal of Computer Applications, V41, P2552, DOI 10.11772/j.issn.1001-9081.2020111743
   Pan H, 2020, MULTIMED TOOLS APPL, V79, P31451, DOI 10.1007/s11042-020-09475-4
   Peng M, 2019, INT CONF AFFECT, DOI [10.1109/ACII.2019.8925525, 10.1109/acii.2019.8925525]
   POLIKOVSKY S., 2009, P IET SEM 2009, P1, DOI [10.1049/ic.2009.0244, DOI 10.1049/IC.2009.0244]
   Qu FB, 2018, IEEE T AFFECT COMPUT, V9, P424, DOI 10.1109/TAFFC.2017.2654440
   See J, 2019, IEEE INT CONF AUTOMA, P647
   SHREVE M., 2009, APPL COMPUTER VISION, P1, DOI [DOI 10.1109/WACV.2009.5403044, 10.1109/WACV.2009.5403044]
   Shu X, 2023, MULTIMEDIA SYST, V29, P1593, DOI 10.1007/s00530-023-01068-z
   Song BL, 2019, IEEE ACCESS, V7, P184537, DOI 10.1109/ACCESS.2019.2960629
   Sun LN, 2021, IEEE T CIRC SYST VID, V31, P4321, DOI 10.1109/TCSVT.2021.3054471
   [唐宏 Tang Hong], 2022, [信号处理, Journal of Signal Processing], V38, P1075
   Thuseethan S, 2023, INFORM SCIENCES, V630, P341, DOI 10.1016/j.ins.2022.11.113
   VanQuang N., 2019, IEEE INT CONF AUTOMA, P1, DOI [DOI 10.1109/fg.2019.8756544, 10.1109/FG.2019.8756544]
   Wang YD, 2015, LECT NOTES COMPUT SC, V9003, P525, DOI 10.1007/978-3-319-16865-4_34
   Yan WJ, 2013, IEEE INT CONF AUTOMA
   Yan WJ, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0086041
   [姚鸿勋 Yao Hongxun], 2022, [中国图象图形学报, Journal of Image and Graphics], V27, P2008
   Zhi RC, 2022, PATTERN RECOGN LETT, V163, P25, DOI 10.1016/j.patrec.2022.09.006
   Zhou L, 2019, IEEE INT CONF MULTI, P102, DOI 10.1109/ICMEW.2019.00025
NR 48
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 12
PY 2024
DI 10.1007/s00371-024-03443-x
EA JUN 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TX6L6
UT WOS:001244596700001
DA 2024-08-05
ER

PT J
AU Wang, HJ
   Cui, BY
   Yuan, QB
   Pu, GQ
   Liu, XL
   Zhu, J
AF Wang, Huijuan
   Cui, Boyan
   Yuan, Quanbo
   Pu, Gangqiang
   Liu, Xueli
   Zhu, Jie
TI Mini-3DCvT: a lightweight lip-reading method based on 3D convolution
   visual transformer
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Lip-reading; 3D convolution; Visual transformer; Spatial-temporal
   features; Model light weighting
AB Lip-reading has attracted more and more attention in recent years, and has wide application prospects and value in areas such as human-computer interaction, surveillance and security and audiovisual speech recognition. However, research on lip-reading has been slow due to the complexity of dealing with the fine spatial features of small-sized images of continuous video frames and the temporal features between images. In this paper, to address the challenges in extracting visual spatial features, temporal features and model light weighting, we propose a high-precision, highly robust and lightweight lip-reading method, Mini-3DCvT, which combines visual transforms and 3D convolution to extract spatiotemporal feature of continuous images, and makes full use of the properties of convolution and transforms to effectively extract local and global features of continuous images, use weight transformation and weight distillation in the convolution and transformer structures for model compression, and then send the extracted features to a bidirectional gated recurrent unit for sequence modeling. Experimental results on the large-scale public lip-reading datasets LRW and LRW-1000 show that this paper's method achieves 88.3% and 57.1% recognition accuracy on both datasets, and effectively reduces the model computation and number of parameters, improving the overall performance of the lip-reading model.
C1 [Wang, Huijuan; Cui, Boyan; Yuan, Quanbo; Pu, Gangqiang; Liu, Xueli; Zhu, Jie] North China Inst Aerosp Engn, Sch Comp, Langfang 065000, Peoples R China.
C3 North China Institute of Aerospace Engineering
RP Wang, HJ (corresponding author), North China Inst Aerosp Engn, Sch Comp, Langfang 065000, Peoples R China.
EM wanghj323@126.com
OI Wang, Huijuan/0000-0003-1598-819X
FU Central Government Guided Local Science and Technology Development
FX No Statement Available
CR Afouras T, 2022, IEEE T PATTERN ANAL, V44, P8717, DOI 10.1109/TPAMI.2018.2889052
   Afouras T, 2019, Arxiv, DOI arXiv:1907.04975
   [Anonymous], Layer Normalization
   Aswani A.V., 2017, P NIPS
   Baart M, 2015, J MEM LANG, V85, P42, DOI 10.1016/j.jml.2015.06.008
   Bowden R, 2013, PROC SPIE, V8901, DOI 10.1117/12.2029464
   Chen LL, 2019, PROC CVPR IEEE, P7824, DOI 10.1109/CVPR.2019.00802
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Chung JS, 2017, LECT NOTES COMPUT SC, V10112, P87, DOI 10.1007/978-3-319-54184-6_6
   Dehghani Mostafa, 2019, Universal transformers
   Devlin J, 2018, ARXIV
   Ding RW, 2018, IEEE IMAGE PROC, P4138, DOI 10.1109/ICIP.2018.8451096
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Feng DL, 2021, IEEE INT CONF MULTI, DOI 10.1109/ICMEW53276.2021.9456014
   Hendrycks D., Gaussian error linear units (gelus)
   Hinton G., 2015, NEURIPS DEEP LEARN R, P9
   Jiao XQ, 2020, Arxiv, DOI arXiv:1909.10351
   Jiao XQ, 2020, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020, P4163
   Kim M, 2022, AAAI CONF ARTIF INTE, P1174
   Kim M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P296, DOI 10.1109/ICCV48922.2021.00036
   Liu JW, 2021, PROC CVPR IEEE, P4368, DOI 10.1109/CVPR46437.2021.00435
   Liu LC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P227, DOI 10.1145/3474085.3475566
   Luo M., 2020, P BMVC
   Ma PC, 2021, Arxiv, DOI arXiv:2007.06504
   Mahajan D, 2018, LECT NOTES COMPUT SC, V11206, P185, DOI 10.1007/978-3-030-01216-8_12
   Martinez B, 2020, INT CONF ACOUST SPEE, P6319, DOI [10.1109/icassp40776.2020.9053841, 10.1109/ICASSP40776.2020.9053841]
   Mathulaprangsan S, 2015, PROCEEDINGS OF 2015 INTERNATIONAL CONFERENCE ON ORANGE TECHNOLOGIES (ICOT), P22, DOI 10.1109/ICOT.2015.7498485
   Petridis S., 2017, P BRIT MACH VIS C
   Petridis S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6548, DOI 10.1109/ICASSP.2018.8461326
   Prajwal K. R., 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13793, DOI 10.1109/CVPR42600.2020.01381
   Pu GQ, 2023, VISUAL COMPUT, V39, P3041, DOI 10.1007/s00371-022-02511-4
   Seymour R, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/810362
   Sharma P., ALBERT: a lite BERT for selfsupervised learning of language representations. International Conference on Learning Representations; 2019
   Stafylakis T., 2018, Zero-shot keyword spotting for visual speech recognition in-the-wild, DOI [10.1007/978-3-030-01225-032, DOI 10.1007/978-3-030-01225-032]
   SUMBY WH, 1954, J ACOUST SOC AM, V26, P212, DOI 10.1121/1.1907309
   Sun ZQ, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2158
   Wang HJ, 2022, IEEE ACCESS, V10, P77205, DOI 10.1109/ACCESS.2022.3193231
   Wang YQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12006, DOI 10.1109/ICCV48922.2021.01181
   Watanabe T, 2017, LECT NOTES COMPUT SC, V10117, P303, DOI 10.1007/978-3-319-54427-4_23
   Wu HP, 2021, Arxiv, DOI [arXiv:2103.15808, 10.48550/arXiv.2103.15808, DOI 10.48550/ARXIV.2103.15808, 10.1109/iccv48922.2021.00009]
   Xiao JY, 2020, IEEE INT CONF AUTOMA, P364, DOI 10.1109/FG47880.2020.00132
   Xu B., 2020, Discriminative multi-modality speech recognition
   [薛峰 Xue Feng], 2022, [模式识别与人工智能, Pattern Recognition and Artificial Intelligence], V35, P1111
   Yang S, 2019, IEEE INT CONF AUTOMA, P222, DOI 10.1109/fg.2019.8756582
   Yang X, 2024, IEEE T MULTIMEDIA, V26, P7237, DOI 10.1109/TMM.2024.3362136
   Yang X, 2021, IEEE T IMAGE PROCESS, V30, P6266, DOI 10.1109/TIP.2021.3093759
   Yuan K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P559, DOI 10.1109/ICCV48922.2021.00062
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang H., 2018, INT C LEARNING REPRE
   Zhang JNA, 2022, PROC CVPR IEEE, P12135, DOI 10.1109/CVPR52688.2022.01183
   Zhang Q, 2023, VISUAL COMPUT, V39, P4593, DOI 10.1007/s00371-022-02611-1
   Zhang XX, 2019, IEEE I CONF COMP VIS, P713, DOI 10.1109/ICCV.2019.00080
   [周飞燕 Zhou Feiyan], 2017, [计算机学报, Chinese Journal of Computers], V40, P1229
   Zhou ZH, 2014, IMAGE VISION COMPUT, V32, P590, DOI 10.1016/j.imavis.2014.06.004
   Zhu XS, 2023, VISUAL COMPUT, V39, P4721, DOI 10.1007/s00371-022-02620-0
NR 55
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 11
PY 2024
DI 10.1007/s00371-024-03515-y
EA JUN 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TY4Z7
UT WOS:001244819600003
DA 2024-08-05
ER

PT J
AU Wang, HQ
   Sourin, A
AF Wang, Hanqin
   Sourin, Alexei
TI Visual signatures for music mood and timbre
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Music visualization; Music information retrieval; Music classification;
   Visual signatures for music
ID COLOR
AB Existing research on music visualization has primarily focused on creating animated visual illustrations to accompany the music being played based on fundamental attributes such as sound frequency or music structure, whereas the higher-level features, including mood and timbre, are mostly overlooked. In this paper, we propose visual signatures to describe the higher-level attributes of music, where the content and the color palette of the visual signatures are controlled by the music mood and timbre, respectively. We expect that the users with different cultural and educational backgrounds will be able to easily interpret the meaning of sound with the proposed visual signatures. In our work, we used a contrastive learning neural network for mood classification and an audio transformer for timbre classification. The performance of the music classification models is examined by their accuracy, while multiple generated images are displayed to showcase the feasibility of visual signatures.
C1 [Wang, Hanqin; Sourin, Alexei] Nanyang Technol Univ, Singapore, Singapore.
C3 Nanyang Technological University
RP Wang, HQ (corresponding author), Nanyang Technol Univ, Singapore, Singapore.
EM hanqin001@e.ntu.edu.sg; assourin@ntu.edu.sg
CR Agostinelli A., 2023, arXiv
   Arai Kota, 2023, IUI '23: Proceedings of the 28th International Conference on Intelligent User Interfaces, P850, DOI 10.1145/3581641.3584053
   Bosch J., 2014, IRMAS: a dataset for instrument recognition in musical audio signals
   Brunner G, 2017, PROC INT C TOOLS ART, P519, DOI 10.1109/ICTAI.2017.00085
   Chen CH, 2008, LECT NOTES COMPUT SC, V4903, P358
   Chong K.S., 2023, 2023 INT C CYB CW, P193, DOI [10.1109/CW58918.2023.00036, DOI 10.1109/CW58918.2023.00036]
   De Carolis L, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0208874
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Dharmapriya J., 2021, Music Emotion Visualization Through Colour
   Dosovitskiy A., 2021, ICLR
   Fort M, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-23623-w
   Gong Y, 2022, AAAI CONF ARTIF INTE, P10699
   Gong Y, 2021, INTERSPEECH, P571, DOI 10.21437/Interspeech.2021-698
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Graven T., 2019, J. Vis. Exp, V151, P59954
   Griscom William., 2014, Visualizing Sound: Cross-Modal Mapping Between Music and Color
   Guzhov A, 2022, INT CONF ACOUST SPEE, P976, DOI 10.1109/ICASSP43922.2022.9747631
   Guzhov A, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9533654
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermann T., 2011, The Sonification Handbook
   Hernandez-Olivan C., 2021, arXiv
   Hu GS, 2020, COLOR RES APPL, V45, P862, DOI 10.1002/col.22532
   Huang QQ, 2022, Arxiv, DOI arXiv:2208.12415
   Hussain AR., 2021, Art Des. Rev, V9, P301
   Jacques D, 2021, LANDSCAPE RES, V46, P116, DOI 10.1080/01426397.2020.1832204
   Jeong D., 2021, arXiv preprint arXiv:2102.04680, V2, P10
   Jolly N., 2021, The odd story of how Brian Eno composed the Windows 95 startup sound
   Jonauskaite D, 2019, COLOR RES APPL, V44, P272, DOI 10.1002/col.22327
   Kandinsky Wassily., 2012, SPIRITUAL ART
   Karras T., 2020, P IEEE CVF C COMP VI, P8110, DOI [10.1109/cvpr42600.2020.00813, DOI 10.1109/CVPR42600.2020.00813]
   Karras T, 2021, ADV NEUR IN, V34
   Khulusi R, 2020, COMPUT GRAPH FORUM, V39, P82, DOI 10.1111/cgf.13905
   Kim JW, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P161, DOI 10.1109/ICASSP.2018.8461329
   Li DY, 2018, LANDSCAPE URBAN PLAN, V173, P33, DOI 10.1016/j.landurbplan.2018.01.009
   Li KK, 2021, FRONT PSYCHOL, V12, DOI 10.3389/fpsyg.2021.629650
   Liang Y, 2021, IUI '21 - 26TH INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P175, DOI 10.1145/3397481.3450700
   Liao NJ, 2022, J INTELL SYST, V31, P289, DOI 10.1515/jisys-2022-0016
   Lima HB, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3461835
   Liu YH, 2019, Arxiv, DOI [arXiv:1907.11692, DOI 10.48550/ARXIV.1907.11692]
   Melechovsky J, 2024, Arxiv, DOI arXiv:2311.08355
   Nasrullah Z, 2019, IEEE IJCNN
   Oord A.v.d., 2016, arXiv, DOI DOI 10.48550/ARXIV.1609.03499
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Podell D, 2023, Arxiv, DOI arXiv:2307.01952
   Radford A, 2021, PR MACH LEARN RES, V139
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Siedenburg K., 2009, WELC 3 INT WORKSH LE, P17
   Skorokhodov I, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14124, DOI 10.1109/ICCV48922.2021.01388
   Sun XH, 2022, PROCEEDINGS OF THE 2022 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2022, P480, DOI 10.1145/3512527.3531393
   Van Cott H., 1972, Human engineering guide to equipment design
   Verma G, 2019, INT CONF ACOUST SPEE, P3975, DOI 10.1109/ICASSP.2019.8683133
   Wahab HYB, 2021, 2021 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2021), P125, DOI 10.1109/CW52790.2021.00027
   Wang H., 2023, SIGGRAPH AS 2023 SA, DOI [10.1145/3610542.3626122, DOI 10.1145/3610542.3626122]
   Wang H., 2023, 2023 INT C CYB CW, P32, DOI [10.1109/CW58918.2023.00015, DOI 10.1109/CW58918.2023.00015]
   Ward J, 2006, CORTEX, V42, P264, DOI 10.1016/S0010-9452(08)70352-6
   Won M, 2020, Arxiv, DOI arXiv:2006.00751
   Wu B, 2014, J AUDIO ENG SOC, V62, P663, DOI 10.17743/jaes.2014.0037
   Wu SD, 2023, Arxiv, DOI arXiv:2304.11029
   Zeghidour N, 2022, IEEE-ACM T AUDIO SPE, V30, P495, DOI 10.1109/TASLP.2021.3129994
   Zhao SC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2945, DOI 10.1145/3394171.3413776
   ZUBE EH, 1981, LANDSCAPE PLAN, V8, P69, DOI 10.1016/0304-3924(81)90041-1
NR 61
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 31
PY 2024
DI 10.1007/s00371-024-03417-z
EA MAY 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR2C4
UT WOS:001236102600002
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Peng, HF
   Lu, X
   Xia, DX
   Xie, XY
AF Peng, Houfu
   Lu, Xing
   Xia, Daoxun
   Xie, Xiaoyao
TI A novel image restoration solution for cross-resolution person
   re-identification
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Cross-resolution person re-identification; Super-resolution; Image
   restoration; Swin transformer
AB Cross-resolution person re-identification (CR-ReID) is a highly practical task that primarily addresses the image misalignment issue due to image resolution variations, which are caused by differences in the distances between cameras and camera performance variations. The existing cutting-edge approaches attempted to use super-resolution (SR) techniques to recover lost details in low-resolution (LR) images. However, the existing SR techniques focus on improving low-level semantic information metrics. Evaluation techniques for high-level semantic recognition tasks are not well suited for low-level image quality metrics. We propose a new framework called image feature restoration using a swin transformer (IFRSW), which uses the feature difference between LR and high-resolution (HR) images as the supervisory signal to constrain the SR module. We further improved the swin transformer by introducing a new multiresolution feature fusion strategy, enhancing its ability to extract features from multiresolution images. Additionally, we introduce a pooling technique called "softpooling" to preserve more information during the feature downsampling process. Our method exhibits a noteworthy 4.2% in rank 1 accuracy improvement on a challenging real LR dataset CAVIR, surpassing the current optimal approach. Our method achieves superior performance to the existing state-of-the-art (SOTA) methods.
C1 [Peng, Houfu; Lu, Xing; Xia, Daoxun] Guizhou Normal Univ, Sch Big Data & Comp Sci, Guiyang 550000, Guizhou, Peoples R China.
   [Xia, Daoxun; Xie, Xiaoyao] Guizhou Normal Univ, Guizhou Key Lab Informat & Comp Sci, Guiyang 550000, Guizhou, Peoples R China.
   [Xia, Daoxun] Guizhou Normal Univ, Engn Lab Appl Technol Big Data Educ, Guiyang 550000, Peoples R China.
C3 Guizhou Normal University; Guizhou Normal University; Guizhou Normal
   University
RP Xia, DX (corresponding author), Guizhou Normal Univ, Sch Big Data & Comp Sci, Guiyang 550000, Guizhou, Peoples R China.; Xia, DX; Xie, XY (corresponding author), Guizhou Normal Univ, Guizhou Key Lab Informat & Comp Sci, Guiyang 550000, Guizhou, Peoples R China.; Xia, DX (corresponding author), Guizhou Normal Univ, Engn Lab Appl Technol Big Data Educ, Guiyang 550000, Peoples R China.
EM dxxia@gznu.edu.cn; xyx@gznu.edu.cn
FU National Natural Science Foundation of China [62166008]; National
   Natural Science Foundation of China [QKHPTRC-YQK[2023]028]; Excellent
   Youth Science and Technology Talent in Guizhou Province [[2022]4054];
   Central Government Guides Local Science and Technology Development
   Special Project [ZKHT[2023]48-9]; Guiyang Science and Technology Talent
   Training Project [[2022]31]; Academic Budding Talent Fund Project of
   Guizhou Normal University [[2023]011]; Natural Science Research Project
   of Guizhou Provincial Department of Education
FX This work is supported by the National Natural Science Foundation of
   China (No. 62166008), the Excellent Youth Science and Technology Talent
   in Guizhou Province (QKHPTRC-YQK[2023]028), the Central Government
   Guides Local Science and Technology Development Special Project (No.
   QKZYD[2022]4054), the Guiyang Science and Technology Talent Training
   Project (No. ZKHT[2023]48-9), the Academic Budding Talent Fund Project
   of Guizhou Normal University (No. QSXM[2022]31), and the Natural Science
   Research Project of Guizhou Provincial Department of Education (No.
   QJJ[2023]011).
CR Bak S, 2014, 2014 11TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P175, DOI 10.1109/AVSS.2014.6918664
   Chen Z, 2023, IEEE I CONF COMP VIS, P12278, DOI 10.1109/ICCV51070.2023.01131
   Chen ZH, 2021, VISUAL COMPUT, V37, P2657, DOI 10.1007/s00371-021-02199-y
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Cheng DS, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.68
   Cheng ZY, 2020, PROC CVPR IEEE, P2602, DOI 10.1109/CVPR42600.2020.00268
   Dong Q, 2019, IEEE I CONF COMP VIS, P3651, DOI 10.1109/ICCV.2019.00375
   Ge YX, 2018, ADV NEUR IN, V31
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   Gu JY, 2023, PROC CVPR IEEE, P19243, DOI 10.1109/CVPR52729.2023.01844
   Gu JX, 2018, PATTERN RECOGN, V77, P354, DOI 10.1016/j.patcog.2017.10.013
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang YK, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P365, DOI 10.1145/3343031.3350994
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Jiao JN, 2018, AAAI CONF ARTIF INTE, P6967
   Jing XY, 2015, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2015.7298669
   Karanam S, 2015, IEEE I CONF COMP VIS, P4516, DOI 10.1109/ICCV.2015.513
   Ke Han, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P193, DOI 10.1007/978-3-030-58574-7_12
   Kim Y, 2020, IEEE T CIRC SYST VID, V30, P1121, DOI 10.1109/TCSVT.2019.2901919
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li X, 2015, IEEE I CONF COMP VIS, P3765, DOI 10.1109/ICCV.2015.429
   Li YJ, 2019, IEEE I CONF COMP VIS, P8089, DOI 10.1109/ICCV.2019.00818
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu JX, 2018, PROC CVPR IEEE, P4099, DOI 10.1109/CVPR.2018.00431
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Martinel N, 2019, IEEE COMPUT SOC CONF, P1544, DOI 10.1109/CVPRW.2019.00196
   Matsukawa T, 2016, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2016.152
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Peng C, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2116
   Sheng B, 2022, IEEE T CYBERNETICS, V52, P6662, DOI 10.1109/TCYB.2021.3079311
   Stergiou A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10337, DOI 10.1109/ICCV48922.2021.01019
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Wang Z., 2016, P IICAI, V2, P6
   Wang Z, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3891
   Xia DX, 2020, J INTELL FUZZY SYST, V39, P7495, DOI 10.3233/JIFS-200793
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Zhang GQ, 2021, IEEE T IMAGE PROCESS, V30, P8913, DOI 10.1109/TIP.2021.3120054
   Zhang JJ, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2036
   Zhang K, 2022, IEEE T PATTERN ANAL, V44, P6360, DOI 10.1109/TPAMI.2021.3088914
   Zhao ZX, 2023, Arxiv, DOI arXiv:2303.08942
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541
   Zhou YP, 2023, Arxiv, DOI arXiv:2303.09735
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 51
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 27
PY 2024
DI 10.1007/s00371-024-03471-7
EA MAY 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SC2Z5
UT WOS:001232207700001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Liao, FX
   Wu, TF
   Gao, FM
   Leng, L
AF Liao, Fengxiang
   Wu, Tengfei
   Gao, Fumeng
   Leng, Lu
TI Translational calibration in region-of-interest localization for
   palmprint recognition
SO VISUAL COMPUTER
LA English
DT Review; Early Access
DE Double-line-single-point; Boundary line calibration; Finger valley
   calibration; Region of line detection; Region of point detection
ID SYSTEM; EXTRACTION
AB Palmprint is a promising biometric modality and has several advantages. However, many serious challenges severely restrict the development of palmprint recognition in mobile environments. Double-line-single-point (DLSP), as a state-of-the-art assisting graph, can be free from pre-processing, and effectively reduce the errors of pre-processing; however, its accuracy heavily depends on users' cooperation and operation proficiency. This paper proposes boundary line calibration (BLC) and finger valley calibration (FVC) to suppress the translational dislocations in DLSP. Firstly, the samples acquired according to DLSP are rotated around the center so that the assisting lines are horizontal. Secondly, a rectangular area is cropped as the region of line detection (RLD) with the assisting line as the reference; while a rectangular area is cropped as the region of point detection (RPD) with the assisting point as the reference. The cropped rectangular regions are convolved with Gabor filters along an optimized direction to enhance edges. The accurate palm boundary is localized in RLD to reduce the vertical dislocation; and the accurate inter-finger valley point (IFVP) is localized in RPD to reduce the horizontal dislocation. The sufficient experiments show that the proposed algorithm can calibrate the translational dislocations caused by users' imperfect cooperation, and accordingly remarkably improve the accuracy and users' comfort.
C1 [Liao, Fengxiang; Wu, Tengfei; Gao, Fumeng; Leng, Lu] Nanchang Hangkong Univ, Key Lab Jiangxi Prov Image Proc & Pattern Recognit, Nanchang 330063, Peoples R China.
   [Liao, Fengxiang; Wu, Tengfei; Gao, Fumeng; Leng, Lu] Nanchang Hangkong Univ, Sch Software, Nanchang 330063, Peoples R China.
C3 Nanchang Hangkong University; Nanchang Hangkong University
RP Leng, L (corresponding author), Nanchang Hangkong Univ, Key Lab Jiangxi Prov Image Proc & Pattern Recognit, Nanchang 330063, Peoples R China.; Leng, L (corresponding author), Nanchang Hangkong Univ, Sch Software, Nanchang 330063, Peoples R China.
EM 2116083500008@stu.nchu.edu.cn; wutengfeics@gmail.com; 483384496@qq.com;
   leng@nchu.edu.cn
FU National Natural Science Foundation of China; Technology Innovation
   Guidance Program Project (Special Project of Technology Cooperation,
   Science and Technology Department of Jiangxi Province) [20212BDH81003];
   Innovation Foundation for Postgraduate Students of Nanchang Hangkong
   University [YC2022-170];  [61866028]
FX This research was supported by National Natural Science Foundation of
   China (61866028), Technology Innovation Guidance Program Project
   (Special Project of Technology Cooperation, Science and Technology
   Department of Jiangxi Province) (20212BDH81003), and Innovation
   Foundation for Postgraduate Students of Nanchang Hangkong University
   (YC2022-170).
CR [Anonymous], 2013, INT WORKSHOP ADV IMA
   Aykut M, 2015, IMAGE VISION COMPUT, V40, P65, DOI 10.1016/j.imavis.2015.05.002
   Aykut M, 2013, PATTERN RECOGN LETT, V34, P955, DOI 10.1016/j.patrec.2013.02.016
   Balwant MK, 2015, PROCEDIA COMPUT SCI, V54, P799, DOI 10.1016/j.procs.2015.06.094
   Bao XJ, 2016, INT CONF IMAG PROC
   [曹奎顺 Cao Kuishun], 2018, [光电子·激光, Journal of Optoelectronics·Laser], V29, P205
   Cao ZW, 2022, LECT NOTES COMPUT SC, V13672, P737, DOI 10.1007/978-3-031-19775-8_43
   Chai T., 2022, Vis. Comput, V39, P1
   Chen ZH, 2021, VISUAL COMPUT, V37, P2657, DOI 10.1007/s00371-021-02199-y
   Fei LK, 2023, IEEE T NEUR NET LEAR, V34, P9783, DOI 10.1109/TNNLS.2022.3160597
   Fei LK, 2016, IEEE T HUM-MACH SYST, V46, P787, DOI 10.1109/THMS.2016.2586474
   Fei LK, 2016, PATTERN RECOGN LETT, V69, P35, DOI 10.1016/j.patrec.2015.10.003
   Fei LK, 2016, PATTERN RECOGN, V49, P89, DOI 10.1016/j.patcog.2015.08.001
   Franzgrote M., 2011 INT C HAND BASE, P1
   Gao F., 2018, J. Multimed. Inf. Sys, V5, P221
   Gomez-Barrero M, 2014, INFORM SCIENCES, V268, P103, DOI 10.1016/j.ins.2013.06.015
   Guo ZH, 2009, PATTERN RECOGN LETT, V30, P1219, DOI 10.1016/j.patrec.2009.05.010
   Izadpanahkakhk M, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8071210
   Jaswal G, 2018, INT CO SIG PROC COMM, P322, DOI 10.1109/SPCOM.2018.8724419
   Jia W, 2008, PATTERN RECOGN, V41, P1504, DOI 10.1016/j.patcog.2007.10.011
   Khan Z, 2011, IEEE I CONF COMP VIS, P1935, DOI 10.1109/ICCV.2011.6126463
   Kim JS, 2015, IEEE T CONSUM ELECTR, V61, P311, DOI 10.1109/TCE.2015.7298090
   Kong A, 2006, PATTERN RECOGN, V39, P478, DOI 10.1016/j.patcog.2005.08.014
   Kong AWK, 2004, INT C PATT RECOG, P520, DOI 10.1109/ICPR.2004.1334184
   Kumar A, 2019, IEEE T INF FOREN SEC, V14, P34, DOI 10.1109/TIFS.2018.2837669
   Leng L., 2014, 2014 11 INT C INFORM, P10
   Leng L, 2018, PERS UBIQUIT COMPUT, V22, P93, DOI 10.1007/s00779-017-1105-2
   Leng L, 2015, PATTERN RECOGN, V48, P2290, DOI 10.1016/j.patcog.2015.01.021
   Liang X, 2023, IEEE J-STSP, V17, P662, DOI 10.1109/JSTSP.2023.3241540
   Liang X, 2021, IEEE SIGNAL PROC LET, V28, P1739, DOI 10.1109/LSP.2021.3103475
   Liang X, 2021, IEEE T SYST MAN CY-S, V51, P1534, DOI 10.1109/TSMC.2019.2898684
   Liu DF, 2021, AAAI CONF ARTIF INTE, V35, P6101
   Matkowski WM, 2020, IEEE T INF FOREN SEC, V15, P1601, DOI 10.1109/TIFS.2019.2945183
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Sang H., 2013, Robust palmprint recognition base on touch-less color palmprint images acquired
   Shi JG, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2094
   Sun XW, 2017, 2017 32ND YOUTH ACADEMIC ANNUAL CONFERENCE OF CHINESE ASSOCIATION OF AUTOMATION (YAC), P1123, DOI 10.1109/YAC.2017.7967579
   Sun ZN, 2005, PROC CVPR IEEE, P279
   Tiwari K, 2016, PROCEEDINGS OF 2016 FUTURE TECHNOLOGIES CONFERENCE (FTC), P577, DOI 10.1109/FTC.2016.7821664
   Xia QW, 2019, IEEE ACCESS, V7, P74327, DOI 10.1109/ACCESS.2019.2918778
   Xu NY, 2021, VISUAL COMPUT, V37, P695, DOI 10.1007/s00371-020-01962-x
   Xu Y, 2018, IEEE T SYST MAN CY-S, V48, P232, DOI 10.1109/TSMC.2016.2597291
   Yan LQ, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2220, DOI 10.1109/ICASSP39728.2021.9414517
   Yang Liu, 2020, IEEE Transactions on Biometrics, Behavior, and Identity Science, V2, P172, DOI 10.1109/TBIOM.2020.2967073
   Yang Z., 2023, IEEE Trans. Inf. Forens. Secur.
   Yang Z., 2023, IEEE Trans. Inst. Meas.
   Yang ZY, 2023, ARTIF INTELL REV, V56, P995, DOI 10.1007/s10462-022-10194-5
   Zhang D, 2003, IEEE T PATTERN ANAL, V25, P1041, DOI 10.1109/TPAMI.2003.1227981
   Zhang L, 2012, IEEE SIGNAL PROC LET, V19, P663, DOI 10.1109/LSP.2012.2211589
   Zhang Q., 2017, Comput. Eng. Design, V38, P2794
NR 50
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 25
PY 2024
DI 10.1007/s00371-024-03378-3
EA APR 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OO3N4
UT WOS:001208177700001
DA 2024-08-05
ER

PT J
AU Agrawal, S
   Natu, P
AF Agrawal, Supriya
   Natu, Prachi
TI OBB detector: occluded object detection based on geometric modeling of
   video frames
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Bounding box; Geometric modeling; Localization error; Occlusion
   detection; Overlapping objects; Security; Video surveillance
AB Object detection is an important research area in video surveillance systems, aimed at identifying and locating target objects within recorded scenes. Various object detectors fail when partial occlusion occurs in which only some features of the objects are visible due to overlapped bounding boxes. This situation can result in miscounting of the objects and misaligning the bounding boxes leading to localization loss. To address these problems, we have proposed a geometric-based axis-aligned bounding box method with occlusion prior conditions to estimate the location of overlapped bounding boxes with a single viewpoint. Firstly, the proposed method detects the closest points of the detected bounding boxes by extracting geometric features namely the width, height, and area of the detected objects. Secondly, occlusion prior condition is used to detect the partial occlusion and compute the overlapped area under different levels of occlusions such as (i) 20-40% and (ii) 40-70%. The performance of the proposed method has been tested on two benchmark datasets: Highway and PETS 2006, both containing outdoor video frames. The experimental results show that the proposed method can detect the objects under partial occlusion which are approximately 65% occluded with 92.7% precision in the Highway dataset and 85.1% precision in the PETS2006 dataset. Also, it has been observed that the bounding box localization loss of the proposed method has been improved by 1.76% in the Highway dataset and 2% in the PETS2006 dataset by generating the correct aligned bounding boxes on the detected objects, especially in the case of partial occlusion.
C1 [Agrawal, Supriya; Natu, Prachi] SVKMs Narsee Monjee Inst Management Studies NMIMS, Mukesh Patel Sch Technol Management & Engn, Dept Comp Engn, Mumbai, India.
C3 SVKM's NMIMS (Deemed to be University)
RP Agrawal, S (corresponding author), SVKMs Narsee Monjee Inst Management Studies NMIMS, Mukesh Patel Sch Technol Management & Engn, Dept Comp Engn, Mumbai, India.
EM supriya.agrawal@nmims.edu; prachi.natu@yahoo.com
CR Agrawal S, 2023, J SUPERCOMPUT, V79, P7937, DOI 10.1007/s11227-022-04972-9
   Amit Y., 2020, Computer Vision: A Reference Guide, P1, DOI 10.1007/978-3-030-03243-2_660-1
   BARTZ Dirk., 2005, - Tighter Bounding Volumes for Better Occlusion Culling Performance
   Bochinski Erik, 2017, 2017 14th IEEE International Conference on Advanced Video and Signal-Based Surveillance (AVSS), DOI 10.1109/AVSS.2017.8078516
   Cai Panpan., 2014, SIMULATIONS SERIOUS, P1
   Fantacci C, 2018, IEEE SIGNAL PROC LET, V25, DOI 10.1109/LSP.2018.2811750
   Ferrari V, 2010, INT J COMPUT VISION, V87, P284, DOI 10.1007/s11263-009-0270-9
   Ghiasi G, 2014, PROC CVPR IEEE, P1899, DOI 10.1109/CVPR.2014.306
   Gholamhosseinian A, 2021, IEEE OPEN J INTEL TR, V2, P173, DOI 10.1109/OJITS.2021.3096756
   Heo J, 2022, PATTERN RECOGN LETT, V159, P70, DOI 10.1016/j.patrec.2022.05.006
   Hu K, 2020, MULTIMED TOOLS APPL, V79, P30861, DOI 10.1007/s11042-020-09566-2
   jacarini.dinf.usherbrooke, Changedetection.net
   Jia Q, 2024, VISUAL COMPUT, V40, P5575, DOI 10.1007/s00371-023-03123-2
   Li  X, 2019, IEEE T PATTERN ANAL, V41, P915, DOI 10.1109/TPAMI.2018.2818132
   Li Y, 2022, IEEE T IMAGE PROCESS, V31, P5661, DOI 10.1109/TIP.2022.3197984
   Li YY., 2008, Comput. Vis. Image Underst, V17, P346, DOI [10.1016/j.cviu.2007.09.014, DOI 10.1016/J.CVIU.2007.09.014]
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mao WB, 2014, PROCEEDINGS OF 2014 IEEE INTERNATIONAL CONFERENCE ON PROGRESS IN INFORMATICS AND COMPUTING (PIC), P269, DOI 10.1109/PIC.2014.6972339
   Meshgi K, 2015, IEICE T INF SYST, VE98D, P1260, DOI 10.1587/transinf.2014EDR0002
   Mishra PK., 2020, Int. J. Comput. Vis. Robot, V10, P167, DOI [10.1504/IJCVR.2020.105683, DOI 10.1504/IJCVR.2020.105683]
   Mostafa Tanzim, 2022, 2022 IEEE 13th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON), P0405, DOI 10.1109/IEMCON56893.2022.9946565
   Moutakki Z, 2017, TRANSP TELECOMMUN J, V18, P297, DOI 10.1515/ttj-2017-0027
   NEVATIA R, 1977, ARTIF INTELL, V8, P77, DOI 10.1016/0004-3702(77)90006-6
   Park S, 2015, MATH PROBL ENG, V2015, DOI 10.1155/2015/217568
   Pepik B, 2013, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2013.422
   Pepikj B., 2013, IEEE Conf on Computer Vision and Pattern Recognition CVPR, P3286, DOI DOI 10.1109/CVPR.2013.422
   Qi L, 2019, PROC CVPR IEEE, P3009, DOI 10.1109/CVPR.2019.00313
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Saleh K., 2022, BBBD: Bounding Box Based Detector for Occlusion Detection and Order Recovery, P78, DOI [10.5220/0011146600003209, DOI 10.5220/0011146600003209]
   Shi P, 2021, SCI PROGRAMMING-NETH, V2021, DOI 10.1155/2021/6698160
   Stewart R, 2016, PROC CVPR IEEE, P2325, DOI 10.1109/CVPR.2016.255
   Sulaiman H.A., 2015, ARPN J. Eng. Appl. Sci., V10, P701
   Tian XY, 2021, J ALGORITHMS COMPUT, V15, DOI 10.1177/1748302620973536
   Wang M, 2023, VISUAL COMPUT, V39, P2865, DOI 10.1007/s00371-022-02498-y
   Wang XY, 2009, IEEE I CONF COMP VIS, P32, DOI 10.1109/iccv.2009.5459207
   Wang XL, 2018, PROC CVPR IEEE, P7774, DOI 10.1109/CVPR.2018.00811
   Weng RL, 2016, IEEE T IMAGE PROCESS, V25, P1163, DOI 10.1109/TIP.2016.2515987
   Xie H, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11136025
   Yuan Y, 2020, EURASIP J IMAGE VIDE, V2020, DOI 10.1186/s13640-020-0496-6
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
   Zhou CL, 2018, LECT NOTES COMPUT SC, V11205, P138, DOI 10.1007/978-3-030-01246-5_9
   Zhou CL, 2020, IEEE T CIRC SYST VID, V30, P2067, DOI 10.1109/TCSVT.2019.2909982
   Zou ZX, 2023, P IEEE, V111, P257, DOI 10.1109/JPROC.2023.3238524
NR 44
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 4
PY 2024
DI 10.1007/s00371-024-03374-7
EA APR 2024
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MX3A6
UT WOS:001196884200003
DA 2024-08-05
ER

PT J
AU Zhou, JS
   Liu, YQ
   Du, XQ
AF Zhou, Jiashuang
   Liu, Yongqi
   Du, Xiaoqin
TI HGMVAE: hierarchical disentanglement in Gaussian mixture variational
   autoencoder
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Variational autoencoder; Disentanglement; Representation learning;
   Gaussian mixture distribution
AB Recent advancements in deep neural networks have shown great potential in generating realistic data and performing clustering tasks. This is due to their ability to capture intricate patterns. However, current generative models face challenges such as poor performance and computational complexity caused by the issue of dimension disaster. The variational autoencoder (VAE), a commonly used method, also encounters problems such as posterior collapse and poor performance in multiclass classification when using the latent variables of VAE. Our goal in this study is to tackle the issue of effective disentanglement in image generation, classification and clustering tasks. We develop a generative network based on VAE incorporating a Gaussian mixture distribution as the prior. This enhancement improves the representation of latent variables and helps to overcome the challenges of matching the ground truth posterior. To further improve clustering performance, we introduce the total correlation as a kernel for computing latent features between embedding points and cluster centers. This technique is particularly useful in cases with complex latent variables and can also be applied for hierarchical disentanglement. Moreover, we employ the Fisher discriminant as a regularization term to minimize the within-class distance and maximize the between-class distance for samples, which has an important effect on the performance of our model viewed from the experimental results. We evaluate our proposed network on four datasets, and the experimental results demonstrate its effectiveness across multiple metrics.
C1 [Zhou, Jiashuang; Liu, Yongqi; Du, Xiaoqin] Wuhan Text Univ, Dept Comp Sci & Artificial Intelligence, Wuhan 430200, Hubei, Peoples R China.
C3 Wuhan Textile University
RP Du, XQ (corresponding author), Wuhan Text Univ, Dept Comp Sci & Artificial Intelligence, Wuhan 430200, Hubei, Peoples R China.
EM 2115363027@mail.wtu.edu.cn; 2115363016@mail.wtu.edu.cn;
   xiaoqindu@wtu.edu.cn
CR Abdulaziz A, 2022, INT CONF ACOUST SPEE, P3538, DOI 10.1109/ICASSP43922.2022.9747313
   Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487
   Bai J., 2022, PMLR, P1383
   Bank D., 2020, Autoencoders, DOI DOI 10.48550/ARXIV.2003.05991
   Bengio Y, 2019, Arxiv, DOI arXiv:1709.08568
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Calinski Tadeusz, 1974, Communications in Statistics, V3, P1, DOI [10.1080/03610927408827101, DOI 10.1080/03610927408827101]
   Chen R.T., 2018, P 32 INT C NEURAL IN, V31, P6572
   Collier M, 2019, Arxiv, DOI arXiv:1909.08994
   DAVIES DL, 1979, IEEE T PATTERN ANAL, V1, P224, DOI 10.1109/TPAMI.1979.4766909
   Dhariwal P, 2021, ADV NEUR IN, V34
   Dilokthanakul Nat, 2016, arXiv, DOI DOI 10.48550/ARXIV.1611.02648
   Esmaeili B., 2019, 22 INT C ART INT STA, P2525, DOI DOI 10.48550/ARXIV
   Figueroa J.A., 2019, NEURIPS WORKSHOP BAY
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gretton A, 2005, LECT NOTES ARTIF INT, V3734, P63
   Higgins I., 2017, INT C LEARN REPR
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Hu MF, 2024, VISUAL COMPUT, V40, P1229, DOI 10.1007/s00371-023-02843-9
   Jiang ZX, 2017, Arxiv, DOI [arXiv:1611.05148, DOI 10.48550/ARXIV.1611.05148]
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim H, 2018, PR MACH LEARN RES, V80
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee D.B., 2021, INT C LEARNING REPRE
   Liu Z., Large-scale celebfaces attributes (celeba) dataset
   Michelucci U, 2022, Arxiv, DOI arXiv:2201.03898
   Nalisnick Eric, 2016, NIPS WORKSHOP BAYESI, V2, P131
   Burgess CP, 2018, Arxiv, DOI arXiv:1804.03599
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Radford A, 2016, Arxiv, DOI [arXiv:1511.06434, DOI 10.48550/ARXIV.1511.06434]
   Reynolds D.A., 2009, Encyclopedia of biometrics, V741, P659, DOI [DOI 10.1007/978-0-387-73003-5_196, DOI 10.1007/978-1-4899-7488-4_196]
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   Salimans T., 2017, arXiv
   Sohn K, 2015, ADV NEUR IN, V28
   Tishby N., 2000, arXiv, DOI DOI 10.48550/ARXIV.PHYSICS/0004057
   Vahdat A., 2020, Proceedings of the 34th International Conference on Neural Information Processing Systems. NIPS'20, Vvol 33, P19667, DOI [DOI 10.5555/3495724.3497374, 10.48550/arXiv.2007.03898]
   van den Oord A, 2017, ADV NEUR IN, V30
   van den Oord A, 2016, PR MACH LEARN RES, V48
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang XL, 2016, LECT NOTES COMPUT SC, V9908, P318, DOI 10.1007/978-3-319-46493-0_20
   Xiao H, 2017, Arxiv, DOI [arXiv:1708.07747, DOI 10.48550/ARXIV.1708.07747]
   Yang LX, 2019, IEEE I CONF COMP VIS, P6449, DOI 10.1109/ICCV.2019.00654
   Zhao SJ, 2018, Arxiv, DOI arXiv:1706.02262
NR 44
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 1
PY 2024
DI 10.1007/s00371-024-03338-x
EA APR 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MP7C0
UT WOS:001194882700003
DA 2024-08-05
ER

PT J
AU Huang, Y
   Lu, XC
   Fu, J
AF Huang, Yan
   Lu, Xinchang
   Fu, Jia
TI Single image reflection removal via self-attention and local
   discrimination
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Reflection removal; Self-attention; Local discrimination; Deep learning
ID SEPARATION
AB In practical scenarios, reflections may impair the visual quality of images, bring negative impacts to both human perception and subsequent computer vision tasks. Removing reflections from an image poses considerable challenges due to the diverse nature of reflection content, often blended with background targets. To address the issue, this paper proposes a single-image reflection removal method, which is based on self-attention and local discrimination. By exploiting the mutual relationship between feature maps with a self-attention mechanism, the proposed method can adaptively re-weights each feature according to the input content, benefiting the separation of reflections. For further improvement, a scaled variance map is generated from local statistics and then used in a local discrimination loss to ensure the consistency within local regions. Experimental comparisons have demonstrated the effectiveness and superiority of the proposed method for single image reflection removal. The related code is released at https://github.com/HighColdMan/SIRR_SALD.
C1 [Huang, Yan; Lu, Xinchang] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 511400, Guangdong, Peoples R China.
   [Fu, Jia] South China Univ Technol, Sch Journalism & Commun, Guangzhou, Guangdong, Peoples R China.
C3 South China University of Technology; South China University of
   Technology
RP Fu, J (corresponding author), South China Univ Technol, Sch Journalism & Commun, Guangzhou, Guangdong, Peoples R China.
EM aihuangy@scut.edu.cn; luxinchang@foxmail.com; scutfujia@163.com
FU National Natural Science Foundation of China [61902130]; Science and
   Technology Plan Project of Guangzhou [2023A04J1681]; National Social
   Science Fund of China [20BXW104]
FX This work was supported in part by National Natural Science Foundation
   of China 61902130, Science and Technology Plan Project of Guangzhou
   2023A04J1681, and National Social Science Fund of China (Grand No.
   20BXW104)
CR Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen ZH, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2061
   Dai L, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-23458-5
   Dong Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4997, DOI 10.1109/ICCV48922.2021.00497
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan QN, 2017, IEEE I CONF COMP VIS, P3258, DOI 10.1109/ICCV.2017.351
   Feng X, 2021, IEEE T IMAGE PROCESS, V30, P4867, DOI 10.1109/TIP.2021.3076589
   Guo HB, 2021, IEEE T CYBERNETICS, V51, P2735, DOI 10.1109/TCYB.2019.2934823
   Huang Y, 2020, IEEE T COMPUT IMAG, V6, P34, DOI 10.1109/TCI.2019.2899320
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Jin YX, 2021, IEEE T NEUR NET LEAR, V32, P2330, DOI 10.1109/TNNLS.2020.3004634
   Kim S, 2020, PROC CVPR IEEE, P5163, DOI 10.1109/CVPR42600.2020.00521
   Levin A, 2004, PROC CVPR IEEE, P306
   Levin A., 2002, Adv. Neural lnf. Process. Syst., P1271
   Li C, 2020, PROC CVPR IEEE, P3562, DOI 10.1109/CVPR42600.2020.00362
   Li HX, 2021, IEEE T IMAGE PROCESS, V30, P8526, DOI 10.1109/TIP.2021.3117061
   Li LY, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2190
   Li P, 2022, IEEE T NEUR NET LEAR, V33, P5346, DOI 10.1109/TNNLS.2021.3070463
   Li Y, 2023, APPL INTELL, V53, P19433, DOI 10.1007/s10489-022-04391-6
   Li Y, 2014, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2014.346
   Liang J, 2022, PROC CVPR IEEE, P5647, DOI 10.1109/CVPR52688.2022.00557
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Peng YT, 2022, IEEE SIGNAL PROC LET, V29, P568, DOI 10.1109/LSP.2022.3148668
   Prasad BHP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2370, DOI 10.1109/ICCV48922.2021.00239
   Sheng B, 2020, IEEE T CIRC SYST VID, V30, P955, DOI 10.1109/TCSVT.2019.2901629
   Sheng B, 2020, IEEE T VIS COMPUT GR, V26, P1332, DOI 10.1109/TVCG.2018.2869326
   Shih YC, 2015, PROC CVPR IEEE, P3193, DOI 10.1109/CVPR.2015.7298939
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan RJ, 2020, IEEE T PATTERN ANAL, V42, P2969, DOI 10.1109/TPAMI.2019.2921574
   Wan RJ, 2018, PROC CVPR IEEE, P4777, DOI 10.1109/CVPR.2018.00502
   Wan RJ, 2018, IEEE T IMAGE PROCESS, V27, P2927, DOI 10.1109/TIP.2018.2808768
   Wan RJ, 2017, IEEE I CONF COMP VIS, P3942, DOI 10.1109/ICCV.2017.423
   Wei KX, 2019, PROC CVPR IEEE, P8170, DOI 10.1109/CVPR.2019.00837
   Wen Q, 2019, PROC CVPR IEEE, P3766, DOI 10.1109/CVPR.2019.00389
   Wen Y, 2021, IEEE T IMAGE PROCESS, V30, P6142, DOI 10.1109/TIP.2021.3092814
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xue FY, 2023, VISUAL COMPUT, V39, P3467, DOI 10.1007/s00371-023-03017-3
   Yan Q, 2013, IEEE INT SYMP CIRC S, P937, DOI 10.1109/ISCAS.2013.6572002
   Yang J, 2018, LECT NOTES COMPUT SC, V11207, P675, DOI 10.1007/978-3-030-01219-9_40
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang Q, 2023, VISUAL COMPUT, V39, P4593, DOI 10.1007/s00371-022-02611-1
   Zhang X, 2018, PROC CVPR IEEE, P4786, DOI 10.1109/CVPR.2018.00503
   Zhao H, 2024, VISUAL COMPUT, V40, P1697, DOI 10.1007/s00371-023-02880-4
   Zheng Q, 2021, PROC CVPR IEEE, P13390, DOI 10.1109/CVPR46437.2021.01319
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
NR 46
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 30
PY 2024
DI 10.1007/s00371-024-03333-2
EA MAR 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MO9V7
UT WOS:001194690600002
DA 2024-08-05
ER

PT J
AU Li, SK
   Lyu, C
   Xia, B
   Chen, ZH
   Zhang, L
AF Li, Shaokang
   Lyu, Chengzhi
   Xia, Bin
   Chen, Ziheng
   Zhang, Lei
TI TAMDepth: self-supervised monocular depth estimation with transformer
   and adapter modulation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Self-supervised learning; Monocular depth estimation; Cross-attention;
   Transformer
AB Self-supervised monocular depth estimation presents a promising result, which utilizes image sequences instead of challenging-to-source ground truth for training. The framework of most current studies on self-supervised depth estimation is based on fully convolutional or transformer architectures, and there is little discussion on the hybrid architecture. In this paper, we proposed TAMDepth, a novel framework that can effectively capture the local and global features of image sequences by combining convolutional blocks and transformer blocks. TAMDepth adopts multi-scale feature fusion convolutional modules capture local details in shallow layers while transformer blocks build the global dependency in higher layers. Furthermore, to enhance the representation of architecture, we introduce an adapter modulation that injects the spatial prior to the transformer blocks through cross-attention, which improves the ability of modeling the scene. Experiments demonstrate that our model exhibits state-of-the-art performance on the KITTI dataset and also shows strong generalization performance on the Make3D dataset. Source code is available at https://github.com/deansaice/TAMDepth.
C1 [Li, Shaokang; Lyu, Chengzhi; Xia, Bin; Chen, Ziheng; Zhang, Lei] Hubei Univ Automot Technol, Inst Automot Engineers, 167 Checheng West Rd, Shiyan 442000, Hubei, Peoples R China.
C3 Hubei University of Automotive Technology
RP Lyu, C (corresponding author), Hubei Univ Automot Technol, Inst Automot Engineers, 167 Checheng West Rd, Shiyan 442000, Hubei, Peoples R China.
EM lsk.512@foxmail.com; sanchilongquan@outlook.com
FU Foundation of Hubei Educational Committee
FX No Statement Available
CR Andraghetti L, 2019, INT CONF 3D VISION, P424, DOI 10.1109/3DV.2019.00054
   Bae J., 2023, P AAAI C ART INT WAS, P187
   Casser V, 2019, IEEE COMPUT SOC CONF, P381, DOI 10.1109/CVPRW.2019.00051
   Chang Shu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P572, DOI 10.1007/978-3-030-58529-7_34
   Chen Z., 2022, 11 INT C LEARN REPR
   Choi H., 2021, P IEEE CVF INT C COM, P12808
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Eigen D, 2014, ADV NEUR IN, V27
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Guizilini V, 2020, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR42600.2020.00256
   Guo JY, 2022, PROC CVPR IEEE, P12165, DOI 10.1109/CVPR52688.2022.01186
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jung H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12622, DOI 10.1109/ICCV48922.2021.01241
   Kline J, 2020, PROCEEDINGS OF THE 2020 32ND INTERNATIONAL TELETRAFFIC CONGRESS (ITC 32), P1, DOI [10.1109/ITC3249928.2020.00009, 10.1007/978-3-030-58565-5_35]
   Koestler Lukas, 2022, P 5 C ROB LEARN, P34
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Li B, 2015, PROC CVPR IEEE, P1119, DOI 10.1109/CVPR.2015.7298715
   Li YZ, 2021, VISUAL COMPUT, V37, P2567, DOI 10.1007/s00371-021-02206-2
   Li ZY, 2022, Arxiv, DOI arXiv:2203.14211
   Lin W., 2023, P IEEE CVF INT C COM, P6015
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo X, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392377
   Lyu XY, 2021, AAAI CONF ARTIF INTE, V35, P2294
   Masoumian A, 2023, NEUROCOMPUTING, V517, P81, DOI 10.1016/j.neucom.2022.10.073
   Mendes RD, 2021, ROBOT AUTON SYST, V136, DOI 10.1016/j.robot.2020.103701
   Park Namuk, 2021, INT C LEARN REPR
   Poggi M, 2018, IEEE INT C INT ROBOT, P5848, DOI 10.1109/IROS.2018.8593814
   Raghu M, 2021, ADV NEUR IN, V34
   Ranjan A, 2019, PROC CVPR IEEE, P12232, DOI 10.1109/CVPR.2019.01252
   Ren SC, 2022, PROC CVPR IEEE, P10843, DOI 10.1109/CVPR52688.2022.01058
   Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Shim D, 2023, IEEE INT CONF ROBOT, P4983, DOI 10.1109/ICRA48891.2023.10160657
   Si C., 2022, Adv. Neural Inf. Process. Syst, V35, P23495
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun QY, 2022, IEEE T NEUR NET LEAR, V33, P2023, DOI 10.1109/TNNLS.2021.3100895
   Uhrig J, 2017, INT CONF 3D VISION, P11, DOI 10.1109/3DV.2017.00012
   Wang RY, 2023, PROC CVPR IEEE, P21425, DOI 10.1109/CVPR52729.2023.02052
   Wang Y, 2019, PROC CVPR IEEE, P8437, DOI 10.1109/CVPR.2019.00864
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Yan JX, 2021, INT CONF 3D VISION, P464, DOI 10.1109/3DV53792.2021.00056
   Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212
   Zhang MJ, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3263848
   Zhang MJ, 2022, PROC CVPR IEEE, P867, DOI 10.1109/CVPR52688.2022.00095
   Zhang N, 2023, PROC CVPR IEEE, P18537, DOI 10.1109/CVPR52729.2023.01778
   Zhou Hang, 2021, BRIT MACH VIS C BMVC
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
   Zhou ZK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12757, DOI 10.1109/ICCV48922.2021.01254
   Zhu X., 2020, INT C LEARN REPR
NR 54
TC 0
Z9 0
U1 10
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 27
PY 2024
DI 10.1007/s00371-024-03332-3
EA MAR 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MO8T2
UT WOS:001194660900001
DA 2024-08-05
ER

PT J
AU Zhang, HR
   Qi, YF
   Chen, HL
   Cao, PP
   Liang, AY
   Wen, SC
AF Zhang, Hengrui
   Qi, Yongfeng
   Chen, Huili
   Cao, Panpan
   Liang, Anye
   Wen, Shengcong
TI LSDNet: lightweight stochastic depth network for human pose estimation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Human pose estimation; Bernoulli distribution; Lightweight network;
   Keypoints detection
AB Human pose estimation plays a critical role in human-centred vision applications. Its influence extends to various aspects of daily life, from healthcare diagnostics and sports training to augmented reality experiences and gesture-controlled interfaces. While current approaches have achieved impressive accuracy, their high model complexity and slow detection speeds significantly limit their deployment on edge devices with limited computing power, such as mobile phones and IoT devices. In this paper, we introduce a novel lightweight network for 2D human pose estimation, called lightweight stochastic depth network (LSDNet). Our approach is based on the observation that the majority of HRNet's parameters are located in the middle and later stages in the network. We reduce some unnecessary branches to significantly reduce these parameters. This is achieved by leveraging the Bernoulli distribution to randomly remove these redundant branches, which improves the network's efficiency while also increasing its robustness. To further reduce the network's parameter count, we introduce two lightweight blocks with simple yet effective architectures. These blocks achieve significant parameter reduction while maintaining good accuracy. Furthermore, we leverage coordinate attention to effectively fuse features from different branches and scales. This mechanism captures both inter-channel dependencies and spatial context, enabling the network to accurately localize keypoints across the human body. We evaluated the effectiveness of our method on the MPII and COCO datasets, demonstrating superior results on human pose estimation compared to popular lightweight networks. Our code is available at: https://github.com/illusory2333/LSDNet.
C1 [Zhang, Hengrui; Qi, Yongfeng; Chen, Huili; Cao, Panpan; Liang, Anye; Wen, Shengcong] Northwest Normal Univ, Lanzhou, Peoples R China.
C3 Northwest Normal University - China
RP Zhang, HR (corresponding author), Northwest Normal Univ, Lanzhou, Peoples R China.
EM 13693477505a@gmail.com
OI Zhang, Hengrui/0009-0001-3154-3513
FU the National Natural Science Foundation of China [62267007]; National
   Natural Science Foundation of China [2022CYZC-16]; Gansu Provincial
   Department of Education Higher Education Industry Support Plan Project
FX The National Natural Science Foundation of China: 62267007, Gansu
   Provincial Department of Education Higher Education Industry Support
   Plan Project: 2022CYZC-16.
CR Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Bahdanau D, 2016, Arxiv, DOI arXiv:1409.0473
   Cai Y, 2020, EUROPEAN C COMPUTER, P455, DOI DOI 10.1007/978-3-030-58580-8_27
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Cheng BW, 2020, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR42600.2020.00543
   Chu X., 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, P1831, DOI DOI 10.1109/CVPR.2017.601
   Fan J., 2021, 2021 IEEE INT C AUTO, P1, DOI [10.1109/ICAS49788.2021.9551165.IEEE, DOI 10.1109/ICAS49788.2021.9551165.IEEE]
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Geng ZG, 2023, PROC CVPR IEEE, P660, DOI 10.1109/CVPR52729.2023.00071
   Geng ZG, 2021, PROC CVPR IEEE, P14671, DOI 10.1109/CVPR46437.2021.01444
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu J, 2018, ADV NEUR IN, V31
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39
   Jin Sheng, 2020, EUR C COMP VIS, P718, DOI DOI 10.1007/978-3-030-58571-642
   Krizhevsky Alex, 2010, Convolutional deep belief networks on cifar-10
   Li SB, 2023, IMAGE VISION COMPUT, V137, DOI 10.1016/j.imavis.2023.104740
   Li WB, 2019, Arxiv, DOI arXiv:1901.00148
   Lin T.-Y., 2017, PROC CVPR IEEE, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Huajun, 2021, arXiv
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   McNally W, 2021, IEEE ACCESS, V9, P139403, DOI 10.1109/ACCESS.2021.3118207
   Iandola FN, 2016, Arxiv, DOI [arXiv:1602.07360, 10.48550/arXiv.1602.07360]
   Neff C, 2020, Arxiv, DOI arXiv:2007.08090
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Ohashi T, 2020, IMAGE VISION COMPUT, V104, DOI 10.1016/j.imavis.2020.104028
   Osokin D, 2018, Arxiv, DOI arXiv:1811.12004
   Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395
   Ramachandran P, 2017, Arxiv, DOI arXiv:1710.05941
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sifre L., 2014, arXiv, DOI [10.48550/arXiv.1403.1687, DOI 10.48550/ARXIV.1403.1687]
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33
   Tan MX, 2019, Arxiv, DOI [arXiv:1907.09595, DOI 10.48550/ARXIV.1907.09595]
   Tan MX, 2019, PR MACH LEARN RES, V97
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang YH, 2022, PROC CVPR IEEE, P13116, DOI 10.1109/CVPR52688.2022.01278
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Xu YL, 2022, IEEE T PATTERN ANAL, V44, P6327, DOI 10.1109/TPAMI.2021.3087695
   Xu YF, 2024, IEEE T PATTERN ANAL, V46, P1212, DOI 10.1109/TPAMI.2023.3330016
   Yang S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11782, DOI 10.1109/ICCV48922.2021.01159
   Yang W, 2017, IEEE I CONF COMP VIS, P1290, DOI 10.1109/ICCV.2017.144
   Yuan Y., 2021, P C NEUR INF PROC SY, P7281
   Zhang F, 2020, PROC CVPR IEEE, P7091, DOI 10.1109/CVPR42600.2020.00712
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhou TF, 2023, IEEE T PATTERN ANAL, V45, P8296, DOI 10.1109/TPAMI.2023.3239194
NR 54
TC 0
Z9 0
U1 12
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 16
PY 2024
DI 10.1007/s00371-024-03323-4
EA MAR 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LC6A7
UT WOS:001184607700002
DA 2024-08-05
ER

PT J
AU Zhao, C
   Cai, WL
   Yuan, Z
AF Zhao, Chen
   Cai, Wei-Ling
   Yuan, Zheng
TI Spectral normalization and dual contrastive regularization for
   image-to-image translation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image-to-image translation; Contrastive learning; Generative adversarial
   network
AB Existing image-to-image (I2I) translation methods achieve state-of-the-art performance by incorporating the patch-wise contrastive learning into generative adversarial networks. However, patch-wise contrastive learning only focuses on the local content similarity but neglects the global structure constraint, which affects the quality of the generated images. In this paper, we propose a new unpaired I2I translation framework based on dual contrastive regularization and spectral normalization, namely SN-DCR. To maintain consistency of the global structure and texture, we design the dual contrastive regularization using different deep feature spaces respectively. In order to improve the global structure information of the generated images, we formulate a semantic contrastive loss to make the global semantic structure of the generated images similar to the real images from the target domain in the semantic feature space. We use gram matrices to extract the style of texture from images. Similarly, we design a style contrastive loss to improve the global texture information of the generated images. Moreover, to enhance the stability of the model, we employ the spectral normalized convolutional network in the design of our generator. We conduct comprehensive experiments to evaluate the effectiveness of SN-DCR, and the results prove that our method achieves SOTA in multiple tasks. The code and pretrained models are available at https://github.com/zhihefang/SN-DCR.
C1 [Zhao, Chen; Cai, Wei-Ling; Yuan, Zheng] Nanjing Normal Univ, Dept Comp Sci & Technol, Nanjing, Peoples R China.
C3 Nanjing Normal University
RP Cai, WL (corresponding author), Nanjing Normal Univ, Dept Comp Sci & Technol, Nanjing, Peoples R China.
EM 2518628273@qq.com; caiwl@njnu.edu.cn; l1455341238@qq.com
FU National Natural Science Foundation of China [62276138, 61876087];
   National Natural Science Foundation of China
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant Nos. 62276138 and 61876087.
CR Benaim S, 2017, ADV NEURAL INFORM PR, P752
   Caron M., 2020, Neural Inf. Process. Syst.
   Chang Y., 2023, IEEE Transactions on Pattern Analysis and Machine Intelligence
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549
   Fu H, 2019, PROC CVPR IEEE, P2422, DOI [10.1109/CVPR.2019.00253, 10.1109/cvpr.2019.00253]
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gou Y, 2023, COMPLEX INTELL SYST, V9, P4111, DOI 10.1007/s40747-022-00924-1
   Han JL, 2021, IEEE COMPUT SOC CONF, P746, DOI 10.1109/CVPRW53098.2021.00084
   He H., 2020, P IEEE CVF C COMP VI, P9729
   Hensel M, 2017, ADV NEUR IN, V30
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu XQ, 2022, PROC CVPR IEEE, P18270, DOI 10.1109/CVPR52688.2022.01775
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Mirza M., 2014, ARXIV
   Miyato T, 2018, CoRR
   Park T., 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-719, 10.1007/978-3-030-58545-719, DOI 10.1007/978-3-030-58545-7_19]
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Phaphuangwittayakul A, 2023, VISUAL COMPUT, V39, P4015, DOI 10.1007/s00371-022-02566-3
   Qin ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P763, DOI 10.1109/ICCV48922.2021.00082
   Rabin J, 2012, LECT NOTES COMPUT SC, V6667, P435, DOI 10.1007/978-3-642-24785-9_37
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Son J., 2017, CORR
   Song S, 2023, AAAI CONF ARTIF INTE, P2292
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Torbunov D, 2023, IEEE WINT CONF APPL, P702, DOI 10.1109/WACV56688.2023.00077
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang WL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14000, DOI 10.1109/ICCV48922.2021.01376
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu G, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3290038
   Wu HY, 2021, PROC CVPR IEEE, P10546, DOI 10.1109/CVPR46437.2021.01041
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Yu F, 2017, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2017.75
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhang DW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P294, DOI 10.1145/3394171.3413743
   Zhang YQ, 2023, NEURAL NETWORKS, V164, P146, DOI 10.1016/j.neunet.2023.04.037
   Zhao C., 2023, CORR
   Zheng CX, 2021, PROC CVPR IEEE, P16402, DOI 10.1109/CVPR46437.2021.01614
   Zhu D., 2023, VISUAL COMPUT
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 43
TC 2
Z9 2
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 13
PY 2024
DI 10.1007/s00371-024-03314-5
EA MAR 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KT9R8
UT WOS:001182339200001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Raffo, A
   Ranieri, A
   Romanengo, C
   Falcidieno, B
   Biasotti, S
AF Raffo, Andrea
   Ranieri, Andrea
   Romanengo, Chiara
   Falcidieno, Bianca
   Biasotti, Silvia
TI CurveML: a benchmark for evaluating and training learning-based methods
   of classification, recognition, and fitting of plane curves
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Curves; Bezier; Dataset; Machine learning; Classification; Regression;
   Fitting
AB We propose CurveML, a benchmark for evaluating and comparing methods for the classification and identification of plane curves represented as point sets. The dataset is composed of 520k curves, of which 280k are generated from specific families characterised by distinctive shapes, and 240k are obtained from Bezier or composite Bezier curves. The dataset was generated starting from the parametric equations of the selected curves making it easily extensible. It is split into training, validation, and test sets to make it usable by learning-based methods, and it contains curves perturbed with different kinds of point set artefacts. To evaluate the detection of curves in point sets, our benchmark includes various metrics with particular care on what concerns the classification and approximation accuracy. Finally, we provide a comprehensive set of accompanying demonstrations, showcasing curve classification, and parameter regression tasks using both ResNet-based and PointNet-based networks. These demonstrations encompass 14 experiments, with each network type comprising 7 runs: 1 for classification and 6 for regression of the 6 defining parameters of plane curves. The corresponding Jupyter notebooks with training procedures, evaluations, and pre-trained models are also included for a thorough understanding of the methodologies employed.
C1 [Raffo, Andrea] Univ Oslo, Dept Biosci, Postboks 0316, N-1066 Oslo, Norway.
   [Ranieri, Andrea; Romanengo, Chiara; Falcidieno, Bianca; Biasotti, Silvia] CNR, Ist Matemat Applicata & Tecnol Informat E Magenes, via Marini 16, I-16149 Genoa, Italy.
C3 University of Oslo; Consiglio Nazionale delle Ricerche (CNR); Istituto
   di Matematica Applicata e Tecnologie Informatiche "Enrico Magenes"
   (IMATI-CNR)
RP Romanengo, C (corresponding author), CNR, Ist Matemat Applicata & Tecnol Informat E Magenes, via Marini 16, I-16149 Genoa, Italy.
EM chiara.romanengo@ge.imati.cnr.it
RI Biasotti, Silvia/G-8602-2012
OI Biasotti, Silvia/0000-0002-9992-825X; Romanengo,
   Chiara/0000-0002-9459-6209
FU Consiglio Nazionale Delle Ricerche (CNR); European Union; Ministry of
   University and Research (MUR) [ECS00000035, DIT.AD004.100,
   DIT.AD021.080.001]; National Recovery and Resilience Plan (NRRP),
   Mission 4, Component 2, Investment 1.5; Smart Infrastructure"
FX The present study has been funded by the European Union -
   NextGenerationEU and by the Ministry of University and Research (MUR),
   National Recovery and Resilience Plan (NRRP), Mission 4, Component 2,
   Investment 1.5, project "RAISE - Robotics and AI for Socio-economic
   Empowerment" (ECS00000035). This work has been partially developed in
   the CNR-IMATI research activities DIT.AD004.100 and DIT.AD021.080.001.
   This study has also been carried out in the framework of the activities
   of NRRP CN MOST-Sustainable Mobility Center, Spoke 7-"Connected Networks
   and Smart Infrastructure", whose financial support is gratefully
   acknowledged.
CR [Anonymous], 2001, TRECVid-TREC Video Retrieval Evaluation
   [Anonymous], 2017, Four Shapes
   [Anonymous], 2023, SHREC-SHape REtrieval Contest
   [Anonymous], 2005, MIREX-Music Information Retrieval Evaluation eXchange
   Caputo A, 2020, COMPUT GRAPH-UK, V91, P232, DOI 10.1016/j.cag.2020.07.014
   Dennis Lawrence J, 2013, A catalog of special plane curves
   Deza MM., 2009, ENCY DISTANCES, P1, DOI [DOI 10.1007/978-3-642-00234-2, 10.1007/978-3-642-00234-2]
   El Korchi A, 2020, DATA BRIEF, V32, DOI 10.1016/j.dib.2020.106090
   Farin G., 2002, Curves and surfaces for CAGD: a practical guide, Vfifth
   Gagliardi L, 2022, COMPUT GRAPH-UK, V107, P20, DOI 10.1016/j.cag.2022.07.005
   He K., 2016, European conference on computer vision, P770, DOI DOI 10.1007/978-3-319-46493-0_38
   Howard J, 2020, INFORMATION, V11, DOI 10.3390/info11020108
   Juneja A, 2023, VISUAL COMPUT, V39, P3905, DOI 10.1007/s00371-022-02534-x
   Koch S, 2019, PROC CVPR IEEE, P9593, DOI 10.1109/CVPR.2019.00983
   Lockwood EH., 1961, A book of curves
   Max Kuhn K.J., 2018, Applied Predictive Modeling
   Or B., 2021, SMART TOOLS APPS GRA, DOI [10.2312/stag.20211472, DOI 10.2312/STAG.20211472]
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Raffo A, 2023, COMPUT GRAPH-UK, V115, P285, DOI 10.1016/j.cag.2023.06.023
   Raffo A, 2021, COMPUT GRAPH-UK, V99, P1, DOI 10.1016/j.cag.2021.06.010
   Rivaldo M.G., 2022, FlatShapeNet
   Romanengo C, 2022, COMPUT GRAPH-UK, V107, P32, DOI 10.1016/j.cag.2022.07.004
   Romanengo C, 2022, COMPUT GRAPH-UK, V102, P133, DOI 10.1016/j.cag.2021.09.013
   Romanengo C, 2020, PATTERN RECOGN LETT, V131, P405, DOI 10.1016/j.patrec.2020.01.025
   Shikin E.V., 1995, Handbook and Atlas of Curves
   Sipiran I, 2014, VISUAL COMPUT, V30, P1293, DOI 10.1007/s00371-014-0937-2
   Thompson EM, 2022, COMPUT GRAPH-UK, V107, P161, DOI 10.1016/j.cag.2022.07.018
   Yan X., 2019, Pointnet/Pointnet++ Pytorch
   Zhao XY, 2024, VISUAL COMPUT, V40, P5187, DOI 10.1007/s00371-023-03097-1
NR 29
TC 1
Z9 1
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 12
PY 2024
DI 10.1007/s00371-024-03292-8
EA MAR 2024
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KP8S9
UT WOS:001181269000001
OA hybrid
DA 2024-08-05
ER

PT J
AU Xiao, S
   Liu, ZG
   Gao, J
   Wang, CX
AF Xiao, Song
   Liu, ZhiGuo
   Gao, Jian
   Wang, ChangXin
TI A high-fidelity face swapping algorithm based on mutual
   information-guided feature decoupling
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Face swapping; Mutual information; Feature decoupling; Detail repair
ID IMAGE INPAINTING ALGORITHM; DIFFUSION
AB A large number of high-quality face swapping images are often required to improve the performance of forgery detection. High-quality face swapping images require fewer face swapping traces and fewer face swapping artificialities in the image while ensuring the same identity consistency with the source face and the same attribute consistency with the target. It is a challenging task today to properly separate identity and non-identity related attribute information. In this paper, we propose a novel framework that called high-fidelity face swapping algorithm (HFSA) which consists of two part networks, a GAN-based mutual information swapping network, MuIn-swap, for face swapping and an MAE-based Detail Repair Net, DRN, for detail repair. We introduce mutual information into feature compression, explicitly computing the mutual information of identity and attribute information obtained by compression of latent features. Therefore, the learning of the network is explicitly guided by formulating the minimum mutual information as the optimization goal, so that we can obtain pure identity and attribute information. In addition to overcome the problem of information loss during face swapping, we additionally design an DRN to repair the details of the face swapping to achieve more realistic and fidelity face swapping images. Through extensive experiments, it has been demonstrated that the forgery samples generated by HFSA guarantee smaller forgery traces and fewer artifacts while guaranteeing identity consistency with the source and attribute preservation. The code is already available on GitHub: https://github.com/karasuma123/HFSA.
C1 [Xiao, Song] Beijing Elect Sci & Technol Inst, Dept Elect & Commun Engn, Beijing 100070, Peoples R China.
   [Xiao, Song; Liu, ZhiGuo; Gao, Jian; Wang, ChangXin] Xidian Univ, Sch Telecommun Engn, Xian 710071, Peoples R China.
C3 Beijing Electronic Science & Technology Institute; Xidian University
RP Xiao, S (corresponding author), Beijing Elect Sci & Technol Inst, Dept Elect & Commun Engn, Beijing 100070, Peoples R China.; Xiao, S (corresponding author), Xidian Univ, Sch Telecommun Engn, Xian 710071, Peoples R China.
EM xiaosong@mail.xidian.edu.cn
RI Wang, Changxin/AAX-4575-2020; Liu, Zhiguo/ISA-5129-2023
FU The National Natural Science Foundation of China [62101414, 62201423];
   National Natural Science Foundation of China [4232034]; Beijing
   Municipal Natural Science Foundation [2022T150508]; China Postdoctoral
   Science Foundation [095920221320]; Young Talent Fund Xi'an Association
   for Science and Technology [2020A1515110856]; Guangdong Basic and
   Applied Basic Research Foundation
FX Foundation Item: The National Natural Science Foundation of China
   (62101414, 62201423), Beijing Municipal Natural Science Foundation
   (4232034), The China Postdoctoral Science Foundation (2021M702546,
   2021M702548), The China Postdoctoral Science Foundation (2022T150508),
   The Young Talent Fund Xi'an Association for Science and
   Technology(095920221320) and The Guangdong Basic and Applied Basic
   Research Foundation (2020A1515110856).
CR Achille A, 2018, Arxiv, DOI arXiv:1706.01350
   Bennasar M, 2015, EXPERT SYST APPL, V42, P8520, DOI 10.1016/j.eswa.2015.07.007
   Bitouk D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360638
   Chen RW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2003, DOI 10.1145/3394171.3413630
   Chen ZH, 2021, VISUAL COMPUT, V37, P2657, DOI 10.1007/s00371-021-02199-y
   DeepFakes, 2020, faceswap
   Hjelm RD, 2019, Arxiv, DOI arXiv:1808.06670
   Doquire G, 2013, NEUROCOMPUTING, V122, P148, DOI 10.1016/j.neucom.2013.06.035
   Gao GG, 2021, PROC CVPR IEEE, P3403, DOI 10.1109/CVPR46437.2021.00341
   Guo Q, 2018, IEEE T VIS COMPUT GR, V24, P2023, DOI 10.1109/TVCG.2017.2702738
   Holden M., 2005, PROC APRS WORKSHOP D
   Hoque N, 2014, EXPERT SYST APPL, V41, P6371, DOI 10.1016/j.eswa.2014.04.019
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jiang LM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13899, DOI 10.1109/ICCV48922.2021.01366
   Jianzhu Guo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P152, DOI 10.1007/978-3-030-58529-7_10
   Jin X, 2018, IEEE ACCESS, V6, P49967, DOI 10.1109/ACCESS.2018.2866089
   Jo Y, 2019, IEEE I CONF COMP VIS, P1745, DOI 10.1109/ICCV.2019.00183
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Kawai N, 2016, IEEE T VIS COMPUT GR, V22, P1236, DOI 10.1109/TVCG.2015.2462368
   Korshunova I., 2016, IEEE INT C COMPUTER, P3677
   Li HD, 2017, IEEE T INF FOREN SEC, V12, P3050, DOI 10.1109/TIFS.2017.2730822
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Li KS, 2016, SOFT COMPUT, V20, P885, DOI 10.1007/s00500-014-1547-7
   Li LZ, 2020, PROC CVPR IEEE, P5073, DOI 10.1109/CVPR42600.2020.00512
   Li Q., 2022, arXiv
   Li X., 2020, Computer Vision-ECCV 2020, P12354
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu DC, 2024, IEEE T MULTIMEDIA, V26, P2894, DOI 10.1109/TMM.2023.3304913
   Liu DC, 2023, IEEE T INF FOREN SEC, V18, P4272, DOI 10.1109/TIFS.2023.3293951
   Liu DC, 2022, IEEE T NEUR NET LEAR, V33, P5611, DOI 10.1109/TNNLS.2021.3071119
   Liu DC, 2018, NEUROCOMPUTING, V302, P46, DOI 10.1016/j.neucom.2018.03.042
   Liu HY, 2019, IEEE I CONF COMP VIS, P4169, DOI 10.1109/ICCV.2019.00427
   Liu JL, 2019, IEEE ACCESS, V7, P114021, DOI 10.1109/ACCESS.2019.2933910
   Luo Y, 2022, Arxiv, DOI [arXiv:2203.16983, 10.48550/arXiv.2203.16983]
   Masse NY, 2012, FRONT NEUROINFORM, V6, DOI 10.3389/fninf.2012.00021
   Mechrez R, 2018, LECT NOTES COMPUT SC, V11218, P800, DOI 10.1007/978-3-030-01264-9_47
   Mo JC, 2019, CLUSTER COMPUT, V22, pS7593, DOI 10.1007/s10586-018-2323-8
   Natsume R, 2018, Arxiv, DOI arXiv:1804.03447
   Natsume R, 2019, LECT NOTES COMPUT SC, V11366, P117, DOI 10.1007/978-3-030-20876-9_8
   Nazeri K., 2019, arXiv
   Nirkin Y, 2018, IEEE INT CONF AUTOMA, P98, DOI 10.1109/FG.2018.00024
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Plattard D, 2000, Comput Aided Surg, V5, P246, DOI 10.1002/1097-0150(2000)5:4<246::AID-IGS4>3.0.CO;2-Z
   Rivaz H, 2014, MED IMAGE ANAL, V18, P343, DOI 10.1016/j.media.2013.12.003
   Rössler A, 2019, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2019.00009
   Ruzic T, 2015, IEEE T IMAGE PROCESS, V24, P444, DOI 10.1109/TIP.2014.2372479
   Sheng B, 2022, IEEE T CYBERNETICS, V52, P6662, DOI 10.1109/TCYB.2021.3079311
   Shwartz-Ziv R, 2017, Arxiv, DOI arXiv:1703.00810
   Smilkov D, 2017, Arxiv, DOI arXiv:1706.03825
   Sridevi G, 2019, CIRC SYST SIGNAL PR, V38, P3802, DOI 10.1007/s00034-019-01029-w
   Tishby N, 2015, 2015 IEEE INFORMATION THEORY WORKSHOP (ITW)
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang HX, 2008, INT CONF ACOUST SPEE, P893
   Wang JL, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13091609
   Wang K, 2022, Arxiv, DOI arXiv:2205.11090
   Wang M, 2021, NEUROCOMPUTING, V429, P215, DOI 10.1016/j.neucom.2020.10.081
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   [徐峻岭 Xu Junling], 2012, [计算机研究与发展, Journal of Computer Research and Development], V49, P372
   Xu ZL, 2022, LECT NOTES COMPUT SC, V13674, P661, DOI 10.1007/978-3-031-19781-9_38
   Xu ZL, 2021, AAAI CONF ARTIF INTE, V35, P3083
   Yang C, 2017, PROC CVPR IEEE, P4076, DOI 10.1109/CVPR.2017.434
   Yang T, 2021, PROC CVPR IEEE, P672, DOI 10.1109/CVPR46437.2021.00073
   Yao F, 2019, CLUSTER COMPUT, V22, P13683, DOI 10.1007/s10586-018-2068-4
   Zhang HR, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1939, DOI 10.1145/3240508.3240625
   Zheng CX, 2019, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2019.00153
   Zhu YH, 2021, PROC CVPR IEEE, P4832, DOI 10.1109/CVPR46437.2021.00480
NR 68
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 29
PY 2024
DI 10.1007/s00371-024-03288-4
EA FEB 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JF9E8
UT WOS:001171862700003
DA 2024-08-05
ER

PT J
AU Li, CZ
   He, KJ
   Xu, D
   Luo, YY
   Zhou, YQ
AF Li, Chengzhou
   He, Kangjian
   Xu, Dan
   Luo, Yueying
   Zhou, Yiqiao
TI MVSFusion: infrared and visible image fusion method for multiple visual
   scenarios
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image fusion; Multiple visual scenarios; Image classification; Saliency
   analysis; Detail preserving
ID INFORMATION; TRANSFORMATION; PERFORMANCE
AB The purpose of infrared and visible fusion is to encompass significant targets and abundant texture details in multiple visual scenarios. However, existing fusion methods have not effectively addressed multiple visual scenarios including small objects, multiple objects, noise, low light, light pollution, overexposure and so on. To better adapt to multiple visual scenarios, we propose a general infrared and visible image fusion method based on saliency weight, termed as MVSFusion. Initially, we use SVM (Support Vector Machine) to classify visible images into two categories based on lighting conditions: Low-Light visible images and Brightly Lit visible images. Designing fusion rules according to distinct lighting conditions ensures adaptability to multiple visual scenarios. Our designed saliency weights guarantee saliency for both small and multiple objects across different scenes. On the other hand, we propose a new texture detail fusion method and an adaptive brightness enhancement technique to better address multiple visual scenarios such as noise, light pollution, nighttime, and overexposure. Extensive experiments indicate that MVSFusion excels not only in visual quality and quantitative evaluation compared to state-of-the-art algorithms but also provides advantageous support for high-level visual tasks. Our code is publicly available at: https://github.com/VCMHE/MVSFusion.
C1 [Li, Chengzhou; He, Kangjian; Xu, Dan; Luo, Yueying; Zhou, Yiqiao] Yunnan Univ, Sch Informat Sci & Engn, Kunming 650091, Peoples R China.
C3 Yunnan University
RP He, KJ (corresponding author), Yunnan Univ, Sch Informat Sci & Engn, Kunming 650091, Peoples R China.
EM hekj@ynu.edu.cn
RI Xu, Dan/KPA-7396-2024; He, Kangjian/CAG-0300-2022
OI Xu, Dan/0000-0003-4602-3550; 
FU National Natural Science Foundation of China; Yunnan provincial major
   science and technology special plan projects [202202AD080003]; Yunnan
   Province Ten Thousand Talents Program and Yunling Scholars Special
   Project [YNWR-YLXZ-2018-022]; Yunnan Provincial Science and Technology
   Department-Yunnan University [202301BF070001-025]; Research Foundation
   of Yunnan Province [202105AF150011];  [62202416];  [62162068]; 
   [62162065]
FX This work was supported in part the Yunnan provincial major science and
   technology special plan projects under Grant 202202AD080003, in part by
   the National Natural Science Foundation of China under Grant 62202416,
   62162068, Grant 62162065, in part by the Yunnan Province Ten Thousand
   Talents Program and Yunling Scholars Special Project under Grant
   YNWR-YLXZ-2018-022, in part by the Yunnan Provincial Science and
   Technology Department-Yunnan University "Double First Class"
   Construction Joint Fund Project under Grant No. 202301BF070001-025, and
   in part by the Research Foundation of Yunnan Province No.
   202105AF150011.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Aghamaleki JA, 2023, VISUAL COMPUT, V39, P1181, DOI 10.1007/s00371-021-02396-9
   Chen J, 2020, INFORM SCIENCES, V508, P64, DOI 10.1016/j.ins.2019.08.066
   Ding ZS, 2023, DIGIT SIGNAL PROCESS, V133, DOI 10.1016/j.dsp.2022.103875
   Ding ZS, 2023, APPL INTELL, V53, P8114, DOI 10.1007/s10489-022-03952-z
   Guo CX, 2023, EXPERT SYST APPL, V211, DOI 10.1016/j.eswa.2022.118631
   Guo HB, 2021, IEEE T CYBERNETICS, V51, P2735, DOI 10.1109/TCYB.2019.2934823
   Ha Q, 2017, IEEE INT C INT ROBOT, P5108, DOI 10.1109/IROS.2017.8206396
   Han MA, 2023, INFORM FUSION, V92, P268, DOI 10.1016/j.inffus.2022.12.005
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KJ, 2023, IEEE T MULTIMEDIA, V25, P4943, DOI 10.1109/TMM.2022.3185887
   Hou R., 2022, 2022 IEEE INT C MULT, P1
   Hou RC, 2020, IEEE T COMPUT IMAG, V6, P640, DOI 10.1109/TCI.2020.2965304
   Jagtap NS, 2022, VISUAL COMPUT, V38, P4353, DOI 10.1007/s00371-021-02300-5
   Karasawa T, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P35, DOI 10.1145/3126686.3126727
   Lee H, 2017, COMPUT GRAPH FORUM, V36, P262, DOI 10.1111/cgf.12875
   Li CZ, 2023, NEURAL COMPUT APPL, V35, P22511, DOI 10.1007/s00521-023-08916-z
   Li GF, 2021, INFORM FUSION, V71, P109, DOI 10.1016/j.inffus.2021.02.008
   Li H.C., 2018, PREPRINT
   Li HF, 2023, INFORM FUSION, V95, P26, DOI 10.1016/j.inffus.2023.02.011
   Li H, 2020, IEEE T IMAGE PROCESS, V29, P4733, DOI 10.1109/TIP.2020.2975984
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li JX, 2020, IEEE T IMAGE PROCESS, V29, P4816, DOI 10.1109/TIP.2020.2976190
   Li L, 2014, ELECTRON LETT, V50, P510, DOI 10.1049/el.2014.0180
   Li L, 2013, APPL OPTICS, V52, P7033, DOI 10.1364/AO.52.007033
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Lin XY, 2022, PROC CVPR IEEE, P20941, DOI 10.1109/CVPR52688.2022.02030
   Liu JY, 2022, PROC CVPR IEEE, P5792, DOI 10.1109/CVPR52688.2022.00571
   Liu JY, 2023, VISUAL COMPUT, V39, P4869, DOI 10.1007/s00371-022-02633-9
   Lu RT, 2023, VISUAL COMPUT, V39, P2321, DOI 10.1007/s00371-022-02438-w
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JL, 2017, INFRARED PHYS TECHN, V82, P8, DOI 10.1016/j.infrared.2017.02.005
   Qu GH, 2002, ELECTRON LETT, V38, P313, DOI 10.1049/el:20020212
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Soroush R, 2023, VISUAL COMPUT, V39, P2725, DOI 10.1007/s00371-022-02488-0
   Tan AL, 2024, VISUAL COMPUT, V40, P3199, DOI 10.1007/s00371-023-03022-6
   Tan W, 2019, APPL OPTICS, V58, P3064, DOI 10.1364/AO.58.003064
   Tang LF, 2023, INFORM FUSION, V91, P477, DOI 10.1016/j.inffus.2022.10.034
   Toet Alexander, 2014, Figshare
   Wang D, 2022, Arxiv, DOI [arXiv:2205.11876, DOI 10.48550/ARXIV.2205.11876]
   Wang X, 2023, IEEE T COMPUT IMAG, V9, P769, DOI 10.1109/TCI.2023.3304471
   Wang XJ, 2023, VISUAL COMPUT, V39, P4801, DOI 10.1007/s00371-022-02628-6
   Xie Q, 2023, VISUAL COMPUT, V39, P4249, DOI 10.1007/s00371-022-02588-x
   Xu H, 2021, IEEE T COMPUT IMAG, V7, P824, DOI 10.1109/TCI.2021.3100986
   Xu H, 2020, AAAI CONF ARTIF INTE, V34, P12484
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Xydeas CS, 2000, ELECTRON LETT, V36, P308, DOI 10.1049/el:20000267
   Yin WX, 2023, VISUAL COMPUT, V39, P6723, DOI 10.1007/s00371-022-02759-w
   Yin WX, 2022, INFRARED PHYS TECHN, V121, DOI 10.1016/j.infrared.2022.104041
   Yu CC, 2024, VISUAL COMPUT, V40, P3347, DOI 10.1007/s00371-023-03037-z
   Zhang H, 2021, INT J COMPUT VISION, V129, P2761, DOI 10.1007/s11263-021-01501-8
   Zhao ZX, 2020, SIGNAL PROCESS, V177, DOI 10.1016/j.sigpro.2020.107734
   Zhou ZQ, 2023, INFORM FUSION, V93, P174, DOI 10.1016/j.inffus.2022.12.022
NR 57
TC 0
Z9 0
U1 15
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 14
PY 2024
DI 10.1007/s00371-024-03273-x
EA FEB 2024
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HR9E7
UT WOS:001161343100002
DA 2024-08-05
ER

PT J
AU Wu, D
   Cheng, JX
   Li, ZD
   Chen, Z
AF Wu, Dan
   Cheng, Jixiang
   Li, Zhidan
   Chen, Zhou
TI Image inpainting based on fusion structure information and pixelwise
   attention
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image inpainting; Deep learning; Parallel prior embedding; Fusion
   structure information; Pixelwise attention; Joint loss function
ID EDGE; NETWORK
AB Image inpainting refers to restoring the damaged areas of an image using the remaining available information. In recent years, deep learning-based image inpainting has been extensively explored and shown remarkable performance, among which the parallel prior embedding methods have the advantages of few network parameters and relatively low training difficulty. However, most methods use a single prior that is unable to provide sufficient guidance information. Hence, they are unable to generate high-quality, realistic, and vivid images. Fusion labels are effective priors that could provide more meaningful guidance information for inpainting. Meanwhile, attention mechanisms can focus on effective features and establish long-range correlations, which is helpful to refine texture details. Therefore, this paper proposes a parallel prior embedding image inpainting method based on fusion structure information (FSI) and pixelwise attention. A FSI module using the color structure and edge information is designed to update structure features and image features alternately, pixelwise attention is utilized to refine image details, and a joint loss is applied to constrain model training. Extensive experiments are conducted on multiple public datasets, and the results show that the proposed method achieves generally superior performance over several compared methods in terms of several quantitative metrics and qualitative analysis.
C1 [Wu, Dan; Cheng, Jixiang; Li, Zhidan; Chen, Zhou] Southwest Petr Univ, Sch Elect Engn & Informat, Chengdu 610500, Peoples R China.
C3 Southwest Petroleum University
RP Cheng, JX (corresponding author), Southwest Petr Univ, Sch Elect Engn & Informat, Chengdu 610500, Peoples R China.
EM 15283775359@163.com; chengjixiang0106@126.com; dan.807@163.com;
   m17828170978@163.com
CR Arya AS, 2024, VISUAL COMPUT, V40, P345, DOI 10.1007/s00371-023-02786-1
   Bertalmío M, 2001, PROC CVPR IEEE, P355
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Cao CJ, 2022, LECT NOTES COMPUT SC, V13675, P306, DOI 10.1007/978-3-031-19784-0_18
   Chen MY, 2021, IEEE SIGNAL PROC LET, V28, P842, DOI 10.1109/LSP.2021.3070738
   Chen MY, 2020, NEUROCOMPUTING, V405, P259, DOI 10.1016/j.neucom.2020.03.090
   Chen YT, 2021, APPL INTELL, V51, P3460, DOI 10.1007/s10489-020-01971-2
   Cheng Q, 2014, IEEE T GEOSCI REMOTE, V52, P175, DOI 10.1109/TGRS.2012.2237521
   Criminisi A, 2003, PROC CVPR IEEE, P721
   Demir U, 2018, Arxiv, DOI arXiv:1803.07422
   Doersch C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185597
   Dong QL, 2022, PROC CVPR IEEE, P11348, DOI 10.1109/CVPR52688.2022.01107
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guillemot C, 2014, IEEE SIGNAL PROC MAG, V31, P127, DOI 10.1109/MSP.2013.2273004
   Guo XF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14114, DOI 10.1109/ICCV48922.2021.01387
   Hedjazi MA, 2021, SIGNAL PROCESS-IMAGE, V93, DOI 10.1016/j.image.2021.116148
   Hedjazi MA, 2021, KNOWL-BASED SYST, V217, DOI 10.1016/j.knosys.2021.106789
   Heusel M., 2017, Asian J. Appl. Sci. Eng, V30, P25
   Hongyu Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P725, DOI 10.1007/978-3-030-58536-5_43
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hui Z, 2020, Arxiv, DOI arXiv:2002.02609
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Gulrajani I, 2017, ADV NEUR IN, V30
   Jain J, 2023, IEEE WINT CONF APPL, P208, DOI 10.1109/WACV56688.2023.00029
   Jingyuan Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7757, DOI 10.1109/CVPR42600.2020.00778
   Kinli Furkan, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12538), P182, DOI 10.1007/978-3-030-66823-5_11
   Komodakis N, 2007, IEEE T IMAGE PROCESS, V16, P2649, DOI 10.1109/TIP.2007.906269
   Li HF, 2019, IEEE T CYBERNETICS, V49, P4398, DOI 10.1109/TCYB.2018.2865036
   Li JY, 2019, IEEE I CONF COMP VIS, P5961, DOI 10.1109/ICCV.2019.00606
   Li WB, 2022, PROC CVPR IEEE, P10748, DOI 10.1109/CVPR52688.2022.01049
   Li XG, 2022, PROC CVPR IEEE, P1859, DOI 10.1109/CVPR52688.2022.00191
   Liao L, 2021, PROC CVPR IEEE, P6535, DOI 10.1109/CVPR46437.2021.00647
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu HY, 2019, IEEE I CONF COMP VIS, P4169, DOI 10.1109/ICCV.2019.00427
   Liu WR, 2021, PATTERN RECOGN LETT, V143, P81, DOI 10.1016/j.patrec.2020.12.008
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Miyato T, 2018, Arxiv, DOI arXiv:1802.05957
   Nazeri K., 2019, arXiv
   Ni MH, 2023, PROC CVPR IEEE, P14183, DOI 10.1109/CVPR52729.2023.01363
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Qin J, 2021, COMPUT VIS IMAGE UND, V204, DOI 10.1016/j.cviu.2020.103155
   Qin Z, 2021, DISPLAYS, V69, DOI 10.1016/j.displa.2021.102028
   Shao H, 2020, SIGNAL PROCESS-IMAGE, V87, DOI 10.1016/j.image.2020.115929
   Shin YG, 2021, IEEE T NEUR NET LEAR, V32, P252, DOI 10.1109/TNNLS.2020.2978501
   Song YH, 2018, Arxiv, DOI arXiv:1805.03356
   Sun LJ, 2021, IEEE ACCESS, V9, P3816, DOI 10.1109/ACCESS.2020.3047740
   Wang N, 2021, IEEE T IMAGE PROCESS, V30, P1784, DOI 10.1109/TIP.2020.3048629
   Wang Y, 2018, ADV NEUR IN, V31
   Wang Z., 2023, Vis. Comput.
   Wong R, 2020, IEEE J-STARS, V13, P4369, DOI 10.1109/JSTARS.2020.3012443
   Xie CH, 2019, IEEE I CONF COMP VIS, P8857, DOI 10.1109/ICCV.2019.00895
   Xie YC, 2022, VISUAL COMPUT, V38, P3149, DOI 10.1007/s00371-022-02523-0
   Xiong W, 2019, PROC CVPR IEEE, P5833, DOI 10.1109/CVPR.2019.00599
   Xu L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366158
   Xu LM, 2020, NEUROCOMPUTING, V402, P220, DOI 10.1016/j.neucom.2020.04.011
   Xu SX, 2021, IEEE T CIRC SYST VID, V31, P1308, DOI 10.1109/TCSVT.2020.3001267
   Yang J, 2020, AAAI CONF ARTIF INTE, V34, P12605
   Yang YZ, 2022, VISUAL COMPUT, V38, P2647, DOI 10.1007/s00371-021-02143-0
   Yanhong Zeng, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P528, DOI 10.1007/978-3-030-58517-4_31
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Yu TX, 2022, INT J COMPUT VISION, V130, P2646, DOI 10.1007/s11263-022-01665-x
   Yu YC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P69, DOI 10.1145/3474085.3475436
   Yu YS, 2022, LECT NOTES COMPUT SC, V13676, P668, DOI 10.1007/978-3-031-19787-1_38
   Zeng YH, 2019, PROC CVPR IEEE, P1486, DOI 10.1109/CVPR.2019.00158
   Zhang HT, 2019, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2019.00281
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang S, 2018, IEEE T INF FOREN SEC, V13, P637, DOI 10.1109/TIFS.2017.2763119
   Zhang XB, 2023, INFORM FUSION, V90, P74, DOI 10.1016/j.inffus.2022.08.033
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
NR 70
TC 0
Z9 0
U1 13
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 7
PY 2024
DI 10.1007/s00371-023-03255-5
EA FEB 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HB6E8
UT WOS:001157060800003
DA 2024-08-05
ER

PT J
AU Xu, BY
   Hou, RC
   Bei, J
   Ren, TW
   Wu, GS
AF Xu, Boyue
   Hou, Ruichao
   Bei, Jia
   Ren, Tongwei
   Wu, Gangshan
TI Jointly modeling association and motion cues for robust infrared UAV
   tracking
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Object tracking; UAV; Infrared modality; Motion estimation
ID NETWORKS
AB UAV tracking plays a crucial role in computer vision by enabling real-time monitoring UAVs, enhancing safety and operational capabilities while expanding the potential applications of drone technology. Off-the-shelf deep learning based trackers have not been able to effectively address challenges such as occlusion, complex motion, and background clutter for UAV objects in infrared modality. To overcome these limitations, we propose a novel tracker for UAV object tracking, named MAMC. To be specific, the proposed method first employs a data augmentation strategy to enhance the training dataset. We then introduce a candidate target association matching method to deal with the problem of interference caused by the presence of a large number of similar targets in the infrared pattern. Next, it leverages a motion estimation algorithm with window jitter compensation to address the tracking instability due to background clutter and occlusion. In addition, a simple yet effective object research and update strategy is used to address the complex motion and localization problem of UAV objects. Experimental results demonstrate that the proposed tracker achieves state-of-the-art performance on the Anti-UAV and LSOTB-TIR dataset.
C1 [Xu, Boyue; Hou, Ruichao; Bei, Jia; Ren, Tongwei; Wu, Gangshan] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China.
C3 Nanjing University
RP Bei, J (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China.
EM xuby@smail.nju.edu.cn; rc_hou@smail.nju.edu.cn; beijia@nju.edu.cn;
   rentw@nju.edu.cn; gswu@nju.edu.cn
OI Hou, Ruichao/0000-0001-8111-7339; Xu, Boyue/0009-0003-7697-9630
FU National Natural ScienceFoundation of China [62072232]; Key R&D Project
   of Jiangsu Province [BE2022138]; Fundamental Research Funds for the
   Central Universities [021714380026]; Program B for Outstanding
   Ph.D,candidate of Nanjing University; Collaborative Innovation Center of
   Novel Software Technology and Industrialization
FX This work was supported by the National Natural ScienceFoundation of
   China (62072232), the Key R&D Project of Jiangsu Province (BE2022138),
   the Fundamental Research Funds for the Central Universities
   (021714380026), the program B for Outstanding Ph.D,candidate of Nanjing
   University, and the Collaborative Innovation Center of Novel Software
   Technology and Industrialization
CR Abbass MY, 2021, VISUAL COMPUT, V37, P993, DOI 10.1007/s00371-020-01848-y
   Al-Jebrni AH, 2023, VISUAL COMPUT, V39, P3675, DOI 10.1007/s00371-023-02984-x
   Andong L., 2022, IEEE Trans. Neural Netw. Learn. Syst., P1
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Cao ZA, 2022, PROC CVPR IEEE, P14778, DOI 10.1109/CVPR52688.2022.01438
   Cao ZA, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15437, DOI 10.1109/ICCV48922.2021.01517
   Chen RM, 2022, INFRARED PHYS TECHN, V123, DOI 10.1016/j.infrared.2022.104190
   Chen X., 2021, IEEE CVF C COMP VIS
   Chen ZH, 2021, VISUAL COMPUT, V37, P2657, DOI 10.1007/s00371-021-02199-y
   Cui YT, 2022, PROC CVPR IEEE, P13598, DOI 10.1109/CVPR52688.2022.01324
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Du YH, 2023, IEEE T MULTIMEDIA, V25, P8725, DOI 10.1109/TMM.2023.3240881
   Fan JW, 2023, VISUAL COMPUT, V39, P319, DOI 10.1007/s00371-021-02331-y
   Fu ZH, 2021, PROC CVPR IEEE, P13769, DOI 10.1109/CVPR46437.2021.01356
   He K., CVPR 2016
   Hou R., 2022, IEEE INT C MULT EXP
   Hou R., 2023, IEEE INT C MULT EXP
   Huang B., 2021, IEEE CVF INT C COMP
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kalsotra R, 2022, VISUAL COMPUT, V38, P4151, DOI 10.1007/s00371-021-02286-0
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li B., 2023, P IEEE CVF INT C COM
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Li X, 2019, KNOWL-BASED SYST, V166, P71, DOI 10.1016/j.knosys.2018.12.011
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu Q., 2020, P 28 ACM INT C MULT
   Liu Q, 2021, IEEE T MULTIMEDIA, V23, P2114, DOI 10.1109/TMM.2020.3008028
   Liu Q, 2023, IEEE T MULTIMEDIA, V25, P1269, DOI 10.1109/TMM.2022.3140929
   Liu Q, 2017, KNOWL-BASED SYST, V134, P189, DOI 10.1016/j.knosys.2017.07.032
   Mayer C, 2022, PROC CVPR IEEE, P8721, DOI 10.1109/CVPR52688.2022.00853
   Mayer C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13424, DOI 10.1109/ICCV48922.2021.01319
   Paul M., 2022, EUR C COMP VIS
   Shi X., 2022, 2022 3 INT C COMP VI
   Sun J., 2021, P 29 ACM INT C MULT
   Wang N, 2021, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR46437.2021.00162
   Welch G. F., 2021, Computer Vision: A ReferenceGuide, P1, DOI [10.1007/978-3-030-03243-2_716-1, DOI 10.1007/978-3-030-03243-2_716-1]
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Wu H, 2020, IEEE COMPUT SOC CONF, P4448, DOI 10.1109/CVPRW50498.2020.00524
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Xing DT, 2022, IEEE WINT CONF APPL, P1898, DOI 10.1109/WACV51458.2022.00196
   Xu YD, 2020, AAAI CONF ARTIF INTE, V34, P12549
   Yan B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10428, DOI 10.1109/ICCV48922.2021.01028
   Yao TT, 2021, INFRARED PHYS TECHN, V117, DOI 10.1016/j.infrared.2021.103825
   Ye BT, 2022, LECT NOTES COMPUT SC, V13682, P341, DOI 10.1007/978-3-031-20047-2_20
   Ye Jianglong, 2022, P IEEE CVF C COMP VI
   Yu XG, 2018, PATTERN RECOGN LETT, V105, P59, DOI 10.1016/j.patrec.2017.05.017
   Yuan D, 2022, NEUROCOMPUTING, V491, P44, DOI 10.1016/j.neucom.2022.03.055
   Yuan D, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3486678
   Zhang JM, 2022, NEURAL COMPUT APPL, V34, P6359, DOI 10.1007/s00521-021-06771-4
   Zhang PY, 2022, PROC CVPR IEEE, P8876, DOI 10.1109/CVPR52688.2022.00868
   Zhao J, 2021, Arxiv, DOI arXiv:2108.09909
   Zhao J, 2022, IEEE T INTELL TRANSP, V23, P25323, DOI 10.1109/TITS.2022.3177627
   Zhu Y., 2023, IEEE Transactions on Neural Networks and Learning Systems, P1
NR 56
TC 0
Z9 0
U1 8
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 3
PY 2024
DI 10.1007/s00371-023-03245-7
EA FEB 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GW9Y9
UT WOS:001155840600001
DA 2024-08-05
ER

PT J
AU Dudas, A
AF Dudas, Adam
TI Graphical representation of data prediction potential: correlation
   graphs and correlation chains
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Prediction potential; Correlation; Data visualization; Graph; Chain;
   Data analysis
AB The correlation of the set of attributes is a crucial statistical value for the measuring of prediction potential present in a dataset. The correlation coefficient, which measures the correlation between the values of two attributes, can be used in order to measure the prediction potential between two-element subsets of a dataset containing a high number of attributes. In this way two common summary visualizations of prediction potential in datasets are formed-correlation matrices and correlation heatmaps. Both of these visualizations are focused on the presentation of correlation between pair of attributes but not much more regarding the context of correlations in the dataset. The main objective of this article is the design and implementation of graphical models usable in a visual representation of data prediction potential-correlation graphs and correlation chains-which emphasize the pseudo-transitivity of prediction potential in a dataset.
C1 [Dudas, Adam] Matej Bel Univ, Fac Nat Sci, Dept Comp Sci, Banska Bystrica, Slovakia.
C3 Matej Bel University
RP Dudas, A (corresponding author), Matej Bel Univ, Fac Nat Sci, Dept Comp Sci, Banska Bystrica, Slovakia.
EM adam.dudas@umb.sk
OI Dudas, Adam/0000-0001-5517-9464
FU Matej Bel University in Bansk Bystrica
FX The author of the work would like to thank all the user study
   participants from the Department of Computer Science of Matej Bel
   University.
CR Bon-Gang H., 2018, Performance and improvement of green construction projects: Management strategies and innovations
   Caro Y, 2023, DISCRETE MATH, V346, DOI 10.1016/j.disc.2022.113221
   Cauteruccio F, 2023, THEOR PRACT LOG PROG, DOI 10.1017/S1471068423000066
   Custode LL, 2023, IEEE ACCESS, V11, P6169, DOI 10.1109/ACCESS.2023.3236260
   Dudas Adam, 2023, 2023 33rd Conference of Open Innovations Association (FRUCT), P21, DOI 10.23919/FRUCT58615.2023.10143001
   Earnshaw RA, 2021, VISUAL COMPUT, V37, P2921, DOI 10.1007/s00371-021-02182-7
   Fisher R. A., 1988, UCI Machine Learning Repository, DOI DOI 10.24432/C56C76
   Fröhlich K, 2018, MRS ADV, V3, P3427, DOI 10.1557/adv.2018.377
   Kutsanedzie F., 2016, Practical Approaches to Measurements
   Kvet M, 2022, LECT NOTE NETW SYST, V470, P473, DOI 10.1007/978-3-031-04829-6_42
   Li X, 2022, VISUAL COMPUT, V38, P3881, DOI 10.1007/s00371-021-02228-w
   Liu H., 2022, Smart Metro Station Systems, P237
   Maack RGC, 2023, VISUAL COMPUT, V39, P6345, DOI 10.1007/s00371-022-02733-6
   Molnar C., 2022, Interpretable Machine Learning, V2, DOI DOI 10.1007/S10290-014-0202-9
   Nettleton D., 2014, COMMERCIAL DATA MINI, DOI [DOI 10.1016/B978-0-12-416602-8.00006-6, 10.1016/B978-0-12-416602-8.00006-6]
   Peña-Araya V, 2020, IEEE T VIS COMPUT GR, V26, P375, DOI 10.1109/TVCG.2019.2934807
   Ramasubramanian K., 2019, Machine Learning Using R, DOI 10.1007/978-1-4842-4215-5_11
   Skiena S.S., 2017, The data science design manual, DOI DOI 10.1007/978-3-319-55444-0
   Song C, 2022, VISUAL COMPUT, V38, P2489, DOI 10.1007/s00371-021-02125-2
   Szucs G, 2023, VISUAL COMPUT, V39, P3949, DOI 10.1007/s00371-022-02540-z
   WEIER DR, 1980, J STAT PLAN INFER, V4, P381, DOI 10.1016/0378-3758(80)90023-3
   Xue LX, 2020, VISUAL COMPUT, V36, P1325, DOI 10.1007/s00371-019-01731-5
   Yang F, 2012, ISA T, V51, P499, DOI 10.1016/j.isatra.2012.03.005
   Yang YS, 2002, J COMPUT APPL MATH, V144, P349, DOI 10.1016/S0377-0427(01)00572-6
NR 24
TC 2
Z9 2
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 23
PY 2024
DI 10.1007/s00371-023-03240-y
EA JAN 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FP2V7
UT WOS:001146990400002
OA hybrid
DA 2024-08-05
ER

PT J
AU Wu, YT
   Shen, IC
AF Wu, Yu-Ting
   Shen, I-Chao
TI Improving cache placement for efficient cache-based rendering
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Ray tracing; Importance caching; Irradiance caching; Ambient occlusion.
ID VISIBILITY
AB This paper proposes a new method to improve cache placement for various rendering algorithms using caching techniques. The proposed method comprises two stages. The first stage computes an initial cache distribution based on shading points' geometric proximity. We present a view-guided method to cluster shading points based on their world-space positions and surface normals, while considering the camera view to avoid producing small clusters in the final image. The proposed method is more robust and easier to control than previous shading point clustering methods. After computing the shading functions at the initial cache locations, the second stage of our method utilizes the results to allocate additional caches to regions with shading discontinuities. To achieve this, a discontinuity map is created to identify these regions and used to insert new caches based on importance sampling. We integrate the proposed method into several cache-based algorithms, including irradiance caching, importance caching, and ambient occlusion. Extensive experiments show that our method outperforms other cache distributions, producing better results both numerically and visually.
C1 [Wu, Yu-Ting] Natl Taipei Univ Educ, Comp Sci Dept, Taipei, Taiwan.
   [Shen, I-Chao] Univ Tokyo, Comp Sci Dept, Tokyo, Japan.
C3 National Taipei University of Education; University of Tokyo
RP Wu, YT (corresponding author), Natl Taipei Univ Educ, Comp Sci Dept, Taipei, Taiwan.
EM yutingwu@mail.ntpu.edu.tw; ichaoshen@g.ecc.u-tokyo.ac.jp
RI Shen, I-Chao/AHA-3605-2022
OI Shen, I-Chao/0000-0003-4201-3793
FU National Science and Technology Council [111-2222-E-305-001-MY2];
   National Science and Technology Council (NSTC) [JP23K16921]; JSPS
FX We thank the anonymous reviewers for their valuable comments, and the
   creators or providers of the models and textures used in this paper:
   Staircase, CornellBox, DiningRoom, Bathroom, and Kitchen, via Benedikt
   Bitterli's rendering resources [34]; CrytekSponza, Conference, and
   Sibenik from McGuire Computer Graphics Archive [35]; This work was
   supported in part by the National Science and Technology Council (NSTC)
   under grants 111-2222-E-305-001-MY2 and JSPS Grant-in-Aid JP23K16921.
CR Andersson P, 2021, Eurographics Short Papers, DOI DOI 10.2312/EGS.20211015
   Bitterli B., 2016, RENDERING RESOURCES
   Bitterli B, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392481
   Brouillat J, 2008, COMPUT GRAPH FORUM, V27, P1971, DOI 10.1111/j.1467-8659.2008.01346.x
   Clarberg Petrik, 2008, Journal of Graphics Tools, V13, P53
   Clarberg P, 2008, COMPUT GRAPH FORUM, V27, P1125, DOI 10.1111/j.1467-8659.2008.01250.x
   Dubouchet Renaud Adrien, 2017, COMPUT GRAPH FORUM, P41, DOI [10.2312/sre.20171193, DOI 10.2312/SRE.20171193]
   Gassenbauer V, 2009, COMPUT GRAPH FORUM, V28, P1189, DOI 10.1111/j.1467-8659.2009.01496.x
   Gautron P., 2005, P EUROGRAPHICS S REN, DOI [10.1145/1187112.1187154, DOI 10.1145/1187112.1187154]
   Georgiev I, 2012, COMPUT GRAPH FORUM, V31, P701, DOI 10.1111/j.1467-8659.2012.03049.x
   Jarosz W, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330518
   Jensen H. W., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P21
   Keller A., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P49, DOI 10.1145/258734.258769
   Krivánek J, 2005, IEEE T VIS COMPUT GR, V11, P550, DOI 10.1109/TVCG.2005.83
   Krivanek J., 2006, P EUROGRAPHICS S REN, V2006, P127, DOI DOI 10.2312/EGWR/EGSR06/127-138
   Lin DQ, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530158
   Marco J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3185225
   McGuire M., 2017, COMPUTER GRAPHICS AR
   McLaren J., 2015, GAME DEVELOPERS C GD
   Müller T, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459812
   Ou JW, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024213
   Patry J., 2021, Advances in real-time rendering in games, Part 2. SIGGRAPH Course
   Pharr M., 2016, Physically based rendering: From theory to implementation
   Rehfeld H., 2014, PGV 14 P 14 EUR S PA, P25, DOI DOI 10.2312/PGV.20141081
   Scherzer D, 2012, COMPUT GRAPH FORUM, V31, P1391, DOI 10.1111/j.1467-8659.2012.03134.x
   Schwarzhaupt J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366212
   Silvennoinen A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130852
   Tabellion E, 2004, ACM T GRAPHIC, V23, P469, DOI 10.1145/1015706.1015748
   Vévoda P, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201340
   Ward G. J., 1992, Proceedings of the Third Eurographics Workshop on Rendering, P85
   Ward G. J., 1988, Computer Graphics, V22, P85, DOI 10.1145/378456.378490
   Wu YT, 2015, IEEE T VIS COMPUT GR, V21, P363, DOI 10.1109/TVCG.2014.2385059
   Wu YT, 2013, IEEE T VIS COMPUT GR, V19, P1566, DOI 10.1109/TVCG.2013.21
   Yoshida H., 2015, J. WSCG, V23, P65
   Zhao Y., 2019, P 45 GRAPH INT C P G, P1, DOI [10.20380/GI2019.22, DOI 10.20380/GI2019.22]
NR 35
TC 0
Z9 0
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 21
PY 2024
DI 10.1007/s00371-023-03231-z
EA JAN 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL3L8
UT WOS:001145905200001
DA 2024-08-05
ER

PT J
AU Dong, HB
   Song, TY
   Qi, XY
   Jin, JY
   Jin, GY
   Fan, L
AF Dong, Haobo
   Song, Tianyu
   Qi, Xuanyu
   Jin, Jiyu
   Jin, Guiyue
   Fan, Lei
TI Exploring high-quality image deraining Transformer via effective large
   kernel attention
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Single image deraining; Transformer; Convolutional modulation; Local
   representations
AB In recent years, Transformer has demonstrated significant performance in single image deraining tasks. However, the standard self-attention in the Transformer makes it difficult to model local features of images effectively. To alleviate the above problem, this paper proposes a high-quality deraining Transformer with effective large kernel attention, named as ELKAformer. The network employs the Transformer-Style Effective Large Kernel Conv-Block (ELKB), which contains 3 key designs: Large Kernel Attention Block (LKAB), Dynamical Enhancement Feed-forward Network (DEFN), and Edge Squeeze Recovery Block (ESRB) to guide the extraction of rich features. To be specific, LKAB introduces convolutional modulation to substitute vanilla self-attention and achieve better local representations. The designed DEFN refines the most valuable attention values in LKAB, allowing the overall design to better preserve pixel-wise information. Additionally, we develop ESRB to obtain long-range dependencies of different positional information. Massive experimental results demonstrate that this method achieves favorable effects while effectively saving computational costs. Our code is available at github
C1 [Dong, Haobo; Song, Tianyu; Qi, Xuanyu; Jin, Jiyu; Jin, Guiyue; Fan, Lei] Dalian Polytech Univ, Sch Informat Sci & Engn, Dalian 116034, Peoples R China.
C3 Dalian Polytechnic University
RP Jin, JY; Jin, GY (corresponding author), Dalian Polytech Univ, Sch Informat Sci & Engn, Dalian 116034, Peoples R China.
EM dhb2638@163.com; songtienyu@163.com; 13993448762@163.com;
   jiyu.jin@dlpu.edu.cn; guiyue.jin@dlpu.edu.cn; fanlei@dlpu.edu.cn
FU Scientific Research Project of the Education Department of Liaoning
   Province [LJKZ0518, LJKZ0519]
FX This work was supported in part by the Scientific Research Project of
   the Education Department of Liaoning Province (LJKZ0518, LJKZ0519).
CR Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen LY, 2022, LECT NOTES COMPUT SC, V13667, P17, DOI 10.1007/978-3-031-20071-7_2
   Chen X, 2021, IEEE COMPUT SOC CONF, P872, DOI 10.1109/CVPRW53098.2021.00097
   Chen YL, 2013, IEEE I CONF COMP VIS, P1968, DOI 10.1109/ICCV.2013.247
   Chen YP, 2022, PROC CVPR IEEE, P5260, DOI 10.1109/CVPR52688.2022.00520
   Chen ZH, 2021, VISUAL COMPUT, V37, P2657, DOI 10.1007/s00371-021-02199-y
   Dai Z., 2021, Advances in neural information processing systems, V34, P3965, DOI DOI 10.48550/ARXIV.2106.04803
   Das S, 2022, IEEE COMPUT SOC CONF, P81, DOI 10.1109/CVPRW56347.2022.00018
   Ding XH, 2022, PROC CVPR IEEE, P11953, DOI 10.1109/CVPR52688.2022.01166
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Feng H, 2024, IEEE T MULTIMEDIA, V26, P6142, DOI 10.1109/TMM.2023.3347094
   Feng H, 2022, Arxiv, DOI arXiv:2110.12942
   Feng H, 2022, LECT NOTES COMPUT SC, V13697, P475, DOI 10.1007/978-3-031-19836-6_27
   Fu XP, 2020, ENG APPL ARTIF INTEL, V91, DOI 10.1016/j.engappai.2020.103609
   Fu XY, 2020, IEEE T NEUR NET LEAR, V31, P1794, DOI 10.1109/TNNLS.2019.2926481
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Fu YH, 2011, INT CONF ACOUST SPEE, P1453
   Guo MH, 2023, COMPUT VIS MEDIA, V9, P733, DOI 10.1007/s41095-023-0364-2
   Hou Q., 2022, arXiv
   Huang BH, 2020, IEEE WINT CONF APPL, P1795, DOI [10.1109/wacv45572.2020.9093471, 10.1109/WACV45572.2020.9093471]
   Huang Z., 2017, P IEEE C COMPUTER VI, P4700, DOI DOI 10.1109/CVPR.2017.243
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Jing Xu, 2012, Proceedings of the 2012 IEEE International Conference on Computer Science and Automation Engineering (CSAE), P304
   Kulkarni A., 2022, EUR C COMP VIS SPRIN, P344
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Li P., 2021, INT C COMM NETW CHIN, P405
   Li X., 2023, P IEEE CVF INT C COM, P12792
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299
   Liang YC, 2022, IEEE COMPUT SOC CONF, P588, DOI 10.1109/CVPRW56347.2022.00074
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu YT, 2023, COMPUT VIS IMAGE UND, V227, DOI 10.1016/j.cviu.2022.103612
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388
   Mehri A, 2021, IEEE WINT CONF APPL, P2703, DOI 10.1109/WACV48630.2021.00275
   Mei KF, 2019, LECT NOTES COMPUT SC, V11361, P203, DOI 10.1007/978-3-030-20887-5_13
   Mou C, 2022, PROC CVPR IEEE, P17378, DOI 10.1109/CVPR52688.2022.01688
   Qin Q, 2021, IEEE ACCESS, V9, P119881, DOI 10.1109/ACCESS.2021.3108516
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Seif G, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1468, DOI 10.1109/ICASSP.2018.8461664
   Sheng B, 2022, IEEE T CYBERNETICS, V52, P6662, DOI 10.1109/TCYB.2021.3079311
   Si C., 2022, Adv. Neural Inf. Process. Syst, V35, P23495
   Song TY, 2023, IEEE INT CON MULTI, P1889, DOI 10.1109/ICME55011.2023.00324
   Song YD, 2023, IEEE T IMAGE PROCESS, V32, P1927, DOI 10.1109/TIP.2023.3256763
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang H, 2024, IEEE T NEUR NET LEAR, V35, P8668, DOI 10.1109/TNNLS.2022.3231453
   Wang HB, 2024, IEEE T NEUR NET LEAR, V35, P10121, DOI 10.1109/TNNLS.2023.3239033
   Wang Huibing, 2022, IEEE T MULTIMEDIA
   Wang TY, 2019, PROC CVPR IEEE, P12262, DOI 10.1109/CVPR.2019.01255
   Wang XL, 2017, PROC CVPR IEEE, P3039, DOI 10.1109/CVPR.2017.324
   Wang Y, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3383-y
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Xiao J, 2023, IEEE T PATTERN ANAL, V45, P12978, DOI 10.1109/TPAMI.2022.3183612
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Yu WH, 2022, PROC CVPR IEEE, P10809, DOI 10.1109/CVPR52688.2022.01055
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407
   Zhang H, 2017, IEEE WINT CONF APPL, P1259, DOI 10.1109/WACV.2017.145
NR 64
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 2
PY 2024
DI 10.1007/s00371-024-03551-8
EA JUL 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XG0S4
UT WOS:001260419000004
DA 2024-08-05
ER

PT J
AU Chiu, YC
   Tsai, CY
   Chang, PH
AF Chiu, Yu-Chen
   Tsai, Chi-Yi
   Chang, Po-Hsiang
TI Development and validation of a real-time vision-based automatic HDMI
   wire-split inspection system
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Object detection; HDMI cable; Wire inspection; Deep learning; Data
   augmentation
AB In the production process of HDMI cables, manual intervention is often required, resulting in low production efficiency and time-consuming. The paper presents a real-time vision-based automatic inspection system for HDMI cables to reduce the labor requirement in the production process. The system consists of hardware and software design. Since the wires in HDMI cables are tiny objects, the hardware design includes an image capture platform with a high-resolution camera and a ring light source to acquire high-resolution and high-quality images of the wires. The software design includes a data augmentation system and an automatic HDMI wire-split inspection system. The former aims to increase the number and diversity of training samples. The latter is designed to detect the coordinate position of the wire center and the corresponding Pin-ID (pid) number and output the results to the wire-bonding machine to perform subsequent tasks. In addition, a new HDMI cable dataset is created to train and evaluate a series of existing detection network models for this study. The experimental results show that the detection accuracy of the wire center using the existing YOLOv4 detector reaches 99.9%. Furthermore, the proposed system reduces the execution time by about 38.67% compared with the traditional manual wire-split inspection operation.
C1 [Chiu, Yu-Chen; Tsai, Chi-Yi] Tamkang Univ, Coll Engn, Doctoral Program Robot, 151 Yingzhuan Rd, New Taipei City 251, Taiwan.
   [Tsai, Chi-Yi; Chang, Po-Hsiang] Tamkang Univ, Dept Elect & Comp Engn, 151 Yingzhuan Rd, New Taipei City 251, Taiwan.
C3 Tamkang University; Tamkang University
RP Tsai, CY (corresponding author), Tamkang Univ, Coll Engn, Doctoral Program Robot, 151 Yingzhuan Rd, New Taipei City 251, Taiwan.; Tsai, CY (corresponding author), Tamkang Univ, Dept Elect & Comp Engn, 151 Yingzhuan Rd, New Taipei City 251, Taiwan.
EM chiyi_tsai@mail.tku.edu.tw
OI Tsai, Chi-Yi/0000-0001-9872-4338
FU National Science and Technology Council; JAWS CO., LTD. [NSTC
   112-2221-E-032-036-MY2]; National Science and Technology Council of
   Taiwan
FX This research was funded in part by JAWS CO., LTD. and the National
   Science and Technology Council of Taiwan under Grant NSTC
   112-2221-E-032-036-MY2.
CR Ai LM, 2024, VISUAL COMPUT, V40, P1453, DOI 10.1007/s00371-023-02860-8
   Allied Market Research, 2020, Research and Markets
   Bhoite A, 2011, IEEE AUTOTESTCON, P226, DOI 10.1109/AUTEST.2011.6058727
   Bin Roslan Muhammad Izzat, 2022, 2022 IEEE 12th Symposium on Computer Applications & Industrial Electronics (ISCAIE)., P111, DOI 10.1109/ISCAIE54458.2022.9794475
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Chethan Kumar B., 2020, 2020 Third International Conference on Smart Systems and Inventive Technology (ICSSIT), P1316, DOI 10.1109/ICSSIT48917.2020.9214094
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Ghidoni S, 2015, IEEE T AUTOM SCI ENG, V12, P596, DOI 10.1109/TASE.2014.2360233
   Gu JX, 2018, PATTERN RECOGN, V77, P354, DOI 10.1016/j.patcog.2017.10.013
   Haofei Xie, 2021, 2021 IEEE 2nd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE), P851, DOI 10.1109/ICBAIE52039.2021.9390006
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Ioffe S, 2015, Arxiv, DOI [arXiv:1502.03167, DOI 10.48550/ARXIV.1502.03167, 10.48550/arXiv.1502.03167]
   Jocher G., 2020, Ultralytics
   Jocher G., 2023, Ultralytics
   Li CY, 2022, Arxiv, DOI [arXiv:2209.02976, DOI 10.48550/ARXIV.2209.02976]
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, Arxiv, DOI [arXiv:1512.02325, 10.48550/arXiv.1512.02325]
   Ning JF, 2010, PATTERN RECOGN, V43, P445, DOI 10.1016/j.patcog.2009.03.004
   Redmon J, 2018, Arxiv, DOI arXiv:1804.02767
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shafi O, 2021, I S WORKL CHAR PROC, P226, DOI 10.1109/IISWC53511.2021.00030
   Sun J, 2023, VISUAL COMPUT, V39, P4391, DOI 10.1007/s00371-022-02597-w
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Van Nhan Nguyen, 2019, IEEE Power and Energy Technology Systems Journal, V6, P11, DOI 10.1109/JPETS.2018.2881429
   Wang CY, 2022, Arxiv, DOI [arXiv:2207.02696, DOI 10.48550/ARXIV.2207.02696]
   Wang CY, 2020, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW50498.2020.00203
   Wang J., 2022, OCEANS 2022 CHENN, P1
   Wu HY, 2024, VISUAL COMPUT, V40, P681, DOI 10.1007/s00371-023-02809-x
   Xi Y, 2023, MACH INTELL RES, V20, P729, DOI 10.1007/s11633-022-1355-y
   Xie J, 2021, 2021 2ND INTERNATIONAL CONFERENCE ON BIG DATA & ARTIFICIAL INTELLIGENCE & SOFTWARE ENGINEERING (ICBASE 2021), P387, DOI 10.1109/ICBASE53849.2021.00078
   Xu B, 2015, Arxiv, DOI arXiv:1505.00853
   Xu C, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3169555
   Yao DZ, 2024, VISUAL COMPUT, V40, P2589, DOI 10.1007/s00371-023-02939-2
   Yichen Zhang, 2020, 2020 IEEE 6th International Conference on Computer and Communications (ICCC), P1224, DOI 10.1109/ICCC51575.2020.9345302
   Yu-Chuan Bian, 2021, 2021 5th International Conference on Automation, Control and Robots (ICACR), P29, DOI 10.1109/ICACR53472.2021.9605165
NR 39
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 21
PY 2024
DI 10.1007/s00371-024-03436-w
EA JUN 2024
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UY0Q4
UT WOS:001251511100004
DA 2024-08-05
ER

PT J
AU Shi, WZ
   Tao, F
   Wen, Y
AF Shi, Wuzhen
   Tao, Fei
   Wen, Yang
TI Joint super-resolution-based fast face image coding for human and
   machine vision
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Scalable coding; Image compression; Video coding for machine;
   Super-resolution
ID COMPRESSION
AB As the Internet of Things continues to grow and thrive, more and more data are consumed by machines for intelligent analysis. How to simultaneously support fast machine vision analysis and obtain high-quality reconstructed images to serve human vision has become a problem that needs to be solved. We propose a fast face image compression scheme based on a Joint Super-Resolution Network (JSRNet), where images are encoded hierarchically to support machine vision and human vision. To support fast machine vision tasks, we compress the downsampled version of the input image at the encoding side and implement base layer decoding at the decoding side with a lightweight super-resolution network, thus effectively reducing the codec time. To obtain high-quality reconstructed images, we design a content transfer module to transfer the visual information generated by the base layer to the enhancement layer. The joint optimization of the base layer and the enhancement layer also allows them to better cooperate to achieve better performance. We verify the performance of this scheme on the facial landmark detection task, and the experimental results show that our method saves more bit-rate and achieves more accurate facial landmark detection results with faster codec time than baselines. The reconstructed images are more in line with human visual characteristics, and the peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) are also improved significantly.
C1 [Shi, Wuzhen; Tao, Fei; Wen, Yang] Shenzhen Univ, Coll Elect & Informat Engn, Guangdong Prov Engn Lab Digital Creat Technol, Guangdong Prov Key Lab Intelligent Informat Proc, 3688 Nanhai Ave, Shenzhen, Peoples R China.
C3 Shenzhen University
RP Wen, Y (corresponding author), Shenzhen Univ, Coll Elect & Informat Engn, Guangdong Prov Engn Lab Digital Creat Technol, Guangdong Prov Key Lab Intelligent Informat Proc, 3688 Nanhai Ave, Shenzhen, Peoples R China.
EM wzhshi@szu.edu.cn
FU National Natural Science Foundation of China [62101346, 62301330];
   National Natural Science Foundation of China [2024A1515010496,
   2022A1515110101]; Guangdong Basic and Applied Basic Research Foundation
   [20231121103807001]; Stable Support Plan for Shenzhen Higher Education
   Institutions [2023B1212060076]; Guangdong Provincial Key Laboratory
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant Nos. 62101346 and 62301330, in part by
   the Guangdong Basic and Applied Basic Research Foundation under Grant
   Nos. 2024A1515010496 and 2022A1515110101, in part by the Stable Support
   Plan for Shenzhen Higher Education Institutions under Grant No.
   20231121103807001, and in part by the Guangdong Provincial Key
   Laboratory under Grant Nos. 2023B1212060076.
CR Agustsson E, 2017, ADV NEUR IN, V30
   Balle J., 2018, ICLR
   Balle J., 2017, P INT C LEARN REPR
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chen HT, 2019, IEEE I CONF COMP VIS, P3513, DOI 10.1109/ICCV.2019.00361
   Chen T, 2021, IEEE T IMAGE PROCESS, V30, P3179, DOI 10.1109/TIP.2021.3058615
   Choi H, 2022, IEEE T IMAGE PROCESS, V31, P2739, DOI 10.1109/TIP.2022.3160602
   CLEARY JG, 1984, IEEE T COMMUN, V32, P396, DOI 10.1109/TCOM.1984.1096090
   Dollár P, 2013, IEEE I CONF COMP VIS, P1841, DOI 10.1109/ICCV.2013.231
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Gao W., 2021, ARXIV, DOI 10.48550/arXiv.2110.06389
   Guo J., 2020, P IEEE CVF C COMP VI
   Hong C., 2022, P IEEECVF WINTER C A, P2675
   Hou Y, 2020, P IEEE CVF C COMP VI, P125
   Hu YY, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102750
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Le N, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P1590, DOI 10.1109/ICASSP39728.2021.9414465
   Li J., 2023, IEEE T MED IMAG
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu K, 2021, INT J COMPUT VISION, V129, P2605, DOI 10.1007/s11263-021-01491-7
   Liu YT, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3414837
   Liu YT, 2020, IEEE ACCESS, V8, P84105, DOI 10.1109/ACCESS.2020.2991842
   Liu YT, 2020, IEEE T CIRC SYST VID, V30, P929, DOI 10.1109/TCSVT.2019.2900472
   Lu G., 2022, ARXIV
   Ma HC, 2022, IEEE T PATTERN ANAL, V44, P1247, DOI 10.1109/TPAMI.2020.3026003
   Nazir A, 2022, IEEE T IMAGE PROCESS, V31, P880, DOI 10.1109/TIP.2021.3136619
   Özyilkan E, 2023, IEEE DATA COMPR CONF, P42, DOI 10.1109/DCC55655.2023.00012
   Parker J, 2023, ANALYSING THE HISTORY OF BRITISH SOCIAL WELFARE, P82, DOI 10.1145/3551389
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Singh S, 2020, IEEE IMAGE PROC, P3349, DOI 10.1109/ICIP40778.2020.9190860
   Sun C., 2023, IEEE T CONSUM ELECTR
   Sun SM, 2021, IEEE T CIRC SYST VID, V31, P3631, DOI 10.1109/TCSVT.2020.3042517
   Suzuki S, 2020, IEEE IMAGE PROC, P3099, DOI 10.1109/ICIP40778.2020.9190933
   Toderici G., 2016, P INT C LEARN REPR I
   Yang S, 2021, IEEE T MULTIMEDIA, V23, P2957, DOI 10.1109/TMM.2021.3068580
   Yang Wenkai, 2021, ARXIV
   Yang ZH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1561, DOI 10.1145/3394171.3413968
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Zhao J, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3580499
NR 42
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 20
PY 2024
DI 10.1007/s00371-024-03428-w
EA MAY 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RM9B6
UT WOS:001228186500001
DA 2024-08-05
ER

PT J
AU Huang, JK
   Zheng, R
   Cheng, YY
   Hu, JQ
   Hu, WJ
   Shang, WL
   Zhang, M
   Cao, Z
AF Huang, Junkai
   Zheng, Rui
   Cheng, Youyong
   Hu, Jiaqian
   Hu, Weijun
   Shang, Wenli
   Zhang, Man
   Cao, Zhong
TI Interactive semantics neural networks for skeleton-based human
   interaction recognition
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Skeleton-based interaction recognition; Action recognition; Graph
   convolutional network; Semantic information; Hierarchical network
AB Skeleton-based human interaction recognition is a formidable challenge that demands the capability to discern spatial, temporal, and interactive features. However, current research still faces some limitations in identifying spatial, temporal, and interaction features. Methods based on graph convolutional networks often prove to be insufficient in capturing interactive features and structural semantic information of skeletons. In order to solve this problem, we construct a Mutual-semantic Adjacency Matrix (MAM) by amalgamating the relative semantic attention of two skeleton sequences. This MAM was then integrated with the convolution of residual graphs to enhance the extraction of spatial and interaction features. We propose a novel interactive semantics neural network (ISNN) for skeleton-based human interaction recognition to hierarchically fuse MAM and structural semantic information. In addition, integrating the bone stream, we propose a two-stream Interactive Semantics Neural Network (2 s-ISNN). Experiments conducted with our models on two interaction datasets, NTU-RGB+D (mutual) and NTU-RGB+D 120 (mutual), demonstrate significantly improved recognition capabilities in comprehending human interactions. The source code is available at: https://github.com/czant1977/ISNN-master//.
C1 [Huang, Junkai; Zheng, Rui; Cheng, Youyong; Hu, Jiaqian; Hu, Weijun; Shang, Wenli; Zhang, Man; Cao, Zhong] Guangzhou Univ, Sch Elect & Commun Engn, Guangzhou 510006, Guangdong, Peoples R China.
C3 Guangzhou University
RP Cao, Z (corresponding author), Guangzhou Univ, Sch Elect & Commun Engn, Guangzhou 510006, Guangdong, Peoples R China.
EM zhongc@gzhu.edu.cn
FU National Natural Science Foundation of China; National Key Research and
   Development Program of China [2021YFB2012400]; Basic and Applied Basic
   Research Funding of Guangdong Province [2022A1515011558,
   2022A1515010865]; Guangzhou Science and Technology [202201020217]; Key
   Laboratory of Guangdong Higher Education Institutes [2023KSYS002]; 
   [62173101]
FX This work was supported in part by National Key Research and Development
   Program of China under Grant 2021YFB2012400, the National Natural
   Science Foundation of China under Grant 62173101, the Basic and Applied
   Basic Research Funding of Guangdong Province under Grant 2022A1515011558
   and Grant 2022A1515010865, the Guangzhou Science and Technology Funding
   under Grant 202201020217, the Key Laboratory of Guangdong Higher
   Education Institutes under Grant 2023KSYS002.
CR Bin YR, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107410
   Bruna J., 2014, ABS13126203 CORR, P1, DOI DOI 10.48550/ARXIV.1312.6203
   Cao CQ, 2019, IEEE T CIRC SYST VID, V29, P3247, DOI 10.1109/TCSVT.2018.2879913
   Chen HB, 2021, 2021 IEEE 14TH INTERNATIONAL SYMPOSIUM ON EMBEDDED MULTICORE/MANY-CORE SYSTEMS-ON-CHIP (MCSOC 2021), P190, DOI 10.1109/MCSoC51149.2021.00036
   Chen Z, 2021, AAAI CONF ARTIF INTE, V35, P1113, DOI 10.1145/3474085.3475574
   Defferrard M, 2016, ADV NEUR IN, V29
   Duvenaudt D, 2015, ADV NEUR IN, V28
   Gao F., 2022, P IEEE INT C MULT EX, P1
   Gao X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P601, DOI 10.1145/3343031.3351170
   Hamilton WL, 2017, ADV NEUR IN, V30
   He T, 2019, PROC CVPR IEEE, P558, DOI 10.1109/CVPR.2019.00065
   Henaff M, 2015, Arxiv, DOI arXiv:1506.05163
   Ioffe Sergey, 2015, INT C MACHINE LEARNI, V37, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Ji YL, 2014, IEEE INT C MULTIMEDI
   Kamel A, 2021, IEEE T MULTIMEDIA, V23, P1330, DOI 10.1109/TMM.2020.2999181
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kipf T, 2018, PR MACH LEARN RES, V80
   Lee J, 2023, IEEE I CONF COMP VIS, P10410, DOI 10.1109/ICCV51070.2023.00958
   Li C, 2017, IEEE INT CONF MULTI
   Li CL, 2018, AAAI CONF ARTIF INTE, P3482
   Li JN, 2021, NEUROCOMPUTING, V444, P338, DOI 10.1016/j.neucom.2019.12.149
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Li S, 2018, PROC CVPR IEEE, P5457, DOI 10.1109/CVPR.2018.00572
   Liu H, 2017, Arxiv, DOI [arXiv:1705.08106, DOI 10.48550/ARXIV.1705.08106]
   Liu J, 2018, IEEE T IMAGE PROCESS, V27, P1586, DOI 10.1109/TIP.2017.2785279
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu J, 2018, IEEE T PATTERN ANAL, V40, P3007, DOI 10.1109/TPAMI.2017.2771306
   Liu J, 2017, PROC CVPR IEEE, P3671, DOI 10.1109/CVPR.2017.391
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Manessi F, 2020, PATTERN RECOGN, V97, DOI 10.1016/j.patcog.2019.107000
   Manzi A, 2018, IET COMPUT VIS, V12, P27, DOI 10.1049/iet-cvi.2017.0118
   Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576
   Niepert M, 2016, PR MACH LEARN RES, V48
   Perez Mauricio, 2020, Pattern Recognition. 5th Asian Conference, ACPR 2019. Revised Selected Papers. Lecture Notes in Computer Science (LNCS 12046), P268, DOI 10.1007/978-3-030-41404-7_19
   Perez M, 2022, IEEE T MULTIMEDIA, V24, P366, DOI 10.1109/TMM.2021.3050642
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192
   Si CY, 2018, LECT NOTES COMPUT SC, V11205, P106, DOI 10.1007/978-3-030-01246-5_7
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HS, 2018, PATTERN RECOGN, V81, P23, DOI 10.1016/j.patcog.2018.03.030
   Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Welling M., 2016, ICLR, P1, DOI DOI 10.48550/ARXIV.1609.02907
   Weng JW, 2018, LECT NOTES COMPUT SC, V11211, P142, DOI 10.1007/978-3-030-01234-2_9
   Wu HM, 2018, IEEE T HUM-MACH SYST, V48, P304, DOI 10.1109/THMS.2017.2776211
   Wu JX, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107382
   Wu YH, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2078
   Xu Q, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2070
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yun K., 2012, 2012 IEEE COMP SOC C, DOI DOI 10.1109/CVPRW.2012.6239234
   Zhang PF, 2021, Arxiv, DOI arXiv:2111.03993
   Zhang PF, 2020, PROC CVPR IEEE, P1109, DOI 10.1109/CVPR42600.2020.00119
   Zhang PF, 2019, IEEE T PATTERN ANAL, V41, P1963, DOI 10.1109/TPAMI.2019.2896631
   Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI [10.1109/ICCV.2017.233, 10.1109/ICCV.2017.231]
   Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24
   Zhang ZH, 2019, PATTERN RECOGN, V88, P38, DOI 10.1016/j.patcog.2018.11.002
   Zheng HL, 2019, ADV NEUR IN, V32
   Zheng W, 2019, Arxiv, DOI [arXiv:1805.02556, DOI 10.48550/ARXIV.1805.02556]
   Zhu AC, 2020, NEUROCOMPUTING, V414, P90, DOI 10.1016/j.neucom.2020.07.068
   Zhu LP, 2021, PATTERN RECOGN, V115, DOI 10.1016/j.patcog.2021.107920
NR 63
TC 0
Z9 0
U1 7
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 7
PY 2024
DI 10.1007/s00371-024-03420-4
EA MAY 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QD2K1
UT WOS:001218872900001
DA 2024-08-05
ER

PT J
AU Li, ZH
   Cui, GM
   Liu, HY
   Chen, ZY
   Zhao, JF
AF Li, Zihan
   Cui, Guangmang
   Liu, Haoyu
   Chen, Ziyi
   Zhao, Jufeng
TI A novel dynamic scene deblurring framework based on hybrid activation
   and edge-assisted dual-branch residuals
SO VISUAL COMPUTER
LA English
DT Article
DE Dynamic scene deblurring; Generative adversarial networks; Residual
   structure; Activation function; Attention mechanism
AB Existing learning-based image deblurring algorithms tend to focus on single source of image information, and the network structure and dynamic scene blur characteristics make it difficult to recover the missing details of the image. Therefore, a novel dynamic scene deblurring framework is proposed based on hybrid activation and edge-assisted dual-branch residuals. Specifically, the network's ability to learn nonlinear features is enhanced by different activation functions, and the feature utilization at different semantic levels is improved by improving the traditional residual structure. In particular, the fixed-parameter training method is adopted to reduce ringing artifacts. And a new dual-source edge extraction algorithm is designed that organically combines edge information from different sources as network inputs. The experimental results demonstrate that our algorithm not only shows advantages in objective evaluation metrics PSNR, SSIM and VIF, but also achieves satisfactory results in subjective visual effects. Source code is publicly available at: https://github.com/Mangolzh/HN.git.
C1 [Li, Zihan; Cui, Guangmang; Liu, Haoyu; Chen, Ziyi; Zhao, Jufeng] Hangzhou Dianzi Univ, Inst Carbon Neutral & New Energy, Sch Elect & Informat, Hangzhou 310018, Peoples R China.
   [Cui, Guangmang; Zhao, Jufeng] Hangzhou Dianzi Univ, Zhejiang Prov Key Lab Equipment Elect, Hangzhou 310018, Peoples R China.
C3 Hangzhou Dianzi University; Hangzhou Dianzi University
RP Cui, GM (corresponding author), Hangzhou Dianzi Univ, Inst Carbon Neutral & New Energy, Sch Elect & Informat, Hangzhou 310018, Peoples R China.; Cui, GM (corresponding author), Hangzhou Dianzi Univ, Zhejiang Prov Key Lab Equipment Elect, Hangzhou 310018, Peoples R China.
EM cuigm@hdu.edu.cn
FU the National Natural Science Foundation of China [LY22F050002]; Natural
   Science Foundation of Zhejiang Province [61805063]; National Natural
   Science Foundation of China [No.CXJJ2023058]; Graduate Scientific
   Research Foundation of Hangzhou Dianzi University
FX This work was supported by the Natural Science Foundation of Zhejiang
   Province under Grant No. LY22F050002 and National Natural Science
   Foundation of China under Grant No. 61805063. This work was also
   supported by the Graduate Scientific Research Foundation of Hangzhou
   Dianzi University under Grant No.CXJJ2023058.
CR Abuolaim A, 2022, IEEE WINT CONF APPL, P82, DOI 10.1109/WACV51458.2022.00016
   Arjovsky M., 2017, ARXIV PREPRINT ARXIV
   Carbajal Guillermo, 2023, IEEE Transactions on Computational Imaging, P928, DOI 10.1109/TCI.2023.3322012
   Chakrabarti A, 2016, LECT NOTES COMPUT SC, V9907, P221, DOI 10.1007/978-3-319-46487-9_14
   Chen K, 2024, IEEE T MULTIMEDIA
   Chen L, 2019, PROC CVPR IEEE, P1742, DOI 10.1109/CVPR.2019.00184
   Chen LY, 2022, LECT NOTES COMPUT SC, V13667, P17, DOI 10.1007/978-3-031-20071-7_2
   Chen Z, 2024, Advances in Neural Information Processing Systems, V36
   Cho SJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4621, DOI 10.1109/ICCV48922.2021.00460
   Cho S, 2011, IEEE I CONF COMP VIS, P495, DOI 10.1109/ICCV.2011.6126280
   Dongwon Park, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P327, DOI 10.1007/978-3-030-58539-6_20
   Glorot X., 2011, P 14 INT C ART INT S, V15, P315, DOI DOI 10.1002/ECS2.1832
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gulrajani F., 2017, Advances in Neural Information Processing Systems, V30, P1
   Guo C, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2066
   He K., 2016, European conference on computer vision, P770, DOI DOI 10.1007/978-3-319-46493-0_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J., 2023, 2023 2 INT C ROBOTIC, P294, DOI [10.1109/RAIIC59453.2023.10280859, DOI 10.1109/RAIIC59453.2023.10280859]
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang Z., 2017, P IEEE C COMPUTER VI, P4700, DOI DOI 10.1109/CVPR.2017.243
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Khan RA, 2022, IET IMAGE PROCESS, V16, P2412, DOI 10.1049/ipr2.12497
   Kim TH, 2013, IEEE I CONF COMP VIS, P3160, DOI 10.1109/ICCV.2013.392
   Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Li LRH, 2018, PROC CVPR IEEE, P6616, DOI 10.1109/CVPR.2018.00692
   Li S., 2023, arXiv
   Li ZH, 2022, J VIS COMMUN IMAGE R, V89, DOI 10.1016/j.jvcir.2022.103663
   Nah S, 2019, IEEE COMPUT SOC CONF, P1996, DOI 10.1109/CVPRW.2019.00251
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Pan JS, 2016, PROC CVPR IEEE, P1628, DOI 10.1109/CVPR.2016.180
   Park S, 2023, APPL INTELL, V53, P2055, DOI 10.1007/s10489-022-03666-2
   Peng Jiaheng, 2023, 2023 42nd Chinese Control Conference (CCC), P7584, DOI 10.23919/CCC58697.2023.10240483
   Redmon J, 2018, Arxiv, DOI arXiv:1804.02767
   Ren MW, 2023, IEEE I CONF COMP VIS, P10687, DOI 10.1109/ICCV51070.2023.00984
   Ruan L, 2023, ARXIV
   Schuler CJ, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2481418
   Sim H, 2019, IEEE COMPUT SOC CONF, P2140, DOI 10.1109/CVPRW.2019.00267
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun J, 2015, PROC CVPR IEEE, P769, DOI 10.1109/CVPR.2015.7298677
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Tsai FJ, 2022, IEEE T IMAGE PROCESS, V31, P6789, DOI 10.1109/TIP.2022.3216216
   Wang YY, 2021, IEEE ACCESS, V9, P26051, DOI 10.1109/ACCESS.2021.3056572
   Whang J, 2022, PROC CVPR IEEE, P16272, DOI 10.1109/CVPR52688.2022.01581
   WOO S, 2018, P EUR C COMP VIS ECC, DOI DOI 10.1007/978-3-030-01234-2_1
   Xu L., 2014, Advances in NIPS 27, V27, P1790
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Yae Shunsuke, 2023, 2023 IEEE International Conference on Consumer Electronics (ICCE), P1, DOI 10.1109/ICCE56470.2023.10043510
   Yang D, 2022, IEEE COMPUT SOC CONF, P1112, DOI 10.1109/CVPRW56347.2022.00120
   Zhang HG, 2019, PROC CVPR IEEE, P5971, DOI 10.1109/CVPR.2019.00613
   Zhang JW, 2018, PROC CVPR IEEE, P2521, DOI 10.1109/CVPR.2018.00267
   Zhang JT, 2022, IEEE ACCESS, V10, P81390, DOI 10.1109/ACCESS.2022.3194524
   Zheng S, 2019, IEEE SIGNAL PROC LET, V26, P1546, DOI 10.1109/LSP.2019.2939752
   Zhou LY, 2017, APPL MATH MODEL, V51, P469, DOI 10.1016/j.apm.2017.07.009
NR 55
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 3849
EP 3869
DI 10.1007/s00371-024-03390-7
EA APR 2024
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001210792300003
DA 2024-08-05
ER

PT J
AU Patil, SB
   Shirgave, S
AF Patil, Sameer Bhimrao
   Shirgave, Suresh
TI Instructor emotion recognition system using manta ray foraging algorithm
   for improving the content delivery in video lecture
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Facial expression recognition; Emotion recognition; Frame
   classification; Code walkthrough sessions
ID CNN
AB In learning environment, instructor's emotions may impact the relationship with students. A wide range of research has been conducted on the facial expression of the student, but the influence of teacher's facial expression remains an unexamined area. Facial expression detection has the ability to detect the impact of an instructor's emotions in a learning environment. During lecture delivery, manual evaluation of the instructor behavior not only improves the learning environment, but can also save time and resources used in manual techniques. Therefore, this research work introduces facial emotion recognition of an instructor in a learning environment with the help of Evolving Hypergraph Manta ray Foraging Convolution Neural Network (EHMFCNN). Initially, the collected lecture videos are converted into frames, and the feature extraction is performed by fast discrete curvelet with wrapping technique. Then, to perform an effective recognition process, the frames are classified into multimedia representation, code walkthrough sessions and instructor presence in the lecture using dense residual dual-shuffle attention network. After classification, the instructor-based frames are selected for further recognition process. Then, evolving hypergraph convolution neural network is utilized to recognize the emotions of the instructor, and manta ray foraging optimization algorithm is employed for weight tuning process. Finally, the proposed system provides appropriate feedback for instructors using recognized emotions. The EHMFCNN system is tested on NPTEL-Indian English speech dataset, and it achieves highest accuracy of 97.5%, which is better than conventional classifiers. Extensive research determines that the introduced scheme improves the performance of the instructor emotion recognition system.
C1 [Patil, Sameer Bhimrao] Shivaji Univ, Dept Technol, Kolhapur 416004, Maharashtra, India.
   [Shirgave, Suresh] DKTE Soc Text & Engn Inst, Comp Sci & Engn, Ichalkaranji 416115, Maharashtra, India.
C3 Shivaji University
RP Patil, SB (corresponding author), Shivaji Univ, Dept Technol, Kolhapur 416004, Maharashtra, India.
EM sbpatil29@gmail.com; skshirgave@gmail.com
CR AlEisa HN, 2023, IEEE ACCESS, V11, P62233, DOI 10.1109/ACCESS.2023.3284457
   Kumar PMA, 2023, IETE J RES, V69, P2595, DOI 10.1080/03772063.2021.1902868
   Bhatti YK, 2021, COMPUT INTEL NEUROSC, V2021, DOI 10.1155/2021/5570870
   Boughanem H, 2023, VISUAL COMPUT, V39, P5693, DOI 10.1007/s00371-022-02690-0
   Chattopadhyay S, 2022, COMPUT BIOL MED, V145, DOI 10.1016/j.compbiomed.2022.105437
   Devaram RR, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22093366
   Di Luzio F, 2023, BIOMED SIGNAL PROCES, V81, DOI 10.1016/j.bspc.2022.104418
   Dukic D, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11081240
   github, About Us
   Guo YY, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20030870
   Hou CB, 2022, FUTURE INTERNET, V14, DOI 10.3390/fi14060177
   Hughes C, 2019, DISTANCE EDUC, V40, P54, DOI 10.1080/01587919.2018.1553559
   Jeong D, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20071936
   Kim J, 2021, IEEE ACCESS, V9, P104367, DOI 10.1109/ACCESS.2021.3099075
   Kim JH, 2019, IEEE ACCESS, V7, P41273, DOI 10.1109/ACCESS.2019.2907327
   Kumar RJR, 2021, VISUAL COMPUT, V37, P2315, DOI 10.1007/s00371-020-01988-1
   Lakshmi D, 2021, MICROPROCESS MICROSY, V82, DOI 10.1016/j.micpro.2021.103834
   Lawson AP, 2021, COMPUT HUM BEHAV, V114, DOI 10.1016/j.chb.2020.106554
   Li CL, 2024, IEEE T CIRC SYST VID, V34, P882, DOI 10.1109/TCSVT.2023.3237006
   Li J, 2020, NEUROCOMPUTING, V411, P340, DOI 10.1016/j.neucom.2020.06.014
   Makhmudkhujaev F, 2019, SIGNAL PROCESS-IMAGE, V74, P1, DOI 10.1016/j.image.2019.01.002
   Mehendale N, 2020, SN APPL SCI, V2, DOI 10.1007/s42452-020-2234-1
   Miao S, 2019, IEEE ACCESS, V7, P78000, DOI 10.1109/ACCESS.2019.2921220
   Minaee S, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21093046
   Mohan K, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3031835
   Muduli D, 2021, BIOMED SIGNAL PROCES, V70, DOI 10.1016/j.bspc.2021.102919
   Nan YH, 2022, ALEX ENG J, V61, P4435, DOI 10.1016/j.aej.2021.09.066
   Pise AA, 2022, WIREL COMMUN MOB COM, V2022, DOI 10.1155/2022/8808283
   Riaz MN, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20041087
   Sadeghi H, 2019, MULTIMED TOOLS APPL, V78, P30335, DOI 10.1007/s11042-019-07863-z
   Sarvakar Ketan, 2023, Materials Today: Proceedings, P3560, DOI 10.1016/j.matpr.2021.07.297
   Saurav S, 2021, APPL INTELL, V51, P5543, DOI 10.1007/s10489-020-02125-0
   Singh Rajesh, 2023, Int J Inf Technol, V15, P1819, DOI 10.1007/s41870-023-01183-0
   Tonguç G, 2020, COMPUT EDUC, V148, DOI 10.1016/j.compedu.2019.103797
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Wang XL, 2022, DIAGNOSTICS, V12, DOI 10.3390/diagnostics12112632
   Wang ZN, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107694
   Wei J, 2024, EXPERT SYST APPL, V237, DOI 10.1016/j.eswa.2023.121419
   Wen ZY, 2023, BIOMIMETICS-BASEL, V8, DOI 10.3390/biomimetics8020199
   Yan LY, 2023, ALEX ENG J, V63, P307, DOI 10.1016/j.aej.2022.08.003
   Yang HS, 2023, VISUAL COMPUT, V39, P2177, DOI 10.1007/s00371-022-02472-8
   Zhang HL, 2019, IEEE ACCESS, V7, P159081, DOI 10.1109/ACCESS.2019.2949741
   Zhang J, 2024, VISUAL COMPUT, V40, P1359, DOI 10.1007/s00371-023-02854-6
   Zhang SY, 2022, MOB INF SYST, V2022, DOI 10.1155/2022/7785929
   Zhang SQ, 2019, IEEE ACCESS, V7, P32297, DOI 10.1109/ACCESS.2019.2901521
   Zhao WG, 2020, ENG APPL ARTIF INTEL, V87, DOI 10.1016/j.engappai.2019.103300
   Zhou SW, 2023, INT J ENV RES PUB HE, V20, DOI 10.3390/ijerph20021400
   Zhu XL, 2020, VISUAL COMPUT, V36, P743, DOI 10.1007/s00371-019-01660-3
NR 48
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 16
PY 2024
DI 10.1007/s00371-024-03369-4
EA APR 2024
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW0H5
UT WOS:001203370000001
DA 2024-08-05
ER

PT J
AU Yang, WJ
   Xie, LP
   Qian, WB
   Wu, CH
   Yang, HY
AF Yang, Wenji
   Xie, Liping
   Qian, Wenbin
   Wu, Canghai
   Yang, Hongyun
TI Coarse-to-fine cascaded 3D hand reconstruction based on SSGC and MHSA
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Multi-head self-attention; GCN; 3D hand reconstruction
AB Recently, graph convolution networks have become the mainstream methods in 3D hand pose and mesh estimation, but there are still some issues hindering its further development. First, the way that previous researchers alleviated small receptive field of vanilla graph convolution by simply stacking multiple GCN layers might lead to over-smoothness of features, thereby misleading the hand pose estimation. Second, most attempts directly reconstructed hand mesh from 3D pose in one step, which ignored the significant gap between sparse pose and dense mesh, resulting in incorrect results and unstable training. To solve these issues, a novel framework integrating multi-head self-attention, spatial-based graph convolution and spectral-based graph convolution for 3D hand pose and mesh estimation is proposed. The proposed framework comprises of two main modules: SemGraAttention and ChebGconv blocks. The SemGraAttention enables all hand joints to interact in global field without weakening the topologies of hand. As a complementary, the ChebGconv formulates implicit semantic relations among joints to further boost performance. In addition, a coarse-to-fine strategy is adopted to reconstruct dense hand mesh from sparse pose step by step, which contributes to refined results and stable training. The extensive evaluations on multiple 3D benchmarks demonstrate that our model outperforms a series of 3D hand pose and mesh estimation approaches.
C1 [Yang, Wenji; Qian, Wenbin; Wu, Canghai; Yang, Hongyun] Jiangxi Agr Univ, Sch Software, Nanchang 330045, Peoples R China.
   [Xie, Liping] Jiangxi Agr Univ, Sch Comp & Informat Engn, Nanchang 330045, Peoples R China.
C3 Jiangxi Agricultural University; Jiangxi Agricultural University
RP Yang, WJ (corresponding author), Jiangxi Agr Univ, Sch Software, Nanchang 330045, Peoples R China.
EM ywenji614@jxau.edu.cn
FU Natural Science Foundation of Jiangxi Province
FX No Statement Available
CR Aboukhadra AT, 2023, IEEE WINT CONF APPL, P1001, DOI 10.1109/WACV56688.2023.00106
   Baek S, 2019, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2019.00116
   Boukhayma A, 2019, PROC CVPR IEEE, P10835, DOI 10.1109/CVPR.2019.01110
   Chen L., 2019, BMVC
   Chen P, 2021, P IEEE CVF INT C COM, P12929
   Chen XY, 2022, PROC CVPR IEEE, P20512, DOI 10.1109/CVPR52688.2022.01989
   Chen YJ, 2021, PROC CVPR IEEE, P10446, DOI 10.1109/CVPR46437.2021.01031
   Choi Hongsuk, 2020, COMPUTER VISION ECCV
   Doosti B, 2020, PROC CVPR IEEE, P6607, DOI 10.1109/CVPR42600.2020.00664
   Ge LH, 2019, PROC CVPR IEEE, P10825, DOI 10.1109/CVPR.2019.01109
   Güler RA, 2019, PROC CVPR IEEE, P10876, DOI 10.1109/CVPR.2019.01114
   Gyeongsik Moon, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P752, DOI 10.1007/978-3-030-58571-6_44
   Han SC, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555378
   Han SC, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392452
   Hasson Y, 2019, PROC CVPR IEEE, P11799, DOI 10.1109/CVPR.2019.01208
   Iqbal U, 2018, LECT NOTES COMPUT SC, V11215, P125, DOI 10.1007/978-3-030-01252-6_8
   Irmak EC, 2020, VISUAL COMPUT, V36, P5, DOI 10.1007/s00371-018-1597-4
   Karunratanakul K, 2023, PROC CVPR IEEE, P12802, DOI 10.1109/CVPR52729.2023.01231
   Kulon D, 2020, PROC CVPR IEEE, P4989, DOI 10.1109/CVPR42600.2020.00504
   Li GQ, 2021, VISUAL COMPUT, V37, P2699, DOI 10.1007/s00371-021-02250-y
   Li J, 2023, VISUAL COMPUT, V39, P2065, DOI 10.1007/s00371-022-02465-7
   Li MC, 2022, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR52688.2022.00278
   Lin FQ, 2021, IEEE WINT CONF APPL, P2372, DOI 10.1109/WACV48630.2021.00242
   Liu SH, 2019, PROC CVPR IEEE, P10298, DOI 10.1109/CVPR.2019.01055
   Liu SW, 2021, PROC CVPR IEEE, P14682, DOI 10.1109/CVPR46437.2021.01445
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luan T, 2023, PROC CVPR IEEE, P16795, DOI 10.1109/CVPR52729.2023.01611
   Mahmud H, 2024, VISUAL COMPUT, V40, P11, DOI 10.1007/s00371-022-02762-1
   Park J, 2022, PROC CVPR IEEE, P1486, DOI 10.1109/CVPR52688.2022.00155
   Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192
   Wang Y, 2023, VISUAL COMPUT, V39, P1485, DOI 10.1007/s00371-022-02424-2
   Xiang DL, 2019, PROC CVPR IEEE, P10957, DOI 10.1109/CVPR.2019.01122
   Yadav KS, 2024, VISUAL COMPUT, V40, P1145, DOI 10.1007/s00371-023-02837-7
   Yang LL, 2019, PROC CVPR IEEE, P9869, DOI 10.1109/CVPR.2019.01011
   Chang JY, 2020, Arxiv, DOI arXiv:1910.12029
   Yu ZD, 2023, PROC CVPR IEEE, P12955, DOI 10.1109/CVPR52729.2023.01245
   Zeng W, 2020, PROC CVPR IEEE, P7052, DOI 10.1109/CVPR42600.2020.00708
   Zhang JW, 2017, IEEE IMAGE PROC, P982, DOI 10.1109/ICIP.2017.8296428
   Zhao L, 2019, PROC CVPR IEEE, P3420, DOI 10.1109/CVPR.2019.00354
   Zhao W., 2021, arXiv
   Zimmermann C, 2019, IEEE I CONF COMP VIS, P813, DOI 10.1109/ICCV.2019.00090
   Zimmermann C, 2017, IEEE I CONF COMP VIS, P4913, DOI 10.1109/ICCV.2017.525
NR 42
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 8
PY 2024
DI 10.1007/s00371-024-03305-6
EA APR 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ND1M4
UT WOS:001198420700003
DA 2024-08-05
ER

PT J
AU Xiong, J
   Wu, J
   Tang, M
   Xiong, PW
   Huang, YS
   Guo, H
AF Xiong, Jian
   Wu, Jie
   Tang, Ming
   Xiong, Pengwen
   Huang, Yushui
   Guo, Hang
TI Combining YOLO and background subtraction for small dynamic target
   detection
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Target detection; Background subtraction; Computer vision; YOLO
AB YOLO, an important algorithm for target detection, is ineffective in detecting small dynamic targets. In this paper, we utilize background subtraction, which is highly sensitive to dynamic pixels, to provide YOLO with the location and features of small dynamic targets, thus reducing the missed detection rate of small targets. This method uses background subtraction and YOLO to obtain the mask and class of the target, respectively. If the target's mask and class can be detected, the features of YOLO and Masks data module are constructed or updated using its characteristics and class. Conversely, if only the target mask is obtained, the target mask is introduced into the features of YOLO and Masks data module for similarity detection, so as to determine the target class. Finally, YOLO performs the forced detection of the target based on the coordinates of the mask with the determined class. Validated with the SBMnet dataset, the experimental results show that for dynamic targets with three different line-of-sight distances, the method proposed in this paper improves the precision by 2.3%, recall by 3.5%, and F1-score by 3.1%.
C1 [Xiong, Jian; Tang, Ming; Xiong, Pengwen] Nanchang Univ, Sch Adv Mfg, Nanchang, Peoples R China.
   [Wu, Jie; Huang, Yushui; Guo, Hang] Nanchang Univ, Sch Informat Engn, Nanchang, Peoples R China.
C3 Nanchang University; Nanchang University
RP Tang, M (corresponding author), Nanchang Univ, Sch Adv Mfg, Nanchang, Peoples R China.
EM xiongjian@ncu.edu.cn; 1113086059@qq.com; foyongkang@ncu.edu.cn;
   steven.xpw@ncu.edu.cn; huangyushui@ncu.edu.cn; guo1_2002@hotmail.com
RI Lei, Ming/JAD-1050-2023
OI , xiong jian/0000-0002-8516-1035
FU National Natural Science Foundation of China
FX No Statement Available
CR Bai YC, 2018, LECT NOTES COMPUT SC, V11217, P210, DOI 10.1007/978-3-030-01261-8_13
   Betti A, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23041865
   Chen G, 2022, IEEE T SYST MAN CY-S, V52, P936, DOI 10.1109/TSMC.2020.3005231
   Gkioxari G, 2019, IEEE I CONF COMP VIS, P9784, DOI 10.1109/ICCV.2019.00988
   Hu XL, 2021, COMPUT ELECTRON AGR, V185, DOI 10.1016/j.compag.2021.106135
   Hui Wang, 2020, 2020 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS). Proceedings, P881, DOI 10.1109/ICITBS49701.2020.00194
   Ji SJ, 2023, COMPUT ELECTR ENG, V105, DOI 10.1016/j.compeleceng.2022.108490
   Jiang JH, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13101909
   Jodoin PM, 2017, IEEE T IMAGE PROCESS, V26, P5244, DOI 10.1109/TIP.2017.2728181
   Junos MH, 2022, VISUAL COMPUT, V38, P2341, DOI 10.1007/s00371-021-02116-3
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Liang ZW, 2018, LECT NOTES COMPUT SC, V11166, P554, DOI 10.1007/978-3-030-00764-5_51
   Lin T.-Y., 2017, PROC CVPR IEEE, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Y, 2021, EXPERT SYST APPL, V172, DOI 10.1016/j.eswa.2021.114602
   Lu XC, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3052575
   Lu X, 2019, PROC CVPR IEEE, P7355, DOI 10.1109/CVPR.2019.00754
   Ma HY, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15092331
   Mansour RF, 2021, IMAGE VISION COMPUT, V112, DOI 10.1016/j.imavis.2021.104229
   Neubeck A, 2006, INT C PATT RECOG, P850, DOI 10.1109/icpr.2006.479
   Romano Y, 2017, IEEE T COMPUT IMAG, V3, P110, DOI 10.1109/TCI.2016.2629284
   Roy AM, 2022, COMPUT ELECTRON AGR, V193, DOI 10.1016/j.compag.2022.106694
   Shan MM, 2022, 2022 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, COMPUTER VISION AND MACHINE LEARNING (ICICML), P501, DOI 10.1109/ICICML57342.2022.10009877
   Wang SH, 2023, IEEE ACCESS, V11, P57951, DOI 10.1109/ACCESS.2023.3284062
   Wang XL, 2020, IEEE ACCESS, V8, P110227, DOI 10.1109/ACCESS.2020.3001279
   Wang ZZ, 2021, IEEE ACCESS, V9, P56416, DOI 10.1109/ACCESS.2021.3072211
   Yu HF, 2023, APPL INTELL, V53, P2434, DOI 10.1007/s10489-022-03622-0
   Zakria, 2022, IEEE J-STARS, V15, P1039, DOI 10.1109/JSTARS.2022.3140776
   Zhang MJ, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3263848
   Zhang MJ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1730, DOI 10.1145/3503161.3547817
   Zhang MJ, 2022, PROC CVPR IEEE, P867, DOI 10.1109/CVPR52688.2022.00095
   Zhang MJ, 2023, IEEE T CYBERNETICS, V53, P578, DOI 10.1109/TCYB.2022.3163294
   Zhang MJ, 2020, IEEE T NEUR NET LEAR, V31, P2623, DOI 10.1109/TNNLS.2019.2933590
   Zhang MJ, 2019, IEEE T NEUR NET LEAR, V30, P3109, DOI 10.1109/TNNLS.2018.2890017
   Zhang R, 2022, ADV THEOR SIMUL, V5, DOI 10.1002/adts.202100631
   Zhao L, 2022, SUSTAINABILITY-BASEL, V14, DOI 10.3390/su14094930
NR 37
TC 0
Z9 0
U1 73
U2 73
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 21
PY 2024
DI 10.1007/s00371-024-03342-1
EA MAR 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LX3Y6
UT WOS:001190084400001
DA 2024-08-05
ER

PT J
AU Li, Z
   Lv, H
   Cheng, LB
   Jia, XN
AF Li, Zhe
   Lv, Hui
   Cheng, Libo
   Jia, Xiaoning
TI Image deblocking algorithm based on GC and SSR
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Structural sparse representation; Gaussian curvature; Image deblocking
ID GAUSSIAN CURVATURE; SPARSE REPRESENTATION; REGULARIZATION MODEL;
   RECONSTRUCTION
AB Block discrete cosine transform coding has been widely used in image and video compression standards. However, at low bit rate coding, the compressed image produces obvious block effects at the block boundaries, which seriously affect the image visualization. This paper combines Gaussian curvature regularization and structural sparse representation to remove the block artifacts appearing in the compressed images, while preserving sharp edges. More precisely, we use the internal structural sparse prior to remove the image noise, and apply the external structural sparse prior to prevent image overfitting. Meanwhile, we perform Gaussian curvature regularization constraint that blends image gradient information, in order to remove the detrimental structure of the compressed image. Concretely, we incorporate filtering technique into the alternating iteration method for handling the nonconvexity problem of the proposed model. Experimental results demonstrate that our algorithm achieves several state-of-the-art deblocking algorithms in terms of both objective and visual perception.
C1 [Li, Zhe; Lv, Hui; Cheng, Libo; Jia, Xiaoning] Changchun Univ Sci & Technol, Sch Math & Stat, Changchun, Peoples R China.
   [Li, Zhe; Lv, Hui; Cheng, Libo; Jia, Xiaoning] Changchun Univ Sci & Technol, Zhongshan Res Inst, Lab Remote Sensing Technol & Big Data Anal, Zhongshan 528437, Peoples R China.
C3 Changchun University of Science & Technology; Changchun University of
   Science & Technology
RP Li, Z (corresponding author), Changchun Univ Sci & Technol, Sch Math & Stat, Changchun, Peoples R China.; Li, Z (corresponding author), Changchun Univ Sci & Technol, Zhongshan Res Inst, Lab Remote Sensing Technol & Big Data Anal, Zhongshan 528437, Peoples R China.
EM zheli200809@163.com
FU National Nature Science Foundation of China
FX The authors would like to acknowledge the contributions of all the
   reviewers and thank them for their insightful comments on the early
   drafts of this paper.
CR Altantawy DA, 2020, VISUAL COMPUT, V36, P333, DOI 10.1007/s00371-018-1611-x
   Altantawy DA, 2020, NEUROCOMPUTING, V380, P321, DOI 10.1016/j.neucom.2019.08.074
   Alter F, 2005, J MATH IMAGING VIS, V23, P199, DOI 10.1007/s10851-005-6467-9
   Amiri SA, 2016, INT J ENG-IRAN, V29, P1684, DOI 10.5829/idosi.ije.2016.29.12c.07
   Arya AS, 2023, SIGNAL PROCESS, V213, DOI 10.1016/j.sigpro.2023.109191
   Brito-Loeza C, 2016, NUMER METH PART D E, V32, P1066, DOI 10.1002/num.22042
   Chu ML, 2015, MAGN RESON MED, V74, P1336, DOI 10.1002/mrm.25527
   Foi A, 2007, IEEE T IMAGE PROCESS, V16, P1395, DOI 10.1109/TIP.2007.891788
   Ge XY, 2022, SIGNAL PROCESS-IMAGE, V100, DOI 10.1016/j.image.2021.116531
   Gong YH, 2013, IEEE IMAGE PROC, P534, DOI 10.1109/ICIP.2013.6738110
   Hu J, 2020, SIGNAL PROCESS-IMAGE, V86, DOI 10.1016/j.image.2020.115874
   Huang J, 2011, COMPUT BIOL MED, V41, P195, DOI 10.1016/j.compbiomed.2011.01.009
   Jiang Q, 2023, IEEE T RADIAT PLASMA, V7, P494, DOI 10.1109/TRPMS.2023.3239520
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim J, 2009, IEEE T CONSUM ELECTR, V55, P933, DOI 10.1109/TCE.2009.5174477
   Lee KJ, 2002, MAGNET RESON MED, V47, P812, DOI 10.1002/mrm.10101
   Lee SH, 2005, IEEE T IMAGE PROCESS, V14, P904, DOI 10.1109/TIP.2005.849294
   Li T, 2018, IEEE T MULTIMEDIA, V20, P1305, DOI 10.1109/TMM.2017.2766889
   Liu H, 2022, SIAM J SCI COMPUT, V44, pA935, DOI 10.1137/21M143772X
   Liu SJ, 2023, OPTIK, V286, DOI 10.1016/j.ijleo.2023.171013
   Mahalakshmi V., 2011, Int. J. Mod. Eng. Res, V1, P261
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Nath V. K., 2012, 2012 3rd National Conference on Emerging Trends and Applications in Computer Science (NCETACS), P93, DOI 10.1109/NCETACS.2012.6203306
   Nath V. K., 2019, 2 INT C DATA ENG COM, V828, P231
   Shen MY, 1998, J VIS COMMUN IMAGE R, V9, P2, DOI 10.1006/jvci.1997.0378
   Suo SY., 2018, Sci. Technol. Eng, V18, P224
   Wang CX, 2023, IEEE T CIRC SYST VID, V33, P1624, DOI 10.1109/TCSVT.2022.3217586
   Wei D., 2013, IEEE INT C MULTIMEDI, P1
   Yeh CH, 2014, J VIS COMMUN IMAGE R, V25, P891, DOI 10.1016/j.jvcir.2014.02.012
   Yuan W, 2023, SIGNAL PROCESS, V206, DOI 10.1016/j.sigpro.2022.108926
   Zha ZY, 2023, IEEE T NEUR NET LEAR, V34, P7593, DOI 10.1109/TNNLS.2022.3144630
   Zha ZY, 2022, IEEE T CYBERNETICS, V52, P12440, DOI 10.1109/TCYB.2021.3084931
   Zha ZY, 2022, IEEE T NEUR NET LEAR, V33, P4451, DOI 10.1109/TNNLS.2021.3057439
   Zha ZY, 2020, IEEE T IMAGE PROCESS, V29, P8561, DOI 10.1109/TIP.2020.3015545
   Zha ZY, 2020, IEEE T IMAGE PROCESS, V29, P7735, DOI 10.1109/TIP.2020.3005515
   Zhai GT, 2008, IEEE T MULTIMEDIA, V10, P735, DOI 10.1109/TMM.2008.922849
   Zhang J, 2016, IEEE T IMAGE PROCESS, V25, P1246, DOI 10.1109/TIP.2016.2515985
   Zhao C, 2017, IEEE T CIRC SYST VID, V27, P2057, DOI 10.1109/TCSVT.2016.2580399
   Zhong QX, 2021, J MATH IMAGING VIS, V63, P30, DOI 10.1007/s10851-020-00992-3
   Zhou F, 2022, DIGIT SIGNAL PROCESS, V120, DOI 10.1016/j.dsp.2021.103270
   Zhu HQ, 2007, COMPUT BIOL MED, V37, P793, DOI 10.1016/j.compbiomed.2006.08.015
   Zhu W, 2012, SIAM J IMAGING SCI, V5, P1, DOI 10.1137/110822268
   Zhu WJ, 2013, IEEE GLOB CONF SIG, P1069, DOI 10.1109/GlobalSIP.2013.6737079
NR 43
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 13
PY 2024
DI 10.1007/s00371-024-03309-2
EA MAR 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KT9R8
UT WOS:001182339200002
DA 2024-08-05
ER

PT J
AU Feng, ZL
   Zhang, J
   Ran, XS
   Li, DL
   Zhang, CF
AF Feng, Ziliang
   Zhang, Ju
   Ran, Xusong
   Li, Donglu
   Zhang, Chengfang
TI Ghost-Unet: multi-stage network for image deblurring via lightweight
   subnet learning
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image deblurring; Multi-stage network; U-Net; Ghost module; Lightweight
   subnet; Neural network
AB Multi-stage networks function by applying the concept of cascading, which alleviates the difficulties of network structure optimization using the single-stage method. Image deblurring methods based on multi-stage networks have previously been proposed and provided satisfactory results. However, owing to an excessive reliance on stacked subnets and residual blocks, most existing image deblurring methods that employ multi-stage networks suffer from two drawbacks: a complicated network structure and insufficient image representation of the model. To avoid these constraints, a novel multi-stage network deblurring model is proposed. Lightweight subnets are embedded in each stage of the model to gradually learn input image characteristics, which facilitates process optimization. The Ghost module is introduced as the basic unit of a neural network to reduce the required number of calculations and parameters. A wavelet reconstruction module is also added to avoid loss of image details. Finally, a more comprehensive loss function is designed to improve the quality of the generated images. Experimental results obtained using the GoPro dataset show that the proposed deblurring model achieves satisfactory performance in terms of both subjective results and objective evaluation.
C1 [Feng, Ziliang; Zhang, Ju; Ran, Xusong; Li, Donglu] Sichuan Univ, Coll Comp Sci, Chengdu 610065, Peoples R China.
   [Zhang, Chengfang] Sichuan Police Coll, Intelligent Policing Key Lab Sichuan Prov, Luzhou 646000, Peoples R China.
C3 Sichuan University; Sichuan Police College
RP Zhang, CF (corresponding author), Sichuan Police Coll, Intelligent Policing Key Lab Sichuan Prov, Luzhou 646000, Peoples R China.
EM fengziliang@scu.edu.cn; 645779858@qq.com; 874030932@qq.com;
   3227908281@qq.com; chengfangzhang@scpolicec.edu.cn
RI zhang, chengfang/AAB-5298-2022
FU Sichuan Science and Technology Program
FX No Statement Available
CR [Anonymous], 2018, IET IMAGE PROCESSING
   [Anonymous], 2013, Image reconstruction based on sparse and redundant representation model: Local vs nonlocal
   Chakrabarti A, 2016, LECT NOTES COMPUT SC, V9907, P221, DOI 10.1007/978-3-319-46487-9_14
   Chen LY, 2021, IEEE COMPUT SOC CONF, P182, DOI 10.1109/CVPRW53098.2021.00027
   Dongwon Park, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P327, DOI 10.1007/978-3-030-58539-6_20
   Gao HY, 2019, PROC CVPR IEEE, P3843, DOI 10.1109/CVPR.2019.00397
   Gong D, 2017, PROC CVPR IEEE, P3806, DOI 10.1109/CVPR.2017.405
   Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165
   Jia Xia, 2020, Advances in 3D Image and Graphics Representation, Analysis, Computing and Information Technology. Methods and Algorithms. Proceedings of IC3DIT 2019. Smart Innovation, Systems and Technologies (SIST 179), P277, DOI 10.1007/978-981-15-3863-6_31
   Jiang Z., 2020, ARXIV
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kalsoom A, 2022, J SUPERCOMPUT, V78, P9668, DOI 10.1007/s11227-021-04266-6
   Kaufman A, 2020, PROC CVPR IEEE, P5810, DOI 10.1109/CVPR42600.2020.00585
   Köhler R, 2012, LECT NOTES COMPUT SC, V7578, P27, DOI 10.1007/978-3-642-33786-4_3
   Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897
   Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304
   Lai WS, 2016, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2016.188
   Liu C, 2023, Vis Comput
   Liu PJ, 2018, IEEE COMPUT SOC CONF, P886, DOI 10.1109/CVPRW.2018.00121
   Lu YC, 2023, MULTIMED TOOLS APPL, V82, P17055, DOI 10.1007/s11042-022-14116-z
   Min C, 2018, IEEE ACCESS, V6, P69242, DOI 10.1109/ACCESS.2018.2880279
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Nimisha TM, 2017, IEEE I CONF COMP VIS, P4762, DOI 10.1109/ICCV.2017.509
   Pan JS, 2020, PROC CVPR IEEE, P3040, DOI 10.1109/CVPR42600.2020.00311
   Pan JS, 2019, IEEE T PATTERN ANAL, V41, P1412, DOI 10.1109/TPAMI.2018.2832125
   Pan Yongming, 2022, Frontier Computing: Proceedings of FC 2021. Lecture Notes in Electrical Engineering (827), P612, DOI 10.1007/978-981-16-8052-6_76
   Rahman JU, 2020, COMMUN MATH STAT, V8, P203, DOI 10.1007/s40304-019-00198-z
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shen ZY, 2018, PROC CVPR IEEE, P8260, DOI 10.1109/CVPR.2018.00862
   Sim H, 2019, IEEE COMPUT SOC CONF, P2140, DOI 10.1109/CVPRW.2019.00267
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Slutsky M, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22186923
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Vasiljevic I, 2017, Arxiv, DOI arXiv:1611.05760
   Wang CY, 2021, FRACTALS, V29, DOI 10.1142/S0218348X21502431
   Xu L., 2014, Adv. Neural Inform. Process. Syst, V27, P1024
   Xu XY, 2018, IEEE T IMAGE PROCESS, V27, P194, DOI 10.1109/TIP.2017.2753658
   Yang PQ, 2023, VISUAL COMPUT, V39, P4279, DOI 10.1007/s00371-022-02590-3
   Ye M., 2020, IEEE Access, V22, P1
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zhang HG, 2019, PROC CVPR IEEE, P5971, DOI 10.1109/CVPR.2019.00613
   Zhang KH, 2022, INT J COMPUT VISION, V130, P2103, DOI 10.1007/s11263-022-01633-5
   Zhang KH, 2020, PROC CVPR IEEE, P2734, DOI 10.1109/CVPR42600.2020.00281
   Zhang Y, 2013, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2013.145
   Zhang ZC, 2023, VISUAL COMPUT, V39, P1375, DOI 10.1007/s00371-022-02415-3
   Zou WB, 2021, IEEE INT CONF COMP V, P1895, DOI 10.1109/ICCVW54120.2021.00216
NR 46
TC 0
Z9 0
U1 16
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 10
PY 2024
DI 10.1007/s00371-024-03315-4
EA MAR 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KF5N6
UT WOS:001178556600001
DA 2024-08-05
ER

PT J
AU Ji, J
   Gao, S
   Zhou, W
AF Ji, Jun
   Gao, Song
   Zhou, Wei
TI Transferable adversarial sample purification by expanding the
   purification space of diffusion models
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Deep neural networks; Adversarial samples; Adversarial defense;
   Diffusion model; Purification space
ID ROBUSTNESS
AB Deep neural networks (DNNs) have been demonstrated to be vulnerable to adversarial samples and many powerful defense methods have been proposed to enhance the adversarial robustness of DNNs. However, these defenses often require adding regularization terms to the loss function or augmenting the training data, which often involves modification of the target model and increases computational consumption. In this paper, we propose a novel adversarial defense approach that leverages the diffusion model with a large purification space to purify potential adversarial samples, and introduce two training strategies termed PSPG and PDPG to defend against different attacks. Our method preprocesses adversarial examples before they are inputted into the target model, and thus can provide protection for DNNs in the inference phase. It does not require modifications to the target model and can protect even deployed models. Extensive experiments on CIFAR-10 and ImageNet demonstrate that our method has good accuracy and transferability, it can provide protection effectively for different models in various defense scenarios. Our code is available at: https://github.com/YNU-JI/PDPG.
C1 [Ji, Jun; Gao, Song; Zhou, Wei] Yunnan Univ, Sch software, Kunming 650504, Peoples R China.
C3 Yunnan University
RP Gao, S (corresponding author), Yunnan Univ, Sch software, Kunming 650504, Peoples R China.
EM jijun1@stu.ynu.edu.cn; gaos@ynu.edu.cn; zwei@ynu.edu.cn
FU National Natural Science Foundation of China [62162067, 62101480];
   National Natural Science Foundation of China
FX This paper was supported by the National Natural Science Foundation of
   China under Grant 62162067 and 62101480.
CR Bansal A, 2022, Arxiv, DOI [arXiv:2208.09392, 10.48550/arXiv.2208.09392, 10.48550/ARXIV.2208.09392]
   Bhattad A, 2020, 8 INT C LEARNING REP
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chattopadhay A, 2018, IEEE WINT CONF APPL, P839, DOI 10.1109/WACV.2018.00097
   Chen JQ, 2023, Arxiv, DOI arXiv:2305.08192
   Chen N, 2021, 9 INT C LEARNING REP
   Chen ZH, 2023, IEEE T PATTERN ANAL, V45, P13489, DOI 10.1109/TPAMI.2023.3293885
   Cheng SI, 2023, IEEE WINT CONF APPL, P4043, DOI 10.1109/WACV56688.2023.00404
   Croce F, 2020, PR MACH LEARN RES, V119
   Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957
   Gao S, 2021, IEEE CONF COMPUT, DOI 10.1109/INFOCOMWKSHPS51825.2021.9484542
   Gokhale T, 2021, AAAI CONF ARTIF INTE, V35, P7574
   Goodfellow IJ, 2015, P 3 INT C LEARNING R
   Gu Shixiang, 2015, 3 INT C LEARNING REP
   Guo C., 2018, 6 INT C LEARNING REP
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hill M., 2021, 9 INT C LEARNING REP
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Dziugaite GK, 2016, Arxiv, DOI arXiv:1608.00853
   Kawar B., 2022, NEURIPS 2022 WORKSHO
   Kurakin A., 2016, ARXIV160702533
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Liao FZ, 2018, PROC CVPR IEEE, P1778, DOI 10.1109/CVPR.2018.00191
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Madry A, 2018, INT C LEARN REPR, DOI 10.48550/arXiv.1706.06083
   Metzen Jan Hendrik, 2017, 5 INT C LEARN REPR I
   Moosavi-Dezfooli SM, 2017, PROC CVPR IEEE, P86, DOI 10.1109/CVPR.2017.17
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Nichol A, 2021, PR MACH LEARN RES, V139
   Pang T., 2018, Adv. Neural Inf. Process. Syst, V31, P25
   Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41
   Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36
   Peebles W, 2023, IEEE I CONF COMP VIS, P4172, DOI 10.1109/ICCV51070.2023.00387
   Ros AS, 2018, AAAI CONF ARTIF INTE, P1660
   Saharia Chitwan, 2022, SIGGRAPH22 Conference Proceeding: Special Interest Group on Computer Graphics and Interactive Techniques Conference Proceedings, DOI 10.1145/3528233.3530757
   Saharia C, 2023, IEEE T PATTERN ANAL, V45, P4713, DOI 10.1109/TPAMI.2022.3204461
   Shamsabadi AS, 2020, PROC CVPR IEEE, P1148, DOI 10.1109/CVPR42600.2020.00123
   Sheng B, 2022, IEEE T CYBERNETICS, V52, P6662, DOI 10.1109/TCYB.2021.3079311
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song C., 2020, 8 INT C LEARNING REP
   Terzi M., P AAAI C ARTIFICIAL, V35, P2674, DOI [10.1609/AAAI.V35I3.16371, DOI 10.1609/AAAI.V35I3.16371]
   Tu ZZ, 2022, LECT NOTES COMPUT SC, V13684, P459, DOI 10.1007/978-3-031-20053-3_27
   Xie CH, 2019, PROC CVPR IEEE, P501, DOI 10.1109/CVPR.2019.00059
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Xu WL, 2018, 25TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2018), DOI 10.14722/ndss.2018.23198
   Ye NY, 2021, AAAI CONF ARTIF INTE, V35, P10691
   Yuan Shengming, 2022, ADV NEURAL INF PROCE, V35, P7546
   Zhao Z., 2020, 31 BRIT MACHINE VISI
   Zheng S, 2016, PROC CVPR IEEE, P4480, DOI 10.1109/CVPR.2016.485
   Zheng Z., 2018, Adv. Neural Inf. Process. Syst, V31, P25
NR 52
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 13
PY 2024
DI 10.1007/s00371-023-03253-7
EA FEB 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HO6Q9
UT WOS:001160490100002
DA 2024-08-05
ER

PT J
AU Mumtaz, A
   Sargano, AB
   Habib, Z
AF Mumtaz, Aqib
   Sargano, Allah Bux
   Habib, Zulfiqar
TI AnomalyNet: a spatiotemporal motion-aware CNN approach for detecting
   anomalies in real-world autonomous surveillance
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Real-world anomalies detection; Autonomous surveillance; Spatiotemporal
   and motion-aware deep learning; Dynamic frame-skipping; AnomalyNet
ID REPRESENTATIONS; LOCALIZATION; RECOGNITION; APPEARANCE; NETWORK; GRAPH
AB Anomaly detection has significant importance for the development of autonomous monitoring systems. Real-world anomalous events are complicated due to diverse human behaviors and class variations. Anomalous activities depend upon speed, length of activity, and motion features to comprehend suspicious behaviors. Fast activities are captured quickly within a few video frames, whereas slow actions may take several hundred video frames to define an anomalous action. Furthermore, a video is more than just a stack of frames with spatiotemporal representations. Most of the existing approaches suffer from learning variable speed fast and slow activities simultaneously and primarily focus on learning spatiotemporal features only. Modeling the spatiotemporal and motion relationships between frames together can help understand the actions better. Motion features when combined with spatiotemporal representations perform higher. Our contribution is two-fold in this research work. Firstly, a novel dynamic frame-skipping approach is proposed to duly generate meaningful representations of spatiotemporal frames and optical-flow-based motion representations for variable speed anomalous actions. Secondly, AnomalyNet, as a new end-to-end deep architecture, is designed to simultaneously learn both spatiotemporal and motion features in image sequences. AnomalyNet is evaluated on the challenging real-world anomaly detection datasets. The results confirm that the proposed model has achieved a competitive AUC of 86.1% on the real-world UCF-Crime dataset and has achieved a superior AUC score of 99.87% compared to state-of-the-art methods on challenging ShanghaiTech dataset in the domain of unsupervised, weakly-supervised, and fully-supervised anomaly detection. Furthermore, the model achieved the highest F1 score for both fast and slow variable speed anomalous activities, such as explosions, road accidents, robbery, and stealing for real-world autonomous surveillance.
C1 [Mumtaz, Aqib; Sargano, Allah Bux; Habib, Zulfiqar] COMSATS Univ Islamabad, Dept Comp Sci, Lahore 54000, Pakistan.
C3 COMSATS University Islamabad (CUI)
RP Habib, Z (corresponding author), COMSATS Univ Islamabad, Dept Comp Sci, Lahore 54000, Pakistan.
EM sp19-pcs-011@cuilahore.edu.pk; allahbux@cuilahore.edu.pk;
   drzhabib@cuilahore.edu.pk
RI ; Habib, Zulfiqar/B-6355-2013
OI Mumtaz, Aqib/0000-0002-1760-6346; Habib, Zulfiqar/0000-0001-9758-9162
FU Horizon 2020 Framework Programme
FX No Statement Available
CR Adam A, 2008, IEEE T PATTERN ANAL, V30, P555, DOI 10.1109/TPAMI.2007.70825
   Bai S., 2019, P CVPR WORKSH, P117
   Bailer C, 2015, IEEE I CONF COMP VIS, P4015, DOI 10.1109/ICCV.2015.457
   Bin Zhao, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3313, DOI 10.1109/CVPR.2011.5995524
   Cao C., 2022, PREPRINT
   Cao CQ, 2023, PROC CVPR IEEE, P20392, DOI 10.1109/CVPR52729.2023.01953
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chalapathy R, 2019, LECT NOTES ARTIF INT, V11051, P173, DOI 10.1007/978-3-030-10925-7_11
   Chang YP, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108213
   Chen YX, 2022, Arxiv, DOI arXiv:2211.15098
   Chidananda K., 2022, Information and Communication Technology for Competitive Strategies (ICTCS 2020): ICT: Applications and Social Interfaces. Lecture Notes in Networks and Systems (191), P791, DOI 10.1007/978-981-16-0739-4_75
   Chong YS, 2017, LECT NOTES COMPUT SC, V10262, P189, DOI 10.1007/978-3-319-59081-3_23
   Cong Y, 2011, PROC CVPR IEEE, P1807, DOI 10.1109/CVPR.2011.5995434
   Dhole H, 2019, INT CONF COMPUT
   Dong F, 2020, IEEE ACCESS, V8, P88170, DOI 10.1109/ACCESS.2020.2993373
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Dubey S, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11031344
   Ehsan TZ, 2024, VISUAL COMPUT, V40, P1515, DOI 10.1007/s00371-023-02865-3
   Gianchandani Urvi., 2019, Weakly-supervised spatiotemporal anomaly detection
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gong D, 2019, IEEE I CONF COMP VIS, P1705, DOI 10.1109/ICCV.2019.00179
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Hao WL, 2020, SECUR COMMUN NETW, V2020, DOI 10.1155/2020/8876056
   Hasan M, 2016, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2016.86
   He CK, 2018, MULTIMED TOOLS APPL, V77, P29573, DOI 10.1007/s11042-017-5255-z
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinami R, 2017, IEEE I CONF COMP VIS, P3639, DOI 10.1109/ICCV.2017.391
   Hou R, 2017, IEEE I CONF COMP VIS, P5823, DOI 10.1109/ICCV.2017.620
   Colque RVHM, 2017, IEEE T CIRC SYST VID, V27, P673, DOI 10.1109/TCSVT.2016.2637778
   Hyunjong Park, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14360, DOI 10.1109/CVPR42600.2020.01438
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jaechul Kim, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2921, DOI 10.1109/CVPRW.2009.5206569
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Koniusz P, 2022, IEEE T PATTERN ANAL, V44, P648, DOI 10.1109/TPAMI.2021.3107160
   Kratz L, 2009, PROC CVPR IEEE, P1446, DOI 10.1109/CVPRW.2009.5206771
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Li WX, 2014, IEEE T PATTERN ANAL, V36, P18, DOI 10.1109/TPAMI.2013.111
   Ling CX, 2003, LECT NOTES ARTIF INT, V2671, P329
   Liu W, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3023
   Liu W, 2018, PROC CVPR IEEE, P6536, DOI 10.1109/CVPR.2018.00684
   Liu Y, 2023, IEEE T PATTERN ANAL, V45, P11624, DOI 10.1109/TPAMI.2023.3284038
   Liu Y, 2022, MACH INTELL RES, V19, P485, DOI 10.1007/s11633-022-1362-z
   Liu Y, 2022, IEEE T CIRCUITS-II, V69, P2498, DOI 10.1109/TCSII.2022.3161049
   Liu Y, 2022, IEEE T IMAGE PROCESS, V31, P1978, DOI 10.1109/TIP.2022.3147032
   Liu Y, 2020, IEEE T IMAGE PROCESS, V29, P3168, DOI 10.1109/TIP.2019.2957930
   Liu Y, 2019, IEEE T CIRC SYST VID, V29, P2416, DOI 10.1109/TCSVT.2018.2868123
   Liu ZA, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13568, DOI 10.1109/ICCV48922.2021.01333
   Lu CW, 2013, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2013.338
   Luo WX, 2017, IEEE I CONF COMP VIS, P341, DOI 10.1109/ICCV.2017.45
   Luo WX, 2017, IEEE INT CON MULTI, P439, DOI 10.1109/ICME.2017.8019325
   Majhi S, 2021, IEEE INT CONF AUTOMA, DOI 10.1109/FG52635.2021.9667006
   Maqsood R, 2021, MULTIMED TOOLS APPL, V80, P18693, DOI 10.1007/s11042-021-10570-3
   Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641
   Morais R, 2019, PROC CVPR IEEE, P11988, DOI 10.1109/CVPR.2019.01227
   Mumtaz A, 2022, COMPUT J, V65, DOI 10.1093/comjnl/bxaa061
   Mumtaz A, 2018, 2018 2ND EUROPEAN CONFERENCE ON ELECTRICAL ENGINEERING AND COMPUTER SCIENCE (EECS 2018), P558, DOI 10.1109/EECS.2018.00109
   Narasimhan MG, 2018, MULTIMED TOOLS APPL, V77, P13173, DOI 10.1007/s11042-017-4940-2
   Nayak R, 2021, IMAGE VISION COMPUT, V106, DOI 10.1016/j.imavis.2020.104078
   Qin ZY, 2024, IEEE T NEUR NET LEAR, V35, P4783, DOI 10.1109/TNNLS.2022.3201518
   Ramachandra B, 2022, IEEE T PATTERN ANAL, V44, P2293, DOI 10.1109/TPAMI.2020.3040591
   Ramachandra B, 2020, IEEE WINT CONF APPL, P2558, DOI [10.1109/wacv45572.2020.9093457, 10.1109/WACV45572.2020.9093457]
   Ravanbakhsh M, 2018, IEEE WINT CONF APPL, P1689, DOI 10.1109/WACV.2018.00188
   Ravanbakhsh M, 2017, IEEE IMAGE PROC, P1577, DOI 10.1109/ICIP.2017.8296547
   Sabokrou M, 2018, COMPUT VIS IMAGE UND, V172, P88, DOI 10.1016/j.cviu.2018.02.006
   Sabokrou M, 2017, IEEE T IMAGE PROCESS, V26, P1992, DOI 10.1109/TIP.2017.2670780
   Saligrama V, 2010, IEEE SIGNAL PROC MAG, V27, P18, DOI 10.1109/MSP.2010.937393
   Sargano AB, 2017, IEEE IJCNN, P463, DOI 10.1109/IJCNN.2017.7965890
   Sargano AB, 2017, APPL SCI-BASEL, V7, DOI 10.3390/app7010110
   Sargano AB, 2016, APPL SCI-BASEL, V6, DOI 10.3390/app6100309
   Shah A. P., 2019, IEEE INT C ADV VIDEO, P1
   Shao J, 2016, PROC CVPR IEEE, P5620, DOI 10.1109/CVPR.2016.606
   Shen L., 2020, Advances in Neural Information Processing Systems, V33, P13016
   Shreyas D. G., 2020, SN Computer Science, V1, P1, DOI DOI 10.1007/S42979-020-00169-0
   Simonyan K., 2014, C TRACK P
   Simonyan K, 2014, ADV NEUR IN, V27
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Sultani W, 2018, PROC CVPR IEEE, P6479, DOI 10.1109/CVPR.2018.00678
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Tang Y, 2020, PATTERN RECOGN LETT, V129, P123, DOI 10.1016/j.patrec.2019.11.024
   Ullah H, 2014, LECT NOTES COMPUT SC, V8749, P62, DOI 10.1007/978-3-319-11839-0_6
   Ullah W, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21082811
   Ullah W, 2021, MULTIMED TOOLS APPL, V80, P16979, DOI 10.1007/s11042-020-09406-3
   Vijay TK, 2022, EXPERT SYST APPL, V201, DOI 10.1016/j.eswa.2022.117030
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Vu TH, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21093179
   Wan BY, 2021, IET IMAGE PROCESS, V15, P3454, DOI 10.1049/ipr2.12258
   Wang G., 2019, P CVPR WORKSH, P382
   Wang L, 2023, PROC CVPR IEEE, P5620, DOI 10.1109/CVPR52729.2023.00544
   Wang L, 2023, LECT NOTES COMPUT SC, V13844, P307, DOI 10.1007/978-3-031-26316-3_19
   Wang L, 2022, LECT NOTES COMPUT SC, V13681, P176, DOI 10.1007/978-3-031-19803-8_11
   Wang L, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4324, DOI 10.1145/3474085.3475572
   Wang L, 2019, IEEE I CONF COMP VIS, P8697, DOI 10.1109/ICCV.2019.00879
   Wang L, 2019, IEEE IMAGE PROC, P974, DOI [10.1109/ICIP.2019.8803051, 10.1109/icip.2019.8803051]
   Xu D., 2015, ARXIV151001553, V8, P12
   Xu D, 2017, COMPUT VIS IMAGE UND, V156, P117, DOI 10.1016/j.cviu.2016.10.010
   Yang M, 2024, VISUAL COMPUT, V40, P303, DOI 10.1007/s00371-023-02783-4
   Ye MC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1805, DOI 10.1145/3343031.3350899
   [袁非牛 Yuan Feiniu], 2019, [计算机学报, Chinese Journal of Computers], V42, P203
   Yunpeng Chang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P329, DOI 10.1007/978-3-030-58555-6_20
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zaheer M. Z., 2021, Preprint at arXiv e-prints, P3
   Zhao YR, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1933, DOI 10.1145/3123266.3123451
   Zhong JX, 2019, PROC CVPR IEEE, P1237, DOI 10.1109/CVPR.2019.00133
   Zhu SJ, 2020, Arxiv, DOI arXiv:2004.00222
   Zhu XB, 2012, IEEE IMAGE PROC, P2705, DOI 10.1109/ICIP.2012.6467457
   Zhu Y., 2019, 30 BR MACH VIS C 201
   Zhu YY, 2023, IEEE T IND INFORM, V19, P1248, DOI 10.1109/TII.2022.3179243
NR 109
TC 0
Z9 0
U1 6
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 2
PY 2024
DI 10.1007/s00371-023-03210-4
EA JAN 2024
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DN8R7
UT WOS:001132825200002
DA 2024-08-05
ER

PT J
AU Flor, A
   Aanjaneya, M
AF Flor, Alon
   Aanjaneya, Mridul
TI Spectral reordering for faster elasticity simulations
SO VISUAL COMPUTER
LA English
DT Article
DE Fiedler vector; Elasticity; Multigrid solver; Physics simulation;
   Laplacian; Reordering
AB We present a novel method for faster physics simulations of elastic solids. Our key idea is to reorder the unknown variables according to the Fiedler vector (i.e., the second-smallest eigenvector) of the combinatorial Laplacian. It is well known in the geometry processing community that the Fiedler vector brings together vertices that are geometrically nearby, causing fewer cache misses when computing differential operators. However, to the best of our knowledge, this idea has not been exploited to accelerate simulations of elastic solids which require an expensive linear (or non-linear) system solve at every time step. The cost of computing the Fiedler vector is negligible, thanks to an algebraic Multigrid-preconditioned Conjugate Gradients (AMGPCG) solver. We observe that our AMGPCG solver requires approximately 1 s for computing the Fiedler vector for a mesh with approximately 50K vertices or 100K tetrahedra. Our method provides a speed-up between 10 % \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$10\%$$\end{document} - 30 % \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$30\%$$\end{document} at every time step, which can lead to considerable savings, noting that even modest simulations of elastic solids require at least 240 time steps. Our method is easy to implement and can be used as a plugin for speeding up existing physics simulators for elastic solids, as we demonstrate through our experiments using the Vega library and the ADMM solver, which use different algorithms for elasticity.
C1 [Flor, Alon; Aanjaneya, Mridul] Rutgers State Univ, Dept Comp Sci, Piscataway, NJ 08854 USA.
C3 Rutgers University System; Rutgers University New Brunswick
RP Aanjaneya, M (corresponding author), Rutgers State Univ, Dept Comp Sci, Piscataway, NJ 08854 USA.
EM af656@cs.rutgers.edu; mridul.aanjaneya@rutgers.edu
FU National Science Foundation [CCF-2110861, IIS-2132972, IIS-2238955,
   CCF-2312220]; National Science Foundation
FX A. F. and M. A. were supported in part by the National Science
   Foundation under awards CCF-2110861, IIS-2132972, IIS-2238955, and
   CCF-2312220 as well as a research gift from Red Hat, Inc. Any opinions,
   findings and conclusions, or recommendations expressed in this material
   are those of the authors and do not necessarily reflect the views of the
   National Science Foundation.
CR Aanjaneya M, 2019, P ACM COMPUT GRAPH, V2, DOI 10.1145/3340255
   Aanjaneya M, 2018, COMPUT GRAPH FORUM, V37, P59, DOI 10.1111/cgf.13512
   Alpert CJ, 1999, DISCRETE APPL MATH, V90, P3, DOI 10.1016/S0166-218X(98)00083-3
   Alwaely B, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3554922
   Ando R, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461982
   Barbic J., 2012, US
   BARNARD ST, 1993, SUPERCOMP PROC, P493
   Bell N., 2023, Journal of Open Source Software, V8, P5495, DOI 10.21105/joss.05495
   Bouaziz S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601116
   Bridson R., 2015, FLUID SIMULATION COM, DOI [10.1201/9781315266008, DOI 10.1201/9781315266008]
   Chu JY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3092818
   Clausen P, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2451236.2451243
   De Goes F, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530060
   deGoes F., 2020, ACM SIGGRAPH 2020 TA
   Demaine E.D., 2002, Lecture Notes from the EEF Summer School on Massive Data Sets, P1
   Díaz J, 2002, ACM COMPUT SURV, V34, P313, DOI 10.1145/568522.568523
   Duersch JA, 2018, SIAM J SCI COMPUT, V40, pC655, DOI 10.1137/17M1129830
   FIEDLER M, 1975, CZECH MATH J, V25, P607
   FIEDLER M, 1973, CZECH MATH J, V23, P298
   Frigo M., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P285, DOI 10.1109/SFFCS.1999.814600
   Geiger W., 2006, ACM SIGGRAPH 2006 Sketches
   Gopi M, 2004, COMPUT GRAPH FORUM, V23, P371, DOI 10.1111/j.1467-8659.2004.00768.x
   Hecht F, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2231816.2231821
   HERHOLZ P, 2020, ACM T GRAPHIC, V39, P1
   Hoppe H, 1999, COMP GRAPH, P269, DOI 10.1145/311535.311565
   Isenburg M, 2005, IEEE Visualization 2005, Proceedings, P231
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Koren Y, 2002, INFOVIS 2002: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2002, P137, DOI 10.1109/INFVIS.2002.1173159
   Koren Y, 2003, LECT NOTES COMPUT SC, V2697, P496
   Lai ZQ, 2010, LECT NOTES COMPUT SC, V6361, P332
   Levy B, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P66
   Li LF, 2021, CHINESE J ELECTRON, V30, P426, DOI 10.1049/cje.2020.11.001
   Liu HX, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982430
   LIU T, 2017, ACM T GRAPHIC, V36, P1, DOI DOI 10.1145/3072959.2990496
   Museth K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487235
   Narain M., 2016, P ACM SIGGRAPH EUR S, P21
   Nguyen D., 2023, ACM SIGGRAPH 2023 TA
   Overby M, 2017, IEEE T VIS COMPUT GR, V23, P2222, DOI 10.1109/TVCG.2017.2730875
   Peng R., 2013, ALGORITHM DESIGN USI
   Rasmussen N., 2004, ACM SIGGRAPH/ Eurographics Symp. Comp. Anim, P193, DOI DOI 10.1145/1028523.1028549
   Setaluri Rajsekhar, 2014, ACM Transactions on Graphics, V33, DOI 10.1145/2661229.2661269
   Shek A., 2010, ACM SIGGRAPH 2010 TA
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Si H, 2015, ACM T MATH SOFTWARE, V41, DOI 10.1145/2629697
   Sifakis E, 2012, ACM SIGGRAPH 2012 CO, P1, DOI 10.1145/2343483.2343501
   Spielman DA, 1996, AN S FDN CO, P96, DOI 10.1109/SFCS.1996.548468
   Stomakhin A, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461948
   Sueda S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964934
   Taubin Gabriel, 1995, P 22 ANN C COMP GRAP, P351, DOI DOI 10.1145/218380.218473
   Tong WH, 2020, IEEE T VIS COMPUT GR, V26, P1807, DOI 10.1109/TVCG.2018.2882212
   van Leuken R.H., STRUCTURAL SYNTACTIC, P167
   Yoon SE, 2005, ACM T GRAPHIC, V24, P886, DOI 10.1145/1073204.1073278
   Yoon SE, 2006, COMPUT GRAPH FORUM, V25, P507, DOI 10.1111/j.1467-8659.2006.00970.x
   Yu C, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555430
   Zhang H, 2010, COMPUT GRAPH FORUM, V29, P1865, DOI 10.1111/j.1467-8659.2010.01655.x
   Zhang XX, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661261
   Zhu YN, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731054
NR 57
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 5067
EP 5077
DI 10.1007/s00371-024-03513-0
EA JUN 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001249543600006
OA hybrid
DA 2024-08-05
ER

PT J
AU Li, T
   Zhang, ZX
   Wang, YX
   Cui, Y
   Li, YQ
   Zhou, DS
   Yin, BC
   Yang, X
AF Li, Tong
   Zhang, Zhaoxuan
   Wang, Yuxin
   Cui, Yan
   Li, Yuqi
   Zhou, Dongsheng
   Yin, Baocai
   Yang, Xin
TI Self-supervised indoor scene point cloud completion from a single
   panorama
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Self-supervised learning; Panorama; Indoor scene; Point cloud completion
AB In this paper, we propose a self-supervised learning method of point cloud completion for indoor scenes. Considering the limited view of single-view image and the time-consuming and labor-intensive acquisition of multi-view images, we take panoramas as input, which makes the acquisition easier and the scope of the scene wider. As it is difficult to obtain complete scene point cloud, we design an auxiliary task to simulate scene missing area by shifting viewpoint of panorama and extract the supervision information of the scene itself. Given the difficulty to complete large-scale scene point cloud, we design a neighborhood integration and feature spreading module for feature extraction and reservation before substantial point cloud downsampling, enabling the completion network to handle large-scale point cloud. Then we propose a transformer-based scene point cloud completion network and show competitive completion results compared to relevant supervised learning methods.
C1 [Li, Tong; Zhang, Zhaoxuan; Wang, Yuxin; Yin, Baocai; Yang, Xin] Dalian Univ Technol, Sch Comp Sci & Technol, Dalian, Liaoning, Peoples R China.
   [Cui, Yan] Zhuhai 4DAGE Technol Co Ltd, Zhuhai, Guangdong, Peoples R China.
   [Li, Yuqi] Ningbo Univ, Ningbo, Zhejiang, Peoples R China.
   [Zhou, Dongsheng] Dalian Univ, Dalian, Liaoning, Peoples R China.
C3 Dalian University of Technology; Ningbo University; Dalian University
RP Yang, X (corresponding author), Dalian Univ Technol, Sch Comp Sci & Technol, Dalian, Liaoning, Peoples R China.
EM xinyang@dlut.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Agrawal S, 2019, IEEE WINT CONF APPL, P1099, DOI 10.1109/WACV.2019.00122
   Cao AQ, 2022, PROC CVPR IEEE, P3981, DOI 10.1109/CVPR52688.2022.00396
   Armeni I., 2017, arXiv
   Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Berger M., 2014, 35 ANN C EUR ASS COM
   Cai XX, 2024, FRONT COMPUT SCI-CHI, V18, DOI 10.1007/s11704-023-3541-7
   Chen SC, 2021, FRONT COMPUT SCI-CHI, V15, DOI 10.1007/s11704-021-1900-9
   Dai A, 2020, PROC CVPR IEEE, P846, DOI 10.1109/CVPR42600.2020.00093
   Dai A, 2018, PROC CVPR IEEE, P4578, DOI 10.1109/CVPR.2018.00481
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Nguyen DT, 2016, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2016.612
   Fei B, 2022, IEEE T INTELL TRANSP, V23, P22862, DOI 10.1109/TITS.2022.3195555
   Guo YX, 2018, Arxiv, DOI arXiv:1806.05361
   Han B, 2022, IMAGE VISION COMPUT, V118, DOI 10.1016/j.imavis.2021.104371
   Han XG, 2019, PROC CVPR IEEE, P234, DOI 10.1109/CVPR.2019.00032
   Hu Q., 2020, P IEEE CVF C COMP VI, P108
   Imambi S., 2021, Pytorch. Programming with TensorFlow: Solution for Edge Computing Applications, P87, DOI [10.1007/978-3-030-57077-4_10, DOI 10.1007/978-3-030-57077-4_10]
   Jiayuan Gu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P283, DOI 10.1007/978-3-030-58558-7_17
   Li DP, 2017, IEEE T VIS COMPUT GR, V23, P1809, DOI 10.1109/TVCG.2016.2553102
   Li YY, 2015, COMPUT GRAPH FORUM, V34, P435, DOI 10.1111/cgf.12573
   Li YY, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964947
   Li YM, 2023, PROC CVPR IEEE, P9087, DOI 10.1109/CVPR52729.2023.00877
   Liu SL, 2023, FRONT COMPUT SCI-CHI, V17, DOI 10.1007/s11704-022-2435-4
   Loshchilov I, 2019, Arxiv, DOI [arXiv:1711.05101, 10.48550/arXiv.1711.05101, DOI 10.48550/ARXIV.1711.05101]
   Luo F, 2023, VIS INFORM, V7, P66, DOI 10.1016/j.visinf.2022.12.002
   Martinovic A, 2013, PROC CVPR IEEE, P201, DOI 10.1109/CVPR.2013.33
   Mitra NJ, 2013, COMPUT GRAPH FORUM, V32, P1, DOI 10.1111/cgf.12010
   Mittal P, 2022, PROC CVPR IEEE, P306, DOI 10.1109/CVPR52688.2022.00040
   Nan LL, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778830
   Navaneet KL, 2020, PROC CVPR IEEE, P1129, DOI 10.1109/CVPR42600.2020.00121
   Qi CR, 2017, ADV NEUR IN, V30
   Qi L, 2023, FRONT COMPUT SCI-CHI, V17, DOI 10.1007/s11704-022-0610-2
   Rock J, 2015, PROC CVPR IEEE, P2484, DOI 10.1109/CVPR.2015.7298863
   Sarkar K, 2017, INT CONF 3D VISION, P383, DOI 10.1109/3DV.2017.00051
   Schnabel R, 2009, COMPUT GRAPH FORUM, V28, P503, DOI 10.1111/j.1467-8659.2009.01389.x
   Shao TJ, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366155
   Shaw P, 2018, Arxiv, DOI [arXiv:1803.02155, 10.48550/arXiv.1803.02155]
   Shi HC, 2023, FRONT COMPUT SCI-CHI, V17, DOI 10.1007/s11704-022-2189-z
   Sipiran I, 2014, COMPUT GRAPH FORUM, V33, P131, DOI 10.1111/cgf.12481
   Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28
   Sun C, 2021, PROC CVPR IEEE, P11333, DOI 10.1109/CVPR46437.2021.01118
   Sung M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818094
   Tan ZH, 2022, FRONT COMPUT SCI-CHI, V16, DOI 10.1007/s11704-020-0298-0
   Taylor CJ, 2015, IEEE INT C INT ROBOT, P6265, DOI 10.1109/IROS.2015.7354271
   Theoharis T., 2001, A survey
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HQ, 2022, FRONT COMPUT SCI-CHI, V16, DOI 10.1007/s11704-020-9521-2
   Wang YD, 2019, IEEE I CONF COMP VIS, P8607, DOI 10.1109/ICCV.2019.00870
   Westover A.L., 1991, Splatting: a parallel, feed-forward volume rendering algorithm
   Wu HJ, 2023, VIS INFORM, V7, P59, DOI 10.1016/j.visinf.2023.06.007
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Wu ZZ, 2024, FRONT COMPUT SCI-CHI, V18, DOI 10.1007/s11704-023-2563-5
   Xu LY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2202, DOI 10.1145/3503161.3547926
   Xu MY, 2023, Arxiv, DOI arXiv:2212.09948
   Yan W, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2067, DOI 10.1145/3394171.3413648
   Yang H, 2016, PROC CVPR IEEE, P5422, DOI 10.1109/CVPR.2016.585
   Yang S, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3389412
   Yang Y, 2018, PROC CVPR IEEE, P3926, DOI 10.1109/CVPR.2018.00413
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yida Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P70, DOI 10.1007/978-3-030-58580-8_5
   Yin KX, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661241
   Yu XM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12478, DOI 10.1109/ICCV48922.2021.01227
   Yu YK, 2020, NEUROCOMPUTING, V384, P192, DOI 10.1016/j.neucom.2019.12.032
   Yuan MZ, 2024, FRONT COMPUT SCI-CHI, V18, DOI 10.1007/s11704-023-2471-8
   Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088
   Zeng W., 2020, BMVC
   Zhang PP, 2019, IEEE I CONF COMP VIS, P7800, DOI 10.1109/ICCV.2019.00789
   Zhang Y, 2022, ALGORITHMS, V15, DOI 10.3390/a15040124
   Zhang ZX, 2023, IEEE I CONF COMP VIS, P8862, DOI 10.1109/ICCV51070.2023.00817
   Zhang ZX, 2023, IEEE T PATTERN ANAL, V45, P11079, DOI 10.1109/TPAMI.2023.3264449
   Zhao WB, 2022, PROC CVPR IEEE, P1998, DOI 10.1109/CVPR52688.2022.00204
   Zhou HR, 2022, LECT NOTES COMPUT SC, V13663, P416, DOI 10.1007/978-3-031-20062-5_24
   Zhu Z., 2023, IEEE Transactions on Visualization and Computer Graphics
   Zitian Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7659, DOI 10.1109/CVPR42600.2020.00768
NR 74
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 17
PY 2024
DI 10.1007/s00371-024-03509-w
EA JUN 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UN1G1
UT WOS:001248641200005
DA 2024-08-05
ER

PT J
AU Mei, AK
   Huo, H
   Xu, JX
   Xu, NY
AF Mei, Aokun
   Huo, Hua
   Xu, Jiaxin
   Xu, Ningya
TI Multistage attention region supplement transformer for fine-grained
   visual categorization
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE FGVC; Vision Transformer; Multi-Stage Attention; Region Supplement
AB The classification of fine-grained images using computer technology employs neural network models to distinguish between instances of different classes that share very similar visual content. Thus, learning to extract nuanced representations of selected object details is essential. This paper introduces a novel fine-grained visual categorization model, the Multistage Attention Region Supplement Transformer (MARS-Trans), which is based on the Vision Transformer (ViT). Our main contributions are threefold. Firstly, we observed that in the ViT's multi-head attention module, the softmaxed feature results of each attention head are directly concatenated and multiplied by weights. Consequently, we propose a Multistage Attention Module (MAM) to grade the attention heads based on their importance. Additionally, we introduce a Region Supplement Module (RSM) to suppress non-critical regions and enhance edge information in key areas, thereby highlighting the discriminative features. Finally, we employ our proposed Approximate Adjust Method (AAM) to refine the final features and improve categorization results. We conducted comprehensive experiments with MARS-Trans on five popular public fine-grained image datasets, validating the effectiveness of these modules. Our model achieved state-of-the-art (SOTA) accuracy on one dataset and SOTA average precisions on three datasets. The code is available on https://github.com/ArrikenMei/MARS-Trans.
C1 [Mei, Aokun; Huo, Hua; Xu, Jiaxin; Xu, Ningya] Henan Univ Sci & Technol, Sch Informat Engn, Kaiyuan Ave, Luoyang 471023, Henan, Peoples R China.
C3 Henan University of Science & Technology
RP Huo, H (corresponding author), Henan Univ Sci & Technol, Sch Informat Engn, Kaiyuan Ave, Luoyang 471023, Henan, Peoples R China.
EM 18438696100@163.com; pacific_huo@126.com; xujiaxin0523@163.com;
   haust_xny@163.co
FU National Natural Science Foundation of China [61672210]; National
   Natural Science Foundation of China [221100210500]; Major Science and
   Technology Program of Henan Province [Z20221343032]; Central Government
   Guiding Local Science and Technology Development Fund Program of Henan
   Province
FX This work was partially supported by the National Natural Science
   Foundation of China (61672210), the Major Science and Technology Program
   of Henan Province (221100210500) and the Central Government Guiding
   Local Science and Technology Development Fund Program of Henan Province
   (Z20221343032).
CR Bera A, 2022, IEEE T IMAGE PROCESS, V31, P6017, DOI 10.1109/TIP.2022.3205215
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Cheng D, 2015, IEEE INT C SEMANT CO, P32, DOI 10.1109/ICOSC.2015.7050775
   Chou PY, 2023, Arxiv, DOI arXiv:2303.06442
   Chu XX, 2021, ADV NEUR IN
   Cui Y, 2017, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2017.325
   Ding Y, 2019, IEEE I CONF COMP VIS, P6598, DOI 10.1109/ICCV.2019.00670
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Dubey A, 2018, Adv. Neural Inf. Process. Syst., V31
   Fan Zhang, 2021, MultiMedia Modeling. 27th International Conference, MMM 2021. Proceedings. Lecture Notes in Computer Science (LNCS 12572), P136, DOI 10.1007/978-3-030-67832-6_12
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Ge W., 2019, PROC CVPR IEEE
   He J, 2022, AAAI CONF ARTIF INTE, P852
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu YQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4239, DOI 10.1145/3474085.3475561
   Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132
   Jung YR, 2023, IEEE ACCESS, V11, P35670, DOI 10.1109/ACCESS.2023.3265472
   Khosla A., 2013, Novel dataset for fine-grained image categorization
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu CB, 2020, AAAI CONF ARTIF INTE, V34, P11555
   Liu HY, 2016, PROC CVPR IEEE, P2167, DOI 10.1109/CVPR.2016.238
   Liu XD, 2022, NEUROCOMPUTING, V492, P137, DOI 10.1016/j.neucom.2022.04.037
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo W, 2019, IEEE I CONF COMP VIS, P8241, DOI 10.1109/ICCV.2019.00833
   Maji S, 2013, Arxiv, DOI arXiv:1306.5151
   Miao Z, 2021, IEEE SIGNAL PROC LET, V28, P1983, DOI 10.1109/LSP.2021.3114622
   Song JW, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9534004
   Sun HB, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5853, DOI 10.1145/3503161.3548308
   Sun PZ, 2021, Arxiv, DOI arXiv:2012.15460
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Touvron H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P32, DOI 10.1109/ICCV48922.2021.00010
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914
   Vaswani A, 2023, Arxiv, DOI arXiv:1706.03762
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang HY, 2021, PROC CVPR IEEE, P5459, DOI 10.1109/CVPR46437.2021.00542
   Wang J, 2022, Arxiv, DOI arXiv:2107.02341
   Wang YM, 2018, PROC CVPR IEEE, P4148, DOI 10.1109/CVPR.2018.00436
   Wei XS, 2018, PATTERN RECOGN, V76, P704, DOI 10.1016/j.patcog.2017.10.002
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Xu Q, 2023, IEEE T MULTIMEDIA, V25, P9015, DOI 10.1109/TMM.2023.3244340
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zhang Y, 2022, INT CONF ACOUST SPEE, P3234, DOI 10.1109/ICASSP43922.2022.9747591
   Zheng HL, 2019, Arxiv, DOI arXiv:1911.03621
   Zhu HW, 2022, PROC CVPR IEEE, P4682, DOI 10.1109/CVPR52688.2022.00465
   Zhu XZ, 2021, Arxiv, DOI [arXiv:2010.04159, 10.48550/arXiv.2010.04159]
   Zhuang PQ, 2020, AAAI CONF ARTIF INTE, V34, P13130
NR 49
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 17
PY 2024
DI 10.1007/s00371-024-03502-3
EA JUN 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UN1G1
UT WOS:001248641200003
DA 2024-08-05
ER

PT J
AU Yu, TS
   Liu, Y
   Liu, H
   Chen, J
   Wang, X
AF Yu, Tongshuai
   Liu, Ye
   Liu, Hao
   Chen, Ji
   Wang, Xing
TI ParaLkResNet: an efficient multi-scale image classification network
SO VISUAL COMPUTER
LA English
DT Article
DE Image classification; Feature fusion; Multi-scale features
AB Recently, deep neural networks have achieved remarkable results in computer vision tasks with the widely used visual attention mechanism. However, the introduction of the visual attention mechanism increases the parameters and computational complexity, which limit its application in resource-constrained environments. To solve this problem, we propose a novel convolutional block, the ParaLk block (PLB), a large kernel parallel convolutional block. Additionally, we apply PLB to PreActResNet by replacing the first 2D convolution to capture feature maps at different scales and call this new network ParaLkResNet. In practice, the effective receptive field of a convolutional network is smaller than that in real-world computation. Therefore the PLB is used to increase the receptive field of the network. Besides extracting multi-scale and high fusion features over normal 2D convolution, it has low latency in typical downstream tasks and good scalability to different data. It is worth noting that PLB as a plug-in block can apply to various computer vision tasks not limited to image classification. The proposed method outperforms most current classification networks in image classification. The accuracy on the CIFAR-10 dataset is improved by 2.42% and 0.66% compared to OTTT and IM-Loss, respectively. Our source code is available at: https://doi.org/10.5281/zenodo.11204902.
C1 [Yu, Tongshuai; Liu, Ye; Liu, Hao; Chen, Ji; Wang, Xing] Linyi Univ, Coll Informat Sci & Engn, Linyi, Peoples R China.
C3 Linyi University
RP Chen, J; Wang, X (corresponding author), Linyi Univ, Coll Informat Sci & Engn, Linyi, Peoples R China.
EM jichen_lntu@163.com; xingwang_lntu@163.com
FU National Natural Science Foundation of China
FX No Statement Available
CR Araujo A., 2019, Distill, V4, DOI [10.23915/distill.00021, DOI 10.23915/DISTILL.00021]
   Arora S, 2018, PR MACH LEARN RES, V80
   Bungert L, 2022, J MACH LEARN RES, V23
   Cao JM, 2022, IEEE T IMAGE PROCESS, V31, P3726, DOI 10.1109/TIP.2022.3175432
   Chen TL, 2021, IEEE COMPUT SOC CONF, P4614, DOI 10.1109/CVPRW53098.2021.00520
   Chen ZH, 2023, IEEE T PATTERN ANAL, V45, P13489, DOI 10.1109/TPAMI.2023.3293885
   Chen ZH, 2020, IEEE T CYBERNETICS, V50, P2152, DOI 10.1109/TCYB.2018.2875983
   Cheng ZZ, 2015, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2015.55
   Chrysos GG, 2022, LECT NOTES COMPUT SC, V13685, P692, DOI 10.1007/978-3-031-19806-9_40
   Dai L, 2024, NAT MED, DOI 10.1038/s41591-023-02702-z
   Dai L, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-23458-5
   Darlow L.N., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1810.03505
   Ding XH, 2022, PROC CVPR IEEE, P11953, DOI 10.1109/CVPR52688.2022.01166
   Ding XH, 2021, PROC CVPR IEEE, P13728, DOI 10.1109/CVPR46437.2021.01352
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Dwibedi D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9568, DOI 10.1109/ICCV48922.2021.00945
   Gavrikov P, 2022, PROC CVPR IEEE, P19044, DOI 10.1109/CVPR52688.2022.01848
   Guo HB, 2021, IEEE T CYBERNETICS, V51, P2735, DOI 10.1109/TCYB.2019.2934823
   Guo S., 1811, PREPRINT
   Guo YF, 2022, ADV NEUR IN
   Han K, 2021, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.2103.00112
   Hassani A., 2021, PREPRINT
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Howard A.G., 2017, Comput. Vis. Pattern Recognit
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huo X., 2022, HIFUSE HIERARCHICAL
   Jeevan P., 2022, PREPRINT
   Jia X, 2022, LECT NOTES COMPUT SC, V13583, P151, DOI 10.1007/978-3-031-21014-3_16
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Kabir H.D., 2022, IEEE T ARTIF INTELL
   Krizhevsky A., 2012, Learning multiple layers of features from tiny images, DOI DOI 10.1145/3079856.3080246
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Moreau Thomas, 2022, ADV NEURAL INFORM PR
   Nazir A, 2022, IEEE T AFFECT COMPUT, V13, P845, DOI 10.1109/TAFFC.2020.2970399
   Pishchik Evgenii., 2023, Trainable activations for image classification
   Qian B, 2024, PATTERNS, V5, DOI 10.1016/j.patter.2024.100929
   QIN Y, 2024, INT J COMPUT VISION, P1
   Romero D. W., 2021, PREPRINT
   Samad SA, 2023, IEEE ACCESS, V11, P132088, DOI 10.1109/ACCESS.2023.3329581
   Sander ME, 2021, PR MACH LEARN RES, V139
   Schuler J.P.S., 2022, MENDEL
   Schuler JPS, 2022, ENTROPY-SWITZ, V24, DOI 10.3390/e24091264
   Sheng B, 2022, IEEE T CYBERNETICS, V52, P6662, DOI 10.1109/TCYB.2021.3079311
   Sheng B, 2020, IEEE T VIS COMPUT GR, V26, P1332, DOI 10.1109/TVCG.2018.2869326
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan MX, 2019, PR MACH LEARN RES, V97
   Trockman A., 2022, PREPRINT
   Wei Y, 2023, IEEE I CONF COMP VIS, P21672, DOI 10.1109/ICCV51070.2023.01986
   Xiao Mingqing, 2022, Advances in Neural Information Processing Systems, V35, P20717
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yao DX, 2021, PROC ACM INTERACT MO, V5, DOI 10.1145/3494981
   Zhang BX, 2020, IEEE T VIS COMPUT GR, V26, P2546, DOI 10.1109/TVCG.2019.2894627
   Zhang H, 2023, LECT NOTES COMPUT SC, V13843, P541, DOI 10.1007/978-3-031-26313-2_33
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng YW, 2021, PROC CVPR IEEE, P8152, DOI 10.1109/CVPR46437.2021.00806
   Zhu Chen, 2021, ADV NEURAL INFORM PR, V34
NR 62
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 5057
EP 5066
DI 10.1007/s00371-024-03508-x
EA JUN 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001244596400001
DA 2024-08-05
ER

PT J
AU Zeng, YJ
   Pun, CM
AF Zeng, Yijia
   Pun, Chi-Man
TI Frequency-constrained transferable adversarial attack on image
   manipulation detection and localization
SO VISUAL COMPUTER
LA English
DT Article
DE Adversarial attack; Forgery image Detection and localization;
   Transferability; Reverse image forensics
AB Recent works have demonstrated the great performance of forgery image forensics based on deep learning, but there is still a risk that detectors could be susceptible to unknown illegal attacks, raising growing security concerns. This paper starts from the perspective of reverse forensics and explores the vulnerabilities of current image manipulation detectors to achieve targeted attacks. We present a novel reverse decision aggregate gradient attack under low-frequency constraints (RevAggAL). Specifically, we first propose a novel pixel reverse content decision-making (PRevCDm) loss to optimize perturbation generation with a specific principle more suitable for segmenting manipulated regions. Then, we introduce the low-frequency component to constrain the perturbation into more imperceptible details, significantly avoiding the degradation of image quality. We also consider aggregating gradients on model-agnostic features to enhance the transferability of adversarial examples in black-box scenarios. We evaluate the effectiveness of our method on three representative detectors (ResFCN, MVSSNet, and OSN) with five widely used forgery datasets (COVERAGE, COLUMBIA, CASIA1, NIST 2016, and Realistic Tampering). Experimental results show that our method improves the attack success rate (ASR) while ensuring better image quality.
C1 [Zeng, Yijia; Pun, Chi-Man] Univ Macau, Dept Comp & Informat Sci, Macau, Peoples R China.
C3 University of Macau
RP Pun, CM (corresponding author), Univ Macau, Dept Comp & Informat Sci, Macau, Peoples R China.
EM cmpun@umac.mo
FU Science and Technology Development Fund, Macau SAR [0087/2020/A2,
   0141/2023/RIA2, 0193/2023/RIA3]
FX This work was supported in part by the Science and Technology
   Development Fund, Macau SAR, under Grants 0087/2020/A2, 0141/2023/RIA2
   and 0193/2023/RIA3.
CR Asnani V, 2022, PROC CVPR IEEE, P15365, DOI 10.1109/CVPR52688.2022.01495
   Bai PX, 2022, IEEE CONF COMPUT, DOI 10.1109/INFOCOMWKSHPS54753.2022.9798253
   Bammey Q., 2020, P IEEE CVF C COMP VI, p14,194
   Cai ZK, 2023, PROC CVPR IEEE, P4045, DOI 10.1109/CVPR52729.2023.00394
   Carlini N, 2020, IEEE COMPUT SOC CONF, P2804, DOI 10.1109/CVPRW50498.2020.00337
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chen XR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14165, DOI 10.1109/ICCV48922.2021.01392
   Dong CB, 2023, IEEE T PATTERN ANAL, V45, P3539, DOI 10.1109/TPAMI.2022.3180556
   Feng Y, 2020, AAAI CONF ARTIF INTE, V34, P10786
   Fontani M, 2013, IEEE T INF FOREN SEC, V8, P593, DOI 10.1109/TIFS.2013.2248727
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Gallagher AC, 2008, PROC CVPR IEEE, P253
   Gao Z, 2023, IEEE T KNOWL DATA EN, V35, P7541, DOI 10.1109/TKDE.2022.3187091
   GOODFELLOW I, 2014, ADV NEURAL INFORM PR, V27
   Guan HY, 2019, IEEE WINT CONF APPL, P63, DOI 10.1109/WACVW.2019.00018
   Guillaro F, 2023, PROC CVPR IEEE, P20606, DOI 10.1109/CVPR52729.2023.01974
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heusel M., 2017, NeurIPS, P6629
   Huang H, 2023, PROC CVPR IEEE, P20514, DOI 10.1109/CVPR52729.2023.01965
   Jia S, 2022, PROC CVPR IEEE, P4093, DOI 10.1109/CVPR52688.2022.00407
   Jing Dong, 2013, 2013 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP), P422, DOI 10.1109/ChinaSIP.2013.6625374
   Kawar B, 2023, PROC CVPR IEEE, P6007, DOI 10.1109/CVPR52729.2023.00582
   Kingma D. P., 2014, arXiv
   Korus P., 2016, Information Forensics and Security (WIFS), 2016 IEEE International Workshop on, P1, DOI DOI 10.1109/WIFS.2016.7823898
   Korus P, 2017, IEEE T INF FOREN SEC, V12, P809, DOI 10.1109/TIFS.2016.2636089
   Kwon MJ, 2022, INT J COMPUT VISION, V130, P1875, DOI 10.1007/s11263-022-01617-5
   Li B., 2020, 2020 IEEE CVF C COMP, P7877
   Li DZ, 2021, PROC CVPR IEEE, P5785, DOI 10.1109/CVPR46437.2021.00573
   Li HD, 2019, IEEE I CONF COMP VIS, P8300, DOI 10.1109/ICCV.2019.00839
   Li QF, 2020, PROC CVPR IEEE, P7243, DOI 10.1109/CVPR42600.2020.00727
   Liu XH, 2022, IEEE T CIRC SYST VID, V32, P7505, DOI 10.1109/TCSVT.2022.3189545
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu CS, 2003, IEEE T MULTIMEDIA, V5, P161, DOI 10.1109/TMM.2003.811621
   Lukás J, 2006, IEEE T INF FOREN SEC, V1, P205, DOI 10.1109/TIFS.2006.873602
   Madry A, 2018, INT C LEARN REPR, DOI 10.48550/arXiv.1706.06083
   Ng T.T., 2004, 4 ADVENT
   Nikolaidis N, 1996, INT CONF ACOUST SPEE, P2168, DOI 10.1109/ICASSP.1996.545849
   Park T., 2020, Advances in Neural Information Processing Systems, V33, P7198
   Qiu X., 2014, P 2 ACM WORKSH INF H, P165
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Salloum R, 2018, J VIS COMMUN IMAGE R, V51, P201, DOI 10.1016/j.jvcir.2018.01.010
   Schwarcz S, 2021, IEEE COMPUT SOC CONF, P933, DOI 10.1109/CVPRW53098.2021.00104
   Swaminathan A, 2006, 2006 40TH ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS, VOLS 1-4, P1194, DOI 10.1109/CISS.2006.286646
   Wang JK, 2022, PROC CVPR IEEE, P2354, DOI 10.1109/CVPR52688.2022.00240
   Wang ZB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7619, DOI 10.1109/ICCV48922.2021.00754
   Warbhe AD, 2016, PROCEDIA COMPUT SCI, V78, P464, DOI 10.1016/j.procs.2016.02.089
   Wei ZP, 2023, PROC CVPR IEEE, P12281, DOI 10.1109/CVPR52729.2023.01182
   Wen BH, 2016, IEEE IMAGE PROC, P161, DOI 10.1109/ICIP.2016.7532339
   Wu HW, 2022, IEEE T INF FOREN SEC, V17, P443, DOI 10.1109/TIFS.2022.3144878
   Zeng Y, 2022, PROC CVPR IEEE, P5941, DOI 10.1109/CVPR52688.2022.00586
   Zhao AQ, 2023, PROC CVPR IEEE, P8153, DOI 10.1109/CVPR52729.2023.00788
   Zhao ZY, 2020, PROC CVPR IEEE, P1036, DOI 10.1109/CVPR42600.2020.00112
   Zhou LJ, 2022, PROC CVPR IEEE, P15233, DOI 10.1109/CVPR52688.2022.01482
   Zhou P, 2018, PROC CVPR IEEE, P1053, DOI 10.1109/CVPR.2018.00116
   Zhu PF, 2023, IEEE I CONF COMP VIS, P4292, DOI 10.1109/ICCV51070.2023.00398
   Zou JH, 2022, AAAI CONF ARTIF INTE, P3662
NR 56
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4817
EP 4828
DI 10.1007/s00371-024-03482-4
EA JUN 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001242155100004
DA 2024-08-05
ER

PT J
AU Tian, XX
   Bi, CK
   Han, J
   Yu, C
AF Tian, Xiaoxian
   Bi, Chongke
   Han, Jun
   Yu, Ce
TI EasyRP-R-CNN: a fast cyclone detection model
SO VISUAL COMPUTER
LA English
DT Article
DE Machine learning; Vortex detection; Cyclone detection; EasyRP-R-CNN
ID TROPICAL CYCLONES
AB Efficient and accurate cyclone detection is essential for avoiding or reducing severe damage in coastal areas. However, traditional detection methods fail to provide accurate results with efficiency. To respond, we propose a convolutional-based cyclone detection framework, EasyRP-R-CNN, focusing on improving efficiency while minimizing the loss of accuracy. Our detection method utilizes satellite cloud images in the visible light band to improve detection accuracy. We designed a new Region of Interest (ROI) selection mechanism, named Easy Region Proposal, and a scale-based ROI grouping module to avoid classifying undersized ROIs. Experimental results demonstrate that our method exhibits satisfactory detection accuracy for cyclones of various intensities, with improved detection efficiency.
C1 [Tian, Xiaoxian; Bi, Chongke; Yu, Ce] Tianjin Univ, Coll Intelligence & Comp, 135 Yaguan Rd, Jinan 300350, Peoples R China.
   [Han, Jun] Chinese Univ Hong Kong, Sch Data Sci, Shenzhen, Guangdong, Peoples R China.
C3 Tianjin University; The Chinese University of Hong Kong, Shenzhen
RP Bi, CK (corresponding author), Tianjin Univ, Coll Intelligence & Comp, 135 Yaguan Rd, Jinan 300350, Peoples R China.
EM 3019234179@tju.edu.cn; bichongke@tju.edu.cn; hanjun@cuhk.edu.cn;
   yuce@tju.edu.cn
FU National Key Research and Development Program of China [2021YFE0108400];
   National Key R &D Program of China [62172294]; National Natural Science
   Foundation of China
FX This work was partly supported by the National Key R &D Program of China
   under Grand No. 2021YFE0108400 and partly supported by the National
   Natural Science Foundation of China under Grant No. 62172294.
CR Abraham K, 2024, EARTH SCI INFORM, V17, P869, DOI 10.1007/s12145-023-01205-2
   Accarino G., 2023, ARXIV
   DVORAK VF, 1975, MON WEATHER REV, V103, P420, DOI 10.1175/1520-0493(1975)103<0420:TCIAAF>2.0.CO;2
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Han H, 2015, REMOTE SENS-BASEL, V7, P9184, DOI 10.3390/rs70709184
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ho SS, 2012, INT C PATT RECOG, P2643
   Jin SH, 2017, IEEE T GEOSCI REMOTE, V55, P280, DOI 10.1109/TGRS.2016.2605766
   Kim M, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11101195
   Knapp KR, 2010, B AM METEOROL SOC, V91, P363, DOI 10.1175/2009BAMS2755.1
   Kossin JP, 2018, NATURE, V558, P104, DOI 10.1038/s41586-018-0158-3
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumler-Bonfanti C, 2020, J APPL METEOROL CLIM, V59, P1971, DOI 10.1175/JAMC-D-20-0117.1
   Lam L, 2023, ATMOSPHERE-BASEL, V14, DOI 10.3390/atmos14020215
   Lee RST, 2001, IEEE T SYST MAN CY B, V31, P413, DOI 10.1109/3477.931532
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lu XQ, 2021, ADV ATMOS SCI, V38, P690, DOI 10.1007/s00376-020-0211-7
   Malothu N, 2022, ACTA GEOPHYS, V70, P2855, DOI 10.1007/s11600-022-00849-w
   Mascarenhas S., 2021, International Conference on Disruptive Technologies for MultiDisciplinary Research and Applications (CENTCON), P96
   Pang SC, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13091860
   Piñeros MF, 2010, IEEE GEOSCI REMOTE S, V7, P826, DOI 10.1109/LGRS.2010.2048694
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shakya S, 2020, IEEE J-STARS, V13, P827, DOI 10.1109/JSTARS.2020.2970253
   The Central People Government of the People Republic of China, US
   Wang C, 2023, ATMOS OCEAN SCI LETT, V16, DOI 10.1016/j.aosl.2023.100373
   Wang PP, 2020, IEEE J-STARS, V13, P2161, DOI 10.1109/JSTARS.2020.2995158
   Wang S, 2021, SCIENCE, V371, P514, DOI 10.1126/science.abb9038
   Xie M, 2022, IEEE J-STARS, V15, P9613, DOI 10.1109/JSTARS.2022.3219809
   Xie M, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12193111
   Ying M, 2014, J ATMOS OCEAN TECH, V31, P287, DOI 10.1175/JTECH-D-12-00119.1
   Zhang W, 2015, WEATHER FORECAST, V30, P446, DOI 10.1175/WAF-D-14-00023.1
NR 35
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4829
EP 4841
DI 10.1007/s00371-024-03483-3
EA JUN 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001237776600001
DA 2024-08-05
ER

PT J
AU Rani, M
   Yadav, J
   Rathee, N
   Panjwani, B
AF Rani, Mamta
   Yadav, Jyoti
   Rathee, Neeru
   Panjwani, Bharti
TI Optifusion: advancing visual intelligence in medical imaging through
   optimized CNN-TQWT fusion
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Medical image fusion; CNN; DWT; TQWT; Hybrid fusion
ID QUALITY ASSESSMENT; SEGMENTATION; SYSTEM
AB The domain of medical image fusion has garnered considerable attention within the biomedical imaging and clinical analysis communities. Despite the transformative impact of deep learning models on image fusion, achieving perfection remains an ongoing challenge. To address this challenge, a network named advancing visual intelligence in medical imaging through optimized CNN-TQWT fusion (OCT-fusion) is proposed to advance the field of multimodal medical image fusion by adeptly integrating spatial and transform domain strategies, ensuring superior image clarity. A modified CNN (MCNN) optimized for superior feature extraction from source images is proposed, and a tunable-Q wavelet transform (TQWT) is employed to refine the fusion process. This combination ensures that the MCNN-enhanced image is further integrated with a source image, reinforcing the overall clarity. The hyper-parameters of the TQWT undergo precision tuning using the covariance matrix adaptation evolution strategy (CMA-ES) optimizer. After applying the inverse TQWT (ITQWT), the final image captures the best from both domains, promising unparalleled clarity and detail. As a contribution in the realm of medical image fusion, the suggested approach when applied to medical and infrared (IV) datasets performed better than existing approaches which is evident in terms of seven performance measures. The novelty of the proposed method lies in the fact that it is the first one to apply TQWT decomposition in an image fusion task along with the optimization of TQWT parameters with Optuna. The method is tested on publicly available dataset, i.e., Whole Brain Atlas Harvard medical dataset (Johnson in The whole brain atlas, 2001). The robustness of the model is also checked on infrared-visual (IV) and color multi-focus datasets (Zhang et al. in Inf Fusion 54:99-118, 2020).
C1 [Rani, Mamta; Yadav, Jyoti] Netaji Subhash Univ Technol, ICE, Sect 3, Delhi 110059, India.
   [Rani, Mamta; Rathee, Neeru] Maharaja Surajmal Inst Technol, C-4, Delhi 110058, India.
   [Panjwani, Bharti] Shri Madhwa Vadiraja Inst Technol & Management, CSE, Bantakal 574115, Karnataka, India.
C3 Netaji Subhas University of Technology; Maharaja Surajmal Institute of
   Technology
RP Rani, M (corresponding author), Netaji Subhash Univ Technol, ICE, Sect 3, Delhi 110059, India.; Rani, M (corresponding author), Maharaja Surajmal Inst Technol, C-4, Delhi 110058, India.
EM mamtatholia@gmail.com; jyoti.yadav@nsut.ac.in; neeru_rathee@msit.in;
   bharti.cs@sode-edu.in
OI Rani, mamta/0000-0001-5213-261X
CR Agarwal S., 2017, BIOMED PHARMACOL J, V10, P831, DOI DOI 10.13005/bpj/1174
   Aghamaleki JA, 2023, VISUAL COMPUT, V39, P1181, DOI 10.1007/s00371-021-02396-9
   Akiba T, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2623, DOI 10.1145/3292500.3330701
   AlAsadi AHH., 2015, J. AL-Qadisiyah Comput. Sci. Math, V7, P146
   Ali R, 2021, IEEE T IND INFORM, V17, P2476, DOI 10.1109/TII.2020.3000204
   Amin-Naji M, 2019, INFORM FUSION, V51, P201, DOI 10.1016/j.inffus.2019.02.003
   Azarang A, 2019, IEEE ACCESS, V7, P35673, DOI 10.1109/ACCESS.2019.2905511
   Bavirisetti DP, 2019, CIRC SYST SIGNAL PR, V38, P5576, DOI 10.1007/s00034-019-01131-z
   Bhavana V, 2015, PROCEDIA COMPUT SCI, V70, P625, DOI 10.1016/j.procs.2015.10.057
   Brahmbhatt KN., 2013, Int. J. Adv. Res. Eng. Technol, V4, P161
   Cheema MN, 2021, IEEE T IND INFORM, V17, P7991, DOI 10.1109/TII.2021.3064369
   Chen J, 2020, INFORM SCIENCES, V508, P64, DOI 10.1016/j.ins.2019.08.066
   Chen Y, 2009, IMAGE VISION COMPUT, V27, P1421, DOI 10.1016/j.imavis.2007.12.002
   Dai L, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-23458-5
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dian RW, 2021, INFORM FUSION, V69, P40, DOI 10.1016/j.inffus.2020.11.001
   Dinh PH, 2023, BIOMED SIGNAL PROCES, V84, DOI 10.1016/j.bspc.2023.104740
   Diwakar M, 2021, MATER TODAY-PROC, V37, P3411, DOI 10.1016/j.matpr.2020.09.278
   Du J, 2016, NEUROCOMPUTING, V215, P3, DOI 10.1016/j.neucom.2015.07.160
   El-Hoseny HM, 2018, INFRARED PHYS TECHN, V94, P223, DOI 10.1016/j.infrared.2018.09.003
   Garima, 2023, MULTIMED TOOLS APPL, V82, P28547, DOI 10.1007/s11042-023-14671-z
   Hashim Fatima, 2022, Nature Environment and Pollution Technology, V21, P867, DOI 10.46488/NEPT.2022.v21i02.050
   Hossny M, 2008, LECT NOTES ARTIF INT, V5314, P469, DOI 10.1007/978-3-540-88513-9_51
   Hu XY, 2023, INFORM FUSION, V92, P127, DOI 10.1016/j.inffus.2022.11.014
   Wei H, 2007, PATTERN RECOGN LETT, V28, P493, DOI 10.1016/j.patrec.2006.09.005
   James AP, 2014, INFORM FUSION, V19, P4, DOI 10.1016/j.inffus.2013.12.002
   Jiang D., 2011, Image Fusion Appl, DOI 10.5772/10548
   Jin ZR, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4315, DOI 10.1145/3474085.3475571
   Johnson KA., 2001, The whole brain atlas
   Jyoti K, 2023, COMPUT BIOL MED, V152, DOI 10.1016/j.compbiomed.2022.106331
   Kaur H, 2021, ARCH COMPUT METHOD E, V28, P4425, DOI 10.1007/s11831-021-09540-7
   Lahoud F, 2019, 2019 22ND INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION 2019), DOI 10.23919/fusion43075.2019.9011178
   Li XS, 2021, INFORM SCIENCES, V569, P302, DOI 10.1016/j.ins.2021.04.052
   Li Y., 2021, Int. J. Cogn. Comput. Eng., V2, P21, DOI DOI 10.1016/J.IJCCE.2020.12.004
   Li Y, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.638976
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Lindfield G., 2019, Chapter 8Analyzing Data Using Discrete Transforms, P383
   Liu F., 2020, P 28 INT C COMP LING, P3586
   Liu RH, 2023, IEEE T MED IMAGING, V42, P1083, DOI 10.1109/TMI.2022.3223683
   Liu RH, 2022, PATTERNS, V3, DOI 10.1016/j.patter.2022.100512
   Liu Y, 2020, INFORM FUSION, V64, P71, DOI 10.1016/j.inffus.2020.06.013
   Liu Y, 2017, 2017 20TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P1070
   Liu Y, 2017, INFORM FUSION, V36, P191, DOI 10.1016/j.inffus.2016.12.001
   Mann S, 2022, BIOMED RES INT, V2022, DOI 10.1155/2022/6392206
   Morris C., 2014, ICTACT J. Image Video Process. Specl. Issue Video Process. Multimed. Syst., V5
   Naidu VPS, 2010, DEFENCE SCI J, V60, P48, DOI 10.14429/dsj.60.105
   Nazir A, 2022, IEEE T IMAGE PROCESS, V31, P880, DOI 10.1109/TIP.2021.3136619
   Nazir A, 2021, IEEE T BIO-MED ENG, V68, P2540, DOI 10.1109/TBME.2021.3050310
   Patil U., 2011, 2011 INT C IM INF PR, P1
   Rajalingam B, 2022, MULTIMEDIA SYST, V28, P1449, DOI 10.1007/s00530-020-00706-0
   Rajalingam B., 2018, International Journal of Engineering Science Invention, V2, P52
   Rani M, 2023, MULTIMED TOOLS APPL, DOI 10.1007/s11042-023-16872-y
   Selesnick IW, 2011, IEEE T SIGNAL PROCES, V59, P3560, DOI 10.1109/TSP.2011.2143711
   Selesnick IW, 2005, IEEE SIGNAL PROC MAG, V22, P123, DOI 10.1109/MSP.2005.1550194
   Shao ZF, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12172796
   Shao ZF, 2018, IEEE J-STARS, V11, P1656, DOI 10.1109/JSTARS.2018.2805923
   Singh R, 2009, ICAPR 2009: SEVENTH INTERNATIONAL CONFERENCE ON ADVANCES IN PATTERN RECOGNITION, PROCEEDINGS, P232, DOI 10.1109/ICAPR.2009.97
   Sinhal R, 2022, CIRC SYST SIGNAL PR, V41, P6370, DOI 10.1007/s00034-022-02090-8
   Su WJ, 2022, INFRARED PHYS TECHN, V127, DOI 10.1016/j.infrared.2022.104417
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZY, 2019, MULTIMED TOOLS APPL, V78, P34483, DOI 10.1007/s11042-019-08070-6
   Wei Q, 2015, IEEE T GEOSCI REMOTE, V53, P3658, DOI 10.1109/TGRS.2014.2381272
   Wu DB, 2014, COMM COM INF SC, V461, P358
   Wu X., 2023, Vis. Comput., P1
   Xie QY, 2023, INFRARED PHYS TECHN, V131, DOI 10.1016/j.infrared.2023.104659
   Xydeas C, 2000, P SOC PHOTO-OPT INS, V4051, P89, DOI 10.1117/12.381668
   Yadav SP, 2020, MED BIOL ENG COMPUT, V58, P669, DOI 10.1007/s11517-020-02136-6
   Yang B, 2010, IEEE T INSTRUM MEAS, V59, P884, DOI 10.1109/TIM.2009.2026612
   Yang C, 2008, INFORM FUSION, V9, P156, DOI 10.1016/j.inffus.2006.09.001
   Zhang Y, 2020, INFORM FUSION, V54, P99, DOI 10.1016/j.inffus.2019.07.011
   Zhao WD, 2019, IEEE T CIRC SYST VID, V29, P1102, DOI 10.1109/TCSVT.2018.2821177
   Zhao ZX, 2023, PROC CVPR IEEE, P5906, DOI 10.1109/CVPR52729.2023.00572
   Zhou T, 2023, COMPUT BIOL MED, V160, DOI 10.1016/j.compbiomed.2023.106959
NR 73
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 29
PY 2024
DI 10.1007/s00371-024-03495-z
EA MAY 2024
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL2G6
UT WOS:001234536800002
DA 2024-08-05
ER

PT J
AU Zhu, LP
   Wu, SL
   Chang, XX
   Yang, YX
   Li, X
AF Zhu, Liping
   Wu, Silin
   Chang, Xianxiang
   Yang, Yixuan
   Li, Xuan
TI Rethinking group activity recognition under the open set condition
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Group activity recognition; Open set recognition; Uncertainty
   estimation; Attention mechanism
ID NETWORK
AB In real-world scenarios, the recognition of unknown activities poses a significant challenge for group activity recognition. Existing methods primarily focus on closed sets, leaving the task of open set group activity recognition unexplored. In this paper, we introduce the concept of open set group activity recognition for the first time and propose a novel recognition framework to deal with it. To mitigate potential scene biases, keypoints extracted from groups are utilized as input. Our framework employs a two-stage approach: Evidence Aware Collection and Evidence Aware Decision, to address the challenge of insufficient evidence for rejecting unknown classes. Specifically, encoders are established at the individual, subgroup, and group scales to collect activity evidence among group members. By applying an attention mechanism, we focus on important evidence, resulting in a set of aggregated evidence. The uncertainty estimated from evidence is then used to effectively distinguish between known and unknown classes. Additionally, we perform open set splits on two publicly available group activity recognition datasets. Experimental results demonstrate that our method shows promising performance in open set group activity recognition while maintaining comparable performance under closed set conditions.
C1 [Zhu, Liping; Wu, Silin; Chang, Xianxiang; Yang, Yixuan; Li, Xuan] China Univ Petr, Beijing Key Lab Petr Data Min, Beijing 102249, Peoples R China.
C3 China University of Petroleum
RP Wu, SL (corresponding author), China Univ Petr, Beijing Key Lab Petr Data Min, Beijing 102249, Peoples R China.
EM zhuliping@cup.edu.cn; 2022211284@student.cup.edu.cn;
   2022211282@student.cup.edu.cn; 2022216053@student.cup.edu.cn;
   2022211283@student.cup.edu.cn
FU National Key R &D Program of China [2022YFB4501600]
FX This work was supported by National Key R &D Program of China
   (2022YFB4501600).
CR Amer MR, 2012, LECT NOTES COMPUT SC, V7575, P187, DOI 10.1007/978-3-642-33765-9_14
   Amer MR, 2014, LECT NOTES COMPUT SC, V8694, P572, DOI 10.1007/978-3-319-10599-4_37
   Bagautdinov T, 2017, PROC CVPR IEEE, P3425, DOI 10.1109/CVPR.2017.365
   Bao WT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13329, DOI 10.1109/ICCV48922.2021.01310
   Bendale A, 2016, PROC CVPR IEEE, P1563, DOI 10.1109/CVPR.2016.173
   Center I., 2021, BRIT MACH VIS C
   Choi J., 2019, Adv. Neural Inf. Process. Syst., V32
   Choi W, 2014, IEEE T PATTERN ANAL, V36, P1242, DOI 10.1109/TPAMI.2013.220
   Ditria Luke, 2020, P AS C COMP VIS
   Du ZX, 2023, IEEE T CIRC SYST VID, V33, P5076, DOI 10.1109/TCSVT.2023.3249906
   Du ZX, 2023, IMAGE VISION COMPUT, V137, DOI 10.1016/j.imavis.2023.104789
   Duan HD, 2022, PROC CVPR IEEE, P2959, DOI 10.1109/CVPR52688.2022.00298
   Ehsanpour Mahsa, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P177, DOI 10.1007/978-3-030-58545-7_11
   Gavrilyuk K, 2020, PROC CVPR IEEE, P836, DOI 10.1109/CVPR42600.2020.00092
   Guangyao Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P507, DOI 10.1007/978-3-030-58580-8_30
   Han MF, 2022, PROC CVPR IEEE, P2980, DOI 10.1109/CVPR52688.2022.00300
   Hendrycks D, 2019, Arxiv, DOI arXiv:1812.04606
   Hendrycks D, 2018, Arxiv, DOI arXiv:1610.02136
   Hu B, 2022, LECT NOTES COMPUT SC, V13664, P439, DOI 10.1007/978-3-031-19772-7_26
   Hu GY, 2020, PROC CVPR IEEE, P977, DOI 10.1109/CVPR42600.2020.00106
   Huang HZ, 2023, IEEE T PATTERN ANAL, V45, P4214, DOI 10.1109/TPAMI.2022.3200384
   Ibrahim MS, 2016, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2016.217
   Jain LP, 2014, LECT NOTES COMPUT SC, V8691, P393, DOI 10.1007/978-3-319-10578-9_26
   Josang A, 2016, ARTIF INTELL-FOUND, P1, DOI 10.1007/978-3-319-42337-1
   Kendall A, 2017, 31 ANN C NEURAL INFO, V30
   Kim D, 2022, PROC CVPR IEEE, P20051, DOI 10.1109/CVPR52688.2022.01945
   Kim Y. W., 2022, Advances in Neural Information Processing Systems, V35, P35710
   Kingma D. P., 2014, arXiv
   Krishnan R, 2018, Arxiv, DOI arXiv:1811.03305
   Li D, 2022, Arxiv, DOI arXiv:2208.14847
   Li F, 2005, IEEE T PATTERN ANAL, V27, P1686, DOI 10.1109/TPAMI.2005.224
   Li SC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13648, DOI 10.1109/ICCV48922.2021.01341
   Li W, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2051, DOI 10.1145/3503161.3547825
   Li YS, 2023, MULTIMED TOOLS APPL, V82, P15515, DOI 10.1007/s11042-022-13867-z
   Lin WY, 2014, IEEE T CIRC SYST VID, V24, P826, DOI 10.1109/TCSVT.2013.2280849
   Lin WY, 2013, IEEE T CIRC SYST VID, V23, P1980, DOI 10.1109/TCSVT.2013.2269780
   Lin WY, 2010, IEEE T CIRC SYST VID, V20, P1057, DOI 10.1109/TCSVT.2010.2057013
   Mao KM, 2023, APPL INTELL, V53, P1149, DOI 10.1007/s10489-022-03470-y
   Neal L, 2018, LECT NOTES COMPUT SC, V11210, P620, DOI 10.1007/978-3-030-01231-1_38
   Noor N, 2023, IEEE INT CONF COMP V, P2171, DOI 10.1109/ICCVW60793.2023.00232
   Oh H, 2022, IEEE ACCESS, V10, P120063, DOI 10.1109/ACCESS.2022.3222310
   Pramono Rizard Renanda Adhi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P71, DOI 10.1007/978-3-030-58452-8_5
   Qi M., 2018, stagnet: An attentive semantic rnn for group activity recognition
   Roitberg A., 2018, arXiv
   Rui Yan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P208, DOI 10.1007/978-3-030-58598-3_13
   Scheirer WJ, 2013, IEEE T PATTERN ANAL, V35, P1757, DOI 10.1109/TPAMI.2012.256
   Sensoy M, 2018, ADV NEUR IN, V31
   Sentz K., 2002, Combination of Evidence in Dempster-Shafer Theory"
   Shu TM, 2017, PROC CVPR IEEE, P4255, DOI 10.1109/CVPR.2017.453
   Shu TM, 2015, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR.2015.7299088
   Shu XB, 2021, IEEE T NEUR NET LEAR, V32, P663, DOI 10.1109/TNNLS.2020.2978942
   Shu XB, 2021, IEEE T PATTERN ANAL, V43, P1110, DOI 10.1109/TPAMI.2019.2942030
   Shu Y, 2018, IEEE INT CON MULTI
   Sinaga KP, 2020, IEEE ACCESS, V8, P80716, DOI 10.1109/ACCESS.2020.2988796
   Tang JH, 2022, IEEE T PATTERN ANAL, V44, P636, DOI 10.1109/TPAMI.2019.2928540
   Tang YS, 2020, IEEE T CIRC SYST VID, V30, P2872, DOI 10.1109/TCSVT.2020.2973301
   Tang YS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1283, DOI 10.1145/3240508.3240576
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang CC, 2023, IEEE ACCESS, V11, P129230, DOI 10.1109/ACCESS.2023.3332651
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang LK, 2023, NEUROCOMPUTING, V556, DOI 10.1016/j.neucom.2023.126646
   Wang MS, 2017, PROC CVPR IEEE, P7408, DOI 10.1109/CVPR.2017.783
   Wongun Choi, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1282, DOI 10.1109/ICCVW.2009.5457461
   Wu JC, 2019, PROC CVPR IEEE, P9956, DOI 10.1109/CVPR.2019.01020
   Wu LF, 2024, IEEE T MULTIMEDIA, V26, P6386, DOI 10.1109/TMM.2024.3349923
   Yan R, 2023, IEEE T PATTERN ANAL, V45, P6955, DOI 10.1109/TPAMI.2020.3034233
   Yan R, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1292, DOI 10.1145/3240508.3240572
   Yang GW, 2022, MATHEMATICS-BASEL, V10, DOI 10.3390/math10244725
   Yang KX, 2023, IEEE INT CON MULTI, P762, DOI 10.1109/ICME55011.2023.00136
   Yoon YS, 2019, IEEE ACCESS, V7, P165997, DOI 10.1109/ACCESS.2019.2953455
   Yoshihashi R, 2019, PROC CVPR IEEE, P4011, DOI 10.1109/CVPR.2019.00414
   Yuan HJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7456, DOI 10.1109/ICCV48922.2021.00738
   Yuan HJ, 2021, AAAI CONF ARTIF INTE, V35, P3261
   Yue RJ, 2022, NEUROCOMPUTING, V512, P287, DOI 10.1016/j.neucom.2022.09.071
   Zhai XL, 2023, LECT NOTES COMPUT SC, V13844, P329, DOI 10.1007/978-3-031-26316-3_20
   Zhang JX, 2022, IEEE T CIRC SYST VID, V32, P8646, DOI 10.1109/TCSVT.2022.3193574
   Zhao C, 2023, PROC CVPR IEEE, P22982, DOI 10.1109/CVPR52729.2023.02201
   Zhou HL, 2022, LECT NOTES COMPUT SC, V13695, P249, DOI 10.1007/978-3-031-19833-5_15
   Zhu XL, 2023, IEEE T CIRC SYST VID, V33, P3383, DOI 10.1109/TCSVT.2022.3233069
NR 79
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 13
PY 2024
DI 10.1007/s00371-024-03424-0
EA MAY 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QL1A9
UT WOS:001220925900002
DA 2024-08-05
ER

PT J
AU Li, JL
   Zhang, JM
   Zhang, XH
   Chen, M
AF Li, Jianliang
   Zhang, Jinming
   Zhang, Xiaohai
   Chen, Ming
TI Edge-guided generative network with attention for point cloud completion
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE 3D vision; Point cloud completion; Graph; Attention; Deep learning
AB Point clouds acquired through 3D scanning devices often suffer from sparsity and incompleteness due to reflection, device resolution, and viewing angle limitations. Therefore, the recovery of the complete shape from partial observations plays a vital role in assisting downstream tasks. Existing point cloud completion networks mostly ignore the encoding of the local region structure in the point cloud. In this work, we propose an edge-guided generative network with attention for point cloud completion. Specifically, the network has three consecutive stages. In the feature extraction stage, we propose the edge attention(EA) block, which can be stacked and applied to effectively capture local geometric details and structural information. The local neighborhood information is dynamically calculated, and the attention mechanism further deepens the relationship between the acquired edge features and position coordinates. We design the deconvolution attention skeleton generation module in the skeleton generation stage to generate a shape skeleton. For the detail refinement stage, we design a layered encoder based on PointNet++ module, which can better fuse the local geometry from the coarse point cloud and the global feature from the input point cloud to facilitate fine-grained point cloud generation. Comprehensive evaluations of several benchmarks indicate the effectiveness of our network and its ability to generate fine-grained point clouds.
C1 [Li, Jianliang; Zhang, Jinming; Zhang, Xiaohai; Chen, Ming] Xinjiang Univ, Sch Informat Sci & Engn, 777 Huarui St, Urumqi 830046, Xinjiang, Peoples R China.
C3 Xinjiang University
RP Zhang, JM (corresponding author), Xinjiang Univ, Sch Informat Sci & Engn, 777 Huarui St, Urumqi 830046, Xinjiang, Peoples R China.
EM 107552101306@stu.xju.edu.cn; zhjmpt@163.com; hello_world@stu.xju.edu.cn;
   107552103603@stu.xju.edu.cn
OI Zhang, Jinming/0000-0003-1426-099X
FU Natural Science Foundation of Xinjiang Uygur Autonomous Region; 
   [2022D01C690]
FX This work was sponsored by the Natural Science Foundation of Xinjiang
   Uygur Autonomous Region (Project number: 2022D01C690).
CR Agarwal S, 2017, IEEE IMAGE PROC, P2199, DOI 10.1109/ICIP.2017.8296672
   Chen SH, 2021, IEEE SIGNAL PROC MAG, V38, P68, DOI 10.1109/MSP.2020.2984780
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Engel N, 2021, IEEE ACCESS, V9, P134826, DOI 10.1109/ACCESS.2021.3116304
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Han XG, 2017, IEEE I CONF COMP VIS, P85, DOI 10.1109/ICCV.2017.19
   Huang TX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12488, DOI 10.1109/ICCV48922.2021.01228
   Li CX, 2024, VISUAL COMPUT, V40, P5503, DOI 10.1007/s00371-023-03118-z
   Liu MH, 2020, AAAI CONF ARTIF INTE, V34, P11596
   Liu Q, 2022, VISUAL COMPUT, V38, P3341, DOI 10.1007/s00371-022-02550-x
   Pan L, 2021, PROC CVPR IEEE, P8520, DOI 10.1109/CVPR46437.2021.00842
   Pan L, 2020, IEEE ROBOT AUTOM LET, V5, P4392, DOI 10.1109/LRA.2020.2994483
   Qi CR, 2017, ADV NEUR IN, V30
   Qiu S, 2023, LECT NOTES COMPUT SC, V13841, P326, DOI 10.1007/978-3-031-26319-4_20
   Schreiberhuber S, 2019, IEEE INT CONF ROBOT, P140, DOI [10.1109/ICRA.2019.8793654, 10.1109/icra.2019.8793654]
   Su ZJ, 2023, COMPUT VIS MEDIA, V9, P71, DOI 10.1007/s41095-022-0276-6
   Tchapmi LP, 2019, PROC CVPR IEEE, P383, DOI 10.1109/CVPR.2019.00047
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang J., 2022, arXiv
   Wang SY, 2024, VISUAL COMPUT, V40, P5241, DOI 10.1007/s00371-023-03103-6
   Wang XG, 2020, PROC CVPR IEEE, P787, DOI 10.1109/CVPR42600.2020.00087
   Wang YD, 2022, INT J COMPUT VISION, V130, P1145, DOI 10.1007/s11263-022-01588-7
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wen X, 2023, IEEE T PATTERN ANAL, V45, P852, DOI 10.1109/TPAMI.2022.3159003
   Wen X, 2021, PROC CVPR IEEE, P7439, DOI 10.1109/CVPR46437.2021.00736
   Wen X, 2020, PROC CVPR IEEE, P1936, DOI 10.1109/CVPR42600.2020.00201
   Chang AX, 2015, Arxiv, DOI [arXiv:1512.03012, DOI 10.48550/ARXIV.1512.03012]
   Xiang P, 2023, IEEE T PATTERN ANAL, V45, P6320, DOI 10.1109/TPAMI.2022.3217161
   Xiao BL, 2022, VISUAL COMPUT, V38, P4373, DOI 10.1007/s00371-021-02301-4
   Xie H., 2020, EUROPEAN C COMPUTER, P365, DOI DOI 10.1007/978-3-030-58545-721
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yin KX, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661241
   Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295
   Yu XM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12478, DOI 10.1109/ICCV48922.2021.01227
   Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088
   Zhang W., 2020, Computer Vision-ECCV 2020, P512, DOI 10.1007/978-3-030-58595-2_31
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhou HR, 2022, LECT NOTES COMPUT SC, V13663, P416, DOI 10.1007/978-3-031-20062-5_24
   Zitian Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7659, DOI 10.1109/CVPR42600.2020.00768
NR 42
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 2
PY 2024
DI 10.1007/s00371-024-03364-9
EA MAY 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PS1T7
UT WOS:001215987700001
DA 2024-08-05
ER

PT J
AU Balreira, DG
   da Silveira, TLT
AF Balreira, Dennis G.
   da Silveira, Thiago L. T.
TI Triangular matrix-based lossless compression algorithm for 3D mesh
   connectivity
SO VISUAL COMPUTER
LA English
DT Article
DE Mesh compression; Connectivity encoding; Triangle meshes; Modeling;
   Computer graphics
ID COMPACT; REPRESENTATION
AB Three-dimensional mesh compression is vital to support advances in many scenarios, such as 3D web-based applications. Existing 3D mesh methods usually require complex data structures and time-consuming processing. Given a mesh represented by its vertices and triangular faces, we present a novel, fast, and straightforward encoding algorithm. Our method encodes the mesh connectivity data based on an upper triangular matrix which is easily recovered by its underlying decoding process. Our technique encodes the mesh edges in linear time without losing any face in the process. Results show that our method provides a connectivity compression rate of 55.29 and an average total compression rate of 27.09. Furthermore, our approach achieves, on average, a similar compressing rate of state-of-the-art algorithms, such as OpenCTM, which considers geometry and connectivity, while our approach considers only their connectivity.
C1 [Balreira, Dennis G.; da Silveira, Thiago L. T.] Univ Fed Rio Grande do Sul, Inst Informat, Ave Bento Goncalves 9500, BR-91501970 Porto Alegre, RS, Brazil.
C3 Universidade Federal do Rio Grande do Sul
RP Balreira, DG (corresponding author), Univ Fed Rio Grande do Sul, Inst Informat, Ave Bento Goncalves 9500, BR-91501970 Porto Alegre, RS, Brazil.
EM dgbalreira@inf.ufrgs.br; tltsilveira@inf.ufrgs.br
RI ; da Silveira, Thiago/C-7925-2018
OI Balreira, Dennis/0000-0002-0801-9393; da Silveira,
   Thiago/0000-0001-6788-2667
FU Coordenao de Aperfeioamento de Pessoal de Nvel Superior - Brasil
   (CAPES); Fundao de Amparo Pesquisa do Estado do Rio Grande do Sul
   (FAPERGS)
FX This study was partially financed by the Coordenac & atilde;o de
   Aperfeicoamento de Pessoal de Nivel Superior - Brasil (CAPES) - Finance
   Code 001 and Fundac & atilde;o de Amparo a Pesquisa do Estado do Rio
   Grande do Sul (FAPERGS).
CR Alliez P, 2005, MATH VIS, P3, DOI 10.1007/3-540-26808-1_1
   [Anonymous], COMMON 3D TEST MODEL
   [Anonymous], AUTODESK 3DS MAX
   Bell T, 2017, APPL OPTICS, V56, P9285, DOI 10.1364/AO.56.009285
   Botsch M., 2006, ACM SIGGRAPH 2006 CO, P1
   Botsch M., 2010, Polygon Mesh Processing, DOI DOI 10.1201/B10688
   Cao C, 2019, PROCEEDINGS WEB3D 2019: THE 24TH INTERNATIONAL ACM CONFERENCE ON 3D WEB TECHNOLOGY, DOI 10.1145/3329714.3338130
   Catmull E., 1998, SEMINAL GRAPHICS, P183, DOI DOI 10.1145/280811.280992
   Cignoni P., 2008, P EUR IT CHAPT C, V2008, P129, DOI [10.2312/LocalChapterEvents/ItalChap/ItalianChapConf2008, DOI 10.2312/LOCALCHAPTEREVENTS/ITALCHAP/ITALIANCHAPCONF2008, DOI 10.2312/LOCALCHAPTEREVENTS/ITALCHAP/ITALIANCHAPCONF2008/129-136]
   Dong YH, 2023, INFORMATION, V14, DOI 10.3390/info14030184
   Finley MG, 2021, OPT LASER ENG, V138, DOI 10.1016/j.optlaseng.2020.106457
   Fisher Y., 1996, VRML, P443
   Foley J.D., 1996, Computer graphics: Principles and practice
   Galligan F., 2018, GOOGLEDRACO LIB COMP
   Geelnard M., 2010, OPENCTM
   Guarda AFR, 2021, IEEE J-STSP, V15, P415, DOI 10.1109/JSTSP.2020.3047520
   Gurung T, 2013, COMPUT AIDED DESIGN, V45, P262, DOI 10.1016/j.cad.2012.10.009
   Gurung T, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964962
   Gurung T, 2011, COMPUT GRAPH FORUM, V30, P355, DOI 10.1111/j.1467-8659.2011.01866.x
   Hooda R, 2022, IEEE SOUTHEASTCON, P522, DOI 10.1109/SoutheastCon48659.2022.9763998
   Huang TX, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555481
   Isenburg M, 2005, IEEE Visualization 2005, Proceedings, P231
   LALOS AS, 2018, IEEE INT C MULTIMEDI, P1
   Luffel M, 2014, IEEE T VIS COMPUT GR, V20, P84, DOI 10.1109/TVCG.2013.81
   Maglo A, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2693443
   McHenry K., 2008, NATL CTR SUPERCOMPUT, V1205, P22
   Michalik K, 2023, COMPUT SCI-AGH, V24, P473, DOI 10.7494/csci.2023.24.4.6036
   Nehmé Y, 2021, IEEE T VIS COMPUT GR, V27, P2202, DOI 10.1109/TVCG.2020.3036153
   Pang Jiahao, 2022, APCCPA '22: Proceedings of the 1st International Workshop on Advances in Point Cloud Compression, Processing and Analysis, P11, DOI 10.1145/3552457.3555727
   Peng JL, 2005, J VIS COMMUN IMAGE R, V16, P688, DOI 10.1016/j.jvcir.2005.03.001
   Ponchio F, 2015, WEB3D 2015, P199, DOI 10.1145/2775292.2775308
   Quach M, 2022, FRONT SIGNAL PROC-SW, V2, DOI 10.3389/frsip.2022.846972
   ROBINSON AH, 1967, PR INST ELECTR ELECT, V55, P356, DOI 10.1109/PROC.1967.5493
   Roscoe L., 1988, Am Syst, V27, P10
   Salinas-Fernández S, 2022, ENG COMPUT-GERMANY, V38, P4545, DOI 10.1007/s00366-022-01643-4
   Shirley P., 2009, Fundamentals of Computer Graphics, V3, DOI [10.1201/9781439865521, DOI 10.1201/9781439865521]
   Siddeq MM, 2016, 3D RES, V7, DOI 10.1007/s13319-016-0091-x
   Technologies W., 1995, WAVEFRONT ADV VISUAL
NR 38
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 3961
EP 3970
DI 10.1007/s00371-024-03400-8
EA APR 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001229927000001
DA 2024-08-05
ER

PT J
AU Cheema, Y
   Cheema, MN
   Nazir, A
   Khokhar, FA
   Li, P
   Ahmed, A
AF Cheema, Yasmeen
   Cheema, Muhammad Nadeem
   Nazir, Anam
   Khokhar, Fahad Ahmed
   Li, Ping
   Ahmed, Ayaz
TI A novel approach for improving open scene text translation with modified
   GAN
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Text detection; Image to text; Text recoginition; Deep learning-based
   translation; Text elimination; Text translation; Generative adversarial
   network (GAN); Convolutional neural network (CNN)
ID RECOGNITION
AB Text, as a vital tool for communication, is playing an imperative role in modern society. Precise high-level text translation systems are essential requirements in a wide range of real-world applications, such as robot navigation, industrial automation, image search, and instant translation. Regardless of improved research, a series of grand challenges may still become upon when translating text automatically in the real-world from open scene images. The difficulties mainly stem from multiplicity and inconsistency of text in open scenes, complication and obstruction of backgrounds, and deficient imaging conditions in uncontrolled circumstances for open scene images. The existing deep learning-based text translation systems do not eliminate the text for translation, and these applications just replace text on the reconstructed scene. To address the abovementioned shortcomings, this study proposed a novel approach for open scene text translation. Our system consists of five modules including scene text detection, text recognition, text elimination, text translation, and text insertion along with scene reconstruction. The novelty presented by our model lies in the idea of first eliminating the text from the open scene for accurate translation and then reconstructs the translated text on the image for its proper alignment. We specifically modified the existing generative adversarial network (GAN) architecture for improved performance of text elimination by introducing a novel strategy of text and scene concatenation to reduce the overall loss function. For this purpose, we created a synthetic dataset to train our GAN for text elimination module. Experiments on various standard text translation systems demonstrate that our integrated system is able to outperform state-of-the-art approaches in terms of result quality. We have achieved 90.87% of precision, 83.66% of recall, 87.116% of F1-score, and reduced both losses ( l 1 \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$l_1$$\end{document} and l 2 \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$l_2$$\end{document} ) up to 50% which is remarkable upon state-of-the-art translation systems.
C1 [Cheema, Yasmeen] Natl Univ Sci & Technol, Sch Interdisciplinary Engn & Sci SINES, Islamabad, Pakistan.
   [Cheema, Muhammad Nadeem] COMSATS Univ Islamabad, Dept Comp Sci, Attock, Pakistan.
   [Nazir, Anam; Ahmed, Ayaz] COMSATS Univ Islamabad, Dept Comp Sci, Wah Cantt, Pakistan.
   [Khokhar, Fahad Ahmed] Univ Florence, Dept Math & Informat, Florence, Italy.
   [Li, Ping] Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong, Peoples R China.
   [Li, Ping] HongKong Polytech Univ, Sch Design, Kowloon, Hong Kong, Peoples R China.
C3 National University of Sciences & Technology - Pakistan; COMSATS
   University Islamabad (CUI); COMSATS University Islamabad (CUI);
   University of Florence; Hong Kong Polytechnic University; Hong Kong
   Polytechnic University
RP Khokhar, FA (corresponding author), Univ Florence, Dept Math & Informat, Florence, Italy.
EM ycheema.phdcse16@rcms.nust.edu.pk; itscheema786@yahoo.com;
   itsanam786@yahoo.com; fahadahmed.khokhar@unifi.it; p.li@polyu.edu.hk;
   ayazahmed201819@gmail.com
FU Universit degli Studi di Firenze
FX No Statement Available
CR [Anonymous], 2023, Opus dataset
   [Anonymous], 2023, Sogou translate
   [Anonymous], 2023, Google translator (app)
   [Anonymous], 2023, Eudic (european dictionary)
   [Anonymous], 2023, Baidu translate
   [Anonymous], 2023, Imagemagick: Convert, edit, and compose images
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bai X, 2016, IEEE T IMAGE PROCESS, V25, P2789, DOI 10.1109/TIP.2016.2555080
   Bartz C, 2017, Arxiv, DOI arXiv:1712.05404
   Bertalmio M., 2001, Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001, pI, DOI 10.1109/CVPR.2001.990497
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Dekel T, 2017, PROC CVPR IEEE, P6864, DOI 10.1109/CVPR.2017.726
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gupta A, 2016, Arxiv, DOI arXiv:1604.06646
   Hanson J, 2018, BIOINFORMATICS, V34, P4039, DOI 10.1093/bioinformatics/bty481
   He KM, 2015, Arxiv, DOI [arXiv:1512.03385, DOI 10.48550/ARXIV.1512.03385]
   He T, 2016, IEEE T IMAGE PROCESS, V25, P2529, DOI 10.1109/TIP.2016.2547588
   He WH, 2018, IEEE T IMAGE PROCESS, V27, P5406, DOI 10.1109/TIP.2018.2855399
   Huang MX, 2022, PROC CVPR IEEE, P4583, DOI 10.1109/CVPR52688.2022.00455
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Karaoglu S, 2017, IEEE T IMAGE PROCESS, V26, P3965, DOI 10.1109/TIP.2017.2707805
   KhoKhar FA, 2022, COMPUT ELECTR ENG, V99, DOI 10.1016/j.compeleceng.2022.107818
   Klein G, 2017, Arxiv, DOI arXiv:1701.02810
   Koo HI, 2016, IEEE T IMAGE PROCESS, V25, P5358, DOI 10.1109/TIP.2016.2607418
   Kumar P, 2022, NEURAL NETWORKS, V150, P392, DOI 10.1016/j.neunet.2022.03.017
   Liao JT, 2017, IEEE T IMAGE PROCESS, V26, P1089, DOI 10.1109/TIP.2016.2636661
   Liao MH, 2018, IEEE T IMAGE PROCESS, V27, P3676, DOI 10.1109/TIP.2018.2825107
   Liu FY, 2015, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2015.7299152
   Long SB, 2020, Arxiv, DOI arXiv:1811.04256
   Lu SY, 2023, INT J COMPUT INT SYS, V16, DOI 10.1007/s44196-023-00233-6
   Mustafa A, 2019, IEEE T IMAGE PROCESS, V28, P1118, DOI 10.1109/TIP.2018.2872906
   Neumann L, 2016, IEEE T PATTERN ANAL, V38, P1872, DOI 10.1109/TPAMI.2015.2496234
   openai, 2023, about us
   Osher S, 2005, MULTISCALE MODEL SIM, V4, P460, DOI 10.1137/040605412
   Pathak D, 2016, Arxiv, DOI [arXiv:1604.07379, 10.48550/arXiv.1604.07379, DOI 10.48550/ARXIV.1604.07379]
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Rong XJ, 2020, IEEE T IMAGE PROCESS, V29, P591, DOI 10.1109/TIP.2019.2930176
   Shen SH, 2013, IEEE T IMAGE PROCESS, V22, P1901, DOI 10.1109/TIP.2013.2237921
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   Tamilselvi M., 2022, 2022 INT C ADV COMPU, P1
   Tang Y, 2017, IEEE T IMAGE PROCESS, V26, P994, DOI [10.1109/TIP.2016.2639440, 10.1109/TIP.2017.2656474]
   Telea A., 2004, Journal of Graphics Tools, V9, P23, DOI 10.1080/10867651.2004.10487596
   Xu YC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2900589
   Yang C., 2016, arXiv
   Yang C, 2017, IEEE T IMAGE PROCESS, V26, P3235, DOI 10.1109/TIP.2017.2695104
   Yao C, 2016, Arxiv, DOI arXiv:1606.09002
   Yao C, 2014, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR.2014.515
   Youdao, 2023, about us
   Yu JH, 2018, Arxiv, DOI [arXiv:1801.07892, 10.48550/arXiv.1801.07892]
   Zhou XY, 2017, Arxiv, DOI [arXiv:1704.03155, 10.48550/ARXIV.1704.03155, DOI 10.48550/ARXIV.1704.03155]
NR 50
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 13
PY 2024
DI 10.1007/s00371-024-03371-w
EA APR 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NO7T5
UT WOS:001201464900002
OA hybrid
DA 2024-08-05
ER

PT J
AU Taheri, F
   Rahbar, K
   Beheshtifard, Z
AF Taheri, Fatemeh
   Rahbar, Kambiz
   Beheshtifard, Ziaeddin
TI Content-based image retrieval through fusion of deep features extracted
   from segmented neutrosophic using depth map
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Content-based image retrieval; Feature fusion; Neutrosophic set; Depth
   map estimation
AB The main challenge of content-based image retrieval systems is the difference between how images are described using algorithms and how humans understand the semantic concepts of an image. To overcome this challenge, many image retrieval methods have focused on scenarios that emphasize important regions of an image. However, losing part of the semantic features of an image is a problem that also exists in these approaches. Therefore, this article introduces a method for image retrieval using the fusion of deep features on a segmented neutrosophic set with the help of the image depth map. By transferring the original image to the neutrosophic domain, the image is decomposed into three levels: true, false, and indeterminate. True and false images have different representations of image brightness. The indeterminate image represents the boundary between the true and false images. It is also a representation of the edges in the image. Convolutional layers of deep neural networks are sensitive to changes in image brightness when extracting feature maps. For this reason, the extracted features from the true and false images are different from each other and can be considered as complementary to each other. In the second step, the image depth map is estimated using a vision transformer. Then the estimated depth map is binarized using a predefined threshold. By applying the binarized depth map to the neutrosophic domain, objects in near and far regions are classified. Effective features of each region are extracted using a pre-trained deep neural network, VGG-16. Important features from each group of images are selected using the Boruta-Shap algorithm. Finally, to reduce redundancy and unify the extracted features, feature fusion is performed in two stages, resulting in the final feature vector for each image. Experimental results confirm that extracting semantic and content features from different regions of an image using the proposed method leads to improved retrieval results and reduces semantic gaps.
C1 [Taheri, Fatemeh; Rahbar, Kambiz; Beheshtifard, Ziaeddin] Islamic Azad Univ, Dept Comp Engn, South Tehran Branch, Tehran, Iran.
C3 Islamic Azad University
RP Rahbar, K (corresponding author), Islamic Azad Univ, Dept Comp Engn, South Tehran Branch, Tehran, Iran.
EM k_rahbar@azad.ac.ir
CR Aich S, 2021, IEEE INT CONF ROBOT, P11746, DOI 10.1109/ICRA48506.2021.9560885
   Alsmadi MK, 2020, ARAB J SCI ENG, V45, P3317, DOI 10.1007/s13369-020-04384-y
   Bai C, 2018, NEUROCOMPUTING, V303, P60, DOI 10.1016/j.neucom.2018.04.034
   Bai C, 2018, J VIS COMMUN IMAGE R, V50, P199, DOI 10.1016/j.jvcir.2017.11.021
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Datta S., 2023, Decis. Anal. J, V7, P100223, DOI [10.1016/j.dajour.2023.100223, DOI 10.1016/J.DAJOUR.2023.100223]
   Dhar Soumyadip, 2021, Applied Soft Computing, V112, DOI 10.1016/j.asoc.2021.107759
   Dong RS, 2019, MATH PROBL ENG, V2019, DOI 10.1155/2019/9794202
   Eisa M., 2014, Int. J. Comput. Appl, V95, P12, DOI [10.5120/16613-6453, DOI 10.5120/16613-6453]
   Ghosh I, 2022, EXPERT SYST APPL, V210, DOI 10.1016/j.eswa.2022.118391
   Gkelios S, 2021, EXPERT SYST APPL, V177, DOI 10.1016/j.eswa.2021.114940
   Gonzalez-Garcia A, 2018, INT J COMPUT VISION, V126, P476, DOI 10.1007/s11263-017-1048-0
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang L, 2022, MULTIMEDIA SYST, V28, P673, DOI 10.1007/s00530-021-00866-7
   Jain A, 2007, LECT NOTES COMPUT SC, V4842, P255
   Janssens B., 2022, Ann. Op. Res, DOI [10.1007/S10479-021-04476-4/TABLES/10, DOI 10.1007/S10479-021-04476-4/TABLES/10]
   Ji P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12767, DOI 10.1109/ICCV48922.2021.01255
   Khan S., 2018, Synthesis Lectures on Computer Vision, V8, P1, DOI [DOI 10.1007/978-3-031-01821-3, DOI 10.2200/S00822ED1V01Y201712COV015, 10.2200/S00822ED1V01Y201712COV015]
   Kursa MB, 2010, J STAT SOFTW, V36, P1, DOI 10.18637/jss.v036.i11
   Lee T, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10121402
   Li YZ, 2022, COMPUT VIS MEDIA, V8, P631, DOI 10.1007/s41095-022-0279-3
   Liu GH, 2024, PATTERN RECOGN, V147, DOI 10.1016/j.patcog.2023.110076
   Liu GH, 2023, INT J MACH LEARN CYB, V14, P483, DOI 10.1007/s13042-022-01645-0
   Lu F, 2023, COGN COMPUT, V15, P1736, DOI 10.1007/s12559-023-10143-6
   Lu F, 2022, DIGIT SIGNAL PROCESS, V123, DOI 10.1016/j.dsp.2022.103457
   Lu Z, 2023, INT J MACH LEARN CYB, V14, P643, DOI 10.1007/s13042-022-01654-z
   Lundberg SM, 2020, NAT MACH INTELL, V2, P56, DOI 10.1038/s42256-019-0138-9
   Mohite NB, 2022, MULTIMED TOOLS APPL, V81, P11379, DOI 10.1007/s11042-022-12085-x
   Mopuri Konda Reddy, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P62, DOI 10.1109/CVPRW.2015.7301273
   Pang SM, 2018, PATTERN RECOGN, V83, P150, DOI 10.1016/j.patcog.2018.05.010
   Philbin J, 2008, PROC CVPR IEEE, P2285
   Pradhan J, 2022, J VIS COMMUN IMAGE R, V83, DOI 10.1016/j.jvcir.2021.103396
   Qiao YG, 2019, IEEE T MULTIMEDIA, V21, P1, DOI 10.1109/TMM.2018.2845699
   Rahman M., 2023, Content based image retrieval using depth maps for colonoscopy images, P301, DOI [10.5220/0011749100003414, DOI 10.5220/0011749100003414]
   Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196
   Ranftl R, 2022, IEEE T PATTERN ANAL, V44, P1623, DOI 10.1109/TPAMI.2020.3019967
   Sezavar A, 2019, MULTIMED TOOLS APPL, V78, P20895, DOI 10.1007/s11042-019-7321-1
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smarandache F, 1999, A unifying field in logics neutrosophy: neutrosophic probability, set and logic, P1
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Taheri F, 2022, MULTIMED TOOLS APPL, DOI 10.1007/s11042-022-13670-w
   Unar S, 2019, KNOWL-BASED SYST, V179, P8, DOI 10.1016/j.knosys.2019.05.001
   Wang HX, 2020, PATTERN RECOGN LETT, V130, P64, DOI 10.1016/j.patrec.2018.08.010
   Wang XD, 2023, IEEE T MULTIMEDIA, V25, P9597, DOI 10.1109/TMM.2023.3256092
   Wang YW., 2023, Neural Process. Lett, DOI [10.1007/S11063-023-11297-Y/FIGURES/10, DOI 10.1007/S11063-023-11297-Y/FIGURES/10]
   Wei XS, 2017, IEEE T IMAGE PROCESS, V26, P2868, DOI 10.1109/TIP.2017.2688133
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhan ZQ, 2020, IEEE ACCESS, V8, P21524, DOI 10.1109/ACCESS.2020.2969287
   Zhang BB, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107167
   Zhou YH, 2021, IEEE INT CONF ROBOT, P5237, DOI 10.1109/ICRA48506.2021.9560987
   Zhou ZY, 2020, J VIS COMMUN IMAGE R, V72, DOI 10.1016/j.jvcir.2020.102860
NR 51
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 9
PY 2024
DI 10.1007/s00371-024-03335-0
EA APR 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NG1R9
UT WOS:001199213300002
DA 2024-08-05
ER

PT J
AU Tang, HA
   Chen, SH
   Liu, Y
   Wang, SY
   Chen, ZY
   Hu, XL
AF Tang, Haonan
   Chen, Shuhan
   Liu, Yang
   Wang, Shiyu
   Chen, Zeyu
   Hu, Xuelong
TI Boundary-aware dichotomous image segmentation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Dichotomous image segmentation; Feature Pyramid Transfer; Boundary-aware
   Bi-resolution Fusion; High resolution
ID SALIENT OBJECT DETECTION
AB Dichotomous Image Segmentation is a category-agnostic task aims to segment highly accurate objects from natural images. In semantic segmentation tasks, the utilization of high-resolution input, due to its richer contextual information, can effectively enhance the precision of segmentation and the accuracy of the boundaries. However, in DIS tasks, owing to the increased complexity and diversity of the targets, it is challenging to generate complete segmentation results. Directly employing high-resolution images as input may result in missed detection due to insufficient receptive field coverage. Furthermore, the additional potential details introduced by high resolution, which may not directly relate to the targets, can negatively impact the accuracy of the model's boundary predictions. To address the above problems, a dual-branch network structure is adopted, where the high-resolution input branch learns detailed information, and the low-resolution input branch captures global semantic information. Specifically, we use the Feature Pyramid Transfer module to enlarge the receptive field and enhance the semantic consistency between different Resnet blocks. In the decoder, we propose the Boundary-Aware structure to fuse features from different backbones and use boundary information to generate more accurate segmentation results. The experimental results demonstrate that our method achieves leading performance in four of the six evaluation metrics used in experiments on the complete DIS-TE testset. For instance, the F-measure reaches 0.801, which is 10.3% higher than that of the ISNet, the baseline method for the DIS task.
C1 [Tang, Haonan; Chen, Shuhan; Liu, Yang; Wang, Shiyu; Chen, Zeyu; Hu, Xuelong] Yangzhou Univ, Sch Informat Engn, Yangzhou, Peoples R China.
C3 Yangzhou University
RP Chen, SH (corresponding author), Yangzhou Univ, Sch Informat Engn, Yangzhou, Peoples R China.
EM smallmoho@foxmail.com; shchen@yzu.edu.cn; 15565048650@163.com;
   13921944403@163.com; Czy1239858139@163.com; xlhu@yzu.edu.cn
FU Natural Science Foundation of China; Yangzhou University "Qinglan
   Project; Yangzhou University Science and Technology innovation venture
   Fund;  [61802336];  [62205283]
FX This work is partially supported by the Natural Science Foundation of
   China (No. 61802336, No. 62205283), Yangzhou University "Qinglan
   Project" and the Yangzhou University Science and Technology innovation
   venture Fund.
CR Bo Li L. T., 2021, P IEEE CVF INT C COM, P3580
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Borji A, 2019, COMPUT VIS MEDIA, V5, P117, DOI 10.1007/s41095-019-0149-9
   Chen H, 2019, PATTERN RECOGN, V86, P376, DOI 10.1016/j.patcog.2018.08.007
   Chen SH, 2020, IEEE T IMAGE PROCESS, V29, P3763, DOI 10.1109/TIP.2020.2965989
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Fan DP, 2018, Arxiv, DOI arXiv:1805.10421
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P6024, DOI 10.1109/TPAMI.2021.3085766
   Fan DP, 2020, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR42600.2020.00285
   Freixenet J, 2002, LECT NOTES COMPUT SC, V2352, P408, DOI 10.1007/3-540-47977-5_27
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu QX, 2023, PROC CVPR IEEE, P7422, DOI 10.1109/CVPR52729.2023.00717
   Hu XL, 2022, INT J COMPUT VISION, V130, P2571, DOI 10.1007/s11263-022-01662-0
   Hussain T, 2022, IEEE COMPUT SOC CONF, P2877, DOI 10.1109/CVPRW56347.2022.00325
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li FD, 2023, INT J MACH LEARN CYB, V14, P387, DOI 10.1007/s13042-021-01496-1
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Li YJ, 2022, IEEE T INTELL TRANSP, V23, P25419, DOI 10.1109/TITS.2022.3141107
   Liew JH, 2021, IEEE WINT CONF APPL, P305, DOI 10.1109/WACV48630.2021.00035
   Lin X., 2021, IEEE Trans. Multimed, V1, P1
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Mei HY, 2021, PROC CVPR IEEE, P8768, DOI 10.1109/CVPR46437.2021.00866
   Nirkin Y, 2021, PROC CVPR IEEE, P4060, DOI 10.1109/CVPR46437.2021.00405
   Oktay O, 2018, Arxiv, DOI [arXiv:1804.03999, DOI 10.48550/ARXIV.1804.03999]
   Pang YW, 2022, PROC CVPR IEEE, P2150, DOI 10.1109/CVPR52688.2022.00220
   Pei Jialun, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P2139, DOI 10.1145/3581783.3611811
   Qi CR, 2021, PROC CVPR IEEE, P6130, DOI 10.1109/CVPR46437.2021.00607
   Qin XB, 2022, LECT NOTES COMPUT SC, V13678, P38, DOI 10.1007/978-3-031-19797-0_3
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shen TC, 2022, PROC CVPR IEEE, P1300, DOI 10.1109/CVPR52688.2022.00137
   Sun S., 2023, IEEE Trans. Pattern Anal. Mach. Intell, V1, P1
   Sun YJ, 2022, Arxiv, DOI arXiv:2207.00794
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang XL, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-34379-2
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Xie CX, 2022, PROC CVPR IEEE, P11707, DOI 10.1109/CVPR52688.2022.01142
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Yang CL, 2020, Arxiv, DOI arXiv:2012.07181
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Yu J, 2016, P 24 ACM INT C MULT, P516, DOI DOI 10.1145/2964284.2967274
   Yuan PC, 2020, Arxiv, DOI arXiv:2010.07621
   Zeng Y, 2019, IEEE I CONF COMP VIS, P7233, DOI 10.1109/ICCV.2019.00733
   Zhai Q, 2021, PROC CVPR IEEE, P12992, DOI 10.1109/CVPR46437.2021.01280
   Zhang G, 2021, PROC CVPR IEEE, P6857, DOI 10.1109/CVPR46437.2021.00679
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao JX, 2019, PROC CVPR IEEE, P3922, DOI 10.1109/CVPR.2019.00405
   Zhao X., 2020, PROC 16 EUR C COMPU, P35, DOI 10.1007/ 978-3-030-58536-5_3
   Zhou Y, 2023, PROCEEDINGS OF THE THIRTY-SECOND INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, IJCAI 2023, P1822
   Zhu QK, 2020, IEEE T MED IMAGING, V39, P753, DOI 10.1109/TMI.2019.2935018
NR 57
TC 0
Z9 0
U1 14
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 26
PY 2024
DI 10.1007/s00371-024-03295-5
EA FEB 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JA9S5
UT WOS:001170555600002
DA 2024-08-05
ER

PT J
AU Li, DJ
   Li, X
   Li, CF
AF Li, Dongjie
   Li, Xu
   Li, Changfeng
TI Embrace descriptors that use point pairs feature
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Feature matching; 3D representation; Local feature descriptor; Point
   cloud registration
ID OBJECT RECOGNITION; HISTOGRAMS; PROJECTION
AB As technology evolves, the cost of 3D scanners is falling, which makes 3D computer vision for industrial applications increasingly popular. More and more researchers have started to study 3D computer vision. Point cloud feature descriptors are a fundamental task in 3D computer vision, and descriptors that use spatial features tend to perform better than those without them. Point cloud descriptors can generally be divided into local reference frames-based (LRF-based) and local reference frames-free (LRF-free). The former uses LRFs to provide spatial features to the descriptors, while the latter uses point pair features to provide spatial features. However, the performance of those LRF-based descriptors is more affected by local reference frames (LRFs), and the descriptors with spatial information LRF-free tend to be more computationally intensive because of its point pair combination strategy. Therefore, we propose a strategy named Multi-scale Point Pair Combination Strategy (MSPPCS) that reduces the computation of point pair-based feature descriptors by nearly 70%\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\%$$\end{document} while ensuring that the performance of the descriptor is almost unaffected. We also propose a new descriptor, Spatial Feature Point Pair Histograms (SFPPH), which has excellent performance and robustness due to the diverse spatial features used. We critically evaluate the performance of our descriptor on the Bologna dataset, Kinect dataset, and UWA dataset. The experimental results show that our descriptor is the most robust and performing point cloud feature descriptor.
C1 [Li, Dongjie; Li, Xu] Harbin Univ Sci & Technol, Key Lab Complex Intelligent Syst & Integrat, Harbin 150080, Peoples R China.
   [Li, Dongjie; Li, Xu] Harbin Univ Sci & Technol, Key Lab Adv Mfg Intelligent Technol, Minist Educ, Harbin 150080, Peoples R China.
   [Li, Changfeng] Changzhou Mingseal Robot Technol Co Ltd, Changzhou 213164, Peoples R China.
C3 Harbin University of Science & Technology; Harbin University of Science
   & Technology
RP Li, DJ; Li, X (corresponding author), Harbin Univ Sci & Technol, Key Lab Complex Intelligent Syst & Integrat, Harbin 150080, Peoples R China.; Li, DJ; Li, X (corresponding author), Harbin Univ Sci & Technol, Key Lab Adv Mfg Intelligent Technol, Minist Educ, Harbin 150080, Peoples R China.
EM dongjieli@hrbust.edu.cn; c2377869241@163.com; lichangfeng@mingseal.com
CR Buch AndersGlent., 2018, BMVC, P143
   Frome A, 2004, LECT NOTES COMPUT SC, V3023, P224
   Golovinskiy A, 2009, IEEE I CONF COMP VIS, P2154, DOI 10.1109/ICCV.2009.5459471
   Gomes RB, 2013, COMPUT GRAPH-UK, V37, P496, DOI 10.1016/j.cag.2013.03.005
   Guo YL, 2016, INT J COMPUT VISION, V116, P66, DOI 10.1007/s11263-015-0824-y
   Guo YL, 2013, INT J COMPUT VISION, V105, P63, DOI 10.1007/s11263-013-0627-y
   Han XF, 2020, Arxiv, DOI arXiv:1802.02297
   Huang J, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P175, DOI 10.1109/3DV.2013.31
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Marton ZC, 2010, IEEE INT C INT ROBOT, P3700, DOI 10.1109/IROS.2010.5650434
   Matei B, 2006, IEEE T PATTERN ANAL, V28, P1111, DOI 10.1109/TPAMI.2006.148
   Mian AS, 2006, IEEE T PATTERN ANAL, V28, P1584, DOI 10.1109/TPAMI.2006.213
   Papazov C, 2012, INT J ROBOT RES, V31, P538, DOI 10.1177/0278364911436019
   Prakhya SM, 2017, IEEE ROBOT AUTOM LET, V2, P1472, DOI 10.1109/LRA.2017.2667721
   Prakhya SM, 2015, IEEE INT C INT ROBOT, P1929, DOI 10.1109/IROS.2015.7353630
   Rusu RB, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3384, DOI 10.1109/IROS.2008.4650967
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Shan Y, 2006, IEEE T PATTERN ANAL, V28, P568, DOI 10.1109/TPAMI.2006.83
   Steder B, 2011, IEEE INT CONF ROBOT, P2601, DOI 10.1109/ICRA.2011.5980187
   Su H, 2018, PROC CVPR IEEE, P2530, DOI 10.1109/CVPR.2018.00268
   Tombari F., 2010, P ACM WORKSH 3D OBJ, P57, DOI 10.1145/1877808.1877821
   Tombari F, 2011, IEEE IMAGE PROC, P809, DOI 10.1109/ICIP.2011.6116679
   Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26
   Wang XL, 2019, PROC CVPR IEEE, P4091, DOI 10.1109/CVPR.2019.00422
   White KB, 2007, RT07: IEEE/EG Symposium on Interactive Ray Tracing 2007, P129, DOI 10.1109/RT.2007.4342600
   Yang JQ, 2017, PATTERN RECOGN, V65, P175, DOI 10.1016/j.patcog.2016.11.019
   Zhao H, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107272
NR 28
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 23
PY 2024
DI 10.1007/s00371-024-03291-9
EA FEB 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IW5D0
UT WOS:001169378100001
DA 2024-08-05
ER

PT J
AU Wu, FY
   Wang, QF
   Wang, Z
   Yu, SY
   Li, YS
   Zhang, BL
   Lim, EG
AF Wu, Fangyu
   Wang, Qiufeng
   Wang, Zhao
   Yu, Siyue
   Li, Yushi
   Zhang, Bailing
   Lim, Eng Gee
TI ITContrast: contrastive learning with hard negative synthesis for
   image-text matching
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Multimodal deep learning; Data retrieval; Contrastive learning
AB Image-text matching aims to bridge vision and language so as to match the instance of one modality with the instance of another modality. Recent years have seen considerable progress in the research area by exploring local alignment between image regions and sentence words. However, there are still open questions regarding how to learn modality-invariant feature embeddings and effectively utilize hard negatives in the training set to infer more accurate matching scores. In this paper, we introduce a new approach called Image-Text Modality Contrastive Learning (abbreviated as ITContrast) for image-text matching. Our method addresses these challenges by leveraging a pre-trained vision-language model, OSCAR, which is firstly fine-tuned to obtain visual and textual features. We also introduce a hard negative synthesis module, which capitalizes on the difficulty of negative samples. This module profiles negative samples within a mini-match and generates representative embeddings that reflect their hardness in relation to the anchor sample. A novel cost function is designed to comprehensively integrate the information from positives, negatives and synthesized hard negatives. Extensive experiments on the MS COCO and Flickr30K datasets demonstrate that our approach is effective for image-text matching.
C1 [Wu, Fangyu; Wang, Qiufeng; Yu, Siyue; Li, Yushi; Lim, Eng Gee] Xian Jiaotong Liverpool Univ, Sch Adv Technol, Suzhou 215123, Jiangsu, Peoples R China.
   [Wang, Zhao] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310000, Zhejiang, Peoples R China.
   [Zhang, Bailing] Zhejiang Univ, Ningbo Innovat Ctr, Ningbo 315100, Zhejiang, Peoples R China.
C3 Xi'an Jiaotong-Liverpool University; Zhejiang University; Zhejiang
   University
RP Wu, FY (corresponding author), Xian Jiaotong Liverpool Univ, Sch Adv Technol, Suzhou 215123, Jiangsu, Peoples R China.
EM fangyu.wu02@xjtlu.edu.cn
FU Science and Technology Innovation 2025 Major Project of Ningbo; XJTLU AI
   University Research Centre, Jiangsu Province Engineering Research Centre
   of Data Science and Cognitive Computation at XJTLU [YZCXPT2022103]; SIP
   AI innovation platform [2021ZD0110505]; National Key Research and
   Development Project of China [BE2020006-4]; Jiangsu Science and
   Technology Programme [LY23F020014]; Natural Science Foundation of
   Zhejiang Province [2019B10128, 2023Z069]; Key Technology R &D Program of
   Ningbo [ZXL2023176]; Gusu Innovation and Entrepreneurship Leading
   Talents Programme
FX This work is partially supported by the XJTLU AI University Research
   Centre, Jiangsu Province Engineering Research Centre of Data Science and
   Cognitive Computation at XJTLU and SIP AI innovation platform
   (YZCXPT2022103); National Key Research and Development Project of China
   Grant (2021ZD0110505); Jiangsu Science and Technology Programme
   (BE2020006-4); Natural Science Foundation of Zhejiang Province
   (LY23F020014); The Key Technology R &D Program of Ningbo (2019B10128,
   2023Z069), Gusu Innovation and Entrepreneurship Leading Talents
   Programme (ZXL2023176).
CR Agrawal H, 2019, IEEE I CONF COMP VIS, P8947, DOI 10.1109/ICCV.2019.00904
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Chen C., 2023, IEEE T MULTIMEDIA, P1
   Chen JC, 2021, PROC CVPR IEEE, P15784, DOI 10.1109/CVPR46437.2021.01553
   Chen TL, 2020, AAAI CONF ARTIF INTE, V34, P10583
   Chen Ting, 2019, 25 AMERICAS C INFORM
   de Macedo DV, 2018, VISUAL COMPUT, V34, P337, DOI 10.1007/s00371-016-1335-8
   Desai K, 2021, PROC CVPR IEEE, P11157, DOI 10.1109/CVPR46437.2021.01101
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Diao HW, 2021, AAAI CONF ARTIF INTE, V35, P1218
   Faghri F, 2018, Arxiv, DOI arXiv:1707.05612
   Feng ZR, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1005
   Ghosh M, 2022, VISUAL COMPUT, V38, P1645, DOI 10.1007/s00371-021-02094-6
   Gordo A, 2017, PROC CVPR IEEE, P5272, DOI 10.1109/CVPR.2017.560
   Guo ZH, 2023, VISUAL COMPUT, V39, P5783, DOI 10.1007/s00371-022-02695-9
   Haoran Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P18, DOI 10.1007/978-3-030-58586-0_2
   He H., 2020, P IEEE CVF C COMP VI, P9729
   Hudson DA, 2019, PROC CVPR IEEE, P6693, DOI 10.1109/CVPR.2019.00686
   Hui Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12652, DOI 10.1109/CVPR42600.2020.01267
   Jia C, 2021, PR MACH LEARN RES, V139
   Jiang T, 2019, VISUAL COMPUT, V35, P1655, DOI 10.1007/s00371-018-1565-z
   Junkert F, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P335, DOI 10.1145/3078971.3078994
   Khosla P., 2020, Adv. Neural Inf. Process. Syst, P18661, DOI DOI 10.48550/ARXIV.2004.11362
   Kim W., 2021, PROC INT C MACH LEAR, P5583
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li G, 2020, AAAI CONF ARTIF INTE, V34, P11336
   Li JH, 2021, ADV NEUR IN, V34
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Li W, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P2592
   Li X., 2020, Lecture Notes in Computer Science, P121, DOI DOI 10.1007/978-3-030-58577-8_8
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu CX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P3, DOI 10.1145/3343031.3350869
   Liu Chunxiao, 2020, CVPR, P10918, DOI DOI 10.1109/CVPR42600.2020.01093
   Liu S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11895, DOI 10.1109/ICCV48922.2021.01170
   Mahajan D, 2018, LECT NOTES COMPUT SC, V11206, P185, DOI 10.1007/978-3-030-01216-8_12
   Miech A, 2021, PROC CVPR IEEE, P9821, DOI 10.1109/CVPR46437.2021.00970
   Pan ZX, 2023, PROC CVPR IEEE, P19275, DOI 10.1109/CVPR52729.2023.01847
   Peter Young M. H., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Qiu R, 2023, VISUAL COMPUT, V39, P2933, DOI 10.1007/s00371-022-02501-6
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sariyildiz M.B., 2020, EUROPEAN C COMPUTER, V2353, P153, DOI 10.1007/978-3-030-58598-310
   Sun B, 2023, VISUAL COMPUT, V39, P9, DOI 10.1007/s00371-021-02309-w
   Tianlang Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P549, DOI 10.1007/978-3-030-58601-0_33
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang LW, 2019, IEEE T PATTERN ANAL, V41, P394, DOI 10.1109/TPAMI.2018.2797921
   Wang SJ, 2020, IEEE WINT CONF APPL, P1497, DOI 10.1109/WACV45572.2020.9093614
   Wehrmann P, 2020, AAAI CONF ARTIF INTE, V34, P12313
   Wei JW, 2022, IEEE T PATTERN ANAL, V44, P6534, DOI 10.1109/TPAMI.2021.3088863
   Xi Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10938, DOI 10.1109/CVPR42600.2020.01095
   Xu X, 2020, IEEE T NEUR NET LEAR, V31, P5412, DOI 10.1109/TNNLS.2020.2967597
   Yan F, 2022, VISUAL COMPUT, V38, P3097, DOI 10.1007/s00371-022-02524-z
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Zellers R, 2019, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2019.00688
   Zhang HT, 2022, AAAI CONF ARTIF INTE, P3262
   Zhang K, 2022, PROC CVPR IEEE, P15640, DOI 10.1109/CVPR52688.2022.01521
   Zhang PC, 2021, PROC CVPR IEEE, P5575, DOI 10.1109/CVPR46437.2021.00553
   Zhang Q, 2020, PROC CVPR IEEE, P3533, DOI 10.1109/CVPR42600.2020.00359
   Zhang Y., 2018, P EUR C COMP VIS ECC, P686
   Zhang Y., 2022, P 7 MACHINE LEARNING, P2
   Zhou LW, 2020, AAAI CONF ARTIF INTE, V34, P13041
NR 61
TC 1
Z9 1
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 15
PY 2024
DI 10.1007/s00371-024-03274-w
EA FEB 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HV1H4
UT WOS:001162185500002
DA 2024-08-05
ER

PT J
AU Tang, RL
   Wang, WW
   Han, Y
   Feng, XC
AF Tang, Ruolan
   Wang, Weiwei
   Han, Yu
   Feng, Xiangchu
TI Grownbb: Gromov-Wasserstein learning of neural best buddies for
   cross-domain correspondence
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Cross-domain correspondence; Optimal transport; Neural best buddies;
   Gromov-Wasserstein distance
ID IMAGE; SCALE; FEATURES
AB Identifying pixel correspondences between two images is a fundamental task in computer vision, and has been widely used for 3D reconstruction, image morphing, and image retrieval. The neural best buddies (NBB) finds sparse correspondences between cross-domain images, which have semantically related local structures, though could be quite different in semantics as well as appearances. This paper presents a new method for cross-domain image correspondence, called GroWNBB, by incorporating the Gromov-Wasserstein learning into the NBB framework. Specifically, we utilize the NBB as the backbone to search feature matching from deep layer and propagate to low layer. While for each layer, we modify the strategy of NBB by further mapping the matching pairs obtained from the NBB within and across images into graphs, then formulate the matches as optimal transport between graphs, and use Gromov-Wasserstein learning to establish matches between these graphs. Consequently, our approach considers the relationships between images as well as the relationships within images, which makes the correspondence more stable. Our experiments demonstrate that GroWNBB achieves state-of-the-art performance on cross-domain correspondence and outperforms other popular methods in intra-class and same object correspondence estimation. Our code is available at https://github.com/NolanInLowland/GroWNBB.
C1 [Tang, Ruolan; Wang, Weiwei; Feng, Xiangchu] Xidian Univ, Sch Math & Stat, Xian 710071, Shaanxi, Peoples R China.
   [Han, Yu] Shenzhen Univ, Sch Math Sci, Shenzhen 518060, Guangdong, Peoples R China.
C3 Xidian University; Shenzhen University
RP Wang, WW (corresponding author), Xidian Univ, Sch Math & Stat, Xian 710071, Shaanxi, Peoples R China.
EM nolandora@163.com; wwwang@mail.xidian.edu.cn; hany@szu.edu.cn;
   xcfeng@mail.xidian.edu.cn
RI wang, weiwei/AAI-2245-2020
FU National Natural Science Foundation of China [61972264, 62072312,
   62372302]; National Natural Science Foundation of China
   [20200807165235002]; Natural Science Foundation of Shenzhen
FX The authors would like to thank the editors and the anonymous reviewers
   for their constructive comments and suggestions. This paper is supported
   by the National Natural Science Foundation of China (Grant Nos.
   61972264, 62072312, 62372302) and Natural Science Foundation of Shenzhen
   (Grant No. 20200807165235002).
CR Aberman K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201332
   Agrawal M, 2008, LECT NOTES COMPUT SC, V5305, P102, DOI 10.1007/978-3-540-88693-8_8
   Alcantarilla PF, 2012, LECT NOTES COMPUT SC, V7577, P214, DOI 10.1007/978-3-642-33783-3_16
   Balntas V., 2016, BMVC, V1, P3, DOI DOI 10.5244/C.30.119
   Barroso-Laguna A, 2023, IEEE T PATTERN ANAL, V45, P698, DOI 10.1109/TPAMI.2022.3145820
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bronstein AM, 2010, INT J COMPUT VISION, V89, P266, DOI 10.1007/s11263-009-0301-6
   Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56
   Cho Y, 2021, IEEE ACCESS, V9, P43898, DOI 10.1109/ACCESS.2021.3065014
   DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060
   Dusmanu M, 2019, PROC CVPR IEEE, P8084, DOI 10.1109/CVPR.2019.00828
   Fan JW, 2023, VISUAL COMPUT, V39, P319, DOI 10.1007/s00371-021-02331-y
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gao Y, 2023, IEEE T PATTERN ANAL, V45, P14404, DOI 10.1109/TPAMI.2023.3307889
   Harris C, 1988, A combined corner and edge detector, P147, DOI [10.5244/C.2.23.23.1-23.6, DOI 10.5244/C.2.23]
   Heinly J, 2015, PROC CVPR IEEE, P3287, DOI 10.1109/CVPR.2015.7298949
   Hong S., 2021, C NEURAL INFORM PROC, DOI [10.48550/arXiv.2106.02520, DOI 10.48550/ARXIV.2106.02520]
   Kim Sunnie S. Y., 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P246, DOI 10.1007/978-3-030-58574-7_15
   LINDEBERG T, 1993, INT J COMPUT VISION, V11, P283, DOI 10.1007/BF01469346
   Liu XC, 2020, Arxiv, DOI arXiv:2007.05471
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Memoli Facundo, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P256, DOI 10.1109/ICCVW.2009.5457690
   Memoli Facundo, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563074
   Mémoli F, 2011, FOUND COMPUT MATH, V11, P417, DOI 10.1007/s10208-011-9093-5
   Mishchuk A., 2017, ADV NEURAL INFORM PR, V30, P4826, DOI DOI 10.48550/ARXIV.1705.10872
   Noh H, 2017, IEEE I CONF COMP VIS, P3476, DOI 10.1109/ICCV.2017.374
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Ono Y, 2018, ADV NEUR IN, V31
   Radenovic F, 2018, PROC CVPR IEEE, P5706, DOI 10.1109/CVPR.2018.00598
   Rosten E, 2010, IEEE T PATTERN ANAL, V32, P105, DOI 10.1109/TPAMI.2008.275
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499
   Schaefer S, 2006, ACM T GRAPHIC, V25, P533, DOI 10.1145/1141911.1141920
   Shen ZW, 2024, VISUAL COMPUT, V40, P1327, DOI 10.1007/s00371-023-02851-9
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smith SM, 1997, INT J COMPUT VISION, V23, P45, DOI 10.1023/A:1007963824710
   Tian YR, 2017, PROC CVPR IEEE, P6128, DOI 10.1109/CVPR.2017.649
   Truong P, 2021, PROC CVPR IEEE, P5710, DOI 10.1109/CVPR46437.2021.00566
   Vayer T, 2019, PR MACH LEARN RES, V97
   Verdie Y, 2015, PROC CVPR IEEE, P5279, DOI 10.1109/CVPR.2015.7299165
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
   Wang CW, 2023, IEEE T MULTIMEDIA, V25, P3989, DOI 10.1109/TMM.2022.3169331
   Xiang Y, 2014, IEEE WINT CONF APPL, P75, DOI 10.1109/WACV.2014.6836101
   Xu H., 2019, INT C MACHINE LEARNI, P6932
   Yan YG, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2969
   Yi KM, 2018, PROC CVPR IEEE, P2666, DOI 10.1109/CVPR.2018.00282
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang P, 2020, PROC CVPR IEEE, P5142, DOI 10.1109/CVPR42600.2020.00519
   Zhao XM, 2023, IEEE T MULTIMEDIA, V25, P3101, DOI 10.1109/TMM.2022.3155927
NR 49
TC 0
Z9 0
U1 7
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 12
PY 2024
DI 10.1007/s00371-023-03251-9
EA FEB 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HK8R5
UT WOS:001159495100002
DA 2024-08-05
ER

PT J
AU Hou, SM
   Li, ZY
   Wu, KK
   Zhao, YG
   Li, H
AF Hou, Shouming
   Li, Ziying
   Wu, Kuikui
   Zhao, Yinggang
   Li, Hui
TI Masked cross-attention and multi-head channel attention guiding
   single-stage generative adversarial networks for text-to-image
   generation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Generative adversarial networks; Text-to-image; Cross-attention; Masked;
   Multi-head channel attention
AB Although the text-to-image model aims to generate realistic images that correspond to the text description, generating high-quality, and accurate images remains a significant challenge. Most existing text-to-image methods are implemented through a two-stage stacking model, where the generation process is initiated by creating an initial image with a basic outline and subsequently refined to generate a high-resolution image. However, the quality of the initial image imposes limitations on this method as it directly impacts the final quality of the high-resolution output and may compromise the level of randomness in the high-resolution image, making it difficult for the model to generate a high-quality and realistic final image if the initial image is of low quality or lacks detail, causing the final image to lack diversity and to appear artificial if the initial image is too rigid or lacks randomness. Therefore, to overcome the limitation of the stacked structure, a new generative adversarial network method has been proposed, which generates high-resolution images directly from text descriptions, thus providing a more efficient and effective way to generate realistic images from text. Multi-head channel attention and masked cross-attention mechanisms are employed to emphasize the importance of relevance from various perspectives in order to enhance significant features associated with the text description and suppress non-essential features unrelated to the textual information. The integration of image and text information at a granular level is accomplished while employing a masked mechanism to minimize computational expenses and expedite the generation time of images. Furthermore, a discriminator-based semantic consistency loss function is devised to bolster the visual coherence between text and images, thereby directing the generator toward the production of more realistic images that align closely with text descriptions. The enhanced model improves the semantic consistency between text and images, leading to higher-quality generated images. Extensive experiments confirm the superiority of our proposed model to ControlGAN. On the CUB dataset, the model achieves an increased IS score from 4.58 to 4.96, while on the COCO dataset, the IS score improves from 24.06 to 33.56. Code is available at https://github.com/Leeziying0307/Github.git.
C1 [Hou, Shouming; Li, Ziying; Zhao, Yinggang] Henan Polytech Univ, Sch Comp Sci & Technol, Jiaozuo 454000, Henan, Peoples R China.
   [Wu, Kuikui] Kaifeng Vocat Skills Appraisal & Guidance Ctr, Kaifeng Vocat & Tech Training Teaching & Res Off, Kaifeng 475000, Henan, Peoples R China.
   [Li, Hui] Kaifeng Univ, Sch Mech & Automot Engn, Kaifeng 475004, Henan, Peoples R China.
C3 Henan Polytechnic University
RP Zhao, YG (corresponding author), Henan Polytech Univ, Sch Comp Sci & Technol, Jiaozuo 454000, Henan, Peoples R China.; Li, H (corresponding author), Kaifeng Univ, Sch Mech & Automot Engn, Kaifeng 475004, Henan, Peoples R China.
EM housm@163.com; 212109020032@home.hpu.edu.cn; 690630303@qq.com;
   ygz129@163.com; lihui@hotmail.com
FU Nation Natural Science Foundation of China
FX No Statement Available
CR Agarwal Vartika, 2023, International Journal of Information Technology, P3371, DOI 10.1007/s41870-023-01399-0
   Agarwal Vartika, 2022, Sustainable Advanced Computing: Select Proceedings of ICSAC 2021. Lecture Notes in Electrical Engineering (840), P161, DOI 10.1007/978-981-16-9012-9_14
   Agarwal V., 2022, International Journal of Image, Graphics and Signal Processing (IJIGSP), V14, P25, DOI 10.5815/ijigsp.2022.02.03
   Chen LY, 2021, IEEE COMPUT SOC CONF, P182, DOI 10.1109/CVPRW53098.2021.00027
   Deldjoo Y, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3439729
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Huang SY, 2022, DIGIT SIGNAL PROCESS, V120, DOI 10.1016/j.dsp.2021.103267
   Karras T., P IEEECVF C COMPUTER
   Karras T., 2017, ARXIV
   Kim H, 2022, MATHEMATICS-BASEL, V10, DOI 10.3390/math10203860
   Lewis P., 2020, ADV NEURAL INFORM PR, P9459, DOI DOI 10.48550/ARXIV.2005.11401
   Li B., 2019, arXiv
   Liao W., P IEEECVF C COMPUTER
   Lin Tsung-Yi., EUROPEAN C COMPUTER
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Peng DL, 2021, NEURAL NETWORKS, V138, P57, DOI 10.1016/j.neunet.2021.01.023
   Ramesh A., 2022, ARXIV
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan HC, 2023, IEEE T MULTIMEDIA, V25, P8620, DOI 10.1109/TMM.2023.3238554
   Tang Y., 2021, ADV NEURAL INF PROCE, V34, P15316
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinker Y., 2022, arXiv
   Wang Y., 2023, ARXIV
   Xu T., 2017, PROC CVPR IEEE
   Zhang H., 2018, IEEE T PATTERN ANAL
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang YB, 2023, VISUAL COMPUT, V39, P1283, DOI 10.1007/s00371-022-02404-6
   Zhu JW, 2022, NEURAL PROCESS LETT, V54, P5371, DOI 10.1007/s11063-022-10866-x
NR 30
TC 0
Z9 0
U1 9
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 9
PY 2024
DI 10.1007/s00371-024-03260-2
EA FEB 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HL7C8
UT WOS:001159716200002
DA 2024-08-05
ER

PT J
AU Ouyang, T
   Zhang, YJ
   Zhao, HL
   Cui, ZW
   Yang, YT
   Xu, YJ
AF Ouyang, Ting
   Zhang, Yongjun
   Zhao, Haoliang
   Cui, Zhongwei
   Yang, Yitong
   Xu, Yujie
TI A multi-color and multistage collaborative network guided by refined
   transmission prior for underwater image enhancement
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Underwater image enhancement; Transmission prior derivation; Multistage
   and multi-color space collaborative network; Physical-inconsistency loss
AB Due to the attenuation and scattering properties of light in underwater scenes, underwater images are generally subject to color deviations and low contrast, which is not conducive to the follow-up algorithms. To alleviate these two problems, we propose a multi-color and multistage collaborative network guided by refined transmission, called MMCGT, to accomplish the enhancement tasks. Specifically, we first design an accurate method of parameter estimation to derive transmission priors that are more suitable for underwater imaging, such as min-max conversion, low-pass filter-based estimation and saturation detection. Then, we propose a multistage and multi-color space collaborative network to decompose the underwater image enhancement task into more straightforward and controllable subtasks, including colorful feature extraction, color deviation detection, and image position information retention. Finally, we apply the derived transmission prior to the transmission-guided block of the network and effectively combine the well-designed physical-inconsistency loss with Charbonnier loss and VGG loss to guide the MMCGT to compensate for the quality-degraded regions better. Extensive experiments show that MMCGT achieves better evaluation results under the dual guidance of physics and deep learning than the competing methods in visual quality and quantitative metrics.
C1 [Ouyang, Ting; Zhang, Yongjun; Zhao, Haoliang; Yang, Yitong; Xu, Yujie] Guizhou Univ, Inst Artificial Intelligence, Coll Comp Sci & Technol, State Key Lab Publ Big Data, Guiyang 550025, Guizhou, Peoples R China.
   [Cui, Zhongwei] GuiZhou Educ Univ, Sch Math & Big Datam, Guiyang 550001, Guizhou, Peoples R China.
C3 Guizhou University; Guizhou Education University
RP Zhang, YJ (corresponding author), Guizhou Univ, Inst Artificial Intelligence, Coll Comp Sci & Technol, State Key Lab Publ Big Data, Guiyang 550025, Guizhou, Peoples R China.
EM zyj6667@126.com
RI Zhang, Yongjun/ABA-5054-2021
OI Zhang, Yongjun/0000-0002-8265-925X; Zhang, Yongjun/0000-0002-7534-1219
FU Natural science research project of Guizhou Provincial Department of
   Education
FX No Statement Available
CR Ancuti CO, 2018, IEEE T IMAGE PROCESS, V27, P379, DOI 10.1109/TIP.2017.2759252
   [Anonymous], 1729, Essai d'optique sur la gradation de la lumiere. chez Claude Jombert, rue S. Jacques
   Berman D, 2021, IEEE T PATTERN ANAL, V43, P2822, DOI 10.1109/TPAMI.2020.2977624
   CHARBONNIER P, 1994, IEEE IMAGE PROC, P168
   Chiang JY, 2012, IEEE T IMAGE PROCESS, V21, P1756, DOI 10.1109/TIP.2011.2179666
   Drews PLJ, 2016, IEEE COMPUT GRAPH, V36, P24, DOI 10.1109/MCG.2016.26
   Fu ZQ, 2022, Arxiv, DOI arXiv:2207.09689
   Galdran A, 2015, J VIS COMMUN IMAGE R, V26, P132, DOI 10.1016/j.jvcir.2014.11.006
   Golts A, 2020, IEEE T IMAGE PROCESS, V29, P2692, DOI 10.1109/TIP.2019.2952032
   Guo YC, 2020, IEEE J OCEANIC ENG, V45, P862, DOI 10.1109/JOE.2019.2911447
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Iqbal K, 2010, IEEE INT C SYSTEMS M
   Islam MJ, 2020, IEEE ROBOT AUTOM LET, V5, P3227, DOI 10.1109/LRA.2020.2974710
   Jerlov NG., 1976, MARINE OPTICS
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karpatne A., 2017, PHYS GUIDED NEURAL N
   Lai Y, 2023, VISUAL COMPUT, V39, P4133, DOI 10.1007/s00371-022-02580-5
   Li CY, 2021, IEEE T IMAGE PROCESS, V30, P4985, DOI 10.1109/TIP.2021.3076367
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li CY, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107038
   Li CY, 2018, IEEE SIGNAL PROC LET, V25, P323, DOI 10.1109/LSP.2018.2792050
   Li J, 2018, IEEE ROBOT AUTOM LET, V3, P387, DOI 10.1109/LRA.2017.2730363
   Li ZG, 2017, IEEE T IMAGE PROCESS, V26, P1243, DOI 10.1109/TIP.2017.2651366
   Lin RJ, 2022, VISUAL COMPUT, V38, P4419, DOI 10.1007/s00371-021-02305-0
   [刘海波 Liu Haibo], 2015, [自动化学报, Acta Automatica Sinica], V41, P1264
   Liu RS, 2020, IEEE T CIRC SYST VID, V30, P4861, DOI 10.1109/TCSVT.2019.2963772
   Loshchilov I, 2017, Sgdr: Stochastic gradient descent with warm restarts, DOI [10.48550/arXiv.1608.03983, DOI 10.48550/ARXIV.1608.03983]
   Mhala NC, 2021, VISUAL COMPUT, V37, P2097, DOI 10.1007/s00371-020-01972-9
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Monika R, 2021, VISUAL COMPUT, V37, P1499, DOI 10.1007/s00371-020-01884-8
   Naik A, 2021, AAAI CONF ARTIF INTE, V35, P15853
   Naik SK, 2003, IEEE T IMAGE PROCESS, V12, P1591, DOI 10.1109/TIP.2003.819231
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Pang Y, 2023, VISUAL COMPUT, V39, P1959, DOI 10.1007/s00371-022-02458-6
   Peng LT, 2023, IEEE T IMAGE PROCESS, V32, P3066, DOI 10.1109/TIP.2023.3276332
   Peng YT, 2018, IEEE T IMAGE PROCESS, V27, P2856, DOI 10.1109/TIP.2018.2813092
   Peng YT, 2016, IEEE IMAGE PROC, P1953, DOI 10.1109/ICIP.2016.7532699
   Qiao NZ, 2023, VISUAL COMPUT, V39, P3029, DOI 10.1007/s00371-022-02510-5
   Sharma G, 2005, COLOR RES APPL, V30, P21, DOI 10.1002/col.20070
   Tang Yi, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P5419, DOI 10.1145/3581783.3612378
   Tian CW, 2022, IEEE T SYST MAN CY-S, V52, P3718, DOI 10.1109/TSMC.2021.3069265
   Tian CW, 2021, KNOWL-BASED SYST, V226, DOI 10.1016/j.knosys.2021.106949
   Tian CW, 2020, KNOWL-BASED SYST, V205, DOI 10.1016/j.knosys.2020.106235
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Wang YD, 2021, SIGNAL PROCESS-IMAGE, V96, DOI 10.1016/j.image.2021.116250
   Yang M, 2015, IEEE T IMAGE PROCESS, V24, P6062, DOI 10.1109/TIP.2015.2491020
   Yuan WQ, 2023, VISUAL COMPUT, V39, P5199, DOI 10.1007/s00371-022-02654-4
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zhang J, 2017, PROC CVPR IEEE, P7016, DOI 10.1109/CVPR.2017.742
   Zhang KB, 2021, SIGNAL PROCESS, V178, DOI 10.1016/j.sigpro.2020.107771
   Zhang S, 2023, VISUAL COMPUT, V39, P5375, DOI 10.1007/s00371-022-02665-1
NR 53
TC 1
Z9 1
U1 5
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 12
PY 2024
DI 10.1007/s00371-023-03215-z
EA JAN 2024
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EW3I7
UT WOS:001141926700001
DA 2024-08-05
ER

PT J
AU Tan, YM
   Xia, HY
   Song, SX
AF Tan, Yumei
   Xia, Haiying
   Song, Shuxiang
TI Robust consistency learning for facial expression recognition under
   label noise
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Facial expression recognition; Consistency learning; Unsupervised
   learning; Robust representation
AB Label noise is inevitable in facial expression recognition (FER) datasets, especially for datasets that collected by web crawling, crowd sourcing in in-the-wild scenarios, which makes FER task more challenging. Recent advances tackle label noise by leveraging sample selection or constructing label distribution. However, they rely heavily on labels, which can result in confirmation bias issues. In this paper, we present RCL-Net, a simple yet effective robust consistency learning network, which combats label noise by learning robust representations and robust losses. RCL-Net can efficiently tackle facial samples with noisy labels commonly found in real-world datasets. Specifically, we first use a two-view-based backbone to embed facial images into high- and low-dimensional subspaces and then regularize the geometric structure of the high- and low-dimensional subspaces using an unsupervised dual-consistency learning strategy. Benefiting from the unsupervised dual-consistency learning strategy, we can obtain robust representations to combat label noise. Further, we impose a robust consistency regularization technique on the predictions of the classifiers to improve the whole network's robustness. Comprehensive evaluations on three popular real-world FER datasets demonstrate that RCL-Net can effectively mitigate the impact of label noise, which significantly outperforms state-of-the-art noisy label FER methods. RCL-Net also shows better generalization capability to other tasks like CIFAR100 and Tiny-ImageNet. Our code and models will be available at this https https://github.com/myt889/RCL-Net.
C1 [Tan, Yumei] Guangxi Normal Univ, Sch Comp Sci & Engn, Guilin 541004, Peoples R China.
   [Xia, Haiying; Song, Shuxiang] Guangxi Normal Univ, Sch Elect & Informat Engn, Guangxi Key Lab Brain Inspired Comp & Intelligent, Guilin 541004, Peoples R China.
C3 Guangxi Normal University; Guangxi Normal University
RP Song, SX (corresponding author), Guangxi Normal Univ, Sch Elect & Informat Engn, Guangxi Key Lab Brain Inspired Comp & Intelligent, Guilin 541004, Peoples R China.
EM tanyumei@stu.gxnu.edu.cn; xhy22@mailbox.gxnu.edu.cn;
   songshuxiang@mailbox.gxnu.edu.cn
FU National Natural Science Foundation of China [62366006, 62106054,
   62167001, 62366005]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62366006, 62106054, 62167001 and
   62366005.
CR Arpit D, 2017, PR MACH LEARN RES, V70
   Barsoum E, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P279, DOI 10.1145/2993148.2993165
   Bose D., 2023, ICASSP 2023, P1, DOI [10.1109/ICASSP49357.2023.10095728, DOI 10.1109/ICASSP49357.2023.10095728]
   Chang CJ, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2076
   Chen DL, 2023, IEEE T CIRC SYST VID, V33, P3848, DOI 10.1109/TCSVT.2023.3234312
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen ZH, 2023, IEEE T PATTERN ANAL, V45, P13489, DOI 10.1109/TPAMI.2023.3293885
   Choi JY, 2023, IEEE T MULTIMEDIA, V25, P100, DOI 10.1109/TMM.2021.3121547
   Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359
   Englesson E., 2021, Adv. Neural Inf. Process. Syst, V34, P30284
   Gao HX, 2023, NEURAL NETWORKS, V158, P228, DOI 10.1016/j.neunet.2022.11.025
   Gera D, 2023, Arxiv, DOI [arXiv:2305.01884, DOI 10.48550/ARXIV.2305.01884]
   Ghosh A, 2021, IEEE COMPUT SOC CONF, P2697, DOI 10.1109/CVPRW53098.2021.00304
   Goodfellow Ian J., 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P117, DOI 10.1007/978-3-642-42051-1_16
   Gu Y, 2023, IEEE T CIRC SYST VID, V33, P2033, DOI 10.1109/TCSVT.2022.3220669
   Guo H, 2019, PROC CVPR IEEE, P729, DOI 10.1109/CVPR.2019.00082
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Z, 2023, MULTIMED TOOLS APPL, V82, P5473, DOI 10.1007/s11042-022-12321-4
   Hongxin Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13723, DOI 10.1109/CVPR42600.2020.01374
   Jiabei Zeng, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11217), P227, DOI 10.1007/978-3-030-01261-8_14
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Krizhevsky A, 2009, CIFAR-10 dataset
   Le N., 2023, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, P6088
   Li CL, 2024, IEEE T CIRC SYST VID, V34, P882, DOI 10.1109/TCSVT.2023.3237006
   Li HH, 2023, VISUAL COMPUT, V39, P4709, DOI 10.1007/s00371-022-02619-7
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Li JC, 2022, LECT NOTES COMPUT SC, V13684, P128, DOI 10.1007/978-3-031-20053-3_8
   Li JJ, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3570329
   Li JN, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9465, DOI 10.1109/ICCV48922.2021.00935
   Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277
   Li SK, 2022, PROC CVPR IEEE, P316, DOI 10.1109/CVPR52688.2022.00041
   Li YF, 2023, PROC CVPR IEEE, P24070, DOI 10.1109/CVPR52729.2023.02305
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P2439, DOI 10.1109/TIP.2018.2886767
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu C, 2023, INFORM SCIENCES, V619, P781, DOI 10.1016/j.ins.2022.11.068
   Liu HW, 2022, IEEE T CIRC SYST VID, V32, P6253, DOI 10.1109/TCSVT.2022.3165321
   Liu S, 2020, P NEUIPS DEC 6 12 VI
   Lukov T, 2022, LECT NOTES COMPUT SC, V13672, P648, DOI 10.1007/978-3-031-19775-8_38
   Ma FY, 2024, IEEE T AFFECT COMPUT, V15, P593, DOI 10.1109/TAFFC.2023.3285231
   Ma FY, 2023, IEEE T AFFECT COMPUT, V14, P1236, DOI 10.1109/TAFFC.2021.3122146
   Ma X., 2020, INT C MACHINE LEARNI, P6543
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Neo D., 2023, P IEEE CVF C COMP VI, P5691
   Ortego D, 2021, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR46437.2021.00654
   Ruan DL, 2021, PROC CVPR IEEE, P7656, DOI 10.1109/CVPR46437.2021.00757
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   She JH, 2021, PROC CVPR IEEE, P6244, DOI 10.1109/CVPR46437.2021.00618
   Shikai Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13981, DOI 10.1109/CVPR42600.2020.01400
   Tan C, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1405, DOI 10.1145/3474085.3475622
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   Wang K, 2020, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR42600.2020.00693
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Wei Q, 2023, PROC CVPR IEEE, P11651, DOI 10.1109/CVPR52729.2023.01121
   Wei Q, 2022, LECT NOTES COMPUT SC, V13690, P516, DOI 10.1007/978-3-031-20056-4_30
   Wu Z., 2023, ICCV, P20698
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Xu T, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2127
   Zeng D, 2022, PROC CVPR IEEE, P20259, DOI 10.1109/CVPR52688.2022.01965
   Zhang CY, 2021, COMMUN ACM, V64, P107, DOI 10.1145/3446776
   Zhang FF, 2022, IEEE T MULTIMEDIA, V24, P1800, DOI 10.1109/TMM.2021.3072786
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang WH, 2019, PROC CVPR IEEE, P7365, DOI 10.1109/CVPR.2019.00755
   Zhang Y., 2021, Advances in Neural Information Processing Systems, V34, P17616
   Zhang YH, 2022, LECT NOTES COMPUT SC, V13686, P418, DOI 10.1007/978-3-031-19809-0_24
   Zhang ZL, 2018, ADV NEUR IN, V31
   Zhang ZY, 2022, PATTERN RECOGN LETT, V164, P23, DOI 10.1016/j.patrec.2022.10.016
   Zhao ZQ, 2021, IEEE T IMAGE PROCESS, V30, P6544, DOI 10.1109/TIP.2021.3093397
   Zheltonozhskii E, 2022, IEEE WINT CONF APPL, P387, DOI 10.1109/WACV51458.2022.00046
NR 71
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 5
PY 2024
DI 10.1007/s00371-024-03558-1
EA JUL 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XR1I7
UT WOS:001263311200002
DA 2024-08-05
ER

PT J
AU Jiang, YG
   Chen, XH
   Pun, CM
   Wang, SQ
   Feng, W
AF Jiang, Yiguo
   Chen, Xuhang
   Pun, Chi-Man
   Wang, Shuqiang
   Feng, Wei
TI MFDNet: Multi-Frequency Deflare Network for efficient nighttime flare
   removal
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Flare removal; Multi-frequency; CNN; Transformer; Efficient
ID FEATURE FUSION; IMAGE
AB When light is scattered or reflected accidentally in the lens, flare artifacts may appear in the captured photographs, affecting the photographs' visual quality. The main challenge in flare removal is to eliminate various flare artifacts while preserving the original content of the image. To address this challenge, we propose a lightweight Multi-Frequency Deflare Network (MFDNet) based on the Laplacian Pyramid. Our network decomposes the flare-corrupted image into low- and high-frequency bands, effectively separating the illumination and content information in the image. The low-frequency part typically contains illumination information, while the high-frequency part contains detailed content information. So our MFDNet consists of two main modules: the Low-Frequency Flare Perception Module (LFFPM) to remove flare in the low-frequency part and the Hierarchical Fusion Reconstruction Module (HFRM) to reconstruct the flare-free image. Specifically, to perceive flare from a global perspective while retaining detailed information for image restoration, LFFPM utilizes Transformer to extract global information while utilizing a convolutional neural network to capture detailed local features. Then HFRM gradually fuses the outputs of LFFPM with the high-frequency component of the image through feature aggregation. Moreover, our MFDNet can reduce the computational cost by processing in multiple frequency bands instead of directly removing the flare on the input image. Experimental results demonstrate that our approach outperforms state-of-the-art methods in removing nighttime flare on real-world and synthetic images from the Flare7K dataset. Furthermore, the computational complexity of our model is remarkably low.
C1 [Jiang, Yiguo; Chen, Xuhang; Pun, Chi-Man] Univ Macau, Macau, Peoples R China.
   [Chen, Xuhang; Wang, Shuqiang] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China.
   [Chen, Xuhang] Huizhou Univ, Huizhou, Peoples R China.
   [Feng, Wei] Tianjin Univ, Tianjin, Peoples R China.
C3 University of Macau; Chinese Academy of Sciences; Shenzhen Institute of
   Advanced Technology, CAS; Huizhou University; Tianjin University
RP Pun, CM (corresponding author), Univ Macau, Macau, Peoples R China.
EM cmpun@umac.mo
RI Chen, Xuhang/JPW-8742-2023
OI Chen, Xuhang/0000-0001-6000-3914
FU Science and Technology Development Fund, Macau SAR [0141/2023/RIA2,
   0193/2023/RIA3]
FX This work was supported in part by the Science and Technology
   Development Fund, Macau SAR, under Grants 0141/2023/RIA2 and
   0193/2023/RIA3.
CR Asha C., 2019, IEEE INT C DISTR COM, P1
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Boynton PA, 2003, P SOC PHOTO-OPT INS, V5080, P370, DOI 10.1117/12.519602
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Chabert F., 2015, Automated lens flare removal
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen L, 2024, INT J COMPUT VISION, V132, P428, DOI 10.1007/s11263-023-01877-9
   Chen LY, 2022, LECT NOTES COMPUT SC, V13667, P17, DOI 10.1007/978-3-031-20071-7_2
   Chen LY, 2021, IEEE COMPUT SOC CONF, P182, DOI 10.1109/CVPRW53098.2021.00027
   Chougule A, 2024, COGN COMPUT, V16, P788, DOI 10.1007/s12559-023-10244-2
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Cun XD, 2020, AAAI CONF ARTIF INTE, V34, P10680
   Dai Y., 2022, NEURIPS
   Faulkner K., 1989, Third International Conference on Image Processing and its Applications (Conf. Publ. No.307), P669
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Hu J., 2018, SQUEEZE AND EXCITATI, P7132
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li W., 2021, IJCAI
   Li YN, 2022, IEEE T CIRC SYST VID, V32, P553, DOI 10.1109/TCSVT.2021.3067502
   Li Z., 2023, IJCAI
   Li ZN, 2023, IEEE I CONF COMP VIS, P12415, DOI 10.1109/ICCV51070.2023.01144
   Liang J, 2021, PROC CVPR IEEE, P9387, DOI 10.1109/CVPR46437.2021.00927
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Luo Q, 2024, SIGNAL IMAGE VIDEO P, V18, P773, DOI 10.1007/s11760-023-02779-6
   Macleod HA, 2010, SER OPT OPTOELECTRON, P1, DOI 10.1201/9781420073034
   Ragini T, 2024, CIRC SYST SIGNAL PR, V43, P1030, DOI 10.1007/s00034-023-02499-9
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sahoo S, 2022, IEEE T CIRC SYST VID, V32, P1103, DOI 10.1109/TCSVT.2021.3074143
   SEIBERT JA, 1985, MED PHYS, V12, P281, DOI 10.1118/1.595720
   Sharma A, 2021, PROC CVPR IEEE, P11972, DOI 10.1109/CVPR46437.2021.01180
   Tu ZZ, 2022, PROC CVPR IEEE, P5759, DOI 10.1109/CVPR52688.2022.00568
   Vitoria P, 2019, J MATH IMAGING VIS, V61, P515, DOI 10.1007/s10851-018-0859-0
   Wang T., 2023, P AAAI C ART INT, P2654, DOI [DOI 10.1609/AAAI.V37I3.25364, 10.1609/AAAI.V37I3.25364]
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Wu Yicheng, 2021, P IEEE CVF INT C COM, P2239
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zha ZY, 2020, IEEE T IMAGE PROCESS, V29, P8561, DOI 10.1109/TIP.2020.3015545
   Zhang D., 2023, CVPR WORKSH, P2824, DOI DOI 10.1109/CVPRW59228.2023.00283
   Zhang J., 2023, The Visual Computer, P1
   Zhang J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2355, DOI 10.1145/3394171.3413763
   Zhang ML, 2019, IEEE T IMAGE PROCESS, V28, P868, DOI 10.1109/TIP.2018.2874284
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang Z, 2018, J MOD OPTIC, V65, P2220, DOI 10.1080/09500340.2018.1506057
NR 45
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 4
PY 2024
DI 10.1007/s00371-024-03540-x
EA JUL 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XM8T6
UT WOS:001262200800001
DA 2024-08-05
ER

PT J
AU Tsai, WK
   Wang, HC
AF Tsai, Wen-Kai
   Wang, Hsin-Chih
TI Real-time salient object detection based on accuracy background and
   salient path source selection
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Boundary and connectivity prior; Salient object detection; Background
   path source selection; Salient path source selection
ID IMAGE; TRACKING
AB Boundary and connectivity prior are common methods for detecting the image salient object. They often address two problems: 1) if the salient object touches the image boundary, the saliency of the object will fail, and 2) accurate pixel-wise or superpixel-wise computation needs high time expenditure. This study proposes a block-wise algorithm to reduce calculation time expenditure and suppress the salient objects touching the image boundary. The algorithm consists of four stages. In the first stage, each block is analyzed by an adaptive micro and macro prediction technique to generate a saliency prediction map. The second stage selects background and salient sources from the saliency prediction map. Background sources are extracted from the image boundary with low saliency value. Salient sources are accurately positioned in the region of salient objects. In the third stage, the background and salient sources are used to generate the background path and salient path based on minimum barrier distance. The block-wise initial saliency map is obtained by fusing the background and salient paths. In the fourth stage, major-color modeling technology and visual focus priors are used to complete the refinement of the saliency map to improve the block effect. In the experimental result, the proposed method produced the best test results among other algorithms in three dataset tests and achieved 284 frames per second (FPS) speed performance on the MSRA-10 K dataset. Our method shows at least 29.09% speed improvement and executes in real-time on a lightweight embedded platform.
C1 [Tsai, Wen-Kai; Wang, Hsin-Chih] Natl Formosa Univ, Dept Elect Engn, 64 Wunhua Rd, Huwei Township 632, Yunlin County, Taiwan.
C3 National Formosa University
RP Tsai, WK (corresponding author), Natl Formosa Univ, Dept Elect Engn, 64 Wunhua Rd, Huwei Township 632, Yunlin County, Taiwan.
EM twk@nfu.edu.tw
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Buckchash H, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3491227
   Chen ZH, 2015, IET IMAGE PROCESS, V9, P758, DOI 10.1049/iet-ipr.2014.0987
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   DELP EJ, 1979, IEEE T COMMUN, V27, P1335, DOI 10.1109/TCOM.1979.1094560
   Dijkstra E. W., 1959, Numer. Math, V1, P269, DOI [10.1007/BF01386390, DOI 10.1007/BF01386390]
   Fang YM, 2018, SIGNAL PROCESS-IMAGE, V69, P1, DOI 10.1016/j.image.2018.07.009
   Fareed MMS, 2018, COMPUT ELECTR ENG, V70, P551, DOI 10.1016/j.compeleceng.2017.08.027
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Huang X., 2018, IEEE Signal Process. Lett, V25, P1
   Huang XM, 2018, PATTERN RECOGN, V76, P95, DOI 10.1016/j.patcog.2017.10.027
   Huang XM, 2017, IEEE T IMAGE PROCESS, V26, P4243, DOI 10.1109/TIP.2017.2710636
   Ji YZ, 2022, INFORM SCIENCES, V584, P399, DOI 10.1016/j.ins.2021.10.055
   Jian MW, 2021, EXPERT SYST APPL, V168, DOI 10.1016/j.eswa.2020.114219
   Jian MW, 2020, MULTIMED TOOLS APPL, V79, P33467, DOI 10.1007/s11042-019-07842-4
   Kuang HL, 2018, IEEE T INTELL TRANSP, V19, P814, DOI 10.1109/TITS.2017.2702665
   Lie MMI, 2018, PATTERN RECOGN LETT, V114, P22, DOI 10.1016/j.patrec.2017.09.010
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu Q, 2017, IEEE T IMAGE PROCESS, V26, P4537, DOI 10.1109/TIP.2017.2703081
   Ma WP, 2021, INT J AUTOM COMPUT, V18, P73, DOI 10.1007/s11633-020-1246-z
   Ngoc MOV, 2020, COMPUT VIS IMAGE UND, V197, DOI 10.1016/j.cviu.2020.102993
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Papushoy A, 2015, DIGIT SIGNAL PROCESS, V36, P156, DOI 10.1016/j.dsp.2014.09.005
   Shang J., 2017, J. Electron. Imaging, V33, P013001
   Shin G, 2022, IEEE COMPUT SOC CONF, P3970, DOI 10.1109/CVPRW56347.2022.00442
   Singh VK, 2021, SIGNAL IMAGE VIDEO P, V15, P1777, DOI 10.1007/s11760-021-01917-2
   Singh VK., 2024, Multimed. Tool. Appl, V83, P35195
   Strand R, 2013, COMPUT VIS IMAGE UND, V117, P429, DOI 10.1016/j.cviu.2012.10.011
   Sun XL, 2016, IET IMAGE PROCESS, V10, P391, DOI 10.1049/iet-ipr.2015.0487
   Tang W, 2020, J VIS COMMUN IMAGE R, V71, DOI 10.1016/j.jvcir.2019.102727
   Tsai WK, 2023, VISUAL COMPUT, V39, P3059, DOI 10.1007/s00371-022-02513-2
   Tsai WK, 2012, IEEE EMBED SYST LETT, V4, P49, DOI 10.1109/LES.2012.2195710
   Tu WC, 2016, PROC CVPR IEEE, P2334, DOI 10.1109/CVPR.2016.256
   Wang AZ, 2017, IEEE SIGNAL PROC LET, V24, P663, DOI 10.1109/LSP.2017.2688136
   Wang HB, 2021, COMPUT ELECTR ENG, V90, DOI 10.1016/j.compeleceng.2021.106993
   Wang YZ, 2022, MULTIMED TOOLS APPL, V81, P27551, DOI 10.1007/s11042-022-12839-7
   Wu KS, 2009, PATTERN ANAL APPL, V12, P117, DOI 10.1007/s10044-008-0109-y
   Wu YW, 2017, IEEE ACCESS, V5, P23969, DOI 10.1109/ACCESS.2017.2764419
   Xiao F, 2018, SIGNAL PROCESS, V144, P392, DOI 10.1016/j.sigpro.2017.10.019
   Xu M, 2015, VISUAL COMPUT, V31, P355, DOI 10.1007/s00371-014-0930-9
   Yang C, 2013, IEEE SIGNAL PROC LET, V20, P637, DOI 10.1109/LSP.2013.2260737
   Yang JF, 2017, IET COMPUT VIS, V11, P710, DOI 10.1049/iet-cvi.2016.0469
   Yang ZY, 2022, PATTERN RECOGN, V121, DOI 10.1016/j.patcog.2021.108231
   Zhang CY, 2013, SIGNAL PROCESS-IMAGE, V28, P1171, DOI 10.1016/j.image.2013.07.004
   Zhang JM, 2015, IEEE I CONF COMP VIS, P1404, DOI 10.1109/ICCV.2015.165
   Zhang M, 2018, J VIS COMMUN IMAGE R, V52, P131, DOI 10.1016/j.jvcir.2018.01.004
   Zhou L, 2017, J ELECTRON IMAGING, V26, DOI 10.1117/1.JEI.26.4.043021
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 49
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 3
PY 2024
DI 10.1007/s00371-024-03559-0
EA JUL 2024
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XK1T9
UT WOS:001261494700002
DA 2024-08-05
ER

PT J
AU Wang, HJ
   Chen, XY
   Yuan, QB
   Liu, P
AF Wang, Huijuan
   Chen, Xinyue
   Yuan, Quanbo
   Liu, Peng
TI A review of 3D object detection based on autonomous driving
SO VISUAL COMPUTER
LA English
DT Review; Early Access
DE 3D object detection; Automatic driving; LIDAR
AB 3D object detection is a popular research direction in recent years, which plays an important role in the fields of automatic driving, intelligent robot navigation, medical treatment, AR&VR and so on. In this paper, the main research progress of 3D object detection is reviewed. Firstly, the basic concept and common data sets of 3D object detection are introduced. Secondly, 3D object detection algorithms are divided into LIDAR-based 3D object detection, camera-based 3D object detection and multi-modal 3D object detection according to the type of equipment that acquired data, and the representative work of each algorithm is described in detail. Finally, the above methods are compared and summarized, and the future development of 3D object detection is summarized and prospected.
C1 [Wang, Huijuan; Chen, Xinyue; Yuan, Quanbo; Liu, Peng] North China Inst Aerosp Engn, Sch Comp, Langfang 065000, Peoples R China.
C3 North China Institute of Aerospace Engineering
RP Wang, HJ (corresponding author), North China Inst Aerosp Engn, Sch Comp, Langfang 065000, Peoples R China.
EM wanghj323@126.com; 2495180611@qq.com; yuanquanbo@126.com;
   pengpeng2583365248@163.com
FU Fund Project of Central Government Guided Local Science and Technology
   Development
FX No Statement Available
CR Bai XY, 2022, PROC CVPR IEEE, P1080, DOI 10.1109/CVPR52688.2022.00116
   Beltrán J, 2018, IEEE INT C INTELL TR, P3517, DOI 10.1109/ITSC.2018.8569311
   Bewley A., 2020, arXiv
   Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164
   Chen C, 2022, AAAI CONF ARTIF INTE, P221
   Chen HS, 2021, PROC CVPR IEEE, P10374, DOI 10.1109/CVPR46437.2021.01024
   Chen JT, 2020, PROC CVPR IEEE, P389, DOI 10.1109/CVPR42600.2020.00047
   Chen XZ, 2018, IEEE T PATTERN ANAL, V40, P1259, DOI 10.1109/TPAMI.2017.2706685
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Chen XZ, 2016, PROC CVPR IEEE, P2147, DOI 10.1109/CVPR.2016.236
   Chen Y, 2020, P P IEEE CVF C COMP
   Chen YB, 2024, IEEE T PATTERN ANAL, V46, P1327, DOI 10.1109/TPAMI.2022.3201576
   Chen YL, 2023, IEEE I CONF COMP VIS, P8360, DOI 10.1109/ICCV51070.2023.00771
   Chen YL, 2019, IEEE I CONF COMP VIS, P9774, DOI [10.1109/iccv.2019.00987, 10.1109/ICCV.2019.00987]
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Deng JJ, 2021, AAAI CONF ARTIF INTE, V35, P1201
   Deng SH, 2022, PROC CVPR IEEE, P8438, DOI 10.1109/CVPR52688.2022.00826
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Engelcke Martin, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P1355, DOI 10.1109/ICRA.2017.7989161
   Fan LE, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2898, DOI 10.1109/ICCV48922.2021.00291
   Fazlali H, 2022, PROC CVPR IEEE, P17171, DOI 10.1109/CVPR52688.2022.01668
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Gu JQ, 2022, IEEE INT C INT ROBOT, P568, DOI 10.1109/IROS47612.2022.9981087
   He C, 2020, P P IEEE CVF C COMP
   He CH, 2022, PROC CVPR IEEE, P8407, DOI 10.1109/CVPR52688.2022.00823
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu HT, 2023, Arxiv, DOI arXiv:2303.17895
   Huang JJ, 2022, Arxiv, DOI arXiv:2112.11790
   Huang KC, 2022, PROC CVPR IEEE, P4002, DOI 10.1109/CVPR52688.2022.00398
   Huang T, 2020, P COMP VIS ECCV 2 15
   Huang XY, 2020, IEEE T PATTERN ANAL, V42, P2702, DOI 10.1109/TPAMI.2019.2926463
   Jiao Y, 2023, PROC CVPR IEEE, P21643, DOI 10.1109/CVPR52729.2023.02073
   Kandelkar A, 2023, LECT NOTE NETW SYST, V473, P299, DOI 10.1007/978-981-19-2821-5_26
   Ku J, 2018, IEEE INT C INT ROBOT, P5750, DOI 10.1109/IROS.2018.8594049
   Ku J, 2019, PROC CVPR IEEE, P11859, DOI 10.1109/CVPR.2019.01214
   Kuang HW, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20030704
   Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298
   Li B, 2016, arXiv
   Li BY, 2019, PROC CVPR IEEE, P1019, DOI 10.1109/CVPR.2019.00111
   Li PL, 2019, PROC CVPR IEEE, P7636, DOI 10.1109/CVPR.2019.00783
   Li X, 2023, PROC CVPR IEEE, P17524, DOI 10.1109/CVPR52729.2023.01681
   Li YW, 2022, PROC CVPR IEEE, P17161, DOI 10.1109/CVPR52688.2022.01667
   Lian Q, 2022, PROC CVPR IEEE, P1060, DOI 10.1109/CVPR52688.2022.00114
   Liang M, 2019, PROC CVPR IEEE, P7337, DOI 10.1109/CVPR.2019.00752
   Liu YF, 2023, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV51070.2023.00302
   Liu YF, 2022, LECT NOTES COMPUT SC, V13687, P531, DOI 10.1007/978-3-031-19812-0_31
   Liu YX, 2021, IEEE INT CONF ROBOT, P13018, DOI 10.1109/ICRA48506.2021.9561423
   Liu ZC, 2020, IEEE COMPUT SOC CONF, P4289, DOI 10.1109/CVPRW50498.2020.00506
   Liu ZJ, 2023, IEEE INT CONF ROBOT, P2774, DOI 10.1109/ICRA48891.2023.10160968
   Lu HH, 2019, INT CONF ACOUST SPEE, P1992, DOI 10.1109/ICASSP.2019.8682746
   Luo SJ, 2021, PROC CVPR IEEE, P6141, DOI 10.1109/CVPR46437.2021.00608
   Mao J., 2022, Neurocomputing, V471, P219, DOI [10.1016/j.neucom.2021.11.048, DOI 10.1016/J.NEUCOM.2021.11.048]
   Meyer GP, 2019, PROC CVPR IEEE, P12669, DOI 10.1109/CVPR.2019.01296
   Miao ZW, 2021, PROC CVPR IEEE, P3278, DOI 10.1109/CVPR46437.2021.00329
   Mousavian A, 2017, PROC CVPR IEEE, P5632, DOI 10.1109/CVPR.2017.597
   Noh J, 2021, PROC CVPR IEEE, P14600, DOI 10.1109/CVPR46437.2021.01437
   Pan XR, 2021, PROC CVPR IEEE, P7459, DOI 10.1109/CVPR46437.2021.00738
   Park D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3122, DOI 10.1109/ICCV48922.2021.00313
   Peng W, 2020, P P IEEE CVF C COMP
   Qi C R, 2017, P P IEEE C COMP VIS
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Qin ZY, 2019, PROC CVPR IEEE, P7607, DOI 10.1109/CVPR.2019.00780
   Reading C, 2021, PROC CVPR IEEE, P8551, DOI 10.1109/CVPR46437.2021.00845
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rukhovich D, 2022, IEEE WINT CONF APPL, P1265, DOI 10.1109/WACV51458.2022.00133
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Shi WJ, 2020, PROC CVPR IEEE, P1708, DOI 10.1109/CVPR42600.2020.00178
   Shreyas E., 2021, 2021 International Conference on Recent Trends on Electronics, Information, Communication & Technology (RTEICT), P735, DOI 10.1109/RTEICT52294.2021.9573964
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Sun J, 2020, P P IEEE CVF C COMP
   Sun P, 2021, PROC CVPR IEEE, P5721, DOI 10.1109/CVPR46437.2021.00567
   Sun P, 2020, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR42600.2020.00252
   Tang YJ, 2023, NEUROCOMPUTING, V553, DOI 10.1016/j.neucom.2023.126587
   Tao CB, 2023, ADV ENG INFORM, V57, DOI 10.1016/j.aei.2023.102069
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tian Zhi, 2022, Advances in Neural Information Processing Systems
   Tsochantaridis I, 2004, ICML 04, V10
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Vora S, 2020, PROC CVPR IEEE, P4603, DOI 10.1109/CVPR42600.2020.00466
   Wang HY, 2023, IEEE I CONF COMP VIS, P6769, DOI 10.1109/ICCV51070.2023.00625
   Wang H, 2021, PROC CVPR IEEE, P14610, DOI 10.1109/CVPR46437.2021.01438
   Wang T, 2021, IEEE INT CONF COMP V, P913, DOI 10.1109/ICCVW54120.2021.00107
   Wang Y, 2020, P COMP VIS ECCV 2 22
   Wang Y, 2022, PMLR
   Wang ZX, 2019, IEEE INT C INT ROBOT, P1742, DOI [10.1109/IROS40897.2019.8968513, 10.1109/iros40897.2019.8968513]
   Wu YT, 2021, IEEE SENS J, V21, P1152, DOI 10.1109/JSEN.2020.3020626
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xie L, 2020, AAAI CONF ARTIF INTE, V34, P12460
   Xu B, 2018, PROC CVPR IEEE, P2345, DOI 10.1109/CVPR.2018.00249
   Xu DF, 2018, PROC CVPR IEEE, P244, DOI 10.1109/CVPR.2018.00033
   Xu XW, 2022, PROC CVPR IEEE, P8428, DOI 10.1109/CVPR52688.2022.00825
   Xu ZB, 2020, AAAI CONF ARTIF INTE, V34, P12557
   Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337
   Yang B, 2018, P C ROB LEARN PMLR
   Yang B, 2018, PROC CVPR IEEE, P7652, DOI 10.1109/CVPR.2018.00798
   Yang JH, 2023, IEEE T PATTERN ANAL, V45, P6354, DOI 10.1109/TPAMI.2022.3216606
   Yang JH, 2021, PROC CVPR IEEE, P10363, DOI 10.1109/CVPR46437.2021.01023
   Yang ZT, 2019, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2019.00204
   Ye MS, 2020, PROC CVPR IEEE, P1628, DOI 10.1109/CVPR42600.2020.00170
   Yin TW, 2021, PROC CVPR IEEE, P11779, DOI 10.1109/CVPR46437.2021.01161
   Yoo J H, 2020, P COMP VIS ECCV 2 27
   Zarzar J, 2019, PointRGCN: Graph convolution networks for 3D vehicles detection refinement
   Zetong Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11037, DOI 10.1109/CVPR42600.2020.01105
   Zhang B, 2023, PROC CVPR IEEE, P9253, DOI 10.1109/CVPR52729.2023.00893
   Zhang YA, 2022, PROC CVPR IEEE, P898, DOI 10.1109/CVPR52688.2022.00098
   Zhao N, 2020, P P IEEE CVF C COMP
   Zheng W, 2021, PROC CVPR IEEE, P14489, DOI 10.1109/CVPR46437.2021.01426
   Zhou Y, 2020, PMLR
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
NR 112
TC 0
Z9 0
U1 24
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 14
PY 2024
DI 10.1007/s00371-024-03480-6
EA JUN 2024
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UH5I1
UT WOS:001247174300001
DA 2024-08-05
ER

PT J
AU Maik, M
   Flotynski, J
   Walczak, K
AF Maik, Mikolaj
   Flotynski, Jakub
   Walczak, Krzysztof
TI Knowledge-based approach to adaptive XR interface design for
   non-programmers
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Extended reality; Knowledge representation; Adaptive 3D user interfaces;
   HCI; Stock exchange visualization
AB Customizing extended reality (XR) interfaces presents a significant challenge, especially for users with limited programming expertise. This paper introduces the method for adaptation of XR interfaces (MAXI-XR), a novel approach to simplify the customization process of XR user interfaces through knowledge technologies. MAXI-XR offers a user-friendly solution for interface design, supporting users with varying levels of technical skills. The basis of MAXI-XR is its Semantic Knowledge Base, which facilitates intelligent adaptations through advanced querying and reasoning, enabling the extraction of user-specific information for context-based XR interface adaptation. The functionality of MAXI-XR is demonstrated by its application in a VR stock market data visualization system. This system demonstrates MAXI-XR's ability to adapt to complex and data-intensive environments according to user requirements, improving the interaction experience. Furthermore, the method's scalability and ease of maintenance make it a versatile tool for a wide range of applications beyond stock market visualization, suggesting its potential for broader adoption in various XR domains.
C1 [Maik, Mikolaj; Flotynski, Jakub; Walczak, Krzysztof] Poznan Univ Econ & Business, Dept Informat Technol, Poznan, Poland.
C3 Poznan University of Economics & Business
RP Maik, M (corresponding author), Poznan Univ Econ & Business, Dept Informat Technol, Poznan, Poland.
EM mikolaj.maik@ue.poznan.pl; jakub.flotynski@ue.poznan.pl;
   krzysztof.walczak@ue.poznan.pl
RI Maik, Mikołaj/IWU-8269-2023; Walczak, Krzysztof/AAF-9685-2021
OI Maik, Mikołaj/0000-0002-9128-9617; Walczak,
   Krzysztof/0000-0001-8170-7910
FU Narodowe Centrum Badanacute; i Rozwoju
   [LIDER/55/0287/L-12/20/NCBR/2021]; Polish National Center for Research
   and Development
FX The presented research was funded by the Polish National Center for
   Research and Development under grant number
   LIDER/55/0287/L-12/20/NCBR/2021.
CR [Anonymous], 2006, 11th international conference on 3D web technology, P85
   Belo J, 2022, PROCEEDINGS OF THE 35TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, UIST 2022, DOI 10.1145/3526113.3545651
   Buche C., 2010, Int. J. Virtual Real, V9, P73, DOI [10.20870/IJVR.2010.9.2.2772, DOI 10.20870/IJVR.2010.9.2.2772]
   Calvary G, 2003, INTERACT COMPUT, V15, P289, DOI 10.1016/S0953-5438(03)00010-9
   De Troyer O., 2007, Tutorials, posters, panels and industrial contributions at the 26th international conference on Conceptual modeling-Volume 83, V83, P3
   Dennemont Y, 2012, IEEE VIRTUAL REALITY CONFERENCE 2012 PROCEEDINGS, P103, DOI 10.1109/VR.2012.6180903
   Flotynski J., Future and backward exploration of xr environments
   Flotynski J, 2017, COMPUT GRAPH FORUM, V36, P329, DOI 10.1111/cgf.13083
   Garcia Sarah, 2023, Virtual, Augmented and Mixed Reality: 15th International Conference, VAMR 2023, Held as Part of the 25th HCI International Conference, HCII 2023, Proceedings. Lecture Notes in Computer Science (14027), P3, DOI 10.1007/978-3-031-35634-6_1
   Heitmayer M., 2021, TMS P 2021
   Horvitz Eric, 1999, P SIGCHI C HUM FACT, P159, DOI [DOI 10.1145/302979.303030, 10.1145/302979.303030]
   Khan MJ, 2019, MULTIMED TOOLS APPL, V78, P14397, DOI 10.1007/s11042-018-6819-2
   Lacoche J, 2014, WORK SOFTW ENG, P19, DOI 10.1109/SEARIS.2014.7152797
   Lee YCJ, 2022, FRONT PUBLIC HEALTH, V10, DOI 10.3389/fpubh.2022.1040018
   MacCallum K, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.888689
   Maik M., 2022, P 27 INT C 3D WEB TE, P1
   Malik US., 2021, Studies in Digital Heritage, V5, P1, DOI DOI 10.14434/SDH.V5I1.32323
   Miñón R, 2016, UNIVERSAL ACCESS INF, V15, P153, DOI 10.1007/s10209-015-0406-3
   Miraz MH, 2021, COMPUT SCI REV, V40, DOI 10.1016/j.cosrev.2021.100363
   Pottle Jack, 2019, Future Healthc J, V6, P181, DOI 10.7861/fhj.2019-0036
   Ranosz J, 2023, 28TH INTERNATIONAL CONFERENCE ON WEB3D TECHNOLOGY, WEB3D 2023, DOI 10.1145/3611314.3615919
   Ruminski D., 2019, Acta Polytech. Hungarica, V16, P223, DOI [10.12700/APH.16.6.2019.6.14, DOI 10.12700/APH.16.6.2019.6.14]
   Todi K, 2023, Arxiv, DOI arXiv:2309.04025
   Todi K, 2022, Arxiv, DOI arXiv:2204.09162
   Walczak K, 2019, LECT N MECH ENG, P368, DOI 10.1007/978-3-030-18715-6_31
   Walczak K, 2008, EUROGR TECH REP SER, P135, DOI 10.1109/HSI.2008.4581455
NR 26
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 8
PY 2024
DI 10.1007/s00371-024-03472-6
EA JUN 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TO9G8
UT WOS:001242314900003
OA hybrid
DA 2024-08-05
ER

PT J
AU Zhang, XY
   Xu, C
   Han, Y
   Baciu, G
AF Zhang, Xuyuan
   Xu, Chen
   Han, Yu
   Baciu, George
TI Fabric image recolorization by fuzzy pretrained neural network
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image recolorization; Color theme; Image segmentation; Intrinsic image
   decomposition; Fabric color design; Variational model
ID DECOMPOSITION
AB In the art of fabric design, the technic of image recolorization is usually used to generate synthetic fabric images that can serve as new fabric design proposals. However, classical non-learning-based image recolorization methods for fabric color design are formulated by variational models that are the integration of image decomposition models and image segmentation models. Although classical image recolorization methods can generate relatively good color design proposals, they have much higher computation burden since classical variational models typically rely on inefficient iterative algorithms for solving minimization problems associated with these models. This work addresses the inefficiency problem by introducing a novel image recolorization architecture that is based on two neural networks: (1) a pretrained neural network for intrinsic image decomposition, and (2) a pretrained plug-and-play denoiser for image segmentation. Numerical results demonstrate that, compared to classical variational-based image recolorization methods, our proposed method shows higher efficiency in the running time, while texture details in fabric images to be recolored are better preserved. Our code and test images can be downloaded available at https://zenodo.org/records/10816651.
C1 [Zhang, Xuyuan] Shenzhen Univ, Coll Elect & Informat Engn, 3688 Nanhai Ave, Shenzhen 518060, Guangdong, Peoples R China.
   [Xu, Chen; Han, Yu] Shenzhen Univ, Sch Math Sci, 3688 Nanhai Ave, Shenzhen 518060, Guangdong, Peoples R China.
   [Baciu, George] Hong Kong Polytech Univ, Dept Comp, Hung Hom, Kowloon, 8 Hung Lok Rd, Hong Kong, Peoples R China.
C3 Shenzhen University; Shenzhen University; Hong Kong Polytechnic
   University
RP Han, Y (corresponding author), Shenzhen Univ, Sch Math Sci, 3688 Nanhai Ave, Shenzhen 518060, Guangdong, Peoples R China.
EM pmfxyz@163.com; xuchen_szu@szu.edu.cn; hany@szu.edu.cn;
   csgeorge@polyu.edu.hk
OI BACIU, George/0000-0002-1766-6357; Zhang, Xuyuan/0000-0002-1637-7078
FU The National Natural Science Foundation of China [62372302, 62072312];
   National Natural Science Foundation of China [2023A1515011394]; Natural
   Science Foundation of Guangdong Province [2023KTSCX116]; Project of
   Educational Commission of Guangdong Province [JCYJ20210324094009026,
   JCYJ20200109105832261]; Shenzhen Basis Research Project
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant [62372302] and Grant [62072312], in part
   by the Natural Science Foundation of Guangdong Province under Grant
   [2023A1515011394], in part by the Project of Educational Commission of
   Guangdong Province under Grant [2023KTSCX116], and in part by Shenzhen
   Basis Research Project under Grant [JCYJ20210324094009026] and Grant
   [JCYJ20200109105832261].
CR Chang HW, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766978
   Das P, 2022, PROC CVPR IEEE, P19758, DOI 10.1109/CVPR52688.2022.01917
   Gao GM, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3078171
   Gros C, 2021, MED IMAGE ANAL, V71, DOI 10.1016/j.media.2021.102038
   Han Y, 2020, TEXT RES J, V90, P1326, DOI 10.1177/0040517519890212
   Han Y, 2017, IEEE T MULTIMEDIA, V19, P80, DOI 10.1109/TMM.2016.2608000
   Hu Q, 2022, TEXT RES J, V92, P4422, DOI 10.1177/00405175221103616
   Huang W, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3332490
   Kumar R, 2023, J MATH IMAGING VIS, V65, P618, DOI 10.1007/s10851-022-01130-x
   Li QW, 2023, SCI CHINA INFORM SCI, V66, DOI 10.1007/s11432-021-3481-3
   Liu YF, 2020, PROC CVPR IEEE, P3245, DOI 10.1109/CVPR42600.2020.00331
   Luan HP, 2023, VISUAL COMPUT, V39, P4351, DOI 10.1007/s00371-022-02595-y
   O'Donovan P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964958
   Shen L, 2013, IEEE T PATTERN ANAL, V35, P2904, DOI 10.1109/TPAMI.2013.136
   Sheng B, 2020, IEEE T VIS COMPUT GR, V26, P1332, DOI 10.1109/TVCG.2018.2869326
   Wang YL, 2019, COMPUT GRAPH FORUM, V38, P11, DOI 10.1111/cgf.13812
   Wei DL, 2023, APPL MATH MODEL, V123, P197, DOI 10.1016/j.apm.2023.06.033
   Xu C, 2019, TEXT RES J, V89, P3617, DOI 10.1177/0040517518817051
   Xu J, 2020, IEEE T IMAGE PROCESS, V29, P5022, DOI 10.1109/TIP.2020.2974060
   Zhang F, 2023, IEEE T CIRC SYST VID, V33, P132, DOI 10.1109/TCSVT.2022.3199428
   Zhang K, 2022, IEEE T PATTERN ANAL, V44, P6360, DOI 10.1109/TPAMI.2021.3088914
   Zhang Q, 2022, IEEE T MULTIMEDIA, V24, P1545, DOI 10.1109/TMM.2021.3067463
   Zhang Q, 2022, IEEE T PATTERN ANAL, V44, P9669, DOI 10.1109/TPAMI.2021.3129795
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang XY, 2023, MATHEMATICS-BASEL, V11, DOI 10.3390/math11051101
   Zhang YS, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3571076
   Zheng DJ, 2022, COLOR RES APPL, V47, P676, DOI 10.1002/col.22745
   Zheng DJ, 2017, TEXT RES J, V87, P2524, DOI 10.1177/0040517516673331
   Zheng DJ, 2015, TEXT RES J, V85, P1520, DOI 10.1177/0040517514561920
   Zhou XC, 2023, IEEE T MULTIMEDIA, V25, P8064, DOI 10.1109/TMM.2022.3233255
NR 30
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 8
PY 2024
DI 10.1007/s00371-024-03510-3
EA JUN 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TO9G8
UT WOS:001242314900001
DA 2024-08-05
ER

PT J
AU Liu, YL
   Yang, DG
   Song, TT
   Ye, YC
   Zhang, X
AF Liu, Yongli
   Yang, Degang
   Song, Tingting
   Ye, Yichen
   Zhang, Xin
TI YOLO-SSP: an object detection model based on pyramid spatial attention
   and improved downsampling strategy for remote sensing images
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Object detection; Remote sensing images; Small object; Attention
   mechanism
ID CONVOLUTIONAL NETWORKS
AB Object detection is an essential task in remote sensing image processing. However, the remote sensing images are characterized by large range of object sizes and complex object backgrounds, which results in challenges in the object detection task. Moreover, the detection effect of existing object detectors on remote sensing images is still not satisfactory. In order to tackle the above problems, an object detection model named YOLO-SSP for remote sensing images is proposed based on the YOLOv8m model in this paper. To begin with, the original downsampling layers are replaced with the proposed lightweight SPD-Conv module, which performs downsampling without loss of fine-grained information and improves the ability of the network to learn the feature representation. In addition, to adapt the large number of small objects in remote sensing images, a small object detection layer is added and achieves the expected results. Finally, a pyramid spatial attention mechanism is proposed to obtain the weights of different spatial positions through hierarchical pooling operations. It effectively improves the detection performance of small objects and those with complex backgrounds. We conducted ablation experiments on the DIOR dataset and compared the YOLO-SSP model with other state-of-the-art models. YOLO-SSP obtains 64.7% of mAP, which is an improvement of 2.3% relative to the baseline model. To demonstrate the generalizability and robustness of the improved model, the comparison experiments are also performed on the TGRS-HRRSD dataset and SIMD dataset with mAP of 77.2 and 64.9%, respectively. The code will be available at https://github.com/YongliLiu/SSP.
C1 [Liu, Yongli; Yang, Degang; Song, Tingting; Zhang, Xin] Chongqing Normal Univ, Coll Comp & Informat Sci, Chongqing 401331, Peoples R China.
   [Yang, Degang] Chongqing Engn Res Ctr, Educ Big Data Intelligent Percept & Applicat, Chongqing 401331, Peoples R China.
   [Ye, Yichen] Southwest Univ, Coll Elect & Informat Engn, Chongqing 400715, Peoples R China.
C3 Chongqing Normal University; Southwest University - China
RP Song, TT (corresponding author), Chongqing Normal Univ, Coll Comp & Informat Sci, Chongqing 401331, Peoples R China.
EM ttsong@cqnu.edu.cn
FU Science and Technology Research Program of Chongqing Municipal Education
   Commission
FX No Statement Available
CR Cao Y., 2024, IEEE Trans. Geosci. Remote Sens, V62, P1
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen F, 2020, REMOTE SENS ENVIRON, V242, DOI 10.1016/j.rse.2020.111706
   Cui LS, 2022, IEEE T CYBERNETICS, V52, P2300, DOI 10.1109/TCYB.2020.3004636
   Gagliardi V, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15020418
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Glenn J., 2022, Github:ultralytics/yolov5
   Glenn J., 2023, Ultralytics yolov8
   Han ZM, 2022, KNOWL-BASED SYST, V253, DOI 10.1016/j.knosys.2022.109512
   Haroon M, 2020, IEEE J-STARS, V13, P3032, DOI 10.1109/JSTARS.2020.3000317
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hu J., 2018, SQUEEZE AND EXCITATI, P7132
   Hu MZ, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23146423
   Jian L, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14174383
   Leng JX, 2024, IEEE T MULTIMEDIA, V26, P3765, DOI 10.1109/TMM.2023.3315558
   Leng JX, 2023, IEEE T CIRC SYST VID, V33, P1320, DOI 10.1109/TCSVT.2022.3210207
   Leng JX, 2020, IEEE T INTELL TRANSP, V21, P1560, DOI 10.1109/TITS.2019.2909275
   Li J., 2024, IEEE Trans. Geosci. Remote Sens, V62, P1
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Li K, 2020, ISPRS J PHOTOGRAMM, V159, P296, DOI 10.1016/j.isprsjprs.2019.11.023
   Li X, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3051383
   Li Z, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23146414
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu XD, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15184459
   Pan JS, 2022, INT J COMPUT VISION, V130, P1440, DOI 10.1007/s11263-022-01583-y
   Qin P, 2021, IEEE J-STARS, V14, P11058, DOI 10.1109/JSTARS.2021.3123080
   Redmon J., 2018, CoRR
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Reedha R, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14030592
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Singh SA, 2023, J INTELL MANUF, V34, P1995, DOI 10.1007/s10845-021-01878-w
   Song TT, 2023, IMAGE VISION COMPUT, V140, DOI 10.1016/j.imavis.2023.104855
   Sunkara R, 2023, LECT NOTES ARTIF INT, V13715, P443, DOI 10.1007/978-3-031-26409-2_27
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xie TY, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15153863
   Zhang YL, 2019, IEEE T GEOSCI REMOTE, V57, P5535, DOI 10.1109/TGRS.2019.2900302
   Zou ZX, 2023, P IEEE, V111, P257, DOI 10.1109/JPROC.2023.3238524
NR 45
TC 0
Z9 0
U1 18
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 22
PY 2024
DI 10.1007/s00371-024-03434-y
EA MAY 2024
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RQ9X0
UT WOS:001229255500001
DA 2024-08-05
ER

PT J
AU Xu, HN
   Mu, P
   Liu, ZY
   Cheng, SC
AF Xu, Hanning
   Mu, Pan
   Liu, Zheyuan
   Cheng, Shichao
TI Underwater image enhancement via color conversion and white
   balance-based fusion
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Computer vision; Convolutional neural networks; Underwater image
   enhancement; Deep learning
ID WATER
AB The task of enhancing underwater images presents a significant challenge due to the refraction and absorption of light in water, resulting in images that often appear bluish or greenish with diminished contrast. Furthermore, the scarcity of underwater datasets complicates the achievement of robust generalization capacity to address complex underwater scenarios. In this study, we introduce generalized underwater image enhancement model with color-guided adaptive feature fusion (GU-CAFF), designed to rectify various degraded underwater images, utilizing a minimal amount of training data. GU-CAFF primarily comprises two modules: a multi-level color-feature encoder (MCE) and a white balance-based fusion (WBF) module. The MCE integrates physical models to extract features from underwater images exhibiting different color deviations, emphasizing essential features while preserving their structural information. In addition, WBF, in conjunction with a statistical model, is proposed to fuse the features extracted by the encoder and rectify the color distortion of specific pixels in degraded images. The proposed method can be trained once on our developed dataset and exhibits robust generalization capabilities on other datasets. Quantitative and qualitative comparisons are conducted with several state-of-the-art underwater image enhancement models, demonstrating our superior performance in enhancing underwater images.The source code will be available at https://github.com/shiningZZ/GU-CAFF.
C1 [Xu, Hanning; Mu, Pan; Liu, Zheyuan] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou 310023, Zhejiang, Peoples R China.
   [Cheng, Shichao] Hangzhou Dianzi Univ, Sch Comp Sci, Hangzhou, Zhejiang, Peoples R China.
C3 Zhejiang University of Technology; Hangzhou Dianzi University
RP Mu, P (corresponding author), Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou 310023, Zhejiang, Peoples R China.
EM hanningxu@zjut.edu.cn; panmu@zjut.edu.cn; zheyuanliu@zjut.edu.cn;
   sccheng@hdu.edu.cn
FU Natural Science Foundation of China [62202429]; Zhejiang Provincial
   Natural Science Foundation of China [LY23F020024]
FX This work is supported by the Natural Science Foundation of China (Grant
   No. 62202429) and the Zhejiang Provincial Natural Science Foundation of
   China under Grant No. LY23F020024.
CR Akkaynak D, 2019, PROC CVPR IEEE, P1682, DOI 10.1109/CVPR.2019.00178
   Akkaynak D, 2018, PROC CVPR IEEE, P6723, DOI 10.1109/CVPR.2018.00703
   Ancuti CO, 2018, IEEE T IMAGE PROCESS, V27, P379, DOI 10.1109/TIP.2017.2759252
   Ancuti C, 2012, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.2012.6247661
   Berman D, 2021, IEEE T PATTERN ANAL, V43, P2822, DOI 10.1109/TPAMI.2020.2977624
   BUCHSBAUM G, 1980, J FRANKLIN I, V310, P1, DOI 10.1016/0016-0032(80)90058-7
   Chen L, 2021, IEEE T CIRC SYST VID, V31, P3078, DOI 10.1109/TCSVT.2020.3035108
   Chen ZH, 2023, IEEE T PATTERN ANAL, V45, P13489, DOI 10.1109/TPAMI.2023.3293885
   Cheng ZZ, 2015, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2015.55
   Dai L, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-23458-5
   Dai YM, 2021, IEEE WINT CONF APPL, P3559, DOI 10.1109/WACV48630.2021.00360
   Drews PLJ, 2016, IEEE COMPUT GRAPH, V36, P24, DOI 10.1109/MCG.2016.26
   Ebner M., 2007, Color Constancy
   Fabbri C, 2018, IEEE INT CONF ROBOT, P7159
   Fan J., 2023, IEEE Transactions on Instrumentation and Measurement, V72, P5004418, DOI DOI 10.1109/TIM.2023.3235420
   Finlayson GD, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P37
   Fu ZQ, 2022, LECT NOTES COMPUT SC, V13678, P465, DOI 10.1007/978-3-031-19797-0_27
   Fu ZQ, 2022, AAAI CONF ARTIF INTE, P643
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guo HB, 2021, IEEE T CYBERNETICS, V51, P2735, DOI 10.1109/TCYB.2019.2934823
   Hambarde P, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3120130
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Huang SR, 2023, PROC CVPR IEEE, P18145, DOI 10.1109/CVPR52729.2023.01740
   Huang ZX, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3189630
   Iqbal K, 2010, IEEE INT C SYSTEMS M
   Islam MJ, 2020, IEEE ROBOT AUTOM LET, V5, P3227, DOI 10.1109/LRA.2020.2974710
   Jin YX, 2021, IEEE T NEUR NET LEAR, V32, P2330, DOI 10.1109/TNNLS.2020.3004634
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li CY, 2016, IEEE T IMAGE PROCESS, V26, P5664, DOI 10.1109/TIP.2016.2612882
   Li CY, 2021, IEEE T IMAGE PROCESS, V30, P4985, DOI 10.1109/TIP.2021.3076367
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li CY, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107038
   Li CY, 2018, IEEE SIGNAL PROC LET, V25, P323, DOI 10.1109/LSP.2018.2792050
   Li HX, 2021, IEEE T IMAGE PROCESS, V30, P8526, DOI 10.1109/TIP.2021.3117061
   Li J, 2018, IEEE ROBOT AUTOM LET, V3, P387, DOI 10.1109/LRA.2017.2730363
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu CF, 2023, IEEE T INSTRUM MEAS, V72, DOI [10.1109/TIM.2023.3298395, 10.1109/TIM.2022.3225047]
   Liu RS, 2020, IEEE T CIRC SYST VID, V30, P4861, DOI 10.1109/TCSVT.2019.2963772
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mu Pan, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P2286, DOI 10.1145/3503161.3548087
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Paul S, 2016, J CIRCUIT SYST COMP, V25, DOI 10.1142/S0218126616501231
   Peng Lintao, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13802), P290, DOI 10.1007/978-3-031-25063-7_18
   Peng YT, 2018, IEEE T IMAGE PROCESS, V27, P2856, DOI 10.1109/TIP.2018.2813092
   Peng YT, 2017, IEEE T IMAGE PROCESS, V26, P1579, DOI 10.1109/TIP.2017.2663846
   Qi Q, 2022, IEEE T IMAGE PROCESS, V31, P6816, DOI 10.1109/TIP.2022.3216208
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Roboflow, aquarium combined dataset [Open Source Dataset]
   Rout DK, 2022, MULTIMED TOOLS APPL, V81, P32907, DOI 10.1007/s11042-022-12692-8
   Schechner YY, 2005, IEEE J OCEANIC ENG, V30, P570, DOI 10.1109/JOE.2005.850871
   Sheng B, 2020, IEEE T CIRC SYST VID, V30, P955, DOI 10.1109/TCSVT.2019.2901629
   Sheng B, 2020, IEEE T VIS COMPUT GR, V26, P1332, DOI 10.1109/TVCG.2018.2869326
   Sun XD, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3290366
   Wen Y, 2021, IEEE T IMAGE PROCESS, V30, P6142, DOI 10.1109/TIP.2021.3092814
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xiao ZY, 2022, Arxiv, DOI [arXiv:2209.02221, DOI 10.48550/ARXIV.2209.02221]
   Yan XH, 2022, KNOWL-BASED SYST, V258, DOI 10.1016/j.knosys.2022.110041
   Yang HH, 2021, IEEE INT CONF ROBOT, P685, DOI 10.1109/ICRA48506.2021.9561263
   Yang M, 2015, IEEE T IMAGE PROCESS, V24, P6062, DOI 10.1109/TIP.2015.2491020
   Zhou JC, 2022, APPL INTELL, V52, P16435, DOI 10.1007/s10489-022-03275-z
   Zhou Y, 2023, IEEE T NEUR NET LEAR, V34, P7719, DOI 10.1109/TNNLS.2022.3146004
   Zhuang PX, 2021, ENG APPL ARTIF INTEL, V101, DOI 10.1016/j.engappai.2021.104171
NR 63
TC 0
Z9 0
U1 6
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 9
PY 2024
DI 10.1007/s00371-024-03421-3
EA MAY 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QJ2S3
UT WOS:001220448100001
DA 2024-08-05
ER

PT J
AU Meng, QK
   Huai, YJ
   Ma, F
   Ye, WT
   Xu, HF
   Yang, SY
AF Meng, Qingkuo
   Huai, Yongjian
   Ma, Fei
   Ye, Wentao
   Xu, Haifeng
   Yang, Siyu
TI Visualization of the occurrence and spread of wildfires in
   three-dimensional natural scenes
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Visualization; Forest fire behavior; Firefighting behavior; Lightning
   strike fires; Weather factor; Cellular automata
ID FOREST-FIRE SPREAD; SIMULATION; MODEL
AB There are generally two ways to ignite wildfires, including natural fire sources represented by lightning strikes and artificial fire sources generated by human production and daily use, both of which have regional and seasonal characteristics. For three-dimensional forest fire research, it is not easy to achieve complex global spread behavior simulation while considering the internal physical reactions of vegetation combustion. The study constructed different natural scenes based on different vegetation cells, described the principle of lightning ignition of combustibles, analyzed the spread results of wildfires under the influence of multiple weather factors in different scenes, and achieved repeatable wildfire research. At the same time, the virtual scene intuitively expresses the real fire extinguishing methods, providing relevant references for the design of fire extinguishing schemes. Compared to directly using physical models, this article uses a single wood pyrolysis model to couple vegetation's morphological structure and physical reactions. By considering the spread of different vegetation types and the influence of multiple factors on forest fire spread, it expresses the complete forest fire behavior from ignition to extinction, significantly improving the realism and immersion of forest fires.
C1 [Meng, Qingkuo; Huai, Yongjian; Ma, Fei; Ye, Wentao; Xu, Haifeng; Yang, Siyu] Beijing Forestry Univ, Sch Informat Sci & Technol, Beijing 100083, Peoples R China.
C3 Beijing Forestry University
RP Huai, YJ (corresponding author), Beijing Forestry Univ, Sch Informat Sci & Technol, Beijing 100083, Peoples R China.
EM mengqingkuo3110@bjfu.edu.cn; huaiyj@bjfu.edu.cn; mf261710@bjfu.edu.cn;
   ywtbl2021@bjfu.edu.cn; xuhaifeng@bjfu.edu.cn; yangsiyu2001@bjfu.edu.cn
OI Meng, Qingkuo/0000-0001-8155-6083
FU National Natural Science Foundation of China [31770589]; National
   Natural Science Foundation of China
FX This work was supported by the National Natural Science Foundation of
   China (31770589). We received their strong support during the paper
   revision process and sincerely thank the reviewing experts for their
   reasonable opinions.
CR Ausonio E, 2021, DRONES-BASEL, V5, DOI 10.3390/drones5010017
   Awad C, 2021, FIRE SAFETY J, V122, DOI 10.1016/j.firesaf.2021.103324
   Aydin B, 2019, DRONES-BASEL, V3, DOI 10.3390/drones3010017
   Bai Y., 2008, Study on compound suppression technique and emergency safety measure in forest fire fighting
   Bartenev I.M., 2018, P INT S ENG EARTH SC, P48, DOI [10.2991/isees-18.2018.10, DOI 10.2991/ISEES-18.2018.10]
   Cheney NP, 1998, INT J WILDLAND FIRE, V8, P1, DOI 10.1071/WF9980001
   Cruz MG, 2005, CAN J FOREST RES, V35, P1626, DOI [10.1139/x05-085, 10.1139/X05-085]
   Hädrich T, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459954
   Han XY, 2022, J FORESTRY RES, V33, P865, DOI 10.1007/s11676-021-01377-x
   Han YNB, 2018, AIVR 2018: 2018 INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY, P20, DOI 10.1145/3293663.3293669
   Hoffman CM, 2016, FIRE TECHNOL, V52, P221, DOI 10.1007/s10694-015-0500-3
   Huai Y., 2023, J. Image Graph, DOI [10.11834/jig.230016, DOI 10.11834/JIG.230016]
   Jellouli O, 2016, MATH COMP MODEL DYN, V22, P493, DOI 10.1080/13873954.2016.1204321
   Li JW, 2018, INT J PATTERN RECOGN, V32, DOI 10.1142/S021800141850026X
   Li Y, 2023, ISPRS INT J GEO-INF, V12, DOI 10.3390/ijgi12120474
   Liu XP, 2023, FIRE SAFETY J, V135, DOI 10.1016/j.firesaf.2022.103713
   Liu YX, 2018, ENVIRON MODELL SOFTW, V108, P14, DOI 10.1016/j.envsoft.2018.07.005
   Lopes AMG, 2019, J WIND ENG IND AEROD, V193, DOI 10.1016/j.jweia.2019.103967
   [陆承 Lu Cheng], 2022, [系统仿真学报, Journal of System Simulation], V34, P1312
   Meng QK, 2023, FORESTS, V14, DOI 10.3390/f14071371
   Meng QK, 2023, COMPUT GRAPH-UK, V110, P58, DOI 10.1016/j.cag.2022.12.002
   Miller C, 2000, LANDSCAPE ECOL, V15, P145, DOI 10.1023/A:1008181313360
   Moinuddin KAM, 2020, MATH COMPUT SIMULAT, V175, P81
   Moinuddin KAM, 2018, INT J WILDLAND FIRE, V27, P800, DOI 10.1071/WF17126
   Moinuddin K, 2021, FIRE SAFETY J, V125, DOI 10.1016/j.firesaf.2021.103422
   Morvan D, 2009, FIRE SAFETY J, V44, P50, DOI 10.1016/j.firesaf.2008.03.004
   Morvan D, 2013, FIRE SAFETY J, V58, P195, DOI 10.1016/j.firesaf.2013.01.027
   Mutthulakshmi K, 2020, CHINESE J PHYS, V65, P642, DOI 10.1016/j.cjph.2020.04.001
   Penney G, 2019, FIRE-BASEL, V2, DOI 10.3390/fire2010003
   Pirk S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130814
   Prakasha P.S., 2021, AIAA AV 2021 FOR, P2455, DOI [10.2514/6.2021-2455, DOI 10.2514/6.2021-2455]
   Rui XP, 2018, NAT HAZARDS, V91, P309, DOI 10.1007/s11069-017-3127-5
   Salis M, 2021, INT J DISAST RISK RE, V58, DOI 10.1016/j.ijdrr.2021.102189
   Scott JH, 2001, US FOR SERV RMRS-RP, P1
   Shu Yang, 2022, World Forestry Research, V35, P34, DOI 10.13348/j.cnki.sjlyyj.2021.0070.y
   Simpson KJ, 2022, TRENDS ECOL EVOL, V37, P749, DOI 10.1016/j.tree.2022.04.010
   Sun LY, 2021, FORESTS, V12, DOI 10.3390/f12111431
   Sun T, 2013, IEEE J-STARS, V6, P1971, DOI 10.1109/JSTARS.2012.2231956
   Wang XH, 2016, PROCEEDINGS OF 2016 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (SSCI), DOI 10.1109/SSCI.2016.7849971
   Wu ZC, 2022, ECOL INDIC, V136, DOI 10.1016/j.ecolind.2022.108653
   [张宏民 Zhang Hongmin], 2022, [工程热物理学报, Journal of Engineering Thermophysics], V43, P840
   Zheng Z, 2017, ECOL MODEL, V348, P33, DOI 10.1016/j.ecolmodel.2016.12.022
NR 42
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 6
PY 2024
DI 10.1007/s00371-024-03408-0
EA MAY 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PN5R5
UT WOS:001214775700002
DA 2024-08-05
ER

PT J
AU Tang, WJ
   Xie, Q
AF Tang, Wenjing
   Xie, Qing
TI A Thangka cultural element classification model based on self-supervised
   contrastive learning and MS Triplet Attention
SO VISUAL COMPUTER
LA English
DT Article
DE Tibetan Thangka classification; Sample imbalance problem;
   Self-supervised contrastive learning; Gradient Harmonizing Mechanism
   Loss; Attention mechanism
AB Being a significant repository of Buddhist imagery, Thangka images are valuable historical materials of Tibetan studies, which covers many domains such as Tibetan history, politics, culture, social life and even traditional medicine and astronomy. Thangka cultural element images are the essence of Thangka images. Hence, Thangka cultural element images classification is one of the most important works of knowledge representation and mining in the field of Thangka and is the foundation of digital protection of Thangka images. However, due to the limited quantity, high complexity and the intricate textures of Thangka images, the classification of Thangka images is limited to a small number of categories and coarse granularity. Thus, a novel fusion texture feature dual-branch Thangka cultural elements classification model based on the attention mechanism and self-supervised contrastive learning has been proposed in this paper. Specifically, to address the issue of insufficient labeled samples and improve the classification performance, this method utilizes a large amount of unlabeled irrelevant data to pre-train the feature extractor through self-supervised learning. During the fine-tuning stage of the downstream task, a dual-branch feature extraction structure incorporating texture features has been designed, and MS Triplet Attention proposed by us is used for the integration of important features. Additionally, to address the problem of sample imbalance and the existence of a large number of difficult samples in the Thangka cultural element dataset, the Gradient Harmonizing Mechanism Loss has been adopted, and it has been improved by introducing a self-designed adaptive mechanism. The experimental results on Thangka cultural elements dataset prove the superiority of the proposed method over the state-of-the-art methods. The source code of our proposed algorithm and the related datasets is available at https://github.com/WiniTang/MS-BiCLR.
C1 [Tang, Wenjing; Xie, Qing] Wuhan Univ Technol, Sch Comp Sci & Artificial Intelligence, Wuhan 430070, Peoples R China.
   [Xie, Qing] Minist Educ, Engn Res Ctr Intelligent Serv Technol Digital Publ, Wuhan, Peoples R China.
C3 Wuhan University of Technology
RP Xie, Q (corresponding author), Wuhan Univ Technol, Sch Comp Sci & Artificial Intelligence, Wuhan 430070, Peoples R China.; Xie, Q (corresponding author), Minist Educ, Engn Res Ctr Intelligent Serv Technol Digital Publ, Wuhan, Peoples R China.
EM WiniTang@whut.edu.cn; felixxq@whut.edu.cn
FU National College Students Innovation and Entrepreneurship Training
   Program [202310497053]; National College Students Innovation and
   Entrepreneurship Training Program [62271360]; National Natural Science
   Foundation of China; Tibet Institute of Scientific and Technical
   Information
FX This work is supported by National College Students Innovation and
   Entrepreneurship Training Program, 202310497053, and National Natural
   Science Foundation of China, 62271360, and we appreciate the data
   support from Tibet Institute of Scientific and Technical Information.
CR [Anonymous], 2021, COMPUTER TECHNOLOGY
   Bachman P, 2019, ADV NEUR IN, V32
   Balaram S, 2022, LECT NOTES COMPUT SC, V13431, P675, DOI 10.1007/978-3-031-16431-6_64
   Hua BS, 2018, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2018.00109
   Caron M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9630, DOI 10.1109/ICCV48922.2021.00951
   Chen D, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P1745, DOI 10.1109/ICASSP39728.2021.9413783
   Chen J, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2050
   Chen T., 2020, Adv. Neural Inf. Process. Syst., V33, P22243, DOI DOI 10.48550/ARXIV.2006.10029
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen XL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9620, DOI 10.1109/ICCV48922.2021.00950
   Chollet F., 2017, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2017.195
   Cong C, 2022, LECT NOTES COMPUT SC, V13583, P348, DOI 10.1007/978-3-031-21014-3_36
   Danzeng XR, 2021, LECT NOTES COMPUT SC, V12878, P74, DOI 10.1007/978-3-030-86608-2_9
   Fan YB, 2017, ADV NEUR IN, V30
   Fuliang Zeng, 2021, Journal of Physics: Conference Series, V1748, DOI 10.1088/1742-6596/1748/4/042054
   Gidaris S, 2019, IEEE I CONF COMP VIS, P8058, DOI 10.1109/ICCV.2019.00815
   Grill J-B., 2020, ADV NEURAL INFORM PR, V33, P21271, DOI DOI 10.48550/ARXIV.2006.07733
   He H., 2020, P IEEE CVF C COMP VI, P9729
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Krishnan R, 2022, NAT BIOMED ENG, V6, P1346, DOI 10.1038/s41551-022-00914-1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   L Z.F., 2021, THESIS NW MINZU U
   Le G, 2024, VISUAL COMPUT, V40, P4167, DOI 10.1007/s00371-023-03075-7
   Li BY, 2019, AAAI CONF ARTIF INTE, P8577
   Li X, 2023, J CHIN MINIMICROCOMP, P787
   Li X, 2020, IEEE T GEOSCI REMOTE, V58, P2615, DOI 10.1109/TGRS.2019.2952758
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li Y, 2023, IEEE T CYBERNETICS, V53, P5826, DOI 10.1109/TCYB.2022.3194099
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   [刘建伟 Liu Jianwei], 2021, [工程科学学报, Chinese Journal of Engineering], V43, P1499
   Liu X, 2023, IEEE T KNOWL DATA EN, V35, P857, DOI 10.1109/TKDE.2021.3090866
   Liu Y., 2023, ATTENTION AWARE DEEP, DOI [10.3233/ATDE230998, DOI 10.3233/ATDE230998]
   Ma YC, 2021, IMAGE VISION COMPUT, V108, DOI 10.1016/j.imavis.2021.104125
   Maenpaa T., 2003, MULTISCALE BINARY PA
   Misra D, 2021, IEEE WINT CONF APPL, P3138, DOI 10.1109/WACV48630.2021.00318
   Nair V., 2010, Proceedings of the 27th International Conference on Machine Learning (ICML-10), P807
   Nath V, 2022, LECT NOTES COMPUT SC, V13438, P297, DOI 10.1007/978-3-031-16452-1_29
   OJALA T, 1994, INT C PATT RECOG, P582, DOI 10.1109/ICPR.1994.576366
   Oord A.v.d., 2018, ARXIV
   Oquab M, 2023, ARXIV
   Park J, 2018, ARXIV
   Peng XY, 2022, PROC CVPR IEEE, P16010, DOI 10.1109/CVPR52688.2022.01556
   Qi Z., 2023, ARXIV
   Sifre L., 2014, ARXIV
   Tian Y., 2021, ARTIFICIAL INTELLIGE, P101
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang P, 2021, PROC CVPR IEEE, P943, DOI 10.1109/CVPR46437.2021.00100
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yang Han, 2022, ICBDT 2022: 2022 5th International Conference on Big Data Technologies (ICBDT), P273, DOI 10.1145/3565291.3565335
   Zbontar J, 2021, PR MACH LEARN RES, V139
   Zhan ZQ, 2024, IEEE J-STARS, V17, P1921, DOI 10.1109/JSTARS.2023.3342453
   Zhang H., 2021, Epsanet: An efficient pyramid squeeze attention block on convolutional neural network. arXiv
   Zhang ZY, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15205056
   Zhang ZX, 2022, SIGNAL IMAGE VIDEO P, V16, P1091, DOI 10.1007/s11760-021-02058-2
NR 55
TC 0
Z9 0
U1 8
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 3919
EP 3935
DI 10.1007/s00371-024-03397-0
EA APR 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001207644700001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Li, ZX
   He, QH
   Yang, WY
AF Li, Zhongxu
   He, Qihan
   Yang, Wenyuan
TI E-FPN: an enhanced feature pyramid network for UAV scenarios detection
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Computer vision; Object detection; Feature pyramid networks; UAV
ID OBJECT DETECTION; NEURAL-NETWORK; IMAGES
AB Unmanned aerial vehicle (UAV) is versatile machines that capture aerial images with a bird's eye view of objects from various perspectives and heights. They are widely utilized in both military and civilian applications. As the domain of computer vision advances, object detection becomes a solid mainstream for UAV applications. However, due to the flight altitude of the drone and the variation of shooting angle, drone images often contain small size, dense, and confusing targets, resulting in low detection accuracy. In this article, we propose a new approach called enhanced feature pyramid network (E-FPN) for detecting objects in UAV scenarios. Our E-FPN architecture incorporates the Simplified Spatial Pyramid Pooling-Fast (SimSPPF) structure into the backbone, enabling the extraction of features at four different scales. These features are subsequently passed through different layers of the E-FPN neck, facilitating the interaction between shallow and deep features. This process gains four distinct feature representations. Firstly, the input images are pre-processed by data augmentation. Then, the CSPDarknet53 with SimSPPF is used as the backbone to extract the multi-scale features from the visuals. Secondly, the integration of Cross-Stage Partial Stage modules into the E-FPN framework enhances the network's ability to capture target details. The E-FPN neck's top-down pathway integrates features from diverse layers and scales of the backbone, generating three richer and multi-scale representations of intermediate features. Meanwhile, its bottom-up pathway fuses semantic information from various layers and scales into four feature maps of equal channels but differing scales. Finally, a detector head is added to improve the accuracy of the detection, and the final results are obtained. The experimental results demonstrate that E-FPN-N achieves mAP50-95 performance of 37.7% on the MS COCO2017 dataset. Moreover, the precision and mAP50 of our model on the VisDrone validation dataset reached 67.5% and 62.0%, respectively.
C1 [Li, Zhongxu; Yang, Wenyuan] Minnan Normal Univ, Fujian Key Lab Granular Comp & Applicat, Zhangzhou 363000, Peoples R China.
   [Li, Zhongxu; He, Qihan; Yang, Wenyuan] Minnan Normal Univ, Sch Math & Stat, Zhangzhou 363000, Peoples R China.
C3 MinNan Normal University; MinNan Normal University
RP Yang, WY (corresponding author), Minnan Normal Univ, Fujian Key Lab Granular Comp & Applicat, Zhangzhou 363000, Peoples R China.; Yang, WY (corresponding author), Minnan Normal Univ, Sch Math & Stat, Zhangzhou 363000, Peoples R China.
EM zhongxulee18@163.com; qihanhe27@163.com; yangwycn@gmail.com
FU National Natural Science Foundation of China [12101289]; National
   Natural Science Foundation of China [2020J01821, 2022J01891]; Natural
   Science Foundation of Fujian Province; Institute of Meteorological Big
   Data-Digital Fujian and Fujian Key Laboratory of Data Science and
   Statistics (Minnan Normal University), China
FX The research is supported by the National Natural Science Foundation of
   China under Grant No.12101289 and the Natural Science Foundation of
   Fujian Province under Grant Nos.2020J01821 and 2022J01891. And it is
   supported by the Institute of Meteorological Big Data-Digital Fujian and
   Fujian Key Laboratory of Data Science and Statistics (Minnan Normal
   University), China.
CR Albaba BM, 2021, INT C PATT RECOG, P10227, DOI 10.1109/ICPR48806.2021.9412847
   Bayoudh K, 2022, VISUAL COMPUT, V38, P2939, DOI 10.1007/s00371-021-02166-7
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Chen CR, 2019, IEEE INT CONF COMP V, P100, DOI 10.1109/ICCVW.2019.00018
   Chen HT, 2023, Arxiv, DOI arXiv:2305.12972
   Chen JR, 2023, PROC CVPR IEEE, P12021, DOI 10.1109/CVPR52729.2023.01157
   Chen WJ, 2021, VISUAL COMPUT, V37, P805, DOI 10.1007/s00371-020-01831-7
   Chen Z, 2024, VISUAL COMPUT, DOI 10.1007/s00371-023-03225-x
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Du DW, 2019, IEEE INT CONF COMP V, P213, DOI 10.1109/ICCVW.2019.00030
   Fan JW, 2023, VISUAL COMPUT, V39, P319, DOI 10.1007/s00371-021-02331-y
   Feng CJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3490, DOI 10.1109/ICCV48922.2021.00349
   Feng D, 2021, IEEE T INTELL TRANSP, V22, P1341, DOI 10.1109/TITS.2020.2972974
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gu JJ, 2018, IEEE COMMUN MAG, V56, P82, DOI 10.1109/MCOM.2018.1700422
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Huang J, 2022, 2022 INT JOINT C NEU, P1
   Jiang YQ, 2022, Arxiv, DOI arXiv:2202.04256
   Jocher G., 2023, YOLO by Ultralytics
   Jocher G., 2020, Zenodo
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Kellenberger B, 2018, REMOTE SENS ENVIRON, V216, P139, DOI 10.1016/j.rse.2018.06.028
   Kiran BR, 2022, IEEE T INTELL TRANSP, V23, P4909, DOI 10.1109/TITS.2021.3054625
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li C., 2023, arXiv, DOI DOI 10.48550/ARXIV.2301.05586
   Li X., 2020, ADV NEURAL INF PROCE, V33, P21002
   Li YL, 2024, VISUAL COMPUT, V40, P4505, DOI 10.1007/s00371-023-03095-3
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2022, IEEE J-STARS, V15, P8085, DOI 10.1109/JSTARS.2022.3206399
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Lu WJ, 2023, IEEE J-STARS, V16, P1211, DOI 10.1109/JSTARS.2023.3234161
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Mehta S, 2022, Arxiv, DOI arXiv:2110.02178
   Redmon J., 2018, CoRR
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tan R., 2020, P IEEE CVF C COMP VI, DOI [10.1109/CVPR42600.2020.01079, DOI 10.1109/CVPR42600.2020.01079]
   Tang Y., 2022, Adv. Neural Inf. Process. Syst, V35, P9969
   Tian GY, 2021, NEUROCOMPUTING, V443, P292, DOI 10.1016/j.neucom.2021.03.016
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang CY, 2020, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW50498.2020.00203
   Wang GT, 2023, Arxiv, DOI [arXiv:2302.11816, DOI 10.1007/S00530-023-01134-6]
   Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929
   Wang TY, 2023, NEUROCOMPUTING, V547, DOI 10.1016/j.neucom.2023.126384
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Xu B, 2024, VISUAL COMPUT, V40, P1659, DOI 10.1007/s00371-023-02877-z
   Xu SL, 2022, Arxiv, DOI [arXiv:2203.16250, DOI 10.48550/ARXIV.2203.16250]
   Xu XZ, 2022, Arxiv, DOI arXiv:2211.15444
   Yang G, 2023, Arxiv, DOI arXiv:2306.15988
   Yu WP, 2021, IEEE WINT CONF APPL, P3257, DOI 10.1109/WACV48630.2021.00330
   Yu Z, 2022, arXiv, DOI DOI 10.1016/J.PATCOG.2024.110714
   Yuan WQ, 2023, VISUAL COMPUT, V39, P5199, DOI 10.1007/s00371-022-02654-4
   Zeng S, 2024, VISUAL COMPUT, V40, P1787, DOI 10.1007/s00371-023-02886-y
   Zhang Guowei, 2023, Cognitive Computation and Systems: First International Conference, ICCCS 2022, Revised Selected Papers. Communications in Computer and Information Science (1732), P3, DOI 10.1007/978-981-99-2789-0_1
   Zhang RQ, 2022, NEUROCOMPUTING, V489, P377, DOI 10.1016/j.neucom.2022.03.033
   Zhang RQ, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12193140
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhao HP, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20071861
   Zhao QJ, 2019, AAAI CONF ARTIF INTE, P9259
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
   Zhu PF, 2022, IEEE T PATTERN ANAL, V44, P7380, DOI 10.1109/TPAMI.2021.3119563
   Zou ZX, 2023, P IEEE, V111, P257, DOI 10.1109/JPROC.2023.3238524
NR 74
TC 0
Z9 0
U1 18
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 22
PY 2024
DI 10.1007/s00371-024-03355-w
EA APR 2024
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OG2J3
UT WOS:001206047800001
DA 2024-08-05
ER

PT J
AU Chen, ZY
   Zhao, Y
   He, JL
   Lu, YJ
   Cui, ZW
   Li, WT
   Zhang, YJ
AF Chen, Ziyang
   Zhao, Yang
   He, Junling
   Lu, Yujie
   Cui, Zhongwei
   Li, Wenting
   Zhang, Yongjun
TI Feature distribution normalization network for multi-view stereo
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE 3D reconstruction; Multi-view stereo; Depth estimation; Stereo vision
AB As a key technique in 3D reconstruction, research in multi-view stereo (MVS) has made significant progress with the development of deep learning. However, MVS faces challenges due to inconsistent feature distributions across different viewpoints. This phenomenon has been recognized as a significant bottleneck in MVS. To overcome this limitation, we propose an innovative approach called feature distribution normalization network (FDN-MVS). To mitigate errors arising from inconsistent relative poses, we leverage homography transformations between multiple views and propose a distribution residual refinement (DRR) module. Expanding the scale, residual connection, and coordinate system fixation results in more meaningful feature distribution learning. In order to further narrow the disparities among different views, we suggest feature normalization, constraining the computation of features within a common distribution. Experimental results demonstrate the effective of our method. Compared to the commonly used benchmark algorithm CasMVSNet, we reduced the error by 13.52% on the DTU dataset and improved the F-score by 18.77% on the Tanks and Temples dataset. Code is available at https://github.com/ZYangChen/FDN-MVS.
C1 [Chen, Ziyang; Zhao, Yang; He, Junling; Lu, Yujie; Zhang, Yongjun] Guizhou Univ, Inst Artificial Intelligence, State Key Lab Publ Big Data, Guiyang 550025, Guizhou, Peoples R China.
   [Cui, Zhongwei] Guizhou Normal Univ, Sch Math & Big Data, Guiyang 550018, Guizhou, Peoples R China.
   [Li, Wenting] Guizhou Univ Commerce, Comp & Informat Engn Coll, Guiyang 550014, Guizhou, Peoples R China.
C3 Guizhou University; Guizhou Normal University; Guizhou University of
   Commerce
RP Zhang, YJ (corresponding author), Guizhou Univ, Inst Artificial Intelligence, State Key Lab Publ Big Data, Guiyang 550025, Guizhou, Peoples R China.
EM ziyangchen2000@gmail.com; z2haoyang@163.com; hjlqiuqiu1@qq.com;
   luyujie79@163.com; zhongweicui@gznc.edu.cn; 201520274@gzcc.edu.cn;
   zyj6667@126.com
RI Chen, Ziyang/KHX-1531-2024
OI Chen, Ziyang/0000-0002-9361-0240; Zhang, Yongjun/0000-0002-7534-1219
FU Science and Technology Planning Project of Guizhou Province, Department
   of Science and Technology of Guizhou Province, China [[2023]159]
FX We are grateful to many friends and colleagues for the discussions on
   this work, including Wei Long (Guizhou University), Haoliang Zhao (The
   Hong Kong Polytechnic University, Guizhou University), Weihao Gao
   (Guizhou University), Xuexue Zhang (Guizhou University), He Yao (Guizhou
   University), Yabo Wu (Guizhou University), and Shu Chen (Southwestern
   University of Finance and Economics). This research is supported by
   Science and Technology Planning Project of Guizhou Province, Department
   of Science and Technology of Guizhou Province, China (Project
   No.[2023]159).
CR Aanæs H, 2016, INT J COMPUT VISION, V120, P153, DOI 10.1007/s11263-016-0902-9
   Agarap A.F., 2018, arXiv, DOI 10.48550/arXiv.1803.08375
   Al-Kababji Ayman, 2022, Intelligent Systems and Pattern Recognition: Second International Conference, ISPR 2022, Revised Selected Papers. Communications in Computer and Information Science (1589), P204, DOI 10.1007/978-3-031-08277-1_17
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Chu, 2023, PROC AAAI C ARTIF IN, P3091
   Chung JY, 2014, Arxiv, DOI [arXiv:1412.3555, DOI 10.48550/ARXIV.1412.3555]
   Dubrofsky E, 2009, HOMOGRAPHY ESTIMATIO
   Galliani S., 2016, A Practical Solution To Measuring Outcomes in youthwork practice, V25, P2
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Gu XD, 2020, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR42600.2020.00257
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Holynski A, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275083
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jensen R, 2014, PROC CVPR IEEE, P406, DOI 10.1109/CVPR.2014.59
   Ji MQ, 2017, IEEE I CONF COMP VIS, P2326, DOI 10.1109/ICCV.2017.253
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Kar A, 2017, ADV NEUR IN, V30
   Kasap Z, 2012, VISUAL COMPUT, V28, P87, DOI 10.1007/s00371-011-0630-7
   Knapitsch A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073599
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lin X., 2021, IEEE Trans. Multimed.
   Lipson L, 2021, INT CONF 3D VISION, P218, DOI 10.1109/3DV53792.2021.00032
   Liu Y, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3265739
   Loshchilov I, 2019, Arxiv, DOI [arXiv:1711.05101, 10.48550/arXiv.1711.05101, DOI 10.48550/ARXIV.1711.05101]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Song S, 2023, PATTERN RECOGN, V136, DOI 10.1016/j.patcog.2022.109198
   Teed Z., 2020, EUR C COMP VIS, P402, DOI DOI 10.1007/978-3-030-58536-524
   Wang FJ, 2021, PROC CVPR IEEE, P14189, DOI 10.1109/CVPR46437.2021.01397
   Wang SQ, 2022, PROC CVPR IEEE, P8645, DOI 10.1109/CVPR52688.2022.00846
   Wang X, 2021, DISPLAYS, V70, DOI 10.1016/j.displa.2021.102102
   Wei Z., 2022, IEEE Trans Visual Comput Grap (TVCG)
   Xu GW, 2023, PROC CVPR IEEE, P21919, DOI 10.1109/CVPR52729.2023.02099
   Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47
   Yao Y, 2020, PROC CVPR IEEE, P1787, DOI 10.1109/CVPR42600.2020.00186
   Yao Y, 2019, PROC CVPR IEEE, P5520, DOI 10.1109/CVPR.2019.00567
   Zaremba W, 2015, Arxiv, DOI [arXiv:1409.2329, DOI 10.48550/ARXIV.1409.2329]
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang JY, 2023, INT J COMPUT VISION, V131, P199, DOI 10.1007/s11263-022-01697-3
   Zhou HZ, 2023, NEURAL NETWORKS, V162, P502, DOI 10.1016/j.neunet.2023.03.012
NR 41
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 17
PY 2024
DI 10.1007/s00371-024-03334-1
EA APR 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NY9U8
UT WOS:001204140500003
DA 2024-08-05
ER

PT J
AU Liu, YX
   Zhan, L
   Feng, Y
   Si, PJ
   Jiang, SW
   Zhao, Q
   Yan, CG
AF Liu, Yixiu
   Zhan, Long
   Feng, Yu
   Si, Pengju
   Jiang, Shaowei
   Zhao, Qiang
   Yan, Chenggang
TI Loose-tight cluster regularization for unsupervised person
   re-identification
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Unsupervised person Re-ID; Silhouette coefficient; Loose-tight cluster;
   Regularization
AB Unsupervised person re-identification (Re-ID) is a critical and challenging task in computer vision. It aims to identify the same person across different camera views or locations without using any labeled data or annotations. Most existing unsupervised Re-ID methods adopt a clustering and fine-tuning strategy, which alternates between generating pseudo-labels through clustering and updating the model parameters through fine-tuning. However, this strategy has two major drawbacks: (1) the pseudo-labels obtained by clustering are often noisy and unreliable, which may degrade the model performance; and (2) the model may overfit to the pseudo-labels and lose its generalization ability during fine-tuning. To address these issues, we propose a novel method that integrates silhouette coefficient-based label correction and contrastive loss regularization based on loose-tight cluster guidance. Specifically, we use silhouette coefficients to measure the quality of pseudo-labels and correct the potential noisy labels, thereby reducing their negative impact on model training. Moreover, we introduce a new contrastive loss regularization term that consists of two components: a cluster-level contrast loss that encourages the model to learn discriminative features, and a regularization loss that prevents the model from overfitting to the pseudo-labels. The weights of these components are dynamically adjusted according to the silhouette coefficients. Furthermore, we adopt Vision Transformer as the backbone network to extract more robust features. We conduct extensive experiments on several public datasets and demonstrate that our method achieves significant improvements over the state-of-the-art unsupervised Re-ID methods.
C1 [Liu, Yixiu; Zhan, Long; Jiang, Shaowei; Zhao, Qiang; Yan, Chenggang] Hangzhou Dianzi Univ, Hangzhou 310018, Peoples R China.
   [Feng, Yu] Fudan Univ, Shanghai 200433, Peoples R China.
   [Si, Pengju] Henan Univ Sci & Technol, Luoyang 471023, Peoples R China.
   [Jiang, Shaowei; Zhao, Qiang] Hangzhou Dianzi Univ, Lishui Inst, Lishui 323000, Peoples R China.
C3 Hangzhou Dianzi University; Fudan University; Henan University of
   Science & Technology; Hangzhou Dianzi University
RP Si, PJ (corresponding author), Henan Univ Sci & Technol, Luoyang 471023, Peoples R China.
EM sipengju@haust.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Chen Z., 2023, IEEE Trans. Pattern Anal. Mach. Intell, V45, p13 489
   Chen ZQ, 2023, IEEE T CIRC SYST VID, V33, P5908, DOI 10.1109/TCSVT.2023.3261898
   Cheng H, 2021, Arxiv, DOI arXiv:2010.02347
   Cho Y, 2022, PROC CVPR IEEE, P7298, DOI 10.1109/CVPR52688.2022.00716
   Dai Z., 2021, ASIAN C COMPUTER VIS
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fan HH, 2017, Arxiv, DOI arXiv:1705.10444
   Feng Y., 2022, 2022 IEEE INT C MULT, P1
   Fu DP, 2021, PROC CVPR IEEE, P14745, DOI 10.1109/CVPR46437.2021.01451
   Ge YX, 2020, Arxiv, DOI [arXiv:2006.02713, 10.48550/arXiv.2006.02713]
   Ge YX, 2020, Arxiv, DOI [arXiv:2001.01526, 10.48550/arXiv.2001.01526]
   He Q., 2023, IEEE T INTELLIGENT T
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kaiwei Zeng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13654, DOI 10.1109/CVPR42600.2020.01367
   Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616
   Lan L, 2023, IEEE T IMAGE PROCESS, V32, P3338, DOI 10.1109/TIP.2023.3278860
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Li SH, 2022, IEEE T CIRC SYST VID, V32, P3825, DOI 10.1109/TCSVT.2021.3118060
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Lin YT, 2020, PROC CVPR IEEE, P3387, DOI 10.1109/CVPR42600.2020.00345
   Lin YT, 2019, AAAI CONF ARTIF INTE, P8738
   Luo H., 2021, arXiv
   Ma AJ, 2013, IEEE I CONF COMP VIS, P3567, DOI 10.1109/ICCV.2013.443
   Miao ZZ, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2173
   Peng JJ, 2023, IEEE T CIRC SYST VID, V33, P5802, DOI 10.1109/TCSVT.2023.3258917
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sheng B, 2022, IEEE T CYBERNETICS, V52, P6662, DOI 10.1109/TCYB.2021.3079311
   Shi JG, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2094
   Si TZ, 2023, VISUAL COMPUT, V39, P5121, DOI 10.1007/s00371-022-02649-1
   Simoudis E., 1996, KDD 96 P 2 INT C KNO, P226
   Sun LB, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2187
   Sun Y., 2017, EUROPEAN C COMPUTER
   Tao YS, 2022, IEEE IJCNN, DOI 10.1109/IJCNN55064.2022.9892516
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Wang Z, 2019, EUROPEAN C COMPUTER
   WARD JH, 1963, J AM STAT ASSOC, V58, P236, DOI 10.2307/2282967
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Xing EP, 2003, NIPS 15, V15, P505
   Xu MY, 2023, IEEE T INTELL TRANSP, V24, P3395, DOI 10.1109/TITS.2022.3224233
   Xu Q, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2070
   Yang EZ, 2022, INT CONF SIGN PROCES, P325, DOI 10.1109/ICSP56322.2022.9965305
   Yu HX, 2019, PROC CVPR IEEE, P2143, DOI 10.1109/CVPR.2019.00225
   Zhai YP, 2020, Arxiv, DOI arXiv:2007.01546
   Zhang X, 2021, PROC CVPR IEEE, P3435, DOI 10.1109/CVPR46437.2021.00344
   Zhang XY, 2022, PROC CVPR IEEE, P7359, DOI 10.1109/CVPR52688.2022.00722
   Zhao CR, 2020, IEEE T MULTIMEDIA, V22, P3180, DOI 10.1109/TMM.2020.2972125
   Zheng K., 2020, AAAI C ARTIFICIAL IN
   Zheng KC, 2021, PROC CVPR IEEE, P5306, DOI 10.1109/CVPR46437.2021.00527
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhu K., 2022, EUROPEAN C COMPUTER
NR 61
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 29
PY 2024
DI 10.1007/s00371-024-03329-y
EA MAR 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MO7Y7
UT WOS:001194640300002
DA 2024-08-05
ER

PT J
AU Li, YS
   Yang, Z
   Qi, JW
   Gao, JP
AF Li, Yingsong
   Yang, Zhen
   Qi, Junwei
   Gao, Jingpeng
TI An enhanced multi-scale weight assignment strategy of two-exposure
   fusion
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Two-exposure image fusion; Guided filtering; Multi-scale image
   decomposition; Weight assignment strategy
ID EXPOSURE IMAGE FUSION; DECOMPOSITION
AB Multi-exposure image fusion (MEF) is a convenient way to get high dynamic range (HDR) images. However, when the input sequence with a large difference in exposure time, the existing MEF algorithms cannot well reconstruct relative contrast, resulting in loss of detail in underexposed or overexposed region. In order to get the details from the images as much as possible, an enhanced multi-scale weight assignment strategy is proposed in this paper. First, the input image guided by itself is decomposed into the first level with base layer (BL) and detail layer (DL) using guided filtering. Then, this BL continues to be decomposed into the second level like the first level. With the going of decomposition, the image can be decomposed into several DLs and 1 BL. Afterward, the image information contained in BLs and DLs is extracted by using the exposure weights and the global gradient weights to reconstruct the fused image, respectively. Finally, 39 sets of two-exposure image sequences are selected and compared with nine representative algorithms. Experimental results show that the proposed algorithm has good visual performance in subjective evaluation and state-of-the-art performance in objective evaluation.
C1 [Li, Yingsong; Yang, Zhen; Qi, Junwei; Gao, Jingpeng] Harbin Engn Univ, Coll Informat & Commun Engn, Nantong St, Harbin 150001, Heilongjiang, Peoples R China.
C3 Harbin Engineering University
RP Qi, JW (corresponding author), Harbin Engn Univ, Coll Informat & Commun Engn, Nantong St, Harbin 150001, Heilongjiang, Peoples R China.
EM qijunwei@hrbeu.edu.cn
FU Fundamental Research Funds for the Central Universities
   [3072022QBZ0803]; Fundamental Research Funds for the Central
   Universities [2018M631911]; China Postdoctoral Science Foundation
   [LBH-Z18055]; Heilongjiang Postdoctoral Foundation, China
FX This work was supported by Fundamental Research Funds for the Central
   Universities (No. 3072022QBZ0803), the China Postdoctoral Science
   Foundation (No. 2018M631911), and the Heilongjiang Postdoctoral
   Foundation, China (No. LBH-Z18055).
CR Bavirisetti DP, 2019, CIRC SYST SIGNAL PR, V38, P5576, DOI 10.1007/s00034-019-01131-z
   Burt P. J., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P173, DOI 10.1109/ICCV.1993.378222
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Gan W, 2015, INFRARED PHYS TECHN, V72, P37, DOI 10.1016/j.infrared.2015.07.003
   Han D, 2022, INFORM FUSION, V79, P248, DOI 10.1016/j.inffus.2021.10.006
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Karakaya D., 2021, arXiv
   Karr BA, 2021, VISUAL COMPUT, V37, P895, DOI 10.1007/s00371-020-01841-5
   Kou F, 2018, J VIS COMMUN IMAGE R, V53, P235, DOI 10.1016/j.jvcir.2018.03.020
   Lee SH, 2018, IEEE IMAGE PROC, P1737, DOI 10.1109/ICIP.2018.8451153
   Li H, 2020, IEEE T IMAGE PROCESS, V29, P5805, DOI 10.1109/TIP.2020.2987133
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Liu SG, 2019, IEEE T CONSUM ELECTR, V65, P303, DOI 10.1109/TCE.2019.2893644
   Liu X., 2022, arXiv
   Liu Y, 2015, J VIS COMMUN IMAGE R, V31, P208, DOI 10.1016/j.jvcir.2015.06.021
   Ma KD, 2018, IEEE T COMPUT IMAG, V4, P60, DOI 10.1109/TCI.2017.2786138
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P2519, DOI 10.1109/TIP.2017.2671921
   Ma KD, 2015, IEEE IMAGE PROC, P1717, DOI 10.1109/ICIP.2015.7351094
   Malik M., 2008, Lecture Notes in Engineering & Computer Science, V1, P2170
   Mertens T, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P382, DOI 10.1109/PG.2007.17
   Paul S, 2016, J CIRCUIT SYST COMP, V25, DOI 10.1142/S0218126616501231
   Prabhakar KR, 2017, IEEE I CONF COMP VIS, P4724, DOI 10.1109/ICCV.2017.505
   Qi Y., 2019, Opto-Electron. Eng., V46
   Qiu XH, 2019, SIGNAL PROCESS-IMAGE, V72, P35, DOI 10.1016/j.image.2018.12.004
   Sha-Wo Huang, 2021, 2021 55th Asilomar Conference on Signals, Systems, and Computers, P867, DOI 10.1109/IEEECONF53345.2021.9723089
   Shen JB, 2012, VISUAL COMPUT, V28, P463, DOI 10.1007/s00371-011-0642-3
   Shen R, 2011, IEEE T IMAGE PROCESS, V20, P3634, DOI 10.1109/TIP.2011.2150235
   Wang CM, 2021, VISUAL COMPUT, V37, P1233, DOI 10.1007/s00371-021-02079-5
   Wang QT, 2020, IEEE T CIRC SYST VID, V30, P2418, DOI 10.1109/TCSVT.2019.2919310
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Xydeas CS, 2000, ELECTRON LETT, V36, P308, DOI 10.1049/el:20000267
   Yang Y, 2018, IEEE SIGNAL PROC LET, V25, P1885, DOI 10.1109/LSP.2018.2877893
   Zhang XC, 2021, INFORM FUSION, V74, P111, DOI 10.1016/j.inffus.2021.02.005
   Zhao H., 2023, Vis. Comput, V06, P1
NR 35
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 19
PY 2024
DI 10.1007/s00371-023-03258-2
EA FEB 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IW6W4
UT WOS:001169424000001
DA 2024-08-05
ER

PT J
AU Gan, Y
   Xiao, XY
   Xiang, T
AF Gan, Yan
   Xiao, Xinyao
   Xiang, Tao
TI Attribute-guided face adversarial example generation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Face adversarial example; Image manipulation; Semantic attribute;
   Adversarial attack
AB Deep neural networks (DNNs) are susceptible to adversarial examples generally generated by adding imperceptible perturbations to the clean images, resulting in the degraded performance of DNNs models. To generate adversarial examples, most methods utilize the Lp\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$L_p$$\end{document} norm to limit the perturbations and satisfy such imperceptibility. However, the Lp\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$L_p$$\end{document} norm cannot fully guarantee the semantic authenticity of adversarial examples. Defenses may take advantage of this defect to weaken the attack capability of adversarial examples. Moreover, existing methods with Lp\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$L_p$$\end{document} restriction have poor generalization ability in white-box attacks and have inferior aggressiveness in black-box attacks. To solve the problems mentioned above, we propose a multiple feature interpolation method to generate face adversarial examples. In the proposed method, we perform the multiple feature interpolation to generate face adversarial examples with new semantics in the process of original image reconstruction and conditional attribute-guided image generation based on StarGAN. Experimental results demonstrate that adversarial examples generated by our method possess new attribute-guided semantics and satisfactory attack success rates under both white-box and black-box settings.
C1 [Gan, Yan; Xiao, Xinyao; Xiang, Tao] Chongqing Univ, Coll Comp Sci, Chongqing 400044, Peoples R China.
C3 Chongqing University
RP Xiang, T (corresponding author), Chongqing Univ, Coll Comp Sci, Chongqing 400044, Peoples R China.
EM shiyangancq@cqu.edu.cn; xyxiaoo@cqu.edu.cn; txiang@cqu.edu.cn
RI Xiang, Tao/N-3706-2016
OI Xiang, Tao/0000-0002-9439-4623
FU Key Technologies Research and Development Program [2022YFB3103500];
   National Key R &D Program of China [62106026, U20A20176, 62072062];
   National Natural Science Foundation of China [GZC20233323]; Postdoctoral
   Fellowship Program of CPSF [cstc2021jcyj-msxmX0273,
   cstc2022ycjhbgzxm0031]; Natural Science Foundation of Chongqing
   [2021YFQ0056]; Sichuan Science and Technology Program [2023CDJXY-039];
   Fundamental Research Funds for the Central Universities
FX This work was supported by the National Key R &D Program of China
   (2022YFB3103500), National Natural Science Foundation of China
   (62106026, U20A20176, and 62072062), Postdoctoral Fellowship Program of
   CPSF (GZC20233323), Natural Science Foundation of Chongqing
   (cstc2021jcyj-msxmX0273 and cstc2022ycjhbgzxm0031), Sichuan Science and
   Technology Program (2021YFQ0056) and Fundamental Research Funds for the
   Central Universities (2023CDJXY-039). We would like to express our
   deepest gratitude to Deqiang Ouyang from Chongqing University for his
   linguistic help.
CR Bhattad A, 2020, Arxiv, DOI [arXiv:1904.06347, DOI 10.48550/ARXIV.1904.06347]
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Cheng BW, 2022, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR52688.2022.00135
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957
   Du J., 2019, arXiv
   Gan Y, 2023, APPL SOFT COMPUT, V136, DOI 10.1016/j.asoc.2023.110086
   Gan Y, 2023, INFORM FUSION, V89, P143, DOI 10.1016/j.inffus.2022.08.006
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   Han GX, 2022, AAAI CONF ARTIF INTE, P780
   Haonan Qiu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P19, DOI 10.1007/978-3-030-58568-6_2
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Hsu SY, 2019, LECT NOTES COMPUT SC, V11364, P338, DOI 10.1007/978-3-030-20870-7_21
   Goodfellow IJ, 2015, Arxiv, DOI [arXiv:1412.6572, DOI 10.48550/ARXIV.1412.6572]
   Joshi A, 2019, IEEE I CONF COMP VIS, P4772, DOI 10.1109/ICCV.2019.00487
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2018, Arxiv, DOI [arXiv:1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Karthik K, 2021, VISUAL COMPUT, V37, P1837, DOI 10.1007/s00371-020-01941-2
   Kurakin A, 2017, Arxiv, DOI arXiv:1607.02533
   Li B, 2022, PROC CVPR IEEE, P6980, DOI 10.1109/CVPR52688.2022.00686
   Li T, 2022, IEEE SIGNAL PROC LET, V29, P827, DOI 10.1109/LSP.2022.3157517
   Liu H., 2023, Learning hierarchical preferences for recommendation with mixture intention neural stochastic processes, P1, DOI [10.1109/TKDE.2023.3348493, DOI 10.1109/TKDE.2023.3348493]
   Liu YH, 2022, Arxiv, DOI arXiv:2109.12492
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Ltd. A.C.C., AliYun Compare-API
   Ltd. M.T., FACE++ Compare-API
   Madry A, 2019, Arxiv, DOI arXiv:1706.06083
   Ranjan Rajeev, 2019, IEEE Transactions on Biometrics, Behavior, and Identity Science, V1, P82, DOI 10.1109/TBIOM.2019.2908436
   Sun Y., 2014, NEURIPS, V27
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Wang C.-C., 2019, ISCAS, P1
   Wang L, 2022, VISUAL COMPUT, V38, P2009, DOI 10.1007/s00371-021-02262-8
   Wang YL, 2018, IEEE WINT CONF APPL, P112, DOI 10.1109/WACV.2018.00019
   Xie CH, 2019, PROC CVPR IEEE, P2725, DOI 10.1109/CVPR.2019.00284
   Zhang G, 2018, LECT NOTES COMPUT SC, V11210, P422, DOI 10.1007/978-3-030-01231-1_26
   Zhang XC, 2018, AAAI CONF ARTIF INTE, P7566
   Zhao ZY, 2020, PROC CVPR IEEE, P1036, DOI 10.1109/CVPR42600.2020.00112
   Zhou F, 2019, VISUAL COMPUT, V35, P1583, DOI 10.1007/s00371-018-1559-x
   Zhu MK, 2021, PR MACH LEARN RES, V139
NR 43
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 7
PY 2024
DI 10.1007/s00371-024-03265-x
EA FEB 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HB6E8
UT WOS:001157060800001
DA 2024-08-05
ER

PT J
AU Gupta, S
   Vishwakarma, DK
   Puri, NK
AF Gupta, Shaurya
   Vishwakarma, Dinesh Kumar
   Puri, Nitin Kumar
TI A human activity recognition framework in videos using segmented human
   subject focus
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Convolutional network; Deep learning; Human activity recognition; Image
   segmentation; Bi-directional LSTMs
AB Automating tasks through human activity recognition in video data has become increasingly vital. Deep learning has yielded versatile activity recognition systems applicable in surveillance, healthcare analysis, sports, and human-computer interaction. Despite various proposed video-based activity recognition techniques over the years, the reliance over RGB frames, accompanied by other modalities like joint locations and depth maps, often proves less effective compared to multimodal methods. In response to this challenge, our paper introduces a competitive approach for identifying human activity in video frames. Leveraging a Convolutional Long Short-Term Memory (Conv-LSTM) network and a novel pre-processing step involving a Human Segmentation network, our method accentuates human subjects in each frame using segmentation maps. These highlighted frames undergo further processing through Convolutional Neural Networks (CNNs) to learn feature vectors, without including other modalities with RGB frames directly. The learned features are then subjected to Long Short-Term Memory (LSTM) units for comprehending sequential video data and drawing meaningful inferences. The proposed methodology undergoes rigorous testing on three publicly available datasets-KARD, MSR Daily Activity, and SBU-Interactions. Remarkably, our approach outperforms similar state-of-the-art methods, achieving benchmark accuracy scores exceeding 98% on MSR Daily Activity and 99% on KARD and SBU-Interactions datasets. In essence, our method not only provides a competitive solution for human activity recognition in video frames but also contributes to advancing the field by integrating Conv-LSTM networks and innovative pre-processing techniques. The comprehensive evaluation on multiple datasets underlines the robustness and superior performance of our proposed approach.
C1 [Gupta, Shaurya; Puri, Nitin Kumar] Delhi Technol Univ, Dept Appl Phys, Delhi, India.
   [Vishwakarma, Dinesh Kumar] Delhi Technol Univ, Dept Informat Technol, Delhi, India.
C3 Delhi Technological University; Delhi Technological University
RP Vishwakarma, DK (corresponding author), Delhi Technol Univ, Dept Informat Technol, Delhi, India.
EM dinesh@dtu.ac.in; nitinkumarpuri@dtu.ac.in
RI VISHWAKARMA, DINESH KUMAR/L-3815-2018
OI VISHWAKARMA, DINESH KUMAR/0000-0002-1026-0047
CR Andrade-Ambriz YA, 2022, EXPERT SYST APPL, V191, DOI 10.1016/j.eswa.2021.116287
   Ashwini K, 2021, MULTIMED TOOLS APPL, V80, P10839, DOI 10.1007/s11042-020-10327-4
   Baccouche M., 2012, P 2 INT C HUM BEH UN, V34
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Cippitelli E, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/4351435
   Dhiman C, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3441628
   Dhiman C, 2020, IEEE T IMAGE PROCESS, V29, P3835, DOI 10.1109/TIP.2020.2965299
   Dhiman C, 2019, IEEE SENS J, V19, P5195, DOI 10.1109/JSEN.2019.2903645
   Dhiman C, 2019, ENG APPL ARTIF INTEL, V77, P21, DOI 10.1016/j.engappai.2018.08.014
   Dong-Jin Kim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P718, DOI 10.1007/978-3-030-58589-1_43
   El Madany NE, 2018, IEEE INT CON MULTI
   Elboushaki A, 2020, EXPERT SYST APPL, V139, DOI 10.1016/j.eswa.2019.112829
   Fei-Fei L., 2010, J. Vis, V9, P1037, DOI [10.1167/9.8.1037, DOI 10.1167/9.8.1037]
   Gaglio S, 2015, IEEE T HUM-MACH SYST, V45, P586, DOI 10.1109/THMS.2014.2377111
   Garcia NC, 2018, LECT NOTES COMPUT SC, V11212, P106, DOI 10.1007/978-3-030-01237-3_7
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang WB, 2023, ACM T EMBED COMPUT S, V22, DOI 10.1145/3551486
   Huang WB, 2023, IEEE T MOBILE COMPUT, V22, P5064, DOI 10.1109/TMC.2022.3174816
   Jain Aayush, 2020, 2020 Proceedings of the International Conference on Communication and Signal Processing (ICCSP), P0813, DOI 10.1109/ICCSP48568.2020.9182433
   Jeevan M, 2013, IEEE IMAGE PROC, P4195, DOI 10.1109/ICIP.2013.6738864
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Lu XK, 2022, IEEE T PATTERN ANAL, V44, P7885, DOI 10.1109/TPAMI.2021.3115815
   Sabih M, 2022, VISUAL COMPUT, V38, P1719, DOI 10.1007/s00371-021-02100-x
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Shahroudy A, 2018, IEEE T PATTERN ANAL, V40, P1045, DOI 10.1109/TPAMI.2017.2691321
   Simonyan K, 2014, ADV NEUR IN, V27
   Singh T, 2021, NEURAL COMPUT APPL, V33, P469, DOI 10.1007/s00521-020-05018-y
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Straka M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.69
   Supervisely, Supervisely Person Dataset-Datasets-Supervisely
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan MX, 2019, PR MACH LEARN RES, V97
   Thien HT, 2018, INFORM SCIENCES, V444, P20, DOI 10.1016/j.ins.2018.02.042
   Ullah A, 2018, IEEE ACCESS, V6, P1155, DOI 10.1109/ACCESS.2017.2778011
   Vishwakarma DK, 2015, EXPERT SYST APPL, V42, P6957, DOI 10.1016/j.eswa.2015.04.039
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Weng WH, 2021, IEEE ACCESS, V9, P16591, DOI 10.1109/ACCESS.2021.3053408
   Yadav A, 2020, MULTIMEDIA SYST, V26, P431, DOI 10.1007/s00530-020-00656-7
   Yan GL, 2021, COMPUT AIDED GEOM D, V86, DOI 10.1016/j.cagd.2021.101964
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yun K., 2012, 2012 IEEE COMP SOC C, DOI DOI 10.1109/CVPRW.2012.6239234
   Zebhi S, 2021, INT J MACH LEARN CYB, V12, P3449, DOI 10.1007/s13042-021-01383-9
   Zeng M, 2014, 2014 6TH INTERNATIONAL CONFERENCE ON MOBILE COMPUTING, APPLICATIONS AND SERVICES (MOBICASE), P197, DOI 10.4108/icst.mobicase.2014.257786
   Zhu JG, 2018, Arxiv, DOI arXiv:1812.05770
NR 50
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 6
PY 2024
DI 10.1007/s00371-023-03256-4
EA FEB 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GY9N6
UT WOS:001156357300001
DA 2024-08-05
ER

PT J
AU Lin, WM
   Li, YH
   Xu, C
   Liu, LL
AF Lin, Wanmin
   Li, Yuhui
   Xu, Chen
   Liu, Lilin
TI A motion denoising algorithm with Gaussian self-adjusting threshold for
   event camera
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Event camera; Denoising; Gaussian time distribution; Motion estimation
ID VISION; FILTER
AB Event cameras, characterized by their low power consumption, expansive dynamic range, and high temporal resolution, have attracted great attentions in various computer vision tasks. Compared to frame-based cameras, event cameras exemplify a marked paradigmatic transition in data formation and output. However, the quality of event streams is compromised by background activity and hot pixels, leading to increased computational overheads and sub-optimal outcomes in subsequent applications, notably in recognition, video reconstruction, and target detection tasks. In this paper, a two-step denoising algorithm (referred as GMCM) is proposed to counteract these challenges. The GMCM algorithm comprises two steps: Gaussian denoising preprocessing and motion denoising. The former incorporates Gaussian temporal distribution and adaptive thresholding mechanisms to discern the inclusion of motion-related information within the event streams. Experimental results demonstrate that Gaussian denoising preprocessing can not only adeptly discern whether the event data stream contains motion information but also enhance computational efficiency. Conclusively, the GMCM algorithm achieves state-of-the-art performance, yielding SNR scores of 37.22 and 26.79 on the DVSCLEAN dataset at the noise ratios of 50% and 100%, respectively.
C1 [Lin, Wanmin; Li, Yuhui; Liu, Lilin] Sun Yat Sen Univ, Sch Elect & Informat Technol, State Key Lab Optoelect Mat & Technol, Xingang West Rd 135, Guangzhou 510275, Guangdong, Peoples R China.
   [Xu, Chen] Pazhou Lab Huangpu, Guangzhou, Guangdong, Peoples R China.
C3 Sun Yat Sen University; Pazhou Lab
RP Liu, LL (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, State Key Lab Optoelect Mat & Technol, Xingang West Rd 135, Guangzhou 510275, Guangdong, Peoples R China.
EM liullin@mail.sysu.edu.cn
RI Lin, Wanmin/JVN-3842-2024; wang, wenjing/KEH-0575-2024
OI Lin, Wanmin/0000-0003-1799-9132; 
FU Guangdong Province Key RD projects; National Natural Science Foundation
   of China [11772359];  [2019B010154002]
FX This work was supported by the Guangdong Province Key R&D projects under
   Grant 2019B010154002 and National Natural Science Foundation of China
   (11772359). And the authors would thank to Peiqi Duan for providing
   assistance and data.
CR Baldwin RW, 2020, PROC CVPR IEEE, P1698, DOI 10.1109/CVPR42600.2020.00177
   Baldwin RW, 2019, LECT NOTES COMPUT SC, V11663, P395, DOI 10.1007/978-3-030-27272-2_35
   Cheng WS, 2019, IEEE COMPUT SOC CONF, P1666, DOI 10.1109/CVPRW.2019.00210
   Czech D, 2016, P IEEE RAS-EMBS INT, P19, DOI 10.1109/BIOROB.2016.7523452
   Duan PQ, 2021, PROC CVPR IEEE, P12819, DOI 10.1109/CVPR46437.2021.01263
   Duan PQ, 2022, IEEE T PATTERN ANAL, V44, P8261, DOI 10.1109/TPAMI.2021.3113344
   Fang HC, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1427, DOI 10.1145/3503161.3548048
   Feng Y, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10062024
   Gallego G, 2022, IEEE T PATTERN ANAL, V44, P154, DOI 10.1109/TPAMI.2020.3008413
   Gallego G, 2019, PROC CVPR IEEE, P12272, DOI 10.1109/CVPR.2019.01256
   Gallego G, 2018, PROC CVPR IEEE, P3867, DOI 10.1109/CVPR.2018.00407
   Guo S., 2022, IEEE Transactions on Pattern Analysis and Machine Intelligence
   Guo SS, 2021, INTEGRATION, V81, P99, DOI 10.1016/j.vlsi.2021.04.006
   Guo SS, 2020, ASIA S PACIF DES AUT, P452, DOI 10.1109/ASP-DAC47756.2020.9045268
   Guo S, 2023, VISUAL COMPUT, V39, P1363, DOI 10.1007/s00371-022-02412-6
   Hu YH, 2021, IEEE COMPUT SOC CONF, P1312, DOI 10.1109/CVPRW53098.2021.00144
   Huang TJ, 2022, ENGINEERING-PRC, V25, P110, DOI 10.1016/j.eng.2022.01.012
   Ieng SH, 2014, P IEEE, V102, P1485, DOI 10.1109/JPROC.2014.2347355
   Kaminski K., 2019, P 1 NEO DEBR DET C D
   Khodamoradi A, 2021, IEEE T EMERG TOP COM, V9, P15, DOI 10.1109/TETC.2017.2788865
   Koseoglu B, 2020, VISUAL COMPUT, V36, P2369, DOI 10.1007/s00371-020-01894-6
   Li J, 2019, IEEE INT CON MULTI, P1396, DOI 10.1109/ICME.2019.00242
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Liu HC, 2017, VISUAL COMPUT, V33, P749, DOI 10.1007/s00371-017-1372-y
   Liu HJ, 2015, IEEE INT SYMP CIRC S, P722, DOI 10.1109/ISCAS.2015.7168735
   Liu SC, 2019, IEEE SIGNAL PROC MAG, V36, P29, DOI 10.1109/MSP.2019.2928127
   Parihar AS, 2022, VISUAL COMPUT, V38, P295, DOI 10.1007/s00371-020-02016-y
   Rebecq H., 2018, C ROB LEARN, P969, DOI DOI 10.1007/978-3-319-24574-4_28
   Stoffregen T, 2019, PROC CVPR IEEE, P12292, DOI 10.1109/CVPR.2019.01258
   Suh Y, 2020, IEEE INT SYMP CIRC S
   Wang YX, 2019, PROC CVPR IEEE, P6351, DOI 10.1109/CVPR.2019.00652
   Wang ZHW, 2020, PROC CVPR IEEE, P1606, DOI 10.1109/CVPR42600.2020.00168
   Wu JJ, 2021, IEEE T MULTIMEDIA, V23, P1148, DOI 10.1109/TMM.2020.2993957
   Wu JJ, 2020, INT CONF ACOUST SPEE, P4437, DOI [10.1109/ICASSP40776.2020.9053002, 10.1109/icassp40776.2020.9053002]
   Xiang X., 2022, 2022 IEEE INT C MULT, P01
   Xie XM, 2017, PROCEEDINGS OF 2017 INTERNATIONAL CONFERENCE ON VIDEO AND IMAGE PROCESSING (ICVIP 2017), P176, DOI 10.1145/3177404.3177411
   Xu J, 2020, IEEE T COMPUT IMAG, V6, P604, DOI 10.1109/TCI.2020.2964255
   Xu Ninghui, 2022, Proceedings of 2021 International Conference on Autonomous Unmanned Systems (ICAUS 2021). Lecture Notes in Electrical Engineering (861), P792, DOI 10.1007/978-981-16-9492-9_78
   Yang J., 2022, The Visual Computer
   Zhang JQ, 2021, VISUAL COMPUT, V37, P2671, DOI 10.1007/s00371-021-02237-9
NR 40
TC 0
Z9 0
U1 9
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 18
PY 2024
DI 10.1007/s00371-023-03183-4
EA JAN 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FE3U7
UT WOS:001144055000002
DA 2024-08-05
ER

PT J
AU Xiong, JB
   Zhang, ZH
   Wang, CD
   Cen, J
   Wang, Q
   Nie, JJ
AF Xiong, Jianbin
   Zhang, Zhenhao
   Wang, Changdong
   Cen, Jian
   Wang, Qi
   Nie, Jinji
TI Pupil localization algorithm based on lightweight convolutional neural
   network
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Pupil localization; Low-resolution images; Deep learning; Convolutional
   neural networks; Pupil classification
ID ROBUST
AB Pupil localization is one of the most critical and essential requirements for eye gaze estimation and eye movement tracking. Because pupil images contain monotonous and uncomplicated information, the dataset uses a single class of labels to describe the image content, and using convolutional neural networks can quickly and accurately identify the pupil position on the input image. On low-resolution images, traditional methods encounter issues of low accuracy and cumbersome design steps. A lightweight pupil localization algorithm is proposed in this paper, utilizing a convolutional neural network (CNN) with additional training samples. The experimental results demonstrate the algorithm's significant effectiveness in identifying the pupil position within the training set, with the accuracy of pupil position in the test set reaching 97.78%. This provides an evidence of the algorithm's feasibility for accurately localizing pupils in low-resolution images.
C1 [Xiong, Jianbin; Zhang, Zhenhao; Cen, Jian; Wang, Qi; Nie, Jinji] Guangdong Polytech Normal Univ, Guangzhou 510665, Peoples R China.
   [Wang, Changdong] Sun Yat Sen Univ, Guangzhou 510275, Peoples R China.
C3 Guangdong Polytechnic Normal University; Sun Yat Sen University
RP Wang, CD (corresponding author), Sun Yat Sen Univ, Guangzhou 510275, Peoples R China.
EM 276158903@qq.com; zhangzh.cn@outlook.com; changdongwang@hotmail.com;
   mmcjian@163.com; aotomwq@sina.com; jinji.nie@ieee.org
RI Wang, Qi/AAJ-2703-2020
OI Wang, Qi/0000-0001-9780-5443
FU Special projects in key areas of ordinary colleges and universities in
   Guangdong Province; National Natural Science Foundation of China; Basic
   and Applied Basic Research Project of Guangdong Procincial
   [2020ZDZX2014]; Natural Science Foundation of Guangdong Province of
   China [62073090, U22A20221]; Intelligent Agricultural Engineering
   Technology Research Center of Guangdong University [2018A030307038];
   Guangdong Provincial Key Laboratory of Intellectual Property Big Data
   [2019A1515010700]; Guangdong Provincial Key Laboratory Project of
   Intellectual Property and Big Data [ZHNY1905]; Key (natural) Project of
   Guangdong Provincial; Introduction of Talents Project of Guangdong
   Polytechnic Normal University of China [2018B030322016]; Special Fund
   for Scientific and Technological Innovation Strategy of Guangdong
   Province [2020KCXTD017, 2019KZDXM020, 2019KZDZX1004, 2019KZDZX1042]; 
   [991512203];  [991560236];  [991682295];  [pdjh2023 b0304];  [pdjh2023
   b0309]
FX This work was supported in part by special projects in key areas of
   ordinary colleges and universities in Guangdong Province No.
   2020ZDZX2014, in part by the National Natural Science Foundation of
   China under Grant No. 62073090, U22A20221, in part by the Basic and
   Applied Basic Research Project of Guangdong Procincial under Grant No.
   2018A030307038, in part by the Natural Science Foundation of Guangdong
   Province of China under Grant No. 2019A1515010700, in part by
   Intelligent Agricultural Engineering Technology Research Center of
   Guangdong University under Grant No. ZHNY1905, in part by Guangdong
   Provincial Key Laboratory of Intellectual Property &Big Data under
   Grant, in part by the Guangdong Provincial Key Laboratory Project of
   Intellectual Property and Big Data under Grant No. 2018B030322016, in
   part by the Key (natural) Project of Guangdong Provincial under Grant
   No. 2020KCXTD017, 2019KZDXM020, 2019KZDZX1004, 2019KZDZX1042, in part by
   the Introduction of Talents Project of Guangdong Polytechnic Normal
   University of China under Grant No. 991512203, 991560236, 991682295, and
   in part by the Special Fund for Scientific and Technological Innovation
   Strategy of Guangdong Province No. pdjh2023 b0304, pdjh2023 b0309.
   (Corresponding author: Changdong Wang).
CR Abbass MY, 2021, VISUAL COMPUT, V37, P993, DOI 10.1007/s00371-020-01848-y
   Avendaño-Valencia LD, 2021, ARTIF INTELL MED, V114, DOI 10.1016/j.artmed.2021.102050
   Bonteanu P, 2019, INT SYM DES TECH ELE, P252, DOI [10.1109/SIITME47687.2019.8990887, 10.1109/siitme47687.2019.8990887, 10.1109/siitme47687.2019.8990688]
   Carrasco M, 2012, IMAGE VISION COMPUT, V30, P860, DOI 10.1016/j.imavis.2012.07.001
   Chaudhary G, 2021, COMPUT ELECTR ENG, V96, DOI 10.1016/j.compeleceng.2021.107554
   Cheng WF, 2024, VISUAL COMPUT, V40, P2419, DOI 10.1007/s00371-023-02927-6
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Duchowski A.T., 2003, Eye Tracking Methodology: Theory and Practice, P55
   Frisoli A, 2012, IEEE T SYST MAN CY C, V42, P1169, DOI 10.1109/TSMCC.2012.2226444
   Fuhl W, 2016, 2016 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2016), P123, DOI 10.1145/2857491.2857505
   Fuhl W, 2015, LECT NOTES COMPUT SC, V9256, P39, DOI 10.1007/978-3-319-23192-1_4
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Guo HY, 2020, IEEE T GEOSCI REMOTE, V58, P5772, DOI 10.1109/TGRS.2020.2969979
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jaddoa MA, 2021, INFRARED PHYS TECHN, V119, DOI 10.1016/j.infrared.2021.103932
   Javadi Amir-Homayoun, 2015, Front Neuroeng, V8, P4, DOI 10.3389/fneng.2015.00004
   Kingma D. P., 2014, arXiv
   Kothari RS, 2021, IEEE T VIS COMPUT GR, V27, P2757, DOI 10.1109/TVCG.2021.3067765
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumawat A, 2022, VISUAL COMPUT, V38, P3681, DOI 10.1007/s00371-021-02196-1
   Lasaponara S, 2021, CORTEX, V134, P265, DOI 10.1016/j.cortex.2020.10.021
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li D., 2005, 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, P79, DOI DOI 10.1109/CVPR.2005.531
   Li T, 2014, VISUAL COMPUT, V30, P59, DOI 10.1007/s00371-013-0780-x
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Iandola FN, 2016, Arxiv, DOI [arXiv:1602.07360, 10.48550/arXiv.1602.07360]
   Radojicic T., 2020, 2020 9 MEDITERRANEAN, P1, DOI [10.1109/MECO49872.2020.9134117, DOI 10.1109/MECO49872.2020.9134117]
   Ridha Jwad Ali, 2020, 2020 International Conference on Computer Science and Software Engineering (CSASE). Proceedings, P32, DOI 10.1109/CSASE48920.2020.9142123
   Ruiz-Beltrán CA, 2022, EXPERT SYST APPL, V194, DOI 10.1016/j.eswa.2022.116505
   Santini T, 2018, COMPUT VIS IMAGE UND, V170, P40, DOI 10.1016/j.cviu.2018.02.002
   Satriya T, 2016, 2016 INTERNATIONAL SYMPOSIUM ON ELECTRONICS AND SMART DEVICES (ISESD), P253, DOI 10.1109/ISESD.2016.7886728
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su D, 2020, IEEE T IND INFORM, V16, P510, DOI 10.1109/TII.2019.2933481
   Swirski L., 2013, A fully-automatic, temporal approach to single camera, glint-free 3 D eye model fitting
   Swirski Lech, 2012, P S EYE TRACK RES AP, P173
   Wan ZH, 2021, COMPUT ELECTR ENG, V93, DOI 10.1016/j.compeleceng.2021.107193
   Wang BL, 2024, VISUAL COMPUT, V40, P1997, DOI 10.1007/s00371-023-02898-8
   Wang XM, 2021, NEUROCOMPUTING, V444, P390, DOI 10.1016/j.neucom.2020.06.137
   Wei ZY, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102974
   Wu JH, 2017, 2017 IEEE CONFERENCE ON DEPENDABLE AND SECURE COMPUTING, P93, DOI 10.1109/DESEC.2017.8073839
   Xiong JB, 2022, ASSEMBLY AUTOM, V42, P595, DOI 10.1108/AA-02-2022-0030
   Xiong JB, 2022, IEEE T IND INFORM, V18, P1061, DOI 10.1109/TII.2021.3073755
   Xiong JB, 2016, INT J COMPUT APPL T, V53, P91
   Xun Z., 2021, 2021 18 CHINA INT FO, P118, DOI [10.1109/SSLChinaIFWS54608.2021.9675166, DOI 10.1109/SSLCHINAIFWS54608.2021.9675166]
   Yang AL, 2024, VISUAL COMPUT, V40, P73, DOI 10.1007/s00371-022-02766-x
   Yang X, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3186155
   Yun-Hai Zou, 2016, 2016 International Conference on Machine Learning and Cybernetics (ICMLC). Proceedings, P1021, DOI 10.1109/ICMLC.2016.7873019
   Zhang D, 2022, J AFFECT DISORDERS, V307, P237, DOI 10.1016/j.jad.2022.03.077
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhao S, 2023, VISUAL COMPUT, DOI 10.1007/s00371-023-02801-5
   Zhao YF, 2016, Adv Inform Managemen, P988, DOI 10.1109/IMCEC.2016.7867358
   Zheng K, 2022, BIOMED SIGNAL PROCES, V75, DOI 10.1016/j.bspc.2022.103609
NR 54
TC 3
Z9 3
U1 17
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 18
PY 2024
DI 10.1007/s00371-023-03222-0
EA JAN 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FE3U7
UT WOS:001144055000004
DA 2024-08-05
ER

PT J
AU Shen, LF
   Hou, B
   Jian, YL
   Tu, XS
   Zhang, YJ
   Shuai, LY
   Ge, FZ
   Chen, DB
AF Shen, Longfeng
   Hou, Bin
   Jian, Yulei
   Tu, Xisong
   Zhang, Yingjie
   Shuai, Lingying
   Ge, Fangzhen
   Chen, Debao
TI TransFGVC: transformer-based fine-grained visual classification
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Computer vision; Fine-grained visual classification; LSTM; Swin
   Transformer; Birds-267-2022 dataset
AB Fine-grained visual classification (FGVC) aims to identify subcategories of objects within the same superclass. This task is challenging owing to high intra-class variance and low inter-class variance. The most recent methods focus on locating discriminative areas and then training the classification network to further capture the subtle differences among them. On the one hand, the detection network often obtains an entire part of the object, and positioning errors occur. On the other hand, these methods ignore the correlations between the extracted regions. We propose a novel highly scalable approach, called TransFGVC, that cleverly combines Swin Transformers with long short-term memory (LSTM) networks to address the above problems. The Swin Transformer is used to obtain remarkable visual tokens through self-attention layer stacking, and LSTM is used to model them globally, which not only accurately locates the discriminative region but also further introduces global information that is important for FGVC. The proposed method achieves competitive performance with accuracy rates of 92.7%, 91.4% and 91.5% using the public CUB-200-2011 and NABirds datasets and our Birds-267-2022 dataset, and the Params and FLOPs of our method are 25% and 27% lower, respectively, than the current SotA method HERBS. To effectively promote the development of FGVC, we developed the Birds-267-2022 dataset, which has 267 categories and 12,233 images.
C1 [Shen, Longfeng; Hou, Bin; Jian, Yulei; Tu, Xisong; Zhang, Yingjie; Ge, Fangzhen; Chen, Debao] Huaibei Normal Univ, Sch Comp Sci & Technol, Anhui Engn Res Ctr Intelligent Comp & Applicat Cog, 100 Dongshen Rd, Huaibei 235000, Anhui, Peoples R China.
   [Shen, Longfeng; Jian, Yulei; Ge, Fangzhen; Chen, Debao] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, 5089 Wangjiang West Rd, Hefei 230088, Anhui, Peoples R China.
   [Shuai, Lingying] Huaibei Normal Univ, Coll Life Sci, 100 Dongshan Rd, Huaibei 235000, Anhui, Peoples R China.
   [Shen, Longfeng; Hou, Bin; Jian, Yulei; Tu, Xisong; Zhang, Yingjie; Ge, Fangzhen; Chen, Debao] Huaibei Normal Univ, Anhui Big Data Res Ctr Univ Manage, 100 Dongshen Rd, Huaibei 235000, Anhui, Peoples R China.
C3 Huaibei Normal University; Huaibei Normal University; Huaibei Normal
   University
RP Shen, LF (corresponding author), Huaibei Normal Univ, Sch Comp Sci & Technol, Anhui Engn Res Ctr Intelligent Comp & Applicat Cog, 100 Dongshen Rd, Huaibei 235000, Anhui, Peoples R China.; Shen, LF (corresponding author), Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, 5089 Wangjiang West Rd, Hefei 230088, Anhui, Peoples R China.; Shuai, LY (corresponding author), Huaibei Normal Univ, Coll Life Sci, 100 Dongshan Rd, Huaibei 235000, Anhui, Peoples R China.; Shen, LF (corresponding author), Huaibei Normal Univ, Anhui Big Data Res Ctr Univ Manage, 100 Dongshen Rd, Huaibei 235000, Anhui, Peoples R China.
EM longfengshen521@126.com; binhou000710@126.com; 1291679786@qq.com;
   2858098146@qq.com; 1813214195@qq.com; 1185369477@qq.com;
   gfz203377@163.com; 450792668@qq.com
RI Shuai, Ling-Ying/AAB-8639-2020
OI Shuai, Ling-Ying/0000-0003-2604-4215
FU National Natural Science Foundation of China [71801108]; Anhui
   Provincial universities outstanding young backbone talents domestic
   visiting study and Research project [gxgnfx2019006]; University Synergy
   Innovation Program of Anhui Province, China [GXXT-2021-002,
   GXXT-2022-033]; Innovation Fund for Postgraduates of Huaibei Normal
   University [cx2022042]; Open Laboratory project of Huaibei Normal
   University [2021sykf026]; The 2022 National Innovation and
   Entrepreneurship Training Program for College Students [202210373005]
FX This work is part supported by the National Natural Science Foundation
   of China (Grant No: 71801108), Anhui Provincial universities outstanding
   young backbone talents domestic visiting study and Research project
   (Grant No: gxgnfx2019006), the University Synergy Innovation Program of
   Anhui Province, China (Grant Nos: GXXT-2021-002, GXXT-2022-033),
   Innovation Fund for Postgraduates of Huaibei Normal University(Grant No:
   cx2022042), Open Laboratory project of Huaibei Normal University(Grant
   No: 2021sykf026), 2022 National Innovation and Entrepreneurship Training
   Program for College Students (Grant No: 202210373005). We would like to
   thank Editage (www.editage.cn) for English language editing.
CR Behera Ardhendu, 2021, AAAI
   Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29
   Branson Steve, 2014, BMVC
   Cai SJ, 2017, IEEE I CONF COMP VIS, P511, DOI 10.1109/ICCV.2017.63
   Chai Y, 2013, IEEE I CONF COMP VIS, P321, DOI 10.1109/ICCV.2013.47
   Chang DL, 2020, IEEE T IMAGE PROCESS, V29, P4683, DOI 10.1109/TIP.2020.2973812
   Chen Wei, 2009, Advances in Neural Information Processing Systems, V22
   Chen Y, 2022, NEUROCOMPUTING, V470, P170, DOI 10.1016/j.neucom.2021.10.096
   Chou P.-Y., 2022, A novel plug-in module for fine-grained visual classification
   Chou P.-Y., 2023, Fine-grained Visual Classification with High-temperature Refinement and Background Suppression
   Chou P.-Y., 2022, arXiv, DOI DOI 10.48550/ARXIV.2202.03822
   Cui Y, 2017, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2017.325
   Diao Qishuai, 2022, METAFORMER UNIFIED M
   Ding YF, 2021, IEEE T IMAGE PROCESS, V30, P2826, DOI 10.1109/TIP.2021.3055617
   Engin M, 2018, LECT NOTES COMPUT SC, V11206, P629, DOI 10.1007/978-3-030-01216-8_38
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Gao Y, 2020, AAAI CONF ARTIF INTE, V34, P10818
   Ge WF, 2019, PROC CVPR IEEE, P3029, DOI 10.1109/CVPR.2019.00315
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Guo C., 2022, Vis. Comput, V8, P1
   He J, 2022, AAAI CONF ARTIF INTE, P852
   Hu YQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4239, DOI 10.1145/3474085.3475561
   Huang SL, 2021, AAAI CONF ARTIF INTE, V35, P1628
   Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132
   Karlinsky L, 2017, PROC CVPR IEEE, P965, DOI 10.1109/CVPR.2017.109
   Khan SD, 2019, COMPUT VIS IMAGE UND, V182, P50, DOI 10.1016/j.cviu.2019.03.001
   Khosla A., 2011, P CVPR WORKSHOP FINE, V2, P1
   Kim S., 2022, INT C MACHINE LEARNI, P11162
   Kong S, 2017, PROC CVPR IEEE, P7025, DOI 10.1109/CVPR.2017.743
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Lam M, 2017, PROC CVPR IEEE, P6497, DOI 10.1109/CVPR.2017.688
   Li H, 2020, IEEE I C VI COM I PR, P243, DOI [10.1109/VCIP49819.2020.9301763, 10.1109/vcip49819.2020.9301763]
   Li M., 2022, Vis. Comput, V8, P1
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu CB, 2020, AAAI CONF ARTIF INTE, V34, P11555
   Liu DC, 2023, PATTERN RECOGN, V140, DOI 10.1016/j.patcog.2023.109550
   Liu XD, 2022, NEUROCOMPUTING, V492, P137, DOI 10.1016/j.neucom.2022.04.037
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo W, 2020, IEEE SIGNAL PROC LET, V27, P1545, DOI 10.1109/LSP.2020.3020227
   Maji S, 2013, Arxiv, DOI arXiv:1306.5151
   Min SB, 2020, IEEE T IMAGE PROCESS, V29, P4996, DOI 10.1109/TIP.2020.2977457
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Parkhi OM, 2012, PROC CVPR IEEE, P3498, DOI 10.1109/CVPR.2012.6248092
   Rao YM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1005, DOI 10.1109/ICCV48922.2021.00106
   Ruoyi Du, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P153, DOI 10.1007/978-3-030-58565-5_10
   Ruyi Ji, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10465, DOI 10.1109/CVPR42600.2020.01048
   Song JW, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9534004
   Sun HB, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5853, DOI 10.1145/3503161.3548308
   Touvron H, 2019, ADV NEUR IN, V32
   Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang J, 2022, Arxiv, DOI arXiv:2107.02341
   Xie LX, 2016, NEUROCOMPUTING, V182, P48, DOI 10.1016/j.neucom.2015.12.008
   Xu Q, 2023, IEEE T MULTIMEDIA, V25, P9015, DOI 10.1109/TMM.2023.3244340
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang TH, 2022, PSYCHOL MED, V52, P3412, DOI 10.1017/S0033291721000064
   Zhang Y, 2022, INT CONF ACOUST SPEE, P3234, DOI 10.1109/ICASSP43922.2022.9747591
   Zhao YF, 2021, PROC CVPR IEEE, P15074, DOI 10.1109/CVPR46437.2021.01483
   Zheng HL, 2019, ADV NEUR IN, V32
   Zheng HL, 2019, PROC CVPR IEEE, P5007, DOI 10.1109/CVPR.2019.00515
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zhu HW, 2022, PROC CVPR IEEE, P4682, DOI 10.1109/CVPR52688.2022.00465
   Zhuang PQ, 2020, AAAI CONF ARTIF INTE, V34, P13130
NR 63
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 28
PY 2024
DI 10.1007/s00371-024-03545-6
EA JUN 2024
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WU7X3
UT WOS:001257463400001
DA 2024-08-05
ER

PT J
AU Khan, S
   Deng, ZG
AF Khan, Saba
   Deng, Zhigang
TI Agent-based crowd simulation: an in-depth survey of determining factors
   for heterogeneous behavior
SO VISUAL COMPUTER
LA English
DT Article
DE Crowd simulation; Autonomous agents; Psychological models; Microscopic
   models; Multi-agent simulation
ID SOCIAL FORCE MODEL; EMOTION CONTAGION; JAMMING TRANSITION;
   DECISION-MAKING; EVACUATION; DYNAMICS; FLOW
AB In recent years, the field of crowd simulation has experienced significant advancements, attributed in part to the improvement of hardware performance, coupled with a notable emphasis on agent-based characteristics. Agent-based simulations stand out as the preferred methodology when researchers seek to model agents with unique behavioral traits and purpose-driven actions, a crucial aspect for simulating diverse and realistic crowd movements. This survey adopts a systematic approach, meticulously delving into the array of factors vital for simulating a heterogeneous microscopic crowd. The emphasis is placed on scrutinizing low-level behavioral details and individual features of virtual agents to capture a nuanced understanding of their interactions. The survey is based on studies published in reputable peer-reviewed journals and conferences. The primary aim of this survey is to present the diverse advancements in the realm of agent-based crowd simulations, with a specific emphasis on the various aspects of agent behavior that researchers take into account when developing crowd simulation models. Additionally, the survey suggests future research directions with the objective of developing new applications that focus on achieving more realistic and efficient crowd simulations.
C1 [Khan, Saba; Deng, Zhigang] Univ Houston, Dept Comp Sci, Houston, TX 77204 USA.
C3 University of Houston System; University of Houston
RP Deng, ZG (corresponding author), Univ Houston, Dept Comp Sci, Houston, TX 77204 USA.
EM zdeng4@central.uh.edu
FU National Science Foundation [IIS-2005430]; NSF
FX This work was supported in part by NSF IIS-2005430.
CR Allbeck J., 2002, EMBODIED CONVERSATIO, V2, P15
   Allbeck JM, 2010, LECT NOTES COMPUT SC, V6459, P182
   An T, 2022, PROCEEDINGS OF THE 2022 THE TWENTY-THIRD INTERNATIONAL SYMPOSIUM ON THEORY, ALGORITHMIC FOUNDATIONS, AND PROTOCOL DESIGN FOR MOBILE NETWORKS AND MOBILE COMPUTING, MOBIHOC 2022, P247, DOI 10.1145/3492866.3557735
   [Anonymous], 2004, Cognitive Systems Research, DOI [10.1016/j.cogsys.2004.02.002, DOI 10.1016/J.COGSYS.2004.02.002]
   Aubé F, 2004, LECT NOTES COMPUT SC, V3305, P601
   Aydt H., 2011, 2011 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies, P72, DOI 10.1109/WI-IAT.2011.154
   Badler N, 2002, COMP ANIM CONF PROC, P133, DOI 10.1109/CA.2002.1017521
   Badler NI, 2000, EMBODIED CONVERSATIONAL AGENTS, P256
   Bartenieff I., 2013, BODY MOVEMENT COPING, DOI [10.4324/9781315025445, DOI 10.4324/9781315025445]
   Bindiganavale R., 2000, Proceedings of the Fourth International Conference on Autonomous Agents, P293, DOI 10.1145/336595.337503
   Boatright CoryD., 2013, ACM SIGGRAPH Interna- tional Conference on Virtual-Reality Continuum and Its Applications in Industry (VRCAI), VRCAI '13, P51, DOI DOI 10.1145/2534329.2534332
   Bosse T, 2015, COGN COMPUT, V7, P111, DOI 10.1007/s12559-014-9277-9
   Bosse T, 2013, AUTON AGENT MULTI-AG, V27, P52, DOI 10.1007/s10458-012-9201-1
   Bosse T, 2009, 23RD EUROPEAN CONFERENCE ON MODELLING AND SIMULATION (ECMS 2009), P212
   Braun A, 2003, COMP ANIM CONF PROC, P143, DOI 10.1109/CASA.2003.1199317
   Braun A., 2005, Proceedings of the ACM symposium on Virtual reality software and technology, P244
   Brogan DC, 1997, AUTON ROBOT, V4, P137, DOI 10.1023/A:1008867321648
   Cao MX, 2017, PHYSICA A, V483, P250, DOI 10.1016/j.physa.2017.04.137
   Capobianco R., 2021, J ARTIF INT RES, V71, P953, DOI DOI 10.1613/JAIR.1.12632
   Charalambous P, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592459
   Chenney Stephen., 2004, Proceedings of the 2004 ACM SIGGRAPH/Euro- graphics symposium on Computer animation, P233, DOI [10.1145/1028523.1028553, DOI 10.1145/1028523.1028553.]
   Chi D, 2000, COMP GRAPH, P173, DOI 10.1145/344779.352172
   Cho K, 2008, LECT NOTES COMPUT SC, V5208, P364
   Durupinar F, 2016, IEEE T VIS COMPUT GR, V22, P2145, DOI 10.1109/TVCG.2015.2501801
   Durupinar F, 2011, IEEE COMPUT GRAPH, V31, P22, DOI 10.1109/MCG.2009.105
   Dutra TB, 2017, COMPUT GRAPH FORUM, V36, P337, DOI 10.1111/cgf.13130
   Espitia E, 2022, TRANSPORT RES REC, DOI 10.1177/03611981221088224
   Eysenck H.J., 1947, DIMENSIONS PERSONALI, DOI 10.1016/S0191-8869(00)00236-1
   Fanliang B., 2013, IEEE C ANTHOLOGY, P1, DOI [10.1109/ANTHOLOGY.2013.6785018, DOI 10.1109/ANTHOLOGY.2013.6785018]
   Fasheng Qiu, 2010, Proceedings of the 2010 IEEE/ACM International Conference on Web Intelligence-Intelligent Agent Technology (WI-IAT 2010), P461, DOI 10.1109/WI-IAT.2010.9
   Festinger L, 1954, HUM RELAT, V7, P117, DOI 10.1177/001872675400700202
   Fiorini P, 1998, INT J ROBOT RES, V17, P760, DOI 10.1177/027836499801700706
   Fu LB, 2014, PHYSICA A, V405, P380, DOI 10.1016/j.physa.2014.03.043
   Gratch J., 2001, Proceedings of the Fifth International Conference on Autonomous Agents, P278, DOI 10.1145/375735.376309
   Guy S. J., 2011, P SCA ACM SIGGRAPH E, P43, DOI [10.1145/2019406.2019413, DOI 10.1145/2019406.2019413]
   Guy SJ, 2010, PROCEEDINGS OF THE TWENTY-SIXTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SCG'10), P115, DOI 10.1145/1810959.1810981
   Ha V, 2012, PHYSICA A, V391, P2740, DOI 10.1016/j.physa.2011.12.034
   Hartman C, 2006, COMPUT ANIMAT VIRT W, V17, P199, DOI 10.1002/cav.123
   Hayes-Roth B., 1995, ICMAS-95 Proceedings. First International Conference on Multi-Agent Systems, P148
   Helbing D, 2000, NATURE, V407, P487, DOI 10.1038/35035023
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   Huang LD, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENCE AND SECURITY INFORMATICS (ISI), P193, DOI 10.1109/ISI.2018.8587352
   Hughes Rowan., 2015, Proceedings of the 8th ACM SIGGRAPH Conference on Motion in Games, P79, DOI DOI 10.1145/2822013.2822030
   Ijaz K., 2015, P 17 UKSIMAMSS INT C, P111, DOI DOI 10.1109/UKSIM.2015.46
   Jiang H, 2018, ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES (I3D 2018), DOI 10.1145/3190834.3190844
   Kim S., 2012, P I3D 12, P55, DOI [DOI 10.1145/2159616.2159626, 10.1145/2159616.2159626]
   Kirchner A., 2003, 2nd International Conference on Pedestrians and Evacuation Dynamics, P51
   Knob P, 2018, 18TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA'18), P233, DOI 10.1145/3267851.3267871
   Kountouriotis V, 2014, PATTERN RECOGN LETT, V44, P30, DOI 10.1016/j.patrec.2013.10.024
   Krontiris Athanasios., 2016, Proceedings of the Conference on Computer Animation and Social Agents, P61, DOI DOI 10.1145/2915926.2915935
   Kwiatkowski A, 2022, COMPUT GRAPH FORUM, V41, P613, DOI 10.1111/cgf.14504
   Lakoba TI, 2005, SIMUL-T SOC MOD SIM, V81, P339, DOI 10.1177/0037549705052772
   Latif M.S. A., 2004, ACM SIGGRAPH International Conference on Virtual Reality Continuum and its Appications in Industry (VRCAI'04), P278, DOI DOI 10.1145/1044588.1044647
   Minh LV, 2012, LECT NOTES ARTIF INT, V7057, P604
   Lee J, 2018, ACM SIGGRAPH CONFERENCE ON MOTION, INTERACTION, AND GAMES (MIG 2018), DOI 10.1145/3274247.3274510
   Lemonari M, 2022, COMPUT GRAPH FORUM, V41, P677, DOI 10.1111/cgf.14506
   Li B, 2014, 2014 PROCEEDINGS OF THE IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM 2014), P497, DOI 10.1109/ASONAM.2014.6921632
   Li QR, 2023, APPL MATH MODEL, V124, P509, DOI 10.1016/j.apm.2023.08.010
   Liu TT, 2019, ARTIF LIFE ROBOT, V24, P59, DOI 10.1007/s10015-018-0459-5
   Liu Z, 2018, COMPUT ANIMAT VIRT W, V29, DOI 10.1002/cav.1817
   López A, 2019, COMPUT GRAPH FORUM, V38, P181, DOI 10.1111/cgf.13629
   Loscos C, 2003, THEORY AND PRACTICE OF COMPUTER GRAPHICS, PROCEEDINGS, P122
   Luo L., 2016, P 29 INT C COMP AN S, P37, DOI [10.1145/2915926.2915944, DOI 10.1145/2915926.2915944]
   Lv P, 2019, IEEE T COMPUT SOC SY, V6, P377, DOI 10.1109/TCSS.2018.2878461
   Mao Y, 2020, MULTIMED TOOLS APPL, V79, P3077, DOI 10.1007/s11042-018-6069-3
   Mao Y, 2019, VISUAL COMPUT, V35, P1725, DOI 10.1007/s00371-018-1568-9
   Mehrabian A, 1996, CURR PSYCHOL, V14, P261, DOI 10.1007/BF02686918
   Mingliang Xu, 2021, IEEE Transactions on Intelligent Transportation Systems, V22, P6977, DOI 10.1109/TITS.2020.3000607
   Moussaïd M, 2011, P NATL ACAD SCI USA, V108, P6884, DOI 10.1073/pnas.1016507108
   Muramatsu H, 2000, PHYSICA A, V275, P281, DOI 10.1016/S0378-4371(99)00447-1
   Muramatsu M, 1999, PHYSICA A, V267, P487, DOI 10.1016/S0378-4371(99)00018-7
   Musse S.R., 1997, COMPUTER ANIMATION S, P39, DOI [10.1007/978-3-7091-6874-53, DOI 10.1007/978-3-7091-6874-5_3]
   Musse SR, 2021, VISUAL COMPUT, V37, P3077, DOI 10.1007/s00371-021-02252-w
   Nasir M, 2014, SIMUL MODEL PRACT TH, V45, P18, DOI 10.1016/j.simpat.2014.03.002
   Nasirpouri F., 2015, 2015 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2015.7157712
   Ondrej J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778860
   Ortony A, 2022, COGNITIVE STRUCTURE OF EMOTIONS, 2 EDITION, P1, DOI 10.1017/9781108934053
   Panayiotou Andreas, 2022, SIGGRAPH22 Conference Proceeding: Special Interest Group on Computer Graphics and Interactive Techniques Conference Proceedings, DOI 10.1145/3528233.3530712
   Parisi DR, 2007, PHYSICA A, V385, P343, DOI 10.1016/j.physa.2007.06.033
   Parisi DR, 2005, PHYSICA A, V354, P606, DOI 10.1016/j.physa.2005.02.040
   Pelechano N, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P99
   Pelechano N, 2006, IEEE COMPUT GRAPH, V26, P80, DOI 10.1109/MCG.2006.133
   PelechanoGomez N., 2005, INT WORKSH CROWD SIM, P1
   Ren CJ, 2009, L N INST COMP SCI SO, V5, P1451
   Reynolds C. W., 1999, P GAM DEV C, P763
   Reynolds C.W., 1987, P 14 ANN C COMPUTE, P25, DOI [10.1145/37402.37406, DOI 10.1145/37402.37406, 10.1145/37401.37406, DOI 10.1145/37401.37406]
   Rockenbach G, 2018, 18TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA'18), P295, DOI 10.1145/3267851.3267872
   Sahin C, 2019, PHYSICA A, V528, DOI 10.1016/j.physa.2019.121432
   Scherer KR., 2005, HDB COGNITION EMOTIO, P637, DOI [10.1002/0470013494.ch30, DOI 10.1002/0470013494.CH30]
   Shao W., 2005, SCA 05 P 2005 ACM SI, P19, DOI DOI 10.1145/1073368.1073371
   Shendarkar A, 2008, SIMUL MODEL PRACT TH, V16, P1415, DOI 10.1016/j.simpat.2008.07.004
   Silva A.R.D., 2009, Comput. Entertain., V7, P1, DOI DOI 10.1145/1658866.1658870
   Siyam N, 2020, IEEE ACCESS, V8, P134435, DOI 10.1109/ACCESS.2019.2956880
   Snape J, 2011, IEEE T ROBOT, V27, P696, DOI 10.1109/TRO.2011.2120810
   Stocker C, 2010, LECT NOTES ARTIF INT, V6356, P15, DOI 10.1007/978-3-642-15892-6_2
   Sudkhot Panich, 2023, Multi-disciplinary Trends in Artificial Intelligence: 16th International Conference, MIWAI 2023, Proceedings. Lecture Notes in Computer Science, Lecture Notes in Artificial Intelligence (14078), P459, DOI 10.1007/978-3-031-36402-0_43
   Tecchia F., 2001, P 1 INT GAM TECHN C, P1
   Tian ZN, 2020, KNOWL-BASED SYST, V208, DOI 10.1016/j.knosys.2020.106451
   Tsai J., 2011, 10 INT C AUTONOMOUS, V2, P457
   van den Berg J, 2008, IEEE INT CONF ROBOT, P1928, DOI 10.1109/ROBOT.2008.4543489
   van den Berg J, 2011, SPRINGER TRAC ADV RO, V70, P3
   van Haeringen ES, 2023, AUTON AGENT MULTI-AG, V37, DOI 10.1007/s10458-022-09589-z
   van Toll W, 2021, COMPUT GRAPH FORUM, V40, P731, DOI 10.1111/cgf.142664
   Vizzari Giuseppe., 2012, Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems - Volume 3, AAMAS '12, V3, P1341, DOI 10.1080/15472450.2013.856718
   von Sivers I, 2016, SAFETY SCI, V89, P288, DOI 10.1016/j.ssci.2016.07.001
   Wang HZ, 2016, TRANSPORT RES C-EMER, V64, P86, DOI 10.1016/j.trc.2015.11.010
   Wang L, 2013, PHYSICA A, V392, P2470, DOI 10.1016/j.physa.2013.01.044
   Weizi Li, 2011, Motion in Games. Proceedings 4th International Conference, MIG 2011, P132, DOI 10.1007/978-3-642-25090-3_12
   Wiggins JS., 1996, 5 FACTOR MODEL PERSO
   Xiao Q, 2021, DISCRETE DYN NAT SOC, V2021, DOI 10.1155/2021/5549188
   Xu ML, 2014, J COMPUT SCI TECH-CH, V29, P799, DOI 10.1007/s11390-014-1469-y
   Xu ML, 2021, IEEE T SYST MAN CY-S, V51, P1567, DOI 10.1109/TSMC.2019.2899047
   Xu TF, 2020, PHYS LETT A, V384, DOI 10.1016/j.physleta.2019.126080
   Xue J., 2023, P VRCAI 23, P1, DOI [10.1145/3574131.3574445, DOI 10.1145/3574131.3574445]
   Xuemei Du, 2018, International Journal of Simulation and Process Modelling, V13, P43
   Yang SW, 2020, GRAPH MODELS, V111, DOI 10.1016/j.gmod.2020.101081
   Yixuan Cheng, 2019, Advances in Human Factors in Simulation and Modeling. Proceedings of the AHFE 2018 International Conferences on Human Factors and Simulation and Digital Human Modeling and Applied Optimization. Advances in Intelligent Systems and Computing (AISC 780), P313, DOI 10.1007/978-3-319-94223-0_30
   Zheng LP, 2016, NEUROCOMPUTING, V172, P180, DOI 10.1016/j.neucom.2014.12.103
   Zhou SP, 2010, ACM T MODEL COMPUT S, V20, DOI 10.1145/1842722.1842725
   Zia Kashif, 2020, SIGSIM-PADS '20: Proceedings of the 2020 ACM SIGSIM Conference on Principles of Advanced Discrete Simulation, P129, DOI 10.1145/3384441.3395973
   Zou QL, 2020, J COMPUT CIVIL ENG, V34, DOI 10.1061/(ASCE)CP.1943-5487.0000889
NR 121
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4993
EP 5004
DI 10.1007/s00371-024-03503-2
EA JUN 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001250256000002
DA 2024-08-05
ER

PT J
AU Abdelaal, M
   Galuschka, M
   Zorn, M
   Kannenberg, F
   Menges, A
   Wortmann, T
   Weiskopf, D
   Kurzhals, K
AF Abdelaal, Moataz
   Galuschka, Marcel
   Zorn, Max
   Kannenberg, Fabian
   Menges, Achim
   Wortmann, Thomas
   Weiskopf, Daniel
   Kurzhals, Kuno
TI Visual analysis of fitness landscapes in architectural design
   optimization
SO VISUAL COMPUTER
LA English
DT Article
DE Architecture; Design; Optimization; Visualization; Visual analytics
ID PARAMETRIC DESIGN; VISUALIZATION
AB In architectural design optimization, fitness landscapes are used to visualize design space parameters in relation to one or more objective functions for which they are being optimized. In our design study with domain experts, we developed a visual analytics framework for exploring and analyzing fitness landscapes spanning data, projection, and visualization layers. Within the data layer, we employ two surrogate models and three sampling strategies to efficiently generate a wide array of landscapes. On the projection layer, we use star coordinates and UMAP as two alternative methods for obtaining a 2D embedding of the design space. Our interactive user interface can visualize fitness landscapes as a continuous density map or a discrete glyph-based map. We investigate the influence of surrogate models and sampling strategies on the resulting fitness landscapes in a parameter study. Additionally, we present findings from a user study (N = 12), revealing how experts' preferences regarding projection methods and visual representations may be influenced by their level of expertise, characteristics of the techniques, and the specific task at hand. Furthermore, we demonstrate the usability and usefulness of our framework by a case study from the architecture domain, involving one domain expert.
C1 [Abdelaal, Moataz; Galuschka, Marcel; Weiskopf, Daniel; Kurzhals, Kuno] Univ Stuttgart, Visualizat Res Ctr VISUS, Allmandring 19, D-70567 Stuttgart, Germany.
   [Zorn, Max; Kannenberg, Fabian; Menges, Achim; Wortmann, Thomas] Univ Stuttgart, Inst Computat Design & Construct ICD, Keplerstr 11, D-70174 Stuttgart, Germany.
C3 University of Stuttgart; University of Stuttgart
RP Abdelaal, M (corresponding author), Univ Stuttgart, Visualizat Res Ctr VISUS, Allmandring 19, D-70567 Stuttgart, Germany.
EM moataz.abdelaal@visus.uni-stuttgart.de; marcel@galuschka-fam.de;
   max-benjamin.zorn@icd.uni-stuttgart.de;
   fabian.kannenberg@icd.uni-stuttgart.de;
   achim.menges@icd.uni-stuttgart.de; thomas.wortmann@icd.uni-stuttgart.de;
   daniel.weiskopf@visus.uni-stuttgart.de;
   kuno.kurzhals@visus.uni-stuttgart.de
RI Weiskopf, Daniel/KWT-7459-2024; Wortmann, Thomas/K-5087-2019
OI Wortmann, Thomas/0000-0002-5604-1624; Menges, Achim/0000-0001-9055-4039;
   Abdelaal, MOATAZ/0000-0003-4630-7916; Zorn, Max
   Benjamin/0000-0003-0109-1963; Weiskopf, Daniel/0000-0003-1174-1026
FU Universitt Stuttgart (1023)
FX No Statement Available
CR Abdelaal M, 2022, IEEE COMPUT GRAPH, V42, P10, DOI 10.1109/MCG.2022.3149837
   ANDREWS DF, 1972, BIOMETRICS, V28, P125, DOI 10.2307/2528964
   Asl MR, 2014, FUSION: DATA INTEGRATION AT ITS BEST, VOL 2, P455
   Bradner E., 2014, P S SIM ARCH URB DES, P26
   Brown NC, 2020, AUTOMAT CONSTR, V118, DOI 10.1016/j.autcon.2020.103252
   Brown NC, 2019, INT J ARCHIT COMPUT, V17, P36, DOI 10.1177/1478077118799491
   Chen KW, 2015, ECAADE 2015: REAL TIME - EXTENDING THE REACH OF COMPUTATION, VOL 1, P251
   Cheng SH, 2016, IEEE T VIS COMPUT GR, V22, P121, DOI 10.1109/TVCG.2015.2467552
   Costa A, 2018, MATH PROGRAM COMPUT, V10, P597, DOI 10.1007/s12532-018-0144-7
   Cross Nigel, 1982, DESIGN STUDIES, V3, P221, DOI [10.1016/0142-694X(82)90040-0, DOI 10.1016/0142-694X(82)90040-0]
   Ellis G, 2006, IEEE T VIS COMPUT GR, V12, P717, DOI 10.1109/TVCG.2006.138
   Erhan H, 2010, INT J ARCHIT COMPUT, V8, P462, DOI 10.1260/1478-0771.8.4.461
   Espadoto M, 2021, IEEE T VIS COMPUT GR, V27, P2153, DOI 10.1109/TVCG.2019.2944182
   Flager F., 2007, 24 INT C INF TECHN C, P625
   Fuchkina E., 2018, ECAADE, V36, P367, DOI [10.52842/conf.ecaade.2018.2.367, DOI 10.52842/CONF.ECAADE.2018.2.367]
   Fuchs J, 2017, IEEE T VIS COMPUT GR, V23, P1863, DOI 10.1109/TVCG.2016.2549018
   Heinrich J., 2013, EUROGRAPHICS 2013 ST, DOI DOI 10.2312/CONF/EG2013/STARS/095-116
   Hlawatsch M, 2011, IEEE T VIS COMPUT GR, V17, P1949, DOI 10.1109/TVCG.2011.203
   Hoffman P, 1997, VISUALIZATION '97 - PROCEEDINGS, P437, DOI 10.1109/VISUAL.1997.663916
   Inselberg A, 1985, VISUAL COMPUT, V1, P69, DOI 10.1007/BF01898350
   Kammer D, 2020, IEEE T VIS COMPUT GR, V26, P1661, DOI 10.1109/TVCG.2020.2969060
   Kandogan E., 2000, P IEEE INF VIS S, V650, P22, DOI DOI 10.1145/502512.502530
   Karamba3D, US
   Keck M., 2017, P 10 INT S VIS INF C, P129, DOI [DOI 10.1145/3105971.3105979, 10.1145/3105971.3105979]
   Kindlmann GordonL., 2004, Joint Eurographics - IEEE TCVG Symposium on Visualization, P147, DOI DOI 10.2312/VISSYM/VISSYM04/147-154
   Knippers Jan, 2021, Civil Engineering Design, V3, P123, DOI 10.1002/cend.202100027
   ladybug, LADYBUG TOOLS HOME P
   Liu SS, 2017, IEEE T VIS COMPUT GR, V23, P1249, DOI 10.1109/TVCG.2016.2640960
   Matejka J, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173943
   McInnes L, 2020, Arxiv, DOI [arXiv:1802.03426, 10.21105/joss.00861]
   Menges A., 2022, ARCHITECTURAL INTELL, V1, DOI [10.1007/s44223- 022-00004-x, DOI 10.1007/S44223-022-00004-X]
   Mueller C., 2013, P 2 INT WORKSH DES C, P46
   Pérez MG, 2022, J COMPUT DES ENG, V9, P310, DOI 10.1093/jcde/qwab081
   Rodrigues N., 2020, GRAPHICS INTERFACE 2
   Ross Joel, 2010, Proceedings of the 28th of the International Conference Extended Abstracts on Human Factors in Computing Systems, P2863, DOI [10.1145/1753846.1753873, DOI 10.1145/1753846.1753873]
   Rubio-Sánchez M, 2016, IEEE T VIS COMPUT GR, V22, P619, DOI 10.1109/TVCG.2015.2467324
   Rubio-Sánchez M, 2014, IEEE T VIS COMPUT GR, V20, P2013, DOI 10.1109/TVCG.2014.2346258
   Sedlmair M, 2014, IEEE T VIS COMPUT GR, V20, P2161, DOI 10.1109/TVCG.2014.2346321
   Sedlmair M, 2012, IEEE T VIS COMPUT GR, V18, P2431, DOI 10.1109/TVCG.2012.213
   Stahnke J, 2016, IEEE T VIS COMPUT GR, V22, P629, DOI 10.1109/TVCG.2015.2467717
   Stasiuk D, 2014, FUSION: DATA INTEGRATION AT ITS BEST, VOL 1, P381
   Vierlinger R., 2013, THESIS U APPL ARTS V
   wallacei, WALLACEI EVOLUTIONAR
   Wang L., 2020, RE ANTHR DES AG HUM, V1, P255, DOI [10.52842/conf.caadria, DOI 10.52842/CONF.CAADRIA]
   Wang LK, 2022, INT J ARCHIT COMPUT, V20, P41, DOI 10.1177/14780771221082254
   WEGMAN EJ, 1990, J AM STAT ASSOC, V85, P664, DOI 10.2307/2290001
   Wise J. A., 1995, Proceedings. Information Visualization (Cat. No.95TB100000), P51, DOI 10.1109/INFVIS.1995.528686
   Wortmann T, 2022, ENERG BUILDINGS, V259, DOI 10.1016/j.enbuild.2022.111863
   Wortmann T, 2017, PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON COMPUTER-AIDED ARCHITECTURAL DESIGN RESEARCH IN ASIA (CAADRIA 2017), P283
   Wortmann T, 2017, INT J ARCHIT COMPUT, V15, P38, DOI 10.1177/1478077117691600
   Wortmann T, 2015, AI EDAM, V29, P471, DOI 10.1017/S0890060415000451
   Zorn M.B., 2023, P 34 FOR BAUINF, P357
NR 52
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4927
EP 4940
DI 10.1007/s00371-024-03491-3
EA JUN 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001248641200006
OA hybrid
DA 2024-08-05
ER

PT J
AU Li, HC
   Liu, XG
AF Li, Hanchao
   Liu, Xinguo
TI Strand-accurate multi-view facial hair reconstruction and tracking
SO VISUAL COMPUTER
LA English
DT Article
DE Facial hair capture; Hair tracking; Performance capture; Multi-view
   stereo
ID GEOMETRY; CAPTURE
AB Accurate modeling of facial hair is crucial not only for reconstructing a clean-shaven face but also for enhancing realism when creating digital human avatars. Previous methods have primarily focused on reconstructing sparse facial hairs from static scans, and more recently, efforts have been made to track facial performances. However, there is still room for improvement in reconstructing thick hairs. In this paper, we address the challenges of facial hair reconstruction and tracking, enhancing the realism and detail of facial hair in digital human avatars. For facial hair reconstruction, we propose a method that combines line-based multi-view stereo with line segment matching to recover a dense hair point cloud. From this point cloud, hair strands are extracted using the forward Euler method. For tracking, we introduce a space-time optimization method that registers the reference facial hair strands to subsequent frames, taking into account both the global shape and the motion between frames. After capturing the facial hair structures, we refine the underlying skin meshes by replacing the noisy hair region points with strand roots. We conducted experiments on various examples captured under different systems, demonstrating that our facial hair capture method outperforms previous methods in terms of accuracy and completeness. Our method provides a comprehensive and accurate solution for capturing and modeling facial hair in various facial performance scenarios.
C1 [Li, Hanchao; Liu, Xinguo] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310000, Peoples R China.
C3 Zhejiang University
RP Liu, XG (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310000, Peoples R China.
EM hanson_li@zju.edu.cn; xinguoliu@zju.edu.cn
FU National Natural Science Foundation of China [62032011]
FX This research was supported by the National Natural Science Foundation
   of China (No. 62032011).
CR BEELER T, 2011, ACM T GRAPHIC, V1, P1
   BEELER T, 2010, ACM SIGGRAPH 2010 PA, P1
   Beeler T, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185613
   Bleyer M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.14
   BRADLEY D, 2010, ACM SIGGRAPH 2010 PA, P1
   Debevec P, 2000, COMP GRAPH, P145, DOI 10.1145/344779.344855
   Fyffe G, 2017, COMPUT GRAPH FORUM, V36, P295, DOI 10.1111/cgf.13127
   Fyffe G, 2016, COMPUT GRAPH FORUM, V35, P353, DOI 10.1111/cgf.12837
   Fyffe G, 2012, SIGGRAPH '12: SPECIAL INTEREST GROUP ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES CONFERENCE, DOI 10.1145/2343045.2343077
   Gafni G, 2021, PROC CVPR IEEE, P8645, DOI 10.1109/CVPR46437.2021.00854
   Galliani S, 2015, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2015.106
   Hu LW, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766931
   Knapitsch A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073599
   Lattas A, 2020, PROC CVPR IEEE, P757, DOI 10.1109/CVPR42600.2020.00084
   Li H., 2022, CHIN C PATT REC COMP, P56
   Luo LJ, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462026
   Nam G, 2019, PROC CVPR IEEE, P155, DOI 10.1109/CVPR.2019.00024
   Olszewski Kyle, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7444, DOI 10.1109/CVPR42600.2020.00747
   Paris S, 2004, ACM T GRAPHIC, V23, P712, DOI 10.1145/1015706.1015784
   Paris S, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360629
   Riviere J, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392464
   Rosu RA, 2022, LECT NOTES COMPUT SC, V13693, P73, DOI 10.1007/978-3-031-19827-4_5
   Rotger Gemma, 2019, Pattern Recognition and Image Analysis. 9th Iberian Conference, IbPRIA 2019. Proceedings. Lecture Notes in Computer Science (LNCS 11867), P423, DOI 10.1007/978-3-030-31332-6_37
   Sun T., 2021, Human hair inverse rendering using multi-view photometric data
   Winberg S, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530116
   Wu FZ, 2019, PROC CVPR IEEE, P959, DOI 10.1109/CVPR.2019.00105
   Xiao QJ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480540
   XU ZX, 2014, IN PRESS, V33
   Yang HT, 2020, PROC CVPR IEEE, P598, DOI 10.1109/CVPR42600.2020.00068
   Yang LC, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356511
   Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47
   Zhang M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073627
   Zhou Y, 2018, LECT NOTES COMPUT SC, V11205, P242, DOI 10.1007/978-3-030-01246-5_15
NR 33
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4713
EP 4724
DI 10.1007/s00371-024-03465-5
EA JUN 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001247174300003
DA 2024-08-05
ER

PT J
AU Mzoughi, MC
   Ben Aoun, N
   Naouali, S
AF Mzoughi, Mohamed Charfeddine
   Ben Aoun, Najib
   Naouali, Sami
TI A review on kinship verification from facial information
SO VISUAL COMPUTER
LA English
DT Review; Early Access
DE Kinship verification; Facial information; Machine learning; Deep
   learning; Metric learning; Ensemble learning; Shallow features
ID FACE; CLASSIFICATION; RECOGNITION; NETWORKS
AB Kinship verification is a challenging computer vision task that aims to mainly answer the question: "Are these two persons blood relatives?". It is an important area of research with many applications, including the identification of criminals or wanted suspects through their relatives, genealogical studies, face recognition improvements, scrapbook organization, character selection for movies, the search for missing family members, and social media analysis. Despite the significant advancements to date, automatic kinship verification remains challenging to accomplish. Moreover, the datasets used to evaluate the developed kinship verification methods reveal intrinsic problems, including changes in the subject's facial information caused by age variation, gender and ethnic differences, as well as external difficulties that are mainly related to the acquisition parameters, such as the various imaging settings and the uncooperative subjects (datasets collected from different sources). Recently, due to the great success of the deep learning models for image and video classification, kinship verification methods have seen a significant shift from the classical metric learning and machine learning methods toward both deep learning and also hybrid methods (ensembles of different methods). Therefore, this paper presents an overview of kinship verification using facial information methods as well as the available image and video datasets used to evaluate them. Besides, an experimental study is carried out to highlight the most efficient methods. Finally, a discussion is provided to weigh up the strengths and limitations of the recent kinship verification approaches and suggest future research directions.
   [GRAPHICS]
   .
C1 [Mzoughi, Mohamed Charfeddine; Naouali, Sami] Mil Res Ctr, Lab Sci & Technol Def STD, Tunis 2045, Tunisia.
   [Ben Aoun, Najib] Al Baha Univ, Fac Comp & Technol, Alaqiq 657797738, Saudi Arabia.
   [Ben Aoun, Najib] Univ Sfax, Natl Sch Engineers Sfax ENIS, Res Grp Intelligent Machines, REGIM Lab, Sfax 3038, Tunisia.
   [Naouali, Sami] King Faisal Univ, Alahsa, Saudi Arabia.
C3 Al Baha University; Universite de Sfax; Ecole Nationale dIngenieurs de
   Sfax (ENIS); King Faisal University
RP Ben Aoun, N (corresponding author), Al Baha Univ, Fac Comp & Technol, Alaqiq 657797738, Saudi Arabia.; Ben Aoun, N (corresponding author), Univ Sfax, Natl Sch Engineers Sfax ENIS, Res Grp Intelligent Machines, REGIM Lab, Sfax 3038, Tunisia.
EM mohamed.c.mzoughi@gmail.com; nbenaoun@bu.edu.sa; snaouali@gmail.com
RI BEN AOUN, Najib/Q-6414-2019
OI BEN AOUN, Najib/0000-0001-9444-8209
CR Almuashi M, 2017, MULTIMED TOOLS APPL, V76, P265, DOI 10.1007/s11042-015-3007-5
   Chen XP, 2022, SIGNAL PROCESS-IMAGE, V101, DOI 10.1016/j.image.2021.116543
   Chouchane A., 2022, 2022 5 INT S INF ITS, P1
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   DALY M, 1982, ETHOL SOCIOBIOL, V3, P69, DOI 10.1016/0162-3095(82)90002-4
   Davis J. V, 2007, P 24 INT C MACH LEAR, P209, DOI DOI 10.1145/1273496.1273523
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Dibeklioglu H, 2013, IEEE I CONF COMP VIS, P1497, DOI 10.1109/ICCV.2013.189
   Dibeklioglu H, 2012, LECT NOTES COMPUT SC, V7574, P525, DOI 10.1007/978-3-642-33712-3_38
   Dikmen M, 2011, LECT NOTES COMPUT SC, V6495, P501, DOI 10.1007/978-3-642-19282-1_40
   Fang RG, 2013, IEEE IMAGE PROC, P2983, DOI 10.1109/ICIP.2013.6738614
   Fang RG, 2010, IEEE IMAGE PROC, P1577, DOI 10.1109/ICIP.2010.5652590
   Fang Y, 2016, 2016 30TH ANNIVERSARY OF VISUAL COMMUNICATION AND IMAGE PROCESSING (VCIP)
   Goyal A, 2022, MACH VISION APPL, V33, DOI 10.1007/s00138-022-01341-7
   Guillaumin M, 2009, IEEE I CONF COMP VIS, P498, DOI 10.1109/ICCV.2009.5459197
   Guo Qiang, 2018, IEEE INT C MULTIMEDI
   Nguyen HV, 2011, LECT NOTES COMPUT SC, V6493, P709
   Hu JL, 2015, LECT NOTES COMPUT SC, V9005, P252, DOI 10.1007/978-3-319-16811-1_17
   Huang KZ, 2009, IEEE DATA MINING, P189, DOI 10.1109/ICDM.2009.22
   Huang S, 2023, IEEE T CYBERNETICS, V53, P6173, DOI 10.1109/TCYB.2022.3163707
   Kan M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011
   Kim H, 2023, COMPUT VIS IMAGE UND, V231, DOI 10.1016/j.cviu.2023.103662
   Klontz JC, 2013, COMPUTER, V46, P91, DOI 10.1109/MC.2013.377
   Köstinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Kohli N., 2012, 2012 IEEE Fifth International Conference On Biometrics: Theory, Applications And Systems (BTAS 2012), P245, DOI 10.1109/BTAS.2012.6374584
   Kohli N, 2019, IEEE T IMAGE PROCESS, V28, P1329, DOI 10.1109/TIP.2018.2840880
   Kohli N, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2609811
   Laiadi O, 2019, IEEE INT CONF AUTOMA, P735, DOI 10.1109/fg.2019.8756627
   Laiadi O, 2020, NEUROCOMPUTING, V377, P286, DOI 10.1016/j.neucom.2019.10.055
   Li D., 2023, Acad. J. Sci. Technol, V5, P57, DOI [10.54097/ajst.v5i1.5348, DOI 10.54097/AJST.V5I1.5348]
   Li HX, 2013, PROC CVPR IEEE, P3499, DOI 10.1109/CVPR.2013.449
   Li L, 2016, LECT NOTES COMPUT SC, V9730, P539, DOI 10.1007/978-3-319-41501-7_60
   Li WH, 2021, PROC CVPR IEEE, P16130, DOI 10.1109/CVPR46437.2021.01587
   Li WH, 2021, IEEE T IMAGE PROCESS, V30, P4947, DOI 10.1109/TIP.2021.3077111
   Li YY, 2017, ADV SOC SCI EDUC HUM, V159, P13
   Liang JQ, 2019, IEEE T IMAGE PROCESS, V28, P1149, DOI 10.1109/TIP.2018.2875346
   Liu F, 2022, MATHEMATICS-BASEL, V10, DOI 10.3390/math10030480
   Liu QF, 2015, INT CONF BIOMETR THE
   Lu JW, 2017, IEEE T IMAGE PROCESS, V26, P4269, DOI 10.1109/TIP.2017.2717505
   Lu JW, 2014, IEEE T PATTERN ANAL, V36, P331, DOI 10.1109/TPAMI.2013.134
   Mahpod S, 2018, COMPUT VIS IMAGE UND, V167, P28, DOI 10.1016/j.cviu.2017.12.003
   Mukherjee M, 2022, J VIS COMMUN IMAGE R, V84, DOI 10.1016/j.jvcir.2022.103470
   OJALA T, 1994, INT C PATT RECOG, P582, DOI 10.1109/ICPR.1994.576366
   Puthenputhussery A, 2016, IEEE IMAGE PROC, P2921, DOI 10.1109/ICIP.2016.7532894
   Qin XQ, 2022, MULTIMED TOOLS APPL, V81, P11049, DOI 10.1007/s11042-022-12032-w
   Qin XQ, 2015, IEEE T MULTIMEDIA, V17, P1855, DOI 10.1109/TMM.2015.2461462
   Robinson J. P., 2016, P 24 ACM INT C MULT, P242, DOI DOI 10.1145/2964284.2967219
   Robinson JP, 2021, IEEE T MULTIMEDIA, V24, P3582, DOI 10.1109/TMM.2021.3103074
   Robinson JP, 2022, IEEE T PATTERN ANAL, V44, P4432, DOI 10.1109/TPAMI.2021.3063078
   Robinson JP, 2018, IEEE T PATTERN ANAL, V40, P2624, DOI 10.1109/TPAMI.2018.2826549
   Serraoui I, 2022, NEURAL NETWORKS, V151, P222, DOI 10.1016/j.neunet.2022.03.020
   Shadrikov A, 2020, IEEE INT CONF AUTOMA, P872, DOI 10.1109/FG47880.2020.00137
   Shao M., 2011, CVPR 2011 WORKSH, P60, DOI DOI 10.1109/CVPRW.2011.5981801
   Song CH, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102891
   Wang SW, 2020, PATTERN RECOGN LETT, V138, P38, DOI 10.1016/j.patrec.2020.06.019
   Wang SY, 2019, IEEE T PATTERN ANAL, V41, P2783, DOI [10.1109/TPAMI.2018.2861871, 10.1109/INTMAG.2018.8508542, 10.1109/TNNLS.2017.2771290]
   Wang SY, 2017, IEEE INT CONF AUTOMA, P216, DOI [10.1109/FG.2017.35, 10.1109/ICEMI.2017.8265769]
   Wang W, 2024, PATTERN RECOGN, V148, DOI 10.1016/j.patcog.2023.110123
   Wang W, 2023, NEUROCOMPUTING, V525, P1, DOI 10.1016/j.neucom.2022.12.031
   Wei Wang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P613, DOI 10.1007/978-3-030-58542-6_37
   Wei ZQ, 2019, IEEE ACCESS, V7, P100029, DOI 10.1109/ACCESS.2019.2929939
   Wu HS, 2021, J VIS COMMUN IMAGE R, V79, DOI 10.1016/j.jvcir.2021.103265
   Wu XT, 2024, IEEE T CYBERNETICS, V54, P1523, DOI 10.1109/TCYB.2022.3220040
   Wu XT, 2019, Arxiv, DOI arXiv:1906.10096
   Xia S., 2012, P 22 INT JOINT C ART, P60
   Xia SY, 2012, IEEE T MULTIMEDIA, V14, P1046, DOI 10.1109/TMM.2012.2187436
   Yan HB, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107541
   Yan HB, 2019, PATTERN RECOGN LETT, V128, P169, DOI 10.1016/j.patrec.2019.08.023
   Yan HB, 2018, PATTERN RECOGN, V75, P15, DOI 10.1016/j.patcog.2017.03.001
   Yan HB, 2014, IEEE T INF FOREN SEC, V9, P1169, DOI 10.1109/TIFS.2014.2327757
   YANG Y., 2017, P INT C ART INT ENG, V10, P947
   Zekrini Fatima, 2022, Artificial Intelligence and Its Applications: Proceeding of the 2nd International Conference on Artificial Intelligence and Its Applications (2021). Lecture Notes in Networks and Systems (413), P486, DOI 10.1007/978-3-030-96311-8_45
   Zhang K., 2015, BRIT MACH VIS C BMVC, P1
   Zhang L, 2021, IEEE T CYBERNETICS, V51, P5883, DOI 10.1109/TCYB.2019.2959403
   Zhao YG, 2018, INFORM SCIENCES, V430, P247, DOI 10.1016/j.ins.2017.11.048
   Zhou X., 2012, P 20 ACM INT C MULT, P725, DOI [DOI 10.1145/2393347.2396297, 10.1145/2393347.2396297, DOI 10.1145/2393347]
   Zhou XZ, 2019, INFORM FUSION, V48, P84, DOI 10.1016/j.inffus.2018.07.011
   Zhou XZ, 2016, INFORM FUSION, V32, P40, DOI 10.1016/j.inffus.2015.08.006
   Zhou X, 2016, IEEE IMAGE PROC, P2911, DOI 10.1109/ICIP.2016.7532892
NR 79
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 14
PY 2024
AR s00371-024-03493-1
DI 10.1007/s00371-024-03493-1
EA JUN 2024
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UH5I1
UT WOS:001247174300002
DA 2024-08-05
ER

PT J
AU Zhao, YF
   Yang, ZJ
   Zhang, YJ
   Chen, YD
AF Zhao, Yanfeng
   Yang, Zhenjian
   Zhang, Yunjie
   Chen, Yadong
TI BGFNet: boundary information-aided graph structure fusion network for
   semantic segmentation of remote sensing images
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Graph structure; Boundary guidance; Attention mechanism; Remote sensing;
   Semantic segmentation; Very high-resolution images
ID NET
AB Semantic segmentation of high-resolution remote sensing (RS) images faces challenges in multi-scale transformation. Although feature fusion is widely used in this task, the existing methods do not fully consider the spatial structure relationship between feature layers in the encoder stage. Firstly, this paper designs BGFNet network, combines the feature extraction in the encoder stage and the topological relationship modeling of graph structure, and proposes the graph structure-guided multi-scale feature fusion module to solve this problem. Secondly, in order to solve the problem of blurred object boundaries in RS image segmentation, we propose a multi-level deformable boundary guidance module, which emphasizes object boundaries by establishing long-range context. Finally, a shared enhanced attention module with shared parameters is proposed to enhance the characteristics of each class to improve the recognition ability of the model. The effectiveness of BGFNet is verified on Potsdam and Vaihingen public RS datasets, and its segmentation performance is obviously better than the existing mainstream methods. The source code will be freely available at https://github.com/zyf-cell/BGF_Net.
C1 [Zhao, Yanfeng; Yang, Zhenjian; Zhang, Yunjie; Chen, Yadong] Tianjin Chengjian Univ, Sch Comp & Informat Engn, Tianjin, Peoples R China.
C3 Tianjin Chengjian University
RP Zhang, YJ (corresponding author), Tianjin Chengjian Univ, Sch Comp & Informat Engn, Tianjin, Peoples R China.
EM zyjtjcj@163.com
CR Bragagnolo L, 2021, CATENA, V201, DOI 10.1016/j.catena.2021.105189
   Cai WW, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2020.3026587
   Cai XH, 2024, Arxiv, DOI arXiv:2403.06258
   Can GC, 2021, PATTERN RECOGN LETT, V150, P108, DOI 10.1016/j.patrec.2021.06.004
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, 10.48550/arXiv.1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen YD, 2024, IEEE T IMAGE PROCESS, V33, P1432, DOI 10.1109/TIP.2024.3364056
   Cheng DC, 2017, IEEE J-STARS, V10, P5769, DOI 10.1109/JSTARS.2017.2747599
   Ding L, 2020, IEEE T GEOSCI REMOTE, V58, P5367, DOI 10.1109/TGRS.2020.2964675
   Pham HN, 2022, SCI TOTAL ENVIRON, V838, DOI 10.1016/j.scitotenv.2022.155826
   Hou JL, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3176028
   Hu J., 2018, SQUEEZE AND EXCITATI, P7132
   Jadhav J. K., 2019, 2018 INT C INT DAT C, P659, DOI [10.1007/978-3-030-34080-374, DOI 10.1007/978-3-030-34080-374]
   Jing W, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3271676
   LeCun Y., 1989, Advances in Neural Information Processing Systems, V2
   Li HC, 2018, Arxiv, DOI [arXiv:1805.10180, 10.48550/arXiv.1805.10180]
   Li LL, 2024, IEEE T PATTERN ANAL, V46, P2123, DOI 10.1109/TPAMI.2023.3332435
   Li R, 2022, INT J REMOTE SENS, V43, P1131, DOI 10.1080/01431161.2022.2030071
   Li R, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3063381
   Li R, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3093977
   Li X, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3243954
   Lin T.-Y., 2017, PROC CVPR IEEE, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu DW, 2018, IEEE GEOSCI REMOTE S, V15, P1274, DOI 10.1109/LGRS.2018.2829807
   Liu JM, 2020, REMOTE SENS LETT, V11, P620, DOI 10.1080/2150704X.2020.1746855
   Liu QH, 2020, IEEE T GEOSCI REMOTE, V58, P6309, DOI 10.1109/TGRS.2020.2976658
   Liu RH, 2021, IEEE T MED IMAGING, V40, P3446, DOI 10.1109/TMI.2021.3087857
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lopez-Fuentes L, 2017, IEEE INT CONF BIG DA, P3746, DOI 10.1109/BigData.2017.8258373
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Maxwell SK, 2007, INT J REMOTE SENS, V28, P5339, DOI 10.1080/01431160601034902
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Nazir A, 2020, IEEE T IMAGE PROCESS, V29, P7192, DOI 10.1109/TIP.2020.2999854
   Peng CL, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3053062
   Rong X, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3170349
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sheng H, 2020, IEEE COMPUT SOC CONF, P267, DOI 10.1109/CVPRW50498.2020.00038
   Tang JX, 2023, VISUAL COMPUT, DOI 10.1007/s00371-023-03145-w
   Tilton JC, 2006, GISCI REMOTE SENS, V43, P39, DOI 10.2747/1548-1603.43.1.39
   TON JC, 1991, IEEE T GEOSCI REMOTE, V29, P222, DOI 10.1109/36.73663
   Vaswani A., 2017, Advances in neural information processing systems, P5998
   Wang JX, 2023, KNOWL-BASED SYST, V267, DOI 10.1016/j.knosys.2023.110415
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xiao XY, 2023, NEURAL NETWORKS, V157, P460, DOI 10.1016/j.neunet.2022.10.034
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Yuan ZQ, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3163706
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao Q, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3085889
   Zhao XQ, 2023, Arxiv, DOI arXiv:2303.10894
   Zhao YY, 2022, IEEE J-STARS, V15, P8118, DOI 10.1109/JSTARS.2022.3205609
   Zheng JW, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3292112
   Zhou RX, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3224477
   Zhou TF, 2024, IEEE T PATTERN ANAL, V46, P5398, DOI 10.1109/TPAMI.2024.3367952
   Zhou TF, 2022, PROC CVPR IEEE, P2572, DOI 10.1109/CVPR52688.2022.00261
   Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]
NR 57
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 6
PY 2024
DI 10.1007/s00371-024-03429-9
EA JUN 2024
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TO4Y3
UT WOS:001242202400001
DA 2024-08-05
ER

PT J
AU Iffath, F
   Gavrilova, M
AF Iffath, Fariha
   Gavrilova, Marina
TI ARF-Net: a multi-modal aesthetic attention-based fusion
SO VISUAL COMPUTER
LA English
DT Article
DE Audio-visual aesthetics; Image processing; Multimedia content; Biometric
   identification; Multi-modal aesthetics; Transfer learning;
   Attention-based fusion
ID IDENTIFICATION
AB Over the last decade, Online Social Media platforms have witnessed a dramatic expansion due to the substantial reliance of individuals on these communication channels. These platforms are widely utilized to convey emotions, share opinions, and express preferences through various means such as artworks, multimedia contents, and blogs. Researchers are exploring these individual-specific traits for biometric identification. Aesthetic biometric systems utilize users' unique preferences across various subjective forms such as images, music, and textual contents. This study introduces a novel multi-modal aesthetic system, with a primary contribution to the development of an attention-based fusion method for person identification. The proposed identification system leverages a deep pre-trained model for high-level feature extraction from visual and auditory modalities. The paper introduces a novel fusion architecture named attention-based residual fusion network (ARF-Net) to incorporate two heterogeneous aesthetic feature vectors. The proposed model yielded a 99.38% identification accuracy on the Aesthetic Image Audio 32 (AIA32) dataset and 98.02% identification accuracy on Aesthetic Image Audio 52 (AIA52) dataset, outperforming other aesthetic biometric systems. The proposed architecture stands out for its efficiency, showcasing a lightweight architecture with minimal parameters, ensuring optimal performance in different modalities.
C1 [Iffath, Fariha; Gavrilova, Marina] Univ Calgary, Calgary, AB, Canada.
C3 University of Calgary
RP Iffath, F (corresponding author), Univ Calgary, Calgary, AB, Canada.
EM fariha.iffath@ucalgary.ca
FU Natural Sciences and Engineering Research Council of Canada
FX No Statement Available
CR Ahmed Dindar M., 2023, 2023 IEEE 14th Control and System Graduate Research Colloquium (ICSGRC), P51, DOI 10.1109/ICSGRC57744.2023.10215398
   Ahmed SU, 2009, LECT NOTES COMPUT SC, V5610, P559, DOI 10.1007/978-3-642-02574-7_63
   Azam S, 2017, LECT NOTES ARTIF INT, V10233, P15, DOI 10.1007/978-3-319-57351-9_2
   Bailey KO, 2014, COMPUT SECUR, V43, P77, DOI 10.1016/j.cose.2014.03.005
   Bari ASMH, 2020, VISUAL COMPUT, V36, P2395, DOI 10.1007/s00371-020-01893-7
   Bowyer K.W., 2006, 2 WORKSH MULT US AUT, V105, P1221
   Cham Karen., 2009, Digital Visual Cult. Theory Pract, P15
   Debas EA, 2023, Art Intel Info Commu, P570, DOI 10.1109/ICAIIC57133.2023.10067017
   Diodato R, 2022, PHILOSOPHIES, V7, DOI 10.3390/philosophies7020029
   Fitzgerald D., 2010, P INT C DIG AUD EFF, V13, P15
   Fu JL, 2022, LECT NOTES COMPUT SC, V13676, P1, DOI 10.1007/978-3-031-19787-1_1
   Gavrilova M., 2022, BREAKTHROUGHS DIGITA, P303
   Gavrilova ML, 2023, IEEE INT CONF INF VI, P329, DOI 10.1109/IV60283.2023.00062
   Guo SH, 2017, VISUAL COMPUT, V33, P63, DOI 10.1007/s00371-016-1329-6
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jain AK., 2008, SCHOLARPEDIA, V3, P3716, DOI [10.4249/scholarpedia.3716, DOI 10.4249/SCHOLARPEDIA.3716]
   Li QY, 2009, COMPUT MATH APPL, V57, P1000, DOI 10.1016/j.camwa.2008.10.058
   Liu D, 2020, IEEE ACCESS, V8, P194894, DOI 10.1109/ACCESS.2020.3033200
   Lovato P., 2012, P AS C COMP VIS, P45, DOI DOI 10.1007/978.3.642.37331.2_4
   Lovato P, 2014, IEEE T INF FOREN SEC, V9, P364, DOI 10.1109/TIFS.2014.2298370
   Magnenat-Thalmann N, 2005, VISUAL COMPUT, V21, P997, DOI 10.1007/s00371-005-0363-6
   Ngiam A., 2011, IEEE INT C MACH LEAR, P689, DOI DOI 10.5555/3104482.3104569
   Paul PP, 2014, VISUAL COMPUT, V30, P1059, DOI 10.1007/s00371-013-0907-0
   Segalin C, 2014, IEEE IMAGE PROC, P4982, DOI 10.1109/ICIP.2014.7026009
   Segalin C., 2014, P 16 INT C MULT INT, P180, DOI DOI 10.1145/2663204.2663259
   Sieu B, 2021, 2021 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2021), P254, DOI 10.1109/CW52790.2021.00050
   Sieu B, 2021, IEEE ACCESS, V9, P102225, DOI 10.1109/ACCESS.2021.3096776
   Sieu B, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20041133
   Sieu B, 2019, 2019 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P279, DOI 10.1109/CW.2019.00053
   Sultana M, 2015, INT J PATTERN RECOGN, V29, DOI 10.1142/S0218001415560133
   Sultana M, 2014, 2014 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P271, DOI 10.1109/CW.2014.44
   Zadeh A., 2017, arXiv
NR 32
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4941
EP 4953
DI 10.1007/s00371-024-03492-2
EA JUN 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001236511100001
DA 2024-08-05
ER

PT J
AU Liu, SH
   Gai, SY
   Da, FP
AF Liu, Shanghuan
   Gai, Shaoyan
   Da, Feipeng
TI Non-corresponding and topology-free 3D face expression transfer
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Geometry-perceiving autoencoder; Non-corresponding; Topology-free; 3D
   expression transfer; Pseudo-ground truth
ID DEFORMATION TRANSFER; FRAMEWORK
AB Expression transfer is an important task in computer graphics and vision. Existing 3D face models constructed on registered meshes or shapes with corresponding vertices cannot transfer expression over practical data. While recent learning-based works achieved pose transfer between 3D unorganized point clouds, they cannot transfer 3D face expressions well because of weak geometry-perceiving ability and lack of ground truth expression faces for training. To solve the problems, we propose an effective framework that can transfer expressions on non-corresponding and topology-free 3D faces for the first time. The framework includes a novel autoencoder that directly processes unordered point clouds to extract identity and expression features and fuse them to generate desired target faces. Multiple geometry-perception operators are introduced to the autoencoder's encoders to obtain 3D faces' valuable geometry information without repetitive modulations in previous methods. Besides, our decoder utilizes cross-attention's powerful interactive perception capability to fuse extracted features and deform target faces in feature space. To train the autoencoder in a supervised manner, we present a submodule that generates pseudo-ground truth expression faces using pre-trained deep models and their latent operations. The experiments demonstrate the proposed method's outstanding 3D face expression transfer performances. Our code and data are available at https://github.com/SEULSH/Non-corresponding-and-Topology-free-3D-Face-Expression-Transfer.
C1 [Liu, Shanghuan; Gai, Shaoyan; Da, Feipeng] Southeast Univ, Key Lab Measurement & Control Complex Syst Engn, Minist Educ, Nanjing 210096, Jiangsu, Peoples R China.
C3 Southeast University - China
RP Gai, SY (corresponding author), Southeast Univ, Key Lab Measurement & Control Complex Syst Engn, Minist Educ, Nanjing 210096, Jiangsu, Peoples R China.
EM 1515646589@qq.com; qxxymm@163.com; dafp@seu.edu.cn
FU Special Project on Basic Research of Frontier Leading Technology of
   Jiangsu Province of China;  [BK20192004C]
FX This work was supported by the Special Project on Basic Research of
   Frontier Leading Technology of Jiangsu Province of China (Grant No.
   BK20192004C).
CR Basset J, 2021, INT CONF 3D VISION, P545, DOI 10.1109/3DV53792.2021.00064
   Ben-Chen Mirela., 2009, P 2009 ACM SIGGRAPH, P67, DOI [10.1145/1599470.1599479, DOI 10.1145/1599470.1599479]
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Booth J, 2018, INT J COMPUT VISION, V126, P233, DOI 10.1007/s11263-017-1009-7
   Bounareli S., 2023, P IEEECVF INT C COMP, P7149
   Bouritsas G, 2019, IEEE I CONF COMP VIS, P7212, DOI 10.1109/ICCV.2019.00731
   Chandran P, 2022, COMPUT GRAPH FORUM, V41, P195, DOI 10.1111/cgf.14468
   Chen H., 2021, BRIT MACH VIS C BMVC, P1
   Chen HY, 2022, AAAI CONF ARTIF INTE, P258
   Chen HY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8610, DOI 10.1109/ICCV48922.2021.00851
   Chen RW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2003, DOI 10.1145/3394171.3413630
   Chen ZX, 2021, PROC CVPR IEEE, P13159, DOI 10.1109/CVPR46437.2021.01296
   Chu HK, 2010, J INF SCI ENG, V26, P379
   Cosmo Luca, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P19, DOI 10.1007/978-3-030-58580-8_2
   Di C, 2021, COMPLEXITY, V2021, DOI 10.1155/2021/6752120
   El-Nouby A, 2021, ADV NEUR IN
   Gao GG, 2021, PROC CVPR IEEE, P3403, DOI 10.1109/CVPR46437.2021.00341
   Gao L, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS
   Gao ZP, 2023, IEEE T NEUR NET LEAR, V34, P8566, DOI 10.1109/TNNLS.2022.3151609
   Gong SW, 2019, IEEE INT CONF COMP V, P4141, DOI 10.1109/ICCVW.2019.00509
   Gu Y., 2023, 2023 IEEE 17 INT C A, P1
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Jiang ZH, 2019, PROC CVPR IEEE, P11949, DOI 10.1109/CVPR.2019.01223
   Jourabloo A., 2022, P IEEE CVF C COMP VI, P20323
   Kacem Anis, 2022, 2022 8th International Conference on Virtual Reality (ICVR), P438, DOI 10.1109/ICVR55215.2022.9848415
   Keyang Zhou, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P341, DOI 10.1007/978-3-030-58542-6_21
   Li HY, 2022, PROC CVPR IEEE, P4156, DOI 10.1109/CVPR52688.2022.00413
   Li HY, 2022, IEEE T IMAGE PROCESS, V31, P4637, DOI 10.1109/TIP.2022.3186536
   Li HY, 2021, IEEE T IMAGE PROCESS, V30, P2016, DOI 10.1109/TIP.2021.3049955
   Li TY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130813
   Liao Z, 2022, LECT NOTES COMPUT SC, V13662, P640, DOI 10.1007/978-3-031-20086-1_37
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Ling JW, 2023, IEEE T VIS COMPUT GR, V29, P3630, DOI 10.1109/TVCG.2022.3166666
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Maejima A, 2022, PROCEEDINGS SIGGRAPH 2022 TALKS, DOI 10.1145/3532836.3536232
   Naruniec J, 2020, COMPUT GRAPH FORUM, V39, P173, DOI 10.1111/cgf.14062
   Olivier N, 2023, COMPUT GRAPH-UK, V110, P69, DOI 10.1016/j.cag.2022.12.004
   Onizuka H., 2019, INT C COMP VIS
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Paszke A, 2019, ADV NEUR IN, V32
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Peng B, 2022, IEEE T CIRC SYST VID, V32, P3673, DOI 10.1109/TCSVT.2021.3106047
   Ploumpis S, 2019, PROC CVPR IEEE, P10926, DOI 10.1109/CVPR.2019.01119
   Qi CR, 2017, ADV NEUR IN, V30
   Ranjan A, 2018, LECT NOTES COMPUT SC, V11207, P725, DOI 10.1007/978-3-030-01219-9_43
   Romero A., 2018, INT C LEARN REPR, P2920, DOI DOI 10.48550/ARXIV.1710.10903
   Rotger G, 2018, INT C PATT RECOG, P2008, DOI 10.1109/ICPR.2018.8545228
   Shi TY, 2019, IEEE I CONF COMP VIS, P161, DOI 10.1109/ICCV.2019.00025
   Song CY, 2023, IEEE T PATTERN ANAL, V45, P10488, DOI 10.1109/TPAMI.2023.3259059
   Song Chaoyue, 2021, ADV NEURAL INF PROCE, P3108
   Song ML, 2007, IEEE T MULTIMEDIA, V9, P1384, DOI 10.1109/TMM.2007.906591
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Sun H, 2022, IEEE WINT CONF APPL, P2334, DOI 10.1109/WACV51458.2022.00239
   Taherkhani F, 2023, IEEE WINT CONF APPL, P826, DOI 10.1109/WACV56688.2023.00089
   Thies J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818056
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JS, 2020, PROC CVPR IEEE, P5830, DOI 10.1109/CVPR42600.2020.00587
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Xu WW, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239535
   Yang HT, 2020, PROC CVPR IEEE, P598, DOI 10.1109/CVPR42600.2020.00068
   Yang J, 2018, GRAPH MODELS, V98, P1, DOI 10.1016/j.gmod.2018.05.003
   Yifan W, 2020, PROC CVPR IEEE, P72, DOI 10.1109/CVPR42600.2020.00015
   Zhang ZH, 2020, INT CONF 3D VISION, P848, DOI 10.1109/3DV50981.2020.00095
   Zhou Yi, 2020, Advances in neural information processing systems, P9251
   Zhu M., 2023, P IEEE CVF INT C COM, P23109
   Zhu MR, 2021, INT J COMPUT VISION, V129, P1820, DOI 10.1007/s11263-021-01442-2
   Zhu MR, 2022, IEEE T NEUR NET LEAR, V33, P893, DOI 10.1109/TNNLS.2020.3030536
NR 69
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 28
PY 2024
DI 10.1007/s00371-024-03473-5
EA MAY 2024
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SJ9P4
UT WOS:001234206700003
DA 2024-08-05
ER

PT J
AU Li, WS
   Zhang, J
   Li, JF
   Zhuo, L
AF Li, Wensheng
   Zhang, Jing
   Li, Jiafeng
   Zhuo, Li
TI Unpaved road segmentation of UAV imagery via a global vision transformer
   with dilated cross window self-attention for dynamic map
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE UAV imagery; Dynamic map; Unpaved road segmentation; Global vision
   transformer; DCWin-attention
ID NETWORK; CLASSIFICATION; EXTRACTION
AB Road segmentation is a fundamental task for dynamic map in unmanned aerial vehicle (UAV) path navigation. In unplanned, unknown and even damaged areas, there are usually unpaved roads with blurred edges, deformations and occlusions. These challenges of unpaved road segmentation pose significant challenges to the construction of dynamic maps. Our major contributions have: (1) Inspired by dilated convolution, we propose dilated cross window self-attention (DCWin-Attention), which is composed of a dilated cross window mechanism and a pixel regional module. Our goal is to model the long-range horizontal and vertical road dependencies for unpaved roads with deformation and blurred edges. (2) A shifted cross window mechanism is introduced through coupling with DCWin-Attention to reduce the influence of occluded roads in UAV imagery. In detail, the GVT backbone is constructed by using the DCWin-Attention block for multilevel deep features with global dependency. (3) The unpaved road is segmented with the confidence map generated by fusing the deep features of different levels in a unified perceptual parsing network. We verify our method on the self-established BJUT-URD dataset and public DeepGlobe dataset, which achieves 67.72 and 52.67% of the highest IoU at proper inference efficiencies of 2.7, 2.8 FPS, respectively, demonstrating its effectiveness and superiority in unpaved road segmentation. Our code is available at https://github.com/BJUT-AIVBD/GVT-URS.
C1 [Li, Wensheng; Zhang, Jing; Li, Jiafeng; Zhuo, Li] Beijing Univ Technol, Fac Informat Technol, 100 Pingleyuan, Beijing 100124, Peoples R China.
   [Li, Wensheng; Zhang, Jing; Li, Jiafeng; Zhuo, Li] Beijing Univ Technol, Beijing Key Lab Computat Intelligence & Intelligen, Beijing, Peoples R China.
C3 Beijing University of Technology; Beijing University of Technology
RP Zhang, J (corresponding author), Beijing Univ Technol, Fac Informat Technol, 100 Pingleyuan, Beijing 100124, Peoples R China.; Zhang, J (corresponding author), Beijing Univ Technol, Beijing Key Lab Computat Intelligence & Intelligen, Beijing, Peoples R China.
EM liwensheng@emails.bjut.edu.cn; zhj@bjut.edu.cn; lijiafenga@163.com;
   zhuoli@bjut.edu.cn
RI li, jiafeng/KVY-4468-2024
OI ZHANG, JING/0000-0003-1290-0738
FU National Natural Science Foundation of China
FX No Statement Available
CR Abdollahi A, 2021, GISCI REMOTE SENS, V58, P1151, DOI 10.1080/15481603.2021.1972713
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen T, 2022, INT GEOSCI REMOTE SE, P3019, DOI 10.1109/IGARSS46834.2022.9883628
   Chen WT, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3188908
   Chen ZH, 2023, IEEE T PATTERN ANAL, V45, P13489, DOI 10.1109/TPAMI.2023.3293885
   Demir I, 2018, IEEE COMPUT SOC CONF, P172, DOI 10.1109/CVPRW.2018.00031
   Ding L, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3168697
   Dong XY, 2022, PROC CVPR IEEE, P12114, DOI 10.1109/CVPR52688.2022.01181
   Elhassan MAM, 2021, EXPERT SYST APPL, V183, DOI 10.1016/j.eswa.2021.115090
   Gao L, 2021, IEEE J-STARS, V14, P10990, DOI 10.1109/JSTARS.2021.3119654
   Gao X, 2018, INT GEOSCI REMOTE SE, P6907, DOI 10.1109/IGARSS.2018.8519093
   Gong S, 2022, IEEE T INTELL TRANSP, V23, P21505, DOI 10.1109/TITS.2022.3192473
   Hassani A, 2023, PROC CVPR IEEE, P6185, DOI 10.1109/CVPR52729.2023.00599
   He X, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3144165
   Huang X, 2014, ISPRS J PHOTOGRAMM, V90, P36, DOI 10.1016/j.isprsjprs.2014.01.008
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Li A, 2022, IEEE IMAGE PROC, P1551, DOI 10.1109/ICIP46576.2022.9897179
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Li XL, 2022, IEEE T CYBERNETICS, V52, P9352, DOI 10.1109/TCYB.2021.3050558
   Liang X, 2020, IEEE T CIRC SYST VID, V30, P1758, DOI 10.1109/TCSVT.2019.2905881
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu F, 2021, J NAVIGATION, V74, P24, DOI 10.1017/S0373463320000375
   Liu SY, 2021, IEEE T INTELL TRANSP, V22, P5520, DOI 10.1109/TITS.2020.2987819
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Pan HH, 2023, IEEE T INTELL TRANSP, V24, P3448, DOI 10.1109/TITS.2022.3228042
   Shamsolmoali P, 2021, IEEE T GEOSCI REMOTE, V59, P4673, DOI 10.1109/TGRS.2020.3016086
   Sheng B, 2022, IEEE T CYBERNETICS, V52, P6662, DOI 10.1109/TCYB.2021.3079311
   Shi WZ, 2002, IEEE T GEOSCI REMOTE, V40, P511, DOI 10.1109/36.992826
   Soni PK, 2021, EGYPT J REMOTE SENS, V24, P211, DOI 10.1016/j.ejrs.2021.01.004
   Soni PK, 2020, J INDIAN SOC REMOTE, V48, P513, DOI 10.1007/s12524-019-01077-4
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang Y, 2021, INT C PATT RECOG, P6949, DOI 10.1109/ICPR48806.2021.9412882
   Xiao TT, 2018, LECT NOTES COMPUT SC, V11209, P432, DOI 10.1007/978-3-030-01228-1_26
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Xu JC, 2023, PROC CVPR IEEE, P19529, DOI 10.1109/CVPR52729.2023.01871
   Yang XF, 2019, IEEE T GEOSCI REMOTE, V57, P7209, DOI 10.1109/TGRS.2019.2912301
   Yu Q., 2021, Adv. Neural Inf. Process. Syst., P12990, DOI [10.48550/arXiv.2106.02277, DOI 10.48550/ARXIV.2106.02277]
   Yuhui Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P173, DOI 10.1007/978-3-030-58539-6_11
   Zhang H, 2022, IEEE T INTELL TRANSP, V23, P11162, DOI 10.1109/TITS.2021.3101053
   Zhang JD, 2024, VISUAL COMPUT, V40, P427, DOI 10.1007/s00371-023-02791-4
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou HL, 2017, IEEE T INTELL TRANSP, V18, P1713, DOI 10.1109/TITS.2016.2622280
NR 45
TC 0
Z9 0
U1 7
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 18
PY 2024
DI 10.1007/s00371-024-03416-0
EA MAY 2024
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RH5D6
UT WOS:001226778800001
DA 2024-08-05
ER

PT J
AU Zawallich, L
AF Zawallich, Lars
TI Unfolding polyhedra via tabu search
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Unfolding; Papercraft; Discrete Optimization; Computational Geometry
AB Folding a discrete geometry from a flat sheet of material is one way to construct a 3D object. A typical creation pipeline first designs the 3D object, unfolds it, prints and cuts the unfold pattern from a 2D material, and then refolds the object. Within this work we focus on the unfold part of this pipeline. Most current unfolding approaches segment the input, which has structural downsides for the refolded result. Therefore, we are aiming to unfold the input into a single-patched pattern. Our algorithm applies tabu search to the topic of unfolding. We show empirically that our algorithm is faster and more reliable than other methods unfolding into single-patched unfold patterns. Moreover, our algorithm can handle any sort of flat polygon as faces, while comparable methods are bound to triangles.
C1 [Zawallich, Lars] Univ Zurich, Zurich, Switzerland.
C3 University of Zurich
RP Zawallich, L (corresponding author), Univ Zurich, Zurich, Switzerland.
EM LarsZawallich@gmail.com
FU University of Zurich
FX I would like to thank Prof. Dr. Marc Alexa for supervising my Master
   Thesis, which was the first step toward this publication. Moreover, I
   would like to thank Prof. Dr. Renato Pajarola for his support and very
   helpful discussions about local minima, as well as for supervising my
   PhD.
CR Alexa M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964997
   An B, 2018, IEEE T ROBOT, V34, P1409, DOI 10.1109/TRO.2018.2862882
   Damian M, 2007, GRAPH COMBINATOR, V23, P179, DOI 10.1007/s00373-007-0701-8
   Damian M, 2017, GRAPH COMBINATOR, V33, P1357, DOI 10.1007/s00373-017-1849-5
   Demaine ED, 2007, GEOMETRIC FOLDING ALGORITHMS: LINKAGES, ORIGAMI, POLYHEDRA, P1, DOI 10.1017/CBO9780511735172
   Demaine E.D., 2020, P CAN C COMP GEOM, P101
   Demaine E.D., 2017, LIPIcs-Leibniz International Proceedings in Informatics, V77, P1, DOI DOI 10.4230/LIPICS.SOCG.2017.34
   Durer Albrecht., 1525, Underweysung der Messung mit dem Zirckel und Richtscheyt
   GLOVER F, 1986, COMPUT OPER RES, V13, P533, DOI 10.1016/0305-0548(86)90048-1
   Haenselmann T, 2012, MULTIMEDIA SYST, V18, P519, DOI 10.1007/s00530-012-0273-1
   Hao Y., 2018, In: Robotics: Science and Systems, V14, p7:1
   Hormann K., 2007, ACM SIGGRAPH Course Notes, DOI DOI 10.1145/1281500.1281510
   Ion A, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417835
   Korpitsch T., 2020, J. WSCG, V28, P47, DOI [10.24132/JWSCG.2020.28.6, DOI 10.24132/JWSCG.2020.28.6]
   Lang R.J., 1988, COMPLETE BOOK ORIGAM
   LIN S, 1973, OPER RES, V21, P498, DOI 10.1287/opre.21.2.498
   Mitani J, 2004, ACM T GRAPHIC, V23, P259, DOI 10.1145/1015706.1015711
   Poranne Roi, 2017, ACM Transactions on Graphics, V36, DOI 10.1145/3130800.3130845
   Robinson N., 2004, The Origami Bible: A Practical Guide to The Art of Paper Folding
   Rus D, 2018, NAT REV MATER, V3, P101, DOI 10.1038/s41578-018-0009-8
   Sawhney R, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3132705
   Schlickenrieder W., 1997, Master's Thesis
   Shatz I, 2006, VISUAL COMPUT, V22, P825, DOI 10.1007/s00371-006-0067-6
   Stein O, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201303
   Straub R., 2011, Creating optimized cut-out sheets for paper models from meshes
   Tachi T, 2010, IEEE T VIS COMPUT GR, V16, P298, DOI 10.1109/TVCG.2009.67
   Takahashi S, 2011, COMPUT GRAPH FORUM, V30, P2077, DOI 10.1111/j.1467-8659.2011.02053.x
   Xi ZH, 2016, COMPUT GRAPH-UK, V58, P139, DOI 10.1016/j.cag.2016.05.022
   Xi ZH, 2015, IEEE INT C INT ROBOT, P3249, DOI 10.1109/IROS.2015.7353828
   Yao MB, 2019, INT J ROBOT RES, V38, P73, DOI 10.1177/0278364918815757
   Zhou QN, 2016, Arxiv, DOI arXiv:1605.04797
NR 31
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 17
PY 2024
DI 10.1007/s00371-024-03395-2
EA MAY 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RH1G1
UT WOS:001226676900002
OA hybrid
DA 2024-08-05
ER

PT J
AU Tong, S
   Liu, H
   Guo, RY
   Wang, WQ
   Liu, D
AF Tong, Shuo
   Liu, Han
   Guo, Runyuan
   Wang, Wenqing
   Liu, Ding
TI Context-Aware Enhanced Virtual Try-On Network with fabric adaptive
   registration
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Virtual try-on; Generative adversarial network; Semantic segmentation;
   Fabric registration
AB Image-based virtual try-on technology provides a better shopping experience for online customers and holds immense commercial value. However, existing methods face challenges in accurately aligning garments and preservation of garment texture details when dealing with challenging body poses or complex target clothing. Furthermore, these methods have been unable to adaptively fuse and generate images based on different body parts in a refined manner, struggling to generate and retain high-quality details of body parts, resulting in limited quality of try-on results. To address these issues, we propose a novel virtual try-on network named Context-Aware Enhanced Virtual Try-On Network (CAE-VTON). The key ideas of our method are as follows: (1) Introducing a Multi-Scale Neighborhood Consensus Warp Module (MNCWM) with matching filtering capability that is sensitive to small semantic differences, which generates highly accurate garment alignment results and coupled natural try-on generation results. (2) Proposing a fabric deformation energy smoothness loss to constrain local deformations of clothing, thus preserving complex details in garments. (3) Designing a Body Reconstruction Module (BRM) that adaptively generates and retains exposed skin areas of the body. (4) Introducing a novel try-on generation module called Context-Adaptive Awareness-Enhanced Try-on Module (CAAETM) that integrates all components and utilizes target semantic label map to adaptively generate the final try-on results for different body parts. We evaluate our model on the VITON-HD and VITON datasets and find that our method achieves state-of-the-art performance in qualitative and quantitative evaluations for virtual try-on.
C1 [Tong, Shuo; Liu, Han; Guo, Runyuan; Wang, Wenqing; Liu, Ding] Xian Univ Technol, Sch Automat & Informat Engn, 5 South Jinhua Rd, Xian 710048, Shaanxi, Peoples R China.
C3 Xi'an University of Technology
RP Liu, H (corresponding author), Xian Univ Technol, Sch Automat & Informat Engn, 5 South Jinhua Rd, Xian 710048, Shaanxi, Peoples R China.
EM ts842029379@gmail.com; liuhan@xaut.edu.cn; xianryan@163.com;
   wangwenqing@xaut.edu.cn; liud@xaut.edu.cn
RI Guo, Runyuan/AAR-4062-2021
OI Guo, Runyuan/0000-0002-7426-3212
FU National Natural Science Foundation of China
FX No Statement Available
CR BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Bynagari N.B., 2019, Asian J. Appl. Sci. Eng.
   Chang Y, 2023, VISUAL COMPUT, V39, P2583, DOI 10.1007/s00371-022-02480-8
   Chen ZY, 2023, VISUAL COMPUT, V39, P3545, DOI 10.1007/s00371-023-02946-3
   Choi S, 2021, PROC CVPR IEEE, P14126, DOI 10.1109/CVPR46437.2021.01391
   Chopra A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5413, DOI 10.1109/ICCV48922.2021.00538
   Du CH, 2022, COMPUT VIS MEDIA, V8, P649, DOI 10.1007/s41095-021-0264-2
   Fele B, 2022, IEEE WINT CONF APPL, P2203, DOI 10.1109/WACV51458.2022.00226
   Ge CJ, 2021, PROC CVPR IEEE, P16923, DOI 10.1109/CVPR46437.2021.01665
   Gong K, 2018, LECT NOTES COMPUT SC, V11208, P805, DOI 10.1007/978-3-030-01225-0_47
   Guo RY, 2024, IEEE T IND INFORM, V20, P2702, DOI 10.1109/TII.2023.3297663
   Guo RY, 2023, IEEE T IND INFORM, V19, P6859, DOI 10.1109/TII.2022.3181692
   Han XT, 2019, IEEE I CONF COMP VIS, P10470, DOI 10.1109/ICCV.2019.01057
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Han Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7847, DOI 10.1109/CVPR42600.2020.00787
   Hu XR, 2023, VISUAL COMPUT, V39, P3347, DOI 10.1007/s00371-023-02999-4
   Hu XR, 2022, VISUAL COMPUT, V38, P3365, DOI 10.1007/s00371-022-02563-6
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jandial Surgan, 2020, 2020 IEEE Winter Conference on Applications of Computer Vision (WACV). Proceedings, P2171, DOI 10.1109/WACV45572.2020.9093458
   Jia X., 2016, P EUR C COMP VIS WOR, P1
   Kang D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8802, DOI 10.1109/ICCV48922.2021.00870
   Kim J, 2020, Arxiv, DOI [arXiv:1907.10830, DOI 10.48550/ARXIV.1907.10830]
   Lee S, 2022, LECT NOTES COMPUT SC, V13677, P204, DOI 10.1007/978-3-031-19790-1_13
   Li CX, 2022, MULTIMED TOOLS APPL, V81, P11071, DOI 10.1007/s11042-021-11398-7
   Li J, 2018, IEEE T MULTIMEDIA, V20, P1672, DOI 10.1109/TMM.2017.2777461
   Li S., 2020, CVPR, P10196
   Li ZY, 2022, IEEE COMPUT SOC CONF, P832, DOI 10.1109/CVPRW56347.2022.00099
   Lin T.-Y., 2017, PROC CVPR IEEE, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu XH, 2019, ADV NEUR IN, V32
   Minar M.R., 2020, CVPR WORKSH, V3, P10
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Patel Chaitanya, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7363, DOI 10.1109/CVPR42600.2020.00739
   Ren B, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3617374
   Rocco I, 2022, IEEE T PATTERN ANAL, V44, P1020, DOI 10.1109/TPAMI.2020.3016711
   Rohr K., 2001, Landmark-Based Image Analysis: Using Geometric and Intensity Models, DOI [10.1007/978-94-015-9787-6, DOI 10.1007/978-94-015-9787-6]
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roy D, 2022, MULTIMED TOOLS APPL, V81, P5051, DOI 10.1007/s11042-021-11647-9
   Shen ZY, 2019, PROC CVPR IEEE, P4219, DOI [10.1109/CVPR.2019.00435, 10.1109/cvpr.2019.00435]
   Song D, 2020, MULTIMED TOOLS APPL, V79, P33757, DOI 10.1007/s11042-019-08363-w
   UNSER M, 1993, IEEE T SIGNAL PROCES, V41, P821, DOI 10.1109/78.193220
   Wahba G., 1990, SIAM
   Wang BC, 2018, LECT NOTES COMPUT SC, V11217, P607, DOI 10.1007/978-3-030-01261-8_36
   Yang B, 2019, ADV NEUR IN, V32
   Yang H, 2022, PROC CVPR IEEE, P3450, DOI 10.1109/CVPR52688.2022.00345
   Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212
   Yu RY, 2019, IEEE I CONF COMP VIS, P10510, DOI 10.1109/ICCV.2019.01061
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18
   Zunair H., 2022, BRIT MACH VIS C
NR 49
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 11
PY 2024
DI 10.1007/s00371-024-03432-0
EA MAY 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QJ3J7
UT WOS:001220465500002
DA 2024-08-05
ER

PT J
AU Guo, MS
   Xiong, MF
   Huang, J
   Hu, XR
   Peng, T
AF Guo, Mengsi
   Xiong, Mingfu
   Huang, Jin
   Hu, Xinrong
   Peng, Tao
TI Face photo-sketch portraits transformation via generation pipeline
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Portraits sketch; Generation pipeline; Style transfer; Generative
   adversarial network
AB Portrait sketching is widely used in digital art, forensic security and other fields with its unique value. However, existing portrait sketch style transfer techniques often focus on overall style transformation, neglecting the hierarchical drawing steps and the integration of details in the sketching process. The generated images still fall short in terms of layering effects and detail representation. To address this issue, this paper proposes a generative adversarial network-based portrait sketch temporal generation pipeline (PSTG). The pipeline simulates the artist's layer-by-layer drawing process, sequentially executing from outline to facial features and then to hair, to generate high-quality sketch images with detailed expressiveness. Additionally, we designed a composite network structure that includes both global and local generators. The global generator is responsible for capturing overall contours and proportions, while the local generators focus on the detailed depiction of facial features and hair. This structure excels in capturing both overall proportions and local details. Experimental results demonstrate that the PSTG method not only restores the detail hierarchy of sketches but also achieves significant success in retaining sketch details and brushstroke effects. It also effectively imitates the brushstroke style of artists, generating portrait sketches that visually resemble the works of professional artists, outperforming existing holistic stylization methods overall.
C1 [Guo, Mengsi; Xiong, Mingfu; Huang, Jin; Hu, Xinrong; Peng, Tao] Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan 430200, Peoples R China.
C3 Wuhan Textile University
RP Xiong, MF (corresponding author), Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan 430200, Peoples R China.
EM guo_o1008@foxmail.com; xmf2013@whu.edu.cn; derick0320@foxmail.com;
   hxr@wtu.edu.cn; pt@wtu.edu.cn
CR Cui AY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14618, DOI 10.1109/ICCV48922.2021.01437
   Duan SC, 2021, IEEE T INF FOREN SEC, V16, P1218, DOI 10.1109/TIFS.2020.3031386
   Goodfellow I. J., 2014, ADV NEURAL INFORM PR
   Heusel M., Advances in Neural Information Processing Systems (NeurIPS), P6736
   Huang ZK, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P652
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kazemi H, 2018, INT C BIOMETRICS SPE INT C BIOMETRICS SPE
   Kazemi H, 2018, IEEE WINT CONF APPL, P1, DOI 10.1109/WACVW.2018.00006
   Li B, 2022, IEEE T MULTIMEDIA, V24, P4077, DOI 10.1109/TMM.2021.3113786
   Li LY, 2021, VISUAL COMPUT, V37, P2855, DOI 10.1007/s00371-021-02236-w
   Li YJ, 2019, PROC CVPR IEEE, P1525, DOI 10.1109/CVPR.2019.00162
   Li YH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P991, DOI 10.1145/3394171.3413684
   Li YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2323, DOI 10.1145/3343031.3350854
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Lu YY, 2018, LECT NOTES COMPUT SC, V11220, P213, DOI 10.1007/978-3-030-01270-0_13
   Pinkney JNM, 2020, Arxiv, DOI arXiv:2010.05334
   Peng CL, 2023, KNOWL-BASED SYST, V259, DOI 10.1016/j.knosys.2022.110026
   Peng CL, 2020, IEEE T IMAGE PROCESS, V29, P8519, DOI 10.1109/TIP.2020.3016502
   Shang MM, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2010, DOI 10.1109/ICASSP39728.2021.9414380
   Sheng B, 2019, IEEE T VIS COMPUT GR, V25, P3216, DOI 10.1109/TVCG.2018.2866090
   Shu YZ, 2022, IEEE T VIS COMPUT GR, V28, P3376, DOI 10.1109/TVCG.2021.3067201
   Shuai Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P601, DOI 10.1007/978-3-030-58555-6_36
   Song GX, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459771
   Suo Y., 2022, ACM Trans. Multimed. Comput. Commun. Appl, V18, P1
   Wan WG, 2021, NEUROCOMPUTING, V438, P107, DOI 10.1016/j.neucom.2021.01.050
   Wang LD, 2018, IEEE INT CONF AUTOMA, P83, DOI 10.1109/FG.2018.00022
   Wang SY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14030, DOI 10.1109/ICCV48922.2021.01379
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Yi R, 2023, IEEE T PATTERN ANAL, V45, P905, DOI 10.1109/TPAMI.2022.3147570
   Yi R, 2021, IEEE T PATTERN ANAL, V43, P3462, DOI 10.1109/TPAMI.2020.2987931
   Yi R, 2019, PROC CVPR IEEE, P10735, DOI 10.1109/CVPR.2019.01100
   Yu J, 2021, IEEE T CYBERNETICS, V51, P4350, DOI 10.1109/TCYB.2020.2972944
   Zenoozi A.D., 2022, INT C MACH VIS IM PR, P1
   Zhang LL, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P627, DOI 10.1145/2671188.2749321
   Zhang MJ, 2018, AAAI CONF ARTIF INTE, P7558
   Zhang MJ, 2020, IEEE T CYBERNETICS, V50, P2701, DOI 10.1109/TCYB.2019.2924589
   Zhang MJ, 2019, IEEE T IMAGE PROCESS, V28, P642, DOI 10.1109/TIP.2018.2869688
   Zhang SC, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1163
   Zhang SC, 2019, IEEE T NEUR NET LEAR, V30, P1419, DOI 10.1109/TNNLS.2018.2869574
   Zhang XM, 2023, INT J COMPUT VISION, V131, P2219, DOI 10.1007/s11263-023-01805-x
   Zhou Y, 2023, IEEE T NEUR NET LEAR, V34, P7719, DOI 10.1109/TNNLS.2022.3146004
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36
   Zhuo Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13515, DOI 10.1109/CVPR42600.2020.01353
NR 46
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 6
PY 2024
DI 10.1007/s00371-024-03403-5
EA MAY 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PN5R5
UT WOS:001214775700001
DA 2024-08-05
ER

PT J
AU Liu, XY
   Huang, GH
   Yuan, XC
   Zheng, ZW
   Zhong, G
   Chen, XH
   Pun, CM
AF Liu, Xinyi
   Huang, Guoheng
   Yuan, Xiaochen
   Zheng, Zewen
   Zhong, Guo
   Chen, Xuhang
   Pun, Chi-Man
TI Weakly supervised semantic segmentation via saliency perception with
   uncertainty-guided noise suppression
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Weakly Supervised Semantic Segmentation; Class Activation Mapping;
   Uncertainty estimation; Attention mechanism
AB Weakly Supervised Semantic Segmentation (WSSS) has become increasingly popular for achieving remarkable segmentation with only image-level labels. Current WSSS approaches extract Class Activation Mapping (CAM) from classification models to produce pseudo-masks for segmentation supervision. However, due to the gap between image-level supervised classification loss and pixel-level CAM generation tasks, the model tends to activate discriminative regions at the image level rather than pursuing pixel-level classification results. Moreover, insufficient supervision leads to unrestricted attention diffusion in the model, further introducing inter-class recognition noise. In this paper, we introduce a framework that employs Saliency Perception and Uncertainty, which includes a Saliency Perception Module (SPM) with Pixel-wise Transfer Loss (SP-PT), and an Uncertainty-guided Noise Suppression method. Specifically, within the SPM, we employ a hybrid attention mechanism to expand the receptive field of the module and enhance its ability to perceive salient object features. Meanwhile, a Pixel-wise Transfer Loss is designed to guide the attention diffusion of the classification model to non-discriminative regions at the pixel-level, thereby mitigating the bias of the model. To further enhance the robustness of CAM for obtaining more accurate pseudo-masks, we propose a noise suppression method based on uncertainty estimation, which applies a confidence matrix to the loss function to suppress the propagation of erroneous information and correct it, thus making the model more robust to noise. We conducted experiments on the PASCAL VOC 2012 and MS COCO 2014, and the experimental results demonstrate the effectiveness of our proposed framework. Code is available at https://github.com/pur-suit/SPU.
C1 [Liu, Xinyi; Huang, Guoheng; Zheng, Zewen] Guangdong Univ Technol, Guangzhou, Peoples R China.
   [Yuan, Xiaochen] Macao Polytech Univ, Macau, Peoples R China.
   [Zhong, Guo] Guangdong Univ Foreign Studies, Guangzhou, Peoples R China.
   [Chen, Xuhang] Huizhou Univ, Huizhou, Peoples R China.
   [Pun, Chi-Man] Univ Macau, Macau, Peoples R China.
C3 Guangdong University of Technology; Macao Polytechnic University;
   Guangdong University of Foreign Studies; Huizhou University; University
   of Macau
RP Huang, GH (corresponding author), Guangdong Univ Technol, Guangzhou, Peoples R China.; Yuan, XC (corresponding author), Macao Polytech Univ, Macau, Peoples R China.
EM 1632170257@qq.com; kevinwong@gdut.edu.cn; xcyuan@mpu.edu.mo;
   zwzheng98@gmail.com; yb77410@um.edu.mo; xuhangc@hzu.edu.cn;
   cmpun@umac.mo
RI Chen, Xuhang/JPW-8742-2023
OI Chen, Xuhang/0000-0001-6000-3914
FU Key Areas Research and Development Program of Guangzhou [2023B01J0029];
   Science and technology research in key areas in Foshan [2020001006832];
   Science and technology projects of Guangzhou [202007040006]; Guangdong
   Provincial Key Laboratory of Cyber-Physical System [2020B1212060069];
   Guangdong Basic and Applied Basic Research Foundation [2023A1515012534];
   National Statistical Science Research Project of China [2022LY096]
FX This work was supported in part by the Key Areas Research and
   Development Program of Guangzhou Grant 2023B01J0029, Science and
   technology research in key areas in Foshan under Grant 2020001006832,
   the Science and technology projects of Guangzhou under Grant
   202007040006, the Guangdong Provincial Key Laboratory of Cyber-Physical
   System under Grant 2020B1212060069, the Guangdong Basic and Applied
   Basic Research Foundation under Grant 2023A1515012534, and the National
   Statistical Science Research Project of China under Grant 2022LY096.
CR Ahn J, 2019, PROC CVPR IEEE, P2204, DOI 10.1109/CVPR.2019.00231
   Ahn J, 2018, PROC CVPR IEEE, P4981, DOI 10.1109/CVPR.2018.00523
   McEver RA, 2020, Arxiv, DOI arXiv:2007.05615
   Bahdanau D, 2016, Arxiv, DOI arXiv:1409.0473
   Blundell C, 2015, PR MACH LEARN RES, V37, P1613
   Chen L., 2020, COMPUTER VISION ECCV, P347, DOI DOI 10.1007/978-3-030-58574-7_21
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Q, 2022, PROC CVPR IEEE, P4278, DOI 10.1109/CVPR52688.2022.00425
   Chen QH, 2022, LECT NOTES COMPUT SC, V13438, P234, DOI 10.1007/978-3-031-16452-1_23
   Chen T, 2023, IEEE T MULTIMEDIA, V25, P1727, DOI 10.1109/TMM.2022.3157481
   Chen X., 2023, Pattern Recognition and Computer Vision, P16
   Chen X., 2023, INT C AC SPEECH SIGN, P1
   Chen XH, 2023, Arxiv, DOI arXiv:2310.02663
   Chen ZH, 2020, IEEE T CYBERNETICS, V50, P2152, DOI 10.1109/TCYB.2018.2875983
   Cheng BW, 2022, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR52688.2022.00135
   Choe J, 2021, IEEE T PATTERN ANAL, V43, P4256, DOI 10.1109/TPAMI.2020.2999099
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fan JS, 2020, AAAI CONF ARTIF INTE, V34, P10762
   Fan JS, 2020, PROC CVPR IEEE, P4282, DOI 10.1109/CVPR42600.2020.00434
   Feng D, 2021, IEEE T INTELL TRANSP, V22, P1341, DOI 10.1109/TITS.2020.2972974
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Gong CW, 2023, FRONT NEUROSCI-SWITZ, V17, DOI 10.3389/fnins.2023.1203104
   Guolei Sun, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P347, DOI 10.1007/978-3-030-58536-5_21
   Pham HN, 2022, SCI TOTAL ENVIRON, V838, DOI 10.1016/j.scitotenv.2022.155826
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He R., 2023, AAAI, V37, P781
   Hernández-Lobato JM, 2015, PR MACH LEARN RES, V37, P1861
   Hou QB, 2018, ADV NEUR IN, V31
   Hu J., 2018, SQUEEZE AND EXCITATI, P7132
   Hu J, 2018, ADV NEUR IN, V31
   Huang Guoli, 2023, Brain Informatics: 16th International Conference, BI 2023, Proceedings. Lecture Notes in Computer Science, Lecture Notes in Artificial Intelligence (13974), P146, DOI 10.1007/978-3-031-43075-6_13
   Huang ZL, 2018, PROC CVPR IEEE, P7014, DOI 10.1109/CVPR.2018.00733
   Jiang PT, 2022, PROC CVPR IEEE, P16865, DOI 10.1109/CVPR52688.2022.01638
   Jiang PT, 2019, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2019.00216
   Jiang Z-H., 2020, Adv. Neural. Inf. Process. Syst, V33, P12837
   Kendall A., 2017, Adv Neural Inf Process Syst, V30
   Khan MZ, 2021, IEEE ACCESS, V9, P83002, DOI 10.1109/ACCESS.2021.3086530
   Kim B, 2021, AAAI CONF ARTIF INTE, V35, P1754
   Kim S., 2023, P AAAI C ART INT, V37, P1142
   Kingma D. P., 2014, arXiv
   Kolesnikov A, 2016, LECT NOTES COMPUT SC, V9908, P695, DOI 10.1007/978-3-319-46493-0_42
   Lee J., 2021, P IEEE CVF C COMP VI, P4071
   Lee J., 2021, P INT C NEUR INF PRO, V34, P27408
   Lee J, 2021, PROC CVPR IEEE, P2643
   Lee J, 2019, PROC CVPR IEEE, P5262, DOI 10.1109/CVPR.2019.00541
   Lee M, 2022, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR52688.2022.00429
   Lee S, 2021, PROC CVPR IEEE, P5491, DOI 10.1109/CVPR46437.2021.00545
   Li JL, 2023, NEUROCOMPUTING, V561, DOI 10.1016/j.neucom.2023.126821
   Li Y, 2022, AAAI CONF ARTIF INTE, P1447
   Li Z., 2023, INT C COMP VIS ICCV, P12449
   Li ZN, 2023, PROCEEDINGS OF THE THIRTY-SECOND INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, IJCAI 2023, P1160
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu S, 2022, PROC CVPR IEEE, P2596, DOI 10.1109/CVPR52688.2022.00263
   Liu W., 2024, arXiv
   Liu W., 2023, AAAI
   Liu WH, 2023, PROC CVPR IEEE, P19434, DOI 10.1109/CVPR52729.2023.01862
   Luo S., 2024, AAAI C ART INT, P4000
   Muhammad K, 2022, IEEE T INTELL TRANSP, V23, P22694, DOI 10.1109/TITS.2022.3207665
   Nazir A, 2020, IEEE T IMAGE PROCESS, V29, P7192, DOI 10.1109/TIP.2020.2999854
   Qiao WF, 2023, IEEE GEOSCI REMOTE S, V20, DOI 10.1109/LGRS.2023.3243575
   Rahman M.M., 2024, P IEEE CVF WINT C AP, P404
   Ru L., 2021, IJCAI, V5, P6
   Ru LX, 2023, PROC CVPR IEEE, P3093, DOI 10.1109/CVPR52729.2023.00302
   Ru LX, 2022, INT J COMPUT VISION, V130, P1127, DOI 10.1007/s11263-022-01586-9
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song CF, 2023, IEEE T PATTERN ANAL, V45, P15996, DOI 10.1109/TPAMI.2023.3301302
   Wang H., 2016, Advances in neural information processing systems, V29
   Wang H, 2022, IEEE T INTELL TRANSP, V23, P21405, DOI 10.1109/TITS.2022.3177615
   Wei YC, 2017, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR.2017.687
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu L, 2022, PROC CVPR IEEE, P4300, DOI 10.1109/CVPR52688.2022.00427
   Xu L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6964, DOI 10.1109/ICCV48922.2021.00690
   Xu RT, 2023, Arxiv, DOI arXiv:2302.13765
   Yao Q, 2020, IEEE ACCESS, V8, P14413, DOI 10.1109/ACCESS.2020.2966647
   Yu-Ting Chang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8988, DOI 10.1109/CVPR42600.2020.00901
   Yude Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12272, DOI 10.1109/CVPR42600.2020.01229
   Zeng X., 2023, IEEE Transactions on Geoscience and Remote Sensing
   Zhang D., 2020, PROC ADV NEURAL IN, V33, P655, DOI DOI 10.5555/3495724.3495780
   Zhang F, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7222, DOI 10.1109/ICCV48922.2021.00715
   Zhao KQ, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23146430
   Zheng ZW, 2023, IEEE T CIRC SYST VID, V33, P2102, DOI 10.1109/TCSVT.2022.3223150
   Zhou T., 2023, 2023 IEEE INT S PROD, P1
   Zhou TF, 2022, PROC CVPR IEEE, P4289, DOI 10.1109/CVPR52688.2022.00426
NR 85
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUL 26
PY 2024
DI 10.1007/s00371-024-03574-1
EA JUL 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZR2E9
UT WOS:001276948900003
DA 2024-08-05
ER

PT J
AU Li, CL
   Wang, XY
   Yi, R
   Zhang, WJ
   Bi, LH
   Ma, LZ
AF Li, Canlin
   Wang, Xinyue
   Yi, Ran
   Zhang, Wenjiao
   Bi, Lihua
   Ma, Lizhuang
TI MCLGAN: a multi-style cartoonization method based on style condition
   information
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Generative adversarial network; Image cartoonization; Multi-style
   learning; Style condition information; Deep learning
AB Image cartoonization, a special kind of style transformation, is a challenging image processing task. Most existing cartoonization methods aim at single-style transformation. While multiple models are trained to achieve multi-style transformation, which is time-consuming and resource-consuming. Meanwhile, existing multi-style cartoonization methods based on generative adversarial network require multiple discriminators to handle different styles, which increases the complexity of the network. To solve the above issues, this paper proposes an image cartoonization method for multi-style transformation based on style condition information, called MCLGAN. This approach integrates two key components for promoting multi-style image cartoonization. Firstly, we design a conditional generator and a multi-style learning discriminator to embed the style condition information into the feature space, so as to enhance the ability of the model in realizing different cartoon styles. Then the new loss mechanism, the conditional contrastive loss, is used strategically to strengthen the difference between different styles, thus effectively realizing multi-style image cartoonization. At the same time, MCLGAN simplifies the cartoonization process of different styles images, and only needs to train the model once, which significantly improves the efficiency. Numerous experiments verify the validity of our method as well as demonstrate the superiority of our method compared to previous methods.
C1 [Li, Canlin; Wang, Xinyue; Zhang, Wenjiao] Zhengzhou Univ Light Ind, Sch Comp Sci & Technol, Zhengzhou 450002, Peoples R China.
   [Yi, Ran; Ma, Lizhuang] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
   [Bi, Lihua] Zhengzhou Univ Light Ind, Coll Software Engn, Zhengzhou 450002, Peoples R China.
C3 Zhengzhou University of Light Industry; Shanghai Jiao Tong University;
   Zhengzhou University of Light Industry
RP Li, CL (corresponding author), Zhengzhou Univ Light Ind, Sch Comp Sci & Technol, Zhengzhou 450002, Peoples R China.
EM lcl_zju@aliyun.com
FU Science and Technology Planning Project of Henan Province; National
   Natural Science Foundation of China [62302297, 61972157]; 
   [242102211003]
FX We would like to express our sincere appreciation to the anonymous
   reviewers for their insightful comments, which have greatly aided us in
   improving the quality of the paper. This work was supported in part by
   the Science and Technology Planning Project of Henan Province under
   Grant 242102211003 and in part by the National Natural Science
   Foundation of China under Grant 62302297 and 61972157.
CR CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen JQ, 2019, CHIN CONTR CONF, P242, DOI [10.23919/chicc.2019.8865931, 10.23919/ChiCC.2019.8865931]
   Chen Y, 2018, PROC CVPR IEEE, P9465, DOI 10.1109/CVPR.2018.00986
   Chen Y, 2017, IEEE IMAGE PROC, P2010, DOI 10.1109/ICIP.2017.8296634
   Chen YC, 2020, PROC CVPR IEEE, P5273, DOI 10.1109/CVPR42600.2020.00532
   Chen YG, 2020, LECT NOTES COMPUT SC, V11961, P176, DOI 10.1007/978-3-030-37731-1_15
   Chen Z., 2023, Visual Comput., P1
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Dong YS, 2022, IEEE T IMAGE PROCESS, V31, P485, DOI 10.1109/TIP.2021.3130539
   Dumoulin Vincent, 2016, ARXIV161007629
   Gao X., 2022, ICML, V2, P6
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Hensel M, 2017, ADV NEUR IN, V30
   Kang M., 2020, Advances in Neural Information Processing Systems, P21357
   Kingma D. P., 2014, arXiv
   Li R, 2021, IEEE T IMAGE PROCESS, V30, P374, DOI 10.1109/TIP.2020.3036754
   Li WB, 2020, NEURAL NETWORKS, V132, P66, DOI 10.1016/j.neunet.2020.08.011
   Lin L., 2023, Vis. Comput., P1
   Liu X., 2020, Information and Computer (Theory Edition), V32, P54
   Luo X., 2022, P AS C COMP VIS, P3206
   Mei Hong, 2016, Computer Engineering and Applications, V52, P213, DOI 10.3778/j.issn.1002-8331.1407-0015
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Miyato Takeru, 2018, ICLR
   Odena A, 2017, PR MACH LEARN RES, V70
   Rao J, 2023, VISUAL COMPUT, V39, P2111, DOI 10.1007/s00371-022-02468-4
   Ronneberger O., 2015, P MED IM COMP COMP A, P234, DOI [DOI 10.48550/ARXIV.1505.04597, DOI 10.1007/978-3-319-24574-4_28]
   Shu YZ, 2022, IEEE T VIS COMPUT GR, V28, P3376, DOI 10.1109/TVCG.2021.3067201
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sutherland J., 2018, INT C LEARN REPR, P1
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Wang YX, 2020, PROC CVPR IEEE, P4452, DOI 10.1109/CVPR42600.2020.00451
   Wu H, 2019, NEUROCOMPUTING, V370, P39, DOI 10.1016/j.neucom.2019.08.075
   Xinrui Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8087, DOI 10.1109/CVPR42600.2020.00811
   Xu LY, 2024, VISUAL COMPUT, DOI 10.1007/s00371-023-03244-8
   Xue R, 2014, OPTIK, V125, P4624, DOI 10.1016/j.ijleo.2014.05.031
   Ye WJ, 2023, VISUAL COMPUT, V39, P609, DOI 10.1007/s00371-021-02361-6
   Yu X., 2023, Vis. Comput., P1
   Zhang P, 2020, PROC CVPR IEEE, P5142, DOI 10.1109/CVPR42600.2020.00519
   Zhang ZJ, 2023, NEUROCOMPUTING, V556, DOI 10.1016/j.neucom.2023.126654
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 41
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 21
PY 2024
DI 10.1007/s00371-024-03550-9
EA JUN 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UY0Q4
UT WOS:001251511100001
DA 2024-08-05
ER

PT J
AU Xu, JY
   Tan, X
   Ju, YX
   Mao, XY
   Zhang, SQ
AF Xu, Jiayi
   Tan, Xuan
   Ju, Yixuan
   Mao, Xiaoyang
   Zhang, Shanqing
TI High similarity controllable face anonymization based on dynamic
   identity perception
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Face anonymization; Saliency analysis; Attribute preservation;
   Generative adversarial network
ID PRIVACY PROTECTION
AB In the meta-universe scenario, with the development of personalized social networks, interactive behaviors such as uploading and sharing personal and family photographs are becoming increasingly widespread. Consequently, the risk of being searched or leaking personal financial information increases. A possible solution is to use anonymized face images instead of real images in the public situations. Most of the existing face anonymization methods attempt to replace a large portion of the face image to modify identity information. However, the resulted faces are often not similar enough to the original faces as seen with the naked eyes. To maintain visual coherence as much as possible while avoiding recognition by face recognition systems, we propose to detect part of the face that is most relevant to the identity based on saliency analysis. Furthermore, we preserve the identification of irrelevant face features by re-injecting them into the regenerated face. The proposed model consists of three stages. Firstly, we employ a dynamic identity perception network to detect the identity-relevant facial region and generate a masked face with removed identity. Secondly, we apply feature selection and preservation network that extracts basic semantic attributes from the original face and also extracts multilevel identity-irrelevant face features from the masked face, and then fuses them into conditional feature vectors for face regeneration. Finally, a pre-trained StyleGAN2 generator is applied to obtain a high-quality identity-obscured face image. The experimental results show that the proposed method can obtain more realistic anonymized face images that retain most of the original facial attributes, while it can deceive face recognition system to protect privacy in the modern digital economy and entertainment scenarios.
C1 [Xu, Jiayi; Tan, Xuan; Zhang, Shanqing] Hangzhou Dianzi Univ, Sch Comp Sci, Hangzhou 310018, Zhejiang, Peoples R China.
   [Ju, Yixuan; Mao, Xiaoyang] Univ Yamanashi, Dept Syst Integrat Engineer, Kofu, Yamanashi 4000016, Japan.
C3 Hangzhou Dianzi University; University of Yamanashi
RP Zhang, SQ (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci, Hangzhou 310018, Zhejiang, Peoples R China.
EM xujiayi@hdu.edu.cn; 211050080@hdu.edu.cn; juyixuan@hdu.edu.cn;
   mao@yamanashi.ac.jp; sqzhang@hdu.edu.cn
FU National Natural Science Foundation of China; Hangzhou Dianzi University
FX We want to express our gratitude to the volunteers from Yamanashi
   University and Hangzhou Dianzi University who generously contributed to
   our study by providing data for the subjective experiment.
CR Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chen X., 2016, Advances in Neural Information Processing Systems NIPS, P2180
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Dietlmeier J, 2021, INT C PATT RECOG, P6912, DOI 10.1109/ICPR48806.2021.9412340
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Harkonen E, 2020, C NEUR INF PROC SYST
   Hensel M, 2017, ADV NEUR IN, V30
   Hou XX, 2022, NEURAL NETWORKS, V145, P209, DOI 10.1016/j.neunet.2021.10.017
   Hu J., 2018, SQUEEZE AND EXCITATI, P7132
   Hu SS, 2022, PROC CVPR IEEE, P14994, DOI 10.1109/CVPR52688.2022.01459
   Hukkelås H, 2020, LECT NOTES COMPUT SC, V11844, P565, DOI 10.1007/978-3-030-33720-9_44
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kingma D. P., 2014, arXiv
   Kuang ZZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3182, DOI 10.1145/3474085.3475464
   Li JZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3891, DOI 10.1145/3474085.3475367
   Li LZ, 2020, Arxiv, DOI arXiv:1912.13457
   Li T, 2021, Arxiv, DOI [arXiv:2103.05472, 10.48550/arXiv.2103.05472]
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Ma T., 2021, arXiv
   Maximov M, 2020, PROC CVPR IEEE, P5446, DOI 10.1109/CVPR42600.2020.00549
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Morales A, 2021, IEEE T PATTERN ANAL, V43, P2158, DOI 10.1109/TPAMI.2020.3015420
   Newton EM, 2005, IEEE T KNOWL DATA EN, V17, P232, DOI 10.1109/TKDE.2005.32
   Nirkin Y, 2019, IEEE I CONF COMP VIS, P7183, DOI 10.1109/ICCV.2019.00728
   Nousi P, 2020, SIGNAL PROCESS-IMAGE, V81, DOI 10.1016/j.image.2019.115699
   Padilla-López JR, 2015, EXPERT SYST APPL, V42, P4177, DOI 10.1016/j.eswa.2015.01.041
   Ribaric S, 2016, SIGNAL PROCESS-IMAGE, V47, P131, DOI 10.1016/j.image.2016.05.020
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shan S, 2020, PROCEEDINGS OF THE 29TH USENIX SECURITY SYMPOSIUM, P1589
   Shao R, 2022, LECT NOTES COMPUT SC, V13673, P712, DOI 10.1007/978-3-031-19778-9_41
   Shen Y., 2020, P IEEECVF C COMPUTER, P9243
   Shen YJ, 2021, PROC CVPR IEEE, P1532, DOI 10.1109/CVPR46437.2021.00158
   Shoshan A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14063, DOI 10.1109/ICCV48922.2021.01382
   Sun QR, 2018, PROC CVPR IEEE, P5050, DOI 10.1109/CVPR.2018.00530
   Sun QR, 2018, LECT NOTES COMPUT SC, V11205, P570, DOI 10.1007/978-3-030-01246-5_34
   Taskiran M, 2020, DIGIT SIGNAL PROCESS, V106, DOI 10.1016/j.dsp.2020.102809
   Vishwamitra N., 2017, P IEEE C COMP VIS PA, P39
   Xia R., 2024, IEEE Trans. Inf. Forensics Secur.
   Xiuye Gu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P727, DOI 10.1007/978-3-030-58592-1_43
   Xu YB, 2022, PROC CVPR IEEE, P7673, DOI 10.1109/CVPR52688.2022.00753
   Yuan L., 2023, IEEE Trans. Circuits Syst. Video Technol.
   Yuan L, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1661, DOI 10.1145/3503161.3548202
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhai LM, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5303, DOI 10.1145/3503161.3547757
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang Y., 2023, IEEE Trans. Inf. Forensics Secur.
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 52
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 18
PY 2024
DI 10.1007/s00371-024-03526-9
EA JUN 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UQ5T1
UT WOS:001249543600004
DA 2024-08-05
ER

PT J
AU Gao, F
   You, SZ
   Ge, YS
   Zhang, SF
AF Gao, Fei
   You, Shengzhe
   Ge, Yisu
   Zhang, Shifeng
TI Gaussian-based adaptive frame skipping for visual object tracking
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Object tracking; Correlation filter-based tracker; Tracking
   optimization; Tracking-skipping
ID SELECTION; DECISION
AB Visual object tracking is a basic computer vision problem, which has been greatly developed in recent years. Although the accuracy of object tracking algorithms has been improved, the efficiency of most trackers is hard to meet practical requirements, especially for devices with limited computational power. To improve visual object tracking efficiency with no or little loss of accuracy, a frame skipping method is proposed for correlation filter-based trackers, which includes an adaptive tracking-skipping algorithm and Gaussian-based movement prediction. According to the movement state of objects in the previous frames, the position of objects in the next frame can be predicted, and whether or not the tracking process should be skipped is determined by the predicted position. Experiments are conducted on both practical video surveillance and well-known public data sets to evaluate the proposed method. Experimental results show that the proposed method can almost double the tracking efficiency of correlation filter-based trackers with no or little accuracy loss.
C1 [Gao, Fei; You, Shengzhe; Zhang, Shifeng] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou, Zhejiang, Peoples R China.
   [Ge, Yisu] Wenzhou Univ, Coll Comp Sci & Artificial Intelligence, Wenzhou, Zhejiang, Peoples R China.
C3 Zhejiang University of Technology; Wenzhou University
RP Gao, F (corresponding author), Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou, Zhejiang, Peoples R China.
EM feig@zjut.edu.cn; 892729609@qq.com; ysg@wzu.edu.cn;
   zhangshifeng@hikvision.com
OI gao, fei/0000-0002-4678-1936
FU Key Research and Development Program of Hunan Province of China
   [2020AAA0104001, 2019KD0AD011005]; National Key Research and Development
   Project of China [2021C03129]; Zhejiang Provincial Science and
   Technology Planning Key Project of China
FX This work is being supported by the National Key Research and
   Development Project of China under Grant No. 2020AAA0104001, the
   Zhejiang Lab. under Grant No. 2019KD0AD011005 and the Zhejiang
   Provincial Science and Technology Planning Key Project of China under
   Grant No. 2021C03129.
CR Abbass MY, 2021, VISUAL COMPUT, V37, P993, DOI 10.1007/s00371-020-01848-y
   Abualigah L., 2023, Intell. Autom. Soft Comput, V5, P94
   Agushaka JO, 2023, NEURAL COMPUT APPL, V35, P4099, DOI 10.1007/s00521-022-07854-6
   Agushaka JO, 2022, COMPUT METHOD APPL M, V391, DOI 10.1016/j.cma.2022.114570
   Bachhuber C, 2019, IEEE T CIRC SYST VID, V29, P2760, DOI 10.1109/TCSVT.2018.2870256
   Bao CL, 2012, PROC CVPR IEEE, P1830, DOI 10.1109/CVPR.2012.6247881
   Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Choi J, 2018, Arxiv, DOI arXiv:1702.06291
   Cui Z., 2010, IEEE INT C COMM TECH
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Danelljan Martin, 2014, P BRIT MACH VIS C 20
   Ezugwu AE, 2022, NEURAL COMPUT APPL, V34, P20017, DOI 10.1007/s00521-022-07530-9
   Fadlallah FA, 2016, PROCEEDINGS OF 6TH INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATION ENGINEERING (ICCCE 2016), P475, DOI 10.1109/ICCCE.2016.105
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Ghasemi M, 2024, J BIONIC ENG, V21, P374, DOI 10.1007/s42235-023-00437-8
   Gorur P, 2014, IEEE T CIRC SYST VID, V24, P1156, DOI 10.1109/TCSVT.2014.2319611
   Held D, 2016, LECT NOTES COMPUT SC, V9905, P749, DOI 10.1007/978-3-319-46448-0_45
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hu G, 2023, ADV ENG INFORM, V58, DOI 10.1016/j.aei.2023.102210
   Hu G, 2023, ADV ENG INFORM, V57, DOI 10.1016/j.aei.2023.102004
   Huang ZH, 2022, VISUAL COMPUT, V38, P2739, DOI 10.1007/s00371-021-02150-1
   Jiang JH, 2010, TENCON IEEE REGION, P2062, DOI 10.1109/TENCON.2010.5686590
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kim K, 2005, REAL-TIME IMAGING, V11, P172, DOI 10.1016/j.rti.2004.12.004
   Kim MH, 2012, IEEE INT SYMP CIRC S, P612, DOI 10.1109/ISCAS.2012.6272106
   Kristan M, 2016, IEEE T PATTERN ANAL, V38, P2137, DOI 10.1109/TPAMI.2016.2516982
   Lim KW, 2018, IEEE SYST J, V12, P1577, DOI 10.1109/JSYST.2016.2589238
   Liu YZ, 2007, J VIS COMMUN IMAGE R, V18, P253, DOI 10.1016/j.jvcir.2007.01.003
   Liu Yinan, 2017, IEEE INT C MULTIMEDI
   Liu YQ, 2019, IEEE ACCESS, V7, P144648, DOI 10.1109/ACCESS.2019.2945136
   Nam H, 2016, Arxiv, DOI arXiv:1608.07242
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Patwardhan KA, 2008, IEEE T PATTERN ANAL, V30, P746, DOI 10.1109/TPAMI.2007.70843
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Seo JJ, 2017, IMAGE VISION COMPUT, V58, P76, DOI 10.1016/j.imavis.2016.06.002
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Vojir T, 2014, PATTERN RECOGN LETT, V49, P250, DOI 10.1016/j.patrec.2014.03.025
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xi Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9381, DOI 10.1109/CVPR42600.2020.00940
   Yang GB, 2005, REAL-TIME IMAGING, V11, P310, DOI 10.1016/j.rti.2005.06.005
   Yang SY, 2005, IEEE IMAGE PROC, P3257
   Zare M, 2023, J BIONIC ENG, V20, P2359, DOI 10.1007/s42235-023-00386-2
   Zhan CH, 2007, PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON IMAGE AND GRAPHICS, P519, DOI 10.1109/ICIG.2007.153
   Zhang JF, 2009, PROCEEDINGS OF 2009 INTERNATIONAL CONFERENCE ON IMAGE ANALYSIS AND SIGNAL PROCESSING, P192
   Zhang KH, 2014, LECT NOTES COMPUT SC, V8693, P127, DOI 10.1007/978-3-319-10602-1_9
   Zhang YC, 2021, NEUROCOMPUTING, V455, P1, DOI 10.1016/j.neucom.2021.05.011
   Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882
   Zivkovic Z, 2004, INT C PATT RECOG, P28, DOI 10.1109/ICPR.2004.1333992
NR 56
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 5
PY 2024
DI 10.1007/s00371-024-03439-7
EA JUN 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TO3D0
UT WOS:001242155100003
DA 2024-08-05
ER

PT J
AU Zhou, HQ
   Huang, WK
   Zhu, ZY
   Chen, XD
   Go, K
   Mao, XY
AF Zhou, Haiqiang
   Huang, Wangkang
   Zhu, Zhenyang
   Chen, Xiaodiao
   Go, Kentaro
   Mao, Xiaoyang
TI Fast image recoloring for red-green anomalous trichromacy with contrast
   enhancement and naturalness preservation
SO VISUAL COMPUTER
LA English
DT Article
DE Color vision deficiency; Personalized image recoloring; Contrast
   compensation; Naturalness preservation; CIE L*a*b* color space
ID COLOR; SIMULATION
AB Color vision deficiency (CVD) is an eye disease caused by genetics that reduces the ability to distinguish colors, affecting approximately 200 million people worldwide. In response, image recoloring approaches have been proposed in existing studies for CVD compensation, and a state-of-the-art recoloring algorithm has even been adapted to offer personalized CVD compensation; however, it is built on a color space that is lacking perceptual uniformity, and its low computation efficiency hinders its usage in daily life by individuals with CVD. In this paper, we propose a fast and personalized degree-adaptive image-recoloring algorithm for CVD compensation that considers naturalness preservation and contrast enhancement. Moreover, we transferred the simulated color gamut of the varying degrees of CVD in RGB color space to CIE L*a*b* color space, which offers perceptual uniformity. To verify the effectiveness of our method, we conducted quantitative and subject evaluation experiments, demonstrating that our method achieved the best scores for contrast enhancement and naturalness preservation.
C1 [Zhou, Haiqiang; Chen, Xiaodiao] Hangzhou Dianzi Univ, Hangzhou 310018, Peoples R China.
   [Zhou, Haiqiang; Huang, Wangkang; Zhu, Zhenyang; Go, Kentaro; Mao, Xiaoyang] Univ Yamanashi, Yamanashi 4008510, Japan.
   [Chen, Xiaodiao] Xinchuang Haihe Lab, Tianjin 300480, Peoples R China.
C3 Hangzhou Dianzi University; University of Yamanashi
RP Zhu, ZY (corresponding author), Univ Yamanashi, Yamanashi 4008510, Japan.
EM haiqiang920@gmail.com; huang10669668281@gmail.com; zzhu@yamanashi.ac.jp;
   xiaodiao@hdu.edu.cn; go@yamanashi.ac.jp; mao@yamanashi.ac.jp
RI Mao, Xiaoyang/AAG-1294-2020
OI Mao, Xiaoyang/0000-0001-5010-6952
FU University of Yamanashi [22H00549, 22K21274, 23K16899, 20K20408]; JSPS
FX This work is supported by JSPS Grants-in-Aid for Scientific Research
   (Grant Nos. 22H00549, 22K21274, 23K16899 and 20K20408). The authors
   express their gratitude to all participants who assessed the suggested
   approach and provided valuable feedback.
CR Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Brettel H, 1997, J OPT SOC AM A, V14, P2647, DOI 10.1364/JOSAA.14.002647
   Chua SH, 2015, ACM T COMPUT-HUM INT, V21, DOI 10.1145/2687923
   Ebelin P., 2023, EUROGRAPHICS 2023 SH, DOI [10.2312/egs.20231011, DOI 10.2312/EGS.20231011]
   Farnsworth D, 1943, J OPT SOC AM, V33, P568, DOI 10.1364/JOSA.33.000568
   Farnsworth D, 1947, FARNSWORTH DICHOTOMO
   Hassan MF, 2019, MULTIDIM SYST SIGN P, V30, P1975, DOI 10.1007/s11045-019-00638-7
   Hassan MF, 2017, SIGNAL PROCESS-IMAGE, V57, P126, DOI 10.1016/j.image.2017.05.011
   Hu XH, 2019, IEEE T COMPUT IMAG, V5, P649, DOI 10.1109/TCI.2019.2908291
   Hu XH, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356534
   Huang CR, 2011, IEEE T MULTIMEDIA, V13, P950, DOI 10.1109/TMM.2011.2135844
   Huang CR, 2010, LECT NOTES COMPUT SC, V6298, P637, DOI 10.1007/978-3-642-15696-0_59
   Huang HB, 2007, IEEE SIGNAL PROC LET, V14, P711, DOI 10.1109/LSP.2007.898333
   Huang JB, 2009, INT CONF ACOUST SPEE, P1161, DOI 10.1109/ICASSP.2009.4959795
   Huang WK, 2022, VISUAL COMPUT, V38, P3405, DOI 10.1007/s00371-022-02549-4
   Hunt R.W. G., 1995, The reproduction of colour, V4
   Ishihara S., 1987, Test for colour-blindness
   Jiang SY, 2023, IEEE I CONF COMP VIS, P22514, DOI 10.1109/ICCV51070.2023.02063
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kuhn GR, 2008, IEEE T VIS COMPUT GR, V14, P1747, DOI 10.1109/TVCG.2008.112
   Lau C, 2011, IEEE I CONF COMP VIS, P1172, DOI 10.1109/ICCV.2011.6126366
   Lin HY, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19102250
   Machado GM, 2010, COMPUT GRAPH FORUM, V29, P933, DOI 10.1111/j.1467-8659.2009.01701.x
   Machado GM, 2009, IEEE T VIS COMPUT GR, V15, P1291, DOI 10.1109/TVCG.2009.113
   Rasche K, 2005, COMPUT GRAPH FORUM, V24, P423, DOI 10.1111/j.1467-8659.2005.00867.x
   Rasche K, 2005, IEEE COMPUT GRAPH, V25, P22, DOI 10.1109/MCG.2005.54
   Sharpe L.T., 1999, OPSIN GENES CONE PHO
   Shen WY, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925878
   Wakita Ken., 2005, Assets '05: Proceedings of the 7th international ACM SIGACCESS conference on Computers and accessibility, P158, DOI DOI 10.1145/1090785.1090815
   Wang XY, 2021, COMPUT GRAPH-UK, V98, P19, DOI 10.1016/j.cag.2021.04.027
   Zhu ZY, 2022, IEEE T MULTIMEDIA, V24, P1721, DOI 10.1109/TMM.2021.3070108
   Zhu ZY, 2021, VISUAL COMPUT, V37, P2999, DOI 10.1007/s00371-021-02240-0
   Zhu ZY, 2019, SIGNAL PROCESS-IMAGE, V76, P68, DOI 10.1016/j.image.2019.04.004
   Zhu Z, 2019, VISUAL COMPUT, V35, P1053, DOI 10.1007/s00371-019-01689-4
NR 34
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4647
EP 4660
DI 10.1007/s00371-024-03454-8
EA MAY 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001232207700002
OA hybrid, Green Submitted
DA 2024-08-05
ER

PT J
AU Bian, FH
   Xiong, SY
   Yi, R
   Ma, LZ
AF Bian, Feihu
   Xiong, Suya
   Yi, Ran
   Ma, Lizhuang
TI Multi-view stereo-regulated NeRF for urban scene novel view synthesis
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Neural radiance fields; KITTI-360; Novel view synthesis
AB Neural radiance fields (NeRF), which encode a scene into a neural representation, have demonstrated impressive novel view synthesis quality on single object and small regions of space. However, when faced with urban outdoor environments, NeRF is limited by the capacity of a single MLP and insufficient input views, leading to incorrect geometries that hinder the production of realistic renderings. In this paper, we present MVSRegNeRF, an extension of neural radiance fields focused on large-scale autonomous driving scenario. We employ traditional patch-match based Multi-view stereo (MVS) method to generate dense depth maps, which we utilize to regulate the geometry optimization of NeRF. We also integrate multi-resolution hash encodings into our neural scene representation to accelerate the training process. Thanks to the relatively precise geometry constraint of our approach, we achieve high-quality novel view synthesis on real-world large-scale street scene. Our experiments on the KITTI-360 dataset demonstrate that MVSRegNeRF outperforms the state-of-the-art methods in Novel View Appearance Synthesis tasks.
C1 [Bian, Feihu; Ma, Lizhuang] East China Normal Univ, Shanghai, Peoples R China.
   [Xiong, Suya; Yi, Ran] Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
C3 East China Normal University; Shanghai Jiao Tong University
RP Ma, LZ (corresponding author), East China Normal Univ, Shanghai, Peoples R China.; Yi, R (corresponding author), Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
EM ranyi@sjtu.edu.cn; lzma@cs.ecnu.edu.cn
CR Barron JT, 2022, PROC CVPR IEEE, P5460, DOI 10.1109/CVPR52688.2022.00539
   Barron JT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5835, DOI 10.1109/ICCV48922.2021.00580
   Chen AP, 2022, LECT NOTES COMPUT SC, V13692, P333, DOI 10.1007/978-3-031-19824-3_20
   Chen AP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14104, DOI 10.1109/ICCV48922.2021.01386
   Deng KL, 2022, PROC CVPR IEEE, P12872, DOI 10.1109/CVPR52688.2022.01254
   Dosovitskiy A., 2017, P 1 ANN C ROB LEARN, P1, DOI DOI 10.48550/ARXIV.1711.03938
   Fu Q, 2022, NeurIPS, V35, P3403
   Fu X, 2022, INT CONF 3D VISION, P301, DOI 10.1109/3DV57658.2022.00042
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI [10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166]
   Hu T, 2022, PROC CVPR IEEE, P12892, DOI 10.1109/CVPR52688.2022.01256
   Jain A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5865, DOI 10.1109/ICCV48922.2021.00583
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Kopanas G, 2021, COMPUT GRAPH FORUM, V40, P29, DOI 10.1111/cgf.14339
   Kundu A, 2022, PROC CVPR IEEE, P12861, DOI 10.1109/CVPR52688.2022.01253
   Labatut P, 2007, IEEE I CONF COMP VIS, P504
   LEVOY M, 1990, ACM T GRAPHIC, V9, P245, DOI 10.1145/78964.78965
   Liao Y., 2022, Pattern Analysis and Machine Intelligence (PAMI)
   Ma L, 2022, PROC CVPR IEEE, P12851, DOI 10.1109/CVPR52688.2022.01252
   Martin-Brualla R, 2021, PROC CVPR IEEE, P7206, DOI 10.1109/CVPR46437.2021.00713
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Mildenhall B, 2022, PROC CVPR IEEE, P16169, DOI 10.1109/CVPR52688.2022.01571
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Rebain D, 2022, PROC CVPR IEEE, P1548, DOI 10.1109/CVPR52688.2022.00161
   Reiser C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14315, DOI 10.1109/ICCV48922.2021.01407
   Rematas K, 2022, PROC CVPR IEEE, P12922, DOI 10.1109/CVPR52688.2022.01259
   Riegler Gernot, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P623, DOI 10.1007/978-3-030-58529-7_37
   Roessle B, 2022, PROC CVPR IEEE, P12882, DOI 10.1109/CVPR52688.2022.01255
   Schönberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31
   Sun C, 2022, Arxiv, DOI arXiv:2206.05085
   Sun Cheng, 2022, CVPR
   Tancik M, 2022, PROC CVPR IEEE, P8238, DOI 10.1109/CVPR52688.2022.00807
   Tancik M, 2020, Arxiv, DOI [arXiv:2006.10739, DOI 10.48550/ARXIV.2006.10739,ARXIV]
   Wang Junke, 2022, EUR C COMP VIS
   Wang P., 2021, Neural Inf. Process. Syst.
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5590, DOI 10.1109/ICCV48922.2021.00556
   Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47
   Yariv L, 2021, ADV NEUR IN
   Yu A, 2021, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR46437.2021.00455
   Yu Z., 2022, Advances in neural information processing systems, V35, P25018
   Zhang K, 2020, Arxiv, DOI arXiv:2010.07492
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
NR 43
TC 0
Z9 0
U1 6
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 27
PY 2024
DI 10.1007/s00371-024-03321-6
EA MAR 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MO8T2
UT WOS:001194660900002
DA 2024-08-05
ER

PT J
AU Kwon, BK
   Kim, SB
   Oh, TH
AF Kwon, Byung-Ki
   Kim, Sung-Bin
   Oh, Tae-Hyun
TI The devil in the details: simple and effective optical flow synthetic
   data generation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Curriculum learning; Deep learning; Optical flow; Synthetic data
AB Recent work on dense optical flow has shown significant progress, primarily in a supervised learning manner requiring a large amount of labeled data. Due to the expensiveness of obtaining large-scale real-world data, computer graphics are typically leveraged for constructing datasets. However, there is a common belief that synthetic-to-real domain gaps limit generalization to real scenes. In this paper, we show that the required characteristics in an optical flow dataset are rather simple and present a simpler synthetic data generation method that achieves a certain level of realism with compositions of elementary operations. With 2D motion-based datasets, we systematically analyze the simplest yet critical factors for generating synthetic datasets. Furthermore, we propose a novel method of utilizing occlusion masks in a supervised method and observe that suppressing gradients on occluded regions serves as a powerful initial state in the curriculum learning sense. The RAFT network initially trained on our dataset outperforms the original RAFT on the two most challenging online benchmarks, MPI Sintel and KITTI 2015.
C1 [Kwon, Byung-Ki; Oh, Tae-Hyun] POSTECH, Grad Sch AI, Pohang, South Korea.
   [Kim, Sung-Bin; Oh, Tae-Hyun] POSTECH, Dept Elect & Elect Engn, Pohang, South Korea.
   [Oh, Tae-Hyun] Yonsei Univ, Inst Convergence Res & Educ Adv Technol, Seoul, South Korea.
C3 Pohang University of Science & Technology (POSTECH); Pohang University
   of Science & Technology (POSTECH); Yonsei University
RP Oh, TH (corresponding author), POSTECH, Grad Sch AI, Pohang, South Korea.; Oh, TH (corresponding author), POSTECH, Dept Elect & Elect Engn, Pohang, South Korea.; Oh, TH (corresponding author), Yonsei Univ, Inst Convergence Res & Educ Adv Technol, Seoul, South Korea.
EM byungki.kwon@postech.ac.kr; sungbin@postech.ac.kr; taehyun@postech.ac.kr
RI Oh, Tae-Hyun/D-7854-2016
OI Oh, Tae-Hyun/0000-0003-0468-1571
FU Korea Research Institute for defense Technology planning & advancement -
   Defense Acquisition Program Administration (DAPA) [KRIT-CT-22-037];
   Hyper-connected space-time information fusion artificial intelligence
   technologies
FX This work was supported by the Korea Research Institute for defense
   Technology planning & advancement (KRIT) grant funded by the Defense
   Acquisition Program Administration (DAPA) (No. KRIT-CT-22-037,
   Hyper-connected space-time information fusion artificial intelligence
   technologies for signs detection and analysis)
CR Aleotti F, 2021, PROC CVPR IEEE, P15196, DOI 10.1109/CVPR46437.2021.01495
   Black M. J., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P231, DOI 10.1109/ICCV.1993.378214
   Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44
   Byung-Ki K., 2022, Dflow: Learning to synthesize better optical flow datasets via a differentiable pipeline
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gaidon A, 2016, PROC CVPR IEEE, P4340, DOI 10.1109/CVPR.2016.470
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Hofinger Markus, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P770, DOI 10.1007/978-3-030-58604-1_46
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hui TW, 2018, PROC CVPR IEEE, P8981, DOI 10.1109/CVPR.2018.00936
   Hur J, 2019, PROC CVPR IEEE, P5747, DOI 10.1109/CVPR.2019.00590
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Janai J, 2017, PROC CVPR IEEE, P1406, DOI 10.1109/CVPR.2017.154
   Jeong J, 2022, PROC CVPR IEEE, P3171, DOI 10.1109/CVPR52688.2022.00318
   Jiang HZ, 2018, PROC CVPR IEEE, P9000, DOI 10.1109/CVPR.2018.00938
   Jonschkowski R., 2020, EUR C COMP VIS ECCV
   Kondermann D, 2016, IEEE COMPUT SOC CONF, P19, DOI 10.1109/CVPRW.2016.10
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Mayer N, 2018, INT J COMPUT VISION, V126, P942, DOI 10.1007/s11263-018-1082-6
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Menze M, 2015, LECT NOTES COMPUT SC, V9358, P16, DOI 10.1007/978-3-319-24947-6_2
   Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925
   Oh TH, 2018, LECT NOTES COMPUT SC, V11208, P663, DOI 10.1007/978-3-030-01225-0_39
   Roth S, 2007, INT J COMPUT VISION, V74, P33, DOI 10.1007/s11263-006-0016-x
   Sun DQ, 2021, PROC CVPR IEEE, P10088, DOI 10.1109/CVPR46437.2021.00996
   Sun DQ, 2020, IEEE T PATTERN ANAL, V42, P1408, DOI 10.1109/TPAMI.2019.2894353
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Teed Z., 2020, EUR C COMP VIS, P402, DOI DOI 10.1007/978-3-030-58536-524
   Yang Gengshan, 2019, Advances in neural information processing systems, V5, P12
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zhao SY, 2020, PROC CVPR IEEE, P6277, DOI 10.1109/CVPR42600.2020.00631
NR 32
TC 0
Z9 0
U1 10
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 17
PY 2024
DI 10.1007/s00371-024-03263-z
EA FEB 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IB1P1
UT WOS:001163773100001
DA 2024-08-05
ER

PT J
AU Wang, C
   Pei, ZC
   Qiu, S
   Wang, YC
   Tang, ZY
AF Wang, Chen
   Pei, Zhongcai
   Qiu, Shuang
   Wang, Yachun
   Tang, Zhiyong
TI StairNetV3: depth-aware stair modeling using deep learning
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Stair modeling; Depth-awareness; Point cloud reconstruction; Deep
   learning
ID IMAGE; RECOGNITION
AB Vision-based stair modeling can help autonomous mobile robots deal with the challenge of climbing stairs, especially in unfamiliar environments. To address the problem that current monocular methods are difficult to model stairs accurately without depth information in scenes with fuzzy visual cues, this paper proposes a depth-aware stair modeling method for monocular vision. Specifically, we take the prediction of depth images and the extraction of stair geometric features as joint tasks in a convolutional neural network, with the designed information propagation architecture, we can achieve effective supervision for stair geometric feature learning by depth features. In addition, to complete the stair modeling, we take the convex lines, concave lines, tread surfaces and riser surfaces as stair geometric features and apply Gaussian kernels to enable StairNetV3 to predict contextual information within the stair lines. Combined with the depth information obtained by depth sensors, we propose a point cloud reconstruction method that can quickly segment point clouds of stair step surfaces. The experiments show that the proposed method has a significant improvement over the previous best monocular vision method, with an intersection over union increase of 3.4%\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\%$$\end{document}, and the lightweight version has a fast detection speed and can meet the requirements of most real-time applications.
C1 [Wang, Chen; Pei, Zhongcai; Qiu, Shuang; Wang, Yachun; Tang, Zhiyong] Beihang Univ, Sch Automat Sci & Elect Engn, Xueyuan Rd, Beijing 100191, Peoples R China.
C3 Beihang University
RP Tang, ZY (corresponding author), Beihang Univ, Sch Automat Sci & Elect Engn, Xueyuan Rd, Beijing 100191, Peoples R China.
EM venus@buaa.edu.cn; peizc@buaa.edu.cn; zb2003108@buaa.edu.cn;
   yachun@buaa.edu.cn; zyt_76@buaa.edu.cn
CR CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Carbonara S, 2014, 2014 IEEE INTERNATIONAL SYMPOSIUM ON INNOVATIONS IN INTELLIGENT SYSTEMS AND APPLICATIONS (INISTA 2014), P313, DOI 10.1109/INISTA.2014.6873637
   Dai XL, 2022, NEUROCOMPUTING, V506, P1, DOI 10.1016/j.neucom.2022.07.026
   Diamantis DE, 2019, COMM COM INF SC, V1000, P522, DOI 10.1007/978-3-030-20257-6_45
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Garcia-Garcia A., 2017, Appl. Semant. Segm, V9, P1
   Glenn J., 2019, yolov5
   Haris M, 2023, VISUAL COMPUT, V39, P519, DOI 10.1007/s00371-021-02353-6
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hough, 1962, METHOD MEANS RECOGNI
   Howard A. G., 2017, ARXIV, DOI DOI 10.48550/ARXIV.1704.04861
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Intel, Depth Camera D435i
   Khaliluzzaman Md, 2018, 2018 2nd International Conference on Innovations in Science, Engineering and Technology (ICISET), P519, DOI 10.1109/ICISET.2018.8745624
   Khaliluzzaman M, 2018, IEEE IND ELEC, P3250, DOI 10.1109/IECON.2018.8591340
   Khaliluzzaman M, 2016, C HUM SYST INTERACT, P330, DOI 10.1109/HSI.2016.7529653
   Kingma D. P., 2014, arXiv
   Krausz NE, 2015, I IEEE EMBS C NEUR E, P615, DOI 10.1109/NER.2015.7146698
   Lee JT, 2017, IEEE I CONF COMP VIS, P3249, DOI 10.1109/ICCV.2017.350
   Lee YH, 2012, INT C PATT RECOG, P3770
   Matsumura H, 2022, IEEE ACCESS, V10, P56249, DOI 10.1109/ACCESS.2022.3178154
   Munoz R, 2016, IEEE INT CONF MULTI
   Murakami S, 2014, JOINT INT CONF SOFT, P1186, DOI 10.1109/SCIS-ISIS.2014.7044705
   Oh K.W., 2015, IEIE Trans. Smart Process. Comput, V4, P403, DOI DOI 10.5573/IEIESPC.2015.4.6.403
   Patil U, 2019, 2019 THIRD IEEE INTERNATIONAL CONFERENCE ON ROBOTIC COMPUTING (IRC 2019), P159, DOI 10.1109/IRC.2019.00031
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Perez-Yus A, 2017, COMPUT VIS IMAGE UND, V154, P192, DOI 10.1016/j.cviu.2016.04.007
   Pérez-Yus A, 2015, LECT NOTES COMPUT SC, V8927, P449, DOI 10.1007/978-3-319-16199-0_32
   Platt J., 1998, SEQUENTIAL MINIMAL O
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Redmon J., 2018, YOLOv 3: An Incremental ImprovementC
   Rekhawar Nilakshi, 2022, 2022 1st International Conference on the Paradigm Shifts in Communication, Embedded Systems, Machine Learning and Signal Processing (PCEMS)., P78, DOI 10.1109/PCEMS55161.2022.9807915
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shahrabadi S, 2013, LECT NOTES COMPUT SC, V7887, P847
   Shuihua Wang, 2012, Proceedings of the 2012 IEEE International Conference on Bioinformatics and Biomedicine Workshops (BIBMW), P732, DOI 10.1109/BIBMW.2012.6470227
   Wang Chen, 2023, Mendeley Data, V1, DOI 10.17632/6KFFMJT7G2.1
   Wang C, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23042175
   Wang Chen, 2023, Mendeley Data, V2, DOI 10.17632/P28NCJNVGK.2
   Wang C, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-20667-w
   Wang SH, 2014, J VIS COMMUN IMAGE R, V25, P263, DOI 10.1016/j.jvcir.2013.11.005
   Westfechtel T, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P473, DOI 10.1109/IROS.2016.7759096
   Xiangdong Huang, 2018, 2018 2nd IEEE Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC). Proceedings, P1130, DOI 10.1109/IMCEC.2018.8469186
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xue N, 2020, PROC CVPR IEEE, P2785, DOI 10.1109/CVPR42600.2020.00286
   [叶一飞 Ye Yifei], 2020, [电子测量与仪器学报, Journal of Electronic Measurement and Instrument], V34, P124
   Zatout C, 2022, VISUAL COMPUT, V38, P2691, DOI 10.1007/s00371-021-02147-w
   Zatout C, 2019, IEEE INT CONF COMP V, P4376, DOI 10.1109/ICCVW.2019.00538
   Zequn Qin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P276, DOI 10.1007/978-3-030-58586-0_17
   Zhang HT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2949, DOI 10.1109/ICCV48922.2021.00296
   Zhao K, 2022, IEEE T PATTERN ANAL, V44, P4793, DOI 10.1109/TPAMI.2021.3077129
   Zhao XM, 2018, CHIN CONT DECIS CONF, P5018, DOI 10.1109/CCDC.2018.8408001
   Zhou QY, 2018, Arxiv, DOI arXiv:1801.09847
   Zhou X, 2019, PSYCHOL HEALTH, V34, P811, DOI 10.1080/08870446.2019.1574348
   Zhou YC, 2019, IEEE I CONF COMP VIS, P962, DOI 10.1109/ICCV.2019.00105
NR 54
TC 1
Z9 1
U1 13
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 FEB 12
PY 2024
DI 10.1007/s00371-024-03268-8
EA FEB 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HK8R5
UT WOS:001159495100003
DA 2024-08-05
ER

PT J
AU Tychola, KA
   Vrochidou, E
   Papakostas, GA
AF Tychola, Kyriaki A.
   Vrochidou, Eleni
   Papakostas, George A.
TI Deep learning based computer vision under the prism of 3D point clouds:
   a systematic review
SO VISUAL COMPUTER
LA English
DT Review; Early Access
DE Point cloud; LiDAR; RGB-D; Deep learning; Computer vision; 3D data;
   Review
ID LIDAR; SEGMENTATION; RECONSTRUCTION; REGISTRATION; MODELS;
   CLASSIFICATION; GENERATION; NETWORKS; DATABASE; DATASET
AB Point clouds consist of 3D data points and are among the most considerable data formats for 3D representations. Their popularity is due to their broad application areas, such as robotics and autonomous driving, and their employment in basic 3D vision tasks such as segmentation, classification, and detection. However, processing point clouds is challenging compared to other visual forms such as images, mainly due to their unstructured nature. Deep learning (DL) has been established as a powerful tool for data processing, reporting remarkable performance enhancements compared to traditional methods for all basic 2D vision tasks. However new challenges are emerging when it comes to processing unstructured 3D point clouds. This work aims to guide future research by providing a systematic review of DL on 3D point clouds, holistically covering all 3D vision tasks. 3D technologies of point cloud formation are reviewed and compared to each other. The application of DL methods for point cloud processing is discussed, and state-of-the-art models' performances are compared focusing on challenges and solutions. Moreover, in this work the most popular 3D point cloud benchmark datasets are summarized based on their task-oriented applications, aiming to highlight existing constraints and to comparatively evaluate them. Future research directions and upcoming trends are also highlighted.
C1 [Tychola, Kyriaki A.; Vrochidou, Eleni; Papakostas, George A.] Int Hellen Univ, Dept Comp Sci, MLV Res Grp, Kavala 65404, Greece.
C3 International Hellenic University
RP Papakostas, GA (corresponding author), Int Hellen Univ, Dept Comp Sci, MLV Res Grp, Kavala 65404, Greece.
EM gpapak@cs.ihu.gr
RI Papakostas, George/F-1038-2017
OI Papakostas, George/0000-0001-5545-1499; Vrochidou,
   Eleni/0000-0002-0148-8592
FU International Hellenic University; MPhil program "Advanced Technologies
   in Informatics and Computers"
FX This work was supported by the MPhil program "Advanced Technologies in
   Informatics and Computers", hosted by the Department of Computer
   Science, International Hellenic University, Kavala, Greece.
CR Abd-Alzhra A. S., 2022, Iraqi J. Sci, V63, P1299, DOI [10.24996/ijs.2022.63.3.34, DOI 10.24996/IJS.2022.63.3.34]
   Acar H, 2019, INT J REMOTE SENS, V40, P138, DOI 10.1080/01431161.2018.1508915
   Ahmed E, 2019, Arxiv, DOI arXiv:1808.01462
   Akagic A., 2022, 2022 28 INT C INFORM, P1
   Alaba SY, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22249577
   Alexandrov SV, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4217, DOI 10.1109/IROS.2016.7759621
   Armeni I., 2017, arXiv
   Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Atik ME, 2021, ISPRS INT J GEO-INF, V10, DOI 10.3390/ijgi10030187
   Atkinson KB, 2003, Photogramm. Rec, V18, P329, DOI DOI 10.1046/J.0031-868X.2003.024_01.X
   Babatunde O., 2015, Journal of Agricultural Informatics, V6, P61
   Bamler R, 2009, PHOTOGRAMM FERNERKUN, P407, DOI 10.1127/1432-8364/2009/0029
   Behley J, 2021, INT J ROBOT RES, V40, P959, DOI 10.1177/02783649211006735
   Bello SA, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12111729
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bi SS, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11093938
   Hua BS, 2018, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2018.00109
   Boulch Alexandre, 2017, P WORKSH 3D OBJ RETR, V3, P17, DOI [10.2312/3dor.20171047, DOI 10.2312/3DOR.20171047]
   Bovenga F, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20071851
   Brightman N, 2023, AIMS GEOSCI, V9, P68, DOI 10.3934/geosci.2023005
   Bucksch A, 2010, VISUAL COMPUT, V26, P1283, DOI 10.1007/s00371-010-0520-4
   Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164
   Camuffo E, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22041357
   Cao C., 2019, 24 INT C 3D WEB TECH, P1
   Cao KM, 2018, IEEE GLOB CONF SIG, P390, DOI 10.1109/GlobalSIP.2018.8646392
   Cha D, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10101144
   Chang A.X., 2015, ArXiv
   Chang AE, 2017, Arxiv, DOI arXiv:1709.06158
   Chang MF, 2019, PROC CVPR IEEE, P8740, DOI 10.1109/CVPR.2019.00895
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Chen Y., 2019, PROC IEEE COMPUT SOC, V9
   Chen YZ, 2019, NEURAL PROCESS LETT, V49, P1355, DOI 10.1007/s11063-018-9877-6
   Cheraghian A, 2020, IEEE WINT CONF APPL, P912, DOI 10.1109/WACV45572.2020.9093545
   Chiang HY, 2019, INT CONF 3D VISION, P155, DOI 10.1109/3DV.2019.00026
   Choy C, 2020, PROC CVPR IEEE, P2511, DOI 10.1109/CVPR42600.2020.00259
   Choy C, 2019, IEEE I CONF COMP VIS, P8957, DOI 10.1109/ICCV.2019.00905
   Dai A, 2018, LECT NOTES COMPUT SC, V11214, P458, DOI 10.1007/978-3-030-01249-6_28
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Darabi S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185578
   Nguyen DT, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P4220, DOI 10.1109/ICASSP39728.2021.9414763
   Davis J, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P428, DOI 10.1109/TDPVT.2002.1024098
   De Deuge M., 2013, AUSTRALASIAN C ROBOT
   Debeunne C, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20072068
   Deng HW, 2018, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2018.00028
   Pais GD, 2020, Arxiv, DOI [arXiv:1904.01701, 10.48550/ARXIV.1904.01701]
   Djahel R., 2021, ISPRS Ann Photogramm. Remote Sens. Spat. Inf. Sci, V2, P51, DOI DOI 10.5194/ISPRS-ANNALS-V-2-2021-51-2021
   Doersch C, 2016, Arxiv, DOI [arXiv:1505.05192, 10.48550/ARXIV.1505.05192]
   Dong K, 2022, VISUAL COMPUT, V38, P51, DOI 10.1007/s00371-020-01999-y
   Dong Z, 2020, ISPRS J PHOTOGRAMM, V163, P327, DOI 10.1016/j.isprsjprs.2020.03.013
   El Mahdaoui A, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22062199
   Engelmann F, 2020, IEEE INT CONF ROBOT, P9463, DOI [10.1109/ICRA40945.2020.9197503, 10.1109/icra40945.2020.9197503]
   Engelmann F, 2017, IEEE INT CONF COMP V, P716, DOI 10.1109/ICCVW.2017.90
   Fan HH, 2019, Arxiv, DOI arXiv:1910.08287
   Fan J., 2019, Engineering, Environmental Science, Computer Science, DOI DOI 10.5281/ZENODO.3566281
   Fan YL, 2018, VISUAL COMPUT, V34, P659, DOI 10.1007/s00371-017-1405-6
   Fang J, 2020, IEEE ROBOT AUTOM LET, V5, P1931, DOI 10.1109/LRA.2020.2969927
   Fathi H, 2011, ADV ENG INFORM, V25, P760, DOI 10.1016/j.aei.2011.06.001
   Fei B, 2022, IEEE T INTELL TRANSP, V23, P22862, DOI 10.1109/TITS.2022.3195555
   Ferrante E, 2018, LECT NOTES COMPUT SC, V11046, P294, DOI 10.1007/978-3-030-00919-9_34
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Geyer J., 2020, arXiv, DOI DOI 10.48550/ARXIV.2004.06320
   Giancola S, 2019, PROC CVPR IEEE, P1359, DOI 10.1109/CVPR.2019.00145
   Goel V, 2018, ADV NEURAL INFORM PR, P5683, DOI DOI 10.48550/ARXIV.1805.07780
   Golla T, 2015, IEEE INT C INT ROBOT, P5087, DOI 10.1109/IROS.2015.7354093
   González-Jorge H, 2017, DRONES-BASEL, V1, DOI 10.3390/drones1010002
   Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961
   Griffiths D, 2019, Arxiv, DOI arXiv:1907.04758
   Griffiths D, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11121499
   Grilli E, 2017, INT ARCH PHOTOGRAMM, V42-2, P339, DOI 10.5194/isprs-archives-XLII-2-W3-339-2017
   Groh F, 2019, LECT NOTES COMPUT SC, V11361, P105, DOI 10.1007/978-3-030-20887-5_7
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Guo Y, 2018, VISUAL COMPUT, V34, P1325, DOI 10.1007/s00371-017-1416-3
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Gupta S, 2015, PROC CVPR IEEE, P4731, DOI 10.1109/CVPR.2015.7299105
   Gur S, 2019, PROC CVPR IEEE, P7675, DOI 10.1109/CVPR.2019.00787
   Haala N, 2010, PHOTOGRAMM FERNERKUN, P99, DOI 10.1127/1432-8364/2010/0043
   Hackel T., 2017, ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, VI V-1/W1, P91
   Han X.-F., 2018, arXiv
   Hansen L, 2021, LECT NOTES COMPUT SC, V12729, P18, DOI 10.1007/978-3-030-78191-0_2
   Baig MH, 2017, Arxiv, DOI [arXiv:1709.08855, DOI 10.48550/ARXIV.1709.08855, 10.48550/ARXIV.1709.08855]
   He YS, 2021, PROC CVPR IEEE, P3002, DOI 10.1109/CVPR46437.2021.00302
   Hemalatha C., 2014, Int. J. Comput. Appl, V91, P38, DOI [10.5120/15969-5407, DOI 10.5120/15969-5407]
   Hering A., 2019, P BILDV MED, V2019, P309
   Hermosilla P, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275110
   Honti R., 2019, Pollack Period, V14, P189, DOI [10.1556/606.2019.14.3.18, DOI 10.1556/606.2019.14.3.18]
   Hooda R, 2022, IEEE SOUTHEASTCON, P522, DOI 10.1109/SoutheastCon48659.2022.9763998
   Hornung A, 2013, AUTON ROBOT, V34, P189, DOI 10.1007/s10514-012-9321-0
   Hu SM, 2020, IEEE T VIS COMPUT GR, V26, P2485, DOI 10.1109/TVCG.2018.2889944
   Hu W, 2019, IEEE T IMAGE PROCESS, V28, P4087, DOI 10.1109/TIP.2019.2906554
   Hua BS, 2016, INT CONF 3D VISION, P92, DOI 10.1109/3DV.2016.18
   Huang JX, 2021, PROC CVPR IEEE, P10128, DOI 10.1109/CVPR46437.2021.01000
   Huang LL, 2020, PROC CVPR IEEE, P1310, DOI 10.1109/CVPR42600.2020.00139
   Huang XS, 2019, Arxiv, DOI [arXiv:1903.04630, 10.48550/ARXIV.1903.04630, DOI 10.48550/ARXIV.1903.04630]
   Ioannidou A, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3042064
   Islam MM, 2021, IEEE SYS MAN CYBERN, P1287, DOI 10.1109/SMC52423.2021.9658639
   Jiang L, 2020, PROC CVPR IEEE, P4866, DOI 10.1109/CVPR42600.2020.00492
   Jiang PY, 2022, PROCEDIA COMPUT SCI, V199, P1066, DOI 10.1016/j.procs.2022.01.135
   Jiao LC, 2019, IEEE ACCESS, V7, P128837, DOI 10.1109/ACCESS.2019.2939201
   Kaimaris D, 2017, INT J INTELL UNMANNE, V5, P18, DOI 10.1108/IJIUS-12-2016-0009
   Kalogerakis E, 2017, PROC CVPR IEEE, P6630, DOI 10.1109/CVPR.2017.702
   Kamnik R, 2020, ACCIDENT ANAL PREV, V135, DOI 10.1016/j.aap.2019.105391
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17
   Kesten R., 2019, Lyft Level5 AV Dataset
   Kim B, 2019, LECT NOTES COMPUT SC, V11769, P166, DOI 10.1007/978-3-030-32226-7_19
   Kingsland Kaitlyn., 2020, Digital Applications in Archaeology and Cultural Heritage, V18, DOI DOI 10.1016/J.DAACH.2020.E00157
   Kitchenham B., 2004, PROCEDURES PERFORMIN, DOI DOI 10.5144/0256-4947.2017.79
   Krebs Julian, 2017, Medical Image Computing and Computer Assisted Intervention  MICCAI 2017. 20th International Conference. Proceedings: LNCS 10433, P344, DOI 10.1007/978-3-319-66182-7_40
   Kurdi F.T., 2021, 2021 DIGITAL IMAGE C, P1
   Kuznietsov Y, 2017, PROC CVPR IEEE, P2215, DOI 10.1109/CVPR.2017.238
   Landrieu L, 2019, PROC CVPR IEEE, P7432, DOI 10.1109/CVPR.2019.00762
   Le T, 2018, PROC CVPR IEEE, P9204, DOI 10.1109/CVPR.2018.00959
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lei H, 2019, PROC CVPR IEEE, P9623, DOI 10.1109/CVPR.2019.00986
   Li B, 2017, IEEE INT C INT ROBOT, P1513, DOI 10.1109/IROS.2017.8205955
   Li RH, 2019, Arxiv, DOI [arXiv:1907.10844, 10.48550/ARXIV.1907.10844]
   Li XQ, 2021, IET COMPUT VIS, V15, P312, DOI 10.1049/cvi2.12045
   Li XH, 2022, KNOWL INF SYST, V64, P3197, DOI 10.1007/s10115-022-01756-8
   Li YY, 2015, COMPUT GRAPH FORUM, V34, P435, DOI 10.1111/cgf.12573
   Li Y, 2020, IEEE SIGNAL PROC MAG, V37, P50, DOI 10.1109/MSP.2020.2973615
   Li ZQ, 2023, COMPUT GRAPH FORUM, V42, DOI 10.1111/cgf.14795
   Liang M, 2019, PROC CVPR IEEE, P7337, DOI 10.1109/CVPR.2019.00752
   Lin CH, 2018, AAAI CONF ARTIF INTE, P7114
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu H, 2020, IEEE T INSTRUM MEAS, V69, P950, DOI 10.1109/TIM.2019.2908715
   Liu L., 2018, Deep learning for generic object detection: A survey, DOI DOI 10.1007/S11263-019-01247-4
   Liu LY, 2022, INFORMATION, V13, DOI 10.3390/info13040169
   Liu S., 2021, 3D Point Cloud Analysis, P1
   Liu WP, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19194188
   Liu YY, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22072666
   Long J., 2015, Fully convolutional networks for semantic segmentation, P3431
   Lu WX, 2019, IEEE I CONF COMP VIS, P12, DOI 10.1109/ICCV.2019.00010
   Luo CY, 2022, Arxiv, DOI arXiv:2201.12769
   Mahjourian R, 2018, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2018.00594
   Malleson C., 2019, RGB-D Image Analysis and Processing, P87, DOI [DOI 10.1007/978-3-030-28603-3_5, 10.1007/978-3-030-28603-3_5]
   Manivasagam Sivabalan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11164, DOI 10.1109/CVPR42600.2020.01118
   Mao J., 2021, arXiv
   McClelland JR, 2017, PHYS MED BIOL, V62, P4273, DOI 10.1088/1361-6560/aa6070
   Mehmood K, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10010043
   Milioto A, 2019, IEEE INT C INT ROBOT, P4213, DOI 10.1109/IROS40897.2019.8967762
   Minaee S, 2022, IEEE T PATTERN ANAL, V44, P3523, DOI 10.1109/TPAMI.2021.3059968
   Mitra NJ, 2006, ACM T GRAPHIC, V25, P560, DOI 10.1145/1141911.1141924
   Mo KC, 2019, PROC CVPR IEEE, P909, DOI 10.1109/CVPR.2019.00100
   Moon D, 2019, AUTOMAT CONSTR, V98, P322, DOI 10.1016/j.autcon.2018.07.020
   Munoz D, 2009, PROC CVPR IEEE, P975, DOI 10.1109/CVPRW.2009.5206590
   Nealen A., 2006, PROC INT C COMPUT GR, P381, DOI [10.1145/1174429.1174494, DOI 10.1145/1174429.1174494]
   Nguyen DT, 2021, IEEE T CIRC SYST VID, V31, P4617, DOI 10.1109/TCSVT.2021.3100279
   Niemirepo TT, 2021, MMSYS '21: PROCEEDINGS OF THE 2021 MULTIMEDIA SYSTEMS CONFERENCE, P12, DOI 10.1145/3458305.3463374
   Nurunnabi A, 2015, PATTERN RECOGN, V48, P1404, DOI 10.1016/j.patcog.2014.10.014
   Ochotta T, 2008, COMPUT GRAPH FORUM, V27, P1647, DOI 10.1111/j.1467-8659.2008.01178.x
   Pal SK, 2021, APPL INTELL, V51, P6400, DOI 10.1007/s10489-021-02293-7
   Pan L, 2021, PROC CVPR IEEE, P8520, DOI 10.1109/CVPR46437.2021.00842
   Paoletti ME, 2019, ISPRS J PHOTOGRAMM, V158, P279, DOI 10.1016/j.isprsjprs.2019.09.006
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Patil A, 2019, IEEE INT CONF ROBOT, P9552, DOI [10.1109/icra.2019.8793925, 10.1109/ICRA.2019.8793925]
   Peng C, 2020, CONSTR BUILD MATER, V263, DOI 10.1016/j.conbuildmat.2020.120080
   Pirasteh S, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11111272
   Poiesi F, 2023, IEEE T PATTERN ANAL, V45, P3979, DOI 10.1109/TPAMI.2022.3175371
   Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Qi HZ, 2020, PROC CVPR IEEE, P6328, DOI 10.1109/CVPR42600.2020.00636
   Qian R, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108796
   Quach M, 2021, Arxiv, DOI [arXiv:2102.12839, DOI 10.48550/ARXIV.2102.12839]
   Quach M, 2019, IEEE IMAGE PROC, P4320, DOI [10.1109/icip.2019.8803413, 10.1109/ICIP.2019.8803413]
   Pham QH, 2020, IEEE INT CONF ROBOT, P2267, DOI [10.1109/icra40945.2020.9197385, 10.1109/ICRA40945.2020.9197385]
   Pham QH, 2019, PROC CVPR IEEE, P8819, DOI 10.1109/CVPR.2019.00903
   Qi CR, 2017, Arxiv, DOI [arXiv:1706.02413, DOI 10.48550/ARXIV.1706.02413]
   Rahaman H, 2019, HERITAGE-BASEL, V2, P1835, DOI 10.3390/heritage2030112
   Riegler G., 2016, P IEEE C COMPUTER VI, P3577, DOI [10.48550/ARXIV.1611.05009, DOI 10.48550/ARXIV.1611.05009]
   Rock J, 2015, PROC CVPR IEEE, P2484, DOI 10.1109/CVPR.2015.7298863
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rottensteiner F., 2012, ISPRS Ann Photogramm Rem Sens Spat Inform Sci; I-3, VI-3, P293, DOI 10.5194/isprsannals-I-3-293-2012
   Roynard X, 2018, INT J ROBOT RES, V37, P545, DOI 10.1177/0278364918767506
   Santurkar S, 2018, PICT COD SYMP, P258, DOI 10.1109/PCS.2018.8456298
   Sarkar K, 2017, INT CONF 3D VISION, P383, DOI 10.1109/3DV.2017.00051
   Sauder J, 2019, Arxiv, DOI arXiv:1901.08396
   Sedghi A, 2018, Arxiv, DOI [arXiv:1804.01565, 10.48550/ARXIV.1804.01565, DOI 10.48550/ARXIV.1804.01565]
   Serna Andres, 2014, 3rd International Conference on Pattern Recognition Applications and Methods (ICPRAM 2014). Proceedings, P819
   Shahzad M, 2012, INT GEOSCI REMOTE SE, P467, DOI 10.1109/IGARSS.2012.6351385
   Shi CH, 2022, ISPRS J PHOTOGRAMM, V184, P177, DOI 10.1016/j.isprsjprs.2021.12.011
   Shi YL, 2019, IEEE T GEOSCI REMOTE, V57, P3015, DOI 10.1109/TGRS.2018.2879382
   Siddiqi K, 2008, MACH VISION APPL, V19, P261, DOI 10.1007/s00138-007-0097-8
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Sindagi VA, 2019, IEEE INT CONF ROBOT, P7276, DOI [10.1109/ICRA.2019.8794195, 10.1109/icra.2019.8794195]
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Song X., 2018, P IEEECVF C COMPUTER, P5452, DOI [10.48550/ARXIV.1811.12222, DOI 10.48550/ARXIV.1811.12222]
   Song Y, 2018, J MAGN RESON IMAGING, V48, P1570, DOI 10.1002/jmri.26047
   Stephan M., 2021, Deep Learning Applications, 2: 173-197
   Stilla U, 2023, ISPRS J PHOTOGRAMM, V197, P228, DOI 10.1016/j.isprsjprs.2023.01.010
   Sun P, 2019, IEEECVF C COMPUTER V, P2446
   Taghizadeh M, 2022, EXPERT SYST APPL, V189, DOI 10.1016/j.eswa.2021.116105
   Tan WK, 2020, IEEE COMPUT SOC CONF, P797, DOI 10.1109/CVPRW50498.2020.00109
   Tchapmi LP, 2019, PROC CVPR IEEE, P383, DOI 10.1109/CVPR.2019.00047
   Tian YF, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13010066
   Tychola K.A., 2022, Digital, V2, P401, DOI [10.3390/digital2030022, DOI 10.3390/DIGITAL2030022]
   Uy MA, 2019, IEEE I CONF COMP VIS, P1588, DOI 10.1109/ICCV.2019.00167
   Vallet B, 2015, COMPUT GRAPH-UK, V49, P126, DOI 10.1016/j.cag.2015.03.004
   Varney N, 2020, IEEE COMPUT SOC CONF, P717, DOI 10.1109/CVPRW50498.2020.00101
   Vaswani A, 2017, ADV NEUR IN, V30
   Vayghan SS, 2022, GEOCARTO INT, V37, P2967, DOI 10.1080/10106049.2020.1844311
   Vinodkumar PK, 2023, ENTROPY-SWITZ, V25, DOI 10.3390/e25040635
   Viroli C, 2019, STAT COMPUT, V29, P43, DOI 10.1007/s11222-017-9793-z
   Wang Changshuo, 2024, IEEE Transactions on Circuits and Systems for Video Technology, V34, P4698, DOI 10.1109/TCSVT.2023.3328712
   Wang C., 2020, ISPRS Annals of the Photogrammetry. Remote Sensing and Spatial Information Sciences V-5-2020, V5, P117, DOI [10.5194/isprs-annals-V-5-2020-117-2020, DOI 10.5194/ISPRS-ANNALS-V-5-2020-117-2020]
   Wang C., 2022, Journal of Software, V34, P1962
   Wang CS, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3170493
   Wang CS, 2021, DISPLAYS, V70, DOI 10.1016/j.displa.2021.102080
   Wang C, 2019, Arxiv, DOI arXiv:1906.01592
   Wang F, 2019, IEEE T INSTRUM MEAS, V68, P2671, DOI 10.1109/TIM.2019.2906416
   Wang JQ, 2019, Arxiv, DOI [arXiv:1909.12037, 10.48550/ARXIV.1909.12037]
   Wang JQ, 2021, IEEE DATA COMPR CONF, P73, DOI 10.1109/DCC50243.2021.00015
   Wang L., 2023, IEEE Transacti. Wirel. Commun.
   Wang PY, 2018, COMPUT GRAPH-UK, V76, P182, DOI 10.1016/j.cag.2018.07.011
   Wang RS, 2018, IEEE J-STARS, V11, P606, DOI 10.1109/JSTARS.2017.2781132
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang WY, 2018, PROC CVPR IEEE, P2569, DOI 10.1109/CVPR.2018.00272
   Wang WY, 2017, IEEE I CONF COMP VIS, P2317, DOI 10.1109/ICCV.2017.252
   Wang X., 2021, P IEEECVF INT C COMP, P13189, DOI DOI 10.48550/ARXIV.2108.09936
   Wang XL, 2019, PROC CVPR IEEE, P4091, DOI 10.1109/CVPR.2019.00422
   Wang Y, 2022, Arxiv, DOI [arXiv:2202.11981, DOI 10.48550/ARXIV.2202.11981]
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Wang ZX, 2019, IEEE INT C INT ROBOT, P1742, DOI [10.1109/IROS40897.2019.8968513, 10.1109/iros40897.2019.8968513]
   Wang ZR, 2020, IEEE WINT CONF APPL, P91, DOI 10.1109/WACV45572.2020.9093302
   Wang ZJ, 2020, IEEE T VIS COMPUT GR, V26, P2919, DOI 10.1109/TVCG.2019.2896310
   Wen X., 2020, 2020 IEEE INT C MULT, P1
   Wen X., 2020, P IEEECVF C COMPUTER, P7443, DOI [10.48550/ARXIV.2012.03408, DOI 10.48550/ARXIV.2012.03408]
   Wen X, 2021, PROC CVPR IEEE, P13075, DOI 10.1109/CVPR46437.2021.01288
   Wiesmann L, 2021, IEEE ROBOT AUTOM LET, V6, P2060, DOI 10.1109/LRA.2021.3059633
   Wiley V., 2018, International Journal of Artificial Intelligence Research, V2, P29
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Wu YT, 2021, IEEE SENS J, V21, P1152, DOI 10.1109/JSEN.2020.3020626
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xiang HY, 2023, PATTERN RECOGN, V134, DOI 10.1016/j.patcog.2022.109046
   Xiao AR, 2023, IEEE T PATTERN ANAL, V45, P11321, DOI 10.1109/TPAMI.2023.3262786
   Xiao AR, 2024, Arxiv, DOI arXiv:2305.19812
   Xiao AR, 2022, AAAI CONF ARTIF INTE, P2795
   Xiao JX, 2013, IEEE I CONF COMP VIS, P1625, DOI 10.1109/ICCV.2013.458
   Xie YX, 2020, IEEE GEOSC REM SEN M, V8, P38, DOI 10.1109/MGRS.2019.2937630
   Xin Wang, 2020, IOP Conference Series: Earth and Environmental Science, V502, DOI 10.1088/1755-1315/502/1/012008
   Xue L, 2023, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR52729.2023.00120
   Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337
   Yan Z, 2020, AUTON ROBOT, V44, P147, DOI 10.1007/s10514-019-09883-y
   Yang C., 2016, P IEEE C COMPUTER VI, P6721, DOI [10.48550/ARXIV.1611.09969, DOI 10.48550/ARXIV.1611.09969]
   Yang J, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21186193
   Yang Z, 2019, IEEE I CONF COMP VIS, P7504, DOI 10.1109/ICCV.2019.00760
   Yang ZT, 2019, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2019.00204
   Yang Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9598, DOI 10.1109/CVPR42600.2020.00962
   Yasir SM, 2022, CMC-COMPUT MATER CON, V72, P5777, DOI 10.32604/cmc.2022.025909
   Ye XQ, 2018, LECT NOTES COMPUT SC, V11211, P415, DOI 10.1007/978-3-030-01234-2_25
   Yew ZJ, 2022, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR52688.2022.00656
   Yi L., 2016, P IEEE C COMPUTER VI, P2282, DOI [10.48550/ARXIV.1612.00606, DOI 10.48550/ARXIV.1612.00606]
   Yin KX, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661241
   You YN, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12152460
   Yu FG, 2019, PROC CVPR IEEE, P9483, DOI 10.1109/CVPR.2019.00972
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu XG, 2023, PROC CVPR IEEE, P9150, DOI 10.1109/CVPR52729.2023.00883
   Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088
   Yue XY, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P458, DOI 10.1145/3206025.3206080
   Zaheer M, 2018, Arxiv, DOI [arXiv:1703.06114, DOI 10.48550/ARXIV.1703.06114]
   Zamorski M, 2020, COMPUT VIS IMAGE UND, V193, DOI 10.1016/j.cviu.2020.102921
   Zarzar J, 2019, Arxiv, DOI arXiv:1911.12236
   Zarzar J, 2020, Arxiv, DOI [arXiv:1903.10168, 10.48550/ARXIV.1903.10168]
   Zeng A., 2016, P IEEE C COMPUTER VI, P1802, DOI [10.48550/ARXIV.1603.08182, DOI 10.48550/ARXIV.1603.08182]
   Zeng YM, 2018, IEEE ROBOT AUTOM LET, V3, P3434, DOI 10.1109/LRA.2018.2852843
   Zetong Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11037, DOI 10.1109/CVPR42600.2020.01105
   Zhang FH, 2020, IEEE INT CONF ROBOT, P9448, DOI [10.1109/ICRA40945.2020.9196622, 10.1109/icra40945.2020.9196622]
   Zhang GC, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21196455
   Zhang H, 2023, DISPLAYS, V79, DOI 10.1016/j.displa.2023.102456
   Zhang JY, 2019, IEEE ACCESS, V7, P179118, DOI 10.1109/ACCESS.2019.2958671
   Zhang JX, 2013, REMOTE SENS-BASEL, V5, P3749, DOI 10.3390/rs5083749
   Zhang K., 2019, arXiv, DOI DOI 10.48550/ARXIV.1904.10014
   Zhang W., 2020, Computer Vision-ECCV 2020, P512, DOI 10.1007/978-3-030-58595-2_31
   Zhang Y., 2020, Virtual Reality & Intelligent Hardware, V2, P222
   Zhao CQ, 2020, SCI CHINA TECHNOL SC, V63, P1612, DOI 10.1007/s11431-020-1582-8
   Zhao GP, 2018, MULTIMED TOOLS APPL, V77, P29589, DOI 10.1007/s11042-017-5320-7
   Zhao Y, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15082060
   Zhao Z., 2019, arXiv
   Zhou J, 2020, COMPUT GRAPH FORUM, V39, P309, DOI 10.1111/cgf.13804
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
   Zhou ZH, 2018, NATL SCI REV, V5, P44, DOI 10.1093/nsr/nwx106
   Zhu H, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19051191
   Zhu XX, 2012, IEEE T GEOSCI REMOTE, V50, P247, DOI 10.1109/TGRS.2011.2160183
   Zi Jian Yew, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11821, DOI 10.1109/CVPR42600.2020.01184
   Zollhöfer M, 2018, COMPUT GRAPH FORUM, V37, P625, DOI 10.1111/cgf.13386
   Zollhofer M., 2019, RGB D IMAGE ANAL PRO, P3, DOI DOI 10.1007/978-3-030-28603-3_1
NR 288
TC 0
Z9 0
U1 52
U2 52
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 29
PY 2024
DI 10.1007/s00371-023-03237-7
EA JAN 2024
PG 43
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GD0W6
UT WOS:001150619200002
OA hybrid
DA 2024-08-05
ER

PT J
AU Zhang, H
   Wang, YF
   Yang, YJ
AF Zhang, Han
   Wang, Yongfang
   Yang, Yingjie
TI Salient-aware multiple instance learning optimized network for weakly
   supervised object detection
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Weakly supervised learning; Object detection; Salient priors; Sample
   weighting
AB In recent years, weakly supervised object detection network has achieved great development. However, due to the lack of bounding box supervision, the framework based on multiple instance learning tends to activate a part of the object rather than the whole object, which severely affects the detection performance for nonrigid objects. To solve this problem, this paper uses traditional features and sample weighting to guide the network to focus on the whole rather than the part of the object. Especially, salient priors are introduced to provide coarse pseudo bounding boxes to assist network initialization and to explore more accurate features in conjunction with the multi-object search strategy. In addition, we design an area-guided sample weighting algorithm to optimize the network to search for objects from larger areas, which avoids local domination. Experiments on public datasets (PASCAL VOC2007, PASCAL VOC2012) show that the proposed algorithm outperforms several state-of-the-art models.
C1 [Zhang, Han; Wang, Yongfang; Yang, Yingjie] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai 200444, Peoples R China.
   [Zhang, Han; Wang, Yongfang; Yang, Yingjie] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
C3 Shanghai University; Shanghai University
RP Wang, YF (corresponding author), Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai 200444, Peoples R China.; Wang, YF (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
EM yfw@shu.edu.cn
RI Zhang, Xiaoxi/KBP-8753-2024; Chen, Jin/KBQ-0163-2024; WANG,
   YUHAO/KBB-0213-2024
OI Chen, Jin/0009-0005-5844-635X; 
FU Natural Science Foundation of China under Grant
FX No Statement Available
CR Bilen H, 2016, PROC CVPR IEEE, P2846, DOI 10.1109/CVPR.2016.311
   Chen YC, 2019, INT CONF ACOUST SPEE, P1907, DOI 10.1109/icassp.2019.8682756
   Diba A, 2017, PROC CVPR IEEE, P5131, DOI 10.1109/CVPR.2017.545
   Dong BW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2856, DOI 10.1109/ICCV48922.2021.00287
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Gao MF, 2018, LECT NOTES COMPUT SC, V11205, P155, DOI 10.1007/978-3-030-01246-5_10
   Gao Y, 2019, IEEE I CONF COMP VIS, P9833, DOI 10.1109/ICCV.2019.00993
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang Z., 2020, P ADV NEUR INF PROC, V33, P16797
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jin RB, 2022, KNOWL-BASED SYST, V237, DOI 10.1016/j.knosys.2021.107726
   Kim D, 2017, IEEE I CONF COMP VIS, P3554, DOI 10.1109/ICCV.2017.382
   Kosugi S, 2019, IEEE I CONF COMP VIS, P6063, DOI 10.1109/ICCV.2019.00616
   Li XY, 2019, IEEE I CONF COMP VIS, P9734, DOI 10.1109/ICCV.2019.00983
   Li YL, 2021, INT C PATT RECOG, P9628, DOI 10.1109/ICPR48806.2021.9412697
   Lin CH, 2020, AAAI CONF ARTIF INTE, V34, P11482
   Lu XK, 2022, IEEE T PATTERN ANAL, V44, P7885, DOI 10.1109/TPAMI.2021.3115815
   Lu XK, 2019, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2019.00374
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Qin ZY, 2023, IEEE-CAA J AUTOMATIC, V10, P1192, DOI 10.1109/JAS.2023.123456
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rochan M, 2016, IMAGE VISION COMPUT, V56, P1, DOI 10.1016/j.imavis.2016.08.015
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shen JB, 2022, IEEE T PATTERN ANAL, V44, P8896, DOI 10.1109/TPAMI.2021.3127492
   Shen Y., 2020, Advances in Neural Information Processing Systems, V33
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh KK, 2016, PROC CVPR IEEE, P3548, DOI 10.1109/CVPR.2016.386
   Tang P, 2018, LECT NOTES COMPUT SC, V11215, P370, DOI 10.1007/978-3-030-01252-6_22
   Tang P, 2020, IEEE T PATTERN ANAL, V42, P176, DOI 10.1109/TPAMI.2018.2876304
   Tang P, 2017, PROC CVPR IEEE, P3059, DOI 10.1109/CVPR.2017.326
   Tang YX, 2014, IEEE IMAGE PROC, P4072, DOI 10.1109/ICIP.2014.7025827
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wan F, 2019, PROC CVPR IEEE, P2194, DOI 10.1109/CVPR.2019.00230
   Wang JJ, 2018, Arxiv, DOI [arXiv:1802.03531, DOI 10.1109/JSTARS.2022.3223845]
   Wei J, 2021, PROC CVPR IEEE, P5989, DOI 10.1109/CVPR46437.2021.00593
   Yang K, 2019, IEEE I CONF COMP VIS, P8371, DOI 10.1109/iccv.2019.00846
   Yuan QS, 2021, IEEE ACCESS, V9, P104356, DOI 10.1109/ACCESS.2021.3099497
   Yuanyi Zhong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P615, DOI 10.1007/978-3-030-58574-7_37
   Yunhang Shen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P118, DOI 10.1007/978-3-030-58598-3_8
   Zeng ZY, 2019, IEEE I CONF COMP VIS, P8291, DOI 10.1109/ICCV.2019.00838
   Zhang DL, 2021, PATTERN ANAL APPL, V24, P283, DOI 10.1007/s10044-020-00893-6
   Zhang XP, 2018, PROC CVPR IEEE, P4262, DOI 10.1109/CVPR.2018.00448
   Zhang Z., 2022, Image Vis. Comput, V121
   Zhongzheng Ren, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10595, DOI 10.1109/CVPR42600.2020.01061
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 47
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JAN 18
PY 2024
DI 10.1007/s00371-023-03234-w
EA JAN 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FE3U7
UT WOS:001144055000003
DA 2024-08-05
ER

PT J
AU Wang, XP
   Wang, JZ
   Liu, Q
   Liu, M
AF Wang, Xueping
   Wang, Jiazheng
   Liu, Qi
   Liu, Min
TI Robust object recognition via context-driven reliability assessment
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Robust object recognition; Context-guided reliability assessment;
   Learning from noisy labels
ID LABEL NOISE; CLASSIFICATION
AB Robust recognition of objects is crucial across various applications, spanning from industrial automation to healthcare and security. Nevertheless, real-world datasets often introduce label noise, encoding inaccurate correlation patterns that compromise the performance of visual recognition systems. Recent approaches utilize the predicted label distributions of individual samples for noise verification and correction, potentially exacerbating the accumulation of noise. To tackle this concern, in this paper, we introduce the context-driven reliability assessment strategy (CRA) for object robust recognition, where the reliability of a candidate sample is re-evaluated and rectified by comparing it with its neighbors in feature space. Specifically, CRA is divided into two steps: (1) consistency-aware pure sample selection (CPS) for estimating and choosing reliable samples based on consistent relations with the candidate sample and its k-reciprocal neighbors; and (2) similarity-guided label noise refurbishment (SLR) for re-labeling noisy samples by penalizing the divergence of each example's prediction from a weighted combination of its neighbors' predictions, with weights determined by their similarity in feature space. Thereafter, auxiliary techniques are used to assist further model optimization. Extensive experiments on three commonly used benchmark datasets, i.e., CIFAR-10, CIFAR-100 and Webvision-\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$-$$\end{document}1.0, demonstrate that our proposed method considerably outperforms state-of-the-art methods.
C1 [Wang, Xueping] Hunan Normal Univ, Coll Informat Sci & Engn, 36 Lushan Rd, Changsha 410081, Hunan, Peoples R China.
   [Wang, Jiazheng; Liu, Qi; Liu, Min] Hunan Univ, Coll Elect & Informat Engn, Changsha 410082, Hunan, Peoples R China.
   [Wang, Jiazheng; Liu, Qi; Liu, Min] Natl Engn Res Ctr Robot Visual Percept & Control T, Lushan Rd, Changsha 410082, Hunan, Peoples R China.
C3 Hunan Normal University; Hunan University
RP Wang, JZ (corresponding author), Hunan Univ, Coll Elect & Informat Engn, Changsha 410082, Hunan, Peoples R China.; Wang, JZ (corresponding author), Natl Engn Res Ctr Robot Visual Percept & Control T, Lushan Rd, Changsha 410082, Hunan, Peoples R China.
EM wang_xueping@hnu.edu.cn; wjiazheng@hnu.edu.cn; lq0326@hnu.edu.cn;
   liu_min@hnu.edu.cn
FU National Natural Science Foundation of China [2022YFB3303800]; National
   Key Research and Development Program of China [62203168,62073126];
   National Natural Science Foundation of China [2023RC1048]; Science and
   Technology Innovation Program of Hunan Province [2023JJ40449,
   2024JJ3013]; Natural Science Foundation of Hunan Province [2022M721120];
   China Postdoctoral Science Foundation
FX This work was supported by the National Key Research and Development
   Program of China under Grant 2022YFB3303800, National Natural Science
   Foundation of China under Grant (62203168,62073126), Science and
   Technology Innovation Program of Hunan Province under Grant 2023RC1048,
   Natural Science Foundation of Hunan Province under Grant (2023JJ40449,
   2024JJ3013), and China Postdoctoral Science Foundation under Grant
   2022M721120.
CR Arazo E, 2019, PR MACH LEARN RES, V97
   Arpit D, 2017, PR MACH LEARN RES, V70
   Bahri D, 2020, PR MACH LEARN RES, V119
   Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020
   Dai L, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-23458-5
   Englesson E., 2021, arXiv
   Frénay B, 2014, IEEE T NEUR NET LEAR, V25, P845, DOI 10.1109/TNNLS.2013.2292894
   Han B., 2020, arXiv
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Iscen A, 2022, PROC CVPR IEEE, P4662, DOI 10.1109/CVPR52688.2022.00463
   Kaneda K, 2024, IEEE ROBOT AUTOM LET, V9, P2088, DOI 10.1109/LRA.2024.3352363
   Krizhevsky A., 2012, Learning multiple layers of features from tiny images, DOI DOI 10.1145/3079856.3080246
   Li J., 2020, INT C LEARN REPR
   Li JC, 2022, LECT NOTES COMPUT SC, V13684, P128, DOI 10.1007/978-3-031-20053-3_8
   Li JN, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9465, DOI 10.1109/ICCV48922.2021.00935
   Li SK, 2022, PROC CVPR IEEE, P316, DOI 10.1109/CVPR52688.2022.00041
   Li W, 2017, Arxiv, DOI arXiv:1708.02862
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu M, 2024, IEEE T PATTERN ANAL, V46, P4944, DOI 10.1109/TPAMI.2024.3361491
   Liu S, 2020, P NEUIPS DEC 6 12 VI
   Lu J, 2018, PR MACH LEARN RES, V80
   Luo X, 2022, IEEE ROBOT AUTOM LET, V7, P12419, DOI 10.1109/LRA.2022.3216997
   Ma ZY, 2011, IEEE T PATTERN ANAL, V33, P2160, DOI 10.1109/TPAMI.2011.63
   Malach E, 2017, ADV NEUR IN, V30
   Patrini G, 2017, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2017.240
   Permuter H, 2006, PATTERN RECOGN, V39, P695, DOI 10.1016/j.patcog.2005.10.028
   Reed S., 2015, P INT C LEARN REPR W
   Sheng B, 2022, IEEE T CYBERNETICS, V52, P6662, DOI 10.1109/TCYB.2021.3079311
   Sohn Kihyuk, 2020, P 34 INT C NEUR INF
   Song H, 2023, IEEE T NEUR NET LEAR, V34, P8135, DOI 10.1109/TNNLS.2022.3152527
   Tabatabaei SM, 2020, VISUAL COMPUT, V36, P967, DOI 10.1007/s00371-019-01704-8
   Tan XX, 2022, VISUAL COMPUT, DOI 10.1007/s00371-022-02686-w
   Tanaka D, 2018, PROC CVPR IEEE, P5552, DOI 10.1109/CVPR.2018.00582
   Wu ZF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P62, DOI 10.1109/ICCV48922.2021.00013
   Yao YZ, 2021, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR46437.2021.00515
   Yi K, 2019, PROC CVPR IEEE, P7010, DOI 10.1109/CVPR.2019.00718
   Yu XR, 2019, PR MACH LEARN RES, V97
   Zhang BX, 2020, IEEE T VIS COMPUT GR, V26, P2546, DOI 10.1109/TVCG.2019.2894627
   Zhu Zhaowei, 2021, INT C MACH LEARN, P12912
NR 39
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 18
PY 2024
DI 10.1007/s00371-024-03530-z
EA JUN 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UQ5T1
UT WOS:001249543600005
DA 2024-08-05
ER

PT J
AU Yu, ZX
   Tang, ZJ
   Liang, XP
   Zhang, HY
   Sun, RH
   Zhang, XQ
AF Yu, Zixuan
   Tang, Zhenjun
   Liang, Xiaoping
   Zhang, Hanyun
   Sun, Ronghai
   Zhang, Xianquan
TI A novel image hashing with low-rank sparse matrix decomposition and
   feature distance
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Low-rank sparse matrix decomposition; Hash code; Statistical features;
   Feature distance
ID TRUNCATED NUCLEAR NORM; RING PARTITION; ROBUST; MODEL
AB Image hashing is an efficient technique of image processing for various applications, such as retrieval, copy detection and authentication. In this paper, we design a novel image hashing algorithm using LRSMD (low-rank sparse matrix decomposition). Firstly, an input image is preprocessed by interpolation, Gaussian blur and color space conversion. Next, the preprocessed image is fed into the LRSMD for learning a low-rank matrix. Then, statistical features of non-overlapping blocks in the low-rank matrix are extracted. Finally, the hash code is obtained by calculating feature distances. Various experiments are done on public datasets to demonstrate the robustness and discrimination of the proposed algorithm. The results show that the proposed algorithm outperforms several advanced algorithms in balancing the performances of robustness and discrimination.
C1 [Yu, Zixuan; Tang, Zhenjun; Liang, Xiaoping; Zhang, Hanyun; Sun, Ronghai; Zhang, Xianquan] Guangxi Normal Univ, Key Lab Educ Blockchain & Intelligent Technol, Minist Educ, Guilin 541004, Peoples R China.
   [Yu, Zixuan; Tang, Zhenjun; Liang, Xiaoping; Zhang, Hanyun; Sun, Ronghai; Zhang, Xianquan] Guangxi Normal Univ, Guangxi Key Lab Multisource Informat Min & Secur, Guilin 541004, Peoples R China.
C3 Guangxi Normal University; Guangxi Normal University
RP Tang, ZJ (corresponding author), Guangxi Normal Univ, Key Lab Educ Blockchain & Intelligent Technol, Minist Educ, Guilin 541004, Peoples R China.; Tang, ZJ (corresponding author), Guangxi Normal Univ, Guangxi Key Lab Multisource Informat Min & Secur, Guilin 541004, Peoples R China.
EM tangzj230@163.com
OI Tang, Zhenjun/0000-0003-3664-1363
FU National Natural Science Foundation of China [2024GXNSFBA010191];
   Guangxi Natural Science Foundation [62272111, 62302108]; National
   Natural Science Foundation of China; Guangxi "Bagui Scholar" Team for
   Innovation and Research, Guangxi Talent Highland Project of Big Data
   Intelligence and Application, Guangxi Collaborative Innovation Center of
   Multi-source Information Integration and Intelligent Processing
   [YCBZ2023083, XYCBZ2024022]; Innovation Project of Guangxi Graduate
   Education
FX This work is partially supported by the Guangxi Natural Science
   Foundation (2024GXNSFBA010191), the National Natural Science Foundation
   of China (62272111, 62302108), Guangxi "Bagui Scholar" Team for
   Innovation and Research, Guangxi Talent Highland Project of Big Data
   Intelligence and Application, Guangxi Collaborative Innovation Center of
   Multi-source Information Integration and Intelligent Processing, and the
   Innovation Project of Guangxi Graduate Education (YCBZ2023083,
   XYCBZ2024022). Zhenjun Tang is the corresponding author. Many thanks to
   the referees for their helpful suggestions.
CR Davarzani R, 2016, MULTIMED TOOLS APPL, V75, P4639, DOI 10.1007/s11042-015-2496-6
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Franzen R., "Kodak lossless true color image suite
   Hu Y, 2013, IEEE T PATTERN ANAL, V35, P2117, DOI 10.1109/TPAMI.2012.271
   Huang X, 2016, IEEE TRUST BIG, P14, DOI [10.1109/TrustCom.2016.39, 10.1109/TrustCom.2016.0040]
   Huang ZQ, 2021, IEEE T CIRC SYST VID, V31, P2808, DOI 10.1109/TCSVT.2020.3027001
   Huang ZQ, 2021, IEEE T MULTIMEDIA, V23, P1516, DOI 10.1109/TMM.2020.2999188
   Karsh RK, 2023, MULTIMED TOOLS APPL, V82, P22083, DOI 10.1007/s11042-022-13349-2
   Khelifi F, 2010, IEEE T IMAGE PROCESS, V19, P981, DOI 10.1109/TIP.2009.2038637
   Li XR, 2022, IEEE T INF FOREN SEC, V17, P1404, DOI 10.1109/TIFS.2022.3161149
   Liang XP, 2022, IET IMAGE PROCESS, V16, P3225, DOI 10.1049/ipr2.12555
   Liang XP, 2021, MULTIMEDIA SYST, V27, P389, DOI 10.1007/s00530-020-00696-z
   Liu J, 2023, VISUAL COMPUT, V39, P3091, DOI 10.1007/s00371-022-02515-0
   Liu SG, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3355394
   Liu S, 2023, COMPUT J, V66, P2844, DOI 10.1093/comjnl/bxac127
   Lv XD, 2012, IEEE T INF FOREN SEC, V7, P1081, DOI 10.1109/TIFS.2012.2190594
   McParlane P.J., 2014, P 23 ACM INT C INF K, P1459
   Monga V, 2006, IEEE T IMAGE PROCESS, V15, P3452, DOI 10.1109/TIP.2006.881948
   Ouyang JL, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2978572
   Pun CM, 2017, IEEE T INF FOREN SEC, V12, P377, DOI 10.1109/TIFS.2016.2615272
   Qin C, 2021, IEEE T CIRC SYST VID, V31, P4523, DOI 10.1109/TCSVT.2020.3047142
   Qin C, 2018, SIGNAL PROCESS, V142, P194, DOI 10.1016/j.sigpro.2017.07.019
   Schaefer G, 2004, PROC SPIE, V5307, P472, DOI 10.1117/12.525375
   Schneider M, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P227, DOI 10.1109/ICIP.1996.560425
   Swaminathan A, 2006, IEEE T INF FOREN SEC, V1, P215, DOI 10.1109/TIFS.2006.873601
   Tang ZJ, 2020, WIREL COMMUN MOB COM, V2020, DOI 10.1155/2020/8870467
   Tang ZJ, 2021, COMPUT J, V64, P1656, DOI 10.1093/comjnl/bxz127
   Tang ZJ, 2019, MATH BIOSCI ENG, V16, P6103, DOI 10.3934/mbe.2019305
   Tang ZJ, 2019, IEEE T KNOWL DATA EN, V31, P549, DOI 10.1109/TKDE.2018.2837745
   Tang ZJ, 2016, IEEE T INF FOREN SEC, V11, P200, DOI 10.1109/TIFS.2015.2485163
   Tang ZJ, 2014, IET IMAGE PROCESS, V8, P142, DOI 10.1049/iet-ipr.2013.0332
   Tang ZJ, 2014, IEEE T KNOWL DATA EN, V26, P711, DOI 10.1109/TKDE.2013.45
   Vaidya SP, 2023, VISUAL COMPUT, V39, P2245, DOI 10.1007/s00371-022-02406-4
   Wang XF, 2015, IEEE T INF FOREN SEC, V10, P1336, DOI 10.1109/TIFS.2015.2407698
   Wu DY, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P155, DOI 10.1145/3078971.3078989
   Xue ZC, 2019, VISUAL COMPUT, V35, P1549, DOI 10.1007/s00371-018-1555-1
   Yan CP, 2017, IEEE T INF FOREN SEC, V12, P2144, DOI 10.1109/TIFS.2017.2699942
   Yan CP, 2016, SIGNAL PROCESS, V121, P1, DOI 10.1016/j.sigpro.2015.10.027
   Yu MZ, 2023, COMPUT J, V66, P1241, DOI 10.1093/comjnl/bxac010
   Zhao Y., 2023, J. Electron. Imaging, V32
   Zhao Y, 2013, IEEE T INF FOREN SEC, V8, P55, DOI 10.1109/TIFS.2012.2223680
NR 42
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 JUN 13
PY 2024
DI 10.1007/s00371-024-03517-w
EA JUN 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UF3N2
UT WOS:001246607000001
DA 2024-08-05
ER

PT J
AU Saillant, B
   Zara, F
   Damiand, G
   Jaillet, F
AF Saillant, Bastien
   Zara, Florence
   Damiand, Guillaume
   Jaillet, Fabrice
TI High-order elements in position-based dynamics
SO VISUAL COMPUTER
LA English
DT Article
DE Physical based simulation; Position-based dynamics; High-order elements;
   Elasticity; Real-time physics
ID SIMULATION
AB The simulation of deformable objects has been the subject of a great deal of work in the field of computer graphics. The constraint-based PBD (Position-Based Dynamics) approach has been proven to be effective in this field for real-time and stable deformable objects simulation. Finite element method with linear tetrahedron discretization is the most widely used in computer graphics despite producing less accurate results than hexahedral or higher-order elements. In this context, our proposal is to integrate higher degree elements within the pbd framework. In addition, we propose a solution to improve convergence of unstable energies (like Neo-Hooke) when used as constraints. We show that our approach improves accuracy compared to linear tetrahedra. We also highlight the time savings, since fewer elements are needed.
C1 [Saillant, Bastien; Zara, Florence; Damiand, Guillaume] UCBL, CNRS, INSA Lyon,LIRIS, UMR5205, F-69622 Villeurbanne, France.
   [Jaillet, Fabrice] UCBL, INSA Lyon, UMR5205, IUT Lyon 1,CNRS,LIRIS, F-69622 Villeurbanne, France.
C3 Universite Claude Bernard Lyon 1; Institut National des Sciences
   Appliquees de Lyon - INSA Lyon; Centre National de la Recherche
   Scientifique (CNRS); Universite Claude Bernard Lyon 1; Centre National
   de la Recherche Scientifique (CNRS); Institut National des Sciences
   Appliquees de Lyon - INSA Lyon
RP Saillant, B (corresponding author), UCBL, CNRS, INSA Lyon,LIRIS, UMR5205, F-69622 Villeurbanne, France.
EM bastien.saillant@liris.cnrs.fr; florence.zara@liris.cnrs.fr;
   guillaume.damiand@liris.cnrs.fr; fabrice.jaillet@liris.cnrs.fr
OI JAILLET, Fabrice/0000-0002-7330-8116; Saillant,
   Bastien/0000-0002-8215-9984; Damiand, Guillaume/0000-0003-1580-5517
CR Adagolodjo Y, 2019, IEEE T ROBOT, V35, P697, DOI 10.1109/TRO.2019.2897858
   Ahrens J., 2005, Visualization handbook, P717, DOI DOI 10.1016/B978-012387582-2/50038-1
   Andrews S., 2022, ACM SIGGRAPH 2022 CO, DOI [10.1145/3532720.3535640, DOI 10.1145/3532720.3535640]
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Bender J., 2017, P EUR ASS COMP GRAPH, P1, DOI 10.2312/egt.20171034
   Bender J, 2014, COMPUT GRAPH-UK, V44, P1, DOI 10.1016/j.cag.2014.07.004
   Berndt I, 2017, IEEE COMPUT GRAPH, V37, P24, DOI 10.1109/MCG.2017.45
   Bouaziz S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601116
   Chentanez N, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531394
   Courtecuisse H, 2014, MED IMAGE ANAL, V18, P394, DOI 10.1016/j.media.2013.11.001
   Duriez C, 2009, LECT NOTES COMPUT SC, V5762, P291, DOI 10.1007/978-3-642-04271-3_36
   Erez T, 2015, IEEE INT CONF ROBOT, P4397, DOI 10.1109/ICRA.2015.7139807
   Ferguson Z, 2023, PROCEEDINGS OF SIGGRAPH 2023 CONFERENCE PAPERS, SIGGRAPH 2023, DOI 10.1145/3588432.3591488
   Francu M., 2017, P WORKSHOP VIRTUAL R, DOI [10.2312/vriphys.20171083, DOI 10.2312/VRIPHYS.20171083]
   Frâncu M, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3444949
   Golec K, 2020, VISUAL COMPUT, V36, P809, DOI 10.1007/s00371-019-01663-0
   Jiang ZS, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459840
   Kim T., 2022, ACM SIGGRAPH 2022 CO, DOI 10.1145/3532720.3535628
   Le Q., 2023, SIGGRAPH AS 2023 C S, DOI [10.1145/3610548.3618186, DOI 10.1145/3610548.3618186]
   Liu TT, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2990496
   Macklin Miles, 2021, MIG '21: Motion, Interaction and Games, DOI 10.1145/3487983.3488289
   Macklin M, 2019, PROCEEDINGS SCA 2019: ACM SIGGRAPH/EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION, DOI 10.1145/3309486.3340247
   Macklin M, 2014, ACM T GRAPHIC, V33, DOI [10.1145/280/109/2601152, 10.1145/2601097.2601152]
   Macklin Miles, 2016, P 9 INT C MOTION GAM, P49, DOI 10.1145/2994258.2994272
   Müller M, 2007, J VIS COMMUN IMAGE R, V18, P109, DOI 10.1016/j.jvcir.2007.01.005
   Muller Matthias., 2008, ACM SIGGRAPH 2008 CL, P1, DOI [DOI 10.1145/1401132.1401245, 10.1145/1401132.1401245]
   Nesme M, 2006, LECT NOTES COMPUT SC, V4072, P40
   Nesme M, 2005, REC RES DEV BIOMECH, V2, P117
   Perrusi P.H.S., 2021, EUR 2021 THE 42 ANN, DOI [10.2312/egs.20211020, DOI 10.2312/EGS.20211020]
   Picinbono G, 2001, IEEE INT CONF ROBOT, P1370, DOI 10.1109/ROBOT.2001.932801
   San-Vicente G, 2012, IEEE T VIS COMPUT GR, V18, P228, DOI 10.1109/TVCG.2011.32
   Schneider T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3508372
   Schneider T, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275067
   Servin M., 2006, P SIGRAD
   Smith Breannan, 2018, ACM Transactions on Graphics, V37, DOI 10.1145/3180491
   Stamp J, 2009, 2009 IEEE/PES POWER SYSTEMS CONFERENCE AND EXPOSITION, VOLS 1-3, P811
   Terzopoulos D., 1987, Computer Graphics, V21, P205, DOI 10.1145/37402.37427
   Tournier M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766969
   Wu XL, 2001, COMPUT GRAPH FORUM, V20, pC349
NR 39
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4737
EP 4749
DI 10.1007/s00371-024-03467-3
EA JUN 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001242180100004
DA 2024-08-05
ER

PT J
AU Kanai, T
   Endo, Y
   Kanamori, Y
AF Kanai, Toshiki
   Endo, Yuki
   Kanamori, Yoshihiro
TI Seasonal terrain texture synthesis via Köppen periodic conditioning
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; GAN; Texture synthesis; Terrain
AB This paper presents the first method for synthesizing seasonal transition of terrain textures for an input heightfield. Our method reproduces a seamless transition of terrain textures according to the seasons by learning measured data on the earth using a convolutional neural network. We attribute the main seasonal texture transition to vegetation and snow, and control the texture synthesis not only with the input heightfield but also with the annual temperature and precipitation based on K & ouml;ppen's climate classification as well as insolation at the location. We found that month-by-month synthesis yields incoherent transitions, while a na & iuml;ve conditioning with explicit temporal information (e.g., month) degrades generalizability due to the north-south hemisphere difference. To address these issues, we introduce a simple solution-periodic conditioning on the annual data without explicit temporal information. Our experiments reveal that our method can synthesize plausible seasonal transitions of terrain textures. We also demonstrate large-scale texture synthesis by tiling the texture output.
C1 [Kanai, Toshiki; Endo, Yuki; Kanamori, Yoshihiro] Univ Tsukuba, Tsukuba, Japan.
C3 University of Tsukuba
RP Kanai, T (corresponding author), Univ Tsukuba, Tsukuba, Japan.
EM kanai.toshiki.as@alumni.tsukuba.ac.jp; endo@cs.tsukuba.ac.jp;
   kanamori@cs.tsukuba.ac.jp
CR Agency E.S., 2023, SENTINEL 2 MSI LEVEL
   Argudo O, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417855
   Argudo O, 2017, VISUAL COMPUT, V33, P1005, DOI 10.1007/s00371-017-1393-6
   Benes B, 2002, COMP ANIM CONF PROC, P33, DOI 10.1109/CA.2002.1017504
   Benes Bedrich., 2009, Eurographics Workshop on Natural Phenomena, P9
   Braaten J., 2020, SENTINEL 2 CLOUD MAS
   Brown S.A., 2017, NONPHOTOREALISTIC AN
   Chen D, 2013, ENVIRON DEV, V6, P69, DOI 10.1016/j.envdev.2013.03.007
   Cordonnier G, 2018, COMPUT GRAPH FORUM, V37, P497, DOI 10.1111/cgf.13379
   Cordonnier G, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592422
   Dachsbacher C., 2006, P VIS MOD VIS, P105
   Foldes D., 2007, P 4 WORKSH VRIPHYS, P35
   Galin E, 2019, COMPUT GRAPH FORUM, V38, P553, DOI 10.1111/cgf.13657
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gorelick N, 2017, REMOTE SENS ENVIRON, V202, P18, DOI 10.1016/j.rse.2017.06.031
   Greene N., 1989, Computer Graphics, V23, P175, DOI 10.1145/74334.74351
   Grenier C, 2024, COMPUT GRAPH FORUM, V43, DOI 10.1111/cgf.14992
   Guérin E, 2022, COMPUT GRAPH FORUM, V41, P85, DOI 10.1111/cgf.14460
   Guérin E, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130804
   Hädrich T, 2017, COMPUT GRAPH FORUM, V36, P49, DOI 10.1111/cgf.13106
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Lochner J, 2023, COMPUT GRAPH FORUM, V42, DOI 10.1111/cgf.14941
   Makowski M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323039
   Mallya A., 2020, ECCV
   Mech R., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P397, DOI 10.1145/237170.237279
   NASA JPL, 2020, NASADEM MERGED GLOBA, DOI [10.5067/MEaSUREs/NASADEM/NASADEM_HGT.001, DOI 10.5067/MEASURES/NASADEM/NASADEM_HGT.001, 10.5067/MEASURES/NASADEM/NASADEM_HGT.001]
   Palubicki W, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530146
   Panagiotou E., 2020, WORKSH 11 EETN C ART, V2844, P9
   Pang YX, 2022, IEEE T MULTIMEDIA, V24, P3859, DOI 10.1109/TMM.2021.3109419
   Park T., 2020, Advances in Neural Information Processing Systems, V33, P7198
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Peel MC, 2007, HYDROL EARTH SYST SC, V11, P1633, DOI 10.5194/hess-11-1633-2007
   Perche S, 2023, COMPUT GRAPH FORUM, V42, DOI 10.1111/cgf.14936
   Pirk S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661252
   Premoze S., 2002, 3 ICA MOUNT CART WOR
   Richter SR, 2023, IEEE T PATTERN ANAL, V45, P1700, DOI 10.1109/TPAMI.2022.3166687
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saharia C., 2022, ACM SIGGRAPH 2022 C, P1
   Schott H, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592787
   Spick R.J., 2019, EUR C VIS MED PROD, V3, P1
   Takahashi H, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2119
   Zhao YW, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356553
   Zhu JL, 2021, COMPUT GRAPH FORUM, V40, P193, DOI 10.1111/cgf.14413
   Zhu JY, 2017, ADV NEUR IN, V30
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 46
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2024
VL 40
IS 7
SI SI
BP 4857
EP 4868
DI 10.1007/s00371-024-03485-1
EA MAY 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN6J1
UT WOS:001235494300001
DA 2024-08-05
ER

PT J
AU Ding, HH
   Lin, C
   Li, FZ
   Pan, YC
AF Ding, Haihua
   Lin, Chuan
   Li, Fuzhang
   Pan, Yongcai
TI A feature aggregation network for contour detection inspired by complex
   cells properties
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Contour detection; Deep learning; Convolutional neural network; Complex
   cells
ID REFINEMENT NETWORK; RECEPTIVE FIELDS; BOUNDARIES
AB Biological vision mechanism plays a very important role in detecting object contour information, so the research of bio-inspired contour detection has attracted widespread attention. In the visual physiological mechanism of contour detection, complex cells in the primary visual cortex (V1) receive the target edges output by simple cells, and subsequently aggregate and integrate the edges into contours. Inspired by the receptive field properties of complex cells in the V1 area, we design a contour detection network with an encoder-decoder structure, called the clustered suppression network. In the encoding network, we propose a feature aggregation encoding framework inspired by the clustered distribution properties of complex cells. At the same time, based on the inhibition mechanism properties of the non-classical receptive field of complex cells and the response properties to motion edges, we propose a suppression module and a spatial motion detection module, which are integrated into the encoding network. In the decoding network, we propose a highly aggregated decoding network inspired by the horizontal connection properties of complex cells. Choosing the BSDS500 natural scene dataset as the experimental object, the F-score is selected as the evaluation index. The average optimal F-score on a single-scale of the proposed method is 0.802. Concurrently, the NYUD-v2 dataset and BIPED dataset are used for further verification and achieved good results. The codes are available at https://github.com/smallsunq/CSNet-Contour-detection.
C1 [Ding, Haihua; Lin, Chuan; Li, Fuzhang; Pan, Yongcai] Guangxi Univ Sci & Technol, Sch Automat, Liuzhou 545006, Peoples R China.
C3 Guangxi University of Science & Technology
RP Lin, C (corresponding author), Guangxi Univ Sci & Technol, Sch Automat, Liuzhou 545006, Peoples R China.
EM chuanlin@gxust.edu.cn
FU National Natural Science Foundation of China [61866002]; National
   Natural Science Foundation of China [2018GXNSFAA138122,
   2015GXNSFAA139293]; Guangxi Natural Science Foundation [2022GXZDSY013];
   Project of the Key Laboratory of Al and Information Processing (Hechi
   University), Education Department of Guangxi Zhuang Autonomous Region
FX The authors appreciate the anonymous reviewers for their helpful and
   constructive comments on an earlier draft of this paper. This work was
   supported by the National Natural Science Foundation of China (Grant
   No.62266006), Guangxi Natural Science Foundation (Grant
   No.2020GXNSFDA297006), National Natural Science Foundation of China
   (Grant No. 61866002), Project of the Key Laboratory of Al and
   Information Processing (Hechi University), Education Department of
   Guangxi Zhuang Autonomous Region (2022GXZDSY013), and Guangxi Natural
   Science Foundation (Grant No. 2018GXNSFAA138122, Grant No.
   2015GXNSFAA139293).
CR Akbarinia A, 2018, INT J COMPUT VISION, V126, P1367, DOI 10.1007/s11263-017-1035-5
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bao H, 2024, VISUAL COMPUT, V40, P4183, DOI 10.1007/s00371-023-03076-6
   Bertasius G, 2015, IEEE I CONF COMP VIS, P504, DOI 10.1109/ICCV.2015.65
   Bertasius G, 2015, PROC CVPR IEEE, P4380, DOI 10.1109/CVPR.2015.7299067
   Braitenberg V., 1998, Cortex: Statistics and Geometry of Neuronal Connectivity, P205
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Cao YJ, 2021, IEEE T MULTIMEDIA, V23, P761, DOI 10.1109/TMM.2020.2987685
   Chen ZH, 2021, VISUAL COMPUT, V37, P2657, DOI 10.1007/s00371-021-02199-y
   Deng RX, 2018, LECT NOTES COMPUT SC, V11210, P570, DOI 10.1007/978-3-030-01231-1_35
   Dollár P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715
   Duda R.O., 2000, Pattern Classification and Scene Analysis
   Ferrari V, 2008, IEEE T PATTERN ANAL, V30, P36, DOI 10.1109/TPAMI.2007.1144
   Gabor Dennis, 1946, Journal IEE, V93, P429
   Ganin Y, 2015, LECT NOTES COMPUT SC, V9004, P536, DOI 10.1007/978-3-319-16808-1_36
   Grigorescu C, 2004, IMAGE VISION COMPUT, V22, P609, DOI 10.1016/j.imavis.2003.12.004
   Grigorescu C, 2002, LECT NOTES COMPUT SC, V2525, P50
   Grigorescu C, 2003, IEEE T IMAGE PROCESS, V12, P729, DOI 10.1109/TIP.2003.814250
   Guo MH, 2022, Arxiv, DOI arXiv:2209.08575
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   Gupta S, 2013, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2013.79
   Hallman S, 2015, PROC CVPR IEEE, P1732, DOI 10.1109/CVPR.2015.7298782
   He JZ, 2019, PROC CVPR IEEE, P3823, DOI 10.1109/CVPR.2019.00395
   Huang Y., 2023, Vis. Comput., P1
   Huang YJ, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2051
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308
   Khan A, 2020, ARTIF INTELL REV, V53, P5455, DOI 10.1007/s10462-020-09825-6
   Kivinen JJ, 2014, JMLR WORKSH CONF PRO, V33, P512
   Lin CA, 2022, APPL INTELL, V52, P11027, DOI 10.1007/s10489-022-03202-2
   Lin C, 2020, NEUROCOMPUTING, V409, P361, DOI 10.1016/j.neucom.2020.06.069
   Liu Y, 2016, PROC CVPR IEEE, P231, DOI 10.1109/CVPR.2016.32
   Liu Y, 2019, IEEE T PATTERN ANAL, V41, P1939, DOI 10.1109/TPAMI.2018.2878849
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Maninis KK, 2018, IEEE T PATTERN ANAL, V40, P819, DOI 10.1109/TPAMI.2017.2700300
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Mnih V, 2014, ADV NEUR IN, V27
   Mottaghi R, 2014, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2014.119
   Nicholls JG., 2001, From Neuron to Brain
   Paszke A, 2019, ADV NEUR IN, V32
   POGGIO T, 1976, Quarterly Reviews of Biophysics, V9, P377
   Prewitt J.M., 1970, Pict. Process. Psychopictorics, V10, P15
   REICHARDT W, 1976, Q REV BIOPHYS, V9, P311, DOI 10.1017/S0033583500002523
   Sharma H, 2020, PROCEEDINGS OF THE CONFLUENCE 2020: 10TH INTERNATIONAL CONFERENCE ON CLOUD COMPUTING, DATA SCIENCE & ENGINEERING, P227, DOI [10.1109/confluence47617.2020.9057809, 10.1109/Confluence47617.2020.9057809]
   Shen W, 2015, PROC CVPR IEEE, P3982, DOI 10.1109/CVPR.2015.7299024
   Shou T., 1997, Brain Mechanisms of Visual Information Processing
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Soria X, 2020, IEEE WINT CONF APPL, P1912, DOI 10.1109/WACV45572.2020.9093290
   Su Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5097, DOI 10.1109/ICCV48922.2021.00507
   Tang QL, 2007, PATTERN RECOGN, V40, P3100, DOI 10.1016/j.patcog.2007.02.009
   Tang QL, 2020, IEEE T IMAGE PROCESS, V29, P1192, DOI 10.1109/TIP.2019.2940690
   Wang GT, 2022, AAAI CONF ARTIF INTE, P2423
   Wang YP, 2017, PROC CVPR IEEE, P1724, DOI 10.1109/CVPR.2017.187
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1007/s11263-017-1004-z, 10.1109/ICCV.2015.164]
   Yang JM, 2016, PROC CVPR IEEE, P193, DOI 10.1109/CVPR.2016.28
   Yang KF, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2425538
   Yuan X., 2023, The Visual Computer, P1
   Zhang Q, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107657
NR 60
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 21
PY 2024
DI 10.1007/s00371-024-03460-w
EA MAY 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RO3K3
UT WOS:001228562800001
DA 2024-08-05
ER

PT J
AU Wei, L
   Freris, NM
AF Wei, Lan
   Freris, Nikolaos M.
TI Multi-scale graph neural network for physics-informed fluid simulation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Particle-based fluid simulation; Graph neural networks (GNNs);
   Physics-informed neural networks (PINNs)
AB Learning-based fluid simulation has proliferated due to its ability to replicate the dynamics with substantial computational savings over traditional numerical solvers. To this end, graph neural networks (GNNs) are a suitable tool to capture fluid dynamics through local particle interactions. Nonetheless, it remains challenging to model the long-range behaviors. To tackle this, this paper models the fluid flow via graphs at different scales in succinct considerability and physical constraints. We propose a novel multi-scale GNN for physics-informed fluid simulation (MSG) by introducing a nonparametric sampling and aggregation method to combine features from graphs with different resolutions. Our design reduces the size of the learnable model and accelerates the model inference time. In addition, zero velocity divergence is explicitly incorporated as a physical constraint through the training loss function. Finally, a fusion mechanism of consecutive predictions is incorporated to alleviate the inductive bias caused by the Markovian assumption. Extensive experiments corroborate the merits over leading particle-based neural network models in terms of both one-step accuracy (+6.7%)\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$(+ 6.7\%)$$\end{document} and long trajectory prediction (+16.9%)\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$(+ 16.9\%)$$\end{document}. This comes with a run-time reduction by 2.8%\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$2.8\%$$\end{document} over the best baseline method.
C1 [Wei, Lan; Freris, Nikolaos M.] Univ Sci & Technol China, Sch Comp Sci & Technol, 443 Huangshan Rd, Hefei 230027, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Freris, NM (corresponding author), Univ Sci & Technol China, Sch Comp Sci & Technol, 443 Huangshan Rd, Hefei 230027, Anhui, Peoples R China.
EM weilan@mail.ustc.edu.cn; nfr@ustc.edu.cn
CR Battaglia PW, 2016, ADV NEUR IN, V29
   Belbute-Peres FD, 2020, PR MACH LEARN RES, V119
   Bridson R., 2015, FLUID SIMULATION COM, DOI [10.1201/9781315266008, DOI 10.1201/9781315266008]
   Cao YD, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3519595
   Chentanez N, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964977
   Chu JY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3092818
   Fey M, 2019, Arxiv, DOI [arXiv:1903.02428, DOI 10.48550/ARXIV.1903.02428]
   Foster N, 1997, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P178, DOI 10.1109/CGI.1997.601299
   Gao H, 2021, J COMPUT PHYS, V428, DOI 10.1016/j.jcp.2020.110079
   Gao M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275044
   Jiang C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766996
   Keramat A, 2012, J FLUID STRUCT, V28, P434, DOI 10.1016/j.jfluidstructs.2011.11.001
   Kim B, 2019, COMPUT GRAPH FORUM, V38, P59, DOI 10.1111/cgf.13619
   Kingma D. P., 2014, arXiv
   Krishnamurthy N, 2008, ADV TOP SCI TECH CHI, P1, DOI 10.1007/978-3-540-73764-3_1
   Ladicky L, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818129
   Li QM, 2018, AAAI CONF ARTIF INTE, P3538
   Li ZJ, 2022, COMPUT GRAPH-UK, V103, P201, DOI 10.1016/j.cag.2022.02.004
   Lino M., 2022, arXiv
   Liu Q, 2022, Arxiv, DOI arXiv:2202.12619
   Macklin M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461984
   Molina-Aiz FD, 2004, AGR FOREST METEOROL, V125, P33, DOI 10.1016/j.agrformet.2004.03.009
   MONAGHAN JJ, 1992, ANNU REV ASTRON ASTR, V30, P543, DOI 10.1146/annurev.aa.30.090192.002551
   Müller M, 2007, J VIS COMMUN IMAGE R, V18, P109, DOI 10.1016/j.jvcir.2007.01.005
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Obiols-Sales O., 2020, P 34 ACM INT C SUP, P1, DOI 10.1145/3392717.3392772
   Oono K., 2019, arXiv, DOI DOI 10.48550/ARXIV.1905.10947
   Paszke A, 2019, ADV NEUR IN, V32
   Pfaff T, 2021, Arxiv, DOI [arXiv:2010.03409, DOI 10.48550/ARXIV.2010.03409]
   Sanchez-Gonzalez A., 2020, P 37 INT C MACHINE L, P8459, DOI DOI 10.48550/ARXIV.2002.09405
   Shao H, 2021, Advances in Neural Information Processing Systems, V34
   Snoek J., 2012, ADV NEURAL INFORM PR, DOI DOI 10.48550/ARXIV.1206.2944
   Tessendorf J., 2001, SIGGRAPH, V1, P5
   Thomas J.W, 2013, Numerical partial differential equations: finite difference methods
   Tompson J, 2017, PR MACH LEARN RES, V70
   Ummenhofer B, 2020, 8 INT C LEARN REPR I
   Wiewel S, 2020, COMPUT GRAPH FORUM, V39, P15, DOI 10.1111/cgf.14097
   Xiao XY, 2020, IEEE T VIS COMPUT GR, V26, P1454, DOI 10.1109/TVCG.2018.2873375
   Zhou K., 2020, Adv Neural Inf Process Syst, V33, P4917
NR 39
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAY 17
PY 2024
DI 10.1007/s00371-024-03402-6
EA MAY 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RH1G1
UT WOS:001226676900001
DA 2024-08-05
ER

PT J
AU Yao, P
   Sang, HW
   Cheng, X
AF Yao, Peng
   Sang, Haiwei
   Cheng, Xu
TI Structured support vector machine with coarse-to-fine PatchMatch
   filtering for stereo matching
SO VISUAL COMPUTER
LA English
DT Article
DE Stereo matching; Structured support vector machine; Coarse-to-fine;
   PatchMatch filtering
ID SCALE COST AGGREGATION
AB In the past decades, a variety of learning-based algorithms have been emerged to try to explore a better solution for stereo matching by leveraging various machine learning algorithms. For enriching learning-based stereo matching algorithm's methodologies, we cast the disparity estimation as a regression problem by leveraging Structured Support Vector Machine (SSVM) in this paper. There are three categories of features have been extracted on account of disparity cues for training the SSVM. Particularly, one of the three feature is named as 'Coarse-to-Fine PatchMatch Filtering', which effectively exploits region and pixel disparity cues. For attaining region disparity cues, we adopt MeshStereo and MeshStereo with Cross-Scale algorithms; for attaining pixel disparity cues, PatchMatch and Cross-Scale PatchMatch stereo matching algorithms are utilized. Performance evaluations on Middlebury v.2 and v.3 stereo data sets demonstrate that the proposed algorithm reveals comparable accuracy with other challenging learning-based ones. It is worth pointing out that our proposal performs over several orders of magnitude faster than others on training time.
C1 [Yao, Peng] Tianjin Univ Commerce, Sch Informat Engn, Tianjin 300134, Peoples R China.
   [Sang, Haiwei] Guizhou Educ Univ, Sch Math & Big Data, Guiyang 550018, Peoples R China.
   [Cheng, Xu] Smart Innovat Norway, N-1776 Halden, Norway.
C3 Tianjin University of Commerce; Guizhou Education University
RP Sang, HW (corresponding author), Guizhou Educ Univ, Sch Math & Big Data, Guiyang 550018, Peoples R China.
EM yaopeng@tjcu.edu.cn; haiwei.sang@gmail.com; xu.cheng@ieee.org
FU Guizhou Provincial Science and Technology Projects [[2019]2390]; Guizhou
   Provincial BasicResearch Program [652]; Science and Technology Program
   of GuiYang [ZK[2024]1-2]
FX This research is supported by Guizhou Provincial Science and Technology
   Projects (grants: [2019]2390), Guizhou Provincial BasicResearch Program
   (Natural Science): ZK[2024] (General 652) and Science and Technology
   Program of GuiYang(ZK[2024]1-2).
CR Batsos K, 2018, PROC CVPR IEEE, P2060, DOI 10.1109/CVPR.2018.00220
   Batsos K, 2018, INT CONF 3D VISION, P238, DOI 10.1109/3DV.2018.00036
   Bleyer M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.14
   Bleyer M, 2013, ADV COMPUT VIS PATT, P143, DOI 10.1007/978-1-4471-5520-1_6
   Chen DM, 2015, IEEE T CIRC SYST VID, V25, P730, DOI 10.1109/TCSVT.2014.2361422
   Drory A, 2014, LECT NOTES COMPUT SC, V8753, P43, DOI 10.1007/978-3-319-11752-2_4
   Facciolo G., 2015, Procedings of the British Machine Vision Conference 2015 (Swansea: British Machine Vision Association), p90.1, DOI DOI 10.5244/C.29.90
   Gidaris S, 2017, PROC CVPR IEEE, P7187, DOI 10.1109/CVPR.2017.760
   Haeusler R, 2013, PROC CVPR IEEE, P305, DOI 10.1109/CVPR.2013.46
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI [10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166]
   Hirschmüller H, 2005, PROC CVPR IEEE, P807, DOI 10.1109/cvpr.2005.56
   Hosni A, 2013, IEEE T PATTERN ANAL, V35, P504, DOI 10.1109/TPAMI.2012.156
   Kendall Alex, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P66, DOI 10.1109/ICCV.2017.17
   Kerkaou Z, 2020, MULTIMED TOOLS APPL, V79, P27039, DOI 10.1007/s11042-020-09260-3
   Li Yanjun, 2008, PLoS One, V3, pe2166, DOI 10.1371/journal.pone.0002166
   Liang ZF, 2018, PROC CVPR IEEE, P2811, DOI 10.1109/CVPR.2018.00297
   Lin YH, 2012, INT CONF ACOUST SPEE, P809, DOI 10.1109/ICASSP.2012.6288007
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Mei X, 2013, PROC CVPR IEEE, P313, DOI 10.1109/CVPR.2013.47
   middlebury, 2022, US
   openjump, 2022, about us
   Pang JH, 2017, IEEE INT CONF COMP V, P878, DOI 10.1109/ICCVW.2017.108
   Park MG, 2015, PROC CVPR IEEE, P101, DOI 10.1109/CVPR.2015.7298605
   Poggi M, 2016, INT CONF 3D VISION, P509, DOI 10.1109/3DV.2016.61
   Rhemann C, 2011, PROC CVPR IEEE, DOI 10.1109/CVPR.2011.5995372
   Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977
   Scharstein D., 2007, 2007 IEEE Conference on Computer Vision and Pattern Recognition CVPR'07, P1, DOI DOI 10.1109/CVPR.2007.383191.IEEE
   Seki A, 2016, PROC BRIT MACH VIS C, P1
   Seki A, 2017, PROC CVPR IEEE, P6640, DOI 10.1109/CVPR.2017.703
   Spyropoulos A, 2016, INT J COMPUT VISION, V118, P300, DOI 10.1007/s11263-015-0877-y
   Spyropoulos A, 2014, PROC CVPR IEEE, P1621, DOI 10.1109/CVPR.2014.210
   Taniai T, 2018, IEEE T PATTERN ANAL, V40, P2725, DOI 10.1109/TPAMI.2017.2766072
   Taniai T, 2014, PROC CVPR IEEE, P1613, DOI 10.1109/CVPR.2014.209
   threadingbuildingblocks, 2022, US
   Tsochantaridis I., 2004, ICML, P1
   Yan TM, 2019, IEEE T IMAGE PROCESS, V28, P3885, DOI 10.1109/TIP.2019.2903318
   Yang Q., 2006, BRIT MACHINE VISION, P989
   Yang QQ, 2015, IEEE SIGNAL PROC LET, V22, P1429, DOI 10.1109/LSP.2015.2409203
   Yang QX, 2012, PROC CVPR IEEE, P1402, DOI 10.1109/CVPR.2012.6247827
   Yang QX, 2009, IEEE T PATTERN ANAL, V31, P492, DOI 10.1109/TPAMI.2008.99
   Yao P, 2021, J VIS COMMUN IMAGE R, V78, DOI 10.1016/j.jvcir.2021.103169
   Yao P, 2021, MACH VISION APPL, V32, DOI 10.1007/s00138-021-01211-8
   Yao P, 2019, IET IMAGE PROCESS, V13, P98, DOI 10.1049/iet-ipr.2018.5801
   Yao P, 2018, IET COMPUT VIS, V12, P908, DOI 10.1049/iet-cvi.2017.0599
   Yao P, 2018, LECT NOTES COMPUT SC, V10704, P67, DOI 10.1007/978-3-319-73603-7_6
   Yin JH, 2017, PATTERN RECOGN, V71, P278, DOI 10.1016/j.patcog.2017.06.015
   Yoon KJ, 2005, PROC CVPR IEEE, P924
   Zabih R., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P151, DOI 10.1007/BFb0028345
   Zbontar J, 2016, J MACH LEARN RES, V17
   Zbontar J, 2015, PROC CVPR IEEE, P1592, DOI 10.1109/CVPR.2015.7298767
   Zhang C, 2015, IEEE I CONF COMP VIS, P2057, DOI 10.1109/ICCV.2015.238
   Zhang K, 2017, IEEE T CIRC SYST VID, V27, P965, DOI 10.1109/TCSVT.2015.2513663
   Zhang K, 2014, PROC CVPR IEEE, P1590, DOI 10.1109/CVPR.2014.206
   Zhang Z., 2011, PROC N AM POWER S NA, P1
NR 54
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 3985
EP 4000
DI 10.1007/s00371-024-03406-2
EA APR 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001210792300004
DA 2024-08-05
ER

PT J
AU Zhang, ZT
   Li, WH
   Cheng, YX
   Huang, QN
   Qiu, TR
AF Zhang, Zhentao
   Li, Wenhao
   Cheng, Yuxi
   Huang, Qingnan
   Qiu, Taorong
TI An improved residual learning model and its application to hardware
   image classification
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Hardware dataset; Convolutional neural network; Residual learning;
   Hardware image classification
ID NETWORK
AB Some hardware is similar in color and shape between different classes, and some hardware varies within a class, thereby decreasing the accuracy of classification models. The baseline of image classification depends on network architectures. To solve this problem, this paper proposes an improved residual network architecture that achieves high accuracy in hardware classification. This paper proposes a hardware dataset that fills the blank of hardware images. The improved residual network architectures are based on depthwise over-parameterized convolution and pyramidal convolution. This model could obtain information from different levels and channels. Moreover, compared with the novel architectures, our model does not require increasing the computation. Experiments on the hardware dataset show that our neural network architectures achieved better accuracy compared with notable architectures such as ResNet50 and DenseNet121. The classification accuracy of the proposed network is 94.6% and the f1 score is 90.26%. To verify the proposed network architecture, we applied our model in fine-grained datasets; our model achieved the best performance in the same training setting.
C1 [Zhang, Zhentao; Li, Wenhao; Cheng, Yuxi; Huang, Qingnan; Qiu, Taorong] Nanchang Univ, Sch Math & Comp Sci, Dept Comp, Xuefu Rd, Nanchang 330031, Peoples R China.
C3 Nanchang University
RP Qiu, TR (corresponding author), Nanchang Univ, Sch Math & Comp Sci, Dept Comp, Xuefu Rd, Nanchang 330031, Peoples R China.
EM 1275171803@qq.com; liwenhao@email.ncu.edu.cn; 1206095125@qq.com;
   542684309@qq.com; qiutaorong@ncu.edu.cn
FU Key R&D Program of JiangXi Province of China [20181BBG70031]; National
   Natural Science Foundation of China [62066027]
FX This work was supported by the Key R&D Program of JiangXi Province of
   China (Grant Nos.20181BBG70031) and the National Natural Science
   Foundation of China(62066027).
CR Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bi Q, 2020, IEEE T IMAGE PROCESS, V29, P4911, DOI 10.1109/TIP.2020.2975718
   Cao JM, 2022, IEEE T IMAGE PROCESS, V31, P3726, DOI 10.1109/TIP.2022.3175432
   Chollet F., 2017, P 2017 IEEE C COMPUT
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Duta I.C., 2020, arXiv
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   He K., 2016, European conference on computer vision, P770, DOI DOI 10.1007/978-3-319-46493-0_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu XG, 2020, IEEE ACCESS, V8, P212647, DOI 10.1109/ACCESS.2020.3038864
   Huang G, 2018, Arxiv, DOI [arXiv:1608.06993, 10.48550/arXiv.1608.06993]
   Ioffe Sergey, 2015, INT C MACHINE LEARNI, V37, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Kingma D. P., 2014, arXiv
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li J, 2020, IEEE T IND INFORM, V16, P566, DOI 10.1109/TII.2019.2935244
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Maji S, 2013, Arxiv, DOI arXiv:1306.5151
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Soltanolkotabi M, 2019, IEEE T INFORM THEORY, V65, P742, DOI 10.1109/TIT.2018.2854560
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan MX, 2019, PR MACH LEARN RES, V97
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang S, 2023, VISUAL COMPUT, V39, P4487, DOI 10.1007/s00371-022-02602-2
   Wang WG, 2022, Arxiv, DOI arXiv:2209.07383
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xing HJ, 2020, CHINA COMMUN, V17, P242, DOI 10.23919/JCC.2020.08.020
   Zagoruyko S, 2017, Arxiv, DOI arXiv:1605.07146
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhou B., 2014, arXiv
   Zhu XS, 2023, VISUAL COMPUT, V39, P4721, DOI 10.1007/s00371-022-02620-0
NR 37
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 APR 9
PY 2024
DI 10.1007/s00371-024-03340-3
EA APR 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NG1R9
UT WOS:001199213300004
DA 2024-08-05
ER

PT J
AU Mailhé, C
   Ammar, A
   Chinesta, F
   Baillargeat, D
AF Mailhe, Clement
   Ammar, Amine
   Chinesta, Francisco
   Baillargeat, Dominique
TI Towards improving synthetic-to-real image correlation for instance
   recognition in structure monitoring
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Computer vision; Image processing; Object recognition; Deep learning
AB Data-driven approaches in deep learning are inevitable in instance recognition applications in the absence of object description models. To alleviate the cost associated with the gathering of the tremendous amount of data needed to train reliable algorithms, the potential use of synthetic data has been envisioned. The latter solution, however presents a major setback labeled as "domain gap" in the field of computer vision. It refers to the ability of detection models to recognize the artificial nature of photo-realistic rendering which poses a serious issue for learning on synthetic images alone. This work takes on a parametric approach in order to assess the influence of common techniques in image processing on the effectiveness of object recognition. To illustrate, their effect the detection of defects in pipeline infrastructures is considered as a study case. Training and test datasets are altered using filters and color processing techniques and detection effectiveness is measured in each configuration. Recommendations are drawn from the results for the future efficient use of synthetic data in recognition model training.
C1 [Mailhe, Clement; Ammar, Amine; Baillargeat, Dominique] CNRSCREATE Ltd, Create Tower,1 Create Way 08-01, Singapore 138602, Singapore.
   [Ammar, Amine] HESAM Univ, Arts & Metiers Inst Technol, LAMPA, 2 Blvd Ronceray, F-49035 Angers, France.
   [Chinesta, Francisco] Arts & Metiers Inst Technol, ESI Chair, PIMM, 151 Blvd lHop, F-75013 Paris, France.
C3 heSam Universite; heSam Universite; Conservatoire National Arts &
   Metiers (CNAM)
RP Mailhé, C (corresponding author), CNRSCREATE Ltd, Create Tower,1 Create Way 08-01, Singapore 138602, Singapore.
EM clement.mailhe@cnrsatcreate.sg
OI Mailhe, Clement/0000-0002-2298-5213
FU National Research Fundation; National Research Foundation, Prime
   Minister's Office, Singapore under its Campus for Research Excellence
   and Technological Enterprise (CREATE) programme
FX This research is supported by the National Research Foundation, Prime
   Minister's Office, Singapore under its Campus for Research Excellence
   and Technological Enterprise (CREATE) programme.
CR ambientCG, AmbientCG-Public Domain Resources for Physically Based Rendering
   Barshooi AH, 2022, BIOMED SIGNAL PROCES, V72, DOI 10.1016/j.bspc.2021.103326
   Beliakov G, 2008, IEEE INT CONF FUZZY, P1474
   Chai JY, 2021, MACH LEARN APPL, V6, DOI 10.1016/j.mlwa.2021.100134
   de Melo CM, 2022, TRENDS COGN SCI, V26, P174, DOI 10.1016/j.tics.2021.11.008
   developer, Omniverse Replicator | NVIDIA.
   Dhruv B, 2019, INT J NEUROSCI, V129, P350, DOI 10.1080/00207454.2018.1536052
   Eversberg L, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21237901
   Foundation B., Blender.Org-Home of the Blender Project-Free and Open 3D Creation Software
   Gonzalez RC., 2006, DIGITAL IMAGE PROCES
   Hinterstoisser S, 2019, LECT NOTES COMPUT SC, V11129, P682, DOI 10.1007/978-3-030-11009-3_42
   Hodan T, 2019, IEEE IMAGE PROC, P66, DOI [10.1109/icip.2019.8803821, 10.1109/ICIP.2019.8803821]
   Huh J, 2018, ASIAPAC SIGN INFO PR, P1518, DOI 10.23919/APSIPA.2018.8659778
   Jiang YH, 2021, J PERFORM CONSTR FAC, V35, DOI 10.1061/(ASCE)CF.1943-5509.0001652
   Jocher Glenn., ULTRALYTICSYOLOV5 V6
   Jung HK, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12147255
   Kaya Y, 2014, VISUAL COMPUT, V30, P71, DOI 10.1007/s00371-013-0782-8
   Lebert D, 2022, LECT NOTES COMPUT SC, V13446, P198, DOI 10.1007/978-3-031-15553-6_15
   Lv XM, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20061562
   Mailhe Clement, 2023, ICMVA '23: Proceedings of the 2023 6th International Conference on Machine Vision and Applications, P81, DOI 10.1145/3589572.3589584
   Mall P.K., 2019, P 2019 IEEE C INFORM, P1
   Masita K.L., 2020, P 2020 INT C ARTIFIC, P1
   Morita A., 2022, The effect of preprocessing with gabor filters on image classification using CNNs, P4
   Munawar HS, 2022, DRONES-BASEL, V6, DOI 10.3390/drones6010005
   Patil K, 2017, 2017 16TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P50, DOI 10.1109/ICMLA.2017.0-179
   Qu Z, 2022, IMAGE VISION COMPUT, V125, DOI 10.1016/j.imavis.2022.104518
   Rasmussen I, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12178534
   Rimiru RM, 2022, PEERJ COMPUT SCI, V8, DOI 10.7717/peerj-cs.890
   Rozantsev A, 2015, COMPUT VIS IMAGE UND, V137, P24, DOI 10.1016/j.cviu.2014.12.006
   Salas AJC, 2020, SIBGRAPI, P226, DOI 10.1109/SIBGRAPI51738.2020.00038
   Schmedemann Ole, 2022, Procedia CIRP, P1101, DOI 10.1016/j.procir.2022.05.115
   Simanjuntak Taufik Ismail, 2019, Journal of Physics: Conference Series, V1361, DOI 10.1088/1742-6596/1361/1/012028
   Tan JX, 2020, IEEE T MED IMAGING, V39, P2013, DOI 10.1109/TMI.2019.2963177
   Tang PZ, 2024, VISUAL COMPUT, V40, P2015, DOI 10.1007/s00371-023-02899-7
   Tobin Josh, 2017, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), P23, DOI 10.1109/IROS.2017.8202133
   Tremblay J, 2018, IEEE COMPUT SOC CONF, P1082, DOI 10.1109/CVPRW.2018.00143
   Ünlü R, 2022, VISUAL COMPUT, V38, P685, DOI 10.1007/s00371-020-02043-9
   Venkatesvara Rao N., 2021, International Journal of Computers and Applications, V43, P119, DOI 10.1080/1206212X.2018.1525929
   Xiao YZ, 2020, MULTIMED TOOLS APPL, V79, P23729, DOI 10.1007/s11042-020-08976-6
   Young I.T., 1995, FUNDAMENTALS IMAGE P
   Zhu XK, 2021, IEEE INT CONF COMP V, P2778, DOI 10.1109/ICCVW54120.2021.00312
NR 41
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 19
PY 2024
DI 10.1007/s00371-024-03325-2
EA MAR 2024
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LN2Z4
UT WOS:001187430700001
DA 2024-08-05
ER

PT J
AU Yadav, S
   Gulia, P
   Gill, NS
   Yahya, M
   Shukla, PK
   Pareek, PK
   Shukla, PK
AF Yadav, Sangeeta
   Gulia, Preeti
   Gill, Nasib Singh
   Yahya, Mohammad
   Shukla, Piyush Kumar
   Pareek, Piyush Kumar
   Shukla, Prashant Kumar
TI A video compression-cum-classification network for classification from
   compressed video streams
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Video analytics; Compression; Content tagging; Deep Learning; CNN;
   ConvGRU; SSIM; PSNR
AB Video analytics can achieve increased speed and efficiency by operating directly on the compressed video format, thereby alleviating the decoding burden on the analytics server. The encoded video streams are rich in semantic binary information and this information can be utilized more efficiently to train the classifiers. Motivated by the same notion, a deep learning-based video compression-cum-classification network has been proposed. In the proposed work, the binary-coded semantic information is extracted by using an auto encoder-based video compression component and the same fed to the MobileNetv2-based classifier for the classification of the given video streams based on their content. Using large-scale user-generated content provided by YouTube UGC dataset, it has been demonstrated that using deep neural networks for compression not only provides on-par compression results to traditional methods, it makes analytical processing of these videos faster. Video content tagging of YouTube UGC dataset has been used as the analytics task. The proposed DLVCC approach performs 10 x faster with 30 x fewer parameters than MobileNetv2 in video tagging of compressed video with no loss in accuracy.
C1 [Yadav, Sangeeta; Gulia, Preeti; Gill, Nasib Singh] Maharshi Dayanand Univ, Dept Comp Sci & Applicat, Rohtak, Haryana, India.
   [Yahya, Mohammad] Oakland Univ, Rochester, MI USA.
   [Shukla, Piyush Kumar] Technol Univ Madhya Pradesh, Univ Inst Technol UIT, Rajiv Gandhi Proudyogiki Vishwavidyalaya RGPV, Dept Comp Sci & Engn, Bhopal, Madhya Pradesh, India.
   [Pareek, Piyush Kumar] Nitte Meenakshi Inst Technol, Dept Artificial Intelligence & Machine Learning &, Bengaluru, Karnataka, India.
   [Shukla, Prashant Kumar] Koneru Lakshmaiah Educ Fdn, Dept Comp Sci & Engn, Guntur 522302, Andhra Pradesh, India.
C3 Maharshi Dayanand University; Oakland University; Rajiv Gandhi
   Technological University; Nitte Meenakshi Institute of Technology;
   Koneru Lakshmaiah Education Foundation (K L Deemed to be University)
RP Shukla, PK (corresponding author), Technol Univ Madhya Pradesh, Univ Inst Technol UIT, Rajiv Gandhi Proudyogiki Vishwavidyalaya RGPV, Dept Comp Sci & Engn, Bhopal, Madhya Pradesh, India.
EM sangeeta.rs.dcsa@mdurohtak.ac.in; preeti@mdurohtak.ac.in;
   nasib.gill@mdurohtak.ac.in; yahya@oakland.edu; piyush@rgpv.ac.in;
   piyush.kumar@nmit.ac.in; prashantshukla2005@gmail.com
RI Shukla, Dr Prashant Kumar/CAF-0286-2022; A, preeti/GYI-9428-2022
OI Shukla, Dr Prashant Kumar/0000-0002-3092-2415; A,
   preeti/0000-0001-8535-4016; Shukla, Dr. Piyush Kumar/0000-0002-3715-3882
CR Benbarrad T, 2021, J SENS ACTUAT NETW, V10, DOI 10.3390/jsan10040073
   Bidwe RV, 2022, BIG DATA COGN COMPUT, V6, DOI 10.3390/bdcc6020044
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chang JW, 2022, Arxiv, DOI [arXiv:2203.10183, 10.48550/arXiv.2203.10183]
   Chen Z., 2019, arXiv
   dos Santos SF, 2020, SIBGRAPI, P62, DOI 10.1109/SIBGRAPI51738.2020.00017
   Fischer F., 2021, IEEE INT C IMAGE PRO
   Gandor T, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22031104
   GIRDHAR R, 2017, PROC CVPR IEEE, P3165, DOI DOI 10.1109/CVPR.2017.337
   Han J., 2018, arXiv
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Huo Y., 2020, Lecture Notes in Computer Science, V12536, DOI [10.1007/978-3-030-66096-3_24, DOI 10.1007/978-3-030-66096-3_24]
   Ingle PY, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22103862
   Jiao LC, 2022, IEEE T NEUR NET LEAR, V33, P3195, DOI 10.1109/TNNLS.2021.3053249
   Joshi S, 2023, EXPERT SYST, DOI 10.1111/exsy.13382
   Kim MJ, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12094525
   Kim S, 2018, Arxiv, DOI arXiv:1811.10673
   Koot R, 2021, arXiv
   Kubiak N., 2021, arXiv, DOI DOI 10.48550/ARXIV.2109.10658
   Liu HY, 2022, ARTIF INTELL REV, V55, P5981, DOI 10.1007/s10462-022-10147-y
   Liu MY, 2020, Arxiv, DOI [arXiv:2008.02793, DOI 10.48550/ARXIV.2008.02793]
   Lu G, 2019, Arxiv, DOI arXiv:1812.00101
   Ma C-Y., 2017, Ts-lstm and temporal-inception: Exploiting spatiotemporal dynamics for activity recognition, DOI 10.48550/arXiv.1703.10667
   Muralidhara S, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22218583
   O'Byrne M, 2022, 2022 18TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS 2022), DOI 10.1109/AVSS56176.2022.9959476
   Poyser M., 2020, 25 INT C PATTERN REC
   Reece J, 2023, COMMUNITY DEV, V54, P468, DOI 10.1080/15575330.2023.2217881
   Rippel O, 2018, Arxiv, DOI arXiv:1811.06981
   Sandula P, 2022, MULTIMED TOOLS APPL, V81, P12759, DOI 10.1007/s11042-022-12363-8
   Shou Z, 2019, PROC CVPR IEEE, P1268, DOI 10.1109/CVPR.2019.00136
   Tran D, 2017, Arxiv, DOI [arXiv:1708.05038, DOI 10.48550/ARXIV.1708.05038]
   Tu ZG, 2022, IEEE T IMAGE PROCESS, V31, P6517, DOI 10.1109/TIP.2022.3212905
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang YL, 2019, IEEE INT WORKSH MULT, DOI 10.1109/mmsp.2019.8901772
   Wu CY, 2018, Arxiv, DOI [arXiv:1712.00636, 10.48550/arXiv.1712.00636, DOI 10.48550/ARXIV.1712.00636]
   Wu CY, 2018, Arxiv, DOI arXiv:1804.06919
   Zhai DH, 2023, MEASUREMENT, V207, DOI 10.1016/j.measurement.2022.112371
   Zhang BW, 2016, PROC CVPR IEEE, P2718, DOI 10.1109/CVPR.2016.297
NR 38
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2024 MAR 8
PY 2024
DI 10.1007/s00371-023-03242-w
EA MAR 2024
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KA8J0
UT WOS:001177328200001
DA 2024-08-05
ER

EF