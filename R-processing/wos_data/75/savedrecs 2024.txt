FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Zhai, ZY
   Liang, J
   Cheng, B
   Zhao, LZ
   Qian, JY
AF Zhai, Zhongyi
   Liang, Jie
   Cheng, Bo
   Zhao, Lingzhong
   Qian, Junyan
TI Strengthening attention: knowledge distillation via cross-layer feature
   fusion for image classification
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Deep learning; Knowledge distillation; Image classification; Attention
AB Deep learning has achieved great success in computer vision, especially in image classification tasks. How to improve the generalization ability and compactness of deep neural networks has gradually attracted widespread attention from researchers. Knowledge distillation is an effective technique for model compression. It transfers general knowledge from a sophisticated teacher model to a smaller student model. Recently, some studies refine knowledge from feature maps or adopt complex attention mechanisms to better supervise students imitating teachers. However, their methods focus too much on how to improve students' accuracy and largely overlook the associated training costs, which violates the original intention of knowledge distillation to compress the model. To achieve a balance between performance and efficiency, in this paper, we introduce a straightforward and effective distillation method to utilize the deepest feature maps to enhance shallow features. Specifically, our method performs processing only on the original feature maps without an extra assisting network. Moreover, we use cross-layer feature fusion to enhance the attention on shallow feature maps. By visualizing the features of different layers, we demonstrate the importance of the fusion operation in our method. Our experimental results on the CIFAR-100, tinyImageNet and miniImageNet datasets show that our approach outperforms previous methods, especially in the balance between performance and training cost. Further ablative studies verify the effectiveness of the design.
C1 [Zhai, Zhongyi; Liang, Jie; Zhao, Lingzhong] Guilin Univ Elect Technol, Guangxi Key Lab Image & Graph Intelligent Proc, Guilin, Peoples R China.
   [Cheng, Bo] Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing, Peoples R China.
   [Qian, Junyan] Guangxi Normal Univ, Key Lab Educ Blockchain & Intelligent Technol, Guilin, Peoples R China.
C3 Guilin University of Electronic Technology; Beijing University of Posts
   & Telecommunications; Guangxi Normal University
RP Zhao, LZ (corresponding author), Guilin Univ Elect Technol, Guangxi Key Lab Image & Graph Intelligent Proc, Guilin, Peoples R China.
EM zhaolingzhong@126.com
RI 梁, 杰/HNC-1165-2023
FU Guangxi Natural Science Foundation of China; National Natural Science
   Foundation of China [62262009,61902086,6202780103, U21A20474]; Open
   Foundation of State key Laboratory of Networking and Switching
   Technology in China;  [2023GXNSFAA026270,2024GXNSFAA010340]
FX This work was supported in part by the Guangxi Natural Science
   Foundation of China(Nos.2023GXNSFAA026270,2024GXNSFAA010340), National
   Natural Science Foundation of China(Nos. 62262009,61902086,6202780103,
   U21A20474), Open Foundation of State key Laboratory of Networking and
   Switching Technology in China(No.SKLNST-2022-1-04).
CR Ahn S, 2019, PROC CVPR IEEE, P9155, DOI 10.1109/CVPR.2019.00938
   Chen DF, 2021, AAAI CONF ARTIF INTE, V35, P7028
   Chen LQ, 2021, PROC CVPR IEEE, P16291, DOI 10.1109/CVPR46437.2021.01603
   Chen YT, 2018, AAAI CONF ARTIF INTE, P2852
   Ding QG, 2021, AAAI CONF ARTIF INTE, V35, P7228
   Dong XY, 2019, ADV NEUR IN, V32
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Heo B, 2019, IEEE I CONF COMP VIS, P1921, DOI 10.1109/ICCV.2019.00201
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hubara I, 2016, ADV NEUR IN, V29
   Jacob B, 2018, PROC CVPR IEEE, P2704, DOI 10.1109/CVPR.2018.00286
   Ji M, 2021, AAAI CONF ARTIF INTE, V35, P7945
   Kaiyu Yue, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P312, DOI 10.1007/978-3-030-58555-6_19
   Kim J, 2018, 32 C NEURAL INFORM P, V31
   Kokhlikyan N, 2020, Arxiv, DOI [arXiv:2009.07896, DOI 10.48550/ARXIV.2009.07896]
   Krizhevsky A., 2009, Learning multiple layers of features from tiny images
   Leng C, 2018, AAAI CONF ARTIF INTE, P3466
   Li H, 2017, Arxiv, DOI arXiv:1608.08710
   Lin MB, 2020, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR42600.2020.00160
   Lin T.-Y., 2017, PROC CVPR IEEE, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Miles R, 2023, PROC CVPR IEEE, P10480, DOI 10.1109/CVPR52729.2023.01010
   Mirzadeh SI, 2020, AAAI CONF ARTIF INTE, V34, P5191
   Muller R., 2019, Adv Neural Inf Process Syst, V32, P66
   Passalis N, 2020, PROC CVPR IEEE, P2336, DOI 10.1109/CVPR42600.2020.00241
   Peng JJ, 2023, SIGNAL PROCESS-IMAGE, V113, DOI 10.1016/j.image.2023.116922
   Peng JJ, 2021, J VIS COMMUN IMAGE R, V79, DOI 10.1016/j.jvcir.2021.103207
   Romero A, 2015, Arxiv, DOI arXiv:1412.6550
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Sundararajan M, 2017, PR MACH LEARN RES, V70
   Tang J, 2023, arXiv
   Tian Y., 2020, Contrastive representation distillation
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Tung F, 2019, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2019.00145
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O., 2016, Adv Neural Inf Process Syst, V29, P66
   Wang HB, 2023, IEEE T MULTIMEDIA, V25, P6629, DOI 10.1109/TMM.2022.3212270
   Xiao X, 2019, ADV NEUR IN, V32
   Yamamoto K, 2021, PROC CVPR IEEE, P5027, DOI 10.1109/CVPR46437.2021.00499
   Yang ZD, 2022, LECT NOTES COMPUT SC, V13671, P53, DOI 10.1007/978-3-031-20083-0_4
   Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754
   Yuan L, 2020, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR42600.2020.00396
   Yushuo Guan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P469, DOI 10.1007/978-3-030-58520-4_28
   Zagoruyko S, 2017, Arxiv, DOI arXiv:1605.07146
   Zagoruyko S, 2017, Arxiv, DOI [arXiv:1612.03928, DOI 10.48550/ARXIV.1612.03928]
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang LF, 2020, PROC CVPR IEEE, P369, DOI 10.1109/CVPR42600.2020.00045
   Zhao BR, 2022, PROC CVPR IEEE, P11943, DOI 10.1109/CVPR52688.2022.01165
NR 51
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD JUN
PY 2024
VL 13
IS 2
AR 23
DI 10.1007/s13735-024-00332-w
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RE3E1
UT WOS:001225945300001
DA 2024-08-05
ER

PT J
AU Elboushaki, A
   Hannane, R
   Afdel, K
AF Elboushaki, Abdessamad
   Hannane, Rachida
   Afdel, Karim
TI Similarity-based face image retrieval using sparsely embedded deep
   features and binary code learning
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Face image retrieval; Face matching; Deep face representation; Sparse
   score fusion; Binary code learning
ID NETWORK
AB Human face retrieval has long been established as one of the most interesting research topics in computer vision. With the recent development of deep learning, many researchers have addressed this problem by building deep hashing models to learn binary code from face images, while performing face retrieval as a classification task. Nevertheless, the performance is still unsatisfactory since these models are incapable of handling inter-class variation between multiple persons, as we need to make a class label for each identity. In this backdrop, we propose in this paper an effective deep learning-based framework for face image retrieval. The key to our framework is mainly based on the matching of face pairs, where a two-stream network, named chi Net+chi Match\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\chi Net+\chi Match$$\end{document}, is designed to learn similarities in terms of person identity. Such similarities are investigated by embedding both deep local representation via face components, and deep global face representation via the whole face image. Since the similarities captured over face components are supposed to diversify due to variation in pose, expression and occlusion, we also introduce a Sparse Score Fusion layer that learns automatically the weight of each component according to its contribution to face matching. To allow fast retrieval, we farther propose a method that generates binary codes corresponding to the groups of similar faces through the hierarchical k-means, where the path down binary tree is exploited as a binary code for indexing. The final retrieval is then conducted within a privileged subset of images in the database. Our experiments on different challenging datasets show that our approach obtains outstanding results while outperforming most existing methods.
C1 [Elboushaki, Abdessamad] Cadi Ayyad Univ, Fac Sci & Technol, Lab Comp Engn & Syst, Marrakech 40000, Morocco.
   [Hannane, Rachida] Cadi Ayyad Univ, Fac Sci Semlalia Marrakech, Lab Comp Syst Engn, Marrakech 40000, Morocco.
   [Afdel, Karim] Ibn Zohr Univ, Fac Sci, Lab Comp Syst & Vis, Agadir 80000, Morocco.
C3 Cadi Ayyad University of Marrakech; Cadi Ayyad University of Marrakech;
   Ibn Zohr University of Agadir
RP Elboushaki, A (corresponding author), Cadi Ayyad Univ, Fac Sci & Technol, Lab Comp Engn & Syst, Marrakech 40000, Morocco.
EM abdessamad.elboushaki@gmail.com; r.hannane@uca.ma; k.afdel@uiz.ac.ma
FU Centre National pour la Recherche Scientifique et Technique [14UIZ2015,
   ALKHAWARIZMI/2020/02]; Moroccan government through the CNRST
FX This work is supported in part by the PPR2-2015 project under grant
   number 14UIZ2015, and in part by the Al Khawarizmi project under grant
   number ALKHAWARIZMI/2020/02 financed by the Moroccan government through
   the CNRST funding program.DAS:All datasets used in this work are
   publicly available and have been properly referenced in the text.
CR Ameur B, 2019, INT J MULTIMED INF R, V8, P143, DOI 10.1007/s13735-019-00175-w
   An L, 2016, NEUROCOMPUTING, V172, P215, DOI 10.1016/j.neucom.2014.09.098
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chen BC, 2017, IEEE T CIRC SYST VID, V27, P1595, DOI 10.1109/TCSVT.2016.2538520
   Chen BC, 2013, IEEE T MULTIMEDIA, V15, P1163, DOI 10.1109/TMM.2013.2242460
   Dai P, 2018, MULTIMED TOOLS APPL, V77, P23547, DOI 10.1007/s11042-018-5684-3
   Dong Z, 2016, AAAI CONF ARTIF INTE, P3471
   Dubey SR, 2019, MULTIMED TOOLS APPL, V78, P16411, DOI 10.1007/s11042-018-7028-8
   Feng P, 2023, LECT NOTES COMPUT SC, V13834, P423, DOI 10.1007/978-3-031-27818-1_35
   Geng X, 2016, IEEE T KNOWL DATA EN, V28, P1734, DOI 10.1109/TKDE.2016.2545658
   Gui J, 2018, IEEE T PATTERN ANAL, V40, P490, DOI 10.1109/TPAMI.2017.2678475
   Han XF, 2015, PROC CVPR IEEE, P3279, DOI 10.1109/CVPR.2015.7298948
   Hannane R, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107504
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G. B., 2007, Technical report
   Jang YK, 2020, PROC CVPR IEEE, P3417, DOI 10.1109/CVPR42600.2020.00348
   Jiang HZ, 2017, IEEE INT CONF AUTOMA, P650, DOI [10.1109/FG.2017.82, 10.1109/MWSYM.2017.8058653]
   Jing CC, 2019, IEEE T MULTIMEDIA, V21, P782, DOI 10.1109/TMM.2018.2866222
   Kafai M, 2014, IEEE T INF FOREN SEC, V9, P2132, DOI 10.1109/TIFS.2014.2359548
   Kafai M, 2014, IEEE T MULTIMEDIA, V16, P1090, DOI 10.1109/TMM.2014.2305633
   Khan MA, 2020, EXPERT SYST APPL, V141, DOI 10.1016/j.eswa.2019.112925
   Klare B.F, 2014, IEEE IEEE INT JOINT, P1
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li Q, 2017, ADV NEUR IN, V30
   Li Wu-Jun, 2016, P INT JOINT C ART IN, P1711
   Li Y, 2014, Compact video code and its application to robust face retrieval in TV-series
   Li Y, 2016, IEEE T IMAGE PROCESS, V25, P5905, DOI 10.1109/TIP.2016.2616297
   Li Y, 2015, IEEE I CONF COMP VIS, P3819, DOI 10.1109/ICCV.2015.435
   Li Y, 2015, PROC CVPR IEEE, P4758, DOI 10.1109/CVPR.2015.7299108
   Lin J, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2266
   Liu Decheng, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P4647, DOI 10.1145/3581783.3612355
   Liu DC, 2022, IEEE T NEUR NET LEAR, V33, P5611, DOI 10.1109/TNNLS.2021.3071119
   Liu DC, 2020, IEEE T NEUR NET LEAR, V31, P4699, DOI 10.1109/TNNLS.2019.2957285
   Liu DC, 2018, NEUROCOMPUTING, V302, P46, DOI 10.1016/j.neucom.2018.03.042
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Mishra A, 2019, INT J MULTIMED INF R, V8, P135, DOI 10.1007/s13735-018-0160-4
   Ng HW, 2014, IEEE IMAGE PROC, P343, DOI 10.1109/ICIP.2014.7025068
   Sun Y., 2014, NIPS, P1988, DOI DOI 10.1007/978-3-030-01252-6_48
   Tang JS, 2019, INT J HUM-COMPUT INT, V35, P846, DOI 10.1080/10447318.2018.1502000
   Tang JH, 2018, PATTERN RECOGN, V75, P25, DOI 10.1016/j.patcog.2017.03.028
   Tarawneh AS, 2019, INT CONF INFORM COMM, P185, DOI [10.1109/IACS.2019.8809127, 10.1109/iacs.2019.8809127]
   Wang DY, 2015, Arxiv, DOI arXiv:1507.07242
   Wang DY, 2015, INT CONF BIOMETR, P473, DOI 10.1109/ICB.2015.7139112
   Wang DY, 2014, IEEE T PATTERN ANAL, V36, P550, DOI 10.1109/TPAMI.2013.145
   Wang F, 2018, LECT NOTES COMPUT SC, V11213, P780, DOI 10.1007/978-3-030-01240-3_47
   Wang R, 2020, WACV
   Wang XF, 2017, LECT NOTES COMPUT SC, V10111, P70, DOI 10.1007/978-3-319-54181-5_5
   Wolf L., 2011, CVPR, P529, DOI [10.1109/CVPR.2011.5995566, DOI 10.1109/CVPR.2011.5995566]
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Xiong Z, 2019, PAC RIM INT C ART IN
   Xu CF, 2017, NEUROCOMPUTING, V222, P62, DOI 10.1016/j.neucom.2016.10.010
   Xu N, 2023, IEEE T PATTERN ANAL, V45, P6537, DOI 10.1109/TPAMI.2022.3203678
   Xu N, 2021, IEEE T KNOWL DATA EN, V33, P1632, DOI 10.1109/TKDE.2019.2947040
   Yan Li, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163089
   Yang SF, 2014, IEEE T AFFECT COMPUT, V5, P432, DOI 10.1109/TAFFC.2014.2364581
   Yuan L, 2020, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR42600.2020.00315
   Zaeemzadeh A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12096, DOI 10.1109/ICCV48922.2021.01190
   Zhang M, 2023, PATTERN RECOGN, V141, DOI 10.1016/j.patcog.2023.109671
   Zhang M, 2021, PATTERN RECOGN, V117, DOI 10.1016/j.patcog.2021.107976
   Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315
   Zhe XF, 2020, IEEE T NEUR NET LEAR, V31, P1681, DOI 10.1109/TNNLS.2019.2921805
   Zhi Xiong, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P136, DOI 10.1145/3372278.3390683
   Zhou LX, 2021, INT C PATT RECOG, P7012, DOI 10.1109/ICPR48806.2021.9412202
NR 63
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD SEP
PY 2024
VL 13
IS 3
AR 28
DI 10.1007/s13735-024-00337-5
PG 20
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XW7D6
UT WOS:001264770900001
DA 2024-08-05
ER

PT J
AU Yadav, A
   Gupta, A
AF Yadav, Ashima
   Gupta, Anika
TI An emotion-driven, transformer-based network for multimodal fake news
   detection
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Deep learning; Emotions; Fake news; Multimodal; Transformer
AB Social media is filled with multimedia data in the form of news and is heavily impacting the daily lives of the people. However, the rise of fake news is causing distress and becoming a major source of concern. Several attempts have been made in the past to detect fake news, but it still remains a challenging problem. In this study, we propose an emotion-driven framework that extracts emotions from the multimodal data to identify fake news. We use the vision transformer, which removes the irrelevant data from the images and enhances the overall classification accuracy. To the best of our knowledge, this is the first work that incorporates multimodal emotions to detect fake news in the multimodal data, comprising of images and text. We conducted several experiments on five datasets: Twitter, Jruvika Fake News Dataset, Pontes Fake News Dataset, Risdal Fake News Dataset, and Fakeddit Multimodal Dataset, and evaluated the performance of the network by using Precision, Recall, F1 scores, Accuracy, and ROC curves. We also conducted an ablation study to verify the effectiveness of different components involved in the proposed architecture. The experimental results show that the proposed architecture outperforms the state-of-the-art and other baseline methods on all the evaluation metrics.
C1 [Yadav, Ashima; Gupta, Anika] Bennett Univ, Sch Comp Sci Engn & Technol, Greater Noida, India.
RP Yadav, A (corresponding author), Bennett Univ, Sch Comp Sci Engn & Technol, Greater Noida, India.
EM ashima.yadav@bennett.edu.in; anika.gupta@bennett.edu.in
OI , Anika/0000-0001-6349-6742
CR Ajao O, 2019, INT CONF ACOUST SPEE, P2507, DOI 10.1109/ICASSP.2019.8683170
   [Anonymous], Pontes Fake News Dataset
   [Anonymous], LIST EMOTICONS
   [Anonymous], Jruvika Fake News Dataset
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Armin K, 2021, INT WORK CONTENT MUL, P207, DOI 10.1109/CBMI50038.2021.9461898
   Boididou C., 2016, CEUR Workshop Proc, V1739, P1
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dong XS, 2020, IEEE T COMPUT SOC SY, V7, P1386, DOI 10.1109/TCSS.2020.3027639
   Jin ZW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P795, DOI 10.1145/3123266.3123454
   Khattar D, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P2915, DOI 10.1145/3308558.3313552
   Kumari R, 2022, INFORM PROCESS MANAG, V59, DOI 10.1016/j.ipm.2021.102740
   Kumari R, 2021, EXPERT SYST APPL, V184, DOI 10.1016/j.eswa.2021.115412
   Li PG, 2021, IEEE T MULTIMEDIA, V24, P3455, DOI 10.1109/TMM.2021.3098988
   Liao Q, 2022, IEEE T KNOWL DATA EN, V34, P5154, DOI 10.1109/TKDE.2021.3054993
   Meel P, 2021, INFORM SCIENCES, V567, P23, DOI 10.1016/j.ins.2021.03.037
   Mohammad SM, 2013, COMPUT INTELL-US, V29, P436, DOI 10.1111/j.1467-8640.2012.00460.x
   Paka WS, 2021, APPL SOFT COMPUT, V107, DOI 10.1016/j.asoc.2021.107393
   Sengan S, 2023, IEEE T COMPUT SOC SY, DOI 10.1109/TCSS.2023.3269087
   Shim JS, 2021, EXPERT SYST APPL, V184, DOI 10.1016/j.eswa.2021.115491
   Singhal S, 2020, AAAI CONF ARTIF INTE, V34, P13915
   Song CG, 2021, INFORM PROCESS MANAG, V58, DOI 10.1016/j.ipm.2020.102437
   Trueman TE, 2021, APPL SOFT COMPUT, V110, DOI 10.1016/j.asoc.2021.107600
   Turney P., 2010, Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, June 2010, P26, DOI DOI 10.5555/1860631.1860635
   Uppada SK, 2023, J INTELL INF SYST, V61, P367, DOI 10.1007/s10844-022-00764-y
   Verma PK, 2021, IEEE T COMPUT SOC SY, V8, P881, DOI 10.1109/TCSS.2021.3068519
   Wu L, 2021, IEEE T KNOWL DATA EN
   Xue JX, 2021, INFORM PROCESS MANAG, V58, DOI 10.1016/j.ipm.2021.102610
   Yang Y., 2018, INT C MACHINE LEARNI, P5571, DOI DOI 10.1115/FEDSM2018-83038
   Yuan H, 2021, DECIS SUPPORT SYST, V151, DOI 10.1016/j.dss.2021.113633
   Zhang XY, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P3465, DOI 10.1145/3442381.3450004
   Zhao SC, 2018, IEEE T AFFECT COMPUT, V9, P526, DOI [10.1109/TAFFC.2016.2628787, 10.1109/TAFFC.2018.2818685]
NR 33
TC 0
Z9 0
U1 25
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD MAR
PY 2024
VL 13
IS 1
AR 7
DI 10.1007/s13735-023-00315-3
PG 16
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FY0D7
UT WOS:001149286600001
DA 2024-08-05
ER

PT J
AU Huang, JY
   Kang, H
AF Huang, Jianying
   Kang, Hoon
TI 3D skeleton-based human motion prediction using spatial-temporal graph
   convolutional network
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Human motion prediction; Human pose; 3D human skeleton; Spatial-Temporal
   graph convolutional network (ST-GCN) model; Spatial graph convolutional
   network (SGCN); Temporal convolutional network (TCN)
ID HUMAN ACTION RECOGNITION; GCN
AB 3D human motion prediction; predicting future human poses in the basis of historically observed motion sequences, is a core task in computer vision. Thus far, it has been successfully applied to both autonomous driving and human-robot interaction. Previous research work has usually employed Recurrent Neural Networks (RNNs)-based models to predict future human poses. However, as previous works have amply demonstrated, RNN-based prediction models suffer from unrealistic and discontinuous problems in human motion prediction due to the accumulation of prediction errors. To address this, we propose a feed-forward, 3D skeleton-based model for human motion prediction. This model, the Spatial-Temporal Graph Convolutional Network (ST-GCN) model, automatically learns the spatial and temporal patterns of human motion from input sequences. This model overcomes the limitations of previous research approaches. Specifically, our ST-GCN model is based on an encoder-decoder architecture. The encoder consists of 5 ST-GCN modules, with each ST-GCN module consisting of a spatial GCN layer and a 2D convolution-based TCN layer, which facilitate the encoding of the spatio-temporal dynamics of human motion. Subsequently, the decoder, consisting of 5 TCN layers, exploits the encoded spatio-temporal representation of human motion to predict future human pose. We leveraged the ST-GCN model to perform extensive experiments on various large-scale human activity 3D pose datasets (Human3.6 M, AMASS, 3DPW) while adopting MPJPE (Mean Per Joint Position Error) as the evaluation metric. The experimental results demonstrate that our ST-GCN model outperforms the baseline models in both short-term (< 400 ms) and long-term (> 400 ms) predictions, thus yielding the best prediction results.
C1 [Huang, Jianying; Kang, Hoon] Chung Ang Univ, Sch Elect & Elect Engn, 84 Heukseok Ro, Seoul 06974, South Korea.
C3 Chung Ang University
RP Huang, JY; Kang, H (corresponding author), Chung Ang Univ, Sch Elect & Elect Engn, 84 Heukseok Ro, Seoul 06974, South Korea.
EM jianying@cau.ac.kr; hkang@cau.ac.kr
FU The National Research Foundation of Korea (NRF) grant funded by the
   Korea government (MSIT) [2021R1A2C1009735]; National Research Foundation
   of Korea (NRF) - Korea government (MSIT)
FX This research was supported by the National Research Foundation of Korea
   (NRF) grant funded by the Korea government (MSIT) (No.
   2021R1A2C1009735).DAS:The Human3.6 M dataset is available in [57], the
   AMASS dataset is available in [67], and the 3DPW dataset is available in
   [68].
CR Bai SJ, 2018, Arxiv, DOI [arXiv:1803.01271, DOI 10.48550/ARXIV.1803.01271]
   Barquero G, 2023, IEEE I CONF COMP VIS, P2317, DOI 10.1109/ICCV51070.2023.00220
   Barsoum E, 2018, IEEE COMPUT SOC CONF, P1499, DOI 10.1109/CVPRW.2018.00191
   Bütepage J, 2017, PROC CVPR IEEE, P1591, DOI 10.1109/CVPR.2017.173
   Lipton ZC, 2015, Arxiv, DOI arXiv:1506.00019
   Chen BJ, 2018, INFORM SCIENCES, V450, P89, DOI 10.1016/j.ins.2018.02.052
   Chen LH, 2023, P IEEE CVF INT C COM, P9544
   Chen S, 2022, MACH LEARN, V111, P2381, DOI 10.1007/s10994-022-06141-8
   Cheng K, 2020, PROC CVPR IEEE, P180, DOI 10.1109/CVPR42600.2020.00026
   Chiu HK, 2019, IEEE WINT CONF APPL, P1423, DOI 10.1109/WACV.2019.00156
   Cho KYHY, 2014, Arxiv, DOI [arXiv:1406.1078, 10.48550/arXiv.1406.1078]
   Cu QJ, 2019, INFORM SCIENCES, V493, P57, DOI 10.1016/j.ins.2019.04.031
   Cui QJ, 2021, INFORM SCIENCES, V545, P427, DOI 10.1016/j.ins.2020.08.123
   Cui QJ, 2020, PROC CVPR IEEE, P6518, DOI 10.1109/CVPR42600.2020.00655
   Dang LW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11447, DOI 10.1109/ICCV48922.2021.01127
   Defferrard M, 2016, ADV NEUR IN, V29
   Fragkiadaki K, 2015, IEEE I CONF COMP VIS, P4346, DOI 10.1109/ICCV.2015.494
   Fu JJ, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3277476
   Gao BK, 2022, APPL INTELL, V52, P5608, DOI 10.1007/s10489-021-02723-6
   Ghosh P, 2017, INT CONF 3D VISION, P458, DOI 10.1109/3DV.2017.00059
   Gopalakrishnan A, 2019, PROC CVPR IEEE, P12108, DOI 10.1109/CVPR.2019.01239
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Gui LY, 2018, LECT NOTES COMPUT SC, V11208, P823, DOI 10.1007/978-3-030-01225-0_48
   Guo W, 2023, IEEE WINT CONF APPL, P4798, DOI 10.1109/WACV56688.2023.00479
   Han F, 2017, COMPUT VIS IMAGE UND, V158, P85, DOI 10.1016/j.cviu.2017.01.011
   Heidari N, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P3220, DOI 10.1109/ICASSP39728.2021.9413860
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kim TS, 2017, IEEE COMPUT SOC CONF, P1623, DOI 10.1109/CVPRW.2017.207
   Koppula HS, 2013, IEEE INT C INT ROBOT, P2071, DOI 10.1109/IROS.2013.6696634
   Lea C, 2017, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2017.113
   Lehrmann AM, 2014, PROC CVPR IEEE, P1314, DOI 10.1109/CVPR.2014.171
   Li C, 2017, Arxiv, DOI arXiv:1704.07595
   Li C, 2018, PROC CVPR IEEE, P5226, DOI 10.1109/CVPR.2018.00548
   Li JN, 2021, NEUROCOMPUTING, V444, P338, DOI 10.1016/j.neucom.2019.12.149
   Li MS, 2021, IEEE T IMAGE PROCESS, V30, P7760, DOI 10.1109/TIP.2021.3108708
   Li MS, 2020, PROC CVPR IEEE, P211, DOI 10.1109/CVPR42600.2020.00029
   Liu CF, 2021, NEUROCOMPUTING, V430, P174, DOI 10.1016/j.neucom.2020.10.016
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu YN, 2023, IEEE T VIS COMPUT GR, V29, P2575, DOI 10.1109/TVCG.2023.3247075
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Ma TZ, 2022, PROC CVPR IEEE, P6427, DOI 10.1109/CVPR52688.2022.00633
   Mahmood N, 2019, IEEE I CONF COMP VIS, P5441, DOI 10.1109/ICCV.2019.00554
   Mandia S, 2023, INT J MULTIMED INF R, V12, DOI 10.1007/s13735-023-00284-7
   Mao W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13289, DOI 10.1109/ICCV48922.2021.01306
   Mao W, 2021, INT J COMPUT VISION, V129, P2513, DOI 10.1007/s11263-021-01483-7
   Mao W, 2019, IEEE I CONF COMP VIS, P9488, DOI 10.1109/ICCV.2019.00958
   Martinez J, 2017, PROC CVPR IEEE, P4674, DOI 10.1109/CVPR.2017.497
   Mazari A, 2024, INT J MULTIMED INF R, V13, DOI 10.1007/s13735-023-00317-1
   Niepert M, 2016, PR MACH LEARN RES, V48
   Paden B, 2016, IEEE T INTELL VEHICL, V1, P33, DOI 10.1109/TIV.2016.2578706
   Ruiz AH, 2019, IEEE I CONF COMP VIS, P7133, DOI 10.1109/ICCV.2019.00723
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Sofianos T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11189, DOI 10.1109/ICCV48922.2021.01102
   Tang YY, 2018, Arxiv, DOI arXiv:1805.02513
   Taylor GW, 2006, Adv Neural Inf Process Syst, V19
   Tian HY, 2022, NEUROCOMPUTING, V473, P116, DOI 10.1016/j.neucom.2021.12.004
   van Welbergen H, 2010, COMPUT GRAPH FORUM, V29, P2530, DOI 10.1111/j.1467-8659.2010.01822.x
   von Marcard T, 2018, LECT NOTES COMPUT SC, V11214, P614, DOI 10.1007/978-3-030-01249-6_37
   Walker J, 2015, IEEE I CONF COMP VIS, P2443, DOI 10.1109/ICCV.2015.281
   Wang BR, 2019, IEEE I CONF COMP VIS, P7123, DOI 10.1109/ICCV.2019.00722
   Wang J.M., 2005, NIPS
   Wang J, 2012, LECT NOTES COMPUT SC, V7573, P872, DOI 10.1007/978-3-642-33709-3_62
   Wang M, 2021, arXiv
   Wang MX, 2023, IEEE ACCESS, V11, P31951, DOI 10.1109/ACCESS.2023.3263117
   WANG QY, 2022, IEEE ACCESS, V10, P41403, DOI 10.1109/ACCESS.2022.3164711
   Wei Mao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P474, DOI 10.1007/978-3-030-58568-6_28
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Zhong CY, 2022, PROC CVPR IEEE, P6437, DOI 10.1109/CVPR52688.2022.00634
NR 70
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD SEP
PY 2024
VL 13
IS 3
AR 33
DI 10.1007/s13735-024-00341-9
PG 18
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A1Z1F
UT WOS:001280573600001
DA 2024-08-05
ER

PT J
AU Wei, YX
   Zheng, LG
   Qiu, GP
   Cai, GC
AF Wei, Yuxin
   Zheng, Ligang
   Qiu, Guoping
   Cai, Guocan
TI Cross-modal retrieval based on shared proxies
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Cross-modal; Retrieval; Proxy; Modality gap; Neighbor component analysis
AB Learning a common space that is simultaneously semantically discriminative and modality invariant stands as the primary challenge in cross-modal retrieval. Conventional approaches usually employ pairwise or triplet data relationships to learn the common space, which can only capture the data similarity locally but would be unable to effectively characterize the global geometry of the common embedding space, and thus would limit the performance of cross-modal retrieval. This paper proposes to integrate the principles of the shared proxy and neighborhood component analysis in order to learn a shared space for different modalities. The objective of this shared space is to minimize the distance between a sample's representation and its corresponding proxy, while also maximizing the distances between a sample's representation and the proxies that are not associated with the sample. Our proposed framework, named Cross-mOdal proXy learnIng (COXI), incorporates a cross-modal shared proxy loss, a discriminative loss, and a modality invariant loss to facilitate supervised cross-modal retrieval. Extensive experiments on benchmark datasets clearly shows that COXI outperforms state-of-the-art cross-modal retrieval techniques. Code is available on https://github.com/LigangZheng/COXI.
C1 [Wei, Yuxin; Zheng, Ligang; Cai, Guocan] Guangzhou Univ, Sch Comp Sci & Cyber Engn, Guangzhou 510006, Peoples R China.
   [Qiu, Guoping] Univ Nottingham, Sch Comp Sci, Nottingham NG8 1BB, England.
C3 Guangzhou University; University of Nottingham
RP Zheng, LG (corresponding author), Guangzhou Univ, Sch Comp Sci & Cyber Engn, Guangzhou 510006, Peoples R China.
EM wyx@e.gzhu.edu.cn; zlg@gzhu.edu.cn; guoping.qiu@nottingham.ac.uk;
   guocancai@e.gzhu.edu.cn
FU Natural Science Foundation of China; Science and Technology Projects in
   Guangzhou [202102010412];  [U1936116]
FX This work is supported by the Natural Science Foundation of China
   (U1936116) and the Science and Technology Projects in Guangzhou
   (202102010412).
CR Andrew R., 2013, INT C MACHINE LEARNI, P1247, DOI DOI 10.5555/3042817.3043076
   Arya D, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2245, DOI 10.1145/3343031.3350572
   Aziere N, 2019, PROC CVPR IEEE, P7291, DOI 10.1109/CVPR.2019.00747
   Carvalho M, 2018, ACM/SIGIR PROCEEDINGS 2018, P35, DOI 10.1145/3209978.3210036
   Castrejon L, 2016, PROC CVPR IEEE, P2940, DOI 10.1109/CVPR.2016.321
   Chen YD, 2019, IEEE I CONF COMP VIS, P9795, DOI 10.1109/ICCV.2019.00989
   Cheng QR, 2024, IEEE T CIRC SYST VID, V34, P6503, DOI 10.1109/TCSVT.2022.3182549
   Chua TS, 2009, P ACM INT C IM VID R, P1
   Pereira JC, 2014, IEEE T PATTERN ANAL, V36, P521, DOI 10.1109/TPAMI.2013.142
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Goldberger J., 2004, Advances in Neural Information Processing Systems, V17
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.2307/2333955
   Hu P, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P635, DOI 10.1145/3331184.3331213
   Huang X, 2020, IEEE T CYBERNETICS, V50, P1047, DOI 10.1109/TCYB.2018.2879846
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Jing LL, 2021, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR46437.2021.00316
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kim S, 2020, PROC CVPR IEEE, P3235, DOI 10.1109/CVPR42600.2020.00330
   Kim S, 2019, PROC CVPR IEEE, P2283, DOI 10.1109/CVPR.2019.00239
   Kim Y., 2014, P 2014 C EMPIRICAL M, DOI 10.3115/v1/D14-1181
   Kingma D. P., 2014, arXiv
   Lan RS, 2022, IEEE T INTELL TRANSP, V23, P25236, DOI 10.1109/TITS.2022.3213320
   Li Z., 2015, 21 INT C DISTRIBUTED, P199, DOI DOI 10.18293/DMS2015-005
   Liao L, 2023, IEEE T CIRC SYST VID, V33, P920, DOI 10.1109/TCSVT.2022.3203247
   Lin QB, 2020, NEUROCOMPUTING, V396, P113, DOI 10.1016/j.neucom.2020.02.043
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Mikolov T., 2013, Advances in Neural Information Processing Systems, P3111
   Movshovitz-Attias Y, 2017, IEEE I CONF COMP VIS, P360, DOI 10.1109/ICCV.2017.47
   Peng Y, 2013, P 27 AAAI C ARTIFICI
   Peng YX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3284750
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Peng Yuxin, 2016, IJCAI, P3846
   Qian Q, 2019, IEEE I CONF COMP VIS, P6459, DOI 10.1109/ICCV.2019.00655
   Radford A, 2021, PR MACH LEARN RES, V139
   Ranjan V, 2015, IEEE I CONF COMP VIS, P4094, DOI 10.1109/ICCV.2015.466
   Rashtchian C., 2010, P NAACL HLT 2010 WOR, P139
   Rasiwasia N, 2007, IEEE T MULTIMEDIA, V9, P923, DOI 10.1109/TMM.2007.900138
   Rasiwasia N, 2014, JMLR WORKSH CONF PRO, V33, P823
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sohn K, 2016, ADV NEUR IN, V29
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Su SP, 2019, IEEE I CONF COMP VIS, P3027, DOI 10.1109/ICCV.2019.00312
   Tan Z, 2023, Semantic pre-alignment and ranking learning with unified framework for cross-modal retrieval
   Teh Eu Wern, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P448, DOI 10.1007/978-3-030-58586-0_27
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang C, 2015, PROC INT C TOOLS ART, P234, DOI 10.1109/ICTAI.2015.45
   Wang WR, 2015, PR MACH LEARN RES, V37, P1083
   Wang X, 2019, PROC CVPR IEEE, P5017, DOI 10.1109/CVPR.2019.00516
   Wei YC, 2017, IEEE T CYBERNETICS, V47, P449, DOI 10.1109/TCYB.2016.2519449
   Wu F, 2020, PATTERN RECOGN, V104, DOI 10.1016/j.patcog.2020.107335
   Xu X, 2019, WORLD WIDE WEB, V22, P657, DOI 10.1007/s11280-018-0541-x
   Yang ZB, 2022, IEEE WINT CONF APPL, P449, DOI 10.1109/WACV51458.2022.00052
   Yuan L, 2020, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR42600.2020.00315
   Zhai XH, 2014, IEEE T CIRC SYST VID, V24, P965, DOI 10.1109/TCSVT.2013.2276704
   Zhai XH, 2012, INT CONF ACOUST SPEE, P2337, DOI 10.1109/ICASSP.2012.6288383
   Zhang CY, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3412847
   Zhang H, 2013, NEUROCOMPUTING, V119, P10, DOI 10.1016/j.neucom.2012.03.033
   Zhen LL, 2019, PROC CVPR IEEE, P10386, DOI 10.1109/CVPR.2019.01064
NR 64
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD MAR
PY 2024
VL 13
IS 1
AR 5
DI 10.1007/s13735-023-00316-2
PG 16
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FJ9P2
UT WOS:001145524700001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Tang, QS
   Chen, YL
   Zhao, MH
   Min, ST
   Jiang, WM
AF Tang, Qingsong
   Chen, Yingli
   Zhao, Minghui
   Min, Shitong
   Jiang, Wuming
TI DAABNet: depth-wise asymmetric attention bottleneck for real-time
   semantic segmentation
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Asymmetric network; Strip pooling attention; Lightweight network;
   Real-time semantic segmentation
ID NETWORK
AB With the increasing demand for the real-world applications such as autonomous driving and video surveillance, lightweight semantic segmentation methods achieving good trade-offs in terms of parameter size, speed and accuracy have attracted more and more attention. In this context, we propose a novel real-time semantic segmentation model. First, we design a two-branch depth-wise asymmetric attention bottleneck (DAAB) based on residual network to reduce the number of parameters and improve the inference speed. Particularly, an attention refinement module (ARM) is added in the DAAB module to make the information extracted from the two branches complement each other. Second, we design a strip pooling attention (SPA) module which combines the strip pooling module and the attention mechanism to pay more attention to strip-shaped objects and to establish long-range dependencies between discrete distributed regions, so that to address the problem of poor segmentation of strip shape objects. In addition, we also fuse information from different stages to compensate for the loss of spatial information, thus improving the ability of the network to segment small objects. Experiments on CityScapes and CamVid dataset demonstrate that the proposed method achieves impressive trade-offs in terms of parameter size, speed and accuracy. Code is available at: https://github.com/mhhz/DAABnet1.
C1 [Tang, Qingsong; Chen, Yingli; Zhao, Minghui; Min, Shitong] Northeastern Univ, Coll Sci, Dept Math, Sanhao St, Shenyang 110819, Liaoning, Peoples R China.
   [Jiang, Wuming] EyeCool Technol Co Ltd, Zhongguancun, Beijing 200050, Peoples R China.
C3 Northeastern University - China
RP Tang, QS (corresponding author), Northeastern Univ, Coll Sci, Dept Math, Sanhao St, Shenyang 110819, Liaoning, Peoples R China.
EM tangqs@mail.neu.edu.cn; 3382441236@qq.com; zhaominghui86@163.com;
   18842397548@163.com; jiangwuming@eyecool.cn
CR Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Elhassan MAM, 2021, EXPERT SYST APPL, V183, DOI 10.1016/j.eswa.2021.115090
   Fan MY, 2021, PROC CVPR IEEE, P9711, DOI 10.1109/CVPR46437.2021.00959
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Gao GW, 2022, IEEE T INTELL TRANSP, V23, P25489, DOI 10.1109/TITS.2021.3098355
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hou QB, 2020, PROC CVPR IEEE, P4002, DOI 10.1109/CVPR42600.2020.00406
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu XG, 2023, IMAGE VISION COMPUT, V139, DOI 10.1016/j.imavis.2023.104823
   Hu XG, 2022, COMPUT GRAPH-UK, V109, P55, DOI 10.1016/j.cag.2022.10.002
   Hu XG, 2022, APPL INTELL, V52, P580, DOI 10.1007/s10489-021-02446-8
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Kundu S, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2225, DOI 10.1109/ICASSP39728.2021.9415117
   Li G, 2019, Arxiv, DOI arXiv:1907.11357
   Li G, 2020, IEEE ACCESS, V8, P27495, DOI 10.1109/ACCESS.2020.2971760
   Li HC, 2019, PROC CVPR IEEE, P9514, DOI 10.1109/CVPR.2019.00975
   Li YQ, 2023, NEURAL PROCESS LETT, V55, P873, DOI 10.1007/s11063-022-10957-9
   Lo S, 2019, ACM Multimedia Asia, P1
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu MX, 2022, IEEE T INTELL TRANSP, V23, P3522, DOI 10.1109/TITS.2020.3037727
   Mazhar S, 2023, ENG APPL ARTIF INTEL, V126, DOI 10.1016/j.engappai.2023.107086
   Mehta S, 2018, LECT NOTES COMPUT SC, V11214, P561, DOI 10.1007/978-3-030-01249-6_34
   Minaee S, 2022, IEEE T PATTERN ANAL, V44, P3523, DOI 10.1109/TPAMI.2021.3059968
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Orsic M, 2019, PROC CVPR IEEE, P12599, DOI 10.1109/CVPR.2019.01289
   Poudel RPK, 2018, Arxiv, DOI arXiv:1805.04554
   Paszke A, 2016, Arxiv, DOI arXiv:1606.02147
   Poudel R. P. K., 2019, arXiv
   Romera E, 2018, IEEE T INTELL TRANSP, V19, P263, DOI 10.1109/TITS.2017.2750080
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Y, 2019, IEEE IMAGE PROC, P1860, DOI [10.1109/icip.2019.8803154, 10.1109/ICIP.2019.8803154]
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu TY, 2021, IEEE T IMAGE PROCESS, V30, P1169, DOI 10.1109/TIP.2020.3042065
   Wu ZF, 2017, Arxiv, DOI arXiv:1712.00213
   Yang ZG, 2021, IEEE T INTELL TRANSP, V22, P5508, DOI 10.1109/TITS.2020.2987816
   Yang ZG, 2020, IEEE T IMAGE PROCESS, V29, P5175, DOI 10.1109/TIP.2020.2976856
   Yu CQ, 2021, INT J COMPUT VISION, V129, P3051, DOI 10.1007/s11263-021-01515-2
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang XT, 2019, IEEE T IND INFORM, V15, P1183, DOI 10.1109/TII.2018.2849348
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou Q, 2020, APPL SOFT COMPUT, V96, DOI 10.1016/j.asoc.2020.106682
   Zhuang MX, 2021, NEUROCOMPUTING, V459, P349, DOI 10.1016/j.neucom.2021.07.019
NR 59
TC 1
Z9 1
U1 18
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD MAR
PY 2024
VL 13
IS 1
AR 12
DI 10.1007/s13735-024-00321-z
PG 16
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IT5O6
UT WOS:001168600700001
DA 2024-08-05
ER

PT J
AU Yuan, X
   Shan, SH
   Huo, YW
   Jiang, JK
   Wu, S
AF Yuan, Xiang
   Shan, Shihao
   Huo, Yuwen
   Jiang, Junkai
   Wu, Song
TI Text-assisted attention-based cross-modal hashing
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Cross-modal retrieval; Deep hashing; Attention mechanism; Cross fusion
AB As one of the hottest research topics in multimedia information retrieval, cross-modal hashing has drawn widespread attention in the past decades. How to minimize the semantic gap of heterogeneous data and accurately calculate the similarity of cross-modal data is a key challenge for this task. A paradigm for tackling this problem is to map features of multi-modal data into common space. However, these approaches lack inter-modal information interaction and may not achieve satisfactory results. To overcome this problem, we propose a novel text-assisted attention-based cross-modal hashing (TAACH) method in this paper. Firstly, TAACH relies on LabelNet supervision to guide the learning of hash functions for each modality. In addition, a novel text-assisted attention mechanism is designed in our TAACH to densely integrate text features into image features, perceiving their spatial correlation and enhancing the consistency of image and text knowledge. Extensive experiments on four benchmark datasets show the effectiveness of our proposed TAACH, and it also achieves competitive performance compared to state-of-the-art methods. The source code is available at https://github.com/SWU-CS-MediaLab/TAACH.
C1 [Yuan, Xiang; Shan, Shihao; Huo, Yuwen; Wu, Song] Southwest Univ, Coll Comp & Informat Sci, Chongqing 400715, Peoples R China.
   [Jiang, Junkai] Deakin Univ, Fac Sci Engn & Built Environm, Melbourne 3125, Australia.
C3 Southwest University - China; Deakin University
RP Wu, S (corresponding author), Southwest Univ, Coll Comp & Informat Sci, Chongqing 400715, Peoples R China.
EM songwuswu@swu.edu.cn
FU the Fundamental Research Funds for the Central Universities
   [SWU-KT22032]; Fundamental Research Funds for the Central Universities,
   China
FX This work was supported by the Fundamental Research Funds for the
   Central Universities, China (SWU-KT22032).
CR Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bronstein MM, 2010, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2010.5539928
   Cao Y, 2017, Arxiv, DOI arXiv:1602.06697
   Cao Y, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1445, DOI 10.1145/2939672.2939812
   Chua TS, 2009, P ACM INT C IM VID R, P1
   Ding GG, 2016, IEEE T IMAGE PROCESS, V25, P5427, DOI 10.1109/TIP.2016.2607421
   Ding K, 2017, IEEE T MULTIMEDIA, V19, P571, DOI 10.1109/TMM.2016.2625747
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   Hovy E., 2016, P 2016 C N AM CHAPT, P1480, DOI DOI 10.18653/V1/N16-1174
   Huang Z., 2013, PROC ACM SIGMOD IN, P785
   Huiskes M.J., 2008, P ACM INT C MULT INF, P39, DOI 10.1145/1460096
   Escalante HJ, 2010, COMPUT VIS IMAGE UND, V114, P419, DOI 10.1016/j.cviu.2009.03.008
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar S., 2011, INT JOINT C ART INT, P1360, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-230
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Ma XH, 2020, IEEE T MULTIMEDIA, V22, P3101, DOI 10.1109/TMM.2020.2969792
   Mandal D, 2017, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR.2017.282
   Nair V., 2010, ICML, P807
   Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068
   Shen YM, 2017, IEEE I CONF COMP VIS, P4117, DOI 10.1109/ICCV.2017.441
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang D, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3890
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang K., 2016, PREPRINT
   Wang XZ, 2020, NEUROCOMPUTING, V400, P255, DOI 10.1016/j.neucom.2020.03.019
   Wu F, 2014, IEEE T MULTIMEDIA, V16, P427, DOI 10.1109/TMM.2013.2291214
   Yang EK, 2017, AAAI CONF ARTIF INTE, P1618
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang X, 2018, LECT NOTES COMPUT SC, V11219, P614, DOI 10.1007/978-3-030-01267-0_36
   Zhou JL, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P415
   Zou XT, 2022, KNOWL-BASED SYST, V239, DOI 10.1016/j.knosys.2021.107927
   Zou XT, 2021, SIGNAL PROCESS-IMAGE, V93, DOI 10.1016/j.image.2020.116131
NR 36
TC 0
Z9 0
U1 10
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD MAR
PY 2024
VL 13
IS 1
AR 3
DI 10.1007/s13735-023-00311-7
PG 17
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EF9T3
UT WOS:001137632500002
DA 2024-08-05
ER

PT J
AU Wang, ZW
   Zhang, DL
   Hu, ZK
AF Wang, Zhiwen
   Zhang, Donglin
   Hu, Zhikai
TI LSECA: local semantic enhancement and cross aggregation for video-text
   retrieval
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Video-text retrieval; Semantic enhancement; Cross aggregation;
   Multi-grained contrast
AB Recently video retrieval based on the pre-training models (e.g., CLIP) has achieved outstanding success. To further improve the search performance, most existing methods usually utilize the multi-grained contrastive fine tuning scheme. For example, frame features and word features are taken as fine-grained representations, aggregate features for frame features and [CLS] token for textual side are used as global representations. However, the above scheme still remains challenging. There are redundant and noise information in the raw output features of pre-training encoders, leading to suboptimal retrieval performance. Besides, a video usually correlates several text descriptions, while video embedding is fixed in previous works, which may also reduce the search performance. To conquer these problems, we propose a novel video-text retrieval model, named Local Semantic Enhancement and Cross Aggregation (LSECA). To be specific, we design a local semantic enhancement scheme, which utilizes global feature for video and keyword information for text to augment fine-grained semantic representations. Moreover, the cross aggregation module is proposed to enhance the interaction between video and text modalities. In this way, the local semantic enhancement scheme can increase the related representation of modalities and the developed cross aggregation module can make the representations of texts and videos more uniform. Extensive experiments on three popular text-video retrieval benchmark datasets demonstrate that our LSECA outperforms several state-of-the-art methods.
C1 [Wang, Zhiwen; Zhang, Donglin] Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, Wuxi 214122, Jiangsu, Peoples R China.
   [Hu, Zhikai] Hong Kong Baptist Univ, Dept Comp Sci, Hong Kong 999077, Peoples R China.
C3 Jiangnan University; Hong Kong Baptist University
RP Zhang, DL (corresponding author), Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, Wuxi 214122, Jiangsu, Peoples R China.
EM 6223112033@stu.jiangnan.edu.cn; zhangdlin@jiangnan.edu.cn;
   cszkhu@comp.hkbu.edu.hk
FU National Natural Science Foundation of China [62202204]; NSFC
   [JUSRP123032]; Fundamental Research Funds for the Central Universities
FX This work was supported by NSFC (62202204) and the Fundamental Research
   Funds for the Central Universities (JUSRP123032).DAS:The MSRVTT dataset
   is given in [27]. The MSVD is given in [28]. The LSMDC is given in [29].
   The source code supporting the results of this study are available upon
   request from the authors.
CR Amrani E, 2021, AAAI CONF ARTIF INTE, V35, P6644
   Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Bain M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1708, DOI 10.1109/ICCV48922.2021.00175
   Bertasius G, 2021, PR MACH LEARN RES, V139
   Bogolin SV, 2022, PROC CVPR IEEE, P5184, DOI 10.1109/CVPR52688.2022.00513
   Chen L, 2024, IEEE T CIRC SYST VID, V34, P6559, DOI 10.1109/TCSVT.2024.3360530
   Cheng Xing, 2021, arXiv
   Croitoru I, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11563, DOI 10.1109/ICCV48922.2021.01138
   Deng C, 2023, P IEEE CVF INT C COM, P15648
   Dong JF, 2019, PROC CVPR IEEE, P9338, DOI 10.1109/CVPR.2019.00957
   Dzabraev M, 2021, IEEE COMPUT SOC CONF, P3349, DOI 10.1109/CVPRW53098.2021.00374
   Fang B, 2023, P IEEE CVF INT C COM, P13723
   Fang H., 2021, arXiv
   Gabeur Valentin, 2020, COMPUTER VISION ECCV, DOI [10.48550/arXiv.2007.10639, DOI 10.48550/ARXIV.2007.10639]
   Ge YY, 2022, PROC CVPR IEEE, P16146, DOI 10.1109/CVPR52688.2022.01569
   Gorti Satya Krishna, 2022, P IEEE CVF C COMP VI, P5006
   He F, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1359, DOI 10.1145/3404835.3462927
   Jia C, 2021, PR MACH LEARN RES, V139
   Lei J, 2021, PROC CVPR IEEE, P7327, DOI 10.1109/CVPR46437.2021.00725
   Li LJ, 2020, Arxiv, DOI arXiv:2005.00200
   Lin C., 2022, Advances in Neural Information Processing Systems (NeurIPS), V35, P38655
   Liu S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11895, DOI 10.1109/ICCV48922.2021.01170
   Liu Y, 2020, Arxiv, DOI arXiv:1907.13487
   Liu YQ, 2022, LECT NOTES COMPUT SC, V13674, P319, DOI 10.1007/978-3-031-19781-9_19
   Luo HS, 2022, NEUROCOMPUTING, V508, P293, DOI 10.1016/j.neucom.2022.07.028
   Ma YW, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, DOI 10.1145/3503161.3547910
   Mithun NC, 2019, INT J MULTIMED INF R, V8, P3, DOI 10.1007/s13735-018-00166-3
   Mithun NC, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/3206025.3206064
   Patrick M, 2021, Arxiv, DOI arXiv:2010.02824
   Radford A, 2021, PR MACH LEARN RES, V139
   Rohrbach A, 2015, LECT NOTES COMPUT SC, V9358, P209, DOI 10.1007/978-3-319-24947-6_17
   Sharma P, 2019, PREPRINT, DOI [10.20944/preprints201908.0073.v1, DOI 10.20944/PREPRINTS201908.0073.V1]
   Song X, 2022, IEEE T MULTIMEDIA, V24, P2914, DOI 10.1109/TMM.2021.3090595
   Sun C, 2019, IEEE I CONF COMP VIS, P7463, DOI 10.1109/ICCV.2019.00756
   Tian K, 2024, P IEEE CVF C COMP VI
   Tong XY, 2020, IEEE T BIG DATA, V6, P507, DOI 10.1109/TBDATA.2019.2948924
   Wang JC, 2023, INT J MULTIMED INF R, V12, DOI 10.1007/s13735-023-00298-1
   Wang Q., 2022, arXiv
   Wang XH, 2021, PROC CVPR IEEE, P5075, DOI 10.1109/CVPR46437.2021.00504
   Wang Z., 2023, INT C COMPUTER VISIO, P2816
   Wu Z, 2017, Frontiers of multimedia research, P3, DOI [DOI 10.1145/3122865.3122867, 10.1145/3122865.3122867]
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Yang JW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11542, DOI 10.1109/ICCV48922.2021.01136
   Yao LW, 2021, Arxiv, DOI arXiv:2111.07783
   Yu JH, 2022, Arxiv, DOI arXiv:2205.01917
   Yu Y, 2018, LECT NOTES COMPUT SC, V11211, P487, DOI 10.1007/978-3-030-01234-2_29
   Yu Y, 2017, PROC CVPR IEEE, P3261, DOI 10.1109/CVPR.2017.347
   Zhang D., 2021, IEEE Trans Knowl Data Eng, V35, P1365
   Zhang DL, 2023, IEEE T KNOWL DATA EN, V35, P6461, DOI 10.1109/TKDE.2022.3159131
   Zhang DL, 2021, LECT NOTES COMPUT SC, V13020, P524, DOI 10.1007/978-3-030-88007-1_43
   Zhang DL, 2021, PATTERN RECOGN LETT, V151, P19, DOI 10.1016/j.patrec.2021.07.018
   Zhang DL, 2021, INT C PATT RECOG, P4845, DOI 10.1109/ICPR48806.2021.9412497
   Zhang DL, 2022, IEEE T CYBERNETICS, V52, P5947, DOI 10.1109/TCYB.2020.3032017
   Zhao S, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P970, DOI 10.1145/3477495.3531950
NR 54
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD SEP
PY 2024
VL 13
IS 3
AR 30
DI 10.1007/s13735-024-00335-7
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZG1F1
UT WOS:001274043200002
DA 2024-08-05
ER

PT J
AU Zhao, P
   Wang, QC
   Yin, YL
AF Zhao, Peng
   Wang, Qiangchang
   Yin, Yilong
TI DSPformer: discovering semantic parts with token growth and clustering
   for zero-shot learning
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Zero-shot Learning; Transformer; Clustering; Token growth
AB Transformers have achieved success in many computer vision tasks, but their potential in Zero-Shot Learning (ZSL) has yet to be fully explored. In this paper, a Transformer architecture is developed, termed DSPformer, which can discover semantic parts by token growth and clustering. This is achieved through two proposed methods: Adaptive Token Growth and Semantic Part Clustering. Firstly, it is observed that the background may distract models, causing the model to rely on irrelevant regions to make decisions. To alleviate this issue, the ATG is proposed to locate discriminative foreground regions and remove meaningless and even noisy backgrounds. Secondly, semantically similar parts may be distributed into different tokens. To address this problem, the SPC is proposed to group semantically consistent parts by token clustering. Extensive experiments on several challenging datasets demonstrate the effectiveness of the proposed DSPformer.
C1 [Zhao, Peng; Wang, Qiangchang; Yin, Yilong] Shandong Univ, Sch Software, Shunhua Rd, Jinan 250101, Shandong, Peoples R China.
C3 Shandong University
RP Wang, QC; Yin, YL (corresponding author), Shandong Univ, Sch Software, Shunhua Rd, Jinan 250101, Shandong, Peoples R China.
EM pzhao@mail.sdu.edu.cn; qiangchang.wang@gmail.com; ylyin@sdu.edu.cn
FU Natural Science Foundation of Shandong province, China [ZR2023QF124];
   Shandong Excellent Young Scientists Fund (Oversea) [2024HWYQ-027]; Young
   Scholars Program of Shandong University; Fundamental Research Funds of
   Shandong University
FX This work was supported in part by the Natural Science Foundation of
   Shandong province, China under Grant ZR2023QF124, in part by the
   Shandong Excellent Young Scientists Fund (Oversea) under Grant
   2024HWYQ-027, in part by the Young Scholars Program of Shandong
   University, and in part by the Fundamental Research Funds of Shandong
   University.
CR Abnar S, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P4190
   Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   Alamri F., 2021, DAGM German conference on pattern recognition, P467, DOI [10.1007/978-3-030-92659-530, DOI 10.1007/978-3-030-92659-530]
   Alamri F, 2021, Arxiv, DOI arXiv:2108.00045
   Annadani Y, 2018, PROC CVPR IEEE, P7603, DOI 10.1109/CVPR.2018.00793
   Caron M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9630, DOI 10.1109/ICCV48922.2021.00951
   Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4
   Chen Dubing, 2022, IJCAI, P813
   Chen SM, 2022, AAAI CONF ARTIF INTE, P330
   Chen SM, 2022, PROC CVPR IEEE, P7602, DOI 10.1109/CVPR52688.2022.00746
   Chen SM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P122, DOI 10.1109/ICCV48922.2021.00019
   Chen Shiming, 2021, NEURIPS, V34, P16622
   Chen Z., 2023, 37 AAAI C ARTIFICIAL, V37, P405
   Chen Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8692, DOI 10.1109/ICCV48922.2021.00859
   Cheng D, 2023, PATTERN RECOGN, V137, DOI 10.1016/j.patcog.2022.109270
   Huynh D, 2020, PROC CVPR IEEE, P4482, DOI 10.1109/CVPR42600.2020.00454
   Dong XY, 2022, PROC CVPR IEEE, P12114, DOI 10.1109/CVPR52688.2022.01181
   Dosovitskiy A., 2021, ICLR
   Du MJ, 2016, KNOWL-BASED SYST, V99, P135, DOI 10.1016/j.knosys.2016.02.001
   Fang JM, 2022, PROC CVPR IEEE, P12053, DOI 10.1109/CVPR52688.2022.01175
   Fang ZY, 2022, AAAI CONF ARTIF INTE, P6605
   Feng YG, 2022, PROC CVPR IEEE, P9336, DOI 10.1109/CVPR52688.2022.00913
   Frome A., 2013, Advances in neural information processing systems, V26
   Fu YW, 2015, IEEE T PATTERN ANAL, V37, P2332, DOI 10.1109/TPAMI.2015.2408354
   Gao W, 2023, FRONT COMPUT SCI-CHI, V17, DOI 10.1007/s11704-021-1237-4
   Ge HW, 2021, FRONT COMPUT SCI-CHI, V15, DOI 10.1007/s11704-020-9002-7
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guo-Sen Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P562, DOI 10.1007/978-3-030-58548-8_33
   Han ZY, 2021, PROC CVPR IEEE, P2371, DOI 10.1109/CVPR46437.2021.00240
   He J, 2021, AAAI
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Hu YQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4239, DOI 10.1145/3474085.3475561
   Jia BB, 2023, FRONT COMPUT SCI-CHI, V17, DOI 10.1007/s11704-023-3076-y
   Joshi K, 2020, INT J MULTIMED INF R, V9, P231, DOI 10.1007/s13735-020-00200-3
   Kingma D. P., 2014, arXiv
   Kong X, 2022, PROC CVPR IEEE, P9296, DOI 10.1109/CVPR52688.2022.00909
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Liang Y., 2022, ICLR
   Liu M, 2023, PROC CVPR IEEE, P15337, DOI 10.1109/CVPR52729.2023.01472
   Liu Y, 2021, PROC CVPR IEEE, P3793, DOI 10.1109/CVPR46437.2021.00379
   Liu Y, 2019, IEEE I CONF COMP VIS, P6697, DOI 10.1109/ICCV.2019.00680
   Liu Z, 2022, PROC CVPR IEEE, P11999, DOI 10.1109/CVPR52688.2022.01170
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Meng LT, 2022, INT J MULTIMED INF R, V11, P525, DOI 10.1007/s13735-022-00258-1
   Naeem MF, 2022, NEURIPS
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shaobo Min, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12661, DOI 10.1109/CVPR42600.2020.01268
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang HY, 2021, ADV NEUR IN, V34
   Wang QC, 2023, Arxiv, DOI arXiv:2306.01929
   Wang XY, 2022, INT J MULTIMED INF R, V11, P149, DOI 10.1007/s13735-022-00231-y
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xie GS, 2019, PROC CVPR IEEE, P9376, DOI 10.1109/CVPR.2019.00961
   Xu W., 2020, Adv. Neural Inf. Process. Syst, P21969
   Yue ZQ, 2021, PROC CVPR IEEE, P15399, DOI 10.1109/CVPR46437.2021.01515
   Zhang CJ, 2022, IEEE T IMAGE PROCESS, V31, P3056, DOI 10.1109/TIP.2021.3120319
   Zhu PK, 2019, PROC CVPR IEEE, P2990, DOI 10.1109/CVPR.2019.00311
NR 62
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD SEP
PY 2024
VL 13
IS 3
AR 27
DI 10.1007/s13735-024-00336-6
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WT3X4
UT WOS:001257096900001
DA 2024-08-05
ER

PT J
AU Ji, Z
   Kong, XY
   Wang, X
   Liu, XY
AF Ji, Zhong
   Kong, Xiangyu
   Wang, Xuan
   Liu, Xiyao
TI Relevance equilibrium network for cross-domain few-shot learning
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Few-shot learning; Cross-domain; Image classification; Meta-learning
AB Cross-domain few-shot learning (CD-FSL) aims to develop a robust and generalizable model from a data-abundant source domain and apply it to the data-scarce target domain. An intrinsic challenge in CD-FSL is the domain shift problem, often manifested as a discrepancy in data distributions. This work addresses the domain shift problem from a model learning perspective, characterizing it in two specific aspects: over-sensitivity and excessive invariance. Specifically, we introduce a novel Relevance Equilibrium Network (ReqNet) to enhance the generalizability of few-shot models on target domain tasks. In particular, we design a Style Augmentation (StyleAug) module to diversify low-level visual styles of feature representations, alleviating the model's over-sensitivity to class- or task-irrelevant changes. Furthermore, to mitigate the excessive invariance to features relevant to the class and task, we devise a Task Context Modeling (TCM) module that strategically employs non-local operations to incorporate comprehensive task-level information. Extensive experiments and ablation studies are conducted on eight datasets to demonstrate the competitive performance of our proposed ReqNet.
C1 [Ji, Zhong; Kong, Xiangyu; Wang, Xuan] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Liu, Xiyao] Chinese Acad Sci, Shenyang Inst Automation, State Key Lab Robot, Shenyang 110016, Peoples R China.
   [Liu, Xiyao] Chinese Acad Sci, Inst Robot & Intelligent Mfg, Shenyang 110169, Peoples R China.
C3 Tianjin University; Chinese Academy of Sciences; Shenyang Institute of
   Automation, CAS; Chinese Academy of Sciences
RP Wang, X (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM jizhong@tju.edu.cn; kongxy99@tju.edu.cn; wang_xuan@tju.edu.cn;
   liuxiyao@sia.cn
FU National Natural Science Foundation of China [62176178, 62303447];
   National Natural Science Foundation of China [BX20230399]; Postdoctoral
   Innovation Talents Support Program [2023M743702]; China Postdoctoral
   Science Foundation
FX This work has been supported by the National Natural Science Foundation
   of China under Grant 62176178 and 62303447, the Postdoctoral Innovation
   Talents Support Program under Grant BX20230399, and the China
   Postdoctoral Science Foundation under Grant 2023M743702.
CR Antoniou A, 2018, Arxiv, DOI arXiv:1711.04340
   Behrmann J., 2019, P 7 INT C LEARN REPR
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Chen W., 2019, INT C LEARNING REPRE
   Chi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12200, DOI 10.1109/CVPR42600.2020.01222
   Das R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9010, DOI 10.1109/ICCV48922.2021.00890
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Finn C, 2017, PR MACH LEARN RES, V70
   Fu YQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5326, DOI 10.1145/3474085.3475655
   Garcia Victor., 2017, 6 INT C LEARN REPR
   Han-Jia Ye, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8805, DOI 10.1109/CVPR42600.2020.00883
   Hariharan B, 2017, IEEE I CONF COMP VIS, P3037, DOI 10.1109/ICCV.2017.328
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Helber P, 2019, IEEE J-STARS, V12, P2217, DOI 10.1109/JSTARS.2019.2918242
   Higgins I., 2017, INT C LEARN REPR
   Hou RB, 2019, ADV NEUR IN, V32
   Hu YX, 2022, LECT NOTES COMPUT SC, V13680, P20, DOI 10.1007/978-3-031-20044-1_2
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Islam A, 2021, P ADV NEUR INF PROC, P3584
   Ji Z, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3260121
   Ji Z, 2022, IEEE T IMAGE PROCESS, V31, P1520, DOI 10.1109/TIP.2022.3143005
   Jo J, 2017, arXiv
   Kang D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8802, DOI 10.1109/ICCV48922.2021.00870
   Koch G., 2015, ICML DEEP LEARN WORK, V2, P1
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Liang HW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9404, DOI 10.1109/ICCV48922.2021.00929
   Liu XY, 2022, IEEE T CYBERNETICS, V52, P7852, DOI 10.1109/TCYB.2021.3049537
   Liu XY, 2023, KNOWL-BASED SYST, V265, DOI 10.1016/j.knosys.2023.110358
   Mahajan D, 2021, PR MACH LEARN RES, V139
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419
   Nichol A, 2018, Arxiv, DOI arXiv:1803.02999
   Oreshkin BN, 2018, ADV NEUR IN, V31
   Pezeshki M, 2021, ADV NEUR IN, V34
   Phoo C. P., 2021, INT C LEARN REPR
   Ravi S., 2017, INT C LEARNING REPRE, P1
   Rusu A.A., 2019, ICLR
   Snell J, 2017, ADV NEUR IN, V30
   Sun JM, 2021, INT C PATT RECOG, P7609, DOI 10.1109/ICPR48806.2021.9412941
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Tschandl P, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.161
   Tseng Hung-Yu, 2019, INT C LEARN REPR
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O., 2016, Advances in neural information processing systems, V29
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang Haoqing, 2021, P 30 INT JOINT C ART, P1075
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang XS, 2017, PROC CVPR IEEE, P3462, DOI 10.1109/CVPR.2017.369
   Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760
   Xin S, 2023, AAAI C ART INT, V37, P10519
   Yonglong Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P266, DOI 10.1007/978-3-030-58568-6_16
   Yuan W, 2022, AAAI CONF ARTIF INTE, P3215
   Yunhui Guo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P124, DOI 10.1007/978-3-030-58583-9_8
   Zhong Z., 2022, NeurIPS, V35, P338
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhou F, 2023, PROC CVPR IEEE, P20061, DOI 10.1109/CVPR52729.2023.01921
   Zhou K., 2021, INT C LEARN REPR
NR 58
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD JUN
PY 2024
VL 13
IS 2
AR 21
DI 10.1007/s13735-024-00333-9
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RR4T6
UT WOS:001229383100001
DA 2024-08-05
ER

PT J
AU Liu, S
   Shan, SH
   Xiao, GQ
   Gao, XB
   Wu, S
AF Liu, Shan
   Shan, Shihao
   Xiao, Guoqiang
   Gao, Xinbo
   Wu, Song
TI Image enhancement with bi-directional normalization and color
   attention-guided generative adversarial networks
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Image enhancement; Image retouching; Generative adversarial network;
   Unsupervised learning
AB The image enhancement aims to improve the quality of images from contrast, detail, and color perspectives by adjusting the color of an image to match the distribution of the high-quality domain. Since the images captured by portable devices often suffer from noise and color bias, this paper designed a novel bi-directional normalization and color attention-guided generative adversarial network (BNCAGAN) for unsupervised image enhancement. Specifically, the bi-directional normalization generator is built upon a feature encoder, an auxiliary attention classifier (AAC), a bi-directional normalization residual (BNR) module, and a feature fusion decoder. With the aid of the AAC and BNR modules, the generator can flexibly control the global style, local details, and color transformation constraints from low-quality and high-quality domains. To improve the visual effect, a spatial color correction module is proposed to assist the multi-scale discriminator in focusing on color fidelity. In addition, a mixed loss, including a content retention loss and an identity fidelity loss, can maintain the structural features to fit the high-quality domain distribution. Extensive experiments on the MIT-Adobe FiveK dataset and DSLR photograph enhancement dataset exhibit that our BNCAGAN outperforms existing methods, and it can improve both the authenticity and naturalness of low-quality images and thus can be widely used for image retrieval preprocessing to improve the understanding of image semantics. The source code is available at https://github.com/SWU-CS-MediaLab/BNCAGAN.
C1 [Liu, Shan; Shan, Shihao; Xiao, Guoqiang; Wu, Song] Southwest Univ, Coll Comp & Informat Sci, Chongqing 400715, Peoples R China.
   [Gao, Xinbo] Chongqing Univ Posts & Telecommun, Coll Comp Sci & Technol, Chongqing 400065, Peoples R China.
C3 Southwest University - China; Chongqing University of Posts &
   Telecommunications
RP Wu, S (corresponding author), Southwest Univ, Coll Comp & Informat Sci, Chongqing 400715, Peoples R China.
EM songwuswu@swu.edu.cn
FU Fundamental Research Funds for the Central Universities
FX No Statement Available
CR Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Chen YS, 2018, PROC CVPR IEEE, P6306, DOI 10.1109/CVPR.2018.00660
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Ignatov A, 2017, IEEE I CONF COMP VIS, P3297, DOI 10.1109/ICCV.2017.355
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Jingwen He, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P679, DOI 10.1007/978-3-030-58601-0_40
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Jolicoeur-Martineau A., 2019, P ICLR
   Kim J., 2020, 8 INT C LEARN REPR I
   Lee C, 2013, IEEE T IMAGE PROCESS, V22, P5372, DOI 10.1109/TIP.2013.2284059
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Liang J, 2022, IEEE Trans. Circuits Syst. Video Technol.
   Liang J, 2021, PROC CVPR IEEE, P9387, DOI 10.1109/CVPR46437.2021.00927
   Liming Jiang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P206, DOI 10.1007/978-3-030-58580-8_13
   Ma L, 2022, PROC CVPR IEEE, P5627, DOI 10.1109/CVPR52688.2022.00555
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Nam H, 2018, ADV NEUR IN, V31
   Ni ZK, 2020, IEEE T IMAGE PROCESS, V29, P9140, DOI 10.1109/TIP.2020.3023615
   PIZER SM, 1987, COMPUT VISION GRAPH, V39, P355, DOI 10.1016/S0734-189X(87)80186-X
   Radford L., 2016, Unsupervised representation learning with deep convolutional generative adversarial networks, P1
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Song YD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4106, DOI 10.1109/ICCV48922.2021.00409
   Sun X., 2021, IJCAI
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Ulyanov D, 2017, Arxiv, DOI arXiv:1607.08022
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang XZ, 2020, NEUROCOMPUTING, V400, P255, DOI 10.1016/j.neucom.2020.03.019
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu S, 2017, NEUROCOMPUTING, V257, P5, DOI 10.1016/j.neucom.2016.12.070
   Zamir Syed Waqas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P492, DOI 10.1007/978-3-030-58595-2_30
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zou XT, 2021, SIGNAL PROCESS-IMAGE, V93, DOI 10.1016/j.image.2020.116131
NR 47
TC 0
Z9 0
U1 6
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD MAR
PY 2024
VL 13
IS 1
AR 1
DI 10.1007/s13735-023-00310-8
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EC7V3
UT WOS:001136790000001
DA 2024-08-05
ER

PT J
AU Achinek, DN
   Shehu, IS
   Athuman, AM
   Fu, XP
AF Achinek, Divine Njengwie
   Shehu, Ibrahim Shehi
   Athuman, Athuman Mohamed
   Fu, Xianping
TI DAF-Net: dense attention feature pyramid network for multiscale object
   detection
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Object detection; Channel attention; Spatial attention; Convolutional
   neural networks; FPN
AB In recent years, object detection has become one of the most prominent components in computer vision. State-of-the-art object detectors now employ convolutional neural networks (CNNs) techniques alongside other deep neural network techniques to improve detection performance and accuracy. Most of the recent object detectors employ feature pyramid network (FPN) and their variants while others use combinations of attention mechanisms to achieve better performance. The open question is object detectors inconsistency between the lower layer features, their resolution receptive field and semantic information with the upper layers features in detecting objects. Although some researchers have attempted to address this issue, we exploit ideas surrounding the field and proposed a more prominent architecture called dense attention feature pyramid network (DAF-Net) for multiscale object detection. DAF-Net consists of two attention models, the spatial attention model and channel attention model. Different from other attention models, we proposed lightweight attention models which are fully data-driven then implemented a dense connected attention FPN to reduce the model's complexity and resolve the learning of redundant feature maps. First, we developed the two attention models then used only the spatial attention model in the backbone of our network, and finally used both attention models to filter and maintain a steady flow of semantic information from lower layers to improve the model's accuracy and efficiency. Experimental results on underwater images from the National Natural Science Foundation of China (NSFC) (Underwater Image Dataset, National Natural Science Foundation of China (NSFC). Online, retrieved from http://www.cnurpc.org/index.html), MS COCO dataset, and PASCAL VOC dataset indicate higher accuracy and better detection results using the proposed model compared to the benchmark model YOLOX-Darknet53 (Ge in Yolox: Exceeding yolo series in 2021. arXiv preprint arXiv:2107.08430). Our model achieved 70.2mAP, 48.9 mAP, and 83.9 mAP on (NSFC), MS COCO, and PASCAL VOC datasets, respectively, compared with benchmark model 68.9mAP on (NSFC), 47.7mAP on MS COCO, and 82.4mAP on PASCAL VOC.
C1 [Achinek, Divine Njengwie; Shehu, Ibrahim Shehi; Athuman, Athuman Mohamed; Fu, Xianping] Dalian Maritime Univ, Informat Sci & Technol Coll, Dalian 116026, Peoples R China.
   [Fu, Xianping] Pengcheng Lab, Shenzhen 518055, Peoples R China.
C3 Dalian Maritime University
RP Achinek, DN (corresponding author), Dalian Maritime Univ, Informat Sci & Technol Coll, Dalian 116026, Peoples R China.
EM achinekdivine002@dlmu.edu.cn; shehu@dlmu.edu.cn; realathu@dlmu.edu.cn;
   fxp@dlmu.edu.cn
FU National Natural Science Foundation of China [62176037, 61802043];
   National Natural Science Foundation of China [XLYC1908007]; Liaoning
   Revitalization Talents Program Grant [201801728]; Foundation of Liaoning
   Key Research and Development Program [3132016352, 3132020215];
   Fundamental Research Funds for the Central Universities [2018J12GX037,
   2019J11CY001]; Dalian Science and Technology Innovation Fund
FX This work was supported in part by the National Natural Science
   Foundation of China Grant 62176037 and 61802043, by the Liaoning
   Revitalization Talents Program Grant XLYC1908007, by the Foundation of
   Liaoning Key Research and Development Program Grant 201801728, by the
   Fundamental Research Funds for the Central Universities Grant 3132016352
   and Grant 3132020215, by the Dalian Science and Technology Innovation
   Fund 2018J12GX037 and 2019J11CY001.
CR [Anonymous], Underwater Image Dataset
   Birodkar V, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6995, DOI 10.1109/ICCV48922.2021.00693
   Cao JX, 2020, Arxiv, DOI arXiv:2005.11475
   Castrillón M, 2011, MACH VISION APPL, V22, P481, DOI 10.1007/s00138-010-0250-7
   Chaoxu Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12592, DOI 10.1109/CVPR42600.2020.01261
   Chen K, 2019, IEEE T MULTIMEDIA, V21, P86, DOI 10.1109/TMM.2018.2846405
   Chen L.-C., 2018, ECCV, P801, DOI [DOI 10.1007/978-3-030-01234-249, 10.1007/978-3-030-01234-2_49]
   Cheng BW, 2022, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR52688.2022.00135
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Feichtenhofer C, 2017, IEEE I CONF COMP VIS, P3057, DOI 10.1109/ICCV.2017.330
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720
   Glenn J., 2023, Ultralytics yolov8
   Glenn-Jocher yfl, 2021, Ultralytics: Github, P3181
   Goyal P, 2018, Arxiv, DOI arXiv:1706.02677
   Han FL, 2020, J SENSORS, V2020, DOI 10.1155/2020/6707328
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Hu HW, 2019, IEEE T MULTIMEDIA, V21, P510, DOI 10.1109/TMM.2018.2859831
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang X, 2021, Arxiv, DOI arXiv:2104.10419
   Jiang HZ, 2017, IEEE INT CONF AUTOMA, P650, DOI [10.1109/FG.2017.82, 10.1109/MWSYM.2017.8058653]
   Kim SW, 2018, LECT NOTES COMPUT SC, V11209, P239, DOI 10.1007/978-3-030-01228-1_15
   Kong T, 2018, LECT NOTES COMPUT SC, V11209, P172, DOI 10.1007/978-3-030-01228-1_11
   Li W, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-67529-x
   Li Y, 2022, APPL INTELL, V52, P9861, DOI 10.1007/s10489-021-02912-3
   Lin T.-Y., 2017, PROC CVPR IEEE, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu ST, 2019, Arxiv, DOI arXiv:1911.09516
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Nie J, 2019, IEEE I CONF COMP VIS, P9536, DOI 10.1109/ICCV.2019.00963
   Njengwie Achinek D, 2021, 5 INT C COMP SCI APP, P1
   Pirinen A, 2018, PROC CVPR IEEE, P6945, DOI 10.1109/CVPR.2018.00726
   Pramanik A, 2022, IEEE TETCI, V6, P171, DOI 10.1109/TETCI.2020.3041019
   Priyadarshni Divya, 2020, Soft Computing: Theories and Applications. Proceedings of SoCTA 2018. Advances in Intelligent Systems and Computing (AISC 1053), P837, DOI 10.1007/978-981-15-0751-9_76
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shafiee M.J., 2017, Fast YOLO: a fast you only look once system for real-time embedded object detection in video, DOI [DOI 10.48550/ARXIV.1709.05943, 10.15353/vsnl.v3i1.171]
   Singh B, 2018, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2018.00377
   Srinivas A, 2021, PROC CVPR IEEE, P16514, DOI 10.1109/CVPR46437.2021.01625
   Sukhbaatar S, 2019, Arxiv, DOI arXiv:1907.01470
   Tan R., 2020, P IEEE CVF C COMP VI, DOI [10.1109/CVPR42600.2020.01079, DOI 10.1109/CVPR42600.2020.01079]
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang CY, 2021, Arxiv, DOI arXiv:2105.04206
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu Y, 2021, APPL SOFT COMPUT, V108, DOI 10.1016/j.asoc.2021.107405
   Yang S, 2018, IEEE T PATTERN ANAL, V40, P1845, DOI 10.1109/TPAMI.2017.2738644
   Yang S, 2016, PROC CVPR IEEE, P5525, DOI 10.1109/CVPR.2016.596
   Yuan Y, 2017, IEEE T INTELL TRANSP, V18, P1918, DOI 10.1109/TITS.2016.2614548
   Zhang Zhenlin, 2022, arXiv
   Zheng W, 2021, PROC CVPR IEEE, P14489, DOI 10.1109/CVPR46437.2021.01426
   Zhu YS, 2019, IEEE T IMAGE PROCESS, V28, P113, DOI 10.1109/TIP.2018.2865280
NR 53
TC 0
Z9 0
U1 14
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD JUN
PY 2024
VL 13
IS 2
AR 18
DI 10.1007/s13735-024-00323-x
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ND7D1
UT WOS:001198569900001
DA 2024-08-05
ER

PT J
AU Sharma, G
   Singh, M
AF Sharma, Gaurav
   Singh, Maheep
TI A spatiotemporal bidirectional network for video salient object
   detection using multiscale transfer learning
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Video; Saliency; Spatiotemporal; Encoder-decoder architecture
ID FUSION
AB Video saliency prediction aims to simulate human visual attention by selecting the most pertinent and important components within a video frame or sequence. When evaluating video saliency, time and space data are essential, particularly in the presence of challenging features such as fast motion, shifting background, and nonrigid deformation. The current video saliency frameworks are highly prone to failure under the specified conditions. Moreover, it is unsuitable to perform video saliency identification by solely relying on image saliency models, disregarding the temporal information in videos. This research proposes a novel Spatiotemporal Bidirectional Network for Video Salient Object Detection using Multiscale Transfer Learning (SBMTL-Net) to solve the issue of detecting important objects in videos. The SBMTL-Net produces significant outcomes for a given sequence of frames by utilizing Multi-scale transfer learning with an encoder and decoder technique to acquire knowledge and spatially and temporally map properties. SBMTL-Net model consists of bidirectional LSTM (Long Short-Term Memory) and CNN (Convolutional Neural Network), where the VGG16 (Video Geometry Group) and VGG19 are utilized for multi-scale feature extraction of the input video frames. The performance of the proposed model has been evaluated on five publically available challenging datasets DAVIS-T, SegTrack-V2, ViSal, VOS-T and DAVSOD-T for the parameters MAE, F-measure and S-measure. The experimental results show the effectiveness of the proposed model as compared with other competitive models.
C1 [Sharma, Gaurav; Singh, Maheep] NIT Uttarakhand, Dept Comp Sci & Engn, Srinagar, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Uttarakhand
RP Sharma, G (corresponding author), NIT Uttarakhand, Dept Comp Sci & Engn, Srinagar, India.
EM gauravsharma.phd2020@nituk.ac.in; maheepsingh@nituk.ac.in
OI Sharma, Gaurav/0009-0006-5982-433X
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Chen CLZ, 2021, IEEE T IMAGE PROCESS, V30, P3995, DOI 10.1109/TIP.2021.3068644
   Chen CLZ, 2020, IEEE T IMAGE PROCESS, V29, P4296, DOI 10.1109/TIP.2020.2968250
   Chen CLZ, 2020, IEEE T IMAGE PROCESS, V29, P1090, DOI 10.1109/TIP.2019.2934350
   Chen CZ, 2017, IEEE T IMAGE PROCESS, V26, P3156, DOI 10.1109/TIP.2017.2670143
   Chen CLZ, 2016, PATTERN RECOGN, V52, P410, DOI 10.1016/j.patcog.2015.09.033
   Chen P., 2021, 2021 IEEE INT C MULT, P1
   Chen YH, 2018, IEEE T IMAGE PROCESS, V27, P3345, DOI 10.1109/TIP.2018.2813165
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Cong RM, 2023, IEEE T EM TOP COMP I, V7, P402, DOI 10.1109/TETCI.2022.3220250
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Gu YC, 2020, AAAI CONF ARTIF INTE, V34, P10869
   Itti L, 2005, PROC CVPR IEEE, P631
   Jain SD, 2017, PROC CVPR IEEE, P2117, DOI 10.1109/CVPR.2017.228
   Ji GP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4902, DOI 10.1109/ICCV48922.2021.00488
   Ji YZ, 2021, INFORM SCIENCES, V546, P835, DOI 10.1016/j.ins.2020.09.003
   Ji YZ, 2021, IEEE T NEUR NET LEAR, V32, P2676, DOI 10.1109/TNNLS.2020.3007534
   Jian MW, 2024, IEEE T COMPUT SOC SY, V11, P2026, DOI 10.1109/TCSS.2023.3270164
   Kim H, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2425544
   Li CY, 2020, NEUROCOMPUTING, V415, P411, DOI 10.1016/j.neucom.2020.05.108
   Li FX, 2013, IEEE I CONF COMP VIS, P2192, DOI 10.1109/ICCV.2013.273
   Li GB, 2018, PROC CVPR IEEE, P3243, DOI 10.1109/CVPR.2018.00342
   Li GB, 2016, IEEE T IMAGE PROCESS, V25, P5012, DOI 10.1109/TIP.2016.2602079
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li HF, 2019, IEEE I CONF COMP VIS, P7273, DOI 10.1109/ICCV.2019.00737
   Li J, 2018, IEEE T IMAGE PROCESS, V27, P349, DOI 10.1109/TIP.2017.2762594
   Li S, 2018, P EUR C COMP VIS ECC
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Li YX, 2020, IEEE T MULTIMEDIA, V22, P1153, DOI 10.1109/TMM.2019.2940851
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu J, 2022, DIGIT SIGNAL PROCESS, V130, DOI 10.1016/j.dsp.2022.103700
   Liu Z, 2017, IEEE T CIRC SYST VID, V27, P2527, DOI 10.1109/TCSVT.2016.2595324
   Liu Z, 2014, IEEE T CIRC SYST VID, V24, P1522, DOI 10.1109/TCSVT.2014.2308642
   Mei Jianbiao, 2021, arXiv
   Rahtu E, 2010, LECT NOTES COMPUT SC, V6315, P366, DOI 10.1007/978-3-642-15555-0_27
   Sharma Gaurav, 2023, 2023 1st International Conference on Innovations in High Speed Communication and Signal Processing (IHCSP), P263, DOI 10.1109/IHCSP56702.2023.10127129
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh H, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3302911
   Tang Y, 2019, IEEE T CIRC SYST VID, V29, P1973, DOI 10.1109/TCSVT.2018.2859773
   Le TN, 2018, IEEE T IMAGE PROCESS, V27, P5002, DOI 10.1109/TIP.2018.2849860
   Tsai MF, 2021, IEEE ACCESS, V9, P13870, DOI 10.1109/ACCESS.2021.3052246
   Tu WC, 2016, PROC CVPR IEEE, P2334, DOI 10.1109/CVPR.2016.256
   Tu ZG, 2017, PATTERN RECOGN, V72, P285, DOI 10.1016/j.patcog.2017.07.028
   Wang WG, 2021, IEEE T PATTERN ANAL, V43, P220, DOI 10.1109/TPAMI.2019.2924417
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2019, PROC CVPR IEEE, P3059, DOI 10.1109/CVPR.2019.00318
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wenguan Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3395, DOI 10.1109/CVPR.2015.7298961
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xi T, 2017, IEEE T IMAGE PROCESS, V26, P3425, DOI 10.1109/TIP.2016.2631900
   Yan P., 2019, P IEEE CVF INT C COM
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao WB, 2021, PROC CVPR IEEE, P16821, DOI 10.1109/CVPR46437.2021.01655
   Zhou H., 2020, P IEEE CVF C COMP VI, P9138, DOI 10.1109/CVPR42600.2020.00916
   Zhou XF, 2023, INFORM SCIENCES, V628, P134, DOI 10.1016/j.ins.2023.01.106
NR 57
TC 0
Z9 0
U1 6
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD JUN
PY 2024
VL 13
IS 2
AR 25
DI 10.1007/s13735-024-00331-x
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PQ0R1
UT WOS:001215433200001
DA 2024-08-05
ER

PT J
AU Mazari, A
   Sahbi, H
AF Mazari, Ahmed
   Sahbi, Hichem
TI Deep multiple aggregation networks for action recognition
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Multiple aggregation design; 2-Stream networks; Action recognition
ID BEHAVIOR
AB Most of the current action recognition algorithms are based on deep networks which stack multiple convolutional, pooling and fully connected layers. While convolutional and fully connected operations have been widely studied in the literature, the design of pooling operations that handle action recognition, with different sources of temporal granularity in action categories, has comparatively received less attention, and existing solutions rely mainly on max or averaging operations. The latter are clearly powerless to fully exhibit the actual temporal granularity of action categories and thereby constitute a bottleneck in classification performances. In this paper, we introduce a novel hierarchical pooling design that captures different levels of temporal granularity in action recognition. Our design principle is coarse-to-fine and achieved using a tree-structured network; as we traverse this network top-down, pooling operations are getting less invariant but timely more resolute and well localized. Learning the combination of operations in this network-which best fits a given ground-truth-is obtained by solving a constrained minimization problem whose solution corresponds to the distribution of weights that capture the contribution of each level (and thereby temporal granularity) in the global hierarchical pooling process. Besides being principled and well grounded, the proposed hierarchical pooling is also video-length and resolution agnostic. Extensive experiments conducted on the challenging UCF-101, HMDB-51 and JHMDB-21 databases corroborate all these statements.
C1 [Mazari, Ahmed; Sahbi, Hichem] Sorbonne Univ, LIP6, CNRS, F-75005 Paris, France.
C3 Centre National de la Recherche Scientifique (CNRS); Sorbonne Universite
RP Sahbi, H (corresponding author), Sorbonne Univ, LIP6, CNRS, F-75005 Paris, France.
EM hichem.sahbi@lip6.fr
CR Aafaq N, 2019, PROC CVPR IEEE, P12479, DOI 10.1109/CVPR.2019.01277
   Bagautdinov T, 2017, PROC CVPR IEEE, P3425, DOI 10.1109/CVPR.2017.365
   Ballan L, 2011, MULTIMED TOOLS APPL, V51, P279, DOI 10.1007/s11042-010-0643-7
   Ben Mabrouk A, 2018, EXPERT SYST APPL, V91, P480, DOI 10.1016/j.eswa.2017.09.029
   Cai JH, 2020, IET COMPUT VIS, V14, P634, DOI 10.1049/iet-cvi.2020.0023
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen L, 2013, PROC CVPR IEEE, P2666, DOI 10.1109/CVPR.2013.344
   Choutas V, 2018, PROC CVPR IEEE, P7024, DOI 10.1109/CVPR.2018.00734
   Christoph R., 2016, Adv. Neural Inf. Process. Syst, V2, P3468
   Csurka G., 2004, EUROPEAN C COMPUTER
   Csurka G, 2011, COMM COM INF SC, V229, P28
   Demiris Yiannis, 2007, Cogn Process, V8, P151, DOI 10.1007/s10339-007-0168-9
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Duchenne O, 2009, IEEE I CONF COMP VIS, P1491, DOI 10.1109/ICCV.2009.5459279
   Feichtenhofer C, 2017, PROC CVPR IEEE, P7445, DOI 10.1109/CVPR.2017.787
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Garcia-Hernando G, 2018, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2018.00050
   Garcia-Hernando G, 2017, PROC CVPR IEEE, P407, DOI 10.1109/CVPR.2017.51
   Gönen M, 2011, J MACH LEARN RES, V12, P2211
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Ha MH, 2021, IEEE ACCESS, V9, P164887, DOI 10.1109/ACCESS.2021.3134694
   Han YM, 2018, PATTERN RECOGN LETT, V107, P83, DOI 10.1016/j.patrec.2017.08.015
   He K., 2015, DELVING DEEP RECTIFI, P1026
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hu JF, 2015, PROC CVPR IEEE, P5344, DOI 10.1109/CVPR.2015.7299172
   Huang ZW, 2018, AAAI CONF ARTIF INTE, P3279
   Huang ZW, 2017, AAAI CONF ARTIF INTE, P2036
   Jaimes A., 2004, 1st ACM workshop on Continuous archival and retrieval of personal experiences (CARPE 2004), P74, DOI DOI 10.1145/1026653.1026665
   Jin Sheng, 2022, 2022 4th International Conference on Intelligent Control, Measurement and Signal Processing (ICMSP), P1063, DOI 10.1109/ICMSP55950.2022.9859092
   Jiu MY, 2016, INT CONF ACOUST SPEE, P1551, DOI 10.1109/ICASSP.2016.7471937
   Kingma D. P., 2014, arXiv
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Kusumoseniarto RH, 2020, 2020 JOINT 9TH INTERNATIONAL CONFERENCE ON INFORMATICS, ELECTRONICS & VISION (ICIEV) AND 2020 4TH INTERNATIONAL CONFERENCE ON IMAGING, VISION & PATTERN RECOGNITION (ICIVPR), DOI 10.1109/icievicivpr48672.2020.9306558
   Laptev I, 2007, IEEE I CONF COMP VIS, P2165
   Li HF, 2023, IEEE T COGN DEV SYST, V15, P65, DOI 10.1109/TCDS.2022.3145839
   Li JP, 2021, NEUROCOMPUTING, V459, P338, DOI 10.1016/j.neucom.2021.06.088
   Li J, 2020, IEEE T MULTIMEDIA, V22, P2990, DOI 10.1109/TMM.2020.2965434
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Liu JB, 2021, Arxiv, DOI arXiv:2106.13391
   Lu ML, 2019, IEEE T IMAGE PROCESS, V28, P3703, DOI 10.1109/TIP.2019.2901707
   Lu W.-L., 2006, The 3rd Canadian Conference on Computer and Robot Vision (CRV'06), P6
   Mahmud T, 2021, Arxiv, DOI arXiv:1908.00943
   Martin PE, 2020, MULTIMED TOOLS APPL, V79, P20429, DOI 10.1007/s11042-020-08917-3
   Mazari A, 2019, IEEE INT C ACOUSTICS
   Mazari A, 2020, IEEE IMAGE PROC, P1541, DOI 10.1109/ICIP40778.2020.9191360
   MENG H., 2007, IEEE C COMPUTER VISI, P1
   Murray N, 2014, PROC CVPR IEEE, P2473, DOI 10.1109/CVPR.2014.317
   Nan M, 2019, I C CONTR SYS COMP S, P675, DOI 10.1109/CSCS.2019.00121
   Obeso AM, 2018, INT WORK CONTENT MUL
   Obeso AM, 2019, 2019 9 INT C IMAGE P, P1
   Ohn-Bar E, 2014, IEEE T INTELL TRANSP, V15, P2368, DOI 10.1109/TITS.2014.2337331
   Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98
   Pantic M, 2007, LECT NOTES COMPUT SC, V4451, P47
   Piergiovanni AJ, 2018, IEEE COMPUT SOC CONF, P1821, DOI 10.1109/CVPRW.2018.00226
   Pirsiavash H, 2012, PROC CVPR IEEE, P2847, DOI 10.1109/CVPR.2012.6248010
   Pramono RRA, 2021, IEEE T IMAGE PROCESS, V30, P8184, DOI 10.1109/TIP.2021.3113570
   Pramono RRA, 2019, IEEE I CONF COMP VIS, P61, DOI 10.1109/ICCV.2019.00015
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Rahmani H, 2016, PROC CVPR IEEE, P1506, DOI 10.1109/CVPR.2016.167
   Sahbi H, 2021, BRIT MACHINE VISION
   Sahbi H, 2022, IEEE IMAGE PROC, P3495, DOI 10.1109/ICIP46576.2022.9897899
   Sahbi H, 2021, IEEE IMAGE PROC, P2329, DOI 10.1109/ICIP42928.2021.9506774
   Sahbi H, 2021, INT C PATT RECOG, P9996, DOI 10.1109/ICPR48806.2021.9412009
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shao J, 2015, PROC CVPR IEEE, P4657, DOI 10.1109/CVPR.2015.7299097
   Shawe-Taylor J., 2004, KERNEL METHODS PATTE, DOI [DOI 10.1017/CBO9780511809682, 10.1017/CBO9780511809682]
   Simonyan K, 2014, ADV NEUR IN, V27
   Soomro ARZK, 2012, CRCV-TR-12-01
   Theodoridis T, 2008, IEEE INT CONF ROBOT, P3064, DOI 10.1109/ROBOT.2008.4543676
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Ullah A, 2018, IEEE ACCESS, V6, P1155, DOI 10.1109/ACCESS.2017.2778011
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang BR, 2018, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR.2018.00795
   Wang HR, 2012, PATTERN RECOGN, V45, P3902, DOI 10.1016/j.patcog.2012.04.024
   Wang JW, 2018, PROC CVPR IEEE, P7190, DOI 10.1109/CVPR.2018.00751
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang LM, 2018, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2018.00155
   Wang L, 2014, IEEE IMAGE PROC, P1550, DOI 10.1109/ICIP.2014.7025310
   Wang L, 2015, LECT NOTES COMPUT SC, V8927, P47, DOI 10.1007/978-3-319-16199-0_4
   Wang L, 2013, IEEE I CONF COMP VIS, P3168, DOI 10.1109/ICCV.2013.393
   Wang Y, 2017, IEEE INT C COMPUTER
   Wu CY, 2018, PROC CVPR IEEE, P6026, DOI 10.1109/CVPR.2018.00631
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Xu D, 2007, IEEE INT C COMPUTER
   Yang K, 2018, IEEE INT C IMAGE PRO
   Yihuang J, 2017, Pretrained 2D two streams network for action recognition on UCF-101 based on temporal segment network
   Zanfir M, 2013, IEEE I CONF COMP VIS, P2752, DOI 10.1109/ICCV.2013.342
   Zhang D, 2018, ASIAN C COMPUTER VIS
   Zhang XK, 2016, PROC CVPR IEEE, P4498, DOI 10.1109/CVPR.2016.487
   Zheng ZX, 2019, NEUROCOMPUTING, V358, P446, DOI 10.1016/j.neucom.2019.05.058
   Zhu JG, 2018, INT C PATT RECOG, P645, DOI 10.1109/ICPR.2018.8545710
   Zhu WH, 2016, PROC INT CONF ANTI, P1, DOI 10.1109/ICASID.2016.7873885
   Zolfaghari M, 2018, LECT NOTES COMPUT SC, V11206, P713, DOI 10.1007/978-3-030-01216-8_43
NR 100
TC 1
Z9 1
U1 8
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD MAR
PY 2024
VL 13
IS 1
AR 9
DI 10.1007/s13735-023-00317-1
PG 16
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GP0Z4
UT WOS:001153767600002
DA 2024-08-05
ER

PT J
AU Schall, K
   Bailer, W
   Barthel, KU
   Carrara, F
   Lokoc, J
   Peska, L
   Schoeffmann, K
   Vadicamo, L
   Vairo, C
AF Schall, Konstantin
   Bailer, Werner
   Barthel, Kai-Uwe
   Carrara, Fabio
   Lokoc, Jakub
   Peska, Ladislav
   Schoeffmann, Klaus
   Vadicamo, Lucia
   Vairo, Claudio
TI Interactive multimodal video search: an extended post-evaluation for the
   VBS 2022 competition
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Interactive video retrieval; Video browsing; Video content analysis;
   Content-based retrieval; Evaluations
AB CLIP-based text-to-image retrieval has proven to be very effective at the interactive video retrieval competition Video Browser Showdown 2022, where all three top-scoring teams had implemented a variant of a CLIP model in their system. Since the performance of these three systems was quite close, this post-evaluation was designed to get better insights on the differences of the systems and compare the CLIP-based text-query retrieval engines by introducing slight modifications to the original competition settings. An extended analysis of the overall results and the retrieval performance of all systems' functionalities shows that a strong text retrieval model certainly helps, but has to be coupled with extensive browsing capabilities and other query-modalities to consistently solve known-item-search tasks in a large-scale video database.
C1 [Schall, Konstantin; Barthel, Kai-Uwe] HTW Berlin, Visual Comp Grp, Berlin, Germany.
   [Bailer, Werner] Joanneum Res, Graz, Austria.
   [Carrara, Fabio; Vadicamo, Lucia; Vairo, Claudio] CNR, Inst Informat Sci & Technol ISTI, Pisa, Italy.
   [Lokoc, Jakub; Peska, Ladislav] Charles Univ Prague, Dept Software Engn, Prague, Czech Republic.
   [Schoeffmann, Klaus] Klagenfurt Univ, Klagenfurt, Austria.
C3 Consiglio Nazionale delle Ricerche (CNR); Istituto di Scienza e
   Tecnologie dell'Informazione "Alessandro Faedo" (ISTI-CNR); Charles
   University Prague; University of Klagenfurt
RP Schall, K (corresponding author), HTW Berlin, Visual Comp Grp, Berlin, Germany.
EM konstantin.schall@htw-berlin.de
RI Vadicamo, Lucia/P-5138-2018; Carrara, Fabio/R-2275-2019
OI Vadicamo, Lucia/0000-0001-7182-7038; Carrara, Fabio/0000-0001-5014-5089
FU Hochschule fr Technik und Wirtschaft Berlin (3378)
FX No Statement Available
CR Ali A, 2022, Video and text matching with conditioned embeddings, P1565
   Amato G, 2022, LECT NOTES COMPUT SC, V13142, P543, DOI 10.1007/978-3-030-98355-0_52
   Awad George, 2022, P TRECVID 2022
   Bain M., 2022, arXiv
   Barthel KU, 2023, Computer graphics forum
   Benavente R, 2008, J OPT SOC AM A, V25, P2582, DOI 10.1364/JOSAA.25.002582
   Bird S., 2009, Natural Language Processing with Python, DOI DOI 10.5555/1717171
   Constantin MG., 2020, ACM SIGMM Rec, V12, P1
   Cox I. J., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P361, DOI 10.1109/ICPR.1996.546971
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Fang H., 2021, arXiv
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gurrin C., 2023, P INT C MULTIMEDIA R
   Gurrin C, 2022, PROCEEDINGS OF THE 2022 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2022, P685, DOI 10.1145/3512527.3531439
   He K., 2017, P IEEE INT C COMP VI, P2961
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heller S, 2022, INT J MULTIMED INF R, V11, P1, DOI 10.1007/s13735-021-00225-2
   Hezel N, 2022, LECT NOTES COMPUT SC, V13142, P487, DOI 10.1007/978-3-030-98355-0_43
   Kim S, 2020, PROC CVPR IEEE, P3235, DOI 10.1109/CVPR42600.2020.00330
   KOHONEN T, 1982, BIOL CYBERN, V43, P59, DOI 10.1007/BF00337288
   Liu Z, 2021, Arxiv, DOI arXiv:2103.14030
   Lokoc J etal, 2022, INT C MULTIMEDIA MOD
   Lokoc J, 2023, MULTIMEDIA SYST, V29, P3481, DOI 10.1007/s00530-023-01143-5
   Lokoc J, 2023, LECT NOTES COMPUT SC, V13833, P397, DOI 10.1007/978-3-031-27077-2_31
   Lokoc J, 2022, LECT NOTES COMPUT SC, V13142, P505, DOI 10.1007/978-3-030-98355-0_46
   Lokoc J, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3445031
   Lokoc J, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3295663
   Lokoc J, 2018, IEEE T MULTIMEDIA, V20, P3361, DOI 10.1109/TMM.2018.2830110
   Ma Y, 2022, X-clip: end-to-end multi-grained contrastive learning for video-text retrieval, P638, DOI [10.1145/3503161.3547910, DOI 10.1145/3503161.3547910]
   Messina N, 2021, INT C PATT RECOG, P5222, DOI 10.1109/ICPR48806.2021.9413172
   Philbin J, 2007, 2007 IEEE C COMPUTER, P1
   Radford A, 2021, PR MACH LEARN RES, V139
   Revaud J, 2019, IEEE I CONF COMP VIS, P5106, DOI 10.1109/ICCV.2019.00521
   Rossetto L, 2021, IEEE MULTIMEDIA, V28, P18, DOI 10.1109/MMUL.2021.3066779
   Rossetto L, 2019, LECT NOTES COMPUT SC, V11295, P349, DOI 10.1007/978-3-030-05710-7_29
   Rossetto Luca, 2021, International Conference on Multimedia Modeling, P385
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Tran LD, 2023, IEEE ACCESS, V11, P30982, DOI 10.1109/ACCESS.2023.3248284
   van de Weijer J, 2009, IEEE T IMAGE PROCESS, V18, P1512, DOI 10.1109/TIP.2009.2019809
   Vaswani A, 2017, ADV NEUR IN, V30
   Zhang HY, 2021, PROC CVPR IEEE, P8510, DOI 10.1109/CVPR46437.2021.00841
NR 41
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD JUN
PY 2024
VL 13
IS 2
AR 15
DI 10.1007/s13735-024-00325-9
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MB9O4
UT WOS:001191286600001
OA hybrid, Green Submitted
DA 2024-08-05
ER

PT J
AU Papadopoulos, SI
   Koutlis, C
   Papadopoulos, S
   Petrantonakis, PC
AF Papadopoulos, Stefanos-Iordanis
   Koutlis, Christos
   Papadopoulos, Symeon
   Petrantonakis, Panagiotis C.
TI VERITE: a Robust benchmark for multimodal misinformation detection
   accounting for unimodal bias
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Multimodal learning; Deep learning; Misinformation detection; Unimodal
   bias; Benchmark
ID DISINFORMATION
AB Multimedia content has become ubiquitous on social media platforms, leading to the rise of multimodal misinformation (MM) and the urgent need for effective strategies to detect and prevent its spread. In recent years, the challenge of multimodal misinformation detection (MMD) has garnered significant attention by researchers and has mainly involved the creation of annotated, weakly annotated, or synthetically generated training datasets, along with the development of various deep learning MMD models. However, the problem of unimodal bias has been overlooked, where specific patterns and biases in MMD benchmarks can result in biased or unimodal models outperforming their multimodal counterparts on an inherently multimodal task, making it difficult to assess progress. In this study, we systematically investigate and identify the presence of unimodal bias in widely used MMD benchmarks, namely VMU-Twitter and COSMOS. To address this issue, we introduce the "VERification of Image-TExt pairs" (VERITE) benchmark for MMD which incorporates real-world data, excludes "asymmetric multimodal misinformation" and utilizes "modality balancing". We conduct an extensive comparative study with a transformer-based architecture that shows the ability of VERITE to effectively address unimodal bias, rendering it a robust evaluation framework for MMD. Furthermore, we introduce a new method-termed Crossmodal HArd Synthetic MisAlignment (CHASMA)-for generating realistic synthetic training data that preserve crossmodal relations between legitimate images and false human-written captions. By leveraging CHASMA in the training process, we observe consistent and notable improvements in predictive performance on VERITE; with a 9.2% increase in accuracy. We release our code at: https://github.com/stevejpapad/image-text-verification
C1 [Papadopoulos, Stefanos-Iordanis; Koutlis, Christos; Papadopoulos, Symeon] Ctr Res & Technol Hellas, Informat Technol Inst, Thessaloniki, Greece.
   [Papadopoulos, Stefanos-Iordanis; Petrantonakis, Panagiotis C.] Aristotle Univ Thessaloniki, Dept Elect & Comp Engn, Thessaloniki, Greece.
C3 Centre for Research & Technology Hellas; Aristotle University of
   Thessaloniki
RP Papadopoulos, SI (corresponding author), Ctr Res & Technol Hellas, Informat Technol Inst, Thessaloniki, Greece.; Papadopoulos, SI (corresponding author), Aristotle Univ Thessaloniki, Dept Elect & Comp Engn, Thessaloniki, Greece.
EM stefpapad@iti.gr; ckoutlis@iti.gr; papadop@iti.gr; ppetrant@ece.auth.gr
RI Koutlis, Christos/AAK-8028-2021
FU Centre for Research & Technology Hellas (CERTH)
FX The publication of the article in OA mode was financially supported by
   HEAL-Link
CR Abdelnabi S, 2022, PROC CVPR IEEE, P14920, DOI 10.1109/CVPR52688.2022.01452
   Agrawal A, 2018, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2018.00522
   Akhtar M, 2023, Arxiv, DOI arXiv:2305.13507
   Alam Firoj, 2022, P 29 INT C COMP LING, P6625
   Aneja S, 2023, AAAI CONF ARTIF INTE, P14084
   Aneja S, 2021, Arxiv, DOI arXiv:2107.05297
   Aneja S, 2022, Arxiv, DOI arXiv:2207.14534
   Bennett WL, 2018, EUR J COMMUN, V33, P122, DOI 10.1177/0267323118760317
   Biamby G, 2022, NAACL 2022: THE 2022 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES, P1530
   Boididou C, 2018, MULTIMED TOOLS APPL, V77, P15545, DOI 10.1007/s11042-017-5132-9
   Cadene R, 2019, Advances in neural information processing systems
   Cardenuto JP, 2023, Arxiv, DOI arXiv:2306.11503
   Cheema GS, 2022, FINDINGS ASS COMPUTA, P962
   Duffy A, 2020, INFORM COMMUN SOC, V23, P1965, DOI 10.1080/1369118X.2019.1623904
   Gamir-Ríos J, 2021, ANALISI, P49, DOI 10.5565/rev/analisi.3398
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Guzhov A, 2022, INT CONF ACOUST SPEE, P976, DOI 10.1109/ICASSP43922.2022.9747631
   Hangloo S, 2022, MULTIMEDIA SYST, V28, P2391, DOI 10.1007/s00530-022-00966-y
   Heller S, 2018, Arxiv, DOI arXiv:1804.04866
   Jaiswal A, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1465, DOI 10.1145/3123266.3123385
   Jindal S., 2020, CEUR WORKSH P, V2560, P138
   Jing J, 2023, INFORM PROCESS MANAG, V60, DOI 10.1016/j.ipm.2022.103120
   Khattar D, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P2915, DOI 10.1145/3308558.3313552
   Koh PW, 2021, PR MACH LEARN RES, V139
   Levi O, 2019, Arxiv, DOI [arXiv:1910.01160, 10.48550/arXiv.1910.01160, DOI 10.48550/ARXIV.1910.01160]
   Li JH, 2021, ADV NEUR IN, V34
   Li JN, 2023, Arxiv, DOI arXiv:2301.12597
   Li Manling, 2022, P IEEE CVF C COMP VI, P16420
   Li YY, 2020, J MARKETING RES, V57, P1, DOI 10.1177/0022243719881113
   Lin ZY, 2022, LECT NOTES COMPUT SC, V13695, P388, DOI 10.1007/978-3-031-19833-5_23
   Liu FX, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P6761
   Luo GC, 2021, Arxiv, DOI [arXiv:2104.05893, DOI 10.48550/ARXIV.2104.05893]
   Mridha MF, 2021, IEEE ACCESS, V9, P156151, DOI 10.1109/ACCESS.2021.3129329
   Muller-Budack Eric, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P16, DOI 10.1145/3372278.3390670
   Nakamura K, 2020, PROCEEDINGS OF THE 12TH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION (LREC 2020), P6149
   Nakov Preslav., 2021, Advances in Information Retrieval, P639, DOI DOI 10.1007/978-3-030-72240-1_75
   Newman EJ, 2012, PSYCHON B REV, V19, P969, DOI 10.3758/s13423-012-0292-0
   Nielsen DS, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P3141, DOI 10.1145/3477495.3531744
   Olan F, 2024, INFORM SYST FRONT, V26, P443, DOI 10.1007/s10796-022-10242-z
   Papadopoulos SI, 2023, PROCEEDINGS OF THE 2ND ACM INTERNATIONAL WORKSHOP ON MULTIMEDIA AI AGAINST DISCRIMINATION, MAD 2023, P36, DOI 10.1145/3592572.3592842
   Radford A, 2021, PR MACH LEARN RES, V139
   Rana MS, 2022, IEEE ACCESS, V10, P25494, DOI 10.1109/ACCESS.2022.3154404
   Roozenbeek J, 2020, ROY SOC OPEN SCI, V7, DOI 10.1098/rsos.201199
   Sabir E, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1337, DOI 10.1145/3240508.3240707
   Singhal S, 2022, COMPANION PROCEEDINGS OF THE WEB CONFERENCE 2022, WWW 2022 COMPANION, P726, DOI 10.1145/3487553.3524650
   Singhal S, 2019, 2019 IEEE FIFTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2019), P39, DOI [10.1109/BigMM.2019.00018, 10.1109/BigMM.2019.00-44]
   Tahmasebi S, 2023, PROCEEDINGS OF THE 2023 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2023, P581, DOI 10.1145/3591106.3592230
   Thorne J., 2018, 2018 C N AM CHAPTER, DOI [DOI 10.18653/V1/N18-1074, 10.18653/v1/n18-1074]
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang YQ, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P849, DOI 10.1145/3219819.3219903
   Wu Y, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P2560
   Yu CM, 2022, INFORM PROCESS MANAG, V59, DOI 10.1016/j.ipm.2022.103063
   Zhang YN, 2023, Arxiv, DOI arXiv:2303.01510
   Zhou YM, 2023, IEEE INT CON MULTI, P2825, DOI 10.1109/ICME55011.2023.00480
   Zlatkova D, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2099
NR 55
TC 0
Z9 0
U1 10
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD MAR
PY 2024
VL 13
IS 1
AR 4
DI 10.1007/s13735-023-00312-6
PG 15
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EF9T3
UT WOS:001137632500001
OA hybrid, Green Submitted
DA 2024-08-05
ER

PT J
AU Deng, Y
   Li, YH
   Xian, SD
   Li, LQ
   Qiu, HY
AF Deng, Yang
   Li, Yonghong
   Xian, Sidong
   Li, Laquan
   Qiu, Haiyang
TI Mual: enhancing multimodal sentiment analysis with cross-modal attention
   and difference loss
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Multimodal sentiment analysis; Attention mechanism; Feature extraction;
   Feature fusion
ID FUSION NETWORK
AB Multimodal sentiment analysis has received much attention in recent years, especially in the context of the abundant multimodal data generated by social platforms. Models like CLIP, BLIP, VisualBERT are all excellent but often come with a large number of parameters and require inputs in the form of image-text pairs, which will constrain their flexibility. The integration of superior unimodal sentiment analysis models allows simultaneous processing of multimodal data, enabling arbitrary addition or removal of modalities for generalized multimodal sentiment analysis. The integrated model that preprocesses and fuses each modality can sometimes significantly improve the accuracy of sentiment analysis. Therefore, this study proposes a novel multimodal sentiment analysis approach, MuAL, based on cross-modal attention and difference loss. Cross-modal attention is used to integrate information from two modalities, and difference loss is utilized to minimize the gap between image and text information, enhancing the model's robustness. Additionally, MuAL uses cls token to capture overall sentiment information, further eliminating noise within modalities and reducing computational expenses. The study evaluates MuAL on five real-world datasets, demonstrating superior performance over baseline methods with fewer parameters. Furthermore, considering MuAL utilizes pre-trained models as encoders, the research assesses its capability in transfer learning. Results reveal that even after freezing the parameters of the pre-trained model, MuAL outperforms the baseline on all five datasets, confirming its superior performance.
C1 [Deng, Yang; Qiu, Haiyang] Chongqing Univ Posts & Telecommun, Sch Comp Sci & Technol, Chongqing 400065, Peoples R China.
   [Li, Yonghong; Xian, Sidong; Li, Laquan] Chongqing Univ Posts & Telecommun, Sch Sci, Chongqing 400065, Peoples R China.
C3 Chongqing University of Posts & Telecommunications; Chongqing University
   of Posts & Telecommunications
RP Deng, Y (corresponding author), Chongqing Univ Posts & Telecommun, Sch Comp Sci & Technol, Chongqing 400065, Peoples R China.
EM S220233010@stu.cqupt.edu.cn; liyh@cqupt.edu.cn; xiansd@cqupt.edu.cn;
   lilq@cqupt.edu.cn; S22023103@stu.cqupt.edu.cn
FU National Natural Science Foundation of China [2020YFC2003502 61876201,
   61901074, 61972060, 72171031, 62136002]; Natural Science Foundation
   Project of Chongqing [cstc2020jcyjmsxmX0649]; China Postdoctoral
   Science, Foundation [2021M693771]
FX This work was supported in part by the National Natural Science
   Foundation of China (Grant No.2020YFC2003502 61876201, 61901074,
   61972060, 72171031, 62136002), the Natural Science Foundation Project of
   Chongqing (Grant No. cstc2020jcyjmsxmX0649) and China Postdoctoral
   Science, Foundation (Grant No. 2021M693771). DAS:The experiment dataset
   for this study is available at
   https://www.kaggle.com/datasets/deeptanshu111/hateful-memes,
   https://www.kaggle.com/datasets/vincemarcs/mvsamultiple,
   https://www.kaggle.com/datasets/vincemarcs/mvsasingle and
   https://www.kaggle.com/datasets/akramkaras/umt-dataset. All the data
   that support the findings of this study are available from corresponding
   author upon reasonable request.
CR Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Peters ME, 2018, Arxiv, DOI arXiv:1802.05365
   Li LH, 2019, Arxiv, DOI [arXiv:1908.03557, DOI 10.48550/ARXIV.1908.03557]
   Hazarika D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1122, DOI 10.1145/3394171.3413678
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Kiela D., 2020, Advances in neural information processing systems, V33, P2611
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai SN, 2023, Arxiv, DOI arXiv:2305.07611
   Lei YX, 2023, Arxiv, DOI arXiv:2307.13205
   Li Junnan, 2022, INT C MACHINE LEARNI, V162, P12888, DOI DOI 10.48550/ARXIV.2201.12086
   Loshchilov I, 2019, Arxiv, DOI [arXiv:1711.05101, 10.48550/arXiv.1711.05101, DOI 10.48550/ARXIV.1711.05101]
   Mai SJ, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P481
   Mao R, 2023, IEEE T AFFECT COMPUT, V14, P1743, DOI 10.1109/TAFFC.2022.3204972
   Mao R, 2021, AAAI CONF ARTIF INTE, V35, P13534
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P1, DOI DOI 10.48550/ARXIV.1310.4546
   Morency L.-P., 2011, P 13 INT C MULT INT, P169, DOI [DOI 10.1145/2070481.2070509, 10.1145/2070481.2070509]
   Niu T., 2016, MultiMedia Modeling, V9517, P15, DOI [DOI 10.1007/978-3-319-27674-82, 10.1007/978-3-319-27674-8_2]
   Nojavanasghari B, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P284, DOI 10.1145/2993148.2993176
   Paszke A, 2019, ADV NEUR IN, V32
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Poria S, 2015, P 2015 C EMP METH NA, P2539, DOI DOI 10.18653/V1/D15-1303
   Radford A, 2021, PR MACH LEARN RES, V139
   Rashed A, 2022, PROCEEDINGS OF THE 16TH ACM CONFERENCE ON RECOMMENDER SYSTEMS, RECSYS 2022, P71, DOI 10.1145/3523227.3546777
   Sanh V, 2020, Arxiv, DOI [arXiv:1910.01108, 10.48550/arXiv.1910.01108]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun T, 2023, PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2023, P5861, DOI 10.1145/3581783.3612051
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan M., 2019, INT C MACHINE LEARNI, P6105, DOI DOI 10.48550/ARXIV.1905.11946
   Toledo GL, 2022, arXiv
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang YT, 2023, Arxiv, DOI arXiv:2305.13583
   Wolf T, 2020, Arxiv, DOI arXiv:1910.03771
   Xiao LW, 2024, INFORM FUSION, V106, DOI 10.1016/j.inffus.2024.102304
   Xiao LW, 2022, NEUROCOMPUTING, V471, P48, DOI 10.1016/j.neucom.2021.10.091
   Xu J, 2022, 2022 INT JOINT C NEU, P1, DOI [10.1109/IJCNN55064.2022.9892027.IEEE, DOI 10.1109/IJCNN55064.2022.9892027.IEEE]
   Yu J, 2020, Assoc Comput Linguis
   Yue T, 2023, INFORM FUSION, V100, DOI 10.1016/j.inffus.2023.101921
   Zhang HY, 2023, Arxiv, DOI arXiv:2310.05804
   Zhang S, 2022, arXiv
NR 40
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD SEP
PY 2024
VL 13
IS 3
AR 31
DI 10.1007/s13735-024-00340-w
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZG1F1
UT WOS:001274043200001
DA 2024-08-05
ER

PT J
AU Giveki, D
AF Giveki, Davar
TI Human action recognition using an optical flow-gated recurrent neural
   network
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Spatial feature; Gated recurrent unit; Convolutional neural networks;
   Motion feature; Action recognition
ID LSTM; FEATURES
AB Recognizing various human actions in videos is considered a highly complicated problem, which has many potential applications in solving real-world problems such as human behavior analysis, artificial intelligence, video surveillance, and smart manufacturing. Therefore, designing novel approaches for automatically understanding video data is highly demanded. Towards this goal, different algorithms have been investigated, concentrating on extracting the spatial information and the temporal dependencies. However, motion feature extraction is engineered and isolated from the learning operations. In this paper, to comprehend motion features along with the spatial information and the time dependencies, an innovative attempt is made by designing a new Gated Recurrent Unit (GRU) network. Moreover, a novel deep neural network is presented using the proposed GRU to recognize human actions. Evaluations on popular datasets (YouTube2011, UCF50, UCF101, and HMDB51) not only convey the superiority of the proposed GRU in action recognition using an end-to-end learning model but also emphasize on the generalizability of the proposed method. Additionally, to show the applicability and functionality of the proposed model in solving real-world problems, an engine block assembly dataset was collected and the performance of the proposed method was measured on this dataset. Finally, the robustness of the proposed method against various kinds of noise was tested. The obtained results demonstrate the high performance of the proposed method and its robustness against noise.
C1 [Giveki, Davar] Malayer Univ, Dept Comp Engn, POB 65719-95863, Malayer, Iran.
RP Giveki, D (corresponding author), Malayer Univ, Dept Comp Engn, POB 65719-95863, Malayer, Iran.
EM davood.giveki@gmail.com
FX DAS:The link to all data analyzed during this study is included in this
   published article.
CR Ahmad T, 2024, IEEE T COMPUT SOC SY, V11, P973, DOI 10.1109/TCSS.2023.3249152
   Aparat, 2023, Tutorial of complete engine repairing
   Bao WT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13329, DOI 10.1109/ICCV48922.2021.01310
   Cao HW, 2023, SIGNAL IMAGE VIDEO P, V17, P1173, DOI 10.1007/s11760-022-02324-x
   Cho KYHY, 2014, Arxiv, DOI [arXiv:1406.1078, 10.48550/arXiv.1406.1078]
   Dai C, 2020, APPL SOFT COMPUT, V86, DOI 10.1016/j.asoc.2019.105820
   Dastbaravardeh E, 2024, INT J INTELL SYST, V2024, DOI 10.1155/2024/1052344
   Diba Ali, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P593, DOI 10.1007/978-3-030-58558-7_35
   Dua N, 2023, MULTIMED TOOLS APPL, V82, P5369, DOI 10.1007/s11042-021-11885-x
   Duta IC, 2017, MULTIMED TOOLS APPL, V76, P22445, DOI 10.1007/s11042-017-4795-6
   Fischer P, 2015, Arxiv, DOI arXiv:1504.06852
   Gupta N, 2022, ARTIF INTELL REV, V55, P4755, DOI 10.1007/s10462-021-10116-x
   Hao WL, 2019, PATTERN RECOGN, V92, P13, DOI 10.1016/j.patcog.2019.03.005
   He JY, 2021, NEUROCOMPUTING, V444, P319, DOI 10.1016/j.neucom.2020.05.118
   Hu K, 2023, ARTIF INTELL REV, V56, P1833, DOI 10.1007/s10462-022-10210-8
   Hua M, 2021, COMPUT AIDED GEOM D, V85, DOI 10.1016/j.cagd.2021.101965
   Nguyen HP, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-39744-9
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Islam MM, 2022, COMPUT BIOL MED, V149, DOI 10.1016/j.compbiomed.2022.106060
   Jiang BY, 2019, IEEE I CONF COMP VIS, P2000, DOI 10.1109/ICCV.2019.00209
   Joe Yue-Hei Ng, 2018, 2018 IEEE Winter Conference on Applications of Computer Vision (WACV). Proceedings, P1616, DOI 10.1109/WACV.2018.00179
   Jung M, 2018, NEURAL NETWORKS, V105, P356, DOI 10.1016/j.neunet.2018.05.009
   Keshavarzian A, 2019, FUTURE GENER COMP SY, V101, P14, DOI 10.1016/j.future.2019.06.009
   Khan MA, 2024, MULTIMED TOOLS APPL, V83, P14885, DOI 10.1007/s11042-020-08806-9
   Khodabandelou G, 2023, ENG APPL ARTIF INTEL, V118, DOI 10.1016/j.engappai.2022.105702
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Li WH, 2018, IEEE ACCESS, V6, P44211, DOI 10.1109/ACCESS.2018.2863943
   Li ZY, 2018, COMPUT VIS IMAGE UND, V166, P41, DOI 10.1016/j.cviu.2017.10.011
   Lin J, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103740
   Liu CC, 2021, VISUAL COMPUT, V37, P1327, DOI 10.1007/s00371-020-01868-8
   Liu JG, 2009, PROC CVPR IEEE, P1996
   Liu JY, 2015, IEEE IMAGE PROC, P793, DOI 10.1109/ICIP.2015.7350908
   Lu LM, 2022, IEEE ACCESS, V10, P66797, DOI 10.1109/ACCESS.2022.3185112
   Mim TR, 2023, EXPERT SYST APPL, V216, DOI 10.1016/j.eswa.2022.119419
   Muhammad K, 2021, FUTURE GENER COMP SY, V125, P820, DOI 10.1016/j.future.2021.06.045
   Nafea O, 2022, INT J MULTIMED INF R, V11, P135, DOI 10.1007/s13735-022-00234-9
   Peng XJ, 2016, COMPUT VIS IMAGE UND, V150, P109, DOI 10.1016/j.cviu.2016.03.013
   Ranasinghe K, 2022, PROC CVPR IEEE, P2864, DOI 10.1109/CVPR52688.2022.00289
   Reddy KK, 2013, MACH VISION APPL, V24, P971, DOI 10.1007/s00138-012-0450-4
   Shanableh T, 2023, IEEE ACCESS, V11, P73971, DOI 10.1109/ACCESS.2023.3296252
   Shi XJ, 2015, ADV NEUR IN, V28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Soomro K., 2012, ARXIV
   Spolaôr N, 2020, ENG APPL ARTIF INTEL, V90, DOI 10.1016/j.engappai.2020.103557
   Sun L, 2017, IEEE I CONF COMP VIS, P2166, DOI 10.1109/ICCV.2017.236
   Sun XJ, 2022, IEEE SYST J, V16, P5845, DOI 10.1109/JSYST.2022.3153503
   Tong LN, 2022, IEEE SENS J, V22, P6164, DOI 10.1109/JSEN.2022.3148431
   Ullah A, 2021, APPL SOFT COMPUT, V103, DOI 10.1016/j.asoc.2021.107102
   Ullah A, 2019, IEEE T IND ELECTRON, V66, P9692, DOI 10.1109/TIE.2018.2881943
   Wang HR, 2021, NEUROCOMPUTING, V423, P1, DOI 10.1016/j.neucom.2020.10.037
   Wang H, 2016, INT J COMPUT VISION, V119, P219, DOI 10.1007/s11263-015-0846-5
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang JM, 2021, NEUROCOMPUTING, V451, P265, DOI 10.1016/j.neucom.2021.04.071
   Wang ZW, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12125784
   Xiao J., 2022, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, P3252
   Xing Z, 2023, PROC CVPR IEEE, P18816, DOI 10.1109/CVPR52729.2023.01804
   Xiong QQ, 2020, J MANUF SYST, V56, P605, DOI 10.1016/j.jmsy.2020.04.007
   Xiong X, 2022, COMPUT INTEL NEUROSC, V2022, DOI 10.1155/2022/6608448
   Xu J, 2021, NEUROCOMPUTING, V441, P350, DOI 10.1016/j.neucom.2020.04.150
   Xu YC, 2021, EXPERT SYST APPL, V178, DOI 10.1016/j.eswa.2021.114829
   Yang H, 2019, PATTERN RECOGN, V85, P1, DOI 10.1016/j.patcog.2018.07.028
   Yang YH, 2016, SIGNAL PROCESS, V124, P36, DOI 10.1016/j.sigpro.2015.10.035
   Yenduri S, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108282
   YouTube, 2023, The Restorator
   Yuanyuan Shi, 2018, 2018 IEEE Power & Energy Society General Meeting (PESGM), DOI 10.1109/PESGM.2018.8586227
   Zhang ZF, 2020, NEUROCOMPUTING, V410, P304, DOI 10.1016/j.neucom.2020.06.032
   Zhou S, 2023, NEURAL NETWORKS, V168, P496, DOI 10.1016/j.neunet.2023.09.031
   Zhu Y, 2019, LECT NOTES COMPUT SC, V11363, P363, DOI 10.1007/978-3-030-20893-6_23
NR 69
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD SEP
PY 2024
VL 13
IS 3
AR 29
DI 10.1007/s13735-024-00338-4
PG 18
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YM7K1
UT WOS:001268968600001
DA 2024-08-05
ER

PT J
AU Kumar, P
AF Kumar, Pranjal
TI Adversarial attacks and defenses for large language models (LLMs):
   methods, frameworks & challenges
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Adversarial attacks; Artificial intelligence; Natural language
   processing; Machine learning; Neural networks; Large language models;
   ChatGPT; GPT
ID COMPUTER VISION
AB Large language models (LLMs) have exhibited remarkable efficacy and proficiency in a wide array of NLP endeavors. Nevertheless, concerns are growing rapidly regarding the security and vulnerabilities linked to the adoption and incorporation of LLM. In this work, a systematic study focused on the most up-to-date attack and defense frameworks for the LLM is presented. This work delves into the intricate landscape of adversarial attacks on language models (LMs) and presents a thorough problem formulation. It covers a spectrum of attack enhancement techniques and also addresses methods for strengthening LLMs. This study also highlights challenges in the field, such as the assessment of offensive or defensive performance, defense and attack transferability, high computational requirements, embedding space size, and perturbation. This survey encompasses more than 200 recent papers concerning adversarial attacks and techniques. By synthesizing a broad array of attack techniques, defenses, and challenges, this paper contributes to the ongoing discourse on securing LM against adversarial threats.
C1 [Kumar, Pranjal] Lovely Profess Univ, Sch Comp Sci & Engn, Dept Intelligent Syst, Phagwara 144411, Punjab, India.
C3 Lovely Professional University
RP Kumar, P (corresponding author), Lovely Profess Univ, Sch Comp Sci & Engn, Dept Intelligent Syst, Phagwara 144411, Punjab, India.
EM pranjal@nith.ac.in
OI KUMAR, PRANJAL/0000-0002-6167-9884
CR Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318
   Abdelali A, 2024, Long Papers, V1, P487
   Abdullah Hadi, 2021, 2021 IEEE Symposium on Security and Privacy (SP), P730, DOI 10.1109/SP40001.2021.00014
   Abe N, 2004, P 10 ACM SIGKDD INT, P3
   Akbik A, 2019, NAACL HLT 2019: THE 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES: PROCEEDINGS OF THE DEMONSTRATIONS SESSION, P54
   Akhtar N, 2021, IEEE ACCESS, V9, P155161, DOI 10.1109/ACCESS.2021.3127960
   Akhtar N, 2018, IEEE ACCESS, V6, P14410, DOI 10.1109/ACCESS.2018.2807385
   Alotaibi A, 2023, FUTURE INTERNET, V15, DOI 10.3390/fi15020062
   Alsmadi I, 2022, IEEE ACCESS, V10, P17043, DOI 10.1109/ACCESS.2022.3146405
   Alwahedi A., 2024, InternetThings Cyber-Phys. Syst., V4, P167
   Alzantot M, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P2890
   [Anonymous], 2017, On the (statistical) detection of adversarial examples
   Antoun W, 2023, ACT CORIA TALN 2023, V1
   Athalye A, 2018, PR MACH LEARN RES, V80
   Bajaj A, 2023, NEUROCOMPUTING, V558, DOI 10.1016/j.neucom.2023.126787
   Bakhtin A, 2019, Arxiv, DOI arXiv:1906.03351
   Bang Y., 2023, P 13 INT JOINT C NAT, V1, P675, DOI DOI 10.18653/V1/2023.IJCNLP-MAIN.45
   Bao RZ, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P3248
   Barham S, 2019, Arxiv, DOI arXiv:1905.12864
   Bastings Jasmijn, 2020, P 3 BLACKBOXNLP WORK, P149, DOI DOI 10.18653/V1/2020.BLACKBOXNLP-1.14
   Bhattacharjee A, 2023, P 13 INT JOINT C NAT, V1
   Birbil SI, 2004, J GLOBAL OPTIM, V30, P301, DOI 10.1007/s10898-004-8270-3
   Blum Oliver, 2019, Pattern Recognition. 40th German Conference, GCPR 2018. Proceedings: Lecture Notes in Computer Science (LNCS 11269), P199, DOI 10.1007/978-3-030-12939-2_15
   Boffa M, 2024, COMPUT SECUR, V141, DOI 10.1016/j.cose.2024.103805
   Bowman S.R, 2019, 7 INT C LEARN REPR I, DOI [10.18653/v1/w18-5446, DOI 10.18653/V1/W18-5446]
   Bowman S. R., 2015, EMNLP, P632, DOI 10.18653/v1/D15-1075
   Brown G, 2021, ACM S THEORY COMPUT, P123, DOI 10.1145/3406325.3451131
   Brown T. B., 2020, P 34 INT C NEURAL IN, P1
   Buckman J., 2018, P INT C LEARN REPR
   Carlini N., 2017, P 10 ACM WORKSH ART, P3, DOI DOI 10.1145/3128572.3140444
   Carlini N, 2023, Arxiv, DOI arXiv:2307.15008
   Carlini N, 2019, PROCEEDINGS OF THE 28TH USENIX SECURITY SYMPOSIUM, P267
   Caucheteux C, 2021, BioRxiv, P2021
   Chen CY, 2015, IEEE I CONF COMP VIS, P2722, DOI 10.1109/ICCV.2015.312
   Chen DF, 2020, AAAI CONF ARTIF INTE, V34, P3430
   Chen J., 2018, P 35 INT C MACH LEAR, P883, DOI DOI 10.48550/ARXIV.1802.07814
   Chen K, 2022, INT C LEARN REPR
   Chen MS, 2024, IEEE ACCESS, V12, P39081, DOI 10.1109/ACCESS.2024.3356568
   Chen QB, 2022, Arxiv, DOI arXiv:2201.08702
   Chen XJ, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11188450
   Chen YQ, 2023, EUR PHYS J C, V83, DOI 10.1140/epjc/s10052-023-11486-y
   Chen YF, 2023, 39TH ANNUAL COMPUTER SECURITY APPLICATIONS CONFERENCE, ACSAC 2023, P366, DOI 10.1145/3627106.3627196
   Chen YK, 2024, ACM T KNOWL DISCOV D, V18, DOI 10.1145/3654674
   Chen YT, 2023, Arxiv, DOI arXiv:2305.07969
   Cheng MH, 2020, AAAI CONF ARTIF INTE, V34, P3601
   Cheng R, 2015, IEEE T CYBERNETICS, V45, P191, DOI 10.1109/TCYB.2014.2322602
   Chia YK, 2024, P 1 ED WORKSH SCAL B, P35
   Choi M, 2023, P 2023 C EMPIRICAL M, P11370, DOI DOI 10.18653/V1/2023.EMNLP-MAIN
   Cohen Jeremy, 2019, Certified Adversarial Robustness via Randomized Smoothing, P1310
   Coleman Cody, 2017, Training, V100, P102
   Conneau A., 2020, P 58 ANN M ASS COMP, P8440, DOI DOI 10.18653/V1/2020.ACL-MAIN.747
   Costa JC, 2024, IEEE Access
   Dagan Y, 2020, PMLR, P1389
   Das RK, 2020, INTERSPEECH, P4213, DOI 10.21437/Interspeech.2020-1052
   Demontis A, 2019, PROCEEDINGS OF THE 28TH USENIX SECURITY SYMPOSIUM, P321
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dhillon G., 2018, Stochastic activation pruning for robust adversarial defense, P1
   Dong X, 2022, Adversarial attacks and defenses in natural language processing
   Dong XB, 2020, FRONT COMPUT SCI-CHI, V14, P241, DOI 10.1007/s11704-019-8208-z
   Dong Xinshuai, 2021, INT C LEARN REPR
   Duan H, 2023, 61 ANN M ASS COMP LI
   Dupuy C, 2022, INT CONF ACOUST SPEE, P4118, DOI 10.1109/ICASSP43922.2022.9746975
   Durmus E, 2024, Arxiv, DOI arXiv:2306.16388
   Dvijotham K, 2018, Arxiv, DOI arXiv:1805.10265
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14
   Ebrahimi J, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P31
   Fagni T, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0251415
   Feldman Vitaly, 2020, ADV NEURAL INFORM PR, V33, P2881, DOI DOI 10.48550/ARXIV.2008.03703
   Feng S, 2021, Collaborative group learning
   Fu Y, 2023, Arxiv, DOI arXiv:2305.17306
   Fursov I, 2022, IEEE ACCESS, V10, P17966, DOI 10.1109/ACCESS.2022.3148413
   Gan WC, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6065
   Gao J, 2018 IEEE SEC PRIV W, P50
   Gao TY, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P6894
   Garg S, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P6174
   Gehman S, 2020, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020
   Gekhman Z, 2023, 2023 C EMP METH NAT
   Ghojogh B, 2020, PREPRINT
   Glockner M, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P650
   Goldstein A, 2022, NAT NEUROSCI, V25, P369, DOI 10.1038/s41593-022-01026-4
   Gong Z, 2018, Adversarial texts with gradient methods
   Gong ZT, 2023, PROCEEDINGS OF THE SIXTH INTERNATIONAL WORKSHOP ON EXPLOITING ARTIFICIAL INTELLIGENCE TECHNIQUES FOR DATA MANAGEMENT, AIDM 2023, DOI 10.1145/3593078.3593935
   Gowal S, 2019, Arxiv, DOI arXiv:1810.12715
   Goyal S, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3593042
   Guo C., 2018, ICLR (Poster)
   Guo C, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P5747
   Gupta M, 2023, IEEE ACCESS, V11, P80218, DOI 10.1109/ACCESS.2023.3300381
   Han X, 2021, AI OPEN, V2, P225, DOI 10.1016/j.aiopen.2021.08.002
   He P., 2021, INT C LEARN REPR
   He XL, 2024, Arxiv, DOI arXiv:2404.19597
   Heilbron M, 2022, P NATL ACAD SCI USA, V119, DOI 10.1073/pnas.2201968119
   Honovich O, 2022, PROCEEDINGS OF THE SECOND DIALDOC WORKSHOP ON DOCUMENT-GROUNDED DIALOGUE AND CONVERSATIONAL QUESTION ANSWERING (DIALDOC 2022), P161
   Hu SS, 2019, IEEE COMMUN MAG, V57, P120, DOI 10.1109/MCOM.2019.1900006
   Huang PS, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P4083
   Huber L, 2022, PROCEEDINGS OF THE 7TH WORKSHOP ON REPRESENTATION LEARNING FOR NLP, P156
   Jain N, 2024, Baseline defenses for adversarial attacks against aligned language models
   Jasser J, 2022, Arxiv, DOI arXiv:2111.10272
   JIA R., 2017, P 2017 C EMP METH NA, P2021, DOI [DOI 10.18653/V1/D17-1215.URL, DOI 10.18653/V1/D17-1215]
   Jia R., 2020, Building robust natural language processing systems
   Jia R, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P4129
   Jiang WB, 2023, PROC CVPR IEEE, P8133, DOI 10.1109/CVPR52729.2023.00786
   Jin D, 2020, AAAI CONF ARTIF INTE, V34, P8018
   Kawaguchi K., 2022, Mathematical aspects of deep learning
   Keraghel I, 2024, INT S INT DAT AN, P205
   Khatiri S, 2024, SOFTWAREX, V27, DOI 10.1016/j.softx.2024.101748
   Khormali A, 2020, Arxiv, DOI arXiv:2007.00146
   Krishna Kalpesh, 2020, INT C LEARN REPR
   Kumar A, 2024, Arxiv, DOI arXiv:2309.02705
   Kumar S, 2022, Nature Communications, P2022
   Kurita K, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2793
   La Malfa E, 2023, On robustness for natural language processing
   Lai VD, 2023, FINDINGS ASS COMPUTA, P13171
   Laidlaw C, 2019, ADV NEUR IN, V32
   Lan X, 2018, ADV NEUR IN, V31
   Lecuyer M, 2019, P IEEE S SECUR PRIV, P656, DOI 10.1109/SP.2019.00044
   Lee D, 2022, PR MACH LEARN RES
   Lei Q., 2019, Proc Mach Learn Syst, V1, P146
   Li H, 2023, FINDINGS ASS COMPUTA, P4138
   Li JF, 2019, 26TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2019), DOI 10.14722/ndss.2019.23138
   Li LY, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P3023
   Li L, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P6193
   Li XD, 2012, IEEE T EVOLUT COMPUT, V16, P210, DOI 10.1109/TEVC.2011.2112662
   Li Xiaoxiao, 2021, ARXIV210207623
   Li X, 2017, IEEE I CONF COMP VIS, P5775, DOI 10.1109/ICCV.2017.615
   Li YF, 2023, Arxiv, DOI arXiv:2305.13242
   Li Z., 2020, P AS C COMP VIS
   Li Z., 2024, P AAAI C ART INT VAN, V38, P18608, DOI 10.1609/aaai.v38i17.29823
   Liang B, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4208
   Lim S, 2023, FRONT COMMUN, V8, DOI 10.3389/fcomm.2023.1129082
   Lin JY, 2021, ACL-IJCNLP 2021: THE 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 2, P333
   Lin Y., 2023, P 5 WORKSHOP NLP CON, P47, DOI DOI 10.18653/V1/2023.NLP4CONVAI-1
   Liu B., 2023, Secur Commun Netw, V1
   Liu HM, 2023, Arxiv, DOI [arXiv:2304.03439, DOI 10.48550/ARXIV.2304.03439, 10.48550/arXiv.2304.03439]
   Liu X, 2020, arXiv
   Liu X, 2023, Coco: coherence-enhanced machine-generated text detection under low resource with contrastive learning, P16167
   Liu XD, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020): SYSTEM DEMONSTRATIONS, P118
   Liu Y, 2023, Socially Responsible Language Modelling Research
   Liu Y, 2023, Arxiv, DOI arXiv:2306.05499
   Liu YK, 2023, Arxiv, DOI arXiv:2304.07666
   Liu YH, 2019, INFORM SYST RES, DOI 10.48550/arXiv.1907.11692
   Liu YY, 2023, NEUROCOMPUTING, V550, DOI 10.1016/j.neucom.2023.126489
   Liu ZY, 2024, Arxiv, DOI arXiv:2306.05524
   Livne M, 2024, CHEM SCI, V15, P8380, DOI 10.1039/d4sc00966e
   Lopez-Lira A, 2023, Return predictability and large language models
   Luong MT, 2016, 4 INT C LEARN REPR I
   Ma X., 2023, Advances in neural information processing systems, V36, P21702
   Madry A, 2018, INT C LEARN REPR, DOI 10.48550/arXiv.1706.06083
   Maheshwary R, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P8396
   Maheshwary R, 2021, AAAI CONF ARTIF INTE, V35, P13525
   Majmudar J., 2022, Differentially private decoding in large language models
   Maus N, 2023, Arxiv, DOI arXiv:2302.04237
   Mewada A, 2023, J SUPERCOMPUT, V79, P5516, DOI 10.1007/s11227-022-04881-x
   Minh DN, 2022, P 2022 C EMPIRICAL M, P6612
   Mirman M., 2018, P 35 INT C MACH LEAR, P3578
   Miyato T, 2021, Arxiv, DOI arXiv:1605.07725
   Mnassri K, 2024, ENTROPY-SWITZ, V26, DOI 10.3390/e26040344
   Morris JX, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS, P119
   Mounsif M, 2023, INT CONF SOFT COMP, P171, DOI 10.1109/ISCMI59957.2023.10458573
   MRKSIC N., 2016, ANN C N AM CHAPT ASS, P142, DOI 10.18653/v1/N16-1018
   Myers D, 2024, CLUSTER COMPUT, V27, P1, DOI 10.1007/s10586-023-04203-7
   Narang S, 2018, INT C LEARN REPR
   Nazir A, 2024, SOFTW IMPACTS, V19, DOI 10.1016/j.simpa.2024.100619
   Omar M, 2022, IEEE ACCESS, V10, P86038, DOI 10.1109/ACCESS.2022.3197769
   Papernot N, 2016, IEEE MILIT COMMUN C, P49, DOI 10.1109/MILCOM.2016.7795300
   Parkhi O, 2015, BMVC 2015
   Peng H, 2023, J KING SAUD UNIV-COM, V35, DOI 10.1016/j.jksuci.2023.03.017
   Perez F, 2022, Arxiv, DOI [arXiv:2211.09527, DOI 10.48550/ARXIV.2211.09527]
   Peris C., 2023, SER WSDM 23, P1291, DOI DOI 10.1145/3539597
   Pruthi D, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P5582
   Qi FC, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9558
   Qi X, 2024, P AAAI C ARTIFICIAL, V38, P21527
   Radford A, 2018, Improving language understanding by generative Pre-Training
   Radford A., 2019, OpenAI blog, V1, P9
   Raghunathan A., 2018, INT C LEARN REPR
   Raghunathan A, Adv Neural Inf Process Syst, V31
   Raiaan MAK, 2024, IEEE ACCESS, V12, P26839, DOI 10.1109/ACCESS.2024.3365742
   Rajpurkar P., 2016, P 2016 C EMP METH NA, DOI DOI 10.18653/V1/D16-1264
   Rame A., 2021, ICLR 2021
   Rane NL., 2023, International Research Journal of Modernization in Engineering Technology and Science, V5, P875
   Ren SH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1085
   Ribeiro MT, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P856
   Roshan K, 2024, COMPUT SECUR, V141, DOI 10.1016/j.cose.2024.103853
   Roth T, 2024, AI Commun, P1
   Ruder S, 2019, AAAI CONF ARTIF INTE, P4822
   Sadrizadeh S, 2022, INT CONF ACOUST SPEE, P7837, DOI 10.1109/ICASSP43922.2022.9747475
   Salman H., 2019, Adv. neural inf. process. syst, V32
   Saon G, 2017, INTERSPEECH, P132, DOI 10.21437/Interspeech.2017-405
   Sato M, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4323
   Shafahi A, 2019, ADV NEUR IN, V32
   Shen LJ, 2021, CCS '21: PROCEEDINGS OF THE 2021 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P3141, DOI 10.1145/3460120.3485370
   Shin T, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P4222
   Shrikumar A, 2017, PR MACH LEARN RES, V70
   Singla S., 2020, INT C MACH LEARN, P8981
   Smith LN, 2017, IEEE WINT CONF APPL, P464, DOI 10.1109/WACV.2017.58
   Song S, 2013, IEEE GLOB CONF SIG, P245, DOI 10.1109/GlobalSIP.2013.6736861
   Steinhardt J, 2017, ADV NEUR IN, V30
   Sundararajan M, 2017, PR MACH LEARN RES, V70
   Sutskever I, 2014, ADV NEUR IN, V27
   Szegedy C, 2014, Arxiv, DOI arXiv:1312.6199
   Szeghy D, 2021, Adversarial perturbation stability of the layered group basis pursuit, P2
   Le T, 2020, IEEE DATA MINING, P282, DOI [10.1109/ICDM50108.2020.00037, 10.1109/CSCI51800.2020.00054]
   Tjong Kim Sang E.F., 2003, P 7 C NAT LANG LEARN, P142
   Tsiligkaridis T, 2022, PROC CVPR IEEE, P50, DOI 10.1109/CVPR52688.2022.00015
   Uchendu A, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P8384
   Uesato J., 2018, P MACHINE LEARNING R, P5025
   Ullah S, 2024, IEEE S SEC PRIV
   Vassilev Apostol, 2024, Adversarial Machine Learning, DOI [10.6028/nist.ai.100-2e2023, DOI 10.6028/NIST.AI.100-2E2023]
   Vaswani A, 2017, ADV NEUR IN, V30
   Wallace E, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2153
   Wang B, 2023, Towards trustworthy large language models
   Wang B, 2022, Findings of the Association for Computational Linguistics: NAACL, P176, DOI DOI 10.18653/V1/2022.FINDINGS-NAACL.14
   Wang BX, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P6134
   Wang Q, 2019, INTERSPEECH, P4010, DOI 10.21437/Interspeech.2019-2983
   Wang T, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P5141
   Wang W., 2024, Adv Neural Inf Process Syst, V36, P61501
   Wang WJ, 2021, 2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL-HLT 2021), P1102
   Wang WQ, 2023, IEEE T KNOWL DATA EN, V35, P3159, DOI 10.1109/TKDE.2021.3117608
   Wang YL, 2023, IEEE COMMUN SURV TUT, V25, P2245, DOI 10.1109/COMST.2023.3319492
   Wang YT, 2023, IEEE OPEN J COMP SOC, V4, P280, DOI 10.1109/OJCS.2023.3300321
   Wei A., 2024, Adv Neural Inf Process Syst, V36, P80079
   Wei C, 2021, Adv Neural Inf Process Syst, P16158
   Wong E., 2020, P 8 INT C LEARN REPR
   Wong E, 2018, PR MACH LEARN RES, V80
   Wu C, 2023, CHIN HLTH INF PROC C, P214
   Wu GL, 2021, AAAI CONF ARTIF INTE, V35, P10302
   Wu JC, 2024, Arxiv, DOI arXiv:2310.14724
   Wu XS, 2024, Arxiv, DOI arXiv:2403.08946
   Xie X., 2024, Vis Intell, V2, P11, DOI [10.1007/s44267-024-00043-0, DOI 10.1007/S44267-024-00043-0]
   Xu H, 2020, INT J AUTOM COMPUT, V17, P151, DOI 10.1007/s11633-019-1211-x
   Xu X, 2024, 12 INT C LEARN REPR
   Xue J., 2024, Adv Neural Inf Process Syst, V36, P65665
   Yan YM, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P5065
   Yang H, 2023, MATH PROGRAM, V201, P409, DOI 10.1007/s10107-022-01912-6
   Yang J., 2024, ACM Transactions on Knowledge Discovery from Data, V18, P1, DOI 10.1145/3653304
   Yang Q, 2018, IEEE T EVOLUT COMPUT, V22, P578, DOI 10.1109/TEVC.2017.2743016
   Yang YQ, 2023, ARTIF INTELL REV, V56, P5545, DOI 10.1007/s10462-022-10283-5
   Yao YF, 2024, HIGH-CONFID COMPUT, V4, DOI 10.1016/j.hcc.2024.100211
   Ye M, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P3465
   Yoo J.Y., 2020, P 3 BLACKBOXNLP WORK, P323
   Yoo K, 2022, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), P3656
   Yu XY, 2019, IEEE T NEUR NET LEAR, V30, P2805, DOI 10.1109/TNNLS.2018.2886017
   Yuan L, 2024, Adv Neural Inf Process Syst, V36
   Yuan LP, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P1612
   Yuan LP, 2021, Arxiv, DOI arXiv:2103.11578
   ZANG Y., 2020, P 58 ANN M ASS COMP, P6066, DOI DOI 10.18653/V1/2020.ACL-MAIN.540
   Zang Y, 2020, Arxiv, DOI arXiv:2009.09192
   Zhang D, 2019, Adv Neural Inf Process Syst, V32
   Zhang WE, 2020, ACM T INTEL SYST TEC, V11, DOI 10.1145/3374217
   Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454
   Zhang Y, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P1298
   Zhang Z, 2023, Arxiv, DOI arXiv:2307.07171
   Zhang ZY, 2023, MACH INTELL RES, V20, P180, DOI 10.1007/s11633-022-1377-5
   Zhao Haiteng, 2022, P MACHINE LEARNING R
   Zhao Y, 2014, Adv Neural Inf Process Syst, V36, P54111
   Zhong WJ, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P2461
   Zou A, 2023, Universal and transferable adversarial attacks on aligned language models
   Zuccon G, 2023, ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL IN THE ASIA PACIFIC REGION, SIGIR-AP 2023, P46, DOI 10.1145/3624918.3625329
NR 257
TC 0
Z9 0
U1 7
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD SEP
PY 2024
VL 13
IS 3
AR 26
DI 10.1007/s13735-024-00334-8
PG 28
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WI1T1
UT WOS:001254154900001
DA 2024-08-05
ER

PT J
AU Verma, N
   De, A
   Mishra, A
AF Verma, Neelu
   De, Anik
   Mishra, Anand
TI Bridging language to visuals: towards natural language query-to-chart
   image retrieval
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Chart images; Chart-encoder; Semantic labeling; Retrieval; BERT
AB Given a natural language query, mining a relevant chart image, i.e., the one that contains the answer to the query, is an overlooked problem in the literature. Our study explores this novel problem. Consider an example of retrieving relevant chart images for a query: Which Indian city has the highest annual rainfall over the past decade?. Retrieving relevant chart images for such natural language queries necessitates a deep semantic understanding of chart images. Towards addressing this problem, in this work, we make two key contributions: (a) We present a dataset, namely WebCIRD (or Web Chart Image Retrieval) for studying this problem, and (b) propose a solution viz. ChartSemBERT that offers a deeper semantic understanding of chart images for effective natural language-to-chart image retrieval. Our proposed approach yields remarkable performance improvements compared to the existing baselines, achieving R@10 as 86.9%.
C1 [Verma, Neelu; De, Anik; Mishra, Anand] Indian Inst Technol Jodhpur IITJ, Jodhpur, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Jodhpur
RP Verma, N (corresponding author), Indian Inst Technol Jodhpur IITJ, Jodhpur, India.
EM verma.14@iitj.ac.in; anekde@gmail.com; mishra@iitj.ac.in
FX DAS:We extended a public dataset that has been released under an MIT
   license by adding paraphrasing of queries and making it suitable for
   retrieval study by creating hard negative and relevant chart image pools
   for every query. We shall make our changes to the dataset publicly
   available upon acceptance of this work.
CR Adam Paszke, 2019, Advances in neural information processing systems 32
   Alon Talmor, 2021, 9 INT C LEARN REPR I
   Ashish Vaswani, 2017, NEURIPS
   Bajic F, 2021, J IMAGING, V7, DOI 10.3390/jimaging7110220
   Chang YS, 2022, PROC CVPR IEEE, P16474, DOI 10.1109/CVPR52688.2022.01600
   Charles Adjetey, 2021, International Journal of Advanced Computer Science and Applications, V12
   Chiu Jason PC, 2015, arXiv
   Church KW, 2017, NAT LANG ENG, V23, P155, DOI 10.1017/S1351324916000334
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Fartash Faghri, 2018, BRIT MACH VIS C 2018, pp12
   Gabriel Ilharco, 2021, Openclip. If you use this software, please cite it as below
   Georgios Fradelos, 2023, AIAI WORKSH, P381
   Li LH, 2019, Arxiv, DOI [arXiv:1908.03557, DOI 10.48550/ARXIV.1908.03557]
   Hsu Chao-Chun, 2021, FINDINGS ACL IJCNLP
   Junnan Li, 2021, Advances in neural information processing systems, P9694
   Kafle K, 2018, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2018.00592
   KellyRossa Sungkono, 2021, Int J Intell Eng Syst, V14, P377
   Kim W, 2021, PR MACH LEARN RES, V139
   Kingma D. P., 2014, arXiv
   Kusner MJ, 2015, PR MACH LEARN RES, V37, P957
   Lebret R, 2015, PR MACH LEARN RES, V37, P2085
   Liu CZ, 2018, 2018 IEEE INTERNATIONAL CONFERENCE OF INTELLIGENT ROBOTICS AND CONTROL ENGINEERING (IRCE), P218, DOI 10.1109/IRCE.2018.8492945
   Manolis Savva, 2011, P 24 ANN ACM S USER, P393
   Methani N, 2020, IEEE WINT CONF APPL, P1516, DOI 10.1109/WACV45572.2020.9093523
   Pan H, 2022, INT J MULTIMED INF R, V11, P369, DOI 10.1007/s13735-022-00237-6
   Pawar A, 2018, Arxiv, DOI [arXiv:1802.05667, DOI arXiv:1802.05667.v2]
   Penamakuri AS, 2023, PROCEEDINGS OF THE THIRTY-SECOND INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, IJCAI 2023, P1312
   Pengyu Yan, 2023, Document Analysis and Recognition-ICDAR
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Radford A, 2021, PR MACH LEARN RES, V139
   Reddy R, 2019, IEEE IJCNN, DOI 10.1109/ijcnn.2019.8851830
   Reimers N, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P567
   SamiraEbrahimi Kahou, 2018, 6 INT C LEARN REPR I
   Socher Richard., 2014, Trans Assoc Comput Linguist, V2, P207, DOI [DOI 10.1162/TACLA00177, DOI 10.1162/TACL_A_00177]
   Wolf T, 2020, Arxiv, DOI arXiv:1910.03771
   Yao L, 2023, INT J MULTIMED INF R, V12, DOI 10.1007/s13735-023-00283-8
   Zhai XH, 2023, IEEE I CONF COMP VIS, P11941, DOI 10.1109/ICCV51070.2023.01100
   Zhang Xinsong, 2023, FINDINGS ASS COMPUTA, P551
NR 39
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD SEP
PY 2024
VL 13
IS 3
AR 32
DI 10.1007/s13735-024-00343-7
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A1Z1F
UT WOS:001280573600002
DA 2024-08-05
ER

PT J
AU Dogan, G
   Ergen, B
AF Dogan, Gurkan
   Ergen, Burhan
TI A new CNN-based semantic object segmentation for autonomous vehicles in
   urban traffic scenes
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Autonomous vehicles; Street scenes; Traffic scenes; Pixel-wise semantic
   segmentation; Deep learning; Computer vision
ID TEXTURE
AB Semantic segmentation is the most important stage of making sense of the visual traffic scene for autonomous driving. In recent years, convolutional neural networks (CNN)-based methods for semantic segmentation of urban traffic scenes are among the trending studies. However, the methods developed in the studies carried out so far are insufficient in terms of accuracy performance criteria. In this study, a new CNN-based semantic segmentation method with higher accuracy performance is proposed. A new module, the Attentional Atrous Feature Pooling (AAFP) Module, has been developed for the proposed method. This module is located between the encoder and decoder in the general network structure and aims to obtain multi-scale information and add attentional features to large and small objects. As a result of experimental tests with the CamVid data set, an accuracy value of approximately 2% higher was achieved with a mIoU value of 70.59% compared to other state-of-art methods. Therefore, the proposed method can semantically segment objects in the urban traffic scene better than other methods.
C1 [Dogan, Gurkan] Munzur Univ, Fac Engn, Dept Comp Engn, Tunceli, Turkiye.
   [Ergen, Burhan] Firat Univ, Fac Engn, Dept Comp Engn, Elazig, Turkiye.
C3 Munzur University; Firat University
RP Dogan, G (corresponding author), Munzur Univ, Fac Engn, Dept Comp Engn, Tunceli, Turkiye.
EM gurkandogan@munzur.edu.tr; bergen@firat.edu.tr
RI ERGEN, Burhan/A-8961-2016; DOĞAN, Gürkan/ITU-2688-2023
OI ERGEN, Burhan/0000-0003-3244-2615; DOĞAN, Gürkan/0000-0003-2497-8348
FU Munzur University
FX No Statement Available
CR Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Benoughidene A, 2022, INT J MULTIMED INF R, V11, P653, DOI 10.1007/s13735-022-00251-8
   Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005
   Cai SL, 2022, INT J MULTIMED INF R, V11, P599, DOI 10.1007/s13735-022-00248-3
   DICE LR, 1945, ECOLOGY, V26, P297, DOI 10.2307/1932409
   Dogan G, 2022, MEASUREMENT, V195, DOI 10.1016/j.measurement.2022.111119
   Dong GS, 2021, IEEE T INTELL TRANSP, V22, P3258, DOI 10.1109/TITS.2020.2980426
   Fan L, 2018, IEEE ACCESS, V6, P50333, DOI 10.1109/ACCESS.2018.2868801
   Hafiz AM, 2021, INT J MULTIMED INF R, V10, P71, DOI 10.1007/s13735-021-00209-2
   Hu XG, 2022, APPL INTELL, V52, P580, DOI 10.1007/s10489-021-02446-8
   Ilesanmi AE, 2022, INT J MULTIMED INF R, V11, P315, DOI 10.1007/s13735-022-00242-9
   Jo K, 2015, IEEE T IND ELECTRON, V62, P5119, DOI 10.1109/TIE.2015.2410258
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Malik J, 2001, INT J COMPUT VISION, V43, P7, DOI 10.1023/A:1011174803800
   Parseh MJ, 2022, INT J MULTIMED INF R, V11, P619, DOI 10.1007/s13735-022-00246-5
   Romera E, 2018, IEEE T INTELL TRANSP, V19, P263, DOI 10.1109/TITS.2017.2750080
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shotton J, 2009, INT J COMPUT VISION, V81, P2, DOI 10.1007/s11263-007-0109-1
   Zhang XT, 2019, IEEE T IND INFORM, V15, P1183, DOI 10.1109/TII.2018.2849348
   Zhao GZ, 2023, CAAI T INTELL TECHNO, V8, P297, DOI 10.1049/cit2.12118
NR 20
TC 0
Z9 0
U1 13
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD MAR
PY 2024
VL 13
IS 1
AR 11
DI 10.1007/s13735-023-00313-5
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IR2Q7
UT WOS:001167995300001
OA hybrid
DA 2024-08-05
ER

PT J
AU Lee, YH
AF Lee, Younghoon
TI Opinion convergence-based sentiment prediction of image advertisement
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Image advertisement; Image sentiment; Sentiment prediction; Opinion
   convergence; Hypothetical labeler
AB In this study, a novel approach was proposed for the sentiment prediction of image advertisements. Unlike general multilabel problems with correct answers, image sentiment prediction is a multilabel problem that involves converging the opinions of various labelers. Therefore, an opinion convergence-based image sentiment prediction methodology was proposed to model the decision-making process of image sentiment prediction. Hypothetical labelers were generated by recombining training datasets to maximize the characteristic distance, and each prediction model was trained with each combination to represent each hypothetical labeler with distinct characteristics. The results of the experiment revealed that the proposed image sentiment prediction method outperformed other existing models with advanced architectures or considered various factors for improving the accuracy of image sentiment prediction tasks. Moreover, the effectiveness of the proposed method was verified through qualitative experiments.
C1 [Lee, Younghoon] Seoul Natl Univ Sci & Technol, Dept Ind Engn, 232 Gongneung Ro, Seoul 01811, South Korea.
C3 Seoul National University of Science & Technology
RP Lee, YH (corresponding author), Seoul Natl Univ Sci & Technol, Dept Ind Engn, 232 Gongneung Ro, Seoul 01811, South Korea.
EM yhoon.lee@seoultech.ac.kr
FU SeoulTech
FX This study was supported by a research program funded by SeoulTech.
CR Achlioptas P, 2021, PROC CVPR IEEE, P11564, DOI 10.1109/CVPR46437.2021.01140
   Adaval Rashmi., 2018, CONSUMER PSYCHOL REV
   Asakawa Tetsuya, 2021, ICCCV'21: 2021 4th International Conference on Control and Computer Vision, P142, DOI 10.1145/3484274.3484296
   Corchs S, 2019, INT J MACH LEARN CYB, V10, P2057, DOI 10.1007/s13042-017-0734-0
   Ghani B, 2023, BEHAV INFORM TECHNOL, V42, P2407, DOI 10.1080/0144929X.2022.2126329
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hussain Z, 2017, PROC CVPR IEEE, P1100, DOI 10.1109/CVPR.2017.123
   Jabreel M, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9061123
   Jia ZW, 2023, Arxiv, DOI arXiv:2305.18373
   Joshi D, 2011, IEEE SIGNAL PROC MAG, V28, P94, DOI 10.1109/MSP.2011.941851
   Kim BK, 2019, J ADVERTISING, V48, P251, DOI 10.1080/00913367.2019.1597787
   Kingma D., 3 INT C LEARNING REP
   Kong SJ, 2019, J VACAT MARK, V25, P130, DOI 10.1177/1356766718757272
   Kujur F, 2020, J THEOR APPL EL COMM, V15, P30, DOI 10.4067/S0718-18762020000100104
   Kumar Y., 2023, P AAAI C ART INT, P57
   Li L, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3359753
   Lin C, 2020, AAAI CONF ARTIF INTE, V34, P2661
   Pilli S, 2020, IEEE COMPUT SOC CONF, P1640, DOI 10.1109/CVPRW50498.2020.00212
   Poels K, 2019, J ADVERTISING, V48, P81, DOI 10.1080/00913367.2019.1579688
   Ruan SL, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102855
   SMITH RA, 1991, J ADVERTISING, V20, P13, DOI 10.1080/00913367.1991.10673351
   Song KK, 2018, NEUROCOMPUTING, V312, P218, DOI 10.1016/j.neucom.2018.05.104
   Tan MX, 2019, PR MACH LEARN RES, V97
   Yang JF, 2017, AAAI CONF ARTIF INTE, P224
   Zhang HM, 2023, IEEE T MULTIMEDIA, V25, P2203, DOI 10.1109/TMM.2022.3144804
   Zhang HZ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P430, DOI 10.1145/3394171.3413582
   Zhao SC, 2022, IEEE T PATTERN ANAL, V44, P6729, DOI 10.1109/TPAMI.2021.3094362
NR 27
TC 0
Z9 0
U1 6
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD MAR
PY 2024
VL 13
IS 1
AR 6
DI 10.1007/s13735-023-00314-4
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FY0D7
UT WOS:001149286600002
DA 2024-08-05
ER

PT J
AU Zhou, SR
   Li, ZX
   Liu, J
   Zhou, JR
   Zhang, JM
AF Zhou, Shuren
   Li, Zhixiong
   Liu, Jie
   Zhou, Jiarui
   Zhang, Jianming
TI Progressive spatial-temporal transfer model for unsupervised person
   re-identification
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Person re-identification; Transfer learning; Spatial-temporal; Feature
   fusion; Neural network
ID NETWORK
AB Over the past decade, a more widespread area of computer vision research has been person re-identification (P-Reid). This technology is applied in fields such as pedestrian tracking, security, and video surveillance. Currently, person re-identification performs well when supervised with labeled data, but accuracy frequently suffers when learning unsupervised on unlabeled samples. Therefore, improving unlabeled samples model is a challenging endeavor. In order to solve this problem, we propose a progressive spatial-temporal transfer model (PSTT), which consists of three stages, including incremental tuning, spatial-temporal fusion and target domain learning. In the first stage, a high-performance multi-scale network that can initially cluster samples is obtained through triplet loss function. In the next stage, to mine spatial-temporal and visual semantic information, we introduce a fusion model that fuses the visual information extracted from the labeled dataset and the unlabeled dataset using a trained network with its spatial-temporal information. In the final stage, with the assistance of fusion model, we employ a strategy that extends learning from labeled to unlabeled samples. During the training, the fusion model is used to select labeled and unlabeled samples, and multiple meta loss function is used for transfer learning. During the testing, the fusion model is employed to enhance the accuracy of network. In the experiment, we evaluate our method on five standard P-Reid benchmarks: Market1501, DukeMTMC-ReID, CUHK03, MSMT17 and Occluded-DukeMTMC. Extensive experiments show that our proposed PSTT achieves state-of-the-art performance, exceeding the previous method by a certain margin. The source code is available at https://github.com/LiZX12/PSTT.
C1 [Zhou, Shuren; Li, Zhixiong; Liu, Jie; Zhou, Jiarui; Zhang, Jianming] Changsha Univ Sci & Technol, Sch Comp & Commun Engn, Changsha 410114, Hunan, Peoples R China.
C3 Changsha University of Science & Technology
RP Zhou, SR (corresponding author), Changsha Univ Sci & Technol, Sch Comp & Commun Engn, Changsha 410114, Hunan, Peoples R China.
EM zsr_hn@163.com
FU Humanities and Social Sciences Planning Fund Projects of Ministry of
   Education of China;  [23YJAZH226];  [2023-09 ~2026-08]
FX This work was supported in part by the Humanities and Social Sciences
   Planning Fund Projects of Ministry of Education of China under Grant
   23YJAZH226 and "Research on the Development Path of Artificial
   Intelligence Based on ChatGPT-like Generated Content", 2023-09 ~2026-08.
CR Barz B, 2019, IEEE T PATTERN ANAL, V41, P1088, DOI 10.1109/TPAMI.2018.2823766
   Chen H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14940, DOI 10.1109/ICCV48922.2021.01469
   Chen L, 2019, IEEE ACCESS, V7, P41230, DOI 10.1109/ACCESS.2019.2907274
   Chen PX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11813, DOI 10.1109/ICCV48922.2021.01162
   Cho Y, 2022, PROC CVPR IEEE, P7298, DOI 10.1109/CVPR52688.2022.00716
   Dai Yongxing, 2021, P IEEE CVF INT C COM, P11864
   Ding GD, 2019, IEEE T MULTIMEDIA, V21, P2891, DOI 10.1109/TMM.2019.2916456
   Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099
   Fu Y, 2019, AAAI CONF ARTIF INTE, P8295
   Ge YX, 2020, Arxiv, DOI [arXiv:2001.01526, 10.48550/arXiv.2001.01526]
   Gupta A., 2022, Intelligent Systems with Applications, V16, P200137, DOI DOI 10.1016/J.ISWA.2022.200137
   Han J, 2022, AAAI CONF ARTIF INTE, P790
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hu RH, 2021, IEEE T INF FOREN SEC, V16, P4721, DOI 10.1109/TIFS.2021.3113517
   Huang YR, 2020, AAAI CONF ARTIF INTE, V34, P11069
   Huang YK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P215, DOI 10.1109/ICCV48922.2021.00028
   Isobe T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8506, DOI 10.1109/ICCV48922.2021.00841
   Jiao BL, 2022, LECT NOTES COMPUT SC, V13674, P285, DOI 10.1007/978-3-031-19781-9_17
   Gómez-Silva MJ, 2019, INTEGR COMPUT-AID E, V26, P329, DOI 10.3233/ICA-190603
   Ke Han, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P193, DOI 10.1007/978-3-030-58574-7_12
   Khan SU, 2022, INT J INTELL SYST, V37, P5924, DOI 10.1002/int.22820
   Lejbolle AR, 2018, IET BIOMETRICS, V7, P125, DOI 10.1049/iet-bmt.2016.0200
   Li Q, 2022, PATTERN RECOGN, V125, DOI 10.1016/j.patcog.2022.108521
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Lin P., 2021, arXiv
   Lin YT, 2020, PROC CVPR IEEE, P3387, DOI 10.1109/CVPR42600.2020.00345
   Lin YT, 2019, AAAI CONF ARTIF INTE, P8738
   Liu YH, 2019, AAAI CONF ARTIF INTE, P8786
   Lv JM, 2018, PROC CVPR IEEE, P7948, DOI 10.1109/CVPR.2018.00829
   Pang ZQ, 2023, KNOWL-BASED SYST, V263, DOI 10.1016/j.knosys.2023.110263
   Pu N, 2023, IEEE Transactions on Pattern Analysis and Machine Intelligence
   Pu N, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2149, DOI 10.1145/3394171.3413673
   Pu Nan, 2021, P IEEECVF C COMPUTER, P7901
   Qi L, 2020, IEEE T CIRC SYST VID, V30, P2815, DOI 10.1109/TCSVT.2020.2983600
   Qian R, 2021, PROC CVPR IEEE, P6960, DOI 10.1109/CVPR46437.2021.00689
   Ren M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14910, DOI 10.1109/ICCV48922.2021.01466
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sun J, 2022, KNOWL-BASED SYST, V251, DOI 10.1016/j.knosys.2022.109162
   Walker W.I, 2023, INT C ARTIFICIAL INT, P4209
   Wang GC, 2019, AAAI CONF ARTIF INTE, P8933
   Wang ML, 2022, IEEE T IMAGE PROCESS, V31, P6548, DOI 10.1109/TIP.2022.3213193
   Wang Y., 2020, INT C MACHINE LEARNI, P10778
   Wang YY, 2020, J INTELL FUZZY SYST, V39, P4453, DOI 10.3233/JIFS-200435
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wei WY, 2022, J VIS COMMUN IMAGE R, V82, DOI 10.1016/j.jvcir.2021.103418
   Wu C, 2022, PROC CVPR IEEE, P20206, DOI 10.1109/CVPR52688.2022.01960
   Wu D, 2021, IEEE T EM TOP COMP I, V5, P70, DOI 10.1109/TETCI.2020.3034606
   Wu YH, 2022, AAAI CONF ARTIF INTE, P2750
   Xi JL, 2022, NEUROCOMPUTING, V483, P116, DOI 10.1016/j.neucom.2022.01.013
   Xuan SY, 2021, PROC CVPR IEEE, P11921, DOI 10.1109/CVPR46437.2021.01175
   Yan CX, 2018, MULTIMED TOOLS APPL, V77, P3553, DOI 10.1007/s11042-017-5202-z
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P1681, DOI 10.1109/TMM.2020.3001522
   Yang JR, 2020, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR42600.2020.00335
   Ye M, 2022, IEEE T IMAGE PROCESS, V31, P379, DOI 10.1109/TIP.2021.3131937
   Zhai Y., 2020, COMPUTER VISION ECCV, P594, DOI DOI 10.1007/978-3-030-58571-6_35
   Zhang GQ, 2022, J VIS COMMUN IMAGE R, V87, DOI 10.1016/j.jvcir.2022.103581
   Zhang PY, 2022, LECT NOTES COMPUT SC, V13674, P215, DOI 10.1007/978-3-031-19781-9_13
   Zhang W, 2020, IEEE T IMAGE PROCESS, V29, P3365, DOI 10.1109/TIP.2019.2959653
   Zhang X., 2022, P IEEE CVF C COMP VI, P7369
   Zhang X, 2021, PROC CVPR IEEE, P3435, DOI 10.1109/CVPR46437.2021.00344
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhou SP, 2022, IEEE T NEUR NET LEAR, V33, P4826, DOI 10.1109/TNNLS.2021.3061164
   Zhu HW, 2022, PROC CVPR IEEE, P4682, DOI 10.1109/CVPR52688.2022.00465
   Zhu H, 2022, MULTIMED TOOLS APPL, V81, P18671, DOI 10.1007/s11042-022-12581-0
   Zhuo JX, 2018, IEEE INT CON MULTI
NR 66
TC 0
Z9 0
U1 6
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD JUN
PY 2024
VL 13
IS 2
AR 17
DI 10.1007/s13735-024-00324-w
PG 16
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MX8W4
UT WOS:001197037100001
DA 2024-08-05
ER

PT J
AU Liao, KY
   Lin, J
   Zheng, YL
   Wang, KR
   Feng, W
AF Liao, Kaiyang
   Lin, Jie
   Zheng, Yuanlin
   Wang, Keer
   Feng, Wen
TI Incremental image retrieval method based on feature perception and deep
   hashing
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Image retrieval; Incremental image; Deep hashing; Atrous convolution
AB How to propose an image retrieval algorithm with adaptable model and wide range of applications for large-scale datasets has become a critical technical problem in current image retrieval. This paper proposed an Incremental Image Retrieval Method Based on Feature Perception and Deep Hashing. The algorithm contains two important parts: the hash function learning part and the incremental hash code mapping part. Firstly, a module is designed called Feature Perception Module to obtain multi-scale global context-aware information. It also keeps the scale and shape of the final extracted deep features invariant. Then, a new incremental hash loss function is designed to maintain the similarity between the query image and the dataset image; the advantage of this is that it can reduce the time cost of updating the model. The experimental results show that the algorithm model can perform well in incremental image retrieval. It is shown that the algorithm can solve the current problem of low retrieval efficiency and high cost due to retraining models caused by the dramatic increase in the number of images in the image retrieval field.
C1 [Liao, Kaiyang; Lin, Jie; Zheng, Yuanlin; Wang, Keer; Feng, Wen] Xian Univ Technol, Sch Printing Packaging & Digital Media, Xian, Shaanxi, Peoples R China.
   [Liao, Kaiyang; Lin, Jie; Zheng, Yuanlin; Wang, Keer; Feng, Wen] Printing & Packaging Engn Technol Res Ctr Shaanxi, Xian, Peoples R China.
C3 Xi'an University of Technology
RP Liao, KY (corresponding author), Xian Univ Technol, Sch Printing Packaging & Digital Media, Xian, Shaanxi, Peoples R China.; Liao, KY (corresponding author), Printing & Packaging Engn Technol Res Ctr Shaanxi, Xian, Peoples R China.
EM liaokaiyang@xaut.edu.cn
CR Amato F, 2013, J APPL BIOMED, V11, P47, DOI 10.2478/v10136-012-0031-x
   Andrea MK, 2017, P IEEE INT C IMAGE P, P2480
   Ashok Kumar P. M., 2021, Intelligent System Design. Proceedings of Intelligent System Design: INDIA 2019. Advances in Intelligent Systems and Computing (AISC 1171), P505, DOI 10.1007/978-981-15-5400-1_52
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Bai JL, 2019, IEEE T MULTIMEDIA, V21, P3178, DOI 10.1109/TMM.2019.2920601
   Cao Y, 2018, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2018.00134
   Chen DC, 2021, J SENSORS, V2021, DOI 10.1155/2021/6631029
   Dubey SR, 2022, IEEE T CIRC SYST VID, V32, P2687, DOI 10.1109/TCSVT.2021.3080920
   Dubey SR, 2021, IEEE T CIRCUITS SYS, P2687
   Garg M, 2021, Neural Comput Appl, ppp1311
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   He ZW, 2023, PATTERN RECOGN, V133, DOI 10.1016/j.patcog.2022.109038
   Hong SA, 2023, MULTIMED TOOLS APPL, V82, P30807, DOI 10.1007/s11042-023-14748-9
   Huang ZQ, 2023, IEEE T DEPEND SECURE, V20, P463, DOI 10.1109/TDSC.2021.3136163
   Islam SM, 2023, MULTIMED TOOLS APPL, V82, P16501, DOI 10.1007/s11042-022-14204-0
   Jiang QY, 2018, AAAI CONF ARTIF INTE, P3342
   Jing LL, 2021, IEEE T PATTERN ANAL, V43, P4037, DOI 10.1109/TPAMI.2020.2992393
   Kang Y, 2012, IEEE DATA MINING, P930, DOI 10.1109/ICDM.2012.24
   Krizhevsky Alex., 2011, 19 EUROPEAN S ARTIFI
   Kuzborskij I, 2013, PROC CVPR IEEE, P3358, DOI 10.1109/CVPR.2013.431
   Li T, 2022, IEEE SIGNAL PROC LET, V29, P827, DOI 10.1109/LSP.2022.3157517
   Li Wu-Jun, 2016, P INT JOINT C ART IN, P1711
   Liang XP, 2023, IEEE T MULTIMEDIA, V25, P1085, DOI 10.1109/TMM.2021.3139217
   Liang XP, 2023, IEEE T KNOWL DATA EN, V35, P3765, DOI 10.1109/TKDE.2021.3131188
   Lin GS, 2014, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2014.253
   Long J, 2022, Neurocomputing, P1
   Mensink T, 2013, IEEE T PATTERN ANAL, V35, P2624, DOI 10.1109/TPAMI.2013.83
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587
   Ruikar SD, 2016, INT C WIRELESS COMMU, ppp1517
   Shen FM, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1522, DOI 10.1145/3123266.3123345
   Shen YM, 2020, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR42600.2020.00289
   Shen YM, 2019, INT J COMPUT VISION, V127, P1614, DOI 10.1007/s11263-019-01166-4
   Su S., 2018, P NIPS, P798
   Ho T, 2012, IEEE INT SYMP SIGNAL, P79, DOI 10.1109/ISSPIT.2012.6621264
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wang R., 2020, P IEEE CVF WINT C AP, P2493
   Wang XG, 2021, PROC CVPR IEEE, P16352, DOI 10.1109/CVPR46437.2021.01609
   Wu D, 2018, P IEEE INT C MULTIME, P1
   Wu DY, 2019, PROC CVPR IEEE, P9061, DOI 10.1109/CVPR.2019.00928
   Wu P, 2012, P 21 ACM INT C MULTI, P153
   Yu Y, 2021, P INT C SYSTEMS INFO, P1
   Yuan L, 2020, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR42600.2020.00315
   Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315
   Zhang Z, 2021, ACM T KNOWL DISCOV D, V15, DOI 10.1145/3442204
   Zhu H, 2016, AAAI CONF ARTIF INTE, P2415
NR 46
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD MAR
PY 2024
VL 13
IS 1
AR 10
DI 10.1007/s13735-024-00319-7
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HK7E7
UT WOS:001159454800001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Kasantikul, R
   Kusakunniran, W
AF Kasantikul, Rangwan
   Kusakunniran, Worapan
TI Augmented inputs for surveillance re-identification
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Biometrics; Pattern recognition; Surveillance; Video-based person
   re-identification
ID PERSON REIDENTIFICATION; RECOGNITION
AB Person Re-Identification (Re-ID) is one of the important applications for Surveillance. However, the performance of Re-ID is dependent on the input quality, which cannot be guaranteed from the surveillance systems. We explored the technique from Gait Re-ID to address viewpoint changes. From our findings, adding horizontally mirrored image into an auxiliary pipeline can achieve a modest performance uplift in our test (0.8% net increase in mean average precision and 0.9% increase in Rank-1 accuracy) in MARS dataset. This extra pipeline can be substituted by Heterogeneous Input Triplet Loss (hiTri) for minimal performance loss. The overall performance of the proposed method outperforms state-of-the-art techniques on well-known datasets. Further investigations on other auxiliary input types are warranted.
C1 [Kasantikul, Rangwan; Kusakunniran, Worapan] Mahidol Univ, Fac Informat & Commun Technol, 999 Phuttamonthon 4 Rd Salaya, Nakhon Pathom 73170, Thailand.
C3 Mahidol University
RP Kusakunniran, W (corresponding author), Mahidol Univ, Fac Informat & Commun Technol, 999 Phuttamonthon 4 Rd Salaya, Nakhon Pathom 73170, Thailand.
EM rangwan.kas@gmail.com; worapan.kun@mahidol.edu
FU National Research Council of Thailand
FX No Statement Available
CR Chao HQ, 2022, IEEE T PATTERN ANAL, V44, P3467, DOI 10.1109/TPAMI.2021.3057879
   Ding WJ, 2020, IEEE IMAGE PROC, P2441, DOI 10.1109/ICIP40778.2020.9191166
   Dong S, 2021, COMPUT SCI REV, V40, DOI 10.1016/j.cosrev.2021.100379
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   FOGEL I, 1989, BIOL CYBERN, V61, P103, DOI 10.1007/BF00204594
   Forssen P.-E., 2007, IEEE International Conference on Computer Vision (ICCV), P1, DOI 10.1109/CVPR.2007.383120
   Fu H, 2022, IMAGE VISION COMPUT, V118, DOI 10.1016/j.imavis.2021.104356
   Fu Y, 2018, CoRR abs/1811.04129
   Gao J, 2018, CoRR abs/1805.02104
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   Han J, 2006, IEEE T PATTERN ANAL, V28, P316, DOI 10.1109/TPAMI.2006.38
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hou RB, 2019, PROC CVPR IEEE, P7176, DOI 10.1109/CVPR.2019.00735
   Ioffe S, 2015, Arxiv, DOI [arXiv:1502.03167, DOI 10.48550/ARXIV.1502.03167, 10.48550/arXiv.1502.03167]
   Klaser A., 2008, BMVC 2008 19 BRIT MA, p275:1
   Köstinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Kusakunniran W, 2014, IEEE T IMAGE PROCESS, V23, P696, DOI 10.1109/TIP.2013.2294552
   Kusakunniran W, 2013, IEEE T INF FOREN SEC, V8, P1642, DOI 10.1109/TIFS.2013.2252342
   Layne R, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.24
   Li J., 2018, arXiv
   Li M, 2018, CoRR abs/1809.02874
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lowe D. G., 1999, Computer vision, V2, P1150, DOI [10.1109/ICCV.1999.790410, DOI 10.1109/ICCV.1999.790410]
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   MichaelJones SSR, 2019, Body part alignment and temporal attention for video-based person re-identification
   Nambiar A, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3243043
   Patrikar DR, 2022, INT J MULTIMED INF R, V11, P85, DOI 10.1007/s13735-022-00227-8
   Sarkar S, 2005, IEEE T PATTERN ANAL, V27, P162, DOI 10.1109/TPAMI.2005.39
   Sathish PK, 2018, INT J MULTIMED INF R, V7, P221, DOI 10.1007/s13735-018-0153-3
   Schmid C, 2001, PROC CVPR IEEE, P39
   Wang TQ, 2014, LECT NOTES COMPUT SC, V8692, P688, DOI 10.1007/978-3-319-10593-2_45
   Wang XY, 2019, IEEE IMAGE PROC, P2249, DOI [10.1109/icip.2019.8803321, 10.1109/ICIP.2019.8803321]
   Xing EP, 2003, NIPS 15, V15, P505
   Xiu Y., 2018, BMVC
   Xu K, 2020, PROC CVPR IEEE, P1737, DOI 10.1109/CVPR42600.2020.00181
   Yao LX, 2021, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.382
   Yao LX, 2021, PATTERN RECOGN LETT, V150, P289, DOI 10.1016/j.patrec.2019.05.012
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2021, IEEE T INF FOREN SEC, V16, P728, DOI 10.1109/TIFS.2020.3001665
   Yim J, 2017, 2017 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING - TECHNIQUES AND APPLICATIONS (DICTA), P345
   Zhang G., 2021, P IEEE INT C MULT EX, P1
   Zhang RM, 2019, IEEE T IMAGE PROCESS, V28, P4870, DOI 10.1109/TIP.2019.2911488
   Zheng L, 2016, Arxiv, DOI arXiv:1610.02984
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 46
TC 0
Z9 0
U1 4
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD MAR
PY 2024
VL 13
IS 1
AR 2
DI 10.1007/s13735-023-00309-1
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EE2H8
UT WOS:001137172500001
DA 2024-08-05
ER

PT J
AU Zhang, HY
   Yanagi, R
   Togo, R
   Ogawa, T
   Haseyama, M
AF Zhang, Huaying
   Yanagi, Rintaro
   Togo, Ren
   Ogawa, Takahiro
   Haseyama, Miki
TI Parameter-efficient tuning of cross-modal retrieval for a specific
   database via trainable textual and visual prompts
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Cross-modal image retrieval; Prompt learning; Multimedia Information
   retrieval; Pre-trained model
AB A novel cross-modal image retrieval method realized by parameter efficiently tuning a pre-trained cross-modal model is proposed in this study. Conventional cross-modal retrieval methods realize text-to-image retrieval by training cross-modal models to bring paired texts and images close in a common embedding space. However, these methods are trained on huge amounts of intentionally annotated image-text pairs, which may be unavailable in specific databases. To reduce the dependency on the amount and quality of training data, fine-tuning a pre-trained model is one approach to improve retrieval accuracy on specific personal image databases. However, this approach is parameter inefficient for separately training and retaining models for different databases. Thus, we propose a cross-modal retrieval method that uses prompt learning to solve these problems. The proposed method constructs two types of prompts, a textual prompt and a visual prompt, which are both multi-dimensional vectors. The textual and visual prompts are then concatenated with input texts and images, respectively. By optimizing the prompts to bring paired texts and images close in the common embedding space, the proposed method can improve retrieval accuracy with only a few parameters being updated. The experimental results demonstrate that the proposed method is effective for improving retrieval accuracy and outperforms conventional methods in terms of parameter efficiency.
C1 [Zhang, Huaying; Yanagi, Rintaro] Hokkaido Univ, Grad Sch Informat Sci & Technol, Sapporo, Hokkaido 0600814, Japan.
   [Togo, Ren; Ogawa, Takahiro; Haseyama, Miki] Hokkaido Univ, Fac Informat Sci & Technol, Sapporo, Hokkaido 0600814, Japan.
C3 Hokkaido University; Hokkaido University
RP Haseyama, M (corresponding author), Hokkaido Univ, Fac Informat Sci & Technol, Sapporo, Hokkaido 0600814, Japan.
EM huaying@lmd.ist.hokudai.ac.jp; yanagi@lmd.ist.hokudai.ac.jp;
   togo@lmd.ist.hokudai.ac.jp; ogawa@lmd.ist.hokudai.ac.jp;
   mhaseyama@lmd.ist.hokudai.ac.jp
FU JSPS KAKENHI;  [JP21H03456];  [JP23K11141]
FX This work was supported by the JSPS KAKENHI Grant Numbers JP21H03456 and
   JP23K11141.
CR Agnolucci L, 2023, P IEEECVF INT C COMP, P2811
   Alayrac J.-B., 2022, NeurIPS, V35, P23716, DOI DOI 10.48550/ARXIV.2204.14198
   Bahng H, 2022, Arxiv, DOI arXiv:2203.17274
   Brown T. B., 2020, P 34 INT C NEURAL IN, P1
   Chen J, 2022, PROC CVPR IEEE, P18009, DOI 10.1109/CVPR52688.2022.01750
   Cheng YH, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3499027
   Chun S, 2021, PROC CVPR IEEE, P8411, DOI 10.1109/CVPR46437.2021.00831
   Diao HW, 2021, AAAI CONF ARTIF INTE, V35, P1218
   Ding N, 2023, NAT MACH INTELL, V5, P220, DOI 10.1038/s42256-023-00626-4
   Dong XY, 2023, Arxiv, DOI arXiv:2208.12262
   Dosovitskiy A., 2021, ICLR
   Faghri F, 2018, Arxiv, DOI arXiv:1707.05612
   Gu JX, 2018, PATTERN RECOGN, V77, P354, DOI 10.1016/j.patcog.2017.10.013
   Hovy E, 2020, IEEECVF C COMPUTER V
   Jia C, 2021, PR MACH LEARN RES, V139
   Jia M, 2022, EUROPEAN C COMPUTER
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kiros R, 2014, Arxiv, DOI arXiv:1411.2539
   Kottur S, 2022, arXiv
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Lester B, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P3045
   Li A, 2017, IEEE I CONF COMP VIS, P4193, DOI 10.1109/ICCV.2017.449
   Li XLS, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P4582
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu LY, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P5747
   Liu P., 2021, arXiv, DOI 10.48550/arXiv.2107.13586
   Logan RL, 2022, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), P2824
   Loshchilov I., 2018, INT C LEARN REPR
   Lu D, 2016, IEEE T MULTIMEDIA, V18, P1628, DOI 10.1109/TMM.2016.2568099
   Lu Yuning, 2022, P IEEE CVF C COMP VI, P5206
   Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045
   Mokady Ron, 2021, arXiv, DOI DOI 10.48550/ARXIV.2111.09734
   Peter Young M. H., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Petroni F, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2463
   Pham H, 2022, Arxiv, DOI arXiv:2111.10050
   Radford, 2018, OPENAI BLOG
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Schick T, 2021, 16TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (EACL 2021), P255
   Schuhmann C, 2021, FZJ-2022-00923
   Song H, 2023, IEEE T NEUR NET LEAR, V34, P8135, DOI 10.1109/TNNLS.2022.3152527
   Song Y, 2019, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2019.00208
   Strauss J, 2011, USENIX ANN TECHNICAL
   Su W., 2020, INT C LEARNING REPRE
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang K., 2016, PREPRINT
   Wang YQ, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3386252
   Wortsman M, 2022, PROC CVPR IEEE, P7949, DOI 10.1109/CVPR52688.2022.00780
   Yanagi R, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3816, DOI 10.1145/3474085.3475681
   Yanagi R, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3485042
   Zhang HY, 2023, IEEE ACCESS, V11, P10675, DOI 10.1109/ACCESS.2023.3239858
   Zhang Si, 2019, Comput Soc Netw, V6, P11, DOI 10.1186/s40649-019-0069-y
   Zhou KY, 2022, PROC CVPR IEEE, P16795, DOI 10.1109/CVPR52688.2022.01631
   Zhou KY, 2022, INT J COMPUT VISION, V130, P2337, DOI 10.1007/s11263-022-01653-1
   Zhou WA, 2017, Arxiv, DOI arXiv:1706.06064
NR 56
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD MAR
PY 2024
VL 13
IS 1
AR 14
DI 10.1007/s13735-024-00322-y
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JD0H1
UT WOS:001171096100001
DA 2024-08-05
ER

PT J
AU Shan, SH
   Sun, PX
   Xiao, GQ
   Wu, S
AF Shan, Shihao
   Sun, Peixin
   Xiao, Guoqiang
   Wu, Song
TI Multi-knowledge-driven enhanced module for visible-infrared cross-modal
   person Re-identification
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Visible-infrared person Re-identification; Knowledge-enhancement;
   Cross-modality retrieval; Bias adjustment strategy
AB Visible-Infrared Person Re-identification (VI-ReID) is challenging in social security surveillance because the semantic gap between cross-modal data significantly reduces VI-ReID performance. To overcome this challenge, this paper proposes a novel Multi Knowledge-driven Enhancement Module (MKEM) for high-performance VI-ReID. It mainly focuses on explicitly learning appropriate transition modalities and effectively synthesizing them to reduce the burden of models learning vastly different cross-modal knowledge. The MKEM consists of a Visible Knowledge-driven Enhancement Module (VKEM) and an Infrared Knowledge-driven Enhancement Module (IKEM), which generate model knowledge-accumulating transition modalities for the visible and infrared modalities, respectively. To effectively leverage the transition modalities, the model needs to learn the original data distribution while accumulating knowledge of the transition modes; thus, a Diversity Loss is designed to guide the representation of the generated transition modalities to be diverse, which can facilitate the model's knowledge accumulation. To prevent redundant knowledge accumulation, a Consistency Loss is proposed to maintain the semantic similarity between the original and modeled transitional modalities. Furthermore, we implemented a Bias Adjustment Strategy (BAS) to effectively adjust the gap between the head and tail categories. We evaluated our proposed MKEM on two VI-ReID benchmark datasets, SYSU-MM01 and RegDB, and the experimental results demonstrate that our method outperforms existing methods significantly. The source code of our proposed MKEM is available at https://github.com/SWU-CS-MediaLab/MKEM.
C1 [Shan, Shihao; Xiao, Guoqiang; Wu, Song] Southwest Univ, Coll Comp & Informat Sci, Chongqing 400715, Peoples R China.
   [Sun, Peixin] Univ Auckland, Facil Sci, Auckland 1010, New Zealand.
C3 Southwest University - China; University of Auckland
RP Wu, S (corresponding author), Southwest Univ, Coll Comp & Informat Sci, Chongqing 400715, Peoples R China.
EM songwuswu@swu.edu.cn
FU Fundamental Research Funds for the Central Universities [SWU-KT22032];
   Fundamental Research Funds for the Central Universities, China
FX This work was supported by the Fundamental Research Funds for the
   Central Universities, China (SWU-KT22032).
CR Chen CQ, 2022, IEEE T IMAGE PROCESS, V31, P2352, DOI 10.1109/TIP.2022.3141868
   Chen YHS, 2021, PROC CVPR IEEE, P587, DOI 10.1109/CVPR46437.2021.00065
   Cho Y, 2022, PROC CVPR IEEE, P7298, DOI 10.1109/CVPR52688.2022.00716
   Fattahi M, 2023, KNOWL-BASED SYST, V259, DOI 10.1016/j.knosys.2022.110088
   Hao X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16383, DOI 10.1109/ICCV48922.2021.01609
   He K, 2023, INT J MULTIMED INF R, V12, DOI 10.1007/s13735-023-00279-4
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu WP, 2022, IEEE T CIRC SYST VID, V32, P5095, DOI 10.1109/TCSVT.2022.3147813
   Huang ZP, 2022, AAAI CONF ARTIF INTE, P1034
   Li DG, 2020, AAAI CONF ARTIF INTE, V34, P4610
   Liu HJ, 2023, IEEE T NEUR NET LEAR, V34, P1958, DOI 10.1109/TNNLS.2021.3105702
   Liu JN, 2022, IEEE T CIRC SYST VID, V32, P7226, DOI 10.1109/TCSVT.2022.3168999
   Liu S, 2022, INT CONF ACOUST SPEE, P2205, DOI 10.1109/ICASSP43922.2022.9746840
   Lu H, 2023, AAAI CONF ARTIF INTE, P1835
   Mang Ye, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P229, DOI 10.1007/978-3-030-58520-4_14
   Nguyen DT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030605
   Park H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12026, DOI 10.1109/ICCV48922.2021.01183
   Pu Nan, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P541, DOI 10.1145/3503161.3548234
   Pu N, 2023, PROC CVPR IEEE, P7579, DOI 10.1109/CVPR52729.2023.00732
   Pu N, 2023, IEEE T PATTERN ANAL, V45, P13567, DOI 10.1109/TPAMI.2023.3297058
   Pu N, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2149, DOI 10.1145/3394171.3413673
   Pu Nan, 2021, P IEEECVF C COMPUTER, P7901
   Sabahi F, 2023, INT J MULTIMED INF R, V12, DOI 10.1007/s13735-023-00274-9
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Seokeon Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10254, DOI 10.1109/CVPR42600.2020.01027
   Shan SH, 2022, LECT NOTES COMPUT SC, V13529, P441, DOI 10.1007/978-3-031-15919-0_37
   Shen WZ, 2023, INT J MULTIMED INF R, V12, DOI 10.1007/s13735-023-00306-4
   Si TZ, 2023, NEUROCOMPUTING, V523, P170, DOI 10.1016/j.neucom.2022.12.042
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tang K., 2020, P NIPS, P1513
   Tian XD, 2021, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR46437.2021.00157
   Tong Wu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P162, DOI 10.1007/978-3-030-58548-8_10
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang GA, 2020, AAAI CONF ARTIF INTE, V34, P12144
   Wang YD, 2022, Arxiv, DOI arXiv:2112.07225
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Xu XH, 2022, KNOWL-BASED SYST, V257, DOI 10.1016/j.knosys.2022.109883
   Yan Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13376, DOI 10.1109/CVPR42600.2020.01339
   Yang MX, 2022, PROC CVPR IEEE, P14288, DOI 10.1109/CVPR52688.2022.01391
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Zhang MX, 2022, KNOWL-BASED SYST, V257, DOI 10.1016/j.knosys.2022.109905
   Zhang Q, 2022, PROC CVPR IEEE, P7339, DOI 10.1109/CVPR52688.2022.00720
   Zhao ZW, 2021, AAAI CONF ARTIF INTE, V35, P3520
   Zhu Y, 2019, NEUROCOMPUTING, V369, P145, DOI 10.1016/j.neucom.2019.08.078
   Zou XT, 2022, KNOWL-BASED SYST, V239, DOI 10.1016/j.knosys.2021.107927
NR 45
TC 0
Z9 0
U1 11
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD JUN
PY 2024
VL 13
IS 2
AR 19
DI 10.1007/s13735-024-00327-7
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NY3H1
UT WOS:001203968900001
DA 2024-08-05
ER

PT J
AU Sun, LN
   Dong, YM
AF Sun, Lina
   Dong, Yumin
TI Unsupervised graph reasoning distillation hashing for multimodal hamming
   space search with vision-language model
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Knowledge distillation; Deep multimodal hashing; Hamming space search;
   Vision-language model
AB Multimodal hash technology maps high-dimensional multimodal data into hash codes, which greatly reduces the cost of data storage and improves query speed through the Hamming similarity calculation. However, existing unsupervised methods still have two key obstacles: (1) With the evolution of large multimodal models, how to efficiently distill the multimodal matching relationship of large models to train a powerful student model? (2) Existing methods do not consider other adjacencies between multimodal instances, resulting in limited similarity representation. To address these obstacles, called Unsupervised Graph Reasoning Distillation Hashing (UGRDH) is proposed. The UGRDH approach uses the CLIP as the teacher model, thus extracting fine-grained multimodal features and relations for teacher-student distillation. Specifically, the multimodal features of the teacher are used to construct a similarity-complementary relation graph matrix, and the proposed graph convolution auxiliary network performs feature aggregation guided by the relation graph matrix to generate a more discriminative hash code. In addition, a cross-attention module was designed to reason potential instance relations to enable effective teacher-student distilled learning. Finally, UGRDH greatly improves search precision while maintaining lightness. Experimental results show that our method achieves about 1.5%, 3%, and 2.8% performance improvements on MS COCO, NUS-WIDE, and MIRFlickr, respectively.
C1 [Sun, Lina; Dong, Yumin] Chongqing Normal Univ, Sch Comp & Informat Sci, Chongqing 401331, Peoples R China.
C3 Chongqing Normal University
RP Dong, YM (corresponding author), Chongqing Normal Univ, Sch Comp & Informat Sci, Chongqing 401331, Peoples R China.
EM dym@cqnu.edu.cn
FU Postgraduate Scientific Research Innovation Project of Chongqing Normal
   University; Open Fund of Advanced Cryptography and System Security Key
   Laboratory of Sichuan Province [202208]; National Natural Science
   Foundation of China [61772295]; Science and Technology Research Program
   of Chongqing Municipal Education Commission [no.KJZD-M202000501]; 
   [YKC23025]
FX This work was supported by the Open Fund of Advanced Cryptography and
   System Security Key Laboratory of Sichuan Province (Grant No.
   SKLACSS-202208), National Natural Science Foundation of China
   (No.61772295), Postgraduate Scientific Research Innovation Project of
   Chongqing Normal University(YKC23025) and the Science and Technology
   Research Program of Chongqing Municipal Education Commission (Grant
   no.KJZD-M202000501).
CR Bao HB, 2022, Arxiv, DOI arXiv:2111.02358
   Chen FL, 2023, MACH INTELL RES, V20, P38, DOI 10.1007/s11633-022-1369-5
   Chua TS, 2009, P ACM INT C IM VID R, P1
   Dejie Yang, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P44, DOI 10.1145/3372278.3390673
   Ding GG, 2016, IEEE T IMAGE PROCESS, V25, P5427, DOI 10.1109/TIP.2016.2607421
   Gou JP, 2021, INT J COMPUT VISION, V129, P1789, DOI 10.1007/s11263-021-01453-z
   Gu Xiuye, 2021, arXiv
   Guo J, 2023, IEEE Geosci Remote Sens Lett
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou CW, 2022, KNOWL-BASED SYST, V256, DOI 10.1016/j.knosys.2022.109891
   Hu HT, 2020, PROC CVPR IEEE, P3120, DOI 10.1109/CVPR42600.2020.00319
   Huiskes M.J., 2008, P ACM INT C MULT INF, P39, DOI 10.1145/1460096
   Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244
   Kim W, 2021, PR MACH LEARN RES, V139
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Li JH, 2021, ADV NEUR IN, V34
   Li L, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3712, DOI 10.1145/3503161.3548431
   Li MY, 2021, PROCEEDINGS OF THE 2021 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR '21), P183, DOI 10.1145/3460426.3463626
   Li X., 2020, Oscar: Object-Semantics Aligned Pre-training for VisionLanguage Tasks, DOI 10.1007/978-3-030-58577-8_8
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu LC, 2016, INT C PATT RECOG, P1713, DOI 10.1109/ICPR.2016.7899883
   Liu LC, 2018, IEEE T IMAGE PROCESS, V27, P4345, DOI 10.1109/TIP.2018.2831454
   Liu S, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1379, DOI 10.1145/3397271.3401086
   Lu X, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1414, DOI 10.1145/3474085.3475598
   Luo X, 2023, ACM T KNOWL DISCOV D, V17, DOI 10.1145/3532624
   Ma YW, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, DOI 10.1145/3503161.3547910
   Mikriukov G, 2022, INT CONF ACOUST SPEE, P4463, DOI 10.1109/ICASSP43922.2022.9746251
   Radford A, 2021, PR MACH LEARN RES, V139
   Shen X, 2021, 2021 IEEE INT C MULT, P1
   Shi Y., 2022, IEEE Trans Circuits Syst Video Technol
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh A, 2022, KNOWL INF SYST, V64, P2565, DOI 10.1007/s10115-022-01734-0
   Su SP, 2019, IEEE I CONF COMP VIS, P3027, DOI 10.1109/ICCV.2019.00312
   Tan W, 2022, IEEE Trans Multimedia
   Tan WT, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P982, DOI 10.1145/3477495.3531947
   Tan Wentao, 2023, IEEE T MULTIMEDIA
   Tu R-C, 2023, IEEE Trans Circuits Syst Video Technol
   Tung F, 2019, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2019.00145
   Wang B, 2023, Signal Processing: Image Communication
   Wang D, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107479
   Wang WW, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P853
   Wang ZR, 2022, Arxiv, DOI arXiv:2108.10904
   Wu F, 2023, PATTERN RECOGN, V136, DOI 10.1016/j.patcog.2022.109211
   Wu GS, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2854
   Yu J, 2021, AAAI CONF ARTIF INTE, V35, P4626
   Zhang C, 2023, IEEE Trans Multimedia
   Zhang J, 2018, AAAI CONF ARTIF INTE, P539
   Zhang PF, 2022, IEEE T MULTIMEDIA, V24, P466, DOI 10.1109/TMM.2021.3053766
   Zhang PF, 2021, WORLD WIDE WEB, V24, P563, DOI 10.1007/s11280-020-00859-y
   Zhang Xinxin, 2023, IEEE T CIRCUITS SYST
   Zhou X, 2020, IEEE T CYBERNETICS, V50, P1460, DOI 10.1109/TCYB.2018.2883970
   Zhu L, 2023, IEEE Transactions on Knowledge and Data Engineering
NR 52
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD JUN
PY 2024
VL 13
IS 2
AR 16
DI 10.1007/s13735-024-00326-8
PG 18
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MQ5Y5
UT WOS:001195118900001
DA 2024-08-05
ER

PT J
AU Kumain, SC
   Singh, M
   Awasthi, LK
AF Kumain, Sandeep Chand
   Singh, Maheep
   Awasthi, Lalit Kumar
TI A voting-based novel spatio-temporal fusion framework for video saliency
   using transfer learning mechanism
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Transfer learning; Neural network; Human visual system; Fusion; Ensemble
   learning; Mean absolute error; Convolution; Transpose convolution
AB In computer vision and image processing, salient object detection in an image and video is one of the complex research problems. The most attention-grabbing object of the image or video is generally considered the most prominent object. Due to the motion effect and change in the object's shape and structure, saliency detection in the video is much more complex than detecting a salient object in an image. In the last two decades, the researcher(s) community has proposed several methods to mimic the human visual capability to find the attention-grabbing object. The techniques presented in the literature are based on statistics or deep learning approaches. Deep learning-based methods have recently gained more attention due to robust detection results. However, it is challenging for one model to function effectively in every situation, so the overall detection accuracy gets degraded. Consequently, an ensemble technique rather than a single model may be preferable to segment salient objects accurately. In this research article, the author(s) have presented a voting-based spatio-temporal fusion framework for saliency detection in video. In the proposed framework, the author(s) used three static networks to estimate spatial saliency using the transfer learning concept and one dynamic network based on 3D convolution to learn the temporal effect. To construct the final saliency output, each spatial saliency map is fused with a temporal saliency map, and finally, a refined saliency map having spatial and temporal information is generated after pixel-wise voting. The proposed framework's findings on four publicly available and widely used video saliency datasets are competitive in terms of S-Measure, F-Measure and mean absolute error as compared to the state-the-art methods.
C1 [Kumain, Sandeep Chand; Singh, Maheep; Awasthi, Lalit Kumar] Natl Inst Technol, Dept Comp Sci & Engn, Srinagar, Uttarakhand, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Uttarakhand
RP Singh, M (corresponding author), Natl Inst Technol, Dept Comp Sci & Engn, Srinagar, Uttarakhand, India.
EM sandeep.chandphd2021@nituk.ac.in; maheepsingh@nituk.ac.in;
   lalit@nituk.ac.in
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Agnolucci L., 2023, IEEE Trans. Multimed, P1
   Bi H, 2021, IEEE Trans Cognit Dev Syst
   Bi HB, 2021, APPL INTELL, V51, P3450, DOI 10.1007/s10489-020-01961-4
   Bi HB, 2019, IEEE IMAGE PROC, P4654, DOI [10.1109/ICIP.2019.8803611, 10.1109/icip.2019.8803611]
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Chen CLZ, 2021, IEEE T IMAGE PROCESS, V30, P3995, DOI 10.1109/TIP.2021.3068644
   Chen CLZ, 2020, IEEE T IMAGE PROCESS, V29, P1090, DOI 10.1109/TIP.2019.2934350
   Chen CZ, 2017, IEEE T IMAGE PROCESS, V26, P3156, DOI 10.1109/TIP.2017.2670143
   Chen CLZ, 2015, PATTERN RECOGN, V48, P2885, DOI 10.1016/j.patcog.2015.01.025
   Chen Y-W, 2022, P IEEECVF WINTER C A
   Chen YH, 2018, IEEE T IMAGE PROCESS, V27, P3345, DOI 10.1109/TIP.2018.2813165
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   classeval.wordpress, Basic Evaluation Metrics
   cse.cuhk, ECSSD Dataset
   DAVIS, 2016, ABOUT US
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Fang YM, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.106987
   Fang YM, 2014, IEEE T CIRC SYST VID, V24, P27, DOI 10.1109/TCSVT.2013.2273613
   Fu KR, 2014, INT C PATT RECOG, P2371, DOI 10.1109/ICPR.2014.411
   github, UVSD DATASET
   github, SAL OBJ DAT
   github, DAVSOD DATASET
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang HG, 2023, IEEE Trans Vis Comput Graph
   Ji YZ, 2021, IEEE T NEUR NET LEAR, V32, P2676, DOI 10.1109/TNNLS.2020.3007534
   Jia YQ, 2013, IEEE I CONF COMP VIS, P1761, DOI 10.1109/ICCV.2013.221
   Kalboussi R, 2017, I C COMP SYST APPLIC, P738, DOI 10.1109/AICCSA.2017.93
   Kim D, 2019, PROC CVPR IEEE, P5785, DOI 10.1109/CVPR.2019.00594
   Kumain SC, 2024, SIGNAL IMAGE VIDEO P, V18, P2037, DOI 10.1007/s11760-023-02833-3
   Le T-N, 2017, BMVC, V1
   Li GB, 2018, PROC CVPR IEEE, P3243, DOI 10.1109/CVPR.2018.00342
   Li HF, 2019, IEEE I CONF COMP VIS, P7273, DOI 10.1109/ICCV.2019.00737
   Liu B, 2022, APPL INTELL, V52, P5922, DOI 10.1007/s10489-021-02649-z
   Liu Z, 2017, IEEE T CIRC SYST VID, V27, P2527, DOI 10.1109/TCSVT.2016.2595324
   Rahtu E, 2010, LECT NOTES COMPUT SC, V6315, P366, DOI 10.1007/978-3-642-15555-0_27
   Ramadan H, 2018, COMPUT ELECTR ENG, V70, P567, DOI 10.1016/j.compeleceng.2017.08.029
   saliencydetection, DUT OMRON DATASET
   Shokri M, 2020, J VIS COMMUN IMAGE R, V68, DOI 10.1016/j.jvcir.2020.102769
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song HM, 2018, LECT NOTES COMPUT SC, V11215, P744, DOI 10.1007/978-3-030-01252-6_44
   Sun MJ, 2019, IEEE T CYBERNETICS, V49, P2900, DOI 10.1109/TCYB.2018.2832053
   Tang Y, 2019, IEEE T CIRC SYST VID, V29, P1973, DOI 10.1109/TCSVT.2018.2859773
   ViSal, ABOUT US
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   web.engr, ABOUT US
   Wolf L, 2007, IEEE I CONF COMP VIS, P1418
   Xi T, 2017, IEEE T IMAGE PROCESS, V26, P3425, DOI 10.1109/TIP.2016.2631900
   Yan CG, 2021, IEEE T PATTERN ANAL, V43, P1445, DOI 10.1109/TPAMI.2020.2975798
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang JM, 2017, IEEE T PATTERN ANAL, V39, P576, DOI 10.1109/TPAMI.2016.2547384
   Zhao WB, 2021, PROC CVPR IEEE, P16821, DOI 10.1109/CVPR46437.2021.01655
   Zhen M, 2020, COMPUTER VISION ECCV
NR 54
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD MAR
PY 2024
VL 13
IS 1
AR 13
DI 10.1007/s13735-024-00320-0
PG 17
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JD0U0
UT WOS:001171109400001
DA 2024-08-05
ER

PT J
AU Tan, YQ
   Liu, HZ
AF Tan, Yiqiao
   Liu, Haizhong
TI How does a kernel based on gradients of infinite-width neural networks
   come to be widely used: a review of the neural tangent kernel
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Neural tangent kernel; Neural network; Over-parameterization; Gradient
   descent
AB The neural tangent kernel (NTK) was created in the context of using the limit idea to study the theory of neural network. NTKs are defined from neural network models in the infinite-width limit trained by gradient descent. Such over-parameterized models achieved good test accuracy in experiments, and the success of the NTK emphasizes not only the importance of describing neural network models in the width limit of h ->infinity\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$h \to \infty$$\end{document}, but also the further development of deep learning theory for gradient flow in the step limit of eta -> 0\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\eta \to 0$$\end{document}. And NTK can be widely used in various machine learning models. This review provides a comprehensive overview of the entire development of NTKs. Firstly, the bias-variance tradeoff in statistics, the popular over-parameterization and gradient descent in deep learning, and the widely used kernel method were introduced. Secondly, the development of research on the infinite-width limit in networks and the introduction of the concept of the NTK were introduced, and the development and latest progress of NTK theory were discussed. Finally, the researches on the migrations of NTKs to neural networks of other structures and the applications of NTKs to other fields of machine learning were presented.
C1 [Tan, Yiqiao; Liu, Haizhong] Lanzhou Jiaotong Univ, Sch Math & Phys, Lanzhou 730070, Peoples R China.
C3 Lanzhou Jiaotong University
RP Liu, HZ (corresponding author), Lanzhou Jiaotong Univ, Sch Math & Phys, Lanzhou 730070, Peoples R China.
EM tanyiqiao1999@163.com; liuhzh@lzjtu.edu.cn
CR Advani MS, 2020, NEURAL NETWORKS, V132, P428, DOI 10.1016/j.neunet.2020.08.022
   Allen-Zhu Z, 2019, PR MACH LEARN RES, V97
   Allen-Zhu Z, 2021, Arxiv, DOI arXiv:2012.09816
   Arora S, 2019, Neural Information Processing Systems
   Arora S, 2019, PR MACH LEARN RES, V97
   Arora Sanjeev, 2019, INT C LEARNING REPRE
   Bai Y, 2019, INT C LEARNING REPRE
   Bansal Y, 2018, Arxiv, DOI arXiv:1806.00730
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bartlett PL, 2020, P NATL ACAD SCI USA, V117, P30063, DOI 10.1073/pnas.1907378117
   Belfer Y, 2021, Arxiv, DOI arXiv:2104.03093
   Belkin M, 2019, P NATL ACAD SCI USA, V116, P15849, DOI 10.1073/pnas.1903070116
   Belkin Mikhail, 2018, P MACHINE LEARNING R, V80
   Bietti Alberto, 2019, Advances in Neural Information Processing Systems, V32, P12873
   Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704
   Cao Y, 2019, Adv. Neural Inf. Process. Syst., V32
   Caron F, 2023, Arxiv, DOI arXiv:2302.01002
   Chen L, 2023, INT C LEARNING REPRE
   Chen Zixiang, 2020, Advances in Neural Information Processing Systems, V33, P13363
   Chizat L, 2019, ADV NEUR IN, V32
   Cho Y., 2009, Adv Neural Inf Process Syst, V22
   Daniely A, 2016, ADV NEUR IN, V29
   Daniely A, 2017, ADV NEUR IN, V30
   Dauphin YN, 2014, ADV NEUR IN, V27
   Du S. S., 2019, INT C LEARNING REPRE
   Du SS, 2019, ADV NEUR IN, V32
   Fan Zhou, 2020, Advances in Neural Information Processing Systems, V33, P7710
   Fiat J, 2019, Arxiv, DOI arXiv:1906.05032
   Fortmann-Roe S, 2012, Understanding the Bias-Variance Trade-off
   Geifman Amnon, 2020, Advances in Neural Information Processing Systems, V33, P1451
   Geiger M, 2020, J STAT MECH-THEORY E, V2020, DOI 10.1088/1742-5468/abc4de
   Geirhos R, 2020, NAT MACH INTELL, V2, P665, DOI 10.1038/s42256-020-00257-z
   GEMAN S, 1992, NEURAL COMPUT, V4, P1, DOI 10.1162/neco.1992.4.1.1
   Han IS, 2021, Arxiv, DOI arXiv:2104.01351
   Hastie T, 2022, ANN STAT, V50, P949, DOI [10.1214/21-AOS2133, 10.1214/21-aos2133]
   He B., 2020, ADV NEURAL INFORM PR, V33, P1010
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Huang BH, 2021, PR MACH LEARN RES, V139
   Jacot A, 2018, ADV NEUR IN, V31
   Jacot A, 2020, PR MACH LEARN RES, V119
   Ju P., 2020, Adv Neural Inf Process Syst, V33, P7956
   Ju P., 2021, PMLR, P5137
   Kanoh R, 2022, 11 INT C LEARNING RE
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee Jaehoon, 2018, INT C LEARNING REPRE
   Lee Jaehoon, 2019, NEURAL INFORM PROCES
   Li YZ, 2018, ADV NEUR IN, V31
   Li ZY, 2019, Arxiv, DOI arXiv:1911.00809
   Matthews Alexander G. de G., 2018, INT C LEARN REPR
   McClenny LD, 2023, J COMPUT PHYS, V474, DOI 10.1016/j.jcp.2022.111722
   Mei S, 2018, P NATL ACAD SCI USA, V115, pE7665, DOI 10.1073/pnas.1806579115
   Mei Song, 2019, C LEARN THEOR, P2388
   Muthukumar Vidya, 2020, IEEE Journal on Selected Areas in Information Theory, V1, P67, DOI 10.1109/JSAIT.2020.2984716
   Neal B, 2019, arXiv
   Neal R. M., 2012, Bayesian Learning for Neural Networks, V118
   Neyshabur B, 2019, INT C LEARNING REPRE
   Neyshabur B., 2017, arXiv
   Nguyen TV, 2021, IEEE T INFORM THEORY, V67, P4669, DOI 10.1109/TIT.2021.3065212
   Novak R, 2018, INT C LEARNING REPRE
   Novak R., 2019, INT C LEARNING REPRE
   Oymak Samet, 2020, IEEE Journal on Selected Areas in Information Theory, V1, P84, DOI 10.1109/JSAIT.2020.2991332
   Park Daniel, 2019, INT C MACH LEARN, P5042
   Peng YF, 2023, J COMPUT PHYS, V472, DOI 10.1016/j.jcp.2022.111690
   Pinkus A., 1999, Acta Numerica, V8, P143, DOI 10.1017/S0962492900002919
   Rahimi A., 2008, Advances in Neural Information Processing Systems, V20, P1177
   Rudi A, 2017, Adv Neural Inf Process Syst, V30
   Scholkopf B., 1996, Artificial Neural Networks - ICANN 96. 1996 International Conference Proceedings, P47
   Shoham Neta, 2020, arXiv
   SIETSMA J, 1991, NEURAL NETWORKS, V4, P67, DOI 10.1016/0893-6080(91)90033-2
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sohl-Dickstein J, 2020, Arxiv, DOI arXiv:2001.07301
   Spigler S, 2019, J PHYS A-MATH THEOR, V52, DOI 10.1088/1751-8121/ab4c8b
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang Yehui, 2022, ADV NEURAL INF PROCE, V35, P6104
   Vapnik VN, 1999, IEEE T NEURAL NETWOR, V10, P988, DOI 10.1109/72.788640
   [王梅 Wang Mei], 2022, [计算机应用, Journal of Computer Applications], V42, P3330
   Wang mei, 2021, Journal of Computer Applications, V41, P3462, DOI 10.11772/j.issn.1001-9081.2021060998
   Wang YT, 2023, Arxiv, DOI arXiv:2304.02840
   Watanabe K, 2023, NEURAL NETWORKS, V160, P148, DOI 10.1016/j.neunet.2022.12.020
   Wei H, 2019, Ultra-wide deep nets and neural tangent kernel (ntk)
   Williams C., 1996, Adv Neural Inf Process Syst, V9
   Yang Greg, 2021, INT C MACH LEARN, P11762
   Yang YL, 2024, Arxiv, DOI arXiv:2303.01687
   Yue K, 2022, PR MACH LEARN RES
   Zagoruyko S., 2016, Wide residual networks, DOI DOI 10.5244/C.30.87
   Zancato Luca, 2020, Advances in Neural Information Processing Systems, V33, P6136
   Zhai YJ, 2022, J INTELL FUZZY SYST, V43, P2731, DOI 10.3233/JIFS-213088
   Zhang C., 2017, INT C LEARN REPR
   Zhang CY, 2022, Arxiv, DOI arXiv:1902.01996
   Zou D, 2019, Adv. Neural Inf. Process. Syst., V32
   Zou DF, 2018, Arxiv, DOI arXiv:1811.08888
NR 93
TC 1
Z9 1
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD MAR
PY 2024
VL 13
IS 1
AR 8
DI 10.1007/s13735-023-00318-0
PG 19
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GP0Z4
UT WOS:001153767600001
DA 2024-08-05
ER

PT J
AU Li, YS
   Guo, YM
   Wu, YL
   Xie, YX
   Lao, MR
   Yu, TY
   Ruan, YR
AF Li, Yishan
   Guo, Yanming
   Wu, Yulun
   Xie, Yuxiang
   Lao, Mingrui
   Yu, Tianyuan
   Ruan, Yirun
TI RDAT: an efficient regularized decoupled adversarial training mechanism
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Adversarial defense; Adversarial training; Decoupled network; Robust
   overfitting
ID ROBUSTNESS
AB Adversarial examples have exposed the inherent vulnerabilities of deep neural networks. Although adversarial training has emerged as the leading strategy for adversarial defenses, it is frequently hindered by a challenging balance between maintaining accuracy on unaltered examples and enhancing model robustness. Recent efforts on decoupling network components can effectively reduce the degradation of classification accuracy, but at the cost of an unsatisfactory in robust accuracy, and may suffer from robust overfitting. In this paper, we delve into the underlying causes of this compromise, and introduce a novel framework, the Regularized Decoupled Adversarial Training Mechanism (RDAT) to effectively deal with the trade-off and overfitting. Specifically, RDAT comprises two distinct modules: Regularization module mitigates harmful perturbations by controlling the data distribution distance of examples before and after adversarial attacks. Decoupling training module separates clean and adversarial examples so that they can have special optimization strategies to avoid the suboptimal result in adversarial training. With marginal compromise on the classification accuracy, RDAT achieves remarkably better model robustness with the improvement of robust accuracy by an average of 4.47% on CIFAR-10 and 3.23% on CIFAR-100 when compared to state-of-the-art methods.
C1 [Li, Yishan; Guo, Yanming; Wu, Yulun; Xie, Yuxiang; Lao, Mingrui; Yu, Tianyuan; Ruan, Yirun] Natl Univ Def Technol, Changsha, Peoples R China.
   [Guo, Yanming] Hunan Inst Adv Technol, Changsha, Peoples R China.
C3 National University of Defense Technology - China
RP Guo, YM (corresponding author), Natl Univ Def Technol, Changsha, Peoples R China.; Guo, YM (corresponding author), Hunan Inst Adv Technol, Changsha, Peoples R China.
EM guoyanming@nudt.edu.cn
FU Natural Science Foundation of Hunan Province; Dreams Foundation of
   Jianghuai Advance Technology Center [2023JJ30082]; Natural Science
   Foundation of Hunan Province
FX This work was supported by Dreams Foundation of Jianghuai Advance
   Technology Center (NO.2023-ZM01Z002) and the Natural Science Foundation
   of Hunan Province under Grant 2023JJ30082.
CR Andriushchenko M, 2020, P 34 ANN C NEUR INF, P16048
   Balaji Y, 2019, Arxiv, DOI [arXiv:1910.08051, DOI 10.48550/ARXIV:1910.08051, 10.48550/arXiv:1910.08051]
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Carmon Y, 2019, 33 C NEURAL INFORM P, V32
   Chen CY, 2015, IEEE I CONF COMP VIS, P2722, DOI 10.1109/ICCV.2015.312
   Chen ZY, 2022, PROC CVPR IEEE, P15127, DOI 10.1109/CVPR52688.2022.01472
   Cheng M., 2022, IJCAI, P673, DOI 10.24963/ijcai.2022/95
   Croce F, 2020, PR MACH LEARN RES, V119
   Croce Francesco, 2022, INT C MACH LEARN, V162, P4421
   Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020
   Cui JQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15701, DOI 10.1109/ICCV48922.2021.01543
   Dong JH, 2023, PROC CVPR IEEE, P24678, DOI 10.1109/CVPR52729.2023.02364
   Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957
   Genzel M, 2023, IEEE T PATTERN ANAL, V45, P1119, DOI 10.1109/TPAMI.2022.3148324
   Ghamizi S, 2023, INT C MACH LEARN ICM, P11255
   Gu TY, 2019, IEEE ACCESS, V7, P47230, DOI 10.1109/ACCESS.2019.2909068
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D, 2019, PR MACH LEARN RES, V97
   Huang H, 2021, 35 C NEURAL INFORM P, V34
   Huang H, 2021, Arxiv, DOI arXiv:2101.02644
   Irvin J, 2019, AAAI CONF ARTIF INTE, P590
   Javanmard A., 2020, C LEARN THEOR, P2034
   Jia XJ, 2022, PROC CVPR IEEE, P13388, DOI 10.1109/CVPR52688.2022.01304
   Jin GJ, 2023, PROC CVPR IEEE, P16447, DOI 10.1109/CVPR52729.2023.01578
   Jin GJ, 2022, PROC CVPR IEEE, P15252, DOI 10.1109/CVPR52688.2022.01484
   Krizhevsky A., 2012, Learning multiple layers of features from tiny images, DOI DOI 10.1145/3079856.3080246
   Lamb Alex, 2019, P 12 ACM WORKSH ART, P95, DOI 10.1145/3338501.3357369
   Li BQ, 2023, AAAI CONF ARTIF INTE, P14982
   Li T, 2022, PROC CVPR IEEE, P13399, DOI 10.1109/CVPR52688.2022.01305
   Liu AS, 2023, PROC CVPR IEEE, P4096, DOI 10.1109/CVPR52729.2023.00399
   Liu Q, 2023, IEEE T NEUR NET LEAR, V34, P3, DOI 10.1109/TNNLS.2021.3089128
   Liu YT, 2017, PR IEEE COMP DESIGN, P45, DOI 10.1109/ICCD.2017.16
   Madry A, 2018, INT C LEARN REPR, DOI 10.48550/arXiv.1706.06083
   Mao X, 2022, Advances in Neural Information Processing Systems, P7520
   Pang TY, 2022, PR MACH LEARN RES
   Poursaeed O, 2021, P IEEE CVF INT C COM, P15711
   Pu N, 2023, PROC CVPR IEEE, P7579, DOI 10.1109/CVPR52729.2023.00732
   Pu N, 2023, IEEE T PATTERN ANAL, V45, P13567, DOI 10.1109/TPAMI.2023.3297058
   Qiu H, 2024, IEEE T COMPUT, V73, P645, DOI 10.1109/TC.2021.3076826
   Rade R., 2022, INT C LEARN REPR ICL
   Raghunathan A., 2020, ICML, P7909
   Rice L., 2020, PMLR, P8093
   Schmidt L, 2018, ADV NEUR IN, V31
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Song Chuanbiao, 2019, INT C LEARN REPR ICL
   Su D, 2018, LECT NOTES COMPUT SC, V11216, P644, DOI 10.1007/978-3-030-01258-8_39
   Szegedy C, 2014, Arxiv, DOI arXiv:1312.6199
   Tack J, 2022, AAAI CONF ARTIF INTE, P8414
   Tramèr F, 2022, PR MACH LEARN RES
   Truong JB, 2021, PROC CVPR IEEE, P4769, DOI 10.1109/CVPR46437.2021.00474
   Uesato J, 2019, ADV NEUR IN, V32
   Wang HJ, 2023, PROC CVPR IEEE, P20554, DOI 10.1109/CVPR52729.2023.01969
   Wong E., 2020, P 8 INT C LEARN REPR
   Xu H, 2021, PR MACH LEARN RES, V139
   Yang S, 2022, P EUR C COMP VIS ECC, P78
   Yu Y, 2021, Computer research repository (CoRR)
   Yuval N, 2011, P NEUR INF PROC SYST
   Zhang, 2020, INT C MACHINE LEARNI, P11278
   Zhang H, 2019, INT C LEARN REPR ICL
   Zhang HY, 2019, PR MACH LEARN RES, V97
   Zhang Jingzhao, 2021, INT C LEARN REPR ICL
   Zhao SJ, 2022, LECT NOTES COMPUT SC, V13664, P585, DOI 10.1007/978-3-031-19772-7_34
   Zhou HS, 2020, PROC INT CONF SOFTW, P347, DOI 10.1145/3377811.3380422
NR 63
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD JUN
PY 2024
VL 13
IS 2
AR 24
DI 10.1007/s13735-024-00330-y
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PQ0R1
UT WOS:001215433200002
DA 2024-08-05
ER

PT J
AU Singhal, S
   Pal, K
AF Singhal, Shilpa
   Pal, Kunwar
TI State of art and emerging trends on group recommender system: a
   comprehensive review
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Recommender system (RS); Group recommender system (GRS); Social
   networks; Aggregation models; Content-based system; Collaborative
   filtering; Knowledge-based system; Similarity measure
ID MATRIX FACTORIZATION; USER; PREFERENCES; ONTOLOGY; DESIGN; MODEL
AB A group recommender system (GRS) generates suggestions for a group of individuals, considering not only each person's preferences but also factors such as social dynamics and behavior to deliver recommendations that balance personal taste and social factors. In this study, 225 papers have been analyzed from different journals and conference papers, covering a variety of literature works on group recommendation systems. The articles in the literature used for the review were published between 2010 and 2023. This overview of the literature focuses on several methods for creating group recommender systems. This review starts by providing an overview of group recommender systems, including the challenges and essential elements for their development. It then examines the existing literature on collaborative, content-based, and knowledge-based group recommendation techniques. Beyond traditional approaches, this study identifies a notable research gap in the integration of audio, image, and video recommendation systems within the group recommendation paradigm. It then discusses the research gaps found in the existing papers. The review also discusses various aggregation techniques and evaluation metrics used to evaluate these techniques. The review concludes by discussing the limitations and potential future directions for group recommendation research. This review aims to give a thorough understanding of the current status of group recommendations and to pinpoint potential areas for future study.
C1 [Singhal, Shilpa; Pal, Kunwar] Dr BR Ambedkar Natl Inst Technol, Dept Comp Sci Engn, Jalandhar, Punjab, India.
C3 National Institute of Technology (NIT System); Dr B R Ambedkar National
   Institute of Technology Jalandhar
RP Singhal, S (corresponding author), Dr BR Ambedkar Natl Inst Technol, Dept Comp Sci Engn, Jalandhar, Punjab, India.
EM shilpas.cs.21@nitj.ac.in; kunwarp@nitj.ac.in
OI Singhal, Shilpa/0000-0001-7542-9707
CR Aarthy S., 2023, Int J Adv Comput Sci Appl, DOI [10.14569/IJACSA.2023.0140262, DOI 10.14569/IJACSA.2023.0140262]
   Abdollahpouri Himan, 2021, UMAP '21: Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization, P119, DOI 10.1145/3450613.3456821
   Abdul Hussien Farah Tawfiq, 2021, Journal of Physics: Conference Series, V1897, DOI 10.1088/1742-6596/1897/1/012024
   Abu-Salih B, 2021, J UNIVERS COMPUT SCI, V27, P208, DOI 10.3897/jucs.65096
   Adamopoulos P, 2015, ACM T INTEL SYST TEC, V5, DOI 10.1145/2559952
   Adiyansjah, 2019, PROCEDIA COMPUT SCI, V157, P99, DOI 10.1016/j.procs.2019.08.146
   Aftab S, 2022, IEEE ACCESS, V10, P30832, DOI 10.1109/ACCESS.2022.3159646
   Agarwal A, 2022, COGENT ENG, V9, DOI 10.1080/23311916.2021.2022568
   Ahmed E, 2023, APPL COMPUT INTELL S, V2023, DOI 10.1155/2023/1514801
   AL-Bakri NF., 2019, Al-Nahrain J Sci, V22, P74, DOI [10.22401/ANJS.22.1.10, DOI 10.22401/ANJS.22.1.10]
   Alamdari PM, 2020, IEEE ACCESS, V8, P115694, DOI 10.1109/ACCESS.2020.3002803
   Ali Y, 2022, PLOS ONE, V17, DOI 10.1371/journal.pone.0266103
   Alizadeh M, 2020, SCI ADV, V6, DOI 10.1126/sciadv.abb5824
   Aljunid Mohammed Fadhel, 2020, Procedia Computer Science, V171, P829, DOI 10.1016/j.procs.2020.04.090
   Aljunid MF, 2020, CAAI T INTELL TECHNO, V5, P268, DOI 10.1049/trit.2020.0031
   Alsaif SA, 2022, COMPUTERS, V11, DOI 10.3390/computers11110161
   Amane M, 2021, MULTIAGENT CONTENT B, P663, DOI [10.1007/978-3-030-73882-2_60, DOI 10.1007/978-3-030-73882-2_60]
   [Anonymous], 2011, Proceedings of the fifth ACM conference on Recommender systems
   Asani E, 2021, MACH LEARN APPL, V6, DOI 10.1016/j.mlwa.2021.100114
   Baatarjav EA, 2008, LECT NOTES COMPUT SC, V5333, P211
   Bagher RC, 2017, EXPERT SYST APPL, V87, P209, DOI 10.1016/j.eswa.2017.06.020
   Baltrunas Linas, 2010, ACM Conference on Recommender Systems (RecSys), P119, DOI [DOI 10.1145/1864708.1864733.URL, 10.1145/1864708.1864733, DOI 10.1145/1864708.1864733]
   Belhadi A, 2022, IEEE T INTELL TRANSP, V23, P9346, DOI 10.1109/TITS.2021.3114064
   Bellini P, 2023, MULTIMED TOOLS APPL, V82, P9989, DOI 10.1007/s11042-021-11837-5
   Berkovsky S., 2010, P 4 ACM C REC SYST N, P111
   Biancalana C, 2011, PROCEEDINGS OF THE RECSYS'2011 ACM CHALLENGE ON CONTEXT-AWARE MOVIE RECOMMENDATION (CAMRA2011), P5
   Bin Hossain A., 2023, Integrated Music Recommendation System Using Collaborative and Content Based Filtering, and Sentiment Analysis, P162
   Blanco-Fernández Y, 2011, INFORM SCIENCES, V181, P4823, DOI 10.1016/j.ins.2011.06.016
   Bobadilla J, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10072441
   Bobadilla J, 2011, KNOWL-BASED SYST, V24, P1310, DOI 10.1016/j.knosys.2011.06.005
   Bobadilla J, 2011, EXPERT SYST APPL, V38, P14609, DOI 10.1016/j.eswa.2011.05.021
   Bogdanov D, 2013, INFORM PROCESS MANAG, V49, P13, DOI 10.1016/j.ipm.2012.06.004
   Bokde D, 2015, PROCEDIA COMPUT SCI, V49, P136, DOI 10.1016/j.procs.2015.04.237
   Boratto Ludovico, 2016, Advances in Information Retrieval. 38th European Conference on IR Research, ECIR 2016. Proceedings; LNCS 9626, P889, DOI 10.1007/978-3-319-30671-1_87
   Boratto L, 2015, J INTELL INF SYST, V45, P221, DOI 10.1007/s10844-014-0346-z
   Boratto L, 2015, INT J MACH LEARN CYB, V6, P953, DOI 10.1007/s13042-015-0371-4
   Bouton L, 2016, J PUBLIC ECON, V134, P114, DOI 10.1016/j.jpubeco.2015.11.003
   Carrer-Neto W, 2012, EXPERT SYST APPL, V39, P10990, DOI 10.1016/j.eswa.2012.03.025
   Castells P., 2015, Novelty and diversity in recommender systems, P881, DOI [DOI 10.1007/978-1-4899-7637-6_26, 10.1007/978-1-4899-7637-6\_26, DOI 10.1007/978-1-4899-7637-6]
   Castells P, 2022, AI MAG, V43, P225, DOI 10.1002/aaai.12051
   Castro J, 2015, INT J INTELL SYST, V30, P887, DOI 10.1002/int.21730
   Cedric Bernier AB, 2010, 3 INT C INF SYST EC, P1
   Celma O, 2008, RECSYS'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON RECOMMENDER SYSTEMS, P179
   Cena F, 2021, INFORM SCIENCES, V546, P60, DOI 10.1016/j.ins.2020.07.075
   Chai T, 2014, GEOSCI MODEL DEV, V7, P1247, DOI 10.5194/gmd-7-1247-2014
   Chakrabarti P, 2023, SOC NETW ANAL MIN, V13, DOI 10.1007/s13278-023-01024-9
   Chalkiadakis G, 2023, ALGORITHMS, V16, DOI 10.3390/a16040215
   Chandra S., 2011, Proceedings of the 2011 IEEE Third International Conference on Privacy, Security, Risk and Trust and IEEE Third International Conference on Social Computing (PASSAT/SocialCom 2011), P838, DOI 10.1109/PASSAT/SocialCom.2011.120
   Chen M., 2017, INT J PERFORMABILITY, V13, DOI [DOI 10.23940/IJPE.17.08.P7.12461256, 10.23940/ijpe.17.08.p7.12461256]
   Chen R, 2018, IEEE ACCESS, V6, P64301, DOI 10.1109/ACCESS.2018.2877208
   Christensen I, 2016, J INTELL INF SYST, V47, P209, DOI 10.1007/s10844-016-0400-0
   Christensen IA, 2011, EXPERT SYST APPL, V38, P14127, DOI 10.1016/j.eswa.2011.04.221
   Christensen IA, 2014, ONLINE INFORM REV, V38, P524, DOI 10.1108/OIR-08-2013-0187
   Colace F, 2022, CONNECT SCI, V34, P2158, DOI 10.1080/09540091.2022.2106943
   Dandouh K, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0169-4
   Dara S, 2020, J INTELL INF SYST, V54, P271, DOI 10.1007/s10844-018-0542-3
   Das J, 2014, 2014 INTERNATIONAL CONFERENCE ON CONTEMPORARY COMPUTING AND INFORMATICS (IC3I), P230, DOI 10.1109/IC3I.2014.7019655
   Davis J., 2006, P 23 INT C MACH LEAR, P233, DOI DOI 10.1145/1143844.1143874
   De Medio C, 2020, COMPUT HUM BEHAV, V104, DOI 10.1016/j.chb.2019.106168
   De Pessemier T, 2016, MULTIMED TOOLS APPL, V75, P3323, DOI 10.1007/s11042-014-2437-9
   De Pessemier T, 2014, MULTIMED TOOLS APPL, V72, P2497, DOI 10.1007/s11042-013-1563-0
   Ding YH, 2015, 2015 7TH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY IN MEDICINE AND EDUCATION (ITME), P318, DOI 10.1109/ITME.2015.99
   Dong M, 2020, INFORM SCIENCES, V540, P469, DOI 10.1016/j.ins.2020.05.094
   Dueñas-Lerín J, 2023, NEURAL COMPUT APPL, V35, P14081, DOI 10.1007/s00521-023-08410-6
   El Idrissi LE, 2023, APPL SYST INNOV, V6, DOI 10.3390/asi6060102
   Elkahky A, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P278, DOI 10.1145/2736277.2741667
   Esfahani MH, 2013, 2013 5TH CONFERENCE ON INFORMATION AND KNOWLEDGE TECHNOLOGY (IKT), P145, DOI 10.1109/IKT.2013.6620054
   Etemadi M, 2023, EXPERT SYST APPL, V213, DOI 10.1016/j.eswa.2022.118823
   Fakhri AA, 2019, J PHYS CONF SER, V1192, DOI 10.1088/1742-6596/1192/1/012023
   Felfernig A, 2021, Arxiv, DOI [arXiv:2102.09005, 10.48550/arXiv.2102.09005, DOI 10.48550/ARXIV.2102.09005]
   Garcia I, 2011, EXPERT SYST APPL, V38, P7683, DOI 10.1016/j.eswa.2010.12.143
   Ge M., 2010, Proceedings of the fourth ACM conference on Recommender systems, P257, DOI [DOI 10.1145/1864708.1864761, 10.1145/1864708.1864761]
   Gharahighehi A, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11167502
   Gomez-Uribe CA, 2016, ACM TRANS MANAG INF, V6, DOI 10.1145/2843948
   Crespo RG, 2011, COMPUT HUM BEHAV, V27, P1445, DOI 10.1016/j.chb.2010.09.012
   Morales LFG, 2022, J MED INTERNET RES, V24, DOI 10.2196/37233
   Gunawardana A, 2009, J MACH LEARN RES, V10, P2935
   Gutiérrez F, 2019, RECSYS 2019: 13TH ACM CONFERENCE ON RECOMMENDER SYSTEMS, P60, DOI 10.1145/3298689.3347001
   Gyrard Amelie, 2020, Smart Health, V15, P26, DOI 10.1016/j.smhl.2019.100083
   Haque P, 2021, EDU J Comput Electr Eng, V2, P30, DOI [10.46603/ejcee.v2i1.36, DOI 10.46603/EJCEE.V2I1.36]
   Hasan M, 2023, FRONT PLANT SCI, V14, DOI 10.3389/fpls.2023.1234555
   Hatami M., 2014, Int J Comput Appl, V88, P46, DOI [10.5120/15440-3981, DOI 10.5120/15440-3981]
   He Li, 2023, WSDM '23: Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, P60, DOI 10.1145/3539597.3570451
   Herlocker JL, 2004, ACM T INFORM SYST, V22, P5, DOI 10.1145/963770.963772
   Hernández-Nieves E, 2021, ENG APPL ARTIF INTEL, V104, DOI 10.1016/j.engappai.2021.104327
   Hernando A, 2016, KNOWL-BASED SYST, V97, P188, DOI 10.1016/j.knosys.2015.12.018
   Hurley N, 2011, ACM T INTERNET TECHN, V10, DOI 10.1145/1944339.1944341
   Jain Gourav, 2023, Procedia Computer Science, P1834, DOI 10.1016/j.procs.2023.01.161
   Jaiswal Sapna, 2020, ITM Web of Conferences, V32, DOI 10.1051/itmconf/20203203034
   Jalui M., 2023, SSRN Electron J, DOI [10.2139/ssrn.4428451, DOI 10.2139/SSRN.4428451]
   Jannach D, 2019, ACM TRANS MANAG INF, V10, DOI 10.1145/3370082
   Jena KK, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12010157
   Jiang W., 2018, A Dynamic Bayesian Network Based Collaborative Filtering Model for Multi-stage Recommendation, P290, DOI [10.1007/978-3-319-66824-6_26, DOI 10.1007/978-3-319-66824-6_26]
   Jomsri P., 2023, F1000Research, V12, P1140, DOI [10.12688/f1000research.133013.1, DOI 10.12688/F1000RESEARCH.133013.1]
   Jouyandeh F., 2022, Procedia Comput Sci, V201, P375, DOI [10.1016/j.procs.2022.03.050, DOI 10.1016/J.PROCS.2022.03.050]
   Ju CH, 2021, BIOMED RES INT-UK, V2021, DOI 10.1155/2021/7431199
   Kanaujia PKM, 2016, PROCEEDINGS OF 2016 INTERNATIONAL CONFERENCE ON ICT IN BUSINESS INDUSTRY & GOVERNMENT (ICTBIG)
   Kang JS, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9204284
   Kaya M, 2020, RECSYS 2020: 14TH ACM CONFERENCE ON RECOMMENDER SYSTEMS, P101, DOI 10.1145/3383313.3412232
   Khadka S., 2021, Asian J. Electr. Sci, V9, P17, DOI [10.51983/ajes-2020.9.2.2552, DOI 10.51983/AJES-2020.9.2.2552]
   Kim HN, 2010, ELECTRON COMMER R A, V9, P73, DOI 10.1016/j.elerap.2009.08.004
   Kim JK, 2010, INT J INFORM MANAGE, V30, P212, DOI 10.1016/j.ijinfomgt.2009.09.006
   Kim J, 2014, MULTIMED TOOLS APPL, V71, P855, DOI 10.1007/s11042-011-0920-0
   Kiran R, 2020, EXPERT SYST APPL, V144, DOI 10.1016/j.eswa.2019.113054
   Kiruthika S., 2023, Meas Sensors, V27, DOI [10.1016/j.measen.2023.100722, DOI 10.1016/J.MEASEN.2023.100722]
   Klimashevskaia A, 2023, Arxiv, DOI arXiv:2308.01118
   Koutsopoulos I, 2018, 2018 16TH IEEE INT CONF ON DEPENDABLE, AUTONOM AND SECURE COMP, 16TH IEEE INT CONF ON PERVAS INTELLIGENCE AND COMP, 4TH IEEE INT CONF ON BIG DATA INTELLIGENCE AND COMP, 3RD IEEE CYBER SCI AND TECHNOL CONGRESS (DASC/PICOM/DATACOM/CYBERSCITECH), P912, DOI 10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.000-9
   Kuanr Madhusree, 2021, Progress in Advanced Computing and Intelligent Engineering. Proceedings of ICACIE 2019. Advances in Intelligent Systems and Computing (AISC 1199), P353, DOI 10.1007/978-981-15-6353-9_32
   Kumar C, 2021, Auto-detecting groups based on textual similarity for group recommendations
   Kumar J., 2021, New Paradigms in Computational Modeling and Its Applications, P209, DOI [10.1016/B978-0-12-822133-4.00005-0, DOI 10.1016/B978-0-12-822133-4.00005-0]
   Lam KY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2447, DOI 10.1145/3474085.3475413
   Lestari S, 2018, 2018 INTERNATIONAL WORKSHOP ON BIG DATA AND INFORMATION SECURITY (IWBIS), P69, DOI 10.1109/IWBIS.2018.8471722
   Lin FC, 2011, EXPERT SYST APPL, V38, P9129, DOI 10.1016/j.eswa.2011.01.051
   Lou F, 2022, WIREL COMMUN MOB COM, V2022, DOI 10.1155/2022/7321021
   Lourenco J, 2020, IEEE INT CONF BIG DA, P4636, DOI 10.1109/BigData50022.2020.9377807
   Masthoff J, 2004, USER MODEL USER-ADAP, V14, P37, DOI 10.1023/B:USER.0000010138.79319.fd
   Meymandpour R, 2016, KNOWL-BASED SYST, V109, P276, DOI 10.1016/j.knosys.2016.07.012
   Mohammadpour T, 2019, GENOMICS, V111, P1902, DOI 10.1016/j.ygeno.2019.01.001
   Mubarak M, 2022, ISPRS INT J GEO-INF, V11, DOI 10.3390/ijgi11030178
   Musto C, 2014, LECT NOTES COMPUT SC, V8538, P381
   Nadee W, 2013, 2013 IEEE/WIC/ACM INTERNATIONAL JOINT CONFERENCE ON WEB INTELLIGENCE AND INTELLIGENT AGENT TECHNOLOGY - WORKSHOPS (WI-IAT), VOL 3, P5, DOI 10.1109/WI-IAT.2013.140
   Narasimhan H, 2020, Arxiv, DOI arXiv:1906.05330
   Naseri S., 2015, Recommend Search Soc Netw, DOI [10.1007/978-3-319-14379-8_7, DOI 10.1007/978-3-319-14379-8_7]
   Nawi RM, 2021, IEEE ACCESS, V9, P150753, DOI 10.1109/ACCESS.2021.3124939
   Nesmaoui Redwane, 2023, Procedia Computer Science, P456, DOI 10.1016/j.procs.2023.03.058
   Nguyen LV, 2023, BIG DATA COGN COMPUT, V7, DOI 10.3390/bdcc7020106
   Nilashi M, 2018, EXPERT SYST APPL, V92, P507, DOI 10.1016/j.eswa.2017.09.058
   Ning HS, 2019, IEEE T COMPUT SOC SY, V6, P394, DOI 10.1109/TCSS.2019.2903857
   Ofem OA, 2023, INT J SOFTW INNOV, V11, P27, DOI 10.4018/IJSI.315660
   Colombo-Mendoza LO, 2018, J INF SCI, V44, P464, DOI 10.1177/0165551517698787
   Colombo-Mendoza LO, 2015, EXPERT SYST APPL, V42, P1202, DOI 10.1016/j.eswa.2014.09.016
   Ortega F, 2016, INFORM SCIENCES, V345, P313, DOI 10.1016/j.ins.2016.01.083
   Parra D, 2013, STUD COMPUT INTELL, V452, P149
   Patel K., 2023, Curr Agric Res J, V11, P137, DOI [10.12944/CARJ.11.1.12, DOI 10.12944/CARJ.11.1.12]
   Pérez-Almaguer Y, 2021, EXPERT SYST APPL, V184, DOI 10.1016/j.eswa.2021.115444
   Pincay J, 2019, INT CONF EDEMOC EGOV, P47, DOI [10.1109/icedeg.2019.8734362, 10.1109/ICEDEG.2019.8734362]
   Pujahari A, 2020, EXPERT SYST APPL, V156, DOI 10.1016/j.eswa.2020.113476
   Pujahari A, 2015, 2015 14TH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY (ICIT 2015), P148, DOI 10.1109/ICIT.2015.36
   Qazi M, 2020, WIRES DATA MIN KNOWL, V10, DOI 10.1002/widm.1363
   Quijano-Sánchez L, 2014, KNOWL-BASED SYST, V71, P72, DOI 10.1016/j.knosys.2014.05.013
   Rahayu N.W., 2022, Comput. Educ. Artif. Intell, V3, P100047, DOI DOI 10.1016/J.CAEAI.2022.100047
   Ramakrishna MT, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12061365
   Rao J, 2022, 2022 IEEE 5 INT C AR, P82, DOI [10.1109/AIKE55402.2022.00020, DOI 10.1109/AIKE55402.2022.00020]
   Ren Weijieying, 2022, arXiv
   Ricci F, 2011, RECOMMENDER SYSTEMS HANDBOOK, P1, DOI 10.1007/978-0-387-85820-3_1
   Rosa RL, 2019, IEEE T IND INFORM, V15, P2124, DOI 10.1109/TII.2018.2867174
   Rossetti M, 2013, INT WORKSHOP DATABAS, P162, DOI 10.1109/DEXA.2013.26
   Roy D, 2023, 2023 4 INT C COMP CO, P1, DOI [10.1109/I3CS58314.2023.10127394, DOI 10.1109/I3CS58314.2023.10127394]
   Sajde Maryam, 2022, Informatics in Medicine Unlocked, DOI 10.1016/j.imu.2022.100950
   Samin H, 2019, IEEE ACCESS, V7, P67081, DOI 10.1109/ACCESS.2019.2912012
   Sampath D., 2010, P 4 ACM C REC SYST, V10, P293, DOI [DOI 10.1145/1864708.1864770, 10.1145/1864708.1864770]
   Sarkar JL, 2023, MULTIMED TOOLS APPL, V82, P8983, DOI 10.1007/s11042-022-12167-w
   Sarna G, 2017, INT J MACH LEARN CYB, V8, P677, DOI 10.1007/s13042-015-0463-1
   Senot C., 2011, IJCAI INT JT C ARTIF, P2728, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-454
   Seo YD, 2021, EXPERT SYST APPL, V183, DOI 10.1016/j.eswa.2021.115396
   Seo YD, 2018, EXPERT SYST APPL, V93, P299, DOI 10.1016/j.eswa.2017.10.027
   Shambour Qusai Yousef, 2023, Interdisciplinary Journal of Information, Knowledge, and Management, P435, DOI 10.28945/5172
   Sharaf M, 2022, MULTIMED TOOLS APPL, V81, P16761, DOI 10.1007/s11042-022-12564-1
   Sharma S., 2023, Recommendation Systems for a Group of Users Which Recommend Recent Attention: Using Hybrid Recommendation Model, P659, DOI [10.1007/978-3-031-25088-0_58, DOI 10.1007/978-3-031-25088-0_58]
   Shi X, 2023, Research on house rental recommendation algorithm based on deep learning, P604, DOI [10.2991/978-94-6463-124-1_70, DOI 10.2991/978-94-6463-124-1_70]
   Shi XH, 2023, LIBR HI TECH, DOI 10.1108/LHT-08-2022-0400
   Shokrzadeh Z, 2024, AIN SHAMS ENG J, V15, DOI 10.1016/j.asej.2023.102263
   Shuhao Jiang, 2021, Journal of Physics: Conference Series, V1920, DOI 10.1088/1742-6596/1920/1/012109
   Siddiquee MMR, 2014, I C SOFTWARE KNOWL I
   Silveira JD., 2022, "Enabling Reproducibility in Group Recommender Systems, DOI [10.3233/FAIA220324, DOI 10.3233/FAIA220324]
   Singh A, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2219, DOI 10.1145/3219819.3220088
   Smyth B, 2001, LECT NOTES ARTIF INT, V2080, P347
   Sojahrood ZB, 2023, EXPERT SYST APPL, V219, DOI 10.1016/j.eswa.2023.119681
   SongJie Gong, 2010, Journal of Software, V5, P745, DOI 10.4304/jsw.5.7.745-752
   Sonoda A, 2022, J COMPUT SOC SCI, V5, P1595, DOI 10.1007/s42001-022-00179-3
   Sridhar S, 2023, INTELL AUTOM SOFT CO, V35, P3241, DOI 10.32604/iasc.2023.030361
   Stratigi M, 2020, ALGORITHMS, V13, DOI 10.3390/a13030054
   Su X., 2009, Advances in Artificial Intelligence, V2009, DOI [DOI 10.1155/2009/421425, 10.1155/2009/421425]
   Suganeshwari G, 2023, NEURAL COMPUT APPL, V35, P25235, DOI 10.1007/s00521-023-08694-8
   Swaminathan B, 2023, EXPERT SYST APPL, V216, DOI 10.1016/j.eswa.2022.119441
   Takayanagi T, 2023, PROCEEDINGS OF THE 46TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, SIGIR 2023, P3339, DOI 10.1145/3539618.3591850
   Tan WY, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-24494-x
   Tang Y, 2016, 2016 IEEE ACIS 15 IN, P0, DOI [10.1109/ICIS.2016.7550761, DOI 10.1109/ICIS.2016.7550761]
   Tarus JK, 2018, ARTIF INTELL REV, V50, P21, DOI 10.1007/s10462-017-9539-5
   Tarus JK, 2017, FUTURE GENER COMP SY, V72, P37, DOI 10.1016/j.future.2017.02.049
   Tewari AS, 2020, PROCEDIA COMPUT SCI, V167, P1934, DOI 10.1016/j.procs.2020.03.215
   Tran TNT, 2018, J INTELL INF SYST, V50, P501, DOI 10.1007/s10844-017-0469-0
   Thorat T, 2023, SMART AGR TECHNOL, V3, DOI 10.1016/j.atech.2022.100114
   Nguyen TN, 2018, INF TECHNOL TOUR, V18, P5, DOI 10.1007/s40558-017-0099-y
   Tran C, 2019, IEEE ACCESS, V7, P62115, DOI 10.1109/ACCESS.2019.2914556
   Tran TNT, 2021, J INTELL INF SYST, V57, P171, DOI 10.1007/s10844-020-00633-6
   Valcarce D, 2020, INFORM RETRIEVAL J, V23, P411, DOI 10.1007/s10791-020-09377-x
   Valdiviezo-Diaz P, 2019, IEEE ACCESS, V7, P108581, DOI 10.1109/ACCESS.2019.2933048
   Valera A, 2021, INFORMATION, V12, DOI 10.3390/info12120506
   Villavicencio C, 2019, KNOWL-BASED SYST, V164, P436, DOI 10.1016/j.knosys.2018.11.013
   Wan SS, 2018, KNOWL-BASED SYST, V160, P71, DOI 10.1016/j.knosys.2018.06.014
   Wang FR, 2020, CONCURR COMP-PRACT E, V32, DOI 10.1002/cpe.5565
   Wang H, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1235, DOI 10.1145/2783258.2783273
   Wang LL, 2023, MATHEMATICS-BASEL, V11, DOI 10.3390/math11061346
   Wang W, 2016, DECIS SUPPORT SYST, V87, P80, DOI 10.1016/j.dss.2016.05.002
   Wang XB, 2020, DISCRETE DYN NAT SOC, V2020, DOI 10.1155/2020/6480273
   Wang Yu, 2023, arXiv, DOI [10.48550/arXiv.2307.04644, DOI 10.48550/ARXIV.2307.04644]
   Wayesa F, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-30987-0
   Wei J, 2017, EXPERT SYST APPL, V69, P29, DOI 10.1016/j.eswa.2016.09.040
   Wei SY, 2012, 2012 FIFTH INTERNATIONAL CONFERENCE ON BUSINESS INTELLIGENCE AND FINANCIAL ENGINEERING (BIFE), P69, DOI 10.1109/BIFE.2012.23
   Wei WY, 2023, J KING SAUD UNIV-COM, V35, DOI 10.1016/j.jksuci.2023.101640
   Wu C., 2021, arXiv
   Wu F., 2022, MULTIMODAL TRANSP, V1
   Wu QY, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1927, DOI 10.1145/3132847.3133025
   Wu Y, 2024, ACM T KNOWL DISCOV D, V18, DOI 10.1145/3604558
   Xu LJ, 2022, SCI PROGRAMMING-NETH, V2022, DOI 10.1155/2022/4823828
   Xu ZY, 2021, MOB INF SYST, V2021, DOI 10.1155/2021/7080116
   Yalcin E, 2021, EXPERT SYST APPL, V174, DOI 10.1016/j.eswa.2021.114709
   Yao Yu, 2006, Wuhan University Journal of Natural Sciences, V11, P1086, DOI 10.1007/BF02829215
   Yera R, 2022, INT J APPROX REASON, V150, P273, DOI 10.1016/j.ijar.2022.08.015
   Yin P, 2022, MATH PROBL ENG, V2022, DOI 10.1155/2022/4655030
   Zarzour H, 2018, INT CONF INFORM COMM, P102, DOI 10.1109/IACS.2018.8355449
   Zhang CB, 2018, INFORM SCIENCES, V454, P128, DOI 10.1016/j.ins.2018.04.061
   Zhang CY, 2022, SUSTAIN CITIES SOC, V76, DOI 10.1016/j.scs.2021.103373
   Zhang HJ, 2023, INFORMATION, V14, DOI 10.3390/info14020060
   Zhang J., 2022, Front Comput Intell Syst, V1, P63, DOI [10.54097/fcis.v1i2.1774, DOI 10.54097/FCIS.V1I2.1774]
   Zhang WJ, 2021, WIREL COMMUN MOB COM, V2021, DOI 10.1155/2021/7072849
   Zhe ding, 2020, Journal of Electronic Science and Technology, P1, DOI 10.1016/j.jnlest.2020.100054
   Zheng XL, 2015, IEEE T LEARN TECHNOL, V8, P345, DOI 10.1109/TLT.2015.2419262
   Zheng ZQ, 2020, EXPERT SYST APPL, V162, DOI 10.1016/j.eswa.2019.113006
   Zhou Q, 2019, CCF T PERVAS COMPUT, V1, P260, DOI 10.1007/s42486-019-00022-1
   Zhou T, 2010, P NATL ACAD SCI USA, V107, P4511, DOI 10.1073/pnas.1000488107
   Zhou WZ, 2023, KNOWL-BASED SYST, V276, DOI 10.1016/j.knosys.2023.110731
   Zhuang K, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app122413001
   Ziegler C. N., 2005, P 14 INT C WORLD WID, P22, DOI DOI 10.1145/1060745.1060754
NR 225
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD JUN
PY 2024
VL 13
IS 2
AR 22
DI 10.1007/s13735-024-00329-5
PG 39
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RE3E1
UT WOS:001225945300002
DA 2024-08-05
ER

PT J
AU Sharma, H
   Padha, D
AF Sharma, Himanshu
   Padha, Devanand
TI Domain-specific image captioning: a comprehensive review
SO INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL
LA English
DT Article
DE Computer vision; Deep learning; Medical image captioning; Natural image
   captioning; Remote sensing image captioning
ID AUTOMATIC IMAGE; GENERATION; MODELS; RETRIEVAL; SPEECH
AB An image caption is a sentence summarizing the semantic details of an image. It is a blended application of computer vision and natural language processing. The earlier research addressed this domain using machine learning approaches by modeling image captioning frameworks using hand-engineered feature extraction techniques. With the resurgence of deep-learning approaches, the development of improved and efficient image captioning frameworks is on the rise. Image captioning is witnessing tremendous growth in various domains as medical, remote sensing, security, visual assistance, and multimodal search engines. In this survey, we comprehensively study the image captioning frameworks based on our proposed domain-specific taxonomy. We explore the benchmark datasets and metrics leveraged for training and evaluating image captioning models in various application domains. In addition, we also perform a comparative analysis of the reviewed models. Natural image captioning, medical image captioning, and remote sensing image captioning are currently among the most prominent application domains of image captioning. The efficacy of real-time image captioning is a challenging obstacle limiting its implementation in sensitive areas such as visual aid, remote security, and healthcare. Further challenges include the scarcity of rich domain-specific datasets, training complexity, evaluation difficulty, and a deficiency of cross-domain knowledge transfer techniques. Despite the significant contributions made, there is a need for additional efforts to develop steadfast and influential image captioning models.
C1 [Sharma, Himanshu; Padha, Devanand] Cent Univ Jammu, Dept Comp Sci & Informat Technol, Jammu 181124, Jammu & Kashmir, India.
C3 Central University of Jammu
RP Sharma, H (corresponding author), Cent Univ Jammu, Dept Comp Sci & Informat Technol, Jammu 181124, Jammu & Kashmir, India.
EM himanshusharma.csit@gmail.com
CR Alam M, 2020, NEUROCOMPUTING, V417, P302, DOI 10.1016/j.neucom.2020.07.053
   Amirian S, 2020, IEEE ACCESS, V8, P218386, DOI 10.1109/ACCESS.2020.3042484
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Bai S, 2018, NEUROCOMPUTING, V311, P291, DOI 10.1016/j.neucom.2018.05.080
   Banerjee S., 2005, P ACL WORKSHOP INTRI, P65, DOI DOI 10.3115/1626355.1626389
   Beddiar DR, 2023, ARTIF INTELL REV, V56, P4019, DOI 10.1007/s10462-022-10270-w
   Berg TL, 2010, LECT NOTES COMPUT SC, V6311, P663, DOI 10.1007/978-3-642-15549-9_48
   Bernardi R, 2016, J ARTIF INTELL RES, V55, P409, DOI 10.1613/jair.4900
   Bin Y, 2022, IEEE T CIRC SYST VID, V32, P52, DOI 10.1109/TCSVT.2021.3063297
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen Z, 2023, P IEEECVF INT C COMP, P18109
   Chen ZH, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2022.3192062
   Cheng QM, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3201474
   Cheng QM, 2021, IEEE J-STARS, V14, P4284, DOI 10.1109/JSTARS.2021.3070872
   Dai B, 2017, IEEE I CONF COMP VIS, P2989, DOI 10.1109/ICCV.2017.323
   Das B., 2023, SN Comput Sci, V4, P208, DOI [10.1007/s42979-023-01671-x, DOI 10.1007/S42979-023-01671-X]
   Das R, 2022, MULTIMED TOOLS APPL, V81, P10051, DOI 10.1007/s11042-022-12042-8
   Demner-Fushman D, 2016, J AM MED INFORM ASSN, V23, P304, DOI 10.1093/jamia/ocv080
   Dittakan K, 2023, MULTIMED TOOLS APPL, DOI 10.1007/s11042-023-17275-9
   Dognin P, 2022, J ARTIF INTELL RES, V73, P437
   Johnson AEW, 2019, Arxiv, DOI arXiv:1901.07042
   Effendi J, 2021, IEEE ACCESS, V9, P55144, DOI 10.1109/ACCESS.2021.3071541
   Elbedwehy S, 2023, NEURAL COMPUT APPL, V35, P19051, DOI 10.1007/s00521-023-08744-1
   Elliott D., 2013, ASS COMPUTATIONAL LI, V13, P1292
   Elliott D, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P42
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Feng YS, 2013, IEEE T PATTERN ANAL, V35, P797, DOI 10.1109/TPAMI.2012.118
   Gajbhiye GO, 2022, ENG APPL ARTIF INTEL, V114, DOI 10.1016/j.engappai.2022.105076
   Gong YC, 2014, LECT NOTES COMPUT SC, V8692, P529, DOI 10.1007/978-3-319-10593-2_35
   Grubinger Michael, 2006, The IAPR TC-12 benchmark: A new evaluation resource for visual information systems. In International Workshop ontoImage (Vol. 2), V2
   Herdade S, 2019, ADV NEUR IN, V32
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Hossain MZ, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3295748
   Hou DB, 2021, IEEE ACCESS, V9, P21236, DOI 10.1109/ACCESS.2021.3056175
   Hoxha G, 2022, 2022 IEEE MEDITERRANEAN AND MIDDLE-EAST GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (M2GARSS), P1, DOI 10.1109/M2GARSS52314.2022.9840136
   Hoxha G, 2020, IEEE J-STARS, V13, P4462, DOI 10.1109/JSTARS.2020.3013818
   Huang W, 2021, IEEE GEOSCI REMOTE S, V18, P436, DOI 10.1109/LGRS.2020.2980933
   Huang YQ, 2020, IEEE T IMAGE PROCESS, V29, P4013, DOI 10.1109/TIP.2020.2969330
   Jia X, 2015, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2015.277
   Jiang WH, 2022, IEEE T IMAGE PROCESS, V31, P3920, DOI 10.1109/TIP.2022.3177318
   Jing BY, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2577
   Karpathy A, 2014, ADV NEUR IN, V27
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kastner MA, 2021, IEEE ACCESS, V9, P162951, DOI 10.1109/ACCESS.2021.3131393
   Kiros R, 2014, PR MACH LEARN RES, V32, P595
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Kumar A., 2017, Int. J. Hybrid Intell. Syst., V14, P123, DOI [10.3233/HIS-170246, DOI 10.3233/HIS-170246]
   Kumar SC, 2019, PROCEDIA COMPUT SCI, V165, P32, DOI 10.1016/j.procs.2020.01.067
   Li W, 2021, IEEE ACCESS, V9, P1420, DOI 10.1109/ACCESS.2020.3047091
   Li XL, 2021, IEEE T GEOSCI REMOTE, V59, P5246, DOI 10.1109/TGRS.2020.3010106
   Lin C.-Y., 2004, ANN M ASS COMP LING, P74
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu CY, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3218921
   Liu MF, 2022, IEEE T CYBERNETICS, V52, P1247, DOI 10.1109/TCYB.2020.2997034
   Liu SQ, 2017, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2017.100
   Liu XX, 2019, VISUAL COMPUT, V35, P445, DOI 10.1007/s00371-018-1566-y
   Lu XQ, 2020, IEEE T GEOSCI REMOTE, V58, P1985, DOI 10.1109/TGRS.2019.2951636
   Lu XX, 2018, IEEE T GEOSCI REMOTE, V56, P2183, DOI 10.1109/TGRS.2017.2776321
   Ma XF, 2021, IEEE GEOSCI REMOTE S, V18, P2001, DOI 10.1109/LGRS.2020.3009243
   Makav B, 2019, 2019 11TH INTERNATIONAL CONFERENCE ON ELECTRICAL AND ELECTRONICS ENGINEERING (ELECO 2019), P945, DOI [10.23919/ELECO47770.2019.8990630, 10.23919/eleco47770.2019.8990630]
   Malla S, 2023, IEEE WINT CONF APPL, P1043, DOI 10.1109/WACV56688.2023.00110
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Min K, 2021, IEEE ACCESS, V9, P113550, DOI 10.1109/ACCESS.2021.3104276
   Mishra SK, 2021, ACM T ASIAN LOW-RESO, V20, DOI 10.1145/3432246
   Mitchell M, 2012, P 13 C EUR CHAPT ASS, P747
   Mokady Ron, 2021, arXiv, DOI DOI 10.48550/ARXIV.2111.09734
   Ordonez V., 2011, NeurIPS, V24
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Park CC, 2017, PROC CVPR IEEE, P6432, DOI 10.1109/CVPR.2017.681
   Park H, 2021, IEEE ACCESS, V9, P150560, DOI 10.1109/ACCESS.2021.3124564
   Patterson G, 2014, INT J COMPUT VISION, V108, P59, DOI 10.1007/s11263-013-0695-z
   Qu B, 2016, INT CONF COMP INFO, P124
   Rashtchian C., 2010, P NAACL HLT 2010 WOR, P139
   Selivanov A, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-31223-5
   Shao ZF, 2020, IEEE J-STARS, V13, P318, DOI 10.1109/JSTARS.2019.2961634
   Sharma H, 2023, 2023 INT C COMPUTER, P1
   Sharma H, 2023, ARTIF INTELL REV, V56, P13619, DOI 10.1007/s10462-023-10488-2
   Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556
   Shen XQ, 2020, MULTIMED TOOLS APPL, V79, P26661, DOI 10.1007/s11042-020-09294-7
   Shuster K, 2019, PROC CVPR IEEE, P12508, DOI 10.1109/CVPR.2019.01280
   Socher Richard., 2014, Trans Assoc Comput Linguist, V2, P207, DOI [DOI 10.1162/TACLA00177, DOI 10.1162/TACL_A_00177]
   Srihari K., 2022, Intelligent Data Communication Technologies and Internet of Things: Proceedings of ICICI 2021. Lecture Notes on Data Engineering and Communications Technologies (101), P59, DOI 10.1007/978-981-16-7610-9_5
   Sugano Y, 2016, Arxiv, DOI arXiv:1608.05203
   Sumbul G, 2021, IEEE T GEOSCI REMOTE, V59, P6922, DOI 10.1109/TGRS.2020.3031111
   Ushiku Y, 2015, IEEE I CONF COMP VIS, P2668, DOI 10.1109/ICCV.2015.306
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Verma Y., 2014, BMVC, V1, P2
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang BQ, 2020, IEEE J-STARS, V13, P256, DOI 10.1109/JSTARS.2019.2959208
   Wang Cheng, 2016, P 24 ACM INT C MULT, P988, DOI [DOI 10.1145/2964284.2964299, 10.1145/2964284.2964299]
   Wang Dalin, 2019, P VISION LANGUAGE IN, P29, DOI 10.18653/v1/D19-6405
   Wang Q, 2021, IEEE T GEOSCI REMOTE, V59, P10532, DOI 10.1109/TGRS.2020.3044054
   Wang QZ, 2018, Arxiv, DOI arXiv:1805.09019
   Wang S, 2022, ISPRS J PHOTOGRAMM, V184, P1, DOI 10.1016/j.isprsjprs.2021.11.020
   Wu Q, 2018, IEEE T PATTERN ANAL, V40, P1367, DOI 10.1109/TPAMI.2017.2708709
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang M, 2020, IEEE T IMAGE PROCESS, V29, P9627, DOI 10.1109/TIP.2020.3028651
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yang QQ, 2022, ISPRS J PHOTOGRAMM, V186, P190, DOI 10.1016/j.isprsjprs.2022.02.001
   Yang Y., 2011, Proceedings of the Conference on Empirical Methods in Natural Language Processing, P444
   Yatskar Mark., 2014, P 3 JOINT C LEXICAL, P110
   Ye XT, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3224244
   Yuan ZH, 2020, IEEE ACCESS, V8, P2608, DOI 10.1109/ACCESS.2019.2962195
   Zeng XH, 2020, COMPUT METH PROG BIO, V197, DOI 10.1016/j.cmpb.2020.105700
   Zhang ZY, 2019, IEEE ACCESS, V7, P137355, DOI 10.1109/ACCESS.2019.2942154
   Zhang ZZ, 2017, PROC CVPR IEEE, P3549, DOI 10.1109/CVPR.2017.378
   Zhao BG, 2021, IEEE ACCESS, V9, P154086, DOI 10.1109/ACCESS.2021.3128140
   Zhao WT, 2021, IEEE T IMAGE PROCESS, V30, P1180, DOI 10.1109/TIP.2020.3042086
   Zhao YM, 2021, IEEE ACCESS, V9, P108017, DOI 10.1109/ACCESS.2021.3093650
   Zhou JF, 2023, NEURAL COMPUT APPL, V35, P9481, DOI 10.1007/s00521-022-08072-w
   Zhou L, 2020, IEEE T IMAGE PROCESS, V29, P694, DOI 10.1109/TIP.2019.2928144
   Zohourianshahzadi Z, 2022, ARTIF INTELL REV, V55, P3833, DOI 10.1007/s10462-021-10092-2
NR 112
TC 0
Z9 0
U1 22
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 2192-6611
EI 2192-662X
J9 INT J MULTIMED INF R
JI Int. J. Multimed. Inf. Retr.
PD JUN
PY 2024
VL 13
IS 2
AR 20
DI 10.1007/s13735-024-00328-6
PG 27
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC6R2
UT WOS:001205104300001
DA 2024-08-05
ER

EF