FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Andrienko, G
   Andrienko, N
   Hecker, D
AF Andrienko, Gennady
   Andrienko, Natalia
   Hecker, Dirk
TI Topic modelling for spatial insights: Uncovering space use from movement
   data
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Visual analytics; Movement
ID OF-THE-ART; VISUAL ANALYTICS; MASS MOBILITY; VISUALIZATION; ABSTRACTION;
   REDUCTION; PATTERNS
AB We present a novel approach to understanding space use by moving entities based on repeated patterns of place visits and transitions. Our approach represents trajectories as text documents consisting of sequences of place visits or transitions and applies topic modelling to the corpus of these documents. The resulting topics represent combinations of places or transitions, respectively, that repeatedly co-occur in trips. Visualisation of the results in the spatial context reveals the regions of place connectivity through movements and the major channels used to traverse the space. This enables understanding of the use of space as a medium for movement. We compare the possibilities provided by topic modelling to alternative approaches exploiting a numeric measure of pairwise connectedness. We have extensively explored the potential of utilising topic modelling by applying our approach to multiple real-world movement data sets with different data collection procedures and varying spatial and temporal properties: GPS road traffic of cars, unconstrained movement on a football pitch, and episodic movement data reflecting social media posting events. The approach successfully demonstrated the ability to uncover meaningful patterns and interesting insights. We thoroughly discuss different aspects of the approach and share the knowledge and experience we have gained with people who might be potentially interested in analysing movement data by means of topic modelling methods.
C1 [Andrienko, Gennady; Andrienko, Natalia] City Univ London, London EC1V 0HB, England.
   [Andrienko, Gennady; Andrienko, Natalia; Hecker, Dirk] Fraunhofer Inst IAIS, D-53757 St Augustin, Germany.
C3 City University London; Fraunhofer Gesellschaft
RP Andrienko, G (corresponding author), City Univ London, London EC1V 0HB, England.
EM gennady.andrienko.1@city.ac.uk
FU Federal Ministry of Education and Research of Germany; State of
   North-Rhine Westphalia [Lamarr22B]; EU [101092749]
FX This work was supported by Federal Ministry of Education and Research of
   Germany and the state of North-Rhine Westphalia as part of the Lamarr
   Institute for Machine Learning and Artificial Intelligence (Lamarr22B) ,
   and by EU in projects SoBigData++ and CrexData (grant agreement no.
   101092749) .
CR Aggarwal CC, 2001, LECT NOTES COMPUT SC, V1973, P420
   Albalawi R, 2020, FRONT ARTIF INTELL, V3, DOI 10.3389/frai.2020.00042
   Andrienko G, 2023, EUROVIS WORKSH VIS A, DOI [10.2312/eurova.20231091, DOI 10.2312/EUROVA.20231091]
   Andrienko G., 2013, Visual Analytics of Movement, DOI DOI 10.1007/978-3-642-37583-5
   Andrienko G, 2017, IEEE T VIS COMPUT GR, V23, P2120, DOI 10.1109/TVCG.2016.2616404
   Andrienko G, 2017, IEEE T INTELL TRANSP, V18, P2232, DOI 10.1109/TITS.2017.2683539
   Andrienko N, 2023, COMPUT GRAPH FORUM, V42, DOI 10.1111/cgf.14926
   Andrienko N, 2023, COMPUT GRAPH FORUM, V42, DOI 10.1111/cgf.14845
   Andrienko N, 2021, VIS INFORM, V5, P23, DOI 10.1016/j.visinf.2020.12.002
   Andrienko N, 2020, IEEE T INTELL TRANSP, V21, P3196, DOI 10.1109/TITS.2019.2924796
   Andrienko N, 2016, INFORM VISUAL, V15, P117, DOI 10.1177/1473871615581216
   Andrienko N, 2015, ISPRS INT J GEO-INF, V4, P591, DOI 10.3390/ijgi4020591
   Andrienko N, 2012, KUNSTL INTELL, V26, P241, DOI 10.1007/s13218-012-0177-4
   Andrienko N, 2011, IEEE T VIS COMPUT GR, V17, P205, DOI 10.1109/TVCG.2010.44
   Ankerst M, 1999, SIGMOD RECORD, VOL 28, NO 2 - JUNE 1999, P49
   Ayesha S, 2020, INFORM FUSION, V59, P44, DOI 10.1016/j.inffus.2020.01.005
   BELLMAN R, 1966, SCIENCE, V153, P34, DOI 10.1126/science.153.3731.34
   Bernard J., 2012, J WSCG, V2, P97
   Bernard J, 2015, PROC SPIE, V9397, DOI 10.1117/12.2079841
   Bernard Jurgen., 2016, Proceedings of the EuroVis Workshop on Visual Analytics, P31, DOI [10.2312/eurova.20161121, DOI 10.2312/EUROVA.20161121]
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Boriah S, 2008, Proceedings of the 2008 SIAM international conference on data mining, P243, DOI DOI 10.1137/1.9781611972788.22
   Brandes U, 2005, Network Analysis: Methodological Foundations
   Cassisi C, 2012, J. Adv. Data Mining Knowl. Discov. Appl., P71, DOI [10.5772/49941, DOI 10.5772/49941]
   Chen BY, 2018, ANN AM ASSOC GEOGR, V108, P1115, DOI 10.1080/24694452.2017.1411244
   Chen L., 2009, Encyclopedia of Database Systems, P545, DOI [DOI 10.1007/978-0-387-39940-9133, 10.1007/978-0-387-39940-9_133, DOI 10.1007/978-0-387-39940-9_133]
   Chen S, 2020, IEEE T VIS COMPUT GR, V26, P2775, DOI 10.1109/TVCG.2019.2904069
   Chen TH, 2016, EMPIR SOFTW ENG, V21, P1843, DOI 10.1007/s10664-015-9402-8
   Chu D, 2014, IEEE PAC VIS SYMP, P137, DOI 10.1109/PacificVis.2014.50
   CLEVELAND WS, 1984, J AM STAT ASSOC, V79, P531, DOI 10.2307/2288400
   Demsar U, 2015, MOV ECOL, V3, DOI 10.1186/s40462-015-0032-y
   Duran BS, 2013, Springer Science & Business Media, V100, DOI [10.1007/978-3-642-46309-9, DOI 10.1007/978-3-642-46309-9]
   Dzemyda G., 2013, Multidimensional Data Visualization: Methods and Applications, V75, DOI DOI 10.1007/978-1-4419-0236-8
   Egger R, 2022, FRONT SOCIOL, V7, DOI 10.3389/fsoc.2022.886498
   El-Assady M, 2019, IEEE T VIS COMPUT GR, V25, P374, DOI 10.1109/TVCG.2018.2864769
   Espadoto M, 2021, IEEE T VIS COMPUT GR, V27, P2153, DOI 10.1109/TVCG.2019.2944182
   Fahrig L, 2003, ANNU REV ECOL EVOL S, V34, P487, DOI 10.1146/annurev.ecolsys.34.011802.132419
   Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002
   Gomez S., 2019, BUSINESS CONSUMER AN, P401, DOI [DOI 10.1007/978-3-030-06222-4_8, DOI 10.1007/978-3-030-06222-48]
   Guo DS, 2006, IEEE T VIS COMPUT GR, V12, P1461, DOI 10.1109/TVCG.2006.84
   Guo DS, 2009, IEEE T VIS COMPUT GR, V15, P1041, DOI 10.1109/TVCG.2009.143
   Huang Z, 2023, COMPUT GRAPH FORUM, V42, P539, DOI 10.1111/cgf.14859
   Irani J., 2016, International Journal of Computer Applications, V134, P9, DOI [DOI 10.5120/IJCA2016910336, 10.5120/ijca2016907841]
   Kaur S, 2021, Int J Adv Comput Sci Appl., V12, DOI [10.14569/IJACSA.2021.01208100, DOI 10.14569/IJACSA.2021.01208100]
   Khaled S, 2023, A guide to football pitch zones.
   Kindlmann P, 2008, LANDSCAPE ECOL, V23, P879, DOI 10.1007/s10980-008-9245-4
   KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565
   Lesot M.-J, 2009, International Journal of Knowledge Engineering and Soft Data Paradigms, V1, P63, DOI 10.1504/IJKESDP.2009.021985
   Li ZL, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-94300-7
   Liu HY, 2021, VIS INFORM, V5, P1, DOI 10.1016/j.visinf.2021.10.002
   Liu H, 2019, VIS INFORM, V3, P140, DOI 10.1016/j.visinf.2019.10.002
   Liu L, 2016, SPRINGERPLUS, V5, DOI 10.1186/s40064-016-3252-8
   Luo MN, 2017, AAAI CONF ARTIF INTE, P2308
   Macdonald K, 2007, Mobilities, V1, P1, DOI 10.1080/17450100601106153
   Mazimpaka JD, 2016, J SPAT INF SCI, P61, DOI 10.5311/JOSIS.2016.13.263
   Nonato LG, 2019, IEEE T VIS COMPUT GR, V25, P2650, DOI 10.1109/TVCG.2018.2846735
   Palomo C, 2016, IEEE T VIS COMPUT GR, V22, P170, DOI 10.1109/TVCG.2015.2467592
   Ramalho Brilhante Igo, 2012, Proceedings of the 2012 13th IEEE International Conference on Mobile Data Management (MDM), P268, DOI 10.1109/MDM.2012.17
   Rieck K, 2011, WIRES DATA MIN KNOWL, V1, P296, DOI 10.1002/widm.36
   Rinzivillo S, 2012, KUNSTL INTELL, V26, P253, DOI 10.1007/s13218-012-0181-8
   Robinson A.H., 1967, IMAGO MUNDI, V21, P95, DOI [10.1016/0169-328x(95)00056-x, DOI 10.1016/0169-328X(95)00056-X]
   Schwering A., 2008, T GIS, V12, P5, DOI DOI 10.1111/J.1467-9671.2008.01084.X
   Spence I, 2005, J EDUC BEHAV STAT, V30, P353, DOI 10.3102/10769986030004353
   Teitelbaum CS, 2020, MOV ECOL, V8, DOI 10.1186/s40462-020-00233-7
   TOBLER WR, 1987, AM CARTOGRAPHER, V14, P155, DOI 10.1559/152304087783875273
   Van Der Maaten L., 2009, J MACH LEARN RES, V10, P66
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vayansky I, 2020, INFORM SYST, V94, DOI 10.1016/j.is.2020.101582
   Vieira VD, 2020, APPL NETW SCI, V5, DOI 10.1007/s41109-020-00289-9
   von Landesberger T, 2016, IEEE T VIS COMPUT GR, V22, P11, DOI 10.1109/TVCG.2015.2468111
   Wallach H. M., 2006, Proceedings of the 23rd International Conference on Machine Learning, P977, DOI DOI 10.1145/1143844.1143967
   Wang S, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3440207
   Wattenberg M., 2016, Distill, V1, pe2
   Wenskovitch JE, 2019, [Ph.D. thesis]
   Wenskovitch J, 2018, IEEE T VIS COMPUT GR, V24, P131, DOI 10.1109/TVCG.2017.2745258
   Wood J, 2010, CARTOGR J, V47, P117, DOI 10.1179/000870410X12658023467367
   Yuan G, 2017, ARTIF INTELL REV, V47, P123, DOI 10.1007/s10462-016-9477-7
   Zhang PP, 2022, PHYSICA A, V586, DOI 10.1016/j.physa.2021.126438
   Zheng Y, 2015, ACM T INTEL SYST TEC, V6, DOI 10.1145/2743025
NR 79
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103989
DI 10.1016/j.cag.2024.103989
EA JUL 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YM4R5
UT WOS:001268897700001
OA hybrid
DA 2024-08-05
ER

PT J
AU Olivier, P
   Butler, T
   Guehl, P
   Coll, JL
   Chabrier, R
   Memari, P
   Cani, MP
AF Olivier, Pauline
   Butler, Tara
   Guehl, Pascal
   Coll, Jean-Luc
   Chabrier, Renaud
   Memari, Pooran
   Cani, Marie-Paule
TI DynBioSketch: A tool for sketching dynamic visual summaries in biology,
   and its application to infection phenomena
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Sketch-based modeling; Interactive geometric modeling; Sketch-based
   animation; Narration
AB Having simple methods of illustration is essential to scientific thinking. To complement the abstract sketches regularly used in cell biology, we propose DynBioSketch, an easy-to-use digital modeling and animation tool, enabling biologists to resort to less simplified representations when necessary without having to call professional artists. DynBioSketch is an interactive sketching system dedicated to the design and communication of biological phenomena at the cellular scale that can be illustrated in a few minutes of animation. Our model integrates 3D modeling, pattern-based design of 3D shape distributions, and sketch-based animation. These elements can be combined to create complex scenarios such as the infection phenomenon on which we focus, allowing a narrative design adapted to communication between researchers or educational applications in biology. Our results, along with a user study conducted with biology researchers, highlight the potential of DynBioSketch in enabling the direct design of dynamic visual summaries that convey relevant information, as shown in our infection case study. By bridging the gap between abstract representations used by experts and more illustrative depictions, DynBioSketch opens a new avenue for communicating biological concepts.
C1 [Olivier, Pauline; Butler, Tara; Guehl, Pascal; Memari, Pooran; Cani, Marie-Paule] IP Paris, Ecole Polytech, LIX, CNRS, Palaiseau, France.
   [Chabrier, Renaud] Atelier Renaud Chabrier, Strasbourg, France.
   [Coll, Jean-Luc] Univ Grenoble Alpes, Inst Adv Biosci, INSERM, CNRS, Grenoble, France.
C3 Centre National de la Recherche Scientifique (CNRS); Institut
   Polytechnique de Paris; Ecole Polytechnique; Communaute Universite
   Grenoble Alpes; Universite Grenoble Alpes (UGA); Centre National de la
   Recherche Scientifique (CNRS); Institut National de la Sante et de la
   Recherche Medicale (Inserm)
RP Olivier, P (corresponding author), IP Paris, Ecole Polytech, LIX, CNRS, Palaiseau, France.
EM paulinehnolivier@gmail.com; butler@lix.polytechnique.fr;
   guehl@lix.polytechnique.fr; jean-luc.coll@univ-grenoble-alpes.fr;
   renaud.chabrier@m4x.org; memari@lix.polytechnique.fr;
   marie-paule.cani@polytechnique.edu
FU Marie-Paule Cani's fellowship
FX We are grateful to the reviewers for their useful comments and remarks
   that helped us improve the paper. We are thankful to the researchers
   from IAB Grenoble for their time, interest in this project, and
   participation in our user study. This paper was partly funded by
   Marie-Paule Cani's fellowship on Creative AI from Hi!Paris.
CR Abdrashitov R, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459769
   Barla P, 2006, COMPUT GRAPH FORUM, V25, P663, DOI 10.1111/j.1467-8659.2006.00986.x
   BERNARDES A., 2008, Sisifo/Revista de Ciencias da Educacao., P57, DOI DOI 10.2312/SBM/SBM08/057-064
   Cakmak E, 2022, IEEE T VIS COMPUT GR, V28, P4918, DOI 10.1109/TVCG.2021.3109387
   Chabrier R., 2018, Globule, the magazine of all cells
   Chen Q, 2024, IEEE T VIS COMPUT GR, V30, P4429, DOI 10.1109/TVCG.2023.3261320
   Galin E, 1996, COMPUT GRAPH FORUM, V15, pC143, DOI 10.1111/1467-8659.1530143
   Gamara J, 2015, J IMMUNOL RES, V2015, DOI 10.1155/2015/235170
   Gardner A, 2021, FRONT BIOINFORM, V1, DOI 10.3389/fbinf.2021.660936
   Gardner A, 2018, IEEE COMPUT GRAPH, V38, P51, DOI 10.1109/MCG.2018.2877076
   Garrison L, 2021, VCBM, P1, DOI DOI 10.2312/VCBM.20211339
   Garrison LA, 2023, Approaches for science illustration and communication, P95, DOI [10.1007/978-3-031-41652-14, DOI 10.1007/978-3-031-41652-14]
   Garrison LA, 2023, IEEE COMPUT GRAPH, V43, P94, DOI 10.1109/MCG.2023.3250680
   Gershon N, 2001, COMMUN ACM, V44, P31, DOI 10.1145/381641.381653
   Goodsell D.S., 2009, The machinery of life, V2nd, DOI [10.1007/978-0-387-84925-6, DOI 10.1007/978-0-387-84925-6]
   Hurtut T, 2009, P 7 INT S NONPH AN R, P51, DOI [DOI 10.1145/1572614.1572623, 10.1145/1572614.1572623]
   Ijiri T, 2008, COMPUT GRAPH FORUM, V27, P429, DOI 10.1111/j.1467-8659.2008.01140.x
   Isenberg T, 2015, MATH VIS, P235, DOI 10.1007/978-3-319-15090-1_12
   Kazi RH, 2014, Proceedings of the 27th annual ACM symposium on User interface software and technology UIST '14, P395, DOI [DOI 10.1145/2642918.2647375, 10.1145/2642918.2647375]
   Kazi RH, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P351, DOI 10.1145/2556288.2556987
   Kleinau A, 2022, EUR WORKSH VIS COMP, DOI [10.2312/vcbm.20221183, DOI 10.2312/VCBM.20221183]
   Kouril D, 2023, IEEE T VIS COMPUT GR, V29, P1733, DOI 10.1109/TVCG.2021.3130670
   Landes PE, 2013, COMPUT GRAPH FORUM, V32, P67, DOI 10.1111/cgf.12152
   Lawonn K, 2018, COMPUT GRAPH FORUM, V37, P205, DOI 10.1111/cgf.13322
   Ma CY, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964957
   McGill G, 2008, CELL, V133, P1127, DOI 10.1016/j.cell.2008.06.013
   Meuschke M, 2017, COMPUT GRAPH FORUM, V36, P99, DOI 10.1111/cgf.13171
   Meuschke M, 2021, Arxiv, DOI arXiv:2108.05462
   Meuschke M, 2022, COMPUT GRAPH-UK, V107, P144, DOI 10.1016/j.cag.2022.07.017
   Milliez A, 2016, COMPUT GRAPH FORUM, V35, P67, DOI 10.1111/cgf.13004
   Milliez A, 2014, P WORKSH NONPH AN RE, P71, DOI [10.1145/2630397.2630402, DOI 10.1145/2630397.2630402]
   Mörth E, 2023, IEEE T VIS COMPUT GR, V29, P5165, DOI 10.1109/TVCG.2022.3205769
   Olivier P., 2022, P GRAPHICS INTERFACE, P161, DOI [10.20380/GI2022.17, DOI 10.20380/GI2022.17]
   Olivier P, 2023, EUR WORKSH VIS COMP, P63, DOI [10.2312/vcbm.20231214, DOI 10.2312/VCBM.20231214]
   Pihuit A., 2010, Eurographics Workshop on Sketch-Based Interfaces and Modeling (SBIM), P151, DOI [10.2312/SBM/SBM10/151-158, DOI 10.2312/SBM/SBM10/151-158]
   Preim B, 2018, COMPUT GRAPH-UK, V71, P132, DOI 10.1016/j.cag.2018.01.005
   Reddy P, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417830
   Roveri R, 2015, COMPUT GRAPH FORUM, V34, P39, DOI 10.1111/cgf.12695
   Saalfeld A., 2016, P EUR WORKSH VIS COM, P123, DOI [10.2312/vcbm.20161280, DOI 10.2312/VCBM.20161280, DOI 10.5555/3061507.3061528]
   Saalfeld P, 2016, COMM COM INF SC, V598, P19, DOI 10.1007/978-3-319-29971-6_2
   Sendik O, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3015461
   Stylianopoulos T, 2018, TRENDS CANCER, V4, P292, DOI 10.1016/j.trecan.2018.02.005
   Tong C, 2018, INFORMATION, V9, DOI 10.3390/info9030065
   Tu PH, 2019, COMPUT GRAPH FORUM, V38, P109, DOI 10.1111/cgf.13793
   Viola I, 2005, EUROGRAPHICS 2005 TU, DOI [10.2312/egt.20051052, DOI 10.2312/EGT.20051052]
   Wei L. Y., 2009, EUROGRAPHICS 2009 ST, P93, DOI DOI 10.2312/EGST.20091063
   Whitby M., 2003, ACM SIGGRApH 2003 video review on electronic theater program on electronic theater program, V144, P4, DOI [10.1145/1006032.1006036, DOI 10.1145/1006032.1006036]
   Wither J, 2009, COMPUT GRAPH FORUM, V28, P541, DOI 10.1111/j.1467-8659.2009.01394.x
   Witko-Sarsat V, 2000, LAB INVEST, V80, P617, DOI 10.1038/labinvest.3780067
   Xing J, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P755, DOI 10.1145/2984511.2984585
   Zanni C, 2013, COMPUT GRAPH FORUM, V32, P219, DOI 10.1111/cgf.12199
   Zhu B, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024168
NR 52
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103956
DI 10.1016/j.cag.2024.103956
EA JUL 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XX3G8
UT WOS:001264930700001
DA 2024-08-05
ER

PT J
AU Kosalaraman, KK
   Kendre, PP
   Manilal, RD
   Muthuganapathy, R
AF Kosalaraman, Kamalesh Kumar
   Kendre, Prasad Pralhad
   Manilal, Raghwani Dhaval
   Muthuganapathy, Ramanathan
TI SketchCleanGAN: A generative network to enhance and correct query
   sketches for improving 3D CAD model retrieval systems
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE 3D CAD models; Search and retrieval; Generative adversarial networks
   (GANs); Sketch completion; Sketch inpainting; CAD model sketches
ID IMAGE
AB Given an input query, a search and retrieval system fetches relevant information from a dataset. In the Engineering domain, such a system is beneficial for tasks such as design reuse. A two-dimensional (2D) sketch is more conducive for an end user to give as a query than a three-dimensional (3D) object. Such query sketches, nevertheless, will inevitably contain defects like incomplete lines, mesh lines, overdrawn areas, missing areas, etc. Since a retrieval system's results are only as good as the query, it is necessary to improve the query sketches. In this paper, the problem of transforming a defective CAD sketch into a defect-free sketch is addressed using Generative Adversarial Networks (GANs), which, to the best of our knowledge, has not been investigated before. We first create a dataset of 534 hand-drawn sketches by tracing the boundaries of images of CAD models. We then pair the corrected sketches with their corresponding defective sketches and use them for training a C-WGAN (Conditional Wasserstein Generative Adversarial Network), called SketchCleanGAN. We model the transformation from defective to defect-free sketch as a factorization of the defective input sketch and then translate it to the space of defect-free sketch. We propose a three-branch strategy to this problem. Ablation studies and comparisons with other state-of-the-art techniques demonstrate the efficacy of the proposed technique. Additionally, we also contribute to a dataset of around 58000 improved sketches using the proposed framework.
C1 [Kosalaraman, Kamalesh Kumar; Kendre, Prasad Pralhad; Manilal, Raghwani Dhaval; Muthuganapathy, Ramanathan] Indian Inst Technol Madras, Dept Engn Design, Adv Geometr Comp Lab, Chennai, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Madras
RP Muthuganapathy, R (corresponding author), Indian Inst Technol Madras, Dept Engn Design, Adv Geometr Comp Lab, Chennai, India.
EM kamalvidya.2002@gmail.com; prasadkendre20@gmail.com;
   raghwanidhaval19@gmail.com; emry01@gmail.com
CR Arjovsky M, 2017, PR MACH LEARN RES, V70
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Cao L, 2006, P 14 ACM INT C MULT, P105, DOI [10.1145/1180639.1180671, DOI 10.1145/1180639.1180671]
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   DeCarlo D, 2003, ACM T GRAPHIC, V22, P848, DOI 10.1145/882262.882354
   Dunford R, 2021, The Race
   Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185527
   FLICKNER M, 1995, COMPUTER, V28, P23, DOI 10.1109/2.410146
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gryaditskaya Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356533
   Ha D, 2018, 6 INT C LEARN REPR I
   He JZ, 2022, IEEE T PATTERN ANAL, V44, P100, DOI 10.1109/TPAMI.2020.3007074
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jayanti S, 2006, COMPUT AIDED DESIGN, V38, P939, DOI 10.1016/j.cad.2006.06.007
   Kazhdan M., 2003, Symposium on Geometry Processing, P156
   Kendre PP, 2023, COMPUT GRAPH-UK, V115, P55, DOI 10.1016/j.cag.2023.06.028
   Kingma D. P., 2014, arXiv
   Lee J., 2008, P EUROGRAPHICS WORKS, P97, DOI DOI 10.2312/SBM/SBM08/097-104
   Li B, 2014, COMPUT VIS IMAGE UND, V119, P57, DOI 10.1016/j.cviu.2013.11.008
   Liu CX, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592130
   Liu CX, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201314
   Liu DF, 2020, PROC CVPR IEEE, P5427, DOI 10.1109/CVPR42600.2020.00547
   Liu MY, 2016, ADV NEUR IN, V29
   Liu MY, 2017, ADV NEUR IN, V30
   Liu XT, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818067
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Manda B, 2022, Comput Graph
   Manda B, 2021, Comput Graph
   Manda B, 2021, IEEE ACCESS, V9, P22711, DOI 10.1109/ACCESS.2021.3055826
   Ogawa T, 2016, INT C PATT RECOG, P1065, DOI 10.1109/ICPR.2016.7899777
   Pang KY, 2018, LECT NOTES COMPUT SC, V11219, P37, DOI 10.1007/978-3-030-01267-0_3
   Parakkat AD, 2018, COMPUT GRAPH-UK, V74, P171, DOI 10.1016/j.cag.2018.05.011
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sahillioglu Y, 2017, IEEE COMPUT GRAPH, V37, P88, DOI 10.1109/MCG.2017.4031063
   Sangpil Kim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P175, DOI 10.1007/978-3-030-58523-5_11
   Sasaki K, 2018, VISUAL COMPUT, V34, P1077, DOI 10.1007/s00371-018-1528-4
   Seo CW, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592392
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Simo-Serra E, 2016, ACM T GRAPH, V35, P1, DOI DOI 10.1145/2897824.2925972
   Simo-Serra E, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3132703
   Simo-Serra E, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925972
   Soria X, 2023, PATTERN RECOGN, V139, DOI 10.1016/j.patcog.2023.109461
   STRICKER M, 1995, P SOC PHOTO-OPT INS, V2410, P381, DOI 10.1117/12.205308
   Wang F, 2015, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2015.7298797
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1007/s11263-017-1004-z, 10.1109/ICCV.2015.164]
   Yan C, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417784
   YOON S.M., 2010, Proceedings of the international conference on Multimedia, P193, DOI [10.1145/1873951.1873961, DOI 10.1145/1873951.1873961]
   Yu Deng, 2024, IEEE Trans Vis Comput Graph, V30, P6533, DOI 10.1109/TVCG.2023.3346995
   Zhang H, 2019, Proceedings of ICML, P7354, DOI DOI 10.48550/ARXIV.1805.08318
   Zhong Y, 2021, IEEE T CIRC SYST VID, V31, P3518, DOI 10.1109/TCSVT.2020.3040900
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 60
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD OCT
PY 2024
VL 123
AR 104000
DI 10.1016/j.cag.2024.104000
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A0Z0O
UT WOS:001279894100001
DA 2024-08-05
ER

PT J
AU Yao, ZH
   Chen, YH
   Cui, JH
   Zhang, SL
   Li, S
   Hao, AM
AF Yao, Zhihan
   Chen, Yuhang
   Cui, Jiahao
   Zhang, Shoulong
   Li, Shuai
   Hao, Aimin
TI Special Section on SMI 2024 Conditional room layout generation based on
   graph neural networks
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Scene synthesis; Conditional generation; Latent representation; Graph
   neural network
AB In this paper, we present a novel end -to -end variational generative model that utilizes graph -based latent representations for indoor scene synthesis at one time. In contrast to prior research, our method deviates from the practice of gradually introducing and arranging furniture in an empty room using autoregression. Instead, it focuses on acquiring a comprehensive implicit representation of the room's original architectural structure and the placement of furniture. We initially transform the 3D room scene into a dense scene graph, where nodes correspond to the objects present in the room while edges reflect the spatial location links and functional correlations between objects. Then, a neural network is trained to acquire the graph -based latent representation of the room scene through iterative message passing, ultimately resulting in the acquisition of the data distribution on the latent space of the room layout. Given the architectural structure of an empty room as a prerequisite for scene synthesis, the generative model has the ability to sample from the prior distribution of the room's latent representation. This allows the model to then decode and generate a variety of room layouts. We evaluate our method with the state-of-the-art 3D indoor scene dataset and generation methods. The experimental results demonstrate that our method achieves more rational and diverse outcomes in the context of generating scenes under specific conditions.
C1 [Yao, Zhihan; Chen, Yuhang; Cui, Jiahao; Li, Shuai; Hao, Aimin] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Zhang, Shoulong] Zhongguancun Lab, Beijing, Peoples R China.
C3 Beihang University; Zhongguancun Laboratory
RP Cui, JH (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
EM loeyyzh@163.com; by2106129@buaa.edu.cn; cuijh@buaa.edu.cn;
   zhangsl@zgclab.edu.cn; lishuai@buaa.edu.cn; ham@buaa.edu.cn
FU National Key Research and Development Program of China [2023YFF1203803];
   National Natural Science Foundation of China [62272021]; Beijing
   Sci-ence and Technology Plan Project [Z231100005923039]
FX This research is partially supported by the National Key Research and
   Development Program of China No. 2023YFF1203803, National Natural
   Science Foundation of China No. 62272021, and Beijing Sci-ence and
   Technology Plan Project No. Z231100005923039.
CR Chang Angel., 2014, P 2014 C EMP METH NA, P2028, DOI [DOI 10.3115/V1/D14-1217, 10.3115/v1/d14-1217, 10.3115/v1/D14-1217.]
   Cho KYHY, 2014, Arxiv, DOI arXiv:1409.1259
   Dhamo H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16332, DOI 10.1109/ICCV48922.2021.01604
   Fisher M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818057
   Fisher M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366154
   Fu H, 2021, P IEEE CVF INT C COM, P10933
   Fu H, 2021, INT J COMPUT VISION, V129, P3313, DOI 10.1007/s11263-021-01534-z
   Gao L, 2023, IEEE T PATTERN ANAL, V45, P8902, DOI 10.1109/TPAMI.2023.3237577
   Gilmer J, 2017, PR MACH LEARN RES, V70
   Hensel M, 2017, ADV NEUR IN, V30
   Hyo Jong Shin, 2007, Proceedings Graphics Interface 2007, P63, DOI 10.1145/1268517.1268530
   Jyothi AA, 2019, IEEE I CONF COMP VIS, P9894, DOI 10.1109/ICCV.2019.00999
   Lei JB, 2023, PROC CVPR IEEE, P8422, DOI 10.1109/CVPR52729.2023.00814
   Li MY, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3303766
   Li YJ, 2017, Arxiv, DOI arXiv:1511.05493
   Liu LJ, 2021, AAAI CONF ARTIF INTE, V35, P336
   Luo A, 2020, PROC CVPR IEEE, P3753, DOI 10.1109/CVPR42600.2020.00381
   Ma R, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275035
   Merrell P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964982
   Merrell P, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866203
   Kipf TN, 2016, Arxiv, DOI arXiv:1611.07308
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Parmar G, 2022, Arxiv, DOI arXiv:2104.11222
   Paschalidou D, 2021, Advances in Neural Information Processing Systems, V34, P12013
   Puig X, 2023, IEEE INT CONF ROBOT, P7628, DOI 10.1109/ICRA48891.2023.10161352
   Ritchie D, 2019, PROC CVPR IEEE, P6175, DOI 10.1109/CVPR.2019.00634
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Solah M, 2022, IEEE T VIS COMPUT GR, V28, P2058, DOI 10.1109/TVCG.2022.3150513
   Song Liangchen, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P6898, DOI 10.1145/3581783.3611800
   Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28
   Tahara T, 2020, ADJUNCT PROCEEDINGS OF THE 2020 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT 2020), P249, DOI 10.1109/ISMAR-Adjunct51615.2020.00072
   Tang J., 2024, P IEEE CVF C COMP VI, P1
   Wang K, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322941
   Wang K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201362
   Wang XP, 2021, INT CONF 3D VISION, P106, DOI 10.1109/3DV53792.2021.00021
   Xu K, 2002, PROC GRAPH INTERF, P25
   Xu K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461968
   Yang HT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5610, DOI 10.1109/ICCV48922.2021.00558
   Yu LF, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964981
   Zhang S, 2023, Propose-and-complete: Auto-regressive semantic group generation for personalized scene synthesis
   Zhang S, 2021, Adv Neural Inf Process Syst, V34, P18620
   Zhang SL, 2021, AAAI CONF ARTIF INTE, V35, P3385
   Zhang YD, 2017, PROC CVPR IEEE, P5057, DOI 10.1109/CVPR.2017.537
   Zhang ZW, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3381866
   Zhao Kaifeng, 2023, P IEEECVF INT C COMP, P14738
   Zhou Y, 2019, IEEE I CONF COMP VIS, P7383, DOI 10.1109/ICCV.2019.00748
NR 46
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103971
DI 10.1016/j.cag.2024.103971
EA JUN 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XH8F2
UT WOS:001260875900001
DA 2024-08-05
ER

PT J
AU Chheang, V
   Schott, D
   Saalfeld, P
   Vradelis, L
   Huber, T
   Huettl, F
   Lang, HK
   Preim, B
   Hansen, C
AF Chheang, Vuthea
   Schott, Danny
   Saalfeld, Patrick
   Vradelis, Lukas
   Huber, Tobias
   Huettl, Florentine
   Lang, Hauke
   Preim, Bernhard
   Hansen, Christian
TI Advanced liver surgery training in collaborative VR environments
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Virtual reality; Collaborative VR; Medical training; Liver surgery
   planning; Laparoscopic surgery training
ID VIRTUAL-REALITY; SIMULATION; ARCHITECTURE; EDUCATION
AB Virtual surgical training systems are crucial for enabling mental preparation, supporting decision -making, and improving surgical skills. Many virtual surgical training environments focus only on training for a specific medical skill and take place in a single virtual room. However, surgical education and training include the planning of procedures as well as interventions in the operating room context. Moreover, collaboration among surgeons and other medical professionals is only applicable to a limited extent. This work presents a collaborative VR environment similar to a virtual teaching hospital to support surgical training and interprofessional collaboration in a co -located or remote environment. The environment supports photorealistic avatars and scenarios ranging from planning to training procedures in the virtual operating room. It includes a lobby, a virtual surgical planning room with four surgical planning stations, laparoscopic liver surgery training with the integration of laparoscopic surgical instruments, and medical training scenarios for interprofessional team training in a virtual operating room. Each component was evaluated by domain experts as well as in a series of user studies, providing insights on usability, usefulness, and potential research directions. The proposed environment may serve as a foundation for future medical training simulators.
C1 [Chheang, Vuthea] Lawrence Livermore Natl Lab, Ctr Appl Sci Comp, Livermore, CA 94550 USA.
   [Schott, Danny; Saalfeld, Patrick; Preim, Bernhard; Hansen, Christian] Univ Magdeburg, Fac Comp Sci & Res, Campus STIMULATE, Magdeburg, Germany.
   [Vradelis, Lukas; Huber, Tobias; Huettl, Florentine; Lang, Hauke] Johannes Gutenberg Univ Mainz, Univ Med Ctr, Dept Gen Visceral & Transplant Surg, Mainz, Germany.
   [Chheang, Vuthea] Univ Magdeburg, Magdeburg, Germany.
C3 United States Department of Energy (DOE); Lawrence Livermore National
   Laboratory; Otto von Guericke University; Johannes Gutenberg University
   of Mainz; Otto von Guericke University
RP Chheang, V (corresponding author), Lawrence Livermore Natl Lab, Ctr Appl Sci Comp, Livermore, CA 94550 USA.; Chheang, V (corresponding author), Univ Magdeburg, Magdeburg, Germany.
EM chheang1@llnl.gov
RI Chheang, Vuthea/GWR-2087-2022
OI Chheang, Vuthea/0000-0001-5999-4968
FU U.S. Department of Energy by Lawrence Livermore National Laboratory
   [DE-AC52-07NA27344]; Federal Ministry of Education and Research (BMBF),
   Germany [16SV8054]
FX This work was performed under the auspices of the U.S. Department of
   Energy by Lawrence Livermore National Laboratory under Contract
   DE-AC52-07NA27344, and the study was funded by the Federal Ministry of
   Education and Research (BMBF), Germany under grant number 16SV8054.
CR Alaker M, 2016, INT J SURG, V29, P85, DOI 10.1016/j.ijsu.2016.03.034
   Allgaier M, 2023, INT J COMPUT ASS RAD, V18, P2013, DOI 10.1007/s11548-023-02851-z
   Allgaier M, 2022, COMPUT BIOL MED, V145, DOI 10.1016/j.compbiomed.2022.105429
   Alsofy SZ, 2020, J CRANIOFAC SURG, V31, P1865, DOI 10.1097/SCS.0000000000006525
   Bari H, 2021, WORLD J GASTRO SURG, V13, P7, DOI 10.4240/wjgs.v13.i1.7
   Bashkanov O, 2019, ANN M GERM SOC COMP, P264
   Boedecker C, 2021, LANGENBECK ARCH SURG, V406, P911, DOI 10.1007/s00423-021-02127-7
   Bowman DA, 2007, COMPUTER, V40, P36, DOI 10.1109/MC.2007.257
   Bray A., 2019, SN COMPR CLIN MED, V1, P362, DOI DOI 10.1007/S42399-019-00053-W
   Brooke J, 2013, J USABILITY STUD, V8, P29
   Brun H, 2019, EUR HEART J-CARD IMG, V20, P883, DOI 10.1093/ehjci/jey184
   Brunges M, 2020, ASSOC OPER ROOM NURS, V111, P617, DOI 10.1002/aorn.13046
   Buhr M, 2022, Virtual and augmented reality (VR/aR), P245
   Cecil J, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3232678
   Chheang Vuthea, 2022, 2022 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR), P82, DOI 10.1109/AIVR56993.2022.00018
   Chheang V, 2022, Ph.D. thesis
   Chheang V, 2023, 2023 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS, VRW, P166, DOI 10.1109/VRW58643.2023.00041
   Chheang V, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P401, DOI 10.1109/VRW55335.2022.00089
   Chheang V, 2021, COMPUT GRAPH-UK, V99, P234, DOI 10.1016/j.cag.2021.07.009
   Chheang V, 2020, INT J COMPUT ASS RAD, V15, P2109, DOI 10.1007/s11548-020-02276-y
   Chheang V, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR), P1, DOI 10.1109/AIVR46125.2019.00011
   Cordar A, 2017, P IEEE VIRT REAL ANN, P148, DOI 10.1109/VR.2017.7892242
   Desselle MR, 2020, COMPUT SCI ENG, V22, P18, DOI 10.1109/MCSE.2020.2972822
   Diaz C, 2015, PRESENCE-TELEOP VIRT, V23, P393, DOI 10.1162/PRES_a_00208
   Dubosc C, 2021, COMPUT GRAPH-UK, V101, P82, DOI 10.1016/j.cag.2021.08.011
   Elbamby MS, 2018, IEEE NETWORK, V32, P78, DOI 10.1109/MNET.2018.1700268
   Faure F, 2012, SOFA: a multi-model framework for interactive physical simulation, P283, DOI [DOI 10.1007/8415_2012_125, DOI 10.1007/84152012125]
   Fischer Roland, 2020, Virtual Reality and Augmented Reality. 17th EuroVR International Conference, EuroVR 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12499), P178, DOI 10.1007/978-3-030-62655-6_11
   Ganni S, 2020, INDIAN J SURG, V82, P810, DOI 10.1007/s12262-020-02131-z
   Hansen C, 2014, INT J COMPUT ASS RAD, V9, P473, DOI 10.1007/s11548-013-0937-0
   Hansen C, 2013, INT J COMPUT ASS RAD, V8, P419, DOI 10.1007/s11548-012-0790-6
   Hattab G, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-92536-x
   He ZY, 2020, INT SYM MIX AUGMENT, P542, DOI 10.1109/ISMAR50242.2020.00082
   Hongjie Zeng, 2021, ICBIP '21: 2021 6th International Conference on Biomedical Signal and Image Processing, P14, DOI 10.1145/3484424.3484427
   Huber T, 2018, SURG INNOV, V25, P280, DOI 10.1177/1553350618761756
   Huber T, 2018, INT J COMPUT ASS RAD, V13, P281, DOI 10.1007/s11548-017-1686-2
   Huber T, 2017, SURG ENDOSC, V31, P4472, DOI 10.1007/s00464-017-5500-6
   Hudson Matthew., 2014, Interacting with Presence: HCI and the Sense of Presence in Computer-mediated Environments, V83, P78
   Huettl F, 2021, ANN TRANSL MED, V9, DOI 10.21037/atm-21-512
   Hutchinson L, 1999, BRIT MED J, V318, P1267
   Ivaschenko A, 2019, SMART INNOV SYST TEC, V107, P361, DOI 10.1007/978-981-13-1747-7_34
   Kenngott HG, 2022, SURG ENDOSC, V36, P126, DOI 10.1007/s00464-020-08246-4
   Kockro RA, 2007, NEUROSURGERY, V61, P379, DOI 10.1227/01.neu.0000303997.12645.26
   Konrad-Verse Olaf., 2004, P SIMULATION VISUALI, P203
   Kumar RP, 2020, J Biomed Inform: X
   Kurul R, 2020, ANAT SCI EDUC, V13, P648, DOI 10.1002/ase.1959
   Lamata P, 2010, SURG ENDOSC, V24, P2327, DOI 10.1007/s00464-010-0915-3
   Lang HK, 2020, ANN SURG, V271, pE8, DOI 10.1097/SLA.0000000000003601
   Laskay NMB, 2023, WORLD J SURG, V47, P2367, DOI 10.1007/s00268-023-07064-8
   Lee SK, 2021, INT J HUM-COMPUT ST, V150, DOI 10.1016/j.ijhcs.2021.102608
   Li M, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P566, DOI [10.1109/VR46266.2020.00-26, 10.1109/VR46266.2020.1581301697128]
   Liaw SY, 2018, NURS EDUC TODAY, V65, P136, DOI 10.1016/j.nedt.2018.01.006
   Liu RC, 2021, SOFTWARE PRACT EXPER, V51, P1080, DOI 10.1002/spe.2935
   Lonauer P, 2021, PROCEDIA COMPUT SCI, V180, P190, DOI 10.1016/j.procs.2021.01.156
   Lungu AJ, 2021, EXPERT REV MED DEVIC, V18, P47, DOI 10.1080/17434440.2021.1860750
   Marín-Conesa E, 2021, FRONT SURG, V8, DOI 10.3389/fsurg.2021.643611
   Minge M., 2016, The meCUE Questionnaire: A Modular Tool for Measuring User Experience, V486, P115, DOI DOI 10.1007/978-3-319-41685-4_11
   Mönch J, 2013, INT J COMPUT ASS RAD, V8, P809, DOI 10.1007/s11548-013-0812-z
   Moore J, 2023, FRONT VIRTUAL REAL, V4, DOI 10.3389/frvir.2023.1130156
   Muender T, 2022, EXTENDED ABSTRACTS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2022, DOI 10.1145/3491101.3519715
   Negrillo-Cárdenas J, 2020, COMPUT METH PROG BIO, V191, DOI 10.1016/j.cmpb.2020.105407
   Paiva PVF, 2018, COMPUT ENTERTAIN, V16, DOI 10.1145/3177747
   Paiva PV, 2017, SBC J Interact Syst, V8, P89
   Pan JJ, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P548, DOI [10.1109/VR46266.2020.00-28, 10.1109/VR46266.2020.1580817835575]
   Paulus CJ, 2017, INT J COMPUT ASS RAD, V12, P461, DOI 10.1007/s11548-016-1502-4
   Pelanis E, 2020, MINIM INVASIV THER, V29, P154, DOI 10.1080/13645706.2019.1616558
   Prasolova-Forland E, 2017, INT C SMART ED SMART, P191
   Preim B, 2018, COMPUT GRAPH-UK, V71, P132, DOI 10.1016/j.cag.2018.01.005
   Qian K, 2021, COMPUT ANIMAT VIRT W, V32, DOI 10.1002/cav.1986
   Qian Kun., 2015, Proceedings of the 21st ACM Symposium on Virtual Reality Software and Technology, P69, DOI DOI 10.1145/2821592.2821599
   Reinschluessel AV, 2022, FRONT SURG, V9, DOI 10.3389/fsurg.2022.821060
   Reitinger B, 2006, IEEE COMPUT GRAPH, V26, P36, DOI 10.1109/MCG.2006.131
   Richardson A, 2011, ANAT SCI EDUC, V4, P39, DOI 10.1002/ase.195
   Saalfeld P, 2020, EUR WORKSH VIS COMP, P55
   Saito Y, 2020, ANN SURG, V271, pE4, DOI 10.1097/SLA.0000000000003552
   Sanchez-Margallo J.A., 2021, FRONT VIRTUAL REAL, V2, P144
   Sankaran NK, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P664, DOI 10.1109/vr.2019.8798089
   Schott D, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P296, DOI 10.1109/VR50410.2021.00052
   Schubert T, 2001, PRESENCE-VIRTUAL AUG, V10, P266, DOI 10.1162/105474601300343603
   Selle D, 2002, IEEE T MED IMAGING, V21, P1344, DOI 10.1109/TMI.2002.801166
   Sinatra AM, 2021, COMPUT HUM BEHAV, V114, DOI 10.1016/j.chb.2020.106562
   Singhal S., 1999, Networked Virtual Environments
   Sujka JA, 2018, J SURG EDUC, V75, P1351, DOI 10.1016/j.jsurg.2018.01.012
   Taba JV, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0252609
   Tokuyasu T, 2021, SURG ENDOSC, V35, P1651, DOI 10.1007/s00464-020-07548-x
   Volonte M, 2021, J MULTIMODAL USER IN, V15, P109, DOI 10.1007/s12193-020-00341-z
   Wang P, 2021, ROBOT CIM-INT MANUF, V72, DOI 10.1016/j.rcim.2020.102071
   Weissker T, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P363, DOI 10.1109/VRW52623.2021.00073
   Yiannakopoulou E, 2015, INT J SURG, V13, P60, DOI 10.1016/j.ijsu.2014.11.014
   Zhang JL, 2018, PROCEEDINGS CVMP 2018: THE 15TH ACM SIGGRAPH EUROPEAN CONFERENCE ON VISUAL MEDIA PRODUCTION, DOI 10.1145/3278471.3278474
NR 90
TC 0
Z9 0
U1 4
U2 4
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103879
DI 10.1016/j.cag.2024.01.006
EA FEB 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LK5H7
UT WOS:001186702600001
OA hybrid
DA 2024-08-05
ER

PT J
AU Black, D
   Nogami, M
   Salcudean, S
AF Black, David
   Nogami, Mika
   Salcudean, Septimiu
TI Mixed reality human teleoperation with device-agnostic remote
   ultrasound: Communication and user interaction
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Human computer interaction; Mixed reality; Augmented reality;
   Teleoperation; Tele-ultrasound
ID AUGMENTED REALITY; GUIDANCE; TRANSPARENCY; MARKERS; SYSTEMS; DESIGN
AB For many applications, remote guidance and telerobotics provide great advantages. For example, teleultrasound can bring much -needed expert healthcare to isolated communities. However, existing tele-guidance methods have serious limitations including either low precision for video conference -based systems, or high complexity and cost for telerobotics. A new concept called human teleoperation leverages mixed reality, haptics, and high-speed communication to provide tele-guidance that gives an expert nearly -direct remote control without requiring a robot. This paper provides an overview of the human teleoperation concept and its application to tele-ultrasound. The concept and its impact are discussed. A new approach to remote streaming and control of point -of -care ultrasound systems independent of their manufacturer is described, as is a highspeed communication system for the HoloLens 2 that is compatible with ResearchMode API sensor stream access. Details of these systems are shown in supplementary video demonstrations. Novel interaction methods enabled by HoloLens 2 -based pose tracking are also introduced and tests of the communication and user interaction are presented. The results show continued improvement of the system compared to previous work in instrumentation, HCI, and communication. The system thus has good potential for tele-ultrasound, as well as possible other applications of human teleoperation including remote maintenance, inspection, and training. The remote ultrasound streaming and control application is made available open source.
C1 [Black, David; Nogami, Mika; Salcudean, Septimiu] Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC, Canada.
C3 University of British Columbia
RP Black, D (corresponding author), Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC, Canada.
EM dgblack@ece.ubc.ca; mikarei@student.ubc.ca; tims@ece.ubc.ca
OI Black, David/0000-0001-6907-9851
FU Vanier Canada Graduate Scholarships program, Canada; NSERC, Canada;
   Charles Laszlo Chair in Biomedical Engineering, Canada; Rogers
   Communications, Canada; MITACS, Canada
FX We gratefully acknowledge scholarship support from the Vanier Canada
   Graduate Scholarships program, Canada, infrastructure support from CFI
   and funding support from NSERC, Canada and the Charles Lazlo Chair in
   Biomedical Engineering, Canada, as well as infrastructure, technical,
   and funding support from Rogers Communications, Canada and MITACS,
   Canada.
CR Akbari M, 2021, FRONT ROBOT AI, V8, DOI 10.3389/frobt.2021.645424
   Ameri G, 2018, INT J COMPUT ASS RAD, V13, P495, DOI 10.1007/s11548-017-1665-7
   [Anonymous], 2007, RFC 4960: Stream control transmission protocol
   [Anonymous], 1996, RFC 1889: RTP: A transport protocol for real-time applications
   BAJURA M, 1992, COMP GRAPH, V26, P203, DOI 10.1145/142920.134061
   Black D., 2022, P HAMLS MED ROB, P91
   Black D, 2023, arXiv
   Black D, 2022, TechRxiv, DOI [10.36227/techrxiv.21432009.v1, DOI 10.36227/TECHRXIV.21432009.V1]
   Black D, 2023, TechRxiv
   Black D, 2023, World Haptics Confer, P333, DOI 10.1109/WHC56415.2023.10224473
   Black D, 2023, HUM-COMPUT INTERACT, DOI 10.1080/07370024.2023.2218355
   Black D, 2023, 2023 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS, VRW, P375, DOI 10.1109/VRW58643.2023.00083
   Black D, 2023, INT J COMPUT ASS RAD, V18, P1811, DOI 10.1007/s11548-023-02896-0
   Carmigniani J, 2011, HANDBOOK OF AUGMENTED REALITY, P3, DOI 10.1007/978-1-4614-0064-6_1
   Cartucho J, 2020, INT J COMPUT ASS RAD, V15, P819, DOI 10.1007/s11548-020-02165-4
   Daniel RW, 1998, INT J ROBOT RES, V17, P811, DOI 10.1177/027836499801700801
   Delgorge C, 2005, IEEE T INF TECHNOL B, V9, P50, DOI 10.1109/TITB.2004.840062
   Drake AE, 2021, ULTRASOUND J, V13, DOI 10.1186/s13089-021-00210-0
   Gajarawala SN, 2021, JNP-J NURSE PRACT, V17, P218, DOI 10.1016/j.nurpra.2020.09.013
   Garrido-Jurado S, 2014, PATTERN RECOGN, V47, P2280, DOI 10.1016/j.patcog.2014.01.005
   Gilbertson MW, 2013, IEEE ENG MED BIO, P140, DOI 10.1109/EMBC.2013.6609457
   Groves L, 2022, J IMAGING, V8, DOI 10.3390/jimaging8010007
   Hannaford B., 1988, Proceedings of the 1988 IEEE International Conference on Robotics and Automation (Cat. No.88CH2555-1), P584, DOI 10.1109/ROBOT.1988.12114
   Harris-Love MO, 2016, PEERJ, V4, DOI 10.7717/peerj.2146
   Hashtrudi-Zaad K, 2002, IEEE T ROBOTIC AUTOM, V18, P108, DOI 10.1109/70.988981
   Hashtrudi-Zaad K, 1999, ICRA '99: IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-4, PROCEEDINGS, P1863, DOI 10.1109/ROBOT.1999.770380
   Huang QH, 2019, IEEE T IND INFORM, V15, P1173, DOI 10.1109/TII.2018.2871864
   Ichihara S, 2007, PERCEPTION, V36, P686, DOI 10.1068/p5696
   Jay C, 2005, World Haptics Conference: First Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virutual Environment and Teleoperator Systems, Proceedings, P655
   Jay C, 2007, ACM T COMPUT-HUM INT, V14, DOI 10.1145/1275511.1275514
   Jemal K, 2022, J TELEMED TELECARE, DOI 10.1177/1357633X221115746
   Jiang ZL, 2021, IEEE T IND ELECTRON, V68, P11200, DOI 10.1109/TIE.2020.3036215
   JULESZ B, 1960, AT&T TECH J, V39, P1125, DOI 10.1002/j.1538-7305.1960.tb03954.x
   Kaber D.B., 2011, REV HUMAN FACTORS ER, V7, P323, DOI [10.1177/1557234X11410389, DOI 10.1177/1557234X11410389]
   Kalia M, 2021, INT J COMPUT ASS RAD, V16, P1181, DOI 10.1007/s11548-021-02419-9
   Kaneko T, 2022, J ECHOCARDIOGR, V20, P16, DOI 10.1007/s12574-021-00542-9
   Kolagunda A, 2018, LECT NOTES COMPUT SC, V11041, P164, DOI 10.1007/978-3-030-01201-4_18
   LAWRENCE DA, 1993, IEEE T ROBOTIC AUTOM, V9, P624, DOI 10.1109/70.258054
   Leuze C, 2018, ADJUNCT PROCEEDINGS OF THE 2018 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P377, DOI 10.1109/ISMAR-Adjunct.2018.00109
   Li KY, 2021, IEEE T MED ROBOT BIO, V3, P510, DOI 10.1109/TMRB.2021.3072190
   Mariani PJ, 2010, ACAD EMERG MED, V17, P293, DOI 10.1111/j.1553-2712.2009.00678.x
   Massie T. H., 1994, P S HAPT INT VIRT EN, P295
   Mathiassen K, 2016, FRONT ROBOT AI, V3, DOI 10.3389/frobt.2016.00001
   MILGRAM P, 1994, IEICE T INF SYST, VE77D, P1321
   Misra S, 2006, SYMPOSIUM ON HAPTICS INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS 2006, PROCEEDINGS, P301
   Montoya J, 2016, EUR J TRAUMA EMERG S, V42, P119, DOI 10.1007/s00068-015-0512-1
   Mourtzis D, 2017, PROC CIRP, V63, P46, DOI 10.1016/j.procir.2017.03.154
   Mylonas GP, 2013, IEEE INT C INT ROBOT, P3251, DOI 10.1109/IROS.2013.6696818
   Navab N, 2007, IEEE COMPUT GRAPH, V27, P10, DOI 10.1109/MCG.2007.117
   Nee AYC, 2012, CIRP ANN-MANUF TECHN, V61, P657, DOI 10.1016/j.cirp.2012.05.010
   NIEMEYER G, 1991, IEEE J OCEANIC ENG, V16, P152, DOI 10.1109/48.64895
   Ning GC, 2021, IEEE T BIO-MED ENG, V68, P2787, DOI 10.1109/TBME.2021.3054413
   Ntourakis D, 2016, WORLD J SURG, V40, P419, DOI 10.1007/s00268-015-3229-8
   Orts-Escolano S, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P741, DOI 10.1145/2984511.2984517
   Priester AM, 2013, IEEE T ULTRASON FERR, V60, P507, DOI 10.1109/TUFFC.2013.2593
   Reboulet C., 1997, Experimental Robotics IV. 4th International Symposium, P498, DOI 10.1007/BFb0035239
   Rokhsaritalemi S, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10020636
   Rosenthal M, 2002, MED IMAGE ANAL, V6, P313, DOI 10.1016/S1361-8415(02)00088-9
   Salcudean SE, 1999, IEEE CONTR SYST MAG, V19, P29, DOI 10.1109/37.806913
   Salcudean SE, 1999, LECT NOTES COMPUT SC, V1679, P1062
   Salcudean SE, 2022, P IEEE, V110, P951, DOI 10.1109/JPROC.2022.3162840
   Schimmoeller T, 2019, J BIOMECH, V83, P117, DOI 10.1016/j.jbiomech.2018.11.032
   Schneider CM, 2010, LECT NOTES COMPUT SC, V6135, P67, DOI 10.1007/978-3-642-13711-2_7
   Shuangyi Wang, 2019, Towards Autonomous Robotic Systems. 20th Annual Conference, TAROS 2019. Proceedings: Lecture Notes in Artificial Intelligence (LNAI 11650), P27, DOI 10.1007/978-3-030-25332-5_3
   Skarbez R, 2022, Presence and beyond: evaluating user experience in AR/MR/VR, P8
   Smallwood N, 2020, CLIN MED, V20, P486, DOI 10.7861/clinmed.2020-0442
   Song H., 2022, IEEE Robot Autom Lett
   Soni NJ, 2021, ULTRASOUND J, V13, DOI 10.1186/s13089-021-00242-6
   Stocco L, 1996, IEEE INT CONF ROBOT, P404, DOI 10.1109/ROBOT.1996.503810
   Ungureanu D, 2020, Arxiv, DOI [arXiv:2008.11239, DOI 10.48550/ARXIV.2008.11239]
   Uschnig C, 2022, ULTRASOUND MED BIOL, V48, P965, DOI 10.1016/j.ultrasmedbio.2022.01.001
   Vieyres P., 2006, M-Health, P461
   Virga S, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P508, DOI 10.1109/IROS.2016.7759101
   Wild E, 2016, INT J COMPUT ASS RAD, V11, P899, DOI 10.1007/s11548-016-1385-4
   Yamamoto Takuya, 2018, Multimodal Technologies and Interaction, V2, DOI 10.3390/mti2030055
   Ye YQ, 2010, IEEE-ASME T MECH, V15, P321, DOI 10.1109/TMECH.2009.2020733
NR 76
TC 4
Z9 4
U1 8
U2 8
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD FEB
PY 2024
VL 118
BP 184
EP 193
DI 10.1016/j.cag.2024.01.003
EA JAN 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IB0L6
UT WOS:001163743500001
OA hybrid
DA 2024-08-05
ER

PT J
AU Veldhuijzen, B
   Veltkamp, RC
   Ikne, O
   Allaert, B
   Wannous, H
   Emporio, M
   Giachetti, A
   LaViola, JJ Jr
   He, RW
   Benhabiles, H
   Cabani, A
   Fleury, A
   Hammoudi, K
   Gavalas, K
   Vlachos, C
   Papanikolaou, A
   Romanelis, I
   Fotis, V
   Arvanitis, G
   Moustakas, K
   Hanik, M
   Nava-Yazdani, E
   von Tycowicz, C
AF Veldhuijzen, Ben
   Veltkamp, Remco C.
   Ikne, Omar
   Allaert, Benjamin
   Wannous, Hazem
   Emporio, Marco
   Giachetti, Andrea
   LaViola Jr, Joseph J.
   He, Ruiwen
   Benhabiles, Halim
   Cabani, Adnane
   Fleury, Anthony
   Hammoudi, Karim
   Gavalas, Konstantinos
   Vlachos, Christoforos
   Papanikolaou, Athanasios
   Romanelis, Ioannis
   Fotis, Vlassis
   Arvanitis, Gerasimos
   Moustakas, Konstantinos
   Hanik, Martin
   Nava-Yazdani, Esfandiar
   von Tycowicz, Christoph
TI SHREC 2024: Recognition of dynamic hand motions molding clay
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Gesture recognition; SHREC; Hand skeleton gestures; 3D shape retrieval
   challenge; Motion capture; Neural networks
AB Gesture recognition is a tool to enable novel interactions with different techniques and applications, like Mixed Reality and Virtual Reality environments. With all the recent advancements in gesture recognition from skeletal data, it is still unclear how well state-of-the-art techniques perform in a scenario using precise motions with two hands. This paper presents the results of the SHREC 2024 contest organized to evaluate methods for their recognition of highly similar hand motions using the skeletal spatial coordinate data of both hands. The task is the recognition of 7 motion classes given their spatial coordinates in a frame-by-frame motion. The skeletal data has been captured using a Vicon system and pre-processed into a coordinate system using Blender and Vicon Shogun Post. We created a small, novel dataset with a high variety of durations in frames. This paper shows the results of the contest, showing the techniques created by the 5 research groups on this challenging task and comparing them to our baseline method.
C1 [Veldhuijzen, Ben; Veltkamp, Remco C.] Univ Utrecht, Dept Comp Sci, Utrecht, Netherlands.
   [Ikne, Omar; Allaert, Benjamin; Wannous, Hazem; Benhabiles, Halim; Fleury, Anthony] Univ Lille, Inst Mines Telecom, Ctr Digital Syst, IMT Nord Europe, F-59000 Lille, France.
   [Emporio, Marco; Giachetti, Andrea] Univ Verona, Dept Comp Sci, Verona, Italy.
   [LaViola Jr, Joseph J.] Univ Cent Florida, Dept Comp Sci, Orlando, FL USA.
   [He, Ruiwen] Ecole Super ingenieurs Leonard Devinci ESILV, Comp Sci Dept, F-75000 Paris, France.
   [Cabani, Adnane] Univ Rouen Normandie, ESIGELEC, IRSEEM, F-76000 Rouen, France.
   [Hammoudi, Karim] Univ Haute Alsace, IRIMAS, Mulhouse, France.
   [Hammoudi, Karim] Univ Strasbourg, Strasbourg, France.
   [Gavalas, Konstantinos; Vlachos, Christoforos; Papanikolaou, Athanasios; Romanelis, Ioannis; Fotis, Vlassis; Arvanitis, Gerasimos; Moustakas, Konstantinos] Univ Patras, Patras, Greece.
   [Hanik, Martin] Free Univ Berlin, Kaiserswerther Str 16-18, D-14195 Berlin, Germany.
   [Hanik, Martin] Tech Univ Berlin, Str 17 Juni 135, D-10623 Berlin, Germany.
   [Hanik, Martin; Nava-Yazdani, Esfandiar; von Tycowicz, Christoph] Zuse Inst Berlin, Takustr 7, D-14195 Berlin, Germany.
C3 Utrecht University; IMT - Institut Mines-Telecom; Universite de Lille;
   IMT Nord Europe; University of Verona; State University System of
   Florida; University of Central Florida; Universite de Rouen Normandie;
   Universites de Strasbourg Etablissements Associes; Universite de
   Haute-Alsace (UHA); Universites de Strasbourg Etablissements Associes;
   Universite de Strasbourg; University of Patras; Free University of
   Berlin; Technical University of Berlin; Zuse Institute Berlin
RP Veldhuijzen, B (corresponding author), Univ Utrecht, Dept Comp Sci, Utrecht, Netherlands.
EM B.veldhuijzen@students.uu.nl; r.c.veltkamp@uu.nl;
   omar.ikne@imt-nord-europe.fr; benjamin.allaert@imt-nord-europe.fr;
   hazem.wannous@imt-nord-europe.fr; marco.emporio@univr.it;
   andrea.giachetti@univr.it; jlaviola@ucf.edu; ruiwen.he@devinci.fr;
   halim.benhabiles@imt-nord-europe.fr; adnane.cabani@esigelec.fr;
   anthony.fleury@imt-nord-europe.fr; karim.hammoudi@uha.fr;
   k_gavalas@ac.upatras.gr; chris.vlachos@ac.upatras.gr;
   up1053560@ac.upatras.gr; iroman@ece.upatras.gr; vfotis@ece.upatras.gr;
   arvanitis@ece.upatras.gr; moustakas@ece.upatras.gr; hanik@zib.de;
   navayazdani@zib.de; vontycowicz@zib.de
FU Provinciaals Utrechts Genootschap Van Kunsten En Wetenschappen
FX Capturing the hand movements of modeling clay was initiated by artist
   Isabel Ferrand, who together with Remco Veltkamp received the PUG prize
   from Provinciaals Utrechts Genootschap Van Kunsten En Wetenschappen.
   Thanks to potter Kees Agterberg for having his hand movements recorded.
   Thanks to Tariq Bakhtali for helping in the motion capture sessions.
CR Akiba T, 2019, Arxiv, DOI [arXiv:1907.10902, 10.48550/arXiv.1907.10902, DOI 10.48550/ARXIV.1907.10902]
   Caputo A, 2021, COMPUT GRAPH-UK, V99, P201, DOI 10.1016/j.cag.2021.07.007
   Caputo FM, 2019, SHREC 2019 track: Online gesture recognition
   Cunico F, 2023, Arxiv, DOI arXiv:2304.05956
   De Smedt Q, 2019, COMPUT VIS IMAGE UND, V181, P60, DOI 10.1016/j.cviu.2019.01.008
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Dill A, 2020, Arxiv, DOI arXiv:1912.10787
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Emporio M, 2022, COMPUT GRAPH-UK, V107, P241, DOI 10.1016/j.cag.2022.07.015
   Fletcher PT, 2009, NEUROIMAGE, V45, pS143, DOI 10.1016/j.neuroimage.2008.10.052
   Glorot X., 2010, P 13 INT C ART INT S, P249
   Hanik M, 2024, arXiv
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   Hendriks J, 2021, VIBRATION-BASEL, V4, P284, DOI 10.3390/vibration4020019
   Ikne B, 2024, IEEE FG 2024
   Loshchilov I, 2017, Sgdr: Stochastic gradient descent with warm restarts, DOI [10.48550/arXiv.1608.03983, DOI 10.48550/ARXIV.1608.03983]
   Papadopoulos Georgios Th, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8325, P473, DOI 10.1007/978-3-319-04114-8_40
   Quentin De Smedt J-PV, 2017, SHREC 17 TRACK 3D HA
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Srivastava A, 2007, 2007 IEEE C COMP VIS, P1
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tancik M., 2020, ADV NEURAL INFORM PR, V33, P7537, DOI DOI 10.48550/ARXIV.2006.10739
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Vaswani A, 2017, ADV NEUR IN, V30
   Yan H, 2023, arXiv
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang Fan, 2019, Make skeleton-based action recognition model smaller, faster and better
NR 27
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD OCT
PY 2024
VL 123
AR 104012
DI 10.1016/j.cag.2024.104012
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A2W3A
UT WOS:001281178400001
DA 2024-08-05
ER

PT J
AU Wang, Y
   Su, P
   Pan, XY
   Wang, HY
   Gao, Y
AF Wang, Yan
   Su, Peng
   Pan, Xiaoying
   Wang, Hongyu
   Gao, Yuan
TI Channel Self-Attention Based Low-Light Image Enhancement Network *
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Low-light image enhancement; Channel self-attention; Noise suppression;
   Dual branch fusion
ID TRANSFORMER
AB Low -light image enhancement is crucial in applications such as traffic safety and medical imaging. Besides having characteristics like low luminance and poor visibility, low -light images are inevitably affected by noise. Noise not only covers image details, introduces artifacts, and decreases image quality, but also interferes with downstream computer vision tasks like object detection, image segmentation, and object tracking. Previous methods in image enhancement often overlook noise handling or fail to accurately suppress noise in adaptive denoising processes, resulting in more severe noise artifacts in the enhanced images. To effectively suppress noise, this paper proposes a Channel Self -Attention Based Low -Light Image Enhancement Network (CAENet), which leverages Transformers and CNNs to model long-range and short-range pixel dependencies, extract global and local features, and construct a Noise Suppression Transformer Block that adaptively suppresses noise regions guided by signal-to-noise ratio priors and attention maps. After adaptive noise suppression, the resulting images exhibit fewer noise artifacts and improved details. The experimental results show that the network in this paper outperforms other state-of-the-art methods overall on five representative paired datasets as well as six unpaired datasets, improving the image quality while effectively suppressing the noise.
C1 [Wang, Yan; Su, Peng; Pan, Xiaoying; Wang, Hongyu; Gao, Yuan] Xian Univ Posts & Telecommun, Sch Comp Sci & Technol, Xian 710121, Peoples R China.
C3 Xi'an University of Posts & Telecommunications
RP Wang, Y (corresponding author), Xian Univ Posts & Telecommun, Sch Comp Sci & Technol, Xian 710121, Peoples R China.
EM wangyanlxz@126.com; idforsu@163.com; panxiaoying@xupt.edu.cn;
   wangyanlxz@126.com; 1821493622@qq.com
FU Key Research and Development Plan of Shaanxi Province, China
   [2022ZDLSF04-05]
FX This work was supported by the Key Research and Development Plan of
   Shaanxi Province, China (2022ZDLSF04-05) .
CR Adam Paszke, 2019, Adv Neural Inf Process Syst, V32
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Bioucas-Dias JM, 2010, IEEE T IMAGE PROCESS, V19, P1720, DOI 10.1109/TIP.2010.2045029
   Celik T, 2011, IEEE T IMAGE PROCESS, V20, P3431, DOI 10.1109/TIP.2011.2157513
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Chen W., 2018, ARXIV
   Demir Y, 2023, DIGIT SIGNAL PROCESS, V138, DOI 10.1016/j.dsp.2023.104054
   Erkan U, 2018, COMPUT ELECTR ENG, V70, P789, DOI 10.1016/j.compeleceng.2018.01.019
   Glorot X., 2011, P 14 INT C ART INT S, V15, P315, DOI DOI 10.1002/ECS2.1832
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo DN, 2011, IEEE T INFORM THEORY, V57, P2371, DOI 10.1109/TIT.2011.2111010
   Guo XJ, 2023, INT J COMPUT VISION, V131, P48, DOI 10.1007/s11263-022-01667-9
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hao SJ, 2020, IEEE T MULTIMEDIA, V22, P3025, DOI 10.1109/TMM.2020.2969790
   Jiang Hai, 2023, ACM Transactions on Graphics (TOG), V42, P1
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Kingma D. P., 2014, arXiv
   Kong XY, 2021, IEEE SIGNAL PROC LET, V28, P1540, DOI 10.1109/LSP.2021.3096160
   Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304
   Lee C, 2012, IEEE IMAGE PROC, P965, DOI 10.1109/ICIP.2012.6467022
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Liu H, 2021, PHYS REV LETT, V126, DOI 10.1103/PhysRevLett.126.250502
   Liu JY, 2021, INT J COMPUT VISION, V129, P1153, DOI 10.1007/s11263-020-01418-8
   Liu RS, 2021, PROC CVPR IEEE, P10556, DOI 10.1109/CVPR46437.2021.01042
   Liu YF, 2013, INT CONF ACOUST SPEE, P2444, DOI 10.1109/ICASSP.2013.6638094
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Peng LT, 2023, IEEE T IMAGE PROCESS, V32, P3066, DOI 10.1109/TIP.2023.3276332
   Ren XT, 2020, IEEE T IMAGE PROCESS, V29, P5862, DOI 10.1109/TIP.2020.2984098
   Shen Z, 2023, COMPUT GRAPH-UK, V111, P77, DOI 10.1016/j.cag.2023.01.009
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Wang RX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9680, DOI 10.1109/ICCV48922.2021.00956
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wu WH, 2022, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR52688.2022.00581
   Wu YH, 2023, PROC CVPR IEEE, P1662, DOI 10.1109/CVPR52729.2023.00166
   Xu J, 2020, IEEE T IMAGE PROCESS, V29, P5022, DOI 10.1109/TIP.2020.2974060
   Xu K, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3566125
   Xu K, 2020, PROC CVPR IEEE, P2278, DOI 10.1109/CVPR42600.2020.00235
   Xu XG, 2023, PROC CVPR IEEE, P9893, DOI 10.1109/CVPR52729.2023.00954
   Xu XG, 2022, PROC CVPR IEEE, P17693, DOI 10.1109/CVPR52688.2022.01719
   Yang SL, 2023, CIRC SYST SIGNAL PR, V42, P4221, DOI 10.1007/s00034-023-02311-8
   Yang SZ, 2023, IEEE I CONF COMP VIS, P12872, DOI 10.1109/ICCV51070.2023.01187
   Yang WH, 2021, IEEE T IMAGE PROCESS, V30, P2072, DOI 10.1109/TIP.2021.3050850
   Yang WH, 2020, PROC CVPR IEEE, P3060, DOI 10.1109/CVPR42600.2020.00313
   Zamir Syed Waqas, 2022, P IEEE CVF C COMP VI, P5728, DOI [DOI 10.48550/ARXIV.2111.09881, DOI 10.1109/CVPR52688.2022.00564]
   Zhang YH, 2021, INT J COMPUT VISION, V129, P1013, DOI 10.1007/s11263-020-01407-x
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhang YD, 2021, LECT NOTES COMPUT SC, V12901, P14, DOI 10.1007/978-3-030-87193-2_2
   Zhao M, 2022, IEEE SIGNAL PROC LET, V29, P1252, DOI 10.1109/LSP.2022.3176486
   Zheng CJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4419, DOI 10.1109/ICCV48922.2021.00440
   Zhou DW, 2023, Arxiv, DOI [arXiv:2305.10028, DOI 10.48550/ARXIV.2305.10028]
NR 51
TC 1
Z9 1
U1 2
U2 2
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD MAY
PY 2024
VL 120
AR 103921
DI 10.1016/j.cag.2024.103921
EA MAY 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SW3T7
UT WOS:001237453800001
DA 2024-08-05
ER

PT J
AU Ye, YP
   Han, JC
   Liang, JX
   Wu, D
   Song, Z
AF Ye, Yuping
   Han, Juncheng
   Liang, Jixin
   Wu, Di
   Song, Zhan
TI Retargeting of facial model for unordered dense point cloud
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Facial retargeting; Point cloud; Non-rigid registration
ID DEFORMATION TRANSFER; 3D; DATABASE
AB Facial retargeting is a widely used technique in the game and film industries that replicates the expressions of a source facial model onto a target model. Existing methods for facial retargeting rely on either hand-crafted uniform triangle meshes or sparse points obtained from motion capture(mocap). In this paper, we propose an end-to-end facial retargeting algorithm that copies facial expressions from unordered dense point clouds onto the target model. First, a corresponding building method based on bi-harmonic function is introduced to ensure that the template model and a cluster of point clouds share the same triangle topology. Second, a deformation transferring method is presented to transfer the calculated deformation onto the target model. Several experiments are conducted on the SIAT-3DFE dataset to demonstrate the accuracy and efficiency of our method.
C1 [Ye, Yuping; Han, Juncheng; Liang, Jixin; Wu, Di; Song, Zhan] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China.
   [Ye, Yuping] Fujian Univ Technol, Sch Smart Ocean Sci & Technol, Fuzhou, Peoples R China.
   [Ye, Yuping; Song, Zhan] Chinese Univ Hong Kong, Hong Kong, Peoples R China.
   [Liang, Jixin] Southern Univ Sci & Technol, Shenzhen, Peoples R China.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; Fujian University of Technology; Chinese University of Hong Kong;
   Southern University of Science & Technology
RP Song, Z (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China.
EM yeahpingye@gmail.com; jc.han@siat.ac.cn; jx.liang2@siat.ac.cn;
   d.wu@siat.ac.cn; zhan.song@siat.ac.cn
OI Liang, Jixin/0009-0000-2556-4556; Ye, Yuping/0000-0002-3116-9410
FU Shenzhen High-tech Zone Development Special Plan Innovation Platform
   Construction Project; National Natural Science Foundation of China
   [62105352]; Shenzhen Science and Technology Program
   [JCYJ20230807140705012]
FX Our deepest gratitude goes to the four anonymous reviewers and editors
   for their careful work and thoughtful suggestions that have helped
   improve this paper substantially. This work is supported by the Shenzhen
   High-tech Zone Development Special Plan Innovation Platform Construction
   Project, the proof of concept center for high precision and high
   resolution 4D imaging; National Natural Science Foundation of China
   under Grant 62105352; Shenzhen Science and Technology Program
   (JCYJ20230807140705012) .
CR Amberg B, 2007, IEEE I CONF COMP VIS, P1326
   ARFaceAnchor, 2023, BlendShapeLocation
   Baran I, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531342
   Ben-Chen Mirela., 2009, P 2009 ACM SIGGRAPH, P67, DOI [10.1145/1599470.1599479, DOI 10.1145/1599470.1599479]
   Bickel B, 2008, Proceedings of the 2008 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '08, P57
   Ribera RBI, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073674
   Bouaziz S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461976
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Chandran P, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530114
   Chen L, 2010, COMPUT GRAPH-UK, V34, P107, DOI 10.1016/j.cag.2010.01.003
   Chuang E, 2002, Comput Sci Tech Rep, Stanf Univ, V2, P3
   Cong M, 2019, SIGGRAPH '19 -ACM SIGGRAPH 2019 TALKS, DOI 10.1145/3306307.3328154
   Costigan T, 2014, P 7 INT C MOT GAM, P31
   Deng BL, 2022, COMPUT GRAPH FORUM, V41, P559, DOI 10.1111/cgf.14502
   Fankhauser P, 2015, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS (ICAR), P388, DOI 10.1109/ICAR.2015.7251485
   Feng WQ, 2021, PROC CVPR IEEE, P10292, DOI 10.1109/CVPR46437.2021.01016
   Gao L, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275028
   Giese MA, 2006, P 3 S APPL PERC GRAP, P77
   Gilani SZ, 2015, PROC CVPR IEEE, P4639, DOI 10.1109/CVPR.2015.7299095
   Grunnet-Jepsen A, 2018, Depth post-processing for intel RealSenseTM D400 depth cameras
   Hirose O, 2023, IEEE T PATTERN ANAL, V45, P5816, DOI 10.1109/TPAMI.2022.3214191
   Jacobson A, 2014, ACM SIGGRApH 2014 courses, P1
   Keselman L, 2017, IEEE COMPUT SOC CONF, P1267, DOI 10.1109/CVPRW.2017.167
   Kim PH, 2011, PG (short papers)
   Kim S, 2021, COMPUT GRAPH FORUM, V40, P45, DOI 10.1111/cgf.14400
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Lewis JP, 2014, Eurographics (State of the Art Reports), V1, P2, DOI [DOI 10.2312/EGST.20141042, 10.2312/egst.20141042]
   Lipman Y, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P181, DOI 10.1109/SMI.2004.1314505
   Lipman Y., 2004, P 2004 EUROGRAPHICSA, P175, DOI DOI 10.1145/1057432.1057456
   Monji-Azad S, 2023, ISPRS J PHOTOGRAMM, V196, P58, DOI 10.1016/j.isprsjprs.2022.12.023
   Noh JY, 2001, COMP GRAPH, P277, DOI 10.1145/383259.383290
   Onizuka H, 2019, IEEE INT CONF COMP V, P2100, DOI 10.1109/ICCVW.2019.00265
   Peng Z, 2023, COMPUT VIS MEDIA, V9, P109, DOI 10.1007/s41095-021-0267-z
   Perakis P, 2013, IEEE T PATTERN ANAL, V35, P1552, DOI 10.1109/TPAMI.2012.247
   Pighin F, 2006, ACM SIGGRAPH 2006 CO, P2, DOI DOI 10.1145/1185657.1185842
   Pyun Hyewon., 2006, SIGGRAPH 06, P23, DOI DOI 10.1145/1185657.1185863
   Roberts RA, 2021, COMPUT GRAPH-UK, V94, P52, DOI 10.1016/j.cag.2020.10.004
   Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6
   Seol Y, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024196
   Smisek J., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1154, DOI 10.1109/ICCVW.2011.6130380
   Sorkine O., 2003, Symposium on Geometry Processing, P42
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Tam GKL, 2013, IEEE T VIS COMPUT GR, V19, P1199, DOI 10.1109/TVCG.2012.310
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Thies J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818056
   Tu ZQ, 2023, MULTIMED TOOLS APPL, V82, P23017, DOI 10.1007/s11042-023-14547-2
   Wang S, 2008, PROC CVPR IEEE, P3490
   White JD, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-42533-y
   Xu F, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601210
   Xu WW, 2009, J COMPUT SCI TECH-CH, V24, P6, DOI 10.1007/s11390-009-9209-4
   Yang HT, 2020, PROC CVPR IEEE, P598, DOI 10.1109/CVPR42600.2020.00068
   Yang J, 2018, GRAPH MODELS, V98, P1, DOI 10.1016/j.gmod.2018.05.003
   Yao YX, 2023, IEEE T PATTERN ANAL, V45, P9681, DOI 10.1109/TPAMI.2023.3247603
   Yao Yuxin, 2020, IEEE C COMPUT VIS PA, P7600
   Ye YP, 2022, COMPUT GRAPH-UK, V104, P46, DOI 10.1016/j.cag.2022.03.007
   Ye YP, 2020, IEEE ACCESS, V8, P48205, DOI 10.1109/ACCESS.2020.2979518
   Zabatani A, 2019, IEEE Trans Pattern Anal Mach Intell
   Zeng L, 2023, P IEEE C COMP VIS PA
   Zeng XS, 2021, PROCEEDINGS OF ACM SIGGRAPH SYMPOSIUM ON COMPUTER ANIMATION, SCA 2021, DOI 10.1145/3475946.3480959
   Zhang JC, 2020, NEUROCOMPUTING, V406, P89, DOI 10.1016/j.neucom.2020.04.025
   Zhang JY, 2022, IEEE T VIS COMPUT GR, V28, P1274, DOI 10.1109/TVCG.2020.3013876
   Zhang S, 2018, OPT LASER ENG, V106, P119, DOI 10.1016/j.optlaseng.2018.02.017
   Zuo C, 2018, OPT LASER ENG, V109, P23, DOI 10.1016/j.optlaseng.2018.04.019
NR 63
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103972
DI 10.1016/j.cag.2024.103972
EA JUN 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XC7B0
UT WOS:001259539500001
DA 2024-08-05
ER

PT J
AU Kurzhals, K
AF Kurzhals, Kuno
TI Anonymizing eye-tracking stimuli with stable diffusion
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Eye tracking; Visualization; Stable diffusion
ID VISUALIZATION; IDENTIFICATION; PRIVACY
AB Casual users nowadays can create almost arbitrary image content by providing textual prompts to generative machine-learning models. These models rapidly improve image quality with each new generation, providing means to create photos, paintings in different styles, and even videos. One feature of such models is the ability to take an image as input and adjust content according to a prompt. A visual obfuscation of content can be achieved for static images and videos by slightly changing persons, text, and other objects. The potential of this technique can be applied in eye-tracking experiments for post-hoc dissemination of analysis results and visualization. In this work, we discuss how the technique could serve to anonymize stimuli (e.g., for double-blind reviews, remove product placements, etc.) and protect the privacy of people visible in the stimuli. We further investigate how the application of this anonymization process influences visual saliency and the depiction of stimuli in visualization techniques. Our results show that slight image transformations do not drastically change the saliency of a scene but obfuscate objects and faces while keeping important image structures for context.
C1 [Kurzhals, Kuno] Univ Stuttgart, Allmandring 19, D-70569 Stuttgart, Germany.
C3 University of Stuttgart
RP Kurzhals, K (corresponding author), Univ Stuttgart, Allmandring 19, D-70569 Stuttgart, Germany.
EM Kuno.Kurzhals@visus.uni-stuttgart.de
FU Deutsche Forschungsgemeinschaft (DFG, German Research Foundation);  [EXC
   2120/1 - 390831618]
FX This work is supported by the Deutsche Forschungsgemeinschaft (DFG,
   German Research Foundation) under Germany's Excellence Strategy - EXC
   2120/1 - 390831618.
CR Berkovsky S, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300451
   Birnstill P, 2015, 2015 12TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Blascheck T, 2017, COMPUT GRAPH FORUM, V36, P260, DOI 10.1111/cgf.13079
   Blascheck T, 2016, 2016 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2016), P111, DOI 10.1145/2857491.2857524
   Blattmann A, 2023, PROC CVPR IEEE, P22563, DOI 10.1109/CVPR52729.2023.02161
   Cetinic E, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3475799
   Ceylan Duygu, 2023, P IEEECVF INT C COMP, P23206
   Chesney B, 2019, CALIF LAW REV, V107, P1753, DOI 10.15779/Z38RV0D15J
   Dang H, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3502141
   David-John B, 2021, IEEE T VIS COMPUT GR, V27, P2555, DOI 10.1109/TVCG.2021.3067787
   Davis RL, 2023, EXTENDED ABSTRACTS C, P1
   Gafni O, 2019, IEEE I CONF COMP VIS, P9377, DOI 10.1109/ICCV.2019.00947
   Gobel Fabian, 2020, CHI 2020 WORKSHOP EX
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Hukkelås H, 2020, LECT NOTES COMPUT SC, V11844, P565, DOI 10.1007/978-3-030-33720-9_44
   Jing YC, 2020, IEEE T VIS COMPUT GR, V26, P3365, DOI 10.1109/TVCG.2019.2921336
   Katsini C, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376840
   Khamis M, 2022, PROCEEDINGS OF THE WORKING CONFERENCE ON ADVANCED VISUAL INTERFACES AVI 2022, DOI 10.1145/3531073.3531125
   Klemp M, 2023, P IEEECVF C COMPUTER, P3198
   Koch M, 2022, P ACM COMPUT GRAPH, V5, DOI 10.1145/3530795
   Koch M, 2018, 2018 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2018), DOI 10.1145/3204493.3204581
   Kurzhals K, 2023, P ACM S EYE TRACKING, P1
   Kurzhals K, 2021, PROCEEDINGS ETRA 2021: ACM SYMPOSIUM ON EYE TRACKING RESEARCH AND APPLICATIONS, DOI 10.1145/3448017.3457382
   Kurzhals K, 2017, IEEE T VIS COMPUT GR, V23, P301, DOI 10.1109/TVCG.2016.2598695
   Kurzhals K, 2016, IEEE T VIS COMPUT GR, V22, P1005, DOI 10.1109/TVCG.2015.2468091
   Le Meur O, 2013, BEHAV RES METHODS, V45, P251, DOI 10.3758/s13428-012-0226-9
   Lee S, 2021, 23RD INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS AND ACCESSIBILITY, ASSETS 2021, DOI 10.1145/3441852.3471200
   Li JZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3891, DOI 10.1145/3474085.3475367
   Li JN, 2022, PR MACH LEARN RES
   Li T, 2019, IEEE COMPUT SOC CONF, P56, DOI 10.1109/CVPRW.2019.00013
   Liebling DJ, 2014, PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP'14 ADJUNCT), P1169, DOI 10.1145/2638728.2641688
   Linardos A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12899, DOI 10.1109/ICCV48922.2021.01268
   Liu VV, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3501825
   Lohr D, 2022, IEEE T INF FOREN SEC, V17, P3151, DOI 10.1109/TIFS.2022.3201369
   Maximov M, 2020, PROC CVPR IEEE, P5446, DOI 10.1109/CVPR42600.2020.00549
   Mirsky Y, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3425780
   Nirkin Y, 2019, IEEE I CONF COMP VIS, P7183, DOI 10.1109/ICCV.2019.00728
   Oliva A, 2006, PROG BRAIN RES, V155, P23, DOI 10.1016/S0079-6123(06)55002-2
   Radford A, 2021, PR MACH LEARN RES, V139
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Sammaknejad N, 2017, ADV COGN PSYCHOL, V13, P232, DOI 10.5709/acp-0223-1
   Schetinger V, 2023, COMPUT GRAPH FORUM, V42, P423, DOI 10.1111/cgf.14841
   Steil J, 2019, ETRA 2019: 2019 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, DOI 10.1145/3314111.3319913
   Steil J, 2019, ETRA 2019: 2019 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, DOI 10.1145/3314111.3319915
   Thapar D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2300, DOI 10.1109/ICCV48922.2021.00232
   Wen YQ, 2022, NEUROCOMPUTING, V501, P197, DOI 10.1016/j.neucom.2022.06.039
   Wilson Ethan, 2022, arXiv
   Xu C, 2022, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR52688.2022.00748
   Xue HY, 2023, CONCURR COMP-PRACT E, V35, DOI 10.1002/cpe.7554
   Zhang AT, 2018, IEEE IMAGE PROC, P2660, DOI 10.1109/ICIP.2018.8451219
   Zhang LM, 2023, IEEE I CONF COMP VIS, P3813, DOI 10.1109/ICCV51070.2023.00355
   Zhao WL, 2023, PROC CVPR IEEE, P8568, DOI 10.1109/CVPR52729.2023.00828
   Zhu BQ, 2020, PROCEEDINGS OF THE 3RD AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY AIES 2020, P414, DOI 10.1145/3375627.3375849
NR 53
TC 0
Z9 0
U1 1
U2 1
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103898
DI 10.1016/j.cag.2024.103898
EA MAR 2024
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OW1K1
UT WOS:001210221300001
OA hybrid
DA 2024-08-05
ER

PT J
AU Pareja-Corcho, J
   Montoya-Zapata, D
   Moreno, A
   Cadavid, C
   Posada, J
   Arenas-Tobon, K
   Ruiz-Salguero, O
AF Pareja-Corcho, Juan
   Montoya-Zapata, Diego
   Moreno, Aitor
   Cadavid, Carlos
   Posada, Jorge
   Arenas-Tobon, Ketzare
   Ruiz-Salguero, Oscar
TI On the shape description of general solids using Morse theory
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Shape description; Handle decomposition; Morse theory; Solid geometry
ID REEB GRAPHS; MESH
AB The automatic shape description of solids is a problem of interest in manufacturing engineering, amongst other related areas. This description can be either geometrical or topological in nature and can be applied to either surfaces or solids (embedded manifolds). Topological descriptions are specially interesting for the problem of shape comparison and retrieval, where one wants to know if a given shape resembles some other known shape. Some popular topological descriptions use Morse theory to study the topology of manifolds and encode their shape characteristics. A Morse function f is defined on the manifold and the manifold's shape is indirectly studied by studying the behavior of the critical points of f . This family of methods is well defined for surfaces but does not consider the case of solids. In this paper we address the topological description of solids using Morse theory. Our methodology considers three cases: solids without internal boundaries, solids with internal boundaries and thin-walled solids. We present an algorithm to identify topological changes on these solids using the principle of shape decomposition by Morse handles. The presented algorithm deals with Morse functions that produce parallel planar level sets. Future endeavors should consider other candidate functions.
C1 [Pareja-Corcho, Juan; Moreno, Aitor; Posada, Jorge; Ruiz-Salguero, Oscar] Vicomtech Fdn, Basque Res & Technol Alliance BRTA, Mikeletegi 57, San Sebastian 20009, Spain.
   [Montoya-Zapata, Diego; Arenas-Tobon, Ketzare; Ruiz-Salguero, Oscar] Univ EAFIT, CAD CAM CAE Lab, Cr 49 Cl 7-Sur 50, Medellin 050022, Colombia.
   [Cadavid, Carlos] Univ EAFIT, Math & Applicat, Cr 49 Cl 7-Sur 50, Medellin 050022, Colombia.
   [Pareja-Corcho, Juan] Univ Basque Country UPV EHU, Fac Informat, Manuel Lardizabal 1, Donostia San Sebastian 20018, Spain.
C3 Universidad EAFIT; Universidad EAFIT; University of Basque Country
RP Pareja-Corcho, J (corresponding author), Vicomtech Fdn, Basque Res & Technol Alliance BRTA, Mikeletegi 57, San Sebastian 20009, Spain.
EM jcpareja@vicomtech.org
FU Basque Government/Eusko Jarlitza project BIO4CURE, Spain (program
   ELKARTEK) [KK-2022/00019]
FX We thank students Samuel Martinez-Londono and Jefferson Salcedo-Chavez
   from Universidad EAFIT for their help in the production of the
   experiments. This work was partly funded by the Basque Govern-ment/Eusko
   Jarlitza project BIO4CURE, Spain (program ELKARTEK, ref-KK-2022/00019) .
CR Agathos A, 2007, Comput.-Aided Des. Appl., V4, P827, DOI DOI 10.1080/16864360.2007.10738515
   Ara£jo C, 2019, Arxiv, DOI arXiv:1904.10213
   Biasotti S, 2008, THEOR COMPUT SCI, V392, P5, DOI 10.1016/j.tcs.2007.10.018
   Biasotti S, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P371, DOI 10.1109/SMI.2004.1314530
   Bohm N, 2019, Morse theory and handle decompositions
   Brandolini L, 2012, ICPRAM 2, V12, P80
   Chao Ma, 2021, 2021 International Conference on Artificial Intelligence and Electromechanical Automation (AIEA), P277, DOI 10.1109/AIEA53260.2021.00065
   Cole-McLaughlin K, 2003, P 19 ANN S COMP GEOM, P344, DOI [10.1145/777792.777844, DOI 10.1145/777792.777844]
   Doraiswamy H, 2012, IEEE T VIS COMPUT GR, V18, P146, DOI 10.1109/TVCG.2011.37
   Edelsbrunner H, 2008, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SGG'08), P242, DOI 10.1145/1377676.1377720
   FEITO F, 1995, COMPUT GRAPH, V19, P595, DOI 10.1016/0097-8493(95)00037-D
   Guo HS, 2023, J MECH SCI TECHNOL, V37, P317, DOI 10.1007/s12206-022-1231-2
   Hachani M, 2014, INT C PATT RECOG, P3981, DOI 10.1109/ICPR.2014.682
   Hajij M, 2020, ALGORITHMS, V13, DOI 10.3390/a13100258
   Hajij M, 2016, GRAPH MODELS, V88, P12, DOI 10.1016/j.gmod.2016.09.003
   Harvey W, 2010, PROCEEDINGS OF THE TWENTY-SIXTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SCG'10), P267, DOI 10.1145/1810959.1811005
   He C, 2018, WIRELESS PERS COMMUN, V102, P3835, DOI 10.1007/s11277-018-5414-1
   Ho TC, 2012, J INF SCI ENG, V28, P705
   Khatkar J, 2022, IEEE INT CON AUTO SC, P277, DOI 10.1109/CASE49997.2022.9926485
   Li X, 2009, IEEE T VIS COMPUT GR, V15, P558, DOI 10.1109/TVCG.2008.200
   Lim CW, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0093747
   Mamou K, 2009, IEEE IMAGE PROC, P3501, DOI 10.1109/ICIP.2009.5414068
   Matsumoto, 2002, INTRO MORSE THEORY, V208
   Mejia-Parra D, 2020, MATHEMATICS-BASEL, V8, DOI 10.3390/math8091624
   Pareja-Corcho J, 2023, SMART TOOLS APPL GRA, DOI [10.2312/stag.20231302, DOI 10.2312/STAG.20231302]
   Parsa S, 2012, P 28 ANN S COMP GEOM, P269
   Pascucci V, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239509, 10.1145/1276377.1276449]
   REEB G, 1946, CR HEBD ACAD SCI, V222, P847
   Ruiz OE, 2005, COMPUT GRAPH-UK, V29, P81, DOI 10.1016/j.cag.2004.11.009
   Sander PV, 2001, COMP GRAPH, P409, DOI 10.1145/383259.383307
   Shi YG, 2008, PROC CVPR IEEE, P511
   SHINAGAWA Y, 1991, IEEE COMPUT GRAPH, V11, P44, DOI 10.1109/38.103393
   Strodthoff B, 2015, COMPUT GRAPH-UK, V46, P186, DOI 10.1016/j.cag.2014.09.026
   Strodthoff B, 2017, COMPUT AIDED DESIGN, V90, P157, DOI 10.1016/j.cad.2017.05.006
   Tai CL, 1998, COMPUT GRAPH, V22, P255, DOI 10.1016/S0097-8493(98)00036-3
   Takahashi S, 2004, GRAPH MODELS, V66, P24, DOI 10.1016/j.gmod.2003.08.002
   Theologou P, 2014, Comput-Aided Des Appl, V11, P670
   Tung T, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P157, DOI 10.1109/SMI.2004.1314503
   Tuteski O, 2018, Ind 4.0, V3, P82
   Wang J, 2011, COMPUT GRAPH-UK, V35, P661, DOI 10.1016/j.cag.2011.03.016
   Wang ZH, 2014, LECT NOTES COMPUT SC, V8673, P666, DOI 10.1007/978-3-319-10404-1_83
NR 41
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103994
DI 10.1016/j.cag.2024.103994
EA JUL 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YT3R1
UT WOS:001270703700001
DA 2024-08-05
ER

PT J
AU Woodworth, JW
   Borst, CW
AF Woodworth, Jason W.
   Borst, Christoph W.
TI Visual cues in VR for guiding attention vs. restoring attention after a
   short distraction
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Virtual reality; Attention guidance; Attention restoration; Visual cues
ID INTERFACE
AB Distraction in VR training environments may be mitigated with a visual cue intended to guide user attention to a target. A survey of related literature suggests a past focus on "search and selection"tasks to evaluate a cue's capability for guidance. We investigate the capability of 9 eye-tracked cues with a new type of task that focuses on how to restore attention when a short distraction (e.g., a notification) shifts focus away from a target. Our study includes a guidance task in which subjects gaze at objects in a randomized order and a restoration task in which gaze sequences are interrupted by distraction events after which gaze must be returned to an object. We consider a wider variety of factors and metrics than previous studies, varying object spacing, gaze dwell time, and distraction distance and duration, and breaking down guidance time into subcomponents. Results show a general positive trend for cues that directly connect the user's gaze to the target rather than indirectly suggesting direction. Results further reveal different patterns of cue effectiveness for the restoration task than for conventional guidance. This may be attributed to knowledge that subjects have about the location of the object from which they were distracted. An implication for more complex distraction tasks is that we expect them to be between the short distraction and regular guidance in terms of memory of object position. So, we speculate cue performance for other tasks would vary between the short distraction and guidance results. For restoration, some cues add complexity that reduces, rather than improves, performance.
C1 [Woodworth, Jason W.; Borst, Christoph W.] Univ Louisiana Lafayette, CACS VR Lab, Lafayette, LA 70504 USA.
C3 University of Louisiana Lafayette
RP Woodworth, JW (corresponding author), Univ Louisiana Lafayette, CACS VR Lab, Lafayette, LA 70504 USA.
EM jason.woodworth1@louisiana.edu; cwborst@gmail.com
CR Albawaneh A, 2023, 2023 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS, VRW, P432, DOI 10.1109/VRW58643.2023.00093
   Bailey R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1559755.1559757
   Binetti N, 2021, DISPLAYS, V69, DOI 10.1016/j.displa.2021.102032
   Biocca F., 2006, Conference on Human Factors in Computing Systems. CHI2006, P1115
   Bork F, 2018, IEEE T VIS COMPUT GR, V24, P2983, DOI 10.1109/TVCG.2018.2868584
   Borst Christoph W., 2007, Proceedings of the 2007 International Conference on Computer Graphics & Virtual Reality. CGVR2007, P72
   Bozzi LOS, 2023, 2023 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS, VRW, P384, DOI 10.1109/VRW58643.2023.00084
   Khuong BM, 2014, 2014 IEEE VIRTUAL REALITY (VR), P57, DOI 10.1109/VR.2014.6802051
   Cai JN, 2020, CONSTRUCTION RESEARCH CONGRESS 2020: COMPUTER APPLICATIONS, P116, DOI 10.1061/9780784482865.013
   Danieau F, 2017, P IEEE VIRT REAL ANN, P205, DOI 10.1109/VR.2017.7892248
   dos Santos IHF, 2012, Int J Virtual Real, V11, P1, DOI [10.20870/IJVR.2012.11.1.2832, DOI 10.20870/IJVR.2012.11.1.2832]
   FEINER S, 1993, COMMUN ACM, V36, P53, DOI 10.1145/159544.159587
   Grogorick S, 2018, IEEE IMAGE PROC, P2805, DOI 10.1109/ICIP.2018.8451784
   Grogorick S, 2017, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2017), DOI 10.1145/3119881.3119890
   Gruenefeld U, 2018, 20TH INTERNATIONAL CONFERENCE ON HUMAN-COMPUTER INTERACTION WITH MOBILE DEVICES AND SERVICES (MOBILEHCI 2018), DOI 10.1145/3229434.3229438
   Gruenefeld U, 2018, PROCEEDINGS PERVASIVE DISPLAYS 2018: THE 7TH ACM INTERNATIONAL SYMPOSIUM ON PERVASIVE DISPLAYS, DOI 10.1145/3205873.3205881
   Gruenefeld U, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P742, DOI [10.1109/VR.2019.8797725, 10.1109/vr.2019.8797725]
   Gruenefeld U, 2017, SUI'17: PROCEEDINGS OF THE 2017 SYMPOSIUM ON SPATIAL USER INTERACTION, P109, DOI 10.1145/3131277.3132175
   Gugenheimer J, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P1996, DOI 10.1145/2858036.2858040
   Harada Y, 2022, VIRTUAL REAL-LONDON, V26, P759, DOI 10.1007/s10055-021-00574-7
   Healey CG, 2012, IEEE T VIS COMPUT GR, V18, P1170, DOI 10.1109/TVCG.2011.127
   HOLM S, 1979, SCAND J STAT, V6, P65
   Hu S, 2021, Graphics Interface, V2021, DOI [10.20380/GI2021.32, DOI 10.20380/GI2021.32]
   Jo H, 2011, COMPUT GRAPH-UK, V35, P841, DOI 10.1016/j.cag.2011.04.005
   Kim S, 2019, APPL ERGON, V74, P186, DOI 10.1016/j.apergo.2018.08.026
   Lange D, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376803
   Li YF, 2023, 2023 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS, VRW, P434, DOI 10.1109/VRW58643.2023.00094
   Lin YC, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P2535, DOI 10.1145/3025453.3025757
   Markov-Vetter D, 2020, P 2020 ACM S SPATIAL, V10, DOI [10.1145/3385959.3418449, DOI 10.1145/3385959.3418449]
   Nielsen LT, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P229, DOI 10.1145/2993369.2993405
   Petersen N, 2013, INT SYM MIX AUGMENT, P117, DOI 10.1109/ISMAR.2013.6671771
   Renner P, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P671, DOI 10.1109/VR.2018.8446127
   Renner P, 2017, ADJUNCT PROCEEDINGS OF THE 2017 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT), P176, DOI 10.1109/ISMAR-Adjunct.2017.59
   Renner P, 2017, IEEE SYMP 3D USER, P186, DOI 10.1109/3DUI.2017.7893338
   Wang P, 2018, INT J ENV RES PUB HE, V15, DOI 10.3390/ijerph15061204
   Woodworth JW, 2023, 2023 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS, VRW, P442, DOI 10.1109/VRW58643.2023.00096
   Ziho Kang, 2020, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V64, P821, DOI 10.1177/1071181320641191
NR 37
TC 1
Z9 1
U1 2
U2 2
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD FEB
PY 2024
VL 118
BP 194
EP 209
DI 10.1016/j.cag.2023.12.008
EA JAN 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC5K2
UT WOS:001164134900001
DA 2024-08-05
ER

PT J
AU Png, FGSWH
   Aun, Y
   Gan, ML
AF Png, Feature-guided Style Wen Hao
   Aun, Yichiet
   Gan, Ming Lee
TI FeaST: Feature-guided Style Transfer for high-fidelity art synthesis
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Image style transfer; Img2Img synthesis; Generative art
AB Text-conditioned image synthesis methods such as DALLE -2, IMAGEN, and Stable Diffusion are gaining strong attention from deep learning and art communities recently. Meanwhile, Image -to -Image (Img2Img) synthesis applications that emerged from the pioneering Neural Style Transfer (NST) approach have swiftly transitioned towards the feed-forward Automatic Style Transfer (AST) methods, due to numerous constraints inherent in the former method, including inconsistent synthesis outcomes and sluggish optimization-based synthesis process. However, NST holds significant potential yet remains relatively underexplored within this research domain. In this paper, we revisited the original NST method and uncovered its potential to attain image quality comparable to the AST synthesis methods across a diverse range of artistic styles. We propose a two-stage Feature-guided Style Transfer (FeaST) which consists (a) pre-stylization step called Sketching to address the poor initialization issue, and (b) Finetuning to guide the synthesis process based on high -frequency (HF) and low-frequency (LF) guidance channels. By addressing the issues of inconsistent synthesis and slow convergence inherent in the original method, FeaST unlocks the full capabilities of NST and significantly enhances its efficiency.
C1 [Png, Feature-guided Style Wen Hao; Aun, Yichiet; Gan, Ming Lee] Univ Tunku Abdul Rahman UTAR, Fac Informat & Commun Technol, Kampar 31900, Malaysia.
RP Png, FGSWH (corresponding author), Univ Tunku Abdul Rahman UTAR, Fac Informat & Commun Technol, Kampar 31900, Malaysia.
EM pngwh@utar.edu.my; aunyc@utar.edu.my; ganml@utar.edu.my
FX This research was supported by Grant IPSR/RMC/UTARRF/2023-C2/P02,
   Malaysia.
CR Gatys LA, 2016, Arxiv, DOI [arXiv:1606.05897, 10.48550/ARXIV.1606.05897, DOI 10.48550/ARXIV.1606.05897]
   Ahn N, 2023, Arxiv, DOI arXiv:2309.06933
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Deng YY, 2022, Arxiv, DOI [arXiv:2105.14576, 10.48550/arXiv.2105.14576, DOI 10.48550/ARXIV.2105.14576]
   Dumoulin V, 2017, Arxiv, DOI arXiv:1610.07629
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Henighan T., 2017, Spatial control in neural style transfer
   Huang SY, 2023, Arxiv, DOI arXiv:2212.10431
   Jia Y., 2014, Caffe: Convolutional architecture for fast feature embedding, DOI DOI 10.48550/ARXIV.1408.5093
   Jing YC, 2022, Arxiv, DOI arXiv:2207.11681
   Johnson J, 2016, Arxiv, DOI [arXiv:1603.08155, 10.48550/arXiv.1603.08155, DOI 10.48550/ARXIV.1603.08155, 10.48550/ARXIV.1603.08155]
   Li XT, 2018, Arxiv, DOI [arXiv:1808.04537, DOI 10.48550/ARXIV.1808.04537]
   Li YJ, 2017, Arxiv, DOI [arXiv:1705.08086, 10.48550/ARXIV.1705.08086, DOI 10.48550/ARXIV.1705.08086]
   Liu SH, 2021, Arxiv, DOI [arXiv:2108.03647, 10.48550/ARXIV.2108.03647, DOI 10.48550/ARXIV.2108.03647]
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Ulyanov D, 2016, arXiv
   Wang ZZ, 2023, Arxiv, DOI arXiv:2308.07863
   Winnemoeller H, 2012, Comput Graph
   Wu Z, 2022, arXiv
   Zhang YL, 2020, Arxiv, DOI [arXiv:1904.04443, 10.48550/ARXIV.1904.04443, DOI 10.48550/ARXIV.1904.04443]
   Zhang YQ, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3150568
NR 22
TC 0
Z9 0
U1 1
U2 1
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103975
DI 10.1016/j.cag.2024.103975
EA JUN 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WZ8E4
UT WOS:001258781800001
DA 2024-08-05
ER

PT J
AU Zhuang, QB
   Chen, ZG
   He, KY
   Cao, J
   Wang, WP
AF Zhuang, Qiubing
   Chen, Zhonggui
   He, Keyu
   Cao, Juan
   Wang, Wenping
TI Dynamics simulation-based packing of irregular 3D objects
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Geometry processing; 3D irregular packing; Dynamics simulation
ID BIN PACKING; SPHERES
AB The 3D packing problem has a wide range of applications. However, the complex geometry of irregular objects leads to a sharp increase in the number of placement combinations, making it a challenging problem. In this paper, we propose a packing pipeline based on rigid body dynamics simulation to deal with two types of 3D packing problems. One is the variant bin packing problem, which involves placing more objects into a container of given dimensions to maximize space utilization. The other is the open dimension problem, where the goal is to minimize the container that can accommodate all objects. We first use heuristic placement strategies and a fast collision detection algorithm to efficiently obtain initial packing results. Then, we simulate the shaking of the container according to the dynamic principle. Combined with the vacant space filling operation, shaking the container drives the movement of objects in the container to make the arrangement of objects more compact. For the open dimension packing, the container height is optimized by adjusting the constraints of simulation in the basic pipeline. Experimental results show that our method has advantages over existing methods in both speed and packing density.
C1 [Zhuang, Qiubing; Chen, Zhonggui; He, Keyu; Cao, Juan] Xiamen Univ, Xiamen, Peoples R China.
   [Wang, Wenping] Texas A&M Univ, College Stn, TX USA.
C3 Xiamen University; Texas A&M University System; Texas A&M University
   College Station
RP Chen, ZG (corresponding author), Xiamen Univ, Sch Informat, Xiamen, Peoples R China.
EM zhuangqiubing@163.com; chenzhonggui@xmu.edu.cn; 1205486810@qq.com;
   Juancao@xmu.edu.cn; wenping@tamu.edu
FU National Natural Science Foun-dation of China [61972327, 62272402,
   62372389]; Natural Science Foundation of Fujian Province, China
   [2022J01001]; Fundamental Research Funds for the Central Universities,
   China [20720220037]
FX The work was supported by the National Natural Science Foun-dation of
   China (Nos. 61972327, 62272402, 62372389) , the Natural Science
   Foundation of Fujian Province, China (No. 2022J01001) , and the
   Fundamental Research Funds for the Central Universities, China (No.
   20720220037) .
CR Amirifar R, 2021, POWDER TECHNOL, V380, P47, DOI 10.1016/j.powtec.2020.11.036
   BERNAL JD, 1960, NATURE, V188, P910, DOI 10.1038/188910a0
   Bortfeldt A, 2013, EUR J OPER RES, V229, P1, DOI 10.1016/j.ejor.2012.12.006
   Catto E, 2023, Box2D: A 2D physics engine for games
   Chen XL, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818087
   Cui QD, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592126
   GILBERT EG, 1988, IEEE T ROBOTIC AUTOM, V4, P193, DOI 10.1109/56.2083
   Gzara F, 2020, EUR J OPER RES, V287, P1062, DOI 10.1016/j.ejor.2020.04.053
   Hu H., 2017, ARXIV
   Hu RZ, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417796
   Lamas-Fernandez C, 2023, OPER RES, V71, P1298, DOI 10.1287/opre.2022.2260
   Ma Y, 2018, COMPUT GRAPH FORUM, V37, P49, DOI 10.1111/cgf.13490
   NVIDIA Corporation, 2023, NVIDIA PhysX SDK
   NVIDIA Corporation, 2023, cuFFT: Fast fourier transform library
   Pouliquen O, 1997, PHYS REV LETT, V79, P3640, DOI 10.1103/PhysRevLett.79.3640
   Radin C, 2008, J STAT PHYS, V131, P567, DOI 10.1007/s10955-008-9523-1
   Romanova T, 2018, EUR J OPER RES, V268, P37, DOI 10.1016/j.ejor.2018.01.025
   Vanek J, 2014, COMPUT GRAPH FORUM, V33, P322, DOI 10.1111/cgf.12353
   Wang F, 2021, IEEE T AUTOM SCI ENG, V18, P1901, DOI 10.1109/TASE.2020.3024291
   Wang F, 2022, IEEE T ROBOT, V38, P1160, DOI 10.1109/TRO.2021.3097261
   Wang L, 2010, LECT NOTES ARTIF INT, V6230, P256, DOI 10.1007/978-3-642-15246-7_25
   Wei XY, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530103
   Xue T, 2023, SIGGRAPH AS 2023 C P, P1
   Yu AB, 2006, PHYS REV LETT, V97, DOI 10.1103/PhysRevLett.97.265501
   Zhao H, 2021, INT C LEARN REPR
   Zhao H, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3603544
   Zhao H, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3348-6
   Zhao H, 2021, AAAI CONF ARTIF INTE, V35, P741
   Zhao XZ, 2016, INT T OPER RES, V23, P287, DOI 10.1111/itor.12094
   Zhou QN, 2016, Arxiv, DOI arXiv:1605.04797
NR 30
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD OCT
PY 2024
VL 123
AR 103996
DI 10.1016/j.cag.2024.103996
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZN1W8
UT WOS:001275897300001
DA 2024-08-05
ER

PT J
AU Hácha, F
   Dvorak, J
   Kacereková, Z
   Vása, L
AF Hacha, Filip
   Dvorak, Jan
   Kacerekova, Zuzana
   Vasa, Libor
TI Editing mesh sequences with varying connectivity
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Computer graphics; Animation; Shape modeling
ID TIME; ANIMATION
AB Time -varying connectivity of triangle mesh sequences leads to substantial difficulties in their processing. Unlike editing sequences with constant connectivity, editing sequences with varying connectivity requires addressing the problem of temporal correspondence between the frames of the sequence. We present a method for timeconsistent editing of triangle mesh sequences with varying connectivity using sparse temporal correspondence, which can be obtained using existing methods. Our method includes a deformation model based on the usage of the sparse temporal correspondence, which is suitable for the temporal propagation of user -specified deformations of the edited surface with respect to the shape and true topology of the surface while preserving the individual connectivity of each frame. Since there is no other method capable of comparable types of editing on time -varying meshes, we compare our method and the proposed deformation model with a baseline approach and demonstrate the benefits of our framework.
C1 [Hacha, Filip; Dvorak, Jan; Kacerekova, Zuzana; Vasa, Libor] Univ West Bohemia, Fac Appl Sci, Dept Comp Sci & Engn, Univerzitni 8, Plzen 30100, Czech Republic.
C3 University of West Bohemia Pilsen
RP Hácha, F (corresponding author), Univ West Bohemia, Fac Appl Sci, Dept Comp Sci & Engn, Univerzitni 8, Plzen 30100, Czech Republic.
EM hachaf@kiv.zcu.cz; jdvorak@kiv.zcu.cz; kacerekz@kiv.zcu.cz;
   lvasa@kiv.zcu.cz
RI Hácha, Filip/AGJ-4622-2022
OI Hacha, Filip/0000-0001-8956-6411
FU Czech Science Foundation [23-04622L]; University specific research
   project [SGS-2022-015]
FX The authors have no competing interests to declare that are relevant to
   the content of this article. This work was supported by the project
   23-04622L, Data compression paradigm based on omitting self-evident
   information - COMPROMISE, of the Czech Science Foundation. Filip Hacha,
   Jan Dvorak and Zuzana Kacerekova were partially supported by the
   University specific research project SGS-2022-015, New Methods for
   Medical, Spatial and Communication Data.
CR Adams Bart, 2008, Comput Animat, V2008, P77
   Barbic J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185566
   Bojsen-Hansen M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185549
   Boltzmann L, 1868, SITZUNG AM
   Botsch M, 2006, P EUR S GEOM PROC, DOI 10.2312/SGP/SGP06/011-020
   Botsch M, 2010, Polygon Mesh Processing, DOI DOI 10.1201/B10688
   Botsch M, 2008, IEEE T VIS COMPUT GR, V14, P213, DOI 10.1109/TVCG.2007.1054
   Casas Dan, 2011, Motion in Games. Proceedings 4th International Conference, MIG 2011, P242, DOI 10.1007/978-3-642-25090-3_21
   Casas D., 2012, P ACM SIGGRAPH S INT, P103, DOI DOI 10.1145/2159616.2159633
   Casas D, 2013, IEEE T VIS COMPUT GR, V19, P762, DOI 10.1109/TVCG.2012.314
   Cashman TJ, 2012, COMPUT GRAPH FORUM, V31, P735, DOI 10.1111/j.1467-8659.2012.03032.x
   de Aguiar E., 2012, 2012 XXV SIBGRAPI - Conference on Graphics, Patterns and Images (SIBGRAPI 2012), P198, DOI 10.1109/SIBGRAPI.2012.35
   de Aguiar E, 2008, COMPUT GRAPH FORUM, V27, P389, DOI 10.1111/j.1467-8659.2008.01136.x
   Dean CJ, 2018, SA'18: SIGGRAPH ASIA 2018 TECHNICAL BRIEFS, DOI 10.1145/3283254.3283268
   Doumanoglou A, 2014, IEEE T CIRC SYST VID, V24, P2099, DOI 10.1109/TCSVT.2014.2319631
   Dvorak Jan, 2023, Computational Science - ICCS 2023: 23rd International Conference, Proceedings. Lecture Notes in Computer Science (10476), P113, DOI 10.1007/978-3-031-36027-5_9
   Dvorak Jan, 2021, Computational Science - ICCS 2021. 21st International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12746), P45, DOI 10.1007/978-3-030-77977-1_4
   Dvorak J, 2022, COMPUT GRAPH-UK, V102, P329, DOI 10.1016/j.cag.2021.10.015
   Eisenberger M, 2019, COMPUT GRAPH FORUM, V38, P1, DOI 10.1111/cgf.13785
   Abrevaya VF, 2016, COMPUT GRAPH-UK, V58, P12, DOI 10.1016/j.cag.2016.05.018
   Gleicher M., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P139, DOI 10.1145/253284.253321
   Heck R, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P129
   Hildebrandt K, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185567
   Huang CHP, 2018, IEEE T PATTERN ANAL, V40, P1994, DOI 10.1109/TPAMI.2017.2740308
   Huang P, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2699643
   Jacobson A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964973
   James DL, 2005, ACM T GRAPHIC, V24, P399, DOI 10.1145/1073204.1073206
   Jin M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818114
   KABSCH W, 1976, ACTA CRYSTALLOGR A, V32, P922, DOI 10.1107/S0567739476001873
   Kavan L, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409625.1409627
   Kim HB, 2007, INT J COMPUT SCI NET, V7, P55
   Kircher S, 2006, ACM T GRAPHIC, V25, P1098, DOI 10.1145/1141911.1142000
   Kircher S, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1356682.1356685
   Kovar L, 2008, ACM SIGGRAPH 2008 classes, DOI [10.1145/1401132.1401202, DOI 10.1145/1401132.1401202]
   Koyama Y, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173735
   Le BH, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601161
   Le Naour T, 2013, COMPUT ANIMAT VIRT W, V24, P419, DOI 10.1002/cav.1518
   Li SW, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601217
   Li SW, 2013, COMPUT ANIMAT VIRT W, V24, P409, DOI 10.1002/cav.1521
   Li YJ, 2017, IEEE T VIS COMPUT GR, V23, P2301, DOI 10.1109/TVCG.2016.2620467
   Lipman Y., 2004, P 2004 EUROGRAPHICSA, P175, DOI DOI 10.1145/1057432.1057456
   Prada F, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925967
   Slavcheva M, 2017, PROC CVPR IEEE, P5474, DOI 10.1109/CVPR.2017.581
   Sorkine O, 2007, Proc. Symposium on Geometry Processing, V4, P109, DOI [DOI 10.1145/1281991.1282006, 10.1145/1073204.1073323]
   Sorkine-Hornung O., 2016, Least-squares rigid motion using svd
   Sumner RW, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239531
   Tejera M., 2011, 2011 Conference for Visual Media Production, P148, DOI 10.1109/CVMP.2011.23
   Tejera M, 2013, IEEE T CYBERNETICS, V43, P1532, DOI 10.1109/TCYB.2013.2260328
   Tevs A, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2159516.2159517
   Thiery JM, 2012, COMPUT GRAPH FORUM, V31, P2303, DOI 10.1111/j.1467-8659.2012.03159.x
   Vása L, 2011, IEEE T VIS COMPUT GR, V17, P220, DOI 10.1109/TVCG.2010.38
   Vlasic D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360696
   Wu Y, 2008, LECT NOTES COMPUT SC, V4975, P437
   Xu JF, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/592812
   Xu WW, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239535
   Yang L, 2014, GRAPH MODELS, V76, P413, DOI 10.1016/j.gmod.2014.03.010
   Yifan W, 2020, PROC CVPR IEEE, P72, DOI 10.1109/CVPR42600.2020.00015
   Yuan YJ, 2021, J COMPUT SCI TECH-CH, V36, P520, DOI 10.1007/s11390-021-1414-9
   Zhao Y, 2015, COMPUT GRAPH-UK, V46, P80, DOI 10.1016/j.cag.2014.09.013
NR 59
TC 0
Z9 0
U1 3
U2 3
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD JUN
PY 2024
VL 121
AR 103943
DI 10.1016/j.cag.2024.103943
EA MAY 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TW5B4
UT WOS:001244298500001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Lin, YH
   Yang, GY
   Ze, YF
   Zhang, LK
   Xing, BX
   Liu, XY
   Lyu, R
AF Lin, Yunhui
   Yang, Guoying
   Ze, Yuefeng
   Zhang, Lekai
   Xing, Baixi
   Liu, Xinya
   Lyu, Ruimin
TI The Impact of Motion Features of Hand-drawn Lines on Emotional
   Expression: an Experimental Study
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Hand-drawn lines; Emotion; Affective Computing; Visual Art;
   Human-computer interaction
ID STATE RECOGNITION; MODEL; APPRECIATION; PREDICTION; PREFERENCE;
   CURVATURE; SHAPES
AB This study delves into the nuanced interplay between the motion features of hand -drawn lines and their capacity to convey emotions, a relatively underexplored facet within the realm of human -computer interaction and visual art. By initiating an original experimental design, we generated a pioneering dataset, capturing both static and motion features of lines drawn to express a spectrum of emotions. Through meticulous analysis employing multivariate ordered logistic regression, we unearthed significant motion features that significantly influence emotional expression, alongside corroborating the relevance of certain static features. Our investigation extends beyond mere feature identification, exploring how these attributes correlate with emotional perceptions across a broad emotional spectrum. This research not only bridges a gap in existing literature but also lays foundational insights for future explorations into the emotional dimensions of visual art and design, offering new perspectives for enhancing creative processes and understanding the art -emotion nexus.
C1 [Lin, Yunhui; Liu, Xinya; Lyu, Ruimin] Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, Wuxi 214122, Peoples R China.
   [Yang, Guoying] Univ London, Goldsmiths Coll, London SE14 6NW, England.
   [Ze, Yuefeng] Shanghai Inst Visual Arts, Sch New Media Art, Shanghai 201620, Peoples R China.
   [Zhang, Lekai; Xing, Baixi] Zhejiang Univ Technol, Hangzhou 310014, Peoples R China.
   [Lyu, Ruimin] Jiangnan Univ, Jiangsu Key Lab Media Design & Software Technol, Wuxi 214122, Peoples R China.
C3 Jiangnan University; University of London; Goldsmiths University London;
   Shanghai Institute of Visual Arts; Zhejiang University of Technology;
   Jiangnan University
RP Lyu, R (corresponding author), Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, Wuxi 214122, Peoples R China.; Lyu, R (corresponding author), Jiangnan Univ, Jiangsu Key Lab Media Design & Software Technol, Wuxi 214122, Peoples R China.
EM ruiminlyu@jiangnan.edu.cn
FU National Natural Science Foundation of China [72304249]; Fundamental
   Research Funds for the Provincial Universities of Zhejiang [GB202003008,
   GB202302009]
FX star This research was supported by the National Natural Science
   Foundation of China (72304249) , and the Fundamental Research Funds for
   the Provincial Universities of Zhejiang (GB202003008) (GB202302009) .
CR ARONOFF J, 1992, J PERS SOC PSYCHOL, V62, P1050, DOI 10.1037/0022-3514.62.6.1050
   Ayzeren YB, 2019, IEEE ACCESS, V7, P164759, DOI 10.1109/ACCESS.2019.2952313
   Bar M, 2006, PSYCHOL SCI, V17, P645, DOI 10.1111/j.1467-9280.2006.01759.x
   Bara I, 2021, EUR J NEUROSCI, V54, P7231, DOI 10.1111/ejn.15479
   Bertamini M, 2016, BRIT J PSYCHOL, V107, P154, DOI 10.1111/bjop.12132
   Bijlsma S, 2006, ANAL CHEM, V78, P567, DOI 10.1021/ac051495j
   Blazhenkova O, 2018, PERCEPTION, V47, P67, DOI 10.1177/0301006617731048
   Chamberlain R, 2022, BRIT J PSYCHOL, V113, P105, DOI 10.1111/bjop.12527
   Cotter KN, 2017, I-PERCEPTION, V8, DOI 10.1177/2041669517693023
   Cowen AS, 2017, P NATL ACAD SCI USA, V114, pE7900, DOI 10.1073/pnas.1702247114
   Danna J, 2013, RES DEV DISABIL, V34, P4375, DOI 10.1016/j.ridd.2013.09.012
   Erbilek M, 2012, IET BIOMETRICS, V1, P136, DOI 10.1049/iet-bmt.2012.0011
   Fairhurst M, 2014, I W BIOMETRIC FORENS
   Fairhurst M, 2015, IET BIOMETRICS, V4, P90, DOI 10.1049/iet-bmt.2014.0097
   Freedberg D, 2007, TRENDS COGN SCI, V11, P197, DOI 10.1016/j.tics.2007.02.003
   Gombrich ErnestH., 1964, Journal of the Warburg and Courtauld Institutes, V27, P293, DOI DOI 10.2307/750521
   Gómez-Puerto G, 2018, PSYCHOL AESTHET CREA, V12, P432, DOI 10.1037/aca0000135
   Han JW, 2019, PROCEEDINGS OF THE 10TH AUGMENTED HUMAN INTERNATIONAL CONFERENCE 2019 (AH2019), DOI 10.1145/3311823.3311868
   Hanbury A., 2010, P 18 ACM INT C MULTI, V18, P83, DOI DOI 10.1145/1873951.1873965
   Harrell F.E., 2018, Bios, V330, P14
   Hepp-Reymond MC, 2009, BRAIN RES BULL, V79, P365, DOI 10.1016/j.brainresbull.2009.05.013
   Hevner K, 1935, J APPL PSYCHOL, V19, P385, DOI 10.1037/h0055538
   Hu Y, 2021, P ANN M COGNITIVE SC, V43
   Ibanez Jesus, 2014, Smart Graphics. 12th International Symposium (SG 2014). Proceedings: LNCS 8698, P98, DOI 10.1007/978-3-319-11650-1_9
   Ibáñez J, 2013, IEEE T SYST MAN CY-S, V43, P901, DOI 10.1109/TSMCA.2012.2220542
   Ibáñez J, 2011, COMPUT HUM BEHAV, V27, P561, DOI 10.1016/j.chb.2010.10.004
   Jin XY, 2022, WIREL COMMUN MOB COM, V2022, DOI 10.1155/2022/6238930
   Knoblich G, 2002, Q J EXP PSYCHOL-A, V55, P1027, DOI 10.1080/02724980143000631
   Koelsch S, 2015, PHYS LIFE REV, V13, P1, DOI 10.1016/j.plrev.2015.03.001
   Leder H, 2005, APPL COGNITIVE PSYCH, V19, P603, DOI 10.1002/acp.1088
   Leder H, 2014, BRIT J PSYCHOL, V105, P443, DOI 10.1111/bjop.12084
   [李慧敏 Lee Huimin], 2015, [心理科学进展, Advances in Psychological Science], V23, P1843
   Li-Tsang CWP, 2022, medRxiv, DOI [10.1101/2022.02.19.22270984, 10.1101/2022.02.19.22270984, DOI 10.1101/2022.02.19.22270984]
   Likforman-Sulem L, 2017, IEEE T HUM-MACH SYST, V47, P273, DOI 10.1109/THMS.2016.2635441
   Likforman-Sulem L, 2015, SMART INNOV SYST TEC, V37, P347, DOI 10.1007/978-3-319-18164-6_34
   Lin AA, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445373
   Liu JL, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00134
   Liu X, 2023, Graphics interface
   Lundholm H, 1921, PSYCHOL REV, V28, P43, DOI 10.1037/h0072647
   Lyu RM, 2021, P ACM COMPUT GRAPH, V4, DOI 10.1145/3465625
   Marchetti L, 2022, BRIT J AESTHET, V62, P353, DOI 10.1093/aesthj/ayab044
   Mechtcheriakov S, 2006, J NEUROL, V253, P349, DOI 10.1007/s00415-005-0995-5
   Munar E, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0141106
   Nolazco-Flores JA, 2021, IEEE ACCESS, V9, P28496, DOI 10.1109/ACCESS.2021.3058443
   Ojha A, 2021, SEMIOTICA, P305, DOI 10.1515/sem-2019-0079
   Palumbo L, 2016, EMPIR STUD ARTS, V34, P35, DOI 10.1177/0276237415621185
   Peirce C. S., 1974, Collected papers of charles sanders peirce, V1
   Poffenberger AT, 1924, J APPL PSYCHOL, V8, P187, DOI 10.1037/h0073513
   PUSTEL G, 1969, J PSYCHOL, V73, P159, DOI 10.1080/00223980.1969.10544963
   Rahman AU, 2023, APPL INTELL, V53, P2798, DOI 10.1007/s10489-022-03552-x
   Redies C, 2015, FRONT HUM NEUROSCI, V9, DOI 10.3389/fnhum.2015.00218
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Ruta N, 2023, PSYCHOL AESTHET CREA, V17, P307, DOI 10.1037/aca0000395
   Salgado-Montejo A, 2017, COGNITION EMOTION, V31, P511, DOI 10.1080/02699931.2015.1133401
   Sbriscia-Fioretti B, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0075241
   Schober P, 2018, ANESTH ANALG, V126, P1763, DOI 10.1213/ANE.0000000000002864
   Székely GJ, 2007, ANN STAT, V35, P2769, DOI 10.1214/009053607000000505
   Thömmes K, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.01050
   Ticini LF, 2014, FRONT HUM NEUROSCI, V8, DOI 10.3389/fnhum.2014.00391
   Tinio PPL, 2013, PSYCHOL AESTHET CREA, V7, P265, DOI 10.1037/a0030872
   Torgerson WS, 1952, PSYCHOMETRIKA, V17, P401
   Tucker B, 2007, GER QUART, V80, P185
   Umilta' MA, 2012, FRONT HUM NEUROSCI, V6, DOI 10.3389/fnhum.2012.00311
   Urquhart L, 2018, Hum Technol, V14, P27, DOI [10.17011/ht/urn.201805242751, DOI 10.17011/HT/URN.201805242751]
   Valentine CW, 2015, The experimental psychology of beauty, DOI [10.4324/9781315707617, DOI 10.4324/9781315707617]
   Vartanian Oshin, 2021, Brain, Beauty and Art: Essays Bringing Neuroaesthetics into Focus, P27
   Wang CM, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20205741
   Westerman SJ, 2012, PSYCHOL MARKET, V29, P595, DOI 10.1002/mar.20546
   Winner Ellen., 2019, ART WORKS PSYCHOL EX
   Yu K, 2011, P 16 INT C INTELLIGE, P423, DOI [10.1145/1943403.1943481, DOI 10.1145/1943403.1943481]
   Zagorskis V, 2019, Period Eng Nat Sci, V7, P228, DOI [10.21533/pen.v7i1.355, DOI 10.21533/PEN.V7I1.355]
NR 71
TC 0
Z9 0
U1 2
U2 2
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103897
DI 10.1016/j.cag.2024.103897
EA MAR 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OE6Z7
UT WOS:001205641500001
DA 2024-08-05
ER

PT J
AU Viganò, G
   Melzi, S
AF Vigano, Giulio
   Melzi, Simone
TI Bijective upsampling and learned embedding for point clouds
   correspondences
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Shape correspondence; Functional maps; Point clouds; Machine learning
AB In this paper, we present a novel pipeline to compute and refine a data-driven solution for estimating the correspondence between 3D point clouds. Our method is compatible with the functional map framework, so it relies on a functional representation of the correspondence, but, differently from other similar approaches, this method is specifically designed to exploit this functional scenario for point cloud matching. Our new method merges a data-driven approach to compute functional basis and descriptors on the shape's surface and a new refinement method designed for the learned basis. This refinement algorithm arises from a different way of converting functional operators into point-to-point correspondence, which we prove to promote bijectivity between maps, exploiting a theoretical result. Iterating this procedure and performing basis upsampling in the same way as other similar methods, ours increases the accuracy of the correspondence, leading to more bijective correspondences. Different from other approaches, our method allows us to train a functional basis, considering the refinement stage. Combining our new pipeline with an improved feature extractor, our solution outperforms previous methods in various evaluations and settings. We test our method over different datasets, comprising near-isometric and non-isometric pairs.
C1 [Vigano, Giulio; Melzi, Simone] Univ Milano Bicocca, Dept Informat Syst & Commun DISCo, Milan, Italy.
C3 University of Milano-Bicocca
RP Viganò, G (corresponding author), Univ Milano Bicocca, Dept Informat Syst & Commun DISCo, Milan, Italy.
EM g.vigano36@campus.unimib.it
FU MUR under the grant "Dipartimenti di Eccellenza'' of the Department of
   Informatics, Systems and Communication of the University of
   Milano-Bicocca, Italy; PRIN project "GEOPRIDE Geometric primitive
   fitting on 3D data for geometric analysis and 3D shapes"; NVIDIA
   Corporation; RTX A5000 GPUs through the Academic Hardware Grant Program
FX This work was partially supported by the MUR under the grant
   "Dipartimenti di Eccellenza 2023-2027'' of the Department of
   Informatics, Systems and Communication of the University of
   Milano-Bicocca, Italy and by the PRIN 2022 project "GEOPRIDE Geometric
   primitive fitting on 3D data for geometric analysis and 3D shapes". We
   gratefully acknowledge the support of NVIDIA Corporation with the RTX
   A5000 GPUs granted through the Academic Hardware Grant Program to the
   University of Milano-Bicocca for the project "Learned representations
   for implicit binary operations on real-world 2D-3D data".
CR Attaiki S, 2023, Arxiv, DOI arXiv:2303.16527
   Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Biasotti S, 2016, COMPUT GRAPH FORUM, V35, P87, DOI 10.1111/cgf.12734
   Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491
   Cao DL, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592107
   Deng B, 2022, arXiv
   Donati Nicolas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8589, DOI 10.1109/CVPR42600.2020.00862
   Donati N, 2022, arXiv
   Donati N, 2022, COMPUT GRAPH FORUM, V41, P317, DOI 10.1111/cgf.14437
   Eisenberger M, 2019, Arxiv, DOI arXiv:1905.12512
   Eisenberger Marvin., 2020, arXiv
   Eynard D, 2016, INT CONF 3D VISION, P399, DOI 10.1109/3DV.2016.49
   Ezuz D, 2017, COMPUT GRAPH FORUM, V36, P165, DOI 10.1111/cgf.13254
   Groueix T, 2018, Arxiv, DOI arXiv:1806.05228
   Halimi O, 2019, PROC CVPR IEEE, P4365, DOI 10.1109/CVPR.2019.00450
   Hartwig F, 2023, PROCEEDINGS OF SIGGRAPH 2023 CONFERENCE PAPERS, SIGGRAPH 2023, DOI 10.1145/3588432.3591518
   Huang RQ, 2020, COMPUT GRAPH FORUM, V39, P265, DOI 10.1111/cgf.14084
   Huang RQ, 2017, COMPUT GRAPH FORUM, V36, P151, DOI 10.1111/cgf.13253
   Jiang PH, 2023, PROC CVPR IEEE, P21835, DOI 10.1109/CVPR52729.2023.02091
   Kim VG, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964974
   Levy B, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P66
   Li L, 2022, arXiv
   Litany O, 2017, IEEE I CONF COMP VIS, P5660, DOI 10.1109/ICCV.2017.603
   Magnet R, 2022, INT CONF 3D VISION, P495, DOI 10.1109/3DV57658.2022.00061
   Magnet R, 2023, Arxiv, DOI arXiv:2303.05965
   Marin R, 2022, arXiv
   Marin R, 2020, Arxiv, DOI arXiv:2010.13136
   Melzi S, 2020, COMPUT GRAPH-UK, V88, P1, DOI 10.1016/j.cag.2020.02.002
   Melzi S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356524
   Melzi Simone, 2019, EUROGRAPHICS WORKSHO
   Nogneng D, 2017, COMPUT GRAPH FORUM, V36, P259, DOI 10.1111/cgf.13124
   Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526
   Ovsjanikov Maks, 2017, ACM SIGGRAPH 2017 CO
   Pai G, 2021, PROC CVPR IEEE, P384, DOI 10.1109/CVPR46437.2021.00045
   Panine M, 2022, COMPUT GRAPH FORUM, V41, P394, DOI 10.1111/cgf.14579
   Qi C. R., 2016, CoRR, DOI DOI 10.48550/ARXIV.1612.00593
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Raganato A, 2023, COMPUT GRAPH FORUM, V42, DOI 10.1111/cgf.14912
   Ren J, 2021, COMPUT GRAPH FORUM, V40, P81, DOI 10.1111/cgf.14359
   Ren J, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417800
   Ren J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275040
   Rodolà E, 2017, COMPUT GRAPH FORUM, V36, P222, DOI 10.1111/cgf.12797
   Rodola E, 2015, P VIS MOD VIS
   Sahillioglu Y, 2020, VISUAL COMPUT, V36, P1705, DOI 10.1007/s00371-019-01760-0
   Sánchez-Belenguer C, 2020, ROBOT AUTON SYST, V123, DOI 10.1016/j.robot.2019.103324
   Sharma A, 2020, Arxiv, DOI arXiv:2009.13339
   Sharp N, 2022, Arxiv, DOI arXiv:2012.00888
   Sharp N, 2020, COMPUT GRAPH FORUM, V39, P69, DOI 10.1111/cgf.14069
   Siddiqi S, 2023, GERM C PATT REC
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Sun MZ, 2023, Arxiv, DOI arXiv:2308.08871
   Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26
   Trappolini G, 2021, Arxiv, DOI arXiv:2106.13679
   van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P1681, DOI 10.1111/j.1467-8659.2011.01884.x
   Varol G, 2018, Arxiv, DOI arXiv:1701.01370
   Vigano G, 2023, SMART TOOLS APPL GRA, DOI [10.2312/stag.20231293, DOI 10.2312/STAG.20231293]
   Wang H, 2020, Arxiv, DOI arXiv:2006.07029
   Wiersma R, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530166
   Xiang R, 2020, Arxiv, DOI arXiv:2007.13049
   Zuffi S, 2017, PROC CVPR IEEE, P5524, DOI 10.1109/CVPR.2017.586
NR 60
TC 0
Z9 0
U1 1
U2 1
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103985
DI 10.1016/j.cag.2024.103985
EA JUL 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XZ2Q4
UT WOS:001265440400001
DA 2024-08-05
ER

PT J
AU Wang, ZR
   Yi, JJ
   Su, L
   Pan, YH
AF Wang, Zhuoran
   Yi, Jianjun
   Su, Lin
   Pan, Yihan
TI Coherent point drift with Skewed Distribution for accurate point cloud
   registration
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Point set registration; Probabilistic mixture model; Skewed distribution
ID MAXIMUM-LIKELIHOOD; ALIGNMENT; ICP
AB Point cloud registration methods based on Gaussian Mixture Models (GMMs) exhibit high robustness. However, GMM cannot precisely depict point clouds, because the Gaussian distribution is spatially symmetric and local surfaces of point clouds are typically non -symmetric. In this paper, we propose a novel method for rigid point cloud registration, termed coherent point drift with Skewed Distribution (Skewed CPD). Our method employs an asymmetric distribution constructed from the local surface normals and curvature radii. Compared to the Gaussian distribution, this skewed distribution provides a more accurate spatial description of points on local surfaces. Additionally, we integrate an adaptive multiplier to the covariance, which reallocates the weight of the covariance for different components in the probabilistic mixture model. We employ the EM algorithm to address this maximum likelihood estimation (MLE) issue and leverage GPU acceleration. In the M -step, we adopt an unconstrained optimization technique rooted in a Lie group and Lie algebra to attain the optimal transformation. Experimental results indicate that our method outperforms state-of-the-art methods in both accuracy and robustness. Remarkably, even without loop closure detection, the cumulative error of our approach remains minimal.
C1 [Wang, Zhuoran; Yi, Jianjun; Su, Lin; Pan, Yihan] East China Univ Sci & Technol, Dept Mech Engn, Shanghai 200237, Peoples R China.
C3 East China University of Science & Technology
RP Yi, JJ (corresponding author), East China Univ Sci & Technol, Dept Mech Engn, Shanghai 200237, Peoples R China.
EM jjyi@ecust.edu.cn
OI YI, Jianjun/0000-0003-0899-177X
FU Shanghai Science and Technology Ac-Plan, China [21JM0010300]; Shanghai
   Aerospace Science and Technology Innovation Fund (SAST) , China
   [2021-037]; Special Fund Technology Innovation Support Project of
   Shanghai, China [2021-cyxt-kj13]; Na-tional Defense Basic Scientific
   Research Program of China [JCKY2021606B002]
FX This paper was supported by Shanghai Science and Technology Ac-Plan,
   China under Grant No. 21JM0010300, Shanghai Aerospace Science and
   Technology Innovation Fund (SAST) , China under Grant 2021-037 and the
   Special Fund Technology Innovation Support Project of Shanghai, China
   under Grant No. 2021-cyxt-kj13 the Na-tional Defense Basic Scientific
   Research Program of China (Grant No. JCKY2021606B002) .
CR Babin P, 2019, IEEE INT CONF ROBOT, P1451, DOI [10.1109/icra.2019.8793791, 10.1109/ICRA.2019.8793791]
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bishop C.M., 2006, Machine Learning, P430, DOI DOI 10.1117/1.2819119
   Bouaziz S, 2013, COMPUT GRAPH FORUM, V32, P113, DOI 10.1111/cgf.12178
   Briales J, 2017, PROC CVPR IEEE, P5612, DOI 10.1109/CVPR.2017.595
   CHEN Y, 1992, IMAGE VISION COMPUT, V10, P145, DOI 10.1016/0262-8856(92)90066-C
   Chetverikov D, 2005, IMAGE VISION COMPUT, V23, P299, DOI 10.1016/j.imavis.2004.05.007
   Chetverikov D, 2002, INT C PATT RECOG, P545, DOI 10.1109/ICPR.2002.1047997
   Chirikjian GS, 2012, APPL NUMER HARMON AN, P1, DOI 10.1007/978-0-8176-4944-9
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Eckart B, 2018, LECT NOTES COMPUT SC, V11219, P730, DOI 10.1007/978-3-030-01267-0_43
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gao W, 2019, PROC CVPR IEEE, P11087, DOI 10.1109/CVPR.2019.01135
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Granger S, 2002, LECT NOTES COMPUT SC, V2353, P418
   Hirose O, 2021, IEEE T PATTERN ANAL, V43, P2269, DOI 10.1109/TPAMI.2020.2971687
   Horaud R, 2011, IEEE T PATTERN ANAL, V33, P587, DOI 10.1109/TPAMI.2010.94
   Jian B, 2011, IEEE T PATTERN ANAL, V33, P1633, DOI 10.1109/TPAMI.2010.223
   Jin Zhang, 2021, AS C PATT REC
   Liu WX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15273, DOI 10.1109/ICCV48922.2021.01501
   Low K.L., 2004, Chapel Hill, University of North Carolina, V4, P1
   Luo B, 2003, COMPUT VIS IMAGE UND, V92, P26, DOI 10.1016/S1077-3142(03)00097-3
   MacTavish K, 2015, 2015 12TH CONFERENCE ON COMPUTER AND ROBOT VISION CRV 2015, P62, DOI 10.1109/CRV.2015.52
   McNeill G, 2006, IEEE IMAGE PROC, P937, DOI 10.1109/ICIP.2006.312629
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46
   Myung IJ, 2003, J MATH PSYCHOL, V47, P90, DOI 10.1016/S0022-2496(02)00028-7
   Nüchter A, 2007, J FIELD ROBOT, V24, P699, DOI 10.1002/rob.20209
   Pauly M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P163, DOI 10.1109/VISUAL.2002.1183771
   Qin HX, 2022, COMPUT GRAPH FORUM, V41, P365, DOI 10.1111/cgf.14614
   Schmidt T, 2014, P ROB SCI SYST
   Scrafin J, 2015, IEEE INT C INT ROBOT, P742, DOI 10.1109/IROS.2015.7353455
   Segal A, 2009, P ROBOTICS SCI SYSTE
   Stoyanov T, 2012, INT J ROBOT RES, V31, P1377, DOI 10.1177/0278364912460895
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Yang H, 2021, IEEE T ROBOT, V37, P314, DOI 10.1109/TRO.2020.3033695
   Yang JL, 2013, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2013.184
   Zhou QY, 2016, LECT NOTES COMPUT SC, V9906, P766, DOI 10.1007/978-3-319-46475-6_47
   Zhou ZY, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0091381
NR 39
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103974
DI 10.1016/j.cag.2024.103974
EA JUN 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XH6E2
UT WOS:001260822500001
DA 2024-08-05
ER

PT J
AU Henstrom, J
   De Amicis, R
   Sanchez, CA
   Turkan, Y
AF Henstrom, Jordan
   De Amicis, Raffaele
   Sanchez, Christopher A.
   Turkan, Yelda
TI Immersive engineering instruction: Using Virtual Reality to enhance
   students' experience in the classroom
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Virtual reality learning environment; Virtual building inspection;
   Learning technologies; NASA TLX; SUS; UEQ
ID SPATIAL ABILITIES; USER EXPERIENCE; LEARNERS; DESIGN; MODELS
AB With the growing complexity of construction projects, the Architectural, Engineering, and Construction (AEC) sector increasingly requires a skilled workforce. This need is magnified by the challenge of less than ideal retention rates in engineering fields, prompting a search for methods to enhance the learning experience. Virtual Reality (VR) has emerged as an effective tool in this context, offering immersive and interactive learning environments that improve the grasp and retention of complex concepts in engineering. This research assesses the benefits of implementing a VR solution for building inspections in an undergraduate civil engineering program. It compares the use of a VR system against traditional desktop -based inspection techniques and considers cognitive load, user experience, and usability aspects of this implementation. Additionally, it examines how students' spatial abilities affect their performance with both methods. Results demonstrate that VR does not increase cognitive load and is often perceived as requiring less effort by student users. While both the desktop and VR options produced similar usability scores, the VR implementation provided a much more engaging user experience. These results were replicated and echoed in a second study with actual engineering instructors, who likewise held a strong positive attitude towards the VR system, consistent with student attitudes. Taken together, these results highlight the potential for VR to revolutionize educational opportunities, suggesting that its further adoption and exploration should be prioritized as a useful and viable academic tool. Specifically, these results suggest that VR can enhance the learning process in civil engineering and make collaborative virtual building inspections more efficient and engaging.
C1 [Henstrom, Jordan; De Amicis, Raffaele; Sanchez, Christopher A.; Turkan, Yelda] Oregon State Univ, 601 SW 17th St, Corvallis, OR 97331 USA.
C3 Oregon State University
RP Henstrom, J (corresponding author), Oregon State Univ, 601 SW 17th St, Corvallis, OR 97331 USA.
EM henstroj@oregonstate.edu; raffaele.deamicis@oregonstate.edu;
   christopher.sanchez@oregonstate.edu; yelda.turkan@oregonstate.edu
OI de amicis, raffaele/0000-0002-6435-4364; Turkan,
   Yelda/0000-0002-3224-5462
FU Online Learning SeedGrants-CREEdO-Oregon State University [CREEdO Index]
   [ENG351-ECEO]
FX This work was supported by the Online Learning SeedGrants-CREEdO-Oregon
   State University [CREEdO Index: ENG351-ECEO]
CR Ademci E, 2018, 5 INT PROJ CONSTR MA, P1046
   Anton D, 2018, FUTURE GENER COMP SY, V82, P77, DOI 10.1016/j.future.2017.12.055
   Ardiny H, 2018, RSI INT CONF ROBOT M, P482, DOI 10.1109/ICRoM.2018.8657615
   Arif F, 2021, EDUC INF TECHNOL, V26, P3607, DOI 10.1007/s10639-021-10429-y
   Aydin S, 2020, FRONT ROBOT AI, V7, DOI 10.3389/frobt.2020.495468
   Azhar S., 2011, LEADERSHIP MANAGE EN, V11, P241, DOI [10.1061/(ASCE)LM.1943-5630.0000127, DOI 10.1061/(ASCE)LM.1943-5630.0000127]
   Beh HJ, 2022, ENG CONSTR ARCHIT MA, V29, P2854, DOI 10.1108/ECAM-02-2021-0174
   Bodner GM, 1997, Chem. Educ, V2, P1, DOI DOI 10.1007/S00897970138A
   Bonet G., 2016, College student journal, V50, P224
   Bowman D, 2004, 3D User Interfaces: Theory and Practice
   Brito C, 2018, EasyChair Preprints, DOI [10.29007/jx9r, DOI 10.29007/JX9R]
   Brooke J., 1996, SUS-a quick and dirty usability scale, DOI [DOI 10.1201/9781498710411-35, DOI 10.1201/9781498710411]
   Casey BM, 2021, DEV REV, V60, DOI 10.1016/j.dr.2021.100963
   Ceylan S, 2020, PROCEEDINGS OF THE 12TH INTERNATIONAL CONFERENCE ON COMPUTER SUPPORTED EDUCATION (CSEDU), VOL 2, P54, DOI 10.5220/0009346800540063
   Chang E, 2020, INT J HUM-COMPUT INT, V36, P1658, DOI 10.1080/10447318.2020.1778351
   Chernosky J, 2021, J CONTIN HIGH EDUC, V69, P100, DOI 10.1080/07377363.2020.1786342
   Chilana PK, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P2337
   Concannon BJ, 2019, FRONT EDUC, V4, DOI 10.3389/feduc.2019.00080
   De Amicis R, 2019, INT J INTERACT DES M, V13, P331, DOI 10.1007/s12008-019-00546-x
   Desai N., 2017, P 2017 ASEE ZON 2 C, P2
   Du J, 2018, J CONSTR ENG M, V144, DOI 10.1061/(ASCE)CO.1943-7862.0001426
   ElGewely Maha, 2020, 10th International Conference on Engineering, Project, and Production Management. Lecture Notes in Mechanical Engineering (LNME), P101, DOI 10.1007/978-981-15-1910-9_9
   Elgewely MH, 2021, Open House Int, V46, P359
   Erdogmus E, 2021, 2021 IEEE FRONTIERS IN EDUCATION CONFERENCE (FIE 2021), DOI 10.1109/FIE49875.2021.9637182
   Erfanian A, 2017, IEEE T HUM-MACH SYST, V47, P1052, DOI 10.1109/THMS.2017.2700431
   Farrow C. Ben, 2021, International Journal of Construction Education and Research, V17, P299, DOI 10.1080/15578771.2020.1757536
   Frederiksen JG, 2020, SURG ENDOSC, V34, P1244, DOI 10.1007/s00464-019-06887-8
   Gebczynska-Janowicz A., 2020, Virtual reality technology in architectural education
   Halabi O, 2020, MULTIMED TOOLS APPL, V79, P2987, DOI 10.1007/s11042-019-08214-8
   Han Y, 2023, P 2022 INT C ED SCI, V157, P02001, DOI [10.1051/shsconf/202315702001, DOI 10.1051/SHSCONF/202315702001]
   HART S G, 1988, P139
   Hays TA, 1996, J EDUC COMPUT RES, V14, P139, DOI 10.2190/60Y9-BQG9-80HX-UEML
   Henstrom J, 2023, 28TH INTERNATIONAL CONFERENCE ON WEB3D TECHNOLOGY, WEB3D 2023, DOI 10.1145/3611314.3615917
   Hughes BE., 2019, American Society for Engineering Education, DOI DOI 10.18260/1-2--32674
   Huk T, 2006, J COMPUT ASSIST LEAR, V22, P392, DOI 10.1111/j.1365-2729.2006.00180.x
   Keil D., 2021, KN-J. Cartography Geographic Inf., V71, P53
   Kharvari F, 2019, INT CONF GAMES VIRTU, P252, DOI 10.1109/vs-games.2019.8864576
   Kim YM, 2020, INT J HUM-COMPUT INT, V36, P893, DOI 10.1080/10447318.2019.1699746
   Koch Michael, 2018, PROC 16 EUR C COMPUT, DOI 10.18420/ecscw2018_3
   Kühl T, 2022, EDUC PSYCHOL REV, V34, P1063, DOI 10.1007/s10648-021-09650-5
   Kühl T, 2018, J EDUC PSYCHOL, V110, P561, DOI 10.1037/edu0000226
   Kuley ElizabethA., 2015, Proceedings of the Canadian Engineering Education Association, DOI DOI 10.24908/PCEEA.V0I0.5813
   Lavric P, 2017, 2017 40TH INTERNATIONAL CONVENTION ON INFORMATION AND COMMUNICATION TECHNOLOGY, ELECTRONICS AND MICROELECTRONICS (MIPRO), P259, DOI 10.23919/MIPRO.2017.7973430
   Lee EAL, 2014, COMPUT EDUC, V79, P49, DOI 10.1016/j.compedu.2014.07.010
   LINN MC, 1985, CHILD DEV, V56, P1479, DOI 10.2307/1130467
   Liu D, 2017, SMART COMPUT INTELL, P1, DOI 10.1007/978-981-10-5490-7
   Lucas Jason, 2022, International Journal of Construction Education and Research, P374, DOI 10.1080/15578771.2021.1931570
   Martin F, 2018, ONLINE LEARN, V22, P205, DOI 10.24059/olj.v22i1.1092
   Mayer R. E., 2005, The Cambridge Handbook of Multimedia Learning, P31, DOI [10.1017/CBO9781139547369.005, DOI 10.1017/CBO9780511816819.004]
   MAYER RE, 1994, J EDUC PSYCHOL, V86, P389, DOI 10.1037/0022-0663.86.3.389
   Miguel-Alonso I, 2023, APPL SCI-BASEL, V13, DOI 10.3390/app13010593
   Modjeska D, 2003, J AM SOC INF SCI TEC, V54, P216, DOI 10.1002/asi.10197
   Osti F, 2021, VIRTUAL REAL-LONDON, V25, P523, DOI 10.1007/s10055-020-00470-6
   Paas F, 2003, EDUC PSYCHOL-US, V38, P63, DOI 10.1207/S15326985EP3801_8
   Paes D, 2018, CONSTRUCTION RESEARCH CONGRESS 2018: CONSTRUCTION INFORMATION TECHNOLOGY, P419
   Prithul A, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.730792
   Richter F., 2023, AR & VR adoption is still in its infancy
   Rosenbaum S., 1989, IEEE Transactions on Professional Communications, V32, P210, DOI 10.1109/47.44533
   Sacks R, 2010, J PROF ISS ENG ED PR, V136, P30, DOI 10.1061/(ASCE)EI.1943-5541.0000003
   Schott C, 2021, AUSTRALAS J EDUC TEC, V37, P96, DOI 10.14742/ajet.5166
   Schrepp M., 2015, USER EXPERIENCE QUES
   Schrepp M, 2017, INT J INTERACT MULTI, V4, P103, DOI 10.9781/ijimai.2017.09.001
   Seeber KG, 2015, Routledge encyclopedia of interpreting studies, V60
   Sepasgozar SME, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10134678
   Skulmowski A, 2022, EDUC PSYCHOL REV, V34, P171, DOI 10.1007/s10648-021-09624-7
   Slater M, 2009, PHILOS T R SOC B, V364, P3549, DOI 10.1098/rstb.2009.0138
   Soliman M, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11062879
   Sorby S, 2018, LEARN INDIVID DIFFER, V67, P209, DOI 10.1016/j.lindif.2018.09.001
   Spitzer BO, 2022, BUILDINGS-BASEL, V12, DOI 10.3390/buildings12122169
   Steenkamp H, 2017, IEEE GLOB ENG EDUC C, P693, DOI 10.1109/EDUCON.2017.7942922
   Stork A, 2000, 20 COMP INF ENG C, V1, P729, DOI [10.1115/DETC2000/CIE-14589, DOI 10.1115/DETC2000/CIE-14589]
   Strand I., 2020, Form Academic, V13, DOI DOI 10.7577/FORMAKADEMISK.3874
   Sun R, 2019, VIRTUAL REAL-LONDON, V23, P385, DOI 10.1007/s10055-018-0355-2
   Try S, 2021, COMPUT APPL ENG EDUC, V29, P1771, DOI 10.1002/cae.22422
   Vesga JB, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P645, DOI 10.1109/VR50410.2021.00090
   Walker J, 2019, P 2019 INT C OP INN, P610
   Wang AN, 2022, INTERACT LEARN ENVIR, V30, P677, DOI 10.1080/10494820.2019.1678489
   Wang C, 2022, COMPUT APPL ENG EDUC, V30, P335, DOI 10.1002/cae.22458
   Wang P, 2018, INT J ENV RES PUB HE, V15, DOI 10.3390/ijerph15061204
   Yoghourdjian V, 2021, IEEE T VIS COMPUT GR, V27, P1677, DOI 10.1109/TVCG.2020.3030459
NR 80
TC 0
Z9 0
U1 1
U2 1
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD JUN
PY 2024
VL 121
AR 103944
DI 10.1016/j.cag.2024.103944
EA MAY 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UE5Y1
UT WOS:001246409800001
DA 2024-08-05
ER

PT J
AU Poglitsch, C
   Safikhani, S
   List, E
   Pirker, J
AF Poglitsch, Christian
   Safikhani, Saeed
   List, Erin
   Pirker, Johanna
TI XR technologies to enhance the emotional skills of people with autism
   spectrum disorder: A systematic review
SO COMPUTERS & GRAPHICS-UK
LA English
DT Review
DE Extended reality (XR); Virtual reality (VR); Augmented reality (AR);
   Autism spectrum disorder; Emotional skills; Emotion recognition
ID VIRTUAL-REALITY; AUGMENTED REALITY; FACIAL EXPRESSIONS; CHILDREN; MIND;
   COMMUNICATION; DISPLAYS; ABILITY; DESIGN; ADULTS
AB In this paper, we present a systematic review of the applications of (1) Extended Reality (XR), (2) Augmented Reality (AR), and (3) Virtual Reality (VR) technologies to enhance emotion recognition and emotion expression in people with Autism Spectrum Disorder (ASD). ASD can affect various abilities, and poses challenges to the recognition of emotions in others, which is often referred to as "social blindness". Treating this condition typically requires intensive one-on-one or small -group therapy sessions, which can be costly and limited in terms of availability. With the growing number of diagnoses of ASD, concerns have risen regarding a potential "lost generation"that may face difficulties in fulfilling its potential. Through this comprehensive review, we aim to provide an overview of innovative approaches that use XR technologies to improve the learning experience of individuals with ASD.
C1 [Poglitsch, Christian; Safikhani, Saeed; List, Erin; Pirker, Johanna] Graz Univ Technol, IICM, Rechbauer str 12-I, A-8010 Graz, Austria.
   [Poglitsch, Christian] Sandgasse 36-III, A-8010 Graz, Austria.
C3 Graz University of Technology
RP Poglitsch, C (corresponding author), Sandgasse 36-III, A-8010 Graz, Austria.
EM christian.poglitsch@tugraz.at; s.safikhani@tugraz.at;
   erin.list@tugraz.at; johanna.pirker@tugraz.at
FU Austrian Science Fund (FWF) [I 6465-B]
FX This research was funded in whole or in part by the Austrian Science
   Fund (FWF) [I 6465-B].
CR Adnan N, 2018, J Adv Res Dyn Control Syst, V10, P26
   Akçayir M, 2017, EDUC RES REV-NETH, V20, P1, DOI 10.1016/j.edurev.2016.11.002
   Almurashi H, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22031250
   [Anonymous], 2013, Diagnostic and statistical manual of mental disorders, V5th, DOI 10.1176/appi.books.9780890425596
   Baron-Cohen S, 2001, J CHILD PSYCHOL PSYC, V42, P241, DOI 10.1111/1469-7610.00715
   BARONCOHEN S, 1989, J CHILD PSYCHOL PSYC, V30, P285, DOI 10.1111/j.1469-7610.1989.tb00241.x
   Bellani M, 2011, EPIDEMIOL PSYCH SCI, V20, P235, DOI 10.1017/S2045796011000448
   Berenguer C, 2020, INT J ENV RES PUB HE, V17, DOI 10.3390/ijerph17176143
   Borish M, 2014, Utilizing real-time human-assisted virtual humans to increase real-world interaction empathy
   Bradley R, 2018, J ENABLING TECHNOL, V12, P101, DOI 10.1108/JET-01-2018-0004
   Bravou V, 2022, RETOS-NUEV TEND EDUC, P779
   Brewer N, 2017, J AUTISM DEV DISORD, V47, P1927, DOI 10.1007/s10803-017-3080-x
   Chen CH, 2016, COMPUT HUM BEHAV, V55, P477, DOI 10.1016/j.chb.2015.09.033
   Chen CH, 2015, RES DEV DISABIL, V36, P396, DOI 10.1016/j.ridd.2014.10.015
   Chen YH, 2022, NEUROSCI BIOBEHAV R, V138, DOI 10.1016/j.neubiorev.2022.104683
   Cheng YC, 2024, J AUTISM DEV DISORD, V54, P1317, DOI 10.1007/s10803-022-05878-4
   Chung S, 2019, ADJUNCT PROCEEDINGS OF THE 2019 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT 2019), P435, DOI 10.1109/ISMAR-Adjunct.2019.00049
   Cooper J, 2001, BRIT J PSYCHIAT, V179, P85, DOI 10.1192/bjp.179.1.85-a
   Daud NFNM, 2023, INT J ADV COMPUT SC, V14, P961
   de Oliveira TR, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3565020
   Dechsling A, 2022, J AUTISM DEV DISORD, V52, P4692, DOI 10.1007/s10803-021-05338-5
   Didehbani N, 2016, COMPUT HUM BEHAV, V62, P703, DOI 10.1016/j.chb.2016.04.033
   Ekman P., 1975, Unmasking the Face: A Guide to Recognizing Emotions from Facial Expressions
   Elkin TD, 2022, BRAIN SCI, V12, DOI 10.3390/brainsci12111568
   Endres D, 2017, MOL AUTISM, V8, DOI 10.1186/s13229-017-0122-3
   Faso DJ, 2015, J AUTISM DEV DISORD, V45, P75, DOI 10.1007/s10803-014-2194-7
   Guarnera M, 2015, EUR J PSYCHOL, V11, P183, DOI 10.5964/ejop.v11i2.890
   Huang YC, 2019, LECT NOTES COMPUT SC, V11575, P283, DOI 10.1007/978-3-030-21565-1_19
   Hughes CE, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.968312
   Ip HHS, 2018, COMPUT EDUC, V117, P1, DOI 10.1016/j.compedu.2017.09.010
   Karami B, 2021, FRONT PSYCHIATRY, V12, DOI 10.3389/fpsyt.2021.665326
   Keating CT, 2020, CHILD ADOL PSYCH CL, V29, P557, DOI 10.1016/j.chc.2020.02.006
   Khalil A, 2020, IEEE ACCESS, V8, P130751, DOI 10.1109/ACCESS.2020.3006051
   Khowaja K., 2019, 2019 IEEE 6 INT C EN, P1, DOI [DOI 10.1109/ICETAS48360.2019.9117290, 10.1109/ICETAS48360.2019.9117290]
   Khowaja K, 2020, IEEE ACCESS, V8, P78779, DOI 10.1109/ACCESS.2020.2986608
   Kourtesis P, 2023, BEHAV SCI-BASEL, V13, DOI 10.3390/bs13040336
   Lerna A, 2012, INT J LANG COMM DIS, V47, P609, DOI 10.1111/j.1460-6984.2012.00172.x
   Li JJ, 2023, INT J HUM-COMPUT ST, V175, DOI 10.1016/j.ijhcs.2023.103032
   Lian XJ, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11104550
   Lim JZ, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20082384
   Lin T, 2020, P 13 ACM INT C PERVA, P1
   Liu RP, 2017, FRONT PEDIATR, V5, DOI 10.3389/fped.2017.00145
   Lorenzo G, 2016, COMPUT EDUC, V98, P192, DOI 10.1016/j.compedu.2016.03.018
   Lorenzo GG, 2023, EDUC INF TECHNOL, V28, P9557, DOI 10.1007/s10639-022-11545-z
   Marto Anabela, 2019, VipIMAGE 2019. Proceedings of the VII ECCOMAS Thematic Conference on Computational Vision and Medical Image Processing. Lecture Notes in Computational Vision and Biomechanics (LNCVB 34), P454, DOI 10.1007/978-3-030-32040-9_46
   Mehta D, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18020416
   Mekbib E, 2021, Designing a smart virtual environment for autism spectrum disorder detection, P288, DOI [10.1109/SWC50871.2021.00047, DOI 10.1109/SWC50871.2021.00047]
   Mesa-Gresa P, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18082486
   Miao Y, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3311747
   MILGRAM P, 1994, P SOC PHOTO-OPT INS, V2351, P282
   Miningrum T, 2021, INT J ADV COMPUT SC, V12, P632
   Moher D, 2015, SYST REV-LONDON, V4, DOI [10.1186/2046-4053-4-1, 10.1371/journal.pmed.1000097, 10.1136/bmj.i4086, 10.1016/j.ijsu.2010.02.007, 10.1136/bmj.b2535, 10.1016/j.ijsu.2010.07.299, 10.1136/bmj.b2700]
   Morgan B, 2003, DEV PSYCHOL, V39, P646, DOI 10.1037/0012-1649.39.4.646
   Mosher MA, 2022, REV J AUTISM DEV DIS, V9, P334, DOI 10.1007/s40489-021-00259-6
   Murre JMJ, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0120644
   Newbutt N, 2023, J AUTISM DEV DISORD, DOI 10.1007/s10803-023-06130-3
   Newbutt N, 2022, J ENABLING TECHNOL, V16, P124, DOI 10.1108/JET-01-2022-0010
   Paletta L, 2014, Smartphone eye tracking toolbox: Accurate gaze recovery on mobile displays, DOI [10.1145/2578153.2628813, DOI 10.1145/2578153.2628813]
   Parsons S, 2016, EDUC RES REV-NETH, V19, P138, DOI 10.1016/j.edurev.2016.08.001
   Plutchik R, 2001, AM SCI, V89, P344, DOI 10.1511/2001.28.739
   Prizant BM, 2003, INFANT YOUNG CHILD, V16, P296, DOI 10.1097/00001163-200310000-00004
   Rauschnabel PA, 2022, COMPUT HUM BEHAV, V133, DOI 10.1016/j.chb.2022.107289
   Robinson S, 2009, BRAIN COGNITION, V71, P362, DOI 10.1016/j.bandc.2009.06.007
   Romero Pazmino M, 2020, Augmented reality for children with autism spectrum disorder-A systematic review, P1, DOI [10.1109/ISCV49265.2020.9204125, DOI 10.1109/ISCV49265.2020.9204125]
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Schmidt M, 2024, AUTISM, V28, P1809, DOI 10.1177/13623613231208579
   Schwarze A, 2019, EUR C INF SYST
   Serrat O., 2009, Knowledge Solutions, V49, P1
   Stallmann L, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.826241
   Voss C, 2019, JAMA PEDIATR, V173, P446, DOI 10.1001/jamapediatrics.2019.0285
   Waizbard-Bartov E, 2023, AUTISM RES, V16, P685, DOI 10.1002/aur.2898
   Wan GB, 2022, COMPUT INTEL NEUROSC, V2022, DOI 10.1155/2022/9213526
   Wedyan M, 2021, MULTIMODAL TECHNOLOG, V5, DOI 10.3390/mti5080048
   Yuan SNV, 2018, LONDON J PRIM CARE, V10, P110, DOI 10.1080/17571472.2018.1483000
   Zeidan J, 2022, AUTISM RES, V15, P778, DOI 10.1002/aur.2696
NR 75
TC 0
Z9 0
U1 3
U2 3
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD JUN
PY 2024
VL 121
AR 103942
DI 10.1016/j.cag.2024.103942
EA MAY 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UG5B5
UT WOS:001246907400001
OA hybrid
DA 2024-08-05
ER

PT J
AU Ayala, N
   Mardanbegi, D
   Zafar, A
   Niechwiej-Szwedo, E
   Cao, S
   Kearns, S
   Irving, E
   Duchowski, AT
AF Ayala, Naila
   Mardanbegi, Diako
   Zafar, Abdullah
   Niechwiej-Szwedo, Ewa
   Cao, Shi
   Kearns, Suzanne
   Irving, Elizabeth
   Duchowski, Andrew T.
TI Does fiducial marker visibility impact task performance and information
   processing in novice and low-time pilots?
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Eye tracking; Augmented reality; Aviation
ID EXPERTISE; DISTRACTION; STRATEGIES; PERCEPTION; BEHAVIOR; ENTROPY
AB Invisible fiducial markers are introduced for localization of Areas Of Interest (AOIs) in mobile eye tracking applications. Fiducial markers are made invisible through the use of film passing Infra -Red (IR) light while blocking the visible spectrum. An IR light source is used to illuminate the markers which are then detected by an IR-sensitive camera, but which are imperceptible by the human eye. We provide the first empirical study that demonstrates such invisible markers are not distracting to a given task, as demonstrated in a flight simulator where distraction of visible and invisible markers are compared between experienced and novice pilots. Fixation frequency and subjective distraction scores showed that visible markers disrupted natural gaze behavior, particularly in novice pilots. Our findings show that invisible markers should be used when there is a need for them to remain inconspicuous.
C1 [Ayala, Naila; Zafar, Abdullah; Niechwiej-Szwedo, Ewa; Cao, Shi; Kearns, Suzanne; Irving, Elizabeth] Univ Waterloo, Waterloo, ON, Canada.
   [Mardanbegi, Diako] AdHawk Microsyst, Waterloo, ON, Canada.
   [Duchowski, Andrew T.] Clemson Univ, Sch Comp, 100 McAdams Hall, Clemson, SC 29634 USA.
C3 University of Waterloo; Clemson University
RP Ayala, N (corresponding author), Univ Waterloo, Waterloo, ON, Canada.
EM nayala@uwaterloo.ca; diako@adhawkmicrosystems.com;
   a35zafar@uwaterloo.ca; eniechwiej@uwaterloo.ca; shi.cao@uwaterloo.ca;
   suzanne.kearns@uwaterloo.ca; elizabeth.irving@uwaterloo.ca;
   duchowski@clemson.edu
OI Ayala, Naila/0000-0002-6817-1629
FU Natural Sciences and Engineering Research Council (NSERC) of Canada; New
   Frontiers in Research Fund, Canada
FX This work is supported in part by the Canadian Graduate Scholarship
   (CGS) from the Natural Sciences and Engineering Research Council (NSERC)
   of Canada, and the Exploration Grant from the New Frontiers in Research
   Fund, Canada. Any opinions, findings and conclusions or recommendations
   expressed in this material are the author (s) and do not necessarily
   reflect those of the sponsors.
CR Ayala N, 2023, J EYE MOVEMENT RES, V16, DOI 10.16910/jemr.16.1.3
   Ayala N, 2022, VISION RES, V199, DOI 10.1016/j.visres.2022.108072
   Balslev T, 2012, EUR J PAEDIATR NEURO, V16, P161, DOI 10.1016/j.ejpn.2011.07.004
   Bornard JC, 2011, P 1 INT S DIG HUM MO
   Bykowski A, 2018, SIG P ALGO ARCH ARR, P255, DOI 10.23919/SPA.2018.8563387
   Caserman P, 2022, IEEE T GAMES, V14, P706, DOI 10.1109/TG.2022.3148791
   Ciuperca G, 2007, COMMUN STAT-THEOR M, V36, P2543, DOI 10.1080/03610920701270964
   Craig A., 2013, Understanding augmented reality: Concepts and applications, DOI [10.1016/C2011-0-07249-6, DOI 10.1016/C2011-0-07249-6]
   Di Nocera F, 2007, J COGN ENG DECIS MAK, V1, P271, DOI 10.1518/155534307X255627
   Donovan T, 2013, APPL COGNITIVE PSYCH, V27, P43, DOI 10.1002/acp.2869
   Duchowski Andrew T., 2020, Procedia Computer Science, V176, P3771, DOI 10.1016/j.procs.2020.09.010
   Ehambram A, 2019, ICINCO: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON INFORMATICS IN CONTROL, AUTOMATION AND ROBOTICS, VOL 2, P190, DOI 10.5220/0007810301900197
   Ellsworth JJ, 2016, US patent application publication, Patent No. [US2016/0339337A1, 20160339337]
   Garrido-Jurado S, 2014, PATTERN RECOGN, V47, P2280, DOI 10.1016/j.patcog.2014.01.005
   Garrison TM, 2013, APPL COGNITIVE PSYCH, V27, P396, DOI 10.1002/acp.2917
   Gegenfurtner A, 2011, EDUC PSYCHOL REV, V23, P523, DOI 10.1007/s10648-011-9174-7
   Godwin HJ, 2015, VIS COGN, V23, P415, DOI 10.1080/13506285.2015.1030488
   Haider H, 1999, J EXP PSYCHOL-APPL, V5, P129
   Hales J., 2013, P 3 INT WORKSH PERV, V6
   Hancock PA, 2009, Hum Factors Simul Train, P169
   Harbluk JL, 2007, ACCIDENT ANAL PREV, V39, P372, DOI 10.1016/j.aap.2006.08.013
   Harris R. L., 1986, ANAL TECHNIQUES PILO
   Hedlund J, 2006, INT C DISTR DRIV
   Helleberg JR, 2003, INT J AVIAT PSYCHOL, V13, P189, DOI 10.1207/S15327108IJAP1303_01
   Jaarsma T, 2015, ADV HEALTH SCI EDUC, V20, P1089, DOI 10.1007/s10459-015-9589-x
   Kaplan AD, 2021, HUM FACTORS, V63, P706, DOI 10.1177/0018720820904229
   Khor WS, 2016, ANN TRANSL MED, V4, DOI 10.21037/atm.2016.12.23
   Kirby CE, 2014, AVIAT SPACE ENVIR MD, V85, P740, DOI 10.3357/ASEM.3888.2014
   Koike H, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P163
   Koshikawa Koki, 2022, Proceedings of the ACM on Human-Computer Interaction, V6, DOI 10.1145/3530888
   Krejtz K., 2014, P S EYE TRACK RES AP, P159, DOI [10.1145/2578153.2578176, DOI 10.1145/2578153.2578176]
   Krejtz K, 2015, ACM T APPL PERCEPT, V13, DOI 10.1145/2834121
   Kundel HL, 2007, RADIOLOGY, V242, P396, DOI 10.1148/radiol.2422051997
   Kunz Christian, 2020, Current Directions in Biomedical Engineering, V6, DOI 10.1515/cdbme-2020-0027
   Lee WS, 1998, IEEE INT CONF ROBOT, P71, DOI 10.1109/ROBOT.1998.676264
   Marti P., 2021, P DRIV SIM C EUR 202, P99
   Martinez-Marquez D, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21134289
   McKnight RR, 2020, CURR REV MUSCULOSKE, V13, P663, DOI 10.1007/s12178-020-09667-3
   Munoz-Salinas R, 2013, PLoS One, V13
   Nyström M, 2010, BEHAV RES METHODS, V42, P188, DOI 10.3758/BRM.42.1.188
   Peissl S, 2018, INT J AEROSP PSYCHOL, V28, P98, DOI 10.1080/24721840.2018.1514978
   Rantanen E., 2005, P HUMAN FACTORS ERGO, P764
   Rowe I, 2013, RAIL HUMAN FACTORS: SUPPORTING RELIABILITY, SAFETY AND COST REDUCTION, P262
   Sarter NB, 2007, HUM FACTORS, V49, P347, DOI 10.1518/001872007X196685
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x
   Shiferaw B, 2019, NEUROSCI BIOBEHAV R, V96, P353, DOI 10.1016/j.neubiorev.2018.12.007
   Spitz J, 2016, COGN RES, V1, DOI 10.1186/s41235-016-0013-8
   Velosa F, 2020, ICAI WORKSH, P15
   Vickers JN., 2017, Oxford Research Encyclopedia of Psychology, P1, DOI 10.1093/acrefore/9780190236557.013.161
   Waller MC, 1976, Tech. rep.
   Wang L, 2016, CUREUS J MED SCIENCE, V8, DOI 10.7759/cureus.694
   Wood G, 2013, ATTEN PERCEPT PSYCHO, V75, P830, DOI 10.3758/s13414-013-0489-y
   Ziv G, 2017, INT J AVIAT PSYCHOL, V26, P75, DOI 10.1080/10508414.2017.1313096
NR 53
TC 0
Z9 0
U1 1
U2 1
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103889
DI 10.1016/j.cag.2024.103889
EA FEB 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LK6K7
UT WOS:001186731800001
OA hybrid
DA 2024-08-05
ER

PT J
AU Xia, ZX
   Hao, JY
   Li, K
   Tian, AX
   Du, ZJ
AF Xia, Zi-Xun
   Hao, Jian-Yu
   Li, Kang
   Tian, Ao-Xiang
   Du, Zheng-Jun
TI Edit propagation via color palettes
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Edit propagation; Color palette; Optimization; Color editing
ID IMAGES
AB The ability to ease operation and real-time feedback to image editing has attracted increasing attention in our everyday lives. Existing edit propagation approaches typically formulate the color editing task as a quadratic energy optimization problem, which is computationally expensive and requires a lot of user interactions. To address this problem, in this paper, we propose a novel edit propagation method based on color palette. We first extract a color palette from the input image, and then calculate the mixing weights of the image pixels with respect to the extracted palette. Finally, we solve for an edited palette according to user edits, to propagate local edits to the whole image. Our main contributions include formulating the edit propagation as a palette-based optimization problem that can be solved efficiently, and requiring only fewer user interactions than existing methods. We have demonstrated our method for color editing on a wide range of examples. Compared to existing methods, our approach is more efficient and friendly for novice users due to its ease of interaction.
C1 [Xia, Zi-Xun; Hao, Jian-Yu; Li, Kang; Tian, Ao-Xiang; Du, Zheng-Jun] Qinghai Univ, Dept Comp Technol & Applicat, Xining 810016, Peoples R China.
   [Du, Zheng-Jun] Qinghai Prov Key Lab Media Integrat Technol & Comm, Xining 810016, Peoples R China.
C3 Qinghai University
RP Du, ZJ (corresponding author), Qinghai Univ, Dept Comp Technol & Applicat, Xining 810016, Peoples R China.
EM dzj@qhu.edu.cn
OI Du, Zheng-Jun/0000-0002-6763-2892
FU Youth Program of the Natural Science Foundation of Qinghai Province
   [2023-ZJ-951Q]
FX We appreciate the anonymous reviewers for their valuable comments and
   suggestions. This work is supported by the Youth Program of the Natural
   Science Foundation of Qinghai Province (Project Number: 2023-ZJ-951Q) .
CR Akimoto Naofumi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8274, DOI 10.1109/CVPR42600.2020.00830
   Aksoy Y, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3002176
   An XH, 2008, ADVANCES IN MATRIX THEORY AND ITS APPLICATIONS, VOL II, P1, DOI 10.1145/1399504.1360639
   Bie XH, 2011, COMPUT GRAPH FORUM, V30, P2041, DOI 10.1111/j.1467-8659.2011.02059.x
   Chang HW, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766978
   Chao CKT, 2023, COMPUT GRAPH FORUM, V42, DOI 10.1111/cgf.14892
   Chao CKT, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592405
   Chen XB, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185525
   Chen XW, 2014, PROC CVPR IEEE, pCP5, DOI 10.1109/CVPR.2014.365
   Cho J, 2017, IEEE COMPUT SOC CONF, P1058, DOI 10.1109/CVPRW.2017.143
   Du ZJ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459675
   Endo Y, 2016, COMPUT GRAPH FORUM, V35, P189, DOI 10.1111/cgf.12822
   Gui Y, 2020, VISUAL COMPUT, V36, P469, DOI 10.1007/s00371-019-01633-6
   Gurobi Optimization LLC, 2023, GUROBI OPTIMIZER REF
   Ju T, 2005, ACM T GRAPHIC, V24, P561, DOI 10.1145/1073204.1073229
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Li F, 2019, CMC-COMPUT MATER CON, V61, P643, DOI 10.32604/cmc.2019.06094
   Li Y, 2010, COMPUT GRAPH FORUM, V29, P2049, DOI 10.1111/j.1467-8659.2010.01791.x
   Lischinski D, 2006, ACM T GRAPHIC, V25, P646, DOI 10.1145/1141911.1141936
   Pellacini F, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239505, 10.1145/1276377.1276444]
   Sun Q, 2023, IEEE Trans Vis Comput Graphics
   Tan JC, 2018, Arxiv, DOI arXiv:1804.01225
   Tan JC, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275054
   Tan JC, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2988229
   Wang YL, 2019, COMPUT GRAPH FORUM, V38, P11, DOI 10.1111/cgf.13812
   Xiao CX, 2011, IEEE T VIS COMPUT GR, V17, P1135, DOI 10.1109/TVCG.2010.125
   Xu K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618464
   Xu K, 2009, COMPUT GRAPH FORUM, V28, P1871, DOI 10.1111/j.1467-8659.2009.01565.x
   Zhang Q, 2022, IEEE T MULTIMEDIA, V24, P1545, DOI 10.1109/TMM.2021.3067463
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 30
TC 0
Z9 0
U1 3
U2 3
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103875
DI 10.1016/j.cag.2024.01.002
EA FEB 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LK5S8
UT WOS:001186713700001
DA 2024-08-05
ER

PT J
AU Castellani, U
   Bartolomioli, R
   Marchioro, G
   Calomino, D
AF Castellani, Umberto
   Bartolomioli, Riccardo
   Marchioro, Giacomo
   Calomino, Dario
TI From coin to 3D face sculpture portraits in the round of Roman emperors
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE 3D face; Morphable model; Model fitting; 3D scanning
ID MORPHABLE MODEL; RECONSTRUCTION
AB Representing historical figures on visual media has always been a crucial aspect of political communication in the ancient world, as it is in modern society. A great example comes from ancient Rome, when the emperor's portraits were serially replicated on visual media to disseminate his image across the countries ruled by the Romans and to assert the power and authority that he embodied by making him universally recognizable. In particular, one of the most common media through which ancient Romans spread the imperial image was coinage, which showed a bi-dimensional projection of his portrait on the very low relief produced by the impression of the coin-die. In this work, we propose a new method that uses a multi-modal 2D and 3D approach to reconstruct the full portrait in the round of Roman emperors from their images adopted on ancient coins. A well-defined pipeline is introduced from the digitization of coins using 3D scanning techniques to the estimation of the 3D model of the portrait represented by a polygonal mesh. A morphable model trained on real 3D faces is exploited to infer the morphological (i.e., geometric) characteristics of the Roman emperor from the contours extracted from a coin portrait using a model fitting procedure. We present examples of face reconstruction of different emperors from coins produced in Rome as well as in the imperial provinces, which sometimes showed local variations of the official portraits centrally designed.
C1 [Castellani, Umberto; Bartolomioli, Riccardo] Univ Verona, Dept Comp Sci, I-37134 Verona, Italy.
   [Marchioro, Giacomo; Calomino, Dario] Univ Verona, Dept Cultures & Civilisat, I-37129 Verona, Italy.
C3 University of Verona; University of Verona
RP Castellani, U (corresponding author), Univ Verona, Dept Comp Sci, I-37134 Verona, Italy.
EM umberto.castellani@univr.it
FU European Research Council (ERC) under the European Union [101002763]
FX This paper is published within the project RESP (The Roman Emperor Seen
   From the Provinces) , which has received funding from the European
   Research Council (ERC) under the European Union's Horizon 2020 research
   and innovation programme (grant agreement no. 101002763) . The authors
   wish to thank Richard Abdy, curator in the Department of Coins and
   Medals at the British Museum, and Antonella Arzone, curator in the
   Museum of Castelevecchio in Verona, for allowing the 3D scanning of
   coins in the respective collections. The scans in the British Museum
   were performed by Mike Donnelly, Paul Wilson and Mark Williams from the
   WMG at Warwick University (using a Nikon ModelMaker H120 scan head
   mounted on a portable MCAx25+ scanning arm) ; in the Castelvecchio
   Museum they were carried out by Giacomo Marchioro (with a conoscopic
   holography microprofilometer developed at the OpDaTeCH laboratory of the
   University of Verona lead by Claudia Daffara) .
CR Abate AF, 2004, J VISUAL LANG COMPUT, V15, P373, DOI 10.1016/j.jvlc.2003.11.004
   Arzone A, 2017, Sylloge Nummorum Graecorum Italia
   Bai HR, 2023, PROC CVPR IEEE, P362, DOI 10.1109/CVPR52729.2023.00043
   Bas A, 2017, LECT NOTES COMPUT SC, V10117, P377, DOI 10.1007/978-3-319-54427-4_28
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Botsch M, 2010, Polygon Mesh Processing, DOI DOI 10.1201/B10688
   Burnett A, 1986, Coinage in the Roman world
   Calomino D, 2023, BRITANNIA-CAMBRIDGE, DOI 10.1017/S0068113X23000387
   Duda RO, 2001, Pattern classification, V2nd
   Egger B, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3395208
   Elkhuizen WS, 2019, HERIT SCI, V7, DOI 10.1186/s40494-019-0331-5
   Ferrari C, 2017, IEEE T MULTIMEDIA, V19, P2666, DOI 10.1109/TMM.2017.2707341
   Fittschen K., 2011, The portraits of Roman emperors and their families: Controversial positions and unsolved problems
   Gaburro N, 2017, PROC SPIE, V10331, DOI 10.1117/12.2270307
   Galteri L, 2019, COMPUT VIS IMAGE UND, V185, P31, DOI 10.1016/j.cviu.2019.05.002
   Horta R, 2016, J CRANIOFAC SURG, V27, pE473, DOI 10.1097/SCS.0000000000002788
   Hu GS, 2017, PATTERN RECOGN, V67, P366, DOI 10.1016/j.patcog.2017.02.007
   Huber Patrik, 2016, VISIGRAPP 2016. 11th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications. Proceedings: VISAPP 2016, P79
   Jackson AS, 2017, IEEE I CONF COMP VIS, P1031, DOI 10.1109/ICCV.2017.117
   Ji ZP, 2021, COMPUT AIDED DESIGN, V130, DOI 10.1016/j.cad.2020.102928
   Lanitis A, 2009, 2009 15TH INTERNATIONAL CONFERENCE ON VIRTUAL SYSTEMS AND MULTIMEDIA PROCEEDINGS (VSMM 2009), P15, DOI 10.1109/VSMM.2009.8
   Levy B, 2010, ACM SIGGRAPH 2010 courses, P1, DOI DOI 10.1145/1837101.1837109
   Li TY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130813
   Maim Jonathan., 2007, Proceedings of the 8th International conference on Virtual Reality, Archaeology and Intelligent Cultural Heritage, P109, DOI [10.2312/VAST/VAST07/109-116, DOI 10.2312/VAST/VAST07/109-116]
   Mattingly H, 1923, The Roman Imperial Coinage, P1
   Mattingly H, 1923, British Museum Coins of the Roman Empire, P1
   Pintus R, 2016, COMPUT GRAPH FORUM, V35, P4, DOI 10.1111/cgf.12668
   Ploumpis S, 2019, PROC CVPR IEEE, P10926, DOI 10.1109/CVPR.2019.01119
   Sariyanidi E, 2024, IEEE T PATTERN ANAL, V46, P1305, DOI 10.1109/TPAMI.2023.3334948
   Sipiran I, 2021, COMPUT GRAPH-UK, V100, P1, DOI 10.1016/j.cag.2021.07.010
   Wilkinson CM, 2023, J ARCHAEOL SCI, V160, DOI 10.1016/j.jas.2023.105884
   Xu Haoxin, 2024, WWW '24: Companion Proceedings of the ACM on Web Conference 2024, P633, DOI 10.1145/3589335.3651460
   Xu W, 2005, 6 INT S VIRT REAL AR, P59, DOI [10.2312/VAST/VAST05/059-065, DOI 10.2312/VAST/VAST05/059-065]
   Yazli NC, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2075
NR 34
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD OCT
PY 2024
VL 123
SI SI
AR 103999
DI 10.1016/j.cag.2024.103999
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZW4C7
UT WOS:001278300700001
OA hybrid
DA 2024-08-05
ER

PT J
AU Yang, JM
   Da, F
   Hong, R
AF Yang, Jiming
   Da, Feipeng
   Hong, Ru
TI Self-supervised domain adaptation on point clouds via homomorphic
   augmentation ☆
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Domain adaptation; Self-supervise; Point clouds
AB With the widespread application of 3D data, the demand for 3D data annotation is increasing. However, 3D models often experience performance degradation in the target domain due to differences in data distribution. Unsupervised domain adaptation is considered an effective solution to this problem. In this paper, we propose a novel unsupervised domain adaptation method for 3D point clouds. Specifically, to better learn the representation that captures the pattern of the target domain, we devise a self-supervised learning framework based on contrastive learning. This framework is designed to effectively learn feature representations in a large-scale unlabeled target domain while enhancing resilience to noise and variations. Additionally, beyond generating a chain of intermediate domains to improve transfer process stability, we propose the Intermediate Homomorphic Feature Augmentation Approach (IHFA). IHFA generates augmented source domain features corresponding to intermediate domains, facilitating the model's acquisition of stronger transferable classification capabilities. Furthermore, we propose an unsupervised generation strategy of complementary labels to provide additional information for training. Extensive experiments on two benchmarks, PointDA-10 and GraspNetPC-10 demonstrate the effectiveness and superiority of our method.
C1 [Yang, Jiming; Da, Feipeng; Hong, Ru] Southeast Univ, Sch Automat, Nanjing, Jiangsu, Peoples R China.
   [Yang, Jiming; Hong, Ru] Minist Educ, Key Lab Measurement & Control Complex Syst Engn, Nanjing, Jiangsu, Peoples R China.
C3 Southeast University - China
RP Da, F (corresponding author), Southeast Univ, Sch Automat, Nanjing, Jiangsu, Peoples R China.
EM dafp@seu.edu.cn
CR Achituve I, 2021, IEEE WINT CONF APPL, P123, DOI 10.1109/WACV48630.2021.00017
   Ahmed S. M., 2021, P IEEE CVF C COMP VI, P10103
   Vo AV, 2015, ISPRS J PHOTOGRAMM, V104, P88, DOI 10.1016/j.isprsjprs.2015.01.011
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Chen CQ, 2019, PROC CVPR IEEE, P627, DOI 10.1109/CVPR.2019.00072
   Ding N, 2022, PROC CVPR IEEE, P7202, DOI 10.1109/CVPR52688.2022.00707
   Fan HH, 2022, PROC CVPR IEEE, P6367, DOI 10.1109/CVPR52688.2022.00627
   Fang H.S., 2020, P IEEECVF C COMPUTER, P11441
   Feng L, 2019, 25TH AMERICAS CONFERENCE ON INFORMATION SYSTEMS (AMCIS 2019)
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gao W, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108233
   Gholenji E, 2020, APPL INTELL, V50, P2050, DOI 10.1007/s10489-019-01610-5
   He CM, 2023, APPL INTELL, V53, P3034, DOI 10.1007/s10489-022-03709-8
   Hsu HK, 2020, IEEE WINT CONF APPL, P738, DOI [10.1109/wacv45572.2020.9093358, 10.1109/WACV45572.2020.9093358]
   Ishida T, 2017, ADV NEUR IN, V30
   Ishida Takashi, 2019, PR MACH LEARN RES
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kim Y, 2019, IEEE I CONF COMP VIS, P101, DOI 10.1109/ICCV.2019.00019
   Kundu JN, 2020, PROC CVPR IEEE, P4543, DOI 10.1109/CVPR42600.2020.00460
   Li S, 2021, PROC CVPR IEEE, P11511, DOI 10.1109/CVPR46437.2021.01135
   Liang HX, 2022, LECT NOTES COMPUT SC, V13663, P156, DOI 10.1007/978-3-031-20062-5_10
   Liang J., 2020, International Conference on Machine Learning, P6028
   Litrico M, 2023, Arxiv, DOI arXiv:2303.03770
   Luo ZP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8846, DOI 10.1109/ICCV48922.2021.00874
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Niu YC, 2024, IMAGE VISION COMPUT, V142, DOI 10.1016/j.imavis.2024.104916
   Papon J, 2013, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR.2013.264
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qin C, 2019, ADV NEUR IN, V32
   Sauder J, 2019, ADV NEUR IN, V32
   Shen YF, 2022, PROC CVPR IEEE, P7213, DOI 10.1109/CVPR52688.2022.00708
   Song D, 2023, COMPUT GRAPH-UK, V115, P25, DOI 10.1016/j.cag.2023.06.033
   Sun WQ, 2023, IEEE T CIRC SYST VID, V33, P354, DOI 10.1109/TCSVT.2022.3201540
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Wang F, 2022, PROC CVPR IEEE, P7141, DOI 10.1109/CVPR52688.2022.00701
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wang ZC, 2023, IEEE T CIRC SYST VID, V33, P7604, DOI 10.1109/TCSVT.2023.3275950
   Wu ZZ, 2018, COMPUT GRAPH-UK, V70, P140, DOI 10.1016/j.cag.2017.07.013
   Xia HF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8990, DOI 10.1109/ICCV48922.2021.00888
   Xu QG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15426, DOI 10.1109/ICCV48922.2021.01516
   Yang SQ, 2023, Arxiv, DOI arXiv:2010.12427
   Yang SQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8958, DOI 10.1109/ICCV48922.2021.00885
   Yi L, 2021, PROC CVPR IEEE, P15358, DOI 10.1109/CVPR46437.2021.01511
   Yu XY, 2018, LECT NOTES COMPUT SC, V11205, P69, DOI 10.1007/978-3-030-01246-5_5
   Zhang L, 2020, IEEE T NEUR NET LEAR, V31, P3374, DOI 10.1109/TNNLS.2019.2944455
   Zhang XL, 2018, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2018.00144
   Zhang YY, 2022, IEEE T NEUR NET LEAR, V33, P7667, DOI 10.1109/TNNLS.2021.3086093
   Zou LK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6383, DOI 10.1109/ICCV48922.2021.00634
NR 49
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD JUN
PY 2024
VL 121
AR 103948
DI 10.1016/j.cag.2024.103948
EA MAY 2024
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UH4H4
UT WOS:001247147600001
DA 2024-08-05
ER

PT J
AU Wu, YC
   Zhao, HH
   Chen, WH
   Yang, YF
   Bu, JY
AF Wu, Yichun
   Zhao, Huihuang
   Chen, Wenhui
   Yang, Yunfei
   Bu, Jiayi
TI TextStyler: A CLIP-based approach to text-guided style transfer
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Style transfer; CLIP; Text-guided image manipulation
AB Recent research on text -guided image style transfer using CLIP (Contrastive Language -Image Pre -training) models has made good progress. Existing work does not rely on additional generative models, but it cannot guarantee the quality of the generated images, and often suffers from problems such as distortion of content images and uneven stylization of the generated images. To address such problems, this work proposes the TextStyler model, a CLIP -based approach for text -guided style transfer. In the TextStyler model, we propose a style transformation network STNet, which consists of an encoder and a multi -scale decoder. The network can capture the hierarchical features of the content image, and the decoder feature fusion module in the network, designed based on the channel attention mechanism, helps the network to maximize the retention of the detailed information of the content image while realizing texture transfer. In addition, we design a patch -wise perceptual loss, which is able to transfer the stylized texture to each local region of the image and improve the balance of model stylization. The experimental results show that the TextStyler model can achieve a wider range of style transfer than existing methods using stylized images, and the generated artistic images are more in line with human visual perception than state-of-the-art text -guided style transfer methods.
C1 [Wu, Yichun; Zhao, Huihuang; Chen, Wenhui; Yang, Yunfei; Bu, Jiayi] Hengyang Normal Univ, Coll Comp Sci & Technol, Hengyang 421002, Peoples R China.
   [Zhao, Huihuang] Hunan Prov Key Lab Intelligent Informat Proc & App, Hengyang 421002, Hunan, Peoples R China.
C3 Hengyang Normal University
RP Zhao, HH (corresponding author), Hengyang Normal Univ, Coll Comp Sci & Technol, Hengyang 421002, Peoples R China.
EM wjaycyf@gmail.com; happyday.huihuang@gmail.com; whchen@hynu.edu.cn;
   yf196369@gmail.com; coral_bjy@126.com
RI chen, wenhui/HGB-6912-2022
OI wu, yichun/0000-0003-1108-8549
FU National Natural Science Foundation of China [61772179]; Hunan
   Provincial Natural Science Foundation of China [2020JJ4152, 2022JJ50016,
   2023JJ50095]; Science and Technology Innovation Project of Hengyang
   [202250045231]; Scientific Research Fund of Hunan Provincial Education
   Department [22B0728]; The 14th Five-Year Plan Key Disciplines and
   Application-oriented Special Disciplines of Hunan Province [[2022] 351];
   Postgraduate Scientific Research Innovation Project of Hunan Province
   [CX20221285]
FX This work was supported by the National Natural Science Foundation of
   China (61772179) , Hunan Provincial Natural Science Foundation of China
   (2020JJ4152, 2022JJ50016) , the Science and Technology Innovation
   Project of Hengyang (202250045231) , Scientific Research Fund of Hunan
   Provincial Education Department (No. 22B0728) , Hunan Provincial Natural
   Science Foundation of China (No. 2023JJ50095) , The 14th Five-Year Plan
   Key Disciplines and Application-oriented Special Disciplines of Hunan
   Province (Xiangjiaotong [2022] 351) and Postgraduate Scientific Research
   Innovation Project of Hunan Province (CX20221285) .
CR [Anonymous], 2022, Disco Diffusion
   Avrahami O, 2022, PROC CVPR IEEE, P18187, DOI 10.1109/CVPR52688.2022.01767
   Bau D, 2021, Arxiv, DOI arXiv:2103.10951
   Chen HB, 2021, ADV NEUR IN, V34
   Deng YY, 2022, PROC CVPR IEEE, P11316, DOI 10.1109/CVPR52688.2022.01104
   Deng YY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2719, DOI 10.1145/3394171.3414015
   Ding KY, 2022, IEEE T PATTERN ANAL, V44, P2567, DOI 10.1109/TPAMI.2020.3045810
   Gal R, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530164
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Goodfellow I. J., 2014, ADV NEURAL INFORM PR
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Katherine Crowson, 2021, Vqgan+clip
   Kim G, 2022, PROC CVPR IEEE, P2416, DOI 10.1109/CVPR52688.2022.00246
   Kong XY, 2024, IEEE T NEUR NET LEAR, V35, P8482, DOI 10.1109/TNNLS.2022.3230084
   Kotovenko D, 2021, PROC CVPR IEEE, P12191, DOI 10.1109/CVPR46437.2021.01202
   Kwon G, 2022, PROC CVPR IEEE, P18041, DOI 10.1109/CVPR52688.2022.01753
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li YJ, 2017, ADV NEUR IN, V30
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu SH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6629, DOI 10.1109/ICCV48922.2021.00658
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Park DY, 2019, PROC CVPR IEEE, P5873, DOI 10.1109/CVPR.2019.00603
   Patashnik O, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2065, DOI 10.1109/ICCV48922.2021.00209
   Phillips F, 2011, ISS ACCOUNT EDUC, V26, P593, DOI 10.2308/iace-50038
   Radford A, 2021, PR MACH LEARN RES, V139
   Risser E, 2017, Arxiv, DOI arXiv:1701.08893
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ryan Murdock, 2021, The big sleep
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song YR, 2023, AAAI CONF ARTIF INTE, P2312
   Sun JX, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2290, DOI 10.1145/3474085.3475391
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Tao M, 2023, PROC CVPR IEEE, P14214, DOI 10.1109/CVPR52729.2023.01366
   Ulyanov D, 2017, PROC CVPR IEEE, P4105, DOI 10.1109/CVPR.2017.437
   Wu ZJ, 2022, LECT NOTES COMPUT SC, V13676, P189, DOI 10.1007/978-3-031-19787-1_11
   Yoo J, 2019, IEEE I CONF COMP VIS, P9035, DOI 10.1109/ICCV.2019.00913
   Zhang YX, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3605548
   Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1
NR 39
TC 0
Z9 0
U1 5
U2 5
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103887
DI 10.1016/j.cag.2024.103887
EA FEB 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MY0B9
UT WOS:001197068600001
DA 2024-08-05
ER

PT J
AU Cui, TY
   Wang, YF
   Yang, YJ
   Wang, YH
AF Cui, Tengyao
   Wang, Yongfang
   Yang, Yingjie
   Wang, Yihan
TI GLHDR: HDR video reconstruction driven by global to local alignment
   strategy
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE High Dynamic Range video; Alternating exposure; Reconstruction
AB Reconstructing High Dynamic Range (HDR) video from alternating exposure Low Dynamic Range (LDR) sequence is an exceptionally challenging task. It not only demands the reliable reconstruction of missing information caused by occlusion or motion without introducing artifacts but also balances the exposure differences between frames to ensure a visually pleasing reconstructed HDR video. Unfortunately, existing methods are typically complex and struggle with unavoidable artifacts and noise, especially when dealing with low -exposed scenes. To tackle this formidable challenge, we propose a two -stage HDR video reconstruction method that employs a global to local alignment strategy. Firstly, we utilize iterative optical flow estimation and hybrid weighting to achieve global alignment, ensuring well -reconstructed in majority of areas. Secondly, the recursive refinement network further addresses locally misaligned areas, reconstructing HDR frames from bottom to top and recursively refining them to yield faithful reconstruction results. Extensive experimental results demonstrate that our method generates the HDR video with fine details and superior visually, surpassing the state-of-the-art method across diverse scenes.
C1 [Cui, Tengyao; Wang, Yongfang; Yang, Yingjie; Wang, Yihan] Shanghai Univ, Sch Commun & Informat Engn, Shanghai, Peoples R China.
   [Cui, Tengyao; Wang, Yongfang; Wang, Yihan] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai, Peoples R China.
C3 Shanghai University; Shanghai University
RP Wang, YF (corresponding author), 99 Shangda Rd, Shanghai, Peoples R China.
EM tyaocui@shu.edu.cn; yfw@shu.edu.cn; yyj145236@shu.edu.cn;
   wangyihan@shu.edu.cn
FU National Natural Science Foundation of China [61671283]
FX <B>Acknowledgment</B> This work is supported by the National Natural
   Science Foundation of China under Grant No. 61671283.
CR Beek PV, 2019, Electron Imaging, V2019
   Chen GY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2482, DOI 10.1109/ICCV48922.2021.00250
   Chen XY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4480, DOI 10.1109/ICCV48922.2021.00446
   Choi I, 2017, IEEE T IMAGE PROCESS, V26, P5353, DOI 10.1109/TIP.2017.2731211
   Eilertsen G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130816
   Grosch T., 2006, VISION MODELING VISU, V277284, P2
   Gryaditskaya Y, 2015, COMPUT GRAPH FORUM, V34, P119, DOI 10.1111/cgf.12684
   Han J, 2020, PROC CVPR IEEE, P1727, DOI 10.1109/CVPR42600.2020.00180
   Hasinoff SW, 2010, PROC CVPR IEEE, P553, DOI 10.1109/CVPR.2010.5540167
   Jacobs K, 2008, IEEE COMPUT GRAPH, V28, P84, DOI 10.1109/MCG.2008.23
   Kalantari NK, 2019, COMPUT GRAPH FORUM, V38, P193, DOI 10.1111/cgf.13630
   Kalantari NK, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073609
   Kalantari NK, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508402
   Kang SB, 2003, ACM T GRAPHIC, V22, P319, DOI 10.1145/882262.882270
   Kim SY, 2019, IEEE I CONF COMP VIS, P3116, DOI 10.1109/ICCV.2019.00321
   Lee S, 2018, LECT NOTES COMPUT SC, V11206, P613, DOI 10.1007/978-3-030-01216-8_37
   Li YL, 2017, IEEE T IMAGE PROCESS, V26, P1143, DOI 10.1109/TIP.2016.2642790
   Mangiat S, 2010, PROC SPIE, V7798, DOI 10.1117/12.862492
   Metzler CA, 2020, PROC CVPR IEEE, P1372, DOI 10.1109/CVPR42600.2020.00145
   Myszkowski K, 2008, ACM Trans Graph, V22, P319
   Niu YZ, 2021, IEEE T IMAGE PROCESS, V30, P3885, DOI 10.1109/TIP.2021.3064433
   Pang ZW, 2022, INT CONF SIGN PROCES, P285, DOI 10.1109/ICSP56322.2022.9965323
   Pourreza-Shahri R, 2015, IEEE IMAGE PROC, P320, DOI 10.1109/ICIP.2015.7350812
   Prabhakar KR, 2017, IEEE I CONF COMP VIS, P4724, DOI 10.1109/ICCV.2017.505
   Rodríguez RG, 2019, SIAM J IMAGING SCI, V12, P1627, DOI 10.1137/19M1250248
   Saharia C, 2023, IEEE T PATTERN ANAL, V45, P4713, DOI 10.1109/TPAMI.2022.3204461
   Sen P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366222
   Seshadrinathan K, 2012, IEEE IMAGE PROC, P2785, DOI 10.1109/ICIP.2012.6467477
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun QL, 2020, PROC CVPR IEEE, P1383, DOI 10.1109/CVPR42600.2020.00146
   Teed Zachary, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P402, DOI 10.1007/978-3-030-58536-5_24
   Tocci MD, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964936
   Wu SZ, 2018, LECT NOTES COMPUT SC, V11206, P120, DOI 10.1007/978-3-030-01216-8_8
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yan Q, 2023, IEEE Trans Circuits Syst Video Technol, V1
   Yan QS, 2022, INT J COMPUT VISION, V130, P76, DOI 10.1007/s11263-021-01535-y
   Yan QS, 2020, IEEE T IMAGE PROCESS, V29, P4308, DOI 10.1109/TIP.2020.2971346
   Yan QS, 2019, PROC CVPR IEEE, P1751, DOI 10.1109/CVPR.2019.00185
   Yan QS, 2019, IEEE WINT CONF APPL, P41, DOI 10.1109/WACV.2019.00012
   Yang WH, 2020, PROC CVPR IEEE, P3060, DOI 10.1109/CVPR42600.2020.00313
   Yang YJ, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103713
   Zhao H, 2015, IEEE INT CONF COMPUT
NR 42
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103980
DI 10.1016/j.cag.2024.103980
EA JUN 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XH5W7
UT WOS:001260815000001
DA 2024-08-05
ER

PT J
AU Jovanovic, M
   Vucic, M
   Stojakovic, V
   Tepavcevic, B
   Rakovic, M
AF Jovanovic, Marko
   Vucic, Marko
   Stojakovic, Vesna
   Tepavcevic, Bojan
   Rakovic, Mirko
TI Creating terracotta panels through grayscale image processing and
   robotic hotwire cut molds
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Digital ceramics; Clay; Hotwire cutting; Molds; Image interpretation;
   Ridges
ID FABRICATION
AB This research presents a method for fabricating relief terracotta panels, utilizing polystyrene molds shaped by robotic hotwire cutting. A crucial aspect of this work is the use of grayscale images as input for generating toolpaths for hotwire cutting, bridging the gap between digital image processing in computer graphics and the physical creation of architectural elements. The process integrates computational tools with fabrication constraints and material properties to generate an integrated design approach. Utilizing computational tools, we generate toolpaths that translate grayscale images into tangible tile patterns. These patterns are formed by ridges, designed to accurately depict the grayscale images while ensuring functional tiling. The methodology synergizes traditional clay handling techniques with digital modeling, providing crucial data for material handling during drying and baking stages. The research presents a significant contribution to the realm of architectural design and computer graphics. It demonstrates the potential of image processing techniques in conjunction with robotic fabrication for creating built environments. The final tiles' appearance, evaluated using the peak signal-to-noise ratio (PSNR), exhibits values between 20 dB and 30 dB, aligning within the realms of acceptable image resemblance according to established image compression standards.
C1 [Jovanovic, Marko; Vucic, Marko; Stojakovic, Vesna; Tepavcevic, Bojan] Univ Novi Sad, Fac Tech Sci, Dept Architecture, Trg Dositeja Obradovica 6, Novi Sad 21101, Serbia.
   [Rakovic, Mirko] Univ Novi Sad, Fac Tech Sci, Dept Ind Engn & Engn Management, Trg Dositeja Obradovica 6, Novi Sad 21101, Serbia.
C3 University of Novi Sad; University of Novi Sad
RP Jovanovic, M (corresponding author), Univ Novi Sad, Fac Tech Sci, Dept Architecture, Trg Dositeja Obradovica 6, Novi Sad 21101, Serbia.
EM markojovanovic@uns.ac.rs
FU Ministry of Science, Technological Development and Innovation
   [451-03-65/2024-03/200156]; Faculty of Technical Sciences, University of
   Novi Sad [01-3394/1]
FX The authors would like to thank the company ABB for providing the
   licenses for the RobotStudio software and the authors of the Taco add on
   for Grasshopper. The authors would like to thank collaborators in this
   project (Lidija Gigovi & cacute;, Ivana Vrtunski and Jelena Pepi &
   cacute;) . This research has been supported by the Ministry of Science,
   Technological Development and Innovation (Contract No.
   451-03-65/2024-03/200156) and the Faculty of Technical Sciences,
   University of Novi Sad through the project "Scientific and Artistic
   Research Work of Researchers in Teaching and Associate Positions at the
   Faculty of Technical Sciences, University of Novi Sad" (No. 01-3394/1) .
CR ABB, 2023, Product manual IRB 140 check in in 2020-05-27
   Ahlquist S, 2011, ECAADE 2011: RESPECTING FRAGILE PLACES, P799
   Anton A, 2018, COMPUTING FOR A BETTER TOMORROW, (ECAADE 2018), VOL 2, P71
   Bechthold M, 2016, INF CONSTR, V68, DOI 10.3989/ic.15.170.m15
   Bechthold M., 2011, 2011 Proceedings of the 28th Conference of the International Association for Automation and Robotics in Construction, P70
   Bechthold M, 2010, ARCHIT DESIGN, P116
   Bernhard M, 2018, AAG, P392
   Birol EB, 2022, Structures and architecture a viable urban perspective?, P83
   Birsak M, 2018, COMPUT GRAPH FORUM, V37, P263, DOI 10.1111/cgf.13359
   Bonwetsch T, 2012, NEXUS NETW J, V14, P483, DOI 10.1007/s00004-012-0119-3
   Bonwetsch Tobias, 2007, MANUBUILD 1 INT C 25, P191
   BRELL-COKCAN Sigrid., 2010, ACADIA 10 LIFE INFOR, P357
   Brugnaro G., 2019, Robotic fabrication in architecture, art and design 2018, P336, DOI [DOI 10.1007/978-3-319-92294-226, 10.1007/978-3-319-92294-2_26, DOI 10.1007/978-3-319-92294-2_26]
   Brugnaro Giulio., 2017, Proceedings of the 37th annual conference of the association for computer aided design in architecture (ACADIA), P164
   Bull D.R, 2021, Intelligent image and video compression: communicating pictures
   Clifford B., 2014, Robotic Fabrication in Architecture, Art and Design 2014, P3, DOI [10.1007/978-3-319-04663-1_1, DOI 10.1007/978-3-319-04663-1_1]
   Creanga E, 2010, WIT TRANS ECOL ENVIR, V128, P157, DOI 10.2495/ARC100141
   Duenser S, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392465
   Dunn K, 2016, Robot Fabr Archit Art Des, V2016, P316
   Friedman J, 2014, Robot Fabr Archit Art Des, V2014, P261, DOI 10.1007/978-3-319-04663-1_18
   Grafton Anthony., 2002, LEON BATTISTA ALBERT
   Gramazio F, 2009, Urban Flux, V8, P88
   Gramazio F, 2012, Digital materiality in architecture, P2008
   Guo Z, 2022, POSTCARBON P 27 CAAD, P747, DOI [10.52842/conf.caadria.2022.1.747, DOI 10.52842/CONF.CAADRIA.2022.1.747]
   Hansmeyer M, 2013, SAJ-Serb Archit J, V5, P194
   Jovanovic M, 2019, ECAADE SIGRADI 2019: ARCHITECTURE IN THE AGE OF THE 4TH INDUSTRIAL REVOLUTION, VOL 2, P779
   Jovanovic M, 2020, ADV INTELL SYST, V980, P179, DOI 10.1007/978-3-030-19648-6_21
   Jovanovic M, 2017, ECAADE 2017: SHARING OF COMPUTABLE KNOWLEDGE! (SHOCK!), VOL 2, P135
   Jovanovic M, 2017, ADV INTELL SYST, V540, P568, DOI 10.1007/978-3-319-49058-8_62
   Jovanovic M, 2016, ECAADE 2016: COMPLEXITY & SIMPLICITY, VOL 1, P185
   Jovanovic M, 2017, AUTOMAT CONSTR, V74, P28, DOI 10.1016/j.autcon.2016.11.003
   Kanouni A, 2004, KEY ENG MAT, V264-268, P1589, DOI 10.4028/www.scientific.net/KEM.264-268.1589
   Ko M, 2019, Robotic fabrication in architecture, art and design 2018, P297
   Kontovourkis O, 2018, ISARC P INT S AUT RO, V3
   Kontovourkis O, 2020, AUTOMAT CONSTR, V110, DOI 10.1016/j.autcon.2019.103005
   Langenberg S, 2012, ROB ARCH C 2012, P13
   Ma Z, 2021, COMPUT GRAPH-UK, V98, P150, DOI 10.1016/j.cag.2021.05.008
   Ma Z, 2020, ACM SYMPOSIUM ON COMPUTATIONAL FABRICATION (SCF 2020), DOI [10.1038/s41598-020-62659-8, 10.1145/3424630.3425415]
   Nan C, 2016, ECAADE 2016: COMPLEXITY & SIMPLICITY, VOL 1, P345
   Oxman R., 2010, Int J Archit Comput, V8, P31
   Pigram D., 2011, Proceedings of the 31st Annual Conference of the Association for Computer Aided Design in Architecture (ACADIA), P122, DOI DOI 10.52842/CONF.ACADIA.2011.122
   Rosenwasser D, 2017, P 37 ANN C ASS COMP, P2
   Rust R, 2016, PROCEEDINGS OF THE 21ST INTERNATIONAL CONFERENCE ON COMPUTER-AIDED ARCHITECTURAL DESIGN RESEARCH IN ASIA (CAADRIA 2016), P529
   Sabin JE, 2010, ACADIA 10 LIFE FORMA, P174
   Saha S, 2023, HERIT SCI, V11, DOI 10.1186/s40494-023-00870-2
   Sondergaard A., 2016, Robotic Fabrication in Architecture, Art and Design 2016, P150
   Stojakovic V, 2023, NEXUS NETW J, V25, P55, DOI 10.1007/s00004-023-00699-z
   Tamke M, 2014, Int J Archit Comput, V12, P311
   Tan R, 2016, PROCEEDINGS OF THE 21ST INTERNATIONAL CONFERENCE ON COMPUTER-AIDED ARCHITECTURAL DESIGN RESEARCH IN ASIA (CAADRIA 2016), P579
   Tepavcevic B, 2023, NEXUS NETW J, V25, P887, DOI 10.1007/s00004-023-00739-8
   Van Herat O, 2014, Functional 3D printed ceramics, explorations in functional 3D printing ceramics
   Wortmann T, 2019, P S SIM ARCH URB DES, P1
   Yabanigül MN, 2021, AUTOMAT CONSTR, V126, DOI 10.1016/j.autcon.2021.103671
   Zahner LV, 2008, Manufacturing material effects: rethinking design and making in architecture, P67
   Zboinska MA, 2021, INT J ARCHIT COMPUT, V19, P250, DOI 10.1177/1478077120976493
NR 55
TC 0
Z9 0
U1 1
U2 1
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103905
DI 10.1016/j.cag.2024.103905
EA MAR 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TH4F4
UT WOS:001240355700001
DA 2024-08-05
ER

PT J
AU Pintore, G
   Jaspe-Villanueva, A
   Hadwiger, M
   Schneider, J
   Agus, M
   Marton, F
   Bettio, F
   Gobbetti, E
AF Pintore, Giovanni
   Jaspe-Villanueva, Alberto
   Hadwiger, Markus
   Schneider, Jens
   Agus, Marco
   Marton, Fabio
   Bettio, Fabio
   Gobbetti, Enrico
TI Deep synthesis and exploration of omnidirectional stereoscopic
   environments from a single surround-view panoramic image
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Indoor environments; Omnidirectional images; Data-driven methods;
   Immersive stereoscopic exploration; WebXR
ID OF-THE-ART
AB We introduce an innovative approach to automatically generate and explore immersive stereoscopic indoor environments derived from a single monoscopic panoramic image in an equirectangular format. Once per 360 degrees shot, we estimate the per -pixel depth using a gated deep network architecture. Subsequently, we synthesize a collection of panoramic slices through reprojection and view -synthesis employing deep learning. These slices are distributed around the central viewpoint, with each slice's projection center placed on the circular path covered by the eyes during a head rotation. Furthermore, each slice encompasses an angular extent sufficient to accommodate the potential gaze directions of both the left and right eye and to provide context for reconstruction. For fast display, a stereoscopic multiple -center -of -projection stereo pair in equirectangular format is composed by suitably blending the precomputed slices. At run-time, the pair is loaded in a lightweight WebXR viewer that responds to head rotations, offering both motion and stereo cues. The approach combines and extends state-of-the-art data -driven techniques, incorporating several innovations. Notably, a gated architecture is introduced for panoramic monocular depth estimation. Leveraging the predicted depth, the same gated architecture is then applied to the re -projection of visible pixels, facilitating the inpainting of occluded and disoccluded regions by incorporating a mixed Generative Adversarial Network (GAN). The resulting system works on a variety of available VR headsets and can serve as a base component for immersive applications. We demonstrate our technology on several indoor scenes from publicly available data.
C1 [Pintore, Giovanni; Marton, Fabio; Bettio, Fabio; Gobbetti, Enrico] CRS4, Cagliari, Italy.
   [Pintore, Giovanni; Marton, Fabio; Gobbetti, Enrico] Natl Res Ctr High Performance Comp Big Data & Quan, Bologna, Italy.
   [Jaspe-Villanueva, Alberto; Hadwiger, Markus] KAUST, VCC, Thuwal, Saudi Arabia.
   [Schneider, Jens; Agus, Marco] Hamad Bin Khalifa Univ, Coll Sci & Engn, Educ City, Doha, Qatar.
C3 King Abdullah University of Science & Technology; Qatar Foundation (QF);
   Hamad Bin Khalifa University-Qatar
RP Pintore, G (corresponding author), CRS4, Cagliari, Italy.; Pintore, G (corresponding author), Natl Res Ctr High Performance Comp Big Data & Quan, Bologna, Italy.
EM giovanni.pintore@crs4.it; alberto.jaspe@kaust.edu.sa;
   markus.hadwiger@kaust.edu.sa; jeschneider@hbku.edu.qa;
   MAgus@hbku.edu.qa; fabio.marton@crs4.it; fabio.marton@crs4.it;
   enrico.gobbetti@crs4.it
OI Jaspe-Villanueva, Alberto/0000-0003-3899-308X; Pintore,
   Giovanni/0000-0001-8944-1045
FU NPRP-Standard (NPRP-S) 14th Cycle grant from the Qatar National Research
   Fund;  [0403-210132]
FX GP, FM, EG acknowledge the contribution of the Italian National Research
   Center in High Performance Computing, Big Data and Quantum Computing. FB
   acknowledges the contribution of Sardinian regional authorities under
   the XDATA project. MA received funding from NPRP-Standard (NPRP-S) 14th
   Cycle grant 0403-210132 AIN2 from the Qatar National Research Fund (a
   member of Qatar Foundation). The findings herein reflect the work, and
   are solely the responsibility, of the authors.
CR Attal Benjamin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P441, DOI 10.1007/978-3-030-58452-8_26
   Bertel T, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417770
   Bourke Paul, 2010, 2010 Proceedings of 16th International Conference on Virtual Systems and Multimedia (VSMM 2010), P179, DOI 10.1109/VSMM.2010.5665988
   Broxton M, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392485
   Trinidad MC, 2019, IEEE I CONF COMP VIS, P4100, DOI 10.1109/ICCV.2019.00420
   Coors B, 2018, LECT NOTES COMPUT SC, V11213, P525, DOI 10.1007/978-3-030-01240-3_32
   da Silveira TLT, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3519021
   Dong HW, 2022, IEEE MULTIMEDIA, V29, P123, DOI 10.1109/MMUL.2022.3217627
   Gao SH, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3216675
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gkitsas V, 2021, IEEE COMPUT SOC CONF, P3711, DOI 10.1109/CVPRW53098.2021.00412
   Hedman P, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201384
   Huang JW, 2017, P IEEE VIRT REAL ANN, P37, DOI 10.1109/VR.2017.7892229
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jia Zheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P519, DOI 10.1007/978-3-030-58545-7_30
   Jokela T, 2019, MUM 2019: 18TH INTERNATIONAL CONFERENCE ON MOBILE AND UBIQUITOUS MULTIMEDIA, DOI 10.1145/3365610.3365645
   Kai-En Lin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P328, DOI 10.1007/978-3-030-58601-0_20
   Kingma D. P., 2014, arXiv
   Lambert-Lacroix S, 2016, J NONPARAMETR STAT, V28, P487, DOI 10.1080/10485252.2016.1190359
   Le H, 2019, COMPUT GRAPH FORUM, V38, P555, DOI 10.1111/cgf.13860
   Li QB, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417785
   Li YY, 2022, PROC CVPR IEEE, P2791, DOI 10.1109/CVPR52688.2022.00282
   Luo BC, 2018, IEEE T VIS COMPUT GR, V24, P1545, DOI 10.1109/TVCG.2018.2794071
   Marrinan T, 2021, IEEE T VIS COMPUT GR, V27, P2587, DOI 10.1109/TVCG.2021.3067780
   Martin D., 2020, CVPR WORKSH COMP VIS
   Matzen K, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073645
   Mohanto B, 2022, COMPUT GRAPH-UK, V102, P474, DOI 10.1016/j.cag.2021.10.010
   Peleg S., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P395, DOI 10.1109/CVPR.1999.786969
   Pintore G, 2023, 28TH INTERNATIONAL CONFERENCE ON WEB3D TECHNOLOGY, WEB3D 2023, DOI 10.1145/3611314.3615914
   Pintore G, 2024, COMPUT VIS MEDIA, DOI 10.1007/s41095-023-0358-0
   Pintore G, 2023, IEEE T VIS COMPUT GR, V29, P4708, DOI 10.1109/TVCG.2023.3320219
   Pintore G, 2021, PROC CVPR IEEE, P11531, DOI 10.1109/CVPR46437.2021.01137
   Pintore G, 2020, COMPUT GRAPH FORUM, V39, P667, DOI 10.1111/cgf.14021
   Rademacher P., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P199, DOI 10.1145/280814.280871
   Reda F, 2022, LECT NOTES COMPUT SC, V13667, P250, DOI 10.1007/978-3-031-20071-7_15
   Rey-Area M, 2022, PROC CVPR IEEE, P3752, DOI 10.1109/CVPR52688.2022.00374
   Richardt Christian, 2020, Real VR - Immersive Digital Reality: How to Import the Real World into Head-Mounted Immersive Displays. Lecture Notes in Computer Science (LNCS 11900), P3, DOI 10.1007/978-3-030-41816-8_1
   Serrano A, 2019, IEEE T VIS COMPUT GR, V25, P1817, DOI 10.1109/TVCG.2019.2898757
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su Y.-C., 2017, ADV NEURAL INF PROCE, P529
   Sulaiman M.Z., 2020, P INT C INN MED VIS, P221, DOI DOI 10.2991/ASSEHR.K.201202.079
   Sun C, 2021, PROC CVPR IEEE, P2573, DOI 10.1109/CVPR46437.2021.00260
   Tateno K, 2018, LECT NOTES COMPUT SC, V11220, P732, DOI 10.1007/978-3-030-01270-0_43
   Tucker R, 2020, PROC CVPR IEEE, P548, DOI 10.1109/CVPR42600.2020.00063
   Tukur M, 2023, GRAPH MODELS, V128, DOI 10.1016/j.gmod.2023.101182
   Tukur M., 2022, P SMART TOOLS APPL G, P131, DOI [10.2312/stag.20221267, DOI 10.2312/STAG.20221267]
   Tulsiani S, 2018, LECT NOTES COMPUT SC, V11211, P311, DOI 10.1007/978-3-030-01234-2_19
   Ulyanov D, 2017, Arxiv, DOI arXiv:1607.08022
   Waidhofer J, 2022, INT SYM MIX AUGMENT, P584, DOI 10.1109/ISMAR55827.2022.00075
   Wang XT, 2021, IEEE INT CONF COMP V, P1905, DOI 10.1109/ICCVW54120.2021.00217
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Wiles Olivia, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7465, DOI 10.1109/CVPR42600.2020.00749
   Xu JL, 2021, PROC CVPR IEEE, P16433, DOI 10.1109/CVPR46437.2021.01617
   Xu M, 2020, IEEE J-STSP, V14, P5, DOI 10.1109/JSTSP.2020.2966864
   Yi Z, 2020, P IEEE CVF C COMP VI, P7508, DOI DOI 10.1109/CVPR42600.2020.00753
   Yu F., 2016, ICLR, P1
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhou TH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201323
   Zioulis N, 2019, INT CONF 3D VISION, P690, DOI 10.1109/3DV.2019.00081
   Zioulis N, 2018, LECT NOTES COMPUT SC, V11210, P453, DOI 10.1007/978-3-030-01231-1_28
NR 63
TC 0
Z9 0
U1 1
U2 1
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103907
DI 10.1016/j.cag.2024.103907
EA MAR 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PT5O6
UT WOS:001216350900001
DA 2024-08-05
ER

PT J
AU Bektas, K
   Strecker, J
   Mayer, S
   Garcia, K
AF Bektas, Kenan
   Strecker, Jannis
   Mayer, Simon
   Garcia, Kimberly
TI Gaze-enabled activity recognition for augmented reality feedback
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Pervasive eye tracking; Augmented reality; Attention; Human activity
   recognition; Context-awareness; Ubiquitous computing
AB Head-mounted Augmented Reality (AR) displays overlay digital information on physical objects. Through eye tracking, they provide insights into user attention, intentions, and activities, and allow novel interaction methods based on this information. However, in physical environments, the implications of using gaze-enabled AR for human activity recognition have not been explored in detail. In an experimental study with the Microsoft HoloLens 2, we collected gaze data from 20 users while they performed three activities: Reading a text, Inspecting a device, and Searching for an object. We trained machine learning models (SVM, Random Forest, Extremely Randomized Trees) with extracted features and achieved up to 89.6% activity-recognition accuracy. Based on the recognized activity, our system-GEAR-then provides users with relevant AR feedback. Due to the sensitivity of the personal (gaze) data GEAR collects, the system further incorporates a novel solution based on the Solid specification for giving users fine-grained control over the sharing of their data. The provided code and anonymized datasets may be used to reproduce and extend our findings, and as teaching material.
C1 [Bektas, Kenan; Strecker, Jannis; Mayer, Simon; Garcia, Kimberly] Univ St Gallen, Rosenbergstr 30, CH-9000 St Gallen, Switzerland.
C3 University of St Gallen
RP Bektas, K (corresponding author), Univ St Gallen, Rosenbergstr 30, CH-9000 St Gallen, Switzerland.
EM kenan.bektas@unisg.ch
RI Bektas, Kenan/AEO-2264-2022
OI Bektas, Kenan/0000-0003-2937-0542; Strecker, Jannis/0000-0001-7607-8064
FU Swiss Innovation Agency Innosuisse [48342.1 IP-ICT]; Basic Research Fund
   of the University of St.Gallen, Switzerland
FX We thank our participants for providing us with their data, Stefan
   Bucher for providing us with realistic physical props for testing the
   GEAR system, Mathias Lenz and Markus Stolze for the virtual 3D model.
   This work was funded by the Swiss Innovation Agency Innosuisse (#48342.1
   IP-ICT) and the Basic Research Fund of the University of St.Gallen,
   Switzerland.
CR Alinaghi N, 2021, LIPICS 2, DOI [10.4230/LIPIcs.GIScience.2021.II.5, DOI 10.4230/LIPICS.GISCIENCE.2021.II.5]
   [Anonymous], 2014, Flow and the foundations of positive psychology: The collected works of Mihaly Csikszentmihalyi, DOI [DOI 10.1007/978-94-017-9088-8_14, 10.1007/978-94-017-9088-8_14]
   Azuma R, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.963459
   Bektas K, 2022, AutomationXP22: Engaging with automation, cHI'22
   Bektas K., 2020, ACM S EYE TRACK RES, DOI [10.1145/3379157.3391657, DOI 10.1145/3379157.3391657]
   Bektas K., 2015, EUR C VIS EUROVIS SH, DOI [10.2312/eurovisshort.20151127, DOI 10.2312/EUROVISSHORT.20151127]
   Bektas K, 2023, ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, ETRA 2023, DOI 10.1145/3588015.3588402
   Bektas K, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0259977
   Bektas K, 2019, ETRA 2019: 2019 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, DOI 10.1145/3317959.3321488
   Billinghurst Mark, 2015, Foundations and Trends in Human-Computer Interaction, V8, P73, DOI 10.1561/1100000049
   Borji A, 2014, J VISION, V14, DOI 10.1167/14.3.29
   Bozkir E, 2020, ETRA 2020 SHORT PAPERS: ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, DOI 10.1145/3379156.3391364
   Braunagel C, 2015, IEEE INT C INTELL TR, P1652, DOI 10.1109/ITSC.2015.268
   Bulling A, 2011, IEEE T PATTERN ANAL, V33, P741, DOI 10.1109/TPAMI.2010.86
   Bulling A, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2499621
   Bulling A, 2014, IEEE PERVAS COMPUT, V13, P80, DOI 10.1109/MPRV.2014.42
   Bulling A, 2010, IEEE PERVAS COMPUT, V9, P8, DOI 10.1109/MPRV.2010.86
   Campbell C.S., 2001, P 2001 WORKSHOP PERC, P1, DOI DOI 10.1145/971478.971503
   Christoff K, 2016, NAT REV NEUROSCI, V17, P718, DOI 10.1038/nrn.2016.113
   Ciortea A, 2020, PERS UBIQUIT COMPUT, DOI 10.1007/s00779-020-01415-1
   Cornacchia M, 2017, IEEE SENS J, V17, P386, DOI 10.1109/JSEN.2016.2628346
   David B, 2021, AUST ARCHAEOL, V87, P1, DOI 10.1080/03122417.2020.1859963
   Dunn MJ, 2023, BEHAV RES METHODS, DOI 10.3758/s13428-023-02187-1
   Garcia K, 2020, 2020 IEEE 6TH INTERNATIONAL CONFERENCE ON COLLABORATION AND INTERNET COMPUTING (CIC 2020), P54, DOI 10.1109/CIC50333.2020.00017
   Grau J, 2024, 2024 CHI C HUM FACT, DOI [10.1145/3613905.3651066, DOI 10.1145/3613905.3651066]
   Gressel C, 2023, IEEE PERVAS COMPUT, V22, P95, DOI 10.1109/MPRV.2022.3228660
   Grubert J, 2017, IEEE T VIS COMPUT GR, V23, P1706, DOI 10.1109/TVCG.2016.2543720
   Hodges J, 2017, COMPUTER, V50, P26, DOI 10.1109/MC.2017.4041353
   Holmqvist K., 2012, Proceedings of the symposium on eye tracking research and applications, P45, DOI [DOI 10.1145/2168556.2168563, 10.1145/2168556.2168563]
   Holmqvist K., 2011, EYE TRACKING COMPREH
   Hostettler D, 2023, ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, ETRA 2023, DOI 10.1145/3588015.3590123
   Jacob R., 2016, interactions, V23, P62, DOI DOI 10.1145/2978577
   Kapp S, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21062234
   Kassner M, 2014, PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP'14 ADJUNCT), P1151, DOI 10.1145/2638728.2641695
   Katsini C, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376840
   Keshava A, 2024, bioRxiv, DOI [10.1101/2021.01.29.428782, 10.1101/2021.01.29.428782, DOI 10.1101/2021.01.29.428782]
   Kiefer P., 2013, P 21 ACM SIGSPATIAL, P488, DOI [10.1145/2525314.2525467, DOI 10.1145/2525314.2525467]
   Konrad R, 2024, Arxiv, DOI [arXiv:2401.17217, 10.48550/arXiv.2401.17217, DOI 10.48550/ARXIV.2401.17217]
   Krejtz K, 2016, ACM T APPL PERCEPT, V13, DOI 10.1145/2896452
   Kroger J. L., 2019, On the Privacy Implications of Eye Tracking, P226, DOI DOI 10.1007/978-3-030-42504-315
   Kunze K., 2013, Proceedings of the 2013 International Symposium on Wearable Computers, ISWC '13, Zurich, Switzerland, P113
   Lan GH, 2022, 2022 21ST ACM/IEEE INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2022), P233, DOI 10.1109/IPSN54338.2022.00026
   Langheinrich M., 2001, INT C UB COMP, P273, DOI DOI 10.1007/3-540-45427-6_23
   Liebling DJ, 2014, PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP'14 ADJUNCT), P1169, DOI 10.1145/2638728.2641688
   Microsoft, 2024, MixedRealityToolkit-Unity
   Microsoft, 2023, Eye tracking on HoloLens 2, V2
   Microsoft, 2024, EyesPose Class (Windows.Perception.People)-Windows UWP
   Microsoft, 2022, Extended eye tracking in unity
   MILGRAM P, 1994, IEICE T INF SYST, VE77D, P1321
   Orlosky J, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.763340
   Ostermaier Benedikt., 2010, 2010 Internet of Things (IOT), P1, DOI DOI 10.1109/IOT.2010.5678450
   Pandjaitan A, 2024, 2024 CHI C HUM FACT, DOI [10.1145/3613905.3650941, DOI 10.1145/3613905.3650941]
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Pfeuffer K, 2021, COMPUT GRAPH-UK, V95, P1, DOI 10.1016/j.cag.2021.01.001
   Plopski A, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3491207
   Rook K, 2019, INT CONF PERVAS COMP, P227, DOI [10.1109/percomw.2019.8730692, 10.1109/PERCOMW.2019.8730692]
   Salvucci Dario D., 2000, P 2000 S EYE TRACK R, DOI [10.1145/355017.355028, DOI 10.1145/355017.355028]
   Sambra A.V., 2016, Tech. Rep.
   Scargill T, 2022, 2022 21ST ACM/IEEE INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2022), P503, DOI 10.1109/IPSN54338.2022.00052
   Seeliger A, 2024, INT J HUM-COMPUT INT, V40, P761, DOI 10.1080/10447318.2022.2122114
   Spirig J, 2021, 11TH INTERNATIONAL CONFERENCE ON THE INTERNET OF THINGS, IOT 2021, P25, DOI 10.1145/3494322.3494326
   Startsev M, 2023, BEHAV RES METHODS, V55, P1653, DOI 10.3758/s13428-021-01763-7
   Startsev M, 2019, BEHAV RES METHODS, V51, P556, DOI 10.3758/s13428-018-1144-2
   Steil J, 2019, ETRA 2019: 2019 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, DOI 10.1145/3314111.3319913
   Steil J, 2019, ETRA 2019: 2019 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, DOI 10.1145/3314111.3319915
   Strecker J, 2023, PROC ACM INTERACT MO, V7, DOI 10.1145/3610879
   Strecker J, 2022, PROCEEDINGS OF THE 12TH INTERNATIONAL CONFERENCE ON THE INTERNET OF THINGS 2022, IOT 2022, P25, DOI 10.1145/3567445.3567453
   Sutherland I. E., 1968, Proceedings of the December 9-11, 1968, Fall Joint Computer Conference, Part I, AFIPS '68, P757, DOI DOI 10.1145/1476589.1476686
   Toyama T., 2015, P 20 INT C INT US IN, P322, DOI DOI 10.1145/2678025.2701384
   Vertegaal R, 2003, COMMUN ACM, V46, P30, DOI 10.1145/636772.636794
   Wang X, 2023, HoloAssist: An egocentric human interaction dataset for interactive AI assistants in the real world, P20270
   WEISER M, 1991, SCI AM, V265, P94, DOI 10.1038/scientificamerican0991-94
   Yarbus A. L., 1967, EYE MOVEMENTS VISION, DOI [10.1007/978-1-4899-5379-7, DOI 10.1007/978-1-4899-5379-7]
   Zemblys R, 2018, BEHAV RES METHODS, V50, P160, DOI 10.3758/s13428-017-0860-3
   Zhai Shumin., 1999, Proceedings of CHI, P246, DOI DOI 10.1145/302979.303053
NR 75
TC 1
Z9 1
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103909
DI 10.1016/j.cag.2024.103909
EA APR 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QR9R6
UT WOS:001222720600001
OA hybrid
DA 2024-08-05
ER

PT J
AU Zingsheim, D
   Klein, R
AF Zingsheim, Domenic
   Klein, Reinhard
TI Learning subsurface scattering solutions of tightly-packed granular
   mediausing optimal transport
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Rendering; Grain; Granular media; Optimal transport; Neural network;
   BSSRDF
ID MULTIPLE-SCATTERING; PATH
AB Many materials, such as sand, rice, wheat, or other kinds of seeds, consist of numerous individual grains thatdetermine the visual appearance of these materials. When generating images of these mixtures, the primarychallenge is to simulate the interaction of light with each individual grain. While subsurface scattering effectsare crucial for producing realistic images, the computation of light transport using standard path tracingmethods for each grain can be prohibitively expensive. Although there have been several methods developed toaddress this issue, they all assume that bounding spheres of individual grains do not intersect. This restrictionlimits the application of these methods to almost spherical grains. Nonetheless, various grains, such as seedsand rice, are non-spherical, making this assumption lead to impractical stackings in situations involving coarse-grained materials. We address this issue by presenting a subsurface scattering model that utilizes a neuralnetwork and is trained using an optimal transport framework. Our model surpasses path tracing approachesconclusively, allowing for efficient rendering of granular mixtures that were previously unfeasible. Additionally,this method can be utilized in large-scale procedural generated scenes based on sphere packings and obtainssimilar results as previous methods in these cases.
C1 [Zingsheim, Domenic; Klein, Reinhard] Friedrich Hirzebruch Allee 8, D-53115 Bonn, Germany.
RP Zingsheim, D (corresponding author), Friedrich Hirzebruch Allee 8, D-53115 Bonn, Germany.
EM zingsheim@cs.uni-bonn.de; rk@cs.uni-bonn.de
FU Federal Ministry of Educationand Research of Germany; Federal Ministry
   of Education and Research,Germany [01IS22094E WEST-AI]
FX This research has been funded by the Federal Ministry of Educationand
   Research of Germany and the state of North-Rhine Westphaliaas part of
   the Lamarr-Institute for Machine Learning and ArtificialIntelligence and
   by the Federal Ministry of Education and Research,Germany under grant
   no. 01IS22094E WEST-AI.
CR Aakash KT, 2023, COMPUT GRAPH FORUM, V42, DOI 10.1111/cgf.14895
   Aakash KT, 2022, P ACM COMPUT GRAPH, V5, DOI 10.1145/3522612
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bartell F. O., 1980, Proceedings of the Society of Photo-Optical Instrumentation Engineers, V257, P154
   Bitterli B, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392481
   Bonneel N, 2015, J MATH IMAGING VIS, V51, P22, DOI 10.1007/s10851-014-0506-3
   Bonneel N, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024192
   Chiang MJ-Y, 2015, ACM SIGGRAPH 2015 TA, P1
   d'Eon E, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964951
   Diolatzis S, 2020, COMPUT GRAPH FORUM, V39, P23, DOI 10.1111/cgf.14051
   DIXMIER M, 1978, J PHYS-PARIS, V39, P873, DOI 10.1051/jphys:01978003908087300
   Flamary R, 2021, J MACH LEARN RES, V22
   Frogner C, 2015, ADV NEUR IN, V28
   Ge LS, 2021, IEEE T VIS COMPUT GR, V27, P3123, DOI 10.1109/TVCG.2019.2963015
   Giusto A, 2003, APPL OPTICS, V42, P4375, DOI 10.1364/AO.42.004375
   Guérin CA, 2006, J OPT SOC AM A, V23, P349, DOI 10.1364/JOSAA.23.000349
   Guo J, 2022, COMPUT VIS MEDIA, V8, P425, DOI 10.1007/s41095-021-0253-5
   Habel R, 2013, COMPUT GRAPH FORUM, V32, P27, DOI 10.1111/cgf.12148
   Haber T., 2005, GI 05 P GRAPHICS INT, P79, DOI DOI 10.5555/1089508.1089522
   Jensen HW, 1995, SPRING COMP SCI, P326, DOI 10.1007/978-3-7091-9430-0_31
   Kajiya J. T., 1984, Computers & Graphics, V18, P165
   Kajiya J. T., 1986, Computer Graphics, V20, P143, DOI 10.1145/15886.15902
   Kajs N, 2022, 2022 INT C ART REAL
   Kallweit S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130880
   Kantorovitch L, 1942, CR ACAD SCI URSS, V37, P199
   Keller A, 2021, ACM SIGGRAPH 2021 TA, P1
   Kingma D. P., 2014, arXiv
   Kingma DP, 2019, FOUND TRENDS MACH LE, V12, P4, DOI 10.1561/2200000056
   Krivánek J, 2005, IEEE T VIS COMPUT GR, V11, P550, DOI 10.1109/TVCG.2005.83
   Laine Samuli, 2013, Proceedings of the 5th High-Performance Graphics Conference, P137, DOI [10.1145/2492045.2492060, DOI 10.1145/2492045.2492060]
   Leonard L, 2021, COMPUT GRAPH FORUM, V40, P165, DOI 10.1111/cgf.142623
   Meng J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766949
   Michel É, 2020, COMPUT GRAPH FORUM, V39, P169, DOI 10.1111/cgf.14135
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Moon JT, 2006, ACM T GRAPHIC, V25, P1067, DOI 10.1145/1141911.1141995
   Moon JT, 2007, P 18 EUR C REND TECH, P231, DOI [10.2312/EGWR/EGSR07/231-242., DOI 10.2312/EGWR/EGSR07/231-242]
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Müller T, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982429
   Müller T, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459812
   Müller T, 2017, COMPUT GRAPH FORUM, V36, P91, DOI 10.1111/cgf.13227
   Novák J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661292
   Ouyang Y, 2021, COMPUT GRAPH FORUM, V40, P17, DOI 10.1111/cgf.14378
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Parker SG, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778803
   Paszke A, 2019, ADV NEUR IN, V32
   Pharr M., 2016, Physically based rendering: From theory to implementation
   Reibold F, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275030
   Ren PR, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462009
   Solomon J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766963
   Sun T, 2021, EUROGRAPHICS S RENDE, DOI [10.2312/sr.20211301, DOI 10.2312/SR.20211301]
   Veach E., 1995, P 22 ANN C COMP GRAP, P419, DOI DOI 10.1145/218380.218498
   Vicini D, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322974
   Vorba J, 2014, ACM T GRAPHIC, V33, DOI [10.1145/2601097.2601203, 10.1145/2801097.2801203]
   Wei LY, 2018, LECT NOTES COMPUT SC, V11208, P105, DOI 10.1007/978-3-030-01225-0_7
   Yan LQ, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130802
   Zhang C, 2020, EGSR. DL, P25, DOI [10.2312/sr.20201134, DOI 10.2312/SR.20201134]
   Zhu JQ, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530105
NR 57
TC 0
Z9 0
U1 3
U2 3
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103895
DI 10.1016/j.cag.2024.103895
EA FEB 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OE9E6
UT WOS:001205699300001
OA hybrid
DA 2024-08-05
ER

PT J
AU Segear, S
   Chheang, V
   Baron, L
   Li, JC
   Kim, K
   Barmaki, RL
AF Segear, Sydney
   Chheang, Vuthea
   Baron, Lauren
   Li, Jicheng
   Kim, Kangsoo
   Barmaki, Roghayeh Leila
TI Visual feedback and guided balance training in an immersive virtual
   reality environment for lower extremity rehabilitation
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Virtual reality; Balance training; Lower extremity rehabilitation;
   Guided VR training; Human-computer interaction
ID PARKINSONS-DISEASE; PROGRAM
AB Balance training is essential for physical rehabilitation procedures, as it can improve functional mobility and enhance cognitive coordination. However, conventional balance training methods may have limitations in terms of motivation, real-time objective feedback, and personalization, which a virtual reality (VR) setup may provide a better alternative. In this work, we present an immersive VR training environment for lower extremity balance rehabilitation with real-time guidance and feedback. The VR training environment immerses the user in a 3D ice rink model where a virtual coach (agent) leads them through a series of balance poses, and the user controls a trainee avatar with their own movements. We developed two coaching styles: positive -reinforcement and autonomous -supportive, and two viewpoints of the trainee avatar: first -person and third -person. The proposed environment was evaluated in a user study with healthy, non -clinical participants (n = 16, 24.4 +/- 5.7 years old, 9 females). Our results show that participants showed stronger performance in the positive -reinforcement style compared to the autonomous -supportive style. Additionally, in the thirdperson viewpoint, the participants exhibited more stability in the positive -reinforcement style compared to the autonomous -supportive style. For viewpoint, participants exhibited stronger performance in the first -person viewpoint compared to third -person in the autonomous -supportive style, while they were comparable in the positive -reinforcement style. We observed no significant effects on the foot height and number of mistakes. Furthermore, we report the analysis of user performance with balance training poses and subjective measures based on questionnaires to assess the user experience, usability, and task load. The proposed VR balance training could offer an interactive, adaptive, and engaging environment and open new potential research directions for lower extremity rehabilitation.
C1 [Segear, Sydney; Baron, Lauren; Li, Jicheng; Barmaki, Roghayeh Leila] Univ Delaware, Dept Comp & Informat Sci, Newark, DE 19716 USA.
   [Chheang, Vuthea] Lawrence Livermore Natl Lab, Ctr Appl Sci Comp, Livermore, CA USA.
   [Kim, Kangsoo] Univ Calgary, Dept Elect & Software Engn, Calgary, AB, Canada.
C3 University of Delaware; United States Department of Energy (DOE);
   Lawrence Livermore National Laboratory; University of Calgary
RP Barmaki, RL (corresponding author), Univ Delaware, Dept Comp & Informat Sci, Newark, DE 19716 USA.
EM rlb@udel.edu
RI Kim, Kangsoo/AAL-9592-2020; Chheang, Vuthea/GWR-2087-2022
OI Kim, Kangsoo/0000-0002-0925-378X; Chheang, Vuthea/0000-0001-5999-4968;
   Barmaki, Roghayeh/0000-0002-7570-5270
FU National Science Foundation, United States [2222663]; National Institute
   of General Medical Sciences of the National Institutes of Health [P20
   GM103446-E]; LLNL, United States [DE-AC52-07NA27344]
FX We express our gratitude to Dr. Pinar Kullu for her valuable
   contributions in providing consultation and project brainstorming. We
   are thankful to our study participants for their support and
   participation. We also wish to acknowledge the support from our
   sponsors, the National Science Foundation, United States (2222663) , the
   National Institute of General Medical Sciences of the National
   Institutes of Health (P20 GM103446-E) . This work was also partly
   prepared by LLNL, United States under Contract DE-AC52-07NA27344. Any
   opinions, findings, conclusions, or recommendations expressed in this
   material are those of the authors and do not necessarily reflect the
   view of the sponsors.
CR Akbas A, 2019, J HUM KINET, V69, P5, DOI 10.2478/hukin-2019-0023
   Andrews AR, 2014, Ph.D. thesis
   Bailenson JN, 2003, PERS SOC PSYCHOL B, V29, P819, DOI 10.1177/0146167203029007002
   Bangor A, 2009, J USABILITY STUD, V4, P114
   Barmaki R, 2018, P AAAI C ART INT, V32
   Barmaki R, 2019, ANAT SCI EDUC, V12, P599, DOI 10.1002/ase.1858
   Barmaki R, 2018, J COMPUT ASSIST LEAR, V34, P387, DOI 10.1111/jcal.12268
   Baron L, 2023, IEEE ACM CHASE
   Bertram J, 2023, PLOS ONE, V18, DOI 10.1371/journal.pone.0279697
   Blomqvist S, 2021, BMC GERIATR, V21, DOI 10.1186/s12877-021-02061-9
   Borrego A, 2019, FRONT NEUROL, V10, DOI 10.3389/fneur.2019.01061
   Brooke J., 1996, SUS-a quick and dirty usability scale, DOI [DOI 10.1201/9781498710411-35, DOI 10.1201/9781498710411]
   Chheang V, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P401, DOI 10.1109/VRW55335.2022.00089
   Debarba HG, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0190109
   Emmerich Katharina., 2021, Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, P1
   Esculier JF, 2012, J REHABIL MED, V44, P144, DOI 10.2340/16501977-0922
   Faul F, 2007, BEHAV RES METHODS, V39, P175, DOI 10.3758/BF03193146
   Gonzalez-Franco M, 2020, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.561558
   Gonzalez-Franco M, 2020, 2020 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR 2020), P91, DOI 10.1109/AIVR50618.2020.00026
   Grassini S, 2020, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.01743
   Guo Z, 2019, EDM
   Guo Z, 2023, P INT C COOP HUM ASP
   Hart SG., 2006, P HUM FACT ERG SOC A, V50, P904, DOI [10.1177/154193120605000909, DOI 10.1177/154193120605000909]
   Jicheng Li, 2021, ICMI '21: Proceedings of the 2021 International Conference on Multimodal Interaction, P397, DOI 10.1145/3462244.3479891
   Li J, 2021, P AAAI 21 WORKSH AFF, P1
   Li JC, 2022, INT C PATT RECOG, P762, DOI 10.1109/ICPR56361.2022.9956680
   Li JT, 2022, IEEE WRK SIG PRO SYS, P73, DOI [10.1145/3536221.3556627, 10.1109/SIPS55645.2022.9919246]
   Marcone M, 2017, Master's thesis
   Nieto-Escamez F, 2023, BRAIN SCI, V13, DOI 10.3390/brainsci13050819
   Nishchyk Anna, 2020, Computers Helping People with Special Needs. 17th International Conference, ICCHP 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12376), P233, DOI 10.1007/978-3-030-58796-3_28
   O'Neil L, 2020, J APPL SPORT PSYCHOL, V32, P607, DOI 10.1080/10413200.2019.1581302
   Parker K, 2012, INT SPORT COACH J, V5, P5, DOI 10.1123/jce.5.2.5
   Peck TC, 2021, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.575943
   Prasertsakul T, 2018, BIOMED ENG ONLINE, V17, DOI 10.1186/s12938-018-0550-0
   Ragogna M, 2017, Master's thesis
   Rose T, 2018, APPL ERGON, V69, P153, DOI 10.1016/j.apergo.2018.01.009
   Salamin P., 2006, P ACM S VIRT REAL SO, P27, DOI 10.1145/1180495.1180502
   Salamin P, 2010, IEEE T LEARN TECHNOL, V3, P272, DOI 10.1109/TLT.2010.13
   Schedler S, 2020, BMC SPORTS SCI MED R, V12, DOI 10.1186/s13102-020-00218-4
   Shaw LA, 2015, P AUSTR US INT C
   Steed A, 2016, IEEE T VIS COMPUT GR, V22, P1406, DOI 10.1109/TVCG.2016.2518135
   Sturnieks DL, 2008, NEUROPHYSIOL CLIN, V38, P467, DOI 10.1016/j.neucli.2008.09.001
   Thatcher B, 2021, INT SPORT COACH J, V8, P234, DOI 10.1123/iscj.2020-0011
   Tölgyessy M, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21020413
   Tropea P, 2019, J MED INTERNET RES, V21, DOI 10.2196/12805
   Yang WC, 2016, J FORMOS MED ASSOC, V115, P734, DOI 10.1016/j.jfma.2015.07.012
   Zhang T, 2022, J REHABIL MED, V54, DOI 10.2340/jrm.v54.2209
NR 47
TC 0
Z9 0
U1 6
U2 6
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103880
DI 10.1016/j.cag.2024.01.007
EA FEB 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LB2L6
UT WOS:001184251400001
PM 38645661
DA 2024-08-05
ER

PT J
AU Shu, J
   Yu, SQ
   Shu, XY
   Hu, JW
AF Shu, Jun
   Yu, Shiqi
   Shu, Xinyi
   Hu, Jiewen
TI SOA: Seed point offset attention for indoor 3D object detection in point
   clouds
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE 3D object detection; Point cloud; Cross-attention
AB Three-dimensional object detection plays a pivotal role in scene understanding and holds significant importance in various indoor perception applications. Traditional methods based on Hough voting are susceptible to interference from background points or neighboring objects when casting votes for the target's center from each seed point. Moreover, fixed-size set abstraction modules may result in the loss of structural information for large objects. To address these challenges, this paper proposes a three-dimensional object detection model based on seed point offset attention. The objective of this model is to enhance the model's resilience to voting noise interference and alleviate feature loss for large-scale objects. Specifically, a seed point offset tensor is first defined, and then the offset tensor self-attention network is employed to learn the weights between votes, thereby establishing a correlation between the voting semantic features and the object structural information. Furthermore, an object surface perception module is introduced, which incorporates detailed features of local object surfaces into global feature representations through vote backtracking and surface mapping. Experimental results indicate that the model achieved excellent performance on the ScanNet-V2 (mAP@0.5, 60.3%) and SUN RGB-D (mAP@0.5, 64.0%) datasets, respectively improving by 2.6% (mAP@0.5) and 5.4% (mAP@0.5) compared to VoteNet.
C1 [Shu, Jun; Yu, Shiqi] Hubei Univ Technol, Coll Elect & Elect Engn, Wuhan 430068, Peoples R China.
   [Shu, Jun; Yu, Shiqi] Hubei Univ Technol, Hubei Key Lab High Efficiency Utilizat Solar Energ, Wuhan 430068, Peoples R China.
   [Shu, Xinyi] Univ Melbourne, Fac Sci, Melbourne, Vic 3010, Australia.
   [Hu, Jiewen] Hubei Second Normal Univ, Sch Comp Sci, Wuhan 430205, Peoples R China.
C3 Hubei University of Technology; Hubei University of Technology;
   University of Melbourne
RP Yu, SQ (corresponding author), Hubei Univ Technol, Coll Elect & Elect Engn, Wuhan 430068, Peoples R China.
EM 102200244@hbut.edu.cn
CR Bhattacharyya P, 2021, IEEE INT CONF COMP V, P3022, DOI 10.1109/ICCVW54120.2021.00337
   Chen C, 2019, GAPNet: Graph attention based point neural network for exploiting local feature of point cloud
   Chen C, 2022, AAAI CONF ARTIF INTE, P221
   Chen YJ, 2023, MULTIMED TOOLS APPL, DOI 10.1007/s11042-023-15702-5
   Cheng BW, 2021, PROC CVPR IEEE, P8959, DOI 10.1109/CVPR46437.2021.00885
   Cruz S, 2021, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR46437.2021.00217
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Duan Y, 2022, PROC CVPR IEEE, P16959, DOI 10.1109/CVPR52688.2022.01647
   Ergün O, 2023, TURK J ELECTR ENG CO, V31, P381, DOI 10.55730/1300-0632.3990
   Janjos F, 2022, IEEE INT VEH SYM, P280, DOI 10.1109/IV51971.2022.9827091
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Liu C, 2023, IET COMPUT VIS, V17, P415, DOI 10.1049/cvi2.12177
   Lu T, 2023, PROC CVPR IEEE, P1105, DOI 10.1109/CVPR52729.2023.00113
   McCormac J, 2018, INT CONF 3D VISION, P32, DOI 10.1109/3DV.2018.00015
   Misra I, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2886, DOI 10.1109/ICCV48922.2021.00290
   Pan XR, 2021, PROC CVPR IEEE, P7459, DOI 10.1109/CVPR46437.2021.00738
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2020, PROC CVPR IEEE, P4403, DOI 10.1109/CVPR42600.2020.00446
   Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qiu S, 2023, IEEE T PATTERN ANAL, V45, P1312, DOI 10.1109/TPAMI.2021.3137794
   Ren Z, 2016, PROC CVPR IEEE, P1525, DOI 10.1109/CVPR.2016.169
   Samet N, 2023, IEEE T PATTERN ANAL, V45, P4667, DOI 10.1109/TPAMI.2022.3200413
   Shu J, 2023, CMC-COMPUT MATER CON, V77, P2677, DOI 10.32604/cmc.2023.045818
   Song SR, 2016, PROC CVPR IEEE, P808, DOI 10.1109/CVPR.2016.94
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Tian XY, 2023, IEEE T CIRC SYST VID, V33, P5844, DOI 10.1109/TCSVT.2023.3260115
   Wang C, 2019, PROC CVPR IEEE, P3338, DOI 10.1109/CVPR.2019.00346
   Wang HY, 2022, PROC CVPR IEEE, P1100, DOI 10.1109/CVPR52688.2022.00118
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu P, 2023, VISUAL COMPUT, V39, P2425, DOI 10.1007/s00371-022-02672-2
   Xie Q, 2020, MLCVNet: Multi-level context VoteNet for 3D object detection, P10444, DOI [10.1109/CVPR42600.2020.01046, DOI 10.1109/CVPR42600.2020.01046]
   Xie Q, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3692, DOI 10.1109/ICCV48922.2021.00369
   Yao T, 2023, PROC CVPR IEEE, P21846, DOI 10.1109/CVPR52729.2023.02092
   Ye XQ, 2018, LECT NOTES COMPUT SC, V11211, P415, DOI 10.1007/978-3-030-01234-2_25
   Yi L, 2019, PROC CVPR IEEE, P3942, DOI 10.1109/CVPR.2019.00407
   Yu H, 2023, INT J APPL EARTH OBS, V118, DOI 10.1016/j.jag.2023.103231
   Zetong Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11037, DOI 10.1109/CVPR42600.2020.01105
   Zaiwei Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P311, DOI [10.1061/9780784482933.027, 10.1007/978-3-030-58610-2_19]
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhao LC, 2021, IEEE T CIRC SYST VID, V31, P4735, DOI 10.1109/TCSVT.2021.3102025
   Zhou J, 2024, NEURAL PROCESS LETT, V56, DOI 10.1007/s11063-024-11447-w
NR 43
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD OCT
PY 2024
VL 123
AR 103992
DI 10.1016/j.cag.2024.103992
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZM9A3
UT WOS:001275822800001
DA 2024-08-05
ER

PT J
AU Alfonso-Arsuaga, M
   García-González, J
   Castiella-Aguirrezabala, A
   Alonso, MA
   Garcés, E
AF Alfonso-Arsuaga, Mario
   Garcia-Gonzalez, Jorge
   Castiella-Aguirrezabala, Andrea
   Alonso, Miguel Andres
   Garces, Elena
TI DyNeRFactor: Temporally consistent intrinsic scene decomposition for
   dynamic NeRFs
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE NeRF; Inverse rendering; Novel view synthesis; Relighting; Dynamic
AB We present a method for estimating the intrinsic components of a dynamic scene captured with multi-view video sequences. Unlike previous work focused either on static scenes or single view videos, our method simultaneously addresses the challenges of dealing with the extra computational complexity given by the dynamic motion while enables novel view synthesis. Key to our method to make the output temporally consistent is to encode the temporal information in a latent embedding that leverages the redundant information of the dynamic scene. Our intrinsic components includes diffuse and specular albedo, as well as scene geometry and environment illumination. We explicitly account for light visibility, which we estimate efficiently by considering dynamic and static points separately, making the problem computationally tractable. We demonstrate the effectiveness of our approach through quantitative and qualitative experiments, showing that it outperforms the na & iuml;ve per-frame decomposition approach in several real-world scenes.
C1 [Alfonso-Arsuaga, Mario; Castiella-Aguirrezabala, Andrea; Alonso, Miguel Andres] Arquimea Res Ctr, Tenerife, Spain.
   [Alfonso-Arsuaga, Mario; Garces, Elena] Univ Rey Juan Carlos, Mostoles, Spain.
   [Garcia-Gonzalez, Jorge] Univ Malaga, Malaga, Spain.
C3 Universidad Rey Juan Carlos; Universidad de Malaga
RP Alfonso-Arsuaga, M (corresponding author), Univ Rey Juan Carlos, Mostoles, Spain.
EM malfonsoars@gmail.com; jorgegarcia@uma.es; acastiella@arquimea.com;
   malonso@arquimea.com; elenagarces@gmail.com
RI Garcia-Gonzalez, Jorge/Q-9572-2018
OI Garcia-Gonzalez, Jorge/0000-0001-8610-3462
FU European Union Framework Program, Horizon Europe [101059999]; Spanish
   Ministry of Science, Innovation and Universities (MCIN/AEI); Juan de la
   Cierva-Incorporacion Fellowship [IJC2020-044192-I]
FX Funded by the European Union Framework Program, Horizon Europe
   GA#101059999. Views and opinions expressed are however those of the
   author (s) only and do not necessarily reflect those of the European
   Union. Neither the European Union nor the granting au-thority can be
   held responsible for them. This work was funded in part by the Spanish
   Ministry of Science, Innovation and Universities
   (MCIN/AEI/10.13039/501100011033) . Dr. Elena Garces was partially
   supported by a Juan de la Cierva-Incorporacion Fellowship
   (IJC2020-044192-I) .
CR Bi S, 2020, ARXIV
   Boss M, 2021, NeurIPS, V34, P10691
   Boss M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12664, DOI 10.1109/ICCV48922.2021.01245
   Boss Mark, 2022, arXiv
   Chen AP, 2022, LECT NOTES COMPUT SC, V13692, P333, DOI 10.1007/978-3-031-19824-3_20
   Chen AP, 2022, Arxiv, DOI arXiv:2203.09517
   Du YL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14304, DOI 10.1109/ICCV48922.2021.01406
   Gafni G, 2021, PROC CVPR IEEE, P8645, DOI 10.1109/CVPR46437.2021.00854
   Gao C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5692, DOI 10.1109/ICCV48922.2021.00566
   Garces E, 2022, INT J COMPUT VISION, V130, P836, DOI 10.1007/s11263-021-01563-8
   Gardner J, 2022, Adv Neural Inf Process Syst, V35, P26309
   Gardner JAD, 2023, Arxiv, DOI arXiv:2311.16937
   Guo KW, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356571
   Guo YD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5764, DOI 10.1109/ICCV48922.2021.00573
   Jin H, 2023, PROC CVPR IEEE, P165, DOI 10.1109/CVPR52729.2023.00024
   Laffont PY, 2013, IEEE T VIS COMPUT GR, V19, P210, DOI 10.1109/TVCG.2012.112
   Li TY, 2022, PROC CVPR IEEE, P5511, DOI 10.1109/CVPR52688.2022.00544
   Li ZQ, 2021, PROC CVPR IEEE, P6494, DOI 10.1109/CVPR46437.2021.00643
   Li ZQ, 2020, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR42600.2020.00255
   Liang RF, 2023, Arxiv, DOI arXiv:2210.08398
   Liu Jia-Wei, 2022, arXiv, DOI DOI 10.48550/ARXIV.2205
   Liu L., 2020, Advances in Neural Information Processing Systems, V33, P15651
   Liu Y, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592134
   Martin R, 2022, COMPUT GRAPH FORUM, V41, P163, DOI 10.1111/cgf.14466
   Martin-Brualla R, 2021, PROC CVPR IEEE, P7206, DOI 10.1109/CVPR46437.2021.00713
   Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343
   Meka A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925907
   Mildenhall B, 2020, P ECCV, DOI DOI 10.1007/978-3-030-58452-8
   Munkberg J, 2022, PROC CVPR IEEE, P8270, DOI 10.1109/CVPR52688.2022.00810
   Oechsle M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5569, DOI 10.1109/ICCV48922.2021.00554
   Park K, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480487
   Philip J, 2023, EUROGRAPHICS S RENDE, DOI DOI 10.2312/SR.20231122
   Pumarola A, 2021, PROC CVPR IEEE, P10313, DOI 10.1109/CVPR46437.2021.01018
   Rakotosaona MJ, 2023, Arxiv, DOI arXiv:2303.09431
   Rodriguez-Pardo C, 2023, PROC CVPR IEEE, P5764, DOI 10.1109/CVPR52729.2023.00558
   Rudnev V, 2021, EUR C COMP VIS
   Sabour S, 2023, PROC CVPR IEEE, P20626, DOI 10.1109/CVPR52729.2023.01976
   Song LC, 2023, IEEE T VIS COMPUT GR, V29, P2732, DOI 10.1109/TVCG.2023.3247082
   Srinivasan PP, 2021, PROC CVPR IEEE, P7491, DOI 10.1109/CVPR46437.2021.00741
   Sun C, 2022, PROC CVPR IEEE, P5449, DOI 10.1109/CVPR52688.2022.00538
   Sun Tiancheng, 2021, EUROGRAPHICS S RENDE
   Tancik M, 2022, PROC CVPR IEEE, P8238, DOI 10.1109/CVPR52688.2022.00807
   Toschi M, 2023, PROC CVPR IEEE, P20762, DOI 10.1109/CVPR52729.2023.01989
   Tretschk E, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12939, DOI 10.1109/ICCV48922.2021.01272
   Verbin D, 2022, PROC CVPR IEEE, P5481, DOI 10.1109/CVPR52688.2022.00541
   Wang F, 2023, Arxiv, DOI arXiv:2212.00190
   Wang P, 2021, Arxiv, DOI arXiv:2106.10689
   Wang ZY, 2021, PROC CVPR IEEE, P5700, DOI 10.1109/CVPR46437.2021.00565
   Xian WQ, 2021, PROC CVPR IEEE, P9416, DOI 10.1109/CVPR46437.2021.00930
   Yang B, 2022, ACM Trans Graph, V41, P1
   Yao Y, 2022, LECT NOTES COMPUT SC, V13691, P700, DOI 10.1007/978-3-031-19821-2_40
   Yariv L, 2023, Arxiv, DOI arXiv:2302.14859
   Ye GZ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601135
   Zhang J, 2021, EUROGRAPHICS S RENDE, DOI [10.2312/sr.20211301, DOI 10.2312/SR.20211301]
   Zhang JY, 2023, IEEE I CONF COMP VIS, P3578, DOI 10.1109/ICCV51070.2023.00333
   Zhang JM, 2023, FEM MEDIA STUD, V23, P1327, DOI [10.1080/14680777.2022.2041694, 10.13995/j.cnki.11-1802/ts.030215]
   Zhang K, 2021, PROC CVPR IEEE, P5449, DOI 10.1109/CVPR46437.2021.00541
   Zhang XM, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480496
   Zhang XM, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3446328
   Zhao BM, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1455, DOI 10.1145/3503161.3548125
   Zhu JS, 2023, PROC CVPR IEEE, P12489, DOI 10.1109/CVPR52729.2023.01202
NR 61
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103984
DI 10.1016/j.cag.2024.103984
EA JUL 2024
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YB4Z3
UT WOS:001266025600001
OA hybrid
DA 2024-08-05
ER

PT J
AU Wang, Q
   Fang, Q
   Zhai, XY
   Liu, LG
   Fu, XM
AF Wang, Qi
   Fang, Qing
   Zhai, Xiaoya
   Liu, Ligang
   Fu, Xiao-Ming
TI Differentiable microstructures design via anisotropic thermal diffusion
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Differentiable microstructures; Anisotropic thermal diffusion; Metric
   field optimization
ID TOPOLOGY OPTIMIZATION; METAMATERIALS
AB We propose a novel method to design differentiable microstructures. Central to our algorithm is a new representation of the mapping from the parameters to microstructures, formulated as the anisotropic thermal diffusion. A metric field governs the anisotropic diffusion. The metric associated with each point is represented as a 2 x 2 symmetric positive definite matrix that becomes the design variable. To alleviate the difficulties caused by symmetric positive definite constraints, we perform the singular value decomposition of the metric matrix so that the design variable includes a rotation angle and a diagonal matrix. Then, the positive definiteness is converted to requiring the two diagonal entries of the diagonal matrix to be positive, which is easier to deal with. The effectiveness of our algorithm is demonstrated through evaluations and comparisons over various examples.
C1 [Wang, Qi; Fang, Qing; Zhai, Xiaoya; Liu, Ligang; Fu, Xiao-Ming] Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhai, XY (corresponding author), Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Anhui, Peoples R China.
EM xiaoyazhai@ustc.edu.cn
OI Zhai, Xiaoya/0000-0002-5209-8738
FU Youth Innovation Key Research Funds for the Central Universities, China
   [YD0010002010]; Open Project Program of the State Key Laboratory of
   CAD&CG, Zhejiang University [A2303]; Strategic Priority Research Program
   of the Chinese Academy of Sciences, China [XDB0640000]
FX This work is supported by the Youth Innovation Key Research Funds for
   the Central Universities, China (No. YD0010002010), the Open Project
   Program of the State Key Laboratory of CAD&CG, Zhejiang University (No.
   A2303), and the Strategic Priority Research Program of the Chinese
   Academy of Sciences, China (No. XDB0640000).
CR Al Ali M, 2022, P 15 WORLD C COMP ME, P12
   Al Ali M, 2022, INT J THERM SCI, V179, DOI 10.1016/j.ijthermalsci.2022.107653
   Andreassen E, 2014, MECH MATER, V69, P1, DOI 10.1016/j.mechmat.2013.09.018
   Andreassen E, 2011, STRUCT MULTIDISCIP O, V43, P1, DOI 10.1007/s00158-010-0594-7
   BENDSOE MP, 1988, COMPUT METHOD APPL M, V71, P197, DOI 10.1016/0045-7825(88)90086-2
   Berger JB, 2017, NATURE, V543, P533, DOI 10.1038/nature21075
   Challis VJ, 2019, INT J SOLIDS STRUCT, V171, P17, DOI 10.1016/j.ijsolstr.2019.05.009
   Coelho PG, 2008, STRUCT MULTIDISCIP O, V35, P107, DOI 10.1007/s00158-007-0141-3
   Coelho PG, 2009, J BIOMECH, V42, P830, DOI 10.1016/j.jbiomech.2009.01.020
   Cramer AD, 2016, STRUCT MULTIDISCIP O, V53, P489, DOI 10.1007/s00158-015-1344-7
   Da DC, 2019, ENG COMPUTATION, V36, P126, DOI 10.1108/EC-01-2018-0007
   de Kruijf N, 2007, INT J SOLIDS STRUCT, V44, P7092, DOI 10.1016/j.ijsolstr.2007.03.028
   Du ZL, 2018, J MECH DESIGN, V140, DOI 10.1115/1.4041176
   Ferro N, 2022, STRUCT MULTIDISCIP O, V65, DOI 10.1007/s00158-022-03354-2
   Gao J, 2019, STRUCT MULTIDISCIP O, V60, P2621, DOI 10.1007/s00158-019-02323-6
   Garner E, 2019, ADDIT MANUF, V26, P65, DOI 10.1016/j.addma.2018.12.007
   Gu XC, 2023, STRUCT MULTIDISCIP O, V66, DOI 10.1007/s00158-023-03687-6
   Hao Geng P, 2024, Trans Nonferr Met Soc China, V29, P2483
   HASHIN Z, 1962, J MECH PHYS SOLIDS, V10, P335, DOI 10.1016/0022-5096(62)90004-2
   Kim JE, 2022, J COMPUT DES ENG, V9, P1602, DOI 10.1093/jcde/qwac078
   Lee DK, 2024, ADV MATER, V36, DOI 10.1002/adma.202305254
   Li H, 2018, COMPUT METHOD APPL M, V328, P340, DOI 10.1016/j.cma.2017.09.008
   Mahamood Rasheedat M., 2012, Proceedings of the World Congress on Engineering (WCE 2012), P1593
   Martínez J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925922
   Matsui M, 2023, STRUCT MULTIDISCIP O, V66, DOI 10.1007/s00158-023-03613-w
   Miyamoto Yoshinari, 2013, Functionally Graded Materials: Design, Processing and Applications
   Muñoz D, 2023, COMPUT METHOD APPL M, V405, DOI 10.1016/j.cma.2022.115859
   Panetta J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766937
   Prifling B, 2019, COMP MATER SCI, V169, DOI 10.1016/j.commatsci.2019.109083
   Rodrigues H, 2002, STRUCT MULTIDISCIP O, V24, P1, DOI 10.1007/s00158-002-0209-z
   SIGMUND O, 1994, INT J SOLIDS STRUCT, V31, P2313, DOI 10.1016/0020-7683(94)90154-6
   Sigmund O, 1997, J MECH PHYS SOLIDS, V45, P1037, DOI 10.1016/S0022-5096(96)00114-7
   Silva ECN, 2006, J MATER SCI, V41, P6991, DOI 10.1007/s10853-006-0232-3
   Sivapuram R, 2016, STRUCT MULTIDISCIP O, V54, P1267, DOI 10.1007/s00158-016-1519-x
   SVANBERG K, 1987, INT J NUMER METH ENG, V24, P359, DOI 10.1002/nme.1620240207
   Wang LW, 2022, P NATL ACAD SCI USA, V119, DOI 10.1073/pnas.2122185119
   Wu J, 2021, STRUCT MULTIDISCIP O, V63, P1455, DOI 10.1007/s00158-021-02881-8
   Xu W, 2023, IEEE Trans Vis Comput Graphics, P1
   Zhai XY, 2024, COMPUT METHOD APPL M, V418, DOI 10.1016/j.cma.2023.116530
   Zhang CH, 2022, ADDIT MANUF, V54, DOI 10.1016/j.addma.2022.102786
   Zhou SW, 2008, J MATER SCI, V43, P5157, DOI 10.1007/s10853-008-2722-y
   Zhou XY, 2019, STRUCT MULTIDISCIP O, V60, P1, DOI 10.1007/s00158-019-02293-9
   Zobaer SMT, 2020, COMPUT METHOD APPL M, V370, DOI 10.1016/j.cma.2020.113278
NR 43
TC 0
Z9 0
U1 2
U2 2
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103977
DI 10.1016/j.cag.2024.103977
EA JUN 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XC9K3
UT WOS:001259600900001
DA 2024-08-05
ER

PT J
AU Yao, ZP
   Bi, J
   Deng, W
   He, WL
   Wang, ZH
   Kuang, X
   Zhou, M
   Gao, QQ
   Tong, T
AF Yao, Zhiping
   Bi, Jiang
   Deng, Wei
   He, Wenlin
   Wang, Zihan
   Kuang, Xu
   Zhou, Mi
   Gao, Qinquan
   Tong, Tong
TI DEUNet: Dual-encoder UNet for simultaneous denoising and reconstruction
   of single HDR image
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE HDR reconstruction; Deep learning; Image denoising; Image enhancement
ID DYNAMIC-RANGE EXPANSION; FILTERS
AB High Dynamic Range (HDR) images have more powerful image expression capabilities and better visual quality than Low Dynamic Range (LDR) images, which can sufficiently represent real -world scenes, and will be widely used in the field of film and television in future. However, it is a very challenging task to generate an HDR image from a single -exposure LDR image. In this work, we propose a novel learning -based network, DEUNet, to reconstruct single -frame HDR image with simultaneous denoising and detail reconstruction. The proposed framework consists of two feature extraction branches, which can learn the brightness information and texture information separately for HDR image reconstruction. Each network branch is based on the UNet network structure and the two branches are interacted via spatial feature transformation. As a result, the proposed network can make full use of the multi -scale information at different levels of the image. In addition to the two encoding branches for feature extraction, the proposed network consists of another decoding network for fusing image brightness information and texture information, and a weighting network that selectively preserves most useful information. Compared with state-of-the-art methods, DEUNet can better reduce image noise while reconstructing the details in both the high and the low exposure areas. Experiments have shown that the proposed method achieves state-of-the-art performance on public datasets, indicating the effectiveness of the proposed method in this study.
C1 [Yao, Zhiping; Wang, Zihan; Kuang, Xu; Gao, Qinquan; Tong, Tong] Fuzhou Univ, Coll Phys & Informat Engn, Fuzhou, Peoples R China.
   [Bi, Jiang; He, Wenlin] Beijing Radio & Televis Stn, Beijing, Peoples R China.
   [Deng, Wei] Imperial Vis Technol, Fujian, Peoples R China.
   [Zhou, Mi] Beijing Tongzhou Media Convergence Ctr, Beijing, Peoples R China.
C3 Fuzhou University
RP Tong, T (corresponding author), Fuzhou Univ, Coll Phys & Informat Engn, Fuzhou, Peoples R China.
EM ttraveltong@gmail.com
FU National Natural Science Foundation of China [62171133]; Artificial
   Intelligence and Economy Integration Platform of Fujian Province; Fujian
   Health Commission [2022ZD01003]
FX This work was supported by National Natural Science Foundation of China
   under Grant 62171133, in part by the Artificial Intelligence and Economy
   Integration Platform of Fujian Province, and the Fujian Health
   Commission under Grant 2022ZD01003.
CR Akhil KA, 2021, IEEE COMPUT SOC CONF, P526, DOI 10.1109/CVPRW53098.2021.00064
   Akyüz AG, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239489
   Banterle F, 2006, Proceedings of the 4th international conference on Computer graphics and interactive techniques in Australasia and Southeast Asia, P349
   Banterle F, 2008, Proceedings of Spring Conference on Computer Graphics (SCCG 2008), P33, DOI 10.1145/1921264.1921275
   Banterle F, 2007, VISUAL COMPUT, V23, P467, DOI 10.1007/s00371-007-0124-9
   Banterle F, 2009, COMPUT GRAPH FORUM, V28, P2343, DOI 10.1111/j.1467-8659.2009.01541.x
   BERNSTEIN R, 1987, IEEE T CIRCUITS SYST, V34, P1275, DOI 10.1109/TCS.1987.1086066
   Cao GF, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3550277
   Chen XY, 2021, IEEE COMPUT SOC CONF, P354, DOI 10.1109/CVPRW53098.2021.00045
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Debevec PE, 2008, ACM SIGGRAPH 2008 CL
   Eilertsen G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130816
   Endo Y, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130834
   Froehlich J, 2014, PROC SPIE, V9023, DOI 10.1117/12.2040003
   Guanting Dong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P35, DOI 10.1007/978-3-030-58586-0_3
   He JW, 2019, PROC CVPR IEEE, P11048, DOI 10.1109/CVPR.2019.01131
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hong SW, 2000, IMAGE VISION COMPUT, V18, P573, DOI 10.1016/S0262-8856(99)00020-7
   Huang J, 2019, LECT NOTES COMPUT SC, V11133, P230, DOI 10.1007/978-3-030-11021-5_15
   Huo YQ, 2014, VISUAL COMPUT, V30, P507, DOI 10.1007/s00371-013-0875-4
   Jiang WZ, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3127988
   Jingwen He, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P53, DOI 10.1007/978-3-030-58565-5_4
   Kingma D. P., 2014, arXiv
   Kinoshita Y, 2019, IEEE ACCESS, V7, P73555, DOI 10.1109/ACCESS.2019.2919296
   Kovaleski RP, 2014, 2014 27TH SIBGRAPI CONFERENCE ON GRAPHICS, PATTERNS AND IMAGES (SIBGRAPI), P49, DOI 10.1109/SIBGRAPI.2014.29
   Li C, 2016, IEEE INT SEMICONDUCT
   Liu YH, 2023, IEEE T MULTIMEDIA, V25, P4638, DOI 10.1109/TMM.2022.3179904
   Liu YL, 2020, PROC CVPR IEEE, P1648, DOI 10.1109/CVPR42600.2020.00172
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935
   Mao XJ, 2016, ADV NEUR IN, V29
   Marnerides D, 2018, COMPUT GRAPH FORUM, V37, P37, DOI 10.1111/cgf.13340
   Narwaria M, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.1.010501
   Perez-Pellitero E, 2021, IEEE COMPUT SOC CONF, P691, DOI 10.1109/CVPRW53098.2021.00078
   PITAS I, 1986, IEEE T ACOUST SPEECH, V34, P573, DOI 10.1109/TASSP.1986.1164857
   Plötz T, 2018, ADV NEUR IN, V31
   Quan TM, 2021, FRONT COMP SCI-SWITZ, V3, DOI 10.3389/fcomp.2021.613981
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Santos MS, 2020, Arxiv, DOI arXiv:2005.07335
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Vien AG, 2022, LECT NOTES COMPUT SC, V13667, P435, DOI 10.1007/978-3-031-20071-7_26
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Wang Y, 2020, EUR C COMP VIS, P1
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu GT, 2022, PATTERN RECOGN, V127, DOI 10.1016/j.patcog.2022.108620
   Yang X, 2018, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2018.00193
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zhang JS, 2017, IEEE I CONF COMP VIS, P4529, DOI 10.1109/ICCV.2017.484
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang L, 2012, IEEE IMAGE PROC, P1477, DOI 10.1109/ICIP.2012.6467150
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
NR 54
TC 0
Z9 0
U1 3
U2 3
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103882
DI 10.1016/j.cag.2024.01.009
EA MAR 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PA7E3
UT WOS:001211416400001
DA 2024-08-05
ER

PT J
AU Kushwaha, M
   Choudhary, J
   Singh, DP
AF Kushwaha, Mohit
   Choudhary, Jaytrilok
   Singh, Dhirendra Pratap
TI 3DPMesh: An enhanced and novel approach for the reconstruction of 3D
   human meshes from a single 2D image
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE 3D human mesh; Mesh articulation; 3D human pose; 3D pose enhancement; 3D
   pose fitting
AB Currently, virtual reality, the gaming industry, cloth parsing, surveillance and security systems, and avatars have gained much interest in computer vision. To fulfil the growing demand in this field, an accurate 3D human mesh plays an important role. We proposed a novel approach, 3DPMesh, to reconstruct more accurate 3D meshes for humans from a single 2D image. 3DPMesh stands for 3DPose estimation, 3DPose enhancement and Mesh articulation. We achieve enhanced results of 3D meshes with the ultimate fusion of these three modules. The depth predictor regresses the 3D keypoints of the human body, whereas the pose alignment enhances the accuracy of the predicted 3D joint locations. Finally, the mesh articulation module articulates the parametric mesh so that it accurately matches the 3D pose of a person. The proposed method offers more accurate 3D human meshes and better alignment to the content of an input image when compared to the state -of -the -art methods on the challenging UP -3D, Human3.6M, and 3DPW datasets. Additionally, there is no need for a training phase in the proposed model, therefore, it does not require an extensive collection of datasets to train the model, so it saves a lot of computational time and ultimately generates a 3D mesh in less inference time. The experimental results and analysis of the proposed method indicate that it advances the mesh fitting state -of -the -art methods at any scale variation.
C1 [Kushwaha, Mohit; Choudhary, Jaytrilok; Singh, Dhirendra Pratap] Maulana Azad Natl Inst Technol MANIT, Dept Comp Sci & Engn, Bhopal 462003, India.
C3 National Institute of Technology (NIT System); Maulana Azad National
   Institute of Technology Bhopal
RP Kushwaha, M (corresponding author), Maulana Azad Natl Inst Technol MANIT, Dept Comp Sci & Engn, Bhopal 462003, India.
EM mohitkushwaha786@gmail.com; jaytrilok@manit.ac.in; dpsingh@manit.ac.in
RI SINGH, DHIRENDRA PRATAP/H-1108-2014; SINGH, DHIRENDRA
   PRATAP/AAF-3481-2019
OI Kushwaha, Mohit/0000-0002-5414-5008
CR Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   [Anonymous], 1987, Bull Int Stat Inst
   BenAbdelkader C., 2008, P 8 IEEE INT C AUT F, P1, DOI [DOI 10.1109/AFGR.2008.4813453, 10.1109/AFGR.2008.4813453]
   Biggs B, 2020, ADV NEUR IN, V33
   Choi H, 2022, PROC CVPR IEEE, P1465, DOI 10.1109/CVPR52688.2022.00153
   Choi Hongsuk, 2020, COMPUTER VISION ECCV
   Choutas Vasileios, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P20, DOI 10.1007/978-3-030-58607-2_2
   Fan TS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11437, DOI 10.1109/ICCV48922.2021.01126
   Fang HS, 2023, IEEE T PATTERN ANAL, V45, P7157, DOI 10.1109/TPAMI.2022.3222784
   Georgakis Georgios, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P768, DOI 10.1007/978-3-030-58520-4_45
   Güler RA, 2019, PROC CVPR IEEE, P10876, DOI 10.1109/CVPR.2019.01114
   Gyeongsik Moon, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P752, DOI 10.1007/978-3-030-58571-6_44
   Hao CH, 2023, COMPUT GRAPH-UK, V115, P339, DOI 10.1016/j.cag.2023.07.011
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Iqbal U, 2021, INT CONF 3D VISION, P689, DOI 10.1109/3DV53792.2021.00078
   Johnson DavidB., 2001, AD HOC NETWORKING, P139
   Joo H, 2021, INT CONF 3D VISION, P42, DOI 10.1109/3DV53792.2021.00015
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Khirodkar R, 2022, PROC CVPR IEEE, P1705, DOI 10.1109/CVPR52688.2022.00176
   Kocabas M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11015, DOI 10.1109/ICCV48922.2021.01085
   Kocabas M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11107, DOI 10.1109/ICCV48922.2021.01094
   Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530
   Kolotouros N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11585, DOI 10.1109/ICCV48922.2021.01140
   Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234
   Kolotouros N, 2019, PROC CVPR IEEE, P4496, DOI 10.1109/CVPR.2019.00463
   Kundu Jogendra Nath, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P794, DOI 10.1007/978-3-030-58452-8_46
   Kushwaha M, 2022, COMPUT GRAPH-UK, V107, P172, DOI 10.1016/j.cag.2022.07.021
   Lassner C, 2017, PROC CVPR IEEE, P4704, DOI 10.1109/CVPR.2017.500
   Li JF, 2021, PROC CVPR IEEE, P3382, DOI 10.1109/CVPR46437.2021.00339
   Lin K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12919, DOI 10.1109/ICCV48922.2021.01270
   Lin K, 2021, PROC CVPR IEEE, P1954, DOI 10.1109/CVPR46437.2021.00199
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Luan TY, 2021, AAAI CONF ARTIF INTE, V35, P2269
   Moon G, 2022, Arxiv, DOI arXiv:2011.11534
   Omran M, 2018, INT CONF 3D VISION, P484, DOI 10.1109/3DV.2018.00062
   Osokin D, 2019, ICPRAM: PROCEEDINGS OF THE 8TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION APPLICATIONS AND METHODS, P744, DOI 10.5220/0007555407440748
   Pavlakos G, 2022, PROC CVPR IEEE, P1475, DOI 10.1109/CVPR52688.2022.00154
   Pavlakos G, 2019, IEEE I CONF COMP VIS, P803, DOI 10.1109/ICCV.2019.00089
   Pavlakos G, 2018, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2018.00055
   Ran H, 2023, NEUROCOMPUTING, V548, DOI 10.1016/j.neucom.2023.126284
   Rockwell Chris, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P522, DOI 10.1007/978-3-030-58520-4_31
   Rong Y, 2019, IEEE I CONF COMP VIS, P5339, DOI 10.1109/ICCV.2019.00544
   Rüegg N, 2020, AAAI CONF ARTIF INTE, V34, P5561
   Sengupta A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11199, DOI 10.1109/ICCV48922.2021.01103
   Sengupta A, 2021, PROC CVPR IEEE, P16089, DOI 10.1109/CVPR46437.2021.01583
   Sengupta Akash, 2020, BRIT MACH VIS C BMVC
   Sun Y, 2019, IEEE I CONF COMP VIS, P5348, DOI 10.1109/ICCV.2019.00545
   Tian YT, 2023, IEEE T PATTERN ANAL, V45, P15406, DOI 10.1109/TPAMI.2023.3298850
   Tianshu Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7374, DOI 10.1109/CVPR42600.2020.00740
   Tung HYF, 2017, ADV NEUR IN, V30
   Varol G, 2018, LECT NOTES COMPUT SC, V11211, P20, DOI 10.1007/978-3-030-01234-2_2
   Volk N., 2016, Bones can tell us more, P1
   von Marcard T, 2018, LECT NOTES COMPUT SC, V11214, P614, DOI 10.1007/978-3-030-01249-6_37
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Xiangyu Xu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P284, DOI 10.1007/978-3-030-58545-7_17
   Xu YL, 2019, IEEE I CONF COMP VIS, P7759, DOI 10.1109/ICCV.2019.00785
   Yao PF, 2019, Arxiv, DOI arXiv:1903.10153
   Yu ZB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8599, DOI 10.1109/ICCV48922.2021.00850
   Zanfir A, 2021, PROC CVPR IEEE, P14479, DOI 10.1109/CVPR46437.2021.01425
   Zanfir Andrei, 2020, EUR C COMP VIS, P465, DOI DOI 10.48550/ARXIV.2003.10350
   Zanfir M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12951, DOI 10.1109/ICCV48922.2021.01273
   Zeng W, 2022, PROC CVPR IEEE, P11091, DOI 10.1109/CVPR52688.2022.01082
   Zeng W, 2020, PROC CVPR IEEE, P7052, DOI 10.1109/CVPR42600.2020.00708
   Zhang HW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11426, DOI 10.1109/ICCV48922.2021.01125
   Zhang HW, 2022, IEEE T PATTERN ANAL, V44, P2610, DOI 10.1109/TPAMI.2020.3042341
   Zhang HW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P935, DOI 10.1145/3343031.3351057
   Zheng ZR, 2019, IEEE I CONF COMP VIS, P7738, DOI 10.1109/ICCV.2019.00783
   Zhengyi Luo, 2021, Computer Vision - ACCV 2020 15th Asian Conference on Computer Vision. Revised Selected Papers. Lecture Notes in Computer Science (LNCS 12626), P324, DOI 10.1007/978-3-030-69541-5_20
   Zou SH, 2023, IEEE T MULTIMEDIA, V25, P3560, DOI 10.1109/TMM.2022.3162469
NR 70
TC 0
Z9 0
U1 4
U2 4
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103894
DI 10.1016/j.cag.2024.103894
EA FEB 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LK3I7
UT WOS:001186650900001
DA 2024-08-05
ER

PT J
AU Titov, A
   Kersten-Oertel, M
   Drouin, S
AF Titov, Andrey
   Kersten-Oertel, Marta
   Drouin, Simon
TI Contextual Ambient Occlusion: A volumetric rendering technique that
   supports real-time clipping
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Volume rendering; Ambient occlusion; Clipping; Carving
ID ILLUMINATION
AB In this paper, we present a new volumetric ambient occlusion algorithm called Contextual Ambient Occlusion (CAO) that supports real-time clipping. The algorithm produces ambient occlusion images of exactly the same quality as Local Ambient Occlusion (LAO) while enabling real-time modification to the shape used to clip the volume. The main idea of the algorithm is that clipping only affects the ambient value of a small number of voxels, so by identifying these voxels and recalculating the ambient factor only for them, it is possible to significantly increase the rendering performance (by 2-5x) without decreasing the quality of the rendered image. Due to its fast performance, the algorithm is suitable for interactive environments where clipping changes could occur every frame. Additionally, the algorithm does not have any stereoscopic inconsistency, which makes it suitable for mixed reality environments. This paper is an extended version of the "Contextual Ambient Occlusion"article presented during the 2023 Graphics Interface conference, and includes, among other additions, the source code of the algorithm.
C1 [Titov, Andrey; Drouin, Simon] Ecole Technol Super, Synchromedia Lab Multimedia Commun Telepresence, Montreal, PQ H3C 1K3, Canada.
   [Titov, Andrey; Kersten-Oertel, Marta] Concordia Univ, Appl Percept Lab, Montreal, PQ H3G 1M8, Canada.
C3 University of Quebec; Ecole de Technologie Superieure - Canada;
   Concordia University - Canada
RP Titov, A (corresponding author), Ecole Technol Super, Synchromedia Lab Multimedia Commun Telepresence, Montreal, PQ H3C 1K3, Canada.
EM andrey.titov.1@ens.etsmtl.ca; marta@ap-lab.ca; simon.drouin@etsmtl.ca
OI Titov, Andrey/0000-0001-5042-2736
FU Natural Sciences and Engineering Research Council of Canada
   [RGPIN-2020-05084]; Fonds de recherche du Quebec - Nature et
   technologies (B2X Doctoral Scholarship)
FX This work was supported by the Natural Sciences and Engineering Research
   Council of Canada, Discovery program (RGPIN-2020-05084) , as well as by
   the Fonds de recherche du Quebec - Nature et technologies (B2X Doctoral
   Scholarship) .
CR Bavoil L., 2008, Tech. rep
   Díaz J, 2010, COMPUT GRAPH-UK, V34, P337, DOI 10.1016/j.cag.2010.03.005
   Engel D, 2021, IEEE T VIS COMPUT GR, V27, P1268, DOI 10.1109/TVCG.2020.3030344
   Everitt C, 2001, INTERACTIVE ORDER IN
   Hernell F, 2007, Efficient ambient and emissive tissue illumination using local occlusion in multiresolution volume rendering, DOI [10.2312/VG/VG07/001-008, DOI 10.2312/VG/VG07/001-008]
   Hernell F, 2010, IEEE T VIS COMPUT GR, V16, P548, DOI 10.1109/TVCG.2009.45
   illiams Lance, 1978, SIGGRAPH Comput. Graph, P270
   Joshi A, 2008, IEEE T VIS COMPUT GR, V14, P1587, DOI 10.1109/TVCG.2008.150
   Keinert B, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818131
   MAX N, 1995, IEEE T VIS COMPUT GR, V1, P99, DOI 10.1109/2945.468400
   Miller G., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P319, DOI 10.1145/192161.192244
   PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839
   Ropinski T, 2008, COMPUT GRAPH FORUM, V27, P567, DOI 10.1111/j.1467-8659.2008.01154.x
   Schlegel P, 2011, IEEE T VIS COMPUT GR, V17, P1795, DOI 10.1109/TVCG.2011.198
   Shi PT, 2022, P ACM COMPUT GRAPH, V5, DOI 10.1145/3522614
   Stewart AJ, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P355, DOI 10.1109/VISUAL.2003.1250394
   Titov A, 2023, Graphics interface 2023
   Weiskopf D, 2003, IEEE T VIS COMPUT GR, V9, P298, DOI 10.1109/TVCG.2003.1207438
   Weiskopf D, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P93, DOI 10.1109/VISUAL.2002.1183762
NR 19
TC 0
Z9 0
U1 2
U2 2
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103884
DI 10.1016/j.cag.2024.01.011
EA FEB 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LK5H5
UT WOS:001186702400001
OA hybrid, Green Published
DA 2024-08-05
ER

PT J
AU Nguyen, A
   Francis, M
   Windfeld, E
   Lhermie, G
   Kim, K
AF Nguyen, Anh
   Francis, Michael
   Windfeld, Emma
   Lhermie, Guillaume
   Kim, Kangsoo
TI Developing an immersive virtual farm simulation for engaging and
   effective public education about the dairy industry
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Virtual reality; Dairy farm simulation; Public education; Dairy
   industry; Usability
ID ITERATION; CONSUMER; FOOD
AB Growing public interest in understanding the origins and production methods of dairy products, driven by concerns related to environmental impact, local sourcing, and ethics, highlights an important trend. Nevertheless, a knowledge-trust gap persists between consumers and the dairy industry. Addressing this gap, in this paper, we developed an immersive virtual farm simulation to provide realistic on-farm experiences to the public. Within the virtual farm, users can explore various sites where dairy cows are raised and gain insights into dairy production processes using a head-mounted display (HMD). This simulation was demonstrated at local libraries, involving 48 public participants. We collected and analyzed participants' feedback on various aspects, including usability and their overall perceptions, to assess the simulation's effectiveness as an agricultural education tool. We investigated the impact of the virtual experience on participants' perceived knowledge gain and their awareness of the dairy industry. The results indicate that our dairy farm simulation was positively received as an effective tool for public education. Emphasizing the potential of virtual reality (VR) simulations in agricultural education and the industry, we discuss our key findings and future plans.
C1 [Nguyen, Anh] Univ Calgary, Dept Comp Sci, 2500 Univ Drive NW, Calgary, AB T2N 1N4, Canada.
   [Francis, Michael; Kim, Kangsoo] Univ Calgary, Dept Elect & Software Engn, 2500 Univ Drive NW, Calgary, AB T2N 1N4, Canada.
   [Windfeld, Emma; Lhermie, Guillaume] Univ Calgary, Simpson Ctr, Sch Publ Policy, 906 8 Ave SW, Calgary, AB T2P 1H9, Canada.
C3 University of Calgary; University of Calgary; University of Calgary
RP Kim, K (corresponding author), Univ Calgary, Dept Elect & Software Engn, 2500 Univ Drive NW, Calgary, AB T2N 1N4, Canada.
EM anh.nguyen5@ucalgary.ca; mjfranci@ucalgary.ca;
   emma.windfeld@ucalgary.ca; guillaume.lhermie@ucalgary.ca;
   kangsoo.kim@ucalgary.ca
RI Nguyen, Anh Quang/KRR-1429-2024; Kim, Kangsoo/AAL-9592-2020
OI Nguyen, Anh Quang/0009-0004-5609-076X; Kim, Kangsoo/0000-0002-0925-378X
FU Simpson Centre; School of Public Policy; Schulich School of Engineering
   Undergraduate Student Research Award at the University of Calgary,
   Canada; Natural Sciences and Engineering Research Council of Canada
   (NSERC) [RGPIN-2022-03294]
FX This work was supported by grants from the Simpson Centre, the School of
   Public Policy, and the Schulich School of Engineering Undergraduate
   Student Research Award at the University of Calgary, Canada. We
   acknowledge the support of the Natural Sciences and Engineering Research
   Council of Canada (NSERC) , [RGPIN-2022-03294] . The authors wish to
   thank Dr. Maxime Delsart (Veterinary Medicine, Ecole Nationale
   Veterinaire d'Alfort) , Tietsia Huyzer (Huntcliff Dairy Ltd., Olds, AB)
   , and other domain experts for sharing their knowledge and experience.
   We also thank our graphic designer, Harry Cho, for his contribution to
   designing the 3D models and animations.
CR Ahn SJ, 2014, COMPUT HUM BEHAV, V39, P235, DOI 10.1016/j.chb.2014.07.025
   Allen S, 2018, BRIT FOOD J, V120, P2304, DOI 10.1108/BFJ-12-2017-0733
   Allison D., 2000, Proceedings of the ACM Symposium on Virtual Reality Software and Technology, P160
   Anastasiou E, 2023, SMART AGR TECHNOL, V3, DOI 10.1016/j.atech.2022.100105
   Nguyen A, 2023, 2023 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS, VRW, P438, DOI 10.1109/VRW58643.2023.00095
   Arenna, 2019, CAN J AGR ECON, V67, P151, DOI 10.1111/cjag.12190
   Awori J, 2023, 3D PRINT MED, V9, DOI 10.1186/s41205-022-00164-6
   Bendigeri C, 2023, Metaverse and immersive technologies, P191, DOI [10.1002/9781394177165.ch7, DOI 10.1002/9781394177165.CH7]
   Bricken M., 1991, Computer Graphics, V25, P178, DOI 10.1145/126640.126657
   Brooke J., 1996, SUS-a quick and dirty usability scale, DOI [DOI 10.1201/9781498710411-35, DOI 10.1201/9781498710411]
   Calsamiglia S, 2020, J DAIRY SCI, V103, P2896, DOI 10.3168/jds.2019-16714
   Chang AY, 2012, ISLE-INTERDISCIP STU, V19, P237, DOI 10.1093/isle/iss007
   Chateau H, 2021, Virtual visits of pig farms in times of COVID-19, a feedback
   Clement C, 2015, To market to market: Exploring distribution models in nova scotia
   David S, 2014, PROCEEDINGS OF INTERNATIONAL CONFERENCE INFORMATION SYSTEMS AND DESIGN OF COMMUNICATION (ISDOC2014), P1, DOI 10.1145/2618168.2618169
   Ericson JD, 2022, J KNOWL ECON, V13, P406, DOI 10.1007/s13132-021-00733-w
   Gonzalez-Franco M, 2020, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.561558
   Hallström J, 2022, INT J TECHNOL DES ED, V32, P17, DOI 10.1007/s10798-020-09600-2
   Hamilton D, 2021, J COMPUT EDUC, V8, P1, DOI 10.1007/s40692-020-00169-2
   Han B, 2023, COMPUT APPL ENG EDUC, V31, P1174, DOI 10.1002/cae.22632
   Happer C, 2019, FOOD SECUR, V11, P123, DOI 10.1007/s12571-018-0877-1
   Ippolito PM, 1999, FOOD POLICY, V24, P295, DOI 10.1016/S0306-9192(99)00025-1
   Jimenez IAC, 2020, MDPI
   Lampropoulos G, 2022, Augmented reality and virtual reality in education: Public perspectives, sentiments, attitudes, and discourses, V12
   Li J, 2021, EXTENDED ABSTRACTS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'21), DOI 10.1145/3411763.3441346
   MUMTAZ M, 2022, IEEE ACCESS, V10, P65355, DOI 10.1109/ACCESS.2022.3182703
   Muringai V, 2020, CAN J AGR ECON, V68, P47, DOI 10.1111/cjag.12221
   Özacar K, 2023, COMPUT GRAPH-UK, V113, P1, DOI 10.1016/j.cag.2023.04.008
   Pabian S, 2020, FRONT COMMUN, V5, DOI 10.3389/fcomm.2020.00069
   Radianti J, 2020, COMPUT EDUC, V147, DOI 10.1016/j.compedu.2019.103778
   Rubio-Tamayo Jose Luis, 2017, Multimodal Technologies and Interaction, V1, DOI 10.3390/mti1040021
   Sauro J, 2012, QUANTIFYING THE USER EXPERIENCE: PRACTICAL STATISTICS FOR USER RESEARCH, P1
   Schuetz A, 2022, PLOS ONE, V17, DOI 10.1371/journal.pone.0261248
   Sutherland C, 2020, J AGRIC FOOD INF, V21, P50, DOI 10.1080/10496505.2020.1724114
   Tarng W, 2012, INT J DIST EDUC, V10, P1, DOI 10.4018/jdet.2012040101
   Turcotte J, 2015, J COMPUT-MEDIAT COMM, V20, P520, DOI 10.1111/jcc4.12127
   Urem F., 2021, 2021 44th International Convention on Information, Communication and Electronic Technology (MIPRO), P532, DOI 10.23919/MIPRO52101.2021.9596752
   Weatherell C, 2003, J RURAL STUD, V19, P233, DOI 10.1016/S0743-0167(02)00083-9
   Wu HY, 2021, INT J HUM-COMPUT ST, V147, DOI 10.1016/j.ijhcs.2020.102576
   Wynn DC, 2017, RES ENG DES, V28, P153, DOI 10.1007/s00163-016-0226-3
   Ye S, 2023, JICTE
   Yu F, 2010, IFIP ADV INF COMM TE, V317, P546
NR 42
TC 1
Z9 1
U1 1
U2 1
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD FEB
PY 2024
VL 118
BP 173
EP 183
DI 10.1016/j.cag.2023.12.011
EA JAN 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IE4P3
UT WOS:001164640800001
DA 2024-08-05
ER

PT J
AU Yurtoglu, A
   Sonlu, S
   Dogan, Y
   Güdükbay, U
AF Yurtoglu, Ayda
   Sonlu, Sinan
   Dogan, Yalim
   Gudukbay, Ugur
TI Personality perception in human videos altered by motion transfer
   networks
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Personality perception; Five-Factor model; Motion transfer networks;
   Optical flow; Motion cues; Physical appearance
ID PEER; SELF; APPEARANCE; AGREEMENT; HAIR
AB The successful portrayal of personality in digital characters improves communication and immersion. Current research focuses on expressing personality through modifying animations using heuristic rules or data -driven models. While studies suggest motion style highly influences the apparent personality, the role of appearance can be similarly essential. This work analyzes the influence of movement and appearance on the perceived personality of short videos altered by motion transfer networks. We label the personalities in conference video clips with a user study to determine the samples that best represent the Five -Factor model's high, neutral, and low traits. We alter these videos using the Thin -Plate Spline Motion Model, utilizing the selected samples as the source and driving inputs. We follow five different cases to study the influence of motion and appearance on personality perception. Our comparative study reveals that motion and appearance influence different factors: motion strongly affects perceived extraversion, and appearance helps convey agreeableness and neuroticism.
C1 [Yurtoglu, Ayda; Sonlu, Sinan; Dogan, Yalim; Gudukbay, Ugur] Bilkent Univ, Dept Comp Engn, TR-06800 Ankara, Turkiye.
C3 Ihsan Dogramaci Bilkent University
RP Güdükbay, U (corresponding author), Bilkent Univ, Dept Comp Engn, TR-06800 Ankara, Turkiye.
EM ayda.yurtoglu@ug.bilkent.edu.tr; sinan.sonlu@bilkent.edu.tr;
   yalim.dogan@bilkent.edu.tr; gudukbay@cs.bilkent.edu.tr
OI Dogan, Yalim/0000-0002-0814-2439
FU Scientific and Technological Research Council of Turkiye (TUBIdot;TAK)
   [122E123]
FX This research is supported by The Scientific and Technological Research
   Council of Turkiye (TUB & Idot;TAK) under Grant No. 122E123.
CR AGNEW R, 1984, YOUTH SOC, V15, P285, DOI 10.1177/0044118X84015003002
   Ajili I, 2019, VISUAL COMPUT, V35, P1411, DOI 10.1007/s00371-018-01619-w
   Amadou N, 2023, P 23 ACM INT C INTEL, P8
   Ao TL, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592097
   Aristidou A, 2015, COMPUT GRAPH FORUM, V34, P262, DOI 10.1111/cgf.12598
   Bansal A, 2018, Arxiv, DOI arXiv:1808.05174
   Blake R, 2007, ANNU REV PSYCHOL, V58, P47, DOI 10.1146/annurev.psych.57.102904.190152
   Bornstein RF, 2012, Handbook of Psychology, V5
   Boyle GJ., 2008, The SAGE handbook of personality theory and assessment. Vol. 1. Personality Theories and Models, P1, DOI DOI 10.4135/9781849200462.N1
   Calsius F, 2019, P 31 BENELUX C ARTIF, P1
   Chan CRLE, 2019, Arxiv, DOI arXiv:1808.07371
   Choi Y, 2018, Arxiv, DOI arXiv:1711.09020
   Davidson RJ, 2001, ANN NY ACAD SCI, V935, P191, DOI 10.1111/j.1749-6632.2001.tb03481.x
   Deng Y, 2020, Arxiv, DOI arXiv:2004.11660
   Dewan S, 2018, INT C PATT RECOG, P2911, DOI 10.1109/ICPR.2018.8545251
   Doukas M. C., 2020, arXiv
   Durupinar F, 2020, Arxiv, DOI arXiv:2012.02224
   Durupinar F, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2983620
   Erkoc Z., 2022, UNDERSTANDING SOCIAL, P74
   Eshraghian JK, 2020, NAT MACH INTELL, V2, P157, DOI 10.1038/s42256-020-0161-x
   Feinberg RA., 1992, CLOTH TEXT RES J, V11, P18, DOI [10.1177/0887302X9201100103, DOI 10.1177/0887302X9201100103]
   Ferstl Y, 2021, COMPUT ANIMAT VIRT W, V32, DOI 10.1002/cav.2016
   Funder DC, 2012, CURR DIR PSYCHOL SCI, V21, P177, DOI 10.1177/0963721412445309
   FUNDER DC, 1988, J PERS SOC PSYCHOL, V55, P149, DOI 10.1037/0022-3514.55.1.149
   Geng ZL, 2019, Arxiv, DOI arXiv:1902.08900
   Ghorbani S, 2022, PROCEEDINGS OF THE 2022 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, ICMI 2022, P778, DOI 10.1145/3536221.3558068
   Ghorbani S, 2023, COMPUT GRAPH FORUM, V42, P206, DOI 10.1111/cgf.14734
   Giraud T, 2015, ACM T APPL PERCEPT, V13, DOI 10.1145/2791294
   Gloor PA, 2022, FUTURE INTERNET, V14, DOI 10.3390/fi14010005
   Google Developers, 2023, MediaPipe
   Gosling SD, 2003, J RES PERS, V37, P504, DOI 10.1016/S0092-6566(03)00046-1
   Ha S., 2019, arXiv
   Hachimura K, 2005, 2005 IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (RO-MAN), P294
   He Y, 2022, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS, IVA 2022, DOI 10.1145/3514197.3549697
   Higgins D, 2023, PROCEEDINGS OF THE ACM SYMPOSIUM ON APPLIED PERCEPTION, SAP 2023, DOI 10.1145/3605495.3605799
   Hu Y, 2023, COGNITION, V231, DOI 10.1016/j.cognition.2022.105309
   Hu Y, 2018, PSYCHOL SCI, V29, P1969, DOI 10.1177/0956797618799300
   Hyde J, 2016, ACM T APPL PERCEPT, V13, DOI 10.1145/2851499
   Jones AL, 2012, J EXP PSYCHOL HUMAN, V38, P1353, DOI 10.1037/a0027078
   Kammoun A, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3527850
   Kandler C, 2010, J PERS, V78, P1565, DOI 10.1111/j.1467-6494.2010.00661.x
   Kathleen Bishop S, 2020, Encyclopedia of Personality and Individual Differences, P421
   Kim HJ, 2022, ACM T APPL PERCEPT, V19, DOI 10.1145/3473041
   Koppensteiner M, 2013, J EXP SOC PSYCHOL, V49, P1137, DOI 10.1016/j.jesp.2013.08.002
   Lang YN, 2019, AAAI CONF ARTIF INTE, P1707
   LAWSON ED, 1971, PSYCHOL REP, V28, P311, DOI 10.2466/pr0.1971.28.1.311
   Liu LJ, 2019, Arxiv, DOI arXiv:1809.03658
   Liu Y., 2022, arXiv
   McCrae R. R., 2005, Personality in adulthood: afive-factor theory perspective
   Mcdonnell R, 2009, ACM T APPL PERCEPT, V5, DOI 10.1145/1462048.1462051
   Milcent AS, 2022, INT J HUM-COMPUT INT, V38, P240, DOI 10.1080/10447318.2021.1938387
   Mori M, 2012, IEEE ROBOT AUTOM MAG, V19, P98, DOI 10.1109/MRA.2012.2192811
   Namatame H, 2016, Int J Affect Eng, V15, P161, DOI DOI 10.5057/IJAE.IJAE-D-15-00021
   Naumann LP, 2009, PERS SOC PSYCHOL B, V35, P1661, DOI 10.1177/0146167209346309
   Neff M, 2010, LECT NOTES ARTIF INT, V6356, P222, DOI 10.1007/978-3-642-15892-6_24
   Nirkin Y, 2019, Arxiv, DOI arXiv:1908.05932
   Paetzel-Prüsmann M, 2021, COMPUT HUM BEHAV, V120, DOI 10.1016/j.chb.2021.106756
   PANCER SM, 1978, PERCEPT MOTOR SKILL, V46, P1328, DOI 10.2466/pms.1978.46.3c.1328
   Paunonen SV, 1999, J PERS, V67, P555, DOI 10.1111/1467-6494.00065
   Garcia MP, 2020, ADV INTELL SYST COMP, V1037, P1017, DOI 10.1007/978-3-030-29516-5_76
   Pozzebon JA, 2009, J INDIVID DIFFER, V30, P122, DOI 10.1027/1614-0001.30.3.122
   Ready RE, 2000, J RES PERS, V34, P208, DOI 10.1006/jrpe.1999.2280
   Ren YR, 2020, Arxiv, DOI arXiv:2003.00696
   Sajjadi P, 2019, ENTERTAIN COMPUT, V32, DOI 10.1016/j.entcom.2019.100313
   Seymour M, 2019, PROCEEDINGS OF THE 52ND ANNUAL HAWAII INTERNATIONAL CONFERENCE ON SYSTEM SCIENCES, P1748
   Sha T, 2021, ACM Comput Surv, V55, P37
   Siarohin A., 2019, ADV NEURAL INFORM PR, V32, P7137
   Siarohin A, 2019, Arxiv, DOI arXiv:1812.08861
   Siarohin A, 2021, PROC CVPR IEEE, P13648, DOI 10.1109/CVPR46437.2021.01344
   Smith HJ, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073697
   Sonlu S, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3439795
   Suen HY, 2019, IEEE ACCESS, V7, P61018, DOI 10.1109/ACCESS.2019.2902863
   Tevet G, 2023, P 11 INT C LEARNING
   Tewari A, 2022, COMPUT GRAPH FORUM, V41, P703, DOI 10.1111/cgf.14507
   Thakkar J, 2022, COMPANION PUBLICATION OF THE 2022 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, ICMI 2022, P61, DOI 10.1145/3536220.3558802
   Thies J, 2020, Arxiv, DOI arXiv:2007.14808
   Thomas S, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P11, DOI 10.1109/VR51125.2022.00018
   Toshpulatov M, 2021, IMAGE VISION COMPUT, V108, DOI 10.1016/j.imavis.2021.104119
   TUKEY JW, 1949, BIOMETRICS, V5, P99, DOI 10.2307/3001913
   Tulyakov S, 2017, Arxiv, DOI arXiv:1707.04993
   Vinciarelli A, 2014, IEEE T AFFECT COMPUT, V5, P273, DOI 10.1109/TAFFC.2014.2330816
   Wang YY, 2016, ACM T APPL PERCEPT, V13, DOI 10.1145/2874357
   Yang Zhuoran, 2020, ARXIV
   Ye Z, 2022, AI Open, V3, P35
   Yuan Ye, 2023, 2023 IEEE/CVF International Conference on Computer Vision (ICCV), P15964, DOI 10.1109/ICCV51070.2023.01467
   Zabala U, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11104639
   Zakharov E., 2019, arXiv
   Zhang MY, 2022, Arxiv, DOI arXiv:2208.15001
   Zhao J, 2022, PROC CVPR IEEE, P3647, DOI 10.1109/CVPR52688.2022.00364
   Zhou MX, 2019, ACM T INTERACT INTEL, V9, DOI 10.1145/3232077
   Zhu Z, 2019, Arxiv, DOI arXiv:1904.03349
   Zibrek K, 2020, ACM T APPL PERCEPT, V17, DOI 10.1145/3419985
   Zibrek K, 2019, ACM T APPL PERCEPT, V16, DOI 10.1145/3349609
   Zibrek K, 2018, IEEE T VIS COMPUT GR, V24, P1681, DOI 10.1109/TVCG.2018.2794638
   ZUCKERMAN M, 1993, J PERS SOC PSYCHOL, V65, P757, DOI 10.1037/0022-3514.65.4.757
NR 95
TC 0
Z9 0
U1 1
U2 1
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103886
DI 10.1016/j.cag.2024.01.013
EA FEB 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LJ9D6
UT WOS:001186540600001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Thijssen, J
   Tian, ZL
   Telea, A
AF Thijssen, Julian
   Tian, Zonglin
   Telea, Alexandru
TI Interactive tools for explaining multidimensional projections for
   high-dimensional tabular data
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Multidimensional projections; Explanatory visualizations; User
   evaluations
ID VISUAL ANALYSIS; REDUCTION
AB We present a set of interactive visual analysis techniques aiming at explaining data patterns in multidimensional projections. Our novel techniques include a global value-based encoding that highlights point groups having outlier values in any dimension as well as several local tools that provide details on the statistics of all dimensions for a user-selected projection area. Our techniques generically apply to any projection algorithm and scale computationally well to hundreds of thousands of points and hundreds of dimensions. We describe a user study that shows that our visual tools can be quickly learned and applied by users to obtain non-trivial insights in real-world multidimensional datasets. We also show how our techniques can help understanding a real-world dataset containing quantitative, ordinal, and categorical attributes.
C1 [Thijssen, Julian; Tian, Zonglin; Telea, Alexandru] Univ Utrecht, Dept Informat & Comp Sci, NL-3584 CC Utrecht, Netherlands.
C3 Utrecht University
RP Telea, A (corresponding author), Univ Utrecht, Dept Informat & Comp Sci, NL-3584 CC Utrecht, Netherlands.
EM a.c.telea@uu.nl
FU China Scholarship [201906080046]
FX We thank Dr. Tamara Mchedlidze for sharing with us the EVS dataset and
   insights thereon. Zonglin Tian was supported in his work by the China
   Scholarship grant number 201906080046.
CR Angelini M, 2022, IEEE T VIS COMPUT GR, V28, P4770, DOI 10.1109/TVCG.2021.3104879
   Aupetit M, 2014, P 5 WORKSH TIM ERR N, P134
   Aupetit M, 2007, NEUROCOMPUTING, V70, P1304, DOI 10.1016/j.neucom.2006.11.018
   Broeksema B, 2013, COMPUT GRAPH FORUM, V32, P158, DOI 10.1111/cgf.12194
   Coimbra DB, 2016, INFORM VISUAL, V15, P154, DOI 10.1177/1473871615600010
   Cortez P, 2009, DECIS SUPPORT SYST, V47, P547, DOI 10.1016/j.dss.2009.05.016
   da Silva R, 2015, P EUROVA, P97
   Dua D, 2022, UCI Machine Learning Repository
   Elmqvist N, 2008, IEEE T VIS COMPUT GR, V14, P1141, DOI 10.1109/TVCG.2008.153
   Espadoto M, 2021, IEEE T VIS COMPUT GR, V27, P2153, DOI 10.1109/TVCG.2019.2944182
   EVS/WVS, 2022, GESIS, V4.0.0
   Gower J.C., 2011, Understanding Biplots
   Greenacre M.J., 2010, BIPLOTS PRACTICE
   Hancock JT, 2020, J BIG DATA-GER, V7, DOI 10.1186/s40537-020-00305-w
   Hoffman P, 1997, VISUALIZATION '97 - PROCEEDINGS, P437, DOI 10.1109/VISUAL.1997.663916
   INSELBERG A, 1990, PROCEEDINGS OF THE FIRST IEEE CONFERENCE ON VISUALIZATION - VISUALIZATION 90, P361, DOI 10.1109/VISUAL.1990.146402
   Joia P, 2015, COMPUT GRAPH FORUM, V34, P281, DOI 10.1111/cgf.12640
   Joia P, 2011, IEEE T VIS COMPUT GR, V17, P2563, DOI 10.1109/TVCG.2011.220
   Kelly KL, 1965, Color Eng, V3, P26
   Lespinats S, 2011, COMPUT GRAPH FORUM, V30, P113, DOI 10.1111/j.1467-8659.2010.01835.x
   Martins R, 2015, P CGVC, P121
   Martins RM, 2014, COMPUT GRAPH-UK, V41, P26, DOI 10.1016/j.cag.2014.01.006
   McInnes L, 2020, Arxiv, DOI [arXiv:1802.03426, 10.21105/joss.00861]
   Munzner Tamara, 2014, Visualization analysis and design
   Nonato LG, 2019, IEEE T VIS COMPUT GR, V25, P2650, DOI 10.1109/TVCG.2018.2846735
   Pagliosa L, 2016, SIBGRAPI, P297, DOI [10.1109/SIBGRAPI.2016.048, 10.1109/SIBGRAPI.2016.45]
   RAO R, 1994, HUMAN FACTORS IN COMPUTING SYSTEMS, CHI '94 CONFERENCE PROCEEDINGS - CELEBRATING INTERDEPENDENCE, P318, DOI 10.1145/191666.191776
   Schreck T, 2010, INFORM VISUAL, V9, P181, DOI 10.1057/ivs.2010.2
   Sips M, 2009, COMPUT GRAPH FORUM, V28, P831, DOI 10.1111/j.1467-8659.2009.01467.x
   Stahnke J, 2016, IEEE T VIS COMPUT GR, V22, P629, DOI 10.1109/TVCG.2015.2467717
   Tatu A, 2010, P INT C ADV VISUAL I, P49, DOI DOI 10.1145/1842993.1843002
   Thijssen J, 2023, P EUROVA
   Thijssen J, 2023, Visual explanation system for multidimensional projections
   Tian ZL, 2021, COMPUT GRAPH-UK, V98, P93, DOI 10.1016/j.cag.2021.04.034
   Tukey J.W., 1977, Exploratory data analysis, V2
   van der Maaten L, 2009, Tech. rep.
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   van Driel D, 2020, P EUROVA, P37
   Venna J, 2006, PROC EUROPEAN S ARTI, P557
   Vieth A, 2023, IEEE Trans Vis Comput Graphics
   Yin HJ, 2007, INT J AUTOM COMPUT, V4, P294, DOI 10.1007/s11633-007-0294-y
   Zhang Y, 2022, bioRxiv, DOI [10.1101/2022.03.28.486139, 10.1101/2022.03.28.486139, DOI 10.1101/2022.03.28.486139]
NR 42
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103987
DI 10.1016/j.cag.2024.103987
EA JUL 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YA6D9
UT WOS:001265794500001
OA hybrid
DA 2024-08-05
ER

PT J
AU Pujol, E
   Chica, A
AF Pujol, Eduard
   Chica, Antonio
TI Rendering piecewise approximations of SDFs through analytic
   intersections
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Distance fields; Ray marching; Rendering; Implicit functions
AB Signed distance fields (SDFs) have emerged as an alternative shape representation for real-time collision detection and lighting effects. Computing these for complex models can be expensive, so one popular approach is to prepare an approximation via sampling and interpolation. Then, these may be rendered using sphere marching, which gets close to the surface quickly, but needs several iterations to converge to it. In this paper, we propose an alternative that computes the intersection of a given ray and the surface analytically at a narrow band. This may be combined with other enhancements like having variable error for the approximation depending on the distance to the surface and skipping regions that do not contain the surface to accelerate the outer band ray traversal while reducing the required memory. To achieve smoother representations with minimal computational cost, we propose a method for computing surface intersections and normals from separate interpolants. We evaluate all these to find the optimal combination improving the rendering performance and memory consumption of these SDF approximations.
C1 [Pujol, Eduard; Chica, Antonio] Univ Politecn Cataluna, ViRVIG, Barcelona 08028, Spain.
C3 Universitat Politecnica de Catalunya
RP Pujol, E (corresponding author), Univ Politecn Cataluna, ViRVIG, Barcelona 08028, Spain.
EM eduard.pujol.puig@upc.edu; achica@cs.upc.edu
FU Ministeri de Ciencia i Innovacio (MICIN); Agencia Estatal de
   Investigacion (AEI); Fons Europeu de Desenvolupament Regional (FEDER) -
   MCIN/AEI/FEDER, UE [PID2021-122136OB-C21]; Department of Research and
   Universities of the Government of Catalonia [2021 SGR 01035];
   Universitat Politecnica de Catalunya; Banco Santander
FX This work has been partially funded by Ministeri de Ciencia i Innovacio
   (MICIN) , Agencia Estatal de Investigacion (AEI) and the Fons Europeu de
   Desenvolupament Regional (FEDER) (project PID2021-122136OB-C21 funded by
   MCIN/AEI/10.13039/501100011033/FEDER, UE) ; and by the Department of
   Research and Universities of the Government of Catalonia (2021 SGR
   01035) . The first author gratefully acknowledges the Universitat
   Politecnica de Catalunya and Banco Santander for the financial support
   of his predoctoral grant FPI-UPC grant. We are also grateful to Joan
   Hervas for his assistance with algorithm testing.
CR Balint C, 2018, EG 2018-short papers, P4, DOI [10.2312/egs20181037, DOI 10.2312/EGS20181037]
   Calakli F, 2011, COMPUT GRAPH FORUM, V30, P1993, DOI 10.1111/j.1467-8659.2011.02058.x
   De Cusatis A.  Jr., 1999, XII Brazilian Symposium on Computer Graphics and Image Processing (Cat. No.PR00481), P65, DOI 10.1109/SIBGRA.1999.805711
   Frisken S.F., 2006, ACM SIGGRAPH COURSE, P60, DOI DOI 10.1145/1185657.1185675
   Frisken SF, 2000, COMP GRAPH, P249, DOI 10.1145/344779.344899
   Fu Q, 2022, NeurIPS, V35, P3403
   Galin E, 2020, COMPUT GRAPH FORUM, V39, P545, DOI 10.1111/cgf.13951
   Hansson Soderlund H, 2022, J Comput Graph Tech (JCGT), V11, P94
   Hart JC, 1996, VISUAL COMPUT, V12, P527, DOI 10.1007/s003710050084
   Hu JK, 2021, VISUAL COMPUT, V37, P2539, DOI 10.1007/s00371-021-02197-0
   Keinert B, 2014, SMART TOOLS APPS GRA, P1, DOI [10.2312/stag.20141233.001-008, DOI 10.2312/STAG.20141233.001-008]
   Koschier D, 2016, EUR ACM SIGGRAPH S C, P189
   Li QD, 2018, VIS COMPUT IND BIOME, V1, DOI 10.1186/s42492-018-0009-y
   Li ZS, 2023, PROC CVPR IEEE, P8456, DOI 10.1109/CVPR52729.2023.00817
   Lorensen H. E., 1987, Proc. SIGGRAPH, V21, P163, DOI 10.1145/37401.37422
   Macklin M, 2020, P ACM COMPUT GRAPH, V3, DOI 10.1145/3384538
   Marschner Z, 2023, SIGGRAPH ASIA 2023 C, P1
   MITCHELL DP, 1990, GRAPH INTER, P68
   Nielson GM, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P489, DOI 10.1109/VISUAL.2004.28
   Parker S, 1998, VISUALIZATION '98, PROCEEDINGS, P233, DOI 10.1109/VISUAL.1998.745713
   Peters C, 2023, Vision, modeling, and visualization, P21, DOI [10.2312/vmv20231223, DOI 10.2312/VMV20231223]
   Pujol E, 2023, COMPUT GRAPH-UK, V114, P337, DOI 10.1016/j.cag.2023.06.020
   Sellan S, 2023, SIGGRAPH Asia 2023 conference papers, P1, DOI [10.1145/3610548.3618196, DOI 10.1145/3610548.3618196]
   Sharp N, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530155
   Shen TC, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592430
   Takikawa T, 2022, J Comput Graph Tech (JCGT), V11
   Vasilopoulos V, 2024, Arxiv, DOI arXiv:2310.09463
   Wright D, 2015, ACM SIGGRAPH, V3
   Wu JH, 2003, VISION, MODELING, AND VISUALIZATION 2003, P513
   Yuksel C, 2022, P ACM COMPUT GRAPH, V5, DOI 10.1145/3543865
NR 30
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103981
DI 10.1016/j.cag.2024.103981
EA JUL 2024
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XZ2O1
UT WOS:001265438100001
OA hybrid
DA 2024-08-05
ER

PT J
AU Yang, YS
   Gao, ZY
   Zhang, JH
   Hui, WB
   Shi, H
   Xie, YM
AF Yang, Yusheng
   Gao, Zhiyuan
   Zhang, Jinghan
   Hui, Wenbo
   Shi, Hang
   Xie, Yangmin
TI UVS-CNNs: Constructing general convolutional neural networks on
   quasi-uniform spherical images
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Omnidirectional sensing; Uniform pixelization; Spherical CNN
AB Omnidirectional images, also known as spherical images, offer a significant advantage for the environmental sensing of mobile robots due to their wide field of view. However, previous studies of constructing convolutional neural networks on spherical images have been limited by non-uniform pixel sampling, leading to suboptimal performance in semantic segmentation. To address this issue, a novel pixel segmentation approach is proposed to achieve near-uniform pixel distribution across the entire spherical surface. The corresponding convolution operation for the resulting image is designed as well, which extends the capabilities of spherical CNNs from semantic segmentation to more complex tasks such as instance segmentation. The method evaluated on the Stanford 2D3DS dataset and shows superior performance compared to conventional spherical CNNs. Furthermore, the method also achieves impressive instance segmentation results on our experimental LiDAR data, demonstrating the general feasibility of our approach for common CNN tasks. The related code and dataset are released in the following link: https://github.com/YoungRainy/UVS-U-Net.
C1 [Yang, Yusheng; Gao, Zhiyuan; Zhang, Jinghan; Shi, Hang; Xie, Yangmin] Shanghai Univ, Sch Mechatron Engn & Automat, Shanghai 200444, Peoples R China.
   [Yang, Yusheng; Gao, Zhiyuan; Zhang, Jinghan; Xie, Yangmin] Shanghai Key Lab Intelligent Mfg & Robot, Shanghai 200444, Peoples R China.
   [Hui, Wenbo] Shanghai Univ, Sch Future Technol, Shanghai 200444, Peoples R China.
C3 Shanghai University; Shanghai University
RP Xie, YM (corresponding author), Shanghai Univ, Sch Mechatron Engn & Automat, Shanghai 200444, Peoples R China.; Xie, YM (corresponding author), Shanghai Key Lab Intelligent Mfg & Robot, Shanghai 200444, Peoples R China.
EM yysshu@shu.edu.cn; gaoyy2021@shu.edu.cn; zhangjinghan@shu.edu.cn;
   atlantis_hui@shu.edu.cn; zhangjinghan@shu.edu.cn; xieym@shu.edu.cn
FU National Natural Science Founda-tion of China (NSFC) [62173220,
   62303294]; Natural Science Foundation of Shanghai, China [20ZR1419100]
FX <B>Acknowledgments</B> This work was supported by the National Natural
   Science Founda-tion of China (NSFC) (No. 62173220 and No. 62303294) ,
   and Natural Science Foundation of Shanghai, China (No. 20ZR1419100) .
CR Ai H, 2022, arXiv
   Armeni I., 2017, arXiv
   Berenguel-Baeta B, 2022, PATTERN RECOGN, V129, DOI 10.1016/j.patcog.2022.108740
   Cao Min, 2022, ARXIV
   Chen L.-C., 2018, P EUR C COMP VIS ECC, P801, DOI DOI 10.1007/978-3-030-01234-2_49
   Cohen T, 2019, Proceedings of the 36th International Conference on Machine Learning, P1321
   Coors B, 2018, LECT NOTES COMPUT SC, V11213, P525, DOI 10.1007/978-3-030-01240-3_32
   Du CY, 2020, PATTERN RECOGN LETT, V133, P62, DOI 10.1016/j.patrec.2019.06.018
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Geng H, 2022, ACTUATORS, V11, DOI 10.3390/act11010013
   Guan H, 2017, PROC CVPR IEEE, P4886, DOI 10.1109/CVPR.2017.519
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoogeboom E, 2018, Arxiv, DOI arXiv:1803.02108
   Jiang H, 2021, IEEE T IMAGE PROCESS, V30, P2364, DOI 10.1109/TIP.2021.3052073
   Khan MA, 2021, PATTERN RECOGN LETT, V143, P58, DOI 10.1016/j.patrec.2020.12.015
   Khasanova R, 2017, IEEE INT CONF COMP V, P860, DOI 10.1109/ICCVW.2017.106
   Lee S, 2017, COMPUT GEOSCI-UK, V103, P142, DOI 10.1016/j.cageo.2017.03.012
   Lee Y, 2022, IEEE T PATTERN ANAL, V44, P834, DOI 10.1109/TPAMI.2020.2997045
   Li YY, 2022, PROC CVPR IEEE, P2791, DOI 10.1109/CVPR52688.2022.00282
   Li ZW, 2022, IEEE T NEUR NET LEAR, V33, P6999, DOI 10.1109/TNNLS.2021.3084827
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Pi JY, 2020, IEEE I C VI COM I PR, P168, DOI 10.1109/vcip49819.2020.9301893
   Renka RJ, 1997, ACM T MATH SOFTWARE, V23, P416, DOI 10.1145/275323.275329
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shakerinava M, 2021, PMLR, V139, P9477
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smale S, 1998, MATH INTELL, V20, P7, DOI 10.1007/BF03025291
   Su Y-C, 2021, IEEE Trans Pattern Anal Mach Intell
   Su YC, 2019, PROC CVPR IEEE, P9434, DOI 10.1109/CVPR.2019.00967
   Yang WY, 2018, INT C PATT RECOG, P2190, DOI 10.1109/ICPR.2018.8546070
   Yosinski J, 2014, ADV NEUR IN, V27
   Zhang C, 2019, IEEE I CONF COMP VIS, P3532, DOI 10.1109/ICCV.2019.00363
   Zhao FQ, 2019, LECT NOTES COMPUT SC, V11492, P855, DOI 10.1007/978-3-030-20351-1_67
   Zhao PY, 2020, AAAI CONF ARTIF INTE, V34, P12959
NR 35
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103973
DI 10.1016/j.cag.2024.103973
EA JUN 2024
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA7J8
UT WOS:001259023100001
DA 2024-08-05
ER

PT J
AU Zhou, W
   Jiao, JB
   Wei, MG
   Nei, W
   Xu, HX
AF Zhou, Wei
   Jiao, Jianbin
   Wei, Mingan
   Nei, Wang
   Xu, Haixia
TI SRX Net: A point cloud segmentation framework based on surface
   representation and X-Net
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Point cloud; Segmentation framework; SRX Net; Surface representation;
   Network architecture
AB We propose a novel unified point cloud segmentation framework called SRX Net. This framework is composed of our proposed SurRep Group module and X -Net architecture, reconstructing point cloud segmentation models from two aspects: point cloud surface representation and network architecture. The SurRep Group can model the local surface structure of the point cloud through structures such as the Ellipsoid Surface Representation and Umbrella Surface Representation, providing rich geometric information to the network. The XNet establishes an inverse encoding-decoding path to deepen the modeling of shallow semantics, solving the interference of shallow semantics on deep semantics in prediction in the U -Net architecture. It also introduces a GLFEnhancement module to enhance each point's perception of global information. As a unified framework, SRX Net can improve the semantic segmentation effects of various existing models. Extensive experimental results show that our model produces more accurate segmentation edges and significantly reduces discrete predicted points. Based on Point Transformer, our SRX-PT Net achieves state-of-the-art performance of 72.1 mIoU on S3DIS, Scannet V2, and WCS3D datasets, respectively.
C1 [Zhou, Wei; Jiao, Jianbin; Wei, Mingan; Nei, Wang] Xiangtan Univ, Sch Comp Sci, Xiangtan 411105, Peoples R China.
   [Xu, Haixia] Xiangtan Univ, Sch Automat & Elect Informat, Xiangtan 411105, Peoples R China.
C3 Xiangtan University; Xiangtan University
RP Jiao, JB (corresponding author), Xiangtan Univ, Sch Comp Sci, Xiangtan 411105, Peoples R China.
EM zhou_wei@xtu.edu.cn; jjbinary@smail.xtu.edu.cn;
   202121632890@smail.xtu.edu.cn; 202221632988@smail.xtu.edu.cn;
   haixiaxu@xtu.edu.cn
OI Jiao, Jianbin/0009-0008-8723-4120
FU Key Program Scientific Re-search Fund of the Hunan Provincial Education
   Department [23A0155, 22A0127]; Key Laboratory of Intelligent Computing
   and Information Processing of the Ministry of Education, Xiangtan
   University [2023ICIP03, 2023ICIP07]
FX <B>Acknowledgments</B> This work has been supported by the Key Program
   Scientific Re-search Fund of the Hunan Provincial Education Department
   (Grant Nos. 23A0155 and 22A0127) and by the Key Laboratory of
   Intelligent Computing and Information Processing of the Ministry of
   Education, Xiangtan University (Grant Nos. 2023ICIP03 and 2023ICIP07) .
CR Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Chen G., 2023, arXiv
   Chen LF, 2023, VISUAL COMPUT, V39, P863, DOI 10.1007/s00371-021-02351-8
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Choe J, 2022, LECT NOTES COMPUT SC, V13687, P620, DOI 10.1007/978-3-031-19812-0_36
   Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319
   Dai A, 2018, LECT NOTES COMPUT SC, V11214, P458, DOI 10.1007/978-3-030-01249-6_28
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961
   Han XF, 2023, IEEE T MULTIMEDIA, V25, P5638, DOI 10.1109/TMM.2022.3198318
   Hassan R, 2023, Residual learning with annularly convolutional neural networks for classification and segmentation of 3D point clouds
   Hu Q., 2020, P IEEECVF C COMPUTER, P11108, DOI [DOI 10.1109/CVPR42600.2020.01112, 10.1109/CVPR42600.2020.01112]
   Huan Lei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11608, DOI 10.1109/CVPR42600.2020.01163
   Huang ZN, 2023, Arxiv, DOI arXiv:2303.08274
   Kapl M, 2013, Technical Report G+ S
   Lai X, 2022, PROC CVPR IEEE, P8490, DOI 10.1109/CVPR52688.2022.00831
   Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298
   Lavoue Guillaume., 2008, 3DOR@ Eurographics, P25
   Li B, 2016, Arxiv, DOI arXiv:1608.07916
   Li X, 2022, IEEE INT C INTELL TR, P3165, DOI 10.1109/ITSC55140.2022.9922529
   Li YZ, 2018, ADV NEUR IN, V31
   Lin HJ, 2023, PROC CVPR IEEE, P17682, DOI 10.1109/CVPR52729.2023.01696
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Park J, 2023, PROC CVPR IEEE, P21814, DOI 10.1109/CVPR52729.2023.02089
   Qi CR, 2017, ADV NEUR IN, V30
   Qian G. C., 2022, ADV NEURAL INFORM PR, V35, P23192, DOI [DOI 10.48550/ARXIV.2206.04670, https://doi.org/10.48550/arXiv.2206.04670]
   Ran HX, 2022, PROC CVPR IEEE, P18920, DOI 10.1109/CVPR52688.2022.01837
   Rodrigues RSV, 2018, COMPUT GRAPH FORUM, V37, P235, DOI 10.1111/cgf.13323
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sharf A, 2007, COMPUT GRAPH FORUM, V26, P323, DOI 10.1111/j.1467-8659.2007.01054.x
   Shen YR, 2018, PROC CVPR IEEE, P4548, DOI 10.1109/CVPR.2018.00478
   Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Wang X, 2023, SOFT COMPUT, V27, P1005, DOI 10.1007/s00500-022-07543-5
   Wang Y, 2023, COMPUT GRAPH-UK, V116, P24, DOI 10.1016/j.cag.2023.07.040
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wang Ziyi, 2022, arXiv
   Wu WX, 2023, PROC CVPR IEEE, P21802, DOI 10.1109/CVPR52729.2023.02088
   Wu X., 2022, arXiv
   Xu MT, 2021, PROC CVPR IEEE, P3172, DOI 10.1109/CVPR46437.2021.00319
   Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zetong Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P600, DOI 10.1007/978-3-030-58607-2_35
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhao HS, 2019, PROC CVPR IEEE, P5550, DOI 10.1109/CVPR.2019.00571
   Zhou W, 2023, Water conservancy segment 3D
NR 48
TC 1
Z9 1
U1 1
U2 1
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD MAY
PY 2024
VL 120
AR 103923
DI 10.1016/j.cag.2024.103923
EA MAY 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SY3W0
UT WOS:001237981500001
DA 2024-08-05
ER

PT J
AU Sundt, PB
   Theoharis, T
AF Sundt, Peder Bergebakken
   Theoharis, Theoharis
TI Towards multi-view consistency in neural ray fields using parametric
   medial surfaces
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Representation learning; Neural fields; Medial Axis Transform
AB Deep learning methods are revolutionizing the solutions to visual computing problems, such as shape retrieval and generative shape modeling, but require novel shape representations that are both fast and differentiable. Neural ray fields and their improved rendering performance are promising in this regard, but struggle with a reduced fidelity and multi-view consistency when compared to the more studied coordinate-based methods which, however, are slower in training and evaluation. We propose PMARF, an improved ray field which explicitly models the skeleton of the target shape as a set of (0-thickness) parametric medial surfaces. This formulation reduces by construction the degrees-of-freedom available in the reconstruction domain, improving multi-view consistency even from sparse training views. This in turn improves fidelity while facilitating a reduction in the network size.
C1 [Sundt, Peder Bergebakken; Theoharis, Theoharis] Norwegian Univ Sci & Technol, Trondheim, Norway.
C3 Norwegian University of Science & Technology (NTNU)
RP Sundt, PB (corresponding author), Norwegian Univ Sci & Technol, Trondheim, Norway.
EM peder.b.sundt@ntnu.no; theotheo@ntnu.no
FX The authors would like to thank Sjalander et al. [74] at NTNU for
   computing resources.
CR [Anonymous], 2014, The stanford 3d scanning repository
   Anthony Scopatz, Pyembree: Python wrapper for intel embree 2.17.7. 2022
   Attal B, 2022, PROC CVPR IEEE, P19787, DOI 10.1109/CVPR52688.2022.01920
   Attali D, 2009, MATH VIS, P109, DOI 10.1007/b106657_6
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Barron Jonathan T, 2021, Mip-NeRF: A multiscale representation for anti-aliasing neural radiance fields, P5855
   Bouix S, 2000, LECT NOTES COMPUT SC, V1842, P603
   Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Du H., 2004, SM 04, P25
   Falcon William, 2020, Zenodo, DOI 10.5281/ZENODO.3828935
   Feng BY, 2022, LECT NOTES COMPUT SC, V13663, P138, DOI 10.1007/978-3-031-20062-5_9
   Feng BY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14204, DOI 10.1109/ICCV48922.2021.01396
   Fridovich-Keil S, 2022, PROC CVPR IEEE, P5491, DOI 10.1109/CVPR52688.2022.00542
   Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29
   Han XG, 2017, IEEE I CONF COMP VIS, P85, DOI 10.1109/ICCV.2017.19
   Hanocka R, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322959
   Haoqiang FanHao Su Leonidas Guibas., 2016, A point set generation network for 3d object reconstruction from a single image
   Jaeyoung Chung, 2024, P IEEECVF C COMPUTER, P811
   Jonathan Frankle, 2019, INT C LEARN REPR
   Pontes JK, 2017, Arxiv, DOI [arXiv:1711.10669, DOI 10.48550/ARXIV.1711.10669, 10.48550/arXiv.1711.10669]
   Karnewar Animesh, 2022, SIGGRAPH22 Conference Proceeding: Special Interest Group on Computer Graphics and Interactive Techniques Conference Proceedings, DOI 10.1145/3528233.3530707
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Kerbl B, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592433
   Keselman L, 2023, Arxiv, DOI [arXiv:2308.14737, 10.48550/arXiv.2308.14737, DOI 10.48550/ARXIV.2308.14737]
   Kingma D. P., 2014, arXiv
   Kleineberg M, 2020, Arxiv, DOI [arXiv:2002.00349, DOI 10.48550/ARXIV.2002.00349]
   Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954
   Lindell DB, 2021, PROC CVPR IEEE, P14551, DOI 10.1109/CVPR46437.2021.01432
   Liu SC, 2019, IEEE I CONF COMP VIS, P7707, DOI 10.1109/ICCV.2019.00780
   Liu ZM, 2023, Arxiv, DOI [arXiv:2310.19629, 10.48550/arXiv.2310.19629, DOI 10.48550/ARXIV.2310.19629]
   Matusik W, 2000, COMP GRAPH, P369, DOI 10.1145/344779.344951
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Mukund Varma T, 2023, OpenReview
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Neff T., 2021, COMPUT GRAPH FORUM, V40, P45, DOI [DOI 10.1111/CGF.14340, 10.1111/cgf.14340]
   Niu CJ, 2018, PROC CVPR IEEE, P4521, DOI 10.1109/CVPR.2018.00475
   Özyesil O, 2017, ACTA NUMER, V26, P305, DOI 10.1017/S096249291700006X
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Paszke A., 2019, Advances in Neural Information Processing Systems, ppp 8024, DOI DOI 10.48550/ARXIV.1912.01703
   Paul Renteln, 2013, Manifolds, tensors, and forms: an introduction for mathematicians and physicists
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Qi CR, 2017, ADV NEUR IN, V30
   Ramasinghe S, 2022, LECT NOTES COMPUT SC, V13693, P142, DOI 10.1007/978-3-031-19827-4_9
   Rebain D, 2021, Arxiv, DOI [arXiv:2106.03804, 10.48550/arXiv.2106.03804, DOI 10.48550/ARXIV.2106.03804]
   Rebain D, 2019, COMPUT GRAPH FORUM, V38, P5, DOI 10.1111/cgf.13599
   Saragadam V, 2023, PROC CVPR IEEE, P18507, DOI 10.1109/CVPR52729.2023.01775
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Singh A, 2014, IEEE INT CONF ROBOT, P509, DOI 10.1109/ICRA.2014.6906903
   Sitzmann Vincent, 2021, Advances in Neural Information Processing Systems, V34, P19313
   Själander M, 2024, Arxiv, DOI [arXiv:1912.05848, DOI 10.48550/ARXIV.1912.05848, 10.48550/arXiv.1912.05848]
   Sundt PB, 2023, COMPUT GRAPH-UK, V115, P122, DOI 10.1016/j.cag.2023.06.032
   Tagliasacchi A, 2016, COMPUT GRAPH FORUM, V35, P573, DOI 10.1111/cgf.12865
   Tam R, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P481, DOI 10.1109/VISUAL.2003.1250410
   Tancik M., 2020, ADV NEURAL INFORM PR, V33, P7537, DOI DOI 10.48550/ARXIV.2006.10739
   Tang JX, 2024, Arxiv, DOI [arXiv:2309.16653, 10.48550/arXiv.2309.16653]
   Tarun Yenamandra, 2024, FIRe: Fast inverse rendering using directional and signed distance functions, P3077
   Tulsiani S, 2017, PROC CVPR IEEE, P209, DOI 10.1109/CVPR.2017.30
   Vicini D, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530139
   Vincent Sitzmann, 2020, P NEURIPS
   Wu J., 2017, Advances in Neural Information Processing Systems, P540
   Wu JJ, 2018, LECT NOTES COMPUT SC, V11215, P673, DOI 10.1007/978-3-030-01252-6_40
   Chang AX, 2015, Arxiv, DOI [arXiv:1512.03012, DOI 10.48550/ARXIV.1512.03012]
   Xie YH, 2022, COMPUT GRAPH FORUM, V41, P641, DOI 10.1111/cgf.14505
   Xinchen Yan, 2017, Perspective transformer nets: Learning single-view 3D object reconstruction without 3D supervision
   Yan-Bin Jia, 2020, Plucker coordinates for lines in the space
   Yang BR, 2020, COMPUT AIDED GEOM D, V80, DOI 10.1016/j.cagd.2020.101874
   Yang GD, 2018, LECT NOTES COMPUT SC, V11219, P90, DOI 10.1007/978-3-030-01267-0_6
   Yang JW, 2023, PROC CVPR IEEE, P8254, DOI 10.1109/CVPR52729.2023.00798
   Yariv L, 2021, ADV NEUR IN
   Zhou Y, 2019, PROC CVPR IEEE, P5738, DOI 10.1109/CVPR.2019.00589
   Zhu R, 2017, IEEE I CONF COMP VIS, P57, DOI 10.1109/ICCV.2017.16
   Zou CH, 2017, IEEE I CONF COMP VIS, P900, DOI 10.1109/ICCV.2017.103
NR 77
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD OCT
PY 2024
VL 123
AR 103991
DI 10.1016/j.cag.2024.103991
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZI0V8
UT WOS:001274559000001
OA hybrid
DA 2024-08-05
ER

PT J
AU Zhao, Y
   Deng, JH
   Liu, FH
   Tang, W
   Feng, J
AF Zhao, Yan
   Deng, Jiahui
   Liu, Feihong
   Tang, Wen
   Feng, Jun
TI GO: A two-step generative optimization method for point cloud
   registration
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Mathematical optimization; Point cloud registration; Supervised
   learning; Deep learning
ID DISCRIMINATIVE OPTIMIZATION; ALGORITHM
AB Point cloud registration aligns point clouds through transformation or deformation. The existing advancedregistration methods perform poorly in registering under various perturbations, especially for large rotationsand small overlaps. Moreover, these methods cannot achieve rigid and non-rigid registrations simultaneously.In this paper, we propose a two-step generative optimization (GO) method to achieve registration, whichalleviates the constraints of transformation representation and completely circumvents the matching ofcorrespondences, making it suitable for rigid registration with large rotations and small overlaps, as wellas for non-rigid registration. GO achieves registration by generating a pseudo point cloud approximatingthe target point cloud. The pseudo point cloud is iteratively deformed to approach the target point cloudthrough an estimation step (ES) and an updating step (US). The ES step generates initial pseudo point cloudsin three different views and estimates their deformation via the extracted features decoding the similaritybetween the pseudo point clouds and the target point cloud. The US step calculates regressors to update thecurrent estimations, generating new pseudo point clouds closer to the target point cloud. The final pseudopoint clouds generated are averaged to attain the registration result. We evaluate the performance of GOfor rigid and non-rigid registrations through the comparison with seven deep learning-based methods on eightdatasets, consisting of both 3D and 2D data and ranging from synthetic to real scenes. Our experimental resultsdemonstrate the high robustness and stability of GO in rigid registrations and illustrate the feasibility and highaccuracy for non-rigid registrations.
C1 [Zhao, Yan; Deng, Jiahui; Liu, Feihong; Feng, Jun] Northwest Univ, Sch Informat Sci & Technol, Xian 710127, Peoples R China.
   [Tang, Wen] Bournemouth Univ, Dept Creat Technol, Poole BH12 5BB, England.
C3 Northwest University Xi'an; Bournemouth University
RP Feng, J (corresponding author), Northwest Univ, Sch Informat Sci & Technol, Xian 710127, Peoples R China.
EM yanzhaonancy@gmail.com; dengjiahui@stumail.nwu.edu.cn; fhliu@nwu.edu.cn;
   wtang@bournemouth.ac.uk; fengjun@nwu.edu.cn
FU National Natural Science Foundation of China [62073260, 62203355]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62073260 and 62203355.
CR Aoki Y, 2019, PROC CVPR IEEE, P7156, DOI 10.1109/CVPR.2019.00733
   Benkner M.S., 2021, P IEEECVF INT C COMP, P7586
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bhatnagar Bharat Lal, 2020, Advances in Neural Information Processing Systems
   Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491
   Chen X, 2021, MED IMAGE ANAL, V74, DOI 10.1016/j.media.2021.102228
   Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2
   Dong Z, 2020, ISPRS J PHOTOGRAMM, V163, P327, DOI 10.1016/j.isprsjprs.2020.03.013
   Du SY, 2020, PATTERN RECOGN LETT, V132, P91, DOI 10.1016/j.patrec.2018.06.028
   Dyke RM, 2020, COMPUT GRAPH-UK, V92, P28, DOI 10.1016/j.cag.2020.08.008
   Fanbo Xiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11094, DOI 10.1109/CVPR42600.2020.01111
   Feng WQ, 2021, PROC CVPR IEEE, P10292, DOI 10.1109/CVPR46437.2021.01016
   Hagbi N, 2011, IEEE T VIS COMPUT GR, V17, P1369, DOI 10.1109/TVCG.2010.241
   Hirose O, 2021, IEEE T PATTERN ANAL, V43, P2269, DOI 10.1109/TPAMI.2020.2971687
   Huang JH, 2023, IEEE T PATTERN ANAL, V45, P2038, DOI 10.1109/TPAMI.2022.3164653
   Huang XS, 2021, Arxiv, DOI [arXiv:2103.02690, DOI 10.48550/ARXIV.2103.02690]
   Kadam P, 2022, IEEE T IMAGE PROCESS, V31, P2710, DOI 10.1109/TIP.2022.3160609
   Liu F, 2020, IEEE T PATTERN ANAL, V42, P664, DOI 10.1109/TPAMI.2018.2885995
   Liu WX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15273, DOI 10.1109/ICCV48922.2021.01501
   Lucas BruceD., 1981, ITERATIVE IMAGE REGI, P81
   Ma JY, 2019, INT J COMPUT VISION, V127, P512, DOI [10.1007/s11263-018-1117-z, 10.1109/TMAG.2017.2763198]
   Ma JY, 2019, IEEE T NEUR NET LEAR, V30, P3584, DOI 10.1109/TNNLS.2018.2872528
   Maharjan A, 2022, IEEE WINT CONF APPL, P2264, DOI 10.1109/WACV51458.2022.00232
   Marin R, 2020, COMPUT GRAPH FORUM, V39, P160, DOI 10.1111/cgf.13751
   Marin Riccardo, 2020, Advances in Neural Information Processing Systems, V33, P1608
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46
   Pons-Moll G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073711
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Saiti E., 2022, Computers & Graphics, V106, P259, DOI 10.1016/j.cag.2022.06.012
   Shen Y., 2022, arXiv
   Tam GKL, 2013, IEEE T VIS COMPUT GR, V19, P1199, DOI 10.1109/TVCG.2012.310
   Thiel KK, 2022, IEEE T VIS COMPUT GR, V28, P4434, DOI 10.1109/TVCG.2021.3089096
   Trappolini Giovanni, 2021, ADV NEURAL INFORM PR, V34, P5731
   Tuzel O, 2008, PROC CVPR IEEE, P1389
   Vongkulbhisal J, 2019, IEEE T PATTERN ANAL, V41, DOI 10.1109/TPAMI.2018.2826536
   Vongkulbhisal J, 2017, PROC CVPR IEEE, P3975, DOI 10.1109/CVPR.2017.423
   Wan T, 2022, IEEE T NEUR NET LEAR, V33, P3547, DOI 10.1109/TNNLS.2021.3053274
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Wu ZZ, 2019, PATTERN RECOGN, V93, P14, DOI 10.1016/j.patcog.2019.03.013
   Xi L, 2022, NEUROCOMPUTING, V506, P336, DOI 10.1016/j.neucom.2022.07.082
   Xiong XH, 2015, PROC CVPR IEEE, P2664, DOI 10.1109/CVPR.2015.7298882
   Yang H, 2021, IEEE T ROBOT, V37, P314, DOI 10.1109/TRO.2020.3033695
   Yang JL, 2013, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2013.184
   Yew ZJ, 2022, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR52688.2022.00656
   Zhao Y, 2022, COMPUT GRAPH-UK, V102, P521, DOI 10.1016/j.cag.2021.11.001
   Zhao Y, 2021, NEUROCOMPUTING, V464, P48, DOI 10.1016/j.neucom.2021.08.080
   Zhou ZY, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-26288-6
NR 48
TC 1
Z9 1
U1 9
U2 9
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103904
DI 10.1016/j.cag.2024.103904
EA MAR 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OR7Z3
UT WOS:001209080400001
DA 2024-08-05
ER

PT J
AU Karciauskas, K
   Peters, J
AF Karciauskas, Kestutis
   Peters, Jorg
TI Quadratic-attraction subdivision with contraction-ratio<i> λ</i>=1/2
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Subdivision algorithm; Surface quality; Quadratic expansion; Uniform
   contraction; Curvature
ID SPLINE SURFACES; EXTRAORDINARY; CURVATURE; SCHEMES
AB Classic generalized subdivision, such as Catmull-Clark subdivision, as well as recent subdivision algorithms for high-quality surfaces, rely on slower convergence towards extraordinary points for mesh nodes surrounded by n > 4 quadrilaterals. Slow convergence corresponds to a contraction-ratio of ) > 0 . 5 . To improve shape, prevent parameterization discordant with surface growth, or to improve convergence in isogeometric analysis near extraordinary points, a number of algorithms explicitly adjust ) by altering refinement rules. However, such tuning of ) has so far led to poorer surface quality, visible as uneven distribution or oscillation of highlight lines. The recent Quadratic-Attraction Subdivision (QAS) generates high-quality, bounded curvature surfaces based on a careful choice of quadratic expansion at the central point and, just like Catmull-Clark subdivision, creates the control points of the next subdivision ring by matrix multiplication. But QAS shares the contraction- ratio ) lambda(CC) > 1/2 of Catmull-Clark subdivision when n > 4. For n = 5 , ... , 10, , QAS, improves the convergence to the uniform ) lambda = 1/2 of binary domain refinement and without sacrificing surface quality compared to QAS.
C1 [Karciauskas, Kestutis] Vilnius Univ, Inst Math, Vilnius, Lithuania.
   [Peters, Jorg] Univ Florida, Dept CISE, Gainesville, FL 32611 USA.
C3 Vilnius University; State University System of Florida; University of
   Florida
RP Peters, J (corresponding author), Univ Florida, Dept CISE, Gainesville, FL 32611 USA.
EM jorg.peters@gmail.com
CR Augsdörfer UH, 2006, COMPUT GRAPH FORUM, V25, P263, DOI 10.1111/j.1467-8659.2006.00945.x
   BEIER KP, 1994, COMPUT AIDED DESIGN, V26, P268, DOI 10.1016/0010-4485(94)90073-6
   Bonneau GP, 2014, GRAPH MODELS, V76, P669, DOI 10.1016/j.gmod.2014.09.001
   CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   de Boor C, 1987, Geometric modeling: Algorithms and new trends, P131
   De Boor C, 2013, Box splines, V98
   DOO D, 1978, COMPUT AIDED DESIGN, V10, P356, DOI 10.1016/0010-4485(78)90111-2
   Farin G., 1988, Curves and surfaces for computer aided geometric design: A practical guide
   Gregory J., 1974, COMPUT AIDED GEOM D, P71
   Gu X., 2005, SPM 05, P27
   Hettinga GJ, 2020, COMPUT AIDED DESIGN, V127, DOI 10.1016/j.cad.2020.102855
   Kapl M, 2017, COMPUT AIDED GEOM D, V52-53, P75, DOI 10.1016/j.cagd.2017.02.013
   Karciauskas K, 2023, COMPUT GRAPH FORUM, V42, DOI 10.1111/cgf.14900
   Karciauskas K, 2022, COMPUT GRAPH FORUM, V41, P13, DOI 10.1111/cgf.14653
   Karciauskas K, 2023, Eurographics 2023
   Li X, 2019, COMPUT METHOD APPL M, V352, P606, DOI 10.1016/j.cma.2019.04.036
   Loop C, 2008, COMPUT GRAPH FORUM, V27, P1373, DOI 10.1111/j.1467-8659.2008.01277.x
   Ma Y, 2019, COMPUT GRAPH FORUM, V38, P127, DOI 10.1111/cgf.13822
   Ma Y, 2018, COMPUT GRAPH FORUM, V37, P455, DOI 10.1111/cgf.13582
   Marsala M, 2022, COMPUT AIDED GEOM D, V99, DOI 10.1016/j.cagd.2022.102158
   Peters J, Quadratic-Attraction Subdivision, C++ code
   Peters J, equiQAS3: equi-spaced Quadratic-Attraction Subdi- vision, C++ code
   Peters Jorg., 2019, The SMAI journal of computational mathematics, VS5, P161, DOI DOI 10.5802/SMAI-JCM.57
   Pfluger PR, 1993, Numer Algorithms, V5, P569
   Prautzsch H., 1984, Computer-Aided Geometric Design, V1, P95, DOI 10.1016/0167-8396(84)90007-4
   Reif U, 1997, J APPROX THEORY, V90, P174, DOI 10.1006/jath.1996.3079
   Sabin MA, 2022, COMPUT AIDED DESIGN, V143, DOI 10.1016/j.cad.2021.103137
   Stam J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P395, DOI 10.1145/280814.280945
   Vaitkus M, 2021, COMPUT AIDED GEOM D, V89, DOI 10.1016/j.cagd.2021.102019
   Wang X, 2023, COMPUT AIDED DESIGN, V162, DOI 10.1016/j.cad.2023.103544
   Wei XD, 2021, INT J NUMER METH ENG, V122, P2117, DOI 10.1002/nme.6608
NR 31
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD OCT
PY 2024
VL 123
AR 104001
DI 10.1016/j.cag.2024.104001
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZM4K2
UT WOS:001275701100001
DA 2024-08-05
ER

PT J
AU Gisbert, G
   Chaine, R
   Coeurjolly, D
AF Gisbert, Guillaume
   Chaine, Raphaelle
   Coeurjolly, David
TI Neural inpainting of folded fabrics with interactive editing
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Inpainting; Deep learning; Fabric surfaces; Developability
AB We propose a deep learning approach for inpainting holes in digital models of fabric surfaces. Leveraging the developable nature of fabric surfaces, we flatten the area surrounding the holes with minor distortion and regularly sample it to obtain a discrete 2D map of the 3D embedding, with an indicator mask outlining holes locations. This enables the use of a standard 2D convolutional neural network to inpaint holes given the 3D positioning of the surface. The provided neural architecture includes an attention mechanism to capture long-range relationships on the surface. Finally, we provide ScarfFolds , a database of folded fabrics patches with varying complexity, which is used to train our convolutional network in a supervised manner. We successfully tested our approach on various examples and illustrated that previous 3D deep learning approaches suffer from several issues when applied to fabrics. Also, our method allows the users to interact with the construction of the inpainted surface. The editing is interactive and supports many tools like vertex grabbing, drape twisting or pinching.
C1 [Gisbert, Guillaume; Chaine, Raphaelle; Coeurjolly, David] Univ Claude Bernard Lyon 1, INSA Lyon, CNRS, CETHIL UMR5008, F-69621 Villeurbanne, France.
C3 Centre National de la Recherche Scientifique (CNRS); CNRS - Institute
   for Engineering & Systems Sciences (INSIS); Institut National des
   Sciences Appliquees de Lyon - INSA Lyon; Universite Claude Bernard Lyon
   1
RP Gisbert, G; Chaine, R (corresponding author), Univ Claude Bernard Lyon 1, INSA Lyon, CNRS, CETHIL UMR5008, F-69621 Villeurbanne, France.
EM guillaume.gisbert@liris.cnrs.fr; raphaelle.chaine@liris.cnrs.fr;
   david.coeurjolly@liris.cnrs.fr
FU French Agence Nationale de la Recherche [ANR-16-CE38-0009]; L'Agence
   Nationale de la Recherche (ANR) [ANR-22-CE46-0006]
FX This work is part of the e-Roma project from the French Agence Nationale
   de la Recherche (ANR-16-CE38-0009) . It was funded, in whole or in part,
   by l'Agence Nationale de la Recherche (ANR) , project StableProxies
   ANR-22-CE46-0006. For the purpose of open access, the author has applied
   a CC-BY public copyright licence to any Author Accepted Manuscript (AAM)
   version.
CR Attene M, 2010, VISUAL COMPUT, V26, P1393, DOI 10.1007/s00371-010-0416-3
   Baumgartner L, 2020, Mesh denoising and inpainting using the total variation of the normal
   Bednarík J, 2020, PROC CVPR IEEE, P4715, DOI 10.1109/CVPR42600.2020.00477
   Bertiche H, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555491
   Blender Online Community, 2018, Blender-a 3D modelling and rendering package
   Bonneel N, 2018, COMPUT GRAPH FORUM, V37, P75, DOI 10.1111/cgf.13549
   Brunton A, 2009, SMI 2009: IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P66, DOI 10.1109/SMI.2009.5170165
   Davis J, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P428, DOI 10.1109/TDPVT.2002.1024098
   De Luigi L, 2023, PROC CVPR IEEE, P1451, DOI 10.1109/CVPR52729.2023.00146
   Deng Y, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P6559, DOI 10.1145/3503161.3548446
   Deprelle T, 2019, Arxiv, DOI arXiv:1908.04725
   Dong QL, 2022, Arxiv, DOI arXiv:2203.00867
   Feng C, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10030969
   Gisbert G, 2023, COMPUT GRAPH-UK, V114, P201, DOI 10.1016/j.cag.2023.05.025
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Gundogdu E, 2020, Arxiv, DOI arXiv:2007.10867
   Gundogdu E, 2019, IEEE I CONF COMP VIS, P8738, DOI 10.1109/ICCV.2019.00883
   Han XG, 2017, IEEE I CONF COMP VIS, P85, DOI 10.1109/ICCV.2017.19
   Hanocka R, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392415
   Hanocka R, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322959
   Harary G, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2532548
   Hattori S, 2021, Arxiv, DOI [arXiv:2107.02909, DOI 10.48550/ARXIV.2107.02909]
   Hernández-Bautista M, 2023, COMPUT GRAPH-UK, V115, P204, DOI 10.1016/j.cag.2023.07.007
   Jacobson A., 2018, libigl: A simple C++ geometry processing library
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Lamb N, 2022, arXiv
   Lévy B, 2002, ACM T GRAPHIC, V21, P362, DOI 10.1145/566570.566590
   Liepa P., 2003, Symposium on Geometry Processing, P200
   Liu GL, 2018, Arxiv, DOI [arXiv:1804.07723, DOI 10.48550/ARXIV.1804.07723]
   Liu LG, 2008, COMPUT GRAPH FORUM, V27, P1495, DOI 10.1111/j.1467-8659.2008.01290.x
   Lugmayr A, 2022, arXiv
   Oktay O, 2018, Arxiv, DOI [arXiv:1804.03999, DOI 10.48550/ARXIV.1804.03999]
   Paszke A., 2019, Advances in Neural Information Processing Systems, ppp 8024, DOI DOI 10.48550/ARXIV.1912.01703
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Sarmad M, 2019, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR.2019.00605
   Sharma A, 2016, LECT NOTES COMPUT SC, V9915, P236, DOI 10.1007/978-3-319-49409-8_20
   Sharp N, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3507905
   Smith E. J., 2017, C ROB LEARN, P87
   Sorkine O., 2007, As-rigid-as-possible surface modeling, P109, DOI 10.1145/1281991.1282006
   Suvorov R, 2021, arXiv
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Wang YH, 2022, Arxiv, DOI arXiv:2212.00490
   Wen X, 2020, PROC CVPR IEEE, P1936, DOI 10.1109/CVPR42600.2020.00201
   Wu JJ, 2016, ADV NEUR IN, V29
   Chang AX, 2015, Arxiv, DOI [arXiv:1512.03012, DOI 10.48550/ARXIV.1512.03012]
   Xiang HY, 2023, PATTERN RECOGN, V134, DOI 10.1016/j.patcog.2022.109046
   Yang YQ, 2018, Arxiv, DOI arXiv:1712.07262
NR 49
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103997
DI 10.1016/j.cag.2024.103997
EA JUL 2024
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZB6H6
UT WOS:001272864700001
DA 2024-08-05
ER

PT J
AU Lin, H
   Chen, Q
   Liu, C
   Hu, JS
AF Lin, Hong
   Chen, Qi
   Liu, Chun
   Hu, Jingsong
TI TDG-Diff: Advancing customized text-to-image synthesis with two-stage
   diffusion guidance
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Customized image synthesis; Diffusion model; Multimodal; Text-to-image
   synthesis
AB Recently, there has been widespread attention and significant progress in customized text-to-image synthesis based on diffusion models. However, reconstructing multiple concepts in the same scene remains highly challenging. Therefore, we propose a novel framework called TDG-Diff, which employs a two-stage diffusion guidance to achieve customized image synthesis with multiple concepts. TDG-Diff focuses on improving the sampling process of the diffusion model. Specifically, TDG-Diff subdivides the sampling process into two key stages: attribute separation and appearance refinement, introducing spatial constraints and concept representations for sampling guidance. In the attribute separation stage, TDG-Diff introduces a novel attention modulation method. This method effectively separates the attributes of different concepts based on spatial constraint information, reducing the risk of entanglement between attributes of different concepts. In the appearance refinement stage, TDG-Diff proposes a fusion sampling approach, which combines global text descriptions and concept representations to optimize and enhance the model's ability to capture and represent fine-grained details of concepts. Extensive qualitative and quantitative results demonstrate the effectiveness of TDG-Diff in customized text-to-image synthesis.
C1 [Lin, Hong; Chen, Qi; Liu, Chun; Hu, Jingsong] Wuhan Univ Technol, Wuhan 430070, Peoples R China.
C3 Wuhan University of Technology
RP Liu, C (corresponding author), Wuhan Univ Technol, Wuhan 430070, Peoples R China.
EM liuchun@whut.edu.cn
CR Arjovsky M., 2017, INT C LEARNING REPRE
   Avrahami O, 2023, PROC CVPR IEEE, P18370, DOI 10.1109/CVPR52729.2023.01762
   Balaji Y., 2022, arXiv
   Chefer H, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592116
   Cohen-Or D, 2023, SIGGRAPH AS 2023, P1
   Couairon G, 2023, ICLR 2023 11 INT C L
   Dhariwal P, 2021, ADV NEUR IN, V34
   Endo Y, 2023, Vis Comput, P1
   Feng ZD, 2023, PROC CVPR IEEE, P10135, DOI 10.1109/CVPR52729.2023.00977
   Gal R, 2022, 11 INT C LEARN REPR
   Hertz A., 2022, 11 INT C LEARN REPR
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Ho Jonathan, 2021, NEURIPS 2021 WORKSH
   Hong S, 2018, PROC CVPR IEEE, P7986, DOI 10.1109/CVPR.2018.00833
   Hu E. J., 2022, INT C LEARN REPR
   Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133
   Kumari N, 2023, PROC CVPR IEEE, P1931, DOI 10.1109/CVPR52729.2023.00192
   Li WH, 2019, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2019.00514
   Li YH, 2023, PROC CVPR IEEE, P22511, DOI 10.1109/CVPR52729.2023.02156
   Liao WT, 2022, PROC CVPR IEEE, P18166, DOI 10.1109/CVPR52688.2022.01765
   Liu C, 2024, Arxiv, DOI arXiv:2305.11520
   Liu C, 2023, COMPUT GRAPH-UK, V115, P500, DOI 10.1016/j.cag.2023.07.038
   Liu XH, 2023, IEEE WINT CONF APPL, P289, DOI [10.1109/WACV56688.2023.00037, 10.1007/978-3-031-33545-7_20]
   Lugmayr A, 2022, PROC CVPR IEEE, P11451, DOI 10.1109/CVPR52688.2022.01117
   Mou C, 2024, P AAAI C ARTIFICIAL, V38, P4296, DOI DOI 10.1609/AAAI.V38I5.28226
   Ni JC, 2021, COMPUT GRAPH-UK, V97, P54, DOI 10.1016/j.cag.2021.04.003
   Nichol AQ, 2022, INT C MACHINE LEARNI, P16784, DOI [10.48550/ARXIV.2112.10741, DOI 10.48550/ARXIV.2112.10741]
   Patashnik O, 2023, P IEEE CVF INT C COM, P23051
   Qin C, 2023, Advances in neural information processing systems, V36, P42961
   Qu LG, 2023, PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2023, P643, DOI 10.1145/3581783.3612012
   Radford L., 2016, Unsupervised representation learning with deep convolutional generative adversarial networks, P1
   Ramesh A., 2022, Hierarchical text-conditional image generation with clip latents, DOI 10.48550/arXiv.2204.06125
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Ruan SL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13940, DOI 10.1109/ICCV48922.2021.01370
   Ruiz N, 2023, PROC CVPR IEEE, P22500, DOI 10.1109/CVPR52729.2023.02155
   Ryu S, 2022, Low-Rank Adaptation for Fast Text-to-Image Diffusion Fine-Tuning
   Saharia C., 2022, ADV NEURAL INF PROCE, V35, P36479
   Tao M, 2022, PROC CVPR IEEE, P16494, DOI 10.1109/CVPR52688.2022.01602
   Voynov A, 2023, PROCEEDINGS OF SIGGRAPH 2023 CONFERENCE PAPERS, SIGGRAPH 2023, DOI 10.1145/3588432.3591560
   Xie J, 2023, P IEEECVF INT C COMP, P7452
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Ye Y, 2024, P AAAI C ARTIFICIAL, V38, P6693
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang LM, 2023, IEEE I CONF COMP VIS, P3813, DOI 10.1109/ICCV51070.2023.00355
NR 45
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103986
DI 10.1016/j.cag.2024.103986
EA JUL 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YD0U1
UT WOS:001266439400001
DA 2024-08-05
ER

PT J
AU Tao, R
   Zhang, XK
   Ren, HX
   Yang, X
   Zhou, Y
AF Tao, Rui
   Zhang, Xianku
   Ren, Hongxiang
   Yang, Xiao
   Zhou, Yi
TI Wall-bounded flow simulation on vortex dynamics
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Wall-bounded flow; Vortex particle method; Boundary treatment; Boundary
   layer viscosity
ID SMOKE SIMULATION; HAIRPIN REMOVAL; RING
AB Vortical flow animation has attracted considerable attention within the realm of computer graphics. Given that boundaries are the source of vorticity, we introduce a novel approach for the wall-bounded flow simulation in the vortex dynamics framework. We enhance the traditional Lagrangian vortex particle method in terms of boundary treatment and viscosity computation for boundary layer to furnish support for the simulation of the vortex shedding phenomenon behind the moving solids. We extend the boundary treatment strategy based on the idea of vortex generation with a reasonable placement algorithm for the generated vortex element to better satisfy the impermeability of solids. Furthermore, we modify the particle strength exchange method at solid boundaries to capture momentum transfer of moving solids. We demonstrate the efficacy of our approach by simulating a series of wall-bounded flows, such as the wake behind a delta wing and the vortex shedding behind the rotating sphere.
C1 [Tao, Rui; Zhang, Xianku; Ren, Hongxiang; Yang, Xiao] Dalian Maritime Univ, Nav Coll, Dalian, Peoples R China.
   [Zhou, Yi] CNOOC Energy Technol & Serv Oil Prod Serv Co, Beijing, Peoples R China.
C3 Dalian Maritime University; China National Offshore Oil Corporation
   (CNOOC)
RP Ren, HX (corresponding author), Dalian Maritime Univ, Nav Coll, Dalian, Peoples R China.
EM tr2023@dlmu.edu.cn; Zhangxk@dlmu.edu.cn; dmu_rhx@dlmu.edu.cn;
   dlmuyx@dlmu.edu.cn; zhouyi@cnooc.com.cn
FX This work was supported by National Science Foundation of China (Grant
   No. 52071312) , Key Science and Technology Projects in Trans-portation
   Industry (Grant No. 2022-ZD3-035) , Applied Basic Research Program
   Project of Liaoning Province (Grant NO. 2023JH2/101300144) , Guangxi Key
   Research and Development Plan (Grant No. GUIKE AB22080106) , Dalian
   Science and Technology Innovation Fund Project (No. 2022JJ12GX035) .
CR Angelidis A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073606
   Angelidis Alexis., 2006, S COMPUTER ANIMATION, P25
   Angelidis Alexis., 2005, P 2005 ACM SIGGRAPH, P87, DOI [10.1145/1073368.1073380, DOI 10.1145/1073368.1073380]
   Bhosale Y, 2021, J COMPUT PHYS, V444, DOI 10.1016/j.jcp.2021.110577
   Brochu T, 2012, P ACM SIGGRAPH EUR S, P87
   Chern A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925868
   CHORIN AJ, 1990, J COMPUT PHYS, V91, P1, DOI 10.1016/0021-9991(90)90001-H
   CHORIN AJ, 1993, J COMPUT PHYS, V107, P1, DOI 10.1006/jcph.1993.1120
   Chorin AJ, 1973, J FLUID MECH, V57, P785, DOI 10.1017/S0022112073002016
   CHURCH CR, 1977, B AM METEOROL SOC, V58, P900, DOI 10.1175/1520-0477(1977)058<0900:TVSAPU>2.0.CO;2
   Cottet G.-H., 2000, Vortex methods: theory and practice, V8
   Cui QD, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201352
   De Witt T, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2077341.2077351
   DEGOND P, 1989, MATH COMPUT, V53, P485, DOI 10.2307/2008716
   Délery JM, 2001, ANNU REV FLUID MECH, V33, P129, DOI 10.1146/annurev.fluid.33.1.129
   Elcott S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1189762.1189766
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Gaitonde DV, 2023, ANNU REV FLUID MECH, V55, P291, DOI 10.1146/annurev-fluid-120720-022542
   He S, 2013, COMPUT GRAPH FORUM, V32, P27, DOI 10.1111/j.1467-8659.2012.03228.x
   Huang ZP, 2016, COMPUT ANIMAT VIRT W, V27, P14, DOI 10.1002/cav.1625
   Huang ZP, 2015, GRAPH MODELS, V78, P10, DOI 10.1016/j.gmod.2014.12.002
   KOUMOUTSAKOS P, 1994, J COMPUT PHYS, V113, P52, DOI 10.1006/jcph.1994.1117
   KUCHEMAN.D, 1965, J FLUID MECH, V21, P1, DOI 10.1017/S0022112065000010
   Liao XY, 2018, IEEE T VIS COMPUT GR, V24, P1260, DOI 10.1109/TVCG.2017.2665551
   Lighthill M.J., 1963, INTRO BOUNDARY LAYER
   Liu BB, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818130
   Park SI, 2005, P 2005 ACM SIGGRAPH, P261
   Pfaff T, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185608
   Pinkall U, 2007, J PHYS A-MATH THEOR, V40, P12563, DOI 10.1088/1751-8113/40/42/S04
   Ploumhans P, 2002, J COMPUT PHYS, V178, P427, DOI 10.1006/jcph.2002.7035
   Selle A, 2005, ACM T GRAPHIC, V24, P910, DOI 10.1145/1073204.1073282
   Stock MJ, 2008, J COMPUT PHYS, V227, P9021, DOI 10.1016/j.jcp.2008.05.022
   Tao R, 2022, COMPUT GRAPH-UK, V107, P289, DOI 10.1016/j.cag.2022.08.007
   Vines M, 2014, IEEE T VIS COMPUT GR, V20, P303, DOI 10.1109/TVCG.2013.95
   Wang C, 2017, INT J NUMER METHOD H, V27, P1186, DOI 10.1108/HFF-08-2015-0320
   Weissmann S, 2010, ACM SIGGRApH 2010 papers
   Weissmann S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601171
   Wu J.Z., 2007, Vorticity and Vortex Dynamics
   Wu J.Z., 2015, Vortical flows
   Xiong SY, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530150
   Xiong SY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459865
   Xu YX, 2012, COMM COM INF SC, V346, P467
   Yang SQ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459866
   Zhang XX, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661261
   Zhang XX, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766982
   Zhao ZZ, 2022, OCEAN ENG, V253, DOI 10.1016/j.oceaneng.2022.111266
NR 46
TC 0
Z9 0
U1 1
U2 1
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103990
DI 10.1016/j.cag.2024.103990
EA JUL 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YX5D8
UT WOS:001271789100001
DA 2024-08-05
ER

PT J
AU Hua, MJ
   Wu, JJ
   Zhong, ZC
AF Hua, Michael J.
   Wu, Junjie
   Zhong, Zichun
TI Multi-scale Knowledge Transfer Vision Transformer for 3D vessel shape
   segmentation
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Vision transformer; Convolutional embedding; Knowledge transfer;
   Multi-scale encoding
AB In order to facilitate the robust and precise 3D vessel shape extraction and quantification from in -vivo Magnetic Resonance Imaging (MRI), this paper presents a novel multi -scale Knowledge Transfer Vision Transformer (i.e., KT-ViT) for 3D vessel shape segmentation. First, it uniquely integrates convolutional embeddings with transformer in a U -net architecture, which simultaneously responds to local receptive fields with convolution layers and global contexts with transformer encoders in a multi -scale fashion. Therefore, it intrinsically enriches local vessel feature and simultaneously promotes global connectivity and continuity for a more accurate and reliable vessel shape segmentation. Furthermore, to enable using relatively low -resolution (LR) images to segment fine scale vessel shapes, a novel knowledge transfer network is designed to explore the inter -dependencies of data and automatically transfer the knowledge gained from high -resolution (HR) data to the low -resolution handling network at multiple levels, including the multi -scale feature levels and the decision level, through an integration of multi -level loss functions. The modeling capability of fine -scale vessel shape data distribution, possessed by the HR image transformer network, can be transferred to the LR image transformer to enhance its knowledge for fine vessel shape segmentation. Extensive experimental results on public image datasets have demonstrated that our method outperforms all other state-of-the-art deep learning methods.
C1 [Hua, Michael J.] Cranbrook Sch, Bloomfield Hills, MI 48303 USA.
   [Wu, Junjie; Zhong, Zichun] Wayne State Univ, Detroit, MI 48202 USA.
C3 Wayne State University
RP Hua, MJ (corresponding author), Cranbrook Sch, Bloomfield Hills, MI 48303 USA.
EM mhua27@cranbrook.edu
CR Brown WR, 2011, NEUROPATH APPL NEURO, V37, P56, DOI 10.1111/j.1365-2990.2010.01139.x
   Cai Q, 2022, The application of knowledge distillation toward fine-grained segmentation for three-vessel view of fetal heart ultrasound images
   Cao Hu, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13803), P205, DOI 10.1007/978-3-031-25066-8_9
   Cervantes J, 2023, NEUROCOMPUTING, V556, DOI 10.1016/j.neucom.2023.126626
   Chen C, 2023, IEEE T MED IMAGING, V42, P346, DOI 10.1109/TMI.2022.3184675
   Chen JE, 2023, Arxiv, DOI arXiv:2310.07781
   Child Rewon, 2019, ARXIV
   Cicek Ozgun, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P424, DOI 10.1007/978-3-319-46723-8_49
   Dang H, 2023, IEEE INT C AC SPEECH, P1
   Delannoy Q, 2020, COMPUT BIOL MED, V120, DOI 10.1016/j.compbiomed.2020.103755
   Deshpande A, 2021, NEUROIMAGE-CLIN, V30, DOI 10.1016/j.nicl.2021.102573
   Dorr A, 2012, BRAIN, V135, P3039, DOI 10.1093/brain/aws243
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Gou JP, 2021, INT J COMPUT VISION, V129, P1789, DOI 10.1007/s11263-021-01453-z
   Gouw AA, 2011, J NEUROL NEUROSUR PS, V82, P126, DOI 10.1136/jnnp.2009.204685
   Guo H, 2023, arXiv
   Hilbert A, 2020, FRONT ARTIF INTELL, V3, DOI 10.3389/frai.2020.552258
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Jenkinson M, 2005, 11 ANN M ORG HUMAN B
   Joo B, 2020, EUR RADIOL, V30, P5785, DOI 10.1007/s00330-020-06966-8
   Kamnitsas K, 2017, MED IMAGE ANAL, V36, P61, DOI 10.1016/j.media.2016.10.004
   Kingma D. P., 2014, arXiv
   Lee K, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11052014
   Li Y, 2017, PROC CVPR IEEE, P4438, DOI 10.1109/CVPR.2017.472
   Liu Y, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12052288
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Moccia S, 2018, COMPUT METH PROG BIO, V158, P71, DOI 10.1016/j.cmpb.2018.02.001
   Mott M, 2014, STROKE, V45, pE257, DOI 10.1161/STROKEAHA.114.007113
   Mou L, 2019, LECT NOTES COMPUT SC, V11764, P721, DOI 10.1007/978-3-030-32239-7_80
   Nakao T, 2018, J MAGN RESON IMAGING, V47, P948, DOI 10.1002/jmri.25842
   Parmar N, 2018, PR MACH LEARN RES, V80
   Peiris H, 2022, LECT NOTES COMPUT SC, V13435, P162, DOI 10.1007/978-3-031-16443-9_16
   Petit O, 2021, Arxiv, DOI arXiv:2103.06104
   Phellan R, 2017, LECT NOTES COMPUT SC, V10552, P39, DOI 10.1007/978-3-319-67534-3_5
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sanches P, 2019, I S BIOMED IMAGING, P768, DOI 10.1109/isbi.2019.8759569
   Strudel R., 2021, arXiv
   Tetteh G, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.592352
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HY, 2021, LECT NOTES COMPUT SC, V12901, P131, DOI 10.1007/978-3-030-87193-2_13
   Wang L, 2020, PROC CVPR IEEE, P3773, DOI 10.1109/CVPR42600.2020.00383
   Wang YF, 2021, IEEE T VIS COMPUT GR, V27, P1301, DOI 10.1109/TVCG.2020.3030374
   Wu HS, 2021, MED IMAGE ANAL, V70, DOI 10.1016/j.media.2021.102025
   Yu XH, 2023, PATTERN RECOGN, V135, DOI 10.1016/j.patcog.2022.109131
   Zhao YC, 2022, MED PHYS, V49, P1635, DOI 10.1002/mp.15483
NR 46
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103976
DI 10.1016/j.cag.2024.103976
EA JUN 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XG7Z9
UT WOS:001260609400001
DA 2024-08-05
ER

PT J
AU Negrao, MD
   Maciel, A
AF Negrao, Matheus D.
   Maciel, Anderson
TI Characterizing head-gaze and hand affordances using AR for laparoscopy
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Augmented reality; Head-gaze interaction; Laparoscopy interface
ID SURGERY; COMMUNICATION; FUNDAMENTALS; REALITY
AB Laparoscopic surgery techniques are complex and impose postural and communication constraints on the surgeon that may affect surgery outcomes. This paper explores the possibilities of designing intraoperative AR interfaces for laparoscopy surgery. We suggest that the laparoscopic video be displayed on an AR headset and that surgeons consult preoperative image data on that display. Interaction with these elements is necessary. Thus, we propose a head -gaze and hand clicker approach that is effective and minimalist, as well as the implementation of a prototype. We conduct a user study to evaluate the prototype and to comprehend the impact and improvements that headgaze average filtering and the scale method can bring to perform annotations in the laparoscopy video feed on a virtual monitor positioned straight in front of the user. The user experiment was performed in a between -subject protocol with 32 volunteers from the Institute of Informatics, and the proposed task involves communication in the interface through drawing annotations with proposed interaction approaches: A hand device for input confirmations and the head -gaze stabilization methods to pointing and selection. The study found that the users were confident about their performance and demonstrated low physical and temporal demand. The proposed head -gaze methods showed that independent of the stabilization applied, the difference between the error sensibility of the axis of the head -gaze in annotation positioning is significant. The vertical axis presented a higher error rate than the horizontal axis. When we compared other variables, we found some differences in specific circumstances, but overall, the interaction with HL1 is very distributed.
C1 [Negrao, Matheus D.; Maciel, Anderson] Fed Univ Rio Grande Do Sul UFRGS, Inst Informat, Porto Alegre, Brazil.
   [Maciel, Anderson] ULisboa INESC ID, Inst Super Tecn, Lisbon, Portugal.
C3 Universidade Federal do Rio Grande do Sul; Universidade de Lisboa
RP Negrao, MD (corresponding author), Fed Univ Rio Grande Do Sul UFRGS, Inst Informat, Porto Alegre, Brazil.
EM mdnegrao@inf.ufrgs.br; anderson.maciel@tecnico.ulisboa.pt
RI Maciel, Anderson/F-7734-2012
OI Maciel, Anderson/0000-0002-0780-6555
FU CAPES-Brazil [001]; CNPq-Brazil [311251/2020-0]; Fundacao para a Ciencia
   e Tecnologia (Portuguese Foundation for Science and Technology) under
   UNESCO Chair on AIXR [2022.09212.PTDC]
FX This work is partly funded by CAPES-Brazil-Finance Code 001, CNPq-Brazil
   project 311251/2020-0 and Fundacao para a Ciencia e Tecnologia
   (Portuguese Foundation for Science and Technology) grant 2022.09212.PTDC
   (XAVIER) under the auspices of the UNESCO Chair on AI&XR.
CR Accot J, 1997, Proceedings of the ACM Conference on Human Factors in Computing Systems, P295, DOI DOI 10.1145/258549.258760
   Aggarwal R, 2004, BRIT J SURG, V91, P1549, DOI 10.1002/bjs.4816
   Al Janabi HF, 2020, SURG ENDOSC, V34, P1143, DOI 10.1007/s00464-019-06862-3
   Asao T, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11104505
   Barsom EZ, 2016, SURG ENDOSC, V30, P4174, DOI 10.1007/s00464-016-4800-6
   Bradski G, 2000, DR DOBBS J, V25, P120
   Buia Alexander, 2015, World J Methodol, V5, P238, DOI 10.5662/wjm.v5.i4.238
   Derossis AM, 1998, SURG ENDOSC-ULTRAS, V12, P1117, DOI 10.1007/s004649900796
   Feng YY, 2020, SURG ENDOSC, V34, P3533, DOI 10.1007/s00464-019-07141-x
   Izard SG, 2018, J MED SYST, V42, DOI 10.1007/s10916-018-0900-2
   Grandi JG, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P127, DOI [10.1109/vr.2019.8798080, 10.1109/VR.2019.8798080]
   Grinshpoon A, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P751, DOI 10.1109/VR.2018.8446259
   Hafford ML, 2013, SURG ENDOSC, V27, P118, DOI 10.1007/s00464-012-2437-7
   HART S G, 1988, P139
   Hart SG., 2006, P HUM FACT ERG SOC A, V50, P904, DOI [10.1177/154193120605000909, DOI 10.1177/154193120605000909]
   Heinrich F, 2021, INT J COMPUT ASS RAD, V16, P161, DOI 10.1007/s11548-020-02272-2
   Jayender J, 2018, LECT NOTES COMPUT SC, V11073, P72, DOI 10.1007/978-3-030-00937-3_9
   Kobayashi T, 2019, Human interface and the management of information. information in intelligent systems, P164
   Kumar Rahul Prasanna, 2020, J Biomed Inform, V112S, P100077, DOI 10.1016/j.yjbinx.2020.100077
   Li Y, 2022, IEEE T VIS COMPUT GR, V28, P3896, DOI 10.1109/TVCG.2022.3203094
   Lin BX, 2016, INT J MED ROBOT COMP, V12, P158, DOI 10.1002/rcs.1661
   MacKenzie I. S., 1992, Human-Computer Interaction, V7, P91, DOI 10.1207/s15327051hci0701_3
   Mentis HM, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P2113, DOI 10.1145/2556288.2557387
   Morales Mojica C.M., 2018, VCBM 2018 - Eurographics Workshop on Visual Computing for Biology and Medicine, P17, DOI DOI 10.2312/VCBM.20181225
   Munz Y, 2004, SURG ENDOSC, V18, P485, DOI 10.1007/s00464-003-9043-7
   Qian Kun., 2015, Proceedings of the 21st ACM Symposium on Virtual Reality Software and Technology, P69, DOI DOI 10.1145/2821592.2821599
   Ritter EM, 2007, SURG INNOV, V14, P107, DOI 10.1177/1553350607302329
   Rodrigues SP, 2012, SURG ENDOSC, V26, P1005, DOI 10.1007/s00464-011-1986-5
   Sánchez-Margallo JA, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.692641
   Sarmiento W. J., 2014, 2014 International Workshop on Collaborative Virtual Environments (3DCVE). Proceedings, P1, DOI 10.1109/3DCVE.2014.7160931
   Sarmiento WJ, 2013, P IEEE VIRT REAL ANN, P63, DOI 10.1109/VR.2013.6549364
   Schneider Daniel, 2021, SUI '21: Symposium on Spatial User Interaction, DOI 10.1145/3485279.3485283
   Scott DJ, 2000, J AM COLL SURGEONS, V191, P272, DOI 10.1016/S1072-7515(00)00339-2
   Sebajang H, 2006, SURG ENDOSC, V20, P1389, DOI 10.1007/s00464-005-0260-0
   Sevdalis N, 2012, SURG ENDOSC, V26, P2931, DOI 10.1007/s00464-012-2287-3
   Souza V, 2020, SYMP VIRTUAL AUGMENT, P92, DOI 10.1109/SVR51698.2020.00028
   Supe Avinash N, 2010, J Minim Access Surg, V6, P31, DOI 10.4103/0972-9941.65161
   Taweel A, 2023, 2023 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS, VRW, P191, DOI 10.1109/VRW58643.2023.00047
   Thomaschewski M, 2021, BJS OPEN, V5, DOI 10.1093/bjsopen/zraa012
   Trejos AL, 2015, SURG ENDOSC, V29, P3655, DOI 10.1007/s00464-015-4122-0
   Trepkowski C, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P575, DOI [10.1109/VR.2019.8798312, 10.1109/vr.2019.8798312]
   Vassiliou MC, 2006, SURG ENDOSC, V20, P744, DOI 10.1007/s00464-005-3008-y
   Wang P, 2020, INTERACT COMPUT, V32, P153, DOI 10.1093/iwcomp/iwaa012
   Zorzal ER, 2020, J BIOMED INFORM, V107, DOI 10.1016/j.jbi.2020.103463
NR 44
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD JUN
PY 2024
VL 121
AR 103936
DI 10.1016/j.cag.2024.103936
EA MAY 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL9U4
UT WOS:001248340900001
DA 2024-08-05
ER

PT J
AU Heim, A
   Gall, A
   Waldner, M
   Groeller, E
   Heinzl, C
AF Heim, Anja
   Gall, Alexander
   Waldner, Manuela
   Groeller, Eduard
   Heinzl, Christoph
TI AccuStripes: Visual exploration and comparison of univariate data
   distributions using color and binning
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Visual analysis; Univariate data distributions; Adaptive binning;
   Crowd-sourced experiment
ID VISUALIZATION; PERCEPTION; GRAMMAR
AB Understanding and analyzing univariate distributions of data in terms of their shapes as well as their specific characteristics, regarding gaps, spikes, or outliers, is crucial in many scientific disciplines. In this paper, we propose a design space composed of the visual channels position and color for representing accumulated distributions. The designs are a mixture of color -coded stripes with density lines. The width and coloring of the stripes is based on the applied binning technique. In a crowd -sourced experiment we explore a subspace, called the AccuStripes (i.e., "accumulated stripes") design space, consisting of nine representations. These AccuStripes designs integrate three composition strategies (color only, overlay, filled curve) with three binning techniques, one uniform (UB) and two adaptive methods, namely Bayesian Blocks (BB) and Jenks' Natural Breaks (NB). We evaluate the accuracy, efficiency, and confidence ratings of the nine AccuStripes designs for structural estimation and comparison tasks. Across all study tasks, the overlay composition was found to be most accurate and preferred by observers. Furthermore, the results demonstrate that while no binning method performed best in both identification and comparison, detection of structures using adaptive binning was the most accurate one. For validation we compared the best AccuStripes' design, i.e., the overlay composition, to line charts. Our results show that the AccuStripes' design outperformed the line charts in accuracy for all study tasks.
C1 [Heim, Anja; Gall, Alexander] Univ Appl Sci Upper Austria, Roseggerstr 15, A-4600 Wels, Austria.
   [Heim, Anja; Heinzl, Christoph] Fraunhofer Inst Integrated Circuits IIS, Div Dev Ctr Xray Technol, Flugplatzstr 75, Furth, Germany.
   [Waldner, Manuela; Groeller, Eduard] Vienna Univ Technol, Favoritenstr 9-11, A-1040 Vienna, Austria.
   [Heim, Anja; Gall, Alexander; Heinzl, Christoph] Univ Passau, Innstr 43, D-94032 Passau, Germany.
C3 Fraunhofer Gesellschaft; Technische Universitat Wien; University of
   Passau
RP Heim, A (corresponding author), Univ Appl Sci Upper Austria, Roseggerstr 15, A-4600 Wels, Austria.
EM anja.heim@iis.fraunhofer.de; alexander.gall@uni-passau.de;
   waldner@cg.tuwien.ac.at; groeller@cg.tuwien.ac.at;
   christoph.heinzl@uni-passau.de
RI Waldner, Manuela/JZC-9267-2024
OI Waldner, Manuela/0000-0003-1387-5132
FU Government of Upper Austria [881298, 881309]; COMET by FFG [879730]
FX The research leading to these results has received funding by research
   subsidies granted by the government of Upper Austria within the projects
   "AugmeNDT", grant no. 881298, and "COMPARE", grant no. 881309. Part of
   the research was enabled by VRV is funded in "COMET" (879730) a program
   managed by FFG.
CR Aigner W, 2012, COMPUT GRAPH FORUM, V31, P995, DOI 10.1111/j.1467-8659.2012.03092.x
   Aigner W, 2011, HUM-COMPUT INT-SPRIN, P1, DOI 10.1007/978-0-85729-079-3
   Aigner W, 2011, COMPUT GRAPH FORUM, V30, P215, DOI 10.1111/j.1467-8659.2010.01845.x
   Albers D, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P551, DOI 10.1145/2556288.2557200
   Albers D, 2011, IEEE T VIS COMPUT GR, V17, P2392, DOI 10.1109/TVCG.2011.232
   Apps script, 2009, google developers
   Bade R, 2004, P SIGCHI C HUM FACT, DOI [DOI 10.1145/985692.985706, 10.1145/985692.985706]
   Bazan E, 2019, BRIT MACH VIS C 2019
   Berry Lior., 2004, INFORM VISUAL, P2, DOI DOI 10.1109/INFVIS.2004.11
   Blumenschein M, 2020, COMPUT GRAPH FORUM, V39, P565, DOI 10.1111/cgf.14002
   Brewer CA, 2002, ANN ASSOC AM GEOGR, V92, P662, DOI 10.1111/1467-8306.00310
   Cha S.-H., 2007, Int. J. Math. Model. Meth. Appl. Sci., V1, P300
   Cho M, 2014, IEEE T VIS COMPUT GR, V20, P808, DOI 10.1109/TVCG.2013.2297933
   Cleff T, 2013, Exploratory Data Analysis in Business and Economics, P23, DOI [DOI 10.1007/978-3-319-01517-0_3, 10.1007/978-3-319-01517-0_3]
   Correll M, 2012, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, DOI [DOI 10.1145/2207676.22085562, 10.1145/2207676.2208556, DOI 10.1145/2207676.2208556]
   Correll M, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174216
   Correll M, 2019, IEEE T VIS COMPUT GR, V25, P830, DOI 10.1109/TVCG.2018.2864907
   Elkin Lisa A., 2021, UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology, P754, DOI 10.1145/3472749.3474784
   FISHER WD, 1958, J AM STAT ASSOC, V53, P789, DOI 10.2307/2281952
   Floricel C, 2022, IEEE T VIS COMPUT GR, V28, P151, DOI 10.1109/TVCG.2021.3114810
   Gogolouis A, 2019, IEEE T VIS COMPUT GR, V25, P523, DOI 10.1109/TVCG.2018.2865077
   Han HL, 2020, P GRAPHICS INTERFACE, P225, DOI DOI 10.20380/GI2020.23
   Heer J, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1303
   Heim A, 2021, Vision, modeling, and visualization, DOI DOI 10.2312/VMV.20211378
   Jabbari A, 2018, P 30 C INT HOMM MACH, DOI [10.1145/3286689.3286694, DOI 10.1145/3286689.3286694]
   Javed W, 2010, IEEE T VIS COMPUT GR, V16, P927, DOI 10.1109/TVCG.2010.162
   Jenks G.F., 1977, Optimal data classification for choropleth maps
   Lam H, 2007, IEEE T VIS COMPUT GR, V13, P1278, DOI 10.1109/TVCG.2007.70583
   Lampe OD, 2011, IEEE PAC VIS SYMP, P171, DOI 10.1109/PACIFICVIS.2011.5742387
   Leow WK, 2004, COMPUT VIS IMAGE UND, V94, P67, DOI 10.1016/j.cviu.2003.10.010
   Liu Y, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174172
   Ma Y, 2010, COMPUT VIS IMAGE UND, V114, P981, DOI 10.1016/j.cviu.2010.03.006
   Mansmann F, 2013, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI'13, DOI [10.1145/2470654.2466443, DOI 10.1145/2470654.2466443]
   Maurer J, 2022, POLYM TEST, V109, DOI 10.1016/j.polymertesting.2022.107551
   McColeman CM, 2022, IEEE T VIS COMPUT GR, V28, P707, DOI 10.1109/TVCG.2021.3114684
   McNutt A, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376420
   Menning KM, 2007, OECOLOGIA, V154, P75, DOI 10.1007/s00442-007-0810-3
   Mittelstädt S, 2014, COMPUT GRAPH FORUM, V33, P231, DOI 10.1111/cgf.12379
   Munzner T., 2014, Visualization analysis and design, DOI DOI 10.1201/B17511
   Ondov B, 2019, IEEE T VIS COMPUT GR, V25, P861, DOI 10.1109/TVCG.2018.2864884
   Palacio-Ni¤o JO, 2019, Arxiv, DOI [arXiv:1905.05667, DOI 10.48550/ARXIV.1905.05667.ARXIV1905.05667]
   Pollack B, 2019, Arxiv, DOI [arXiv:1708.00810, 10.48550/ARXIV.1708.00810, DOI 10.48550/ARXIV.1708.00810]
   Quadri GJ, 2022, IEEE T VIS COMPUT GR, V28, P5026, DOI 10.1109/TVCG.2021.3098240
   Reda K, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173846
   Rodrigues AMB, 2019, SIBGRAPI, P84, DOI 10.1109/SIBGRAPI.2019.00020
   Sahann R, 2021, 2021 IEEE VISUALIZATION CONFERENCE - SHORT PAPERS (VIS 2021), P66, DOI 10.1109/VIS49827.2021.9623301
   Saito T, 2005, INFOVIS 05: IEEE Symposium on Information Visualization, Proceedings, P173, DOI 10.1109/INFVIS.2005.1532144
   Satyanarayan A, 2017, IEEE T VIS COMPUT GR, V23, P341, DOI 10.1109/TVCG.2016.2599030
   Schubert E, 2017, ACM T DATABASE SYST, V42, DOI 10.1145/3068335
   Setlur V, 2022, IEEE VIS CONF, P100, DOI 10.1109/VIS54862.2022.00029
   Silverman B., 1986, DENSITY ESTIMATION S, DOI [10.1201/9781315140919, DOI 10.1201/9781315140919]
   Szafir DA, 2016, COMPUT GRAPH FORUM, V35, P421, DOI 10.1111/cgf.12918
   Szafir DA, 2016, J VISION, V16, DOI 10.1167/16.5.11
   Talbot J, 2014, IEEE T VIS COMPUT GR, V20, P2152, DOI 10.1109/TVCG.2014.2346320
   Thrun MC, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0238835
   Vanderplas S, 2020, ANNU REV STAT APPL, V7, P61, DOI 10.1146/annurev-statistics-031219-041252
   Wang CL, 2008, IEEE T VIS COMPUT GR, V14, P1547, DOI 10.1109/TVCG.2008.140
   Weglarczyk S, 2018, ITM WEB CONF, V23, DOI 10.1051/itmconf/20182300037
   Wickham H, 2010, J COMPUT GRAPH STAT, V19, P3, DOI 10.1198/jcgs.2009.07098
   Wobbrock JO, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P143, DOI 10.1145/1978942.1978963
   Wu CH, 2013, INFORM SCIENCES, V239, P154, DOI 10.1016/j.ins.2013.03.014
   Zeileis A, 2020, J STAT SOFTW, V96, P1, DOI 10.18637/jss.v096.i01
   Zeng ZH, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2023), DOI 10.1145/3544548.3581349
NR 63
TC 0
Z9 0
U1 1
U2 1
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103906
DI 10.1016/j.cag.2024.103906
EA MAR 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PV6N9
UT WOS:001216898900001
DA 2024-08-05
ER

PT J
AU Wang, HC
   Cao, YL
   Wei, XY
   Shou, YJ
   Shen, LF
   Xu, ZJ
   Ren, K
AF Wang, Haocheng
   Cao, Yanlong
   Wei, Xiaoyao
   Shou, Yejun
   Shen, Lingfeng
   Xu, Zhijie
   Ren, Kai
TI Structerf-SLAM: Neural implicit representation SLAM for structural
   environments
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Neural implicit representation; Visual SLAM; Implicit scene
   reconstruction; Structural constraints
AB In recent years, research on simultaneous localization and mapping (SLAM) using neural implicit representation has shown promising outcomes due to its smooth mapping and low memory consumption, particularly suitable for structured environments with limited boundaries. However, there is currently no implicit SLAM that can effectively utilize prior structural constraints to accurately build 3D maps. In this study, we propose an RGB-D dense tracking and mapping approach, Structerf-SLAM, that combines visual odometry with neural implicit representation. Our scene representation consists of dual -layer feature grids and pre-trained decoders that decode the interpolated features into RGB and depth values. Moreover, structured planar constraints are integrated. In the tracking stage, utilizing the three-dimensional plane features under the Manhattan assumption achieves more stable and rapid data association, consequently resolving the tracking misalignment issue in textureless regions (e.g., floor, wall, etc.). In the mapping stage, by enforcing planar consistency, the depth predicted by the neural radiation field is well-fitted by a plane, resulting in smoother and more realistic map reconstruction. Experiments on synthetic and real scene datasets demonstrate competitive results of Structerf-SLAM in both mapping and tracking quality.
C1 [Wang, Haocheng; Cao, Yanlong; Wei, Xiaoyao; Shou, Yejun; Shen, Lingfeng; Ren, Kai] Zhejiang Univ, Coll Mech Engn, Key Lab Adv Mfg Technol Zhejiang Prov, Hangzhou 310027, Peoples R China.
   [Wang, Haocheng; Cao, Yanlong; Wei, Xiaoyao; Shou, Yejun; Shen, Lingfeng; Ren, Kai] Zhejiang Univ, Coll Mech Engn, State Key Lab Fluid Power & Mechatron Syst, Hangzhou 310027, Peoples R China.
   [Xu, Zhijie] Univ Huddersfield, Sch Comp & Engn, Huddersfield HD1 3DH, England.
   [Cao, Yanlong] 866 Yuhangtang Rd, Hangzhou, Zhejiang, Peoples R China.
C3 Zhejiang University; Zhejiang University; University of Huddersfield
RP Cao, YL (corresponding author), 866 Yuhangtang Rd, Hangzhou, Zhejiang, Peoples R China.
EM sdcaoyl@zju.edu.cn
OI cao, yanlong/0000-0003-0383-6586; Wei, Xiaoyao/0009-0008-3638-9296
FU Foundation of Science and Technology In-novation leading talent project
   of special support plan for high-level talents of Zhejiang Province,
   China [2022R52053]; Open Fund of Anhui Key Laboratory of Mine
   Intelligent Equipment and Technology, China [KSZN202001001]
FX Research supported by Foundation of Science and Technology In-novation
   leading talent project of special support plan for high-level talents of
   Zhejiang Province, China (2022R52053) , Open Fund of Anhui Key
   Laboratory of Mine Intelligent Equipment and Technology, China
   (KSZN202001001) .
CR Chung CM, 2023, IEEE INT CONF ROBOT, P9400, DOI 10.1109/ICRA48891.2023.10160950
   Coughlan J.M., 1999, ICCV, V2, P941, DOI DOI 10.1109/ICCV.1999.790349
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Guan YR, 2023, COMPUT GRAPH-UK, V114, P257, DOI 10.1016/j.cag.2023.06.013
   Huang JH, 2021, PROC CVPR IEEE, P8928, DOI 10.1109/CVPR46437.2021.00882
   Jiang HL, 2021, INT CONF 3D VISION, P741, DOI 10.1109/3DV53792.2021.00083
   Johari MM, 2023, PROC CVPR IEEE, P17408, DOI 10.1109/CVPR52729.2023.01670
   Kim P, 2018, LECT NOTES COMPUT SC, V11208, P350, DOI 10.1007/978-3-030-01225-0_21
   Klein George, 2007, P1
   Kruzhkov Evgenii, 2022, 2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC), P430, DOI 10.1109/SMC53654.2022.9945381
   Li MR, 2023, IEEE ROBOT AUTOM LET, V8, P7138, DOI 10.1109/LRA.2023.3311365
   Li QY, 2022, COMPUT GRAPH-UK, V107, P10, DOI 10.1016/j.cag.2022.06.013
   Ma LN, 2016, IEEE INT CONF ROBOT, P1285, DOI 10.1109/ICRA.2016.7487260
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Ming YH, 2022, Arxiv, DOI arXiv:2209.07919
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Peng S., 2020, COMPUTER VISION ECCV
   Pire T, 2017, ROBOT AUTON SYST, V93, P27, DOI 10.1016/j.robot.2017.03.019
   Pu HY, 2023, IEEE SENS J, V23, P22119, DOI 10.1109/JSEN.2023.3306371
   Rosinol A, 2023, IEEE INT C INT ROBOT, P3437, DOI 10.1109/IROS55552.2023.10341922
   Schöps T, 2019, PROC CVPR IEEE, P134, DOI 10.1109/CVPR.2019.00022
   Straub J, 2019, Arxiv, DOI arXiv:1906.05797
   Sucar E, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6209, DOI 10.1109/ICCV48922.2021.00617
   Taguchi Y, 2013, IEEE INT CONF ROBOT, P5182, DOI 10.1109/ICRA.2013.6631318
   Tancik M., 2020, ADV NEURAL INFORM PR, V33, P7537, DOI DOI 10.48550/ARXIV.2006.10739
   Tang Y, 2023, IEEE T NEUR NET LEAR, V34, P9604, DOI 10.1109/TNNLS.2022.3167688
   Tongbuasirilai T, 2020, VISUAL COMPUT, V36, P855, DOI 10.1007/s00371-019-01664-z
   Whelan T, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI
   Yang XR, 2022, INT SYM MIX AUGMENT, P499, DOI 10.1109/ISMAR55827.2022.00066
   Yen-Chen L, 2021, IEEE INT C INT ROBOT, P1323, DOI 10.1109/IROS51168.2021.9636708
   Zhang XY, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19173795
   Zheng Chen, 2023, IEEE Trans Pattern Anal Mach Intell
   Zhou HZ, 2015, IEEE T VEH TECHNOL, V64, P1364, DOI 10.1109/TVT.2015.2388780
   Zhu ZH, 2022, PROC CVPR IEEE, P12776, DOI 10.1109/CVPR52688.2022.01245
NR 39
TC 0
Z9 0
U1 24
U2 24
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103893
DI 10.1016/j.cag.2024.103893
EA FEB 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MM2X8
UT WOS:001193984300001
DA 2024-08-05
ER

PT J
AU Liu, YT
   Liu, ZH
   Yin, H
   Wan, J
   Wu, ZY
   Wu, XY
   Wang, S
AF Liu, Yanting
   Liu, Zhihao
   Yin, Hui
   Wan, Jin
   Wu, Zhenyao
   Wu, Xinyi
   Wang, Song
TI Estimating intrinsic characteristics of images for shadow removal
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Shadow removal; New shadow illumination model; Mask guidance
AB Existing works show that shadow removal tasks can benefit from the physical illumination model on the formation of shadows. Inspired by prior works that recover the intrinsic characteristics by decomposing an image for its reflectance and illumination components, we study a variant of shadow illumination model that can better reflect the complexity in the real world - in this model, shadow -free pixels can be expressed by a translation formed of reflectance and illumination components. Based on the new illumination model, we develop a new LR-ShadowNet, which contains two sub -nets for estimating the illumination and reflectance components, respectively, and one sub -net for refining the shadow -removal result. Besides, several mask guidance module are incorporated into LR-ShadowNet for guiding components estimation and result refinement based on shadow region information. The whole network is trained in an end -to -end fashion guided by the shadow masks. Extensive experiments on the ISTD dataset and SBU-Timelapse dataset show that the proposed LR-ShadowNet achieves competitive performance with less computational cost and strong generalisation ability.
C1 [Liu, Yanting; Yin, Hui] Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
   [Liu, Zhihao] China Mobile Res Inst, Artificial Intelligence & Intelligent Operat R&D C, Beijing 100053, Peoples R China.
   [Wan, Jin] Beijing Jiaotong Univ, Key Lab Beijing Railway Engn, Beijing 100044, Peoples R China.
   [Wu, Zhenyao; Wu, Xinyi; Wang, Song] Univ South Carolina, Dept Comp Sci & Engn, Columbia, SC 29201 USA.
C3 Beijing Jiaotong University; China Mobile; Beijing Jiaotong University;
   University of South Carolina System; University of South Carolina
   Columbia
RP Yin, H (corresponding author), Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
EM 18120383@bjtu.edu.cn; liuzhihao@chinamobile.com; hyin@bjtu.edu.cn;
   jinwan@bjtu.edu.cn; zhenyao@email.sc.edu; xinyiw@email.sc.edu;
   songwang@cec.sc.edu
FU National Key R&D Program "Trans-portation Infrastructure""Reveal the
   List and Take Command"Project [2022YFB2603302]; National Nature Science
   Foundation of China [51827813]; Fundamental Research Funds for the
   Central Universities (Science and technology leading talent team
   project) [2022 JBQY009]
FX This work is supported by National Key R&D Program "Trans-portation
   Infrastructure""Reveal the List and Take Command"Project
   (2022YFB2603302) , National Nature Science Foundation of China
   (51827813) and the Fundamental Research Funds for the Central
   Universities (Science and technology leading talent team project) (2022
   JBQY009) .
CR Barrow H. G., 1978, COMPUTER VISION SYST, P3
   Baslamisli AS, 2021, INT J COMPUT VISION, V129, P2445, DOI 10.1007/s11263-021-01477-5
   Bonneel N, 2017, COMPUT GRAPH FORUM, V36, P593, DOI 10.1111/cgf.13149
   Chen QF, 2013, IEEE I CONF COMP VIS, P241, DOI 10.1109/ICCV.2013.37
   Cun XD, 2020, AAAI CONF ARTIF INTE, V34, P10680
   Einy T, 2022, IEEE COMPUT SOC CONF, P3011, DOI 10.1109/CVPRW56347.2022.00340
   Finlayson GD, 2006, IEEE T PATTERN ANAL, V28, P59, DOI 10.1109/TPAMI.2006.18
   Finlayson GD, 2002, LECT NOTES COMPUT SC, V2353, P823
   Finlayson G, 2016, J OPT SOC AM A, V33, P589, DOI 10.1364/JOSAA.33.000589
   Fu L, 2021, PROC CVPR IEEE, P10566, DOI 10.1109/CVPR46437.2021.01043
   Fu Y, 2022, Comput Graph Forum, V41
   Gao JH, 2022, IEEE COMPUT SOC CONF, P598, DOI 10.1109/CVPRW56347.2022.00075
   Gong H, 2014, BRIT MACH VIS C
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo RQ, 2013, IEEE T PATTERN ANAL, V35, P2956, DOI 10.1109/TPAMI.2012.214
   Hu XW, 2019, IEEE I CONF COMP VIS, P2472, DOI 10.1109/ICCV.2019.00256
   Hu XW, 2020, IEEE T PATTERN ANAL, V42, P2795, DOI 10.1109/TPAMI.2019.2919616
   Jin YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5007, DOI 10.1109/ICCV48922.2021.00498
   Jung CR, 2009, IEEE T MULTIMEDIA, V11, P571, DOI 10.1109/TMM.2009.2012924
   Laffont PY, 2013, IEEE T VIS COMPUT GR, V19, P210, DOI 10.1109/TVCG.2012.112
   Le H, 2018, EUR C COMP VIS
   Le H, 2019, IEEE C COMPUT VIS PA, P18
   Le H, 2022, IEEE T PATTERN ANAL, V44, P9088, DOI 10.1109/TPAMI.2021.3124934
   Le H, 2019, IEEE I CONF COMP VIS, P8577, DOI 10.1109/ICCV.2019.00867
   Le H, 2020, INT CONF IMAG VIS
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Liu ZH, 2021, PROC CVPR IEEE, P4925, DOI 10.1109/CVPR46437.2021.00489
   Liu ZH, 2021, IEEE T IMAGE PROCESS, V30, P1853, DOI 10.1109/TIP.2020.3048677
   Matsushita Y, 2004, IEEE T PATTERN ANAL, V26, P1336, DOI 10.1109/TPAMI.2004.86
   Müller T, 2019, PROC SPIE, V11009, DOI 10.1117/12.2518618
   Nadimi S, 2004, IEEE T PATTERN ANAL, V26, P1079, DOI 10.1109/TPAMI.2004.51
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Qu LQ, 2017, PROC CVPR IEEE, P2308, DOI 10.1109/CVPR.2017.248
   Shor Y, 2008, COMPUT GRAPH FORUM, V27, P577, DOI 10.1111/j.1467-8659.2008.01155.x
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su N, 2016, IEEE J-STARS, V9, P2568, DOI 10.1109/JSTARS.2016.2570234
   Surkutlawar S, 2013, INT J ADV COMPUT SC, V4, P164
   Vincente TFY, 2016, LECT NOTES COMPUT SC, V9910, P816, DOI 10.1007/978-3-319-46466-4_49
   Wang JF, 2018, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR.2018.00192
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Weiss Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P68, DOI 10.1109/ICCV.2001.937606
   Wu W, 2021, COMPUT GRAPH-UK, V95, P156, DOI 10.1016/j.cag.2021.02.005
   Xiao C, 2013, Fast shadow removal using adaptive multi-scale illumination transfer
   Yang QX, 2012, IEEE T IMAGE PROCESS, V21, P4361, DOI 10.1109/TIP.2012.2208976
   Zhang WM, 2019, IEEE T PATTERN ANAL, V41, P611, DOI 10.1109/TPAMI.2018.2803179
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu L, 2018, LECT NOTES COMPUT SC, V11210, P122, DOI 10.1007/978-3-030-01231-1_8
NR 47
TC 1
Z9 1
U1 4
U2 4
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD MAY
PY 2024
VL 120
AR 103922
DI 10.1016/j.cag.2024.103922
EA MAY 2024
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SX1N3
UT WOS:001237656200001
DA 2024-08-05
ER

PT J
AU Nicolas, W
   Ulrich, S
   Mario, B
AF Nicolas, Wagner
   Ulrich, Schwanecke
   Mario, Botsch
TI SparseSoftDECA - Efficient high-resolution physics-based facial
   animation from sparse landmarks
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Facial animation; Deep learning; Physics-based simulation
AB Facial animation on computationally limited systems still heavily relies on linear blendshape models. Nonetheless, these models exhibit common issues like volume loss, self -collisions, and inaccuracies in soft tissue elasticity. Furthermore, personalizing blendshapes models demands significant effort, but there are limited options for simulating or manipulating physical and anatomical characteristics afterwards. Also, second-order dynamics can only be partially represented. For many years, physics-based facial simulations have been explored as an alternative to linear blendshapes, however, those remain cumbersome to implement and result in a high computational burden. We present a novel deep learning approach that offers the advantages of physics-based facial animations while being effortless and fast to use on top of linear blendshapes. For this, we design an innovative hypernetwork that efficiently approximates a physics-based facial simulation while generalizing over the extensive DECA model of human identities, facial expressions, and a wide range of material properties that can be locally adjusted without re-training. In addition to our previous work, we also demonstrate how the hypernetwork can be applied to facial animation from a sparse set of tracked landmarks. Unlike before, we no longer require linear blendshapes as the foundation of our system but directly operate on neutral head representations. This application is also used to complement an existing framework for commodity smartphones that already implements high resolution scanning of neutral faces and expression tracking.
C1 [Nicolas, Wagner; Mario, Botsch] TU Dortmund Univ, Otto Hahn Str 16, D-44227 Dortmund, Germany.
   [Ulrich, Schwanecke] Univ Appl Sci RheinMain, Kurt Schumacher Ring 18, D-65197 Wiesbaden, Germany.
C3 Dortmund University of Technology
RP Nicolas, W (corresponding author), TU Dortmund Univ, Otto Hahn Str 16, D-44227 Dortmund, Germany.
EM nicolas.wagner@tu-dortmund.de
OI Wagner, Nicolas/0000-0002-6225-1003
FU German Federal Ministry of Education and Research (BMBF) [16SV8785]
FX This research was supported by the German Federal Ministry of Education
   and Research (BMBF) through the project HiAvA (ID 16SV8785) .
CR Achenbach Jascha., 2018, P EUR WORKSH VIS COM, P67, DOI DOI 10.2312/VCBM.20181230
   Ali-Hamadi D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508415
   Athar S, 2022, PROC CVPR IEEE, P20332, DOI 10.1109/CVPR52688.2022.01972
   Bao Michael, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10794, DOI 10.1109/CVPR.2019.01106
   Barrielle V, 2016, COMPUT GRAPH FORUM, V35, P341, DOI 10.1111/cgf.12836
   Beeler T, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601182
   Bickel B, 2008, Proceedings of the 2008 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '08, P57
   Botsch M, 2005, COMPUT GRAPH FORUM, V24, P611, DOI 10.1111/j.1467-8659.2005.00886.x
   Botsch M, 2010, Polygon Mesh Processing, DOI DOI 10.1201/B10688
   Botsch Mario., 2006, VISION MODELING VISU, P357
   Bouaziz S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601116
   Bradley D, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778778
   Brandt C, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201387
   Cao C, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530143
   Casas D, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3203187
   Choi B, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555398
   Chung JY, 2014, Arxiv, DOI [arXiv:1412.3555, DOI 10.48550/ARXIV.1412.3555]
   Cong M, 2019, SIGGRAPH '19 -ACM SIGGRAPH 2019 TALKS, DOI 10.1145/3306307.3328154
   Cong MatthewDeying., 2016, Art-Directed Muscle Simulation for High-End Facial Animation
   Feng Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459936
   Garbin Stephan J, 2022, arXiv
   Gietzen T, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0210257
   Gilles B, 2010, COMPUT GRAPH FORUM, V29, P2340, DOI 10.1111/j.1467-8659.2010.01718.x
   Grassal PW, 2022, PROC CVPR IEEE, P18632, DOI 10.1109/CVPR52688.2022.01810
   Ha D, 2016, Arxiv, DOI arXiv:1609.09106
   Holden D, 2019, PROCEEDINGS SCA 2019: ACM SIGGRAPH/EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION, DOI 10.1145/3309486.3340245
   Ichim AE, 2016, S COMP AN, P107
   Ichim AE, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073664
   Ichim AE, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766974
   Kadlecek P, 2019, P ACM COMPUT GRAPH, V2, DOI 10.1145/3340256
   Kadlecek P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982438
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Keller M, 2023, ACM TOG PROC SIGGRAP
   Keller M, 2022, PROC CVPR IEEE, P20460, DOI 10.1109/CVPR52688.2022.01984
   Komaritzan M, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.694244
   Komaritzan M, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3203203
   Kozlov Y, 2017, COMPUT GRAPH FORUM, V36, P75, DOI 10.1111/cgf.13108
   Lee J, 2019, PR MACH LEARN RES, V97
   Lewis JP, 2005, P 2005 S INT 3D GRAP, P25
   Lewis JP, 2014, Eurographics (State of the Art Reports), V1, P2, DOI [DOI 10.2312/EGST.20141042, 10.2312/egst.20141042]
   Li H, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778769
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Maalin N, 2021, BEHAV RES METHODS, V53, P1308, DOI 10.3758/s13428-020-01494-1
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Parke Frederic, 1991, COMPUTER ANIMATION 9, V91, P3
   Romero C, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530182
   Saito S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766957
   Santesteban I, 2020, COMPUT GRAPH FORUM, V39, P65, DOI 10.1111/cgf.13912
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Schleicher R, 2021, GRAPP: PROCEEDINGS OF THE 16TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS - VOL. 1: GRAPP, P25, DOI 10.5220/0010210600250036
   Sifakis E, 2005, ACM T GRAPHIC, V24, P417, DOI 10.1145/1073204.1073208
   Song SL, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392491
   Srinivasan SG, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459883
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Wagner N, 2023, 15TH ANNUAL ACM SIGGRAPH CONFERENCE ON MOTION, INTERACTION AND GAMES, MIG 2023, DOI 10.1145/3623264.3624439
   Wenninger Stephan., 2020, P 26 ACM S VIRTUAL R
   Yang LC, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530156
   Zhang Li, 2008, Data-driven 3D facial animation, P248
   Zheng YF, 2022, PROC CVPR IEEE, P13535, DOI 10.1109/CVPR52688.2022.01318
   Zielonka W, 2023, PROC CVPR IEEE, P4574, DOI 10.1109/CVPR52729.2023.00444
NR 60
TC 1
Z9 1
U1 1
U2 1
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103903
DI 10.1016/j.cag.2024.103903
EA MAR 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OI3Z3
UT WOS:001206616200001
OA hybrid
DA 2024-08-05
ER

PT J
AU Wei, JX
   Cheng, L
   Hu, ZM
   Ren, MF
   Xu, XY
   Yan, GW
AF Wei, Jiangxia
   Cheng, Lan
   Hu, Zhimin
   Ren, Mifeng
   Xu, Xinying
   Yan, Gaowei
TI HPNet: High precision point cloud registration using feature pyramid and
   hybrid position encoding
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Point cloud registration; Hybrid position encoding; Deep learning
ID SCAN REGISTRATION; HISTOGRAMS
AB Point cloud registration precision degrades when the scene contains symmetric structures or repeated patches. Also, the common operation of continuous downsampling in existing registration methods causes the loss of detailed information and leads to further registration precision degradation. In this paper, we propose a high precision point cloud registration network (HPNet) to learn discriminative features by considering both multi -scale information and position information. We first propose a multi -scale feature extraction module similar to the feature pyramid that allows the extracted features to contain multi -scale information. Then, an information interaction is performed on the features to learn global contextual information by using Transformer with a hybrid position encoding, which takes into account both absolute and relative positions of points. Finally, the features obtained from the information interaction module are directly used to predict the point correspondences. Comprehensive experiments on 3DMatch, 3DLoMatch, ModelNet, and ModelLoNet datasets demonstrate the state-of-the-art performance of the proposed method and the implementation of HPNet in SLAM shows its effectiveness in real application.
C1 [Wei, Jiangxia; Cheng, Lan; Hu, Zhimin; Ren, Mifeng; Xu, Xinying; Yan, Gaowei] Taiyuan Univ Technol, Coll Elect & Power Engn, Taiyuan 030024, Peoples R China.
C3 Taiyuan University of Technology
RP Cheng, L (corresponding author), Taiyuan Univ Technol, Coll Elect & Power Engn, Taiyuan 030024, Peoples R China.
EM chenglan@tyut.edu.cn
FU National Natural Science Foun-dation of China [62073232, 61973226];
   Shanxi Sci-ence and Technology Cooperation and Exchange Project
   [202104041101030]; Natural Science Foundation of Shanxi Province
   [20210302123189]
FX <B>Acknowledgments</B> This work was supported by the National Natural
   Science Foun-dation of China under Grants 62073232 and 61973226, Shanxi
   Sci-ence and Technology Cooperation and Exchange Project under Grant
   202104041101030, and the Natural Science Foundation of Shanxi Province
   under Grant 20210302123189.
CR Aiger D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360684
   Bai XY, 2020, PROC CVPR IEEE, P6358, DOI 10.1109/CVPR42600.2020.00639
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Brown T., 2020, ADV NEURAL INFORM PR, V33, P1877, DOI DOI 10.48550/ARXIV.2005.14165
   Cao AQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13209, DOI 10.1109/ICCV48922.2021.01298
   Carion N., 2020, EUR C COMP VIS, P213
   Choy C, 2020, PROC CVPR IEEE, P2511, DOI 10.1109/CVPR42600.2020.00259
   Choy C, 2019, IEEE I CONF COMP VIS, P8957, DOI 10.1109/ICCV.2019.00905
   Dai A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3054739
   Das A, 2014, INT J ROBOT RES, V33, P1645, DOI 10.1177/0278364914539404
   Deng HW, 2018, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2018.00028
   Deng Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6118, DOI 10.1109/ICCV48922.2021.00608
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Drost B, 2010, PROC CVPR IEEE, P998, DOI 10.1109/CVPR.2010.5540108
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fu KX, 2023, IEEE T PATTERN ANAL, V45, P6183, DOI [10.1109/TPAMI.2022.3204713, 10.1109/CVPR46437.2021.00878]
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gojcic Z, 2019, PROC CVPR IEEE, P5540, DOI 10.1109/CVPR.2019.00569
   Halber Maciej., 2017, P IEEE C COMPUTER VI, P1755
   Horache S, 2021, INT CONF 3D VISION, P1351, DOI 10.1109/3DV53792.2021.00142
   Huang SY, 2021, PROC CVPR IEEE, P4265, DOI 10.1109/CVPR46437.2021.00425
   Li Y, 2022, PROC CVPR IEEE, P5544, DOI 10.1109/CVPR52688.2022.00547
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Loshchilov I, 2019, Arxiv, DOI [arXiv:1711.05101, 10.48550/arXiv.1711.05101, DOI 10.48550/ARXIV.1711.05101]
   Magnusson M, 2007, J FIELD ROBOT, V24, P803, DOI 10.1002/rob.20204
   Mellado N, 2014, COMPUT GRAPH FORUM, V33, P205, DOI 10.1111/cgf.12446
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Qin Z, 2022, PROC CVPR IEEE, P11133, DOI 10.1109/CVPR52688.2022.01086
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Rusu RB, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3384, DOI 10.1109/IROS.2008.4650967
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Salti S, 2014, COMPUT VIS IMAGE UND, V125, P251, DOI 10.1016/j.cviu.2014.04.011
   Shotton J, 2013, PROC CVPR IEEE, P2930, DOI 10.1109/CVPR.2013.377
   Su JL, 2023, Arxiv, DOI arXiv:2104.09864
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wang Yulin, 2019, ADV NEURAL INFORM PR, V32
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xu H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3112, DOI 10.1109/ICCV48922.2021.00312
   Yang JL, 2016, IEEE T PATTERN ANAL, V38, P2241, DOI 10.1109/TPAMI.2015.2513405
   Yew ZJ, 2022, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR52688.2022.00656
   Yu H, 2021, ADV NEUR IN, V34
   Zeng A, 2017, PROC CVPR IEEE, P199, DOI 10.1109/CVPR.2017.29
   Zhu LF, 2022, Arxiv, DOI arXiv:2201.12094
   Zi Jian Yew, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11821, DOI 10.1109/CVPR42600.2020.01184
NR 49
TC 0
Z9 0
U1 2
U2 2
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103896
DI 10.1016/j.cag.2024.103896
EA MAR 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3L2
UT WOS:001205020300001
DA 2024-08-05
ER

PT J
AU Yildiz, T
   Akleman, E
AF Yildiz, Tolga
   Akleman, Ergun
TI Volumetric nonwoven structures: An algebraic framework for systematic
   design of infinite polyhedral frames using nonwoven fabric patterns
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Volumetric nonwoven structures; Shape algebra; Shape modeling and
   design; P-adic numbers
ID CATALOG; NUMBER; WOVEN; SPACE
AB In this paper, we present an algebraic framework that can be used to construct a large class of 3D shapes and structures that can potentially provide unusual material properties. We formalized this framework as a 3D generalization of planar nonwoven textile structures that are used to mimic the woven structures. Our extension is based on the fact that it is straightforward to extend planar nonwoven textile structures into volumetric nonwoven textile structures, which we also call nonwoven volumetric fabrics. This property is essential because such an extension is impossible with planar woven structures. In other words, using this approach, it can be possible to easily produce volumetric structures that mimic the fabric behavior as if they were planar nonwoven textile structures, which is impossible to produce. These volumetric structures also correspond to regular & semiregular frame structures and are capable of representing previously unknown infinite regular polyhedra and flexible wood structures.
C1 [Yildiz, Tolga] Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX 77840 USA.
   [Akleman, Ergun] Texas A&M Univ, Sch Performance Visualizat & Fine Arts, Joint Dept Comp Sci & Engn, Visual Comp & Computat Media Sect, College Stn, TX 77845 USA.
C3 Texas A&M University System; Texas A&M University College Station; Texas
   A&M University System; Texas A&M University College Station
RP Akleman, E (corresponding author), Texas A&M Univ, Sch Performance Visualizat & Fine Arts, Joint Dept Comp Sci & Engn, Visual Comp & Computat Media Sect, College Stn, TX 77845 USA.
EM tolgayildiz@tamu.edu; ergun@tamu.edu
OI Akleman, Ergun/0000-0003-3618-4166
CR Akleman E, 2009, ACM T GRAPHICS, P1
   Akleman E, 2020, SIAM J DISCRETE MATH, V34, P2457, DOI 10.1137/20M1312721
   Akleman E, 2015, DISCRETE APPL MATH, V193, P61, DOI 10.1016/j.dam.2015.04.015
   Akleman E, 2011, COMPUT GRAPH-UK, V35, P623, DOI 10.1016/j.cag.2011.03.003
   Allison KG, 1967, Google Patents, Patent No. [3,338,992, 3338992]
   ARVESON W, 1984, INDIANA U MATH J, V33, P583, DOI 10.1512/iumj.1984.33.33031
   Balogh A, 2015, J PHARM SCI-US, V104, P1767, DOI 10.1002/jps.24399
   Bhandari A, 2023, P-ADIC NUMBERS ULTRA, V15, P104, DOI 10.1134/S2070046623020036
   BREKKE L, 1993, PHYS REP, V233, P1, DOI 10.1016/0370-1573(93)90043-D
   Brewster NH, 1934, Google Patents, Patent No. [1,978,620, 1978620]
   Burt M, 2007, Visual mathematics
   Calderbank A. R., 1995, Designs, Codes and Cryptography, V6, P21, DOI 10.1007/BF01390768
   Caruso X, 2017, Les cours du CIRM, V5, DOI [10.5802/ccirm.25, DOI 10.5802/CCIRM.25]
   CASPER MS, 1975, NONWOVEN TEXTILES
   Chapman RA, 2010, WOODHEAD PUBL TEXT, P1, DOI 10.1533/9781845699741
   Chen Y-L, 2010, Hyperseeing, V6
   Clapham C, 1980, Bull Lond Math Soc, V12, P161
   Clayton EL, 1955, Google Patents, Patent No. [2,704,734, 2704734]
   Cornelissen G, 2005, Notices Amer Math Soc, V52, P720
   Coxeter HSM, 1937, P LOND MATH SOC, V43, P33
   CURTIS PT, 1987, INT J FATIGUE, V9, P67, DOI 10.1016/0142-1123(87)90047-8
   da Silva DWHA, 2020, Ph.D. thesis
   Delaney C, 1984, Ars Combin, V15, P70
   DIXON JD, 1982, NUMER MATH, V40, P137, DOI 10.1007/BF01459082
   Dragovich B, 2009, P-ADIC NUMBERS ULTRA, V1, P1, DOI 10.1134/S2070046609010014
   Dragovich B, 2021, BIOSYSTEMS, V199, DOI 10.1016/j.biosystems.2020.104288
   Dragovich B, 2010, COMPUT J, V53, P432, DOI 10.1093/comjnl/bxm083
   Duffield C, 2024, Arxiv, DOI arXiv:2401.04031
   Dupre EJ, 1960, Google Patents, Patent No. [2,958,593, 2958593]
   Ebert M, 2024, ADV ENG MATER, V26, DOI 10.1002/adem.202301359
   Ebert M, 2024, ADV ENG MATER, V26, DOI 10.1002/adem.202300831
   Enns T, 1984, Geom Dedicata, V15, P259
   Estrin Y, 2021, J MATER RES TECHNOL, V15, P1165, DOI 10.1016/j.jmrt.2021.08.064
   Gambini I, 2012, THEOR COMPUT SCI, V432, P52, DOI 10.1016/j.tcs.2012.01.014
   Garonzi M, 2018, B BRAZ MATH SOC, V49, P515, DOI 10.1007/s00574-018-0068-x
   GILLIES DB, 1964, MATH COMPUT, V18, P93, DOI 10.2307/2003409
   Gillispie SB, 2009, ELECTRON J COMB, V16
   Gouvea FQ, 1997, p-adic Numbers
   Graham B, 1956, Google Patents, Patent No. [2,765,247, 2765247]
   Griswold R, 2004, Web Technical Report
   GRUNBAUM B, 1986, DISCRETE MATH, V60, P155, DOI 10.1016/0012-365X(86)90010-5
   GRUNBAUM B, 1985, ANN NY ACAD SCI, V440, P279, DOI 10.1111/j.1749-6632.1985.tb14560.x
   GRUNBAUM B, 1988, AM MATH MON, V95, P5, DOI 10.2307/2323440
   Grunbaum B, 1993, Geombinatorics, V2, P53
   Grunbaum B, 1980, Math Mag, V53, P139
   Hochberg R, 2000, DISCRETE MATH, V214, P255, DOI 10.1016/S0012-365X(99)00310-6
   Horrocks A.R., 2000, Handbook of technical textiles
   Istrail S, 2009, COMMUN INF SYST, V9, P303
   Khrennikov A, 2016, J FOURIER ANAL APPL, V22, P809, DOI 10.1007/s00041-015-9433-y
   Kitto W. Z., 1991, Applicable Algebra in Engineering, Communication and Computing, V2, P105, DOI 10.1007/BF01810571
   KITTO WZ, 1994, DISCRETE APPL MATH, V52, P39, DOI 10.1016/0166-218X(92)00186-P
   KRISHNAMURTHY EV, 1975, P INDIAN ACAD SCI A, V81, P58, DOI 10.1007/BF03051174
   Krishnamurthy VR, 2022, IEEE T VIS COMPUT GR, V28, P3391, DOI 10.1109/TVCG.2021.3065457
   Kumar S, 2013, INT J ADHES ADHES, V41, P57, DOI 10.1016/j.ijadhadh.2012.09.001
   Mahler K, 1973, Introduction to p-adic numbers and their functions
   Nicholson E., 2015, Finite multiplicative subgroups of fields are cyclic part 1 and 2
   Powel C, 1947, Google Patents, Patent No. [2,417,586, 2417586]
   Purdy A, 1983, Text Prog, V12, P1, DOI [10.1080/00405168308688896, DOI 10.1080/00405168308688896]
   RAMAKRISHNAN R, 1995, J CHEM PHYS, V103, P7592, DOI 10.1063/1.470277
   ROTH RL, 1993, GEOMETRIAE DEDICATA, V48, P191, DOI 10.1007/BF01264067
   Sakata N, 2022, P ROY SOC A-MATH PHY, V478, DOI 10.1098/rspa.2022.0073
   Sheffield S, 2014, ELECTRON J PROBAB, V19, P1, DOI 10.1214/EJP.v19-3073
   Smith J, 2022, Unique Home Decor By Bending Wood At Home! | Woodworking Bend Technique
   Srinivasan V, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P429, DOI 10.1109/PCCGA.2002.1167888
   Wachman A, 1977, Infinite Polyhedra
   Yildiz T, 2023, COMPUT GRAPH-UK, V114, P357, DOI 10.1016/j.cag.2023.06.017
   Zarrinmehr S, 2023, Arxiv, DOI arXiv:2311.05639
   Zarrinmehr S, 2017, COMPUT GRAPH-UK, V66, P93, DOI 10.1016/j.cag.2017.05.010
NR 68
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103979
DI 10.1016/j.cag.2024.103979
EA JUN 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XH6R8
UT WOS:001260836200001
DA 2024-08-05
ER

PT J
AU Zhang, C
   Liang, LY
   Zhou, JJ
   Xu, Y
AF Zhang, Chi
   Liang, Lingyu
   Zhou, Jijun
   Xu, Yong
TI Multi-view depth estimation based on multi-feature aggregation for 3D
   reconstruction
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Multi-view stereo; Depth estimation; 3D reconstruction; Neural radiance
   fields; Deep learning
ID STEREO
AB Robust 3D reconstruction depends critically on the accuracy of depth maps, and multi-view depth estimate is greatly aided by Multi-View Stereo (MVS). Nevertheless, challenges still exist, particularly in accurately estimating depth in regions with low texture and occlusion edges. We introduce MFA-MVSNet, a novel Multi-Feature Aggregation Multi-View Stereo Network that aggregates diverse features in order to address these issues. The Local Feature Extractor (LFE) is incorporated into MFA-MVSNet to adaptively capture local features across various receptive fields. It also incorporates a Multi-feature Fusion Module (MFM) to combine local features with global features extracted by the Global Feature Extractor (GFE). We utilize neural edge information in a CNN-based Depth Refinement Module (DRM) to iteratively filter the depth map to address depth errors in low-textured regions and occlusion edges. To further improve network robustness and training efficiency, we also introduce a Region Perception Loss (RPL) to lessen the impact of both easily matched and mismatched areas. On both the DTU and BlendedMVS datasets, experimental results demonstrate that MFA-MVSNet outperforms recent advanced methods in terms of depth map quality. Additionally, we incorporate MFA-MVSNet into a multi-view 3D reconstruction pipeline, utilizing its depth maps to enhance downstream tasks such as point cloud reconstruction and neural radiance fields (NeRF), leading to improved 3D reconstruction performance.
C1 [Zhang, Chi; Liang, Lingyu; Zhou, Jijun; Xu, Yong] South China Univ Technol, Guangzhou, Peoples R China.
   [Liang, Lingyu; Xu, Yong] Guangdong Artificial Intelligence & Digital Econ L, Pazhou Lab Guangzhou, Guangzhou, Peoples R China.
   [Xu, Yong] Guangdong Prov Key Lab Multimodal Big Data Intelli, Guangzhou, Peoples R China.
C3 South China University of Technology; Pazhou Lab
RP Liang, LY; Xu, Y (corresponding author), South China Univ Technol, Guangzhou, Peoples R China.
EM ee202221012779@mail.scut.edu.cn; lianglysky@gmail.com;
   202321012249@mail.scut.edu.cn; yxu@scut.edu.cn
FU Guangdong Nat-ural Science Foundation [2024A1515012217]; National
   Natural Science Foundation of China [62072188]; National Foreign Expert
   Project of the Ministry of Science and Technology of China
   [G2023163015L]; Guangzhou Sci-ence and Technology Plan Project-Key RD
   Plan [2024B01W0007]
FX Lingyu Liang would like to thank the supports by Guangdong Nat-ural
   Science Foundation (No. 2024A1515012217) . Yong Xu would like to thank
   the supports by National Natural Science Foundation of China (No.
   62072188) , National Foreign Expert Project of the Ministry of Science
   and Technology of China (No. G2023163015L) , Guangzhou Sci-ence and
   Technology Plan Project-Key R&D Plan (No. 2024B01W0007) .
CR Aanæs H, 2016, INT J COMPUT VISION, V120, P153, DOI 10.1007/s11263-016-0902-9
   Cao C, 2023, Trans Mach Learn Res (TMLR)
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LC, 2016, PROC CVPR IEEE, P4545, DOI 10.1109/CVPR.2016.492
   Collins RT, 1996, PROC CVPR IEEE, P358, DOI 10.1109/CVPR.1996.517097
   Darmon F, 2021, INT CONF 3D VISION, P484, DOI 10.1109/3DV53792.2021.00058
   Deng KL, 2022, PROC CVPR IEEE, P12872, DOI 10.1109/CVPR52688.2022.01254
   Ding YK, 2022, PROC CVPR IEEE, P8575, DOI 10.1109/CVPR52688.2022.00839
   Galliani S, 2015, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2015.106
   Giang KT, 2022, Arxiv, DOI arXiv:2112.05999
   Gu XD, 2020, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR42600.2020.00257
   Jiang J, 2024, ICASSP 2024, P3180
   Jiang PF, 2023, COMPUT GRAPH-UK, V116, P128, DOI 10.1016/j.cag.2023.08.014
   Katharopoulos A, 2020, PR MACH LEARN RES, V119
   Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17
   Knapitsch A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073599
   Li BY, 2019, AAAI CONF ARTIF INTE, P8577
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Ma XJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5712, DOI 10.1109/ICCV48922.2021.00568
   Mildenhall B, 2020, P ECCV, DOI DOI 10.1007/978-3-030-58452-8
   Peng R, 2022, PROC CVPR IEEE, P8635, DOI 10.1109/CVPR52688.2022.00845
   Qi XJ, 2022, IEEE T PATTERN ANAL, V44, P969, DOI 10.1109/TPAMI.2020.3020800
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Schönberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31
   Shirley Peter, 2009, Fundamentals of Computer Graphics
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang FJ, 2021, PROC CVPR IEEE, P14189, DOI 10.1109/CVPR46437.2021.01397
   Wang XF, 2022, LECT NOTES COMPUT SC, V13691, P573, DOI 10.1007/978-3-031-19821-2_33
   Wang YD, 2023, COMPUT GRAPH-UK, V116, P383, DOI 10.1016/j.cag.2023.08.017
   Wei Z., 2022, IEEE Trans Visual Comput Grap (TVCG)
   Wei ZZ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6167, DOI 10.1109/ICCV48922.2021.00613
   Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47
   Yao Y, 2020, PROC CVPR IEEE, P1787, DOI 10.1109/CVPR42600.2020.00186
   Yao Y, 2019, PROC CVPR IEEE, P5520, DOI 10.1109/CVPR.2019.00567
   Yu AZ, 2021, ISPRS J PHOTOGRAMM, V175, P448, DOI 10.1016/j.isprsjprs.2021.03.010
   Zbontar J, 2016, J MACH LEARN RES, V17
   Zhang FH, 2019, PROC CVPR IEEE, P185, DOI 10.1109/CVPR.2019.00027
   Zhang J, 2020, BRIT MACH VIS C
   Zhang Z, 2023, PROC CVPR IEEE, P21508, DOI 10.1109/CVPR52729.2023.02060
   Zhu QT, 2021, Arxiv, DOI arXiv:2106.15328
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 42
TC 0
Z9 0
U1 1
U2 1
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103954
DI 10.1016/j.cag.2024.103954
EA JUN 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WZ0Z7
UT WOS:001258591500001
DA 2024-08-05
ER

PT J
AU Simoes, B
   Carretero, MD
   Martínez, J
   Muñoz, S
   Alcain, N
AF Simoes, Bruno
   Carretero, Maria del Puy
   Martinez, Jorge
   Munoz, Sebastian
   Alcain, Nieves
TI Implementing Digital Twins via micro-frontends, micro-services, and web
   3D
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Web 3D; Digital Twins (DTs); Micro-frontends; Industry 4.0;
   Manufacturing industry; Visual programming; Immersive environments
AB The introduction of Digital Twin (DT) technology has sparked interest in virtual replication of physical assets and processes. However, to fully capitalize on DT's potential, we need robust and scalable frontend solutions. This study proposes an innovative approach to overcome the limitations of traditional monolithic frontend frameworks by leveraging micro -frontend technologies. Our goal is to enable seamless connectivity between DTs and develop tailored presentation layers by decomposing complex systems into modular web -based DTs. Additionally, our framework tackles the challenges posed by multi-vendor elements within DTs and facilitates service and virtual environment orchestration for seamless integration and operation, enabling real-time data exchange between virtual models and physical machinery. Insights from its implementation across various applications, including training, monitoring, and control, shed light on its advantages and limitations. This research aims to revolutionize DT creation and management by introducing a scalable and adaptable framework designed to enable Web 3D for Industry and Manufacturing. By integrating cutting-edge micro -frontend technologies, our approach seeks to enhance maintainability, efficiency, and productivity. However, potential limitations such as code duplication and app payload concerns must be acknowledged. Future research will address these limitations to broaden applicability and improve performance. Overall, our proposed framework offers promising solutions to many challenges in industrial settings, paving the way for more efficient and effective DT deployment and management.
C1 [Simoes, Bruno; Carretero, Maria del Puy] Basque Res & Technol Alliance BRTA, Vicomtech Fdn, Mikeletegi 57, San Sebastian 20009, Spain.
   [Martinez, Jorge] Segula Technol Grp, Avda Bruselas 8-oficina 8, Vitoria, Spain.
   [Munoz, Sebastian] Robert Bosch Espana SLU, Calle Hermanos Garcia Noblejas 19, Madrid 28037, Spain.
   [Alcain, Nieves] ALECOP SCoop, Loramendi 11, Arrasate Mondragon 20500, Spain.
RP Simoes, B (corresponding author), Basque Res & Technol Alliance BRTA, Vicomtech Fdn, Mikeletegi 57, San Sebastian 20009, Spain.
EM bsimoes@vicomtech.org
FU Provincial Council of Gipuzkoa [2023-CIEN-000018-04-01]; Basque
   Government [ZL-2022/00394, ZL-2022/00326]; NextGenerationEU funds under
   the Plan de Recuperacion, Transformacion y Resiliencia, Misiones program
   of the CDTI (Center for Technological and Industrial Development) ,
   under the Ministerio de Ciencia e Innovacion (SPAIN) [MIG-2022106]
FX This research has been supported by funding from the Provincial Council
   of Gipuzkoa under the grant 2023-CIEN-000018-04-01 (TWINARK) , and from
   the Basque Government, specifically through grants ZL-2022/00394 (CLEO)
   and ZL-2022/00326 (EGGEDI) . It has also received financial backing from
   the NextGenerationEU funds under the Plan de Recuperacion,
   Transformacion y Resiliencia, as part of the Misiones program of the
   CDTI (Center for Technological and Industrial Development) , under the
   Ministerio de Ciencia e Innovacion (SPAIN) for the QCDI project
   (MIG-2022106) .
CR [Anonymous], 2024, Node-RED: low-code programming for event-driven applications
   Bader Sebastian R., 2019, Semantic Systems. The Power of AI and Knowledge Graphs. 15th International Conference, SEMANTiCS 2019. Proceedings: Lecture Notes in Computer Science (LNCS 11702), P159, DOI 10.1007/978-3-030-33220-4_12
   Badii Claudio, 2020, 2020 International Conferences on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics (Cybermatics), P54, DOI 10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics50389.2020.00028
   Bosch J, 2016, IEEE SOFTWARE, V33, P82, DOI 10.1109/MS.2016.14
   Duan HH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P153, DOI 10.1145/3474085.3479238
   Far S. B., 2022, Journal of Metaverse, V2, P8
   Ferencz K, 2019, 35 JUB KAND K, P52
   Grieves M., 2015, White Paper
   Gröger C, 2013, PROC CIRP, V7, P664, DOI 10.1016/j.procir.2013.06.050
   Hasan HR, 2020, IEEE ACCESS, V8, P34113, DOI 10.1109/ACCESS.2020.2974810
   Jadhav Madhuri A, 2015, Int J Comput Sci Inf Technol, V6, P2876
   Kulvatunyou B, 2016, J COMPUT INF SCI ENG, V16, DOI 10.1115/1.4033725
   Lin XQ, 2023, IEEE COMMUN MAG, V61, P72, DOI 10.1109/MCOM.001.2200830
   Lv Z, 2022, Virtual Reality & Intelligent Hardware, V4, P459
   Michael J, 2022, 10TH IEEE/ACM INTERNATIONAL WORKSHOP ON SOFTWARE ENGINEERING FOR SYSTEMS-OF-SYSTEMS AND SOFTWARE ECOSYSTEMS (SESOS 2022), P9, DOI 10.1145/3528229.3529384
   Miclaus A, 2016, P 7 INT WORKSH WEB T, P17
   Negri E, 2017, PROCEDIA MANUF, V11, P939, DOI 10.1016/j.promfg.2017.07.198
   Pavlenko A, 2020, J. Internet Serv. Inf. Secur., V10, P49
   Peltonen S, 2021, INFORM SOFTWARE TECH, V136, DOI 10.1016/j.infsof.2021.106571
   Sanchit, 2018, INT J RECENT RES ASP, V5, P133
   Simoes Bruno, 2011, Proceedings of the 2011 International Conference on Internet Computing, P176
   Simoes B, 2023, P 28 INT ACM C 3D WE, P1
   Sumak B, 2011, COMPUT HUM BEHAV, V27, P2067, DOI 10.1016/j.chb.2011.08.005
   Tabaa M, 2018, PROCEDIA COMPUT SCI, V130, P583, DOI 10.1016/j.procs.2018.04.107
   van Schalkwyk Pieter, 2022, Digital twin capabilities periodic table: A digital twin consortium user guide
   Wei K, 2019, IN C IND ENG ENG MAN, P1460, DOI [10.1109/ieem44572.2019.8978536, 10.1109/IEEM44572.2019.8978536]
NR 26
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD JUN
PY 2024
VL 121
AR 103946
DI 10.1016/j.cag.2024.103946
EA MAY 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UM5R0
UT WOS:001248495100001
DA 2024-08-05
ER

PT J
AU Leonard, L
   Westermann, R
AF Leonard, Ludwic
   Westermann, Ruediger
TI Image-based reconstruction of heterogeneous media in the presence of
   multiple light-scattering
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Differentiable rendering; Inverse rendering; Volume reconstruction
ID ALGEBRAIC RECONSTRUCTION; ALGORITHM
AB Image -based reconstruction of a three-dimensional heterogeneous density field in the presence of multiple scattering of light is intrinsically under -constrained. This leads to reconstructions that look similar to the ground truth when rendered, but the recovered field is often far off the real one. We shed light on the sources of uncertainty in the reconstruction process which are responsible for this ambiguity, and propose the following approaches to improve the reconstruction quality: Firstly, we introduce a new path sampling strategy, which yields more accurate estimates of the gradients of the extinction field. Secondly, we build upon the observation that the variance in the loss computation is one source of bias in the optimization process. To reduce this variance in the primal estimator, we propose exploiting temporal coherence by reusing previously rendered images. All this is coupled with a constraint on spatial object occupancy, which restricts the problem to reconstructed shape prior. In a number of examples we demonstrate that compared to existing approaches the proposed reconstruction pipeline leads to improved accuracy of the reconstructed density fields.
C1 [Leonard, Ludwic; Westermann, Ruediger] Tech Univ Munich, Munich, Germany.
C3 Technical University of Munich
RP Leonard, L (corresponding author), Tech Univ Munich, Munich, Germany.
EM ludwig.mendez@tum.de; westermann@tum.de
RI Leonard, Ludwig/KVA-8452-2024
OI Leonard, Ludwig/0000-0002-6111-4031
CR ANDERSEN AH, 1984, ULTRASONIC IMAGING, V6, P81, DOI 10.1016/0161-7346(84)90008-7
   ANDERSEN AH, 1989, IEEE T MED IMAGING, V8, P50, DOI 10.1109/42.20361
   Azinovic D, 2019, PROC CVPR IEEE, P2442, DOI 10.1109/CVPR.2019.00255
   Che CQ, 2018, Arxiv, DOI arXiv:1809.10820
   Donner C, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409093
   FELDKAMP LA, 1984, J OPT SOC AM A, V1, P612, DOI 10.1364/JOSAA.1.000612
   Franz E, 2021, PROC CVPR IEEE, P1632, DOI 10.1109/CVPR46437.2021.00168
   Fridovich-Keil S, 2022, PROC CVPR IEEE, P5491, DOI 10.1109/CVPR52688.2022.00542
   Gao KY, 2023, Arxiv, DOI arXiv:2210.00379
   Gkioulekas I, 2016, LECT NOTES COMPUT SC, V9907, P685, DOI 10.1007/978-3-319-46487-9_42
   Gkioulekas I, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508377
   GORDON R, 1974, IEEE T NUCL SCI, VNS21, P78, DOI 10.1109/TNS.1974.6499238
   Hasselgren J, 2022, Arxiv, DOI arXiv:2206.03380
   Huang D, 2008, J GEOPHYS RES-ATMOS, V113, DOI 10.1029/2007JD009133
   HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732
   Kajiya J. T., 1984, Computers & Graphics, V18, P165
   Kajiya J. T., 1986, Computer Graphics, V20, P143, DOI 10.1145/15886.15902
   Kak A. C., 1988, Principles of computerized tomographic imaging, DOI 10.1118/1.1455742
   Karimi D, 2016, BIOMED PHYS ENG EXPR, V2, DOI 10.1088/2057-1976/2/1/015008
   Karis B, 2014, Adv Real-Time Render Games, SIGGRAPH Courses, V1
   Katsevich A, 2004, ADV APPL MATH, V32, P681, DOI 10.1016/S0196-8858(03)00099-X
   Katsevich A, 2002, SIAM J APPL MATH, V62, P2012, DOI 10.1137/S0036139901387186
   Levis A, 2015, IEEE I CONF COMP VIS, P3379, DOI 10.1109/ICCV.2015.386
   Lombardi S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323020
   Lu Y, 2021, COMPUT GRAPH FORUM, V40, P135, DOI 10.1111/cgf.14295
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Natterer F., 2001, Classics in Applied Mathematics
   Ng A.Y., 2004, P 21 INT C MACH LEAR, P78, DOI DOI 10.1145/1015330.1015435
   Nimier-David M, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530073
   Nimier-David M, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392406
   Nimier-David M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356498
   Nuiachristos V, 2006, ANNU REV BIOMED ENG, V8, P1, DOI 10.1146/annurev.bioeng.8.061505.095831
   Panin VY, 1999, IEEE T NUCL SCI, V46, P2202, DOI 10.1109/23.819305
   Pharr M., 2016, Physically based rendering: From theory to implementation
   Raab M, 2008, MONTE CARLO AND QUASI-MONTE CARLO METHODS 2006, P591, DOI 10.1007/978-3-540-74496-2_35
   Rudin L. I., 1994, Proceedings ICIP-94 (Cat. No.94CH35708), P31, DOI 10.1109/ICIP.1994.413269
   Schied C, 2017, HPG '17: PROCEEDINGS OF HIGH PERFORMANCE GRAPHICS, DOI 10.1145/3105762.3105770
   Sitzmann V., 2019, Advances in neural information processing systems, P32
   Sitzmann V, 2019, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2019.00254
   Stuker Florian, 2011, Pharmaceutics, V3, P229, DOI 10.3390/pharmaceutics3020229
   Tang JQ, 2020, IEEE T COMPUT IMAG, V6, P1471, DOI 10.1109/TCI.2020.3032101
   Tong JG, 2023, Arxiv, DOI arXiv:2303.13805
   Veach E., 1995, Photorealistic Rendering Techniques, P145
   Vicini D, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459804
   Weiss S, 2022, COMPUT GRAPH FORUM, V41, P196, DOI 10.1111/cgf.14578
   Weiss S, 2022, IEEE T VIS COMPUT GR, V28, P562, DOI 10.1109/TVCG.2021.3114769
   Woodcock E., 1965, P C APPL COMP METH R, P557
   Yu A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5732, DOI 10.1109/ICCV48922.2021.00570
   Zeltner T, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459807
   Zhang C, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459782
   Zhang C, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356522
   Zhang K, 2020, Arxiv, DOI arXiv:2010.07492
   Zhang YJ, 2023, Arxiv, DOI arXiv:2304.00782
   Zhu C., 2022, P ADV NEUR INF PROC, V35, P38994
NR 54
TC 0
Z9 0
U1 1
U2 1
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103877
DI 10.1016/j.cag.2024.01.004
EA FEB 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LK1X0
UT WOS:001186612300001
OA hybrid
DA 2024-08-05
ER

PT J
AU Hu, KD
   Xie, ZX
   Hu, QH
AF Hu, Kaidi
   Xie, Zongxia
   Hu, Qinghua
TI Dual-resolution transformer combined with multi-layer separable
   convolution fusion network for real-time semantic segmentation
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Lightweight neural network; Transformer; Multi-scale feature fusion;
   Real-time semantic segmentation; Environmental perception
AB Environmental perception is crucial for unmanned mobile platforms such as autonomous vehicles and robots. Precise and fast semantic segmentation of the surrounding scene is a key task to enhance this capability. Existing real-time semantic segmentation networks are typically based on convolutional neural networks (CNNs), which have achieved good results, but they still lack control over global context features. In recent years, the Transformer architecture has achieved significant success in capturing global context, which is beneficial for improving segmentation accuracy. However, Transformers tend to ignore local connections, and their computational complexity makes real-time segmentation challenging. We propose a lightweight real-time semantic segmentation network called DTMC-Net, which combines the advantages of CNNs and Transformers. We design a special residual convolution module called the Lightweight Multi -layer Separable Convolution Attention module (LMSCA) to reduce the parameter count and perform multi -scale feature fusion to capture local features effectively. We introduce the Simple Dual -Resolution Transformer (SDR Transformer) that utilizes lightweight attention mechanisms and residual feed forward networks to capture and maintain features, with multiple bilateral fusions between two branches to exchange information. The proposed Antiartifact Aggregation Pyramid Pooling Module (AAPPM) optimizes the upsampling process, refines features, and performs multi -scale feature fusion again. DTMC-Net only contains 4.2M parameters and achieves good performance on multiple public datasets with different scenarios.
C1 [Hu, Kaidi; Xie, Zongxia; Hu, Qinghua] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
C3 Tianjin University
RP Hu, QH (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
EM kaidihu@tju.edu.cn; huqinghua@tju.edu.cn
RI Hu, Qinghua/GWM-6136-2022
FU National Natural Science Foundation of China [61925602]
FX This research was funded by National Natural Science Foundation of China
   under Grant 61925602.
CR Arsalan M, 2019, EXPERT SYST APPL, V122, P217, DOI 10.1016/j.eswa.2019.01.010
   Taghanaki SA, 2021, ARTIF INTELL REV, V54, P137, DOI 10.1007/s10462-020-09854-1
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005
   Chen L.-C., 2018, ECCV, P801, DOI [DOI 10.1007/978-3-030-01234-249, 10.1007/978-3-030-01234-2_49]
   Chen LC, 2016, Arxiv, DOI [arXiv:1412.7062, DOI 10.48550/ARXIV.1412.7062, 10.48550/ARXIV.1412.7062]
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng JR, 2022, INT J INTELL SYST, V37, P5617, DOI 10.1002/int.22804
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai YP, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15102649
   Dai YP, 2022, INT J CONTROL AUTOM, V20, P2702, DOI 10.1007/s12555-021-0430-4
   Dong Bo, 2023, arXiv
   Dong GS, 2021, IEEE T INTELL TRANSP, V22, P3258, DOI 10.1109/TITS.2020.2980426
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Elhassan MAM, 2021, EXPERT SYST APPL, V183, DOI 10.1016/j.eswa.2021.115090
   Fan JQ, 2024, IEEE T INTELL TRANSP, V25, P1586, DOI 10.1109/TITS.2023.3313982
   Fan JQ, 2023, IEEE T INTELL VEHICL, V8, P756, DOI 10.1109/TIV.2022.3176860
   Fan ZY, 2023, APPL SCI-BASEL, V13, DOI 10.3390/app13031493
   Feng D, 2021, IEEE T INTELL TRANSP, V22, P1341, DOI 10.1109/TITS.2020.2972974
   Ferianc M, 2021, LECT NOTES COMPUT SC, V12893, P483, DOI 10.1007/978-3-030-86365-4_39
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gao GW, 2023, IEEE T MULTIMEDIA, V25, P3273, DOI 10.1109/TMM.2022.3157995
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Gould S, 2009, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2009.5459211
   Guo MH, 2023, IEEE T PATTERN ANAL, V45, P5436, DOI 10.1109/TPAMI.2022.3211006
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu XG, 2022, COMPUT GRAPH-UK, V109, P55, DOI 10.1016/j.cag.2022.10.002
   Hu XG, 2022, APPL INTELL, V52, P580, DOI 10.1007/s10489-021-02446-8
   Jégou S, 2017, IEEE COMPUT SOC CONF, P1175, DOI 10.1109/CVPRW.2017.156
   Jing LL, 2020, IEEE T IMAGE PROCESS, V29, P225, DOI 10.1109/TIP.2019.2926748
   Kirillov A, 2023, Arxiv, DOI arXiv:2304.02643
   Kreso I, 2016, LECT NOTES COMPUT SC, V9796, P64, DOI 10.1007/978-3-319-45886-1_6
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li HC, 2019, PROC CVPR IEEE, P9514, DOI 10.1109/CVPR.2019.00975
   Li YQ, 2022, NEURAL PROCESS LETT, V54, P4647, DOI 10.1007/s11063-022-10740-w
   Lin X, 2023, IEEE T MED IMAGING, V42, P2325, DOI 10.1109/TMI.2023.3247814
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu K, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23146382
   Lv QX, 2022, IEEE T INTELL TRANSP, V23, P4432, DOI 10.1109/TITS.2020.3044672
   Neupane B, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13040808
   Odena A., 2016, Distill, DOI [10.23915/distill.00003.-URL, 10.23915/distill.00003, DOI 10.23915/DISTILL.00003]
   Orsic M, 2019, PROC CVPR IEEE, P12599, DOI 10.1109/CVPR.2019.01289
   Pan HH, 2023, IEEE T INTELL TRANSP, V24, P3448, DOI 10.1109/TITS.2022.3228042
   Poudel R. P. K., 2019, arXiv
   Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196
   Romera E, 2018, IEEE T INTELL TRANSP, V19, P263, DOI 10.1109/TITS.2017.2750080
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shi M, 2023, IEEE T NEUR NET LEAR, V34, P3205, DOI 10.1109/TNNLS.2022.3176493
   Tang XY, 2021, INFORM SCIENCES, V565, P326, DOI 10.1016/j.ins.2021.02.004
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Junke, 2022, Advances in Neural Information Processing Systems
   Wang LB, 2022, ISPRS J PHOTOGRAMM, V190, P196, DOI 10.1016/j.isprsjprs.2022.06.008
   Wang W, 2018, arXiv
   Wu TY, 2021, IEEE T IMAGE PROCESS, V30, P1169, DOI 10.1109/TIP.2020.3042065
   Wu Y, 2022, APPL INTELL, V52, P3319, DOI 10.1007/s10489-021-02603-z
   Xiangtai Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P775, DOI 10.1007/978-3-030-58452-8_45
   Xie EZ, 2021, ADV NEUR IN, V34
   Xu GA, 2023, IEEE T INTELL TRANSP, V24, P15897, DOI 10.1109/TITS.2023.3248089
   Xu JC, 2023, PROC CVPR IEEE, P19529, DOI 10.1109/CVPR52729.2023.01871
   Yang LX, 2021, PR MACH LEARN RES, V139
   Yang W, 2014, PROC CVPR IEEE, P3182, DOI 10.1109/CVPR.2014.407
   Yu CQ, 2021, INT J COMPUT VISION, V129, P3051, DOI 10.1007/s11263-021-01515-2
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu Wang, 2019, Pattern Recognition and Computer Vision. Second Chinese Conference, PRCV 2019. Proceedings. Lecture Notes in Computer Science (LNCS 11858), P41, DOI 10.1007/978-3-030-31723-2_4
   Zhang WQ, 2022, PROC CVPR IEEE, P12073, DOI 10.1109/CVPR52688.2022.01177
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
NR 69
TC 2
Z9 2
U1 9
U2 9
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD FEB
PY 2024
VL 118
BP 220
EP 232
DI 10.1016/j.cag.2023.12.015
EA JAN 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC4W2
UT WOS:001164120900001
DA 2024-08-05
ER

PT J
AU Asiler, M
   Sahillioglu, Y
AF Asiler, Merve
   Sahillioglu, Yusuf
TI 3D geometric kernel computation in polygon mesh structures☆
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE 3D kernel; 3D visibility; Mesh guidance; Star-shape
ID ALGORITHM
AB This paper introduces a novel approach to compute the geometric kernel of a polygon mesh embedded in 3D. The geometric kernel defines the set of points inside or on the shape's boundary, ensuring visibility of the entire shape. The proposed method utilizes scattered rays to identify a sufficient number of sample points on the kernel surface and subsequently leverages these points to locate as many surface vertices as possible. By computing the convex hull of these identified points, we derive an approximation of the kernel. Notably, the output of our method consists exclusively of interior or boundary points of the actual kernel. Comparative evaluations against established CGAL and Polyhedron Kernel algorithms highlight our method's superior computational speed and high approximation accuracy. The parametric structure of our solution allows for different levels of accuracy to be obtained, enabling the user to tailor the approximation to their specific needs. This property sets our algorithm apart from others and provides greater flexibility in its use. Additionally, adjusting the algorithmic settings also enables the computation of the kernel itself with a tradeoff in computational speed. Furthermore, our algorithm swiftly and accurately identifies an empty kernel for non-star-shaped configurations.
C1 [Asiler, Merve; Sahillioglu, Yusuf] Middle East Tech Univ, Comp Engn Dept, TR-06800 Ankara, Turkiye.
C3 Middle East Technical University
RP Asiler, M (corresponding author), Middle East Tech Univ, Comp Engn Dept, TR-06800 Ankara, Turkiye.
EM asiler@ceng.metu.edu.tr; ys@ceng.metu.edu.tr
FU TUBITAK (The Scientific and Techno-logical Research Council of Turkey)
   [EEEAG-119E572]
FX This work was supported by TUBITAK (The Scientific and Techno-logical
   Research Council of Turkey) under the project EEEAG-119E572.
CR Acar UA, 2008, LECT NOTES COMPUT SC, V5193, P29, DOI 10.1007/978-3-540-87744-8_3
   Amit Y, 2010, INT J COMPUT GEOM AP, V20, P601, DOI 10.1142/S0218195910003451
   AVIS D, 1981, PATTERN RECOGN, V13, P395, DOI 10.1016/0031-3203(81)90002-9
   Banerjee D, 2021, Internat J Comput Geom Appl, V31, P123
   Bose P, 1997, COMP GEOM-THEOR APPL, V7, P173, DOI 10.1016/0925-7721(95)00034-8
   CGAL Editorial Board, 2022, The computational geometry algorithms library
   Chen XB, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531379
   Chun S, 2008, World Acad Sci Eng Technol, V2, P2603
   De Berg M., 2000, Computational geometry, DOI DOI 10.1007/978-3-662-03427-9
   Elber G, 2006, Int J Shape Model, V12, P129
   Etzion M, 1997, IEEE T VIS COMPUT GR, V3, P87, DOI 10.1109/2945.582388
   Floater MS, 2005, COMPUT AIDED GEOM D, V22, P623, DOI 10.1016/j.cagd.2005.06.004
   Gewali L., 2020, INT C SYST ENG, P151
   Ghosh SK, 2010, DISCRETE APPL MATH, V158, P718, DOI 10.1016/j.dam.2009.12.004
   Gomez F, 2002, J Math Model Algorithms, V1, P57
   Hiraki T, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P583, DOI 10.1109/VRW52623.2021.00174
   Hong QY, 2022, COMPUT AIDED GEOM D, V94, DOI 10.1016/j.cagd.2022.102075
   Jacobson A, 2017, SA'17: SIGGRAPH ASIA 2017 COURSES, DOI 10.1145/3134472.3134497
   KEIL JM, 1985, SIAM J COMPUT, V14, P799, DOI 10.1137/0214056
   KENT JR, 1992, COMP GRAPH, V26, P47, DOI 10.1145/142920.134007
   Kroller A, 2012, J Exp Algorithmics, V17
   LEE DT, 1979, J ACM, V26, P415, DOI 10.1145/322139.322142
   LEE DT, 1986, IEEE T INFORM THEORY, V32, P276, DOI 10.1109/TIT.1986.1057165
   Natarajan S, 2017, ENG ANAL BOUND ELEM, V80, P218, DOI 10.1016/j.enganabound.2017.03.007
   Ooi ET, 2020, COMPUT MECH, V66, P27, DOI 10.1007/s00466-020-01839-9
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Preparata F. P., 1979, Theoretical Computer Science, V8, P45, DOI 10.1016/0304-3975(79)90055-0
   Schvartzman SC, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778817
   Shamos MI, 1976, 17 ANN S FDN COMP SC, P208
   SHAPIRA M, 1995, IEEE COMPUT GRAPH, V15, P44, DOI 10.1109/38.365005
   Sorgente T, 2023, COMPUT GRAPH FORUM, V42, P461, DOI 10.1111/cgf.14779
   Sorgente T, 2022, COMPUT MATH APPL, V114, P151, DOI 10.1016/j.camwa.2022.03.042
   Sorgente T, 2022, ADV COMPUT MATH, V48, DOI 10.1007/s10444-021-09913-3
   Sorgente T., 2022, A geometric approach for computing the kernel of a polyhedron
   Sorgente T, 2023, CALCOLO, V60, DOI 10.1007/s10092-023-00517-5
   Sorgente T, 2022, COMPUT GRAPH-UK, V105, P94, DOI 10.1016/j.cag.2022.05.001
   Stein A, 2012, COMPUT GRAPH-UK, V36, P265, DOI 10.1016/j.cag.2012.02.012
   Subedi B., 2019, Ph.D. thesis
   Wong SK, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461951
   Worman C, 2007, INT J COMPUT GEOM AP, V17, P105, DOI 10.1142/S0218195907002264
   Yu W, 2011, COMPUT GRAPH FORUM, V30, P2087, DOI 10.1111/j.1467-8659.2011.02056.x
   Yu WY, 2013, PROCEEDINGS OF THE 2013 8TH INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE & EDUCATION (ICCSE 2013), P1023
NR 42
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103951
DI 10.1016/j.cag.2024.103951
EA JUN 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WD8R9
UT WOS:001253028700001
DA 2024-08-05
ER

PT J
AU Xie, YZ
   Xie, XN
   Wang, YR
   Zhang, YC
   Lv, ZJ
AF Xie, Yizhou
   Xie, Xiangning
   Wang, Yuran
   Zhang, Yanci
   Lv, Zejun
TI Terrain point cloud inpainting via signal decomposition
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Point cloud inpainting; B-spline surface fitting; Image inpainting
AB The rapid development of 3D acquisition technology has made it possible to obtain point clouds of realworld terrains. However, due to limitations in sensor acquisition technology or specific requirements, point clouds often contain defects such as holes with missing data. Inpainting algorithms are widely used to patch these holes. However, existing traditional inpainting algorithms rely on precise hole boundaries, which limits their ability to handle cases where the boundaries are not well-defined. On the other hand, learning -based completion methods often prioritize reconstructing the entire point cloud instead of solely focusing on hole filling. Based on the fact that real -world terrain exhibits both global smoothness and rich local detail, we propose a novel representation for terrain point clouds. This representation can help to repair the holes without clear boundaries. Specifically, it decomposes terrains into low -frequency and high -frequency components, which are represented by B -spline surfaces and relative height maps respectively. In this way, the terrain point cloud inpainting problem is transformed into a B -spline surface fitting and 2D image inpainting problem. By solving the two problems, the highly complex and irregular holes on the terrain point clouds can be well -filled, which not only satisfies the global terrain undulation but also exhibits rich geometric details. The experimental results also demonstrate the effectiveness of our method.
C1 [Xie, Yizhou; Xie, Xiangning; Wang, Yuran; Zhang, Yanci; Lv, Zejun] Sichuan Univ, Coll Comp Sci, Chengdu 610065, Peoples R China.
C3 Sichuan University
RP Zhang, YC (corresponding author), Sichuan Univ, Coll Comp Sci, Chengdu 610065, Peoples R China.
EM yzxie@stu.scu.edu.cn; xnxie@stu.scu.edu.cn; wangyuran@stu.scu.edu.cn;
   yczhang@scu.edu.cn; lvzj186@163.com
OI Zhang, Yanci/0000-0001-7045-185X
FU National Key Project of China [GJXM92579]; Sichuan Science and
   Technology Program [2023YFG0122]
FX This work was supported by the National Key Project of China (Project
   Number GJXM92579) and Sichuan Science and Technology Program (Project
   Number 2023YFG0122) .
CR Alliegro A, 2021, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR46437.2021.00460
   Altantsetseg E, 2017, CGI'17: PROCEEDINGS OF THE COMPUTER GRAPHICS INTERNATIONAL CONFERENCE, DOI 10.1145/3095140.3095150
   Attene M, 2010, VISUAL COMPUT, V26, P1393, DOI 10.1007/s00371-010-0416-3
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Brujic D, 2011, INT J ADV MANUF TECH, V54, P691, DOI 10.1007/s00170-010-2947-1
   Cheng M, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3105551
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dinesh C, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Ding YH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3955, DOI 10.1109/ICCV48922.2021.00394
   Doria D., 2012, 2012 IEEE COMP SOC C, P65
   Fei B, 2022, IEEE T INTELL TRANSP, V23, P22862, DOI 10.1109/TITS.2022.3195555
   Fu ZQ, 2021, IEEE T MULTIMEDIA, V23, P3022, DOI 10.1109/TMM.2021.3068606
   Fu ZQ, 2018, IEEE IMAGE PROC, P2137, DOI 10.1109/ICIP.2018.8451550
   HALTON JH, 1964, COMMUN ACM, V7, P701, DOI 10.1145/355588.365104
   Hu W, 2019, IEEE T IMAGE PROCESS, V28, P4087, DOI 10.1109/TIP.2019.2906554
   Huang Y, 2023, VISUAL COMPUT, V39, P723, DOI 10.1007/s00371-021-02370-5
   jianshankeji, 2024, About us
   Ju M, 2019, PROCEEDINGS OF CHINESE CHI 2019: SEVENTH INTERNATIONAL SYMPOSIUM OF CHINESE CHI (CHINESE CHI 2019), P105, DOI 10.1145/3332169.3332175
   Khaloo A, 2017, ADV ENG INFORM, V34, P1, DOI 10.1016/j.aei.2017.07.002
   Lin F., 2023, ICCV, P14595
   Ma W, 1998, INT J ADV MANUF TECH, V14, P918, DOI 10.1007/BF01179082
   Martìnez AM, 2001, IEEE T PATTERN ANAL, V23, P228, DOI 10.1109/34.908974
   Mingqiang Wei, 2010, 2010 Proceedings of International Conference on Artificial Intelligence and Computational Intelligence (AICI 2010), P306, DOI 10.1109/AICI.2010.302
   nationalheraldindia, 2024, ABOUT US
   Sahay P, 2015, IEEE COMPUT SOC CONF
   Shi Y, 2022, MULTIMEDIA SYST, V28, P521, DOI 10.1007/s00530-021-00856-9
   Tabib RA, 2023, P IEEE CVF INT C COM, P1603
   Tchapmi LP, 2019, PROC CVPR IEEE, P383, DOI 10.1109/CVPR.2019.00047
   Wang F, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13173512
   Wang H., 2002, P 5 INT C CURV SURF, P397
   Wang WP, 2006, ACM T GRAPHIC, V25, P214, DOI 10.1145/1138450.1138453
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Chang AX, 2015, Arxiv, DOI [arXiv:1512.03012, DOI 10.48550/ARXIV.1512.03012]
   Xiang P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5479, DOI 10.1109/ICCV48922.2021.00545
   Yida Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P70, DOI 10.1007/978-3-030-58580-8_5
   Yu WH, 2021, INT J GEOGR INF SCI, V35, P273, DOI 10.1080/13658816.2020.1772479
   Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088
   Zhang YN, 2021, AAAI CONF ARTIF INTE, V35, P3430, DOI 10.1609/aaai.v35i4.16456
NR 38
TC 1
Z9 1
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD MAY
PY 2024
VL 120
AR 103915
DI 10.1016/j.cag.2024.103915
EA APR 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PV6Q5
UT WOS:001216901500001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Wilson, E
   Shic, F
   Jörg, S
   Jain, E
AF Wilson, Ethan
   Shic, Frederick
   Joerg, Sophie
   Jain, Eakta
TI Towards mitigating uncann(eye)ness in face swaps via gaze-centric loss
   terms
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Face swapping; Gaze estimation; Perception
AB Advances in face swapping have enabled the automatic generation of highly realistic faces. Yet face swaps are perceived differently than when looking at real faces, with key differences in viewer behavior surrounding the eyes. Face swapping algorithms generally place no emphasis on the eyes, relying on pixel or feature matching losses that consider the entire face to guide the training process. We further investigate viewer perception of face swaps, focusing our analysis on the presence of an uncanny valley effect. We additionally propose a novel loss equation for the training of face swapping models, leveraging a pretrained gaze estimation network to directly improve representation of the eyes. We confirm that viewed face swaps do elicit uncanny responses from viewers. Our proposed improvements significant reduce viewing angle errors between face swaps and their source material. Our method additionally reduces the prevalence of the eyes as a deciding factor when viewers perform deepfake detection tasks. Our findings have implications on face swapping for special effects, as digital avatars, as privacy mechanisms, and more; negative responses from users could limit effectiveness in said applications. Our gaze improvements are a first step towards alleviating negative viewer perceptions via a targeted approach.
C1 [Wilson, Ethan; Jain, Eakta] Univ Florida, Comp & Informat Sci & Engn Dept, Gainesville, FL 32611 USA.
   [Shic, Frederick] Univ Washington, Washington, DC USA.
   [Joerg, Sophie] Univ Bamberg, Bamberg, Germany.
C3 State University System of Florida; University of Florida; University of
   Washington; Otto Friedrich University Bamberg
RP Wilson, E (corresponding author), Univ Florida, Comp & Informat Sci & Engn Dept, Gainesville, FL 32611 USA.
EM ethanwilson@ufl.edu; fshic@uw.edu; sophie.joerg@uni-bamberg.de;
   ejain@ufl.edu
OI Wilson, Ethan/0000-0003-0944-2641
FU NIH R21 Protecting the Privacy of the Child Through Facial Identity
   Removal in Recorded Behavioral Observation Sessions [R21MH123997];
   University of Florida Graduate School Preeminence Award (GSPA), USA;
   Generation NEXT scholarship [1833908]
FX All authors acknowledge funding from NIH R21 Protecting the Privacy of
   the Child Through Facial Identity Removal in Recorded Behavioral
   Observation Sessions (Award R21MH123997) . Ethan Wilson acknowledges
   funding from the University of Florida Graduate School Preeminence Award
   (GSPA), USA and Generation NEXT scholarship (Award 1833908) .
CR Abdelrahman Ahmed A., 2023, 2023 8th International Conference on Frontiers of Signal Processing (ICFSP), P98, DOI 10.1109/ICFSP59764.2023.10372944
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Caporusso N., 2021, International Conference on Applied Human Factors and Ergonomics, DOI 10.1007/978-3-030-51328-3_33
   Carrigan Emma, 2020, MIG '20: Motion, Interaction and Games, DOI 10.1145/3424636.3426904
   Carter E.J., 2013, Proceedings of the ACM Symposium on Applied Perception, P35, DOI [10.1145/2492494.2502059, DOI 10.1145/2492494.2502059]
   Chen RW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2003, DOI 10.1145/3394171.3413630
   Ciftci UA, 2020, IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2020)
   Ciftci Umur Aybars, 2020, IEEE Trans Pattern Anal Mach Intell, VPP, DOI 10.1109/TPAMI.2020.3009287
   Dana Kristin, 2017, MULTISTYLE GENERATIV
   Dang M, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12163407
   Demir I, 2021, PROCEEDINGS ETRA 2021: ACM SYMPOSIUM ON EYE TRACKING RESEARCH AND APPLICATIONS, DOI 10.1145/3448017.3457387
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Dill Vanderson, 2012, Intelligent Virtual Agents. Proceedings 12th International Conference, IVA 2012, P511, DOI 10.1007/978-3-642-33197-8_62
   Gafni O, 2019, IEEE I CONF COMP VIS, P9377, DOI 10.1109/ICCV.2019.00947
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Geller T, 2008, IEEE COMPUT GRAPH, V28, P11, DOI 10.1109/MCG.2008.79
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Groh M, 2024, Arxiv, DOI [arXiv:2202.12883, DOI 10.48550/ARXIV.2202.12883, 10.48550/arXiv.2202.12883]
   Groh M, 2022, P NATL ACAD SCI USA, V119, DOI 10.1073/pnas.2110013119
   Gupta Parul, 2020, ICMI '20: Proceedings of the 2020 International Conference on Multimodal Interaction, P519, DOI 10.1145/3382507.3418857
   Ho CC, 2017, INT J SOC ROBOT, V9, P129, DOI 10.1007/s12369-016-0380-9
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Hodgins J, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1823738.1823740
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   JANIK SW, 1978, PERCEPT MOTOR SKILL, V47, P857, DOI 10.2466/pms.1978.47.3.857
   Jung T, 2020, IEEE ACCESS, V8, P83144, DOI 10.1109/ACCESS.2020.2988660
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Kätsyri J, 2019, PERCEPTION, V48, P968, DOI 10.1177/0301006619869134
   Korshunova I, 2017, IEEE I CONF COMP VIS, P3697, DOI 10.1109/ICCV.2017.397
   Kuang ZZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3182, DOI 10.1145/3474085.3475464
   Lee S, 2021, 23RD INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS AND ACCESSIBILITY, ASSETS 2021, DOI 10.1145/3441852.3471200
   Li LZ, 2020, PROC CVPR IEEE, P5073, DOI 10.1109/CVPR42600.2020.00512
   Li T, 2019, IEEE COMPUT SOC CONF, P56, DOI 10.1109/CVPRW.2019.00013
   Li YX, 2023, PROC CVPR IEEE, P12705, DOI 10.1109/CVPR52729.2023.01222
   Li YZ, 2018, IEEE INT WORKS INFOR
   Liu KL, 2023, PATTERN RECOGN, V141, DOI 10.1016/j.patcog.2023.109628
   Liu MY, 2016, ADV NEUR IN, V29
   Liu MC, 2021, ADV NEUR IN, V34
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   LYU S, 2020, IEEE INT CONF MULTI, pNI249, DOI DOI 10.1109/icmew46912.2020.9105991
   MacDorman KF, 2017, COGNITION, V161, P132, DOI 10.1016/j.cognition.2017.01.009
   MacDorman KF, 2016, COGNITION, V146, P190, DOI 10.1016/j.cognition.2015.09.019
   MacDorman KF, 2009, COMPUT HUM BEHAV, V25, P695, DOI 10.1016/j.chb.2008.12.026
   McDonnell R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185587
   McKnight P.E., 2010, Mann-Whitney U Test, The Corsini Encyclopedia of Psychology
   Meskys E, 2019, Regulating deep fakes: legal and ethical considerations
   Mori M, 2012, IEEE ROBOT AUTOM MAG, V19, P98, DOI 10.1109/MRA.2012.2192811
   Mullen M, 2022, Mitchell Hamline Law Review, V48, P210
   Naruniec J, 2020, COMPUT GRAPH FORUM, V39, P173, DOI 10.1111/cgf.14062
   Nightingale SJ, 2022, P NATL ACAD SCI USA, V119, DOI 10.1073/pnas.2120481119
   Nirkin Y, 2019, IEEE I CONF COMP VIS, P7183, DOI 10.1109/ICCV.2019.00728
   Nitzan Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417826
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Piwek L, 2014, COGNITION, V130, P271, DOI 10.1016/j.cognition.2013.11.001
   Preu E, 2022, 2022 IEEE 13TH ANNUAL UBIQUITOUS COMPUTING, ELECTRONICS & MOBILE COMMUNICATION CONFERENCE (UEMCON), P547, DOI 10.1109/UEMCON54665.2022.9965697
   Radford A, 2016, Arxiv, DOI [arXiv:1511.06434, DOI 10.48550/ARXIV.1511.06434]
   Razavi A, 2019, ADV NEUR IN, V32
   Rössler A, 2019, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2019.00009
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Schuman H., 1996, Questions and Answers in Attitude Surveys-Experiments on Question, Form, Wording, and Context
   Siarohin A, 2021, PROC CVPR IEEE, P13648, DOI 10.1109/CVPR46437.2021.01344
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun QR, 2018, PROC CVPR IEEE, P5050, DOI 10.1109/CVPR.2018.00530
   Tahir R., 2021, P 2021 CHI C HUM FAC, DOI DOI 10.1145/3411764.3445699
   Tang H, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2052, DOI 10.1145/3343031.3350980
   Thies J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3182644
   / u/deepfakes, 2023, Faceswap
   Wagner T. L., 2019, OPEN INFORM SCI, V3, P32, DOI DOI 10.1515/OPIS-2019-0003
   Walczyna T, 2023, APPL SCI-BASEL, V13, DOI 10.3390/app13116711
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang Y, 2021, HifiFace: 3D shape and semantic prior guided high fidelity face swapping, V2, P1136, DOI [10.24963/ijcai.2021/157, DOI 10.24963/IJCAI.2021/157]
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   WHITE G, 2007, J VISION, V7, P477, DOI DOI 10.1167/7.9.477
   Whler L, 2021, P 2021 CHI C HUM FAC, DOI DOI 10.1145/3411764.3445627
   Wilson E, 2022, J Vis, V22, P4225, DOI [10.1167/jov.22.14.4225, DOI 10.1167/JOV.22.14.4225]
   Wilson E, 2023, ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, ETRA 2023, DOI 10.1145/3588015.3588416
   Wilson Ethan, 2022, arXiv
   Wöhler L, 2022, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS, IVA 2022, DOI 10.1145/3514197.3549687
   Wohler L, 2020, Computer animation and social agents. Communications in computer and information science, P120, DOI [10.1007/978-3-030-63426-1_13, DOI 10.1007/978-3-030-63426-1_13]
   Woolson R. F., 2007, International Encyclopedia of Statistical Science, P1, DOI 10.1002/0470011815. b2a15177
   Xue HY, 2023, CONCURR COMP-PRACT E, V35, DOI 10.1002/cpe.7554
   Zhang T, 2022, MULTIMED TOOLS APPL, V81, P6259, DOI 10.1007/s11042-021-11733-y
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zhu BQ, 2020, PROCEEDINGS OF THE 3RD AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY AIES 2020, P414, DOI 10.1145/3375627.3375849
   Zhu YH, 2021, PROC CVPR IEEE, P4832, DOI 10.1109/CVPR46437.2021.00480
NR 86
TC 0
Z9 0
U1 5
U2 5
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103888
DI 10.1016/j.cag.2024.103888
EA FEB 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LK6J9
UT WOS:001186731000001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Shang, K
   Shao, MW
   Qiao, YJ
   Liu, H
AF Shang, Kai
   Shao, Mingwen
   Qiao, Yuanjian
   Liu, Huan
TI Frequency-aware network for low-light image enhancement
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Low-light image enhancement; Frequency domain analysis; Multi-scale;
   Computer vision
ID RETINEX; GAP
AB Low-light images often suffer from severe visual degradation, affecting both human perception and high-level computer vision tasks. Most existing methods process images in the spatial domain, making it challenging to simultaneously improve brightness while suppressing noise. In this paper, we present a novel perspective to enhance images based on frequency domain characteristics. Specifically, we reveal that the low-frequency components are closely related to luminance and color, whereas the high-frequency components are not. Based on this observation, we propose the Frequency-aware Network (FaNet) for low-light image enhancement. By selectively adjusting low-frequency components, FaNet preserves more high-frequency details while achieving low-light image enhancement. Additionally, we employ a multi-scale framework and selective fusion for effective feature learning and image reconstruction. Experimental results demonstrate the superiority of the proposed method.
C1 [Shang, Kai; Shao, Mingwen; Qiao, Yuanjian; Liu, Huan] China Univ Petr East China, Sch Comp Sci & Technol, Qingdao 266580, Peoples R China.
   [Shang, Kai] Shandong Inst Petr & Chem Technol, Key Lab Intelligent Informat Proc, Dongying 257000, Peoples R China.
C3 China University of Petroleum
RP Shao, MW (corresponding author), China Univ Petr East China, Sch Comp Sci & Technol, Qingdao 266580, Peoples R China.
EM skkkyup@163.com; smw278@126.com; yjqiao@s.upc.edu.cn; liuhhappy@163.com
OI Shang, Kai/0000-0002-4357-0252
FU National Key Research and development Program of China [2021YFA1000102];
   National Natural Science Foundation of China [62376285, 62272375,
   61673396]; Natural Science Foundation of Shandong Province, China
   [ZR2022MF260]
FX The authors are very indebted to the anonymous referees for their
   critical comments and suggestions for the improvement of this paper.
   This work was supported by National Key Research and development Program
   of China (2021YFA1000102) , and in part by the grants from the National
   Natural Science Foundation of China (Nos. 62376285, 62272375, 61673396)
   , Natural Science Foundation of Shandong Province, China (No.
   ZR2022MF260) . All authors read and approved the final manuscript.
CR AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784
   Al Sobbahi R, 2022, SIGNAL PROCESS-IMAGE, V109, DOI 10.1016/j.image.2022.116848
   Al Sobbahi R, 2022, SIGNAL PROCESS-IMAGE, V100, DOI 10.1016/j.image.2021.116527
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   CHARBONNIER P, 1994, IEEE IMAGE PROC, P168
   COOLEY JW, 1965, MATH COMPUT, V19, P297, DOI 10.2307/2003354
   Cui ZT, 2022, Arxiv, DOI arXiv:2205.14871
   Dudhane A, 2023, PROC CVPR IEEE, P5703, DOI 10.1109/CVPR52729.2023.00552
   Fu ZQ, 2023, PROC CVPR IEEE, P22252, DOI 10.1109/CVPR52729.2023.02131
   Gao H, 2022, PROC CVPR IEEE, P9903, DOI 10.1109/CVPR52688.2022.00968
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo JT, 2023, PROC CVPR IEEE, P24132, DOI 10.1109/CVPR52729.2023.02311
   Hu YM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3181974
   Jiang LM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13899, DOI 10.1109/ICCV48922.2021.01366
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kennerley M, 2023, PROC CVPR IEEE, P11484, DOI 10.1109/CVPR52729.2023.01105
   Kim W, 2022, IEEE ACCESS, V10, P84535, DOI 10.1109/ACCESS.2022.3197629
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Lee CH, 2013, 2013 INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY & INTERNET-BASED SYSTEMS (SITIS), P43, DOI 10.1109/SITIS.2013.19
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P9396, DOI 10.1109/TPAMI.2021.3126387
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Li ZW, 2023, NEUROCOMPUTING, V518, P332, DOI 10.1016/j.neucom.2022.10.083
   Lim S, 2021, IEEE T MULTIMEDIA, V23, P4272, DOI 10.1109/TMM.2020.3039361
   Liu JY, 2021, INT J COMPUT VISION, V129, P1153, DOI 10.1007/s11263-020-01418-8
   Liu RS, 2021, PROC CVPR IEEE, P10556, DOI 10.1109/CVPR46437.2021.01042
   Liu X, 2022, IEEE Access, V10
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Lu K, 2021, IEEE T MULTIMEDIA, V23, P4093, DOI 10.1109/TMM.2020.3037526
   Lv F., 2018, Proc. BMVC, V220, P4
   Nakai K, 2013, I S INTELL SIG PROC, P445, DOI 10.1109/ISPACS.2013.6704591
   Ni ZK, 2020, IEEE T IMAGE PROCESS, V29, P9140, DOI 10.1109/TIP.2020.3023615
   Park J, 2018, PROC CVPR IEEE, P5928, DOI 10.1109/CVPR.2018.00621
   Pizer S. M., 1990, Proceedings of the First Conference on Visualization in Biomedical Computing (Cat. No.90TH0311-1), P337, DOI 10.1109/VBC.1990.109340
   Rao Yongming, 2021, Advances in neural information processing systems, V34
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren XT, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351427
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Tu Z., 2022, IEEE C COMPUT VIS PA, P5769
   Wang CY, 2023, PROC CVPR IEEE, P22356, DOI 10.1109/CVPR52729.2023.02141
   Wang LW, 2020, IEEE T IMAGE PROCESS, V29, P7984, DOI 10.1109/TIP.2020.3008396
   Wang LQ, 2014, IEEE T IMAGE PROCESS, V23, P3381, DOI 10.1109/TIP.2014.2324813
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang T., 2023, P AAAI C ART INT, P2654, DOI [DOI 10.1609/AAAI.V37I3.25364, 10.1609/AAAI.V37I3.25364]
   Wang WC, 2022, SIGNAL PROCESS-IMAGE, V106, DOI 10.1016/j.image.2022.116742
   Wang WJ, 2018, IEEE INT CONF AUTOMA, P751, DOI 10.1109/FG.2018.00118
   Wang Y, 2023, COMPUT GRAPH-UK, V110, P49, DOI 10.1016/j.cag.2022.12.001
   Wang YF, 2022, AAAI CONF ARTIF INTE, P2604
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZC, 2023, PROC CVPR IEEE, P18156, DOI 10.1109/CVPR52729.2023.01741
   Wei C, 2018, Arxiv, DOI arXiv:1808.04560
   Wu XM, 2017, IEEE IMAGE PROC, P3190, DOI 10.1109/ICIP.2017.8296871
   Xu XG, 2022, PROC CVPR IEEE, P17693, DOI 10.1109/CVPR52688.2022.01719
   Yang WH, 2021, IEEE T IMAGE PROCESS, V30, P3461, DOI 10.1109/TIP.2021.3062184
   Yang WH, 2021, IEEE T IMAGE PROCESS, V30, P2072, DOI 10.1109/TIP.2021.3050850
   Yang WH, 2020, PROC CVPR IEEE, P3060, DOI 10.1109/CVPR42600.2020.00313
   Yoo J, 2018, PROC CVPR IEEE, P6684, DOI 10.1109/CVPR.2018.00699
   Yuanming Hu, 2018, ACM Transactions on Graphics, V37, DOI 10.1145/3181974
   Zamir SW, 2023, IEEE T PATTERN ANAL, V45, P1934, DOI 10.1109/TPAMI.2022.3167175
   Zeng H, 2022, IEEE T PATTERN ANAL, V44, P2058, DOI 10.1109/TPAMI.2020.3026740
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang YH, 2021, INT J COMPUT VISION, V129, P1013, DOI 10.1007/s11263-020-01407-x
   Zheng Y, 2023, PROC CVPR IEEE, P5785, DOI 10.1109/CVPR52729.2023.00560
NR 68
TC 2
Z9 2
U1 11
U2 11
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD FEB
PY 2024
VL 118
BP 210
EP 219
DI 10.1016/j.cag.2023.12.014
EA JAN 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HZ6X1
UT WOS:001163384800001
DA 2024-08-05
ER

PT J
AU Fang, H
   Weng, DD
   Tian, ZY
   Ma, Y
   Lu, XJ
AF Fang, Hui
   Weng, Dongdong
   Tian, Zeyu
   Ma, Yin
   Lu, Xiangju
TI Audio-to-Deep-Lip: Speaking lip synthesis based on 3D landmarks
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Landmarks; Lip animation; 3D talking meshes; Viseme
ID DATABASE
AB Generating talking lips in sync with input speech has the potential to enhance speech communication and enable novel applications. This paper presents a system that can generate accurate 3D talking lips, readily applicable to unseen subjects and different languages. The developed head-mounted facial acquisition device and automated data processing pipeline can generate precise landmarks while mitigating the difficulty of acquiring 3D facial data. Our system consists of three stages to generate accurate lip movements. In the first stage, the fine-tuned Wav2Vec2.0+Transformer captures long-range audio context dependencies. In the second stage, we propose the Viseme Fixing method, which significantly improves lip accuracy at/b//p//m//f/ phonemes. In the last stage, we innovatively use the structural relationship between the inner and outer lips and learn to map the outer lip landmarks to the inner lip landmarks. Subjective evaluations show that the generated talking lips match the input audio significantly. We demonstrate two applications that animate 2D face videos and 3D face models using our landmarks. The precise lip landmarks allow the generated animations to exceed the results of state -of -the -art methods.
C1 [Fang, Hui; Weng, Dongdong; Tian, Zeyu] Beijing Inst Technol, Beijing Engn Res Ctr Mixed Real & Adv Display, Sch Opt & Photon, 5 Zhongguancun South St, Beijing 100081, Peoples R China.
   [Ma, Yin] Ningxia Baofeng Grp Co LTD, 19 Int Trade City, Yinchuan 750003, Ningxia, Peoples R China.
   [Lu, Xiangju] iQIYI Inc, Hongcheng Bldg,2 North First St, Beijing 100027, Peoples R China.
C3 Beijing Institute of Technology
RP Weng, DD (corresponding author), Beijing Inst Technol, Beijing Engn Res Ctr Mixed Real & Adv Display, Sch Opt & Photon, 5 Zhongguancun South St, Beijing 100081, Peoples R China.
EM crgj@bit.edu.cn
FU National Key Research and Development Program of China [2022YFF0902303];
   Administrative Commission of Zhongguancun Science Park
   [Z221100007722002]; National Natural Science Foundation of China
   [62072036]; Beijing Municipal Science & Technology Commission
FX This work was supported by the National Key Research and Development
   Program of China (No.2022YFF0902303) and the Beijing Municipal Science &
   Technology Commission and Administrative Commission of Zhongguancun
   Science Park (Z221100007722002) and the National Natural Science
   Foundation of China (No. 62072036) .
CR Amodei D, 2016, PR MACH LEARN RES, V48
   Ardila R, 2019, arXiv
   Baevski A, 2020, Advances in neural information processing systems, V33, P12449, DOI 10.5555/3495724.3496768
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Bregler C., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P353, DOI 10.1145/258734.258880
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Chen LL, 2019, PROC CVPR IEEE, P7824, DOI 10.1109/CVPR.2019.00802
   Chen LL, 2018, LECT NOTES COMPUT SC, V11211, P538, DOI 10.1007/978-3-030-01234-2_32
   Cheng K, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555399
   Cheng SY, 2018, PROC CVPR IEEE, P5117, DOI 10.1109/CVPR.2018.00537
   Chung YA, 2020, INT CONF ACOUST SPEE, P3497, DOI [10.1109/ICASSP40776.2020.9054438, 10.1109/icassp40776.2020.9054438]
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cudeiro D, 2019, PROC CVPR IEEE, P10093, DOI 10.1109/CVPR.2019.01034
   Edwards P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925984
   Eskimez SE, 2021, IEEE T MULTIMEDIA, V24, P3480, DOI 10.1109/TMM.2021.3099900
   Eskimez SE, 2020, IEEE-ACM T AUDIO SPE, V28, P27, DOI 10.1109/TASLP.2019.2947741
   Fan YR, 2022, PROC CVPR IEEE, P18749, DOI 10.1109/CVPR52688.2022.01821
   Fanelli G, 2010, IEEE T MULTIMEDIA, V12, P591, DOI 10.1109/TMM.2010.2052239
   Feng Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459936
   Fried O, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323028
   Hsu WN, 2021, IEEE-ACM T AUDIO SPE, V29, P3451, DOI 10.1109/TASLP.2021.3122291
   Hussen Abdelaziz Ahmed, 2020, ICMI '20: Proceedings of the 2020 International Conference on Multimodal Interaction, P378, DOI 10.1145/3382507.3418840
   Karras T, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073658
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Lele Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P35, DOI 10.1007/978-3-030-58545-7_3
   Li RL, 2020, PROC CVPR IEEE, P3407, DOI 10.1109/CVPR42600.2020.00347
   Li TY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130813
   Lu YX, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480484
   Microsoft, 2023, Bing speech API
   MontrealCorpusTools, 2018, Montreal forced aligner
   Nishimura R, 2019, 2019 12 AS PAC WORKS, P1
   Nocentini F, 2023, LECT NOTES COMPUT SC, V14233, P340, DOI 10.1007/978-3-031-43148-7_29
   Paier W, 2020, CVMP 2020: THE 17TH ACM SIGGRAPH EUROPEAN CONFERENCE ON VISUAL MEDIA PRODUCTION, DOI 10.1145/3429341.3429356
   Pham HX, 2017, IEEE COMPUT SOC CONF, P2328, DOI 10.1109/CVPRW.2017.287
   Prajwal KR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P484, DOI 10.1145/3394171.3413532
   Redmon J., 2018, CoRR
   Senst T, 2016, IEEE IMAGE PROC, P4478, DOI 10.1109/ICIP.2016.7533207
   Song Y, 2019, Arxiv, DOI arXiv:1804.04786
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Sun M, 2016, GitHub Repository
   Suwajanakorn S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073640
   Take-Two Interactive Software I, 2010, Dynamixyz: Markerless facial motion capture solution for achievers
   Thakare V M., 2013, International Journal of Computer Applications in Engineering, Technology and Sciences (IJCAETS), P412
   The Speech Group at Carnegie Mellon University, 2015, The CMU pronouncing dictionary
   Tian ZY, 2024, VISUAL COMPUT, V40, P2471, DOI 10.1007/s00371-023-02931-w
   Tzirakis P, 2020, IEEE INT CONF AUTOMA, P265, DOI 10.1109/FG47880.2020.00100
   Vaswani A, 2017, ADV NEUR IN, V30
   Vicon, 2016, About us
   Yao XW, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3449063
   Zhang CX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3847, DOI 10.1109/ICCV48922.2021.00384
   Zheng RB, 2021, Arxiv, DOI arXiv:2002.08700
   Zhou H, 2021, PROC CVPR IEEE, P4174, DOI 10.1109/CVPR46437.2021.00416
   Zhou Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201292
   Zhu H, 2018, ARXIV
NR 55
TC 1
Z9 1
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD MAY
PY 2024
VL 120
AR 103925
DI 10.1016/j.cag.2024.103925
EA MAY 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TQ0X6
UT WOS:001242618700001
DA 2024-08-05
ER

PT J
AU Mayer, B
   Donnay, K
   Lawonn, K
   Preim, B
   Meuschke, M
AF Mayer, Benedikt
   Donnay, Karsten
   Lawonn, Kai
   Preim, Bernhard
   Meuschke, Monique
TI Expert exploranation for communicating scientific methods - A case study
   in conflict research
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Visual storytelling; Science communication; Domain experts;
   Methodological education
ID SCIENCE COMMUNICATION; TELLING STORIES; VISUALIZATION; TEXT
AB Science communication aims at making key research insights accessible to the broad public. If explanatory and exploratory visualization techniques are combined to do so, the approach is also referred to as exploranation . In this context, the audience is usually not required to have domain expertise. However, we show that exploranation can not only support the communication between researchers and a broad audience, but also between researchers directly. With the goal of communicating an existing method for conducting causal inference on spatio-temporal conflict event data, we investigated how to perform exploranation for experts, i.e., expert exploranation . Based on application scenarios of the inference method, we developed three versions of an interactive visual story to explain the method to conflict researchers. We abstracted the corresponding design process and evaluated the stories both with experts who were unfamiliar with the explained method and experts who were already familiar with it. The positive and extensive feedback from the evaluation shows that expert exploranation is a promising direction for visual storytelling, as it can help to improve scientific outreach, methodological understanding, and accessibility for researchers new to a field.
C1 [Mayer, Benedikt; Preim, Bernhard; Meuschke, Monique] Otto von Guericke Univ, Dept Simulat & Graph, Univ Pl 2, D-39106 Magdeburg, Germany.
   [Donnay, Karsten] Univ Zurich, Dept Polit Sci, Affolternstr 56, CH-8050 Zurich, Switzerland.
   [Lawonn, Kai] Friedrich Schiller Univ Jena, Inst Informat, Ernst Abbe Pl 2, D-07743 Jena, Germany.
C3 Otto von Guericke University; University of Zurich; Friedrich Schiller
   University of Jena
RP Mayer, B (corresponding author), Otto von Guericke Univ, Dept Simulat & Graph, Univ Pl 2, D-39106 Magdeburg, Germany.
EM benedikt@isg.cs.uni-magdeburg.de
RI Donnay, Karsten/S-2990-2016
OI Donnay, Karsten/0000-0002-9080-6539
CR Aisch G., 2023, In defense of interactive graphics
   Amini F, 2018, AVI'18: PROCEEDINGS OF THE 2018 INTERNATIONAL CONFERENCE ON ADVANCED VISUAL INTERFACES, DOI 10.1145/3206505.3206552
   Amini F, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1459, DOI 10.1145/2702123.2702431
   [Anonymous], 2017, P CHI C HUM FACT COM, DOI [DOI 10.1145/3027063.3053113, 10.1145, 10.1145/3027063, DOI 10.1145/3027063]
   [Anonymous], 2023, Explorable explanations
   [Anonymous], 2014, Health Literacy and Numeracy: Workshop Summary
   [Anonymous], 2023, Collection of Observable notebooks
   Arevalo VJC, 2020, SUSTAIN SCI, V15, P1013, DOI 10.1007/s11625-020-00793-y
   Bateman S, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P2573
   Beyer J, 2020, Foundations of data visualization, P255, DOI [10.1007/978-3-030-34444-313, DOI 10.1007/978-3-030-34444-313]
   Bloom B., 1984, Taxonomyof educational objectives: The classification of educational goals, Handbook I: Cognitivedomain
   Bock A, 2018, IEEE COMPUT GRAPH, V38, P44, DOI 10.1109/MCG.2018.032421653
   Bottinger M., 2020, Foundations of Data Visualization, P297, DOI [10.1007/978-3-030-34444-3, DOI 10.1007/978-3-030-34444-3]
   Brossier M, 2023, COMPUT GRAPH-UK, V112, P22, DOI 10.1016/j.cag.2023.02.006
   Burns A, 2020, 2020 IEEE WORKSHOP ON EVALUATION AND BEYOND - METHODOLOGICAL APPROACHES TO VISUALIZATION (BELIV 2020), P19, DOI 10.1109/BELIV51497.2020.00010
   Cawthon N, 2007, IEEE INT CONF INF VI, P637
   Chen Q, 2024, IEEE T VIS COMPUT GR, V30, P4429, DOI 10.1109/TVCG.2023.3261320
   D3, 2023, About us
   de Bruin WB, 2013, P NATL ACAD SCI USA, V110, P14062, DOI 10.1073/pnas.1212729110
   Distill, 2023, About us
   Gershon N, 2001, COMMUN ACM, V44, P31, DOI 10.1145/381641.381653
   Green SJ, 2018, FACETS, V3, P164, DOI 10.1139/facets-2016-0079
   Heer J, 2007, IEEE T VIS COMPUT GR, V13, P1240, DOI 10.1109/TVCG.2007.70539
   Hlawitschka M, 2020, Foundations of data visualization, P285, DOI [10.1007/978-3-030-34444-3_15, DOI 10.1007/978-3-030-34444-3_15]
   Hohman Fred, 2020, Distill, V5, pe28, DOI [10.23915/distill.00028, DOI 10.23915/DISTILL.00028]
   Hullman J, 2013, IEEE T VIS COMPUT GR, V19, P2406, DOI 10.1109/TVCG.2013.119
   Hullman J, 2011, IEEE T VIS COMPUT GR, V17, P2231, DOI 10.1109/TVCG.2011.255
   Hung Y-H., 2019, Ph.D. thesis
   HUNSAKER A, 1979, JOURNALISM QUART, V56, P617, DOI 10.1177/107769907905600323
   Iacus SM, 2012, POLIT ANAL, V20, P1, DOI 10.1093/pan/mpr013
   Karell D, 2018, J PEACE RES, V55, P711, DOI 10.1177/0022343318777566
   Kelly M., 2019, CEPR Discussion Paper 13783, DOI DOI 10.2139/SSRN.3398303
   Lan FF, 2021, COMPUT GRAPH FORUM, V40, P635, DOI 10.1111/cgf.14332
   Latif S, 2021, COMPUT GRAPH FORUM, V40, P311, DOI 10.1111/cgf.14309
   Lee B, 2015, IEEE COMPUT GRAPH, V35, P84, DOI 10.1109/MCG.2015.99
   Lloyd D, 2011, IEEE T VIS COMPUT GR, V17, P2498, DOI 10.1109/TVCG.2011.209
   Ma KL, 2012, IEEE COMPUT GRAPH, V32, P12, DOI 10.1109/MCG.2012.24
   MatchedWake, 2023, About us
   Mayer B, 2023, COMPUT GRAPH FORUM, V42, DOI 10.1111/cgf.14922
   Mayr E, 2018, ISPRS INT J GEO-INF, V7, DOI 10.3390/ijgi7030096
   McKenna S, 2017, COMPUT GRAPH FORUM, V36, P377, DOI 10.1111/cgf.13195
   Meredith D., 2021, Explaining research: How to reach key audiences to advance your work, DOI [10.1080/0889311X.2013.769530, DOI 10.1080/0889311X.2013.769530]
   Meuschke M, 2022, COMPUT GRAPH-UK, V107, P144, DOI 10.1016/j.cag.2022.07.017
   Miro, 2023, About us
   Natl Acad Sci Engn Med, 2017, COMMUNICATING SCIENCE EFFECTIVELY: A RESEARCH AGENDA, P1, DOI 10.17226/23674
   Negrete A, 2010, INDIAN J SCI COMMUN, V9, P23
   O'Brien HL, 2008, J AM SOC INF SCI TEC, V59, P938, DOI 10.1002/asi.20801
   React, 2023, about us
   Ren DH, 2017, IEEE PAC VIS SYMP, P230, DOI 10.1109/PACIFICVIS.2017.8031599
   Riche NH, 2018, Data-Driven Storytelling, DOI [10.1201/9781315281575, DOI 10.1201/9781315281575]
   Roth RE, 2021, CARTOGR J, V58, P83, DOI 10.1080/00087041.2019.1633103
   Satyanarayan A, 2014, COMPUT GRAPH FORUM, V33, P361, DOI 10.1111/cgf.12392
   Schutte Sebastian, 2022, CRAN
   Schutte S, 2014, POLIT GEOGR, V41, P1, DOI 10.1016/j.polgeo.2014.03.001
   Segel E, 2010, IEEE T VIS COMPUT GR, V16, P1139, DOI 10.1109/TVCG.2010.179
   Shu XH, 2021, IEEE T VIS COMPUT GR, V27, P1492, DOI 10.1109/TVCG.2020.3030396
   Sokoloff D.R., 2004, INTERACTIVE LECT DEM
   Stolper CD, 2016, Tech. rep. MSR-TR-2016-14
   Sun Mengdi, 2023, IEEE Trans Vis Comput Graph, V29, P983, DOI 10.1109/TVCG.2022.3209428
   Tong C, 2018, INFORMATION, V9, DOI 10.3390/info9030065
   Udovicich C, 2017, J CLIN UROL, V10, P396, DOI 10.1177/2051415816668941
   Windhager F, 2019, P EG WORKSH VIS ENV, DOI [10.2312/envirvis.20191098, DOI 10.2312/ENVIRVIS.20191098]
   Yang LN, 2022, IEEE T VIS COMPUT GR, V28, P922, DOI 10.1109/TVCG.2021.3114774
   Ynnerman A, 2018, IEEE COMPUT GRAPH, V38, P13, DOI 10.1109/MCG.2018.032421649
   Zhang YJB, 2022, INFORMATICS-BASEL, V9, DOI 10.3390/informatics9040073
   Zhi Q, 2019, COMPUT GRAPH FORUM, V38, P675, DOI 10.1111/cgf.13719
NR 66
TC 1
Z9 1
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD MAY
PY 2024
VL 120
AR 103937
DI 10.1016/j.cag.2024.103937
EA MAY 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TQ0G4
UT WOS:001242601200001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Banaeizadeh, A
   Ramos, S
   Silva, JD
   Samavati, FF
   Sousa, MC
   Eaton, D
AF Banaeizadeh, Arya
   Ramos, Saulo
   Silva, Julio Daniel
   Samavati, Faramarz Famil
   Sousa, Mario Costa
   Eaton, David
TI Fault-sketch: A framework for modeling geological faults and
   displacements
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Geological modeling; Sketch-based interfaces & modeling
ID VISUALIZATION
AB Geological faults, which result from rock fracturing and displacement in the subsurface, play an important role in shaping the Earth's crust. Modeling fault surfaces and their impact on rock formations is, thus, useful for various geological applications. However, due to the complex configurations of faults, the modeling process can become difficult, requiring a framework with a specialized modeling toolkit. The primary challenge in fault modeling lies in properly correlating the geometry of faults with their effects on rock layers and other fault surfaces during displacement. To address this, we have developed a series of operators designed for fault modeling. In this manner, we present Fault -Sketch, a Sketch -Based Interface and Modeling (SBIM) framework that tackles the task of fault modeling and captures their influence on various geological structures. This framework enables the creation of surfaces as a result of rock configurations that have previously been largely overlooked in the existing literature. The collection of operators presented in Fault -Sketch are built upon a data structure fit for the purpose of consistently representing fault surfaces and their boundaries together with the influence they cause both on other surfaces and on the volumes they bound. Moreover, the operators are designed to mimic the geologic processes relevant to fault creation and displacement to facilitate their use by domain experts.
C1 [Banaeizadeh, Arya; Ramos, Saulo; Silva, Julio Daniel; Samavati, Faramarz Famil; Sousa, Mario Costa; Eaton, David] Univ Calgary, Calgary, AB, Canada.
C3 University of Calgary
RP Banaeizadeh, A (corresponding author), Univ Calgary, Calgary, AB, Canada.
EM Arya.banaeizadeh@ucalgary.ca; saulo.ramos@ucalgary.ca;
   machadoj@ucalgary.ca; samavati@ucalgary.ca; smcosta@ucalgary.ca;
   eatond@ucalgary.ca
FU Alliance Grant from the Natural Sciences and Engineering Research
   Council of Canada (NSERC) [ALLRP 548576\-2019]
FX The authors thank Dr. Vincent Roche, Thomas Jerome, and Jesus
   Rojas-Parra for their insight and expert evaluation of Fault-Sketch. The
   authors would also like to thank Petroleum Experts (Petex) for providing
   an academic licence of the Move software for fault interpretation. This
   research was supported by an Alliance Grant from the Natural Sciences
   and Engineering Research Council of Canada (NSERC), ALLRP 548576\-2019
   entitled "Dynamics of fault activation by hydraulic fracturing: Insights
   from new technologies".
CR Amorim R, 2012, P INT S SKETCH BAS I, P1
   Amorim R, 2014, P 4 JOINT S COMP AES, P17
   Brazil EV, 2010, P 7 SKETCH BAS INT M, P1
   Caumon G, 2009, MATH GEOSCI, V41, P927, DOI 10.1007/s11004-009-9244-2
   Cavero J, 2016, First Break., V34
   Ferreira RS, 2020, IEEE GEOSCI REMOTE S, V17, P1460, DOI 10.1109/LGRS.2019.2945680
   Fossen H, 2007, J GEOL SOC LONDON, V164, P755, DOI 10.1144/0016-76492006-036
   Ghosh SK., 2013, Structural geology: Fundamentals and modern developments
   Grose L, 2021, GEOSCI MODEL DEV, V14, P6197, DOI 10.5194/gmd-14-6197-2021
   Gürboga S, 2014, TURK J EARTH SCI, V23, P615, DOI 10.3906/yer-1405-16
   Hu ZW, 2019, PETROL EXPLOR DEV+, V46, P265, DOI 10.1016/S1876-3804(19)60007-8
   Jacquemyn C, 2021, J GEOL SOC LONDON, V178, DOI 10.1144/jgs2020-187
   Laurent G, 2013, TECTONOPHYSICS, V590, P83, DOI 10.1016/j.tecto.2013.01.015
   Lidal EM, 2012, P INT S SKETCH BAS I, P11, DOI DOI 10.2312/SBM/SBM12/011-020
   Lidal EM, 2013, EnvirVis@ euroVis
   Liu RC, 2019, J IMAGING SCI TECHN, V63, DOI 10.2352/J.ImagingSci.Technol.2019.63.6.060505
   Lorensen H. E., 1987, Proc. SIGGRAPH, V21, P163, DOI 10.1145/37401.37422
   Macêdo I, 2011, COMPUT GRAPH FORUM, V30, P27, DOI 10.1111/j.1467-8659.2010.01785.x
   Motta S, 2019, SEG INT EXP ANN M, DOI [10.1190/segam2019-3215843.1, DOI 10.1190/SEGAM2019-3215843.1]
   Motta S, 2020, COMPUT GEOSCI-UK, V142, DOI 10.1016/j.cageo.2020.104457
   Natali M., 2012, 2012 XXV SIBGRAPI - Conference on Graphics, Patterns and Images (SIBGRAPI 2012), P150, DOI 10.1109/SIBGRAPI.2012.29
   Natali M, 2013, Modeling terrains and subsurface geology.
   Natali M, 2014, P 30 SPRING C COMP G, P5
   Natali M, 2014, COMPUT GEOSCI-UK, V67, P40, DOI 10.1016/j.cageo.2014.02.010
   Ohnaka M, 2013, PHYSICS OF ROCK FAILURE AND EARTHQUAKES, P1, DOI 10.1017/CBO9781139342865
   Olsen L., 2010, Proc. of GI, P225
   Olsen L, 2011, IEEE COMPUT GRAPH, V31, P24, DOI 10.1109/MCG.2011.84
   Olsen L, 2009, COMPUT GRAPH-UK, V33, P85, DOI 10.1016/j.cag.2008.09.013
   Ramsay J.G., 1987, Folds and Fractures, V2, P309
   Samavati F.F., 2004, P INT WORKSHOP BIOME, P105
   Saqab MM, 2021, BASIN RES, V33, P1776, DOI 10.1111/bre.12535
   Sousa MC, 2020, SMART TOOLS APPS GRA, DOI [10.2312/stag.20201243, DOI 10.2312/STAG.20201243]
   Van Der Pluijm BenA., 2004, Earth Structure
   Wellmann F, 2018, ADV GEOPHYS, V59, P1, DOI 10.1016/bs.agph.2018.09.001
   Wu Q, 2003, COMPUT GEOSCI-UK, V29, P503, DOI 10.1016/S0098-3004(03)00018-9
   Yin P, 2010, P 24 INT WORKSH QUAL
   Zhang YC, 2018, COMPUT-AIDED CIV INF, V33, P545, DOI 10.1111/mice.12343
NR 37
TC 1
Z9 1
U1 1
U2 1
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD MAY
PY 2024
VL 120
AR 103920
DI 10.1016/j.cag.2024.103920
EA APR 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL1Q9
UT WOS:001234521100001
DA 2024-08-05
ER

PT J
AU Huang, H
   Yuan, SH
   Peng, Z
   Hao, Y
   Wen, CC
   Fang, Y
AF Huang, Hao
   Yuan, Shuaihang
   Peng, Zheng
   Hao, Yu
   Wen, Congcong
   Fang, Yi
TI A single 3D shape wavelet-based generative model
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Generative adversarial networks; 3D shape generation; Wavelet analysis;
   Single example
AB 3D shape generation, vital in fields including computer graphics, industrial design, and robotics, has seen a significant growth due to deep learning advancements. Nevertheless, a prevailing challenge in this area lies in its heavy reliance on extensive data for training. Consequently, the ability to generate 3D shapes with a limited quantity of training samples emerges as a desirable objective. The aim of this research is to design deep generative models capable of learning from a single reference 3D shape, thereby eliminating the requirement for sizeable datasets. Drawing inspiration from contemporary Generative Adversarial Networks (GANs) that operate on individual 3D shapes in a coarse -to -fine manner hierarchically, we propose a novel wavelet -based framework for single 3D shape generation, which preserves the global shape structure whilst inducing local variability. Our key observation is that, through wavelet decomposition, the low -frequency components of two inputs, where one input is a corrupted version of the other, are very similar. This similarity enables reconstruction of the uncorrupted input by leveraging the low -frequency components of the corrupted version. This observation motivates us to propose the wavelet decomposition of the 2D tri-plane feature maps of a given 3D shape, followed by the synthesis of new tri-plane feature maps for shape generation. To the best of our knowledge, this work represents the first endeavor to incorporate wavelet analysis into a deep generative model for the purpose of generating novel 3D shapes with a single example. Furthermore, we adapt data augmentation and Coulomb adversarial generative loss to facilitate training and generation procedures. We demonstrate the effectiveness of our approach by generating diverse 3D shapes and conducting quantitative comparisons with established baseline methods. Our implementation is available at https://github.com/hhuang-code/SinWavelet.
C1 [Huang, Hao; Yuan, Shuaihang; Hao, Yu; Fang, Yi] New York Univ Abu Dhabi, Ctr Artificial Intelligence & Robot, Abu Dhabi 129188, Saadiyat Island, U Arab Emirates.
   [Huang, Hao; Yuan, Shuaihang; Peng, Zheng; Hao, Yu; Wen, Congcong; Fang, Yi] NYU, Multimedia & Visual Comp Lab, 6 MetroTech Ctr, Brooklyn, NY 11201 USA.
C3 New York University Abu Dhabi; New York University
RP Wen, CC (corresponding author), NYU, Multimedia & Visual Comp Lab, 6 MetroTech Ctr, Brooklyn, NY 11201 USA.
EM hh1811@nyu.edu; sy2366@nyu.edu; zp2053@nyu.edu; yh3252@nyu.edu;
   cw3437@nyu.edu; yfang@nyu.edu
OI Huang, Hao/0000-0002-9131-5854
FU NYU Abu Dhabi Institute [AD131]
FX The authors would like to express their gratitude for the generous
   support provided by the NYU Abu Dhabi Institute through research grant
   AD131, which made this research possible. Additionally, the authors are
   thankful for the GPU computational support furnished by the NYUAD
   High-Performance Computing (HPC) facility.
CR Achlioptas P, 2018, PR MACH LEARN RES, V80
   Alcaide-Marzal J, 2020, DESIGN STUD, V66, P144, DOI 10.1016/j.destud.2019.11.003
   Alexa M, 2003, IEEE T VIS COMPUT GR, V9, P3, DOI 10.1109/TVCG.2003.1175093
   Ali U, 2020, J MATH IMAGING VIS, V62, P54, DOI 10.1007/s10851-019-00918-8
   Bank RE, 2000, SIAM J SCI COMPUT, V22, P1411, DOI 10.1137/S1064827599353701
   Rodriguez MXB, 2020, IEEE WINT CONF APPL, P3100, DOI [10.1109/WACV45572.2020.9093580, 10.1109/wacv45572.2020.9093580]
   Bloomenthal J., 1990, Computer Graphics, V24, P109, DOI 10.1145/91394.91427
   Cao XY, 2017, NEUROCOMPUTING, V226, P90, DOI 10.1016/j.neucom.2016.11.034
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   Chan ER, 2022, PROC CVPR IEEE, P16102, DOI 10.1109/CVPR52688.2022.01565
   Chan TF, 2005, IMAGE PROCESSING AND ANALYSIS, P1, DOI 10.1137/1.9780898717877
   Chen AP, 2022, LECT NOTES COMPUT SC, V13692, P333, DOI 10.1007/978-3-031-19824-3_20
   Chen H., 2022, P IEEE CVF C COMP VI, P3708
   Chen ZQ, 2021, PROC CVPR IEEE, P15735, DOI 10.1109/CVPR46437.2021.01548
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   Claypoole RL, 2003, IEEE T IMAGE PROCESS, V12, P1449, DOI 10.1109/TIP.2003.817237
   Cotter F., 2020, Ph.D. thesis
   Cotter F, 2018, Pytorch wavelets
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Daubechies I., 1992, CBMS NSF REG C SER A, DOI DOI 10.1137/1.9781611970104
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Granot N, 2022, PROC CVPR IEEE, P13450, DOI 10.1109/CVPR52688.2022.01310
   Grathwohl Will, 2019, INT C LEARN REPR
   Greshler G., 2021, P C NEUR INF PROC SY, V34, P20916
   GUIBAS L, 1985, ACM T GRAPHIC, V4, P74, DOI 10.1145/282918.282923
   Guth F., 2022, Advances in Neural Information Processing Systems, P478
   Haim N, 2022, LECT NOTES COMPUT SC, V13677, P491, DOI 10.1007/978-3-031-19790-1_30
   Hensel M, 2017, ADV NEUR IN, V30
   Hertz A, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392471
   Hinz T, 2021, IEEE WINT CONF APPL, P1299, DOI 10.1109/WACV48630.2021.00134
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Hu JY, 2023, Arxiv, DOI arXiv:2302.00190
   Huang Hanxun, 2021, INT C LEARN REPR
   Huang HB, 2017, IEEE I CONF COMP VIS, P1698, DOI 10.1109/ICCV.2017.187
   Hui KH, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555394
   Gulrajani I, 2017, ADV NEUR IN, V30
   Jiangning Zhang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P300, DOI 10.1007/978-3-030-58558-7_18
   Jolicoeur-Martineau A., 2018, INT C LEARN REPR
   Kalischek N, 2023, Arxiv, DOI arXiv:2211.13220
   Karras T., 2020, Advances in neural information processing systems, V33, P12104, DOI DOI 10.48550/ARXIV.2006.06676
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Kingma D.P., 2014, Proc. of ICLR
   Kingma DP, 2014, ADV NEUR IN, V27
   Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954
   Laine S., 2010, Proceedings of the Symposium on Interactive 3D Graphics and Games, P55, DOI DOI 10.1145/1730804.1730814
   Li PZ, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530157
   Li RH, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459766
   Lin Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P86, DOI 10.1007/978-3-030-58601-0_6
   Lorensen H. E., 1987, Proc. SIGGRAPH, V21, P163, DOI 10.1145/37401.37422
   Losasso F, 2004, ACM T GRAPHIC, V23, P769, DOI 10.1145/1015706.1015799
   Luo A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16218, DOI 10.1109/ICCV48922.2021.01593
   Luo ST, 2021, PROC CVPR IEEE, P2836, DOI 10.1109/CVPR46437.2021.00286
   Magistri F, 2022, IEEE ROBOT AUTOM LET, V7, P10120, DOI 10.1109/LRA.2022.3193239
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Mo KC, 2019, PROC CVPR IEEE, P909, DOI 10.1109/CVPR.2019.00100
   Nash C, 2017, COMPUT GRAPH FORUM, V36, P1, DOI 10.1111/cgf.13240
   Nash Charlie, 2020, ICML
   Noorani R., 2017, 3D PRINTING TECHNOLO
   Nooruddin FS, 2003, IEEE T VIS COMPUT GR, V9, P191, DOI 10.1109/TVCG.2003.1196006
   Ohtake Y, 2003, ACM T GRAPHIC, V22, P463, DOI 10.1145/882262.882293
   Oyallon E, 2017, IEEE I CONF COMP VIS, P5619, DOI 10.1109/ICCV.2017.599
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Paszke A., 2017, NIPS-W
   Peng S., 2020, COMPUTER VISION ECCV
   Phung H, 2023, PROC CVPR IEEE, P10199, DOI 10.1109/CVPR52729.2023.00983
   Postels J, 2021, INT CONF 3D VISION, P1249, DOI 10.1109/3DV53792.2021.00132
   Ramamonjisoa M, 2021, PROC CVPR IEEE, P11084, DOI 10.1109/CVPR46437.2021.01094
   Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530
   RIVARA MC, 1992, COMMUN APPL NUMER M, V8, P281, DOI 10.1002/cnm.1630080502
   Shaham TR, 2019, IEEE I CONF COMP VIS, P4569, DOI 10.1109/ICCV.2019.00467
   Shocher A, 2019, IEEE I CONF COMP VIS, P4491, DOI 10.1109/ICCV.2019.00459
   Smith E. J., 2017, C ROB LEARN, P87
   Sohl-Dickstein J, 2015, PR MACH LEARN RES, V37, P2256
   Sun J, 2022, ACM SIGGRAPH C P, P1
   Taubin G., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P351, DOI 10.1145/218380.218473
   Tran NT, 2021, IEEE T IMAGE PROCESS, V30, P1882, DOI 10.1109/TIP.2021.3049346
   Unterthiner T., 2018, INT C LEARN REPR
   Vahdat Arash, 2022, P 36 INT C NEURAL IN, V35, P10021
   Varley J, 2017, IEEE INT C INT ROBOT, P2442, DOI 10.1109/IROS.2017.8206060
   Vaswani A, 2017, ADV NEUR IN, V30
   Vollmer J, 1999, COMPUT GRAPH FORUM, V18, pC131, DOI 10.1111/1467-8659.00334
   Williams T., 2018, INT C LEARNING REPRE
   Wu JJ, 2016, ADV NEUR IN, V29
   Wu RD, 2023, Arxiv, DOI arXiv:2305.15399
   Wu RD, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555480
   Chang AX, 2015, Arxiv, DOI [arXiv:1512.03012, DOI 10.48550/ARXIV.1512.03012]
   Xie JW, 2018, PROC CVPR IEEE, P8629, DOI 10.1109/CVPR.2018.00900
   Xuejian Rong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P240, DOI 10.1007/978-3-030-58601-0_15
   Yang GD, 2019, IEEE I CONF COMP VIS, P4540, DOI 10.1109/ICCV.2019.00464
   Yao T, 2022, LECT NOTES COMPUT SC, V13685, P328, DOI 10.1007/978-3-031-19806-9_19
   Yu Jason J, 2020, ADV NEURAL INF PROCE, V33, P6184
   Zhang D., 2019, Fundamentals of Image Data Mining: Analysis, Features, Classification and Retrieval, P35, DOI [DOI 10.1007/978-3-030-17989-23, DOI 10.1007/978-3-030-17989-2_3]
   Zhou LQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5806, DOI 10.1109/ICCV48922.2021.00577
NR 94
TC 0
Z9 0
U1 4
U2 4
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103891
DI 10.1016/j.cag.2024.103891
EA FEB 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NV9K1
UT WOS:001203346600001
DA 2024-08-05
ER

PT J
AU Kuták, D
   Langlois, D
   Rozic, R
   Byska, J
   Miao, HC
   Kriglstein, S
   Kozlíková, B
AF Kutak, David
   Langlois, Danielle
   Rozic, Roman
   Byska, Jan
   Miao, Haichao
   Kriglstein, Simone
   Kozlikova, Barbora
TI Design and evaluation of alphabetic and numeric input methods for
   virtual reality
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Virtual reality; Head-mounted displays; User evaluation; Alphabetic
   input; Numeric input; Alphanumeric input; Writing; Virtual keyboards
AB In today's virtual reality (VR), users have various ways to influence their VR experience, including alphanumeric input. While typing characters and numbers is straightforward on desktop computers, it presents challenges and opportunities in head-mounted display VR due to specific interaction methods and a lack of real -world visual stimuli. Addressing these open questions, our work implements and evaluates ten approaches to alphabetic and numeric inputs in VR. We describe the design motivation behind these input methods and evaluate them in a user study with 40 participants divided into groups for alphabetic and numeric keyboards. This comparison investigates each method's performance and user interactions. Our findings suggest that different input methods significantly impact words per minute and error rates, and that certain keyboard designs may receive better subjective evaluations despite poorer objective performance.
C1 [Kutak, David; Langlois, Danielle; Rozic, Roman; Byska, Jan; Kriglstein, Simone; Kozlikova, Barbora] Masaryk Univ, Brno, Czech Republic.
   [Byska, Jan] Univ Bergen, Bergen, Norway.
   [Miao, Haichao] Lawrence Livermore Natl Lab, Ctr Appl Sci Comp, Livermore, CA USA.
C3 Masaryk University Brno; University of Bergen; United States Department
   of Energy (DOE); Lawrence Livermore National Laboratory
RP Kuták, D (corresponding author), Masaryk Univ, Brno, Czech Republic.
EM kutak@mail.muni.cz; 528203@mail.muni.cz; 485422@mail.muni.cz;
   byska@mail.muni.cz; miao1@llnl.gov; kriglstein@mail.muni.cz;
   kozlikova@mail.muni.cz
FU US DOE LLNL-LDRD under the auspices of the U.S. Department of Energy by
   Lawrence Livermore National Laboratory [23-SI-003, DE-AC52-07NA27344,
   LLNL-CONF-850222]; Horizon Europe Cluster 1-Health [101080665]
FX This work was supported by the US DOE LLNL-LDRD 23-SI-003 and was
   performed under the auspices of the U.S. Department of Energy by
   Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344
   (LLNL-CONF-850222). This work was also funded by Horizon Europe Cluster
   1-Health as a part of the ASP-belong project (project number:
   101080665).
CR Adhikary J, 2021, IEEE T VIS COMPUT GR, V27, P2648, DOI 10.1109/TVCG.2021.3067776
   Aleven V, 2004, LECT NOTES COMPUT SC, V3220, P443
   [Anonymous], 2024, Cloud computing services: Microsoft azure
   [Anonymous], 2016, GOOGLE
   Boletsis C, 2019, TECHNOLOGIES, V7, DOI 10.3390/technologies7020031
   Boletsis Costas., 2019, INT J VIRTUAL REALIT, V19, P2, DOI [DOI 10.20870/IJVR.2019.19.3.2917, 10.20870/IJVR.2019.19.3.2917, DOI 10.20870/IJVR.2019.19.32917]
   BRADLEY JV, 1958, J AM STAT ASSOC, V53, P525, DOI 10.2307/2281872
   Brooke J., 1996, SUS-a quick and dirty usability scale, DOI [DOI 10.1201/9781498710411-35, DOI 10.1201/9781498710411]
   Brooke J, 2013, J USABILITY STUD, V8, P29
   Brun D, 2020, CHI'20: EXTENDED ABSTRACTS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3334480.3382837
   Brun D, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3313258
   Cejka J, 2021, MULTIMED TOOLS APPL, V80, P31085, DOI 10.1007/s11042-020-09305-7
   Chen SB, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3312762
   Chunming Gao, 2012, 2012 IEEE International Conference on Virtual Environments Human-Computer Interfaces and Measurement Systems (VECIMS), P30
   de Zoeten R, 2013, Ph.D. thesis
   Derby Jessyca L., 2020, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V64, P2102, DOI 10.1177/1071181320641509
   Dube TJ, 2020, CHI'20: EXTENDED ABSTRACTS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3334480.3382882
   Elmgren Rasmus., 2017, Handwriting in VR as a Text Input Method
   Fashimpaur Jacqui, 2020, CHI EA '20: Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems, P1, DOI 10.1145/3334480.3382888
   Fourrier N, 2023, IEEE T VIS COMPUT GR, V29, P4438, DOI 10.1109/TVCG.2023.3320215
   Grubert J, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P151, DOI 10.1109/VR.2018.8446250
   Gugenheimer J, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P49, DOI 10.1145/2984511.2984576
   Gupta A, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300244
   Hutama W, 2021, ADJUNCT PROCEEDINGS OF THE 34TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, UIST 2021, P115, DOI 10.1145/3474349.3480195
   International Telecommunication Union, 2001, E.161: Arrangement of digits, letters and symbols on telephones and other devices that can be used for gaining access to a telephone network
   Jiang HY, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P692, DOI [10.1109/VR46266.2020.1581236395562, 10.1109/VR46266.2020.00-12]
   Jimenez JG, 2018, Electron Imaging, V30, P1, DOI [10.2352/ISSN.2470-1173.2018.03. ERVR-450, DOI 10.2352/ISSN.2470-1173.2018.03.ERVR-450]
   Kern Florian, 2023, IEEE Trans Vis Comput Graph, VPP, DOI 10.1109/TVCG.2023.3247098
   Knierim P, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173919
   Komiya K, 2017, 2017 TENTH INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND UBIQUITOUS NETWORK (ICMU), P65
   Kus S, 2021, 22TH INTERNATIONAL CONFERENCE COMPUTATIONAL PROBLEMS OF ELECTRICAL ENGINEERING (CPEE 2021), DOI 10.1109/CPEE54040.2021.9585252
   Lee Jihyun., 2016, Proceedings of the 2016 chi conference extended abstracts on human factors in computing systems, P2585, DOI DOI 10.1145/2851581.2892344
   Lee Y, 2017, LECT NOTES COMPUT SC, V10280, P111, DOI 10.1007/978-3-319-57987-0_9
   Lepouras G, 2018, VIRTUAL REAL-LONDON, V22, P63, DOI 10.1007/s10055-017-0312-5
   Lepouras G, 2009, 2009 IEEE INTERNATIONAL CONFERENCE ON VIRTUAL ENVIRONMENTS, HUMAN-COMPUTER INTERFACES AND MEASUREMENT SYSTEMS, P240, DOI 10.1109/VECIMS.2009.5068901
   Li YX, 2021, PEER PEER NETW APPL, V14, P2826, DOI 10.1007/s12083-021-01103-8
   Li Z, 2021, Frameless, V3
   Lu XS, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1060, DOI [10.1109/vr.2019.8797901, 10.1109/VR.2019.8797901]
   Ma XY, 2018, IUI 2018: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P263, DOI 10.1145/3172944.3172988
   MacKenzie I.S., 2003, CHI'03 Extended Abstracts on Human Factors in Computing Systems, CHI EA'03, P754, DOI [DOI 10.1145/765891.765971, 10.1145/765891.765971]
   Masson D, 2024, Balanced Latin square online generator
   Meier M, 2021, EXTENDED ABSTRACTS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'21), DOI 10.1145/3411763.3451553
   Menzner T, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1080, DOI 10.1109/VR.2019.8797754
   Monteiro P, 2021, IEEE T VIS COMPUT GR, V27, P2702, DOI 10.1109/TVCG.2021.3067687
   Motejlek J, 2021, IEEE T LEARN TECHNOL, V14, P415, DOI 10.1109/TLT.2021.3092964
   Nguyen A, 2020, P VRST, P1, DOI [10.1145/3385956.3422114, DOI 10.1145/3385956.3422114]
   Nooruddin, 2020, IEEE SYS MAN CYBERN, P744, DOI [10.1109/smc42975.2020.9283348, 10.1109/SMC42975.2020.9283348]
   Ogitani T, 2018, INT CON ADV INFO NET, P342, DOI 10.1109/AINA.2018.00059
   Otte A, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1729, DOI 10.1109/VR.2019.8797740
   Pastor A, 2020, Virtual hands: a comparative study of two text input paradigms for VR
   Pick S, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P109, DOI 10.1109/3DUI.2016.7460039
   Poupyrev I, 1998, P IEEE VIRT REAL ANN, P126, DOI 10.1109/VRAIS.1998.658467
   Rajanna V, 2018, 2018 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2018), DOI 10.1145/3204493.3204541
   Son J, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3312926
   Speicher M, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174221
   Swieso Sloan, 2021, SUI '21: Symposium on Spatial User Interaction, DOI 10.1145/3485279.3485302
   Takahashi R, 2021, INT SYM MIX AUGMENT, P251, DOI 10.1109/ISMAR-Adjunct54149.2021.00058
   Tomaru Yuki, 2019, 2019 8th International Congress on Advanced Applied Informatics (IIAI-AAI), P1037, DOI 10.1109/IIAI-AAI.2019.00215
   Tominaga K, 2021, 5TH ASIAN CHI SYMPOSIUM PROCEEDINGS, P25, DOI 10.1145/3429360.3468174
   Wan Tingjie, 2024, IEEE Trans Vis Comput Graph, V30, P6493, DOI 10.1109/TVCG.2024.3349428
   Witt H, 2007, CHI 07 EXTENDED ABST, P2759, DOI [10.1145/1240866.1241075, DOI 10.1145/1240866.1241075]
   Yanagihara N, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3365026
   Yanagihara N, 2016, SUI'18: PROCEEDINGS OF THE 2018 SYMPOSIUM ON SPATIAL USER INTERACTION, P170, DOI 10.1145/3267782.3274687
   Yang Z, 2019, APPL ERGON, V78, P164, DOI 10.1016/j.apergo.2019.03.006
   Yu C, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P4479, DOI 10.1145/3025453.3025964
   Zhang ZG, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P530, DOI [10.1109/VRW52623.2021.00147, 10.1109/ICPADS53394.2021.00072]
NR 66
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103955
DI 10.1016/j.cag.2024.103955
EA JUN 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WV3N0
UT WOS:001257609400001
DA 2024-08-05
ER

PT J
AU Kleanthous, T
   Martini, A
AF Kleanthous, Tobias
   Martini, Antonio
TI Making motion matching stable and fast with Lipschitz-continuous neural
   networks and Sparse Mixture of Experts
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Character animation; Animation; Neural networks; Motion matching;
   Regularization
AB Motion matching has become a widely adopted technique for generating high -quality interactive animation systems in video games. However, its current implementations suffer from significant computational and memory resource overheads, limiting its scalability in the context of modern video game performance profiles. Our method significantly reduces the computational complexity of approaches to motion synthesis, such as "Learned Motion Matching", while simultaneously improving the compactness of the data that can be stored and the robustness of pose output. As a result, our method enables the efficient execution of motion matching that significantly outperforms other implementations, by 8.5 x times in CPU execution cost and at 80% of the memory requirements of "Learned Motion Matching", on contemporary video game hardware, thereby enhancing its practical applicability and scalability in the gaming industry and unlocking the ability to apply on large numbers of animated in -game characters. In this paper, we expand upon our published paper "Learning Robust and Scalable Motion with Lipschitz Continuity and Sparse Mixture of Experts", where we successfully proposed a novel method for learning motion matching that combines a Sparse Mixture of Experts model architecture and a Lipschitz -continuous latent space for representation of poses. We present further details on our method, with extensions to our approach to expert utilization within our neural networks.
C1 [Kleanthous, Tobias; Martini, Antonio] Prod R&D, Tencent, Edinburgh, Scotland.
RP Kleanthous, T (corresponding author), Prod R&D, Tencent, Edinburgh, Scotland.
EM tobiask@global.tencent.com; antonio@global.tencent.com
OI Kleanthous, Tobias/0009-0001-3206-1874
CR Blanco J. L., 2014, nanoflann: a C++header-only fork of FLANN, a library for Nearest Neighbor (NN) with KD-trees
   Bollo D, 2018, Inertialization: highperformance animation transitions in gears of war. In Proc. of GDC 2018
   Buttner M, 2015, Motion matching-the road to next gen animation
   Clavet S., 2016, PROC GAME DEVELOPERS
   Fedus W, 2022, J MACH LEARN RES, V23
   Frechette Nicholas, 2017, Animation compression library
   Gouk H, 2021, MACH LEARN, V110, P393, DOI 10.1007/s10994-020-05929-w
   He KM, 2015, Arxiv, DOI [arXiv:1502.01852, DOI 10.48550/ARXIV.1502.01852]
   Hempel T, 2022, IEEE IMAGE PROC, P2496, DOI 10.1109/ICIP46576.2022.9897219
   Hendrycks D, 2020, Arxiv, DOI arXiv:1606.08415
   Holden D, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392440
   Holden D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073663
   Johnson J, 2021, IEEE T BIG DATA, V7, P535, DOI 10.1109/TBDATA.2019.2921572
   Kleanthous T., 2021, GDC 21
   Kleanthous T, 2023, 15TH ANNUAL ACM SIGGRAPH CONFERENCE ON MOTION, INTERACTION AND GAMES, MIG 2023, DOI 10.1145/3623264.3624442
   Lee Yongjoon, 2010, ACM SIGGRAPH ASIA 20, DOI [10.1145/1882262.1866160, DOI 10.1145/1882262.1866160]
   Lepikhin Dmitry, 2021, 9 INT C LEARN REPR I
   Li PZ, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530157
   Ling HY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392422
   Liu Hsueh-Ti Derek, 2022, SIGGRAPH22 Conference Proceeding: Special Interest Group on Computer Graphics and Interactive Techniques Conference Proceedings, DOI 10.1145/3528233.3530713
   Maiorca A, 2022, VISIGRAPP, P286, DOI 10.5220/0010908700003124
   Miyato T, 2018, Arxiv, DOI arXiv:1802.05957
   ONNX Runtime developers, 2021, ONNX Runtime
   Paszke A, 2019, ADV NEUR IN, V32
   Pavlitska S, 2023, IEEE IJCNN, DOI 10.1109/IJCNN54540.2023.10191904
   Pavllo D, 2018, Arxiv, DOI arXiv:1805.06485
   Raparthi N, 2020, SIGGRAPH ASIA 2020 P, DOI [10.1145/3415264.3425474, DOI 10.1145/3415264.3425474]
   Starke S, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530178
   Starke S, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392450
   Suju DA, 2017, 2017 FOURTH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, COMMUNICATION AND NETWORKING (ICSCN)
   Xie XY, 2022, Arxiv, DOI [arXiv:2208.06677, 10.1109/TPAMI.2024.3423382, DOI 10.1109/TPAMI.2024.3423382]
   Zhou Y, 2020, Arxiv, DOI arXiv:1812.07035
   Zoph B., 2022, arXiv
NR 33
TC 1
Z9 1
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD MAY
PY 2024
VL 120
AR 103911
DI 10.1016/j.cag.2024.103911
EA APR 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QZ5P5
UT WOS:001224707600001
DA 2024-08-05
ER

PT J
AU David, E
   Gutiérrez, J
   Vo, MLH
   Coutrot, A
   Da Silva, MP
   Le Callet, P
AF David, Erwan
   Gutierrez, Jesus
   Vo, Melissa Le-Hoa
   Coutrot, Antoine
   Da Silva, Matthieu Perreira
   Le Callet, Patrick
TI The<i> Salient360!</i> toolbox: Handling gaze data in 3D made easy
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Toolbox; Gaze tracking; 360 degrees stimuli; Processing; Comparison;
   Visualisation
ID EYE-TRACKING DATA; SALIENCY; MOVEMENT; MATLAB
AB Eye tracking has historically been a very popular tool. The data it records allow us to understand how people behave and what they attend to within our visual world; under this perspective the experiments, applications and use -cases are endless. Therefore, it is not surprising to witness a strong rise in the use of eXtended Reality (XR) devices with embedded eye trackers in research. These devices allow for less obtrusive experimenting conditions, and a significantly higher experimental control compared to traditional desktop testing. The use of eye tracking in XR is increasing and so is the need for a toolbox enabling consensus about eye tracking methods in 3D. We present the Salient360! toolbox: it implements functions to identify saccades and fixations and output gaze features (e.g., saccade directions) to generate saliency maps, fixation maps, and scanpath data. It implements comparisons of gaze data with methods adapted to 3D. We plan continuous improvements of the toolbox as the community develops new tools and methods dedicated to 360 gaze tracking. We hope that this toolbox will spark discussions about the methodology of 3D gaze processing, facilitate running experiments, and improve studying gaze in 3D. https://github.com/David-Ef/salient360Toolbox
C1 [David, Erwan; Vo, Melissa Le-Hoa] Goethe Univ, Dept Psychol, Scene Grammar Lab, D-60323 Frankfurt, Germany.
   [David, Erwan] Le Mans Univ, LIUM, F-72085 Le Mans 9, France.
   [Gutierrez, Jesus] Univ Politecn Madrid, Grp Tratamiento Imagenes, Madrid 28040, Spain.
   [Coutrot, Antoine] Univ Lyon, LIRIS, CNRS, F-69622 Lyon, France.
   [Da Silva, Matthieu Perreira; Le Callet, Patrick] Nantes Univ, Ecole Cent Nantes, CNRS, UMR 6004,LS2N, F-44000 Nantes, France.
C3 Goethe University Frankfurt; Universidad Politecnica de Madrid; Centre
   National de la Recherche Scientifique (CNRS); Institut National des
   Sciences Appliquees de Lyon - INSA Lyon; Nantes Universite; Ecole
   Centrale de Nantes; Centre National de la Recherche Scientifique (CNRS)
RP David, E (corresponding author), Goethe Univ, Dept Psychol, Scene Grammar Lab, D-60323 Frankfurt, Germany.
EM erwan.david@univ-lemans.fr
OI David, Erwan/0000-0002-5307-1705
FU RFI Atlanstic2020; SFB/TRR 26 135 project C7; Hessisches Ministerium fur
   Wissenschaft und Kunst (HMWK; project 'The Adaptive Mind')
FX This work was supported by RFI Atlanstic2020, the SFB/TRR 26 135 project
   C7 to Melissa L.-H. V & otilde; and the Hessisches Ministerium fur
   Wissenschaft und Kunst (HMWK; project 'The Adaptive Mind') .
CR Agtzidis I, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1007, DOI 10.1145/3343031.3350947
   Anderson Nicola C, 2023, Curr Top Behav Neurosci, V65, P73, DOI 10.1007/7854_2022_409
   Andreu-Perez J, 2016, NEUROINFORMATICS, V14, P51, DOI 10.1007/s12021-015-9275-4
   Borji A, 2013, IEEE I CONF COMP VIS, P921, DOI 10.1109/ICCV.2013.118
   Botch TL, 2023, J Vis, P5206, DOI [10.1167/jov.23.9.5206, DOI 10.1167/JOV.23.9.5206]
   Bradski G, 2000, DR DOBBS J, V25, P120
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Cercenelli L, 2017, COMPUT BIOL MED, V80, P45, DOI 10.1016/j.compbiomed.2016.11.007
   Clay V, 2019, J EYE MOVEMENT RES, V12, DOI 10.16910/jemr.12.1.3
   Cornelissen FW, 2002, BEHAV RES METH INS C, V34, P613, DOI 10.3758/BF03195489
   Coutrot A, 2018, BEHAV RES METHODS, V50, P362, DOI 10.3758/s13428-017-0876-8
   David E, 2023, ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, ETRA 2023, DOI 10.1145/3588015.3588406
   David EJ, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P432, DOI 10.1145/3204949.3208139
   David EJ, 2022, J VISION, V22, DOI 10.1167/jov.22.4.12
   Deane O, 2023, BEHAV RES METHODS, V55, P1372, DOI 10.3758/s13428-022-01833-4
   Dewhurst R, 2012, BEHAV RES METHODS, V44, P1079, DOI 10.3758/s13428-012-0212-2
   Diaz G, 2013, J VISION, V13, DOI 10.1167/13.12.5
   Duchowski Andrew T., 2022, Procedia Computer Science, P1641, DOI 10.1016/j.procs.2022.09.221
   Ghosh S, 2024, IEEE T PATTERN ANAL, V46, P61, DOI 10.1109/TPAMI.2023.3321337
   Gutiérrez J, 2018, INT WORK QUAL MULTIM, P171
   Gutiérrez J, 2018, SIGNAL PROCESS-IMAGE, V69, P35, DOI 10.1016/j.image.2018.05.003
   Harris CR, 2020, NATURE, V585, P357, DOI 10.1038/s41586-020-2649-2
   Krassanakis V, 2014, J EYE MOVEMENT RES, V7
   Kümmerer M, 2015, P NATL ACAD SCI USA, V112, P16054, DOI 10.1073/pnas.1510393112
   Lam SK., 2015, P 2 WORKSH LLVM COMP, P1
   Lappi O, 2016, NEUROSCI BIOBEHAV R, V69, P49, DOI 10.1016/j.neubiorev.2016.06.006
   Larsson L, 2016, J NEUROSCI METH, V274, P13, DOI 10.1016/j.jneumeth.2016.09.005
   Larsson L, 2014, PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP'14 ADJUNCT), P1161, DOI 10.1145/2638728.2641693
   Le Meur O, 2013, BEHAV RES METHODS, V45, P251, DOI 10.3758/s13428-012-0226-9
   Liversedge SP, 2000, TRENDS COGN SCI, V4, P6, DOI 10.1016/S1364-6613(99)01418-7
   Llanes-Jurado J, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20174956
   Marighetto P, 2017, IEEE IMAGE PROC, P1802, DOI 10.1109/ICIP.2017.8296592
   Moacdieh N.M., 2012, Proceedings of the Human Factors and Ergonomics Society 56th Annual Meeting, P1366, DOI DOI 10.1177/1071181312561391
   Paszke A, 2019, ADV NEUR IN, V32
   Pathmanathan N, 2023, COMPUT GRAPH FORUM, V42, P385, DOI 10.1111/cgf.14838
   Prasad D, 2023, P 2023 C COGNITIVE C, P15, DOI [10.32470/CCN.2023.1555-0, DOI 10.32470/CCN.2023.1555-0]
   Riche N, 2013, IEEE I CONF COMP VIS, P1153, DOI 10.1109/ICCV.2013.147
   Rothkopf ConstantinA., 2004, ETRA 04, P123, DOI DOI 10.1145/968363.968388
   Salvucci D.D., 2000, P 2000 S EYE TRACKIN, P71, DOI [10.1145/355017.355028, DOI 10.1145/355017.355028]
   Seabold S, 2010, STATSMODELS ECONOMET, DOI 10.25080/Majora-92bf1922-012
   Shoemake Ken, 1985, P 12 ANN C COMPUTER, P245, DOI DOI 10.1145/325165.325242
   Simoudis E., 1996, KDD 96 P 2 INT C KNO, P226
   Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599
   Stuart S, 2018, MED BIOL ENG COMPUT, V56, P289, DOI 10.1007/s11517-017-1669-z
   Tomar S., 2006, Linux J, V2006, P10
   Tomasi M, 2016, J VISION, V16, DOI 10.1167/16.3.27
   Ugwitz P, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12031027
   van der Walt S, 2014, PEERJ, V2, DOI 10.7717/peerj.453
   Virtanen P, 2020, NAT METHODS, V17, P261, DOI 10.1038/s41592-019-0686-2
NR 49
TC 0
Z9 0
U1 2
U2 2
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103890
DI 10.1016/j.cag.2024.103890
EA FEB 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MY6D8
UT WOS:001197226900001
OA hybrid
DA 2024-08-05
ER

PT J
AU Caputo, A
   Bartolomioli, R
   Orso, V
   Mingardi, M
   Da Granaiola, L
   Gamberini, L
   Giachetti, A
AF Caputo, Ariel
   Bartolomioli, Riccardo
   Orso, Valeria
   Mingardi, Michele
   Da Granaiola, Leonardo
   Gamberini, Luciano
   Giachetti, Andrea
TI Comparison of deviceless methods for distant object manipulation in
   mixed reality
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Interaction design; Virtual object manipulation; User study
ID VIRTUAL-REALITY
AB Mixed Reality (MR) applications based on the current generation of Head -mounted displays (HMDs) support a deviceless manipulation of virtual objects based on finger tracking and control of the 3D transformation with gestures. However, when the object manipulation is performed at some distance, and when the transform includes scaling, it is not apparent how to remap the hand motions over the degrees of freedom of the object. The most popular software toolkits used for XR development provide specific solutions, but there are still usability issues and a need for clear guidelines for the interaction design. In this work, we compare three different solutions for the devices and remote translation, rotation, and scaling of virtual objects in a real environment in two user studies aimed at assessing the usability of the methods for different tasks. The tasks are a fast and rough docking of virtual cubes on a tangible shelf from varying distances and the accurate placement of cubes on the shelf in a fixed amount of time. The outcomes of these studies show that the usability of the manipulation methods is strongly affected by the use of separate or integrated control of the degrees of freedom, by the use of the hands in a symmetric or specialized way, by the visual feedback, and by the previous experience of the users and provides valuable guidelines for the design of interactive mixed environments.
C1 [Caputo, Ariel; Giachetti, Andrea] Univ Verona, Dept Engn Med Innovat, I-37134 Verona, Italy.
   [Bartolomioli, Riccardo] Univ Verona, Dept Comp Sci, I-37134 Verona, Italy.
   [Orso, Valeria; Da Granaiola, Leonardo; Gamberini, Luciano] Univ Padua, Dept Gen Psychol, I-35122 Padua, Italy.
   [Orso, Valeria; Mingardi, Michele; Gamberini, Luciano] Univ Padua, Human Inspired Technol Res Ctr, I-35121 Padua, Italy.
C3 University of Verona; University of Verona; University of Padua;
   University of Padua
RP Caputo, A (corresponding author), Univ Verona, Dept Engn Med Innovat, I-37134 Verona, Italy.
EM ariel.caputo@univr.it
OI Bartolomioli, Riccardo/0000-0002-7938-4929
FU European Union Next-GenerationEU [ECS_00000043, 1058]
FX This study was partially carried out within the PNRR research activities
   of the consortium iNEST (Interconnected North-Est Innovation Ecosystem)
   funded by the European Union Next-GenerationEU (Piano Nazionale di
   Ripresa e Resilienza (PNRR) - Missione 4 Componente 2, Investimento 1.5
   - D.D. 1058 23/06/2022, ECS_00000043) .
CR Balakrishnan R., 2000, CHI 2000 Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2000. The Future is Here, P33, DOI 10.1145/332040.332404
   Bangor A, 2009, J USABILITY STUD, V4, P114
   Bergström J, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445193
   Bettio F., 2007, Euro-graphics Italian Chapter Conference, P145
   Bossavit B, 2015, PRESENCE-TELEOP VIRT, V23, P377, DOI 10.1162/PRES_a_00207
   Bowman D. A., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P35, DOI 10.1145/253284.253301
   Brooke J., 1996, USABILITY EVALUATION, P189, DOI DOI 10.1201/9781498710411-35
   Caputo A, 2023, SMART TOOLS APPL GRA, DOI [10.2312/stag.20231290, DOI 10.2312/STAG.20231290]
   Caputo A, 2020, IEEE INT C EMERG, P603, DOI 10.1109/ETFA46521.2020.9212043
   Caputo FabioMarco., 2015, P 11 BIANN C IT SIGC, P2
   Caputo FM, 2018, Comput Graph
   Chen LC, 2017, LECT NOTES COMPUT SC, V10278, P393, DOI 10.1007/978-3-319-58703-5_29
   Cho Isaac, 2015, 2015 IEEE Symposium on 3D User Interfaces (3DUI), P133, DOI 10.1109/3DUI.2015.7131738
   Flavián C, 2019, J BUS RES, V100, P547, DOI 10.1016/j.jbusres.2018.10.050
   GUIARD Y, 1987, J MOTOR BEHAV, V19, P486
   Hall E. T., 1966, The hidden dimension
   Jacob R.J. K., 1994, ACM Transactions Computer-Human Interaction, V1, P3, DOI DOI 10.1145/174630.174631
   Janusz J, 2019, IOP CONF SER-MAT SCI, V471, DOI 10.1088/1757-899X/471/10/102065
   Kang HJ, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P275, DOI [10.1109/VR46266.2020.00047, 10.1109/VR46266.2020.00-57]
   Kim MG, 2022, VISUAL COMPUT, V38, P3463, DOI 10.1007/s00371-022-02555-6
   Krichenbauer M, 2018, IEEE T VIS COMPUT GR, V24, P1038, DOI 10.1109/TVCG.2017.2658570
   Kulik A, 2020, IEEE T VIS COMPUT GR, V26, P2041, DOI 10.1109/TVCG.2020.2973034
   Mendes D, 2019, COMPUT GRAPH FORUM, V38, P21, DOI 10.1111/cgf.13390
   Mendes D, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P261, DOI 10.1145/2993369.2993396
   Mendes D, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P3, DOI 10.1109/3DUI.2014.6798833
   Microsoft, 2022, MRTK 2-Unity documentation.
   Norman D.A., 2010, interactions, V17, P6, DOI DOI 10.1145/1744161.1744163
   Oprea S, 2019, COMPUT GRAPH-UK, V83, P77, DOI 10.1016/j.cag.2019.07.003
   Pierce J. S., 1999, Proceedings 1999 Symposium on Interactive 3D Graphics, P141, DOI 10.1145/300523.300540
   Pohl H, 2021, P 2021 CHI C HUM FAC, P1
   SHOEMAKE K, 1992, GRAPH INTER, P151
   Song Peng., 2012, P 2012 ACM ANN C HUM, P1297, DOI DOI 10.1145/2207676.2208585
   Unity Technologies, 2022, Unity website.
   Weichel C, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P3855, DOI 10.1145/2556288.2557090
   Whitlock M, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P41, DOI 10.1109/VR.2018.8446381
NR 35
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103959
DI 10.1016/j.cag.2024.103959
EA JUN 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XD6A9
UT WOS:001259774100001
DA 2024-08-05
ER

PT J
AU Gan, HQ
   Wang, LH
   Su, YZW
   Ruan, WJ
   Jiao, XZ
AF Gan, Haiqing
   Wang, Lihui
   Su, Yuzuwei
   Ruan, Wenjun
   Jiao, Xize
TI Prior-information-guided corresponding point regression network for 6D
   pose estimation
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE 6D pose estimation; Deep learning; Corresponding point regression; RGB-D
   perception
AB A key step in bin -picking is to obtain the 6DoF pose of the object. However, One of the reasons why many pose estimation methods fail to deal with occlusions and noise is that they lack reliable correspondence and effective utilization of CAD model information. In order to solve such issue, we propose a new prior -information -guided 6D pose estimation network based on 3D -3D correspondence prediction. Specifically, the network establishes dense correspondences between the object and the CAD model by predicting the object's corresponding points on the CAD point cloud. This correspondence prediction method allows the network to learn more geometric details and perform better than direct regression. Furthermore, we implement the embedding of CAD model features in the network, which utilizes the prior information of the CAD model to compensate for the missing object information due to occlusion and viewpoint and provide more precise point regression. In addition, thanks to this points regression method, we can intuitively eliminate erroneous points via distance threshold judgment, thus achieving robustness to noise and outliers. Finally, the pose are then estimated through a SVD solver. Experiments show that our method outperforms most methods on LM, LM -O and YCB-V datasets, and its performance on LM -O datasets is 9% higher than the most advanced method. This proves that our method has excellent robustness and achieves the state-of-the-art performance.
C1 [Gan, Haiqing; Ruan, Wenjun; Jiao, Xize] Jiangsu Prov Elect Power Corp, Nanjing 210024, Peoples R China.
   [Wang, Lihui; Su, Yuzuwei] Southeast Univ, Nanjing 210096, Peoples R China.
C3 Southeast University - China
RP Su, YZW (corresponding author), Southeast Univ, Nanjing 210096, Peoples R China.
EM wangyi_sy1256@163.com
FU National Key Research and Devel-opment Program [2021YFB2501600]
FX This work was supported by the National Key Research and Devel-opment
   Program (2021YFB2501600) .
CR Brachmann E, 2019, IEEE I CONF COMP VIS, P4321, DOI 10.1109/ICCV.2019.00442
   Brachmann E, 2017, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2017.267
   Brachmann E, 2014, LECT NOTES COMPUT SC, V8690, P536, DOI 10.1007/978-3-319-10605-2_35
   Chang J, 2021, IEEE INT C INT ROBOT, P5749, DOI 10.1109/IROS51168.2021.9636459
   Chen W, 2020, PROC CVPR IEEE, P4232, DOI 10.1109/CVPR42600.2020.00429
   Drost B, 2010, PROC CVPR IEEE, P998, DOI 10.1109/CVPR.2010.5540108
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Haugaard RL, 2022, PROC CVPR IEEE, P6739, DOI 10.1109/CVPR52688.2022.00663
   He W., 2020, P IEEE CVF C COMP VI, P11629, DOI [10.1109/CVPR42600.2020.01165, DOI 10.1109/CVPR42600.2020.01165]
   He YS, 2021, PROC CVPR IEEE, P3002, DOI 10.1109/CVPR46437.2021.00302
   Hinterstoisser S, 2012, LECT NOTES COMPUT SC, V7585, P593, DOI 10.1007/978-3-642-33885-4_60
   Hodan Tomas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11700, DOI 10.1109/CVPR42600.2020.01172
   Hua WT, 2021, IEEE ROBOT AUTOM LET, V6, P2886, DOI 10.1109/LRA.2021.3062304
   Jocher G., 2023, Ultralytics YOLO
   Kehl W, 2017, IEEE I CONF COMP VIS, P1530, DOI 10.1109/ICCV.2017.169
   Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336
   Kleeberger K, 2020, IEEE INT CONF ROBOT, P6239, DOI [10.1109/icra40945.2020.9197207, 10.1109/ICRA40945.2020.9197207]
   Lee D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5663, DOI 10.1109/ICCV48922.2021.00563
   Li PL, 2019, PROC CVPR IEEE, P7636, DOI 10.1109/CVPR.2019.00783
   Li ZG, 2019, IEEE I CONF COMP VIS, P7677, DOI 10.1109/ICCV.2019.00777
   Liang M, 2018, LECT NOTES COMPUT SC, V11220, P663, DOI 10.1007/978-3-030-01270-0_39
   Liu X, 2022, Gdrnpp
   Liu YX, 2021, IEEE ROBOT AUTOM LET, V6, P919, DOI 10.1109/LRA.2021.3052442
   Marchand E, 2016, IEEE T VIS COMPUT GR, V22, P2633, DOI 10.1109/TVCG.2015.2513408
   Park K, 2019, IEEE I CONF COMP VIS, P7667, DOI 10.1109/ICCV.2019.00776
   Patten T, 2021, IEEE INT C INT ROBOT, P4831, DOI 10.1109/IROS51168.2021.9635884
   Peng SD, 2022, IEEE T PATTERN ANAL, V44, P3212, DOI 10.1109/TPAMI.2020.3047388
   Qi CR, 2017, ADV NEUR IN, V30
   Qian Guocheng, 2022, PointNeXt: Revisiting PointNet++ with improved training and scaling strategies
   Rad M, 2017, IEEE I CONF COMP VIS, P3848, DOI 10.1109/ICCV.2017.413
   Sahin C, 2020, IMAGE VISION COMPUT, V96, DOI 10.1016/j.imavis.2020.103898
   Song C, 2020, PROC CVPR IEEE, P428, DOI 10.1109/CVPR42600.2020.00051
   Su YZ, 2022, PROC CVPR IEEE, P6728, DOI 10.1109/CVPR52688.2022.00662
   Sundermeyer Martin, 2023, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P2785, DOI 10.1109/CVPRW59228.2023.00279
   Nguyen VN, 2022, PROC CVPR IEEE, P6761, DOI 10.1109/CVPR52688.2022.00665
   Wang C, 2019, PROC CVPR IEEE, P3338, DOI 10.1109/CVPR.2019.00346
   Wang G, 2021, PROC CVPR IEEE, P16606, DOI 10.1109/CVPR46437.2021.01634
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Xiang Y, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV
   Zakharov S, 2019, IEEE I CONF COMP VIS, P1941, DOI 10.1109/ICCV.2019.00203
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou GY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2773, DOI 10.1109/ICCV48922.2021.00279
   Zhou Y, 2019, PROC CVPR IEEE, P5738, DOI 10.1109/CVPR.2019.00589
NR 43
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD JUN
PY 2024
VL 121
AR 103950
DI 10.1016/j.cag.2024.103950
EA JUN 2024
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WA3Z9
UT WOS:001252122500001
DA 2024-08-05
ER

PT J
AU Sun, PY
   Hou, ML
   Lyu, S
   Wang, WF
   Shaker, A
   Li, SN
AF Sun, Pengyu
   Hou, Miaole
   Lyu, Shuqiang
   Wang, Wanfu
   Shaker, Ahmed
   Li, Songnian
TI Virtual cleaning of sooty murals in ancient temples using twice colour
   attenuation prior
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Mural; Soot; Virtual cleaning; Inverse operation; Colour attenuation
   prior; Hyperspectral image
ID IMAGE; CALIBRATION; PAINTINGS; REMOVAL
AB Murals are artworks painted on walls that represent various cultures, traditions, historical periods, spiritual narratives and civilisations. Unfortunately, before modern preventive measures were implemented, murals suffered considerable degradation from accumulating substances, such as incense oil and carbon, leading to a darkening or blackening phenomenon called soot. The soot severely and extensively damaged the murals' colour and motifs. However, no highly effective remedy is available to restore them to their original state. A novel approach for virtual cleaning of this degradation uses hyperspectral and image enhancement technologies. Existing techniques for sooty mural image restoration primarily use dark channel prior and Retinex bilateral filtering. During this process, the dark channel prior estimates the concentration of soot and removes it, restoring the mural's colour with Retinex bilateral filtering. However, this restoration method has issues, such as neglecting the difference between soot and haze, resulting in a generally darker mural image. This result requires brightness adjustment, and the method presents further challenges, such as texture loss, colour distortion in some areas and a reduction in clarity. Therefore, we propose a new technique that integrates an inverse operation and twice colour attenuation prior to cleaning sooty murals. This technique involves normalised cross -correlation to match and stitch images to the different synthesised bands to reconstruct the mural image to be processed, which removes black marks from areas of mural loss. The sooty mural then undergoes an inverse operation, and the colour attenuation prior is used on the inverse image. The processed mural is then inverted again, transforming the colour of soot to white. Finally, the colour attenuation prior is again applied to the colour -changing mural, enabling virtual cleaning of the sooty murals. The results confirm the enhanced adaptability of our novel approach for the virtual cleaning of sooty murals. Moreover, the proposed method exhibits superior performance for various performance parameters.
C1 [Sun, Pengyu; Hou, Miaole; Lyu, Shuqiang] Beijing Univ Civil Engn & Architecture, Sch Elect & Informat Engn, 15 Yongyuan Rd, Beijing 102616, Peoples R China.
   [Sun, Pengyu; Hou, Miaole; Lyu, Shuqiang] Beijing Key Lab Architectural Heritage Fine Recons, 15 Yongyuan Rd, Beijing 102616, Peoples R China.
   [Wang, Wanfu] Dunhuang Acad, Dunhuang 736200, Peoples R China.
   [Shaker, Ahmed; Li, Songnian] Toronto Metropolitan Univ, 350 Victoria St, Toronto, ON M5B 2K3, Canada.
C3 Beijing University of Civil Engineering & Architecture; Toronto
   Metropolitan University
RP Lyu, S (corresponding author), Beijing Univ Civil Engn & Architecture, Sch Elect & Informat Engn, 15 Yongyuan Rd, Beijing 102616, Peoples R China.; Lyu, S (corresponding author), Beijing Key Lab Architectural Heritage Fine Recons, 15 Yongyuan Rd, Beijing 102616, Peoples R China.
EM lvshuqiang@bucea.edu.cn
OI Sun, pengyu/0009-0005-5319-1925
FU National Key R and D Program of China [2022YFF0904400]; National Natural
   Science Foundation of China [42171356, 42171444]
FX The research project "Protection of Murals in Qutan Temple"was conducted
   in collaboration with the Dunhuang Academy, which generously provided
   their expertise and research resources to support this study. The author
   expresses sincere gratitude to the dedicated staff members of both
   Dunhuang Academy and Qutan Temple for their valuable contributions. This
   research was supported by the National Key R and D Program of China (No.
   2022YFF0904400) and the National Natural Science Foundation of China
   (No.42171356, No.42171444) .
CR Al-Emam E, 2021, HERIT SCI, V9, DOI 10.1186/s40494-020-00473-1
   Amiri MM, 2023, HERIT SCI, V11, DOI 10.1186/s40494-023-00859-x
   Amiri MM, 2021, HERIT SCI, V9, DOI 10.1186/s40494-021-00567-4
   Bruneton E, 2008, COMPUT GRAPH FORUM, V27, P1079, DOI 10.1111/j.1467-8659.2008.01245.x
   Burger J, 2005, J CHEMOMETR, V19, P355, DOI 10.1002/cem.938
   Cao N, 2021, HERIT SCI, V9, DOI 10.1186/s40494-021-00504-5
   Eppenberger PE, 2020, EUR RADIOL EXP, V4, DOI 10.1186/s41747-020-00166-1
   Hou ML, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9173591
   Kansal I, 2020, MULTIMED TOOLS APPL, V79, P12069, DOI 10.1007/s11042-019-08240-6
   Kaya B, 2019, IEEE INT CONF COMP V, P3546, DOI 10.1109/ICCVW.2019.00439
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li Y, 2017, J RAMAN SPECTROSC, V48, P1479, DOI 10.1002/jrs.5158
   Li YH, 2023, OPT LASER TECHNOL, V157, DOI 10.1016/j.optlastec.2022.108679
   Linhares J, 2020, CONSERV PATRIM, P50, DOI 10.14568/cp2018064
   Munoz-Pandiella I, Comput Graph
   Nielsen AA, 2011, IEEE T IMAGE PROCESS, V20, P612, DOI 10.1109/TIP.2010.2076296
   Nishita T., 1993, Computer Graphics Proceedings, P175, DOI 10.1145/166117.166140
   Nishita Tomoyuki., 1987, COMPUTER GRAPHICS P, V21, P303, DOI [10.1145/37402.37437, DOI 10.1145/37401.37437]
   Niu H, 2022, Sci Conserv Archaeol, V34, P53, DOI [10.16334/j.cnki.cn31-1652/k.20211202342, DOI 10.16334/J.CNKI.CN31-1652/K.20211202342]
   Palomero CMT, 2011, OPT EXPRESS, V19, P21011, DOI 10.1364/OE.19.021011
   Polder G, 2003, J NEAR INFRARED SPEC, V11, P193, DOI 10.1255/jnirs.366
   Rakhimol V, 2022, COMPUT GRAPH-UK, V109, P100, DOI 10.1016/j.cag.2022.11.001
   Sun PY, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22249780
   Tang KT, 2014, PROC CVPR IEEE, P2995, DOI 10.1109/CVPR.2014.383
   Trumpy G, 2015, OPT EXPRESS, V23, P33836, DOI 10.1364/OE.23.033836
   Tsang Js, 2011, PREPR ICOM CC INT CO
   Wang YH, 2023, HERIT SCI, V11, DOI 10.1186/s40494-023-00904-9
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 28
TC 1
Z9 1
U1 4
U2 4
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD MAY
PY 2024
VL 120
AR 103924
DI 10.1016/j.cag.2024.103924
EA MAY 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TB9D2
UT WOS:001238906100001
DA 2024-08-05
ER

PT J
AU Chen, YZ
   Han, YS
   Chen, JY
   Ma, SQ
   Fedkiw, R
   Teran, J
AF Chen, Yizhou
   Han, Yushan
   Chen, Jingyu
   Ma, Shiqian
   Fedkiw, Ronald
   Teran, Joseph
TI Primal residual reduction with extended position based dynamics and
   hyperelasticity
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Position-based dynamics; Physics simulation; Constrained dynamics
ID ANIMATION; MUSCLE; MODEL
AB The Extended Position Based Dynamics (XPBD) approach of Macklin et al. (2016) addresses issues with iteration -dependent behavior in the original Position Based Dynamics (M & uuml;ller et al., 2007) (PBD). PBD itself is a powerful method for the real-time simulation of elastic objects, however, it is limited in its application to hyperelastic solids. It can only treat models with a strain energy density that is quadratic in some notion of constraint. Furthermore, we show that even when applicable the formulation does not always lead to convergent behaviors with hyperelasticity. We isolate the root cause to be the approximate linearization of the nonlinear backward Euler systems utilized by XPBD. We provide two fixes to these terms that allow for convergent behavior. The first (B-PXPBD) is a small modification to an existing XPBD code, but can only be used with models addressable by the original XPBD. The second (FP-PXPBD) is a more general formulation that extends XPBD (and our residual correction) to arbitrary hyperelasticity. We show that our modifications allow for convergent behavior that rivals accurate techniques like Newton's method when the computational budget is large without sacrificing the stable and robust behavior exhibited by the original PBD and XPBD when the computational budget is limited.
C1 [Chen, Yizhou; Han, Yushan; Chen, Jingyu] Univ Calif Los Angeles, Los Angeles, CA 90095 USA.
   [Chen, Yizhou; Han, Yushan; Fedkiw, Ronald; Teran, Joseph] Epic Games Inc, Cary, NC 27518 USA.
   [Ma, Shiqian] Rice Univ, Houston, TX USA.
   [Fedkiw, Ronald] Stanford Univ, Stanford, CA USA.
   [Teran, Joseph] Univ Calif Davis, Davis, CA USA.
C3 University of California System; University of California Los Angeles;
   Rice University; Stanford University; University of California System;
   University of California Davis
RP Chen, YZ (corresponding author), Univ Calif Los Angeles, Los Angeles, CA 90095 USA.; Chen, YZ (corresponding author), Epic Games Inc, Cary, NC 27518 USA.
EM chenyizhou@ucla.edu
OI Chen, Yizhou/0009-0000-2439-7880
CR Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Barbic J, 2005, ACM T GRAPHIC, V24, P982, DOI 10.1145/1073204.1073300
   Batty C, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276502
   Bertsekas D., 1995, Journal of the Operational Research Society, DOI [10.1057/palgrave.jors.2600425, DOI 10.1057/PALGRAVE.JORS.2600425]
   Blemker SS, 2005, J BIOMECH, V38, P657, DOI 10.1016/j.jbiomech.2004.04.009
   Bonet J, 2008, NONLINEAR CONTINUUM MECHANICS FOR FINITE ELEMENT ANALYSIS, 2ND EDITION, P1, DOI 10.1017/CBO9780511755446
   Bouaziz S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601116
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Chao I, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778775
   Chen Y, 2023, Supplementary technical document
   Chen YZ, 2023, 15TH ANNUAL ACM SIGGRAPH CONFERENCE ON MOTION, INTERACTION AND GAMES, MIG 2023, DOI 10.1145/3623264.3624437
   Fan Y, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601215
   Fratarcangeli M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982437
   Gast T., 2016, IMPLICIT SHIFTED SYM
   Gast TF, 2015, IEEE T VIS COMPUT GR, V21, P1103, DOI 10.1109/TVCG.2015.2459687
   Geoffrey Irving, 2004, P S COMP AN, P131, DOI DOI 10.1145/1028523.1028541
   Goldenthal R, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276438
   HAGER WW, 1989, SIAM REV, V31, P221, DOI 10.1137/1031049
   Hecht F, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2231816.2231821
   Jiang C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766996
   Kharevych L., 2006, P 2006 ACM SIGGRAPHE, P43
   Kim T, 2021, P 14 ACM SIGGRAPH C, P7
   Kim T, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323014
   Kovalsky SZ, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925920
   Kwatra N, 2009, J COMPUT PHYS, V228, P4146, DOI 10.1016/j.jcp.2009.02.027
   Kwatra Nipun., 2010, ACM SIGGRAPH/Eurographics Symp. on Comput. Anim, P207
   Li MC, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322951
   Liu LG, 2008, COMPUT GRAPH FORUM, V27, P1495, DOI 10.1111/j.1467-8659.2008.01290.x
   Liu TT, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2990496
   Liu TT, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508406
   Macklin Miles, 2021, MIG '21: Motion, Interaction and Games, DOI 10.1145/3487983.3488289
   Macklin Miles, 2016, P 9 INT C MOTION GAM, P49, DOI 10.1145/2994258.2994272
   Martin S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964967
   McAdams A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964932
   Modi V, 2021, COMPUT GRAPH FORUM, V40, P234, DOI 10.1111/cgf.14185
   Müller M, 2007, J VIS COMMUN IMAGE R, V18, P109, DOI 10.1016/j.jvcir.2007.01.005
   Müller M, 2005, ACM T GRAPHIC, V24, P471, DOI 10.1145/1073204.1073216
   Müller M, 2004, PROC GRAPH INTERF, P239
   Narain M., 2016, P ACM SIGGRAPH EUR S, P21
   NEUBERGER JW, 1985, J MATH SOC JPN, V37, P187
   Nocedal J, 2006, SPRINGER SER OPER RE, P101
   O'Brien JF, 2002, ACM T GRAPHIC, V21, P291, DOI 10.1145/566570.566579
   O'Brien JF, 1999, COMP GRAPH, P137, DOI 10.1145/311535.311550
   PROVOT X, 1995, GRAPH INTER, P147
   Rabinovich M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2983621
   Sifakis E, 2012, ACM SIGGRAPH 2012 CO, P1, DOI 10.1145/2343483.2343501
   Smith Breannan, 2018, ACM Transactions on Graphics, V37, DOI 10.1145/3180491
   Smith B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3241041
   Sorkine O, 2007, Proc. Symposium on Geometry Processing, V4, P109, DOI [DOI 10.1145/1281991.1282006, 10.1145/1073204.1073323]
   Stern Ari, 2006, ACM SIGGRApH 2006 courses, P75
   Stomakhin A, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601176
   Stomakhin Alexey., 2012, Proc. Symp. Comp. Anim, P25
   Teran J., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P68
   Teran J, 2005, IEEE T VIS COMPUT GR, V11, P317, DOI 10.1109/TVCG.2005.42
   Teran J., 2005, Proc_2005_ACM_SIGGRAPH/Eurograph_Symp_Comp Anim, P181
   Tournier M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766969
   Turk G., 1994, Stanford bunny
   Wang B, 2020, COMPUT GRAPH FORUM, V39, P69, DOI 10.1111/cgf.14127
   Wang HM, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980236
   Wang HM, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818063
   Xu HY, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2699648
   Zhao D., 2016, Asynchronous implicit backward Euler integration
   Zhu YF, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201359
NR 63
TC 0
Z9 0
U1 1
U2 1
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103902
DI 10.1016/j.cag.2024.103902
EA MAR 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PX7U6
UT WOS:001217452800001
DA 2024-08-05
ER

PT J
AU Trunz, E
   Klein, J
   Müller, J
   Bode, L
   Sarlette, R
   Weinmann, M
   Klein, R
AF Trunz, Elena
   Klein, Jonathan
   Mueller, Jan
   Bode, Lukas
   Sarlette, Ralf
   Weinmann, Michael
   Klein, Reinhard
TI Neural inverse procedural modeling of knitting yarns from images
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Inverse procedural modeling; Model fitting; Yarn modeling; Neural
   networks
AB We investigate the capabilities of neural inverse procedural modeling to infer high-quality procedural yarn models with fiber-level details from single images of depicted yarn samples. While directly inferring all parameters of the underlying yarn model based on a single neural network may seem an intuitive choice, we show that the complexity of yarn structures in terms of twisting and migration characteristics of the involved fibers can be better encountered in terms of ensembles of networks that focus on individual characteristics. We analyze the effect of different loss functions including a parameter loss to penalize the deviation of inferred parameters to ground truth annotations, a reconstruction loss to enforce similar statistics of the image generated for the estimated parameters in comparison to training images as well as an additional regularization term to explicitly penalize deviations between latent codes of synthetic images and the average latent code of real images in the encoder's latent space. We demonstrate that the combination of a carefully designed parametric, procedural yarn model with respective network ensembles as well as loss functions even allows robust parameter inference when solely trained on synthetic data. Since our approach relies on the availability of a yarn database with parameter annotations and we are not aware of such a respectively available dataset, we additionally provide, to the best of our knowledge, the first dataset of yarn images with annotations regarding the respective yarn parameters. For this purpose, we use a novel yarn generator that improves the realism of the produced results over previous approaches.
C1 [Trunz, Elena; Klein, Jonathan; Mueller, Jan; Bode, Lukas; Sarlette, Ralf; Klein, Reinhard] Univ Bonn, Bonn, Germany.
   [Klein, Jonathan] KAUST, Thuwal, Saudi Arabia.
   [Weinmann, Michael] Delft Univ Technol, Delft, Netherlands.
   [Weinmann, Michael] Visual Comp Dept, FriedrichHirzebruch-Allee 5, D-53115 Bonn, Germany.
C3 University of Bonn; King Abdullah University of Science & Technology;
   Delft University of Technology
RP Weinmann, M (corresponding author), Visual Comp Dept, FriedrichHirzebruch-Allee 5, D-53115 Bonn, Germany.
EM trunz@cs.uni-bonn.de; kleinj@cs.uni-bonn.de;
   muellerj@informatik.uni-bonn.de; lbode@cs.uni-bonn.de;
   sarlette@cs.uni-bonn.de; M.Weinmann@tudelft.nl; rk@cs.uni-bonn.de
RI Bode, Lukas/IXN-2967-2023
OI Bode, Lukas/0000-0002-8710-8561; Weinmann, Michael/0000-0003-3634-0093
CR Aliaga C, 2017, COMPUT GRAPH FORUM, V36, P35, DOI 10.1111/cgf.13222
   Amor N, 2021, POLYMERS-BASEL, V13, DOI 10.3390/polym13162592
   Bouman KL, 2013, IEEE I CONF COMP VIS, P1984, DOI 10.1109/ICCV.2013.455
   Castillo C, 2017, P WORKSH MAT APP MOD, P21
   Castillo C, 2019, COMPUT GRAPH-UK, V84, P103, DOI 10.1016/j.cag.2019.07.007
   Chiang MJY, 2016, COMPUT GRAPH FORUM, V35, P275, DOI 10.1111/cgf.12830
   Cirio G, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661279
   Dana KJ, 1997, P IEEE COMP SOC C CO
   Dobashi Y, 2019, VISUAL COMPUT, V35, P175, DOI 10.1007/s00371-017-1455-9
   Dong Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778835
   Drago F, 2004, VISUAL COMPUT, V20, P314, DOI 10.1007/s00371-004-0240-8
   Filip J, 2018, The visual computer (computer graphics international 2018)
   Gong D, 2022, arXiv, DOI DOI 10.48550/ARXIV.2202.00504
   Guarnera GC, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3132187
   He KM, 2015, Arxiv, DOI [arXiv:1512.03385, DOI 10.48550/ARXIV.1512.03385]
   Irawan P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2077341.2077352
   Jakob W, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778790
   Jin W, 2022, SIGGRAPH AS 2022 C P, P1
   Kaldor JM, 2011, Simulating yarn-based cloth
   Kaldor JM, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360664
   Kaspar A, 2019, PR MACH LEARN RES, V97
   KEEFE M, 1994, J TEXT I, V85, P338, DOI 10.1080/00405009408631278
   Khungurn P, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2998578
   Khungurn P, 2015, ACM T GRAPHIC, V35, DOI 10.1145/2818648
   Kingma D.P., 2015, PROC INT C LEARN RE
   Li YF, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3527660
   Liang JB, 2019, ADV NEUR IN, V32
   Liu Z., 2022, arXiv, DOI DOI 10.48550/ARXIV.2201.03545
   Miguel E, 2018, CEIG, P21
   Mohammadi SO, 2021, arXiv, DOI DOI 10.48550/ARXIV.2111.00905
   Montazeri Z, 2021, CoRR abs/2105.02475
   Montazeri Z, 2021, Google Patents US Patent, Patent No. [11,049,291, 11049291]
   Montazeri Z, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417777
   Morris PJ, 1999, J TEXT I, V90, P322
   Nishida G, 2018, COMPUT GRAPH FORUM, V37, P415, DOI 10.1111/cgf.13372
   Noor A, 2022, J TEXT I, V113, P505, DOI 10.1080/00405000.2021.1880088
   Pagán EA, 2020, SUSTAINABILITY-BASEL, V12, DOI 10.3390/su12198279
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   Rasheed A. H., 2020, P IEEECVF C COMPUTER, P9912
   Runia T. F., 2020, P IEEECVF C COMPUTER, P10498, DOI 10.1109/cvpr42600.2020.010519
   Saalfeld A, 2018, WORKSH MAT APP MOD E, DOI [10.2312/mam20181194, DOI 10.2312/MAM20181194]
   Sadeghi I, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2451236.2451240
   Schroder K, 2012, SIGGRAPH ASIA 2012 C, DOI DOI 10.1145/2407783.2407795
   Schröder K, 2011, COMPUT GRAPH FORUM, V30, P1277, DOI 10.1111/j.1467-8659.2011.01987.x
   Schröder K, 2015, IEEE T VIS COMPUT GR, V21, P188, DOI 10.1109/TVCG.2014.2339831
   Shinohara T, 2010, TEXT RES J, V80, P623, DOI 10.1177/0040517509342320
   Simonyan K, 2014, Arxiv, DOI [arXiv:1312.6034, DOI 10.48550/ARXIV.1312.6034]
   Tao XM, 1996, TEXT RES J, V66, P754
   Trunz E, 2019, PROC CVPR IEEE, P8622, DOI 10.1109/CVPR.2019.00883
   Voborova J, 2004, 2 INT TEXT CLOTH DES
   Wang J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330512
   Weinmann M, 2014, LECT NOTES COMPUT SC, V8691, P156, DOI 10.1007/978-3-319-10578-9_11
   Wu HY, 2019, FRONT INFORM TECH EL, V20, P1165, DOI 10.1631/FITEE.1800693
   Wu K, 2019, IEEE T VIS COMPUT GR, V25, P1297, DOI 10.1109/TVCG.2017.2731949
   Xu YQ, 2001, COMP GRAPH, P391
   Yuksel C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185533
   Zhao S, 2014, Modeling and rendering fabrics at micron -resolution
   Zhao S, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925932
   Zhao S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461938
   Zhao S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185571
   Zhao S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964939
NR 61
TC 1
Z9 1
U1 2
U2 2
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD FEB
PY 2024
VL 118
BP 161
EP 172
DI 10.1016/j.cag.2023.12.013
EA JAN 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HH3J1
UT WOS:001158559000001
OA Green Submitted, Green Published, hybrid
DA 2024-08-05
ER

PT J
AU Gerbaud, S
   Cavalier, A
   Horna, S
   Zrour, R
   Naudin, M
   Guillevin, C
   Meseure, P
AF Gerbaud, Sylvain
   Cavalier, Arthur
   Horna, Sebastien
   Zrour, Rita
   Naudin, Mathieu
   Guillevin, Carole
   Meseure, Philippe
TI Topological 3D reconstruction of multiple anatomical structures from
   volumetric medical data
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Volumetric reconstruction; Generalized maps; Topology-based modeling;
   Medical images
ID SURFACE RECONSTRUCTION; CEREBRAL-CORTEX; SEGMENTATION; ACCURACY; IMAGES
AB In the medical field, usually, practitioners mainly base their analysis on 2D slices produced from MRI or CT -scans that correspond to restricted views of a pathology. To facilitate the work of doctors, increase diagnostic accuracy and cross-reference multimodal data, a 3D reconstruction is required. However, most of the time, reconstruction methods fail at visualizing complex and noisy data made up of several tissues. Indeed, these methods often build each tissue independently so that the consistency of the global model is not ensured: overlaps may appear between segments whereas some disjointed volumes exhibit empty spaces. This paper presents a complete topologically consistent reconstruction system from 3D medical acquisitions such as MRI or CT -scans. Compared to other methods, our system offers a single volumetric representation of an organ corresponding to a 3D space partition, where a semantic label is associated to each volume to identify the represented tissue and adjacency between volumes is explicitly and precisely defined. This partition is controlled and free from topological and geometric defects usually found in other 3D reconstruction approaches. Experimental studies were conducted on MRI datasets of brains resulting in consistent reconstructions. An application of the model for calculating the distribution of physiological data in brain tissue is also shown.
C1 [Gerbaud, Sylvain; Horna, Sebastien; Zrour, Rita; Naudin, Mathieu; Guillevin, Carole] Univ Poitiers, Univ Hosp Ctr, CNRS, Siemens Healthineers,Labcom I3M, F-86000 Poitiers, France.
   [Gerbaud, Sylvain; Cavalier, Arthur; Horna, Sebastien; Zrour, Rita; Meseure, Philippe] Univ Poitiers, CNRS UMR 7252, XLIM, F-86000 Poitiers, France.
   [Naudin, Mathieu; Guillevin, Carole] Univ Poitiers, Lab Appl Math LMA, CNRS UMR 7348, F-86000 Poitiers, France.
C3 CHU Poitiers; Universite de Poitiers; Centre National de la Recherche
   Scientifique (CNRS); Universite de Poitiers; Centre National de la
   Recherche Scientifique (CNRS); CNRS - Institute for Engineering &
   Systems Sciences (INSIS); Universite de Poitiers
RP Gerbaud, S (corresponding author), Univ Poitiers, Univ Hosp Ctr, CNRS, Siemens Healthineers,Labcom I3M, F-86000 Poitiers, France.
EM sylvain.gerbaud@univ-poitiers.fr; arthur.cavalier@univ-poitiers.fr;
   sebastien.horna@univ-poitiers.fr; rita.zrour@univ-poitiers.fr;
   mathieu.naudin@chu-poitiers.fr; carole.guillevin@chu-poitiers.fr;
   philippe.meseure@uinv-poitiers.fr
FU Region Nouvelle-Aquitaine [AAPR2020A-2020-8595610]; SIEMENS Healthineers
FX This research is funded by Region Nouvelle-Aquitaine
   AAPR2020A-2020-8595610, France and SIEMENS Healthineers.
CR Alliez P, 2022, CGAL user and reference manual
   Baumgart BruceG., 1975, P MAY 19 22 1975 NAT, P589, DOI [DOI 10.1145/1499949.1500071, 10.1145/1499949.1500071]
   Bazin PL, 2007, COMPUT METH PROG BIO, V88, P182, DOI 10.1016/j.cmpb.2007.08.006
   Belhaouari H, 2018, Comput-Aided Des Appl., V16
   Bongratz F, 2022, PROC CVPR IEEE, P20741, DOI 10.1109/CVPR52688.2022.02011
   Braude I, 2007, GRAPH MODELS, V69, P139, DOI 10.1016/j.gmod.2006.09.007
   Bruel-Gabrielsson R, 2020, COMPUT GRAPH FORUM, V39, P197, DOI 10.1111/cgf.14079
   Chen ZQ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480518
   Cheng QQ, 2020, COMPUT METH PROG BIO, V193, DOI 10.1016/j.cmpb.2020.105495
   Chernyaev Evgeni, 1995, P GRAPHICON 95
   Cruz RS, 2021, IEEE WINT CONF APPL, P806, DOI 10.1109/WACV48630.2021.00085
   Custodio L, 2013, COMPUT GRAPH-UK, V37, P840, DOI 10.1016/j.cag.2013.04.004
   Dale AM, 1999, NEUROIMAGE, V9, P179, DOI 10.1006/nimg.1998.0395
   Damiand G, 2003, LECT NOTES COMPUT SC, V2886, P408
   Damiand G., 2014, Combinatorial Maps: Efficient Data Structures for Computer Graphics and Image Processing
   de Araújo BR, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2732197
   Fang XZ, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530149
   Fedorov A, 2012, MAGN RESON IMAGING, V30, P1323, DOI 10.1016/j.mri.2012.05.001
   Fischl B, 2004, CEREB CORTEX, V14, P11, DOI 10.1093/cercor/bhg087
   Fischl B, 2002, NEURON, V33, P341, DOI 10.1016/S0896-6273(02)00569-X
   Fischl B, 2000, P NATL ACAD SCI USA, V97, P11050, DOI 10.1073/pnas.200033797
   Frisken SF, 2022, J Comput Graph Tech., V11
   Gonzalez Ballester MA, 2002, MED IMAGE ANAL, V6, P389, DOI 10.1016/S1361-8415(02)00061-0
   Grossner E, 2021, Preprints
   Grossner EC, 2018, NEUROPSYCHOLOGY, V32, P484, DOI 10.1037/neu0000446
   Grosso R, 2020, PROCEEDINGS OF THE 15TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 1: GRAPP, P102, DOI 10.5220/0008948701020112
   Grosso R, 2016, COMPUT GRAPH FORUM, V35, P187, DOI 10.1111/cgf.12975
   Han X, 2004, NEUROIMAGE, V23, P997, DOI 10.1016/j.neuroimage.2004.06.043
   Han X, 2003, IEEE T PATTERN ANAL, V25, P755, DOI 10.1109/TPAMI.2003.1201824
   Heller N, 2021, MED IMAGE ANAL, V67, DOI 10.1016/j.media.2020.101821
   Heller Nicholas, 2019, ARXIV190400445
   Henschel L, 2020, NEUROIMAGE, V219, DOI 10.1016/j.neuroimage.2020.117012
   Hohne K, 1992, Medical images: formation, handling and evaluation, V98, P145
   Jamin C, 2015, ACM T MATH SOFTWARE, V41, DOI 10.1145/2699463
   Kigka VI, 2018, BIOMED SIGNAL PROCES, V40, P286, DOI 10.1016/j.bspc.2017.09.009
   KIMIA BB, 1995, INT J COMPUT VISION, V15, P189, DOI 10.1007/BF01451741
   KONG TY, 1989, COMPUT VISION GRAPH, V48, P357, DOI 10.1016/0734-189X(89)90147-3
   Lazar R, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201348
   Lebrat L, 2021, ADV NEUR IN, V34
   Lewiner T., 2003, Journal of Graphics Tools, V8, P1, DOI 10.1080/10867651.2003.10487582
   Liao YY, 2018, PROC CVPR IEEE, P2916, DOI 10.1109/CVPR.2018.00308
   Lienhardt P, 1994, INT J COMPUT GEOM AP, V4, P275, DOI 10.1142/S0218195994000173
   Lorensen H. E., 1987, Proc. SIGGRAPH, V21, P163, DOI 10.1145/37401.37422
   Lu S, 2021, J Phys Conf Ser., P1732
   Ma Q, 2021, LECT NOTES COMPUT SC, V13001, P73, DOI 10.1007/978-3-030-87586-2_8
   MacDonald D, 2000, NEUROIMAGE, V12, P340, DOI 10.1006/nimg.1999.0534
   Maret D, 2012, DENTOMAXILLOFAC RAD, V41, P649, DOI 10.1259/dmf/81804525
   Nielson GM, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P489, DOI 10.1109/VISUAL.2004.28
   Nielson GM, 2003, IEEE T VIS COMPUT GR, V9, P283, DOI 10.1109/TVCG.2003.1207437
   Paiva A, 2006, SIBGRAPI, P205
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Quadrelli S, 2016, MAGN RESON INSIGHTS, V9, P1, DOI 10.4137/MRI.S32903
   Rineau L, 2023, CGAL 3D surface mesh generation user manual
   Schroeder W, 2015, SYMP LARG DATA ANAL, P33, DOI 10.1109/LDAV.2015.7348069
   Ségonne F, 2004, NEUROIMAGE, V22, P1060, DOI 10.1016/j.neuroimage.2004.03.032
   Segonne F, 2008, INT J COMPUT VISION, V79, P107, DOI 10.1007/s11263-007-0102-8
   Smith SM, 2002, HUM BRAIN MAPP, V17, P143, DOI 10.1002/hbm.10062
   The CGAL Project, 2013, CGAL user and reference manual, V4
   Trettner P, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530181
   Vidil F, 2002, Moka: 3D topological modeler
   Weiler K, 1986, P IFIP WG 5 2 WORK C, P3
   Yamina Y, 2020, Biomed Signal Process Control., V64, P997
   Zhang YY, 2001, IEEE T MED IMAGING, V20, P45, DOI 10.1109/42.906424
NR 63
TC 0
Z9 0
U1 8
U2 8
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD JUN
PY 2024
VL 121
AR 103947
DI 10.1016/j.cag.2024.103947
EA JUN 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UX7O0
UT WOS:001251430700001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Wang, P
   Liu, Y
   Lin, GY
   Gu, JT
   Liu, LJ
   Komura, T
   Wang, WP
AF Wang, Peng
   Liu, Yuan
   Lin, Guying
   Gu, Jiatao
   Liu, Lingjie
   Komura, Taku
   Wang, Wenping
TI ProLiF: Progressively-connected Light Field network for efficient view
   synthesis
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Neural rendering; View synthesis; Light field
AB This paper presents a simple yet practical network architecture, ProLiF ( Pro gressively -connected Li ght F ield network), for the efficient differentiable view synthesis of complex forward -facing scenes in both the training and inference stages. The progress of view synthesis has advanced significantly due to the recent Neural Radiance Fields (NeRF). However, when training a NeRF, hundreds of network evaluations are required to synthesize a single pixel color, which is highly consuming of device memory and time. This issue prevents the differentiable rendering of a large patch of pixels in the training stage for semantic -level supervision, which is critical for many practical applications such as robust scene fitting, style transferring, and adversarial training. On the contrary, our proposed simple architecture ProLiF, encodes a two -plane light field, which allows rendering a large batch of rays in one training step for image- or patch -level losses. To keep the multi -view 3D consistency of the neural light field, we propose a progressive training strategy with novel regularization losses. We demonstrate that ProLiF has good compatibility with LPIPS loss to achieve robustness to varying light conditions, and NNFM loss as well as CLIP loss to edit the rendering style of the scene.
C1 [Wang, Peng; Liu, Yuan; Lin, Guying; Liu, Lingjie; Komura, Taku] Univ Hong Kong, Hong Kong, Peoples R China.
   [Gu, Jiatao] Apple, Cupertino, CA USA.
   [Liu, Lingjie] Univ Penn, Philadelphia, PA USA.
   [Wang, Wenping] Texas A&M Univ, PETR 416,400 Bizzell St, College Stn, TX 77843 USA.
C3 University of Hong Kong; Apple Inc; University of Pennsylvania; Texas
   A&M University System; Texas A&M University College Station
RP Wang, WP (corresponding author), Texas A&M Univ, PETR 416,400 Bizzell St, College Stn, TX 77843 USA.
EM pwang3@cs.hku.hk; yuanly@connect.hku.hk; guying@connect.hku.hk;
   jgu32@apple.com; lingjie.liu@seas.upenn.edu; taku@cs.hku.hk;
   wenping@tamu.edu
FU Innovation and Technology Commission of the HKSAR Government; Hong Kong
   RGC [T45-205/21-N]
FX This research is partially supported by the Innovation and Technology
   Commission of the HKSAR Government under the InnoHK initiative and Ref.
   T45-205/21-N of Hong Kong RGC.
CR Aliev Kara-Ali, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P696, DOI 10.1007/978-3-030-58542-6_42
   Attal B, 2022, Arxiv, DOI arXiv:2112.01523
   Barron JT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5835, DOI 10.1109/ICCV48922.2021.00580
   Barron Jonathan T, 2023, ARXIV
   Cao JL, 2023, Arxiv, DOI arXiv:2212.08057
   Cau JL, 2023, PROC CVPR IEEE, P8328, DOI 10.1109/CVPR52729.2023.00805
   Chan ER, 2022, PROC CVPR IEEE, P16102, DOI 10.1109/CVPR52688.2022.01565
   Chan ER, 2021, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR46437.2021.00574
   Chang A. X., 2022, arXiv
   Chaurasia G, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487238
   Chen AP, 2022, LECT NOTES COMPUT SC, V13692, P333, DOI 10.1007/978-3-031-19824-3_20
   Cho J, 2022, LECT NOTES COMPUT SC, V13680, P595, DOI 10.1007/978-3-031-20044-1_34
   Choi I, 2019, IEEE I CONF COMP VIS, P7780, DOI 10.1109/ICCV.2019.00787
   Davis A, 2012, COMPUT GRAPH FORUM, V31, P305, DOI 10.1111/j.1467-8659.2012.03009.x
   Fang Jiemin, 2021, arXiv
   Fridovich-Keil S, 2022, PROC CVPR IEEE, P5491, DOI 10.1109/CVPR52688.2022.00542
   Garbin SJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14326, DOI 10.1109/ICCV48922.2021.01408
   Girshick R., 2015, Proceedings of the IEEE international conference on computer vision, DOI [DOI 10.1109/ICCV.2015.169, 10.1109/ICCV.2015.169]
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   Gu Jiatao, 2022, ICLR
   Gupta A, 2024, Adv Neural Inf Process Syst, V36
   Habermann M, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459749
   Hedman P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5855, DOI 10.1109/ICCV48922.2021.00582
   Hedman P, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275084
   Hedman P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982420
   Hong F, 2022, arXiv
   Hong FZ, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530094
   Jain A, 2022, Arxiv, DOI arXiv:2112.01455
   Jain A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5865, DOI 10.1109/ICCV48922.2021.00583
   Kalantari NK, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980251
   Kellnhofer P, 2021, PROC CVPR IEEE, P4285, DOI 10.1109/CVPR46437.2021.00427
   Kingma D. P., 2014, arXiv
   Kopanas G, 2021, COMPUT GRAPH FORUM, V40, P29, DOI 10.1111/cgf.14339
   Kurz A, 2022, LECT NOTES COMPUT SC, V13677, P254, DOI 10.1007/978-3-031-19790-1_16
   Landgraf Z, 2022, Arxiv, DOI arXiv:2202.04713
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Li D, 2022, INT CONF 3D VISION, P231, DOI 10.1109/3DV57658.2022.00035
   Li Z, 2022, EUROGRAPHICS S RENDE
   Lindell DB, 2022, PROC CVPR IEEE, P16231, DOI 10.1109/CVPR52688.2022.01577
   Lindell DB, 2021, PROC CVPR IEEE, P14551, DOI 10.1109/CVPR46437.2021.01432
   Liu L., 2020, Advances in Neural Information Processing Systems, V33, P15651
   Liu L, 2020, IEEE Trans Visual Comput Graph
   Liu LJ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480528
   Liu LJ, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3333002
   Liu SH, 2020, PROC CVPR IEEE, P2016, DOI 10.1109/CVPR42600.2020.00209
   Liu Y, 2022, PROC CVPR IEEE, P7814, DOI 10.1109/CVPR52688.2022.00767
   Lombardi S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323020
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Mildenhall B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322980
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Museth K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487235
   Neff T, 2021, COMPUTER GRAPHICS FO
   Niemeyer M, 2021, PROC CVPR IEEE, P11448, DOI 10.1109/CVPR46437.2021.01129
   Niemeyer M, 2020, PROC CVPR IEEE, P3501, DOI 10.1109/CVPR42600.2020.00356
   Peng SD, 2021, PROC CVPR IEEE, P9050, DOI 10.1109/CVPR46437.2021.00894
   Penner E, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130855
   Piala M, 2021, INT CONF 3D VISION, P1106, DOI 10.1109/3DV53792.2021.00118
   Radford A, 2021, PR MACH LEARN RES, V139
   Reiser C, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592426
   Reiser C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14315, DOI 10.1109/ICCV48922.2021.01407
   Riegler Gernot, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P623, DOI 10.1007/978-3-030-58529-7_37
   Riegler G, 2021, PROC CVPR IEEE, P12211, DOI 10.1109/CVPR46437.2021.01204
   Rückert D, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530122
   Salimans T, 2016, ADV NEUR IN, V29
   Schwarz Katja, 2020, Advances in Neural Information Processing Systems
   Sitzmann V., 2020, ADV NEURAL INFORM PR, V33
   Sitzmann V, 2021, ADV NEUR IN, V34
   Sitzmann V, 2019, ADV NEUR IN, V32
   Sitzmann V, 2019, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2019.00254
   Smith C, 2022, Arxiv, DOI arXiv:2205.03923
   Suhail M, 2021, arXiv
   Sun C, 2022, Arxiv, DOI arXiv:2111.11215
   Thies J., 2020, ICLR, P1
   Thies J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323035
   Wang C, 2022, PROC CVPR IEEE, P3825, DOI 10.1109/CVPR52688.2022.00381
   Wang H, 2022, Arxiv, DOI arXiv:2203.17261
   Wang Peng, 2021, NeurIPS
   Wang QQ, 2021, PROC CVPR IEEE, P4688, DOI 10.1109/CVPR46437.2021.00466
   Wizadwongsa S, 2021, PROC CVPR IEEE, P8530, DOI 10.1109/CVPR46437.2021.00843
   Wu MY, 2020, PROC CVPR IEEE, P1679, DOI 10.1109/CVPR42600.2020.00175
   Xiangli YB, 2022, LECT NOTES COMPUT SC, V13692, P106, DOI 10.1007/978-3-031-19824-3_7
   Xu ZX, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323007
   Yan H, 2023, PROC CVPR IEEE, P88, DOI 10.1109/CVPR52729.2023.00017
   Yu A, 2021, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR46437.2021.00455
   Yu A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5732, DOI 10.1109/ICCV48922.2021.00570
   Zhang K, 2022, LECT NOTES COMPUT SC, V13691, P717, DOI 10.1007/978-3-031-19821-2_41
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
NR 87
TC 1
Z9 1
U1 3
U2 3
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD MAY
PY 2024
VL 120
AR 103913
DI 10.1016/j.cag.2024.103913
EA APR 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SA9D3
UT WOS:001231847500001
DA 2024-08-05
ER

PT J
AU Park, BS
   Kim, JK
   Seo, YH
AF Park, Byung-Seo
   Kim, Jin-Kyum
   Seo, Young -Ho
TI 3D pose estimation using joint-based calibration in distributed RGB-D
   camera system
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE 3D pose estimation; 3D joint; RGB-D; 3D sensor; Point cloud
ID OF-THE-ART; RECONSTRUCTION; TRACKING
AB This paper proposes a new approach that acquires camera parameters and generates an integrated 3D joint using an RGB-D camera network distributed in an arbitrary location in space. The proposed technique consists of three steps. In the first step, camera parameters are calculated using partial joints as feature points. The internal and external parameters between cameras are not calculated using a specially manufactured calibration plate. In the second step, a 3D joint set is estimated by integrating the joints obtained from each camera using the calculated camera parameters. At the same time, a 3D volumetric model in the form of a point cloud is reconstructed. The third step consists of a joint correction algorithm. The resultant 3D joint with high reliability is estimated by correcting the position of the joint using the 3D point cloud reconstructed previously. The generated 3D joint can accurately express the shape and movement of 3D human. The estimated 3D joint was compared with the joint measured using a motion capture device to evaluate its performance. The temporal standard deviation between two measurements shows very low value from 1.966 mm to 7.99 mm.
C1 [Park, Byung-Seo; Kim, Jin-Kyum; Seo, Young -Ho] Kwangwoon Univ, Elect Mat Engn, 601 Chambit Hall,Kwangwoon Ro 20, Seoul 101897, South Korea.
C3 Kwangwoon University
RP Seo, YH (corresponding author), Kwangwoon Univ, Elect Mat Engn, 601 Chambit Hall,Kwangwoon Ro 20, Seoul 101897, South Korea.
EM bspark@kw.ac.kr; jkkim@kw.ac.kr; yhseo@kw.ac.kr
RI Park, Byung-Seo/KVB-9673-2024
OI Park, Byung-Seo/0000-0003-3396-8264; SEO, YOUNG HO/0000-0003-1046-395X
FU MSIT (Ministry of Science and ICT) , Korea, under the ITRC (Information
   Technology Research Center) support program
   [IITP-2023-RS-2022-00156225]; MSIT (Ministry of Science and ICT) ,
   Korea, under the ICAN (ICT Challenge and Advanced Network of HRD)
   program [IITP-2022-RS-2022-00156215]; Kwangwoon University
FX This research was supported by the MSIT (Ministry of Science and ICT) ,
   Korea, under the ITRC (Information Technology Research Center) support
   program (IITP-2023-RS-2022-00156225) supervised by the IITP (Institute
   for Information & Communications Technology Planning & Evaluation) .
   This research was supported by the MSIT (Ministry of Science and ICT) ,
   Korea, under the ICAN (ICT Challenge and Advanced Network of HRD)
   program (IITP-2022-RS-2022-00156215) supervised by the IITP (Institute
   of Information & Communications Technology Planning & Evaluation) . The
   present research has been conducted by the Excellent researcher support
   project of Kwangwoon University in 2023.
CR Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen TL, 2022, IEEE T CIRC SYST VID, V32, P198, DOI 10.1109/TCSVT.2021.3057267
   Cheng Y, 2019, IEEE I CONF COMP VIS, P723, DOI 10.1109/ICCV.2019.00081
   Desai K, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P40, DOI 10.1145/3204949.3204958
   Desai K, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P250, DOI 10.1145/3204949.3204969
   Giancola S, 2018, SPRINGERBRIEF COMPUT, P29, DOI 10.1007/978-3-319-91761-0_3
   He YH, 2020, IEEE COMPUT SOC CONF, P4466, DOI 10.1109/CVPRW50498.2020.00526
   Holz D, 2015, IEEE ROBOT AUTOM MAG, V22, P110, DOI 10.1109/MRA.2015.2432331
   Huang FY, 2020, IEEE WINT CONF APPL, P418, DOI [10.1109/wacv45572.2020.9093526, 10.1109/WACV45572.2020.9093526]
   Pham HH, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20071825
   Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3
   Iskakov K, 2019, IEEE I CONF COMP VIS, P7717, DOI 10.1109/ICCV.2019.00781
   Kim KJ, 2020, OPT EXPRESS, V28, P35972, DOI 10.1364/OE.411141
   Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234
   Labbé M, 2014, IEEE INT C INT ROBOT, P2661, DOI 10.1109/IROS.2014.6942926
   Lie WN, 2019, IEEE INT CONF MULTI, P1, DOI 10.1109/ICMEW.2019.0-120
   Liu HB, 2021, INTERVIROLOGY, V64, P126, DOI [10.1109/TMM.2021.3081873, 10.1159/000513687, 10.1109/TPWRD.2021.3054889]
   Mekuria R, 2014, IEEE T MULTIMEDIA, V16, P1809, DOI 10.1109/TMM.2014.2331919
   Microsoft, 2021, Azure kinect body tracking joints
   Microsoft, 2021, Quickstart: Set up azure kinect body tracking
   Munaro M, 2014, AUTON ROBOT, V37, P227, DOI 10.1007/s10514-014-9385-0
   Munea TL, 2020, IEEE ACCESS, V8, P133330, DOI 10.1109/ACCESS.2020.3010248
   Nie WZ, 2021, IEEE T MULTIMEDIA, V23, P1021, DOI 10.1109/TMM.2020.2991532
   Noitom, 2021, Perception neuron
   Núñez JC, 2017, MULTIMED TOOLS APPL, V76, P4249, DOI 10.1007/s11042-016-3759-6
   Park BS, 2022, SIGNAL PROCESS, V197, DOI 10.1016/j.sigpro.2022.108535
   Park BS, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22031097
   Qiu HB, 2019, IEEE I CONF COMP VIS, P4341, DOI 10.1109/ICCV.2019.00444
   Schonberger JL, 2018, Doctoral thesis, DOI [10.3929/ethz-b-000295763, DOI 10.3929/ETHZ-B-000295763]
   Tang J, 2012, IEEE INT CONF ROBOT, P3467, DOI 10.1109/ICRA.2012.6224891
   Unal G, 2007, IEEE T PATTERN ANAL, V29, P1322, DOI 10.1109/TPAMI.2007.1035
   Wu HP, 2020, AAAI CONF ARTIF INTE, V34, P12378
   Yeh SC, 2016, J COMMUN NETW-S KOR, V18, P837, DOI 10.1109/JCN.2016.000112
   Yun WJ, 2020, I C INF COMM TECH CO, P240, DOI 10.1109/ICTC49870.2020.9289518
   Zabatani A, 2020, IEEE T PATTERN ANAL, V42, P2333, DOI 10.1109/TPAMI.2019.2915841
   Zhang XY, 2022, IEEE T MULTIMEDIA, V24, P166, DOI 10.1109/TMM.2020.3047552
   Zhao TH, 2019, IEEE T MULTIMEDIA, V21, P114, DOI 10.1109/TMM.2018.2844087
   Zhou XY, 2017, IEEE I CONF COMP VIS, P398, DOI 10.1109/ICCV.2017.51
   Zhu AC, 2014, EUR SIGNAL PR CONF, P366
   Zollhöfer M, 2018, COMPUT GRAPH FORUM, V37, P625, DOI 10.1111/cgf.13386
NR 41
TC 1
Z9 1
U1 1
U2 1
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD MAY
PY 2024
VL 120
AR 103917
DI 10.1016/j.cag.2024.103917
EA MAY 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SY7Z7
UT WOS:001238089900001
DA 2024-08-05
ER

PT J
AU Prasse, P
   Reich, DR
   Makowski, S
   Scheffer, T
   Jäger, LA
AF Prasse, Paul
   Reich, David R.
   Makowski, Silvia
   Scheffer, Tobias
   Jaeger, Lena A.
TI Improving cognitive-state analysis from eye gaze with synthetic
   eye-movement data
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Eye tracking; Generative adversarial networks; Reading comprehension;
   Biometric verification; ADHD detection; Gender classification
ID MODEL; FIXATION; SACCADE
AB Eye movements can be used to analyze a viewer's cognitive capacities or mental state. Neural networks that process the raw eye -tracking signal can outperform methods that operate on scan paths preprocessed into fixations and saccades. However, the scarcity of such data poses a major challenge. We therefore develop SP-EyeGAN, a neural network that generates synthetic raw eye -tracking data. SP-EyeGAN consists of Generative Adversarial Networks; it produces a sequence of gaze angles indistinguishable from human ocular micro- and macro -movements. We explore the use of these synthetic eye movements for pre -training neural networks using contrastive learning. We find that pre -training on synthetic data does not help for biometric identification, while results are inconclusive for the detection of ADHD and gender classification. However, for the eye movement -based assessment of higher -level cognitive skills such general reading comprehension, text comprehension, and the distinction of native from non-native readers, pre -training on synthetic eye -gaze data improves the models' performance and even advances the state-of-the-art for reading comprehension. The SP-EyeGAN model, pre -trained on GazeBase, along with the code for developing your own raw eye -tracking machine learning model with contrastive learning, is available at https://github.com/aeye-lab/sp-eyegan.
C1 [Prasse, Paul; Makowski, Silvia; Scheffer, Tobias; Jaeger, Lena A.] Univ Potsdam, Potsdam, Germany.
   [Jaeger, Lena A.] Univ Zurich, Zurich, Switzerland.
C3 University of Potsdam; University of Zurich
RP Prasse, P (corresponding author), Univ Potsdam, Potsdam, Germany.
EM prasse@uni-potsdam.de
OI Jager, Lena/0000-0001-9018-9713
FU German Federal Ministry of Education and Research [01| S20043]
FX This work was partially funded by the German Federal Ministry of
   Education and Research under grant 01| S20043.
CR Ahn S, 2020, ETRA 2020 SHORT PAPERS: ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, DOI 10.1145/3379156.3391335
   Alexander LM, 2017, SCI DATA, V4, DOI 10.1038/sdata.2017.181
   Assens M, 2019, LECT NOTES COMPUT SC, V11133, P406, DOI 10.1007/978-3-030-11021-5_25
   Aziz S, 2024, Arxiv, DOI arXiv:2402.08655
   Bautista LGC, 2021, EUR SIGNAL PR CONF, P1241, DOI 10.23919/EUSIPCO54536.2021.9616181
   Benfatto MN, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0165508
   Berzak Y, 2018, long papers, V1, P1986, DOI DOI 10.18653/V1/N18-1180
   Bjornsdottir M, 2023, P 24 NORDIC C COMPUT, P60
   Bolliger L, 2023, P 2023 C EMPIRICAL M, P15513, DOI [10.18653/v1/2023.emnlp-main.960, DOI 10.18653/V1/2023.EMNLP-MAIN.960]
   Bowles C, 2018, Arxiv, DOI arXiv:1810.10863
   Campbell DanielJ., 2014, Pro- ceedings of the Symposium on Eye Tracking Research and Applications, P51
   Carlini N, 2021, PROCEEDINGS OF THE 30TH USENIX SECURITY SYMPOSIUM, P2633
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Coutrot A, 2016, J VISION, V16, DOI 10.1167/16.14.16
   David-John B, 2022, 2022 ACM SYMPOSIUM ON EYE TRACKING RESEARCH AND APPLICATIONS, ETRA 2022, DOI 10.1145/3517031.3529618
   Deng S, 2022, P EUROPEAN C MACHINE, P403
   Deng Shuwen, 2023, P ACM HUMAN COMPUTER, V7, P1
   Doyle D, 2001, EXP BRAIN RES, V139, P333, DOI 10.1007/s002210100742
   Duchowski Andrew., 2015, P 8 ACM SIGGRAPH C M, P47
   Duchowski AT, 2016, 2016 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2016), P147, DOI 10.1145/2857491.2857528
   Duchowski AT, 2015, P COMPUTER GRAPHICS, P1
   Engbert R, 2005, PSYCHOL REV, V112, P777, DOI 10.1037/0033-295X.112.4.777
   Fosco C, 2020, PROC CVPR IEEE, P4472, DOI 10.1109/CVPR42600.2020.00453
   Fuhl W, 2018, Arxiv, DOI arXiv:1804.00970
   Fuhl W, 2018, Arxiv, DOI arXiv:1808.09296
   Fuhl W, 2022, 2022 ACM SYMPOSIUM ON EYE TRACKING RESEARCH AND APPLICATIONS, ETRA 2022, DOI 10.1145/3517031.3529625
   Fuhl W, 2021, INT C PATT RECOG, P142, DOI 10.1109/ICPR48806.2021.9413268
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Griffith H, 2021, SCI DATA, V8, DOI 10.1038/s41597-021-00959-y
   Haller P, 2022, P WORKSHOP TEXT SIMP, P111
   Jäger LA, 2020, LECT NOTES ARTIF INT, V11907, P299, DOI 10.1007/978-3-030-46147-8_18
   Kümmerer M, 2022, J VISION, V22, DOI 10.1167/jov.22.5.7
   Kummerer M, 2021, arXiv
   Lahey JN, 2021, J POLICY ANAL MANAG, V40, P1083, DOI 10.1002/pam.22281
   Lan GH, 2022, 2022 21ST ACM/IEEE INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2022), P233, DOI 10.1109/IPSN54338.2022.00026
   Le BH, 2012, IEEE T VIS COMPUT GR, V18, P1902, DOI 10.1109/TVCG.2012.74
   Lee SP, 2002, ACM T GRAPHIC, V21, P637
   Lohr D, 2022, IEEE T INF FOREN SEC, V17, P3151, DOI 10.1109/TIFS.2022.3201369
   Ma XH, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P143, DOI 10.1109/VR.2009.4811014
   Makowski Silvia, 2023, Procedia Computer Science, V225, P2086, DOI 10.1016/j.procs.2023.10.199
   Makowski Silvia, 2021, IEEE Transactions on Biometrics, Behavior, and Identity Science, V3, P506, DOI 10.1109/TBIOM.2021.3116875
   Makowski S, 2020, JuDo1000 eye tracking data set
   Makowski S., 2020, IEEEIAPR INT JOINT, P1, DOI [DOI 10.1109/IJCB48548.2020.9304900, DOI 10.1109/ijcb48548.2020.9304900]
   Makowski S, 2019, LECT NOTES ARTIF INT, V11051, P209, DOI 10.1007/978-3-030-10925-7_13
   Manning C. D., 1999, Foundations of Statistical Natural Language Processing
   Nuthmann A, 2010, PSYCHOL REV, V117, P382, DOI 10.1037/a0018924
   Nyström M, 2010, BEHAV RES METHODS, V42, P188, DOI 10.3758/BRM.42.1.188
   Prasse P, 2023, P 2023 S EYE TRACKIN, P1
   Prasse P, 2022, P 2022 S EYE TRACKIN
   Qin HF, 2024, Arxiv, DOI arXiv:2401.04956
   RAYNER K, 1976, VISION RES, V16, P829, DOI 10.1016/0042-6989(76)90143-7
   Reich DR, 2022, 2022 ACM SYMPOSIUM ON EYE TRACKING RESEARCH AND APPLICATIONS, ETRA 2022, DOI 10.1145/3517031.3529639
   Reichle ED, 2003, BEHAV BRAIN SCI, V26, P445, DOI 10.1017/S0140525X03000104
   Rigas I, 2017, J EYE MOVEMENT RES, V11, DOI 10.16910/jemr.11.1.3
   Salvucci D.D., 2000, P 2000 S EYE TRACKIN, P71, DOI [10.1145/355017.355028, DOI 10.1145/355017.355028]
   Schleicher R, 2008, ERGONOMICS, V51, P982, DOI 10.1080/00140130701817062
   Shiferaw BA, 2019, DRUG ALCOHOL DEPEN, V204, DOI 10.1016/j.drugalcdep.2019.06.021
   Simon D, 2016, P ACM S APPL PERCEPT, P130
   Unger M, 2023, Predicting consumer choice from raw eyemovement data using the RETINA deep learning architecture
   Wood E, 2015, IEEE I CONF COMP VIS, P3756, DOI 10.1109/ICCV.2015.428
   Yeo SH, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185538
NR 61
TC 1
Z9 1
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103901
DI 10.1016/j.cag.2024.103901
EA MAR 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OZ3V7
UT WOS:001211069200001
OA hybrid
DA 2024-08-05
ER

PT J
AU Wang, Z
   Liu, Z
   Liu, TT
   Zhao, YM
   Chai, YJ
AF Wang, Zhuang
   Liu, Zhen
   Liu, Tingting
   Zhao, Yumeng
   Chai, Yanjie
TI An emotional crowd simulation method based on audiovisual linkage for
   terrorist attacks
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Audiovisual linkage; Crowd simulation; Emotion modeling; Terrorist
   attack
ID MODEL; BEHAVIOR
AB In terrorist attacks, crowd emotions have a significant impact on the evacuation process. Current researches of crowd emotion modeling are of great importance in formulating emergency plans. However, existing crowd emotional models do not describe the perceptual process in detail. They overlook the audiovisual linkage (AVL), and do not demonstrate head -turning behaviors in evacuation simulation. We propose an AVL-based perception model, which can describe head -turning behaviors in response to dynamic stimuli during emotional events. We also propose a model for calculating crowd emotions and provide a method for representing crowd movements. Compared to existing researches, our method not only improves the perception model but also realizes the influence of perception on agents' emotions and movements. The results of comparative experiments show that the proposed method can effectively simulate audiovisual linkage phenomena in crowd evacuation during terrorist attacks.
C1 [Wang, Zhuang; Liu, Zhen; Zhao, Yumeng; Chai, Yanjie] Ningbo Univ, Fac Elect Engn & Comp Sci, Ningbo 315000, Peoples R China.
   [Liu, Tingting] Ningbo Univ, Coll Sci & Technol, Cixi 315300, Peoples R China.
C3 Ningbo University; Ningbo University
RP Wang, Z (corresponding author), Ningbo Univ, Fac Elect Engn & Comp Sci, Ningbo 315000, Peoples R China.
EM 543860797@qq.com; liuzhen@nbu.edu.cn; liutingting@nbu.edu.cn;
   yumeng9605@foxmail.com; chaiyanjie@nbu.edu.cn
FU Zhejiang Provincial Natural Science Foundation of China [LZ23F020005];
   Ningbo Science Technology Plan projects of China [2022Z077]
FX <B>Acknowledgments</B> This research was supported by Zhejiang
   Provincial Natural Science Foundation of China under Grant No.
   LZ23F020005 and by the Ningbo Science Technology Plan projects of China
   under Grant No. 2022Z077.
CR Bernardini G, 2021, SAFETY SCI, V143, DOI 10.1016/j.ssci.2021.105405
   Clore Gerald., 1994, Affective causes and consequences of social information processing, P323
   COLBY BN, 1989, CONTEMP SOCIOL, V18, P957, DOI 10.2307/2074241
   Durupinar F, 2016, IEEE T VIS COMPUT GR, V22, P2145, DOI 10.1109/TVCG.2015.2501801
   Durupinar F, 2011, IEEE COMPUT GRAPH, V31, P22, DOI 10.1109/MCG.2009.105
   Franchak JM, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0256463
   Goestchel Q, 2022, J SOUND VIB, V531, DOI 10.1016/j.jsv.2022.116974
   Guy SJ, 2010, PROCEEDINGS OF THE TWENTY-SIXTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SCG'10), P115, DOI 10.1145/1810959.1810981
   Helbing D, 2000, NATURE, V407, P487, DOI 10.1038/35035023
   Hong X, 2022, IEEE T SYST MAN CY-S, V52, P1638, DOI 10.1109/TSMC.2020.3034395
   Huang P, 2022, TUNN UNDERGR SP TECH, V124, DOI 10.1016/j.tust.2022.104485
   Karamouzas I, 2014, PHYS REV LETT, V113, DOI 10.1103/PhysRevLett.113.238701
   Koilias A, 2020, COMPUT ANIMAT VIRT W, V31, DOI 10.1002/cav.1928
   Li CC, 2022, IEEE T AFFECT COMPUT, V13, P729, DOI 10.1109/TAFFC.2019.2954394
   Li SY, 2017, PHYSICA A, V481, P127, DOI 10.1016/j.physa.2017.04.011
   Li ZW, 2020, AUTOMAT CONSTR, V120, DOI 10.1016/j.autcon.2020.103395
   Liu H, 2021, INFORM SCIENCES, V575, P155, DOI 10.1016/j.ins.2021.06.036
   Liu Q, 2020, INT J DISAST RISK RE, V46, DOI 10.1016/j.ijdrr.2020.101605
   Liu Z, 2018, COMPUT ANIMAT VIRT W, V29, DOI 10.1002/cav.1817
   López A, 2019, COMPUT GRAPH FORUM, V38, P181, DOI 10.1111/cgf.13629
   [马峻 Ma Jun], 2021, [系统仿真学报, Journal of System Simulation], V33, P2289
   Qun Wu, 2010, Proceedings 2010 3rd International Symposium on Computational Intelligence and Design (ISCID 2010), P225, DOI 10.1109/ISCID.2010.147
   Tian ZN, 2020, KNOWL-BASED SYST, V208, DOI 10.1016/j.knosys.2020.106451
   van den Berg J, 2008, IEEE INT CONF ROBOT, P1928, DOI 10.1109/ROBOT.2008.4543489
   Weiss T, 2019, COMPUT GRAPH-UK, V78, P12, DOI 10.1016/j.cag.2018.10.008
   Xu ML, 2021, IEEE T INTELL TRANSP, V22, P6977, DOI 10.1109/TITS.2020.3000607
   Xu ML, 2021, IEEE T SYST MAN CY-S, V51, P1567, DOI 10.1109/TSMC.2019.2899047
   Xue JX, 2023, ACM T MODEL COMPUT S, V33, DOI 10.1145/3577589
   Xue JX, 2019, SCI CHINA INFORM SCI, V62, DOI 10.1007/s11432-018-9654-6
   Zhou M, 2016, INFORM SCIENCES, V360, P112, DOI 10.1016/j.ins.2016.04.018
   Zhuo L, 2021, COMPUT ANIMAT VIRT W, V32, DOI 10.1002/cav.1988
   Zou BB, 2020, PHYSICA A, V547, DOI 10.1016/j.physa.2019.122943
NR 32
TC 0
Z9 0
U1 5
U2 5
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103892
DI 10.1016/j.cag.2024.103892
EA FEB 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF4Z0
UT WOS:001192211200001
DA 2024-08-05
ER

PT J
AU Yang, JY
   Zhao, MY
   Wu, YR
   Jia, XH
AF Yang, Jieyin
   Zhao, Mingyang
   Wu, Yingrui
   Jia, Xiaohong
TI Accurate and robust registration of low overlapping point clouds
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Point cloud registration; ICP; HMRF; Low overlapping; Outliers
ID SAMPLE CONSENSUS; PARAMETERS
AB Point cloud registration has various applications within the computer -aided design (CAD) community, such as model reconstruction, retrieving, and analysis. Previous approaches mainly deal with the registration with a high overlapping hypothesis, while few existing methods explore the registration between low overlapping point clouds. However, the latter registration task is both challenging and essential, since the weak correspondence in point clouds usually leads to an inappropriate initialization, making the algorithm get stuck in a local minimum. To improve the performance against low overlapping scenarios, in this work, we develop a novel algorithm for accurate and robust registration of low overlapping point clouds using optimal transformation. The core of our method is the effective integration of geometric features with the probabilistic model hidden Markov random field. First, we determine and remove the outliers of the point clouds by modeling a hidden Markov random field based on a high dimensional feature distribution. Then, we derive a necessary and sufficient condition when the symmetric function is minimized and present a new curvature -aware symmetric function to make the point correspondence more discriminative. Finally, we integrate our curvature -aware symmetric function into a geometrically stable sampling framework, which effectively constrains unstable transformations. We verify the accuracy and robustness of our method on a wide variety of datasets, particularly on low overlapping range scanned point clouds. Results demonstrate that our proposed method attains better performance with higher accuracy and robustness compared to representative state-of-the-art approaches.
C1 [Yang, Jieyin; Jia, Xiaohong] Chinese Acad Sci, Acad Math & Syst Sci, KLMM, Beijing, Peoples R China.
   [Yang, Jieyin; Wu, Yingrui; Jia, Xiaohong] Univ Chinese Acad Sci, Beijing, Peoples R China.
   [Zhao, Mingyang] Chinese Acad Sci, CAIR Hong Kong Inst Sci & Innovat, Hong Kong, Peoples R China.
   [Zhao, Mingyang; Wu, Yingrui] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Academy of Mathematics & System Sciences,
   CAS; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS; Chinese Academy of Sciences; Chinese Academy of Sciences;
   Institute of Automation, CAS
RP Jia, XH (corresponding author), Chinese Acad Sci, Acad Math & Syst Sci, KLMM, Beijing, Peoples R China.; Jia, XH (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.
EM xhjia@amss.ac.cn
RI Zhao, MingYang/GLR-8497-2022
FU National Key R&D Program of China [2021YFB1715900]; National Natural
   Science Foundation of China [12022117]; CAS Project for Young Scientists
   in Basic Research [YSBR-034]
FX <B>Acknowledgements</B> This work is supported by National Key R&D
   Program of China (2021YFB1715900) , the National Natural Science
   Foundation of China (12022117) , and the CAS Project for Young
   Scientists in Basic Research (YSBR-034) .
CR [Anonymous], STANFORD 3D SCANNING
   [Anonymous], KITTI VISION BENCHMA
   [Anonymous], TERRESTRIAL LASER SC
   [Anonymous], AIM@SHAPE Shape Repository
   Ao S, 2023, PROC CVPR IEEE, P1255, DOI 10.1109/CVPR52729.2023.00127
   Aoki Y, 2019, PROC CVPR IEEE, P7156, DOI 10.1109/CVPR.2019.00733
   Barath D, 2019, Arxiv, DOI arXiv:1906.02295
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bolles RC, 1981, INT JOINT C ARTIFICI, P637, DOI DOI 10.5555/1623264.1623272
   Bouaziz S, 2013, COMPUT GRAPH FORUM, V32, P113, DOI 10.1111/cgf.12178
   Cao XM, 2022, COMPUT AIDED DESIGN, V144, DOI 10.1016/j.cad.2021.103164
   Chatzis SP, 2008, IEEE T FUZZY SYST, V16, P1351, DOI 10.1109/TFUZZ.2008.2005008
   Chatzis SP, 2010, IEEE T NEURAL NETWOR, V21, P1004, DOI 10.1109/TNN.2010.2046910
   CHEN Y, 1992, IMAGE VISION COMPUT, V10, P145, DOI 10.1016/0262-8856(92)90066-C
   Choy C, 2020, PROC CVPR IEEE, P2511, DOI 10.1109/CVPR42600.2020.00259
   Clifford P., 1990, Markov random fields in statistics. Disorder in physical systems: A volume in honour of John M Hammersley, P19
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Danelljan M, 2016, PROC CVPR IEEE, P1818, DOI 10.1109/CVPR.2016.201
   Fiorucci M, 2020, PATTERN RECOGN LETT, V133, P102, DOI 10.1016/j.patrec.2020.02.017
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fusiello A, 2002, LECT NOTES COMPUT SC, V2351, P805
   Gelfand N, 2003, FOURTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P260, DOI 10.1109/IM.2003.1240258
   Granger S, 2002, LECT NOTES COMPUT SC, V2353, P418
   Hirose O, 2021, IEEE T PATTERN ANAL, V43, P2269, DOI 10.1109/TPAMI.2020.2971687
   Hontani H, 2012, PROC CVPR IEEE, P174, DOI 10.1109/CVPR.2012.6247673
   Huang SY, 2021, PROC CVPR IEEE, P4265, DOI 10.1109/CVPR46437.2021.00425
   Huang YB, 2009, COMPUT AIDED DESIGN, V41, P240, DOI 10.1016/j.cad.2008.10.003
   Ip Cheuk Yiu., 2007, Computer-Aided Design and Applications, V4, P629
   Jian B, 2011, IEEE T PATTERN ANAL, V33, P1633, DOI 10.1109/TPAMI.2010.223
   Kolluri R., 2004, P 2004 EUR ACM SIGGR, P11, DOI DOI 10.1145/1057432.1057434
   Lawin FJ, 2018, PROC CVPR IEEE, P3829, DOI 10.1109/CVPR.2018.00403
   Li JY, 2023, IEEE T PATTERN ANAL, V45, P11136, DOI 10.1109/TPAMI.2023.3262780
   Li JY, 2022, ISPRS J PHOTOGRAMM, V185, P219, DOI 10.1016/j.isprsjprs.2022.01.019
   Li JY, 2021, IEEE T GEOSCI REMOTE, V59, P9716, DOI 10.1109/TGRS.2020.3045456
   Li JY, 2020, ISPRS J PHOTOGRAMM, V167, P363, DOI 10.1016/j.isprsjprs.2020.07.012
   Li JJ, 2022, IEEE T PATTERN ANAL, V44, P8196, DOI 10.1109/TPAMI.2021.3109287
   Liu Y, 2006, COMPUT AIDED DESIGN, V38, P572, DOI 10.1016/j.cad.2006.01.014
   Lu WX, 2019, IEEE I CONF COMP VIS, P12, DOI 10.1109/ICCV.2019.00010
   Lucas Bruce D, 1981, IJCAI 81, V81, DOI DOI 10.5555/1623264.1623280
   Myatt D. R., 2002, BRIT MACHINE VISION, P3
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46
   Pottmann H, 2006, INT J COMPUT VISION, V67, P277, DOI 10.1007/s11263-006-5167-2
   Qin Z, 2022, PROC CVPR IEEE, P11133, DOI 10.1109/CVPR52688.2022.01086
   Rusinkiewicz S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323037
   Scrafin J, 2015, IEEE INT C INT ROBOT, P742, DOI 10.1109/IROS.2015.7353455
   Segal A, 2009, Robotics: science and systems, DOI 10.15607/rss.2009.v.021
   Sofien Bouaziz, 2016, ACM trans. graph, P1
   Stechschulte J, 2019, IEEE INT CONF ROBOT, P7143, DOI [10.1109/icra.2019.8793857, 10.1109/ICRA.2019.8793857]
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   Sun W, 2005, COMPUT AIDED DESIGN, V37, P1097, DOI 10.1016/j.cad.2005.02.002
   Tabib W, 2018, IEEE ROBOT AUTOM LET, V3, P3805, DOI 10.1109/LRA.2018.2856279
   Tam GKL, 2013, IEEE T VIS COMPUT GR, V19, P1199, DOI 10.1109/TVCG.2012.310
   Thomson C., 2013, ISPRS Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci, VII-5-W2, P289, DOI [DOI 10.5194/ISPRSANNALS-II-5-W2-289-2013, 10.5194/isprsannals-II-5-W2-289-2013]
   UMEYAMA S, 1991, IEEE T PATTERN ANAL, V13, P376, DOI 10.1109/34.88573
   WALKER MW, 1991, CVGIP-IMAG UNDERSTAN, V54, P358, DOI 10.1016/1049-9660(91)90036-O
   Wang G, 2021, PATTERN RECOGN, V117, DOI 10.1016/j.patcog.2021.107986
   Wang WP, 2006, ACM T GRAPHIC, V25, P214, DOI 10.1145/1138450.1138453
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Wu Y, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3286943
   Wu Y, 2023, IEEE T EM TOP COMP I, V7, P357, DOI 10.1109/TETCI.2022.3205384
   Wu ZZ, 2019, PATTERN RECOGN, V93, P14, DOI 10.1016/j.patcog.2019.03.013
   Xie Q, 2021, COMPUT AIDED DESIGN, V137, DOI 10.1016/j.cad.2021.103042
   Yang H, 2020, PROC CVPR IEEE, P618, DOI 10.1109/CVPR42600.2020.00070
   Yew ZJ, 2022, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR52688.2022.00656
   Zhang JY, 2022, IEEE T PATTERN ANAL, V44, P3450, DOI 10.1109/TPAMI.2021.3054619
   Zhao Mingyang, 2023, IEEE Transactions on Visualization and Computer Graphics
   Zhou Lei., 2018, P EUROPEAN C COMPUTE, P505
   Zhou QY, 2016, LECT NOTES COMPUT SC, V9906, P766, DOI 10.1007/978-3-319-46475-6_47
NR 68
TC 1
Z9 1
U1 18
U2 18
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD FEB
PY 2024
VL 118
BP 146
EP 160
DI 10.1016/j.cag.2023.12.003
EA JAN 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GI7F1
UT WOS:001152097000001
DA 2024-08-05
ER

PT J
AU Tang, KY
   Wang, CL
AF Tang, Kaiyuan
   Wang, Chaoli
TI STSR-INR: Spatiotemporal super-resolution for multivariate time-varying
   volumetric data via implicit neural representation
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Spatiotemporal super-resolution; Implicit neural representation;
   Multivariate time-varying data
ID FLOW
AB Implicit neural representation (INR) has surfaced as a promising direction for solving different scientific visualization tasks due to its continuous representation and flexible input and output settings. We present STSR-INR, an INR solution for generating simultaneous spatiotemporal super -resolution for multivariate timevarying volumetric data. Inheriting the benefits of the INR-based approach, STSR-INR supports unsupervised learning and permits data upscaling with arbitrary spatial and temporal scale factors. Unlike existing GANor INR-based super -resolution methods, STSR-INR focuses on tackling variables or ensembles and enabling joint training across datasets of various spatiotemporal resolutions. We achieve this capability via a variable embedding scheme that learns latent vectors for different variables. In conjunction with a modulated structure in the network design, we employ a variational auto -decoder to optimize the learnable latent vectors to enable latent -space interpolation. To combat the slow training of INR, we leverage a multi -head strategy to improve training and inference speed with significant speedup. We demonstrate the effectiveness of STSR-INR with multiple scalar field datasets and compare it with conventional tricubic+linear interpolation and state-of-the-art deep -learning -based solutions (STNet and CoordNet).
C1 [Tang, Kaiyuan; Wang, Chaoli] Univ Notre Dame, Dept Comp Sci & Engn, Notre Dame, IN 46556 USA.
C3 University of Notre Dame
RP Tang, KY (corresponding author), Univ Notre Dame, Dept Comp Sci & Engn, Notre Dame, IN 46556 USA.
EM ktang2@nd.edu; chaoli.wang@nd.edu
RI Wang, Chaoli/AAJ-5173-2020
OI Wang, Chaoli/0000-0002-0859-3619; Tang, Kaiyuan/0009-0001-3512-0112
FU U.S. National Science Foundation [IIS-1955395, IIS-2101696,
   OAC-2104158]; U.S. Department of Energy [DE-SC0023145]; U.S. Department
   of Energy (DOE) [DE-SC0023145] Funding Source: U.S. Department of Energy
   (DOE)
FX This research was supported in part by the U.S. National Science
   Foundation through grants IIS-1955395, IIS-2101696, OAC-2104158, and the
   U.S. Department of Energy through grant DE-SC0023145. The authors would
   like to thank the anonymous reviewers for their insightful comments.
CR Aftab A, 2022, INT CONF ACOUST SPEE, P2510, DOI 10.1109/ICASSP43922.2022.9747352
   An YF, 2021, IEEE COMPUT GRAPH, V41, P122, DOI 10.1109/MCG.2021.3097555
   Barrow Harry G, 1977, Proceedings of the 5th international joint conference on Artificial intelligence
   Chen HB, 2021, ADV NEUR IN, V34
   Chen ZY, 2022, PROC CVPR IEEE, P2037, DOI 10.1109/CVPR52688.2022.00209
   CRAWFIS RA, 1993, VISUALIZATION 93, PROCEEDINGS, P261
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Gu PF, 2022, IEEE PAC VIS SYMP, P31, DOI 10.1109/PacificVis53943.2022.00012
   Gu PF, 2021, IEEE COMPUT GRAPH, V41, P111, DOI 10.1109/MCG.2021.3089627
   Han J, 2023, IEEE T VIS COMPUT GR, V29, P4951, DOI 10.1109/TVCG.2022.3197203
   Han J, 2022, IEEE T VIS COMPUT GR, V28, P2445, DOI 10.1109/TVCG.2020.3032123
   Han J, 2022, IEEE T VIS COMPUT GR, V28, P270, DOI 10.1109/TVCG.2021.3114815
   Han J, 2021, IEEE T VIS COMPUT GR, V27, P1290, DOI 10.1109/TVCG.2020.3030346
   Han Mengjiao, 2022, Journal of Flow Visualization and Image Processing, P73, DOI 10.1615/JFlowVisImageProc.2022041197
   Hawkes ER, 2007, P COMBUST INST, V31, P1633, DOI 10.1016/j.proci.2006.08.079
   He WB, 2020, IEEE T VIS COMPUT GR, V26, P23, DOI 10.1109/TVCG.2019.2934312
   Jiang HZ, 2018, PROC CVPR IEEE, P9000, DOI 10.1109/CVPR.2018.00938
   Jiao CY, 2023, Arxiv, DOI arXiv:2308.12508
   Li TY, 2022, PROC CVPR IEEE, P5511, DOI 10.1109/CVPR52688.2022.00544
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lu Y, 2021, COMPUT GRAPH FORUM, V40, P135, DOI 10.1111/cgf.14295
   Mehta I, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14194, DOI 10.1109/ICCV48922.2021.01395
   Meyer S, 2015, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2015.7298747
   Mildenhall Ben, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P405, DOI 10.1007/978-3-030-58452-8_24
   Niklaus S, 2017, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2017.37
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Popinet S, 2004, J ATMOS OCEAN TECH, V21, P1575, DOI 10.1175/1520-0426(2004)021<1575:EANSOT>2.0.CO;2
   Rojo IB, 2020, IEEE T VIS COMPUT GR, V26, P280, DOI 10.1109/TVCG.2019.2934375
   Shi N, 2022, IEEE Trans Vis Comput Graphics, V29, P820
   Silver D, 1997, IEEE T VIS COMPUT GR, V3, P129, DOI 10.1109/2945.597796
   Sitzmann V., 2020, Proceedings of Advances in Neural Information Processing Systems, P1
   Wang C, 2008, IEEE T VIS COMPUT GR, V14, P590, DOI 10.1109/TVCG.2007.70628
   Wang CL, 2023, IEEE T VIS COMPUT GR, V29, P3714, DOI 10.1109/TVCG.2022.3167896
   Weiss S, 2022, COMPUT GRAPH FORUM, V41, P196, DOI 10.1111/cgf.14578
   Whalen D, 2008, ASTROPHYS J, V673, P664, DOI 10.1086/524400
   Wu Q, 2023, IEEE Trans Vis Comput Graphics
   Wu Q, 2023, Arxiv, DOI arXiv:2304.04188
   Wu Q, 2023, IEEE J BIOMED HEALTH, V27, P1004, DOI 10.1109/JBHI.2022.3223106
   Yuce G, 2022, PROC CVPR IEEE, P19206, DOI 10.1109/CVPR52688.2022.01863
   Zadeh A, 2021, Arxiv, DOI arXiv:1903.00840
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhou ZL, 2017, CGI'17: PROCEEDINGS OF THE COMPUTER GRAPHICS INTERNATIONAL CONFERENCE, DOI 10.1145/3095140.3095178
NR 44
TC 0
Z9 0
U1 3
U2 3
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103874
DI 10.1016/j.cag.2024.01.001
EA FEB 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LN2L3
UT WOS:001187416200001
DA 2024-08-05
ER

PT J
AU Zhang, FK
   Zhang, LL
   He, TC
   Sun, YR
   Zhao, S
   Zhang, YM
   Zhao, XL
   Zhao, WY
AF Zhang, Fukai
   Zhang, Lulu
   He, Tiancheng
   Sun, Yiran
   Zhao, Shan
   Zhang, Yanmei
   Zhao, Xueliang
   Zhao, Weiye
TI An overlap estimation guided feature metric approach for real point
   cloud registration
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Point cloud registration; Cross-source 3D data; Overlap estimation;
   Feature metric
AB Real point cloud registration, involving homologous and cross -source 3D data, poses significant challenges such as partial overlap, high noise, density disparities, and scale variations. In this paper, we introduce a framework called OEFM to guide transform estimation on the basis of feature metrics through overlap estimation with overlap region center weighting, which estimates transform parameters by minimizing feature projection error on overlap region features. Our proposed approach utilizes an overlap filtering module with a center -weighting mechanism to filter overlap points and feeds these points into a feature metric framework with a forward-backward transformation to estimate the transformation parameters. The rationale behind our approach is that only overlapping regions are useful for point cloud registration, while non -overlapping regions are anomalous. Furthermore, our proposed approach requires no correspondence search, making it robust to partial overlaps, large noise, density differences, and scale variations. We show that our approach achieves the highest registration recall for 3DMatch and 3DCSR in extensive experiments on both homologous and cross -source datasets.
C1 [Zhang, Fukai; Zhang, Lulu; He, Tiancheng; Sun, Yiran; Zhao, Shan] Henan Polytech Univ, Sch Software, Jiaozuo 454000, Peoples R China.
   [Zhang, Fukai] Henan Polytech Univ, State Key Lab Cultivat Base Gas Geol & Gas Control, Jiaozuo 454000, Peoples R China.
   [Zhang, Yanmei] China Univ Min & Technol, Sch Comp Sci & Technol, Xuzhou 221116, Peoples R China.
   [Zhao, Xueliang; Zhao, Weiye] Jincheng Anthracite Min Grp Co LTD, Shanxi Technol Inst, Jincheng 048006, Peoples R China.
C3 Henan Polytechnic University; Henan Polytechnic University; China
   University of Mining & Technology
RP Zhang, FK (corresponding author), Henan Polytech Univ, Sch Software, Jiaozuo 454000, Peoples R China.
EM zhangfukai@hpu.edu.cn
OI Zhang, Fukai/0000-0002-7378-3478
FU National Natural Science Foundation of China [71774159]; Key Scientific
   Research Projects of Colleges and Universities in Henan Province, China
   [24B520014]; Henan Polytechnic University Youth Backbone Teacher Support
   Program, China [2023XQG-14]
FX This work was supported in part by National Natural Science Foundation
   of China under Grant 71774159, in part by the Key Scientific Research
   Projects of Colleges and Universities in Henan Province, China under
   Grant 24B520014, in part by Henan Polytechnic University Youth Backbone
   Teacher Support Program, China under Grant 2023XQG-14.
CR Aoki Y, 2019, PROC CVPR IEEE, P7156, DOI 10.1109/CVPR.2019.00733
   Bai XY, 2020, PROC CVPR IEEE, P6358, DOI 10.1109/CVPR42600.2020.00639
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Campbell D, 2016, PROC CVPR IEEE, P5685, DOI 10.1109/CVPR.2016.613
   Cao AQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13209, DOI 10.1109/ICCV48922.2021.01298
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen Y, 2022, AAAI CONF ARTIF INTE, P365
   Chen Z, 2022, PROC CVPR IEEE, P13211, DOI 10.1109/CVPR52688.2022.01287
   Choi S, 2015, PROC CVPR IEEE, P5556, DOI 10.1109/CVPR.2015.7299195
   Choy C, 2020, PROC CVPR IEEE, P2511, DOI 10.1109/CVPR42600.2020.00259
   Choy C, 2019, IEEE I CONF COMP VIS, P8957, DOI 10.1109/ICCV.2019.00905
   Deng HW, 2018, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2018.00028
   Du SY, 2020, PATTERN RECOGN LETT, V132, P91, DOI 10.1016/j.patrec.2018.06.028
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Förstner W, 2017, IEEE INT CONF COMP V, P2165, DOI 10.1109/ICCVW.2017.253
   Fotsing C, 2020, ISPRS INT J GEO-INF, V9, DOI 10.3390/ijgi9110647
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Hermans J., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2465, DOI 10.1109/CVPR.2011.5995744
   Huang JD, 2017, J MECH DESIGN, V139, DOI 10.1115/1.4037477
   Huang SY, 2021, PROC CVPR IEEE, P4265, DOI 10.1109/CVPR46437.2021.00425
   Huang XS, 2021, Arxiv, DOI [arXiv:2103.02690, DOI 10.48550/ARXIV.2103.02690]
   Huang XS, 2019, IEEE INT CON MULTI, P1552, DOI 10.1109/ICME.2019.00268
   Huang XS, 2018, IEEE T CIRC SYST VID, V28, P2965, DOI 10.1109/TCSVT.2017.2730232
   Huang XS, 2017, IEEE T IMAGE PROCESS, V26, P3261, DOI 10.1109/TIP.2017.2695888
   Huang XS, 2016, 2016 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA), P53
   Jian B, 2011, IEEE T PATTERN ANAL, V33, P1633, DOI 10.1109/TPAMI.2010.223
   Katharopoulos A, 2020, PR MACH LEARN RES, V119
   Lee J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15974, DOI 10.1109/ICCV48922.2021.01569
   Li Jiahao, 2020, PROC EUR C COMPUT VI, P378, DOI [DOI 10.1007/978-3-030-58586-0_23, 10.1007/978-3-030-58586-0_23]
   Li XQ, 2021, PROC CVPR IEEE, P12758, DOI 10.1109/CVPR46437.2021.01257
   Loshchilov I, 2019, Arxiv, DOI [arXiv:1711.05101, 10.48550/arXiv.1711.05101, DOI 10.48550/ARXIV.1711.05101]
   Lu WX, 2019, IEEE I CONF COMP VIS, P12, DOI 10.1109/ICCV.2019.00010
   Mellado N, 2016, IEEE T VIS COMPUT GR, V22, P2160, DOI 10.1109/TVCG.2015.2505287
   Mellado N, 2014, COMPUT GRAPH FORUM, V33, P205, DOI 10.1111/cgf.12446
   Mian A, 2010, INT J COMPUT VISION, V89, P348, DOI 10.1007/s11263-009-0296-z
   Peng FR, 2014, IEEE IMAGE PROC, P2026, DOI 10.1109/ICIP.2014.7025406
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qin Z, 2022, PROC CVPR IEEE, P11133, DOI 10.1109/CVPR52688.2022.01086
   Radu B.S., 2010, P IEEE RSJ INT C INT, V44
   Rusu RB, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3384, DOI 10.1109/IROS.2008.4650967
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Tombari F., 2010, P ACM WORKSH 3D OBJ, P57, DOI 10.1145/1877808.1877821
   Wan T, 2022, IEEE T NEUR NET LEAR, V33, P3547, DOI 10.1109/TNNLS.2021.3053274
   Wang HP, 2023, IEEE T PATTERN ANAL, V45, P10376, DOI 10.1109/TPAMI.2023.3244951
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Wang Yulin, 2019, ADV NEURAL INFORM PR, V32
   Wentao Yuan, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P733, DOI 10.1007/978-3-030-58558-7_43
   Xiaoshui Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11363, DOI 10.1109/CVPR42600.2020.01138
   Xu H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3112, DOI 10.1109/ICCV48922.2021.00312
   Yang JL, 2016, IEEE T PATTERN ANAL, V38, P2241, DOI 10.1109/TPAMI.2015.2513405
   Yew ZJ, 2022, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR52688.2022.00656
   Yew ZJ, 2018, LECT NOTES COMPUT SC, V11219, P630, DOI 10.1007/978-3-030-01267-0_37
   Yu H, 2021, ADV NEUR IN, V34
   Yu H, 2023, PROC CVPR IEEE, P5384, DOI 10.1109/CVPR52729.2023.00521
   Yu XM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12478, DOI 10.1109/ICCV48922.2021.01227
   Yu Zhong, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P689, DOI 10.1109/ICCVW.2009.5457637
   Zeng A, 2017, PROC CVPR IEEE, P199, DOI 10.1109/CVPR.2017.29
   Zhang XY, 2023, PROC CVPR IEEE, P17745, DOI 10.1109/CVPR52729.2023.01702
   Zhou QY, 2016, LECT NOTES COMPUT SC, V9906, P766, DOI 10.1007/978-3-319-46475-6_47
   Zi Jian Yew, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11821, DOI 10.1109/CVPR42600.2020.01184
NR 63
TC 2
Z9 2
U1 4
U2 4
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103883
DI 10.1016/j.cag.2024.01.010
EA FEB 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LK9Z1
UT WOS:001186824400001
DA 2024-08-05
ER

PT J
AU Qi, YF
   Zhang, HR
   Wen, SC
   Liang, AY
   Cao, PP
   Chen, HL
AF Qi, Yongfeng
   Zhang, Hengrui
   Wen, Shengcong
   Liang, Anye
   Cao, Panpan
   Chen, Huili
TI Adaptive module and accurate heatmap translator for multi-person human
   pose estimation
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Human pose estimation; Heatmap translator; Multi-scale fusion;
   Localization of keypoints; Pixel offset
AB Human pose estimation is an important task in computer vision and an essential step for computers to understand human motion and behavior. However, accurate localization of keypoints for small individuals in multi-person pictures is often ignored by current methods, resulting in limited improvements in accuracy. In addition, the current mainstream methods of translating a predicted heatmap into a coordinate in the original image space is too coarse. This coarseness also affects the localization of keypoints. To address these challenges, we propose an adaptive human body size module(AHBZM), spatial selective attention module(SSAM) and more accurate heatmap translator(MAHT) for human pose estimation. The proposed AHBZM utilizes trainable parameters to select a more appropriate multi-scale fusion method to further refine the localization of keypoints for different body sizes. To further improve keypoints localization, SSAM is used to capture target spatial information during feature fusion. The proposed MAHT will more accurately add the pixel offsets when translating heatmap coordinates into original image coordinates, while more closely associating the global maximum value in the heatmap with the surrounding local maximum values. The experimental results show that the proposed method has achieved good results on the two benchmark datasets of COCO and MPII. Our code is available at: https://github.com/illusory2333/Adaptive-module-and-heatmap-translator.
C1 [Qi, Yongfeng; Zhang, Hengrui; Wen, Shengcong; Liang, Anye; Cao, Panpan; Chen, Huili] Northwest Normal Univ, Lanzhou 730070, Peoples R China.
C3 Northwest Normal University - China
RP Zhang, HR (corresponding author), Northwest Normal Univ, Lanzhou 730070, Peoples R China.
EM yongfeng_qi@163.com; 13693477505a@gmail.com; 2022222193@nwnu.edu.cn;
   liang_anye@163.com; 2022222206@nwnu.edu.cn; 18464413664@163.com
FU National Natural Science Foundation of China [62267007]; Gansu
   Provincial Department of Education Higher Education Industry Support
   Plan Project [2022CYZC-16]
FX The National Natural Science Foundation of China: 62267007, Gansu
   Provincial Department of Education Higher Education Industry Support
   Plan Project:2022CYZC-16.
CR Andriluka M, 2018, PROC CVPR IEEE, P5167, DOI 10.1109/CVPR.2018.00542
   Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Andriluka M, 2009, PROC CVPR IEEE, P1014, DOI 10.1109/CVPRW.2009.5206754
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Chen YC, 2020, COMPUT VIS IMAGE UND, V192, DOI 10.1016/j.cviu.2019.102897
   Cheng BW, 2020, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR42600.2020.00543
   Fang HS, 2023, IEEE T PATTERN ANAL, V45, P7157, DOI 10.1109/TPAMI.2022.3222784
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Geng ZG, 2023, PROC CVPR IEEE, P660, DOI 10.1109/CVPR52729.2023.00071
   Girdhar R, 2018, PROC CVPR IEEE, P350, DOI 10.1109/CVPR.2018.00044
   He DL, 2019, AAAI CONF ARTIF INTE, P8401
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang JJ, 2020, PROC CVPR IEEE, P5699, DOI 10.1109/CVPR42600.2020.00574
   Huang LZ, 2023, PROC CVPR IEEE, P693, DOI 10.1109/CVPR52729.2023.00074
   Jain A, 2014, Arxiv, DOI arXiv:1312.7302
   Jin Sheng, 2020, EUR C COMP VIS, P718, DOI DOI 10.1007/978-3-030-58571-642
   Johnson S., 2010, BMVC, V2, P5, DOI DOI 10.5244/C.24.12
   Kan ZH, 2022, LECT NOTES COMPUT SC, V13665, P729, DOI 10.1007/978-3-031-20065-6_42
   Kingma D. P., 2014, arXiv
   Li K, 2021, PROC CVPR IEEE, P1944, DOI 10.1109/CVPR46437.2021.00198
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3524497
   Luo ZX, 2021, PROC CVPR IEEE, P13259, DOI 10.1109/CVPR46437.2021.01306
   Luvizon DC, 2019, COMPUT GRAPH-UK, V85, P15, DOI 10.1016/j.cag.2019.09.002
   Mao W., 2021, arXiv
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sohn Samuel S., 2022, Computers & Graphics, V106, P130, DOI 10.1016/j.cag.2022.05.010
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33
   Sun X, 2017, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2017.284
   Tasoren AE, 2024, COMPUT GRAPH-UK, V118, P1, DOI 10.1016/j.cag.2023.10.011
   Tompson J, 2014, Advances in neural information processing systems27, V1, P1799, DOI 10.5555/2968826.2969027
   Voulodimos A, 2018, COMPUT INTEL NEUROSC, V2018, DOI 10.1155/2018/7068349
   Wang HX, 2022, LECT NOTES COMPUT SC, V13666, P107, DOI 10.1007/978-3-031-20068-7_7
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Xie ZD, 2023, PROC CVPR IEEE, P14475, DOI 10.1109/CVPR52729.2023.01391
   Xu YF, 2022, ADV NEUR IN
   Xue N, 2022, PROC CVPR IEEE, P13055, DOI 10.1109/CVPR52688.2022.01272
   Yang W, 2017, IEEE I CONF COMP VIS, P1290, DOI 10.1109/ICCV.2017.144
   Zhang F, 2020, PROC CVPR IEEE, P7091, DOI 10.1109/CVPR42600.2020.00712
   Zhang JB, 2019, Arxiv, DOI arXiv:1908.05593
   Zheng C, 2024, ACM COMPUT SURV, V56, DOI 10.1145/3603618
NR 50
TC 1
Z9 1
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD MAY
PY 2024
VL 120
AR 103926
DI 10.1016/j.cag.2024.103926
EA MAY 2024
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TA4M3
UT WOS:001238522400001
DA 2024-08-05
ER

PT J
AU Lesar, Z
   Bohak, C
   Marolt, M
AF Lesar, Ziga
   Bohak, Ciril
   Marolt, Matija
TI Evaluation of depth perception in crowded volumes
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Volume rendering; Crowded volumes; Depth perception; User study
ID VISUALIZATION; ILLUMINATION; SCATTERING; SURFACES; MODELS
AB Depth perception in volumetric visualization plays a crucial role in the understanding and interpretation of volumetric data. Numerous visualization techniques, many of which rely on physically based optical effects, promise to improve depth perception but often do so without considering camera movement or the content of the volume. As a result, the findings from previous studies may not be directly applicable to crowded volumes, where a large number of contained structures disrupts spatial perception. Crowded volumes therefore require special analysis and visualization tools with sparsification capabilities. Interactivity is an integral part of visualizing and exploring crowded volumes, but has received little attention in previous studies. To address this gap, we conducted a study to assess the impact of different rendering techniques on depth perception in crowded volumes, with a particular focus on the effects of camera movement. The results show that depth perception considering camera motion depends much more on the content of the volume than on the chosen visualization technique. Furthermore, we found that conventional non-photorealistic rendering techniques, which have often performed poorly in previous studies, showed comparable performance to modern photorealistic techniques in our study. The source code for the visualization system, survey, and analysis, as well as the data set used in the study and the participants' responses, have been made publicly available.
C1 [Lesar, Ziga; Bohak, Ciril; Marolt, Matija] Univ Ljubljana, Fac Comp & Informat Sci, Vecna pot 113, Ljubljana 1000, Slovenia.
C3 University of Ljubljana
RP Lesar, Z (corresponding author), Univ Ljubljana, Fac Comp & Informat Sci, Vecna pot 113, Ljubljana 1000, Slovenia.
EM ziga.lesar@fri.uni-lj.si; ciril.bohak@fri.uni-lj.si;
   matija.marolt@fri.uni-lj.si
CR Bohak C, 2019, LECT NOTES COMPUT SC, V11613, P36, DOI 10.1007/978-3-030-25965-5_4
   Boucheny C, 2009, ACM T APPL PERCEPT, V5, DOI 10.1145/1462048.1462054
   Bruckner S, 2007, IEEE T VIS COMPUT GR, V13, P1344, DOI 10.1109/TVCG.2007.70555
   Bruckner S, 2009, COMPUT GRAPH FORUM, V28, P775, DOI 10.1111/j.1467-8659.2009.01474.x
   Chan MY, 2009, IEEE T VIS COMPUT GR, V15, P1283, DOI 10.1109/TVCG.2009.172
   Correa CD, 2011, IEEE T VIS COMPUT GR, V17, P192, DOI 10.1109/TVCG.2010.35
   Diaz C, 2017, PROCEEDINGS OF THE 2017 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P111, DOI 10.1109/ISMAR.2017.28
   Diaz J, 2010, IEEE EG S VOL GRAPH, DOI [10.2312/VG/VG10/093-100, DOI 10.2312/VG/VG10/093-100]
   Díaz J, 2017, VISUAL COMPUT, V33, P47, DOI 10.1007/s00371-015-1151-6
   Drouin S, 2020, IEEE T VIS COMPUT GR, V26, P2247, DOI 10.1109/TVCG.2018.2884940
   Englund R, 2018, COMPUT GRAPH FORUM, V37, P174, DOI 10.1111/cgf.13320
   Englund R, 2016, SIGGRAPH ASIA 2016 S, P1, DOI [10.1145/3002151.3002164, DOI 10.1145/3002151.3002164]
   Englund R, 2016, IEEE PAC VIS SYMP, P40, DOI 10.1109/PACIFICVIS.2016.7465249
   Grosset AVP, 2013, IEEE PAC VIS SYMP, P81, DOI 10.1109/PacificVis.2013.6596131
   Heinrich F, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3364245
   Howard IP, 2002, Stevens' handbook of experimental psychology, DOI [10.1002/0471214426.pas0103, DOI 10.1002/0471214426.PAS0103]
   Hu P, 2022, IEEE INT SYMP M AU R, P369, DOI 10.1109/ISMAR-Adjunct57072.2022.00080
   Jönsson D, 2014, COMPUT GRAPH FORUM, V33, P27, DOI 10.1111/cgf.12252
   Keinert B, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818131
   Kersten MA, 2006, IEEE T VIS COMPUT GR, V12, P1117, DOI 10.1109/TVCG.2006.139
   Kersten-Oertel M, 2014, IEEE T VIS COMPUT GR, V20, P391, DOI 10.1109/TVCG.2013.240
   Kniss J, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P109, DOI 10.1109/VISUAL.2002.1183764
   Koerner D, 2014, COMPUT GRAPH FORUM, V33, P178, DOI 10.1111/cgf.12342
   Kreiser J, 2021, IEEE T VIS COMPUT GR, V27, P3913, DOI 10.1109/TVCG.2020.2993992
   Kroes T, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0038586
   Kronander J, 2012, IEEE T VIS COMPUT GR, V18, P447, DOI 10.1109/TVCG.2011.35
   Lafortune E. P., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P91
   Langer MS, 2000, PERCEPTION, V29, P649, DOI 10.1068/p3060
   Le Muzic M, 2016, COMPUT GRAPH FORUM, V35, P161, DOI 10.1111/cgf.12892
   Lesar Ziga, 2024, Zenodo, DOI 10.5281/ZENODO.10555909
   Lesar Z, 2024, VISUAL COMPUT, V40, P1005, DOI 10.1007/s00371-023-02828-8
   Lesar Z, 2018, WEB3D 2018: THE 23RD INTERNATIONAL ACM CONFERENCE ON 3D WEB TECHNOLOGY, DOI 10.1145/3208806.3208814
   Lesar Z, 2015, PROC SPIE, V9416, DOI 10.1117/12.2082179
   LEVOY M, 1988, IEEE COMPUT GRAPH, V8, P29, DOI 10.1109/38.511
   Lindemann F, 2011, IEEE T VIS COMPUT GR, V17, P1922, DOI 10.1109/TVCG.2011.161
   Mamassian P, 1996, VISION RES, V36, P2351, DOI 10.1016/0042-6989(95)00286-3
   MAX N, 1995, IEEE T VIS COMPUT GR, V1, P99, DOI 10.1109/2945.468400
   McAuley S, 2012, ACM SIGGRAPH 2012 CO, P1, DOI DOI 10.1145/2343483.2343493
   Mekuc MÄ, 2022, COMPUT METH PROG BIO, V223, DOI 10.1016/j.cmpb.2022.106959
   Mekuc MZ, 2020, COMPUT BIOL MED, V119, DOI 10.1016/j.compbiomed.2020.103693
   Preim B, 2016, COMPUT GRAPH FORUM, V35, P501, DOI 10.1111/cgf.12927
   Ropinski T, 2006, LECT NOTES COMPUT SC, V4073, P93
   Ropinski T, 2010, IEEE PAC VIS SYMP, P169, DOI 10.1109/PACIFICVIS.2010.5429594
   Schott M, 2009, COMPUT GRAPH FORUM, V28, P855, DOI 10.1111/j.1467-8659.2009.01464.x
   Schretter C, 2012, Journal of Graphics Tools, V16, P95, DOI [10.1080/2165347X.2012.679555, DOI 10.1080/2165347X.2012.679555]
   Smajdek U, 2024, VISUAL COMPUT, V40, P2491, DOI 10.1007/s00371-023-02932-9
   Titov A, 2022, COMP M BIO BIO E-IV, V10, P357, DOI 10.1080/21681163.2021.1999332
   Toublanc D, 1996, APPL OPTICS, V35, P3270, DOI 10.1364/AO.35.003270
   Viola I, 2005, Proceedings of the First Eurographics Conference on Computational Aesthetics in Graphics, Visualization and Imaging, P209, DOI [10.2312/COMPAESTH/COMPAESTH05/209-216, DOI 10.2312/COMPAESTH/COMPAESTH05/209216, DOI 10.2312/COMPAESTH/COMPAESTH05/209-216]
   Viola I, 2005, EUROGRAPHICS 2005 TU, DOI [10.2312/egt.20051052, DOI 10.2312/EGT.20051052]
   Wanger L., 1992, ACM I3D Symposium on Interactive 3D Graphics, P39, DOI 10.1145/147156.147161
   Weier M, 2017, COMPUT GRAPH FORUM, V36, P611, DOI 10.1111/cgf.13150
   Weissenböck J, 2014, IEEE PAC VIS SYMP, P153, DOI 10.1109/PacificVis.2014.52
   Wexler M, 2005, TRENDS COGN SCI, V9, P431, DOI 10.1016/j.tics.2005.06.018
NR 54
TC 1
Z9 1
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD MAY
PY 2024
VL 120
AR 103918
DI 10.1016/j.cag.2024.103918
EA APR 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RY2T0
UT WOS:001231159700001
OA Green Submitted, hybrid
DA 2024-08-05
ER

PT J
AU Jurado-Rodríguez, D
   Latorre-Hortelano, P
   René-Dominguez, L
   Ortega, LM
AF Jurado-Rodriguez, David
   Latorre-Hortelano, Pablo
   Rene-Dominguez, Luis
   Ortega, Lidia M.
TI 3D Modeling of rural environments from multiscale aerial imagery
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE 3d modeling; Environmental preservation; Multiscale data sources;
   Semantic segmentation
AB Given the increasing attention to environmental preservation and sustainable development, the digitization of rural landscapes stands out as a pivotal strategy for effective environmental management and sustainability, land use planning, and preservation of cultural heritage. This work proposes a novel methodology for generating 3D models of rural landscapes by integrating multiscale data sources. Although Unmanned Aerial Vehicles (UAV) simplify the acquisition of multi-source data, their coverage is typically restricted to small landscapes due to their limited range and flight time. On the other hand, although the use of aerial images provides a broader view of the terrain, it is important to note that the low resolution of these images interferes with the task of accurate 3D modeling. Given these challenges, we propose a methodology that combines UAV data and high -resolution aerial imagery provided by the Spanish National Orthophoto Program (PNOA). This multi-source data integration is crucial to generating detailed and accurate 3D models of rural environments. The proposed methodology involves three steps: (1) semantic segmentation of aerial images identifying features such as vegetation, ground, and human -made structures, (2) estimation of the Digital Elevation Model (DEM), and (3) 3D modeling of rural environments using the point clouds generated from UAV images. The conducted experiments demonstrate the effectiveness of our approach identifying and representing previously mentioned features. Thus, this work presents advances in 3D representation techniques for real scenarios, contributing to the coordination of land utilization and environmental sustainability in rural landscapes.
C1 [Jurado-Rodriguez, David; Latorre-Hortelano, Pablo; Rene-Dominguez, Luis; Ortega, Lidia M.] Univ Jaen, Dept Comp Sci, Jaen, Spain.
C3 Universidad de Jaen
RP Jurado-Rodríguez, D (corresponding author), Univ Jaen, Dept Comp Sci, Jaen, Spain.
EM drodrigu@ujaen.es
FU Junta de Andalucia; Ministerio de Ciencia e Innovacion (Spain); European
   Union's ERDF funds;  [PID2022-137938OA-I00];  [PID2021-126339OB-I00]; 
   [TED2021-132120B-I00]
FX This project has been funded under the research projects with references
   PID2022-137938OA-I00, PID2021-126339OB-I00 and TED2021-132120B-I00.
   These projects are co-financed by the Junta de Andalucia, Ministerio de
   Ciencia e Innovacion (Spain) , and the European Union's ERDF funds.
CR Argudo O, 2018, COMPUT GRAPH FORUM, V37, P101, DOI 10.1111/cgf.13345
   Argudo O, 2018, COMPUT GRAPH-UK, V71, P23, DOI 10.1016/j.cag.2017.11.004
   Bhatnagar S, 2021, ISPRS J PHOTOGRAMM, V174, P151, DOI 10.1016/j.isprsjprs.2021.01.012
   Cardama FJ, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15112889
   Chehreh B, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15092263
   Deng LW, 2023, INT J APPL EARTH OBS, V125, DOI 10.1016/j.jag.2023.103588
   Deng SS, 2013, INT ARCH PHOTOGRAMM, V40-2, P105
   Du LM, 2024, GEO-SPAT INF SCI, V27, P811, DOI 10.1080/10095020.2023.2249037
   Hell M, 2022, PFG-J PHOTOGRAMM REM, V90, P103, DOI 10.1007/s41064-022-00200-4
   Iglhaut J, 2019, CURR FOR REP, V5, P155, DOI 10.1007/s40725-019-00094-3
   Jayakumari R, 2021, PRECIS AGRIC, V22, P1617, DOI 10.1007/s11119-021-09803-0
   Jurado JM, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20082244
   Kirillov A, 2023, IEEE I CONF COMP VIS, P3992, DOI 10.1109/ICCV51070.2023.00371
   Lindberg E, 2017, CURR FOR REP, V3, P19, DOI 10.1007/s40725-017-0051-6
   Lopez Ruiz A, 2019, Prototipo de control avanzado de grandes plantaciones mediante teledeteccion
   Ma HC, 2018, ISPRS J PHOTOGRAMM, V146, P260, DOI 10.1016/j.isprsjprs.2018.09.009
   Murcia HF, 2021, PLANTS-BASEL, V10, DOI 10.3390/plants10122804
   Murugan D, 2016, INT CONF IND INF SYS, P910, DOI 10.1109/ICIINFS.2016.8263068
   Sabins F, 2020, Remote Sensing: Principles, Interpretation, and Applications, Vfourth
   Sajedizadeh S, 2023, INT J REMOTE SENS, V44, P6375, DOI 10.1080/01431161.2023.2266121
   Tomková M, 2022, GEOMORPHOLOGY, V414, DOI 10.1016/j.geomorph.2022.108377
   Wegmann M., 2016, REMOTE SENSING GIS E
   Xu KJ, 2019, INT J REMOTE SENS, V40, P2784, DOI 10.1080/01431161.2018.1533656
   Zhang CL, 2023, MEASUREMENT, V223, DOI 10.1016/j.measurement.2023.113788
NR 24
TC 0
Z9 0
U1 4
U2 4
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103982
DI 10.1016/j.cag.2024.103982
EA JUN 2024
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XL3L4
UT WOS:001261799400001
OA hybrid
DA 2024-08-05
ER

PT J
AU Feng, WH
   Jiao, MX
   Liu, N
   Yang, L
   Zhang, ZY
   Hu, SJ
AF Feng, Weihuan
   Jiao, Mingxin
   Liu, Ning
   Yang, Long
   Zhang, Zhiyi
   Hu, Shaojun
TI Realistic reconstruction of trees from sparse images in volumetric space
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Tree modeling; Volumetric space; Multi-view images; Point cloud
AB The realistic reconstruction of real-world trees is a challenging task in the community of computer graphics because natural trees have complex structures of branches and leaves. Existing terrestrial laser scanning (TLS) system is able to capture dense and precise tree point clouds, yet the TLS system is expensive and not easy to carry around. An alternative low-cost and portable way is the reconstruction of tree point cloud from multiple view images. However, it is usually difficult to reconstruct a complete tree point cloud because of the texture similarity of branches and leaves as well as the lack of a sufficient number of images. Thus, we propose a new approach for reconstructing tree point clouds and geometries from sparse images. We first infer the camera parameters of each image, and then calculate the bounding volume of a tree from the camera parameters. Next, we set the mask of each image and the resolution of voxel, and then project each voxel in 3D space to all the mask images to determine the validity of the voxel. To alleviate the miss deletion of valid voxel, we utilize a boundary threshold and adjust mask resolution for robust point cloud reconstruction. Finally, an efficient tree reconstruction method is proposed to generate plausible tree geometries. We tested 6 different tree species that contain deciduous and evergreen trees, and the results showed that our approach is able to generate complete tree point cloud and realistic tree models even from a few number of images.
C1 [Feng, Weihuan; Jiao, Mingxin; Liu, Ning; Yang, Long; Zhang, Zhiyi; Hu, Shaojun] Northwest A&F Univ, Coll Informat Engn, Dept Comp Sci, Yangling, Peoples R China.
C3 Northwest A&F University - China
RP Hu, SJ (corresponding author), Northwest A&F Univ, Coll Informat Engn, 22 Xinong Rd, Yangling 712100, Peoples R China.
EM fwh999ds@163.com; jiaomx@nwafu.edu.cn; liuning1234@nwafu.edu.cn;
   yl@nwafu.edu.cn; zhangzhiyi@nwafu.edu.cn; hsj@nwsuaf.edu.cn
OI HU, Shaojun/0000-0002-4686-7633
FU Natural Science Basis Research (NSBR) Plan of Shaanxi, China
   [2022JM-363]; Jiangsu Water Resources Science and Technology Project,
   China [2023042]; National Natural Science Foundation of China [61303124]
FX <B>Acknowledgments</B> This work was supported in part by the Natural
   Science Basis Research (NSBR) Plan of Shaanxi, China under Grant
   2022JM-363, Jiangsu Water Resources Science and Technology Project,
   China under Grant 2023042 and the National Natural Science Foundation of
   China under Grant 61303124.
CR Argudo O, 2016, COMPUT GRAPH-UK, V57, P55, DOI 10.1016/j.cag.2016.03.005
   Deussen O., 2005, Digital Design of Nature: Computer Generated Plants and Organics
   Du SL, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11182074
   Furukawa Y, 2009, INT J COMPUT VISION, V81, P53, DOI 10.1007/s11263-008-0134-8
   Guo JW, 2020, IEEE T VIS COMPUT GR, V26, P1372, DOI 10.1109/TVCG.2018.2869784
   Hu SJ, 2017, COMPUT GRAPH-UK, V67, P1, DOI 10.1016/j.cag.2017.04.004
   Hu SJ, 2017, VISUAL COMPUT, V33, P1017, DOI 10.1007/s00371-017-1377-6
   Igarashi T, 1999, COMP GRAPH, P409, DOI 10.1145/311535.311602
   Kirillov A, 2023, Arxiv, DOI arXiv:2304.02643
   Lazebnik S, 2007, INT J COMPUT VISION, V74, P137, DOI 10.1007/s11263-006-0008-x
   Liu LJ, 2021, Arxiv, DOI arXiv:2007.11571
   Liu YC, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480486
   Liu ZH, 2021, GRAPH MODELS, V117, DOI 10.1016/j.gmod.2021.101115
   Livny Y, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964948
   Livny Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866177
   Lv YX, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3055773
   Mildenhall B, 2020, Arxiv, DOI [arXiv:2003.08934, 10.1145/3503250, DOI 10.1145/3503250]
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Neubert B, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239539
   Palubicki W, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531364
   Prusinkiewicz P., 1990, ALGORITHMIC BEAUTY P
   Reche A, 2004, ACM T GRAPHIC, V23, P720, DOI 10.1145/1015706.1015785
   Runions A., 2007, MODELING TREES SPACE, P63, DOI DOI 10.2312/NPH/NPH07/063-070
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Shlyakhter I, 2001, IEEE COMPUT GRAPH, V21, P53, DOI 10.1109/38.920627
   Tan P, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409061
   Tan P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239538
   Wang P, 2021, Arxiv, DOI arXiv:2106.10689
   Wu CC, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P127, DOI 10.1109/3DV.2013.25
   Xu H, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1289603.1289610
NR 30
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD JUN
PY 2024
VL 121
AR 103953
DI 10.1016/j.cag.2024.103953
EA JUN 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XT1P6
UT WOS:001263840700001
DA 2024-08-05
ER

PT J
AU Liu, JJ
   Yao, YY
   Fei, Y
   Zhang, GF
   Zheng, LP
AF Liu, Jingjing
   Yao, Yuyou
   Fei, Yue
   Zhang, Gaofeng
   Zheng, Liping
TI Surface remeshing with preservation of sharp features through iterative
   identification and optimization of sample points
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Sample point projection; Edge continuity; Feature preservation; Surface
   remeshing
ID RECONSTRUCTION
AB High -quality triangular meshes with sharp features are necessary for geometric processing applications, e.g., CAD models. Existing approaches either directly utilize the original sharp features as outputs, or predetermine feature sample points and reconstruct feature edges after an optimization process. However, these may result in poor -quality triangular facets close to feature edges or even feature preservation failure. With this regard, we present a feature -preservation surface remeshing method to generate high -quality triangular meshes with sharp feature preservation from raw manifold meshes. Unlike existing predetermination strategy, we introduce a heuristic strategy to dynamically determine feature sample points in each iteration. Based on the projection distance, each feature sample point is identified and projected onto the original feature edges, achieving the preservation of feature sample points. Besides, to eliminate the large gaps between two adjacent feature sample points and edge -close sample points, a feature edge continuity preservation strategy is proposed. Some additional sample points are further projected onto the large gaps and the edge -close sample points are deviated from the feature edges, achieving the preservation of reconstructed feature edges. Experimental results on several models demonstrate the effectiveness and efficiency of our method in generating high -quality triangular meshes with sharp feature preservation.
C1 [Liu, Jingjing; Fei, Yue; Zhang, Gaofeng; Zheng, Liping] Hefei Univ Technol, Sch Comp Sci Informat Engn, Hefei 230601, Peoples R China.
   [Yao, Yuyou] Anhui Univ Sci & Technol, Sch Publ Secur & Emergency Management, Hefei 231131, Peoples R China.
C3 Hefei University of Technology; Anhui University of Science & Technology
RP Zheng, LP (corresponding author), Hefei Univ Technol, Sch Comp Sci Informat Engn, Hefei 230601, Peoples R China.
EM 2021111072@mail.hfut.edu.cn; yaoyy@aust.edu.cn; fy@mail.hfut.edu.cn;
   g.zhang@hfut.edu.cn; zhenglp@hfut.edu.cn
FU National Natural Science of Foundation of China [62372152]; National Key
   Research and Development Plan [2022YFC3900800]
FX <B>Acknowledgments</B> The models used in this paper are selected from
   the AIM@Shape and courtesy of the Standford 3D Scanning Repository. We
   would like to thank Kai-Mo Hu for their executable programs. This work
   was supported in part by a grant from the National Natural Science of
   Foundation of China (No. 62372152) and the National Key Research and
   Development Plan (No. 2022YFC3900800) .
CR Abdelkader A, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3337680
   Alliez P, 2005, GRAPH MODELS, V67, P204, DOI 10.1016/j.gmod.2004.06.007
   Boltcheva D, 2017, COMPUT AIDED DESIGN, V90, P123, DOI 10.1016/j.cad.2017.05.011
   Botsch M, 2010, Polygon Mesh Processing, DOI DOI 10.1201/B10688
   Chen JT, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508375
   Chen Y, 2021, 2021 IEEE INT WORKSH, P1
   Engwirda D, 2014, PROCEDIA ENGINEER, V82, P8, DOI 10.1016/j.proeng.2014.10.364
   Eulitz M, 2015, J STRUCT BIOL, V191, P190, DOI 10.1016/j.jsb.2015.06.010
   Fuhrmann S., 2010, VMV, P9, DOI DOI 10.2312/PE/VMV/VMV10/009-016
   Hu KM, 2017, IEEE T VIS COMPUT GR, V23, P2560, DOI 10.1109/TVCG.2016.2632720
   Hurtado J, 2022, ENG COMPUT-GERMANY, V38, P781, DOI 10.1007/s00366-020-01040-9
   Jakob W, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818078
   Jiao X., 2002, PROC 8 INT C NUMER G, P705
   Jong BS, 2010, VISUAL COMPUT, V26, P121, DOI 10.1007/s00371-009-0392-7
   Kazhdan M, 2020, COMPUT GRAPH FORUM, V39, P173, DOI 10.1111/cgf.14077
   Khan D, 2022, IEEE T VIS COMPUT GR, V28, P1680, DOI 10.1109/TVCG.2020.3016645
   Liang XH, 2011, COMPUT METHOD APPL M, V200, P2005, DOI 10.1016/j.cma.2011.03.002
   Sellán S, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555441
   Sun Y, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P825, DOI 10.1109/ICIP.2002.1039099
   Valette S, 2008, IEEE T VIS COMPUT GR, V14, P369, DOI 10.1109/TVCG.2007.70430
   Vorsatz J, 2001, COMPUT GRAPH FORUM, V20, pC393, DOI 10.1111/1467-8659.00532
   Wang YQ, 2019, IEEE T VIS COMPUT GR, V25, P2430, DOI 10.1109/TVCG.2018.2837115
   Xu M, 2019, P 2019 7 INT C INF T, P536
   Yan DM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2516971.2516973
   Yan DM, 2009, COMPUT GRAPH FORUM, V28, P1445, DOI 10.1111/j.1467-8659.2009.01521.x
   Yao YY, 2023, COMPUT GRAPH FORUM, V42, DOI 10.1111/cgf.14897
   Yao YY, 2023, COMPUT AIDED GEOM D, V104, DOI 10.1016/j.cagd.2023.102216
   Yue W, 2007, P 2007 ACM S SOL PHY, P23
   Zhang WX, 2022, COMPUT GRAPH FORUM, V41, P237, DOI 10.1111/cgf.14471
NR 29
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD JUN
PY 2024
VL 121
AR 103949
DI 10.1016/j.cag.2024.103949
EA JUN 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UV5T4
UT WOS:001250857100001
DA 2024-08-05
ER

PT J
AU Tadeja, SK
   Bohné, T
   Godula, K
   Cybulski, A
   Wozniak, MM
AF Tadeja, Slawomir Konrad
   Bohne, Thomas
   Godula, Kacper
   Cybulski, Artur
   Wozniak, Magdalena Maria
TI Immersive presentations of real-world medical equipment through
   interactive VR environment populated with the high-fidelity 3D model of
   mobile MRI unit
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Virtual reality; VR; Immersive interface; Magnetic resonance imaging;
   MRI; Virtual presentation
ID VISUALIZATION
AB The primary goal behind the system presented in this paper is to investigate the efficacy of using virtual reality (VR) for showcasing sizable medical equipment. Specifically, we focused on a mobile magnetic resonance imaging (MRI) scanner mounted on a truck trailer. The latter is integral to the mobile MRI setup and must be presented as part of the immersive experience. Therefore, we not only have to depict the medical apparatus but also provide the means of understanding its surroundings. This is especially important to radiologists and other medical personnel to ascertain if a given mobile medical facility fulfills their needs and wants. Furthermore, despite such MRI devices being designed for mobility, their long-distance transportation can be time-consuming, troublesome and expensive. Therefore, we can observe the need for showcasing such mobile MRI units without additional cost and burden related to transportation. To achieve this, we designed an immersive environment in which the users can interact with the real-life scale 3D model of a mobile MRI. In addition, we also verified the usability and expressiveness of our system using established heuristical approaches.
C1 [Tadeja, Slawomir Konrad; Bohne, Thomas] Univ Cambridge, Cambridge CB3 0FS, England.
   [Tadeja, Slawomir Konrad; Godula, Kacper; Cybulski, Artur] Immers Sp Zoo, PL-00013 Warsaw, Poland.
   [Wozniak, Magdalena Maria] Med Univ Lublin, PL-20059 Lublin, Poland.
   [Tadeja, Slawomir Konrad; Wozniak, Magdalena Maria] Eurodiagnost Sp Zoo, PL-02530 Warsaw, Poland.
C3 University of Cambridge; Medical University of Lublin
RP Tadeja, SK (corresponding author), Univ Cambridge, Cambridge CB3 0FS, England.
EM skt40@eng.cam.ac.uk
FU European Regional Development Fund
FX The authors thank Agnieszka Dopiera & lstrok;a for the administration
   and logistics of this project. We also thank Reciprocal Space for
   preparing the mobile MRI unit model. This research was supported by the
   project entitled "The product expansion of the Mobile MRI Clinic in the
   Middle East as a pillar of business development for Eurodiagnostic sp. z
   o.o". under mea- sure 1.2 Internationalisation of SMEs of the
   Operational Programme Eastern Poland 2014-2020, co -financed by the
   European Regional Development Fund.
CR Alcañiz M, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.01530
   Allison B, 2020, 2020 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR 2020), P247, DOI 10.1109/AIVR50618.2020.00052
   Allison B, 2020, 2020 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR 2020), P252, DOI 10.1109/AIVR50618.2020.00053
   Alyami A, 2023, J RADIAT RES APPL SC, V16, DOI 10.1016/j.jrras.2023.100669
   Ando H., 1998, Design Studies, V19, P289, DOI 10.1016/S0142-694X(98)00007-6
   [Anonymous], 2014, Magnetic Resonance Imaging: Physical Principles and Sequence Design, V2
   Ard T, 2017, P IEEE VIRT REAL ANN, P465, DOI 10.1109/VR.2017.7892381
   August K, 2006, 2006 28TH ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING IN MEDICINE AND BIOLOGY SOCIETY, VOLS 1-15, P2487
   Blackwell AF, 2001, LECT NOTES ARTIF INT, V2117, P325
   Borkent B, 2016, Recordings of an MRI scanner of the UMC Utrecht, used under CC by 3.0
   Buckingham G, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.728461
   Cecotti H, 2020, PROCEEDINGS OF 2020 6TH INTERNATIONAL CONFERENCE OF THE IMMERSIVE LEARNING RESEARCH NETWORK (ILRN 2020), P205, DOI [10.23919/ilrn47897.2020.9155206, 10.23919/iLRN47897.2020.9155206]
   Chheang V, 2021, COMPUT GRAPH-UK, V99, P234, DOI 10.1016/j.cag.2021.07.009
   Cigánek J, 2020, PROCEEDINGS OF THE 2020 30TH INTERNATIONAL CONFERENCE CYBERNETICS & INFORMATICS (K&I '20), DOI 10.1109/ki48306.2020.9039896
   Damaser MS, 2002, P ANN INT IEEE EMBS, P2395
   Deoni SCL, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-09760-2
   Di Diodato LM, 2007, IEEE T NEUR SYS REH, V15, P570, DOI 10.1109/TNSRE.2007.906962
   Duncan D, 2017, P IEEE VIRT REAL ANN, P467, DOI 10.1109/VR.2017.7892382
   Dzardanova E, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P813, DOI 10.1109/VRW55335.2022.00262
   Eghbali P, 2019, MUM 2019: 18TH INTERNATIONAL CONFERENCE ON MOBILE AND UBIQUITOUS MULTIMEDIA, DOI 10.1145/3365610.3365647
   Fischer Roland, 2020, Virtual Reality and Augmented Reality. 17th EuroVR International Conference, EuroVR 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12499), P178, DOI 10.1007/978-3-030-62655-6_11
   Glover GH, 2011, NEUROSURG CLIN N AM, V22, P133, DOI 10.1016/j.nec.2010.11.001
   Grilo Ana Monteiro, 2023, Tech Innov Patient Support Radiat Oncol, V25, P100203, DOI 10.1016/j.tipsro.2023.100203
   Halbig A, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.837616
   HART S G, 1988, P139
   Jerald J, 2017, P IEEE VIRT REAL ANN, P431, DOI 10.1109/VR.2017.7892361
   Jian Siong Fong, 2010, 2010 2nd International Conference on Software Technology and Engineering (ICSTE 2010), P41, DOI 10.1109/ICSTE.2010.5608963
   Kennedy R.S., 1993, Int. J. Aviat. Psy, P203
   KIKINIS R, 1994, ELECTRO '94, P629, DOI 10.1109/ELECTR.1994.472661
   Kim MJ, 2021, CLIN SIMUL NURS, V60, P11, DOI 10.1016/j.ecns.2021.06.010
   Klonig J, 2020, IEEE INT CONF HEALT, P171, DOI 10.1109/ICHI48887.2020.9374344
   Kokelj Z, 2018, 2018 41ST INTERNATIONAL CONVENTION ON INFORMATION AND COMMUNICATION TECHNOLOGY, ELECTRONICS AND MICROELECTRONICS (MIPRO), P299, DOI 10.23919/MIPRO.2018.8400057
   Lam H, 2012, IEEE T VIS COMPUT GR, V18, P1520, DOI 10.1109/TVCG.2011.279
   Langbehn E, 2017, P IEEE VIRT REAL ANN, P449, DOI 10.1109/VR.2017.7892373
   Likert R., 1932, Archives of Psychology, V22, P55
   van den Bor MVL, 2022, PATIENT EDUC COUNS, V105, P1828, DOI 10.1016/j.pec.2021.12.015
   Liu MM, 2022, FRONT NEUROSCI-SWITZ, V16, DOI 10.3389/fnins.2022.975217
   Lorensen H. E., 1987, Proc. SIGGRAPH, V21, P163, DOI 10.1145/37401.37422
   Mason L, 2015, CONTEMP EDUC PSYCHOL, V41, P172, DOI 10.1016/j.cedpsych.2015.01.004
   Naderifar F, 2020, 2020 IEEE/ACM INTERNATIONAL CONFERENCE ON CONNECTED HEALTH: APPLICATIONS, SYSTEMS AND ENGINEERING TECHNOLOGIES (CHASE 2020), P7, DOI 10.1145/1234567890
   Nakagomi M, 2019, J MAGN RESON, V304, P1, DOI 10.1016/j.jmr.2019.04.017
   NIELSEN J, 1994, HUMAN FACTORS IN COMPUTING SYSTEMS, CHI '94 CONFERENCE PROCEEDINGS - CELEBRATING INTERDEPENDENCE, P152, DOI 10.1145/191666.191729
   Nielsen J., 1994, Usability inspection methods, DOI [10.1145/259963.260531, DOI 10.1145/259963.260531]
   Nobaew B, 2022, 2022 JOINT INT C DIG, P480, DOI [10.1109/ECTIDAMTNCON53731.2022.9720394, DOI 10.1109/ECTIDAMTNCON53731.2022.9720394]
   Portelli M, 2020, ANN ROY COLL SURG, V102, P672, DOI 10.1308/rcsann.2020.0178
   Pottle Jack, 2019, Future Healthc J, V6, P181, DOI 10.7861/fhj.2019-0036
   Prithul A, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.730792
   Reddivari S, 2020, P INT COMP SOFTW APP, P1129, DOI 10.1109/COMPSAC48688.2020.0-106
   Regenbrecht H, 2002, PRESENCE-TELEOP VIRT, V11, P425, DOI 10.1162/105474602760204318
   Ruddle RA, 2009, ACM T COMPUT-HUM INT, V16, DOI 10.1145/1502800.1502805
   Sato H, 1998, P ANN INT IEEE EMBS, V20, P1246, DOI 10.1109/IEMBS.1998.747101
   Schubert T, 2001, PRESENCE-VIRTUAL AUG, V10, P266, DOI 10.1162/105474601300343603
   Serrell B., 1997, Curator, V40, P108, DOI [https://doi.org/10.1111/j.2151-6952.1997.tb01292.x, DOI 10.1111/J.2151-6952.1997.TB01292.X]
   Shefelbine Sandra., 2002, GOOD DESIGN PRACTICE
   Shewaga R, 2020, IEEE T EMERG TOP COM, V8, P218, DOI 10.1109/TETC.2017.2746085
   Shindo Y, 2019, IEEE IND ELEC, P6961, DOI 10.1109/IECON.2019.8927160
   Smith-Bindman R, 2012, JAMA-J AM MED ASSOC, V307, P2400, DOI 10.1001/jama.2012.5960
   Soccini AM, 2020, 2020 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR 2020), P394, DOI 10.1109/AIVR50618.2020.00082
   Soler L, 2004, ISMAR 2004: THIRD IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P278, DOI 10.1109/ISMAR.2004.64
   Stefanidis D, 2012, ANN SURG, V255, P30, DOI 10.1097/SLA.0b013e318220ef31
   Stunden C, 2021, J MED INTERNET RES, V23, DOI 10.2196/22942
   Tadeja SK, 2020, AERONAUT J, V124, P1615, DOI 10.1017/aer.2020.49
   Tadeja S, 2023, 2023 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS, VRW, P183, DOI 10.1109/VRW58643.2023.00045
   Tadeja SK, 2021, IEEE COMPUT GRAPH, V41, P143, DOI 10.1109/MCG.2021.3114955
   Tadeja SK, 2020, AEROSP CONF PROC, DOI 10.1109/aero47225.2020.9172389
   Taswell SK, 2017, IEEE ENG MED BIO, P3704, DOI 10.1109/EMBC.2017.8037662
   Thamrongrat P, 2023, Emerg Sci J, V7, P1063, DOI [10.28991/ESJ-2023-07-04-03, DOI 10.28991/ESJ-2023-07-04-03]
   Torner J, 2019, IEEE T NEUR SYS REH, V27, P1511, DOI 10.1109/TNSRE.2019.2926786
   Tucker N, 2022, PROCEEDINGS OF THE 2022 ANNUAL MODELING AND SIMULATION CONFERENCE (ANNSIM'22), P392, DOI 10.23919/ANNSIM55834.2022.9859317
   Villaseñor MA, 2001, P ANN INT IEEE EMBS, V23, P3769, DOI 10.1109/IEMBS.2001.1019658
   Warrick P A, 1998, IEEE Trans Inf Technol Biomed, V2, P55, DOI 10.1109/4233.720523
   Williams C, 2019, COMP MED SY, P495, DOI 10.1109/CBMS.2019.00101
   Wixson J.R., 1999, INCOSE Int Symp, P800, DOI [DOI 10.1002/J.2334-5837.1999.TB00241.X, 10.1002/j.2334-5837.1999.tb00241.x]
   Wong CW, 2013, I IEEE EMBS C NEUR E, P1533, DOI 10.1109/NER.2013.6696238
   Woods JC, 2020, J MAGN RESON IMAGING, V52, P1306, DOI 10.1002/jmri.27030
   Woodworth JW, 2023, 2023 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS, VRW, P442, DOI 10.1109/VRW58643.2023.00096
NR 76
TC 1
Z9 1
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD MAY
PY 2024
VL 120
AR 103919
DI 10.1016/j.cag.2024.103919
EA APR 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6G7
UT WOS:001231512900001
OA hybrid
DA 2024-08-05
ER

PT J
AU Jia, HB
   Yin, QB
   Lu, MY
AF Jia, Hongbin
   Yin, Qingbo
   Lu, Mingyu
TI Steering Kernel Weighted Guided Image Filtering with Gradient Constraint
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Weighted guided image filter; Edge-preserving filtering; Gradient
   constraints; Tone mapping of high range dynamic images; Denoising
AB In order to make the weighted guided image filter (WGIF) more fully utilize the edge direction of the image, a steering kernel weighted guided image filter (SKWGIF) is proposed by employing the steering kernel to learn the edge direction and incorporating the learning results into the WGIF. However, SKWGIF does not provide a good compromise between the two possibly contradictory objectives of edge-preserving and smoothing, where the image edges are inevitably smoothed. To overcome the drawback, a SKWGIF with gradient constraint (GC-SKWGIF) is proposed by introducing the gradient constraints into the SKWGIF. The gradient constraints allow the filter to take into account the gradient variation of the edges in the filtering process, and therefore the image edges can be better preserved. To verify the effectiveness of the proposed filter, the GC-SKWGIF is applied to edge-aware smoothing, tone mapping of high range dynamic images, image denoising and haze removal. Both theoretical analysis and experimental results show that the proposed filter can produce good resultant images.
C1 [Jia, Hongbin; Yin, Qingbo; Lu, Mingyu] Dalian Maritime Univ, Coll Informat Sci & Technol, Intelligent Technol Res Ctr, 1 LingHai Rd, Dalian 116026, Peoples R China.
C3 Dalian Maritime University
RP Lu, MY (corresponding author), Dalian Maritime Univ, Coll Informat Sci & Technol, Intelligent Technol Res Ctr, 1 LingHai Rd, Dalian 116026, Peoples R China.
EM lumingyu@dlmu.edu.cn
FU National Natural Science Foundation of China [61976124]
FX This work was supported by National Natural Science Foundation of China
   (No. 61976124) .
CR Awate SP, 2006, IEEE T PATTERN ANAL, V28, P364, DOI 10.1109/TPAMI.2006.64
   Bhat P, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731048
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dai LQ, 2019, IEEE T IMAGE PROCESS, V28, P767, DOI 10.1109/TIP.2018.2869720
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Fattal R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239502, 10.1145/1276377.1276441]
   Guo XJ, 2020, IEEE T PATTERN ANAL, V42, P694, DOI 10.1109/TPAMI.2018.2883553
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hu JW, 2012, INFORM FUSION, V13, P196, DOI 10.1016/j.inffus.2011.01.002
   Hua M, 2014, PROC CVPR IEEE, pCP1, DOI 10.1109/CVPR.2014.363
   Kang Y, 2012, IEEE T IND ELECTRON, V59, P4360, DOI 10.1109/TIE.2012.2185013
   Kou F, 2015, IEEE T IMAGE PROCESS, V24, P4528, DOI 10.1109/TIP.2015.2468183
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Li ZG, 2015, IEEE T IMAGE PROCESS, V24, P120, DOI 10.1109/TIP.2014.2371234
   Lu ZW, 2018, IEEE SIGNAL PROC LET, V25, P1585, DOI 10.1109/LSP.2018.2867896
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Niu Y, 2019, IEEE T IMAGE PROCESS, V28, P2415, DOI 10.1109/TIP.2018.2883815
   Ochotorena CN, 2020, IEEE T IMAGE PROCESS, V29, P1397, DOI 10.1109/TIP.2019.2941326
   Panigrahi SK, 2021, INT J IMAGE GRAPH, V21, DOI 10.1142/S0219467821500492
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Shi ZL, 2021, IEEE T IMAGE PROCESS, V30, P7472, DOI 10.1109/TIP.2021.3106812
   Sun ZG, 2020, IEEE T IMAGE PROCESS, V29, P500, DOI 10.1109/TIP.2019.2928631
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
NR 26
TC 0
Z9 0
U1 2
U2 2
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103908
DI 10.1016/j.cag.2024.103908
EA MAR 2024
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PV6U1
UT WOS:001216905100001
DA 2024-08-05
ER

PT J
AU Simon, C
   Herfort, L
   Lebrun, F
   Brocas, E
   Otmane, S
   Chellali, A
AF Simon, Cassandre
   Herfort, Lucas
   Lebrun, Flavien
   Brocas, Elsa
   Otmane, Samir
   Chellali, Amine
TI Design and evaluation of UltRASim: An immersive simulator for learning
   ultrasound-guided regional anesthesia basic skills
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Immersive simulation; Medical training; Simulator fidelity; Haptic
   feedback; Ultrasound-guided regional anesthesia
ID VIRTUAL ENVIRONMENT; REALITY SIMULATION; VALIDATION; FIDELITY
AB Virtual reality shows great promise as a technology for training healthcare professionals within a secure simulated environment. This work presents the design, development, and assessment of UltRASim: an immersive simulator for ultrasound-guided regional anesthesia. First, task and skills analyses were performed with domain experts to build the task model of the procedure and determine the simulator's learning objectives and design constraints. Then, a face and content validity study was conducted with eighteen anesthesiologists to assess the simulator's prototype. The responses to seven of eleven face validity questions were predominantly positive, indicating a favorable reception. The primary concerns pertained to the fidelity of haptic feedback during needle insertion. This suggests incorporating a higher fidelity haptic device in future design iterations. Conversely, responses to all six questions related to the content validity were predominantly positive. Participants found that the simulator held significant potential as a training tool, particularly for developing hand-eye coordination skills. These findings validate several design choices and highlight areas for improvement in subsequent iterations of UltRASim before its formal validation as a training tool.
C1 [Simon, Cassandre; Herfort, Lucas; Lebrun, Flavien; Otmane, Samir; Chellali, Amine] Univ Paris Saclay, Univ Evry, IBISC Lab, F-91020 Evry, France.
   [Brocas, Elsa] Ctr Hosp Sud Francilien, Dept Diabetol, F-91000 Corbeil essonnes, France.
   [Chellali, Amine] Univ Paris Saclay, Univ Evry, IBISC Lab, 40,Rue Pelvoux,CE1455, F-91020 Courcouronnes, France.
C3 Universite Paris Saclay; Universite Paris Cite; Centre Hospitalier Sud
   Francilien; Universite Paris Cite; Universite Paris Saclay
RP Chellali, A (corresponding author), Univ Paris Saclay, Univ Evry, IBISC Lab, 40,Rue Pelvoux,CE1455, F-91020 Courcouronnes, France.
EM simon.cassandre@univ-evry.fr; lucas.herfort@univ-evry.fr;
   flavien.lebrun@univ-evry.fr; elsa.brocas@chsf.fr;
   samir.otmane@univ-evry.fr; amine.chellali@univ-evry.fr
RI Chellali, Amine/K-1059-2019
OI Chellali, Amine/0000-0002-6143-5898
FU French National Research Agency, France [ANR-20-CE33-0010]; Genopole;
   Agence Nationale de la Recherche (ANR) [ANR-20-CE33-0010] Funding
   Source: Agence Nationale de la Recherche (ANR)
FX This work was supported by the French National Research Agency, France
   (grant #ANR-20-CE33-0010 Show-me) and Genopole.
CR Alamilla MA, 2022, IEEE T MED ROBOT BIO, V4, P634, DOI 10.1109/TMRB.2022.3175095
   Alsalamah A, 2017, GYNECOL SURG, V14, DOI 10.1186/s10397-017-1020-6
   [Anonymous], Official Gazette FR. Arrete du 27 novembre 2017 modifiant l'arrete du 12 avril 2017 relatif a l'organisation du troisieme cycle des etudes de medecine et arrete du 21 avril 2017 relatif aux connaissances, aux competences et aux maquettes de formation des diplomes d'etudes specialisees et fixant la liste de ces diplomes et des options et formations specialisees transversales du troisieme cycle des etudes de medecine
   Arikatla VS, 2013, SURG ENDOSC, V27, P1721, DOI 10.1007/s00464-012-2664-y
   Balcombe J, 2004, ATLA-ALTERN LAB ANIM, V32, P553, DOI 10.1177/026119290403201s90
   Bibin L., 2008, ACM Symposium on Virtual Reality Software and Technology (VRST 2008), P97, DOI [10.1145/1450579.1450600, DOI 10.1145/1450579.1450600]
   Bisseret A, 1999, Techniques pratiques pour l'etude des activites expertes
   Blum T, 2013, SIMUL HEALTHC, V8, P98, DOI 10.1097/SIH.0b013e31827ac273
   Botden SMBI, 2009, SURG ENDOSC, V23, P1693, DOI 10.1007/s00464-008-0144-1
   Breedveld P, 2001, MINIM INVASIV THER, V10, P155
   Brickler D, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P28, DOI [10.1109/vr.2019.8797744, 10.1109/VR.2019.8797744]
   Buckley CE, 2012, VIRTUAL REALITY IN PSYCHOLOGICAL, MEDICAL AND PEDAGOGICAL APPLICATIONS, P139, DOI 10.5772/46415
   Chan V, 2006, Guide pratique des blocs nerveux echoguides
   Chellali A, 2016, INT J HUM-COMPUT ST, V96, P22, DOI 10.1016/j.ijhcs.2016.07.005
   Chen XX, 2017, REGION ANESTH PAIN M, V42, P741, DOI 10.1097/AAP.0000000000000639
   Chuan A, 2023, ANAESTHESIA, V78, P739, DOI 10.1111/anae.16015
   Coles TR, 2011, IEEE T HAPTICS, V4, P199, DOI [10.1109/ToH.2011.32, 10.1109/TOH.2011.32]
   Coles TR, 2011, IEEE T HAPTICS, V4, P51, DOI 10.1109/ToH.2010.19
   Correa Cleber G., 2014, Virtual, Augmented and Mixed Reality. Applications of Virtual and Augmented Reality. 6th International Conference, VAMR 2014, Held as Part of HCI International 2014. Proceedings: LNCS 8526, P267, DOI 10.1007/978-3-319-07464-1_25
   Corrêa CG, 2019, MED ENG PHYS, V63, P6, DOI 10.1016/j.medengphy.2018.11.002
   Dieckmann P, 2009, WORK RES MULTIDISCIP, V3, P1
   Drews F.A., 2013, Reviews of Human Factors and Ergonomics, V8, P191
   Enquobahrie A, 2019, HEALTHC TECHNOL LETT, V6, P210, DOI 10.1049/htl.2019.0081
   Escobar-Castillejos D, 2016, J MED SYST, V40, DOI 10.1007/s10916-016-0459-8
   Forsslund J, 2011, 1 WORKSHOP ENG INTER, P42
   Geis WP, 1996, SURG ENDOSC-ULTRAS, V10, P768
   Grottke O, 2009, BRIT J ANAESTH, V103, P594, DOI 10.1093/bja/aep224
   Halsted WS, 1904, B JOHNS HOPKINS HOSP, V15, P267
   Hamblin C.J., 2005, Transfer of training from virtual reality environments
   Harris DJ, 2020, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.00605
   Henshall G, 2015, P IEEE VIRT REAL ANN, P191, DOI 10.1109/VR.2015.7223360
   Huang VW, 2020, INT ANESTHESIOL CLIN, V58, P31, DOI 10.1097/AIA.0000000000000298
   Huguet L, 2016, STUD HEALTH TECHNOL, V220, P146, DOI 10.3233/978-1-61499-625-5-146
   Hutton M, 2018, BJA EDUC, V18, P52, DOI 10.1016/j.bjae.2017.10.002
   Jain Saurabh, 2020, IISE Transactions on Healthcare Systems Engineering, V10, P127, DOI 10.1080/24725579.2019.1692263
   Kawaguchi K, 2014, MINIM INVASIV THER, V23, P287, DOI 10.3109/13645706.2014.903853
   Kim HK, 2003, LECT NOTES COMPUT SC, V2878, P1
   Lecuyer A., 2000, Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048), P83, DOI 10.1109/VR.2000.840369
   Lev DD, 2010, LECT NOTES COMPUT SC, V6192, P432, DOI 10.1007/978-3-642-14075-4_64
   Lin YP, 2014, J BIOMED INFORM, V48, P122, DOI 10.1016/j.jbi.2013.12.010
   Mariano ER, 2014, BEST PRACT RES-CLIN, V28, P29, DOI 10.1016/j.bpa.2013.11.001
   Matsumoto ED, 2011, CUAJ-CAN UROL ASSOC, V5, P27, DOI 10.5489/cuaj.11016
   McMains K, 2008, Internet J Otorhinolaryngol, V11
   Mentis HM, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P2113, DOI 10.1145/2556288.2557387
   Monzon Emilio Loren Roth, 2012, 2012 IEEE Haptics Symposium (HAPTICS), P199, DOI 10.1109/HAPTIC.2012.6183791
   Nix CM, 2013, REGION ANESTH PAIN M, V38, P471, DOI 10.1097/AAP.0b013e3182a4ed7a
   Okamura AM, 2004, IEEE T BIO-MED ENG, V51, P1707, DOI 10.1109/TBME.2004.831542
   Orzech N, 2012, ANN SURG, V255, P833, DOI 10.1097/SLA.0b013e31824aca09
   Panait L, 2009, J SURG RES, V156, P312, DOI 10.1016/j.jss.2009.04.018
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   Ravali Gourishetti, 2017, IEEE Rev Biomed Eng, V10, P63, DOI 10.1109/RBME.2017.2706966
   Ricca A, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P103, DOI 10.1109/VR50410.2021.00031
   Ricca A, 2020, INT SYM MIX AUGMENT, P260, DOI 10.1109/ISMAR50242.2020.00049
   Ricca A, 2021, VIRTUAL REAL-LONDON, V25, P191, DOI 10.1007/s10055-020-00445-7
   Rosenberg AD, 2012, REGION ANESTH PAIN M, V37, P106, DOI 10.1097/AAP.0b013e31823699ab
   Sainsbury B, 2020, FRONT ROBOT AI, V6, DOI 10.3389/frobt.2019.00145
   Satava R M, 2003, Surg Endosc, V17, P220, DOI 10.1007/s00464-002-8869-8
   Seymour NE, 2002, ANN SURG, V236, P458, DOI 10.1097/00000658-200210000-00008
   Simon C, 2023, COMPUT GRAPH-UK, V117, P31, DOI 10.1016/j.cag.2023.09.011
   Simon C, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P436, DOI 10.1109/VRW55335.2022.00098
   Sites BD, 2010, REGION ANESTH PAIN M, V35, pS74, DOI 10.1097/AAP.0b013e3181d34ff5
   Sparks S, 2014, CRIT ULTRASOUND J, V6, DOI 10.1186/s13089-014-0012-2
   Steigerwald S, 2014, Do fundamentals of laparoscopic surgery (FLS) and LapVR evaluation metrics predict intra-operative performance?
   Stoffregen TA, 2003, VIRTUAL AND ADAPTIVE ENVIRONMENTS: APPLICATIONS, IMPLICATIONS, AND HUMAN PERFORMANCE ISSUES, P111, DOI 10.1201/9781410608888.ch6
   Swapp David, 2006, Virtual Reality, V10, P24
   Udani AD, 2015, LOCAL REG ANESTH, V8, P33, DOI 10.2147/LRA.S68223
   Ullrich S, 2009, STUD HEALTH TECHNOL, V142, P392, DOI 10.3233/978-1-58603-964-6-392
   Varoquier M, 2017, Int J Otolaryngol, V2017, P2707690, DOI 10.1155/2017/2707690
   Vidal FP, 2008, COMPUT ANIMAT VIRT W, V19, P111, DOI 10.1002/cav.217
   Vidal FP, 2009, STUD HEALTH TECHNOL, V142, P398, DOI 10.3233/978-1-58603-964-6-398
   Waller D, 1998, PRESENCE-TELEOP VIRT, V7, P129, DOI 10.1162/105474698565631
   Yiannakopoulou E, 2015, INT J SURG, V13, P60, DOI 10.1016/j.ijsu.2014.11.014
NR 72
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103878
DI 10.1016/j.cag.2024.01.005
EA FEB 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LK5S4
UT WOS:001186713300001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Wagner, N
   Schwanecke, U
   Botsch, M
AF Wagner, Nicolas
   Schwanecke, Ulrich
   Botsch, Mario
TI AnaConDaR: Anatomically-Constrained Data-Adaptive Facial Retargeting
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Facial animation; Offline performance retargeting; Physics-based
   simulation
AB Offline facial retargeting, i.e., transferring facial expressions from a source to a target character, is a common production task that still regularly leads to considerable algorithmic challenges. This task can be roughly dissected into the transfer of sequential facial animations and non-sequential blendshape personalization. Both problems are typically solved by data-driven methods that require an extensive corpus of costly target examples. Other than that, geometrically motivated approaches do not require intensive data collection but cannot account for character-specific deformations and are known to cause manifold visual artifacts. We present AnaConDaR, a novel method for offline facial retargeting, as a hybrid of data-driven and geometry-driven methods that incorporates anatomical constraints through a physics-based simulation. As a result, our approach combines the advantages of both paradigms while balancing out the respective disadvantages. In contrast to other recent concepts, AnaConDaR achieves substantially individualized results even when only a handful of target examples are available. At the same time, we do not make the common assumption that for each target example a matching source expression must be known. Instead, AnaConDaR establishes correspondences between the source and the target character by a data-driven embedding of the target examples in the source domain. We evaluate our offline facial retargeting algorithm visually, quantitatively, and in two user studies.
C1 [Wagner, Nicolas; Botsch, Mario] TU Dortmund Univ, Otto Hahn Str 16, D-44227 Dortmund, Germany.
   [Schwanecke, Ulrich] Univ Appl Sci RheinMain, Kurt Schumacher Ring 18, D-65197 Wiesbaden, Germany.
C3 Dortmund University of Technology
RP Wagner, N (corresponding author), TU Dortmund Univ, Otto Hahn Str 16, D-44227 Dortmund, Germany.
EM nicolas.wagner@tu-dortmund.de
FU German Federal Ministry of Education and Research (BMBF) [16SV8785]
FX We want to thank the German Federal Ministry of Education and Research
   (BMBF) that supported this research through the project HiAvA (ID
   16SV8785) . We also want to express our appreciation to the reviewers
   who have improved this work with their helpful comments.
CR Achenbach J, 2015, Accurate face reconstruction through anisotropic fitting and eye correction, P1
   Achenbach Jascha., 2018, P EUR WORKSH VIS COM, P67, DOI DOI 10.2312/VCBM.20181230
   Beeler T, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601182
   Bhat R., 2013, P 12 ACM SIGGRAPH EU, P7, DOI 10.1145/2485895.24859153J.P.
   Ribera RBI, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073674
   Botsch M, 2005, COMPUT GRAPH FORUM, V24, P611, DOI 10.1111/j.1467-8659.2005.00886.x
   Botsch Mario., 2006, VISION MODELING VISU, P357
   Bouaziz S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601116
   Bouaziz S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461976
   Cao C, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530143
   Chandran P, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530114
   Chandran P, 2020, INT CONF 3D VISION, P345, DOI 10.1109/3DV50981.2020.00044
   Chen LL, 2021, PROC CVPR IEEE, P13054, DOI 10.1109/CVPR46437.2021.01286
   Chen RW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2003, DOI 10.1145/3394171.3413630
   Choi B, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555398
   Deuss M., 2015, SHAPEOP A ROBUST EXT, P505
   Feng Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459936
   Garbin Stephan J, 2022, arXiv
   Garrido P, 2014, PROC CVPR IEEE, P4217, DOI 10.1109/CVPR.2014.537
   Hong Y, 2022, PROC CVPR IEEE, P20342, DOI 10.1109/CVPR52688.2022.01973
   Ichim AE, 2016, S COMP AN, P107
   Karypis G., 1997, METIS SOFTWARE PACK
   Kim H, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356500
   Kim PH, 2011, PG (short papers)
   Kim S, 2021, COMPUT GRAPH FORUM, V40, P45, DOI 10.1111/cgf.14400
   Komaritzan M, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3203203
   Lewis JP, 2014, Eurographics (State of the Art Reports), V1, P2, DOI [DOI 10.2312/EGST.20141042, 10.2312/egst.20141042]
   Li H, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778769
   Li JM, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417817
   Li TY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130813
   Lombardi S, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201401
   Moser L, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480515
   Nirkin Y, 2019, IEEE I CONF COMP VIS, P7183, DOI 10.1109/ICCV.2019.00728
   Onizuka H, 2019, IEEE INT CONF COMP V, P2100, DOI 10.1109/ICCVW.2019.00265
   Perov I, 2021, Arxiv, DOI [arXiv:2005.05535, DOI 10.48550/ARXIV.2005.05535]
   Qian SH, 2024, Arxiv, DOI arXiv:2312.02069
   Ren Y., 2021, ICCV, P13759
   Schmidt P, 2023, COMPUT GRAPH FORUM, V42, P103, DOI 10.1111/cgf.14747
   Sifakis D., 2023, An implicit physical face model driven by expression and style-supplemental
   Song J, 2011, COMPUT ANIMAT VIRT W, V22, P187, DOI 10.1002/cav.414
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Wagner N, 2023, 15TH ANNUAL ACM SIGGRAPH CONFERENCE ON MOTION, INTERACTION AND GAMES, MIG 2023, DOI 10.1145/3623264.3624439
   Wang QS, 2021, COMPUT GRAPH FORUM, V40, P382, DOI 10.1111/cgf.14385
   Wang YH, 2021, Arxiv, DOI arXiv:2106.09965
   Wu CL, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925882
   Xu F, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601210
   Yang HT, 2020, PROC CVPR IEEE, P598, DOI 10.1109/CVPR42600.2020.00068
   Yang LC, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530156
   Zhang JN, 2020, PROC CVPR IEEE, P5325, DOI 10.1109/CVPR42600.2020.00537
   Zhang JY, 2022, IEEE T VIS COMPUT GR, V28, P1274, DOI 10.1109/TVCG.2020.3013876
   Zielonka W, 2023, PROC CVPR IEEE, P4574, DOI 10.1109/CVPR52729.2023.00444
NR 51
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103988
DI 10.1016/j.cag.2024.103988
EA JUL 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XZ3S8
UT WOS:001265468800001
OA hybrid
DA 2024-08-05
ER

PT J
AU Vaitkus, M
   Salvi, P
   Várady, T
AF Vaitkus, Marton
   Salvi, Peter
   Varady, Tamas
TI Interior control structure for Generalized Bézier patches over curved
   domains
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Multi-sided surfaces; Curved domain; Medial axis transform; Template;
   Interior control
ID MESH GENERATION; SURFACES
AB Generalized B & eacute;zier patches with curved domains can represent complex, multi -sided surfaces, but do not provide explicit control over the interior of the surface, as they are defined by means of side -based ribbons. In this paper we extend this representation by proposing a uniform, intuitive control structure, based on templates - a collection of quadrilaterals that covers and affects the 3D shape. It is constructed based on a variant of the Medial Axis Transform (MAT) that uses the local parameterization of the domain. For a given patch a hierarchical sequence of 2D templates can be defined, each determining the topology of the corresponding 3D control structure. First we introduce templates, then present the way of associating biparametric Bernstein blend functions with the control points. Next we describe how to position the control points of the MAT skeleton and the remaining interior control points, while ribbons are preserved. Finally we show a few examples that demonstrate the method and discuss the pros and cons of the approach.
C1 [Vaitkus, Marton; Salvi, Peter; Varady, Tamas] Budapest Univ Technol & Econ, Budapest, Hungary.
C3 Budapest University of Technology & Economics
RP Vaitkus, M (corresponding author), Budapest Univ Technol & Econ, Budapest, Hungary.
EM vaitkus@iit.bme.hu
RI Salvi, Peter/H-1918-2012
OI Salvi, Peter/0000-0003-2456-2051
FU Hungarian Scientific Research Fund (OTKA) [145970]
FX This project has been supported by the Hungarian Scientific Research
   Fund (OTKA, No. 145970) . The authors would like to acknowledge the
   significant programming contribution of Gyoergy Kariko for extending our
   prototype surfacing system Sketches (ShapEx Ltd., Budapest) .
CR Ball AA, 2001, COMPUT AIDED GEOM D, V18, P135, DOI 10.1016/S0167-8396(01)00020-6
   Blender Online Community, 2024, Blender 4.0 reference manual
   Buchegger F, 2017, COMPUT AIDED DESIGN, V82, P2, DOI 10.1016/j.cad.2016.05.019
   Campen M, 2017, COMPUT GRAPH FORUM, V36, P567, DOI 10.1111/cgf.13153
   Fogg HJ, 2016, COMPUT AIDED DESIGN, V72, P87, DOI 10.1016/j.cad.2015.07.001
   Goldman R, 2004, COMPUT AIDED GEOM D, V21, P243, DOI 10.1016/j.cagd.2003.10.003
   Gregory J., 1974, COMPUT AIDED GEOM D, P71
   Hettinga GJ, 2018, COMPUT AIDED GEOM D, V62, P166, DOI 10.1016/j.cagd.2018.03.005
   Höllig K, 2001, SIAM J NUMER ANAL, V39, P442, DOI 10.1137/S0036142900373208
   Huerta A, 2018, Encyclopedia of computational mechanics, P1, DOI [DOI 10.1002/9781119176817.ECM2005, 10.1002/9781119176817.ecm2005]
   Jacobson A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964973
   Karoiauskas K., 2003, Topics in algebraic geometry and geometric modeling. Contemporary mathematics, P101, DOI DOI 10.1090/CONM/334
   Krasauskas R, 2002, ADV COMPUT MATH, V17, P89, DOI 10.1023/A:1015289823859
   LOOP CT, 1989, ACM T GRAPHIC, V8, P204, DOI 10.1145/77055.77059
   Lyon M, 2021, COMPUT GRAPH FORUM, V40, P305, DOI 10.1111/cgf.142634
   Martin F., 2022, Springer INdAM series, P123, DOI DOI 10.1007/978-3-030-92313-66
   Qin KK, 2023, COMPUT AIDED GEOM D, V105, DOI 10.1016/j.cagd.2023.102222
   Quadros WR, 2004, INT J NUMER METH ENG, V61, P209, DOI 10.1002/nme.1063
   Rigby D., 2003, Fluids engineering division summer meeting, V36967, P1991, DOI [10.1115/FEDSM2003-45527, DOI 10.1115/FEDSM2003-45527]
   Sabin MA, 1983, EUROGRAPHICS C P, DOI [10.2312/eg.19831004, DOI 10.2312/EG.19831004]
   Salvi P., 2024, Comput-Aided Des Appl, V21, P143, DOI [/10.14733/cadaps.2024.143-154, DOI 10.14733/CADAPS.2024.143-154]
   Salvi P., 2016, P 8 HUNG C COMP GRAP, P61
   Salvi P., 2021, P WORKSH ADV INF TEC, P35
   Salvi P, 2023, COMPUT GRAPH-UK, V114, P86, DOI 10.1016/j.cag.2023.05.020
   Salvi P, 2018, COMPUT GRAPH-UK, V74, P56, DOI 10.1016/j.cag.2018.05.006
   Shen JJ, 2014, COMPUT AIDED GEOM D, V31, P486, DOI 10.1016/j.cagd.2014.06.004
   Siddiqi K, 2008, COMPUT IMAGING VIS, V37, P1, DOI 10.1007/978-1-4020-8658-8
   TAM TKH, 1991, ADV ENG SOFTW WORKST, V13, P313, DOI 10.1016/0961-3552(91)90035-3
   Tarini M, 2022, COMPUT GRAPH-UK, V107, P60, DOI 10.1016/j.cag.2022.06.015
   Vaitkus M, 2021, COMPUT AIDED GEOM D, V89, DOI 10.1016/j.cagd.2021.102019
   Várady T, 2024, COMPUT AIDED GEOM D, V110, DOI 10.1016/j.cagd.2024.102286
   Várady T, 2020, COMPUT AIDED GEOM D, V78, DOI 10.1016/j.cagd.2020.101828
   Várady T, 2017, COMPUT AIDED GEOM D, V55, P69, DOI 10.1016/j.cagd.2017.05.002
   Várady T, 2016, COMPUT GRAPH FORUM, V35, P307, DOI 10.1111/cgf.12833
   Várady T, 2012, GRAPH MODELS, V74, P311, DOI 10.1016/j.gmod.2012.03.003
   Worchel M, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3618387
   Zhang C, 2021, COMPUT AIDED DESIGN, V140, DOI 10.1016/j.cad.2021.103084
   Zheng JJ, 1997, COMPUT AIDED GEOM D, V14, P807, DOI 10.1016/S0167-8396(97)00007-1
   Zorin D., 2006, ACM SIGGRAPH 2006 CO, P30, DOI DOI 10.1145/1185657.1185673
NR 39
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD JUN
PY 2024
VL 121
AR 103952
DI 10.1016/j.cag.2024.103952
EA JUN 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WC8T1
UT WOS:001252769000001
OA hybrid
DA 2024-08-05
ER

PT J
AU Yalçiner, B
   Akyüz, AO
AF Yalciner, Bora
   Akyuz, Ahmet Oguz
TI Path guiding for wavefront path tracing: A memory efficient approach for
   GPU path tracers
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Graphics processors; Monte Carlo rendering; Path tracing; Path guiding
AB We propose a path-guiding algorithm to be incorporated into the wavefront style of path tracers (WFPTs). As WFPTs are primarily implemented on graphics processing units (GPUs), the proposed method aims to leverage the capabilities of the GPUs and reduce the hierarchical data structure and memory usage typically required for such techniques. To achieve this, our algorithm only stores the radiant exitance on a single global sparse voxel octree (SVO) data structure. Probability density functions required to guide the rays are generated on-the-fly using this data structure. The proposed approach reduces the scene-related persistent memory requirements compared to other path-guiding techniques while producing similar or better results depending on scene characteristics. To our knowledge, our algorithm is the first one that incorporates path guiding into a WFPT.
C1 [Yalciner, Bora; Akyuz, Ahmet Oguz] Middle East Tech Univ, Comp Engn Dept, Ankara, Turkiye.
C3 Middle East Technical University
RP Yalçiner, B (corresponding author), Middle East Tech Univ, Comp Engn Dept, Ankara, Turkiye.
EM yalciner.bora@metu.edu.tr; akyuz@ceng.metu.edu.tr
CR Amanatides J., 1984, Computers & Graphics, V18, P129
   Andersson P, 2021, Eurographics 2021-short papers
   Bako S, 2019, COMPUT GRAPH FORUM, V38, P527, DOI 10.1111/cgf.13858
   Bitterli B., 2016, RENDERING RESOURCES
   Clarberg Petrik, 2008, Journal of Graphics Tools, V13, P53
   Crassin C., 2009, P 2009 S INT 3D GRAP, P15, DOI DOI 10.1145/1507149.1507152
   Crassin C, 2012, Octree-based sparse voxelization using the GPU hardware rasterizer
   Crassin Cyril., 2011, S INT 3D GRAPH GAM I, P207
   Dahm K, 2017, ACM SIGGRAPH 2017 TALKS, DOI 10.1145/3084363.3085032
   Derevyannykh M, 2022, Eurographics 2022-short papers
   Diolatzis S, 2020, COMPUT GRAPH FORUM, V39, P23, DOI 10.1111/cgf.14051
   Dittebrandt Addis, 2020, EUROGRAPHICS S RENDE
   Dodik A, 2022, COMPUT GRAPH FORUM, V41, P172, DOI 10.1111/cgf.14428
   Estevez AC, 2018, SIGGRAPH'18: ACM SIGGRAPH 2018 TALKS, DOI 10.1145/3214745.3214760
   Heitz E, 2014, COMPUT GRAPH FORUM, V33, P103, DOI 10.1111/cgf.12417
   Herholz S, 2016, COMPUT GRAPH FORUM, V35, P67, DOI 10.1111/cgf.12950
   Huo YC, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3368313
   Jakob W, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530099
   Jensen HW, 1995, SPRING COMP SCI, P326, DOI 10.1007/978-3-7091-9430-0_31
   Kajiya J. T., 1986, Computer Graphics, V20, P143, DOI 10.1145/15886.15902
   Karras Tero, 2012, P 4 ACM SIGGRAPH EUR, P33, DOI [10.2312/EGGH/HPG12/033-037, DOI 10.2312/EGGH/HPG12/033-037]
   Kim J, 2021, Pacific graphics short papers, posters, and work-in-progress papers
   Lafortune EP, 1995, SPRING COMP SCI, P11
   Laine S, 2010, NVIDIA technical report NVR-2010-001
   Laine Samuli, 2013, Proceedings of the 5th High-Performance Graphics Conference, P137, DOI [10.1145/2492045.2492060, DOI 10.1145/2492045.2492060]
   Müller T, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3341156
   Müller T, 2017, COMPUT GRAPH FORUM, V36, P91, DOI 10.1111/cgf.13227
   NVIDIA, 2023, NVIDIA RTX Path Tracing
   Parker SG, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778803
   Pharr M., 2023, Physically Based Rendering: From Theory to Implementation, Vfourth
   Pharr M., 2016, Physically based rendering: From theory to implementation
   Reinhard E, 2023, Seminal graphics papers: pushing the boundaries, V2, P661
   Ruppert L, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392421
   Schüssler V, 2022, COMPUT GRAPH FORUM, V41, P1, DOI 10.1111/cgf.14582
   Shirley P., 2019, Sampling Transformations Zoo, P223, DOI [10.1007/978-1-4842-4427-216, DOI 10.1007/978-1-4842-4427-216]
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   Veach E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P419, DOI 10.1145/218380.218498
   Vévoda P, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201340
   Vorba J, 2014, ACM T GRAPHIC, V33, DOI [10.1145/2601097.2601203, 10.1145/2801097.2801203]
   Yalciner B, 2024, METU ray-GPU-based renderer/framework
   Zheng SK, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555463
   Zhu SL, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3476828
NR 42
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD JUN
PY 2024
VL 121
AR 103945
DI 10.1016/j.cag.2024.103945
EA MAY 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UC1X2
UT WOS:001245782100001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Absetan, A
   Fathi, A
AF Absetan, Ahmad
   Fathi, Abdolhossein
TI Quality assessment of retargeted images using deep learning capabilities
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Image quality assessment; Image retargeting; Deep learning; Image
   importance map
ID COLOR
AB To display images on panels and screens of different dimensions, there is a need to create algorithms that help adjust them to desired sizes through image retargeting (IR). In this sense, choosing the right algorithms seems to be a challenge. From this perspective, this paper was to propose a method for the assessment of retargeted images using deep learning (DL) models. To this end, the Conditional Random Fields as Recurrent Neural Networks (CRF-RNN) approach was applied to find image pixels in terms of importance and, the YouOnly -Look -Once (YOLO) model was employed to identify semantic objects of images. As well, image patch similarity was computed based on Siamese Neural Network (SNN), and output image distortion was recognized by the CNN model. With reference to three well-known databases, viz., MIT RetargetMe, NRID and CUHK, the assessment results of the proposed algorithm demonstrated its superior usage as compared to the existing ones.
C1 [Absetan, Ahmad] Razi Univ, Dept Comp Engn & Informat Technol, Kermanshah, Iran.
   [Fathi, Abdolhossein] Razi Univ, Dept Comp Engn & Informat Technol, Kermanshah, Iran.
C3 Razi University; Razi University
RP Fathi, A (corresponding author), Razi Univ, Dept Comp Engn & Informat Technol, Kermanshah, Iran.
EM ahmadabsetan@gmail.com; a.fathi@razi.ac.ir
CR Absetan A, 2023, CYBERNET SYST, V54, P673, DOI 10.1080/01969722.2022.2071408
   Avidan S., 2007, Seam carving for content-aware image resizing, V26, DOI [10.1145/1276377.1276390, DOI 10.1145/1276377.1276390]
   Boltz S, 2010, IEEE IMAGE PROC, P4597, DOI 10.1109/ICIP.2010.5651708
   Chen ZB, 2017, IEEE T IMAGE PROCESS, V26, P5138, DOI 10.1109/TIP.2017.2736422
   Dong WM, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618471
   Fang YM, 2014, IEEE J EM SEL TOP C, V4, P95, DOI 10.1109/JETCAS.2014.2298919
   Fu ZQ, 2018, IEEE ACCESS, V6, P12008, DOI 10.1109/ACCESS.2018.2808322
   Hsu CC, 2014, IEEE J-STSP, V8, P377, DOI 10.1109/JSTSP.2014.2311884
   Jiang B, 2020, IEEE T CYBERNETICS, V50, P87, DOI 10.1109/TCYB.2018.2864158
   Jiang QP, 2018, IEEE T CYBERNETICS, V48, P1276, DOI 10.1109/TCYB.2017.2690452
   Jocher G., 2023, Ultralytics YOLO
   Karimi M, 2017, J VIS COMMUN IMAGE R, V43, P108, DOI 10.1016/j.jvcir.2016.12.011
   Karni Z, 2009, COMPUT GRAPH FORUM, V28, P1257, DOI 10.1111/j.1467-8659.2009.01503.x
   Kasutani E, 2001, IEEE IMAGE PROC, P674, DOI 10.1109/ICIP.2001.959135
   Kendall MG, 1938, BIOMETRIKA, V30, P81, DOI 10.2307/2332226
   Krähenbühl P, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1618452.1618472, 10.1145/1616452.1618472]
   Li LD, 2021, IEEE T MULTIMEDIA, V23, P2757, DOI 10.1109/TMM.2020.3016124
   Liang Y, 2017, IEEE T VIS COMPUT GR, V23, P1099, DOI 10.1109/TVCG.2016.2517641
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu AM, 2015, SIGNAL PROCESS-IMAGE, V39, P444, DOI 10.1016/j.image.2015.08.001
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Ma L, 2012, IEEE J-STSP, V6, P626, DOI 10.1109/JSTSP.2012.2211996
   Manjunath BS, 2001, IEEE T CIRC SYST VID, V11, P703, DOI 10.1109/76.927424
   Oliveira SAF, 2018, COMPUT VIS IMAGE UND, V168, P172, DOI 10.1016/j.cviu.2017.11.011
   Peng ZY, 2022, IEEE T CIRC SYST VID, V32, P3422, DOI 10.1109/TCSVT.2021.3112933
   Pritch Y, 2009, IEEE I CONF COMP VIS, P151, DOI 10.1109/ICCV.2009.5459159
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Rubinstein M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531329
   Shao F, 2021, IEEE T SYST MAN CY-S, V51, P3053, DOI 10.1109/TSMC.2019.2917496
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Simakov Denis., 2008, Summarizing visual data using bidirectional similarity. pages, P1, DOI DOI 10.1109/CVPR.2008.4587842
   Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wolf L, 2007, IEEE I CONF COMP VIS, P1418
   Zhang YB, 2018, IEEE T IMAGE PROCESS, V27, P451, DOI 10.1109/TIP.2017.2761556
   Zhang YB, 2016, IEEE T IMAGE PROCESS, V25, P4286, DOI 10.1109/TIP.2016.2585884
   Zhang YC, 2017, IEEE T IMAGE PROCESS, V26, P5980, DOI 10.1109/TIP.2017.2746260
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
NR 38
TC 1
Z9 1
U1 4
U2 4
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD MAY
PY 2024
VL 120
AR 103914
DI 10.1016/j.cag.2024.103914
EA APR 2024
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RX1S5
UT WOS:001230872200001
DA 2024-08-05
ER

PT J
AU Xing, JL
   Liu, JP
   Wang, J
   Sun, LL
   Chen, X
   Gu, XX
   Wang, YF
AF Xing, Jialu
   Liu, Jianping
   Wang, Jian
   Sun, Lulu
   Chen, Xi
   Gu, Xunxun
   Wang, Yingfei
TI A survey of efficient fine-tuning methods for Vision-Language Models -
   Prompt and Adapter
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Vision-language; Computer vision; Efficient fine-tuning; Pre-training
   model; Prompt; Adapter
AB Vision Language Model (VLM) is a popular research field located at the fusion of computer vision and natural language processing (NLP). With the emergence of transformer networks and mass web data, numerous large scale VLMs or Vision -Language Pre-training Models (VLPM) have been achieving state-of-the-art results in many tasks, such as retrieval (CLIP) and generation (DALL-E). Although large models have shown impressive results, the cost of retraining and full fine-tuning is prohibitive for general researchers. In recent years, Efficient fine-tuning (EFT) which a very low-cost tuning method has been a good solution to this problem has greatly alleviated this problem, and driven by this, a new fine-tuning paradigm has developed. Since Prompt and Adapter are most widely used in the field of visual language, this review focuses on analysing the progress of the application of these two methods. Firstly, we reviewed the VLM research paradigm based on the differences in pre-training-fine-tuning methods; Next, We categorized the Prompt into 3 types (7 subtypes) of usage patterns based on the different modal information, and categorized the Adapter into 2 types of usage patterns based on whether it plays a role in modal fusion, furthermore we discussed them in vision and vision-language tasks. Finally, we discussed the stability and social ethics of EFT, and possible future research directions were proposed.
C1 [Xing, Jialu; Liu, Jianping; Sun, Lulu; Chen, Xi; Gu, Xunxun; Wang, Yingfei] North Minzu Univ, Coll Comp Sci & Engn, Yinchuan 750021, Peoples R China.
   [Liu, Jianping] North Minzu Univ, Key Lab Images & Grap Intelligent Proc, State Ethn Affairs Commiss, Yinchuan 750021, Peoples R China.
   [Wang, Jian] Chinese Acad Agr Sci, Agr Informat Inst, Beijing 100081, Peoples R China.
   [Liu, Jianping] 204,Wenchang North St, Yinchuan, Ningxia, Peoples R China.
C3 North Minzu University; North Minzu University; Chinese Academy of
   Agricultural Sciences; Agriculture Information Institute, CAAS
RP Liu, JP (corresponding author), 204,Wenchang North St, Yinchuan, Ningxia, Peoples R China.
EM 20227440@stu.nmu.edu.cn; liujianping01@nmu.edu.cn; wangjian0l@caas.cn;
   20227515@stu.nmu.edu.cn; 20237468@stu.nmu.edu.cn;
   20227489@stu.nmu.edu.cn; 20217426@stu.nmu.edu.cn
FU Natural Science Foundation Project of Ningxia Province, China
   [2024A0272]; Research and Development Program for Talent Introduction of
   Ningxia Province, China [2022BSB03 044]; North Minzu Univer-sity
   [2020KYQD37]
FX This work is supported by the Natural Science Foundation Project of
   Ningxia Province, China, 2024A0272; Research and Development Program for
   Talent Introduction of Ningxia Province, China, 2022BSB03 044; Starting
   Project of Scientific Research in the North Minzu Univer-sity,
   2020KYQD37.
CR Alayrac J.-B., 2022, NeurIPS, V35, P23716, DOI DOI 10.48550/ARXIV.2204.14198
   Anil GTGR, 2023, Arxiv, DOI arXiv:2312.11805
   [Anonymous], 2014, T ASSOC COMPUT LING, DOI DOI 10.1162/TACL_A_00166
   [Anonymous], 2004, Workshop on Generative Model Based Vision
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Bao H., 2021, arXiv preprint arXiv:2106.08254, DOI DOI 10.48550/ARXIV.2106.08254
   Bao H., 2022, Advances in Neural Information Pro- cessing Systems, V35, P32897
   Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29
   Brown T. B., 2020, P 34 INT C NEURAL IN, P1
   Chen XL, 2015, Arxiv, DOI arXiv:1504.00325
   Chowdhury S, 2023, P 2023 C EMPIRICAL M, P10173
   Chua TS, 2009, P ACM INT C IM VID R, P1
   Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Ding N, 2023, NAT MACH INTELL, V5, P220, DOI 10.1038/s42256-023-00626-4
   Driess D, 2023, P 40 INT C MACH LEAR, DOI [10.5555/3618408.3618748, DOI 10.5555/3618408.3618748]
   Du Yifan, 2022, P INT JOINT C ART IN, P5436
   Eichenberg C, 2022, FINDINGS ASS COMPUTA, P2416
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gao P, 2024, INT J COMPUT VISION, V132, P581, DOI 10.1007/s11263-023-01891-x
   Ge YY, 2019, PROC CVPR IEEE, P5332, DOI 10.1109/CVPR.2019.00548
   Goh Gabriel, 2021, Distill, V6, pe30, DOI DOI 10.23915/DISTILL.00030
   Gu Jiaxi, 2022, Advances in Neural Information Processing Systems
   Guo Z, 2023, EMNLP, P5356, DOI 10.18653/v1/2023.findings-emnlp.356
   Han WJ, 2021, Arxiv, DOI [arXiv:2108.02340, 10.18653/v1/2021.acl-short.108, DOI 10.18653/V1/2021.ACL-SHORT.108]
   He J, 2022, INT C LEARNING REPRE
   Helber P, 2019, IEEE J-STARS, V12, P2217, DOI 10.1109/JSTARS.2019.2918242
   Houlsby N, 2019, PR MACH LEARN RES, V97
   Hu EJ, 2021, Arxiv, DOI arXiv:2106.09685
   Hudson DA, 2019, PROC CVPR IEEE, P6693, DOI 10.1109/CVPR.2019.00686
   Escalante HJ, 2010, COMPUT VIS IMAGE UND, V114, P419, DOI 10.1016/j.cviu.2009.03.008
   Jia ML, 2022, LECT NOTES COMPUT SC, V13693, P709, DOI 10.1007/978-3-031-19827-4_41
   Jin W, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P2763
   Kahana J, 2022, arXiv, DOI 10.48550/arXiv.2212.00784
   Kamath A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1760, DOI 10.1109/ICCV48922.2021.00180
   Kaur P, 2021, COMPUT SCI REV, V39, DOI 10.1016/j.cosrev.2020.100336
   Khattak MU, 2023, PROC CVPR IEEE, P19113, DOI 10.1109/CVPR52729.2023.01832
   Koh JY, 2023, Grounding language models to images for multimodal inputs and outputs
   Krause Jonathan, 2013, COLLECTING LARGE SCA
   Krizhevsky A., 2012, Learning multiple layers of features from tiny images, DOI DOI 10.1145/3079856.3080246
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Li W, 2018, BRIT MACHINE VISION, DOI [10.48550/arXiv.1809.00716, DOI 10.48550/ARXIV.1809.00716]
   Li XLS, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P4582
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Y, 2022, IEEE T CYBERNETICS, V52, P4520, DOI 10.1109/TCYB.2020.3029423
   Long S., 2022, arXiv
   Lu XC, 2023, Arxiv, DOI [arXiv:2305.01239, 10.48550/arXiv.2305.01239]
   Lu YN, 2022, PROC CVPR IEEE, P5196, DOI 10.1109/CVPR52688.2022.00514
   Mahabadi RK, 2021, ADV NEUR IN, V34
   Mahabadi RK, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1 (ACL-IJCNLP 2021), P565
   Maji S, 2013, Arxiv, DOI arXiv:1306.5151
   Mancini M, 2021, PROC CVPR IEEE, P5218, DOI 10.1109/CVPR46437.2021.00518
   Mogadala Aditya, 2021, Journal of Artificial Intelligence Research, P1183
   Najdenkoska I, 2023, arXiv
   Nayak NV, 2023, 11 INT C LEARNING RE
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Pantazis O, 2022, 33 BRIT MACHINE VISI
   Parkhi OM, 2012, PROC CVPR IEEE, P3498, DOI 10.1109/CVPR.2012.6248092
   Patterson D, 2021, Arxiv, DOI [arXiv:2104.10350, DOI 10.48550/ARXIV.2104.10350]
   Peng F, 2024, IEEE T MULTIMEDIA, V26, P3469, DOI 10.1109/TMM.2023.3311646
   Pfeiffer J, 2022, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), P2497
   Pfeiffer J, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS, P46
   Radford A, 2021, PR MACH LEARN RES, V139
   Rao YM, 2022, PROC CVPR IEEE, P18061, DOI 10.1109/CVPR52688.2022.01755
   Ren SH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1085
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Rostamzadeh N, 2018, Arxiv, DOI [arXiv:1806.08317, 10.48550/ARXIV.1806.08317]
   Schuhmann C., 2022, ADV NEURAL INFORM PR, V35, P25278
   Schuhmann C, 2021, Arxiv, DOI arXiv:2111.02114
   Shen S, 2022, Arxiv, DOI [arXiv:2211.11720, 10.48550/arXiv.2211.11720]
   Shin T, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P4222
   Shu M., 2022, Advances in Neural Information Processing Systems, P14274
   Subramanian S, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P5198
   Sun GY, 2024, Arxiv, DOI [arXiv:2210.01708, 10.48550/arXiv.2210.01708]
   Sun TX, 2023, PROCEEDINGS OF THE 61ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2023): LONG PAPERS, VOL 1, P11156
   Sun TX, 2022, Arxiv, DOI arXiv:2201.03514
   Sung YL, 2022, Arxiv, DOI arXiv:2206.06522
   Sung YL, 2022, PROC CVPR IEEE, P5217, DOI 10.1109/CVPR52688.2022.00516
   Szegedy C, 2014, Arxiv, DOI arXiv:1312.6199
   Tang M, 2023, Findings of the association for computational linguistics: ACL 2023, P6368, DOI [10.18653/v1/2023.findings-acl.397, DOI 10.18653/V1/2023.FINDINGS-ACL.397]
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Tian YJ, 2024, Arxiv, DOI [arXiv:2305.06221, 10.48550/arXiv.2305.06221, DOI 10.48550/ARXIV.2305.06221]
   Touvron H, 2023, Arxiv, DOI [arXiv:2302.13971, DOI 10.48550/ARXIV.2302.13971]
   Tsimpoukelli M, 2021, ADV NEUR IN, V34
   Udandarao V, 2023, IEEE I CONF COMP VIS, P2725, DOI 10.1109/ICCV51070.2023.00257
   Vaswani A, 2023, Arxiv, DOI arXiv:1706.03762
   Wang C, 2023, arXiv
   Wang T, 2023, Findings of the association for computational linguistics: ACL 2023, P13899, DOI DOI 10.18653/V1/2023.FINDINGS-ACL.873
   Wang WH, 2023, PROC CVPR IEEE, P19175, DOI 10.1109/CVPR52729.2023.01838
   Wang ZC, 2022, AAAI CONF ARTIF INTE, P5914
   Wu H, 2021, PROC CVPR IEEE, P11302, DOI 10.1109/CVPR46437.2021.01115
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Xing YH, 2024, IEEE T MULTIMEDIA, V26, P2056, DOI 10.1109/TMM.2023.3291588
   Yang ZY, 2023, Arxiv, DOI [arXiv:2309.17421, 10.48550/arXiv.2309.17421, DOI 10.48550/ARXIV.2309.17421]
   Yao Y, 2022, Arxiv, DOI arXiv:2109.11797
   Yao Yuan, 2022, P 2022 C EMPIRICAL M, P11104
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
   Zang Y, 2022, arXiv
   Zhang J, 2024, Arxiv, DOI [arXiv:2304.00685, 10.48550/arXiv.2304.00685, DOI 10.48550/ARXIV.2304.00685]
   Zhang RR, 2022, LECT NOTES COMPUT SC, V13695, P493, DOI 10.1007/978-3-031-19833-5_29
   Zhang RR, 2023, Arxiv, DOI [arXiv:2303.16199, DOI 10.48550/ARXIV.2303.16199]
   Zhang Z, 2023, Findings of the association for computational linguistics: EMNLP 2023, P7258, DOI [10.18653/v1/2023.findings-emnlp.483, DOI 10.18653/V1/2023.FINDINGS-EMNLP.483]
   Zhang ZR, 2023, 61ST CONFERENCE OF THE THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL 2023, VOL 2, P1239
   Zhao WX, 2023, Arxiv, DOI arXiv:2303.18223
   Zhong Z, 2021, 2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL-HLT 2021), P5017
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
   Zhou KY, 2022, PROC CVPR IEEE, P16795, DOI 10.1109/CVPR52688.2022.01631
   Zhou KY, 2022, INT J COMPUT VISION, V130, P2337, DOI 10.1007/s11263-022-01653-1
   Zhu BE, 2024, Arxiv, DOI [arXiv:2205.14865, 10.48550/arXiv.2205.14865]
   Zong YS, 2023, Arxiv, DOI [arXiv:2304.01008, 10.48550/arXiv.2304.01008]
NR 113
TC 0
Z9 0
U1 12
U2 12
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103885
DI 10.1016/j.cag.2024.01.012
EA FEB 2024
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LK5I5
UT WOS:001186703400001
DA 2024-08-05
ER

PT J
AU Yoo, S
   Ramalhinho, J
   Dowrick, T
   Somasundaram, M
   Gurusamy, K
   Davidson, B
   Clarkson, MJ
   Blandford, A
AF Yoo, Soojeong
   Ramalhinho, Joao
   Dowrick, Thomas
   Somasundaram, Murali
   Gurusamy, Kurinchi
   Davidson, Brian
   Clarkson, Matthew J.
   Blandford, Ann
TI Can engineers represent surgeons in usability studies? Comparison of
   results from evaluating augmented reality guidance for laparoscopic
   surgery
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Laparoscopic surgery; Augmented reality; Operating theatre; Usability
   study
AB Obtaining feedback from time-constrained end-users is a major challenge in evaluating novel systems for specialised applications. The performance and feedback of engineers and surgeons was evaluated through an experiment where participants were asked to identify tumour locations within an anatomically realistic silicon liver model across three different conditions of an Augmented Reality (AR) prototype system (Baseline, Split AR and Full AR). Our findings show that engineers and surgeons share some similarities in their performance, feedback and behaviour, particularly when reliance on the AR system is high for both groups. However, engineers typically focus more on accuracy of the image alignment and are more accurate in their responses when supported by AR. Senior surgeons typically perform faster and use AR as supplementary information, while the performance of junior surgeons is more closely aligned to the performance of engineers. We conclude that engineers could be involved in preliminary evaluations of a surgical system or in evaluations of systems which are aimed at training junior surgeons, but that it is essential to involve surgeons in later evaluations, where ecological validity is a more important consideration.
C1 [Yoo, Soojeong; Ramalhinho, Joao; Dowrick, Thomas; Clarkson, Matthew J.; Blandford, Ann] Univ Coll London UCL, Wellcome ESPRC Ctr Intervent Surg Sci WEISS, London, England.
   [Yoo, Soojeong; Blandford, Ann] UCL, Univ Coll London Interact Ctr UCLIC, London, England.
   [Somasundaram, Murali; Gurusamy, Kurinchi; Davidson, Brian] UCL, Div Surg & Intervent Sci, London, England.
C3 University of London; University College London; University of London;
   University College London; University of London; University College
   London
RP Yoo, S (corresponding author), UCL, Univ Coll London Interact Ctr UCLIC, London, England.
EM soojeong.yoo@ucl.ac.uk; joao.ramalhinho.15@ucl.ac.uk;
   t.dowrick@ucl.ac.uk; murali.somasundaram1@nhs.net; k.gurusamy@ucl.ac.uk;
   b.davidson@ucl.ac.uk; m.clarkson@ucl.ac.uk; a.blandford@ucl.ac.uk
RI Gurusamy, Kurinchi/C-2082-2008; Clarkson, Matthew J./C-1512-2008
OI Gurusamy, Kurinchi/0000-0002-0313-9134; Yoo,
   Soojeong/0000-0003-3681-6784; Clarkson, Matthew/0000-0002-5565-1252;
   Ramalhinho, Joao/0000-0002-8438-2215
FU National Institute for Health and Care Research (NIHR) under its
   Invention for Innovation (i4i) Programme [II-LA-1116-20005]; EPSRC,
   United Kingdom [EP/T029404/1]; Wellcome/EPSRC [203145Z/16/Z]; National
   Institutes of Health Research (NIHR) [II-LA-1116-20005] Funding Source:
   National Institutes of Health Research (NIHR)
FX We would like to acknowledge surgical and theatre nursing teams at the
   Royal Free Hospital (London, UK) for their support and accessibility,
   enabling the execution of this study in real operating theatres. This
   research was funded in whole, or in part, by the National Institute for
   Health and Care Research (NIHR) under its Invention for Innovation (i4i)
   Programme (Grant Reference Number [II-LA-1116-20005] ) , the EPSRC,
   United Kingdom [EP/T029404/1] and Wellcome/EPSRC [203145Z/16/Z] . The
   views expressed are those of the author (s) and not necessarily those of
   the NIHR or the Department of Health and Social Care. For the purpose of
   Open Access, the author has applied a CC BY public copyright licence to
   any Author Accepted Manuscript version arising from this submission.
CR Bednarik R, 2022, CHI C HUMAN FACTORS, P1
   Bernhardt S, 2017, MED IMAGE ANAL, V37, P66, DOI 10.1016/j.media.2017.01.007
   Birlo M, 2022, MED IMAGE ANAL, V77, DOI 10.1016/j.media.2022.102361
   Blandford A, 2018, DIGIT HEALTH, V4, DOI 10.1177/2055207618770325
   Braun V., 2006, QUAL RES PSYCHOL, V3, P77, DOI 10.1191/1478088706qp063oa
   Brooke J., 1996, SUS-a quick and dirty usability scale, DOI [DOI 10.1201/9781498710411-35, DOI 10.1201/9781498710411]
   Ciria R, 2016, ANN SURG, V263, P761, DOI 10.1097/SLA.0000000000001413
   Cresswell J.W., 2015, A Concise Introduction to Mixed Methods Research
   Demirel D, 2022, INT J COMPUT ASS RAD, V17, P1823, DOI 10.1007/s11548-022-02683-3
   Dowrick T, 2023, MED PHYS, V50, P2695, DOI 10.1002/mp.16310
   Feng YY, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300841
   Franz AM, 2014, IEEE T MED IMAGING, V33, P1702, DOI 10.1109/TMI.2014.2321777
   Fuchs H, 1998, LECT NOTES COMPUT SC, V1496, P934, DOI 10.1007/BFb0056282
   Gasques D, 2021, Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, DOI DOI 10.1145/3411764.3445576
   HART S G, 1988, P139
   Hsu KE, 2008, SURG ENDOSC, V22, P196, DOI 10.1007/s00464-007-9452-0
   Kang X, 2014, SURG ENDOSC, V28, P2227, DOI 10.1007/s00464-014-3433-x
   Khan RSA, 2012, SURG ENDOSC, V26, P3536, DOI 10.1007/s00464-012-2400-7
   Koo B, 2022, INT J COMPUT ASS RAD, V17, P167, DOI 10.1007/s11548-021-02518-7
   Kujala S., 2004, P 3 NORDIC C HUMAN C, P297, DOI DOI 10.1145/1028014.1028060
   Kumcu A, 2017, INT J MED ROBOT COMP, V13, DOI 10.1002/rcs.1758
   Labrunie M, 2022, INT J COMPUT ASS RAD, V17, P1429, DOI 10.1007/s11548-022-02641-z
   Lau LW, 2019, J LAPAROENDOSC ADV S, V29, P88, DOI 10.1089/lap.2018.0183
   Law B., 2004, P 2004 S EYE TRACKIN, P41, DOI DOI 10.1145/968363.968370
   Lewis JR, 2018, J USABILITY STUD, V13, P158
   Lim AK, 2022, SURG ENDOSC, V36, P988, DOI 10.1007/s00464-021-08363-8
   Maria S, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2023, DOI 10.1145/3544548.3580714
   Mentis Helena M., 2017, Proceedings of the ACM on Human-Computer Interaction, V1, DOI 10.1145/3134713
   Mentis HM, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P428, DOI 10.1109/VRW55335.2022.00096
   Nomura S, 2008, CSCW: 2008 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK, CONFERENCE PROCEEDINGS, P427
   Park B, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-62361-9
   Potter S, 2014, TRIALS, V15, DOI 10.1186/1745-6215-15-80
   Ramalhinho J, 2023, MED IMAGE ANAL, V90, DOI 10.1016/j.media.2023.102943
   Reissis A, 2023, Medical imaging 2023: image-guided procedures, robotic interventions, and modeling, V12466, P364
   Schneider C, 2020, SURG ENDOSC, V34, P4702, DOI 10.1007/s00464-020-07807-x
   Schütz L, 2023, COMP M BIO BIO E-IV, V11, P1158, DOI 10.1080/21681163.2022.2154277
   Sears A, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2235
   Shuhaiber JH, 2004, ARCH SURG-CHICAGO, V139, P170, DOI 10.1001/archsurg.139.2.170
   Soojeong Yoo, 2022, 2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), P459, DOI 10.1109/VRW55335.2022.00101
   Thompson S, 2020, INT J COMPUT ASS RAD, V15, P1075, DOI 10.1007/s11548-020-02180-5
   Thompson S, 2018, INT J COMPUT ASS RAD, V13, P865, DOI 10.1007/s11548-018-1761-3
   van Berkel N, 2020, J BIOMED INFORM, V110, DOI 10.1016/j.jbi.2020.103553
   van Veelen MA, 2002, SURG ENDOSC, V16, P674, DOI 10.1007/s00464-001-9116-4
   Wakabayashi G, 2015, ANN SURG, V261, P619, DOI 10.1097/SLA.0000000000001184
   Walczak DA, 2015, VIDEOSURGERY MINIINV, V10, P87, DOI 10.5114/wiitm.2014.47434
   Zhang X, 2022, IEEE Trans Biomed Eng
NR 46
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103881
DI 10.1016/j.cag.2024.01.008
EA FEB 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LK6U9
UT WOS:001186742000001
OA Green Published, hybrid
DA 2024-08-05
ER

PT J
AU Lu, YK
   Wang, YH
   Song, P
   Wong, HS
   Mok, YJ
   Liu, LG
AF Lu, Yukun
   Wang, Yuhang
   Song, Peng
   Wong, Hang Siang
   Mok, Yingjuan
   Liu, Ligang
TI Computational design of custom-fit PAP masks
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Custom-fit PAP mask; Geometric modeling; Finite element analysis;
   Computational design; Digital fabrication
AB Positive airway pressure (PAP) therapy refers to sleep disordered breathing treatment that uses a stream of compressed air to support the airway during sleep. Even though the use of PAP therapy has been shown to be effective in improving the symptoms and quality of life, many patients are intolerant of the treatment due to poor mask fit. In this paper, our goal is to develop a computational approach for designing custom-fit PAP masks such that they can achieve better mask fit performance in terms of mask leakage and comfort. Our key observation is that a custom-fit PAP mask should fit a patient's face in its deformed state instead of in its rest state since the PAP mask cushion undergoes notable deformation before reaching an equilibrium state during PAP therapy. To this end, we compute the equilibrium state of a mask cushion using the finite element method, and quantitatively measure the leakage and comfort of the mask cushion in this state. We further optimize the mask cushion geometry to minimize the two measures while ensuring that the cushion can be easily fabricated with molding. We demonstrate the effectiveness of our computational approach on a variety of face models and different types of PAP masks. Experimental results on real subjects show that our designed custom-fit PAP masks are able to achieve better mask fit performance than a generic PAP mask and custom-fit PAP masks designed by a state-of-the-art approach.
C1 [Lu, Yukun; Liu, Ligang] Univ Sci & Technol China, Hefei, Peoples R China.
   [Wang, Yuhang] Nanjing Univ Informat Sci & Technol, Nanjing, Peoples R China.
   [Lu, Yukun; Wang, Yuhang; Song, Peng] Singapore Univ Technol & Design, Singapore, Singapore.
   [Wong, Hang Siang; Mok, Yingjuan] Changi Gen Hosp, Singapore, Singapore.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Nanjing University of Information Science & Technology;
   Singapore University of Technology & Design; Changi General Hospital
RP Song, P (corresponding author), Singapore Univ Technol & Design, Singapore, Singapore.
EM peng_song@sutd.edu.sg
FU CGH - SUTD HealthTech Innovation Fund [CGH-SUTD-HTIF-2021-001]; MOE
   Academic Research Fund Tier 2 Grant, Singapore [MOE-T2EP20222-0008];
   National Natural Science Foundation of China [62025207]
FX We thank the reviewers for their valuable comments, Pengyun Qiu for
   participating in the user study, Thileepan Stalin and Pablo Valdivia y
   Alvarado for providing advice and facilities on fabricat-ing mask
   interfaces with silicone. We would also like to thank the Changi General
   Hospital Office of Innovation and Clinical Trials Research Unit for
   their support in this project. This work was supported by the CGH - SUTD
   HealthTech Innovation Fund (CGH-SUTD-HTIF-2021-001) , the MOE Academic
   Research Fund Tier 2 Grant (MOE-T2EP20222-0008) , Singapore, and the
   National Natural Science Foundation of China (62025207) .
CR Alglib LTD, 2023, ALGLIB library
   Azernikov S, 2010, COMPUT AIDED DESIGN, V42, P87, DOI 10.1016/j.cad.2009.02.015
   Benjafield AV, 2019, LANCET RESP MED, V7, P687, DOI 10.1016/S2213-2600(19)30198-5
   Bickel B, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778800
   Chen X, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601189
   Cheng YL, 2015, J PROSTHET DENT, V113, P29, DOI 10.1016/j.prosdent.2014.01.030
   Chu CH, 2017, ADV ENG INFORM, V32, P202, DOI 10.1016/j.aei.2017.03.001
   Fu Q, 2017, IEEE T VIS COMPUT GR, V23, P2574, DOI 10.1109/TVCG.2017.2739159
   HACON D, 1989, EUR J COMBIN, V10, P435, DOI 10.1016/S0195-6698(89)80017-4
   Hu YX, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201353
   Kakkar RK, 2007, CHEST, V132, P1057, DOI 10.1378/chest.06-2432
   Kim VG, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601117
   Kwon YJ, 2022, PLOS ONE, V17, DOI 10.1371/journal.pone.0270092
   Lee W, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11125332
   Leimer K, 2020, COMPUT AIDED GEOM D, V79, DOI 10.1016/j.cagd.2020.101855
   Leimer K, 2018, COMPUT GRAPH FORUM, V37, P349, DOI 10.1111/cgf.13573
   Li S, 2020, P INT DES ENG TECHN, DOI [10.1115/DETC2020-22316,V11AT11A026:1-V11AT11A026:7, DOI 10.1115/DETC2020-22316,V11AT11A026:1-V11AT11A026:7]
   Liu KX, 2022, LECT NOTES COMPUT SC, V13323, P157, DOI 10.1007/978-3-031-05906-3_12
   Liu ZS, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459868
   Logan D.L., 2022, First Course in the Finite Element Method, Enhanced Edition, SI Version
   Ma LK, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130850
   Martelly E, 2021, J MED DEVICES, V15, DOI 10.1115/1.4049981
   Montes J, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392477
   Müller M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766907
   Pérez J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766998
   Sela M, 2016, Arxiv, DOI arXiv:1609.07049
   Sickel K, 2011, COMPUT AIDED DESIGN, V43, P1793, DOI 10.1016/j.cad.2011.06.005
   Skouras M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461979
   Smith M, ABAQUS/standard user's manual
   Vechev V, 2022, COMPUT GRAPH FORUM, V41, P535, DOI 10.1111/cgf.14492
   Wang CCL, 2010, COMPUT AIDED DESIGN, V42, P78, DOI 10.1016/j.cad.2009.02.018
   Wang HM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201320
   Wang WP, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330513
   Wolff K, 2023, COMPUT GRAPH FORUM, V42, P180, DOI 10.1111/cgf.14728
   Wu YY, 2018, J MED DEVICES, V12, DOI 10.1115/1.4040187
   Yang HT, 2020, PROC CVPR IEEE, P598, DOI 10.1109/CVPR42600.2020.00068
   Zehnder J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130881
   Zhang J, 2023, COMPUT AIDED DESIGN, V159, DOI 10.1016/j.cad.2023.103483
   Zhang J, 2022, COMPUT AIDED DESIGN, V150, DOI 10.1016/j.cad.2022.103271
   Zhang XT, 2016, COMPUT GRAPH FORUM, V35, P157, DOI 10.1111/cgf.12972
   Zhao DY, 2022, IEEE T VIS COMPUT GR, V28, P4032, DOI 10.1109/TVCG.2021.3112127
   Zheng YY, 2016, IEEE T VIS COMPUT GR, V22, P1732, DOI 10.1109/TVCG.2015.2448084
NR 42
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD AUG
PY 2024
VL 122
AR 103998
DI 10.1016/j.cag.2024.103998
EA JUL 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YV3M6
UT WOS:001271221200001
DA 2024-08-05
ER

PT J
AU de Figueiredo, LH
AF de Figueiredo, Luiz Henrique
TI A vertex-centric representation for adaptive diamond-kite meshes
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Mesh representation; Geometric modeling; Data structures
ID TILINGS
AB We describe a concise representation for adaptive diamond-kite meshes based solely on the vertices and their stars. The representation is exact because it uses only integers, is much smaller than standard topological data structures, and is highly compressible. All topological elements are reconstructed in expected constant time per element.
C1 [de Figueiredo, Luiz Henrique] IMPA, Rio De Janeiro, Brazil.
C3 Instituto Nacional de Matematica Pura e Aplicada (IMPA)
RP de Figueiredo, LH (corresponding author), IMPA, Rio De Janeiro, Brazil.
EM lhf@impa.br
FU FINEP; CNPq; FAPERJ
FX I thank David Eppstein for generously sharing his proof-of-concept
   Python implementation of adaptive diamond-kite meshes; Craig Kaplan for
   pointing me to the work of Robert Fathauer on kite fractals; Asla
   Medeiros e Sa for showing me Eppstein's paper and hearing my thoughts
   about it; and an anonymous referee for suggesting it might be possible
   to remove some internal vertices, which led me to reduced
   representations. This research was done in the Visgraf Computer Graphics
   laboratory at IMPA in Rio de Janeiro, Brazil. Visgraf is supported by
   the funding agencies FINEP, CNPq, and FAPERJ, and also by gifts from IBM
   Brasil, Microsoft, NVIDIA, and other companies.
CR Bommes D, 2013, COMPUT GRAPH FORUM, V32, P51, DOI 10.1111/cgf.12014
   de Figueiredo LH, 2024, A vertex-centric representation for adaptive diamond-kite meshes
   De Floriani L, 2007, EUROGRAPHICS 2007 ST, P63
   Diaz RG, 2010, WSCG 2010: FULL PAPERS PROCEEDINGS, P205
   Eppstein D, 2014, ENG COMPUT-GERMANY, V30, P223, DOI 10.1007/s00366-013-0327-9
   Fathauer RW, 2001, COMPUT GRAPH-UK, V25, P323, DOI 10.1016/S0097-8493(00)00134-5
   Fellegara R, 2021, COMPUT GRAPH-UK, V98, P322, DOI 10.1016/j.cag.2021.05.002
   Fuentes-Sepúlveda J, 2023, COMP GEOM-THEOR APPL, V109, DOI 10.1016/j.comgeo.2022.101922
   Kallmann M., 2001, J GRAPHICS TOOLS, V6, P7
   King D, 2000, arXiv
   Navarro G, 2016, COMPACT DATA STRUCTURES: A PRACTICAL APPROACH, P1, DOI 10.1017/CBO9781316588284
   Sánchez JES, 2021, COMPUT GRAPH-UK, V95, P69, DOI 10.1016/j.cag.2021.01.007
   Von Herzen B., 1987, SIGGRAPH 87, P103
NR 13
TC 0
Z9 0
U1 0
U2 0
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2024
VL 119
AR 103910
DI 10.1016/j.cag.2024.103910
EA MAR 2024
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QY2I5
UT WOS:001224360000001
DA 2024-08-05
ER

EF