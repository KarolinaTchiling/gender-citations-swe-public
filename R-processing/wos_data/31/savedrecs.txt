FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Deng, ZK
   Weng, D
   Liu, SH
   Tian, Y
   Xu, ML
   Wu, YC
AF Deng, Zikun
   Weng, Di
   Liu, Shuhan
   Tian, Yuan
   Xu, Mingliang
   Wu, Yingcai
TI A survey of urban visual analytics: Advances and future directions
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review
DE visual analytics; smart city; spatiotemporal data analysis; urban
   analytics
ID INTERACTIVE VISUALIZATION; MASS MOBILITY; CONTEXT-AWARE; OD PATTERNS;
   EXPLORATION; MOVEMENT; SYSTEM; DYNAMICS; DESIGN; ROUTE
AB Developing effective visual analytics systems demands care in characterization of domain problems and integration of visualization techniques and computational models. Urban visual analytics has already achieved remarkable success in tackling urban problems and providing fundamental services for smart cities. To promote further academic research and assist the development of industrial urban analytics systems, we comprehensively review urban visual analytics studies from four perspectives. In particular, we identify 8 urban domains and 22 types of popular visualization, analyze 7 types of computational method, and categorize existing systems into 4 types based on their integration of visualization techniques and computational models. We conclude with potential research directions and opportunities.
C1 [Deng, Zikun; Liu, Shuhan; Tian, Yuan; Wu, Yingcai] Zhejiang Univ, State Key Lab Cad & CG, Hangzhou 310058, Peoples R China.
   [Weng, Di] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Xu, Mingliang] Zhengzhou Univ, Sch Informat Engn, Zhengzhou, Peoples R China.
   [Xu, Mingliang] Zhengzhou Univ, Henan Inst Adv Technol, Zhengzhou 450001, Peoples R China.
C3 Zhejiang University; Microsoft; Microsoft Research Asia; Zhengzhou
   University; Zhengzhou University
RP Wu, YC (corresponding author), Zhejiang Univ, State Key Lab Cad & CG, Hangzhou 310058, Peoples R China.; Weng, D (corresponding author), Microsoft Res Asia, Beijing 100080, Peoples R China.
EM zikun_rain@zju.edu.cn; diweng@microsoft.com; shliu@zju.edu.cn;
   ytian@zju.edu.cn; iexumingliang@zzu.edu.cn; ycwu@zju.edu.cn
RI wang, yixuan/JGM-3893-2023; wang, David/KFR-2555-2024; yang,
   li/JGM-1009-2023; Li, J N/JXL-5833-2024; long, chen/JVM-8568-2024; Yu,
   ZH/KBC-6889-2024; yuanyuan, Li/JEZ-6497-2023; Wang, Yanan/JVZ-7957-2024;
   zhao, yan/JNT-6961-2023; zhang, xueying/JMB-7808-2023; FENG,
   X/JPL-4188-2023; zhang, yimeng/JLL-7337-2023; Weng, Di/ABG-7408-2020;
   Zhang, Jinfan/JPK-7588-2023; zhao, lin/JPK-8436-2023; Zhang,
   Xiaofeng/JMC-6060-2023; Li, Wen/JQI-4757-2023; Yang, Lili/JTT-5215-2023;
   Yang, Jie/JDM-6213-2023; Zhang, Wenbin/JXX-8070-2024; zhou,
   yang/JED-3951-2023; zhang, ying/JQX-1479-2023; li, li/JVP-2971-2024; LI,
   WEI/JUE-9796-2023; Wang, Han/JJF-2614-2023; wu, meng/JPK-1930-2023;
   wang, wenjuan/JGD-0428-2023; Chen, Feng/JQW-8742-2023; Li,
   Yan/JRW-0176-2023; xu, lingzhi/JVZ-8748-2024; 王, 娅冰/JGE-0541-2023;
   zhang, jt/JVE-1333-2024; li, yansong/JXL-5023-2024; Tian,
   Yuan/HTQ-2330-2023; li, qing/JEF-9044-2023; Zhou, heng/JCN-6493-2023;
   yang, yue/KCK-7870-2024; li, jiaxin/JNT-5073-2023; Deng,
   Zikun/IQT-3106-2023; Wang, Jing/JRW-1512-2023; Liu, Yang/JVD-6777-2023;
   Yan, Xin/KGL-5903-2024; Liu, Liu/JXM-8208-2024; LI, YUN/JTV-7108-2023;
   LI, SHA/JNR-9956-2023; Jiang, Yuan/JED-3759-2023; Yuan,
   Yu/KBQ-0606-2024; Zhang, Han/JMR-0670-2023
OI Weng, Di/0000-0003-2712-7274; Zhang, Xiaofeng/0000-0003-2738-3286; Yang,
   Lili/0009-0008-2926-484X; Yang, Jie/0000-0002-3941-0053; wang,
   wenjuan/0000-0002-4220-8817; Deng, Zikun/0000-0002-4477-5292
FU National Natural Science Foundation of China [62072400]; Collaborative
   Innovation Center of Artificial Intel-ligence by MOE; Zhejiang
   Provincial Government (ZJU); Zhejiang Lab [2021KE0AC02]
FX This work was supported by National Natural Science Foundation of China
   (62072400), the Collaborative Innovation Center of Artificial
   Intel-ligence by MOE and Zhejiang Provincial Government (ZJU), and the
   Zhejiang Lab (2021KE0AC02).
CR Accorsi P, 2014, IEEE CONF VIS ANAL, P123, DOI 10.1109/VAST.2014.7042488
   Afzal S., 2011, 2011 IEEE Conference on Visual Analytics Science and Technology, P191, DOI 10.1109/VAST.2011.6102457
   Al-Dohuki S, 2017, IEEE T VIS COMPUT GR, V23, P11, DOI 10.1109/TVCG.2016.2598416
   Andrienko G., 2011, 2011 IEEE Conference on Visual Analytics Science and Technology, P161, DOI 10.1109/VAST.2011.6102454
   Andrienko Gennady, 2010, 2010 Proceedings of IEEE Symposium on Visual Analytics Science and Technology (VAST 2010), P59, DOI 10.1109/VAST.2010.5652478
   Andrienko G, 2017, IEEE T VIS COMPUT GR, V23, P2120, DOI 10.1109/TVCG.2016.2616404
   Andrienko G, 2017, IEEE T INTELL TRANSP, V18, P2232, DOI 10.1109/TITS.2017.2683539
   Andrienko G, 2013, COMPUT SCI ENG, V15, P72, DOI 10.1109/MCSE.2013.70
   Andrienko G, 2013, IEEE T VIS COMPUT GR, V19, P1078, DOI 10.1109/TVCG.2012.311
   Andrienko N, 2020, IEEE T INTELL TRANSP, V21, P3196, DOI 10.1109/TITS.2019.2924796
   Andrienko N, 2016, INFORM SYST, V57, P172, DOI 10.1016/j.is.2015.08.007
   Andrienko N, 2011, IEEE T VIS COMPUT GR, V17, P205, DOI 10.1109/TVCG.2010.44
   [Anonymous], 2015, P SIGGRAPH ASIA VISU
   [Anonymous], 2004, P WORKING C ADV VISU, DOI DOI 10.1145/989863.989885
   Arietta SM, 2014, IEEE T VIS COMPUT GR, V20, P2624, DOI 10.1109/TVCG.2014.2346446
   Baumgartl T, 2021, IEEE T VIS COMPUT GR, V27, P711, DOI 10.1109/TVCG.2020.3030437
   Brehmer M, 2020, IEEE T VIS COMPUT GR, V26, P364, DOI 10.1109/TVCG.2019.2934397
   Brehmer M, 2019, IEEE T VIS COMPUT GR, V25, P619, DOI 10.1109/TVCG.2018.2865234
   Cao N, 2018, IEEE T VIS COMPUT GR, V24, P23, DOI 10.1109/TVCG.2017.2744419
   Cao N, 2012, IEEE T VIS COMPUT GR, V18, P2649, DOI 10.1109/TVCG.2012.291
   Chae J, 2012, IEEE CONF VIS ANAL, P143, DOI 10.1109/VAST.2012.6400557
   Chen R, 2022, IEEE T VIS COMPUT GR, V28, P4127, DOI 10.1109/TVCG.2021.3076222
   Chen SM, 2018, J VISUAL LANG COMPUT, V48, P187, DOI 10.1016/j.jvlc.2018.06.007
   Chen SM, 2017, COMPUT GRAPH FORUM, V36, P563, DOI 10.1111/cgf.13211
   Chen SM, 2016, IEEE CONF VIS ANAL, P41, DOI 10.1109/VAST.2016.7883510
   Chen SM, 2016, IEEE T VIS COMPUT GR, V22, P270, DOI 10.1109/TVCG.2015.2467619
   Chen W, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3200766
   Chen W, 2018, IEEE T VIS COMPUT GR, V24, P2636, DOI 10.1109/TVCG.2017.2758362
   Chen W, 2015, IEEE T INTELL TRANSP, V16, DOI 10.1109/TITS.2015.2436897
   Chen ZT, 2020, IEEE T VIS COMPUT GR, V26, P2645, DOI 10.1109/TVCG.2019.2892415
   Chen ZT, 2017, VIS INFORM, V1, P132, DOI 10.1016/j.visinf.2017.11.002
   Chu D, 2014, IEEE PAC VIS SYMP, P137, DOI 10.1109/PacificVis.2014.50
   Chu XT, 2022, IEEE T VIS COMPUT GR, V28, P118, DOI 10.1109/TVCG.2021.3114861
   Deng DZ, 2022, Arxiv, DOI arXiv:2007.04584
   Deng ZK, 2022, IEEE T VIS COMPUT GR, V28, P1051, DOI 10.1109/TVCG.2021.3114875
   Deng ZK, 2022, IEEE T VIS COMPUT GR, V28, P2486, DOI 10.1109/TVCG.2021.3071387
   Deng ZK, 2020, IEEE T VIS COMPUT GR, V26, P800, DOI 10.1109/TVCG.2019.2934670
   Di Lorenzo G, 2016, IEEE T VIS COMPUT GR, V22, P1036, DOI 10.1109/TVCG.2015.2440259
   Doraiswamy H, 2016, PROC INT CONF DATA, P1086, DOI 10.1109/ICDE.2016.7498315
   Doraiswamy H, 2014, IEEE T VIS COMPUT GR, V20, P2634, DOI 10.1109/TVCG.2014.2346449
   Eirich J, 2022, IEEE T VIS COMPUT GR, V28, P11, DOI 10.1109/TVCG.2021.3114797
   Feng ZZ, 2021, IEEE T VIS COMPUT GR, V27, P828, DOI 10.1109/TVCG.2020.3030469
   Ferreira N, 2015, IEEE CONF VIS ANAL, P97, DOI 10.1109/VAST.2015.7347636
   Ferreira N, 2013, IEEE T VIS COMPUT GR, V19, P2149, DOI 10.1109/TVCG.2013.226
   Garcia G, 2021, IEEE T VIS COMPUT GR, V27, P2313, DOI 10.1109/TVCG.2019.2947515
   Gautier J, 2020, 2020 IEEE VISUALIZATION CONFERENCE - SHORT PAPERS (VIS 2020), P71, DOI 10.1109/VIS47514.2020.00021
   Görtler J, 2018, IEEE T VIS COMPUT GR, V24, P719, DOI 10.1109/TVCG.2017.2743959
   Gou L, 2021, IEEE T VIS COMPUT GR, V27, P261, DOI 10.1109/TVCG.2020.3030350
   Gratzl S, 2013, IEEE T VIS COMPUT GR, V19, P2277, DOI 10.1109/TVCG.2013.173
   Gu TL, 2018, IEEE T COMPUT SOC SY, V5, P1121, DOI 10.1109/TCSS.2018.2858439
   Guo FZ, 2019, ACM T INTERACT INTEL, V9, DOI 10.1145/3182187
   Guo HQ, 2011, IEEE PAC VIS SYMP, P163, DOI 10.1109/PACIFICVIS.2011.5742386
   Guo Y, 2022, IEEE T VIS COMPUT GR, V28, P5091, DOI 10.1109/TVCG.2021.3100413
   Han SY, 2020, COMPUT VIS MEDIA, V6, P333, DOI 10.1007/s41095-020-0178-4
   He Liu, 2011, 2011 IEEE Conference on Visual Analytics Science and Technology, P171, DOI 10.1109/VAST.2011.6102455
   He TF, 2020, IEEE T KNOWL DATA EN, V32, P1529, DOI 10.1109/TKDE.2019.2907091
   He WB, 2022, IEEE T VIS COMPUT GR, V28, P1040, DOI 10.1109/TVCG.2021.3114855
   Holten D, 2006, IEEE T VIS COMPUT GR, V12, P741, DOI 10.1109/TVCG.2006.147
   Hou YJ, 2022, IEEE T VIS COMPUT GR, V28, P1030, DOI 10.1109/TVCG.2021.3114777
   Hu K, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300892
   Hu Mengdie, 2012, P SIGCHI C HUM FACT, P2751, DOI [10.1145/2207676.2208672, DOI 10.1145/2207676.2208672]
   Huang KT, 2021, IEEE COMPUT GRAPH, V41, P26, DOI 10.1109/MCG.2020.3041371
   Huang XK, 2016, IEEE T VIS COMPUT GR, V22, P160, DOI 10.1109/TVCG.2015.2467771
   Huang ZS, 2020, IEEE T VIS COMPUT GR, V26, P1256, DOI 10.1109/TVCG.2019.2934671
   Huang ZS, 2020, IEEE T VIS COMPUT GR, V26, P2576, DOI 10.1109/TVCG.2019.2892483
   Hurter C, 2019, IEEE T VIS COMPUT GR, V25, P704, DOI 10.1109/TVCG.2018.2865191
   Isenberg P, 2012, IEEE T VIS COMPUT GR, V18, P689, DOI 10.1109/TVCG.2011.287
   Jamonnak S, 2022, IEEE T VIS COMPUT GR, V28, P1019, DOI 10.1109/TVCG.2021.3114853
   Jin ZC, 2021, J VISUAL-JAPAN, V24, P349, DOI 10.1007/s12650-020-00707-1
   Jing Yuan, 2010, Proceedings 11th International Conference on Mobile Data Management (MDM 2010), P43, DOI 10.1109/MDM.2010.14
   Kamw F, 2020, IEEE T INTELL TRANSP, V21, P104, DOI 10.1109/TITS.2018.2888994
   Kim S, 2018, IEEE T VIS COMPUT GR, V24, P1287, DOI 10.1109/TVCG.2017.2666146
   Knittel J, 2022, IEEE T VIS COMPUT GR, V28, P879, DOI 10.1109/TVCG.2021.3114800
   Krueger R, 2015, IEEE T VIS COMPUT GR, V21, P903, DOI 10.1109/TVCG.2014.2371856
   Lee C, 2020, IEEE T VIS COMPUT GR, V26, P3133, DOI 10.1109/TVCG.2019.2922597
   Li CH, 2022, IEEE T VIS COMPUT GR, V28, P1062, DOI 10.1109/TVCG.2021.3114762
   Li CL, 2018, VIS INFORM, V2, P50, DOI 10.1016/j.visint2018.04.006
   Li JPK, 2020, IEEE T VIS COMPUT GR, V26, P1151, DOI 10.1109/TVCG.2019.2934537
   Li JPK, 2020, IEEE T VIS COMPUT GR, V26, P1548, DOI 10.1109/TVCG.2018.2871139
   Li Jie, 2020, IEEE Trans Vis Comput Graph, V26, P1789, DOI 10.1109/TVCG.2018.2882449
   Li J, 2019, IEEE T VIS COMPUT GR, V25, P2554, DOI 10.1109/TVCG.2018.2851227
   Li J, 2014, IEEE CONF VIS ANAL, P133, DOI 10.1109/VAST.2014.7042489
   Li Q, 2020, COMPUT GRAPH FORUM, V39, P523, DOI 10.1111/cgf.13999
   Li Q, 2020, COMPUT GRAPH FORUM, V39, P483, DOI 10.1111/cgf.13996
   Li Q, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376281
   Li RY, 2020, PROC INT CONF DATA, P1558, DOI 10.1109/ICDE48307.2020.00138
   Li RY, 2020, IEEE T BIG DATA, V6, P66, DOI 10.1109/TBDATA.2018.2868936
   Li YH, 2018, IEEE T BIG DATA, V4, P556, DOI 10.1109/TBDATA.2017.2717978
   Li Yuhong, 2016, PROC SIGSPATIAL INT, P51
   Liang YX, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P1833, DOI 10.1145/3442381.3449792
   Liang YX, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P3132, DOI 10.1145/3292500.3330646
   Liang YX, 2017, 25TH ACM SIGSPATIAL INTERNATIONAL CONFERENCE ON ADVANCES IN GEOGRAPHIC INFORMATION SYSTEMS (ACM SIGSPATIAL GIS 2017), DOI 10.1145/3139958.3139960
   Liao BB, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P537, DOI 10.1145/3219819.3219895
   Liao HS, 2015, 2015 IEEE SCIENTIFIC VISUALIZATION CONFERENCE (SCIVIS), P25, DOI 10.1109/SciVis.2015.7429488
   Lins L, 2013, IEEE T VIS COMPUT GR, V19, P2456, DOI 10.1109/TVCG.2013.179
   Liu C, 2020, IEEE T VIS COMPUT GR, V26, P790, DOI 10.1109/TVCG.2019.2934434
   Liu DY, 2019, IEEE T VIS COMPUT GR, V25, P1, DOI 10.1109/TVCG.2018.2865018
   Liu DY, 2017, IEEE T VIS COMPUT GR, V23, P1, DOI 10.1109/TVCG.2016.2598432
   Liu HY, 2021, VIS INFORM, V5, P1, DOI 10.1016/j.visinf.2021.10.002
   Liu MC, 2016, IEEE T VIS COMPUT GR, V22, P250, DOI 10.1109/TVCG.2015.2467554
   Liu Q. Q., 2020, P EUROVIS POSTERS, P21
   Liu QQ, 2020, 2020 IEEE VISUALIZATION CONFERENCE - SHORT PAPERS (VIS 2020), P76, DOI 10.1109/VIS47514.2020.00022
   Liu SX, 2014, VISUAL COMPUT, V30, P1373, DOI 10.1007/s00371-013-0892-3
   Liu SX, 2013, IEEE T VIS COMPUT GR, V19, P2436, DOI 10.1109/TVCG.2013.196
   Liu SY, 2013, IEEE T INTELL TRANSP, V14, P1586, DOI 10.1109/TITS.2013.2263225
   Lou Y., 2009, P 17 ACM SIGSPATIAL, P352, DOI [10.1145/1653771.1653820, DOI 10.1145/1653771.1653820]
   Lu Lu, 2014, Advances in Knowledge Discovery and Data Mining. 18th Pacific-Asia Conference (PAKDD 2014). Proceedings: LNCS 8443, P509, DOI 10.1007/978-3-319-06608-0_42
   Lu M, 2017, IEEE T BIG DATA, V3, P234, DOI 10.1109/TBDATA.2017.2667700
   Lu M, 2016, J VISUAL-JAPAN, V19, P811, DOI 10.1007/s12650-016-0357-7
   Lu M, 2015, IEEE PAC VIS SYMP, P311, DOI 10.1109/PACIFICVIS.2015.7156392
   Lu M, 2015, IEEE PAC VIS SYMP, P87, DOI 10.1109/PACIFICVIS.2015.7156361
   Lu YF, 2015, WWW'15 COMPANION: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P1211, DOI 10.1145/2740908.2741720
   Lukasczyk J, 2015, 23RD ACM SIGSPATIAL INTERNATIONAL CONFERENCE ON ADVANCES IN GEOGRAPHIC INFORMATION SYSTEMS (ACM SIGSPATIAL GIS 2015), DOI 10.1145/2820783.2820817
   Lv P, 2022, COMPUT VIS MEDIA, V8, P213, DOI 10.1007/s41095-021-0236-6
   Ma YX, 2016, IEEE T INTELL TRANSP, V17, P2627, DOI 10.1109/TITS.2015.2498187
   MacEachren A. M., 2011, 2011 IEEE Conference on Visual Analytics Science and Technology, P181, DOI 10.1109/VAST.2011.6102456
   Maciejewski R, 2011, IEEE T VIS COMPUT GR, V17, P440, DOI 10.1109/TVCG.2010.82
   Maciejewski R, 2010, IEEE T VIS COMPUT GR, V16, P205, DOI 10.1109/TVCG.2009.100
   Maciejewski R, 2007, IEEE S VIS ANAL, P27, DOI 10.1109/VAST.2007.4388993
   Malik A., 2011, 2011 IEEE Conference on Visual Analytics Science and Technology, P221, DOI 10.1109/VAST.2011.6102460
   Malik A, 2014, IEEE T VIS COMPUT GR, V20, P1863, DOI 10.1109/TVCG.2014.2346926
   Malik A, 2012, IEEE CONF VIS ANAL, P33, DOI 10.1109/VAST.2012.6400491
   Meghdadi AH, 2013, IEEE T VIS COMPUT GR, V19, P2119, DOI 10.1109/TVCG.2013.168
   Mei HH, 2020, IEEE T VIS COMPUT GR, V26, P1161, DOI 10.1109/TVCG.2019.2934800
   Miranda F, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376399
   Miranda F, 2019, IEEE T VIS COMPUT GR, V25, P1559, DOI 10.1109/TVCG.2018.2802945
   Miranda F, 2017, IEEE T VIS COMPUT GR, V23, P791, DOI 10.1109/TVCG.2016.2598585
   Pahins CAL, 2017, IEEE T VIS COMPUT GR, V23, P671, DOI 10.1109/TVCG.2016.2598624
   Palomo C, 2016, IEEE T VIS COMPUT GR, V22, P170, DOI 10.1109/TVCG.2015.2467592
   Pan ZY, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1720, DOI 10.1145/3292500.3330884
   Peña-Araya V, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376350
   Pezzotti N, 2017, IEEE T VIS COMPUT GR, V23, P1739, DOI 10.1109/TVCG.2016.2570755
   Pi M, 2021, IEEE T VIS COMPUT GR, V27, P2186, DOI 10.1109/TVCG.2019.2940580
   Piringer H, 2012, IEEE CONF VIS ANAL, P153, DOI 10.1109/VAST.2012.6400556
   Poco J, 2015, COMPUT GRAPH FORUM, V34, P161, DOI 10.1111/cgf.12628
   Pu JS, 2013, 2013 IEEE 14TH INTERNATIONAL CONFERENCE ON MOBILE DATA MANAGEMENT (MDM 2013), VOL 1, P127, DOI 10.1109/MDM.2013.23
   Qu HM, 2009, IEEE T VIS COMPUT GR, V15, P1547, DOI 10.1109/TVCG.2009.144
   Qu H, 2007, IEEE T VIS COMPUT GR, V13, P1408, DOI 10.1109/TVCG.2007.70523
   Nguyen QV, 2020, VIS INFORM, V4, P1, DOI 10.1016/j.visinf.2020.09.004
   Quinan PS, 2016, IEEE T VIS COMPUT GR, V22, P389, DOI 10.1109/TVCG.2015.2467754
   Scheepens R, 2011, IEEE T VIS COMPUT GR, V17, P2518, DOI 10.1109/TVCG.2011.181
   Schöttler S, 2021, COMPUT GRAPH FORUM, V40, P5, DOI 10.1111/cgf.14198
   Schulz C, 2017, IEEE T VIS COMPUT GR, V23, P531, DOI 10.1109/TVCG.2016.2598919
   Schwab M, 2021, IEEE T VIS COMPUT GR, V27, P347, DOI 10.1109/TVCG.2020.3030366
   Sedlmair M, 2012, IEEE T VIS COMPUT GR, V18, P2431, DOI 10.1109/TVCG.2012.213
   Shen QM, 2020, IEEE PAC VIS SYMP, P61, DOI 10.1109/PacificVis48177.2020.2785
   Shen QM, 2018, IEEE T VIS COMPUT GR, V24, P1004, DOI 10.1109/TVCG.2017.2744159
   Shi L, 2021, IEEE T VIS COMPUT GR, V27, P3881, DOI 10.1109/TVCG.2020.2992200
   Shneiderman B, 1996, IEEE SYMPOSIUM ON VISUAL LANGUAGES, PROCEEDINGS, P336, DOI 10.1109/VL.1996.545307
   Shu XH, 2021, J VISUAL-JAPAN, V24, P85, DOI 10.1007/s12650-020-00689-0
   Steptoe M, 2018, ACM T INTERACT INTEL, V8, DOI 10.1145/3162076
   Stolper CD, 2014, IEEE T VIS COMPUT GR, V20, P1653, DOI 10.1109/TVCG.2014.2346574
   Su CY, 2021, VIS INFORM, V5, P56, DOI 10.1016/j.visinf.2021.12.005
   Sun GD, 2019, J VISUAL-JAPAN, V22, P1193, DOI 10.1007/s12650-019-00600-6
   Sun GD, 2017, IEEE T VIS COMPUT GR, V23, P1506, DOI 10.1109/TVCG.2016.2535234
   Sun GD, 2014, IEEE PAC VIS SYMP, P185, DOI 10.1109/PacificVis.2014.56
   Sun GD, 2014, IEEE T VIS COMPUT GR, V20, P1753, DOI 10.1109/TVCG.2014.2346919
   Sun GD, 2013, SCI CHINA INFORM SCI, V56, DOI 10.1007/s11432-013-4830-9
   Tang JX, 2022, IEEE T VIS COMPUT GR, V28, P857, DOI 10.1109/TVCG.2021.3114878
   Tominski C, 2012, IEEE T VIS COMPUT GR, V18, P2565, DOI 10.1109/TVCG.2012.265
   von Landesberger T, 2016, IEEE T VIS COMPUT GR, V22, P11, DOI 10.1109/TVCG.2015.2468111
   Wang F, 2017, IEEE T INTELL TRANSP, V18, P2250, DOI 10.1109/TITS.2017.2711644
   Wang F, 2014, IEEE CONF VIS ANAL, P103, DOI 10.1109/VAST.2014.7042486
   Wang GZ, 2020, IEEE CONF VIS ANAL, P72, DOI 10.1109/VAST50239.2020.00012
   Wang HX, 2021, VIS INFORM, V5, P82, DOI 10.1016/j.visinf.2021.09.001
   Wang H, 2019, IEEE T VIS COMPUT GR, V25, P331, DOI 10.1109/TVCG.2018.2864844
   Wang JC, 2022, J VISUAL-JAPAN, V25, P59, DOI 10.1007/s12650-021-00778-8
   Wang QW, 2022, IEEE T VIS COMPUT GR, V28, P5134, DOI 10.1109/TVCG.2021.3106142
   Wang ZC, 2014, IEEE T VIS COMPUT GR, V20, P1813, DOI 10.1109/TVCG.2014.2346746
   Wang ZC, 2013, IEEE T VIS COMPUT GR, V19, P2159, DOI 10.1109/TVCG.2013.228
   Wei DT, 2021, J VISUAL-JAPAN, V24, P597, DOI 10.1007/s12650-020-00717-z
   Weng D, 2021, IEEE T INTELL TRANSP, V22, P1185, DOI 10.1109/TITS.2020.2964012
   Weng D, 2021, IEEE T VIS COMPUT GR, V27, P817, DOI 10.1109/TVCG.2020.3030458
   Weng D, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173821
   Weng D, 2019, IEEE T VIS COMPUT GR, V25, P459, DOI 10.1109/TVCG.2018.2865126
   Whitlock M, 2020, IEEE T VIS COMPUT GR, V26, P503, DOI 10.1109/TVCG.2019.2934282
   Wu AY, 2022, IEEE T VIS COMPUT GR, V28, P5049, DOI 10.1109/TVCG.2021.3099002
   Wu AY, 2021, IEEE T VIS COMPUT GR, V27, P464, DOI 10.1109/TVCG.2020.3030423
   Wu WC, 2016, IEEE T VIS COMPUT GR, V22, P935, DOI 10.1109/TVCG.2015.2467194
   Wu WC, 2014, IEEE CONF VIS ANAL, P143, DOI 10.1109/VAST.2014.7042490
   Wu YC, 2021, IEEE T INTELL TRANSP, V22, P3387, DOI 10.1109/TITS.2020.2983226
   Wu YC, 2018, IEEE T VIS COMPUT GR, V24, P2758, DOI 10.1109/TVCG.2017.2764459
   Wu YC, 2019, IEEE T VIS COMPUT GR, V25, P65, DOI 10.1109/TVCG.2018.2865041
   Wu Y, 2018, IEEE T VIS COMPUT GR, V24, P709, DOI 10.1109/TVCG.2017.2744218
   Wu YC, 2016, IEEE T MULTIMEDIA, V18, P2135, DOI 10.1109/TMM.2016.2614220
   Wu YC, 2014, IEEE T VIS COMPUT GR, V20, P1763, DOI 10.1109/TVCG.2014.2346920
   Xie X, 2021, IEEE T VIS COMPUT GR, V27, P1322, DOI 10.1109/TVCG.2020.3030359
   Xu PP, 2013, IEEE T VIS COMPUT GR, V19, P2012, DOI 10.1109/TVCG.2013.221
   Yang YL, 2017, IEEE T VIS COMPUT GR, V23, P411, DOI 10.1109/TVCG.2016.2598885
   Ye SN, 2021, IEEE T VIS COMPUT GR, V27, P860, DOI 10.1109/TVCG.2020.3030392
   Ying L, 2022, IEEE T VIS COMPUT GR, V28, P400, DOI 10.1109/TVCG.2021.3114877
   Yixian Zheng, 2016, IEEE Transactions on Big Data, V2, P276, DOI 10.1109/TBDATA.2016.2586447
   Yu L, 2015, IEEE CONF VIS ANAL, P49, DOI 10.1109/VAST.2015.7347630
   Yuan J, 2021, COMPUT VIS MEDIA, V7, P3, DOI 10.1007/s41095-020-0191-7
   Yuan J, 2021, IEEE T VIS COMPUT GR, V27, P1720, DOI 10.1109/TVCG.2020.3030432
   Zeng W, 2019, COMPUT GRAPH FORUM, V38, P581, DOI 10.1111/cgf.13712
   Zeng W, 2016, COMPUT GRAPH FORUM, V35, P95, DOI 10.1111/cgf.12778
   Zeng W, 2021, IEEE T VIS COMPUT GR, V27, P839, DOI 10.1109/TVCG.2020.3030410
   Zeng W, 2018, IEEE COMPUT GRAPH, V38, P38, DOI 10.1109/MCG.2018.053491730
   Zeng W, 2017, VIS INFORM, V1, P81, DOI 10.1016/j.visinf.2017.07.001
   Zeng W, 2017, IEEE T INTELL TRANSP, V18, P2271, DOI 10.1109/TITS.2016.2639320
   Zeng W, 2014, IEEE T VIS COMPUT GR, V20, P1833, DOI 10.1109/TVCG.2014.2346893
   Zeng W, 2013, COMPUT GRAPH FORUM, V32, P271, DOI 10.1111/cgf.12114
   Zhang JW, 2014, IEEE T VIS COMPUT GR, V20, P1843, DOI 10.1109/TVCG.2014.2346898
   Zhang YF, 2017, IEEE T VIS COMPUT GR, V23, P371, DOI 10.1109/TVCG.2016.2598541
   Zhao Y, 2021, IEEE T VIS COMPUT GR, V27, P1698, DOI 10.1109/TVCG.2020.3030428
   Zheng FL, 2021, J VISUAL-JAPAN, V24, P1303, DOI 10.1007/s12650-021-00777-9
   Zheng Y, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P843, DOI 10.1109/BigData.2016.7840677
   Zheng YX, 2015, PROCEEDINGS 2015 IEEE INTERNATIONAL CONFERENCE ON BIG DATA, P581, DOI 10.1109/BigData.2015.7363802
   Zheng Y, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2267, DOI 10.1145/2783258.2788573
   Zheng Y, 2014, ACM T INTEL SYST TEC, V5, DOI 10.1145/2629592
   Zheng Y, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P1436, DOI 10.1145/2487575.2488188
   Zhou ZG, 2020, IEEE CONF VIS ANAL, P60, DOI 10.1109/VAST50239.2020.00011
   Zhou ZG, 2021, IEEE T VIS COMPUT GR, V27, P1709, DOI 10.1109/TVCG.2020.3030440
   Zhou ZG, 2019, IEEE T VIS COMPUT GR, V25, P43, DOI 10.1109/TVCG.2018.2864503
   Zhu HY, 2021, VIS INFORM, V5, P51, DOI 10.1016/j.visinf.2021.06.002
   Zhu MF, 2019, IEEE T INTELL TRANSP, V20, P3981, DOI 10.1109/TITS.2019.2901117
   Zicheng Liao, 2010, 2010 Proceedings of IEEE Symposium on Visual Analytics Science and Technology (VAST 2010), P51, DOI 10.1109/VAST.2010.5652467
NR 218
TC 14
Z9 15
U1 11
U2 65
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2023
VL 9
IS 1
BP 3
EP 39
DI 10.1007/s41095-022-0275-7
PG 37
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5K7GQ
UT WOS:000869891100002
PM 36277276
OA gold, Green Published
DA 2024-07-18
ER

PT J
AU Huang, JH
   Yang, S
   Zhao, ZS
   Lai, YK
   Hu, SM
AF Huang, Jiahui
   Yang, Sheng
   Zhao, Zishuo
   Lai, Yu-Kun
   Hu, Shi-Min
TI ClusterSLAM: A SLAM backend for simultaneous rigid body clustering and
   motion estimation
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE dynamic SLAM; motion segmentation; scene perception
ID FACTORIZATION; ALGORITHM; CONSENSUS; TRACKING
AB We present a practical backend for stereo visual SLAM which can simultaneously discover individual rigid bodies and compute their motions in dynamic environments. While recent factor graph based state optimization algorithms have shown their ability to robustly solve SLAM problems by treating dynamic objects as outliers, their dynamic motions are rarely considered. In this paper, we exploit the consensus of 3D motions for landmarks extracted from the same rigid body for clustering, and to identify static and dynamic objects in a unified manner. Specifically, our algorithm builds a noise-aware motion affinity matrix from landmarks, and uses agglomerative clustering to distinguish rigid bodies. Using decoupled factor graph optimization to revise their shapes and trajectories, we obtain an iterative scheme to update both cluster assignments and motion estimation reciprocally. Evaluations on both synthetic scenes and KITTI demonstrate the capability of our approach, and further experiments considering online efficiency also show the effectiveness of our method for simultaneously tracking ego-motion and multiple objects.
C1 [Huang, Jiahui; Zhao, Zishuo; Hu, Shi-Min] Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing 100084, Peoples R China.
   [Yang, Sheng] Alibaba AI Labs, Hangzhou 311121, Peoples R China.
   [Lai, Yu-Kun] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF24 3AA, Wales.
C3 Tsinghua University; Alibaba Group; Cardiff University
RP Hu, SM (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing 100084, Peoples R China.
EM huang-jh18@mails.tsinghua.edu.cn; shengyang93fs@gmail.com;
   wingedkuriboh@126.com; LaiY4@cardiff.ac.uk; shimin@tsinghua.edu.cn
RI Hu, Shi-Min/AAW-1952-2020; Huang, Jiahui/AAN-1773-2021; Lai,
   Yu-Kun/D-2343-2010
FU National Key Technology RD Program [2017YFB1002604]; Joint NSFC-DFG
   Research Program [61761136018]; National Natural Science Foundation of
   China [61521002]
FX This work was supported by the National Key Technology R&D Program
   (Project No. 2017YFB1002604), the Joint NSFC-DFG Research Program
   (Project No. 61761136018), and the National Natural Science Foundation
   of China (Project No. 61521002).
CR Agarwal P, 2013, IEEE INT CONF ROBOT, P62, DOI 10.1109/ICRA.2013.6630557
   Alcantarilla PF, 2012, IEEE INT CONF ROBOT, P1290, DOI 10.1109/ICRA.2012.6224690
   [Anonymous], 2018, ACM T GRAPHICS, V37, P5
   Azartash Haleh, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1280, DOI 10.1109/ICASSP.2014.6853803
   Bârsan IA, 2018, IEEE INT CONF ROBOT, P7510
   Bescos B, 2018, IEEE ROBOT AUTOM LET, V3, P4076, DOI 10.1109/LRA.2018.2860039
   Carlone L, 2014, IEEE INT C INT ROBOT, P2667, DOI 10.1109/IROS.2014.6942927
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Costeira JP, 1998, INT J COMPUT VISION, V29, P159, DOI 10.1023/A:1008000628999
   DEFAYS D, 1977, COMPUT J, V20, P364, DOI 10.1093/comjnl/20.4.364
   DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060
   Dosovitskiy A., 2017, P 1 ANN C ROB LEARN, P1, DOI DOI 10.48550/ARXIV.1711.03938
   Fan Ruochen, 2017, [Computational Visual Media, 计算可视媒体], V3, P285
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Guha S, 2001, INFORM SYST, V26, P35, DOI 10.1016/S0306-4379(01)00008-4
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Huang JH, 2019, IEEE I CONF COMP VIS, P5874, DOI 10.1109/ICCV.2019.00597
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Isack H, 2012, INT J COMPUT VISION, V97, P123, DOI 10.1007/s11263-011-0474-7
   Jaimez Mariano, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3992, DOI 10.1109/ICRA.2017.7989459
   Judd KM, 2018, IEEE INT C INT ROBOT, P3949, DOI 10.1109/IROS.2018.8594213
   Kim DH, 2016, IEEE T ROBOT, V32, P1565, DOI 10.1109/TRO.2016.2609395
   Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607
   Kundu A, 2011, IEEE I CONF COMP VIS, P2080, DOI 10.1109/ICCV.2011.6126482
   Lenz P, 2011, IEEE INT VEH SYM, P926, DOI 10.1109/IVS.2011.5940558
   Li PL, 2018, LECT NOTES COMPUT SC, V11206, P664, DOI 10.1007/978-3-030-01216-8_40
   Li T, 2007, PROC CVPR IEEE, P922
   Meila M, 2003, LECT NOTES ARTIF INT, V2777, P173, DOI 10.1007/978-3-540-45167-9_14
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Murali V, 2017, IEEE INT C INTELL TR
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Nguyen N, 2007, IEEE DATA MINING, P607, DOI 10.1109/ICDM.2007.73
   Paull L, 2015, IEEE INT CONF ROBOT, P509, DOI 10.1109/ICRA.2015.7139227
   Ravankar A, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18093170
   Reddy ND, 2018, PROC CVPR IEEE, P1906, DOI 10.1109/CVPR.2018.00204
   Runz Martin, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4471, DOI 10.1109/ICRA.2017.7989518
   Rünz M, 2018, INT SYM MIX AUGMENT, P10, DOI 10.1109/ISMAR.2018.00024
   Saputra MRU, 2018, ACM COMPUT SURV, V51, DOI 10.1145/3177853
   SOKAL ROBERT R., 1958, UNIV KANSAS SCI BULL, V38, P1409
   Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28
   Strecke M, 2019, IEEE I CONF COMP VIS, P5864, DOI 10.1109/ICCV.2019.00596
   Vidal R, 2008, IEEE T PATTERN ANAL, V30, P214, DOI [10.1109/TPAMI.2007.1179, 10.1109/TPAMl.2007.1179]
   Vidal R, 2006, INT J COMPUT VISION, V68, P7, DOI 10.1007/s11263-005-4839-7
   Xie ZF, 2018, J COMPUT SCI TECH-CH, V33, P487, DOI 10.1007/s11390-018-1833-4
   Xu BB, 2019, IEEE INT CONF ROBOT, P5231, DOI [10.1109/icra.2019.8794371, 10.1109/ICRA.2019.8794371]
   Xu X, 2018, PROC CVPR IEEE, P2859, DOI 10.1109/CVPR.2018.00302
   Yuan G, 2017, ARTIF INTELL REV, V47, P123, DOI 10.1007/s10462-016-9477-7
   Zhang CC, 2017, J COMPUT SCI TECH-CH, V32, P520, DOI 10.1007/s11390-017-1741-z
NR 50
TC 8
Z9 9
U1 1
U2 26
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2021
VL 7
IS 1
BP 87
EP 101
DI 10.1007/s41095-020-0195-3
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FQ
UT WOS:000648692800005
OA gold, Green Accepted
DA 2024-07-18
ER

PT J
AU Wang, XC
   Liang, XH
   Yang, BL
   Li, FWB
AF Wang, Xiaochuan
   Liang, Xiaohui
   Yang, Bailin
   Li, Frederick W. B.
TI No-reference synthetic image quality assessment with convolutional
   neural network and local image saliency
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE image quality assessment; synthetic image; depth-image-based rendering
   (DIBR); convolutional neural network; local image saliency
ID DATABASE
AB Depth-image-based rendering (DIBR) is widely used in 3DTV, free-viewpoint video, and interactive 3D graphics applications. Typically, synthetic images generated by DIBR-based systems incorporate various distortions, particularly geometric distortions induced by object dis-occlusion. Ensuring the quality of synthetic images is critical to maintaining adequate system service. However, traditional 2D image quality metrics are ineffective for evaluating synthetic images as they are not sensitive to geometric distortion. In this paper, we propose a novel no-reference image quality assessment method for synthetic images based on convolutional neural networks, introducing local image saliency as prediction weights. Due to the lack of existing training data, we construct a new DIBR synthetic image dataset as part of our contribution. Experiments were conducted on both the public benchmark IRCCyN/IVC DIBR image dataset and our own dataset. Results demonstrate that our proposed metric outperforms traditional 2D image quality metrics and state-of-the-art DIBR-related metrics.
C1 [Wang, Xiaochuan; Liang, Xiaohui] Beihang Univ, State Kay Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Yang, Bailin] Zhejiang Gongshang Univ, Sch Comp Sci & Informat Engn, Hangzhou 310018, Peoples R China.
   [Li, Frederick W. B.] Univ Durham, Dept Comp Sci, Durham, England.
C3 Beihang University; Zhejiang Gongshang University; Durham University
RP Liang, XH (corresponding author), Beihang Univ, State Kay Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM wangxc@buaa.edu.cn; liang_xiaohui@buaa.edu.cn; ybl@mail.zjgsu.edu.cn;
   frederick.li@durham.ac.uk
OI liang, xiaohui/0000-0001-6351-2538; Li, Frederick W.
   B./0000-0002-4283-4228
FU National Key R&D Program of China [2017YFB1002702]; National Natural
   Science Foundation of China [61572058, 61472363]
FX The authors would like to thank the anonymous reviewers for their
   valuable comments. They would also thank Kai Wang and Jialei Li for
   their assistance in dataset construction and public release. The work
   was sponsored by the National Key R&D Program of China (No.
   2017YFB1002702), and the National Natural Science Foundation of China
   (Nos. 61572058, 61472363).
CR ACM, 2012, T MULTIMEDIA COMPUTI, V8, p3s
   Baker S, 2007, IEEE I CONF COMP VIS, P588, DOI 10.1109/cvpr.2007.383191
   Bao P, 2006, IEEE T MULTIMEDIA, V8, P382, DOI 10.1109/TMM.2005.864337
   Bao P., 2003, International Conference on Visual Information Engineering (VIE 2003) (IEE Conf. Publ.No.495), P61, DOI 10.1049/cp:20030487
   Bare B, 2017, IEEE INT CON MULTI, P1356, DOI 10.1109/ICME.2017.8019508
   Battisti F, 2015, SIGNAL PROCESS-IMAGE, V30, P78, DOI 10.1016/j.image.2014.10.005
   Bosc E, 2011, PROC SPIE, V8135, DOI 10.1117/12.895433
   Bosc E, 2011, IEEE J-STSP, V5, P1332, DOI 10.1109/JSTSP.2011.2166245
   Bosse S, 2016, IEEE IMAGE PROC, P3773, DOI 10.1109/ICIP.2016.7533065
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Conze PH, 2012, PROC SPIE, V8288, DOI 10.1117/12.908762
   Domanski M., 2009, POZNAN MULTIVIEW VID
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Gu K, 2018, IEEE T IMAGE PROCESS, V27, P394, DOI 10.1109/TIP.2017.2733164
   Heng W, 2017, INT CONF ACOUST SPEE, P1238, DOI 10.1109/ICASSP.2017.7952354
   I. T. Union, 1999, BT910 ITUR
   I. T. Union, 1993, BT50012 ITUR
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kim J, 2018, IEEE IMAGE PROC, P291, DOI 10.1109/ICIP.2018.8451346
   Kimata H., 2004, NTT TECHNICAL REV, V2, P21
   Lin KY, 2018, PROC CVPR IEEE, P732, DOI 10.1109/CVPR.2018.00083
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Mark W.R., 1999, Post-Rendering 3D Image Warping: Visiblity, Reconstruction, and Performance for Depth-Image Warping
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Ponomarenko N., 2009, ADV MODERN RADIOELEC, V10, P30, DOI 10.1109/TIP.2015.2439035
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Sandic-Stankovic D, 2015, INT WORK QUAL MULTIM
   Sandic-Stankovic D, 2016, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-016-0124-7
   SHARIFI K, 1995, IEEE T CIRC SYST VID, V5, P52, DOI 10.1109/76.350779
   Sheikh HR, 2005, LIVE IMAGE QUALITY A, DOI DOI 10.1109/CVPR.2015.7298594
   Smolic A, 2011, PATTERN RECOGN, V44, P1958, DOI 10.1016/j.patcog.2010.09.005
   Smolic A, 2009, PCS: 2009 PICTURE CODING SYMPOSIUM, P389
   Song R, 2015, J INF SCI ENG, V31, P1593
   Tian SS, 2018, IEEE T IMAGE PROCESS, V27, P1652, DOI 10.1109/TIP.2017.2781420
   Wang XC, 2018, IEEE ACCESS, V6, P36595, DOI 10.1109/ACCESS.2018.2853132
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Winkler S, 2012, IEEE J-STSP, V6, P616, DOI 10.1109/JSTSP.2012.2215007
   Yang XK, 2005, SIGNAL PROCESS-IMAGE, V20, P662, DOI 10.1016/j.image.2005.04.001
   Zhang FL, 2018, IEEE T MULTIMEDIA, V20, P1987, DOI 10.1109/TMM.2018.2790163
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhou Y, 2016, IEEE IMAGE PROC, P1012, DOI 10.1109/ICIP.2016.7532510
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
   Zitnick CL, 2004, ACM T GRAPHIC, V23, P600, DOI 10.1145/1015706.1015766
NR 45
TC 20
Z9 20
U1 0
U2 7
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2019
VL 5
IS 2
BP 193
EP 208
DI 10.1007/s41095-019-0131-6
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UM
UT WOS:000648690800005
OA Green Published, Green Accepted, gold
DA 2024-07-18
ER

PT J
AU Wu, WS
   Wang, BB
   Yan, LQ
AF Wu, Wenshi
   Wang, Beibei
   Yan, Ling-Qi
TI A survey on rendering homogeneous participating media
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review
DE participating media; Monte Carlo methods; rendering; volume density
   estimation
ID LIGHT-TRANSPORT; SINGLE SCATTERING; COMPUTATION; MODEL
AB Participating media are frequent in real-world scenes, whether they contain milk, fruit juice, oil, or muddy water in a river or the ocean. Incoming light interacts with these participating media in complex ways: refraction at boundaries and scattering and absorption inside volumes. The radiative transfer equation is the key to solving this problem. There are several categories of rendering methods which are all based on this equation, but using different solutions. In this paper, we introduce these groups, which include volume density estimation based approaches, virtual point/ray/beam lights, point based approaches, Monte Carlo based approaches, acceleration techniques, accurate single scattering methods, neural network based methods, and spatially-correlated participating media related methods. As well as discussing these methods, we consider the challenges and open problems in this research area.
C1 [Wu, Wenshi; Wang, Beibei] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing, Peoples R China.
   [Yan, Ling-Qi] Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA.
C3 Nanjing University of Science & Technology; University of California
   System; University of California Santa Barbara
RP Wang, BB (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing, Peoples R China.; Yan, LQ (corresponding author), Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA.
EM beibei.wang@njust.edu.cn; lingqi@cs.ucsb.edu
CR [Anonymous], 2014, ACM T GRAPHIC, V33
   [Anonymous], 2012, ACM T GRAPHIC, V31
   [Anonymous], 2019, ACM T GRAPHIC, V38
   [Anonymous], 2020, ACM T GRAPHIC, V39
   [Anonymous], 2017, ACM T GRAPHIC, V36
   [Anonymous], 2019, ACM T GRAPHIC, V38
   [Anonymous], 2018, ARXIV180803856
   [Anonymous], 2018, ACM T GRAPHIC, V37
   [Anonymous], 2019, ACM T GRAPHIC, V38
   [Anonymous], 2016, ACM T GRAPHIC, V35
   [Anonymous], 2018, ACM T GRAPHIC, V37
   [Anonymous], 2013, ACM T GRAPHIC, V32
   [Anonymous], 2014, ACM T GRAPHIC, V33
   [Anonymous], 2016, ACM T GRAPHIC, V35
   [Anonymous], 2018, ACM T GRAPHIC, V37
   [Anonymous], 2011, ACM T GRAPHIC, V30
   [Anonymous], 2018, ACM T GRAPHIC, V37
   [Anonymous], 2019, ACM T GRAPHIC, V38
   [Anonymous], 2015, ACM T GRAPHIC, V34
   [Anonymous], 2019, ACM T GRAPHIC, V38
   [Anonymous], 2017, ACM T GRAPHIC, V36
   [Anonymous], 2009, ACM T GRAPHIC, V28
   [Anonymous], 2012, ACM T GRAPHIC, V31
   [Anonymous], 2014, ACM T GRAPHIC, V33
   [Anonymous], 2017, ACM T GRAPHIC, V36
   [Anonymous], 2015, ACM T GRAPHIC, V34
   [Anonymous], 2019, ACM T GRAPHIC, V37
   [Anonymous], 2008, ACM T GRAPHIC, V27
   [Anonymous], 2019, ACM T GRAPHIC, V38
   [Anonymous], 2013, ACM T GRAPHIC, V32
   Arbree A, 2008, COMPUT GRAPH FORUM, V27, P507, DOI 10.1111/j.1467-8659.2008.01148.x
   Chandrasekhar S., 1950, RAD TRANSFER
   Christensen PerH., 2008, POINT BASED APPROXIM
   Deng H, 2020, COMPUT VIS MEDIA, V6, P37, DOI 10.1007/s41095-020-0160-1
   Durand F, 2005, ACM T GRAPHIC, V24, P1115, DOI 10.1145/1073204.1073320
   Fong J., 2017, ACM SIGGRAPH 2017 CO, DOI DOI 10.1145/3084873.3084907
   Frederickx R., 2015, P EG 2015
   Ge LS, 2021, IEEE T VIS COMPUT GR, V27, P3123, DOI 10.1109/TVCG.2019.2963015
   Ge LS, 2018, SIGGRAPH'18: ACM SIGGRAPH 2018 TALKS, DOI 10.1145/3214745.3214758
   Guo Jerry, 2018, EUROGRAPHICS S RENDE, V2018, P73
   Hanika J, 2015, COMPUT GRAPH FORUM, V34, P87, DOI 10.1111/cgf.12681
   Hasan M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618489
   Herholz S, 2016, COMPUT GRAPH FORUM, V35, P67, DOI 10.1111/cgf.12950
   Holzschuch N, 2015, COMPUT GRAPH FORUM, V34, P48, DOI 10.1111/cgf.12517
   Hua BS, 2019, COMPUT GRAPH FORUM, V38, P455, DOI 10.1111/cgf.13652
   Jarosz W, 2008, COMPUT GRAPH FORUM, V27, P557, DOI 10.1111/j.1467-8659.2008.01153.x
   Jarosz W, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899409
   Jensen H. W., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P311, DOI 10.1145/280814.280925
   Jensen HW, 2001, COMP GRAPH, P511, DOI 10.1145/383259.383319
   Kajiya J. T., 1984, Computers & Graphics, V18, P165
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Keller A., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P49, DOI 10.1145/258734.258769
   Koerner David., 2016, Eurographics Symposium on Rendering - Experimental Ideas Implementations, P91
   Krvanek J., 2014, P ACM SIGGRAPH 2014
   Lafortune E. P., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P91
   Larsen EW, 2011, J QUANT SPECTROSC RA, V112, P619, DOI 10.1016/j.jqsrt.2010.07.003
   Liang YL, 2020, IEEE T VIS COMPUT GR, V26, P2961, DOI 10.1109/TVCG.2019.2909875
   Meng J, 2016, COMPUT GRAPH FORUM, V35, P37, DOI 10.1111/cgf.12947
   Moon JT, 2007, P 18 EUR C REND TECH, P231, DOI [10.2312/EGWR/EGSR07/231-242., DOI 10.2312/EGWR/EGSR07/231-242]
   Müller T, 2017, COMPUT GRAPH FORUM, V36, P91, DOI 10.1111/cgf.13227
   Novak J., 2011, S INTERACTIVE 3D GRA, P119
   Novák J, 2018, COMPUT GRAPH FORUM, V37, P551, DOI 10.1111/cgf.13383
   Novák J, 2012, COMPUT GRAPH FORUM, V31, P1407, DOI 10.1111/j.1467-8659.2012.03136.x
   Ou JW, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024213
   Pauly M, 2000, SPRING COMP SCI, P11
   Sun B, 2005, ACM T GRAPHIC, V24, P1040, DOI 10.1145/1073204.1073309
   Walter B, 2005, ACM T GRAPHIC, V24, P1098, DOI 10.1145/1073204.1073318
   Walter B, 2006, ACM T GRAPHIC, V25, P1081, DOI 10.1145/1141911.1141997
   Wang B., 2016, PROC EUROGRAPHICS S
   Wang BB, 2020, IEEE T VIS COMPUT GR, V26, P2456, DOI 10.1109/TVCG.2018.2890466
   Wang BB, 2018, IEEE T VIS COMPUT GR, V24, P2743, DOI 10.1109/TVCG.2017.2768525
   Weber P, 2017, COMPUT GRAPH FORUM, V36, P21, DOI 10.1111/cgf.13103
   Xu ZL, 2020, COMPUT GRAPH FORUM, V39, P193, DOI 10.1111/cgf.14137
   Zheng Q, 2019, COMPUT GRAPH FORUM, V38, P169, DOI 10.1111/cgf.13628
NR 74
TC 2
Z9 2
U1 1
U2 45
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2022
VL 8
IS 2
BP 177
EP 198
DI 10.1007/s41095-021-0249-1
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XJ0YS
UT WOS:000726525400001
OA gold
DA 2024-07-18
ER

PT J
AU Le, TT
   Almansa, A
   Gousseau, Y
   Masnou, S
AF Le, Thuc Trinh
   Almansa, Andres
   Gousseau, Yann
   Masnou, Simon
TI Object removal from complex videos using a few annotations
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE object removal; object segmentation; object tracking; video inpainting;
   video completion
ID SEGMENTATION; TRACKING; STABILIZATION
AB We present a system for the removal of objects from videos. As input, the system only needs a user to draw a few strokes on the first frame, roughly delimiting the objects to be removed. To the best of our knowledge, this is the first system allowing the semi-automatic removal of objects from videos with complex backgrounds. The key steps of our system are the following: after initialization, segmentation masks are first refined and then automatically propagated through the video. Missing regions are then synthesized using video inpainting techniques. Our system can deal with multiple, possibly crossing objects, with complex motions, and with dynamic textures. This results in a computational tool that can alleviate tedious manual operations for editing high-quality videos.
C1 [Le, Thuc Trinh; Gousseau, Yann] Univ Paris Saclay, Telecom ParisTech, LTCI, F-75013 Paris, France.
   [Almansa, Andres] CNRS, MAP5, F-75006 Paris, France.
   [Almansa, Andres] Univ Paris 05, F-75006 Paris, France.
   [Masnou, Simon] Univ Claude Bernard Lyon 1, Univ Lyon, Inst Camille Jordan, CNRS,UMR 5208, F-69622 Villeurbanne, France.
C3 IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; Telecom
   Paris; Universite Paris Saclay; Universite Paris Cite; Centre National
   de la Recherche Scientifique (CNRS); Universite Paris Cite; Universite
   Paris Cite; Centre National de la Recherche Scientifique (CNRS); CNRS -
   National Institute for Mathematical Sciences (INSMI); Ecole Centrale de
   Lyon; Institut National des Sciences Appliquees de Lyon - INSA Lyon;
   Universite Claude Bernard Lyon 1; Universite Jean Monnet
RP Le, TT (corresponding author), Univ Paris Saclay, Telecom ParisTech, LTCI, F-75013 Paris, France.
EM thuc.le@telecomparistech.fr; andres.almansa@parisdescartes.fr;
   yann.gousseau@telecom-paristech.fr; masnou@math.univ-lyon1.fr
RI Almansa, Andrés/A-4152-2008
OI Almansa, Andrés/0000-0001-8196-1329
FU French Research Agency (ANR) [ANR-14-CE27-001]
FX We gratefully acknowledge the support of NVIDIA who donated a Titan Xp
   GPU used for this research. This work was funded by the French Research
   Agency (ANR) under Grant No. ANR-14-CE27-001 (MIRIAM).
CR ACM, 2009, T GRAPHICS, V28, P3
   [Anonymous], 2017, 2017 DAVIS CHALLENGE
   [Anonymous], 2018, 2018 DAVIS CHALLENGE
   Bay H., 2008, COMPUT VIS IMAGE UND, V10, P346, DOI DOI 10.1016/j.cviu.2007.09.014
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Bokov A, 2018, IEEE IMAGE PROC, P2122, DOI 10.1109/ICIP.2018.8451683
   Bonneel N, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661253
   Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565
   Caelles S, 2017, SEMANTICALLY GUIDED
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen YH, 2018, PROC CVPR IEEE, P1189, DOI 10.1109/CVPR.2018.00130
   Cheng JC, 2018, PROC CVPR IEEE, P7415, DOI 10.1109/CVPR.2018.00774
   Chiu WC, 2013, PROC CVPR IEEE, P321, DOI 10.1109/CVPR.2013.48
   Choi S, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P1897, DOI 10.1109/IROS.2009.5354240
   Chuang YY, 2002, ACM T GRAPHIC, V21, P243, DOI 10.1145/566570.566572
   Colombari A, 2007, PATTERN RECOGN, V40, P1307, DOI 10.1016/j.patcog.2006.07.008
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343
   Dehghan A, 2015, PROC CVPR IEEE, P4091, DOI 10.1109/CVPR.2015.7299036
   Drayer B., 2016, OBJECT DETECTION TRA
   Drori I, 2003, ACM T GRAPHIC, V22, P303, DOI 10.1145/882262.882267
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Granados M, 2012, COMPUT GRAPH FORUM, V31, P219, DOI 10.1111/j.1467-8659.2012.03000.x
   Granados M, 2012, LECT NOTES COMPUT SC, V7572, P682, DOI 10.1007/978-3-642-33718-5_49
   Grossauer H, 2006, MATH IND, V10, P151, DOI [10.1007/978-3-540-34767-5_7, DOI 10.1007/978-3-540-34767-5_7]
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   Herling J, 2012, INT SYM MIX AUGMENT, P141, DOI 10.1109/ISMAR.2012.6402551
   Hu Yuan-Ting, 2017, NIPS, P324
   Vo HV, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1948, DOI 10.1145/3240508.3240678
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Jain K., 2016, AAAI HUMAN COMPUTATI, V4, P89
   Jia JY, 2006, IEEE T PATTERN ANAL, V28, P832, DOI 10.1109/TPAMI.2006.108
   Khoreva A, 2017, P DAV CHALL VID OBJ
   Korman S, 2016, IEEE T PATTERN ANAL, V38, P1099, DOI 10.1109/TPAMI.2015.2477814
   Lee YJ, 2011, IEEE I CONF COMP VIS, P1995, DOI 10.1109/ICCV.2011.6126471
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Levinkov E., 2016, P 24 PAC C COMP GRAP, P33
   Li FX, 2013, IEEE I CONF COMP VIS, P2192, DOI 10.1109/ICCV.2013.273
   Li X, 2017, P DAV CHALL VID OBJ
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luiten J, 2019, LECT NOTES COMPUT SC, V11364, P565, DOI 10.1007/978-3-030-20870-7_35
   Märki N, 2016, PROC CVPR IEEE, P743, DOI 10.1109/CVPR.2016.87
   Masnou S, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P259, DOI 10.1109/ICIP.1998.999016
   Matsushita Y, 2006, IEEE T PATTERN ANAL, V28, P1150, DOI 10.1109/TPAMI.2006.141
   MEYER F, 1994, SIGNAL PROCESS, V38, P113, DOI 10.1016/0165-1684(94)90060-4
   Nagaraja NS, 2015, IEEE I CONF COMP VIS, P3235, DOI 10.1109/ICCV.2015.370
   Newson A, 2017, IMAGE PROCESS ON LIN, V7, P373, DOI 10.5201/ipol.2017.189
   Newson A, 2014, SIAM J IMAGING SCI, V7, P1993, DOI 10.1137/140954933
   ODOBEZ JM, 1995, J VIS COMMUN IMAGE R, V6, P348, DOI 10.1006/jvci.1995.1029
   Oh SW, 2018, PROC CVPR IEEE, P7376, DOI 10.1109/CVPR.2018.00770
   Papazoglou A, 2013, IEEE I CONF COMP VIS, P1777, DOI 10.1109/ICCV.2013.223
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Patwardhan K., 2005, P IEEE INT C IMAGE P, V2, P69
   Patwardhan KA, 2007, IEEE T IMAGE PROCESS, V16, P545, DOI 10.1109/TIP.2006.888343
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Perazzi F, 2017, PROC CVPR IEEE, P3491, DOI 10.1109/CVPR.2017.372
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Ramakanth SA, 2014, PROC CVPR IEEE, P376, DOI 10.1109/CVPR.2014.55
   Ramakanth SA, 2014, IEEE T IMAGE PROCESS, V23, P2193, DOI 10.1109/TIP.2014.2309436
   Sánchez J, 2017, IMAGE PROCESS ON LIN, V7, P309, DOI 10.5201/ipol.2017.209
   Seguin G, 2016, PROC CVPR IEEE, P3678, DOI 10.1109/CVPR.2016.400
   Shiratori T., 2006, Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition, P411
   Tang NC, 2011, IEEE T MULTIMEDIA, V13, P602, DOI 10.1109/TMM.2011.2112642
   Le TT, 2018, SA'18: SIGGRAPH ASIA 2018 TECHNICAL BRIEFS, DOI 10.1145/3283254.3283276
   Le TT, 2017, IEEE IMAGE PROC, P4587, DOI 10.1109/ICIP.2017.8297152
   Tokmakov P, 2017, PROC CVPR IEEE, P531, DOI 10.1109/CVPR.2017.64
   Tsai YH, 2016, PROC CVPR IEEE, P3899, DOI 10.1109/CVPR.2016.423
   Voigtlaender P, 2017, P DAV CHALL VID OBJ
   Wang S, 2011, IEEE I CONF COMP VIS, P1323, DOI 10.1109/ICCV.2011.6126385
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
   Xie SN, 2017, INT J COMPUT VISION, V125, P3, DOI 10.1007/s11263-017-1004-z
   Xu BB, 2017, IEEE ROBOT AUTOM LET, V2, P2032, DOI 10.1109/LRA.2017.2718106
   Xu Ning., 2019, Deep grabcut for object selection, V1, P1, DOI [DOI 10.5244/C.31.182, 10.5244/C.31.182]
   Xu Ning, 2018, YOUTUBE VOS LARGE SC
   Yang HX, 2011, NEUROCOMPUTING, V74, P3823, DOI 10.1016/j.neucom.2011.07.024
   Yang LJ, 2018, PROC CVPR IEEE, P6499, DOI 10.1109/CVPR.2018.00680
   Yang MY, 2015, LECT NOTES COMPUT SC, V9474, P198, DOI 10.1007/978-3-319-27857-5_18
   Yang YC, 2015, IEEE I CONF COMP VIS, P4408, DOI 10.1109/ICCV.2015.501
   You S., 2013, P 2013 IAPR INT C MA, P181
   Zamir AR, 2012, LECT NOTES COMPUT SC, V7573, P343, DOI 10.1007/978-3-642-33709-3_25
NR 82
TC 10
Z9 11
U1 0
U2 6
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2019
VL 5
IS 3
BP 267
EP 291
DI 10.1007/s41095-019-0145-0
PG 25
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UN
UT WOS:000648690900004
OA Green Accepted, gold
DA 2024-07-18
ER

PT J
AU Guo, J
   Hu, BY
   Chen, YJ
   Li, YQ
   Guo, YW
   Yan, LQ
AF Guo, Jie
   Hu, Bingyang
   Chen, Yanjun
   Li, Yuanqi
   Guo, Yanwen
   Yan, Ling-Qi
TI Rendering discrete participating media using geometrical optics
   approximation
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE light scattering; geometrical optics approximation (GOA); discrete
   participating media; volume rendering
ID LIGHT-SCATTERING; APPEARANCE MODEL; TRANSPORT
AB We consider the scattering of light in participating media composed of sparsely and randomly distributed discrete particles. The particle size is expected to range from the scale of the wavelength to several orders of magnitude greater, resulting in an appearance with distinct graininess as opposed to the smooth appearance of continuous media. One fundamental issue in the physically-based synthesis of such appearance is to determine the necessary optical properties in every local region. Since these properties vary spatially, we resort to geometrical optics approximation (GOA), a highly efficient alternative to rigorous Lorenz-Mie theory, to quantitatively represent the scattering of a single particle. This enables us to quickly compute bulk optical properties for any particle size distribution. We then use a practical Monte Carlo rendering solution to solve energy transfer in the discrete participating media. Our proposed framework is the first to simulate a wide range of discrete participating media with different levels of graininess, converging to the continuous media case as the particle concentration increases.
C1 [Guo, Jie; Hu, Bingyang; Chen, Yanjun; Li, Yuanqi; Guo, Yanwen] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.
   [Yan, Ling-Qi] UC Santa Barbara, Dept Comp Sci, Santa Barbara, CA USA.
C3 Nanjing University; University of California System; University of
   California Santa Barbara
RP Guo, YW (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.
EM guojie@nju.edu.cn; fhymyang@gmail.com; cujooyer@gmail.com;
   dz1833015@smail.nju.edu.cn; ywguo@nju.edu.cn; lingqi@cs.ucsb.edu
RI Zhang, Can/JUU-9511-2023
FU National Natural Science Foundation of China [61972194, 62032011]
FX We would like to thank the reviewers for their valuable suggestions.
   This work was supported by National Natural Science Foundation of China
   (Grant Nos. 61972194 and 62032011).
CR Aliaga C, 2017, COMPUT GRAPH FORUM, V36, P35, DOI 10.1111/cgf.13222
   Amanatides J., 1987, EUROGRAPHICS, P3, DOI DOI 10.2312/EGTP.19871000
   Arvo J., 1993, GLOBAL ILLUMINATION
   Bitterli B, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275103
   Bitterli B, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073698
   Bohren CF., 1983, ABSORPTION SCATTERIN
   Bosch C, 2004, COMPUT GRAPH FORUM, V23, P361, DOI 10.1111/j.1467-8659.2004.00767.x
   CACHORRO VE, 1991, J ELECTROMAGNET WAVE, V5, P913, DOI 10.1163/156939391X00950
   Callet P, 1996, COMPUT GRAPH FORUM, V15, P119, DOI 10.1111/1467-8659.1520119
   Cerezo E, 2005, VISUAL COMPUT, V21, P303, DOI 10.1007/s00371-005-0287-1
   Chandrasekhar S., 1950, RAD TRANSFER
   d'Eon E, 2019, J COMPUT THEOR TRANS, V48, P201, DOI 10.1080/23324309.2019.1677717
   d'Eon E, 2018, J COMPUT THEOR TRANS, V47, P84, DOI 10.1080/23324309.2018.1481433
   Dal Corso A., 2016, WORKSH MAT APP MOD E
   Davidovic T, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2602144
   Deng X, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323041
   DOBBINS RA, 1977, APPL OPTICS, V16, P281, DOI 10.1364/AO.16.000281
   Dupuy J., 2016, EUROGRAPHICS S RENDE
   Frisvad JR, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276452
   Gkioulekas I, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508377
   GLANTSCHNIG WJ, 1981, APPL OPTICS, V20, P2499, DOI 10.1364/AO.20.002499
   Guo J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323031
   Guo J, 2018, COMPUT GRAPH FORUM, V37, P67, DOI 10.1111/cgf.13476
   Hachisuka T., 2013, P SIGGRAPH AS 2013 C
   Hawkins T, 2005, ACM T GRAPHIC, V24, P812, DOI 10.1145/1073204.1073266
   He HL, 2012, J QUANT SPECTROSC RA, V113, P1467, DOI 10.1016/j.jqsrt.2012.03.011
   Heitz E, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766988
   Henyey LG, 1941, ASTROPHYS J, V93, P70, DOI 10.1086/144246
   HOVENAC EA, 1991, APPL OPTICS, V30, P4739, DOI 10.1364/AO.30.004739
   Jackel D, 1997, COMPUT GRAPH FORUM, V16, P201, DOI 10.1111/1467-8659.00180
   Jakob W, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601186
   Jakob W, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778790
   Jakob Wenzel, 2010, Mitsuba renderer
   Jarabo A, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201282
   Jarosz W, 2008, COMPUT GRAPH FORUM, V27, P557, DOI 10.1111/j.1467-8659.2008.01153.x
   Jarosz W, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899409
   Jensen H. W., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P311, DOI 10.1145/280814.280925
   Khungurn P, 2015, ACM T GRAPHIC, V35, DOI 10.1145/2818648
   Kuznetsov A, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356525
   Lafortune E. P., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P91
   Laven P, 2003, APPL OPTICS, V42, P436, DOI 10.1364/AO.42.000436
   Lorenz L., 1890, KONGELIGE DANSKE VID, V6, P2
   Loubet G, 2018, COMPUT GRAPH FORUM, V37, P111, DOI 10.1111/cgf.13346
   Lu YF, 2019, J QUANT SPECTROSC RA, V228, P90, DOI 10.1016/j.jqsrt.2019.02.028
   Meng J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766949
   Merillou S, 2001, VISUAL COMPUT, V17, P30, DOI 10.1007/s003710000093
   Mie G, 1908, ANN PHYS-BERLIN, V25, P377, DOI 10.1002/andp.19083300302
   Miller B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323025
   Moon JT, 2007, P 18 EUR C REND TECH, P231, DOI [10.2312/EGWR/EGSR07/231-242., DOI 10.2312/EGWR/EGSR07/231-242]
   Müller T, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982429
   Nishita T, 2001, COMPUTER GRAPHICS INTERNATIONAL 2001, PROCEEDINGS, P149, DOI 10.1109/CGI.2001.934669
   Novák J, 2018, COMPUT GRAPH FORUM, V37, P551, DOI 10.1111/cgf.13383
   Pauly M, 2000, SPRING COMP SCI, P11
   Raymond B, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925945
   RILEY K, 2004, P EUR S REND 2004 JU, P375
   Rushmeier H., 2008, PROC ACM SIGGRAPH 20
   Sadeghi I, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2077341.2077344
   Szirmay-Kalos L, 2018, COMPUT GRAPH FORUM, V37, P63, DOI 10.1111/cgf.13342
   Toublanc D, 1996, APPL OPTICS, V35, P3270, DOI 10.1364/AO.35.003270
   UNGUT A, 1981, APPL OPTICS, V20, P2911, DOI 10.1364/AO.20.002911
   van de Hulst H.C, 1981, LIGHT SCATTERING SMA
   Veach E., 1997, Robust Monte Carlo Methods for Light Transport Simulation
   Veach E., 1995, P 22 ANN C COMP GRAP, P419, DOI [DOI 10.1145/218380.218498, 10.1145/218380.218498]
   Wald I, 2006, ACM T GRAPHIC, V25, P485, DOI 10.1145/1141911.1141913
   Wang BB, 2018, COMPUT GRAPH FORUM, V37, P55, DOI 10.1111/cgf.13547
   Werner S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130840
   Wu L, 2007, J QUANT SPECTROSC RA, V108, P54, DOI 10.1016/j.jqsrt.2007.03.008
   Yan LQ, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201351
   Yan LQ, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925915
   Yan LQ, 2015, ACM T GRAPHIC, V35, DOI 10.1145/2816814
   Yu HT, 2008, PARTICUOLOGY, V6, P340, DOI 10.1016/j.partic.2008.07.003
   Yu HT, 2009, J QUANT SPECTROSC RA, V110, P1178, DOI 10.1016/j.jqsrt.2009.03.025
   Zhao S., 2016, ACM T GRAPHIC, V35, P1, DOI [DOI 10.1145/2897824.2925932, 10.1145/2897824.2925932]
   Zhao S, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980228
   Zhao S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185571
   Zhao S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964939
   Zhou XB, 2003, APPL OPTICS, V42, P4295, DOI 10.1364/AO.42.004295
NR 77
TC 3
Z9 4
U1 1
U2 11
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2022
VL 8
IS 3
BP 425
EP 444
DI 10.1007/s41095-021-0253-5
EA APR 2022
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0Q6DG
UT WOS:000777413400001
OA gold
DA 2024-07-18
ER

PT J
AU Zin, T
   Nakahara, Y
   Yamaguchi, T
   Ikehara, M
AF Zin, Theingi
   Nakahara, Yusuke
   Yamaguchi, Takuro
   Ikehara, Masaaki
TI Improved image denoising via RAISR with fewer filters
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE block matching and 3D filtering; weighted nuclear norm minimization;
   super-resolution; geometric conversion; census transform
AB In recent years, accurate Gaussian noise removal has attracted considerable attention for mobile applications, as in smart phones. Accurate conventional denoising methods have the potential ability to improve denoising performance with no additional time. Therefore, we propose a rapid post-processing method for Gaussian noise removal in this paper. Block matching and 3D filtering and weighted nuclear norm minimization are utilized to suppress noise. Although these nonlocal image denoising methods have quantitatively high performance, some fine image details are lacking due to the loss of high frequency information. To tackle this problem, an improvement to the pioneering RAISR approach (rapid and accurate image super-resolution), is applied to rapidly post-process the denoised image. It gives performance comparable to state-of-the-art super-resolution techniques at low computational cost, preserving important image structures well. Our modification is to reduce the hash classes for the patches extracted from the denoised image and the pixels from the ground truth to 18 filters by two improvements: geometric conversion and reduction of the strength classes. In addition, following RAISR, the census transform is exploited by blending the image processed by noise removal methods with the filtered one to achieve artifact-free results. Experimental results demonstrate that higher quality and more pleasant visual results can be achieved than by other methods, efficiently and with low memory requirements.
C1 [Zin, Theingi; Nakahara, Yusuke; Yamaguchi, Takuro; Ikehara, Masaaki] Keio Univ, Dept Elect & Elect Engn, Yokohama, Kanagawa 2238522, Japan.
C3 Keio University
RP Ikehara, M (corresponding author), Keio Univ, Dept Elect & Elect Engn, Yokohama, Kanagawa 2238522, Japan.
EM theingizinec@keio.jp; nakahara@tkhm.elec.keio.ac.jp;
   yamaguchi@tkhm.elec.keio.ac.jp; ikehara@tkhm.elec.keio.ac.jp
RI Ikehara, Masaaki/F-7675-2014
CR Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Feng XG, 2002, CONF REC ASILOMAR C, P478
   Gonzalez R. C., 2006, PEARSON ED INDIA, V3rd
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   Jeong SC, 2010, EUR SIGNAL PR CONF, P1791
   Papyan V, 2016, IEEE T IMAGE PROCESS, V25, P249, DOI 10.1109/TIP.2015.2499698
   Pitas I., 1990, NONLINEAR DIGITALFIL
   Romano Y, 2017, IEEE T COMPUT IMAG, V3, P110, DOI 10.1109/TCI.2016.2629284
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Yang CY, 2013, IEEE I CONF COMP VIS, P561, DOI 10.1109/ICCV.2013.75
   YANG RK, 1995, IEEE T SIGNAL PROCES, V43, P591, DOI 10.1109/78.370615
   Yu GS, 2012, IEEE T IMAGE PROCESS, V21, P2481, DOI 10.1109/TIP.2011.2176743
   Zabih R., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P151, DOI 10.1007/BFb0028345
NR 18
TC 2
Z9 2
U1 3
U2 10
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2021
VL 7
IS 4
BP 499
EP 511
DI 10.1007/s41095-021-0213-0
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UK7UX
UT WOS:000692172600006
OA gold
DA 2024-07-18
ER

PT J
AU Liu, HC
   Mu, TJ
   Huang, XL
AF Liu, Hanchao
   Mu, Tai-Jiang
   Huang, Xiaolei
TI Detecting human-object interaction with multi-level pairwise feature
   network
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE human-object interaction detection; pairwise feature network; deep
   learning; multi-level; object instance
AB Human-object interaction (HOI) detection is crucial for human-centric image understanding which aims to infer human, action, object triplets within an image. Recent studies often exploit visual features and the spatial configuration of a human-object pair in order to learn the action linking the human and object in the pair. We argue that such a paradigm of pairwise feature extraction and action inference can be applied not only at the whole human and object instance level, but also at the part level at which a body part interacts with an object, and at the semantic level by considering the semantic label of an object along with human appearance and human-object spatial configuration, to infer the action. We thus propose a multi-level pairwise feature network (PFNet) for detecting human-object interactions. The network consists of three parallel streams to characterize HOI utilizing pairwise features at the above three levels; the three streams are finally fused to give the action prediction. Extensive experiments show that our proposed PFNet outperforms other state-of-the-art methods on the V-COCO dataset and achieves comparable results to the state-of-the-art on the HICO-DET dataset.
C1 [Liu, Hanchao; Mu, Tai-Jiang] Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Key Lab Pervas Comp,Minist Educ, Beijing 100084, Peoples R China.
   [Huang, Xiaolei] Penn State Univ, Coll Informat Sci & Technol, University Pk, PA 16802 USA.
C3 Tsinghua University; Pennsylvania Commonwealth System of Higher
   Education (PCSHE); Pennsylvania State University; Pennsylvania State
   University - University Park
RP Mu, TJ (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Key Lab Pervas Comp,Minist Educ, Beijing 100084, Peoples R China.
EM liuhc17@mails.tsinghua.edu.cn; taijiang@tsinghua.edu.cn; suh972@psu.edu
RI Huang, Xiaolei/AAE-7238-2019; Mu, Tai-Jiang/JWO-1381-2024
OI Mu, Tai-Jiang/0000-0002-9197-346X; Huang, Sharon
   Xiaolei/0000-0003-2338-6535
FU National Natural Science Foundation of China [61902210]; Research Grant
   of Beijing Higher Institution Engineering Research Center;
   Tsinghua-Tencent Joint Laboratory for Internet Innovation Technology
FX We thank the reviewers for their constructive comments. This work was
   supported by the National Natural Science Foundation of China (Project
   No. 61902210), a Research Grant of Beijing Higher Institution
   Engineering Research Center, and the Tsinghua-Tencent Joint Laboratory
   for Internet Innovation Technology.
CR [Anonymous], 2017, ARXIV170205448
   [Anonymous], 2016, ARXIV161200137
   [Anonymous], 2017, ARXIV170407333
   [Anonymous], 2016, COMPUT VISUAL MEDIA
   Bansal A, 2020, AAAI CONF ARTIF INTE, V34, P10460
   Bingjie Xu, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P2019, DOI 10.1109/CVPR.2019.00212
   Borji A, 2019, COMPUT VIS MEDIA, V5, P117, DOI 10.1007/s41095-019-0149-9
   Fang HS, 2018, LECT NOTES COMPUT SC, V11214, P52, DOI 10.1007/978-3-030-01249-6_4
   Gao Chen, 2018, ARXIV180810437
   Girdhar Rohit, 2017, ARXIV171101467
   Girshick Ross, 2018, Detectron
   Gupta Saurabh, 2015, ARXIV150504474, P5
   Gupta T, 2019, IEEE I CONF COMP VIS, P9676, DOI 10.1109/ICCV.2019.00977
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Kato K, 2018, LECT NOTES COMPUT SC, V11218, P247, DOI 10.1007/978-3-030-01264-9_15
   King DB, 2015, ACS SYM SER, V1214, P1
   Li Y. L., 2020, ARXIV200400945
   Li Y. L., 2020, ARXIV200408154
   Li Y. L., 2019, ARXIV188108264
   Liao Y, 2020, PROC CVPR IEEE, P479, DOI 10.1109/CVPR42600.2020.00056
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Luvizon DC, 2018, PROC CVPR IEEE, P5137, DOI 10.1109/CVPR.2018.00539
   Ma CY, 2018, PROC CVPR IEEE, P6790, DOI 10.1109/CVPR.2018.00710
   Mallya A, 2016, LECT NOTES COMPUT SC, V9905, P414, DOI 10.1007/978-3-319-46448-0_25
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Peyre J, 2019, IEEE I CONF COMP VIS, P1981, DOI 10.1109/ICCV.2019.00207
   Qi SY, 2018, LECT NOTES COMPUT SC, V11213, P407, DOI 10.1007/978-3-030-01240-3_25
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shen LY, 2018, IEEE WINT CONF APPL, P1568, DOI 10.1109/WACV.2018.00181
   Ulutan O., 2020, CVPR, P13617
   Wan B, 2019, IEEE I CONF COMP VIS, P9468, DOI 10.1109/ICCV.2019.00956
   Wang TC, 2020, PROC CVPR IEEE, P4115, DOI 10.1109/CVPR42600.2020.00417
   Wang TC, 2019, IEEE I CONF COMP VIS, P5693, DOI 10.1109/ICCV.2019.00579
   Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330
   Zhao ZC, 2017, IEEE I CONF COMP VIS, P3411, DOI 10.1109/ICCV.2017.367
   Zhou PH, 2019, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2019.00093
   Zhou TF, 2020, PROC CVPR IEEE, P4262, DOI 10.1109/CVPR42600.2020.00432
NR 39
TC 13
Z9 18
U1 1
U2 9
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2021
VL 7
IS 2
BP 229
EP 239
DI 10.1007/s41095-020-0188-2
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FR
UT WOS:000648692900006
OA gold
DA 2024-07-18
ER

PT J
AU Zhao, ZX
   Wu, J
   Wang, LL
AF Zhao, Zixiang
   Wu, Jian
   Wang, Lili
TI AR assistance for efficient dynamic target search
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE augmented reality (AR); search; guidance
ID AUGMENTED REALITY; OPTIMIZATION; SYSTEMS; VISION
AB When searching for a dynamic target in an unknown real world scene, search efficiency is greatly reduced if users lack information about the spatial structure of the scene. Most target search studies, especially in robotics, focus on determining either the shortest path when the target's position is known, or a strategy to find the target as quickly as possible when the target's position is unknown. However, the target's position is often known intermittently in the real world, e.g., in the case of using surveillance cameras. Our goal is to help user find a dynamic target efficiently in the real world when the target's position is intermittently known. In order to achieve this purpose, we have designed an AR guidance assistance system to provide optimal current directional guidance to users, based on searching a prediction graph. We assume that a certain number of depth cameras are fixed in a real scene to obtain dynamic target's position. The system automatically analyzes all possible meetings between the user and the target, and generates optimal directional guidance to help the user catch up with the target. A user study was used to evaluate our method, and its results showed that compared to free search and a top-view method, our method significantly improves target search efficiency.
C1 [Zhao, Zixiang; Wu, Jian; Wang, Lili] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
C3 Beihang University
RP Wang, LL (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
EM zhaozixiang@buaa.edu.cn; lanayawj@buaa.edu.cn; wanglily@buaa.edu.cn
RI wang, lili/HJP-8047-2023
FU National Key R&D Program of China [2019YFC1521102]; National Natural
   Science Foundation of China [61932003, 61772051]
FX We sincerely thank the reviewers for their their constructive
   suggestions and comments. This work was supported by National Key R&D
   Program of China (2019YFC1521102) and the National Natural Science
   Foundation of China (61932003 and 61772051).
CR Al Delail B, 2012, 2012 IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE AND INTELLIGENT AGENT TECHNOLOGY WORKSHOPS (WI-IAT WORKSHOPS 2012), VOL 3, P286, DOI 10.1109/WI-IAT.2012.99
   Alnabhan A, 2014, P 6 ACM SIGSPATIAL I, P36
   [Anonymous], 2020, KIN DK DOC
   [Anonymous], 1987, Art gallery theorems and algorithms
   Barakonyi Istvan, 2006, 2006 IEEE/ACM International Symposium on Mixed and Augmented Reality, P145, DOI 10.1109/ISMAR.2006.297806
   Bhattacharya S, 2010, SPRINGER TRAC ADV RO, V57, P251
   Biocca F., 2006, Conference on Human Factors in Computing Systems. CHI2006, P1115
   Chittaro L., 2003, PROC WEB 3D, DOI [10.1145/636593.636598, DOI 10.1145/636593.636598]
   Darken R. P., 1993, Sixth Annual Symposium on User Interface Software and Technology. Proceedings of the ACM Symposium on User Interface Software and Technology, P157, DOI 10.1145/168642.168658
   Darken RP, 2002, HUM FAC ER, P493
   Darken RP, 1999, P IEEE VIRT REAL ANN, P133, DOI 10.1109/VR.1999.756944
   Elmqvist N, 2007, VRST 2007: ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, PROCEEDINGS, P207
   Foraker J, 2016, J OPTIMIZ THEORY APP, V169, P530, DOI 10.1007/s10957-015-0768-y
   Fukuju Y, 2003, WSTFES 2003: IEEE WORKSHOP ON SOFTWARE TECHNOLOGIES FOR FUTURE EMBEDDED SYSTEMS, PROCEEDINGS, P53, DOI 10.1109/WSTFES.2003.1201360
   Grammenos D., 2002, Virtual Environments 2002. Eurographics Workshop Proceedings, P131
   Hart S. G., 2006, P HUM FACT ERG SOC A, V50, P904, DOI DOI 10.1177/154193120605000909
   He YH, 2020, IEEE T IMAGE PROCESS, V29, P5191, DOI 10.1109/TIP.2020.2980070
   Henderson S, 2011, IEEE T VIS COMPUT GR, V17, P1355, DOI 10.1109/TVCG.2010.245
   Hu JW, 2014, SENSORS-BASEL, V14, P9408, DOI 10.3390/s140609408
   Hu JW, 2012, I C CONT AUTOMAT ROB, P895, DOI 10.1109/ICARCV.2012.6485276
   Huang JH, 2021, COMPUT VIS MEDIA, V7, P87, DOI 10.1007/s41095-020-0195-3
   Hui Peng, 2015, 2015 27th Chinese Control and Decision Conference (CCDC), P4855, DOI 10.1109/CCDC.2015.7162793
   Kasprzak S, 2013, NINTH INTERNATIONAL CONFERENCE ON INTELLIGENT ENVIRONMENTS (IE 2013), P100, DOI 10.1109/IE.2013.51
   Kim J, 2008, IEEE T CONSUM ELECTR, V54, P954, DOI 10.1109/TCE.2008.4637573
   Koopman B. O., 1980, SEARCH SCREENING GEN
   Kratzke T. M., 2010, Proceedings of the 13th International Conference on Information Fusion (FUSION 2010)
   Kraus M, 2020, INT SYM MIX AUGMENT, P227, DOI 10.1109/ISMAR50242.2020.00046
   Lau H, 2008, EUR J OPER RES, V190, P383, DOI 10.1016/j.ejor.2007.06.043
   LaViola Joseph J., 2017, 3D User interfaces: theory and practice
   Li P, 2019, IEEE COMPUT SOC CONF, P1506, DOI 10.1109/CVPRW.2019.00192
   Liu MY, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (ROBIO), P2339, DOI 10.1109/ROBIO.2013.6739819
   Lynch Kevin., 1984, CITIES MIND IMAGES T, P151, DOI [10.1007/978-1-4757-9697-1_9, DOI 10.1007/978-1-4757-9697-1_9, 10.1007/978-1-4757-9697-1_9.]
   Martins G.H. A., 1993, A new branch-and-bound procedure for computing optimal search paths
   Meng W, 2017, IEEE T CONTR SYST T, V25, P1480, DOI 10.1109/TCST.2016.2601287
   Minami M, 2004, LECT NOTES COMPUT SC, V3205, P347
   Morin M, 2009, FUSION: 2009 12TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION, VOLS 1-4, P2217
   Mulloni Alessandro, 2011, P 13 INT C HUM COMP, P211
   Nassani A., 2015, P SOC PHOTO-OPT INS, P1
   Neumann U, 1998, P IEEE VIRT REAL ANN, P4, DOI 10.1109/VRAIS.1998.658416
   Perez-Carabaza S, 2017, PROCEEDINGS OF THE 2017 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE (GECCO'17), P35, DOI 10.1145/3071178.3071299
   Pierce JS, 2004, P IEEE VIRT REAL ANN, P173, DOI 10.1109/VR.2004.1310071
   Rehman U, 2017, IEEE T HUM-MACH SYST, V47, P140, DOI 10.1109/THMS.2016.2620106
   Rey D., 2011, Wilcoxon-Signed-Rank Test, P1658, DOI [DOI 10.1007/978-3-642-04898-2616, 10.1007/978-3-642-04898-2616]
   Royset JO, 2010, NAV RES LOG, V57, P701, DOI 10.1002/nav.20432
   Rubino I., 2014, RECENT ADV ELECTR EL, P295
   Sato H., 2008, Path Optimization for Single and Multiple Searchers: Models and Algorithms
   Sato H, 2010, NAV RES LOG, V57, P422, DOI 10.1002/nav.20411
   Sawilowsky SS, 2009, J MOD APPL STAT METH, V8, P597, DOI 10.22237/jmasm/1257035100
   Schwerdtfeger B, 2011, VIRTUAL REAL-LONDON, V15, P213, DOI 10.1007/s10055-011-0187-9
   SHAPIRO SS, 1965, BIOMETRIKA, V52, P591, DOI 10.1093/biomet/52.3-4.591
   Steck SD, 2000, PRESENCE-TELEOP VIRT, V9, P69, DOI 10.1162/105474600566628
   Stoakley R., 1995, Human Factors in Computing Systems. CHI'95 Conference Proceedings, P265
   Subakti H, 2016, PROCEEDINGS OF 2016 IEEE 18TH INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING AND COMMUNICATIONS; IEEE 14TH INTERNATIONAL CONFERENCE ON SMART CITY; IEEE 2ND INTERNATIONAL CONFERENCE ON DATA SCIENCE AND SYSTEMS (HPCC/SMARTCITY/DSS), P1373, DOI [10.1109/HPCC-SmartCity-DSS.2016.0194, 10.1109/HPCC-SmartCity-DSS.2016.42]
   Syberfeldt A, 2015, PROCEDIA MANUF, V1, P98, DOI 10.1016/j.promfg.2015.09.068
   van Diggelen F, 2009, ARTECH HSE GNSS TECH, P1
   Wang M, 2020, COMPUT VIS MEDIA, V6, P3, DOI 10.1007/s41095-020-0162-z
   WANT R, 1992, ACM T INFORM SYST, V10, P91, DOI 10.1145/128756.128759
   WASSERTHEIL S, 1970, BIOMETRICS, V26, P588, DOI 10.2307/2529115
   Webel S, 2013, ROBOT AUTON SYST, V61, P398, DOI 10.1016/j.robot.2012.09.013
   Zauner J, 2003, SECOND IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P237, DOI 10.1109/ISMAR.2003.1240707
   Zeng R, 2020, COMPUT VIS MEDIA, V6, P225, DOI 10.1007/s41095-020-0179-3
NR 61
TC 0
Z9 0
U1 4
U2 15
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2023
VL 9
IS 1
BP 177
EP 194
DI 10.1007/s41095-021-0266-0
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5K7GQ
UT WOS:000869891100011
OA gold
DA 2024-07-18
ER

PT J
AU Huang, ZY
   Peng, YC
   Hibino, T
   Zhao, CQ
   Xie, HR
   Fukusato, T
   Miyata, K
AF Huang, Zhengyu
   Peng, Yichen
   Hibino, Tomohiro
   Zhao, Chunqi
   Xie, Haoran
   Fukusato, Tsukasa
   Miyata, Kazunori
TI DualFace: Two-stage drawing guidance for freehand portrait sketching
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE portrait painting; user interface; freehand sketching; generative model;
   drawing guidance
AB Special skills are required in portrait painting, such as imagining geometric structures and facial detail for final portrait designs. This makes it a difficult task for users, especially novices without prior artistic training, to draw freehand portraits with high-quality details. In this paper, we propose dualFace, a portrait drawing interface to assist users with different levels of drawing skills to complete recognizable and authentic face sketches. inspired by traditional artist workflows for portrait drawing, dualFace gives two-stages of drawing assistance to provide global and local visual guidance. The former helps users draw contour lines for portraits (i.e., geometric structure), and the latter helps users draw details of facial parts, which conform to the user-drawn contour lines. In the global guidance stage, the user draws several contour lines, and dualFace then searches for several relevant images from an internal database and displays the suggested face contour lines on the background of the canvas. In the local guidance stage, we synthesize detailed portrait images with a deep generative model from user-drawn contour lines, and then use the synthesized results as detailed drawing guidance. We conducted a user study to verify the effectiveness of dualFace, which confirms that dualFace significantly helps users to produce a detailed portrait sketch.
C1 [Huang, Zhengyu; Peng, Yichen; Hibino, Tomohiro; Xie, Haoran; Miyata, Kazunori] Japan Adv Inst Sci & Technol, Nomi, Ishikawa 9231211, Japan.
   [Zhao, Chunqi] Univ Tokyo, Tokyo 1138654, Japan.
   [Fukusato, Tsukasa] Univ Tokyo, Comp Sci Dept, Tokyo 1138654, Japan.
C3 Japan Advanced Institute of Science & Technology (JAIST); University of
   Tokyo; University of Tokyo
RP Xie, HR (corresponding author), Japan Adv Inst Sci & Technol, Nomi, Ishikawa 9231211, Japan.
EM huang.zhengyu@jaist.ac.jp; puckikk@gmail.com; t_hibino@jaist.ac.jp;
   shunnki.chou@ui.is.s.u-tokyo.ac.jp; xie@jaist.ac.jp;
   tsukasafukusato@is.s.u-tokyo.ac.jp; miyata@jaist.ac.jp
RI Huang, Zheng-Yu/O-7530-2016; Xie, Haoran/ADP-8087-2022
OI Xie, Haoran/0000-0002-6926-3082; Miyata, Kazunori/0000-0002-1582-0058
FU Tateishi Science and Technology Foundation, JSPS KAKENHI, Japan
   [JP20K19845, JP19K20316]
FX We thank the reviewers for their valuable comments. We also thank Toby
   Chong for his discussion of ideas. This work was supported by Grant from
   Tateishi Science and Technology Foundation, JSPS KAKENHI grant
   JP20K19845 and JP19K20316, Japan.
CR Bradley B., 2003, DRAWING PEOPLE PORTR
   Chen SY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392386
   Chen ZY, 2008, LECT NOTES COMPUT SC, V5353, P931, DOI 10.1007/978-3-540-89796-5_117
   Choi J, 2019, IEEE T MULTIMEDIA, V21, P2083, DOI 10.1109/TMM.2019.2892301
   Dekel T, 2018, PROC CVPR IEEE, P3511, DOI 10.1109/CVPR.2018.00370
   Dixon D, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P897
   Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185527
   Flagg M., 2006, P THE19TH ANN ACM S, P235, DOI DOI 10.1145/1166253.1166290
   Fukusato T., 2020, Journal of Computer Graphics Techniques, V9, P39
   Ghosh A, 2019, IEEE I CONF COMP VIS, P1171, DOI 10.1109/ICCV.2019.00126
   He Y., P SPIE, V11766, P2021
   Hu ZY, 2019, COMPUT ANIMAT VIRT W, V30, DOI 10.1002/cav.1889
   Hung-Yu Tseng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P158, DOI 10.1007/978-3-030-58523-5_10
   Iarussi E., 2013, P 26 ANN ACM S USER, DOI [DOI 10.1145/2501988.2501997, 10.1145/2501988.2501997]
   Igarashi T., 1997, Proceedings of the ACM Symposium on User Interface Software and Technology. 10th Annual Symposium. UIST '97, P105, DOI 10.1145/263407.263525
   Igarashi T., 2001, 01UIST. Proceedings of the 14th Annual ACM Symposium on User Interface Software and Technology, P173, DOI 10.1145/502348.502379
   Jingwei Zhang, 2017, 2017 IEEE International Conference on Multimedia and Expo: Workshops (ICMEW), P261, DOI 10.1109/ICMEW.2017.8026301
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   Laviole J., 2012, 2012 IEEE Symposium on 3D User Interfaces (3DUI), P3, DOI 10.1109/3DUI.2012.6184167
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Lee YJ, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964922
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li YH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P991, DOI 10.1145/3394171.3413684
   Li YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2323, DOI 10.1145/3343031.3350854
   Liao J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073683
   Liu L, 2017, PROC CVPR IEEE, P2298, DOI 10.1109/CVPR.2017.247
   Matsui Y, 2017, IEEE T VIS COMPUT GR, V23, P1852, DOI 10.1109/TVCG.2016.2554113
   Me DE, 2007, INT C COMP AID DES C, P185
   Portenier T, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201393
   Richardson E., 2020, ARXIV200800951
   Rosin P. L., 2017, P S NONPH AN REND JU
   Shuai Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P601, DOI 10.1007/978-3-030-58555-6_36
   Silva FC, 2019, IEEE IMAGE PROC, P3257, DOI [10.1109/ICIP.2019.8803667, 10.1109/icip.2019.8803667]
   Su QK, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601202
   Xie J., 2014, P 27 ANN ACM S USER, P407
   Yi R, 2019, PROC CVPR IEEE, P10735, DOI 10.1109/CVPR.2019.01100
   Yichen Peng, 2020, 2020 Nicograph International (NicoInt). Proceedings, P32, DOI 10.1109/NicoInt50878.2020.00013
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu Q, 2016, PROC CVPR IEEE, P799, DOI 10.1109/CVPR.2016.93
   Zhizhou He, 2020, 2020 Nicograph International (NicoInt). Proceedings, P55, DOI 10.1109/NicoInt50878.2020.00018
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36
NR 42
TC 16
Z9 16
U1 0
U2 23
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2022
VL 8
IS 1
BP 63
EP 77
DI 10.1007/s41095-021-0227-7
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WO6VR
UT WOS:000712590200004
OA Green Submitted, gold
DA 2024-07-18
ER

PT J
AU Guo, YD
   Zhang, JY
   Chen, YH
   Cai, HR
   Huang, ZJ
   Deng, BL
AF Guo, Yudong
   Zhang, Juyong
   Chen, Yihua
   Cai, Hongrui
   Huang, Zhangjin
   Deng, Bailin
TI Real-time face view correction for front-facing cameras
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE face view correction; 3D face reconstruction; deep learning; online
   communication
ID IMAGE; RECONSTRUCTION; GEOMETRY
AB Face views are particularly important in person-to-person communication. Differenes between the camera location and the face orientation can result in undesirable facial appearances of the participants during video conferencing. This phenomenon is particularly noticeable when using devices where the front-facing camera is placed in unconventional locations such as below the display or within the keyboard. In this paper, we take a video stream from a single RGB camera as input, and generate a video stream that emulates the view from a virtual camera at a designated location. The most challenging issue in this problem is that the corrected view often needs out-of-plane head rotations. To address this challenge, we reconstruct the 3D face shape and re-render it into synthesized frames according to the virtual camera location. To output the corrected video stream with natural appearance in real time, we propose several novel techniques including accurate eyebrow reconstruction, high-quality blending between the corrected face image and background, and template-based 3D reconstruction of glasses. Our system works well for different lighting conditions and skin tones, and can handle users wearing glasses. Extensive experiments and user studies demonstrate that our method provides high-quality results.
C1 [Guo, Yudong; Zhang, Juyong; Chen, Yihua; Cai, Hongrui; Huang, Zhangjin] Univ Sci & Technol China, Hefei 230026, Peoples R China.
   [Deng, Bailin] Cardiff Univ, Sch Comp Sci & Informat, Cardiff, Wales.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Cardiff University
RP Zhang, JY (corresponding author), Univ Sci & Technol China, Hefei 230026, Peoples R China.
EM gyd2011@mail.ustc.edu.cn; juyong@ustc.edu.cn;
   chenyihua@mail.ustc.edu.cn; hrcai@mail.ustc.edu.cn; zhuang@ustc.edu.cn;
   DengB3@cardiff.ac.uk
RI Cai, Hongrui/AAU-4385-2021; Huang, Zhangjin/I-7929-2014
CR Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Bouaziz S, 2012, COMPUT GRAPH FORUM, V31, P1657, DOI 10.1111/j.1467-8659.2012.03171.x
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   BURT PJ, 1983, ACM T GRAPHIC, V2, P217, DOI 10.1145/245.247
   Cao C, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601204
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   Fried O, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925933
   Garrido P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2890493
   Gecer B, 2019, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2019.00125
   Genova K, 2018, PROC CVPR IEEE, P8377, DOI 10.1109/CVPR.2018.00874
   Giger D, 2014, IEEE INT CON MULTI
   Grayson D. M., 2003, ACM Transactions on Computer-Human Interaction, V10, P221, DOI 10.1145/937549.937552
   Guillemot C, 2014, IEEE SIGNAL PROC MAG, V31, P127, DOI 10.1109/MSP.2013.2273004
   Guo YD, 2019, IEEE T PATTERN ANAL, V41, P1294, DOI 10.1109/TPAMI.2018.2837742
   Hassner T, 2015, PROC CVPR IEEE, P4295, DOI 10.1109/CVPR.2015.7299058
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Z, 2019, IEEE I CONF COMP VIS, P6931, DOI 10.1109/ICCV.2019.00703
   Hsu CF, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3311784
   Ishii H., 1992, CHI '92 Conference Proceedings. ACM Conference on Human Factors in Computing Systems. Striking a Balance, P525, DOI 10.1145/142750.142977
   Jackson AS, 2017, IEEE I CONF COMP VIS, P1031, DOI 10.1109/ICCV.2017.117
   Jiang L, 2018, IEEE T IMAGE PROCESS, V27, P4756, DOI 10.1109/TIP.2018.2845697
   Jichao Zhang, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P1588, DOI 10.1145/3394171.3413981
   Kim H, 2018, PROC CVPR IEEE, P4625, DOI 10.1109/CVPR.2018.00486
   Kononenko D, 2015, PROC CVPR IEEE, P4667, DOI 10.1109/CVPR.2015.7299098
   Kuster C., 2011, Proc. Annual Workshop on Vision, P17
   Kuster C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366193
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Li TY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130813
   Tran L, 2018, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2018.00767
   Matusik W, 2004, ACM T GRAPHIC, V23, P814, DOI 10.1145/1015706.1015805
   Matusik W, 2000, COMP GRAPH, P369, DOI 10.1145/344779.344951
   Monk AF, 2002, DISCOURSE PROCESS, V33, P257, DOI 10.1207/S15326950DP3303_4
   Mukawa N., 2005, C HUMAN FACTORS COMP, P1677, DOI DOI 10.1145/1056808.1056995
   Muller Claus., 1966, Spherical Harmonics, V17, DOI DOI 10.1007/BFB0094775
   Nagano K, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356568
   Okada K., 1994, Transcending Boundaries, CSCW '94. Proceedings of the Conference on Computer Supported Cooperative Work, P385, DOI 10.1145/192844.193054
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Richardson E, 2017, PROC CVPR IEEE, P5553, DOI 10.1109/CVPR.2017.589
   Richardson E, 2016, INT CONF 3D VISION, P460, DOI 10.1109/3DV.2016.56
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sela M, 2017, IEEE I CONF COMP VIS, P1585, DOI 10.1109/ICCV.2017.175
   Shu ZX, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3095816
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Tewari A, 2019, PROC CVPR IEEE, P10804, DOI 10.1109/CVPR.2019.01107
   Tewari A, 2018, PROC CVPR IEEE, P2549, DOI 10.1109/CVPR.2018.00270
   Tewari A, 2017, IEEE I CONF COMP VIS, P3735, DOI 10.1109/ICCV.2017.401
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Yin Y, 2020, IEEE INT CONF AUTOMA, P249, DOI 10.1109/FG47880.2020.00004
   Zhai DM, 2019, IEEE T IND ELECTRON, V66, P9601, DOI 10.1109/TIE.2018.2889616
   Zhao YJ, 2019, IEEE I CONF COMP VIS, P7848, DOI 10.1109/ICCV.2019.00794
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
   Zollhöfer M, 2018, COMPUT GRAPH FORUM, V37, P523, DOI 10.1111/cgf.13382
NR 55
TC 0
Z9 0
U1 1
U2 14
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2021
VL 7
IS 4
BP 437
EP 452
DI 10.1007/s41095-021-0215-y
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UK7UX
UT WOS:000692172600002
OA Green Accepted, gold
DA 2024-07-18
ER

PT J
AU Zhang, F
   Li, JJ
   Liu, PQ
   Fan, H
AF Zhang, Fan
   Li, Jinjiang
   Liu, Peiqiang
   Fan, Hui
TI Computing knots by quadratic and cubic polynomial curves
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE knot; interpolation; polynomial curve; affine invariant
ID B-SPLINE CURVE; PARAMETERIZATION; INTERPOLATION; PARAMETRIZATION
AB A new method is presented to determine parameter values (knot) for data points for curve and surface generation. With four adjacent data points, a quadratic polynomial curve can be determined uniquely if the four points form a convex polygon. When the four data points do not form a convex polygon, a cubic polynomial curve with one degree of freedom is used to interpolate the four points, so that the interpolant has better shape, approximating the polygon formed by the four data points. The degree of freedom is determined by minimizing the cubic coefficient of the cubic polynomial curve. The advantages of the new method are, firstly, the knots computed have quadratic polynomial precision, i.e., if the data points are sampled from a quadratic polynomial curve, and the knots are used to construct a quadratic polynomial, it reproduces the original quadratic curve. Secondly, the new method is affine invariant, which is significant, as most parameterization methods do not have this property. Thirdly, it computes knots using a local method. Experiments show that curves constructed using knots computed by the new method have better interpolation precision than for existing methods.
C1 [Zhang, Fan; Li, Jinjiang; Liu, Peiqiang; Fan, Hui] Shandong Technol & Business Univ, Sch Comp Sci & Technol, Yantai, Shandong, Peoples R China.
   [Zhang, Fan; Li, Jinjiang; Liu, Peiqiang; Fan, Hui] Coinnovat Ctr Shandong Coll & Univ Future Intelli, Yantai 264005, Peoples R China.
C3 Shandong Technology & Business University
RP Zhang, F (corresponding author), Shandong Technol & Business Univ, Sch Comp Sci & Technol, Yantai, Shandong, Peoples R China.; Zhang, F (corresponding author), Coinnovat Ctr Shandong Coll & Univ Future Intelli, Yantai 264005, Peoples R China.
EM zhangfan51@sina.com; lijinjiang@gmail.com; liupq@126.com;
   fanlinw@263.net
RI Fan, Hui/AAC-5992-2022; Zhang, Fan/GLT-6231-2022
OI Zhang, Fan/0000-0002-0343-3499
FU National Natural Science Foundation of China [61602277, 61772319];
   Natural Science Foundation of Shandong Province [ZR2016FQ12,
   ZR2018BF009]; Key Research and Development Program of Yantai City
   [2017ZH065]; CERNET Innovation Project [NGII20161204]; Science and
   Technology Innovation Program for Distributed Young Talents of Shandong
   Province Higher Education Institutions [2019KJN042]
FX This work was supported in part by the following: National Natural
   Science Foundation of China under Grant Nos. 61602277 and 61772319,
   Natural Science Foundation of Shandong Province under Grant Nos.
   ZR2016FQ12 and ZR2018BF009, Key Research and Development Program of
   Yantai City under Grant No. 2017ZH065, CERNET Innovation Project under
   Grant No. NGII20161204, and Science and Technology Innovation Program
   for Distributed Young Talents of Shandong Province Higher Education
   Institutions under Grant No. 2019KJN042.
CR Ahlberg J.H., 1967, The Theory of Splines and Their Applications
   Antonelli M, 2016, COMPUT AIDED GEOM D, V46, P103, DOI 10.1016/j.cagd.2016.06.005
   Bashir U, 2013, APPL MATH COMPUT, V219, P10183, DOI 10.1016/j.amc.2013.03.110
   Bastl B, 2012, COMPUT AIDED GEOM D, V29, P231, DOI 10.1016/j.cagd.2011.04.003
   Bastl B, 2011, COMPUT AIDED GEOM D, V28, P127, DOI 10.1016/j.cagd.2010.11.001
   Boor C D, 1978, PRACTICAL GUIDE SPLI, DOI [10.1007/978-1-4612-6333-3, DOI 10.1007/978-1-4612-6333-3]
   Brodlie KW, 1980, MATH METHODS COMPUTE, P33
   Fang JJ, 2013, COMPUT AIDED DESIGN, V45, P1005, DOI 10.1016/j.cad.2013.01.005
   Farin G., 2014, Curves and Surfaces for Computer-Aided Geometric Design: A Practical Guide
   Farin G, 2006, COMPUT AIDED GEOM D, V23, P722, DOI 10.1016/j.cagd.2006.08.002
   FAUX I.D., 1979, COMPUTATIONAL GEOMET
   Floater MS, 2001, COMPUT AIDED GEOM D, V18, P77, DOI 10.1016/S0167-8396(01)00013-9
   Gotsman C, 2003, ACM T GRAPHIC, V22, P358, DOI 10.1145/882262.882276
   Han XL, 2011, COMPUT AIDED GEOM D, V28, P151, DOI 10.1016/j.cagd.2011.02.001
   HARTLEY PJ, 1980, COMPUT AIDED DESIGN, V12, P235, DOI 10.1016/0010-4485(80)90028-7
   Hui Xie, 2001, International Journal of Shape Modeling, V7, P199, DOI 10.1142/S0218654301000126
   Jeong SY, 2006, COMPUT AIDED DESIGN, V38, P39, DOI 10.1016/j.cad.2005.06.002
   LEE ETY, 1989, COMPUT AIDED DESIGN, V21, P363, DOI 10.1016/0010-4485(89)90003-1
   Li WS, 2004, COMPUT AIDED GEOM D, V21, P499, DOI 10.1016/j.cagd.2004.03.004
   Lim CG, 1999, COMPUT AIDED GEOM D, V16, P407, DOI 10.1016/S0167-8396(99)00010-2
   Lin FM, 2019, COMPUT AIDED DESIGN, V106, P13, DOI 10.1016/j.cad.2018.08.001
   Lü W, 2009, COMPUT AIDED GEOM D, V26, P342, DOI 10.1016/j.cagd.2008.08.001
   MARIN SP, 1984, J APPROX THEORY, V41, P64, DOI 10.1016/0021-9045(84)90121-7
   Su B., 1989, COMPUTATIONAL GEOMET
   Tsuchie S, 2016, COMPUT AIDED DESIGN, V71, P39, DOI 10.1016/j.cad.2015.09.004
   Xianfeng Gu, 2003, Symposium on Geometry Processing, P127
   Yang ZY, 2015, COMPUT AIDED DESIGN, V66, P62, DOI 10.1016/j.cad.2015.04.010
   Yuksel C, 2011, COMPUT AIDED DESIGN, V43, P747, DOI 10.1016/j.cad.2010.08.008
   Zhang CM, 2013, COMPUT AIDED DESIGN, V45, P853, DOI 10.1016/j.cad.2011.08.004
   Zhang CM, 1998, COMPUT AIDED GEOM D, V15, P399, DOI 10.1016/S0167-8396(97)00041-1
NR 30
TC 6
Z9 6
U1 0
U2 5
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2020
VL 6
IS 4
BP 417
EP 430
DI 10.1007/s41095-020-0186-4
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FN
UT WOS:000648692500004
OA gold
DA 2024-07-18
ER

PT J
AU Debelee, TG
   Schwenker, F
   Rahimeto, S
   Yohannes, D
AF Debelee, Taye Girma
   Schwenker, Friedhelm
   Rahimeto, Samuel
   Yohannes, Dereje
TI Evaluation of modified adaptive <i>k</i>-means segmentation algorithm
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE clustering; modified adaptive k-means (MAKM); segmentation; Q-value
ID MEANS CLUSTERING-ALGORITHM; CLASSIFICATION
AB Segmentation is the act of partitioning an image into different regions by creating boundaries between regions. k-means image segmentation is the simplest prevalent approach. However, the segmentation quality is contingent on the initial parameters (the cluster centers and their number). In this paper, a convolution-based modified adaptive k-means (MAKM) approach is proposed and evaluated using images collected from different sources (MATLAB, Berkeley image database, VOC2012, BGH, MIAS, and MRI). The evaluation shows that the proposed algorithm is superior to k-means++, fuzzy c-means, histogram-based k-means, and subtractive k-means algorithms in terms of image segmentation quality (Q-value), computational cost, and RMSE. The proposed algorithm was also compared to state-of-the-art learning-based methods in terms of IoU and MIoU; it achieved a higher MIoU value.
C1 [Debelee, Taye Girma; Schwenker, Friedhelm] Ulm Univ, Inst Neural Informat Proc, D-89081 Ulm, Germany.
   [Debelee, Taye Girma; Rahimeto, Samuel; Yohannes, Dereje] Addis Ababa Sci & Technol Univ, Addis Ababa 120611, Ethiopia.
C3 Ulm University
RP Debelee, TG (corresponding author), Ulm Univ, Inst Neural Informat Proc, D-89081 Ulm, Germany.; Debelee, TG (corresponding author), Addis Ababa Sci & Technol Univ, Addis Ababa 120611, Ethiopia.
EM taye.debelee@uni-ulm.de; friedhelm.schwenker@uni-ulm.de;
   samuelrahimeto@gmail.com; derejey@yahoo.com
RI Schwenker, Friedhelm/G-6069-2015; Kebede, Samuel/KIL-1177-2024; Debelee,
   Taye Girma/AAY-1250-2021; Debelee, Taye Girma/IUP-8447-2023
OI Debelee, Taye Girma/0000-0002-0876-2021; Rahimeto,
   Samuel/0000-0002-9764-6427
FU Ethiopian Ministry of Education (MoE); Deutscher Akademischer
   Auslandsdienst (DAAD) [57162925]
FX The corresponding author would like to thank the Ethiopian Ministry of
   Education (MoE) and the Deutscher Akademischer Auslandsdienst (DAAD) for
   funding this research work (funding number 57162925).
CR [Anonymous], 2016, P INT C LEARN REPR
   [Anonymous], 2015, COMPUTER SCI
   [Anonymous], 2014, ADAPTIVE KMEANS CLUS
   [Anonymous], 2013, INT J COMPUT APPL
   [Anonymous], 1992, R. woods digital image processing
   Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Benaichouche AN, 2013, DIGIT SIGNAL PROCESS, V23, P1390, DOI 10.1016/j.dsp.2013.07.005
   Bezdek J C, 1981, ADV APPL PATTERN REC, P155, DOI [10.1007/978-1-4757-0450-1_5, DOI 10.1007/978-1-4757-0450-1_5]
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Dai JF, 2015, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2015.7299025
   Dhanachandra N, 2015, PROCEDIA COMPUT SCI, V54, P764, DOI 10.1016/j.procs.2015.06.090
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fausser S, 2012, INT C PATT RECOG, P501
   Fausser S, 2010, LECT NOTES ARTIF INT, V5998, P131, DOI 10.1007/978-3-642-12159-3_12
   Jaglan Poonam, 2019, Proceedings of 2nd International Conference on Communication, Computing and Networking. ICCCN 2018. Lecture Notes in Networks and Systems (LNNS 46), P359, DOI 10.1007/978-981-13-1217-5_36
   Kamran SA, 2018, 2018 INTERNATIONAL SYMPOSIUM ON ADVANCED INTELLIGENT INFORMATICS (SAIN), P123, DOI 10.1109/SAIN.2018.8673354
   Khan Z, 2017, INT J INNOV COMPUT I, V13, P1509
   Khanmohammadi S, 2017, EXPERT SYST APPL, V67, P12, DOI 10.1016/j.eswa.2016.09.025
   Küçükkülahli E, 2016, NEURAL COMPUT APPL, V27, P1445, DOI 10.1007/s00521-016-2287-7
   Lei T, 2018, IEEE T FUZZY SYST, V26, P3027, DOI 10.1109/TFUZZ.2018.2796074
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Madhu YedlaS.R. Pathakota., 2010, International Journal of Computer Science and Information Technologies, V1, P121
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Minaee S, 2019, IEEE T IMAGE PROCESS, V28, P3192, DOI 10.1109/TIP.2019.2894966
   Minaee S, 2016, IEEE J EM SEL TOP C, V6, P573, DOI 10.1109/JETCAS.2016.2597701
   Minaee S, 2015, IEEE IMAGE PROC, P3295, DOI 10.1109/ICIP.2015.7351413
   Mostajahi M, 2015, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2015.7298959
   Saffor R, 2001, J COMPUTER SCI, V14, P39
   Schwenker F, 2014, PATTERN RECOGN LETT, V37, P4, DOI 10.1016/j.patrec.2013.10.017
   Shi PF, 2016, STRUCT HEALTH MONIT, V15, P541, DOI 10.1177/1475921716651039
   SUCKLING J, 1994, INT CONGR SER, V1069, P375
   Zadegan SMR, 2013, KNOWL-BASED SYST, V39, P133, DOI 10.1016/j.knosys.2012.10.012
   Zaitoun NM, 2015, PROCEDIA COMPUT SCI, V65, P797, DOI 10.1016/j.procs.2015.09.027
   Zhang Y, 2011, EXPERT SYST APPL, V38, P9036, DOI 10.1016/j.eswa.2011.01.041
NR 36
TC 40
Z9 41
U1 2
U2 21
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2019
VL 5
IS 4
BP 347
EP 361
DI 10.1007/s41095-019-0151-2
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UO
UT WOS:000648691000003
OA gold
DA 2024-07-18
ER

PT J
AU Zhang, W
   Ke, W
   Yang, D
   Sheng, H
   Xiong, Z
AF Zhang, Wei
   Ke, Wei
   Yang, Da
   Sheng, Hao
   Xiong, Zhang
TI Light field super-resolution using complementary-view feature attention
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE light field (LF); super-resolution (SR); attention
ID NETWORK
AB Light field (LF) cameras record multiple perspectives by a sparse sampling of real scenes, and these perspectives provide complementary information. This information is beneficial to LF super-resolution (LFSR). Compared with traditional single-image super-resolution, LF can exploit parallax structure and perspective correlation among different LF views. Furthermore, the performance of existing methods are limited as they fail to deeply explore the complementary information across LF views. In this paper, we propose a novel network, called the light field complementary-view feature attention network (LF-CFANet), to improve LFSR by dynamically learning the complementary information in LF views. Specifically, we design a residual complementary-view spatial and channel attention module (RCSCAM) to effectively interact with complementary information between complementary views. Moreover, RCSCAM captures the relationships between different channels, and it is able to generate informative features for reconstructing LF images while ignoring redundant information. Then, a maximum-difference information supplementary branch (MDISB) is used to supplement information from the maximum-difference angular positions based on the geometric structure of LF images. This branch also can guide the process of reconstruction. Experimental results on both synthetic and real-world datasets demonstrate the superiority of our method. The proposed LF-CFANet has a more advanced reconstruction performance that displays faithful details with higher SR accuracy than state-of-the-art methods.
C1 [Zhang, Wei; Ke, Wei; Sheng, Hao; Xiong, Zhang] Macao Polytech Univ, Fac Appl Sci, Macau 999078, Peoples R China.
   [Yang, Da; Sheng, Hao; Xiong, Zhang] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Software Dev Environm, Beijing 100191, Peoples R China.
   [Yang, Da; Sheng, Hao; Xiong, Zhang] Beihang Hangzhou Innovat Inst Yuhang, Hangzhou 310023, Peoples R China.
C3 Macao Polytechnic University; Beihang University
RP Ke, W (corresponding author), Macao Polytech Univ, Fac Appl Sci, Macau 999078, Peoples R China.; Yang, D (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Software Dev Environm, Beijing 100191, Peoples R China.; Yang, D (corresponding author), Beihang Hangzhou Innovat Inst Yuhang, Hangzhou 310023, Peoples R China.
EM wke@mpu.edu.mo; da.yang@buaa.edu.cn
RI Ke, Wei/JXM-8153-2024
OI Ke, Wei/0000-0003-0952-0961
FU National Key Ramp;D Program of China [2018YFB2100500]; National Natural
   Science Foundation of China [61872025]; Science and Technology
   Development Fund, Macau SAR [0001/2018/AFJ]; Open Fund of the State Key
   Laboratory of Software Development Environment [SKLSDE-2021ZX-03];
   HAWKEYE Group
FX This study was partially supported by the National Key R&D Program of
   China (2018YFB2100500), the National Natural Science Foundation of China
   (61872025), the Science and Technology Development Fund, Macau SAR
   (0001/2018/AFJ), and the Open Fund of the State Key Laboratory of
   Software Development Environment (SKLSDE-2021ZX-03). We thank the
   HAWKEYE Group for their support.
CR Alain M, 2018, IEEE IMAGE PROC, P2501, DOI 10.1109/ICIP.2018.8451162
   Bishop TE, 2012, IEEE T PATTERN ANAL, V34, P972, DOI 10.1109/TPAMI.2011.168
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Honauer K, 2017, LECT NOTES COMPUT SC, V10113, P19, DOI 10.1007/978-3-319-54187-7_2
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang F.-C., 2015, SIGGRAPH EMERGING TE, P24
   Huang Y, 2015, ADV NEUR IN, V28
   Jin J, 2020, PROC CVPR IEEE, P2257, DOI 10.1109/CVPR42600.2020.00233
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Le Pendu M, 2018, IEEE T IMAGE PROCESS, V27, P1981, DOI 10.1109/TIP.2018.2791864
   Li DL, 2021, LECT NOTES COMPUT SC, V12937, P314, DOI 10.1007/978-3-030-85928-2_25
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Miaofei Zhang, 2019, 2019 International Conference on Networking and Network Applications (NaNA), P81, DOI 10.1109/NaNA.2019.00023
   Mitra Kaushik, 2012, 2012 IEEE COMPUTER S, P22
   Mo Y, 2022, IEEE T CIRC SYST VID, V32, P4431, DOI 10.1109/TCSVT.2021.3121679
   Piao YR, 2020, IEEE T IMAGE PROCESS, V29, P1879, DOI 10.1109/TIP.2019.2942434
   Raj A.S., 2016, Stanford lytro light field archive
   Rerabek Martin, 2016, P 8 INT C QUAL MULT
   Rossi M, 2018, IEEE T IMAGE PROCESS, V27, P4207, DOI 10.1109/TIP.2018.2828983
   Sheng H, 2021, IEEE INTERNET THINGS, V8, P2193, DOI 10.1109/JIOT.2020.3035415
   Sheng H, 2020, IEEE T CIRC SYST VID, V30, P2971, DOI 10.1109/TCSVT.2020.2988649
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Vaish V, 2008, The (New) Stanford Light Field Archive
   Wang LG, 2019, PROC CVPR IEEE, P12242, DOI 10.1109/CVPR.2019.01253
   Wang S, 2024, IEEE T IND INFORM, V20, P369, DOI [10.1109/TIV.2023.3284394, 10.1109/TII.2023.3261890]
   Wang S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13199, DOI 10.1109/ICCV48922.2021.01297
   Wang YQ, 2021, IEEE COMPUT SOC CONF, P766, DOI 10.1109/CVPRW53098.2021.00086
   Wang YL, 2018, IEEE T IMAGE PROCESS, V27, P4274, DOI 10.1109/TIP.2018.2834819
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Wanner S., 2013, INT S VIS MOD VIS, P225
   Wanner S, 2014, IEEE T PATTERN ANAL, V36, P606, DOI 10.1109/TPAMI.2013.147
   Wanner S, 2012, LECT NOTES COMPUT SC, V7576, P608, DOI 10.1007/978-3-642-33715-4_44
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Yeung HWF, 2019, IEEE T IMAGE PROCESS, V28, P2319, DOI 10.1109/TIP.2018.2885236
   Ying XY, 2020, IEEE SIGNAL PROC LET, V27, P496, DOI 10.1109/LSP.2020.2973813
   Yingqian Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P290, DOI 10.1007/978-3-030-58592-1_18
   Yoon Y, 2017, IEEE SIGNAL PROC LET, V24, P848, DOI 10.1109/LSP.2017.2669333
   Yoon Y, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P57, DOI 10.1109/ICCVW.2015.17
   Yu JY, 2017, IEEE MULTIMEDIA, V24, P104, DOI 10.1109/MMUL.2017.24
   Yuan Y, 2018, IEEE SIGNAL PROC LET, V25, P1359, DOI 10.1109/LSP.2018.2856619
   Zhang FL, 2017, IEEE T VIS COMPUT GR, V23, P1561, DOI 10.1109/TVCG.2016.2532329
   Zhang SF, 2019, PROC CVPR IEEE, P919, DOI 10.1109/CVPR.2019.00101
   Zhang YL, 2021, IEEE T PATTERN ANAL, V43, P2480, DOI 10.1109/TPAMI.2020.2968521
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhu H, 2017, IEEE J-STSP, V11, P965, DOI 10.1109/JSTSP.2017.2730818
NR 48
TC 7
Z9 7
U1 6
U2 15
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2023
VL 9
IS 4
BP 843
EP 858
DI 10.1007/s41095-022-0297-1
EA JUL 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P1QC2
UT WOS:001022797900001
OA gold
DA 2024-07-18
ER

PT J
AU Rosin, PL
   Lai, YK
   Mould, D
   Yi, R
   Berger, I
   Doyle, L
   Lee, S
   Li, C
   Liu, YJ
   Semmo, A
   Shamir, A
   Son, M
   Winnemöller, H
AF Rosin, Paul L.
   Lai, Yu-Kun
   Mould, David
   Yi, Ran
   Berger, Itamar
   Doyle, Lars
   Lee, Seungyong
   Li, Chuan
   Liu, Yong-Jin
   Semmo, Amir
   Shamir, Ariel
   Son, Minjung
   Winnemoller, Holger
TI NPRportrait 1.0: A three-level benchmark for non-photorealistic
   rendering of portraits
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE non-photorealistic rendering (NPR); image stylization; style transfer;
   portrait; evaluation; benchmark
ID IMAGE QUALITY ASSESSMENT; STYLIZATION
AB Recently, there has been an upsurge of activity in image-based non-photorealistic rendering (NPR), and in particular portrait image stylisation, due to the advent of neural style transfer (NST). However, the state of performance evaluation in this field is poor, especially compared to the norms in the computer vision and machine learning communities. Unfortunately, the task of evaluating image stylisation is thus far not well defined, since it involves subjective, perceptual, and aesthetic aspects. To make progress towards a solution, this paper proposes a new structured, three-level, benchmark dataset for the evaluation of stylised portrait images. Rigorous criteria were used for its construction, and its consistency was validated by user studies. Moreover, a new methodology has been developed for evaluating portrait stylisation algorithms, which makes use of the different benchmark levels as well as annotations provided by user studies regarding the characteristics of the faces. We perform evaluation for a wide variety of image stylisation methods (both portrait-specific and general purpose, and also both traditional NPR approaches and NST) using the new benchmark dataset.
C1 [Rosin, Paul L.; Lai, Yu-Kun] Cardiff Univ, Sch Comp Sci & Informat, Cardiff, Wales.
   [Mould, David; Doyle, Lars] Carleton Univ, Sch Comp Sci, Ottawa, ON, Canada.
   [Yi, Ran; Liu, Yong-Jin] Tsinghua Univ, Dept Comp Sci & Technol, Beijing, Peoples R China.
   [Berger, Itamar; Shamir, Ariel] Reichman Univ, Interdisciplinary Ctr, Herzliyya, Israel.
   [Lee, Seungyong] Pohang Univ Sci & Technol, Dept Comp Sci & Engn, Pohang, South Korea.
   [Li, Chuan] Lambda Labs Inc, San Francisco, CA USA.
   [Semmo, Amir] Univ Potsdam, Hasso Plattner Inst, Potsdam, Germany.
   [Son, Minjung] Samsung Adv Inst Technol, Multimedia Proc Lab, Suwon, South Korea.
   [Winnemoller, Holger] Adobe Syst Inc, San Jose, CA USA.
C3 Cardiff University; Carleton University; Tsinghua University; Reichman
   University; Pohang University of Science & Technology (POSTECH);
   University of Potsdam; Samsung; Adobe Systems Inc.
RP Rosin, PL (corresponding author), Cardiff Univ, Sch Comp Sci & Informat, Cardiff, Wales.
EM RosinPL@cardiff.ac.uk; Yukun.Lai@cs.cardiff.ac.uk;
   mould@scs.carleton.ca; yr16@mails.tsinghua.edu.cn;
   berger.itamar@gmail.com; larsdoyle@cmail.carleton.ca;
   leesy@postech.ac.kr; c@lambdal.com; liuyongjin@tsinghua.edu.cn;
   Amir.Semmo@hpi.de; arik@idc.ac.il; minjungs.son@samsung.com;
   hwinnemo@adobe.com
RI Son, Minjung/JPQ-1876-2023; Yi, Ran/AAU-6636-2021; Li,
   Chuan/A-3959-2017; Semmo, Amir/KPA-5814-2024; Lai, Yu-Kun/D-2343-2010;
   Liu, Yong/GWQ-6163-2022
OI Yi, Ran/0000-0003-1858-3358; Son, Minjung/0000-0002-7915-7320; Rosin,
   Paul/0000-0002-4965-3884
CR [Anonymous], 2013, COMPUTATIONAL IMAGIN
   [Anonymous], 2017, P S NONPH AN REND NP, DOI [DOI 10.1145/3092919.3092920, 10.1145/3092919.3092920]
   Atkinson AC, 2007, OPTIMUM EXPT DESIGNS
   Azami R., 2017, P S COMP AESTH
   Batres C, 2017, HUM NATURE-INT BIOS, V28, P344, DOI 10.1007/s12110-017-9289-8
   Berger I, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461964
   Binkowski Mikolaj, 2018, INT C LEARNING REPRE
   Bruce V., 2012, Face perception
   Buolamwini J., 2018, P 1 C FAIRNESS ACCOU, P77
   Cha SH, 2002, PATTERN RECOGN, V35, P1355, DOI 10.1016/S0031-3203(01)00118-2
   Cooper PA, 2008, PERCEPTION, V37, P1216, DOI 10.1068/p5865
   Dobs K, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-09239-1
   Doyle L, 2020, ARXIV PREPRINT ARXIV
   Doyle L, 2019, COMPUT VIS MEDIA, V5, P33, DOI 10.1007/s41095-019-0129-0
   Doyle R, 1998, SCI AM, V279, P30
   Du L, 2020, IEEE WINT CONF APPL, P3139, DOI 10.1109/WACV45572.2020.9093537
   Fahsing IA, 2004, J APPL PSYCHOL, V89, P722, DOI 10.1037/0021-9010.89.4.722
   Fedorov V. V., 1972, Theory of Optimal Experiments
   Fisher Robert., CVONLINE
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Gooch A.A., 2010, NPAR 10 P 8 INT S NO, P165
   Hall P., 2013, Image and Video- Based Artistic Stylisation, P333, DOI [DOI 10.1007/978-1-4471-4519-6_16, DOI 10.1007/978-1-4471-4519-6]
   Hensel M, 2017, ADV NEUR IN, V30
   Hertzmann A., 2010, Proc. NPAR '10, P147
   Isenberg T., 2013, Image and Video-Based Artistic Stylisation, V42, P311
   Jing YH, 2021, IEEE T CYBERNETICS, V51, P568, DOI 10.1109/TCYB.2019.2904768
   Kettunen M., 2019, ARXIV PREPRINT ARXIV
   Klingbeil, 2017, P SIGGRAPH AS MOB GR
   Kumar M.P.P., 2019, Iran J. Comput. Sci, V2, P131, DOI [10.1007/s42044-019-00034-1, DOI 10.1007/S42044-019-00034-1]
   Kyprianidis JE, 2013, IEEE T VIS COMPUT GR, V19, P866, DOI 10.1109/TVCG.2012.160
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Low PE, 2021, SIGNAL PROCESS-IMAGE, V91, DOI 10.1016/j.image.2020.116090
   MCLELLAN B, 1993, CAN J BEHAV SCI, V25, P135, DOI 10.1037/h0078790
   Meier, 2019, P COMP INT WORKSH
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Mould D., 2014, P WORKSHOP NONPHOTOR, P49
   Mould D, 2017, COMPUT GRAPH-UK, V67, P58, DOI 10.1016/j.cag.2017.05.025
   Rosin P. L., 2017, P S NONPH AN REND JU
   Rosin PL, 2018, LECT NOTES COMPUT SC, V10799, P268, DOI 10.1007/978-3-319-92753-4_22
   Rosin PaulL., 2015, Proceedings of the Workshop on Computational Aesthetics, P159
   Sanakoyeu A, 2018, LECT NOTES COMPUT SC, V11212, P715, DOI 10.1007/978-3-030-01237-3_43
   Semmo A, 2016, COMPUT GRAPH-UK, V55, P157, DOI 10.1016/j.cag.2015.12.001
   Shen Q, 2020, 2020 5TH INTERNATIONAL CONFERENCE ON MATHEMATICS AND ARTIFICIAL INTELLIGENCE (ICMAI 2020), P5, DOI [10.1145/3395260.3395286, 10.1109/APPEEC48164.2020.9220530]
   Son M, 2011, GRAPH MODELS, V73, P74, DOI 10.1016/j.gmod.2010.12.001
   Trapp, 2018, P ANN EUR ASS COMP G, P17
   vanKoppen PJ, 1997, LAW HUMAN BEHAV, V21, P661, DOI 10.1023/A:1024812831576
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wauthier F., 2013, INT C MACH LEARN, P109
   Wheeler B., 2014, ALGDESIGN ALGORITHMI
   Winnemoeller H, 2012, COMPUT GRAPH-UK, V36, P740, DOI 10.1016/j.cag.2012.03.004
   Wu T, 2018, MATH PROBL ENG, V2018, DOI 10.1155/2018/3670498
   Yaniv J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322984
   Yi R, 2019, PROC CVPR IEEE, P10735, DOI 10.1109/CVPR.2019.01100
   Yuanzhong Li, 1995, Proceedings. International Conference on Image Processing (Cat. No.95CB35819), P520, DOI 10.1109/ICIP.1995.537686
   Zamir SW, 2021, IEEE T PATTERN ANAL, V43, P1777, DOI 10.1109/TPAMI.2019.2938499
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhao Mingtian, 2013, IMAGE VIDEO BASED AR, P237
NR 59
TC 3
Z9 3
U1 0
U2 8
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2022
VL 8
IS 3
BP 445
EP 465
DI 10.1007/s41095-021-0255-3
EA APR 2022
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 0Q6DG
UT WOS:000782171500001
OA Green Accepted, Green Submitted, gold
DA 2024-07-18
ER

PT J
AU Yan, FH
   Li, ZX
   Zhou, Z
AF Yan, Feihu
   Li, Zhaoxin
   Zhou, Zhong
TI Robust and efficient edge-based visual odometry
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE visual odometry (VO); edge structure; distance transform; low-texture
ID MONOCULAR SLAM
AB Visual odometry, which aims to estimate relative camera motion between sequential video frames, has been widely used in the fields of augmented reality, virtual reality, and autonomous driving. However, it is still quite challenging for state-of-the-art approaches to handle low-texture scenes. In this paper, we propose a robust and efficient visual odometry algorithm that directly utilizes edge pixels to track camera pose. In contrast to direct methods, we choose reprojection error to construct the optimization energy, which can effectively cope with illumination changes. The distance transform map built upon edge detection for each frame is used to improve tracking efficiency. A novel weighted edge alignment method together with sliding window optimization is proposed to further improve the accuracy. Experiments on public datasets show that the method is comparable to state-of-the-art methods in terms of tracking accuracy, while being faster and more robust.
C1 [Yan, Feihu; Zhou, Zhong] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Li, Zhaoxin] Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
C3 Beihang University; Chinese Academy of Sciences; Institute of Computing
   Technology, CAS
RP Zhou, Z (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM yanfeihu@buaa.edu.cn; cszli@hotmail.com; zz@buaa.edu.cn
FU National Key R&D Program of China [2018YFB2100601]; National Natural
   Science Foundation of China [61872024, 61702482]
FX This work was supported by the National Key R&D Program of China under
   Grant No. 2018YFB2100601, and the National Natural Science Foundation of
   China under Grant Nos. 61872024 and 61702482.
CR [Anonymous], 2006, P BRIT MACH VIS C
   Blöchliger F, 2018, IEEE INT CONF ROBOT, P3818
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Caruso D, 2015, IEEE INT C INT ROBOT, P141, DOI 10.1109/IROS.2015.7353366
   Du Z. -J., 2020, IEEE T VIS COMPUT GR
   Engel J, 2018, IEEE T PATTERN ANAL, V40, P611, DOI 10.1109/TPAMI.2017.2658577
   Engel J, 2015, IEEE INT C INT ROBOT, P1935, DOI 10.1109/IROS.2015.7353631
   Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54
   Engel J, 2013, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2013.183
   Forster C, 2017, IEEE T ROBOT, V33, P249, DOI 10.1109/TRO.2016.2623335
   Gao X, 2018, IEEE INT C INT ROBOT, P2198, DOI 10.1109/IROS.2018.8593376
   Gee AP, 2006, LECT NOTES COMPUT SC, V4292, P354
   Gomez-Ojeda R, 2019, IEEE T ROBOT, V35, P734, DOI 10.1109/TRO.2019.2899783
   Gomez-Ojeda R, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4211, DOI 10.1109/IROS.2016.7759620
   Gomez-Ojeda R, 2016, IEEE INT CONF ROBOT, P2521, DOI 10.1109/ICRA.2016.7487406
   Handa A, 2014, IEEE INT CONF ROBOT, P1524, DOI 10.1109/ICRA.2014.6907054
   Hirose K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.83
   Hsiao Ming, 2017, 2017 IEEE INT C ROBO, P5110
   Huang JH, 2020, PROC CVPR IEEE, P2165, DOI 10.1109/CVPR42600.2020.00224
   Huang SS, 2020, IEEE INT CONF ROBOT, P1091, DOI [10.1109/ICRA40945.2020.9196613, 10.1109/icra40945.2020.9196613]
   Huttenlocher, 2012, THEORY COMPUT, V8, P415, DOI [10.4086/toc.2012.v008a019, DOI 10.4086/TOC.2012.V008A019]
   Klein G, 2008, INT SYM MIX AUGMENT, P57, DOI 10.1109/ISMAR.2008.4637324
   Kuse M, 2016, IEEE INT CONF ROBOT, P573, DOI 10.1109/ICRA.2016.7487181
   Lee SH, 2019, IEEE ROBOT AUTOM LET, V4, P399, DOI 10.1109/LRA.2018.2889156
   Li SJ, 2018, IEEE INT CONF ROBOT, P5137, DOI 10.1109/ICRA.2018.8461003
   Lilian Zhang, 2011, 2011 Irish Machine Vision and Image Processing Conference, P7, DOI 10.1109/IMVIP.2011.11
   Ling FF, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1044, DOI [10.1109/VR.2019.8798315, 10.1109/vr.2019.8798315]
   Liu HM, 2016, INT SYM MIX AUGMENT, P1, DOI 10.1109/ISMAR.2016.24
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Nescher T, 2016, P IEEE VIRT REAL ANN, P239, DOI 10.1109/VR.2016.7504742
   Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513
   Palazzolo E, 2019, IEEE INT C INT ROBOT, P7855, DOI [10.1109/IROS40897.2019.8967590, 10.1109/iros40897.2019.8967590]
   Pumarola A., 2017, P 2017 IEEE INT C RO, P4503, DOI DOI 10.1109/ICRA.2017.7989522
   Schenk F., 2017, P BRIT MACH VIS C
   Schenk F, 2019, IEEE INT CONF ROBOT, P154, DOI [10.1109/icra.2019.8794462, 10.1109/ICRA.2019.8794462]
   Schenk F, 2017, IEEE INT C INT ROBOT, P1297, DOI 10.1109/IROS.2017.8202305
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   von Gioi RG, 2012, IMAGE PROCESS ON LIN, V2, P35, DOI 10.5201/ipol.2012.gjmr-lsd
   Wang R, 2017, IEEE I CONF COMP VIS, P3923, DOI 10.1109/ICCV.2017.421
   Wang XQ, 2016, PROCEEDINGS OF THE 13TH INTENATIONAL SYMPOSIUM OF SUPERALLOYS (SUPERALLOYS 2016), P351
   Yang Y, 2016, IEEE INT CONF ROBOT, P3871, DOI 10.1109/ICRA.2016.7487575
   Zhang JY, 2017, COMM COM INF SC, V773, P477, DOI 10.1007/978-981-10-7305-2_41
   Zhang LL, 2013, J VIS COMMUN IMAGE R, V24, P794, DOI 10.1016/j.jvcir.2013.05.006
   Zhou Y, 2019, IEEE T ROBOT, V35, P184, DOI 10.1109/TRO.2018.2875382
   Zuo XX, 2017, IEEE INT C INT ROBOT, P1775, DOI 10.1109/IROS.2017.8205991
NR 46
TC 2
Z9 2
U1 3
U2 24
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2022
VL 8
IS 3
BP 467
EP 481
DI 10.1007/s41095-021-0251-7
EA MAR 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0Q6DG
UT WOS:000765643300001
OA gold
DA 2024-07-18
ER

PT J
AU Zhang, TL
   Li, JJ
   Fan, H
AF Zhang, Tianlin
   Li, Jinjiang
   Fan, Hui
TI Progressive edge-sensing dynamic scene deblurring
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE image deblurring; dynamic scenes; multi-scale; edge features
AB Deblurring images of dynamic scenes is a challenging task because blurring occurs due to a combination of many factors. In recent years, the use of multi-scale pyramid methods to recover high-resolution sharp images has been extensively studied. We have made improvements to the lack of detail recovery in the cascade structure through a network using progressive integration of data streams. Our new multi-scale structure and edge feature perception design deals with changes in blurring at different spatial scales and enhances the sensitivity of the network to blurred edges. The coarse-to-fine architecture restores the image structure, first performing global adjustments, and then performing local refinement. In this way, not only is global correlation considered, but also residual information is used to significantly improve image restoration and enhance texture details. Experimental results show quantitative and qualitative improvements over existing methods.
C1 [Zhang, Tianlin] Shandong Technol & Business Univ, Sch Informat & Elect Engn, Yantai 264005, Peoples R China.
   [Li, Jinjiang; Fan, Hui] Shandong Technol & Business Univ, Sch Comp Sci & Technol, Yantai 264005, Peoples R China.
   [Zhang, Tianlin] Inst ZhongKe Network Technol, Yantai 264005, Peoples R China.
C3 Shandong Technology & Business University; Shandong Technology &
   Business University
RP Li, JJ (corresponding author), Shandong Technol & Business Univ, Sch Comp Sci & Technol, Yantai 264005, Peoples R China.
EM zhangtianlin@sdtbu.edu.cn; lijinjiang@sdtbu.edu.cn; fanhui@sdtbu.edu.cn
RI Zhang, Tianlin/V-8168-2019
OI Zhang, Tianlin/0000-0003-0843-1916
FU National Natural Science Foundation of China [61772319, 62002200,
   61976125, 61976124]; Shandong Natural Science Foundation of China
   [ZR2017MF049]
FX This research was supported by the National Natural Science Foundation
   of China (61772319, 62002200, 61976125, 61976124), and Shandong Natural
   Science Foundation of China (ZR2017MF049).
CR Bai YC, 2019, IEEE T IMAGE PROCESS, V28, P1404, DOI 10.1109/TIP.2018.2874290
   Chakrabarti A, 2016, LECT NOTES COMPUT SC, V9907, P221, DOI 10.1007/978-3-319-46487-9_14
   Chan TF, 1998, IEEE T IMAGE PROCESS, V7, P370, DOI 10.1109/83.661187
   Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491
   Couzinié-Devy F, 2013, PROC CVPR IEEE, P1075, DOI 10.1109/CVPR.2013.143
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dongwon Park, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P327, DOI 10.1007/978-3-030-58539-6_20
   Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956
   Gao HY, 2019, PROC CVPR IEEE, P3843, DOI 10.1109/CVPR.2019.00397
   Gupta A, 2010, LECT NOTES COMPUT SC, V6311, P171, DOI 10.1007/978-3-642-15549-9_13
   Harmeling Stefan, 2010, P ADV NEUR INF PROC, P829
   Hirsch M, 2011, IEEE I CONF COMP VIS, P463, DOI 10.1109/ICCV.2011.6126276
   Kim TH, 2013, IEEE I CONF COMP VIS, P3160, DOI 10.1109/ICCV.2013.392
   Kingma D. P., 2014, arXiv
   Köhler R, 2012, LECT NOTES COMPUT SC, V7578, P27, DOI 10.1007/978-3-642-33786-4_3
   Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Levin A, 2009, PROC CVPR IEEE, P1964, DOI 10.1109/CVPRW.2009.5206815
   Li JJ, 2021, IEEE T CIRC SYST VID, V31, P4227, DOI 10.1109/TCSVT.2021.3049940
   Li JJ, 2018, IEEE ACCESS, V6, P26831, DOI 10.1109/ACCESS.2018.2833888
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu J, 2021, IEEE T PATTERN ANAL, V43, P1041, DOI 10.1109/TPAMI.2019.2941472
   Mao XJ, 2016, ADV NEUR IN, V29
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Pan JS, 2016, PROC CVPR IEEE, P1628, DOI 10.1109/CVPR.2016.180
   Pan JS, 2014, PROC CVPR IEEE, P2901, DOI 10.1109/CVPR.2014.371
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schuler CJ, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2481418
   Schuler CJ, 2013, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2013.142
   Sun LB, 2013, IEEE INT CONF COMPUT
   Tai YW, 2013, IEEE T PATTERN ANAL, V35, P2498, DOI 10.1109/TPAMI.2013.40
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Valberg A., 2005, Light vision color
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Whyte O, 2012, INT J COMPUT VISION, V98, P168, DOI 10.1007/s11263-011-0502-7
   Xu L, 2014, ADV NEUR IN, V27
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Ye MY, 2020, IEEE ACCESS, V8, P18316, DOI 10.1109/ACCESS.2020.2967823
   Zhang HG, 2019, PROC CVPR IEEE, P5971, DOI 10.1109/CVPR.2019.00613
   Zhang TL, 2021, IET IMAGE PROCESS, V15, P1583, DOI 10.1049/ipr2.12127
NR 41
TC 14
Z9 14
U1 1
U2 3
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2022
VL 8
IS 3
BP 495
EP 508
DI 10.1007/s41095-021-0246-4
EA JAN 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0Q6DG
UT WOS:000740203600001
OA gold
DA 2024-07-18
ER

PT J
AU Li, MP
   Zhou, ZM
   Liu, XG
AF Li, Miaopeng
   Zhou, Zimeng
   Liu, Xinguo
TI 3D hypothesis clustering for cross-view matching in multi-person motion
   capture
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE multi-person motion capture; cross-view matching; clustering; human pose
   estimation
ID POSE ESTIMATION
AB We present a multiview method for markerless motion capture of multiple people. The main challenge in this problem is to determine cross-view correspondences for the 2D joints in the presence of noise. We propose a 3D hypothesis clustering technique to solve this problem. The core idea is to transform joint matching in 2D space into a clustering problem in a 3D hypothesis space. In this way, evidence from photometric appearance, multiview geometry, and bone length can be integrated to solve the clustering problem efficiently and robustly. Each cluster encodes a set of matched 2D joints for the same person across different views, from which the 3D joints can be effectively inferred. We then assemble the inferred 3D joints to form full-body skeletons for all persons in a bottom-up way. Our experiments demonstrate the robustness of our approach even in challenging cases with heavy occlusion, closely interacting people, and few cameras. We have evaluated our method on many datasets, and our results show that it has significantly lower estimation errors than many state-of-the-art methods.
C1 [Li, Miaopeng; Zhou, Zimeng; Liu, Xinguo] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Peoples R China.
C3 Zhejiang University
RP Liu, XG (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Peoples R China.
EM limiaopeng@zju.edu.cn; zmzhou@zju.edu.cn; xgliu@cad.zju.edu.cn
FU National Natural Science Foundation of China [61872317]; FaceUnity
   Technology
FX The authors would like to thank the anonymous reviewers for their
   valuable comments. This work was partially supported by National Natural
   Science Foundation of China (No. 61872317) and FaceUnity Technology.
CR Amin S, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.45
   Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   [Anonymous], 2012, ACM T GRAPHICS, V31, P6
   Belagiannis V, 2016, IEEE T PATTERN ANAL, V38, P1929, DOI 10.1109/TPAMI.2015.2509986
   Belagiannis V, 2014, PROC CVPR IEEE, P1669, DOI 10.1109/CVPR.2014.216
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Dong JT, 2019, PROC CVPR IEEE, P7784, DOI 10.1109/CVPR.2019.00798
   Ershadi-Nasab S, 2018, MULTIMED TOOLS APPL, V77, P15573, DOI 10.1007/s11042-017-5133-8
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3
   Joo H, 2019, IEEE T PATTERN ANAL, V41, P190, DOI 10.1109/TPAMI.2017.2782743
   Joo H, 2015, IEEE I CONF COMP VIS, P3334, DOI 10.1109/ICCV.2015.381
   Kadkhodamohammadi A, 2018, PREPRINT
   Levenberg K., 1944, Quarterly of Applied Mathematics, V2, P164, DOI [10.1090/QAM/10666, DOI 10.1090/QAM/10666]
   Li MP, 2019, IEEE T MULTIMEDIA, V21, P2653, DOI 10.1109/TMM.2019.2903455
   Li MP, 2018, INT C PATT RECOG, P115, DOI 10.1109/ICPR.2018.8546194
   MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Wang HS, 2018, PALAEONTOL ELECTRON, V21, DOI 10.26879/841
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541
   Zhou TH, 2015, PROC CVPR IEEE, P1191, DOI 10.1109/CVPR.2015.7298723
   Zhou XW, 2015, IEEE I CONF COMP VIS, P4032, DOI 10.1109/ICCV.2015.459
NR 25
TC 6
Z9 6
U1 1
U2 6
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2020
VL 6
IS 2
BP 147
EP 156
DI 10.1007/s41095-020-0171-y
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FB
UT WOS:000648691300003
OA gold
DA 2024-07-18
ER

PT J
AU Doyle, L
   Anderson, F
   Choy, E
   Mould, D
AF Doyle, Lars
   Anderson, Forest
   Choy, Ehren
   Mould, David
TI Automated pebble mosaic stylization of images
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE non-photorealistic rendering; digital mosaics; image stylization;
   segmentation; image processing
AB Digital mosaics have usually used regular tiles, simulating historical tessellated mosaics. In this paper, we present a method for synthesizing pebble mosaics, a historical mosaic style in which the tiles are rounded pebbles. We address both the tiling problem, of distributing pebbles over the image plane so as to approximate the input image content, and the problem of geometry, creating a smooth rounded shape for each pebble. We adopt simple linear iterative clustering (SLIC) to obtain elongated tiles conforming to image content, and smooth the resulting irregular shapes into shapes resembling pebble cross-sections. Then, we create an interior and exterior contour for each pebble and solve a Laplace equation over the region between them to obtain height-field geometry. The resulting pebble set approximates the input image while representing full geometry that can be rendered and textured for a highly detailed representation of a pebble mosaic.
C1 [Doyle, Lars] Carleton Univ, Sch Comp Sci, Ottawa, ON, Canada.
   [Doyle, Lars] Carleton Univ, Graph Imaging & Games Lab, Ottawa, ON, Canada.
   [Anderson, Forest; Choy, Ehren; Mould, David] Carleton Univ, Ottawa, ON, Canada.
   [Mould, David] Carleton Univ, Sch Comp Sci, Graph Imaging & Games Lab, Ottawa, ON, Canada.
C3 Carleton University; Carleton University; Carleton University; Carleton
   University
RP Doyle, L (corresponding author), Carleton Univ, Sch Comp Sci, Ottawa, ON, Canada.; Doyle, L (corresponding author), Carleton Univ, Graph Imaging & Games Lab, Ottawa, ON, Canada.
EM lars.doyle@carleton.ca
FU NSERC; OGS; Carleton University
FX We would like to thank the anonymous reviewers for many insightful
   comments. We also thank members of the Graphics, Imaging and Games Lab
   for productive comments and discussions. Funding for this work was
   provided by NSERC, OGS, and Carleton University.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   ACM, 2016, ACM T GRAPHICS, V35, P6
   [Anonymous], 2002, P 2 INT S NONPH AN R, DOI DOI 10.1145/508535.508537
   Battiato S, 2007, COMPUT GRAPH FORUM, V26, P794, DOI 10.1111/j.1467-8659.2007.01021.x
   Brox T, 2006, VISUALIZATION AND PROCESSING OF TENSOR FIELDS, P17, DOI 10.1007/3-540-31272-2_2
   Dalal K., 2006, Proceedings of the 4th international symposium on Nonphotorealistic animation and rendering, P71, DOI 10.1145/1124728.1124741
   Di Blasi G, 2005, VISUAL COMPUT, V21, P373, DOI 10.1007/s00371-005-0292-4
   Dunbabin Katherine M. D., 1999, Mosaics of the Greek and Roman World
   Elber G, 2003, VISUAL COMPUT, V19, P67, DOI 10.1007/s00371-002-0175-x
   Goferman S, 2010, COMPUT GRAPH FORUM, V29, P459, DOI 10.1111/j.1467-8659.2009.01615.x
   Gonzalez R C, 2008, DIGITAL IMAGE PROCES
   Haeberli P., 1990, Computer Graphics, V24, P207, DOI 10.1145/97880.97902
   Hausner A, 2001, COMP GRAPH, P573, DOI 10.1145/383259.383327
   Hiller S, 2003, COMPUT GRAPH FORUM, V22, P515, DOI 10.1111/1467-8659.00699
   Howarth M., 2003, COMPLETE PEBBLE MOSA
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kaplan CS, 2004, PROC GRAPH INTERF, P255
   Kaplan CS, 2000, COMP GRAPH, P499, DOI 10.1145/344779.345022
   Kim J, 2002, ACM T GRAPHIC, V21, P657
   Ling Robert., 1998, ANCIENT MOSAICS
   Liu LJ, 2018, IEEE T VIS COMPUT GR, V24, P1956, DOI 10.1109/TVCG.2017.2703853
   Liu Y, 2010, COMPUT GRAPH FORUM, V29, P2387, DOI 10.1111/j.1467-8659.2010.01752.x
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Saputra R A., 2018, REPULSIONPAK DEFORMA, P10
   Saputra R A., 2017, FLOWPAK FLOW BASED O, P8
   Schlechtweg S, 2005, COMPUT GRAPH FORUM, V24, P137, DOI 10.1111/j.1467-8659.2005.00838.x
   Silvers R., 1997, Photomosaics
   Smith K., 2005, ANIMOSAICS, P201
NR 28
TC 8
Z9 8
U1 1
U2 2
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2019
VL 5
IS 1
BP 33
EP 44
DI 10.1007/s41095-019-0129-0
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UL
UT WOS:000648690700004
OA Green Submitted, gold
DA 2024-07-18
ER

PT J
AU Jing, YP
   Lu, XQ
   Gao, S
AF Jing, Yaping
   Lu, Xuequan
   Gao, Shang
TI 3D face recognition: A comprehensive survey in 2022
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review
DE 3D face recognition; 3D face databases; deep learning; local features;
   global features
ID FACIAL EXPRESSION; LOCAL DESCRIPTORS; INVARIANT; EFFICIENT; FEATURES;
   DATABASE; REPRESENTATION; MULTISCALE; SCALE; SHAPE
AB In the past ten years, research on face recognition has shifted to using 3D facial surfaces, as 3D geometric information provides more discriminative features. This comprehensive survey reviews 3D face recognition techniques developed in the past decade, both conventional methods and deep learning methods. These methods are evaluated with detailed descriptions of selected representative works. Their advantages and disadvantages are summarized in terms of accuracy, complexity, and robustness to facial variations (expression, pose, occlusion, etc.). A review of 3D face databases is also provided, and a discussion of future research challenges and directions of the topic.
C1 [Jing, Yaping; Lu, Xuequan; Gao, Shang] Deakin Univ, Sch Informat Technol, Waurn Ponds, Vic, Australia.
C3 Deakin University
RP Lu, XQ (corresponding author), Deakin Univ, Sch Informat Technol, Waurn Ponds, Vic, Australia.
EM jingyap@deakin.edu.au; xuequan.lu@deakin.edu.au; shang.gao@deakin.edu.au
RI zhang, jt/JVE-1333-2024; li, tao/JVO-9006-2024; Li, YiXue/JRW-6306-2023;
   wang, jiaqi/JSL-7112-2023; Lu, Lu/JPE-5187-2023; Zhang,
   Chi/JSK-0744-2023; Wei, Wei/JVM-8876-2024; Chen, Chao/JHS-6563-2023;
   Yang, Min/JPY-3791-2023; Lin, Xiaoqi/KFS-5750-2024; Chen,
   Fang/JZE-4446-2024; Wang, Xiaojun/JUU-9683-2023; ZHU,
   JIALI/JNE-3065-2023; cheng, chen/JHS-9462-2023
OI Wei, Wei/0000-0002-4109-3878; Lu, Xuequan/0000-0003-0959-408X
CR Abate AF, 2007, PATTERN RECOGN LETT, V28, P1885, DOI 10.1016/j.patrec.2006.12.018
   Abbad A, 2018, COMPUT ELECTR ENG, V70, P525, DOI 10.1016/j.compeleceng.2017.08.017
   Al-Osaimi FR, 2016, IEEE T IMAGE PROCESS, V25, P658, DOI 10.1109/TIP.2015.2492826
   Alyuz N., 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P111, DOI 10.1109/ICB.2012.6199767
   Alyuz N, 2013, IEEE T INF FOREN SEC, V8, P789, DOI 10.1109/TIFS.2013.2256130
   [Anonymous], 2011, 2011 INT JOINT C BIO, DOI DOI 10.1109/IJCB.2011.6117521
   [Anonymous], 2011, Biometrics (IJCB), 2011 International Joint Conference on
   [Anonymous], 2009, 2009 sixth IEEE international conference on advanced video and signal based surveillance, DOI DOI 10.1109/AVSS.2009.58
   Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Bagchi P., 2014, ARXIV
   Bagchi P., 2015, TENCON 2015 2015 IEE, P1
   Ballihi L, 2012, IEEE T INF FOREN SEC, V7, P1766, DOI 10.1109/TIFS.2012.2209876
   Berretti S., 2011, P JOINT ACM WORKSH H, P65
   Berretti S, 2014, VISUAL COMPUT, V30, P1275, DOI 10.1007/s00371-014-0932-7
   Berretti S, 2013, COMPUT GRAPH-UK, V37, P509, DOI 10.1016/j.cag.2013.04.001
   Berretti S, 2013, IEEE T INF FOREN SEC, V8, P374, DOI 10.1109/TIFS.2012.2235833
   Beumier C, 2000, IMAGE VISION COMPUT, V18, P315, DOI 10.1016/S0262-8856(99)00052-9
   Bhople AR, 2021, MULTIMED TOOLS APPL, V80, P35973, DOI 10.1007/s11042-020-10160-9
   Bhople AR, 2021, MULTIMED TOOLS APPL, V80, P30237, DOI 10.1007/s11042-020-09008-z
   Blackburn D.M., 2001, Facial Recognition Vendor Test 2000, DOI DOI 10.21236/ADA415962
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Bowyer KW, 2006, COMPUT VIS IMAGE UND, V101, P1, DOI 10.1016/j.cviu.2005.05.005
   Bruna J., 2014, ABS13126203 CORR, P1, DOI [10.48550/arXiv.1312.6203, DOI 10.48550/ARXIV.1312.6203]
   Cai Y, 2019, NEUROCOMPUTING, V363, P375, DOI 10.1016/j.neucom.2019.07.047
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Castellani U., 2020, 3D Imaging, Analysis and Applications, P353
   Chang KI, 2005, IEEE T PATTERN ANAL, V27, P619, DOI 10.1109/TPAMI.2005.70
   Cheng SY, 2018, PROC CVPR IEEE, P5117, DOI 10.1109/CVPR.2018.00537
   Chiu M.-T., 2021, P 16 IEEE INT C AUT, P1
   Chuqi Cao, 2020, 2020 7th International Conference on Signal Processing and Integrated Networks (SPIN), P696, DOI 10.1109/SPIN48934.2020.9071409
   Ciravegna G, 2020, SMART INNOV SYST TEC, V151, P223, DOI 10.1007/978-981-13-8950-4_21
   Colombo A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P2113, DOI 10.1109/ICCVW.2011.6130509
   Conde C, 2006, IEEE IMAGE PROC, P2061, DOI 10.1109/ICIP.2006.312863
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Creusot C., 2011, 2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT), P204, DOI 10.1109/3DIMPVT.2011.33
   Creusot C, 2013, INT J COMPUT VISION, V102, P146, DOI 10.1007/s11263-012-0605-9
   D'Errico J., 2005, SURFACE FITTING USIN
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Deng JK, 2018, PROC CVPR IEEE, P7093, DOI 10.1109/CVPR.2018.00741
   Deng X, 2020, COMPUT ELECTR ENG, V85, DOI 10.1016/j.compeleceng.2020.106700
   Deng X, 2017, COMPUT ELECTR ENG, V62, P81, DOI 10.1016/j.compeleceng.2017.01.028
   Dou P, 2017, PROC CVPR IEEE, P1503, DOI 10.1109/CVPR.2017.164
   Drira H, 2013, IEEE T PATTERN ANAL, V35, P2270, DOI 10.1109/TPAMI.2013.48
   Drira Hassen., 2010, BMVC, P1
   Dutta K, 2020, MULTIMED TOOLS APPL, V79, P31329, DOI 10.1007/s11042-020-09554-6
   Elaiwat S, 2015, PATTERN RECOGN, V48, P1235, DOI 10.1016/j.patcog.2014.10.013
   Elaiwat S, 2014, IEEE SIGNAL PROC LET, V21, P172, DOI 10.1109/LSP.2013.2295119
   Emambakhsh M, 2017, IEEE T PATTERN ANAL, V39, P995, DOI 10.1109/TPAMI.2016.2565473
   Fadaifard H, 2013, GRAPH MODELS, V75, P157, DOI 10.1016/j.gmod.2013.01.002
   Faltemier TC, 2007, 2007 FIRST IEEE INTERNATIONAL CONFERENCE ON BIOMETRICS: THEORY, APPLICATIONS AND SYSTEMS, P19
   Farkas LG, 1994, Anthropometry of Head and Face, Vsecond
   Feng JY, 2019, ADV INTELL SYST COMP, V670, P123, DOI 10.1007/978-981-10-8971-8_12
   Freund Y., 1999, Journal of Japanese Society for Artificial Intelligence, V14, P771
   Gilani S. Z., 2016, 2016 INT C DIGITAL I, P1
   Gilani SZ, 2018, PROC CVPR IEEE, P1896, DOI 10.1109/CVPR.2018.00203
   Gilani SZ, 2018, IEEE T PATTERN ANAL, V40, P1584, DOI 10.1109/TPAMI.2017.2725279
   Gilani SZ, 2017, PATTERN RECOGN, V69, P238, DOI 10.1016/j.patcog.2017.04.013
   Guo GD, 2019, COMPUT VIS IMAGE UND, V189, DOI 10.1016/j.cviu.2019.102805
   Guo YL, 2016, PATTERN RECOGN LETT, V83, P403, DOI 10.1016/j.patrec.2016.04.003
   Gupta SD, 2010, METHODS MOL BIOL, V589, P97, DOI [10.1007/978-1-60327-114-1_10, 10.1109/SSIAI.2010.5483908]
   Hariri W, 2016, PATTERN RECOGN LETT, V78, P1, DOI 10.1016/j.patrec.2016.03.028
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heseltine T, 2008, IMAGE VISION COMPUT, V26, P382, DOI 10.1016/j.imavis.2006.12.008
   Hesher C, 2003, SEVENTH INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND ITS APPLICATIONS, VOL 2, PROCEEDINGS, P201, DOI 10.1109/ISSPA.2003.1224850
   Howard A., 2018, ARXIV
   Huang D, 2012, IEEE T INF FOREN SEC, V7, P1551, DOI 10.1109/TIFS.2012.2206807
   Huang G B, 2008, P WORKSHOP FACESREAL
   Huibin Li, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3053, DOI 10.1109/ICIP.2011.6116308
   Inan T, 2012, IEEE T INF FOREN SEC, V7, P577, DOI 10.1109/TIFS.2012.2186293
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jia S., 2020, ARXIV
   Jiang C., 2021, 2021 IEEE INT JOINT, P1
   Jiang L, 2020, IEEE T PATTERN ANAL, V42, P2552, DOI 10.1109/TPAMI.2019.2919284
   Kangming Xu, 2019, 2019 IEEE 19th International Conference on Communication Technology (ICCT), P1665, DOI 10.1109/ICCT46805.2019.8947113
   Kim D, 2017, 2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB), P133, DOI 10.1109/BTAS.2017.8272691
   Lawrence S, 1997, IEEE T NEURAL NETWOR, V8, P98, DOI 10.1109/72.554195
   Lei YJ, 2016, PATTERN RECOGN, V52, P218, DOI 10.1016/j.patcog.2015.09.035
   Lei YJ, 2014, PATTERN RECOGN, V47, P509, DOI 10.1016/j.patcog.2013.07.018
   Lei YJ, 2013, PATTERN RECOGN, V46, P24, DOI 10.1016/j.patcog.2012.06.023
   Li BYL, 2013, IEEE WORK APP COMP, P186, DOI 10.1109/WACV.2013.6475017
   Li HB, 2015, INT J COMPUT VISION, V113, P128, DOI 10.1007/s11263-014-0785-6
   Li HB, 2014, NEUROCOMPUTING, V133, P179, DOI 10.1016/j.neucom.2013.11.018
   Li Q, 2021, 2021 5TH IEEE ELECTRON DEVICES TECHNOLOGY & MANUFACTURING CONFERENCE (EDTM), DOI [10.1109/EDTM50988.2021.9420992, 10.1145/3465332.3470877]
   Li XL, 2012, IMAGE VISION COMPUT, V30, P668, DOI 10.1016/j.imavis.2012.07.011
   Liang Y, 2017, SIGNAL PROCESS-IMAGE, V57, P84, DOI 10.1016/j.image.2017.05.004
   Lin S., 2019, 2019 14 IEEE INT C A, P1
   Lin SS, 2022, PLATELETS, V33, P543, DOI 10.1080/09537104.2021.1945569
   Liu PJ, 2013, IEEE T IMAGE PROCESS, V22, P914, DOI 10.1109/TIP.2012.2222897
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo MD, 2021, IEEE T INF FOREN SEC, V16, P2341, DOI 10.1109/TIFS.2021.3053460
   Lüthi M, 2018, IEEE T PATTERN ANAL, V40, P1860, DOI 10.1109/TPAMI.2017.2739743
   Mark TAN Y. C., 2019, 2019 13th European Conference on Antennas and Propagation, P1
   Marriott RT, 2021, PROC CVPR IEEE, P13440, DOI 10.1109/CVPR46437.2021.01324
   Masi I, 2018, SIBGRAPI, P471, DOI 10.1109/SIBGRAPI.2018.00067
   Min R, 2014, IEEE T SYST MAN CY-S, V44, P1534, DOI 10.1109/TSMC.2014.2331215
   Ming Y, 2015, IMAGE VISION COMPUT, V35, P14, DOI 10.1016/j.imavis.2014.12.003
   Ming Y, 2014, NEUROCOMPUTING, V129, P445, DOI 10.1016/j.neucom.2013.09.014
   Ming Y, 2012, IMAGE VISION COMPUT, V30, P524, DOI 10.1016/j.imavis.2012.05.001
   Mohammadzade H, 2013, IEEE T PATTERN ANAL, V35, P381, DOI 10.1109/TPAMI.2012.107
   Moreno A.B., 2004, WHITENING RACE ESSAY, P75
   Mu GD, 2019, PROC CVPR IEEE, P5766, DOI 10.1109/CVPR.2019.00592
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Olivetti EC, 2020, LECT N MECH ENG, P665, DOI 10.1007/978-3-030-31154-4_56
   Papadopoulos K., 2021, ARXIV
   Parkhi OM, 2015, Proceedings of the British Machine Vision Conference, DOI DOI 10.5244/C.29.41
   Passalis G, 2011, IEEE T PATTERN ANAL, V33, P1938, DOI 10.1109/TPAMI.2011.49
   Patil H, 2015, ARTIF INTELL REV, V44, P393, DOI 10.1007/s10462-015-9431-0
   Peter M, 2019, LECT NOTES ELECTR EN, V481, P77, DOI 10.1007/978-981-13-2622-6_8
   Phillips P. J., 2003, 2003 IEEE International Workshop on Analysis and Modeling of Faces and Gestures
   Phillips PJ, 2010, IEEE T PATTERN ANAL, V32, P831, DOI 10.1109/TPAMI.2009.59
   Phillips PJ, 2005, PROC CVPR IEEE, P947
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Ratyal NI, 2015, COMPUT ELECTR ENG, V46, P241, DOI 10.1016/j.compeleceng.2015.06.007
   Richardson E, 2016, INT CONF 3D VISION, P460, DOI 10.1109/3DV.2016.56
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Samir C, 2006, IEEE T PATTERN ANAL, V28, P1858, DOI 10.1109/TPAMI.2006.235
   Samir C, 2009, INT J COMPUT VISION, V82, P80, DOI 10.1007/s11263-008-0187-8
   Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shabayek AR, 2020, INT CONF ACOUST SPEE, P2138, DOI [10.1109/icassp40776.2020.9054733, 10.1109/ICASSP40776.2020.9054733]
   Shen YJ, 2018, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2018.00092
   Shi LL, 2020, OPTIK, V220, DOI 10.1016/j.ijleo.2020.165157
   Smeets D, 2013, COMPUT VIS IMAGE UND, V117, P158, DOI 10.1016/j.cviu.2012.10.002
   Smeets D, 2012, IEEE T SYST MAN CY C, V42, P710, DOI 10.1109/TSMCC.2011.2174221
   Smith M., 2020, P 16 INT C MACH LEAR, P209
   Soltanpour S, 2019, IEEE T IMAGE PROCESS, V28, P3020, DOI 10.1109/TIP.2019.2893524
   Soltanpour S, 2017, IEEE IMAGE PROC, P2811, DOI 10.1109/ICIP.2017.8296795
   Soltanpour S, 2017, PATTERN RECOGN, V72, P391, DOI 10.1016/j.patcog.2017.08.003
   Soltanpour S, 2017, IET BIOMETRICS, V6, P27, DOI 10.1049/iet-bmt.2015.0120
   Spreeuwers L, 2011, INT J COMPUT VISION, V93, P389, DOI 10.1007/s11263-011-0426-2
   Sun Y, 2015, ARXIV
   Sun Y, 2014, ADV NEUR IN, V27
   Sun Y, 2015, PROC CVPR IEEE, P2892, DOI 10.1109/CVPR.2015.7298907
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Tabia H, 2014, PROC CVPR IEEE, P4185, DOI 10.1109/CVPR.2014.533
   Taghizadegan Y., 2012, ACEEE INT J CONTROL, V3, P1
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tang HL, 2013, SIGNAL PROCESS, V93, P2190, DOI 10.1016/j.sigpro.2012.04.002
   Tang YH, 2015, INT CONF BIOMETR, P466, DOI 10.1109/ICB.2015.7139111
   ter Haar FB, 2008, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2008, PROCEEDINGS, P225
   URBANOVA P, 2018, ANTHROPOL REV, V81, P202, DOI DOI 10.2478/anre-2018-0016
   Veltkamp R., 2011, P EUR WORKSH 3D OBJ, P89
   Vezzetti E, 2018, MULTIMED TOOLS APPL, V77, P14177, DOI 10.1007/s11042-017-5025-y
   Vezzetti E, 2014, ROBOT AUTON SYST, V62, P1768, DOI 10.1016/j.robot.2014.07.009
   Wang F, 2018, IEEE SIGNAL PROC LET, V25, P926, DOI 10.1109/LSP.2018.2822810
   Wang YM, 2006, LECT NOTES COMPUT SC, V3851, P581
   Werghi N, 2016, IEEE T INF FOREN SEC, V11, P964, DOI 10.1109/TIFS.2016.2515505
   Xinyu Zhang, 2020, 2020 Proceedings of IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA), P162, DOI 10.1109/ICAICA50127.2020.9182406
   Xu CH, 2006, LECT NOTES COMPUT SC, V3952, P416
   Yang HT, 2020, PROC CVPR IEEE, P598, DOI 10.1109/CVPR42600.2020.00068
   Ye YP, 2020, IEEE ACCESS, V8, P48205, DOI 10.1109/ACCESS.2020.2979518
   [尹宝才 Yin Baocai], 2009, [计算机研究与发展, Journal of Computer Research and Development], V46, P1009
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Yu Y, 2019, IEEE T INF FOREN SEC, V14, P1917, DOI 10.1109/TIFS.2018.2889255
   Yuqi Ding, 2019, Advances in Visual Computing. 14th International Symposium on Visual Computing, ISVC 2019. Proceedings. Lecture Notes in Computer Science (LNCS 11844), P430, DOI 10.1007/978-3-030-33720-9_33
   Zhang GP, 2011, PATTERN RECOGN LETT, V32, P1009, DOI 10.1016/j.patrec.2011.02.004
   Zhang JJ, 2016, INT CONF BIOMETR
   [张醒 Zhang Xing], 2013, [火工品, Initiators & Pyrotechnics], P1
   Zhang YN, 2012, CHINESE J ELECTRON, V21, P283
   Zhang Z, 2019, arXiv
   Zhang ZY, 2022, PATTERN RECOGN, V123, DOI 10.1016/j.patcog.2021.108394
   Zhao J, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1184
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
   Zhou HL, 2014, IEEE T HUM-MACH SYST, V44, P701, DOI 10.1109/THMS.2014.2340578
   Zhou S, 2018, HUM-CENT COMPUT INFO, V8, DOI 10.1186/s13673-018-0157-2
NR 165
TC 3
Z9 3
U1 14
U2 28
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2023
VL 9
IS 4
BP 657
EP 685
DI 10.1007/s41095-022-0317-1
EA AUG 2023
PG 29
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P1QC2
UT WOS:001043349800001
OA gold
DA 2024-07-18
ER

PT J
AU Shi, ZQ
   Lin, XY
   Song, Y
AF Shi, Zeqi
   Lin, Xiangyu
   Song, Ying
TI An attention-embedded GAN for SVBRDF recovery from a single image
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE spatially-varying bidirectional reflectance distribution function
   (SVBRDF); appearance capture; generative adversarial network (GAN);
   attention mechanism
AB Learning-based approaches have made substantial progress in capturing spatially-varying bidirectional reflectance distribution functions (SVBRDFs) from a single image with unknown lighting and geometry. However, most existing networks only consider per-pixel losses which limit their capability to recover local features such as smooth glossy regions. A few generative adversarial networks use multiple discriminators for different parameter maps, increasing network complexity. We present a novel end-to-end generative adversarial network (GAN) to recover appearance from a single picture of a nearly-flat surface lit by flash. We use a single unified adversarial framework for each parameter map. An attention module guides the network to focus on details of the maps. Furthermore, the SVBRDF map loss is combined to prevent paying excess attention to specular highlights. We demonstrate and evaluate our method on both public datasets and real data. Quantitative analysis and visual comparisons indicate that our method achieves better results than the state-of-the-art in most cases.
C1 [Shi, Zeqi; Lin, Xiangyu; Song, Ying] Zhejiang Sci Tech Univ, Collaborat Innovat Ctr Garment Personal Customizat, Hangzhou 310018, Peoples R China.
C3 Zhejiang Sci-Tech University
RP Song, Y (corresponding author), Zhejiang Sci Tech Univ, Collaborat Innovat Ctr Garment Personal Customizat, Hangzhou 310018, Peoples R China.
EM shizeqi6@gmail.com; linxiangyu@zstu.edu.cn; ysong@zstu.edu.cn
RI Lin, Xiangyu/C-6929-2015
FU National Natural Science Foundation of China [61602416]; Shaoxing
   Science and Technology Plan Project [2020B41006]
FX AcknowledgementsThe authors would like to thank Jie Guo from Nanjing
   University for his kind help with the comparison. Ying Song was
   partially supported by the National Natural Science Foundation of China
   (No. 61602416) and Shaoxing Science and Technology Plan Project (No.
   2020B41006).
CR Aittala M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925917
   Aittala M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766967
   Asselin LP, 2020, INT CONF 3D VISION, P1157, DOI 10.1109/3DV50981.2020.00126
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Boivin S, 2002, CGIV'2002: FIRST EUROPEAN CONFERENCE ON COLOUR IN GRAPHICS, IMAGING, AND VISION, CONFERENCE PROCEEDINGS, P268
   Chandraker M, 2014, LECT NOTES COMPUT SC, V8695, P202, DOI 10.1007/978-3-319-10584-0_14
   Chaudhari S, 2021, ACM T INTEL SYST TEC, V12, DOI 10.1145/3465055
   Chorowski J, 2015, Arxiv, DOI arXiv:1506.07503
   Cook R. L., 1982, ACM T GRAPHIC, V1, P7, DOI DOI 10.1145/357290.357293
   Deschaintre V, 2020, COMPUT GRAPH FORUM, V39, P91, DOI 10.1111/cgf.14056
   Deschaintre V, 2019, COMPUT GRAPH FORUM, V38, P1, DOI 10.1111/cgf.13765
   Deschaintre V, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201378
   Dong Y, 2019, VIS INFORM, V3, P59, DOI 10.1016/j.visinf.2019.07.003
   Dorsey J., 2010, Digital modeling of material appearance
   Fu G, 2021, PROC CVPR IEEE, P7748, DOI 10.1109/CVPR46437.2021.00766
   Gao D, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323042
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guarnera D, 2016, COMPUT GRAPH FORUM, V35, P625, DOI 10.1111/cgf.12867
   Guo J, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459854
   Guo Y., 2020, arXiv
   Hui Z, 2015, IEEE INT CONF COMPUT
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Kingma D. P, 2015, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1412.6980
   Li X, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073641
   Li ZQ, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275055
   Li ZQ, 2018, LECT NOTES COMPUT SC, V11207, P74, DOI 10.1007/978-3-030-01219-9_5
   Maas Andrew L, 2013, P INT C MACH LEARN A, V30, P3
   Maini R, 2010, Arxiv, DOI arXiv:1003.4053
   Mnih V, 2014, ADV NEUR IN, V27
   Nguyen TV, 2018, INT J COMPUT VISION, V126, P86, DOI 10.1007/s11263-017-1042-6
   Riviere J, 2016, COMPUT GRAPH FORUM, V35, P191, DOI 10.1111/cgf.12719
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Team T, 2015, TENSORFLOW LARGE SCA
   Wang ZB, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417824
   Weinmann Michael, 2015, SIGGRAPH ASIA COURSE, P1
   Weyrich T, 2008, FOUND TRENDS COMPUT, V4, P75, DOI 10.1561/0600000022
   Xia R, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980248
   Xu ZX, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982396
   Ye WJ, 2018, COMPUT GRAPH FORUM, V37, P201, DOI 10.1111/cgf.13560
   Zhao Y. Z., 2020, EGSR (DL), P53
   Zhou B., 2014, arXiv
   Zhou XL, 2021, COMPUT GRAPH FORUM, V40, P315, DOI 10.1111/cgf.142635
NR 43
TC 0
Z9 0
U1 0
U2 3
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2023
VL 9
IS 3
BP 551
EP 561
DI 10.1007/s41095-022-0289-1
EA MAR 2023
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA G9DP6
UT WOS:000953957100001
OA gold
DA 2024-07-18
ER

PT J
AU Zeng, XX
   Wu, ZL
   Peng, XJ
   Qiao, Y
AF Zeng, Xiaoxing
   Wu, Zhelun
   Peng, Xiaojiang
   Qiao, Yu
TI Joint 3D facial shape reconstruction and texture completion from a
   single image
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE 3D face reconstruction; U-V completion; pose invariant face recognition;
   deep learning
ID FACE RECONSTRUCTION; MODEL
AB Recent years have witnessed significant progress in image-based 3D face reconstruction using deep convolutional neural networks. However, current reconstruction methods often perform improperly in self-occluded regions and can lead to inaccurate correspondences between a 2D input image and a 3D face template, hindering use in real applications. To address these problems, we propose a deep shape reconstruction and texture completion network, SRTC-Net, which jointly reconstructs 3D facial geometry and completes texture with correspondences from a single input face image. In SRTC-Net, we leverage the geometric cues from completed 3D texture to reconstruct detailed structures of 3D shapes. The SRTC-Net pipeline has three stages. The first introduces a correspondence network to identify pixel-wise correspondence between the input 2D image and a 3D template model, and transfers the input 2D image to a U-V texture map. Then we complete the invisible and occluded areas in the U-V texture map using an inpainting network. To get the 3D facial geometries, we predict coarse shape (U-V position maps) from the segmented face from the correspondence network using a shape network, and then refine the 3D coarse shape by regressing the U-V displacement map from the completed U-V texture map in a pixel-to-pixel way. We examine our methods on 3D reconstruction tasks as well as face frontalization and pose invariant face recognition tasks, using both in-the-lab datasets (MICC, MultiPIE) and in-the-wild datasets (CFP). The qualitative and quantitative results demonstrate the effectiveness of our methods on inferring 3D facial geometry and complete texture; they outperform or are comparable to the state-of-the-art.
C1 [Zeng, Xiaoxing; Wu, Zhelun; Peng, Xiaojiang; Qiao, Yu] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China.
   [Zeng, Xiaoxing] Univ Chinese Acad Sci, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS
RP Qiao, Y (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China.
EM xx.zeng@siat.ac.cn; zl.wu@siat.ac.cn; xj.peng@siat.ac.cn;
   yu.qiao@siat.ac.cn
FU National Natural Science Foundation of China [U1613211, U1813218];
   Shenzhen Research Program [JCYJ20170818164704758, JCYJ20150925163005055]
FX This work was supported by the National Natural Science Foundation of
   China (Nos. U1613211 and U1813218), and Shenzhen Research Program (Nos.
   JCYJ20170818164704758 and JCYJ20150925163005055). We would like to thank
   Yu Deng et al. for their Deep 3D Face work in 3D face analysis, whose
   contribution to this field permitted our further study.
CR Alexander O, 2010, IEEE COMPUT GRAPH, V30, P20, DOI 10.1109/MCG.2010.65
   [Anonymous], 2017, ACM T GRAPHIC, V36
   [Anonymous], 2009, State of the Art in Example-Based Texture Synthesis R
   [Anonymous], CoRR abs/1511.07122
   [Anonymous], 2014, COMPUTER GRAPHICS PR
   [Anonymous], 2015, ACM T GRAPHIC, V34
   [Anonymous], 2008, P 8 IEEE INT C AUT F
   Asthana A, 2011, IEEE I CONF COMP VIS, P937, DOI 10.1109/ICCV.2011.6126336
   Bagdanov Andrew D, 2011, P JOINT ACM WORKSH H
   Baltrusaitis T, 2018, IEEE INT CONF AUTOMA, P59, DOI 10.1109/FG.2018.00019
   Bas A, 2017, IEEE INT CONF COMP V, P895, DOI 10.1109/ICCVW.2017.110
   Bessaoudi M, 2019, APPL INTELL, V49, P1339, DOI 10.1007/s10489-018-1318-8
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Cao KD, 2018, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2018.00544
   Catmull E. E., 1974, A subdivision algorithm for computer display of curved surfaces
   Chang F. J., 2019, ARXIV PREPRINT ARXIV
   Chen D, 2012, LECT NOTES COMPUT SC, V7574, P566, DOI 10.1007/978-3-642-33712-3_41
   Cootes T. F., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P227, DOI 10.1109/AFGR.2000.840639
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng JK, 2018, PROC CVPR IEEE, P7093, DOI 10.1109/CVPR.2018.00741
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   Devi PRS, 2021, APPL INTELL, V51, P2253, DOI 10.1007/s10489-020-02000-y
   Ding CX, 2017, PATTERN RECOGN, V66, P144, DOI 10.1016/j.patcog.2016.11.024
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33
   Gecer B, 2019, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2019.00125
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   Hassner T, 2015, PROC CVPR IEEE, P4295, DOI 10.1109/CVPR.2015.7299058
   Hormann K., 2007, ACM Siggraph 2007 Course, V11, P1
   Hu YB, 2018, PROC CVPR IEEE, P8398, DOI 10.1109/CVPR.2018.00876
   Huang R, 2017, IEEE I CONF COMP VIS, P2458, DOI 10.1109/ICCV.2017.267
   Jackson AS, 2017, IEEE I CONF COMP VIS, P1031, DOI 10.1109/ICCV.2017.117
   Jianzhu Guo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P152, DOI 10.1007/978-3-030-58529-7_10
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Lattas A, 2020, PROC CVPR IEEE, P757, DOI 10.1109/CVPR42600.2020.00084
   Lee GH, 2020, PROC CVPR IEEE, P6099, DOI 10.1109/CVPR42600.2020.00614
   Li AN, 2012, IEEE T IMAGE PROCESS, V21, P305, DOI 10.1109/TIP.2011.2160957
   Li YJ, 2017, PROC CVPR IEEE, P5892, DOI 10.1109/CVPR.2017.624
   Li Y, 2018, PROCEEDINGS CVMP 2018: THE 15TH ACM SIGGRAPH EUROPEAN CONFERENCE ON VISUAL MEDIA PRODUCTION, DOI 10.1145/3278471.3278473
   Lin JK, 2020, PROC CVPR IEEE, P5890, DOI 10.1109/CVPR42600.2020.00593
   Tran L, 2018, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2018.00767
   Ma LM, 2019, ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES (I3D 2019), DOI 10.1145/3306131.3317016
   Mescheder L, 2018, PR MACH LEARN RES, V80
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Qian YC, 2019, PROC CVPR IEEE, P9843, DOI 10.1109/CVPR.2019.01008
   Richardson E, 2017, PROC CVPR IEEE, P5553, DOI 10.1109/CVPR.2017.589
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sankaranarayanan S., 2016, 2016 IEEE 8th international conference on biometrics theory, applications and systems (BTAS), P1
   Sanyal S, 2019, PROC CVPR IEEE, P7755, DOI 10.1109/CVPR.2019.00795
   Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6
   Sela M, 2017, IEEE I CONF COMP VIS, P1585, DOI 10.1109/ICCV.2017.175
   Sengupta S, 2016, IEEE WINT CONF APPL
   Sengupta S, 2018, PROC CVPR IEEE, P6296, DOI 10.1109/CVPR.2018.00659
   Shen YJ, 2018, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2018.00092
   Simonyan K., 2014, 14091556 ARXIV
   Tewari A, 2020, IEEE T PATTERN ANAL, V42, P357, DOI 10.1109/TPAMI.2018.2876842
   Tewari A, 2018, PROC CVPR IEEE, P2549, DOI 10.1109/CVPR.2018.00270
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Trân AT, 2017, PROC CVPR IEEE, P1493, DOI 10.1109/CVPR.2017.163
   Tran L, 2019, PROC CVPR IEEE, P1126, DOI 10.1109/CVPR.2019.00122
   Tran L, 2019, IEEE T PATTERN ANAL, V41, P3007, DOI 10.1109/TPAMI.2018.2868350
   Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   White JD, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-42533-y
   Wu SZ, 2020, PROC CVPR IEEE, P1, DOI 10.1109/CVPR42600.2020.00008
   Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032
   Xiong C, 2015, IEEE I CONF COMP VIS, P3667, DOI 10.1109/ICCV.2015.418
   Xue NN, 2019, IEEE T PATTERN ANAL, V41, P2349, DOI 10.1109/TPAMI.2019.2902556
   Yin X, 2017, IEEE I CONF COMP VIS, P4010, DOI 10.1109/ICCV.2017.430
   Yuan ZH, 2013, APPL INTELL, V39, P761, DOI 10.1007/s10489-012-0410-8
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhao J, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1184
   Zhao J, 2018, PROC CVPR IEEE, P2207, DOI 10.1109/CVPR.2018.00235
   Zhou YX, 2019, PROC CVPR IEEE, P1097, DOI 10.1109/CVPR.2019.00119
   Zhu XY, 2019, IEEE T PATTERN ANAL, V41, P78, DOI 10.1109/TPAMI.2017.2778152
NR 78
TC 8
Z9 9
U1 1
U2 38
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2022
VL 8
IS 2
BP 239
EP 256
DI 10.1007/s41095-021-0238-4
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XJ0YS
UT WOS:000726525400005
OA gold
DA 2024-07-18
ER

PT J
AU Fu, XM
   Su, JP
   Zhao, ZY
   Fang, Q
   Ye, CY
   Liu, LG
AF Fu, Xiao-Ming
   Su, Jian-Ping
   Zhao, Zheng-Yu
   Fang, Qing
   Ye, Chunyang
   Liu, Ligang
TI Inversion-free geometric mapping construction: A survey
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review
DE inversion-free mapping; Jacobian matrix; distortion; first-order
   methods; second-order methods
ID CONFORMAL PARAMETERIZATION; ISOGEOMETRIC ANALYSIS; CUT CONSTRUCTION;
   STEINER PROBLEM; FAST ALGORITHM; POLYCUBE-MAPS; ROBUST; COMPUTATION;
   GRAPH; FLOW
AB A geometric mapping establishes a correspondence between two domains. Since no real object has zero or negative volume, such a mapping is required to be inversion-free. Computing inversion-free mappings is a fundamental task in numerous computer graphics and geometric processing applications, such as deformation, texture mapping, mesh generation, and others. This task is usually formulated as a non-convex, nonlinear, constrained optimization problem. Various methods have been developed to solve this optimization problem. As well as being inversion-free, different applications have various further requirements. We expand the discussion in two directions to (i) problems imposing specific constraints and (ii) combinatorial problems. This report provides a systematic overview of inversion-free mapping construction, a detailed discussion of the construction methods, including their strengths and weaknesses, and a description of open problems in this research field.
C1 [Fu, Xiao-Ming; Su, Jian-Ping; Zhao, Zheng-Yu; Fang, Qing; Ye, Chunyang; Liu, Ligang] Univ Sci & Technol China, Sch Math Sci, Hefei, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Fu, XM (corresponding author), Univ Sci & Technol China, Sch Math Sci, Hefei, Peoples R China.
EM fuxm@ustc.edu.cn; SJPing@mail.ustc.edu.cn; zyzhao18@mail.ustc.edu.cn;
   fq1208@mail.ustc.edu.cn; yechyang@mail.ustc.edu.cn; lgliu@ustc.edu.cn
RI Fu, Xiao-Ming/V-8253-2019; su, Jian-Ping/ABC-5407-2021
OI Su, Jian-Ping/0000-0003-3692-6510
FU National Natural Science Foundation of China [61802359, 61672482]; USTC
   Research Funds of the Double FirstClass Initiative [YD0010002003]
FX We would like to thank the anonymous reviewers for their constructive
   suggestions and comments. This work was supported by the National
   Natural Science Foundation of China (Nos. 61802359 and 61672482) and the
   USTC Research Funds of the Double FirstClass Initiative (No.
   YD0010002003).
CR Aigerman N, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818099
   Aigerman N, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766921
   Aigerman N, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601158
   Aigerman N, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461931
   Alexa M, 1999, SHAPE MODELING INTERNATIONAL '99 - INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P202, DOI 10.1109/SMA.1999.749341
   Alliez P, 2008, MATH VIS, P53, DOI 10.1007/978-3-540-33265-7_2
   [Anonymous], 1980, MATH JAPONICA
   [Anonymous], 2015, ACM COMPUT SURV, DOI DOI 10.1145/2668020
   [Anonymous], 2005, ACMEUROGRAPHICS S CO
   BEASLEY JE, 1989, NETWORKS, V19, P1, DOI 10.1002/net.3230190102
   Ben-Chen M, 2008, COMPUT GRAPH FORUM, V27, P449, DOI 10.1111/j.1467-8659.2008.01142.x
   BERMAN P, 1994, J ALGORITHM, V17, P381, DOI 10.1006/jagm.1994.1041
   Bommes D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531383
   Botsch M., 2010, Polygon Mesh Processing
   Bright A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073646
   Byrka J, 2013, J ACM, V60, DOI 10.1145/2432622.2432628
   Campen M, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3360511
   Campen M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925890
   Campen M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818140
   Chai SM, 2021, IEEE T VIS COMPUT GR, V27, P2469, DOI 10.1109/TVCG.2019.2947420
   Chai SM, 2018, COMPUT GRAPH-UK, V74, P66, DOI 10.1016/j.cag.2018.05.007
   Chang CC, 2010, J INF SCI ENG, V26, P291
   Chen RJ, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130843
   Chen RJ, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766989
   Chern A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766916
   Chien E, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982426
   Claici S, 2017, COMPUT GRAPH FORUM, V36, P37, DOI 10.1111/cgf.13243
   Crane K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461986
   DARIPA P, 1993, J COMPUT PHYS, V106, P355
   Degener P., 2003, P 12 INT MESH ROUNDT, P201
   Dong ZC, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3345554
   Dong ZC, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130893
   Du XY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392484
   Eppstein D, 2010, PROCEEDINGS OF THE TWENTY-SIXTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SCG'10), P429, DOI 10.1145/1810959.1811030
   Escobar JM, 2003, COMPUT METHOD APPL M, V192, P2775, DOI 10.1016/S0045-7825(03)00299-8
   Fang Q, 2020, COMPUT AIDED DESIGN, V126, DOI 10.1016/j.cad.2020.102863
   Fang XZ, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925957
   Floater MS, 2003, MATH COMPUT, V72, P685, DOI 10.1090/S0025-5718-02-01466-7
   Fomin FV, 2008, LECT NOTES COMPUT SC, V5193, P430, DOI 10.1007/978-3-540-87744-8_36
   Fu XM, 2016, COMPUT GRAPH FORUM, V35, P97, DOI 10.1111/cgf.13007
   Fu XM, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980231
   Fu XM, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766938
   Gao XF, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130848
   Ge HB, 2018, T AM MATH SOC, V370, P1377, DOI 10.1090/tran/7196
   Golla B, 2018, COMPUT GRAPH FORUM, V37, P233, DOI 10.1111/cgf.13563
   Gregson J, 2011, COMPUT GRAPH FORUM, V30, P1407, DOI 10.1111/j.1467-8659.2011.02015.x
   Gu XF, 2002, ACM T GRAPHIC, V21, P355
   Guo HX, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392378
   Hakimi S. L., 1971, Networks, V1, P113, DOI 10.1002/net.3230010203
   HE ZX, 1990, T AM MATH SOC, V322, P657, DOI 10.2307/2001719
   Hefetz EF, 2019, COMPUT GRAPH FORUM, V38, P105, DOI 10.1111/cgf.13623
   Ho KT, 2016, ADV COMPUT MATH, V42, P279, DOI 10.1007/s10444-015-9424-1
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   HORMANN K, 2001, THESIS U ERLANGEN
   Hormann K., 2000, CURVE SURFACE DESIGN, P153
   Hormann K., 2007, ACM Siggraph 2007 Course, V11, P1
   Hu X, 2018, IEEE T VIS COMPUT GR, V24, P1930, DOI 10.1109/TVCG.2017.2704119
   Huang J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2602141
   Hughes TJR, 2005, COMPUT METHOD APPL M, V194, P4135, DOI 10.1016/j.cma.2004.10.008
   Hwang F., 1992, STEINER TREE PROBLEM
   Jiang L. J., 2004, CUCS98204 U COL DEP
   Jiang ZS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130895
   Jin M, 2008, IEEE T VIS COMPUT GR, V14, P1030, DOI 10.1109/TVCG.2008.57
   Jin Y, 2014, COMPUT GRAPH FORUM, V33, P269, DOI 10.1111/cgf.12452
   Julius D, 2005, COMPUT GRAPH FORUM, V24, P581, DOI 10.1111/j.1467-8659.2005.00883.x
   Kharevych L, 2006, ACM T GRAPHIC, V25, P412, DOI 10.1145/1138450.1138461
   KOU L, 1981, ACTA INFORM, V15, P141, DOI 10.1007/BF00288961
   Kovalsky SZ, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925920
   Kovalsky SZ, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818098
   Kovalsky SZ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601142
   Kraevoy V, 2004, ACM T GRAPHIC, V23, P861, DOI 10.1145/1015706.1015811
   Kwok TH, 2012, IEEE T VIS COMPUT GR, V18, P1678, DOI 10.1109/TVCG.2011.115
   Levi Z, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3439828
   Levi Z, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925929
   Lévy B, 2002, ACM T GRAPHIC, V21, P362, DOI 10.1145/566570.566590
   Li MC, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392425
   Li MC, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275042
   Lipman Y, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2231816.2231822
   Liu CL, 2017, COMPUT AIDED DESIGN, V90, P5, DOI 10.1016/j.cad.2017.05.005
   Liu H, 2020, COMPUT AIDED GEOM D, V79, DOI 10.1016/j.cagd.2020.101853
   Liu H, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323000
   Liu HY, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323001
   Liu LG, 2008, COMPUT GRAPH FORUM, V27, P1495, DOI 10.1111/j.1467-8659.2008.01290.x
   Liu LG, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201331
   Liu TT, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2990496
   Liu TT, 2016, COMPUT GRAPH FORUM, V35, P1, DOI 10.1111/cgf.12806
   Livesu M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508388
   Lui LM, 2012, J SCI COMPUT, V50, P557, DOI 10.1007/s10915-011-9506-2
   Luo F, 2004, COMMUN CONTEMP MATH, V6, P765, DOI 10.1142/S0219199704001501
   Ma M, 2017, GRAPH MODELS, V90, P13, DOI 10.1016/j.gmod.2017.01.002
   MASTIN CW, 1978, Z ANGEW MATH PHYS, V29, P1, DOI 10.1007/BF01797299
   Misztal MK, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2167076.2167082
   Müller M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766907
   Myles Ashish, 2014, ACM Transactions on Graphics, V33, DOI 10.1145/2601097.2601154
   Myles A, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185605
   Myles A, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461970
   Nian XS, 2016, COMPUT METHOD APPL M, V311, P41, DOI 10.1016/j.cma.2016.07.035
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Paillé GP, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766900
   Pajor T, 2018, MATH PROGRAM COMPUT, V10, P69, DOI 10.1007/s12532-017-0123-4
   Peng Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201290
   Poranne R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130845
   Poranne R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601123
   Praun E, 2003, ACM T GRAPHIC, V22, P340, DOI 10.1145/882262.882274
   Rabinovich M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2983621
   Robins G, 2005, SIAM J DISCRETE MATH, V19, P122, DOI 10.1137/S0895480101393155
   ROCEK M, 1984, Z PHYS C PART FIELDS, V21, P371, DOI 10.1007/BF01581603
   Sander P. V., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P87
   Sawhney R, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3132705
   Schmidt P, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392399
   Schmidt P, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356519
   Schreiner J, 2004, ACM T GRAPHIC, V23, P870, DOI 10.1145/1015706.1015812
   Schüller C, 2013, COMPUT GRAPH FORUM, V32, P125, DOI 10.1111/cgf.12179
   Sheffer A, 2005, ACM T GRAPHIC, V24, P311, DOI 10.1145/1061347.1061354
   Sheffer A, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P291, DOI 10.1109/VISUAL.2002.1183787
   Sheffer A, 2002, SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P61, DOI 10.1109/SMI.2002.1003529
   Sheffer A, 2001, ENG COMPUT-GERMANY, V17, P326, DOI 10.1007/PL00013391
   Shen HX, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323012
   Shtengel A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073618
   Smith B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3241041
   Smith J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766947
   Soliman Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201367
   Sorkine O, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P355, DOI 10.1109/VISUAL.2002.1183795
   Springborn B, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360676
   Su JP, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392435
   Su JP, 2019, COMPUT GRAPH FORUM, V38, P287, DOI 10.1111/cgf.13837
   Su KH, 2019, COMPUT GRAPH FORUM, V38, P707, DOI 10.1111/cgf.13873
   Sun Q, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925883
   Tarini M, 2004, ACM T GRAPHIC, V23, P853, DOI 10.1145/1015706.1015810
   Toulorge T, 2013, J COMPUT PHYS, V254, P8, DOI 10.1016/j.jcp.2013.07.022
   Tutte W.T., 1963, Proc. London Math. Soc., V13, P743, DOI DOI 10.1112/PLMS/S3-13.1.743
   van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P1681, DOI 10.1111/j.1467-8659.2011.01884.x
   Wang CX, 2016, COMPUT GRAPH-UK, V58, P161, DOI 10.1016/j.cag.2016.05.005
   Wang YL, 2012, IEEE T MED IMAGING, V31, P251, DOI 10.1109/TMI.2011.2168233
   Weber O, 2012, COMPUT GRAPH FORUM, V31, P1679, DOI 10.1111/j.1467-8659.2012.03173.x
   Wei Zeng, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2457, DOI 10.1109/CVPR.2011.5995410
   Wong TW, 2014, SIAM J IMAGING SCI, V7, P2675, DOI 10.1137/14097104X
   Xia J., 2011, S INTERACTIVE 3D GRA, P151
   Xianfeng Gu, 2003, Symposium on Geometry Processing, P127
   Xiao SW, 2018, COMPUT AIDED GEOM D, V62, P29, DOI 10.1016/j.cagd.2018.03.008
   Xu Y, 2011, COMPUT AIDED GEOM D, V28, P349, DOI 10.1016/j.cagd.2011.07.001
   Yang Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392434
   Yang Y, 2019, COMPUT GRAPH FORUM, V38, P299, DOI 10.1111/cgf.13838
   Yang Y, 2019, IEEE T VIS COMPUT GR, V25, P2999, DOI 10.1109/TVCG.2018.2861396
   Yao CY, 2008, IEEE T VIS COMPUT GR, V14, P948, DOI 10.1109/TVCG.2008.39
   Ye CY, 2020, COMPUT GRAPH FORUM, V39, P1, DOI 10.1111/cgf.14122
   Yu YZ, 2004, ACM T GRAPHIC, V23, P644, DOI 10.1145/1015706.1015774
   Zayer R., 2007, Fifth Eurographics Symposium on Geometry Processing - SGP 2007, P135, DOI DOI 10.1145/571647.571651
   Zeng W, 2012, NUMER MATH, V121, P671, DOI 10.1007/s00211-012-0446-z
   Zhang C, 2020, COMPUT AIDED GEOM D, V79, DOI 10.1016/j.cagd.2020.101854
   Zhang E, 2005, ACM T GRAPHIC, V24, P1, DOI 10.1145/1037957.1037958
   Zhang WJ, 2020, COMPUT AIDED GEOM D, V81, DOI 10.1016/j.cagd.2020.101909
   Zhao H, 2018, COMPUT AIDED GEOM D, V63, P96, DOI 10.1016/j.cagd.2018.03.001
   Zhou J, 2020, COMPUT GRAPH FORUM, V39, P179, DOI 10.1111/cgf.13922
   Zhou K., 2004, P EUR ICS ACM SIGGRA, P45
   Zhu TY, 2020, COMPUT GRAPH FORUM, V39, P191, DOI 10.1111/cgf.13923
   Zhu YF, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201359
NR 157
TC 16
Z9 17
U1 4
U2 38
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2021
VL 7
IS 3
BP 289
EP 318
DI 10.1007/s41095-021-0233-9
PG 30
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TR1TJ
UT WOS:000678754100002
OA gold
DA 2024-07-18
ER

PT J
AU Pan, YH
   Niu, ZB
   Wu, J
   Zhang, JW
AF Pan, Yaohua
   Niu, Zhibin
   Wu, Jing
   Zhang, Jiawan
TI InSocialNet: Interactive visual analytics for role-event videos
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE visual analytics; behavioral psychology; role-event videos; social
   network; video analysis
AB Role-event videos are rich in information but challenging to be understood at the story level. The social roles and behavior patterns of characters largely depend on the interactions among characters and the background events. Understanding them requires analysis of the video contents for a long duration, which is beyond the ability of current algorithms designed for analyzing short-time dynamics. In this paper, we propose InSocialNet, an interactive video analytics tool for analyzing the contents of role-event videos. It automatically and dynamically constructs social networks from role-event videos making use of face and expression recognition, and provides a visual interface for interactive analysis of video contents. Together with social network analysis at the back end, InSocialNet supports users to investigate characters, their relationships, social roles, factions, and events in the input video. We conduct case studies to demonstrate the effectiveness of InSocialNet in assisting the harvest of rich information from role-event videos. We believe the current prototype implementation can be extended to applications beyond movie analysis, e.g., social psychology experiments to help understand crowd social behaviors.
C1 [Pan, Yaohua; Niu, Zhibin; Zhang, Jiawan] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300354, Peoples R China.
   [Pan, Yaohua; Niu, Zhibin; Zhang, Jiawan] Tianjin Univ, Sch New Media & Commun, Tianjin 300354, Peoples R China.
   [Wu, Jing] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF24 3AA, Wales.
C3 Tianjin University; Tianjin University; Cardiff University
RP Niu, ZB (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300354, Peoples R China.; Niu, ZB (corresponding author), Tianjin Univ, Sch New Media & Commun, Tianjin 300354, Peoples R China.
EM lypyh522@126.com; zniu@tju.edu.cn; J.Wu@cs.cardiff.ac.uk;
   jwzhang@tju.edu.cn
RI Niu, Zhibin/AAE-7765-2020
OI Niu, Zhibin/0000-0002-5171-7648
FU National Natural Science Foundation of China [61802278]
FX The research is supported by National Natural Science Foundation of
   China (No. 61802278).
CR ACM, 2017, T INTELLIGENT SYSTEM, V8, P6
   [Anonymous], 2006, PHYS REV E, V74, P3
   [Anonymous], 2004, PHYS REV E, V69, P2
   [Anonymous], 2016, J VISION, V16, P3
   [Anonymous], 2015, NAIVE DEEP FACE RECO
   Avril M, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.01437
   Bernstein GA, 2017, J CHILD ADOL PSYCHOP, V27, P140, DOI 10.1089/cap.2016.0067
   Blondel VD, 2008, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2008/10/P10008
   Forczmanski P, 2016, LECT NOTES COMPUT SC, V9972, P462, DOI 10.1007/978-3-319-46418-3_41
   FRUCHTERMAN TMJ, 1991, SOFTWARE PRACT EXPER, V21, P1129, DOI 10.1002/spe.4380211102
   Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754
   Jack RE, 2012, P NATL ACAD SCI USA, V109, P7241, DOI 10.1073/pnas.1200155109
   Jiang YG, 2012, LECT NOTES COMPUT SC, V7576, P425, DOI 10.1007/978-3-642-33715-4_31
   Kagan D, 2019, ARXIV190306469
   Khorrami P, 2016, IEEE IMAGE PROC, P619, DOI 10.1109/ICIP.2016.7532431
   Kim M, 2008, PROC CVPR IEEE, P1787
   Lu ZQ, 2015, IEEE T PARALL DISTR, V26, P2916, DOI 10.1109/TPDS.2014.2370031
   Lv JN, 2018, IEEE ACCESS, V6, P25958, DOI 10.1109/ACCESS.2018.2832087
   Ma XK, 2017, IEEE T KNOWL DATA EN, V29, P1045, DOI 10.1109/TKDE.2017.2657752
   Maaten L. V. D., 2009, J MACH LEARN RES, P384
   Park HS, 2013, IEEE I CONF COMP VIS, P3503, DOI 10.1109/ICCV.2013.435
   Pons P, 2005, LECT NOTES COMPUT SC, V3733, P284
   Raghavan UN, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.036106
   Ramanathan V, 2013, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2013.320
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Renoust B, 2015, 2015 11TH INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY & INTERNET-BASED SYSTEMS (SITIS), P474, DOI 10.1109/SITIS.2015.30
   Rosvall M, 2008, P NATL ACAD SCI USA, V105, P1118, DOI 10.1073/pnas.0706851105
   Schmitt DT, 2009, ISI: 2009 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENCE AND SECURITY INFORMATICS, P217, DOI 10.1109/ISI.2009.5137307
   Seng KP, 2018, IEEE T HUM-MACH SYST, V48, P266, DOI 10.1109/THMS.2017.2695613
   Sun QR, 2017, PROC CVPR IEEE, P435, DOI 10.1109/CVPR.2017.54
   Taha K, 2018, IEEE T COMPUT SOC SY, V5, P493, DOI 10.1109/TCSS.2018.2822738
   Vrigkas M, 2017, IEEE T AFFECT COMPUT, V8, P54, DOI 10.1109/TAFFC.2015.2507168
   Weng CY, 2009, IEEE T MULTIMEDIA, V11, P256, DOI 10.1109/TMM.2008.2009684
   Xiao YP, 2018, IEEE T COMPUT SOC SY, V5, P430, DOI 10.1109/TCSS.2018.2812721
   Yu C, 2009, INFORM VISUAL, V8, P56, DOI 10.1057/ivs.2008.32
   Zhang LQ, 2017, INT CON DISTR COMP S, P129, DOI 10.1109/ICDCS.2017.95
NR 36
TC 4
Z9 4
U1 1
U2 13
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2019
VL 5
IS 4
BP 375
EP 390
DI 10.1007/s41095-019-0157-9
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UO
UT WOS:000648691000005
OA gold, Green Accepted
DA 2024-07-18
ER

PT J
AU Liu, SL
   Wang, XC
   Liu, XY
   Wu, ZK
   Seah, HS
AF Liu, Shaolong
   Wang, Xingce
   Liu, Xiangyuan
   Wu, Zhongke
   Seah, Hock Soon
TI Shape correspondence for cel animation based on a shape association
   graph and spectral matching
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE cel animation; shape correspondence; shape association graph (SAG);
   spectral matching; quadratic assignment; Kendall shape space
AB We present an effective spectral matching method based on a shape association graph for finding region correspondences between two cel animation keyframes. We formulate the correspondence problem as an adapted quadratic assignment problem, which comprehensively considers both the intrinsic geometric and topology of regions to find the globally optimal correspondence. To simultaneously represent the geometric and topological similarities between regions, we propose a shape association graph (SAG), whose node attributes indicate the geometric distance between regions, and whose edge attributes indicate the topological distance between combined region pairs. We convert topological distance to geometric distance between geometric objects with topological features of the pairs, and introduce Kendall shape space to calculate the intrinsic geometric distance. By utilizing the spectral properties of the affinity matrix induced by the SAG, our approach can efficiently extract globally optimal region correspondences, even if shapes have inconsistent topology and severe deformation. It is also robust to shapes undergoing similarity transformations, and compatible with parallel computing techniques.
C1 [Liu, Shaolong; Wang, Xingce; Liu, Xiangyuan; Wu, Zhongke] Beijing Normal Univ, Beijing 100875, Peoples R China.
   [Seah, Hock Soon] Nanyang Technol Univ, Singapore 639798, Singapore.
C3 Beijing Normal University; Nanyang Technological University
RP Wu, ZK (corresponding author), Beijing Normal Univ, Beijing 100875, Peoples R China.
EM zwu@bnu.edu.cn
RI JIANG, Peng/KGL-3427-2024; Han, Yang/JVN-5921-2024; chen,
   bin/KBQ-8114-2024; li, jixiang/JXN-7599-2024
OI chen, bin/0000-0002-3398-1314; Shaolong, Liu/0000-0002-9265-0777
FU National Key Ramp;D Program of China [2020YFC1523302]; National Natural
   Science Foundation of China [61972041, 62072045]
FX AcknowledgementsThis research was partially supported by the National
   Key R&D Program of China (2020YFC1523302), and the National Natural
   Science Foundation of China (61972041, 62072045). Additionally we give
   many thanks to Jiang jie and Liew Hongze for their instructive advice
   and useful suggestions.
CR Alexa M, 2000, COMP GRAPH, P157, DOI 10.1145/344779.344859
   Belongie S, 2001, ADV NEUR IN, V13, P831
   Casey E, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11303, DOI 10.1109/ICCV48922.2021.01113
   Chang CW, 1997, J VISUAL COMP ANIMAT, V8, P165, DOI 10.1002/(SICI)1099-1778(199707)8:3<165::AID-VIS157>3.0.CO;2-2
   Cho M, 2010, LECT NOTES COMPUT SC, V6315, P492
   Dryden IL., 2016, STAT SHAPE ANAL APPL
   Egenhofer M. J., 1990, Proceedings of the 4th International Symposium on Spatial Data Handling, P803
   Jiang J. E., 2020, DIGITAL GAMING HDB, P245
   Kanamori Y, 2013, INT CONF IMAG VIS, P483, DOI 10.1109/IVCNZ.2013.6727062
   Kanamori Yoshihiro, 2012, SIGGRAPH ASIA 2012 T
   KENDALL DG, 1984, B LOND MATH SOC, V16, P81, DOI 10.1112/blms/16.2.81
   Kort A., 2002, NPAR 02, P125, DOI DOI 10.1145/508530.508552
   Lasseter J., 1987, Proc. SIGGRAPH 87, V21, P35, DOI DOI 10.1145/37402.37407
   Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482
   Li XY, 2022, IEEE T VIS COMPUT GR, V28, P2938, DOI 10.1109/TVCG.2021.3049419
   Liu SL, 2020, VISUAL COMPUT, V36, P2457, DOI 10.1007/s00371-020-01958-7
   Liu XT, 2022, COMPUT VIS MEDIA, V8, P135, DOI 10.1007/s41095-021-0228-6
   Liu Xueting, 2017, [Computational Visual Media, 计算可视媒体], V3, P61
   Lv CL, 2019, PATTERN RECOGN, V88, P458, DOI 10.1016/j.patcog.2018.12.006
   Madeira JS, 1996, VISUAL COMPUT, V12, P1
   Maejima A, 2019, SIGGRAPH '19 - ACM SIGGRAPH 2019 POSTERS, DOI 10.1145/3306214.3338560
   Mao Xiangyu., 2015, Comput. Vis. Med, V1, P69, DOI [DOI 10.1007/S41095-015-0007-3, 10.1007/s41095-015-0007-3]
   Miyauchi Ryoma, 2021, 2021 Nicograph International (NicoInt), P34, DOI 10.1109/NICOINT52941.2021.00014
   Qiu J, 2005, COMPUT ANIMAT VIRT W, V16, P463, DOI 10.1002/cav.86
   Qiu J, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P175
   Qiu J, 2008, INT J COMPUT GAMES T, V2008, DOI 10.1155/2008/135398
   Sato K., 2014, SA 14 SIGGRAPH ASIA, DOI 10.1145/2669024.2669037
   Skora D., 2004, P 3 INT S NONPH AN R, P121, DOI DOI 10.1145/987657.987677
   Sykora D., 2011, Proceedings of International Symposium on Non-Photorealistic Animation and Rendering, P75
   Trémeau A, 2000, IEEE T IMAGE PROCESS, V9, P735, DOI 10.1109/83.841950
   Trigo PG, 2009, IEICE T INF SYST, VE92D, P1289, DOI 10.1587/transinf.E92.D.1289
   Yang WW, 2018, COMPUT GRAPH FORUM, V37, P125, DOI 10.1111/cgf.13518
   Zhang L, 2012, IEEE T VIS COMPUT GR, V18, P1156, DOI 10.1109/TVCG.2011.111
   Zhou F, 2012, PROC CVPR IEEE, P127, DOI 10.1109/CVPR.2012.6247667
   Zhu HC, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925872
NR 35
TC 0
Z9 0
U1 3
U2 10
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2023
VL 9
IS 3
BP 633
EP 656
DI 10.1007/s41095-022-0298-0
EA APR 2023
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA G9DP6
UT WOS:000979603600001
OA gold, Green Submitted
DA 2024-07-18
ER

PT J
AU Gong, JY
   Ye, Z
   Ma, LZ
AF Gong, Jingyu
   Ye, Zhou
   Ma, Lizhuang
TI Neighborhood co-occurrence modeling in 3D point cloud segmentation
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE 3D vision; point cloud; co-occurrence relation modeling; semantic
   segmentation
AB A significant performance boost has been achieved in point cloud semantic segmentation by utilization of the encoder-decoder architecture and novel convolution operations for point clouds. However, co-occurrence relationships within a local region which can directly influence segmentation results are usually ignored by current works. In this paper, we propose a neighborhood co-occurrence matrix (NCM) to model local co-occurrence relationships in a point cloud. We generate target NCM and prediction NCM from semantic labels and a prediction map respectively. Then, Kullback-Leibler (KL) divergence is used to maximize the similarity between the target and prediction NCMs to learn the co-occurrence relationship. Moreover, for large scenes where the NCMs for a sampled point cloud and the whole scene differ greatly, we introduce a reverse form of KL divergence which can better handle the difference to supervise the prediction NCMs. We integrate our method into an existing backbone and conduct comprehensive experiments on three datasets: Semantic3D for outdoor space segmentation, and S3DIS and ScanNet v2 for indoor scene segmentation. Results indicate that our method can significantly improve upon the backbone and outperform many leading competitors.
C1 [Gong, Jingyu; Ma, Lizhuang] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
   [Ye, Zhou] Shanghai CLS Fintech Co LTD, Shanghai 200030, Peoples R China.
   [Ma, Lizhuang] Shanghai Jiao Tong Univ, MoE Key Lab Artificial Intelligence, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University
RP Ma, LZ (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.; Ma, LZ (corresponding author), Shanghai Jiao Tong Univ, MoE Key Lab Artificial Intelligence, Shanghai 200240, Peoples R China.
EM gongjingyu@sjtu.edu.cn; yezhou@cls.cn; ma-lz@cs.sjtu.edu.cn
RI Sun, Peng/KDO-4243-2024; Li, Shiyu/KHE-1376-2024
FU National Natural Science Foundation of China [61972157]; Natural Science
   Foundation of Shanghai [20ZR1417700]; National Key R&D Program of China
   [2019YFC1521104, 2020AAA0108301]; Shanghai Municipal Commission of
   Economy and Information [XX-RGZN-01-19-6348]; Art Major Project of
   National Social Science Fund [I8ZD22]
FX We thank the support of the National Natural Science Foundation of China
   (61972157), the Natural Science Foundation of Shanghai (20ZR1417700),
   the National Key R&D Program of China (2019YFC1521104, 2020AAA0108301),
   Shanghai Municipal Commission of Economy and Information
   (XX-RGZN-01-19-6348), and the Art Major Project of National Social
   Science Fund (I8ZD22). We thank Yao Li for server management.
CR [Anonymous], 2011, P ADV NEURAL INFORM
   Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Deng Z, 2015, IEEE I CONF COMP VIS, P1733, DOI 10.1109/ICCV.2015.202
   Feihu Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P644, DOI 10.1007/978-3-030-58586-0_38
   Gong JY, 2021, PROC CVPR IEEE, P11668, DOI 10.1109/CVPR46437.2021.01150
   Gong JY, 2021, AAAI CONF ARTIF INTE, V35, P1424
   Hackel T, 2017, ISPRS ANN PHOTOGRAMM, P91, DOI 10.5194/isprs-annals-IV-1-W1-91-2017
   Han WK, 2020, AAAI CONF ARTIF INTE, V34, P10925
   Hu SM, 2020, IEEE T VIS COMPUT GR, V26, P2485, DOI 10.1109/TVCG.2018.2889944
   Huan Lei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11608, DOI 10.1109/CVPR42600.2020.01163
   Huang JW, 2019, PROC CVPR IEEE, P4435, DOI 10.1109/CVPR.2019.00457
   Huang QG, 2018, PROC CVPR IEEE, P2626, DOI 10.1109/CVPR.2018.00278
   Jiang L, 2019, IEEE I CONF COMP VIS, P10432, DOI 10.1109/ICCV.2019.01053
   Khan SA, 2020, IEEE COMPUT SOC CONF, P778, DOI 10.1109/CVPRW50498.2020.00107
   Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479
   Lei H, 2021, IEEE T PATTERN ANAL, V43, P3664, DOI 10.1109/TPAMI.2020.2983410
   Li YY, 2018, ADV NEUR IN, V31
   Lin YQ, 2020, PROC CVPR IEEE, P4292, DOI 10.1109/CVPR42600.2020.00435
   Ma YN, 2020, IEEE WINT CONF APPL, P2920, DOI [10.1109/WACV45572.2020.9093411, 10.1109/wacv45572.2020.9093411]
   Mattausch O, 2014, COMPUT GRAPH FORUM, V33, P11, DOI 10.1111/cgf.12286
   Mottaghi R, 2014, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2014.119
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qingyong Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11105, DOI 10.1109/CVPR42600.2020.01112
   Pham QH, 2019, PROC CVPR IEEE, P8819, DOI 10.1109/CVPR.2019.00903
   Schult Jonas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8609, DOI 10.1109/CVPR42600.2020.00864
   Tchapmi LP, 2017, INT CONF 3D VISION, P537, DOI 10.1109/3DV.2017.00067
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Thomas H, 2018, INT CONF 3D VISION, P390, DOI 10.1109/3DV.2018.00052
   Verdoja F, 2017, IEEE INT CON MULTI, P1285, DOI 10.1109/ICME.2017.8019382
   Wang L, 2019, PROC CVPR IEEE, P10288, DOI 10.1109/CVPR.2019.01054
   Wang YS, 2019, IEEE I CONF COMP VIS, P322, DOI 10.1109/ICCV.2019.00041
   Wu BC, 2018, IEEE INT CONF ROBOT, P1887
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Xu JC, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P601
   Yan X, 2020, PROC CVPR IEEE, P5588, DOI 10.1109/CVPR42600.2020.00563
   Ye XQ, 2018, LECT NOTES COMPUT SC, V11211, P415, DOI 10.1007/978-3-030-01234-2_25
   Zeyu Hu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P222, DOI 10.1007/978-3-030-58565-5_14
   Zhang H, 2019, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2019.00064
   Zhang JZ, 2020, PROC CVPR IEEE, P4533, DOI 10.1109/CVPR42600.2020.00459
   Zhang ZY, 2019, IEEE I CONF COMP VIS, P1607, DOI 10.1109/ICCV.2019.00169
   Zhao L, 2020, AAAI CONF ARTIF INTE, V34, P12951
   Zhao S., 2019, Advances in neural information processing systems (NeurIPS)
   Zhao Z., 2017, P 2017 C EMP METH NA, P244
NR 45
TC 7
Z9 7
U1 1
U2 31
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2022
VL 8
IS 2
BP 303
EP 315
DI 10.1007/s41095-021-0244-6
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XJ0YS
UT WOS:000726525400009
OA gold
DA 2024-07-18
ER

PT J
AU Chlubna, T
   Milet, T
   Zemcík, P
AF Chlubna, T.
   Milet, T.
   Zemcik, P.
TI Real-time per-pixel focusing method for light field rendering
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE image-based rendering; light field; plenoptic function; computational
   photography
ID RECONSTRUCTION
AB Light field rendering is an image-based rendering method that does not use 3D models but only images of the scene as input to render new views. Light field approximation, represented as a set of images, suffers from so-called refocusing artifacts due to different depth values of the pixels in the scene. Without information about depths in the scene, proper focusing of the light field scene is limited to a single focusing distance. The correct focusing method is addressed in this work and a real-time solution is proposed for focusing of light field scenes, based on statistical analysis of the pixel values contributing to the final image. Unlike existing techniques, this method does not need precomputed or acquired depth information. Memory requirements and streaming bandwidth are reduced and real-time rendering is possible even for high resolution light field data, yielding visually satisfactory results. Experimental evaluation of the proposed method, implemented on a GPU, is presented in this paper.
C1 [Chlubna, T.; Milet, T.; Zemcik, P.] Brno Univ Technol, Fac Informat Technol, Brno 61200, Czech Republic.
C3 Brno University of Technology
RP Chlubna, T (corresponding author), Brno Univ Technol, Fac Informat Technol, Brno 61200, Czech Republic.
EM ichlubna@fit.vut.cz; imilet@fit.vut.cz; zemcik@fit.vut.cz
RI Zemcik, Pavel/G-6439-2010; Chlubna, Tomáš/Y-7496-2018; Chlubna,
   Tomáš/AAX-5104-2021
OI Zemcik, Pavel/0000-0001-7969-5877; Chlubna, Tomáš/0000-0003-3126-0545;
   Milet, Tomas/0000-0003-0841-4198
FU Ministry of Education, Youth and Sports from the National Programme of
   Sustainability (NPU II) project IT4Innovations excellence in science
   [LQ1602]
FX This work was supported by The Ministry of Education, Youth and Sports
   from the National Programme of Sustainability (NPU II) project
   IT4Innovations excellence in science, LQ1602.
CR Adelson E.H., 1991, Computational Models of Visual Processing, P3
   Alain M., 2019, ARXIV PREPRINT ARXIV
   Anisimov Y, 2019, INT C INTELL COMP CO, P109, DOI [10.1109/iccp48234.2019.8959680, 10.1109/ICCP48234.2019.8959680]
   [Anonymous], 2018, IEEE IMAGE PROC
   Banz C., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P514, DOI 10.1109/ICCVW.2011.6130286
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Buehler C, 2001, COMP GRAPH, P425, DOI 10.1145/383259.383309
   Chai JX, 2000, COMP GRAPH, P307, DOI 10.1145/344779.344932
   Cheggour H., BLENDER DEMO FILES B
   Chen Y., 2020, ARXIV200300517
   Chuchvara A, 2020, IEEE T IMAGE PROCESS, V29, P2492, DOI 10.1109/TIP.2019.2959233
   Debevec P., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P105
   Eigen D, 2014, ADV NEUR IN, V27
   Eslami SMA, 2018, SCIENCE, V360, P1204, DOI 10.1126/science.aar6170
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   Han XF, 2021, IEEE T PATTERN ANAL, V43, P1578, DOI 10.1109/TPAMI.2019.2954885
   Heigl Benno, 1999, MUSTERERKENNUNG 1999, P2
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Isaksen A, 2000, COMP GRAPH, P297, DOI 10.1145/344779.344929
   Jeon HG, 2015, PROC CVPR IEEE, P1547, DOI 10.1109/CVPR.2015.7298762
   Kalantari NK, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980251
   Kauvar I, 2015, ACM T GRAPHIC, V34, DOI [10.1145/2682631, 10.1145/2816795.2818070]
   Kolmogorov V, 2002, LECT NOTES COMPUT SC, V2352, P82
   Kubota A., 2004, P 15 EUR C REND TECH, P235
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Li C, 2015, PROCEEDINGS OF THE 2015 7TH IEEE INTERNATIONAL CONFERENCE ON CYBERNETICS AND INTELLIGENT SYSTEMS (CIS) AND ROBOTICS, AUTOMATION AND MECHATRONICS (RAM), P7, DOI 10.1109/ICCIS.2015.7274539
   Lin HT, 2015, IEEE I CONF COMP VIS, P3451, DOI 10.1109/ICCV.2015.394
   Lin ZC, 2004, INT J COMPUT VISION, V58, P121, DOI 10.1023/B:VISI.0000015916.91741.27
   Neri A, 2015, IEEE IMAGE PROC, P3358, DOI 10.1109/ICIP.2015.7351426
   Ni LX, 2019, COMPUT GRAPH FORUM, V38, P425, DOI 10.1111/cgf.13849
   Peng JY, 2018, INT CONF 3D VISION, P295, DOI 10.1109/3DV.2018.00042
   Rerabek M., 2016, P 8 INT C QUAL MULT
   Sabater N, 2017, IEEE COMPUT SOC CONF, P1743, DOI 10.1109/CVPRW.2017.221
   Sanda Mahama A.T., 2016, Electron. Imaging, V2016, P1, DOI [DOI 10.2352/ISSN.2470-1173.2016.20.COLOR-349, 10.2352/]
   Schedl DC, 2018, COMPUT VIS IMAGE UND, V168, P93, DOI 10.1016/j.cviu.2017.06.009
   Sugita K, 2004, P IEEE VIRT REAL ANN, P255, DOI 10.1109/VR.2004.1310096
   Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939
   Takahashi K., 2003, P 13 INT C ART REAL P 13 INT C ART REAL
   Tao MW, 2013, IEEE I CONF COMP VIS, P673, DOI 10.1109/ICCV.2013.89
   Vagharshakyan S, 2018, IEEE T PATTERN ANAL, V40, P133, DOI 10.1109/TPAMI.2017.2653101
   Vaish V, 2008, The (New) Stanford Light Field Archive
   WELFORD BP, 1962, TECHNOMETRICS, V4, P419, DOI 10.2307/1266577
NR 42
TC 7
Z9 8
U1 1
U2 16
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2021
VL 7
IS 3
BP 319
EP 333
DI 10.1007/s41095-021-0205-0
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TR1TJ
UT WOS:000678754100003
OA gold
DA 2024-07-18
ER

PT J
AU Zheng, Q
   Wu, WK
   Pan, HT
   Mitra, N
   Cohen-Or, D
   Huang, H
AF Zheng, Qian
   Wu, Weikai
   Pan, Hanting
   Mitra, Niloy
   Cohen-Or, Daniel
   Huang, Hui
TI Inferring object properties from human interaction and transferring them
   to new motions
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE human interaction motion; object property inference; motion analysis;
   motion synthesis
ID VISUAL-PERCEPTION
AB Humans regularly interact with their surrounding objects. Such interactions often result in strongly correlated motions between humans and the interacting objects. We thus ask: "Is it possible to infer object properties from skeletal motion alone, even without seeing the interacting object itself?" In this paper, we present a fine-grained action recognition method that learns to infer such latent object properties from human interaction motion alone. This inference allows us to disentangle the motion from the object property and transfer object properties to a given motion. We collected a large number of videos and 3D skeletal motions of performing actors using an inertial motion capture device. We analyzed similar actions and learned subtle differences between them to reveal latent properties of the interacting objects. In particular, we learned to identify the interacting object, by estimating its weight, or its spillability. Our results clearly demonstrate that motions and interacting objects are highly correlated and that related object latent properties can be inferred from 3D skeleton sequences alone, leading to new synthesis possibilities for motions involving human interaction. Our dataset is available at http://vcc.szu.edu.cn/research/2020/IT.html.
C1 [Zheng, Qian] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen, Peoples R China.
   [Wu, Weikai; Pan, Hanting; Huang, Hui] Shenzhen Univ, Shenzhen, Peoples R China.
   [Huang, Hui] Shenzhen Univ, Visual Comp Res Ctr, Shenzhen, Peoples R China.
   [Mitra, Niloy] UCL, Smart Geometry Proc Grp, Dept Comp Sci, London, England.
   [Cohen-Or, Daniel] Tel Aviv Univ, Sch Comp Sci, Tel Aviv, Israel.
C3 Shenzhen University; Shenzhen University; Shenzhen University;
   University of London; University College London; Tel Aviv University
RP Huang, H (corresponding author), Shenzhen Univ, Shenzhen, Peoples R China.; Huang, H (corresponding author), Shenzhen Univ, Visual Comp Res Ctr, Shenzhen, Peoples R China.
EM qianzheng85@gmail.com; wuweikai0617pk@gmail.com; panhanting95@gmail.com;
   n.mitra@cs.ucl.ac.uk; cohenor@gmail.com; hhzhiyan@gmail.com
RI Huang, Hui/JGB-1049-2023; Wu, Wei-Kai/AAK-9631-2021
OI Huang, Hui/0000-0003-3212-0544; Wu, Wei-Kai/0000-0002-9476-8998
FU Shenzhen Innovation Program [JCYJ20180305125709986]; National Natural
   Science Foundation of China [61861130365, 61761146002]; GD Science and
   Technology Program [2020A0505100064, 2015A030312015]; DEGP Key Project
   [2018KZDXM058]
FX We sincerely thank the reviewers for their valuable comments. This work
   was supported in part by Shenzhen Innovation Program
   (JCYJ20180305125709986), National Natural Science Foundation of China
   (61861130365, 61761146002), GD Science and Technology Program
   (2020A0505100064, 2015A030312015), and DEGP Key Project (2018KZDXM058).
CR Aberman K, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322999
   Andriluka M, 2018, PROC CVPR IEEE, P5167, DOI 10.1109/CVPR.2018.00542
   [Anonymous], 2014, ACM T GRAPHIC
   [Anonymous], 2006, PROC IEEE COMPUT SOC
   Aristidou A, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275038
   Bellini Rachele, 2018, Computational Visual Media, V4, P197, DOI 10.1007/s41095-018-0115-y
   Blake R, 2007, ANNU REV PSYCHOL, V58, P47, DOI 10.1146/annurev.psych.57.102904.190152
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   CMU, 2018, CMU GRAPH LAB MOT CA
   Davis JW, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1463
   Gkioxari G, 2018, PROC CVPR IEEE, P8359, DOI 10.1109/CVPR.2018.00872
   Grabner H, 2011, PROC CVPR IEEE, P1529, DOI 10.1109/CVPR.2011.5995327
   Gui LY, 2018, LECT NOTES COMPUT SC, V11208, P823, DOI 10.1007/978-3-030-01225-0_48
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Gupta A., IEEE COMPUT SOC C CO, DOI [10.1109/CVPR.2007.383331, DOI 10.1109/CVPR.2007.383331]
   Hamilton AFD, 2007, PSYCHOL RES-PSYCH FO, V71, P13, DOI 10.1007/s00426-005-0032-4
   Ho ESL, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778770
   Holden D, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925975
   Hsu E, 2005, ACM T GRAPHIC, V24, P1082, DOI 10.1145/1073204.1073315
   Hu R, 2018, COMPUT GRAPH FORUM, V37, P603, DOI 10.1111/cgf.13385
   Hu RZ, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201287
   Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3
   Jiang Y, 2016, IEEE T PATTERN ANAL, V38, P2040, DOI 10.1109/TPAMI.2015.2501811
   Jiang Y, 2013, PROC CVPR IEEE, P2993, DOI 10.1109/CVPR.2013.385
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kang C, 2017, GRAPH MODELS, V94, P25, DOI 10.1016/j.gmod.2017.10.002
   Kato K, 2018, LECT NOTES COMPUT SC, V11218, P247, DOI 10.1007/978-3-030-01264-9_15
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Koppula HS, 2013, INT J ROBOT RES, V32, P951, DOI 10.1177/0278364913478446
   Li C, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P786
   Li XT, 2019, PROC CVPR IEEE, P12360, DOI 10.1109/CVPR.2019.01265
   Liu C., 2017, ARXIV PREPRINT ARXIV
   Liu J, 2017, PROC CVPR IEEE, P3671, DOI 10.1109/CVPR.2017.391
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Lo Presti L, 2016, PATTERN RECOGN, V53, P130, DOI 10.1016/j.patcog.2015.11.019
   Mehta D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073596
   Monszpart A, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322961
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Pavlakos G, 2018, PROC CVPR IEEE, P7307, DOI 10.1109/CVPR.2018.00763
   Podda J, 2017, COGNITION, V168, P140, DOI 10.1016/j.cognition.2017.06.023
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   RUNESON S, 1981, J EXP PSYCHOL HUMAN, V7, P733, DOI 10.1037/0096-1523.7.4.733
   Savva M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661230
   Schmidt F, 2017, J VISION, V17, DOI 10.1167/17.3.18
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shen YJ, 2020, IEEE T VIS COMPUT GR, V26, P2620, DOI 10.1109/TVCG.2019.2893247
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   STOFFREGEN TA, 1994, ECOL PSYCHOL, V6, P33, DOI 10.1207/s15326969eco0601_2
   Tekin B, 2016, PROC CVPR IEEE, pCP8, DOI 10.1109/CVPR.2016.113
   Tome D, 2017, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR.2017.603
   VAINA LM, 1995, SYNTHESE, V104, P43, DOI 10.1007/BF01063674
   Wang H, 2021, IEEE T VIS COMPUT GR, V27, P216, DOI 10.1109/TVCG.2019.2936810
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Wu J., 2015, ADV NEURAL INFORM PR, V28, P127
   Wu J. J., 2016, P BRIT MACH VIS C
   Xia SH, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766999
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yao BP, 2010, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2010.5540235
   Yumer ME, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925955
   Zhang PF, 2018, LECT NOTES COMPUT SC, V11213, P136, DOI 10.1007/978-3-030-01240-3_9
NR 62
TC 5
Z9 5
U1 0
U2 13
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2021
VL 7
IS 3
BP 375
EP 392
DI 10.1007/s41095-021-0218-8
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA TR1TJ
UT WOS:000678754100007
OA Green Submitted, gold
DA 2024-07-18
ER

PT J
AU Wang, AZ
   He, C
   Hou, F
   Cai, ZC
   Zhao, G
AF Wang, Aizeng
   He, Chuan
   Hou, Fei
   Cai, Zhanchuan
   Zhao, Gang
TI Designing planar cubic B-spline curves with monotonic curvature for
   curve interpolation
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
C1 [Wang, Aizeng; He, Chuan; Zhao, Gang] Beihang Univ, Sch Mech Engn & Automat, Beijing 100191, Peoples R China.
   [Wang, Aizeng; He, Chuan; Zhao, Gang] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Hou, Fei] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100190, Peoples R China.
   [Hou, Fei] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Cai, Zhanchuan] Macau Univ Sci & Technol, Fac Informat Technol, Macau 999078, Peoples R China.
C3 Beihang University; Beihang University; Chinese Academy of Sciences;
   Institute of Software, CAS; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; Macau University of Science &
   Technology
RP Zhao, G (corresponding author), Beihang Univ, Sch Mech Engn & Automat, Beijing 100191, Peoples R China.; Zhao, G (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM azwang@buaa.edu.cn; hc1994@buaa.edu.cn; houfei@ios.ac.cn;
   zccai@must.edu.mo; zhaog@buaa.edu.cn
RI He, Chuan/AAN-4356-2020; Zhao, Gang/JMC-6248-2023
OI He, Chuan/0000-0002-8532-2898; 
FU State Key Laboratory of Lunar and Planetary Sciences, Macau University
   of Science and Technology (Macau FDCT) [119/2017/A3]; Open Project
   Program of the State Key Lab of CAD&CG, Zhejiang University [A2024];
   National Natural Science Foundation of China [61572056, 61872347];
   Special Plan for the Development of Distinguished Young Scientists of
   ISCAS [Y8RC535018]
FX This work was supported by the Opening Fund of the State Key Laboratory
   of Lunar and Planetary Sciences, Macau University of Science and
   Technology (Macau FDCT Grant No. 119/2017/A3), the Open Project Program
   of the State Key Lab of CAD&CG, Zhejiang University (No. A2024), the
   National Natural Science Foundation of China (Nos. 61572056, 61872347),
   and the Special Plan for the Development of Distinguished Young
   Scientists of ISCAS (No. Y8RC535018).
CR Birkhoff G.D., 2013, Aesthetic Measure
   Farin G., 2014, CURVES SURFACES COMP
   Harary G, 2012, COMP GEOM-THEOR APPL, V45, P115, DOI 10.1016/j.comgeo.2011.10.001
   Harary G, 2011, COMPUT GRAPH FORUM, V30, P237, DOI 10.1111/j.1467-8659.2011.01855.x
   Lin H., 2016, Comput. Vis. Media, V2, P329
   Zhou HL, 2012, COMPUT GRAPH-UK, V36, P642, DOI 10.1016/j.cag.2012.04.001
NR 6
TC 3
Z9 4
U1 0
U2 10
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2020
VL 6
IS 3
BP 349
EP 354
DI 10.1007/s41095-020-0182-8
PG 6
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FH
UT WOS:000648691900009
OA gold
DA 2024-07-18
ER

PT J
AU Mei, J
   Zheng, YB
   Cheng, MM
AF Mei, Jie
   Zheng, Yi-Bo
   Cheng, Ming-Ming
TI D2ANet: Difference-aware attention network for multi-level change
   detection from satellite imagery
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE change detection; building localization; satellite imagery;
   dual-temporal aggregation; difference attention
ID BUILDING CHANGE DETECTION; REMOTE-SENSING IMAGES; MODEL
AB Recognizing dynamic variations on the ground, especially changes caused by various natural disasters, is critical for assessing the severity of the damage and directing the disaster response. However, current workflows for disaster assessment usually require human analysts to observe and identify damaged buildings, which is labor-intensive and unsuitable for large-scale disaster areas. In this paper, we propose a difference-aware attention network (D2ANet) for simultaneous building localization and multi-level change detection from the dual-temporal satellite imagery. Considering the differences in different channels in the features of pre- and post-disaster images, we develop a dual-temporal aggregation module using paired features to excite change-sensitive channels of the features and learn the global change pattern. Since the nature of building damage caused by disasters is diverse in complex environments, we design a difference-attention module to exploit local correlations among the multi-level changes, which improves the ability to identify damage on different scales. Extensive experiments on the large-scale building damage assessment dataset xBD demonstrate that our approach provides new state-of-the-art results.
C1 [Mei, Jie; Cheng, Ming-Ming] Nankai Univ, TMCC, CS, Tianjin 300350, Peoples R China.
   [Zheng, Yi-Bo] Beijing Normal Univ, Fac Geog Sci, State Key Lab Earth Surface Proc & Resource Ecol, Beijing 100875, Peoples R China.
C3 Nankai University; Beijing Normal University
RP Cheng, MM (corresponding author), Nankai Univ, TMCC, CS, Tianjin 300350, Peoples R China.
EM cmm@nankai.edu.cn
RI mei, jie/KMA-6084-2024; Cheng, Ming-Ming/A-2527-2009
OI Cheng, Ming-Ming/0000-0001-5550-8758
FU National Key R&D Program of China [2018AAA0100400]; Fundamental Research
   Funds for the Central Universities (Nankai University) [63223050];
   National Natural Science Foundation of China [62176130]
FX AcknowledgementsThis work is supported by the National Key R&D Program
   of China (Grant No. 2018AAA0100400), Fundamental Research Funds for the
   Central Universities (Nankai University, Grant No. 63223050), and
   National Natural Science Foundation of China (Grant No. 62176130).
CR [Anonymous], 2018, ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci, DOI DOI 10.5194/ISPRS-ANNALS-IV-2-89-2018
   Bello I, 2019, IEEE I CONF COMP VIS, P3285, DOI 10.1109/ICCV.2019.00338
   Bovcon B, 2022, IEEE T CYBERNETICS, V52, P12661, DOI 10.1109/TCYB.2021.3085856
   Chaurasia A, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Chen CF, 2013, REMOTE SENS-BASEL, V5, P6408, DOI 10.3390/rs5126408
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Chen YP, 2017, ADV NEUR IN, V30
   CHOLLET F, 2017, PROC CVPR IEEE, P1800, DOI DOI 10.1109/CVPR.2017.195
   Daudt Rodrigo Caye, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Proceedings, P1461, DOI 10.1109/CVPRW.2019.00187
   Daudt RC, 2018, IEEE IMAGE PROC, P4063, DOI 10.1109/ICIP.2018.8451652
   Demir I, 2018, IEEE COMPUT SOC CONF, P172, DOI 10.1109/CVPRW.2018.00031
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dosovitskiy Alexey, 2021, ICLR
   Fu Jun, 2019, IEEE Trans Image Process, DOI 10.1109/TIP.2019.2895460
   Gapper JJ, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11131525
   Golovanov S, 2018, IEEE COMPUT SOC CONF, P219, DOI 10.1109/CVPRW.2018.00040
   Gueguen L, 2015, PROC CVPR IEEE, P1321, DOI 10.1109/CVPR.2015.7298737
   Guo M.-H., 2022, arXiv
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   Gupta R., 2019, P IEEE CVF C COMP VI, P10
   Gupta R, 2021, INT C PATT RECOG, P4405, DOI 10.1109/ICPR48806.2021.9412295
   Hamaguchi R, 2018, IEEE COMPUT SOC CONF, P223, DOI 10.1109/CVPRW.2018.00041
   Hauskrecht M, 2013, J BIOMED INFORM, V46, P47, DOI 10.1016/j.jbi.2012.08.004
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Huiyu Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P108, DOI 10.1007/978-3-030-58548-8_7
   Im J, 2005, REMOTE SENS ENVIRON, V99, P326, DOI 10.1016/j.rse.2005.09.008
   Ji M, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10111689
   Kasetkasem T, 2002, IEEE T GEOSCI REMOTE, V40, P1815, DOI 10.1109/TGRS.2002.802498
   Kim JY, 2018, INFORM SCIENCES, V460, P83, DOI 10.1016/j.ins.2018.04.092
   Lei YJ, 2021, IEEE T IMAGE PROCESS, V30, P55, DOI 10.1109/TIP.2020.3031173
   Li LY, 2023, IEEE T KNOWL DATA EN, V35, P6058, DOI 10.1109/TKDE.2022.3171562
   Li LY, 2021, IEEE T NEUR NET LEAR, V32, P1177, DOI 10.1109/TNNLS.2020.2980749
   Lin D, 2020, IEEE T CYBERNETICS, V50, P1120, DOI 10.1109/TCYB.2018.2885062
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu Y, 2021, IEEE GEOSCI REMOTE S, V18, P811, DOI 10.1109/LGRS.2020.2988032
   Liu Z, 2018, IEEE T IMAGE PROCESS, V27, P1822, DOI 10.1109/TIP.2017.2784560
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu XQ, 2017, IEEE T CYBERNETICS, V47, P884, DOI 10.1109/TCYB.2016.2531179
   Mahdavi S, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11161854
   Marin C, 2015, IEEE T GEOSCI REMOTE, V53, P2664, DOI 10.1109/TGRS.2014.2363548
   Mei J, 2022, IEEE T PATTERN ANAL, V44, P4374, DOI 10.1109/TPAMI.2021.3065086
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Nemmour H, 2006, ISPRS J PHOTOGRAMM, V61, P125, DOI 10.1016/j.isprsjprs.2006.09.004
   Pan XR, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11080917
   Papadomanolaki M, 2019, INT GEOSCI REMOTE SE, P214, DOI [10.1109/IGARSS.2019.8900330, 10.1109/igarss.2019.8900330]
   Paszke A, 2019, ADV NEUR IN, V32
   Ramachandran P, 2019, ADV NEUR IN, V32
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rudner TGJ, 2019, AAAI CONF ARTIF INTE, P702
   Rüther H, 2002, ISPRS J PHOTOGRAMM, V56, P269, DOI 10.1016/S0924-2716(02)00062-X
   Shen Y, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3080580
   Shi WZ, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12101688
   SINGH A, 1989, INT J REMOTE SENS, V10, P989, DOI 10.1080/01431168908903939
   Sirmaçek B, 2008, 23RD INTERNATIONAL SYMPOSIUM ON COMPUTER AND INFORMATION SCIENCES, P6
   Taghanaki SA, 2019, COMPUT MED IMAG GRAP, V75, P24, DOI 10.1016/j.compmedimag.2019.04.005
   Tsai VJD, 2006, IEEE T GEOSCI REMOTE, V44, P1661, DOI 10.1109/TGRS.2006.869980
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Van Etten A., 2018, arXiv
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HZ, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9050446
   Wang JH, 2020, IEEE T CYBERNETICS, V50, P2971, DOI 10.1109/TCYB.2019.2891265
   Wang MY, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12020205
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang Y, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3383-y
   Weber E., 2020, P INT C LEARNING REP
   Wu C, 2022, IEEE T CYBERNETICS, V52, P12084, DOI 10.1109/TCYB.2021.3086884
   Wu CY, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13050905
   Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI [10.1109/CSTIC.2018.8369274, 10.1007/s11263-019-01198-w]
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu J., 2019, arXiv
   Xu YF, 2022, COMPUT VIS MEDIA, V8, P33, DOI 10.1007/s41095-021-0247-3
   Yang X, 2023, IEEE T PATTERN ANAL, V45, P2384, DOI 10.1109/TPAMI.2022.3166956
   Yang X, 2022, INT J COMPUT VISION, V130, P1340, DOI 10.1007/s11263-022-01593-w
   Yu J, 2021, IEEE T CYBERNETICS, V51, P1731, DOI 10.1109/TCYB.2020.2969046
   Yuan JY, 2018, IEEE T PATTERN ANAL, V40, P2793, DOI 10.1109/TPAMI.2017.2750680
   Zha K, 2018, IEEE COMPUT SOC CONF, P242, DOI 10.1109/CVPRW.2018.00045
   Zhao H., 2020, P IEEE CVF C COMP VI, V2020, P10076, DOI 10.1109/CVPR42600.2020.01009
   Zhong P, 2007, IEEE T GEOSCI REMOTE, V45, P3978, DOI 10.1109/TGRS.2007.907109
   Zhu X. Y., 2021, P IEEE WINTER C APPL
NR 83
TC 10
Z9 11
U1 5
U2 12
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2023
VL 9
IS 3
BP 563
EP 579
DI 10.1007/s41095-022-0325-1
EA MAR 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA G9DP6
UT WOS:000946186100001
OA gold
DA 2024-07-18
ER

PT J
AU Yi, YA
   Sun, DY
   Li, PX
   Kim, TK
   Xu, TM
   Pei, YR
AF Yi, Yunai
   Sun, Diya
   Li, Peixin
   Kim, Tae-Kyun
   Xu, Tianmin
   Pei, Yuru
TI Unsupervised random forest for affinity estimation
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE affinity estimation; forest-based metric; unsupervised clustering
   forest; pseudo-leaf-splitting (PLS)
ID SHAPE; CLASSIFICATION; SEGMENTATION; REGRESSION
AB This paper presents an unsupervised clustering random-forest-based metric for affinity estimation in large and high-dimensional data. The criterion used for node splitting during forest construction can handle rank-deficiency when measuring cluster compactness. The binary forest-based metric is extended to continuous metrics by exploiting both the common traversal path and the smallest shared parent node.
   The proposed forest-based metric efficiently estimates affinity by passing down data pairs in the forest using a limited number of decision trees. A pseudo-leaf-splitting (PLS) algorithm is introduced to account for spatial relationships, which regularizes affinity measures and overcomes inconsistent leaf assign-ments. The random-forest-based metric with PLS facilitates the establishment of consistent and point-wise correspondences. The proposed method has been applied to automatic phrase recognition using color and depth videos and point-wise correspondence. Extensive experiments demonstrate the effectiveness of the proposed method in affinity estimation in a comparison with the state-of-the-art.
C1 [Yi, Yunai; Sun, Diya; Li, Peixin; Pei, Yuru] Peking Univ, Dept Machine Intelligence, Key Lab Machine Percept MOE, Beijing 100871, Peoples R China.
   [Kim, Tae-Kyun] Imperial Coll London, Dept Elect & Elect Engn, London, England.
   [Xu, Tianmin] Peking Univ, Stomatol Hosp, Sch Stomatol, Beijing 100081, Peoples R China.
C3 Peking University; Imperial College London; Peking University
RP Pei, YR (corresponding author), Peking Univ, Dept Machine Intelligence, Key Lab Machine Percept MOE, Beijing 100871, Peoples R China.
EM yiyunai521@126.com; dysun@pku.edu.cn; lipeixin@pku.edu.cn;
   yrpei@pku.edu.cn
RI Kim, Tae-Kyun/HTL-2208-2023
OI Kim, Tae-Kyun/0000-0002-7587-6053
FU National Natural Science Foundation of China [61876008, 82071172];
   Beijing Natural Science Foundation [7192227]; Research Center of
   Engineering and Technology for Digital Dentistry, the Ministry of Health
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant Nos. 61876008 and 82071172, Beijing
   Natural Science Foundation under Grant No. 7192227, and the Research
   Center of Engineering and Technology for Digital Dentistry, the Ministry
   of Health.
CR Aflalo Y, 2016, INT J COMPUT VISION, V118, P380, DOI 10.1007/s11263-016-0883-8
   Alzubaidi L., 2018, J ENG APPL SCI, V13, P9189
   Nguyen A, 2011, COMPUT GRAPH FORUM, V30, P1481, DOI 10.1111/j.1467-8659.2011.02022.x
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], 2011, ACM T GRAPHIC, V30
   [Anonymous], 2006, P 20 ANN C NEUR INF
   [Anonymous], 2008, ACM T GRAPHIC, V27
   [Anonymous], 2013, ACM T GRAPHIC, V32
   [Anonymous], 2012, ACM T GRAPHIC, V31
   [Anonymous], 2013, Decision forests for computer vision and medical image analysis, DOI DOI 10.1007/978-1-4471-4929-3
   [Anonymous], 2014, ACM T GRAPHIC, V33
   Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Bing Liu, 2000, Proceedings of the Ninth International Conference on Information and Knowledge Management. CIKM 2000, P20
   Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491
   Boscaini D, 2016, COMPUT GRAPH FORUM, V35, P431, DOI 10.1111/cgf.12844
   Boyer DM, 2011, P NATL ACAD SCI USA, V108, P18221, DOI 10.1073/pnas.1112822108
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1
   Brox T, 2010, LECT NOTES COMPUT SC, V6315, P282, DOI 10.1007/978-3-642-15555-0_21
   Chen QF, 2015, IEEE I CONF COMP VIS, P2039, DOI 10.1109/ICCV.2015.236
   Chung JS, 2017, LECT NOTES COMPUT SC, V10117, P251, DOI 10.1007/978-3-319-54427-4_19
   Chung JS, 2017, LECT NOTES COMPUT SC, V10112, P87, DOI 10.1007/978-3-319-54184-6_6
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Criminisil A, 2011, FOUND TRENDS COMPUT, V7, P81, DOI [10.1561/0600000035, 10.1501/0000000035]
   DICE LR, 1945, ECOLOGY, V26, P297, DOI 10.2307/1932409
   Gall J, 2011, IEEE T PATTERN ANAL, V33, P2188, DOI 10.1109/TPAMI.2011.70
   Ganapathi-Subramanian V, 2018, COMPUT GRAPH FORUM, V37, P199, DOI 10.1111/cgf.13502
   Groueix T, 2018, LECT NOTES COMPUT SC, V11206, P235, DOI 10.1007/978-3-030-01216-8_15
   Hengl T, 2018, PEERJ, V6, DOI 10.7717/peerj.5518
   Huang QX, 2013, COMPUT GRAPH FORUM, V32, P177, DOI 10.1111/cgf.12184
   Jeung M, 2019, J HYDROL, V575, P1099, DOI 10.1016/j.jhydrol.2019.05.079
   Kanavati F, 2017, PATTERN RECOGN, V63, P561, DOI 10.1016/j.patcog.2016.09.026
   Lee D, 2017, LECT NOTES COMPUT SC, V10117, P290, DOI 10.1007/978-3-319-54427-4_22
   Li ZH, 2018, IEEE T NEUR NET LEAR, V29, P6323, DOI 10.1109/TNNLS.2018.2829867
   Litany O, 2017, IEEE I CONF COMP VIS, P5660, DOI 10.1109/ICCV.2017.603
   Pei YR, 2018, IEEE T MED IMAGING, V37, P2310, DOI 10.1109/TMI.2018.2829629
   Pei YR, 2017, I S BIOMED IMAGING, P481, DOI 10.1109/ISBI.2017.7950565
   Pei YR, 2016, IEEE IMAGE PROC, P2941, DOI 10.1109/ICIP.2016.7532898
   Pei YR, 2013, IEEE I CONF COMP VIS, P129, DOI 10.1109/ICCV.2013.23
   Pietikainen M., 2015, 11 IEEE INT C WORKSH, P1, DOI DOI 10.1109/FG.2015.7163155
   Rao S, 2010, IEEE T PATTERN ANAL, V32, P1832, DOI 10.1109/TPAMI.2009.191
   Rodolà E, 2014, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2014.532
   Sahillioglu Y, 2011, COMPUT GRAPH FORUM, V30, P1461, DOI 10.1111/j.1467-8659.2011.02020.x
   Shi T, 2006, J COMPUT GRAPH STAT, V15, P118, DOI 10.1198/106186006X94072
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Sun D., 2018, P BRIT MACH VIS C
   Vrigkas M, 2014, COMPUT VIS IMAGE UND, V119, P27, DOI 10.1016/j.cviu.2013.11.007
   Wang F, 2013, IEEE I CONF COMP VIS, P849, DOI 10.1109/ICCV.2013.110
   Wang WY, 2019, PROC CVPR IEEE, P1038, DOI 10.1109/CVPR.2019.00113
   Wei L. Y., 2015, ARXIV PREPRINT ARXIV
   Yesilkanat CM, 2020, CHAOS SOLITON FRACT, V140, DOI 10.1016/j.chaos.2020.110210
   Yu G, 2011, PROC CVPR IEEE, P865, DOI 10.1109/CVPR.2011.5995488
   Yuru Pei, 2017, Medical Image Computing and Computer Assisted Intervention  MICCAI 2017. 20th International Conference. Proceedings: LNCS 10433, P283, DOI 10.1007/978-3-319-66182-7_33
   Zhao GY, 2009, IEEE T MULTIMEDIA, V11, P1254, DOI 10.1109/TMM.2009.2030637
   Zhou ZH, 2014, IEEE T PATTERN ANAL, V36, P181, DOI 10.1109/TPAMI.2013.173
   Zhu XT, 2014, PROC CVPR IEEE, P1450, DOI 10.1109/CVPR.2014.188
   Zhu XT, 2013, IEEE I CONF COMP VIS, P81, DOI 10.1109/ICCV.2013.17
NR 58
TC 7
Z9 7
U1 1
U2 18
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2022
VL 8
IS 2
BP 257
EP 272
DI 10.1007/s41095-021-0241-9
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XJ0YS
UT WOS:000726525400006
PM 34900375
OA gold, Green Published
DA 2024-07-18
ER

PT J
AU Waschk, A
   Krüger, J
AF Waschk, Andre
   Krueger, Jens
TI Automatic route planning for GPS art generation
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE GPS art; walking; routing; shape detection; navigation
ID SNAKES
AB In this paper, we present a novel approach to automated route generation of global positioning system (GPS) artwork. The term GPS artwork describes the generation of drawings by leaving virtual traces on digital maps. Until now, the creation of these images has required a manual planning phase in which an artist designs the route by hand. Once the route for this artwork has been planned, GPS devices have been used to track the movement. Using our solution, the lengthy planning phase can be significantly shortened, thereby opening art creation to a broader public.
C1 [Waschk, Andre; Krueger, Jens] Univ Duisburg Essen, D-47057 Duisburg, Germany.
C3 University of Duisburg Essen
RP Waschk, A; Krüger, J (corresponding author), Univ Duisburg Essen, D-47057 Duisburg, Germany.
EM Andre.Waschk@uni-due.de; Jens.Krueger@uni-due.de
CR ACM, 2016, T GRAPHICS, V35, P4
   [Anonymous], 2015, CREATIVE SPIN PEDALI
   Balduz P., 2017, WALK LINE DRAWING
   Dijkstra EW., 1959, NUMER MATH, V1, P269, DOI [10.1007/BF01386390, DOI 10.1007/BF01386390]
   Google Inc, 2005, GOOGL MAPS
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Lee YJ, 2002, COMPUT GRAPH FORUM, V21, P229, DOI 10.1111/1467-8659.t01-1-00582
   Lin SS, 2014, IEEE T VIS COMPUT GR, V20, P1241, DOI 10.1109/TVCG.2014.2312010
   Lund S, 2015, GPS DOODLES
   Microsoft, 2005, BING MAPS
   OpenStreetMap Contributors, 2006, OPENSTREETMAP
   Reimann B, 2013, 12 UEBER DARSTELLBAR, P213, DOI [10.1017/CBO9781139568050.013, DOI 10.1017/CBO9781139568050.013]
   Rosner DK, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P397, DOI 10.1145/2702123.2702467
   Strava, 2018, STRAVA
   Takahashi Y., 2015, YASSANS GPS DRAWING
   UNESCO, 1994, LIN GEOGL NASC PALP
   Wood J., 2010, TRAVERSE ME
   Yang H, 2018, COMPUT GRAPH FORUM, V37, P191, DOI 10.1111/cgf.13559
NR 18
TC 3
Z9 3
U1 2
U2 12
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2019
VL 5
IS 3
BP 303
EP 310
DI 10.1007/s41095-019-0146-z
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UN
UT WOS:000648690900006
OA gold
DA 2024-07-18
ER

PT J
AU Liu, M
   Shi, YF
   Zheng, LT
   Xu, K
   Huang, H
   Manocha, D
AF Liu, Min
   Shi, Yifei
   Zheng, Lintao
   Xu, Kai
   Huang, Hui
   Manocha, Dinesh
TI Recurrent 3D attentional networks for end-to-end active object
   recognition
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE active object recognition; recurrent neural network; next-best-view; 3D
   attention
AB Active vision is inherently attention-driven: an agent actively selects views to attend in order to rapidly perform a vision task while improving its internal representation of the scene being observed. Inspired by the recent success of attention-based models in 2D vision tasks based on single RGB images, we address multi-view depth-based active object recognition using an attention mechanism, by use of an end-to-end recurrent 3D attentional network. The architecture takes advantage of a recurrent neural network to store and update an internal representation. Our model, trained with 3D shape datasets, is able to iteratively attend the best views targeting an object of interest for recognizing it. To realize 3D view selection, we derive a 3D spatial transformer network. It is differentiable, allowing training with backpropagation, and so achieving much faster convergence than the reinforcement learning employed by most existing attention-based models. Experiments show that our method, with only depth input, achieves state-of-the-art next-best-view performance both in terms of time taken and recognition accuracy.
C1 [Liu, Min; Shi, Yifei; Zheng, Lintao; Xu, Kai] Natl Univ Def Technol, Sch Comp, Changsha 410073, Peoples R China.
   [Liu, Min; Manocha, Dinesh] Univ Maryland, Dept Comp Sci & Elect & Comp Engn, College Pk, MD 20742 USA.
   [Huang, Hui] Shenzhen Univ, Visual Comp Res Ctr, Shenzhen 518060, Peoples R China.
C3 National University of Defense Technology - China; University System of
   Maryland; University of Maryland College Park; Shenzhen University
RP Xu, K (corresponding author), Natl Univ Def Technol, Sch Comp, Changsha 410073, Peoples R China.
EM gfsliumin@gmail.com; jerrysyf@gmail.com; kevin.kai@ugmail.com;
   huihuang@szu.edu.cn; dm@cs.umd.edu
RI Shi, Yifei/ABI-4966-2020; Huang, Hui/JGB-1049-2023
OI Shi, Yifei/0000-0002-6182-7365; Huang, Hui/0000-0003-3212-0544
FU National Natural Science Foundation of China [61572507, 61622212,
   61532003]; China Scholarship Council
FX We thank the anonymous reviewers for their valuable comments. This work
   was supported, in part, by National Natural Science Foundation of China
   (Nos. 61572507, 61622212, and 61532003). Min Liu is supported by the
   China Scholarship Council.
CR ACM, 2016, ACM T GRAPHICS, V35, P6
   [Anonymous], 2014, CORR
   [Anonymous], 1989, Complex Syst.
   Arbel T, 2001, IMAGE VISION COMPUT, V19, P779, DOI 10.1016/S0262-8856(00)00103-7
   BAJCSY R, 1988, P IEEE, V76, P996, DOI 10.1109/5.5968
   Borotschnig H, 2000, IMAGE VISION COMPUT, V18, P715, DOI 10.1016/S0262-8856(99)00075-X
   Callari FG, 2001, INT J COMPUT VISION, V43, P189, DOI 10.1023/A:1011135513777
   Chang A X, 2015, COMPUTER SCI, V1512, P3
   Chen S, 2018, IEEE T VISUALIZATION
   Corbetta M, 2002, NAT REV NEUROSCI, V3, P201, DOI 10.1038/nrn755
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Denzler J, 2002, IEEE T PATTERN ANAL, V24, P145, DOI 10.1109/34.982896
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Huber M. F., 2012, 2012 15th International Conference on Information Fusion (FUSION 2012), P1718
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jayaraman D, 2016, LECT NOTES COMPUT SC, V9909, P489, DOI 10.1007/978-3-319-46454-1_30
   Johns E, 2016, PROC CVPR IEEE, P3813, DOI 10.1109/CVPR.2016.414
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kurniawati H., 2008, P ROB SCI SYST, V2008
   Lauri M, 2015, ACTIVE OBJECT RECOGN
   Levine S, 2016, J MACH LEARN RES, V17
   Malmir M, 2016, DEEP Q LEARNING ACTI, P161
   Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684
   Paletta L, 2000, ROBOT AUTON SYST, V31, P71, DOI 10.1016/S0921-8890(99)00079-2
   Qi CR, 2017, ADV NEUR IN, V30
   Roy SD, 2004, PATTERN RECOGN, V37, P429, DOI 10.1016/j.patcog.2003.01.002
   Scott WR, 2003, ACM COMPUT SURV, V35, P64, DOI 10.1145/641865.641868
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Xie SN, 2018, PROC CVPR IEEE, P4606, DOI 10.1109/CVPR.2018.00484
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
NR 32
TC 10
Z9 10
U1 2
U2 10
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2019
VL 5
IS 1
BP 91
EP 104
DI 10.1007/s41095-019-0135-2
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UL
UT WOS:000648690700008
OA gold
DA 2024-07-18
ER

PT J
AU Sun, CY
   Tong, X
   Liu, Y
AF Sun, Chun-Yu
   Tong, Xin
   Liu, Yang
TI Semantic segmentation-assisted instance feature fusion for multi-level
   3D part instance segmentation
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE 3D part instance segmentation; feature fusion; 3D deep learning
AB Recognizing 3D part instances from a 3D point cloud is crucial for 3D structure and scene understanding. Several learning-based approaches use semantic segmentation and instance center prediction as training tasks and fail to further exploit the inherent relationship between shape semantics and part instances. In this paper, we present a new method for 3D part instance segmentation. Our method exploits semantic segmentation to fuse nonlocal instance features, such as center prediction, and further enhances the fusion scheme in a multi- and cross-level way. We also propose a semantic region center prediction task to train and leverage the prediction results to improve the clustering of instance points. Our method outperforms existing methods with a large-margin improvement in the PartNet benchmark. We also demonstrate that our feature fusion scheme can be applied to other existing methods to improve their performance in indoor scene instance segmentation tasks.
C1 [Sun, Chun-Yu] Tsinghua Univ, Inst Adv Study, Beijing 100084, Peoples R China.
   [Tong, Xin; Liu, Yang] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Tsinghua University; Microsoft; Microsoft Research Asia
RP Liu, Y (corresponding author), Microsoft Res Asia, Beijing 100080, Peoples R China.
EM sunchyqd@gmail.com; xtong@microsoft.com; yangliu@microsoft.com
RI Liu, Yang/ABD-2239-2020
OI Liu, Yang/0000-0002-3768-6654
CR Abadi Martin, 2016, arXiv
   Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Bai M, 2017, PROC CVPR IEEE, P2858, DOI 10.1109/CVPR.2017.305
   Chen SY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15447, DOI 10.1109/ICCV48922.2021.01518
   Chen XL, 2019, IEEE I CONF COMP VIS, P2061, DOI 10.1109/ICCV.2019.00215
   Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Dai JF, 2016, LECT NOTES COMPUT SC, V9910, P534, DOI 10.1007/978-3-319-46466-4_32
   Engelmann F., 2020, CVPR, P9028, DOI 10.1109/CVPR42600.2020.00905
   Graham Benjamin, 2017, arXiv
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Hafiz AM, 2020, INT J MULTIMED INF R, V9, P171, DOI 10.1007/s13735-020-00195-x
   Haiyong Jiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12793, DOI 10.1109/CVPR42600.2020.01281
   Han L, 2020, PROC CVPR IEEE, P2937, DOI 10.1109/CVPR42600.2020.00301
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He T, 2021, PROC CVPR IEEE, P354, DOI 10.1109/CVPR46437.2021.00042
   He Y., 2021, arXiv
   Hou J, 2019, PROC CVPR IEEE, P4416, DOI 10.1109/CVPR.2019.00455
   Jiang L, 2020, PROC CVPR IEEE, P4866, DOI 10.1109/CVPR42600.2020.00492
   Lahoud J, 2019, IEEE I CONF COMP VIS, P9255, DOI 10.1109/ICCV.2019.00935
   Liang ZH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2763, DOI 10.1109/ICCV48922.2021.00278
   Liu C, 2019, ARXIV
   Liu S, 2020, ARXIV
   Minaee S, 2022, IEEE T PATTERN ANAL, V44, P3523, DOI 10.1109/TPAMI.2021.3059968
   Mo KC, 2019, PROC CVPR IEEE, P909, DOI 10.1109/CVPR.2019.00100
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Qi CR, 2017, arXiv
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sharma Gopal, 2020, COMPUTER VISION ECCV, P261, DOI DOI 10.1007/978-3-030-58571-6_16
   Tan JG, 2021, IET COMPUT VIS, V15, P366, DOI 10.1049/cvi2.12033
   Tchapmi LP, 2017, INT CONF 3D VISION, P537, DOI 10.1109/3DV.2017.00067
   Wang PS, 2020, IEEE COMPUT SOC CONF, P1074, DOI 10.1109/CVPRW50498.2020.00141
   Wang WY, 2018, PROC CVPR IEEE, P2569, DOI 10.1109/CVPR.2018.00272
   Wang XL, 2019, PROC CVPR IEEE, P4091, DOI 10.1109/CVPR.2019.00422
   Xinlong Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P649, DOI 10.1007/978-3-030-58523-5_38
   Yang B., 2019, ADV NEURAL INFORM PR, P6740
   Yi L, 2019, PROC CVPR IEEE, P3942, DOI 10.1109/CVPR.2019.00407
   Yu FG, 2019, PROC CVPR IEEE, P9483, DOI 10.1109/CVPR.2019.00972
   Zhang BA, 2021, PROC CVPR IEEE, P8879, DOI 10.1109/CVPR46437.2021.00877
   Zhang FH, 2020, IEEE INT CONF ROBOT, P9448, DOI [10.1109/ICRA40945.2020.9196622, 10.1109/icra40945.2020.9196622]
   Zhang HY, 2021, INT J INNOV COMPUT I, V17, P1041, DOI 10.24507/ijicic.17.03.1041
   Zhao L, 2020, AAAI CONF ARTIF INTE, V34, P12951
NR 43
TC 0
Z9 0
U1 7
U2 23
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2023
VL 9
IS 4
BP 699
EP 715
DI 10.1007/s41095-022-0300-x
EA JUN 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P1QC2
UT WOS:001022137200002
OA Green Submitted, gold
DA 2024-07-18
ER

PT J
AU Wu, ZQ
   Guo, JW
   Zhuang, CQ
   Xiao, J
   Yan, DM
   Zhang, XP
AF Wu, Zhongqi
   Guo, Jianwei
   Zhuang, Chuanqing
   Xiao, Jun
   Yan, Dong-Ming
   Zhang, Xiaopeng
TI Joint specular highlight detection and removal in single images via
   Unet-Transformer
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE specular highlight detection; specular highlight removal;
   Unet-Transformer
ID COLOR CONSTANCY; REFLECTION COMPONENTS; SEPARATION; POLARIZATION
AB Specular highlight detection and removal is a fundamental problem in computer vision and image processing. In this paper, we present an efficient end-to-end deep learning model for automatically detecting and removing specular highlights in a single image. In particular, an encoder-decoder network is utilized to detect specular highlights, and then a novel Unet-Transformer network performs highlight removal; we append transformer modules instead of feature maps in the Unet architecture. We also introduce a highlight detection module as a mask to guide the removal task. Thus, these two networks can be jointly trained in an effective manner. Thanks to the hierarchical and global properties of the transformer mechanism, our framework is able to establish relationships between continuous self-attention layers, making it possible to directly model the mapping between the diffuse area and the specular highlight area, and reduce indeterminacy within areas containing strong specular highlight reflection. Experiments on public benchmark and real-world images demonstrate that our approach outperforms state-of-the-art methods for both highlight detection and removal tasks.
C1 [Wu, Zhongqi; Guo, Jianwei; Yan, Dong-Ming; Zhang, Xiaopeng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Wu, Zhongqi; Guo, Jianwei; Zhuang, Chuanqing; Xiao, Jun; Yan, Dong-Ming; Zhang, Xiaopeng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Guo, JW (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Guo, JW; Xiao, J (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
EM wuzhongqi2019@ia.ac.cn; jianwei.guo@nlpr.ia.ac.cn;
   zhuangchuanqing19@mails.ucas.ac.cn; xiaojun@ucas.ac.cn;
   yandongming@gmail.com; xiaopeng.zhang@ia.ac.cn
RI Wu, zhongqi/GQP-4928-2022; XIAO, JUN/GRS-1866-2022
OI XIAO, JUN/0000-0002-4935-7866
FU National Natural Science Foundation of China [U21A20515, 62172416,
   62172415, U2003109]; Youth Innovation Promotion Association of the
   Chinese Academy of Sciences [2022131]
FX This work was partially funded by the National Natural Science
   Foundation of China (U21A20515, 62172416, 62172415, U2003109), and Youth
   Innovation Promotion Association of the Chinese Academy of Sciences
   (2022131).
CR Akashi Y, 2015, LECT NOTES COMPUT SC, V9007, P611, DOI 10.1007/978-3-319-16814-2_40
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Artusi A, 2011, COMPUT GRAPH FORUM, V30, P2208, DOI 10.1111/j.1467-8659.2011.01971.x
   Brainard DH, 1997, J OPT SOC AM A, V14, P1393, DOI 10.1364/JOSAA.14.001393
   Cao H., 2021, arXiv
   Cui ZP, 2017, PROC CVPR IEEE, P369, DOI 10.1109/CVPR.2017.47
   El Meslouhi O, 2011, OPEN COMPUT SCI, V1, P341, DOI 10.2478/s13537-011-0020-2
   Finayson GD, 2001, IEEE T PATTERN ANAL, V23, P1209, DOI 10.1109/34.969113
   FORSYTH DA, 1990, INT J COMPUT VISION, V5, P5, DOI 10.1007/BF00056770
   Fu G, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1873, DOI 10.1145/3394171.3413586
   Fu G, 2021, PROC CVPR IEEE, P7748, DOI 10.1109/CVPR46437.2021.00766
   Fu G, 2019, COMPUT GRAPH FORUM, V38, P253, DOI 10.1111/cgf.13834
   Funke I, 2018, PROC SPIE, V10576, DOI 10.1117/12.2293755
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Guo J, 2018, LECT NOTES COMPUT SC, V11208, P282, DOI 10.1007/978-3-030-01225-0_17
   Guo XJ, 2014, PROC CVPR IEEE, P2195, DOI 10.1109/CVPR.2014.281
   Hansen T, 2006, NAT NEUROSCI, V9, P1367, DOI 10.1038/nn1794
   Hou SY, 2021, LECT NOTES COMPUT SC, V13022, P115, DOI 10.1007/978-3-030-88013-2_10
   Hoyer PO, 2004, J MACH LEARN RES, V5, P1457
   Imai Y, 2011, LECT NOTES COMPUT SC, V6626, P85, DOI 10.1007/978-3-642-20404-3_7
   Joze HRV, 2014, IEEE T PATTERN ANAL, V36, P860, DOI 10.1109/TPAMI.2013.169
   Khanian Maryam, 2018, Computational Visual Media, V4, P83, DOI 10.1007/s41095-017-0101-9
   Kingma DP., 2014, ADAM METHOD STOCHAST
   Li RY, 2020, IEEE T MED IMAGING, V39, P328, DOI 10.1109/TMI.2019.2926501
   Lin J, 2019, LECT NOTES COMPUT SC, V11482, P3, DOI 10.1007/978-3-030-20205-7_1
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   MALONEY LT, 1986, J OPT SOC AM A, V3, P29, DOI 10.1364/JOSAA.3.000029
   Muhammad S, 2020, IMAGE VISION COMPUT, V93, DOI 10.1016/j.imavis.2019.11.001
   Murmann L, 2019, IEEE I CONF COMP VIS, P4079, DOI 10.1109/ICCV.2019.00418
   Nayar SK, 1997, INT J COMPUT VISION, V21, P163, DOI 10.1023/A:1007937815113
   Osadchy M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1512
   Park JB, 2003, IEEE INT CONF ROBOT, P1397
   Ramadan H, 2020, COMPUT VIS MEDIA, V6, P355, DOI 10.1007/s41095-020-0177-5
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sapiro G, 1999, IEEE T PATTERN ANAL, V21, P1210, DOI 10.1109/34.809114
   SHAFER SA, 1985, COLOR RES APPL, V10, P210, DOI 10.1002/col.5080100409
   Shen HL, 2008, PATTERN RECOGN, V41, P2461, DOI 10.1016/j.patcog.2008.01.026
   Shen HL, 2013, APPL OPTICS, V52, P4483, DOI 10.1364/AO.52.004483
   Shen HL, 2009, APPL OPTICS, V48, P2711, DOI 10.1364/AO.48.002711
   Shi J, 2017, PROC CVPR IEEE, P5844, DOI 10.1109/CVPR.2017.619
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tan P, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P164, DOI 10.1109/ICCV.2003.1238333
   Tan P., 2006, COMPUTER VISION PATT, V2, P1855
   Tan RT, 2004, J OPT SOC AM A, V21, P321, DOI 10.1364/JOSAA.21.000321
   Tan RT, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P870
   Tan RT, 2003, PROC CVPR IEEE, P673
   Tao MW, 2016, IEEE T PATTERN ANAL, V38, P1155, DOI 10.1109/TPAMI.2015.2477811
   Umeyama S, 2004, IEEE T PATTERN ANAL, V26, P639, DOI 10.1109/TPAMI.2004.1273960
   Wang F, 2017, COMPUT VIS IMAGE UND, V158, P31, DOI 10.1016/j.cviu.2017.03.003
   Wei KX, 2019, PROC CVPR IEEE, P8170, DOI 10.1109/CVPR.2019.00837
   Wen SJ, 2021, IEEE T IMAGE PROCESS, V30, P7280, DOI 10.1109/TIP.2021.3104188
   Wu Z. Q., 2020, P SIGGRAPH ASIA 2020
   Wu ZQ, 2022, IEEE T MULTIMEDIA, V24, P3782, DOI 10.1109/TMM.2021.3107688
   Xu YF, 2022, COMPUT VIS MEDIA, V8, P33, DOI 10.1007/s41095-021-0247-3
   Xue ML, 2021, IEEE T MULTIMEDIA, V23, P2706, DOI 10.1109/TMM.2020.3015037
   Yamamoto T, 2017, IEEE IMAGE PROC, P4222, DOI 10.1109/ICIP.2017.8297078
   Yang JW, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P891, DOI 10.1109/ICCVW.2013.122
   Yang QX, 2015, IEEE T PATTERN ANAL, V37, P1304, DOI 10.1109/TPAMI.2014.2360402
   Yi RJ, 2020, AAAI CONF ARTIF INTE, V34, P12685
   Zhang WM, 2019, IEEE T PATTERN ANAL, V41, P611, DOI 10.1109/TPAMI.2018.2803179
NR 60
TC 10
Z9 10
U1 9
U2 74
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2023
VL 9
IS 1
BP 141
EP 154
DI 10.1007/s41095-022-0273-9
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5K7GQ
UT WOS:000869891100009
OA gold
DA 2024-07-18
ER

PT J
AU Luo, S
   Zhang, QT
   Feng, JQ
AF Luo, Shan
   Zhang, Qitong
   Feng, Jieqing
TI Automatic location and semantic labeling of landmarks on 3D human body
   models
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE 3D human body model; descriptor; land-marking; shape alignment
ID OBJECT RECOGNITION; ANIMATION
AB Landmarks on human body models are of great significance for applications such as digital anthropometry and clothing design. The diversity of pose and shape of human body models and the semantic gap make landmarking a challenging problem. In this paper, a learning-based method is proposed to locate landmarks on human body models by analyzing the relationship between geometric descriptors and semantic labels of landmarks. A shape alignment algorithm is proposed to align human body models to break symmetric ambiguity. A symmetry-aware descriptor is proposed based on the structure of the human body models, which is robust to both pose and shape variations in human body models. An AdaBoost regression algorithm is adopted to establish the correspondence between several descriptors and semantic labels of the landmarks. Quantitative and qualitative analyses and comparisons show that the proposed method can obtain more accurate landmarks and distinguish symmetrical landmarks semantically. Additionally, a dataset of landmarked human body models is also provided, containing 271 human body models collected from current human body datasets; each model has 17 landmarks labeled manually.
C1 [Luo, Shan; Zhang, Qitong; Feng, Jieqing] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Peoples R China.
C3 Zhejiang University
RP Feng, JQ (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Peoples R China.
EM luoshan@zju.edu.cn; zhangqitong@zju.edu.cn; jqfeng@cad.zju.edu.cn
OI Luo, Shan/0009-0004-9378-7291
FU National Natural Science Foundation of China [61732015, 61932018,
   61472349]
FX This work was jointly supported by the National Natural Science
   Foundation of China under Grants Nos. 61732015, 61932018, and 61472349.
CR Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Baran I, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239523, 10.1145/1276377.1276467]
   Ben Azouz Z, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P750
   Bogo F., 2014, P IEEE C COMPUTER VI
   Bronstein M. M., 2010, P IEEE COMPUTER SOC
   Chaouch M, 2009, GRAPH MODELS, V71, P63, DOI 10.1016/j.gmod.2008.12.006
   Chen XB, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531379
   Creusot C, 2013, INT J COMPUT VISION, V102, P146, DOI 10.1007/s11263-012-0605-9
   Frome A, 2004, LECT NOTES COMPUT SC, V3023, P224
   Giachetti A., 2014, EUROGRAPHICS WORKSHO
   Guo JW, 2020, COMPUT VIS MEDIA, V6, P95, DOI 10.1007/s41095-020-0163-y
   Guo YL, 2014, IEEE T PATTERN ANAL, V36, P2270, DOI 10.1109/TPAMI.2014.2316828
   Haim N., 2019, P IEEECVF INT C COMP
   Hanocka R, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322959
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Jones PRM, 1997, OPT LASER ENG, V28, P89, DOI 10.1016/S0143-8166(97)00006-7
   Li Y, 2012, TEXT RES J, V82, P622, DOI 10.1177/0040517511418565
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Lovato C, 2014, GRAPH MODELS, V76, P648, DOI 10.1016/j.gmod.2014.07.001
   Luo S, 2020, MULTIMED TOOLS APPL, V79, P20579, DOI 10.1007/s11042-020-08933-3
   Marin R, 2020, COMPUT GRAPH FORUM, V39, P160, DOI 10.1111/cgf.13751
   Maron H, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073616
   Ren J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275040
   RubenWiersma Elmar Eisemann, 2020, ACM T GRAPHIC, V39
   Rustamov R. M, 2007, P 5 EUROGRAPHICS S G
   Shapira L, 2008, VISUAL COMPUT, V24, P249, DOI 10.1007/s00371-007-0197-5
   Shu ZY, 2019, IEEE T VIS COMPUT GR, V25, P2583, DOI 10.1109/TVCG.2018.2848628
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Sung M, 2018, ADV NEUR IN, V31
   Treleaven P, 2007, COMPUTER, V40, P28, DOI 10.1109/MC.2007.225
   Wang HY, 2018, LECT NOTES COMPUT SC, V11212, P3, DOI 10.1007/978-3-030-01237-3_1
   Wang Y. Q., 2019, P IEEECVF C COMPUTER
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu W. X., 2019, P IEEECVF C COMPUTER
   Wuhrer S., 2010, P CANADIAN C COMPUTE
   Xi PC, 2017, 2017 14TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV 2017), P197, DOI 10.1109/CRV.2017.11
   Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6
   Yang Y. P., 2014, P 2 INT C 3D VISION, P4148
   Yang Y, 2019, IEEE T VIS COMPUT GR, V25, P2999, DOI 10.1109/TVCG.2018.2861396
   Yi L., 2017, P IEEE C COMPUTER VI
   You Y., 2020, P IEEECVF C COMPUTER
   Zhou ZK, 2017, NEUROCOMPUTING, V253, P162, DOI 10.1016/j.neucom.2016.09.131
NR 44
TC 0
Z9 0
U1 0
U2 12
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2022
VL 8
IS 4
BP 553
EP 570
DI 10.1007/s41095-021-0254-4
EA MAY 2022
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2X6VI
UT WOS:000796328900002
OA gold
DA 2024-07-18
ER

PT J
AU Sun, CY
   Yang, YQ
   Guo, HX
   Wang, PS
   Tong, X
   Liu, Y
   Shum, HY
AF Sun, Chun-Yu
   Yang, Yu-Qi
   Guo, Hao-Xiang
   Wang, Peng-Shuai
   Tong, Xin
   Liu, Yang
   Shum, Heung-Yeung
TI Semi-supervised 3D shape segmentation with multilevel consistency and
   part substitution
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE shape segmentation; semi-supervised learning; multilevel consistency
ID MESH SEGMENTATION; MODEL
AB The lack of fine-grained 3D shape segmentation data is the main obstacle to developing learning-based 3D segmentation techniques. We propose an effective semi-supervised method for learning 3D segmentations from a few labeled 3D shapes and a large amount of unlabeled 3D data. For the unlabeled data, we present a novel multilevel consistency loss to enforce consistency of network predictions between perturbed copies of a 3D shape at multiple levels: point level, part level, and hierarchical level. For the labeled data, we develop a simple yet effective part substitution scheme to augment the labeled 3D shapes with more structural variations to enhance training. Our method has been extensively validated on the task of 3D object semantic segmentation on PartNet and ShapeNetPart, and indoor scene semantic segmentation on ScanNet. It exhibits superior performance to existing semi-supervised and unsupervised pre-training 3D approaches.
C1 [Sun, Chun-Yu; Yang, Yu-Qi; Guo, Hao-Xiang; Shum, Heung-Yeung] Tsinghua Univ, Inst Adv Study, Beijing 100084, Peoples R China.
   [Wang, Peng-Shuai; Tong, Xin; Liu, Yang] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Tsinghua University; Microsoft; Microsoft Research Asia
RP Liu, Y (corresponding author), Microsoft Res Asia, Beijing 100080, Peoples R China.
EM sunchyqd@gmail.com; yangyq18@mails.tsinghua.edu.cn;
   ghx17@mails.tsinghua.edu.cn; Pengshuai.Wang@microsoft.com;
   xtong@microsoft.com; yangliu@microsoft.com; msraharry@hotmail.com
RI Wang, Peng-Shuai/AAK-6216-2021; Liu, Yang/ABD-2239-2020; Guo,
   Haoxiang/GQR-2700-2022
OI Liu, Yang/0000-0002-3768-6654; Tong, Xin/0000-0001-8788-2453
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Alhashim I, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601102
   Alliegro A, 2021, INT C PATT RECOG, P6718, DOI 10.1109/ICPR48806.2021.9412483
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Berthelot D, 2019, ADV NEUR IN, V32
   Chaudhuri S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964930
   Chen LH, 2022, FOOD CONTROL, V132, DOI 10.1016/j.foodcont.2021.108555
   Chen ZQ, 2019, IEEE I CONF COMP VIS, P8489, DOI 10.1109/ICCV.2019.00858
   Chenyang Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8540, DOI 10.1109/CVPR42600.2020.00857
   Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319
   Dai A, 2018, LECT NOTES COMPUT SC, V11214, P458, DOI 10.1007/978-3-030-01249-6_28
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Deng BY, 2020, PROC CVPR IEEE, P31, DOI 10.1109/CVPR42600.2020.00011
   Fellbaum C., 1998, WORDNET ELECT LEXICA, DOI DOI 10.7551/MITPRESS/7287.001.0001
   French G., 2020, P 31 BRIT MACHINE VI
   Fu Qiang, 2017, IEEE Trans Vis Comput Graph, V23, P2574, DOI 10.1109/TVCG.2017.2739159
   Funkhouser T, 2004, ACM T GRAPHIC, V23, P652, DOI 10.1145/1015706.1015775
   Gadelha Matheus, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P473, DOI 10.1007/978-3-030-58607-2_28
   Genova K., 2020, P IEEECVF C COMPUTER
   Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961
   Guan YR, 2022, IEEE T VIS COMPUT GR, V28, P1758, DOI 10.1109/TVCG.2020.3029759
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Hanocka R, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322959
   Hassani K, 2019, IEEE I CONF COMP VIS, P8159, DOI 10.1109/ICCV.2019.00825
   Hou J, 2021, PROC CVPR IEEE, P15582, DOI 10.1109/CVPR46437.2021.01533
   Huang HB, 2015, COMPUT GRAPH FORUM, V34, P25, DOI 10.1111/cgf.12694
   Huang SS, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3453485
   Kaichun Mo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8856, DOI 10.1109/CVPR42600.2020.00888
   Kalogerakis E, 2017, PROC CVPR IEEE, P6630, DOI 10.1109/CVPR.2017.702
   Kalogerakis E, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778839
   Laine Samuli, 2017, 5 INT C LEARNING REP, DOI DOI 10.48550/ARXIV.1610.02242
   Lee D, 2021, PROC CVPR IEEE, P15895, DOI 10.1109/CVPR46437.2021.01564
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Li RH, 2020, PROC CVPR IEEE, P6377, DOI 10.1109/CVPR42600.2020.00641
   Li YY, 2018, ADV NEUR IN, V31
   Masci J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P832, DOI 10.1109/ICCVW.2015.112
   Mo KC, 2019, PROC CVPR IEEE, P909, DOI 10.1109/CVPR.2019.00100
   Muralikrishnan S, 2018, PROC CVPR IEEE, P2926, DOI 10.1109/CVPR.2018.00309
   Ouali Yassine, 2020, P IEEECVF C COMPUTER, DOI DOI 10.1109/CVPR42600.2020.01269
   Paschalidou D, 2019, PROC CVPR IEEE, P10336, DOI 10.1109/CVPR.2019.01059
   Poulenard A, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275102
   Qi CR, 2017, ADV NEUR IN, V30
   Rodrigues RSV, 2018, COMPUT GRAPH FORUM, V37, P235, DOI 10.1111/cgf.13323
   Saining Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P574, DOI 10.1007/978-3-030-58580-8_34
   Shamir A, 2008, COMPUT GRAPH FORUM, V27, P1539, DOI 10.1111/j.1467-8659.2007.01103.x
   Sharma G, 2019, INT CONF 3D VISION, P67, DOI 10.1109/3DV.2019.00017
   Sohn Kihyuk, 2020, Advances in Neural Information Processing Systems, P596, DOI DOI 10.48550/ARXIV.2001.07685
   Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28
   Sun CY, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356529
   Tarvainen A, 2017, ADV NEUR IN, V30
   Thabet A, 2020, IEEE COMPUT SOC CONF, P4048, DOI 10.1109/CVPRW50498.2020.00477
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Tulsiani S, 2017, PROC CVPR IEEE, P1466, DOI 10.1109/CVPR.2017.160
   van Engelen JE, 2020, MACH LEARN, V109, P373, DOI 10.1007/s10994-019-05855-6
   Wang KP, 2021, LECT NOTES COMPUT SC, V12902, P450, DOI 10.1007/978-3-030-87196-3_42
   Wang LJ, 2020, PROC CVPR IEEE, P4503, DOI 10.1109/CVPR42600.2020.00456
   Wang PS, 2020, IEEE COMPUT SOC CONF, P1074, DOI 10.1109/CVPRW50498.2020.00141
   Wang PS, 2021, AAAI CONF ARTIF INTE, V35, P2773
   Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608
   Wang XG, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275009
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu RD, 2020, PROC CVPR IEEE, P826, DOI 10.1109/CVPR42600.2020.00091
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Xie XH, 2013, COMPUT GRAPH FORUM, V32, P233, DOI 10.1111/cgf.12200
   Xie ZG, 2015, COMPUT GRAPH FORUM, V34, P1, DOI 10.1111/cgf.12740
   Xu K, 2017, COMPUT GRAPH FORUM, V36, P101, DOI 10.1111/cgf.12790
   Xu K, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185553
   Xun Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13703, DOI 10.1109/CVPR42600.2020.01372
   Yi L, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073652
   Yunlu Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P330, DOI 10.1007/978-3-030-58580-8_20
   Yuqi Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13575, DOI 10.1109/CVPR42600.2020.01359
   Zhang JZ, 2020, PROC CVPR IEEE, P4533, DOI 10.1109/CVPR42600.2020.00459
   Zhanghan Ke, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P429, DOI 10.1007/978-3-030-58601-0_26
   Zhao YH, 2019, PROC CVPR IEEE, P1009, DOI 10.1109/CVPR.2019.00110
   Zheng YY, 2013, COMPUT GRAPH FORUM, V32, P195, DOI 10.1111/cgf.12039
   Zhu CY, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275008
NR 77
TC 3
Z9 4
U1 12
U2 47
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2023
VL 9
IS 2
BP 229
EP 247
DI 10.1007/s41095-022-0281-9
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7N8FL
UT WOS:000907570700002
OA Green Submitted, gold
DA 2024-07-18
ER

PT J
AU Yang, GY
   Zhou, WY
   Cai, Y
   Zhang, SH
   Zhang, FL
AF Yang, Guo-Ye
   Zhou, Wen-Yang
   Cai, Yun
   Zhang, Song-Hai
   Zhang, Fang-Lue
TI Focusing on your subject: Deep subject-aware image composition
   recommendation networks
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE subject-aware image composition; image cropping; deep learning;
   recommendation
ID AESTHETICS; SYSTEM
AB Photo composition is one of the most important factors in the aesthetics of photographs. As a popular application, composition recommendation for a photo focusing on a specific subject has been ignored by recent deep-learning-based composition recommendation approaches. In this paper, we propose a subject-aware image composition recommendation method, SAC-Net, which takes an RGB image and a binary subject window mask as input, and returns good compositions as crops containing the subject. Our model first determines candidate scores for all possible coarse cropping windows. The crops with high candidate scores are selected and further refined by regressing their corner points to generate the output recommended cropping windows. The final scores of the refined crops are predicted by a final score regression module. Unlike existing methods that need to preset several cropping windows, our network is able to automatically regress cropping windows with arbitrary aspect ratios and sizes. We propose novel stability losses for maximizing smoothness when changing cropping windows along with view changes. Experimental results show that our method outperforms state-of-the-art methods not only on the subject-aware image composition recommendation task, but also for general purpose composition recommendation. We also have designed a multistage labeling scheme so that a large amount of ranked pairs can be produced economically. We use this scheme to propose the first subject-aware composition dataset SACD, which contains 2777 images, and more than 5 million composition ranked pairs. The SACD dataset is publicly available at https://cg.cs.tsinghua.edu.cn/SACD/.
C1 [Yang, Guo-Ye; Zhou, Wen-Yang; Cai, Yun; Zhang, Song-Hai] Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beiing 100084, Peoples R China.
   [Zhang, Fang-Lue] Victoria Univ Wellington, Sch Engn & Comp Sci, Wellington 6012, New Zealand.
C3 Tsinghua University; Victoria University Wellington
RP Zhang, SH (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beiing 100084, Peoples R China.
EM yanggy19@mails.tsinghua.edu.cn; zhouwy19@mails.tsinghua.edu.cn;
   cynthiacai1107@gmail.com; shz@tsinghua.edu.cn; fanglue.zhang@vuw.ac.nz
FU National Natural Science Foundation of China [61521002, 62132012];
   Marsden Fund Council [MFP-20-VUW-180]
FX This work was supported by the National Natural Science Foundation of
   China (61521002, 62132012) and the Marsden Fund Council managed by the
   Royal Society of New Zealand (MFP-20-VUW-180).
CR [Anonymous], 2007, P 5 INT C COMP VIS S
   [Anonymous], 2010, ACM MULTIMEDIA
   [Anonymous], 2017, arXiv
   Chang HW, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925908
   Chang YY, 2009, IEEE I CONF COMP VIS, P2225, DOI 10.1109/ICCV.2009.5459470
   Chen LQ, 2003, MULTIMEDIA SYST, V9, P353, DOI 10.1007/s00530-003-0105-4
   Chen YL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P37, DOI 10.1145/3123266.3123274
   Chen YL, 2017, IEEE WINT CONF APPL, P226, DOI 10.1109/WACV.2017.32
   Christensen CL, 2021, IEEE ACCESS, V9, P107600, DOI 10.1109/ACCESS.2021.3100816
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   DeGroot M., 2018, PYTORCH
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Esmaeili SA, 2017, PROC CVPR IEEE, P4178, DOI 10.1109/CVPR.2017.445
   Everingham M., 2007, The PASCAL Visual Object Classes Chal- lenge (VOC) Results, DOI 10.1007/s11263-009-0275-4
   Fang C, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1105, DOI 10.1145/2647868.2654979
   Goodale Mark, 2007, PHOTOGRAPHERS EYE CO, P1
   Hong CY, 2021, PROC CVPR IEEE, P7053, DOI 10.1109/CVPR46437.2021.00698
   Hosu V, 2019, PROC CVPR IEEE, P9367, DOI 10.1109/CVPR.2019.00960
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40
   Li DB, 2018, PROC CVPR IEEE, P8193, DOI 10.1109/CVPR.2018.00855
   Li XW, 2019, J ELECTRON IMAGING, V28, DOI 10.1117/1.JEI.28.2.023008
   Liang Y, 2018, IEEE T VIS COMPUT GR, V24, P2728, DOI 10.1109/TVCG.2017.2764895
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lu P, 2019, Arxiv, DOI arXiv:1907.01432
   Lu P, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P120, DOI 10.1145/3394171.3413824
   Lu P, 2019, SIGNAL PROCESS-IMAGE, V77, P1, DOI 10.1016/j.image.2019.05.010
   Lu WR, 2019, IEEE ACCESS, V7, P91904, DOI 10.1109/ACCESS.2019.2925430
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Luo JB, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P2218
   Luo W, 2011, IEEE I CONF COMP VIS, P2206, DOI 10.1109/ICCV.2011.6126498
   MacQueen J., 1967, P 5 BERK S MATH STAT, P281
   Mai L, 2016, PROC CVPR IEEE, P497, DOI 10.1109/CVPR.2016.60
   Marchesotti L, 2009, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2009.5459467
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Nishiyama M., 2009, P 17 ACM INT C MULTI, P669
   Rawat YS, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P217, DOI 10.1145/2647868.2656409
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Santella A., 2006, Conference on Human Factors in Computing Systems. CHI2006, P771
   Sheng KK, 2021, COMPUT VIS MEDIA, V7, P139, DOI 10.1007/s41095-020-0193-5
   Shiming Ge, 2018, Computational Visual Media, V4, P71, DOI 10.1007/s41095-017-0102-8
   Su HH, 2012, IEEE T MULTIMEDIA, V14, P833, DOI 10.1109/TMM.2012.2186123
   Suh Bongwon, 2003, P ACM S US INT SOFTW, P95, DOI DOI 10.1145/964696.964707
   Tu Y, 2020, AAAI CONF ARTIF INTE, V34, P12104
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Wang WG, 2017, IEEE I CONF COMP VIS, P2205, DOI 10.1109/ICCV.2017.240
   Wang WG, 2017, IEEE T VIS COMPUT GR, V23, P2014, DOI 10.1109/TVCG.2016.2600594
   Wei ZJ, 2018, PROC CVPR IEEE, P5437, DOI 10.1109/CVPR.2018.00570
   Xu PF, 2019, COMPUT VIS MEDIA, V5, P45, DOI 10.1007/s41095-019-0130-7
   Yan JZ, 2015, INT J COMPUT VISION, V114, P74, DOI 10.1007/s11263-015-0801-5
   Yan JZ, 2013, PROC CVPR IEEE, P971, DOI 10.1109/CVPR.2013.130
   Zeng H, 2019, PROC CVPR IEEE, P5942, DOI 10.1109/CVPR.2019.00610
   Zhang FL, 2013, IEEE T MULTIMEDIA, V15, P1480, DOI 10.1109/TMM.2013.2268051
   Zhang LM, 2013, IEEE T IMAGE PROCESS, V22, P802, DOI 10.1109/TIP.2012.2223226
   Zhang SH, 2020, COMPUT VIS MEDIA, V6, P79, DOI 10.1007/s41095-020-0158-8
NR 57
TC 0
Z9 0
U1 0
U2 12
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2023
VL 9
IS 1
BP 87
EP 107
DI 10.1007/s41095-021-0263-3
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5K7GQ
UT WOS:000869891100006
OA gold, Green Submitted
DA 2024-07-18
ER

PT J
AU Liang, JB
   Lin, MC
AF Liang, Junbang
   Lin, Ming C.
TI Machine learning for digital try-on: Challenges and progress
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review
DE machine learning; digital try-on; garment modeling; human body
   estimation; material modeling
AB Digital try-on systems for e-commerce have the potential to change people's lives and provide notable economic benefits. However, their development is limited by practical constraints, such as accurate sizing of the body and realism of demonstrations. We enumerate three open challenges remaining for a complete and easy-to-use try-on system that recent advances in machine learning make increasingly tractable. For each, we describe the problem, introduce state-of-the-art approaches, and provide future directions.
C1 [Liang, Junbang; Lin, Ming C.] Univ Maryland, College Pk, MD 20785 USA.
C3 University System of Maryland; University of Maryland College Park
RP Liang, JB (corresponding author), Univ Maryland, College Pk, MD 20785 USA.
EM liangjb@cs.umd.edu; lin@cs.umd.edu
FU National Science Foundation
FX This research was supported in part by the Iribe Professorship and the
   National Science Foundation.
CR ACM, 2018, T GRAPHICS, V37, P6
   ACM, 2018, T GRAPHICS, V37, P5
   ACM, 2015, T GRAPHICS, V34, P6
   ACM, 2017, T GRAPHICS, V36, P4
   Alldieck T, 2019, PROC CVPR IEEE, P1175, DOI 10.1109/CVPR.2019.00127
   Alldieck T, 2018, PROC CVPR IEEE, P8387, DOI 10.1109/CVPR.2018.00875
   Belbute-Peres FD, 2018, ADV NEUR IN, V31
   Blan A O, 2008, LECT NOTES COMPUTER, P5303
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Degrave J, 2019, FRONT NEUROROBOTICS, V13, DOI 10.3389/fnbot.2019.00006
   Dibra E, 2016, INT CONF 3D VISION, P108, DOI 10.1109/3DV.2016.19
   Hu Y M, 2019, PREPRINT
   Hu YM, 2019, IEEE INT CONF ROBOT, P6265, DOI [10.1109/icra.2019.8794333, 10.1109/ICRA.2019.8794333]
   Huang P, 2016, I C VIRTUAL REALITY, P250, DOI 10.1109/ICVRV.2016.48
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234
   Lassner C, 2017, PROC CVPR IEEE, P4704, DOI 10.1109/CVPR.2017.500
   Liang J, 2019, P 33 C NEUR INF PROC
   Liang JB, 2019, IEEE I CONF COMP VIS, P4351, DOI 10.1109/ICCV.2019.00445
   Liu KX, 2018, COMPUT AIDED DESIGN, V104, P113, DOI 10.1016/j.cad.2018.07.003
   Qiao Y L, 2020, SCALABLE DIFFERENTIA
   Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239
   Smith D, 2019, IEEE I CONF COMP VIS, P5329, DOI 10.1109/ICCV.2019.00543
   Varol G, 2018, LECT NOTES COMPUT SC, V11211, P20, DOI 10.1007/978-3-030-01234-2_2
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Xu YL, 2019, IEEE I CONF COMP VIS, P7759, DOI 10.1109/ICCV.2019.00785
   Yang S, 2017, IEEE I CONF COMP VIS, P4393, DOI 10.1109/ICCV.2017.470
   Zheng Z., 2017, ADV SCI, V4, P3, DOI DOI 10.1007/S41095-017-0084-6
   Zheng ZR, 2019, IEEE I CONF COMP VIS, P7738, DOI 10.1109/ICCV.2019.00783
NR 29
TC 6
Z9 8
U1 2
U2 21
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2021
VL 7
IS 2
BP 159
EP 167
DI 10.1007/s41095-020-0189-1
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FR
UT WOS:000648692900001
OA gold
DA 2024-07-18
ER

PT J
AU Ritz, M
   Breitfelder, S
   Santos, P
   Kuijper, A
   Fellner, DW
AF Ritz, Martin
   Breitfelder, Simon
   Santos, Pedro
   Kuijper, Arjan
   Fellner, Dieter W.
TI Seamless and non-repetitive 4D texture variation synthesis and real-time
   rendering for measured optical material behavior
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE optical material behavior; reflectance modeling; 4D texture synthesis;
   texturing
AB We show how to overcome the single weakness of an existing fully automatic system for acquisition of spatially varying optical material behavior of real object surfaces. While the expression of spatially varying material behavior with spherical dependence on incoming light as a 4D texture (an ABTF material model) allows flexible mapping onto arbitrary 3D geometry, with photo-realistic rendering and interaction in real time, this very method of texture-like representation exposes it to common problems of texturing, striking in two disadvantages. Firstly, non-seamless textures create visible artifacts at boundaries. Secondly, even a perfectly seamless texture causes repetition artifacts due to their organised placement in large numbers over a 3D surface. We have solved both problems through our novel texture synthesis method that generates a set of seamless texture variations randomly distributed over the surface at shading time. When compared to regular 2D textures, the inter-dimensional coherence of the 4D ABTF material model poses entirely new challenges to texture synthesis, which includes maintaining the consistency of material behavior throughout the 4D space spanned by the spatial image domain and the angular illumination hemisphere. In addition, we tackle the increased memory consumption caused by the numerous variations through a fitting scheme specifically designed to reconstruct the most prominent effects captured in the material model.
C1 [Ritz, Martin; Santos, Pedro; Kuijper, Arjan; Fellner, Dieter W.] Fraunhofer IGD, D-64283 Darmstadt, Germany.
   [Breitfelder, Simon; Kuijper, Arjan; Fellner, Dieter W.] Tech Univ Darmstadt, D-64289 Darmstadt, Germany.
   [Fellner, Dieter W.] Graz Univ Technol, A-8010 Graz, Austria.
C3 Technical University of Darmstadt; Graz University of Technology
RP Ritz, M (corresponding author), Fraunhofer IGD, D-64283 Darmstadt, Germany.
EM martin.ritz@igd.fraunhofer.de; simon.breitfelder@gmail.com;
   pedro.santos@igd.fraunhofer.de; arjan.kuijper@igd.fraunhofer.de;
   dieter.fellner@igd.fraunhofer.de
OI Santos, Pedro/0000-0003-1813-1714; Fellner, Dieter
   W./0000-0001-7756-0901
FU European project MAXIMUS [FP7-ICT-2007-1-217039]; German Federal
   Ministry for Economic Affairs and Energy project CultLab3D [01MT12022E]
FX This work was partially supported by the European project MAXIMUS (No.
   FP7-ICT-2007-1-217039) and the German Federal Ministry for Economic
   Affairs and Energy project CultLab3D (No. 01MT12022E).
CR Cohen MF, 2003, ACM T GRAPHIC, V22, P287, DOI 10.1145/882262.882265
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Kautz J, 2004, PROC GRAPH INTERF, P177
   Lagae A, 2006, ACM T GRAPHIC, V25, P1442, DOI 10.1145/1183287.1183296
   Liu XG, 2001, COMP GRAPH, P97
   Ng TY, 2005, COMPUTER GRAPHICS INTERNATIONAL 2005, PROCEEDINGS, P177
   Nozick V, 2008, 8 ANN INT C ART REAL, P242
   Ritz M, 2018, P ACM SIGGRAPH 2018
   Schwartz C, 2013, P WORKSH MAT APP MOD
   Weinmann M, 2015, P SIGGRAPHASIA 2015
NR 10
TC 0
Z9 0
U1 0
U2 0
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2019
VL 5
IS 2
BP 161
EP 170
DI 10.1007/s41095-019-0141-4
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UM
UT WOS:000648690800003
OA gold, Green Submitted
DA 2024-07-18
ER

PT J
AU Roberts, R
   Lewis, JP
   Anjyo, K
   Seo, J
   Seol, Y
AF Roberts, Richard
   Lewis, J. P.
   Anjyo, Ken
   Seo, Jaewoo
   Seol, Yeongho
TI Optimal and interactive keyframe selection for motion capture
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE motion capture; motion editing; keyframe animation; dynamic programming
ID EXTRACTION; COMPRESSION; ANIMATION
AB Motion capture is increasingly used in games and movies, but often requires editing before it can be used, for many reasons. The motion may need to be adjusted to correctly interact with virtual objects or to fix problems that result from mapping the motion to a character of a different size or, beyond such technical requirements, directors can request stylistic changes. Unfortunately, editing is laborious because of the low-level representation of the data. While existing motion editing methods accomplish modest changes, larger edits can require the artist to "re-animate" the motion by manually selecting a subset of the frames as keyframes. In this paper, we automatically find sets of frames to serve as keyframes for editing the motion. We formulate the problem of selecting an optimal set of keyframes as a shortest-path problem, and solve it efficiently using dynamic programming. We create a new simplified animation by interpolating the found keyframes using a naive curve fitting technique. Our algorithm can simplify motion capture to around 10% of the original number of frames while retaining most of its detail. By simplifying animation with our algorithm, we realize a new approach to motion editing and stylization founded on the time-tested keyframe interface. We present results that show our algorithm outperforms both research algorithms and a leading commercial tool.
C1 [Roberts, Richard; Anjyo, Ken] Victoria Univ Wellington, Wellington, New Zealand.
   [Lewis, J. P.] Elect Arts, SEED, Los Angeles, CA USA.
   [Anjyo, Ken] OLM Digital, Tokyo, Japan.
   [Seo, Jaewoo] Pinscreen, Los Angeles, CA USA.
   [Seol, Yeongho] Weta Digital, Wellington, New Zealand.
C3 Victoria University Wellington
RP Roberts, R (corresponding author), Victoria Univ Wellington, Wellington, New Zealand.
EM richard.andrew.roberts@gmail.com; noisebrain@gmail.com; anjyo@acm.org;
   goongsang@gmail.com; seolyeongho@gmail.com
RI Lewis, J P/GYI-9327-2022; Seol, Yeongho/KIE-6801-2024
OI Lewis, J.P./0000-0002-6835-7263; Roberts, Richard/0000-0002-3462-8539
CR [Anonymous], 2018, ACM T GRAPHICS, V37, P4
   [Anonymous], 2002, ACM T GRAPHIC, DOI DOI 10.1145/566654.566605
   [Anonymous], 2011, ACM T GRAPHICS, V30, P6
   [Anonymous], 2015, TERMINUS, V34, P2
   [Anonymous], 1990, ALGORITHM AUTOMATICA
   [Anonymous], 1998, Network Optimization: Continuous and Discrete Models
   [Anonymous], 2016, ACM T GRAPHICS, V35, P4
   [Anonymous], 2013, ACM T GRAPHICS, V32, P6
   Arikan O, 2006, ACM T GRAPHIC, V25, P890, DOI 10.1145/1141911.1141971
   Assa J, 2005, ACM T GRAPHIC, V24, P667, DOI 10.1145/1073204.1073246
   BELLMAN R, 1972, J MATH ANAL APPL, V38, P471, DOI 10.1016/0022-247X(72)90105-9
   Bulut E., KEYFRAME EXTRACTION
   Casas D., 2012, P ACM SIGGRAPH S INT, P103, DOI DOI 10.1145/2159616.2159633
   Chang XJ, 2016, STUD COMPUT INTELL, V642, P335, DOI 10.1007/978-3-319-31277-4_29
   Cuntoor NP, 2006, LECT NOTES COMPUT SC, V3852, P499
   Dijkstra EW., 1959, NUMER MATH, V1, P269, DOI [10.1007/BF01386390, DOI 10.1007/BF01386390]
   Douglas D.H., 1973, Cartographica: The International Journal for Geographic Information and Geovisualization, V10, P112, DOI [https://doi.org/10.3138/FM57-6770-U75U-7727, DOI 10.1002/9780470669488.CH2]
   Gleicher M, 1999, COMPUT GRAPHICS-US, V33, P51, DOI 10.1145/345370.345409
   Halit C, 2011, COMPUT ANIMAT VIRT W, V22, P3, DOI 10.1002/cav.380
   Hu YQ, 2010, IEEE PAC VIS SYMP, P153, DOI 10.1109/PACIFICVIS.2010.5429596
   Huang KS, 2005, VISUAL COMPUT, V21, P532, DOI 10.1007/s00371-005-0316-0
   Kaufman James C., 2013, The social science of cinema
   Lam D., 2017, COMMUNICATION
   Lasseter J., 1987, Proc. SIGGRAPH 87, V21, P35, DOI DOI 10.1145/37402.37407
   Lee J, 1999, COMP GRAPH, P39
   Lewis J. P., 2009, P ACM SIGGRAPH ASIA
   Lim IS, 2001, P ANN INT IEEE EMBS, V23, P1167
   Liu R., 2012, J CONVERG INF TECHNO, V7, P11
   Liu XM, 2013, VISUAL COMPUT, V29, P85, DOI 10.1007/s00371-012-0676-1
   LOWE DG, 1987, ARTIF INTELL, V31, P355, DOI 10.1016/0004-3702(87)90070-1
   Marr D., 1982, Visual perception
   Miura T, 2014, IEEJ T ELECTR ELECTR, V9, P697, DOI 10.1002/tee.22029
   Miura Takeshi., 2014, Journal of Information Processing, V22, P67, DOI DOI 10.2197/ipsjjip.22.67
   Park MJ, 2004, COMPUT ANIMAT VIRT W, V15, P245, DOI 10.1002/cav.27
   Ramer U., 1972, Comput. Graph. Image Process., V1, P244, DOI DOI 10.1016/S0146-664X(72)80017-0
   Roy K., 2013, CHEAT MAYA 2014 TOOL, DOI [10.4324/9780203527306, DOI 10.4324/9780203527306]
   Shelton D., 2017, COMMUNICATION
   So CKF, 2005, COMPUT ANIMAT VIRT W, V16, P225, DOI 10.1002/cav.107
   Tournier M, 2009, COMPUT GRAPH FORUM, V28, P355, DOI 10.1111/j.1467-8659.2009.01375.x
   Wang X, 2014, COMPUT MATH METHOD M, V2014, DOI 10.1155/2014/104535
   White Tony., 2006, Animation from Pencils to Pixels: Classical Techniques for Digital Animators
   Witkin A., 1988, Computer Graphics, V22, P159, DOI 10.1145/378456.378507
   Witkin A., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P105, DOI 10.1145/218380.218422
   Wolin A., 2008, P EUR WORKSH SKETCH, P33, DOI DOI 10.2312/SBM/SBM08/033-040
   Xia GY, 2017, IEEE T IND ELECTRON, V64, P1589, DOI 10.1109/TIE.2016.2610946
   Yasuda H, 2008, IEICE T INF SYST, VE91D, P1159, DOI 10.1093/ietisy/e91-d.4.1159
   Zhang Q, 2014, SYMMETRY-BASEL, V6, P926, DOI 10.3390/sym6040926
   Zhang Y, 2015, INT CONF INSTR MEAS, P1617, DOI 10.1109/IMCCC.2015.343
NR 48
TC 7
Z9 8
U1 0
U2 6
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2019
VL 5
IS 2
BP 171
EP 191
DI 10.1007/s41095-019-0138-z
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UM
UT WOS:000648690800004
OA gold
DA 2024-07-18
ER

PT J
AU Song, Y
   Tang, F
   Dong, WM
   Xu, CS
AF Song, Yu
   Tang, Fan
   Dong, Weiming
   Xu, Changsheng
TI Non-dominated sorting based multi-page photo collage
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE multi-page photo collage; balance-aware measurements; non-dominated
   sorting; genetic algorithm
AB The development of social networking services (SNSs) revealed a surge in image sharing. The sharing mode of multi-page photo collage (MPC), which posts several image collages at a time, can often be observed on many social network platforms, which enables uploading images and arrangement in a logical order. This study focuses on the construction of MPC for an image collection and its formulation as an issue of joint optimization, which involves not only the arrangement in a single collage but also the arrangement among different collages. Novel balance-aware measurements, which merge graphic features and psychological achievements, are introduced. Non-dominated sorting genetic algorithm is adopted to optimize the MPC guided by the measurements. Experiments demonstrate that the proposed method can lead to diverse, visually pleasant, and logically clear MPC results, which are comparable to manually designed MPC results.
C1 [Song, Yu; Dong, Weiming; Xu, Changsheng] Chinese Acad Sci, Inst Automat, NLPR, Beijing 100040, Peoples R China.
   [Song, Yu; Dong, Weiming; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100190, Peoples R China.
   [Tang, Fan] Jilin Univ, Sch Artificial Intelligence, Changchun 130012, Peoples R China.
   [Song, Yu; Dong, Weiming; Xu, Changsheng] CASIA LLvis Joint Lab, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Jilin University
RP Dong, WM (corresponding author), Chinese Acad Sci, Inst Automat, NLPR, Beijing 100040, Peoples R China.; Dong, WM (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100190, Peoples R China.; Dong, WM (corresponding author), CASIA LLvis Joint Lab, Beijing 100190, Peoples R China.
EM weiming.dong@ia.ac.cn
RI Tang, Fan/O-3923-2018; xu, cj/HJZ-3488-2023; DONG, Weiming/AAG-7678-2020
OI Tang, Fan/0000-0002-3975-2483; DONG, Weiming/0000-0001-6502-145X
FU National Key R&D Program of China [2020AAA0106200]; National Natural
   Science Foundation of China [61832016, U20B2070]
FX We thank the anonymous reviewers for their valuable comments. We thank
   Dr. Xiaowen Huang for providing test images. This work was supported by
   National Key R&D Program of China under No. 2020AAA0106200, and by
   National Natural Science Foundation of China under Nos. 61832016 and
   U20B2070.
CR Cao Y, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601183
   Cao Y, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366160
   Chankyu Lee, 2021, Proceedings of the 18th International Conference on Computing in Civil and Building Engineering. ICCCBE 2020. Lecture Notes in Civil Engineering (LNCE 98), P1252, DOI 10.1007/978-3-030-51295-8_88
   Chen T, 2012, COMPUT GRAPH-UK, V36, P241, DOI 10.1016/j.cag.2012.02.010
   Chen ZB, 2017, IEEE T IMAGE PROCESS, V26, P5138, DOI 10.1109/TIP.2017.2736422
   Coello C. A. C., 2007, EVOLUTIONARY ALGORIT
   Deb K, 2002, IEEE T EVOLUT COMPUT, V6, P182, DOI 10.1109/4235.996017
   Deb K., 2001, WIL INT S SYS OPT
   Gan Y, 2020, COMPUT GRAPH-UK, V88, P35, DOI 10.1016/j.cag.2020.02.006
   Han XT, 2016, IEEE T CYBERNETICS, V46, P1286, DOI 10.1109/TCYB.2015.2448236
   Hübner R, 2019, I-PERCEPTION, V10, DOI 10.1177/2041669519856040
   Hübner R, 2016, FRONT PSYCHOL, V7, DOI 10.3389/fpsyg.2016.00335
   Jing GM, 2015, IEEE T MULTIMEDIA, V17, P2122, DOI 10.1109/TMM.2015.2474263
   Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40
   Li D, 2020, ACM T MODEL COMPUT S, V30, DOI 10.1145/3391407
   Liang Y, 2018, IEEE T VIS COMPUT GR, V24, P2728, DOI 10.1109/TVCG.2017.2764895
   Liang Y, 2017, IEEE T VIS COMPUT GR, V23, P1099, DOI 10.1109/TVCG.2016.2517641
   Liu LJ, 2018, IEEE T VIS COMPUT GR, V24, P1956, DOI 10.1109/TVCG.2017.2703853
   Ma L, 2016, IEEE T MULTIMEDIA, V18, P2228, DOI 10.1109/TMM.2016.2614187
   Pan XJ, 2021, IEEE T VIS COMPUT GR, V27, P2298, DOI 10.1109/TVCG.2019.2948611
   Rejeesh MR, 2020, MULTIMED TOOLS APPL, V79, P28411, DOI 10.1007/s11042-020-09234-5
   Ren Y. R., 2020, P 28 ACM INT C MULT, P798
   Sheng KK, 2021, COMPUT VIS MEDIA, V7, P139, DOI 10.1007/s41095-020-0193-5
   Sheng KK, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P879, DOI 10.1145/3240508.3240554
   Simonyan K, 2015, IEEE INT C ICLR
   Song Y., 2019, P SIGGRAPH AS 2019
   Su J., 2020, NeurIPS
   Tan L, 2012, IEEE COMPUT GRAPH, V32, P46, DOI 10.1109/MCG.2011.89
   Thömmes K, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.01050
   Wagemans J, 2012, PSYCHOL BULL, V138, P1172, DOI 10.1037/a0029333
   Wilson A., 2005, EMPIR STUD ARTS, V23, P165, DOI [DOI 10.2190/B1LR-MVF3-F36X-XR64, 10.2190/B1LR-MVF3-F36X-XR64]
   Wu ZP, 2016, MULTIMED TOOLS APPL, V75, P1813, DOI 10.1007/s11042-014-2375-6
   Yu ZQ, 2014, IEEE T VIS COMPUT GR, V20, P182, DOI 10.1109/TVCG.2013.106
   Zhang YB, 2018, IEEE T IMAGE PROCESS, V27, P451, DOI 10.1109/TIP.2017.2761556
   Zheng XR, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322971
NR 35
TC 2
Z9 2
U1 0
U2 18
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2022
VL 8
IS 2
BP 199
EP 212
DI 10.1007/s41095-021-0221-0
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XJ0YS
UT WOS:000726525400002
OA gold
DA 2024-07-18
ER

PT J
AU He, Y
   Zhao, GD
   Zhang, SH
AF He, Yu
   Zhao, Guo-Dong
   Zhang, Song-Hai
TI Smoothness preserving layout for dynamic labels by hybrid optimization
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE label layout; smoothness preserving; dynamic label; forced based; static
   optimization; hybrid optimization
ID PLACEMENT
AB Stable label movement and smooth label trajectory are critical for effective information understanding. Sudden label changes cannot be avoided by whatever forced directed methods due to the unreliability of resultant force or global optimization methods due to the complex trade-off on the different aspects. To solve this problem, we proposed a hybrid optimization method by taking advantages of the merits of both approaches. We first detect the spatial-temporal intersection regions from whole trajectories of the features, and initialize the layout by optimization in decreasing order by the number of the involved features. The label movements between the spatial-temporal intersection regions are determined by force directed methods. lb cope with some features with high speed relative to neighbors, we introduced a force from future, called temporal force, so that the labels of related features can elude ahead of time and retain smooth movements. We also proposed a strategy by optimizing the label layout to predict the trajectories of features so that such global optimization method can be applied to streaming data.
C1 [He, Yu; Zhang, Song-Hai] Tsinghua Univ, Dept Comp Sci & Technol, Beijing, Peoples R China.
   [Zhao, Guo-Dong] Tianjin Univ Technol, Dept Comp Sci & Technol, Tianjin, Peoples R China.
C3 Tsinghua University; Tianjin University of Technology
RP Zhang, SH (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing, Peoples R China.
EM hooyeeevan@tsinghua.edu.cn; z159426124115@163.com; shz@tsinghua.edu.cn
RI Yu, He/B-5774-2017
OI Yu, He/0000-0002-0357-681X
FU National Key Technology RD Program [2017YFB1002604]; National Natural
   Science Foundation of China [61772298, 61832016]; Research Grant of
   Beijing Higher Institution Engineering Research Center; Tsinghua-Tencent
   Joint Laboratory for Internet Innovation Technology
FX The authors would like to thank all reviewers for their thoughtful
   comments. This work was supported by the National Key Technology R&D
   Program (Project No. 2017YFB1002604), the National Natural Science
   Foundation of China (Project Nos. 61772298 and 61832016), Research Grant
   of Beijing Higher Institution Engineering Research Center, and
   Tsinghua-Tencent Joint Laboratory for Internet Innovation Technology.
CR [Anonymous], 1996, MAP LABELING BIBLIO
   Bell B., 2001, 01UIST. Proceedings of the 14th Annual ACM Symposium on User Interface Software and Technology, P101, DOI 10.1145/502348.502363
   Christensen J, 1995, ACM T GRAPHIC, V14, P203, DOI 10.1145/212332.212334
   Ekman S., 2018, THESIS CHALMERS U TE
   FREEMAN H, 1988, INFORM SCIENCES, V45, P367, DOI 10.1016/0020-0255(88)90011-4
   HIRSCH SA, 1982, AM CARTOGRAPHER, V9, P5, DOI 10.1559/152304082783948367
   Imhof E., 1975, AM CARTOGRAPHER, V2, P128, DOI [DOI 10.1559/152304075784313304, 10.1559/152304075784313304]
   Krogmann S., 2017, AUTOMATIC 2D LABEL P
   Lhuillier A, 2019, VISUAL COMPUT, V35, P1041, DOI 10.1007/s00371-019-01686-7
   Meng Y, 2015, IEEE PAC VIS SYMP, P207, DOI 10.1109/PACIFICVIS.2015.7156379
   Pick S., 2010, Proceedings of the 16th Eurographics conference on Virtual Environments Second Joint Virtual Reality, P1
   Rosten E, 2005, LECT NOTES COMPUT SC, V3804, P294
   Sirk C., 2017, P EG VGTC C VIS, P139
   Stadler G., 2006, CARTOGR GEOGR INF SC, V33, P207
   Tatzgern M, 2014, 2014 IEEE VIRTUAL REALITY (VR), P27, DOI 10.1109/VR.2014.6802046
   Vaaraniemi M., 2012, P 3 INT C COMP GEOSP
   Vaaraniemi M., 2012, P 3 INT C COMP GEOSP
   Van Garderen M., 2018, THESIS U KONSTANZ
   Wagner F, 1998, LECT NOTES COMPUT SC, V1547, P316
   Yuzhen Lu, 2012, 2012 Third International Conference on Intelligent Control and Information Processing (ICICIP 2012), P513, DOI 10.1109/ICICIP.2012.6391473
   Zoraster S., 1986, Cartographica: The International Journal for Geographic Information and Geovisualization, V23, P16, DOI [https://doi.org/10.3138/9258-63QL-3988-110H, DOI 10.3138/9258-63QL-3988-110H]
NR 21
TC 0
Z9 0
U1 2
U2 10
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2022
VL 8
IS 1
BP 149
EP 163
DI 10.1007/s41095-021-0231-y
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WO6VR
UT WOS:000712590200010
PM 34721936
OA Green Published, gold
DA 2024-07-18
ER

PT J
AU Liu, XT
   Wu, WL
   Li, CZ
   Li, YF
   Wu, HS
AF Liu, Xueting
   Wu, Wenliang
   Li, Chengze
   Li, Yifan
   Wu, Huisi
TI Reference-guided structure-aware deep sketch colorization for cartoons
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE sketch colorization; image style editing; deep feature understanding;
   reference-based image colorization
ID IMAGE; COLOR
AB Digital cartoon production requires extensive manual labor to colorize sketches with visually pleasant color composition and color shading. During colorization, the artist usually takes an existing cartoon image as color guidance, particularly when colorizing related characters or an animation sequence. Reference-guided colorization is more intuitive than colorization with other hints, such as color points or scribbles, or text-based hints. Unfortunately, reference-guided colorization is challenging since the style of the colorized image should match the style of the reference image in terms of both global color composition and local color shading. In this paper, we propose a novel learning-based framework which colorizes a sketch based on a color style feature extracted from a reference color image. Our framework contains a color style extractor to extract the color feature from a color image, a colorization network to generate multi-scale output images by combining a sketch and a color feature, and a multi-scale discriminator to improve the reality of the output image. Extensive qualitative and quantitative evaluations show that our method outperforms existing methods, providing both superior visual quality and style reference consistency in the task of reference-based colorization.
C1 [Liu, Xueting; Li, Chengze] Caritas Inst Higher Educ, Sch Comp & Informat Sci, Hong Kong, Peoples R China.
   [Wu, Wenliang; Li, Yifan; Wu, Huisi] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
C3 Saint Francis University Hong Kong; Shenzhen University
RP Li, CZ (corresponding author), Caritas Inst Higher Educ, Sch Comp & Informat Sci, Hong Kong, Peoples R China.
EM tliu@cihe.edu.hk; wuwenliang2019@email.szu.edu.cn; czli@cihe.edu.hk;
   1810272030@email.szu.edu.cn; hswu@zszu.edu.cn
RI Li, Chengze/AAU-7168-2021; Liu, Xueting/AAG-9648-2019
OI Li, Chengze/0000-0002-1519-750X; Liu, Xueting/0000-0002-0868-5353
FU National Natural Science Foundation of China [IDG200107]; Natural
   Science Foundation of Guangdong Province of China [61973221]; 
   [2018A030313381];  [2019A1515011165]
FX This work was supported in part by a CIHE Institutional Development
   Grant No. IDG200107, the National Natural Science Foundation of China
   under Grant No. 61973221, and the Natural Science Foundation of
   Guangdong Province of China under Grant Nos. 2018A030313381 and
   2019A1515011165.
CR Bugeau A, 2014, IEEE T IMAGE PROCESS, V23, P298, DOI 10.1109/TIP.2013.2288929
   Chia AYS, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024190
   DOWSON DC, 1982, J MULTIVARIATE ANAL, V12, P450, DOI 10.1016/0047-259X(82)90077-X
   Gao W, 2020, IEEE WINT CONF APPL, P3211, DOI 10.1109/WACV45572.2020.9093420
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gonzalezgarcia A., 2018, ADV NEURAL INFORM PR, P1287
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gupta R. K., 2012, P 20 ACM INT C MULT, P369, DOI DOI 10.1145/2393347.2393402
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He MM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201365
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim H, 2019, IEEE I CONF COMP VIS, P9055, DOI 10.1109/ICCV.2019.00915
   Kim T, 2017, PR MACH LEARN RES, V70
   Kingma D. P., 2014, arXiv
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Lee J, 2020, PROC CVPR IEEE, P5800, DOI 10.1109/CVPR42600.2020.00584
   Li X., 2015, Comput. Vis. Media, V1, P143, DOI DOI 10.1007/S41095-015-0013-5
   Li XX, 2018, LECT NOTES COMPUT SC, V11207, P93, DOI 10.1007/978-3-030-01219-9_6
   Li YJ, 2017, ADV NEUR IN, V30
   Liu XP, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409105
   Mescheder L, 2018, PR MACH LEARN RES, V80
   MIAO Y., 2015, Computational Visual Media, V1, P3
   Miyato T, 2018, INT C LEARN REPR
   Sanakoyeu A, 2018, LECT NOTES COMPUT SC, V11212, P715, DOI 10.1007/978-3-030-01237-3_43
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song CJ, 2019, ADV NEUR IN, V32
   Sun TH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P683, DOI 10.1145/3343031.3351041
   Tai YW, 2005, PROC CVPR IEEE, P747
   Todo Hideki, 2017, [Computational Visual Media, 计算可视媒体], V3, P21
   Ulyanov Dmitry, 2016, arXiv
   Wang H, 2020, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR42600.2020.00193
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Welsh T, 2002, ACM T GRAPHIC, V21, P277, DOI 10.1145/566570.566576
   Yonetsuji T., 2017, Paints chainer
   Yu Xiaoming, 2019, P ADV NEUR INF PROC, P2990
   Zhang B, 2019, PROC CVPR IEEE, P8044, DOI 10.1109/CVPR.2019.00824
   Zhang LM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275090
   Zhang LM, 2017, PROCEEDINGS 2017 4TH IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR), P506, DOI 10.1109/ACPR.2017.61
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhang YL, 2019, IEEE I CONF COMP VIS, P5942, DOI 10.1109/ICCV.2019.00604
   Zhu JY, 2017, ADV NEUR IN, V30
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 47
TC 7
Z9 7
U1 2
U2 35
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2022
VL 8
IS 1
BP 135
EP 148
DI 10.1007/s41095-021-0228-6
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WO6VR
UT WOS:000712590200009
OA gold
DA 2024-07-18
ER

PT J
AU Liu, XX
   Zhang, YF
   Bao, FX
   Shao, K
   Sun, ZY
   Zhang, CM
AF Liu, Xinxin
   Zhang, Yunfeng
   Bao, Fangxun
   Shao, Kai
   Sun, Ziyi
   Zhang, Caiming
TI Kernel-blending connection approximated by a neural network for image
   classification
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE image classification; blending neural network; function approximation;
   kernel mapping connection; generalizability
ID REPRESENTATION; SVM
AB This paper proposes a kernel-blending connection approximated by a neural network (KBNN) for image classification. A kernel mapping connection structure, guaranteed by the function approximation theorem, is devised to blend feature extraction and feature classification through neural network learning. First, a feature extractor learns features from the raw images. Next, an automatically constructed kernel mapping connection maps the feature vectors into a feature space. Finally, a linear classifier is used as an output layer of the neural network to provide classification results. Furthermore, a novel loss function involving a cross-entropy loss and a hinge loss is proposed to improve the generalizability of the neural network. Experimental results on three well-known image datasets illustrate that the proposed method has good classification accuracy and generalizability.
C1 [Liu, Xinxin; Zhang, Yunfeng; Shao, Kai; Sun, Ziyi; Zhang, Caiming] Shandong Univ Finance & Econ, Jinan 250014, Peoples R China.
   [Bao, Fangxun; Zhang, Caiming] Shandong Univ, Jinan 250100, Peoples R China.
C3 Shandong University of Finance & Economics; Shandong University
RP Zhang, YF (corresponding author), Shandong Univ Finance & Econ, Jinan 250014, Peoples R China.
EM liuxxin26@163.com; yfzhang@sdufe.edu.cn; fxbao@sdu.edu.cn;
   shaokai17862921498@126.com; 17862921505@163.com; czhang@sdu.edu.cn
RI Cheng, Lin/KFQ-3111-2024; Liu, xinxin/GYD-5918-2022
OI Liu, xinxin/0000-0003-0224-8838
FU National Natural Science Foundation of China [61972227, 61672018];
   Natural Science Foundation of Shandong Province [ZR2019MF051]; Primary
   Research and Development Plan of Shandong Province [2018GGX101013];
   Fostering Project of Dominant Discipline and Talent Team of Shandong
   Province Higher Education Institutions
FX This work was supported in part by the National Natural Science
   Foundation of China (Grant Nos. 61972227 and 61672018), the Natural
   Science Foundation of Shandong Province (Grant No. ZR2019MF051), the
   Primary Research and Development Plan of Shandong Province (Grant No.
   2018GGX101013), and the Fostering Project of Dominant Discipline and
   Talent Team of Shandong Province Higher Education Institutions.
CR [Anonymous], 2013, ARXIV13013516
   [Anonymous], 2013, P INT C MACHINE LEAR
   [Anonymous], 2013, ARXIV201313013557
   [Anonymous], 2013, ARXIV PREPRINT ARXIV
   Bagarinao E, 2010, LECT NOTES COMPUT SC, V5996, P363
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bristow H, 2013, PROC CVPR IEEE, P391, DOI 10.1109/CVPR.2013.57
   Chan TH, 2015, IEEE T IMAGE PROCESS, V24, P5017, DOI 10.1109/TIP.2015.2475625
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   Eitel A, 2015, IEEE INT C INT ROBOT, P681, DOI 10.1109/IROS.2015.7353446
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Guo Y. Q., 2017, ARXIV170604719
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hosseini-Asl E, 2016, IEEE T NEUR NET LEAR, V27, P2486, DOI 10.1109/TNNLS.2015.2479223
   HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H., 2009, P 26 ANN INT C MACH, P609
   Lin M., 2013, ARXIV13124400
   Niu XX, 2012, PATTERN RECOGN, V45, P1318, DOI 10.1016/j.patcog.2011.09.021
   Shi WW, 2019, IEEE T NEUR NET LEAR, V30, P683, DOI 10.1109/TNNLS.2018.2852721
   Sun XL, 2017, IEEE SYS MAN CYBERN, P1001, DOI 10.1109/SMC.2017.8122741
   Tang Y., 2015, ARXIV13060239
   Wan L, 2013, P 30 INT C MACH LEAR, P1058, DOI DOI 10.5555/3042817.3043055
   Wan WT, 2018, PROC CVPR IEEE, P9117, DOI 10.1109/CVPR.2018.00950
   Wen YH, 2019, AAAI CONF ARTIF INTE, P8989
   Xu CY, 2016, IEEE T CIRC SYST VID, V26, P2273, DOI 10.1109/TCSVT.2015.2477937
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang FL, 2018, IEEE T MULTIMEDIA, V20, P1987, DOI 10.1109/TMM.2018.2790163
NR 33
TC 13
Z9 14
U1 1
U2 14
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2020
VL 6
IS 4
BP 467
EP 476
DI 10.1007/s41095-020-0181-9
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FN
UT WOS:000648692500008
OA gold
DA 2024-07-18
ER

PT J
AU Archer, J
   Leach, G
   Knowles, P
AF Archer, Jesse
   Leach, Geoff
   Knowles, Pyarelal
TI Fast raycasting using a compound deep image for virtual point light
   range determination
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE deep image; indirect lighting; raycasting
AB The concept of using multiple deep images, under a variety of different names, has been explored as a possible acceleration approach for finding ray-geometry intersections. We leverage recent advances in deep image processing from order independent transparency for fast building of a compound deep image (CDI) using a coherent memory format well suited for raycasting. We explore the use of a CDI and raycasting for the problem of determining distance between virtual point lights (VPLs) and geometry for indirect lighting, with the key raycasting step being a small fraction of total frametime.
C1 [Archer, Jesse; Leach, Geoff; Knowles, Pyarelal] RMIT Univ, Sch Sci, Melbourne, Vic 3000, Australia.
C3 Royal Melbourne Institute of Technology (RMIT)
RP Archer, J (corresponding author), RMIT Univ, Sch Sci, Melbourne, Vic 3000, Australia.
EM jesse.archer@rmit.edu.au; geoff.leach@rmit.edu.au
CR Aalund F P, 2015, P 26 EUR S REND EXP
   ACM, 2008, ACM T GRAPHIC, P27
   Archer J., 2017, P SIGGRAPH AS TECHN
   Archer J, 2018, VISUAL COMPUT, V34, P853, DOI 10.1007/s00371-018-1535-5
   Archibald J, 2018, PROCEEDINGS OF THE TWELFTH INTERNATIONAL SYMPOSIUM ON HUMAN ASPECTS OF INFORMATION SECURITY & ASSURANCE (HAISA 2018), P147
   Barák T, 2013, COMPUT GRAPH FORUM, V32, P87, DOI 10.1111/cgf.12154
   Cook R. L., 1981, Computer Graphics, V15, P307, DOI 10.1145/965161.806819
   Dachsbacher C., 2005, Proc. Symp. Interactive Graph. and Games, P203, DOI DOI 10.1145/1053427.1053460
   Harada Takahiro., 2012, Eurographics (Short Papers), P5
   Harris M., 2007, GPU GEMS, V3, P851
   Hu W, 2014, VISUAL COMPUT, V30, P697, DOI 10.1007/s00371-014-0968-8
   Keller A., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P49, DOI 10.1145/258734.258769
   Knowles P, 2013, OIT P PAC GRAPH 2013, P59
   Knowles P, 2014, VISUAL COMPUT, V30, P603, DOI 10.1007/s00371-014-0956-z
   Knowles P, 2012, OPENGL INSIGHTS, P279
   Laurent G, 2016, COMPUT GRAPH FORUM, V35, P79, DOI 10.1111/cgf.12951
   Mara M, 2014, ILLUMINATION APPROXI
   Maule M., 2012, 2012 XXV SIBGRAPI - Conference on Graphics, Patterns and Images (SIBGRAPI 2012), P134, DOI 10.1109/SIBGRAPI.2012.27
   Nalbach O., 2014, Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, P79
   Niessner M, 2010, VISUAL COMPUT, V26, P679, DOI 10.1007/s00371-010-0486-2
   Olsson O., 2012, Proceedings of the Fourth ACM SIGGRAPH/Eurographics conference on HighPerformance Graphics, P87
   Olsson O., 2011, Journal of Graphics, GPU, and Game Tools, V15, P235, DOI DOI 10.1080/2151237X.2011.621761
   Ritschel T, 2011, COMPUT GRAPH FORUM, V30, P2258, DOI 10.1111/j.1467-8659.2011.01998.x
   Tokuyoshi Y., 2016, J COMPUT GRAPH TECH, V5, P35
   Tokuyoshi Y, 2017, COMPUT GRAPH FORUM, V36, P55, DOI 10.1111/cgf.13224
   Vardis K, 2016, PROCEEDINGS I3D 2016: 20TH ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, P171, DOI 10.1145/2856400.2856401
NR 26
TC 1
Z9 1
U1 0
U2 1
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2019
VL 5
IS 3
BP 257
EP 265
DI 10.1007/s41095-019-0144-1
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UN
UT WOS:000648690900003
OA gold
DA 2024-07-18
ER

PT J
AU Fu, K
   Jiang, Y
   Ji, GP
   Zhou, T
   Zhao, QJ
   Fan, DP
AF Fu, Keren
   Jiang, Yao
   Ji, Ge-Peng
   Zhou, Tao
   Zhao, Qijun
   Fan, Deng-Ping
TI Light field salient object detection: A review and benchmark
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review
DE light field; salient object detection (SOD); deep learning; benchmarking
ID FRAMEWORK; DEPTH; MODEL
AB Salient object detection (SOD) is a longstanding research topic in computer vision with increasing interest in the past decade. Since light fields record comprehensive information of natural scenes that benefit SOD in a number of ways, using light field inputs to improve saliency detection over conventional RGB inputs is an emerging trend. This paper provides the first comprehensive review and a benchmark for light field SOD, which has long been lacking in the saliency community. Firstly, we introduce light fields, including theory and data forms, and then review existing studies on light field SOD, covering ten traditional models, seven deep learning-based models, a comparative study, and a brief review. Existing datasets for light field SOD are also summarized. Secondly, we benchmark nine representative light field SOD models together with several cutting-edge RGB-D SOD models on four widely used light field datasets, providing insightful discussions and analyses, including a comparison between light field SOD and RGB-D SOD models. Due to the inconsistency of current datasets, we further generate complete data and supplement focal stacks, depth maps, and multi-view images for them, making them consistent and uniform. Our supplemental data make a universal benchmark possible. Lastly, light field SOD is a specialised problem, because of its diverse data representations and high dependency on acquisition hardware, so it differs greatly from other saliency detection tasks. We provide nine observations on challenges and future directions, and outline several open issues. All the materials including models, datasets, benchmarking results, and supplemented light field datasets are publicly available at https://github.com/kerenfu/LFSOD Survey.
C1 [Fu, Keren; Jiang, Yao; Zhao, Qijun] Sichuan Univ, Coll Comp Sci, Chengdu 610065, Peoples R China.
   [Ji, Ge-Peng] Sichuan Univ, Natl Key Lab Fundamental Sci Synthet Vis, Chengdu 610065, Peoples R China.
   [Zhou, Tao] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
   [Fan, Deng-Ping] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, PCA Lab, Nanjing 210094, Peoples R China.
   [Fan, Deng-Ping] Nankai Univ, Coll Comp Sci, Tianjin 300350, Peoples R China.
C3 Sichuan University; Sichuan University; Wuhan University; Nanjing
   University of Science & Technology; Nankai University
RP Zhou, T (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
EM fkrsuper@scu.edu.cn; yaojiangyj@foxmail.com; gepengai.ji@gmail.com;
   taozhou.ai@gmail.com; qjzhao@scu.edu.cn; dengpfan@gmail.com
RI Fan, Deng-Ping/ABD-4052-2020; 吴, 恒/HZI-8034-2023; Fu,
   Keren/HPG-4742-2023; Zhao, QiJun/KIH-9623-2024
OI Fan, Deng-Ping/0000-0002-5245-7518; Fu, Keren/0000-0002-3195-2077; 
FU National Natural Science Foundation of China [62176169, 62172228,
   61703077, 61773270]; SCU-Luzhou Municipal People's Government Strategic
   Cooperation Project [2020CDLZ-10]
FX This work was supported by the National Natural Science Foundation of
   China (Nos. 62176169, 62172228, 61703077, 61773270), and SCU-Luzhou
   Municipal People's Government Strategic Cooperation Project (No.
   2020CDLZ-10).
CR Achanta R., 2009, P IEEE C COMPUTER VI
   Adelson E.H., 1991, COMPUTATIONAL MODELS, P320
   Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718
   [Anonymous], 2016, INPROC INT JOINT C A
   [Anonymous], 2002, P 10 ACM INT C MULT
   Borji A, 2021, IEEE T PATTERN ANAL, V43, P679, DOI 10.1109/TPAMI.2019.2935715
   Borji Ali, 2019, [Computational Visual Media, 计算可视媒体], V5, P117
   Borji A, 2014, J VISION, V14, DOI 10.1167/14.3.29
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Buehler C., 2001, P 28 ANN C COMPUTER
   Cai ZW, 2021, IEEE T PATTERN ANAL, V43, P1483, DOI 10.1109/TPAMI.2019.2956516
   Chen K., 2019, P IEEECVF C COMPUTER
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen T. L., 2020, P IEEECVF C COMPUTER
   Chen X. L., 2015, P IEEE INT C COMPUTE
   Cheng M. M., 2011, P IEEECVF C COMPUTER
   Cheng MM, 2019, COMPUT VIS MEDIA, V5, P3, DOI 10.1007/s41095-018-0120-1
   Chongyi Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P225, DOI 10.1007/978-3-030-58598-3_14
   Dai A., 2020, P IEEECVF C COMPUTER
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Fan D. P., 2019, P IEEECVF C COMPUTER
   Fan D. P., 2017, P IEEE INT C COMPUTE
   Fan D.-P., 2018, P 27 INT JOINT C ART
   Fan DP, 2020, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR42600.2020.00285
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fan RC, 2020, COMPUT VIS MEDIA, V6, P191, DOI 10.1007/s41095-020-0173-9
   Feng M. Y., 2019, P IEEECVF C COMPUTER
   Fu K. R., 2020, P IEEECVF C COMPUTER
   Fu KR, 2022, IEEE T PATTERN ANAL, V44, P5541, DOI 10.1109/TPAMI.2021.3073689
   Fu KR, 2019, NEUROCOMPUTING, V356, P69, DOI 10.1016/j.neucom.2019.04.062
   Gershun Andrei, 1939, J MATH PHYS, V18, P51, DOI [10.1002/sapm193918151, DOI 10.1002/SAPM193918151]
   Han JG, 2013, NEUROCOMPUTING, V111, P70, DOI 10.1016/j.neucom.2012.12.015
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Jeon H. G., 2015, P IEEE C COMPUTER VI
   Jiang P., 2013, P IEEE INT C COMPUTE
   Jin S., 2011, P INT C COMPUTER VIS
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kuthirummal S, 2011, IEEE T PATTERN ANAL, V33, P58, DOI 10.1109/TPAMI.2010.66
   Lai BS, 2016, PROC CVPR IEEE, P3630, DOI 10.1109/CVPR.2016.395
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Li G., 2017, P IEEE C COMPUTER VI
   Li GY, 2020, IEEE T IMAGE PROCESS, V29, P4873, DOI 10.1109/TIP.2020.2976689
   Li N., 2014, P IEEE C COMPUTER VI
   Li N. Y., 2015, P IEEE C COMPUTER VI
   Li NY, 2017, IEEE T PATTERN ANAL, V39, P1605, DOI 10.1109/TPAMI.2016.2610425
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma YF, 2005, IEEE T MULTIMEDIA, V7, P907, DOI 10.1109/TMM.2005.854410
   Miao Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P374, DOI 10.1007/978-3-030-58604-1_23
   Moosmann F., 2006, P ECCV06 WORKSHOP RE
   Ng R, 2005, RES REPORT, DOI DOI 10.1145/3097571
   Nian Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13753, DOI 10.1109/CVPR42600.2020.01377
   Ouerhani N., 2001, P 11 INT C IMAGE ANA
   Park J., 2017, P IEEE C COMPUTER VI
   Perazzi F., 2012, P IEEE C COMPUTER VI
   Piao Y., 2021, ARXIV PREPRINT ARXIV
   Piao YR, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P904
   Piao YR, 2020, AAAI CONF ARTIF INTE, V34, P11865
   Piao YR, 2020, IEEE T IMAGE PROCESS, V29, P1879, DOI 10.1109/TIP.2019.2942434
   Qian MY, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.06.021
   Qin X. B., 2019, P IEEECVF C COMPUTER
   Ren ZX, 2014, IEEE T CIRC SYST VID, V24, P769, DOI 10.1109/TCSVT.2013.2280096
   Rutishauser U., 2004, P IEEE COMPUTER SOC, VII
   Sheng H., 2016, PROC IEEE INT CONF A
   Shi X., 2015, P 28 INT C NEURAL IN
   Song HM, 2018, LECT NOTES COMPUT SC, V11215, P744, DOI 10.1007/978-3-030-01252-6_44
   Sugano Y., 2010, P IEEE COMPUTER SOC
   Tao M. W., 2013, P IEEE INT C COMPUTE
   Tao M. W., 2015, P IEEE C COMPUTER VI
   Tian X., 2020, P BRIT MACH VIS C
   Tsiami A., 2020, P IEEECVF C COMPUTER
   Wang AZ, 2017, NEURAL PROCESS LETT, V46, P1083, DOI 10.1007/s11063-017-9610-x
   Wang HQ, 2018, MULTIMED TOOLS APPL, V77, P14655, DOI 10.1007/s11042-017-5052-8
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang T. C., 2015, P IEEE INT C COMPUTE
   Wang T. T., 2019, P IEEECVF INT C COMP
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang WG, 2016, IEEE T IMAGE PROCESS, V25, P5025, DOI 10.1109/TIP.2016.2601784
   Wang X., 2018, P IEEECVF C COMPUTER
   Wang X, 2021, MULTIMED TOOLS APPL, V80, P16329, DOI 10.1007/s11042-020-08890-x
   Wei Y. C., 2017, P IEEE C COMPUTER VI
   Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929
   Wei Yunchao., 2014, CoRR
   Wu Z., 2019, P IEEECVF INT C COMP
   Xu YC, 2015, IEEE I CONF COMP VIS, P3442, DOI 10.1109/ICCV.2015.393
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Youwei Pang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P235, DOI 10.1007/978-3-030-58595-2_15
   Zeng Y., 2019, P IEEECVF C COMPUTER
   Zeng Y., 2019, P IEEECVF INT C COMP
   Zhang J., 2020, P IEEE CVF C COMP VI, P8579, DOI DOI 10.1109/CVPR42600.2020.00861
   Zhang J., 2015, P 24 INT C ARTIFICIA
   Zhang J, 2020, IEEE T IMAGE PROCESS, V29, P4421, DOI 10.1109/TIP.2020.2970529
   Zhang J, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3107956
   Zhang M, 2020, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR42600.2020.00353
   Zhang M, 2020, IEEE T IMAGE PROCESS, V29, P6276, DOI 10.1109/TIP.2020.2990341
   Zhang MH, 2019, ADV NEUR IN, V32
   Zhang P. P., 2017, P IEEE INT C COMPUTE
   Zhang PP, 2017, IEEE I CONF COMP VIS, P212, DOI 10.1109/ICCV.2017.32
   Zhang QD, 2021, IEEE T CIRC SYST VID, V31, P1849, DOI 10.1109/TCSVT.2020.3013119
   Zhang XD, 2015, NEUROCOMPUTING, V166, P389, DOI 10.1016/j.neucom.2015.03.042
   Zhao J. X., 2019, P IEEECVF C COMPUTER
   Zhao WD, 2020, IEEE T PATTERN ANAL, V42, P1884, DOI 10.1109/TPAMI.2019.2906588
   Zhou T, 2021, COMPUT VIS MEDIA, V7, P37, DOI 10.1007/s41095-020-0199-z
   Zhu W., 2014, P IEEE C COMPUTER VI
NR 107
TC 25
Z9 26
U1 15
U2 68
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2022
VL 8
IS 4
BP 509
EP 534
DI 10.1007/s41095-021-0256-2
EA MAY 2022
PG 26
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2X6VI
UT WOS:000796328900001
OA Green Submitted, gold
DA 2024-07-18
ER

PT J
AU Taylor, S
   Sharpe, O
   Peethambaran, J
AF Taylor, Sheldon
   Sharpe, Owen
   Peethambaran, Jiju
TI Prime gradient noise
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE procedural noise; procedural texture; Perlin noise; prime numbers
AB Procedural noise functions are fundamental tools in computer graphics used for synthesizing virtual geometry and texture patterns. Ideally, a procedural noise function should be compact, aperiodic, parameterized, and randomly accessible. Traditional lattice noise functions such as Perlin noise, however, exhibit periodicity due to the axial correlation induced while hashing the lattice vertices to the gradients. In this paper, we introduce a parameterized lattice noise called prime gradient noise (PGN) that minimizes discernible periodicity in the noise while enhancing the algorithmic efficiency. PGN utilizes prime gradients, a set of random unit vectors constructed from subsets of prime numbers plotted in polar coordinate system. To map axial indices of lattice vertices to prime gradients, PGN employs Szudzik pairing, a bijection F : N-2 -> N. Compositions of Szudzik pairing functions are used in higher dimensions. At the core of PGN is the ability to parameterize noise generation though prime sequence offsetting which facilitates the creation of fractal noise with varying levels of heterogeneity ranging from homogeneous to hybrid multifractals. A comparative spectral analysis of the proposed noise with other noises including lattice noises show that PGN significantly reduces axial correlation and hence, periodicity in the noise texture. We demonstrate the utility of the proposed noise function with several examples in procedural modeling, parameterized pattern synthesis, and solid texturing.
C1 [Taylor, Sheldon; Sharpe, Owen; Peethambaran, Jiju] St Marys Univ, Graph & Spatial Comp Lab, Dept Math & CS, Halifax, NS B3H3C3, Canada.
C3 Saint Marys University - Canada
RP Peethambaran, J (corresponding author), St Marys Univ, Graph & Spatial Comp Lab, Dept Math & CS, Halifax, NS B3H3C3, Canada.
EM sheldontaylor.7@gmail.com; owensharpe19@gmail.com;
   jiju.poovvancheri@smu.ca
FU National Science and Engineering Research Council of Canada (NSERC)
   [2019-05092]
FX This work is supported by the National Science and Engineering Research
   Council of Canada (NSERC) Discovery Grant No. 2019-05092.
CR Benard P., 2010, P ACM SIGGRAPH 2010 P ACM SIGGRAPH 2010
   Bridson R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276435, 10.1145/1239451.1239497]
   Cook RL, 2005, ACM T GRAPHIC, V24, P803, DOI 10.1145/1073204.1073264
   Ebert D.S., 2003, TEXTURING MODELING, V3rd
   Fisher R. A., 1963, J I ACTUARIES, V90, P370
   Forbes T., 2002, MATH GAZ, V86, P552
   FOURNIER A, 1982, COMMUN ACM, V25, P371, DOI 10.1145/358523.358553
   Frisvad JR, 2007, GRAPHITE 2007: 5TH INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES IN AUSTRALASIA AND SOUTHERN ASIA, PROCEEDINGS, P243
   Galerne B, 2017, COMPUT GRAPH FORUM, V36, P205, DOI 10.1111/cgf.13073
   Galerne B, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185569
   Gilet G, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661249
   Goldberg A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360653
   Guehl P, 2020, COMPUT GRAPH FORUM, V39, P159, DOI 10.1111/cgf.14061
   Hart J. C., 1999, Proceedings 1999 EUROGRAPHICS/SIGGRAPH Workshop on Graphics Hardware, P45, DOI 10.1145/311534.311575
   Hart J. C., 2001, P ACM SIGGRAPH EUROG P ACM SIGGRAPH EUROG, P87
   Kensler A., 2008, BETTER GRADIENT NOIS BETTER GRADIENT NOIS
   Kirillov A, 2018, P ACM COMPUT GRAPH, V1, P1
   Kuipers L., 1974, UNIFORM DISTRIBUTION
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Lagae A, 2010, COMPUT GRAPH FORUM, V29, P2579, DOI 10.1111/j.1467-8659.2010.01827.x
   Lagae A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964946
   Lagae A, 2011, IEEE T VIS COMPUT GR, V17, P1096, DOI 10.1109/TVCG.2010.238
   Lagae A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531360
   Lewis J.-P., 1984, Computers & Graphics, V18, P245
   Lewis J. P., 1989, Computer Graphics, V23, P263, DOI 10.1145/74334.74360
   Lewis J. P., 1986, Proceedings of Graphics Interface '86 and Vision Interface '86, P173
   LEWIS JP, 1987, ACM T GRAPHIC, V6, P167, DOI 10.1145/35068.35069
   LINDENMAYER A, 1968, J THEOR BIOL, V18, P280, DOI 10.1016/0022-5193(68)90079-9
   Liu J, 2018, NEUROCOMPUTING, V291, P21, DOI 10.1016/j.neucom.2018.02.061
   Olano M., 2005, HWWS 05, P105
   Olano M., 2004, P SIGGRAPH 2004 COUR
   Perlin K, 2002, ACM T GRAPHIC, V21, P681, DOI 10.1145/566570.566636
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   Perlin K., 2001, SIGGRAPH TECHNICAL S, V187
   Planetside Software, 2020, PERL 3D SCAL PERL 3D SCAL
   Rost R.J., 2006, OpenGL Shading Language, V2nd
   Sendik O, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3015461
   Spjut JB, 2009, GLSVLSI 2009: PROCEEDINGS OF THE 2009 GREAT LAKES SYMPOSIUM ON VLSI, P457
   Szudzik M, 2006, P NKS WOLFR SCI C P NKS WOLFR SCI C
   Szudzik M. P., 2017, ARXIV PREPRINT ARXIV
   Thorimbert Y., 2016, ARXIV PREPRINT ARXIV
   Tricard T, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322990
   Unity, 2020, MATHF PERL MATHF PERL
   VANWIJK JJ, 1991, COMP GRAPH, V25, P309, DOI 10.1145/127719.122751
   Vinogradov I., 1985, SELECTED WORKS
   Wei Li-Yi., 2004, SIGGRAPHEUROGRAPHICS, P55
   Weisstein E., Sphere Point Picking
   Worley S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P291, DOI 10.1145/237170.237267
   Wyvill G., 1999, P ACM SIGGRAPH 99 C P ACM SIGGRAPH 99 C, P242
   Zhou Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201285
   Zsolnai-Fehér K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201307
NR 51
TC 0
Z9 1
U1 1
U2 3
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2021
VL 7
IS 3
BP 349
EP 362
DI 10.1007/s41095-021-0206-z
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TR1TJ
UT WOS:000678754100005
OA gold
DA 2024-07-18
ER

PT J
AU Filip, J
   Vávra, R
AF Filip, Jiri
   Vavra, Radomir
TI Image-based appearance acquisition of effect coatings
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE effect coatings; measurement; bidirectional texture function (BTF);
   appearance; psychophysical experiment
ID TEXTURE; REFLECTANCE
AB Paint manufacturers strive to introduce unique visual effects to coatings in order to visually communicate functional properties of products using value-added, customized design. However, these effects often feature complex, angularly dependent, spatially-varying behavior, thus representing a challenge in digital reproduction. In this paper we analyze several approaches to capturing spatially-varying appearances of effect coatings. We compare a baseline approach based on a bidirectional texture function (BTF) with four variants of half-difference parameterization. Through a psychophysical study, we determine minimal sampling along individual dimensions of this parameterization. We conclude that, compared to BTF, bivariate representations better preserve visual fidelity of effect coatings, better characterizing near-specular behavior and significantly the restricting number of images which must be captured.
C1 [Filip, Jiri; Vavra, Radomir] Czech Acad Sci, Inst Informat Theory & Automat, Vodarenskou Vezi4, Prague 18508 8, Czech Republic.
C3 Czech Academy of Sciences; Institute of Information Theory & Automation
   of the Czech Academy of Sciences
RP Filip, J (corresponding author), Czech Acad Sci, Inst Informat Theory & Automat, Vodarenskou Vezi4, Prague 18508 8, Czech Republic.
EM filipj@utia.cas.cz; vavra@utia.cas.cz
RI Filip, Jiri/D-3396-2012; Vavra, Radomir/H-4349-2014
FU Czech Science Foundation [17-18407S]
FX The authors would like to thank Frank J. Maile from Schlenk Metallic
   Pigments GmbH for sample preparation and inspiring discussions, our
   colleague Martina Kolafova for organization and running of
   psychophysical experiments, and all anonymous subjects for the time they
   devoted to participation in visual experiments. This research was
   supported by Czech Science Foundation grant 17-18407S.
CR [Anonymous], 2013, ACM T GRAPHICS, V32, P6
   [Anonymous], 2008, ACM T GRAPHICS, V27, P5
   [Anonymous], 2007, ACM T GRAPHICS, V26, P3
   [Anonymous], 2013, ACM T GRAPHICS, V32, P4
   [Anonymous], 2014, ACM T GRAPHICS, V33, P4
   [Anonymous], 2014, ACM T GRAPHICS, V33, P4
   [Anonymous], 2016, ACM T GRAPHICS, V35, P4
   [Anonymous], 2016, ACM T GRAPHICS, V35, P6
   [Anonymous], 1998, Inverse rendering for computer graphics
   [Anonymous], 2010, ACM T GRAPHICS, V29, P4
   Aydin TO, 2008, PROC SPIE, V6806, DOI 10.1117/12.765095
   Burley B, 2018, BRDF RELATED RESOURC
   COCHRAN WG, 1950, BIOMETRIKA, V37, P256, DOI 10.1093/biomet/37.3-4.256
   Cook R. L., 1981, Computer Graphics, V15, P307, DOI 10.1145/965161.806819
   Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778
   den Brok D, 2018, COMPUT GRAPH-UK, V73, P26, DOI 10.1016/j.cag.2018.03.003
   Durikovic R, 2013, J APPL MATH STAT INF, V9, P25, DOI 10.2478/jamsi-2013-0010
   Ergun S, 2016, GEN MICROFLAKE MODEL, P65
   Ershov S, 2001, COMPUT GRAPH FORUM, V20, pC227, DOI 10.1111/1467-8659.00515
   Ferrero A, 2013, J OPT SOC AM A, V30, P206, DOI 10.1364/JOSAA.30.000206
   Filip J, 2017, BRDF MEASUREMENT HIG, DOI 10.1145/3154353.3154370
   Filip J, 2013, PROC CVPR IEEE, P1468, DOI 10.1109/CVPR.2013.193
   Golla T, 2018, P VIS MOD VIS
   Golla T, 2017, LECT NOTES COMPUT SC, V10700, P51, DOI 10.1007/978-3-319-72323-5_4
   Gunther J., 2005, VISION, MODELING, AND VISUALIZATION 2005 (VMV'05), P487
   Jarabo A, 2014, IEEE T VIS COMPUT GR, V20, P880, DOI 10.1109/TVCG.2014.2312016
   Maile FJ, 2005, PROG ORG COAT, V54, P150, DOI 10.1016/j.porgcoat.2005.07.003
   Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343
   McAuley S., 2012, ACM, P1, DOI [DOI 10.1145/2343483.2343493, 10.1145/2343483.2343493]
   Nicodemus F E, 1992, RADIOMETRY, V94, P145
   Palmer C., 2005, Diffraction Grating Handbook
   Pellacini F, 2000, COMP GRAPH, P55, DOI 10.1145/344779.344812
   rikovi R, 2003, SIMULATION SPARKLING, P193
   Romeiro F, 2008, LECT NOTES COMPUT SC, V5305, P859, DOI 10.1007/978-3-540-88693-8_63
   Rump M, 2009, EFFICIENT RESAMPLING, P11
   Rump M, 2008, COMPUT GRAPH FORUM, V27, P527, DOI 10.1111/j.1467-8659.2008.01150.x
   Rusinkiewicz S. M., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P11
   Sattler M., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P167
   Somol P, 2005, NOVEL PATH SEARCH AL, P155
   Strothkämper C, 2016, J OPT SOC AM A, V33, P1, DOI 10.1364/JOSAA.33.000001
   Vavra Radomir, 2018, Computational Visual Media, V4, P55, DOI 10.1007/s41095-017-0099-z
   Vávra R, 2016, COMPUT GRAPH FORUM, V35, P299, DOI 10.1111/cgf.13027
   Velinov Z, 2016, INTERACTIVE APPEARAN, P145
   Ward G, 2014, REDUCING ANISOTROPIC, P5
   WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078
   Xu K, 2009, COMPUT GRAPH FORUM, V28, P1871, DOI 10.1111/j.1467-8659.2009.01565.x
NR 46
TC 3
Z9 3
U1 0
U2 3
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2019
VL 5
IS 1
BP 73
EP 89
DI 10.1007/s41095-019-0134-3
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UL
UT WOS:000648690700007
OA gold
DA 2024-07-18
ER

PT J
AU Cao, J
   Zhang, XY
   Huang, JN
   Zhang, YJ
AF Cao, Juan
   Zhang, Xiaoyi
   Huang, Jiannan
   Zhang, Yongjie Jessica
TI Polygonal finite element-based content-aware image warping
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE image warping; finite element method (FEM); polygonal element; mesh
   generation
AB Mesh-based image warping techniques typically represent image deformation using linear functions on triangular meshes or bilinear functions on rectangular meshes. This enables simple and efficient implementation, but in turn, restricts the representation capability of the deformation, often leading to unsatisfactory warping results. We present a novel, flexible polygonal finite element (poly-FEM) method for content-aware image warping. Image deformation is represented by high-order poly-FEMs on a content-aware polygonal mesh with a cell distribution adapted to saliency information in the source image. This allows highly adaptive meshes and smoother warping with fewer degrees of freedom, thus significantly extending the flexibility and capability of the warping representation. Benefiting from the continuous formulation of image deformation, our poly-FEM warping method is able to compute the optimal image deformation by minimizing existing or even newly designed warping energies consisting of penalty terms for specific transformations. We demonstrate the versatility of the proposed poly-FEM warping method in representing different deformations and its superiority by comparing it to other existing state-of-the-art methods.
C1 [Cao, Juan; Zhang, Xiaoyi; Huang, Jiannan] Xiamen Univ, Sch Math Sci, Xiamen 361005, Peoples R China.
   [Cao, Juan; Zhang, Xiaoyi; Huang, Jiannan] Xiamen Univ, Fujian Prov Key Lab Math Modeling & High Performan, Xiamen 361005, Peoples R China.
   [Zhang, Yongjie Jessica] Carnegie Mellon Univ, Dept Mech Engn, Pittsburgh, PA 15213 USA.
C3 Xiamen University; Xiamen University; Carnegie Mellon University
RP Cao, J (corresponding author), Xiamen Univ, Sch Math Sci, Xiamen 361005, Peoples R China.; Cao, J (corresponding author), Xiamen Univ, Fujian Prov Key Lab Math Modeling & High Performan, Xiamen 361005, Peoples R China.
EM Juancao@xmu.edu.cn; zxyzjcx@163.com; huangjn821@163.com;
   Jessicaz@andrew.cmu.edu
RI Zhang, Yongjie Jessica/F-8733-2012
OI Zhang, Yongjie Jessica/0000-0001-7436-9757
FU National Natural Science Foundation of China [61872308, 61972327,
   62272402]; Xiamen Youth Innovation Funds [3502Z20206029]; NSF
   [CMMI-1953323]; Honda grant
FX We would like to thank the reviewers for their valuable comments. The
   research of Juan Cao was supported by the National Natural Science
   Foundation of China (Nos. 61872308, 61972327, and 62272402), and the
   Xiamen Youth Innovation Funds (No. 3502Z20206029). Yongjie Jessica Zhang
   was supported in part by NSF CMMI-1953323 and a Honda grant.
CR [Anonymous], 2010, P 17 ACM S VIRTUAL R
   [Anonymous], 2007, 2007 IEEE 11 INT C C, DOI DOI 10.1109/ICCV.2007.4409010
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Bajaj C, 2008, LECT NOTES COMPUT SC, V4975, P344
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Cao J, 2018, COMPUT AIDED GEOM D, V63, P149, DOI 10.1016/j.cagd.2018.05.005
   Chen R.C., 2010, 2010 IEEE International Conference on Fuzzy Systems, P1
   Cho D, 2017, IEEE I CONF COMP VIS, P4568, DOI 10.1109/ICCV.2017.488
   Danon D, 2021, COMPUT VIS MEDIA, V7, P453, DOI 10.1007/s41095-021-0216-x
   Du Q, 1999, SIAM REV, V41, P637, DOI 10.1137/S0036144599352836
   Engelke U, 2015, IEEE SIGNAL PROC LET, V22, P705, DOI 10.1109/LSP.2014.2368136
   Floater MS, 2016, SIAM J NUMER ANAL, V54, P797, DOI 10.1137/15M101155X
   GEE JC, 1994, P SOC PHOTO-OPT INS, V2167, P327, DOI 10.1117/12.175067
   Guo YW, 2009, IEEE T MULTIMEDIA, V11, P856, DOI 10.1109/TMM.2009.2021781
   GUPTA S., 2013, Int. J. Comput. Sci. Manag. Res, V2, P1578
   Hormann K, 2006, ACM T GRAPHIC, V25, P1424, DOI 10.1145/1183287.1183295
   Hu S.-M., 2004, Proceedings of the ninth ACM symposium on Solid modeling and applications, P309
   Jacobson A, 2010, COMPUT GRAPH FORUM, V29, P1565, DOI 10.1111/j.1467-8659.2010.01765.x
   Karni Z, 2009, COMPUT GRAPH FORUM, V28, P1257, DOI 10.1111/j.1467-8659.2009.01503.x
   Kaufmann P, 2013, COMPUT GRAPH FORUM, V32, P31, DOI 10.1111/cgf.12023
   Kiess J, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3231598
   Krähenbühl P, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1616452.1618472, 10.1145/1618452.1618472]
   Laffont P.-Y., 2010, GRAPHICS INTERFACE 2, P79
   Lau CP, 2018, IEEE T IMAGE PROCESS, V27, P5787, DOI 10.1109/TIP.2018.2858146
   Li XY, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461917
   Liu YJ, 2020, IEEE T PATTERN ANAL, V42, P1798, DOI 10.1109/TPAMI.2019.2923998
   Meyer M., 2002, Journal of Graphics Tools, V7, P13, DOI 10.1080/10867651.2002.10487551
   Nishiyama M., 2009, P 17 ACM INT C MULTI, P669
   Pritch Y, 2009, IEEE I CONF COMP VIS, P151, DOI 10.1109/ICCV.2009.5459159
   Rand A, 2014, MATH COMPUT, V83, P2691, DOI 10.1090/S0025-5718-2014-02807-X
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Rubinstein M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531329
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Schneider T., 2017, THESIS U SVIZZERA IT
   Setlur V., 2005, 4 INT C MOB UB MULT, P59
   Sieger D., 2010, Proceedings of the 19th international meshing roundtable, P335
   Suh Bongwon, 2003, P ACM S US INT SOFTW, P95, DOI DOI 10.1145/964696.964707
   Sukumar N, 2013, COMPUT METHOD APPL M, V263, P27, DOI 10.1016/j.cma.2013.04.009
   Tan WM, 2020, IEEE T MULTIMEDIA, V22, P1730, DOI 10.1109/TMM.2019.2959925
   Tang F, 2020, IEEE T MULTIMEDIA, V22, P641, DOI 10.1109/TMM.2019.2932620
   Vaquero D, 2010, PROC SPIE, V7798, DOI 10.1117/12.862419
   Wachspress E.L., 1975, A rational finite element basis
   Wang JD, 2017, INT J COMPUT VISION, V123, P251, DOI 10.1007/s11263-016-0977-3
   Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071
   Yan HB, 2004, COMPUT ANIMAT VIRT W, V15, P443, DOI 10.1002/cav.48
   Zhang GX, 2009, COMPUT GRAPH FORUM, V28, P1897, DOI 10.1111/j.1467-8659.2009.01568.x
   Zhang Y, 2016, GEOMETRIC MODELING M, P1, DOI [DOI 10.1201/B19466, 10.1201/b19466]
   Zhou Y, 2021, IEEE T CIRC SYST VID, V31, P126, DOI 10.1109/TCSVT.2020.2977943
NR 48
TC 1
Z9 1
U1 1
U2 14
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2023
VL 9
IS 2
BP 367
EP 383
DI 10.1007/s41095-022-0283-7
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7N8FL
UT WOS:000907570700010
OA gold
DA 2024-07-18
ER

PT J
AU Zhang, DJ
   He, LC
   Luo, MT
   Xu, ZY
   He, FZ
AF Zhang, Dejun
   He, Linchao
   Luo, Mengting
   Xu, Zhanya
   He, Fazhi
TI Weight asynchronous update: Improving the diversity of filters in a deep
   convolutional network
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE deep convolutional network; model compression; convolutional filter
ID NEURAL-NETWORKS
AB Deep convolutional networks have obtained remarkable achievements on various visual tasks due to their strong ability to learn a variety of features. A well-trained deep convolutional network can be compressed to 20%-40% of its original size by removing filters that make little contribution, as many overlapping features are generated by redundant filters. Model compression can reduce the number of unnecessary filters but does not take advantage of redundant filters since the training phase is not affected. Modern networks with residual, dense connections and inception blocks are considered to be able to mitigate the overlap in convolutional filters, but do not necessarily overcome the issue. To do so, we propose a new training strategy, weight asynchronous update, which helps to significantly increase the diversity of filters and enhance the representation ability of the network. The proposed method can be widely applied to different convolutional networks without changing the network topology. Our experiments show that the stochastic subset of filters updated in different iterations can significantly reduce filter overlap in convolutional networks. Extensive experiments show that our method yields noteworthy improvements in neural network performance.
C1 [Zhang, Dejun; Xu, Zhanya] China Univ Geosci, Sch Geog & Informat Engn, Wuhan 430074, Peoples R China.
   [He, Linchao; Luo, Mengting] Sichuan Agr Univ, Coll Informat & Engn, Yaan 625014, Peoples R China.
   [He, Fazhi] Wuhan Univ, Sch Comp, Wuhan 430072, Peoples R China.
C3 China University of Geosciences; Sichuan Agricultural University; Wuhan
   University
RP Xu, ZY (corresponding author), China Univ Geosci, Sch Geog & Informat Engn, Wuhan 430074, Peoples R China.
EM zhangdejun@cug.edu.cn; fpsandnoob@hotmail.com; sookie0331@icloud.com;
   zhanyaxu@163.com; fzhe@whu.edu.cn
RI He, Fazhi/Q-3691-2018
OI He, Linchao/0000-0002-0562-3026; Zhang, Dejun/0000-0001-9129-534X
FU National Natural Science Foundation of China [61702350]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant No. 61702350.
CR [Anonymous], 2009, Technical report
   [Anonymous], 2016, ARXIV160704381
   [Anonymous], 2016, PRUNING CONVOLUTIONA
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Furlanello T., 2018, P MACHINE LEARNING R, V80, P1607, DOI DOI 10.48550/ARXIV.1805.04770
   Gastaldi X., 2017, PROC INT C LEARN REP
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hu Hengyuan, 2016, Network trimming: A data-driven neuron pruning approach towards efficient deep architectures
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   King DB, 2015, ACS SYM SER, V1214, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Latham P. E., 2001, P 14 INT C NEUR INF, P237
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Li J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073637
   Li X, 2019, PROC CVPR IEEE, P2677, DOI 10.1109/CVPR.2019.00279
   Li Yixuan, 2015, ARXIV151107543
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   NORTON JD, 1994, SYNTHESE, V99, P3, DOI 10.1007/BF01064528
   Pan YT, 2019, NEUROCOMPUTING, V332, P137, DOI 10.1016/j.neucom.2018.12.025
   Prakash A, 2019, PROC CVPR IEEE, P10658, DOI 10.1109/CVPR.2019.01092
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Simonyan K., 2014, 14091556 ARXIV
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yamada Y, 2019, IEEE ACCESS, V7, P186126, DOI 10.1109/ACCESS.2019.2960566
   Zagoruyko S., 2016, BMVC, P1
   Zhang DJ, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107312
   Zhang DJ, 2020, INTEGR COMPUT-AID E, V27, P57, DOI 10.3233/ICA-190608
   Zhang DJ, 2019, INT J COMPUT INT SYS, V12, P723, DOI 10.2991/ijcis.d.190710.001
   Zhang DJ, 2019, NEUROCOMPUTING, V362, P83, DOI 10.1016/j.neucom.2019.06.082
NR 35
TC 4
Z9 5
U1 0
U2 4
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2020
VL 6
IS 4
BP 455
EP 466
DI 10.1007/s41095-020-0185-5
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FN
UT WOS:000648692500007
OA gold
DA 2024-07-18
ER

PT J
AU Deng, CY
   Huang, JH
   Yang, YL
AF Deng, Congyue
   Huang, Jiahui
   Yang, Yong-Liang
TI Interactive modeling of lofted shapes from a single image
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE interactive modeling; freeform shapes; image-guided modeling; lofting
ID INTERPOLATION
AB Modeling the complete geometry of general shapes from a single image is an ill-posed problem. User hints are often incorporated to resolve ambiguities and provide guidance during the modeling process. In this work, we present a novel interactive approach for extracting high-quality freeform shapes from a single image. This is inspired by the popular lofting technique in many CAD systems, and only requires minimal user input. Given an input image, the user only needs to sketch several projected cross sections, provide a "main axis", and specify some geometric relations. Our algorithm then automatically optimizes the common normal to the sections with respect to these constraints, and interpolates between the sections, resulting in a high-quality 3D model that conforms to both the original image and the user input. The entire modeling session is efficient and intuitive. We demonstrate the effectiveness of our approach based on qualitative tests on a variety of images, and quantitative comparisons with the ground truth using synthetic images.
C1 [Deng, Congyue; Huang, Jiahui] Tsinghua Univ, Beijing 100084, Peoples R China.
   [Yang, Yong-Liang] Univ Bath, Bath BA2 7AY, Avon, England.
C3 Tsinghua University; University of Bath
RP Deng, CY (corresponding author), Tsinghua Univ, Beijing 100084, Peoples R China.
EM dengcy16@mails.tsinghua.edu.cn; huang-jh18@mails.tsinghua.edu.cn;
   y.yang@cs.bath.ac.uk
RI Huang, Jiahui/AAN-1773-2021
FU EPSRC [EP/M023281/1] Funding Source: UKRI
CR [Anonymous], 2015, ACM T GRAPHICS, V34, P4
   [Anonymous], 2012, ACM T GRAPHICS, V31, P4
   [Anonymous], 2011, P 8 EUR S SKETCH BAS, DOI [10.1145/2021164.2021189, DOI 10.1145/2021164.2021189]
   [Anonymous], 2009, ACM T GRAPHICS, V28, P5
   [Anonymous], 2013, ACM T GRAPHICS, V32, P4
   [Anonymous], 2011, ACM T GRAPHICS, V30, P4
   [Anonymous], 2017, ACM T GRAPHIC, V36, P4
   [Anonymous], 2014, ACM T GRAPHICS, V33, P4
   [Anonymous], 2013, ACM T GRAPHICS, V32, P6
   [Anonymous], 2008, ACM T GRAPHICS, V27, P5
   Cao YP, 2014, COMPUT GRAPH FORUM, V33, P101, DOI 10.1111/cgf.12478
   Catmull E., 1998, SEMINAL GRAPHICS, P183, DOI DOI 10.1145/280811.280992
   Chiyokura H., 1983, Computer Graphics, V17, P289, DOI 10.1145/964967.801160
   Debevec P E, 1996, P SIGGRAPH 96 C
   Ding C, 2016, FRONT COMPUT SCI-CHI, V10, P985, DOI 10.1007/s11704-016-5422-9
   Hermann T, 1996, COMPUT AIDED GEOM D, V13, P873, DOI 10.1016/S0167-8396(96)00013-1
   Igarashi T., 2007, ACM SIGGRAPH 2007 courses, P21
   Li YW, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P387, DOI 10.1145/3126594.3126611
   Mitra NJ, 2006, ACM T GRAPHIC, V25, P560, DOI 10.1145/1141911.1141924
   Nasri A, 1997, COMPUT AIDED GEOM D, V14, P13, DOI 10.1016/S0167-8396(96)00018-0
   Nasri AH, 2003, COMPUT GRAPH FORUM, V22, P87, DOI 10.1111/1467-8659.t01-1-00648
   Nishida G, 2018, COMPUT GRAPH FORUM, V37, P415, DOI 10.1111/cgf.13372
   SCHAEFER S., 2004, SGP 04, P103, DOI DOI 10.1145/1057432.1057447
   Schmidt Ryan., 2006, ACM SIGGRAPH 2006 CO, P14, DOI [10.1145/1185657.1185775, DOI 10.1145/1185657.1185775]
   Sederberg T. W., 1993, Computer Graphics Proceedings, P15, DOI 10.1145/166117.166118
   SEDERBERG TW, 1992, COMP GRAPH, V26, P25, DOI 10.1145/142920.134001
   Shtof A, 2013, COMPUT GRAPH FORUM, V32, P245, DOI 10.1111/cgf.12044
   Watanabe N, 2007, UIST 2007: PROCEEDINGS OF THE 20TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P173
   Xie XH, 2013, COMPUT GRAPH FORUM, V32, P233, DOI 10.1111/cgf.12200
   Xin C., 2018, IEEE T VISUALIZATION
   Zhou B, 2013, COMPUT GRAPH FORUM, V32, P85, DOI 10.1111/cgf.12215
NR 31
TC 7
Z9 9
U1 0
U2 3
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2020
VL 6
IS 3
BP 279
EP 289
DI 10.1007/s41095-019-0153-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FH
UT WOS:000648691900004
OA gold
DA 2024-07-18
ER

PT J
AU Lin, WH
   Wang, BB
   Wang, L
   Holzschuch, N
AF Lin, Weiheng
   Wang, Beibei
   Wang, Lu
   Holzschuch, Nicolas
TI A detail preserving neural network model for Monte Carlo denoising
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE deep learning; light transport covariance; perceptual loss; Monte Carlo
   denoising
ID IMAGE
AB Monte Carlo based methods such as path tracing are widely used in movie production. To achieve low noise, they require many samples per pixel, resulting in long rendering time. To reduce the cost, one solution is Monte Carlo denoising, which renders the image with fewer samples per pixel (as little as 128) and then denoises the resulting image. Many Monte Carlo denoising methods rely on deep learning: they use convolutional neural networks to learn the relationship between noisy images and reference images, using auxiliary features such as position and normal together with image color as inputs. The network predicts kernels which are then applied to the noisy input. These methods show powerful denoising ability, but tend to lose geometric or lighting details and to blur sharp features during denoising.In this paper, we solve this issue by proposing a novel network structure, a new input feature-light transport covariance from path space-and an improved loss function. Our network separates feature buffers from the color buffer to enhance detail effects. The features are extracted separately and then integrated into a shallow kernel predictor. Our loss function considers perceptual loss, which also improves detail preservation. In addition, we use a light transport covariance feature in path space as one of the features, which helps to preserve illumination details. Our method denoises Monte Carlo path traced images while preserving details much better than previous methods.
C1 [Lin, Weiheng; Wang, Beibei] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Wang, Lu] Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Peoples R China.
   [Holzschuch, Nicolas] Univ Grenoble Alpes, LJK, Grenoble INP, INRIA,CNRS, F-38000 Grenoble, France.
C3 Nanjing University of Science & Technology; Shandong University; Centre
   National de la Recherche Scientifique (CNRS); Communaute Universite
   Grenoble Alpes; Institut National Polytechnique de Grenoble; Inria;
   Universite Grenoble Alpes (UGA)
RP Wang, BB (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.; Wang, L (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Peoples R China.
EM 442920398@qq.com; beibei.wang@njust.edu.cn; luwang_hcivr@sdu.edu.cn;
   Nicolas.Holzschuch@inria.fr
RI Holzschuch, Nicolas/E-8861-2014
CR Abadi M, 2015, TENSOROW LARGE SCALE
   [Anonymous], 2015, ACM T GRAPHICS, V34, P4
   [Anonymous], 2016, ACM T GRAPHICS, V35, P4
   [Anonymous], 2013, ACM T GRAPHICS, V32, P3
   [Anonymous], 2017, ACM T GRAPHICS, V36, P4
   [Anonymous], 2018, ACM T GRAPHICS, V37, P4
   [Anonymous], 2014, ACM T GRAPHICS, V33, P5
   [Anonymous], 2017, ACM T GRAPHIC, V36, P4
   [Anonymous], 2019, ACM T GRAPHICS, V38, P4
   [Anonymous], 2012, ACM T GRAPHICS, V31, P3
   [Anonymous], 2014, ACM T GRAPHICS, V33, P5
   Bitterli B, 2016, RENDERING
   Bitterli B, 2016, COMPUT GRAPH FORUM, V35, P107, DOI 10.1111/cgf.12954
   Bitterli Benedikt., TUNGSTEN RENDERER
   Boughida M, 2017, COMPUT GRAPH FORUM, V36, P137, DOI 10.1111/cgf.13231
   Durand F, 2005, ACM T GRAPHIC, V24, P1115, DOI 10.1145/1073204.1073320
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Keller A, 2015, P ACM SIGGRAPH 2015
   King DB, 2015, ACS SYM SER, V1214, P1
   Liang Y L, 2019, IEEE T VISUALIZATION
   Moon B, 2013, COMPUT GRAPH FORUM, V32, P139, DOI 10.1111/cgf.12004
   Rousselle F, 2013, COMPUT GRAPH FORUM, V32, P121, DOI 10.1111/cgf.12219
   Sen P, 2015, P ACM SIGGRAPH 2015
   Simonyan K., 2014, CORR
   Simonyan K, 2014, ADV NEUR IN, V27
   Yang QS, 2018, IEEE T MED IMAGING, V37, P1348, DOI 10.1109/TMI.2018.2827462
   Yang X, 2019, J COMPUT SCI TECH-CH, V34, P1123, DOI 10.1007/s11390-019-1964-2
   Zimmer H, 2015, COMPUT GRAPH FORUM, V34, P131, DOI 10.1111/cgf.12685
NR 28
TC 8
Z9 11
U1 1
U2 9
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2020
VL 6
IS 2
BP 157
EP 168
DI 10.1007/s41095-020-0167-7
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FB
UT WOS:000648691300004
OA Green Submitted, gold
DA 2024-07-18
ER

PT J
AU Chang, Y
   Zhang, CY
   Chen, YS
   Wang, GP
AF Chang, Yuan
   Zhang, Congyi
   Chen, Yisong
   Wang, Guoping
TI Homography-guided stereo matching for wide-baseline image interpolation
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE image interpolation; view synthesis; homography propagation; belief
   propagation
ID OPTICAL-FLOW; BELIEF PROPAGATION
AB Image interpolation has a wide range of applications such as frame rate-up conversion and free viewpoint TV. Despite significant progresses, it remains an open challenge especially for image pairs with large displacements. In this paper, we first propose a novel optimization algorithm for motion estimation, which combines the advantages of both global optimization and a local parametric transformation model. We perform optimization over dynamic label sets, which are modified after each iteration using the prior of piecewise consistency to avoid local minima. Then we apply it to an image interpolation framework including occlusion handling and intermediate image interpolation. We validate the performance of our algorithm experimentally, and show that our approach achieves state-of-the-art performance.
C1 [Chang, Yuan; Zhang, Congyi; Chen, Yisong; Wang, Guoping] Peking Univ, Beijing 100871, Peoples R China.
   [Chang, Yuan; Zhang, Congyi; Chen, Yisong; Wang, Guoping] Peking Univ, Beijiing Engn Technol Res Ctr Virtual Simulat & V, Beijing 100871, Peoples R China.
   [Zhang, Congyi] Univ Hong Kong, Hong Kong, Peoples R China.
C3 Peking University; Peking University; University of Hong Kong
RP Wang, GP (corresponding author), Peking Univ, Beijing 100871, Peoples R China.; Wang, GP (corresponding author), Peking Univ, Beijiing Engn Technol Res Ctr Virtual Simulat & V, Beijing 100871, Peoples R China.
EM changyuan@pku.edu.cn; chenyisong@pku.edu.cn; cyzh@hku.hk; wgp@pku.edu.cn
RI DAI, Jinjia/KCL-5110-2024; yu, zhang/JWO-7724-2024; CHEN,
   MINGWEI/KHT-6744-2024; Zhang, Lanyue/JNS-8209-2023; meng,
   meng/KHW-8303-2024; Li, Ren/JVZ-9153-2024; wang, guoping/KQU-3394-2024;
   Chang, Yuan/KHW-2623-2024
OI Li, Ren/0000-0002-2579-2580; Zhang, Congyi/0000-0002-4259-2863
FU National Key Technology Research and Development Program of China
   [2017YFB1002601]; PKU-Baidu Fund [2019BD007]; National Natural Science
   Foundation of China (NSFC) [61632003]
FX This project was supported by the National Key Technology Research and
   Development Program of China (No. 2017YFB1002601), PKU-Baidu Fund (No.
   2019BD007), and National Natural Science Foundation of China (NSFC) (No.
   61632003).
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2006, BMVC
   Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2
   Bao LC, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2359374
   Bao WB, 2019, PROC CVPR IEEE, P3698, DOI 10.1109/CVPR.2019.00382
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Besse F, 2014, INT J COMPUT VISION, V110, P2, DOI 10.1007/s11263-013-0653-9
   Bleyer M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.14
   BURT PJ, 1983, ACM T GRAPHIC, V2, P217, DOI 10.1145/245.247
   Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44
   Chaurasia G, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487238
   Chen J, 2019, IEEE T CIRC SYST VID, V29, P1350, DOI 10.1109/TCSVT.2018.2805101
   Chen J, 2018, IEEE T CIRC SYST VID, V28, P664, DOI 10.1109/TCSVT.2016.2615324
   Chen QF, 2016, PROC CVPR IEEE, P4706, DOI 10.1109/CVPR.2016.509
   Chen ZY, 2013, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR.2013.316
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Felzenszwalb PR, 2004, PROC CVPR IEEE, P261
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Hedman P, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201384
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hornácek M, 2014, LECT NOTES COMPUT SC, V8691, P220, DOI 10.1007/978-3-319-10578-9_15
   Hu YL, 2017, PROC CVPR IEEE, P4791, DOI 10.1109/CVPR.2017.509
   Huang PH, 2018, PROC CVPR IEEE, P2821, DOI 10.1109/CVPR.2018.00298
   Hui TW, 2021, IEEE T PATTERN ANAL, V43, P2555, DOI 10.1109/TPAMI.2020.2976928
   Hur J., 2020, Model Hum Motion Hum Percept Robot Des, P119, DOI 10.1007/978-3-030-46732-67
   Jeong SG, 2013, IEEE T IMAGE PROCESS, V22, P4497, DOI 10.1109/TIP.2013.2274731
   Jiang HZ, 2018, PROC CVPR IEEE, P9000, DOI 10.1109/CVPR.2018.00938
   Kaviani HR, 2015, IEEE IMAGE PROC, P3387, DOI 10.1109/ICIP.2015.7351432
   Kothapa R., 2011, Max-product particle belief propagation
   Li Y, 2015, IEEE I CONF COMP VIS, P4006, DOI 10.1109/ICCV.2015.456
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Long GC, 2016, LECT NOTES COMPUT SC, V9910, P434, DOI 10.1007/978-3-319-46466-4_26
   Mahajan D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531348
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Menze M, 2015, LECT NOTES COMPUT SC, V9358, P16, DOI 10.1007/978-3-319-24947-6_2
   Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925
   Meyer S, 2015, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2015.7298747
   Mildenhall Ben, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P405, DOI 10.1007/978-3-030-58452-8_24
   Nie YW, 2017, IEEE T VIS COMPUT GR, V23, P2328, DOI 10.1109/TVCG.2016.2618878
   Niklaus S, 2017, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2017.37
   Niklaus S, 2017, PROC CVPR IEEE, P2270, DOI 10.1109/CVPR.2017.244
   Penner E, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130855
   Revaud J, 2015, PROC CVPR IEEE, P1164, DOI 10.1109/CVPR.2015.7298720
   Stich T, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/1870076.1870079
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Taniai T, 2014, PROC CVPR IEEE, P1613, DOI 10.1109/CVPR.2014.209
   Wang S, 2019, INT CONF ACOUST SPEE, P2297, DOI 10.1109/ICASSP.2019.8683037
   Xu ZX, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323007
   Yang GS, 2019, ADV NEUR IN, V32
   Yu A, 2021, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR46437.2021.00455
   Zhao SY, 2020, PROC CVPR IEEE, P6277, DOI 10.1109/CVPR42600.2020.00631
   Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18
NR 53
TC 0
Z9 1
U1 1
U2 9
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2022
VL 8
IS 1
BP 119
EP 133
DI 10.1007/s41095-021-0225-9
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WO6VR
UT WOS:000712590200008
OA gold
DA 2024-07-18
ER

PT J
AU Zeng, R
   Wen, YH
   Zhao, W
   Liu, YJ
AF Zeng, Rui
   Wen, Yuhui
   Zhao, Wang
   Liu, Yong-Jin
TI View planning in robot active vision: A survey of systems, algorithms,
   and applications
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review
DE robotic; view planning; active vision; next-best view; sensor planning
ID 3-DIMENSIONAL OBJECT RECONSTRUCTION; SIMULTANEOUS LOCALIZATION; SENSOR;
   RECOGNITION
AB Rapid development of artificial intelligence motivates researchers to expand the capabilities of intelligent and autonomous robots. In many robotic applications, robots are required to make planning decisions based on perceptual information to achieve diverse goals in an efficient and effective way. The planning problem has been investigated in active robot vision, in which a robot analyzes its environment and its own state in order to move sensors to obtain more useful information under certain constraints. View planning, which aims to find the best view sequence for a sensor, is one of the most challenging issues in active robot vision. The quality and efficiency of view planning are critical for many robot systems and are influenced by the nature of their tasks, hardware conditions, scanning states, and planning strategies. In this paper, we first summarize some basic concepts of active robot vision, and then review representative work on systems, algorithms and applications from four perspectives: object reconstruction, scene reconstruction, object recognition, and pose estimation. Finally, some potential directions are outlined for future work.
C1 [Zeng, Rui; Wen, Yuhui; Zhao, Wang; Liu, Yong-Jin] Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing, Peoples R China.
C3 Tsinghua University
RP Liu, YJ (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing, Peoples R China.
EM zengr17@mails.tsinghua.edu.cn; zhao-w19@mails.tsinghua.edu.cn;
   liuyongjin@tsinghua.edu.cn
RI Liu, Yong/GWQ-6163-2022; Wen, Yu-Hui/JCO-0775-2023
OI Wen, Yu-Hui/0000-0001-6195-9782
FU Science and Technology Department of Jiangsu Province, China
FX The authors would like to thank Dr. Shihao Wu of Shenzhen VisuCA Key
   Laboratory/SIAT for providing Fig. 8, Dr. Vasquez-Gomez of the Instituto
   Nacional de AstrofA.sica ASptica y ElectrAsnica (INAOE) for providing
   Fig. 3, Prof. Kai Xu of the National University of Defense Technology
   and AICFVE Beijing Film Academy for providing Fig. 12, Dr. Xi Xia of the
   University of Science and Technology of China for providing Fig. 11, and
   Dr. Delmerico of the Robotics and Perception Group, University of Zurich
   for providing Fig. 9. This work was partially supported by a grant from
   the Science and Technology Department of Jiangsu Province, China.
CR [Anonymous], 1987, ART GALLERY THEOREMS
   [Anonymous], 2006, P 4 EUR S GEOM PROC
   ARUN KS, 1987, IEEE T PATTERN ANAL, V9, P699, DOI 10.1109/TPAMI.1987.4767965
   Bailey T, 2006, IEEE ROBOT AUTOM MAG, V13, P108, DOI 10.1109/MRA.2006.1678144
   Banta JE, 2000, IEEE T SYST MAN CY A, V30, P589, DOI 10.1109/3468.867866
   Bay H., 2008, COMPUT VIS IMAGE UND, V10, P346, DOI DOI 10.1016/j.cviu.2007.09.014
   Bektas T, 2006, OMEGA-INT J MANAGE S, V34, P209, DOI 10.1016/j.omega.2004.10.004
   Bircher A, 2016, IEEE INT CONF ROBOT, P1462, DOI 10.1109/ICRA.2016.7487281
   Blaer P, 2003, IEEE INT CONF ROBOT, P1582, DOI 10.1109/ROBOT.2003.1241820
   Blaer PS, 2007, 2007 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-9, P423
   Breiman L., 2001, Mach. Learn., V45, P5
   Browatzki B, 2012, IEEE INT CONF ROBOT, P2021, DOI 10.1109/ICRA.2012.6225218
   Chang A X, 2015, COMPUTER SCI, V1512, P3
   Chen SY, 2011, INT J ROBOT RES, V30, P1343, DOI 10.1177/0278364911410755
   Coates A., 2011, P 14 INT C ART INT S, V15, P215
   Connolly C., 1985, PROC IEEE INT C ROBO, V2, P432
   Corsini M, 2012, IEEE T VIS COMPUT GR, V18, P914, DOI 10.1109/TVCG.2012.34
   Cui JD, 2019, IEEE INT CONF ROBOT, P8769, DOI [10.1109/icra.2019.8794423, 10.1109/ICRA.2019.8794423]
   Dai A, 2018, PROC CVPR IEEE, P4578, DOI 10.1109/CVPR.2018.00481
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Delmerico J, 2018, AUTON ROBOT, V42, P197, DOI 10.1007/s10514-017-9634-0
   Diankov R., 2008, ROBOTICS
   Dickinson SJ, 1997, COMPUT VIS IMAGE UND, V67, P239, DOI 10.1006/cviu.1997.0532
   Dinur I, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P624, DOI 10.1145/2591796.2591884
   Doumanoglou A, 2016, PROC CVPR IEEE, P3583, DOI 10.1109/CVPR.2016.390
   Doumanoglou A, 2014, LECT NOTES COMPUT SC, V8693, P644, DOI 10.1007/978-3-319-10602-1_42
   Durrant-Whyte H, 2006, IEEE ROBOT AUTOM MAG, V13, P99, DOI 10.1109/MRA.2006.1638022
   Eidenberger R, 2010, IEEE INT C INT ROBOT, P1036, DOI 10.1109/IROS.2010.5651927
   Feige U, 1998, J ACM, V45, P634, DOI 10.1145/285055.285059
   Fox D, 1999, SIXTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-99)/ELEVENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE (IAAI-99), P343
   Gall J, 2011, IEEE T PATTERN ANAL, V33, P2188, DOI 10.1109/TPAMI.2011.70
   González-Baños H, 2000, ROBOTICS RESEARCH, P345
   Han XG, 2019, PROC CVPR IEEE, P234, DOI 10.1109/CVPR.2019.00032
   Hinterstoisser S, 2011, IEEE I CONF COMP VIS, P858, DOI 10.1109/ICCV.2011.6126326
   Hornung A, 2013, AUTON ROBOT, V34, P189, DOI 10.1007/s10514-012-9321-0
   HUTCHINSON SA, 1989, IEEE T ROBOTIC AUTOM, V5, P765, DOI 10.1109/70.88098
   Johns E, 2016, PROC CVPR IEEE, P3813, DOI 10.1109/CVPR.2016.414
   Johns E, 2015, PROC CVPR IEEE, P2616, DOI 10.1109/CVPR.2015.7298877
   Kaba MD, 2017, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2017.541
   Karaman S, 2011, INT J ROBOT RES, V30, P846, DOI 10.1177/0278364911406761
   Keselman L, 2017, IEEE COMPUT SOC CONF, P1267, DOI 10.1109/CVPRW.2017.167
   Khalfaoui S, 2013, COMPUT IND, V64, P1152, DOI 10.1016/j.compind.2013.04.005
   Kouskouridas R, 2014, AUTON ROBOT, V37, P191, DOI 10.1007/s10514-014-9388-x
   Krainin M, 2011, IEEE INT CONF ROBOT
   Kriegel S, 2011, IEEE INT CONF ROBOT
   Kriegel S, 2015, J REAL-TIME IMAGE PR, V10, P611, DOI 10.1007/s11554-013-0386-6
   Kriegel S, 2013, IEEE INT C INT ROBOT, P2384, DOI 10.1109/IROS.2013.6696691
   Kriegel S, 2012, IEEE INT C INT ROBOT, P2850, DOI 10.1109/IROS.2012.6385624
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LaValle SM, 2001, ALGORITHMIC AND COMPUTATIONAL ROBOTICS: NEW DIRECTIONS, P293
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu YJ, 2013, IEEE T VIS COMPUT GR, V19, P1700, DOI 10.1109/TVCG.2013.74
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Makris S, 2016, CIRP ANN-MANUF TECHN, V65, P61, DOI 10.1016/j.cirp.2016.04.038
   MARR D, 1979, PROC R SOC SER B-BIO, V204, P301, DOI 10.1098/rspb.1979.0029
   Martinez M, 2010, IEEE INT CONF ROBOT, P2043, DOI 10.1109/ROBOT.2010.5509801
   Massios N. A., 1998, BMVC 98. Proceedings of the Ninth British Machine Vision Conference, P780
   Mendoza M, 2020, PATTERN RECOGN LETT, V133, P224, DOI 10.1016/j.patrec.2020.02.024
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Monica R, 2018, IEEE ROBOT AUTOM LET, V3, P3324, DOI 10.1109/LRA.2018.2852778
   Nuchter A, 2003, P 11 INT C ADV ROB, V78
   Palomeras N, 2019, IEEE ROBOT AUTOM LET, V4, P1619, DOI 10.1109/LRA.2019.2896759
   Pito R., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P941, DOI 10.1109/ICPR.1996.546162
   Pito R, 1999, IEEE T PATTERN ANAL, V21, P1016, DOI 10.1109/34.799908
   Richtsfeld A, 2012, IEEE INT C INT ROBOT, P4791, DOI 10.1109/IROS.2012.6385661
   Roy SD, 2004, PATTERN RECOGN, V37, P429, DOI 10.1016/j.patcog.2003.01.002
   Scott WR, 2009, MACH VISION APPL, V20, P47, DOI 10.1007/s00138-007-0110-2
   Scott WR, 2003, ACM COMPUT SURV, V35, P64, DOI 10.1145/641865.641868
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Sock J, 2017, IEEE INT CONF COMP V, P2228, DOI 10.1109/ICCVW.2017.260
   Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28
   Tang J, 2012, IEEE INT CONF ROBOT, P3467, DOI 10.1109/ICRA.2012.6224891
   TARABANIS K, 1991, 1991 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-3, P76, DOI 10.1109/ROBOT.1991.131556
   TARABANIS KA, 1995, IEEE T ROBOTIC AUTOM, V11, P86, DOI 10.1109/70.345940
   TARBOX GH, 1995, COMPUT VIS IMAGE UND, V61, P84, DOI 10.1006/cviu.1995.1007
   Tejani A, 2014, LECT NOTES COMPUT SC, V8694, P462, DOI 10.1007/978-3-319-10599-4_30
   Vasquez-Gomez JI, 2017, AUTON ROBOT, V41, P89, DOI 10.1007/s10514-015-9531-3
   Vasquez-Gomez JI, 2014, INT J ADV ROBOT SYST, V11, DOI 10.5772/58759
   Wong L. M., 1999, Proceedings 1999 IEEE International Symposium on Computational Intelligence in Robotics and Automation. CIRA'99 (Cat. No.99EX375), P306, DOI 10.1109/CIRA.1999.810066
   Wu CM, 2019, IEEE ROBOT AUTOM LET, V4, P3113, DOI 10.1109/LRA.2019.2924125
   Wu KZ, 2015, IEEE INT CONF ROBOT, P4230, DOI 10.1109/ICRA.2015.7139782
   Ye YM, 1999, COMPUT VIS IMAGE UND, V73, P145, DOI 10.1006/cviu.1998.0736
   Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088
   Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24
   Zheng LT, 2019, COMPUT GRAPH FORUM, V38, P103, DOI 10.1111/cgf.13820
NR 85
TC 49
Z9 50
U1 6
U2 85
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2020
VL 6
IS 3
BP 225
EP 245
DI 10.1007/s41095-020-0179-3
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FH
UT WOS:000648691900001
OA gold
DA 2024-07-18
ER

PT J
AU Xi, WJ
   Chen, XJ
AF Xi, Weijie
   Chen, Xuejin
TI Reconstructing piecewise planar scenes with multi-view regularization
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE scene modeling; multi-view; regularization; neural network
ID STEREO
AB Reconstruction of man-made scenes from multi-view images is an important problem in computer vision and computer graphics. Observing that man-made scenes are usually composed of planar surfaces, we encode plane shape prior in reconstructing man-made scenes. Recent approaches for single-view reconstruction employ multi-branch neural networks to simultaneously segment planes and recover 3D plane parameters. However, the scale of available annotated data heavily limits the generalizability and accuracy of these supervised methods. In this paper, we propose multi-view regularization to enhance the capability of piecewise planar reconstruction during the training phase, without demanding extra annotated data. Our multi-view regularization enables the consistency among multiple views by making the feature embedding more robust against view change and lighting variations. Thus, the neural network trained by multi-view regularization performs better on a wide range of views and lightings in the test phase. Based on more consistent prediction results, we merge the recovered models from multiple views to reconstruct scenes. Our approach achieves state-of-the-art reconstruction performance compared to previous approaches on the public ScanNet dataset.
C1 [Xi, Weijie; Chen, Xuejin] Univ Sci & Technol China, Hefei 230026, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Chen, XJ (corresponding author), Univ Sci & Technol China, Hefei 230026, Peoples R China.
EM xiwj@mail.ustc.edu.cn; xjchen99@ustc.edu.cn
FU National Key R&D Program of China [2017YFB1002202]; National Natural
   Science Foundation of China (NSFC) [61632006]; Fundamental Research
   Funds for the Central Universities [WK3490000003, WK2100100030]
FX This work was supported by the National Key R&D Program of China under
   Grant 2017YFB1002202, the National Natural Science Foundation of China
   (NSFC) under Grant 61632006, as well as the Fundamental Research Funds
   for the Central Universities under Grants WK3490000003 and WK2100100030.
CR [Anonymous], 2015, ACM T GRAPHICS, V34, P4
   [Anonymous], 2015, ACM T GRAPHICS, V34, P5
   [Anonymous], 2017, ACM T GRAPHIC, V36, P4
   Barinova O, 2008, LECT NOTES COMPUT SC, V5303, P100, DOI 10.1007/978-3-540-88688-4_8
   Chen R, 2019, IEEE I CONF COMP VIS, P1538, DOI 10.1109/ICCV.2019.00162
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   De Brabandere B, 2017, IEEE COMPUT SOC CONF, P478, DOI 10.1109/CVPRW.2017.66
   Delage E, 2007, SPRINGER TRAC ADV RO, V28, P305
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Gallup David, 2007, CVPR
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Jensen R, 2014, PROC CVPR IEEE, P406, DOI 10.1109/CVPR.2014.59
   Liu C, 2019, PROC CVPR IEEE, P4445, DOI 10.1109/CVPR.2019.00458
   Liu C, 2018, PROC CVPR IEEE, P2579, DOI 10.1109/CVPR.2018.00273
   Luo KY, 2019, IEEE I CONF COMP VIS, P10451, DOI 10.1109/ICCV.2019.01055
   Paszke A, 2017, NIPS 2017 WORKSHOP
   Saxena A., 2005, ADV NEURAL INFORM PR, V18, P1
   Schönberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Yang FT, 2018, LECT NOTES COMPUT SC, V11214, P87, DOI 10.1007/978-3-030-01249-6_6
   Yang RG, 2003, PROC CVPR IEEE, P211, DOI 10.1109/ISCS.2003.1239980
   Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47
   Yu ZH, 2019, PROC CVPR IEEE, P1029, DOI 10.1109/CVPR.2019.00112
   Zhang T., 2004, PROC ICML, DOI 10.1145/1015330.-1015332
NR 24
TC 6
Z9 7
U1 0
U2 1
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2019
VL 5
IS 4
BP 337
EP 345
DI 10.1007/s41095-019-0159-7
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UO
UT WOS:000648691000002
OA gold
DA 2024-07-18
ER

PT J
AU Li, X
   McMains, S
AF Li, Xiang
   McMains, Sara
TI A Voronoi diagram approach for detecting defects in 3D printed
   fiber-reinforced polymers from microscope images
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE 3D printing (3DP); microscope image processing; fiber-reinforced polymer
   (FRP); Voronoi diagrams; alpha-shapes; resin-rich areas
ID CIRCLE SET; POINT SET; IMPLEMENTATION; DAMAGE
AB Fiber-reinforced polymer (FRP) composites are increasingly popular due to their superior strength to weight ratio. In contrast to significant recent advances in automating the FRP manufacturing process via 3D printing, quality inspection and defect detection remain largely manual and inefficient. In this paper, we propose a new approach to automatically detect, from microscope images, one of the major defects in 3D printed FRP parts: fiber-deficient areas (or equivalently, resin-rich areas). From cross-sectional microscope images, we detect the locations and sizes of fibers, construct their Voronoi diagram, and employ cx-shape theory to determine fiber-deficient areas. Our Voronoi diagram and a-shape construction algorithms are specialized to exploit typical characteristics of 3D printed FRP parts, giving significant efficiency gains. Our algorithms robustly handle real-world inputs containing hundreds of thousands of fiber cross-sections, whether in general or non-general position.
C1 [Li, Xiang; McMains, Sara] Univ Calif Berkeley, Berkeley, CA 94720 USA.
C3 University of California System; University of California Berkeley
RP McMains, S (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM xli@berkeley.edu; mcmains@berkeley.edu
RI Li, Xiang/ITU-1445-2023
OI Li, Xiang/0000-0001-5936-8457
CR CANTWELL WJ, 1992, J STRAIN ANAL ENG, V27, P29, DOI 10.1243/03093247V271029
   Choi S, 2020, ADDIT MANUF, V35, DOI 10.1016/j.addma.2020.101254
   Der Klift Van., 2016, OPEN J COMPOSITE MAT, V6, P18, DOI DOI 10.4236/OJCM.2016.61003
   EDELSBRUNNER H, 1994, ACM T GRAPHIC, V13, P43, DOI 10.1145/174462.156635
   EDELSBRUNNER H, 1983, IEEE T INFORM THEORY, V29, P551, DOI 10.1109/TIT.1983.1056714
   Emiris IZ, 2008, COMPUT AIDED DESIGN, V40, P691, DOI 10.1016/j.cad.2008.05.001
   Emiris IZ, 2013, COMPUT AIDED GEOM D, V30, P760, DOI 10.1016/j.cagd.2013.06.005
   Ghayoor H, 2019, COMPOS PART A-APPL S, V117, P125, DOI 10.1016/j.compositesa.2018.11.016
   Gommer F, 2016, COMPOS PART A-APPL S, V87, P131, DOI 10.1016/j.compositesa.2016.04.019
   Harish S, 2009, MATER CHARACT, V60, P44, DOI 10.1016/j.matchar.2008.07.001
   Hayes B., 2010, Optical Microscopy of Fiber-Reinforced Composites
   Hu Z, 2017, COMPUTER AIDED DESIG, V14, P572, DOI DOI 10.1080/16864360.2016.1273576
   Jin L, 2006, COMPUT AIDED DESIGN, V38, P260, DOI 10.1016/j.cad.2005.11.001
   Kim DS, 2010, COMPUT AIDED DESIGN, V42, P911, DOI 10.1016/j.cad.2010.06.004
   Kim D, 2006, COMPUT AIDED DESIGN, V38, P417, DOI 10.1016/j.cad.2005.11.007
   Kim DS, 2001, COMPUT AIDED GEOM D, V18, P563, DOI 10.1016/S0167-8396(01)00051-6
   Kim DS, 2001, COMPUT AIDED GEOM D, V18, P541, DOI 10.1016/S0167-8396(01)00050-4
   Kim JK, 2014, J COMPUT DES ENG, V1, P79, DOI 10.7315/JCDE.2014.008
   Lee M, 2018, COMPUT AIDED DESIGN, V101, P23, DOI 10.1016/j.cad.2018.03.007
   Lee M, 2016, ACM T MATH SOFTWARE, V43, DOI 10.1145/2939366
   Li X, 2021, IEEE IMAGE PROC, P2623, DOI 10.1109/ICIP42928.2021.9506571
   Li X, 2019, COMPUT GRAPH-UK, V82, P332, DOI 10.1016/j.cag.2019.06.007
   OHYA T, 1984, J OPER RES SOC JPN, V27, P306, DOI 10.15807/jorsj.27.306
   Parandoush P, 2019, ADV ENG MATER, V21, DOI 10.1002/adem.201800622
   Park CH, 2016, NATURE, V532, P480, DOI 10.1038/nature17634
   Schneider CA, 2012, NAT METHODS, V9, P671, DOI 10.1038/nmeth.2089
   Shewchuk J.R., 1996, WORKSHOP APPL COMPUT, V1148, P203
   SUGIHARA K, 1993, CVGIP-GRAPH MODEL IM, V55, P522, DOI 10.1006/cgip.1993.1039
   Sugiyama K, 2020, COMPOS SCI TECHNOL, V186, DOI 10.1016/j.compscitech.2019.107905
   Yang CC, 2017, RAPID PROTOTYPING J, V23, P209, DOI 10.1108/RPJ-08-2015-0098
   YANG HC, 1994, POLYM COMPOSITE, V15, P46, DOI 10.1002/pc.750150108
   Zhang J, 2009, J COMPOS MATER, V43, P1939, DOI 10.1177/0021998308078686
   Zheng XY, 2007, PHYS REV E, V75, DOI 10.1103/PhysRevE.75.061709
NR 33
TC 2
Z9 2
U1 0
U2 19
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2023
VL 9
IS 1
BP 41
EP 56
DI 10.1007/s41095-021-0265-1
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5K7GQ
UT WOS:000869891100003
OA gold
DA 2024-07-18
ER

PT J
AU Ren, L
   Song, Y
AF Ren, Lei
   Song, Ying
TI AOGAN: A generative adversarial network for screen space ambient
   occlusion
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE ambient occlusion (AO); attention mechanism; generative adversarial
   network (GAN); perceptual loss
AB Ambient occlusion (AO) is a widely-used real-time rendering technique which estimates light intensity on visible scene surfaces. Recently, a number of learning-based AO approaches have been proposed, which bring a new angle to solving screen space shading via a unified learning framework with competitive quality and speed. However, most such methods have high error for complex scenes or tend to ignore details. We propose an end-to-end generative adversarial network for the production of realistic AO, and explore the importance of perceptual loss in the generative model to AO accuracy. An attention mechanism is also described to improve the accuracy of details, whose effectiveness is demonstrated on a wide variety of scenes.
C1 [Ren, Lei; Song, Ying] Zhejiang Sci Tech Univ, Hangzhou 310018, Peoples R China.
   [Song, Ying] 2011 Collaborat Innovat Ctr Garment Personal Cust, Ningbo, Peoples R China.
   [Song, Ying] Minist Culture & Tourism, Key Lab Silk & Culture Heritage & Prod Design Dig, Hangzhou, Peoples R China.
C3 Zhejiang Sci-Tech University
RP Song, Y (corresponding author), Zhejiang Sci Tech Univ, Hangzhou 310018, Peoples R China.; Song, Y (corresponding author), 2011 Collaborat Innovat Ctr Garment Personal Cust, Ningbo, Peoples R China.; Song, Y (corresponding author), Minist Culture & Tourism, Key Lab Silk & Culture Heritage & Prod Design Dig, Hangzhou, Peoples R China.
EM ysong@zstu.edu.cn
FU National Natural Science Foundation of China [61602416]; Shaoxing
   Science and Technology Bureau Key Project [2020B41006]; Opening Fund of
   Key Laboratory of Silk Culture Heritage and Product Design Digital
   Technology [2020WLB10]
FX The authors would like to thank the anonymous reviewers for their
   helpful suggestions and comments. Ying Song was supported by the
   National Natural Science Foundation of China (No. 61602416), Shaoxing
   Science and Technology Bureau Key Project (No. 2020B41006), and the
   Opening Fund (No. 2020WLB10) of the Key Laboratory of Silk Culture
   Heritage and Product Design Digital Technology.
CR [Anonymous], 2011, ACM T INTEL SYST TEC, V2
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Avcibas I, 2002, J ELECTRON IMAGING, V11, P206, DOI 10.1117/1.1455011
   Bavoil L., SCREEN SPACE AMBIENT
   Bavoil L., 2008, ACM SIGGRAPH 2008 TA, DOI DOI 10.1145/1401032.1401061
   Blau Y, 2018, PROC CVPR IEEE, P6228, DOI 10.1109/CVPR.2018.00652
   Chen GY, 2018, LECT NOTES COMPUT SC, V11213, P3, DOI 10.1007/978-3-030-01240-3_1
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dosovitskiy A., 2016, Advances in Neural Information Processing Systems, P658
   Erra U., 2017, P EUR SHORT PAP, P13, DOI DOI 10.2312/EGSH.20171003
   Filion D., 2008, SIGGRAPH 08, P133, DOI DOI 10.1145/1404435.1404441
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Holden D., 2016, P SIGGRAPH AS TECH B
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Jimenez J., 2016, P SIGGRAPH 2016 COUR
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lin Zhouhan, 2017, A structured self-attentive sentence embedding
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   McGuire M., 2011, P ACM SIGGRAPH S HIG, P25, DOI DOI 10.1145/2018323.2018327
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Mittring M., 2007, ACM SIGGRAPH 2007 CO, P97, DOI [DOI 10.1145/1281500.1281671, 10.1145/1281500.1281671]
   Nalbach O, 2017, COMPUT GRAPH FORUM, V36, P65, DOI 10.1111/cgf.13225
   Oktay O., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1804.03999
   Park E, 2017, PROC CVPR IEEE, P702, DOI 10.1109/CVPR.2017.82
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Radford A., 2015, ARXIV
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shanmugam P, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P73
   Shen T, 2018, AAAI CONF ARTIF INTE, P5446
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tewari A, 2020, COMPUT GRAPH FORUM, V39, P701, DOI 10.1111/cgf.14022
   Vardis K., 2013, P ACM SIGGRAPH S INT, P111, DOI DOI 10.1145/2448196.2448214
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Zhang DJ, 2020, IEEE ACCESS, V8, P64434, DOI 10.1109/ACCESS.2020.2984771
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]
NR 40
TC 2
Z9 2
U1 0
U2 6
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2022
VL 8
IS 3
BP 483
EP 494
DI 10.1007/s41095-021-0248-2
EA JAN 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0Q6DG
UT WOS:000740203600002
OA gold
DA 2024-07-18
ER

PT J
AU Kadosh, M
   Moses, Y
   Shamir, A
AF Kadosh, Moti
   Moses, Yael
   Shamir, Ariel
TI On the role of geometry in geo-localization
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE geo-localization; geometry; CNN-based solutions; synthetic lean images
AB Consider the geo-localization task of finding the pose of a camera in a large 3D scene from a single image. Most existing CNN-based methods use as input textured images. We aim to experimentally explore whether texture and correlation between nearby images are necessary in a CNN-based solution for the geo-localization task. To do so, we consider lean images, textureless projections of a simple 3D model of a city. They only contain information related to the geometry of the scene viewed (edges, faces, and relative depth). The main contributions of this paper are: (i) to demonstrate the ability of CNNs to recover camera pose using lean images; and (ii) to provide insight into the role of geometry in the CNN learning process.
C1 [Kadosh, Moti] Tel Aviv Univ, Dept Elect Engn, Tel Aviv, Israel.
   [Moses, Yael; Shamir, Ariel] Interdisciplinary Ctr Herzliya, Efi Arazi Sch Comp Sci, Herzliyya, Israel.
C3 Tel Aviv University; Reichman University
RP Kadosh, M (corresponding author), Tel Aviv Univ, Dept Elect Engn, Tel Aviv, Israel.
EM moti.kadosh84@gmail.com; yael@idc.ac.il; arik@idc.ac.il
CR [Anonymous], 2004, P BRIT MACH C
   Baatz G, 2012, LECT NOTES COMPUT SC, V7573, P517, DOI 10.1007/978-3-642-33709-3_37
   Bansal M, 2014, PROC CVPR IEEE, P3978, DOI 10.1109/CVPR.2014.508
   Bergamo A, 2013, PROC CVPR IEEE, P763, DOI 10.1109/CVPR.2013.104
   Berlin Partner fur Wirtschaft und Technologie GmbH, 2016, BERLIN 3D CITY MODEL
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Irschara A, 2009, PROC CVPR IEEE, P2591, DOI 10.1109/CVPRW.2009.5206587
   Kendall A, 2017, PROC CVPR IEEE, P6555, DOI 10.1109/CVPR.2017.694
   Kendall A, 2016, IEEE INT CONF ROBOT, P4762, DOI 10.1109/ICRA.2016.7487679
   Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336
   Li YP, 2010, LECT NOTES COMPUT SC, V6312, P791
   Liu Y, 2008, PROC CVPR IEEE, P2008
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matei BC, 2013, IEEE WORK APP COMP, P413, DOI 10.1109/WACV.2013.6475048
   Melekhov I, 2017, IEEE INT CONF COMP V, P870, DOI 10.1109/ICCVW.2017.107
   Nister David, 2006, CVPR
   Nowak E, 2007, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2007.382969
   OpenStreetMap Wiki contributors, 2018, OSM 3D ORG OPENSTREE
   Piasco N, 2018, PATTERN RECOGN, V74, P90, DOI 10.1016/j.patcog.2017.09.013
   Ramalingam S, 2011, IEEE INT CONF ROBOT
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sattler T, 2017, PROC CVPR IEEE, P6175, DOI 10.1109/CVPR.2017.654
   Sattler T, 2011, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2011.6126302
   Se S, 2002, INT J ROBOT RES, V21, P735, DOI 10.1177/027836402761412467
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Svärm L, 2017, IEEE T PATTERN ANAL, V39, P1455, DOI 10.1109/TPAMI.2016.2598331
   Svärm L, 2014, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2014.75
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Walch F, 2017, IEEE I CONF COMP VIS, P627, DOI 10.1109/ICCV.2017.75
   Zeng HQ, 2017, PROC INT CONF RECON
   Zhang W, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P33, DOI 10.1109/3dpvt.2006.80
NR 32
TC 0
Z9 0
U1 0
U2 7
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2021
VL 7
IS 1
BP 103
EP 113
DI 10.1007/s41095-020-0196-2
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FQ
UT WOS:000648692800006
OA gold, Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, S
   Gang, RP
   Li, CH
   Song, RX
AF Liu, Shuai
   Gang, Ruipeng
   Li, Chenghua
   Song, Ruixia
TI Adaptive deep residual network for single image super-resolution
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE single image super-resolution (SISR); adaptive deep residual network;
   deep learning
AB In recent years, deep learning has achieved great success in the field of image processing. In the single image super-resolution (SISR) task, the convolutional neural network (CNN) extracts the features of the image through deeper layers, and has achieved impressive results. In this paper, we propose a single image super-resolution model based on Adaptive Deep Residual named as ADR-SR, which uses the Input Output Same Size (IOSS) structure, and releases the dependence of upsampling layers compared with the existing SR methods. Specifically, the key element of our model is the Adaptive Residual Block (ARB), which replaces the commonly used constant factor with an adaptive residual factor. The experiments prove the effectiveness of our ADR-SR model, which can not only reconstruct images with better visual effects, but also get better objective performances.
C1 [Liu, Shuai; Gang, Ruipeng; Song, Ruixia] North China Univ Technol, Beijing 100043, Peoples R China.
   [Li, Chenghua] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
C3 North China University of Technology; Chinese Academy of Sciences;
   Institute of Automation, CAS
RP Li, CH (corresponding author), Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
EM 18601200232@163.com; gangrp909@sina.com; lichenghua2014@ia.ac.cn;
   songrx@ncut.edu.cn
FU National Natural Science Foundation of China [61571046]; National Key
   R&D Program of China [2017YFF0209806]
FX This work was supported in part by National Natural Science Foundation
   of China (No. 61571046) and National Key R&D Program of China (No.
   2017YFF0209806).
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Barnes Connelly, 2017, [Computational Visual Media, 计算可视媒体], V3, P3
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Choi JS, 2017, IEEE T IMAGE PROCESS, V26, P1300, DOI 10.1109/TIP.2017.2651411
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   King DB, 2015, ACS SYM SER, V1214, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai W-S, 2017, PROC CVPR IEEE, P624, DOI DOI 10.1109/CVPR.2017.618
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li DP, 2015, PROC CVPR IEEE, P213, DOI 10.1109/CVPR.2015.7298617
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu H, 2019, INFORM SCIENCES, V473, P44, DOI 10.1016/j.ins.2018.09.018
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma H, 2019, MATRIX IN MATRIX NEU
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Su Binghua, 2003, Acta Photonica Sinica, V32, P502
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Vaswani A, 2017, ADV NEUR IN, V30
   Yang JC, 2012, IEEE T IMAGE PROCESS, V21, P3467, DOI 10.1109/TIP.2012.2192127
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang FL, 2017, IEEE T VIS COMPUT GR, V23, P1561, DOI 10.1109/TVCG.2016.2532329
NR 31
TC 8
Z9 8
U1 6
U2 17
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2019
VL 5
IS 4
BP 391
EP 401
DI 10.1007/s41095-019-0158-8
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UO
UT WOS:000648691000006
OA gold
DA 2024-07-18
ER

PT J
AU Chen, MH
   Xu, F
   Lu, L
AF Chen, Minghai
   Xu, Fan
   Lu, Lin
TI Manufacturable pattern collage along a boundary
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE pattern collage; interactive design; digital fabrication
AB Recent years have shown rapid development of digital fabrication techniques, making manufacturing individual models reachable for ordinary users. Thus, tools for designing customized objects in a user-friendly way are in high demand. In this paper, we tackle the problem of generating a collage of patterns along a given boundary, aimed at digital fabrication. We represent the packing space by a pipe-like closed shape along the boundary and use ellipses as packing elements for computing an initial layout of the patterns. Then we search for the best matching pattern for each ellipse and construct the initial pattern collage in an automatic manner. To facilitate editing the collage, we provide interactive operations which allow the user to adjust the layout at the coarse level. The patterns are fine-tuned based on a spring-mass system after each interaction step. After this interactive process, the collage result is further optimized to enforce connectivity. Finally, we perform structural analysis on the collage and enhance its stability, so that the result can be fabricated. To demonstrate the effectiveness of our method, we show results fabricated by 3D printing and laser cutting.
C1 [Chen, Minghai; Xu, Fan; Lu, Lin] Shandong Univ, Sch Comp Sci & Technol, Qingdao, Peoples R China.
C3 Shandong University
RP Lu, L (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Qingdao, Peoples R China.
EM lulin.linda@gmail.com
RI Lu, Lin/HDN-2655-2022
OI Lu, Lin/0000-0001-5881-892X
FU National Natural Science Foundation of China (NSFC) [61572291]; Young
   Scholars Program of Shandong University (YSPSDU); Open Project Program
   of the State Key Laboratory of Virtual Reality Technology and Systems,
   Beihang University [VRLAB2019A01]
FX We thank all anonymous reviewers for their valuable comments and
   constructive suggestions. This work was supported by grants from
   National Natural Science Foundation of China (NSFC) (No. 61572291), the
   Young Scholars Program of Shandong University (YSPSDU), and the Open
   Project Program of the State Key Laboratory of Virtual Reality
   Technology and Systems, Beihang University (VRLAB2019A01).
CR Bo Z, 2017, COMPUT GRAPH FORUM, V36, P29, DOI 10.1111/cgf.13269
   Chu Han, 2018, Computational Visual Media, V4, P161, DOI 10.1007/s41095-018-0104-1
   Gal R, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P7
   Hu WC, 2016, IEEE T VIS COMPUT GR, V22, P1302, DOI 10.1109/TVCG.2015.2498620
   Huang Z, 2014, IEEE T VIS COMPUT GR, V20, P1076, DOI 10.1109/TVCG.2014.2303087
   Jacobs Jennifer., P SIGCHI C HUMAN FAC, DOI [10.1145/2470654.2466211, DOI 10.1145/2470654.2466211]
   Jaramillo A, 2013, COMPUT AIDED DESIGN, V45, P1128, DOI 10.1016/j.cad.2013.02.005
   Jie Xu, 2007, Proceedings Graphics Interface 2007, P43
   Kampas F.J., 2016, GEN ELLIPSE PACKINGS
   Kim J, 2002, ACM T GRAPHIC, V21, P657
   Komyak Va., 2017, VOSTOCHNO EVROPEISKI, P17, DOI DOI 10.15587/1729-4061.2017.91902
   Latecki LJ, 1999, COMPUT VIS IMAGE UND, V73, P441, DOI 10.1006/cviu.1998.0738
   Ma Y, 2018, COMPUT GRAPH FORUM, V37, P49, DOI 10.1111/cgf.13490
   Patzák B, 2012, ACTA POLYTECH, V52, P59
   Rother C, 2006, ACM T GRAPHIC, V25, P847, DOI 10.1145/1141911.1141965
   Saputra Reza Adhitya, 2017, P 43 GRAPH INT C, P8, DOI DOI 10.20380/GI2017.02
   Torres C., 2015, P 2015 ACM SIGCHI C, P73
   Wang YH, 2018, IEEE T VIS COMPUT GR, V24, P647, DOI 10.1109/TVCG.2017.2745859
   Xu PF, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2787, DOI 10.1145/2702123.2702198
   Yu ZQ, 2014, IEEE T VIS COMPUT GR, V20, P182, DOI 10.1109/TVCG.2013.106
   Zhang JS, 2017, COMPUT GRAPH FORUM, V36, P64, DOI 10.1111/cgf.12785
   Zheng C, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON CREATIVITY AND COGNITION (C&C 2017), P63, DOI 10.1145/3059454.3059459
NR 22
TC 3
Z9 4
U1 1
U2 8
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2019
VL 5
IS 3
BP 293
EP 302
DI 10.1007/s41095-019-0143-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UN
UT WOS:000648690900005
OA gold
DA 2024-07-18
ER

PT J
AU Wang, C
   Tang, F
   Zhang, Y
   Wu, TR
   Dong, WM
AF Wang, Cong
   Tang, Fan
   Zhang, Yong
   Wu, Tieru
   Dong, Weiming
TI Towards harmonized regional style transfer and manipulation for facial
   images
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE face manipulation; style transfer; generative models; facial
   harmonization
AB Regional facial image synthesis conditioned on a semantic mask has achieved great attention in the field of computational visual media. However, the appearances of different regions may be inconsistent with each other after performing regional editing. In this paper, we focus on harmonized regional style transfer for facial images. A multi-scale encoder is proposed for accurate style code extraction. The key part of our work is a multi-region style attention module. It adapts multiple regional style embeddings from a reference image to a target image, to generate a harmonious result. We also propose style mapping networks for multi-modal style synthesis. We further employ an invertible flow model which can serve as mapping network to fine-tune the style code by inverting the code to latent space. Experiments on three widely used face datasets were used to evaluate our model by transferring regional facial appearance between datasets. The results show that our model can reliably perform style transfer and multi-modal manipulation, generating output comparable to the state of the art.
C1 [Wang, Cong; Wu, Tieru] Jilin Univ, Sch Math, Changchun 130012, Peoples R China.
   [Tang, Fan; Wu, Tieru] Jilin Univ, Sch Artificial Intelligence, Changchun 130012, Peoples R China.
   [Zhang, Yong] Tencent, AI Lab, Shenzhen 518054, Peoples R China.
   [Dong, Weiming] Chinese Acad Sci, Inst Automation, Beijing 100190, Peoples R China.
C3 Jilin University; Jilin University; Tencent; Chinese Academy of
   Sciences; Institute of Automation, CAS
RP Wu, TR (corresponding author), Jilin Univ, Sch Math, Changchun 130012, Peoples R China.; Tang, F; Wu, TR (corresponding author), Jilin Univ, Sch Artificial Intelligence, Changchun 130012, Peoples R China.
EM wang16@mails.jlu.edu.cn; tanhgfan@jlu.edu.cn; zhangyong201303@gmail.com;
   wutr@jlu.edu.cn; wmlake@gmail.com
RI DONG, Weiming/AAG-7678-2020; Tang, Fan/O-3923-2018
OI DONG, Weiming/0000-0001-6502-145X; Tang, Fan/0000-0002-3975-2483
FU National Key R&D Program of China; National Natural Science Foundation
   of China;  [2020YFA0714100];  [61872162];  [62102162];  [61832016]; 
   [U20B2070]
FX AcknowledgementsThis work was partly supported by the National Key R&D
   Program of China (No. 2020YFA0714100) and the National Natural Science
   Foundation of China (Nos. 61872162, 62102162, 61832016, U20B2070).
CR Abdal R, 2020, Arxiv, DOI arXiv:2008.02401
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168
   Chen RTQ, 2018, 32 C NEURAL INFORM P, V31
   Chen SY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459760
   Cun XD, 2020, IEEE T IMAGE PROCESS, V29, P4759, DOI 10.1109/TIP.2020.2975979
   Denton E.L., 2015, CoRR, P1486
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Grathwohl W., 2018, P INT C LEARNING REP
   Gu SY, 2019, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2019.00355
   Gulrajani I., 2017, P 31 INT C NEUR INF, P5769
   Heusel M., 2017, P ADV NEUR INF PROC, P6629
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang ZY, 2022, COMPUT VIS MEDIA, V8, P63, DOI 10.1007/s41095-021-0227-7
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang WT, 2020, PROC CVPR IEEE, P5193, DOI 10.1109/CVPR42600.2020.00524
   Jiapeng Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P592, DOI 10.1007/978-3-030-58520-4_35
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T., 2018, arXiv, DOI [10.48550/arXiv.1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kingma D, 2014, ICLR P, V2014, P1
   Kingma Diederik P, 2014, INT C LEARNING REPRE
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Lee J, 2020, PROC CVPR IEEE, P5800, DOI 10.1109/CVPR42600.2020.00584
   Liu YL, 2020, AAAI CONF ARTIF INTE, V34, P11637
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Miyato T., 2018, 6 INT C LEARNING REP
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Portenier T, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201393
   Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Shen YJ, 2021, Arxiv, DOI [arXiv:2007.06600, 10.48550/ARXIV.2007.06600]
   Sun RQ, 2021, COMPUT VIS MEDIA, V7, P363, DOI 10.1007/s41095-021-0219-7
   Tan ZT, 2022, IEEE T PATTERN ANAL, V44, P4852, DOI 10.1109/TPAMI.2021.3076487
   Tan ZT, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392488
   Tewari A, 2020, PROC CVPR IEEE, P6141, DOI 10.1109/CVPR42600.2020.00618
   Tsai YH, 2017, PROC CVPR IEEE, P2799, DOI 10.1109/CVPR.2017.299
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang M, 2019, PROC CVPR IEEE, P1495, DOI 10.1109/CVPR.2019.00159
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wenyan Cong, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8391, DOI 10.1109/CVPR42600.2020.00842
   Yang Dingdong, 2019, INT C LEARN REPR
   Yang GD, 2019, IEEE I CONF COMP VIS, P4540, DOI 10.1109/ICCV.2019.00464
   Yujun Shen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9240, DOI 10.1109/CVPR42600.2020.00926
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang P, 2020, PROC CVPR IEEE, P5142, DOI 10.1109/CVPR42600.2020.00519
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhu JY, 2017, ADV NEUR IN, V30
   Zhu JY, 2015, IEEE I CONF COMP VIS, P3943, DOI 10.1109/ICCV.2015.449
   Zhu PH, 2020, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR42600.2020.00515
   Zhu Z, 2020, PROC CVPR IEEE, P5466, DOI 10.1109/CVPR42600.2020.00551
NR 55
TC 4
Z9 4
U1 4
U2 28
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2023
VL 9
IS 2
BP 351
EP 366
DI 10.1007/s41095-022-0284-6
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7N8FL
UT WOS:000907570700009
OA Green Submitted, gold
DA 2024-07-18
ER

PT J
AU Wang, Q
   Zhang, WX
   Cheng, YY
   Liu, LG
   Fu, XM
AF Wang, Qi
   Zhang, Wen-Xiang
   Cheng, Yuan-Yuan
   Liu, Ligang
   Fu, Xiao-Ming
TI Practical construction of globally injective parameterizations with
   positional constraints
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE globally injective parameterization; constrained parameterization;
   bijection; flip-free; scaffold mesh
ID SURFACE PARAMETERIZATION; OPTIMIZATION; GEOMETRY; MAPPINGS
AB We propose a novel method to compute globally injective parameterizations with arbitrary positional constraints on disk topology meshes. Central to this method is the use of a scaffold mesh that reduces the globally injective constraint to a locally flipfree condition. Hence, given an initial parameterized mesh containing flipped triangles and satisfying the positional constraints, we only need to remove the flips of a overall mesh consisting of the parameterized mesh and the scaffold mesh while always meeting positional constraints. To successfully apply this idea, we develop two key techniques. Firstly, an initialization method is used to generate a valid scaffold mesh and mitigate difficulties in eliminating flips. Secondly, edge-based remeshing is used to optimize the regularity of the scaffold mesh containing flips, thereby improving practical robustness. Compared to state-of-the-art methods, our method is much more robust. We demonstrate the capability and feasibility of our method on a large number of complex meshes.
C1 [Wang, Qi; Zhang, Wen-Xiang; Cheng, Yuan-Yuan; Liu, Ligang; Fu, Xiao-Ming] Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Fu, XM (corresponding author), Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Peoples R China.
EM wq2014@mail.ustc.edu.cn; zwx111@mail.ustc.edu.cn; chyy@mail.ustc.edu.cn;
   lgliu@ustc.eu.cn; fuxm@ustc.edu.cn
RI Fu, Xiao-Ming/V-8253-2019; Cheng, Yuan/JKJ-0794-2023; lin,
   qing/JTU-4293-2023; Zhang, Yulin/KEI-1610-2024; Wang,
   YUJIE/JXY-8442-2024
FU National Natural Science Foundation of China; USTC Research Funds of the
   Double First-Class Initiative;  [61802359];  [62025207];  [YD0010002003]
FX AcknowledgementsWe would like to thank the anonymous reviewers for their
   constructive suggestions and comments. This work was supported by the
   National Natural Science Foundation of China (61802359, 62025207) and
   USTC Research Funds of the Double First-Class Initiative (YD0010002003).
CR Aigerman N, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982412
   Aigerman N, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818099
   Ainsley S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366170
   Botsch M., 2004, PROC EUROGRAPHICS AC, P185, DOI [10.1145/1057432.1057457, DOI 10.1145/1057432.1057457]
   Bouaziz S, 2012, COMPUT GRAPH FORUM, V31, P1657, DOI 10.1111/j.1467-8659.2012.03171.x
   Claici S, 2017, COMPUT GRAPH FORUM, V36, P37, DOI 10.1111/cgf.13243
   Du XY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480556
   Du XY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392484
   Escobar JM, 2003, COMPUT METHOD APPL M, V192, P2775, DOI 10.1016/S0045-7825(03)00299-8
   Fang Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459757
   Floater M, 2002, TUTORIALS ON MULTIRESOLUTION IN GEOMETRIC MODELLING, P287
   Floater MS, 2005, MATH VIS, P157, DOI 10.1007/3-540-26808-1_9
   Floater MS, 2003, MATH COMPUT, V72, P685, DOI 10.1090/S0025-5718-02-01466-7
   Fu XM, 2021, COMPUT VIS MEDIA, V7, P289, DOI 10.1007/s41095-021-0233-9
   Fu XM, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980231
   Fu XM, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766938
   Garanzha V, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459847
   Golla B, 2018, COMPUT GRAPH FORUM, V37, P233, DOI 10.1111/cgf.13563
   Hormann K., 2007, P ACM SIGGRAPH 2007
   Hormann K., 2000, CURVE SURFACE DESIGN, P153
   HORN BKP, 1987, J OPT SOC AM A, V4, P629, DOI 10.1364/JOSAA.4.000629
   Jiang ZS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130895
   Jin Y, 2014, COMPUT GRAPH FORUM, V33, P269, DOI 10.1111/cgf.12452
   Kim Doyub, 2013, ACM Transactions on Graphics (TOG), V32, P4
   Kovalsky SZ, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925920
   Kovalsky SZ, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818098
   Lipman Y, 2014, SIAM J IMAGING SCI, V7, P1263, DOI 10.1137/130939754
   Liu LG, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201331
   Misztal MK, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2167076.2167082
   Müller M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766907
   Overby M, 2021, COMPUT GRAPH FORUM, V40, P111, DOI 10.1111/cgf.14361
   Rabinovich M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2983621
   Schreiner J, 2004, ACM T GRAPHIC, V23, P870, DOI 10.1145/1015706.1015812
   Schüller C, 2013, COMPUT GRAPH FORUM, V32, P125, DOI 10.1111/cgf.12179
   Sheffer A, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000011
   Shen HX, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323012
   Shewchuk J.R., 1996, WORKSHOP APPL COMPUT, V1148, P203
   Shtengel A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073618
   Smith B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3241041
   Smith J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766947
   Su JP, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392435
   Su JP, 2019, COMPUT GRAPH FORUM, V38, P287, DOI 10.1111/cgf.13837
   Tutte W.T., 1963, Proc. London Math. Soc., V13, P743, DOI DOI 10.1112/PLMS/S3-13.1.743
   Xu Y, 2011, COMPUT AIDED GEOM D, V28, P349, DOI 10.1016/j.cagd.2011.07.001
   Zhang E, 2005, ACM T GRAPHIC, V24, P1, DOI 10.1145/1037957.1037958
   Zhu YF, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201359
NR 46
TC 1
Z9 1
U1 1
U2 5
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2023
VL 9
IS 2
BP 265
EP 277
DI 10.1007/s41095-022-0269-5
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7N8FL
UT WOS:000907570700004
OA gold
DA 2024-07-18
ER

PT J
AU Hotta, K
   Akashi, T
   Tokai, S
   Zhang, C
AF Hotta, Katsuya
   Akashi, Takuya
   Tokai, Shogo
   Zhang, Chao
TI PMSSC: Parallelizable multi-subset based self-expressive model for
   subspace clustering
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE subspace clustering; self-expressive model; big data; subsetting
ID MOTION SEGMENTATION; SPARSE
AB Subspace clustering methods which embrace a self-expressive model that represents each data point as a linear combination of other data points in the dataset provide powerful unsupervised learning techniques. However, when dealing with large datasets, representation of each data point by referring to all data points via a dictionary suffers from high computational complexity. To alleviate this issue, we introduce a parallelizable multi-subset based self-expressive model (PMS) which represents each data point by combining multiple subsets, with each consisting of only a small proportion of the samples. The adoption of PMS in subspace clustering (PMSSC) leads to computational advantages because the optimization problems decomposed over each subset are small, and can be solved efficiently in parallel. Furthermore, PMSSC is able to combine multiple self-expressive coefficient vectors obtained from subsets, which contributes to an improvement in self-expressiveness. Extensive experiments on synthetic and real-world datasets show the efficiency and effectiveness of our approach in comparison to other methods.
C1 [Hotta, Katsuya; Tokai, Shogo; Zhang, Chao] Univ Fukui, Dept Engn, Fukui 9108507, Japan.
   [Akashi, Takuya] Iwate Univ, Fac Sci & Engn, Morioka 0208551, Japan.
C3 University of Fukui; Iwate University
RP Zhang, C (corresponding author), Univ Fukui, Dept Engn, Fukui 9108507, Japan.
EM k-hotta@u-fukui.ac.jp; akashi@iwate-u.ac.jp; tokai@u-fukui.ac.jp;
   zhang@u-fukui.ac.jp
RI Hotta, Katsuya/JOZ-5395-2023; Wang, Yifan/KDO-8319-2024; Akashi,
   Takuya/JFJ-1977-2023
OI Hotta, Katsuya/0000-0002-8148-1445; Akashi, Takuya/0000-0001-9177-7306
FU JSPS KAKENHI [JP20K19568]; Grants-in-Aid for Scientific Research
   [20K19568] Funding Source: KAKEN
FX AcknowledgementsThis work was supported by JSPS KAKENHI Grant Number
   JP20K19568.
CR Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230
   Cai D, 2007, PROC CVPR IEEE, P650
   Chen GL, 2009, INT J COMPUT VISION, V81, P317, DOI 10.1007/s11263-008-0178-9
   Chen Y, 2020, PROC CVPR IEEE, P4154, DOI 10.1109/CVPR42600.2020.00421
   Chung F. R., 1997, Spectral Graph Theory, V92, DOI DOI 10.1090/CBMS/092
   Cohen G, 2017, IEEE IJCNN, P2921, DOI 10.1109/IJCNN.2017.7966217
   Davenport MA, 2010, IEEE T INFORM THEORY, V56, P4395, DOI 10.1109/TIT.2010.2054653
   Dong WH, 2019, PATTERN ANAL APPL, V22, P165, DOI 10.1007/s10044-018-00774-z
   Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132
   Dyer EL, 2013, J MACH LEARN RES, V14, P2487
   Elhamifar Ehsan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2790, DOI 10.1109/CVPRW.2009.5206547
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Greene D., 2006, P 23 INT C MACH LEAR, V148, P377
   Guo Y, 2021, SIGNAL PROCESS, V185, DOI 10.1016/j.sigpro.2021.108082
   Heckel R, 2013, INT CONF ACOUST SPEE, P3263, DOI 10.1109/ICASSP.2013.6638261
   Hotta K., 2021, P SOC PHOTO-OPT INS, P267
   Hotta K., 2020, IRISH MACHINE VISION, P125
   Ji P, 2014, IEEE WINT CONF APPL, P461, DOI 10.1109/WACV.2014.6836065
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Lane C, 2019, IEEE INT CONF COMP V, P678, DOI 10.1109/ICCVW.2019.00082
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92
   Lipor J, 2021, INF INFERENCE, V10, P73, DOI 10.1093/imaiai/iaaa031
   Liu G., 2010, P INT C MACH LEARN, P663
   Lu CY, 2012, LECT NOTES COMPUT SC, V7578, P347, DOI 10.1007/978-3-642-33786-4_26
   Lu CY, 2019, IEEE T PATTERN ANAL, V41, P487, DOI 10.1109/TPAMI.2018.2794348
   Matsushima S, 2019, ADV NEUR IN, V32
   Nasihatkon B., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2137, DOI 10.1109/CVPR.2011.5995679
   PATI YC, 1993, CONFERENCE RECORD OF THE TWENTY-SEVENTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P40, DOI 10.1109/ACSSC.1993.342465
   Peng X, 2013, PROC CVPR IEEE, P430, DOI 10.1109/CVPR.2013.62
   Samaria F. S., 1994, Proceedings of the Second IEEE Workshop on Applications of Computer Vision (Cat. No.94TH06742), P138, DOI 10.1109/ACV.1994.341300
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Stallkamp J, 2012, NEURAL NETWORKS, V32, P323, DOI 10.1016/j.neunet.2012.02.016
   Tierney S, 2014, PROC CVPR IEEE, P1019, DOI 10.1109/CVPR.2014.134
   Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793
   Tseng P, 2000, J OPTIMIZ THEORY APP, V105, P249, DOI 10.1023/A:1004678431677
   Vidal R, 2008, INT J COMPUT VISION, V79, P85, DOI 10.1007/s11263-007-0099-z
   Vidal R, 2014, PATTERN RECOGN LETT, V43, P47, DOI 10.1016/j.patrec.2013.08.006
   Vidal R, 2011, IEEE SIGNAL PROC MAG, V28, P52, DOI 10.1109/MSP.2010.939739
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   WONG CK, 1980, SIAM J COMPUT, V9, P111, DOI 10.1137/0209009
   Yan JY, 2006, LECT NOTES COMPUT SC, V3954, P94
   Yang AY, 2008, COMPUT VIS IMAGE UND, V110, P212, DOI 10.1016/j.cviu.2007.07.005
   You C, 2022, IEEE T PATTERN ANAL, V44, P2698, DOI 10.1109/TPAMI.2020.3035599
   You C, 2018, LECT NOTES COMPUT SC, V11213, P68, DOI 10.1007/978-3-030-01240-3_5
   You C, 2016, CONF REC ASILOMAR C, P1014, DOI 10.1109/ACSSC.2016.7869521
   You C, 2016, PROC CVPR IEEE, P3918, DOI 10.1109/CVPR.2016.425
   You C, 2016, PROC CVPR IEEE, P3928, DOI 10.1109/CVPR.2016.426
   Yu Y., 2020, ARXIV200608558
   Zhang C, 2020, COMPUT VIS MEDIA, V6, P135, DOI 10.1007/s41095-020-0166-8
   Zhang C, 2019, IEICE T INF SYST, VE102D, P2485, DOI 10.1587/transinf.2019EDP7138
   Zhang L, 2009, PROCEEDINGS OF 2009 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS TECHNOLOGY AND APPLICATIONS, P212, DOI 10.1109/ICCOMTA.2009.5349209
   Zhang SZ, 2021, PROC CVPR IEEE, P12388, DOI 10.1109/CVPR46437.2021.01221
NR 53
TC 0
Z9 0
U1 0
U2 1
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2023
VL 9
IS 3
BP 479
EP 494
DI 10.1007/s41095-022-0293-5
EA MAR 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA G9DP6
UT WOS:000961341400001
OA gold
DA 2024-07-18
ER

PT J
AU Meng, WL
   Bo, PB
   Zhang, XD
   Hong, JX
   Xin, SQ
   Tu, CH
AF Meng, Wenlong
   Bo, Pengbo
   Zhang, Xiaodong
   Hong, Jixiang
   Xin, Shiqing
   Tu, Changhe
TI An efficient algorithm for approximate Voronoi diagram construction on
   triangulated surfaces
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE geodesic Voronoi diagrams (GVDs); triangular surfaces; mesh surfaces;
   approximate geodesics; Apollonius diagrams
ID FRECHET DISTANCE; GEODESIC PATHS; SHORTEST PATHS; COMPUTATION;
   TESSELLATION; GENERATION; MESH
AB Voronoi diagrams on triangulated surfaces based on the geodesic metric play a key role in many applications of computer graphics. Previous methods of constructing such Voronoi diagrams generally depended on having an exact geodesic metric. However, exact geodesic computation is time-consuming and has high memory usage, limiting wider application of geodesic Voronoi diagrams (GVDs). In order to overcome this issue, instead of using exact methods, we reformulate a graph method based on Steiner point insertion, as an effective way to obtain geodesic distances. Further, since a bisector comprises hyperbolic and line segments, we utilize Apollonius diagrams to encode complicated structures, enabling Voronoi diagrams to encode a medial-axis surface for a dense set of boundary samples. Based on these strategies, we present an approximation algorithm for efficient Voronoi diagram construction on triangulated surfaces. We also suggest a measure for evaluating similarity of our results to the exact GVD. Although our GVD results are constructed using approximate geodesic distances, we can get GVD results similar to exact results by inserting Steiner points on triangle edges. Experimental results on many 3D models indicate the improved speed and memory requirements compared to previous leading methods.
C1 [Meng, Wenlong; Bo, Pengbo; Zhang, Xiaodong] Harbin Inst Technol, Sch Comp Sci & Technol, Weihai 264209, Peoples R China.
   [Hong, Jixiang; Xin, Shiqing; Tu, Changhe] Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Peoples R China.
C3 Harbin Institute of Technology; Shandong University
RP Xin, SQ; Tu, CH (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Peoples R China.
EM xinshiqing@sdu.edu.cn; chtu@sdu.edu.cn
RI Tu, Changhe/H-5162-2013
FU Youth Teacher Development Foundation of Harbin Institute of Technology
   [IDGA10002143]; National Natural Science Foundation of China [U22A2033,
   62072139, 62272277, 62072284]; National Key R&D Program of China
   [2021YFB1715900]
FX The authors would like to thank the reviewers for their valuable
   suggestions. This work was supported in part by the Youth Teacher
   Development Foundation of Harbin Institute of Technology (IDGA10002143),
   the National Natural Science Foundation of China (62072139, 62272277,
   62072284), the National Key R&D Program of China (2021YFB1715900), and
   the Joint Funds of the National Natural Science Foundation of China
   (U22A2033).
CR Adikusuma Yohanes Yudhi, 2022, Computer-Aided Design, V150, DOI 10.1016/j.cad.2022.103333
   Adikusuma YY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3144567
   Aleksandov L., 2000, Proceedings of the Thirty Second Annual ACM Symposium on Theory of Computing, P286, DOI 10.1145/335305.335339
   Aleksandrov L, 2005, J ACM, V52, P25, DOI 10.1145/1044731.1044733
   Aleksandrov L, 1998, LECT NOTES COMPUT SC, V1432, P11, DOI 10.1007/BFb0054351
   ALT H, 1995, INT J COMPUT GEOM AP, V5, P75, DOI 10.1142/S0218195995000064
   [Anonymous], 2006, Effective Computational Geometry for Curves and Surfaces, DOI [DOI 10.1007/978-3-540-33259-6_2, DOI 10.1007/978-3-540-33259-6-2]
   [Anonymous], 2000, Computational Geometry Algorithms and Applications
   AUGENBAUM JM, 1985, J COMPUT PHYS, V59, P177, DOI 10.1016/0021-9991(85)90140-8
   AURENHAMMER F, 1987, SIAM J COMPUT, V16, P78, DOI 10.1137/0216006
   AURENHAMMER F, 1991, COMPUT SURV, V23, P345, DOI 10.1145/116873.116880
   Boissonnat J. D., 2013, ARXIV
   Bose P, 2011, COMP GEOM-THEOR APPL, V44, P486, DOI 10.1016/j.comgeo.2011.05.006
   CHEN JD, 1990, PROCEEDINGS OF THE SIXTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY, P360, DOI 10.1145/98524.98601
   Crane K., 2020, ARXIV
   Crane K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2516971.2516977
   Dong X, 2021, IEEE T IMAGE PROCESS, V30, P8847, DOI 10.1109/TIP.2021.3120878
   Du J, 2021, COMPUT AIDED DESIGN, V130, DOI 10.1016/j.cad.2020.102943
   Eiter T., 1994, Computing Discrete Frechet Distance
   FORTUNE S, 1987, ALGORITHMICA, V2, P153, DOI 10.1007/BF01840357
   Gavrilova M, 1996, INT J COMPUT MATH, V61, P49, DOI 10.1080/00207169608804499
   Giblin P, 2004, IEEE T PATTERN ANAL, V26, P238, DOI 10.1109/TPAMI.2004.1262192
   Karavelas M I, 2002, PREDICATES PLANAR AD
   Karavelas MI, 2002, LECT NOTES COMPUT SC, V2461, P586
   Kimmel R, 1998, IEEE T ROBOTIC AUTOM, V14, P427, DOI 10.1109/70.678452
   Kimmel R, 1998, P NATL ACAD SCI USA, V95, P8431, DOI 10.1073/pnas.95.15.8431
   Kimmel R, 2000, FAST VORONOI DIAGRAM
   Kunze R, 1997, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P230, DOI 10.1109/CGI.1997.601311
   Lanthier M., 1997, Proceedings of the Thirteenth Annual Symposium on Computational Geometry, P274, DOI 10.1145/262839.262984
   Lanthier M, 2001, ALGORITHMICA, V30, P527, DOI 10.1007/s00453-001-0027-5
   LEE DT, 1982, IEEE T PATTERN ANAL, V4, P363, DOI 10.1109/TPAMI.1982.4767267
   Leibon G., 2000, P 16 ANN S COMP GEOM, V5, P341, DOI [10.1145/336154.336221, DOI 10.1145/336154.336221]
   [刘金义 Liu Jinyi], 2004, [工程图学学报, Journal of Engineering Graphics], V25, P125
   Liu Y, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1559755.1559758
   Liu YJ, 2018, IEEE T PATTERN ANAL, V40, P653, DOI 10.1109/TPAMI.2017.2686857
   Liu YJ, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2999532
   Liu YJ, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982424
   Liu YJ, 2013, INFORM PROCESS LETT, V113, P132, DOI 10.1016/j.ipl.2012.12.010
   Liu YJ, 2011, IEEE T PATTERN ANAL, V33, P1502, DOI 10.1109/TPAMI.2010.221
   LO SH, 1985, INT J NUMER METH ENG, V21, P1403, DOI 10.1002/nme.1620210805
   Lu L, 2012, COMPUT GRAPH FORUM, V31, P775, DOI 10.1111/j.1467-8659.2012.03058.x
   Medimegh N, 2015, INT J MULTIMEDIA, V1, P1, DOI DOI 10.16966/IJM.102
   Meng WL, 2022, IEEE T VIS COMPUT GR, V28, P4887, DOI 10.1109/TVCG.2021.3109042
   MITCHELL JSB, 1987, SIAM J COMPUT, V16, P647, DOI 10.1137/0216045
   Na HS, 2002, COMP GEOM-THEOR APPL, V23, P183, DOI 10.1016/S0925-7721(02)00077-9
   Onishi K, 1996, IEICE T FUND ELECTR, VE79A, P533
   Peethambaran J, 2015, COMPUT AIDED DESIGN, V58, P62, DOI 10.1016/j.cad.2014.08.021
   Peyré G, 2006, INT J COMPUT VISION, V69, P145, DOI 10.1007/s11263-006-6859-3
   Qin YP, 2017, COMPUT GRAPH FORUM, V36, P93, DOI 10.1111/cgf.13248
   Qin YP, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925930
   Rong GD, 2011, IEEE T VIS COMPUT GR, V17, P345, DOI 10.1109/TVCG.2010.53
   Rote G, 2007, COMP GEOM-THEOR APPL, V37, P162, DOI 10.1016/j.comgeo.2005.01.004
   SENECHAL M, 1993, SCIENCE, V260, P1170, DOI 10.1126/science.260.5111.1170
   Solomon J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601175
   Stankovic T, 2020, J MECH DESIGN, V142, DOI 10.1115/1.4046916
   Surazhsky V, 2005, ACM T GRAPHIC, V24, P553, DOI 10.1145/1073204.1073228
   Tsai J, 1997, PROTEIN SCI, V6, P2606
   Wang PH, 2020, COMPUT GRAPH FORUM, V39, P43, DOI 10.1111/cgf.14125
   Wang XN, 2015, COMPUT AIDED DESIGN, V58, P51, DOI 10.1016/j.cad.2014.08.023
   Weber O, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409625.1409626
   Xin S Q., 2012, Proc. - I3D: ACM SIGGRAPH Symp. Interact. 3D Graph. Games, P31
   Xin SQ, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1559755.1559761
   Xu CX, 2015, IEEE T VIS COMPUT GR, V21, P822, DOI 10.1109/TVCG.2015.2407404
   Xu CX, 2014, COMPUT GRAPH FORUM, V33, P161, DOI 10.1111/cgf.12484
   Xu P., 2016, Int. J. Hybrid Inf. Technol, V9, P273
   Ying X, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2534161
   Ying X, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508379
   Zhou Q, 2016, ARXIV
NR 68
TC 1
Z9 1
U1 2
U2 5
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2023
VL 9
IS 3
BP 443
EP 459
DI 10.1007/s41095-022-0326-0
EA MAR 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA G9DP6
UT WOS:000943883100002
OA gold
DA 2024-07-18
ER

PT J
AU Sun, RQ
   Huang, C
   Zhu, HL
   Ma, LZ
AF Sun, Ruoqi
   Huang, Chen
   Zhu, Hengliang
   Ma, Lizhuang
TI Mask-aware photorealistic facial attribute manipulation
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE face attribute manipulation; generative adversarial network (GAN);
   variational autoencoder (VAE); partial dilated layers; photorealism
AB The technique of facial attribute manipulation has found increasing application, but it remains challenging to restrict editing of attributes so that a face's unique details are preserved. In this paper, we introduce our method, which we call a mask-adversarial autoencoder (M-AAE). It combines a variational autoencoder (VAE) and a generative adversarial network (GAN) for photorealistic image generation. We use partial dilated layers to modify a few pixels in the feature maps of an encoder, changing the attribute strength continuously without hindering global information. Our training objectives for the VAE and GAN are reinforced by supervision of face recognition loss and cycle consistency loss, to faithfully preserve facial details. Moreover, we generate facial masks to enforce background consistency, which allows our training to focus on the foreground face rather than the background. Experimental results demonstrate that our method can generate high-quality images with varying attributes, and outperforms existing methods in detail preservation.
C1 [Sun, Ruoqi; Zhu, Hengliang] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
   [Ma, Lizhuang] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Digital Media Technol & Data Reconstruct Lab, Shanghai 200240, Peoples R China.
   [Huang, Chen] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; Carnegie
   Mellon University
RP Ma, LZ (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Digital Media Technol & Data Reconstruct Lab, Shanghai 200240, Peoples R China.
EM ruoqisun7@sjtu.edu.cn; chen-huang@apple.com; hengliangzhu@sjtu.edu.cn;
   ma-lz@cs.sjtu.edu.cn
RI Sun, Peng/KDO-4243-2024
FU National Natural Science Foundation of China [61972157]; National Social
   Science Foundation of China [18ZD22]; Science and Technology Commission
   of Shanghai Municipality Program [18D1205903]; Science and Technology
   Commission of Pudong Municipality Program [PKJ2018-Y46];
   Multidisciplinary Project of Shanghai Jiao Tong University
   [ZH2018ZDA25]; Shanghai Jiao Tong University; project of SenseTime
FX This paper was partially funded by the National Natural Science
   Foundation of China (No. 61972157), the National Social Science
   Foundation of China (No. 18ZD22), the Science and Technology Commission
   of Shanghai Municipality Program (No. 18D1205903), the Science and
   Technology Commission of Pudong Municipality Program (No. PKJ2018-Y46),
   and the Multidisciplinary Project of Shanghai Jiao Tong University (No.
   ZH2018ZDA25), and is also partially supported by a joint project of
   SenseTime and Shanghai Jiao Tong University.
CR [Anonymous], 2016, P ADV NEURAL INFORM
   Bahng H, 2020, PROC CVPR IEEE, P5820, DOI 10.1109/CVPR42600.2020.00586
   Chen P, 2019, 2019 INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE BIG DATA AND INTELLIGENT SYSTEMS (HPBD&IS), P97, DOI [10.1109/hpbdis.2019.8735455, 10.1109/HPBDIS.2019.8735455]
   Chen YC, 2019, PROC CVPR IEEE, P9851, DOI 10.1109/CVPR.2019.01009
   Duong CN, 2017, IEEE I CONF COMP VIS, P3755, DOI 10.1109/ICCV.2017.403
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gauthier JP, 2014, CONF P INDIUM PHOSPH
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He Z., 2017, ARXIV PREPRINT ARXIV
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Hou XX, 2017, IEEE WINT CONF APPL, P1133, DOI 10.1109/WACV.2017.131
   Hung-Yu Tseng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P242, DOI 10.1007/978-3-030-58598-3_15
   Katzir Oren, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P673, DOI 10.1007/978-3-030-58536-5_40
   Kim H, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201283
   Kim T, 2017, PR MACH LEARN RES, V70
   Kingma D. P., 2014, arXiv
   Lample Guillaume, 2017, P ANN C NEUR INF PRO, P5967
   Larsen ABL, 2016, PR MACH LEARN RES, V48
   Li YH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2230
   Liu M, 2019, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2019.00379
   Liu MY, 2017, ADV NEUR IN, V30
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Park U, 2010, IEEE T PATTERN ANAL, V32, P947, DOI 10.1109/TPAMI.2010.14
   Qian SJ, 2019, IEEE I CONF COMP VIS, P10032, DOI 10.1109/ICCV.2019.01013
   Radford A., 2015, ARXIV
   Richardson E, 2017, PROC CVPR IEEE, P5553, DOI 10.1109/CVPR.2017.589
   Shen W, 2017, PROC CVPR IEEE, P1225, DOI 10.1109/CVPR.2017.135
   Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8
   Suwajanakorn S, 2014, LECT NOTES COMPUT SC, V8692, P796, DOI 10.1007/978-3-319-10593-2_52
   Wang C, 2018, P EUR C COMP VIS ECC, P770
   Yang J., 2017, P INT C LEARN REPR
   Yujun Shen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9240, DOI 10.1109/CVPR42600.2020.00926
   Zhang G, 2018, LECT NOTES COMPUT SC, V11210, P422, DOI 10.1007/978-3-030-01231-1_26
   Zhang Z., 2018, P INT C LEARN REPR
   Zhang ZF, 2017, PROC CVPR IEEE, P4352, DOI 10.1109/CVPR.2017.463
   Zhou WY, 2021, COMPUT VIS MEDIA, V7, P153, DOI 10.1007/s41095-021-0203-2
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu WB, 2020, PROC CVPR IEEE, P4957, DOI 10.1109/CVPR42600.2020.00501
NR 41
TC 8
Z9 8
U1 2
U2 18
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2021
VL 7
IS 3
BP 363
EP 374
DI 10.1007/s41095-021-0219-7
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TR1TJ
UT WOS:000678754100006
OA gold, Green Submitted
DA 2024-07-18
ER

PT J
AU Deng, H
   Wang, BB
   Wang, R
   Holzschuch, N
AF Deng, Hong
   Wang, Beibei
   Wang, Rui
   Holzschuch, Nicolas
TI A practical path guiding method for participating media
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE Monte Carlo; ray tracing; path guiding; volume light transport;
   participating media; rendering
ID LIGHT-TRANSPORT; COMPUTATION; SCATTERING
AB Rendering translucent materials is costly: light transport algorithms need to simulate a large number of scattering events inside the material before reaching convergence. The cost is especially high for materials with a large albedo or a small mean-free-path, where higher-order scattering effects dominate. In simple terms, the paths get lost in the medium. Path guiding has been proposed for surface rendering to make convergence faster by guiding the sampling process. In this paper, we introduce a path guiding solution for translucent materials. We learn an adaptive approximate representation of the radiance distribution in the volume and use it to sample the scattering direction, combining it with phase function sampling by resampled importance sampling. The proposed method significantly improves the performance of light transport simulation in participating media, especially for small lights and media with refractive boundaries. Our method can handle any homogeneous participating medium, with high or low scattering, with high or low absorption, and from isotropic to highly anisotropic.
C1 [Deng, Hong; Wang, Beibei] Nanjing Univ Sci & Technol, Nanjing 210094, Peoples R China.
   [Wang, Rui] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Peoples R China.
   [Holzschuch, Nicolas] Univ Grenoble Alpes, LJK, Grenoble INP, CNRS,INRIA, F-38000 Grenoble, France.
C3 Nanjing University of Science & Technology; Zhejiang University;
   Communaute Universite Grenoble Alpes; Institut National Polytechnique de
   Grenoble; Inria; Centre National de la Recherche Scientifique (CNRS);
   Universite Grenoble Alpes (UGA)
RP Wang, BB (corresponding author), Nanjing Univ Sci & Technol, Nanjing 210094, Peoples R China.
EM hong.deng@njust.edu.cn; beibei.wang@njust.edu.cn; rwang@cad.zju.edu.cn;
   Nicolas.Holzschuch@inria.fr
RI Holzschuch, Nicolas/E-8861-2014
FU National Key R&D Program of China [2017YFB0203000]; National Natural
   Science Foundation of China [61802187, 61872223]; Natural Science
   Foundation of Jiangsu [BK20170857]; fundamental research funds for the
   central universities [30918011320]; ANR [ANR-15-CE380005]
FX We thank the reviewers for their valuable comments. This work was
   partially supported by the National Key R&D Program of China under Grant
   No. 2017YFB0203000, the National Natural Science Foundation of China
   under Grant Nos. 61802187 and 61872223, the Natural Science Foundation
   of Jiangsu under Grant No. BK20170857, the fundamental research funds
   for the central universities No. 30918011320, and ANR project
   ANR-15-CE380005 "Materials".
CR Chandrasekhar S., 1950, RAD TRANSFER
   Guo Jerry, 2018, EUROGRAPHICS S RENDE, V2018, P73
   Herholz S, 2016, COMPUT GRAPH FORUM, V35, P67, DOI 10.1111/cgf.12950
   Holzschuch N, 2015, COMPUT GRAPH FORUM, V34, P48, DOI 10.1111/cgf.12517
   Jakob W, 2010, MITSUBA TENDERER
   Jarosz W, 2011, P SIGGRAPH AISA C
   Jarosz W, 2008, COMPUT GRAPH FORUM, V27, P557, DOI 10.1111/j.1467-8659.2008.01153.x
   Jensen H. W., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P311, DOI 10.1145/280814.280925
   Jensen HW, 1995, SPRING COMP SCI, P326, DOI 10.1007/978-3-7091-9430-0_31
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Kivanek J, 2014, P ACM SIGGRAPH 2014
   Lafortune E. P., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P91
   Meng J, 2016, COMPUT GRAPH FORUM, V35, P37, DOI 10.1111/cgf.12947
   Müller T, 2017, COMPUT GRAPH FORUM, V36, P91, DOI 10.1111/cgf.13227
   Muller Thomas, 2018, NEURAL IMPORTANCE SA
   Narasimhan SG, 2006, ACM T GRAPHIC, V25, P1003, DOI 10.1145/1141911.1141986
   PATTANAIK SN, 1993, J VISUAL COMP ANIMAT, V4, P133, DOI 10.1002/vis.4340040303
   Pauly M, 2000, SPRING COMP SCI, P11
   Talbot JustinF., 2005, RENDERING TECHNIQUES, P139, DOI [10.2312/EGWR/EGSR05/139-146, DOI 10.2312/EGWR/EGSR05/139-146]
   Veach E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P65, DOI 10.1145/258734.258775
   Zheng Q, 2019, COMPUT GRAPH FORUM, V38, P169, DOI 10.1111/cgf.13628
NR 21
TC 5
Z9 9
U1 0
U2 4
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2020
VL 6
IS 1
BP 37
EP 51
DI 10.1007/s41095-020-0160-1
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FA
UT WOS:000648691200004
OA Green Submitted, gold
DA 2024-07-18
ER

PT J
AU Wong, KM
   Wong, TT
AF Wong, Kin-Ming
   Wong, Tien-Tsin
TI Deep residual learning for denoising Monte Carlo renderings
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE Monte Carlo rendering; denoising; deep learning; deep residual learning;
   filter-free denoising
ID IMAGE; SPACE
AB Learning-based techniques have recently been shown to be effective for denoising Monte Carlo rendering methods. However, there remains a quality gap to state-of-the-art handcrafted denoisers. In this paper, we propose a deep residual learning based method that outperforms both state-of-the-art handcrafted denoisers and learning-based denoisers. Unlike the indirect nature of existing learning-based methods (which e.g., estimate the parameters and kernel weights of an explicit feature based filter), we directly map the noisy input pixels to the smoothed output. Using this direct mapping formulation, we demonstrate that even a simple-and-standard ResNet and three common auxiliary features (depth, normal, and albedo) are sufficient to achieve high-quality denoising. This minimal requirement on auxiliary data simplifies both training and integration of our method into most production rendering pipelines. We have evaluated our method on unseen images created by a different renderer. Consistently superior quality denoising is obtained in all cases.
C1 [Wong, Kin-Ming] Artixels, Hong Kong, Peoples R China.
   [Wong, Tien-Tsin] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong
RP Wong, KM (corresponding author), Artixels, Hong Kong, Peoples R China.
EM mwkm@artixels.com; ttwong@cse.cuhk.edu.hk
RI Wong, Kin-Ming/AAU-1616-2021
OI Wong, Kin-Ming/0000-0003-4736-0831
FU Research Grants Council of the Hong Kong Special Administrative Region
   [CUHK14217516]
FX This work was supported by the Research Grants Council of the Hong Kong
   Special Administrative Region, under RGC General Research Fund (Project
   No. CUHK14217516).
CR [Anonymous], 2015, HIGHWAY NETWORKS
   [Anonymous], 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.123
   Aurich V., 1995, Informatik Aktuell, P538, DOI DOI 10.1007/978-3-642-79980-8_63
   Bako S, 2017, ACM T GRAPHIC, V36, DOI [10.1145/3072959.3073703, 10.1145/3072959.3073708]
   Bauszat P, 2011, COMPUT GRAPH FORUM, V30, P1361, DOI 10.1111/j.1467-8659.2011.01996.x
   Bitterli B., 2016, RENDERING RESOURCES
   Bitterli B, 2016, COMPUT GRAPH FORUM, V35, P107, DOI 10.1111/cgf.12954
   Briggs William L, 2000, A multigrid tutorial, V72
   Buades A, 2008, INT J COMPUT VISION, V76, P123, DOI 10.1007/s11263-007-0052-1
   CNTK, 2018, MICR COGN TOOLK
   Dammertz H., 2010, P C HIGH PERF GRAPH, P67, DOI DOI 10.5555/1921479.1921491
   Eisemann E, 2004, ACM T GRAPHIC, V23, P673, DOI 10.1145/1015706.1015778
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Goodfellow I, 2016, Deep learning. vol
   Gritz L, 2008, OPENIMAGEIO SOFTWARE
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39
   Ioffe S, 2015, BATCH NORMALIZATION
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   King DB, 2015, ACS SYM SER, V1214, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Mao XJ, 2016, ADV NEUR IN, V29
   McCool MD, 1999, ACM T GRAPHIC, V18, P171, DOI 10.1145/318009.318015
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Rousselle F, 2013, COMPUT GRAPH FORUM, V32, P121, DOI 10.1111/cgf.12219
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shirley P., 2011, Symp. Ictv. 3D Graphics Games, P5, DOI 10.1145/1944745.1944747
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Van De Ville D, 2009, IEEE SIGNAL PROC LET, V16, P973, DOI 10.1109/LSP.2009.2027669
   van der Walt S, 2011, COMPUT SCI ENG, V13, P22, DOI 10.1109/MCSE.2011.37
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie J., 2012, ADV NEURAL INFORM PR, P341
   Xu Li, 2014, P ANN C NEUR INF PRO, P1790
   Xu RF, 2005, IEEE COMPUT GRAPH, V25, P31, DOI 10.1109/MCG.2005.31
   Yu D., 2014, INTRO COMPUTATIONAL
   Zagoruyko S., 2016, BMVC, P1
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zimmer H, 2015, COMPUT GRAPH FORUM, V34, P131, DOI 10.1111/cgf.12685
   Zwicker M, 2015, COMPUT GRAPH FORUM, V34, P667, DOI 10.1111/cgf.12592
NR 49
TC 14
Z9 16
U1 1
U2 5
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2019
VL 5
IS 3
BP 239
EP 255
DI 10.1007/s41095-019-0142-3
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UN
UT WOS:000648690900002
OA gold
DA 2024-07-18
ER

PT J
AU Yang, GW
   Liu, ZN
   Li, DY
   Peng, HY
AF Yang, Guo-Wei
   Liu, Zheng-Ning
   Li, Dong-Yang
   Peng, Hao-Yang
TI JNeRF: An efficient heterogeneous NeRF model zoo based on Jittor
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
C1 [Yang, Guo-Wei; Li, Dong-Yang; Peng, Hao-Yang] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Liu, Zheng-Ning] Fitten Tech Co Ltd, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Yang, GW (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
EM ygw19@mails.tsinghua.edu.cn; lzhengning@gmail.com;
   lidongyang2001@gmail.com; phy22@mails.tsinghua.edu.cn
FU National Key R&D Program of China;  [2021ZD0112902]
FX AcknowledgementsThis paper was supported by National Key R&D Program of
   China (Project No. 2021ZD0112902)
CR Hu SM, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-020-3097-4
   Mildenhall B., 2022, NERF COMMUNICATIONS, V65, P99
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Paszke A, 2019, ADV NEUR IN, V32
NR 4
TC 4
Z9 4
U1 2
U2 13
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2023
VL 9
IS 2
BP 401
EP 404
DI 10.1007/s41095-022-0327-z
PG 4
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7N8FL
UT WOS:000907570700012
OA gold
DA 2024-07-18
ER

PT J
AU Fu, HY
   Zhang, Y
   Ma, HD
AF Fu, Huiyuan
   Zhang, Yu
   Ma, Huadong
TI See clearly on rainy days: Hybrid multiscale loss guided multi-feature
   fusion network for single image rain removal
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE single image rain removal; multiple feature fusion; deep learning;
   hybrid multiscale loss
ID STREAKS REMOVAL
AB The quality of photos is highly susceptible to severe weather such as heavy rain; it can also degrade the performance of various visual tasks like object detection. Rain removal is a challenging problem because rain streaks have different appearances even in one image. Regions where rain accumulates appear foggy or misty, while rain streaks can be clearly seen in areas where rain is less heavy. We propose removing various rain effects in pictures using a hybrid multiscale loss guided multiple feature fusion de-raining network (MSGMFFNet). Specially, to deal with rain streaks, our method generates a rain streak attention map, while preprocessing uses gamma correction and contrast enhancement to enhanced images to address the problem of rain accumulation. Using these tools, the model can restore a result with abundant details. Furthermore, a hybrid multiscale loss combining L-1 loss and edge loss is used to guide the training process to pay attention to edge and content information. Comprehensive experiments conducted on both synthetic and real-world datasets demonstrate the effectiveness of our method.
C1 [Fu, Huiyuan; Zhang, Yu; Ma, Huadong] Beijing Univ Posts & Telecommun, Beijing Key Lab Intelligent Telecommun Software &, Beijing 100876, Peoples R China.
C3 Beijing University of Posts & Telecommunications
RP Fu, HY (corresponding author), Beijing Univ Posts & Telecommun, Beijing Key Lab Intelligent Telecommun Software &, Beijing 100876, Peoples R China.
EM fhy@bupt.edu.cn; yzhang@bupt.edu.cn; mhd@bupt.edu.cn
CR Ancuti CO, 2013, IEEE T IMAGE PROCESS, V22, P3271, DOI 10.1109/TIP.2013.2262284
   [Anonymous], 1997, NEURAL COMPUT
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Chen DY, 2014, IEEE T CIRC SYST VID, V24, P1430, DOI 10.1109/TCSVT.2014.2308627
   Chen K., 2019, arXiv preprint arXiv:1906.07155
   Chen YL, 2013, IEEE I CONF COMP VIS, P1968, DOI 10.1109/ICCV.2013.247
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   De-An Huang, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P164, DOI 10.1109/ICME.2012.92
   Eigen D, 2013, IEEE I CONF COMP VIS, P633, DOI 10.1109/ICCV.2013.84
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kaihao Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P71, DOI 10.1007/978-3-030-58583-9_5
   Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   King DB, 2015, ACS SYM SER, V1214, P1
   Li SY, 2019, PROC CVPR IEEE, P3833, DOI 10.1109/CVPR.2019.00396
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren WQ, 2020, INT J COMPUT VISION, V128, P240, DOI 10.1007/s11263-019-01235-8
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tan J, 2019, LECT NOTES COMPUT SC, V11956, P351, DOI 10.1007/978-3-030-37429-7_35
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang TY, 2019, PROC CVPR IEEE, P12262, DOI 10.1109/CVPR.2019.01255
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Yasarla R, 2019, PROC CVPR IEEE, P8397, DOI 10.1109/CVPR.2019.00860
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zhou K, 2016, DESTECH TRANS COMP
NR 39
TC 2
Z9 2
U1 1
U2 20
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2021
VL 7
IS 4
BP 467
EP 482
DI 10.1007/s41095-021-0210-3
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UK7UX
UT WOS:000692172600004
OA gold
DA 2024-07-18
ER

PT J
AU Guo, MH
   Liu, ZN
   Mu, TJ
   Liang, D
   Martin, RR
   Hu, SM
AF Guo, Meng-Hao
   Liu, Zheng-Ning
   Mu, Tai-Jiang
   Liang, Dun
   Martin, Ralph R.
   Hu, Shi-Min
TI Can attention enable MLPs to catch up with CNNs?
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
C1 [Guo, Meng-Hao; Liu, Zheng-Ning; Mu, Tai-Jiang; Liang, Dun; Hu, Shi-Min] Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing 100084, Peoples R China.
   [Martin, Ralph R.] Cardiff Univ, Cardiff CF243AA, Wales.
C3 Tsinghua University; Cardiff University
RP Hu, SM (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing 100084, Peoples R China.
EM gmh20@mails.tsinghua.edu.cn; liu-zn17@mails.tsinghua.edu.cn;
   taijiang@tsinghua.edu.cn; liangd16@mails.tsinghua.edu.cn;
   ralph@cs.cf.ac.uk; shimin@tsinghua.edu.cn
RI Hu, Shi-Min/AAW-1952-2020; Martin, Ralph R/D-2366-2010; Mu,
   Tai-Jiang/JWO-1381-2024
OI Mu, Tai-Jiang/0000-0002-9197-346X
FU National Natural Science Foundation of China [61521002]
FX This work was supported by the National Natural Science Foundation of
   China (Project No. 61521002).
CR [Anonymous], 2018, OPENAL BLOG
   Ba J.L., 2016, P ADV NEUR INF PROC
   Brown T. B., 2020, P 34 C NEUR INF PROC
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Devlin J., 2018, BERT PRE TRAINING DE
   Dosovitskiy A., 2021, P INT C LEARN REPR
   Guo M. H., ARXIV PREPRINT ARXIV, P2021
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D., 2016, PROC 5 INT C LEARN R
   Koltun V., 2016, ARXIV PREPRINT ARXIV
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Melas-Kyriazi L., ARXIV PREPRINT ARXIV, P2021
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Rumelhart D.E., 2013, Learning internal representations by error propagation, P399, DOI [10.1016/b978-1-4832-1446-7.50035-2, 10.1016/B978-1-4832-1446-7.50035-2]
   TOLSTIKHIN I., 2021, P 35 C NEUR INF PROC
   Touvron H., ARXIV PREPRINT ARXIV, P2021
   Vaswani A, 2017, ADV NEUR IN, V30
   Yuan L., ARXIV PREPRINT ARXIV, P2021
NR 20
TC 7
Z9 7
U1 1
U2 12
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2021
VL 7
IS 3
BP 283
EP 288
DI 10.1007/s41095-021-0240-x
PG 6
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TR1TJ
UT WOS:000678754100001
OA gold, Green Submitted
DA 2024-07-18
ER

PT J
AU Li, XY
   Wei, GS
   Wang, J
   Zhou, YF
AF Li, Xinyu
   Wei, Guangshun
   Wang, Jie
   Zhou, Yuanfeng
TI Multi-scale joint feature network for micro-expression recognition
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE micro-expression recognition; multi-scale feature; optical flow; deep
   learning
AB Micro-expression recognition is a substantive cross-study of psychology and computer science, and it has a wide range of applications (e.g., psychological and clinical diagnosis, emotional analysis, criminal investigation, etc.). However, the subtle and diverse changes in facial muscles make it difficult for existing methods to extract effective features, which limits the improvement of micro-expression recognition accuracy. Therefore, we propose a multi-scale joint feature network based on optical flow images for micro-expression recognition. First, we generate an optical flow image that reflects subtle facial motion information. The optical flow image is then fed into the multi-scale joint network for feature extraction and classification. The proposed joint feature module (JFM) integrates features from different layers, which is beneficial for the capture of micro-expression features with different amplitudes. To improve the recognition ability of the model, we also adopt a strategy for fusing the feature prediction results of the three JFMs with the backbone network. Our experimental results show that our method is superior to state-of-the-art methods on three benchmark datasets (SMIC, CASME II, and SAMM) and a combined dataset (3DB).
C1 [Li, Xinyu; Wei, Guangshun; Wang, Jie; Zhou, Yuanfeng] Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
C3 Shandong University
RP Zhou, YF (corresponding author), Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
EM xinyuli@mail.sdu.edu.cn; guangshunwei@gmail.com;
   chiehwang@mail.sdu.edu.cn; yfzhou@sdu.edu.cn
RI Zhou, Yuanfeng/AAT-4670-2020
FU NSFC-Zhejiang Joint Fund of the Integration of Informatization and
   Industrialization [U1909210]; National Natural Science Foundation of
   China [61772312]; Fundamental Research Funds of Shandong University
   [2018JC030]
FX The work was supported by the NSFC-Zhejiang Joint Fund of the
   Integration of Informatization and Industrialization under Grant No.
   U1909210, the the National Natural Science Foundation of China under
   Grant No. 61772312, and the Fundamental Research Funds of Shandong
   University (Grant No. 2018JC030).
CR [Anonymous], 2003, METT. micro expression training tool
   [Anonymous], 1966, Methods of research in psychotherapy, DOI [DOI 10.1007/978-1-4684-6045-2_14, 10.1007/978-1-4684-6045-2_14]
   Ben XY, 2018, PATTERN RECOGN LETT, V107, P50, DOI 10.1016/j.patrec.2017.07.010
   Davison AK, 2018, IEEE T AFFECT COMPUT, V9, P116, DOI 10.1109/TAFFC.2016.2573832
   EKMAN P, 1969, PSYCHIATR, V32, P88, DOI 10.1080/00332747.1969.11023575
   Gan YS, 2019, SIGNAL PROCESS-IMAGE, V74, P129, DOI 10.1016/j.image.2019.02.005
   Gedeon T, 2019, IEEE INT CONF AUTOMA, P1, DOI [DOI 10.1017/S0033291719003349, DOI 10.1109/fg.2019.8756583]
   Huang XH, 2019, IEEE T AFFECT COMPUT, V10, P32, DOI 10.1109/TAFFC.2017.2713359
   Huang XH, 2016, NEUROCOMPUTING, V175, P564, DOI 10.1016/j.neucom.2015.10.096
   Huang XH, 2013, LECT NOTES COMPUT SC, V7944, P1
   Khor HQ, 2019, IEEE IMAGE PROC, P36, DOI [10.1109/icip.2019.8802965, 10.1109/ICIP.2019.8802965]
   Khor HQ, 2018, IEEE INT CONF AUTOMA, P667, DOI 10.1109/FG.2018.00105
   Kim DH, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P382, DOI 10.1145/2964284.2967247
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Le Ngo AC, 2015, LECT NOTES COMPUT SC, V9006, P33, DOI 10.1007/978-3-319-16817-3_3
   Li XB, 2013, IEEE INT CONF AUTOMA, DOI 10.1109/FG.2013.6553717
   Li YT, 2018, IEEE IMAGE PROC, P3094, DOI 10.1109/ICIP.2018.8451376
   Liong ST, 2019, IEEE INT CONF AUTOMA, P658, DOI 10.1109/fg.2019.8756567
   Liong ST, 2018, SIGNAL PROCESS-IMAGE, V62, P82, DOI 10.1016/j.image.2017.11.006
   Liu YJ, 2021, IEEE T AFFECT COMPUT, V12, P254, DOI 10.1109/TAFFC.2018.2854166
   Liu YJ, 2016, IEEE T AFFECT COMPUT, V7, P299, DOI 10.1109/TAFFC.2015.2485205
   Peng M, 2019, INT CONF AFFECT, DOI [10.1109/ACII.2019.8925525, 10.1109/acii.2019.8925525]
   Peng M, 2018, IEEE INT CONF AUTOMA, P657, DOI 10.1109/FG.2018.00103
   Peng S, 2020, NEUROCOMPUTING, V411, P9, DOI 10.1016/j.neucom.2020.05.022
   POLIKOVSKY S., 2009, 3 INT C IM CRIM DET, P1, DOI [10.1049/ic.2009.0244, DOI 10.1049/IC.2009.0244]
   Quang N. V., 2019, P 2019 14 IEEE INT C, P1
   See J, 2019, IEEE INT CONF AUTOMA, P647
   SHREVE M., 2009, 2009 WORKSH APPL COM, P1, DOI [10.1109/WACV.2009.5403044, DOI 10.1109/WACV.2009.5403044]
   Sun DQ, 2014, INT J COMPUT VISION, V106, P115, DOI 10.1007/s11263-013-0644-x
   Wang CY, 2020, NEUROCOMPUTING, V410, P354, DOI 10.1016/j.neucom.2020.06.005
   Wang S, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2747-y
   Wang SJ, 2018, NEUROCOMPUTING, V312, P251, DOI 10.1016/j.neucom.2018.05.107
   Wang SJ, 2014, INT C PATT RECOG, P4678, DOI 10.1109/ICPR.2014.800
   Wang YD, 2015, LECT NOTES COMPUT SC, V9003, P525, DOI 10.1007/978-3-319-16865-4_34
   Wu HY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185561
   Xia ZQ, 2020, IEEE T MULTIMEDIA, V22, P626, DOI 10.1109/TMM.2019.2931351
   Xu F, 2017, IEEE T AFFECT COMPUT, V8, P254, DOI 10.1109/TAFFC.2016.2518162
   Yan WJ, 2013, IEEE INT CONF AUTOMA
   Yan WJ, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0086041
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhou L, 2019, IEEE INT CONF AUTOMA, P642
NR 42
TC 6
Z9 7
U1 1
U2 34
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2021
VL 7
IS 3
BP 407
EP 417
DI 10.1007/s41095-021-0217-9
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TR1TJ
UT WOS:000678754100009
OA gold
DA 2024-07-18
ER

PT J
AU Huo, Y
   Yoon, SE
AF Huo, Yuchi
   Yoon, Sung-eui
TI A survey on deep learning-based Monte Carlo denoising
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review
DE rendering; Monte Carlo (MC) denoising; deep learning; ray tracing
ID RECONSTRUCTION; NETWORKS
AB Monte Carlo (MC) integration is used ubiquitously in realistic image synthesis because of its flexibility and generality. However, the integration has to balance estimator bias and variance, which causes visually distracting noise with low sample counts. Existing solutions fall into two categories, in-process sampling schemes and post-processing reconstruction schemes. This report summarizes recent trends in the post-processing reconstruction scheme. Recent years have seen increasing attention and significant progress in denoising MC rendering with deep learning, by training neural networks to reconstruct denoised rendering results from sparse MC samples. Many of these techniques show promising results in real-world applications, and this report aims to provide an assessment of these approaches for practitioners and researchers.
C1 [Huo, Yuchi; Yoon, Sung-eui] Korea Adv Inst Sci & Technol, Daejeon 31414, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST)
RP Yoon, SE (corresponding author), Korea Adv Inst Sci & Technol, Daejeon 31414, South Korea.
EM huo.yuchi.sc@gmail.com; sungeui@kaist.edu
RI Yoon, Sung-eui/C-1678-2011
FU National Research Foundation of Korea (NRF) grant (MSIT)
   [2019R1A2C3002833]
FX This work was supported by National Research Foundation of Korea (NRF)
   grant (MSIT) (No. 2019R1A2C3002833).
CR Alsaiari A, 2019, 2019 IEEE 2ND INTERNATIONAL CONFERENCE ON INFORMATION AND COMPUTER TECHNOLOGIES (ICICT), P126, DOI [10.1109/INFOCT.2019.8710893, 10.1109/infoct.2019.8710893]
   [Anonymous], 1989, P ADV NEURAL INFORM
   [Anonymous], 2016, PHYS BASED RENDERING
   [Anonymous], 2002, P 18 SPR C COMP GRAP, DOI [DOI 10.1145/584458.584476, 10.1145/584458.584476]
   [Anonymous], 2017, ACM T GRAPHIC, DOI DOI 10.1145/3072959.3073601
   Back J, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417847
   Bako S, 2019, COMPUT GRAPH FORUM, V38, P527, DOI 10.1111/cgf.13858
   Bako S, 2017, ACM T GRAPHIC, V36, DOI [10.1145/3072959.3073703, 10.1145/3072959.3073708]
   Ballard D. H., 1987, AAAI, P279
   Belcour L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487239
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   Dahlberg H, 2019, SIGGRAPH '19 -ACM SIGGRAPH 2019 TALKS, DOI 10.1145/3306307.3328150
   Dammertz H., 2010, P C HIGH PERF GRAPH, P67, DOI DOI 10.5555/1921479.1921491
   Drebin R. A., 1988, Computer Graphics, V22, P65, DOI 10.1145/378456.378484
   Durand F, 2005, ACM T GRAPHIC, V24, P1115, DOI 10.1145/1073204.1073320
   Gharbi M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322954
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Guo J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356538
   Hachisuka T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409083
   Hanika J, 2015, COMPUT GRAPH FORUM, V34, P87, DOI 10.1111/cgf.12681
   Hasselgren J, 2020, COMPUT GRAPH FORUM, V39, P147, DOI 10.1111/cgf.13919
   Hastie T., 2009, The Elements of Statistical Learning
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hofmann N., 2020, P ACM COMP GRAPH INT
   Hua BS, 2019, COMPUT GRAPH FORUM, V38, P455, DOI 10.1111/cgf.13652
   Huang Y, 2015, ADV NEUR IN, V28
   Huo YC, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3368313
   Jensen HW, 1995, SPRING COMP SCI, P326, DOI 10.1007/978-3-7091-9430-0_31
   Jensen HW., 2001, REALISTIC IMAGE SYNT, DOI [10.1201/9780429294907, DOI 10.1201/9780429294907]
   Jiang G, 2021, COMPUT GRAPH-UK, V94, P22, DOI 10.1016/j.cag.2020.09.007
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Kalantari NK, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766977
   Kallweit S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130880
   Kang CM, 2016, FRONT INFORM TECH EL, V17, P185, DOI 10.1631/FITEE.1500251
   Kettunen M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766997
   Kettunen M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323038
   Kuznetsov A, 2018, COMPUT GRAPH FORUM, V37, P35, DOI 10.1111/cgf.13473
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lehtinen J, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461943
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Li TM, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366213
   Liang YL, 2020, IEEE T VIS COMPUT GR, V26, P2961, DOI 10.1109/TVCG.2019.2909875
   Lin WH, 2020, COMPUT VIS MEDIA, V6, P157, DOI 10.1007/s41095-020-0167-7
   Lin WH, 2021, COMPUT GRAPH FORUM, V40, P369, DOI 10.1111/cgf.14194
   MAX N, 1995, IEEE T VIS COMPUT GR, V1, P99, DOI 10.1109/2945.468400
   Mehta SU, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366182
   Meng X., 2020, P EUR S REND
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Munkberg J, 2020, COMPUT GRAPH FORUM, V39, P1, DOI 10.1111/cgf.14049
   Nvidia, 2020, INT REC M CARLO IM S
   Panin M, 2019, SA'19: SIGGRAPH ASIA 2019 TECHNICAL BRIEFS, P21, DOI 10.1145/3355088.3365150
   Rosenblatt F., 1961, AD0256582 CORN AER L
   Rubinstein R.Y., 2016, Simulation and the Monte Carlo method
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632
   Vincent P., 2008, INT C MACH LEARN ICM, P1096, DOI [10.1145/1390156.1390294, DOI 10.1145/1390156.1390294]
   Vogels T, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201388
   Wong KM, 2019, COMPUT VIS MEDIA, V5, P239, DOI 10.1007/s41095-019-0142-3
   Xing QW, 2020, IEEE ACCESS, V8, P116336, DOI 10.1109/ACCESS.2020.2999891
   Xu B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356547
   Xu ZL, 2020, COMPUT GRAPH FORUM, V39, P193, DOI 10.1111/cgf.14137
   Yang X, 2019, J COMPUT SCI TECH-CH, V34, P1123, DOI 10.1007/s11390-019-1964-2
   Yang X, 2019, IEEE ACCESS, V7, P21177, DOI 10.1109/ACCESS.2018.2886005
   Zeng Z, 2020, J COMPUT SCI TECH-CH, V35, P506, DOI 10.1007/s11390-020-0264-1
   Zhu SL, 2020, COMPUT GRAPH FORUM, V39, P35, DOI 10.1111/cgf.14052
   Zwicker M, 2015, COMPUT GRAPH FORUM, V34, P667, DOI 10.1111/cgf.12592
NR 69
TC 31
Z9 34
U1 2
U2 65
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2021
VL 7
IS 2
BP 169
EP 185
DI 10.1007/s41095-021-0209-9
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FR
UT WOS:000648692900002
OA gold
DA 2024-07-18
ER

PT J
AU Zhang, FLE
   Barnes, C
   Zhang, HT
   Zhao, JH
   Salas, G
AF Zhang, Fang-Lue
   Barnes, Connelly
   Zhang, Hao-Tian
   Zhao, Junhong
   Salas, Gabriel
TI Coherent video generation for multiple hand-held cameras with dynamic
   foreground
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE video editing; smooth temporal transitions; dynamic foreground; multiple
   cameras; hand-held cameras
ID PANORAMIC VIDEO; IMAGE
AB For many social events such as public performances, multiple hand-held cameras may capture the same event. This footage is often collected by amateur cinematographers who typically have little control over the scene and may not pay close attention to the camera. For these reasons, each individually captured video may fail to cover the whole time of the event, or may lose track of interesting foreground content such as a performer. We introduce a new algorithm that can synthesize a single smooth video sequence of moving foreground objects captured by multiple hand-held cameras. This allows later viewers to gain a cohesive narrative experience that can transition between different cameras, even though the input footage may be less than ideal. We first introduce a graph-based method for selecting a good transition route. This allows us to automatically select good cut points for the hand-held videos, so that smooth transitions can be created between the resulting video shots. We also propose a method to synthesize a smooth photorealistic transition video between each pair of hand-held cameras, which preserves dynamic foreground content during this transition. Our experiments demonstrate that our method outperforms previous state-of-the-art methods, which struggle to preserve dynamic foreground content.
C1 [Zhang, Fang-Lue; Zhao, Junhong; Salas, Gabriel] Victoria Univ Wellington, Wellington 6012, New Zealand.
   [Barnes, Connelly] Adobe Res, Seattle, WA USA.
   [Zhang, Hao-Tian] Stanford Univ, San Francisco, CA USA.
C3 Victoria University Wellington; Adobe Systems Inc.; Stanford University
RP Zhang, FLE (corresponding author), Victoria Univ Wellington, Wellington 6012, New Zealand.
EM fanglue.zhang@vuw.ac.nz; cbarnes@adobe.com; haotianz@cs.stanford.edu;
   junhong.zhao@vuw.ac.nz; gabrielsalas@vuw.ac.nz
RI Zhang, Haotian/IAM-0232-2023
FU Research Establishment Grant of Victoria University of Wellington
   [8-1620-216786-3744]; Victoria Research Excellence Award
FX This work was supported by a Research Establishment Grant of Victoria
   University of Wellington (Project No. 8-1620-216786-3744) and a Victoria
   Research Excellence Award.
CR Agarwala A, 2005, ACM T GRAPHIC, V24, P821, DOI 10.1145/1073204.1073268
   Anderson R, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980257
   [Anonymous], 2016, P ACM SIGGRAPH 2016
   [Anonymous], 2012, ACM T GRAPHIC, DOI DOI 10.1145/2185520.2185564
   Arev I, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601198
   Ballan L, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778824
   Barnes C, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778826
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Carranza J, 2003, ACM T GRAPHIC, V22, P569, DOI 10.1145/882262.882309
   Cheng MM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778820
   Collet A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766945
   Cui ZP, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073612
   El-Saban M.A., 2009, Proceedings_of_the_17th_ACM_international_conference_on_ Multimedia, P1009
   Guo H, 2018, IEEE T COMPUT IMAG, V4, P573, DOI 10.1109/TCI.2018.2866227
   Guo H, 2016, IEEE T IMAGE PROCESS, V25, P5491, DOI 10.1109/TIP.2016.2607419
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Lee SM, 2005, COLOR RES APPL, V30, P265, DOI 10.1002/col.20122
   Lee WT, 2017, COMPUT GRAPH FORUM, V36, P115, DOI 10.1111/cgf.13277
   Lin KM, 2016, COMPUT GRAPH FORUM, V35, P479, DOI 10.1111/cgf.12848
   Lin WY, 2011, PROC CVPR IEEE, P345, DOI 10.1109/CVPR.2011.5995314
   Liu F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531350
   Liu QX, 2020, MULTIMED TOOLS APPL, V79, P3107, DOI 10.1007/s11042-018-6337-2
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu Q, 2011, PR ELECTROMAGN RES S, P576
   Ma TZ, 2020, IEEE T VIS COMPUT GR, V26, P3163, DOI 10.1109/TVCG.2019.2923196
   Newson A, 2014, SIAM J IMAGING SCI, V7, P1993, DOI 10.1137/140954933
   Nie YW, 2018, IEEE T IMAGE PROCESS, V27, P164, DOI 10.1109/TIP.2017.2736603
   Perazzi F, 2015, COMPUT GRAPH FORUM, V34, P57, DOI 10.1111/cgf.12541
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Szeliski R., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P251, DOI 10.1145/258734.258861
   Wang M, 2020, COMPUT VIS MEDIA, V6, P3, DOI 10.1007/s41095-020-0162-z
   Wang M, 2018, IEEE T IMAGE PROCESS, V27, P5854, DOI 10.1109/TIP.2018.2859628
   Wang O, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601208
   Wei XL, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778779
   Wu C., 2011, VisualSFM: A visual structure from motion system
   Wu X, 2020, COMPUT VIS MEDIA, V6, P215, DOI 10.1007/s41095-020-0168-6
   Zaragoza J, 2014, IEEE T PATTERN ANAL, V36, P1285, DOI 10.1109/TPAMI.2013.247
   Zhanabaev Z, 2017, PROCEEDINGS OF THE SIXTH INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS AND REMOTE SENSING (ICTRS 2017), P17, DOI 10.1145/3152808.3152811
   Zhang FL, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980243
   Zhang Y., 2018, ARXIV181011220
   Zhu Z, 2018, IEEE T IMAGE PROCESS, V27, P2952, DOI 10.1109/TIP.2018.2808766
NR 43
TC 3
Z9 3
U1 1
U2 13
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2020
VL 6
IS 3
BP 291
EP 306
DI 10.1007/s41095-020-0187-3
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FH
UT WOS:000648691900005
OA gold
DA 2024-07-18
ER

PT J
AU Fan, RC
   Cheng, MM
   Hou, QB
   Mu, TJ
   Wang, JD
   Hu, SM
AF Fan, Ruochen
   Cheng, Ming-Ming
   Hou, Qibin
   Mu, Tai-Jiang
   Wang, Jingdong
   Hu, Shi-Min
TI S4Net: Single stage salient-instance segmentation
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE salient-instance segmentation; salient object detection; single stage;
   region-of-interest masking
ID OBJECT DETECTION; GRADIENTS
AB In this paper, we consider salient instance segmentation. As well as producing bounding boxes, our network also outputs high-quality instance-level segments as initial selections to indicate the regions of interest. Taking into account the category-independent property of each target, we design a single stage salient instance segmentation framework, with a novel segmentation branch. Our new branch regards not only local context inside each detection window but also the surrounding context, enabling us to distinguish instances in the same scope even with partial occlusion. Our network is end-to-end trainable and is fast (running at 40 fps for images with resolution 320 x 320). We evaluate our approach on a publicly available benchmark and show that it outperforms alternative solutions. We also provide a thorough analysis of our design choices to help readers better understand the function of each part of our network. Source code can be found at https://github.com/RuochenFan/S4Net.
C1 [Fan, Ruochen; Mu, Tai-Jiang; Hu, Shi-Min] Tsinghua Univ, BNRist, Beijing 100086, Peoples R China.
   [Cheng, Ming-Ming; Hou, Qibin] Nankai Univ, Tianjin 300071, Peoples R China.
   [Wang, Jingdong] MSRA, Beijing 100086, Peoples R China.
C3 Tsinghua University; Nankai University
RP Hu, SM (corresponding author), Tsinghua Univ, BNRist, Beijing 100086, Peoples R China.
EM frc16@mails.tsinghua.edu.cn; cmm@nankai.edu.cn; andrewhoux@gmail.com;
   mmmutj@gmail.com; jingdw@microsoft.com; shimin@tsinghua.edu.cn
RI Hu, Shi-Min/AAW-1952-2020; Cheng, Ming-Ming/A-2527-2009; fan,
   ruochen/AAQ-1758-2021; Mu, Tai-Jiang/JWO-1381-2024; Wang,
   Jingdong/E-9920-2017
OI Cheng, Ming-Ming/0000-0001-5550-8758; Mu, Tai-Jiang/0000-0002-9197-346X;
   Wang, Jingdong/0000-0002-4888-4445
FU National Natural Science Foundation of China [61521002, 61572264,
   61620106008]; National Youth Talent Support Program, Tianjin Natural
   Science Foundation [17JCJQJC43700, 18ZXZNGX00110]; Fundamental Research
   Funds for the Central Universities (Nankai University) [63191501]
FX This research was supported by National Natural Science Foundation of
   China (61521002, 61572264, 61620106008), the National Youth Talent
   Support Program, Tianjin Natural Science Foundation (17JCJQJC43700,
   18ZXZNGX00110), and the Fundamental Research Funds for the Central
   Universities (Nankai University, No. 63191501).
CR Abadi M, ARXIV, DOI DOI 10.48550/ARXIV.1603.04467
   Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2010, ACM T GRAPHICS, V29, P4
   [Anonymous], 2009, ACM T GRAPHICS, V28, P5
   [Anonymous], 2014, P ROB SCI SYST
   [Anonymous], 2010, ACM T GRAPHICS, V29, P6
   [Anonymous], 2015, COMPUT VIS MEDIA
   [Anonymous], 2013, ARXIV PREPRINT ARXIV, DOI [DOI 10.48550/ARXIV.1312.6229, 10.48550/arXiv.1312.6229]
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Borji A, 2019, COMPUT VIS MEDIA, V5, P117, DOI 10.1007/s41095-019-0149-9
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Dai JF, 2016, LECT NOTES COMPUT SC, V9910, P534, DOI 10.1007/978-3-319-46466-4_32
   Dai JF, 2015, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2015.7299025
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   DESIMONE R, 1995, ANNU REV NEUROSCI, V18, P193, DOI 10.1146/annurev-psych-122414-033400
   Elazary L, 2008, J VISION, V8, DOI 10.1167/8.3.3
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fan DP, 2018, LECT NOTES COMPUT SC, V11219, P196, DOI 10.1007/978-3-030-01267-0_12
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Hariharan B, 2015, PROC CVPR IEEE, P447, DOI 10.1109/CVPR.2015.7298642
   Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Howard A. G., 2017, arXiv
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Kolesnikov A, 2016, LECT NOTES COMPUT SC, V9908, P695, DOI 10.1007/978-3-319-46493-0_42
   Lee GY, 2016, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2016.78
   Li FF, 2002, P NATL ACAD SCI USA, V99, P9596, DOI 10.1073/pnas.092277599
   Li GB, 2017, PROC CVPR IEEE, P247, DOI 10.1109/CVPR.2017.34
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li Y, 2017, PROC CVPR IEEE, P4438, DOI 10.1109/CVPR.2017.472
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mannan SK, 2009, CURR BIOL, V19, pR247, DOI 10.1016/j.cub.2009.02.020
   Pinheiro P.O., 2015, Learning to Segment Object Candidates
   Pont-Tuset J, 2017, IEEE T PATTERN ANAL, V39, P128, DOI 10.1109/TPAMI.2016.2537320
   Qibin Hou, 2018, Energy Minimization Methods in Computer Vision and Pattern Recognition. 11th International Conference, EMMCVPR 2017. Revised Selected Papers: LNCS 10746, P263, DOI 10.1007/978-3-319-78199-0_18
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wang JD, 2017, INT J COMPUT VISION, V123, P251, DOI 10.1007/s11263-016-0977-3
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wei Y C, 2017, segmentation approach, V6488, P6496
   Wei YC, 2017, IEEE T PATTERN ANAL, V39, P2314, DOI 10.1109/TPAMI.2016.2636150
   Wolfe JM, 2004, NAT REV NEUROSCI, V5, P495, DOI 10.1038/nrn1411
   Yosinski J., 2015, ARXIV150606579, V2015, P12
   Zhang JM, 2016, PROC CVPR IEEE, P5733, DOI 10.1109/CVPR.2016.618
   Zhang J, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901336
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 70
TC 15
Z9 16
U1 1
U2 15
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2020
VL 6
IS 2
BP 191
EP 204
DI 10.1007/s41095-020-0173-9
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FB
UT WOS:000648691300006
OA gold, Green Submitted
DA 2024-07-18
ER

PT J
AU Xu, JZ
   Yuan, MK
   Yan, DM
   Wu, TR
AF Xu, Jingzhao
   Yuan, Mengke
   Yan, Dong-Ming
   Wu, Tieru
TI Deep unfolding multi-scale regularizer network for image denoising
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE image denoising; deep unfolding network; multi-scale regularizer; deep
   learning
ID NONLOCAL IMAGE; SPARSE
AB Existing deep unfolding methods unroll an optimization algorithm with a fixed number of steps, and utilize convolutional neural networks (CNNs) to learn data-driven priors. However, their performance is limited for two main reasons. Firstly, priors learned in deep feature space need to be converted to the image space at each iteration step, which limits the depth of CNNs and prevents CNNs from exploiting contextual information. Secondly, existing methods only learn deep priors at the single full-resolution scale, so ignore the benefits of multi-scale context in dealing with high level noise. To address these issues, we explicitly consider the image denoising process in the deep feature space and propose the deep unfolding multi-scale regularizer network (DUMRN) for image denoising. The core of DUMRN is the feature-based denoising module (FDM) that directly removes noise in the deep feature space. In each FDM, we construct a multi-scale regularizer block to learn deep prior information from multi-resolution features. We build the DUMRN by stacking a sequence of FDMs and train it in an end-to-end manner. Experimental results on synthetic and real-world benchmarks demonstrate that DUMRN performs favorably compared to state-of-the-art methods.
C1 [Xu, Jingzhao; Wu, Tieru] Jilin Univ, Sch Math, Changchun 130012, Peoples R China.
   [Yuan, Mengke; Yan, Dong-Ming] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Yuan, Mengke; Yan, Dong-Ming] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
C3 Jilin University; Chinese Academy of Sciences; Institute of Automation,
   CAS; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS
RP Wu, TR (corresponding author), Jilin Univ, Sch Math, Changchun 130012, Peoples R China.
EM wutr@jlu.edu.cn
FU National Key R&D Program of China; National Nature Science Foundation of
   China;  [2020YFA0714101];  [61872162];  [62102414];  [62172415]; 
   [52175493]
FX AcknowledgementsThis work was partially supported by the National Key
   R&D Program of China (No. 2020YFA0714101) and the National Nature
   Science Foundation of China (Nos. 61872162, 62102414, 62172415, and
   52175493).
CR Abdelhamed A, 2018, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2018.00182
   Adler J, 2018, IEEE T MED IMAGING, V37, P1322, DOI 10.1109/TMI.2018.2799231
   Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Anaya J, 2018, J VIS COMMUN IMAGE R, V51, P144, DOI 10.1016/j.jvcir.2018.01.012
   Anwar S, 2021, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2021.3131739
   Barbu A, 2009, IEEE T IMAGE PROCESS, V18, P2451, DOI 10.1109/TIP.2009.2028254
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Buades A, 2008, INT J COMPUT VISION, V76, P123, DOI 10.1007/s11263-007-0052-1
   Chang M., 2020, EUR C COMP VIS, P171
   Chen F, 2015, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2015.76
   Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743
   Dabov K, 2007, IEEE IMAGE PROC, P313, DOI 10.1109/icip.2007.4378954
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dai SY, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1039
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P700, DOI 10.1109/TIP.2012.2221729
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Feng WS, 2017, SIAM J IMAGING SCI, V10, P1234, DOI 10.1137/16M1093707
   Franzen R, 1999, Kodak lossless true color image suite
   Glorot X., 2010, 13 INT C ARTIFICIAL, V9, P249
   Gong D, 2020, IEEE T NEUR NET LEAR, V31, P5468, DOI 10.1109/TNNLS.2020.2968289
   Gu SH, 2019, IEEE I CONF COMP VIS, P2511, DOI 10.1109/ICCV.2019.00260
   Gu SH, 2017, INT J COMPUT VISION, V121, P183, DOI 10.1007/s11263-016-0930-5
   Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181
   Haris M, 2021, IEEE T PATTERN ANAL, V43, P4323, DOI 10.1109/TPAMI.2020.3002836
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Huang T, 2021, PROC CVPR IEEE, P14776, DOI 10.1109/CVPR46437.2021.01454
   Ioffe S., 2015, P INT C MACH LEARN, VVolume 1, P448, DOI DOI 10.48550/ARXIV.1502.03167
   IRANI M, 1991, CVGIP-GRAPH MODEL IM, V53, P231, DOI 10.1016/1049-9652(91)90045-L
   Jian Sun, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2745, DOI 10.1109/CVPR.2011.5995520
   Kim Y, 2020, PROC CVPR IEEE, P3479, DOI 10.1109/CVPR42600.2020.00354
   Kingma D, 2014, ICLR P, V2014, P1
   Lan XY, 2006, LECT NOTES COMPUT SC, V3952, P269
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu D, 2018, ADV NEUR IN, V31
   Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mou C, 2022, IEEE T MULTIMEDIA, V24, P1366, DOI 10.1109/TMM.2021.3063916
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Plötz T, 2017, PROC CVPR IEEE, P2750, DOI 10.1109/CVPR.2017.294
   Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6
   Ren C, 2021, PROC CVPR IEEE, P8592, DOI 10.1109/CVPR46437.2021.00849
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roth S, 2009, INT J COMPUT VISION, V82, P205, DOI 10.1007/s11263-008-0197-6
   Schmidt U, 2014, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2014.349
   Simon D., 2019, Advances in Neural Information Processing Systems, P2274
   Sutskever I., 2013, INT C MACHINE LEARNI, P1139
   Tian CW, 2021, KNOWL-BASED SYST, V226, DOI 10.1016/j.knosys.2021.106949
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu XC, 2020, COMPUT VIS MEDIA, V6, P319, DOI 10.1007/s41095-020-0176-6
   Xie Q, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-020-2880-3
   Yang Y, 2016, 30 C NEURAL INFORM P, V29
   Yu K, 2022, IEEE T PATTERN ANAL, V44, P7078, DOI 10.1109/TPAMI.2021.3096255
   Yu XJ, 2022, SIGNAL IMAGE VIDEO P, V16, P257, DOI 10.1007/s11760-021-01984-5
   Zamir Syed Waqas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P492, DOI 10.1007/978-3-030-58595-2_30
   Zhang K, 2020, PROC CVPR IEEE, P3214, DOI 10.1109/CVPR42600.2020.00328
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang YL, 2021, IEEE T PATTERN ANAL, V43, P2480, DOI 10.1109/TPAMI.2020.2968521
   Zontak M, 2013, PROC CVPR IEEE, P1195, DOI 10.1109/CVPR.2013.158
NR 65
TC 5
Z9 6
U1 7
U2 52
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2023
VL 9
IS 2
BP 335
EP 350
DI 10.1007/s41095-022-0277-5
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7N8FL
UT WOS:000907570700008
OA gold
DA 2024-07-18
ER

PT J
AU Cui, MY
   Zhu, Z
   Yang, YL
   Lu, SP
AF Cui, Meng-Yao
   Zhu, Zhe
   Yang, Yulu
   Lu, Shao-Ping
TI Towards natural object-based image recoloring
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE color editing; object recognition; color palette representation; natural
   color
ID COLOR
AB Existing color editing algorithms enable users to edit the colors in an image according to their own aesthetics. Unlike artists who have an accurate grasp of color, ordinary users are inexperienced in color selection and matching, and allowing non-professional users to edit colors arbitrarily may lead to unrealistic editing results. To address this issue, we introduce a palette-based approach for realistic object-level image recoloring. Our data-driven approach consists of an offline learning part that learns the color distributions for different objects in the real world, and an online recoloring part that first recognizes the object category, and then recommends appropriate realistic candidate colors learned in the offline step for that category. We also provide an intuitive user interface for efficient color manipulation. After color selection, image matting is performed to ensure smoothness of the object boundary. Comprehensive evaluation on various color editing examples demonstrates that our approach outperforms existing state-of-the-art color editing algorithms.
C1 [Cui, Meng-Yao; Yang, Yulu; Lu, Shao-Ping] Nankai Univ, TKLNDST, CS, Tianjin 300350, Peoples R China.
   [Zhu, Zhe] Duke Univ, Dept Radiol, Durham, NC 27705 USA.
C3 Nankai University; Duke University
RP Lu, SP (corresponding author), Nankai Univ, TKLNDST, CS, Tianjin 300350, Peoples R China.
EM cuimengyao@mail.nankai.edu.cn; zhe.zhu@duke.edu; slu@nankai.edu.cn;
   slu@nankai.edu.cn
OI CUI, Mengyao/0009-0001-0149-7923
FU National Natural Science Foundation of China [61972216, 62111530097];
   NSF of Tianjin City [18JCYBJC41300, 18ZXZNGX00110]
FX This work was supported by National Natural Science Foundation of China
   (Grant Nos. 61972216 and 62111530097) and NSF of Tianjin City (Grant
   Nos. 18JCYBJC41300 and 18ZXZNGX00110).
CR Aharoni-Mack E., 2017, Proceedings of the Symposium on Non-Photorealistic Animation and Rendering, NPAR'17, DOI DOI 10.1145/3092919.3092926
   An XB, 2010, COMPUT GRAPH FORUM, V29, P263, DOI 10.1111/j.1467-8659.2009.01595.x
   An XB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360639
   [Anonymous], 2015, COMPUT VIS ME DIA
   Bahng H, 2018, LECT NOTES COMPUT SC, V11216, P443, DOI 10.1007/978-3-030-01258-8_27
   Chen K., 2019, arXiv preprint arXiv:1906.07155
   Chen QF, 2013, IEEE T PATTERN ANAL, V35, P2175, DOI 10.1109/TPAMI.2013.18
   Cho J, 2017, IEEE COMPUT SOC CONF, P1058, DOI 10.1109/CVPRW.2017.143
   Chuang YY, 2001, PROC CVPR IEEE, P264
   Cui MY, 2020, COMPUT VIS MEDIA, V6, P265, DOI 10.1007/s41095-020-0180-x
   Currim S, 2017, ACM T DATABASE SYST, V42, DOI 10.1145/2996454
   Deshpande A, 2017, PROC CVPR IEEE, P2877, DOI 10.1109/CVPR.2017.307
   Duchene S., 2017, P S NONPH AN REND LO
   Faridul HS, 2016, COMPUT GRAPH FORUM, V35, P59, DOI 10.1111/cgf.12671
   Gupta R. K., 2012, P 20 ACM INT C MULT, P369, DOI DOI 10.1145/2393347.2393402
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hu ZY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3320, DOI 10.1145/3394171.3413853
   HUANG, 2011, ACM T GRAPHIC, V30
   HUANG J, 2019, ACM T GRAPHIC, V37, P262
   Huang J, 2018, PROC SPIE, V10806, DOI 10.1117/12.2502935
   Huang YF, 2018, COMPUT GRAPH FORUM, V37, P421, DOI 10.1111/cgf.13579
   Ironi R., 2005, RENDERING TECHNIQUES, P201
   Jheng-Wei Su, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7965, DOI 10.1109/CVPR42600.2020.00799
   Jing YH, 2021, IEEE T CYBERNETICS, V51, P568, DOI 10.1109/TCYB.2019.2904768
   Kang JM, 2018, IEEE IMAGE PROC, P2252, DOI 10.1109/ICIP.2018.8451526
   Kutz P, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073665
   Lee J, 2020, VISUAL COMPUT, V36, P2129, DOI 10.1007/s00371-020-01921-6
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li B, 2020, IEEE T IMAGE PROCESS, V29, P5408, DOI 10.1109/TIP.2020.2980962
   Li ZQ, 2020, LECT NOTES COMPUT SC, V11961, P127, DOI 10.1007/978-3-030-37731-1_11
   Liang LY, 2013, IEEE SYS MAN CYBERN, P2922, DOI 10.1109/SMC.2013.498
   LIN, 2010, ACM T GRAPHIC, V29, P6
   Lin S., 2013, P ACM SIGCHI C HUM F, P3101
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lu H, 2019, IEEE I CONF COMP VIS, P3265, DOI 10.1109/ICCV.2019.00336
   Lu SP, 2021, IEEE T IMAGE PROCESS, V30, P1072, DOI 10.1109/TIP.2020.3042064
   Lu SP, 2015, IEEE T MULTIMEDIA, V17, P577, DOI 10.1109/TMM.2015.2412879
   Lu SP, 2016, TSINGHUA SCI TECHNOL, V21, P678, DOI 10.1109/TST.2016.7787010
   LUAN, 2013, ACM T GRAPHIC, V32, P6
   Luan Q., 2007, Proceedings of the 18th Eurographics conference on Rendering Techniques, P309
   Luo MR, 2001, COLOR RES APPL, V26, P340, DOI 10.1002/col.1049
   Ma S, 2018, PROC CVPR IEEE, P5657, DOI 10.1109/CVPR.2018.00593
   Nguyen RMH, 2017, COMPUT GRAPH FORUM, V36, P83, DOI 10.1111/cgf.13274
   Phan HQ, 2018, IEEE T VIS COMPUT GR, V24, P1942, DOI 10.1109/TVCG.2017.2697948
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Royer A., 2017, P BRIT MACH VIS C
   Shen X, 2016, LECT NOTES COMPUT SC, V9905, P92, DOI 10.1007/978-3-319-46448-0_6
   Sun J, 2004, ACM T GRAPHIC, V23, P315, DOI 10.1145/1015706.1015721
   Sun X, 2021, Advances in Artificial Intelligence, Software and Systems Engineering. AHFE 2020. Advances in Intelligent Systems and Computing, V1213, DOI [10.1007/978-3-030-51328-3_14, DOI 10.1007/978-3-030-51328-3_14]
   Tan JC, 2019, IEEE T VIS COMPUT GR, V25, P2791, DOI 10.1109/TVCG.2018.2858238
   Wang YL, 2019, COMPUT GRAPH FORUM, V38, P11, DOI 10.1111/cgf.13812
   Welsh T, 2002, ACM T GRAPHIC, V21, P277, DOI 10.1145/566570.566576
   Wu X, 2020, COMPUT VIS MEDIA, V6, P215, DOI 10.1007/s41095-020-0168-6
   Xu HY, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925916
   Xu K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618464
   Xu N, 2017, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.2017.41
   Ye SQ, 2017, SIGNAL PROCESS-IMAGE, V53, P40, DOI 10.1016/j.image.2017.01.004
   ZHANG, 2015, ACM T GRAPHIC, V34
   Zhang Q, 2017, IEEE T IMAGE PROCESS, V26, P1952, DOI 10.1109/TIP.2017.2671779
NR 60
TC 2
Z9 2
U1 3
U2 23
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2022
VL 8
IS 2
BP 317
EP 328
DI 10.1007/s41095-021-0245-5
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XJ0YS
UT WOS:000726525400010
OA gold
DA 2024-07-18
ER

PT J
AU Ma, X
   Li, XM
   Zhou, YF
   Zhang, CM
AF Ma, Xiang
   Li, Xuemei
   Zhou, Yuanfeng
   Zhang, Caiming
TI Image smoothing based on global sparsity decomposition and a variable
   parameter
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE image smoothing; texture removal; global sparse decomposition; Bessel
   method
ID ALGORITHM
AB Smoothing images, especially with rich texture, is an important problem in computer vision. Obtaining an ideal result is difficult due to complexity, irregularity, and anisotropicity of the texture. Besides, some properties are shared by the texture and the structure in an image. It is a hard compromise to retain structure and simultaneously remove texture. To create an ideal algorithm for image smoothing, we face three problems. For images with rich textures, the smoothing effect should be enhanced. We should overcome inconsistency of smoothing results in different parts of the image. It is necessary to create a method to evaluate the smoothing effect. We apply texture pre-removal based on global sparse decomposition with a variable smoothing parameter to solve the first two problems. A parametric surface constructed by an improved Bessel method is used to determine the smoothing parameter. Three evaluation measures: edge integrity rate, texture removal rate, and gradient value distribution are proposed to cope with the third problem. We use the alternating direction method of multipliers to complete the whole algorithm and obtain the results. Experiments show that our algorithm is better than existing algorithms both visually and quantitatively. We also demonstrate our method's ability in other applications such as clip-art compression artifact removal and content-aware image manipulation.
C1 [Ma, Xiang; Li, Xuemei; Zhou, Yuanfeng; Zhang, Caiming] Shandong Univ, Jinan 250101, Peoples R China.
   [Li, Xuemei; Zhang, Caiming] Shandong Coinnovat Ctr Future Intelligent Comp, Yantai 264025, Peoples R China.
   [Zhang, Caiming] Digital Media Technol Key Lab Shandong Prov, Jinan 250014, Peoples R China.
C3 Shandong University
RP Zhang, CM (corresponding author), Shandong Univ, Jinan 250101, Peoples R China.; Zhang, CM (corresponding author), Shandong Coinnovat Ctr Future Intelligent Comp, Yantai 264025, Peoples R China.; Zhang, CM (corresponding author), Digital Media Technol Key Lab Shandong Prov, Jinan 250014, Peoples R China.
EM sdu_max@163.com; xmli@sdu.edu.cn; yfzhou@sdu.edu.cn; czhang@sdu.edu.cn
RI Cheng, Lin/KFQ-3111-2024; Zhou, Yuanfeng/AAT-4670-2020
OI ma, xiang/0000-0002-4963-8705
FU NSFC Joint Fund; Zhejiang Integration of Informatization and
   Industrialization [U1609218]
FX This work was supported by NSFC Joint Fund with Zhejiang Integration of
   Informatization and Industrialization under Key Project (U1609218).
CR ACM Transactions on Graphics, 2014, ACM T GRAPHICS, V33
   ACM Transactions on Graphics, 2012, ACM T GRAPHICS, V31
   Bao LC, 2014, IEEE T IMAGE PROCESS, V23, P555, DOI 10.1109/TIP.2013.2291328
   Boyd S., 2011, FOUND TRENDS MACH LE, V3, P1, DOI DOI 10.1561/2200000016
   Cai BL, 2017, IEEE IMAGE PROC, P250, DOI 10.1109/ICIP.2017.8296281
   Chen J, 2007, ACM T GRAPHIC, V26, DOI [10.1109/SARNOF.2007.4567317, 10.1145/1276377.1276506, 10.1145/1239451.1239554]
   Chen QF, 2017, IEEE I CONF COMP VIS, P2516, DOI 10.1109/ICCV.2017.273
   ECKSTEIN J, 1992, MATH PROGRAM, V55, P293, DOI 10.1007/BF01581204
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Gu SH, 2017, INT J COMPUT VISION, V121, P183, DOI 10.1007/s11263-016-0930-5
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Jian MW, 2018, J VIS COMMUN IMAGE R, V57, P1, DOI 10.1016/j.jvcir.2018.10.008
   Kim Y, 2019, IEEE T IMAGE PROCESS, V28, P2692, DOI 10.1109/TIP.2018.2889531
   Lindeberg T., 1994, Journal of AppliedStatistics, V21, P225
   Ma GH, 2018, J COMPUT SCI TECH-CH, V33, P502, DOI 10.1007/s11390-018-1834-3
   Mikolajczyk K., 2002, P INT C COMPUTER VIS, P128
   Ono S, 2017, INT CONF ACOUST SPEE, P1492, DOI 10.1109/ICASSP.2017.7952405
   Ono S, 2017, IEEE T IMAGE PROCESS, V26, P1554, DOI 10.1109/TIP.2017.2651392
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Subr K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618493
   Sun Dennis L., 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P6201, DOI 10.1109/ICASSP.2014.6854796
   Sun YJ, 2018, IEEE T VIS COMPUT GR, V24, P2129, DOI 10.1109/TVCG.2017.2711614
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wang YL, 2008, SIAM J IMAGING SCI, V1, P248, DOI 10.1137/080724265
   Weiss B, 2006, ACM T GRAPHIC, V25, P519, DOI 10.1145/1141911.1141918
   Xu L, 2015, PR MACH LEARN RES, V37, P1669
   Zhang F, 2020, COMPUT VIS MEDIA, V6, P417, DOI 10.1007/s41095-020-0186-4
   Zhang ML, 2019, IEEE T IMAGE PROCESS, V28, P868, DOI 10.1109/TIP.2018.2874284
   Zhang RL, 2014, PR MACH LEARN RES, V32, P1701
   Zhao M, 2016, J VIS COMMUN IMAGE R, V38, P73, DOI 10.1016/j.jvcir.2016.02.016
   Zhu FD, 2019, IEEE T IMAGE PROCESS, V28, P3556, DOI 10.1109/TIP.2019.2908778
NR 32
TC 16
Z9 17
U1 2
U2 13
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2021
VL 7
IS 4
BP 483
EP 497
DI 10.1007/s41095-021-0220-1
EA MAY 2021
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UK7UX
UT WOS:000654196700001
OA gold
DA 2024-07-18
ER

PT J
AU Xiao, YP
   Lai, YK
   Zhang, FL
   Li, CP
   Gao, L
AF Xiao, Yun-Peng
   Lai, Yu-Kun
   Zhang, Fang-Lue
   Li, Chunpeng
   Gao, Lin
TI A survey on deep geometry learning: From a representation perspective
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review
DE 3D shape representation; geometry learning; neural networks; computer
   graphics
ID AUTOENCODERS
AB Researchers have achieved great success in dealing with 2D images using deep learning. In recent years, 3D computer vision and geometry deep learning have gained ever more attention. Many advanced techniques for 3D shapes have been proposed for different applications. Unlike 2D images, which can be uniformly represented by a regular grid of pixels, 3D shapes have various representations, such as depth images, multi-view images, voxels, point clouds, meshes, implicit surfaces, etc. The performance achieved in different applications largely depends on the representation used, and there is no unique representation that works well for all applications. Therefore, in this survey, we review recent developments in deep learning for 3D geometry from a representation perspective, summarizing the advantages and disadvantages of different representations for different applications. We also present existing datasets in these representations and further discuss future research directions.
C1 [Xiao, Yun-Peng; Li, Chunpeng; Gao, Lin] Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China.
   [Lai, Yu-Kun] Cardiff Univ, Sch Comp Sci & Informat, Cardiff, Wales.
   [Zhang, Fang-Lue] Victoria Univ Wellington, Sch Engn & Comp Sci, Wellington, New Zealand.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Cardiff University; Victoria University Wellington
RP Gao, L (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China.
EM xiaoypgk@gmail.com; LaiY4@cardiff.ac.uk; fanglue.zhang@ecs.vuw.ac.nz;
   cpli@ict.ac.cn; gaolin@ict.ac.cn
RI Lai, Yu-Kun/D-2343-2010; Gao, Lin/JNF-0375-2023; Li,
   Chunpeng/AAE-6134-2019
FU National Natural Science Foundation of China [61828204, 61872440];
   Beijing Municipal Natural Science Foundation [L182016]; Youth Innovation
   Promotion Association CAS; CCF-Tencent Open Fund; Royal Society-Newton
   Advanced Fellowship [NAF\R2\192151]; Royal Society [IES\R1\180126]
FX This work was supported by the National Natural Science Foundation of
   China (61828204, 61872440), Beijing Municipal Natural Science Foundation
   (L182016), Youth Innovation Promotion Association CAS, CCF-Tencent Open
   Fund, Royal Society-Newton Advanced Fellowship (NAF\R2\192151), and the
   Royal Society (IES\R1\180126).
CR Ahmed E., 2018, ARXIV180801462, P1
   [Anonymous], 2015, DEEP CONVOLUTIONAL N
   [Anonymous], 2019, ACM T GRAPHICS, V37, P6
   [Anonymous], 2016, ACM T GRAPHICS, V35, P5
   [Anonymous], 2017, ACM T GRAPHICS, V36, P4
   [Anonymous], 2019, ACM T GRAPHICS, V38, P4
   [Anonymous], 2019, ACM T GRAPHICS, V38, P6
   [Anonymous], 2019, ACM T GRAPHICS, V38, P6
   [Anonymous], 2018, ACM T GRAPHICS, V37, P6
   [Anonymous], 2018, ACM T GRAPHICS, V37, P6
   [Anonymous], 2017, ACM T GRAPHIC, V36, P4
   [Anonymous], 2017, ACM T GRAPHICS, V36, P4
   [Anonymous], 2019, ACM T GRAPHICS, V38, P4
   [Anonymous], 2018, ACM T GRAPHICS, V37, P6
   [Anonymous], 2019, ACM T GRAPHICS, V38, P5
   Atwood J, 2016, NIPS, P2001
   Bane C, 2017, INT CONF 3D VISION, P412, DOI 10.1109/3DV.2017.00054
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491
   Boscaini D, 2015, COMPUT GRAPH FORUM, V34, P13, DOI 10.1111/cgf.12693
   Boscaini D, 2016, ADV NEUR IN, V29
   Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1
   Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418
   Bruna J., 2013, INT C LEARNING REPRE
   Cao YP, 2018, LECT NOTES COMPUT SC, V11213, P626, DOI 10.1007/978-3-030-01240-3_38
   Chang A X, 2015, COMPUTER SCI, V1512, P3
   Chen Z., 2019, ARXIV191106971
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Defferrard M, 2016, ADV NEUR IN, V29
   Eigen D, 2014, ADV NEUR IN, V27
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Fey M, 2018, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2018.00097
   Fu H, 2020, 3D FUTURE 3D FURNITU
   Gao L, 2019, IEEE T VISUALIZATION
   Gao Lin, 2019, ARXIV191006511
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Genova K, 2019, IEEE I CONF COMP VIS, P7153, DOI 10.1109/ICCV.2019.00725
   Genova Kyle, 2019, ABS191206126 CORR
   Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29
   Gómez-Bombarelli R, 2018, ACS CENTRAL SCI, V4, P268, DOI 10.1021/acscentsci.7b00572
   Goodfellow IJ, 2014, ADV NEUR IN, P2672, DOI DOI 10.1145/3422622
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Gupta S, 2015, PROC CVPR IEEE, P4731, DOI 10.1109/CVPR.2015.7299105
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang JW, 2019, PROC CVPR IEEE, P4435, DOI 10.1109/CVPR.2019.00457
   Huang SS, 2016, IEEE T VIS COMPUT GR, V22, P2024, DOI 10.1109/TVCG.2015.2473845
   Jeruzalski T, ARXIV PREPRINT ARXIV
   Kipf TN, 2016, ARXIV
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Larsen ABL, 2016, PR MACH LEARN RES, V48
   LeCun Y, 2010, IEEE INT SYMP CIRC S, P253, DOI 10.1109/ISCAS.2010.5537907
   Li RH, 2019, IEEE I CONF COMP VIS, P7202, DOI 10.1109/ICCV.2019.00730
   Li YY, 2018, ADV NEUR IN, V31
   Li YY, 2016, ADV NEUR IN, V29
   Lin M., 2013, ARXIV13124400
   Liu SC, 2019, ADV NEUR IN, V32
   Liu XH, 2019, AAAI CONF ARTIF INTE, P8778
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Mahmood N, 2019, IEEE I CONF COMP VIS, P5441, DOI 10.1109/ICCV.2019.00554
   Masci J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P832, DOI 10.1109/ICCVW.2015.112
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Maturana D, 2015, IEEE INT CONF ROBOT, P3471, DOI 10.1109/ICRA.2015.7139679
   MEAGHER D, 1982, COMPUT VISION GRAPH, V19, P129, DOI 10.1016/0146-664X(82)90104-6
   Mehr É, 2019, IEEE I CONF COMP VIS, P3473, DOI 10.1109/ICCV.2019.00357
   Meng HY, 2019, IEEE I CONF COMP VIS, P8499, DOI 10.1109/ICCV.2019.00859
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Mo KC, 2019, PROC CVPR IEEE, P909, DOI 10.1109/CVPR.2019.00100
   Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576
   Nash C, 2020, ARXIV200210880
   Pan Hao, 2018, ARXIV180804952
   Pan JY, 2019, IEEE I CONF COMP VIS, P9963, DOI 10.1109/ICCV.2019.01006
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Qiao Y-L, 2019, ARXIV191014063
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Sedaghat N, 2017, P BRIT MACH VIS C
   Sharma A, 2016, LECT NOTES COMPUT SC, V9915, P236, DOI 10.1007/978-3-319-49409-8_20
   Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802
   Silberman N, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Sinha A, 2017, PROC CVPR IEEE, P791, DOI 10.1109/CVPR.2017.91
   Sinha A, 2016, LECT NOTES COMPUT SC, V9910, P223, DOI 10.1007/978-3-319-46466-4_14
   Socher R., 2011, PROC INT C MACH LEAR, P129
   Socher R., 2012, NIPS, V3, P8
   Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28
   Song SR, 2016, PROC CVPR IEEE, P808, DOI 10.1109/CVPR.2016.94
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Tan Q, 2018, P 32 AAAI C ART INT
   Tan QY, 2020, IEEE ROBOT AUTOM LET, V5, P2325, DOI 10.1109/LRA.2020.2970624
   Tan QY, 2018, PROC CVPR IEEE, P5841, DOI 10.1109/CVPR.2018.00612
   Tang JP, 2019, PROC CVPR IEEE, P4536, DOI 10.1109/CVPR.2019.00467
   Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230
   Vaswani A, 2017, ADV NEUR IN, V30
   Verma N, 2018, PROC CVPR IEEE, P2598, DOI 10.1109/CVPR.2018.00275
   Vincent E, 2008, INT CONF ACOUST SPEE, P109, DOI 10.1109/ICASSP.2008.4517558
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4
   Wang YF, 2019, PROC CVPR IEEE, P5951, DOI 10.1109/CVPR.2019.00611
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Wen C, 2019, IEEE I CONF COMP VIS, P1042, DOI 10.1109/ICCV.2019.00113
   Wu JJ, 2016, ADV NEUR IN, V29
   Wu R, 2019, ARXIV191110949
   Xiang Y, 2016, LECT NOTES COMPUT SC, V9912, P160, DOI 10.1007/978-3-319-46484-8_10
   Xu H, 2017, IEEE INT C COMPUT VI, P2698
   Xu Qiangeng, 2019, P ADV NEUR INF PROC, P490
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295
   Yuan Y-J, 2019, ARXIV190802507
   Zou CH, 2017, IEEE I CONF COMP VIS, P900, DOI 10.1109/ICCV.2017.103
NR 119
TC 64
Z9 70
U1 5
U2 66
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2020
VL 6
IS 2
BP 113
EP 133
DI 10.1007/s41095-020-0174-8
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FB
UT WOS:000648691300001
OA Green Accepted, Green Submitted, gold
DA 2024-07-18
ER

PT J
AU Li, RT
   Si, WX
   Liao, XY
   Wang, Q
   Klein, R
   Heng, PA
AF Li, Ruotong
   Si, Weixin
   Liao, Xiangyun
   Wang, Qiong
   Klein, Reinhard
   Heng, Pheng-Ann
TI Mixed reality based respiratory liver tumor puncture navigation
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE mixed reality; human computer interaction; statistical motion model
AB This paper presents a novel mixed reality based navigation system for accurate respiratory liver tumor punctures in radiofrequency ablation (RFA). Our system contains an optical see-through head-mounted display device (OST-HMD), Microsoft HoloLens for perfectly overlaying the virtual information on the patient, and a optical tracking system NDI Polaris for calibrating the surgical utilities in the surgical scene. Compared with traditional navigation method with CT, our system aligns the virtual guidance information and real patient and real-timely updates the view of virtual guidance via a position tracking system. In addition, to alleviate the difficulty during needle placement induced by respiratory motion, we reconstruct the patient-specific respiratory liver motion through statistical motion model to assist doctors precisely puncture liver tumors. The proposed system has been experimentally validated on vivo pigs with an accurate real-time registration approximately 5-mm mean FRE and TRE, which has the potential to be applied in clinical RFA guidance.
C1 [Li, Ruotong; Klein, Reinhard] Univ Bonn, Inst Comp Sci 2, D-53115 Bonn, Germany.
   [Si, Weixin; Liao, Xiangyun; Wang, Qiong] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen Key Lab Virtual Real & Human Interact Te, Beijing, Peoples R China.
   [Heng, Pheng-Ann] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
C3 University of Bonn; Chinese Academy of Sciences; Shenzhen Institute of
   Advanced Technology, CAS; Chinese University of Hong Kong
RP Wang, Q (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen Key Lab Virtual Real & Human Interact Te, Beijing, Peoples R China.
EM liruoton@cs.uni-bonn.de; wxsics@gmail.com; xiangyun-l@163.com;
   wangqiong@siat.ac.cn; rk@cs.uni-bonn.de; pheng@cse.cuhk.edu.hk
OI Li, Ruotong/0000-0002-0882-8510
FU National Natural Science Foundation of China [U1813204, 61802385]; HK
   RGC TRS project [T42-409/18-R]; HK RGC [CUHK14225616]; CUHK T Stone
   Robotics Institute, CUHK; Science and Technology Plan Project of
   Guangzhou [201704020141]
FX This work was supported in part by the National Natural Science
   Foundation of China (Nos. U1813204 and 61802385), in part by HK RGC TRS
   project T42-409/18-R, in part by HK RGC project CUHK14225616, in part by
   CUHK T Stone Robotics Institute, CUHK, and in part by the Science and
   Technology Plan Project of Guangzhou (No. 201704020141). The authors
   would like to thank Yanfang Zhang and Jianxi Guo (Shenzhen People's
   Hospital) for providing the medical support, and Rui Zheng for the
   useful discussions. Special thanks to the reviewers and editors of
   Computational Visual Media.
CR Ahmed M, 2011, RADIOLOGY, V258, P351, DOI 10.1148/radiol.10081634
   Amalou H, 2012, J BRONCHOL INTERN PU, V19, P323, DOI 10.1097/LBR.0b013e31827157c9
   Bernhardt S, 2017, MED IMAGE ANAL, V37, P66, DOI 10.1016/j.media.2017.01.007
   Biro P, 2009, BRIT J ANAESTH, V102, P650, DOI 10.1093/bja/aep051
   Breen DJ, 2015, NAT REV CLIN ONCOL, V12, P175, DOI 10.1038/nrclinonc.2014.237
   Cai K, 2016, J MED SYST, V40, DOI 10.1007/s10916-015-0397-x
   Cazzato RL, 2016, CARDIOVASC INTER RAD, V39, P1315, DOI 10.1007/s00270-016-1334-1
   Chan WY, 2014, IEEE J BIOMED HEALTH, V18, P643, DOI 10.1109/JBHI.2013.2275741
   Clasen S, 2008, J MAGN RESON IMAGING, V27, P421, DOI 10.1002/jmri.21264
   Crocetti LD, 2016, INTERVENTIONAL ONCOL, V91, DOI [10.1017/CBO9781107338555.011, DOI 10.1017/CBO9781107338555.011]
   Flaherty DC Bilchik AJ, 2017, BLUMGARTS SURG LIVER, P1436
   Jud C., 2017, STAT SHAPE DEFORMATI, P379, DOI [10.1016/ B978-0-12-810493- 4.0 0 017-1, DOI 10.1016/B978-0-12-810493-4.00017-1]
   Khan MF, 2006, INVEST RADIOL, V41, P713, DOI 10.1097/01.rli.0000236910.75905.cc
   Kim PN, 2012, J VASC INTERV RADIOL, V23, P627, DOI 10.1016/j.jvir.2011.12.026
   Livraghi T, 2011, SCAND J SURG, V100, P22, DOI 10.1177/145749691110000105
   Nicolau SA, 2007, LECT NOTES COMPUT SC, V4792, P77
   Ren HL, 2014, IEEE J BIOMED HEALTH, V18, P920, DOI 10.1109/JBHI.2013.2287202
   Ren Q, 2007, PHYS MED BIOL, V52, P6651, DOI 10.1088/0031-9155/52/22/007
   Sauer F, 2003, PROC SPIE, V5029, P384, DOI 10.1117/12.480383
   Schweikard A, 2004, MED PHYS, V31, P2738, DOI 10.1118/1.1774132
   Tiong L, 2011, BRIT J SURG, V98, P1210, DOI 10.1002/bjs.7669
   Wang ZG, 2012, INT J COMPUT ASS RAD, V7, P941, DOI 10.1007/s11548-012-0769-3
   Wunderink W, 2008, INT J RADIAT ONCOL, V71, P907, DOI 10.1016/j.ijrobp.2008.03.010
NR 23
TC 8
Z9 8
U1 2
U2 18
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2019
VL 5
IS 4
BP 363
EP 374
DI 10.1007/s41095-019-0156-x
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UO
UT WOS:000648691000004
OA gold
DA 2024-07-18
ER

PT J
AU Cheng, MM
   Liu, Y
   Lin, WY
   Zhang, ZM
   Rosin, PL
   Torr, PHS
AF Cheng, Ming-Ming
   Liu, Yun
   Lin, Wen-Yan
   Zhang, Ziming
   Rosin, Paul L.
   Torr, Philip H. S.
TI BING: Binarized normed gradients for objectness estimation at 300fps
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE object proposals; objectness; visual attention; category agnostic
   proposals
ID IMAGE SEGMENTATION; VISUAL-ATTENTION; RANKING; MODEL; DEEP
AB Training a generic objectness measure to produce object proposals has recently become of significant interest. We observe that generic objects with well-defined closed boundaries can be detected by looking at the norm of gradients, with a suitable resizing of their corresponding image windows to a small fixed size. Based on this observation and computational reasons, we propose to resize the window to 8 x 8 and use the norm of the gradients as a simple 64D feature to describe it, for explicitly training a generic objectness measure. We further show how the binarized version of this feature, namely binarized normed gradients (BING), can be used for efficient objectness estimation, which requires only a few atomic operations (e.g., add, bitwise shift, etc.). To improve localization quality of the proposals while maintaining efficiency, we propose a novel fast segmentation method and demonstrate its effectiveness for improving BING's localization performance, when used in multi-thresholding straddling expansion (MTSE) post-processing. On the challenging PASCAL VOC2007 dataset, using 1000 proposals per image and intersection-over-union threshold of 0.5, our proposal method achieves a 95.6% object detection rate and 78.6% mean average best overlap in less than 0.005 second per image.
C1 [Cheng, Ming-Ming; Liu, Yun] Nankai Univ, CCS, Tianjin 300350, Peoples R China.
   [Lin, Wen-Yan] Inst Infocomm Res, Singapore 138632, Singapore.
   [Zhang, Ziming] MERL, Cambridge, MA 02139 USA.
   [Rosin, Paul L.] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF24 3AA, Wales.
   [Torr, Philip H. S.] Univ Oxford, Oxford OX1 3PJ, England.
C3 Nankai University; Agency for Science Technology & Research (A*STAR);
   A*STAR - Institute for Infocomm Research (I2R); Cardiff University;
   University of Oxford
RP Cheng, MM (corresponding author), Nankai Univ, CCS, Tianjin 300350, Peoples R China.
EM cmm@nankai.edu.cn
RI Zhang, ziming/HGA-8604-2022; Cheng, Ming-Ming/A-2527-2009
OI Cheng, Ming-Ming/0000-0001-5550-8758; Rosin, Paul/0000-0002-4965-3884
FU National Natural Science Foundation of China [61572264, 61620106008];
   EPSRC [EP/I001107/2, EP/N019474/1] Funding Source: UKRI
FX This research was supported by the National Natural Science Foundation
   of China (Nos. 61572264, 61620106008).
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   Alexe B, 2010, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2010.5540226
   [Anonymous], 2015, TRANSFERRING RICH FE
   [Anonymous], 2016, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2015.2474388
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   [Anonymous], 2012, Technical Report
   [Anonymous], 2014, P BRIT MACH VIS C
   Arbeláez P, 2012, PROC CVPR IEEE, P3378, DOI 10.1109/CVPR.2012.6248077
   Borji A., 2014, SALIENT OBJECT DETEC
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Borji A, 2013, IEEE T IMAGE PROCESS, V22, P55, DOI 10.1109/TIP.2012.2210727
   Carreira J, 2012, LECT NOTES COMPUT SC, V7578, P430, DOI 10.1007/978-3-642-33786-4_32
   Carreira J, 2012, IEEE T PATTERN ANAL, V34, P1312, DOI 10.1109/TPAMI.2011.231
   Chavali N, 2016, PROC CVPR IEEE, P835, DOI 10.1109/CVPR.2016.97
   Chen T, 2013, IEEE T VIS COMPUT GR, V19, P824, DOI 10.1109/TVCG.2012.148
   Chen TC, 2009, PROC EUR SOLID-STATE, P1
   Chen W, 2014, PROC CVPR IEEE, P748, DOI 10.1109/CVPR.2014.101
   Chen XZ, 2015, PROC CVPR IEEE, P2587, DOI 10.1109/CVPR.2015.7298874
   Cheng MM, 2016, LECT NOTES COMPUT SC, V9907, P867, DOI 10.1007/978-3-319-46487-9_53
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Cheng MM, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2682628
   Cheng MM, 2014, VISUAL COMPUT, V30, P443, DOI 10.1007/s00371-013-0867-4
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Chia AYS, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024190
   Cho M, 2015, PROC CVPR IEEE, P1201, DOI 10.1109/CVPR.2015.7298724
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   DESIMONE R, 1995, ANNU REV NEUROSCI, V18, P193, DOI 10.1146/annurev-psych-122414-033400
   Endres I, 2014, IEEE T PATTERN ANAL, V36, P222, DOI 10.1109/TPAMI.2013.122
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Forsyth D.A., 1996, International workshop on object representation in computer vision, P335
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gottlieb JP, 1998, NATURE, V391, P481, DOI 10.1038/35135
   Hare S, 2012, PROC CVPR IEEE, P1894, DOI 10.1109/CVPR.2012.6247889
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   He JF, 2012, PROC CVPR IEEE, P3005, DOI 10.1109/CVPR.2012.6248030
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Heitz G, 2008, LECT NOTES COMPUT SC, V5302, P30, DOI 10.1007/978-3-540-88682-2_4
   Hosang J, 2016, IEEE T PATTERN ANAL, V38, P814, DOI 10.1109/TPAMI.2015.2465908
   Hu SM, 2013, VISUAL COMPUT, V29, P393, DOI 10.1007/s00371-013-0792-6
   Huang H, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024189
   Humayun A, 2014, PROC CVPR IEEE, P336, DOI 10.1109/CVPR.2014.50
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Käding C, 2015, PROC CVPR IEEE, P4343, DOI 10.1109/CVPR.2015.7299063
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Krähenbühl P, 2014, LECT NOTES COMPUT SC, V8693, P725, DOI 10.1007/978-3-319-10602-1_47
   Krähenbühl P, 2015, PROC CVPR IEEE, P1574, DOI 10.1109/CVPR.2015.7298765
   Kuo WC, 2015, IEEE I CONF COMP VIS, P2479, DOI 10.1109/ICCV.2015.285
   Kwak S, 2015, IEEE I CONF COMP VIS, P3173, DOI 10.1109/ICCV.2015.363
   Lee YJ, 2015, INT J COMPUT VISION, V114, P38, DOI 10.1007/s11263-014-0794-5
   Li K, 2016, PATTERN RECOGN, V51, P59, DOI 10.1016/j.patcog.2015.08.008
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Tie, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383047
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lu CW, 2015, IEEE I CONF COMP VIS, P2021, DOI 10.1109/ICCV.2015.234
   Ma Y.F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   Manen S., 2013, P IEEE INT C COMPUTE, V2536, P2543
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Pinheiro PO, 2015, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2015.7298780
   Pont-Tuset J, 2017, IEEE T PATTERN ANAL, V39, P128, DOI 10.1109/TPAMI.2016.2537320
   Rahtu E, 2011, IEEE I CONF COMP VIS, P1052, DOI 10.1109/ICCV.2011.6126351
   Rantalankila P, 2014, PROC CVPR IEEE, P2417, DOI 10.1109/CVPR.2014.310
   Ren C.Y., 2015, gSLICr: SLIC superpixels at over 250Hz
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sener F, 2012, LECT NOTES COMPUT SC, V7585, P263, DOI 10.1007/978-3-642-33885-4_27
   Simonyan K., 2014, 14091556 ARXIV
   Sun J, 2011, IEEE I CONF COMP VIS, P1511, DOI 10.1109/ICCV.2011.6126409
   Teuber HL, 1955, ANNU REV PSYCHOL, V6, P267, DOI 10.1146/annurev.ps.06.020155.001411
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929
   Wolfe JM, 2004, NAT REV NEUROSCI, V5, P495, DOI 10.1038/nrn1411
   Wu JJ, 2015, PROC CVPR IEEE, P3460, DOI 10.1109/CVPR.2015.7298968
   Zha Shengxin., 2015, BMVC
   Zhang DW, 2016, INT J COMPUT VISION, V120, P215, DOI 10.1007/s11263-016-0907-4
   Zhang GX, 2009, COMPUT GRAPH FORUM, V28, P1897, DOI 10.1111/j.1467-8659.2009.01568.x
   Zhang ZM, 2018, IEEE T PATTERN ANAL, V40, P1209, DOI 10.1109/TPAMI.2017.2707492
   Zhang ZM, 2011, PROC CVPR IEEE, P1497, DOI 10.1109/CVPR.2011.5995411
   Zheng SH, 2013, ADV INTEL SYS RES, V84, P1
   Zheng S, 2014, PROC CVPR IEEE, P3214, DOI 10.1109/CVPR.2014.411
   Zheng YY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185595
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 90
TC 45
Z9 49
U1 0
U2 11
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2019
VL 5
IS 1
BP 3
EP 20
DI 10.1007/s41095-018-0120-1
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UL
UT WOS:000648690700002
OA gold, Green Accepted, Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Xu, QC
   Mu, TJ
   Yang, YL
AF Xu, Qun-Ce
   Mu, Tai-Jiang
   Yang, Yong-Liang
TI A survey of deep learning-based 3D shape generation
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review
DE 3D representations; geometry learning; generative models; deep learning
ID MODELS
AB Deep learning has been successfully used for tasks in the 2D image domain. Research on 3D computer vision and deep geometry learning has also attracted attention. Considerable achievements have been made regarding feature extraction and discrimination of 3D shapes. Following recent advances in deep generative models such as generative adversarial networks, effective generation of 3D shapes has become an active research topic. Unlike 2D images with a regular grid structure, 3D shapes have various representations, such as voxels, point clouds, meshes, and implicit functions. For deep learning of 3D shapes, shape representation has to be taken into account as there is no unified representation that can cover all tasks well. Factors such as the representativeness of geometry and topology often largely affect the quality of the generated 3D shapes. In this survey, we comprehensively review works on deep-learning-based 3D shape generation by classifying and discussing them in terms of the underlying shape representation and the architecture of the shape generator. The advantages and disadvantages of each class are further analyzed. We also consider the 3D shape datasets commonly used for shape generation. Finally, we present several potential research directions that hopefully can inspire future works on this topic.
C1 [Xu, Qun-Ce; Mu, Tai-Jiang] Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing 100084, Peoples R China.
   [Yang, Yong-Liang] Univ Bath, Dept Comp Sci, Bath, England.
C3 Tsinghua University; University of Bath
RP Mu, TJ (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing 100084, Peoples R China.
EM quncexu@tsinghua.edu.cn; taijiang@tsinghua.edu.cn; y.yang@cs.bath.ac.uk
RI Mu, Tai-Jiang/JWO-1381-2024
OI Mu, Tai-Jiang/0000-0002-9197-346X
FU National Natural Science Foundation of China [61902210]; RCUK grant
   CAMERA [EP/M023281/1, EP/T022523/1]
FX AcknowledgementsThis work was supported by the National Natural Science
   Foundation of China (Grant No. 61902210) and RCUK grant CAMERA (Grant
   Nos. EP/M023281/1, EP/T022523/1).
CR Achlioptas P, 2018, PR MACH LEARN RES, V80
   Aigerman N, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818099
   [Anonymous], 2016, P ADV NEURAL INFORM
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Arshad MS, 2020, INT CONF 3D VISION, P712, DOI 10.1109/3DV50981.2020.00081
   Atwood J, 2016, ADV NEUR IN, V29
   Aumentado-Armstrong T, 2022, PROC CVPR IEEE, P19321, DOI 10.1109/CVPR52688.2022.01874
   Balashova E, 2018, INT CONF 3D VISION, P140, DOI 10.1109/3DV.2018.00026
   Bane C, 2017, INT CONF 3D VISION, P412, DOI 10.1109/3DV.2017.00054
   Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491
   Brock A, 2016, Arxiv, DOI arXiv:1608.04236
   Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1
   Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418
   Bruna J., 2014, ABS13126203 CORR, P1, DOI [10.48550/arXiv.1312.6203, DOI 10.48550/ARXIV.1312.6203]
   Cai Ruojin, 2020, EUR C COMP VIS ECCV
   Cao WM, 2020, IEEE ACCESS, V8, P35929, DOI 10.1109/ACCESS.2020.2975067
   Cao YP, 2018, LECT NOTES COMPUT SC, V11213, P626, DOI 10.1007/978-3-030-01240-3_38
   Chen C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12446, DOI 10.1109/ICCV48922.2021.01224
   Chen K, 2019, LECT NOTES COMPUT SC, V11363, P100, DOI 10.1007/978-3-030-20893-6_7
   Chen R. T., 2018, P 32 INT C NEUR INF, P6572
   Chen W., 2019, Proceedings of the Advances in Neural Information Processing Systems, P9609
   Chen ZQ, 2021, PROC CVPR IEEE, P15735, DOI 10.1109/CVPR46437.2021.01548
   Chen ZQ, 2020, PROC CVPR IEEE, P42, DOI 10.1109/CVPR42600.2020.00012
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   Chibane J., 2020, Adv. Neural Inf. Process. Syst
   Chibane J, 2020, PROC CVPR IEEE, P6968, DOI 10.1109/CVPR42600.2020.00700
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Choi S, 2016, Arxiv, DOI [arXiv:1602.02481, DOI 10.48550/ARXIV.1602.02481]
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   Defferrard M, 2016, ADV NEUR IN, V29
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dinh L, 2015, NICE NONLINEAR INDEP
   Dinh L., 2017, P INT C LEARNING REP
   Dosovitskiy A., 2021, P THE INT C LEARNING
   Edwards Harrison, 2016, ICLR
   Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Feng YT, 2019, AAAI CONF ARTIF INTE, P8279
   Fu H, 2021, INT J COMPUT VISION, V129, P3313, DOI 10.1007/s11263-021-01534-z
   Gadelha M, 2018, LECT NOTES COMPUT SC, V11211, P105, DOI 10.1007/978-3-030-01234-2_7
   Gadelha M, 2017, INT CONF 3D VISION, P402, DOI 10.1109/3DV.2017.00053
   Gal R, 2020, Arxiv, DOI arXiv:2007.12944
   Gao L, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480503
   Gao L, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356488
   Gao L, 2021, IEEE T VIS COMPUT GR, V27, P2085, DOI 10.1109/TVCG.2019.2941200
   Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29
   Gkioxari G, 2019, IEEE I CONF COMP VIS, P9784, DOI 10.1109/ICCV.2019.00988
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Grathwohl Will, 2019, P INT C LEARNING REP
   Gregor K, 2015, PR MACH LEARN RES, V37, P1462
   Grigorev A, 2021, PROC CVPR IEEE, P5147, DOI 10.1109/CVPR46437.2021.00511
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Han Z., 2020, PROC INT C MACH LEAR, P3994
   Hanocka R, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322959
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henderson Paul, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7495, DOI 10.1109/CVPR42600.2020.00752
   Henzler P, 2019, IEEE I CONF COMP VIS, P9983, DOI 10.1109/ICCV.2019.01008
   Hinton GE, 1993, ADV NEURAL INFORM PR, P3, DOI [DOI 10.1021/JP906511Z, DOI 10.5555/2987189.2987190]
   Hu SM, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3506694
   Hu SM, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-020-3097-4
   Hu T, 2021, IEEE WINT CONF APPL, P2169, DOI 10.1109/WACV48630.2021.00222
   Huang HB, 2015, COMPUT GRAPH FORUM, V34, P25, DOI 10.1111/cgf.12694
   Huang JH, 2021, PROC CVPR IEEE, P8928, DOI 10.1109/CVPR46437.2021.00882
   Huang JH, 2021, PROC CVPR IEEE, P7104, DOI 10.1109/CVPR46437.2021.00703
   Huang SS, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3453485
   Huang WL, 2019, AAAI CONF ARTIF INTE, P8481
   Hui KH, 2022, PROC CVPR IEEE, P18551, DOI 10.1109/CVPR52688.2022.01802
   Hui L., 2020, P EUR C COMP VIS, P397
   Ibing M, 2021, PROC CVPR IEEE, P13554, DOI 10.1109/CVPR46437.2021.01335
   Ibing Moritz, 2021, Octree transformer: Autoregressive 3d shape generation on hierarchically structured sequences
   Insafutdinov E, 2018, ADV NEUR IN, V31
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang CY, 2020, PROC CVPR IEEE, P6000, DOI 10.1109/CVPR42600.2020.00604
   Jones RK, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417812
   Kaichun Mo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P683, DOI 10.1007/978-3-030-58539-6_41
   Kalogerakis E, 2012, ACM T GRAPHIC, V31, DOI [10.1145/2077341.2077342, 10.1145/2185520.2185551]
   Kar A, 2017, ADV NEUR IN, V30
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kato H, 2018, PROC CVPR IEEE, P3907, DOI 10.1109/CVPR.2018.00411
   Kawana Y., 2020, ADV NEURAL INFORM PR, V33
   Khan SH, 2019, PROC CVPR IEEE, P9731, DOI 10.1109/CVPR.2019.00997
   Kim H., 2020, Adv. Neural Inf. Process. Syst., V33, P16388
   Kim J, 2021, PROC CVPR IEEE, P15054, DOI 10.1109/CVPR46437.2021.01481
   Kim VG, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461933
   Kingma D. P., 2014, arXiv
   Kingma DP, 2018, 32 C NEURAL INFORM P
   Kirk D., 2007, PROC 6 INT S MEMORY, P103
   Klokov Roman, 2020, EUR C COMP VIS, P694, DOI [DOI 10.1007/978-3-030-58592-141, 10.1007/978-3-030-58592-1_41, 10.1007/978-3-030-58592]
   Knyaz VA, 2019, LECT NOTES COMPUT SC, V11129, P601, DOI 10.1007/978-3-030-11009-3_37
   Komarichev A, 2022, COMPUT AIDED GEOM D, V93, DOI 10.1016/j.cagd.2022.102076
   Kurenkov A, 2018, IEEE WINT CONF APPL, P858, DOI 10.1109/WACV.2018.00099
   Lazarow J, 2017, IEEE I CONF COMP VIS, P2793, DOI 10.1109/ICCV.2017.302
   LeCun Y., 2006, PREDICTING STRUCTURE, V1
   Li J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073637
   Li MY, 2021, PROC CVPR IEEE, P10241, DOI 10.1109/CVPR46437.2021.01011
   Li RH, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459766
   Li RH, 2020, PROC CVPR IEEE, P6377, DOI 10.1109/CVPR42600.2020.00641
   Li RH, 2019, IEEE I CONF COMP VIS, P7202, DOI 10.1109/ICCV.2019.00730
   Li SD, 2022, AAAI CONF ARTIF INTE, P1386
   Li X, 2019, PROC CVPR IEEE, P5530, DOI 10.1109/CVPR.2019.00568
   Li YY, 2018, ADV NEUR IN, V31
   Li YS, 2020, PROCEEDINGS OF 2020 IEEE 19TH INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS & COGNITIVE COMPUTING (ICCI*CC 2020), P52, DOI 10.1109/ICCICC50026.2020.9450255
   Li YS, 2021, IEEE T IMAGE PROCESS, V30, P4540, DOI 10.1109/TIP.2021.3073318
   Liang YQ, 2022, Arxiv, DOI arXiv:2207.10228
   Liao YY, 2018, PROC CVPR IEEE, P2916, DOI 10.1109/CVPR.2018.00308
   Lin CH, 2018, AAAI CONF ARTIF INTE, P7114
   Liu HTD, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392418
   Liu SL, 2021, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR46437.2021.00183
   Liu SC, 2019, ADV NEUR IN, V32
   Liu SC, 2019, IEEE I CONF COMP VIS, P7707, DOI 10.1109/ICCV.2019.00780
   Liu SK, 2018, INT CONF 3D VISION, P542, DOI 10.1109/3DV.2018.00068
   Liu ZN, 2021, IEEE T VIS COMPUT GR, V27, P83, DOI 10.1109/TVCG.2019.2937300
   Liu ZZ, 2022, PROC CVPR IEEE, P17875, DOI 10.1109/CVPR52688.2022.01737
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Luo ST, 2021, PROC CVPR IEEE, P2836, DOI 10.1109/CVPR46437.2021.00286
   Maron H, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073616
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Maturana D, 2015, IEEE INT CONF ROBOT, P3471, DOI 10.1109/ICRA.2015.7139679
   Jiang C, 2017, Arxiv, DOI arXiv:1709.07581
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Mezghanni M, 2021, PROC CVPR IEEE, P9326, DOI 10.1109/CVPR46437.2021.00921
   Michalkiewicz M, 2019, Arxiv, DOI arXiv:1901.06802
   Mildenhall B., LECT NOTES COMPUTER, V12346, P405
   Mittal P, 2022, PROC CVPR IEEE, P306, DOI 10.1109/CVPR52688.2022.00040
   Mo KC, 2019, PROC CVPR IEEE, P909, DOI 10.1109/CVPR.2019.00100
   Mo KC, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356527
   Nash C, 2017, COMPUT GRAPH FORUM, V36, P1, DOI 10.1111/cgf.13240
   Nash C., 2020, ICML, P7220
   Navaneet KL, 2019, AAAI CONF ARTIF INTE, P8819
   Pan JY, 2019, IEEE I CONF COMP VIS, P9963, DOI 10.1109/ICCV.2019.01006
   Pan XR, 2021, PROC CVPR IEEE, P7459, DOI 10.1109/CVPR46437.2021.00738
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Paschalidou D, 2021, PROC CVPR IEEE, P3203, DOI 10.1109/CVPR46437.2021.00322
   Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123
   Pavllo D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13859, DOI 10.1109/ICCV48922.2021.01362
   Pavllo Dario, 2020, ADV NEURAL INFORM PR, P870
   Peng Songyou, 2020, Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part III 16, P523
   Pontes JK, 2019, LECT NOTES COMPUT SC, V11361, P365, DOI 10.1007/978-3-030-20887-5_23
   Postels J, 2021, INT CONF 3D VISION, P1249, DOI 10.1109/3DV53792.2021.00132
   Poursaeed O., 2020, COMPUTER VISION ECCV, V12355, P667, DOI 10.1007/978-3-030- 58607-2 39
   Pumarola Albert, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7946, DOI 10.1109/CVPR42600.2020.00797
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qiao YL, 2022, IEEE T VIS COMPUT GR, V28, P1317, DOI 10.1109/TVCG.2020.3014449
   Radford A., 2015, ARXIV
   Radford A, 2021, PR MACH LEARN RES, V139
   Ramasinghe S, 2020, IEEE INT C INT ROBOT, P8169, DOI 10.1109/IROS45743.2020.9341265
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Razavi A, 2019, ADV NEUR IN, V32
   Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239
   Sanghi Aditya, 2022, P IEEECVF C COMPUTER, P18603
   Saquil Y, 2020, AAAI CONF ARTIF INTE, V34, P5586
   Schor N, 2019, IEEE I CONF COMP VIS, P8758, DOI 10.1109/ICCV.2019.00885
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Sedaghat N., 2017, P BRIT MACHINE VISIO, V97
   Sederberg T. W., 1986, Computer Graphics, V20, P151, DOI 10.1145/15886.15903
   Shen TC, 2021, ADV NEUR IN, V34
   Shi Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13077, DOI 10.1109/ICCV48922.2021.01285
   Shu DW, 2019, IEEE I CONF COMP VIS, P3858, DOI 10.1109/ICCV.2019.00396
   Smith Edward J., 2017, P MACH LEARN RES, V78, P87
   Sohl-Dickstein J, 2015, PR MACH LEARN RES, V37, P2256
   Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Song-Hai Zhang, 2021, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P6000, DOI 10.1109/CVPR46437.2021.00595
   Sun XY, 2018, PROC CVPR IEEE, P2974, DOI 10.1109/CVPR.2018.00314
   Sun YB, 2020, IEEE WINT CONF APPL, P61, DOI [10.1109/wacv45572.2020.9093430, 10.1109/WACV45572.2020.9093430]
   Sung M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130821
   Tan QY, 2018, PROC CVPR IEEE, P5841, DOI 10.1109/CVPR.2018.00612
   Tang J.-H., 2021, P INT C NEUR INF PRO, P12648
   Tang JP, 2019, PROC CVPR IEEE, P4536, DOI 10.1109/CVPR.2019.00467
   Tang YZ, 2022, PROC CVPR IEEE, P6387, DOI 10.1109/CVPR52688.2022.00629
   Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230
   Valsesia Diego, 2019, INT C LEARNING REPRE
   Vaswani A, 2017, ADV NEUR IN, V30
   Venkatesh R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12633, DOI 10.1109/ICCV48922.2021.01242
   Wang H, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275025
   Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4
   Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608
   Wang WY, 2019, ADV NEUR IN, V32
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wei Y, 2019, PROC CVPR IEEE, P9643, DOI 10.1109/CVPR.2019.00988
   Welinder P., 2010, 2010001 COMP NEUR SY
   Wen C, 2019, IEEE I CONF COMP VIS, P1042, DOI 10.1109/ICCV.2019.00113
   Wen C, 2021, PROC CVPR IEEE, P10261, DOI 10.1109/CVPR46437.2021.01013
   Wu J., 2017, PROC ADVNEURAL INF P, V30, P153
   Wu JJ, 2018, LECT NOTES COMPUT SC, V11215, P673, DOI 10.1007/978-3-030-01252-6_40
   Wu JJ, 2016, ADV NEUR IN, V29
   Wu RD, 2020, PROC CVPR IEEE, P826, DOI 10.1109/CVPR42600.2020.00091
   Wu ZJ, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322956
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Xiang Y, 2016, LECT NOTES COMPUT SC, V9912, P160, DOI 10.1007/978-3-319-46484-8_10
   Xiang Y, 2014, IEEE WINT CONF APPL, P75, DOI 10.1109/WACV.2014.6836101
   Xiao YP, 2020, COMPUT VIS MEDIA, V6, P113, DOI 10.1007/s41095-020-0174-8
   Xie HZ, 2019, IEEE I CONF COMP VIS, P2690, DOI 10.1109/ICCV.2019.00278
   Xie JW, 2022, IEEE T PATTERN ANAL, V44, P2468, DOI 10.1109/TPAMI.2020.3045010
   Xie JW, 2021, PROC CVPR IEEE, P14971, DOI 10.1109/CVPR46437.2021.01473
   Xie JW, 2018, PROC CVPR IEEE, P8629, DOI 10.1109/CVPR.2018.00900
   Xie JW, 2016, PR MACH LEARN RES, V48
   Yan XC, 2016, ADV NEUR IN, V29
   Yan XG, 2022, PROC CVPR IEEE, P6229, DOI 10.1109/CVPR52688.2022.00614
   Yang GD, 2019, IEEE I CONF COMP VIS, P4540, DOI 10.1109/ICCV.2019.00464
   Yang J, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3526212
   Yang S, 2021, PROC CVPR IEEE, P3151, DOI 10.1109/CVPR46437.2021.00317
   Yang XM, 2021, AAAI CONF ARTIF INTE, V35, P3154
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yin KX, 2020, INT CONF 3D VISION, P61, DOI 10.1109/3DV50981.2020.00016
   Yuan YJ, 2022, PROC CVPR IEEE, P18332, DOI 10.1109/CVPR52688.2022.01781
   Yuan YJ, 2020, IEEE COMPUT SOC CONF, P1105, DOI 10.1109/CVPRW50498.2020.00145
   Yue Qian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P752, DOI 10.1007/978-3-030-58529-7_44
   Zamorski M, 2020, COMPUT VIS IMAGE UND, V193, DOI 10.1016/j.cviu.2020.102921
   Zhang SH, 2022, IEEE T VIS COMPUT GR, V28, P3082, DOI 10.1109/TVCG.2021.3050143
   Zhang XM, 2018, ADV NEUR IN, V31
   Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhou LQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5806, DOI 10.1109/ICCV48922.2021.00577
   Zhu Jun-Yan, 2018, Advances in Neural Information Processing Systems
   Zou CH, 2017, IEEE I CONF COMP VIS, P900, DOI 10.1109/ICCV.2017.103
NR 226
TC 2
Z9 2
U1 29
U2 113
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2023
VL 9
IS 3
BP 407
EP 442
DI 10.1007/s41095-022-0321-5
PG 36
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA G9DP6
UT WOS:000992080000002
OA gold
DA 2024-07-18
ER

PT J
AU Wang, D
   Li, GQ
   Gao, CY
   Fu, SW
   Liang, Y
AF Wang, Dong
   Li, Guiqing
   Gao, Chengying
   Fu, Shengwu
   Liang, Yun
TI Feature-preserving color pencil drawings from photographs
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE non-photorealistic rendering; pencil drawings; image editing; feature
   preservation
ID GENERATION
AB Color pencil drawing is well-loved due to its rich expressiveness. This paper proposes an approach for generating feature-preserving color pencil drawings from photographs. To mimic the tonal style of color pencil drawings, which are much lighter and have relatively lower saturation than photographs, we devise a lightness enhancement mapping and a saturation reduction mapping. The lightness mapping is a monotonically decreasing derivative function, which not only increases lightness but also preserves input photograph features. Color saturation is usually related to lightness, so we suppress the saturation dependent on lightness to yield a harmonious tone. Finally, two extremum operators are provided to generate a foreground-aware outline map in which the colors of the generated contours and the foreground object are consistent. Comprehensive experiments show that color pencil drawings generated by our method surpass existing methods in tone capture and feature preservation.
C1 [Wang, Dong; Fu, Shengwu; Liang, Yun] South China Agr Univ, Coll Math & Informat, Guangzhou, Peoples R China.
   [Li, Guiqing] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou, Peoples R China.
   [Gao, Chengying] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou, Peoples R China.
C3 South China Agricultural University; South China University of
   Technology; Sun Yat Sen University
RP Li, GQ (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou, Peoples R China.
EM ligq@scut.edu.cn
FU GD Natural Science Foundation [2021A1515012301, 2022A1515011425]; Key
   Research and Development Project of Guangzhou [202206010091,
   SL2022B03J01235]
FX AcknowledgementsWe thank the reviewers for their valuable comments and
   constructive suggestions. This work was supported in parts by GD Natural
   Science Foundation (2021A1515012301, 2022A1515011425), and the Key
   Research and Development Project of Guangzhou (202206010091,
   SL2022B03J01235).
CR Bhat P, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731048
   Chan C, 2022, PROC CVPR IEEE, P7905, DOI 10.1109/CVPR52688.2022.00776
   Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296
   Chen Hong., 2004, P 3 INT S NONPHOTORE, P95, DOI DOI 10.1145/987
   Cole F, 2008, ACM T GRAPHIC, V27, DOI [10.1145/1360612.1360657, 10.1145/1360612.1360687]
   Gao CY, 2018, COMPUT GRAPH FORUM, V37, P395, DOI 10.1111/cgf.13334
   github, SKETCHKERAS
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Huang ZY, 2022, COMPUT VIS MEDIA, V8, P63, DOI 10.1007/s41095-021-0227-7
   Inoue N, 2019, COMPUT GRAPH FORUM, V38, P69, DOI 10.1111/cgf.13817
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jin YX, 2019, COMPUT ANIMAT VIRT W, V30, DOI 10.1002/cav.1890
   Jing YH, 2021, IEEE T CYBERNETICS, V51, P568, DOI 10.1109/TCYB.2019.2904768
   Kang H, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P43
   Kim GJS, 2014, PALGRAVE PIVOT, P1, DOI 10.1057/9781137382986.0005
   Lewis D., 1984, PENCIL DRAWING TECHN
   Li SC, 2020, LECT NOTES COMPUT SC, V12143, P285, DOI 10.1007/978-3-030-50436-6_21
   Li T, 2022, MULTIMED TOOLS APPL, V81, P34245, DOI 10.1007/s11042-021-11028-2
   Li YJ, 2019, PROC CVPR IEEE, P1525, DOI 10.1109/CVPR.2019.00162
   Liao J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073683
   Lu Cewu., 2012, Proc. NPAR, P65
   Ma SP, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103583
   MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020
   Peng H.-W., 2014, Journal of Applied Science and Engineering, V17, P341
   Sheng L, 2018, PROC CVPR IEEE, P8242, DOI 10.1109/CVPR.2018.00860
   Son MJ, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P333, DOI 10.1109/PG.2007.63
   Spicker M., 2015, SIGGRAPH AS 2015 TEC
   Tong ZY, 2021, AAAI CONF ARTIF INTE, V35, P609
   Veliz Zahira., 1982, Studies in Conservation, V27, P49
   Wang D, 2017, COMPUT GRAPH FORUM, V36, P93, DOI 10.1111/cgf.13275
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Winnemoeller H, 2012, COMPUT GRAPH-UK, V36, P740, DOI 10.1016/j.cag.2012.03.004
   Xue Y, 2022, COMPUT VIS MEDIA, V8, P3, DOI 10.1007/s41095-021-0234-8
   Yi R, 2021, IEEE T PATTERN ANAL, V43, P3462, DOI 10.1109/TPAMI.2020.2987931
   Yi R, 2019, PROC CVPR IEEE, P10735, DOI 10.1109/CVPR.2019.01100
   Zhou J, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P1027
   Zhou L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2438546
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 39
TC 1
Z9 1
U1 3
U2 6
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2023
VL 9
IS 4
BP 807
EP 825
DI 10.1007/s41095-022-0320-6
EA JUN 2023
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P1QC2
UT WOS:001022137200001
OA gold
DA 2024-07-18
ER

PT J
AU Fu, Q
   He, SH
   Fu, HB
   Li, XM
   Deng, ZG
AF Fu, Qiang
   He, Shuhan
   Fu, Hongbo
   Li, Xueming
   Deng, Zhigang
TI Fuzzy-based indoor scene modeling with differentiated examples
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE indoor scene modeling; modeling by example; fuzzy sets
ID VISUAL COMFORT
AB Well-designed indoor scenes incorporate interior design knowledge, which has been an essential prior for most indoor scene modeling methods. However, the layout qualities of indoor scene datasets are often uneven, and most existing data-driven methods do not differentiate indoor scene examples in terms of quality. In this work, we aim to explore an approach that leverages datasets with differentiated indoor scene examples for indoor scene modeling. Our solution conducts subjective evaluations on lightweight datasets having various room configurations and furniture layouts, via pairwise comparisons based on fuzzy set theory. We also develop a systemto use such examples to guide indoor scene modeling using user-specified objects. Specifically, we focus on object groups associated with certain human activities, and define room features to encode the relations between the position and direction of an object group and the room configuration. To perform indoor scene modeling, given an empty room, our system first assesses it in terms of the user-specified object groups, and then places associated objects in the room guided by the assessment results. A series of experimental results and comparisons to state-of-the-art indoor scene synthesis methods are presented to validate the usefulness and effectiveness of our approach.
C1 [Fu, Qiang; He, Shuhan; Li, Xueming] Beijing Univ Posts & Telecommun, Sch Digital Media & Design Arts, Beijing, Peoples R China.
   [Fu, Hongbo] City Univ Hong Kong, Sch Creat Media, Hong Kong, Peoples R China.
   [Deng, Zhigang] Univ Houston, Dept Comp Sci, Houston, TX 77204 USA.
C3 Beijing University of Posts & Telecommunications; City University of
   Hong Kong; University of Houston System; University of Houston
RP Deng, ZG (corresponding author), Univ Houston, Dept Comp Sci, Houston, TX 77204 USA.
EM u.john.qiang@gmail.com; heshuhan@bupt.edu.cn; lixm@bupt.edu.cn;
   zdeng4@uh.edu
OI Deng, Zhigang/0000-0002-0452-8676
FU National Natural Science Foundation of China [61902032]; Research Grants
   Council of the Hong Kong Special Administrative Region, China [CityU
   11237116]; City University of Hong Kong [7004915]
FX AcknowledgementsWe thank the anonymous reviewers for their constructive
   comments. This work was partially supported by grants from the National
   Natural Science Foundation of China (61902032), Research Grants Council
   of the Hong Kong Special Administrative Region, China (CityU 11237116),
   and City University of Hong Kong (7004915).
CR Chen K, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661239
   Chen XW, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P321, DOI 10.1145/2733373.2806274
   Fisher M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818057
   Fisher M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366154
   Fisher M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964929
   Fisher M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866204
   Frontczak M, 2011, BUILD ENVIRON, V46, P922, DOI 10.1016/j.buildenv.2010.10.021
   Fu Q, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130805
   Huang L, 2012, BUILD ENVIRON, V49, P304, DOI 10.1016/j.buildenv.2011.07.022
   Jiang Yun., 2012, P INT C MACH LEARN
   Klir G. J., 1995, Fuzzy Sets and Fuzzy Logic: Theory and Applications
   Konis K, 2014, ENERG BUILDINGS, V77, P67, DOI 10.1016/j.enbuild.2014.03.035
   Li MY, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3303766
   Liu TQ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661243
   Ma R, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275035
   Ma R, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980223
   Merrell P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964982
   Ochoa CE, 2006, BUILD ENVIRON, V41, P1128, DOI 10.1016/j.buildenv.2005.05.001
   Qi SY, 2018, PROC CVPR IEEE, P5899, DOI 10.1109/CVPR.2018.00618
   Saaty T., 1980, The Analytic Hierarchy Process: Planning, Priority Setting, Resource Allocation
   Savva M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925867
   Shao TJ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661288
   Sharf A, 2014, COMPUT GRAPH FORUM, V33, P2, DOI 10.1111/cgf.12204
   sketchup, 3D WAREHOUSE
   Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28
   Wang K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201362
   Wang K, 2020, AM J EMERG MED, V38, P132, DOI 10.1016/j.ajem.2019.06.046
   Xu K., 2013, ACM Trans. Graph., V32, P1
   Xu K, 2014, IEEE IPCCC, DOI 10.1109/PCCC.2014.7017103
   Yu LF, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964981
   Zhang SK, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P965, DOI 10.1145/3474085.3475194
   Zhang SH, 2022, IEEE T VIS COMPUT GR, V28, P3082, DOI 10.1109/TVCG.2021.3050143
   Zhang ZW, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3381866
NR 33
TC 0
Z9 0
U1 5
U2 9
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2023
VL 9
IS 4
BP 717
EP 732
DI 10.1007/s41095-022-0299-z
EA MAY 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P1QC2
UT WOS:000999033300001
OA gold
DA 2024-07-18
ER

PT J
AU Su, ZJ
   Huang, HB
   Ma, CY
   Huang, H
   Hu, RZ
AF Su, Zejia
   Huang, Haibin
   Ma, Chongyang
   Huang, Hui
   Hu, Ruizhen
TI Point cloud completion via structured feature maps using feedback
   network
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE 3D point clouds; shape completion; geometry processing; deep learning
AB In this paper, we tackle the challenging problem of point cloud completion from the perspective of feature learning. Our key observation is that to recover the underlying structures as well as surface details, given partial input, a fundamental component is a good feature representation that can capture both global structure and local geometric details. We accordingly first propose FSNet, a feature structuring module that can adaptively aggregate point-wise features into a 2D structured feature map by learning multiple latent patterns from local regions. We then integrate FSNet into a coarse-to-fine pipeline for point cloud completion. Specifically, a 2D convolutional neural network is adopted to decode feature maps from FSNet into a coarse and complete point cloud. Next, a point cloud upsampling network is used to generate a dense point cloud from the partial input and the coarse intermediate output. To efficiently exploit local structures and enhance point distribution uniformity, we propose IFNet, a point upsampling module with a self-correction mechanism that can progressively refine details of the generated dense point cloud. We have conducted qualitative and quantitative experiments on ShapeNet, MVP, and KITTI datasets, which demonstrate that our method outperforms state-of-the-art point cloud completion approaches.
C1 [Su, Zejia; Huang, Hui; Hu, Ruizhen] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen, Peoples R China.
   [Huang, Haibin; Ma, Chongyang] Kuaishou Technol, Beijing, Peoples R China.
C3 Shenzhen University
RP Hu, RZ (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen, Peoples R China.
EM suzejia2021@email.szu.edu.cn; jackiehuanghaibin@gmail.com;
   chongyangm@gmail.com; huihuang@szu.edu.cn; ruizhen.hu@szu.edu.cn
RI Huang, Haibin/HHZ-1901-2022
FU National Natural Science Foundation of China [61872250, U2001206,
   U21B2023]; GD Natural Science Foundation [2021B1515020085]; DEGP
   Innovation Team [2022KCXTD025]; Shenzhen Science and Technology
   Innovation Program [JCYJ20210324120213036]; Guangdong Laboratory of
   Artificial Intelligence and Digital Economy
FX We thank the anonymous reviewers for their valuable comments. This work
   was supported by the National Natural Science Foundation of China
   (61872250, U2001206, U21B2023), the GD Natural Science Foundation
   (2021B1515020085), DEGP Innovation Team (2022KCXTD025), Shenzhen Science
   and Technology Innovation Program (JCYJ20210324120213036), and Guangdong
   Laboratory of Artificial Intelligence and Digital Economy (SZ).
CR Alliegro A, 2021, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR46437.2021.00460
   Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Berger M., 2014, EUROGRAPHICS 2014 ST, P161
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   Hu T, 2019, IEEE INT CONF COMP V, P4114, DOI 10.1109/ICCVW.2019.00506
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Kingma D. P., 2014, arXiv
   Knapitsch A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073599
   Kruse T, 2013, ROBOT AUTON SYST, V61, P1726, DOI 10.1016/j.robot.2013.05.007
   Lee J, 2019, PR MACH LEARN RES, V97
   Li RH, 2019, IEEE I CONF COMP VIS, P7202, DOI 10.1109/ICCV.2019.00730
   Liu MH, 2020, AAAI CONF ARTIF INTE, V34, P11596
   Mitra NJ, 2013, COMPUT GRAPH FORUM, V32, P1, DOI 10.1111/cgf.12010
   Pan L, 2021, PROC CVPR IEEE, P8520, DOI 10.1109/CVPR46437.2021.00842
   Pan L, 2020, IEEE ROBOT AUTOM LET, V5, P4392, DOI 10.1109/LRA.2020.2994483
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qian GC, 2021, PROC CVPR IEEE, P11678, DOI 10.1109/CVPR46437.2021.01151
   Qi CR, 2017, Arxiv, DOI [arXiv:1706.02413, DOI 10.48550/ARXIV.1706.02413]
   Sarmad M, 2019, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR.2019.00605
   Stutz D, 2018, PROC CVPR IEEE, P1955, DOI 10.1109/CVPR.2018.00209
   Tarini M, 2005, GRAPH MODELS, V67, P233, DOI 10.1016/j.gmod.2004.11.002
   Tchapmi LP, 2019, PROC CVPR IEEE, P383, DOI 10.1109/CVPR.2019.00047
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang XX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13169, DOI 10.1109/ICCV48922.2021.01294
   Wang XG, 2020, PROC CVPR IEEE, P787, DOI 10.1109/CVPR42600.2020.00087
   Wang YF, 2019, PROC CVPR IEEE, P5951, DOI 10.1109/CVPR.2019.00611
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wen X, 2021, PROC CVPR IEEE, P7439, DOI 10.1109/CVPR46437.2021.00736
   Wen X, 2021, PROC CVPR IEEE, P13075, DOI 10.1109/CVPR46437.2021.01288
   Wen X, 2020, PROC CVPR IEEE, P1936, DOI 10.1109/CVPR42600.2020.00201
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xia YQ, 2021, Arxiv, DOI [arXiv:2104.09587, 10.48550/arXiv.2104.09587]
   Xiang P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5479, DOI 10.1109/ICCV48922.2021.00545
   Xie CL, 2021, PROC CVPR IEEE, P4617, DOI 10.1109/CVPR46437.2021.00459
   Xie Haozhe, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P365, DOI 10.1007/978-3-030-58545-7_21
   Yida Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P70, DOI 10.1007/978-3-030-58580-8_5
   Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295
   Yu LQ, 2018, LECT NOTES COMPUT SC, V11211, P398, DOI 10.1007/978-3-030-01234-2_24
   Yu XM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12478, DOI 10.1109/ICCV48922.2021.01227
   Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088
   Yue Qian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P752, DOI 10.1007/978-3-030-58529-7_44
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang W., 2020, P EUR C COMP VIS, P512, DOI 10.1007/978-3-030-58595-2_31
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zitian Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7659, DOI 10.1109/CVPR42600.2020.00768
   Zong DM, 2021, AAAI CONF ARTIF INTE, V35, P3625
NR 51
TC 7
Z9 7
U1 4
U2 39
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2023
VL 9
IS 1
BP 71
EP 85
DI 10.1007/s41095-022-0276-6
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5K7GQ
UT WOS:000869891100005
OA gold, Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, D
   Tang, F
   Dong, WM
   Yao, HX
   Xu, CS
AF Chen, Dong
   Tang, Fan
   Dong, Weiming
   Yao, Hanxing
   Xu, Changsheng
TI SiamCPN: Visual tracking with the Siamese center-prediction network
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE Siamese network; single object tracking; anchor-free; center point
   detection
ID OBJECT TRACKING
AB Object detection is widely used in object tracking; anchor-free object tracking provides an end-to-end single-object-tracking approach. In this study, we propose a new anchor-free network, the Siamese center-prediction network (SiamCPN). Given the presence of referenced object features in the initial frame, we directly predict the center point and size of the object in subsequent frames in a Siamese-structure network without the need for perframe post-processing operations. Unlike other anchor-free tracking approaches that are based on semantic segmentation and achieve anchor-free tracking by pixel-level prediction, SiamCPN directly obtains all information required for tracking, greatly simplifying the model. A center-prediction sub-network is applied to multiple stages of the backbone to adaptively learn from the experience of different branches of the Siamese net. The model can accurately predict object location, implement appropriate corrections, and regress the size of the target bounding box. Compared to other leading Siamese networks, SiamCPN is simpler, faster, and more efficient as it uses fewer hyperparameters. Experiments demonstrate that our method outperforms other leading Siamese networks on GOT-10K and UAV123 benchmarks, and is comparable to other excellent trackers on LaSOT, VOT2016, and OTB-100 while improving inference speed 1.5 to 2 times.
C1 [Chen, Dong; Dong, Weiming; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100040, Peoples R China.
   [Chen, Dong; Dong, Weiming; Xu, Changsheng] Chinese Acad Sci, Inst Automat, NLPR, Beijing 100190, Peoples R China.
   [Tang, Fan] Jilin Univ, Sch Artificial Intelligence, Changchun 130012, Peoples R China.
   [Chen, Dong; Dong, Weiming; Yao, Hanxing; Xu, Changsheng] CASIA LLVISION Joint Lab, Beijing 100190, Peoples R China.
   [Yao, Hanxing] LLVISION Technol Co LTD, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; Institute of Automation, CAS; Jilin
   University
RP Dong, WM (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100040, Peoples R China.; Dong, WM (corresponding author), Chinese Acad Sci, Inst Automat, NLPR, Beijing 100190, Peoples R China.; Dong, WM (corresponding author), CASIA LLVISION Joint Lab, Beijing 100190, Peoples R China.
EM chendong2018@ia.ac.cn; tangfan@jlu.edu.cn; weiming.dong@ia.ac.cn;
   yaohx@llvision.com; changsheng.xu@ia.ac.cn
RI Chen, Dong/JUF-1340-2023; Xu, Chang/GQP-7280-2022; xu, cj/HJZ-3488-2023;
   DONG, Weiming/AAG-7678-2020; Tang, Fan/O-3923-2018
OI DONG, Weiming/0000-0001-6502-145X; Tang, Fan/0000-0002-3975-2483; Chen,
   Dong/0009-0000-5028-656X
FU National Key RAMP;D Program of China [2018YFC0807500]; National Natural
   Science Foundation of China [U20B2070, 61832016]
FX We thank the anonymous reviewers for their valuable comments. This work
   was supported by the National Key R&D Program of China (Grant No.
   2018YFC0807500), and the National Natural Science Foundation of China
   (Grant Nos. U20B2070 and 61832016).
CR Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan Martin, 2014, BRIT MACH VIS C
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Fan Ruochen, 2017, [Computational Visual Media, 计算可视媒体], V3, P285
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   Han G, 2019, IEEE ACCESS, V7, P123934, DOI 10.1109/ACCESS.2019.2937998
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hong ZB, 2015, PROC CVPR IEEE, P749, DOI 10.1109/CVPR.2015.7298675
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kristan M, 2016, LECT NOTES COMPUT SC, V9914, P777, DOI 10.1007/978-3-319-48881-3_54
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Law H, CORNERNET DETECTING, P765, DOI [10.1007/978-3-030-01264-9_45Vol.11218, DOI 10.1007/978-3-030-01264-9_45VOL.11218]
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395
   Peng S.Y., 2020, ARXIV200607560
   Real E, 2017, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR.2017.789
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Wang GT, 2019, PROC CVPR IEEE, P3638, DOI 10.1109/CVPR.2019.00376
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xu YD, 2020, AAAI CONF ARTIF INTE, V34, P12549
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhipeng Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P771, DOI 10.1007/978-3-030-58589-1_46
   Zhou X., 2019, arXiv
   Zhu Z, 2018, LECT NOTES COMPUT SC, V11213, P103, DOI 10.1007/978-3-030-01240-3_7
NR 48
TC 4
Z9 5
U1 1
U2 51
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2021
VL 7
IS 2
BP 253
EP 265
DI 10.1007/s41095-021-0212-1
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FR
UT WOS:000648692900008
OA gold
DA 2024-07-18
ER

PT J
AU Ramadan, H
   Lachqar, C
   Tairi, H
AF Ramadan, Hiba
   Lachqar, Chaymae
   Tairi, Hamid
TI A survey of recent interactive image segmentation methods
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review
DE interactive image segmentation; user interaction; label propagation;
   deep learning; superpixels
ID ACTIVE CONTOUR MODEL; OBJECT SEGMENTATION; GRAPH CUT; FOREGROUND
   EXTRACTION; ENERGY MINIMIZATION; RANDOM-WALKS; LIVE WIRE; DIFFUSION;
   INFORMATION; ALGORITHMS
AB Image segmentation is one of the most basic tasks in computer vision and remains an initial step of many applications. In this paper, we focus on interactive image segmentation (IIS), often referred to as foreground-background separation or object extraction, guided by user interaction. We provide an overview of the IIS literature by covering more than 150 publications, especially recent works that have not been surveyed before. Moreover, we try to give a comprehensive classification of them according to different viewpoints and present a general and concise comparison of the most recent published works. Furthermore, we survey widely used datasets, evaluation metrics, and available resources in the field of IIS.
C1 [Ramadan, Hiba; Tairi, Hamid] Univ Sidi Mohamed Ben Abdellah, Fac Sci Dhar El Mahraz, Dept Informat, Fes 30000, Morocco.
   [Lachqar, Chaymae] Univ Sidi Mohamed Ben Abdellah, Fac Med & Pharm, Fes 30000, Morocco.
C3 Sidi Mohamed Ben Abdellah University of Fez; Sidi Mohamed Ben Abdellah
   University of Fez
RP Ramadan, H (corresponding author), Univ Sidi Mohamed Ben Abdellah, Fac Sci Dhar El Mahraz, Dept Informat, Fes 30000, Morocco.
EM hiba.ramadan@usmba.ac.ma
RI Ramadan, Hiba/L-3244-2019
OI Ramadan, Hiba/0000-0002-3586-4760
CR Abdelsamea MM, 2015, NEUROCOMPUTING, V149, P820, DOI 10.1016/j.neucom.2014.07.052
   Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Acuna D, 2018, PROC CVPR IEEE, P859, DOI 10.1109/CVPR.2018.00096
   ADAMS R, 1994, IEEE T PATTERN ANAL, V16, P641, DOI 10.1109/34.295913
   Agustsson E, 2019, PROC CVPR IEEE, P11614, DOI 10.1109/CVPR.2019.01189
   Ali H, 2018, IEEE T IMAGE PROCESS, V27, P3729, DOI 10.1109/TIP.2018.2825101
   Alpert S, 2012, IEEE T PATTERN ANAL, V34, P315, DOI 10.1109/TPAMI.2011.130
   [Anonymous], 2010, P 2010 ACM MULT WORK
   [Anonymous], 2013, SPRINGERBRIEFS ELECT, DOI DOI 10.3897/ZOOKEYS.257.3832
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Badshah N, 2010, COMMUN COMPUT PHYS, V7, P759, DOI 10.4208/cicp.2009.09.026
   Bai JJ, 2014, PROC CVPR IEEE, P392, DOI 10.1109/CVPR.2014.57
   Bai XF, 2007, IEEE IC COMP COM NET, P1
   Bampis CG, 2017, IEEE T IMAGE PROCESS, V26, P35, DOI 10.1109/TIP.2016.2621663
   Bampis CG, 2015, IEEE IMAGE PROC, P2265, DOI 10.1109/ICIP.2015.7351205
   Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080
   Beare R, 2006, IEEE T PATTERN ANAL, V28, P1063, DOI 10.1109/TPAMI.2006.132
   Beare R, 2002, MATHEMATICAL MORPHOLOGY, PROCEEDINGS, P91
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Ben Ayed I, 2013, PROC CVPR IEEE, P1304, DOI 10.1109/CVPR.2013.172
   Benenson R, 2019, PROC CVPR IEEE, P11692, DOI 10.1109/CVPR.2019.01197
   BESAG J, 1986, J R STAT SOC B, V48, P259
   Blake A, 2004, LECT NOTES COMPUT SC, V3021, P428
   Boroujerdi AS, 2017, 2017 13TH INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY AND INTERNET-BASED SYSTEMS (SITIS), P103, DOI 10.1109/SITIS.2017.27
   Borovec J, 2017, J ELECTRON IMAGING, V26, DOI 10.1117/1.JEI.26.6.061611
   Borovec J, 2017, J ELECTRON IMAGING, V26, DOI 10.1117/1.JEI.26.6.061610
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Bresson X, 2007, J MATH IMAGING VIS, V28, P151, DOI 10.1007/s10851-007-0002-0
   Breve F, 2019, EXPERT SYST APPL, V123, P18, DOI 10.1016/j.eswa.2019.01.031
   Bulò SR, 2017, EUR J OPER RES, V262, P1, DOI 10.1016/j.ejor.2017.03.056
   Casaca W, 2014, PROC CVPR IEEE, P384, DOI 10.1109/CVPR.2014.56
   Castrejón L, 2017, PROC CVPR IEEE, P4485, DOI 10.1109/CVPR.2017.477
   Cerrone L, 2019, PROC CVPR IEEE, P12551, DOI 10.1109/CVPR.2019.01284
   Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291
   Chen D., 2016, P BMVC
   Chen D, 2017, INT J COMPUT VISION, V122, P458, DOI 10.1007/s11263-016-0975-5
   Chen D, 2016, PROC CVPR IEEE, P355, DOI 10.1109/CVPR.2016.45
   Chen DJ, 2018, J VIS COMMUN IMAGE R, V55, P393, DOI 10.1016/j.jvcir.2018.06.011
   Chen DJ, 2018, MACH VISION APPL, V29, P617, DOI 10.1007/s00138-018-0923-1
   Chen DJ, 2017, LECT NOTES COMPUT SC, V10111, P261, DOI 10.1007/978-3-319-54181-5_17
   CHEN L, 1994, INFORM SCI-APPL, V1, P77, DOI 10.1016/1069-0115(94)90009-4
   Chen LC, 2014, PROC CVPR IEEE, P3198, DOI 10.1109/CVPR.2014.409
   Chen Xinjian, 2018, IEEE Rev Biomed Eng, V11, P112, DOI 10.1109/RBME.2018.2798701
   Chen YS, 2015, IEEE T IMAGE PROCESS, V24, P873, DOI 10.1109/TIP.2015.2389612
   Chen YS, 2012, PROC CVPR IEEE, P654, DOI 10.1109/CVPR.2012.6247733
   Cheng MM, 2015, COMPUT GRAPH FORUM, V34, P193, DOI 10.1111/cgf.12758
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Choi J, 2015, IEEE IMAGE PROC, P1090, DOI 10.1109/ICIP.2015.7350968
   Chongbo Zhou, 2012, 2012 International Conference on Computer Science and Service System (CSSS), P1781, DOI 10.1109/CSSS.2012.444
   Ciesielski KC, 2013, MED IMAGE ANAL, V17, P1046, DOI 10.1016/j.media.2013.06.006
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Couprie C, 2011, IEEE T PATTERN ANAL, V33, P1384, DOI 10.1109/TPAMI.2010.200
   Cremers D, 2004, LECT NOTES COMPUT SC, V3175, P36
   Criminisi A, 2008, LECT NOTES COMPUT SC, V5302, P99, DOI 10.1007/978-3-540-88682-2_9
   Dai LZ, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P903, DOI 10.1109/ICCVW.2015.120
   Das P, 2009, IMAGE VISION COMPUT, V27, P206, DOI 10.1016/j.imavis.2008.02.006
   Dong RR, 2016, CHIN CONT DECIS CONF, P777, DOI 10.1109/CCDC.2016.7531090
   Dong XP, 2016, IEEE T IMAGE PROCESS, V25, P516, DOI 10.1109/TIP.2015.2505184
   DUBUISSON MP, 1994, INT C PATT RECOG, P566, DOI 10.1109/ICPR.1994.576361
   Duchenne O., 2008, IEEE COMPUTER VISION, P1, DOI DOI 10.1109/CVPR.2008.4587419
   Ducournau A, 2014, COMPUT VIS IMAGE UND, V120, P91, DOI 10.1016/j.cviu.2013.10.012
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Falcao AX, 1998, GRAPH MODEL IM PROC, V60, P233, DOI 10.1006/gmip.1998.0475
   Falcao AX, 2004, IEEE T PATTERN ANAL, V26, P19, DOI 10.1109/TPAMI.2004.1261076
   Falcao AX, 2000, IEEE T MED IMAGING, V19, P55, DOI 10.1109/42.832960
   Fan HP, 2005, PATTERN RECOGN LETT, V26, P1139, DOI 10.1016/j.patrec.2004.10.010
   Fan MJ, 2015, IET IMAGE PROCESS, V9, P478, DOI 10.1049/iet-ipr.2014.0490
   Fathi A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.78
   FIGUEIREDO M.A. T., 2006, find journal (adv Neural Inform process Systems), V19, P401
   Freedman D, 2005, PROC CVPR IEEE, P755
   Freixenet J, 2002, LECT NOTES COMPUT SC, V2352, P408, DOI 10.1007/3-540-47977-5_27
   Friedland G, 2005, IEEE INT SYM MULTIM, P253
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Gimp G N U, 2008, USER MANUAL EDGE DET, V8, P8
   Goldstein T, 2010, J SCI COMPUT, V45, P272, DOI 10.1007/s10915-009-9331-z
   Gong YC, 2016, INT CONF ACOUST SPEE, P1801, DOI 10.1109/ICASSP.2016.7471987
   Gorelick L, 2014, LECT NOTES COMPUT SC, V8693, P675, DOI 10.1007/978-3-319-10602-1_44
   Gorelick L, 2013, PROC CVPR IEEE, P1714, DOI 10.1109/CVPR.2013.224
   Gout C, 2005, NUMER ALGORITHMS, V39, P155, DOI 10.1007/s11075-004-3627-8
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   GREIG DM, 1989, J ROY STAT SOC B MET, V51, P271, DOI 10.1111/j.2517-6161.1989.tb01764.x
   Gulshan V, 2010, PROC CVPR IEEE, P3129, DOI 10.1109/CVPR.2010.5540073
   Ham B, 2013, IEEE T IMAGE PROCESS, V22, P2574, DOI 10.1109/TIP.2013.2253479
   Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He K, 2018, IEEE ACCESS, V6, P67732, DOI 10.1109/ACCESS.2018.2878422
   Heimann T., 2004, 20 ISPRS C, VXXXV, P317
   Hernandez-Vela A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1276, DOI 10.1109/ICCVW.2011.6130398
   Hu EL, 2010, IEEE T NEURAL NETWOR, V21, P1831, DOI 10.1109/TNN.2010.2076301
   Hu Y, 2019, NEURAL NETWORKS, V109, P31, DOI 10.1016/j.neunet.2018.10.009
   Jain S, 2018, LECT NOTES ELECTR EN, V453, P189, DOI 10.1007/978-981-10-5565-2_17
   Jain SD, 2019, INT J COMPUT VISION, V127, P1321, DOI 10.1007/s11263-019-01184-2
   Jang WD, 2019, PROC CVPR IEEE, P5292, DOI 10.1109/CVPR.2019.00544
   Jegelka S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1897, DOI 10.1109/CVPR.2011.5995589
   Jian M, 2016, IEEE T IMAGE PROCESS, V25, P1301, DOI 10.1109/TIP.2016.2518480
   Jung C, 2014, PATTERN RECOGN, V47, P2745, DOI 10.1016/j.patcog.2014.02.010
   Kim TH, 2008, LECT NOTES COMPUT SC, V5304, P264
   Kim TH, 2010, PROC CVPR IEEE, P3201, DOI 10.1109/CVPR.2010.5540078
   Kohli P, 2013, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2013.257
   Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0
   Kolotouros N., 2017, ARXIV171004346
   Kowdle A, 2011, PROC CVPR IEEE, P929, DOI 10.1109/CVPR.2011.5995638
   Lee CP, 2005, IEEE INT CONF ROBOT, P4242
   Lempitsky V, 2009, IEEE I CONF COMP VIS, P277, DOI 10.1109/ICCV.2009.5459262
   Li H., 2015, COMPUTATIONAL VISUAL, V1, P183
   Li H, 2018, INFORM SCIENCES, V450, P53, DOI 10.1016/j.ins.2018.03.016
   Li KQ, 2015, IEEE T MULTIMEDIA, V17, P994, DOI 10.1109/TMM.2015.2433795
   Li WB, 2015, IEEE IMAGE PROC, P2900, DOI 10.1109/ICIP.2015.7351333
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Li YP, 2020, INFORM SCIENCES, V506, P443, DOI 10.1016/j.ins.2019.08.021
   Li ZW, 2018, PROC CVPR IEEE, P577, DOI 10.1109/CVPR.2018.00067
   Liew JH, 2019, IEEE I CONF COMP VIS, P662, DOI 10.1109/ICCV.2019.00075
   Liew JH, 2017, IEEE I CONF COMP VIS, P2746, DOI 10.1109/ICCV.2017.297
   Likas A, 2003, PATTERN RECOGN, V36, P451, DOI 10.1016/S0031-3203(02)00060-2
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Ling H, 2019, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR.2019.00540
   Liu DD, 2010, IEEE IMAGE PROC, P225, DOI 10.1109/ICIP.2010.5652012
   Liu YG, 2012, IEEE T VIS COMPUT GR, V18, P202, DOI 10.1109/TVCG.2011.77
   Lobacheva E, 2015, IEEE I CONF COMP VIS, P1626, DOI 10.1109/ICCV.2015.190
   Long JW, 2018, SYMMETRY-BASEL, V10, DOI 10.3390/sym10050169
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo LK, 2019, VISUAL COMPUT, V35, P1869, DOI 10.1007/s00371-018-1580-0
   Luo LK, 2017, J VIS COMMUN IMAGE R, V43, P138, DOI 10.1016/j.jvcir.2016.12.012
   Mahadevan S., 2018, BRIT MACH VIS C 2018, P212
   Majumder S, 2019, PROC CVPR IEEE, P11594, DOI 10.1109/CVPR.2019.01187
   Maninis KK, 2018, PROC CVPR IEEE, P616, DOI 10.1109/CVPR.2018.00071
   Mansilla LAC, 2016, SIBGRAPI, P289, DOI [10.1109/SIBGRAPI.2016.047, 10.1109/SIBGRAPI.2016.44]
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mathieu B, 2017, J ELECTRON IMAGING, V26, DOI 10.1117/1.JEI.26.6.061606
   McGuinness K, 2010, PATTERN RECOGN, V43, P434, DOI 10.1016/j.patcog.2009.03.008
   Meena S, 2016, IEEE IMAGE PROC, P844, DOI 10.1109/ICIP.2016.7532476
   Mehnert A, 1997, PATTERN RECOGN LETT, V18, P1065, DOI 10.1016/S0167-8655(97)00131-1
   Meila M., 2005, P 22 INT C MACHINE L, P577
   Mequanint EZ, 2019, IEEE T PATTERN ANAL, V41, P2438, DOI 10.1109/TPAMI.2018.2858243
   Meshry M., 2015, P BRIT MACH VIS C
   Mille J, 2015, INT J COMPUT VISION, V112, P1, DOI 10.1007/s11263-014-0751-3
   Mille J, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.133
   Minaee S, 2020, ARXIV200105566
   Miranda PAV, 2012, IEEE T IMAGE PROCESS, V21, P3042, DOI 10.1109/TIP.2012.2188034
   Mortensen EN, 1998, GRAPH MODEL IM PROC, V60, P349, DOI 10.1006/gmip.1998.0480
   Mylona EA, 2013, IEEE INT SYMP SIGNAL, P344, DOI 10.1109/ISSPIT.2013.6781905
   Nieuwenhuis C, 2013, IEEE T PATTERN ANAL, V35, P1234, DOI 10.1109/TPAMI.2012.183
   Ning JF, 2010, PATTERN RECOGN, V43, P445, DOI 10.1016/j.patcog.2009.03.004
   Noma A, 2012, PATTERN RECOGN, V45, P1159, DOI 10.1016/j.patcog.2011.08.017
   Noma A, 2011, PATTERN RECOGN LETT, V32, P3, DOI 10.1016/j.patrec.2010.02.016
   Oh C, 2017, EXPERT SYST APPL, V79, P90, DOI 10.1016/j.eswa.2017.02.031
   Oh C, 2017, LECT NOTES COMPUT SC, V10111, P229, DOI 10.1007/978-3-319-54181-5_15
   Peng B, 2013, PATTERN RECOGN, V46, P1020, DOI 10.1016/j.patcog.2012.09.015
   Peng B, 2011, PATTERN RECOGN, V44, P2527, DOI 10.1016/j.patcog.2011.03.024
   Peng Y, 2014, IEEE IMAGE PROC, P4383, DOI 10.1109/ICIP.2014.7025889
   Peng ZL, 2019, SIGNAL PROCESS-IMAGE, V78, P159, DOI 10.1016/j.image.2019.06.012
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Price BL, 2010, PROC CVPR IEEE, P3161, DOI 10.1109/CVPR.2010.5540079
   Rajchl M, 2017, IEEE T MED IMAGING, V36, P674, DOI 10.1109/TMI.2016.2621185
   Ren Y, 2003, PATTERN RECOGN LETT, V24, P183, DOI 10.1016/S0167-8655(02)00210-6
   Rhemann C, 2009, PROC CVPR IEEE, P1826, DOI 10.1109/CVPRW.2009.5206503
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Rupprecht C, 2015, PROC CVPR IEEE, P3314, DOI 10.1109/CVPR.2015.7298952
   Salembier P, 2000, IEEE T IMAGE PROCESS, V9, P561, DOI 10.1109/83.841934
   Scheuermann B, 2011, LECT NOTES COMPUT SC, V6688, P656, DOI 10.1007/978-3-642-21227-7_61
   Sener O., 2012, P 2 ACM INT WORKSH I, P9
   Shen JB, 2014, IEEE T CIRC SYST VID, V24, P1088, DOI 10.1109/TCSVT.2014.2302545
   Shen JB, 2014, IEEE T IMAGE PROCESS, V23, P1451, DOI 10.1109/TIP.2014.2302892
   Shi R., 2011, P VIS COMM IM PROC, P1
   Shi R, 2018, SIGNAL PROCESS-IMAGE, V65, P107, DOI 10.1016/j.image.2018.03.020
   Sinop AK, 2007, IEEE I CONF COMP VIS, P1016, DOI 10.1109/iccv.2007.4408927
   Sofiiuk K., 2020, IEEE CVF C COMP VIS, P8623
   Song G, 2018, PROC CVPR IEEE, P1760, DOI 10.1109/CVPR.2018.00189
   Sourati J, 2014, IEEE T IMAGE PROCESS, V23, P3057, DOI 10.1109/TIP.2014.2325783
   Spina Thiago Vallin, 2014, IEEE Trans Image Process, V23, P5756, DOI 10.1109/TIP.2014.2367319
   Straehle CN, 2012, PROC CVPR IEEE, P765, DOI 10.1109/CVPR.2012.6247747
   Stühmer J, 2013, IEEE I CONF COMP VIS, P2336, DOI 10.1109/ICCV.2013.290
   Stutz D, 2018, COMPUT VIS IMAGE UND, V166, P1, DOI 10.1016/j.cviu.2017.03.007
   Subr K, 2013, COMPUT GRAPH FORUM, V32, P41, DOI 10.1111/cgf.12024
   Sung MC, 2018, PROC INT WORKSH ADV
   SURI JS, 2001, ADV ALGORITHMIC APPR
   Taha A., 2017, ARXIV170200882
   Tang M., 2015, ARXIV150607439
   Tang M, 2013, IEEE I CONF COMP VIS, P1769, DOI 10.1109/ICCV.2013.222
   Tang M, 2016, LECT NOTES COMPUT SC, V9906, P748, DOI 10.1007/978-3-319-46475-6_46
   Tang M, 2015, IEEE I CONF COMP VIS, P1555, DOI 10.1109/ICCV.2015.182
   Tang M, 2014, LECT NOTES COMPUT SC, V8693, P691, DOI 10.1007/978-3-319-10602-1_45
   Nguyen TNA, 2012, IEEE T IMAGE PROCESS, V21, P3734, DOI 10.1109/TIP.2012.2191566
   Unnikrishnan R, 2007, IEEE T PATTERN ANAL, V29, P929, DOI 10.1109/TPAMI.2007.1046
   Van den Bergh M, 2012, LECT NOTES COMPUT SC, V7578, P13, DOI 10.1007/978-3-642-33786-4_2
   Veksler O, 2008, LECT NOTES COMPUT SC, V5304, P454, DOI 10.1007/978-3-540-88690-7_34
   Vezhnevets V., 2005, Proc. Graphicon, V1, P150
   Vicente S, 2008, PROC CVPR IEEE, P767
   VINCENT L, 1991, IEEE T PATTERN ANAL, V13, P583, DOI 10.1109/34.87344
   Wang B, 2012, PROC CVPR IEEE, P2312, DOI 10.1109/CVPR.2012.6247942
   Wang J, 2007, INT CONF ACOUST SPEE, P601
   Wang T, 2019, IEEE T IMAGE PROCESS, V28, P330, DOI 10.1109/TIP.2018.2867941
   Wang T, 2018, INFORM SCIENCES, V460, P103, DOI 10.1016/j.ins.2018.05.040
   Wang T, 2018, PATTERN RECOGN, V79, P440, DOI 10.1016/j.patcog.2018.02.023
   Wang T, 2016, IEEE T MULTIMEDIA, V18, P2358, DOI 10.1109/TMM.2016.2600441
   Wang T, 2016, PATTERN RECOGN, V55, P28, DOI 10.1016/j.patcog.2016.01.018
   Wang T, 2015, J VIS COMMUN IMAGE R, V33, P10, DOI 10.1016/j.jvcir.2015.08.013
   Wang T, 2015, NEUROCOMPUTING, V158, P13, DOI 10.1016/j.neucom.2015.02.010
   Wang T, 2018, IEEE SENS J, V18, P8493, DOI 10.1109/JSEN.2018.2866943
   Wang TH, 2014, COMPUT VIS IMAGE UND, V120, P14, DOI 10.1016/j.cviu.2013.10.013
   Wang XF, 2015, IEEE T IMAGE PROCESS, V24, P1399, DOI 10.1109/TIP.2015.2397313
   Wang Z, 2019, PROC CVPR IEEE, P7492, DOI 10.1109/CVPR.2019.00768
   Wu JJ, 2014, PROC CVPR IEEE, P256, DOI 10.1109/CVPR.2014.40
   Wu SQ, 2017, IEEE SIGNAL PROC LET, V24, P1803, DOI 10.1109/LSP.2017.2761393
   Xian M, 2016, INT C PATT RECOG, P1982, DOI 10.1109/ICPR.2016.7899927
   Xian M, 2016, IEEE T IMAGE PROCESS, V25, P4691, DOI 10.1109/TIP.2016.2594485
   Xian M, 2014, INT C PATT RECOG, P2495, DOI 10.1109/ICPR.2014.431
   Xiang SM, 2011, IEEE T MULTIMEDIA, V13, P342, DOI 10.1109/TMM.2010.2103930
   Xiang SM, 2009, IEEE T IMAGE PROCESS, V18, P1623, DOI 10.1109/TIP.2009.2018570
   Xie X., 2018, ARXIV180803002
   Xu N., 2017, P 28 BRIT MACH VIS C
   Xu N, 2016, PROC CVPR IEEE, P373, DOI 10.1109/CVPR.2016.47
   Yan Q, 2013, P IEEE C COMP VIS PA
   Yang WX, 2010, IEEE T IMAGE PROCESS, V19, P2470, DOI 10.1109/TIP.2010.2048611
   Yao R., 2019, ARXIV PREPRINT ARXIV
   Yu HK, 2017, IEEE IMAGE PROC, P3335, DOI 10.1109/ICIP.2017.8296900
   Yu HS, 2018, NEUROCOMPUTING, V304, P82, DOI 10.1016/j.neucom.2018.03.037
   Zemene E, 2016, LECT NOTES COMPUT SC, V9912, P278, DOI 10.1007/978-3-319-46484-8_17
   Zeng Y, 2008, COMPUT VIS IMAGE UND, V112, P81, DOI 10.1016/j.cviu.2008.07.008
   Zhang J, 2017, FRONT INFORM TECH EL, V18, P1002, DOI 10.1631/FITEE.1601401
   Zhang JY, 2010, PROC CVPR IEEE, P2125, DOI 10.1109/CVPR.2010.5539891
   Zheng HY, 2019, THIRD INTERNATIONAL SYMPOSIUM ON IMAGE COMPUTING AND DIGITAL MEDICINE (ISICDM 2019), P244, DOI 10.1145/3364836.3364885
   Zheng L., 2020, 2020 IEEECVF C COMPU, P13339
   Zhou CB, 2016, COMPUT ELECTR ENG, V54, P220, DOI 10.1016/j.compeleceng.2015.09.013
   Zhou HL, 2013, PATTERN RECOGN, V46, P1719, DOI 10.1016/j.patcog.2012.12.005
   Zhou YJ, 2015, IEEE T IMAGE PROCESS, V24, P3834, DOI 10.1109/TIP.2015.2449552
   Zhu HY, 2016, J VIS COMMUN IMAGE R, V34, P12, DOI 10.1016/j.jvcir.2015.10.012
NR 232
TC 34
Z9 35
U1 5
U2 67
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2020
VL 6
IS 4
BP 355
EP 384
DI 10.1007/s41095-020-0177-5
PG 30
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FN
UT WOS:000648692500001
OA gold
DA 2024-07-18
ER

PT J
AU Chadha, A
   Britto, J
   Roja, MM
AF Chadha, Aman
   Britto, John
   Roja, M. Mani
TI iSeeBetter: Spatio-temporal video super-resolution using recurrent
   generative back-projection networks
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE super resolution; video upscaling; frame recurrence; optical flow;
   generative adversarial networks; convolutional neural networks
ID IMAGE; RESOLUTION
AB Recently, learning-based models have enhanced the performance of single-image super-resolution (SISR). However, applying SISR successively to each video frame leads to a lack of temporal coherency. Convolutional neural networks (CNNs) outperform traditional approaches in terms of image quality metrics such as peak signal to noise ratio (PSNR) and structural similarity (SSIM). On the other hand, generative adversarial networks (GANs) offer a competitive advantage by being able to mitigate the issue of a lack of finer texture details, usually seen with CNNs when super-resolving at large upscaling factors. We present iSeeBetter, a novel GAN-based spatio-temporal approach to video super-resolution (VSR) that renders temporally consistent super-resolution videos. iSeeBetter extracts spatial and temporal information from the current and neighboring frames using the concept of recurrent back-projection networks as its generator. Furthermore, to improve the "naturality" of the super-resolved output while eliminating artifacts seen with traditional algorithms, we utilize the discriminator from super-resolution generative adversarial network. Although mean squared error (MSE) as a primary loss-minimization objective improves PSNR/SSIM, these metrics may not capture fine details in the image resulting in misrepresentation of perceptual quality. To address this, we use a four-fold (MSE, perceptual, adversarial, and total-variation loss function. Our results demonstrate that iSeeBetter offers superior VSR fidelity and surpasses state-of-the-art performance.
C1 [Chadha, Aman] Stanford Univ, Dept Comp Sci, 450 Serra Mall, Stanford, CA 94305 USA.
   [Britto, John] Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA.
   [Roja, M. Mani] Univ Mumbai, Dept Elect & Telecommun Engn, Mumbai 400032, Maharashtra, India.
C3 Stanford University; University of Massachusetts System; University of
   Massachusetts Amherst; University of Mumbai
RP Chadha, A (corresponding author), Stanford Univ, Dept Comp Sci, 450 Serra Mall, Stanford, CA 94305 USA.
EM aman@amanchadha.com; jnadar@umass.edu; maniroja@tsec.edu
RI Edinburgh, Maniroja/AAN-1996-2021
OI Edinburgh, Maniroja/0000-0003-4848-3841; Chadha,
   Aman/0000-0001-6621-9003
CR Aly HA, 2005, IEEE T IMAGE PROCESS, V14, P1647, DOI 10.1109/TIP.2005.851684
   [Anonymous], 2016, PROC 4 INT C LEARN R
   [Anonymous], 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.123
   Bruna J., 2016, P 4 INT C LEARN REPR
   Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dosovitskiy A., 2016, Advances in Neural Information Processing Systems, P658
   Drulea M, 2011, IEEE INT C INTELL TR, P318, DOI 10.1109/ITSC.2011.6082986
   Faramarzi E, 2013, IEEE T IMAGE PROCESS, V22, P2101, DOI 10.1109/TIP.2013.2237915
   Garcia DC, 2012, IEEE T CIRC SYST VID, V22, P1249, DOI 10.1109/TCSVT.2012.2198134
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gers FA, 2000, NEURAL COMPUT, V12, P2451, DOI 10.1162/089976600300015015
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hany J, 2019, IMPLEMENT NEXT GENER
   Haris M, 2019, PROC CVPR IEEE, P3892, DOI 10.1109/CVPR.2019.00402
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   Haris M, 2017, APPL OPTICS, V56, P6043, DOI 10.1364/AO.56.006043
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Huang Y., 2015, ADV NEURAL INFPROCES, V28, P235
   Irani M., 1993, Journal of Visual Communication and Image Representation, V4, P324, DOI 10.1006/jvci.1993.1030
   IRANI M, 1991, CVGIP-GRAPH MODEL IM, V53, P231, DOI 10.1016/1049-9652(91)90045-L
   Jo Y, 2018, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2018.00340
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494
   Kappeler A, 2016, IEEE T COMPUT IMAG, V2, P109, DOI 10.1109/TCI.2016.2532323
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Liao RJ, 2015, IEEE I CONF COMP VIS, P531, DOI 10.1109/ICCV.2015.68
   Liu C, 2011, PROC CVPR IEEE, P209, DOI 10.1109/CVPR.2011.5995614
   Liu D, 2017, IEEE I CONF COMP VIS, P2526, DOI 10.1109/ICCV.2017.274
   Makansi O, 2017, LECT NOTES COMPUT SC, V10496, P203, DOI 10.1007/978-3-319-66709-6_17
   Mao J, 2014, CELL DEATH DIS, V5, DOI 10.1038/cddis.2013.515
   Ren H, 2018, INT SYM COMPUT INTEL, P11, DOI 10.1109/ISCID.2018.00009
   Sajjadi MSM, 2018, PROC CVPR IEEE, P6626, DOI 10.1109/CVPR.2018.00693
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shi XJ, 2015, ADV NEUR IN, V28
   Simonyan K., 2014, 14091556 ARXIV
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tao X, 2017, IEEE I CONF COMP VIS, P4482, DOI 10.1109/ICCV.2017.479
   Tsai R.Y., 1984, Proc. Inst Elect Eng, V1, P317
   Venugopalan S., 2015, P ANN C N AM CHAPT A, P1494, DOI 10.3115/v1/N15-1173
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yang J. C., 2017, Super-resolution imaging, P1
   Yang Q, 2012, IEEE SYS MAN CYBERN, P1, DOI 10.1109/ICSMC.2012.6377667
   Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496
NR 49
TC 15
Z9 15
U1 0
U2 8
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2020
VL 6
IS 3
BP 307
EP 317
DI 10.1007/s41095-020-0175-7
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FH
UT WOS:000648691900006
OA gold, Green Submitted
DA 2024-07-18
ER

PT J
AU Zhu, JQ
   Zhao, SZ
   Xu, YN
   Meng, XX
   Wang, L
   Yan, LQ
AF Zhu, Junqiu
   Zhao, Sizhe
   Xu, Yanning
   Meng, Xiangxu
   Wang, Lu
   Yan, Ling-Qi
TI Recent advances in glinty appearance rendering
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review
DE glinty appearance; Monte Carlo methods; rendering
ID TEXTURE; REFLECTANCE; IMAGE; MODEL
AB The interaction between light and materials is key to physically-based realistic rendering. However, it is also complex to analyze, especially when the materials contain a large number of details and thus exhibit "glinty" visual effects. Recent methods of producing glinty appearance are expected to be important in next-generation computer graphics. We provide here a comprehensive survey on recent glinty appearance rendering. We start with a definition of glinty appearance based on microfacet theory, and then summarize research works in terms of representation and practical rendering. We have implemented typical methods using our unified platform and compare them in terms of visual effects, rendering speed, and memory consumption. Finally, we briefly discuss limitations and future research directions. We hope our analysis, implementations, and comparisons will provide insight for readers hoping to choose suitable methods for applications, or carry out research.
C1 [Zhu, Junqiu; Zhao, Sizhe; Xu, Yanning; Meng, Xiangxu; Wang, Lu] Shandong Univ, Jinan, Peoples R China.
   [Yan, Ling-Qi] Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA.
C3 Shandong University; University of California System; University of
   California Santa Barbara
RP Xu, YN (corresponding author), Shandong Univ, Jinan, Peoples R China.
EM zhujunqiu@mail.sdu.edu.cn; zhaosizhe@mail.sdu.edu.ccn; xyn@sdu.edu.cn;
   mxx@sdu.edu.cn; luwang_hcivr@sdu.edu.cn; lingqi@cs.ucsb.edu
FU National Key R&D Program of China [2020YFB1708900]; National Natural
   Science Foundation of China [61872223]; Shandong Provincial Natural
   Science Foundation of China [ZR2020LZH016]
FX L This work was partially supported by the National Key R&D Program of
   China under Grant No. 2020YFB1708900, the National Natural Science
   Foundation of China under Grant No. 61872223, and the Shandong
   Provincial Natural Science Foundation of China under Grant No.
   ZR2020LZH016.
CR Atanasov A, 2021, COMPUT GRAPH FORUM, V40, P103, DOI 10.1111/cgf.142618
   Bruneton E, 2012, IEEE T VIS COMPUT GR, V18, P242, DOI 10.1109/TVCG.2011.81
   Chandraker M, 2014, LECT NOTES COMPUT SC, V8695, P202, DOI 10.1007/978-3-319-10584-0_14
   Chermain X, 2020, COMPUT GRAPH FORUM, V39, P243, DOI 10.1111/cgf.14141
   Chermain X, 2021, P ACM COMPUT GRAPH, V4, DOI 10.1145/3451257
   Chermain X, 2019, COMPUT GRAPH FORUM, V38, P27, DOI 10.1111/cgf.13767
   Cignoni P, 1998, VISUALIZATION '98, PROCEEDINGS, P59, DOI 10.1109/VISUAL.1998.745285
   Cohen MF, 2003, ACM T GRAPHIC, V22, P287, DOI 10.1145/882262.882265
   Cook R. L., 1982, ACM T GRAPHIC, V1, P7, DOI DOI 10.1145/357290.357293
   Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778
   Deng H, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3507915
   Durikovic R., 2003, P 19 SPRING C COMP G, P193
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Ershov S, 2001, COMPUT GRAPH FORUM, V20, pC227, DOI 10.1111/1467-8659.00515
   Fournier A., 1992, GRAPHICS INTERFACE W, P45
   Galerne B, 2017, COMPUT GRAPH FORUM, V36, P205, DOI 10.1111/cgf.13073
   Gamboa LE, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275058
   Gao D, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323042
   Gunther J., 2005, VISION, MODELING, AND VISUALIZATION 2005 (VMV'05), P487
   Guo J, 2018, COMPUT GRAPH FORUM, V37, P67, DOI 10.1111/cgf.13476
   Guo Y, 2020, COMPUT GRAPH FORUM, V39, P255, DOI 10.1111/cgf.14142
   Han C, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239479
   HARVEY JE, 1979, AM J PHYS, V47, P974, DOI 10.1119/1.11600
   Heitz E, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3233304
   Heitz E, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766988
   Hui Z, 2017, IEEE I CONF COMP VIS, P5372, DOI 10.1109/ICCV.2017.573
   Hui Z, 2015, IEEE INT CONF COMPUT
   Jakob W, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601186
   Jakob Wenzel, 2010, Mitsuba renderer
   Jensen H. W., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P21
   Kajiya J. T., 1986, Computer Graphics, V20, P143, DOI 10.1145/15886.15902
   Krywonos Andrey, 2006, THESIS U CENTRAL FLO
   Kurt, 2018, DEU MUHENDISLIK FAKU, V20, P87, DOI DOI 10.21205/DEUFMD.2018205808
   Kuznetsov A, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356525
   Lagae A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531360
   Li X, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073641
   Li ZQ, 2018, LECT NOTES COMPUT SC, V11207, P74, DOI 10.1007/978-3-030-01219-9_5
   Ma W.-C., 2005, I3D '05: Proceedings of the 2005 sym109 posium on Interactive 3D graphics and games, P187
   Mityashev BI., 1964, USSR COMP MATH MATH, V4, P247, DOI 10.1016/0041-5553(64)90099-0
   Nam G, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980220
   Neyret F, 1998, IEEE T VIS COMPUT GR, V4, P55, DOI 10.1109/2945.675652
   Ogilvy J. A., 1991, Theory of wave scattering from random rough surfaces, DOI 10.1121/1.401410
   OLANO M, 2010, LEAN MAPPING, P181, DOI [10.1145/1730804.1730834, DOI 10.1145/1730804.1730834]
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839
   Raymond B, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925945
   Riviere J, 2016, COMPUT GRAPH FORUM, V35, P191, DOI 10.1111/cgf.12719
   Suykens F, 2003, COMPUT GRAPH FORUM, V22, P463, DOI 10.1111/1467-8659.00694
   Tan P., 2005, EUROGRAPHICS S RENDE, P111
   Tan P, 2008, IEEE T VIS COMPUT GR, V14, P412, DOI 10.1109/TVCG.2007.70439
   Tessendorf Jerry., 2001, ACM SIGGRAPH Course Notes 2001
   Toksvig M., 2005, Journal of Graphics Tools, V10, P65
   TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105
   Turquin Emmanuel, 2019, Technical Report
   Veach E., 1998, ROBUST MONTE CARLO M
   Velinov Z, 2018, COMPUT GRAPH FORUM, V37, P123, DOI 10.1111/cgf.13347
   Wang BB, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3406836
   Wang BB, 2020, COMPUT GRAPH FORUM, V39, P144, DOI 10.1111/cgf.14007
   WANG H, 1961, AT&T TECH J, V40, P1
   Werner S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130840
   Wu HZ, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024179
   Wu HZ, 2009, COMPUT GRAPH FORUM, V28, P1227, DOI 10.1111/j.1467-8659.2009.01500.x
   Yan LQ, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201351
   Yan LQ, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925915
   Yan LQ, 2015, ACM T GRAPHIC, V35, DOI 10.1145/2816814
   Zhu JQ, 2019, COMPUT GRAPH FORUM, V38, P745, DOI 10.1111/cgf.13876
   Zirr T, 2016, PROCEEDINGS I3D 2016: 20TH ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, P139, DOI 10.1145/2856400.2856409
NR 68
TC 2
Z9 2
U1 0
U2 20
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2022
VL 8
IS 4
BP 535
EP 552
DI 10.1007/s41095-022-0280-x
EA JUN 2022
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2X6VI
UT WOS:000811985700001
OA gold
DA 2024-07-18
ER

PT J
AU Ma, L
   Yao, SD
   Zheng, JM
   Liu, Y
   Zhou, YF
   Xin, SQ
   He, Y
AF Ma, Long
   Yao, Sidan
   Zheng, Jianmin
   Liu, Yang
   Zhou, Yuanfeng
   Xin, Shi-Qing
   He, Ying
TI Constructing self-supporting surfaces with planar quadrilateral elements
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE self-supporting surfaces; planar quadrilateral (PQ) meshes; conjugate
   direction fields (CDFs)
ID MESHES
AB We present a simple yet effective method for constructing 3D self-supporting surfaces with planar quadrilateral (PQ) elements. Starting with a triangular discretization of a self-supporting surface, we first compute the principal curvatures and directions of each triangular face using a new discrete differential geometry approach, yielding more accurate results than existing methods. Then, we smooth the principal direction field to reduce the number of singularities. Next, we partition all faces into two groups in terms of principal curvature difference. For each face with small curvature difference, we compute a stretch matrix that turns the principal directions into a pair of conjugate directions. For the remaining triangular faces, we simply keep their smoothed principal directions. Finally, applying a mixed-integer programming solver to the mixed principal and conjugate direction field, we obtain a planar quadrilateral mesh. Experimental results show that our method is computationally efficient and can yield high-quality PQ meshes that well approximate the geometry of the input surfaces and maintain their self-supporting properties.
C1 [Ma, Long; Zhou, Yuanfeng] Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
   [Ma, Long; Yao, Sidan; Zheng, Jianmin; He, Ying] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Liu, Yang] Microsoft Res Asia, Internet Graph Grp, Beijing 100080, Peoples R China.
   [Xin, Shi-Qing] Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Peoples R China.
C3 Shandong University; Nanyang Technological University; Microsoft
   Research Asia; Microsoft; Shandong University
RP He, Y (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
EM malong@sdu.edu.cn; syao003@e.ntu.edu.sg; ASJMZheng@ntu.edu.sg;
   yangliu@microsoft.com; yfzhou@sdu.edu.cn; xinshiqing@sdu.edu.cn;
   yhe@ntu.edu.sg
RI liu, xinyi/KFB-4466-2024; Zheng, Jianmin/A-3717-2011; Liu,
   Yang/ABD-2239-2020
OI Zheng, Jianmin/0000-0002-5062-6226; Liu, Yang/0000-0002-3768-6654; Yao,
   Sidan/0000-0002-4911-9972
FU National Natural Science Foundation of China [62172257, 61772312,
   61772016, 61802228]; Singapore Ministry of Education [T2EP20220-0014];
   RIE2020 Industry Alignment Fund-Industry Collaboration Projects
   (IAF-ICP) Funding Initiative
FX We would like to thank the anonymous reviewers for their constructive
   comments. This work was partially supported by National Natural Science
   Foundation of China (62172257, 61772312, 61772016, 61802228), Singapore
   Ministry of Education (T2EP20220-0014), and the RIE2020 Industry
   Alignment Fund-Industry Collaboration Projects (IAF-ICP) Funding
   Initiative, as well as cash and in-kind contribution from the industrial
   partner, Rolls-Royce.
CR Block P, 2007, J Int Assoc Shell Spat Struct, V48, P167
   Block P., 2011, P IABSE IASS S
   Bobenko AI., 2008, Discrete Differential Geometry: Integrable Structure, DOI [DOI 10.1007/978-3-7643-8621-4, 10.1007/978-3-7643-8621-4]
   Bommes D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531383
   de Goes F, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461932
   Eigensatz M., 2010, P ACM SIGGRAPH 2010
   Glymph J, 2004, AUTOMAT CONSTR, V13, P187, DOI 10.1016/j.autcon.2003.09.008
   Green A. E., 1954, Theoretical Elasticity
   LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116
   Liu Y, 2006, ACM T GRAPHIC, V25, P681, DOI 10.1145/1141911.1141941
   Liu Y, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461927
   Liu Y, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024174
   Lu CL, 2002, J VIS COMMUN IMAGE R, V13, P65, DOI 10.1006/jvci.2001.0476
   Ma L, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3188735
   Meyer N, 2003, VISUALIZATION AND MATHEMATICS III, P35
   Miki M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766888
   Pottmann H, 2015, COMPUT GRAPH-UK, V47, P145, DOI 10.1016/j.cag.2014.11.002
   Rusinkiewicz S., 2004, P 2 INT S 3D DATA PR
   Sauer R., 1970, DIFFERENZENGEOMETRIE, DOI [10.1007/978-3-642-86411-7, DOI 10.1007/978-3-642-86411-7]
   Truesdell, 1952, FINITE DEFORMATION E, V58, P577, DOI [10.1090/S0002-9904-1952-09627-0, DOI 10.1090/S0002-9904-1952-09627-0]
   van der Heijden A. W. T., 2008, Koiter's elastic stability of solids and structures
   Vouga E, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185583
   Zadravec M, 2010, COMPUT GRAPH FORUM, V29, P1671, DOI 10.1111/j.1467-8659.2010.01776.x
NR 23
TC 2
Z9 2
U1 1
U2 4
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2022
VL 8
IS 4
BP 571
EP 583
DI 10.1007/s41095-021-0257-1
EA MAY 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2X6VI
UT WOS:000793625400001
OA Green Published, gold
DA 2024-07-18
ER

PT J
AU Li, XP
   Zhu, LF
   Xue, QF
   Wang, D
   Zhang, YJ
AF Li, Xuanpeng
   Zhu, Lifeng
   Xue, Qifan
   Wang, Dong
   Zhang, Yongjie Jessica
TI Fluid-inspired field representation for risk assessment in road scenes
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE fluid-inspired risk field; multi-object tracking; road scenes
ID TRACKING
AB Prediction of the likely evolution of traffic scenes is a challenging task because of high uncertainties from sensing technology and the dynamic environment. It leads to failure of motion planning for intelligent agents like autonomous vehicles. In this paper, we propose a fluid-inspired model to estimate collision risk in road scenes. Multi-object states are detected and tracked, and then a stable fluid model is adopted to construct the risk field. Objects' state spaces are used as the boundary conditions in the simulation of advection and diffusion processes. We have evaluated our approach on the public KITTI dataset; our model can provide predictions in the cases of misdetection and tracking error caused by occlusion. It proves a promising approach for collision risk assessment in road scenes.
C1 [Li, Xuanpeng; Zhu, Lifeng; Xue, Qifan; Wang, Dong] Southeast Univ, Sch Instrument Sci & Engn, Nanjing 210096, Peoples R China.
   [Zhang, Yongjie Jessica] Carnegie Mellon Univ, Dept Mech Engn, Pittsburgh, PA 15213 USA.
C3 Southeast University - China; Carnegie Mellon University
RP Zhu, LF (corresponding author), Southeast Univ, Sch Instrument Sci & Engn, Nanjing 210096, Peoples R China.
EM li_xuanpeng@seu.edu.cn; lfzhulf@gmail.com; xue_qifan@seu.edu.cn;
   kingeast16@seu.edu.cn; jessicaz@andrew.cmu.edu
RI Zhang, Yongjie Jessica/F-8733-2012; LI, Xuanpeng/O-6873-2018; Xue,
   Qifan/HHS-1587-2022; zhu, lifeng/IST-2069-2023
OI Zhang, Yongjie Jessica/0000-0001-7436-9757; LI,
   Xuanpeng/0000-0001-9320-0658; 
FU National Natural Science Foundation of China [61906038]; Fundamental
   Research Funds for the Central Universities [2242019K40039]; Zhishan
   Youth Scholar Program of Southeast University
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant No. 61906038, the Fundamental Research
   Funds for the Central Universities under Grant No. 2242019K40039, and
   the Zhishan Youth Scholar Program of Southeast University.
CR Bae SH, 2014, PROC CVPR IEEE, P1218, DOI 10.1109/CVPR.2014.159
   Beltrán J, 2018, IEEE INT C INTELL TR, P3517, DOI 10.1109/ITSC.2018.8569311
   Butt AA, 2013, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2013.241
   Chen CT, 2018, INT GEOSCI REMOTE SE, P7617, DOI 10.1109/IGARSS.2018.8517422
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Chorin A J, 1990, A Mathematical Introduction to Fluid Mechanics, DOI DOI 10.1007/978-1-4684-0364-0
   Coué C, 2006, INT J ROBOT RES, V25, P19, DOI 10.1177/0278364906061158
   Deo N, 2018, IEEE INT VEH SYM, P1179, DOI 10.1109/IVS.2018.8500493
   Dueholm JV, 2016, IEEE T INTELL VEHICL, V1, P203, DOI 10.1109/TIV.2016.2622921
   Engelcke Martin, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P1355, DOI 10.1109/ICRA.2017.7989161
   Hamrick J, 2011, J QUANT ANAL SPORTS, V7, DOI 10.2202/1559-0410.1278
   Kim K, 2017, IEEE INTEL TRANSP SY, V9, P57, DOI 10.1109/MITS.2016.2580714
   Ku J, 2018, IEEE INT C INT ROBOT, P5750, DOI 10.1109/IROS.2018.8594049
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Kuo CH, 2010, PROC CVPR IEEE, P685, DOI 10.1109/CVPR.2010.5540148
   Laugier C, 2011, IEEE INTEL TRANSP SY, V3, P4, DOI 10.1109/MITS.2011.942779
   Leal-Taixé L, 2016, IEEE COMPUT SOC CONF, P418, DOI 10.1109/CVPRW.2016.59
   Lee K, 2019, IEEE ACCESS, V7, P52846, DOI 10.1109/ACCESS.2019.2912067
   Lee M, 2018, ROBOT AUTON SYST, V106, P179, DOI 10.1016/j.robot.2018.05.005
   Lefevre S., 2014, ROBOMECH J, V1, P1, DOI 10.1186/s40648-014-0001-z
   Li B, 2017, IEEE INT C INT ROBOT, P1513, DOI 10.1109/IROS.2017.8205955
   Li JC, 2019, IEEE INT CONF ROBOT, P6658, DOI [10.1109/icra.2019.8793661, 10.1109/ICRA.2019.8793661]
   Li JC, 2018, IEEE INT C INTELL TR, P3218, DOI 10.1109/ITSC.2018.8569780
   Park SH, 2016, IEEE IMAGE PROC, P3484, DOI 10.1109/ICIP.2016.7533007
   Reichardt D., 1994, Proceedings of the Intelligent Vehicles '94 Symposium (Cat. No.94TH8011), P74, DOI 10.1109/IVS.1994.639475
   Schulz J, 2018, IEEE INT C INT ROBOT, P3999, DOI 10.1109/IROS.2018.8594095
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Simon M, 2019, LECT NOTES COMPUT SC, V11129, P197, DOI 10.1007/978-3-030-11009-3_11
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Tang SY, 2016, LECT NOTES COMPUT SC, V9914, P100, DOI 10.1007/978-3-319-48881-3_8
   Nguyen TN, 2012, IEEE T INTELL TRANSP, V13, P154, DOI 10.1109/TITS.2011.2165705
   Villegas R., 2017, International Conference on Machine Learning, P3560
   Wang JQ, 2016, TRANSPORT RES C-EMER, V72, P306, DOI 10.1016/j.trc.2016.10.003
   Wang JQ, 2015, IEEE T INTELL TRANSP, V16, P2203, DOI 10.1109/TITS.2015.2401837
   Wolf MT, 2008, IEEE INT CONF ROBOT, P3731, DOI 10.1109/ROBOT.2008.4543783
   Xiang Y, 2015, IEEE I CONF COMP VIS, P4705, DOI 10.1109/ICCV.2015.534
   Xie GT, 2018, IEEE T IND ELECTRON, V65, P5999, DOI 10.1109/TIE.2017.2782236
   Yang ZS, 2013, ADV MECH ENG, DOI 10.1155/2013/207104
   Yoon JH, 2016, PROC CVPR IEEE, P1392, DOI 10.1109/CVPR.2016.155
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
NR 40
TC 1
Z9 1
U1 2
U2 14
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2020
VL 6
IS 4
BP 401
EP 415
DI 10.1007/s41095-020-0190-8
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FN
UT WOS:000648692500003
OA gold
DA 2024-07-18
ER

PT J
AU Patashnik, O
   Lu, M
   Bermano, AH
   Cohen-Or, D
AF Patashnik, Or
   Lu, Min
   Bermano, Amit H.
   Cohen-Or, Daniel
TI Temporal scatterplots
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE scatterplot; temporal data; visual clutter; principle component analysis
   (PCA)
ID MATRIX
AB Visualizing high-dimensional data on a 2D canvas is generally challenging. It becomes significantly more difficult when multiple time-steps are to be presented, as the visual clutter quickly increases. Moreover, the challenge to perceive the significant temporal evolution is even greater. In this paper, we present a method to plot temporal high-dimensional data in a static scatterplot; it uses the established PCA technique to project data from multiple time-steps. The key idea is to extend each individual displacement prior to applying PCA, so as to skew the projection process, and to set a projection plane that balances the directions of temporal change and spatial variance. We present numerous examples and various visual cues to highlight the data trajectories, and demonstrate the effectiveness of the method for visualizing temporal data.
C1 [Patashnik, Or; Bermano, Amit H.; Cohen-Or, Daniel] Tel Aviv Univ, Tel Aviv, Israel.
   [Lu, Min] Shenzhen Univ, Shenzhen, Peoples R China.
C3 Tel Aviv University; Shenzhen University
RP Lu, M (corresponding author), Shenzhen Univ, Shenzhen, Peoples R China.
EM orpatashnik@gmail.com; lumin.vis@gmail.com; amit.bermano@gmail.com;
   cohenor@gmail.com
RI Bermano, Amit/GSD-9278-2022; Bermano, Amit/IZQ-5472-2023
OI Bermano, Amit/0000-0002-3592-1112
FU Israel Science Foundation [2366/16, 2472/17]
FX This work was supported in part by the Israel Science Foundation (Grant
   No. 2366/16 and 2472/17).
CR Alvarez GA, 2007, J VISION, V7, DOI 10.1167/7.13.14
   [Anonymous], INT ENCY STAT SCI
   Archambault D, 2011, IEEE T VIS COMPUT GR, V17, P539, DOI 10.1109/TVCG.2010.78
   Bach B, 2014, IEEE T VIS COMPUT GR, V20, P740, DOI 10.1109/TVCG.2013.254
   Beck F, 2017, COMPUT GRAPH FORUM, V36, P133, DOI 10.1111/cgf.12791
   Chan YH, 2013, IEEE T VIS COMPUT GR, V19, P1768, DOI 10.1109/TVCG.2013.20
   CHERNOFF H, 1973, J AM STAT ASSOC, V68, P361, DOI 10.2307/2284077
   Crnovrsanin Tarik, 2009, Proceedings of the 2009 IEEE Symposium on Visual Analytics Science and Technology. VAST 2009. Held co-jointly with VisWeek 2009, P11, DOI 10.1109/VAST.2009.5332593
   De Leeuw J., 2000, MULTIDIMENSIONAL SCA
   Elmqvist N, 2008, IEEE T VIS COMPUT GR, V14, P1141, DOI 10.1109/TVCG.2008.153
   Fisher D., 2010, BEAUTIFUL VISUALIZAT, P329
   Fujiwara T, 2020, IEEE T VIS COMPUT GR, V26, P418, DOI 10.1109/TVCG.2019.2934433
   Hong D., 2018, ARXIV181012862
   Im JF, 2013, IEEE T VIS COMPUT GR, V19, P2606, DOI 10.1109/TVCG.2013.160
   Inselberg A, 1985, VISUAL COMPUT, V1, P69, DOI 10.1007/BF01898350
   Jäckle D, 2016, IEEE T VIS COMPUT GR, V22, P141, DOI 10.1109/TVCG.2015.2467553
   Kandogan E., 2000, P IEEE INF VIS S, V650, P22, DOI DOI 10.1145/502512.502530
   KEIM DA, 1994, IEEE COMPUT GRAPH, V14, P40, DOI 10.1109/38.310723
   Keim DA, 2002, IEEE T VIS COMPUT GR, V8, P1, DOI 10.1109/2945.981847
   Krstajic Milos, 2013, 2013 IEEE International Conference on Big Data, P41, DOI 10.1109/BigData.2013.6691713
   Liu SX, 2016, IEEE T VIS COMPUT GR, V22, P2451, DOI 10.1109/TVCG.2015.2509990
   Liu SS, 2017, IEEE T VIS COMPUT GR, V23, P1249, DOI 10.1109/TVCG.2016.2640960
   Lu M, 2020, IEEE T VIS COMPUT GR, V26, P770, DOI 10.1109/TVCG.2019.2934811
   Mayorga A, 2013, IEEE T VIS COMPUT GR, V19, P1526, DOI 10.1109/TVCG.2013.65
   McInnes L, 2020, Arxiv, DOI [arXiv:1802.03426, DOI 10.48550/ARXIV.1802.03426, 10.21105/joss.00861]
   Nonato LG, 2019, IEEE T VIS COMPUT GR, V25, P2650, DOI 10.1109/TVCG.2018.2846735
   Rauber P., 2016, P EUR IEEE VGTC C VI, P73
   Robertson G, 2008, IEEE T VIS COMPUT GR, V14, P1325, DOI 10.1109/TVCG.2008.125
   Dang TN, 2014, IEEE PAC VIS SYMP, P73, DOI 10.1109/PacificVis.2014.42
   Tufte ER, 1990, Envisioning Information
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang Y, 2018, IEEE T VIS COMPUT GR, V24, P2487, DOI 10.1109/TVCG.2017.2750689
   Wang YH, 2019, IEEE T VIS COMPUT GR, V25, P820, DOI 10.1109/TVCG.2018.2864912
   Wilkinson L, 2006, IEEE T VIS COMPUT GR, V12, P1363, DOI 10.1109/TVCG.2006.94
   Wulms J., 2019, ARXIV191200719
NR 35
TC 2
Z9 2
U1 1
U2 4
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2020
VL 6
IS 4
BP 385
EP 400
DI 10.1007/s41095-020-0197-1
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FN
UT WOS:000648692500002
PM 33194253
OA Green Published, gold
DA 2024-07-18
ER

PT J
AU Yang, XL
   Jia, XH
AF Yang, Xiaolong
   Jia, Xiaohong
TI Simple primitive recognition via hierarchical face clustering
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE quadric primitive extraction; mesh; hierarchical clustering
ID 3D SHAPE SEGMENTATION; MESH SEGMENTATION; SURFACE EXTRACTION;
   DECOMPOSITION
AB We present a simple yet efficient algorithm for recognizing simple quadric primitives (plane, sphere, cylinder, cone) from triangular meshes. Our approach is an improved version of a previous hierarchical clustering algorithm, which performs pairwise clustering of triangle patches from bottom to top. The key contributions of our approach include a strategy for priority and fidelity consideration of the detected primitives, and a scheme for boundary smoothness between adjacent clusters. Experimental results demonstrate that the proposed method produces qualitatively and quantitatively better results than representative state-of-the-art methods on a wide range of test data.
C1 [Yang, Xiaolong; Jia, Xiaohong] Chinese Acad Sci, Acad Math & Syst Sci, Beijing 100190, Peoples R China.
   [Yang, Xiaolong; Jia, Xiaohong] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Academy of Mathematics & System Sciences,
   CAS; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS
RP Jia, XH (corresponding author), Chinese Acad Sci, Acad Math & Syst Sci, Beijing 100190, Peoples R China.; Jia, XH (corresponding author), Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
EM yangxiaolong17@mails.ucas.ac.cn; xhjia@amss.ac.cn
RI Jia, Xiaohong/A-1012-2017
FU National Natural Science of Foundation for Outstanding Young Scholars
   [12022117]; National Natural Science Foundation of China [61872354];
   Beijing Natural Science Foundation [Z190004]; Intelligent Science and
   Technology Advanced subject project of University of Chinese Academy of
   Sciences [115200S001]
FX This work was supported by the National Natural Science of Foundation
   for Outstanding Young Scholars (12022117), the National Natural Science
   Foundation of China (61872354), the Beijing Natural Science Foundation
   (Z190004), and the Intelligent Science and Technology Advanced subject
   project of University of Chinese Academy of Sciences (115200S001).
CR Attene M, 2006, VISUAL COMPUT, V22, P181, DOI 10.1007/s00371-006-0375-x
   Au OKC, 2012, IEEE T VIS COMPUT GR, V18, P1125, DOI 10.1109/TVCG.2011.131
   Cao YH, 2015, COMPUT GRAPH-UK, V46, P275, DOI 10.1016/j.cag.2014.09.022
   Chen XB, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531379
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Nguyen DT, 2018, IEEE T VIS COMPUT GR, V24, P3005, DOI 10.1109/TVCG.2017.2772238
   Fitzgibbon AW, 1997, COMPUT AIDED DESIGN, V29, P321, DOI 10.1016/S0010-4485(96)00059-0
   Garland M., 2001, I3D 01, P49, DOI [DOI 10.1145/364338.364345, 10.1145/364338.364345]
   Ji ZP, 2006, COMPUT GRAPH FORUM, V25, P283, DOI 10.1111/j.1467-8659.2006.00947.x
   Julius D, 2005, COMPUT GRAPH FORUM, V24, P581, DOI 10.1111/j.1467-8659.2005.00883.x
   Kaiser A, 2019, COMPUT GRAPH FORUM, V38, P167, DOI 10.1111/cgf.13451
   Kalogerakis E, 2017, PROC CVPR IEEE, P6630, DOI 10.1109/CVPR.2017.702
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   Lafarge F, 2010, IEEE T IMAGE PROCESS, V19, P1683, DOI 10.1109/TIP.2010.2045695
   Lai YK, 2009, COMPUT AIDED GEOM D, V26, P665, DOI 10.1016/j.cagd.2008.09.007
   Lavoué G, 2005, COMPUT AIDED DESIGN, V37, P975, DOI 10.1016/j.cad.2004.09.001
   Le T, 2017, COMPUT GRAPH-UK, V66, P103, DOI 10.1016/j.cag.2017.05.011
   Lee Y, 2005, COMPUT AIDED GEOM D, V22, P444, DOI 10.1016/j.cagd.2005.04.002
   Li H, 2016, COMPUT GRAPH FORUM, V35, P79, DOI 10.1111/cgf.12965
   Lien JM, 2008, COMPUT AIDED GEOM D, V25, P503, DOI 10.1016/j.cagd.2008.05.003
   Liu R, 2007, COMPUT GRAPH FORUM, V26, P385, DOI 10.1111/j.1467-8659.2007.01061.x
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Lu L, 2007, COMPUT GRAPH FORUM, V26, P329, DOI 10.1111/j.1467-8659.2007.01055.x
   Mitra NJ, 2006, ACM T GRAPHIC, V25, P560, DOI 10.1145/1141911.1141924
   Ohtake Y, 2004, ACM T GRAPHIC, V23, P609, DOI 10.1145/1015706.1015768
   Petitjean S, 2002, ACM COMPUT SURV, V34, P211, DOI 10.1145/508352.508354
   Rodrigues RSV, 2018, COMPUT GRAPH FORUM, V37, P235, DOI 10.1111/cgf.13323
   Schnabel R, 2007, COMPUT GRAPH FORUM, V26, P214, DOI 10.1111/j.1467-8659.2007.01016.x
   Shamir A, 2008, COMPUT GRAPH FORUM, V27, P1539, DOI 10.1111/j.1467-8659.2007.01103.x
   Shu ZY, 2016, COMPUT AIDED GEOM D, V43, P39, DOI 10.1016/j.cagd.2016.02.015
   Simari P. D., 2005, Graphics Interface, P161
   TAUBIN G, 1991, IEEE T PATTERN ANAL, V13, P1115, DOI 10.1109/34.103273
   Theologou P, 2017, IEEE T PATTERN ANAL, V39, P397, DOI 10.1109/TPAMI.2016.2544311
   Varady T, 1997, COMPUT AIDED DESIGN, V29, P255, DOI 10.1016/S0010-4485(96)00054-1
   Vieira M, 2005, COMPUT AIDED GEOM D, V22, P771, DOI 10.1016/j.cagd.2005.03.006
   Wu JH, 2005, COMPUT GRAPH FORUM, V24, P277, DOI 10.1111/j.1467-8659.2005.00852.x
   Xu H, 2017, IEEE INT C COMPUT VI, P2698
   Yan DM, 2006, LECT NOTES COMPUT SC, V4077, P73
   Yan DM, 2012, COMPUT AIDED DESIGN, V44, P1072, DOI 10.1016/j.cad.2012.04.005
   Zheng YY, 2012, IEEE T VIS COMPUT GR, V18, P1304, DOI 10.1109/TVCG.2011.140
   Zhuang Yixin, 2017, [Computational Visual Media, 计算可视媒体], V3, P147
NR 42
TC 3
Z9 3
U1 1
U2 8
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2020
VL 6
IS 4
BP 431
EP 443
DI 10.1007/s41095-020-0192-6
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FN
UT WOS:000648692500005
OA gold
DA 2024-07-18
ER

PT J
AU Zhang, C
   Lu, XQ
   Hotta, K
   Yang, X
AF Zhang, Chao
   Lu, Xuequan
   Hotta, Katsuya
   Yang, Xi
TI G2MF-WA: Geometric multi-model fitting with weakly annotated data
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE geometric multi-model fitting; weak annotation; multi-homography
   detection; two-view motion segmentation
AB In this paper we address the problem of geometric multi-model fitting using a few weakly annotated data points, which has been little studied so far. In weak annotating (WA), most manual annotations are supposed to be correct yet inevitably mixed with incorrect ones. SuchWA data can naturally arise through interaction in various tasks. For example, in the case of homography estimation, one can easily annotate points on the same plane or object with a single label by observing the image. Motivated by this, we propose a novel method to make full use of WA data to boost multi-model fitting performance. Specifically, a graph for model proposal sampling is first constructed using the WA data, given the prior that WA data annotated with the same weak label has a high probability of belonging to the same model. By incorporating this prior knowledge into the calculation of edge probabilities, vertices (i.e., data points) lying on or near the latent model are likely to be associated and further form a subset or cluster for effective proposal generation. Having generated proposals, a-expansion is used for labeling, and our method in return updates the proposals. This procedure works in an iterative way. Extensive experiments validate our method and show that it produces noticeably better results than state-of-the-art techniques in most cases.
C1 [Zhang, Chao] Univ Fukui, Fac Engn, Fukui 9108507, Japan.
   [Hotta, Katsuya] Univ Fukui, Fukui 9108507, Japan.
   [Lu, Xuequan] Deakin Univ, Waurn Ponds 3216, Australia.
   [Yang, Xi] Univ Tokyo, Grad Sch Informat Sci & Technol, Tokyo 1138656, Japan.
C3 University of Fukui; University of Fukui; Deakin University; University
   of Tokyo
RP Zhang, C (corresponding author), Univ Fukui, Fac Engn, Fukui 9108507, Japan.
EM zhang@u-fukui.ac.jp; xuequan.lu@deakin.edu.au;
   k-hotta@monju.fuis.u-fukui.ac.jp; earthyangxi@gmail.com
RI Zhang, Cheng/JAD-2236-2023; zhang, luyu/JJC-4227-2023; zhang,
   chao/HTO-2468-2023; zhang, chao/IXD-9965-2023; Wang,
   Yifan/KDO-8319-2024; Hotta, Katsuya/JOZ-5395-2023
OI Hotta, Katsuya/0000-0002-8148-1445; Lu, Xuequan/0000-0003-0959-408X;
   Zhang, Chao/0000-0002-0845-9217
FU JSPS KAKENHI [JP18K17823]; Deakin [CY01-251301-F003-PJ03906-PG00447];
   Grants-in-Aid for Scientific Research [20K19568] Funding Source: KAKEN
FX Chao Zhang is supported in part by JSPS KAKENHI Grant JP18K17823.
   Xuequan Lu is supported in part by Deakin
   CY01-251301-F003-PJ03906-PG00447.
CR Amayo P, 2018, PROC CVPR IEEE, P8138, DOI 10.1109/CVPR.2018.00849
   [Anonymous], 2015, Photogrammetric Engineering Remote Sensing, DOI [DOI 10.1080/10671188.1967.10616517, 10.14358/PERS.81.2.103, DOI 10.14358/PERS.81.2.103]
   [Anonymous], 2011, P 24 INT C NEUR INF
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Brachmann E, 2017, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2017.267
   Chin TJ, 2010, LECT NOTES COMPUT SC, V6315, P533, DOI 10.1007/978-3-642-15555-0_39
   Chum O, 2005, PROC CVPR IEEE, P220, DOI 10.1109/cvpr.2005.221
   Chum O, 2003, LECT NOTES COMPUT SC, V2781, P236
   Delong A, 2012, INT J COMPUT VISION, V96, P1, DOI 10.1007/s11263-011-0437-z
   Figueiredo MAT, 2002, IEEE T PATTERN ANAL, V24, P381, DOI 10.1109/34.990138
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Hartley R, 2004, AMULTIPLE VIEW GEOME, DOI [10.1017/CBO9780511811685, DOI 10.1017/CBO9780511811685]
   Isack H, 2012, INT J COMPUT VISION, V97, P123, DOI 10.1007/s11263-011-0474-7
   Lazic N, 2009, IEEE I CONF COMP VIS, P825, DOI 10.1109/ICCV.2009.5459302
   Magri L, 2016, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2016.361
   Magri L, 2014, PROC CVPR IEEE, P3954, DOI 10.1109/CVPR.2014.505
   Meer P., 2004, Emerging Topics in Computer Vision, P107
   Nieuwenhuis C, 2013, INT J COMPUT VISION, V104, P223, DOI 10.1007/s11263-013-0619-y
   Nistér D, 2005, MACH VISION APPL, V16, P321, DOI 10.1007/s00138-005-0006-y
   Pham TT, 2014, IEEE T PATTERN ANAL, V36, P1658, DOI 10.1109/TPAMI.2013.2296310
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   SWENDSEN RH, 1987, PHYS REV LETT, V58, P86, DOI 10.1103/PhysRevLett.58.86
   Toldo R, 2008, LECT NOTES COMPUT SC, V5302, P537, DOI 10.1007/978-3-540-88682-2_41
   Tordoff BJ, 2005, IEEE T PATTERN ANAL, V27, P1523, DOI 10.1109/TPAMI.2005.199
   Torr PHS, 1998, PHILOS T R SOC A, V356, P1321, DOI 10.1098/rsta.1998.0224
   Vincent E, 2001, ISPA 2001: PROCEEDINGS OF THE 2ND INTERNATIONAL SYMPOSIUM ON IMAGE AND SIGNAL PROCESSING AND ANALYSIS, P182, DOI 10.1109/ISPA.2001.938625
   Wong HS, 2011, IEEE I CONF COMP VIS, P1044, DOI 10.1109/ICCV.2011.6126350
   Yong J, 2007, PROCEEDINGS OF THE 2007 1ST INTERNATIONAL SYMPOSIUM ON INFORMATION TECHNOLOGIES AND APPLICATIONS IN EDUCATION (ISITAE 2007), P1
   Zuliani M, 2005, IEEE IMAGE PROC, P2969
NR 29
TC 2
Z9 2
U1 1
U2 4
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2020
VL 6
IS 2
BP 135
EP 145
DI 10.1007/s41095-020-0166-8
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FB
UT WOS:000648691300002
OA gold
DA 2024-07-18
ER

PT J
AU Jiang, DQ
   Jin, YW
   Zhang, FL
   Zhu, Z
   Zhang, Y
   Tong, RF
   Tang, M
AF Jiang, Diqiong
   Jin, Yiwei
   Zhang, Fang-Lue
   Zhu, Zhe
   Zhang, Yun
   Tong, Ruofeng
   Tang, Min
TI Sphere Face Model: A 3D morphable model with hypersphere manifold latent
   space using joint 2D/3D training
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE facial modeling; deep learning; face reconstruction; 3D morphable model
   (3DMM)
ID RECONSTRUCTION; REPRESENTATION; IMAGE
AB 3D morphable models (3DMMs) are generative models for face shape and appearance. Recent works impose face recognition constraints on 3DMM shape parameters so that the face shapes of the same person remain consistent. However, the shape parameters of traditional 3DMMs satisfy the multivariate Gaussian distribution. In contrast, the identity embeddings meet the hypersphere distribution, and this conflict makes it challenging for face reconstruction models to preserve the faithfulness and the shape consistency simultaneously. In other words, recognition loss and reconstruction loss can not decrease jointly due to their conflict distribution. To address this issue, we propose the Sphere Face Model (SFM), a novel 3DMM for monocular face reconstruction, preserving both shape fidelity and identity consistency. The core of our SFM is the basis matrix which can be used to reconstruct 3D face shapes, and the basic matrix is learned by adopting a two-stage training approach where 3D and 2D training data are used in the first and second stages, respectively. We design a novel loss to resolve the distribution mismatch, enforcing that the shape parameters have the hyperspherical distribution. Our model accepts 2D and 3D data for constructing the sphere face models. Extensive experiments show that SFM has high representation ability and clustering performance in its shape parameter space. Moreover, it produces high-fidelity face shapes consistently in challenging conditions in monocular face reconstruction. The code will be released at
C1 [Jiang, Diqiong; Jin, Yiwei; Tong, Ruofeng; Tang, Min] Zhejiang Univ, Hangzhou 310058, Peoples R China.
   [Zhang, Fang-Lue] Victoria Univ Wellington, Wellington 6012, New Zealand.
   [Zhu, Zhe] Duke Univ, Durham, NC 27708 USA.
   [Zhang, Yun] Commun Univ Zhejiang, Hangzhou 310019, Peoples R China.
C3 Zhejiang University; Victoria University Wellington; Duke University;
   Communication University of Zhejiang
RP Tong, RF (corresponding author), Zhejiang Univ, Hangzhou 310058, Peoples R China.
EM jdq1994@zju.edu.cn; yw0506@zju.edu.cn; fanglue.zhang@ecs.vuw.ac.nz;
   ajex1988@gmail.com; zhangyunzju@zju.edu.cn; trf@zju.edu.cn;
   tangm@zju.edu.cn
RI Tang, Min/KOC-3090-2024
FU National Natural Science Foundation of China [61972342, 61832016];
   Science and Technology Department of Zhejiang Province [2018C01080];
   Zhejiang Province Public Welfare Technology Application Research
   [LGG22F020009]; Key Laboratory of Film and TV Media Technology of
   Zhejiang Province [2020E10015]; Teaching Reform Project of Communication
   University of Zhejiang [jgxm202131]
FX The research is supported in part by National Natural Science Foundation
   of China (61972342, 61832016), Science and Technology Department of
   Zhejiang Province (2018C01080), Zhejiang Province Public Welfare
   Technology Application Research (LGG22F020009), Key Laboratory of Film
   and TV Media Technology of Zhejiang Province (2020E10015), and Teaching
   Reform Project of Communication University of Zhejiang (jgxm202131).
CR Aldrian O., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P822, DOI 10.1109/ICCVW.2011.6130337
   Amberg B, 2007, IEEE I CONF COMP VIS, P1326
   Amberg B, 2008, IEEE INT CONF AUTOMA, P667
   Bagautdinov T, 2018, PROC CVPR IEEE, P3877, DOI 10.1109/CVPR.2018.00408
   Bagdanov Andrew D, 2011, P JOINT ACM WORKSH H
   Bas A, 2017, LECT NOTES COMPUT SC, V10117, P377, DOI 10.1007/978-3-319-54427-4_28
   BESL PJ, 1992, P SOC PHOTO-OPT INS, V1611, P586, DOI 10.1117/12.57955
   Bian SJ, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12010027
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Blanz V, 2003, COMPUT GRAPH FORUM, V22, P641, DOI 10.1111/1467-8659.t01-1-00712
   Booth J, 2016, PROC CVPR IEEE, P5543, DOI 10.1109/CVPR.2016.598
   Bouaziz S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461976
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chen SY, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P525, DOI 10.1109/VR.2018.8446494
   Chen YJ, 2020, IEEE T IMAGE PROCESS, V29, P8696, DOI 10.1109/TIP.2020.3017347
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   Feng Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459936
   Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33
   Gecer B, 2019, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2019.00125
   Gerig T, 2018, IEEE INT CONF AUTOMA, P75, DOI 10.1109/FG.2018.00021
   Huang YG, 2020, PROC CVPR IEEE, P5900, DOI 10.1109/CVPR42600.2020.00594
   Jiang DQ, 2022, COMPUT GRAPH FORUM, V41, P348, DOI 10.1111/cgf.14513
   Jiang ZH, 2019, PROC CVPR IEEE, P11949, DOI 10.1109/CVPR.2019.01223
   Jianzhu Guo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P152, DOI 10.1007/978-3-030-58529-7_10
   Jin YW, 2020, COMMUN INF SYST, V20, P389
   Johnson J., 2020, P SIGGRAPH ASIA 2020, V1
   Jourabloo A, 2016, PROC CVPR IEEE, P4188, DOI 10.1109/CVPR.2016.454
   Kingma Diederik P, 2014, INT C LEARNING REPRE
   Lattas A, 2020, PROC CVPR IEEE, P757, DOI 10.1109/CVPR42600.2020.00084
   Li HB, 2010, PROCEEDINGS OF CHINA-CANADA WORKSHOP ON FINANCIAL ENGINEERING AND ENTERPRISE RISK MANAGEMENT 2010, P32
   Li TY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130813
   Lin JK, 2021, AAAI CONF ARTIF INTE, V35, P311
   Lin JK, 2020, PROC CVPR IEEE, P5890, DOI 10.1109/CVPR42600.2020.00593
   Liu F, 2018, PROC CVPR IEEE, P5216, DOI 10.1109/CVPR.2018.00547
   Liu H, 2019, PROC CVPR IEEE, P11939, DOI 10.1109/CVPR.2019.01222
   Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
   Liu WY, 2016, PR MACH LEARN RES, V48
   Liu Y, 2017, Arxiv, DOI arXiv:1710.00870
   Tran L, 2018, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2018.00767
   Patel A, 2016, PATTERN RECOGN, V52, P206, DOI 10.1016/j.patcog.2015.10.003
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Paysan P, 2009, LECT NOTES COMPUT SC, V5748, P232, DOI 10.1007/978-3-642-03798-6_24
   Phillips PJ, 2005, PROC CVPR IEEE, P947
   Sagonas C, 2016, IMAGE VISION COMPUT, V47, P3, DOI 10.1016/j.imavis.2016.01.002
   Sanyal S, 2019, PROC CVPR IEEE, P7755, DOI 10.1109/CVPR.2019.00795
   Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6
   Schneider A, 2017, IEEE I CONF COMP VIS, P3885, DOI 10.1109/ICCV.2017.417
   Schönborn S, 2017, INT J COMPUT VISION, V123, P160, DOI 10.1007/s11263-016-0967-5
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shang J., 2020, EUROPEAN C COMPUTER, P53
   Shi TY, 2020, AAAI CONF ARTIF INTE, V34, P1733
   Smith WAP, 2020, PROC CVPR IEEE, P5010, DOI 10.1109/CVPR42600.2020.00506
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Tewari A, 2018, PROC CVPR IEEE, P2549, DOI 10.1109/CVPR.2018.00270
   Thies J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818056
   Trân AT, 2017, PROC CVPR IEEE, P1493, DOI 10.1109/CVPR.2017.163
   Tran L, 2019, PROC CVPR IEEE, P1126, DOI 10.1109/CVPR.2019.00122
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vlasic D, 2005, ACM T GRAPHIC, V24, P426, DOI 10.1145/1073204.1073209
   Wang F, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1041, DOI 10.1145/3123266.3123359
   Wang F, 2018, IEEE SIGNAL PROC LET, V25, P926, DOI 10.1109/LSP.2018.2822810
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu SZ, 2023, IEEE T PATTERN ANAL, V45, P5268, DOI 10.1109/TPAMI.2021.3076536
   Xiao YP, 2020, COMPUT VIS MEDIA, V6, P113, DOI 10.1007/s41095-020-0174-8
   Yang HT, 2020, PROC CVPR IEEE, P598, DOI 10.1109/CVPR42600.2020.00068
   Yang L, 2021, GRAPH MODELS, V115, DOI 10.1016/j.gmod.2021.101102
   Zeng XX, 2022, COMPUT VIS MEDIA, V8, P239, DOI 10.1007/s41095-021-0238-4
   Zhang X, 2019, PROC CVPR IEEE, P10815, DOI 10.1109/CVPR.2019.01108
   Zhu WB, 2020, PROC CVPR IEEE, P4957, DOI 10.1109/CVPR42600.2020.00501
   Zhu XY, 2019, IEEE T PATTERN ANAL, V41, P78, DOI 10.1109/TPAMI.2017.2778152
   Zhu XY, 2015, PROC CVPR IEEE, P787, DOI 10.1109/CVPR.2015.7298679
NR 73
TC 2
Z9 3
U1 4
U2 28
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2023
VL 9
IS 2
BP 279
EP 296
DI 10.1007/s41095-022-0286-4
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7N8FL
UT WOS:000907570700005
OA gold
DA 2024-07-18
ER

PT J
AU Yamauchi, Y
   Yatagawa, T
   Ohtake, Y
   Suzuki, H
AF Yamauchi, Yuta
   Yatagawa, Tatsuya
   Ohtake, Yutaka
   Suzuki, Hiromasa
TI Bin-scanning: Segmentation of X-ray CT volume of binned parts using
   Morse skeleton graph of distance transform
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE X-ray computed tomography (CT); volume segmentation; graph matching;
   nondestructive inspection
ID OBJECT RECOGNITION; ALGORITHM; COMPLEXES; NETWORK; IMAGES
AB X-ray CT scanners, due to the transmissive nature of X-rays, have enabled the non-destructive evaluation of industrial products, even inside their bodies. In light of its effectiveness, this study introduces a new approach to accelerate the inspection of many mechanical parts with the same shape in a bin. The input to this problem is a volumetric image (i.e., CT volume) of many parts obtained by a single CT scan. We need to segment the parts in the volume to inspect each of them; however, random postures and dense contacts of the parts prohibit part segmentation using traditional template matching. To address this problem, we convert both the scanned volumetric images of the template and the binned parts to simpler graph structures and solve a subgraph matching problem to segment the parts. We perform a distance transform to convert the CT volume into a distance field. Then, we construct a graph based on Morse theory, in which graph nodes are located at the extremum points of the distance field. The experimental evaluation demonstrates that our fully automatic approach can detect target parts appropriately, even for a heap of 50 parts. Moreover, the overall computation can be performed in approximately 30 min for a large CT volume of approximately 2000x2000x1000 voxels.
C1 [Yamauchi, Yuta; Yatagawa, Tatsuya; Ohtake, Yutaka; Suzuki, Hiromasa] Univ Tokyo, Sch Engn, 7-3-1 Hongo,Bunkyo Ku, Tokyo, Japan.
C3 University of Tokyo
RP Suzuki, H (corresponding author), Univ Tokyo, Sch Engn, 7-3-1 Hongo,Bunkyo Ku, Tokyo, Japan.
EM yamauchi@den.t.u-tokyo.ac.jp; tatsy@den.t.u-tokyo.ac.jp;
   ohtake@den.t.u-tokyo.ac.jp; suzuki@den.t.u-tokyo.ac.jp
OI Yatagawa, Tatsuya/0000-0003-4653-2435
CR [Anonymous], 2006, P 23 INT C MACH LEAR
   [Anonymous], 2010, P BRIT MACH VIS C
   Barrett JF, 2004, RADIOGRAPHICS, V24, P1679, DOI 10.1148/rg.246045065
   Barrow H. G., 1977, P IMAGE UNDERSTANDIN, P659
   Berg AC, 2005, PROC CVPR IEEE, P26
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Bremer PT, 2004, IEEE T VIS COMPUT GR, V10, P385, DOI 10.1109/TVCG.2004.3
   Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043
   Cicek Ozgun, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P424, DOI 10.1007/978-3-319-46723-8_49
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Comaniciu D, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P297, DOI 10.1109/ICIP.2002.1038964
   Comic L, 2008, LECT NOTES COMPUT SC, V4992, P117, DOI 10.1007/978-3-540-79126-3_12
   Cordella LP, 2004, IEEE T PATTERN ANAL, V26, P1367, DOI 10.1109/TPAMI.2004.75
   Cui ZM, 2019, PROC CVPR IEEE, P6361, DOI 10.1109/CVPR.2019.00653
   De Floriani L, 2015, COMPUT GRAPH FORUM, V34, P761, DOI 10.1111/cgf.12596
   Dong S, 2006, ACM T GRAPHIC, V25, P1057, DOI 10.1145/1141911.1141993
   Dou Q, 2017, MED IMAGE ANAL, V41, P40, DOI 10.1016/j.media.2017.05.001
   Drost B, 2010, PROC CVPR IEEE, P998, DOI 10.1109/CVPR.2010.5540108
   Edelsbrunner H., 2001, PROC 17 ANN ACM SYMP, P70, DOI DOI 10.1145/378583.378626
   Fang XZ, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201354
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Forman R, 1998, ADV MATH, V134, P90, DOI 10.1006/aima.1997.1650
   FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330
   Gelfand N, 2003, FOURTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P260, DOI 10.1109/IM.2003.1240258
   Gruber R, 2021, J NONDESTRUCT EVAL, V40, DOI 10.1007/s10921-020-00734-w
   Günther D, 2014, IEEE T VIS COMPUT GR, V20, P2476, DOI 10.1109/TVCG.2014.2346403
   Heine C, 2016, COMPUT GRAPH FORUM, V35, P643, DOI 10.1111/cgf.12933
   Januszewski M., 2016, Floodfilling networks
   Kang Y, 2003, IEEE T MED IMAGING, V22, P586, DOI 10.1109/TMI.2003.812265
   Kass M., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P259, DOI 10.1007/BF00133570
   Li PC, 2020, IEEE INT C INT ROBOT, P2906, DOI 10.1109/IROS45743.2020.9341426
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu L, 2008, LECT NOTES COMPUT SC, V5241, P296, DOI 10.1007/978-3-540-85988-8_36
   Long J., 2015, P IEEE C COMP VIS PA, P3431, DOI DOI 10.48550/ARXIV.1411.4038
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mian AS, 2006, IEEE T PATTERN ANAL, V28, P1584, DOI 10.1109/TPAMI.2006.213
   Milnor J. W., 1963, Morse Theory, V51
   Nagai Y, 2019, COMPUT AIDED DESIGN, V107, P23, DOI 10.1016/j.cad.2018.09.002
   Ni XL, 2004, ACM T GRAPHIC, V23, P613, DOI 10.1145/1015706.1015769
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Papazov C, 2011, LECT NOTES COMPUT SC, V6492, P135, DOI 10.1007/978-3-642-19315-6_11
   Pardo XM, 2001, IMAGE VISION COMPUT, V19, P461, DOI 10.1016/S0262-8856(00)00092-5
   Paris S, 2007, PROC CVPR IEEE, P1978
   Reddy P, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417830
   Robins V, 2011, IEEE T PATTERN ANAL, V33, P1646, DOI 10.1109/TPAMI.2011.95
   Rolinek Michal, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P407, DOI 10.1007/978-3-030-58604-1_25
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   ROSENFELD A, 1977, IEEE T SYST MAN CYB, V7, P104
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499
   Shivashankar N, 2016, IEEE T VIS COMPUT GR, V22, P1745, DOI 10.1109/TVCG.2015.2452919
   Soille P, 2009, PATTERN RECOGN LETT, V30, P456, DOI 10.1016/j.patrec.2008.10.015
   Taud H, 2005, J PETROL SCI ENG, V47, P209, DOI 10.1016/j.petrol.2005.03.009
   ULLMANN JR, 1976, J ACM, V23, P31, DOI 10.1145/321921.321925
   UMEYAMA S, 1991, IEEE T PATTERN ANAL, V13, P376, DOI 10.1109/34.88573
   VINCENT L, 1991, IEEE T PATTERN ANAL, V13, P583, DOI 10.1109/34.87344
   Vock R, 2019, COMPUT GRAPH-UK, V79, P36, DOI 10.1016/j.cag.2018.12.007
   Wang RZ, 2022, IEEE T PATTERN ANAL, V44, P5261, DOI 10.1109/TPAMI.2021.3078053
   Westin CF, 1998, LECT NOTES COMPUT SC, V1496, P1205, DOI 10.1007/BFb0056310
   Yoo JC, 2009, CIRC SYST SIGNAL PR, V28, P819, DOI [10.1007/s00034-009-9130-7, 10.1007/S00034-009-9130-7]
   Yu TQ, 2016, IEEE GLOB COMM CONF, DOI 10.1109/GLOCOM.2016.7842144
   Yuyin Zhou, 2017, Medical Image Computing and Computer Assisted Intervention  MICCAI 2017. 20th International Conference. Proceedings: LNCS 10433, P693, DOI 10.1007/978-3-319-66182-7_79
   Zhao HK, 2005, MATH COMPUT, V74, P603
   Zhou F, 2016, IEEE T PATTERN ANAL, V38, P1774, DOI 10.1109/TPAMI.2015.2501802
   Zhou XR, 2017, MED PHYS, V44, P5221, DOI 10.1002/mp.12480
NR 67
TC 1
Z9 1
U1 0
U2 8
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2023
VL 9
IS 2
BP 319
EP 333
DI 10.1007/s41095-022-0296-2
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7N8FL
UT WOS:000907570700007
OA gold
DA 2024-07-18
ER

PT J
AU Yang, ZY
   Dai, QH
   Zhang, JS
AF Yang, Zuyi
   Dai, Qinghui
   Zhang, Junsong
TI Visual perception driven collage synthesis
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE collage; gestalt psychology; saliency map; layout optimization; human
   visual perception
AB A collage is a composite artwork made from the spatial layout of multiple pictures on a canvas, collected from the Internet or user photographs. Collages, usually made by skilled artists, involve a complex manual process, especially when searching for component pictures and adjusting their spatial layout to meet artistic requirements. In this paper, we present a visual perception driven method for automatically synthesizing visually pleasing collages. Unlike previous works, we focus on how to design a collage layout which not only provides easy access to the theme of the overall image, but also conforms to human visual perception. To achieve this goal, we formulate the generation of collages as a mapping problem: given a canvas image, first, compute a saliency map for it and a vector field for each sub-region of it. Second, using a divide-and-conquer strategy, generate a series of patch sets from the canvas image, where the salient map and the vector field are used to determine each patch's size and direction respectively. Third, construct a Gestalt-based energy function to choose the most visually pleasing and orderly patch set as the final layout. Finally, using a semantic-color metric, map the picture set to the patch set to generate the final collage. Extensive experimental and user study results show that this method can generate visual pleasing collages.
C1 [Yang, Zuyi; Dai, Qinghui; Zhang, Junsong] Xiamen Univ, Dept Artificial Intelligence, Xiamen 361005, Peoples R China.
C3 Xiamen University
RP Zhang, JS (corresponding author), Xiamen Univ, Dept Artificial Intelligence, Xiamen 361005, Peoples R China.
EM zhangjs@xmu.edu.cn
RI Zhang, Junsong/HKW-6976-2023; Zhang, JunSong/HTQ-4981-2023
FU National Natural Science Foundation of China [61772440]; Aeronautical
   Science Foundation of China [20165168007]; Science and Technology of
   Electro-optic Control Laboratory
FX This work was supported by the National Natural Science Foundation of
   China (No. 61772440), the Aeronautical Science Foundation of China (No.
   20165168007), and Science and Technology of Electro-optic Control
   Laboratory.
CR [Anonymous], 2006, P CVPR
   Battiato S, 2008, LECT NOTES COMPUT SC, V4918, P211, DOI 10.1007/978-3-540-79860-6_17
   Bianco S, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2801126
   Bylinskii Z., 2015, MIT saliency benchmark
   Cheung V., 2007, SHAPE COLLAGE
   Goferman S, 2010, COMPUT GRAPH FORUM, V29, P459, DOI 10.1111/j.1467-8659.2009.01615.x
   Han XT, 2016, IEEE T CYBERNETICS, V46, P1286, DOI 10.1109/TCYB.2015.2448236
   Huang H, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024189
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kim J, 2002, ACM T GRAPHIC, V21, P657
   Lee HY, 2017, MULTIMED TOOLS APPL, V76, P24281, DOI 10.1007/s11042-016-4175-7
   Lee M. H., 2010, P IEEE COMP SOC C CO
   Liu CX, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201314
   Liu LJ, 2018, IEEE T VIS COMPUT GR, V24, P1956, DOI 10.1109/TVCG.2017.2703853
   Lun ZL, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130841
   Mei T, 2009, VISUAL COMPUT, V25, P39, DOI 10.1007/s00371-008-0282-4
   Nan LL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024219
   Patterson G, 2014, INT J COMPUT VISION, V108, P59, DOI 10.1007/s11263-013-0695-z
   Pavic D, 2009, COMPUT GRAPH FORUM, V28, P2244, DOI 10.1111/j.1467-8659.2009.01437.x
   Qi YG, 2015, PROC CVPR IEEE, P1856, DOI 10.1109/CVPR.2015.7298795
   Qi YG, 2013, IEEE IMAGE PROC, P270, DOI 10.1109/ICIP.2013.6738056
   Yu ZQ, 2014, IEEE T VIS COMPUT GR, V20, P182, DOI 10.1109/TVCG.2013.106
   Zhang J. H., 2018, P JOINT S COMP AESTH
NR 23
TC 0
Z9 1
U1 4
U2 24
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2022
VL 8
IS 1
BP 79
EP 91
DI 10.1007/s41095-021-0226-8
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WO6VR
UT WOS:000712590200005
OA gold
DA 2024-07-18
ER

PT J
AU Zhang, WQ
   Fang, JM
   Wang, XG
   Liu, WY
AF Zhang, Wenqiang
   Fang, Jiemin
   Wang, Xinggang
   Liu, Wenyu
TI EfficientPose: Efficient human pose estimation with neural architecture
   search
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE pose estimation; neural architecture search; efficient deep learning
AB Human pose estimation from image and video is a key task in many multimedia applications. Previous methods achieve great performance but rarely take efficiency into consideration, which makes it difficult to implement the networks on lightweight devices. Nowadays, real-time multimedia applications call for more efficient models for better interaction. Moreover, most deep neural networks for pose estimation directly reuse networks designed for image classification as the backbone, which are not optimized for the pose estimation task. In this paper, we propose an efficient framework for human pose estimation with two parts, an efficient backbone and an efficient head. By implementing a differentiable neural architecture search method, we customize the backbone network design for pose estimation, and reduce computational cost with negligible accuracy degradation. For the efficient head, we slim the transposed convolutions and propose a spatial information correction module to promote the performance of the final prediction. In experiments, we evaluate our networks on the MPII and COCO datasets. Our smallest model requires only 0.65 GFLOPs with 88.1% PCKh@0.5 on MPII and our large model needs only 2 GFLOPs while its accuracy is competitive with the state-of-the-art large model, HRNet, which takes 9.5 GFLOPs.
C1 [Zhang, Wenqiang; Fang, Jiemin; Wang, Xinggang; Liu, Wenyu] Huazhong Univ Sci & Technol, Sch EIC, Wuhan 430074, Peoples R China.
   [Fang, Jiemin] Huazhong Univ Sci & Technol, Inst Artificial Intelligence, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology
RP Wang, XG (corresponding author), Huazhong Univ Sci & Technol, Sch EIC, Wuhan 430074, Peoples R China.
EM wqzhang@hust.edu.cn; jaminfong@hust.edu.cn; xgwang@hust.edu.cn;
   liuwy@hust.edu.cn
RI Liu, Wenyu/AAG-1426-2019; Wang, Xinggang/W-4374-2019
OI Liu, Wenyu/0000-0002-4582-7488; Wang, Xinggang/0000-0001-6732-7823;
   Fang, Jiemin/0000-0002-0322-4582
FU National Natural Science Foundation of China (NSFC) [61733007,
   61876212]; Zhejiang Lab [2019NB0AB02]
FX This work was in part supported by National Natural Science Foundation
   of China (NSFC) (Nos. 61733007 and 61876212) and Zhejiang Lab (No.
   2019NB0AB02).
CR Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   [Anonymous], 2019, PROC INT C LEARN REP
   [Anonymous], 2017, ARXIV170506820
   Bender G, 2018, INT C MACH LEARN STO, P549
   Bosman P. A. N., 2020, ARXIV PREPRINT ARXIV
   Brock A., 2018, INT C LEARN REPR
   Bulat A, 2017, IEEE I CONF COMP VIS, P3726, DOI 10.1109/ICCV.2017.400
   Cai Han, 2019, INT C LEARN REPR
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Dong XY, 2019, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2019.00186
   Fang J., 2020, P INT C LEARN REPR
   Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720
   Gong X., 2020, ARXIV PREPRINT ARXIV
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Huang SL, 2017, IEEE I CONF COMP VIS, P3047, DOI 10.1109/ICCV.2017.329
   Jiemin Fang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10625, DOI 10.1109/CVPR42600.2020.01064
   Li W., 2019, arXiv
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu CX, 2019, PROC CVPR IEEE, P82, DOI 10.1109/CVPR.2019.00017
   Liu H, 2019, INT C LEARNING REPRE, DOI DOI 10.1109/CVPR42600.2020.00243
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Odena A., 2016, DISTILL, V1, pe3, DOI 10.23915/distill.00003.-URL
   Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395
   Pishchulin L, 2013, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2013.82
   Real E, 2019, AAAI CONF ARTIF INTE, P4780
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sugawara Y, 2019, APSIPA TRANS SIGNAL, V8, DOI 10.1017/ATSIP.2019.2
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Tan M., 2020, P 2020 IEEE CVF C CO, P10
   Tan MX, 2019, PROC CVPR IEEE, P2815, DOI [arXiv:1807.11626, 10.1109/CVPR.2019.00293]
   Tang W, 2018, LECT NOTES COMPUT SC, V11207, P197, DOI 10.1007/978-3-030-01219-9_12
   Tang ZQ, 2018, LECT NOTES COMPUT SC, V11207, P348, DOI 10.1007/978-3-030-01219-9_21
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Wojna Z, 2017, P BRIT MACH VIS C BM, P1, DOI [DOI 10.5244/C.31.10, 10.5244/C.31.10]
   Wu BC, 2019, PROC CVPR IEEE, P10726, DOI 10.1109/CVPR.2019.01099
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Yang S, 2020, J R STAT SOC B, V82, P445, DOI 10.1111/rssb.12354
   Yang W, 2017, IEEE I CONF COMP VIS, P1290, DOI 10.1109/ICCV.2017.144
   Yang Y, 2011, PROC CVPR IEEE, P1385, DOI 10.1109/CVPR.2011.5995741
   Zhang, 2019, ARXIV PREPRINT ARXIV
   Zhang F, 2019, PROC CVPR IEEE, P3512, DOI 10.1109/CVPR.2019.00363
   Zhang YH, 2019, PROC CVPR IEEE, P11633, DOI 10.1109/CVPR.2019.01191
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
NR 46
TC 21
Z9 25
U1 3
U2 32
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2021
VL 7
IS 3
BP 335
EP 347
DI 10.1007/s41095-021-0214-z
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TR1TJ
UT WOS:000678754100004
OA Green Submitted, gold
DA 2024-07-18
ER

PT J
AU Guo, MH
   Cai, JX
   Liu, ZN
   Mu, TJ
   Martin, RR
   Hu, SM
AF Guo, Meng-Hao
   Cai, Jun-Xiong
   Liu, Zheng-Ning
   Mu, Tai-Jiang
   Martin, Ralph R.
   Hu, Shi-Min
TI PCT: Point cloud transformer
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE 3D computer vision; deep learning; point cloud processing; Transformer
AB The irregular domain and lack of ordering make it challenging to design deep neural networks for point cloud processing. This paper presents a novel framework named Point Cloud Transformer (PCT) for point cloud learning. PCT is based on Transformer, which achieves huge success in natural language processing and displays great potential in image processing. It is inherently permutation invariant for processing a sequence of points, making it well-suited for point cloud learning. To better capture local context within the point cloud, we enhance input embedding with the support of farthest point sampling and nearest neighbor search. Extensive experiments demonstrate that the PCT achieves the state-of-the-art performance on shape classification, part segmentation, semantic segmentation, and normal estimation tasks.
C1 [Guo, Meng-Hao; Cai, Jun-Xiong; Liu, Zheng-Ning; Mu, Tai-Jiang; Hu, Shi-Min] Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beiing 100084, Peoples R China.
   [Martin, Ralph R.] Cardiff Univ, Cardiff CF243AA, Wales.
C3 Tsinghua University; Cardiff University
RP Hu, SM (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beiing 100084, Peoples R China.
EM gmh20@mails.tsinghua.edu.cn; junxiong20@mails.tsinghua.edu.cn;
   lzhengning@gmail.com; taijiang@tsinghua.edu.cn; ralph@cs.cf.ac.uk;
   shimin@tsinghua.edu.cn
RI Mu, Tai-Jiang/JWO-1381-2024; Hu, Shi-Min/AAW-1952-2020; Martin, Ralph
   R/D-2366-2010
OI Mu, Tai-Jiang/0000-0002-9197-346X; Liu, Zhengning/0000-0001-6643-6016;
   Martin, Ralph/0000-0002-8495-8536
FU National Natural Science Foundation of China [61521002]; Joint NSFC-DFG
   Research Program [61761136018]
FX This work was supported by the National Natural Science Foundation of
   China (Project Number 61521002) and the Joint NSFC-DFG Research Program
   (Project Number 61761136018).
CR [Anonymous], 2015, 3 INT C LEARNING REP
   Atzmon M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201301
   Bruna J., 2014, ICLR
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai ZH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2978
   Devlin J., 2018, BERT PRE TRAINING DE
   Dosovitskiy Alexey, 2021, 2021 INT C LEARN REP
   Hermosilla P, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275110
   Hertz Amir, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12051, DOI 10.1109/CVPR42600.2020.01207
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu SM, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-020-3097-4
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Komarichev A, 2019, PROC CVPR IEEE, P7413, DOI 10.1109/CVPR.2019.00760
   Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479
   Le T, 2018, PROC CVPR IEEE, P9204, DOI 10.1109/CVPR.2018.00959
   Lee J, 2020, BIOINFORMATICS, V36, P1234, DOI 10.1093/bioinformatics/btz682
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Li YY, 2018, ADV NEUR IN, V31
   Lin Z., 2017, PROC 5 INT C LEARN R
   Liu XH, 2019, AAAI CONF ARTIF INTE, P8778
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Qi C. R., 2017, Advances in neural information processing systems, P5099
   Tatarchenko M, 2018, PROC CVPR IEEE, P3887, DOI 10.1109/CVPR.2018.00409
   Tchapmi LP, 2017, INT CONF 3D VISION, P537, DOI 10.1109/3DV.2017.00067
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu B., 2020, Visual transformers: Token-based image representation and processing for computer vision
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Xie SN, 2018, PROC CVPR IEEE, P4606, DOI 10.1109/CVPR.2018.00484
   Yan X, 2020, PROC CVPR IEEE, P5588, DOI 10.1109/CVPR42600.2020.00563
   Yang ZL, 2019, ADV NEUR IN, V32
   Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238
   Yuqi Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13575, DOI 10.1109/CVPR42600.2020.01359
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhao HS, 2019, PROC CVPR IEEE, P5550, DOI 10.1109/CVPR.2019.00571
NR 39
TC 685
Z9 743
U1 61
U2 367
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2021
VL 7
IS 2
BP 187
EP 199
DI 10.1007/s41095-021-0229-5
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FR
UT WOS:000648692900003
OA Green Accepted, gold, Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Zhou, T
   Fan, DP
   Cheng, MM
   Shen, JB
   Shao, L
AF Zhou, Tao
   Fan, Deng-Ping
   Cheng, Ming-Ming
   Shen, Jianbing
   Shao, Ling
TI RGB-D salient object detection: A survey
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review
DE RGB-D; saliency; light fields; benchmarks
ID VISUAL-ATTENTION; FUSION; SEGMENTATION; INTEGRATION; NETWORK; IMAGE
AB Salient object detection, which simulates human visual perception in locating the most significant object(s) in a scene, has been widely applied to various computer vision tasks. Now, the advent of depth sensors means that depth maps can easily be captured; this additional spatial information can boost the performance of salient object detection. Although various RGB-D based salient object detection models with promising performance have been proposed over the past several years, an in-depth understanding of these models and the challenges in this field remains lacking. In this paper, we provide a comprehensive survey of RGB-D based salient object detection models from various perspectives, and review related benchmark datasets in detail. Further, as light fields can also provide depth maps, we review salient object detection models and popular benchmark datasets from this domain too. Moreover, to investigate the ability of existing models to detect salient objects, we have carried out a comprehensive attribute-based evaluation of several representative RGB-D based salient object detection models. Finally, we discuss several challenges and open directions of RGB-D based salient object detection for future research. All collected models, benchmark datasets, datasets constructed for attribute-based evaluation, and related code are publicly available at https://github.com/taozh2017/RGBD-SODsurvey.
C1 [Zhou, Tao; Fan, Deng-Ping; Shen, Jianbing; Shao, Ling] Incept Inst Artificial Intelligence IIAI, Abu Dhabi, U Arab Emirates.
   [Cheng, Ming-Ming] Nankai Univ, CS, Tianjin 300350, Peoples R China.
C3 Nankai University
RP Fan, DP (corresponding author), Incept Inst Artificial Intelligence IIAI, Abu Dhabi, U Arab Emirates.
EM dengpfan@gmail.com
RI Cheng, Ming-Ming/A-2527-2009; Shao, Ling/D-3535-2011; Fan,
   Deng-Ping/ABD-4052-2020
OI Cheng, Ming-Ming/0000-0001-5550-8758; Fan, Deng-Ping/0000-0002-5245-7518
FU Major Project for a New Generation of AI [2018AAA0100400]; National
   Natural Science Foundation of China [61922046]; Tianjin Natural Science
   Foundation [17JCJQJC43700]
FX This research was supported by a Major Project for a New Generation of
   AI under Grant No. 2018AAA0100400, National Natural Science Foundation
   of China (61922046), and Tianjin Natural Science Foundation
   (17JCJQJC43700).
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2016, Proceedings of the IEEE conference on computer vision and pattern recognition, DOI DOI 10.1109/CVPR.2016.257
   Ao Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P346, DOI 10.1007/978-3-030-58610-2_21
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Borji A, 2019, COMPUT VIS MEDIA, V5, P117, DOI 10.1007/s41095-019-0149-9
   Carlberg B, 2008, ELEC COMP C, P191, DOI 10.1109/ECTC.2008.4549969
   Chen C., 2020, ARXIV200804159
   Chen CLZ, 2020, IEEE T IMAGE PROCESS, V29, P4296, DOI 10.1109/TIP.2020.2968250
   Chen H, 2019, ARXIV190909309
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Chen H, 2020, IEEE T IMAGE PROCESS, V29, P8407, DOI 10.1109/TIP.2020.3014734
   Chen H, 2020, IEEE T CYBERNETICS, V50, P4808, DOI 10.1109/TCYB.2019.2934986
   Chen H, 2018, IEEE INT C INT ROBOT, P6821, DOI 10.1109/IROS.2018.8594373
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen H, 2019, PATTERN RECOGN, V86, P376, DOI 10.1016/j.patcog.2018.08.007
   Chen H, 2017, IEEE INT C INT ROBOT, P4911, DOI 10.1109/IROS.2017.8206370
   Chen H, 2017, LECT NOTES COMPUT SC, V10528, P459, DOI 10.1007/978-3-319-68345-4_41
   Chen TL, 2020, PROC CVPR IEEE, P696, DOI 10.1109/CVPR42600.2020.00078
   Chen Z., 2020, ARXIV200308608
   Cheng Yu, 2017, ARXIV
   Cheng-Feng Sun, 2020, Learning and Collaboration Technologies. Human and Technology Ecosystems. 7th International Conference, LCT 2020. Held as Part of the 22nd HCI International Conference, HCII 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12206), P520, DOI 10.1007/978-3-030-50506-6_35
   Ciptadi A, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.112
   Cong RM, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2557347
   Cong RM, 2020, IEEE T CYBERNETICS, V50, P3627, DOI 10.1109/TCYB.2019.2932005
   Cong RM, 2019, IEEE T MULTIMEDIA, V21, P1660, DOI 10.1109/TMM.2018.2884481
   Cong RM, 2019, IEEE T CIRC SYST VID, V29, P2941, DOI 10.1109/TCSVT.2018.2870832
   Cong RM, 2019, IEEE T CYBERNETICS, V49, P233, DOI 10.1109/TCYB.2017.2771488
   Cong RM, 2018, IEEE T IMAGE PROCESS, V27, P568, DOI 10.1109/TIP.2017.2763819
   Dai A, 2020, PROC CVPR IEEE, P846, DOI 10.1109/CVPR42600.2020.00093
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Desingh K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.98
   Ding Y, 2019, J VIS COMMUN IMAGE R, V61, P1, DOI 10.1016/j.jvcir.2019.03.019
   Du H, 2019, MULTIMED TOOLS APPL, V78, P12125, DOI 10.1007/s11042-018-6736-4
   Du H, 2016, IEEE ACCESS, V4, P8987, DOI 10.1109/ACCESS.2016.2632724
   Fan D P, 2020, LECT NOTES COMPUTER, P263
   Fan D.-P, 2020, ARXIV200703380
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2020, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR42600.2020.00285
   Fan DP, 2020, IEEE T MED IMAGING, V39, P2626, DOI 10.1109/TMI.2020.2996645
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fan DP, 2018, LECT NOTES COMPUT SC, V11219, P196, DOI 10.1007/978-3-030-01267-0_12
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Fan XX, 2014, INT CONF DIGIT SIG, P454, DOI 10.1109/ICDSP.2014.6900706
   Fang HS, 2018, LECT NOTES COMPUT SC, V11214, P52, DOI 10.1007/978-3-030-01249-6_4
   Feng D, 2017, PROC INT C DIGIT IMA, P1
   Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312
   Gao Y, 2018, 2018 INTERNATIONAL CONFERENCE ON NETWORKING AND NETWORK APPLICATIONS (NANA), P74, DOI 10.1109/NaNA2018.2018.00020
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Gongyang Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P665, DOI 10.1007/978-3-030-58520-4_39
   Guanghai Liu, 2013, 2013 International Conference on Information Science and Cloud Computing Companion (ISCC-C), P728, DOI 10.1109/ISCC-C.2013.21
   Guo J., 2016, P ANN REL MAINT S, P1, DOI DOI 10.1109/RAMS.2016.7448068
   Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775
   Han JW, 2018, IEEE SIGNAL PROC MAG, V35, P84, DOI 10.1109/MSP.2017.2749125
   He YH, 2018, LECT NOTES COMPUT SC, V11211, P815, DOI 10.1007/978-3-030-01234-2_48
   Hongyi Su, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9225, P1, DOI 10.1007/978-3-319-22180-9_1
   Hou YT, 2007, IEEE INFOCOM SER, P1, DOI 10.1109/INFCOM.2007.9
   Huang F, 2017, IEEE T IMAGE PROCESS, V26, P1911, DOI 10.1109/TIP.2017.2669878
   HUANG P, 2018, 2018 IEEE 23 INT C D, P1, DOI DOI 10.1109/ICDSP.2018.8631584
   Huang R, 2020, IEEE SIGNAL PROC LET, V27, P775, DOI 10.1109/LSP.2020.2989674
   Huang R, 2019, IEEE SIGNAL PROC LET, V26, P552, DOI 10.1109/LSP.2019.2898508
   Huang XM, 2018, PATTERN RECOGN, V76, P95, DOI 10.1016/j.patcog.2017.10.027
   Huang XM, 2017, IEEE T IMAGE PROCESS, V26, P4243, DOI 10.1109/TIP.2017.2710636
   Huang Z., 2020, ARXIV200714352
   Imamoglu N, 2018, SIGNAL IMAGE VIDEO P, V12, P307, DOI 10.1007/s11760-017-1159-7
   Jiang B, 2021, IEEE T MULTIMEDIA, V23, P1343, DOI 10.1109/TMM.2020.2997184
   Jiang LX, 2015, IEEE INT CONF ROBOT, P1323, DOI 10.1109/ICRA.2015.7139362
   Jin L, 2020, PROC CVPR IEEE, P886, DOI 10.1109/CVPR42600.2020.00097
   Jin ZG, 2019, IEEE ACCESS, V7, P141311, DOI 10.1109/ACCESS.2019.2943899
   Jing Zhang, 2018, Understanding Human Activities Through 3D Sensors. Second International Workshop, UHA3DS 2016 Held in Conjunction with the 23rd International Conference on Pattern Recognition, ICPR 2016. Revised Selected Papers: LNCS 10188, P101, DOI 10.1007/978-3-319-91863-1_8
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Junyao Guo, 2015, 2015 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference (ISGT). Proceedings, P1, DOI 10.1109/ISGT.2015.7131832
   Lai K, 2011, IEEE INT CONF ROBOT, P1817
   Lang CY, 2012, LECT NOTES COMPUT SC, V7573, P101, DOI 10.1007/978-3-642-33709-3_8
   Lei JJ, 2013, NEUROCOMPUTING, V120, P24, DOI 10.1016/j.neucom.2012.08.057
   Li C Y, 2020, LECT NOTES COMPUTER, P225
   Li C, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC 2017), P8, DOI 10.1109/ICIVC.2017.7984449
   Li CY, 2021, IEEE T CYBERNETICS, V51, P88, DOI 10.1109/TCYB.2020.2969255
   Li GY, 2020, IEEE T IMAGE PROCESS, V29, P4873, DOI 10.1109/TIP.2020.2976689
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li NY, 2017, IEEE T PATTERN ANAL, V39, P1605, DOI 10.1109/TPAMI.2016.2610425
   Li NY, 2015, PROC CVPR IEEE, P5216, DOI 10.1109/CVPR.2015.7299158
   Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359
   Li X, 2018, LECT NOTES COMPUT SC, V11219, P370, DOI 10.1007/978-3-030-01267-0_22
   Liang FF, 2020, PATTERN RECOGN, V104, DOI 10.1016/j.patcog.2020.107329
   Liang FF, 2018, NEUROCOMPUTING, V275, P2227, DOI 10.1016/j.neucom.2017.10.052
   Liu D, 2019, IEEE IMAGE PROC, P3925, DOI [10.1109/icip.2019.8803653, 10.1109/ICIP.2019.8803653]
   Liu FY, 2015, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2015.7299152
   Liu N., 2020, P IEEE C COMP VIS PA
   Liu ZY, 2020, MULTIMED TOOLS APPL, V79, P25403, DOI 10.1007/s11042-020-09188-8
   Liu ZY, 2020, NEUROCOMPUTING, V387, P210, DOI 10.1016/j.neucom.2020.01.045
   Liu ZY, 2019, NEUROCOMPUTING, V363, P46, DOI 10.1016/j.neucom.2019.07.012
   Lu JS, 2016, ADV NEUR IN, V29
   Lu XK, 2019, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2019.00374
   Ma CY, 2015, J VISION, V15, DOI 10.1167/15.6.19
   Ma YP, 2017, INT SYM COMPUT INTEL, P389, DOI 10.1109/ISCID.2017.92
   Mahadevan V, 2009, PROC CVPR IEEE, P1007, DOI 10.1109/CVPRW.2009.5206573
   Martinel N, 2015, IEEE T IMAGE PROCESS, V24, P5645, DOI 10.1109/TIP.2015.2487048
   Mei J., 2020, ARXIV PREPRINT ARXIV
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Nguyen TV, 2018, INT J COMPUT VISION, V126, P86, DOI 10.1007/s11263-017-1042-6
   Nie GY, 2019, PROC CVPR IEEE, P3278, DOI 10.1109/CVPR.2019.00340
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Pan J., 2017, PROC IEEE C COMPUT V
   Pang Y W, 2020, LECT NOTES COMPUTER, P235
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Piao Y. R., 2018, P FRONT OPT LAS SCI
   Piao YR, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P904
   Piao YR, 2020, AAAI CONF ARTIF INTE, V34, P11865
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Piao YR, 2020, IEEE T IMAGE PROCESS, V29, P1879, DOI 10.1109/TIP.2019.2942434
   Qian MY, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.06.021
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Rapantzikos K, 2009, PROC CVPR IEEE, P1454, DOI 10.1109/CVPRW.2009.5206525
   Ren J, 2013, 2013 INTERNATIONAL CONFERENCE ON ADVANCED CLOUD AND BIG DATA (CBD), P20, DOI 10.1109/CBD.2013.30
   Sheng H, 2016, INT CONF ACOUST SPEE, P1631, DOI 10.1109/ICASSP.2016.7471953
   Sheng H, 2016, INT CONF ACOUST SPEE, P1347, DOI 10.1109/ICASSP.2016.7471896
   Shigematsu R, 2017, IEEE INT CONF COMP V, P2749, DOI 10.1109/ICCVW.2017.323
   Shimoda W, 2016, LECT NOTES COMPUT SC, V9908, P218, DOI 10.1007/978-3-319-46493-0_14
   Song HK, 2017, IEEE T IMAGE PROCESS, V26, P4204, DOI 10.1109/TIP.2017.2711277
   Song HK, 2016, INT CONF ACOUST SPEE, P1626, DOI 10.1109/ICASSP.2016.7471952
   Song HM, 2018, LECT NOTES COMPUT SC, V11215, P744, DOI 10.1007/978-3-030-01252-6_44
   Su JM, 2019, IEEE I CONF COMP VIS, P3798, DOI 10.1109/ICCV.2019.00390
   Sun D D, 2020, COMMUNICATIONS COMPU, V1160, P670
   Tang YL, 2016, VISUAL COMPUT, V32, P111, DOI 10.1007/s00371-014-1059-6
   Tu WC, 2016, PROC CVPR IEEE, P2334, DOI 10.1109/CVPR.2016.256
   Tu Z., 2022, IEEE Trans. Multimed.
   Tu Z., 2020, ARXIV200502315
   Tu ZZ, 2020, IEEE T MULTIMEDIA, V22, P160, DOI 10.1109/TMM.2019.2924578
   Tu ZZ, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P141, DOI 10.1109/MIPR.2019.00032
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang AZ, 2017, NEURAL PROCESS LETT, V46, P1083, DOI 10.1007/s11063-017-9610-x
   Wang AZ, 2017, IEEE SIGNAL PROC LET, V24, P663, DOI 10.1109/LSP.2017.2688136
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang G., 2018, CHINESE C IMAGE GRAP, P359
   Wang LJ, 2020, PROC CVPR IEEE, P538, DOI 10.1109/CVPR42600.2020.00062
   Wang NN, 2019, IEEE ACCESS, V7, P55277, DOI 10.1109/ACCESS.2019.2913107
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang ST, 2016, INT C PATT RECOG, P1881, DOI 10.1109/ICPR.2016.7899911
   Wang ST, 2017, LECT NOTES COMPUT SC, V10115, P20, DOI 10.1007/978-3-319-54193-8_2
   Wang TT, 2019, IEEE I CONF COMP VIS, P8837, DOI 10.1109/ICCV.2019.00893
   Wang W., 2019, ARXIV190409146
   Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang Wenguan, 2018, IEEE Trans Image Process, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang X., 2020, ARXIV200804157
   Wang X, 2021, MULTIMED TOOLS APPL, V80, P16329, DOI 10.1007/s11042-020-08890-x
   Wang XH, 2021, IEEE T IMAGE PROCESS, V30, P458, DOI 10.1109/TIP.2020.3037470
   Wang Y, 2020, P AS C COMP VIS, P1
   Wei Ji, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P52, DOI 10.1007/978-3-030-58523-5_4
   Xia C., 2017, PROC CVPR IEEE, P4142, DOI DOI 10.1109/CVPR.2017.468
   Xiao F, 2020, IEEE ACCESS, V8, P26602, DOI 10.1109/ACCESS.2020.2971509
   Xiaoqi Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P646, DOI 10.1007/978-3-030-58542-6_39
   Xu C, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2490539
   Xu XD, 2013, TRANSPORT RES REC, P23, DOI 10.3141/2399-03
   Xue HY, 2015, IEEE IMAGE PROC, P666, DOI 10.1109/ICIP.2015.7350882
   Yan PX, 2019, IEEE I CONF COMP VIS, P7283, DOI 10.1109/ICCV.2019.00738
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yongri Piao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9057, DOI 10.1109/CVPR42600.2020.00908
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Zeng Y, 2019, IEEE I CONF COMP VIS, P7222, DOI 10.1109/ICCV.2019.00732
   Zhang DW, 2018, ACM T INTEL SYST TEC, V9, DOI 10.1145/3158674
   Zhang DW, 2017, IEEE T PATTERN ANAL, V39, P865, DOI 10.1109/TPAMI.2016.2567393
   Zhang Dingwen, 2017, ARXIV170301290
   Zhang D, 2017, P AMER CONTR CONF, P4042, DOI 10.23919/ACC.2017.7963575
   Zhang HL, 2012, INT CONF COMP SCI ED, P763, DOI 10.1109/ICCSE.2012.6295184
   Zhang J., 2020, P IEEE CVF C COMP VI, P8579, DOI DOI 10.1109/CVPR42600.2020.00861
   Zhang J, 2017, CHIN J EXP TRADIT ME, V13, P1
   Zhang J, 2020, IEEE T IMAGE PROCESS, V29, P4421, DOI 10.1109/TIP.2020.2970529
   Zhang J, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2212
   Zhang M, 2020, LECT NOTES COMPUTER, P374
   Zhang M, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4107, DOI [10.1145/3394171.3413969, 10.1145/33941713413969]
   Zhang M, 2020, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR42600.2020.00353
   Zhang M, 2019, ADV NEUR IN, V32
   Zhang M, 2020, IEEE T IMAGE PROCESS, V29, P6276, DOI 10.1109/TIP.2020.2990341
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang PP, 2017, IEEE I CONF COMP VIS, P212, DOI 10.1109/ICCV.2017.32
   Zhang Q, 2020, IEEE T IMAGE PROCESS, V29, P3321, DOI 10.1109/TIP.2019.2959253
   Zhang Z., 2020, ARXIV200414582
   Zhao J. W., 2020, ARXIV200600269
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao J, 2020, IEEE T FUZZY SYST, V28, P2287, DOI 10.1109/TFUZZ.2019.2930492
   Zhao R, 2017, IEEE T PATTERN ANAL, V39, P356, DOI 10.1109/TPAMI.2016.2544310
   Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320
   Zhao ZQ, 2019, IEEE T NEUR NET LEAR, V30, P3212, DOI 10.1109/TNNLS.2018.2876865
   Zhou L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2438546
   Zhou T, 2020, IEEE T MED IMAGING, V39, P2772, DOI 10.1109/TMI.2020.2975344
   Zhou T, 2020, MED IMAGE ANAL, V60, DOI 10.1016/j.media.2019.101630
   Zhou T, 2019, IEEE T MED IMAGING, V38, P2411, DOI 10.1109/TMI.2019.2913158
   Zhou T, 2019, HUM BRAIN MAPP, V40, P1001, DOI 10.1002/hbm.24428
   Zhou WJ, 2020, IEEE SIGNAL PROC LET, V27, P800, DOI 10.1109/LSP.2020.2993471
   Zhou WJ, 2021, IEEE T SYST MAN CY-S, V51, P3641, DOI 10.1109/TSMC.2019.2957386
   Zhou XF, 2020, IMAGE VISION COMPUT, V95, DOI 10.1016/j.imavis.2020.103888
   Zhou Y, 2019, IEEE T CYBERNETICS, V49, P1173, DOI 10.1109/TCYB.2018.2793278
   Zhu CB, 2019, IEEE INT CON MULTI, P199, DOI 10.1109/ICME.2019.00042
   Zhu CB, 2017, IEEE INT CONF COMP V, P1509, DOI 10.1109/ICCVW.2017.178
   Zhu CB, 2017, IEEE INT CONF COMP V, P3008, DOI 10.1109/ICCVW.2017.355
   Zhu DD, 2018, SYMMETRY-BASEL, V10, DOI 10.3390/sym10100457
   Zhu JY, 2015, IEEE T PATTERN ANAL, V37, P862, DOI 10.1109/TPAMI.2014.2353617
   Zhu L, 2015, CHIN AUTOM CONGR, P512, DOI 10.1109/CAC.2015.7382554
NR 204
TC 171
Z9 178
U1 35
U2 321
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2021
VL 7
IS 1
BP 37
EP 69
DI 10.1007/s41095-020-0199-z
PG 33
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FQ
UT WOS:000648692800003
PM 33432275
OA Green Published, gold, Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Murali, S
   Govindan, VK
   Kalady, S
AF Murali, Saritha
   Govindan, V. K.
   Kalady, Saidalavi
TI Single image shadow removal by optimization using non-shadow anchor
   values
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE mean-shift segmentation; particle swarm optimization; HSV; YCbCr; anchor
   values
AB Shadow removal has evolved as a pre-processing step for various computer vision tasks. Several studies have been carried out over the past two decades to eliminate shadows from videos and images. Accurate shadow detection is an open problem because it is often considered difficult to interpret whether the darkness of a surface is contributed by a shadow incident on it or not. This paper introduces a color-model based technique to remove shadows from images. We formulate shadow removal as an optimization problem that minimizes the dissimilarities between a shadow area and its non-shadow counterpart. To achieve this, we map each shadow region to a set of non-shadow pixels, and compute an anchor value from the non-shadow pixels. The shadow region is then modified using a factor computed from the anchor value using particle swarm optimization. We demonstrate the efficiency of our technique on indoor shadows, outdoor shadows, soft shadows, and document shadows, both qualitatively and quantitatively.
C1 [Murali, Saritha; Govindan, V. K.; Kalady, Saidalavi] Natl Inst Technol Calicut, Dept Comp Sci & Engn, Kozhikode 673601, Kerala, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Calicut
RP Murali, S (corresponding author), Natl Inst Technol Calicut, Dept Comp Sci & Engn, Kozhikode 673601, Kerala, India.
EM saritha.mkv@gmail.com; vkg@nitc.ac.in; said@nitc.ac.in
OI MURALI, SARITHA/0000-0002-6778-6138
CR [Anonymous], 2015, ACM T GRAPHICS, V34, P5
   Arbel E, 2011, IEEE T PATTERN ANAL, V33, P1202, DOI 10.1109/TPAMI.2010.157
   Baba M, 2004, P ACM SIGGRAPH 2004, V60, DOI [10.1145/1186415.1186484, DOI 10.1145/1186415.1186484]
   Bako S, 2017, LECT NOTES COMPUT SC, V10113, P173, DOI 10.1007/978-3-319-54187-7_12
   Barrow H. G., 1978, Computer Vision Systems, P3
   Christoudias CM, 2002, INT C PATT RECOG, P150, DOI 10.1109/ICPR.2002.1047421
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Eberhart R.C., 1995, Proc Int Symp Micro Mach Hum Sci, P39, DOI [DOI 10.1109/MHS.1995.494215, 10.1109/mhs.1995.494215]
   Finlayson GD, 2002, LECT NOTES COMPUT SC, V2353, P823
   Finlayson GD, 2009, INT J COMPUT VISION, V85, P35, DOI 10.1007/s11263-009-0243-z
   FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330
   Gilchrist A, 1999, PSYCHOL REV, V106, P795, DOI 10.1037/0033-295X.106.4.795
   Gong H, 2017, IMAGE VISION COMPUT, V62, P19, DOI 10.1016/j.imavis.2017.04.001
   Gong H, 2016, J OPT SOC AM A, V33, P1798, DOI 10.1364/JOSAA.33.001798
   Guo Ruiqi, 2013, IEEE Trans Pattern Anal Mach Intell, V35, P2956, DOI 10.1109/TPAMI.2012.214
   Khan SH, 2016, IEEE T PATTERN ANAL, V38, P431, DOI 10.1109/TPAMI.2015.2462355
   Krawczyk G, 2005, COMPUT GRAPH FORUM, V24, P635, DOI 10.1111/j.1467-8659.2005.00888.x
   Meer P, 2001, IEEE T PATTERN ANAL, V23, P1351, DOI 10.1109/34.977560
   Murali Saritha, 2016, International Journal of Image, Graphics and Signal Processing, V8, P38, DOI 10.5815/ijigsp.2016.12.05
   Murali S, 2018, INF TECHNOL CONTROL, V47, P75, DOI 10.5755/j01.itc.47.1.15012
   Murali S, 2013, CYBERN INF TECHNOL, V13, P95, DOI 10.2478/cait-2013-0009
   Niu JW, 2016, PATTERN RECOGN, V59, P225, DOI 10.1016/j.patcog.2015.12.010
   Oliveira DM, 2013, LECT NOTES COMPUT SC, V7950, P308, DOI 10.1007/978-3-642-39094-4_35
   Sanin A, 2012, PATTERN RECOGN, V45, P1684, DOI 10.1016/j.patcog.2011.10.001
   Sasi RK, 2015, PROCEEDING OF THE THIRD INTERNATIONAL SYMPOSIUM ON WOMEN IN COMPUTING AND INFORMATICS (WCI-2015), P309, DOI 10.1145/2791405.2791450
   Sasi RK, 2015, EGYPT INFORM J, V16, P29, DOI 10.1016/j.eij.2014.11.003
   Shen L, 2015, PROC CVPR IEEE, P2067, DOI 10.1109/CVPR.2015.7298818
   Su YF, 2010, IEEE T IMAGE PROCESS, V19, P2749, DOI 10.1109/TIP.2010.2050626
   Vicente TFY, 2018, IEEE T PATTERN ANAL, V40, P682, DOI 10.1109/TPAMI.2017.2691703
   Wang JF, 2018, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR.2018.00192
   Wu TP, 2005, IEEE I CONF COMP VIS, P480
   Xu ML, 2017, IEEE T IMAGE PROCESS, V26, P5811, DOI 10.1109/TIP.2017.2737321
   Xu ML, 2016, NEUROCOMPUTING, V195, P117, DOI 10.1016/j.neucom.2015.08.117
   Yang QX, 2012, IEEE T IMAGE PROCESS, V21, P4361, DOI 10.1109/TIP.2012.2208976
   Yu XM, 2017, LECT NOTES COMPUT SC, V10425, P307, DOI 10.1007/978-3-319-64698-5_26
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, P4623, DOI 10.1109/TIP.2015.2465159
   Zhu JJ, 2010, PROC CVPR IEEE, P223, DOI 10.1109/CVPR.2010.5540209
NR 37
TC 6
Z9 8
U1 0
U2 12
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2019
VL 5
IS 3
BP 311
EP 324
DI 10.1007/s41095-019-0148-x
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UN
UT WOS:000648690900007
OA gold
DA 2024-07-18
ER

PT J
AU Sakai, H
   Nabata, K
   Yasuaki, S
   Iwasaki, K
AF Sakai, Hirokazu
   Nabata, Kosuke
   Yasuaki, Shinya
   Iwasaki, Kei
TI A method for estimating the errors in many-light rendering with
   supersampling
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE anti-aliasing; depth of field; many-light rendering; participating media
AB In many-light rendering, a variety of visual and illumination effects, including anti-aliasing, depth of field, volumetric scattering, and subsurface scattering, are combined to create a number of virtual point lights (VPLs). This is done in order to simplify computation of the resulting illumination. Naive approaches that sum the direct illumination from many VPLs are computationally expensive; scalable methods can be computed more efficiently by clustering VPLs, and then estimating their sum by sampling a small number of VPLs. Although significant speed-up has been achieved using scalable methods, clustering leads to uncontrollable errors, resulting in noise in the rendered images. In this paper, we propose a method to improve the estimation accuracy of many-light rendering involving such visual and illumination effects. We demonstrate that our method can improve the estimation accuracy by a factor of 2.3 over the previous method.
C1 [Sakai, Hirokazu; Nabata, Kosuke; Yasuaki, Shinya; Iwasaki, Kei] Wakayama Univ, Wakayama, Wakayama 6408510, Japan.
   [Iwasaki, Kei] Dwango CG Res, Bunkyo Ku, KADOKAWA Hongo Bldg 5245 Hongo, Tokyo 1130033, Japan.
C3 Wakayama University
RP Iwasaki, K (corresponding author), Wakayama Univ, Wakayama, Wakayama 6408510, Japan.; Iwasaki, K (corresponding author), Dwango CG Res, Bunkyo Ku, KADOKAWA Hongo Bldg 5245 Hongo, Tokyo 1130033, Japan.
EM iwasaki@sys.wakayama-u.ac.jp
RI Iwasaki, Kei/GNH-6504-2022
OI Iwasaki, Kei/0000-0002-5235-536X
FU JSPS KAKENHI [15H05924, 18H03348]; Grants-in-Aid for Scientific Research
   [15H05924, 18H03348] Funding Source: KAKEN
FX This work was partially supported by JSPS KAKENHI 15H05924 and 18H03348.
CR [Anonymous], 2012, ACM T GRAPHICS, V31
   [Anonymous], 2016, ACM T GRAPHICS, V35
   Arbree A, 2008, COMPUT GRAPH FORUM, V27, P507, DOI 10.1111/j.1467-8659.2008.01148.x
   Dachsbacher C, 2014, COMPUT GRAPH FORUM, V33, P88, DOI 10.1111/cgf.12256
   Engelhardt T, 2012, COMPUT GRAPH FORUM, V31, P2145, DOI 10.1111/j.1467-8659.2012.03207.x
   Frederickx Roald, 2015, EUROGRAPHICS 2015 SH, P61
   Georgiev I, 2012, COMPUT GRAPH FORUM, V31, P701, DOI 10.1111/j.1467-8659.2012.03049.x
   Huo YC, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818120
   Jensen HW, 2001, COMP GRAPH, P511, DOI 10.1145/383259.383319
   Keller A., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P49, DOI 10.1145/258734.258769
   King A., 2013, P ACM SIGGRAPH 2013
   Kivanek J, 2013, ACM SIGGRAPH
   Nabata K, 2016, COMPUT GRAPH FORUM, V35, P431, DOI 10.1111/cgf.13040
   Walter B, 2005, ACM T GRAPHIC, V24, P1098, DOI 10.1145/1073204.1073318
   Walter B, 2006, ACM T GRAPHIC, V25, P1081, DOI 10.1145/1141911.1141997
   Walter B, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185555
   Wu YT, 2015, IEEE T VIS COMPUT GR, V21, P363, DOI 10.1109/TVCG.2014.2385059
   Wu YT, 2013, IEEE T VIS COMPUT GR, V19, P1566, DOI 10.1109/TVCG.2013.21
   Yoshida H., 2015, J. WSCG, V23, P65
NR 19
TC 1
Z9 1
U1 0
U2 0
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2019
VL 5
IS 2
BP 151
EP 160
DI 10.1007/s41095-019-0137-0
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UM
UT WOS:000648690800002
OA gold
DA 2024-07-18
ER

PT J
AU Lv, P
   Wei, H
   Gu, TX
   Zhang, YZ
   Jiang, XH
   Zhou, B
   Xu, ML
AF Lv, Pei
   Wei, Hui
   Gu, Tianxin
   Zhang, Yuzhen
   Jiang, Xiaoheng
   Zhou, Bing
   Xu, Mingliang
TI Trajectory distributions: A new description of movement for trajectory
   prediction
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE trajectory prediction; convolutional LSTM; trajectory distributions;
   social probability method
AB Trajectory prediction is a fundamental and challenging task for numerous applications, such as autonomous driving and intelligent robots. Current works typically treat pedestrian trajectories as a series of 2D point coordinates. However, in real scenarios, the trajectory often exhibits randomness, and has its own probability distribution. Inspired by this observation and other movement characteristics of pedestrians, we propose a simple and intuitive movement description called a trajectory distribution, which maps the coordinates of the pedestrian trajectory to a 2D Gaussian distribution in space. Based on this novel description, we develop a new trajectory prediction method, which we call the social probability method. The method combines trajectory distributions and powerful convolutional recurrent neural networks. Both the input and output of our method are trajectory distributions, which provide the recurrent neural network with sufficient spatial and random information about moving pedestrians. Furthermore, the social probability method extracts spatio-temporal features directly from the new movement description to generate robust and accurate predictions. Experiments on public benchmark datasets show the effectiveness of the proposed method.
C1 [Lv, Pei; Wei, Hui; Gu, Tianxin; Zhang, Yuzhen; Jiang, Xiaoheng; Zhou, Bing; Xu, Mingliang] Zhengzhou Univ, Sch Informat Engn, Zhengzhou 450001, Peoples R China.
C3 Zhengzhou University
RP Xu, ML (corresponding author), Zhengzhou Univ, Sch Informat Engn, Zhengzhou 450001, Peoples R China.
EM ielvpei@zzu.edu.cn; weihui@gs.zzu.edu.cn; txgu@gs.zzu.edu.cn;
   zyzzhang@gs.zzu.edu.cn; iexhjiang@zzu.edu.cn; iebzhou@zzu.edu.cn;
   iexumingliang@zzu.edu.cn
RI zhang, xueying/JMB-7808-2023; wu, meng/JPK-1930-2023; Zhou,
   heng/JCN-6493-2023; Jiang, Yuan/JED-3759-2023; Yu, ZH/KBC-6889-2024
FU National Natural Science Foundation of China [61772474, 61802351,
   61822701, 61872324]; Program for Science and Technology Innovation
   Talents in Universities of Henan Province [20HASTIT021]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant Nos. 61772474, 61802351, 61822701, and
   61872324, and in part by the Program for Science and Technology
   Innovation Talents in Universities of Henan Province under Grant No.
   20HASTIT021. We also thank the anonymous reviewers for their valuable
   comments and suggestions that helped improve the quality of this
   manuscript
CR Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110
   Bengio Y., 2014, TECHNICAL REPORT
   Chai Y., 2019, ARXIV191005449
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Chorowski J., 2014, NIPS 2014 WORKSH DEE
   Chung J., 2015, P 28 INT C NEURAL IN, P2980
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Feng ZH, 2018, PROC CVPR IEEE, P2235, DOI 10.1109/CVPR.2018.00238
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Graves A, 2014, PR MACH LEARN RES, V32, P1764
   Gupta A, 2018, PROC CVPR IEEE, P2255, DOI 10.1109/CVPR.2018.00240
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   Henaff M., 2015, ARXIV150605163
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang YF, 2019, IEEE I CONF COMP VIS, P6281, DOI 10.1109/ICCV.2019.00637
   Junwei Liang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10505, DOI 10.1109/CVPR42600.2020.01052
   Karpathy A, 2014, ADV NEUR IN, V27
   Kingma D. P., 2014, arXiv
   Lerner A, 2007, COMPUT GRAPH FORUM, V26, P655, DOI 10.1111/j.1467-8659.2007.01089.x
   Li YK, 2019, PROC CVPR IEEE, P294, DOI 10.1109/CVPR.2019.00038
   Liang JW, 2019, IEEE COMPUT SOC CONF, P2960, DOI 10.1109/CVPRW.2019.00358
   Makansi O, 2019, PROC CVPR IEEE, P7137, DOI 10.1109/CVPR.2019.00731
   Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641
   Pellegrini S, 2009, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2009.5459260
   Sadeghian A, 2019, PROC CVPR IEEE, P1349, DOI 10.1109/CVPR.2019.00144
   Shi XJ, 2015, ADV NEUR IN, V28
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Su H, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2772
   Sun JH, 2020, PROC CVPR IEEE, P657, DOI 10.1109/CVPR42600.2020.00074
   Tang QC, 2019, J ADV TRANSPORT, DOI 10.1155/2019/8392592
   Tang YC, 2019, ADV NEUR IN, V32
   Thiede LA, 2019, IEEE I CONF COMP VIS, P9953, DOI 10.1109/ICCV.2019.01005
   Vemula A, 2018, IEEE INT CONF ROBOT, P4601
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wong Saikeung, 2017, [Computational Visual Media, 计算可视媒体], V3, P243
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu ML, 2018, IEEE T CIRC SYST VID, V28, P2814, DOI 10.1109/TCSVT.2017.2731866
   Xu YY, 2018, PROC CVPR IEEE, P5275, DOI 10.1109/CVPR.2018.00553
   Xue H, 2018, IEEE WINT CONF APPL, P1186, DOI 10.1109/WACV.2018.00135
   Yang BL, 2019, NEUROCOMPUTING, V332, P320, DOI 10.1016/j.neucom.2018.12.016
   Yi S, 2016, LECT NOTES COMPUT SC, V9905, P263, DOI 10.1007/978-3-319-46448-0_16
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Yu B, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3634
   Zhang P, 2019, PROC CVPR IEEE, P12077, DOI 10.1109/CVPR.2019.01236
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zheng CAP, 2020, AAAI CONF ARTIF INTE, V34, P1234
NR 47
TC 5
Z9 5
U1 10
U2 66
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2022
VL 8
IS 2
BP 213
EP 224
DI 10.1007/s41095-021-0236-6
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XJ0YS
UT WOS:000726525400003
OA gold
DA 2024-07-18
ER

PT J
AU Li, JW
   Gao, W
   Wu, YH
   Liu, YD
   Shen, YF
AF Li, Jianwei
   Gao, Wei
   Wu, Yihong
   Liu, Yangdong
   Shen, Yanfei
TI High-quality indoor scene 3D reconstruction with RGB-D cameras: A brief
   review
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review
DE 3D reconstruction; image processing; camera pose estimation; surface
   fusion
ID DEPTH; SHAPE; REGISTRATION; INTEGRATION; COMPLETION; FRAGMENTS; IMAGE
AB High-quality 3D reconstruction is an important topic in computer graphics and computer vision with many applications, such as robotics and augmented reality. The advent of consumer RGB-D cameras has made a profound advance in indoor scene reconstruction. For the past few years, researchers have spent significant effort to develop algorithms to capture 3D models with RGB-D cameras. As depth images produced by consumer RGB-D cameras are noisy and incomplete when surfaces are shiny, bright, transparent, or far from the camera, obtaining high-quality 3D scene models is still a challenge for existing systems. We here review high-quality 3D indoor scene reconstruction methods using consumer RGB-D cameras. In this paper, we make comparisons and analyses from the following aspects: (i) depth processing methods in 3D reconstruction are reviewed in terms of enhancement and completion, (ii) ICP-based, feature-based, and hybrid methods of camera pose estimation methods are reviewed, and (iii) surface reconstruction methods are reviewed in terms of surface fusion, optimization, and completion. The performance of state-of-the-art methods is also compared and analyzed. This survey will be useful for researchers who want to follow best practices in designing new high-quality 3D reconstruction methods.
C1 [Li, Jianwei; Shen, Yanfei] Beijing Sports Univ, Sch Sports Engn, Beijing 100084, Peoples R China.
   [Gao, Wei; Wu, Yihong] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100080, Peoples R China.
   [Gao, Wei; Wu, Yihong] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Liu, Yangdong] Huawei Technol Co Ltd, Beijing 100085, Peoples R China.
C3 Beijing Sport University; Chinese Academy of Sciences; Institute of
   Automation, CAS; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS; Huawei Technologies
RP Shen, YF (corresponding author), Beijing Sports Univ, Sch Sports Engn, Beijing 100084, Peoples R China.
EM jianwei@bsu.edu.cn; wgao@nlpr.ia.ac.cn; yhwu@nlpr.ia.ac.cn;
   liuyangdong@huawei.com; syf@bsu.edu.cn
RI Li, Jing/GYU-5036-2022; li, jian/IAQ-2794-2023; li, jian/GSE-0245-2022;
   LI, JIAN/JAX-3092-2023; li, jy/HTT-1535-2023; Shen, Yanfei/S-6525-2016;
   LI, JIAN/GRY-2197-2022; Zhang, Jun/JPK-7723-2023; Liu,
   Jing/IQX-0664-2023; LI, Jing/HNB-5575-2023; Gao, Wei/A-4473-2015
OI Shen, Yanfei/0000-0002-6752-2829; Gao, Wei/0000-0003-2257-5684
FU National Key R&D Program of China [2018YFC2000600]; Open Projects
   Program of National Laboratory of Pattern Recognition [202100009];
   National Natural Science Foundation of China [72071018]; Fundamental
   Research Funds for Central Universities [2021TD006]
FX This work is supported by the National Key R&D Program of China under
   Grant No. 2018YFC2000600, the Open Projects Program of National
   Laboratory of Pattern Recognition under Grant No. 202100009, the
   National Natural Science Foundation of China under Grant No. 72071018,
   and the Fundamental Research Funds for Central Universities under Grant
   No. 2021TD006.
CR Andersen V., 2010, THEORY PRACTICE COMP, P39
   Aoki Y, 2019, PROC CVPR IEEE, P7156, DOI 10.1109/CVPR.2019.00733
   Ba Y., 2019, ARXIV PREPRINT ARXIV
   Berger M., 2014, EUROGRAPHICS 2014 ST, P161
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Cai YJ, 2021, PROC CVPR IEEE, P324, DOI 10.1109/CVPR46437.2021.00039
   Cao YP, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3182157
   Chang A, 2017, INT CONF 3D VISION, P667, DOI 10.1109/3DV.2017.00081
   Chen K., 2015, COMP VISUAL MEDIA, V1, P267, DOI [10.1007/s41095-015-0029-x, DOI 10.1007/S41095-015-0029-X]
   Chen K, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661239
   Chen QF, 2014, PROC CVPR IEEE, P3914, DOI 10.1109/CVPR.2014.500
   Chen Z, 2018, LECT NOTES COMPUT SC, V11208, P176, DOI 10.1007/978-3-030-01225-0_11
   Chen ZQ, 2021, PROC CVPR IEEE, P15735, DOI 10.1109/CVPR46437.2021.01548
   Cheng XJ, 2020, IEEE T PATTERN ANAL, V42, P2361, DOI 10.1109/TPAMI.2019.2947374
   Cheng XJ, 2018, LECT NOTES COMPUT SC, V11220, P108, DOI 10.1007/978-3-030-01270-0_7
   Cheng XJ, 2020, AAAI CONF ARTIF INTE, V34, P10615
   Choi C, 2013, IEEE INT C INT ROBOT, P1568, DOI 10.1109/IROS.2013.6696558
   Choi S, 2015, PROC CVPR IEEE, P5556, DOI 10.1109/CVPR.2015.7299195
   Cui ZP, 2017, PROC CVPR IEEE, P369, DOI 10.1109/CVPR.2017.47
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   Dai A, 2020, PROC CVPR IEEE, P846, DOI 10.1109/CVPR42600.2020.00093
   Dai A, 2018, PROC CVPR IEEE, P4578, DOI 10.1109/CVPR.2018.00481
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Dai A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3054739
   Davis J, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P428, DOI 10.1109/TDPVT.2002.1024098
   Delaunoy A, 2011, INT J COMPUT VISION, V95, P100, DOI 10.1007/s11263-010-0408-9
   Deschaintre V, 2021, PROC CVPR IEEE, P15562, DOI 10.1109/CVPR46437.2021.01531
   Dong W, 2018, LECT NOTES COMPUT SC, V11213, P714, DOI 10.1007/978-3-030-01240-3_43
   Dou MS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925969
   Dzitsiuk Maksym, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3976, DOI 10.1109/ICRA.2017.7989457
   Endres F, 2012, IEEE INT CONF ROBOT, P1691, DOI 10.1109/ICRA.2012.6225199
   Ferstl D, 2015, IEEE I CONF COMP VIS, P513, DOI 10.1109/ICCV.2015.66
   Ferstl D, 2013, IEEE I CONF COMP VIS, P993, DOI 10.1109/ICCV.2013.127
   Firman M, 2016, PROC CVPR IEEE, P5431, DOI 10.1109/CVPR.2016.586
   Ginzburg D., 2021, ARXIV PREPRINT ARXIV
   Golodetz S, 2018, IEEE T VIS COMPUT GR, V24, P2895, DOI 10.1109/TVCG.2018.2868533
   Haehnel D., 2003, IJCAI'03, P915
   Han XF, 2021, IEEE T PATTERN ANAL, V43, P1578, DOI 10.1109/TPAMI.2019.2954885
   Han Y, 2013, IEEE I CONF COMP VIS, P1617, DOI 10.1109/ICCV.2013.204
   Handa A, 2014, IEEE INT CONF ROBOT, P1524, DOI 10.1109/ICRA.2014.6907054
   Harary G, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2532548
   Hermosilla P, 2019, IEEE I CONF COMP VIS, P52, DOI 10.1109/ICCV.2019.00014
   Hou J, 2020, PROC CVPR IEEE, P2095, DOI 10.1109/CVPR42600.2020.00217
   Hou J, 2019, PROC CVPR IEEE, P4416, DOI 10.1109/CVPR.2019.00455
   Hua BS, 2016, INT CONF 3D VISION, P92, DOI 10.1109/3DV.2016.18
   Huang JW, 2020, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR42600.2020.00163
   Huang JW, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130824
   Hui TW, 2016, LECT NOTES COMPUT SC, V9907, P353, DOI 10.1007/978-3-319-46487-9_22
   Imran Saif, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12438, DOI 10.1109/CVPR.2019.01273
   Jeon J, 2016, VISUAL COMPUT, V32, P955, DOI 10.1007/s00371-016-1249-5
   Johnson AE, 1999, IMAGE VISION COMPUT, V17, P135, DOI 10.1016/S0262-8856(98)00117-6
   Kadambi A, 2015, IEEE I CONF COMP VIS, P3370, DOI 10.1109/ICCV.2015.385
   Kähler O, 2015, IEEE T VIS COMPUT GR, V21, P1241, DOI 10.1109/TVCG.2015.2459891
   Kehl W, 2017, PROC CVPR IEEE, P465, DOI 10.1109/CVPR.2017.57
   Keller M, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P1, DOI 10.1109/3DV.2013.9
   Kerl C, 2013, IEEE INT CONF ROBOT, P3748, DOI 10.1109/ICRA.2013.6631104
   Kiechle M, 2013, IEEE I CONF COMP VIS, P1545, DOI 10.1109/ICCV.2013.195
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276497, 10.1145/1239451.1239547]
   Lee BU, 2019, IEEE INT CONF ROBOT, P3281, DOI [10.1109/icra.2019.8794161, 10.1109/ICRA.2019.8794161]
   Lee JK, 2017, IEEE I CONF COMP VIS, P162, DOI 10.1109/ICCV.2017.27
   Li JW, 2018, INT J AUTOM COMPUT, V15, P443, DOI 10.1007/s11633-018-1114-2
   Li JW, 2019, IEEE ACCESS, V7, P19370, DOI 10.1109/ACCESS.2019.2895653
   Li J, 2020, IEEE ROBOT AUTOM LET, V5, P219, DOI 10.1109/LRA.2019.2953639
   Li W. B., 2018, ARXIV PREPRINT ARXIV
   Li ZQ, 2021, PROC CVPR IEEE, P6494, DOI 10.1109/CVPR46437.2021.00643
   Liu YZ, 2021, ANN BIOMED ENG, V49, P2791, DOI 10.1007/s10439-021-02821-z
   Liu ZN, 2021, IEEE T VIS COMPUT GR, V27, P83, DOI 10.1109/TVCG.2019.2937300
   Low K.-L., 2004, Tech. Rep. TR04-004
   Lu Y, 2015, IEEE I CONF COMP VIS, P3934, DOI 10.1109/ICCV.2015.448
   Ma FC, 2018, IEEE INT CONF ROBOT, P4796
   Ma LN, 2016, IEEE INT CONF ROBOT, P1285, DOI 10.1109/ICRA.2016.7487260
   Maier R, 2017, IEEE I CONF COMP VIS, P3133, DOI 10.1109/ICCV.2017.338
   Mandikal P, 2019, IEEE WINT CONF APPL, P1052, DOI 10.1109/WACV.2019.00117
   McCormac John, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4628, DOI 10.1109/ICRA.2017.7989538
   McCormac J, 2017, IEEE I CONF COMP VIS, P2697, DOI 10.1109/ICCV.2017.292
   Meerits Siim, 2018, Computational Visual Media, V4, P287, DOI 10.1007/s41095-018-0121-0
   Mihajlovic M, 2021, PROC CVPR IEEE, P14519, DOI 10.1109/CVPR46437.2021.01429
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Newcombe RA, 2015, PROC CVPR IEEE, P343, DOI 10.1109/CVPR.2015.7298631
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Nguyen CV, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P524, DOI 10.1109/3DIMPVT.2012.84
   Nicastro A, 2019, IEEE I CONF COMP VIS, P1517, DOI 10.1109/ICCV.2019.00160
   Niessner M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508374
   Or-El R, 2015, PROC CVPR IEEE, P5407, DOI 10.1109/CVPR.2015.7299179
   Orts-Escolano S, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P741, DOI 10.1145/2984511.2984517
   Palazzolo E, 2019, IEEE INT C INT ROBOT, P7855, DOI [10.1109/IROS40897.2019.8967590, 10.1109/iros40897.2019.8967590]
   Park J, 2014, IEEE T IMAGE PROCESS, V23, P5559, DOI 10.1109/TIP.2014.2361034
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Pfister H, 2000, COMP GRAPH, P335, DOI 10.1145/344779.344936
   Prisacariu V. A., 2017, ARXIV170800783
   Puri P, 2017, IEEE INT C INT ROBOT, P6506, DOI 10.1109/IROS.2017.8206559
   Riegler G, 2016, ARXIV160708569
   Riegler G, 2017, INT CONF 3D VISION, P57, DOI 10.1109/3DV.2017.00017
   Riegler G, 2016, LECT NOTES COMPUT SC, V9907, P268, DOI 10.1007/978-3-319-46487-9_17
   Rock J, 2015, PROC CVPR IEEE, P2484, DOI 10.1109/CVPR.2015.7298863
   Roldao L., 2021, INT J COMPUT VISION
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Saito S, 2020, PROC CVPR IEEE, P81, DOI 10.1109/CVPR42600.2020.00016
   Salas-Moreno RF, 2014, INT SYM MIX AUGMENT, P157, DOI 10.1109/ISMAR.2014.6948422
   Salas-Moreno RF, 2013, PROC CVPR IEEE, P1352, DOI 10.1109/CVPR.2013.178
   Sarbolandi H, 2015, COMPUT VIS IMAGE UND, V139, P1, DOI 10.1016/j.cviu.2015.05.006
   Sarlin PE, 2021, PROC CVPR IEEE, P3246, DOI 10.1109/CVPR46437.2021.00326
   Schertler N, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073635
   Schönberger JL, 2018, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR.2018.00721
   Scrafin J, 2015, IEEE INT C INT ROBOT, P742, DOI 10.1109/IROS.2015.7353455
   Segal AV, 2009, GEN ICP, DOI DOI 10.15607/RSS.2009.V.021
   Shao TJ, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366155
   Sharma A, 2016, LECT NOTES COMPUT SC, V9915, P236, DOI 10.1007/978-3-319-49409-8_20
   Shi XS, 2020, IEEE INT CONF ROBOT, P3139, DOI [10.1109/ICRA40945.2020.9196638, 10.1109/icra40945.2020.9196638]
   Shi YF, 2018, LECT NOTES COMPUT SC, V11212, P767, DOI 10.1007/978-3-030-01237-3_46
   Silberman N, 2014, LECT NOTES COMPUT SC, V8691, P488, DOI 10.1007/978-3-319-10578-9_32
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Sitzmann V, 2019, ADV NEUR IN, V32
   Slavcheva M, 2016, LECT NOTES COMPUT SC, V9905, P680, DOI 10.1007/978-3-319-46448-0_41
   Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28
   Steinbrücker F, 2014, IEEE INT CONF ROBOT, P2021, DOI 10.1109/ICRA.2014.6907127
   Steinbrücker F, 2013, IEEE I CONF COMP VIS, P3264, DOI 10.1109/ICCV.2013.405
   Sterzentsenko V, 2019, IEEE I CONF COMP VIS, P1242, DOI 10.1109/ICCV.2019.00133
   Stotko P., 2016, P CENTR EUR SEM COMP
   Straub Julian, 2019, ARXIV190605797
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   Sumner RW, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239531
   Sung M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818094
   Taguchi Y, 2013, IEEE INT CONF ROBOT, P5182, DOI 10.1109/ICRA.2013.6631318
   Tang ST, 2021, PROC CVPR IEEE, P1831, DOI 10.1109/CVPR46437.2021.00187
   Thomas D, 2017, COMPUT VIS IMAGE UND, V157, P103, DOI 10.1016/j.cviu.2016.11.008
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Tsai CY, 2019, PROC CVPR IEEE, P1545, DOI 10.1109/CVPR.2019.00164
   Wang PS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980232
   Wang YX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3792
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Wasenmüller O, 2016, IEEE WINT CONF APPL
   Wasenmüller O, 2017, LECT NOTES COMPUT SC, V10117, P34, DOI 10.1007/978-3-319-54427-4_3
   Whelan T., 2012, Proceedings of the RSS Workshop on RGB-D: Advanced Reasoning with Depth Cameras
   Whelan T, 2016, INT J ROBOT RES, V35, P1697, DOI 10.1177/0278364916669237
   Whelan T, 2015, INT J ROBOT RES, V34, P598, DOI 10.1177/0278364914551008
   Whelan T, 2013, IEEE INT CONF ROBOT, P5724, DOI 10.1109/ICRA.2013.6631400
   Wolff K, 2016, INT CONF 3D VISION, P118, DOI 10.1109/3DV.2016.20
   Wu CL, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661232
   Wu JJ, 2016, ADV NEUR IN, V29
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xiao JX, 2013, IEEE I CONF COMP VIS, P1625, DOI 10.1109/ICCV.2013.458
   Xu D, 2018, IEEE T PATTERN ANAL, V40, P423, DOI 10.1109/TPAMI.2017.2671458
   Xu K., 2016, P SPE IMPR OIL REC C, P4, DOI DOI 10.2118/179691-MS
   Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212
   Yu LF, 2013, PROC CVPR IEEE, P1415, DOI 10.1109/CVPR.2013.186
   Yunus Raza, 2021, 2021 IEEE International Conference on Robotics and Automation (ICRA), P6687, DOI 10.1109/ICRA48506.2021.9562030
   Zeng A, 2017, PROC CVPR IEEE, P199, DOI 10.1109/CVPR.2017.29
   Zhang JZ, 2021, PROC CVPR IEEE, P1768, DOI 10.1109/CVPR46437.2021.00181
   Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284
   Zhang YD, 2017, PROC CVPR IEEE, P5057, DOI 10.1109/CVPR.2017.537
   Zheng ZR, 2019, IEEE I CONF COMP VIS, P7738, DOI 10.1109/ICCV.2019.00783
   Zhou QY, 2015, PROC CVPR IEEE, P632, DOI 10.1109/CVPR.2015.7298662
   Zhou QY, 2013, IEEE I CONF COMP VIS, P473, DOI 10.1109/ICCV.2013.65
   Zhu LY, 2021, PROC CVPR IEEE, P4647, DOI 10.1109/CVPR46437.2021.00462
   Zollhöfer M, 2018, COMPUT GRAPH FORUM, V37, P625, DOI 10.1111/cgf.13386
   Zollhöfer M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766887
NR 158
TC 30
Z9 31
U1 24
U2 174
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2022
VL 8
IS 3
BP 369
EP 393
DI 10.1007/s41095-021-0250-8
EA MAR 2022
PG 25
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0Q6DG
UT WOS:000765189700001
OA gold
DA 2024-07-18
ER

PT J
AU Höller, B
   Mossel, A
   Kaufmann, H
AF Hoeller, Benjamin
   Mossel, Annette
   Kaufmann, Hannes
TI Automatic object annotation in streamed and remotely explored large 3D
   reconstructions
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE dense 3D reconstruction; object detection; CNN; distributed virtual
   reality
AB We introduce a novel framework for 3D scene reconstruction with simultaneous object annotation, using a pre-trained 2D convolutional neural network (CNN), incremental data streaming, and remote exploration, with a virtual reality setup. It enables versatile integration of any 2D box detection or segmentation network. We integrate new approaches to (i) asynchronously perform dense 3D-reconstruction and object annotation at interactive frame rates, (ii) efficiently optimize CNN results in terms of object prediction and spatial accuracy, and (iii) generate computationally-efficient colliders in large triangulated 3D-reconstructions at run-time for 3D scene interaction. Our method is novel in combining CNNs with long and varying inference time with live 3D-reconstruction from RGB-D camera input. We further propose a lightweight data structure to store the 3D-reconstruction data and object annotations to enable fast incremental data transmission for real-time exploration with a remote client, which has not been presented before. Our framework achieves update rates of 22 fps (SSD Mobile Net) and 19 fps (Mask RCNN) for indoor environments up to 800 m(3). We evaluated the accuracy of 3D-object detection. Our work provides a versatile foundation for semantic scene understanding of large streamed 3D-reconstructions, while being independent from the CNN's processing time. Source code is available for non-commercial use.
C1 [Hoeller, Benjamin; Mossel, Annette; Kaufmann, Hannes] Vienna Univ Technol, Inst Visual Comp & Human Ctr Technol, Favoritenstr 9-11-193-06, A-1040 Vienna, Austria.
C3 Technische Universitat Wien
RP Kaufmann, H (corresponding author), Vienna Univ Technol, Inst Visual Comp & Human Ctr Technol, Favoritenstr 9-11-193-06, A-1040 Vienna, Austria.
EM benjamin.hoeller@alumni.tuwien.ac.at; annette.mossel@tuwien.ac.at;
   hannes.kaufmann@tuwien.ac.at
FU Vienna University of Technology
FX The work was solely supported by Vienna University of Technology.
CR ACM, 2013, T GRAPHICS, V32, P4
   ACM, 2013, ACM T GRAPHIC, V32, P6
   ACM, 2009, ACM T COMPUT-HUM INT, V16, P1
   [Anonymous], 2008, 2008 IEEE Hot Chips 20 Symposium (HCS), DOI 10.1109/HOTCHIPS.2008.7476516
   [Anonymous], 1996, RFC 1951, DOI [10.17487/RFC1951, DOI 10.17487/RFC1951]
   [Anonymous], 2014, ARXIV14100925
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Golla T, 2015, IEEE INT C INT ROBOT, P5087, DOI 10.1109/IROS.2015.7354093
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   Hou J, 2019, PROC CVPR IEEE, P4416, DOI 10.1109/CVPR.2019.00455
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Kähler O, 2015, IEEE T VIS COMPUT GR, V21, P1241, DOI 10.1109/TVCG.2015.2459891
   Kammerl J, 2012, IEEE INT CONF ROBOT, P778, DOI 10.1109/ICRA.2012.6224647
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W., SSD MOBILENET V2 COC
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   McCormac John, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4628, DOI 10.1109/ICRA.2017.7989538
   Morell V, 2014, PATTERN RECOGN LETT, V50, P55, DOI 10.1016/j.patrec.2014.05.016
   Mossel A, 2016, ADJUNCT PROCEEDINGS OF THE 2016 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT), P43, DOI [10.1109/ISMAR-Adjunct.2016.28, 10.1109/ISMAR-Adjunct.2016.0035]
   Nakajima Y, 2019, IEEE ACCESS, V7, P3206, DOI 10.1109/ACCESS.2018.2887022
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Prisacariu V. A., 2017, ARXIV170800783
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rünz M, 2018, INT SYM MIX AUGMENT, P10, DOI 10.1109/ISMAR.2018.00024
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Sünderhauf N, 2017, IEEE INT C INT ROBOT, P5079, DOI 10.1109/IROS.2017.8206392
   Tateno K, 2015, IEEE INT C INT ROBOT, P4465, DOI 10.1109/IROS.2015.7354011
   Waleed A, 2017, GITHUB REPOSITORY
   Wang L, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19194092
   Whelan T, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI
NR 33
TC 2
Z9 2
U1 0
U2 16
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2021
VL 7
IS 1
BP 71
EP 86
DI 10.1007/s41095-020-0194-4
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FQ
UT WOS:000648692800004
OA gold
DA 2024-07-18
ER

PT J
AU Cui, MY
   Lu, SP
   Wang, M
   Yang, YL
   Lai, YK
   Rosin, PL
AF Cui, Meng-Yao
   Lu, Shao-Ping
   Wang, Miao
   Yang, Yong-Liang
   Lai, Yu-Kun
   Rosin, Paul L.
TI 3D computational modeling and perceptual analysis of kinetic depth
   effects
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE kinetic depth effects; 3D reconstruction; perceptual factor analysis
ID ROTATION; OCCLUSION; DIRECTION; COLOR
AB Humans have the ability to perceive kinetic depth effects, i.e., to perceived 3D shapes from 2D projections of rotating 3D objects. This process is based on a variety of visual cues such as lighting and shading effects. However, when such cues are weak or missing, perception can become faulty, as demonstrated by the famous silhouette illusion example of the spinning dancer. Inspired by this, we establish objective and subjective evaluation models of rotated 3D objects by taking their projected 2D images as input. We investigate five different cues: ambient luminance, shading, rotation speed, perspective, and color difference between the objects and background. In the objective evaluation model, we first apply 3D reconstruction algorithms to obtain an objective reconstruction quality metric, and then use quadratic stepwise regression analysis to determine weights of depth cues to represent the reconstruction quality. In the subjective evaluation model, we use a comprehensive user study to reveal correlations with reaction time and accuracy, rotation speed, and perspective. The two evaluation models are generally consistent, and potentially of benefit to inter-disciplinary research into visual perception and 3D reconstruction.
C1 [Cui, Meng-Yao; Lu, Shao-Ping] Nankai Univ, CS, TKLNDST, Tianjin, Peoples R China.
   [Wang, Miao] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Yang, Yong-Liang] Univ Bath, Dept Comp Sci, Bath, Avon, England.
   [Lai, Yu-Kun; Rosin, Paul L.] Cardiff Univ, Sch Comp Sci & Informat, Cardiff, S Glam, Wales.
C3 Nankai University; Beihang University; University of Bath; Cardiff
   University
RP Lu, SP (corresponding author), Nankai Univ, CS, TKLNDST, Tianjin, Peoples R China.
EM im-climmy@qq.com; slu@nankai.edu.cn
RI Lai, Yu-Kun/D-2343-2010
OI Rosin, Paul/0000-0002-4965-3884; CUI, Mengyao/0009-0001-0149-7923
FU Tianjin NSF [18JCYBJC41300, 18ZXZNGX00110]; National Natural Science
   Foundation of China [61972216]; Open Project Program of the State Key
   Laboratory of Virtual Reality Technology and Systems, Beihang University
   [VRLAB2019B04]; EPSRC [EP/M023281/1] Funding Source: UKRI
FX This work was supported by Tianjin NSF (Nos. 18JCYBJC41300 and
   18ZXZNGX00110), National Natural Science Foundation of China (No.
   61972216), and the Open Project Program of the State Key Laboratory of
   Virtual Reality Technology and Systems, Beihang University (No.
   VRLAB2019B04).
CR Agresti A., 2012, ANAL ORDINAL CATEGOR, P44
   ANDERSEN GJ, 1983, PERCEPT PSYCHOPHYS, V34, P356, DOI 10.3758/BF03203048
   [Anonymous], 2015, COMPUT VIS ME DIA
   [Anonymous], 2016, IEEE INT CON MULTI
   Bach M., 1922, PULFRICH EFFECT
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   BRAUNSTEIN ML, 1977, PERCEPT PSYCHOPHYS, V21, P553, DOI 10.3758/BF03198736
   BRAUNSTEIN ML, 1982, PERCEPT PSYCHOPHYS, V31, P261, DOI 10.3758/BF03202532
   Ceulemans B, 2018, IEEE T MULTIMEDIA, V20, P2235, DOI 10.1109/TMM.2018.2802646
   Chi MT, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360661
   Chu HK, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778788
   DOSHER BA, 1989, J EXP PSYCHOL HUMAN, V15, P816
   Elber G, 2011, COMPUT GRAPH-UK, V35, P632, DOI 10.1016/j.cag.2011.03.015
   Gibsen JJ, 1986, ECOLOGICAL APPROACH
   GREEN BF, 1961, J EXP PSYCHOL, V62, P272, DOI 10.1037/h0045622
   Guibal CRC, 2004, PSYCHOL RES-PSYCH FO, V69, P30, DOI 10.1007/s00426-003-0167-0
   HOWARD IP, 1990, PERCEPTION, V19, P523, DOI 10.1068/p190523
   Hu Shi-Min, 2011, P NPAR 11 ACM SIGGRA, P27, DOI DOI 10.1145/2024676.2024681
   Isono H., 1989, Systems and Computers in Japan, V19, P32, DOI 10.1002/scj.4690190904
   Kar A., 2017, P 31 C NEUR INF PROC
   Kayahara N., SPINNING DANCER
   Lai CFW, 2016, IEEE T VIS COMPUT GR, V22, P2275, DOI 10.1109/TVCG.2015.2507584
   Lai YK, 2006, VISUAL COMPUT, V22, P604, DOI 10.1007/s00371-006-0047-x
   Lu SP, 2015, IEEE T MULTIMEDIA, V17, P577, DOI 10.1109/TMM.2015.2412879
   Ma LQ, 2013, IEEE T VIS COMPUT GR, V19, P1808, DOI 10.1109/TVCG.2013.99
   Mitra NJ, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618509
   Moulon P., Openmvg. an open multiple view geometry library
   Moulon P, 2013, IEEE I CONF COMP VIS, P3248, DOI 10.1109/ICCV.2013.403
   Pashler H., 2002, STEVENS HDB EXPT PSY, V3rd
   PETERSIK JT, 1980, PERCEPTION, V9, P271, DOI 10.1068/p090271
   Pisanpeeti AP, 2017, INT CONF ACOUST SPEE, P1867, DOI 10.1109/ICASSP.2017.7952480
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Sarin SK, 2016, HEPATOL INT, V10, P1, DOI 10.1007/s12072-015-9675-4
   Tam GKL, 2013, IEEE T VIS COMPUT GR, V19, P1199, DOI 10.1109/TVCG.2012.310
   Tateno K, 2017, PROC CVPR IEEE, P6565, DOI 10.1109/CVPR.2017.695
   TODD JT, 1983, J EXP PSYCHOL HUMAN, V9, P583, DOI 10.1037/0096-1523.9.4.583
   Tong J, 2013, VISUAL COMPUT, V29, P535, DOI 10.1007/s00371-013-0799-z
   Troje NF, 2010, I-PERCEPTION, V1, P143, DOI 10.1068/i0408
   WALLACH H, 1953, J EXP PSYCHOL, V45, P205, DOI 10.1037/h0056880
   Wisessing P., 2019, P ACM S APPL PERC
   Wu TP, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731051
   Xu PF, 2019, COMPUT VIS MEDIA, V5, P45, DOI 10.1007/s41095-019-0130-7
   YOUNG LR, 1969, AEROSPACE MED, V40, P1076
NR 43
TC 2
Z9 2
U1 0
U2 4
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2020
VL 6
IS 3
BP 265
EP 277
DI 10.1007/s41095-020-0180-x
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA RZ6FH
UT WOS:000648691900003
OA gold, Green Accepted
DA 2024-07-18
ER

PT J
AU Wang, M
   Lyu, XQ
   Li, YJ
   Zhang, FL
AF Wang, Miao
   Lyu, Xu-Quan
   Li, Yi-Jun
   Zhang, Fang-Lue
TI VR content creation and exploration with deep learning: A survey
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review
DE virtual reality; deep learning; neural networks; 360 degrees image and
   video virtual content
ID 3D FACE RECONSTRUCTION; 360-DEGREES; GENERATION; PREDICTION
AB Virtual reality (VR) offers an artificial, computer generated simulation of a real life environment. It originated in the 1960s and has evolved to provide increasing immersion, interactivity, imagination, and intelligence. Because deep learning systems are able to represent and compose information at various levels in a deep hierarchical fashion, they can build very powerful models which leverage large quantities of visual media data. Intelligence of VR methods and applications has been significantly boosted by the recent developments in deep learning techniques. VR content creation and exploration relates to image and video analysis, synthesis and editing, so deep learning methods such as fully convolutional networks and general adversarial networks are widely employed, designed specifically to handle panoramic images and video and virtual 3D scenes. This article surveys recent research that uses such deep learning methods for VR content creation and exploration. It considers the problems involved, and discusses possible future directions in this active and emerging research area.
C1 [Wang, Miao; Lyu, Xu-Quan; Li, Yi-Jun] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Wang, Miao] Peng Cheng Lab, Shenzhen 518000, Peoples R China.
   [Zhang, Fang-Lue] Victoria Univ Wellington, Sch Engn & Comp Sci, Wellington, New Zealand.
C3 Beihang University; Peng Cheng Laboratory; Victoria University
   Wellington
RP Wang, M (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.; Wang, M (corresponding author), Peng Cheng Lab, Shenzhen 518000, Peoples R China.
EM miaow@buaa.edu.cn; aincrad@buaa.edu.cn; yaoling@buaa.edu.cn;
   fanglue.zhang@ecs.vuw.ac.nz
OI Li, Yi-Jun/0000-0003-2733-2913
FU National Natural Science Foundation of China [61902012, 61932003];
   Victoria Early-Career Research Excellence Award
FX The authors would like to thank the reviewers. This work was supported
   by the National Natural Science Foundation of China (Grant Nos.
   61902012, 61932003). Fang-Lue Zhang was supported by a Victoria
   Early-Career Research Excellence Award.
CR Altwaijry H, 2016, P BRIT MACH VIS C
   Tran AT, 2018, PROC CVPR IEEE, P3935, DOI 10.1109/CVPR.2018.00414
   [Anonymous], 2015, HANDS DEEP DEEP LEAR
   Balntas V, 2017, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2017.410
   Bertel T, 2019, IEEE T VIS COMPUT GR, V25, P1828, DOI 10.1109/TVCG.2019.2898799
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   Buehler C, 2001, COMP GRAPH, P425, DOI 10.1145/383259.383309
   Cao YP, 2018, LECT NOTES COMPUT SC, V11213, P626, DOI 10.1007/978-3-030-01240-3_38
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chalasani T, 2018, ADJUNCT PROCEEDINGS OF THE 2018 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P109, DOI 10.1109/ISMAR-Adjunct.2018.00045
   Chen Z, 2010, ARTECH HSE SIG PROC, P1
   Cheng HT, 2018, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2018.00154
   Cheng Y, 2020, COARSE TO FINE ADAPT
   Cheng YH, 2018, LECT NOTES COMPUT SC, V11218, P105, DOI 10.1007/978-3-030-01264-9_7
   Cheng Y, 2019, IEEE I CONF COMP VIS, P723, DOI 10.1109/ICCV.2019.00081
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Trinidad MC, 2019, IEEE I CONF COMP VIS, P4100, DOI 10.1109/ICCV.2019.00420
   Coors B, 2018, LECT NOTES COMPUT SC, V11213, P525, DOI 10.1007/978-3-030-01240-3_32
   Dai B, 2017, PROC CVPR IEEE, P3298, DOI 10.1109/CVPR.2017.352
   DeTone D, 2016, DEEP IMAGE HOMOGRAPH
   Dou PF, 2018, IMAGE VISION COMPUT, V80, P80, DOI 10.1016/j.imavis.2018.09.004
   Dou P, 2017, PROC CVPR IEEE, P1503, DOI 10.1109/CVPR.2017.164
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Flynn J, 2019, PROC CVPR IEEE, P2362, DOI 10.1109/CVPR.2019.00247
   Flynn J, 2016, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2016.595
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Gao L, 2019, IEEE T VISUALIZATION
   Ge LH, 2019, PROC CVPR IEEE, P10825, DOI 10.1109/CVPR.2019.01109
   Gecer B, 2019, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2019.00125
   Goesele M, 2007, IEEE I CONF COMP VIS, P825, DOI 10.1109/iccv.2007.4408933
   He Kaiming, 2017, P IEEE INT C COMPUTE
   Hu HN, 2017, PROC CVPR IEEE, P1396, DOI 10.1109/CVPR.2017.153
   Huang J, 2006, ACM T GRAPHIC, V25, P1126, DOI 10.1145/1141911.1142003
   Huang JW, 2017, P IEEE VIRT REAL ANN, P37, DOI 10.1109/VR.2017.7892229
   Huang Z, 2018, LECT NOTES COMPUT SC, V11220, P351, DOI 10.1007/978-3-030-01270-0_21
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jackson AS, 2017, IEEE I CONF COMP VIS, P1031, DOI 10.1109/ICCV.2017.117
   Jancosek M., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3121, DOI 10.1109/CVPR.2011.5995693
   Jeong D, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P827, DOI [10.1109/VR.2019.8798334, 10.1109/vr.2019.8798334]
   Ji MQ, 2017, IEEE I CONF COMP VIS, P2326, DOI 10.1109/ICCV.2017.253
   Jin S, 2019, PROC CVPR IEEE, P5657, DOI 10.1109/CVPR.2019.00581
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Jung R, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1, DOI [10.1109/VR.2019.8798326, 10.1109/vr.2019.8798326]
   Kim H, 2018, PROC CVPR IEEE, P4625, DOI 10.1109/CVPR.2018.00486
   Kim J, 2019, IEEE I CONF COMP VIS, P10580
   Kolasinski EM, 1995, TECHNICAL REPORT
   Lai W-S, 2019, P BRIT MACH VIS C
   Lai WS, 2018, IEEE T VIS COMPUT GR, V24, P2610, DOI 10.1109/TVCG.2017.2750671
   Lee S, 2018, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2018.00153
   Lee TM, 2019, IEEE T VIS COMPUT GR, V25, P1919, DOI 10.1109/TVCG.2019.2899186
   Lee Y, 2019, PROC CVPR IEEE, P9173, DOI 10.1109/CVPR.2019.00940
   Levoy M., 1990, Computer Graphics, V24, P217, DOI 10.1145/91394.91449
   Li J, 2020, IEEE J-STSP, V14, P38, DOI 10.1109/JSTSP.2019.2957982
   Lin KM, 2016, COMPUT GRAPH FORUM, V35, P479, DOI 10.1111/cgf.12848
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu F, 2016, IEEE T MULTIMEDIA, V18, P1772, DOI 10.1109/TMM.2016.2576284
   Luan FJ, 2017, PROC CVPR IEEE, P6997, DOI 10.1109/CVPR.2017.740
   Lyu W, 2019, VIRTUAL REALITY INTE, V1, P55, DOI [10.3724/SP.J.2096-5796.2018.0008, DOI 10.3724/SP.J.2096-5796.2018.0008]
   Meng HY, 2019, IEEE I CONF COMP VIS, P8499, DOI 10.1109/ICCV.2019.00859
   Monroy R, 2018, SIGNAL PROCESS-IMAGE, V69, P26, DOI 10.1016/j.image.2018.05.005
   Nah Jae Ho, 2017, [Computational Visual Media, 计算可视媒体], V3, P349
   Nakano K, 2019, INT SYM MIX AUGMENT, P212, DOI 10.1109/ISMAR.2019.000-1
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Nirkin Y, 2019, IEEE I CONF COMP VIS, P7183, DOI 10.1109/ICCV.2019.00728
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Pavllo D, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P651, DOI 10.1109/VR.2018.8446173
   Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Revaud J, 2016, INT J COMPUT VISION, V120, P300, DOI 10.1007/s11263-016-0908-3
   Rhee T, 2017, IEEE T VIS COMPUT GR, V23, P1302, DOI 10.1109/TVCG.2017.2657178
   Richardson E, 2017, PROC CVPR IEEE, P5553, DOI 10.1109/CVPR.2017.589
   Ritchie D, 2019, PROC CVPR IEEE, P6175, DOI 10.1109/CVPR.2019.00634
   Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239
   Serrano A, 2019, IEEE T VIS COMPUT GR, V25, P1817, DOI 10.1109/TVCG.2019.2898757
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Soccini AM, 2017, P IEEE VIRT REAL ANN, P413, DOI 10.1109/VR.2017.7892352
   Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28
   Su YC, 2017, ADV NEUR IN, V30
   Su YC, 2019, PROC CVPR IEEE, P9434, DOI 10.1109/CVPR.2019.00967
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Sun C, 2019, PROC CVPR IEEE, P1047, DOI 10.1109/CVPR.2019.00114
   Szeliski R, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000009
   Tan Q, 2018, P 32 AAAI C ART INT
   Tewari A, 2017, IEEE INT CONF COMP V, P1274, DOI 10.1109/ICCVW.2017.153
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Tome D, 2017, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR.2017.603
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Trân AT, 2017, PROC CVPR IEEE, P1493, DOI 10.1109/CVPR.2017.163
   Tu ZG, 2019, SIGNAL PROCESS-IMAGE, V72, P9, DOI 10.1016/j.image.2018.12.002
   Nguyen T, 2018, IEEE ROBOT AUTOM LET, V3, P2346, DOI 10.1109/LRA.2018.2809549
   Ummenhofer B, 2015, IEEE I CONF COMP VIS, P1341, DOI 10.1109/ICCV.2015.158
   Wandt B, 2019, PROC CVPR IEEE, P7774, DOI 10.1109/CVPR.2019.00797
   Wang M., 2016, COMPUT VIS MEDIA, V1, P3, DOI DOI 10.1007/s41095-016-0037-5
   Wang M, 2019, ADJUNCT PROCEEDINGS OF THE 2019 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT 2019), P251, DOI 10.1109/ISMAR-Adjunct.2019.00-36
   Wang M, 2019, PROC CVPR IEEE, P1495, DOI 10.1109/CVPR.2019.00159
   Wang M, 2018, IEEE T IMAGE PROCESS, V27, P5854, DOI 10.1109/TIP.2018.2859628
   Wang YY, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1874, DOI [10.1109/VR.2019.8798213, 10.1109/vr.2019.8798213]
   Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175
   Wu FZ, 2019, PROC CVPR IEEE, P959, DOI 10.1109/CVPR.2019.00105
   Wu WN, 2018, LECT NOTES COMPUT SC, V11205, P622, DOI 10.1007/978-3-030-01246-5_37
   Wu X, 2020, IEEE T IMAGE PROCESS, V29, P2344, DOI 10.1109/TIP.2019.2945866
   Wu X, 2017, TSINGHUA SCI TECHNOL, V22, P660, DOI 10.23919/TST.2017.8195348
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Xi WJ, 2019, COMPUT VIS MEDIA, V5, P337, DOI 10.1007/s41095-019-0159-7
   Xiao JX, 2012, PROC CVPR IEEE, P2695, DOI 10.1109/CVPR.2012.6247991
   Xiong YY, 2019, PROC CVPR IEEE, P7735, DOI 10.1109/CVPR.2019.00793
   Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330
   Xu YY, 2018, PROC CVPR IEEE, P5333, DOI 10.1109/CVPR.2018.00559
   Yang ST, 2019, PROC CVPR IEEE, P3358, DOI 10.1109/CVPR.2019.00348
   Yang WY, 2018, INT C PATT RECOG, P2190, DOI 10.1109/ICPR.2018.8546070
   Ye N., 2019, DEEPMESHFLOW CONTENT
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Yu Y, 2018, P 32 AAAI C ART INT
   Zhang J, 2019, CONTENT AWARE UNSUPE
   Zhang J, 2019, VISUAL COMPUT, V35, P1181, DOI 10.1007/s00371-019-01667-w
   Zhang Y, 2020, IEEE T VISUALIZATION
   Zhang Y, 2019, VISUAL COMPUT, V35, P823, DOI 10.1007/s00371-019-01694-7
   Zhang ZH, 2018, LECT NOTES COMPUT SC, V11211, P504, DOI 10.1007/978-3-030-01234-2_30
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao X, 2019, IEEE T VISUALIZATION
   Zheng ZR, 2019, IEEE I CONF COMP VIS, P7738, DOI 10.1109/ICCV.2019.00783
   Zhou GB, 2017, 2017 INTERNATIONAL CONFERENCE ON COMPUTER SYSTEMS, ELECTRONICS AND CONTROL (ICCSEC), P633, DOI 10.1109/ICCSEC.2017.8446713
   Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18
   Zhou X., 2016, MODEL BASED DEEP HAN
   Zhu J.-Y., 2017, IEEE I CONF COMP VIS, P2223, DOI DOI 10.1109/ICCV.2017.244
   Zhu Z, 2018, IEEE T IMAGE PROCESS, V27, P2952, DOI 10.1109/TIP.2018.2808766
   Zhuang BH, 2017, PROC CVPR IEEE, P2915, DOI 10.1109/CVPR.2017.311
   Zollhöfer M, 2018, COMPUT GRAPH FORUM, V37, P523, DOI 10.1111/cgf.13382
   Zou CH, 2018, PROC CVPR IEEE, P2051, DOI 10.1109/CVPR.2018.00219
NR 132
TC 45
Z9 50
U1 16
U2 55
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2020
VL 6
IS 1
BP 3
EP 28
DI 10.1007/s41095-020-0162-z
PG 26
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FA
UT WOS:000648691200002
OA gold
DA 2024-07-18
ER

PT J
AU Yamaguchi, T
   Yatagawa, T
   Tokuyoshi, Y
   Morishima, S
AF Yamaguchi, Tomoya
   Yatagawa, Tatsuya
   Tokuyoshi, Yusuke
   Morishima, Shigeo
TI Real-time rendering of layered materials with anisotropic normal
   distributions
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE layered materials; microfacet BSDF; reflection modeling; real-time
   rendering
AB This paper proposes a lightweight bidirectional scattering distribution function (BSDF) model for layered materials with anisotropic reflection and refraction properties. In our method, each layer of the materials can be described by a microfacet BSDF using an anisotropic normal distribution function (NDF). Furthermore, the NDFs of layers can be defined on tangent vector fields, which differ from layer to layer. Our method is based on a previous study in which isotropic BSDFs are approximated by projecting them onto base planes. However, the adequateness of this previous work has not been well investigated for anisotropic BSDFs. In this paper, we demonstrate that the projection is also applicable to anisotropic BSDFs and that the BSDFs are approximated by elliptical distributions using covariance matrices.
C1 [Yamaguchi, Tomoya; Morishima, Shigeo] Waseda Univ, Tokyo 1558885, Japan.
   [Yatagawa, Tatsuya] Univ Tokyo, Tokyo 1538656, Japan.
   [Tokuyoshi, Yusuke] Intel Corp, Santa Clara, CA 95054 USA.
C3 Waseda University; University of Tokyo; Intel Corporation
RP Yamaguchi, T (corresponding author), Waseda Univ, Tokyo 1558885, Japan.
EM tomoya.tomoya@akane.waseda.jp; tatsy@den.t.u-tokyo.ac.jp;
   yusuke.tokuyoshi@intel.com; shigeo@waseda.jp
RI Yatagawa, Tatsuya/HKV-3976-2023
OI Yatagawa, Tatsuya/0000-0003-4653-2435; Morishima,
   Shigeo/0000-0001-8859-6539
FU JST ACCEL [JPMJAC1602]; JSPS KAKENHI [JP17H06101, 18K18075, JP19H01129];
   Grants-in-Aid for Scientific Research [18K18075] Funding Source: KAKEN
FX This research was supported by the JST ACCEL (JPMJAC1602) and JSPS
   KAKENHI (JP17H06101, 18K18075, and JP19H01129).
CR [Anonymous], 2019, ACM T GRAPHICS, V37, P6
   [Anonymous], 2016, ACM T GRAPHICS, V35, P4
   [Anonymous], 2018, ACM T GRAPHICS, V37, P4
   [Anonymous], 2014, ACM T GRAPHICS, V33, P4
   [Anonymous], 2013, ACM T GRAPHICS, V32, P6
   Elek O, 2010, P 14 CENTR EUR SEM C
   Guo J, 2017, IEEE T VIS COMPUT GR, V23, P2108, DOI 10.1109/TVCG.2016.2617872
   Heitz E., 2018, J. Comput. Graph. Techn., V7, P1
   Henyey LG, 1941, ASTROPHYS J, V93, P70, DOI 10.1086/144246
   Marmoset LLC, 2019, MARM TOOLB, V3
   Stam J, 2001, SPRING EUROGRAP, P39
   Unity Technologies, 2019, UN SCRIP REND PIP
   Walter B., 2007, EUROGRAPHICS C RENDE
   Weidlich A, 2007, GRAPHITE 2007: 5TH INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES IN AUSTRALASIA AND SOUTHERN ASIA, PROCEEDINGS, P171
NR 14
TC 3
Z9 4
U1 0
U2 4
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2020
VL 6
IS 1
BP 29
EP 36
DI 10.1007/s41095-019-0154-z
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FA
UT WOS:000648691200003
OA gold
DA 2024-07-18
ER

PT J
AU Danon, D
   Averbuch-Elor, H
   Fried, O
   Cohen-Or, D
AF Danon, Dov
   Averbuch-Elor, Hadar
   Fried, Ohad
   Cohen-Or, Daniel
TI Unsupervised natural image patch learning
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE unsupervised learning; metric learning
ID TEXTURE CLASSIFICATION
AB A metric for natural image patches is an important tool for analyzing images. An efficient means of learning one is to train a deep network to map an image patch to a vector space, in which the Euclidean distance reflects patch similarity. Previous attempts learned such an embedding in a supervised manner, requiring the availability of many annotated images. In this paper, we present an unsupervised embedding of natural image patches, avoiding the need for annotated images. The key idea is that the similarity of two patches can be learned from the prevalence of their spatial proximity in natural images. Clearly, relying on this simple principle, many spatially nearby pairs are outliers. However, as we show, these outliers do not harm the convergence of the metric learning. We show that our unsupervised embedding approach is more effective than a supervised one or one that uses deep patch representations. Moreover, we show that it naturally lends itself to an efficient self-supervised domain adaptation technique onto a target domain that contains a common foreground object.
C1 [Danon, Dov; Averbuch-Elor, Hadar; Cohen-Or, Daniel] Tel Aviv Univ, IL-6997801 Tel Aviv, Israel.
   [Fried, Ohad] Stanford Univ, Stanford, CA 94305 USA.
C3 Tel Aviv University; Stanford University
RP Danon, D (corresponding author), Tel Aviv Univ, IL-6997801 Tel Aviv, Israel.
EM dov84d@gmail.com; hadar.a.elor@gmail.com; ohad@stanford.edu;
   dcor@tau.ac.il
RI su, Jian-Ping/ABC-5407-2021; Fried, Ohad/R-2688-2019; Averbuch-Elor,
   Hadar/AAO-4246-2021
OI Fried, Ohad/0000-0001-7109-4006; 
CR [Anonymous], 2015, ACM T GRAPHICS, V34, P4
   [Anonymous], 2015, ARXIV PREPRINT ARXIV
   [Anonymous], 2013, ACM T GRAPHICS, V32, P6
   [Anonymous], 2014, ACM T GRAPHICS, V33, P6
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bagon S., 2006, MATLAB WRAPPER GRAPH
   Barnes Connelly, 2017, [Computational Visual Media, 计算可视媒体], V3, P3
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Chen M., 2012, P 29 INT C MACHINE L, P1627
   CIMPOI M, 2015, PROC CVPR IEEE, P3828, DOI DOI 10.1109/CVPR.2015.7299007
   De Bonet JS, 1998, PROC CVPR IEEE, P641, DOI 10.1109/CVPR.1998.698672
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Fried O, 2017, COMPUT GRAPH FORUM, V36, P183, DOI 10.1111/cgf.13284
   Gabor D., 1946, Journal of the Institution of Electrical Engineers-part III: radio and communication engineering, V93, P429, DOI [DOI 10.1049/JI-3-2.1946.0074, 10.1049/ji-3-2.1946.0074]
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Heeger DJ, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC648
   JULESZ B, 1981, NATURE, V290, P91, DOI 10.1038/290091a0
   Kodirov E, 2015, IEEE I CONF COMP VIS, P2452, DOI 10.1109/ICCV.2015.282
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Matviychuk Y, 2015, P BRIDGES, P339
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P53, DOI 10.1109/MSP.2014.2347059
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Randen T, 1999, IEEE T PATTERN ANAL, V21, P291, DOI 10.1109/34.761261
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Rubinstein M, 2013, PROC CVPR IEEE, P1939, DOI 10.1109/CVPR.2013.253
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shi K, 2007, PROC CVPR IEEE, P87
   Simo-Serra E, 2015, IEEE I CONF COMP VIS, P118, DOI 10.1109/ICCV.2015.22
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   VARMA M, 2003, PROC CVPR IEEE, P691, DOI DOI 10.1109/CVPR.2003.1211534
   Wang XL, 2015, IEEE I CONF COMP VIS, P2794, DOI 10.1109/ICCV.2015.320
   Zbontar J, 2016, J MACH LEARN RES, V17
NR 33
TC 11
Z9 14
U1 4
U2 11
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2019
VL 5
IS 3
BP 229
EP 237
DI 10.1007/s41095-019-0147-y
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UN
UT WOS:000648690900001
OA gold
DA 2024-07-18
ER

PT J
AU Duan, Y
   Yi, RJ
   Gao, YM
   Xu, K
   Zhu, CY
AF Duan, Yao
   Yi, Renjiao
   Gao, Yuanming
   Xu, Kai
   Zhu, Chenyang
TI EFECL: Feature encoding enhancement with contrastive learning for indoor
   3D object detection
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE indoor scene; object detection; contrastive learning; feature
   enhancement
AB Good proposal initials are critical for 3D object detection applications. However, due to the significant geometry variation of indoor scenes, incomplete and noisy proposals are inevitable in most cases. Mining feature information among these "bad" proposals may mislead the detection. Contrastive learning provides a feasible way for representing proposals, which can align complete and incomplete/noisy proposals in feature space. The aligned feature space can help us build robust 3D representation even if bad proposals are given. Therefore, we devise a new contrast learning framework for indoor 3D object detection, called EFECL, that learns robust 3D representations by contrastive learning of proposals on two different levels. Specifically, we optimize both instance-level and category-level contrasts to align features by capturing instance-specific characteristics and semantic-aware common patterns. Furthermore, we propose an enhanced feature aggregation module to extract more general and informative features for contrastive learning. Evaluations on ScanNet V2 and SUN RGB-D benchmarks demonstrate the generalizability and effectiveness of our method, and our method can achieve 12.3% and 7.3% improvements on both datasets over the benchmark alternatives. The code andmodels are publicly available at https://github.com/YaraDuan/EFECL.
C1 [Duan, Yao; Yi, Renjiao; Gao, Yuanming; Xu, Kai; Zhu, Chenyang] Natl Univ Def Technol, Sch Comp, Changsha 410000, Peoples R China.
C3 National University of Defense Technology - China
RP Zhu, CY (corresponding author), Natl Univ Def Technol, Sch Comp, Changsha 410000, Peoples R China.
EM duanyao16@nudt.edu.cn; yirenjiao@nudt.edu.cn; 356232063@qq.com;
   kevin.kai.xu@gmail.com; zhuchenyang07@nudt.edu.cn
OI Yi, Renjiao/0000-0002-6057-1089
FU National Key R&D Program of China [2018AAA0102200]; National Natural
   Science Foundation of China [62002375, 62002376, 62132021]; Natural
   Science Foundation of Hunan Province of China [2021RC3071, 2022RC1104,
   2021JJ40696]; NUDT Research Grants [ZK22-52]
FX We thank Yuqing Lan for visualizing the results. This work is supported
   in part by the National Key R&D Program of China (2018AAA0102200),
   National Natural Science Foundation of China (62002375, 62002376,
   62132021), Natural Science Foundation of Hunan Province of China
   (2021RC3071, 2022RC1104, 2021JJ40696), and NUDT Research Grants
   (ZK22-52).
CR Afham M, 2022, PROC CVPR IEEE, P9892, DOI 10.1109/CVPR52688.2022.00967
   Bai YT, 2022, PROC CVPR IEEE, P16040, DOI 10.1109/CVPR52688.2022.01559
   Chen JT, 2020, PROC CVPR IEEE, P389, DOI 10.1109/CVPR42600.2020.00047
   Chen SX, 2021, PROC CVPR IEEE, P9791, DOI 10.1109/CVPR46437.2021.00967
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Cheng BW, 2021, PROC CVPR IEEE, P8959, DOI 10.1109/CVPR46437.2021.00885
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Duan Y, 2022, PROC CVPR IEEE, P16959, DOI 10.1109/CVPR52688.2022.01647
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   Hénaff OJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10066, DOI 10.1109/ICCV48922.2021.00993
   Hjelm R. D., 2018, ARXIV
   Hou J, 2021, PROC CVPR IEEE, P15582, DOI 10.1109/CVPR46437.2021.01533
   Kingma D. P., 2014, arXiv
   Lan YQ, 2022, COMPUT VIS MEDIA, V8, P395, DOI 10.1007/s41095-021-0252-6
   Lan YQ, 2021, COMPUT GRAPH-UK, V98, P58, DOI 10.1016/j.cag.2021.04.033
   Liu KC, 2023, PROC CVPR IEEE, P9476, DOI 10.1109/CVPR52729.2023.00914
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2929, DOI 10.1109/ICCV48922.2021.00294
   MMDetection3D Contributors, 2020, OPENMMLABS NEXT GEN
   Purushwalkam S., 2020, ADV NEURAL INFORM PR, P3407, DOI 10.5555/3495724.3496011
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qi CR, 2020, PROC CVPR IEEE, P4403, DOI 10.1109/CVPR42600.2020.00446
   Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937
   Rao YM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3263, DOI 10.1109/ICCV48922.2021.00327
   Saining Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P574, DOI 10.1007/978-3-030-58580-8_34
   Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054
   Shi SS, 2021, IEEE T PATTERN ANAL, V43, P2647, DOI 10.1109/TPAMI.2020.2977026
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Sun B, 2021, PROC CVPR IEEE, P7348, DOI 10.1109/CVPR46437.2021.00727
   Sun P, 2020, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR42600.2020.00252
   van den Oord A., 2018, ARXIV
   Van Gansbeke Wouter, 2021, NEURIPS, V34, P16238
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HY, 2022, PROC CVPR IEEE, P1100, DOI 10.1109/CVPR52688.2022.00118
   Wang WG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7283, DOI 10.1109/ICCV48922.2021.00721
   Wei FY, 2021, Advances in Neural Information Processing Systems, V34
   Xiao TT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10519, DOI 10.1109/ICCV48922.2021.01037
   Xie EZ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8372, DOI 10.1109/ICCV48922.2021.00828
   Xie Q, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3692, DOI 10.1109/ICCV48922.2021.00369
   Xie Qizhe, 2020, SELF TRAINING NOISY, DOI 10.1109/cvpr42600.2020.01046
   Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337
   Yang B, 2018, PROC CVPR IEEE, P7652, DOI 10.1109/CVPR.2018.00798
   Yang CY, 2021, PROC CVPR IEEE, P3986, DOI 10.1109/CVPR46437.2021.00398
   Yang H, 2022, ARXIV
   Yin JB, 2022, LECT NOTES COMPUT SC, V13698, P727, DOI 10.1007/978-3-031-19839-7_42
   Yin JB, 2022, LECT NOTES COMPUT SC, V13699, P17, DOI 10.1007/978-3-031-19842-7_2
   Yin TW, 2021, PROC CVPR IEEE, P11779, DOI 10.1109/CVPR46437.2021.01161
   Zhan FN, 2022, PROC CVPR IEEE, P10653, DOI 10.1109/CVPR52688.2022.01040
   Zaiwei Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P311, DOI [10.1061/9780784482933.027, 10.1007/978-3-030-58610-2_19]
   Zhang ZW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10232, DOI 10.1109/ICCV48922.2021.01009
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
NR 53
TC 0
Z9 0
U1 3
U2 14
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2023
VL 9
IS 4
BP 875
EP 892
DI 10.1007/s41095-023-0366-0
EA AUG 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FF9B9
UT WOS:001042593800001
OA gold
DA 2024-07-18
ER

PT J
AU Guo, MH
   Lu, CZ
   Liu, ZN
   Cheng, MM
   Hu, SM
AF Guo, Meng-Hao
   Lu, Cheng-Ze
   Liu, Zheng-Ning
   Cheng, Ming-Ming
   Hu, Shi-Min
TI Visual attention network
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE vision backbone; deep learning; ConvNets; attention
ID REPRESENTATION
AB While originally designed for natural language processing tasks, the self-attention mechanism has recently taken various computer vision areas by storm. However, the 2D nature of images brings three challenges for applying self-attention in computer vision: (1) treating images as 1D sequences neglects their 2D structures; (2) the quadratic complexity is too expensive for high-resolution images; (3) it only captures spatial adaptability but ignores channel adaptability. In this paper, we propose a novel linear attention named large kernel attention (LKA) to enable self-adaptive and long-range correlations in self-attention while avoiding its shortcomings. Furthermore, we present a neural network based on LKA, namely Visual Attention Network (VAN). While extremely simple, VAN achieves comparable results with similar size convolutional neural networks (CNNs) and vision transformers (ViTs) in various tasks, including image classification, object detection, semantic segmentation, panoptic segmentation, pose estimation, etc. For example, VAN-B6 achieves 87.8% accuracy on ImageNet benchmark, and sets new state-of-the-art performance (58.2 PQ) for panoptic segmentation. Besides, VAN-B2 surpasses Swin-T 4 mIoU (50.1 vs. 46.1) for semantic segmentation on ADE20K benchmark, 2.6 AP (48.8 vs. 46.2) for object detection on COCO dataset. It provides a novel method and a simple yet strong baseline for the community.
C1 [Guo, Meng-Hao; Hu, Shi-Min] Tsinghua Univ, Dept Comp Sci, Beijing, Peoples R China.
   [Lu, Cheng-Ze; Cheng, Ming-Ming] Nankai Univ, Tianjin, Peoples R China.
   [Liu, Zheng-Ning] Fitten Tech, Beijing, Peoples R China.
C3 Tsinghua University; Nankai University
RP Hu, SM (corresponding author), Tsinghua Univ, Dept Comp Sci, Beijing, Peoples R China.
EM mh20@mails.tsinghua.edu.cn; czlu919@outlook.com; lhengning@gmail.com;
   cmm@nankai.edu.cn; shimin@tsinghua.edu.cn
RI Hu, Shi-Min/AAW-1952-2020; Cheng, Ming-Ming/A-2527-2009
OI Cheng, Ming-Ming/0000-0001-5550-8758
FU National Key Ramp;D Program of China [2021ZD0112902]; National Natural
   Science Foundation of China [62220106003]; Tsinghua-Tencent Joint
   Laboratory for Internet Innovation Technology
FX This paper was supported by National Key R&D Program of China (Project
   No. 2021ZD0112902), the National Natural Science Foundation of China
   (Project No. 62220106003), and Tsinghua-Tencent Joint Laboratory for
   Internet Innovation Technology.
CR [Anonymous], 2020, OPENMMLAB PRETRAININ
   [Anonymous], 2020, OPENMMLAB POSE ESTIM
   Bai S., 2018, EMPIRICAL EVALUATION
   Bao H. B., 2022, P INT C LEARNING REP
   Bello I., 2021, P 35 C NEUR INF PROC
   Bello I., 2021, P INT C LEARNING REP
   Bello I, 2019, IEEE I CONF COMP VIS, P3285, DOI 10.1109/ICCV.2019.00338
   Branson S., 2010, 2010001 CAL I TECHN
   Brown T., 2020, Advances in Neural Information Processing Systems, V33, P1877, DOI [DOI 10.48550/ARXIV.2005.14165, DOI 10.5555/3495724.3495883]
   Cai ZW, 2021, IEEE T PATTERN ANAL, V43, P1483, DOI 10.1109/TPAMI.2019.2956516
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Chen Liang-Chieh, 2014, Comput Sci, DOI DOI 10.48550/ARXIV.1412.7062
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen Q, 2022, PROC CVPR IEEE, P5239, DOI 10.1109/CVPR52688.2022.00518
   Cheng BW, 2022, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR52688.2022.00135
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai Z., 2021, P 35 C NEUR INF PROC
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J., 2018, BERT PRE TRAINING DE
   Ding XH, 2022, PROC CVPR IEEE, P11953, DOI 10.1109/CVPR52688.2022.01166
   Dosovitskiy Alexey, 2021, ICLR
   El-Nouby A, 2021, ADV NEUR IN
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Geng Z. Y., 2021, P INT C LEARNING REP
   Gottlieb JP, 1998, NATURE, V391, P481, DOI 10.1038/35135
   Guo JY, 2022, PROC CVPR IEEE, P12165, DOI 10.1109/CVPR52688.2022.01186
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P283, DOI 10.1007/s41095-021-0240-x
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Guo M, 2023, NAT PROD RES, V37, P1411, DOI 10.1080/14786419.2021.2011271
   Han K, 2021, ADV NEUR IN
   Han Q., 2021, ARXIV
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D., 2016, ARXIV
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Hu J, 2018, ADV NEUR IN, V31
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu SM, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-020-3097-4
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Kingma D. P., 2014, arXiv
   Kirillov A, 2019, PROC CVPR IEEE, P6392, DOI 10.1109/CVPR.2019.00656
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee Y, 2022, PROC CVPR IEEE, P7277, DOI 10.1109/CVPR52688.2022.00714
   Li X., 2020, ADV NEURAL INF PROCE, V33, P21002, DOI DOI 10.48550/ARXIV.2006.04388
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu H., 2021, P 35 C NEURAL INFORM
   Liu R., 2021, ARXIV
   Liu R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14020, DOI 10.1109/ICCV48922.2021.01378
   Liu RY, 2022, PATTERNS, V3, DOI 10.1016/j.patter.2022.100520
   Liu S., 2021, ARXIV
   [刘尚鹏 Liu Shangpeng], 2022, [高分子通报, Polymer Bulletin], P1
   Liu Z., 2022, ARXIV
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Loshchilov I., 2018, arXiv
   Loshchilov I, 2016, ARXIV
   Luo WJ, 2016, ADV NEUR IN, V29
   MMSegmentation, 2020, OPENMMLAB SEM SEGM T
   Mnih V, 2014, ADV NEUR IN, V27
   Müller R, 2019, ADV NEUR IN, V32
   Park J., 2018, ARXIV
   Paszke A, 2019, ADV NEUR IN, V32
   POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046
   Qin ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P763, DOI 10.1109/ICCV48922.2021.00082
   Ramachandran P, 2019, ADV NEUR IN, V32
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Rumelhart D.E., 2013, Learning internal representations by error propagation, P399, DOI [10.1016/b978-1-4832-1446-7.50035-2, 10.1016/B978-1-4832-1446-7.50035-2]
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Sermanet P., 2013, ARXIV
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srinivas A, 2021, PROC CVPR IEEE, P16514, DOI 10.1109/CVPR46437.2021.01625
   Sun PZ, 2021, PROC CVPR IEEE, P14449, DOI 10.1109/CVPR46437.2021.01422
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tan MX, 2021, PR MACH LEARN RES, V139, P7102
   TOLSTIKHIN I., 2021, P 35 C NEUR INF PROC
   Touvron H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P32, DOI 10.1109/ICCV48922.2021.00010
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Touvron Hugo, 2021, P INT C MACH LEARN I
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   TSOTSOS JK, 1995, ARTIF INTELL, V78, P507, DOI 10.1016/0004-3702(95)00025-9
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang JH, 2020, IEEE T CYBERNETICS, V50, P2971, DOI 10.1109/TCYB.2019.2891265
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang W., 2021, ARXIV
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wolfe JM, 2004, NAT REV NEUROSCI, V5, P495, DOI 10.1038/nrn1411
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Wu Y.-H., 2021, ARXIV
   Wu YH, 2022, IEEE T IMAGE PROCESS, V31, P3125, DOI 10.1109/TIP.2022.3164550
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Xiao TT, 2018, LECT NOTES COMPUT SC, V11209, P432, DOI 10.1007/978-3-030-01228-1_26
   Xie E.Z., 2021, P 35 C NEUR INF PROC
   Xie SN, 2018, PROC CVPR IEEE, P4606, DOI 10.1109/CVPR.2018.00484
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu WJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9961, DOI 10.1109/ICCV48922.2021.00983
   Xu Y., 2021, P 35 C NEUR INF PROC
   Xu YF, 2022, COMPUT VIS MEDIA, V8, P33, DOI 10.1007/s41095-021-0247-3
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang J., 2022, P 36 C NEURAL INFORM
   Yang J., 2021, arXiv
   Yu F., 2015, ARXIV
   Yu WH, 2022, PROC CVPR IEEE, P10809, DOI 10.1109/CVPR52688.2022.01055
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Yuan Y, 2018, OCNET OBJECT CONTEXT
   Yuhui Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P173, DOI 10.1007/978-3-030-58539-6_11
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang H., 2017, ARXIV
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang S., 2020, P IEEECVF C COMPUTER, P9759
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou BL, 2019, INT J COMPUT VISION, V127, P302, DOI 10.1007/s11263-018-1140-0
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 129
TC 78
Z9 82
U1 45
U2 61
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2023
VL 9
IS 4
BP 733
EP 752
DI 10.1007/s41095-023-0364-2
EA JUL 2023
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P1QC2
UT WOS:001040567600001
OA Green Submitted, gold
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Wang, ZJ
   Liu, YF
   Lu, F
AF Wang, Zongji
   Liu, Yunfei
   Lu, Feng
TI Discriminative feature encoding for intrinsic image decomposition
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE intrinsic image decomposition; deep learning; feature distribution; data
   refinement
ID RETINEX
AB Intrinsic image decomposition is an important and long-standing computer vision problem. Given an input image, recovering the physical scene properties is ill-posed. Several physically motivated priors have been used to restrict the solution space of the optimization problem for intrinsic image decomposition. This work takes advantage of deep learning, and shows that it can solve this challenging computer vision problem with high efficiency. The focus lies in the feature encoding phase to extract discriminative features for different intrinsic layers from an input image. To achieve this goal, we explore the distinctive characteristics of different intrinsic components in the high-dimensional feature embedding space. We define feature distribution divergence to efficiently separate the feature vectors of different intrinsic components. The feature distributions are also constrained to fit the real ones through a feature distribution consistency. In addition, a data refinement approach is provided to remove data inconsistency from the Sintel dataset, making it more suitable for intrinsic image decomposition. Our method is also extended to intrinsic video decomposition based on pixel-wise correspondences between adjacent frames. Experimental results indicate that our proposed network structure can outperform the existing state-of-the-art.
C1 [Wang, Zongji] Chinese Acad Sci, Aerosp Informat Res Inst, Key Lab Network Informat Syst Technol NIST, Beijing 100190, Peoples R China.
   [Liu, Yunfei; Lu, Feng] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Lu, Feng] Peng Cheng Lab, Shenzhen 518000, Peoples R China.
C3 Chinese Academy of Sciences; Aerospace Information Research Institute,
   CAS; Beihang University; Peng Cheng Laboratory
RP Lu, F (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.; Lu, F (corresponding author), Peng Cheng Lab, Shenzhen 518000, Peoples R China.
EM lufeng@buaa.edu.cn
FU Special Funds for Creative Research [2022C61540]; National Natural
   Science Foundation of China (NSFC) [61972012, 61732016]
FX Portions of this work were presented at the International Conference on
   Computer Vision Workshops in 2019 [18]. This work was supported by the
   Special Funds for Creative Research (Grant No. 2022C61540) and the
   National Natural Science Foundation of China (NSFC, Grant Nos. 61972012
   and 61732016).
CR Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712
   Barron JT, 2013, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2013.10
   BARRON JT, 2015, PROC CVPR IEEE, P4466, DOI DOI 10.1109/CVPR.2015.7299076
   Baslamisli AS, 2018, PROC CVPR IEEE, P6674, DOI 10.1109/CVPR.2018.00698
   Baslamisli AS, 2021, INT J COMPUT VISION, V129, P2445, DOI 10.1007/s11263-021-01477-5
   Baslamisli AS, 2021, COMPUT VIS IMAGE UND, V205, DOI 10.1016/j.cviu.2021.103183
   Bell Sean, 2014, ACM Transactions on Graphics, V33, DOI 10.1145/2601097.2601206
   Bi S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766946
   Bonneel N, 2017, COMPUT GRAPH FORUM, V36, P593, DOI 10.1111/cgf.13149
   Bousseau A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618476
   Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44
   Chang A. X., 2015, ARXIV
   Chen QF, 2013, IEEE I CONF COMP VIS, P241, DOI 10.1109/ICCV.2013.37
   Cheng Z, 2019, IEEE I CONF COMP VIS, P2521, DOI 10.1109/ICCV.2019.00261
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fan QN, 2018, PROC CVPR IEEE, P8944, DOI 10.1109/CVPR.2018.00932
   Fu G, 2021, PROC CVPR IEEE, P7748, DOI 10.1109/CVPR46437.2021.00766
   Fu G, 2019, IEEE INT CON MULTI, P175, DOI 10.1109/ICME.2019.00038
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Garces E, 2012, COMPUT GRAPH FORUM, V31, P1415, DOI 10.1111/j.1467-8659.2012.03137.x
   Gehler P., 2011, P ADV NEUR INF PROC, P765
   Gong WY, 2019, IEEE ACCESS, V7, P4024, DOI 10.1109/ACCESS.2018.2888946
   Grosse R, 2009, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2009.5459428
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim S, 2016, LECT NOTES COMPUT SC, V9912, P143, DOI 10.1007/978-3-319-46484-8_9
   Kong N, 2014, LECT NOTES COMPUT SC, V8690, P360
   KREBS A, 2020, 2019 IEEE 4 INT C SI, V86, P15872
   Laffont PY, 2015, IEEE I CONF COMP VIS, P433, DOI 10.1109/ICCV.2015.57
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Lee KJ, 2012, LECT NOTES COMPUT SC, V7577, P327, DOI 10.1007/978-3-642-33783-3_24
   Lei C., 2020, P ADV NEUR INF PROC, V33, P1083
   Lettry L, 2018, COMPUT GRAPH FORUM, V37, P409, DOI 10.1111/cgf.13578
   Li Y, 2014, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2014.346
   Li ZM, 2018, LECT NOTES COMPUT SC, V11213, P339, DOI [10.1007/978-3-030-01240-3_21, 10.1007/978-3-030-01219-9_23]
   Li ZQ, 2018, PROC CVPR IEEE, P9039, DOI 10.1109/CVPR.2018.00942
   Liu YF, 2020, AAAI CONF ARTIF INTE, V34, P11661
   Matsushita Y, 2004, LECT NOTES COMPUT SC, V3022, P274
   Meka A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925907
   Narihira T, 2015, IEEE I CONF COMP VIS, P2992, DOI 10.1109/ICCV.2015.342
   Narihira T, 2015, PROC CVPR IEEE, P2965, DOI 10.1109/CVPR.2015.7298915
   Nestmeyer T, 2017, PROC CVPR IEEE, P1771, DOI 10.1109/CVPR.2017.192
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Seo K, 2021, 2021 IEEE 3RD GLOBAL CONFERENCE ON LIFE SCIENCES AND TECHNOLOGIES (IEEE LIFETECH 2021), P1, DOI 10.1109/LIFETECH52111.2021.9391914
   Shen JB, 2013, IEEE T CYBERNETICS, V43, P425, DOI 10.1109/TSMCB.2012.2208744
   Shen L, 2008, PROC CVPR IEEE, P2479
   Shen L, 2011, PROC CVPR IEEE, P697, DOI 10.1109/CVPR.2011.5995738
   Sheng B, 2020, IEEE T VIS COMPUT GR, V26, P1332, DOI 10.1109/TVCG.2018.2869326
   Shi J, 2017, PROC CVPR IEEE, P5844, DOI 10.1109/CVPR.2017.619
   Sial HA, 2020, J OPT SOC AM A, V37, P1, DOI 10.1364/JOSAA.37.000001
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tang Y, 2012, Proceedings of the 29th International Coference on International Conference on Machine Learning
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZJ, 2019, IEEE INT CONF COMP V, P4310, DOI 10.1109/ICCVW.2019.00531
   Weiss Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P68, DOI 10.1109/ICCV.2001.937606
   Yao CH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P777, DOI 10.1145/3123266.3123363
   Ye GZ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601135
   Zhao Q, 2012, IEEE T PATTERN ANAL, V34, P1437, DOI 10.1109/TPAMI.2012.77
   Zhou TH, 2015, IEEE I CONF COMP VIS, P3469, DOI 10.1109/ICCV.2015.396
   Zhu Y., 2021, P IEEE ICC, P1
   Zoran D, 2015, IEEE I CONF COMP VIS, P388, DOI 10.1109/ICCV.2015.52
NR 60
TC 1
Z9 1
U1 2
U2 11
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2023
VL 9
IS 3
BP 597
EP 618
DI 10.1007/s41095-022-0294-4
EA APR 2023
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA G9DP6
UT WOS:000974409700002
OA gold
DA 2024-07-18
ER

PT J
AU Zimmer, A
   Hilsmann, A
   Morgenstern, W
   Eisert, P
AF Zimmer, Alexandra
   Hilsmann, Anna
   Morgenstern, Wieland
   Eisert, Peter
TI Imposing temporal consistency on deep monocular body shape and pose
   estimation
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE motion capture; surface reconstruction; face modeling; body model
   estimation
AB Accurate and temporally consistent modeling of human bodies is essential for a wide range of applications, including character animation, understanding human social behavior, and AR/VR interfaces. Capturing human motion accurately from a monocular image sequence remains challenging; modeling quality is strongly influenced by temporal consistency of the captured body motion. Our work presents an elegant solution to integrating temporal constraints during fitting. This increases both temporal consistency and robustness during optimization. In detail, we derive parameters of a sequence of body models, representing shape and motion of a person. We optimize these parameters over the complete image sequence, fitting a single consistent body shape while imposing temporal consistency on the body motion, assuming body joint trajectories to be linear over short time. Our approach enables the derivation of realistic 3D body models from image sequences, including jaw pose, facial expression, and articulated hands. Our experiments show that our approach accurately estimates body shape and motion, even for challenging movements and poses. Further, we apply it to the particular application of sign language analysis, where accurate and temporally consistent motion modelling is essential, and show that the approach is well-suited to this kind of application.
C1 [Zimmer, Alexandra; Hilsmann, Anna; Morgenstern, Wieland; Eisert, Peter] Fraunhofer Heinrich Hertz Inst, D-10587 Berlin, Germany.
   [Zimmer, Alexandra] Tech Univ Berlin, D-10623 Berlin, Germany.
   [Eisert, Peter] Humboldt Univ, D-10117 Berlin, Germany.
C3 Fraunhofer Gesellschaft; Technical University of Berlin; Humboldt
   University of Berlin
RP Zimmer, A (corresponding author), Fraunhofer Heinrich Hertz Inst, D-10587 Berlin, Germany.; Zimmer, A (corresponding author), Tech Univ Berlin, D-10623 Berlin, Germany.
EM alexandra.zimmer@hhi.fraunhofer.de; anna.hilsmann@hhi.fraunhofer.de;
   wieland.morgenstern@hhi.fraunhofer.de; peter.eisert@hhi.fraunhofer.de
RI Eisert, Peter/AAX-7968-2020
OI Morgenstern, Wieland/0000-0001-5817-7464
FU European Union [952147]; German Federal Ministry of Education and
   Research (BMBF) through the Research Program MoDL [01 IS 20044]
FX This work was partly funded by the European Union's Horizon 2020
   Research and Innovation Programme under Agreement No. 952147 (Invictus)
   as well as the German Federal Ministry of Education and Research (BMBF)
   through the Research Program MoDL under Contract No. 01 IS 20044.
CR Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34
   Caliskan A, 2021, IEEE COMPUT SOC CONF, P1780, DOI 10.1109/CVPRW53098.2021.00197
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Choutas Vasileios, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P20, DOI 10.1007/978-3-030-58607-2_2
   Fournier M., 2006, P 4 INT C COMPUTER G, P407
   Ghorbani S, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0253157
   He YN, 2021, PROC CVPR IEEE, P11395, DOI 10.1109/CVPR46437.2021.01124
   Hodgins J. K, Cmu Graphics Lab Motion Capture Database
   Joo H, 2018, PROC CVPR IEEE, P8320, DOI 10.1109/CVPR.2018.00868
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530
   Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234
   Lassner C, 2017, PROC CVPR IEEE, P4704, DOI 10.1109/CVPR.2017.500
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Mahmood N, 2019, IEEE I CONF COMP VIS, P5441, DOI 10.1109/ICCV.2019.00554
   Nocedal J., 2006, NUMERICAL OPTIMIZATI, P276
   Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123
   Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794
   Romero J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130883
   Rong Y, 2020, Arxiv, DOI arXiv:2008.08324
   Shihao Zou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P351, DOI 10.1007/978-3-030-58568-6_21
   Teschner M., 2004, P 25 ANN C EUROPEAN
   Wen YH, 2019, AAAI CONF ARTIF INTE, P8989
   Xiang DL, 2019, PROC CVPR IEEE, P10957, DOI 10.1109/CVPR.2019.01122
   Xu L, 2020, PROC CVPR IEEE, P4967, DOI 10.1109/CVPR42600.2020.00502
   Xu XY, 2022, IEEE T PATTERN ANAL, V44, P4490, DOI 10.1109/TPAMI.2021.3070002
   Zhang HW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11426, DOI 10.1109/ICCV48922.2021.01125
   Zhang Jason Y., 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P34, DOI 10.1007/978-3-030-58610-2_3
   Zheng C, 2022, Arxiv, DOI [arXiv:2012.13392, DOI 10.48550/ARXIV.2012.13392]
   Zhou YX, 2021, PROC CVPR IEEE, P4809, DOI 10.1109/CVPR46437.2021.00478
   Zuo XX, 2021, IEEE T MULTIMEDIA, V23, P1617, DOI 10.1109/TMM.2020.3001506
NR 31
TC 1
Z9 1
U1 3
U2 22
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2023
VL 9
IS 1
BP 123
EP 139
DI 10.1007/s41095-022-0272-x
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5K7GQ
UT WOS:000869891100008
OA Green Submitted, gold
DA 2024-07-18
ER

PT J
AU Bian, XW
   Wang, CQ
   Quan, WZ
   Ye, JT
   Zhang, XP
   Yan, DM
AF Bian, Xuewei
   Wang, Chaoqun
   Quan, Weize
   Ye, Juntao
   Zhang, Xiaopeng
   Yan, Dong-Ming
TI Scene text removal via cascaded text stroke detection and erasing
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE scene text removal; text stroke detection; generative adversarial
   networks; cascaded network design; real-world dataset
AB Recent learning-based approaches show promising performance improvement for the scene text removal task but usually leave several remnants of text and provide visually unpleasant results. In this work, a novel end-to-end framework is proposed based on accurate text stroke detection. Specifically, the text removal problem is decoupled into text stroke detection and stroke removal; we design separate networks to solve these two subproblems, the latter being a generative network. These two networks are combined as a processing unit, which is cascaded to obtain our final model for text removal. Experimental results demonstrate that the proposed method substantially outperforms the state-of-the-art for locating and erasing scene text. A new large-scale real-world dataset with 12,120 images has been constructed and is being made available to facilitate research, as current publicly available datasets are mainly synthetic so cannot properly measure the performance of different methods.
C1 [Bian, Xuewei; Wang, Chaoqun; Quan, Weize; Ye, Juntao; Zhang, Xiaopeng; Yan, Dong-Ming] Inst Automat, Natl Lab Pattern Recognit, Beijing 100049, Peoples R China.
   [Bian, Xuewei; Wang, Chaoqun; Quan, Weize; Ye, Juntao; Zhang, Xiaopeng; Yan, Dong-Ming] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100084, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Quan, WZ (corresponding author), Inst Automat, Natl Lab Pattern Recognit, Beijing 100049, Peoples R China.; Quan, WZ (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100084, Peoples R China.
EM bianxuewei2017@ia.ac.cn; wangchaoqun2018@ia.ac.cn; qweizework@gmail.com;
   yejuntao@gmail.com; xiaopeng.zhang@ia.ac.cn; yandongming@gmail.com
FU National Natural Science Foundation of China [62102418, 62172415];
   National Key R&D Program of China [2019YFB2204104]; Open Research Fund
   Program of State key Laboratory of Hydroscience and Engineering,
   Tsinghua University [sklhse-2020D-07]
FX This work was partially supported by the National Natural Science
   Foundation of China (62102418 and 62172415), the National Key R&D
   Program of China (2019YFB2204104), and the Open Research Fund Program of
   State key Laboratory of Hydroscience and Engineering, Tsinghua
   University (sklhse-2020D-07). We would like to thank Shiyu Hou and Shuo
   Liu for helping collect the dataset.
CR Aly HA, 2005, IEEE T IMAGE PROCESS, V14, P1647, DOI 10.1109/TIP.2005.851684
   Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959
   Bai X, 2017, PATTERN RECOGN, V66, P437, DOI 10.1016/j.patcog.2016.12.005
   Chen J, 2019, SCI CHINA INFORM SCI, V62, DOI 10.1007/s11432-019-2673-8
   Dutta A, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2276, DOI 10.1145/3343031.3350535
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   Gupta Neeraj, 2020, 2020 International Conference on Contemporary Computing and Applications (IC3A), P150, DOI 10.1109/IC3A48958.2020.233287
   He WH, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107026
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Joachims Thorsten, 1998, EUROPEAN C MACHINE L, P137, DOI 10.1007/
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Khodadadi M., 2012, 2012 20th Iranian Conference on Electrical Engineering (ICEE 2012), P1035, DOI 10.1109/IranianCEE.2012.6292505
   Kingma D, 2014, ICLR P, V2014, P1
   Liu CY, 2020, IEEE T IMAGE PROCESS, V29, P8760, DOI 10.1109/TIP.2020.3018859
   Liu YL, 2019, PATTERN RECOGN, V90, P337, DOI 10.1016/j.patcog.2019.02.002
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Miao Zhao, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1458, DOI 10.1109/ICDAR.2019.00234
   Miyato T, 2018, INT C LEARN REPR
   Modha U., 2012, INT J ENG RES APPL, V2, P930
   Nakamura T, 2017, PROC INT CONF DOC, P832, DOI 10.1109/ICDAR.2017.141
   Nayef N, 2017, PROC INT CONF DOC, P1454, DOI 10.1109/ICDAR.2017.237
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371
   Tran D, 2017, ADV NEUR IN, V30
   Tursun Osman, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P39, DOI 10.1109/ICDAR.2019.00016
   Tursun O, 2020, COMPUT VIS IMAGE UND, V201, DOI 10.1016/j.cviu.2020.103066
   Wagh PD, 2015, 2015 INTERNATIONAL CONFERENCE ON PERVASIVE COMPUTING (ICPC)
   Wolf C, 2006, INT J DOC ANAL RECOG, V8, P280, DOI 10.1007/s10032-006-0014-0
   Wu L, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1500, DOI 10.1145/3343031.3350929
   Ye QX, 2015, IEEE T PATTERN ANAL, V37, P1480, DOI 10.1109/TPAMI.2014.2366765
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zhang CQ, 2015, PROC INT CONF DOC, P886, DOI 10.1109/ICDAR.2015.7333889
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang ST, 2019, AAAI CONF ARTIF INTE, P801
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 38
TC 9
Z9 9
U1 2
U2 21
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2022
VL 8
IS 2
BP 273
EP 287
DI 10.1007/s41095-021-0242-8
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XJ0YS
UT WOS:000726525400007
OA gold
DA 2024-07-18
ER

PT J
AU Guo, JW
   Wang, HY
   Cheng, ZL
   Zhang, XP
   Yan, DM
AF Guo, Jianwei
   Wang, Hanyu
   Cheng, Zhanglin
   Zhang, Xiaopeng
   Yan, Dong-Ming
TI Learning local shape descriptors for computing non-rigid dense
   correspondence
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE local feature descriptor; triplet CNN; dense correspondence; geometry
   image; non-rigid shape
ID OBJECT RECOGNITION; SIMILARITY; SIGNATURES; FEATURES
AB A discriminative local shape descriptor plays an important role in various applications. In this paper, we present a novel deep learning framework that derives discriminative local descriptors for deformable 3D shapes. We use local "geometry images" to encode the multi-scale local features of a point, via an intrinsic parameterization method based on geodesic polar coordinates. This new parameterization provides robust geometry images even for badly-shaped triangular meshes. Then a triplet network with shared architecture and parameters is used to perform deep metric learning; its aim is to distinguish between similar and dissimilar pairs of points. Additionally, a newly designed triplet loss function is minimized for improved, accurate training of the triplet network. To solve the dense correspondence problem, an efficient sampling approach is utilized to achieve a good compromise between training performance and descriptor quality. During testing, given a geometry image of a point of interest, our network outputs a discriminative local descriptor for it. Extensive testing of non-rigid dense shape matching on a variety of benchmarks demonstrates the superiority of the proposed descriptors over the state-of-the-art alternatives.
C1 [Guo, Jianwei; Zhang, Xiaopeng; Yan, Dong-Ming] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Wang, Hanyu] Univ Maryland, College Pk, MD 20742 USA.
   [Cheng, Zhanglin] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen VisuCA Key Lab, Shenzhen, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; University
   System of Maryland; University of Maryland College Park; Chinese Academy
   of Sciences; Shenzhen Institute of Advanced Technology, CAS
RP Yan, DM (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Cheng, ZL (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen VisuCA Key Lab, Shenzhen, Peoples R China.
EM jianwei.guo@nlpr.ia.ac.cn; hywang66@cs.umd.edu; zl.cheng@siat.ac.cn;
   xiaopeng.zhang@ia.ac.cn; yandongming@gmail.com
OI Yan, Dong-Ming/0000-0003-2209-2404; Wang, Hanyu/0000-0002-9162-9730;
   Cheng, Zhanglin/0000-0002-3360-2679
FU National Key R&D Program of China [2018YFB2100602]; National Natural
   Science Foundation of China [61802406, 61772523, 61702488]; Beijing
   Natural Science Foundation [L182059]; CCF-Tencent Open Research Fund,
   Shenzhen Basic Research Program [JCYJ20180507182222355]; Open Project
   Program of the State Key Lab of CAD&CG Zhejiang University [A2004]
FX This work was partially funded by the National Key R&D Program of China
   (2018YFB2100602), the National Natural Science Foundation of China
   (61802406, 61772523, 61702488), Beijing Natural Science Foundation
   (L182059), the CCF-Tencent Open Research Fund, Shenzhen Basic Research
   Program (JCYJ20180507182222355), and the Open Project Program of the
   State Key Lab of CAD&CG (A2004) Zhejiang University.
CR Abadi M, ARXIV, DOI DOI 10.48550/ARXIV.1603.04467
   ACM, 2018, T GRAPHICS, V37, P1
   ACM, 2011, T GRAPHICS, V3, P1
   ACM, 2012, T GRAPHICS, V31, P6
   ACM, 2012, T GRAPHICS, V31, P4
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], P 30 INT C MACH LEAR
   Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Bai S, 2017, IEEE T MULTIMEDIA, V19, P1257, DOI 10.1109/TMM.2017.2652071
   Biasotti S, 2016, COMPUT GRAPH FORUM, V35, P87, DOI 10.1111/cgf.12734
   Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491
   Boscaini D, 2016, COMPUT GRAPH FORUM, V35, P431, DOI 10.1111/cgf.12844
   Boscaini D, 2015, COMPUT GRAPH FORUM, V34, P13, DOI 10.1111/cgf.12693
   Boscaini D, 2016, ADV NEUR IN, V29
   Bronstein A, 2008, RIGID KINGDOM NUMERI, P119
   Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418
   Chen QF, 2015, IEEE I CONF COMP VIS, P2039, DOI 10.1109/ICCV.2015.236
   Coifman RR, 2005, P NATL ACAD SCI USA, V102, P7426, DOI 10.1073/pnas.0500334102
   Corman É, 2015, LECT NOTES COMPUT SC, V8928, P283, DOI 10.1007/978-3-319-16220-1_20
   Cosmo L, 2016, P EUR WORKSH 3D OBJ
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Elad A, 2003, IEEE T PATTERN ANAL, V25, P1285, DOI 10.1109/TPAMI.2003.1233902
   Frome A, 2004, LECT NOTES COMPUT SC, V3023, P224
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Gao L, 2015, IEEE T VIS COMPUT GR, V21, P1390, DOI 10.1109/TVCG.2014.2369039
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Gu XF, 2002, ACM T GRAPHIC, V21, P355
   Guo YL, 2016, INT J COMPUT VISION, V116, P66, DOI 10.1007/s11263-015-0824-y
   Guo YL, 2014, IEEE T PATTERN ANAL, V36, P2270, DOI 10.1109/TPAMI.2014.2316828
   Guo YL, 2013, INT J COMPUT VISION, V105, P63, DOI 10.1007/s11263-013-0627-y
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Khoury M, 2017, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2017.26
   King DB, 2015, ACS SYM SER, V1214, P1
   Kokkinos I, 2012, PROC CVPR IEEE, P159, DOI 10.1109/CVPR.2012.6247671
   Kovnatsky A, 2015, PROC CVPR IEEE, P905, DOI 10.1109/CVPR.2015.7298692
   Kumar BGV, 2016, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2016.581
   Lian ZH, 2013, PATTERN RECOGN, V46, P449, DOI 10.1016/j.patcog.2012.07.014
   Litany O, 2017, IEEE I CONF COMP VIS, P5660, DOI 10.1109/ICCV.2017.603
   Litman R, 2014, IEEE T PATTERN ANAL, V36, P171, DOI 10.1109/TPAMI.2013.148
   Masci J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P832, DOI 10.1109/ICCVW.2015.112
   Melvær EL, 2012, COMPUT GRAPH FORUM, V31, P2423, DOI 10.1111/j.1467-8659.2012.03187.x
   Mémoli F, 2005, FOUND COMPUT MATH, V5, P313, DOI 10.1007/s10208-004-0145-y
   Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576
   Nogneng D, 2017, COMPUT GRAPH FORUM, V36, P259, DOI 10.1111/cgf.13124
   Ovsjanikov M, 2010, COMPUT GRAPH FORUM, V29, P1555, DOI 10.1111/j.1467-8659.2010.01764.x
   Ovsjanikov Maks, 2016, SIGGRAPH ASIA 2016 C, P9
   Pokrass J, 2013, COMPUT GRAPH FORUM, V32, P459, DOI 10.1111/cgf.12066
   Rodolà E, 2014, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2014.532
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shah SAA, 2015, PATTERN RECOGN, V48, P2859, DOI 10.1016/j.patcog.2015.03.014
   Sinha A, 2016, LECT NOTES COMPUT SC, V9910, P223, DOI 10.1007/978-3-319-46466-4_14
   Sipiran I, 2011, VISUAL COMPUT, V27, P963, DOI 10.1007/s00371-011-0610-y
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Svoboda J, 2016, INT C PATT RECOG, P4232, DOI 10.1109/ICPR.2016.7900298
   Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26
   van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P1681, DOI 10.1111/j.1467-8659.2011.01884.x
   Vestner M, 2017, PROC CVPR IEEE, P6681, DOI 10.1109/CVPR.2017.707
   Wang HY, 2018, LECT NOTES COMPUT SC, V11212, P3, DOI 10.1007/978-3-030-01237-3_1
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wang Y., 2019, P IEEECVF C COMPUTER, P6231
   Wei LY, 2016, PROC CVPR IEEE, P1544, DOI 10.1109/CVPR.2016.171
   Yan DM, 2014, COMPUT GRAPH FORUM, V33, P167, DOI 10.1111/cgf.12442
   Yang YP, 2014, 2014 2ND INTERNATIONAL CONFERENCE ON 3D VISION, VOL. 2, P41, DOI 10.1109/3DV.2014.47
   Zaharescu A, 2009, PROC CVPR IEEE, P373, DOI 10.1109/CVPRW.2009.5206748
   Zeng A, 2017, PROC CVPR IEEE, P199, DOI 10.1109/CVPR.2017.29
NR 66
TC 11
Z9 11
U1 1
U2 15
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2020
VL 6
IS 1
BP 95
EP 112
DI 10.1007/s41095-020-0163-y
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FA
UT WOS:000648691200008
OA gold
DA 2024-07-18
ER

PT J
AU Fan, RC
   Wang, XR
   Hou, QB
   Liu, HC
   Mu, TJ
AF Fan, Ruochen
   Wang, Xuanrun
   Hou, Qibin
   Liu, Hanchao
   Mu, Tai-Jiang
TI SpinNet: Spinning convolutional network for lane boundary detection
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE object detection; lane boundary detection; autonomous driving; deep
   learning
AB In this paper, we propose a simple but effective framework for lane boundary detection, called SpinNet. Considering that cars or pedestrians often occlude lane boundaries and that the local features of lane boundaries are not distinctive, therefore, analyzing and collecting global context information is crucial for lane boundary detection. To this end, we design a novel spinning convolution layer and a brand-new lane parameterization branch in our network to detect lane boundaries from a global perspective. To extract features in narrow strip-shaped fields, we adopt strip-shaped convolutions with kernels which have 1 x n or n x 1 shape in the spinning convolution layer. To tackle the problem of that straight strip-shaped convolutions are only able to extract features in vertical or horizontal directions, we introduce the concept of feature map rotation to allow the convolutions to be applied in multiple directions so that more information can be collected concerning a whole lane boundary. Moreover, unlike most existing lane boundary detectors, which extract lane boundaries from segmentation masks, our lane boundary parameterization branch predicts a curve expression for the lane boundary for each pixel in the output feature map. And the network utilizes this information to predict the weights of the curve, to better form the final lane boundaries. Our framework is easy to implement and end-to-end trainable. Experiments show that our proposed SpinNet outperforms state-of-the-art methods.
C1 [Fan, Ruochen] Tsinghua Univ, Dept Comp Sci, Beijing 100084, Peoples R China.
   [Wang, Xuanrun; Liu, Hanchao; Mu, Tai-Jiang] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Hou, Qibin] Nankai Univ, Media Grp, Tianjin 300350, Peoples R China.
C3 Tsinghua University; Tsinghua University; Nankai University
RP Mu, TJ (corresponding author), Tsinghua Univ, Dept Comp Sci, Beijing 100084, Peoples R China.
EM frc16@mails.tsinghua.edu.cn; xuanrun-16@mails.tsinghua.edu.cn;
   andrewhoux@gmail.com; liuhc17@mails.tsinghua.edu.cn; mmmutj@gmail.com
RI fan, ruochen/AAQ-1758-2021; Mu, Tai-Jiang/JWO-1381-2024
OI Mu, Tai-Jiang/0000-0002-9197-346X
FU National Natural Science Foundation of China [61572264]; Research Grant
   of Beijing Higher Institution Engineering Research Center;
   Tsinghua-Tencent Joint Laboratory for Internet Innovation Technology
FX This work was supported by the National Natural Science Foundation of
   China (Project No. 61572264), Research Grant of Beijing Higher
   Institution Engineering Research Center, and Tsinghua-Tencent Joint
   Laboratory for Internet Innovation Technology.
CR Abadi M, 2016, ACM SIGPLAN NOTICES, V51, P1, DOI [10.1145/3022670.2976746, 10.1145/2951913.2976746]
   Aly M, 2008, IEEE INT VEH SYM, P165, DOI 10.1109/ivs.2008.4621152
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bar Hillel A, 2014, MACH VISION APPL, V25, P727, DOI 10.1007/s00138-011-0404-2
   Borkar A, 2012, IEEE T INTELL TRANSP, V13, P365, DOI 10.1109/TITS.2011.2173196
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen P.-R., 2018, INT CONF DIGIT SIG, P1
   Chiu KY, 2005, 2005 IEEE INTELLIGENT VEHICLES SYMPOSIUM PROCEEDINGS, P706
   Quach CH, 2018, PROCEEDINGS OF 2018 2ND INTERNATIONAL CONFERENCE ON RECENT ADVANCES IN SIGNAL PROCESSING, TELECOMMUNICATIONS & COMPUTING (SIGTELCOM 2018), P152, DOI 10.1109/SIGTELCOM.2018.8325781
   Dieleman S, 2015, MON NOT R ASTRON SOC, V450, P1441, DOI 10.1093/mnras/stv632
   Garnett N, 2019, IEEE I CONF COMP VIS, P2921, DOI 10.1109/ICCV.2019.00301
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Huval Brody., 2015, COMPUTER VISION PATT
   Jung S, 2016, IEEE T INTELL TRANSP, V17, P289, DOI 10.1109/TITS.2015.2464253
   Kim J, 2017, IEEE COMPUT SOC CONF, P1194, DOI 10.1109/CVPRW.2017.158
   Lee S, 2017, IEEE I CONF COMP VIS, P1965, DOI 10.1109/ICCV.2017.215
   Liang D., 2018, LINENET ZOOMABLE CNN
   Lipton Z.C., 2015, Learning to diagnose with lstm recurrent neural networks, P2015
   Liu GL, 2010, IEEE INT VEH SYM, P993, DOI 10.1109/IVS.2010.5548021
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Loose H, 2009, IEEE INT VEH SYM, P60, DOI 10.1109/IVS.2009.5164253
   Neven D, 2018, IEEE INT VEH SYM, P286
   Ouyang ZY, 2006, INT C PATT RECOG, P417
   Pan XG, 2018, AAAI CONF ARTIF INTE, P7276
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Simonyan K., 2014, 14091556 ARXIV
   Son J, 2015, EXPERT SYST APPL, V42, P1816, DOI 10.1016/j.eswa.2014.10.024
   Teng Z, 2010, INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS 2010), P2334
   TuSimple, TUS DAT
   Van Gansbeke W, 2019, IEEE INT CONF COMP V, P905, DOI 10.1109/ICCVW.2019.00119
   Worrall DE, 2017, PROC CVPR IEEE, P7168, DOI 10.1109/CVPR.2017.758
   Zhang J, 2018, LECT NOTES COMPUT SC, V11205, P502, DOI 10.1007/978-3-030-01246-5_30
   Zhang W., 2018, ARXIV181205914
   Zou Q, 2020, IEEE T VEH TECHNOL, V69, P41, DOI 10.1109/TVT.2019.2949603
NR 35
TC 17
Z9 18
U1 0
U2 10
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2019
VL 5
IS 4
BP 417
EP 428
DI 10.1007/s41095-019-0160-1
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UO
UT WOS:000648691000008
OA gold
DA 2024-07-18
ER

PT J
AU Zhang, SY
   Liang, RZ
   Wang, M
AF Zhang, Shuyang
   Liang, Runze
   Wang, Miao
TI ShadowGAN: Shadow synthesis for virtual objects with conditional
   adversarial networks
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE shadow synthesis; deep learning; generative adversarial networks; image
   synthesis
ID ILLUMINATION ESTIMATION; IMAGE
AB We introduce ShadowGAN, a generative adversarial network (GAN) for synthesizing shadows for virtual objects inserted in images. Given a target image containing several existing objects with shadows, and an input source object with a specified insertion position, the network generates a realistic shadow for the source object. The shadow is synthesized by a generator; using the proposed local adversarial and global adversarial discriminators, the synthetic shadow's appearance is locally realistic in shape, and globally consistent with other objects' shadows in terms of shadow direction and area. To overcome the lack of training data, we produced training samples based on public 3D models and rendering technology. Experimental results from a user study show that the synthetic shadowed results look natural and authentic.
C1 [Zhang, Shuyang] Univ Michigan, Ann Arbor, MI 48109 USA.
   [Liang, Runze] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Wang, Miao] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
C3 University of Michigan System; University of Michigan; Tsinghua
   University; Beihang University
RP Zhang, SY (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.
EM zhangshuyangmary@outlook.com; liangrz15@mails.tsinghua.edu.cn;
   miaow@buaa.edu.cn
FU National Natural Science Foundation of China [61561146393, 61521002];
   China Postdoctoral Science Foundation [2016M601032]; Research Grant of
   Beijing Higher Institution Engineering Research Center
FX The authors would like to thank all the reviewers. This work was
   supported by the National Natural Science Foundation of China (Project
   Nos. 61561146393 and 61521002), the China Postdoctoral Science
   Foundation (Project No. 2016M601032), and a Research Grant of Beijing
   Higher Institution Engineering Research Center.
CR [Anonymous], 2016, INT C LEARNING REPRE
   Chang A X, 2015, COMPUTER SCI, V1512, P3
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Debevec P, 2008, RENDERING SYNTHETIC, DOI 10.1145/1401132.1401175
   Fan QN, 2018, PROC CVPR IEEE, P8944, DOI 10.1109/CVPR.2018.00932
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo R, 2011, SINGLE IMAGE SHADOW, P2033
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Khan EA, 2006, ACM T GRAPHIC, V25, P654, DOI 10.1145/1141911.1141937
   King DB, 2015, ACS SYM SER, V1214, P1
   Kronander J, 2015, COMPUT GRAPH FORUM, V34, P643, DOI 10.1111/cgf.12591
   Lafortune E. P., 1993, EDUGRAPHICS '93. First International Conference on Graphics Education. COMPUGRAPHICS '93. Third International Conference on Computational Graphics and Visualization Techniques. Combined Proceedings, P145
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li M., 2016, ARXIV161005586
   Liu B, 2017, J COMPUT SCI TECH-CH, V32, P430, DOI 10.1007/s11390-017-1734-y
   Ma LQ, 2016, COMPUT GRAPH FORUM, V35, P189, DOI 10.1111/cgf.13016
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Panagopoulos A, 2009, PROC CVPR IEEE, P651, DOI 10.1109/CVPRW.2009.5206665
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Qu LQ, 2017, PROC CVPR IEEE, P2308, DOI 10.1109/CVPR.2017.248
   Sato I, 2003, IEEE T PATTERN ANAL, V25, P290, DOI 10.1109/TPAMI.2003.1182093
   Shen W, 2017, PROC CVPR IEEE, P1225, DOI 10.1109/CVPR.2017.135
   Shiming Ge, 2018, Computational Visual Media, V4, P71, DOI 10.1007/s41095-017-0102-8
   Shor Y, 2008, COMPUT GRAPH FORUM, V27, P577, DOI 10.1111/j.1467-8659.2008.01155.x
   Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241
   Sun YH, 2017, IEEE ICC
   Wang JF, 2018, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR.2018.00192
   Xu L, 2006, ISDA 2006: SIXTH INTERNATIONAL CONFERENCE ON INTELLIGENT SYSTEMS DESIGN AND APPLICATIONS, VOL 2, P1049
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, P4623, DOI 10.1109/TIP.2015.2465159
   Zhen Wei, 2018, Computational Visual Media, V4, P231, DOI 10.1007/s41095-018-0112-1
NR 30
TC 32
Z9 34
U1 0
U2 6
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2019
VL 5
IS 1
BP 105
EP 115
DI 10.1007/s41095-019-0136-1
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UL
UT WOS:000648690700009
OA gold
DA 2024-07-18
ER

PT J
AU Miao, ZZ
   Zhang, Y
   Peng, Y
   Peng, HC
   Yin, BC
AF Miao, Zhuangzhuang
   Zhang, Yong
   Peng, Yuan
   Peng, Haocheng
   Yin, Baocai
TI DTCC: Multi-level dilated convolution with transformer for
   weakly-supervised crowd counting
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE crowd counting; transformer; dilated convolution; global perspective
   field; pyramid
AB Crowd counting provides an important foundation for public security and urban management. Due to the existence of small targets and large density variations in crowd images, crowd counting is a challenging task. Mainstream methods usually apply convolution neural networks (CNNs) to regress a density map, which requires annotations of individual persons and counts. Weakly-supervised methods can avoid detailed labeling and only require counts as annotations of images, but existing methods fail to achieve satisfactory performance because a global perspective field and multi-level information are usually ignored. We propose a weakly-supervised method, DTCC, which effectively combines multi-level dilated convolution and transformer methods to realize end-to-end crowd counting. Its main components include a recursive swin transformer and a multi-level dilated convolution regression head. The recursive swin transformer combines a pyramid visual transformer with a fine-tuned recursive pyramid structure to capture deep multi-level crowd features, including global features. The multi-level dilated convolution regression head includes multi-level dilated convolution and a linear regression head for the feature extraction module. This module can capture both low- and high-level features simultaneously to enhance the receptive field. In addition, two regression head fusion mechanisms realize dynamic and mean fusion counting. Experiments on four well-known benchmark crowd counting datasets (UCF_CC_50, ShanghaiTech, UCF_QNRF, and JHU-Crowd++) show that DTCC achieves results superior to other weakly-supervised methods and comparable to fully-supervised methods.
C1 [Miao, Zhuangzhuang; Zhang, Yong; Peng, Haocheng; Yin, Baocai] Beijing Univ Technol, Beijing Inst Artificial Intelligence, Fac Informat Technol, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
   [Peng, Yuan] Taiji Comp Corp Ltd, Beijing, Peoples R China.
C3 Beijing University of Technology
RP Zhang, Y (corresponding author), Beijing Univ Technol, Beijing Inst Artificial Intelligence, Fac Informat Technol, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
EM zhangyong2010@bjut.edu.cn
RI Zhang, Yong/AAW-8880-2021; peng, haocheng/HJK-1383-2023
OI Zhang, Yong/0000-0001-6650-6790; 
FU National Natural Science Foundation of China [62072015, U19B2039,
   U1811463]; National Key R&D Program of China [2018YFB1600903]
FX AcknowledgementsThis research project was partially supported by the
   National Natural Science Foundation of China (Grant Nos. 62072015,
   U19B2039, U1811463), and the National Key R&D Program of China (Grant
   No. 2018YFB1600903).A portion of the work in this paper was carried out
   using the Taiji machine learning engine, and we thank Taiji for their
   support.
CR Abousamra S, 2021, AAAI CONF ARTIF INTE, V35, P872
   Bai S, 2020, PROC CVPR IEEE, P4593, DOI 10.1109/CVPR42600.2020.00465
   Cao XK, 2018, LECT NOTES COMPUT SC, V11209, P757, DOI 10.1007/978-3-030-01228-1_45
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chen, 2021, ARXIV
   Chen CF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P347, DOI 10.1109/ICCV48922.2021.00041
   Chu XX, 2021, ADV NEUR IN
   Dongdong Hongyu, 2022, ARXIV
   Dosovitskiy Alexey, 2021, ICLR
   Fu M, 2015, ENG APPL ARTIF INTEL, V43, P81, DOI 10.1016/j.engappai.2015.04.006
   Gao J. Y., 2021, ARXIV
   He L, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1507, DOI 10.1145/3474085.3475285
   Huang Z., 2021, ARXIV
   Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33
   Idrees H, 2013, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2013.329
   Lei YJ, 2021, PATTERN RECOGN, V109, DOI 10.1016/j.patcog.2020.107616
   Lempitsky V., 2010, P ADV NEUR INF PROC, V23, P1, DOI DOI 10.5555/2997189.2997337
   Li M, 2008, INT C PATT RECOG, P1998
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Liang D., 2022, ARXIV
   Liang DK, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3445-y
   Liu LB, 2019, IEEE I CONF COMP VIS, P1774, DOI 10.1109/ICCV.2019.00186
   Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524
   Liu XL, 2019, IEEE T PATTERN ANAL, V41, P1862, DOI 10.1109/TPAMI.2019.2899857
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZH, 2021, IEEE INT CONF COMP V, P2830, DOI 10.1109/ICCVW54120.2021.00317
   Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624
   Meng YD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15529, DOI 10.1109/ICCV48922.2021.01526
   Qiao SY, 2021, PROC CVPR IEEE, P10208, DOI 10.1109/CVPR46437.2021.01008
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sam DB, 2017, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2017.429
   Savner S. S., 2022, ARXIV
   Shang C, 2016, IEEE IMAGE PROC, P1215, DOI 10.1109/ICIP.2016.7532551
   Shen Z, 2018, PROC CVPR IEEE, P5245, DOI 10.1109/CVPR.2018.00550
   Shi ML, 2019, 2019 1ST INTERNATIONAL CONFERENCE ON INDUSTRIAL ARTIFICIAL INTELLIGENCE (IAI 2019), DOI 10.1109/iciai.2019.8850794
   Sindagi V. A., 2017, 2017 14 IEEE INT C A, P1
   Sindagi VA, 2022, IEEE T PATTERN ANAL, V44, P2594, DOI 10.1109/TPAMI.2020.3035969
   Sindagi VA, 2019, IEEE I CONF COMP VIS, P1002, DOI 10.1109/ICCV.2019.00109
   Sindagi VA, 2017, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2017.206
   Song QY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3345, DOI 10.1109/ICCV48922.2021.00335
   Song QY, 2021, AAAI CONF ARTIF INTE, V35, P2576
   Sun G, 2021, ARXIV
   Walach E, 2016, LECT NOTES COMPUT SC, V9906, P660, DOI 10.1007/978-3-319-46475-6_41
   Wan J, 2021, PROC CVPR IEEE, P1974, DOI 10.1109/CVPR46437.2021.00201
   Wang C, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1299, DOI 10.1145/2733373.28063370-12345-67-8/90/01
   Wang F, 2022, ARXIV
   Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839
   Wu B, 2007, INT J COMPUT VISION, V75, P247, DOI 10.1007/s11263-006-0027-7
   Xiong HP, 2019, IEEE I CONF COMP VIS, P8361, DOI 10.1109/ICCV.2019.00845
   Yang YF, 2020, PROC CVPR IEEE, P4373, DOI 10.1109/CVPR42600.2020.00443
   Yifan Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P1, DOI 10.1007/978-3-030-58598-3_1
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
NR 52
TC 2
Z9 2
U1 8
U2 26
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2023
VL 9
IS 4
BP 859
EP 873
DI 10.1007/s41095-022-0313-5
EA APR 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P1QC2
UT WOS:000961866100001
OA gold
DA 2024-07-18
ER

PT J
AU Zhang, KY
   Zhou, L
   Chen, L
   He, ST
   Weiskopf, D
   Wang, YH
AF Zhang, Kaiyi
   Zhou, Liang
   Chen, Lu
   He, Shitong
   Weiskopf, Daniel
   Wang, Yunhai
TI Angle-uniform parallel coordinates
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE parallel coordinates; multidimensional data; deformation; correlations
AB We present angle-uniform parallel coordinates, a data-independent technique that deforms the image plane of parallel coordinates so that the angles of linear relationships between two variables are linearly mapped along the horizontal axis of the parallel coordinates plot. Despite being a common method for visualizing multidimensional data, parallel coordinates are ineffective for revealing positive correlations since the associated parallel coordinates points of such structures may be located at infinity in the image plane and the asymmetric encoding of negative and positive correlations may lead to unreliable estimations. To address this issue, we introduce a transformation that bounds all points horizontally using an angleuniform mapping and shrinks them vertically in a structure-preserving fashion; polygonal lines become smooth curves and a symmetric representation of data correlations is achieved. We further propose a combined subsampling and density visualization approach to reduce visual clutter caused by overdrawing. Our method enables accurate visual pattern interpretation of data correlations, and its data-independent nature makes it applicable to all multidimensional datasets. The usefulness of our method is demonstrated using examples of synthetic and real-world datasets.
C1 [Zhang, Kaiyi; Chen, Lu; He, Shitong; Wang, Yunhai] Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Peoples R China.
   [Zhou, Liang] Peking Univ, Natl Inst Hlth Data Sci, Beijing 100191, Peoples R China.
   [Weiskopf, Daniel] Univ Stuttgart, Visualizat Res Ctr VISUS, D-70569 Stuttgart, Germany.
C3 Shandong University; Peking University; University of Stuttgart
RP Zhou, L (corresponding author), Peking Univ, Natl Inst Hlth Data Sci, Beijing 100191, Peoples R China.
EM zhang.kaiyi42@gmail.com; zhoul@bjmu.edu.cn; chenlu.scien@gmail.com;
   heshitong2021@gmail.com; weiskopf@visus.uni-stuttgart.de;
   cloudseawang@gmail.com
FU Data for Better Health Project of Peking University-Master Kong;
   National Natural Science Foundation of China [62132017]; Deutsche
   Forschungsgemeinschaft (DFG) [251654672-TRR 161]
FX LZ acknowledges support from the Data for Better Health Project of
   Peking University-Master Kong, YW from the National Natural Science
   Foundation of China (62132017), and DW from the Deutsche
   Forschungsgemeinschaft (DFG) Project-ID 251654672-TRR 161.
CR Anderson JW., 2005, Hyperbolic Geometry
   Artero AO, 2004, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2004, PROCEEDINGS, P81, DOI 10.1109/INFVIS.2004.68
   Chang Kai., Parallel coordinates toolkit
   Claessen JHT, 2011, IEEE T VIS COMPUT GR, V17, P2310, DOI 10.1109/TVCG.2011.201
   Cortez P, 2009, DECIS SUPPORT SYST, V47, P547, DOI 10.1016/j.dss.2009.05.016
   de Boor C., 2001, A Practical Guide to Splines
   Ellis G, 2002, SIXTH INTERNATIONAL CONFERENCE ON INFORMATION VISUALISATION, PROCEEDINGS, P82, DOI 10.1109/IV.2002.1028760
   Ellis G, 2007, IEEE T VIS COMPUT GR, V13, P1216, DOI 10.1109/TVCG.2007.70535
   Graham M, 2003, IEEE INFOR VIS, P10, DOI 10.1109/IV.2003.1217950
   Hauser H, 2002, INFOVIS 2002: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2002, P127, DOI 10.1109/INFVIS.2002.1173157
   Heinrich Julian, 2012, Proceedings of the International Conference on Computer Graphics Theory and Applications (GRAPP 2012) and International Conference on Information Visualization Theory and Applications (IVAPP 2012), P594
   Heinrich J., 2013, STAR Proc. Eurographics, V2013, P95, DOI [DOI 10.2312/C0NF/EG2013/STARS/095, DOI 10.2312/CONF/EG2013/STARS/095-116]
   Heinrich J, 2009, IEEE T VIS COMPUT GR, V15, P1531, DOI 10.1109/TVCG.2009.131
   Nguyen H, 2018, IEEE T VIS COMPUT GR, V24, P1301, DOI 10.1109/TVCG.2017.2661309
   Holten D, 2010, COMPUT GRAPH FORUM, V29, P793, DOI 10.1111/j.1467-8659.2009.01666.x
   IEEE, 2004, IEEE VIS 2004 CONT D
   INSELBERG A., 2009, Parallel Coordinates: Visual Multidimensional Geometry and Its Applications, DOI DOI 10.1007/978-0-387-68628-8
   Inselberg A, 1985, VISUAL COMPUT, V1, P69, DOI 10.1007/BF01898350
   Johansson J, 2005, INFOVIS 05: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION, PROCEEDINGS, P125, DOI 10.1109/INFVIS.2005.1532138
   Li J, 2010, INFORM VISUAL, V9, P13, DOI 10.1057/ivs.2008.13
   M.C. Escher Foundation and the M.C. Escher Company, OFF MC ESCH WEBS
   McDonnell KT, 2008, COMPUT GRAPH FORUM, V27, P1031, DOI 10.1111/j.1467-8659.2008.01239.x
   Munzner T, 1997, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION, PROCEEDINGS, P2, DOI 10.1109/INFVIS.1997.636718
   Novotny M, 2006, IEEE T VIS COMPUT GR, V12, P893, DOI 10.1109/TVCG.2006.170
   Qu H, 2007, IEEE T VIS COMPUT GR, V13, P1408, DOI 10.1109/TVCG.2007.70523
   Steed Chad A., 2009, Proceedings of the 2009 IEEE Symposium on Visual Analytics Science and Technology. VAST 2009. Held co-jointly with VisWeek 2009, P19, DOI 10.1109/VAST.2009.5332586
   Sugiyama M., 2013, NIPS, V26, P467
   Theisel Holger., 2000, P C VISION MODELING, P415
   Viau C, 2010, IEEE T VIS COMPUT GR, V16, P1100, DOI 10.1109/TVCG.2010.205
   Ward M. O., 1994, Proceedings. Visualization '94 (Cat. No.94CH35707), P326, DOI 10.1109/VISUAL.1994.346302
   Wegman EJ, 2010, WIRES COMPUT STAT, V2, P678, DOI 10.1002/wics.122
   WEGMAN EJ, 1990, J AM STAT ASSOC, V85, P664, DOI 10.2307/2290001
   Ying-Huey Fua, 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P43, DOI 10.1109/VISUAL.1999.809866
   Yuan XR, 2009, IEEE T VIS COMPUT GR, V15, P1001, DOI 10.1109/TVCG.2009.179
   Zhou H, 2008, COMPUT GRAPH FORUM, V27, P1047, DOI 10.1111/j.1467-8659.2008.01241.x
   Zhou L, 2018, IEEE T VIS COMPUT GR, V24, P1997, DOI 10.1109/TVCG.2017.2698041
NR 36
TC 0
Z9 0
U1 2
U2 2
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2023
VL 9
IS 3
BP 495
EP 512
DI 10.1007/s41095-022-0291-7
EA MAR 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA G9DP6
UT WOS:000961656200001
OA gold
DA 2024-07-18
ER

PT J
AU Yang, Y
   Lu, KC
   Wu, Y
   Wang, YH
   Cao, Y
AF Yang, Yang
   Lu, Kecheng
   Wu, Yu
   Wang, Yunhai
   Cao, Yi
TI Correlation-aware probabilistic data summarization for large-scale
   multi-block scientific data visualization
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE correlation-awareness; large-scale data; multi-block methods;
   probabilistic data summarization
ID NONPARAMETRIC MODELS
AB In this paper, we propose a correlation-aware probabilistic data summarization technique to efficiently analyze and visualize large-scale multi-block volume data generated by massively parallel scientific simulations. The core of our technique is correlation modeling of distribution representations of adjacent data blocks using copula functions and accurate data value estimation by combining numerical information, spatial location, and correlation distribution using Bayes' rule. This effectively preserves statistical properties without merging data blocks in different parallel computing nodes and repartitioning them, thus significantly reducing the computational cost. Furthermore, this enables reconstruction of the original data more accurately than existing methods. We demonstrate the effectiveness of our technique using six datasets, with the largest having one billion grid points. The experimental results show that our approach reduces the data storage cost by approximately one order of magnitude compared to state-of-the-art methods while providing a higher reconstruction accuracy at a lower computational cost.
C1 [Yang, Yang; Wu, Yu; Cao, Yi] Inst Appl Phys & Computat Math, Beijing 100094, Peoples R China.
   [Lu, Kecheng; Wang, Yunhai] Shandong Univ, Sch Comp Sci & Technol, Qingdao, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Applied Physics &
   Computational Mathematics - China; Shandong University
RP Cao, Y (corresponding author), Inst Appl Phys & Computat Math, Beijing 100094, Peoples R China.
EM cao_yi@iapcm.ac.cn
OI Cao, Yi/0000-0002-2131-5453
FU Chinese Postdoctoral Science Foundation [2021M700016]
FX AcknowledgementsThis work was supported by the Chinese Postdoctoral
   Science Foundation (2021M700016).
CR Ahrens J., 2010, LAUR1007088 LOS AL N
   [Anonymous], 2008, P INT C COMP GRAPG V
   Athawale T, 2016, IEEE T VIS COMPUT GR, V22, P777, DOI 10.1109/TVCG.2015.2467958
   Bilmes J., 1998, GENTLE TUTORIAL EM A
   Chaudhuri A, 2014, IEEE PAC VIS SYMP, P201, DOI 10.1109/PacificVis.2014.60
   DEERING M, 1995, P 22 ANN C COMP GRAP, P13
   Dutta S, 2017, IEEE PAC VIS SYMP, P111, DOI 10.1109/PACIFICVIS.2017.8031585
   Dutta S, 2017, IEEE T VIS COMPUT GR, V23, P811, DOI 10.1109/TVCG.2016.2598604
   Dutta S, 2016, IEEE T VIS COMPUT GR, V22, P837, DOI 10.1109/TVCG.2015.2467436
   Gosink LJ, 2011, IEEE T VIS COMPUT GR, V17, P264, DOI 10.1109/TVCG.2010.80
   Gu XF, 2002, ACM T GRAPHIC, V21, P355
   Hazarika S, 2019, IEEE T VIS COMPUT GR, V25, P1214, DOI 10.1109/TVCG.2018.2864801
   Hazarika S, 2018, IEEE T VIS COMPUT GR, V24, P934, DOI 10.1109/TVCG.2017.2744099
   Hladuvka J, 2001, SPRING EUROGRAP, P203
   Ihm IS, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P107
   Johnson CR, 2009, IEEE T VIS COMPUT GR, V15, P734, DOI 10.1109/TVCG.2009.25
   Khodakovsky A, 2000, COMP GRAPH, P271, DOI 10.1145/344779.344922
   Kim T., 1999, Proceedings. Seventh Pacific Conference on Computer Graphics and Applications (Cat. No.PR00293), P147, DOI 10.1109/PCCGA.1999.803358
   Kindlmann G, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P513, DOI 10.1109/VISUAL.2003.1250414
   Kniss J, 2002, IEEE T VIS COMPUT GR, V8, P270, DOI 10.1109/TVCG.2002.1021579
   Kniss JM, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P287
   Luo A., 2003, Data Visualisation 2003. Joint Eurographics/IEEE TCVG. Symposium on Visualization, P29
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Nouanesengsy B, 2014, SYMP LARG DATA ANAL, P43, DOI 10.1109/LDAV.2014.7013203
   Nowell L., 2014, SCI EXTREME SCALE AR
   Peng JL, 2005, ACM T GRAPHIC, V24, P609, DOI 10.1145/1073204.1073237
   Pöthkow K, 2013, COMPUT GRAPH FORUM, V32, P131, DOI 10.1111/cgf.12100
   Potter K, 2010, COMPUT GRAPH FORUM, V29, P823, DOI 10.1111/j.1467-8659.2009.01677.x
   Reynolds D. R., 2019, ARXIV
   Sasaki N, 2015, INT PARALL DISTRIB P, P914, DOI 10.1109/IPDPS.2015.67
   Schmidt T., 2006, Copulas, from Theory to Application in Finance, P3
   Shusen Liu, 2012, 2012 IEEE Symposium on Large Data Analysis and Visualization (LDAV 2012), P73, DOI 10.1109/LDAV.2012.6378978
   Sklar M., 1959, Publ. Inst. Statist. Univ. Paris, V8, P229, DOI DOI 10.1007/978-3-642-33590-7
   Tenginakai S, 2001, IEEE VISUAL, P231, DOI 10.1109/VISUAL.2001.964516
   Thompson D., 2011, Proceedings of the IEEE Symposium on Large Data Analysis and Visualization (LDAV 2011), P23, DOI 10.1109/LDAV.2011.6092313
   Tzeng FY, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P505, DOI 10.1109/VISUAL.2003.1250413
   Wang CL, 2011, ENTROPY-SWITZ, V13, P254, DOI 10.3390/e13010254
   Wang KC, 2019, IEEE PAC VIS SYMP, P303, DOI 10.1109/PacificVis.2019.00043
   Wang KC, 2017, IEEE PAC VIS SYMP, P161, DOI 10.1109/PACIFICVIS.2017.8031590
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei TH, 2015, COMPUT GRAPH FORUM, V34, P81, DOI 10.1111/cgf.12620
NR 41
TC 0
Z9 0
U1 2
U2 8
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2023
VL 9
IS 3
BP 513
EP 529
DI 10.1007/s41095-022-0304-6
EA MAR 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA G9DP6
UT WOS:000950868100001
OA gold
DA 2024-07-18
ER

PT J
AU Zhang, XF
   Wang, H
   Zhang, Y
   Gao, X
   Wang, G
   Zhang, CM
AF Zhang, Xiaofeng
   Wang, Hua
   Zhang, Yan
   Gao, Xin
   Wang, Gang
   Zhang, Caiming
TI Improved fuzzy clustering for image segmentation based on a low-rank
   prior
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE image segmentation; fuzzy clustering; non-local information; low-rank
   prior; medical images
ID C-MEANS ALGORITHM; INFORMATION; SPARSE
AB Image segmentation is a basic problem in medical image analysis and useful for disease diagnosis. However, the complexity of medical images makes image segmentation difficult. In recent decades, fuzzy clustering algorithms have been preferred due to their simplicity and efficiency. However, they are sensitive to noise. To solve this problem, many algorithms using non-local information have been proposed, which perform well but are inefficient. This paper proposes an improved fuzzy clustering algorithm utilizing nonlocal self-similarity and a low-rank prior for image segmentation. Firstly, cluster centers are initialized based on peak detection. Then, a pixel correlation model between corresponding pixels is constructed, and similar pixel sets are retrieved. To improve efficiency and robustness, the proposed algorithm uses a novel objective function combining non-local information and a low-rank prior. Experiments on synthetic images and medical images illustrate that the algorithm can improve efficiency greatly while achieving satisfactory results.
C1 [Zhang, Xiaofeng; Wang, Hua; Zhang, Yan; Gao, Xin; Wang, Gang] Ludong Univ, Sch Informat & Elect Engn, Yantai 264025, Peoples R China.
   [Zhang, Caiming] Shandong Univ Finance & Econ, Shandong Prov Key Lab Digital Media Technol, Jinan 250061, Peoples R China.
C3 Ludong University; Shandong University of Finance & Economics
RP Wang, H (corresponding author), Ludong Univ, Sch Informat & Elect Engn, Yantai 264025, Peoples R China.
EM iamzxf@126.com; hwa229@163.com; 15684177618@163.com;
   gao_xin_private@163.com; happy_wg@163.com; czhang@sdu.edu.cn
RI Cheng, Lin/KFQ-3111-2024; Gao, Xin/D-5487-2013
FU National Natural Science Foundation of China [61873117, 62007017,
   61773244, 61772253, 61771231]
FX This research was funded by the National Natural Science Foundation of
   China under Grant Nos. 61873117, 62007017, 61773244, 61772253, and
   61771231. The authors also gratefully acknowledge the reviewers' helpful
   comments and suggestions, which improved the presentation significantly.
CR Ahmed MN, 2002, IEEE T MED IMAGING, V21, P193, DOI 10.1109/42.996338
   Ben Ishak A, 2017, APPL SOFT COMPUT, V52, P306, DOI 10.1016/j.asoc.2016.10.034
   Cai WL, 2007, PATTERN RECOGN, V40, P825, DOI 10.1016/j.patcog.2006.07.011
   Chaira T, 2011, APPL SOFT COMPUT, V11, P1711, DOI 10.1016/j.asoc.2010.05.005
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dong WS, 2009, IEEE IMAGE PROC, P349, DOI 10.1109/ICIP.2009.5414423
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Guo Q, 2018, IEEE T VIS COMPUT GR, V24, P2023, DOI 10.1109/TVCG.2017.2702738
   Guo Q, 2016, IEEE T CIRC SYST VID, V26, P868, DOI 10.1109/TCSVT.2015.2416631
   Kim GR, 2016, J ULTRAS MED, V35, P519, DOI 10.7863/ultra.15.04014
   Krinidis S, 2010, IEEE T IMAGE PROCESS, V19, P1328, DOI 10.1109/TIP.2010.2040763
   Kwan RKS, 1999, IEEE T MED IMAGING, V18, P1085, DOI 10.1109/42.816072
   Liu H, 2019, MULTIMED TOOLS APPL, V78, P9033, DOI 10.1007/s11042-017-5277-6
   Liu H, 2018, INFORM SCIENCES, V468, P142, DOI 10.1016/j.ins.2018.08.022
   Liu XX, 2020, COMPUT VIS MEDIA, V6, P467, DOI 10.1007/s41095-020-0181-9
   Liu Y, 2019, IEEE T PATTERN ANAL, V41, P1939, DOI 10.1109/TPAMI.2018.2878849
   Ma DY, 2021, IEEE T IMAGE PROCESS, V30, P1825, DOI 10.1109/TIP.2020.3045640
   Nambura A, 2017, APPL SOFT COMPUT, V54, P456, DOI 10.1016/j.asoc.2016.08.020
   Orduna R, 2014, INFORM SCIENCES, V258, P339, DOI 10.1016/j.ins.2013.09.028
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Ren TB, 2019, APPL SOFT COMPUT, V81, DOI 10.1016/j.asoc.2019.105503
   Singh C, 2018, APPL SOFT COMPUT, V68, P447, DOI 10.1016/j.asoc.2018.03.054
   Szilágyi L, 2014, PATTERN RECOGN LETT, V36, P29, DOI 10.1016/j.patrec.2013.08.027
   Pham TX, 2018, APPL SOFT COMPUT, V65, P230, DOI 10.1016/j.asoc.2018.01.003
   Verma H, 2016, APPL SOFT COMPUT, V46, P543, DOI 10.1016/j.asoc.2015.12.022
   Zhang F, 2020, COMPUT VIS MEDIA, V6, P417, DOI 10.1007/s41095-020-0186-4
   Zhang XF, 2019, SOFT COMPUT, V23, P3081, DOI 10.1007/s00500-017-2955-2
   Zhang XF, 2017, MULTIMED TOOLS APPL, V76, P7869, DOI 10.1007/s11042-016-3399-x
   Zhang XF, 2012, SCI CHINA INFORM SCI, V55, P1052, DOI 10.1007/s11432-012-4556-0
   Zhang YX, 2017, IEEE T CIRC SYST VID, V27, P1502, DOI 10.1109/TCSVT.2016.2539839
NR 30
TC 12
Z9 12
U1 0
U2 23
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2021
VL 7
IS 4
BP 513
EP 528
DI 10.1007/s41095-021-0239-3
EA AUG 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UK7UX
UT WOS:000683521600001
OA gold
DA 2024-07-18
ER

PT J
AU Zhou, J
   Zhang, Q
   Fan, JH
   Sun, W
   Zheng, WS
AF Zhou, Jin
   Zhang, Qing
   Fan, Jian-Hao
   Sun, Wei
   Zheng, Wei-Shi
TI Joint regression and learning from pairwise rankings for personalized
   image aesthetic assessment
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE personalized image aesthetic assessment; deep convolutional neural
   networks; pairwise ranking; regression
ID CONVOLUTIONAL NEURAL-NETWORK
AB Recent image aesthetic assessment methods have achieved remarkable progress due to the emergence of deep convolutional neural networks (CNNs). However, these methods focus primarily on predicting generally perceived preference of an image, making them usually have limited practicability, since each user may have completely different preferences for the same image. To address this problem, this paper presents a novel approach for predicting personalized image aesthetics that fit an individual user's personal taste. We achieve this in a coarse to fine manner, by joint regression and learning from pairwise rankings. Specifically, we first collect a small subset of personal images from a user and invite him/her to rank the preference of some randomly sampled image pairs. We then search for the K-nearest neighbors of the personal images within a large-scale dataset labeled with average human aesthetic scores, and use these images as well as the associated scores to train a generic aesthetic assessment model by CNN-based regression. Next, we fine-tune the generic model to accommodate the personal preference by training over the rankings with a pairwise hinge loss. Experiments demonstrate that our method can effectively learn personalized image aesthetic preferences, clearly outperforming state-of-the-art methods. Moreover, we show that the learned personalized image aesthetic benefits a wide variety of applications.
C1 [Zhou, Jin; Sun, Wei] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Peoples R China.
   [Zhang, Qing; Fan, Jian-Hao; Zheng, Wei-Shi] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Zheng, Wei-Shi] Sun Yat Sen Univ, Minist Educ, Key Lab Machine Intelligence & Adv Comp, Guangzhou 510006, Peoples R China.
   [Zheng, Wei-Shi] Peng Cheng Lab, Shenzhen 518000, Peoples R China.
C3 Sun Yat Sen University; Sun Yat Sen University; Sun Yat Sen University;
   Peng Cheng Laboratory
RP Zhang, Q (corresponding author), Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
EM zhouj289@mail2.sysu.edu.cn; zhangqing.whu.cs@gmail.com;
   fanjh8@mail2.sysu.edu.cn; sunwei@mail.sysu.edu.cn; wszheng@ieee.org
RI Zhang, Qing/ABB-1569-2021; zheng, wei/IQT-9639-2023
FU National Key Research and Development Program of China [2018YFB1004903];
   National Natural Science Foundation of China [61802453, U1911401,
   U1811461]; Fundamental Research Funds for the Central Universities
   [19lgpy216]; Research Projects of Zhejiang Lab [2019KD0AB03]
FX The authors thank the reviewers for their valuable comments. This work
   was supported partially by the National Key Research and Development
   Program of China (2018YFB1004903), National Natural Science Foundation
   of China (61802453, U1911401, U1811461), Fundamental Research Funds for
   the Central Universities (19lgpy216), and Research Projects of Zhejiang
   Lab (2019KD0AB03).
CR [Anonymous], 2013, ARXIV13124894
   Breese J., 1998, P 14 C UNC ART INT, P43
   Burges C., 2005, ICML, P89
   Cui CR, 2020, INFORM SCIENCES, V512, P780, DOI 10.1016/j.ins.2019.10.011
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng YB, 2017, IEEE SIGNAL PROC MAG, V34, P80, DOI 10.1109/MSP.2017.2696576
   Deng ZJ, 2019, IEEE I CONF COMP VIS, P2453, DOI 10.1109/ICCV.2019.00254
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Hosu V, 2019, PROC CVPR IEEE, P9367, DOI 10.1109/CVPR.2019.00960
   Joachims T, 2002, P 8 ACM SIGKDD INT C, P133, DOI [DOI 10.1145/775047.775067, 10.1145/775047.775067]
   Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40
   Konstan J., 2001, P 10 INT C WORLD WID, P285
   Korhonen J, 2019, PROC CVPR IEEE, P8161, DOI 10.1109/CVPR.2019.00836
   Lee JT, 2019, IEEE I CONF COMP VIS, P1191, DOI 10.1109/ICCV.2019.00128
   Li LD, 2019, IEEE INT CON MULTI, P430, DOI 10.1109/ICME.2019.00081
   Liu XL, 2018, PROC CVPR IEEE, P7661, DOI 10.1109/CVPR.2018.00799
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Luo W, 2011, IEEE I CONF COMP VIS, P2206, DOI 10.1109/ICCV.2011.6126498
   Mai L, 2016, PROC CVPR IEEE, P497, DOI 10.1109/CVPR.2016.60
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Myers J. L., 2010, RES DESIGN STAT ANAL
   O'Donovan P., 2014, Proceedings of the Workshop on Computational Aesthetics, P33
   Paisitkriangkrai S, 2015, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2015.7298794
   Pan BW, 2019, AAAI CONF ARTIF INTE, P679
   Park J, 2018, PROC CVPR IEEE, P5928, DOI 10.1109/CVPR.2018.00621
   Park K, 2017, IEEE WINT CONF APPL, P1206, DOI 10.1109/WACV.2017.139
   Ren J, 2017, IEEE I CONF COMP VIS, P638, DOI 10.1109/ICCV.2017.76
   Sheng KK, 2020, AAAI CONF ARTIF INTE, V34, P5709
   Sheng KK, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P879, DOI 10.1145/3240508.3240554
   Simonyan K., 2014, 14091556 ARXIV
   Talebi H, 2018, IEEE INT CONF COMPUT
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Wang GL, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P957
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang XC, 2019, COMPUT VIS MEDIA, V5, P193, DOI 10.1007/s41095-019-0131-6
   Wang YL, 2016, PROC CVPR IEEE, P6005, DOI 10.1109/CVPR.2016.646
   Yan JZ, 2014, PROC CVPR IEEE, P2987, DOI 10.1109/CVPR.2014.382
   Zeng H, 2020, IEEE T IMAGE PROCESS, V29, P1548, DOI 10.1109/TIP.2019.2941778
   Zhang FL, 2018, IEEE T MULTIMEDIA, V20, P1987, DOI 10.1109/TMM.2018.2790163
   Zhang FL, 2013, IEEE T MULTIMEDIA, V15, P1480, DOI 10.1109/TMM.2013.2268051
   Zhang Q, 2020, AAAI CONF ARTIF INTE, V34, P12845
   Zhang Q, 2021, IEEE T MULTIMEDIA, V23, P189, DOI 10.1109/TMM.2020.2982045
   Zhang Q, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P582, DOI 10.1145/3240508.3240595
   Zhang Q, 2019, COMPUT GRAPH FORUM, V38, P243, DOI 10.1111/cgf.13833
   Zhang Q, 2016, IEEE T VIS COMPUT GR, V22, P1773, DOI 10.1109/TVCG.2015.2461157
   Zhang XD, 2019, IEEE T MULTIMEDIA, V21, P2815, DOI 10.1109/TMM.2019.2911428
   Zhu HC, 2022, IEEE T CYBERNETICS, V52, P1798, DOI 10.1109/TCYB.2020.2984670
NR 49
TC 6
Z9 6
U1 0
U2 11
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2021
VL 7
IS 2
BP 241
EP 252
DI 10.1007/s41095-021-0207-y
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FR
UT WOS:000648692900007
OA gold
DA 2024-07-18
ER

PT J
AU Qiao, Z
   Kanai, T
AF Qiao, Zhi
   Kanai, Takashi
TI A GAN-based temporally stable shading model for fast animation of
   photorealistic hair
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE hair animation; fast shading; neural networks
AB We introduce an unsupervised GAN-based model for shading photorealistic hair animations. Our model is much faster than previous rendering algorithms and produces fewer artifacts than other neural image translation methods. The main idea is to extend the Cycle-GAN structure to avoid semitransparent hair appearance and to exactly reproduce the interaction of the lights with the scene. We use two constraints to ensure temporal coherence and highlight stability. Our approach outperforms and is computationally more efficient than previous methods.
C1 [Qiao, Zhi; Kanai, Takashi] Univ Tokyo, Grad Sch Arts & Sci, Dept Gen Syst Studies, Tokyo, Japan.
C3 University of Tokyo
RP Qiao, Z (corresponding author), Univ Tokyo, Grad Sch Arts & Sci, Dept Gen Syst Studies, Tokyo, Japan.
EM zhiqiao.p.r.c@gmail.com; kanait@acm.org
FU Japanese government scholarships (MEXT); JSPS KAKENHI [JP19K11990]
FX Zhi Qiao acknowledges receipt of Japanese government scholarships
   (MEXT). This work was partially supported by JSPS KAKENHI, Grant Number
   JP19K11990.
CR Bansal A, 2018, LECT NOTES COMPUT SC, V11209, P122, DOI 10.1007/978-3-030-01228-1_8
   Chai M, 2020, LECT NOTES COMPUTER, V12363, P371, DOI [10.1007/978-3-030-58523-5_22, DOI 10.1007/978-3-030-58523-5_22]
   Chen Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P647, DOI 10.1145/3343031.3350937
   d'Eon E, 2011, COMPUT GRAPH FORUM, V30, P1181, DOI 10.1111/j.1467-8659.2011.01976.x
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Hensel M, 2017, ADV NEUR IN, V30
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Janssen E, 2019, PSYCHOL HEALTH, V34, P1294, DOI 10.1080/08870446.2019.1604954
   Jing YC, 2020, IEEE T VIS COMPUT GR, V26, P3365, DOI 10.1109/TVCG.2019.2921336
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kajiya J. T., 1989, Computer Graphics, V23, P271, DOI 10.1145/74334.74361
   Luan FJ, 2017, PROC CVPR IEEE, P6997, DOI 10.1109/CVPR.2017.740
   Marschner SR, 2003, ACM T GRAPHIC, V22, P780, DOI 10.1145/882262.882345
   Moon JT, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360630
   Ren Z, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778792
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Ward K, 2007, IEEE T VIS COMPUT GR, V13, P213, DOI 10.1109/TVCG.2007.30
   Wei LY, 2018, LECT NOTES COMPUT SC, V11208, P105, DOI 10.1007/978-3-030-01225-0_7
   Yan LQ, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818080
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zinke A., 2007, AUSGEZEICHNETE INFOR
NR 22
TC 2
Z9 2
U1 1
U2 12
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2021
VL 7
IS 1
BP 127
EP 138
DI 10.1007/s41095-020-0201-9
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FQ
UT WOS:000648692800008
OA gold
DA 2024-07-18
ER

PT J
AU Yuan, J
   Chen, CJ
   Yang, WK
   Liu, MC
   Xia, JZ
   Liu, SX
AF Yuan, Jun
   Chen, Changjian
   Yang, Weikai
   Liu, Mengchen
   Xia, Jiazhi
   Liu, Shixia
TI A survey of visual analytics techniques for machine learning
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review
DE visual analytics; machine learning; data quality; feature selection;
   model understanding; content analysis
ID SOCIAL MEDIA ANALYTICS; SELF-ORGANIZING MAPS; ANOMALY DETECTION;
   INTERACTIVE EXPLORATION; QUALITY ASSESSMENT; FEATURE-SELECTION; TEXT
   COLLECTIONS; TIME; VISUALIZATION; INFORMATION
AB Visual analytics for machine learning has recently evolved as one of the most exciting areas in the field of visualization. To better identify which research topics are promising and to learn how to apply relevant techniques in visual analytics, we systematically review 259 papers published in the last ten years together with representative works before 2010. We build a taxonomy, which includes three first-level categories: techniques before model building, techniques during modeling building, and techniques after model building. Each category is further characterized by representative analysis tasks, and each task is exemplified by a set of recent influential works. We also discuss and highlight research challenges and promising potential future research opportunities useful for visual analytics researchers.
C1 [Yuan, Jun; Chen, Changjian; Yang, Weikai; Liu, Shixia] Tsinghua Univ, BNRist, Beijing 100086, Peoples R China.
   [Liu, Mengchen] Microsoft, Redmond, WA 98052 USA.
   [Xia, Jiazhi] Cent South Univ, Changsha 410083, Peoples R China.
C3 Tsinghua University; Microsoft; Central South University
RP Liu, SX (corresponding author), Tsinghua Univ, BNRist, Beijing 100086, Peoples R China.
EM yuanj19@mails.tsinghua.edu.cn; ccj17@mails.tsinghua.edu.cn;
   ywk19@mails.tsinghua.edu.cn; mengcliu@microsoft.com;
   xiajiazhi@csu.edu.cn; shixia@tsinghua.edu.cn
RI Chen, Changjian/KBA-9462-2024; Liu, Shi-Xia/C-5574-2016
OI Chen, Changjian/0000-0003-2715-8839; Liu, Shi-Xia/0000-0001-6104-4320;
   Yuan, Jun/0000-0002-8160-6383; yang, wei kai/0000-0002-6520-1642
FU National Key R&D Program of China [2018YFB1004300, 2019YFB1405703];
   National Natural Science Foundation of China [61761136020, 61672307,
   61672308, 61936002, TC190A4DA/3]; Tsinghua-Kuaishou Institute of Future
   Media Data
FX This research is supported by the National Key R&D Program of China
   (Nos. 2018YFB1004300 and 2019YFB1405703), the National Natural Science
   Foundation of China (Nos. 61761136020, 61672307, 61672308, and
   61936002), TC190A4DA/3, and in part by Tsinghua-Kuaishou Institute of
   Future Media Data.
CR Abadi M, ARXIV, DOI DOI 10.48550/ARXIV.1603.04467
   ACM, 2012, T INTELLIGENT SYSTEM, V3, P2
   Ahn Y, 2020, IEEE T VIS COMPUT GR, V26, P1086, DOI 10.1109/TVCG.2019.2934262
   Alemzadeh S, 2020, COMPUT GRAPH FORUM, V39, P63, DOI 10.1111/cgf.13662
   Alexander E, 2014, IEEE CONF VIS ANAL, P173, DOI 10.1109/VAST.2014.7042493
   Alsakran J, 2011, IEEE PAC VIS SYMP, P131, DOI 10.1109/PACIFICVIS.2011.5742382
   Alsakran J, 2012, IEEE COMPUT GRAPH, V32, P34, DOI 10.1109/MCG.2011.91
   Alsallakh B, 2018, IEEE T VIS COMPUT GR, V24, P152, DOI 10.1109/TVCG.2017.2744683
   Alsallakh B, 2014, IEEE T VIS COMPUT GR, V20, P1703, DOI 10.1109/TVCG.2014.2346660
   Andrienko G, 2010, COMPUT GRAPH FORUM, V29, P913, DOI 10.1111/j.1467-8659.2009.01664.x
   Andrienko G, 2021, IEEE T VIS COMPUT GR, V27, P2280, DOI 10.1109/TVCG.2019.2952129
   Andrienko G, 2013, IEEE T VIS COMPUT GR, V19, P1078, DOI 10.1109/TVCG.2012.311
   [Anonymous], 2015, P 28 INT C NEUR INF
   [Anonymous], 2015, DECISION COMMITTEE E
   [Anonymous], 2010, P 16 ACM SIGKDD INT
   Arbesser C, 2017, IEEE T VIS COMPUT GR, V23, P641, DOI 10.1109/TVCG.2016.2598592
   Ayinde B. O., 2018, ARXIV PREPRINT ARXIV
   Bäuerle A, 2020, COMPUT GRAPH FORUM, V39, P195, DOI 10.1111/cgf.13973
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Berger M, 2017, IEEE T VIS COMPUT GR, V23, P691, DOI 10.1109/TVCG.2016.2598667
   Bernard J, 2019, COMPUT GRAPH FORUM, V38, P401, DOI 10.1111/cgf.13698
   Bernard J, 2018, COMPUT GRAPH FORUM, V37, P121, DOI 10.1111/cgf.13406
   Bernard J, 2018, IEEE T VIS COMPUT GR, V24, P298, DOI 10.1109/TVCG.2017.2744818
   Blascheck T, 2016, IEEE CONF VIS ANAL, P141, DOI 10.1109/VAST.2016.7883520
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Blumenschein M, 2018, IEEE CONF VIS ANAL, P36, DOI 10.1109/VAST.2018.8802486
   Bögl M, 2013, IEEE T VIS COMPUT GR, V19, P2237, DOI 10.1109/TVCG.2013.222
   Bögl M, 2017, COMPUT GRAPH FORUM, V36, P227, DOI 10.1111/cgf.13182
   Bors C, 2019, IEEE COMPUT GRAPH, V39, P61, DOI 10.1109/MCG.2019.2941856
   Bosch H, 2013, IEEE T VIS COMPUT GR, V19, P2022, DOI 10.1109/TVCG.2013.186
   Bradel L, 2014, IEEE CONF VIS ANAL, P163, DOI 10.1109/VAST.2014.7042492
   Broeksema B, 2013, COMPUT GRAPH FORUM, V32, P158, DOI 10.1111/cgf.12194
   Broeksema B, 2013, IEEE T VIS COMPUT GR, V19, P1972, DOI 10.1109/TVCG.2013.146
   Brooks M, 2015, IEEE CONF VIS ANAL, P105, DOI 10.1109/VAST.2015.7347637
   Buchmüller J, 2015, COMPUT GRAPH FORUM, V34, P181, DOI 10.1111/cgf.12630
   Cabrera AA, 2019, IEEE CONF VIS ANAL, P46, DOI [10.1109/vast47406.2019.8986948, 10.1109/VAST47406.2019.8986948]
   Cao KL, 2021, IEEE T VIS COMPUT GR, V27, P3289, DOI 10.1109/TVCG.2020.2969185
   Cao N, 2018, IEEE T VIS COMPUT GR, V24, P23, DOI 10.1109/TVCG.2017.2744419
   Cao N, 2016, IEEE T VIS COMPUT GR, V22, P280, DOI 10.1109/TVCG.2015.2467196
   Cao N, 2012, IEEE T VIS COMPUT GR, V18, P2649, DOI 10.1109/TVCG.2012.291
   Cao N, 2010, IEEE T VIS COMPUT GR, V16, P1172, DOI 10.1109/TVCG.2010.154
   Cashman D, 2020, IEEE T VIS COMPUT GR, V26, P863, DOI 10.1109/TVCG.2019.2934261
   Cashman D, 2018, IEEE COMPUT GRAPH, V38, P39, DOI 10.1109/MCG.2018.2878902
   Cavallo M, 2019, IEEE T VIS COMPUT GR, V25, P267, DOI 10.1109/TVCG.2018.2864477
   Cavallo M, 2018, COMPUT GRAPH FORUM, V37, P339, DOI 10.1111/cgf.13424
   Chae J, 2012, IEEE CONF VIS ANAL, P143, DOI 10.1109/VAST.2012.6400557
   Chandrasegaran S, 2017, COMPUT GRAPH FORUM, V36, P201, DOI 10.1111/cgf.13180
   Chandrashekar G, 2014, COMPUT ELECTR ENG, V40, P16, DOI 10.1016/j.compeleceng.2013.11.024
   Chen C. J., 2020, IEEE T VIS COMPUT GR, DOI [10.1109/TVCG.2020.2973258, DOI 10.1109/TVCG.2020.2973258]
   Chen Q, 2020, IEEE T VIS COMPUT GR, V26, P1622, DOI 10.1109/TVCG.2018.2872961
   Chen S, 2020, IEEE T VIS COMPUT GR, V26, P2775, DOI 10.1109/TVCG.2019.2904069
   Chen SM, 2017, IEEE CONF VIS ANAL, P36, DOI 10.1109/VAST.2017.8585638
   Chen SM, 2016, IEEE CONF VIS ANAL, P41, DOI 10.1109/VAST.2016.7883510
   Chen SM, 2016, IEEE T VIS COMPUT GR, V22, P270, DOI 10.1109/TVCG.2015.2467619
   Chen YZ, 2018, IEEE T VIS COMPUT GR, V24, P45, DOI 10.1109/TVCG.2017.2745083
   Chen YZ, 2016, IEEE CONF VIS ANAL, P111, DOI 10.1109/VAST.2016.7883517
   Choo J, 2018, IEEE COMPUT GRAPH, V38, P84, DOI 10.1109/MCG.2018.042731661
   Choo J, 2013, IEEE T VIS COMPUT GR, V19, P1992, DOI 10.1109/TVCG.2013.212
   Chu D, 2014, IEEE PAC VIS SYMP, P137, DOI 10.1109/PacificVis.2014.50
   Collaris D, 2020, IEEE PAC VIS SYMP, P26, DOI 10.1109/PacificVis48177.2020.7090
   Correll M, 2011, COMPUT GRAPH FORUM, V30, P731, DOI 10.1111/j.1467-8659.2011.01922.x
   Cui WW, 2014, IEEE T VIS COMPUT GR, V20, P2281, DOI 10.1109/TVCG.2014.2346433
   Cui WW, 2011, IEEE T VIS COMPUT GR, V17, P2412, DOI 10.1109/TVCG.2011.239
   Das S, 2019, IEEE COMPUT GRAPH, V39, P20, DOI 10.1109/MCG.2019.2922592
   de Rooij O, 2010, IEEE COMPUT GRAPH, V30, P42, DOI 10.1109/MCG.2010.66
   Dextras-Romagnino K, 2019, COMPUT GRAPH FORUM, V38, P623, DOI 10.1111/cgf.13715
   Di Lorenzo G, 2016, IEEE T VIS COMPUT GR, V22, P1036, DOI 10.1109/TVCG.2015.2440259
   Diehl A, 2017, COMPUT GRAPH FORUM, V36, P135, DOI 10.1111/cgf.13279
   Dingen D, 2019, IEEE T VIS COMPUT GR, V25, P246, DOI 10.1109/TVCG.2018.2865043
   Donahue J, 2014, PR MACH LEARN RES, V32
   Dou WW, 2015, IEEE CONF VIS ANAL, P57, DOI 10.1109/VAST.2015.7347631
   Dou WW, 2013, IEEE T VIS COMPUT GR, V19, P2002, DOI 10.1109/TVCG.2013.162
   Dou WW, 2012, IEEE CONF VIS ANAL, P93, DOI 10.1109/VAST.2012.6400485
   Du F, 2016, IEEE CONF VIS ANAL, P61, DOI 10.1109/VAST.2016.7883512
   Eichner C, 2020, COMPUT GRAPH FORUM, V39, P607, DOI 10.1111/cgf.13894
   El-Assady M, 2020, IEEE T VIS COMPUT GR, V26, P1001, DOI 10.1109/TVCG.2019.2934654
   El-Assady M, 2019, IEEE T VIS COMPUT GR, V25, P374, DOI 10.1109/TVCG.2018.2864769
   El-Assady M, 2018, COMPUT GRAPH FORUM, V37, P351, DOI 10.1111/cgf.13425
   El-Assady M, 2018, IEEE T VIS COMPUT GR, V24, P382, DOI 10.1109/TVCG.2017.2745080
   El-Assady M, 2017, COMPUT GRAPH FORUM, V36, P213, DOI 10.1111/cgf.13181
   El-Assady M, 2016, COMPUT GRAPH FORUM, V35, P431, DOI 10.1111/cgf.12919
   Fan MM, 2020, IEEE T VIS COMPUT GR, V26, P343, DOI 10.1109/TVCG.2019.2934797
   Ferreira N, 2013, IEEE T VIS COMPUT GR, V19, P2149, DOI 10.1109/TVCG.2013.226
   Ferreira N, 2011, IEEE T VIS COMPUT GR, V17, P2374, DOI 10.1109/TVCG.2011.176
   Filipov V, 2019, COMPUT GRAPH FORUM, V38, P107, DOI 10.1111/cgf.13675
   Foulds J, 2010, KNOWL ENG REV, V25, P1, DOI 10.1017/S026988890999035X
   Fried D, 2014, IEEE PAC VIS SYMP, P113, DOI 10.1109/PacificVis.2014.47
   Fröhler B, 2016, COMPUT GRAPH FORUM, V35, P191, DOI 10.1111/cgf.12895
   Fulda J, 2016, IEEE T VIS COMPUT GR, V22, P300, DOI 10.1109/TVCG.2015.2467531
   Garcia G, 2021, IEEE T VIS COMPUT GR, V27, P2313, DOI 10.1109/TVCG.2019.2947515
   Gleicher M, 2020, COMPUT GRAPH FORUM, V39, P181, DOI 10.1111/cgf.13972
   Glueck M, 2018, IEEE T VIS COMPUT GR, V24, P371, DOI 10.1109/TVCG.2017.2745118
   Gobbo B, 2019, COMPUT GRAPH FORUM, V38, P609, DOI 10.1111/cgf.13714
   Görg C, 2013, IEEE T VIS COMPUT GR, V19, P1646, DOI 10.1109/TVCG.2012.324
   Gotz D, 2014, IEEE T VIS COMPUT GR, V20, P1783, DOI 10.1109/TVCG.2014.2346682
   Gschwandtner T, 2018, IEEE PAC VIS SYMP, P205, DOI 10.1109/PacificVis.2018.00034
   Guo H, 2020, IEEE T VIS COMPUT GR, V26, P1592, DOI 10.1109/TVCG.2018.2873011
   Guo SN, 2019, IEEE T VIS COMPUT GR, V25, P417, DOI 10.1109/TVCG.2018.2864885
   Guo SN, 2018, IEEE T VIS COMPUT GR, V24, P56, DOI 10.1109/TVCG.2017.2745320
   Gutenko L, 2017, IEEE T VIS COMPUT GR, V23, P171, DOI 10.1109/TVCG.2016.2598463
   Halter G, 2019, COMPUT GRAPH FORUM, V38, P119, DOI 10.1111/cgf.13676
   Havre S, 2002, IEEE T VIS COMPUT GR, V8, P9, DOI 10.1109/2945.981848
   He WB, 2020, IEEE PAC VIS SYMP, P36, DOI 10.1109/PacificVis48177.2020.7127
   Heimerl F, 2016, IEEE CONF VIS ANAL, P11, DOI 10.1109/VAST.2016.7883507
   Heimerl F, 2016, IEEE T VIS COMPUT GR, V22, P190, DOI 10.1109/TVCG.2015.2467621
   Heimerl F, 2012, IEEE T VIS COMPUT GR, V18, P2839, DOI 10.1109/TVCG.2012.277
   Höferlin B, 2012, IEEE CONF VIS ANAL, P23, DOI 10.1109/VAST.2012.6400492
   Hohman F, 2019, IEEE T VIS COMPUT GR, V25, P2674, DOI 10.1109/TVCG.2018.2843369
   Hohman F, 2020, IEEE T VIS COMPUT GR, V26, P1096, DOI 10.1109/TVCG.2019.2934659
   Hong F, 2014, IEEE T VIS COMPUT GR, V20, P2545, DOI 10.1109/TVCG.2014.2346416
   Hoque E, 2014, COMPUT GRAPH FORUM, V33, P221, DOI 10.1111/cgf.12378
   Hu MD, 2017, IEEE T VIS COMPUT GR, V23, P621, DOI 10.1109/TVCG.2016.2598590
   Ingram S., 2010, 2010 Proceedings of IEEE Symposium on Visual Analytics Science and Technology (VAST 2010), P3, DOI 10.1109/VAST.2010.5652392
   Itoh M, 2014, IEEE PAC VIS SYMP, P129, DOI 10.1109/PacificVis.2014.49
   Itoh M, 2012, IEEE PAC VIS SYMP, P57, DOI 10.1109/PacificVis.2012.6183574
   Jänicke S, 2017, IEEE CONF VIS ANAL, P127, DOI 10.1109/VAST.2017.8585505
   Jänicke H, 2010, COMPUT GRAPH FORUM, V29, P357, DOI 10.1111/j.1467-8659.2009.01605.x
   Jankowska M, 2012, IEEE CONF VIS ANAL, P103, DOI 10.1109/VAST.2012.6400484
   Jaunet T, 2020, COMPUT GRAPH FORUM, V39, P49, DOI 10.1111/cgf.13962
   Ji XN, 2019, IEEE T VIS COMPUT GR, V25, P2181, DOI 10.1109/TVCG.2019.2903946
   Kahng M, 2019, IEEE T VIS COMPUT GR, V25, P310, DOI 10.1109/TVCG.2018.2864500
   Kahng M, 2018, IEEE T VIS COMPUT GR, V24, P88, DOI 10.1109/TVCG.2017.2744718
   Kakar T, 2019, COMPUT GRAPH FORUM, V38, P95, DOI 10.1111/cgf.13674
   Kamaleswaran R, 2016, COMPUT GRAPH FORUM, V35, P331, DOI 10.1111/cgf.12909
   Kandel S, 2012, PROCEEDINGS OF THE INTERNATIONAL WORKING CONFERENCE ON ADVANCED VISUAL INTERFACES, P547, DOI 10.1145/2254556.2254659
   Karduni A, 2017, IEEE COMPUT GRAPH, V37, P50, DOI 10.1109/MCG.2017.3621223
   Khayat M, 2020, IEEE T VIS COMPUT GR, V26, P874, DOI 10.1109/TVCG.2019.2934266
   Kim H, 2019, IEEE CONF VIS ANAL, P35, DOI [10.1109/VAST47406.2019.8986922, 10.1109/vast47406.2019.8986922]
   Kim Minjeong, 2017, IEEE Trans Vis Comput Graph, V23, P151, DOI 10.1109/TVCG.2016.2598445
   Kochtchi A, 2014, COMPUT GRAPH FORUM, V33, P211, DOI 10.1111/cgf.12377
   Kosara R, 2006, IEEE T VIS COMPUT GR, V12, P558, DOI 10.1109/TVCG.2006.76
   Krause J, 2017, IEEE CONF VIS ANAL, P162, DOI 10.1109/VAST.2017.8585720
   Krause J, 2014, IEEE T VIS COMPUT GR, V20, P1614, DOI 10.1109/TVCG.2014.2346482
   Krueger R, 2019, COMPUT GRAPH FORUM, V38, P595, DOI 10.1111/cgf.13713
   Krueger R, 2015, IEEE T VIS COMPUT GR, V21, P903, DOI 10.1109/TVCG.2014.2371856
   Krueger R, 2014, IEEE PAC VIS SYMP, P193, DOI 10.1109/PacificVis.2014.57
   Kurzhals K, 2017, IEEE T VIS COMPUT GR, V23, P301, DOI 10.1109/TVCG.2016.2598695
   Kwon BC, 2021, IEEE T VIS COMPUT GR, V27, P3685, DOI 10.1109/TVCG.2020.2985689
   Kwon BC, 2019, IEEE T VIS COMPUT GR, V25, P299, DOI 10.1109/TVCG.2018.2865027
   Lakshminarayanan B, 2017, ADV NEUR IN, V30
   Lee C, 2020, IEEE T VIS COMPUT GR, V26, P3133, DOI 10.1109/TVCG.2019.2922597
   Lee H, 2012, COMPUT GRAPH FORUM, V31, P1155, DOI 10.1111/j.1467-8659.2012.03108.x
   Lee K., 2018, ARXIV PREPRINT ARXIV
   Leite RA, 2018, IEEE T VIS COMPUT GR, V24, P330, DOI 10.1109/TVCG.2017.2744758
   Lekschas F., 2020, BIORXIV 597518, DOI [10.1101/597518, DOI 10.1101/597518]
   Li J, 2019, IEEE Transactions on Visualization and Computer Graphics, V26, P1806
   Li MZ, 2018, COMPUT GRAPH FORUM, V37, P217, DOI 10.1111/cgf.13414
   Li Q, 2020, IEEE T VIS COMPUT GR, V26, P1399, DOI 10.1109/TVCG.2018.2867776
   Li YN, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1725, DOI 10.1145/2939672.2939842
   Li YF, 2021, IEEE T PATTERN ANAL, V43, P334, DOI 10.1109/TPAMI.2019.2922396
   Li ZY, 2020, IEEE T VIS COMPUT GR, V26, P1182, DOI 10.1109/TVCG.2019.2934667
   Liu H, 2019, VIS INFORM, V3, P140, DOI 10.1016/j.visinf.2019.10.002
   Liu MC, 2018, IEEE T VIS COMPUT GR, V24, P77, DOI 10.1109/TVCG.2017.2744938
   Liu MC, 2017, IEEE T VIS COMPUT GR, V23, P91, DOI 10.1109/TVCG.2016.2598831
   Liu MC, 2016, IEEE T VIS COMPUT GR, V22, P250, DOI 10.1109/TVCG.2015.2467554
   Liu Mengchen, 2017, IJCAI, P2329
   Liu S, 2015, COMPUT GRAPH FORUM, V34, P271, DOI 10.1111/cgf.12639
   Liu SX, 2019, IEEE T VIS COMPUT GR, V25, P2482, DOI 10.1109/TVCG.2018.2834341
   Liu SX, 2018, VIS INFORM, V2, P191, DOI 10.1016/j.visinf.2018.12.001
   Liu SX, 2019, IEEE T VIS COMPUT GR, V25, P235, DOI 10.1109/TVCG.2018.2864843
   Liu SX, 2018, IEEE T VIS COMPUT GR, V24, P163, DOI 10.1109/TVCG.2017.2744378
   Liu SX, 2017, VIS INFORM, V1, P48, DOI 10.1016/j.visinf.2017.01.006
   Liu SX, 2016, IEEE T VIS COMPUT GR, V22, P2451, DOI 10.1109/TVCG.2015.2509990
   Liu SX, 2014, IEEE CONF VIS ANAL, P183, DOI 10.1109/VAST.2014.7042494
   Liu SX, 2014, VISUAL COMPUT, V30, P1373, DOI 10.1007/s00371-013-0892-3
   Liu SS, 2019, IEEE T VIS COMPUT GR, V25, P651, DOI 10.1109/TVCG.2018.2865230
   Liu XT, 2016, IEEE CONF VIS ANAL, P71, DOI 10.1109/VAST.2016.7883513
   Liu ZC, 2017, COMPUT GRAPH FORUM, V36, P527, DOI 10.1111/cgf.13208
   Liu ZC, 2017, IEEE T VIS COMPUT GR, V23, P321, DOI 10.1109/TVCG.2016.2598797
   Löwe T, 2016, IEEE T VIS COMPUT GR, V22, P151, DOI 10.1109/TVCG.2015.2467612
   Lu JS, 2019, ADV NEUR IN, V32
   Lu J, 2019, IEEE T KNOWL DATA EN, V31, P2346, DOI 10.1109/TKDE.2018.2876857
   Lu YF, 2018, IEEE T VIS COMPUT GR, V24, P2501, DOI 10.1109/TVCG.2017.2752166
   Lu YF, 2017, COMPUT GRAPH FORUM, V36, P539, DOI 10.1111/cgf.13210
   Lu YF, 2016, IEEE T VIS COMPUT GR, V22, P220, DOI 10.1109/TVCG.2015.2467991
   Lu YF, 2014, IEEE COMPUT GRAPH, V34, P58, DOI 10.1109/MCG.2014.61
   Luo DN, 2012, IEEE T VIS COMPUT GR, V18, P93, DOI 10.1109/TVCG.2010.225
   Ma YX, 2020, IEEE T VIS COMPUT GR, V26, P1075, DOI 10.1109/TVCG.2019.2934631
   Ma ZQ, 2013, 2013 IEEE/WIC/ACM INTERNATIONAL JOINT CONFERENCES ON WEB INTELLIGENCE (WI) AND INTELLIGENT AGENT TECHNOLOGIES (IAT), VOL 1, P260, DOI 10.1109/WI-IAT.2013.38
   Maciejewski R, 2011, IEEE T VIS COMPUT GR, V17, P440, DOI 10.1109/TVCG.2010.82
   MacInnes J, 2010, IEEE COMPUT GRAPH, V30, P8, DOI 10.1109/MCG.2010.18
   Malik A, 2014, IEEE T VIS COMPUT GR, V20, P1863, DOI 10.1109/TVCG.2014.2346926
   May T., 2011, 2011 IEEE Conference on Visual Analytics Science and Technology, P111, DOI 10.1109/VAST.2011.6102448
   Migut M., 2010, 2010 Proceedings of IEEE Symposium on Visual Analytics Science and Technology (VAST 2010), P11, DOI 10.1109/VAST.2010.5652398
   Migut M. A., 2011, 2011 IEEE Conference on Visual Analytics Science and Technology, P141, DOI 10.1109/VAST.2011.6102451
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Ming Y, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P903, DOI 10.1145/3292500.3330908
   Ming Y, 2020, IEEE T VIS COMPUT GR, V26, P238, DOI 10.1109/TVCG.2019.2934267
   Ming Y, 2017, IEEE CONF VIS ANAL, P13, DOI 10.1109/VAST.2017.8585721
   Ming Y, 2019, IEEE T VIS COMPUT GR, V25, P342, DOI 10.1109/TVCG.2018.2864812
   Miranda F, 2017, IEEE T VIS COMPUT GR, V23, P791, DOI 10.1109/TVCG.2016.2598585
   Moehrmann J, 2011, LECT NOTES COMPUT SC, V6761, P618, DOI 10.1007/978-3-642-21602-2_67
   Mühlbacher T, 2018, IEEE T VIS COMPUT GR, V24, P174, DOI 10.1109/TVCG.2017.2745158
   Mühlbacher T, 2013, IEEE T VIS COMPUT GR, V19, P1962, DOI 10.1109/TVCG.2013.125
   Murugesan S, 2019, IEEE COMPUT GRAPH, V39, P47, DOI 10.1109/MCG.2019.2919033
   Ng A., 2013, Machine learning and AI via brain simulations
   Hung NQV, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P999, DOI 10.1145/2723372.2723731
   Nie SL, 2018, IEEE PAC VIS SYMP, P180, DOI 10.1109/PacificVis.2018.00031
   Nilsson N. J., 2005, MACH LEARN, V56, P387
   Oelke D, 2014, COMPUT GRAPH FORUM, V33, P201, DOI 10.1111/cgf.12376
   Packer E, 2013, IEEE T VIS COMPUT GR, V19, P2179, DOI 10.1109/TVCG.2013.224
   Paiva JGS, 2015, IEEE T VIS COMPUT GR, V21, P4, DOI 10.1109/TVCG.2014.2331979
   Park D, 2018, IEEE T VIS COMPUT GR, V24, P361, DOI 10.1109/TVCG.2017.2744478
   Park H, 2020, IEEE T VIS COMPUT GR, DOI DOI 10.1109/TVCG.2020.2981456
   Park JH, 2021, IEEE T VIS COMPUT GR, V27, P2869, DOI 10.1109/TVCG.2019.2953026
   Park JH, 2016, IEEE CONF VIS ANAL, P21, DOI 10.1109/VAST.2016.7883508
   Paulovich FV, 2012, COMPUT GRAPH FORUM, V31, P1145, DOI 10.1111/j.1467-8659.2012.03107.x
   Pezzotti N, 2018, IEEE T VIS COMPUT GR, V24, P98, DOI 10.1109/TVCG.2017.2744358
   Piringer H, 2010, COMPUT GRAPH FORUM, V29, P983, DOI 10.1111/j.1467-8659.2009.01684.x
   Purwantiningsih O, 2016, IEEE PAC VIS SYMP, P229, DOI 10.1109/PACIFICVIS.2016.7465276
   Rauber PE, 2017, IEEE T VIS COMPUT GR, V23, P101, DOI 10.1109/TVCG.2016.2598838
   Ren DH, 2017, IEEE T VIS COMPUT GR, V23, P61, DOI 10.1109/TVCG.2016.2598828
   Riehmann P, 2019, IEEE T VIS COMPUT GR, V25, P1803, DOI 10.1109/TVCG.2018.2824822
   Röhlig M, 2015, IEEE CONF VIS ANAL, P41, DOI 10.1109/VAST.2015.7347629
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sacha D, 2017, COMPUT GRAPH FORUM, V36, P305, DOI 10.1111/cgf.13189
   Sacha D, 2019, IEEE T VIS COMPUT GR, V25, P385, DOI 10.1109/TVCG.2018.2864838
   Sacha D, 2018, IEEE T VIS COMPUT GR, V24, P120, DOI 10.1109/TVCG.2017.2744805
   Sarikaya A, 2016, COMPUT GRAPH FORUM, V35, P151, DOI 10.1111/cgf.12891
   Scheepens R, 2015, COMPUT GRAPH FORUM, V34, P191, DOI 10.1111/cgf.12631
   Schultz T, 2013, IEEE T VIS COMPUT GR, V19, P2100, DOI 10.1109/TVCG.2013.181
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Seo J., 2005, Information Visualization, V4, P96, DOI 10.1057/palgrave.ivs.9500091
   Shen QM, 2020, IEEE PAC VIS SYMP, P61, DOI 10.1109/PacificVis48177.2020.2785
   Shen QM, 2018, IEEE T VIS COMPUT GR, V24, P1004, DOI 10.1109/TVCG.2017.2744159
   Shi CL, 2014, IEEE T VIS COMPUT GR, V20, P1733, DOI 10.1109/TVCG.2014.2346912
   Snyder LS, 2020, IEEE T VIS COMPUT GR, V26, P558, DOI 10.1109/TVCG.2019.2934614
   Soares A, 2017, IEEE COMPUT GRAPH, V37, P28, DOI 10.1109/MCG.2017.3621221
   Sperrle F, 2019, IEEE CONF VIS ANAL, P11, DOI [10.1109/vast47406.2019.8986917, 10.1109/VAST47406.2019.8986917]
   Spinner T, 2020, IEEE T VIS COMPUT GR, V26, P1064, DOI 10.1109/TVCG.2019.2934629
   St Jean C, 2016, COMPUT GRAPH FORUM, V35, P311, DOI 10.1111/cgf.12907
   Steiger M, 2014, COMPUT GRAPH FORUM, V33, P401, DOI 10.1111/cgf.12396
   Stein M, 2016, IEEE COMPUT GRAPH, V36, P50, DOI 10.1109/MCG.2016.102
   Stopar L, 2019, IEEE T VIS COMPUT GR, V25, P1788, DOI 10.1109/TVCG.2018.2825424
   Strobelt H, 2019, IEEE T VIS COMPUT GR, V25, P353, DOI 10.1109/TVCG.2018.2865044
   Strobelt H, 2018, IEEE T VIS COMPUT GR, V24, P667, DOI 10.1109/TVCG.2017.2744158
   Sultanum N, 2019, IEEE T VIS COMPUT GR, V25, P142, DOI 10.1109/TVCG.2018.2864905
   Sun GD, 2014, IEEE T VIS COMPUT GR, V20, P1753, DOI 10.1109/TVCG.2014.2346919
   Sung CY, 2017, COMPUT GRAPH FORUM, V36, P145, DOI 10.1111/cgf.13280
   Tam GKL, 2011, COMPUT GRAPH FORUM, V30, P901, DOI 10.1111/j.1467-8659.2011.01939.x
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
   Thom D, 2016, IEEE T VIS COMPUT GR, V22, P1816, DOI 10.1109/TVCG.2015.2511733
   Thom D, 2012, IEEE PAC VIS SYMP, P41, DOI 10.1109/PacificVis.2012.6183572
   Tkachev G, 2021, IEEE T VIS COMPUT GR, V27, P3091, DOI 10.1109/TVCG.2019.2961893
   Tzeng FY, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P383
   van den Elzen S., 2011, 2011 IEEE Conference on Visual Analytics Science and Technology, P151, DOI 10.1109/VAST.2011.6102453
   Vehlow C, 2015, COMPUT GRAPH FORUM, V34, P277, DOI 10.1111/cgf.12512
   von Landesberger T, 2016, IEEE T VIS COMPUT GR, V22, P2537, DOI 10.1109/TVCG.2015.2501813
   von Landesberger T, 2016, IEEE T VIS COMPUT GR, V22, P11, DOI 10.1109/TVCG.2015.2468111
   Vrotsou K, 2019, IEEE T VIS COMPUT GR, V25, P2597, DOI 10.1109/TVCG.2018.2848247
   Wall E, 2018, IEEE T VIS COMPUT GR, V24, P288, DOI 10.1109/TVCG.2017.2745078
   Wang JP, 2020, IEEE PAC VIS SYMP, P51, DOI 10.1109/PacificVis48177.2020.3542
   Wang JP, 2019, IEEE T VIS COMPUT GR, V25, P2168, DOI 10.1109/TVCG.2019.2903943
   Wang JP, 2019, IEEE T VIS COMPUT GR, V25, P288, DOI 10.1109/TVCG.2018.2864504
   Wang JP, 2018, IEEE T VIS COMPUT GR, V24, P1905, DOI 10.1109/TVCG.2018.2816223
   Wang QW, 2020, IEEE T VIS COMPUT GR, V26, P3340, DOI 10.1109/TVCG.2019.2921323
   Wang X, 2012, COMPUT GRAPH FORUM, V31, P1275, DOI 10.1111/j.1467-8659.2012.03120.x
   Wang X., 2020, ARXIV200715272
   Wang XT, 2016, IEEE CONF VIS ANAL, P51, DOI 10.1109/VAST.2016.7883511
   Wang XT, 2016, IEEE T VIS COMPUT GR, V22, P2508, DOI 10.1109/TVCG.2016.2515592
   Wang XT, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P722
   Wang XM, 2019, IEEE T VIS COMPUT GR, V25, P193, DOI 10.1109/TVCG.2018.2865021
   Wang XM, 2018, IEEE T VIS COMPUT GR, V24, P351, DOI 10.1109/TVCG.2017.2745139
   Wang Y, 2018, COMPUT GRAPH FORUM, V37, P63, DOI 10.1111/cgf.13401
   Wei JS, 2012, IEEE CONF VIS ANAL, P3, DOI 10.1109/VAST.2012.6400494
   Wenwen Dou, 2011, 2011 IEEE Conference on Visual Analytics Science and Technology, P231, DOI 10.1109/VAST.2011.6102461
   Wexler J, 2020, IEEE T VIS COMPUT GR, V26, P56, DOI 10.1109/TVCG.2019.2934619
   Willett W, 2013, IEEE T VIS COMPUT GR, V19, P2198, DOI 10.1109/TVCG.2013.164
   Wongsuphasawat K, 2018, IEEE T VIS COMPUT GR, V24, P1, DOI 10.1109/TVCG.2017.2744878
   Wu AY, 2020, IEEE T VIS COMPUT GR, V26, P2429, DOI 10.1109/TVCG.2018.2889081
   Wu WC, 2017, IEEE PAC VIS SYMP, P91, DOI 10.1109/PACIFICVIS.2017.8031583
   Wu YH, 2016, IEEE T VIS COMPUT GR, V22, P260, DOI [10.1109/TVCG.2015.2468151, 10.1109/TVCG.2015.2465151]
   Wu YC, 2018, IEEE T VIS COMPUT GR, V24, P2758, DOI 10.1109/TVCG.2017.2764459
   Wu YC, 2014, IEEE T VIS COMPUT GR, V20, P1763, DOI 10.1109/TVCG.2014.2346920
   Xiang SX, 2019, IEEE CONF VIS ANAL, P57, DOI [10.1109/vast47406.2019.8986943, 10.1109/VAST47406.2019.8986943]
   Xie C, 2014, IEEE T VIS COMPUT GR, V20, P1743, DOI 10.1109/TVCG.2014.2346913
   Xie X, 2019, IEEE T VIS COMPUT GR, V25, P2362, DOI 10.1109/TVCG.2018.2835485
   Xu J, 2020, IEEE T VIS COMPUT GR, V26, P2387, DOI 10.1109/TVCG.2018.2887230
   Xu J, 2017, IEEE PAC VIS SYMP, P240, DOI 10.1109/PACIFICVIS.2017.8031600
   Xu PP, 2017, IEEE T VIS COMPUT GR, V23, P291, DOI 10.1109/TVCG.2016.2598664
   Xu PP, 2013, IEEE T VIS COMPUT GR, V19, P2012, DOI 10.1109/TVCG.2013.221
   Yang W., 2020, IEEE C VIS AN SCI TE
   Yang WK, 2021, IEEE T VIS COMPUT GR, V27, P3953, DOI 10.1109/TVCG.2020.2995100
   Yu L, 2015, IEEE CONF VIS ANAL, P49, DOI 10.1109/VAST.2015.7347630
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zeng HP, 2021, IEEE T VIS COMPUT GR, V27, P3168, DOI 10.1109/TVCG.2019.2963659
   Zeng HP, 2020, IEEE T VIS COMPUT GR, V26, P927, DOI 10.1109/TVCG.2019.2934656
   Zeng W, 2016, COMPUT GRAPH FORUM, V35, P95, DOI 10.1111/cgf.12778
   Zhang C, 2016, IEEE PAC VIS SYMP, P136, DOI 10.1109/PACIFICVIS.2016.7465261
   Zhang JW, 2014, IEEE T VIS COMPUT GR, V20, P1843, DOI 10.1109/TVCG.2014.2346898
   Zhang JW, 2019, IEEE T VIS COMPUT GR, V25, P364, DOI 10.1109/TVCG.2018.2864499
   Zhang JW, 2016, COMPUT GRAPH FORUM, V35, P441, DOI 10.1111/cgf.12920
   Zhang L, 2012, COMPUT GRAPH FORUM, V31, P2173, DOI 10.1111/j.1467-8659.2012.03210.x
   Zhang QS, 2018, FRONT INFORM TECH EL, V19, P27, DOI 10.1631/FITEE.1700808
   Zhao J, 2014, IEEE T VIS COMPUT GR, V20, P1773, DOI 10.1109/TVCG.2014.2346922
   Zhao J, 2012, IEEE T VIS COMPUT GR, V18, P2639, DOI 10.1109/TVCG.2012.226
   Zhao KY, 2014, COMPUT GRAPH FORUM, V33, P331, DOI 10.1111/cgf.12389
   Zhao X, 2019, IEEE T VIS COMPUT GR, V25, P407, DOI 10.1109/TVCG.2018.2864475
   Zhao Y, 2020, IEEE T VIS COMPUT GR, V26, P590, DOI 10.1109/TVCG.2019.2934655
   Zhou ZH, 2006, J COMPUT SCI TECH-CH, V21, P800, DOI 10.1007/s11390-006-0800-7
   Zhou ZH, 2018, NATL SCI REV, V5, P44, DOI 10.1093/nsr/nwx106
   Zhou ZG, 2019, IEEE T VIS COMPUT GR, V25, P43, DOI 10.1109/TVCG.2018.2864503
   Zhou ZG, 2017, IEEE COMPUT GRAPH, V37, P98, DOI 10.1109/MCG.2017.3621228
NR 303
TC 117
Z9 126
U1 11
U2 139
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2021
VL 7
IS 1
BP 3
EP 36
DI 10.1007/s41095-020-0191-7
PG 34
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FQ
UT WOS:000648692800002
OA gold, Green Submitted
DA 2024-07-18
ER

PT J
AU Liang, YQ
   Jafari, N
   Luo, X
   Chen, Q
   Cao, YP
   Li, X
AF Liang, Yongqing
   Jafari, Navid
   Luo, Xing
   Chen, Qin
   Cao, Yanpeng
   Li, Xin
TI WaterNet: An adaptive matching pipeline for segmenting water with
   volatile appearance
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE video segmentation; water segmentation; appearance adaptation
AB We develop a novel network to segment water with significant appearance variation in videos. Unlike existing state-of-the-art video segmentation approaches that use a pre-trained feature recognition network and several previous frames to guide segmentation, we accommodate the object's appearance variation by considering features observed from the current frame. When dealing with segmentation of objects such as water, whose appearance is non-uniform and changing dynamically, our pipeline can produce more reliable and accurate segmentation results than existing algorithms.
C1 [Liang, Yongqing; Li, Xin] Louisiana State Univ, Sch Elect Engn & Comp Sci, Baton Rouge, LA 70803 USA.
   [Jafari, Navid] Louisiana State Univ, Dept Civil Engn, Baton Rouge, LA 70803 USA.
   [Luo, Xing; Cao, Yanpeng] Zhejiang Univ, Dept Mech Engn, Hangzhou, Peoples R China.
   [Chen, Qin] Northeastern Univ, Dept Civil Engn, Boston, MA 02115 USA.
C3 Louisiana State University System; Louisiana State University; Louisiana
   State University System; Louisiana State University; Zhejiang
   University; Northeastern University
RP Li, X (corresponding author), Louisiana State Univ, Sch Elect Engn & Comp Sci, Baton Rouge, LA 70803 USA.
EM yqliang@cct.lsu.edu; njafari@lsu.edu; luoxing@zju.edu.cn;
   q.chen@northeastern.edu; caoyp@zju.edu.cn; xinli@cct.lsu.edu
RI Liang, Yongqing/ABG-7122-2020; Li, Xin/AAS-5967-2021
OI Liang, Yongqing/0000-0002-7282-0476; Li, Xin/0000-0002-0144-9489; Chen,
   Qin Jim/0000-0002-6540-8758
FU National Science Foundation [EAR 1760582]; Louisiana Board of Regents
   ITRS [LEQSF(2018-21)-RD-B-03]
FX This work was supported in part by the National Science Foundation under
   Grant EAR 1760582, and the Louisiana Board of Regents ITRS
   LEQSF(2018-21)-RD-B-03. We would like to express our appreciation to
   anonymous reviewers whose comments helped improve and clarify this
   manuscript.
CR Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565
   Chen YH, 2018, PROC CVPR IEEE, P1189, DOI 10.1109/CVPR.2018.00130
   Cheng JC, 2018, PROC CVPR IEEE, P7415, DOI 10.1109/CVPR.2018.00774
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu YT, 2018, LECT NOTES COMPUT SC, V11212, P56, DOI 10.1007/978-3-030-01237-3_4
   Hu YT, 2017, ADV NEUR IN, V30
   Khoreva Anna, 2017, ARXIV170309554
   Li XX, 2018, LECT NOTES COMPUT SC, V11207, P93, DOI 10.1007/978-3-030-01219-9_6
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lopez-Fuentes L, 2017, IEEE INT CONF BIG DA, P3746, DOI 10.1109/BigData.2017.8258373
   Luiten J, 2019, LECT NOTES COMPUT SC, V11364, P565, DOI 10.1007/978-3-030-20870-7_35
   Maninis KK, 2019, IEEE T PATTERN ANAL, V41, P1515, DOI 10.1109/TPAMI.2018.2838670
   Oh SW, 2018, PROC CVPR IEEE, P7376, DOI 10.1109/CVPR.2018.00770
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Perazzi F, 2017, PROC CVPR IEEE, P3491, DOI 10.1109/CVPR.2017.372
   Pont-Tuset J., 2019, ARXIV190500737
   Pont-Tuset J., 2017, ARXIV170400675
   Voigtlaender P, 2019, PROC CVPR IEEE, P9473, DOI 10.1109/CVPR.2019.00971
   Voigtlaender Paul, 2017, P BRIT MACH VIS C BM
   Yang LJ, 2018, PROC CVPR IEEE, P6499, DOI 10.1109/CVPR.2018.00680
   Yoon JS, 2017, IEEE I CONF COMP VIS, P2186, DOI 10.1109/ICCV.2017.238
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
NR 22
TC 11
Z9 12
U1 0
U2 9
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2020
VL 6
IS 1
BP 65
EP 78
DI 10.1007/s41095-020-0156-x
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FA
UT WOS:000648691200006
OA gold
DA 2024-07-18
ER

PT J
AU Song, YZ
   Fan, RC
   Huang, SR
   Zhu, Z
   Tong, RF
AF Song, Yizhi
   Fan, Ruochen
   Huang, Sharon
   Zhu, Zhe
   Tong, Ruofeng
TI A three-stage real-time detector for traffic signs in large panoramas
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE traffic sign; detection; real time
AB Traffic sign detection is one of the key components in autonomous driving. Advanced autonomous vehicles armed with high quality sensors capture high definition images for further analysis. Detecting traffic signs, moving vehicles, and lanes is important for localization and decision making. Traffic signs, especially those that are far from the camera, are small, and so are challenging to traditional object detection methods. In this work, in order to reduce computational cost and improve detection performance, we split the large input images into small blocks and then recognize traffic signs in the blocks using another detection module. Therefore, this paper proposes a three-stage traffic sign detector, which connects a BlockNet with an RPN-RCNN detection network. BlockNet, which is composed of a set of CNN layers, is capable of performing block-level foreground detection, making inferences in less than 1 ms. Then, the RPN-RCNN two-stage detector is used to identify traffic sign objects in each block; it is trained on a derived dataset named TT100KPatch. Experiments show that our framework can achieve both state-of-the-art accuracy and recall; its fastest detection speed is 102 fps.
C1 [Song, Yizhi] Purdue Univ, Dept Comp Sci, 305 N Univ St, W Lafayette, IN 47907 USA.
   [Fan, Ruochen] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Huang, Sharon] Penn State Univ, Coll Informat Sci & Technol, University Pk, PA 16802 USA.
   [Zhu, Zhe] Duke Univ, Dept Radiol, Durham, NC 27705 USA.
   [Tong, Ruofeng] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310007, Peoples R China.
C3 Purdue University System; Purdue University; Tsinghua University;
   Pennsylvania Commonwealth System of Higher Education (PCSHE);
   Pennsylvania State University; Pennsylvania State University -
   University Park; Duke University; Zhejiang University
RP Tong, RF (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310007, Peoples R China.
EM song630@purdue.edu; frc16@mails.tsinghua.edu.cn; suh972@ist.psu.edu;
   zhe.zhu@duke.edu; trf@zju.edu.cn
RI fan, ruochen/AAQ-1758-2021
OI Huang, Sharon Xiaolei/0000-0003-2338-6535
FU National Natural Science Foundation of China [61832016]; Science and
   Technology Project of Zhejiang Province [2018C01080]
FX We thank all anonymous reviewers for their valuable comments and
   suggestions. This paper was supported by the National Natural Science
   Foundation of China (No. 61832016) and Science and Technology Project of
   Zhejiang Province (No. 2018C01080).
CR [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   [Anonymous], 2018, COMMUN COMPUT INF SC
   [Anonymous], 2013, ARXIV PREPRINT ARXIV, DOI [DOI 10.48550/ARXIV.1312.6229, 10.48550/arXiv.1312.6229]
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Houben S, 2013, IEEE IJCNN
   Kong T, 2017, PROC CVPR IEEE, P5244, DOI 10.1109/CVPR.2017.557
   Larsson F, 2011, LECT NOTES COMPUT SC, V6688, P238, DOI 10.1007/978-3-642-21227-7_23
   Li JA, 2017, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2017.211
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Meng ZB, 2017, 2017 IEEE 18TH INTERNATIONAL CONFERENCE ON INFORMATION REUSE AND INTEGRATION (IEEE IRI 2017), P217, DOI 10.1109/IRI.2017.57
   Pon AD, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P102, DOI 10.1109/CRV.2018.00024
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Stallkamp J, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1453, DOI 10.1109/IJCNN.2011.6033395
   Timofte R, 2014, MACH VISION APPL, V25, P633, DOI 10.1007/s00138-011-0391-3
   Yang Jianwei, 2017, FASTER PYTORCH IMPLE
   Yang TT, 2018, COMPUT NETW, V136, P95, DOI 10.1016/j.comnet.2018.02.026
   Yang Y, 2016, IEEE T INTELL TRANSP, V17, P2022, DOI 10.1109/TITS.2015.2482461
   Yifan Lu, 2018, Computational Visual Media, V4, P253, DOI 10.1007/s41095-018-0116-x
   Zhu Z, 2016, PROC CVPR IEEE, P2110, DOI 10.1109/CVPR.2016.232
NR 26
TC 9
Z9 9
U1 0
U2 10
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2019
VL 5
IS 4
BP 403
EP 416
DI 10.1007/s41095-019-0152-1
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UO
UT WOS:000648691000007
OA gold
DA 2024-07-18
ER

PT J
AU Chen, SJ
   Wang, JX
   Pan, W
   Gao, S
   Wang, ML
   Lu, XQ
AF Chen, Shuaijun
   Wang, Jinxi
   Pan, Wei
   Gao, Shang
   Wang, Meili
   Lu, Xuequan
TI Towards uniform point distribution in feature-preserving point cloud
   filtering
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE point cloud filtering; point distribution; feature preservation
AB While a popular representation of 3D data, point clouds may contain noise and need filtering before use. Existing point cloud filtering methods either cannot preserve sharp features or result in uneven point distributions in the filtered output. To address this problem, this paper introduces a point cloud filtering method that considers both point distribution and feature preservation during filtering. The key idea is to incorporate a repulsion term with a data term in energy minimization. The repulsion term is responsible for the point distribution, while the data term aims to approximate the noisy surfaces while preserving geometric features. This method is capable of handling models with fine-scale features and sharp features. Extensive experiments show that our method quickly yields good results with relatively uniform point distribution.
C1 [Chen, Shuaijun; Gao, Shang; Lu, Xuequan] Deakin Univ, Sch Informat Technol, Geelong, Vic 3220, Australia.
   [Wang, Jinxi; Wang, Meili] Northwest A&F Univ, Coll Informat Engn, Yangling 712100, Peoples R China.
   [Pan, Wei] South China Univ Technol, Sch Mech & Automot Engn, Guangzhou 510641, Peoples R China.
C3 Deakin University; Northwest A&F University - China; South China
   University of Technology
RP Lu, XQ (corresponding author), Deakin Univ, Sch Informat Technol, Geelong, Vic 3220, Australia.; Wang, ML (corresponding author), Northwest A&F Univ, Coll Informat Engn, Yangling 712100, Peoples R China.
EM wml@nwsuaf.edu.cn; xuequan.lu@deakin.edu.au
RI Zhang, Ge/KGL-7634-2024; zhang, jingxing/KCY-4726-2024; Zhang,
   Chi/JSK-0744-2023; ZHU, JIALI/JNE-3065-2023; Zhang, Wenli/JXL-4317-2024;
   Yang, Min/JPY-3791-2023; WU, SHAN/KGM-5484-2024; cheng,
   chen/JHS-9462-2023; Li, YiXue/JRW-6306-2023; Wei, Wei/JVM-8876-2024; Lu,
   Lu/JPE-5187-2023; Chen, Chao/JHS-6563-2023; wang, yi/KBB-3614-2024; li,
   tao/JVO-9006-2024; Zhang, Xiaoyu/JXR-6386-2024; Wang,
   Xiaojun/JUU-9683-2023; Lin, Xiaoqi/KFS-5750-2024; Chen,
   Fang/JZE-4446-2024; ren, jing/JXN-8411-2024; wang, jiaqi/JSL-7112-2023
OI Wei, Wei/0000-0002-4109-3878; Lu, Xuequan/0000-0003-0959-408X
CR Alexa M, 2003, IEEE T VIS COMPUT GR, V9, P3, DOI 10.1109/TVCG.2003.1175093
   Chen HH, 2020, IEEE T VIS COMPUT GR, V26, P3255, DOI 10.1109/TVCG.2019.2920817
   Deschaud JE, 2010, INT ARCH PHOTOGRAMM, V38, P109
   Digne J., 2012, P IEEE CVF C COMP VI, P73
   Duan CJ, 2019, INT CONF ACOUST SPEE, P8553, DOI [10.1109/icassp.2019.8682812, 10.1109/ICASSP.2019.8682812]
   Erler Philipp, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P108, DOI 10.1007/978-3-030-58558-7_7
   Guennebaud G, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239474
   Guerrero P, 2018, COMPUT GRAPH FORUM, V37, P75, DOI 10.1111/cgf.13343
   Hermosilla P, 2019, IEEE I CONF COMP VIS, P52, DOI 10.1109/ICCV.2019.00014
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421645
   Huang H, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618522
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Levin D, 2004, MATH VISUAL, P37
   Levin D, 1998, MATH COMPUT, V67, P1517, DOI 10.1090/S0025-5718-98-00974-0
   Liao B, 2013, COMPUT AIDED DESIGN, V45, P861, DOI 10.1016/j.cad.2013.02.003
   Lipman Y, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276405
   Liu YC, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480486
   Liu Z, 2020, COMPUT AIDED DESIGN, V127, DOI 10.1016/j.cad.2020.102857
   Lu DN, 2020, COMPUT AIDED DESIGN, V125, DOI 10.1016/j.cad.2020.102860
   Lu XQ, 2022, IEEE T VIS COMPUT GR, V28, P1835, DOI 10.1109/TVCG.2020.3026785
   Lu XQ, 2019, COMPUT VIS IMAGE UND, V188, DOI 10.1016/j.cviu.2019.102792
   Lu XQ, 2018, AAAI CONF ARTIF INTE, P7226
   Lu XQ, 2018, IEEE T VIS COMPUT GR, V24, P2315, DOI 10.1109/TVCG.2017.2725948
   Lu XQ, 2014, COMPUT ANIMAT VIRT W, V25, P363, DOI 10.1002/cav.1575
   Öztireli AC, 2009, COMPUT GRAPH FORUM, V28, P493, DOI 10.1111/j.1467-8659.2009.01388.x
   Pei Y, 2020, INT C COMP AN SOC AG, P73
   Preiner R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601172
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Rakotosaona MJ, 2020, COMPUT GRAPH FORUM, V39, P185, DOI 10.1111/cgf.13753
   Roveri R, 2018, COMPUT GRAPH FORUM, V37, P87, DOI 10.1111/cgf.13344
   Rusu RB, 2007, 2007 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-9, P3197
   Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295
   Yu LQ, 2018, LECT NOTES COMPUT SC, V11211, P398, DOI 10.1007/978-3-030-01234-2_24
   Zeng J, 2020, IEEE T IMAGE PROCESS, V29, P3474, DOI 10.1109/TIP.2019.2961429
   Zhang DB, 2021, IEEE T VIS COMPUT GR, V27, P2015, DOI 10.1109/TVCG.2020.3027069
NR 36
TC 10
Z9 10
U1 12
U2 39
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2023
VL 9
IS 2
BP 249
EP 263
DI 10.1007/s41095-022-0278-4
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7N8FL
UT WOS:000907570700003
OA Green Submitted, gold
DA 2024-07-18
ER

PT J
AU Zhou, T
   Fan, DP
   Chen, G
   Zhou, Y
   Fu, HZ
AF Zhou, Tao
   Fan, Deng-Ping
   Chen, Geng
   Zhou, Yi
   Fu, Huazhu
TI Specificity-preserving RGB-D saliency detection
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE salient object detection (SOD); RGB-D; cross-enhanced integration module
   (CIM); multi-modal feature aggregation (MFA)
ID OBJECT DETECTION; INTERACTION NETWORK; FUSION; MODEL
AB Salient object detection (SOD) in RGB and depth images has attracted increasing research interest. Existing RGB-D SOD models usually adopt fusion strategies to learn a shared representation from RGB and depth modalities, while few methods explicitly consider how to preserve modality-specific characteristics. In this study, we propose a novel framework, the specificity-preserving network (SPNet), which improves SOD performance by exploring both the shared information and modality-specific properties. Specifically, we use two modality-specific networks and a shared learning network to generate individual and shared saliency prediction maps. To effectively fuse cross-modal features in the shared learning network, we propose a cross-enhanced integration module (CIM) and propagate the fused feature to the next layer to integrate cross-level information. Moreover, to capture rich complementary multi-modal information to boost SOD performance, we use a multi-modal feature aggregation (MFA) module to integrate the modality-specific features from each individual decoder into the shared decoder. By using skip connections between encoder and decoder layers, hierarchical features can be fully combined. Extensive experiments demonstrate that our SPNet outperforms cutting-edge approaches on six popular RGB-D SOD and three camouflaged object detection benchmarks. The project is publicly available at .
C1 [Zhou, Tao] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Zhou, Tao] Minist Educ, Key Lab Syst Control & Informat Proc, Shanghai, Peoples R China.
   [Fan, Deng-Ping] Swiss Fed Inst Technol, Comp Vis Lab, Zurich, Switzerland.
   [Chen, Geng] Northwestern Polytech Univ, Sch Comp Sci & Engn, Xian, Peoples R China.
   [Zhou, Yi] Southeast Univ, Sch Comp Sci & Engn, Nanjing, Peoples R China.
   [Fu, Huazhu] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates.
C3 Nanjing University of Science & Technology; Swiss Federal Institutes of
   Technology Domain; ETH Zurich; Northwestern Polytechnical University;
   Southeast University - China
RP Fan, DP (corresponding author), Swiss Fed Inst Technol, Comp Vis Lab, Zurich, Switzerland.
EM taozhou.ai@gmail.com; dengpfan@gmail.com; geng.chen.cs@gmail.com;
   zhou.szcn@gmail.com; zfu@ieee.org
RI Chen, Geng/KMA-8119-2024; Wang, Meng/ITR-8699-2023; Fu,
   Huazhu/A-1411-2014; Fan, Deng-Ping/ABD-4052-2020
OI Fu, Huazhu/0000-0002-9702-5524; Fan, Deng-Ping/0000-0002-5245-7518
FU National Natural Science Foundation of China [62172228]; Open Project of
   the Key Laboratory of System Control and Information Processing,
   Ministry of Education (Shanghai Jiao Tong University) [Scip202102]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant No. 62172228, in part by an Open Project
   of the Key Laboratory of System Control and Information Processing,
   Ministry of Education (Shanghai Jiao Tong University, No. Scip202102).
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2011, P ICML
   Ao Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P346, DOI 10.1007/978-3-030-58610-2_21
   Chaudhuri K., 2009, P 26 ANN INT C MACH, P129
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Chen H, 2021, INT J COMPUT VISION, V129, P2076, DOI 10.1007/s11263-021-01452-0
   Chen H, 2018, IEEE INT C INT ROBOT, P6821, DOI 10.1109/IROS.2018.8594373
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen H, 2019, PATTERN RECOGN, V86, P376, DOI 10.1016/j.patcog.2018.08.007
   Chen S., 2020, ECCV, P520, DOI DOI 10.1007/978-3-030-58598-3_31
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Cheng Y, 2014, IEEE INT CON MULTI
   Chongyi Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P225, DOI 10.1007/978-3-030-58598-3_14
   Cong RM, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2557347
   Cong RM, 2020, IEEE T CYBERNETICS, V50, P3627, DOI 10.1109/TCYB.2019.2932005
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Desingh K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.98
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Ding Y, 2019, J VIS COMMUN IMAGE R, V61, P1, DOI 10.1016/j.jvcir.2019.03.019
   Eitel A, 2015, IEEE INT C INT ROBOT, P681, DOI 10.1109/IROS.2015.7353446
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P6024, DOI 10.1109/TPAMI.2021.3085766
   Fan DP, 2020, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR42600.2020.00285
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Feng D, 2016, PROC CVPR IEEE, P2343, DOI 10.1109/CVPR.2016.257
   Fu KR, 2022, IEEE T PATTERN ANAL, V44, P5541, DOI 10.1109/TPAMI.2021.3073689
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Gönen M, 2011, J MACH LEARN RES, V12, P2211
   Gongyang Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P665, DOI 10.1007/978-3-030-58520-4_39
   Guo JF, 2016, IEEE INT CON MULTI
   Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Hu JL, 2018, IEEE T PATTERN ANAL, V40, P2281, DOI 10.1109/TPAMI.2017.2749576
   Huang Z, 2021, NEUROCOMPUTING, V452, P200, DOI 10.1016/j.neucom.2021.04.053
   Ji W, 2021, PROC CVPR IEEE, P9466, DOI 10.1109/CVPR46437.2021.00935
   Jiang B, 2021, IEEE T MULTIMEDIA, V23, P1343, DOI 10.1109/TMM.2020.2997184
   Jiang ZL, 2013, PROC CVPR IEEE, P2043, DOI 10.1109/CVPR.2013.266
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Lang CY, 2012, LECT NOTES COMPUT SC, V7573, P101, DOI 10.1007/978-3-642-33709-3_8
   Li CY, 2021, IEEE T CYBERNETICS, V51, P88, DOI 10.1109/TCYB.2020.2969255
   Li GY, 2021, IEEE T IMAGE PROCESS, V30, P3528, DOI 10.1109/TIP.2021.3062689
   Li GY, 2020, IEEE T IMAGE PROCESS, V29, P4873, DOI 10.1109/TIP.2020.2976689
   Li L, 2022, IEEE T CIRC SYST VID, V32, P2303, DOI 10.1109/TCSVT.2021.3093890
   Liang FF, 2018, NEUROCOMPUTING, V275, P2227, DOI 10.1016/j.neucom.2017.10.052
   Lin T, 2019, PROC CVPR IEEE, P936, DOI DOI 10.1109/CVPR.2017.106
   Liu D, 2019, IEEE IMAGE PROC, P3925, DOI [10.1109/icip.2019.8803653, 10.1109/ICIP.2019.8803653]
   Liu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4702, DOI 10.1109/ICCV48922.2021.00468
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu ZY, 2019, NEUROCOMPUTING, V363, P46, DOI 10.1016/j.neucom.2019.07.012
   Nian Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13753, DOI 10.1109/CVPR42600.2020.01377
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Pingping Zhang, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P202, DOI 10.1109/ICCV.2017.31
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Rapantzikos K, 2009, PROC CVPR IEEE, P1454, DOI 10.1109/CVPRW.2009.5206525
   Ren JQ, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301391
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russakovsky O., 2015, INT J COMPUT VISION, V115, P211
   Shimoda W, 2016, LECT NOTES COMPUT SC, V9908, P218, DOI 10.1007/978-3-319-46493-0_14
   Song HK, 2017, IEEE T IMAGE PROCESS, V26, P4204, DOI 10.1109/TIP.2017.2711277
   Sun Y., 2021, P INT JOINT C ART IN, P1025
   Le TN, 2019, COMPUT VIS IMAGE UND, V184, P45, DOI 10.1016/j.cviu.2019.04.006
   Wang L, 2018, IEEE IPCCC
   Wang NN, 2019, IEEE ACCESS, V7, P55277, DOI 10.1109/ACCESS.2019.2913107
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang X, 2018, IEEE T IMAGE PROCESS, V27, P121, DOI 10.1109/TIP.2017.2756825
   Wei Ji, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P52, DOI 10.1007/978-3-030-58523-5_4
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   White M., 2012, ADV NEURAL INFORM PR, P1682
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xiaoqi Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P646, DOI 10.1007/978-3-030-58542-6_39
   Yan Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13376, DOI 10.1109/CVPR42600.2020.01339
   Yongri Piao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9057, DOI 10.1109/CVPR42600.2020.00908
   Youwei Pang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P235, DOI 10.1007/978-3-030-58595-2_15
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Zhai YJ, 2021, IEEE T IMAGE PROCESS, V30, P8727, DOI 10.1109/TIP.2021.3116793
   Zhang CQ, 2017, PROC CVPR IEEE, P4333, DOI 10.1109/CVPR.2017.461
   Zhang Jie, 2021, arXiv
   Zhang J, 2022, IEEE T PATTERN ANAL, V44, P5761, DOI 10.1109/TPAMI.2021.3073564
   Zhang J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4318, DOI 10.1109/ICCV48922.2021.00430
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang M, 2020, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR42600.2020.00353
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao JX, 2019, PROC CVPR IEEE, P3922, DOI 10.1109/CVPR.2019.00405
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao R, 2017, IEEE T PATTERN ANAL, V39, P356, DOI 10.1109/TPAMI.2016.2544310
   Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320
   Zhao YF, 2021, IEEE T IMAGE PROCESS, V30, P7717, DOI 10.1109/TIP.2021.3108412
   Zhou L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2438546
   Zhou T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4661, DOI 10.1109/ICCV48922.2021.00464
   Zhou T, 2020, IEEE T MED IMAGING, V39, P2772, DOI 10.1109/TMI.2020.2975344
   Zhou T, 2021, COMPUT VIS MEDIA, V7, P37, DOI 10.1007/s41095-020-0199-z
   Zhou T, 2020, IEEE T CYBERNETICS, V50, P3517, DOI 10.1109/TCYB.2019.2918495
   Zhu CB, 2019, IEEE INT CON MULTI, P199, DOI 10.1109/ICME.2019.00042
   Zhu CB, 2017, IEEE INT CONF COMP V, P1509, DOI 10.1109/ICCVW.2017.178
   Zhu CB, 2017, IEEE INT CONF COMP V, P3008, DOI 10.1109/ICCVW.2017.355
   Zhu JY, 2015, IEEE T PATTERN ANAL, V37, P862, DOI 10.1109/TPAMI.2014.2353617
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 104
TC 4
Z9 4
U1 7
U2 35
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2023
VL 9
IS 2
BP 297
EP 317
DI 10.1007/s41095-022-0268-6
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7N8FL
UT WOS:000907570700006
OA gold, Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Peng, ZX
   Wang, H
   Weng, YL
   Yang, Y
   Shao, TJ
AF Peng, Zhexi
   Wang, He
   Weng, Yanlin
   Yang, Yin
   Shao, Tianjia
TI Unsupervised image translation with distributional semantics awareness
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE generative adversarial networks (GANs); manifold alignment; unsupervised
   learning; image-to-image translation; distributional semantics
AB Unsupervised image translation (UIT) studies the mapping between two image domains. Since such mappings are under-constrained, existing research has pursued various desirable properties such as distributional matching or two-way consistency. In this paper, we re-examine UIT from a new perspective: distributional semantics consistency, based on the observation that data variations contain semantics, e.g., shoes varying in colors. Further, the semantics can be multi-dimensional, e.g., shoes also varying in style, functionality, etc. Given two image domains, matching these semantic dimensions during UIT will produce mappings with explicable correspondences, which has not been investigated previously. We propose distributional semantics mapping (DSM), the first UIT method which explicitly matches semantics between two domains. We show that distributional semantics has been rarely considered within and beyond UIT, even though it is a common problem in deep learning. We evaluate DSM on several benchmark datasets, demonstrating its general ability to capture distributional semantics. Extensive comparisons show that DSM not only produces explicable mappings, but also improves image quality in general.
C1 [Peng, Zhexi; Weng, Yanlin; Shao, Tianjia] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Peoples R China.
   [Wang, He] Univ Leeds, Sch Comp, Leeds, England.
   [Yang, Yin] Clemson Univ, Sch Comp, Clemson, SC USA.
C3 Zhejiang University; University of Leeds; Clemson University
RP Weng, YL (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Peoples R China.
EM zhexipeng@zju.edu.cn; H.E.Wang@leeds.ac.uk; weng@cad.zju.edu.cn;
   yin5@clemson.edu; tjshao@zju.edu.cn
RI Wang, He/ABD-8303-2021
OI Wang, He/0000-0002-2281-5679
FU National Natural Science Foundation of China [61772462]; 100 Talents
   Program of Zhejiang University
FX AcknowledgementsWe thank the anonymous reviewers for their valuable
   comments. The work was supported by National Natural Science Foundation
   of China (Grant No. 61772462), and the 100 Talents Program of Zhejiang
   University.
CR Almahairi A, 2018, PR MACH LEARN RES, V80
   [Anonymous], 2018, NIPS
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Benaim Sagie, 2017, Advances in neural information processing systems, P752
   Chen SY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459760
   Chen SY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392386
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Fu H, 2019, PROC CVPR IEEE, P2422, DOI [10.1109/CVPR.2019.00253, 10.1109/cvpr.2019.00253]
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Harkonen E., 2020, Advances in Neural Information Processing Systems, V33, P9841
   Hensel M, 2017, ADV NEUR IN, V30
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kang, 2019, ARXIV
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim T., 2017, P 34 INT C MACH LEAR, P1857, DOI DOI 10.1109/WPT.2017.7953894
   Kingma D. P., 2014, arXiv
   Kingma DP, 2013, ARXIV
   LeCun Y., 2010, MNIST HANDWRITTEN DI
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Lee HY, 2020, INT J COMPUT VISION, V128, P2402, DOI 10.1007/s11263-019-01284-z
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065
   Liu MY, 2017, ADV NEUR IN, V30
   Lu G., 2020, ARXIV
   Lu GS, 2019, AAAI CONF ARTIF INTE, P4432
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Miyato T., 2018, ARXIV
   Tomei M, 2019, PROC CVPR IEEE, P5842, DOI 10.1109/CVPR.2019.00600
   Ulyanov Dmitry, 2016, arXiv
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Yu A, 2014, PROC CVPR IEEE, P192, DOI 10.1109/CVPR.2014.32
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36
NR 40
TC 2
Z9 2
U1 1
U2 2
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2023
VL 9
IS 3
BP 619
EP 631
DI 10.1007/s41095-022-0295-3
EA APR 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA G9DP6
UT WOS:000974409700001
OA gold
DA 2024-07-18
ER

PT J
AU Liu, SJ
   Liu, HY
   Chen, W
   Yan, DM
   Hu, L
   Liu, XR
   Li, QS
AF Liu, Shengjun
   Liu, Hongyan
   Chen, Wang
   Yan, Dong-Ming
   Hu, Ling
   Liu, Xinru
   Li, Qinsong
TI An anisotropic Chebyshev descriptor and its optimization for deformable
   shape correspondence
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE anisotropic descriptor; spectral descriptor; shape descriptor; shape
   matching; spectral convolution; deep learning
AB Shape descriptors have recently gained popularity in shape matching, statistical shape modeling, etc. Their discriminative ability and efficiency play a decisive role in these tasks. In this paper, we first propose a novel handcrafted anisotropic spectral descriptor using Chebyshev polynomials, called the anisotropic Chebyshev descriptor (ACD); it can effectively capture shape features in multiple directions. The ACD inherits many good characteristics of spectral descriptors, such as being intrinsic, robust to changes in surface discretization, etc. Furthermore, due to the orthogonality of Chebyshev polynomials, the ACD is compact and can disambiguate intrinsic symmetry since several directions are considered. To improve the ACD's discrimination ability, we construct a Chebyshev spectral manifold convolutional neural network (CSMCNN) that optimizes the ACD and produces a learned ACD. Our experimental results show that the ACD outperforms existing state-of-the-art handcrafted descriptors. The combination of the ACD and the CSMCNN is better than other state-of-the-art learned descriptors in terms of discrimination, efficiency, and robustness to changes in shape resolution and discretization.
C1 [Liu, Shengjun; Liu, Hongyan; Chen, Wang; Liu, Xinru] Cent South Univ, Sch Math & Stat, Changsha 410000, Peoples R China.
   [Yan, Dong-Ming] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.
   [Yan, Dong-Ming] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 101408, Peoples R China.
   [Hu, Ling] Hunan First Normal Univ, Sch Math & Stat, Changsha 410000, Peoples R China.
   [Li, Qinsong] Cent South Univ, Big Data Inst, Changsha 410000, Peoples R China.
C3 Central South University; Chinese Academy of Sciences; Institute of
   Automation, CAS; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS; Hunan First Normal University; Central South
   University
RP Li, QS (corresponding author), Cent South Univ, Big Data Inst, Changsha 410000, Peoples R China.
EM shjliu.cg@csu.edu.cn; 02111046@csu.edu.cn; 819974758@qq.com;
   yandongming@gmail.com; huling.cg@foxmail.com; liuxinru@csu.edu.cn;
   qinsli.cg@foxmail.com
RI Hu, Ling/AAA-5764-2020; Liu, Xinru/KEH-2341-2024
FU National Natural Science Foundation of China [62172447, 61876191]; Hunan
   Provincial Natural Science Foundation of China [2021JJ30172]; Open
   Project Program of the National Laboratory of Pattern Recognition (NLPR)
   [202200025]
FX AcknowledgementsWe acknowledge the anonymous reviewers for their
   valuable comments. This work was supported by the National Natural
   Science Foundation of China (Nos. 62172447, 61876191), Hunan Provincial
   Natural Science Foundation of China (No. 2021JJ30172), and the Open
   Project Program of the National Laboratory of Pattern Recognition (NLPR)
   (No. 202200025).
CR Andreux M, 2015, LECT NOTES COMPUT SC, V8928, P299, DOI 10.1007/978-3-319-16220-1_21
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491
   Boscaini D, 2016, COMPUT GRAPH FORUM, V35, P431, DOI 10.1111/cgf.12844
   Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1
   Cosmo Luca, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P1, DOI 10.1007/978-3-030-58565-5_1
   Defferrard M, 2016, ADV NEUR IN, V29
   Donati Nicolas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8589, DOI 10.1109/CVPR42600.2020.00862
   Fey M, 2018, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2018.00097
   Fu YP, 2020, PROC CVPR IEEE, P5949, DOI 10.1109/CVPR42600.2020.00599
   Gao L, 2021, IEEE T VIS COMPUT GR, V27, P3007, DOI 10.1109/TVCG.2020.3003823
   Guo JW, 2020, COMPUT VIS MEDIA, V6, P95, DOI 10.1007/s41095-020-0163-y
   Guo YL, 2016, INT J COMPUT VISION, V116, P66, DOI 10.1007/s11263-015-0824-y
   Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005
   Hu L, 2021, PROC CVPR IEEE, P14531, DOI 10.1109/CVPR46437.2021.01430
   Hu Ling, 2019, Journal of Zhejiang University (Engineering Science), V53, P761, DOI 10.3785/j.issn.1008-973X.2019.04.017
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kwok TH, 2012, IEEE T VIS COMPUT GR, V18, P1678, DOI 10.1109/TVCG.2011.115
   Li QS, 2021, COMPUT GRAPH FORUM, V40, P81, DOI 10.1111/cgf.14120
   Litman R, 2014, IEEE T PATTERN ANAL, V36, P171, DOI 10.1109/TPAMI.2013.148
   Masci J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P832, DOI 10.1109/ICCVW.2015.112
   Melzi S., 2019, P EUR WORKSH 3D OBJ, P121, DOI DOI 10.2312/3DOR.20191070
   Melzi S, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3144454
   Melzi S, 2016, INT CONF 3D VISION, P470, DOI 10.1109/3DV.2016.57
   Pickup D, 2016, INT J COMPUT VISION, V120, P169, DOI 10.1007/s11263-016-0903-8
   Qinsong Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14646, DOI 10.1109/CVPR42600.2020.01467
   Qiu S, 2021, PROC CVPR IEEE, P1757, DOI 10.1109/CVPR46437.2021.00180
   Ren J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275040
   Robinette K. M., 1999, 2 INT C 3D DIG IM MO, P380, DOI DOI 10.1109/IM.1999.805368
   Salti S, 2014, COMPUT VIS IMAGE UND, V125, P251, DOI 10.1016/j.cviu.2014.04.011
   Sharp N, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3507905
   Shuman DI, 2016, APPL COMPUT HARMON A, V40, P260, DOI 10.1016/j.acha.2015.02.005
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Sun ZY, 2020, J COMPUT DES ENG, V7, P18, DOI 10.1093/jcde/qwaa003
   Tan QY, 2022, IEEE T PATTERN ANAL, V44, P6297, DOI 10.1109/TPAMI.2021.3085887
   Vallet B, 2008, COMPUT GRAPH FORUM, V27, P251, DOI 10.1111/j.1467-8659.2008.01122.x
   Wang YQ, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392443
   Wang YQ, 2019, PROC CVPR IEEE, P6224, DOI 10.1109/CVPR.2019.00639
   Wang YQ, 2019, IEEE T VIS COMPUT GR, V25, P2430, DOI 10.1109/TVCG.2018.2837115
   Wu HY, 2011, IEEE T VIS COMPUT GR, V17, P1531, DOI 10.1109/TVCG.2010.231
   Xiao XY, 2021, INT J ADV MANUF TECH, V116, P3431, DOI 10.1007/s00170-021-07681-4
   Xiao YP, 2020, COMPUT VIS MEDIA, V6, P113, DOI 10.1007/s41095-020-0174-8
   Yan DM, 2014, IEEE T VIS COMPUT GR, V20, P1418, DOI 10.1109/TVCG.2014.2330574
   Yifan W, 2020, PROC CVPR IEEE, P72, DOI 10.1109/CVPR42600.2020.00015
NR 45
TC 1
Z9 1
U1 2
U2 10
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2023
VL 9
IS 3
BP 461
EP 477
DI 10.1007/s41095-022-0290-8
EA MAR 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA G9DP6
UT WOS:000953263800001
OA gold
DA 2024-07-18
ER

PT J
AU Mu, TJ
   Chen, HX
   Cai, JX
   Guo, N
AF Mu, Tai-Jiang
   Chen, Hao-Xiang
   Cai, Jun-Xiong
   Guo, Ning
TI Neural 3D reconstruction from sparse views using geometric priors
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE sparse views; 3D reconstruction; volume rendering; geometric priors;
   neural implicit 3D representation
AB Sparse view 3D reconstruction has attracted increasing attention with the development of neural implicit 3D representation. Existing methods usually only make use of 2D views, requiring a dense set of input views for accurate 3D reconstruction. In this paper, we show that accurate 3D reconstruction can be achieved by incorporating geometric priors into neural implicit 3D reconstruction. Our method adopts the signed distance function as the 3D representation, and learns a generalizable 3D surface reconstruction model from sparse views. Specifically, we build a more effective and sparse feature volume from the input views by using corresponding depth maps, which can be provided by depth sensors or directly predicted from the input views. We recover better geometric details by imposing both depth and surface normal constraints in addition to the color loss when training the neural implicit 3D representation. Experiments demonstrate that our method both outperforms state-of-the-art approaches, and achieves good generalizability.
C1 [Mu, Tai-Jiang; Chen, Hao-Xiang; Cai, Jun-Xiong] Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing 100084, Peoples R China.
   [Guo, Ning] Acad Mil Sci, Beijing 100091, Peoples R China.
C3 Tsinghua University
RP Guo, N (corresponding author), Acad Mil Sci, Beijing 100091, Peoples R China.
EM guoning10@nudt.edu.cn
RI Mu, Tai-Jiang/JWO-1381-2024
OI Mu, Tai-Jiang/0000-0002-9197-346X
FU National Natural Science Foundation of China [61902210]
FX AcknowledgementsWe thank the anonymous reviewers for their valuable
   comments on this paper. This work was supported by the National Natural
   Science Foundation of China (Grant No. 61902210).
CR [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.445
   Azinovic D, 2022, PROC CVPR IEEE, P6280, DOI 10.1109/CVPR52688.2022.00619
   Barron JT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5835, DOI 10.1109/ICCV48922.2021.00580
   Chen AP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14104, DOI 10.1109/ICCV48922.2021.01386
   Chibane J, 2021, PROC CVPR IEEE, P7907, DOI 10.1109/CVPR46437.2021.00782
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   Darmon F, 2022, PROC CVPR IEEE, P6250, DOI 10.1109/CVPR52688.2022.00616
   Eftekhar A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10766, DOI 10.1109/ICCV48922.2021.01061
   Fuentes-Pacheco J, 2015, ARTIF INTELL REV, V43, P55, DOI 10.1007/s10462-012-9365-8
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Gu XD, 2020, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR42600.2020.00257
   Guo HY, 2022, PROC CVPR IEEE, P5501, DOI 10.1109/CVPR52688.2022.00543
   Guo YC, 2022, PROC CVPR IEEE, P18388, DOI 10.1109/CVPR52688.2022.01786
   Hu SM, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-020-3097-4
   Huang PH, 2018, PROC CVPR IEEE, P2821, DOI 10.1109/CVPR.2018.00298
   Insafutdinov E, 2022, LECT NOTES COMPUT SC, V13692, P367, DOI 10.1007/978-3-031-19824-3_22
   Jensen R, 2014, PROC CVPR IEEE, P406, DOI 10.1109/CVPR.2014.59
   Ji MQ, 2021, IEEE T PATTERN ANAL, V43, P4078, DOI 10.1109/TPAMI.2020.2996798
   Johari MM, 2022, PROC CVPR IEEE, P18344, DOI 10.1109/CVPR52688.2022.01782
   Kar A., 2017, P ADV NEUR INF PROC, P364
   Kellnhofer P, 2021, PROC CVPR IEEE, P4285, DOI 10.1109/CVPR46437.2021.00427
   Lhuillier M, 2005, IEEE T PATTERN ANAL, V27, P418, DOI 10.1109/TPAMI.2005.44
   Li YZ, 2022, COMPUT VIS MEDIA, V8, P631, DOI 10.1007/s41095-022-0279-3
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu Lingjie, 2020, NEURIPS, V33, P15651
   Liu S. H., 2020, P IEEECVF C COMPUTER
   Long XX, 2022, LECT NOTES COMPUT SC, V13692, P210, DOI 10.1007/978-3-031-19824-3_13
   Long XX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12829, DOI 10.1109/ICCV48922.2021.01261
   Long XX, 2021, PROC CVPR IEEE, P8254, DOI 10.1109/CVPR46437.2021.00816
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Niemeyer M, 2020, PROC CVPR IEEE, P3501, DOI 10.1109/CVPR42600.2020.00356
   Oechsle M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5569, DOI 10.1109/ICCV48922.2021.00554
   Özyesil O, 2017, ACTA NUMER, V26, P305, DOI 10.1017/S096249291700006X
   Roessle B, 2022, PROC CVPR IEEE, P12882, DOI 10.1109/CVPR52688.2022.01255
   Seitz SM, 1999, INT J COMPUT VISION, V35, P151, DOI 10.1023/A:1008176507526
   Sun JM, 2021, PROC CVPR IEEE, P15593, DOI 10.1109/CVPR46437.2021.01534
   Tang Haotian, 2020, EUR C COMP VIS, P685, DOI DOI 10.1007/978-3-030-58604-1_41
   Trevithick A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15162, DOI 10.1109/ICCV48922.2021.01490
   Verbin D, 2022, PROC CVPR IEEE, P5481, DOI 10.1109/CVPR52688.2022.00541
   Wang D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5702, DOI 10.1109/ICCV48922.2021.00567
   Wang JP, 2022, LECT NOTES COMPUT SC, V13692, P139, DOI 10.1007/978-3-031-19824-3_9
   Wang P., 2021, Advances in Neural Information Processing Systems, V34, P27171
   Wang QQ, 2021, PROC CVPR IEEE, P4688, DOI 10.1109/CVPR46437.2021.00466
   Xiaoxiao Long, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P640, DOI 10.1007/978-3-030-58545-7_37
   Xu ZW, 2021, VIS COMPUT IND BIOME, V4, DOI 10.1186/s42492-021-00086-w
   Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47
   Yariv L., 2020, Advances in Neural Information Processing Systems, V33, P2492, DOI DOI 10.48550/ARXIV.2003.09852
   Yariv Lior, 2021, ADV NEURAL INFORM PR, V34, P4805, DOI DOI 10.48550/ARXIV.2106.12052
   Yu A, 2021, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR46437.2021.00455
   Yu Z., 2022, P 36 C NEURAL INFORM
   Zhang JY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6505, DOI 10.1109/ICCV48922.2021.00646
NR 51
TC 0
Z9 0
U1 16
U2 35
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2023
VL 9
IS 4
BP 687
EP 697
DI 10.1007/s41095-023-0337-5
EA MAR 2023
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P1QC2
UT WOS:000943883100001
OA gold
DA 2024-07-18
ER

PT J
AU Ji, GP
   Fan, DP
   Fu, KR
   Wu, Z
   Shen, JB
   Shao, L
AF Ji, Ge-Peng
   Fan, Deng-Ping
   Fu, Keren
   Wu, Zhe
   Shen, Jianbing
   Shao, Ling
TI Full-duplex strategy for video object segmentation
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE video object segmentation (VOS); video salient object detection (V-SOD);
   visual attention
ID SALIENCY DETECTION
AB Previous video object segmentation approaches mainly focus on simplex solutions linking appearance and motion, limiting effective feature collaboration between these two cues. In this work, we study a novel and efficient full-duplex strategy network (FSNet) to address this issue, by considering a better mutual restraint scheme linking motion and appearance allowing exploitation of cross-modal features from the fusion and decoding stage. Specifically, we introduce a relational cross-attention module (RCAM) to achieve bidirectional message propagation across embedding sub-spaces. To improve the model's robustness and update inconsistent features from the spatiotemporal embeddings, we adopt a bidirectional purification module after the RCAM. Extensive experiments on five popular benchmarks show that our FSNet is robust to various challenging scenarios (e.g., motion blur and occlusion), and compares well to leading methods both for video object segmentation and video salient object detection. The project is publicly available at https://github.com/GewelsJI/FSNet.
C1 [Ji, Ge-Peng] Wuhan Univ, Sch Comp Sci, Wuhan, Peoples R China.
   [Fan, Deng-Ping] Swiss Fed Inst Technol, Comp Vis Lab, C113-2,Sternwartstr 7, CH-8092 Zurich, Switzerland.
   [Fu, Keren] Sichuan Univ, Coll Comp Sci, Chengdu, Peoples R China.
   [Wu, Zhe] Peng Cheng Lab, Shenzhen, Peoples R China.
   [Shen, Jianbing] Beijing Inst Technol, Sch Comp Sci, Beijing, Peoples R China.
   [Shao, Ling] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates.
C3 Wuhan University; Swiss Federal Institutes of Technology Domain; ETH
   Zurich; Sichuan University; Peng Cheng Laboratory; Beijing Institute of
   Technology
RP Fan, DP (corresponding author), Swiss Fed Inst Technol, Comp Vis Lab, C113-2,Sternwartstr 7, CH-8092 Zurich, Switzerland.
EM gepengai.ji@gmail.com; dengpingfan@mail.nankai.edu.cn;
   fkrsuper@scu.edu.cn; wuzh02@pcl.ac.cn; shenjianbingcg@gmail.com;
   ling.shao@ieee.org
RI Fan, Deng-Ping/ABD-4052-2020; Fu, Keren/HPG-4742-2023; Shao,
   Ling/D-3535-2011
OI Fan, Deng-Ping/0000-0002-5245-7518; Fu, Keren/0000-0002-3195-2077; 
FU National Natural Science Foundation of China [62176169, 61703077,
   62102207]
FX This work was supported by the National Natural Science Foundation of
   China (62176169, 61703077, and 62102207).
CR Abramov A., 2012, 2012 IEEE Workshop on Applications of Computer Vision (WACV), P457, DOI 10.1109/WACV.2012.6163000
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2016, P AAAI C HUM COMP CR
   [Anonymous], 2011, ADV NEURAL INF PROCE
   Bharadia D, 2013, ACM SIGCOMM COMP COM, V43, P375, DOI 10.1145/2534169.2486033
   Bhat Goutam, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P777, DOI 10.1007/978-3-030-58536-5_46
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Borji A, 2019, COMPUT VIS MEDIA, V5, P117, DOI 10.1007/s41095-019-0149-9
   Bowen Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P293, DOI 10.1007/978-3-030-58601-0_18
   Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565
   Chen CLZ, 2020, IEEE T IMAGE PROCESS, V29, P1090, DOI 10.1109/TIP.2019.2934350
   Chen YH, 2018, IEEE T IMAGE PROCESS, V27, P3345, DOI 10.1109/TIP.2018.2813165
   Chen ZX, 2020, IEEE T CIRC SYST VID, V30, P1613, DOI 10.1109/TCSVT.2019.2908779
   Cheng JC, 2018, PROC CVPR IEEE, P7415, DOI 10.1109/CVPR.2018.00774
   Cheng JC, 2017, IEEE I CONF COMP VIS, P686, DOI 10.1109/ICCV.2017.81
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Cong RM, 2019, IEEE T IMAGE PROCESS, V28, P4819, DOI 10.1109/TIP.2019.2910377
   Ding MY, 2020, AAAI CONF ARTIF INTE, V34, P10713
   Duke B, 2021, PROC CVPR IEEE, P5908, DOI 10.1109/CVPR46437.2021.00585
   Faisal M, 2020, IEEE WINT CONF APPL, P1873, DOI [10.1109/WACV45572.2020.9093589, 10.1109/wacv45572.2020.9093589]
   Fan D.-P., 2021, Scientia Sinica Informationis, V6, P6
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Fragkiadaki K, 2012, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2012.6247883
   Galasso Fabio., 2012, ACCV, P760
   Gu YC, 2020, AAAI CONF ARTIF INTE, V34, P10869
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Hu L, 2021, PROC CVPR IEEE, P4142, DOI 10.1109/CVPR46437.2021.00413
   Hu P, 2020, IEEE T PATTERN ANAL, V42, P1957, DOI 10.1109/TPAMI.2019.2906175
   Hu YT, 2018, LECT NOTES COMPUT SC, V11205, P813, DOI 10.1007/978-3-030-01246-5_48
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Jain SD, 2017, PROC CVPR IEEE, P2117, DOI 10.1109/CVPR.2017.228
   Ji GP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4902, DOI 10.1109/ICCV48922.2021.00488
   Ji GP, 2021, LECT NOTES COMPUT SC, V12901, P142, DOI 10.1007/978-3-030-87193-2_14
   Johnander J, 2019, PROC CVPR IEEE, P8945, DOI 10.1109/CVPR.2019.00916
   Khoreva A., 2017, P 2017 DAVIS CHALL V
   Kim H, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2425544
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Koh YJ, 2017, PROC CVPR IEEE, P7417, DOI 10.1109/CVPR.2017.784
   Lao D, 2018, LECT NOTES COMPUT SC, V11214, P441, DOI 10.1007/978-3-030-01249-6_27
   Le T. N., 2017, P IEEE C COMP VIS PA, V38, P1
   Lee SH, 2017, PROC CVPR IEEE, P5863, DOI 10.1109/CVPR.2017.621
   Li FX, 2013, IEEE I CONF COMP VIS, P2192, DOI 10.1109/ICCV.2013.273
   Li GB, 2018, PROC CVPR IEEE, P3243, DOI 10.1109/CVPR.2018.00342
   Li HF, 2019, IEEE I CONF COMP VIS, P7273, DOI 10.1109/ICCV.2019.00737
   Li SY, 2018, LECT NOTES COMPUT SC, V11207, P215, DOI 10.1007/978-3-030-01219-9_13
   Li YX, 2020, IEEE T MULTIMEDIA, V22, P1153, DOI 10.1109/TMM.2019.2940851
   Lin FQ, 2020, IMAGE VISION COMPUT, V94, DOI 10.1016/j.imavis.2019.103864
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lu XK, 2019, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2019.00374
   Maddern W, 2017, INT J ROBOT RES, V36, P3, DOI 10.1177/0278364916679498
   Mahadevan S., 2020, BMVC
   Min K, 2019, IEEE I CONF COMP VIS, P2394, DOI 10.1109/ICCV.2019.00248
   Nilsson D, 2018, PROC CVPR IEEE, P6819, DOI 10.1109/CVPR.2018.00713
   Ochs P, 2014, IEEE T PATTERN ANAL, V36, P1187, DOI 10.1109/TPAMI.2013.242
   Ochs P, 2012, PROC CVPR IEEE, P614, DOI 10.1109/CVPR.2012.6247728
   Oh SW, 2018, PROC CVPR IEEE, P7376, DOI 10.1109/CVPR.2018.00770
   Pan YW, 2017, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2017.111
   Papazoglou A, 2013, IEEE I CONF COMP VIS, P1777, DOI 10.1109/ICCV.2013.223
   Paszke A, 2019, ADV NEUR IN, V32
   Peng QM, 2019, IEEE T MULTIMEDIA, V21, P3083, DOI 10.1109/TMM.2019.2918730
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Perazzi F, 2017, PROC CVPR IEEE, P3491, DOI 10.1109/CVPR.2017.372
   Perazzi F, 2015, IEEE I CONF COMP VIS, P3227, DOI 10.1109/ICCV.2015.369
   Reso M, 2013, IEEE I CONF COMP VIS, P385, DOI 10.1109/ICCV.2013.55
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Seong H., 2020, EUR C COMP VIS, P629
   Seonguk Seo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P208, DOI 10.1007/978-3-030-58555-6_13
   Sevilla-Lara Laura, 2019, Pattern Recognition. 40th German Conference, GCPR 2018. Proceedings: Lecture Notes in Computer Science (LNCS 11269), P281, DOI 10.1007/978-3-030-12939-2_20
   Siam M, 2019, IEEE INT CONF ROBOT, P50, DOI [10.1109/ICRA.2019.8794254, 10.1109/icra.2019.8794254]
   Song HM, 2018, LECT NOTES COMPUT SC, V11215, P744, DOI 10.1007/978-3-030-01252-6_44
   Tang Y, 2019, IEEE T CIRC SYST VID, V29, P1973, DOI 10.1109/TCSVT.2018.2859773
   Teed Zachary, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P402, DOI 10.1007/978-3-030-58536-5_24
   Tokmakov P, 2019, INT J COMPUT VISION, V127, P282, DOI 10.1007/s11263-018-1122-2
   Tokmakov P, 2017, IEEE I CONF COMP VIS, P4491, DOI 10.1109/ICCV.2017.480
   Tokmakov P, 2017, PROC CVPR IEEE, P531, DOI 10.1109/CVPR.2017.64
   Le TN, 2018, IEEE T IMAGE PROCESS, V27, P5002, DOI 10.1109/TIP.2018.2849860
   Tsai YH, 2016, PROC CVPR IEEE, P3899, DOI 10.1109/CVPR.2016.423
   Voigtlaender P, 2019, PROC CVPR IEEE, P9473, DOI 10.1109/CVPR.2019.00971
   Wang H, 2020, AAAI CONF ARTIF INTE, V34, P12152
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang WG, 2019, IEEE I CONF COMP VIS, P9235, DOI 10.1109/ICCV.2019.00933
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2019, PROC CVPR IEEE, P3059, DOI 10.1109/CVPR.2019.00318
   Wang WG, 2021, IEEE T PATTERN ANAL, V43, P2413, DOI 10.1109/TPAMI.2020.2966453
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P3137, DOI 10.1109/TIP.2015.2438550
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang YQ, 2021, PROC CVPR IEEE, P8737, DOI 10.1109/CVPR46437.2021.00863
   Wang Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P873, DOI 10.1145/3343031.3350882
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Wenguan Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3395, DOI 10.1109/CVPR.2015.7298961
   WOLFE JM, 1989, J EXP PSYCHOL HUMAN, V15, P419, DOI 10.1037/0096-1523.15.3.419
   Wu Z, 2019, IEEE I CONF COMP VIS, P7263, DOI 10.1109/ICCV.2019.00736
   Xi Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9381, DOI 10.1109/CVPR42600.2020.00940
   Xiao HX, 2020, IEEE T PATTERN ANAL, V42, P1205, DOI 10.1109/TPAMI.2018.2890659
   Xu CL, 2012, LECT NOTES COMPUT SC, V7577, P626, DOI 10.1007/978-3-642-33783-3_45
   Xu MZ, 2020, IEEE T CIRC SYST VID, V30, P2191, DOI 10.1109/TCSVT.2019.2920652
   Xu MZ, 2019, IEEE T MULTIMEDIA, V21, P2790, DOI 10.1109/TMM.2019.2914889
   Xu N, 2018, LECT NOTES COMPUT SC, V11209, P603, DOI 10.1007/978-3-030-01228-1_36
   Yan PX, 2019, IEEE I CONF COMP VIS, P7283, DOI 10.1109/ICCV.2019.00738
   Yang Z, 2019, IEEE I CONF COMP VIS, P931, DOI 10.1109/ICCV.2019.00102
   Zhang ZL, 2018, LECT NOTES COMPUT SC, V11214, P273, DOI 10.1007/978-3-030-01249-6_17
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao WB, 2021, PROC CVPR IEEE, P16821, DOI 10.1109/CVPR46437.2021.01655
   Zheng J, 2019, IEEE ACCESS, V7, P132120, DOI 10.1109/ACCESS.2019.2940768
   Zhou T, 2021, COMPUT VIS MEDIA, V7, P37, DOI 10.1007/s41095-020-0199-z
   Zhou TF, 2020, IEEE T IMAGE PROCESS, V29, P8326, DOI 10.1109/TIP.2020.3013162
   Zhou TF, 2020, AAAI CONF ARTIF INTE, V34, P13066
   Zhou XF, 2018, IEEE T MULTIMEDIA, V20, P2993, DOI 10.1109/TMM.2018.2829605
   Zhuge MC, 2021, PROC CVPR IEEE, P12642, DOI 10.1109/CVPR46437.2021.01246
   Zongxin Yang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P332, DOI 10.1007/978-3-030-58558-7_20
NR 114
TC 4
Z9 4
U1 1
U2 25
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2023
VL 9
IS 1
BP 155
EP 175
DI 10.1007/s41095-021-0262-4
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5K7GQ
UT WOS:000869891100010
OA Green Submitted, gold, Green Published
DA 2024-07-18
ER

PT J
AU Peng, Z
   Jiang, BY
   Xu, HF
   Feng, WQ
   Zhang, JY
AF Peng, Zhuang
   Jiang, Boyi
   Xu, Haofei
   Feng, Wanquan
   Zhang, Juyong
TI Facial optical flow estimation via neural non-rigid registration
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE human face; optical flow; self-supervised; non-rigid registration;
   neural networks; facial priors
AB Optical flow estimation in human facial video, which provides 2D correspondences between adjacent frames, is a fundamental pre-processing step for many applications, like facial expression capture and recognition. However, it is quite challenging as human facial images contain large areas of similar textures, rich expressions, and large rotations. These characteristics also result in the scarcity of large, annotated real-world datasets. We propose a robust and accurate method to learn facial optical flow in a self-supervised manner. Specifically, we utilize various shape priors, including face depth, landmarks, and parsing, to guide the self-supervised learning task via a differentiable nonrigid registration framework. Extensive experiments demonstrate that our method achieves remarkable improvements for facial optical flow estimation in the presence of significant expressions and large rotations.
C1 [Peng, Zhuang; Jiang, Boyi; Xu, Haofei; Feng, Wanquan; Zhang, Juyong] Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhang, JY (corresponding author), Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Peoples R China.
EM pz511@mail.ustc.edu.cn; jby1993@mail.ustc.edu.cn; xhf@mail.ustc.edu.cn;
   lcfwq@mail.ustc.edu.cn; juyong@ustc.edu.cn
CR Bai M, 2016, LECT NOTES COMPUT SC, V9910, P154, DOI 10.1007/978-3-319-46466-4_10
   Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Bottou L., 2012, Neural networks: Tricks of the trade, P421, DOI DOI 10.1007/978-3-642-35289-8_25
   Boykov Y, 1998, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.1998.698673
   Bozic A, 2020, ADV NEURAL INFORM PR, V33, P18727
   Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Feng WQ, 2021, PROC CVPR IEEE, P10292, DOI 10.1109/CVPR46437.2021.01016
   Garg R, 2013, PROC CVPR IEEE, P1272, DOI 10.1109/CVPR.2013.168
   Garg R, 2013, INT J COMPUT VISION, V104, P286, DOI 10.1007/s11263-012-0607-7
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Guo YD, 2021, IEEE T IMAGE PROCESS, V30, P3815, DOI 10.1109/TIP.2021.3065798
   Han XF, 2015, PROC CVPR IEEE, P3279, DOI 10.1109/CVPR.2015.7298948
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hurlburt N, 2015, EARTH SCI INFORM, V8, P959, DOI 10.1007/s12145-015-0224-4
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Koujan MR, 2020, IEEE INT CONF AUTOMA, P24, DOI 10.1109/FG47880.2020.00084
   Koujan MR, 2020, PROC CVPR IEEE, P6617, DOI 10.1109/CVPR42600.2020.00665
   Koujan MR, 2018, PROCEEDINGS CVMP 2018: THE 15TH ACM SIGGRAPH EUROPEAN CONFERENCE ON VISUAL MEDIA PRODUCTION, DOI 10.1145/3278471.3278476
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Li S. Z., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P361, DOI 10.1007/BFb0028368
   Liu PP, 2019, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2019.00470
   Liu PP, 2019, AAAI CONF ARTIF INTE, P8770
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Meister S, 2018, AAAI CONF ARTIF INTE, P7251
   Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925
   Paszke A, 2019, ADV NEUR IN, V32
   Ranjan A., 2018, P BRIT MACH VIS C, V297
   Ranjan A, 2020, INT J COMPUT VISION, V128, P873, DOI 10.1007/s11263-019-01279-w
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Ren Z, 2017, AAAI CONF ARTIF INTE, P1495
   Revaud J, 2015, PROC CVPR IEEE, P1164, DOI 10.1109/CVPR.2015.7298720
   Sidhu Vikramjit, 2020, COMPUTER VISION ECCV, P204
   Simo-Serra E, 2015, IEEE I CONF COMP VIS, P118, DOI 10.1109/ICCV.2015.22
   Simonyan K, 2014, ADV NEUR IN, V27
   Sumner RW, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239531
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Teed Zachary, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P402, DOI 10.1007/978-3-030-58536-5_24
   Wang S, 2018, IEEE IMAGE PROC, P2665, DOI 10.1109/ICIP.2018.8451742
   Wang Y, 2018, PROC CVPR IEEE, P4884, DOI 10.1109/CVPR.2018.00513
   Xing JB, 2021, COMPUT VIS MEDIA, V7, P393, DOI 10.1007/s41095-021-0208-x
   Xu HF, 2020, PROC CVPR IEEE, P1956, DOI 10.1109/CVPR42600.2020.00203
   Yang J, 2020, AAAI CONF ARTIF INTE, V34, P12621
   Yu JJ, 2016, LECT NOTES COMPUT SC, V9915, P3, DOI 10.1007/978-3-319-49409-8_1
   Yu T, 2017, IEEE I CONF COMP VIS, P910, DOI 10.1109/ICCV.2017.104
   Yuxin Yao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7597, DOI 10.1109/CVPR42600.2020.00762
   Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064
   Zhu Y, 2017, IEEE IMAGE PROC, P790, DOI 10.1109/ICIP.2017.8296389
NR 49
TC 3
Z9 3
U1 3
U2 16
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2023
VL 9
IS 1
BP 109
EP 122
DI 10.1007/s41095-021-0267-z
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5K7GQ
UT WOS:000869891100007
OA gold
DA 2024-07-18
ER

PT J
AU Wu, H
   Fu, K
   Zhao, YF
   Song, HK
   Li, J
AF Wu, Heng
   Fu, Kui
   Zhao, Yifan
   Song, Haokun
   Li, Jia
TI Joint self-supervised and reference-guided learning for depth inpainting
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE depth inpainting; self-supervised learning; reference-guided learning
ID RECONSTRUCTION; ENHANCEMENT; SAMPLES
AB Depth information can benefit various computer vision tasks on both images and videos. However, depth maps may suffer from invalid values in many pixels, and also large holes. To improve such data, we propose a joint self-supervised and reference-guided learning approach for depth inpainting. For the self-supervised learning strategy, we introduce an improved spatial convolutional sparse coding module in which total variation regularization is employed to enhance the structural information while preserving edge information. This module alternately learns a convolutional dictionary and sparse coding from a corrupted depth map. Then, both the learned convolutional dictionary and sparse coding are convolved to yield an initial depth map, which is effectively smoothed using local contextual information. The reference-guided learning part is inspired by the fact that adjacent pixels with close colors in the RGB image tend to have similar depth values. We thus construct a hierarchical joint bilateral filter module using the corresponding color image to fill in large holes. In summary, our approach integrates a convolutional sparse coding module to preserve local contextual information and a hierarchical joint bilateral filter module for filling using specific adjacent information. Experimental results show that the proposed approach works well for both invalid value restoration and large hole inpainting.
C1 [Wu, Heng; Fu, Kui; Zhao, Yifan; Song, Haokun; Li, Jia] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
C3 Beihang University
RP Li, J (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM wuheng@buaa.edu.cn; kuifu@buaa.edu.cn; zhaoyf@buaa.edu.cn;
   songhk@buaa.edu.cn; jiali@buaa.edu.cn
RI Li, Jia/AAB-6431-2019
OI Li, Jia/0000-0002-4346-8696
FU National Natural Science Foundation of China [61922006]
FX The authors would like to thank Z.-X. Ma for his helpful participation
   in experiments. This work was partially supported by a grant from the
   National Natural Science Foundation of China (No. 61922006).
CR Affara L., 2018, ARXIV PREPRINT ARXIV
   [Anonymous], 2016, P BRIT MACH VIS C
   Barron JT, 2016, LECT NOTES COMPUT SC, V9907, P617, DOI 10.1007/978-3-319-46487-9_38
   Bristow H., 2014, ARXIV PREPRINT ARXIV
   Bristow H, 2013, PROC CVPR IEEE, P391, DOI 10.1109/CVPR.2013.57
   Chen L, 2012, INT C PATT RECOG, P3070
   Cheng XJ, 2018, LECT NOTES COMPUT SC, V11220, P108, DOI 10.1007/978-3-030-01270-0_7
   Ferstl D, 2013, IEEE I CONF COMP VIS, P993, DOI 10.1109/ICCV.2013.127
   Gong XJ, 2013, IMAGE VISION COMPUT, V31, P695, DOI 10.1016/j.imavis.2013.07.006
   Gu SH, 2015, IEEE I CONF COMP VIS, P1823, DOI 10.1109/ICCV.2015.212
   Hawe S, 2011, IEEE I CONF COMP VIS, P2126, DOI 10.1109/ICCV.2011.6126488
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Heide F, 2015, PROC CVPR IEEE, P5135, DOI 10.1109/CVPR.2015.7299149
   Herrera CD, 2013, LECT NOTES COMPUT SC, V7944, P555
   Hornácek M, 2013, PROC CVPR IEEE, P1123, DOI 10.1109/CVPR.2013.149
   Imran Saif, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12438, DOI 10.1109/CVPR.2019.01273
   Jaritz M, 2018, INT CONF 3D VISION, P52, DOI 10.1109/3DV.2018.00017
   Junyi Liu, 2013, Advances in Multimedia Information Processing - PCM 2013. 14th Pacific-Rim Conference on Multimedia. Proceedings: LNCS 8294, P408, DOI 10.1007/978-3-319-03731-8_38
   Keaomanee Y, 2020, ICT EXPRESS, V6, P209, DOI 10.1016/j.icte.2020.05.004
   Kim J, 2021, INT J ADV ROBOT SYST, V18, DOI 10.1177/1729881421996544
   Ku J, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P16, DOI 10.1109/CRV.2018.00013
   Liu JY, 2012, INT C PATT RECOG, P2055
   Liu LK, 2015, IEEE T IMAGE PROCESS, V24, P1983, DOI 10.1109/TIP.2015.2409551
   Ma FC, 2019, IEEE INT CONF ROBOT, P3288, DOI [10.1109/ICRA.2019.8793637, 10.1109/icra.2019.8793637]
   Ma FC, 2018, IEEE INT CONF ROBOT, P4796
   Matyunin S, 2011, 3DTV CONF
   Miao Liao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P1, DOI 10.1007/978-3-030-58589-1_1
   Mori S, 2020, IEEE T VIS COMPUT GR, V26, P2994, DOI 10.1109/TVCG.2020.3003768
   Neven D, 2017, FAST SCENE UNDERSTAN
   Papyan V, 2017, IEEE I CONF COMP VIS, P5306, DOI 10.1109/ICCV.2017.566
   Qi F, 2013, PATTERN RECOGN LETT, V34, P70, DOI 10.1016/j.patrec.2012.06.003
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28
   Steidl G, 2004, SIAM J NUMER ANAL, V42, P686, DOI 10.1137/S0036142903422429
   Tolgyessy M., 2011, Proc. 2nd Int. Conf. Robotics Education, P143
   Uhrig J, 2017, INT CONF 3D VISION, P11, DOI 10.1109/3DV.2017.00012
   Wang X, 2016, ADV MANUF, V4, P1, DOI 10.1007/s40436-015-0131-4
   Xue HY, 2017, IEEE T IMAGE PROCESS, V26, P4311, DOI 10.1109/TIP.2017.2718183
   Yang LL, 2017, IEEE J-STSP, V11, P1072, DOI 10.1109/JSTSP.2017.2743683
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
   Zhang C., 2021, J. Phys.: Conf. Ser., V1732
   Zhang H, 2017, IEEE WINT CONF APPL, P1259, DOI 10.1109/WACV.2017.145
   Zhang YD, 2018, PROC CVPR IEEE, P175, DOI 10.1109/CVPR.2018.00026
   Zisselman E, 2019, PROC CVPR IEEE, P8200, DOI 10.1109/CVPR.2019.00840
NR 44
TC 1
Z9 1
U1 5
U2 23
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2022
VL 8
IS 4
BP 597
EP 612
DI 10.1007/s41095-021-0259-z
EA MAY 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2X6VI
UT WOS:000801981000001
OA gold
DA 2024-07-18
ER

PT J
AU Song, HX
   Huang, JH
   Cao, YP
   Mu, TJ
AF Song, Haoxuan
   Huang, Jiahui
   Cao, Yan-Pei
   Mu, Tai-Jiang
TI HDR-Net-Fusion: Real-time 3D dynamic scene reconstruction with a
   hierarchical deep reinforcement network
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE dynamic 3D scene reconstruction; deep reinforcement learning; point
   cloud completion; deep neural networks
AB Reconstructing dynamic scenes with commodity depth cameras has many applications in computer graphics, computer vision, and robotics. However, due to the presence of noise and erroneous observations from data capturing devices and the inherently ill-posed nature of non-rigid registration with insufficient information, traditional approaches often produce low-quality geometry with holes, bumps, and misalignments. We propose a novel 3D dynamic reconstruction system, named HDR-Net-Fusion, which learns to simultaneously reconstruct and refine the geometry on the fly with a sparse embedded deformation graph of surfels, using a hierarchical deep reinforcement (HDR) network. The latter comprises two parts: a global HDR-Net which rapidly detects local regions with large geometric errors, and a local HDR-Net serving as a local patch refinement operator to promptly complete and enhance such regions. 'Raining the global HDR-Net is formulated as a novel reinforcement learning problem to implicitly learn the region selection strategy with the goal of improving the overall reconstruction quality. The applicability and efficiency of our approach are demonstrated using a large-scale dynamic reconstruction dataset. Our method can reconstruct geometry with higher quality than traditional methods.
C1 [Song, Haoxuan; Huang, Jiahui; Mu, Tai-Jiang] Tsinghua Univ, BNRist, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Cao, Yan-Pei] Kuaishou Technol Co Ltd, Beijing 100085, Peoples R China.
C3 Tsinghua University
RP Mu, TJ (corresponding author), Tsinghua Univ, BNRist, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
EM songhx17@mails.tsinghua.edu.cn; huang-jh18@mails.tsinghua.edu.cn;
   caoyanpei@gmail.com; taijiang@tsinghua.edu.cn
RI Mu, Tai-Jiang/JWO-1381-2024; Huang, Jiahui/AAN-1773-2021
OI Mu, Tai-Jiang/0000-0002-9197-346X; 
FU National Natural Science Foundation of China [61902210, 61521002]
FX We thank the anonymous reviewers for their helpful comments on this
   paper. This work was supported by the National Natural Science
   Foundation of China (Grant Nos. 61902210 and 61521002).
CR Arulkumaran K, 2017, IEEE SIGNAL PROC MAG, V34, P26, DOI 10.1109/MSP.2017.2743240
   Bozic A, 2020, PROC CVPR IEEE, P7000, DOI 10.1109/CVPR42600.2020.00703
   Brown BJ, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276404, 10.1145/1239451.1239472]
   Chen K., 2015, COMPUT VIS MEDIA, V1, P267, DOI DOI 10.1007/S41095-015-0029-X
   Cifuentes CG, 2017, IEEE ROBOT AUTOM LET, V2, P577, DOI 10.1109/LRA.2016.2645124
   Collet A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766945
   Dou MS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925969
   Du ZJ, 2022, IEEE T VIS COMPUT GR, V28, P1745, DOI 10.1109/TVCG.2020.3028218
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Fujiwara K, 2011, IEEE I CONF COMP VIS, P1527, DOI 10.1109/ICCV.2011.6126411
   Gao W, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV
   Gojcic Z, 2020, PROC CVPR IEEE, P1756, DOI 10.1109/CVPR42600.2020.00183
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Gu XY, 2019, PROC CVPR IEEE, P3249, DOI 10.1109/CVPR.2019.00337
   Guo KW, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356571
   Guo KW, 2018, INT CONF 3D VISION, P596, DOI 10.1109/3DV.2018.00074
   Guo KW, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3083722
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Huang JH, 2021, COMPUT VIS MEDIA, V7, P87, DOI 10.1007/s41095-020-0195-3
   Huang JH, 2020, PROC CVPR IEEE, P2165, DOI 10.1109/CVPR42600.2020.00224
   Innmann M, 2016, LECT NOTES COMPUT SC, V9912, P362, DOI 10.1007/978-3-319-46484-8_22
   Kavan L, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409625.1409627
   Kellert M., 2013, 2013 Conference on Lasers & Electro-Optics. Europe & International Quantum Electronics Conference (CLEO EUROPE/IQEC), DOI 10.1109/CLEOE-IQEC.2013.6800663
   Li RH, 2019, IEEE I CONF COMP VIS, P7202, DOI 10.1109/ICCV.2019.00730
   Lillicrap, 2015, ARXIV150902971, P1
   Liu XY, 2019, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2019.00062
   Liu ZN, 2021, IEEE T VIS COMPUT GR, V27, P83, DOI 10.1109/TVCG.2019.2937300
   Meerits Siim, 2018, Computational Visual Media, V4, P287, DOI 10.1007/s41095-018-0121-0
   Mingsong Dou, 2017, ACM Transactions on Graphics, V36, DOI 10.1145/3130800.3130801
   Mnih V, 2016, PR MACH LEARN RES, V48
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   NEWCOMBE RA, 2015, PROC CVPR IEEE, P343, DOI DOI 10.1109/CVPR.2015.7298631
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Niessner M., 2013, ACM T GRAPHIC, V32, DOI DOI 10.1145/2508363.2508374
   Orts-Escolano S, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P741, DOI 10.1145/2984511.2984517
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Peng XB, 2021, ACM T GRAPHIC, V40, DOI [10.1145/3197517.3201311, 10.1145/3450626.3459670]
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qi CR, 2017, ADV NEUR IN, V30
   Runz Martin, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4471, DOI 10.1109/ICRA.2017.7989518
   Rünz M, 2018, INT SYM MIX AUGMENT, P10, DOI 10.1109/ISMAR.2018.00024
   Schmidt T, 2015, AUTON ROBOT, V39, P239, DOI 10.1007/s10514-015-9462-z
   Slavcheva M, 2018, PROC CVPR IEEE, P2646, DOI 10.1109/CVPR.2018.00280
   Slavcheva M, 2017, PROC CVPR IEEE, P5474, DOI 10.1109/CVPR.2017.581
   Taylor J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925965
   Tchapmi LP, 2019, PROC CVPR IEEE, P383, DOI 10.1109/CVPR.2019.00047
   Tzionas D, 2016, LECT NOTES COMPUT SC, V9915, P620, DOI 10.1007/978-3-319-49409-8_53
   van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094
   Wang SL, 2016, PROC CVPR IEEE, P127, DOI 10.1109/CVPR.2016.21
   Wang YF, 2019, PROC CVPR IEEE, P5951, DOI 10.1109/CVPR.2019.00611
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wang Z., 2016, ARCH COMPUTATIONAL M, P1, DOI DOI 10.1007/s11831-016-9181-4
   Whelan T., 2012, Proceedings of the RSS Workshop on RGB-D: Advanced Reasoning with Depth Cameras
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Xiao YP, 2020, COMPUT VIS MEDIA, V6, P113, DOI 10.1007/s41095-020-0174-8
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yu T, 2020, IEEE T PATTERN ANAL, V42, P2523, DOI 10.1109/TPAMI.2019.2928296
   Yu T, 2017, IEEE I CONF COMP VIS, P910, DOI 10.1109/ICCV.2017.104
   Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088
   Zhu YK, 2017, INT CONF ACOUST SPEE, P5335, DOI 10.1109/ICASSP.2017.7953175
   Zollhöfer M, 2018, COMPUT GRAPH FORUM, V37, P625, DOI 10.1111/cgf.13386
NR 62
TC 6
Z9 7
U1 3
U2 39
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2021
VL 7
IS 4
BP 419
EP 435
DI 10.1007/s41095-021-0230-z
EA AUG 2021
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UK7UX
UT WOS:000683521600002
OA gold
DA 2024-07-18
ER

PT J
AU Wu, X
   Fang, XN
   Chen, T
   Zhang, FL
AF Wu, Xian
   Fang, Xiao-Nan
   Chen, Tao
   Zhang, Fang-Lue
TI JMNet: A joint matting network for automatic human matting
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE alpha matting; human images; deep learning; pose estimation
AB We propose a novel end-to-end deep learning framework, the Joint Matting Network (JMNet), to automatically generate alpha mattes for human images. We utilize the intrinsic structures of the human body as seen in images by introducing a pose estimation module, which can provide both global structural guidance and a local attention focus for the matting task. Our network model includes a pose network, a trimap network, a matting network, and a shared encoder to extract features for the above three networks. We also append a trimap refinement module and utilize gradient loss to provide a sharper alpha matte. Extensive experiments have shown that our method outperforms state-of-theart human matting techniques; the shared encoder leads to better performance and lower memory costs. Our model can process real images downloaded from the Internet for use in composition applications.
C1 [Wu, Xian; Fang, Xiao-Nan] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Chen, Tao] Visual China Grp, AI Ctr, Burlingame, CA 94010 USA.
   [Zhang, Fang-Lue] Victoria Univ Wellington, Sch Engn & Comp Sci, Wellington, New Zealand.
C3 Tsinghua University; Victoria University Wellington
RP Zhang, FL (corresponding author), Victoria Univ Wellington, Sch Engn & Comp Sci, Wellington, New Zealand.
EM thuwx95@gmail.com; fangxn18@mails.tsinghua.edu.cn;
   generalmilk@gmail.com; z.fanglue@gmail.com
RI Fang, Xiaonan/JRW-7699-2023; Chen, Tao/ABB-5983-2022
OI Fang, Xiaonan/0000-0002-4787-5977; 
FU National Natural Science Foundation of China [61561146393, 61521002];
   Victoria Early-Career Research Excellence Award
FX The authors would like to thank all the reviewers. We gratefully
   acknowledge the support of Jian-Cheng Liu who helped prepare and
   preprocess the dataset. This work was supported by National Natural
   Science Foundation of China (Grant Nos. 61561146393 and 61521002).
   Fang-Lue Zhang was supported by a Victoria Early-Career Research
   Excellence Award.
CR Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen QF, 2013, IEEE T PATTERN ANAL, V35, P2175, DOI 10.1109/TPAMI.2013.18
   Chen Q, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P618, DOI 10.1145/3240508.3240610
   Chen X, 2019, ARXIV PREPRINT
   Cho D, 2016, LECT NOTES COMPUT SC, V9906, P626, DOI 10.1007/978-3-319-46475-6_39
   Chu X, 2016, PROC CVPR IEEE, P4715, DOI 10.1109/CVPR.2016.510
   Chuang YY, 2001, PROC CVPR IEEE, P264
   Gastal ESL, 2010, COMPUT GRAPH FORUM, V29, P575, DOI 10.1111/j.1467-8659.2009.01627.x
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2017, P IEEE INT C COMPUTE
   Kaiming He, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2049, DOI 10.1109/CVPR.2011.5995495
   Kikuchi Takazumi, 2018, Computational Visual Media, V4, P43, DOI 10.1007/s41095-017-0098-0
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Liang XD, 2019, IEEE T PATTERN ANAL, V41, P871, DOI 10.1109/TPAMI.2018.2820063
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lutz S., 2018, ARXIV PREPRINT
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Rhemann C, 2009, PROC CVPR IEEE, P1826, DOI 10.1109/CVPRW.2009.5206503
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shen X, 2016, LECT NOTES COMPUT SC, V9905, P92, DOI 10.1007/978-3-319-46448-0_6
   Shen XY, 2016, COMPUT GRAPH FORUM, V35, P93, DOI 10.1111/cgf.12814
   Tang JW, 2019, PROC CVPR IEEE, P3050, DOI 10.1109/CVPR.2019.00317
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Wang Jue, 2007, P IEEE C COMP VIS PA, P1
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Wu X, 2020, IEEE T IMAGE PROCESS, V29, P2344, DOI 10.1109/TIP.2019.2945866
   Xu N, 2017, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.2017.41
   Zhang YK, 2019, PROC CVPR IEEE, P7461, DOI 10.1109/CVPR.2019.00765
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
NR 33
TC 5
Z9 5
U1 2
U2 13
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2020
VL 6
IS 2
BP 215
EP 224
DI 10.1007/s41095-020-0168-6
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FB
UT WOS:000648691300008
OA gold
DA 2024-07-18
ER

PT J
AU Han, L
   Tao, P
   Martin, RR
AF Han, Liang
   Tao, Pin
   Martin, Ralph R.
TI Livestock detection in aerial images using a fully convolutional network
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE livestock detection; segmentation; classification
ID VEHICLE DETECTION
AB In order to accurately count the number of animals grazing on grassland, we present a livestock detection algorithm using modified versions of U-net and Google Inception-v4 net. This method works well to detect dense and touching instances. We also introduce a dataset for livestock detection in aerial images, consisting of 89 aerial images collected by quadcopter. Each image has resolution of about 3000x4000 pixels, and contains livestock with varying shapes, scales, and orientations.We evaluate our method by comparison against Faster RCNN and Yolo-v3 algorithms using our aerial livestock dataset. The average precision of our method is better than Yolo-v3 and is comparable to Faster RCNN.
C1 [Han, Liang] Qinghai Univ, Dept Comp Technol & Applicat, Xining, Peoples R China.
   [Tao, Pin] Tsinghua Univ, Dept Comp Sci & Technol, Beijing, Peoples R China.
   [Martin, Ralph R.] Cardiff Univ, Sch Comp Sci & Informat, Cardiff, Wales.
C3 Qinghai University; Tsinghua University; Cardiff University
RP Han, L (corresponding author), Qinghai Univ, Dept Comp Technol & Applicat, Xining, Peoples R China.
EM hanl2010@qq.com; taopin@tsinghua.edu.cn; MartinRR@cardiff.ac.uk
RI Martin, Ralph R/D-2366-2010
OI Martin, Ralph/0000-0002-8495-8536
FU Scientific and Technological Achievements Transformation Project of
   Qinghai, China [2018-SF-110]; National Natural Science Foundation of
   China [61866031, 61862053]
FX This work was supported by the Scientific and Technological Achievements
   Transformation Project of Qinghai, China (Project No. 2018-SF-110), and
   the National Natural Science Foundation of China (Projects Nos. 61866031
   and 61862053).
CR ACM, 2010, T GRAPHICS, V29
   Cheng G, 2016, IEEE T GEOSCI REMOTE, V54, P7405, DOI 10.1109/TGRS.2016.2601622
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He Kaiming, 2017, P IEEE INT C COMPUTE
   Heitz G, 2008, LECT NOTES COMPUT SC, V5302, P30, DOI 10.1007/978-3-540-88682-2_4
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu K, 2015, IEEE GEOSCI REMOTE S, V12, P1938, DOI 10.1109/LGRS.2015.2439517
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu ZK, 2016, IEEE GEOSCI REMOTE S, V13, P1074, DOI 10.1109/LGRS.2016.2565705
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mundhenk TN, 2016, LECT NOTES COMPUT SC, V9907, P785, DOI 10.1007/978-3-319-46487-9_48
   Razakarivony S, 2016, J VIS COMMUN IMAGE R, V34, P187, DOI 10.1016/j.jvcir.2015.11.002
   Redmon J., 2018, IEEE C COMPUTER VISI
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Sakla W, 2017, IEEE WINT CONF APPL, P916, DOI 10.1109/WACV.2017.107
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Xia GS, 2018, PROC CVPR IEEE, P3974, DOI 10.1109/CVPR.2018.00418
   Zhang FL, 2018, IEEE T MULTIMEDIA, V20, P1987, DOI 10.1109/TMM.2018.2790163
   Zhu HG, 2015, IEEE IMAGE PROC, P3735, DOI 10.1109/ICIP.2015.7351502
NR 26
TC 25
Z9 26
U1 0
U2 26
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2019
VL 5
IS 2
BP 221
EP 228
DI 10.1007/s41095-019-0132-5
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UM
UT WOS:000648690800007
OA gold
DA 2024-07-18
ER

PT J
AU Jing, XY
   Feng, Q
   Lai, YK
   Zhang, JS
   Yu, YQ
   Li, K
AF Jing, Xinyi
   Feng, Qiao
   Lai, Yu-Kun
   Zhang, Jinsong
   Yu, Yuanqiang
   Li, Kun
TI STATE: Learning structure and texture representations for novel view
   synthesis
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE novel view synthesis; sparse views; spatio-view attention; structure
   representation; texture representation
AB Novel viewpoint image synthesis is very challenging, especially from sparse views, due to large changes in viewpoint and occlusion. Existing image-based methods fail to generate reasonable results for invisible regions, while geometry-based methods have difficulties in synthesizing detailed textures. In this paper, we propose STATE, an end-to-end deep neural network, for sparse view synthesis by learning structure and texture representations. Structure is encoded as a hybrid feature field to predict reasonable structures for invisible regions while maintaining original structures for visible regions, and texture is encoded as a deformed feature map to preserve detailed textures. We propose a hierarchical fusion scheme with intra-branch and inter-branch aggregation, in which spatio-view attention allows multi-view fusion at the feature level to adaptively select important information by regressing pixel-wise or voxel-wise confidence maps. By decoding the aggregated features, STATE is able to generate realistic images with reasonable structures and detailed textures. Experimental results demonstrate that our method achieves qualitatively and quantitatively better results than state-of-the-art methods. Our method also enables texture and structure editing applications benefiting from implicit disentanglement of structure and texture.
C1 [Jing, Xinyi; Feng, Qiao; Zhang, Jinsong; Yu, Yuanqiang; Li, Kun] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
   [Lai, Yu-Kun] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF24 4AG, Wales.
C3 Tianjin University; Cardiff University
RP Li, K (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
EM jingxinyi@tju.edu.cn; fengqiao@tju.edu.cn; LaiY4@cardiff.ac.uk;
   jinszhang@tju.edu.cn; yuyuanqiang@tju.edu.cn; lik@tju.edu.cn
RI Lai, Yu-Kun/D-2343-2010
FU National Natural Science Foundation of China [62171317, 62122058]
FX This work was supported in part by the National Natural Science
   Foundation of China (62171317 and 62122058).
CR Chang A. X., 2015, ARXIV
   Chen X, 2019, IEEE I CONF COMP VIS, P4089, DOI 10.1109/ICCV.2019.00419
   Chibane J, 2021, PROC CVPR IEEE, P7907, DOI 10.1109/CVPR46437.2021.00782
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   Eslami SMA, 2018, SCIENCE, V360, P1204, DOI 10.1126/science.aar6170
   Flynn J, 2016, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2016.595
   Galama Y, 2019, COMPUT VIS IMAGE UND, V189, DOI 10.1016/j.cviu.2019.102803
   Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gretton A, 2006, Adv Neural Inf Process Syst, V19
   Guo PS, 2022, IEEE WINT CONF APPL, P11, DOI 10.1109/WACV51458.2022.00009
   Heusel M., 2017, Advances in Neural Information Processing Systems, P6627, DOI [DOI 10.48550/ARXIV.1706.08500, 10.48550/arXiv.1706.08500]
   Hou YX, 2021, IEEE WINT CONF APPL, P3118, DOI 10.1109/WACV48630.2021.00316
   Hu SM, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-020-3097-4
   Jaderberg M, 2015, ADV NEUR IN, V28
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kar A, 2017, ADV NEUR IN, V30
   Kim J, 2020, IEEE IMAGE PROC, P1616, DOI [10.1109/ICIP40778.2020.9191076, 10.1109/icip40778.2020.9191076]
   Kingma D. P., 2014, arXiv
   Kwon Y, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2308, DOI 10.1145/3394171.3413754
   Le H. A., 2020, P BRIT MACHINE VISIO
   Liu XF, 2020, IEEE T INF FOREN SEC, V15, P1501, DOI 10.1109/TIFS.2019.2938418
   Liu XF, 2018, LECT NOTES COMPUT SC, V11215, P573, DOI 10.1007/978-3-030-01252-6_34
   Lombardi S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323020
   Mildenhall Ben, 2020, EUR C COMP VIS ECCV
   Nguyen-Phuoc Thu, 2020, Advances in Neural Information Processing Systems
   Niemeyer M, 2020, PROC CVPR IEEE, P3501, DOI 10.1109/CVPR42600.2020.00356
   Olszewski K, 2019, IEEE I CONF COMP VIS, P7647, DOI 10.1109/ICCV.2019.00774
   Park E, 2017, PROC CVPR IEEE, P702, DOI 10.1109/CVPR.2017.82
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Riegler Gernot, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P623, DOI 10.1007/978-3-030-58529-7_37
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sitzmann V, 2019, ADV NEUR IN, V32
   Sun SH, 2018, LECT NOTES COMPUT SC, V11207, P162, DOI 10.1007/978-3-030-01219-9_10
   Tatarchenko M, 2016, LECT NOTES COMPUT SC, V9911, P322, DOI 10.1007/978-3-319-46478-7_20
   Tewari A, 2020, COMPUT GRAPH FORUM, V39, P701, DOI 10.1111/cgf.14022
   Nguyen-Phuoc T, 2019, IEEE I CONF COMP VIS, P7587, DOI 10.1109/ICCV.2019.00768
   Trevithick A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15162, DOI 10.1109/ICCV48922.2021.01490
   Tulsiani S, 2017, PROC CVPR IEEE, P209, DOI 10.1109/CVPR.2017.30
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yan XC, 2016, ADV NEUR IN, V29
   Yang J., 2015, Advances in Neural Information Processing Systems, P1099
   Yin MY, 2021, PROC CVPR IEEE, P7216, DOI 10.1109/CVPR46437.2021.00714
   Yu A, 2021, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR46437.2021.00455
   Yurui Ren, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7687, DOI 10.1109/CVPR42600.2020.00771
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18
   Zhou WY, 2021, COMPUT VIS MEDIA, V7, P153, DOI 10.1007/s41095-021-0203-2
NR 50
TC 0
Z9 0
U1 3
U2 5
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2023
VL 9
IS 4
BP 767
EP 786
DI 10.1007/s41095-022-0301-9
EA JUL 2023
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P1QC2
UT WOS:001027734000001
OA gold
DA 2024-07-18
ER

PT J
AU Zheng, Q
   Lu, M
   Wu, SC
   Hu, RZ
   Lanir, J
   Huang, H
AF Zheng, Qian
   Lu, Min
   Wu, Sicong
   Hu, Ruizhen
   Lanir, Joel
   Huang, Hui
TI Image-guided color mapping for categorical data visualization
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE color palette; discriminability; image-guided; categorical data
   visualization
ID OPTIMIZATION; PALETTES
AB Appropriate color mapping for categorical data visualization can significantly facilitate the discovery of underlying data patterns and effectively bring out visual aesthetics. Some systems suggest predefined palettes for this task. However, a predefined color mapping is not always optimal, failing to consider users' needs for customization. Given an input categorical data visualization and a reference image, we present an effective method to automatically generate a coloring that resembles the reference while allowing classes to be easily distinguished. We extract a color palette with high perceptual distance between the colors by sampling dominant and discriminable colors from the image's color space. These colors are assigned to given classes by solving an integer quadratic program to optimize point distinctness of the given chart while preserving the color spatial relations in the source image. We show results on various coloring tasks, with a diverse set of new coloring appearances for the input data. We also compare our approach to state-of-the-art palettes in a controlled user study, which shows that our method achieves comparable performance in class discrimination, while being more similar to the source image. User feedback after using our system verifies its efficiency in automatically generating desirable colorings that meet the user's expectations when choosing a reference.
C1 [Zheng, Qian] Suzhou Univ Sci & Technol, Suzhou 215009, Peoples R China.
   [Lu, Min; Wu, Sicong; Hu, Ruizhen; Huang, Hui] Shenzhen Univ, Shenzhen 518052, Peoples R China.
   [Lanir, Joel] Univ Haifa, IL-3498838 Haifa, Israel.
C3 Suzhou University of Science & Technology; Shenzhen University;
   University of Haifa
RP Huang, H (corresponding author), Shenzhen Univ, Shenzhen 518052, Peoples R China.
EM qianzheng85@gmail.com; lumin.vis@gmail.com; wuuusicong@gmail.com;
   ruizhen.hu@gmail.com; ylanir@is.haifa.ac.il; hhzhiyan@gmail.com
RI Huang, Hui/JGB-1049-2023
OI Huang, Hui/0000-0003-3212-0544
FU National Natural Science Foundation of China [U2001206, 61872250]; GD
   Talent Program [2019JC05X328]; GD Natural Science Foundation
   [2020A0505100064, 2021B1515020085]; Shenzhen Science and Technology Key
   Program [RCJC20200714114435012, JCYJ20210324120213036]; DEGP Key Project
   [2018KZDXM058]
FX We thank the reviewers for their valuable comments and constructive
   suggestions. This work was supported in parts by National Natural
   Science Foundation of China (U2001206, 61872250), GD Talent Program
   (2019JC05X328), GD Natural Science Foundation (2020A0505100064,
   2021B1515020085), DEGP Key Project (2018KZDXM058), and Shenzhen Science
   and Technology Key Program (RCJC20200714114435012,
   JCYJ20210324120213036).
CR Aksoy Y, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3002176
   [Anonymous], 2013, ACM T GRAPHIC, DOI DOI 10.1145/2461912.2461988
   Bartram L, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1364, DOI 10.1145/3025453.3026041
   Behrisch M, 2018, COMPUT GRAPH FORUM, V37, P625, DOI 10.1111/cgf.13446
   Bohra M., 2020, P GRAPHICS INTERFACE, P95
   Bridson R., 2007, P ACM SIGGRAPH SKETC
   Chang HW, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766978
   Fang H, 2017, IEEE T VIS COMPUT GR, V23, P871, DOI 10.1109/TVCG.2016.2599214
   Gramazio CC, 2017, IEEE T VIS COMPUT GR, V23, P521, DOI 10.1109/TVCG.2016.2598918
   Harrower M, 2003, CARTOGR J, V40, P27, DOI 10.1179/000870403235002042
   Healey CG, 1996, IEEE VISUAL, P263, DOI 10.1109/VISUAL.1996.568118
   Kim HR, 2014, COMPUT GRAPH FORUM, V33, P309, DOI 10.1111/cgf.12499
   Kita N, 2016, COMPUT GRAPH FORUM, V35, P127, DOI 10.1111/cgf.13010
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee S, 2013, IEEE T VIS COMPUT GR, V19, P1746, DOI 10.1109/TVCG.2012.315
   Leordeanu M., 2009, P 22 INT C NEURAL IN
   Lin S., 2013, P SIGCHI C HUMAN FAC
   Lin SR, 2013, COMPUT GRAPH FORUM, V32, P401, DOI 10.1111/cgf.12127
   Lu KC, 2021, IEEE T VIS COMPUT GR, V27, P475, DOI 10.1109/TVCG.2020.3030406
   Nguyen RMH, 2017, COMPUT GRAPH FORUM, V36, P83, DOI 10.1111/cgf.13274
   O'Donovan P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964958
   Phan HQ, 2018, IEEE T VIS COMPUT GR, V24, P1942, DOI 10.1109/TVCG.2017.2697948
   Poco J, 2018, IEEE T VIS COMPUT GR, V24, P637, DOI 10.1109/TVCG.2017.2744320
   Schlmer T., 2011, P ACM SIGGRAPH S HIG
   Setlur V, 2016, IEEE T VIS COMPUT GR, V22, P698, DOI 10.1109/TVCG.2015.2467471
   Shapira L, 2009, COMPUT GRAPH FORUM, V28, P629, DOI 10.1111/j.1467-8659.2009.01403.x
   Sharma G, 2005, COLOR RES APPL, V30, P21, DOI 10.1002/col.20070
   Shugrina M, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392461
   Silva S, 2011, COMPUT GRAPH-UK, V35, P320, DOI 10.1016/j.cag.2010.11.015
   Stone M., 2014, P COLOR IMAGING C
   Tan JC, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275054
   Tan JC, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2988229
   TinEye, 2021, COLOR EXTRACTION
   TRUMBO BE, 1981, AM STAT, V35, P220, DOI 10.2307/2683294
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang YH, 2019, IEEE T VIS COMPUT GR, V25, P820, DOI 10.1109/TVCG.2018.2864912
   Zeileis A, 2009, COMPUT STAT DATA AN, V53, P3259, DOI 10.1016/j.csda.2008.11.033
   Zhang Q, 2017, IEEE T IMAGE PROCESS, V26, P1952, DOI 10.1109/TIP.2017.2671779
NR 38
TC 2
Z9 3
U1 5
U2 12
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2022
VL 8
IS 4
BP 613
EP 629
DI 10.1007/s41095-021-0258-0
EA MAY 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2X6VI
UT WOS:000805472300001
OA gold
DA 2024-07-18
ER

PT J
AU Wang, WH
   Xie, EZ
   Li, X
   Fan, DP
   Song, KT
   Liang, D
   Lu, T
   Luo, P
   Shao, L
AF Wang, Wenhai
   Xie, Enze
   Li, Xiang
   Fan, Deng-Ping
   Song, Kaitao
   Liang, Ding
   Lu, Tong
   Luo, Ping
   Shao, Ling
TI PVT v2: Improved baselines with Pyramid Vision Transformer
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE transformers; dense prediction; image classification; object detection;
   semantic segmentation
AB Transformers have recently lead to encouraging progress in computer vision. In this work, we present new baselines by improving the original Pyramid Vision Transformer (PVT v1) by adding three designs: (i) a linear complexity attention layer, (ii) an overlapping patch embedding, and (iii) a convolutional feed-forward network. With these modifications, PVT v2 reduces the computational complexity of PVT v1 to linearity and provides significant improvements on fundamental vision tasks such as classification, detection, and segmentation. In particular, PVT v2 achieves comparable or better performance than recent work such as the Swin transformer. We hope this work will facilitate state-of-the-art transformer research in computer vision. Code is available at https://github.com/whai362/PVT.
C1 [Wang, Wenhai] Shanghai AI Lab, Shanghai 200232, Peoples R China.
   [Wang, Wenhai; Lu, Tong] Nanjing Univ, Dept Comp Sci & Technol, Nanjing 210023, Peoples R China.
   [Xie, Enze; Luo, Ping] Univ Hong Kong, Dept Comp Sci, Hong Kong 999077, Peoples R China.
   [Li, Xiang; Song, Kaitao] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210014, Peoples R China.
   [Fan, Deng-Ping] Swiss Fed Inst Technol, Comp Vis Lab, CH-8092 Zurich, Switzerland.
   [Liang, Ding] SenseTime, Beijing 100080, Peoples R China.
   [Shao, Ling] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates.
C3 Shanghai Artificial Intelligence Laboratory; Nanjing University;
   University of Hong Kong; Nanjing University of Science & Technology;
   Swiss Federal Institutes of Technology Domain; ETH Zurich
RP Wang, WH (corresponding author), Shanghai AI Lab, Shanghai 200232, Peoples R China.; Wang, WH (corresponding author), Nanjing Univ, Dept Comp Sci & Technol, Nanjing 210023, Peoples R China.
EM wangwenhai362@gmail.com; xieenzeO@hku.hk; xiang.li.implus@njust.edu.cn;
   dengpfan@gmail.com; kt.song@njust.edu.cn; liangding@scnsetime.com;
   lutong@nju.edu.cn; pluo@cs.hku.hk; ling.shao@inceptioniai.org
RI Li, Xiang/IZQ-4056-2023; Fan, Deng-Ping/ABD-4052-2020; Luo,
   Ping/HGE-7623-2022; wang, wenhai/GUX-3226-2022; Song,
   Kaitao/JKJ-5832-2023; Shao, Ling/D-3535-2011
OI Fan, Deng-Ping/0000-0002-5245-7518; Luo, Ping/0000-0002-6685-7950; 
FU National Natural Science Foundation of China [61672273, 61832008];
   Science Foundation for Distinguished Young Scholars of Jiangsu
   [BK20160021]; Postdoctoral Innovative Talent Support Program of China
   [BX20200168, 2020M681608]; General Research Fund of Hong Kong [27208720]
FX This work was supported by the National Natural Science Foundation of
   China under Grant Nos. 61672273 and 61832008, the Science Foundation for
   Distinguished Young Scholars of Jiangsu under Grant No. BK20160021, the
   Postdoctoral Innovative Talent Support Program of China under Grant Nos.
   BX20200168 and 2020M681608, and the General Research Fund of Hong Kong
   under Grant No. 27208720.
CR Chen C.-F., 2021, P IEEECVF INT C COMP, P357
   Chen K., 2019, arXiv preprint arXiv:1906.07155
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chu X., 2021, P 35 C NEURAL INFORM
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong B., 2021, ARXIV PREPRINT ARXIV
   Dosovitskiy Alexey, 2021, P ICLR 2021, DOI DOI 10.48550/ARXIV.2010.11929
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Graham B., 2021, P IEEE CVF INT C COM, P12259, DOI 10.48550/arXiv.2104.01136
   Han K., 2021, Adv. Neural Inf. Process. Syst., V34, P15908, DOI DOI 10.48550/ARXIV.2103.00112
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D., 2016, PREPRINT
   Howard A. G., 2017, arXiv
   Islam M. A., 2020, P INT C LEARNING REP
   Kirillov A, 2019, PROC CVPR IEEE, P6392, DOI 10.1109/CVPR.2019.00656
   Li X., 2020, P 34 C NEURAL INFORM
   Li YF, 2021, MICROPROCESS MICROSY, V87, DOI 10.1016/j.micpro.2021.104359
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Loshchilov I., 2017, P INT C LEARN REPR
   Loshkarev IY, 2019, J PHYS CONF SER, V1333, DOI 10.1088/1742-6596/1333/4/042019
   Radosavovic Ilija, 2020, P IEEE CVF C COMP VI, P10428
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sun PZ, 2021, PROC CVPR IEEE, P14449, DOI 10.1109/CVPR46437.2021.01422
   Szegedy C., 2015, PROC IEEE C COMPUT V, P1
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Touvron H., 2021, P 38 INT C MACHINE L
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu WJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9961, DOI 10.1109/ICCV48922.2021.00983
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zhang S., 2020, P IEEECVF C COMPUTER, P9759
   Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
NR 41
TC 490
Z9 514
U1 22
U2 22
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2022
VL 8
IS 3
BP 415
EP 424
DI 10.1007/s41095-022-0274-8
EA MAR 2022
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0Q6DG
UT WOS:000769835000001
OA gold
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Xue, Y
   Guo, YC
   Zhang, H
   Xu, T
   Zhang, SH
   Huang, XL
AF Xue, Yuan
   Guo, Yuan-Chen
   Zhang, Han
   Xu, Tao
   Zhang, Song-Hai
   Huang, Xiaolei
TI Deep image synthesis from intuitive user input: A review and
   perspectives
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review
DE image synthesis; intuitive user input; deep generative models;
   synthesized image quality evaluation
ID GENERATION
AB In many applications of computer graphics, art, and design, it is desirable for a user to provide intuitive non-image input, such as text, sketch, stroke, graph, or layout, and have a computer system automatically generate photo-realistic images according to that input. While classically, works that allow such automatic image content generation have followed a framework of image retrieval and composition, recent advances in deep generative models such as generative adversarial networks (GANs), variational autoencoders (VAEs), and flow-based methods have enabled more powerful and versatile image generation approaches. This paper reviews recent works for image synthesis given intuitive user input, covering advances in input versatility, image generation methodology, benchmark datasets, and evaluation metrics. This motivates new perspectives on input representation and interactivity, cross fertilization between major image generation paradigms, and evaluation and comparison of generation methods.
C1 [Xue, Yuan; Huang, Xiaolei] Penn State Univ, Coll Informat Sci & Technol, University Pk, PA 16802 USA.
   [Guo, Yuan-Chen; Zhang, Song-Hai] Tsinghua Univ, Dept Comp Sci & Technol, Beijing, Peoples R China.
   [Guo, Yuan-Chen; Zhang, Song-Hai] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing, Peoples R China.
   [Zhang, Han] Google Brain, Mountain View, CA USA.
   [Xu, Tao] Facebook, Menlo Pk, CA USA.
C3 Pennsylvania Commonwealth System of Higher Education (PCSHE);
   Pennsylvania State University; Pennsylvania State University -
   University Park; Tsinghua University; Tsinghua University; Google
   Incorporated; Facebook Inc
RP Huang, XL (corresponding author), Penn State Univ, Coll Informat Sci & Technol, University Pk, PA 16802 USA.
EM shz@tsinghua.edu.cn; sharon.x.huang@psu.edu
RI Xue, Yuan/ABD-9010-2021; Huang, Xiaolei/AAE-7238-2019
OI Xue, Yuan/0000-0002-5390-9037; Huang, Sharon Xiaolei/0000-0003-2338-6535
FU National Natural Science Foundation of China [61521002, 61772298];
   Research Grant of Beijing Higher Institution Engineering Research
   Center; Tsinghua-Tencent Joint Laboratory for Internet Innovation
   Technology
FX The co-authors Y.-C. Guo and S.-H. Zhang were supported by the National
   Natural Science Foundation of China (Project Nos. 61521002 and
   61772298), a Research Grant of Beijing Higher Institution Engineering
   Research Center, and the Tsinghua-Tencent Joint Laboratory for Internet
   Innovation Technology.
CR Arjovsky M, 2017, PR MACH LEARN RES, V70
   Balakrishnan G, 2018, PROC CVPR IEEE, P8340, DOI 10.1109/CVPR.2018.00870
   Banssal A, 2019, PROC CVPR IEEE, P2312, DOI 10.1109/CVPR.2019.00242
   Bao JM, 2017, IEEE I CONF COMP VIS, P2764, DOI 10.1109/ICCV.2017.299
   Belongie S, 2001, ADV NEUR IN, V13, P831
   Bodla N, 2018, LECT NOTES COMPUT SC, V11209, P689, DOI 10.1007/978-3-030-01228-1_41
   Caesar H, 2018, PROC CVPR IEEE, P1209, DOI 10.1109/CVPR.2018.00132
   Chen LL, 2019, PROC CVPR IEEE, P7824, DOI 10.1109/CVPR.2019.00802
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168
   Chen SY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392386
   Chen T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618470
   Chen WL, 2018, PROC CVPR IEEE, P9416, DOI 10.1109/CVPR.2018.00981
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   de Bem R, 2019, IEEE WINT CONF APPL, P1449, DOI 10.1109/WACV.2019.00159
   de Vries H, 2017, ADV NEUR IN, V30
   Dinh L., 2016, DENSITY ESTIMATION U
   Dong H., 2018, NEURIPS, P474
   Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185540
   Eitz M, 2011, IEEE COMPUT GRAPH, V31, P56, DOI 10.1109/MCG.2011.67
   Gao CY, 2020, PROC CVPR IEEE, P5173, DOI 10.1109/CVPR42600.2020.00522
   Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964
   Ghosh A, 2019, IEEE I CONF COMP VIS, P1171, DOI 10.1109/ICCV.2019.00126
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Goodfellow IJ, 2014, ADV NEUR IN, P2672, DOI DOI 10.1145/3422622
   Gucluturk Yagmur, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P810, DOI 10.1007/978-3-319-46604-0_56
   Gulrajani I., 2017, P 31 INT C NEUR INF, P5769
   Hahn-Powell Gustave V., 2014, Journal of the Acoustical Society of America, V136, DOI 10.1121/1.4899570
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Hensel M, 2017, ADV NEUR IN, V30
   Hinz T., 2019, ICLR
   Hinz Tobias, 2019, ARXIV PREPRINT ARXIV
   Hong S, 2018, PROC CVPR IEEE, P7986, DOI 10.1109/CVPR.2018.00833
   Hu SM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508381
   Huang G. B., 2007, Technical Report, DOI 10.1.1. 122.8268
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Ivanov O., 2018, ARXIV180602382
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133
   Johnson M, 2006, COMPUT GRAPH FORUM, V25, P407, DOI 10.1111/j.1467-8659.2006.00960.x
   Kang H, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P43
   Karras T., 2018, arXiv, DOI [10.48550/arXiv.1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kingma D. P., 2013, ARXIV13126114
   Kingma DP, 2018, 32 C NEURAL INFORM P
   Kipf TN, 2016, ARXIV
   Klys J, 2018, ADV NEURAL INFORM PR, P6444
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Larsen ABL, 2016, PR MACH LEARN RES, V48
   Lassner C, 2017, IEEE I CONF COMP VIS, P853, DOI 10.1109/ICCV.2017.98
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Lee H, 2019, IEEE WINT CONF APPL, P462, DOI 10.1109/WACV.2019.00055
   Li J, 2019, 7 INT C LEARN REPR I
   Li MT, 2019, IEEE WINT CONF APPL, P1403, DOI 10.1109/WACV.2019.00154
   Li WB, 2019, PROC CVPR IEEE, P12166, DOI 10.1109/CVPR.2019.01245
   Li YB, 2018, AAAI CONF ARTIF INTE, P7041
   Li YJ, 2019, PROC CVPR IEEE, P1525, DOI 10.1109/CVPR.2019.00162
   Li YN, 2019, PROC CVPR IEEE, P3688, DOI 10.1109/CVPR.2019.00381
   Li YT, 2019, PROC CVPR IEEE, P6322, DOI 10.1109/CVPR.2019.00649
   Li YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2323, DOI 10.1145/3343031.3350854
   Liang XD, 2015, IEEE I CONF COMP VIS, P1386, DOI 10.1109/ICCV.2015.163
   Liang XD, 2015, IEEE T PATTERN ANAL, V37, P2402, DOI 10.1109/TPAMI.2015.2408360
   Lim J. H., 2017, Geometric gan
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu R., 2019, ARXIV PREPRINT ARXIV
   Liu X., 2019, Advances in Neural Information Processing Systems, P570
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lu YY, 2018, LECT NOTES COMPUT SC, V11220, P213, DOI 10.1007/978-3-030-01270-0_13
   Luo A, 2020, PROC CVPR IEEE, P3753, DOI 10.1109/CVPR42600.2020.00381
   Ma LQ, 2017, ADV NEUR IN, V30
   Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018
   Mansimov Elman, 2015, ARXIV151102793
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Men YF, 2020, PROC CVPR IEEE, P5083, DOI 10.1109/CVPR42600.2020.00513
   Mescheder L., 2017, PR MACH LEARN RES, P2391
   Mescheder L, 2018, PR MACH LEARN RES, V80
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Miyato T., 2018, Proceedings of the 6th International Conference on Learning Representations, P1
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Odena A, 2017, PR MACH LEARN RES, V70
   Pan YW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1789, DOI 10.1145/3123266.3127905
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Portenier T, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201393
   Pumarola A, 2018, PROC CVPR IEEE, P8620, DOI 10.1109/CVPR.2018.00899
   Qi XJ, 2018, PROC CVPR IEEE, P8808, DOI 10.1109/CVPR.2018.00918
   Qian SJ, 2019, IEEE I CONF COMP VIS, P10032, DOI 10.1109/ICCV.2019.01013
   Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160
   Radford A., 2015, ARXIV151106434
   Reed S. E., 2016, ADV NEURAL INFORM PR, V29, P217
   Reed S, 2016, PR MACH LEARN RES, V48
   Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530
   Salimans T, 2016, ADV NEUR IN, V29
   Salimans Tim, 2017, ICLR
   Sangkloy P, 2017, PROC CVPR IEEE, P6836, DOI 10.1109/CVPR.2017.723
   Sangkloy P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925954
   Siarohin A, 2018, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2018.00359
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Simo-Serra E, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925972
   Sohn K., 2015, Neural Information Processing Systems, P3483
   Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28
   Song SJ, 2019, PROC CVPR IEEE, P2352, DOI 10.1109/CVPR.2019.00246
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan F., 2018, ARXIV PREPRINT ARXIV, P6703
   Tang H., 2020, P IEEE CVPR, P7870
   Turmukhambetov D, 2015, COMPUT GRAPH FORUM, V34, P130, DOI 10.1111/cgf.12665
   Vahdat A., 2020, P 34 INT C NEUR INF, V33, P19667
   van den Oord A, 2016, ADV NEUR IN, V29
   van den Oord A, 2016, PR MACH LEARN RES, V48
   Velifickovific P., 2018, P INT C LEARN REPR
   Wang JY, 2018, IEEE ACCESS, V6, P3765, DOI 10.1109/ACCESS.2018.2796638
   Wang M, 2020, COMPUT VIS MEDIA, V6, P3, DOI 10.1007/s41095-020-0162-z
   Wang M, 2019, PROC CVPR IEEE, P1495, DOI 10.1109/CVPR.2019.00159
   Wang M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356520
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P1955, DOI 10.1109/TPAMI.2008.222
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Welinder P., 2010, Technical Report CNS-TR-2010-001
   Wen X, 2020, IEEE T VIS COMPUT GR, V26, P3457, DOI 10.1109/TVCG.2020.3023573
   Winnemoeller H, 2012, COMPUT GRAPH-UK, V36, P740, DOI 10.1016/j.cag.2012.03.004
   Xia W., 2019, ARXIV PREPRINT ARXIV
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47
   Yin GJ, 2019, PROC CVPR IEEE, P2322, DOI 10.1109/CVPR.2019.00243
   Yu A, 2017, IEEE I CONF COMP VIS, P5571, DOI 10.1109/ICCV.2017.594
   Yu A, 2014, PROC CVPR IEEE, P192, DOI 10.1109/CVPR.2014.32
   Yu Q, 2016, PROC CVPR IEEE, P799, DOI 10.1109/CVPR.2016.93
   Yuan Xue, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P279, DOI 10.1007/978-3-030-58574-7_17
   Zhang G, 2018, LECT NOTES COMPUT SC, V11210, P422, DOI 10.1007/978-3-030-01231-1_26
   Zhang H., 2021, ARXIV PREPRINT ARXIV
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang ZZ, 2018, PROC CVPR IEEE, P6199, DOI 10.1109/CVPR.2018.00649
   Zhao B, 2019, PROC CVPR IEEE, P8576, DOI 10.1109/CVPR.2019.00878
   Zhen Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P5466, DOI 10.1109/CVPR42600.2020.00551
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhou BL, 2019, INT J COMPUT VISION, V127, P302, DOI 10.1007/s11263-018-1140-0
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
   Zhou H, 2019, AAAI CONF ARTIF INTE, P9299
   Zhu MF, 2019, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR.2019.00595
   Zhu PH, 2020, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR42600.2020.00515
   Zhu Z, 2019, PROC CVPR IEEE, P2342, DOI 10.1109/CVPR.2019.00245
NR 152
TC 21
Z9 21
U1 7
U2 78
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2022
VL 8
IS 1
BP 3
EP 31
DI 10.1007/s41095-021-0234-8
PG 29
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WO6VR
UT WOS:000712590200002
OA gold, Green Submitted
DA 2024-07-18
ER

PT J
AU Ge, YL
   Zhang, C
   Wang, K
   Liu, ZQ
   Bi, HB
AF Ge, Yanliang
   Zhang, Cong
   Wang, Kang
   Liu, Ziqi
   Bi, Hongbo
TI WGI-Net: A weighted group integration network for RGB-D salient object
   detection
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE weighted group; depth information; RGB-D information; salient object
   detection; deep learning
ID IMAGE; SEGMENTATION; FUSION; MODEL
AB Salient object detection is used as a pre-process in many computer vision tasks (such as salient object segmentation, video salient object detection, etc.). When performing salient object detection, depth information can provide clues to the location of target objects, so effective fusion of RGB and depth feature information is important. In this paper, we propose a new feature information aggregation approach, weighted group integration (WGI), to effectively integrate RGB and depth feature information. We use a dual-branch structure to slice the input RGB image and depth map separately and then merge the results separately by concatenation. As grouped features may lose global information about the target object, we also make use of the idea of residual learning, taking the features captured by the original fusion method as supplementary information to ensure both accuracy and completeness of the fused information. Experiments on five datasets show that our model performs better than typical existing approaches for four evaluation metrics.
C1 [Ge, Yanliang; Zhang, Cong; Wang, Kang; Liu, Ziqi; Bi, Hongbo] Northeast Petr Univ, Sch Elect Informat Engn, Daqing 163000, Peoples R China.
C3 Northeast Petroleum University
RP Bi, HB (corresponding author), Northeast Petr Univ, Sch Elect Informat Engn, Daqing 163000, Peoples R China.
EM 15804593399@139.com; congzhang98@126.com; kangwangwww@163.com;
   zicky2015@sina.com; bhbdq@126.com
RI Liu, Ziqi/AAP-2559-2020
OI Liu, Ziqi/0000-0001-9096-840X
FU NEPU Natural Science Foundation [2017PY ZL-05, 2018QNL-51, JY_CX_CX06
   2018, JY_CX_JG06 2018, JY_CX_14_2020]
FX This work was supported by the NEPU Natural Science Foundation under
   Grants Nos. 2017PY ZL-05, 2018QNL-51, JY_CX_CX06 2018, JY_CX_JG06 2018,
   and JY_CX_14_2020.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Bi HB, 2019, IEEE IMAGE PROC, P4654, DOI [10.1109/icip.2019.8803611, 10.1109/ICIP.2019.8803611]
   Borji A., 2012, 2012 IEEE COMPUTER S, P23, DOI 10.1109/CVPRW.2012.6239191
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen H, 2019, PATTERN RECOGN, V86, P376, DOI 10.1016/j.patcog.2018.08.007
   Cheng Y., 2014, ICIMCS, P23
   Cong RM, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2557347
   Cong RM, 2020, IEEE T CYBERNETICS, V50, P3627, DOI 10.1109/TCYB.2019.2932005
   Desingh K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.98
   Ding YY, 2011, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2011.5995445
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Fan XX, 2014, INT CONF DIGIT SIG, P454, DOI 10.1109/ICDSP.2014.6900706
   Feng D, 2016, PROC CVPR IEEE, P2343, DOI 10.1109/CVPR.2016.257
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P4290, DOI 10.1109/TIP.2012.2199502
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Guo J., 2016, P ANN REL MAINT S, P1, DOI DOI 10.1109/RAMS.2016.7448068
   Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775
   Han JW, 2006, IEEE T CIRC SYST VID, V16, P141, DOI 10.1109/TCSVT.2005.859028
   He JF, 2012, PROC CVPR IEEE, P3005, DOI 10.1109/CVPR.2012.6248030
   Hong S, 2015, PR MACH LEARN RES, V37, P597
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Huang PS, 2018, INT CONF DIGIT SIG
   Jerripothula KR, 2016, IEEE T MULTIMEDIA, V18, P1896, DOI 10.1109/TMM.2016.2576283
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Lee H, 2018, IEEE WINT CONF APPL, P1170, DOI 10.1109/WACV.2018.00133
   Li CY, 2021, IEEE T CYBERNETICS, V51, P88, DOI 10.1109/TCYB.2020.2969255
   Liang FF, 2018, NEUROCOMPUTING, V275, P2227, DOI 10.1016/j.neucom.2017.10.052
   Liu ZY, 2019, NEUROCOMPUTING, V363, P46, DOI 10.1016/j.neucom.2019.07.012
   Liu Z, 2012, IEEE T MULTIMEDIA, V14, P1275, DOI 10.1109/TMM.2012.2190385
   Marchesotti L, 2009, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2009.5459467
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Rutishauser U, 2004, PROC CVPR IEEE, P37
   Shigematsu R, 2017, IEEE INT CONF COMP V, P2749, DOI 10.1109/ICCVW.2017.323
   Song HK, 2017, IEEE T IMAGE PROCESS, V26, P4204, DOI 10.1109/TIP.2017.2711277
   Wang AZ, 2017, NEURAL PROCESS LETT, V46, P1083, DOI 10.1007/s11063-017-9610-x
   Wang AZ, 2017, IEEE SIGNAL PROC LET, V24, P663, DOI 10.1109/LSP.2017.2688136
   Wang NN, 2019, IEEE ACCESS, V7, P55277, DOI 10.1109/ACCESS.2019.2913107
   Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154
   Wang WG, 2017, IEEE I CONF COMP VIS, P2205, DOI 10.1109/ICCV.2017.240
   Xie WH, 2013, CHINA SATELLITE NAVIGATION CONFERENCE (CSNC) 2013 PROCEEDINGS: BEIDOU/GNSS NAVIGATION APPLICATIONS, TEST & ASSESSMENT TECHNOLOGY, USER TERMINAL TECHNOLOGY, P585, DOI 10.1007/978-3-642-37398-5_54
   Ye LW, 2017, IEEE T MULTIMEDIA, V19, P1742, DOI 10.1109/TMM.2017.2693022
   Yongri Piao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9057, DOI 10.1109/CVPR42600.2020.00908
   Zhang J, 2020, ARXIV200903075
   Zhang J, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3107956
   Zhang JL, 2020, LECT NOTES COMPUT SC, V11962, P361, DOI 10.1007/978-3-030-37734-2_30
   Zhang K., 2020, P IEEE CVF C COMP VI, P9050
   Zhang M, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4107, DOI [10.1145/3394171.3413969, 10.1145/33941713413969]
   Zhang M, 2020, IEEE T IMAGE PROCESS, V29, P6276, DOI 10.1109/TIP.2020.2990341
   Zhao JX, 2019, PROC CVPR IEEE, P3922, DOI 10.1109/CVPR.2019.00405
   Zhou T, 2021, COMPUT VIS MEDIA, V7, P37, DOI 10.1007/s41095-020-0199-z
   Zhu CB, 2019, IEEE INT CON MULTI, P199, DOI 10.1109/ICME.2019.00042
NR 62
TC 10
Z9 10
U1 1
U2 19
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2021
VL 7
IS 1
BP 115
EP 125
DI 10.1007/s41095-020-0200-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FQ
UT WOS:000648692800007
OA gold
DA 2024-07-18
ER

PT J
AU Sheng, KK
   Dong, WM
   Huang, HB
   Chai, ML
   Zhang, Y
   Ma, CY
   Hu, BG
AF Sheng, Kekai
   Dong, Weiming
   Huang, Haibin
   Chai, Menglei
   Zhang, Yong
   Ma, Chongyang
   Hu, Bao-Gang
TI Learning to assess visual aesthetics of food images
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE image aesthetic assessment; food image analysis; dataset; regularization
ID NETWORKS
AB Distinguishing aesthetically pleasing food photos from others is an important visual analysis task for social media and ranking systems related to food. Nevertheless, aesthetic assessment of food images remains a challenging and relatively unexplored task, largely due to the lack of related food image datasets and practical knowledge. Thus, we present the Gourmet Photography Dataset (GPD), the first large-scale dataset for aesthetic assessment of food photos. It contains 24,000 images with corresponding binary aesthetic labels, covering a large variety of foods and scenes. We also provide a non-stationary regularization method to combat over-fitting and enhance the ability of tuned models to generalize. Quantitative results from extensive experiments, including a generalization ability test, verify that neural networks trained on the GPD achieve comparable performance to human experts on the task of aesthetic assessment. We reveal several valuable findings to support further research and applications related to visual aesthetic analysis of food images. To encourage further research, we have made the GPD publicly available at https://github.com/Openning07/GPA.
C1 [Sheng, Kekai] Tencent, Youtu Lab, Shanghai 200233, Peoples R China.
   [Sheng, Kekai; Dong, Weiming; Hu, Bao-Gang] Chinese Acad Sci, Inst Automat, NLPR, Beijing 100190, Peoples R China.
   [Huang, Haibin; Ma, Chongyang] Kuaishou Technol, Beijing 100085, Peoples R China.
   [Chai, Menglei] Snap Inc, Santa Monica, CA 90405 USA.
   [Zhang, Yong] Tencent Inc, AI Lab, Shenzhen 518000, Peoples R China.
C3 Tencent; Chinese Academy of Sciences; Institute of Automation, CAS;
   Tencent
RP Dong, WM (corresponding author), Chinese Acad Sci, Inst Automat, NLPR, Beijing 100190, Peoples R China.
EM saulsheng@tencent.com; weiming.dong@ia.ac.cn;
   huanghaibin03@kuaishou.com; mchai@snap.com; norriszhang@tencent.com;
   chongyangma@kuaishou.com; hubg@nlpr.ia.ac.cn
RI Huang, Haibin/HHZ-1901-2022; DONG, Weiming/AAG-7678-2020
OI DONG, Weiming/0000-0001-6502-145X; Hu, Bao-Gang/0000-0002-6916-5394
FU National Natural Science Foundation of China [61832016, 61672520];
   CASIA-Tencent Youtu joint research project
FX This work was supported by the National Natural Science Foundation of
   China under Grant Nos. 61832016 and 61672520, and by a CASIA-Tencent
   Youtu joint research project.
CR [Anonymous], 2014, ARXIV150302531
   Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29
   Chang HW, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925908
   Chang KY, 2017, IEEE I CONF COMP VIS, P3534, DOI 10.1109/ICCV.2017.380
   Chen X., 2017, arXiv preprint arXiv:1705.02743
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng YB, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P870, DOI 10.1145/3240508.3240531
   Hassannejad H, 2016, MADIMA'16: PROCEEDINGS OF THE 2ND INTERNATIONAL WORKSHOP ON MULTIMEDIA ASSISTED DIETARY MANAGEMENT, P41, DOI 10.1145/2986035.2986042
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hein M, 2019, PROC CVPR IEEE, P41, DOI 10.1109/CVPR.2019.00013
   Hosu V, 2019, PROC CVPR IEEE, P9367, DOI 10.1109/CVPR.2019.00960
   Hu YM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3181974
   Hung W.-C., 2018, P EUR C COMP VIS ECC, P70
   Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kucer M, 2018, IEEE T IMAGE PROCESS, V27, P5100, DOI 10.1109/TIP.2018.2845100
   Li Y, 2015, IEEE IMAGE PROC, P311, DOI 10.1109/ICIP.2015.7350810
   Liu Zhenguang, 2018, IEEE Trans Image Process, DOI 10.1109/TIP.2018.2828326
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Luo W, 2011, IEEE I CONF COMP VIS, P2206, DOI 10.1109/ICCV.2011.6126498
   Ma S, 2017, PROC CVPR IEEE, P722, DOI 10.1109/CVPR.2017.84
   Mai L, 2016, PROC CVPR IEEE, P497, DOI 10.1109/CVPR.2016.60
   MANNA Lou., 2015, Digital Food Photography
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Myers A, 2015, IEEE I CONF COMP VIS, P1233, DOI 10.1109/ICCV.2015.146
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Papadopoulos DP, 2019, PROC CVPR IEEE, P7994, DOI 10.1109/CVPR.2019.00819
   Salvador A, 2017, PROC CVPR IEEE, P3068, DOI 10.1109/CVPR.2017.327
   Sanderson M, 2010, NAT LANG ENG, V16, P100, DOI 10.1017/S1351324909005129
   Sheng K. K., 2018, SA18 SIGGR AS 2018, DOI DOI 10.1145/3283254.3283260
   Sheng KK, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P879, DOI 10.1145/3240508.3240554
   Sheng KK, 2015, COMPUT GRAPH FORUM, V34, P213, DOI 10.1111/cgf.12760
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sun RJ, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2510
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Xu Z, 2018, IEEE T PATTERN ANAL, V40, P1100, DOI 10.1109/TPAMI.2016.2637331
   Yu WH, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P649, DOI 10.1145/3178876.3186146
   Zhang FL, 2013, IEEE T MULTIMEDIA, V15, P1480, DOI 10.1109/TMM.2013.2268051
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang XJ, 2016, J COMPUT SCI TECH-CH, V31, P489, DOI 10.1007/s11390-016-1642-6
   Zhang XD, 2019, IEEE T MULTIMEDIA, V21, P2815, DOI 10.1109/TMM.2019.2911428
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
NR 46
TC 14
Z9 14
U1 2
U2 20
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2021
VL 7
IS 1
BP 139
EP 152
DI 10.1007/s41095-020-0193-5
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FQ
UT WOS:000648692800009
OA gold
DA 2024-07-18
ER

PT J
AU Zou, DN
   Zhang, SH
   Mu, TJ
   Zhang, M
AF Zou, Ding-Nan
   Zhang, Song-Hai
   Mu, Tai-Jiang
   Zhang, Min
TI A new dataset of dog breed images and a benchmark for finegrained
   classification
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE fine-grained classification; dog; dataset; benchmark
AB In this paper, we introduce an image dataset for fine-grained classification of dog breeds: the Tsinghua Dogs Dataset. It is currently the largest dataset for fine-grained classification of dogs, including 130 dog breeds and 70,428 real-world images. It has only one dog in each image and provides annotated bounding boxes for the whole body and head. In comparison to previous similar datasets, it contains more breeds and more carefully chosen images for each breed. The diversity within each breed is greater, with between 200 and 7000+ images for each breed. Annotation of the whole body and head makes the dataset not only suitable for the improvement of finegrained image classification models based on overall features, but also for those locating local informative parts. We show that dataset provides a tough challenge by benchmarking several state-of-the-art deep neural models. The dataset is available for academic purposes at https://cg.cs.tsinghua.edu.cn/ThuDogs/.
C1 [Zou, Ding-Nan; Zhang, Song-Hai; Mu, Tai-Jiang] Tsinghua Univ, BNRist, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Zou, Ding-Nan] NaJiu Co, Najiu 410022, Hunan, Peoples R China.
   [Zhang, Min] Harvard Med Sch, Brigham & Womens Hosp, Boston, MA 02115 USA.
C3 Tsinghua University; Harvard University; Brigham & Women's Hospital;
   Harvard Medical School
RP Zhang, SH (corresponding author), Tsinghua Univ, BNRist, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
EM zoudn14@mails.tsinghua.edu.cn; shz@tsinghua.edu.cn;
   taijiang@tsinghua.edu.cn; mzhang@bwh.harvard.edu
RI Mu, Tai-Jiang/JWO-1381-2024
OI Mu, Tai-Jiang/0000-0002-9197-346X
FU National Natural Science Foundation of China [61521002, 61772298];
   Research Grant of Beijing Higher Institution Engineering Research
   Center; Tsinghua-Tencent Joint Laboratory for Internet Innovation
   Technology
FX The authors would like to thank Wei-Yu Xie for his assistance on paper
   writing, and also thank Qiu Xin and Zhi-Ping Zhang for much help on
   image processing and labeling. This work was supported by the National
   Natural Science Foundation of China (Project Nos. 61521002 and
   61772298), a Research Grant of Beijing Higher Institution Engineering
   Research Center, and Tsinghua-Tencent Joint Laboratory for Internet
   Innovation Technology.
CR [Anonymous], 2013, Tech. rep.
   [Anonymous], 2020, ARXIV200303836
   [Anonymous], 2014, BRIT C MACH VIS
   Berg T, 2013, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2013.128
   Cai SJ, 2017, IEEE I CONF COMP VIS, P511, DOI 10.1109/ICCV.2017.63
   Chen Lin, 2017, [Computational Visual Media, 计算可视媒体], V3, P83
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Cui Y, 2018, PROC CVPR IEEE, P4109, DOI 10.1109/CVPR.2018.00432
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dubey A., 2018, ARXIV180905934
   Dubey A, 2018, LECT NOTES COMPUT SC, V11216, P71, DOI 10.1007/978-3-030-01258-8_5
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Ge W. F., 2019, ARXIV190302827
   Hu Tao, 2019, See better before looking closer: Weakly supervised data augmentation network for fine-grained visual classification
   Huang GL, 2017, IEEE ICC
   Kai-Xuan Chen, 2018, Computational Visual Media, V4, P245, DOI 10.1007/s41095-018-0119-7
   Khosla A., 2011, CVPR
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lam M, 2017, PROC CVPR IEEE, P6497, DOI 10.1109/CVPR.2017.688
   Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu JX, 2012, LECT NOTES COMPUT SC, V7572, P172, DOI 10.1007/978-3-642-33718-5_13
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Niu L., 2018, ARXIV181107567
   Qian Q, 2015, PROC CVPR IEEE, P3716, DOI 10.1109/CVPR.2015.7298995
   Ren Jieyi, 2017, [Computational Visual Media, 计算可视媒体], V3, P379
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Sun G., 2019, ARXIV191206842
   Sun M, 2018, LECT NOTES COMPUT SC, V11220, P834, DOI 10.1007/978-3-030-01270-0_49
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   Wah Catherine, 2011, Technical report
   Wang YM, 2018, PROC CVPR IEEE, P4148, DOI 10.1109/CVPR.2018.00436
   Wang YM, 2016, PROC CVPR IEEE, P1163, DOI 10.1109/CVPR.2016.131
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu Z, 2015, IEEE I CONF COMP VIS, P2524, DOI 10.1109/ICCV.2015.290
   Yang Z, 2018, LECT NOTES COMPUT SC, V11218, P438, DOI 10.1007/978-3-030-01264-9_26
   Yu CJ, 2018, LECT NOTES COMPUT SC, V11220, P595, DOI 10.1007/978-3-030-01270-0_35
   Zhang F., 2020, ARXIV200309150
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang XF, 2016, PROC CVPR IEEE, P1114, DOI 10.1109/CVPR.2016.126
   Zheng HL, 2019, PROC CVPR IEEE, P5007, DOI 10.1109/CVPR.2019.00515
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zhuang P., 2020, P AAAI C ART INT
NR 47
TC 13
Z9 14
U1 2
U2 18
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2020
VL 6
IS 4
BP 477
EP 487
DI 10.1007/s41095-020-0184-6
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FN
UT WOS:000648692500009
OA gold
DA 2024-07-18
ER

PT J
AU Borji, A
   Cheng, MM
   Hou, QB
   Jiang, HZ
   Li, J
AF Borji, Ali
   Cheng, Ming-Ming
   Hou, Qibin
   Jiang, Huaizu
   Li, Jia
TI Salient object detection: A survey
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review
DE salient object detection; saliency; visual attention; regions of
   interest
ID IMAGE QUALITY ASSESSMENT; VISUAL-ATTENTION; REGION DETECTION; MODEL;
   INTEGRATION; EXTRACTION; SCENE; COLOR; SEGMENTATION; CONTEXT
AB Detecting and segmenting salient objects from natural scenes, often referred to as salient object detection, has attracted great interest in computer vision. While many models have been proposed and several applications have emerged, a deep understanding of achievements and issues remains lacking. We aim to provide a comprehensive review of recent progress in salient object detection and situate this field among other closely related areas such as generic scene segmentation, object proposal generation, and saliency for fixation prediction. Covering 228 publications, we survey i) roots, key concepts, and tasks, ii) core techniques and main modeling trends, and iii) datasets and evaluation metrics for salient object detection. We also discuss open problems such as evaluation metrics and dataset bias in model performance, and suggest future research directions.
C1 [Borji, Ali] MarkableAI, New York, NY USA.
   [Cheng, Ming-Ming; Hou, Qibin] Nankai Univ, Coll Comp Sci, TKLNDST, Tianjin, Peoples R China.
   [Jiang, Huaizu] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.
   [Li, Jia] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
C3 Nankai University; University of Massachusetts System; University of
   Massachusetts Amherst; Beihang University
RP Cheng, MM (corresponding author), Nankai Univ, Coll Comp Sci, TKLNDST, Tianjin, Peoples R China.
EM ali@markable.ai.com; cmm@nankai.edu.cn; jiali@buaa.edu.cn
RI Jiang, Huaizu/AAE-5876-2022; Cheng, Ming-Ming/A-2527-2009; Li,
   Jia/AAB-6431-2019
OI Jiang, Huaizu/0000-0002-2300-4237; Cheng, Ming-Ming/0000-0001-5550-8758;
   Li, Jia/0000-0002-4346-8696
CR Achanta R, 2008, LECT NOTES COMPUT SC, V5008, P66
   Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Adams A, 2010, COMPUT GRAPH FORUM, V29, P753, DOI 10.1111/j.1467-8659.2009.01645.x
   Alexe B, 2010, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2010.5540226
   Allili M.S., 2007, IEEE Conf. on Computer Vision and pattern Recognition, P1
   Alpert S, 2007, P IEEE C COMPUTER VI, P1
   [Anonymous], 2016, P 2016 ACM MULT C
   [Anonymous], 2011, ACM T GRAPHICS, V30, P6
   [Anonymous], 2006, P CVPR
   [Anonymous], IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2010.70
   [Anonymous], 2011, ADV NEURAL INF PROCE
   [Anonymous], 2010, P BRIT MACH VIS C
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2009, ACM T GRAPHICS, V28, P5
   [Anonymous], 2015, PROCIEEE CONFCOMPUT
   Arbeláez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080
   Berg AC, 2012, PROC CVPR IEEE, P3562, DOI 10.1109/CVPR.2012.6248100
   Borji A., 2012, 2012 IEEE COMPUTER S, P23, DOI 10.1109/CVPRW.2012.6239191
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P742, DOI 10.1109/TIP.2014.2383320
   Borji A, 2013, IEEE I CONF COMP VIS, P921, DOI 10.1109/ICCV.2013.118
   Borji A, 2013, VISION RES, V91, P62, DOI 10.1016/j.visres.2013.07.016
   Borji A, 2013, IEEE T IMAGE PROCESS, V22, P55, DOI 10.1109/TIP.2012.2210727
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Borji A, 2012, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2012.6247711
   Borji A, 2012, PROC CVPR IEEE, P438, DOI 10.1109/CVPR.2012.6247706
   Borji A, 2011, IEEE INT CONF ROBOT, P1902
   Borji A, 2011, MACH VISION APPL, V22, P61, DOI 10.1007/s00138-009-0192-0
   Brown M, 2011, PROC CVPR IEEE, P177, DOI 10.1109/CVPR.2011.5995637
   Bruce N., 2005, ADV NEURAL INFORM PR, V18, P155
   Bylinskii Z., 2015, MIT saliency benchmark
   Bylinskii Z, 2016, LECT NOTES COMPUT SC, V9909, P809, DOI 10.1007/978-3-319-46454-1_49
   Chang KY, 2011, IEEE I CONF COMP VIS, P914, DOI 10.1109/ICCV.2011.6126333
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen TS, 2016, IEEE T NEUR NET LEAR, V27, P1135, DOI 10.1109/TNNLS.2015.2506664
   Cheng HD, 2001, PATTERN RECOGN, V34, P2259, DOI 10.1016/S0031-3203(00)00149-7
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193
   Cheng MM, 2014, VISUAL COMPUT, V30, P443, DOI 10.1007/s00371-013-0867-4
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Chia AYS, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024190
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Desingh K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.98
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Donoser M, 2009, IEEE I CONF COMP VIS, P817, DOI 10.1109/ICCV.2009.5459296
   Ehinger K A, 2011, P ANN COGN SCI C
   Elder, 2010, P IEEE C COMP VIS PA, P49, DOI [10.1109/CVPRW.2010.5543739, DOI 10.1109/CVPRW.2010.5543739]
   Endres I, 2010, LECT NOTES COMPUT SC, V6315, P575, DOI 10.1007/978-3-642-15555-0_42
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Feng J, 2011, IEEE I CONF COMP VIS, P1028, DOI 10.1109/ICCV.2011.6126348
   Feng SH, 2010, SIGNAL PROCESS, V90, P1, DOI 10.1016/j.sigpro.2009.05.017
   Frintrop S, 2014, INT C PATT RECOG, P2329, DOI 10.1109/ICPR.2014.404
   Frintrop S, 2009, IEEE INT CONF ROBOT, P758
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Garcia German Martin, 2012, Pattern Recognition. Proceedings Joint 34th DAGM and 36th OAGM Symposium, P357, DOI 10.1007/978-3-642-32717-9_36
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Goferman S, 2010, COMPUT GRAPH FORUM, V29, P459, DOI 10.1111/j.1467-8659.2009.01615.x
   Goldberg C, 2012, COMPUT GRAPH FORUM, V31, P265, DOI 10.1111/j.1467-8659.2012.03005.x
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Gygli M, 2013, IEEE I CONF COMP VIS, P1633, DOI 10.1109/ICCV.2013.205
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He SF, 2015, INT J COMPUT VISION, V115, P330, DOI 10.1007/s11263-015-0822-0
   Hosang Jan, 2016, IEEE Trans Pattern Anal Mach Intell, V38, P814, DOI 10.1109/TPAMI.2015.2465908
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Hou XD, 2007, PROC CVPR IEEE, P2280
   Hua G, 2006, IEEE T PATTERN ANAL, V28, P1701, DOI 10.1109/TPAMI.2006.209
   Isola P, 2011, PROC CVPR IEEE, P145, DOI 10.1109/CVPR.2011.5995721
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2009, VISION RES, V49, P1295, DOI 10.1016/j.visres.2008.09.007
   Jaderberg M, 2015, ADV NEUR IN, V28
   Ji QG, 2013, SIGNAL PROCESS-IMAGE, V28, P241, DOI 10.1016/j.image.2012.11.008
   Jia YQ, 2013, IEEE I CONF COMP VIS, P1761, DOI 10.1109/ICCV.2013.221
   Jiang BW, 2013, IEEE I CONF COMP VIS, P1665, DOI 10.1109/ICCV.2013.209
   Jiang HZ, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.110
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jiang P, 2013, IEEE I CONF COMP VIS, P1976, DOI 10.1109/ICCV.2013.248
   Jiang Y-G, 2013, P 27 AAAI C ART INT
   Jiang ZL, 2013, PROC CVPR IEEE, P2043, DOI 10.1109/CVPR.2013.266
   Johnson-Roberson M, 2010, IEEE INT C INT ROBOT, P1165, DOI 10.1109/IROS.2010.5649872
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kai-Yueh Chang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2129, DOI 10.1109/CVPR.2011.5995415
   Kanan C, 2010, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2010.5539947
   Karpathy A, 2013, IEEE INT CONF ROBOT, P2088, DOI 10.1109/ICRA.2013.6630857
   Katti H, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P1433, DOI 10.1109/ICME.2008.4607714
   Khuwuthyakorn P, 2010, LECT NOTES COMPUT SC, V6312, P636, DOI 10.1007/978-3-642-15552-9_46
   Kim G, 2011, IEEE I CONF COMP VIS, P169, DOI 10.1109/ICCV.2011.6126239
   Kim J, 2014, PROC CVPR IEEE, P883, DOI 10.1109/CVPR.2014.118
   Kim J, 2016, INT C PATT RECOG, P609, DOI 10.1109/ICPR.2016.7899701
   Klein DA, 2011, IEEE I CONF COMP VIS, P2214, DOI 10.1109/ICCV.2011.6126499
   Klein DA, 2010, IEEE INT C INT ROBOT, P772, DOI 10.1109/IROS.2010.5650583
   Ko BC, 2006, INT C PATT RECOG, P45
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Koehler K, 2014, J VISION, V14, DOI 10.1167/14.3.14
   Kolmogorov V., 2006, P IEEE CVPR, V1, P993, DOI DOI 10.1109/CVPR.2006.91
   Kruthiventi SSS, 2016, PROC CVPR IEEE, P5781, DOI 10.1109/CVPR.2016.623
   Kuen J, 2016, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2016.399
   Lang CY, 2012, LECT NOTES COMPUT SC, V7573, P101, DOI 10.1007/978-3-642-33709-3_8
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562
   Lee GY, 2016, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2016.78
   Lee YJ, 2012, PROC CVPR IEEE, P1346, DOI 10.1109/CVPR.2012.6247820
   Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96
   Li A, 2013, P SPIE
   Li GB, 2017, PROC CVPR IEEE, P247, DOI 10.1109/CVPR.2017.34
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li HL, 2011, IEEE T IMAGE PROCESS, V20, P3365, DOI 10.1109/TIP.2011.2156803
   Li HY, 2017, NEUROCOMPUTING, V226, P212, DOI 10.1016/j.neucom.2016.11.056
   Li J, 2013, IEEE SIGNAL PROC LET, V20, P845, DOI 10.1109/LSP.2013.2268868
   Li J, 2009, IEEE INT CON MULTI, P442, DOI 10.1109/ICME.2009.5202529
   Li J, 2013, IEEE T PATTERN ANAL, V35, P996, DOI 10.1109/TPAMI.2012.147
   Li L, 2013, IEEE MULTIMEDIA, V20, P13, DOI 10.1109/MMUL.2013.15
   Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359
   Li Q, 2011, INT C COMP AID IND D, P650, DOI 10.1109/ICPIM.2011.5983658
   Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306
   Li X, 2013, IEEE I CONF COMP VIS, P3328, DOI 10.1109/ICCV.2013.413
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Li Y, 2013, IEEE T CIRC SYST VID, V23, P2067, DOI 10.1109/TCSVT.2013.2270367
   Liu F, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1477, DOI 10.1109/ICME.2006.262821
   Liu H., 2007, ACM MULTIMEDIA, P305
   Liu HT, 2009, IEEE IMAGE PROC, P3097, DOI 10.1109/ICIP.2009.5414466
   Liu H, 2012, VISUAL COMPUT, V28, P279, DOI 10.1007/s00371-011-0638-z
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu RS, 2014, PROC CVPR IEEE, P3866, DOI 10.1109/CVPR.2014.494
   Liu Tie, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383047
   Liu TI, 2008, C IND ELECT APPL, P1, DOI 10.1109/ICIEA.2008.4582469
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu S, 2014, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2014.357
   Lu Y, 2011, IEEE I CONF COMP VIS, P233, DOI 10.1109/ICCV.2011.6126247
   Ma Y.F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   Ma YF, 2005, IEEE T MULTIMEDIA, V7, P907, DOI 10.1109/TMM.2005.854410
   Mai L, 2013, PROC CVPR IEEE, P1131, DOI 10.1109/CVPR.2013.150
   Marchesotti L, 2009, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2009.5459467
   Margolin R, 2013, PROC CVPR IEEE, P1139, DOI 10.1109/CVPR.2013.151
   Margolin R, 2013, VISUAL COMPUT, V29, P381, DOI 10.1007/s00371-012-0740-x
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Meger D, 2008, ROBOT AUTON SYST, V56, P503, DOI 10.1016/j.robot.2008.03.008
   Mishra AK, 2012, IEEE T PATTERN ANAL, V34, P639, DOI 10.1109/TPAMI.2011.171
   Moosmann F, 2006, P INT WORKSH REPR US
   Mukherjee Lopamudra, 2011, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit, P1881, DOI 10.1109/CVPR.2011.5995420
   Ninassi A, 2007, IEEE IMAGE PROC, P733
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Parkhurst D, 2002, VISION RES, V42, P107, DOI 10.1016/S0042-6989(01)00250-4
   Peng LK, 2013, PROCEEDINGS 2013 INTERNATIONAL CONFERENCE ON MECHATRONIC SCIENCES, ELECTRIC ENGINEERING AND COMPUTER (MEC), P796, DOI 10.1109/MEC.2013.6885168
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Qin CC, 2014, NEUROCOMPUTING, V129, P378, DOI 10.1016/j.neucom.2013.09.021
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606
   Rahtu E, 2010, LECT NOTES COMPUT SC, V6315, P366, DOI 10.1007/978-3-642-15555-0_27
   Ren ZX, 2014, IEEE T CIRC SYST VID, V24, P769, DOI 10.1109/TCSVT.2013.2280096
   Rosenholtz R, 2007, J VISION, V7, DOI 10.1167/7.2.17
   Rosin PL, 2009, PATTERN RECOGN, V42, P2363, DOI 10.1016/j.patcog.2009.04.021
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Rutishauser U, 2004, PROC CVPR IEEE, P37
   Scharfenberger C, 2013, PROC CVPR IEEE, P979, DOI 10.1109/CVPR.2013.131
   Shen H, 2013, CHINESE J AERONAUT, V26, P1211, DOI 10.1016/j.cja.2013.07.038
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Shi KY, 2013, PROC CVPR IEEE, P2115, DOI 10.1109/CVPR.2013.275
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Siva P, 2013, PROC CVPR IEEE, P3238, DOI 10.1109/CVPR.2013.416
   Spain M, 2011, INT J COMPUT VISION, V91, P59, DOI 10.1007/s11263-010-0376-0
   Stalder Severin, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P43, DOI 10.1007/978-3-642-37431-9_4
   Sugano Y, 2010, PROC CVPR IEEE, P2667, DOI 10.1109/CVPR.2010.5539984
   Sun JD, 2013, IEEE T BROADCAST, V59, P602, DOI 10.1109/TBC.2013.2272172
   't Hart BM, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00455
   Tang YB, 2016, LECT NOTES COMPUT SC, V9912, P809, DOI 10.1007/978-3-319-46484-8_49
   Tatler BW, 2007, J VISION, V7, DOI 10.1167/7.14.4
   Tian YH, 2015, INT J COMPUT VISION, V111, P153, DOI 10.1007/s11263-014-0737-1
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Tu ZW, 2010, IEEE T PATTERN ANAL, V32, P1744, DOI 10.1109/TPAMI.2009.186
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Valenti R, 2009, IEEE I CONF COMP VIS, P2185, DOI 10.1109/ICCV.2009.5459240
   Vidal R, 2005, IEEE T PATTERN ANAL, V27, P1945, DOI 10.1109/TPAMI.2005.244
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Vogel J, 2004, LECT NOTES COMPUT SC, V3175, P195
   Walther D, 2006, NEURAL NETWORKS, V19, P1395, DOI 10.1016/j.neunet.2006.10.001
   Wang L, 2011, IEEE I CONF COMP VIS, P105, DOI 10.1109/ICCV.2011.6126231
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang M, 2011, PROC CVPR IEEE, P417, DOI 10.1109/CVPR.2011.5995743
   Wang P, 2012, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2012.6248054
   Wang Q, 2013, PATTERN RECOGN LETT, V34, P34, DOI 10.1016/j.patrec.2012.06.002
   Wang X, 2016, IEEE IMAGE PROC, P1042
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, INT CONF ACOUST SPEE, P3313
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   WOLFE JM, 1989, J EXP PSYCHOL HUMAN, V15, P419, DOI 10.1037/0096-1523.15.3.419
   Wu Y, 2011, CHINESE SCI BULL, V56, P1055, DOI 10.1007/s11434-010-4387-1
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xie YL, 2013, IEEE T IMAGE PROCESS, V22, P1689, DOI 10.1109/TIP.2012.2216276
   Xu J, 2014, J VISION, V14, DOI 10.1167/14.1.28
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang C, 2013, IEEE SIGNAL PROC LET, V20, P637, DOI 10.1109/LSP.2013.2260737
   Yiqun Hu, 2005, 13th Annual ACM International Conference on Multimedia, P716
   Yu H., 2010, PROC ANN ACM INT C M, P891
   Yu ZW, 2007, IEEE T MULTIMEDIA, V9, P766, DOI 10.1109/TMM.2007.893351
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zhang G, 2010, LECT NOTES COMPUT SC, V5995, P193
   Zhang GX, 2009, COMPUT GRAPH FORUM, V28, P1897, DOI 10.1111/j.1467-8659.2009.01568.x
   Zhang J, 2017, P IEEE WINTER C APPL, V1, P10
   Zhang JM, 2013, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2013.26
   Zhang J, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3107956
   Zhang W, 2016, IEEE T NEUR NET LEAR, V27, P1266, DOI 10.1109/TNNLS.2015.2461603
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhu JY, 2012, PROC CVPR IEEE, P3218, DOI 10.1109/CVPR.2012.6248057
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
   Zou WB, 2015, IEEE I CONF COMP VIS, P406, DOI 10.1109/ICCV.2015.54
   Zou WB, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.78
NR 222
TC 281
Z9 305
U1 32
U2 299
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2019
VL 5
IS 2
BP 117
EP 150
DI 10.1007/s41095-019-0149-9
PG 34
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UM
UT WOS:000648690800001
OA Green Submitted, gold
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Zhou, Q
   Li, MY
   Zeng, Q
   Aristidou, A
   Zhang, XJ
   Chen, L
   Tu, CH
AF Zhou, Qiu
   Li, Manyi
   Zeng, Qiong
   Aristidou, Andreas
   Zhang, Xiaojing
   Chen, Lin
   Tu, Changhe
TI Let's all dance: Enhancing amateur dance motions
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE animation; music-to-motion alignment; dance motion enhancement; dance
   motion analysis
ID COMPUTER-GRAPHICS; STYLE; MUSIC
AB Professional dance is characterized by high impulsiveness, elegance, and aesthetic beauty. In order to reach the desired professionalism, it requires years of long and exhausting practice, good physical condition, musicality, but also, a good understanding of choreography. Capturing dance motions and transferring them to digital avatars is commonly used in the film and entertainment industries. However, so far, access to high-quality dance data is very limited, mainly due to the many practical difficulties in capturing the movements of dancers, making it prohibitive for large-scale data acquisition. In this paper, we present a model that enhances the professionalism of amateur dance movements, allowing movement quality to be improved in both spatial and temporal domains. Our model consists of a dance-to-music alignment stage responsible for learning the optimal temporal alignment path between dance and music, and a dance-enhancement stage that injects features of professionalism in both spatial and temporal domains. To learn a homogeneous distribution and credible mapping between the heterogeneous professional and amateur datasets, we generate amateur data from professional dances taken from the AIST++ dataset. We demonstrate the effectiveness of our method by comparing it with two baseline motion transfer methods via thorough qualitative visual controls, quantitative metrics, and a perceptual study. We also provide temporal and spatial module analysis to examine the mechanisms and necessity of key components of our framework.
C1 [Zhou, Qiu; Zeng, Qiong; Zhang, Xiaojing; Tu, Changhe] Shandong Univ, Sch Comp Sci & Technol, Qingdao 266000, Peoples R China.
   [Li, Manyi] Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
   [Aristidou, Andreas] Univ Cyprus, Dept Comp Sci, CY-1678 Nicosia, Cyprus.
   [Aristidou, Andreas] CYENS Ctr Excellence, CY-1016 Nicosia, Cyprus.
   [Chen, Lin] Shandong Univ, Qingdao Inst Humanities & Social Sci, Qingdao 266000, Peoples R China.
C3 Shandong University; Shandong University; University of Cyprus; Shandong
   University
RP Zeng, Q; Tu, CH (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Qingdao 266000, Peoples R China.
EM zhouqiulv@gmail.com; manyili@sdu.edu.cn; qiong.zn@sdu.edu.cn;
   a.aristidou@ieee.org; xj.zhang@mail.sdu.edu.cn;
   chenlin.spring@gmail.com; chtu@sdu.edu.cn
RI Zhang, Xiao/ABC-8933-2021
OI Zhang, Xiao/0000-0001-9445-0026
FU National Natural Science Foundation of China [62072284]; Natural Science
   Foundation of Shandong Province [ZR2021MF102]; Special Project of
   Shandong Province for Software Engineering [11480004042015]; University
   of Cyprus
FX AcknowledgementsThis research was supported by National Natural Science
   Foundation of China (Grant No. 62072284), Natural Science Foundation of
   Shandong Province (Grant No. ZR2021MF102), a Special Project of Shandong
   Province for Software Engineering (Grant No. 11480004042015), and
   internal funds from the University of Cyprus. The authors would like to
   thank Anastasios Yiannakidis (University of Cyprus) for capturing the
   amateur dances, and the volunteers for participating in the perceptual
   studies. The authors would also like to thank the anonymous reviewers
   and editors for their fruitful comments and suggestions.
CR Aberman K, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392469
   Andreou N., 2022, P ACM SIGGRAPH EUR S
   [Anonymous], 2011, P 19 ACM INT C MULTI, DOI DOI 10.1145/2072298.2072412
   Arikan O, 2002, ACM T GRAPHIC, V21, P483, DOI 10.1145/566570.566606
   Aristidou A, 2018, COMPUT GRAPH FORUM, V37, P35, DOI 10.1111/cgf.13310
   Aristidou A, 2023, IEEE T VIS COMPUT GR, V29, P3519, DOI 10.1109/TVCG.2022.3163676
   Aristidou A, 2019, ACM J COMPUT CULT HE, V12, DOI 10.1145/3344383
   Aristidou A, 2017, ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION (SCA 2017), DOI 10.1145/3099564.3099566
   Aristidou A, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275038
   Aristidou A, 2015, ACM J COMPUT CULT HE, V8, DOI 10.1145/2755566
   Bellini Rachele, 2018, Computational Visual Media, V4, P197, DOI 10.1007/s41095-018-0115-y
   Brand M, 2000, COMP GRAPH, P183, DOI 10.1145/344779.344865
   Butterworth J., 2011, DANCE STUDIES BASICS
   Cardle M, 2002, 20TH EUROGRAPHICS UK CONFERENCE, PROCEEDINGS, P38, DOI 10.1109/EGUK.2002.1011270
   Chan JCP, 2011, IEEE T LEARN TECHNOL, V4, P187, DOI 10.1109/TLT.2010.27
   Chen HY, 2023, RES DANC EDUC, V24, P342, DOI 10.1080/14647893.2021.1980524
   Chen K, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459932
   Chung JS, 2017, LECT NOTES COMPUT SC, V10117, P251, DOI 10.1007/978-3-319-54427-4_19
   DAUGMAN JG, 1985, J OPT SOC AM A, V2, P1160, DOI 10.1364/JOSAA.2.001160
   Davis A, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201371
   Dong Y. Z., 2020, P 13 ACM SIGGRAPH C
   DOWSON DC, 1982, J MULTIVARIATE ANAL, V12, P450, DOI 10.1016/0047-259X(82)90077-X
   Du Han., 2019, Eurographics 2019 - Short Papers, DOI DOI 10.2312/EGS.20191002
   El Raheb K, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3323335
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Halperin T, 2019, INT CONF ACOUST SPEE, P3980, DOI 10.1109/ICASSP.2019.8682863
   Hanna Judith-Lynn., 1983, PERFORMER AUDIENCE C
   Holden D., 2015, P SIGGRAPH AS TECH B, DOI DOI 10.1145/2820903.2820918
   Holden D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073663
   Holden D, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925975
   Hsu E, 2005, ACM T GRAPHIC, V24, P1082, DOI 10.1145/1073204.1073315
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Kim TH, 2003, ACM T GRAPHIC, V22, P392, DOI 10.1145/882262.882283
   Koutedakis Y., 1999, FIT HLTH DANCER
   Krasnow Donna, 2009, J Dance Med Sci, V13, P101
   Laban Rudolf., 2011, MASTERY MOVEMENT
   Laichuthai Auttawut, 2011, 8th Electrical Engineering/ Electronics, Computer, Telecommunications and Information Technology (ECTI) Association of Thailand - Conference 2011, P496
   Lee H. Y., 2019, P 33 C NEUR INF PROC
   Lee HC, 2005, COMPUT GRAPH FORUM, V24, P353, DOI 10.1111/j.1467-8659.2005.00860.x
   Li RL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13381, DOI 10.1109/ICCV48922.2021.01315
   Mason I, 2018, COMPUT GRAPH FORUM, V37, P143, DOI 10.1111/cgf.13555
   McFee B., 2015, P 14 PYTH SCI C, P18, DOI [DOI 10.25080/MAJORA-7B98E3ED-003, 10.25080/Majora-7b98e3ed-003]
   Neave N, 2011, BIOL LETTERS, V7, P221, DOI 10.1098/rsbl.2010.0619
   Phillips G. M., 2003, INTERPOLATION APPROX, V14, DOI DOI 10.1007/B97417
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   SAKOE H, 1978, IEEE T ACOUST SPEECH, V26, P43, DOI 10.1109/TASSP.1978.1163055
   Shiratori T, 2006, COMPUT GRAPH FORUM, V25, P449, DOI 10.1111/j.1467-8659.2006.00964.x
   Smith HJ, 2019, P ACM COMPUT GRAPH, V2, DOI 10.1145/3340254
   Tadamura K, 1998, IEEE MULTIMEDIA, V5, P63, DOI 10.1109/93.735869
   Tang TR, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1598, DOI 10.1145/3240508.3240526
   Tenenbaum JB, 1997, ADV NEUR IN, V9, P662
   Torrents C, 2013, PERCEPTION, V42, P447, DOI 10.1068/p7117
   Tsuchida Shuhei, 2019, P 20 INT SOC MUSIC I, P501, DOI 10 . 5281 / zenodo . 3527854
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang JR, 2020, IEEE WINT CONF APPL, P3298, DOI 10.1109/WACV45572.2020.9093345
   Wen YH, 2021, PROC CVPR IEEE, P13607, DOI 10.1109/CVPR46437.2021.01340
   Xia SH, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766999
   Zhou Y, 2019, PROC CVPR IEEE, P5738, DOI 10.1109/CVPR.2019.00589
   Zhuang WL, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3485664
   박양선, 2016, [Korean Journal of Sport Biomechanics, 한국운동역학회지], V26, P1
NR 60
TC 2
Z9 2
U1 2
U2 6
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2023
VL 9
IS 3
BP 531
EP 550
DI 10.1007/s41095-022-0292-6
EA MAR 2023
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA G9DP6
UT WOS:000961656200002
OA gold
DA 2024-07-18
ER

PT J
AU Chen, YL
   Kwan, KC
   Fu, HB
AF Chen, Yilan
   Kwan, Kin Chung
   Fu, Hongbo
TI Autocompletion of repetitive stroking with image guidance
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE interaction; autocompletion; digital drawing; prediction; texture
   synthesis
AB Image-guided drawing can compensate for a lack of skill but often requires a significant number of repetitive strokes to create textures. Existing automatic stroke synthesis methods are usually limited to predefined styles or require indirect manipulation that may break the spontaneous flow of drawing. We present an assisted drawing system to autocomplete repetitive short strokes during a user's normal drawing process. Users draw over a reference image as usual; at the same time, our system silently analyzes the input strokes and the reference to infer strokes that follow the user's input style when certain repetition is detected. Users can accept, modify, or ignore the system's predictions and continue drawing, thus maintaining fluid control over drawing. Our key idea is to jointly analyze image regions and user input history to detect and predict repetition. The proposed system can effectively reduce the user's workload when drawing repetitive short strokes, helping users to create results with rich patterns.
C1 [Chen, Yilan; Fu, Hongbo] City Univ Hong Kong, Sch Creat Media, Tat Chee Ave, Hong Kong, Peoples R China.
   [Kwan, Kin Chung] Univ Konstanz, Univ St 10, D-78464 Constance, Germany.
C3 City University of Hong Kong; University of Konstanz
RP Fu, HB (corresponding author), City Univ Hong Kong, Sch Creat Media, Tat Chee Ave, Hong Kong, Peoples R China.
EM yil.ellen.chan@gmail.com; kckwan@ieee.org; hongbofu@cityu.edu.hk
OI CHEN, Yilan/0000-0003-3926-1809
FU Adobe Research; Deutsche Forschungsgemeinschaft [251654672-TRR 161]
FX We are grateful to Li-Yi Wei for his insightful comments and
   suggestions. We also thank the anonymous reviewers for feedback, and
   funding from Adobe Research and the Deutsche Forschungsgemeinschaft,
   Project-ID 251654672-TRR 161.
CR Adobe, 2017, PAINT STYL STROK ART
   Alves dos Passos Vladimir, 2010, 2010 Pacific Graphics (PG). Proceedings 18th Pacific Conference on Computer Graphics and Applications, P109, DOI 10.1109/PacificGraphics.2010.22
   [Anonymous], 2015, P WORKSHOP SKETCH BA
   [Anonymous], 2014, Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology. UIST'14, DOI 10.1145/2642918.2647415
   Barla P., 2006, INRIA RES REPORT
   Chen X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7325, DOI 10.1109/ICCV48922.2021.00725
   Dunn Alphonso, 2015, Pen and Ink Drawing: A Simple Guide
   Fiser J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925948
   Gatys LA, 2017, PROC CVPR IEEE, P3730, DOI 10.1109/CVPR.2017.397
   Gerl M, 2013, COMPUT GRAPH-UK, V37, P65, DOI 10.1016/j.cag.2012.11.003
   Haeberli P., 1990, Computer Graphics, V24, P207, DOI 10.1145/97880.97902
   HART S G, 1988, P139
   Hegde S, 2013, COMPUT ANIMAT VIRT W, V24, P43, DOI 10.1002/cav.1435
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Hiller S, 2003, COMPUT GRAPH FORUM, V22, P515, DOI 10.1111/1467-8659.00699
   Hsu CY, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376248
   Iarussi E., 2013, P 26 ANN ACM S USER, DOI [DOI 10.1145/2501988.2501997, 10.1145/2501988.2501997]
   Ijiri T, 2008, COMPUT GRAPH FORUM, V27, P429, DOI 10.1111/j.1467-8659.2008.01140.x
   Kalogerakis E, 2012, ACM T GRAPHIC, V31, DOI [10.1145/2077341.2077342, 10.1145/2185520.2185551]
   Kang HW, 2005, VISUAL COMPUT, V21, P821, DOI 10.1007/s00371-005-0328-9
   Kaspar A, 2015, COMPUT GRAPH FORUM, V34, P349, DOI 10.1111/cgf.12565
   Kazi RubaiatHabib., 2012, Proceedings of the ACM Conference on Human Factors in Computing Systems, P1727
   Kyprianidis JE, 2013, IEEE T VIS COMPUT GR, V19, P866, DOI 10.1109/TVCG.2012.160
   Kyprianidis JE, 2011, COMPUT GRAPH FORUM, V30, P593, DOI 10.1111/j.1467-8659.2011.01882.x
   Li GB, 2017, IEEE COMPUT GRAPH, V37, P70, DOI 10.1109/MCG.2016.37
   Ma CY, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461921
   Ma CY, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964957
   Martín D, 2017, COMPUT GRAPH-UK, V67, P24, DOI 10.1016/j.cag.2017.05.001
   Matsui Y, 2017, IEEE T VIS COMPUT GR, V23, P1852, DOI 10.1109/TVCG.2016.2554113
   Mortensen E. N., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P191, DOI 10.1145/218380.218442
   Nancel M, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P1777, DOI 10.1145/2556288.2556990
   Peng MQ, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201297
   Peng Mengqi, 2020, P 33 ANN ACM S USER
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Salisbury M. P., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P401, DOI 10.1145/258734.258890
   Su QK, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601202
   Suzuki R., 2018, P PACIFIC C COMPUTER, P29
   Tsai Huichi, 2017, [Computational Visual Media, 计算可视媒体], V3, P177
   Tu PH, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417780
   Williford B, 2019, PROCEEDINGS OF THE 2019 ON CREATIVITY AND COGNITION - C&C 19, P198, DOI [10.1145/3325480.3325507, 10.1145/3059454.3078695]
   Xie J., 2014, P 27 ANN ACM S USER, P407
   Xing J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818079
   Xing J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661247
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao M., 2011, Proc. NPAR '11, P137, DOI DOI 10.1145/2024676.2024698
   Zitnick CL, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461985
NR 46
TC 0
Z9 0
U1 0
U2 4
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2023
VL 9
IS 3
BP 581
EP 596
DI 10.1007/s41095-022-0288-2
EA MAR 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA G9DP6
UT WOS:000946186100002
OA gold
DA 2024-07-18
ER

PT J
AU Darzi, A
   Lang, IT
   Taklikar, A
   Averbuch-Elor, H
   Avidan, S
AF Darzi, Anna
   Lang, Itai
   Taklikar, Ashutosh
   Averbuch-Elor, Hadar
   Avidan, Shai
TI Co-occurrence based texture synthesis
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE co-occurrence; texture synthesis; deep learning; generative adversarial
   networks (GANs)
ID IMAGE; STATISTICS; COMPLEX
AB As image generation techniques mature, there is a growing interest in explainable representations that are easy to understand and intuitive to manipulate. In this work, we turn to co-occurrence statistics, which have long been used for texture analysis, to learn a controllable texture synthesis model. We propose a fully convolutional generative adversarial network, conditioned locally on co-occurrence statistics, to generate arbitrarily large images while having local, interpretable control over texture appearance. To encourage fidelity to the input condition, we introduce a novel differentiable co-occurrence loss that is integrated seamlessly into our framework in an end-to-end fashion. We demonstrate that our solution offers a stable, intuitive, and interpretable latent representation for texture synthesis, which can be used to generate smooth texture morphs between different textures. We further show an interactive texture tool that allows a user to adjust local characteristics of the synthesized texture by directly using the co-occurrence values.
C1 [Darzi, Anna; Lang, Itai; Taklikar, Ashutosh; Avidan, Shai] Tel Aviv Univ, IL-6997801 Tel Aviv, Israel.
   [Averbuch-Elor, Hadar] Cornell Univ, Cornell Tech, New York, NY 10044 USA.
C3 Tel Aviv University; Cornell University
RP Averbuch-Elor, H (corresponding author), Cornell Univ, Cornell Tech, New York, NY 10044 USA.
EM annadarzi@mail.tau.ac.il; itailang@mail.tau.ac.il;
   ashutosht@mail.tau.ac.il; hadarelor@cornell.edu; avidan@eng.tau.ac.il
RI Averbuch-Elor, Hadar/AAO-4246-2021
CR [Anonymous], 2018, ACM T GRAPHIC, V37
   [Anonymous], 2017, ACM T GRAPHIC, V36
   [Anonymous], 2016, ACM T GRAPHIC, V35
   [Anonymous], 2019, ACM T GRAPHIC, V38
   [Anonymous], 2012, ACM T GRAPHIC, V31
   Barnes Connelly, 2017, [Computational Visual Media, 计算可视媒体], V3, P3
   Bau D., 2019, ICLR
   Bergmann U., 2017, ARXIV PREPRINT ARXIV
   Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598
   Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461
   De Benet J. S., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P361, DOI 10.1145/258734.258882
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Heeger DJ, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC648
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola P, 2014, LECT NOTES COMPUT SC, V8691, P799, DOI 10.1007/978-3-319-10578-9_52
   Jetchev N., 2016, ARXIV PREPRINT ARXIV
   Jevnisek RJ, 2017, PROC CVPR IEEE, P3816, DOI 10.1109/CVPR.2017.406
   Julesz B., 1962, IEEE T INFORM THEORY, V8, P84, DOI DOI 10.1109/TIT.1962.1057698
   Kaspar A, 2015, COMPUT GRAPH FORUM, V34, P349, DOI 10.1111/cgf.12565
   Kat R, 2018, PROC CVPR IEEE, P1751, DOI 10.1109/CVPR.2018.00188
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43
   Li YJ, 2017, PROC CVPR IEEE, P266, DOI 10.1109/CVPR.2017.36
   Liu G, 2016, INT C PATT RECOG, P3234, DOI 10.1109/ICPR.2016.7900133
   Matusik W, 2005, ACM T GRAPHIC, V24, P787, DOI 10.1145/1073204.1073262
   Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983
   Rabin J, 2012, LECT NOTES COMPUT SC, V6667, P435, DOI 10.1007/978-3-642-24785-9_37
   Radford A., 2015, ARXIV151106434
   Shen Yujun, 2019, Interpreting the Latent Space of GANs for Semantic Face Editing Original Pose Age Gender Eyeglasses
   Ulyanov D, 2016, PR MACH LEARN RES, V48
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
   Wu FZ, 2016, VISUAL COMPUT, V32, P43, DOI 10.1007/s00371-014-1054-y
   YELLOTT JI, 1993, J OPT SOC AM A, V10, P777, DOI 10.1364/JOSAA.10.000777
   Yu N, 2019, PROC CVPR IEEE, P12156, DOI 10.1109/CVPR.2019.01244
NR 37
TC 2
Z9 3
U1 0
U2 10
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2022
VL 8
IS 2
BP 289
EP 302
DI 10.1007/s41095-021-0243-7
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XJ0YS
UT WOS:000726525400008
OA gold
DA 2024-07-18
ER

PT J
AU Gong, LX
   Zhang, YQ
   Zhang, YK
   Yang, Y
   Xu, WW
AF Gong, Lixue
   Zhang, Yiqun
   Zhang, Yunke
   Yang, Yin
   Xu, Weiwei
TI Erroneous pixel prediction for semantic image segmentation
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE erroneous pixel prediction; image segmentation; deep learning
AB We consider semantic image segmentation. Our method is inspired by Bayesian deep learning which improves image segmentation accuracy by modeling the uncertainty of the network output. In contrast to uncertainty, our method directly learns to predict the erroneous pixels of a segmentation network, which is modeled as a binary classification problem. It can speed up training comparing to the Monte Carlo integration often used in Bayesian deep learning. It also allows us to train a branch to correct the labels of erroneous pixels. Our method consists of three stages: (i) predict pixel-wise error probability of the initial result, (ii) redetermine new labels for pixels with high error probability, and (iii) fuse the initial result and the redetermined result with respect to the error probability. We formulate the error-pixel prediction problem as a classification task and employ an error-prediction branch in the network to predict pixel-wise error probabilities. We also introduce a detail branch to focus the training process on the erroneous pixels. We have experimentally validated our method on the Cityscapes and A DE2OK datasets. Our model can be easily added to various advanced segmentation networks to improve their performance. Taking DeepLabv3+ as an example, our network can achieve 82.88% of mIoU on Cityscapes testing dataset and 45.73% on ADE2OK validation dataset, improving corresponding DeepLabv3+ results by 0.74% and 0.13% respectively.
C1 [Gong, Lixue; Zhang, Yiqun; Zhang, Yunke; Xu, Weiwei] Zhejiang Univ, State Key Lab CADS CG, Hangzhou 310058, Peoples R China.
   [Yang, Yin] Clemson Univ, Sch Comp, Clemson, SC 29634 USA.
C3 Zhejiang University; Clemson University
RP Xu, WW (corresponding author), Zhejiang Univ, State Key Lab CADS CG, Hangzhou 310058, Peoples R China.
EM gonglx@zju.edu.cn; zyqlouise@zju.edu.cn; yunkezhang@zju.edu.cn;
   yin5@clemson.edu; xww@cad.zju.edu.cn
RI Zhang, YiQun/HCI-2427-2022
OI Yang, Yin/0000-0001-7645-5931
FU National Natural Science Foundation of China [61732016]
FX We would like to thank the anonymous reviewers for their constructive
   comments. Weiwei Xu is partially supported by the National Natural
   Science Foundation of China (No. 61732016).
CR [Anonymous], 2014, INT C LEARN REPR
   [Anonymous], CoRR abs/1511.07122
   [Anonymous], PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2018.00745, DOI 10.1109/TPAMI.2019.2913372]
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Chen LC, 2018, ADV NEUR IN, V31
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Ding HH, 2018, PROC CVPR IEEE, P2393, DOI 10.1109/CVPR.2018.00254
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Ghiasi G, 2016, LECT NOTES COMPUT SC, V9907, P519, DOI 10.1007/978-3-319-46487-9_32
   Guo YM, 2018, INT J MULTIMED INF R, V7, P87, DOI 10.1007/s13735-017-0141-z
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Kendall A, 2017, 31 ANN C NEURAL INFO, V30
   King DB, 2015, ACS SYM SER, V1214, P1
   Li H., 2018, P BRIT MACH VIS C
   Li HX, 2015, PROC CVPR IEEE, P5325, DOI 10.1109/CVPR.2015.7299170
   Li XX, 2017, PROC CVPR IEEE, P6459, DOI 10.1109/CVPR.2017.684
   Lienhart R, 2002, IEEE IMAGE PROC, P900
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Liu CX, 2019, PROC CVPR IEEE, P82, DOI 10.1109/CVPR.2019.00017
   Liu W., 2015, ARXIV150604579
   Long J., 2015, P IEEE C COMP VIS PA, P3431
   Nekrasov V, 2019, PROC CVPR IEEE, P9118, DOI 10.1109/CVPR.2019.00934
   Pang JH, 2017, IEEE INT CONF COMP V, P878, DOI 10.1109/ICCVW.2017.108
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Tian Z, 2019, PROC CVPR IEEE, P3121, DOI 10.1109/CVPR.2019.00324
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1
   Yu H., 2018, 2018 International Joint Conference on Neural Networks (IJCNN)
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou BL, 2019, INT J COMPUT VISION, V127, P302, DOI 10.1007/s11263-018-1140-0
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
NR 38
TC 5
Z9 5
U1 0
U2 14
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2022
VL 8
IS 1
BP 165
EP 175
DI 10.1007/s41095-021-0235-7
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WO6VR
UT WOS:000712590200011
OA gold
DA 2024-07-18
ER

PT J
AU Ju, YK
   Peng, YX
   Jian, MW
   Gao, F
   Dong, JY
AF Ju, Yakun
   Peng, Yuxin
   Jian, Muwei
   Gao, Feng
   Dong, Junyu
TI Learning conditional photometric stereo with high-resolution features
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE photometric stereo; normal estimation; deep neural networks; 3D
   reconstruction
AB Photometric stereo aims to reconstruct 3D geometry by recovering the dense surface orientation of a 3D object from multiple images under differing illumination. Traditional methods normally adopt simplified reflectance models to make the surface orientation computable. However, the real reflectances of surfaces greatly limit applicability of such methods to real-world objects. While deep neural networks have been employed to handle non-Lambertian surfaces, these methods are subject to blurring and errors, especially in high-frequency regions (such as crinkles and edges), caused by spectral bias: neural networks favor low-frequency representations so exhibit a bias towards smooth functions. In this paper, therefore, we propose a self-learning conditional network with multi-scale features for photometric stereo, avoiding blurred reconstruction in such regions. Our explorations include: (i) a multi-scale feature fusion architecture, which keeps high-resolution representations and deep feature extraction, simultaneously, and (ii) an improved gradient-motivated conditionally parameterized convolution (GM-CondConv) in our photometric stereo network, with different combinations of convolution kernels for varying surfaces. Extensive experiments on public benchmark datasets show that our calibrated photometric stereo method outperforms the state-of-the-art.
C1 [Ju, Yakun; Gao, Feng; Dong, Junyu] Ocean Univ China, Dept Comp Sci & Technol, Qingdao 266100, Peoples R China.
   [Peng, Yuxin] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
   [Jian, Muwei] Shandong Univ Finance & Econ, Sch Comp Sci & Technol, Jinan 250002, Peoples R China.
C3 Ocean University of China; Peking University; Shandong University of
   Finance & Economics
RP Dong, JY (corresponding author), Ocean Univ China, Dept Comp Sci & Technol, Qingdao 266100, Peoples R China.
EM juyakun@stu.ouc.edu.cn; pengyuxin@pku.edu.cn; jianmuweihk@163.com;
   gaofeng@ouc.edu.cn; dongjunyu@ouc.edu.cn
RI Ju, Yakun/JEP-0636-2023; Jian, Muwei/Q-8319-2018
OI Ju, Yakun/0000-0003-4065-4108; Jian, Muwei/0000-0002-4249-2264
FU National Key Scientific Instrument and Equipment Development Projects of
   China [41927805]; National Natural Science Foundation of China
   [61501417, 61976123]; Key Development Program for Basic Research of
   Shandong Province [ZR2020ZD44]; Taishan Young Scholars Program of
   Shandong Province
FX This work was supported by the National Key Scientific Instrument and
   Equipment Development Projects of China (41927805), the National Natural
   Science Foundation of China (61501417, 61976123), the Key Development
   Program for Basic Research of Shandong Province (ZR2020ZD44), and the
   Taishan Young Scholars Program of Shandong Province.
CR Alldrin NG, 2007, PROC CVPR IEEE, P1822
   [Anonymous], 2010, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2009.102
   [Anonymous], 2008, P IEEE C COMP VIS PA
   Chen GY, 2019, PROC CVPR IEEE, P8731, DOI 10.1109/CVPR.2019.00894
   Chen GY, 2018, LECT NOTES COMPUT SC, V11213, P3, DOI 10.1007/978-3-030-01240-3_1
   Chen T., 2006, 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Volume 2 (CVPR'06), P1825
   Chung HS, 2008, PROC CVPR IEEE, P3337
   Einarsson P., 2006, Rendering Techniques, P183
   Georghiades AS, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P816
   Herbort S, 2011, 3D RES, V2, DOI 10.1007/3DRes.03(2011)4
   Higo T, 2010, PROC CVPR IEEE, P1157, DOI 10.1109/CVPR.2010.5540084
   Ikehata S, 2018, LECT NOTES COMPUT SC, V11219, P3, DOI 10.1007/978-3-030-01267-0_1
   Ikehata S, 2014, PROC CVPR IEEE, P2187, DOI 10.1109/CVPR.2014.280
   Ikehata S, 2012, PROC CVPR IEEE, P318, DOI 10.1109/CVPR.2012.6247691
   Jakob Wenzel, 2010, Mitsuba renderer
   Jian MW, 2020, IEEE T MULTIMEDIA, V22, P970, DOI 10.1109/TMM.2019.2937187
   Johnson M. K., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2553, DOI 10.1109/CVPR.2011.5995510
   Ju YK, 2020, IEEE I C VI COM I PR, P411, DOI 10.1109/vcip49819.2020.9301860
   Ju YK, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P694
   Ju YK, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107162
   Khanian Maryam, 2018, Computational Visual Media, V4, P83, DOI 10.1007/s41095-017-0101-9
   Kingma D. P., 2014, arXiv
   Li JX, 2019, PROC CVPR IEEE, P7560, DOI 10.1109/CVPR.2019.00775
   Liu L., 2020, LECT NOTES COMPUTER, V12358, P86, DOI [10.1007/978-3-030-58601-0_6, DOI 10.1007/978-3-030-58601-0_6]
   Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343
   Mildenhall Ben, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P405, DOI 10.1007/978-3-030-58452-8_24
   Papadhimitri T, 2014, INT J COMPUT VISION, V107, P139, DOI 10.1007/s11263-013-0665-5
   Paszke A, 2019, ADV NEUR IN, V32
   Rahaman N, 2019, PR MACH LEARN RES, V97
   Santo H, 2017, IEEE INT CONF COMP V, P501, DOI 10.1109/ICCVW.2017.66
   Shi BX, 2019, IEEE T PATTERN ANAL, V41, P271, DOI 10.1109/TPAMI.2018.2799222
   Shi BX, 2014, IEEE T PATTERN ANAL, V36, P1078, DOI 10.1109/TPAMI.2013.196
   Shi BX, 2010, PROC CVPR IEEE, P1118, DOI 10.1109/CVPR.2010.5540091
   SIMCHONY T, 1990, IEEE T PATTERN ANAL, V12, P435, DOI 10.1109/34.55103
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sunkavalli K, 2010, LECT NOTES COMPUT SC, V6312, P251, DOI 10.1007/978-3-642-15552-9_19
   Taniai T, 2018, PR MACH LEARN RES, V80
   Tozza S, 2016, J MATH IMAGING VIS, V56, P57, DOI 10.1007/s10851-016-0633-0
   Verbiest F., 2008, P IEEE C COMP VIS PA, P1
   Wang C, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2517, DOI 10.1145/3394171.3413559
   Wei K, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P551
   Wei K, 2019, IEEE I CONF COMP VIS, P3740, DOI 10.1109/ICCV.2019.00384
   Wiles O., 2017, BRIT MACH VIS C
   WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479
   Wu L, 2011, LECT NOTES COMPUT SC, V6494, P703, DOI 10.1007/978-3-642-19318-7_55
   Wu Z, 2013, PROC CVPR IEEE, P1498, DOI 10.1109/CVPR.2013.197
   Yang B, 2019, ADV NEUR IN, V32
   Yang X., IEEE T PATTERN ANAL, DOI [10.1109/TPAMI.2020.3026079, 2020, DOI 10.1109/TPAMI.2020.3026079,2020]
   Yeung SK, 2015, IEEE T PATTERN ANAL, V37, P890, DOI 10.1109/TPAMI.2014.2346195
   Yu C, 2010, LECT NOTES COMPUT SC, V6314, P115
   Zheng Q, 2019, IEEE I CONF COMP VIS, P8548, DOI 10.1109/ICCV.2019.00864
NR 51
TC 18
Z9 19
U1 0
U2 23
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2022
VL 8
IS 1
BP 105
EP 118
DI 10.1007/s41095-021-0223-y
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WO6VR
UT WOS:000712590200007
OA Green Submitted, gold
DA 2024-07-18
ER

PT J
AU Wu, JL
   Shi, JJ
   Zhang, L
AF Wu, Jin-Liang
   Shi, Jun-Jie
   Zhang, Lei
TI Rectangling irregular videos by optimal spatio-temporal warping
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE rectangling; warping; content-preserving
ID IMAGE COMPLETION; PERSPECTIVE; STATISTICS
AB Image and video processing based on geometric principles typically changes the rectangular shape of video frames to an irregular shape. This paper presents a warping based approach for rectangling such irregular frame boundaries in space and time, i.e., making them rectangular again. To reduce geometric distortion in the rectangling process, we employ content-preserving deformation of a mesh grid with line structures as constraints to warp the frames. To conform to the original inter-frame motion, we keep feature trajectory distribution as constraints during motion compensation to ensure stability after warping the frames. Such spatially and temporally optimized warps enable the output of regular rectangular boundaries for the video frames with low geometric distortion and jitter. Our experiments demonstrate that our approach can generate plausible video rectangling results in a variety of applications.
C1 [Wu, Jin-Liang] 54th Res Inst CETC, Shijiazhuang 050050, Hebei, Peoples R China.
   [Shi, Jun-Jie; Zhang, Lei] Beijing Inst Technol, Sch Comp Sci, Beijing 100081, Peoples R China.
C3 China Electronics Technology Group; Beijing Institute of Technology
RP Zhang, L (corresponding author), Beijing Inst Technol, Sch Comp Sci, Beijing 100081, Peoples R China.
EM jinliangwuzju@gmail.com; shijunjie@outlook.com; leizhang@bit.edu.cn
RI Jiang, Cheng/JHU-0179-2023; Wang, Huiyan/JXW-9178-2024; cai,
   wen/JWP-4797-2024; LU, lpp pp/JFJ-9011-2023; WANG, YILUN/KFB-0627-2024;
   Wang, Jinguo/JED-9233-2023
FU National Natural Science Foundation of China [61922014, 61772069]
FX The authors would like to thank the anonymous reviewers for their
   helpful comments. This work was supported by the National Natural
   Science Foundation of China under Grant Nos. 61922014 and 61772069.
CR Bertalmío M, 2001, PROC CVPR IEEE, P355
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Carroll R, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778864
   Criminisi A, 2003, PROC CVPR IEEE, P721
   Drori I, 2003, ACM T GRAPHIC, V22, P303, DOI 10.1145/882262.882267
   Du SP, 2013, IEEE T VIS COMPUT GR, V19, P1288, DOI 10.1109/TVCG.2013.14
   Hays J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239455
   He KM, 2014, IEEE T PATTERN ANAL, V36, P2423, DOI 10.1109/TPAMI.2014.2330611
   He KM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462004
   Heeger D. J., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P229, DOI 10.1145/218380.218446
   Hu SM, 2013, VISUAL COMPUT, V29, P393, DOI 10.1007/s00371-013-0792-6
   Igarashi T, 2005, ACM T GRAPHIC, V24, P1134, DOI 10.1145/1073204.1073323
   Jia YT, 2005, VISUAL COMPUT, V21, P601, DOI 10.1007/s00371-005-0313-3
   Karni Z, 2009, COMPUT GRAPH FORUM, V28, P1257, DOI 10.1111/j.1467-8659.2009.01503.x
   Komodakis N, 2007, IEEE T IMAGE PROCESS, V16, P2649, DOI 10.1109/TIP.2007.906269
   Kopf J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366150
   Levin A, 2004, LECT NOTES COMPUT SC, V2034, P377
   Li ZT, 2018, IEEE T IMAGE PROCESS, V27, P4478, DOI 10.1109/TIP.2018.2839916
   Liu F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531350
   Pavic D, 2006, VISUAL COMPUT, V22, P671, DOI 10.1007/s00371-006-0050-2
   Perazzi F, 2015, COMPUT GRAPH FORUM, V34, P57, DOI 10.1111/cgf.12541
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983
   Schaefer S, 2006, ACM T GRAPHIC, V25, P533, DOI 10.1145/1141911.1141920
   Shiratori T., 2006, Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition, P411
   Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939
   Sun J, 2005, ACM T GRAPHIC, V24, P861, DOI 10.1145/1073204.1073274
   Wang M, 2018, IEEE T IMAGE PROCESS, V27, P5854, DOI 10.1109/TIP.2018.2859628
   Wang M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661278
   Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071
   Wei J, 2012, IEEE T VIS COMPUT GR, V18, P1771, DOI 10.1109/TVCG.2011.130
   Werner T, 2002, LECT NOTES COMPUT SC, V2351, P541
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
   Xu Y, 2011, COMPUT AIDED GEOM D, V28, P349, DOI 10.1016/j.cagd.2011.07.001
   Yeh IC, 2020, MULTIMED TOOLS APPL, V79, P26123, DOI 10.1007/s11042-020-09159-z
   Zhang FLE, 2020, COMPUT VIS MEDIA, V6, P291, DOI 10.1007/s41095-020-0187-3
   Zhang Y, 2021, IEEE T VIS COMPUT GR, V27, P3198, DOI 10.1109/TVCG.2020.2965097
   Zhang Y, 2019, VISUAL COMPUT, V35, P823, DOI 10.1007/s00371-019-01694-7
   Zhu Z, 2016, IEEE T VIS COMPUT GR, V22, P1945, DOI 10.1109/TVCG.2015.2480081
NR 40
TC 2
Z9 2
U1 2
U2 2
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2022
VL 8
IS 1
BP 93
EP 103
DI 10.1007/s41095-021-0222-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WO6VR
UT WOS:000712590200006
OA gold
DA 2024-07-18
ER

PT J
AU Xing, JB
   Hu, WB
   Zhang, YC
   Wong, TT
AF Xing, Jinbo
   Hu, Wenbo
   Zhang, Yuechen
   Wong, Tien-Tsin
TI Flow-aware synthesis: A generic motion model for video frame
   interpolation
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE flow-aware; generic motion model; video frame interpolation
AB A popular and challenging task in video research, frame interpolation aims to increase the frame rate of video. Most existing methods employ a fixed motion model, e.g., linear, quadratic, or cubic, to estimate the intermediate warping field. However, such fixed motion models cannot well represent the complicated non-linear motions in the real world or rendered animations. Instead, we present an adaptive flow prediction module to better approximate the complex motions in video. Furthermore, interpolating just one intermediate frame between consecutive input frames may be insufficient for complicated non-linear motions. To enablemulti-frame interpolation, we introduce the time as a control variable when interpolating frames between original ones in our generic adaptive flow prediction module. Qualitative and quantitative experimental results show that our method can produce high-quality results and outperforms the existing state-of-the-art methods on popular public datasets.
C1 [Xing, Jinbo; Hu, Wenbo; Zhang, Yuechen; Wong, Tien-Tsin] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
   [Xing, Jinbo; Hu, Wenbo; Wong, Tien-Tsin] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen Key Lab Virtual Real & Human Interact Te, Shenzhen, Peoples R China.
C3 Chinese University of Hong Kong; Chinese Academy of Sciences; Shenzhen
   Institute of Advanced Technology, CAS
RP Wong, TT (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China.; Wong, TT (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen Key Lab Virtual Real & Human Interact Te, Shenzhen, Peoples R China.
EM jbxing@cse.cuhk.edu.hk; wbhu@cse.cuhk.edu.hk; zhangyc@link.cuhk.edu.hk;
   ttwong@cse.cuhk.edu.hk
RI XING, Jinbo/JHT-1415-2023; HU, WENBO/C-7541-2014
FU Research Grants Council of the Hong Kong Special Administrative Region,
   under RGC General Research Fund [CUHK 14201017]; Shenzhen Science and
   Technology Program [JCYJ20180507182410327]; Science and Technology Plan
   Project of Guangzhou [201704020141]
FX This project was supported by the Research Grants Council of the Hong
   Kong Special Administrative Region, under RGC General Research Fund
   (Project No. CUHK 14201017), Shenzhen Science and Technology Program
   (No. JCYJ20180507182410327), and the Science and Technology Plan Project
   of Guangzhou (No. 201704020141).
CR Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2
   Bao WB, 2019, PROC CVPR IEEE, P3698, DOI 10.1109/CVPR.2019.00382
   Bao WB, 2021, IEEE T PATTERN ANAL, V43, P933, DOI 10.1109/TPAMI.2019.2941941
   CHARBONNIER P, 1994, IEEE IMAGE PROC, P168
   Cheng XH, 2020, AAAI CONF ARTIF INTE, V34, P10607
   Choi M, 2020, AAAI CONF ARTIF INTE, V34, P10663
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dosovitskiy A., 2016, Advances in Neural Information Processing Systems, P658
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Flynn J, 2016, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2016.595
   Fourure D., 2017, ARXIV170707958, P181
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jiang HZ, 2018, PROC CVPR IEEE, P9000, DOI 10.1109/CVPR.2018.00938
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Junheum Park, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P109, DOI 10.1007/978-3-030-58568-6_7
   Karargyris A, 2011, IEEE T MED IMAGING, V30, P957, DOI 10.1109/TMI.2010.2098882
   Kingma D. P, 2015, International Conference on Learning Representations
   Lee H, 2020, PROC CVPR IEEE, P5315, DOI 10.1109/CVPR42600.2020.00536
   Liu YL, 2019, AAAI CONF ARTIF INTE, P8794
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCVW.2017.361, 10.1109/ICCV.2017.478]
   Long GC, 2016, LECT NOTES COMPUT SC, V9910, P434, DOI 10.1007/978-3-319-46466-4_26
   Lu G, 2018, IEEE T IMAGE PROCESS, V27, P678, DOI 10.1109/TIP.2017.2767782
   Maas AL., 2013, P ICML WORKSHOP DEEP, V28, P1
   Meyer S, 2018, PROC CVPR IEEE, P498, DOI 10.1109/CVPR.2018.00059
   Meyer S, 2015, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2015.7298747
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Niklaus S, 2020, PROC CVPR IEEE, P5436, DOI 10.1109/CVPR42600.2020.00548
   Niklaus S, 2018, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2018.00183
   Niklaus S, 2017, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2017.37
   Niklaus S, 2017, PROC CVPR IEEE, P2270, DOI 10.1109/CVPR.2017.244
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Shurui Gui, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14001, DOI 10.1109/CVPR42600.2020.01402
   Su SC, 2017, PROC CVPR IEEE, P237, DOI 10.1109/CVPR.2017.33
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu CY, 2018, LECT NOTES COMPUT SC, V11212, P425, DOI 10.1007/978-3-030-01237-3_26
   Xu X, P ADV NEURAL INFORM
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yuan LZ, 2019, PROC CVPR IEEE, P12175, DOI 10.1109/CVPR.2019.01246
   Zhixiang Chi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P107, DOI 10.1007/978-3-030-58583-9_7
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 42
TC 5
Z9 5
U1 0
U2 11
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2021
VL 7
IS 3
BP 393
EP 405
DI 10.1007/s41095-021-0208-x
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TR1TJ
UT WOS:000678754100008
OA gold
DA 2024-07-18
ER

PT J
AU Iazzi, A
   Rziza, M
   Thami, ROH
AF Iazzi, Abderrazak
   Rziza, Mohammed
   Thami, Rachid Oulad Haj
TI Efficient fall activity recognition by combining shape and motion
   features
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE fall detection; elderly people; shape features; motion features;
   classification
ID DETECTION SYSTEM; SEGMENTATION; VECTOR
AB This paper presents a vision-based system for recognizing when elderly adults fall. A fall is characterized by shape deformation and high motion. We represent shape variation using three features, the aspect ratio of the bounding box, the orientation of an ellipse representing the body, and the aspect ratio of the projection histogram. For motion variation, we extract several features from three blocks corresponding to the head, center of the body, and feet using optical flow. For each block, we compute the speed and the direction of motion. Each activity is represented by a feature vector constructed from variations in shape and motion features for a set of frames. A support vector machine is used to classify fall and non-fall activities. Experiments on three different datasets show the effectiveness of our proposed method.
C1 [Iazzi, Abderrazak; Rziza, Mohammed] Univ Mohammed 5 Rabat, Fac Sci, RABAT IT CTR, LRIT, Rabat, Morocco.
   [Thami, Rachid Oulad Haj] Univ Mohammed 5 Rabat, ENSIAS, RABAT IT CTR, ADMIR LAB,IRDA, Rabat, Morocco.
C3 Mohammed V University in Rabat; Mohammed V University in Rabat
RP Iazzi, A (corresponding author), Univ Mohammed 5 Rabat, Fac Sci, RABAT IT CTR, LRIT, Rabat, Morocco.
EM abderrazak.iazzi@gmail.com; mohammed.rziza@gmail.com;
   rachid.ouladhajthami@gmail.com
RI IAZZI, Abderrazak/O-9073-2019
OI RZIZA, Mohammed/0000-0003-4974-5903; iazzi,
   abderrazak/0000-0001-8877-0648
FU MERSFC; CNRST
FX This research is part of the ANGEL project supported by MERSFC and
   CNRST.
CR Akagündüz E, 2017, IEEE J BIOMED HEALTH, V21, P756, DOI 10.1109/JBHI.2016.2570300
   Akagündüz E, 2015, IET COMPUT VIS, V9, P750, DOI 10.1049/iet-cvi.2015.0012
   [Anonymous], 2015, P 8 ACM INT C PERV T
   [Anonymous], 2016, COMPUT VISUAL MEDIA
   [Anonymous], 2014, J WSCG
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Aslan M, 2015, APPL SOFT COMPUT, V37, P1023, DOI 10.1016/j.asoc.2014.12.035
   Auvinet E., 2010, MULTIPLE CAMERAS FAL
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bae T M, 2014, LECT NOTES COMPUTER, V3115, P401
   Bakkay MC, 2018, IEEE IMAGE PROC, P4018, DOI 10.1109/ICIP.2018.8451603
   Ben Ismail MM, 2017, J COMPUT SCI TECH-CH, V32, P356, DOI 10.1007/s11390-017-1725-z
   Bergen G, 2016, MMWR-MORBID MORTAL W, V65, P993, DOI 10.15585/mmwr.mm6537a2
   Bouguet J. Y., INTEL CORPORATION MI
   Charfi I, 2012, 8TH INTERNATIONAL CONFERENCE ON SIGNAL IMAGE TECHNOLOGY & INTERNET BASED SYSTEMS (SITIS 2012), P218, DOI 10.1109/SITIS.2012.155
   Chelli A, 2019, IEEE ACCESS, V7, P38670, DOI 10.1109/ACCESS.2019.2906693
   Chua JL, 2015, SIGNAL IMAGE VIDEO P, V9, P623, DOI 10.1007/s11760-013-0493-7
   Doukas CN, 2011, IEEE T INF TECHNOL B, V15, P277, DOI 10.1109/TITB.2010.2091140
   Fan KB, 2019, MULTIMED TOOLS APPL, V78, P9101, DOI 10.1007/s11042-018-5638-9
   Fan KB, 2017, INT J DISTRIB SENS N, V13, DOI 10.1177/1550147717707418
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Gomes V, 2017, PATTERN RECOGN, V63, P30, DOI 10.1016/j.patcog.2016.09.008
   Gonzalez L, 2008, J BIOMED OPT, V13, DOI 10.1117/1.2960621
   Hammerla N.Y., 2016, P 25 INT JOINT C ART
   Harrou F, 2019, IEEE ACCESS, V7, P114966, DOI 10.1109/ACCESS.2019.2936320
   Iazzi A, 2016, LECT NOTES COMPUTER, V10073, P156
   Igual R, 2013, BIOMED ENG ONLINE, V12, DOI 10.1186/1475-925X-12-66
   Kepski M, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, THEORY AND APPLICATIONS (VISAPP 2014), VOL 2, P640
   Khan SS, 2017, MED ENG PHYS, V39, P12, DOI 10.1016/j.medengphy.2016.10.014
   Kim K, 2004, IEEE IMAGE PROC, P3061
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar DP, 2016, INT CONF ACOUST SPEE, P1337, DOI 10.1109/ICASSP.2016.7471894
   MCFARLANE NJB, 1995, MACH VISION APPL, V8, P187, DOI 10.1007/BF01215814
   Minaee S, 2020, ARXIV200105566
   Minaee S, 2019, IEEE T IMAGE PROCESS, V28, P3192, DOI 10.1109/TIP.2019.2894966
   Núñez-Marcos A, 2017, WIREL COMMUN MOB COM, DOI 10.1155/2017/9474806
   Reyes-Ortiz JL, 2016, NEUROCOMPUTING, V171, P754, DOI 10.1016/j.neucom.2015.07.085
   Rougier C, 2011, IEEE T CIRC SYST VID, V21, P611, DOI 10.1109/TCSVT.2011.2129370
   Sree J, 2015, NATL J PHYSL PHARM P, V5, P275
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Nguyen VA, 2016, PROCEEDINGS OF THE SEVENTH SYMPOSIUM ON INFORMATION AND COMMUNICATION TECHNOLOGY (SOICT 2016), P339, DOI 10.1145/3011077.3011103
   Vishwakarma V, 2007, LECT NOTES COMPUT SC, V4815, P616
   Wiskott L, 2002, NEURAL COMPUT, V14, P715, DOI 10.1162/089976602317318938
   World Health Organization, 2008, WHO WHO GLOB REP FAL
   Yang L, 2016, DIGIT COMMUN NETW, V2, P24, DOI 10.1016/j.dcan.2015.12.001
   Yu M, 2013, IEEE J BIOMED HEALTH, V17, P1002, DOI 10.1109/JBHI.2013.2274479
   Yu M, 2012, IEEE T INF TECHNOL B, V16, P1274, DOI 10.1109/TITB.2012.2214786
   Yun YX, 2016, COMPUT VIS IMAGE UND, V148, P111, DOI 10.1016/j.cviu.2015.12.002
   Zhai Y, 2006, IEEE T MULTIMEDIA, V8, P686, DOI 10.1109/TMM.2006.876299
NR 49
TC 4
Z9 4
U1 3
U2 8
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2020
VL 6
IS 3
BP 247
EP 263
DI 10.1007/s41095-020-0183-7
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FH
UT WOS:000648691900002
OA gold
DA 2024-07-18
ER

PT J
AU Xu, PF
   Ding, JQ
   Zhang, H
   Huang, H
AF Xu, Pengfei
   Ding, Jianqiang
   Zhang, Hao
   Huang, Hui
TI Discernible image mosaic with edge-aware adaptive tiles
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE image mosaic; image retrieval; image synthesis
AB We present a novel method to produce discernible image mosaics, with relatively large image tiles replaced by images drawn from a database, to resemble a target image. Compared to existing works on image mosaics, the novelty of our method is two-fold. Firstly, believing that the presence of visual edges in the final image mosaic strongly supports image perception, we develop an edge-aware photo retrieval scheme which emphasizes the preservation of visual edges in the target image. Secondly, unlike most previous works which apply a pre-determined partition to an input image, our image mosaics are composed of adaptive tiles, whose sizes are determined based on the available images in the database and the objective of maximizing resemblance to the target image. We show discernible image mosaics obtained by our method, using image collections of only moderate size. To evaluate our method, we conducted a user study to validate that the image mosaics generated present both globally and locally appropriate visual impressions to the human observers. Visual comparisons with existing techniques demonstrate the superiority of our method in terms of mosaic quality and perceptibility.
C1 [Xu, Pengfei; Ding, Jianqiang; Huang, Hui] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen, Guangdong, Peoples R China.
   [Zhang, Hao] Simon Fraser Univ, Sch Comp Sci, Vancouver, BC, Canada.
C3 Shenzhen University; Simon Fraser University
RP Huang, H (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen, Guangdong, Peoples R China.
EM xupengfei.cg@gmail.com; dingjianqiang0x@gmail.com;
   hao.r.zhang@gmail.com; hhzhiyan@gmail.com
RI Zhang, Hao/HHM-1940-2022; wu, tong/ITV-6896-2023; Huang,
   Hui/JGB-1049-2023
OI Huang, Hui/0000-0003-3212-0544; Ding, Jianqiang/0000-0003-0705-0345;
   Zhang, Hao/0000-0003-1991-119X; Xu, Pengfei/0000-0003-4770-4374
FU National Natural Science Foundation of China [61602310, 61522213,
   61528208]; Guangdong Science and Technology Program [2015A030312015];
   Shenzhen Innovation Program [JCYJ20170302154106666,
   KQJSCX20170727101233642]; NSERC [611370]
FX We thank the anonymous reviewers and the editors for their valuable
   comments. This work was supported in part by the National Natural
   Science Foundation of China (Nos. 61602310, 61522213, and 61528208),
   Guangdong Science and Technology Program (No. 2015A030312015), Shenzhen
   Innovation Program (Nos. JCYJ20170302154106666,
   KQJSCX20170727101233642), and NSERC (No. 611370).
CR [Anonymous], 2002, P EUR A PHICS SAARBR
   [Anonymous], 2005, PUZZLE IMAGE MOSAIC
   Barnes Connelly, 2017, [Computational Visual Media, 计算可视媒体], V3, P3
   Battiato S, 2007, COMPUT GRAPH FORUM, V26, P794, DOI 10.1111/j.1467-8659.2007.01021.x
   Battiato S, 2008, ARTIFICIAL MOSAICS G, P53
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   DIBLASI G, 2006, P EUR IT CHAPT 2006, P267
   Faustino GM, 2005, SIBGRAPI 2005: XVIII BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, CONFERENCE PROCEEDINGS, P315
   Goferman S, 2010, COMPUT GRAPH FORUM, V29, P459, DOI 10.1111/j.1467-8659.2009.01615.x
   Hausner A, 2001, COMP GRAPH, P573, DOI 10.1145/383259.383327
   Hu Shi-Min, 2011, P NPAR 11 ACM SIGGRA, P27, DOI DOI 10.1145/2024676.2024681
   Huisi Wu, 2018, Computational Visual Media, V4, P173, DOI 10.1007/s41095-018-0106-z
   Kim J, 2002, ACM T GRAPHIC, V21, P657
   Lai YK, 2006, VISUAL COMPUT, V22, P604, DOI 10.1007/s00371-006-0047-x
   Liu Y, 2010, COMPUT GRAPH FORUM, V29, P2387, DOI 10.1111/j.1467-8659.2010.01752.x
   Miller J, 2012, SCI CULT NINET CENT, P153
   Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331
   Oliva A, 2006, ACM T GRAPHIC, V25, P527, DOI 10.1145/1141911.1141919
   Orchard J, 2008, CUT OUT IMAGE MOSAIC, P79
   Pavic D, 2009, COMPUT GRAPH FORUM, V28, P2244, DOI 10.1111/j.1467-8659.2009.01437.x
   Rother C, 2006, ACM T GRAPHIC, V25, P847, DOI 10.1145/1141911.1141965
   Silvers R., 1997, Photomosaics
   Yu ZQ, 2014, IEEE T VIS COMPUT GR, V20, P182, DOI 10.1109/TVCG.2013.106
   Zhang FL, 2017, IEEE T VIS COMPUT GR, V23, P1561, DOI 10.1109/TVCG.2016.2532329
   Zhang L, 2011, IMAGE MOSAICS IRREGU, P155
   Zhang L, 2016, ENTROPY, P122
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 28
TC 4
Z9 5
U1 1
U2 35
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2019
VL 5
IS 1
BP 45
EP 58
DI 10.1007/s41095-019-0130-7
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UL
UT WOS:000648690700005
OA gold
DA 2024-07-18
ER

PT J
AU Wang, LL
   Shi, XH
   Liu, Y
AF Wang, Lili
   Shi, Xuehuai
   Liu, Yi
TI Foveated rendering: A state-of-the-art survey
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review
DE foveated rendering; virtual reality (VR); real-time rendering
ID CONTRAST-SENSITIVITY; CONE OPPONENCY; COLOR-VISION; RED-GREEN;
   RESOLUTION; SIMPLIFICATION; SUPPRESSION; ATTENTION; DISPLAYS; SYSTEM
AB Recently, virtual reality (VR) technology has been widely used in medical, military, manufacturing, entertainment, and other fields. These applications must simulate different complex material surfaces, various dynamic objects, and complex physical phenomena, increasing the complexity of VR scenes. Current computing devices cannot efficiently render these complex scenes in real time, and delayed rendering makes the content observed by the user inconsistent with the user's interaction, causing discomfort. Foveated rendering is a promising technique that can accelerate rendering. It takes advantage of human eyes' inherent features and renders different regions with different qualities without sacrificing perceived visual quality. Foveated rendering research has a history of 31 years and is mainly focused on solving the following three problems. The first is to apply perceptual models of the human visual system into foveated rendering. The second is to render the image with different qualities according to foveation principles. The third is to integrate foveated rendering into existing rendering paradigms to improve rendering performance. In this survey, we review foveated rendering research from 1990 to 2021. We first revisit the visual perceptual models related to foveated rendering. Subsequently, we propose a new foveated rendering taxonomy and then classify and review the research on this basis. Finally, we discuss potential opportunities and open questions in the foveated rendering field. We anticipate that this survey will provide new researchers with a high-level overview of the state-of-the-art in this field, furnish experts with up-to-date information, and offer ideas alongside a framework to VR display software and hardware designers and engineers.
C1 [Wang, Lili; Shi, Xuehuai; Liu, Yi] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100000, Peoples R China.
   [Wang, Lili] Peng Cheng Lab, Shengzhen 518000, Peoples R China.
   [Wang, Lili] Beihang Univ, Beijing Adv Innovat Ctr Biomed Engn, Beijing 100000, Peoples R China.
C3 Beihang University; Peng Cheng Laboratory; Beihang University
RP Shi, XH (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100000, Peoples R China.
EM wanglily@buaa.edu.cn; shixuehuaireal@buaa.edu.cn; song1997@buaa.edu.cn
RI Wang, Yining/JQW-2010-2023; wang, lili/HZJ-5080-2023; ..,
   What/IXW-6776-2023; Wang, Chao/JHT-6081-2023; Wang, Yifan/KDO-8319-2024;
   wang, wei/JBS-7400-2023; li, xiang/JCN-9316-2023; LI,
   Xiang/JBJ-8387-2023; Shi, Xuehuai/ISB-5757-2023; li, liu/JXN-7328-2024;
   Liu, Zhe/KEJ-5299-2024; Yang, Lili/JTT-5215-2023; Liu,
   Yixin/ABC-7725-2021; Wang, lili/IXD-9828-2023; wang,
   yingying/JSK-6741-2023; Li, Ly/JCD-4746-2023
OI Yang, Lili/0009-0008-2926-484X; 
FU National Key R&D Program of China; National Natural Science Foundation
   of China; Beijing Science and Technology Plan;  [2019YFC1521102]; 
   [61932003];  [Z221100007722004]
FX AcknowledgementsThis work was supported by National Key R&D Program of
   China (2019YFC1521102), the National Natural Science Foundation of China
   (61932003), and Beijing Science and Technology Plan (Z221100007722004).
CR Ahir Kunjal, 2019, Augmented Human Research, V5, P1, DOI [DOI 10.1007/S41133-019-0025-2, 10.1007/s41133-019-0025-2]
   Albert R, 2017, ACM T APPL PERCEPT, V14, DOI 10.1145/3127589
   Alwani R, 2018, MICROSOFT NVIDIA TEC
   Ananpiriyakul T., 2020, ELECT IMAGING, V32, P374
   ANDERSON SJ, 1991, J PHYSIOL-LONDON, V442, P47, DOI 10.1113/jphysiol.1991.sp018781
   [Anonymous], 2003, LEVEL DETAIL 3D GRAP
   [Anonymous], 2000, CS200004 U VIRG
   Arabadzhiyska E, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073642
   Avila L, 2014, IEEE COMPUT GRAPH, V34, P103, DOI 10.1109/MCG.2014.103
   Bialkova S, 2017, 2017 IEEE 3RD WORKSHOP ON EVERYDAY VIRTUAL REALITY (WEVR)
   Bitterli B, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392481
   Blackmon S., 2017, US Patent App, Patent No. [15/372,589, 15372589]
   Bruder V., 2019, EUROVIS 2019
   CAMPBELL FW, 1968, J PHYSIOL-LONDON, V197, P551, DOI 10.1113/jphysiol.1968.sp008574
   Carpendale MST, 1996, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION '96, PROCEEDINGS, P46, DOI 10.1109/INFVIS.1996.559215
   Cater K., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P270
   Cater K., 2002, Proceedings of the ACM symposium on Virtual reality software and technology, VRST '02, P17, DOI [10.1145/585740.585744, DOI 10.1145/585740.585744]
   Chakravarthula P, 2021, IEEE T VIS COMPUT GR, V27, P4194, DOI 10.1109/TVCG.2021.3106433
   Cheng I, 2003, SEVENTH INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND ITS APPLICATIONS, VOL 1, PROCEEDINGS, P241, DOI 10.1109/ISSPA.2003.1224685
   Choi S, 2015, CONCURRENT ENG-RES A, V23, P40, DOI 10.1177/1063293X14568814
   Chwesiuk M, 2019, ACM CONFERENCE ON APPLIED PERCEPTION (SAP 2019), DOI 10.1145/3343036.3343123
   CLARK JH, 1976, COMMUN ACM, V19, P547, DOI 10.1145/360349.360354
   Cline D., 1980, DICT VISUAL SCI, V3rd
   Corra C. G., 2009, P 2009 ACM S APPL CO, P821, DOI [DOI 10.1145/1529282.1529457, https://doi.org/10.1145/1529282.1529457]
   DANIEL PM, 1961, J PHYSIOL-LONDON, V159, P203, DOI 10.1113/jphysiol.1961.sp006803
   Deza A., 2020, arXiv
   Doolani S, 2020, TECHNOLOGIES, V8, DOI 10.3390/technologies8040077
   DUCHOWSKI A, 1995, P SOC PHOTO-OPT INS, V2501, P175, DOI 10.1117/12.206720
   Duchowski A. T., 2004, GAZECONTINGENT DISPL
   Duchowski A.T., 2014, P ACM S APPL PERC, P39, DOI DOI 10.1145/2628257.2628259
   Duchowski AT, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1498700.1498703
   Duchowski AT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1314303.1314309
   Fairchild Mark D., 2013, COLOR APPEARANCE MOD, DOI [DOI 10.1002/9781118653128, 10.1002/9781118653128]
   FENDER D, 1967, J OPT SOC AM, V57, P819, DOI 10.1364/JOSA.57.000819
   Ferdani D, 2020, J CULT HERIT, V43, P129, DOI 10.1016/j.culher.2019.12.004
   FLIPSE JP, 1988, VISION RES, V28, P819, DOI 10.1016/0042-6989(88)90029-6
   Franke L, 2021, COMPUT GRAPH FORUM, V40, P110, DOI 10.1111/cgf.14176
   Friess F, 2021, IEEE T VIS COMPUT GR, V27, P1850, DOI 10.1109/TVCG.2020.3030445
   Friston S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323033
   Fujita M., 2014, Foveated real-time ray tracing for virtual reality headset
   Funkhouser T. A., 1993, Computer Graphics Proceedings, P247, DOI 10.1145/166117.166149
   Furnas G. W., 1986, ACM Sigchi Bull., V17, P16, DOI DOI 10.1145/22339.22342
   Gallo L., 2013, ISRN BIOMEDICAL ENG, V2013
   Geisler W. S., 1999, Society for Information Display 1999 International Symposium, P420
   Geisler W. S., 2002, Proceedings ETRA 2002. Eye Tracking Research and Applications Symposium, P83, DOI 10.1145/507072.507090
   Geisler WS, 1998, P SOC PHOTO-OPT INS, V3299, P294, DOI 10.1117/12.320120
   Georgeson MA, 2014, OPHTHAL PHYSL OPT, V34, P163, DOI 10.1111/opo.12108
   Georgiev I, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366211
   Guenter B, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366183
   Guérin C, 2013, NEW ENGL J MED, V368, P2159, DOI 10.1056/NEJMoa1214103
   Gupta K., 2016, GAZE CONTINGENT DEPT
   Gutierrez Mlot Esteban, 2016, BIOSTEC 2016. 9th International Joint Conference on Biomedical Engineering Systems and Technologies. Proceedings: HealthInf, P125
   Hameed B, 2019, INT J ADV COMPUT SC, V10, P491
   HENDRICKSON AE, 1984, OPHTHALMOLOGY, V91, P603
   Hillaire S, 2008, IEEE COMPUT GRAPH, V28, P47, DOI 10.1109/MCG.2008.113
   Hillaire S, 2008, IEEE VIRTUAL REALITY 2008, PROCEEDINGS, P47
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Hoppe H., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P189, DOI 10.1145/258734.258843
   Hsieh M.-C., 2018, Journalof Nursing and Health Studies, V3, P1, DOI [DOI 10.21767/2574-2825.100030, 10.21767/2574-2825.100030]
   Hsieh Min-Chai, 2017, Hu Li Za Zhi, V64, P12, DOI 10.6224/JN.000078
   Hsu CF, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P55, DOI 10.1145/3123266.3123434
   Ivancic Valenko S, 2019, TEH VJESN, V26, P1444, DOI 10.17559/TV-20190214125057
   Jin B., 2009, P C HIGH PERFORMANCE, P117
   Jindal A, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480514
   Joshi Y, 2020, IEEE ACCESS, V8, P39013, DOI 10.1109/ACCESS.2020.2975032
   Kang J, 2020, IEEE ACCESS, V8, P93335, DOI 10.1109/ACCESS.2020.2994378
   Kaplanyan AS, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356557
   KELLY DH, 1979, J OPT SOC AM, V69, P1340, DOI 10.1364/JOSA.69.001340
   Kim J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322987
   Kim KJ, 2013, PROC SPIE, V8651, DOI 10.1117/12.2002178
   Kim Y, 2021, INT SYM MIX AUGMENT, P413, DOI 10.1109/ISMAR52148.2021.00058
   Konrad R, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3361330
   Kosara R, 2002, IEEE COMPUT GRAPH, V22, P22, DOI 10.1109/38.974515
   Koskela M., 2020, THESIS TAMPERE U
   Koskela Matias K., 2018, Computational Visual Media, V4, P267, DOI 10.1007/s41095-018-0113-0
   Koskela M, 2017, IGGRAPH ASIA 2017 TECHNICAL BRIEFS (SA'17), DOI 10.1145/3145749.3149423
   Koskela M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3269978
   Koskela Matias, 2019, EUROGRAPHICS S RENDE, P1, DOI [DOI 10.2312/SR.20191219, 10.2312/sr.20191219]
   Koskela Matias., 2016, ISVC 16, P723
   Krajancich B, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459784
   Lamping J., 1995, Human Factors in Computing Systems. CHI'95 Conference Proceedings, P401
   Lee JS, 2019, OPT EXPRESS, V27, P689, DOI 10.1364/OE.27.000689
   Lee S, 2002, IEEE T MULTIMEDIA, V4, P129, DOI 10.1109/6046.985561
   Lele A, 2013, J AMB INTEL HUM COMP, V4, P17, DOI 10.1007/s12652-011-0052-4
   LEVI DM, 1985, VISION RES, V25, P963, DOI 10.1016/0042-6989(85)90207-X
   Levoy M., 1990, Computer Graphics, V24, P217, DOI 10.1145/91394.91449
   Li D, 2021, IEEE T VIS COMPUT GR, V27, P2638, DOI 10.1109/TVCG.2021.3067762
   Lindeberg T., 2016, THESIS KTH ROYAL I T
   Liu JY, 2021, INT SYM MIX AUGMENT, P1, DOI 10.1109/ISMAR52148.2021.00014
   LIU YM, 1984, SCI SIN B-CHEM B A M, V27, P710
   Loschky L. C., 2001, P ARL FED LAB 5 ANN, P53
   Loschky LC, 2000, ETRA '00, P97, DOI DOI 10.1145/355017.355032
   LU A, 2006, P EUROVIS, P115
   Luebke D, 2001, SPRING EUROGRAP, P223
   Luebke D., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P199, DOI 10.1145/258734.258847
   Lungaro P, 2018, IEEE T VIS COMPUT GR, V24, P1535, DOI 10.1109/TVCG.2018.2794119
   Mantiuk R, 2011, LECT NOTES COMPUT SC, V6944, P1, DOI 10.1007/978-3-642-23834-5_1
   Mauderer M, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P217, DOI 10.1145/2556288.2557089
   Meng X. X., 2018, P ACM COMPUT GRAPH, V1
   Meng XX, 2020, IEEE T VIS COMPUT GR, V26, P1972, DOI 10.1109/TVCG.2020.2973442
   Meng XX, 2021, IEEE T VIS COMPUT GR, V27, P3350, DOI 10.1109/TVCG.2020.2975801
   Mikkola M., 2010, P 3 WORKSH MOB VID D, P63, DOI [DOI 10.1145/1878022.1878038, 10.1145/1878022.1878038, 10.1145/1878022, DOI 10.1145/1878022]
   MITCHELL DE, 1966, AMER J OPT ARCH AM A, V43, P387
   Molenaar E. N., 2018, THESIS U UTRECHT
   Mukhina K, 2015, PROCEDIA COMPUT SCI, V66, P697, DOI 10.1016/j.procs.2015.11.079
   Mullen KT, 2005, PERCEPTION, V34, P951, DOI 10.1068/p5374
   MULLEN KT, 1991, VISION RES, V31, P119, DOI 10.1016/0042-6989(91)90079-K
   Mullen KT, 2002, VISUAL NEUROSCI, V19, P109, DOI 10.1017/S0952523802191103
   MULLEN KT, 1985, J PHYSIOL-LONDON, V359, P381, DOI 10.1113/jphysiol.1985.sp015591
   Munzner T, 2003, ACM T GRAPHIC, V22, P453, DOI 10.1145/882262.882291
   Murphy H.A., 2001, P EUR 2001 SHORT PRE
   Murphy HA, 2009, ACM T APPL PERCEPT, V5, DOI 10.1145/1462048.1462053
   Myszkowski K, 2001, COMP GRAPH, P221, DOI 10.1145/383259.383284
   Nakayama K., 1990, PERCEPTION CONTROL S, P93
   Ohshima T, 1996, P IEEE VIRT REAL ANN, P103, DOI 10.1109/VRAIS.1996.490517
   Ong SK, 2004, VIRTUAL AND AUGMENTED REALITY APPLICATIONS IN MANUFACTURING, P1
   OTADUY M.A., 2004, P 1 S APPL PERCEPTIO, V1, P123
   Owsley Cynthia, 2003, Ophthalmol Clin North Am, V16, P171, DOI 10.1016/S0896-1549(03)00003-8
   Pai Yun Suen, 2016, ACM SIGGRAPH 2016 posters, P1
   Panum P.L., 1858, PHYSIOLOGISCHE UNTER
   Park DS, 2000, PHARMACOLOGY OF CEREBRAL ISCHEMIA 2000, P105, DOI 10.1145/355017.355033
   PARKHURST D., 2001, EVALUATING GAZE CONT
   Parkhurst DJ, 2002, HUM FACTORS, V44, P611, DOI 10.1518/0018720024497015
   Patney A., 2016, ACM SIGGRAPH 2016 emerging technologies, P1, DOI [10.1145/2929464.2929472, DOI 10.1145/2929464.2929472]
   Patney A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980246
   Plaisant C, 2002, INFOVIS 2002: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2002, P57, DOI 10.1109/INFVIS.2002.1173148
   PORAC C, 1976, PSYCHOL BULL, V83, P880, DOI 10.1037/0033-2909.83.5.880
   Potter MC, 2014, ATTEN PERCEPT PSYCHO, V76, P270, DOI 10.3758/s13414-013-0605-z
   Puggioni MP, 2020, LECT NOTES COMPUT SC, V12243, P205, DOI 10.1007/978-3-030-58468-9_16
   Radkowski R, 2019, LECT NOTES COMPUT SC, V11574, P258, DOI 10.1007/978-3-030-21607-8_20
   Ramasubramanian M, 1999, COMP GRAPH, P73, DOI 10.1145/311535.311543
   Reddy M, 2001, IEEE COMPUT GRAPH, V21, P68, DOI 10.1109/38.946633
   Reingold EM, 2003, HUM FACTORS, V45, P307, DOI 10.1518/hfes.45.2.307.27235
   Rimac-Drlje S, 2010, MULTIMED TOOLS APPL, V49, P425, DOI 10.1007/s11042-009-0442-1
   Rizzo A., 2005, Human emotional state and its relevance for military VR training
   ROBSON JG, 1966, J OPT SOC AM, V56, P1141, DOI 10.1364/JOSA.56.001141
   ROVAMO J, 1979, EXP BRAIN RES, V37, P495, DOI 10.1007/bf00236819
   Saint-Louis C., 2021, P SOUTHEASTCON, P1
   Sanzharov VV, 2020, PROGRAM COMPUT SOFT+, V46, P297, DOI 10.1134/S0361768820030068
   Sarkar M., 1992, CHI '92 Conference Proceedings. ACM Conference on Human Factors in Computing Systems. Striking a Balance, P83, DOI 10.1145/142750.142763
   Schaadt A. K., 2015, DISSERTATION
   SCHADE OH, 1956, J OPT SOC AM, V46, P721, DOI 10.1364/JOSA.46.000721
   Schütz M, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P103, DOI [10.1109/VR.2019.8798284, 10.1109/vr.2019.8798284]
   Shen C, 2020, CELL STEM CELL, V27, P64, DOI 10.1016/j.stem.2020.04.009
   Sherrington C S, 1897, J Physiol, V21, P33
   Shi XH, 2021, IEEE T VIS COMPUT GR, V27, P4183, DOI 10.1109/TVCG.2021.3106488
   Shneor E, 2006, VISION RES, V46, P4258, DOI 10.1016/j.visres.2006.08.006
   Siekawa A, 2019, LECT NOTES COMPUT SC, V11295, P106, DOI 10.1007/978-3-030-05710-7_9
   Spjut J, 2020, IEEE T VIS COMPUT GR, V26, P2126, DOI 10.1109/TVCG.2020.2973053
   Stafford J. R, 2019, US Patent, Patent No. [10,169,846, 10169846]
   Stafford J. R, 2019, US Patent, Patent No. [10,192,528, 10192528]
   Stengel M, 2016, COMPUT GRAPH FORUM, V35, P129, DOI 10.1111/cgf.12956
   Strasburger H, 2011, J VISION, V11, DOI 10.1167/11.5.13
   Sun Q, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130807
   Sundstedt V., 2004, VISION MODELING VISU, P209
   Surace L, 2022, Arxiv, DOI arXiv:2108.03499
   Swafford N. T., 2016, P ACM S APPL PERC SA, P7, DOI DOI 10.1145/2931002.2931011
   Tan GJ, 2018, OPT EXPRESS, V26, P25076, DOI 10.1364/OE.26.025076
   Tanenbaum TJ, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376606
   Tsai WJ, 2014, 2014 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING CONFERENCE, P25, DOI 10.1109/VCIP.2014.7051495
   Turner Eric., 2018, 2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR), P1
   Tursun OT, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322985
   TYLER CW, 1993, J OPT SOC AM A, V10, P2084, DOI 10.1364/JOSAA.10.002084
   TYLER CW, 1990, J OPT SOC AM A, V7, P743, DOI 10.1364/JOSAA.7.000743
   Vaidyanathan K., 2014, P HIGH PERF GRAPH, P9, DOI DOI 10.2312/HPG.20141089
   VANMEETEREN A, 1972, VISION RES, V12, P825, DOI 10.1016/0042-6989(72)90008-9
   Veach E., 1998, ROBUST MONTE CARLO M
   Vinnikov Margarita, 2014, P S EYE TRACK RES AP, P119, DOI DOI 10.1145/2578153.2578170
   Viola I, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P139, DOI 10.1109/VISUAL.2004.48
   Walton DR, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459943
   Wang LL, 2020, INT SYM MIX AUGMENT, P1, DOI 10.1109/ISMAR50242.2020.00017
   Wang Z, 2001, PROC SPIE, V4472, P42, DOI 10.1117/12.449797
   Watson AB, 2000, OPT EXPRESS, V6, P12, DOI 10.1364/OE.6.000012
   Wei LJ, 2019, APPL OPTICS, V58, pA258, DOI 10.1364/AO.58.00A258
   Weier M, 2017, COMPUT GRAPH FORUM, V36, P611, DOI 10.1111/cgf.13150
   Weier M, 2018, ACM T APPL PERCEPT, V15, DOI 10.1145/3238301
   Weier M, 2016, COMPUT GRAPH FORUM, V35, P289, DOI 10.1111/cgf.13026
   Westland S, 2006, COLOR RES APPL, V31, P315, DOI 10.1002/col.20230
   Weymouth F. W., 1963, OPTOMETRY VISION SCI, V40, P550
   WEYMOUTH FW, 1958, AM J OPHTHALMOL, V46, P102, DOI 10.1016/0002-9394(58)90042-4
   Willberger Thomas, 2019, Ray Tracing Gems: High-Quality and Real-Time Rendering with DXR and Other APIs, P475, DOI [10.1007/978-1-4842-4427-226, DOI 10.1007/978-1-4842-4427-226]
   Wilson A., 2018, US Patent, Patent No. [9,972,071, 9972071]
   Xia JC, 1996, IEEE VISUAL, P327, DOI 10.1109/VISUAL.1996.568126
   Yang QQ, 2021, COMPUT GRAPH-UK, V97, P200, DOI 10.1016/j.cag.2021.04.021
   Yee H, 2001, ACM T GRAPHIC, V20, P39, DOI 10.1145/383745.383748
   Yoo CY, 2020, OPT EXPRESS, V28, P23690, DOI 10.1364/OE.399808
   You JY, 2014, IEEE T IMAGE PROCESS, V23, P200, DOI 10.1109/TIP.2013.2287611
   Young A, 2020, US Patent, Patent No. [10,650,544, 10650544]
   Young A., 2019, US Patent, Patent No. [10,339,692, 10339692]
   Yu H, 2005, VISUAL COMPUT, V21, P735, DOI 10.1007/s00371-005-0331-1
   Zheng ZP, 2018, 24TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2018), DOI 10.1145/3281505.3281588
   Zhou Jianlong., 2004, Proceedings of Bildverarbeitung fur die Medizin04, P199
NR 192
TC 7
Z9 9
U1 14
U2 47
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2023
VL 9
IS 2
BP 195
EP 228
DI 10.1007/s41095-022-0306-4
PG 34
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7N8FL
UT WOS:000907570700001
OA gold, Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, Q
   Guo, C
   Dai, HN
   Li, P
AF Wang, Qian
   Guo, Cai
   Dai, Hong-Ning
   Li, Ping
TI Stroke-GAN Painter: Learning to paint artworks using stroke-style
   generative adversarial networks
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE AI painting; painting strokes; artistic style
AB It is a challenging task to teach machines to paint like human artists in a stroke-by-stroke fashion. Despite advances in stroke-based image rendering and deep learning-based image rendering, existing painting methods have limitations: they (i) lack flexibility to choose different art-style strokes, (ii) lose content details of images, and (iii) generate few artistic styles for paintings. In this paper, we propose a stroke-style generative adversarial network, called Stroke-GAN, to solve the first two limitations. Stroke-GAN learns styles of strokes from different stroke-style datasets, so can produce diverse stroke styles. We design three players in Stroke-GAN to generate pure-color strokes close to human artists' strokes, thereby improving the quality of painted details. To overcome the third limitation, we have devised a neural network named Stroke-GAN Painter, based on Stroke-GAN; it can generate different artistic styles of paintings. Experiments demonstrate that our artful painter can generate various styles of paintings while well-preserving content details (such as details of human faces and building textures) and retaining high fidelity to the input images.
C1 [Wang, Qian; Guo, Cai] Macau Univ Sci & Technol, Sch Comp Sci & Engn, Macau, Peoples R China.
   [Wang, Qian; Li, Ping] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
   [Guo, Cai] Hanshan Normal Univ, Sch Comp & Informat Engn, Chaozhou, Peoples R China.
   [Dai, Hong-Ning] Hong Kong Baptist Univ, Dept Comp Sci, Hong Kong, Peoples R China.
   [Li, Ping] Hong Kong Polytech Univ, Sch Design, Hong Kong, Peoples R China.
C3 Macau University of Science & Technology; Hong Kong Polytechnic
   University; Hanshan Normal University; Hong Kong Baptist University;
   Hong Kong Polytechnic University
RP Li, P (corresponding author), Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.; Dai, HN (corresponding author), Hong Kong Baptist Univ, Dept Comp Sci, Hong Kong, Peoples R China.; Li, P (corresponding author), Hong Kong Polytech Univ, Sch Design, Hong Kong, Peoples R China.
EM anrogim@outlook.com; c.guo@hstc.edu.cn; hndai@ieee.org;
   p.li@polyu.edu.hk
RI Li, Ping/AAO-2019-2020; Guo, Cai/HZI-6956-2023; Dai,
   Hong-Ning/B-1931-2012
OI Li, Ping/0000-0002-1503-0240; Dai, Hong-Ning/0000-0001-6165-4196; Guo,
   Cai/0000-0001-7524-2272
FU Hong Kong Institute of Business Studies (HKIBS) Research Seed Fund
   [HKIBS RSF-212-004]; Hong Kong Polytechnic University [P0030419,
   P0030929, P0035358]
FX AcknowledgementsThe authors would like to thank the anonymous reviewers
   for their helpful suggestions and comments. This work was supported in
   part by the Hong Kong Institute of Business Studies (HKIBS) Research
   Seed Fund under Grant HKIBS RSF-212-004, and in part by The Hong Kong
   Polytechnic University under Grant P0030419, Grant P0030929, and Grant
   P0035358.
CR [Anonymous], 2013, IMAGE VIDEO BASED AR
   [Anonymous], 2019, ARXIV190408410
   Chu WT, 2018, IEEE T MULTIMEDIA, V20, P2491, DOI 10.1109/TMM.2018.2801718
   Deussen O, 2000, COMP GRAPH, P13, DOI 10.1145/344779.344792
   Deussen O, 2000, COMPUT GRAPH FORUM, V19, pC41, DOI 10.1111/1467-8659.00396
   DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009
   Dutta T, 2021, IEEE T MULTIMEDIA, V23, P2833, DOI 10.1109/TMM.2020.3017918
   Ganin Yaroslav, 2018, INT C MACHINE LEARNI, P1666, DOI DOI 10.48550/ARXIV.1804.01118
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Ha David, 2018, INT C LEARN REPR
   Haeberli P., 1990, Computer Graphics, V24, P207, DOI 10.1145/97880.97902
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hertzmann A, 2003, IEEE COMPUT GRAPH, V23, P70, DOI 10.1109/MCG.2003.1210867
   Hertzmann A., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P453, DOI 10.1145/280814.280951
   Huang HZ, 2014, COMPUT GRAPH FORUM, V33, P299, DOI 10.1111/cgf.12498
   Huang ZW, 2019, IEEE I CONF COMP VIS, P8708, DOI 10.1109/ICCV.2019.00880
   Jia B., 2019, ARXIV
   Jing YH, 2021, IEEE T CYBERNETICS, V51, P568, DOI 10.1109/TCYB.2019.2904768
   Justin R., 2018, THESIS STONY BROOK U
   Lee H, 2013, MULTIMED TOOLS APPL, V64, P277, DOI 10.1007/s11042-012-1036-x
   Liddell TM, 2018, J EXP SOC PSYCHOL, V79, P328, DOI 10.1016/j.jesp.2018.08.009
   Liu SH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6578, DOI 10.1109/ICCV48922.2021.00653
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Mellor J., 2019, P NEURAL INFORM PROC
   Radford A., 2016, INT C LEARN REPR
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Song JF, 2018, PROC CVPR IEEE, P801, DOI 10.1109/CVPR.2018.00090
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tong ZY, 2021, AAAI CONF ARTIF INTE, V35, P609
   Wang L, 2020, VISUAL COMPUT, V36, P317, DOI 10.1007/s00371-018-1609-4
   Wang Q., 2021, P SIGGRAPH ASIA 2021
   Wilson Brett., 2004, NPAR 04, P129
   Xie N, 2013, IEICE T INF SYST, VE96D, P1134, DOI 10.1587/transinf.E96.D.1134
   Xu ML, 2019, IEEE T MULTIMEDIA, V21, P1960, DOI 10.1109/TMM.2019.2891420
   Zhao YR, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P244, DOI 10.1145/3123266.3123450
   Zheng N., 2019, P INT C LEARNING REP, P1
   Zhou WY, 2021, COMPUT VIS MEDIA, V7, P153, DOI 10.1007/s41095-021-0203-2
   Zou ZX, 2021, PROC CVPR IEEE, P15684, DOI 10.1109/CVPR46437.2021.01543
NR 38
TC 1
Z9 1
U1 10
U2 38
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2023
VL 9
IS 4
BP 787
EP 806
DI 10.1007/s41095-022-0287-3
EA MAR 2023
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P1QC2
UT WOS:000947086600001
OA gold
DA 2024-07-18
ER

PT J
AU Yang, X
   Xia, D
   Kin, TC
   Igarashi, T
AF Yang, Xi
   Xia, Ding
   Kin, Taichi
   Igarashi, Takeo
TI A two-step surface-based 3D deep learning pipeline for segmentation of
   intracranial aneurysms
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE intracranial aneurysm (IA) segmentation; point-based 3D deep learning;
   medical image segmentation
ID CEREBRAL ANEURYSMS; NETWORK; FILTERS
AB The exact shape of intracranial aneurysms is critical in medical diagnosis and surgical planning. While voxel-based deep learning frameworks have been proposed for this segmentation task, their performance remains limited. In this study, we offer a two-step surface-based deep learning pipeline that achieves significantly better results. Our proposed model takes a surface model of an entire set of principal brain arteries containing aneurysms as input and returns aneurysm surfaces as output. A user first generates a surface model by manually specifying multiple thresholds for time-of-flight magnetic resonance angiography images. The system then samples small surface fragments from the entire set of brain arteries and classifies the surface fragments according to whether aneurysms are present using a point-based deep learning network (PointNet++). Finally, the system applies surface segmentation (SO-Net) to surface fragments containing aneurysms. We conduct a direct comparison of the segmentation performance of our proposed surface-based framework and an existing voxel-based method by counting voxels: our framework achieves a much higher Dice similarity (72%) than the prior approach (46%).
C1 [Yang, Xi; Xia, Ding; Kin, Taichi; Igarashi, Takeo] Univ Tokyo, 7 Chome 3-1 Hongo, Bunkyo City, Tokyo 1138654, Japan.
   [Yang, Xi] Jilin Univ, 2699 Qianjin Rd, Changchun, Peoples R China.
C3 University of Tokyo; Jilin University
RP Yang, X (corresponding author), Univ Tokyo, 7 Chome 3-1 Hongo, Bunkyo City, Tokyo 1138654, Japan.; Yang, X (corresponding author), Jilin Univ, 2699 Qianjin Rd, Changchun, Peoples R China.
EM earthyangxi@gmail.com; dingxia1995@gmail.com
RI Xia, Ding/GNM-6261-2022; Igarashi, Takeo/ITT-5921-2023
FU AMED [JP18he1602001]
FX This research was supported by AMED under Grant No. JP18he1602001.
CR Alaraj A, 2015, OPER NEUROSURG, V11, P52, DOI 10.1227/NEU.0000000000000583
   [Anonymous], 2011, ADV NEURAL INF PROCE
   Barill G, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201337
   Bizjak Ziga, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P128, DOI 10.1007/978-3-030-59725-2_13
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dakua SP, 2018, MULTIDIM SYST SIGN P, V29, P257, DOI 10.1007/s11045-016-0464-6
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Hanocka R, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322959
   Jerman T, 2020, IEEE T BIO-MED ENG, V67, P577, DOI 10.1109/TBME.2019.2918921
   Kamnitsas K, 2017, MED IMAGE ANAL, V36, P61, DOI 10.1016/j.media.2016.10.004
   Kin T, 2012, J NEUROSURG, V117, P78, DOI 10.3171/2012.3.JNS111541
   Law MWK, 2007, LECT NOTES COMPUT SC, V4791, P866
   Law MWK, 2013, IEEE T IMAGE PROCESS, V22, P845, DOI 10.1109/TIP.2012.2216274
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Morita A, 2012, NEW ENGL J MED, V366, P2474, DOI 10.1056/NEJMoa1113260
   Nakao T, 2018, J MAGN RESON IMAGING, V47, P948, DOI 10.1002/jmri.25842
   Nikravanshalmani A., 2010, P 10 IEEE INT C INF, P1
   Nikravanshalmani A, 2013, INT SYMP IMAGE SIG, P505
   Park A, 2019, JAMA NETW OPEN, V2, DOI 10.1001/jamanetworkopen.2019.5600
   Podgorsak AR, 2020, J NEUROINTERV SURG, V12, P417, DOI 10.1136/neurintsurg-2019-015214
   Qi CR, 2017, ADV NEUR IN, V30
   Sichtermann T, 2019, AM J NEURORADIOL, V40, P25, DOI 10.3174/ajnr.A5911
   Sulayman N, 2016, EGYPT J RADIOL NUC M, V47, P859, DOI 10.1016/j.ejrnm.2016.03.016
   Ueda D, 2019, RADIOLOGY, V290, P187, DOI 10.1148/radiol.2018180901
   Wang Y, 2016, MED PHYS, V43, P1777, DOI 10.1118/1.4943375
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Yang X, 2020, PROC CVPR IEEE, P2653, DOI 10.1109/CVPR42600.2020.00273
   Zhou MS, 2019, LECT NOTES COMPUT SC, V11767, P243, DOI 10.1007/978-3-030-32251-9_27
NR 29
TC 2
Z9 3
U1 6
U2 33
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2023
VL 9
IS 1
BP 57
EP 69
DI 10.1007/s41095-022-0270-z
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5K7GQ
UT WOS:000869891100004
OA Green Submitted, gold
DA 2024-07-18
ER

PT J
AU Han, WK
   Wu, H
   Wen, CL
   Wang, C
   Li, X
AF Han, Wenkai
   Wu, Hai
   Wen, Chenglu
   Wang, Cheng
   Li, Xin
TI BLNet: Bidirectional learning network for point clouds
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE point clouds; irregularity; shape features; bidirectional learning
AB The key challenge in processing point clouds lies in the inherent lack of ordering and irregularity of the 3D points. By relying on perpoint multi-layer perceptions (MLPs), most existing point-based approaches only address the first issue yet ignore the second one. Directly convolving kernels with irregular points will result in loss of shape information. This paper introduces a novel point-based bidirectional learning network (BLNet) to analyze irregular 3D points. BLNet optimizes the learning of 3D points through two iterative operations: feature-guided point shifting and feature learning from shifted points, so as to minimise intra-class variances, leading to a more regular distribution. On the other hand, explicitly modeling point positions leads to a new feature encoding with increased structure-awareness. Then, an attention pooling unit selectively combines important features. This bidirectional learning alternately regularizes the point cloud and learns its geometric features, with these two procedures iteratively promoting each other for more effective feature learning. Experiments show that BLNet is able to learn deep point features robustly and efficiently, and outperforms the prior state-of-the-art on multiple challenging tasks.
C1 [Han, Wenkai; Wu, Hai; Wen, Chenglu; Wang, Cheng] Xiamen Univ, Sch Informat, 422 Siming South Rd, Xiamen 361005, Peoples R China.
   [Li, Xin] Louisiana State Univ, Sch Elect Engn & Comp Sci, Baton Rouge, LA 70803 USA.
C3 Xiamen University; Louisiana State University System; Louisiana State
   University
RP Wen, CL (corresponding author), Xiamen Univ, Sch Informat, 422 Siming South Rd, Xiamen 361005, Peoples R China.
EM hlxwk@stu.xmu.edu.cn; wuhai@stu.xmu.edu.cn; clwen@xmu.edu.cn;
   cwang@xmu.edu.cn; xinli@cct.lsu.edu
RI Wen, Chenglu/D-5158-2018
FU National Natural Science Foundation of China [62171393]; National Key
   R&D Program of China [2021YFF0704600]
FX This work was supported by the National Natural Science Foundation of
   China (Grant No. 62171393), and National Key R&D Program of China (Grant
   No. 2021YFF0704600).
CR [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   [Anonymous], 2019, ACM T GRAPHIC, V38
   [Anonymous], 2016, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, DOI [DOI 10.1109/CVPR.2016.170, 10.1109/CVPR.2016.170]
   [Anonymous], 2016, ACM T GRAPHIC, V35
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Han WK, 2020, AAAI CONF ARTIF INTE, V34, P10925
   He D, 2016, ADV NEUR IN, V29
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang QG, 2018, PROC CVPR IEEE, P2626, DOI 10.1109/CVPR.2018.00278
   Jiang L, 2019, IEEE I CONF COMP VIS, P10432, DOI 10.1109/ICCV.2019.01053
   Jiang M., 2018, ARXIV180700652
   Kalogerakis E, 2017, PROC CVPR IEEE, P6630, DOI 10.1109/CVPR.2017.702
   Kipf TN, 2016, ARXIV
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Komarichev A, 2019, PROC CVPR IEEE, P7413, DOI 10.1109/CVPR.2019.00760
   Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Li YY, 2018, ADV NEUR IN, V31
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu XH, 2019, AAAI CONF ARTIF INTE, P8778
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Liu ZJ, 2019, ADV NEUR IN, V32
   Mao JG, 2019, IEEE I CONF COMP VIS, P1578, DOI 10.1109/ICCV.2019.00166
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Niu X, 2018, NEURAL MACHINE TRANSLATION AND GENERATION, P84
   Pontes S, 2019, IEEE IJCNN, DOI 10.1109/ijcnn.2019.8852120
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Rosa S., 2020, 2020 P IEEECVF C COM, P11108, DOI [DOI 10.1109/CVPR42600.2020.01112, 10.1109/CVPR42600.2020.01112]
   Russo P, 2018, PROC CVPR IEEE, P8099, DOI 10.1109/CVPR.2018.00845
   Shen YR, 2018, PROC CVPR IEEE, P4548, DOI 10.1109/CVPR.2018.00478
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tchapmi LP, 2017, INT CONF 3D VISION, P537, DOI 10.1109/3DV.2017.00067
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Wang L, 2019, PROC CVPR IEEE, P10288, DOI 10.1109/CVPR.2019.01054
   Wang X, 2019, PROC SPIE, V11321, DOI 10.1117/12.2539428
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Xu QG, 2020, PROC CVPR IEEE, P5660, DOI 10.1109/CVPR42600.2020.00570
   Yan X, 2020, PROC CVPR IEEE, P5588, DOI 10.1109/CVPR42600.2020.00563
   Zhang ZY, 2019, IEEE I CONF COMP VIS, P1607, DOI 10.1109/ICCV.2019.00169
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhao HS, 2019, PROC CVPR IEEE, P5550, DOI 10.1109/CVPR.2019.00571
NR 48
TC 2
Z9 3
U1 0
U2 10
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2022
VL 8
IS 4
BP 585
EP 596
DI 10.1007/s41095-021-0260-6
EA MAR 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2X6VI
UT WOS:000765189700002
OA gold
DA 2024-07-18
ER

PT J
AU Danon, D
   Arar, M
   Cohen-Or, D
   Shamir, A
AF Danon, Dov
   Arar, Moab
   Cohen-Or, Daniel
   Shamir, Ariel
TI Image resizing by reconstruction from deep features
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE image retargeting; reconstruction; deep seam carving; image resizing
AB Traditional image resizing methods usually work in pixel space and use various saliency measures. The challenge is to adjust the image shape while trying to preserve important content. In this paper we perform image resizing in feature space using the deep layers of a neural network containing rich important semantic information. We directly adjust the image feature maps, extracted from a pre-trained classification network, and reconstruct the resized image using neural-network based optimization. This novel approach leverages the hierarchical encoding of the network, and in particular, the high-level discriminative power of its deeper layers, that can recognize semantic regions and objects, thereby allowing maintenance of their aspect ratios. Our use of reconstruction from deep features results in less noticeable artifacts than use of imagespace resizing operators. We evaluate our method on benchmarks, compare it to alternative approaches, and demonstrate its strengths on challenging images.
C1 [Danon, Dov; Arar, Moab; Cohen-Or, Daniel] Tel Aviv Univ, IL-69978 Tel Aviv, Israel.
   [Shamir, Ariel] Interdisciplinary Ctr Herzliya, IL-4610101 Herzliyya, Israel.
C3 Tel Aviv University; Reichman University
RP Danon, D (corresponding author), Tel Aviv Univ, IL-69978 Tel Aviv, Israel.
EM dovdanon@post.tau.ac.il; moabarar@mail.tau.ac.il; dcor@tau.ac.il;
   arik@idc.ac.il
CR [Anonymous], 2016, ARXIV E PRINTS
   [Anonymous], 2007, 2007 IEEE 11 INT C C, DOI DOI 10.1109/ICCV.2007.4409010
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Blau Y, 2018, PROC CVPR IEEE, P6228, DOI 10.1109/CVPR.2018.00652
   Cho D, 2017, IEEE I CONF COMP VIS, P4568, DOI 10.1109/ICCV.2017.488
   Esmaeili SA, 2017, PROC CVPR IEEE, P4178, DOI 10.1109/CVPR.2017.445
   Everingham Mark, 2007, PASCAL VISUAL OBJECT
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Guo YW, 2009, IEEE T MULTIMEDIA, V11, P856, DOI 10.1109/TMM.2009.2021781
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Jaderberg M, 2015, ADV NEUR IN, V28
   Kajiura N., P 28 ACM INT C MULT, P1755
   Kiess J, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3231598
   King DB, 2015, ACS SYM SER, V1214, P1
   Krähenbühl P, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1616452.1618472, 10.1145/1618452.1618472]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Liao J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073683
   Lin JX, 2019, IEEE INT CONF MULTI, P54, DOI 10.1109/ICMEW.2019.0-111
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu ZG, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P501, DOI 10.1145/3123266.3123377
   Odena A., 2016, DISTILL, V1, P3, DOI [10.23915/distill.00003., DOI 10.23915/DISTILL, 10.23915/distill.00003, DOI 10.23915/DISTILL.00003]
   Panozzo D, 2012, COMPUT GRAPH FORUM, V31, P229, DOI 10.1111/j.1467-8659.2012.03001.x
   Pritch Y, 2009, IEEE I CONF COMP VIS, P151, DOI 10.1109/ICCV.2009.5459159
   Ren S, 2015, FASTER R CNN REAL TI, P91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Rubinstein M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531329
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shaham TR, 2019, IEEE I CONF COMP VIS, P4569, DOI 10.1109/ICCV.2019.00467
   Shocher A, 2019, IEEE I CONF COMP VIS, P4491, DOI 10.1109/ICCV.2019.00459
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song Y, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1047, DOI 10.1145/3240508.3240623
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Ulyanov D., 2016, P 33 INT C INT C MAC, V48, P1349
   Vaquero D, 2010, PROC SPIE, V7798, DOI 10.1117/12.862419
   Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071
   Wu HS, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866185
NR 41
TC 14
Z9 14
U1 1
U2 6
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2021
VL 7
IS 4
BP 453
EP 466
DI 10.1007/s41095-021-0216-x
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UK7UX
UT WOS:000692172600003
OA Green Submitted, gold
DA 2024-07-18
ER

PT J
AU Mu, Q
   Wang, XY
   Wei, YY
   Li, ZL
AF Mu, Qi
   Wang, Xinyue
   Wei, Yanyan
   Li, Zhanli
TI Low and non-uniform illumination color image enhancement using weighted
   guided image filtering
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE color image enhancement; non-uniform illumination; low illumination;
   weighted guided image filter (WGIF); color restoration
ID RETINEX THEORY; CONTRAST ENHANCEMENT; DETAIL ENHANCEMENT; FRAMEWORK;
   ALGORITHM; WAVELET
AB In the state of the art, grayscale image enhancement algorithms are typically adopted for enhancement of RGB color images captured with low or non-uniform illumination. As these methods are applied to each RGB channel independently, imbalanced inter-channel enhancements (color distortion) can often be observed in the resulting images. On the other hand, images with non-uniform illumination enhanced by the retinex algorithm are prone to artifacts such as local blurring, halos, and over-enhancement. To address these problems, an improved RGB color image enhancement method is proposed for images captured under non-uniform illumination or in poor visibility, based on weighted guided image filtering (WGIF). Unlike the conventional retinex algorithm and its variants, WGIF uses a surround function instead of a Gaussian filter to estimate the illumination component; it avoids local blurring and halo artifacts due to its anisotropy and adaptive local regularization. To limit color distortion, RGB images are first converted to HSI (hue, saturation, intensity) color space, where only the intensity channel is enhanced, before being converted back to RGB space by a linear color restoration algorithm. Experimental results show that the proposed method is effective for both RGB color and grayscale images captured under low exposure and non-uniform illumination, with better visual quality and objective evaluation scores than from comparator algorithms. It is also efficient due to use of a linear color restoration algorithm.
C1 [Mu, Qi; Wang, Xinyue; Li, Zhanli] Xian Univ Sci & Technol, Xian 710054, Peoples R China.
   [Wei, Yanyan] Shanghai Pudong Dev Bank Applicat Dev Serv Subctr, Xian 710054, Peoples R China.
C3 Xi'an University of Science & Technology
RP Mu, Q (corresponding author), Xian Univ Sci & Technol, Xian 710054, Peoples R China.
EM muqi@xust.edu.cn; 18392629616@163.com
FU National Natural Science Foundation of China [2019YFB1405000]; National
   Natural Science Basic Research Plan Program of Shannxi, China
   [2019JM-162, 2019JM-348]
FX This work was supported by the National Natural Science Foundation of
   China (Grant No. 2019YFB1405000), and the National Natural Science Basic
   Research Plan Program of Shannxi, China (Grant Nos. 2019JM-162 and
   2019JM-348).
CR [Anonymous], 2010, COMPUTER KNOWLEDGE T
   Asmare MH, 2015, SIGNAL IMAGE VIDEO P, V9, P1679, DOI 10.1007/s11760-014-0626-7
   Bhateja V, 2015, IEEE SENS J, V15, P6783, DOI 10.1109/JSEN.2015.2465935
   Boyat A, 2013, NIRMA UNIV INT CONF
   Pineda IAB, 2019, SIGNAL IMAGE VIDEO P, V13, P843, DOI 10.1007/s11760-019-01420-9
   Chaudhury KN, 2016, IEEE T IMAGE PROCESS, V25, P2519, DOI 10.1109/TIP.2016.2548363
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   [方帅 Fang Shuai], 2012, [中国图象图形学报, Journal of Image and Graphics], V17, P748
   Gianini G, 2016, INFORM SCIENCES, V327, P149, DOI 10.1016/j.ins.2015.08.015
   Gorai A., 2011, 2011 IEEE Recent Advances in Intelligent Computational Systems (RAICS 2011), P563, DOI 10.1109/RAICS.2011.6069375
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   [胡韦伟 Hu Weiwei], 2010, [工程图学学报, Journal of Engineering Graphics], V31, P104
   Huan Dai, 2010, Proceedings of the 2010 3rd International Congress on Image and Signal Processing (CISP 2010), P626, DOI 10.1109/CISP.2010.5647240
   Huang Li-hong, 2010, Journal of Applied Optics, V31, P728
   [纪则轩 JI Ze-xuan], 2009, [微电子学与计算机, Microelectronics & Computer], V26, P99
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Jobson DJ, 2002, P SOC PHOTO-OPT INS, V4736, P25, DOI 10.1117/12.477589
   Jung YJ, 2017, OPT EXPRESS, V25, P12029, DOI 10.1364/OE.25.012029
   Ko S, 2017, IEEE T IND ELECTRON, V64, P6392, DOI 10.1109/TIE.2017.2682034
   LAND EH, 1983, P NATL ACAD SCI USA, V80, P5163, DOI 10.1073/pnas.80.16.5163
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   LAND EH, 1964, AM SCI, V52, P247
   Lee C, 2012, IEEE IMAGE PROC, P965, DOI 10.1109/ICIP.2012.6467022
   Li CY, 2018, PATTERN RECOGN LETT, V104, P15, DOI 10.1016/j.patrec.2018.01.010
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Li X., 2015, Comput. Vis. Media, V1, P143, DOI DOI 10.1007/S41095-015-0013-5
   Li XY, 2013, IEEE T IMAGE PROCESS, V22, P1915, DOI 10.1109/TIP.2013.2237922
   Li ZG, 2015, IEEE T IMAGE PROCESS, V24, P120, DOI 10.1109/TIP.2014.2371234
   Liu N, 2014, INFRARED PHYS TECHN, V67, P138, DOI 10.1016/j.infrared.2014.07.013
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Lv FF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1450, DOI 10.1145/3394171.3413925
   Ma JX, 2017, INT J MOD PHYS B, V31, DOI 10.1142/S0217979217440775
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Park S, 2017, IEEE T CONSUM ELECTR, V63, P178, DOI 10.1109/TCE.2017.014847
   Park S, 2017, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-017-0192-3
   PIZER SM, 1987, COMPUT VISION GRAPH, V39, P355, DOI 10.1016/S0734-189X(87)80186-X
   Rahman ZU, 2004, J ELECTRON IMAGING, V13, P100, DOI 10.1117/1.1636183
   Sharma P, 2014, INT J WAVELETS MULTI, V12, DOI 10.1142/S0219691314500386
   Shin Y, 2014, I SYMP CONSUM ELECTR
   Sun XK, 2017, INT J DIGIT MULTIMED, V2017, DOI 10.1155/2017/9029315
   Tao FY, 2018, SOFT COMPUT, V22, P1399, DOI 10.1007/s00500-017-2813-2
   Tohl D, 2019, SIGNAL PROCESS-IMAGE, V71, P45, DOI 10.1016/j.image.2018.10.011
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Tu Qing-hua, 2016, Computer Engineering and Science, V38, P1830, DOI 10.3969/j.issn.1007-130X.2016.09.014
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang WJ, 2018, IEEE INT CONF AUTOMA, P751, DOI 10.1109/FG.2018.00118
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Xie SJ, 2015, SENSORS-BASEL, V15, P17089, DOI 10.3390/s150717089
   Yang MX, 2018, OPTOELECTRON LETT, V14, P470, DOI 10.1007/s11801-018-8046-5
   Yoshinari K, 2013, I S INTELL SIG PROC, P429, DOI 10.1109/ISPACS.2013.6704588
   Yu SY, 2019, IEEE T CIRC SYST VID, V29, P28, DOI 10.1109/TCSVT.2017.2763180
   Zhang Xuefeng, 2016, Journal of Nanjing University of Science and Technology, V40, P24, DOI 10.14177/j.cnki.32-1397n.2016.40.01.004
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhao Quan-you, 2008, Journal of Computer Applications, V28, P448, DOI 10.3724/SP.J.1087.2008.00448
   Zhou M, 2018, IEEE T BIO-MED ENG, V65, P521, DOI 10.1109/TBME.2017.2700627
NR 57
TC 11
Z9 11
U1 0
U2 48
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2021
VL 7
IS 4
BP 529
EP 546
DI 10.1007/s41095-021-0232-x
EA JUL 2021
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UK7UX
UT WOS:000677663300001
OA gold
DA 2024-07-18
ER

PT J
AU Zhou, WY
   Yang, GW
   Hu, SM
AF Zhou, Wen-Yang
   Yang, Guo-Wei
   Hu, Shi-Min
TI Jittor-GAN: A fast-training generative adversarial network model zoo
   based on Jittor
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
C1 [Zhou, Wen-Yang; Yang, Guo-Wei; Hu, Shi-Min] Tsinghua Univ, BNRist, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Hu, SM (corresponding author), Tsinghua Univ, BNRist, Beijing 100084, Peoples R China.
EM zhouwy19@mails.tsinghua.edu.cn; ygw19@mails.tsinghua.edu.cn;
   shimin@tsinghua.edu.cn
RI Hu, Shi-Min/AAW-1952-2020
FU National Natural Science Foundation of China [61521002]
FX This work was supported by National Natural Science Foundation of China
   (No. 61521002).
CR Cao YJ, 2019, IEEE ACCESS, V7, P14985, DOI 10.1109/ACCESS.2018.2886814
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hu SM, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-020-3097-4
NR 3
TC 12
Z9 13
U1 5
U2 13
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2021
VL 7
IS 1
BP 153
EP 157
DI 10.1007/s41095-021-0203-2
PG 5
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FQ
UT WOS:000648692800010
OA gold
DA 2024-07-18
ER

PT J
AU Ishikawa, N
   Dobashi, Y
AF Ishikawa, Naoto
   Dobashi, Yoshinori
TI Estimating camera parameters from starry night photographs
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE astrophotography; constellation; star identification; pattern matching
ID RECOGNITION ALGORITHM; PATTERN-RECOGNITION; GRID ALGORITHM;
   IDENTIFICATION
AB We propose an efficient, specific method for estimating camera parameters from a single starry night image. Such an image consists of a collection of disks representing stars, so traditional estimation methods for common pictures do not work. Our method uses a database, a star catalog, that stores the positions of stars on the celestial sphere. Our method computes magnitudes (i.e., brightnesses) of stars in the input image and uses them to find the corresponding stars in the star catalog. Camera parameters can then be estimated by a simple geometric calculation. Our method is over ten times faster and more accurate than a previous method.
C1 [Ishikawa, Naoto; Dobashi, Yoshinori] Hokkaido Univ, Grad Sch Informat Sci & Technol, Sapporo, Hokkaido, Japan.
C3 Hokkaido University
RP Ishikawa, N (corresponding author), Hokkaido Univ, Grad Sch Informat Sci & Technol, Sapporo, Hokkaido, Japan.
EM ishikawa@ime.ist.hokudai.ac.jp; doba@ime.ist.hokudai.ac.jp
CR Ajdadi MJ, 2015, ASTRON J, V149, DOI 10.1088/0004-6256/149/6/182
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Cole CL, 2006, J GUID CONTROL DYNAM, V29, P64, DOI 10.2514/1.13314
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Ho K, 2012, ACTA ASTRONAUT, V73, P156, DOI 10.1016/j.actaastro.2011.10.017
   JUNKINS JL, 1977, J ASTRONAUT SCI, V25, P251
   Kolomenkin M, 2008, IEEE T AERO ELEC SYS, V44, P441, DOI 10.1109/TAES.2008.4560198
   Lang D, 2010, ASTRON J, V139, P1782, DOI 10.1088/0004-6256/139/5/1782
   Liebe C. C., 1993, IEEE Aerospace and Electronics Systems Magazine, V8, P31, DOI 10.1109/62.180383
   Liming Song, 2013, 2013 5th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC 2013), P389, DOI 10.1109/IHMSC.2013.240
   Mehta DS, 2019, IEEE T AERO ELEC SYS, V55, P689, DOI 10.1109/TAES.2018.2864431
   Pham MD, 2013, IEEE T AERO ELEC SYS, V49, P1467, DOI 10.1109/TAES.2013.6557999
   Mortari D., 2004, Navigation. Journal of the Institute of Navigation, V51, P171
   Na M, 2009, IEEE T AERO ELEC SYS, V45, P516, DOI 10.1109/TAES.2009.5089538
   Padgett C, 1997, IEEE T AERO ELEC SYS, V33, P202, DOI 10.1109/7.570743
   Pál A, 2006, PUBL ASTRON SOC PAC, V118, P1474, DOI 10.1086/508573
   Perryman MAC, 1997, ASTRON ASTROPHYS, V323, pL49
   Pogson N., 1856, Mon. Not. R. Astron. Soc, V17, P12, DOI DOI 10.1093/MNRAS/17.1.12
   Remondino F., 2006, INE ARCH PHOTOGRAMM, V36, P266272
   Rousseau GLA, 2005, IEEE AERO EL SYS MAG, V20, P27, DOI 10.1109/MAES.2005.1397146
   Salvi J, 2002, PATTERN RECOGN, V35, P1617, DOI 10.1016/S0031-3203(01)00126-1
   Samirbhai MD, 2019, IEEE T AERO ELEC SYS, V55, P17, DOI 10.1109/TAES.2018.2845198
   Spratling BB IV, 2009, ALGORITHMS, V2, P93, DOI 10.3390/a2010093
   Yoon Y, 2013, IEEE T AERO ELEC SYS, V49, P2065, DOI 10.1109/TAES.2013.6558041
   Zhang Z., 2005, CAMERA CALIBRATION
NR 25
TC 0
Z9 0
U1 0
U2 7
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2020
VL 6
IS 4
BP 445
EP 454
DI 10.1007/s41095-020-0198-0
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FN
UT WOS:000648692500006
OA gold
DA 2024-07-18
ER

PT J
AU Deakin, LJ
   Knackstedt, MA
AF Deakin, Lachlan J.
   Knackstedt, Mark A.
TI Efficient ray casting of volumetric images using distance maps for empty
   space skipping
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE ray casting; volume rendering; isosurface rendering; distance maps
AB Volume and isosurface rendering are methods of projecting volumetric images to two dimensions for visualisation. These methods are common in medical imaging and scientific visualisation.Head-mounted optical see-through displays have recently become an affordable technology and are a promising platform for volumetric image visualisation. Images displayed on a head-mounted display must be presented at a high frame rate and with low latency to compensate for head motion. High latency can be jarring and may cause cybersickness which has similar symptoms to motion sickness.Volumetric images can be very computationally expensive to render as they often have hundreds of millions of scalar values. Fortunately, certain materials in images such as air surrounding an object boundary are often made transparent and need not be sampled, which improves rendering efficiency.In our previous work we introduced a novel ray traversal technique for rendering large sparse volumetric images at high frame rates. The method relied on the computation of an occupancy and distance map to speed up ray traversal through empty regions.In this work we achieve higher frame rates than our previous work with an improved method of resuming empty space skipping and the use of anisotropic Chebyshev distance maps. An optimised algorithm for computing Chebyshev distance maps on a graphical processing unit is introduced supporting real-time transfer function editing.
C1 [Deakin, Lachlan J.; Knackstedt, Mark A.] Australian Natl Univ, Canberra, ACT 2601, Australia.
C3 Australian National University
RP Deakin, LJ (corresponding author), Australian Natl Univ, Canberra, ACT 2601, Australia.
EM lachlan.deakin@anu.edu.au; mark.knackstedt@anu.edu.au
OI Deakin, Lachlan/0000-0003-2026-759X
FU Australian Government Research Training Program (AGRTP)
FX This work is funded by the Australian Government Research Training
   Program (AGRTP) with additional support from the Australian National
   Laboratory for X-ray Micro Computed Tomography.
CR Beyer J, 2015, COMPUT GRAPH FORUM, V34, P13, DOI 10.1111/cgf.12605
   Deakin L, 2019, VKVOLUME
   Deakin L, 2019, SA'19: SIGGRAPH ASIA 2019 TECHNICAL BRIEFS, P25, DOI 10.1145/3355088.3365164
   Es A, 2007, J PARALLEL DISTR COM, V67, P1201, DOI 10.1016/j.jpdc.2007.06.011
   Gobbetti E, 2008, VISUAL COMPUT, V24, P797, DOI 10.1007/s00371-008-0261-9
   Hadwiger M, 2005, COMPUT GRAPH FORUM, V24, P303, DOI 10.1111/j.1467-8659.2005.00855.x
   Hadwiger M, 2018, IEEE T VIS COMPUT GR, V24, P974, DOI 10.1109/TVCG.2017.2744238
   Klacansky P, 2019, OPEN SCI VISUALIZATI
   Krüger J, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P287, DOI 10.1109/VISUAL.2003.1250384
   Parker S, 1998, VISUALIZATION '98, PROCEEDINGS, P233, DOI 10.1109/VISUAL.1998.745713
   Quilez I, 2018, WEBSITE ARTICLES RAY
   SAITO T, 1994, PATTERN RECOGN, V27, P1551, DOI 10.1016/0031-3203(94)90133-3
   Sramek M, 2000, IEEE T VIS COMPUT GR, V6, P236, DOI 10.1109/2945.879785
   Stegmaier S, 2005, VOLUME GRAPHICS 2005, P187
NR 14
TC 6
Z9 7
U1 0
U2 6
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2020
VL 6
IS 1
BP 53
EP 63
DI 10.1007/s41095-019-0155-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FA
UT WOS:000648691200005
OA gold
DA 2024-07-18
ER

PT J
AU Tian, HY
   Zhang, L
   Li, SJ
   Yao, M
   Pan, G
AF Tian, Huiyuan
   Zhang, Li
   Li, Shijian
   Yao, Min
   Pan, Gang
TI Pyramid-VAE-GAN: Transferring hierarchical latent variables for image
   inpainting
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE image inpainting; variational autoencoder (VAE); latent variable
   transfer (LTN); pyramid structure; generative model
AB Significant progress has been made in image inpainting methods in recent years. However, they are incapable of producing inpainting results with reasonable structures, rich detail, and sharpness at the same time. In this paper, we propose the Pyramid-VAE-GAN network for image inpainting to address this limitation. Our network is built on a variational autoencoder (VAE) backbone that encodes high-level latent variables to represent complicated high-dimensional prior distributions of images. The prior assists in reconstructing reasonable structures when inpainting. We also adopt a pyramid structure in our model to maintain rich detail in low-level latent variables. To avoid the usual incompatibility of requiring both reasonable structures and rich detail, we propose a novel cross-layer latent variable transfer module. This transfers information about long-range structures contained in high-level latent variables to low-level latent variables representing more detailed information. We further use adversarial training to select the most reasonable results and to improve the sharpness of the images. Extensive experimental results on multiple datasets demonstrate the superiority of our method. Our code is available at https://github.com/ thy960112/Pyramid-VAE-GAN.
C1 [Tian, Huiyuan; Zhang, Li; Li, Shijian; Yao, Min; Pan, Gang] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
   [Zhang, Li] Zhejiang Univ, Adv Technol Res Inst, Hangzhou 310027, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Zhang, L (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.; Zhang, L (corresponding author), Zhejiang Univ, Adv Technol Res Inst, Hangzhou 310027, Peoples R China.
EM zhangli85@zju.edu.cn
RI Li, Shijian/AAH-7273-2020
OI Li, Shijian/0000-0002-1475-1716
FU National Natural Science Foundation of China [61925603]
FX AcknowledgementsThe authors gratefully acknowledge the financial support
   of the National Natural Science Foundation of China (Grant No.
   61925603).
CR [Anonymous], 2015, arXiv
   Barnes Connelly, 2017, [Computational Visual Media, 计算可视媒体], V3, P3
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Chen YT, 2021, APPL INTELL, V51, P3460, DOI 10.1007/s10489-020-01971-2
   Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Devroye L., 1986, 1986 Winter Simulation Conference Proceedings, P260, DOI 10.1145/318242.318443
   Doersch Carl, 2016, ARXIV160605908
   Frazer J, 2021, NATURE, V599, P91, DOI 10.1038/s41586-021-04043-8
   Fu MC, 2015, INT SER OPER RES MAN, V216, P105, DOI 10.1007/978-1-4939-1384-8_5
   Fukushima S., 1982, inCompetition and Cooperation in Neural Nets, P267, DOI 10.1007/978-3-642-46466-9_18
   Gao R, 2020, IEEE T IMAGE PROCESS, V29, P3665, DOI 10.1109/TIP.2020.2964429
   Gonzalez R.C., 2018, Digital Image Processing
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hensel M, 2017, ADV NEUR IN, V30
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Jingyuan Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7757, DOI 10.1109/CVPR42600.2020.00778
   Karras T, 2018, P INT C LEARN REPR I
   Kingma D. P., 2014, arXiv
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Kulkarni TD, 2015, ADV NEUR IN, V28
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lim J.H., 2017, arXiv
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu HY, 2021, PROC CVPR IEEE, P9367, DOI 10.1109/CVPR46437.2021.00925
   Lu M, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9050858
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Peng JL, 2021, PROC CVPR IEEE, P10770, DOI 10.1109/CVPR46437.2021.01063
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278
   Salimans T, 2015, PR MACH LEARN RES, V37, P1218
   Shetty Rakshith, 2018, INT CONFNEURAL INF P, P7717
   Sohn K, 2015, ADV NEUR IN, V28
   Sun RQ, 2021, COMPUT VIS MEDIA, V7, P363, DOI 10.1007/s41095-021-0219-7
   Szeliski R, 2011, TEXTS COMPUT SCI, P181, DOI 10.1007/978-1-84882-935-0_4
   Tylecek R, 2013, LECT NOTES COMPUT SC, V8142, P364, DOI 10.1007/978-3-642-40602-7_39
   Vahdat A., 2020, P 34 INT C NEUR INF, V33, P19667
   Walker J, 2016, LECT NOTES COMPUT SC, V9911, P835, DOI 10.1007/978-3-319-46478-7_51
   Wan ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4672, DOI 10.1109/ICCV48922.2021.00465
   Wang N, 2021, IEEE T IMAGE PROCESS, V30, P1784, DOI 10.1109/TIP.2020.3048629
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wu X, 2020, IEEE T IMAGE PROCESS, V29, P2344, DOI 10.1109/TIP.2019.2945866
   Wu X, 2017, TSINGHUA SCI TECHNOL, V22, P660, DOI 10.23919/TST.2017.8195348
   Xue Y, 2022, COMPUT VIS MEDIA, V8, P3, DOI 10.1007/s41095-021-0234-8
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zeng XX, 2022, COMPUT VIS MEDIA, V8, P239, DOI 10.1007/s41095-021-0238-4
   Zeng YH, 2019, PROC CVPR IEEE, P1486, DOI 10.1109/CVPR.2019.00158
   Zheng CX, 2019, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2019.00153
NR 50
TC 4
Z9 4
U1 9
U2 19
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2023
VL 9
IS 4
BP 827
EP 841
DI 10.1007/s41095-022-0331-3
EA JUL 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P1QC2
UT WOS:001036012200001
OA gold
DA 2024-07-18
ER

PT J
AU Du, CH
   Yu, F
   Jiang, MH
   Hua, AL
   Zhao, YX
   Wei, X
   Peng, T
   Hu, XR
AF Du, Chenghu
   Yu, Feng
   Jiang, Minghua
   Hua, Ailing
   Zhao, Yaxin
   Wei, Xiong
   Peng, Tao
   Hu, Xinrong
TI High fidelity virtual try-on network via semantic adaptation and
   distributed componentization
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE virtual try-on; conditional image synthesis; human parsing; thin plate
   spline; semantic adaptation
AB Image-based virtual try-on systems have significant commercial value in online garment shopping. However, prior methods fail to appropriately handle details, so are defective in maintaining the original appearance of organizational items including arms, the neck, and in-shop garments. We propose a novel high fidelity virtual try-on network to generate realistic results. Specifically, a distributed pipeline is used for simultaneous generation of organizational items. First, the in-shop garment is warped using thin plate splines (TPS) to give a coarse shape reference, and then a corresponding target semantic map is generated, which can adaptively respond to the distribution of different items triggered by different garments. Second, organizational items are componentized separately using our novel semantic map-based image adjustment network (SMIAN) to avoid interference between body parts. Finally, all components are integrated to generate the overall result by SMIAN. A priori dual-modal information is incorporated in the tail layers of SMIAN to improve the convergence rate of the network. Experiments demonstrate that the proposed method can retain better details of condition information than current methods. Our method achieves convincing quantitative and qualitative results on existing benchmark datasets.
C1 [Du, Chenghu; Yu, Feng; Jiang, Minghua; Hua, Ailing; Zhao, Yaxin; Wei, Xiong; Peng, Tao; Hu, Xinrong] Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan 430200, Peoples R China.
   [Yu, Feng; Jiang, Minghua; Peng, Tao; Hu, Xinrong] Engn Res Ctr Hubei Prov Clothing Informat, Wuhan 430200, Peoples R China.
C3 Wuhan Textile University
RP Yu, F (corresponding author), Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan 430200, Peoples R China.; Yu, F (corresponding author), Engn Res Ctr Hubei Prov Clothing Informat, Wuhan 430200, Peoples R China.
EM duceh_lzy@163.com; yufeng@wtu.edu.cn; minghuajiang@wtu.edu.cn;
   hal_wtu@163.com; zyx@mail.wtu.edu.cn; wx_wh@wtu.edu.cn; pt@wtu.edu.cn;
   hxr@wtu.edu.cn
RI Hu, Xinrong/HGA-1351-2022; Du, Chenghu/HTP-3100-2023; Yu,
   Feng/JTD-1798-2023
OI Yu, Feng/0000-0001-8252-5131; Du, Chenghu/0000-0001-7275-5064
FU Young Talents Programme of Scientific Research Program of Hubei
   Education Department [Q20201709]; Research on the Key Technology of
   Flexible Intelligent Manufacturing of Clothing based on Digital Twin of
   Hubei Key Research and Development Program [2021BAA042]; Open Topic of
   Engineering Research Center of Hubei Province for Clothing Information
   [900204]
FX This work was supported by Young Talents Programme of Scientific
   Research Program of Hubei Education Department (Project No. Q20201709),
   Research on the Key Technology of Flexible Intelligent Manufacturing of
   Clothing based on Digital Twin of Hubei Key Research and Development
   Program (Project No. 2021BAA042), and Open Topic of Engineering Research
   Center of Hubei Province for Clothing Information (Project No. 900204).
CR Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Cheng BW, 2020, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR42600.2020.00543
   Choi S, 2021, PROC CVPR IEEE, P14126, DOI 10.1109/CVPR46437.2021.01391
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Cui A., 2021, P IEEE CVF INT C COM, P14638
   Cui YR, 2018, COMPUT GRAPH FORUM, V37, P109, DOI 10.1111/cgf.13552
   Duchon Jean, 1976, LECT NOTES MATH, V571, P85, DOI DOI 10.1007/BFB0086566
   Ge CJ, 2021, PROC CVPR IEEE, P16923, DOI 10.1109/CVPR46437.2021.01665
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Han XT, 2019, IEEE I CONF COMP VIS, P10470, DOI 10.1109/ICCV.2019.01057
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Han Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7847, DOI 10.1109/CVPR42600.2020.00787
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Heming Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P512, DOI 10.1007/978-3-030-58452-8_30
   Hensel M, 2017, ADV NEUR IN, V30
   Honda Shion, 2019, P EUR 2019
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Jandial Surgan, 2020, 2020 IEEE Winter Conference on Applications of Computer Vision (WACV). Proceedings, P2171, DOI 10.1109/WACV45572.2020.9093458
   Jetchev N, 2017, IEEE INT CONF COMP V, P2287, DOI 10.1109/ICCVW.2017.269
   Jo Y, 2019, IEEE I CONF COMP VIS, P1745, DOI 10.1109/ICCV.2019.00183
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kingma D. P., 2014, arXiv
   Lähner Z, 2018, LECT NOTES COMPUT SC, V11208, P698, DOI 10.1007/978-3-030-01225-0_41
   Lee HJ, 2019, IEEE INT CONF COMP V, P3129, DOI 10.1109/ICCVW.2019.00381
   Liang JB, 2021, COMPUT VIS MEDIA, V7, P159, DOI 10.1007/s41095-020-0189-1
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma QL, 2020, PROC CVPR IEEE, P6468, DOI 10.1109/CVPR42600.2020.00650
   Men YF, 2020, PROC CVPR IEEE, P5083, DOI 10.1109/CVPR42600.2020.00513
   Minar M. R., 2020, CVPR WORKSH
   Mir A, 2020, PROC CVPR IEEE, P7021, DOI 10.1109/CVPR42600.2020.00705
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Neuberger A, 2020, PROC CVPR IEEE, P5183, DOI 10.1109/CVPR42600.2020.00523
   Osman Ahmed A. A., 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P598, DOI 10.1007/978-3-030-58539-6_36
   Rocco I, 2019, IEEE T PATTERN ANAL, V41, P2553, DOI 10.1109/TPAMI.2018.2865351
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Salimans T, 2016, ADV NEUR IN, V29
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Wang BC, 2018, LECT NOTES COMPUT SC, V11217, P607, DOI 10.1007/978-3-030-01261-8_36
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang W, 2019, IEEE I CONF COMP VIS, P2142, DOI 10.1109/ICCV.2019.00223
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yu RY, 2019, IEEE I CONF COMP VIS, P10510, DOI 10.1109/ICCV.2019.01061
   Zhang F, 2020, PROC CVPR IEEE, P7091, DOI 10.1109/CVPR42600.2020.00712
   Zhao FW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13219, DOI 10.1109/ICCV48922.2021.01299
   Zheng Zhaoheng, 2017, [Computational Visual Media, 计算可视媒体], V3, P337
NR 47
TC 1
Z9 1
U1 4
U2 16
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2022
VL 8
IS 4
BP 649
EP 663
DI 10.1007/s41095-021-0264-2
EA JUN 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2X6VI
UT WOS:000811985700003
OA gold
DA 2024-07-18
ER

PT J
AU Li, YZ
   Luo, F
   Xiao, CX
AF Li, Yuanzhen
   Luo, Fei
   Xiao, Chunxia
TI Self-supervised coarse-to-fine monocular depth estimation using a
   lightweight attention module
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE monocular depth estimation; texture copy; depth drift; attention module
ID PREDICTION; SLAM
AB Self-supervised monocular depth estimation has been widely investigated and applied in previous works. However, existing methods suffer from texture-copy, depth drift, and incomplete structure. It is difficult for normal CNN networks to completely understand the relationship between the object and its surrounding environment. Moreover, it is hard to design the depth smoothness loss to balance depth smoothness and sharpness. To address these issues, we propose a coarse-to-fine method with a normalized convolutional block attention module (NCBAM). In the coarse estimation stage, we incorporate the NCBAM into depth and pose networks to overcome the texture-copy and depth drift problems. Then, we use a new network to refine the coarse depth guided by the color image and produce a structure-preserving depth result in the refinement stage. Our method can produce results competitive with state-of-the-art methods. Comprehensive experiments prove the effectiveness of our two-stage method using the NCBAM.
C1 [Li, Yuanzhen; Luo, Fei; Xiao, Chunxia] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
C3 Wuhan University
RP Luo, F; Xiao, CX (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
EM yuanzhen@whu.edu.cn; luofei@whu.edu.cn; cxxiao@whu.edu.cn
RI Luo, Fei/IZQ-5485-2023
OI Luo, Fei/0000-0002-8481-8357
FU Key Technological Innovation Projects of Hubei Province [2018AAA062];
   National Natural Science Foundation of China [61972298]
FX This work is partially supported by the Key Technological Innovation
   Projects of Hubei Province (2018AAA062), National Natural Science
   Foundation of China (61972298), Wuhan University-Huawei GeoInformatics
   Innovation Lab.
CR Aleotti F, 2019, LECT NOTES COMPUT SC, V11129, P337, DOI 10.1007/978-3-030-11009-3_20
   Cao YP, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3182157
   Casser V, 2019, AAAI CONF ARTIF INTE, P8001
   Chen YH, 2019, IEEE I CONF COMP VIS, P7062, DOI 10.1109/ICCV.2019.00716
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Eigen D, 2014, ADV NEUR IN, V27
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Fan XY, 2020, VISUAL COMPUT, V36, P2175, DOI 10.1007/s00371-020-01916-3
   Fu YP, 2023, IEEE T VIS COMPUT GR, V29, P1845, DOI 10.1109/TVCG.2021.3134105
   Fu YP, 2020, PROC CVPR IEEE, P5949, DOI 10.1109/CVPR42600.2020.00599
   Fu YP, 2018, PROC CVPR IEEE, P4645, DOI 10.1109/CVPR.2018.00488
   Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Guizilini V, 2020, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR42600.2020.00256
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   Guo XY, 2018, LECT NOTES COMPUT SC, V11215, P506, DOI 10.1007/978-3-030-01252-6_30
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Jaderberg M, 2015, ADV NEUR IN, V28
   Johnston A, 2020, PROC CVPR IEEE, P4755, DOI 10.1109/CVPR42600.2020.00481
   Karsch K, 2014, IEEE T PATTERN ANAL, V36, P2144, DOI 10.1109/TPAMI.2014.2316835
   Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17
   Kingma D, 2014, ICLR P, V2014, P1
   Kline J, 2020, PROCEEDINGS OF THE 2020 32ND INTERNATIONAL TELETRAFFIC CONGRESS (ITC 32), P1, DOI [10.1109/ITC3249928.2020.00009, 10.1007/978-3-030-58565-5_35]
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Li RH, 2018, IEEE INT CONF ROBOT, P7286, DOI 10.1109/ICRA.2018.8461251
   Li ZS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6177, DOI 10.1109/ICCV48922.2021.00614
   Liu MM, 2014, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2014.97
   Luo CX, 2020, IEEE T PATTERN ANAL, V42, P2624, DOI 10.1109/TPAMI.2019.2930258
   Luo HC, 2019, IEEE T MULTIMEDIA, V21, P470, DOI 10.1109/TMM.2018.2859034
   Mahjourian R, 2018, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2018.00594
   Mehta I, 2018, INT CONF 3D VISION, P314, DOI 10.1109/3DV.2018.00044
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Paszke Adam, 2017, NIPS 2017 WORKSH AUT
   Pillai S, 2019, IEEE INT CONF ROBOT, P9250, DOI [10.1109/icra.2019.8793621, 10.1109/ICRA.2019.8793621]
   Pilzer A, 2018, INT CONF 3D VISION, P587, DOI 10.1109/3DV.2018.00073
   Poggi M, 2018, INT CONF 3D VISION, P324, DOI 10.1109/3DV.2018.00045
   Ramamonjisoa M, 2021, PROC CVPR IEEE, P11084, DOI 10.1109/CVPR46437.2021.01094
   Ramamonjisoa M, 2019, IEEE INT CONF COMP V, P2109, DOI 10.1109/ICCVW.2019.00266
   Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196
   Ranjan A, 2019, PROC CVPR IEEE, P12232, DOI 10.1109/CVPR.2019.01252
   Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Shengjie Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13113, DOI 10.1109/CVPR42600.2020.01313
   Tosi F, 2019, PROC CVPR IEEE, P9791, DOI 10.1109/CVPR.2019.01003
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang CY, 2018, PROC CVPR IEEE, P2022, DOI 10.1109/CVPR.2018.00216
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Watson Jamie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P722, DOI 10.1007/978-3-030-58452-8_42
   Watson J, 2019, IEEE I CONF COMP VIS, P2162, DOI 10.1109/ICCV.2019.00225
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xie JY, 2016, LECT NOTES COMPUT SC, V9908, P842, DOI 10.1007/978-3-319-46493-0_51
   Yang GL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16249, DOI 10.1109/ICCV48922.2021.01596
   Yang L, 2018, IEEE T VIS COMPUT GR, V24, P1190, DOI 10.1109/TVCG.2017.2657766
   Yang ZH, 2018, AAAI CONF ARTIF INTE, P7493
   Yang ZH, 2018, PROC CVPR IEEE, P225, DOI 10.1109/CVPR.2018.00031
   Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212
   Yuan YH, 2021, INT J COMPUT VISION, V129, P2375, DOI 10.1007/s11263-021-01465-9
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zhao Wang, 2020, P IEEE CVF C COMP VI
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
   Zou YL, 2018, LECT NOTES COMPUT SC, V11209, P38, DOI 10.1007/978-3-030-01228-1_3
NR 63
TC 8
Z9 9
U1 3
U2 23
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2022
VL 8
IS 4
BP 631
EP 647
DI 10.1007/s41095-022-0279-3
EA JUN 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2X6VI
UT WOS:000811985700002
OA gold
DA 2024-07-18
ER

PT J
AU Guo, MH
   Xu, TX
   Liu, JJ
   Liu, ZN
   Jiang, PT
   Mu, TJ
   Zhang, SH
   Martin, RR
   Cheng, MM
   Hu, SM
AF Guo, Meng-Hao
   Xu, Tian-Xing
   Liu, Jiang-Jiang
   Liu, Zheng-Ning
   Jiang, Peng-Tao
   Mu, Tai-Jiang
   Zhang, Song-Hai
   Martin, Ralph R.
   Cheng, Ming-Ming
   Hu, Shi-Min
TI Attention mechanisms in computer vision: A survey
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review
DE attention; transformer; computer vision; deep learning; salience
ID SPATIAL-TEMPORAL ATTENTION; NETWORK; MODEL
AB Humans can naturally and effectively find salient regions in complex scenes. Motivated by this observation, attention mechanisms were introduced into computer vision with the aim of imitating this aspect of the human visual system. Such an attention mechanism can be regarded as a dynamic weight adjustment process based on features of the input image. Attention mechanisms have achieved great success in many visual tasks, including image classification, object detection, semantic segmentation, video understanding, image generation, 3D vision, multimodal tasks, and self-supervised learning. In this survey, we provide a comprehensive review of various attention mechanisms in computer vision and categorize them according to approach, such as channel attention, spatial attention, temporal attention, and branch attention; a related repository https://github.com/MenghaoGuo/Awesome-Vision-Attentions is dedicated to collecting related work. We also suggest future directions for attention mechanism research.
C1 [Guo, Meng-Hao; Xu, Tian-Xing; Liu, Zheng-Ning; Mu, Tai-Jiang; Zhang, Song-Hai; Hu, Shi-Min] Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing 100084, Peoples R China.
   [Liu, Jiang-Jiang; Jiang, Peng-Tao; Cheng, Ming-Ming] Nankai Univ, Coll Comp Sci, TKLNDST, Tianjin 300350, Peoples R China.
   [Martin, Ralph R.] Cardiff Univ, Sch Comp Sci & Informat, Cardiff, Wales.
C3 Tsinghua University; Nankai University; Cardiff University
RP Hu, SM (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing 100084, Peoples R China.
EM gmh20@mails.stsinghua.edu.cn; xutx21@mails.tsinghua.edu.cn;
   j04.liu@gmail.com; lzhengning@gmail.com; pt.jiang@mail.nankai.edu.cn;
   taijiang@tsinghua.edu.cn; shimin@tsinghua.edu.cn; MartinRR@cs.cf.ac.uk;
   cmm@nankai.edu.cn; shimin@tsinghua.edu.cn
RI Liu, Jiang-Jiang/AFW-6314-2022; Jiang, Peng-Tao/HHN-3328-2022; Martin,
   Ralph R/D-2366-2010; Mu, Tai-Jiang/JWO-1381-2024; Hu,
   Shi-Min/AAW-1952-2020; Cheng, Ming-Ming/A-2527-2009
OI Jiang, Peng-Tao/0000-0002-1786-4943; Mu, Tai-Jiang/0000-0002-9197-346X;
   Cheng, Ming-Ming/0000-0001-5550-8758
FU National Natural Science Foundation of China [61521002, 62132012]
FX This work was supported by the National Natural Science Foundation of
   China (Grant Nos. 61521002 and 62132012). We would like to thank
   Cheng-Ze Lu, Zhengyang Geng, Shilong Liu, He Wang, Huiying Lu, and
   Chenxi Huang for their helpful discussions and insightful suggestions.
CR Ba J. L., 2016, LAYER NORMALIZATION, DOI DOI 10.48550/ARXIV.1607.06450
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bao H. B., 2021, ARXIV PREPRINT ARXIV
   Bello I, 2019, IEEE I CONF COMP VIS, P3285, DOI 10.1109/ICCV.2019.00338
   Brown T., 2020, P ADV NEUR INF PROC, V33, P1877
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Caron M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9630, DOI 10.1109/ICCV48922.2021.00951
   Chaudhari S, 2021, ACM T INTEL SYST TEC, V12, DOI 10.1145/3465055
   Chen BH, 2019, IEEE I CONF COMP VIS, P371, DOI 10.1109/ICCV.2019.00046
   Chen DP, 2018, PROC CVPR IEEE, pCP1, DOI 10.1109/CVPR.2018.00128
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen M, 2020, PR MACH LEARN RES, V119
   Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844
   Chen Xinlei, 2021, P IEEE CVF INT C COM, P9640, DOI DOI 10.1109/ICCV48922.2021.00950
   Chen XY, 2022, IEEE T INTELL TRANSP, V23, P12301, DOI 10.1109/TITS.2021.3113608
   Chen YP, 2019, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2019.00052
   Chen YP, 2018, ADV NEUR IN, V31
   Chen ZR, 2019, PROC CVPR IEEE, P9164, DOI 10.1109/CVPR.2019.00939
   Choromanski KM, 2020, RETHINKING ATTENTION
   Chu X., 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, P1831, DOI DOI 10.1109/CVPR.2017.601
   Corbetta M, 2002, NAT REV NEUROSCI, V3, P201, DOI 10.1038/nrn755
   Dai J., 2021, ICLR
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dai Z., 2021, ADV NEURAL INFORM PR
   Dai ZH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2978
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J., 2018, BERT PRE TRAINING DE
   Diba A, 2018, LECT NOTES COMPUT SC, V11208, P299, DOI 10.1007/978-3-030-01225-0_18
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Du WB, 2018, IEEE T IMAGE PROCESS, V27, P1347, DOI 10.1109/TIP.2017.2778563
   Fang PF, 2019, IEEE I CONF COMP VIS, P8029, DOI 10.1109/ICCV.2019.00812
   Foret P., 2020, ARXIV PREPRINT ARXIV
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Fu Y, 2019, AAAI CONF ARTIF INTE, P8287
   Gao LL, 2020, IEEE T PATTERN ANAL, V42, P1112, DOI 10.1109/TPAMI.2019.2894139
   Gao ZL, 2019, PROC CVPR IEEE, P3019, DOI 10.1109/CVPR.2019.00314
   Geng Z., 2021, P INT C LEARN REPR
   Girdhar R, 2017, ADV NEUR IN, V30
   Gregor K, 2015, PR MACH LEARN RES, V37, P1462
   Guan Q., 2018, ARXIV PREPRINT ARXIV
   Guo M. H., 2021, ARXIV PREPRINT ARXIV
   Guo M.-H., 2022, ARXIV PREPRINT ARXIV
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P283, DOI 10.1007/s41095-021-0240-x
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Han K., 2021, Adv. Neural Inf. Process. Syst., V34, P15908, DOI DOI 10.48550/ARXIV.2103.00112
   Han Kai, 2020, ARXIV201212556
   Hayhoe M, 2005, TRENDS COGN SCI, V9, P188, DOI 10.1016/j.tics.2005.02.009
   He, 2020, ARXIV PREPRINT ARXIV
   He K. M., 2021, ARXIV PREPRINT ARXIV
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   He P, 2017, IEEE I CONF COMP VIS, P3066, DOI 10.1109/ICCV.2017.331
   Hendrycks Dan, 2017, INT C LEARNING REPRE
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hou QB, 2020, PROC CVPR IEEE, P4002, DOI 10.1109/CVPR42600.2020.00406
   Hu H, 2019, IEEE I CONF COMP VIS, P3463, DOI 10.1109/ICCV.2019.00356
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Hu J, 2018, ADV NEUR IN, V31
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jiang-Jiang Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10093, DOI 10.1109/CVPR42600.2020.01011
   Khan S., ACM COMPUT SURV, DOI [10.1145/3505244,2022, DOI 10.1145/3505244,2022]
   King DB, 2015, ACS SYM SER, V1214, P1
   Kingma D. P., 2014, arXiv
   Lee H, 2019, IEEE I CONF COMP VIS, P1854, DOI 10.1109/ICCV.2019.00194
   Lee J, 2019, PR MACH LEARN RES, V97
   Li JN, 2019, IEEE I CONF COMP VIS, P3957, DOI 10.1109/ICCV.2019.00406
   Li KP, 2018, PROC CVPR IEEE, P9215, DOI 10.1109/CVPR.2018.00960
   Li S, 2018, PROC CVPR IEEE, P369, DOI 10.1109/CVPR.2018.00046
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li X, 2019, IEEE I CONF COMP VIS, P9166, DOI 10.1109/ICCV.2019.00926
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li XB, 2020, IEEE COMPUT SOC CONF, P1274, DOI 10.1109/CVPRW50498.2020.00165
   Li ZY, 2018, COMPUT VIS IMAGE UND, V166, P41, DOI 10.1016/j.cviu.2017.10.011
   Lin Zhouhan, 2017, A structured self-attentive sentence embedding
   LINSLEY D., 2019, INT C LEARNING REPRE
   Liu H, 2017, IEEE T IMAGE PROCESS, V26, P3492, DOI 10.1109/TIP.2017.2700762
   Liu R., 2021, P IEEE CVF INT C COM, P14040
   Liu R, 2021, 2021 IEEE 13TH INTERNATIONAL CONFERENCE ON COMPUTER RESEARCH AND DEVELOPMENT (ICCRD 2021), P130, DOI 10.1109/ICCRD51685.2021.9386366
   Liu S., 2022, ARXIV PREPRINT ARXIV
   Liu S., 2021, ARXIV PREPRINT ARXIV
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu XH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P989, DOI 10.1145/3343031.3350960
   Liu Z. Y., 2020, ARXIV PREPRINT ARXIV
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Loshchilov Ilya, 2017, ARXIV171105101
   Meng LL, 2019, IEEE INT CONF COMP V, P1513, DOI 10.1109/ICCVW.2019.00189
   Minghao Yin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P191, DOI 10.1007/978-3-030-58555-6_12
   Minho Shim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P335, DOI 10.1007/978-3-030-58568-6_20
   Misra D, 2021, IEEE WINT CONF APPL, P3138, DOI 10.1109/WACV48630.2021.00318
   Mnih V, 2014, ADV NEUR IN, V27
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Oktay O., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1804.03999
   Paigwar A, 2019, IEEE COMPUT SOC CONF, P1297, DOI 10.1109/CVPRW.2019.00169
   Park J., 2018, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.1807.06514
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P1487, DOI 10.1109/TIP.2017.2774041
   Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6
   Qin ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P763, DOI 10.1109/ICCV48922.2021.00082
   Ramachandran P, 2019, ADV NEUR IN, V32
   Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667
   Roy AG, 2019, IEEE T MED IMAGING, V38, P540, DOI 10.1109/TMI.2018.2867261
   Salakhutdinov, 2015, ARXIV151104119
   Shaw P., 2018, P 2018 NAACL, V2, P464, DOI [DOI 10.18653/V1/N18-2074, 10.18653/v1/N18-2074]
   Shi HY, 2020, PROC CVPR IEEE, P4573, DOI 10.1109/CVPR42600.2020.00463
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Spillmann L, 2015, J VISION, V15, DOI 10.1167/15.9.7
   Srivastava RK, 2015, ADV NEUR IN, V28
   Su Weijie, 2020, INT C LEARN REPR
   Sun C, 2017, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2017.97
   Sutton RS, 2000, ADV NEUR IN, V12, P1057
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   TOLSTIKHIN I., 2021, P 35 C NEUR INF PROC
   Touvron H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P32, DOI 10.1109/ICCV48922.2021.00010
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang, 2016, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.1601.06823
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang Q., 2020, P IEEE CVF C COMP VI, P8326, DOI [DOI 10.1109/CVPR42600.2020.00835, 10.1109/CVPR42600.2020.00835]
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang WX, 2021, LECT NOTES COMPUT SC, V12901, P109, DOI 10.1007/978-3-030-87193-2_11
   Webb BS, 2005, J NEUROSCI, V25, P11666, DOI 10.1523/JNEUROSCI.3414-05.2005
   Wen X, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1661, DOI 10.1145/3394171.3413829
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI [10.1109/CSTIC.2018.8369274, 10.1007/s11263-019-01198-w]
   Xia B, 2019, IEEE I CONF COMP VIS, P3759, DOI 10.1109/ICCV.2019.00386
   Xie EZ, 2021, ADV NEUR IN, V34
   Xie Qizhe, 2020, SELF TRAINING NOISY, DOI 10.1109/cvpr42600.2020.01046
   Xie SN, 2018, PROC CVPR IEEE, P4606, DOI 10.1109/CVPR.2018.00484
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu SJ, 2017, IEEE I CONF COMP VIS, P4743, DOI 10.1109/ICCV.2017.507
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Xu YF, 2022, COMPUT VIS MEDIA, V8, P33, DOI 10.1007/s41095-021-0247-3
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yan X, 2020, PROC CVPR IEEE, P5588, DOI 10.1109/CVPR42600.2020.00563
   Yang B, 2019, ADV NEUR IN, V32
   Yang G, 2022, INFORM FUSION, V77, P29, DOI 10.1016/j.inffus.2021.07.016
   Yang JC, 2019, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2019.00344
   Yang JR, 2020, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR42600.2020.00335
   YANG JL, 2017, PROC CVPR IEEE, P5216, DOI DOI 10.1109/CVPR.2017.554
   Yang LX, 2021, PR MACH LEARN RES, V139
   Yang Z., 2019, P 33 C NEURAL INFORM
   Yinpeng Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11027, DOI 10.1109/CVPR42600.2020.01104
   You HX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1310, DOI 10.1145/3240508.3240702
   Yuan L, 2021, ARXIV210613112
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Yuan Y., 2019, ARXIV PREPRINT ARXIV
   Yuan Y., 2018, ARXIV PREPRINT ARXIV
   Yue KY, 2018, ADV NEUR IN, V31
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang H, 2019, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2019.00064
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang H, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P1541
   Zhang RM, 2019, IEEE T IMAGE PROCESS, V28, P4870, DOI 10.1109/TIP.2019.2911488
   Zhang SY, 2019, PR MACH LEARN RES, V97
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang ZZ, 2020, INT CONF CONDIT MON, P404, DOI 10.1109/CVPR42600.2020.01042
   Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
   Zhao H., 2020, P IEEE CVF C COMP VI, V2020, P10076, DOI 10.1109/CVPR42600.2020.01009
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng HL, 2019, PROC CVPR IEEE, P5007, DOI 10.1109/CVPR.2019.00515
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zheng ZD, 2019, IEEE T CIRC SYST VID, V29, P3037, DOI 10.1109/TCSVT.2018.2873599
   Zhou D. Q., 2021, ARXIV PREPRINT ARXIV
   Zhu XZ, 2019, IEEE I CONF COMP VIS, P6687, DOI 10.1109/ICCV.2019.00679
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
   Zhu Z, 2019, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2019.00068
   Zongxin Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11791, DOI 10.1109/CVPR42600.2020.01181
NR 184
TC 744
Z9 808
U1 440
U2 2006
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2022
VL 8
IS 3
BP 331
EP 368
DI 10.1007/s41095-022-0271-y
EA MAR 2022
PG 38
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0Q6DG
UT WOS:000769267500001
OA gold, Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Lan, YQ
   Duan, Y
   Liu, CY
   Zhu, CY
   Xiong, YS
   Huang, H
   Xu, K
AF Lan, Yuqing
   Duan, Yao
   Liu, Chenyi
   Zhu, Chenyang
   Xiong, Yueshan
   Huang, Hui
   Xu, Kai
TI ARM3D: Attention-based relation module for indoor 3D object detection
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE attention mechanism; scene understanding; relational reasoning; 3D
   indoor object detection
ID NETWORK
AB Relation contexts have been proved to be useful for many challenging vision tasks. In the field of 3D object detection, previous methods have been taking the advantage of context encoding, graph embedding, or explicit relation reasoning to extract relation contexts. However, there exist inevitably redundant relation contexts due to noisy or low-quality proposals. In fact, invalid relation contexts usually indicate underlying scene misunderstanding and ambiguity, which may, on the contrary, reduce the performance in complex scenes. Inspired by recent attention mechanism like Transformer, we propose a novel 3D attention-based relation module (ARM3D). It encompasses objectaware relation reasoning to extract pair-wise relation contexts among qualified proposals and an attention module to distribute attention weights towards different relation contexts. In this way, ARM3D can take full advantage of the useful relation contexts and filter those less relevant or even confusing contexts, which mitigates the ambiguity in detection. We have evaluated the effectiveness of ARM3D by plugging it into several state-of-the-art 3D object detectors and showing more accurate and robust detection results. Extensive experiments show the capability and generalization of ARM3D on 3D object detection. Our source code is available at https://github.com/lanlan96/ARM3D.
C1 [Lan, Yuqing; Duan, Yao; Liu, Chenyi; Zhu, Chenyang; Xiong, Yueshan; Xu, Kai] Natl Univ Def Technol, Coll Comp, Changsha 410073, Peoples R China.
   [Huang, Hui] Shenzhen Univ, Visual Comp Res Ctr, Shenzhen 518061, Peoples R China.
C3 National University of Defense Technology - China; Shenzhen University
RP Zhu, CY; Xu, K (corresponding author), Natl Univ Def Technol, Coll Comp, Changsha 410073, Peoples R China.
EM lanyuqingkd@nudt.edu.cn; duanyao16@nudt.edu.cn;
   liuchenyi1013@nudt.edu.cn; zhuchenyang07@nudt.edu.cn;
   ysxiong@hotmail.com; hhzhiyan@gmail.com; kevin.kai.xu@gmail.com
RI Huang, Hui/JGB-1049-2023
OI Huang, Hui/0000-0003-3212-0544
FU National Nature Science Foundation of China [62132021, 62102435,
   62002375, 62002376]; National Key R&D Program of China [2018AAA0102200];
   NUDT Research Grants [ZK19-30]
FX We thank Jiazhao Zhang for server management. This paper is supported in
   part by National Nature Science Foundation of China (62132021, 62102435,
   62002375, 62002376), National Key R&D Program of China (2018AAA0102200),
   and NUDT Research Grants (ZK19-30).
CR Atzmon M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201301
   Cadene R, 2019, PROC CVPR IEEE, P1989, DOI 10.1109/CVPR.2019.00209
   Chen Can, 2019, CoRR
   Chen JT, 2020, PROC CVPR IEEE, P389, DOI 10.1109/CVPR42600.2020.00047
   Chen X., 2017, PROC CVPR IEEE, V1, P3, DOI DOI 10.1109/CVPR.2017.691
   Chen XL, 2017, IEEE I CONF COMP VIS, P4106, DOI 10.1109/ICCV.2017.440
   Chenchen Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10837, DOI 10.1109/CVPR42600.2020.01085
   Cheng BW, 2021, PROC CVPR IEEE, P8959, DOI 10.1109/CVPR46437.2021.00885
   Cui QJ, 2020, PROC CVPR IEEE, P6518, DOI 10.1109/CVPR42600.2020.00655
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Duan YQ, 2019, PROC CVPR IEEE, P949, DOI 10.1109/CVPR.2019.00104
   Engelmann F., 2020, CVPR, P9028, DOI 10.1109/CVPR42600.2020.00905
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Huang SS, 2016, GRAPH MODELS, V85, P46, DOI 10.1016/j.gmod.2016.03.004
   Huang SY, 2018, ADV NEUR IN, V31
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Ku J, 2018, IEEE INT C INT ROBOT, P5750, DOI 10.1109/IROS.2018.8594049
   Kulkarni N, 2019, IEEE I CONF COMP VIS, P2212, DOI 10.1109/ICCV.2019.00230
   Lahoud J, 2017, IEEE I CONF COMP VIS, P4632, DOI 10.1109/ICCV.2017.495
   Lan YQ, 2021, COMPUT GRAPH-UK, V98, P58, DOI 10.1016/j.cag.2021.04.033
   Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298
   Li XB, 2020, IEEE COMPUT SOC CONF, P1274, DOI 10.1109/CVPRW50498.2020.00165
   Li YY, 2018, ADV NEUR IN, V31
   Li Y, 2020, ISPRS J PHOTOGRAMM, V165, P43, DOI 10.1016/j.isprsjprs.2020.05.008
   Lin DH, 2013, IEEE I CONF COMP VIS, P1417, DOI 10.1109/ICCV.2013.179
   Mou LC, 2019, PROC CVPR IEEE, P12408, DOI 10.1109/CVPR.2019.01270
   Pang G, 2016, INT C PATT RECOG, P585, DOI 10.1109/ICPR.2016.7899697
   Paszke A, 2019, ADV NEUR IN, V32
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Qi Sun, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P224, DOI 10.1145/3372278.3390693
   Qi XJ, 2017, IEEE I CONF COMP VIS, P5209, DOI 10.1109/ICCV.2017.556
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Santoro A., 2017, Advances in Neural Information Processing Systems
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Shi SS, 2021, IEEE T PATTERN ANAL, V43, P2647, DOI 10.1109/TPAMI.2020.2977026
   Shi YF, 2019, PROC CVPR IEEE, P1771, DOI 10.1109/CVPR.2019.00187
   Shi YF, 2016, COMPUT GRAPH-UK, V55, P55, DOI 10.1016/j.cag.2015.11.003
   Song PH, 2017, LECT NOTES COMPUT SC, V10345, P235, DOI 10.1007/978-3-319-65849-0_26
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Wang L, 2019, PROC CVPR IEEE, P10288, DOI 10.1109/CVPR.2019.01054
   Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608
   Wang WB, 2019, PROC CVPR IEEE, P8180, DOI 10.1109/CVPR.2019.00838
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Wen CC, 2021, ISPRS J PHOTOGRAMM, V173, P181, DOI 10.1016/j.isprsjprs.2021.01.007
   Wen X, 2020, PROC CVPR IEEE, P1936, DOI 10.1109/CVPR42600.2020.00201
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Xie Qizhe, 2020, SELF TRAINING NOISY, DOI 10.1109/cvpr42600.2020.01046
   Xu H, 2019, PROC CVPR IEEE, P9290, DOI 10.1109/CVPR.2019.00952
   Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337
   Yew ZJ, 2018, LECT NOTES COMPUT SC, V11219, P630, DOI 10.1007/978-3-030-01267-0_37
   Yi L, 2019, PROC CVPR IEEE, P3942, DOI 10.1109/CVPR.2019.00407
   Yifei Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14021, DOI 10.1109/CVPR42600.2020.01404
   Zetong Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11037, DOI 10.1109/CVPR42600.2020.01105
   Zhang WX, 2019, PROC CVPR IEEE, P12428, DOI 10.1109/CVPR.2019.01272
   Zhang YD, 2017, IEEE I CONF COMP VIS, P1201, DOI 10.1109/ICCV.2017.135
   Zaiwei Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P311, DOI [10.1061/9780784482933.027, 10.1007/978-3-030-58610-2_19]
   Zhao H., 2020, P IEEE CVF C COMP VI, V2020, P10076, DOI 10.1109/CVPR42600.2020.01009
NR 62
TC 8
Z9 8
U1 1
U2 19
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2022
VL 8
IS 3
BP 395
EP 414
DI 10.1007/s41095-021-0252-6
EA MAR 2022
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0Q6DG
UT WOS:000766092500001
OA gold, Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, P
   Fu, HY
   Ma, HD
AF Liu, Peng
   Fu, Huiyuan
   Ma, Huadong
TI An end-to-end convolutional network for joint detecting and denoising
   adversarial perturbations in vehicle classification
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE adversarial defense; adversarial detection; vehicle classification; deep
   learning
AB Deep convolutional neural networks (DCNNs) have been widely deployed in real-world scenarios. However, DCNNs are easily tricked by adversarial examples, which present challenges for critical applications, such as vehicle classification. To address this problem, we propose a novel end-to-end convolutional network for joint detection and removal of adversarial perturbations by denoising (DDAP). It gets rid of adversarial perturbations using the DDAP denoiser based on adversarial examples discovered by the DDAP detector. The proposed method can be regarded as a pre-processing step-it does not require modifying the structure of the vehicle classification model and hardly affects the classification results on clean images. We consider four kinds of adversarial attack (FGSM, BIM, DeepFool, PGD) to verify DDAP's capabilities when trained on BIT-Vehicle and other public datasets. It provides better defense than other state-of-the-art defensive methods.
C1 [Liu, Peng; Fu, Huiyuan; Ma, Huadong] Beijing Univ Posts & Telecommun, Beijing Key Lab Intelligent Telecommun Software &, Beijing 100876, Peoples R China.
C3 Beijing University of Posts & Telecommunications
RP Fu, HY (corresponding author), Beijing Univ Posts & Telecommun, Beijing Key Lab Intelligent Telecommun Software &, Beijing 100876, Peoples R China.
EM lp864172708@bupt.edu.cn; fhy@bupt.edu.cn; mhd@bupt.edu.cn
FU National Natural Science Foundation of China [61872047, 61720106007];
   National Key R&D Program of China [2017YFB1003000]; Beijing Nova Program
   [Z201100006820124]; Beijing Natural Science Foundation [L191004]; 111
   Project [B18008]
FX This work was supported in part by the National Natural Science
   Foundation of China (61872047, 61720106007), the National Key R&D
   Program of China (2017YFB1003000), the Beijing Nova Program
   (Z201100006820124), the Beijing Natural Science Foundation (L191004),
   and the 111 Project (B18008).
CR [Anonymous], 2018, ARXIV180510652
   Bengio S, 2016, ARXIV
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Carrara F, 2017, PROCEEDINGS OF THE 15TH INTERNATIONAL WORKSHOP ON CONTENT-BASED MULTIMEDIA INDEXING (CBMI), DOI 10.1145/3095713.3095753
   Dong Z, 2014, INT C PATT RECOG, P172, DOI 10.1109/ICPR.2014.39
   Feinman R., 2017, ARXIV170300410
   Fu HY, 2020, NEUROCOMPUTING, V395, P178, DOI 10.1016/j.neucom.2018.02.111
   Goodfellow I. J., 2015, 3 INT C LEARNING REP
   Guo F, 2019, INFORM SCIENCES, V501, P182, DOI 10.1016/j.ins.2019.05.084
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Howard A. G., 2017, PREPRINT
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liang B, 2021, IEEE T DEPEND SECURE, V18, P72, DOI 10.1109/TDSC.2018.2874243
   Liao FZ, 2018, PROC CVPR IEEE, P1778, DOI 10.1109/CVPR.2018.00191
   Liu XM, 2016, AER ADV ENG RES, V116, P1, DOI 10.1145/2875194.2875248
   Liu Y., 2016, ARXIV161102770
   Madry A., 2018, ARXIV
   Metzen J. H., 2017, P INT C LEARN REPR T
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Mustafa A, 2020, IEEE T IMAGE PROCESS, V29, P1711, DOI 10.1109/TIP.2019.2940533
   Oh M, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9010158
   Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41
   Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36
   Prakash A, 2018, PROC CVPR IEEE, P8571, DOI 10.1109/CVPR.2018.00894
   Rakin AS, 2019, IEEE COMP SOC ANN, P333, DOI 10.1109/ISVLSI.2019.00067
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Samangouei P., 2018, DefenseGAN: Protecting classifiers against adversarial attacks using generative models. In International Conference on Learning Representations (ICLR) 2018, Vancouver
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Vincent E, 2008, INT CONF ACOUST SPEE, P109, DOI 10.1109/ICASSP.2008.4517558
   Won M, 2020, IEEE ACCESS, V8, P73340, DOI 10.1109/ACCESS.2020.2987634
   Xie CH, 2019, PROC CVPR IEEE, P501, DOI 10.1109/CVPR.2019.00059
   Yang LJ, 2015, PROC CVPR IEEE, P3973, DOI 10.1109/CVPR.2015.7299023
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhuo L, 2017, MACH VISION APPL, V28, P793, DOI 10.1007/s00138-017-0846-2
NR 35
TC 11
Z9 11
U1 2
U2 17
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2021
VL 7
IS 2
BP 217
EP 227
DI 10.1007/s41095-021-0202-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FR
UT WOS:000648692900005
OA gold
DA 2024-07-18
ER

PT J
AU Han, SY
   Ye, SJ
   Zhang, HX
AF Han, Songye
   Ye, Shaojie
   Zhang, Hongxin
TI Visual exploration of Internet news via sentiment score and topic models
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE Internet news visualization; sentiment score; topic models; event
   detection
ID PREDICTION
AB Analyzing and understanding Internet news are important for many applications, such as market sentiment investigation and crisis management. However, it is challenging for users to interpret a massive amount of unstructured text, to dig out its accurate meaning, and to spot noteworthy news events. To overcome these challenges, we propose a novel visualization-driven approach for analyzing news text. We first collect Internet news from different sources and encode sentences into a vector representation suitable for input to a neural network, which calculates a sentiment score, to help detect news event patterns. A subsequent interactive visualization framework allows the user to explore the development of and relationships between Internet news topics. In addition, a method for detecting news events enables users and domain experts to interactively explore the correlations between market sentiment, topic distribution, and event patterns. We use this framework to provide a web-based interactive visualization system. We demonstrate the applicability and effectiveness of our proposed system using case studies involving blockchain news.
C1 [Han, Songye; Ye, Shaojie] Zhejiang Univ, Hangzhou, Peoples R China.
   [Zhang, Hongxin] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Zhang, HX (corresponding author), Zhejiang Univ, Hangzhou, Peoples R China.
EM 3160102019@zju.edu.cn; JaYE@zju.edu.cn; zhx@cad.zju.edu.cn
RI Zhang, Hongxin/T-3714-2019
FU National Key Research and Development Project of China [2017YFC0804401];
   National Natural Science Foundation of China [U1909204]
FX This work was supported by the National Key Research and Development
   Project of China (No. 2017YFC0804401) and the National Natural Science
   Foundation of China (No. U1909204). The work was supported by Prof. Wei
   Chen, who provided suggestions on how to build an interactive
   visualization system, and Mrs. Liyan Liu, who provided valuable ideas on
   how to conduct standardized tests.
CR [Anonymous], 1992, Stocks Commodities, DOI DOI 10.1111/J.1365-2044.1992.TB03213.X
   [Anonymous], 1987, MATH ITS APPL, DOI DOI 10.1007/978-94-015-7744-1_2
   [Anonymous], 2009, Visual Analytics Science and Technology
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Chen CM, 2006, IEEE CONF VIS ANAL, P59
   Devlin J., 2018, BERT PRE TRAINING DE
   Di Battista G, 2015, IEEE SYM VIS CYB SEC
   Dörk M, 2010, IEEE T VIS COMPUT GR, V16, P1129, DOI 10.1109/TVCG.2010.129
   Doumit S., 2011, P ICCS 11, P251
   Eubank R.L., 1999, Nonparametric regression and spline smoothing
   Fernando T, 2018, NEURAL NETWORKS, V108, P466, DOI 10.1016/j.neunet.2018.09.002
   Gers FA, 2000, NEURAL COMPUT, V12, P2451, DOI 10.1162/089976600300015015
   Goldberg Y., 2014, ARXIV PREPRINT ARXIV, V1402, P3722
   Hoffman M., 2010, P ADV NEUR INF PROC, V23:856-864
   KAILATH T, 1968, IEEE T AUTOMAT CONTR, VAC13, P655, DOI 10.1109/TAC.1968.1099019
   Liu B., 2005, Proceedings of 14th International Conference of World Wide Web, P342
   Liu X, 2013, LECT NOTES COMPUTER, V7812, P321
   MARCHAND P, 1983, REV SCI INSTRUM, V54, P1034, DOI 10.1063/1.1137498
   McGinn D, 2018, ROY SOC OPEN SCI, V5, DOI 10.1098/rsos.180298
   Morinaga S., 2002, P 8 ACM SIGKDD INT C, P341, DOI [DOI 10.1145/775047.775098, 10.1145/775047.775098]
   Mousa AED, 2017, 15TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (EACL 2017), VOL 1: LONG PAPERS, P1023
   Nakamoto S., 2008, DECENT BUS REV, V21260, DOI https://bitcoin.org/bitcoin.pdf
   Ranshous Stephen, 2017, Financial Cryptography and Data Security. FC 2017 International Workshops WAHC, BITCOIN, VOTING, WTSC, and TA. Revised Selected Papers: LNCS 10323, P248, DOI 10.1007/978-3-319-70278-0_16
   Reuter T., 2013, P MED MULT BENCHM WO
   Sievert C., 2014, P WORKSH INT LANG LE, P63, DOI [10.3115/v1/W14-3110, DOI 10.3115/V1/W14-3110]
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wu YC, 2018, IEEE T VIS COMPUT GR, V24, P2758, DOI 10.1109/TVCG.2017.2764459
   Wu YC, 2014, IEEE T VIS COMPUT GR, V20, P1763, DOI 10.1109/TVCG.2014.2346920
   Wu YC, 2010, IEEE T VIS COMPUT GR, V16, P1109, DOI 10.1109/TVCG.2010.183
   Yang Z., 2016, P 2016 C N AM CHAPT, P1480, DOI [DOI 10.18653/V1/N16-1174, 10.18653/v1/n16-1174]
   Yuan NJ, 2015, IEEE T KNOWL DATA EN, V27, P712, DOI 10.1109/TKDE.2014.2345405
   Yue XW, 2019, IEEE T VIS COMPUT GR, V25, P162, DOI 10.1109/TVCG.2018.2864814
   Zhao J, 2014, IEEE T VIS COMPUT GR, V20, P1773, DOI 10.1109/TVCG.2014.2346922
   Zhu MF, 2019, IEEE T INTELL TRANSP, V20, P3981, DOI 10.1109/TITS.2019.2901117
NR 34
TC 3
Z9 3
U1 0
U2 11
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2020
VL 6
IS 3
BP 333
EP 347
DI 10.1007/s41095-020-0178-4
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FH
UT WOS:000648691900008
OA gold
DA 2024-07-18
ER

PT J
AU Wu, XC
   Zhou, BY
   Ren, QY
   Guo, W
AF Wu, Xiaoce
   Zhou, Bingyin
   Ren, Qingyun
   Guo, Wei
TI Multispectral image denoising using sparse and graph Laplacian Tucker
   decomposition
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE denoising; tensor decomposition; sparsity; graph Laplacian;
   multispectral image
ID HYPERSPECTRAL IMAGES; TENSOR DECOMPOSITIONS; OPTIMIZATION; REDUCTION;
   ALGORITHM
AB Multispectral image denoising is a basic problem whose results affect subsequent processes such as target detection and classification. Numerous approaches have been proposed, but there are still many challenges, particularly in using prior knowledge of multispectral images, which is crucial for solving the ill-posed problem of noise removal. This paper considers both non-local self-similarity in space and global correlation in spectrum. We propose a novel low-rank Tucker decomposition model for removing the noise, in which sparse and graph Laplacian regularization terms are employed to encode this prior knowledge. It can jointly learn a sparse and low-rank representation while preserving the local geometrical structure between spectral bands, so as to better capture simultaneously the correlation in spatial and spectral directions. We adopt the alternating direction method of multipliers to solve the resulting problem. Experiments demonstrate that the proposed method outperforms the state-of-the-art, such as cube-based and tensor-based methods, both quantitatively and qualitatively.
C1 [Wu, Xiaoce; Zhou, Bingyin; Ren, Qingyun; Guo, Wei] Hebei Normal Univ, Sch Math Sci, Key Lab Augmented Real, Shijiazhuang 050024, Hebei, Peoples R China.
C3 Hebei Normal University
RP Zhou, BY (corresponding author), Hebei Normal Univ, Sch Math Sci, Key Lab Augmented Real, Shijiazhuang 050024, Hebei, Peoples R China.
EM zhoubingyin@163.com
RI Zhou, Bingyin/O-4327-2017
OI Zhou, Bingyin/0000-0003-0190-7520
FU Science and Technology Foundation of Higher Education in Hebei Province
   [QN2019166]; Science Foundation of Hebei Normal University [L2019K01];
   National Natural Science Foundation of China [11301137, 61802109]
FX The authors would like to thank the anonymous reviewers for their
   valuable comments. The work was sponsored by the Science and Technology
   Foundation of Higher Education in Hebei Province (QN2019166), the
   Science Foundation of Hebei Normal University (L2019K01), and the
   National Natural Science Foundation of China (11301137, 61802109).
CR Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1
   Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], 2002, DATA FUSION DEFINITI
   Bader B. W., 2015, MATLAB TENSOR TOOLBO
   Boumal N, 2014, J MACH LEARN RES, V15, P1455
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Chang Y, 2019, IEEE T GEOSCI REMOTE, V57, P667, DOI 10.1109/TGRS.2018.2859203
   Chen GY, 2011, IEEE T GEOSCI REMOTE, V49, P973, DOI 10.1109/TGRS.2010.2075937
   Chen YL, 2014, IEEE T PATTERN ANAL, V36, P577, DOI 10.1109/TPAMI.2013.164
   Cichocki A, 2015, IEEE SIGNAL PROC MAG, V32, P145, DOI 10.1109/MSP.2013.2297439
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dong WS, 2015, IEEE I CONF COMP VIS, P442, DOI 10.1109/ICCV.2015.58
   DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009
   Fauvel M, 2013, P IEEE, V101, P652, DOI 10.1109/JPROC.2012.2197589
   Fu Y, 2016, NEUROCOMPUTING, V195, P30, DOI 10.1016/j.neucom.2015.09.125
   Gao B, 2018, SIAM J OPTIMIZ, V28, P302, DOI 10.1137/16M1098759
   Gong SQ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENT RAIL TRANSPORTATION (ICIRT), P37, DOI 10.1109/ICIRT.2013.6696264
   Hitchcock F. L., 1927, J MATH PHYS, V6, P189, DOI [10.1002/sapm192761164, DOI 10.1002/SAPM192761164]
   Karami A, 2011, IEEE J-STSP, V5, P487, DOI 10.1109/JSTSP.2011.2132692
   Kilmer ME, 2011, LINEAR ALGEBRA APPL, V435, P641, DOI 10.1016/j.laa.2010.09.020
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Letexier D, 2008, IEEE T GEOSCI REMOTE, V46, P2061, DOI 10.1109/TGRS.2008.916641
   Li C, 2015, J OPT SOC AM A, V32, P1604, DOI 10.1364/JOSAA.32.001604
   Li XT, 2017, IEEE T NEUR NET LEAR, V28, P1787, DOI 10.1109/TNNLS.2016.2545400
   Liu J, 2013, IEEE T PATTERN ANAL, V35, P208, DOI 10.1109/TPAMI.2012.39
   Liu XF, 2012, IEEE T GEOSCI REMOTE, V50, P3717, DOI 10.1109/TGRS.2012.2187063
   Lu CY, 2015, AAAI CONF ARTIF INTE, P1805
   Maggioni M, 2013, IEEE T IMAGE PROCESS, V22, P119, DOI 10.1109/TIP.2012.2210725
   Manjón JV, 2010, J MAGN RESON IMAGING, V31, P192, DOI 10.1002/jmri.22003
   MIRSKY L, 1975, MONATSH MATH, V79, P303, DOI 10.1007/BF01647331
   Narita A, 2012, DATA MIN KNOWL DISC, V25, P298, DOI 10.1007/s10618-012-0280-z
   Peng Y, 2014, PROC CVPR IEEE, P2949, DOI 10.1109/CVPR.2014.377
   Rasti B, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10030482
   Renard N, 2008, IEEE GEOSCI REMOTE S, V5, P138, DOI 10.1109/LGRS.2008.915736
   Sun WW, 2018, IEEE T GEOSCI REMOTE, V56, P3185, DOI 10.1109/TGRS.2018.2794443
   TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464
   Uzkent B, 2019, IEEE T GEOSCI REMOTE, V57, P449, DOI 10.1109/TGRS.2018.2856370
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie Q, 2018, IEEE T PATTERN ANAL, V40, P1888, DOI 10.1109/TPAMI.2017.2734888
   Xie Q, 2016, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2016.187
   Yasuma F, 2010, IEEE T IMAGE PROCESS, V19, P2241, DOI 10.1109/TIP.2010.2046811
   Yuan Y, 2016, IEEE T CYBERNETICS, V46, P3123, DOI 10.1109/TCYB.2015.2497711
   Yuan Y, 2015, IEEE T GEOSCI REMOTE, V53, P3815, DOI 10.1109/TGRS.2014.2385082
   Yuhas R. H., 1993, JPL SUMMARIES 4 ANN, P205
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang ZM, 2014, PROC CVPR IEEE, P3842, DOI 10.1109/CVPR.2014.485
   Zhao Q, 2015, IEEE I CONF COMP VIS, P271, DOI 10.1109/ICCV.2015.39
NR 48
TC 8
Z9 8
U1 4
U2 16
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD SEP
PY 2020
VL 6
IS 3
BP 319
EP 331
DI 10.1007/s41095-020-0176-6
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FH
UT WOS:000648691900007
OA gold
DA 2024-07-18
ER

PT J
AU Awano, N
   Hayashi, Y
AF Awano, Naoyuki
   Hayashi, Yuki
TI Psychological potential field and human eye fixation on binary
   line-drawing images: A comparative experimental study
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE psychological potential field (PPF); eye fixation; eye-tracking; visual
   perception; line drawing
ID NATURAL SCENE CATEGORIZATION; HEATMAPS
AB Quantitatively evaluating the psychological and perceptual effects of objects is an important issue, but is difficult. In cognitive studies, the psychological potential field (PPF), which represents psychological intensities in vision and can be calculated by applying computational algorithms to digital images, may help with this issue. Although studies have reported using the PPF to evaluate psychological effects, such as impressions, detailed investigations on how the PPF represents psychological perception and its limitations have not yet been performed. Another relevant tool is the fixation map, which visualizes human eye fixations; this map is generated from actual measurements acquired by eye-tracking and does not represent psychological effects directly. Although the PPF and the fixation map are based on visual imaging, they have never been compared. In this paper, we do so for the first time, using psychological and perceptual properties of line-drawing images. The results demonstrate the difference between these methods, including their representation of different properties with respect to visual perception. Moreover, the similarity between the two methods highlights the possibility of assessing perceptual phenomena such as categorization and cognition of objects based on human vision.
C1 [Awano, Naoyuki] Osaka Univ Econ, Osaka 5338533, Japan.
   [Hayashi, Yuki] Osaka Prefecture Univ, Sakai, Osaka 5998531, Japan.
C3 Osaka Metropolitan University
RP Awano, N (corresponding author), Osaka Univ Econ, Osaka 5338533, Japan.
EM awanonaoyuki@gmail.com
OI Awano, Naoyuki/0000-0001-8327-0310
CR ACM, 2017, T COMPUTER HUMAN INT, V24, P5
   Aramaki E, 2013, INTERACTIVE INFORM A, V47, P51
   Awano N, 2018, IEICE T INF SYST, VE101D, P1648, DOI 10.1587/transinf.2018EDP7065
   Bojko A, 2009, LECT NOTES COMPUT SC, V5610, P30, DOI 10.1007/978-3-642-02574-7_4
   Borji A., 2015, P CVPR WORKSH FUT DA
   Borji A, 2013, IEEE I CONF COMP VIS, P921, DOI 10.1109/ICCV.2013.118
   Borji A, 2013, IEEE T IMAGE PROCESS, V22, P55, DOI 10.1109/TIP.2012.2210727
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Courtemanche F, 2018, MULTIMED TOOLS APPL, V77, P11547, DOI 10.1007/s11042-017-5091-1
   Fu QF, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.00210
   Fu XL, 2014, SCI CHINA INFORM SCI, V57, DOI 10.1007/s11432-013-4911-9
   Fukouzu Y., 1998, B JPN SOC SCI DES, V45, P75
   ICHIKAWA N, 1967, JPN J PSYCHOL, V38, P274, DOI 10.4992/jjpsy.38.274
   Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kaiser D, 2019, J PERS DISORD, V33, P671, DOI 10.1521/pedi_2019_33_363
   KAJI S, 1974, VISION RES, V14, P113, DOI 10.1016/0042-6989(74)90124-2
   Kenneth HolmqvistMarcus Nystrom., 2011, Eye Tracking: A Comprehensive Guide to Methods and Measures
   Kümmerer M, 2015, P NATL ACAD SCI USA, V112, P16054, DOI 10.1073/pnas.1510393112
   Liu YJ, 2016, FRONT COMPUT SCI-CHI, V10, P216, DOI 10.1007/s11704-015-4450-1
   Lutteroth C, 2015, UIST'15: PROCEEDINGS OF THE 28TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P385, DOI 10.1145/2807442.2807461
   Majaranta P., 2014, ADV PHYSL COMPUTING, P39, DOI DOI 10.1007/978-1-4471-6392-3_3
   Miyoshi M., 2001, P 9 INT C HCI INT 20, V1, P1348
   Miyoshi M, 2005, P INT C HUM COMP INT
   Nagaishi M., 1993, Proceedings of the Second International Conference on Document Analysis and Recognition (Cat. No.93TH0578-5), P926, DOI 10.1109/ICDAR.1993.395585
   Onaga H., 1996, B JPN SOC SCI DES, V43, P77
   Pan JT, 2016, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2016.71
   Peelen MV, 2009, NATURE, V460, P94, DOI 10.1038/nature08103
   Peters RJ, 2005, VISION RES, V45, P2397, DOI 10.1016/j.visres.2005.03.019
   Riche N, 2013, IEEE I CONF COMP VIS, P1153, DOI 10.1109/ICCV.2013.147
   RODGERS JL, 1988, AM STAT, V42, P59, DOI 10.2307/2685263
   Sammaknejad N, 2017, ADV COGN PSYCHOL, V13, P232, DOI 10.5709/acp-0223-1
   Tavakoli HR, 2017, PROC CVPR IEEE, P6354, DOI 10.1109/CVPR.2017.673
   Walther DB, 2011, P NATL ACAD SCI USA, V108, P9661, DOI 10.1073/pnas.1015666108
   YOKOSE Z, 1970, JPN PSYCHOL RES, V12, P18, DOI 10.4992/psycholres1954.12.18
   Yu MJ, 2016, NEUROCOMPUTING, V173, P2041, DOI 10.1016/j.neucom.2015.09.046
   Zhu WN, 2016, FRONT HUM NEUROSCI, V10, DOI 10.3389/fnhum.2016.00513
   Zhu WN, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0075816
NR 38
TC 1
Z9 1
U1 0
U2 3
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2020
VL 6
IS 2
BP 205
EP 214
DI 10.1007/s41095-020-0169-5
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FB
UT WOS:000648691300007
OA gold
DA 2024-07-18
ER

PT J
AU Alqazzaz, S
   Sun, XF
   Yang, X
   Nokes, L
AF Alqazzaz, Salma
   Sun, Xianfang
   Yang, Xin
   Nokes, Len
TI Automated brain tumor segmentation on multi-modal MR image using SegNet
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE brain tumor segmentation; multi-modal MRI; convolutional neural
   networks; fully convolutional networks; decision tree
AB The potential of improving disease detection and treatment planning comes with accurate and fully automatic algorithms for brain tumor segmentation. Glioma, a type of brain tumor, can appear at different locations with different shapes and sizes. Manual segmentation of brain tumor regions is not only time-consuming but also prone to human error, and its performance depends on pathologists' experience. In this paper, we tackle this problem by applying a fully convolutional neural network SegNet to 3D data sets for four MRI modalities (Flair, T1, T1ce, and T2) for automated segmentation of brain tumor and subtumor parts, including necrosis, edema, and enhancing tumor. To further improve tumor segmentation, the four separately trained SegNet models are integrated by post-processing to produce four maximum feature maps by fusing the machine-learned feature maps from the fully convolutional layers of each trained model. The maximum feature maps and the pixel intensity values of the original MRI modalities are combined to encode interesting information into a feature representation. Taking the combined feature as input, a decision tree (DT) is used to classify the MRI voxels into different tumor parts and healthy brain tissue. Evaluating the proposed algorithm on the dataset provided by the Brain Tumor Segmentation 2017 (BraTS 2017) challenge, we achieved F-measure scores of 0.85, 0.81, and 0.79 for whole tumor, tumor core, and enhancing tumor, respectively.Experimental results demonstrate that using SegNet models with 3D MRI datasets and integrating the four maximum feature maps with pixel intensity values of the original MRI modalities has potential to perform well on brain tumor segmentation.
C1 [Alqazzaz, Salma; Yang, Xin; Nokes, Len] Cardiff Univ, Sch Engn, Cardiff CF24 3AA, Wales.
   [Alqazzaz, Salma] Baghdad Univ, Coll Sci Women, Dept Phys, Baghdad, Iraq.
   [Sun, Xianfang] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF24 3AA, Wales.
C3 Cardiff University; University of Baghdad; Cardiff University
RP Alqazzaz, S (corresponding author), Cardiff Univ, Sch Engn, Cardiff CF24 3AA, Wales.; Alqazzaz, S (corresponding author), Baghdad Univ, Coll Sci Women, Dept Phys, Baghdad, Iraq.
EM Al-QazzazSA@cardiff.ac.uk; SunX2@cardiff.ac.uk; YangX26@cardiff.ac.uk;
   Nokes@cardiff.ac.uk
RI Sun, Xianfang/ABG-8970-2021
OI Yang, Xin/0000-0002-8429-7598
CR [Anonymous], 2016, J BIOMEDICAL ENG MED, DOI DOI 10.14738/jbemi.31.1696
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bakas S, 2017, SCI DATA, V4, DOI 10.1038/sdata.2017.117
   Bakas Spyridon, 2016, Brainlesion, V9556, P144, DOI [10.1007/978-3-319-30858-6_13, 10.1007/978-3-319-30858-6_1]
   Bauer S, 2013, PHYS MED BIOL, V58, pR97, DOI 10.1088/0031-9155/58/13/R97
   Casamitjana A, 2018, LECT NOTES COMPUT SC, V10670, P381, DOI 10.1007/978-3-319-75238-9_33
   Chang PD, 2016, LECT NOTES COMPUT SC, V10154, P108, DOI 10.1007/978-3-319-55524-9_11
   Cicek Ozgun, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P424, DOI 10.1007/978-3-319-46723-8_49
   Drozdzal M, 2016, LECT NOTES COMPUT SC, V10008, P179, DOI 10.1007/978-3-319-46976-8_19
   Gooya A, 2011, LECT NOTES COMPUT SC, V6892, P532, DOI 10.1007/978-3-642-23629-7_65
   Havaei M, 2017, MED IMAGE ANAL, V35, P18, DOI 10.1016/j.media.2016.05.004
   Juan-Albarracín J, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0125143
   Kamnitsas K, 2018, LECT NOTES COMPUT SC, V10670, P450, DOI 10.1007/978-3-319-75238-9_38
   Kamnitsas K, 2016, LECT NOTES COMPUT SC, V10154, P138, DOI 10.1007/978-3-319-55524-9_14
   Kamnitsas K, 2017, MED IMAGE ANAL, V36, P61, DOI 10.1016/j.media.2016.10.004
   Lai M., 2015, DEEP LEARNING MED IM
   Louis DN, 2016, ACTA NEUROPATHOL, V131, P803, DOI 10.1007/s00401-016-1545-1
   Menze BH, 2015, IEEE T MED IMAGING, V34, P1993, DOI 10.1109/TMI.2014.2377694
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Pereira S, 2016, IEEE T MED IMAGING, V35, P1240, DOI 10.1109/TMI.2016.2538465
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Tustison NJ, 2015, NEUROINFORMATICS, V13, P209, DOI 10.1007/s12021-014-9245-2
   Tustison NJ, 2010, IEEE T MED IMAGING, V29, P1310, DOI 10.1109/TMI.2010.2046908
   Zikic D, 2012, LECT NOTES COMPUT SC, V7512, P369, DOI 10.1007/978-3-642-33454-2_46
NR 25
TC 91
Z9 96
U1 1
U2 21
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2019
VL 5
IS 2
BP 209
EP 219
DI 10.1007/s41095-019-0139-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UM
UT WOS:000648690800006
OA gold
DA 2024-07-18
ER

PT J
AU Lueangwattana, C
   Mori, S
   Saito, H
AF Lueangwattana, Chanya
   Mori, Shohei
   Saito, Hideo
TI Removing fences from sweep motion videos using global 3D reconstruction
   and fence-aware light field rendering
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE video; fence; video repair; diminished reality (DR); structure from
   motion (SfM); light field rendering (LFR)
AB Diminishing the appearance of a fence in an image is a challenging research area due to the characteristics of fences (thinness, lack of texture, etc.) and the need for occluded background restoration. In this paper, we describe a fence removal method for an image sequence captured by a user making a sweep motion, in which occluded background is potentially observed. To make use of geometric and appearance information such as consecutive images, we use two well-known approaches: structure from motion and light field rendering. Results using real image sequences show that our method can stably segment fences and preserve background details for various fence and background combinations. A new video without the fence, with frame coherence, can be successfully provided.
C1 [Lueangwattana, Chanya; Saito, Hideo] Keio Univ, Dept Sci & Technol, Tokyo, Japan.
   [Mori, Shohei] Graz Univ Technol, Inst Comp Graph & Vis, Graz, Austria.
C3 Keio University; Graz University of Technology
RP Mori, S (corresponding author), Graz Univ Technol, Inst Comp Graph & Vis, Graz, Austria.
EM chanyal@hvrl.ics.keio.ac.jp; s.mori.jp@ieee.org;
   saito@hvrl.ics.keio.ac.jp
RI Saito, Hideo/D-6223-2014; Mori, Shohei/AAL-6642-2020
OI Saito, Hideo/0000-0002-2421-9862; Mori, Shohei/0000-0003-0540-7312
FU Japan Society for the Promotion of Science [16J05114]; Grants-in-Aid for
   Scientific Research [16J05114] Funding Source: KAKEN
FX This work was supported in part by Grant-in-Aid from the Japan Society
   for the Promotion of Science, following Grant No. 16J05114.
CR [Anonymous], 2011, LECT NOTES COMPUTER
   Barnes Connelly, 2017, [Computational Visual Media, 计算可视媒体], V3, P3
   Barnes C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766934
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Buehler C, 2001, COMP GRAPH, P425, DOI 10.1145/383259.383309
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Davis A, 2012, COMPUT GRAPH FORUM, V31, P305, DOI 10.1111/j.1467-8659.2012.03009.x
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Goldstein T, 2009, SIAM J IMAGING SCI, V2, P323, DOI 10.1137/080725891
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   Isaksen A, 2000, COMP GRAPH, P297, DOI 10.1145/344779.344929
   Jonna S., 2015, 2015 8 INT C ADV PAT, P1
   Jonna S, 2017, INT CONF ACOUST SPEE, P1792, DOI 10.1109/ICASSP.2017.7952465
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Khasare VS, 2013, IEEE IMAGE PROC, P1351, DOI 10.1109/ICIP.2013.6738278
   Kusumoto N, 2009, PROC CVPR IEEE, P2544
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Liu Y, 2008, PROC CVPR IEEE, P2008
   Mori S., 2017, IPSJ Transactions on Computer Vision and Applications, V9, P17, DOI [10.1186/s41074-017-0028-1, DOI 10.1186/S41074-017-0028-1, 10.3390/electronics10080900]
   Mu YD, 2014, IEEE T CIRC SYST VID, V24, P1111, DOI 10.1109/TCSVT.2013.2241351
   Negi CS, 2014, IEEE INT CON MULTI, P1, DOI 10.1109/ICMEW.2014.6890641
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Schönberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31
   Xue TF, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766940
   Yamashita A., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P4532, DOI 10.1109/ICPR.2010.1101
   Yi RJ, 2016, PROC CVPR IEEE, P705, DOI 10.1109/CVPR.2016.83
   Zhang FL, 2017, IEEE T VIS COMPUT GR, V23, P1561, DOI 10.1109/TVCG.2016.2532329
   Zhang QM, 2016, SPRINGER SER MATER S, V229, P1, DOI 10.1007/978-3-319-24990-2_1
NR 29
TC 1
Z9 1
U1 0
U2 2
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2019
VL 5
IS 1
BP 21
EP 32
DI 10.1007/s41095-018-0126-8
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UL
UT WOS:000648690700003
OA gold
DA 2024-07-18
ER

PT J
AU Lin, Z
   Zhang, Z
   Zhu, ZY
   Fan, DP
   Liu, XL
AF Lin, Zheng
   Zhang, Zhao
   Zhu, Zi-Yue
   Fan, Deng-Ping
   Liu, Xia-Lei
TI Sequential interactive image segmentation
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE interactive segmentation; user interaction; object segmentation
AB Interactive image segmentation (IIS) is an important technique for obtaining pixel-level annotations. In many cases, target objects share similar semantics. However, IIS methods neglect this connection and in particular the cues provided by representations of previously segmented objects, previous user interaction, and previous prediction masks, which can all provide suitable priors for the current annotation. In this paper, we formulate a sequential interactive image segmentation (SIIS) task for minimizing user interaction when segmenting sequences of related images, and we provide a practical approach to this task using two pertinent designs. The first is a novel interaction mode. When annotating a new sample, our method can automatically propose an initial click proposal based on previous annotation. This dramatically helps to reduce the interaction burden on the user. The second is an online optimization strategy, with the goal of providing semantic information when annotating specific targets, optimizing the model with dense supervision from previously labeled samples. Experiments demonstrate the effectiveness of regarding SIIS as a particular task, and our methods for addressing it.
C1 [Lin, Zheng; Zhang, Zhao; Zhu, Zi-Yue; Liu, Xia-Lei] Nankai Univ, Coll Comp Sci, TKLNDST, Tianjin, Peoples R China.
   [Fan, Deng-Ping] Swiss Fed Inst Technol, Comp Vis Lab, Zurich, Switzerland.
C3 Nankai University; Swiss Federal Institutes of Technology Domain; ETH
   Zurich
RP Fan, DP (corresponding author), Swiss Fed Inst Technol, Comp Vis Lab, Zurich, Switzerland.
EM azer.linzheng@mail.nankai.edu.cn; zzhang@mail.nankai.edu.cn;
   zhuziyue@mail.nankai.edu.cn; dengpfan@gmail.com; xialei@nankai.edu.cn
RI Fan, Deng-Ping/ABD-4052-2020
OI Fan, Deng-Ping/0000-0002-5245-7518
CR Bai JJ, 2014, PROC CVPR IEEE, P392, DOI 10.1109/CVPR.2014.57
   Bai X, 2009, INT J COMPUT VISION, V82, P113, DOI 10.1007/s11263-008-0191-z
   Benenson R, 2019, PROC CVPR IEEE, P11692, DOI 10.1109/CVPR.2019.01197
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Castrejón L, 2017, PROC CVPR IEEE, P4485, DOI 10.1109/CVPR.2017.477
   Cermelli F., 2020, P IEEE CVF C COMP VI, P9230, DOI DOI 10.1109/CVPR42600.2020.00925
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   David Acuna, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P859, DOI 10.1109/CVPR.2018.00096
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P4339, DOI 10.1109/TPAMI.2021.3060412
   Fan DP, 2020, PROC CVPR IEEE, P2916, DOI 10.1109/CVPR42600.2020.00299
   Gong LX, 2022, COMPUT VIS MEDIA, V8, P165, DOI 10.1007/s41095-021-0235-7
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   Gulshan V, 2010, PROC CVPR IEEE, P3129, DOI 10.1109/CVPR.2010.5540073
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu Y, 2019, NEURAL NETWORKS, V109, P31, DOI 10.1016/j.neunet.2018.10.009
   Jain SD, 2019, INT J COMPUT VISION, V127, P1321, DOI 10.1007/s11263-019-01184-2
   Jang WD, 2019, PROC CVPR IEEE, P5292, DOI 10.1109/CVPR.2019.00544
   Jian M, 2016, IEEE T IMAGE PROCESS, V25, P1301, DOI 10.1109/TIP.2016.2518480
   Kim TH, 2008, LECT NOTES COMPUT SC, V5304, P264
   Kim TH, 2010, PROC CVPR IEEE, P3201, DOI 10.1109/CVPR.2010.5540078
   Kontogianni T., 2020, EUR C COMP VIS, P579
   Le H, 2018, LECT NOTES COMPUT SC, V11218, P20, DOI 10.1007/978-3-030-01264-9_2
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Li ZW, 2018, PROC CVPR IEEE, P577, DOI 10.1109/CVPR.2018.00067
   Liew JH, 2019, IEEE I CONF COMP VIS, P662, DOI 10.1109/ICCV.2019.00075
   Liew JH, 2017, IEEE I CONF COMP VIS, P2746, DOI 10.1109/ICCV.2017.297
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin Z, 2022, PROC CVPR IEEE, P2627, DOI 10.1109/CVPR52688.2022.00266
   Ling H, 2019, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR.2019.00540
   Mahadevan S., 2018, ARXIV
   Majumder S, 2019, PROC CVPR IEEE, P11594, DOI 10.1109/CVPR.2019.01187
   Majumder Soumajit, 2020, BMVC
   Maninis KK, 2018, PROC CVPR IEEE, P616, DOI 10.1109/CVPR.2018.00071
   Menglin Jia, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P316, DOI 10.1007/978-3-030-58452-8_19
   Mortensen E. N., 1995, P 22 ANN C COMP GRAP, P191, DOI DOI 10.1145/218380.218442
   Paszke A, 2019, ADV NEUR IN, V32
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Shiyin Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12231, DOI 10.1109/CVPR42600.2020.01225
   Sofiiuk Konstantin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8620, DOI 10.1109/CVPR42600.2020.00865
   Song G, 2018, PROC CVPR IEEE, P1760, DOI 10.1109/CVPR.2018.00189
   Vezhnevets V., 2005, Proc. Graphicon, V1, P150
   Wang J., 2009, P BRIT MACHINE VISIO
   Wang T, 2019, IEEE T IMAGE PROCESS, V28, P330, DOI 10.1109/TIP.2018.2867941
   Wu JJ, 2014, PROC CVPR IEEE, P256, DOI 10.1109/CVPR.2014.40
   Xu N., 2017, P BRIT MACHINE VISIO
   Xu N, 2016, PROC CVPR IEEE, P373, DOI 10.1109/CVPR.2016.47
   Zhang CB, 2022, PROC CVPR IEEE, P7043, DOI 10.1109/CVPR52688.2022.00692
   Zhang XY, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2759-y
   Zhao Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P455, DOI 10.1007/978-3-030-58610-2_27
   Zheng Lin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13336, DOI 10.1109/CVPR42600.2020.01335
NR 54
TC 1
Z9 1
U1 4
U2 9
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2023
VL 9
IS 4
BP 753
EP 765
DI 10.1007/s41095-022-0302-8
EA JUL 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P1QC2
UT WOS:001033457700001
OA gold
DA 2024-07-18
ER

PT J
AU Chen, YD
   Wang, DL
   Chen, ZG
   Yang, ZX
   Wu, EH
AF Chen, Yadang
   Wang, Duolin
   Chen, Zhiguo
   Yang, Zhi-Xin
   Wu, Enhua
TI Global video object segmentation with spatial constraint module
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE video object segmentation; semantic segmentation; global context (GC)
   module; spatial constraint
AB We present a lightweight and efficient semi supervised video object segmentation network based on the space-time memory framework. To some extent, our method solves the two difficulties encountered in traditional video object segmentation: one is that the single frame calculation time is too long, and the other is that the current frame's segmentation should use more information from past frames. The algorithm uses a global context (GC) module to achieve highperformance, real-time segmentation. The GC module can effectively integrate multi-frame image information without increased memory and can process each frame in real time. Moreover, the prediction mask of the previous frame is helpful for the segmentation of the current frame, so we input it into a spatial constraint module (SCM), which constrains the areas of segments in the current frame. The SCM effectively alleviates mismatching of similar targets yet consumes few additional resources. We added a refinement module to the decoder to improve boundary segmentation. Our model achieves state-of-the-art results on various datasets, scoring 80.1% on YouTube-VOS 2018 and a J & F score of 78.0% on DAVIS 2017, while taking 0.05 s per frame on the DAVIS 2016 validation dataset.
C1 [Chen, Yadang; Wang, Duolin; Chen, Zhiguo] Nanjing Univ Informat Sci & Technol, Engn Res Ctr Digital Forens, Sch Comp & Software, Minist Educ, Nanjing 210044, Peoples R China.
   [Yang, Zhi-Xin] Univ Macau, Dept Electromech Engn, State Key Lab Internet Things Smart City, Macau 999078, Peoples R China.
   [Wu, Enhua] Univ Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100190, Peoples R China.
   [Wu, Enhua] Univ Macau, Fac Sci & Technol, Macau 999078, Peoples R China.
C3 Nanjing University of Information Science & Technology; University of
   Macau; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS; University of Macau
RP Wang, DL (corresponding author), Nanjing Univ Informat Sci & Technol, Engn Res Ctr Digital Forens, Sch Comp & Software, Minist Educ, Nanjing 210044, Peoples R China.
EM darren_wangl@foxmail.com
RI Chen, Jin/KBQ-0163-2024; Liu, xuefeng/IUP-1483-2023; chen,
   yue/JXW-9556-2024; wang, jiaqi/JSL-7112-2023; zhao,
   weiwei/JUU-6585-2023; yang, yy/KBR-1536-2024; he, xi/JXN-3817-2024; lin,
   yuan/JXL-9592-2024; wang, KiKi/JFZ-3334-2023; zhang, xu/JXX-7692-2024;
   Li, Huizhen/JPX-2563-2023; zhang, jiayue/JUF-0129-2023; Li,
   Ren/JVZ-9153-2024; Wang, Xingyu/JNE-0602-2023; Yang,
   Zhi-Xin/AAV-1335-2020; Zhang, Xiaoyu/JXR-6386-2024; Wang,
   zhenhua/KFA-8731-2024; Jing, Jing/JSK-6237-2023; yang,
   liu/JXX-5043-2024; Wang, Duo/JVO-2488-2024; Liu, Shaobo/JUU-5767-2023;
   zhang, ling/JXW-6931-2024; Zhang, yuxuan/JXM-9935-2024; FENG,
   X/JPL-4188-2023; Liu, qi/JZT-5038-2024; Li, Wenjuan/KDN-8450-2024;
   Zhang, Wenkai/JWO-2030-2024; li, lan/KCJ-5061-2024; zhang,
   wen/JXN-0191-2024
OI Chen, Jin/0009-0005-5844-635X; Li, Ren/0000-0002-2579-2580; Yang,
   Zhi-Xin/0000-0001-9151-7758; 
FU National Natural Science Foundation of China [61802197, 62072449,
   61632003]; Science and Technology Development Fund, Macau SAR
   [0018/2019/AKP, SKL-IOTSC(UM)-2021-2023]; Guangdong Science and
   Technology Department [2020B1515130001]; University of Macau
   [MYRG2020-00253-FST, MYRG2022-00059-FST]
FX This work was partially supported by the National Natural Science
   Foundation of China (Grant Nos. 61802197, 62072449, and 61632003), the
   Science and Technology Development Fund, Macau SAR (Grant Nos.
   0018/2019/AKP and SKL-IOTSC(UM)-2021-2023), the Guangdong Science and
   Technology Department (Grant No. 2020B1515130001), and University of
   Macau (Grant Nos. MYRG2020-00253-FST and MYRG2022-00059-FST).
CR Bao LC, 2018, PROC CVPR IEEE, P5977, DOI 10.1109/CVPR.2018.00626
   Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565
   Chen D, 2021, COMPUT VIS MEDIA, V7, P253, DOI 10.1007/s41095-021-0212-1
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen YH, 2018, PROC CVPR IEEE, P1189, DOI 10.1109/CVPR.2018.00130
   Cheng JC, 2018, PROC CVPR IEEE, P7415, DOI 10.1109/CVPR.2018.00774
   Danon D, 2021, COMPUT VIS MEDIA, V7, P453, DOI 10.1007/s41095-021-0216-x
   Hu L, 2021, PROC CVPR IEEE, P4142, DOI 10.1109/CVPR46437.2021.00413
   Hu YT, 2018, LECT NOTES COMPUT SC, V11212, P56, DOI 10.1007/978-3-030-01237-3_4
   Hu Yuan-Ting, 2017, NIPS, P324
   Huo Y, 2021, COMPUT VIS MEDIA, V7, P169, DOI 10.1007/s41095-021-0209-9
   Johnander J, 2019, PROC CVPR IEEE, P8945, DOI 10.1109/CVPR.2019.00916
   Cheng HK, 2021, Arxiv, DOI arXiv:2106.05210
   Khoreva A., 2017, P 2017 DAVIS CHALLEN
   Khoreva A, 2019, INT J COMPUT VISION, V127, P1175, DOI 10.1007/s11263-019-01164-6
   Kingma D. P., 2014, arXiv
   Li X., 2019, arXiv
   Li XX, 2018, LECT NOTES COMPUT SC, V11207, P93, DOI 10.1007/978-3-030-01219-9_6
   Li Y., 2019, P 16 IEEE INT C ADV, P1
   Liang Y., 2020, Adv. Neural Inf. Proces. Syst., V33, P3430
   Lin HJ, 2019, IEEE I CONF COMP VIS, P3948, DOI 10.1109/ICCV.2019.00405
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu P, 2021, COMPUT VIS MEDIA, V7, P217, DOI 10.1007/s41095-021-0202-3
   Liu Xueting, 2017, [Computational Visual Media, 计算可视媒体], V3, P61
   Luiten J, 2018, Arxiv, DOI arXiv:1807.09190
   Maninis KK, 2019, IEEE T PATTERN ANAL, V41, P1515, DOI 10.1109/TPAMI.2018.2838670
   Oh SW, 2018, PROC CVPR IEEE, P7376, DOI 10.1109/CVPR.2018.00770
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Perazzi F, 2017, PROC CVPR IEEE, P3491, DOI 10.1109/CVPR.2017.372
   Pont-Tuset J, 2018, Arxiv, DOI arXiv:1704.00675
   Seong H., 2020, EUR C COMP VIS, P629
   Seoung Wug Oh, 2019, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). Proceedings, P9225, DOI 10.1109/ICCV.2019.00932
   Tang LL, 2022, IEEE T CYBERNETICS, V52, P4949, DOI 10.1109/TCYB.2020.3025798
   Ventura C, 2019, PROC CVPR IEEE, P5272, DOI 10.1109/CVPR.2019.00542
   Voigtlaender P, 2019, Arxiv, DOI arXiv:1904.04552
   Voigtlaender P, 2017, Arxiv, DOI arXiv:1706.09364
   Voigtlaender P, 2019, PROC CVPR IEEE, P9473, DOI 10.1109/CVPR.2019.00971
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wang XL, 2019, PROC CVPR IEEE, P2561, DOI 10.1109/CVPR.2019.00267
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang ZQ, 2019, IEEE I CONF COMP VIS, P3977, DOI 10.1109/ICCV.2019.00408
   Wehrwein S., 2017, P BRIT MACH VIS C, DOI 10.5244/C.31.96
   Xie HZ, 2021, PROC CVPR IEEE, P1286, DOI 10.1109/CVPR46437.2021.00134
   Xu N, 2018, Arxiv, DOI arXiv:1809.03327
   Xu N, 2018, LECT NOTES COMPUT SC, V11209, P603, DOI 10.1007/978-3-030-01228-1_36
   Yang LJ, 2018, PROC CVPR IEEE, P6499, DOI 10.1109/CVPR.2018.00680
   Yang ZX, 2018, COGN COMPUT, V10, P908, DOI 10.1007/s12559-018-9598-1
   Yoon JS, 2017, IEEE I CONF COMP VIS, P2186, DOI 10.1109/ICCV.2017.238
   Yu Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P735, DOI 10.1007/978-3-030-58607-2_43
   Zhang B., 2020, P 2020 DAVIS CHALL V
   Zhang FLE, 2020, COMPUT VIS MEDIA, V6, P291, DOI 10.1007/s41095-020-0187-3
   Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019
NR 52
TC 0
Z9 0
U1 3
U2 17
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2023
VL 9
IS 2
BP 385
EP 400
DI 10.1007/s41095-022-0282-8
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7N8FL
UT WOS:000907570700011
OA gold
DA 2024-07-18
ER

PT J
AU Tang, YL
   Zhang, Y
   Han, XG
   Zhang, FL
   Lai, YK
   Tong, RF
AF Tang, Yanlong
   Zhang, Yun
   Han, Xiaoguang
   Zhang, Fang-Lue
   Lai, Yu-Kun
   Tong, Ruofeng
TI 3D corrective nose reconstruction from a single image
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE nose shape recovery; single image 3D reconstruction; contour
   correspondence; Laplacian deformation
ID FACE RECONSTRUCTION; RECOGNITION
AB There is a steadily growing range of applications that can benefit from facial reconstruction techniques, leading to an increasing demand for reconstruction of high-quality 3D face models. While it is an important expressive part of the human face, the nose has received less attention than other expressive regions in the face reconstruction literature. When applying existing reconstruction methods to facial images, the reconstructed nose models are often inconsistent with the desired shape and expression. In this paper, we propose a coarse-to-fine 3D nose reconstruction and correction pipeline to build a nose model from a single image, where 3D and 2D nose curve correspondences are adaptively updated and refined. We first correct the reconstruction result coarsely using constraints of 3D-2D sparse landmark correspondences, and then heuristically update a dense 3D-2D curve correspondence based on the coarsely corrected result. A final refinement step is performed to correct the shape based on the updated 3D-2D dense curve constraints. Experimental results show the advantages of our method for 3D nose reconstruction over existing methods.
C1 [Tang, Yanlong] Tencent Games Lightspeed & Quantum Studios, Shenzhen, Peoples R China.
   [Zhang, Yun] Commun Univ Zhejiang, Hangzhou, Peoples R China.
   [Han, Xiaoguang] Chinese Univ Hong Kong Shenzhen, Shenzhen Res Inst Big Data, Shenzhen, Peoples R China.
   [Zhang, Fang-Lue] Victoria Univ Wellington, Wellington, New Zealand.
   [Lai, Yu-Kun] Cardiff Univ, Cardiff, S Glam, Wales.
   [Tong, Ruofeng] Zhejiang Univ, Hangzhou, Peoples R China.
C3 Communication University of Zhejiang; The Chinese University of Hong
   Kong, Shenzhen; Shenzhen Research Institute of Big Data; Victoria
   University Wellington; Cardiff University; Zhejiang University
RP Zhang, Y (corresponding author), Commun Univ Zhejiang, Hangzhou, Peoples R China.
EM yanlongtang@gmail.com; zhangyun_zju@zju.edu.cn;
   hanxiaoguang@cuhk.edu.cn; fanglue.zhang@ecs.vuw.ac.nz;
   Yukun.Lai@cs.cardiff.ac.uk; trf@zju.edu.cn
RI Lai, Yu-Kun/D-2343-2010
FU National Natural Science Foundation of China [61972342, 61602402,
   61902334]; Zhejiang Provincial Basic Public Welfare Research
   [LGG19F020001]; Shenzhen Fundamental Research (General Project)
   [JCYJ20190814112007258]; Royal Society; IES [\R1\180126]
FX This research was supported by the National Natural Science Foundation
   of China (Grant Nos. 61972342, 61602402, and 61902334), Zhejiang
   Provincial Basic Public Welfare Research (Grant No. LGG19F020001),
   Shenzhen Fundamental Research (General Project) (Grant No.
   JCYJ20190814112007258), and the Royal Society (Grant No. IES\R1\180126).
CR [Anonymous], 2016, ACM T GRAPHIC, V35
   [Anonymous], 2013, ACM T GRAPHIC, V32
   [Anonymous], 2013, ACM T GRAPHIC, V32
   [Anonymous], 2004, P 2004 EUR ACM SIGGR
   [Anonymous], 2017, ACM T GRAPHIC, V36
   [Anonymous], 2016, ACM T GRAPHIC, V35
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Booth J, 2016, PROC CVPR IEEE, P5543, DOI 10.1109/CVPR.2016.598
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   Dinev D, 2018, COMPUT GRAPH FORUM, V37, P93, DOI 10.1111/cgf.13515
   Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33
   Feng ZH, 2018, IEEE INT CONF AUTOMA, P780, DOI 10.1109/FG.2018.00123
   Jiang L, 2018, IEEE T IMAGE PROCESS, V27, P4756, DOI 10.1109/TIP.2018.2845697
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Li Y, 2018, PROCEEDINGS CVMP 2018: THE 15TH ACM SIGGRAPH EUROPEAN CONFERENCE ON VISUAL MEDIA PRODUCTION, DOI 10.1145/3278471.3278473
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Samad MD, 2016, IEEE T HUM-MACH SYST, V46, P522, DOI 10.1109/THMS.2016.2515602
   Sanyal S, 2019, PROC CVPR IEEE, P7755, DOI 10.1109/CVPR.2019.00795
   Tang YL, 2019, VISUAL COMPUT, V35, P783, DOI 10.1007/s00371-019-01695-6
   Tang YL, 2016, VISUAL COMPUT, V32, P111, DOI 10.1007/s00371-014-1059-6
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Vallet B, 2008, COMPUT GRAPH FORUM, V27, P251, DOI 10.1111/j.1467-8659.2008.01122.x
   Werghi N, 2016, IEEE T INF FOREN SEC, V11, P964, DOI 10.1109/TIFS.2016.2515505
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
NR 27
TC 2
Z9 2
U1 4
U2 35
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2022
VL 8
IS 2
BP 225
EP 237
DI 10.1007/s41095-021-0237-5
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XJ0YS
UT WOS:000726525400004
OA gold
DA 2024-07-18
ER

PT J
AU Xu, YF
   Wei, HP
   Lin, MX
   Deng, YY
   Sheng, KK
   Zhang, MD
   Tang, F
   Dong, WM
   Huang, FY
   Xu, CS
AF Xu, Yifan
   Wei, Huapeng
   Lin, Minxuan
   Deng, Yingying
   Sheng, Kekai
   Zhang, Mengdan
   Tang, Fan
   Dong, Weiming
   Huang, Feiyue
   Xu, Changsheng
TI Transformers in computational visual media: A survey
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review
DE visual transformer; computational visual media (CVM); high-level vision;
   low-level vision; image generation; multi-modal learning
AB Transformers, the dominant architecture for natural language processing, have also recently attracted much attention from computational visual media researchers due to their capacity for long-range representation and high performance. Transformers are sequence-to-sequence models, which use a self-attention mechanism rather than the RNN sequential structure. Thus, such models can be trained in parallel and can represent global information. This study comprehensively surveys recent visual transformer works. We categorize them according to task scenario: backbone design, high-level vision, low-level vision and generation, and multimodal learning. Their key ideas are also analyzed. Differing from previous surveys, we mainly focus on visual transformer methods in low-level vision and generation. The latest works on backbone design are also reviewed in detail. For ease of understanding, we precisely describe the main contributions of the latest works in the form of tables. As well as giving quantitative comparisons, we also present image results for low-level vision and generation tasks. Computational costs and source code links for various important works are also given in this survey to assist further development.
C1 [Xu, Yifan; Lin, Minxuan; Deng, Yingying; Dong, Weiming; Xu, Changsheng] Chinese Acad Sci, Inst Automat, NLPR, Beijing 100190, Peoples R China.
   [Xu, Yifan; Lin, Minxuan; Deng, Yingying; Dong, Weiming; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100040, Peoples R China.
   [Wei, Huapeng; Tang, Fan] Jilin Univ, Sch Artificial Intelligence, Changchun 130012, Peoples R China.
   [Sheng, Kekai; Zhang, Mengdan; Huang, Feiyue] Tencent Inc, Youtu Lab, Shanghai 200233, Peoples R China.
   [Dong, Weiming; Xu, Changsheng] CASIA LLVISION Joint Lab, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Jilin University; Tencent
RP Dong, WM (corresponding author), Chinese Acad Sci, Inst Automat, NLPR, Beijing 100190, Peoples R China.; Dong, WM (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100040, Peoples R China.; Dong, WM (corresponding author), CASIA LLVISION Joint Lab, Beijing 100190, Peoples R China.
EM xuyifan2019@ia.ac.cn; weihp20@jlu.edu.cn; linminxuan2018@ia.ac.cn;
   dengyingying2017@ia.ac.cn; saulsheng@tencent.com;
   davinazhang@tencent.com; tangfan@jlu.edu.cn; weiming.dong@ia.ac.cn;
   garyhuang@tencent.com; changsheng.xu@ia.ac.cn
RI Tang, Fan/O-3923-2018; DONG, Weiming/AAG-7678-2020; xu,
   cj/HJZ-3488-2023; Xu, Chang/GQP-7280-2022; xu, ye/JYO-6282-2024; xu,
   yifan/GWZ-7154-2022
OI Tang, Fan/0000-0002-3975-2483; DONG, Weiming/0000-0001-6502-145X; xu,
   ye/0009-0007-9798-2723; Xu, Yifan/0000-0003-2467-888X
FU National Key R&D Program of China [2020AAA0106200]; National Natural
   Science Foundation of China [61832016, U20B2070]
FX We thank the anonymous reviewers for their valuable comments. This work
   was supported by National Key R&D Program of China under Grant No.
   2020AAA0106200, and by National Natural Science Foundation of China
   under Grant Nos. 61832016 and U20B2070.
CR Abnar S, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P4190
   Alberti C, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2131
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Ba J. L., 2016, LAYER NORMALIZATION, DOI DOI 10.48550/ARXIV.1607.06450
   Beal J., 2020, ARXIV PREPRINT ARXIV
   Bebis G., 1994, IEEE Potentials, V13, P27, DOI 10.1109/45.329294
   Binder A, 2016, LECT NOTES COMPUT SC, V9887, P63, DOI 10.1007/978-3-319-44781-0_8
   Cai Z., 2020, ARXIV PREPRINT ARXIV
   Cai ZW, 2021, IEEE T PATTERN ANAL, V43, P1483, DOI 10.1109/TPAMI.2019.2956516
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chefer H, 2021, PROC CVPR IEEE, P782, DOI 10.1109/CVPR46437.2021.00084
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen J., 2021, Transunet: transformers make strong encoders for medical image segmentation
   Chen M, 2020, PR MACH LEARN RES, V119
   Chen X, 2015, Microsoft coco captions: Data collection and evaluation server
   Chen X, 2016, ADV NEUR IN, V29
   Chen ZH, 2021, AAAI CONF ARTIF INTE, V35, P1132
   Choromanski K., 2021, INT C LEARN REPR ICL
   Chu X. X., 2021, ARXIV PREPRINT ARXIV
   d'Ascoli S, 2021, PR MACH LEARN RES, V139, DOI 10.1088/1742-5468/ac9830
   Dai J., 2021, ICLR
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Deng Y, 2021, ARXIV PREPRINT ARXIV
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong X. Y., 2021, ARXIV PREPRINT ARXIV
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Duke B, 2021, PROC CVPR IEEE, P5908, DOI 10.1109/CVPR46437.2021.00585
   DURNER M, 2021, ARXIV PREPRINT ARXIV
   Eiband M, 2018, IUI 2018: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P211, DOI 10.1145/3172944.3172961
   Esser Patrick, 2020, Taming transformers for high-resolution image synthesis
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Gan Zhe, 2020, NEURIPS
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Graham B, 2021, ARXIV PREPRINT ARXIV
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Han K., 2021, Adv. Neural Inf. Process. Syst., V34, P15908, DOI DOI 10.48550/ARXIV.2103.00112
   Han YZ, 2022, IEEE T PATTERN ANAL, V44, P7436, DOI 10.1109/TPAMI.2021.3117837
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks Dan, 2017, INT C LEARNING REPRE
   Heo B., 2021, ARXIV PREPRINT ARXIV
   Heusel M., 2017, Advances in Neural Information Processing Systems, P6627, DOI [DOI 10.48550/ARXIV.1706.08500, 10.48550/arXiv.1706.08500]
   Ho Jonathan, 2019, Axial attention in multidimensional transformers
   Hosang J, 2017, PROC CVPR IEEE, P6469, DOI 10.1109/CVPR.2017.685
   Hou QB, 2020, PROC CVPR IEEE, P4002, DOI 10.1109/CVPR42600.2020.00406
   Hu H, 2019, IEEE I CONF COMP VIS, P3463, DOI 10.1109/ICCV.2019.00356
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Hu Ronghang, 2021, arXiv preprint arXiv:2102.10772
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Hudson D.A., 2021, INT C MACH LEARN, P4487
   Hudson DA, 2019, PROC CVPR IEEE, P6693, DOI 10.1109/CVPR.2019.00686
   Gulrajani I, 2017, ADV NEUR IN, V30
   Jheng-Wei Su, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7965, DOI 10.1109/CVPR42600.2020.00799
   Jiang Yifan, 2021, arXiv preprint arXiv:2102.07074, P2021
   Jiang Z., 2020, P C EMP METH NAT LAN, P3850
   Kaiser L., 2016, ADV NEURAL INFORM PR, P3781
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kazemzadeh S., 2014, EMNLP, P787, DOI DOI 10.3115/V1/D14-1086
   Kitaev Nikita, 2020, INT C LEARN REPR
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Kumar M., P 9 INT C LEARN REPR, P2021
   Kuznetsova A, 2020, INT J COMPUT VISION, V128, P1956, DOI 10.1007/s11263-020-01316-z
   Li C. L., 2021, ARXIV PREPRINT ARXIV
   Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041
   Li X., 2020, P EUR C COMP VIS, DOI DOI 10.1007/978-3-030-58577-8_8
   Li YF, 2021, MICROPROCESS MICROSY, V87, DOI 10.1016/j.micpro.2021.104359
   Li YA, 2021, AAAI CONF ARTIF INTE, V35, P8518
   Liang Jiaming, 2021, ARXIV PREPRINT ARXIV
   Lin JR, 2020, IEEE INT C INT ROBOT, P4870, DOI 10.1109/IROS45743.2020.9340790
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu BC, 2021, AAAI CONF ARTIF INTE, V35, P2082
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu JS, 2019, ADV NEUR IN, V32
   Minghao Yin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P191, DOI 10.1007/978-3-030-58555-6_12
   Ordonez V., 2011, ADV NEURAL INFORM PR, P1143
   Pan X., 2020, P IEEE CVF C COMP VI, P11207
   Pan XJ, 2021, PROC CVPR IEEE, P11637, DOI 10.1109/CVPR46437.2021.01147
   Pan XJ, 2020, IEEE T IMAGE PROCESS, V29, P6745, DOI 10.1109/TIP.2020.2993403
   Pang L, 2016, AAAI CONF ARTIF INTE, P2793
   Radosavovic Ilija, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10425, DOI 10.1109/CVPR42600.2020.01044
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Real E, 2019, AAAI CONF ARTIF INTE, P4780
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi S. H., 2018, ARXIV PREPRINT ARXIV
   Rolfe Jason Tyler, 2016, ARXIV160902200
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salimans T, 2016, ADV NEUR IN, V29
   Schulz K., 2019, P INT C LEARN REPR, P1
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556
   Sheng KK, 2020, AAAI CONF ARTIF INTE, V34, P5709
   Sheng KK, 2021, COMPUT VIS MEDIA, V7, P139, DOI 10.1007/s41095-020-0193-5
   Sheng KK, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P879, DOI 10.1145/3240508.3240554
   Simon T, 2017, PROC CVPR IEEE, P4645, DOI 10.1109/CVPR.2017.494
   So DR, 2019, PR MACH LEARN RES, V97
   Stewart R, 2016, PROC CVPR IEEE, P2325, DOI 10.1109/CVPR.2016.255
   Su Weijie, 2020, INT C LEARN REPR
   Suhr A, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6418
   Sun KK, 2021, IEEE T SYST MAN CY-S, V51, P3968, DOI 10.1109/TSMC.2019.2958072
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tay Yi, 2020, ARXIV PREPRINT ARXIV
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Touvron H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P32, DOI 10.1109/ICCV48922.2021.00010
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   van den Oord A, 2017, ADV NEUR IN, V30
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Voita E, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P5797
   Wang Hanrui, 2020, ARXIV200514187, P7675
   Wang HB, 2020, IEEE MULTIMEDIA, V27, P112, DOI 10.1109/MMUL.2020.2999464
   Wang S., 2020, ARXIV PREPRINT ARXIV
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang WX, 2021, LECT NOTES COMPUT SC, V12901, P109, DOI 10.1007/978-3-030-87193-2_11
   Wang YQ, 2021, PROC CVPR IEEE, P8737, DOI 10.1109/CVPR46437.2021.00863
   Wang Ziheng, 2021, ARXIV PREPRINT ARXIV
   Weijian Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P266, DOI 10.1007/978-3-030-58545-7_16
   Xie Enze, 2021, ARXIV210515203, DOI DOI 10.48550/ARXIV.2105.15203
   Xie N., 2019, ARXIV190106706
   Xu WJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9961, DOI 10.1109/ICCV48922.2021.00983
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Ye LW, 2019, PROC CVPR IEEE, P10494, DOI 10.1109/CVPR.2019.01075
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Young P., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Yu Fei, 2021, P AAAI C ART INT
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Yuan Y., 2018, ARXIV PREPRINT ARXIV
   Zellers R, 2019, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2019.00688
   Zhang F, 2020, PROC CVPR IEEE, P7091, DOI 10.1109/CVPR42600.2020.00712
   Zhang PC, 2021, PROC CVPR IEEE, P5575, DOI 10.1109/CVPR46437.2021.00553
   Zhang R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073703
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhang SF, 2019, PROC CVPR IEEE, P919, DOI 10.1109/CVPR.2019.00101
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhao H., 2020, P IEEE CVF C COMP VI, V2020, P10076, DOI 10.1109/CVPR42600.2020.01009
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou D. Q., 2021, ARXIV PREPRINT ARXIV
   Zhou LW, 2020, AAAI CONF ARTIF INTE, V34, P13041
   Zhou X., 2019, arXiv
   Zoph B., 2017, ICLR, P1, DOI DOI 10.1109/ICAIIC48513.2020.9065031
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
NR 147
TC 65
Z9 70
U1 23
U2 301
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2022
VL 8
IS 1
BP 33
EP 62
DI 10.1007/s41095-021-0247-3
PG 30
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WO6VR
UT WOS:000712590200003
OA gold
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Chen, RJ
   Gotsman, C
AF Chen, Renjie
   Gotsman, Craig
TI Efficient fastest-path computations for road maps
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE shortest-path; road map; heuristic; GPS navigation; A* search
AB In the age of real-time online traffic information and GPS-enabled devices, fastest-path computations between two points in a road network modeled as a directed graph, where each directed edge is weighted by a "travel time" value, are becoming a standard feature of many navigation-related applications. To support this, very efficient computation of these paths in very large road networks is critical. Fastest paths may be computed as minimal-cost paths in a weighted directed graph, but traditional minimal-cost path algorithms based on variants of the classical Dijkstra algorithm do not scale well, as in the worst case they may traverse the entire graph. A common improvement, which can dramatically reduce the number of graph vertices traversed, is the A* algorithm, which requires a good heuristic lower bound on the minimal cost. We introduce a simple, but very effective, heuristic function based on a small number of values assigned to each graph vertex. The values are based on graph separators and are computed efficiently in a preprocessing stage. We present experimental results demonstrating that our heuristic provides estimates of the minimal cost superior to those of other heuristics. Our experiments show that when used in the A* algorithm, this heuristic can reduce the number of vertices traversed by an order of magnitude compared to other heuristics.
C1 [Chen, Renjie] Univ Sci & Technol China, Hefei, Peoples R China.
   [Gotsman, Craig] New Jersey Inst Technol, Ying Wu Coll Comp, Newark, NJ 07102 USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; New Jersey Institute of Technology
RP Chen, RJ (corresponding author), Univ Sci & Technol China, Hefei, Peoples R China.
EM renjiec@ustc.edu.cn; gotsman@njit.edu
RI Chen, Renjie/AFU-3325-2022
FU Anhui Provincial Natural Science Foundation [2008085MF195]; National
   Natural Science Foundation of China [62072422]; Zhejiang Lab
   [2019NB0AB03]
FX We would like to thank the anonymous reviewers for their constructive
   suggestions and comments. This work was partly supported by the Anhui
   Provincial Natural Science Foundation (2008085MF195), the National
   Natural Science Foundation of China (62072422), and Zhejiang Lab
   (2019NB0AB03).
CR [Anonymous], 2011, LECT NOTES COMPUTER, V6630, P376, DOI [10.1007/978-3-642-20662-7_32, DOI 10.1007/978-3-642-20662-7_32]
   [Anonymous], 2005, P ASS ADV ART INT
   [Anonymous], 2009, LECT NOTES COMPUTER, V4007, P316
   Bast H, 2016, LECT NOTES COMPUT SC, V9220, P19, DOI 10.1007/978-3-319-49487-6_2
   Cohen L, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1427
   Delling D, 2007, LECT NOTES COMPUT SC, V4525, P52
   Delling D, 2017, TRANSPORT SCI, V51, P566, DOI 10.1287/trsc.2014.0579
   Dibbelt J, 2014, LECT NOTES COMPUT SC, V8504, P271, DOI 10.1007/978-3-319-07959-2_23
   Dijkstra EW., 1959, NUMER MATH, V1, P269, DOI [10.1007/BF01386390, DOI 10.1007/BF01386390]
   Efentakis Alexandros., 2013, Proceedings of the Sixth ACM SIGSPATIAL International Workshop on Computational Transportation Science, P25
   Fibonacci E T, 1984, P 25TH ANN S FDN COM, P338
   Geisberger R, 2008, LECT NOTES COMPUT SC, V5038, P319, DOI 10.1007/978-3-540-68552-4_24
   Geisberger R, 2012, TRANSPORT SCI, V46, P388, DOI 10.1287/trsc.1110.0401
   Goldberg A., 2003, P ANN ACM SIAM S DIS, P156, DOI 10.1145/1070432.1070455
   HART PE, 1968, IEEE T SYST SCI CYB, VSSC4, P100, DOI 10.1109/TSSC.1968.300136
   Karypis G, 1998, SIAM J SCI COMPUT, V20, P359, DOI 10.1137/S1064827595287997
   LIPTON RJ, 1979, SIAM J APPL MATH, V36, P177, DOI 10.1137/0209046
   Rayner C., 2011, Proc. AAAI, P81
NR 18
TC 4
Z9 4
U1 0
U2 13
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2021
VL 7
IS 2
BP 267
EP 281
DI 10.1007/s41095-021-0211-2
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FR
UT WOS:000648692900009
OA gold, Green Submitted
DA 2024-07-18
ER

PT J
AU Podee, N
   Max, N
   Iwasaki, K
   Dobashi, Y
AF Podee, Namo
   Max, Nelson
   Iwasaki, Kei
   Dobashi, Yoshinori
TI Temporal and spatial anti-aliasing for rendering reflections on water
   waves
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE real-time rendering; anti-aliasing; reflection; water surface; water
   waves; elliptical Gaussian
AB The reflection of a bright light source on a dynamic surface such as water with waves can be difficult to render well in real time due to reflection aliasing and flickering. In this paper, we propose a solution to this problem by approximating the reflection direction distribution for the water surface as an elliptical Gaussian distribution. Then we analytically integrate the reflection contribution throughout the rendering interval time. Our method can render in real time an animation of the time integrated reflection of a spherical light source on highly dynamic waves with reduced aliasing and flickering.
C1 [Podee, Namo; Dobashi, Yoshinori] Hokkaido Univ, Sapporo, Hokkaido 0600814, Japan.
   [Max, Nelson] Univ Calif Davis, Davis, CA 95616 USA.
   [Iwasaki, Kei] Wakayama Univ, Wakayama 6408441, Japan.
C3 Hokkaido University; University of California System; University of
   California Davis; Wakayama University
RP Podee, N (corresponding author), Hokkaido Univ, Sapporo, Hokkaido 0600814, Japan.
EM namo.podee@gmail.com; max@cs.ucdavis.edu; iwasaki@wakayama-u.ac.jp;
   doba@ime.ist.hokudai.ac.jp
RI Iwasaki, Kei/GNH-6504-2022
OI Iwasaki, Kei/0000-0002-5235-536X
FU JSPS KAKENHI [JP15H05924, JP18H03348, JP20H05954]; Grants-in-Aid for
   Scientific Research [20H05954] Funding Source: KAKEN
FX This work was supported by JSPS KAKENHI Grant Nos. JP15H05924,
   JP18H03348, and JP20H05954.
CR Becker B. G., 1993, Computer Graphics Proceedings, P183, DOI 10.1145/166117.166141
   Bruneton E, 2010, COMPUT GRAPH FORUM, V29, P487, DOI 10.1111/j.1467-8659.2009.01618.x
   Cabral B, 1999, COMP GRAPH, P165, DOI 10.1145/311535.311553
   Cook R. L., 1984, Computers & Graphics, V18, P137
   Dupuy J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073694
   Dupuy J, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508422
   Gonzalez-Ochoa C, 2012, SIGGRAPH '12: SPECIAL INTEREST GROUP ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES CONFERENCE, DOI 10.1145/2343045.2343050
   Grant C. W., 1985, Computer Graphics, V19, P79, DOI 10.1145/325165.325184
   Heitz E, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925895
   Hopper R, 2017, ACM SIGGRAPH 2017 TALKS, DOI 10.1145/3084363.3085044
   Korein J., 1983, Computer Graphics, V17, P377, DOI 10.1145/964967.801168
   Lottes T., 2009, FXAA
   McAuley S., 2013, P ACM SIGGRAPH 2013
   McCormack J, 1999, COMP GRAPH, P243, DOI 10.1145/311535.311562
   NEHAB D, 2006, ACM SIGGRAPH 2006 SK, P185
   Norton A., 1982, Computer Graphics, V16, P1, DOI 10.1145/965145.801252
   OLANO M, 2010, LEAN MAPPING, P181, DOI [10.1145/1730804.1730834, DOI 10.1145/1730804.1730834]
   Ross V, 2005, J OPT SOC AM A, V22, P2442, DOI 10.1364/JOSAA.22.002442
   Scherzer Daniel, 2007, P 18 EUR C REND TECH, P45
   SHANNON CE, 1949, P IRE, V37, P10, DOI 10.1109/JRPROC.1949.232969
   Shinya M., 1993, Computer Graphics Proceedings, P289, DOI 10.1145/166117.166154
   SMITH BG, 1967, IEEE T ANTENN PROPAG, VAP15, P668, DOI 10.1109/TAP.1967.1138991
   WU L., 2019, ACM T GRAPHIC, V38, P4
   Zwicker M, 2001, IEEE VISUAL, P29, DOI 10.1109/VISUAL.2001.964490
NR 24
TC 2
Z9 2
U1 1
U2 9
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2021
VL 7
IS 2
BP 201
EP 215
DI 10.1007/s41095-021-0204-1
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FR
UT WOS:000648692900004
OA gold
DA 2024-07-18
ER

PT J
AU Li, JJ
   Feng, XM
   Fan, H
AF Li, Jinjiang
   Feng, Xiaomei
   Fan, Hui
TI Saliency-based image correction for colorblind patients
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE color vision; colorblindness; saliency; color correction
ID OBJECT DETECTION; VISION; SIMULATION; GREEN
AB Improper functioning, or lack, of human cone cells leads to vision defects, making it impossible for affected persons to distinguish certain colors. Colorblind persons have color perception, but their ability to capture color information differs from that of normal people: colorblind and normal people perceive the same image differently. It is necessary to devise solutions to help persons with color blindness understand images and distinguish different colors. Most research on this subject is aimed at adjusting insensitive colors, enabling colorblind persons to better capture color information, but ignores the attention paid by colorblind persons to the salient areas of images. The areas of the image seen as salient by normal people generally differ from those seen by the colorblind. To provide the same saliency for colorblind persons and normal people, we propose a saliency-based image correction algorithm for color blindness. Adjusted colors in the adjusted image are harmonious and realistic, and the method is practical. Our experimental results show that this method effectively improves images, enabling the colorblind to see the same salient areas as normal people.
C1 [Li, Jinjiang; Feng, Xiaomei; Fan, Hui] Shandong Technol & Business Univ, Sch Comp Sci & Technol, Yantai 264005, Peoples R China.
   [Li, Jinjiang; Feng, Xiaomei; Fan, Hui] Coinnovat Ctr Shandong Coll & Univ Future Intelli, Yantai 264005, Peoples R China.
C3 Shandong Technology & Business University
RP Li, JJ (corresponding author), Shandong Technol & Business Univ, Sch Comp Sci & Technol, Yantai 264005, Peoples R China.; Li, JJ (corresponding author), Coinnovat Ctr Shandong Coll & Univ Future Intelli, Yantai 264005, Peoples R China.
EM lijinjiang@gmail.com; xiaomeifeng19@gmail.com; fanlinw@263.net
RI Fan, Hui/AAC-5992-2022
FU National Natural Science Foundation of China [61772319, 61976125,
   61873177, 61773244]; Shandong Natural Science Foundation of China
   [ZR2017MF049]
FX The authors acknowledge the National Natural Science Foundation of China
   (Grant Nos. 61772319, 61976125, 61873177, and 61773244), and Shandong
   Natural Science Foundation of China (Grant No. ZR2017MF049). We thank
   the editors and anonymous reviewers for their comments.
CR [Anonymous], 2015, COMPUT VIS ME DIA
   [Anonymous], 2014, P 1 INTERNATIONALWOR, DOI DOI 10.1145/2662996.2663009
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Brettel H, 1997, J OPT SOC AM A, V14, P2647, DOI 10.1364/JOSAA.14.002647
   Chen C S, 2008, P ECCV WORKSH COMP V
   Chen TS, 2016, IEEE T NEUR NET LEAR, V27, P1135, DOI 10.1109/TNNLS.2015.2506664
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Cong RM, 2018, IEEE T IMAGE PROCESS, V27, P568, DOI 10.1109/TIP.2017.2763819
   Flatla DR, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P2563
   Fried O, 2015, PROC CVPR IEEE, P1703, DOI 10.1109/CVPR.2015.7298779
   Han JW, 2015, IEEE T CIRC SYST VID, V25, P1309, DOI 10.1109/TCSVT.2014.2381471
   Huang JB, 2009, INT CONF ACOUST SPEE, P1161, DOI 10.1109/ICASSP.2009.4959795
   Huang X, 2015, IEEE I CONF COMP VIS, P262, DOI 10.1109/ICCV.2015.38
   Hunt R., 2005, REPROD COLOUR, P92, DOI [10.1002/0470024275.ch8, DOI 10.1002/0470024275.CH8]
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   KESSLER J, 1977, ANN OPHTHALMOL, V9, P431
   Kuhn GR, 2008, IEEE T VIS COMPUT GR, V14, P1747, DOI 10.1109/TVCG.2008.112
   Li GB, 2017, PROC CVPR IEEE, P247, DOI 10.1109/CVPR.2017.34
   Li JJ, 2018, IEEE ACCESS, V6, P26831, DOI 10.1109/ACCESS.2018.2833888
   Lin HY, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19102250
   Machado GM, 2009, IEEE T VIS COMPUT GR, V15, P1291, DOI 10.1109/TVCG.2009.113
   Mateescu VA, 2016, IEEE MULTIMEDIA, V23, P82, DOI 10.1109/MMUL.2015.59
   Mechrez R, 2019, MACH VISION APPL, V30, P189, DOI 10.1007/s00138-018-01000-w
   Meguro M, 2011, IEEJ T EIS, V131, P482, DOI DOI 10.1541/IEEJEISS.131.482
   Melillo P, 2017, IEEE J TRANSL ENG HE, V5, DOI 10.1109/JTEHM.2017.2679746
   Mendez E, 2010, LECT NOTES COMPUT SC, V6133, P232, DOI 10.1007/978-3-642-13544-6_22
   MEYER GW, 1988, IEEE COMPUT GRAPH, V8, P28, DOI 10.1109/38.7759
   Nakayama K, 2019, ELECTR COMMUN JPN, V102, P17, DOI 10.1002/ecj.12197
   NATHANS J, 1986, SCIENCE, V232, P193, DOI 10.1126/science.2937147
   Nguyen TV, 2013, IEEE T MULTIMEDIA, V15, P1910, DOI 10.1109/TMM.2013.2272919
   Ohata F, 2010, IEEJ T EIS, V130, P2176, DOI DOI 10.1541/IEEJEISS.130.2176
   Okajima K, 2007, TECHNICAL REPORT IEI, V107
   Peng HW, 2017, IEEE T PATTERN ANAL, V39, P818, DOI 10.1109/TPAMI.2016.2562626
   ROSENSTO.HB, 1974, AEROSPACE MED, V45, P1194
   Scoles D, 2013, BIOMED OPT EXPRESS, V4, P1710, DOI 10.1364/BOE.4.001710
   Su S. L., 2005, P 2 S APPL PERC GRAP, V164
   Subbian V, 2015, IEEE J TRANSL ENG HE, V3, DOI 10.1109/JTEHM.2015.2424224
   Sun J, 2011, IEEE I CONF COMP VIS, P1511, DOI 10.1109/ICCV.2011.6126409
   Sundaram N, 2010, LECT NOTES COMPUT SC, V6311, P438, DOI 10.1007/978-3-642-15549-9_32
   Tanuwidjaja E, 2014, UBICOMP'14: PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P799, DOI 10.1145/2632048.2632091
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang W., 2019, ANN IEEE INT CONF SE
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang WG, 2016, IEEE T IMAGE PROCESS, V25, P5025, DOI 10.1109/TIP.2016.2601784
   Weale R, 1986, BRIT J OPHTHALMOLOGY, V70, P159, DOI [10.1136/bjo.70.2.159, DOI 10.1136/BJO.70.2.159]
   Wei YC, 2017, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR.2017.687
   Wei YC, 2017, IEEE T PATTERN ANAL, V39, P2314, DOI 10.1109/TPAMI.2016.2636150
   Yanagida T, 2015, COLOR RES APPL, V40, P446, DOI 10.1002/col.21913
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Zhang JM, 2016, PROC CVPR IEEE, P5733, DOI 10.1109/CVPR.2016.618
   Zhang SY, 2019, COMPUT VIS MEDIA, V5, P105, DOI 10.1007/s41095-019-0136-1
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhou L, 2017, IEEE T IMAGE PROCESS, V26, P5882, DOI 10.1109/TIP.2017.2738839
NR 54
TC 20
Z9 20
U1 2
U2 10
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2020
VL 6
IS 2
BP 169
EP 189
DI 10.1007/s41095-020-0172-x
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FB
UT WOS:000648691300005
OA gold
DA 2024-07-18
ER

PT J
AU Zhang, SH
   Zhou, ZP
   Liu, B
   Dong, X
   Hall, P
AF Zhang, Song-Hai
   Zhou, Zheng-Ping
   Liu, Bin
   Dong, Xi
   Hall, Peter
TI What and where: A context-based recommendation system for object
   insertion
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE object recommendation; bounding box prediction; image composition;
   object-level context
AB We propose a novel problem revolving around two tasks: (i) given a scene, recommend objects to insert, and (ii) given an object category, retrieve suitable background scenes. A bounding box for the inserted object is predicted in both tasks, which helps downstream applications such as semiautomated advertising and video composition. The major challenge lies in the fact that the target object is neither present nor localized in the input, and furthermore, available datasets only provide scenes with existing objects. To tackle this problem, we build an unsupervised algorithm based on object-level contexts, which explicitly models the joint probability distribution of object categories and bounding boxes using a Gaussian mixture model. Experiments on our own annotated test set demonstrate that our system outperforms existing baselines on all sub-tasks, and does so using a unified framework. Future extensions and applications are suggested.
C1 [Zhang, Song-Hai; Zhou, Zheng-Ping; Liu, Bin; Dong, Xi] Tsinghua Univ, Beijing 100084, Peoples R China.
   [Zhang, Song-Hai] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing 100084, Peoples R China.
   [Hall, Peter] Univ Bath, Dept Comp Sci, Media Technol Res Ctr, Bath BA2 7AY, Avon, England.
C3 Tsinghua University; Tsinghua University; University of Bath
RP Zhang, SH (corresponding author), Tsinghua Univ, Beijing 100084, Peoples R China.; Zhang, SH (corresponding author), Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing 100084, Peoples R China.
EM shz@tsinghua.edu.cn; zzp.thu@gmail.com; liub91@gmail.com;
   dong_xin@qq.com; maspmh@bath.ac.uk
OI Hall, Peter/0009-0006-5699-5483
FU National Key Technology RD Program [2016YFB1001402]; National Natural
   Science Foundation of China [61521002, 61772298]; Research Grant of
   Beijing Higher Institution Engineering Research Center; Tsinghua-Tencent
   Joint Laboratory for Internet Innovation Technology; EPSRC
   [EP/M023281/1] Funding Source: UKRI
FX We would like to thank all reviewers for their thoughtful comments, and
   we would like to thank Prof. Ralph Martin for his valuable suggestions
   on paper revision. This work was supported by the National Key
   Technology R&D Program (Project Number 2016YFB1001402), the National
   Natural Science Foundation of China (Project Numbers 61521002,
   61772298), Research Grant of Beijing Higher Institution Engineering
   Research Center, and Tsinghua-Tencent Joint Laboratory for Internet
   Innovation Technology.
CR Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   [Anonymous], CoRR abs/1511.07122
   [Anonymous], 2013, ACM T GRAPHICS, V32, P6
   Bilen H, 2016, PROC CVPR IEEE, P2846, DOI 10.1109/CVPR.2016.311
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong S., 2018, P ADV NEUR INF PROC, P2708
   Järvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418
   Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990
   Kantorov V, 2016, LECT NOTES COMPUT SC, V9909, P350, DOI 10.1007/978-3-319-46454-1_22
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee D., 2018, Advances in Neural Information Processing Systems, P10393
   Lin CH, 2018, PROC CVPR IEEE, P9455, DOI 10.1109/CVPR.2018.00985
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W., 2015, ARXIV150604579
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Rabinovich A, 2007, IEEE I CONF COMP VIS, P1237, DOI 10.1109/iccv.2007.4408986
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ricci F, 2011, RECOMMENDER SYSTEMS HANDBOOK, P1, DOI 10.1007/978-0-387-85820-3_1
   Shiming Ge, 2018, Computational Visual Media, V4, P71, DOI 10.1007/s41095-017-0102-8
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tan FW, 2018, IEEE WINT CONF APPL, P1519, DOI 10.1109/WACV.2018.00170
   Todo Hideki, 2017, [Computational Visual Media, 计算可视媒体], V3, P21
   Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976
   Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zhang SH, 2019, J COMPUT SCI TECH-CH, V34, P594, DOI 10.1007/s11390-019-1929-5
   Zheng L, 2018, IEEE T PATTERN ANAL, V40, P1224, DOI 10.1109/TPAMI.2017.2709749
   Zhou B., 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.319
   Zhou W., 2017, Recent Advance in Content-based Image Retrieval: A Literature Survey
NR 34
TC 9
Z9 9
U1 1
U2 18
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2020
VL 6
IS 1
BP 79
EP 93
DI 10.1007/s41095-020-0158-8
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RZ6FA
UT WOS:000648691200007
OA Green Submitted, gold
DA 2024-07-18
ER

PT J
AU Ono, T
   Kubo, H
   Tanaka, K
   Funatomi, T
   Mukaigawa, Y
AF Ono, Taishi
   Kubo, Hiroyuki
   Tanaka, Kenichiro
   Funatomi, Takuya
   Mukaigawa, Yasuhiro
TI Practical BRDF reconstruction using reliable geometric regions from
   multi-view stereo
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE BRDF reconstruction; multi-view stereo (MVS); photogrammetry; rendering
AB In this paper, we present a practical method for reconstructing the bidirectional reflectance distribution function (BRDF) from multiple images of a real object composed of a homogeneous material. The key idea is that the BRDF can be sampled after geometry estimation using multi-view stereo (MVS) techniques. Our contribution is selection of reliable samples of lighting, surface normal, and viewing directions for robustness against estimation errors of MVS. Our method is quantitatively evaluated using synthesized images and its effectiveness is shown via real-world experiments.
C1 [Ono, Taishi; Kubo, Hiroyuki; Tanaka, Kenichiro; Funatomi, Takuya; Mukaigawa, Yasuhiro] Nara Inst Sci & Technol, Nara 6300192, Japan.
C3 Nara Institute of Science & Technology
RP Kubo, H (corresponding author), Nara Inst Sci & Technol, Nara 6300192, Japan.
EM hkubo@is.naist.jp
RI Funatomi, Takuya/K-5919-2018; Kubo, Hiroyuki/AAS-1487-2021
OI Funatomi, Takuya/0000-0001-5588-5932; Kubo, Hiroyuki/0000-0002-7061-7941
FU JSPS KAKENHI [JP15K16027, JP26700013, JP15H05918, JP19H04138]; JST CREST
   [JP179423]; Foundation for Nara Institute of Science and Technology
FX This work was partly supported by JSPS KAKENHI JP15K16027, JP26700013,
   JP15H05918, JP19H04138, JST CREST JP179423, and the Foundation for Nara
   Institute of Science and Technology.
CR ACM, 2016, T GRAPHICS, V35, P6
   ACM, 2009, ACM T GRAPHICS, V28, P3
   ACM, 2015, T GRAPHICS, V34, P6
   Aldosary BM, 2012, CLIN TOXICOL, V50, P57, DOI 10.3109/15563650.2011.641560
   Chandraker M, 2014, PROC CVPR IEEE, P2179, DOI 10.1109/CVPR.2014.279
   Chandraker M, 2013, PROC CVPR IEEE, P2523, DOI 10.1109/CVPR.2013.326
   ERB W, 1980, APPL OPTICS, V19, P3789, DOI 10.1364/AO.19.003789
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Higo T, 2010, PROC CVPR IEEE, P1157, DOI 10.1109/CVPR.2010.5540084
   Koenderink J.J., 1989, Photometric invariants related to solid shape, P301
   Kutulakos K. N., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P307, DOI 10.1109/ICCV.1999.791235
   Li ZQ, 2017, PROC CVPR IEEE, P578, DOI 10.1109/CVPR.2017.69
   Lu F, 2015, IEEE T PATTERN ANAL, V37, P1999, DOI 10.1109/TPAMI.2015.2389841
   Marschner SR, 1999, SPRING EUROGRAP, P131
   Matusik W., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P241
   Miyashita Leo, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P232, DOI 10.1109/3DV.2014.41
   Oxholm G, 2016, IEEE T PATTERN ANAL, V38, P376, DOI 10.1109/TPAMI.2015.2450734
   Pharr M, 2010, PHYSICALLY BASED RENDERING: FROM THEORY TO IMPLEMENTATION, 2ND EDITION, P1
   Rusinkiewicz S. M., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P11
   Seitz SM, 1999, INT J COMPUT VISION, V35, P151, DOI 10.1023/A:1008176507526
   Treuille A, 2004, LECT NOTES COMPUT SC, V3022, P457
   ULLMAN S, 1979, PROC R SOC SER B-BIO, V203, P405, DOI 10.1098/rspb.1979.0006
   Vogiatzis G, 2005, PROC CVPR IEEE, P391
   ZISSERMAN A, 1989, IMAGE VISION COMPUT, V7, P38, DOI 10.1016/0262-8856(89)90018-8
NR 24
TC 5
Z9 5
U1 0
U2 4
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD DEC
PY 2019
VL 5
IS 4
BP 325
EP 336
DI 10.1007/s41095-019-0150-3
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UO
UT WOS:000648691000001
OA gold
DA 2024-07-18
ER

PT J
AU Hallek, M
   Smach, F
   Atri, M
AF Hallek, Mohamed
   Smach, Fethi
   Atri, Mohamed
TI Real-time stereo matching on CUDA using Fourier descriptors and dynamic
   programming
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE generalized Fourier descriptors; stereo matching; dynamic programming;
   CUDA
ID RECOGNITION; REFINEMENT
AB Computation of stereoscopic depth and disparity map extraction are dynamic research topics. A large variety of algorithms has been developed, among which we cite feature matching, moment extraction, and image representation using descriptors to determine a disparity map. This paper proposes a new method for stereo matching based on Fourier descriptors. The robustness of these descriptors under photometric and geometric transformations provides a better representation of a template or a local region in the image. In our work, we specifically use generalized Fourier descriptors to compute a robust cost function. Then, a box filter is applied for cost aggregation to enforce a smoothness constraint between neighboring pixels. Optimization and disparity calculation are done using dynamic programming, with a cost based on similarity between generalized Fourier descriptors using Euclidean distance. This local cost function is used to optimize correspondences. Our stereo matching algorithm is evaluated using the Middlebury stereo benchmark; our approach has been implemented on parallel high-performance graphics hardware using CUDA to accelerate our algorithm, giving a real-time implementation.
C1 [Hallek, Mohamed; Atri, Mohamed] Fac Sci Monastir, Monastir 5000, Tunisia.
   [Smach, Fethi] Technol & Serv Informat, Paris, France.
C3 Universite de Monastir
RP Hallek, M (corresponding author), Fac Sci Monastir, Monastir 5000, Tunisia.
EM hallekmohamed@yahoo.fr
RI ATRI, Mohamed/C-4069-2014
OI ATRI, Mohamed/0000-0001-8528-5647
CR Altantawy DA, 2014, 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER ENGINEERING & SYSTEMS (ICCES), P130, DOI 10.1109/ICCES.2014.7030943
   [Anonymous], 2009, REALTIME DENSE STERE
   Banks J, 2001, INT J ROBOT RES, V20, P512, DOI 10.1177/02783640122067525
   Barnes Connelly, 2017, [Computational Visual Media, 计算可视媒体], V3, P3
   Bhat DN, 1998, IEEE T PATTERN ANAL, V20, P415, DOI 10.1109/34.677275
   Brown MZ, 2003, IEEE T PATTERN ANAL, V25, P993, DOI 10.1109/TPAMI.2003.1217603
   Gong M, 2005, PROC CVPR IEEE, P924
   Gonidis P, 2007, ZERNIKE MOMENTS, P33
   Hamzah RA, 2016, J SENSORS, V2016, DOI 10.1155/2016/8742920
   Haythem B, 2014, FAST GEN FOURIER DES, P1
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Hosni A, 2013, IEEE T PATTERN ANAL, V35, P504, DOI 10.1109/TPAMI.2012.156
   Kitagawa M, 2017, PROCEEDINGS OF THE FIFTEENTH IAPR INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS - MVA2017, P258, DOI 10.23919/MVA.2017.7986850
   Kordelas GA, 2016, IEEE T MULTIMEDIA, V18, P155, DOI 10.1109/TMM.2015.2505905
   Kowalczuk J, 2013, IEEE T CIRC SYST VID, V23, P94, DOI 10.1109/TCSVT.2012.2203200
   LeGendre C, 2017, HIGH RESOLUTION STER
   Li ZY, 2015, OPTOELECTRON LETT, V11, P390, DOI 10.1007/s11801-015-5146-3
   Lucas B., 1981, Proc. DARPA Image Understanding Workshop, P121
   Martins JA, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0129908
   Mattoccia S, 2007, LECT NOTES COMPUT SC, V4844, P517
   Michael M, 2013, IEEE INT VEH SYM, P1197, DOI 10.1109/IVS.2013.6629629
   OHTA Y, 1985, IEEE T PATTERN ANAL, V7, P139, DOI 10.1109/TPAMI.1985.4767639
   Psarakis EZ, 2005, IEEE I CONF COMP VIS, P907
   Richardt C, 2010, LECT NOTES COMPUT SC, V6313, P510
   Sabihuddin S, 2008, DYNAMIC PROGRAMMING, P1461
   Salmen J, 2009, LECT NOTES COMPUT SC, V5702, P1096, DOI 10.1007/978-3-642-03767-2_133
   Saygili G, 2012, IEEE IMAGE PROC, P2973, DOI 10.1109/ICIP.2012.6467524
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Shahbazi M, 2016, ISPRS ANN PHOTO REM, V3, P123, DOI 10.5194/isprsannals-III-3-123-2016
   Smach F, 2008, J MATH IMAGING VIS, V30, P43, DOI 10.1007/s10851-007-0036-3
   Smach F, 2007, J REAL-TIME IMAGE PR, V2, P249, DOI 10.1007/s11554-007-0065-6
   Veksler O, 2005, PROC CVPR IEEE, P384
   Wang L, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P798
   Wang L, 2014, J REAL-TIME IMAGE PR, V9, P447, DOI 10.1007/s11554-012-0275-4
   Wang M., 2016, COMPUT VIS MEDIA, V1, P3, DOI DOI 10.1007/s41095-016-0037-5
   Yang QQ, 2014, IMAGE VISION COMPUT, V32, P202, DOI 10.1016/j.imavis.2014.01.001
   Yang QX, 2009, IEEE T PATTERN ANAL, V31, P492, DOI 10.1109/TPAMI.2008.99
   Yu W, 2010, IEEE T CIRC SYST VID, V20, P1509, DOI 10.1109/TCSVT.2010.2077771
   Zhang DS, 2002, SIGNAL PROCESS-IMAGE, V17, P825, DOI 10.1016/S0923-5965(02)00084-X
   Zhang FL, 2017, IEEE T VIS COMPUT GR, V23, P1561, DOI 10.1109/TVCG.2016.2532329
   Zhang K, 2012, INT C PATT RECOG, P356
NR 41
TC 8
Z9 8
U1 1
U2 4
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD MAR
PY 2019
VL 5
IS 1
BP 59
EP 71
DI 10.1007/s41095-019-0133-4
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA VJ9UL
UT WOS:000648690700006
OA gold
DA 2024-07-18
ER

EF