FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Pintore, G
   Almansa, E
   Sanchez, A
   Vassena, G
   Gobbetti, E
AF Pintore, Giovanni
   Almansa, Eva
   Sanchez, Armando
   Vassena, Giorgio
   Gobbetti, Enrico
TI Deep panoramic depth prediction and completion for indoor scenes
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article; Early Access
DE machine learning; image processing and computer vision; vision and scene
   understanding; 3D stereo scene analysis
ID OF-THE-ART; 3D RECONSTRUCTION; IMAGE
AB We introduce a novel end-to-end deep-learning solution for rapidly estimating a dense spherical depth map of an indoor environment. Our input is a single equirectangular image registered with a sparse depth map, as provided by a variety of common capture setups. Depth is inferred by an efficient and lightweight single-branch network, which employs a dynamic gating system to process together dense visual data and sparse geometric data. We exploit the characteristics of typical man-made environments to efficiently compress multi-resolution features and find short- and long-range relations among scene parts. Furthermore, we introduce a new augmentation strategy to make the model robust to different types of sparsity, including those generated by various structured light sensors and LiDAR setups. The experimental results demonstrate that our method provides interactive performance and outperforms state-of-the-art solutions in computational efficiency, adaptivity to variable depth sparsity patterns, and prediction accuracy for challenging indoor data, even when trained solely on synthetic data without any fine tuning.
C1 [Pintore, Giovanni; Almansa, Eva; Gobbetti, Enrico] Visual & Data intens Comp, CRS4, I-09134 Cagliari, Italy.
   [Sanchez, Armando; Vassena, Giorgio] Gexcel srl, I-09097 Elmas, CA, Italy.
   [Vassena, Giorgio] Univ Brescia UNIBS, Dept Civil Environm Architectural Engn & Math, I-25123 Brescia, BS, Italy.
RP Pintore, G; Almansa, E; Gobbetti, E (corresponding author), Visual & Data intens Comp, CRS4, I-09134 Cagliari, Italy.
EM giovanni.pintore@crs4.it; eval.m.almansa@gmail.com;
   enrico.gobbetti@crs4.it
RI Vassena, Giorgio Paolo Maria GV/F-4559-2016; Almansa, Eva/M-9086-2015
OI Vassena, Giorgio Paolo Maria GV/0000-0002-2249-8853; Almansa,
   Eva/0000-0002-7288-0989
FU Autonomous Region of Sardinia under project XDATA; European Union
   [813170]
FX Giovanni Pintore and Enrico Gobbetti received funding from the
   Autonomous Region of Sardinia under project XDATA. Eva Almansa, Armando
   Sanchez, Giorgio Vassena, and Enrico Gobbetti received funding from the
   European Union's H2020 research and innovation programme under grant
   813170 (EVOCATION).
CR [Anonymous], 2019, Structured3D Data
   Cao YZH, 2018, IEEE T CIRC SYST VID, V28, P3174, DOI 10.1109/TCSVT.2017.2740321
   Cheng HT, 2018, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2018.00154
   Cheng XJ, 2020, IEEE INT CONF ROBOT, P589, DOI [10.1109/ICRA40945.2020.9197123, 10.1109/icra40945.2020.9197123]
   Clevert DA, 2016, Arxiv, DOI [arXiv:1511.07289, DOI 10.48550/ARXIV.1511.07289]
   Davidson Benjamin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P579, DOI 10.1007/978-3-030-58604-1_35
   de La Garanderie GP, 2018, LECT NOTES COMPUT SC, V11217, P812, DOI 10.1007/978-3-030-01261-8_48
   Du Wenchao, 2022, 2022 International Conference on Robotics and Automation (ICRA), P8680, DOI 10.1109/ICRA46639.2022.9811556
   Eigen D, 2014, ADV NEUR IN, V27
   Eldesokey Abdelrahman, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12011, DOI 10.1109/CVPR42600.2020.01203
   Eldesokey A, 2020, IEEE T PATTERN ANAL, V42, P2423, DOI 10.1109/TPAMI.2019.2929170
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Gan YK, 2018, LECT NOTES COMPUT SC, V11207, P232, DOI 10.1007/978-3-030-01219-9_14
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Gkitsas V, 2021, IEEE COMPUT SOC CONF, P3711, DOI 10.1109/CVPRW53098.2021.00412
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Guizilini V., 2020, C ROBOT LEARNING, P503
   Guizilini V, 2021, PROC CVPR IEEE, P11073, DOI 10.1109/CVPR46437.2021.01093
   Harrison A, 2010, SPRINGER TRAC ADV RO, V62, P219
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu M, 2021, IEEE INT CONF ROBOT, P13656, DOI 10.1109/ICRA48506.2021.9561035
   Huang YK, 2021, PROC CVPR IEEE, P16701, DOI 10.1109/CVPR46437.2021.01643
   Huang YK, 2019, IEEE INT CONF COMP V, P1070, DOI 10.1109/ICCVW.2019.00137
   Imran Saif, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12438, DOI 10.1109/CVPR.2019.01273
   Ji P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12767, DOI 10.1109/ICCV48922.2021.01255
   Jia Zheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P519, DOI 10.1007/978-3-030-58545-7_30
   Jinsun Park, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P120, DOI 10.1007/978-3-030-58601-0_8
   Jokela T., 2019, P 18 INT C MOBILE UB, P1
   Jung R, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1, DOI [10.1109/VR.2019.8798326, 10.1109/vr.2019.8798326]
   Junyi Liu, 2013, Advances in Multimedia Information Processing - PCM 2013. 14th Pacific-Rim Conference on Multimedia. Proceedings: LNCS 8294, P408, DOI 10.1007/978-3-319-03731-8_38
   Kingma D. P., 2014, arXiv
   Ku J, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P16, DOI 10.1109/CRV.2018.00013
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Lambert-Lacroix S, 2016, J NONPARAMETR STAT, V28, P487, DOI 10.1080/10485252.2016.1190359
   Lee S, 2020, IEEE ACCESS, V8, P79801, DOI 10.1109/ACCESS.2020.2990212
   Li YW, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19132976
   Liao YY, 2023, IEEE T PATTERN ANAL, V45, P3292, DOI 10.1109/TPAMI.2022.3179507
   Liu FY, 2015, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2015.7299152
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu RY, 2022, IEEE T INTELL TRANSP, V23, P25180, DOI 10.1109/TITS.2022.3155925
   Lopez-Rodriguez Adrian, 2021, Computer Vision - ACCV 2020. 15th Asian Conference on Computer Vision. Revised Selected Papers. Lecture Notes in Computer Science (LNCS 12622), P330, DOI 10.1007/978-3-030-69525-5_20
   Ma FC, 2019, IEEE INT CONF ROBOT, P3288, DOI [10.1109/ICRA.2019.8793637, 10.1109/icra.2019.8793637]
   Ma FC, 2018, IEEE INT CONF ROBOT, P4796
   Matterport, 2017, Matterport3D
   Mertan A, 2022, DIGIT SIGNAL PROCESS, V123, DOI 10.1016/j.dsp.2022.103441
   Ming Y, 2021, NEUROCOMPUTING, V438, P14, DOI 10.1016/j.neucom.2020.12.089
   Morales J, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18020395
   New York University, 2012, NYU-Depth V2
   Oh C, 2022, LECT NOTES COMPUT SC, V13676, P352, DOI 10.1007/978-3-031-19787-1_20
   Pintore G, 2021, PROC CVPR IEEE, P11531, DOI 10.1109/CVPR46437.2021.01137
   Pintore G, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480480
   Pintore G, 2020, COMPUT GRAPH FORUM, V39, P667, DOI 10.1111/cgf.14021
   Qiu JX, 2019, PROC CVPR IEEE, P3308, DOI 10.1109/CVPR.2019.00343
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Stanford University, 2017, BuildingParser Dataset
   Straub J, 2019, Arxiv, DOI arXiv:1906.05797
   Su YC, 2017, ADV NEUR IN, V30
   Su YC, 2019, PROC CVPR IEEE, P9434, DOI 10.1109/CVPR.2019.00967
   Sun C, 2021, PROC CVPR IEEE, P2573, DOI 10.1109/CVPR46437.2021.00260
   Sun C, 2019, PROC CVPR IEEE, P1047, DOI 10.1109/CVPR.2019.00114
   Tang J, 2021, IEEE T IMAGE PROCESS, V30, P1116, DOI 10.1109/TIP.2020.3040528
   Tateno K, 2018, LECT NOTES COMPUT SC, V11220, P732, DOI 10.1007/978-3-030-01270-0_43
   Van Gansbeke W, 2019, PROCEEDINGS OF MVA 2019 16TH INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS (MVA), DOI 10.23919/mva.2019.8757939
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang FE, 2020, PROC CVPR IEEE, P459, DOI 10.1109/CVPR42600.2020.00054
   Wang P, 2015, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2015.7298897
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu T, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10111224
   Xian WQ, 2019, IEEE I CONF COMP VIS, P9973, DOI 10.1109/ICCV.2019.01007
   Xin Xiong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P682, DOI 10.1007/978-3-030-58589-1_41
   Xiong X. H., 2010, P BRIT MACHINE VISIO
   Xu D, 2018, PROC CVPR IEEE, P3917, DOI 10.1109/CVPR.2018.00412
   Yang YC, 2019, PROC CVPR IEEE, P3348, DOI 10.1109/CVPR.2019.00347
   Yi Z, 2020, P IEEE CVF C COMP VI, P7508, DOI DOI 10.1109/CVPR42600.2020.00753
   Yin W, 2019, IEEE I CONF COMP VIS, P5683, DOI 10.1109/ICCV.2019.00578
   You YR, 2020, Arxiv, DOI arXiv:1906.06310
   Yu FS, 2016, Arxiv, DOI arXiv:1511.07122
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zhan HY, 2018, PROC CVPR IEEE, P340, DOI 10.1109/CVPR.2018.00043
   Zhang YD, 2018, PROC CVPR IEEE, P175, DOI 10.1109/CVPR.2018.00026
   Zheng CX, 2019, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2019.00153
   Zioulis N, 2019, INT CONF 3D VISION, P690, DOI 10.1109/3DV.2019.00081
   Zioulis N, 2018, LECT NOTES COMPUT SC, V11210, P453, DOI 10.1007/978-3-030-01231-1_28
   Zollhöfer M, 2018, COMPUT GRAPH FORUM, V37, P625, DOI 10.1111/cgf.13386
NR 86
TC 1
Z9 1
U1 11
U2 11
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD 2024 FEB 8
PY 2024
DI 10.1007/s41095-023-0358-0
EA FEB 2024
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HK9T3
UT WOS:001159523000001
OA gold
DA 2024-08-05
ER

PT J
AU Hou, SY
   Wang, CY
   Zhuang, WL
   Chen, Y
   Wang, YA
   Bao, HJ
   Chai, JX
   Xu, WW
AF Hou, Shuaiying
   Wang, Congyi
   Zhuang, Wenlin
   Chen, Yu
   Wang, Yangang
   Bao, Hujun
   Chai, Jinxiang
   Xu, Weiwei
TI A causal convolutional neural network for multi-subject motion modeling
   and generation
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE deep learning; optimization; motion generation; motion denoising; motion
   control
AB Inspired by the success of WaveNet in multi-subject speech synthesis, we propose a novel neural network based on causal convolutions for multi-subject motion modeling and generation. The network can capture the intrinsic characteristics of the motion of different subjects, such as the influence of skeleton scale variation on motion style. Moreover, after fine-tuning the network using a small motion dataset for a novel skeleton that is not included in the training dataset, it is able to synthesize high-quality motions with a personalized style for the novel skeleton. The experimental results demonstrate that our network can model the intrinsic characteristics of motions well and can be applied to various motion modeling and synthesis tasks.
C1 [Hou, Shuaiying; Chen, Yu; Bao, Hujun; Xu, Weiwei] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Peoples R China.
   [Wang, Congyi; Chai, Jinxiang] Xmov, Shanghai 200030, Peoples R China.
   [Zhuang, Wenlin; Wang, Yangang] Southeast Univ, Sch Automat, Nanjing 210096, Peoples R China.
C3 Zhejiang University; Southeast University - China
RP Xu, WW (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Peoples R China.
EM 1721044@zju.edu.cn; artwang007@gmail.com; wlzhuang@seu.edu.cn;
   jianfenghou4@163.com; yangangwang@seu.edu.cn; bao@cad.zju.edu.cn;
   chaijinxiang@xmov.ai; xww@cad.zju.edu.cn
OI Hou, Shuaiying/0000-0003-0689-9240
FU National Natural Science Foundation of China [61732016]
FX We thank the anonymous reviewers for their constructive comments. Weiwei
   Xu is partially supported by the National Natural Science Foundation of
   China (No. 61732016).
CR Aberman K, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392462
   [Anonymous], 2016, ARXIV PREPRINT ARXIV
   Barsoum E, 2018, IEEE COMPUT SOC CONF, P1499, DOI 10.1109/CVPRW.2018.00191
   Brand M, 2000, COMP GRAPH, P183, DOI 10.1145/344779.344865
   Buss S. R., 2004, IEEE T ROBOT AUTOM, V17, P1
   Chen Z, 2021, AAAI CONF ARTIF INTE, V35, P1113, DOI 10.1145/3474085.3475574
   Corona E, 2020, PROC CVPR IEEE, P6990, DOI 10.1109/CVPR42600.2020.00702
   Cui QJ, 2020, PROC CVPR IEEE, P6518, DOI 10.1109/CVPR42600.2020.00655
   Fragkiadaki K, 2015, IEEE I CONF COMP VIS, P4346, DOI 10.1109/ICCV.2015.494
   Ghorbani S, 2020, COMPUT GRAPH FORUM, V39, P225, DOI 10.1111/cgf.14116
   Ghosh A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1376, DOI 10.1109/ICCV48922.2021.00143
   Ghosh P, 2017, INT CONF 3D VISION, P458, DOI 10.1109/3DV.2017.00059
   Graves A, 2014, Arxiv, DOI [arXiv:1308.0850, DOI 10.48550/ARXIV.1308.0850]
   Gui LY, 2018, LECT NOTES COMPUT SC, V11208, P823, DOI 10.1007/978-3-030-01225-0_48
   Henter GE, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417836
   Holden D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073663
   Holden D, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925975
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jingwei Xu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P178, DOI 10.1007/978-3-030-58621-8_11
   Kundu JN, 2019, AAAI CONF ARTIF INTE, P8553
   Lee K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275071
   Li MS, 2020, PROC CVPR IEEE, P211, DOI 10.1109/CVPR42600.2020.00029
   Ling HY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392422
   Liu ZG, 2021, AAAI CONF ARTIF INTE, V35, P2225
   Martinez J, 2017, PROC CVPR IEEE, P4674, DOI 10.1109/CVPR.2017.497
   Min J., 2010, P 2010 ACM SIGGRAPH, DOI DOI 10.1145/1730804.1730811
   Pavllo D, 2020, INT J COMPUT VISION, V128, P855, DOI 10.1007/s11263-019-01245-6
   Petrovich M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10965, DOI 10.1109/ICCV48922.2021.01080
   Qiang Nie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P102, DOI 10.1007/978-3-030-58529-7_7
   Sebastian Grassia F., 1998, Journal of graphics tools, V3, P29, DOI [DOI 10.1080/10867651.1998.10487493, 10.1080/10867651.1998.10487493]
   Starke S, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459881
   Starke S, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392450
   Taylor G., 2009, P 26 ANN INT C MACHI
   Valle-Pérez G, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480570
   Wang T.-C., 2019, P ADV NEUR INF PROC, P5013
   Wang X, 2014, COMPUT MATH METHOD M, V2014, DOI 10.1155/2014/104535
   Wang ZY, 2020, AAAI CONF ARTIF INTE, V34, P12281
   Wang ZY, 2021, IEEE T VIS COMPUT GR, V27, P14, DOI 10.1109/TVCG.2019.2938520
   Wei Mao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P474, DOI 10.1007/978-3-030-58568-6_28
   Wen YH, 2021, PROC CVPR IEEE, P13607, DOI 10.1109/CVPR46437.2021.01340
   Xia SH, 2017, J COMPUT SCI TECH-CH, V32, P536, DOI 10.1007/s11390-017-1742-y
   Xia SH, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766999
   Yan XC, 2018, LECT NOTES COMPUT SC, V11209, P276, DOI 10.1007/978-3-030-01228-1_17
   Yuan Y., 2020, P 8 INT C LEARN REPR
   Zhang PF, 2020, PROC CVPR IEEE, P1109, DOI 10.1109/CVPR42600.2020.00119
NR 45
TC 1
Z9 1
U1 6
U2 9
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD FEB
PY 2024
VL 10
IS 1
BP 45
EP 59
DI 10.1007/s41095-022-0307-3
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Z5WZ7
UT WOS:001112788200005
OA gold
DA 2024-08-05
ER

PT J
AU Li, YD
   Xiao, J
   Wang, YQ
   Lu, ZD
AF Li, Yidi
   Xiao, Jun
   Wang, Yiqun
   Lu, Zhengda
TI DepthGAN: GAN-based depth generation from semantic layouts
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE depth map generation; generative model; transformer; scene generation
AB Existing GAN-based generative methods are typically used for semantic image synthesis. We pose the question of whether GAN-based architectures can generate plausible depth maps and find that existing methods have difficulty in generating depth maps which reasonably represent 3D scene structure due to the lack of global geometric correlations. Thus, we propose DepthGAN, a novel method of generating a depth map using a semantic layout as input to aid construction, and manipulation of well-structured 3D scene point clouds. Specifically, we first build a feature generation model with a cascade of semantically-aware transformer blocks to obtain depth features with global structural information. For our semantically aware transformer block, we propose a mixed attention module and a semantically aware layer normalization module to better exploit semantic consistency for depth features generation. Moreover, we present a novel semantically weighted depth synthesis module, which generates adaptive depth intervals for the current scene. We generate the final depth map by using a weighted combination of semantically aware depth weights for different depth ranges. In this manner, we obtain a more accurate depth map. Extensive experiments on indoor and outdoor datasets demonstrate that DepthGAN achieves superior results both quantitatively and visually for the depth generation task.
C1 [Li, Yidi; Xiao, Jun; Lu, Zhengda] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Wang, Yiqun] Chongqing Univ, Coll Comp Sci, Chongqing, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chongqing University
RP Xiao, J (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.; Wang, YQ (corresponding author), Chongqing Univ, Coll Comp Sci, Chongqing, Peoples R China.
EM liyidi19@mails.ucas.ac.cn; xiaojun@ucas.ac.cn; yiqun.wang@cqu.edu.cn;
   luzhengda@ucas.ac.cn
FU National Natural Science Foundation of China [U21A20515, 62102393,
   62206263, 62271467]; Beijing Natural Science Foundation [4242053]
FX This work is supported by the National Natural Science Foundation of
   China (U21A20515, 62102393, 62206263, 62271467), and Beijing Natural
   Science Foundation (4242053).
CR Aleotti F, 2019, LECT NOTES COMPUT SC, V11129, P337, DOI 10.1007/978-3-030-11009-3_20
   Armeni Iro, 2017, arXiv
   Bhat S. F., 2022, ARXIV
   Bhat SF, 2021, PROC CVPR IEEE, P4008, DOI 10.1109/CVPR46437.2021.00400
   Brock A., 2018, ARXIV
   Brodt K, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530106
   Brown T., 2020, P 34 C NEURAL INFORM
   Chakravarty P, 2019, IEEE INT CONF ROBOT, P147, DOI [10.1109/icra.2019.8793530, 10.1109/ICRA.2019.8793530]
   Chen WL, 2018, PROC CVPR IEEE, P9416, DOI 10.1109/CVPR.2018.00981
   Cohen T., 2017, ARXIV
   Dhamo H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16332, DOI 10.1109/ICCV48922.2021.01604
   Dong XY, 2022, PROC CVPR IEEE, P12114, DOI 10.1109/CVPR52688.2022.01181
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Eigen D, 2014, ADV NEUR IN, V27
   Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268
   Facil JM, 2019, PROC CVPR IEEE, P11818, DOI 10.1109/CVPR.2019.01210
   Gaidon A, 2016, PROC CVPR IEEE, P4340, DOI 10.1109/CVPR.2016.470
   Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45
   Genova K, 2020, PROC CVPR IEEE, P4856, DOI 10.1109/CVPR42600.2020.00491
   Ghosh A, 2019, IEEE I CONF COMP VIS, P1171, DOI 10.1109/ICCV.2019.00126
   Goodfellow I. J., 2014, ADV NEURAL INFORM PR
   Heusel M., 2017, NeurIPS, P6629
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jia Zheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P519, DOI 10.1007/978-3-030-58545-7_30
   Jiang YF, 2021, ADV NEUR IN
   Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kingma D. P., 2014, arXiv
   Lee J, 2019, PR MACH LEARN RES, V97
   Lee K., 2021, ARXIV
   Li RH, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459766
   Liu X, 2019, ARXIV
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo A, 2020, PROC CVPR IEEE, P3753, DOI 10.1109/CVPR42600.2020.00381
   Luo WJ, 2016, ADV NEUR IN, V29
   Lv ZY, 2022, PROC CVPR IEEE, P11204, DOI 10.1109/CVPR52688.2022.01093
   Mittal P, 2022, PROC CVPR IEEE, P306, DOI 10.1109/CVPR52688.2022.00040
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196
   Ranftl R, 2022, IEEE T PATTERN ANAL, V44, P1623, DOI 10.1109/TPAMI.2020.3019967
   Reed S, 2016, PR MACH LEARN RES, V48
   Shengjie Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13113, DOI 10.1109/CVPR42600.2020.01313
   Sushko V., 2020, ARXIV
   Tan ZT, 2021, PROC CVPR IEEE, P7958, DOI 10.1109/CVPR46437.2021.00787
   Tan ZT, 2022, IEEE T PATTERN ANAL, V44, P4852, DOI 10.1109/TPAMI.2021.3076487
   Tang H, 2023, IEEE T PATTERN ANAL, V45, P768, DOI 10.1109/TPAMI.2022.3155989
   Tang Hao, 2020, P IEEE CVF C COMP VI, P7867
   Tateno K, 2018, LECT NOTES COMPUT SC, V11220, P732, DOI 10.1007/978-3-030-01270-0_43
   Vaswani A, 2021, PROC CVPR IEEE, P12889, DOI 10.1109/CVPR46437.2021.01270
   Wang R, 2019, PROC CVPR IEEE, P5647, DOI 10.1109/CVPR.2019.00570
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13729, DOI 10.1109/ICCV48922.2021.01349
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei XK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5785, DOI 10.1109/ICCV48922.2021.00575
   Wen C, 2019, IEEE I CONF COMP VIS, P1042, DOI 10.1109/ICCV.2019.00113
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Xie JW, 2021, PROC CVPR IEEE, P14971, DOI 10.1109/CVPR46437.2021.01473
   Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47
   Yin W, 2021, PROC CVPR IEEE, P204, DOI 10.1109/CVPR46437.2021.00027
   Zhang BW, 2022, PROC CVPR IEEE, P11294, DOI 10.1109/CVPR52688.2022.01102
   Zhou LQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5806, DOI 10.1109/ICCV48922.2021.00577
   Zhu PH, 2020, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR42600.2020.00515
NR 62
TC 0
Z9 0
U1 2
U2 2
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2024
VL 10
IS 3
BP 505
EP 522
DI 10.1007/s41095-023-0350-8
EA APR 2024
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QO3G5
UT WOS:001208966500001
OA gold
DA 2024-08-05
ER

PT J
AU Gao, ZR
   Yi, RJ
   Qin, Z
   Ye, YF
   Zhu, CY
   Xu, K
AF Gao, Zhirui
   Yi, Renjiao
   Qin, Zheng
   Ye, Yunfan
   Zhu, Chenyang
   Xu, Kai
TI Learning accurate template matching with differentiable coarse-to-fine
   correspondence refinement
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE template matching; differentiable homography; structure-awareness;
   transformers
ID HOMOGRAPHY; CONSENSUS
AB Template matching is a fundamental task in computer vision and has been studied for decades. It plays an essential role in manufacturing industry for estimating the poses of different parts, facilitating downstream tasks such as robotic grasping. Existing methods fail when the template and source images have different modalities, cluttered backgrounds, or weak textures. They also rarely consider geometric transformations via homographies, which commonly exist even for planar industrial parts. To tackle the challenges, we propose an accurate template matching method based on differentiable coarse-to-fine correspondence refinement. We use an edge-aware module to overcome the domain gap between the mask template and the grayscale image, allowing robust matching. An initial warp is estimated using coarse correspondences based on novel structure-aware information provided by transformers. This initial alignment is passed to a refinement network using references and aligned images to obtain sub-pixel level correspondences which are used to give the final geometric transformation. Extensive evaluation shows that our method to be significantly better than state-of-the-art methods and baselines, providing good generalization ability and visually plausible results even on unseen real data.
C1 [Gao, Zhirui; Yi, Renjiao; Qin, Zheng; Ye, Yunfan; Zhu, Chenyang; Xu, Kai] Natl Univ Def Technol, Coll Comp, Changsha 410073, Peoples R China.
C3 National University of Defense Technology - China
RP Xu, K (corresponding author), Natl Univ Def Technol, Coll Comp, Changsha 410073, Peoples R China.
EM gzrer2018@gmail.com; yirenjiao@nudt.edu.cn; qinzheng12@nudt.edu.cn;
   yunfan951202@gmail.com; zhuchenyang07@nudt.edu.cn;
   kevin.kai.xu@gmail.com
RI Ye, Yunfan/JJE-1619-2023
OI Yi, Renjiao/0000-0002-6057-1089
FU National Key R&D Program of China [2018AAA0102200]; National Natural
   Science Foundation of China [62002375, 62002376, 62325221, 62132021]
FX This work is supported in part by the National Key R&D Program of China
   (2018AAA0102200) and the National Natural Science Foundation of China
   (62002375, 62002376, 62325221, 62132021)
CR Bai XY, 2021, PROC CVPR IEEE, P15854, DOI 10.1109/CVPR46437.2021.01560
   BALLARD DH, 1981, PATTERN RECOGN, V13, P111, DOI 10.1016/0031-3203(81)90009-1
   Barath D, 2019, PROC CVPR IEEE, P10189, DOI 10.1109/CVPR.2019.01044
   Barath D, 2018, PROC CVPR IEEE, P6733, DOI 10.1109/CVPR.2018.00704
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Brachmann E, 2019, IEEE I CONF COMP VIS, P4321, DOI 10.1109/ICCV.2019.00442
   Brachmann E, 2017, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2017.267
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen HK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6281, DOI 10.1109/ICCV48922.2021.00624
   Chen Z, 2022, PROC CVPR IEEE, P13211, DOI 10.1109/CVPR52688.2022.01287
   Cheng JX, 2019, PROC CVPR IEEE, P11545, DOI 10.1109/CVPR.2019.01182
   Chum O, 2003, LECT NOTES COMPUT SC, V2781, P236
   Cuturi M., 2013, Ad-vances in Neural Information Processing Systems, V26, P1
   DeTone D, 2016, Arxiv, DOI arXiv:1606.03798
   DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060
   Dusmanu M, 2019, PROC CVPR IEEE, P8084, DOI 10.1109/CVPR.2019.00828
   Efe U, 2021, IEEE COMPUT SOC CONF, P4279, DOI 10.1109/CVPRW53098.2021.00484
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gao Bo, 2022, The International Conference on Image, Vision and Intelligent Systems (ICIVIS 2021). Lecture Notes in Electrical Engineering (813), P333, DOI 10.1007/978-981-16-6963-7_31
   Hartley R, 2003, Multiple view geometry in computer vision, DOI [10.1016/S0143-8166(01)00145-2, DOI 10.1017/CBO9780511811685]
   Hinterstoisser S, 2012, IEEE T PATTERN ANAL, V34, P876, DOI 10.1109/TPAMI.2011.206
   Hinterstoisser S, 2010, PROC CVPR IEEE, P2257, DOI 10.1109/CVPR.2010.5539908
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jau YY, 2020, IEEE INT C INT ROBOT, P4950, DOI 10.1109/IROS45743.2020.9341229
   Jiang B, 2022, PATTERN RECOGN, V121, DOI 10.1016/j.patcog.2021.108167
   Jiang W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6187, DOI 10.1109/ICCV48922.2021.00615
   Jirong Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P653, DOI 10.1007/978-3-030-58452-8_38
   Katharopoulos A, 2020, PR MACH LEARN RES, V119
   Kitaev Nikita, 2020, arXiv
   Koguciuk D, 2021, IEEE COMPUT SOC CONF, P4269, DOI 10.1109/CVPRW53098.2021.00483
   Lan YQ, 2022, COMPUT VIS MEDIA, V8, P395, DOI 10.1007/s41095-021-0252-6
   Lee MCH, 2019, LECT NOTES COMPUT SC, V11765, P337, DOI 10.1007/978-3-030-32245-8_38
   Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482
   Li Y, 2022, PROC CVPR IEEE, P5544, DOI 10.1109/CVPR52688.2022.00547
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lucas Bruce D, 1981, IJCAI 81, V81, DOI DOI 10.5555/1623264.1623280
   Luo ZX, 2020, PROC CVPR IEEE, P6588, DOI 10.1109/CVPR42600.2020.00662
   Mises R. V., 1929, J. Appl. Math. Mech., V9, P58, DOI 10.1002/zamm.19290090105
   Mok TCW, 2022, PROC CVPR IEEE, P20803, DOI 10.1109/CVPR52688.2022.02017
   Muja M., 2011, 2011 IEEE International Conference on Robotics and Automation (ICRA 2011), P2939, DOI 10.1109/ICRA.2011.5980153
   Parihar US, 2021, IEEE INT C INT ROBOT, P1593, DOI 10.1109/IROS51168.2021.9636619
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Qianqian Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P757, DOI 10.1007/978-3-030-58452-8_44
   Qin Z, 2022, PROC CVPR IEEE, P11133, DOI 10.1109/CVPR52688.2022.01086
   Quan SW, 2020, IEEE T GEOSCI REMOTE, V58, P7380, DOI 10.1109/TGRS.2020.2982221
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren Q., 2022, IEEE Geoscience and Remote Sensing Letters, V19
   Riba E, 2020, IEEE WINT CONF APPL, P3663, DOI 10.1109/WACV45572.2020.9093363
   Rocco I, 2018, ADV NEUR IN, V31
   Rocco I, 2019, IEEE T PATTERN ANAL, V41, P2553, DOI 10.1109/TPAMI.2018.2865351
   Roessle B, 2023, Arxiv, DOI arXiv:2205.01694
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499
   Shen Xi, 2020, COMPUTER VISION ECCV, P618, DOI DOI 10.1007/978-3-030-58548-836
   Shi Y, 2022, PROC CVPR IEEE, P12507, DOI 10.1109/CVPR52688.2022.01219
   Shu C, 2018, PROC SPIE, V10581, DOI 10.1117/12.2293264
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su JL, 2023, Arxiv, DOI arXiv:2104.09864
   Su Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5097, DOI 10.1109/ICCV48922.2021.00507
   Sun JM, 2021, PROC CVPR IEEE, P8918, DOI 10.1109/CVPR46437.2021.00881
   Suwanwimolkul S, 2022, PROCEEDINGS OF THE 2022 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2022, P330, DOI 10.1145/3512527.3531369
   Tay Y, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3530811
   Truong P, 2020, PROC CVPR IEEE, P6257, DOI 10.1109/CVPR42600.2020.00629
   Nguyen T, 2018, IEEE ROBOT AUTOM LET, V3, P2346, DOI 10.1109/LRA.2018.2809549
   Tyszkiewicz M., 2020, Advances in Neural Information Processing Systems, V33, P14254
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wu K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10013, DOI 10.1109/ICCV48922.2021.00988
   Wu Y, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1480, DOI 10.1145/3123266.3123411
   Yang JQ, 2021, IEEE T PATTERN ANAL, V43, P1859, DOI 10.1109/TPAMI.2019.2960234
   Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28
   Zhou QJ, 2022, LECT NOTES COMPUT SC, V13670, P407, DOI 10.1007/978-3-031-20080-9_24
NR 75
TC 0
Z9 0
U1 12
U2 13
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD APR
PY 2024
VL 10
IS 2
BP 309
EP 330
DI 10.1007/s41095-023-0333-9
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DW8M2
UT WOS:001135210800006
OA gold, Green Submitted
DA 2024-08-05
ER

PT J
AU Wang, JY
   Qi, Y
AF Wang, Junyi
   Qi, Yue
TI Multi-task learning and joint refinement between camera localization and
   object detection
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article; Early Access
DE visual localization; object detection; joint optimization; multi-task
   learning
ID VERSATILE
AB Visual localization and object detection both play important roles in various tasks. In many indoor application scenarios where some detected objects have fixed positions, the two techniques work closely together. However, few researchers consider these two tasks simultaneously, because of a lack of datasets and the little attention paid to such environments. In this paper, we explore multi-task network design and joint refinement of detection and localization. To address the dataset problem, we construct a medium indoor scene of an aviation exhibition hall through a semi-automatic process. The dataset provides localization and detection information, and is publicly available at https://drive.google.com/drive/folders/1U28zkuN4_I0dbzkqyIAKlAl5k9oUK0jI?usp=sharing for benchmarking localization and object detection tasks. Targeting this dataset, we have designed a multi-task network, JLDNet, based on YOLO v3, that outputs a target point cloud and object bounding boxes. For dynamic environments, the detection branch also promotes the perception of dynamics. JLDNet includes image feature learning, point feature learning, feature fusion, detection construction, and point cloud regression. Moreover, object-level bundle adjustment is used to further improve localization and detection accuracy. To test JLDNet and compare it to other methods, we have conducted experiments on 7 static scenes, our constructed dataset, and the dynamic TUM RGB-D and Bonn datasets. Our results show state-of-the-art accuracy for both tasks, and the benefit of jointly working on both tasks is demonstrated.
C1 [Wang, Junyi; Qi, Yue] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Wang, Junyi; Qi, Yue] Peng Cheng Lab, Shenzhen 518052, Peoples R China.
   [Qi, Yue] Beihang Univ, Qingdao Res Inst, Qingdao 266104, Peoples R China.
   [Wang, Junyi] Shandong Univ, Sch Comp Sci & Technol, Qingdao, Peoples R China.
C3 Beihang University; Peng Cheng Laboratory; Beihang University; Shandong
   University
RP Qi, Y (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.; Qi, Y (corresponding author), Peng Cheng Lab, Shenzhen 518052, Peoples R China.; Qi, Y (corresponding author), Beihang Univ, Qingdao Res Inst, Qingdao 266104, Peoples R China.
EM qy@buaa.edu.cn
RI qi, yue/KLE-0386-2024
FU National Natural Science Foundation of China [62072020]; Key-Area
   Research and the Leading Talents in Innovation and Entrepreneurship of
   Qingdao [19-3-2-21-zhc]
FX This paper was supported by the National Natural Science Foundation of
   China (No. 62072020), and Key-Area Research and the Leading Talents in
   Innovation and Entrepreneurship of Qingdao (No. 19-3-2-21-zhc).
CR Balntas V, 2018, LECT NOTES COMPUT SC, V11218, P782, DOI 10.1007/978-3-030-01264-9_46
   Bao W, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2803-x
   Bescos B, 2018, IEEE ROBOT AUTOM LET, V3, P4076, DOI 10.1109/LRA.2018.2860039
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Brachmann E, 2022, IEEE T PATTERN ANAL, V44, P5847, DOI 10.1109/TPAMI.2021.3070754
   Brachmann E, 2018, PROC CVPR IEEE, P4654, DOI 10.1109/CVPR.2018.00489
   Brachmann E, 2017, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2017.267
   Brachmann E, 2016, PROC CVPR IEEE, P3364, DOI 10.1109/CVPR.2016.366
   Brahmbhatt S, 2018, PROC CVPR IEEE, P2616, DOI 10.1109/CVPR.2018.00277
   Campos C, 2021, IEEE T ROBOT, V37, P1874, DOI 10.1109/TRO.2021.3075644
   Cavallari T, 2020, IEEE T PATTERN ANAL, V42, P2465, DOI 10.1109/TPAMI.2019.2915068
   Cavallari T, 2017, PROC CVPR IEEE, P218, DOI 10.1109/CVPR.2017.31
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Dong SY, 2021, PROC CVPR IEEE, P8540, DOI 10.1109/CVPR46437.2021.00844
   Du ZJ, 2022, IEEE T VIS COMPUT GR, V28, P1745, DOI 10.1109/TVCG.2020.3028218
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Guzman-Rivera A, 2014, PROC CVPR IEEE, P1114, DOI 10.1109/CVPR.2014.146
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Huang JH, 2021, COMPUT VIS MEDIA, V7, P87, DOI 10.1007/s41095-020-0195-3
   Huang SS, 2023, IEEE T VIS COMPUT GR, V29, P1977, DOI 10.1109/TVCG.2021.3137912
   Jocher G., 2020, Yolo v5
   Kendall A, 2017, PROC CVPR IEEE, P6555, DOI 10.1109/CVPR.2017.694
   Kendall A, 2016, IEEE INT CONF ROBOT, P4762, DOI 10.1109/ICRA.2016.7487679
   Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   Kingma D. P., 2014, arXiv
   Lan YQ, 2022, COMPUT VIS MEDIA, V8, P395, DOI 10.1007/s41095-021-0252-6
   Laskar Z, 2017, IEEE INT CONF COMP V, P920, DOI 10.1109/ICCVW.2017.113
   Liu L, 2017, IEEE I CONF COMP VIS, P2391, DOI 10.1109/ICCV.2017.260
   Liu Shuang, 2017, [Computational Visual Media, 计算可视媒体], V3, P33
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Melekhov I, 2017, IEEE INT CONF COMP V, P870, DOI 10.1109/ICCVW.2017.107
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Nakajima Yoshikatsu, 2017, [Computational Visual Media, 计算可视媒体], V3, P189
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Palazzolo E, 2019, IEEE INT C INT ROBOT, P7855, DOI [10.1109/IROS40897.2019.8967590, 10.1109/iros40897.2019.8967590]
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qin T, 2018, IEEE T ROBOT, V34, P1004, DOI 10.1109/TRO.2018.2853729
   Radwan N, 2018, IEEE ROBOT AUTOM LET, V3, P4407, DOI 10.1109/LRA.2018.2869640
   Redmon J., 2018, CoRR
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Rünz M, 2018, INT SYM MIX AUGMENT, P10, DOI 10.1109/ISMAR.2018.00024
   Sattler T, 2017, IEEE T PATTERN ANAL, V39, P1744, DOI 10.1109/TPAMI.2016.2611662
   Schmidt T, 2017, IEEE ROBOT AUTOM LET, V2, P420, DOI 10.1109/LRA.2016.2634089
   Shotton J, 2013, PROC CVPR IEEE, P2930, DOI 10.1109/CVPR.2013.377
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   Taira H, 2021, IEEE T PATTERN ANAL, V43, P1293, DOI 10.1109/TPAMI.2019.2952114
   Tang ST, 2021, PROC CVPR IEEE, P1831, DOI 10.1109/CVPR46437.2021.00187
   Tchapmi LP, 2019, PROC CVPR IEEE, P383, DOI 10.1109/CVPR.2019.00047
   Valada A, 2018, IEEE INT CONF ROBOT, P6939, DOI 10.1109/ICRA.2018.8462979
   Valentin J, 2015, PROC CVPR IEEE, P4400, DOI 10.1109/CVPR.2015.7299069
   Walch F, 2017, IEEE I CONF COMP VIS, P627, DOI 10.1109/ICCV.2017.75
   Wang B, 2020, AAAI CONF ARTIF INTE, V34, P10393
   Wang Chao, 2017, [Computational Visual Media, 计算可视媒体], V3, P95
   Wang JY, 2021, IEEE INT CONF ROBOT, P5891, DOI 10.1109/ICRA48506.2021.9561289
   Xiaotian Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11980, DOI 10.1109/CVPR42600.2020.01200
   Xu BB, 2019, IEEE INT CONF ROBOT, P5231, DOI [10.1109/icra.2019.8794371, 10.1109/ICRA.2019.8794371]
   Xue F, 2019, IEEE I CONF COMP VIS, P2841, DOI 10.1109/ICCV.2019.00293
   Yan FH, 2022, COMPUT VIS MEDIA, V8, P467, DOI 10.1007/s41095-021-0251-7
   Yang LW, 2019, IEEE I CONF COMP VIS, P42, DOI 10.1109/ICCV.2019.00013
   Yang SC, 2019, IEEE T ROBOT, V35, P925, DOI 10.1109/TRO.2019.2909168
   Yu C, 2018, IEEE INT C INT ROBOT, P1168, DOI 10.1109/IROS.2018.8593691
   Zheng T, 2022, IEEE T PATTERN ANAL, V44, P2328, DOI 10.1109/TPAMI.2020.3042881
   Zhou L, 2020, PROC CVPR IEEE, P4918, DOI 10.1109/CVPR42600.2020.00497
   Zou ZX, 2022, GRAPH MODELS, V123, DOI 10.1016/j.gmod.2022.101165
NR 71
TC 0
Z9 0
U1 16
U2 16
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD 2024 FEB 8
PY 2024
DI 10.1007/s41095-022-0319-z
EA FEB 2024
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HK9T3
UT WOS:001159523000003
OA gold
DA 2024-08-05
ER

PT J
AU Wang, ZN
   Ho, TY
   Xiao, Y
   Leung, CS
AF Wang, Zhenni
   Ho, Tze Yui
   Xiao, Yi
   Leung, Chi Sing
TI Temporal vectorized visibility for direct illumination of animated
   models
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article; Early Access
DE visibility; visibility-boundary edge; line sampling; ray tracing
AB Direct illumination rendering is an important technique in computer graphics. Precomputed radiance transfer algorithms can provide high quality rendering results in real time, but they can only support rigid models. On the other hand, ray tracing algorithms are flexible and can gracefully handle animated models. With NVIDIA RTX and the AI denoiser, we can use ray tracing algorithms to render visually appealing results in real time. Visually appealing though, they can deviate from the actual one considerably. We propose a visibility-boundary edge oriented infinite triangle bounding volume hierarchy (BVH) traversal algorithm to dynamically generate visibility in vector form. Our algorithm utilizes the properties of visibility-boundary edges and infinite triangle BVH traversal to maximize the efficiency of the vector form visibility generation. A novel data structure, temporal vectorized visibility, is proposed, which allows visibility in vector form to be shared across time and further increases the generation efficiency. Our algorithm can efficiently render close-to-reference direct illumination results. With the similar processing time, it provides a visual quality improvement around 10 dB in terms of peak signal-to-noise ratio (PSNR) w.r.t. the ray tracing algorithm reservoir-based spatiotemporal importance resampling (ReSTIR).
C1 [Wang, Zhenni; Ho, Tze Yui; Leung, Chi Sing] City Univ Hong Kong, Dept Elect Engn, Kowloon, Hong Kong, Peoples R China.
   [Xiao, Yi] Hunan Univ, Sch Design, Changsha, Peoples R China.
C3 City University of Hong Kong; Hunan University
RP Xiao, Y (corresponding author), Hunan Univ, Sch Design, Changsha, Peoples R China.
EM znwang3@cityu.edu.hk; mahty@hotmail.com; yixiao_csee@hnu.edu.cn;
   eeleungc@cityu.edu.hk
RI Wang, Zhenni/KTI-0913-2024
OI WANG, Zhenni/0000-0001-8717-3606
FU Innovation and Technology Fund from the Innovation and Technology
   Commission of the Hong Kong Special Administrative Region [ITS/065/21]
FX This work was partially supported by the Innovation and Technology Fund
   from the Innovation and Technology Commission of the Hong Kong Special
   Administrative Region (Fund Number ITS/065/21).
CR Akerlund O, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P161, DOI 10.1109/PG.2007.30
   Andersson P, 2020, P ACM COMPUT GRAPH, V3, DOI 10.1145/3406183
   Annen T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360633
   Annen Thomas., 2007, Proc. Eurographics Symposium on Ren- dering, P51
   [Anonymous], NVIDIA RTX platform
   Appel Arthur., 1967, Proceedings of the ACM National Conference, P387
   Bako S, 2017, ACM T GRAPHIC, V36, DOI [10.1145/3072959.3073703, 10.1145/3072959.3073708]
   BERGERON P, 1986, IEEE COMPUT GRAPH, V6, P17, DOI 10.1109/MCG.1986.276543
   Billen N, 2016, COMPUT GRAPH FORUM, V35, P45, DOI 10.1111/cgf.12948
   Bitterli B, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392481
   Chaitanya CRA, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073601
   Cook R. L., 1984, Computers & Graphics, V18, P137
   COOK RL, 1986, ACM T GRAPHIC, V5, P51, DOI 10.1145/7529.8927
   Crow F. C., 1984, Computers & Graphics, V18, P207
   Debevec P. E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P369, DOI 10.1145/258734.258884
   Gribel CJ, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964949
   Ho TY, 2015, IEEE T VIS COMPUT GR, V21, P945, DOI 10.1109/TVCG.2015.2407398
   Kajiya J. T., 1986, Computer Graphics, V20, P143, DOI 10.1145/15886.15902
   Kalantari NK, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766977
   Kautz J., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P291
   Kt A., 2021, P EUR S REND, P111
   Laine S, 2005, ACM T GRAPHIC, V24, P1156, DOI 10.1145/1073204.1073327
   Lehtinen J, 2006, COMPUT GRAPH FORUM, V25, P303, DOI 10.1111/j.1467-8659.2006.00949.x
   McGuire M., 2004, Journal of Graphics Tools, V9, P1, DOI 10.1080/10867651.2004.10487594
   Meister D, 2018, IEEE T VIS COMPUT GR, V24, P1345, DOI 10.1109/TVCG.2017.2669983
   Moller T.A., 2001, J GRAPHICS TOOLS, V6, P29, DOI [DOI 10.1080/10867651.2001.10487535, 10.1080/10867651.2001.10487535]
   Moreau P., 2019, P C HIGH PERF GRAPH, P21, DOI [10.2312/hpg.20191191, DOI 10.2312/HPG.20191191]
   Ng R, 2003, ACM T GRAPHIC, V22, P376, DOI 10.1145/882262.882280
   Nowrouzezahrai D, 2014, COMPUT GRAPH FORUM, V33, P105, DOI 10.1111/cgf.12257
   Ritschel Tobias., 2008, Proceedings of Graphics Interface, P185
   Sen P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2167076.2167083
   Sloan PP, 2005, ACM T GRAPHIC, V24, P1216, DOI 10.1145/1073204.1073335
   Sloan PP, 2003, ACM T GRAPHIC, V22, P382, DOI 10.1145/882262.882281
   Sloan PP, 2002, ACM T GRAPHIC, V21, P527, DOI 10.1145/566570.566612
   Talbot JustinF., 2005, RENDERING TECHNIQUES, P139, DOI DOI 10.2312/EGWR/EGSR05/139-146
   Veach E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P419, DOI 10.1145/218380.218498
   Wang ZN, 2019, COMPUT GRAPH FORUM, V38, P437, DOI 10.1111/cgf.13850
   Whitted T., 1979, P 6 ANN C COMP GRAPH, V14
   Xiao Y, 2013, COMPUT GRAPH FORUM, V32, P201, DOI 10.1111/cgf.12085
   Zhou K, 2005, ACM T GRAPHIC, V24, P1196, DOI 10.1145/1073204.1073332
   Zhou Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3452097
NR 41
TC 0
Z9 0
U1 0
U2 0
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD 2024 MAY 29
PY 2024
DI 10.1007/s41095-023-0339-3
EA MAY 2024
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL2V8
UT WOS:001234552300001
OA gold
DA 2024-08-05
ER

PT J
AU Shi, M
   Feng, WK
   Gao, L
   Zhu, DM
AF Shi, Min
   Feng, Wenke
   Gao, Lin
   Zhu, Dengming
TI Generating diverse clothed 3D human animations via a generative model
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE Transformer; garment animation; conditional variational autoencoder
   (CVAE); computer graphics
AB Data-driven garment animation is a current topic of interest in the computer graphics industry. Existing approaches generally establish the mapping between a single human pose or a temporal pose sequence, and garment deformation, but it is difficult to quickly generate diverse clothed human animations. We address this problem with a method to automatically synthesize dressed human animations with temporal consistency from a specified human motion label. At the heart of our method is a two-stage strategy. Specifically, we first learn a latent space encoding the sequence-level distribution of human motions utilizing a transformer-based conditional variational autoencoder (Transformer-CVAE). Then a garment simulator synthesizes dynamic garment shapes using a transformer encoder-decoder architecture. Since the learned latent space comes from varied human motions, our method can generate a variety of styles of motions given a specific motion label. By means of a novel beginning of sequence (BOS) learning strategy and a self-supervised refinement procedure, our garment simulator is capable of efficiently synthesizing garment deformation sequences corresponding to the generated human motions while maintaining temporal and spatial consistency. We verify our ideas experimentally. This is the first generative model that directly dresses human animation.
C1 [Shi, Min; Feng, Wenke] North China Elect Power Univ, Sch Control & Comp Engn, Beijing 102206, Peoples R China.
   [Gao, Lin; Zhu, Dengming] Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
C3 North China Electric Power University; Chinese Academy of Sciences;
   Institute of Computing Technology, CAS
RP Zhu, DM (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
EM mdzhu@ict.ac.cn
FU National Natural Science Foundation of China [61972379]
FX We thank the volunteers for the user study. This work was supported by
   the National Natural Science Foundation of China (Grant No. 61972379).
CR Agarap A.F., 2018, arXiv, DOI 10.48550/arXiv.1803.08375
   Ahn H, 2018, IEEE INT CONF ROBOT, P5915
   Ahuja C, 2019, INT CONF 3D VISION, P719, DOI 10.1109/3DV.2019.00084
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Barsoum E, 2018, IEEE COMPUT SOC CONF, P1499, DOI 10.1109/CVPRW.2018.00191
   Bertiche Hugo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P344, DOI 10.1007/978-3-030-58565-5_21
   Bertiche H, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480479
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Guan P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185531
   Gundogdu E, 2019, IEEE I CONF COMP VIS, P8738, DOI 10.1109/ICCV.2019.00883
   Guo CA, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2021, DOI 10.1145/3394171.3413635
   Habibie I., 2017, P 28 BRIT MACH VIS C, V119
   Higgins I., 2017, INT C LEARN REPR
   Jiang JY, 2020, INT CONF ACOUST SPEE, P516, DOI [10.1109/icassp40776.2020.9054554, 10.1109/ICASSP40776.2020.9054554]
   Kingma D. P., 2014, arXiv
   Kumar S, 2021, Arxiv, DOI arXiv:2105.04458
   Lähner Z, 2018, LECT NOTES COMPUT SC, V11208, P698, DOI 10.1007/978-3-030-01225-0_41
   Lee H.-Y., 2019, Advances in Neural Information Processing Systems, P3586
   Li C, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417763
   Li Jiaman, 2020, arXiv
   Li YD, 2022, COMPUT GRAPH FORUM, V41, P547, DOI 10.1111/cgf.14493
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Ma QL, 2020, PROC CVPR IEEE, P6468, DOI 10.1109/CVPR42600.2020.00650
   Mahmood N, 2019, IEEE I CONF COMP VIS, P5441, DOI 10.1109/ICCV.2019.00554
   Narain R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366171
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Pan X. Y., 2022, P ACM SIGGRAPH C
   Paszke A., 2017, P 31 C NEUR INF PROC, P1, DOI DOI 10.1017/CB09781107707221.009
   Patel Chaitanya, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7363, DOI 10.1109/CVPR42600.2020.00739
   Petrovich M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10965, DOI 10.1109/ICCV48922.2021.01080
   Provot Xavier, 1997, P EUR WORKSH COMP AN, P177
   Ravi N, 2020, Arxiv, DOI arXiv:2007.08501
   Santesteban I, 2022, PROC CVPR IEEE, P8130, DOI 10.1109/CVPR52688.2022.00797
   Santesteban I, 2021, PROC CVPR IEEE, P11758, DOI 10.1109/CVPR46437.2021.01159
   Santesteban I, 2019, COMPUT GRAPH FORUM, V38, P355, DOI 10.1111/cgf.13643
   Taubin G., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P351, DOI 10.1145/218380.218473
   Tiwari L, 2021, IEEE INT CONF COMP V, P1416, DOI 10.1109/ICCVW54120.2021.00163
   Vása L, 2011, IEEE T VIS COMPUT GR, V17, P220, DOI 10.1109/TVCG.2010.38
   Vaswani A, 2017, ADV NEUR IN, V30
   Vidaurre R, 2020, COMPUT GRAPH FORUM, V39, P145, DOI 10.1111/cgf.14109
   Volino P., 1995, Computer Animation and Simulation '95. Proceedings of the Eurographics Workshop, P55
   Wang HM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778844
   Wang TM, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5233
   Wang TY, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356512
   Wen YH, 2021, PROC CVPR IEEE, P13607, DOI 10.1109/CVPR46437.2021.01340
   Wu NN, 2021, IEEE T VIS COMPUT GR, V27, P4107, DOI 10.1109/TVCG.2021.3106429
   Xu WW, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601136
   Wang TY, 2018, Arxiv, DOI arXiv:1806.11335
   Zhang M, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480497
   Zhou Y, 2019, PROC CVPR IEEE, P5738, DOI 10.1109/CVPR.2019.00589
NR 51
TC 1
Z9 1
U1 3
U2 4
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD APR
PY 2024
VL 10
IS 2
BP 261
EP 277
DI 10.1007/s41095-022-0324-2
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DW8M2
UT WOS:001135210800004
OA gold
DA 2024-08-05
ER

PT J
AU Xi, YF
   Zhu, CY
   Duan, Y
   Yi, RJ
   Zheng, LT
   He, HJ
   Xu, K
AF Xi, Yuefeng
   Zhu, Chenyang
   Duan, Yao
   Yi, Renjiao
   Zheng, Lintao
   He, Hongjun
   Xu, Kai
TI THP: Tensor-field-driven hierarchical path planning for autonomous scene
   exploration with depth sensors
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article; Early Access
DE tensor field; indoor scene exploration; path planning; trajectory
   optimization
ID AVOIDANCE
AB It is challenging to automatically explore an unknown 3D environment with a robot only equipped with depth sensors due to the limited field of view. We introduce THP, a tensor field-based framework for efficient environment exploration which can better utilize the encoded depth information through the geometric characteristics of tensor fields. Specifically, a corresponding tensor field is constructed incrementally and guides the robot to formulate optimal global exploration paths and a collision-free local movement strategy. Degenerate points generated during the exploration are adopted as anchors to formulate a hierarchical TSP for global path optimization. This novel strategy can help the robot avoid long-distance round trips more effectively while maintaining scanning completeness. Furthermore, the tensor field also enables a local movement strategy to avoid collision based on particle advection. As a result, the framework can eliminate massive, time-consuming recalculations of local movement paths. We have experimentally evaluate our method with a ground robot in 8 complex indoor scenes. Our method can on average achieve 14% better exploration efficiency and 21% better exploration completeness than state-of-the-art alternatives using LiDAR scans. Moreover, compared to similar methods, our method makes path decisions 39% faster due to our hierarchical exploration strategy.
C1 [Xi, Yuefeng; Zhu, Chenyang; Duan, Yao; Yi, Renjiao; Zheng, Lintao; He, Hongjun; Xu, Kai] Natl Univ Def Technol, Coll Comp, Changsha 410073, Peoples R China.
C3 National University of Defense Technology - China
RP Zhu, CY; Xu, K (corresponding author), Natl Univ Def Technol, Coll Comp, Changsha 410073, Peoples R China.
EM xiyuefeng@nudt.edu.cn; zhuchenyang07@nudt.edu.cn;
   duanayao16@nudt.edu.cn; yirenjiao@nudt.edu.cn;
   zhenglintao13@nudt.edu.cn; hhjhi@nudt.edu.cn; kevin.kai.xu@gmail.com
FU National Natural Science Foundation of China [62372457, 62002375,
   62002376]; Young Elite Scientists Sponsorship Program by CAST
   [2023QNRC001]; Natural Science Foundation of Hunan Province of China
   [2021RC3071]
FX This work is supported in part by the National Natural Science
   Foundation of China (62372457, 62002375, 62002376), Young Elite
   Scientists Sponsorship Program by CAST (2023QNRC001), and the Natural
   Science Foundation of Hunan Province of China (2021RC3071).
CR Bai S, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P1816, DOI 10.1109/IROS.2016.7759289
   Bourgault F, 2002, 2002 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-3, PROCEEDINGS, P540, DOI 10.1109/IRDS.2002.1041446
   Cao C, 2021, ROBOT SCI SYS
   Chang A, 2017, INT CONF 3D VISION, P667, DOI 10.1109/3DV.2017.00081
   Höller B, 2021, COMPUT VIS MEDIA, V7, P71, DOI 10.1007/s41095-020-0194-4
   Holz D., 2010, ISR, P36
   KHATIB O, 1986, INT J ROBOT RES, V5, P90, DOI 10.1177/027836498600500106
   KOREN Y, 1991, 1991 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-3, P1398, DOI 10.1109/ROBOT.1991.131810
   Kulich M, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19061400
   Kulich M, 2011, IEEE INT CONF ROBOT, DOI 10.1109/ICRA.2011.5980221
   Ok K, 2013, IEEE INT CONF ROBOT, P4596, DOI 10.1109/ICRA.2013.6631230
   PAPADIMITRIOU CH, 1992, SIAM J COMPUT, V21, P450, DOI 10.1137/0221030
   Petrovic I., 2014, IFAC Proc. Vol, V47, P10188, DOI [10.3182/20140824-6-ZA-1003.01275, DOI 10.3182/20140824-6-ZA-1003.01275]
   Rani M, 2015, KNOWL-BASED SYST, V90, P33, DOI 10.1016/j.knosys.2015.10.002
   Senarathne PGCN, 2016, IEEE INT SYMP SAFE, P34, DOI 10.1109/SSRR.2016.7784274
   Shade Robbie, 2011, 2011 IEEE International Conference on Robotics and Automation (ICRA 2011), P2806, DOI 10.1109/ICRA.2011.5980121
   Umari H, 2017, IEEE INT C INT ROBOT, P1396, DOI 10.1109/IROS.2017.8202319
   Xu K, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130812
   Yamauchi B, 1997, 1997 IEEE INTERNATIONAL SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN ROBOTICS AND AUTOMATION - CIRA '97, PROCEEDINGS, P146, DOI 10.1109/CIRA.1997.613851
   Zeng R, 2020, COMPUT VIS MEDIA, V6, P225, DOI 10.1007/s41095-020-0179-3
   Zhang E, 2007, IEEE T VIS COMPUT GR, V13, P94, DOI 10.1109/TVCG.2007.16
   Zhang J, 2020, J FIELD ROBOT, V37, P1300, DOI 10.1002/rob.21952
NR 22
TC 0
Z9 0
U1 0
U2 0
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD 2024 MAY 18
PY 2024
DI 10.1007/s41095-022-0312-6
EA MAY 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RM8Z2
UT WOS:001228184100001
OA gold
DA 2024-08-05
ER

PT J
AU Xu, MY
   Zhou, ZP
   Wang, YL
   Qiao, Y
AF Xu, Mingye
   Zhou, Zhipeng
   Wang, Yali
   Qiao, Yu
TI Towards robustness and generalization of point cloud representation: A
   geometry coding method and a large-scale object-level dataset
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE geometry coding; self-supervised learning; point cloud; classification;
   segmentation; 3D analysis
ID SEGMENTATION
AB Robustness and generalization are two challenging problems for learning point cloud representation. To tackle these problems, we first design a novel geometry coding model, which can effectively use an invariant eigengraph to group points with similar geometric information, even when such points are far from each other. We also introduce a large-scale point cloud dataset, PCNet184. It consists of 184 categories and 51,915 synthetic objects, which brings new challenges for point cloud classification, and provides a new benchmark to assess point cloud cross-domain generalization. Finally, we perform extensive experiments on point cloud classification, using ModelNet40, ScanObjectNN, and our PCNet184, and segmentation, using ShapeNetPart and S3DIS. Our method achieves comparable performance to state-of-the-art methods on these datasets, for both supervised and unsupervised learning. Code and our dataset are available at https://github.com/MingyeXu/PCNet184.
C1 [Xu, Mingye; Wang, Yali; Qiao, Yu] Chinese Acad Sci, Shenzhen Inst Adv Technol, Guangdong Hong Kong Macao Joint Lab Human Machine, Shenzhe 518000, Peoples R China.
   [Xu, Mingye] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Qiao, Yu] Shanghai AI Lab, Shanghai 200001, Peoples R China.
   [Wang, Yali] Shenzhen Inst Artificial Intelligence & Robot Soc, SIAT Branch, Shenzhen 518000, Peoples R China.
   [Zhou, Zhipeng] Alibaba DAMO Acad, Hangzhou 242332, Peoples R China.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS; Shanghai Artificial Intelligence Laboratory; Shenzhen
   Institute of Artificial Intelligence & Robotics for Society
RP Wang, YL; Qiao, Y (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Guangdong Hong Kong Macao Joint Lab Human Machine, Shenzhe 518000, Peoples R China.; Qiao, Y (corresponding author), Shanghai AI Lab, Shanghai 200001, Peoples R China.; Wang, YL (corresponding author), Shenzhen Inst Artificial Intelligence & Robot Soc, SIAT Branch, Shenzhen 518000, Peoples R China.
EM my.xu@siat.ac.cn; zhipeng.zhou@alibaba-inc.com; yl.wang@siat.ac.cn;
   yu.qiao@siat.ac.cn
FU National Natural Science Foundation of China [61876176, U1813218]; Joint
   Lab of CAS; Shenzhen Research Program [RCJC20200714114557087]; Shanghai
   Committee of Science and Technology [21DZ1100100]; Shenzhen Institute of
   Artificial Intelligence and Robotics for Society
FX This work was partially supported by the National Natural Science
   Foundation of China (Grant Nos. 61876176 and U1813218), the Joint Lab of
   CAS-HK, the Shenzhen Research Program (Grant No. RCJC20200714114557087),
   the Shanghai Committee of Science and Technology (Grant No.
   21DZ1100100), and Shenzhen Institute of Artificial Intelligence and
   Robotics for Society.
CR Achituve I, 2021, IEEE WINT CONF APPL, P123, DOI 10.1109/WACV48630.2021.00017
   Alliegro A, 2021, INT C PATT RECOG, P6718, DOI 10.1109/ICPR48806.2021.9412483
   Alqazzaz S, 2019, COMPUT VIS MEDIA, V5, P209, DOI 10.1007/s41095-019-0139-y
   Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Atzmon M, 2018, Arxiv, DOI arXiv:1803.10091
   Bachman P, 2019, ADV NEUR IN, V32
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Hua BS, 2018, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2018.00109
   Boulch A, 2020, COMPUT GRAPH-UK, V88, P24, DOI 10.1016/j.cag.2020.02.005
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Doersch C, 2017, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2017.226
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Gadelha M, 2018, LECT NOTES COMPUT SC, V11211, P105, DOI 10.1007/978-3-030-01234-2_7
   Ganin Y, 2017, ADV COMPUT VIS PATT, P189, DOI 10.1007/978-3-319-58347-1_10
   Groh F, 2020, Arxiv, DOI arXiv:1803.07289
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Han WK, 2022, COMPUT VIS MEDIA, V8, P585, DOI 10.1007/s41095-021-0260-6
   Han WK, 2020, AAAI CONF ARTIF INTE, V34, P10925
   Han XF, 2021, Arxiv, DOI arXiv:2104.13636
   Hassani K, 2019, IEEE I CONF COMP VIS, P8159, DOI 10.1109/ICCV.2019.00825
   Henaff O. J., 2019, ARXIV190509272
   Huang QG, 2018, PROC CVPR IEEE, P2626, DOI 10.1109/CVPR.2018.00278
   Kingma D. P., 2014, arXiv
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Komarichev A, 2019, PROC CVPR IEEE, P7413, DOI 10.1109/CVPR.2019.00760
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Li YZ, 2018, ADV NEUR IN, V31
   Lin YQ, 2020, PROC CVPR IEEE, P4292, DOI 10.1109/CVPR42600.2020.00435
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Ma X, 2022, Arxiv, DOI arXiv:2202.07123
   Mo KC, 2019, PROC CVPR IEEE, P909, DOI 10.1109/CVPR.2019.00100
   Pan L, 2021, PROC CVPR IEEE, P8520, DOI 10.1109/CVPR46437.2021.00842
   Qi CR, 2017, ADV NEUR IN, V30
   Qin C., 2019, Adv. Neural Inf. Process. Syst., V32, P7192
   Qingyong Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11105, DOI 10.1109/CVPR42600.2020.01112
   Rao YM, 2020, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR42600.2020.00542
   Sauder J., 2019, ADV NEURAL INF PROCE, P12962
   Shen YR, 2018, PROC CVPR IEEE, P4548, DOI 10.1109/CVPR.2018.00478
   Simonovsky M, 2017, PROC CVPR IEEE, P29, DOI 10.1109/CVPR.2017.11
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Uy MA, 2019, IEEE I CONF COMP VIS, P1588, DOI 10.1109/ICCV.2019.00167
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P56, DOI 10.1007/978-3-030-01225-0_4
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Chang AX, 2015, Arxiv, DOI [arXiv:1512.03012, DOI 10.48550/ARXIV.1512.03012]
   Xie SN, 2018, PROC CVPR IEEE, P4606, DOI 10.1109/CVPR.2018.00484
   Xu M., 2022, arXiv
   Xu MY, 2021, AAAI CONF ARTIF INTE, V35, P3047
   Xu MY, 2020, AAAI CONF ARTIF INTE, V34, P12500
   Xu MT, 2021, PROC CVPR IEEE, P3172, DOI 10.1109/CVPR46437.2021.00319
   Xu MT, 2021, AAAI CONF ARTIF INTE, V35, P3056
   Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6
   Yan X, 2020, PROC CVPR IEEE, P5588, DOI 10.1109/CVPR42600.2020.00563
   Yang JC, 2019, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2019.00344
   Yang Sheng, 2017, [Computational Visual Media, 计算可视媒体], V3, P131
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238
   Yonglong Tian, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P776, DOI 10.1007/978-3-030-58621-8_45
   Yunlu Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P330, DOI 10.1007/978-3-030-58580-8_20
   Zhang JH, 2021, IEEE T IMAGE PROCESS, V30, P7914, DOI 10.1109/TIP.2021.3109517
   Zhang M, 2020, IEEE I C VI COM I PR, P144, DOI 10.1109/vcip49819.2020.9301786
   Zhang M, 2020, IEEE T MULTIMEDIA, V22, P1744, DOI 10.1109/TMM.2019.2963592
   Zhang ZY, 2019, IEEE I CONF COMP VIS, P1607, DOI 10.1109/ICCV.2019.00169
   Zhao HS, 2019, PROC CVPR IEEE, P5550, DOI 10.1109/CVPR.2019.00571
   Zhao YH, 2019, PROC CVPR IEEE, P1009, DOI 10.1109/CVPR.2019.00110
   Zhou F, 2020, arXiv
NR 70
TC 0
Z9 0
U1 6
U2 9
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD FEB
PY 2024
VL 10
IS 1
BP 27
EP 43
DI 10.1007/s41095-022-0305-5
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Z5WZ7
UT WOS:001112788200003
OA gold
DA 2024-08-05
ER

PT J
AU Xian, CH
   Li, JX
   Wu, H
   Lin, ZS
   Li, GQ
AF Xian, Chuhua
   Li, Jiaxin
   Wu, Hao
   Lin, Zisen
   Li, Guiqing
TI Delving into high-quality SVBRDF acquisition: A new setup and method
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE acquisition setup; SVBRDF acquisition; material capture; global skip
   connection
AB In this study, we present a new and innovative framework for acquiring high-quality SVBRDF maps. Our approach addresses the limitations of the current methods and proposes a new solution. The core of our method is a simple hardware setup consisting of a consumer-level camera, LED lights, and a carefully designed network that can accurately obtain the high-quality SVBRDF properties of a nearly planar object. By capturing a flexible number of images of an object, our network uses different subnetworks to train different property maps and employs appropriate loss functions for each of them. To further enhance the quality of the maps, we improved the network structure by adding a novel skip connection that connects the encoder and decoder with global features. Through extensive experimentation using both synthetic and real-world materials, our results demonstrate that our method outperforms previous methods and produces superior results. Furthermore, our proposed setup can also be used to acquire physically based rendering maps of special materials.
C1 [Xian, Chuhua; Li, Jiaxin; Li, Guiqing] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Wu, Hao; Lin, Zisen] Guangdong Shidi Intelligence Technol Ltd, Guangzhou 510000, Peoples R China.
C3 South China University of Technology
RP Xian, CH (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
EM chhxian@scut.edu.cn; 202020143933@mail.scut.edu.cn; wuh@4dstc.com;
   antonio@4dstc.com; ligq@scut.edu.cn
RI Lin, Zisen/KBC-1538-2024
OI Lin, Zisen/0000-0003-0260-3692
FU Nature Science Fund of Guangdong Province [2021A1515011849]; Key Area
   Research and Development of Guangdong Province [2022A0505050014]
FX This study was supported by the Nature Science Fund of Guangdong
   Province (No. 2021A1515011849) and the Key Area Research and Development
   of Guangdong Province (No. 2022A0505050014).
CR Albert RachelA., 2018, P EUROGRAPHICS S REN, P11, DOI DOI 10.2312/SRE.20181168
   [Anonymous], 2017, XRITE TAC7 APPEARANC
   [Anonymous], 2022, Cycles: Open source production rendering
   Asselin LP, 2020, INT CONF 3D VISION, P1157, DOI 10.1109/3DV50981.2020.00126
   Baek SH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275018
   Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778
   Deschaintre V, 2021, PROC CVPR IEEE, P15562, DOI 10.1109/CVPR46437.2021.01531
   Deschaintre V, 2019, COMPUT GRAPH FORUM, V38, P1, DOI 10.1111/cgf.13765
   Deschaintre V, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201378
   Gao D, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323042
   Guo J, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459854
   Guo Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417779
   Holroyd M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778836
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Kang KZ, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356492
   Kingma D. P., 2014, arXiv
   Lawrence J, 2006, ACM T GRAPHIC, V25, P735, DOI 10.1145/1141911.1141949
   Li ZQ, 2018, LECT NOTES COMPUT SC, V11207, P74, DOI 10.1007/978-3-030-01219-9_5
   Ma XH, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459679
   Nam G, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275017
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rui Xia, 2016, ACM Transactions on Graphics, V35, DOI 10.1145/2980179.2980248
   Smith A. R., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P259, DOI 10.1145/237170.237263
   Tunwattanapong B, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461944
   Wen T, 2022, COMPUT GRAPH FORUM, V41, P110, DOI 10.1111/cgf.14514
   Ye WJ, 2021, COMPUT GRAPH FORUM, V40, P409, DOI 10.1111/cgf.14387
   Zhao Yezi., 2020, JOINT SVBRDF RECOVER, P53
   Zhou XL, 2021, COMPUT GRAPH FORUM, V40, P315, DOI 10.1111/cgf.142635
   Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]
NR 29
TC 0
Z9 0
U1 1
U2 1
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2024
VL 10
IS 3
BP 523
EP 541
DI 10.1007/s41095-023-0352-6
EA FEB 2024
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QO3G5
UT WOS:001159520500001
OA gold
DA 2024-08-05
ER

PT J
AU Yang, WK
   Liu, MC
   Wang, Z
   Liu, SX
AF Yang, Weikai
   Liu, Mengchen
   Wang, Zheng
   Liu, Shixia
TI Foundation models meet visualizations: Challenges and opportunities
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review
DE visualization; artificial intelligence (AI); machine learning;
   foundation models; visualization for foundationmodel (VIS4FM);
   foundation model for visualization (FM4VIS)
ID VISUAL ANALYTICS; TRANSFORMERS; GENERATION; FRAMEWORK
AB Recent studies have indicated that foundation models, such as BERT and GPT, excel at adapting to various downstream tasks. This adaptability has made them a dominant force in building artificial intelligence (AI) systems. Moreover, a new research paradigm has emerged as visualization techniques are incorporated into these models. This study divides these intersections into two research areas: visualization for foundation model (VIS4FM) and foundation model for visualization (FM4VIS). In terms of VIS4FM, we explore the primary role of visualizations in understanding, refining, and evaluating these intricate foundation models. VIS4FM addresses the pressing need for transparency, explainability, fairness, and robustness. Conversely, in terms of FM4VIS, we highlight how foundation models can be used to advance the visualization field itself. The intersection of foundation models with visualizations is promising but also introduces a set of challenges. By highlighting these challenges and promising opportunities, this study aims to provide a starting point for the continued exploration of this research avenue.
C1 [Yang, Weikai; Wang, Zheng; Liu, Shixia] Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
   [Liu, Mengchen] Microsoft, Redmond, WA 98052 USA.
C3 Tsinghua University; Microsoft
RP Liu, SX (corresponding author), Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
EM yangwk21@mails.tsinghua.edu.cn; mengcliu@microsoft.com;
   zheng-wa19@mails.tsinghua.edu.cn; shixia@tsinghua.edu.cn
FU National Natural Science Foundation of China [U21A20469, 61936002];
   National Key R&D Program of China [2020YFB2104100]; Institute Guo Qiang,
   THUIBCS, and BLBCI
FX This work was supported by the National Natural Science Foundation of
   China (Grant Nos. U21A20469 and 61936002), the National Key R&D Program
   of China (Grant No. 2020YFB2104100), and grants from the Institute Guo
   Qiang, THUIBCS, and BLBCI.
CR Abadi Martin, 2016, arXiv
   Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Achiam J., 2023, ARXIV
   AdapterHub, US
   Adhikary J, 2021, IEEE T VIS COMPUT GR, V27, P2648, DOI 10.1109/TVCG.2021.3067776
   Alsallakh B, 2014, IEEE T VIS COMPUT GR, V20, P1703, DOI 10.1109/TVCG.2014.2346660
   Anil R., 2023, arXiv
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Bäuerle A, 2020, COMPUT GRAPH FORUM, V39, P195, DOI 10.1111/cgf.13973
   Bernard J, 2018, COMPUT GRAPH FORUM, V37, P121, DOI 10.1111/cgf.13406
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bommasani R., 2021, ARXIV
   Bostock M, 2011, IEEE T VIS COMPUT GR, V17, P2301, DOI 10.1109/TVCG.2011.185
   Brown ET, 2014, IEEE T VIS COMPUT GR, V20, P1663, DOI 10.1109/TVCG.2014.2346575
   Brown T., 2020, ADV NEURAL INFORM PR, V33, P1877, DOI DOI 10.48550/ARXIV.2005.14165
   Cao KL, 2021, IEEE T VIS COMPUT GR, V27, P3289, DOI 10.1109/TVCG.2020.2969185
   Card S.K., 1999, Readings in Information Visualization: Using Vision to Think. The Morgan Kaufmann series in interactive technologies
   Chen CJ, 2024, IEEE T VIS COMPUT GR, V30, P76, DOI 10.1109/TVCG.2023.3326588
   Chen CJ, 2022, IEEE T VIS COMPUT GR, V28, P1941, DOI 10.1109/TVCG.2021.3138933
   Chen CJ, 2021, IEEE T VIS COMPUT GR, V27, P3335, DOI 10.1109/TVCG.2020.2973258
   Chen CJ, 2021, IEEE T VIS COMPUT GR, V27, P3701, DOI 10.1109/TVCG.2021.3084694
   Chen Q, 2024, IEEE T VIS COMPUT GR, V30, P562, DOI 10.1109/TVCG.2023.3326925
   Chen Q, 2024, IEEE T VIS COMPUT GR, V30, P4429, DOI 10.1109/TVCG.2023.3261320
   Chen Z., 2023, P CHI C HUM FACT COM
   Chen Zhutian, 2023, IEEE Trans Vis Comput Graph, V29, P918, DOI 10.1109/TVCG.2022.3209497
   Chen ZT, 2020, IEEE T VIS COMPUT GR, V26, P917, DOI 10.1109/TVCG.2019.2934810
   Chen ZT, 2020, IEEE T VIS COMPUT GR, V26, P195, DOI 10.1109/TVCG.2019.2934332
   Choo J, 2018, IEEE COMPUT GRAPH, V38, P84, DOI 10.1109/MCG.2018.042731661
   Chung JJY, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3501819
   DeRose JF, 2021, IEEE T VIS COMPUT GR, V27, P1160, DOI 10.1109/TVCG.2020.3028976
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dong Q., 2022, ARXIV
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Eloundou T., 2023, ARXIV
   Feng YCJ, 2024, IEEE T VIS COMPUT GR, V30, P295, DOI 10.1109/TVCG.2023.3327168
   Gortler J., 2022, P CHI C HUM FACT COM
   Guo YX, 2023, PARTICUOLOGY, V82, P157, DOI 10.1016/j.partic.2023.01.017
   Hohman F, 2019, IEEE T VIS COMPUT GR, V25, P2674, DOI 10.1109/TVCG.2018.2843369
   Houlsby N, 2019, PR MACH LEARN RES, V97
   Hu E. J., 2021, P INT C LEARN REPR
   HuggingFace, about us
   Hunter JD, 2007, COMPUT SCI ENG, V9, P90, DOI 10.1109/MCSE.2007.55
   Jiang L, 2019, J VISUAL-JAPAN, V22, P401, DOI 10.1007/s12650-018-0531-1
   Jin ZH, 2024, IEEE T VIS COMPUT GR, V30, P3594, DOI 10.1109/TVCG.2023.3236380
   Jing YH, 2021, IEEE T CYBERNETICS, V51, P568, DOI 10.1109/TCYB.2019.2904768
   Khayat M, 2020, IEEE T VIS COMPUT GR, V26, P874, DOI 10.1109/TVCG.2019.2934266
   Kwon OH, 2020, IEEE T VIS COMPUT GR, V26, P665, DOI 10.1109/TVCG.2019.2934396
   Li R, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS, P220
   Li X., 2021, P CHI C HUM FACT COM
   Li YR, 2023, IEEE T VIS COMPUT GR, V29, P2888, DOI 10.1109/TVCG.2023.3261935
   Li Z, 2022, IEEE T VIS COMPUT GR, V28, P4980, DOI 10.1109/TVCG.2022.3184186
   Liu C, 2024, IEEE T VIS COMPUT GR, V30, P5276, DOI 10.1109/TVCG.2023.3290241
   Liu C, 2021, IEEE PAC VIS SYMP, P11, DOI 10.1109/PacificVis52677.2021.00010
   Liu MC, 2018, IEEE CONF VIS ANAL, P60, DOI 10.1109/VAST.2018.8802509
   Liu MC, 2018, IEEE T VIS COMPUT GR, V24, P77, DOI 10.1109/TVCG.2017.2744938
   Liu MC, 2017, IEEE T VIS COMPUT GR, V23, P91, DOI 10.1109/TVCG.2016.2598831
   Liu SX, 2019, IEEE T VIS COMPUT GR, V25, P2482, DOI 10.1109/TVCG.2018.2834341
   Liu SX, 2018, VIS INFORM, V2, P191, DOI 10.1016/j.visinf.2018.12.001
   Liu SX, 2019, IEEE T VIS COMPUT GR, V25, P235, DOI 10.1109/TVCG.2018.2864843
   Liu SX, 2018, IEEE T VIS COMPUT GR, V24, P163, DOI 10.1109/TVCG.2017.2744378
   Liu SX, 2017, VIS INFORM, V1, P48, DOI 10.1016/j.visinf.2017.01.006
   Lowe D. G., 1999, Computer vision, V2, P1150, DOI [10.1109/ICCV.1999.790410, DOI 10.1109/ICCV.1999.790410]
   Ma KL, 2009, IEEE COMPUT GRAPH, V29, P14, DOI 10.1109/MCG.2009.120
   Ma WH, 2021, LECT NOTES COMPUT SC, V12821, P583, DOI 10.1007/978-3-030-86549-8_37
   Narechania A, 2022, IEEE T VIS COMPUT GR, V28, P486, DOI 10.1109/TVCG.2021.3114820
   Ottley A, 2019, COMPUT GRAPH FORUM, V38, P41, DOI 10.1111/cgf.13670
   Ouyang Long, 2022, Advances in Neural Information Processing Systems, V35, P27730, DOI 10.1177/01454455830072006
   Ouyang Y, 2024, IEEE T VIS COMPUT GR, V30, P1238, DOI 10.1109/TVCG.2023.3326929
   Pan XG, 2023, PROCEEDINGS OF SIGGRAPH 2023 CONFERENCE PAPERS, SIGGRAPH 2023, DOI 10.1145/3588432.3591500
   Park J. S., 2023, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology
   Pryzant R., 2023, ARXIV
   Qiu R, 2024, IEEE T VIS COMPUT GR, V30, P1533, DOI 10.1109/TVCG.2022.3219762
   Radford A, 2021, PR MACH LEARN RES, V139
   Raffel C, 2020, J MACH LEARN RES, V21
   Rapp T, 2022, IEEE T VIS COMPUT GR, V28, P2314, DOI 10.1109/TVCG.2022.3165346
   Reif E., 2023, ARXIV
   Ren DH, 2017, IEEE T VIS COMPUT GR, V23, P61, DOI 10.1109/TVCG.2016.2598828
   Resck LE, 2023, IEEE T VIS COMPUT GR, V29, P3105, DOI 10.1109/TVCG.2022.3152450
   Richer G, 2024, IEEE T VIS COMPUT GR, V30, P3314, DOI 10.1109/TVCG.2022.3231230
   Roziere B., 2023, ARXIV
   Sacha D, 2019, IEEE T VIS COMPUT GR, V25, P385, DOI 10.1109/TVCG.2018.2864838
   Schwartz R, 2020, COMMUN ACM, V63, P54, DOI 10.1145/3381831
   Sevastjanova Rita, 2023, IEEE Trans Vis Comput Graph, V29, P1178, DOI 10.1109/TVCG.2022.3209458
   Shen LX, 2024, IEEE T VIS COMPUT GR, V30, P109, DOI 10.1109/TVCG.2023.3327197
   Shen LX, 2023, IEEE T VIS COMPUT GR, V29, P3121, DOI 10.1109/TVCG.2022.3148007
   Shi Chuhan, 2023, IEEE Trans Vis Comput Graph, V29, P63, DOI 10.1109/TVCG.2022.3209434
   Shi DQ, 2021, IEEE T VIS COMPUT GR, V27, P453, DOI 10.1109/TVCG.2020.3030403
   Singh H, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P3275
   Song SC, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2023, DOI 10.1145/3544548.3581067
   Song SC, 2023, IEEE T VIS COMPUT GR, V29, P3169, DOI 10.1109/TVCG.2022.3153514
   Strobelt Hendrik, 2023, IEEE Trans Vis Comput Graph, V29, P1146, DOI 10.1109/TVCG.2022.3209479
   Sultanum N, 2023, IEEE VIS CONF, P231, DOI 10.1109/VIS54172.2023.00055
   Sun Mengdi, 2023, IEEE Trans Vis Comput Graph, V29, P983, DOI 10.1109/TVCG.2022.3209428
   Tu YM, 2023, IEEE T VIS COMPUT GR, V29, P2862, DOI 10.1109/TVCG.2023.3261944
   Tu YM, 2024, IEEE T VIS COMPUT GR, V30, P1787, DOI 10.1109/TVCG.2022.3225114
   Tu YM, 2021, IEEE PAC VIS SYMP, P206, DOI 10.1109/PacificVis52677.2021.00034
   Wang J, 2023, ARXIV
   Wang L., 2023, arXiv
   Wang QW, 2022, IEEE T VIS COMPUT GR, V28, P5134, DOI 10.1109/TVCG.2021.3106142
   Wang QW, 2020, IEEE T VIS COMPUT GR, V26, P3340, DOI 10.1109/TVCG.2019.2921323
   Wang WH, 2023, PROC CVPR IEEE, P14408, DOI 10.1109/CVPR52729.2023.01385
   Wang XB, 2024, IEEE T VIS COMPUT GR, V30, P273, DOI 10.1109/TVCG.2023.3327153
   Wang Yun, 2023, IEEE Trans Vis Comput Graph, V29, P1222, DOI 10.1109/TVCG.2022.3209357
   Wei JS, 2022, ADV NEUR IN
   Wei YT, 2024, IEEE T VIS COMPUT GR, V30, P3915, DOI 10.1109/TVCG.2023.3243228
   Wexler J, 2020, IEEE T VIS COMPUT GR, V26, P56, DOI 10.1109/TVCG.2019.2934619
   Wu AY, 2022, IEEE T VIS COMPUT GR, V28, P5049, DOI 10.1109/TVCG.2021.3099002
   Wu Sherry, 2023, IUI '23: Proceedings of the 28th International Conference on Intelligent User Interfaces, P353, DOI 10.1145/3581641.3584059
   Wu TS, 2022, EXTENDED ABSTRACTS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2022, DOI 10.1145/3491101.3519729
   Wu TS, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517582
   Wu YC, 2024, IEEE T VIS COMPUT GR, V30, P1117, DOI 10.1109/TVCG.2023.3326911
   Xiang SX, 2019, IEEE CONF VIS ANAL, P57, DOI [10.1109/vast47406.2019.8986943, 10.1109/VAST47406.2019.8986943]
   Xiao SS, 2024, IEEE T VIS COMPUT GR, V30, P284, DOI 10.1109/TVCG.2023.3326913
   Yang WK, 2022, IEEE T VIS COMPUT GR, V28, P3292, DOI 10.1109/TVCG.2022.3182488
   Yang WK, 2020, IEEE CONF VIS ANAL, P12, DOI 10.1109/VAST50239.2020.00007
   Yang WK, 2021, IEEE T VIS COMPUT GR, V27, P3953, DOI 10.1109/TVCG.2020.2995100
   Yeh C, 2024, IEEE T VIS COMPUT GR, V30, P262, DOI 10.1109/TVCG.2023.3327163
   Ying Lu, 2023, IEEE Trans Vis Comput Graph, V29, P331, DOI 10.1109/TVCG.2022.3209447
   Yuan Jun, 2023, IEEE Trans Vis Comput Graph, V29, P288, DOI 10.1109/TVCG.2022.3209404
   Yuan J, 2021, COMPUT VIS MEDIA, V7, P3, DOI 10.1007/s41095-020-0191-7
   Yuan J, 2021, IEEE T VIS COMPUT GR, V27, P1720, DOI 10.1109/TVCG.2020.3030432
   Zamfrescu-Pereira JD, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2023), DOI 10.1145/3544548.3581388
   Zhang X., 2020, P CHI C HUM FACT COM
   Zhang Xiaoyu, 2023, IEEE Trans Vis Comput Graph, V29, P842, DOI 10.1109/TVCG.2022.3209465
   Zhao Y, 2021, IEEE T VIS COMPUT GR, V27, P1698, DOI 10.1109/TVCG.2020.3030428
   Zhou C., 2024, P 37 C NEUR INF PROC
   Zhou C., 2023, arXiv
   Zhou YX, 2024, IEEE T VIS COMPUT GR, V30, P240, DOI 10.1109/TVCG.2023.3326934
   Zhou ZH, 2024, SCI CHINA INFORM SCI, V67, DOI 10.1007/s11432-023-3823-6
NR 129
TC 2
Z9 2
U1 12
U2 12
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2024
VL 10
IS 3
BP 399
EP 424
DI 10.1007/s41095-023-0393-x
EA MAY 2024
PG 26
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QO3G5
UT WOS:001216848500003
OA gold, Green Submitted
DA 2024-08-05
ER

PT J
AU Liu, SH
   Gai, SY
   Da, F
   Waris, F
AF Liu, Shanghuan
   Gai, Shaoyan
   Da, Feipeng
   Waris, Fazal
TI Geometry-aware 3D pose transfer using transformer autoencoder
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article; Early Access
DE 3D pose transfer; geometry-aware; transformer autoencoder;
   cross-covariance attention
ID DEFORMATION TRANSFER
AB 3D pose transfer over unorganized point clouds is a challenging generation task, which transfers a source's pose to a target shape and keeps the target's identity. Recent deep models have learned deformations and used the target's identity as a style to modulate the combined features of two shapes or the aligned vertices of the source shape. However, all operations in these models are point-wise and independent and ignore the geometric information on the surface and structure of the input shapes. This disadvantage severely limits the generation and generalization capabilities. In this study, we propose a geometry-aware method based on a novel transformer autoencoder to solve this problem. An efficient self-attention mechanism, that is, cross-covariance attention, was utilized across our framework to perceive the correlations between points at different distances. Specifically, the transformer encoder extracts the target shape's local geometry details for identity attributes and the source shape's global geometry structure for pose information. Our transformer decoder efficiently learns deformations and recovers identity properties by fusing and decoding the extracted features in a geometry attentional manner, which does not require corresponding information or modulation steps. The experiments demonstrated that the geometry-aware method achieved state-of-the-art performance in a 3D pose transfer task. The implementation code and data are available at https://github.com/SEULSH/Geometry-Aware-3D-Pose-Transfer-Using-Transformer-Autoencoder.
C1 [Liu, Shanghuan; Gai, Shaoyan; Da, Feipeng; Waris, Fazal] Southeast Univ, Key Lab Measurement & Control Complex Syst Engn, Minist Educ, Nanjing 210096, Peoples R China.
C3 Southeast University - China
RP Gai, SY (corresponding author), Southeast Univ, Key Lab Measurement & Control Complex Syst Engn, Minist Educ, Nanjing 210096, Peoples R China.
EM 230189545@seu.edu.cn; 101011375@seu.edu.cn; dafp@seu.edu.cn;
   fazalwaris@seu.edu.cn
FU Special Project on Basic Research of Frontier Leading Technology of
   Jiangsu Province, China [BK20192004C]
FX This work was supported by the Special Project on Basic Research of
   Frontier Leading Technology of Jiangsu Province, China (Grant No.
   BK20192004C).
CR Baran I, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531342
   Basset J, 2020, COMPUT GRAPH-UK, V89, P11, DOI 10.1016/j.cag.2020.04.002
   Ben-Chen M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531340
   Ben-Chen Mirela., 2009, P 2009 ACM SIGGRAPH, P67, DOI [10.1145/1599470.1599479, DOI 10.1145/1599470.1599479]
   Bhatnagar BL, 2019, IEEE I CONF COMP VIS, P5419, DOI 10.1109/ICCV.2019.00552
   Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491
   Chandran P, 2022, COMPUT GRAPH FORUM, V41, P195, DOI 10.1111/cgf.14468
   Chen H., 2021, P BRIT MACHINE VISIO
   Chen HY, 2022, AAAI CONF ARTIF INTE, P258
   Chen HY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8610, DOI 10.1109/ICCV48922.2021.00851
   Chen YG, 2020, LECT NOTES COMPUT SC, V11961, P176, DOI 10.1007/978-3-030-37731-1_15
   Chu HK, 2010, J INF SCI ENG, V26, P379
   Cosmo Luca, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P19, DOI 10.1007/978-3-030-58580-8_2
   Domadiya P, 2019, 16TH ACM SIGGRAPH EUROPEAN CONFERENCE ON VISUAL MEDIA PRODUCTION (CVMP 2019), DOI 10.1145/3359998.3369408
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   El-Nouby A, 2021, ADV NEUR IN
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Gao L, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275028
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jacobson A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964973
   Keyang Zhou, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P341, DOI 10.1007/978-3-030-58542-6_21
   Liao Z, 2022, LECT NOTES COMPUT SC, V13662, P640, DOI 10.1007/978-3-031-20086-1_37
   Lin K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12919, DOI 10.1109/ICCV48922.2021.01270
   Lin K, 2021, PROC CVPR IEEE, P1954, DOI 10.1109/CVPR46437.2021.00199
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Loshchilov I, 2019, Arxiv, DOI [arXiv:1711.05101, 10.48550/arXiv.1711.05101, DOI 10.48550/ARXIV.1711.05101]
   Mahmood N, 2019, IEEE I CONF COMP VIS, P5441, DOI 10.1109/ICCV.2019.00554
   Mao JG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3144, DOI 10.1109/ICCV48922.2021.00315
   Misra I, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2886, DOI 10.1109/ICCV48922.2021.00290
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Paszke A, 2019, ADV NEUR IN, V32
   Roberts RA, 2021, COMPUT GRAPH-UK, V94, P52, DOI 10.1016/j.cag.2020.10.004
   Song CY, 2023, IEEE T PATTERN ANAL, V45, P10488, DOI 10.1109/TPAMI.2023.3259059
   Song CY, 2021, ADV NEUR IN, V34
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Ulyanov D, 2017, Arxiv, DOI arXiv:1607.08022
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JS, 2020, PROC CVPR IEEE, P5830, DOI 10.1109/CVPR42600.2020.00587
   Xu WW, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239535
   Xu YF, 2022, COMPUT VIS MEDIA, V8, P33, DOI 10.1007/s41095-021-0247-3
   Yang J, 2018, GRAPH MODELS, V98, P1, DOI 10.1016/j.gmod.2018.05.003
   Ye YP, 2022, COMPUT GRAPH-UK, V104, P46, DOI 10.1016/j.cag.2022.03.007
   Yifan W, 2020, PROC CVPR IEEE, P72, DOI 10.1109/CVPR42600.2020.00015
   Zhang YZ, 2020, COMPUT GRAPH-UK, V89, P167, DOI 10.1016/j.cag.2020.05.013
   Zuffi S, 2017, PROC CVPR IEEE, P5524, DOI 10.1109/CVPR.2017.586
NR 46
TC 0
Z9 0
U1 4
U2 4
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD 2024 MAR 22
PY 2024
DI 10.1007/s41095-023-0379-8
EA MAR 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LY3D3
UT WOS:001190323500001
OA gold
DA 2024-08-05
ER

PT J
AU Wu, T
   Yuan, YJ
   Zhang, LX
   Yang, J
   Cao, YP
   Yan, LQ
   Gao, L
AF Wu, Tong
   Yuan, Yu-Jie
   Zhang, Ling-Xiao
   Yang, Jie
   Cao, Yan-Pei
   Yan, Ling-Qi
   Gao, Lin
TI Recent advances in 3D Gaussian splatting
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review; Early Access
DE 3D Gaussian splatting (3DGS); radiance field; novel view synthesis; 3D
   editing; scene generation
AB The emergence of 3D Gaussian splatting (3DGS) has greatly accelerated rendering in novel view synthesis. Unlike neural implicit representations like neural radiance fields (NeRFs) that represent a 3D scene with position and viewpoint-conditioned neural networks, 3D Gaussian splatting utilizes a set of Gaussian ellipsoids to model the scene so that efficient rendering can be accomplished by rasterizing Gaussian ellipsoids into images. Apart from fast rendering, the explicit representation of 3D Gaussian splatting also facilitates downstream tasks like dynamic reconstruction, geometry editing, and physical simulation. Considering the rapid changes and growing number of works in this field, we present a literature review of recent 3D Gaussian splatting methods, which can be roughly classified by functionality into 3D reconstruction, 3D editing, and other downstream applications. Traditional point-based rendering methods and the rendering formulation of 3D Gaussian splatting are also covered to aid understanding of this technique. This survey aims to help beginners to quickly get started in this field and to provide experienced researchers with a comprehensive overview, aiming to stimulate future development of the 3D Gaussian splatting representation.
C1 [Wu, Tong; Yuan, Yu-Jie; Zhang, Ling-Xiao; Yang, Jie; Gao, Lin] Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
   [Cao, Yan-Pei] Tencent AI Lab, Beijing 100089, Peoples R China.
   [Cao, Yan-Pei] VAST, Beijing 100000, Peoples R China.
   [Yan, Ling-Qi] Univ Calif Santa Barbara, Dept Comp Sci, Santa Barbara, CA 93106 USA.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Tencent; University of California System; University of California Santa
   Barbara
RP Gao, L (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
EM wutong19s@ict.ac.cn; yangjie01@ict.ac.cn; zhanglingxiao@ict.ac.cn;
   yuanyujie@ict.ac.cn; caoyanpei@gmail.com; lingqi@cs.ucsb.edu;
   gaolin@ict.ac.cn
RI Yuan, Yujie/AES-2662-2022
OI Yuan, Yujie/0000-0002-5003-5872
FU National Natural Science Foundation of China [62322210]; Beijing
   Municipal Natural Science Foundation for Distinguished Young Scholars
   [JQ21013]; Beijing Municipal Science and Technology Commission
   [Z231100005923031]; The 2023 Tencent AI Lab Rhino-Bird Focused Research
   Program
FX This work was supported by the National Natural Science Foundation of
   China (62322210), Beijing Municipal Natural Science Foundation for
   Distinguished Young Scholars (JQ21013), Beijing Municipal Science and
   Technology Commission (Z231100005923031), and 2023 Tencent AI Lab
   Rhino-Bird Focused Research Program. We would like to thank Jia-Mu Sun
   and Shu-Yu Chen for their suggestions concerning the timeline figure.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Abdal W., 2023, arXiv
   Bai JY, 2024, Arxiv, DOI arXiv:2402.00763
   Barron JT, 2023, IEEE I CONF COMP VIS, P19640, DOI 10.1109/ICCV51070.2023.01804
   Barron JT, 2022, PROC CVPR IEEE, P5460, DOI 10.1109/CVPR52688.2022.00539
   Barron JT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5835, DOI 10.1109/ICCV48922.2021.00580
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Blattmann A, 2023, PROC CVPR IEEE, P22563, DOI 10.1109/CVPR52729.2023.02161
   Bonet J, 2008, NONLINEAR CONTINUUM MECHANICS FOR FINITE ELEMENT ANALYSIS, 2ND EDITION, P1, DOI 10.1017/CBO9780511755446
   Botsch M., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P53
   Botsch M, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P335, DOI 10.1109/PCCGA.2003.1238275
   Brooks T, 2023, PROC CVPR IEEE, P18392, DOI 10.1109/CVPR52729.2023.01764
   Bulò SR, 2024, Arxiv, DOI arXiv:2404.06109
   Cai YH, 2024, Arxiv, DOI arXiv:2403.04116
   Cao A, 2023, PROC CVPR IEEE, P130, DOI 10.1109/CVPR52729.2023.00021
   Cao MD, 2023, Arxiv, DOI arXiv:2304.08465
   Caron M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9630, DOI 10.1109/ICCV48922.2021.00951
   Cen JZ, 2024, Arxiv, DOI arXiv:2312.00860
   Chan ER, 2022, PROC CVPR IEEE, P16102, DOI 10.1109/CVPR52688.2022.01565
   Charatan D, 2024, Arxiv, DOI arXiv:2312.12337
   Chen GK, 2024, Arxiv, DOI arXiv:2401.03890
   Chen HL, 2023, Arxiv, DOI arXiv:2312.00846
   Chen IC, 2023, Arxiv, DOI arXiv:2312.10242
   Chen T, 2024, Arxiv, DOI arXiv:2403.02751
   Chen YS, 2024, Arxiv, DOI arXiv:2401.13352
   Chen YH, 2024, Arxiv, DOI arXiv:2403.14530
   Chen YW, 2023, Arxiv, DOI arXiv:2311.14521
   Chen YD, 2024, Arxiv, DOI arXiv:2403.14627
   Chen YR, 2024, Arxiv, DOI arXiv:2311.18561
   Chen ZQ, 2023, PROC CVPR IEEE, P16569, DOI 10.1109/CVPR52729.2023.01590
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   Chen ZL, 2024, Arxiv, DOI arXiv:2309.16585
   Cheng K, 2024, Arxiv, DOI arXiv:2402.14650
   Chung J, 2023, Arxiv, DOI arXiv:2311.13384
   Comi M, 2024, Arxiv, DOI arXiv:2403.20275
   Cotton R. James, 2024, 2024 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW), P60, DOI 10.1109/WACVW60836.2024.00014
   Dahmani H, 2024, Arxiv, DOI arXiv:2403.10427
   Dai XL, 2023, Arxiv, DOI arXiv:2309.15807
   Deitke M, 2023, PROC CVPR IEEE, P13142, DOI 10.1109/CVPR52729.2023.01263
   Deitke M, 2023, Arxiv, DOI arXiv:2307.05663
   Deng TC, 2024, Arxiv, DOI arXiv:2403.11247
   Dhamo H, 2023, Arxiv, DOI arXiv:2312.02902
   Di DL, 2024, Arxiv, DOI arXiv:2403.09236
   Dou B, 2024, Arxiv, DOI arXiv:2401.05925
   Duan YX, 2024, Arxiv, DOI arXiv:2402.03307
   Fan ZW, 2024, Arxiv, DOI arXiv:2403.20309
   Fan ZW, 2024, Arxiv, DOI arXiv:2311.17245
   Fang GC, 2024, Arxiv, DOI arXiv:2403.14166
   Fang JM, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555383
   Fei B, 2024, Arxiv, DOI arXiv:2402.07181
   Feng QJ, 2024, Arxiv, DOI arXiv:2403.10242
   Feng QY, 2024, Arxiv, DOI arXiv:2403.09143
   Feng YT, 2024, Arxiv, DOI arXiv:2401.15318
   Franke L, 2024, COMPUT GRAPH FORUM, V43, DOI 10.1111/cgf.15012
   Fridovich-Keil S, 2023, PROC CVPR IEEE, P12479, DOI 10.1109/CVPR52729.2023.01201
   Fu Y, 2023, Arxiv, DOI arXiv:2312.07504
   Gao J, 2023, Arxiv, DOI arXiv:2311.16043
   Gao L, 2024, Arxiv, DOI arXiv:2402.04796
   Gao L, 2021, Arxiv, DOI arXiv:2010.06217
   Gao L, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356488
   Gao QK, 2024, Arxiv, DOI arXiv:2403.12365
   Ge WH, 2023, IEEE I CONF COMP VIS, P4228, DOI 10.1109/ICCV51070.2023.00392
   Girish S, 2024, Arxiv, DOI arXiv:2312.04564
   Grossman J. P., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P181
   Gu A, 2024, Arxiv, DOI arXiv:2312.00752
   Guédon A, 2024, Arxiv, DOI arXiv:2403.14554
   Guédon A, 2023, Arxiv, DOI arXiv:2311.12775
   Guo J., 2024, arXiv
   Guo ZY, 2024, Arxiv, DOI arXiv:2403.11447
   Hamdi A, 2024, Arxiv, DOI arXiv:2402.10128
   Haque A, 2023, IEEE I CONF COMP VIS, P19683, DOI 10.1109/ICCV51070.2023.01808
   Herau Q, 2024, Arxiv, DOI arXiv:2403.11577
   Hong S, 2024, Arxiv, DOI arXiv:2401.14857
   Hu EJ, 2021, Arxiv, DOI arXiv:2106.09685
   Hu LX, 2024, Arxiv, DOI arXiv:2312.02134
   Hu SM, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-020-3097-4
   Hu SK, 2023, Arxiv, DOI arXiv:2312.02973
   Hu X, 2024, Arxiv, DOI arXiv:2401.17857
   Huang BB, 2024, Arxiv, DOI arXiv:2403.17888
   Huang JJ, 2023, Arxiv, DOI arXiv:2311.16737
   Huang YH, 2024, Arxiv, DOI arXiv:2312.14937
   Huang YM, 2024, Arxiv, DOI arXiv:2401.16416
   Jena R, 2023, Arxiv, DOI arXiv:2311.10812
   Ji YM, 2024, Arxiv, DOI arXiv:2403.11679
   Jiang LT, 2024, Arxiv, DOI arXiv:2403.11273
   Jiang P, 2024, Arxiv, DOI arXiv:2403.11367
   Jiang Y, 2024, Arxiv, DOI [arXiv:2401.16663, DOI 10.48550/ARXIV.2401.16663]
   Jiang YWQ, 2023, Arxiv, DOI arXiv:2311.17977
   Jiang YH, 2023, Arxiv, DOI arXiv:2312.03461
   Jiang ZH, 2023, Arxiv, DOI arXiv:2312.13770
   Jo J, 2024, Arxiv, DOI arXiv:2402.13827
   Jun H, 2023, Arxiv, DOI arXiv:2305.02463
   Jung J, 2024, Arxiv, DOI arXiv:2403.09413
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Katsumata K, 2024, Arxiv, DOI arXiv:2311.12897
   Keetha N, 2024, Arxiv, DOI arXiv:2312.02126
   Kerbl B, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592433
   Kirillov A, 2023, Arxiv, DOI arXiv:2304.02643
   Kobbelt L, 2004, COMPUT GRAPH-UK, V28, P801, DOI 10.1016/j.cag.2004.08.009
   Kocabas J.-H. Rick, 2023, arXiv
   Kratimenos A, 2023, Arxiv, DOI arXiv:2312.00112
   Lee B, 2024, Arxiv, DOI arXiv:2401.00834
   Lee JC, 2024, Arxiv, DOI arXiv:2311.13681
   Lei JH, 2023, Arxiv, DOI arXiv:2311.16099
   Lei XH, 2024, Arxiv, DOI arXiv:2403.11625
   Li H, 2024, Arxiv, DOI arXiv:2403.10147
   Li HR, 2024, Arxiv, DOI arXiv:2404.03575
   Li JH, 2024, Arxiv, DOI arXiv:2403.06912
   Li MT, 2024, Arxiv, DOI arXiv:2401.09720
   Li MR, 2024, Arxiv, DOI arXiv:2402.03246
   Li MW, 2023, Arxiv, DOI arXiv:2312.15258
   Li TY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130813
   Li XH, 2023, Arxiv, DOI arXiv:2311.11221
   Li YY, 2024, Arxiv, DOI arXiv:2403.11324
   Li YT, 2023, Arxiv, DOI arXiv:2312.15676
   Li Z, 2024, Arxiv, DOI arXiv:2312.16812
   Li ZQ, 2024, Arxiv, DOI arXiv:2403.09981
   Liang YQ, 2023, Arxiv, DOI arXiv:2312.11458
   Liang YX, 2023, Arxiv, DOI arXiv:2311.11284
   Liang ZH, 2024, Arxiv, DOI arXiv:2403.11056
   Liang ZH, 2024, Arxiv, DOI arXiv:2311.16473
   Lin CH, 2023, PROC CVPR IEEE, P300, DOI 10.1109/CVPR52729.2023.00037
   Lin JQ, 2024, Arxiv, DOI arXiv:2402.17427
   Lin YT, 2023, Arxiv, DOI arXiv:2312.03431
   Lin YZ, 2024, Arxiv, DOI arXiv:2403.17237
   Ling H, 2024, Arxiv, DOI arXiv:2312.13763
   Liu GY, 2024, Arxiv, DOI arXiv:2403.11396
   Liu ISBL, 2024, Arxiv, DOI arXiv:2404.12379
   Liu KH, 2024, Arxiv, DOI arXiv:2403.07807
   Liu RS, 2023, IEEE I CONF COMP VIS, P9264, DOI 10.1109/ICCV51070.2023.00853
   Liu X, 2024, Arxiv, DOI arXiv:2311.17061
   Liu Y, 2024, Arxiv, DOI arXiv:2404.01133
   Liu YT, 2023, PROC CVPR IEEE, P237, DOI 10.1109/CVPR52729.2023.00031
   Liu Y, 2024, Arxiv, DOI arXiv:2309.03453
   Lombardi S, 2021, ACM T GRAPHIC, V40, DOI [10.1145/3476576.3476608, 10.1145/3450626.3459863]
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Luiten G., 2023, arXiv
   Luo HM, 2024, Arxiv, DOI arXiv:2402.10483
   Luo JH, 2024, Arxiv, DOI arXiv:2403.18784
   Lyu W, 2024, Arxiv, DOI arXiv:2404.07977
   Lyu X, 2024, Arxiv, DOI arXiv:2404.00409
   Macklin Miles, 2016, P 9 INT C MOTION GAM, P49, DOI 10.1145/2994258.2994272
   Maggioni M, 2023, PROC CVPR IEEE, P20226, DOI 10.1109/CVPR52729.2023.01937
   Malarz D, 2024, Arxiv, DOI arXiv:2312.13729
   Marschner SR, 2003, ACM T GRAPHIC, V22, P780, DOI 10.1145/882262.882345
   Matsuki H, 2024, Arxiv, DOI arXiv:2312.06741
   Melas-Kyriazi L, 2024, Arxiv, DOI arXiv:2402.08682
   Meng JR, 2024, Arxiv, DOI arXiv:2404.01168
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Mildenhall B, 2020, P ECCV, DOI DOI 10.1007/978-3-030-58452-8
   Moreau A, 2024, Arxiv, DOI arXiv:2311.17113
   Morgenstern W, 2024, Arxiv, DOI arXiv:2312.13299
   MPEGGroup, mpeg-pcc-tmc13
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Nash C, 2020, Arxiv, DOI arXiv:2002.10880
   Navaneet KL, 2024, Arxiv, DOI arXiv:2311.18159
   Nguyen V, 2024, AEROSPACE-BASEL, V11, DOI 10.3390/aerospace11030183
   Nichol A, 2022, Arxiv, DOI arXiv:2212.08751
   Niedermayr S, 2024, Arxiv, DOI arXiv:2401.02436
   Niemeyer M, 2024, Arxiv, DOI arXiv:2403.13806
   Ouyang H, 2023, Arxiv, DOI arXiv:2312.09242
   Palandra F, 2024, Arxiv, DOI arXiv:2403.05154
   Paliwal A, 2024, Arxiv, DOI arXiv:2403.19495
   Pan ZJ, 2024, Arxiv, DOI arXiv:2401.08742
   Pang HK, 2024, Arxiv, DOI arXiv:2312.05941
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Paszke A., 2017, P 31 C NEUR INF PROC, P1, DOI DOI 10.1017/CB09781107707221.009
   Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123
   Peng C, 2024, Arxiv, DOI arXiv:2403.04926
   Pokhariya C, 2024, Arxiv, DOI arXiv:2312.02137
   Poole B., 2022, arXiv
   Pumarola A, 2021, PROC CVPR IEEE, P10313, DOI 10.1109/CVPR46437.2021.01018
   Qian SH, 2024, Arxiv, DOI arXiv:2312.02069
   Qian ZY, 2024, Arxiv, DOI arXiv:2312.09228
   Qin MH, 2024, Arxiv, DOI arXiv:2312.16084
   Qiu RZ, 2024, Arxiv, DOI arXiv:2404.01223
   Radford A, 2021, PR MACH LEARN RES, V139
   Radl L, 2024, Arxiv, DOI arXiv:2402.00525
   Ren JW, 2024, Arxiv, DOI arXiv:2312.17142
   Ren KR, 2024, Arxiv, DOI arXiv:2403.17898
   Rivero A, 2024, Arxiv, DOI arXiv:2402.03723
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Ronneberger O, 2015, Arxiv, DOI arXiv:1505.04597
   Rückert D, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530122
   Rusinkiewicz S, 2000, COMP GRAPH, P343, DOI 10.1145/344779.344940
   Saito S, 2024, Arxiv, DOI arXiv:2312.03704
   Saroha A, 2024, Arxiv, DOI arXiv:2403.08498
   Seiskari O, 2024, Arxiv, DOI arXiv:2403.13327
   Shao RZ, 2023, PROC CVPR IEEE, P16632, DOI 10.1109/CVPR52729.2023.01596
   Shao ZJ, 2024, Arxiv, DOI arXiv:2403.05087
   Shaw R, 2023, Arxiv, DOI arXiv:2312.13308
   Shen QH, 2024, Arxiv, DOI arXiv:2403.18795
   Shen TC, 2021, Arxiv, DOI arXiv:2111.04276
   Shi JC, 2023, Arxiv, DOI arXiv:2311.18482
   Shi YH, 2023, Arxiv, DOI arXiv:2312.05133
   Shi YC, 2024, Arxiv, DOI arXiv:2308.16512
   Shriram J, 2024, Arxiv, DOI arXiv:2404.07199
   Siddiqui Y, 2023, Arxiv, DOI arXiv:2311.15475
   Song XW, 2024, Arxiv, DOI arXiv:2403.19615
   Stanishevskii G, 2024, Arxiv, DOI arXiv:2402.06390
   Straub J, 2019, Arxiv, DOI arXiv:1906.05797
   Sun JK, 2024, Arxiv, DOI arXiv:2403.01444
   Sun L. C., 2024, arXiv
   Sun Y, 2024, Arxiv, DOI arXiv:2312.09031
   Svitov D, 2024, Arxiv, DOI arXiv:2404.01053
   Szymanowicz S, 2024, Arxiv, DOI arXiv:2312.13150
   Tang JX, 2024, Arxiv, DOI arXiv:2402.05054
   Tang JX, 2024, Arxiv, DOI [arXiv:2309.16653, 10.48550/arXiv.2309.16653]
   Turkulainen M, 2024, Arxiv, DOI arXiv:2403.17822
   Verbin D, 2022, PROC CVPR IEEE, P5481, DOI 10.1109/CVPR52688.2022.00541
   Vilesov P., 2023, arXiv
   Waczyńska J, 2024, Arxiv, DOI [arXiv:2402.01459, DOI 10.48550/ARXIV.2402.01459]
   Wang HY, 2023, PROC CVPR IEEE, P13293, DOI 10.1109/CVPR52729.2023.01277
   Wang J, 2024, Arxiv, DOI arXiv:2312.01632
   Wang JJ, 2024, Arxiv, DOI arXiv:2311.16037
   Wang KL, 2024, Arxiv, DOI arXiv:2403.15124
   Wang P., 2021, PROC NEURIPS, V34, P27171
   Wang P, 2023, Arxiv, DOI [arXiv:2312.02201, 10.48550/ARXIV.2312.02201]
   Wang XE, 2024, Arxiv, DOI arXiv:2402.19441
   Wang YX, 2024, Arxiv, DOI arXiv:2403.11868
   Wang ZY, 2023, Arxiv, DOI arXiv:2305.16213
   Wen J, 2024, Arxiv, DOI arXiv:2404.07991
   Wu GJ, 2024, Arxiv, DOI arXiv:2310.08528
   Wu J, 2024, Arxiv, DOI arXiv:2403.08733
   Wu K., 2024, arXiv
   Wu RD, 2023, Arxiv, DOI arXiv:2312.02981
   Wu T, 2024, Arxiv, DOI arXiv:2404.09412
   Wu ZJ, 2024, Arxiv, DOI arXiv:2404.03736
   Xiang J, 2024, Arxiv, DOI arXiv:2312.02214
   Xiao YT, 2024, Arxiv, DOI arXiv:2403.11453
   Xie TY, 2024, Arxiv, DOI arXiv:2311.12198
   Xiong BT, 2024, Arxiv, DOI arXiv:2401.14032
   Xiong HL, 2024, Arxiv, DOI arXiv:2312.00206
   Xu DJ, 2024, Arxiv, DOI arXiv:2401.04099
   Xu TX, 2024, Arxiv, DOI arXiv:2403.10050
   Xu YH, 2024, Arxiv, DOI arXiv:2403.14621
   Xu YL, 2024, Arxiv, DOI arXiv:2312.03029
   Yan C, 2024, Arxiv, DOI arXiv:2311.11700
   Yan YZ, 2024, Arxiv, DOI arXiv:2401.01339
   Yan ZW, 2024, Arxiv, DOI arXiv:2311.17089
   Yang C, 2024, Arxiv, DOI arXiv:2402.10259
   Yang L., 2024, arXiv
   Yang XF, 2023, Arxiv, DOI arXiv:2312.04820
   Yang XR, 2022, INT SYM MIX AUGMENT, P499, DOI 10.1109/ISMAR55827.2022.00066
   Yang Z., 2024, arXiv
   Yang ZY, 2024, Arxiv, DOI arXiv:2310.10642
   Yang ZY, 2023, Arxiv, DOI arXiv:2309.13101
   Yariv L, 2023, PROCEEDINGS OF SIGGRAPH 2023 CONFERENCE PAPERS, SIGGRAPH 2023, DOI 10.1145/3588432.3591536
   Ye CJ, 2024, Arxiv, DOI arXiv:2403.19632
   Ye MQ, 2023, Arxiv, DOI arXiv:2312.00732
   Yi TR, 2024, Arxiv, DOI arXiv:2310.08529
   Yin YY, 2024, Arxiv, DOI arXiv:2312.17225
   Yu A, 2021, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR46437.2021.00455
   Yu H, 2024, Arxiv, DOI arXiv:2312.05664
   Yu M., 2024, arXiv
   Yu X, 2023, Arxiv, DOI arXiv:2310.19415
   Yu YH, 2024, Arxiv, DOI arXiv:2401.16764
   Yu ZH, 2024, Arxiv, DOI arXiv:2404.10772
   Yu ZH, 2023, Arxiv, DOI arXiv:2311.16493
   Yu ZR, 2024, Arxiv, DOI arXiv:2403.20079
   Yuan Y, 2024, Arxiv, DOI arXiv:2312.11461
   Yugay V, 2023, Arxiv, DOI arXiv:2312.10070
   Zeghidour N, 2022, IEEE-ACM T AUDIO SPE, V30, P495, DOI 10.1109/TASLP.2021.3129994
   Zeng Y., 2024, arXiv
   Zhang BW, 2024, Arxiv, DOI arXiv:2403.19655
   Zhang DX, 2024, Arxiv, DOI arXiv:2404.05220
   Zhang JH, 2024, Arxiv, DOI arXiv:2403.06908
   Zhang JW, 2023, Arxiv, DOI arXiv:2312.13271
   Zhang LM, 2023, IEEE I CONF COMP VIS, P3813, DOI 10.1109/ICCV51070.2023.00355
   Zhang S, 2024, Arxiv, DOI arXiv:2403.19586
   Zhang TY, 2024, Arxiv, DOI arXiv:2403.11427
   Zhang XJ, 2024, Arxiv, DOI arXiv:2403.08551
   Zhang Z., 2024, arXiv
   Zhao LZ, 2024, Arxiv, DOI arXiv:2403.11831
   Zhao ZY, 2024, Arxiv, DOI arXiv:2401.12900
   Zheng SY, 2024, Arxiv, DOI arXiv:2312.02155
   Zheng YH, 2024, Arxiv, DOI arXiv:2403.09637
   Zhong LC, 2024, Arxiv, DOI arXiv:2403.09434
   Zhou HY, 2024, Arxiv, DOI arXiv:2403.12722
   Zhou SJ, 2024, Arxiv, DOI arXiv:2404.06903
   Zhou SJ, 2024, Arxiv, DOI arXiv:2312.03203
   Zhou XY, 2024, Arxiv, DOI arXiv:2402.07207
   Zhou XY, 2024, Arxiv, DOI arXiv:2312.07920
   Zhou ZL, 2024, Arxiv, DOI arXiv:2402.06149
   Zhu LT, 2024, Arxiv, DOI arXiv:2401.11535
   Zhu ST, 2024, Arxiv, DOI arXiv:2403.07494
   Zhu ZH, 2024, Arxiv, DOI arXiv:2312.00451
   Zhu ZH, 2022, PROC CVPR IEEE, P12776, DOI 10.1109/CVPR52688.2022.01245
   Zhuang Y., 2023, P SIGGRAPH AS C PAP
   Zielonka W, 2023, Arxiv, DOI [arXiv:2311.08581, 10.48550/ARXIV.2311.08581]
   Zou ZX, 2023, Arxiv, DOI arXiv:2312.09147
   Zuo XX, 2024, Arxiv, DOI arXiv:2401.01970
   Zwicker M, 2001, IEEE VISUAL, P29, DOI 10.1109/VISUAL.2001.964490
   Zwicker M, 2001, COMP GRAPH, P371, DOI 10.1145/383259.383300
NR 297
TC 0
Z9 0
U1 8
U2 8
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD 2024 JUL 8
PY 2024
DI 10.1007/s41095-024-0436-y
EA JUL 2024
PG 30
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XS5X0
UT WOS:001263690100001
DA 2024-08-05
ER

PT J
AU Bao, ZY
   Fu, G
   Chen, ZP
   Xiao, CX
AF Bao, Zhongyun
   Fu, Gang
   Chen, Zipei
   Xiao, Chunxia
TI Illuminator: Image-based illumination editing for indoor scene
   harmonization
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article; Early Access
DE indoor scene illumination harmonization; object illumination editing;
   seamless integration; shadow residual generation
AB Illumination harmonization is an important but challenging task that aims to achieve illumination compatibility between the foreground and background under different illumination conditions. Most current studies mainly focus on achieving seamless integration between the appearance (illumination or visual style) of the foreground object itself and the background scene or producing the foreground shadow. They rarely considered global illumination consistency (i.e., the illumination and shadow of the foreground object). In our work, we introduce "Illuminator", an image-based illumination editing technique. This method aims to achieve more realistic global illumination harmonization, ensuring consistent illumination and plausible shadows in complex indoor environments. The Illuminator contains a shadow residual generation branch and an object illumination transfer branch. The shadow residual generation branch introduces a novel attention-aware graph convolutional mechanism to achieve reasonable foreground shadow generation. The object illumination transfer branch primarily transfers background illumination to the foreground region. In addition, we construct a real-world indoor illumination harmonization dataset called RIH, which consists of various foreground objects and background scenes captured under diverse illumination conditions for training and evaluating our Illuminator. Our comprehensive experiments, conducted on the RIH dataset and a collection of real-world everyday life photos, validate the effectiveness of our method.
C1 [Bao, Zhongyun; Fu, Gang; Chen, Zipei; Xiao, Chunxia] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
C3 Wuhan University
RP Xiao, CX (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
EM zhongyunbao@whu.edu.cn; xyzgfu@whu.edu.cn; czpp19@whu.edu.cn;
   cxxiao@whu.edu.cn
FU National Natural Science Foundation of China [61972298, 62372336];
   CAAI-Huawei MindSpore Open Fund; Wuhan University-Huawei Geolnformatices
   Innovation Lab
FX This work was partially supported by the National Natural Science
   Foundation of China (61972298 and 62372336), CAAI-Huawei MindSpore Open
   Fund, and Wuhan University-Huawei Geolnformatices Innovation Lab.
CR Arief I, 2012, COLOR IMAG CONF, P111
   Bao ZY, 2022, PROC CVPR IEEE, P18521, DOI 10.1109/CVPR52688.2022.01799
   Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712
   BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029
   Cao T, 2022, PROC CVPR IEEE, P3773, DOI 10.1109/CVPR52688.2022.00376
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Chen ZP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4723, DOI 10.1109/ICCV48922.2021.00470
   Cong W., 2021, IEEE INT C MULTIMEDI, P1
   Cong WY, 2022, PROC CVPR IEEE, P18449, DOI 10.1109/CVPR52688.2022.01792
   Cun XD, 2020, IEEE T IMAGE PROCESS, V29, P4759, DOI 10.1109/TIP.2020.2975979
   Demir U, 2018, Arxiv, DOI arXiv:1803.07422
   Eisemann E, 2004, ACM T GRAPHIC, V23, P673, DOI 10.1145/1015706.1015778
   Fang F, 2020, J COMPUT SCI TECH-CH, V35, P522, DOI 10.1007/s11390-020-0305-9
   Fu G, 2021, PROC CVPR IEEE, P7748, DOI 10.1109/CVPR46437.2021.00766
   Fu K, 2022, COMPUT VIS MEDIA, V8, P509, DOI 10.1007/s41095-021-0256-2
   Fu YP, 2020, PROC CVPR IEEE, P5949, DOI 10.1109/CVPR42600.2020.00599
   Grosse R, 2009, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2009.5459428
   Guerreiro JJA, 2023, PROC CVPR IEEE, P5917, DOI 10.1109/CVPR52729.2023.00573
   Guo KW, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356571
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   Guo Z., 2021, P IEEE CVF INT C COM, P14870
   Guo ZH, 2021, PROC CVPR IEEE, P16362, DOI 10.1109/CVPR46437.2021.01610
   Hang Yucheng, 2022, CVPR, P19710
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Helou ME, 2020, Arxiv, DOI arXiv:2005.05460
   Hong Y., 2021, arXiv
   Huang HZ, 2020, IEEE T IMAGE PROCESS, V29, P214, DOI 10.1109/TIP.2019.2925550
   Jiang YF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4812, DOI 10.1109/ICCV48922.2021.00479
   Jia JY, 2006, ACM T GRAPHIC, V25, P631, DOI 10.1145/1141911.1141934
   Karsch K, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2602146
   Ke ZH, 2022, LECT NOTES COMPUT SC, V13675, P690, DOI 10.1007/978-3-031-19784-0_40
   Kee E, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629646
   Lai WS, 2016, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2016.188
   Lalonde JF, 2007, IEEE I CONF COMP VIS, P2181
   Lan YQ, 2022, COMPUT VIS MEDIA, V8, P395, DOI 10.1007/s41095-021-0252-6
   Li MC, 2022, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR52688.2022.00278
   Li YZ, 2023, J COMPUT SCI TECH-CH, V38, P510, DOI 10.1007/s11390-023-3088-y
   Li YZ, 2022, COMPUT VIS MEDIA, V8, P631, DOI 10.1007/s41095-022-0279-3
   Liao B, 2019, COMPUT GRAPH-UK, V82, P53, DOI 10.1016/j.cag.2019.05.007
   Lin JK, 2020, PROC CVPR IEEE, P5890, DOI 10.1109/CVPR42600.2020.00593
   Lin Z, 2023, COMPUT VIS MEDIA, V9, P753, DOI 10.1007/s41095-022-0302-8
   Ling J, 2021, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR46437.2021.00924
   Liu B, 2017, J COMPUT SCI TECH-CH, V32, P430, DOI 10.1007/s11390-017-1734-y
   Liu D., 2020, P IEEE CVF C COMP VI, P8136
   Liu S, 2023, PROC CVPR IEEE, P18290, DOI 10.1109/CVPR52729.2023.01754
   Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Niu L, 2023, Arxiv, DOI arXiv:2308.00356
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Pitié F, 2005, IEEE I CONF COMP VIS, P1434
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sheng YC, 2023, PROC CVPR IEEE, P16643, DOI 10.1109/CVPR52729.2023.01597
   Sheng YC, 2021, PROC CVPR IEEE, P4378, DOI 10.1109/CVPR46437.2021.00436
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song YZ, 2023, PROC CVPR IEEE, P18310, DOI 10.1109/CVPR52729.2023.01756
   Sunkavalli K, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778862
   Tao MW, 2013, INT J COMPUT VISION, V103, P178, DOI 10.1007/s11263-012-0579-7
   Tsai YH, 2017, PROC CVPR IEEE, P2799, DOI 10.1109/CVPR.2017.299
   Tsai YH, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925942
   Tse THE, 2022, PROC CVPR IEEE, P1654, DOI 10.1109/CVPR52688.2022.00171
   Veličkovic P, 2018, Arxiv, DOI arXiv:1710.10903
   Wan S, 2021, IEEE T GEOSCI REMOTE, V59, P597, DOI 10.1109/TGRS.2020.2994205
   Wang K, 2023, PROC CVPR IEEE, P5927, DOI 10.1109/CVPR52729.2023.00574
   Wang TY, 2020, PROC CVPR IEEE, P1877, DOI 10.1109/CVPR42600.2020.00195
   Wenyan Cong, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8391, DOI 10.1109/CVPR42600.2020.00842
   Worchel M, 2023, PROC CVPR IEEE, P142, DOI 10.1109/CVPR52729.2023.00022
   Wu SC, 2023, PROC CVPR IEEE, P5064, DOI 10.1109/CVPR52729.2023.00490
   Xu ZX, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201313
   Xue B, 2022, LECT NOTES COMPUT SC, V13667, P300, DOI 10.1007/978-3-031-20071-7_18
   Xue S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185580
   Yu HN, 2021, COMPUT GRAPH FORUM, V40, P181, DOI 10.1111/cgf.14412
   Yu JQ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480565
   Zakharov E, 2019, IEEE I CONF COMP VIS, P9458, DOI 10.1109/ICCV.2019.00955
   Zhan Fangneng, 2020, P AS C COMP VIS
   Zhang JS, 2019, PROC CVPR IEEE, P10150, DOI 10.1109/CVPR.2019.01040
   Zhang SY, 2019, COMPUT VIS MEDIA, V5, P105, DOI 10.1007/s41095-019-0136-1
NR 77
TC 0
Z9 0
U1 3
U2 3
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD 2024 JUL 5
PY 2024
DI 10.1007/s41095-023-0397-6
EA JUL 2024
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP5H1
UT WOS:001262893200001
OA gold
DA 2024-08-05
ER

PT J
AU Liu, XD
   Wang, LL
AF Liu, Xinda
   Wang, Lili
TI Multi-granularity sequence generation for hierarchical image
   classification
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE hierarchical multi-granularity classification; vision and text
   transformer; sequence generation; fine-grained image recognition;
   cross-modality attention
AB Hierarchical multi-granularity image classification is a challenging task that aims to tag each given image with multiple granularity labels simultaneously. Existing methods tend to overlook that different image regions contribute differently to label prediction at different granularities, and also insufficiently consider relationships between the hierarchical multi-granularity labels. We introduce a sequence-to-sequence mechanism to overcome these two problems and propose a multi-granularity sequence generation (MGSG) approach for the hierarchical multi-granularity image classification task. Specifically, we introduce a transformer architecture to encode the image into visual representation sequences. Next, we traverse the taxonomic tree and organize the multi-granularity labels into sequences, and vectorize them and add positional information. The proposed multi-granularity sequence generation method builds a decoder that takes visual representation sequences and semantic label embedding as inputs, and outputs the predicted multi-granularity label sequence. The decoder models dependencies and correlations between multi-granularity labels through a masked multi-head self-attention mechanism, and relates visual information to the semantic label information through a cross-modality attention mechanism. In this way, the proposed method preserves the relationships between labels at different granularity levels and takes into account the influence of different image regions on labels with different granularities. Evaluations on six public benchmarks qualitatively and quantitatively demonstrate the advantages of the proposed method. Our project is available at https://github.com/liuxindazz/mgsg.
C1 [Liu, Xinda; Wang, Lili] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Wang, Lili] Peng Cheng Lab, Shengzhen 518000, Peoples R China.
C3 Beihang University; Peng Cheng Laboratory
RP Wang, LL (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.; Wang, LL (corresponding author), Peng Cheng Lab, Shengzhen 518000, Peoples R China.
EM iuxinda@buaa.edu.cn; wanglily@buaa.edu.cn
RI Liu, Xinda/X-1337-2019
OI Liu, Xinda/0000-0002-1981-7243
FU National Key R&D Program of China [2019YFC1521102]; National Natural
   Science Foundation of China [61932003]; Beijing Science and Technology
   Plan [Z221100007722004]
FX This work was supported by National Key R&D Program of China
   (2019YFC1521102), the National Natural Science Foundation of China
   (61932003), and Beijing Science and Technology Plan (Z221100007722004).
CR Barutcuoglu Z, 2006, BIOINFORMATICS, V22, P830, DOI 10.1093/bioinformatics/btk048
   Brown T. B., 2020, P 34 INT C NEURAL IN, P1
   Cao Y, 2019, IEEE ICC
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Cesa-Bianchi N, 2006, J MACH LEARN RES, V7, P31
   Chang DL, 2021, PROC CVPR IEEE, P11471, DOI 10.1109/CVPR46437.2021.01131
   Chen HT, 2023, IEEE T KNOWL DATA EN, V35, P7263, DOI 10.1109/TKDE.2022.3188335
   Chen Lin, 2017, [Computational Visual Media, 计算可视媒体], V3, P83
   Chen TS, 2022, IEEE T PATTERN ANAL, V44, P1371, DOI 10.1109/TPAMI.2020.3025814
   Chen TS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P2023, DOI 10.1145/3240508.3240523
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Chou P.-Y., 2022, arXiv, DOI DOI 10.48550/ARXIV.2202.03822
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dimitrovski I, 2011, PATTERN RECOGN, V44, P2436, DOI 10.1016/j.patcog.2011.03.026
   Donahue J, 2014, PR MACH LEARN RES, V32
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Fan JP, 2017, IEEE T IMAGE PROCESS, V26, P1923, DOI 10.1109/TIP.2017.2667405
   Ge WF, 2019, PROC CVPR IEEE, P3029, DOI 10.1109/CVPR.2019.00315
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   He J, 2022, AAAI CONF ARTIF INTE, P852
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu YQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4239, DOI 10.1145/3474085.3475561
   Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132
   Jiang SQ, 2020, IEEE T IMAGE PROCESS, V29, P265, DOI 10.1109/TIP.2019.2929447
   Kai-Xuan Chen, 2018, Computational Visual Media, V4, P245, DOI 10.1007/s41095-018-0119-7
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Li LL, 2022, PROC CVPR IEEE, P1236, DOI 10.1109/CVPR52688.2022.00131
   Lin TY, 2018, IEEE T PATTERN ANAL, V40, P1309, DOI 10.1109/TPAMI.2017.2723400
   Liu DY, 2023, IEEE T NEUR NET LEAR, V34, P8589, DOI 10.1109/TNNLS.2022.3151631
   Liu XD, 2022, NEUROCOMPUTING, V492, P137, DOI 10.1016/j.neucom.2022.04.037
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Maji S, 2013, Arxiv, DOI arXiv:1306.5151
   Min WQ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P393, DOI 10.1145/3394171.3414031
   Min WQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1331, DOI 10.1145/3343031.3350948
   Min WQ, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3329168
   Niu K, 2020, IEEE T IMAGE PROCESS, V29, P5542, DOI 10.1109/TIP.2020.2984883
   Ren Jieyi, 2017, [Computational Visual Media, 计算可视媒体], V3, P379
   Ren YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6773, DOI 10.1109/ICCV48922.2021.00672
   Rousu J, 2006, J MACH LEARN RES, V7, P1601
   Ruoyi Du, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P153, DOI 10.1007/978-3-030-58565-5_10
   Sheng KK, 2021, COMPUT VIS MEDIA, V7, P139, DOI 10.1007/s41095-020-0193-5
   Silla CN, 2011, DATA MIN KNOWL DISC, V22, P31, DOI 10.1007/s10618-010-0175-9
   Sun GL, 2020, AAAI CONF ARTIF INTE, V34, P12047
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Triguero I, 2016, PATTERN RECOGN, V56, P170, DOI 10.1016/j.patcog.2016.02.017
   Vaswani A, 2017, ADV NEUR IN, V30
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang Jun, 2021, BMVC
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang RZ, 2023, Arxiv, DOI [arXiv:2112.02353, 10.48550/arXiv.2112.02353]
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Y, 2022, INFORM SCIENCES, V586, P644, DOI 10.1016/j.ins.2021.12.009
   Wang Y, 2023, IEEE T NEUR NET LEAR, V34, P761, DOI 10.1109/TNNLS.2021.3100928
   Wang Y, 2022, IEEE T CYBERNETICS, V52, P9546, DOI 10.1109/TCYB.2021.3059631
   Wang Y, 2020, IEEE T FUZZY SYST, V28, P1395, DOI 10.1109/TFUZZ.2019.2936801
   Xu YF, 2022, COMPUT VIS MEDIA, V8, P33, DOI 10.1007/s41095-021-0247-3
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Ye LW, 2019, PROC CVPR IEEE, P10494, DOI 10.1109/CVPR.2019.01075
   Zhang Y, 2022, INT CONF ACOUST SPEE, P3234, DOI 10.1109/ICASSP43922.2022.9747591
   Zhao TY, 2018, IEEE T IMAGE PROCESS, V27, P4740, DOI 10.1109/TIP.2018.2845118
   Zhu X., 2021, ICLR, P1, DOI DOI 10.48550/ARXIV.2010.04159
   Zhuang PQ, 2020, AAAI CONF ARTIF INTE, V34, P13130
   Zou DN, 2020, COMPUT VIS MEDIA, V6, P477, DOI 10.1007/s41095-020-0184-6
NR 65
TC 0
Z9 0
U1 14
U2 16
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD APR
PY 2024
VL 10
IS 2
BP 243
EP 260
DI 10.1007/s41095-022-0332-2
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DW8M2
UT WOS:001135210800008
OA gold
DA 2024-08-05
ER

PT J
AU Deng, SN
   Wu, LF
   Shi, G
   Xing, LH
   Jian, M
   Xiang, Y
   Dong, RH
AF Deng, Sinuo
   Wu, Lifang
   Shi, Ge
   Xing, Lehao
   Jian, Meng
   Xiang, Ye
   Dong, Ruihai
TI Learning to compose diversified prompts for image emotion classification
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article; Early Access
DE image emotion analysis; multimodal learning; pretraining model; prompt
   tuning
ID PREDICTION
AB Image emotion classification (IEC) aims to extract the abstract emotions evoked in images. Recently, language-supervised methods such as contrastive language-image pretraining (CLIP) have demonstrated superior performance in image understanding. However, the underexplored task of IEC presents three major challenges: a tremendous training objective gap between pretraining and IEC, shared suboptimal prompts, and invariant prompts for all instances. In this study, we propose a general framework that effectively exploits the language-supervised CLIP method for the IEC task. First, a prompt-tuning method that mimics the pretraining objective of CLIP is introduced, to exploit the rich image and text semantics associated with CLIP. Subsequently, instance-specific prompts are automatically composed, conditioning them on the categories and image content of instances, diversifying the prompts, and thus avoiding suboptimal problems. Evaluations on six widely used affective datasets show that the proposed method significantly outperforms state-of-the-art methods (up to 9.29% accuracy gain on the EmotionROI dataset) on IEC tasks with only a few trained parameters. The code is publicly available at https://github.com/dsn0w/PT-DPC/for research purposes.
C1 [Deng, Sinuo; Wu, Lifang; Shi, Ge; Xing, Lehao; Jian, Meng; Xiang, Ye] Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
   [Dong, Ruihai] Univ Coll Dublin, Insight Ctr Data Analyt, Dublin D04 V1W8, Ireland.
C3 Beijing University of Technology; University College Dublin
RP Shi, G (corresponding author), Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
EM shige@bjut.edu.cn
FU National Natural Science Foundation of China [62106010, 61976010,
   62176011, 62236010]
FX This study was supported in part by the National Natural Science
   Foundation of China under Grant Nos. 62106010, 61976010, 62176011, and
   62236010.
CR Balouchian P, 2019, IEEE WINT CONF APPL, P1645, DOI 10.1109/WACV.2019.00180
   Bao H., 2021, arXiv preprint arXiv:2106.08254, DOI DOI 10.48550/ARXIV.2106.08254
   Bojar Ondrej, 2014, P 9 WORKSH STAT MACH, P12, DOI DOI 10.3115/V1/W14-3302
   Borth D., 2013, P 21 ACM INT C MULT, P223, DOI 10.1145/2502081.2502282
   Brown T. B., 2020, P 34 INT C NEURAL IN, P1
   Chen T, 2014, Arxiv, DOI arXiv:1410.8586
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng S., 2021, ARTIFICIAL INTELLIGE, P553, DOI 10.1007/978-3-030-93046-2_47
   Deng SN, 2023, IEEE T AFFECT COMPUT, V14, P3317, DOI 10.1109/TAFFC.2022.3225049
   Deng SN, 2022, LECT NOTES COMPUT SC, P222, DOI 10.1007/978-3-031-00129-1_15
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068
   Han SY, 2020, COMPUT VIS MEDIA, V6, P333, DOI 10.1007/s41095-020-0178-4
   Han X, 2021, AI OPEN, V2, P225, DOI 10.1016/j.aiopen.2021.08.002
   Hanbury A., 2010, P 18 ACM INT C MULTI, V18, P83, DOI DOI 10.1145/1873951.1873965
   Hanjalic A, 2006, IEEE SIGNAL PROC MAG, V23, P90, DOI 10.1109/MSP.2006.1621452
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Houlsby N, 2019, PR MACH LEARN RES, V97
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jiang ZB, 2020, T ASSOC COMPUT LING, V8, P423, DOI 10.1162/tacl_a_00324
   Kosti R, 2020, IEEE T PATTERN ANAL, V42, P2755, DOI 10.1109/TPAMI.2019.2916866
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lester B, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P3045
   Li XLS, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P4582
   Li XY, 2021, COMPUT VIS MEDIA, V7, P407, DOI 10.1007/s41095-021-0217-9
   Liu MS, 2023, PROCEEDINGS OF THE 2023 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2023, P576, DOI 10.1145/3591106.3592222
   Liu P., 2021, arXiv, DOI 10.48550/arXiv.2107.13586
   Mikels JA, 2005, BEHAV RES METHODS, V37, P626, DOI 10.3758/BF03192732
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pan YH, 2019, COMPUT VIS MEDIA, V5, P375, DOI 10.1007/s41095-019-0157-9
   Paszke A, 2019, ADV NEUR IN, V32
   Patricia N, 2014, PROC CVPR IEEE, P1442, DOI 10.1109/CVPR.2014.187
   Peng KC, 2016, IEEE IMAGE PROC, P614, DOI 10.1109/ICIP.2016.7532430
   Radford A, 2021, PR MACH LEARN RES, V139
   Rao TR, 2020, NEURAL PROCESS LETT, V51, P2043, DOI 10.1007/s11063-019-10033-9
   Rao TR, 2019, NEUROCOMPUTING, V333, P429, DOI 10.1016/j.neucom.2018.12.053
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wu LF, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11041404
   Xu LH, 2022, IEEE I C VI COM I PR, DOI 10.1109/VCIP56404.2022.10008846
   Xu ZW, 2023, IEEE T AFFECT COMPUT, V14, P357, DOI 10.1109/TAFFC.2021.3071131
   Xue T, 2023, IEEE T MULTIMEDIA, V25, P243, DOI 10.1109/TMM.2021.3124080
   Yang JY, 2022, IEEE T IMAGE PROCESS, V31, P5189, DOI 10.1109/TIP.2022.3193749
   Yang JY, 2021, IEEE T IMAGE PROCESS, V30, P8686, DOI 10.1109/TIP.2021.3118983
   Yang JF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3266
   Yang JF, 2018, PROC CVPR IEEE, P7584, DOI 10.1109/CVPR.2018.00791
   Yang JF, 2018, IEEE T MULTIMEDIA, V20, P2513, DOI 10.1109/TMM.2018.2803520
   You QZ, 2016, AAAI CONF ARTIF INTE, P308
   You QZ, 2015, AAAI CONF ARTIF INTE, P381
   Zhang HM, 2021, IEEE T MULTIMEDIA, V23, P2033, DOI 10.1109/TMM.2020.3007352
   Zhang J, 2022, NEUROCOMPUTING, V469, P221, DOI 10.1016/j.neucom.2021.10.062
   Zhang YH, 2022, LECT NOTES COMPUT SC, V13686, P418, DOI 10.1007/978-3-031-19809-0_24
   Zhao S., 2022, HUMAN PERCEPTION VIS, P85, DOI [10.1007/978-3-030-81465-6, DOI 10.1007/978-3-030-81465-6_4]
   Zhao SC, 2022, IEEE T PATTERN ANAL, V44, P6729, DOI 10.1109/TPAMI.2021.3094362
   Zhao SC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P192, DOI 10.1145/3343031.3351062
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P47, DOI 10.1145/2647868.2654930
   Zhao Sicheng, 2016, P 24 ACM INT C MULT, P1385
   Zhou KY, 2022, Arxiv, DOI arXiv:2109.01134
   Zhu XG, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3595
NR 61
TC 0
Z9 0
U1 1
U2 1
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD 2024 APR 26
PY 2024
DI 10.1007/s41095-023-0389-6
EA APR 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PN5N8
UT WOS:001214772000001
OA gold
DA 2024-08-05
ER

PT J
AU Leng, BQ
   Huang, JW
   Shen, GL
   Wang, B
AF Leng, Baiqiang
   Huang, Jingwei
   Shen, Guanlin
   Wang, Bin
TI Shape embedding and retrieval in multi-flow deformation
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE deformation; shape retrieval; embedding; reconstruction
AB We propose a unified 3D flow framework for joint learning of shape embedding and deformation for different categories. Our goal is to recover shapes from imperfect point clouds by fitting the best shape template in a shape repository after deformation. Accordingly, we learn a shape embedding for template retrieval and a flow-based network for robust deformation. We note that the deformation flow can be quite different for different shape categories. Therefore, we introduce a novel multi-hub module to learn multiple modes of deformation to incorporate such variation, providing a network which can handle a wide range of objects from different categories. The shape embedding is designed to retrieve the best-fit template as the nearest neighbor in a latent space. We replace the standard fully connected layer with a tiny structure in the embedding that significantly reduces network complexity and further improves deformation quality. Experiments show the superiority of our method to existing state-of-the-art methods via qualitative and quantitative comparisons. Finally, our method provides efficient and flexible deformation that can further be used for novel shape design.
C1 [Leng, Baiqiang; Shen, Guanlin; Wang, Bin] Tsinghua Univ, Sch Software, Beijing, Peoples R China.
   [Huang, Jingwei] Huawei Technol, Shenzhen, Peoples R China.
C3 Tsinghua University; Huawei Technologies
RP Wang, B (corresponding author), Tsinghua Univ, Sch Software, Beijing, Peoples R China.
EM lbq19@mails.tsinghua.edu.cn; huangjingwei6@huawei.com;
   shengl21@mails.tsinghua.edu.cn; wangbins@tsinghua.edu.cn
OI Wang, Bin/0000-0002-5176-9202
FU National Key R&D Program of China [2020YFB1708900]; National Natural
   Science Foundation of China [62072271]
FX This work was supported by the National Key R&D Program of China
   (2020YFB1708900) and the National Natural Science Foundation of China
   (62072271).
CR Achlioptas P, 2018, PR MACH LEARN RES, V80
   Chang A.X., 2015, ArXiv
   Chen MJ, 2020, COMPUT GRAPH-UK, V89, P50, DOI 10.1016/j.cag.2020.05.018
   Chen RTQ, 2018, 32 C NEURAL INFORM P, V31
   Chen ZQ, 2020, PROC CVPR IEEE, P42, DOI 10.1109/CVPR42600.2020.00012
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   Dahnert M, 2019, IEEE I CONF COMP VIS, P8748, DOI 10.1109/ICCV.2019.00884
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   De Cao N, 2020, PR MACH LEARN RES, V115, P1263
   Dinh L., 2016, ARXIV
   Gao L, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356488
   Grathwohl W., 2018, ARXIV
   Groueix T, 2018, LECT NOTES COMPUT SC, V11206, P235, DOI 10.1007/978-3-030-01216-8_15
   Groueix T, 2019, COMPUT GRAPH FORUM, V38, P123, DOI 10.1111/cgf.13794
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Han WK, 2022, COMPUT VIS MEDIA, V8, P585, DOI 10.1007/s41095-021-0260-6
   Hanocka R, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3267347
   Huang CW, 2018, PR MACH LEARN RES, V80
   Igarashi T, 2005, ACM T GRAPHIC, V24, P1134, DOI 10.1145/1073204.1073323
   Ishimtsev Vladislav, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P599, DOI 10.1007/978-3-030-58601-0_36
   Jack D, 2019, LECT NOTES COMPUT SC, V11362, P317, DOI 10.1007/978-3-030-20890-5_21
   Jiang C., 2020, ADV NEURAL INFORM PR, V33, P9745
   Jiang ZH, 2019, PROC CVPR IEEE, P11949, DOI 10.1109/CVPR.2019.01223
   Jin A. B., 2020, P S INTERACTIVE 3D G
   Joshi P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239522
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Kingma DP, 2016, 30 C NEURAL INFORM P, V29
   Kurenkov A, 2018, IEEE WINT CONF APPL, P858, DOI 10.1109/WACV.2018.00099
   LEE DT, 1980, INT J COMPUT INF SCI, V9, P219, DOI 10.1007/BF00977785
   Lee T, 2018, INT CONF 3D VISION, P258, DOI 10.1109/3DV.2018.00038
   Li H, 2008, COMPUT GRAPH FORUM, V27, P1421, DOI 10.1111/j.1467-8659.2008.01282.x
   Li YY, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818071
   Lipman Y, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P181, DOI 10.1109/SMI.2004.1314505
   Lipman Y, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360677
   Liu MM, 2020, IEEE T VIS COMPUT GR, V26, P1702, DOI 10.1109/TVCG.2018.2880737
   Mehr É, 2019, IEEE I CONF COMP VIS, P3473, DOI 10.1109/ICCV.2019.00357
   Nan LL, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366156
   Niemeyer M, 2019, IEEE I CONF COMP VIS, P5378, DOI 10.1109/ICCV.2019.00548
   Niu CJ, 2018, PROC CVPR IEEE, P4521, DOI 10.1109/CVPR.2018.00475
   Pan JY, 2019, IEEE I CONF COMP VIS, P9963, DOI 10.1109/ICCV.2019.01006
   Papamakarios G, 2017, ADV NEUR IN, V30
   Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530
   Sahillioglu Y, 2011, COMPUT GRAPH FORUM, V30, P1461, DOI 10.1111/j.1467-8659.2011.02020.x
   Sorkine O, 2007, Proc. Symposium on Geometry Processing, V4, P109, DOI [DOI 10.1145/1281991.1282006, 10.1145/1073204.1073323]
   Szegedy C, 2014, Arxiv, DOI arXiv:1312.6199
   Tabia H, 2017, NEUROCOMPUTING, V253, P24, DOI 10.1016/j.neucom.2017.01.101
   Tatarchenko M, 2019, PROC CVPR IEEE, P3400, DOI 10.1109/CVPR.2019.00352
   Uy Mikaela Angelina, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P397, DOI 10.1007/978-3-030-58571-6_24
   van den Berg R, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P393
   Van den Oord A., 2016, CoRR, P125, DOI DOI 10.1109/ICASSP.2009.4960364
   van den Oord A, 2016, ADV NEUR IN, V29
   Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4
   Wang WY, 2019, PROC CVPR IEEE, P1038, DOI 10.1109/CVPR.2019.00113
   Weicheng Kuo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P260, DOI 10.1007/978-3-030-58580-8_16
   Wu ZJ, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322956
   Wu ZZ, 2018, COMPUT GRAPH-UK, V70, P140, DOI 10.1016/j.cag.2017.07.013
   Yang GD, 2019, IEEE I CONF COMP VIS, P4540, DOI 10.1109/ICCV.2019.00464
   Yang J, 2018, GRAPH MODELS, V98, P1, DOI 10.1016/j.gmod.2018.05.003
   Yang MY, 2021, PROC CVPR IEEE, P3268, DOI 10.1109/CVPR46437.2021.00328
   Yifan W, 2020, PROC CVPR IEEE, P72, DOI 10.1109/CVPR42600.2020.00015
   Yin KX, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356494
   Yumer ME, 2016, LECT NOTES COMPUT SC, V9910, P294, DOI 10.1007/978-3-319-46466-4_18
   Zhou K, 2010, COMPUT GRAPH FORUM, V29, P319, DOI 10.1111/j.1467-8659.2009.01601.x
NR 64
TC 0
Z9 0
U1 1
U2 1
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2024
VL 10
IS 3
BP 439
EP 451
DI 10.1007/s41095-022-0315-3
EA FEB 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QO3G5
UT WOS:001159523000004
OA gold
DA 2024-08-05
ER

PT J
AU Han, Q
   Li, LF
   Min, WD
   Wang, Q
   Zeng, QP
   Cui, SM
   Chen, JJ
AF Han, Qing
   Li, Longfei
   Min, Weidong
   Wang, Qi
   Zeng, Qingpeng
   Cui, Shimiao
   Chen, Jiongjin
TI Joint training with local soft attention and dual cross-neighbor label
   smoothing for unsupervised person re-identification
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE person re-identification (Re-ID); unsupervised learning (USL); local
   soft attention; joint training; dual cross-neighbor label smoothing
   (DCLS)
ID DOMAIN ADAPTATION
AB Existing unsupervised person re-identification approaches fail to fully capture the fine-grained features of local regions, which can result in people with similar appearances and different identities being assigned the same label after clustering. The identity-independent information contained in different local regions leads to different levels of local noise. To address these challenges, joint training with local soft attention and dual cross-neighbor label smoothing (DCLS) is proposed in this study. First, the joint training is divided into global and local parts, whereby a soft attention mechanism is proposed for the local branch to accurately capture the subtle differences in local regions, which improves the ability of the re-identification model in identifying a person's local significant features. Second, DCLS is designed to progressively mitigate label noise in different local regions. The DCLS uses global and local similarity metrics to semantically align the global and local regions of the person and further determines the proximity association between local regions through the cross information of neighboring regions, thereby achieving label smoothing of the global and local regions throughout the training process. In extensive experiments, the proposed method outperformed existing methods under unsupervised settings on several standard person re-identification datasets.
C1 [Han, Qing; Li, Longfei; Min, Weidong; Wang, Qi; Zeng, Qingpeng; Cui, Shimiao; Chen, Jiongjin] Nanchang Univ, Sch Math & Comp Sci, Nanchang 330031, Peoples R China.
   [Min, Weidong] Nanchang Univ, Inst Metaverse, Nanchang 330031, Peoples R China.
   [Min, Weidong] Jiangxi Key Lab Smart City, Nanchang 330031, Peoples R China.
C3 Nanchang University; Nanchang University
RP Min, WD (corresponding author), Nanchang Univ, Sch Math & Comp Sci, Nanchang 330031, Peoples R China.; Min, WD (corresponding author), Nanchang Univ, Inst Metaverse, Nanchang 330031, Peoples R China.; Min, WD (corresponding author), Jiangxi Key Lab Smart City, Nanchang 330031, Peoples R China.
EM hanqing@ncu.edu.cn; lilongfei@email.ncu.edu.cn; minweidong@ncu.edu.cn;
   wangqi@ncu.edu.cn; zengqingpeng@ncu.edu.cn;
   406100210099@email.ncu.edu.cn; 416100210351@email.ncu.edu.cn
RI Min, Weidong/D-4585-2017
FU National Natural Science Foundation of China [62076117, 62166026];
   Jiangxi Key Laboratory of Smart City [20192BCD40002]; Jiangxi Provincial
   Natural Science Foundation [20224BAB212011]
FX This work was supported by the National Natural Science Foundation of
   China under Grant Nos. 62076117 and 62166026, the Jiangxi Key Laboratory
   of Smart City under Grant No. 20192BCD40002, and Jiangxi Provincial
   Natural Science Foundation under Grant No. 20224BAB212011.
CR Bai Y, 2021, IEEE T IMAGE PROCESS, V30, P6715, DOI 10.1109/TIP.2021.3094140
   Chen H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14940, DOI 10.1109/ICCV48922.2021.01469
   Chen H, 2021, PROC CVPR IEEE, P2004, DOI 10.1109/CVPR46437.2021.00204
   Chen YB, 2019, IEEE I CONF COMP VIS, P232, DOI 10.1109/ICCV.2019.00032
   Chuanchen Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P224, DOI 10.1007/978-3-030-58555-6_14
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Ding Gavin Weiguang, 2019, ARXIV
   Ding YH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3369393
   Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099
   Ester M, 1996, P 2 INT C KNOWL DISC, V96, P226, DOI DOI 10.5555/3001460
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Fang Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P526, DOI 10.1007/978-3-030-58621-8_31
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Ge Y., 2020, P INT C LEARNING REP
   Ge Y., 2020, Advances in neural information processing systems, V33, P11309
   Guangyi Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P643, DOI 10.1007/978-3-030-58598-3_38
   Han Q, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3538490
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He T, 2022, AAAI CONF ARTIF INTE, P879
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu Z, 2021, 2021 7 IEEE INT C NE, P91
   Huang YR, 2020, AAAI CONF ARTIF INTE, V34, P11069
   Ji HXY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3641, DOI 10.1109/ICCV48922.2021.00364
   Jianing Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P483, DOI 10.1007/978-3-030-58586-0_29
   Kaiwei Zeng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13654, DOI 10.1109/CVPR42600.2020.01367
   Kingma D. P., 2014, arXiv
   Li MX, 2020, IEEE T PATTERN ANAL, V42, P1770, DOI 10.1109/TPAMI.2019.2903058
   Li YY, 2022, IEEE T MULTIMEDIA, V24, P415, DOI 10.1109/TMM.2021.3052354
   Li YY, 2021, IEEE T IMAGE PROCESS, V30, P7952, DOI 10.1109/TIP.2021.3112039
   Lin S., 2018, ARXIV
   Lin YT, 2020, PROC CVPR IEEE, P3387, DOI 10.1109/CVPR42600.2020.00345
   Lin YT, 2020, IEEE T IMAGE PROCESS, V29, P5481, DOI 10.1109/TIP.2020.2982826
   Lin YT, 2019, AAAI CONF ARTIF INTE, P8738
   Lukasik M., 2020, INT C MACHINE LEARNI, P6448
   Luo H., 2021, ARXIV
   Pan XG, 2018, LECT NOTES COMPUT SC, V11208, P484, DOI 10.1007/978-3-030-01225-0_29
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sun J, 2021, IEEE T IMAGE PROCESS, V30, P2935, DOI 10.1109/TIP.2021.3056889
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang Q, 2022, IEEE T MULTIMEDIA, V24, P1031, DOI 10.1109/TMM.2021.3104141
   Wang Q, 2021, INFORM SCIENCES, V564, P71, DOI 10.1016/j.ins.2021.02.013
   Wang Q, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2811-8
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang Z, 2020, IEEE T IMAGE PROCESS, V29, P2013, DOI 10.1109/TIP.2019.2946975
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu JL, 2019, IEEE INT CON MULTI, P886, DOI 10.1109/ICME.2019.00157
   Yang QZ, 2019, PROC CVPR IEEE, P3628, DOI 10.1109/CVPR.2019.00375
   Yu HX, 2017, IEEE I CONF COMP VIS, P994, DOI 10.1109/ICCV.2017.113
   Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904
   Zagoruyko S., 2017, ICLR, DOI DOI 10.1016/J.CVIU.2019.07.006.ARXIV:1612.0
   Zhai Y., 2020, COMPUTER VISION ECCV, P594, DOI DOI 10.1007/978-3-030-58571-6_35
   Zhang MY, 2021, AAAI CONF ARTIF INTE, V35, P3360
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zheng KC, 2021, PROC CVPR IEEE, P5306, DOI 10.1109/CVPR46437.2021.00527
   Zheng KC, 2021, AAAI CONF ARTIF INTE, V35, P3538
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhong Zhun, 2017, PROC CVPR IEEE, P1318, DOI DOI 10.1109/CVPR.2017.389
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 65
TC 0
Z9 0
U1 8
U2 8
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2024
VL 10
IS 3
BP 543
EP 558
DI 10.1007/s41095-023-0354-4
EA APR 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QO3G5
UT WOS:001208966500002
OA gold
DA 2024-08-05
ER

PT J
AU Xie, XG
   Gao, Y
   Hou, F
   Hao, AM
   Qin, H
AF Xie, Xueguang
   Gao, Yang
   Hou, Fei
   Hao, Aimin
   Qin, Hong
TI Dynamic ocean inverse modeling based on differentiable rendering
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE inverse modeling; surface reconstruction; wave modeling; ocean waves;
   differentiable rendering (DR)
ID WATER; SIMILARITY
AB Learning and inferring underlying motion patterns of captured 2D scenes and then re-creating dynamic evolution consistent with the real-world natural phenomena have high appeal for graphics and animation. To bridge the technical gap between virtual and real environments, we focus on the inverse modeling and reconstruction of visually consistent and property-verifiable oceans, taking advantage of deep learning and differentiable physics to learn geometry and constitute waves in a self-supervised manner. First, we infer hierarchical geometry using two networks, which are optimized via the differentiable renderer. We extract wave components from the sequence of inferred geometry through a network equipped with a differentiable ocean model. Then, ocean dynamics can be evolved using the reconstructed wave components. Through extensive experiments, we verify that our new method yields satisfactory results for both geometry reconstruction and wave estimation. Moreover, the new framework has the inverse modeling potential to facilitate a host of graphics applications, such as the rapid production of physically accurate scene animation and editing guided by real ocean scenes.
C1 [Xie, Xueguang; Gao, Yang; Hao, Aimin] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Xie, Xueguang; Hao, Aimin] Beihang Univ, Qingdao Res Inst, Qingdao 266100, Peoples R China.
   [Xie, Xueguang; Hao, Aimin] Peng Cheng Lab, Shenzhen 518000, Peoples R China.
   [Hou, Fei] Chinese Acad Sci, Inst Software, SKLCS, Beijing 100190, Peoples R China.
   [Hou, Fei] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 Beihang University; Beihang University; Peng Cheng Laboratory; Chinese
   Academy of Sciences; Institute of Software, CAS; Chinese Academy of
   Sciences; University of Chinese Academy of Sciences, CAS; State
   University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook
RP Gao, Y (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.; Hou, F (corresponding author), Chinese Acad Sci, Inst Software, SKLCS, Beijing 100190, Peoples R China.; Hou, F (corresponding author), Univ Chinese Acad Sci, Beijing 100049, Peoples R China.; Qin, H (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM gaoyangvr@buaa.edu.cn; houfei@ios.ac.cn; qin@cs.stonybrook.edu
RI GAO, Yang/HMO-8142-2023
FU National Natural Science Foundation of China [62002010, 61872347]; CAMS
   Innovation Fund for Medical Sciences [2019-I2M5-016]; Special Plan for
   the Development of Distinguished Young Scientists of ISCAS [Y8RC535018]
FX This work was sponsored by grants from the National Natural Science
   Foundation of China (62002010, 61872347), the CAMS Innovation Fund for
   Medical Sciences (2019-I2M5-016), and the Special Plan for the
   Development of Distinguished Young Scientists of ISCAS (Y8RC535018).
CR Ashikhmin M, 2001, COMPUT GRAPH-UK, V25, P287, DOI 10.1016/S0097-8493(00)00131-X
   Baek S, 2019, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2019.00116
   Bane C, 2017, INT CONF 3D VISION, P412, DOI 10.1109/3DV.2017.00054
   BOUWS E, 1985, J GEOPHYS RES-OCEANS, V90, P975, DOI 10.1029/JC090iC01p00975
   Bruneton E, 2010, COMPUT GRAPH FORUM, V29, P487, DOI 10.1111/j.1467-8659.2009.01618.x
   Chen W., 2019, Advances in Neural Information Processing Systems, P9609
   Cole F, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6068, DOI 10.1109/ICCV48922.2021.00603
   Fournier A., 1986, Computer Graphics, V20, P75, DOI 10.1145/15886.15894
   Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   HASSELMANN DE, 1980, J PHYS OCEANOGR, V10, P1264, DOI 10.1175/1520-0485(1980)010<1264:DWSODJ>2.0.CO;2
   Hasselmann K., 1973, DHZ, V8, P95, DOI DOI 10.1093/IJE/27.2.335
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henderson P, 2020, INT J COMPUT VISION, V128, P835, DOI 10.1007/s11263-019-01219-8
   Hopper R, 2017, ACM SIGGRAPH 2017 TALKS, DOI 10.1145/3084363.3085044
   Hu YH, 2006, COMPUT ANIMAT VIRT W, V17, P59, DOI 10.1002/cav.74
   Huang LB, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480495
   Kato H, 2018, PROC CVPR IEEE, P3907, DOI 10.1109/CVPR.2018.00411
   Layton AT, 2002, VISUAL COMPUT, V18, P41, DOI 10.1007/s003710100131
   Li C, 2021, IEEE T VIS COMPUT GR, V27, P3867, DOI 10.1109/TVCG.2020.2991217
   Li TM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275109
   Liu SC, 2019, IEEE I CONF COMP VIS, P7707, DOI 10.1109/ICCV.2019.00780
   Loper MM, 2014, LECT NOTES COMPUT SC, V8695, P154, DOI 10.1007/978-3-319-10584-0_11
   Max N. L., 1981, Computer Graphics, V15, P317, DOI 10.1145/965161.806820
   Mildenhall Ben, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P405, DOI 10.1007/978-3-030-58452-8_24
   Nielsen UD, 2020, MAR STRUCT, V69, DOI 10.1016/j.marstruc.2019.102682
   Nimier-David M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356498
   Pavlakos G, 2018, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2018.00055
   Peachey D. R., 1986, Computer Graphics, V20, P65, DOI 10.1145/15886.15893
   PIERSON WJ, 1964, J GEOPHYS RES, V69, P5181, DOI 10.1029/JZ069i024p05181
   Podee N, 2021, COMPUT VIS MEDIA, V7, P201, DOI 10.1007/s41095-021-0204-1
   Premoze S, 2001, COMPUT GRAPH FORUM, V20, P189, DOI 10.1111/1467-8659.00548
   Qiu S, 2021, COMPUT GRAPH FORUM, V40, P242, DOI 10.1111/cgf.14270
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schneider J., 2001, Vision, Modeling, and Visualization 2001. Proceedings, P211
   Tessendorf J., 2001, Simulating Nature: Realistic and Interactive Techniques Course Notes on SIGGRAPH, V3, P26
   Thapa S, 2020, PROC CVPR IEEE, P21, DOI 10.1109/CVPR42600.2020.00010
   Tulsiani S, 2022, IEEE T PATTERN ANAL, V44, P8754, DOI 10.1109/TPAMI.2019.2898859
   Vasavi S., 2021, Global Transitions Proc, V2, P145, DOI [DOI 10.1016/J.GLTP.2021.08.063, 10.1016/j.gltp.2021.08.063]
   Xie XG, 2019, COMPUT ANIMAT VIRT W, V30, DOI 10.1002/cav.1896
   Xie Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201304
   Xiong SY, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530150
NR 42
TC 1
Z9 1
U1 4
U2 4
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD APR
PY 2024
VL 10
IS 2
BP 279
EP 294
DI 10.1007/s41095-023-0338-4
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DW8M2
UT WOS:001135210800003
OA gold
DA 2024-08-05
ER

PT J
AU Liu, PF
   Deng, WJ
   Li, HD
   Wang, JT
   Zheng, YL
   Ding, YW
   Guo, XH
   Zeng, M
AF Liu, Pengfei
   Deng, Wenjin
   Li, Hengda
   Wang, Jintai
   Zheng, Yinglin
   Ding, Yiwei
   Guo, Xiaohu
   Zeng, Ming
TI MusicFace: Music-driven expressive singing face synthesis
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE face synthesis; singing; music; generative adversarial network
ID VIRTUAL HEAD; VIDEO; TEXT
AB It remains an interesting and challenging problem to synthesize a vivid and realistic singing face driven by music. In this paper, we present a method for this task with natural motions for the lips, facial expression, head pose, and eyes. Due to the coupling of mixed information for the human voice and backing music in common music audio signals, we design a decouple-and-fuse strategy to tackle the challenge. We first decompose the input music audio into a human voice stream and a backing music stream. Due to the implicit and complicated correlation between the two-stream input signals and the dynamics of the facial expressions, head motions, and eye states, we model their relationship with an attention scheme, where the effects of the two streams are fused seamlessly. Furthermore, to improve the expressivenes of the generated results, we decompose head movement generation in terms of speed and direction, and decompose eye state generation into short-term blinking and long-term eye closing, modeling them separately. We have also built a novel dataset, SingingFace, to support training and evaluation of models for this task, including future work on this topic. Extensive experiments and a user study show that our proposed method is capable of synthesizing vivid singing faces, qualitatively and quantitatively better than the prior state-of-the-art.
C1 [Liu, Pengfei; Deng, Wenjin; Li, Hengda; Wang, Jintai; Zheng, Yinglin; Ding, Yiwei; Zeng, Ming] Xiamen Univ, Sch Informat, Xiamen 361000, Peoples R China.
   [Guo, Xiaohu] Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75080 USA.
C3 Xiamen University; University of Texas System; University of Texas
   Dallas
RP Zeng, M (corresponding author), Xiamen Univ, Sch Informat, Xiamen 361000, Peoples R China.
EM zengming@xmu.edu.cn
RI zhu, hao/KHW-3813-2024; guo, yi/KHC-4669-2024; liu, qi/KHC-7509-2024;
   Chen, Yang/KHD-8849-2024; li, jing/KHC-8303-2024; zhang,
   yan/KHC-3163-2024; Zhang, Lu/KHE-5879-2024; Liu, Yu/KFS-0769-2024; liu,
   qi/KFA-4047-2024; su, lin/KHC-5034-2024; li, cheng/KCZ-0615-2024; ren,
   jun/KHG-7717-2024
FU National Key R&D Program of China [2021YFC3300403]; National Natural
   Science Foundation of China [62072382]; Yango Charitable Foundation;
   National Science Foundation [OAC-2007661]
FX This work was supported in part by grants from the National Key R&D
   Program of China (2021YFC3300403), National Natural Science Foundation
   of China (62072382), Yango Charitable Foundation, and the National
   Science Foundation (OAC-2007661).
CR Alemi O., 2017, networks, V8, P26
   Baltrusaitis T, 2016, IEEE WINT CONF APPL
   Brand M, 1999, COMP GRAPH, P21, DOI 10.1145/311535.311537
   Bregler C., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P353, DOI 10.1145/258734.258880
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Cardle M, 2002, 20TH EUROGRAPHICS UK CONFERENCE, PROCEEDINGS, P38, DOI 10.1109/EGUK.2002.1011270
   Chen LL, 2019, PROC CVPR IEEE, P7824, DOI 10.1109/CVPR.2019.00802
   Chen LL, 2018, LECT NOTES COMPUT SC, V11211, P538, DOI 10.1007/978-3-030-01234-2_32
   Chung JS, 2017, LECT NOTES COMPUT SC, V10117, P251, DOI 10.1007/978-3-319-54427-4_19
   Chung JS, 2017, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2017.367
   Chung Joon Son, 2018, arXiv
   Cudeiro D, 2019, PROC CVPR IEEE, P10093, DOI 10.1109/CVPR.2019.01034
   Das Dipanjan, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P408, DOI 10.1007/978-3-030-58577-8_25
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   Ezzat T, 2002, ACM T GRAPHIC, V21, P388, DOI 10.1145/566570.566594
   Fawaz HI, 2019, DATA MIN KNOWL DISC, V33, P917, DOI 10.1007/s10618-019-00619-1
   Fried O, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323028
   Guo YD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5764, DOI 10.1109/ICCV48922.2021.00573
   Hennequin Romain, 2020, J. Open Source Software, V5, P2154, DOI [10.21105/joss.02154, DOI 10.21105/JOSS.02154]
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang RZ, 2021, Arxiv, DOI arXiv:2006.06119
   Iwase S., 2020, P SIGGRAPH AS 2020 T, P1
   Ji XY, 2021, PROC CVPR IEEE, P14075, DOI 10.1109/CVPR46437.2021.01386
   Kao HK, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P147, DOI 10.1145/3394171.3413848
   Lee HY, 2019, Arxiv, DOI arXiv:1911.02001
   Lee JH, 2018, Arxiv, DOI arXiv:1811.00818
   Lee M, 2013, MULTIMED TOOLS APPL, V62, P895, DOI 10.1007/s11042-012-1288-5
   Lele Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P35, DOI 10.1007/978-3-030-58545-7_3
   Li Jiaman, 2020, arXiv
   Li LC, 2021, AAAI CONF ARTIF INTE, V35, P1911
   Li RL, 2021, Arxiv, DOI arXiv:2101.08779
   Li YJ, 2015, PR MACH LEARN RES, V37, P1718
   Lu YX, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480484
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Marcos S, 2010, INTERACT COMPUT, V22, P176, DOI 10.1016/j.intcom.2009.12.002
   Pan YF, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555408
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Prajwal KR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P484, DOI 10.1145/3394171.3413532
   Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50
   Shimba T, 2015, 2015 IEEE/SICE INTERNATIONAL SYMPOSIUM ON SYSTEM INTEGRATION (SII), P100, DOI 10.1109/SII.2015.7404961
   Shiratori T, 2006, COMPUT GRAPH FORUM, V25, P449, DOI 10.1111/j.1467-8659.2006.00964.x
   Si S., 2021, arXiv
   Sinha S, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9206665
   Sun GF, 2021, IEEE T MULTIMEDIA, V23, P497, DOI 10.1109/TMM.2020.2981989
   Suwajanakorn S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073640
   Tang TR, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1598, DOI 10.1145/3240508.3240526
   Thies Justus, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P716, DOI 10.1007/978-3-030-58517-4_42
   Thies J, 2020, Arxiv, DOI arXiv:2007.14808
   Wang L., 2011, INTERSPEECH, P3307
   Wang SZ, 2022, AAAI CONF ARTIF INTE, P2531
   Wang Suzhen, 2021, arXiv
   Wang ZP, 2019, ADJUNCT PROCEEDINGS OF THE 2019 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT 2019), P233, DOI 10.1109/ISMAR-Adjunct.2019.00-40
   Wen X, 2020, IEEE T VIS COMPUT GR, V26, P3457, DOI 10.1109/TVCG.2020.3023573
   Xie TY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1739, DOI 10.1145/3474085.3475318
   Yalta N, 2019, IEEE IJCNN
   Yao XW, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3449063
   Ye ZJ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P744, DOI 10.1145/3394171.3414005
   Yi R, 2020, Arxiv, DOI arXiv:2002.10137
   Yu J, 2015, IEEE T CYBERNETICS, V45, P977, DOI 10.1109/TCYB.2014.2341737
   Zakharov E, 2019, IEEE I CONF COMP VIS, P9458, DOI 10.1109/ICCV.2019.00955
   Zhang C., 2021, arXiv
   Zhang CX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3847, DOI 10.1109/ICCV48922.2021.00384
   Zhang CX, 2023, IEEE T VIS COMPUT GR, V29, P1438, DOI 10.1109/TVCG.2021.3117484
   Zhang Y.., 2019, arXiv
   Zhang ZM, 2021, PROC CVPR IEEE, P3660, DOI 10.1109/CVPR46437.2021.00366
   Zhou H, 2021, PROC CVPR IEEE, P4174, DOI 10.1109/CVPR46437.2021.00416
   Zhou H, 2019, AAAI CONF ARTIF INTE, P9299
   Zhou Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417774
   Zhuang WL, 2020, Arxiv, DOI [arXiv:2006.05743, 10.48550/arXiv.2006.05743]
NR 69
TC 1
Z9 1
U1 0
U2 1
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD FEB
PY 2024
VL 10
IS 1
BP 119
EP 136
DI 10.1007/s41095-023-0343-7
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Z5WZ7
UT WOS:001112788200007
OA gold
DA 2024-08-05
ER

PT J
AU Cheng, WH
   Shan, Y
AF Cheng, Weihao
   Shan, Ying
TI Learning layout generation for virtual worlds
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE layout generation; virtual worlds; Transformer; land-use
AB The emergence of the metaverse has led to the rapidly increasing demand for the generation of extensive 3D worlds. We consider that an engaging world is built upon a rational layout of multiple land-use areas (e.g., forest, meadow, and farmland). To this end, we propose a generative model of land-use distribution that learns from geographic data. The model is based on a transformer architecture that generates a 2D map of the land-use layout, which can be conditioned on spatial and semantic controls, depending on whether either one or both are provided. This model enables diverse layout generation with user control and layout expansion by extending borders with partial inputs. To generate high-quality and satisfactory layouts, we devise a geometric objective function that supervises the model to perceive layout shapes and regularize generations using geometric priors. Additionally, we devise a planning objective function that supervises the model to perceive progressive composition demands and suppress generations deviating from controls. To evaluate the spatial distribution of the generations, we train an autoencoder to embed land-use layouts into vectors to enable comparison between the real and generated data using the Wasserstein metric, which is inspired by the Fr & eacute;chet inception distance.
C1 [Cheng, Weihao; Shan, Ying] ARC Lab, Shenzhen 518057, Peoples R China.
RP Cheng, WH (corresponding author), ARC Lab, Shenzhen 518057, Peoples R China.
EM whcheng@tencent.com
CR Arroyo DM, 2021, PROC CVPR IEEE, P13637, DOI 10.1109/CVPR46437.2021.01343
   Chang A., 2017, ARXIV
   Chang Angel., 2014, P 2014 C EMP METH NA, P2028, DOI [DOI 10.3115/V1/D14-1217, 10.3115/v1/d14-1217, 10.3115/v1/D14-1217.]
   Chen Q., 2020, P IEEE CVF C COMP VI, P12625
   Deussen O., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P275, DOI 10.1145/280814.280898
   Dhamo H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16332, DOI 10.1109/ICCV48922.2021.01604
   Dionisio JDN, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2480741.2480751
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Ebert D.S., 2002, Texturing modeling: a procedural approach, Vthird
   Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268
   Fischer R, 2020, VISUAL COMPUT, V36, P2263, DOI 10.1007/s00371-020-01920-7
   Freiknecht Jonas, 2017, Multimodal Technologies and Interaction, V1, DOI 10.3390/mti1040027
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guerrero P., 2022, ARXIV
   Gupta K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P984, DOI 10.1109/ICCV48922.2021.00104
   Hao ZK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14052, DOI 10.1109/ICCV48922.2021.01381
   Heusel M., 2017, NeurIPS, P6629
   Jyothi AA, 2019, IEEE I CONF COMP VIS, P9894, DOI 10.1109/ICCV.2019.00999
   Keshavarzi M., 2020, ARXIV
   Kingma D. P., 2013, ARXIV
   Li J., 2019, ARXIV
   Li MY, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3303766
   Li WY, 2023, PROC CVPR IEEE, P16762, DOI 10.1109/CVPR52729.2023.01608
   Liu A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14438, DOI 10.1109/ICCV48922.2021.01419
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo A, 2020, PROC CVPR IEEE, P3753, DOI 10.1109/CVPR42600.2020.00381
   Ma R, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275035
   Merrell P., 2010, ACM SIGGRAPH Asia 2010 papers, P1, DOI [DOI 10.1145/1866158.1866203, 10.1145/1882262.1866203]
   Parish YIH, 2001, COMP GRAPH, P301, DOI 10.1145/383259.383292
   Perlin K, 2002, ACM T GRAPHIC, V21, P681, DOI 10.1145/566570.566636
   Radford L., 2016, Unsupervised representation learning with deep convolutional generative adversarial networks, P1
   Ramesh A., 2022, ARXIV
   Ramesh Aditya, 2021, ARXIV
   Ritchie D, 2019, PROC CVPR IEEE, P6175, DOI 10.1109/CVPR.2019.00634
   Vaswani A., 2017, Advances in neural information processing systems, P5998
   Wang K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201362
   Wang K, 2020, AM J EMERG MED, V38, P132, DOI 10.1016/j.ajem.2019.06.046
   Wang X., 2020, ARXIV
   Wu ZJ, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322956
   Xu LN, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5057, DOI 10.1109/ICCV48922.2021.00503
   Yang CF, 2021, PROC CVPR IEEE, P3731, DOI 10.1109/CVPR46437.2021.00373
   Yu J., 2022, arXiv
   Yu LF, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964981
   Zhou H, 2007, IEEE T VIS COMPUT GR, V13, P834, DOI 10.1109/TVCG.2007.1027
   Zhou Y, 2019, IEEE I CONF COMP VIS, P7383, DOI 10.1109/ICCV.2019.00748
NR 45
TC 0
Z9 0
U1 1
U2 1
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2024
VL 10
IS 3
BP 577
EP 592
DI 10.1007/s41095-023-0365-1
EA MAY 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QO3G5
UT WOS:001216848500002
OA gold
DA 2024-08-05
ER

PT J
AU Maejima, A
   Shinagawa, S
   Kubo, H
   Funatomi, T
   Yotsukura, T
   Nakamura, S
   Mukaigawa, Y
AF Maejima, Akinobu
   Shinagawa, Seitaro
   Kubo, Hiroyuki
   Funatomi, Takuya
   Yotsukura, Tatsuo
   Nakamura, Satoshi
   Mukaigawa, Yasuhiro
TI Continual few-shot patch-based learning for anime-style colorization
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article; Early Access
DE anime; colorization; few-shot learning; continuous learning strategy
AB The automatic colorization of anime line drawings is a challenging problem in production pipelines. Recent advances in deep neural networks have addressed this problem; however, collectingmany images of colorization targets in novel anime work before the colorization process starts leads to chicken-and-egg problems and has become an obstacle to using them in production pipelines. To overcome this obstacle, we propose a new patch-based learning method for few-shot anime-style colorization. The learning method adopts an efficient patch sampling technique with position embedding according to the characteristics of anime line drawings. We also present a continuous learning strategy that continuously updates our colorization model using new samples colorized by human artists. The advantage of our method is that it can learn our colorization model from scratch or pre-trained weights using only a few pre- and post-colorized line drawings that are created by artists in their usual colorization work. Therefore, our method can be easily incorporated within existing production pipelines. We quantitatively demonstrate that our colorizationmethod outperforms state-of-the-art methods.
C1 [Maejima, Akinobu; Yotsukura, Tatsuo] OLM Digital Inc, Res & Dev Div, Adv Res Grp, IMAGICA Grp, Tokyo 1540023, Japan.
   [Shinagawa, Seitaro; Funatomi, Takuya; Nakamura, Satoshi; Mukaigawa, Yasuhiro] Nara Inst Sci & Technol, Grad Sch Sci & Technol, Div Informat Sci, Nara 6300192, Japan.
   [Kubo, Hiroyuki] Chiba Univ, Grad Sch Informat, Chiba 2638522, Japan.
C3 Nara Institute of Science & Technology; Chiba University
RP Maejima, A (corresponding author), OLM Digital Inc, Res & Dev Div, Adv Res Grp, IMAGICA Grp, Tokyo 1540023, Japan.
EM akinobu.maejima@olm.co.jp
FX We would like to thank the anonymous reviewers for their constructive
   comments. We are grateful to Zekun Li and Prof. Fang-Lue Zhang for
   providing their code and data for comparison. We give thanks to Benjamin
   Allen for helping the development of leakproof segmentation, also thank
   Mohammad Shafiq Bin Md Shawal, Muhammad Mohamad Din Yati, Yap Fei,
   Raihanah Ayuna Faiz, Kiyoumi Agemura, and Shogo Sakurazawa for testing
   our colorization system and for providing valuable feedback from the
   production side. We thank Ken Anjyo, Marc Salvati, and Alexandre
   Derouet-Jourdan for reviewing the paper and providing feedback. Finally,
   we would like to give our thanks and appreciation to Junpei Inuzuka and
   IMAGICA INFOS for their permission for us to use images from Restaurant
   to Another World 2 for research purposes.
CR [Anonymous], 2014, Proceedings of the SIGGRAPH Asia Technical Briefs
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Casey Evan, 2021, Proceedings of the IEEE/CVF International Conference on Computer Vision
   Chen SY, 2022, IEEE T VIS COMPUT GR, V28, P1198, DOI 10.1109/TVCG.2020.3009949
   Dang T D Q., 2020, Proceedings of the ACM SIGGRAPH Posters, P44
   Furusawa C, 2017, IGGRAPH ASIA 2017 TECHNICAL BRIEFS (SA'17), DOI 10.1145/3145749.3149430
   Hensman P, 2017, PROC INT CONF DOC, P72, DOI 10.1109/ICDAR.2017.295
   Ishii Daichi, 2020, SIGGRAPH '20: Special Interest Group on Computer Graphics and Interactive Techniques Conference Talk, DOI 10.1145/3388767.3407331
   Kanamori Y., 2012, Proceedings of the SIGGRAPH Asia Technical Briefs, P4
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Li ZK, 2022, LECT NOTES COMPUT SC, V13677, P579, DOI 10.1007/978-3-031-19790-1_35
   Liu SL, 2023, COMPUT VIS MEDIA, V9, P633, DOI 10.1007/s41095-022-0298-0
   Maejima A., 2014, Proceedings of the ACM SIGGRAPH Posters, P13
   Maejima A, 2021, PROCEEDINGS OF SIGGRAPH ASIA 2021 TECHNICAL COMMUNICATIONS, DOI 10.1145/3478512.3488604
   Orzan A, 2013, COMMUN ACM, V56, P101, DOI 10.1145/2483852.2483873
   Qu YG, 2006, ACM T GRAPHIC, V25, P1214, DOI 10.1145/1141911.1142017
   Ramassamy S, 2018, SA'18: SIGGRAPH ASIA 2018 POSTERS, DOI 10.1145/3283289.3283307
   Sangkloy P, 2017, PROC CVPR IEEE, P6836, DOI 10.1109/CVPR.2017.723
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Shi M, 2023, IEEE T VIS COMPUT GR, V29, P2965, DOI 10.1109/TVCG.2022.3146000
   Siyao Li, 2022, Proceedings of the 36th Conference on Neural Information Processing Systems, V35, P18996
   Sykora D., 2004, Proceedings of the 3rd International Symposium on Non-Photorealistic Animation and Rendering, P121, DOI DOI 10.1145/987657.987677
   Sykora D, 2009, COMPUT GRAPH FORUM, V28, P599, DOI 10.1111/j.1467-8659.2009.01400.x
   Texler O, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392453
   Yin J, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530097
   Zhang LM, 2021, PROC CVPR IEEE, P9884, DOI 10.1109/CVPR46437.2021.00976
   Zhang LM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275090
   Zhang R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073703
   Zhu HC, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925872
NR 29
TC 0
Z9 0
U1 1
U2 1
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD 2024 JUL 9
PY 2024
DI 10.1007/s41095-024-0414-4
EA JUL 2024
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YJ9U9
UT WOS:001268247800001
DA 2024-08-05
ER

PT J
AU Yin, ZZ
   Jin, Y
   Fang, ZJ
   Zhang, Y
   Zhang, HX
   Zhou, J
   He, LL
AF Yin, Zhengzheng
   Jin, Yao
   Fang, Zhijian
   Zhang, Yun
   Zhang, Huaxiong
   Zhou, Jiu
   He, Lili
TI Symmetrization of quasi-regular patterns with periodic tilting of
   regular polygons
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE quasi-regular patterns (QRP); k-uniform tilings; invariant mappings;
   symmetry; aesthetic patterns
ID AUTOMATIC-GENERATION; AESTHETIC-PATTERNS; SHAPE GRAMMAR; TILINGS
AB Computer-generated aesthetic patterns are widely used as design materials in various fields. The most common methods use fractals or dynamical systems as basic tools to create various patterns. To enhance aesthetics and controllability, some researchers have introduced symmetric layouts along with these tools. One popular strategy employs dynamical systems compatible with symmetries that construct functions with the desired symmetries. However, these are typically confined to simple planar symmetries. The other generates symmetrical patterns under the constraints of tilings. Although it is slightly more flexible, it is restricted to small ranges of tilings and lacks textural variations. Thus, we proposed a new approach for generating aesthetic patterns by symmetrizing quasi-regular patterns using general k-uniform tilings. We adopted a unified strategy to construct invariant mappings for k-uniform tilings that can eliminate texture seams across the tiling edges. Furthermore, we constructed three types of symmetries associated with the patterns: dihedral, rotational, and reflection symmetries. The proposed method can be easily implemented using GPU shaders and is highly efficient and suitable for complicated tiling with regular polygons. Experiments demonstrated the advantages of our method over state-of-the-art methods in terms of flexibility in controlling the generation of patterns with various parameters as well as the diversity of textures and styles.
C1 [Yin, Zhengzheng; Jin, Yao; Fang, Zhijian; Zhang, Huaxiong; Zhou, Jiu; He, Lili] Zhejiang Sci Tech Univ, Sch Mat Sci & Engn, Hangzhou 310018, Peoples R China.
   [Jin, Yao; Zhang, Huaxiong; He, Lili] Zhejiang Prov Innovat Ctr Adv Text Technol, Shaoxing 312000, Peoples R China.
   [Zhang, Yun] Commun Univ Zhejiang, Sch Media Engn, Hangzhou 310018, Peoples R China.
C3 Zhejiang Sci-Tech University; Communication University of Zhejiang
RP Jin, Y; Zhang, HX (corresponding author), Zhejiang Sci Tech Univ, Sch Mat Sci & Engn, Hangzhou 310018, Peoples R China.; Jin, Y; Zhang, HX (corresponding author), Zhejiang Prov Innovat Ctr Adv Text Technol, Shaoxing 312000, Peoples R China.
EM jinyao@zstu.edu.cn; zhxhz@zstu.edu.cn
RI He, Lili/AAG-1660-2020
FU Key R&D Programs of Zhejiang Province [2023C01224, 2022C01220]; National
   Natural Science Foundation of China [61702458]; Zhejiang Province Public
   Welfare Technology Application Research [LGG22F020009]; Key Lab of Film;
   TV Media Technology of Zhejiang Province [2020E10015]
FX This work was supported by the Key R&D Programs of Zhejiang Province
   (Nos. 2023C01224 and 2022C01220) and the National Natural Science
   Foundation of China (No. 61702458). Yun Zhang was partially supported by
   Zhejiang Province Public Welfare Technology Application Research (No.
   LGG22F020009), and Key Lab of Film and TV Media Technology of Zhejiang
   Province (No. 2020E10015).
CR Carter NC, 1998, CHAOS SOLITON FRACT, V9, P2031, DOI 10.1016/S0960-0779(97)00157-4
   Chung K. W., 1993, Computer Graphics Forum, V12, P33, DOI 10.1111/1467-8659.1210033
   Chung KW, 2004, INT J BIFURCAT CHAOS, V14, P3249, DOI 10.1142/S0218127404011314
   Chung KW, 2004, CHAOS SOLITON FRACT, V19, P1177, DOI 10.1016/S0960-0779(03)00307-2
   Gdawiec Krzysztof, 2011, Advances in Visual Computing. Proceedings 7th International Symposium, ISVC 2011, P691, DOI 10.1007/978-3-642-24031-7_69
   Gdawiec K, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12062923
   Gdawiec K, 2017, INT J AP MAT COM-POL, V27, P827, DOI 10.1515/amcs-2017-0058
   Gdawiec K, 2017, NONLINEAR DYNAM, V90, P2457, DOI 10.1007/s11071-017-3813-6
   Gieseke L, 2021, COMPUT GRAPH FORUM, V40, P585, DOI 10.1111/cgf.142658
   Helt G., 2018, P BRIDGES 2018 MATH, P547
   [贾凤霞 Jia Fengxia], 2017, [纺织学报, Journal of Textile Research], V38, P124
   Kaplan C S., 2009, SYNTHESILECT COMPU, V4, P1, DOI [10.1007/978-3-031-79543-5, DOI 10.1007/978-3-031-79543-5]
   Liu S., 2009, INT J INFORM ENG ELE, V1, P50, DOI [10.5815/ijieeb.2009.01.07, DOI 10.5815/IJIEEB.2009.01.07]
   Liu S., 2018, COMPLEXITY, V2018
   Lu J, 2007, VISUAL COMPUT, V23, P445, DOI 10.1007/s00371-007-0116-9
   Mandelbrot B.B., 1979, Phys. Today, V32, P65, DOI [10.1063/1.2995555, DOI 10.1063/1.2995555]
   Medeiros e Sa, 2017, LIVRO MALHAS ARQUIME
   Sá AME, 2018, SIBGRAPI, P17, DOI 10.1109/SIBGRAPI.2018.00009
   Ouyang PC, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3456298
   Ouyang PC, 2019, APPL MATH COMPUT, V347, P653, DOI 10.1016/j.amc.2018.09.052
   Ouyang PC, 2018, NONLINEAR DYNAM, V94, P261, DOI 10.1007/s11071-018-4357-0
   Ouyang PC, 2015, FRACTALS, V23, DOI 10.1142/S0218348X15500358
   Ouyang PC, 2015, IEEE COMPUT GRAPH, V35, P90, DOI 10.1109/MCG.2015.135
   Ouyang PC, 2014, IEEE COMPUT GRAPH, V34, P68, DOI 10.1109/MCG.2014.6
   Sayed Z, 2016, LECT NOTES COMPUT SC, V9590, P146, DOI 10.1007/978-3-662-53090-0_8
   Sánchez JES, 2021, COMPUT GRAPH-UK, V95, P69, DOI 10.1016/j.cag.2021.01.007
   Speller TH, 2007, COMPLEX SYST, V17, P79
   Szyszkowicz M., 1991, Computer Graphics Forum, V10, P255, DOI 10.1111/1467-8659.1030255
   Wang XC, 2019, FRACTALS, V27, DOI 10.1142/S0218348X19500099
   Yin XX, 2017, HEALTH INFOR SCI, DOI 10.1007/978-3-319-57027-3
   Zaslavsky G M., 1992, PHYS TODAY, V45, P70, DOI [10.1063/1.2809778, DOI 10.1063/1.2809778]
   [张聿 Zhang Yu], 2005, [纺织学报, Journal of Textile Research], V26, P58
   [张聿 Zhang Yu], 2002, [纺织学报, Journal of Textile Research], V23, P27
   Zou YR, 2006, COMPUT GRAPH-UK, V30, P470, DOI 10.1016/j.cag.2006.02.009
NR 34
TC 0
Z9 0
U1 2
U2 2
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2024
VL 10
IS 3
BP 559
EP 576
DI 10.1007/s41095-023-0359-z
EA APR 2024
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QO3G5
UT WOS:001208966500004
OA gold
DA 2024-08-05
ER

PT J
AU Wang, BN
   Xu, FJ
   Zheng, Q
AF Wang, Bingnan
   Xu, Fanjiang
   Zheng, Quan
TI A survey on facial image deblurring
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review
DE facial image deblurring; model-based; deep learning-based; semantic or
   structural prior
ID QUALITY ASSESSMENT
AB When a facial image is blurred, it significantly affects high-level vision tasks such as face recognition. The purpose of facial image deblurring is to recover a clear image from a blurry input image, which can improve the recognition accuracy, etc. However, general deblurring methods do not perform well on facial images. Therefore, some face deblurring methods have been proposed to improve performance by adding semantic or structural information as specific priors according to the characteristics of the facial images. In this paper, we survey and summarize recently published methods for facial image deblurring, most of which are based on deep learning. First, we provide a brief introduction to the modeling of image blurring. Next, we summarize face deblurring methods into two categories: model-based methods and deep learning-based methods. Furthermore, we summarize the datasets, loss functions, and performance evaluation metrics commonly used in the neural network training process. We show the performance of classical methods on these datasets and metrics and provide a brief discussion on the differences between model-based and learning-based methods. Finally, we discuss the current challenges and possible future research directions.
C1 [Wang, Bingnan; Xu, Fanjiang; Zheng, Quan] Chinese Acad Sci, Inst Software, Beijing 100190, Peoples R China.
   [Wang, Bingnan] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS
RP Zheng, Q (corresponding author), Chinese Acad Sci, Inst Software, Beijing 100190, Peoples R China.
EM wangbingnan21@mails.ucas.ac.cn; fanjiang@iscas.ac.cn;
   zhengquan@iscas.ac.cn
OI Zheng, Quan/0000-0001-5053-5511
CR Amos B., 2016, Openface: A general-purpose face recognition library with mobile applications
   Anwar S, 2019, IEEE T PATTERN ANAL, V41, P2112, DOI 10.1109/TPAMI.2018.2855177
   Boracchi G, 2012, IEEE T IMAGE PROCESS, V21, P3502, DOI 10.1109/TIP.2012.2192126
   Chakrabarti A, 2016, LECT NOTES COMPUT SC, V9907, P221, DOI 10.1007/978-3-319-46487-9_14
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491
   Chrysos GG, 2019, INT J COMPUT VISION, V127, P801, DOI 10.1007/s11263-018-1138-7
   Chrysos GG, 2017, IEEE COMPUT SOC CONF, P2015, DOI 10.1109/CVPRW.2017.252
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Gao HY, 2019, PROC CVPR IEEE, P3843, DOI 10.1109/CVPR.2019.00397
   Goodfellow I. J., 2014, ADV NEURAL INFORM PR
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   HaCohen Y, 2013, IEEE I CONF COMP VIS, P2384, DOI 10.1109/ICCV.2013.296
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Hu XB, 2022, IEEE T PATTERN ANAL, V44, P8910, DOI 10.1109/TPAMI.2021.3123085
   Huang G.B., 2008, WORKSH FAC REAL LIF
   HUBER PJ, 1973, ANN STAT, V1, P799, DOI 10.1214/aos/1176342503
   Ignatov A, 2019, LECT NOTES COMPUT SC, V11133, P315, DOI 10.1007/978-3-030-11021-5_20
   Jiang JL, 2014, IEEE T IMAGE PROCESS, V23, P2651, DOI 10.1109/TIP.2014.2317985
   Jin MG, 2018, IEEE COMPUT SOC CONF, P858, DOI 10.1109/CVPRW.2018.00118
   Jung SH, 2022, IEEE WINT CONF APPL, P884, DOI 10.1109/WACV51458.2022.00096
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521
   Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250
   Lai WS, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530131
   Lai WS, 2016, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2016.188
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Lee TB, 2020, IEEE ACCESS, V8, P223548, DOI 10.1109/ACCESS.2020.3033890
   Li C., 2022, arXiv
   Lin SN, 2020, AAAI CONF ARTIF INTE, V34, P11523
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lu BY, 2019, PROC CVPR IEEE, P10217, DOI 10.1109/CVPR.2019.01047
   Lu YZ, 2017, Arxiv, DOI arXiv:1710.00620
   Ma C, 2017, COMPUT VIS IMAGE UND, V158, P1, DOI 10.1016/j.cviu.2016.12.009
   Madam NT, 2018, LECT NOTES COMPUT SC, V11214, P358, DOI 10.1007/978-3-030-01249-6_22
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, CONF REC ASILOMAR C, P1718, DOI 10.1109/ACSSC.2012.6489326
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Ng HW, 2014, IEEE IMAGE PROC, P343, DOI 10.1109/ICIP.2014.7025068
   Nishiyama M, 2011, IEEE T PATTERN ANAL, V33, P838, DOI 10.1109/TPAMI.2010.203
   Pan JS, 2014, LECT NOTES COMPUT SC, V8695, P47, DOI 10.1007/978-3-319-10584-0_4
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Qi Q, 2021, MULTIMED TOOLS APPL, V80, P2975, DOI 10.1007/s11042-020-09460-x
   Ren WQ, 2019, IEEE I CONF COMP VIS, P9387, DOI 10.1109/ICCV.2019.00948
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schuler CJ, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2481418
   Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672
   Shen ZY, 2020, INT J COMPUT VISION, V128, P1829, DOI 10.1007/s11263-019-01288-9
   Shen ZY, 2018, PROC CVPR IEEE, P8260, DOI 10.1109/CVPR.2018.00862
   Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154
   Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8
   Song YB, 2019, INT J COMPUT VISION, V127, P785, DOI 10.1007/s11263-019-01148-6
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tian DY, 2016, IEEE T IMAGE PROCESS, V25, P961, DOI 10.1109/TIP.2015.2509418
   Tian L, 2017, LECT NOTES COMPUT SC, V10116, P576, DOI 10.1007/978-3-319-54407-6_39
   Venkatanath N, 2015, NATL CONF COMMUN
   Wang LX, 2017, Arxiv, DOI arXiv:1711.09515
   Wang XT, 2021, PROC CVPR IEEE, P9164, DOI 10.1109/CVPR46437.2021.00905
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xia Z., 2019, P 33 INT C NEUR INF, P2439
   Xu L., 2014, Advances in NIPS 27, V27, P1790
   Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157
   Xu XY, 2017, IEEE I CONF COMP VIS, P251, DOI 10.1109/ICCV.2017.36
   Yasarla R, 2020, IEEE T IMAGE PROCESS, V29, P6251, DOI 10.1109/TIP.2020.2990354
   Yuan Y, 2020, PROC CVPR IEEE, P3552, DOI 10.1109/CVPR42600.2020.00361
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang HC, 2011, IEEE I CONF COMP VIS, P770, DOI 10.1109/ICCV.2011.6126315
   Zhang JW, 2018, PROC CVPR IEEE, P2521, DOI 10.1109/CVPR.2018.00267
   Zhang KH, 2022, INT J COMPUT VISION, V130, P2103, DOI 10.1007/s11263-022-01633-5
   Zhang M., 2022, P IEEECVF C COMPUTER, P5892
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang X., 2021, arXiv
   Zhu FD, 2022, PROC CVPR IEEE, P7652, DOI 10.1109/CVPR52688.2022.00751
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 80
TC 0
Z9 0
U1 20
U2 23
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD FEB
PY 2024
VL 10
IS 1
BP 3
EP 25
DI 10.1007/s41095-023-0336-6
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Z5WZ7
UT WOS:001112788200004
OA gold
DA 2024-08-05
ER

PT J
AU Liu, GX
   van Kaick, O
   Huang, H
   Hu, RZ
AF Liu, Gengxin
   van Kaick, Oliver
   Huang, Hui
   Hu, Ruizhen
TI Active self-training for weakly supervised 3D scene semantic
   segmentation
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article; Early Access
DE semantic segmentation; weakly supervised; self-training; active learning
AB Since the preparation of labeled data for training semantic segmentation networks of point clouds is a time-consuming process, weakly supervised approaches have been introduced to learn from only a small fraction of data. These methods are typically based on learning with contrastive losses while automatically deriving per-point pseudo-labels from a sparse set of user-annotated labels. In this paper, our key observation is that the selection of which samples to annotate is as important as how these samples are used for training. Thus, we introduce a method for weakly supervised segmentation of 3D scenes that combines self-training with active learning. Active learning selects points for annotation that are likely to result in improvements to the trained model, while self-training makes efficient use of the user-provided labels for learning the model. We demonstrate that our approach leads to an effective method that provides improvements in scene segmentation over previous work and baselines, while requiring only a few user annotations.
C1 [Liu, Gengxin; Huang, Hui; Hu, Ruizhen] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
   [van Kaick, Oliver] Carleton Univ, Sch Comp Sci, Ottawa, ON K1S 5B6, Canada.
C3 Shenzhen University; Carleton University
RP Hu, RZ (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
EM 2100271019@email.szu.edu.cn; Oliver.vanKaick@carleton.ca;
   huihuang@szu.edu.cn; ruizhen.hu@szu.edu.cn
FU Guangdong Natural Science Foundation [2021B1515020085]; Shenzhen Science
   and Technology Program [RCYX20210609103121030]; National Natural Science
   Foundation of China [62322207, 61872250, U2001206, U21B2023]; Department
   of Education of Guangdong Province Innovation Team [2022KCXTD025];
   Shenzhen Science and Technology Innovation Program
   [JCYJ20210324120213036]; Natural Sciences and Engineering Research
   Council of Canada (NSERC); Guangdong Laboratory of Artificial
   Intelligence and Digital Economy (ShenZhen)
FX We thank the anonymous reviewers for their valuable comments. This work
   was supported by Guangdong Natural Science Foundation (2021B1515020085),
   Shenzhen Science and Technology Program (RCYX20210609103121030),
   National Natural Science Foundation of China (62322207, 61872250,
   U2001206, U21B2023), Department of Education of Guangdong Province
   Innovation Team (2022KCXTD025), Shenzhen Science and Technology
   Innovation Program (JCYJ20210324120213036), the Natural Sciences and
   Engineering Research Council of Canada (NSERC), and Guangdong Laboratory
   of Artificial Intelligence and Digital Economy (ShenZhen).
CR [Anonymous], 2021, ACM Transactions on Graphics, V40
   [Anonymous], 2016, ACM Transactions on Graphics, V35
   [Anonymous], 2018, ACM Transactions on Graphics, V37
   Armeni I., 2017, arXiv
   Cheng MM, 2021, AAAI CONF ARTIF INTE, V35, P1140
   Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319
   Dai A, 2018, LECT NOTES COMPUT SC, V11214, P458, DOI 10.1007/978-3-030-01249-6_28
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Gadelha Matheus, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P473, DOI 10.1007/978-3-030-58607-2_28
   Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Han L, 2020, PROC CVPR IEEE, P2937, DOI 10.1109/CVPR42600.2020.00301
   Hou J, 2021, PROC CVPR IEEE, P15582, DOI 10.1109/CVPR46437.2021.01533
   Jiang L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6403, DOI 10.1109/ICCV48922.2021.00636
   Komarichev A, 2019, PROC CVPR IEEE, P7413, DOI 10.1109/CVPR.2019.00760
   Kundu A., 2020, COMPUTER VISIONECCV, P518, DOI [DOI 10.1007/978-3-030-58586-0_31, 10.1007/978-3-030-58586-0_31]
   Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479
   Li YZ, 2018, ADV NEUR IN, V31
   Lin YB, 2018, ISPRS J PHOTOGRAMM, V143, P39, DOI 10.1016/j.isprsjprs.2018.05.004
   Liu ZZ, 2021, PROC CVPR IEEE, P1726, DOI 10.1109/CVPR46437.2021.00177
   Liu ZJ, 2019, ADV NEUR IN, V32
   Paszke A, 2019, ADV NEUR IN, V32
   Peng HT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-018-9689-9
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Rizve M, 2021, Arxiv, DOI [arXiv:2101.06329, 10.48550/arXiv.2101.06329]
   Saining Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P574, DOI 10.1007/978-3-030-58580-8_34
   Shi X., 2021, arXiv
   Su H, 2018, PROC CVPR IEEE, P2530, DOI 10.1109/CVPR.2018.00268
   Tatarchenko M, 2018, PROC CVPR IEEE, P3887, DOI 10.1109/CVPR.2018.00409
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Wei JC, 2020, PROC CVPR IEEE, P4383, DOI 10.1109/CVPR42600.2020.00444
   Wu TH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15490, DOI 10.1109/ICCV48922.2021.01522
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xun Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13703, DOI 10.1109/CVPR42600.2020.01372
   Ye XQ, 2018, LECT NOTES COMPUT SC, V11211, P415, DOI 10.1007/978-3-030-01234-2_25
   Zhang JZ, 2020, PROC CVPR IEEE, P4533, DOI 10.1109/CVPR42600.2020.00459
   Zhang ZW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10232, DOI 10.1109/ICCV48922.2021.01009
NR 39
TC 0
Z9 0
U1 0
U2 0
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD 2024 MAR 22
PY 2024
DI 10.1007/s41095-022-0311-7
EA MAR 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LY3D3
UT WOS:001190323500004
OA gold, Green Submitted
DA 2024-08-05
ER

PT J
AU Zhang, PY
   Wang, D
   Lu, HC
AF Zhang, Pengyu
   Wang, Dong
   Lu, Huchuan
TI Multi-modal visual tracking: Review and experimental comparison
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review
DE visual tracking; object tracking; multi-modal fusion; RGB-T tracking;
   RGB-D tracking
ID CORRELATION FILTER TRACKER; ROBUST OBJECT TRACKING; FUSION TRACKING;
   SPARSE REPRESENTATION; PARTICLE FILTER; INFRARED IMAGES; FRAMEWORK;
   BENCHMARK; COMPLEX; CONTEXT
AB Visual object tracking has been drawing increasing attention in recent years, as a fundamental task in computer vision. To extend the range of tracking applications, researchers have been introducing information from multiple modalities to handle specific scenes, with promising research prospects for emerging methods and benchmarks. To provide a thorough review of multi-modal tracking, different aspects of multi-modal tracking algorithms are summarized under a unified taxonomy, with specific focus on visible-depth (RGB-D) and visible-thermal (RGB-T) tracking. Subsequently, a detailed description of the related benchmarks and challenges is provided. Extensive experiments were conducted to analyze the effectiveness of trackers on five datasets: PTB, VOT19-RGBD, GTOT, RGBT234, and VOT19-RGBT. Finally, various future directions, including model design and dataset construction, are discussed from different perspectives for further research.
C1 [Zhang, Pengyu; Wang, Dong; Lu, Huchuan] Dalian Univ Technol, Fac Elect Informat & Elect Engn, Dalian 116024, Peoples R China.
C3 Dalian University of Technology
RP Wang, D (corresponding author), Dalian Univ Technol, Fac Elect Informat & Elect Engn, Dalian 116024, Peoples R China.
EM pyzhang@mail.dlut.edu.cn; wdice@dlut.edu.cn; lhchuan@dlut.edu.cn
FU National Natural Science Foundation of China [U23A20384, 62022021];
   Joint Fund of Ministry of Education [8091B032155]; National Defense
   Basic Scientific Research Program [WDZC20215250205]; Central Guidance on
   Local Science and Technology Development Fund of Liaoning Province
   [2022JH6/100100026]
FX The study was supported in part by National Natural Science Foundation
   of China (Nos. U23A20384 and 62022021), in part by Joint Fund of
   Ministry of Education for Equipment Pre-research (No. 8091B032155), in
   part by the National Defense Basic Scientific Research Program (No.
   WDZC20215250205), and in part by Central Guidance on Local Science and
   Technology Development Fund of Liaoning Province (No.
   2022JH6/100100026).
CR An N, 2016, INT C PATT RECOG, P1231, DOI 10.1109/ICPR.2016.7899805
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Bendjebbour A, 2001, IEEE T GEOSCI REMOTE, V39, P1789, DOI 10.1109/36.942557
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bibi A, 2016, PROC CVPR IEEE, P1439, DOI 10.1109/CVPR.2016.160
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Cai ZY, 2017, MULTIMED TOOLS APPL, V76, P4313, DOI 10.1007/s11042-016-3374-6
   Camplani M., 2015, P BRIT MACHINE VISIO, V145
   Camplani M, 2017, IET COMPUT VIS, V11, P265, DOI 10.1049/iet-cvi.2016.0178
   Chen S, 2008, IEEE INT SYMP CIRC S, P1926, DOI 10.1109/ISCAS.2008.4541820
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chen Y, 2015, SIGNAL PROCESS, V112, P146, DOI 10.1016/j.sigpro.2014.08.046
   Chenglong Li, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P222, DOI 10.1007/978-3-030-58542-6_14
   Conaire C. O., 2006, P 9 INT C INF FUS, P1
   Conaire CO, 2008, MACH VISION APPL, V19, P483, DOI 10.1007/s00138-007-0078-y
   Cvejic N., 2007, P IEEE C COMPUTER VI, P1
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Davis JW, 2007, COMPUT VIS IMAGE UND, V106, P162, DOI 10.1016/j.cviu.2006.06.010
   Ding P, 2015, 2015 12TH INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS AND KNOWLEDGE DISCOVERY (FSKD), P930, DOI 10.1109/FSKD.2015.7382068
   Feng MZ, 2022, KNOWL-BASED SYST, V249, DOI 10.1016/j.knosys.2022.108945
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.129, 10.1109/ICCV.2017.128]
   Gao Shang, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13808), P478, DOI 10.1007/978-3-031-25085-9_27
   Gao Y, 2019, IEEE INT CONF COMP V, P91, DOI 10.1109/ICCVW.2019.00017
   Garcia German Martin, 2012, Pattern Recognition. Proceedings Joint 34th DAGM and 36th OAGM Symposium, P357, DOI 10.1007/978-3-642-32717-9_36
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Guanqun Li, 2019, IOP Conference Series: Earth and Environmental Science, V234, DOI 10.1088/1755-1315/234/1/012005
   Guo Q, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10819, DOI 10.1109/ICCV48922.2021.01066
   Guo Q, 2021, IEEE T IMAGE PROCESS, V30, P1812, DOI 10.1109/TIP.2020.3045630
   Gutev A., 2019, P IEEE EUROCON 18 IN, P1
   Han Zhang, 2018, 2018 Chinese Control And Decision Conference (CCDC), P4856, DOI 10.1109/CCDC.2018.8407972
   Hannuna S, 2019, J REAL-TIME IMAGE PR, V16, P1439, DOI 10.1007/s11554-016-0654-3
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang C, 2017, IEEE I CONF COMP VIS, P105, DOI 10.1109/ICCV.2017.21
   Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650
   Kart U, 2019, PROC CVPR IEEE, P1339, DOI 10.1109/CVPR.2019.00143
   Kart U, 2019, LECT NOTES COMPUT SC, V11129, P148, DOI 10.1007/978-3-030-11009-3_8
   Kart U, 2018, INT C PATT RECOG, P2112, DOI 10.1109/ICPR.2018.8546179
   Kim DY, 2014, INFORM SCIENCES, V278, P641, DOI 10.1016/j.ins.2014.03.080
   Kristan M., 2020, P COMP VIS ECCV 20 5, P547, DOI 10.1007/978-3-030-68238-5_39
   Kristan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P564, DOI 10.1109/ICCVW.2015.79
   Kuai YL, 2019, IEEE SENS J, V19, P9522, DOI 10.1109/JSEN.2019.2925821
   Kulikov GY, 2016, IEEE T SIGNAL PROCES, V64, P948, DOI 10.1109/TSP.2015.2493985
   Lan XY, 2019, IEEE T IND ELECTRON, V66, P9887, DOI 10.1109/TIE.2019.2898618
   Lan XY, 2019, IEEE ACCESS, V7, P67761, DOI 10.1109/ACCESS.2019.2916895
   Lan XY, 2020, PATTERN RECOGN LETT, V130, P12, DOI 10.1016/j.patrec.2018.10.002
   Lan XY, 2018, AAAI CONF ARTIF INTE, P7008
   Leng JX, 2018, IEEE ACCESS, V6, P24256, DOI 10.1109/ACCESS.2018.2831443
   Li C, 2016, PROCEEDINGS OF 2016 IEEE 9TH UK-EUROPE-CHINA WORKSHOP ON MILLIMETRE WAVES AND TERAHERTZ TECHNOLOGIES (UCMMT), P54
   Li CL, 2022, IEEE T IMAGE PROCESS, V31, P392, DOI 10.1109/TIP.2021.3130533
   Li CL, 2019, IEEE INT CONF COMP V, P2262, DOI 10.1109/ICCVW.2019.00279
   Li CL, 2019, IEEE T CIRC SYST VID, V29, P2913, DOI 10.1109/TCSVT.2018.2874312
   Li CL, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.106977
   Li CL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1856, DOI 10.1145/3123266.3123289
   Li CL, 2018, LECT NOTES COMPUT SC, V11217, P831, DOI 10.1007/978-3-030-01261-8_49
   Li CL, 2018, SIGNAL PROCESS-IMAGE, V68, P207, DOI 10.1016/j.image.2018.08.004
   Li CL, 2018, NEUROCOMPUTING, V281, P78, DOI 10.1016/j.neucom.2017.11.068
   Li CL, 2017, IEEE T SYST MAN CY-S, V47, P673, DOI 10.1109/TSMC.2016.2627052
   Li CL, 2016, IEEE T IMAGE PROCESS, V25, P5743, DOI 10.1109/TIP.2016.2614135
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Liu H., 2019, P INT C LEARN REPR, P1
   Liu HP, 2012, SCI CHINA INFORM SCI, V55, P590, DOI 10.1007/s11432-011-4536-9
   Liu J, 2013, IEEE IMAGE PROC, P3088, DOI 10.1109/ICIP.2013.6738636
   Liu WC, 2020, IEEE SENS J, V20, P4496, DOI 10.1109/JSEN.2020.2964019
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P664, DOI 10.1109/TMM.2018.2863604
   Lukezič A, 2018, Arxiv, DOI arXiv:1804.07056
   Lukezic A, 2020, PROC CVPR IEEE, P7131, DOI 10.1109/CVPR42600.2020.00716
   Lukezic A, 2019, IEEE I CONF COMP VIS, P10012, DOI 10.1109/ICCV.2019.01011
   Lukezic A, 2018, INT J COMPUT VISION, V126, P671, DOI 10.1007/s11263-017-1061-3
   Luo CW, 2019, INFRARED PHYS TECHN, V99, P265, DOI 10.1016/j.infrared.2019.04.017
   Ma JY, 2019, INFORM FUSION, V45, P153, DOI 10.1016/j.inffus.2018.02.004
   Ma ZA, 2017, FRONT INFORM TECH EL, V18, P989, DOI 10.1631/FITEE.1601338
   Megherbi N, 2005, AVSS 2005: ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P135
   Meshgi K, 2016, COMPUT VIS IMAGE UND, V150, P81, DOI 10.1016/j.cviu.2016.05.011
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Okuma K, 2004, LECT NOTES COMPUT SC, V3021, P28, DOI 10.1007/978-3-540-24670-1_3
   Ramachandram D, 2017, IEEE SIGNAL PROC MAG, V34, P96, DOI 10.1109/MSP.2017.2738401
   Shi HZ, 2015, CHIN AUTOM CONGR, P518, DOI 10.1109/CAC.2015.7382555
   Weng SK, 2006, J VIS COMMUN IMAGE R, V17, P1190, DOI 10.1016/j.jvcir.2006.03.004
   Shojaeilangari S, 2015, IEEE T IMAGE PROCESS, V24, P2140, DOI 10.1109/TIP.2015.2416634
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song SR, 2013, IEEE I CONF COMP VIS, P233, DOI 10.1109/ICCV.2013.36
   Song X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2414425.2414443
   Torabi A, 2012, COMPUT VIS IMAGE UND, V116, P210, DOI 10.1016/j.cviu.2011.10.006
   Walia GS, 2016, ARTIF INTELL REV, V46, P1, DOI 10.1007/s10462-015-9454-6
   Wang CQ, 2020, PROC CVPR IEEE, P7062, DOI 10.1109/CVPR42600.2020.00709
   Wang Q, 2014, NEUROCOMPUTING, V131, P227, DOI 10.1016/j.neucom.2013.10.021
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wang YL, 2018, LECT NOTES ARTIF INT, V10989, P462, DOI 10.1007/978-3-030-00563-4_45
   Wu Y., 2011, P 14 INT C INFORM FU, P1
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xiao JJ, 2018, IEEE T CYBERNETICS, V48, P2485, DOI 10.1109/TCYB.2017.2740952
   Xiao Y, 2022, AAAI CONF ARTIF INTE, P2831
   Xie Y, 2014, IEEE T CYBERNETICS, V44, P539, DOI 10.1109/TCYB.2013.2259230
   Xie YJ, 2019, 2019 15TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY (CIS 2019), P11, DOI 10.1109/CIS.2019.00011
   Xu H, 2006, ACM T MULTIM COMPUT, V2, P44, DOI 10.1145/1126004.1126007
   Yan B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10428, DOI 10.1109/ICCV48922.2021.01028
   Yan S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10705, DOI 10.1109/ICCV48922.2021.01055
   Yang CJ, 2005, IEEE I CONF COMP VIS, P212
   Yang M, 2014, INT J COMPUT VISION, V109, P209, DOI 10.1007/s11263-014-0722-8
   Yang R., 2019, 2019 IEEE INT C IMAG, P3975, DOI DOI 10.1109/ICIP.2019.8803528
   Zhai SL, 2019, NEUROCOMPUTING, V334, P172, DOI 10.1016/j.neucom.2019.01.022
   Zhai YY, 2018, IEEE ACCESS, V6, P50752, DOI 10.1109/ACCESS.2018.2869766
   Zhang H, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20020393
   Zhang LC, 2019, IEEE INT CONF COMP V, P2252, DOI 10.1109/ICCVW.2019.00278
   Zhang LC, 2019, IEEE T IMAGE PROCESS, V28, P1837, DOI 10.1109/TIP.2018.2879249
   Zhang PY, 2022, PROC CVPR IEEE, P8876, DOI 10.1109/CVPR52688.2022.00868
   Zhang PY, 2021, INT J COMPUT VISION, V129, P2714, DOI 10.1007/s11263-021-01495-3
   Zhang PY, 2021, IEEE T IMAGE PROCESS, V30, P3335, DOI 10.1109/TIP.2021.3060862
   Zhang TZ, 2012, LECT NOTES COMPUT SC, V7577, P470, DOI 10.1007/978-3-642-33783-3_34
   Zhang TZ, 2012, PROC CVPR IEEE, P2042, DOI 10.1109/CVPR.2012.6247908
   Zhang XC, 2020, INFORM FUSION, V63, P166, DOI 10.1016/j.inffus.2020.05.002
   Zhang XM, 2018, 11 INT C IMAGE SIGNA
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhong BN, 2015, NEUROCOMPUTING, V151, P710, DOI 10.1016/j.neucom.2014.06.083
   Zhu X.-F., 2023, P AAAI C ART INT, V37, P3870
   Zhu YB, 2019, Arxiv, DOI arXiv:1811.09855
   Zhu YB, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P465, DOI 10.1145/3343031.3350928
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
NR 125
TC 1
Z9 1
U1 44
U2 45
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD APR
PY 2024
VL 10
IS 2
BP 193
EP 214
DI 10.1007/s41095-023-0345-5
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DW8M2
UT WOS:001135210800009
OA gold, Green Submitted
DA 2024-08-05
ER

PT J
AU Xing, Y
   Wang, XX
   Lu, L
   Sharf, A
   Cohen-Or, D
   Tu, CH
AF Xing, Yu
   Wang, Xiaoxuan
   Lu, Lin
   Sharf, Andrei
   Cohen-Or, Daniel
   Tu, Changhe
TI Shell stand: Stable thin shell models for 3D fabrication
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article; Early Access
DE surface models; thin shell models; optimization; balance; stability; 3D
   fabrication
ID SIMILARITY; RETRIEVAL
AB A thin shell model refers to a surface or structure, where the object's thickness is considered negligible. In the context of 3D printing, thin shell models are characterized by having lightweight, hollow structures, and reduced material usage. Their versatility and visual appeal make them popular in various fields, such as cloth simulation, character skinning, and for thin-walled structures like leaves, paper, or metal sheets. Nevertheless, optimization of thin shell models without external support remains a challenge due to their minimal interior operational space. For the same reasons, hollowing methods are also unsuitable for this task. In fact, thin shell modulation methods are required to preserve the visual appearance of a two-sided surface which further constrain the problem space. In this paper, we introduce a new visual disparity metric tailored for shell models, integrating local details and global shape attributes in terms of visual perception. Our method modulates thin shell models using global deformations and local thickening while accounting for visual saliency, stability, and structural integrity. Thereby, thin shell models such as bas-reliefs, hollow shapes, and cloth can be stabilized to stand in arbitrary orientations, making them ideal for 3D printing.
C1 [Xing, Yu; Wang, Xiaoxuan; Lu, Lin; Tu, Changhe] Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Peoples R China.
   [Sharf, Andrei] Ben Gurion Univ Negev, Dept Comp Sci, IL-84105 Beer Sheva, Israel.
   [Cohen-Or, Daniel] Tel Aviv Univ, Dept Comp Sci, IL-6997801 Tel Aviv, Israel.
C3 Shandong University; Ben Gurion University; Tel Aviv University
RP Lu, L (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Peoples R China.
EM llu@sdu.edu.cn
FU National Natural Science Foundation of China (NSFC) [61972232]; Key
   Research and Development Plan of Shandong Province of China [2020ZLYS01]
FX We thank all the anonymous reviewers for their valuable comments and
   constructive suggestions. This work was supported by Grant No. 61972232
   from the National Natural Science Foundation of China (NSFC) and by
   Grant No. 2020ZLYS01 of the Key Research and Development Plan of
   Shandong Province of China.
CR Al-Ketan O, 2021, ADDIT MANUF, V48, DOI 10.1016/j.addma.2021.102418
   Andersen E.D., 2000, High Performance Optimization, P197, DOI [DOI 10.1007/978-1-4757-3216-0, DOI 10.1007/978-1-4757-3216-08, DOI 10.1007/978-1-4757-3216-0_8]
   Bächer M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601157
   Bustos B, 2005, ACM COMPUT SURV, V37, P345, DOI 10.1145/1118890.1118893
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Christiansen AN, 2015, COMPUT AIDED DESIGN, V58, P236, DOI 10.1016/j.cad.2014.07.009
   Eigensatz M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778782
   Fu HB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360641
   Gao Z, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3377876
   Gibson LJ., 1997, Cellular Solids: Structure and Properties, DOI DOI 10.1017/CBO9781139878326
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Kwok TH, 2015, J MECH DESIGN, V137, DOI 10.1115/1.4031023
   Landis H., 2004, P SIGGRAPH COURS NOT
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Li WH, 2023, INFORM PROCESS MANAG, V60, DOI 10.1016/j.ipm.2023.103372
   Liu PQ, 2022, ADDIT MANUF, V60, DOI 10.1016/j.addma.2022.103258
   Liu SJ, 2011, IEEE T AUTOM SCI ENG, V8, P347, DOI 10.1109/TASE.2010.2066563
   Lu K, 2014, INFORM SCIENCES, V281, P703, DOI 10.1016/j.ins.2014.03.079
   Lu L, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601168
   Lü K, 2009, PROG NAT SCI-MATER, V19, P495, DOI 10.1016/j.pnsc.2008.06.025
   Martínez J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925922
   Musialski P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925886
   Musialski P, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766955
   Nocedal J., 1999, Numerical optimization
   Préost R, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461957
   Prevost R., 2016, P C VIS MOD VIS, P9
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Secord A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2019627.2019628
   Snoek J., 2012, ADV NEURAL INFORM PR, DOI DOI 10.48550/ARXIV.1206.2944
   Sorkine O, 2007, Proc. Symposium on Geometry Processing, V4, P109, DOI [DOI 10.1145/1281991.1282006, 10.1145/1073204.1073323]
   Stava O, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185544
   Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0
   Ulu E, 2019, COMPUT GRAPH FORUM, V38, P85, DOI 10.1111/cgf.13791
   Wang CCL, 2013, RAPID PROTOTYPING J, V19, P395, DOI 10.1108/RPJ-02-2012-0013
   Wang JH, 2017, ENG COMPUTATION, V34, P174, DOI 10.1108/EC-10-2015-0315
   Wang L, 2016, COMPUT GRAPH FORUM, V35, P49, DOI 10.1111/cgf.12810
   Wang WM, 2018, IEEE T VIS COMPUT GR, V24, P2787, DOI 10.1109/TVCG.2017.2764462
   Wohlkinger W., 2011, 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2011), P4865, DOI 10.1109/IROS.2011.6048416
   Wu CC, 2008, PROC CVPR IEEE, P1229
   Wu J, 2016, COMPUT GRAPH-UK, V58, P66, DOI 10.1016/j.cag.2016.05.003
   Xie JW, 2022, IEEE T PATTERN ANAL, V44, P2468, DOI 10.1109/TPAMI.2020.3045010
   Xie Y, 2017, VIS INFORM, V1, P9, DOI 10.1016/j.visinf.2017.01.002
   Xing Y, 2021, COMPUT GRAPH-UK, V97, P160, DOI 10.1016/j.cag.2021.04.031
   Yamanaka D., 2014, P SIGGRAPH AS TECHN
   Zhang LF, 2022, COMPUT AIDED DESIGN, V144, DOI 10.1016/j.cad.2021.103156
   Zhao HM, 2016, COMPUT AIDED GEOM D, V43, P226, DOI 10.1016/j.cagd.2016.02.001
   Zhong FC, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3575859
NR 47
TC 0
Z9 0
U1 3
U2 3
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD 2024 JUN 24
PY 2024
DI 10.1007/s41095-024-0402-8
EA JUN 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WD7C0
UT WOS:001252986400001
OA gold
DA 2024-08-05
ER

PT J
AU Liu, YH
   Zhao, HY
   Chan, KCK
   Wang, XT
   Loy, CC
   Qiao, Y
   Dong, C
AF Liu, Yihao
   Zhao, Hengyuan
   Chan, Kelvin C. K.
   Wang, Xintao
   Loy, Chen Change
   Qiao, Yu
   Dong, Chao
TI Temporally consistent video colorization with deep feature propagation
   and self-regularization learning
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE video colorization; temporal consistency; feature propagation;
   self-regularization
AB Video colorization is a challenging and highly ill-posed problem. Although recent years have witnessed remarkable progress in single image colorization, there is relatively less research effort on video colorization, and existing methods always suffer from severe flickering artifacts (temporal inconsistency) or unsatisfactory colorization. We address this problem from a new perspective, by jointly considering colorization and temporal consistency in a unified framework. Specifically, we propose a novel temporally consistent video colorization (TCVC) framework. TCVC effectively propagates frame-level deep features in a bidirectional way to enhance the temporal consistency of colorization. Furthermore, TCVC introduces a self-regularization learning (SRL) scheme to minimize the differences in predictions obtained using different time steps. SRL does not require any ground-truth color videos for training and can further improve temporal consistency. Experiments demonstrate that our method can not only provide visually pleasing colorized video, but also with clearly better temporal consistency than state-of-the-art methods. A video demo is provided at https://www.youtube.com/watch?v=c7dczMs-olE, while code is available at https://github.com/lyh-18/TCVC-Temporally-Consistent-Video-Colorization.
C1 [Liu, Yihao; Zhao, Hengyuan; Qiao, Yu; Dong, Chao] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
   [Liu, Yihao] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Chan, Kelvin C. K.; Loy, Chen Change] Nanyang Technol Univ, Dept Elect & Comp Engn, 50 Nanyang Ave, Singapore 639798, Singapore.
   [Wang, Xintao] Tencent PCG, Appl Res Ctr, Shenzhen, Peoples R China.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS; Nanyang Technological University
RP Dong, C (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
EM liuyihao14@mails.ucas.ac.cn; hy.zhao1@siat.ac.cn; chan0899@e.ntu.edu.sg;
   xintaowang@tencent.com; ccloy@ntu.edu.sg; yu.qiao@siat.ac.cn;
   chao.dong@siat.ac.cn
RI Liu, Yihao/KIC-0534-2024
OI Liu, Yihao/0000-0001-9874-0602
FU National Natural Science Foundation of China [61906184]; Joint Lab of
   CAS; Shanghai Committee of Science and Technology, China [20DZ1100800,
   21DZ1100100]
FX This work was partially supported by grants from the National Natural
   Science Foundation of China (61906184), the Joint Lab of CAS-HK, and the
   Shanghai Committee of Science and Technology, China (20DZ1100800,
   21DZ1100100).
CR Gatys LA, 2015, Arxiv, DOI [arXiv:1508.06576, 10.1167/16.12.326, DOI 10.1167/16.12.326]
   Bao WB, 2019, PROC CVPR IEEE, P3698, DOI 10.1109/CVPR.2019.00382
   Bonneel N, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818107
   Chen XW, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366151
   Cheng ZZ, 2015, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2015.55
   Chu MY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392457
   Deshpande A, 2017, PROC CVPR IEEE, P2877, DOI 10.1109/CVPR.2017.307
   Dong Y, 2020, AAAI CONF ARTIF INTE, V34, P10729
   Eilertsen G, 2019, PROC CVPR IEEE, P11168, DOI 10.1109/CVPR.2019.01143
   Gupta Raj Kumar, 2012, P 20 ACM INT C MULT, P369, DOI DOI 10.1145/2393347.2393402
   Hasler D, 2003, P SOC PHOTO-OPT INS, V5007, P87, DOI 10.1117/12.477378
   He MM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201365
   Iizuka S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356570
   Iizuka S, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925974
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jampani V, 2017, PROC CVPR IEEE, P3154, DOI 10.1109/CVPR.2017.336
   Jheng-Wei Su, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7965, DOI 10.1109/CVPR42600.2020.00799
   Jingwen He, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P679, DOI 10.1007/978-3-030-58601-0_40
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kingma D. P., 2014, arXiv
   Lai WS, 2018, LECT NOTES COMPUT SC, V11219, P179, DOI 10.1007/978-3-030-01267-0_11
   Larsson G, 2017, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2017.96
   Larsson G, 2016, LECT NOTES COMPUT SC, V9908, P577, DOI 10.1007/978-3-319-46493-0_35
   Lee J, 2020, PROC CVPR IEEE, P5800, DOI 10.1109/CVPR42600.2020.00584
   Lei CY, 2020, Arxiv, DOI arXiv:2010.11838
   Lei CY, 2019, PROC CVPR IEEE, P3748, DOI 10.1109/CVPR.2019.00387
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Liu XP, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409105
   Lu G, 2019, PROC CVPR IEEE, P10998, DOI 10.1109/CVPR.2019.01126
   Luan Q., 2007, P 18 EUR C REND TECH, P309, DOI DOI 10.2312/EGWR/EGSR07/309-320
   Paul S, 2017, IEEE T CIRC SYST VID, V27, P1605, DOI 10.1109/TCSVT.2016.2539539
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Qu YG, 2006, ACM T GRAPHIC, V25, P1214, DOI 10.1145/1141911.1142017
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ruder M, 2016, LECT NOTES COMPUT SC, V9796, P26, DOI 10.1007/978-3-319-45886-1_3
   Sheng B, 2014, IEEE T CIRC SYST VID, V24, P407, DOI 10.1109/TCSVT.2013.2276702
   Shi M, 2023, IEEE T VIS COMPUT GR, V29, P2965, DOI 10.1109/TVCG.2022.3146000
   Thasarathan H, 2019, 2019 16TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV 2019), P189, DOI 10.1109/CRV.2019.00033
   Vondrick C, 2018, LECT NOTES COMPUT SC, V11217, P402, DOI 10.1007/978-3-030-01261-8_24
   Welsh T, 2002, ACM T GRAPHIC, V21, P277, DOI 10.1145/566570.566576
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yao CH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P777, DOI 10.1145/3123266.3123363
   Yoo S, 2019, PROC CVPR IEEE, P11275, DOI 10.1109/CVPR.2019.01154
   Zhang B, 2019, PROC CVPR IEEE, P8044, DOI 10.1109/CVPR.2019.00824
   Zhang R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073703
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhongyou Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9360, DOI 10.1109/CVPR42600.2020.00938
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 51
TC 2
Z9 3
U1 3
U2 4
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD APR
PY 2024
VL 10
IS 2
BP 375
EP 395
DI 10.1007/s41095-023-0342-8
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DW8M2
UT WOS:001135210800010
OA Green Submitted, gold
DA 2024-08-05
ER

PT J
AU Dong, Y
   Liang, CJ
   Chen, Y
   Hua, J
AF Dong, Yu
   Liang, Christy Jie
   Chen, Yi
   Hua, Jie
TI A visual modeling method for spatiotemporal and multidimensional
   features in epidemiological analysis: Applied COVID-19 aggregated
   datasets
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE visual modeling; epidemiological analysis; spatiotemporal;
   multidimensional; COVID-19
ID RISK-FACTORS; VISUALIZATION; ANALYTICS; SYSTEM
AB The visual modeling method enables flexible interactions with rich graphical depictions of data and supports the exploration of the complexities of epidemiological analysis. However, most epidemiology visualizations do not support the combined analysis of objective factors that might influence the transmission situation, resulting in a lack of quantitative and qualitative evidence. To address this issue, we developed a portrait-based visual modeling method called +msRNAer. This method considers the spatiotemporal features of virus transmission patterns and multidimensional features of objective risk factors in communities, enabling portrait-based exploration and comparison in epidemiological analysis. We applied +msRNAer to aggregate COVID-19-related datasets in New South Wales, Australia, combining COVID-19 case number trends, geo-information, intervention events, and expert-supervised risk factors extracted from local government area-based censuses. We perfected the +msRNAer workflow with collaborative views and evaluated its feasibility, effectiveness, and usefulness through one user study and three subject-driven case studies. Positive feedback from experts indicates that +msRNAer provides a general understanding for analyzing comprehension that not only compares relationships between cases in time-varying and risk factors through portraits but also supports navigation in fundamental geographical, timeline, and other factor comparisons. By adopting interactions, experts discovered functional and practical implications for potential patterns of long-standing community factors regarding the vulnerability faced by the pandemic. Experts confirmed that +msRNAer is expected to deliver visual modeling benefits with spatiotemporal and multidimensional features in other epidemiological analysis scenarios.
C1 [Dong, Yu; Liang, Christy Jie; Hua, Jie] Univ Technol Sydney, Sch Comp Sci, Sydney, NSW 2007, Australia.
   [Chen, Yi] Beijing Technol & Business Univ, Beijing Key Lab Big Data Technol Food Safety, Beijing 100048, Peoples R China.
C3 University of Technology Sydney; Beijing Technology & Business
   University
RP Liang, CJ (corresponding author), Univ Technol Sydney, Sch Comp Sci, Sydney, NSW 2007, Australia.; Chen, Y (corresponding author), Beijing Technol & Business Univ, Beijing Key Lab Big Data Technol Food Safety, Beijing 100048, Peoples R China.
EM jie.liang@uts.edu.au; chenyi@th.btbu.edu.cn
RI Dong, Yu/HJQ-0008-2023
OI Dong, Yu/0000-0002-2353-6567
FU National Natural Science Foundation of China (NSFC) [61972010]; UTS-CSC
   Scholarship by the University of Technology Sydney; China Scholarship
   Council [201908200009]
FX This work is supported by National Natural Science Foundation of China
   (NSFC) under Grant No. 61972010 and UTS-CSC Scholarship by the
   University of Technology Sydney and China Scholarship Council under
   Agreement No. 201908200009.
CR Afzal S., 2011, 2011 IEEE Conference on Visual Analytics Science and Technology, P191, DOI 10.1109/VAST.2011.6102457
   Afzal S, 2020, 2020 IEEE VISUALIZATION CONFERENCE - SHORT PAPERS (VIS 2020), P86, DOI 10.1109/VIS47514.2020.00024
   aihw, Health expenditure Australia 2019-20, About-Australian Institute of Health and Welfare
   Andrienko G, 2010, INT J GEOGR INF SCI, V24, P1577, DOI 10.1080/13658816.2010.508043
   Angelini Marco, 2021, Advanced Visual Interfaces Supporting Artificial Intelligence and Big Data Applications AVI 2020 Workshops, AVI-BDA and ITAVIS. Revised Selected Papers. Lecture Notes in Computer Science (LNCS 12585), P163, DOI 10.1007/978-3-030-68007-7_10
   [Anonymous], 2021, Omicron variant in confirmed nsw cases
   [Anonymous], 2021, Fighting the delta outbreak with new restrictions for local government areas
   [Anonymous], NSW COVID-19 cases data
   [Anonymous], Latest media releases from nsw health
   Antweiler D., 2021, P EUROVIS WORKSH VIS, P43
   Australian Government Department of Health and Aged Care, Weekly COVID-19 reporting
   Australian National University, New data visualisation tool to help track COVID-19
   Bachtiger P, 2020, LANCET DIGIT HEALTH, V2, pE391, DOI 10.1016/S2589-7500(20)30162-X
   Badr HS, 2020, LANCET INFECT DIS, V20, P1247, DOI 10.1016/S1473-3099(20)30553-3
   Bing, Conronavirus Australia-live map tracker from Microsoft Bing
   Bostock M, 2011, IEEE T VIS COMPUT GR, V17, P2301, DOI 10.1109/TVCG.2011.185
   Bowe E, 2020, BIG DATA SOC, V7, DOI 10.1177/2053951720939236
   Cao LB, 2022, IEEE INTELL SYST, V37, P3, DOI 10.1109/MIS.2022.3164313
   Carroll LN, 2014, J BIOMED INFORM, V51, P287, DOI 10.1016/j.jbi.2014.04.006
   Chen BQ, 2020, Arxiv, DOI arXiv:2002.07096
   Christakis Nicholas A, 2009, Nor Epidemiol, V19, P5
   Chua J, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0170804
   Chui KKH, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0014683
   Covid-19-au, COVID-19 in Australia real-time report
   DELCOURT C., 2017, Investigative Ophthalmology Visual Science, V58, P2209
   Deodhar S, 2014, ACM TRANS MANAG INF, V5, DOI 10.1145/2629692
   Desai A, 2021, LANCET DIGIT HEALTH, V3, pE619, DOI 10.1016/S2589-7500(21)00178-3
   Dey SK, 2020, J MED VIROL, V92, P632, DOI 10.1002/jmv.25743
   Dong ES, 2020, LANCET INFECT DIS, V20, P533, DOI 10.1016/S1473-3099(20)30120-1
   Dong Y, 2023, J VISUAL-JAPAN, V26, P403, DOI 10.1007/s12650-022-00882-3
   Dong Y, 2020, IEEE PAC VIS SYMP, P131, DOI 10.1109/PacificVis48177.2020.1007
   Dykes J, 2022, PHILOS T R SOC A, V380, DOI 10.1098/rsta.2021.0299
   Giordano G, 2020, NAT MED, V26, P855, DOI 10.1038/s41591-020-0883-7
   Goodwin S, 2016, IEEE T VIS COMPUT GR, V22, P599, DOI 10.1109/TVCG.2015.2467199
   Government of South Australia, New data visualisation tool to help track COVID-19
   Hassan A H M., 2021, International Journal of Information Technology and Computer Science, V13, P16, DOI [10.5815/ijitcs.2021.03.02, DOI 10.5815/IJITCS.2021.03.02]
   Hassan KA, 2019, IEEE INT CON INF VIS, P145, DOI 10.1109/IV.2019.00033
   Healey CG, 2022, BIG DATA, V10, P95, DOI 10.1089/big.2021.0023
   Hemied OS, 2022, IEEJ T ELECTR ELECTR, V17, P1038, DOI 10.1002/tee.23593
   INSELBERG A, 1990, PROCEEDINGS OF THE FIRST IEEE CONFERENCE ON VISUALIZATION - VISUALIZATION 90, P361, DOI 10.1109/VISUAL.1990.146402
   Isenberg P, 2017, IEEE T VIS COMPUT GR, V23, P771, DOI 10.1109/TVCG.2016.2598827
   Kahn P., COVID-19 online visualization collection (COVIC)
   Lan Y, 2021, CARTOGRAPHICA, V56, P2, DOI 10.3138/cart-2020-0027
   Le Bras P, 2020, Arxiv, DOI arXiv:2005.06380
   Leite RA, 2020, 2020 IEEE VISUALIZATION CONFERENCE - SHORT PAPERS (VIS 2020), P56, DOI 10.1109/VIS47514.2020.00018
   Leung CK, 2020, IEEE INT CONF INF VI, P415, DOI 10.1109/IV51561.2020.00073
   Li R, 2021, HUM BEHAV EMERG TECH, V3, P97, DOI 10.1002/hbe2.248
   Liu Q, 2020, J MED INTERNET RES, V22, DOI 10.2196/19118
   Lord SR, 2001, J AM GERIATR SOC, V49, P508, DOI 10.1046/j.1532-5415.2001.49107.x
   Maciejewski R, 2011, J VISUAL LANG COMPUT, V22, P268, DOI 10.1016/j.jvlc.2011.04.002
   Mapbox, Maps and location for developers
   Mathieu E., 2020, Our World in Data
   MATUTE J., 2017, EuroVis, P81
   Mikolov T, 2013, Arxiv, DOI [arXiv:1301.3781, DOI 10.48550/ARXIV.1301.3781]
   Mocnik FB, 2020, J MAPS, V16, P144, DOI 10.1080/17445647.2020.1776646
   Muhammad L J, 2021, SN Comput Sci, V2, P11, DOI 10.1007/s42979-020-00394-7
   Muto K, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0234292
   Northern Territory Government of Australia, COVID-19 data
   NSW Government, NSW administrative boundaries
   NSW Government, Who are very low to moderate income earners?
   Panovska-Griffiths J, 2022, PHILOS T R SOC A, V380, DOI 10.1098/rsta.2021.0315
   Park M, 2020, J CLIN MED, V9, DOI 10.3390/jcm9040967
   Pooley CM, 2022, PHILOS T R SOC A, V380, DOI 10.1098/rsta.2021.0298
   Preim B, 2020, COMPUT GRAPH FORUM, V39, P543, DOI 10.1111/cgf.13891
   Queensland Government, Queensland COVID-19 statistics
   Regulski P, 2021, PROCEDIA COMPUT SCI, V192, P4194, DOI 10.1016/j.procs.2021.09.195
   Reinert A, 2020, COMPUT SCI ENG, V22, P48, DOI 10.1109/MCSE.2020.3023288
   Renton S., 2020, Australians post COVID-19
   Rydow Erik, 2023, IEEE Trans Vis Comput Graph, V29, P1255, DOI 10.1109/TVCG.2022.3209464
   Sanz-Leon P, 2022, PHILOS T R SOC A, V380, DOI 10.1098/rsta.2021.0311
   Shapiro BR, 2017, PROCEEDINGS OF THE 2017 IEEE VIS ARTS PROGRAM (VISAP)
   State Government of Victoria, Victorian COVID-19 data
   Steinger T, 2014, ANN APPL BIOL, V164, P200, DOI 10.1111/aab.12096
   Sweet A., 2021, Fear down, job-seeking up as Australians feel the financial impact of COVID-19
   The University of Sydney, NSW COVID-19 cases and community profile by The University of Sydney
   Thöny M, 2018, ISPRS INT J GEO-INF, V7, DOI 10.3390/ijgi7030123
   Trajkova M, 2020, INFORMATICS-BASEL, V7, DOI 10.3390/informatics7030035
   Ulahannan JP, 2020, J AM MED INFORM ASSN, V27, P1913, DOI 10.1093/jamia/ocaa203
   University of Melbourne, Coronavirus 10-day forecast
   Wei LLY, 2020, J IND INF INTEGR, V18, DOI 10.1016/j.jii.2020.100139
   who, WHO CORONAVIRUS COVI
   Wu YH, 2021, IEEE T IMAGE PROCESS, V30, P3113, DOI 10.1109/TIP.2021.3058783
   Xu HW, 2021, J BIOMED INFORM, V124, DOI 10.1016/j.jbi.2021.103941
   Yu XM, 2021, IEEE ACCESS, V9, P126684, DOI 10.1109/ACCESS.2021.3111833
   Yun Zhou, 2020, 2020 International Conferences on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics (Cybermatics), P62, DOI 10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics50389.2020.00029
   Zeng W., 2021, Advances in Artificial Intelligence and Security. Communications in Computer and Information Science, P541
   Zhang Y, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445054
   Zhang Yixuan, 2023, IEEE Trans Vis Comput Graph, V29, P1037, DOI 10.1109/TVCG.2022.3209493
NR 88
TC 1
Z9 1
U1 2
U2 4
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD FEB
PY 2024
VL 10
IS 1
BP 161
EP 186
DI 10.1007/s41095-023-0353-5
PG 26
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Z5WZ7
UT WOS:001112788200001
OA gold
DA 2024-08-05
ER

PT J
AU Liu, CY
   Chen, F
   Deng, L
   Yi, RJ
   Zheng, LT
   Zhu, CY
   Wang, J
   Xu, K
AF Liu, Chenyi
   Chen, Fei
   Deng, Lu
   Yi, Renjiao
   Zheng, Lintao
   Zhu, Chenyang
   Wang, Jia
   Xu, Kai
TI 6DOF pose estimation of a 3D rigid object based on edge-enhanced point
   pair features
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE point pair feature (PPF); pose estimation; object recognition; 3D point
   cloud
ID UNIQUE SIGNATURES; RECOGNITION; EFFICIENT; HISTOGRAMS; SURFACE
AB The point pair feature (PPF) is widely used for 6D pose estimation. In this paper, we propose an efficient 6D pose estimation method based on the PPF framework. We introduce a well-targeted down-sampling strategy that focuses on edge areas for efficient feature extraction for complex geometry. A pose hypothesis validation approach is proposed to resolve ambiguity due to symmetry by calculating the edge matching degree. We perform evaluations on two challenging datasets and one real-world collected dataset, demonstrating the superiority of our method for pose estimation for geometrically complex, occluded, symmetrical objects. We further validate our method by applying it to simulated punctures.
C1 [Liu, Chenyi; Yi, Renjiao; Zhu, Chenyang; Xu, Kai] Natl Univ Def Technol, Coll Comp, Changsha 410073, Peoples R China.
   [Chen, Fei] Cent South Univ, Xiangya Hosp 2, Dept Spine Surg, Changsha 410011, Peoples R China.
   [Deng, Lu] Second Xiangya Hosp, Clin Nursing Teaching & Res Sect, Changsha 410011, Peoples R China.
   [Zheng, Lintao] Natl Univ Def Technol, Coll Meteorol & Oceanog, Changsha 410073, Peoples R China.
   [Wang, Jia] Beijing Inst Tracking & Commun Technol, Beijing 100094, Peoples R China.
C3 National University of Defense Technology - China; Central South
   University; National University of Defense Technology - China
RP Chen, F (corresponding author), Cent South Univ, Xiangya Hosp 2, Dept Spine Surg, Changsha 410011, Peoples R China.
EM liuchenyi1013@nudt.edu.cn; chenfei1972@csu.edu.cn;
   csdenglu1026@csu.edu.cn; yirenjiao@nudt.edu.cn;
   zhenglintao13@nudt.edu.cn; zhuchenyang07@nudt.edu.cn;
   kelexuebi2009@163.com; kevin.kai.xu@gmail.com
OI Yi, Renjiao/0000-0002-6057-1089
FU National Key R&D Program of China [2018AAA0102200]; National Natural
   Science Foundation of China [62132021, 62102435, 61902419, 62002375,
   62002376]; Natural Science Foundation of Hunan Province of China
   [2021JJ40696]; Huxiang Youth Talent Support Program [2021RC3071]; NUDT
   Research Grants [ZK19-30, ZK22-52]
FX This work was supported in part by the National Key R & D Program of
   China (2018AAA0102200), National Natural Science Foundation of China
   (62132021, 62102435, 61902419, 62002375, 62002376), Natural Science
   Foundation of Hunan Province of China (2021JJ40696), Huxiang Youth
   Talent Support Program (2021RC3071), and NUDT Research Grants (ZK19-30,
   ZK22-52).
CR BESL PJ, 1992, P SOC PHOTO-OPT INS, V1611, P586, DOI 10.1117/12.57955
   Buch AG, 2016, SPRINGERPLUS, V5, DOI 10.1186/s40064-016-1906-1
   Buch AG, 2017, IEEE I CONF COMP VIS, P4137, DOI 10.1109/ICCV.2017.443
   CHEN Y, 1992, IMAGE VISION COMPUT, V10, P145, DOI 10.1016/0262-8856(92)90066-C
   Choi C, 2016, ROBOT AUTON SYST, V75, P595, DOI 10.1016/j.robot.2015.09.020
   Drost B, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P9, DOI 10.1109/3DIMPVT.2012.53
   Drost B, 2010, PROC CVPR IEEE, P998, DOI 10.1109/CVPR.2010.5540108
   Guo JW, 2021, IEEE T IMAGE PROCESS, V30, P5072, DOI 10.1109/TIP.2021.3078109
   Hinterstoisser S, 2016, LECT NOTES COMPUT SC, V9907, P834, DOI 10.1007/978-3-319-46487-9_51
   Hinterstoisser S, 2011, IEEE I CONF COMP VIS, P858, DOI 10.1109/ICCV.2011.6126326
   Hodan Tomas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11700, DOI 10.1109/CVPR42600.2020.01172
   Hodan T, 2016, LECT NOTES COMPUT SC, V9915, P606, DOI 10.1007/978-3-319-49409-8_52
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Jorgensen Troels Bo, 2015, 10th International Conference on Computer Vision Theory and Applications (VISAPP 2015). Proceedings, P333
   Kim K, 2017, COMPUT MED IMAG GRAP, V58, P45, DOI 10.1016/j.compmedimag.2017.02.002
   Lan YQ, 2022, COMPUT VIS MEDIA, V8, P395, DOI 10.1007/s41095-021-0252-6
   Li RT, 2019, COMPUT VIS MEDIA, V5, P363, DOI 10.1007/s41095-019-0156-x
   Liang HZ, 2019, IEEE INT CONF ROBOT, P3629, DOI [10.1109/ICRA.2019.8794435, 10.1109/icra.2019.8794435]
   Liu DY, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18082719
   Lu RR, 2019, ACTA OPT SIN, V39, DOI 10.3788/AOS201939.0815006
   Madry M, 2012, IEEE INT C INT ROBOT, P1379, DOI 10.1109/IROS.2012.6385874
   Marton ZC, 2011, INT J ROBOT RES, V30, P1378, DOI 10.1177/0278364911415897
   Mian AS, 2006, IEEE T PATTERN ANAL, V28, P1584, DOI 10.1109/TPAMI.2006.213
   Papazov C, 2011, LECT NOTES COMPUT SC, V6492, P135, DOI 10.1007/978-3-642-19315-6_11
   Park K, 2019, IEEE I CONF COMP VIS, P7667, DOI 10.1109/ICCV.2019.00776
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Rusu Radu Bogdan, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P47, DOI 10.1109/ICCVW.2009.5457718
   Rusu RB, 2010, IEEE INT C INT ROBOT, P2155, DOI 10.1109/IROS.2010.5651280
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Salti S, 2014, COMPUT VIS IMAGE UND, V125, P251, DOI 10.1016/j.cviu.2014.04.011
   Solund T, 2016, INT CONF 3D VISION, P73, DOI 10.1109/3DV.2016.16
   Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26
   Vidal J, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18082678
   Vock R, 2019, COMPUT GRAPH-UK, V79, P36, DOI 10.1016/j.cag.2018.12.007
   Wang Y, 2021, WORLD J GASTRO SURG, V13, DOI 10.4240/wjgs.v13.i9.904
   Zeng L, 2022, IEEE T AUTOM SCI ENG, V19, P3139, DOI 10.1109/TASE.2021.3108800
NR 36
TC 1
Z9 1
U1 23
U2 28
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD FEB
PY 2024
VL 10
IS 1
BP 61
EP 77
DI 10.1007/s41095-022-0308-2
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Z5WZ7
UT WOS:001112788200006
OA gold
DA 2024-08-05
ER

PT J
AU Wang, XK
   Xu, YR
   Liu, SN
   Ren, B
   Kosinka, J
   Telea, AC
   Wang, JM
   Song, CM
   Chang, J
   Li, CF
   Zhang, JJ
   Ban, XJ
AF Wang, Xiaokun
   Xu, Yanrui
   Liu, Sinuo
   Ren, Bo
   Kosinka, Jiri
   Telea, Alexandru C.
   Wang, Jiamin
   Song, Chongming
   Chang, Jian
   Li, Chenfeng
   Zhang, Jian Jun
   Ban, Xiaojuan
TI Physics-based fluid simulation in computer graphics: Survey, research
   trends, and challenges
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Review; Early Access
DE computer graphics; physical simulation; fluid simulation; fluid coupling
ID PARTIAL-DIFFERENTIAL-EQUATIONS; NARROW-BAND FLIP; LIQUID SIMULATION;
   MULTIPHASE FLOW; MIXTURE MODEL; POINT METHOD; FREE-SURFACE; SPH SOLVER;
   PARTICLE; SMOKE
AB Physics-based fluid simulation has played an increasingly important role in the computer graphics community. Recent methods in this area have greatly improved the generation of complex visual effects and its computational efficiency. Novel techniques have emerged to deal with complex boundaries, multiphase fluids, gas-liquid interfaces, and fine details. The parallel use of machine learning, image processing, and fluid control technologies has brought many interesting and novel research perspectives. In this survey, we provide an introduction to theoretical concepts underpinning physics-based fluid simulation and their practical implementation, with the aim for it to serve as a guide for both newcomers and seasoned researchers to explore the field of physics-based fluid simulation, with a focus on developments in the last decade. Driven by the distribution of recent publications in the field, we structure our survey to cover physical background; discretization approaches; computational methods that address scalability; fluid interactions with other materials and interfaces; and methods for expressive aspects of surface detail and control. From a practical perspective, we give an overview of existing implementations available for the above methods.
C1 [Wang, Xiaokun; Xu, Yanrui; Liu, Sinuo; Wang, Jiamin; Song, Chongming; Ban, Xiaojuan] Univ Sci & Technol Beijing, Beijing Adv Innovat Ctr Mat Genome Engn, Beijing 100083, Peoples R China.
   [Wang, Xiaokun; Xu, Yanrui; Liu, Sinuo; Wang, Jiamin; Song, Chongming; Ban, Xiaojuan] Univ Sci & Technol Beijing, Sch Intelligence Sci & Technol, Beijing 100083, Peoples R China.
   [Wang, Xiaokun; Xu, Yanrui; Liu, Sinuo; Wang, Jiamin; Song, Chongming; Ban, Xiaojuan] Univ Sci & Technol Beijing, Shunde Innovat Sch, Beijing 100083, Peoples R China.
   [Wang, Xiaokun; Chang, Jian; Zhang, Jian Jun] Bournemouth Univ, Natl Ctr Comp Animat, Fac Media & Commun, Poole BH12 5BB, England.
   [Xu, Yanrui; Kosinka, Jiri] Univ Groningen, Bernoulli Inst, NL-9700 AK Groningen, Netherlands.
   [Liu, Sinuo] Peking Univ, Sch Comp Sci, Beijing 100871, Peoples R China.
   [Ren, Bo] Nankai Univ, Coll Comp Sci, TMCC, Tianjin 300350, Peoples R China.
   [Telea, Alexandru C.] Univ Utrecht, Dept Informat & Comp Sci, NL-3584 CC Utrecht, Netherlands.
   [Li, Chenfeng] Swansea Univ, Zienkiewicz Inst Modelling Data & AI, Swansea SA1 8EN, Wales.
   [Ban, Xiaojuan] Inst Mat Intelligent Technol, Liaoning Acad Mat, Shenyang 110004, Peoples R China.
C3 University of Science & Technology Beijing; University of Science &
   Technology Beijing; University of Science & Technology Beijing;
   Bournemouth University; University of Groningen; Peking University;
   Nankai University; Utrecht University; Swansea University; Liaoning
   Academy Materials
RP Ban, XJ (corresponding author), Univ Sci & Technol Beijing, Beijing Adv Innovat Ctr Mat Genome Engn, Beijing 100083, Peoples R China.; Ban, XJ (corresponding author), Univ Sci & Technol Beijing, Sch Intelligence Sci & Technol, Beijing 100083, Peoples R China.; Ban, XJ (corresponding author), Univ Sci & Technol Beijing, Shunde Innovat Sch, Beijing 100083, Peoples R China.; Chang, J (corresponding author), Bournemouth Univ, Natl Ctr Comp Animat, Fac Media & Commun, Poole BH12 5BB, England.; Ban, XJ (corresponding author), Inst Mat Intelligent Technol, Liaoning Acad Mat, Shenyang 110004, Peoples R China.
EM jchang@bournemouth.ac.uk; banxj@ustb.edu.cn
RI Xu, Yanrui/KHU-2854-2024
OI Xu, Yanrui/0000-0002-2154-1178; Kosinka, Jiri/0000-0002-8859-2586
FU National Key R&D Program of China [2022ZD0118001]; National Natural
   Science Foundation of China [62376025, 62332017, 895941]; Guangdong
   Basic and Applied Basic Research Foundation [2023A1515030177]
FX This research was funded by National Key R&D Program of China (No.
   2022ZD0118001), National Natural Science Foundation of China (Nos.
   62376025 and 62332017), Horizon 2020-Marie Sklodowska-Curie
   Action-Individual Fellowships (No. 895941), and Guangdong Basic and
   Applied Basic Research Foundation (No. 2023A1515030177).
CR Aanjaneya M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073625
   Aanjaneya M, 2018, COMPUT GRAPH FORUM, V37, P59, DOI 10.1111/cgf.13512
   Adams B, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239499
   Airy G., 1845, TIDES AND WAVES
   Akbay M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201345
   Akinci N, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508395
   Akinci N, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185558
   Alduán I, 2017, COMPUT GRAPH FORUM, V36, P32, DOI 10.1111/cgf.12992
   Ando R, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392460
   Ando R, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766935
   Ando R, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461982
   Ando R, 2012, IEEE T VIS COMPUT GR, V18, P1202, DOI 10.1109/TVCG.2012.87
   Angelidis A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073606
   [Anonymous], 2008, Fluid Simulation for Computer Graphics
   Arnold DN, 2002, SIAM J NUMER ANAL, V39, P1749, DOI 10.1137/S0036142901384162
   Auer S, 2013, COMPUT GRAPH FORUM, V32, P207, DOI 10.1111/cgf.12228
   Auer S, 2012, COMPUT GRAPH FORUM, V31, P1909, DOI 10.1111/j.1467-8659.2012.03071.x
   Azencot O, 2018, COMPUT GRAPH FORUM, V37, P107, DOI 10.1111/cgf.13495
   Azencot O, 2014, COMPUT GRAPH FORUM, V33, P237, DOI 10.1111/cgf.12449
   Azevedo VC, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925919
   Azevedo VC, 2013, COMPUT GRAPH FORUM, V32, P235, DOI 10.1111/cgf.12231
   Bai K, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480492
   Bai K, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3412360
   Band S, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3180486
   Barreiro H, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130854
   Batty C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185609
   Becker M, 2009, IEEE T VIS COMPUT GR, V15, P493, DOI 10.1109/TVCG.2008.107
   Becker M, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P209
   Bender J, 2020, IEEE T VIS COMPUT GR, V26, P2982, DOI 10.1109/TVCG.2020.3004245
   Bender J, 2017, ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION (SCA 2017), DOI 10.1145/3099564.3099578
   Bender J, 2017, IEEE T VIS COMPUT GR, V23, P1193, DOI 10.1109/TVCG.2016.2578335
   Biddiscombe J, 2012, IEEE T VIS COMPUT GR, V18, P852, DOI 10.1109/TVCG.2012.63
   Bojsen-Hansen M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925963
   Bojsen-Hansen M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185549
   Bojsen-Hansen M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461991
   Boyd L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2159516.2159522
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   BRACKBILL JU, 1986, J COMPUT PHYS, V65, P314, DOI 10.1016/0021-9991(86)90211-1
   Brandt C, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356496
   Busaryev O, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185559
   Canabal JA, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982415
   Cao YD, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3519595
   Chang Y, 2020, COMPUT GRAPH FORUM, V39, P131, DOI 10.1111/cgf.14132
   Chen JY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459874
   Chen XS, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417809
   Chen YL, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417859
   Chen YX, 2022, IEEE T VIS COMPUT GR, V28, P3235, DOI 10.1109/TVCG.2021.3059753
   Chen ZL, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818066
   Chentanez N, 2015, IEEE T VIS COMPUT GR, V21, P1116, DOI 10.1109/TVCG.2015.2449303
   Chentanez N, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766991
   Chentanez N, 2012, IEEE T VIS COMPUT GR, V18, P1191, DOI 10.1109/TVCG.2012.86
   Cho J, 2013, COMPUT GRAPH FORUM, V32, P379, DOI 10.1111/cgf.12058
   Chu JY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3092818
   Chu MY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073643
   Clausen P, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2451236.2451243
   Cornelis J, 2014, COMPUT GRAPH FORUM, V33, P255, DOI 10.1111/cgf.12324
   Courant R, 1928, MATH ANN, V100, P32, DOI 10.1007/BF01448839
   Cui QD, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480536
   Cui QD, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201352
   Da F, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925899
   Da F, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2767003
   Da F, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601146
   Dagenais F, 2017, COMPUT GRAPH FORUM, V36, P444, DOI 10.1111/cgf.13091
   De Witt T, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2077341.2077351
   Deng YT, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530174
   Desbrun M., 1996, Computer Animation and Simulation '96. Proceedings of the Eurographics Workshop, P61
   Eckert ML, 2018, COMPUT GRAPH FORUM, V37, P47, DOI 10.1111/cgf.13511
   Eckert ML, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356545
   Edwards E, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601167
   ENGLISH R.E., 2013, Proceedings of the 12th ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P85, DOI DOI 10.1145/2485895.2485897
   Fang Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392438
   Fang Y, 2018, COMPUT GRAPH FORUM, V37, P195, DOI 10.1111/cgf.13524
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Fei Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356532
   Fei Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201392
   Fei Y, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073630
   Feng F, 2023, IEEE T VIS COMPUT GR, V29, P3081, DOI 10.1109/TVCG.2022.3149466
   Ferstl F, 2016, COMPUT GRAPH FORUM, V35, P225, DOI 10.1111/cgf.12825
   Ferstl F, 2014, IEEE T VIS COMPUT GR, V20, P1405, DOI 10.1109/TVCG.2014.2307873
   Flynn S, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480544
   Flynn S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356572
   Forootanina Z, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417842
   Foster N, 1996, GRAPH MODEL IM PROC, V58, P471, DOI 10.1006/gmip.1996.0039
   Fu CY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130878
   Fujisawa M, 2015, COMPUT GRAPH FORUM, V34, P155, DOI 10.1111/cgf.12754
   Gagnon J, 2021, COMPUT GRAPH FORUM, V40, P367, DOI 10.1111/cgf.142639
   Gagnon J, 2019, COMPUT GRAPH FORUM, V38, P491, DOI 10.1111/cgf.13855
   Gao M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275044
   Gao M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201309
   Gao M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130879
   Gao Y, 2021, IEEE T VIS COMPUT GR, V27, P4483, DOI 10.1109/TVCG.2021.3107597
   Gao Y, 2020, COMPUT GRAPH FORUM, V39, P180, DOI 10.1111/cgf.14010
   Gao Y, 2013, IEEE T VIS COMPUT GR, V19, P178, DOI 10.1109/TVCG.2012.117
   GINGOLD RA, 1977, MON NOT R ASTRON SOC, V181, P375, DOI 10.1093/mnras/181.3.375
   Gissler C, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3284980
   Golas A, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366167
   Goldade R, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392455
   Goldade R, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322939
   Goldade R, 2016, COMPUT GRAPH FORUM, V35, P233, DOI 10.1111/cgf.12826
   Goswami P., 2014, Eurographics 2014-Short Papers, P45
   Greengard L, 1997, J COMPUT PHYS, V135, P280, DOI 10.1006/jcph.1997.5706
   Gregson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601147
   Guo J, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459799
   Guo YL, 2017, IEEE T VIS COMPUT GR, V23, P1479, DOI 10.1109/TVCG.2016.2532335
   Harlow F, 1964, Methods for Computational Physics, V3, P319, DOI DOI 10.1007/BF00230516
   Harlow F.H., 1963, Experimental arithmetic, high-speed computations and mathematics
   HARLOW FH, 1965, PHYS FLUIDS, V8, P2182, DOI 10.1063/1.1761178
   He S, 2013, COMPUT GRAPH FORUM, V32, P27, DOI 10.1111/j.1467-8659.2012.03228.x
   He X., 2015, Proc. ACM SIGGRAPH/Eurograph. Symp. Comp. Anim, P129
   He XW, 2018, IEEE T VIS COMPUT GR, V24, P2589, DOI 10.1109/TVCG.2017.2755646
   He XW, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2682630
   He XW, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366168
   Hikaru I, 2020, IEEE T VIS COMPUT GR, V26, P2288, DOI 10.1109/TVCG.2018.2883628
   Hill DJ, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2879177
   Hochstetter H, 2017, ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION (SCA 2017), DOI 10.1145/3099564.3099580
   Holm D., 2009, OXFORD TEXTS APPL EN
   Hu YM, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459671
   Hu YM, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356506
   Hu YM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201293
   Huang LB, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480495
   Huang LB, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417799
   Huang LB, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322973
   Huang WZ, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392094
   Huber M, 2015, COMPUT GRAPH FORUM, V34, P14, DOI 10.1111/cgf.12455
   Hyde DAB, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417845
   Ihmsen M, 2014, IEEE T VIS COMPUT GR, V20, P426, DOI 10.1109/TVCG.2013.105
   Im J, 2013, COMPUT GRAPH FORUM, V32, P371, DOI 10.1111/cgf.12057
   Inglis T, 2017, COMPUT GRAPH FORUM, V36, P354, DOI 10.1111/cgf.13084
   Ishida S, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392405
   Ishida S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130835
   Jamriska O, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766983
   Jang T, 2010, VISUAL COMPUT, V26, P873, DOI 10.1007/s00371-010-0487-1
   Jeschke S, 2020, COMPUT GRAPH FORUM, V39, P47, DOI 10.1111/cgf.14100
   Jeschke S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073678
   Jeschke S, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201336
   Jeschke S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2714572
   Jiang C., 2016, P ACM SIGGRAPH COURS
   Jiang C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766996
   Jiang CFF, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073623
   Jiang Y, 2021, COMPUT GRAPH FORUM, V40, P85, DOI 10.1111/cgf.14403
   Jiang Y, 2020, COMPUT GRAPH FORUM, V39, P69, DOI 10.1111/cgf.14102
   Jones R, 2017, ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION (SCA 2017), DOI 10.1145/3099564.3099573
   Keeler T., 2014, P ACM SIGGRAPH 2014
   Kim B, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392473
   Kim B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356560
   Kim JH, 2017, IEEE T VIS COMPUT GR, V23, P2056, DOI 10.1109/TVCG.2016.2609429
   Kim T, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2451236.2451241
   Kim T, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461987
   Koike T, 2020, COMPUT GRAPH FORUM, V39, P1, DOI 10.1111/cgf.13907
   Koschier D, 2020, Arxiv, DOI arXiv:2009.06944
   Koschier D, 2017, ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION (SCA 2017), DOI 10.1145/3099564.3099565
   Ladicky L, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818129
   Lai JY, 2020, COMPUT GRAPH FORUM, V39, P23, DOI 10.1111/cgf.13909
   Landau L. D., 1959, Fluid Mechanics, DOI DOI 10.1016/C2013-0-03799-1
   Langlois TR, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925904
   Larionov E, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073628
   Lee M., 2019, P 18 ANN ACM SIGGRAP
   Lee M, 2019, IEEE T VIS COMPUT GR, V25, P1449, DOI 10.1109/TVCG.2018.2808972
   Lentine Michael., 2011, Proceedings of the 2011 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '11, P91, DOI [10.1145/2019406.2019419, DOI 10.1145/2019406.2019419]
   Li C, 2021, IEEE T VIS COMPUT GR, V27, P3867, DOI 10.1109/TVCG.2020.2991217
   Li W, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530132
   Li W, 2021, IEEE T VIS COMPUT GR, V27, P3318, DOI 10.1109/TVCG.2020.2972357
   Li W, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392400
   Li W, 2019, IEEE T VIS COMPUT GR, V25, P2694, DOI 10.1109/TVCG.2018.2859931
   Li XS, 2016, IEEE T VIS COMPUT GR, V22, P1973, DOI 10.1109/TVCG.2015.2476788
   Liao XY, 2018, IEEE T VIS COMPUT GR, V24, P1260, DOI 10.1109/TVCG.2017.2665551
   Liu BB, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818130
   Liu HX, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982430
   Liu SS, 2022, IEEE T VIS COMPUT GR, V28, P3168, DOI 10.1109/TVCG.2021.3055789
   Liu S, 2021, COMPUT GRAPH FORUM, V40, P54, DOI 10.1111/cgf.14095
   Liu X, 2014, IEEE T VIS COMPUT GR, V20, P289, DOI 10.1109/TVCG.2012.303
   Losasso F, 2004, ACM T GRAPHIC, V23, P457, DOI 10.1145/1015706.1015745
   Lu JM, 2019, COMPUT GRAPH FORUM, V38, P501, DOI 10.1111/cgf.13856
   LUCY LB, 1977, ASTRON J, V82, P1013, DOI 10.1086/112164
   Lyu CY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480493
   Macklin M, 2014, ACM T GRAPHIC, V33, DOI [10.1145/280/109/2601152, 10.1145/2601097.2601152]
   Macklin M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461984
   Manteaux PL, 2017, COMPUT GRAPH FORUM, V36, P312, DOI 10.1111/cgf.12941
   Mashayekhi O, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3173551
   Mercier O, 2020, COMPUT GRAPH FORUM, V39, P9, DOI 10.1111/cgf.13908
   Mercier O, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818115
   Misztal MK, 2014, IEEE T VIS COMPUT GR, V20, P4, DOI 10.1109/TVCG.2013.97
   Monaghan JJ, 2005, REP PROG PHYS, V68, P1703, DOI 10.1088/0034-4885/68/8/R01
   Morgenroth D, 2020, COMPUT GRAPH FORUM, V39, P27, DOI 10.1111/cgf.14098
   Müller M, 2007, J VIS COMMUN IMAGE R, V18, P109, DOI 10.1016/j.jvcir.2007.01.005
   Museth K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487235
   Nabizadeh MS, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530120
   Nagasawa K, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322947
   Nakanishi R, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417794
   Ni XY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392445
   Nie XY, 2021, COMPUT GRAPH FORUM, V40, P62, DOI 10.1111/cgf.14203
   Nielsen MB, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461918
   Nielsen MB, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421638
   Oh YJ, 2021, COMPUT GRAPH FORUM, V40, P355, DOI 10.1111/cgf.142638
   Okabe M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766958
   Orthmann J., 2013, P 12 ACM SIGGRAPHEUR, P95
   Orthmann J, 2012, COMPUT GRAPH FORUM, V31, P2436, DOI 10.1111/j.1467-8659.2012.03186.x
   Padilla M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322962
   Pan ZR, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3016963
   Pan ZR, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508429
   Pan ZR, 2012, COMPUT GRAPH FORUM, V31, P2029, DOI 10.1111/j.1467-8659.2012.03195.x
   Panuelos J., 2020, EUROGRAPHICSACM SIGG
   Patkar S., 2013, P 12 ACM SIGGRAPHEUR, P105
   Patkar S, 2013, IEEE T VIS COMPUT GR, V19, P1592, DOI 10.1109/TVCG.2013.8
   Peer A, 2018, COMPUT GRAPH FORUM, V37, P135, DOI 10.1111/cgf.13317
   Peer A, 2017, IEEE T VIS COMPUT GR, V23, P2656, DOI 10.1109/TVCG.2016.2636144
   Peer A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766925
   Popinet S, 2003, J COMPUT PHYS, V190, P572, DOI 10.1016/S0021-9991(03)00298-5
   Qu H, 2020, COMPUT GRAPH FORUM, V39, P375, DOI 10.1111/cgf.13809
   Raveendran K, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601126
   Raveendran Karthik., 2012, Controlling liquids using meshes, P255
   Reeves W. T., 1985, Computer Graphics, V19, P313, DOI 10.1145/325165.325250
   Reinhardt S, 2017, ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION (SCA 2017), DOI 10.1145/3099564.3099571
   Ren B, 2022, IEEE T VIS COMPUT GR, V28, P3417, DOI 10.1109/TVCG.2021.3062643
   Ren B, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459764
   Ren B, 2018, IEEE T VIS COMPUT GR, V24, P2411, DOI 10.1109/TVCG.2017.2720672
   Ren B, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2645703
   Roy B, 2021, P ACM COMPUT GRAPH, V4, DOI 10.1145/3480147
   Ruan LW, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459862
   Rungjiratananon W, 2012, COMPUT GRAPH FORUM, V31, P1993, DOI 10.1111/j.1467-8659.2012.03191.x
   RUSIN M, 1975, CHEM ENG SCI, V30, P937, DOI 10.1016/0009-2509(75)80060-1
   Ruuth SJ, 2008, J COMPUT PHYS, V227, P1943, DOI 10.1016/j.jcp.2007.10.009
   Sandim M, 2016, COMPUT GRAPH FORUM, V35, P215, DOI 10.1111/cgf.12824
   Sato S, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201398
   Sato S, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3213771
   Sato T, 2018, COMPUT GRAPH FORUM, V37, P169, DOI 10.1111/cgf.13351
   Schechter H, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185557
   Schoentgen A, 2020, COMPUT GRAPH FORUM, V39, P79, DOI 10.1111/cgf.14103
   Schreck C, 2022, COMPUT GRAPH FORUM, V41, P343, DOI 10.1111/cgf.14478
   Schreck C, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323002
   Setaluri R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661269
   Shah C, 2018, COMPUT GRAPH FORUM, V37, P35, DOI 10.1111/cgf.13510
   Shao H, 2023, IEEE T VIS COMPUT GR, V29, P5394, DOI 10.1109/TVCG.2022.3211414
   Shao H, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530109
   Shao X, 2015, COMPUT GRAPH FORUM, V34, P191, DOI 10.1111/cgf.12467
   Sito Tom, 2015, Moving Innovation: A History of Computer Animation
   Skrivan T, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392466
   Solenthaler B, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531346
   Solenthaler B., 2008, P 2008 ACM SIGGRAPH, P211, DOI 10.2312/SCA/SCA08/211-218
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Stam J., 1993, Computer Graphics Proceedings, P369, DOI 10.1145/166117.166163
   Stanton M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462006
   Stomakhin A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073597
   Stomakhin A, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601176
   Su HZ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459820
   SULSKY D, 1995, COMPUT PHYS COMMUN, V87, P236, DOI 10.1016/0010-4655(94)00170-7
   Sun YC, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480541
   Sun YX, 2020, COMPUT GRAPH FORUM, V39, P55, DOI 10.1111/cgf.14101
   Takahashi T, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480539
   Takahashi T, 2019, COMPUT GRAPH FORUM, V38, P49, DOI 10.1111/cgf.13618
   Takahashi T, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417798
   Takahashi T, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356551
   Takahashi T, 2018, COMPUT GRAPH FORUM, V37, P313, DOI 10.1111/cgf.13292
   Takahashi T, 2016, COMPUT GRAPH FORUM, V35, P517, DOI 10.1111/cgf.13048
   Takahashi T, 2015, COMPUT GRAPH FORUM, V34, P493, DOI 10.1111/cgf.12578
   Tampubolon AP, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073651
   Tang JW, 2021, COMPUT GRAPH FORUM, V40, P339, DOI 10.1111/cgf.142637
   Tao M, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530138
   Teng Y, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980229
   Tessendorf J., 2001, P SIGGRAPH COURSES
   Thuerey N, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2956233
   Treuille A, 2006, ACM T GRAPHIC, V25, P826, DOI 10.1145/1141911.1141962
   Truong N, 2022, IEEE T VIS COMPUT GR, V28, P4546, DOI 10.1109/TVCG.2021.3093776
   Um K, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073633
   Vantzos O, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275086
   Vantzos O, 2017, IEEE T VIS COMPUT GR, V23, P1179, DOI 10.1109/TVCG.2016.2605083
   Vines M, 2014, IEEE T VIS COMPUT GR, V20, P303, DOI 10.1109/TVCG.2013.95
   Wang HM, 2005, ACM T GRAPHIC, V24, P921, DOI 10.1145/1073204.1073284
   Wang H, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392487
   Wang MD, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459864
   Wang XJ, 2018, IEEE T VIS COMPUT GR, V24, P3214, DOI 10.1109/TVCG.2017.2789203
   Wang XL, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392442
   Weber D, 2015, COMPUT GRAPH FORUM, V34, P481, DOI 10.1111/cgf.12577
   Weiler M, 2018, COMPUT GRAPH FORUM, V37, P145, DOI 10.1111/cgf.13349
   WEJCHERT J, 1991, COMP GRAPH, V25, P19, DOI 10.1145/127719.122719
   Wiewel S, 2020, COMPUT GRAPH FORUM, V39, P15, DOI 10.1111/cgf.14097
   Wiewel S, 2019, COMPUT GRAPH FORUM, V38, P71, DOI 10.1111/cgf.13620
   Winchenbach R, 2020, COMPUT GRAPH FORUM, V39, P527, DOI 10.1111/cgf.14090
   Winchenbach R., 2016, P ACM SIGGRAPHEUROGR, P49, DOI [10.2312/sca.20161222, DOI 10.2312/SCA.20161222]
   Winchenbach R, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3363555
   Winchenbach R, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417829
   Winchenbach R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073713
   Wretborn J, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530059
   Wu K, 2018, COMPUT GRAPH FORUM, V37, P157, DOI 10.1111/cgf.13350
   Wu XY, 2013, COMPUT GRAPH FORUM, V32, P389, DOI 10.1111/cgf.12059
   Xiao XY, 2019, COMPUT GRAPH FORUM, V38, P431, DOI 10.1111/cgf.13649
   Xiao XY, 2020, IEEE T VIS COMPUT GR, V26, P1454, DOI 10.1109/TVCG.2018.2873375
   Xiao YW, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417837
   Xie Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201304
   Xiong SY, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530150
   Xiong SY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459865
   Xue T, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417863
   Yan GW, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417832
   Yan X, 2018, COMPUT GRAPH FORUM, V37, P183, DOI 10.1111/cgf.13523
   Yan X, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925897
   Yang LP, 2014, COMPUT GRAPH FORUM, V33, P199, DOI 10.1111/cgf.12488
   Yang LP, 2012, COMPUT GRAPH FORUM, V31, P2037, DOI 10.1111/j.1467-8659.2012.03196.x
   Yang M, 2019, IEEE T VIS COMPUT GR, V25, P2873, DOI 10.1109/TVCG.2018.2864283
   Yang SQ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459866
   Yang T, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130882
   Yang T, 2017, IEEE T VIS COMPUT GR, V23, P2235, DOI 10.1109/TVCG.2017.2706289
   Yang T, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818117
   Yu JH, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421641
   Yu JH, 2012, COMPUT GRAPH FORUM, V31, P815, DOI 10.1111/j.1467-8659.2012.03062.x
   Yue YH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275095
   Yue YH, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2751541
   Zarifi O, 2017, ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION (SCA 2017), DOI 10.1145/3099564.3099572
   Zehnder J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201324
   Zhai X, 2020, IEEE T VIS COMPUT GR, V26, P2234, DOI 10.1109/TVCG.2018.2886322
   Zhai X, 2017, COMPUT GRAPH FORUM, V36, P100, DOI 10.1111/cgf.12861
   Zhang XX, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661261
   Zhang XX, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925910
   Zhang XX, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766982
   Zhang YZ, 2012, IEEE T VIS COMPUT GR, V18, P1281, DOI 10.1109/TVCG.2011.141
   Zhang YB, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508401
   Zheng Y, 2019, IEEE T VIS COMPUT GR, V25, P2471, DOI 10.1109/TVCG.2018.2832039
   Zhu B, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766981
   Zhu B, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601201
   Zhu B, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461999
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
NR 320
TC 2
Z9 2
U1 6
U2 6
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD 2024 APR 27
PY 2024
DI 10.1007/s41095-023-0368-y
EA APR 2024
PG 56
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OR3R7
UT WOS:001208966500003
OA gold
DA 2024-08-05
ER

PT J
AU Zhang, F
   Chen, GG
   Wang, H
   Zhang, CM
AF Zhang, Fan
   Chen, Gongguan
   Wang, Hua
   Zhang, Caiming
TI CF-DAN: Facial-expression recognition based on cross-fusion
   dual-attention network
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE facial-expression recognition (FER); cubic polynomial activation
   function; dual-attention mechanism; interactive learning; self-attention
   distillation
AB Recently, facial-expression recognition (FER) has primarily focused on images in the wild, including factors such as face occlusion and image blurring, rather than laboratory images. Complex field environments have introduced new challenges to FER. To address these challenges, this study proposes a cross-fusion dual-attention network. The network comprises three parts: (1) a cross-fusion grouped dual-attention mechanism to refine local features and obtain global information; (2) a proposed C2 activation function construction method, which is a piecewise cubic polynomial with three degrees of freedom, requiring less computation with improved flexibility and recognition abilities, which can better address slow running speeds and neuron inactivation problems; and (3) a closed-loop operation between the self-attention distillation process and residual connections to suppress redundant information and improve the generalization ability of the model. The recognition accuracies on the RAF-DB, FERPlus, and AffectNet datasets were 92.78%, 92.02%, and 63.58%, respectively. Experiments show that this model can provide more effective solutions for FER tasks.
C1 [Zhang, Fan; Chen, Gongguan] Shandong Technol & Business Univ, Yantai 264005, Shandong, Peoples R China.
   [Wang, Hua] Ludong Univ, Sch Informat & Elect Engn, Yantai 264025, Peoples R China.
   [Zhang, Caiming] Shangdong Univ, Yantai 250100, Shandong, Peoples R China.
C3 Shandong Technology & Business University; Ludong University; Shandong
   University
RP Wang, H (corresponding author), Ludong Univ, Sch Informat & Elect Engn, Yantai 264025, Peoples R China.
EM hua.wang@ldu.edu.cn
RI Zhang, Caiming/AHD-6558-2022
OI Zhang, Caiming/0000-0002-6365-6221
FU National Natural Science Foundation of China [62272281, 62007017];
   Special Funds for Taishan Scholars Project [tsqn202306274]; Youth
   Innovation Technology Project of the Higher School in Shandong Province
   [2019KJN042]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant Nos. 62272281 and 62007017, the Special
   Funds for Taishan Scholars Project under Grant No. tsqn202306274, and
   Youth Innovation Technology Project of the Higher School in Shandong
   Province under Grant No. 2019KJN042.
CR Amos B., 2016, Openface: A general-purpose face recognition library with mobile applications
   Aouayeb M., 2021, ARXIV
   Baddar WJ, 2022, IEEE T AFFECT COMPUT, V13, P159, DOI 10.1109/TAFFC.2019.2957465
   Barsoum E, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P279, DOI 10.1145/2993148.2993165
   Bin Jiang, 2011, Active Media Technology. Proceedings 7th International Conference, AMT 2011, P92, DOI 10.1007/978-3-642-23620-4_13
   Binte Ali Humayra, 2015, International Journal of Machine Learning and Computing, V5, P142, DOI 10.7763/IJMLC.2015.V5.498
   Bourel Fabrice., 2001, BMVC, P1
   Chen M, 2020, PR MACH LEARN RES, V119
   Cho S., 2022, arXiv
   Dhall A, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   Ding MY, 2022, LECT NOTES COMPUT SC, V13684, P74, DOI 10.1007/978-3-031-20053-3_5
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Edwards J, 2002, CLIN PSYCHOL REV, V22, P1267, DOI 10.1016/S0272-7358(02)00162-9
   Fan Y, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P445, DOI 10.1145/2993148.2997632
   Feffer M, 2018, Machine Learning and Data Mining in Pattern Recognition. Ed. by, P316
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gedamu K., 2022, P SPIE 12342 14 INT
   Glorot X., 2011, P 14 INT C ART INT S, V15, P315, DOI DOI 10.1002/ECS2.1832
   Hammal Z, 2009, J VISION, V9, DOI 10.1167/9.2.22
   Han J, 1995, LECT NOTES COMPUT SC, V930, P195
   Happy SL, 2015, IEEE T AFFECT COMPUT, V6, P1, DOI 10.1109/TAFFC.2014.2386334
   Huang QH, 2021, INFORM SCIENCES, V580, P35, DOI 10.1016/j.ins.2021.08.043
   Jiang SP, 2022, INT CONF ACOUST SPEE, P2599, DOI 10.1109/ICASSP43922.2022.9747322
   Joshi A, 2020, IEEE INT VEH SYM, P207, DOI 10.1109/IV47402.2020.9304579
   Lei Pang, 2018, 2018 International Conference on Security, Pattern Analysis, and Cybernetics (SPAC), P489, DOI 10.1109/SPAC46244.2018.8965443
   Li H., 2021, ARXIV
   Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277
   Li XQ, 2020, IET IMAGE PROCESS, V14, P1059, DOI 10.1049/iet-ipr.2019.0963
   Li YS, 2023, MULTIMED TOOLS APPL, V82, P15515, DOI 10.1007/s11042-022-13867-z
   Li YJ, 2022, IEEE T CIRC SYST VID, V32, P3190, DOI 10.1109/TCSVT.2021.3103782
   Liu HW, 2022, IEEE T CIRC SYST VID, V32, P6253, DOI 10.1109/TCSVT.2022.3165321
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ma F., 2021, ARXIV
   Majumder A, 2018, IEEE T CYBERNETICS, V48, P103, DOI 10.1109/TCYB.2016.2625419
   Mao X, 2009, 2009 10TH INTERNATIONAL WORKSHOP ON IMAGE ANALYSIS FOR MULTIMEDIA INTERACTIVE SERVICES, P113, DOI 10.1109/WIAMIS.2009.5031445
   Otberdout N, 2022, IEEE T PATTERN ANAL, V44, P848, DOI 10.1109/TPAMI.2020.3002500
   Putro MD, 2020, IEEE IND ELEC, P411, DOI [10.1109/iecon43393.2020.9254805, 10.1109/IECON43393.2020.9254805]
   Ruan DL, 2021, PROC CVPR IEEE, P7656, DOI 10.1109/CVPR46437.2021.00757
   Shan CF, 2009, IMAGE VISION COMPUT, V27, P803, DOI 10.1016/j.imavis.2008.08.005
   She JH, 2021, PROC CVPR IEEE, P6244, DOI 10.1109/CVPR46437.2021.00618
   Shi J., 2021, ARXIV
   Shokoohi Z, 2015, 2015 2ND INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION AND IMAGE ANALYSIS (IPRIA)
   Song WY, 2022, IET IMAGE PROCESS, V16, P2157, DOI 10.1049/ipr2.12480
   Tran L, 2019, IEEE T PATTERN ANAL, V41, P3007, DOI 10.1109/TPAMI.2018.2868350
   Ullah H., 2022, ARXIV
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang K, 2020, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR42600.2020.00693
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang Z, 2012, 2012 IEEE INFORMATION THEORY WORKSHOP (ITW), P222, DOI [10.1109/ICNC.2012.6234551, 10.1109/ITW.2012.6404663]
   Wen Z., 2021, arXiv
   Wu Tingfan., 2010, IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P42
   Xue FL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3581, DOI 10.1109/ICCV48922.2021.00358
   Zhang F, 2023, IEEE T CIRC SYST VID, V33, P4496, DOI 10.1109/TCSVT.2023.3278131
   Zhang FF, 2020, IEEE T IMAGE PROCESS, V29, P6574, DOI 10.1109/TIP.2020.2991549
   Zhang QL, 2021, ADV NEUR IN, V34
   Zhang T, 2019, INT J COMPUT MATH, V96, P594, DOI 10.1080/00207160.2018.1455092
   Zhang X, 2022, IEEE T CIRC SYST VID, V32, P1681, DOI 10.1109/TCSVT.2021.3056098
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhao ZQ, 2021, AAAI CONF ARTIF INTE, V35, P3510
   Zheng, 2022, ARXIV
NR 61
TC 2
Z9 2
U1 13
U2 13
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2024
VL 10
IS 3
BP 593
EP 608
DI 10.1007/s41095-023-0369-x
EA FEB 2024
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QO3G5
UT WOS:001159523000002
OA gold
DA 2024-08-05
ER

PT J
AU Li, YX
   Yang, LF
   Li, X
AF Li, Yuxuan
   Yang, Lingfeng
   Li, Xiang
TI APF-GAN: Exploring asymmetric pre-training and fine-tuning strategy for
   conditional generative adversarial network
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
C1 [Li, Yuxuan; Li, Xiang] Nankai Univ, Coll Comp Sci, Tianjin, Peoples R China.
   [Yang, Lingfeng] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing, Peoples R China.
C3 Nankai University; Nanjing University of Science & Technology
RP Li, X (corresponding author), Nankai Univ, Coll Comp Sci, Tianjin, Peoples R China.
EM yuxuan.li.17@ucl.ac.uk; yanglfnjust@njust.edu.cn;
   xiang.li.implus@nankai.edu.cn
RI Liu, Zhe/KEJ-5299-2024
FU Fundamental Research Funds for the Central Universities [07063233084];
   Young Scientists Fund of the National Natural Science Foundation of
   China [62206134]; Tianjin Key Laboratory of Visual Computing and
   Intelligent Perception (VCIP); Supercomputing Center of Nankai
   University
FX This work was supported by the Fundamental Research Funds for the
   Central Universities 07063233084, the Young Scientists Fund of the
   National Natural Science Foundation of China (Grant No. 62206134), and
   the Tianjin Key Laboratory of Visual Computing and Intelligent
   Perception (VCIP). Computation is supported by the Supercomputing Center
   of Nankai University (NKSC).
CR Hensel M, 2017, ADV NEUR IN, V30
   Hu SM, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-020-3097-4
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Karras T, 2018, Arxiv, DOI [arXiv:1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Lee JT, 2019, IEEE I CONF COMP VIS, P1191, DOI 10.1109/ICCV.2019.00128
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   She DY, 2021, PROC CVPR IEEE, P8471, DOI 10.1109/CVPR46437.2021.00837
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Xie EZ, 2021, ADV NEUR IN, V34
   Xue Y, 2022, COMPUT VIS MEDIA, V8, P3, DOI 10.1007/s41095-021-0234-8
   Zhou WY, 2021, COMPUT VIS MEDIA, V7, P153, DOI 10.1007/s41095-021-0203-2
NR 13
TC 0
Z9 0
U1 5
U2 6
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD FEB
PY 2024
VL 10
IS 1
BP 187
EP 192
DI 10.1007/s41095-023-0357-1
PG 6
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Z5WZ7
UT WOS:001112788200008
OA gold
DA 2024-08-05
ER

PT J
AU Herbert, P
   Wu, J
   Ji, Z
   Lai, YK
AF Herbert, Peter
   Wu, Jing
   Ji, Ze
   Lai, Yu-Kun
TI Benchmarking visual SLAM methods in mirror environments
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE visual simultaneous localisation and mapping (vSLAM); mirror;
   localisation; mapping; reflection; dataset
ID SPECULAR REFLECTIONS; CALIBRATION; VERSATILE; ROBUST
AB Visual simultaneous localisation and mapping (vSLAM) finds applications for indoor and outdoor navigation that routinely subjects it to visual complexities, particularly mirror reflections. The effect of mirror presence (time visible and its average size in the frame) was hypothesised to impact localisation and mapping performance, with systems using direct techniques expected to perform worse. Thus, a dataset, MirrEnv, of image sequences recorded in mirror environments, was collected, and used to evaluate the performance of existing representative methods. RGBD ORB-SLAM3 and BundleFusion appear to show moderate degradation of absolute trajectory error with increasing mirror duration, whilst the remaining results did not show significantly degraded localisation performance. The mesh maps generated proved to be very inaccurate, with real and virtual reflections colliding in the reconstructions. A discussion is given of the likely sources of error and robustness in mirror environments, outlining future directions for validating and improving vSLAM performance in the presence of planar mirrors. The MirrEnv dataset is available at https://doi.org/10.17035/d.2023.0292477898.
C1 [Herbert, Peter; Wu, Jing; Lai, Yu-Kun] Cardiff Univ, Sch Comp Sci & Informat, Abacws Bldg,Senghennydd Rd, Cardiff CF24 4AG, Wales.
   [Ji, Ze] Cardiff Univ, Sch Engn, Queens Bldg, Cardiff CF24 3AA, Wales.
C3 Cardiff University; Cardiff University
RP Wu, J (corresponding author), Cardiff Univ, Sch Comp Sci & Informat, Abacws Bldg,Senghennydd Rd, Cardiff CF24 4AG, Wales.
EM Herbertp1@cardiff.ac.uk; WuJ11@cardiff.ac.uk; JiZ1@cardiff.ac.uk;
   LaiY4@cardiff.ac.uk
RI Lai, Yu-Kun/D-2343-2010
FU UK EPSRC [EP/T517951/1, 2435656]
FX This research was funded by the UK EPSRC through a Doctoral Training
   Partnership No. EP/T517951/1(2435656).
CR Andreff N., 1999, Second International Conference on 3-D Digital Imaging and Modeling (Cat. No.PR00062), P430, DOI 10.1109/IM.1999.805374
   [Anonymous], 2007, P IEEE C COMP VIS PA
   Bloesch M, 2018, PROC CVPR IEEE, P2560, DOI 10.1109/CVPR.2018.00271
   Burri M, 2016, INT J ROBOT RES, V35, P1157, DOI 10.1177/0278364915620033
   Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754
   Campos C, 2021, IEEE T ROBOT, V37, P1874, DOI 10.1109/TRO.2021.3075644
   Chen CH, 2020, Arxiv, DOI arXiv:2006.12567
   Czarnowski J, 2020, IEEE ROBOT AUTOM LET, V5, P721, DOI 10.1109/LRA.2020.2965415
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Dai A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3054739
   Daniilidis K, 1999, INT J ROBOT RES, V18, P286, DOI 10.1177/02783649922066213
   Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049
   Duan C, 2019, TRANSP SAFETY ENV, V1, P177, DOI 10.1093/tse/tdz019
   Engel J, 2016, Arxiv, DOI arXiv:1607.02555
   Engel J, 2018, IEEE T PATTERN ANAL, V40, P611, DOI 10.1109/TPAMI.2017.2658577
   Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54
   Engel J, 2013, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2013.183
   Forster C, 2017, IEEE T ROBOT, V33, P249, DOI 10.1109/TRO.2016.2623335
   Forster C, 2014, IEEE INT CONF ROBOT, P15, DOI 10.1109/ICRA.2014.6906584
   Fuentes-Pacheco J, 2015, ARTIF INTELL REV, V43, P55, DOI 10.1007/s10462-012-9365-8
   Gao X, 2018, IEEE INT C INT ROBOT, P2198, DOI 10.1109/IROS.2018.8593376
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Graeter J, 2018, IEEE INT C INT ROBOT, P7872, DOI 10.1109/IROS.2018.8594394
   Handa A, 2014, IEEE INT CONF ROBOT, P1524, DOI 10.1109/ICRA.2014.6907054
   Hart J. W., 2012, P 26 AAAI C ART INT
   Havasi L, 2009, IEEE T IMAGE PROCESS, V18, P1366, DOI 10.1109/TIP.2009.2017137
   Huang BC, 2020, Arxiv, DOI arXiv:1909.05214
   Huang JH, 2021, COMPUT VIS MEDIA, V7, P87, DOI 10.1007/s41095-020-0195-3
   Huang JW, 2019, COMPUT ANIMAT VIRT W, V30, DOI 10.1002/cav.1895
   Huang SS, 2020, IEEE INT CONF ROBOT, P1091, DOI [10.1109/ICRA40945.2020.9196613, 10.1109/icra40945.2020.9196613]
   Kazerouni IA, 2022, EXPERT SYST APPL, V205, DOI 10.1016/j.eswa.2022.117734
   Kerl C, 2013, IEEE INT C INT ROBOT, P2100, DOI 10.1109/IROS.2013.6696650
   Klein George, 2007, P1
   Koch R, 2016, ADV INTELL SYST COMP, V417, P133, DOI 10.1007/978-3-319-27146-0_11
   Lee HS, 2011, IEEE I CONF COMP VIS, P1203, DOI 10.1109/ICCV.2011.6126370
   Lin JY, 2020, PROC CVPR IEEE, P3694, DOI 10.1109/CVPR42600.2020.00375
   Liu PD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5530, DOI 10.1109/ICCV48922.2021.00550
   Ma P, 2019, IEEE ACCESS, V7, P178300, DOI 10.1109/ACCESS.2019.2958374
   Mei HY, 2021, PROC CVPR IEEE, P3043, DOI 10.1109/CVPR46437.2021.00306
   Ming YH, 2022, Arxiv, DOI arXiv:2209.07919
   Mourikis AI, 2007, IEEE INT CONF ROBOT, P3565, DOI 10.1109/ROBOT.2007.364024
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Mur-Artal R, 2017, IEEE ROBOT AUTOM LET, V2, P796, DOI 10.1109/LRA.2017.2653359
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Park D, 2021, IEEE ROBOT AUTOM LET, V6, P635, DOI 10.1109/LRA.2020.3047796
   PARK FC, 1994, IEEE T ROBOTIC AUTOM, V10, P717, DOI 10.1109/70.326576
   Pretto Alberto, 2009, 2009 IEEE International Conference on Robotics and Automation (ICRA), P2250, DOI 10.1109/ROBOT.2009.5152447
   Qin T, 2018, IEEE T ROBOT, V34, P1004, DOI 10.1109/TRO.2018.2853729
   Rabiee S., 2020, P 4 C ROB LEARN, P1100
   Safeea M, 2019, IEEE ROBOT AUTOM MAG, V26, P91, DOI 10.1109/MRA.2018.2877776
   Schöps T, 2019, PROC CVPR IEEE, P134, DOI 10.1109/CVPR.2019.00022
   Seonwook Park, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4523, DOI 10.1109/ICRA.2017.7989525
   Servières M, 2021, J SENSORS, V2021, DOI 10.1155/2021/2054828
   Shah M., 2012, PROC WORKSHOP PERFO, P15, DOI [10.1145/2393091.2393095, DOI 10.1145/2393091.2393095]
   Shah SMZA, 2017, MACH VISION APPL, V28, P409, DOI 10.1007/s00138-017-0826-6
   Sharafutdinov D, 2021, Arxiv, DOI arXiv:2108.01654
   Siegwart R., 2011, Introduction to Autonomous Mobile Robots
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Sirinukulwattana T, 2015, IEEE IMAGE PROC, P1940, DOI 10.1109/ICIP.2015.7351139
   Straub J, 2019, Arxiv, DOI arXiv:1906.05797
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   Taketomi T., 2017, IPSJ Transactions on Computer Vision and Applications, V9, P16, DOI 10.1186/s41074-017-0027-2
   Tan JQ, 2021, PROC CVPR IEEE, P15985, DOI 10.1109/CVPR46437.2021.01573
   Tang JX, 2019, IEEE ROBOT AUTOM LET, V4, P3505, DOI 10.1109/LRA.2019.2927954
   Tang JX, 2018, IEEE ROBOT AUTOM LET, V3, P1010, DOI 10.1109/LRA.2018.2794624
   Tateno K, 2017, PROC CVPR IEEE, P6565, DOI 10.1109/CVPR.2017.695
   TSAI RY, 1989, IEEE T ROBOTIC AUTOM, V5, P345, DOI 10.1109/70.34770
   Wang K, 2022, IEEE T COGN DEV SYST, V14, P35, DOI 10.1109/TCDS.2020.3038898
   Wang WS, 2020, IEEE INT C INT ROBOT, P4909, DOI 10.1109/IROS45743.2020.9341801
   Whelan T, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201319
   Whelan T, 2016, INT J ROBOT RES, V35, P1697, DOI 10.1177/0278364916669237
   Whelan T, 2015, INT J ROBOT RES, V34, P598, DOI 10.1177/0278364914551008
   Yang N, 2020, PROC CVPR IEEE, P1278, DOI 10.1109/CVPR42600.2020.00136
   Yang SW, 2008, IEEE INT CONF ROBOT, P3009, DOI 10.1109/ROBOT.2008.4543667
   Yang X, 2019, IEEE I CONF COMP VIS, P8808, DOI 10.1109/ICCV.2019.00890
   Yi Zeng, 2016, Advances in Brain-Inspired Cognitive Systems. 8th International Conference, BICS 2016. Proceedings: LNAI 10023, P11, DOI 10.1007/978-3-319-49685-6_2
   Yousif K, 2017, ROBOTICA, V35, P809, DOI 10.1017/S0263574715000831
   Zhang ZC, 2018, IEEE INT C INT ROBOT, P7244, DOI 10.1109/IROS.2018.8593941
   Zhao F., 2020, FangGet/bundlefusion_ubuntu_pangolin: Aporting for bundlefusion working on ubuntu, with Pangolin as Visualizer
   Zhou HZ, 2015, IEEE T VEH TECHNOL, V64, P1364, DOI 10.1109/TVT.2015.2388780
   Zhu ZH, 2022, PROC CVPR IEEE, P12776, DOI 10.1109/CVPR52688.2022.01245
NR 81
TC 0
Z9 0
U1 15
U2 22
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD APR
PY 2024
VL 10
IS 2
BP 215
EP 241
DI 10.1007/s41095-022-0329-x
PG 27
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DW8M2
UT WOS:001135210800005
OA Green Accepted, gold
DA 2024-08-05
ER

PT J
AU Deng, Z
   Xiao, HY
   Lang, YN
   Feng, H
   Zhang, JY
AF Deng, Zhi
   Xiao, Haoyao
   Lang, Yining
   Feng, Hao
   Zhang, Juyong
TI Multi-scale hash encoding based neural geometry representation
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE neural geometry representation; hash encoding; point cloud
   reconstruction; multi-view reconstruction
AB Recently, neural implicit function-based representation has attracted more and more attention, and has been widely used to represent surfaces using differentiable neural networks. However, surface reconstruction from point clouds or multi-view images using existing neural geometry representations still suffer from slow computation and poor accuracy. To alleviate these issues, we propose a multi-scale hash encoding-based neural geometry representation which effectively and efficiently represents the surface as a signed distance field. Our novel neural network structure carefully combines low-frequency Fourier position encoding with multi-scale hash encoding. The initialization of the geometry network and geometry features of the rendering module are accordingly redesigned. Our experiments demonstrate that the proposed representation is at least 10 times faster for reconstructing point clouds with millions of points. It also significantly improves speed and accuracy of multi-view reconstruction. Our code and models are available at https://github.com/Dengzhi-USTC/Neural-Geometry-Reconstruction.
C1 [Deng, Zhi; Xiao, Haoyao; Zhang, Juyong] Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Peoples R China.
   [Lang, Yining; Feng, Hao] Alibaba Grp, Alibaba Artificial Intelligence Governance Lab, Hangzhou 310017, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Alibaba Group
RP Zhang, JY (corresponding author), Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Peoples R China.
EM zhideng@mail.ustc.edu.cn; hy1999512@mail.ustc.edu.cn;
   ouis.lyn@alibaba-inc.com; yuanning.fh@alibaba-inc.com;
   juyong@ustc.edu.cn
FU National Natural Science Foundation of China [62122071, 62272433];
   Fundamental Research Funds for the Central Universities [WK3470000021];
   Alibaba Innovation Research Program (AIR)
FX This research was partially supported by the National Natural Science
   Foundation of China (Nos. 62122071 and 62272433), the Fundamental
   Research Funds for the Central Universities (No. WK3470000021), and the
   Alibaba Innovation Research Program (AIR). The authors thank Peng Wang
   (the University of Hong Kong) for providing the script for evaluation of
   multiview reconstruction, and Xueying Wang and Yuxin Yao (both from
   University of Science and Technology of China) for their help with paper
   writing.
CR Atzmon M., 2021, P 9 INT C LEARNING R
   Atzmon M, 2020, PROC CVPR IEEE, P2562, DOI 10.1109/CVPR42600.2020.00264
   Baorui M., 2021, P INT C MACH LEARN, P7246
   Berger Matthew, 2017, Computer Graphics Forum, V36, P301, DOI 10.1111/cgf.12802
   Cai H., 2022, P 36 C NEURAL INFORM, V35, P967
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   Chabra R., 2020, COMPUTER VISION ECCV, P608
   Chan ER, 2022, PROC CVPR IEEE, P16102, DOI 10.1109/CVPR52688.2022.01565
   Chen R, 2019, IEEE I CONF COMP VIS, P1538, DOI 10.1109/ICCV.2019.00162
   Chen ZQ, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530108
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   Cheng X., 2020, Advances in Neural Information Processing Systems (NeurIPS), V33, DOI DOI 10.5555/3495724.3497582
   Chibane J., 2020, Advances in Neural Information Processing Systems, V33, P21638
   Deng Z, 2023, IEEE T VIS COMPUT GR, V29, P3826, DOI 10.1109/TVCG.2022.3170853
   Erler Philipp, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P108, DOI 10.1007/978-3-030-58558-7_7
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Furukawa Y, 2013, FOUND TRENDS COMPUT, V9, P1, DOI 10.1561/0600000052
   Gao X, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555501
   Genova K, 2020, PROC CVPR IEEE, P4856, DOI 10.1109/CVPR42600.2020.00491
   Gropp A., 2020, PMLR, P3789
   Hertz A., 2021, Advances in Neural Information Processing Systems, P8820
   Huang JH, 2021, PROC CVPR IEEE, P8928, DOI 10.1109/CVPR46437.2021.00882
   Jensen R, 2014, PROC CVPR IEEE, P406, DOI 10.1109/CVPR.2014.59
   Jiang BY, 2022, PROC CVPR IEEE, P5595, DOI 10.1109/CVPR52688.2022.00552
   Jiang CY, 2020, PROC CVPR IEEE, P6000, DOI 10.1109/CVPR42600.2020.00604
   Kar Abhishek, 2017, NeurIPS, P364
   Kasten Y., 2020, PROC NEURIPS, V33, P2492, DOI 10.48550/arXiv.2003.09852
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Klingensmith M, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI
   Langguth F, 2016, LECT NOTES COMPUT SC, V9907, P469, DOI 10.1007/978-3-319-46487-9_29
   Liang R., 2022, ARXIV
   Liu SL, 2021, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR46437.2021.00183
   Lorensen H. E., 1987, Proc. SIGGRAPH, V21, P163, DOI 10.1145/37401.37422
   Martel JNP, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459785
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Niemeyer M, 2020, PROC CVPR IEEE, P3501, DOI 10.1109/CVPR42600.2020.00356
   Niessner M, 2013, ACM T GRAPHIC, V32, P1, DOI DOI 10.1145/2508363.2508374
   Oechsle M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5569, DOI 10.1109/ICCV48922.2021.00554
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Paszke A, 2019, ADV NEUR IN, V32
   Peng R, 2022, PROC CVPR IEEE, P8635, DOI 10.1109/CVPR52688.2022.00845
   Peng S., 2020, COMPUTER VISION ECCV
   Rahaman N, 2019, PR MACH LEARN RES, V97
   Schönberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31
   Sitzmann V., 2020, Advances in neural information processing systems, V33, P7462, DOI DOI 10.48550/ARXIV.2006.09661
   Suhail M, 2022, PROC CVPR IEEE, P8259, DOI 10.1109/CVPR52688.2022.00809
   Sun JM, 2021, PROC CVPR IEEE, P15593, DOI 10.1109/CVPR46437.2021.01534
   Takikawa T, 2021, PROC CVPR IEEE, P11353, DOI 10.1109/CVPR46437.2021.01120
   Tancik M., 2020, ADV NEURAL INFORM PR, V33, P7537, DOI DOI 10.48550/ARXIV.2006.10739
   Tiwary K, 2022, LECT NOTES COMPUT SC, V13693, P300, DOI 10.1007/978-3-031-19827-4_18
   Walder C., 2006, P 19 INT C NEURAL IN, P273
   Wang FJ, 2021, PROC CVPR IEEE, P14189, DOI 10.1109/CVPR46437.2021.01397
   Wang P., 2021, P 30 INT JOINT C ART, P1091
   Wang P, 2021, 35 C NEURAL INFORM P, V34
   Wang XY, 2022, IEEE T MULTIMEDIA, V24, P4028, DOI 10.1109/TMM.2021.3111485
   Wardetzky M., 2007, Proceedings of the Fifth Eurographics Symposium on Geometry Processing, P33, DOI [DOI 10.2312/SGP/SGP07/033-037, 10.2312/SGP/SGP07/033-037]
   Wei Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5590, DOI 10.1109/ICCV48922.2021.00556
   Xiao YP, 2020, COMPUT VIS MEDIA, V6, P113, DOI 10.1007/s41095-020-0174-8
   Xu HF, 2020, PROC CVPR IEEE, P1956, DOI 10.1109/CVPR42600.2020.00203
   Yang HT, 2020, PROC CVPR IEEE, P598, DOI 10.1109/CVPR42600.2020.00068
   Yang JY, 2022, IEEE T PATTERN ANAL, V44, P4748, DOI 10.1109/TPAMI.2021.3082562
   Yang ZP, 2022, PROC CVPR IEEE, P8564, DOI 10.1109/CVPR52688.2022.00838
   Yao Y, 2020, PROC CVPR IEEE, P1787, DOI 10.1109/CVPR42600.2020.00186
   Yao Y, 2019, PROC CVPR IEEE, P5520, DOI 10.1109/CVPR.2019.00567
   Yariv L., 2021, ADV NEURAL INFORM PR, P4805, DOI DOI 10.48550/ARXIV.2106.12052
   Zhang JY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6505, DOI 10.1109/ICCV48922.2021.00646
NR 68
TC 0
Z9 0
U1 1
U2 1
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2024
VL 10
IS 3
BP 453
EP 470
DI 10.1007/s41095-023-0340-x
EA MAR 2024
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QO3G5
UT WOS:001190323500005
OA gold
DA 2024-08-05
ER

PT J
AU Dai, YR
   Li, J
   Jiang, YQ
   Qin, HD
   Liang, B
   Hong, SK
   Pan, HZ
   Yang, T
AF Dai, Yanran
   Li, Jing
   Jiang, Yuqi
   Qin, Haidong
   Liang, Bang
   Hong, Shikuan
   Pan, Haozhe
   Yang, Tao
TI Real-time distance field acceleration based free-viewpoint video
   synthesis for large sports fields
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE free-viewpoint video; view synthesis; camera array; distance field;
   sports video
ID DIBR
AB Free-viewpoint video allows the user to view objects from any virtual perspective, creating an immersive visual experience. This technology enhances the interactivity and freedom of multimedia performances. However, many free-viewpoint video synthesis methods hardly satisfy the requirement to work in real time with high precision, particularly for sports fields having large areas and numerous moving objects. To address these issues, we propose a free-viewpoint video synthesis method based on distance field acceleration. The central idea is to fuse multi-view distance field information and use it to adjust the search step size adaptively. Adaptive step size search is used in two ways: for fast estimation of multi-object three-dimensional surfaces, and synthetic view rendering based on global occlusion judgement. We have implemented our ideas using parallel computing for interactive display, using CUDA and OpenGL frameworks, and have used real-world and simulated experimental datasets for evaluation. The results show that the proposed method can render free-viewpoint videos with multiple objects on large sports fields at 25 fps. Furthermore, the visual quality of our synthetic novel viewpoint images exceeds that of state-of-the-art neural-rendering-based methods.
C1 [Dai, Yanran; Li, Jing; Jiang, Yuqi; Hong, Shikuan; Pan, Haozhe] Xidian Univ, Sch Telecommun Engn, Xian 710071, Peoples R China.
   [Qin, Haidong; Liang, Bang; Yang, Tao] Northwestern Polytech Univ, Sch Comp Sci, Natl Engn Lab Integrated Aerosp Ground Ocean Big D, SAIIP, Xian 710072, Peoples R China.
C3 Xidian University; Northwestern Polytechnical University
RP Li, J (corresponding author), Xidian Univ, Sch Telecommun Engn, Xian 710071, Peoples R China.
EM jinglixd@mail.xidian.edu.cn
OI Qin, Haidong/0000-0002-3301-9539
FU National Natural Science Foundation of China [62172315, 62073262,
   61672429]; Fundamental Research Funds for the Central Universities;
   Innovation Fund of Xidian University [20109205456]; Key Research and
   Development Program of Shaanxi [S2021-YF-ZDCXL-ZDLGY-0127]; HUAWEI
FX This work was supported by the National Natural Science Foundation of
   China (Nos. 62172315, 62073262, and 61672429), the Fundamental Research
   Funds for the Central Universities, the Innovation Fund of Xidian
   University (No. 20109205456), the Key Research and Development Program
   of Shaanxi (No. S2021-YF-ZDCXL-ZDLGY-0127), and HUAWEI.
CR Abdel-Aziz YI, 2015, PHOTOGRAMM ENG REM S, V81, P103, DOI 10.14358/PERS.81.2.103
   Broxton M., 2020, P ACM SIGGRAPH IMM P
   Broxton M, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392485
   Carballeira P, 2022, IEEE T MULTIMEDIA, V24, P2378, DOI 10.1109/TMM.2021.3079711
   Ceulemans B, 2018, IEEE T MULTIMEDIA, V20, P2235, DOI 10.1109/TMM.2018.2802646
   Chen J, 2019, Arxiv, DOI arXiv:1908.02446
   Chen J, 2019, IEEE INT C INT ROBOT, P3209
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Cheung CH, 2021, IEEE T MULTIMEDIA, V23, P1908, DOI 10.1109/TMM.2020.3004966
   Collet A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766945
   Dellaert F, 2021, Arxiv, DOI [arXiv:2101.05204, DOI 10.48550/ARXIV.2101.05204]
   Deng KL, 2022, PROC CVPR IEEE, P12872, DOI 10.1109/CVPR52688.2022.01254
   Do L, 2012, IEEE T CONSUM ELECTR, V58, P633, DOI 10.1109/TCE.2012.6227470
   Dou MS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925969
   Flynn J, 2019, PROC CVPR IEEE, P2362, DOI 10.1109/CVPR.2019.00247
   Fridovich-Keil S, 2022, PROC CVPR IEEE, P5491, DOI 10.1109/CVPR52688.2022.00542
   Fukushima N, 2010, 3DTV CONF
   Gao XS, 2020, THIRD INTERNATIONAL CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2020), P275, DOI 10.1109/MIPR49039.2020.00064
   Hedman P, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275084
   Huang Z, 2018, LECT NOTES COMPUT SC, V11220, P351, DOI 10.1007/978-3-030-01270-0_21
   Jin J, 2016, IEEE T MULTIMEDIA, V18, P953, DOI 10.1109/TMM.2016.2539825
   Leroy V, 2021, INT J COMPUT VISION, V129, P284, DOI 10.1007/s11263-020-01377-0
   Li JW, 2022, COMPUT VIS MEDIA, V8, P369, DOI 10.1007/s41095-021-0250-8
   Li S, 2018, IEEE T MULTIMEDIA, V20, P1948, DOI 10.1109/TMM.2018.2791810
   Liu ZM, 2022, IEEE T MULTIMEDIA, V24, P451, DOI 10.1109/TMM.2021.3053401
   Lombardi S, 2021, ACM T GRAPHIC, V40, DOI [10.1145/3476576.3476608, 10.1145/3450626.3459863]
   Martin-Brualla R, 2021, PROC CVPR IEEE, P7206, DOI 10.1109/CVPR46437.2021.00713
   Meerits Siim, 2018, Computational Visual Media, V4, P287, DOI 10.1007/s41095-018-0121-0
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Natsume R, 2019, PROC CVPR IEEE, P4475, DOI 10.1109/CVPR.2019.00461
   Nonaka K, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON VISUAL COMMUNICATIONS AND IMAGE PROCESSING (IEEE VCIP)
   Nonaka K, 2018, IEICE T INF SYST, VE101D, P2381, DOI 10.1587/transinf.2018EDP7039
   Peng SD, 2021, PROC CVPR IEEE, P9050, DOI 10.1109/CVPR46437.2021.00894
   Penner E, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130855
   Sabirin H, 2018, IEEE MULTIMEDIA, V25, P61, DOI 10.1109/MMUL.2018.112142739
   Sankoh H, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1724, DOI 10.1145/3240508.3240514
   Shah S., 2018, FIELD SERVICE ROBOTI, P621, DOI [10.1007/978-3-319-67361-5_40, DOI 10.1007/978-3-319-67361-5_40]
   Shin T, 2010, 3DTV CONF
   Tian SS, 2019, IEEE T MULTIMEDIA, V21, P1235, DOI 10.1109/TMM.2018.2875307
   Wang RG, 2017, IEEE T MULTIMEDIA, V19, P1392, DOI 10.1109/TMM.2017.2654120
   Wang XJ, 2021, IEEE T MULTIMEDIA, V23, P1173, DOI 10.1109/TMM.2020.2993942
   Wang Y., 2020, Virtual Reality Intell. Hardware, V2, P247
   Watanabe Taku, 2010, Proceedings of the SICE 2010 - 49th Annual Conference of the Society of Instrument and Control Engineers of Japan, P2748
   Wei DX, 2021, IEEE T MULTIMEDIA, V23, P2457, DOI 10.1109/TMM.2020.3011290
   Yamada K, 2013, IEEE IMAGE PROC, P2072, DOI 10.1109/ICIP.2013.6738427
   Yao Q, 2016, IEEE IMAGE PROC, P1185, DOI 10.1109/ICIP.2016.7532545
   Yu A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5732, DOI 10.1109/ICCV48922.2021.00570
   Yusuke U, 2018, PROC INT WORKSH ADV
   Zheng ZR, 2022, IEEE T PATTERN ANAL, V44, P3170, DOI 10.1109/TPAMI.2021.3050505
   Zhou TH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201323
NR 51
TC 0
Z9 0
U1 6
U2 6
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD APR
PY 2024
VL 10
IS 2
BP 331
EP 353
DI 10.1007/s41095-022-0323-3
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DW8M2
UT WOS:001135210800007
OA gold
DA 2024-08-05
ER

PT J
AU Xie, HR
   Arihara, K
   Sato, S
   Miyata, K
AF Xie, Haoran
   Arihara, Keisuke
   Sato, Syuhei
   Miyata, Kazunori
TI DualSmoke: Sketch-based smoke illustration design with two-stage
   generative model
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article; Early Access
DE smoke simulation; flow control; generative adversarial network (GAN);
   finite-time lyapunov exponent (FTLE); Lagrangian coherent structure
   (LCS)
ID COHERENT STRUCTURES; SIMULATION
AB The dynamic effects of smoke are impressive in illustration design, but it is a troublesome and challenging issue for inexpert users to design smoke effects without domain knowledge of fluid simulations. In this work, we propose DualSmoke, a two-stage global-to-local generation framework for interactive smoke illustration design. In the global stage, the proposed approach utilizes fluid patterns to generate Lagrangian coherent structures from the user's hand-drawn sketches. In the local stage, detailed flow patterns are obtained from the generated coherent structure. Finally, we apply a guiding force field to the smoke simulator to produce the desired smoke illustration. To construct the training dataset, DualSmoke generates flow patterns using finite-time Lyapunov exponents of the velocity fields. The synthetic sketch data are generated from the flow patterns by skeleton extraction. Our user study verifies that the proposed design interface can provide various smoke illustration designs with good user usability. Our code is available at https://githubcom/shasph/DualSmoke.
C1 [Xie, Haoran; Arihara, Keisuke; Miyata, Kazunori] Japan Adv Inst Sci & Technol, Grad Sch Adv Sci & Technol, 1-1 Nomi, Ishikawa 9231292, Japan.
   [Sato, Syuhei] Hosei Univ, Grad Sch Comp & Informat Sci, 3-7-2 Kajino Cho, Koganei, Tokyo 1848584, Japan.
C3 Japan Advanced Institute of Science & Technology (JAIST); Hosei
   University
RP Xie, HR (corresponding author), Japan Adv Inst Sci & Technol, Grad Sch Adv Sci & Technol, 1-1 Nomi, Ishikawa 9231292, Japan.
EM xie@jaist.ac.jp
RI Xie, Haoran/ADP-8087-2022
OI Xie, Haoran/0000-0002-6926-3082
FU JAIST Research Grant; JSPS KAKENHI [JP20K19845]
FX We would like to thank the anonymous reviewers for their valuable
   comments. We thank the all participants in the user study. This work was
   supported by a JAIST Research Grant, and JSPS KAKENHI grant JP20K19845.
CR [Anonymous], 1996, USABILITY EVALUATION, DOI DOI 10.1201/9781498710411-35
   Au OKC, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360643
   Bangor A, 2009, J USABILITY STUD, V4, P114
   Bhattacharjee S, 2020, COMPUT GRAPH FORUM, V39, P757, DOI 10.1111/cgf.14024
   Brodt K, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530106
   Chu MY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459845
   Chu MY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073643
   Dobashi Y, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360693
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Ferstl F, 2010, IEEE T VIS COMPUT GR, V16, P1569, DOI 10.1109/TVCG.2010.169
   Gao FY, 2018, COMPUT GRAPH-UK, V74, P99, DOI 10.1016/j.cag.2018.05.005
   Garth C, 2007, IEEE T VIS COMPUT GR, V13, P1464, DOI 10.1109/TVCG.2007.70551
   Guay M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766893
   Guérin E, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130804
   HART S G, 1988, P139
   He Y, 2021, PROC SPIE, V11766, DOI 10.1117/12.2590760
   Hu ZY, 2019, COMPUT ANIMAT VIRT W, V30, DOI 10.1002/cav.1889
   Huang Ruoguan., 2011, Proceedings of the 2011 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA'11, P177, DOI DOI 10.1145/2019406.2019430
   Huang ZY, 2022, COMPUT VIS MEDIA, V8, P63, DOI 10.1007/s41095-021-0227-7
   Igarashi T, 1999, COMP GRAPH, P409, DOI 10.1145/311535.311602
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kazi RH, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P351, DOI 10.1145/2556288.2556987
   Kim B, 2022, COMPUT GRAPH FORUM, V41, P97, DOI 10.1111/cgf.14461
   Kim Y., 2006, P 2006 ACM SIGGRAPHE, P33
   Ladicky L, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818129
   Liqun Wu, 2013, 2013 Seventh International Conference on Image and Graphics (ICIG), P694, DOI 10.1109/ICIG.2013.141
   Nielsen MB, 2010, COMPUT GRAPH FORUM, V29, P705, DOI 10.1111/j.1467-8659.2009.01640.x
   Peng YC, 2021, PROCEEDINGS OF ACM SIGGRAPH SYMPOSIUM ON COMPUTER ANIMATION, SCA 2021, DOI 10.1145/3475946.3480960
   Pfaff T., mantaflow
   Rasmussen N., 2004, ACM SIGGRAPH/ Eurographics Symp. Comp. Anim, P193, DOI DOI 10.1145/1028523.1028549
   Ren B, 2018, COMPUT GRAPH FORUM, V37, P485, DOI 10.1111/cgf.13378
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sato S, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459846
   Selle A, 2005, ACM T GRAPHIC, V24, P910, DOI 10.1145/1073204.1073282
   Selle A, 2008, J SCI COMPUT, V35, P350, DOI 10.1007/s10915-007-9166-4
   Shadden SC, 2005, PHYSICA D, V212, P271, DOI 10.1016/j.physd.2005.10.007
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Su WC, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3203186
   Tagliasacchi A, 2012, COMPUT GRAPH FORUM, V31, P1735, DOI 10.1111/j.1467-8659.2012.03178.x
   Tang JW, 2021, COMPUT GRAPH FORUM, V40, P339, DOI 10.1111/cgf.142637
   Treuille A, 2003, ACM T GRAPHIC, V22, P716, DOI 10.1145/882262.882337
   Xie Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201304
   Xing J, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P755, DOI 10.1145/2984511.2984585
   Yan GW, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417832
   Yuan Z, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024170
   Zhu B, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024168
NR 46
TC 0
Z9 0
U1 2
U2 2
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD 2024 FEB 8
PY 2024
DI 10.1007/s41095-022-0318-0
EA FEB 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HK9T3
UT WOS:001159523000005
OA gold
DA 2024-08-05
ER

PT J
AU Yang, F
   Odashima, S
   Yamao, S
   Fujimoto, H
   Masui, S
   Jiang, S
AF Yang, Fan
   Odashima, Shigeyuki
   Yamao, Sosuke
   Fujimoto, Hiroaki
   Masui, Shoichi
   Jiang, Shan
TI A unified multi-view multi-person tracking framework
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE multi-camera multi-person tracking; pose tracking; footprint tracking;
   triangulation; spatiotemporal clustering
ID MULTITARGET
AB Despite significant developments in 3D multi-view multi-person (3D MM) tracking, current frameworks separately target footprint tracking, or pose tracking. Frameworks designed for the former cannot be used for the latter, because they directly obtain 3D positions on the ground plane via a homography projection, which is inapplicable to 3D poses above the ground. In contrast, frameworks designed for pose tracking generally isolate multi-view and multi-frame associations and may not be sufficiently robust for footprint tracking, which utilizes fewer key points than pose tracking, weakening multi-view association cues in a single frame. This study presents a unified multi-view multi-person tracking framework to bridge the gap between footprint tracking and pose tracking. Without additional modifications, the framework can adopt monocular 2D bounding boxes and 2D poses as its input to produce robust 3D trajectories for multiple persons. Importantly, multi-frame and multi-view information are jointly employed to improve association and triangulation. Our framework is shown to provide state-of-the-art performance on the Campus and Shelf datasets for 3D pose tracking, with comparable results on the WILDTRACK and MMPTRACK datasets for 3D footprint tracking.
C1 [Yang, Fan; Odashima, Shigeyuki; Yamao, Sosuke; Fujimoto, Hiroaki; Masui, Shoichi; Jiang, Shan] Fujitsu Res, Tokyo, Japan.
RP Yang, F (corresponding author), Fujitsu Res, Tokyo, Japan.
EM fan.yang@fujitsu.com; sodashima@fujitsu.com; yamao.sosuke@fujitsu.com;
   fujimoto.hiroak@fujitsu.com; masui.shoichi@fujitsu.com;
   jiang.shan@fujitsu.com
OI Yang, Fan/0000-0001-7185-5688
CR Andrew Alex M, 2001, Kybernetes
   Belagiannis V, 2016, IEEE T PATTERN ANAL, V38, P1929, DOI 10.1109/TPAMI.2015.2509986
   Belagiannis V, 2015, LECT NOTES COMPUT SC, V8925, P742, DOI 10.1007/978-3-319-16178-5_52
   Belagiannis V, 2014, PROC CVPR IEEE, P1669, DOI 10.1109/CVPR.2014.216
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Black J, 2006, IMAGE VISION COMPUT, V24, P1256, DOI 10.1016/j.imavis.2005.06.002
   Canton-Ferrer C, 2005, LECT NOTES COMPUT SC, V3515, P281
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chavdarova T, 2018, PROC CVPR IEEE, P5030, DOI 10.1109/CVPR.2018.00528
   Chen L, 2020, PROC CVPR IEEE, P3276, DOI 10.1109/CVPR42600.2020.00334
   Dong JT, 2022, IEEE T PATTERN ANAL, V44, P6981, DOI 10.1109/TPAMI.2021.3098052
   Dong JT, 2019, PROC CVPR IEEE, P7784, DOI 10.1109/CVPR.2019.00798
   Du YH, 2023, Arxiv, DOI [arXiv:2202.13514, 10.48550/ARXIV.2202.13514]
   Ershadi-Nasab S, 2018, MULTIMED TOOLS APPL, V77, P15573, DOI 10.1007/s11042-017-5133-8
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fleuret F, 2008, IEEE T PATTERN ANAL, V30, P267, DOI 10.1109/TPAMI.2007.1174
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Giancola S, 2022, PROCEEDINGS OF THE 5TH ACM INTERNATIONAL WORKSHOP ON MULTIMEDIA CONTENT ANALYSIS IN SPORTS, MMSPORTS 2022, P75, DOI [10.1145/3552437.3558545, 10.1145/3552437.3555703]
   Han XT, 2023, IEEE WINT CONF APPL, P4849, DOI 10.1109/WACV56688.2023.00484
   Hartley R, 2003, Multiple view geometry in computer vision, DOI [10.1016/S0143-8166(01)00145-2, DOI 10.1017/CBO9780511811685]
   He Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P541, DOI 10.1007/978-3-030-58580-8_32
   He YH, 2020, IEEE T IMAGE PROCESS, V29, P5191, DOI 10.1109/TIP.2020.2980070
   Iskakov K, 2019, IEEE I CONF COMP VIS, P7717, DOI 10.1109/ICCV.2019.00781
   Köhl P, 2020, IEEE COMPUT SOC CONF, P4489, DOI 10.1109/CVPRW50498.2020.00529
   Leal-Taix‚ L, 2015, Arxiv, DOI [arXiv:1504.01942, DOI 10.48550/ARXIV.1504.01942]
   Leal-Taixé L, 2012, PROC CVPR IEEE, P1987, DOI 10.1109/CVPR.2012.6247901
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Milan A, 2016, Arxiv, DOI arXiv:1603.00831
   Murtagh F, 2012, WIRES DATA MIN KNOWL, V2, P86, DOI 10.1002/widm.53
   Nguyen DMH, 2022, PROC CVPR IEEE, P8856, DOI 10.1109/CVPR52688.2022.00866
   Ohashi T, 2020, IMAGE VISION COMPUT, V104, DOI 10.1016/j.imavis.2020.104028
   Ong J, 2022, IEEE T PATTERN ANAL, V44, P2246, DOI 10.1109/TPAMI.2020.3034435
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Roberts SJ, 1997, PATTERN RECOGN, V30, P261, DOI 10.1016/S0031-3203(96)00079-9
   Sha ZJ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4488, DOI 10.1145/3394171.3414358
   Sternig S., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1689, DOI 10.1109/ICCVW.2011.6130453
   Tu H., 2020, LNCS, P197, DOI DOI 10.1007/978-3-030-58452-8
   Wen LY, 2017, INT J COMPUT VISION, V122, P313, DOI 10.1007/s11263-016-0943-0
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Yang F, 2023, IEEE WINT CONF APPL, P4788, DOI 10.1109/WACV56688.2023.00478
   Yang F, 2021, Arxiv, DOI arXiv:2007.03200
   Yang F, 2022, IMAGE VISION COMPUT, V124, DOI 10.1016/j.imavis.2022.104514
   Yang F, 2021, IMAGE VISION COMPUT, V106, DOI 10.1016/j.imavis.2020.104091
   Ye H, 2022, LECT NOTES COMPUT SC, V13666, P142, DOI 10.1007/978-3-031-20068-7_9
   You QZ, 2020, Arxiv, DOI arXiv:2003.11753
   Zeng FG, 2022, LECT NOTES COMPUT SC, V13687, P659, DOI 10.1007/978-3-031-19812-0_38
   Zhang YF, 2023, IEEE T PATTERN ANAL, V45, P2613, DOI 10.1109/TPAMI.2022.3163709
   Zhang YX, 2020, PROC CVPR IEEE, P1321, DOI 10.1109/CVPR42600.2020.00140
   Zhou XY, 2022, PROC CVPR IEEE, P8761, DOI 10.1109/CVPR52688.2022.00857
NR 51
TC 0
Z9 0
U1 7
U2 11
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD FEB
PY 2024
VL 10
IS 1
BP 137
EP 160
DI 10.1007/s41095-023-0334-8
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Z5WZ7
UT WOS:001112788200010
OA gold
DA 2024-08-05
ER

PT J
AU Li, YX
   Luo, G
   Xu, YK
   He, Y
   Zhang, FL
   Zhang, SH
AF Li, Yi-Xiao
   Luo, Guan
   Xu, Yi-Ke
   He, Yu
   Zhang, Fang-Lue
   Zhang, Song-Hai
TI AdaPIP: Adaptive picture-in-picture guidance for 360° film watching
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE 360 degrees videos; picture-in-picture (PIP); virtual reality (VR);
   visual guidance
AB 360 degrees videos enable viewers to watch freely from different directions but inevitably prevent them from perceiving all the helpful information. To mitigate this problem, picture-in-picture (PIP) guidance was proposed using preview windows to show regions of interest (ROIs) outside the current view range. We identify several drawbacks of this representation and propose a new method for 360 degrees film watching called AdaPIP. AdaPIP enhances traditional PIP by adaptively arranging preview windows with changeable view ranges and sizes. In addition, AdaPIP incorporates the advantage of arrow-based guidance by presenting circular windows with arrows attached to them to help users locate the corresponding ROIs more efficiently. We also adapted AdaPIP and Outside-In to HMD-based immersive virtual reality environments to demonstrate the usability of PIP-guided approaches beyond 2D screens. Comprehensive user experiments on 2D screens, as well as in VR environments, indicate that AdaPIP is superior to alternative methods in terms of visual experiences while maintaining a comparable degree of immersion.
C1 [Li, Yi-Xiao; Luo, Guan; Xu, Yi-Ke; Zhang, Song-Hai] Tsinghua Univ, Dept Comp Sci & Technol, Beijing, Peoples R China.
   [Li, Yi-Xiao; Luo, Guan; Xu, Yi-Ke; Zhang, Song-Hai] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRist, Beijing, Peoples R China.
   [He, Yu] Chinese Acad Sci, Technol & Engn Ctr Space Utilizat, Key Lab Space Utilizat, Beijing, Peoples R China.
   [Zhang, Fang-Lue] Victoria Univ Wellington, Wellington, New Zealand.
C3 Tsinghua University; Tsinghua University; Chinese Academy of Sciences;
   Technology & Engineering Center for Space Utilization, CAS; Victoria
   University Wellington
RP Zhang, SH (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing, Peoples R China.; Zhang, SH (corresponding author), Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRist, Beijing, Peoples R China.
EM yixiao20@mails.tsinghua.edu.cn; lg22@mails.tsinghua.edu.cn;
   xuyike@xiaomi.com; heyu@csu.ac.cn; fanglue.zhang@vuw.ac.nz;
   shz@tsinghua.edu.cn
FU National Natural Science Foundation of China [62132012]; Beijing Science
   and Technology Program [Z221100007722001]; Tsinghua-Tencent Joint
   Laboratory for Internet Innovation Technology
FX This work was supported by the National Natural Science Foundation of
   China (Project Number 62132012), the Beijing Science and Technology
   Program (Project Number Z221100007722001), and the Tsinghua-Tencent
   Joint Laboratory for Internet Innovation Technology.
CR Adcock M., 2013, P SUI, P1
   [Anonymous], 2016, Human-Harmonized Information Technology, Volume 1, DOI DOI 10.1007/978-4-431-55867-5_8
   [Anonymous], 2017, Lions 360
   AutoNavi Information Technology Co. Ltd, 2021, AUTONAVI
   Baudisch P., 2003, P CHI, P481, DOI [DOI 10.1145/642611.6426951,2, 10.1145/642611.642695]
   Cajar A, 2016, VISION RES, V127, P186, DOI 10.1016/j.visres.2016.05.008
   Corridor, 2016, 360 WIZARD BATTLE
   Cosco Adam, 2019, KNIVES
   David E, 2020, BRAIN SCI, V10, DOI 10.3390/brainsci10110841
   David EJ, 2019, J VISION, V19, DOI 10.1167/19.14.22
   Fonseca D., 2016, Proceedings of the 20th International Mindtrek Conference, P287, DOI [DOI 10.1145/2994310.2994334, 10.1145/2994310.2994334]
   Google Spotlight Stories, 2018, 360 GOOGLE DOODLESSP
   Google Spotlight Stories, 2015, GOOGLE SPOTLIGHT STO
   Google Spotlight Stories, 2016, 360 GOOGLE SPOTLIGHT
   Gustafson S, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P787
   Gustafson Sean., 2007, EXTENDED ABSTRACTS H, P2399
   iNFINITE Production, 2020, CROWD SOURCED DATA
   Iris, 2016, INVISIBLE EPISODE 5
   Kasampalis S., 2014, Broadband Multimedia Systems and Broadcasting (BMSB), P1
   Kit D, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0094362
   Larson AM, 2009, J VISION, V9, DOI 10.1167/9.10.6
   Li CL, 2016, J VISION, V16, DOI 10.1167/16.8.9
   Lin YC, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P2535, DOI 10.1145/3025453.3025757
   Lin YT, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P255, DOI 10.1145/3126594.3126656
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu SJ, 2019, PROCEEDINGS OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY (UIST 2019), P249, DOI 10.1145/3332165.3347887
   Matsuzoe S., 2017, P 8 AUGM HUM INT C, P1
   Millodot M., 2014, Dictionary of optometry and visual science
   Nuthmann A, 2013, VIS COGN, V21, P803, DOI 10.1080/13506285.2013.832449
   Pavel A, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P289, DOI 10.1145/3126594.3126636
   Rhee T, 2017, IEEE T VIS COMPUT GR, V23, P1302, DOI 10.1109/TVCG.2017.2657178
   Rothe S, 2019, MULTIMODAL TECHNOLOG, V3, DOI 10.3390/mti3010019
   Tam WJ, 1998, P SOC PHOTO-OPT INS, V3295, P226, DOI 10.1117/12.307169
   The Rock, 2016, 360 VR ADVENTURE
   Van den Broeck M, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P762, DOI 10.1145/3123266.3123347
   Zhou F, 2014, PROC CVPR IEEE, P3358, DOI 10.1109/CVPR.2014.429
NR 36
TC 0
Z9 0
U1 1
U2 1
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2024
VL 10
IS 3
BP 487
EP 503
DI 10.1007/s41095-023-0347-3
EA MAY 2024
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QO3G5
UT WOS:001216848500001
OA gold
DA 2024-08-05
ER

PT J
AU Li, YD
   Tang, M
   Yang, Y
   Tong, RF
   Yang, SC
   Li, Y
   An, BL
   Kou, QL
AF Li, Yudi
   Tang, Min
   Yang, Yun
   Tong, Ruofeng
   Yang, Shuangcai
   Li, Yao
   An, Bailin
   Kou, Qilong
TI CTSN: Predicting cloth deformation for skeleton-based characters with a
   two-stream skinning network
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE cloth deformation; learning network; skinning
ID CONTACT
AB We present a novel learning method using a two-stream network to predict cloth deformation for skeleton-based characters. The characters processed in our approach are not limited to humans, and can be other targets with skeleton-based representations such as fish or pets. We use a novel network architecture which consists of skeleton-based and mesh-based residual networks to learn the coarse features and wrinkle features forming the overall residual from the template cloth mesh. Our network may be used to predict the deformation for loose or tight-fitting clothing. The memory footprint of our network is low, thereby resulting in reduced computational requirements. In practice, a prediction for a single cloth mesh for a skeleton-based character takes about 7 ms on an nVidia GeForce RTX 3090 GPU. Compared to prior methods, our network can generate finer deformation results with details and wrinkles.
C1 [Li, Yudi; Tang, Min; Yang, Yun; Tong, Ruofeng] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310058, Peoples R China.
   [Yang, Shuangcai; Li, Yao; An, Bailin; Kou, Qilong] Aurora Studios, Tencent, Shenzhen 518057, Peoples R China.
C3 Zhejiang University; Tencent
RP Tang, M (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310058, Peoples R China.
EM drafocus@outlook.com; tang_m@zju.edu.cn; 160102543@zju.edu.cn;
   trf@zju.edu.cn; yscyang@tencent.com; leoyaoli@tencent.com;
   frankan@tencent.com; ambokou@tencent.com
RI Tang, Min/KOC-3090-2024
FU National Natural Science Foundation of China [61972341, 61972342,
   61732015]; Tencent-Zhejiang University Joint Laboratory
FX This work was supported in part by grants from the National Natural
   Science Foundation of China (61972341, 61972342, 61732015), and the
   Tencent-Zhejiang University Joint Laboratory.
CR Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Bertiche H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5451, DOI 10.1109/ICCV48922.2021.00542
   Bertiche H, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480479
   Bouaziz S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601116
   Bridson R, 2002, ACM T GRAPHIC, V21, P594, DOI 10.1145/566570.566623
   Guan P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185531
   Gundogdu E, 2022, IEEE T PATTERN ANAL, V44, P181, DOI 10.1109/TPAMI.2020.3010886
   Gundogdu E, 2019, IEEE I CONF COMP VIS, P8738, DOI 10.1109/ICCV.2019.00883
   Harmon D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360622
   Holden D, 2019, PROCEEDINGS SCA 2019: ACM SIGGRAPH/EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION, DOI 10.1145/3309486.3340245
   Kavan L, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P39
   Kingma D. P., 2014, arXiv
   Li C, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417763
   Li T, 2023, COMPUT GRAPH FORUM, V42, P231, DOI 10.1111/cgf.14651
   Li YD, 2022, COMPUT GRAPH FORUM, V41, P547, DOI 10.1111/cgf.14493
   Liu LJ, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322969
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Magnenat-Thalmann N., 1989, P GRAPHICS INTERFACE, P26, DOI DOI 10.5555/102313.102317
   Narain R, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462010
   Narain R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366171
   Pan XY, 2021, P ACM COMPUT GRAPH, V4, DOI 10.1145/3451262
   Patel Chaitanya, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7363, DOI 10.1109/CVPR42600.2020.00739
   Pfaff T, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601132
   PROVOT X, 1995, GRAPH INTER, P147
   Santesteban I, 2022, PROC CVPR IEEE, P8130, DOI 10.1109/CVPR52688.2022.00797
   Santesteban I, 2021, PROC CVPR IEEE, P11758, DOI 10.1109/CVPR46437.2021.01159
   Santesteban I, 2019, COMPUT GRAPH FORUM, V38, P355, DOI 10.1111/cgf.13643
   Shi Y., 2020, ARXIV
   Tan Q., 2021, ARXIV
   Tan QY, 2021, AAAI CONF ARTIF INTE, V35, P3913
   Tang M., 2014, ACM T GRAPHIC, V33
   Tang M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275005
   Tang M, 2016, COMPUT GRAPH FORUM, V35, P511, DOI 10.1111/cgf.12851
   Vidaurre R, 2020, COMPUT GRAPH FORUM, V39, P145, DOI 10.1111/cgf.14109
   Wang TY, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356512
   Xu Zi, 2020, arXiv
NR 36
TC 0
Z9 0
U1 3
U2 3
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2024
VL 10
IS 3
BP 471
EP 485
DI 10.1007/s41095-023-0344-6
EA APR 2024
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QO3G5
UT WOS:001205249800001
OA gold
DA 2024-08-05
ER

PT J
AU Fu, Q
   Liu, LL
   Hou, F
   He, Y
AF Fu, Qian
   Liu, Linlin
   Hou, Fei
   He, Ying
TI Hierarchical vectorization for facial images
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE face editing; vectorization; Poisson editing; color transfer;
   illumination editing; expression editing
AB The explosive growth of social media means portrait editing and retouching are in high demand. While portraits are commonly captured and stored as raster images, editing raster images is non-trivial and requires the user to be highly skilled. Aiming at developing intuitive and easy-to-use portrait editing tools, we propose a novel vectorization method that can automatically convert raster images into a 3-tier hierarchical representation. The base layer consists of a set of sparse diffusion curves (DCs) which characterize salient geometric features and low-frequency colors, providing a means for semantic color transfer and facial expression editing. The middle level encodes specular highlights and shadows as large, editable Poisson regions (PRs) and allows the user to directly adjust illumination by tuning the strength and changing the shapes of PRs. The top level contains two types of pixel-sized PRs for high-frequency residuals and fine details such as pimples and pigmentation. We train a deep generative model that can produce high-frequency residuals automatically. Thanks to the inherent meaning in vector primitives, editing portraits becomes easy and intuitive. In particular, our method supports color transfer, facial expression editing, highlight and shadow editing, and automatic retouching. To quantitatively evaluate the results, we extend the commonly used FLIP metric (which measures color and feature differences between two images) to consider illumination. The new metric, illumination-sensitive FLIP, can effectively capture salient changes in color transfer results, and is more consistent with human perception than FLIP and other quality measures for portrait images. We evaluate our method on the FFHQR dataset and show it to be effective for common portrait editing tasks, such as retouching, light editing, color transfer, and expression editing.
C1 [Fu, Qian; He, Ying] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Fu, Qian] Commonwealth Sci & Ind Res Org, Data61, Sydney 2015, Australia.
   [Liu, Linlin] Nanyang Technol Univ, Interdisciplinary Grad Sch, Singapore 639798, Singapore.
   [Liu, Linlin] Alibaba Grp, Singapore 639798, Singapore.
   [Hou, Fei] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100190, Peoples R China.
   [Hou, Fei] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
C3 Nanyang Technological University; Commonwealth Scientific & Industrial
   Research Organisation (CSIRO); Nanyang Technological University; Chinese
   Academy of Sciences; Institute of Software, CAS; Chinese Academy of
   Sciences; University of Chinese Academy of Sciences, CAS
RP He, Y (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
EM yhe@ntu.edu.sg
FU Ministry of Education, Singapore [RG20/20]; National Natural Science
   Foundation of China [61872347]; Special Plan for the Development of
   Distinguished Young Scientists of ISCAS [Y8RC535018]
FX This project was supported by the Ministry of Education, Singapore,
   under its Academic Research Fund Tier 1 (RG20/20), the National Natural
   Science Foundation of China (61872347), and the Special Plan for the
   Development of Distinguished Young Scientists of ISCAS (Y8RC535018).
CR Afifi M, 2021, PROC CVPR IEEE, P7937, DOI 10.1109/CVPR46437.2021.00785
   Andersson P, 2020, P ACM COMPUT GRAPH, V3, DOI 10.1145/3406183
   Bang D, 2021, IEEE INT CONF COMP V, P2347, DOI 10.1109/ICCVW54120.2021.00266
   Bell S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601206
   Bi S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766946
   Boyé S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366192
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen KW, 2020, IEEE T MULTIMEDIA, V22, P15, DOI 10.1109/TMM.2019.2922126
   Chen SY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459760
   Cheng Z, 2019, IEEE I CONF COMP VIS, P2521, DOI 10.1109/ICCV.2019.00261
   Dekel T, 2018, PROC CVPR IEEE, P3511, DOI 10.1109/CVPR.2018.00370
   Favreau JD, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130888
   Finch M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024200
   Fu Q, 2019, COMPUT AIDED DESIGN, V115, P111, DOI 10.1016/j.cad.2019.05.005
   Hoang TT, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207181
   Hou F, 2020, IEEE T VIS COMPUT GR, V26, P1361, DOI 10.1109/TVCG.2018.2867478
   Jo Y, 2019, IEEE I CONF COMP VIS, P1745, DOI 10.1109/ICCV.2019.00183
   Lai YK, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531391
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Leordeanu M, 2012, LECT NOTES COMPUT SC, V7575, P516, DOI 10.1007/978-3-642-33765-9_37
   Liao J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073683
   Liao ZC, 2012, IEEE T VIS COMPUT GR, V18, P1858, DOI 10.1109/TVCG.2012.76
   Lu SF, 2019, VISUAL COMPUT, V35, P1027, DOI 10.1007/s00371-019-01671-0
   Lu ZH, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1083, DOI 10.1145/3240508.3240647
   Orzan A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360691
   Portenier T, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201393
   Qizhe Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10684, DOI 10.1109/CVPR42600.2020.01070
   Sengupta S, 2018, PROC CVPR IEEE, P6296, DOI 10.1109/CVPR.2018.00659
   Shafaei A, 2021, IEEE WINT CONF APPL, P989, DOI 10.1109/WACV48630.2021.00103
   Shen HL, 2013, APPL OPTICS, V52, P4483, DOI 10.1364/AO.52.004483
   Sheng L, 2018, PROC CVPR IEEE, P8242, DOI 10.1109/CVPR.2018.00860
   Shih YC, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601137
   Shu ZX, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3095816
   Shu ZX, 2017, PROC CVPR IEEE, P5444, DOI 10.1109/CVPR.2017.578
   Soria X, 2020, IEEE WINT CONF APPL, P1912, DOI 10.1109/WACV45572.2020.9093290
   Sun J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239462
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie GF, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661275
   Zhang SH, 2009, IEEE T VIS COMPUT GR, V15, P618, DOI 10.1109/TVCG.2009.9
   Zhang XM, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3446328
   Zhao S, 2018, IEEE T VIS COMPUT GR, V24, P2153, DOI 10.1109/TVCG.2017.2721400
   Zhou HL, 2014, IEEE T IMAGE PROCESS, V23, P3268, DOI 10.1109/TIP.2014.2327807
   Zhou H, 2019, IEEE I CONF COMP VIS, P7819, DOI 10.1109/ICCV.2019.00791
   Zhou H, 2019, IEEE I CONF COMP VIS, P7193, DOI 10.1109/ICCV.2019.00729
   Zoph B, 2020, ARXIV200606882, DOI DOI 10.48550/ARXIV.2006.06882
NR 46
TC 0
Z9 0
U1 3
U2 3
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD FEB
PY 2024
VL 10
IS 1
BP 97
EP 118
DI 10.1007/s41095-022-0314-4
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Z5WZ7
UT WOS:001112788200011
OA gold
DA 2024-08-05
ER

PT J
AU Huang, YT
   Iizuka, S
   Simo-Serra, E
   Fukui, K
AF Huang, Yuantian
   Iizuka, Satoshi
   Simo-Serra, Edgar
   Fukui, Kazuhiro
TI Controllable multi-domain semantic artwork synthesis
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE semantic artwork synthesis; generative adversarial network (GAN);
   datasets; non-photorealistic rendering
ID MANIPULATION; ART
AB We present a novel framework for the multi-domain synthesis of artworks from semantic layouts. One of the main limitations of this challenging task is the lack of publicly available segmentation datasets for art synthesis. To address this problem, we propose a dataset called ArtSem that contains 40,000 images of artwork from four different domains, with their corresponding semantic label maps. We first extracted semantic maps from landscape photography and used a conditional generative adversarial network (GAN)-based approach for generating high-quality artwork from semantic maps without requiring paired training data. Furthermore, we propose an artwork-synthesis model using domain-dependent variational encoders for high-quality multi-domain synthesis. Subsequently, the model was improved and complemented with a simple but effective normalization method based on jointly normalizing semantics and style, which we call spatially style-adaptive normalization (SSTAN). Compared to the previous methods, which only take semantic layout as the input, our model jointly learns style and semantic information representation, improving the generation quality of artistic images. These results indicate that our model learned to separate the domains in the latent space. Thus, we can perform fine-grained control of the synthesized artwork by identifying hyperplanes that separate the different domains. Moreover, by combining the proposed dataset and approach, we generated user-controllable artworks of higher quality than that of existing approaches, as corroborated by quantitative metrics and a user study.
C1 [Huang, Yuantian; Iizuka, Satoshi; Fukui, Kazuhiro] Univ Tsukuba, Dept Comp Sci, Tsukuba 3058577, Japan.
   [Simo-Serra, Edgar] Waseda Univ, Dept Comp Sci & Engn, Tokyo 1698050, Japan.
C3 University of Tsukuba; Waseda University
RP Huang, YT (corresponding author), Univ Tsukuba, Dept Comp Sci, Tsukuba 3058577, Japan.
EM huang_yuantian@cvlab.cs.tsukuba.ac.jp
OI Huang, Yuantian/0000-0001-9775-7840
FU Japan Science and Technology Agency Support for Pioneering Research
   Initiated by the Next Generation (JST SPRING) [JPMJSP2124]
FX This study was supported by the Japan Science and Technology Agency
   Support for Pioneering Research Initiated by the Next Generation (JST
   SPRING) under Grant No. JPMJSP2124.
CR Gatys LA, 2016, Arxiv, DOI [arXiv:1606.05897, 10.48550/ARXIV.1606.05897, DOI 10.48550/ARXIV.1606.05897]
   [Anonymous], 2017, Proceedings of the 31st International Conference on Neural Information Processing Systems. NIPS'17, DOI DOI 10.5555/3294771.3294816
   Bingchen Liu, 2021, Computer Vision - ACCV 2020 15th Asian Conference on Computer Vision. Revised Selected Papers. Lecture Notes in Computer Science (LNCS 12647), P207, DOI 10.1007/978-3-030-69544-6_13
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen XY, 2019, IEEE T IMAGE PROCESS, V28, P546, DOI 10.1109/TIP.2018.2869695
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Cohen N, 2022, COMPUT GRAPH FORUM, V41, P261, DOI 10.1111/cgf.14473
   Dehlinger H, 2007, J MATH ARTS, V1, P97, DOI 10.1080/17513470701441445
   Dobler K, 2022, Arxiv, DOI arXiv:2202.11777
   Dumoulin V, 2017, Arxiv, DOI arXiv:1610.07629
   Elgammal A, 2017, Arxiv, DOI arXiv:1706.07068
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   He B, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1172, DOI 10.1145/3240508.3240655
   Hensel M, 2017, ADV NEUR IN, V30
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Hertzmann A, 2018, ARTS, V7, DOI 10.3390/arts7020018
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Hsin-Yu Chang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P573, DOI 10.1007/978-3-030-58598-3_34
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Champandard AJ, 2016, Arxiv, DOI arXiv:1603.01768
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Kolkin N, 2019, PROC CVPR IEEE, P10043, DOI 10.1109/CVPR.2019.01029
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kusner MJ, 2015, PR MACH LEARN RES, V37, P957
   Lee HY, 2020, INT J COMPUT VISION, V128, P2402, DOI 10.1007/s11263-019-01284-z
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li YH, 2017, Arxiv, DOI arXiv:1701.01036
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065
   McInnes L, 2020, Arxiv, DOI [arXiv:1802.03426, 10.21105/joss.00861]
   Men YF, 2018, PROC CVPR IEEE, P6353, DOI 10.1109/CVPR.2018.00665
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Park T., 2020, Advances in Neural Information Processing Systems, V33, P7198
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Phon-Amnuaisuk S, 2012, PROCEDIA COMPUT SCI, V13, P43, DOI 10.1016/j.procs.2012.09.112
   Qi XJ, 2018, PROC CVPR IEEE, P8808, DOI 10.1109/CVPR.2018.00918
   Ramesh A., 2022, Hierarchical text-conditional image generation with clip latents, DOI 10.48550/arXiv.2204.06125
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Shuyang Gu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P369, DOI 10.1007/978-3-030-58621-8_22
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sushko V, 2021, Arxiv, DOI arXiv:2012.04781
   Tan WR, 2019, IEEE T IMAGE PROCESS, V28, P394, DOI 10.1109/TIP.2018.2866698
   Tan WR, 2017, IEEE IMAGE PROC, P3760, DOI 10.1109/ICIP.2017.8296985
   Wang C, 2023, COMPUT VIS MEDIA, V9, P351, DOI 10.1007/s41095-022-0284-6
   Wang M, 2019, PROC CVPR IEEE, P1495, DOI 10.1109/CVPR.2019.00159
   Wang TC, 2018, Arxiv, DOI arXiv:1808.06601
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1007/s11263-017-1004-z, 10.1109/ICCV.2015.164]
   Xue A, 2021, IEEE WINT CONF APPL, P3862, DOI 10.1109/WACV48630.2021.00391
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Yin R., 2016, Content aware neural style transfer
   Yujun Shen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9240, DOI 10.1109/CVPR42600.2020.00926
   Zhang SY, 2019, COMPUT VIS MEDIA, V5, P105, DOI 10.1007/s41095-019-0136-1
   Zhao SY, 2021, Arxiv, DOI arXiv:2103.10428
   Zhou BL, 2019, INT J COMPUT VISION, V127, P302, DOI 10.1007/s11263-018-1140-0
   Zhou WY, 2021, COMPUT VIS MEDIA, V7, P153, DOI 10.1007/s41095-021-0203-2
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu PH, 2020, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR42600.2020.00515
   Zhu Z, 2020, PROC CVPR IEEE, P5466, DOI 10.1109/CVPR42600.2020.00551
NR 65
TC 0
Z9 0
U1 8
U2 8
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD APR
PY 2024
VL 10
IS 2
BP 355
EP 373
DI 10.1007/s41095-023-0356-2
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DW8M2
UT WOS:001135210800002
OA gold, Green Submitted
DA 2024-08-05
ER

PT J
AU Zhou, WY
   Yuan, L
   Mu, TJ
AF Zhou, Wenyang
   Yuan, Lu
   Mu, Taijiang
TI Multi3D: 3D-aware multimodal image synthesis
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article; Early Access
DE generate adversarial networks (GANs); neural radiation field (NeRF);
   3D-aware image synthesis; controllable generation
ID MANIPULATION
AB 3D-aware image synthesis has attained high quality and robust 3D consistency. Existing 3D controllable generative models are designed to synthesize 3D-aware images through a single modality, such as 2D segmentation or sketches, but lack the ability to finely control generated content, such as texture and age. In pursuit of enhancing user-guided controllability, we propose Multi3D, a 3D-aware controllable image synthesis model that supports multi-modal input. Our model can govern the geometry of the generated image using a 2D label map, such as a segmentation or sketch map, while concurrently regulating the appearance of the generated image through a textual description. To demonstrate the effectiveness of our method, we have conducted experiments on multiple datasets, including CelebAMask-HQ, AFHQ-cat, and shapenet-car. Qualitative and quantitative evaluations show that our method outperforms existing state-of-the-art methods.
C1 [Zhou, Wenyang; Mu, Taijiang] Tsinghua Univ, BNRist, Beijing 100084, Peoples R China.
   [Yuan, Lu] Stanford Univ, Comp Sci Dept, Stanford, CA 94305 USA.
C3 Tsinghua University; Stanford University
RP Mu, TJ (corresponding author), Tsinghua Univ, BNRist, Beijing 100084, Peoples R China.
EM zhouwy19@mails.tsinghua.edu.cn; luyuan@stanford.edu;
   taijiang@tsinghua.edu.cn
FU National Science and Technology Major Project [2021ZD0112902]; National
   Natural Science Foundation of China [62220106003]; Beijing Higher
   Institution Engineering Research Center
FX This paper was supported by the National Science and Technology Major
   Project (Grant No. 2021ZD0112902), the National Natural Science
   Foundation of China (Project No. 62220106003), a Research Grant from
   Beijing Higher Institution Engineering Research Center, and
   Tsinghua-Tencent Joint Laboratory for Internet Innovation Technology.
CR Brock A, 2018, P INT C LEARNING REP
   Cai SQ, 2022, PROC CVPR IEEE, P3971, DOI 10.1109/CVPR52688.2022.00395
   Chan ER, 2022, PROC CVPR IEEE, P16102, DOI 10.1109/CVPR52688.2022.01565
   Chan ER, 2021, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR46437.2021.00574
   Chang A.X., 2015, ArXiv
   Chen AP, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3470848
   Chen SY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459760
   Chen SY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392386
   Chen Y. D., COMPUTER VISIONECCV, V13674, P730
   Deng KL, 2023, PROC CVPR IEEE, P4434, DOI 10.1109/CVPR52729.2023.00431
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   Deng Y, 2022, PROC CVPR IEEE, P10663, DOI 10.1109/CVPR52688.2022.01041
   Fridovich-Keil S, 2022, PROC CVPR IEEE, P5491, DOI 10.1109/CVPR52688.2022.00542
   Gao L, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592100
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gu J, 2022, P INT C LEARNING REP
   Hensel M, 2017, ADV NEUR IN, V30
   Huang ZY, 2022, COMPUT VIS MEDIA, V8, P63, DOI 10.1007/s41095-021-0227-7
   Huang ZQ, 2023, PROC CVPR IEEE, P6080, DOI 10.1109/CVPR52729.2023.00589
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang K, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555377
   Jiang KW, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3597300
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2021, ADV NEUR IN, V34
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kingma D. P., 2014, arXiv
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Ling H., 2021, Advances in Neural Information Processing Systems (NIPS), P16331
   Liu XT, 2022, COMPUT VIS MEDIA, V8, P135, DOI 10.1007/s41095-021-0228-6
   Loshchilov I., 2018, INT C LEARN REPR
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Niemeyer M, 2021, PROC CVPR IEEE, P11448, DOI 10.1109/CVPR46437.2021.01129
   OrEl R, 2022, PROC CVPR IEEE, P13493, DOI 10.1109/CVPR52688.2022.01314
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Patashnik O, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2065, DOI 10.1109/ICCV48922.2021.00209
   Radford A, 2021, PR MACH LEARN RES, V139
   Shi YC, 2022, PROC CVPR IEEE, P11244, DOI 10.1109/CVPR52688.2022.01097
   Sun JX, 2023, PROC CVPR IEEE, P20991, DOI 10.1109/CVPR52729.2023.02011
   Sun JX, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555506
   Sun JX, 2022, PROC CVPR IEEE, P7662, DOI 10.1109/CVPR52688.2022.00752
   Sun RQ, 2021, COMPUT VIS MEDIA, V7, P363, DOI 10.1007/s41095-021-0219-7
   Sushko V, 2022, INT J COMPUT VISION, V130, P2903, DOI 10.1007/s11263-022-01673-x
   Tang Junshu, 2024, IEEE Trans Vis Comput Graph, V30, P6020, DOI 10.1109/TVCG.2023.3323578
   Wang C, 2023, COMPUT VIS MEDIA, V9, P351, DOI 10.1007/s41095-022-0284-6
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Xia WH, 2021, PROC CVPR IEEE, P2256, DOI 10.1109/CVPR46437.2021.00229
   Xiang JF, 2023, IEEE I CONF COMP VIS, P2195, DOI 10.1109/ICCV51070.2023.00209
   Xue Y, 2022, COMPUT VIS MEDIA, V8, P3, DOI 10.1007/s41095-021-0234-8
   Yu A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5732, DOI 10.1109/ICCV48922.2021.00570
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhou WY, 2024, IEEE T VIS COMPUT GR, V30, P5437, DOI 10.1109/TVCG.2023.3293653
   Zhou WY, 2021, COMPUT VIS MEDIA, V7, P153, DOI 10.1007/s41095-021-0203-2
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu P., 2020, ARXIV
   Zhu PH, 2020, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR42600.2020.00515
NR 56
TC 0
Z9 0
U1 6
U2 6
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD 2024 APR 3
PY 2024
DI 10.1007/s41095-024-0422-4
EA APR 2024
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MT0G2
UT WOS:001195762300001
OA gold
DA 2024-08-05
ER

PT J
AU Wang, Y
   Li, YK
   Elder, JH
   Wu, RM
   Lu, HC
AF Wang, Yue
   Li, Yuke
   Elder, James H.
   Wu, Runmin
   Lu, Huchuan
TI Class-conditional domain adaptation for semantic segmentation
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE domain adaptation; generative adversarial networks; semantic
   segmentation; cityscapes
AB Semantic segmentation is an important sub-task for many applications. However, pixel-level ground-truth labeling is costly, and there is a tendency to overfit to training data, thereby limiting the generalization ability. Unsupervised domain adaptation can potentially address these problems by allowing systems trained on labelled datasets from the source domain (including less expensive synthetic domain) to be adapted to a novel target domain. The conventional approach involves automatic extraction and alignment of the representations of source and target domains globally. One limitation of this approach is that it tends to neglect the differences between classes: representations of certain classes can be more easily extracted and aligned between the source and target domains than others, limiting the adaptation over all classes. Here, we address this problem by introducing a Class-Conditional Domain Adaptation (CCDA) method. This incorporates a class-conditional multi-scale discriminator and class-conditional losses for both segmentation and adaptation. Together, they measure the segmentation, shift the domain in a class-conditional manner, and equalize the loss over classes. Experimental results demonstrate that the performance of our CCDA method matches, and in some cases, surpasses that of state-of-the-art methods.
C1 [Wang, Yue; Lu, Huchuan] Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116024, Peoples R China.
   [Li, Yuke] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
   [Elder, James H.] York Univ, Dept Elect Engn & Comp Sci, Toronto, ON M3J 1P3, Canada.
   [Wu, Runmin] Univ Hong Kong, Dept Comp Sci, Hong Kong 999077, Peoples R China.
C3 Dalian University of Technology; Wuhan University; York University -
   Canada; University of Hong Kong
RP Lu, HC (corresponding author), Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116024, Peoples R China.
EM ellabear@mail.dlut.edu.cn; sunfreshing@whu.edu.cn; jelder@yorku.ca;
   rmwu@cs.hku.hk; lhchuan@dlut.edu.cn
FU York University Vision: Science to Applications - Ontario Research
   Fund-Research Excellence program
FX We would like to thank the York University Vision: Science to
   Applications (VISTA) program and Intelligent Systems for Sustainable
   Urban Mobility (ISSUM) project, funded by the Ontario Research
   Fund-Research Excellence program for their supports.
CR Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Y., 2017, P IEEE INT C COMPUTE, P1
   Cheng YT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9062, DOI 10.1109/ICCV48922.2021.00895
   Choi J, 2019, IEEE I CONF COMP VIS, P6829, DOI 10.1109/ICCV.2019.00693
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Du L, 2019, IEEE I CONF COMP VIS, P982, DOI 10.1109/ICCV.2019.00107
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Geng B, 2011, IEEE T IMAGE PROCESS, V20, P2980, DOI 10.1109/TIP.2011.2134107
   Gong LX, 2022, COMPUT VIS MEDIA, V8, P165, DOI 10.1007/s41095-021-0235-7
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Harms J, 2019, MED PHYS, V46, P3998, DOI 10.1002/mp.13656
   Hoffman J, 2016, Arxiv, DOI arXiv:1612.02649
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Ji GP, 2023, COMPUT VIS MEDIA, V9, P155, DOI 10.1007/s41095-021-0262-4
   Kang JX, 2021, IEEE INT SYMP PARAL, P509, DOI 10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00076
   Kingma D.P., 2014, Proc. of ICLR
   Klingner M, 2022, IEEE WINT C APPL COM, P210, DOI 10.1109/WACVW54805.2022.00027
   Lan YQ, 2022, COMPUT VIS MEDIA, V8, P395, DOI 10.1007/s41095-021-0252-6
   Li YS, 2019, PROC CVPR IEEE, P6929, DOI 10.1109/CVPR.2019.00710
   Liang TT, 2022, IEEE T IMAGE PROCESS, V31, P6893, DOI 10.1109/TIP.2022.3216771
   Liu Y, 2020, IEEE T IMAGE PROCESS, V29, P5206, DOI 10.1109/TIP.2020.2980170
   Liu Y, 2021, PROC CVPR IEEE, P1215, DOI 10.1109/CVPR46437.2021.00127
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo YW, 2022, IEEE T PATTERN ANAL, V44, P3940, DOI 10.1109/TPAMI.2021.3064379
   Luo YW, 2019, IEEE I CONF COMP VIS, P6777, DOI 10.1109/ICCV.2019.00688
   Maas A.L., P 30 INT C MACH LEAR
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Myeongjin Kim, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12972, DOI 10.1109/CVPR42600.2020.01299
   Nie D, 2018, LECT NOTES COMPUT SC, V11073, P370, DOI 10.1007/978-3-030-00937-3_43
   Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7
   Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352
   Rozantsev A, 2019, IEEE T PATTERN ANAL, V41, P801, DOI 10.1109/TPAMI.2018.2814042
   Shan YH, 2020, NEUROCOMPUTING, V380, P125, DOI 10.1016/j.neucom.2019.11.008
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun BC, 2016, AAAI CONF ARTIF INTE, P2058
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Toldo M, 2021, IEEE WINT CONF APPL, P1357, DOI 10.1109/WACV48630.2021.00140
   Tsai YH, 2019, IEEE I CONF COMP VIS, P1456, DOI 10.1109/ICCV.2019.00154
   Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780
   Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8
   Wong KCL, 2018, LECT NOTES COMPUT SC, V11072, P612, DOI 10.1007/978-3-030-00931-1_70
   Yang JH, 2020, AAAI CONF ARTIF INTE, V34, P12613
   Yao T, 2015, PROC CVPR IEEE, P2142, DOI 10.1109/CVPR.2015.7298826
   You MY, 2023, PATTERN RECOGN, V133, DOI 10.1016/j.patcog.2022.109023
   Zhang XH, 2021, IEEE INT CONF ROBOT, P13560, DOI 10.1109/ICRA48506.2021.9560785
   Zhang XH, 2022, IEEE T INTELL TRANSP, V23, P9529, DOI 10.1109/TITS.2022.3140481
   Zhang Y, 2020, IEEE T PATTERN ANAL, V42, P1823, DOI 10.1109/TPAMI.2019.2903401
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou W, 2021, IEEE T IMAGE PROCESS, V30, P2549, DOI 10.1109/TIP.2020.3018221
   Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI [10.1007/978-3-030-01219-9_, 10.1007/978-3-030-01219-9_18]
NR 55
TC 0
Z9 0
U1 7
U2 7
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD JUN
PY 2024
VL 10
IS 3
BP 425
EP 438
DI 10.1007/s41095-023-0362-4
EA MAR 2024
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QO3G5
UT WOS:001190323500003
OA gold, Green Submitted
DA 2024-08-05
ER

PT J
AU Wang, WJ
   Liu, X
   Zhou, HL
   Wei, L
   Deng, ZG
   Murshed, M
   Lu, XQ
AF Wang, Weijia
   Liu, Xiao
   Zhou, Hailing
   Wei, Lei
   Deng, Zhigang
   Murshed, Manzur
   Lu, Xuequan
TI Noise4Denoise: Leveraging noise for unsupervised point cloud denoising
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article; Early Access
DE point clouds; denoising; geometry processing; deep learning
AB Existing deep learning-based point cloud denoising methods are generally trained in a supervised manner that requires clean data as ground-truth labels. However, in practice, it is not always feasible to obtain clean point clouds. In this paper, we introduce a novel unsupervised point cloud denoising method that eliminates the need to use clean point clouds as groundtruth labels during training. We demonstrate that it is feasible for neural networks to only take noisy point clouds as input, and learn to approximate and restore their clean versions. In particular, we generate two noise levels for the original point clouds, requiring the second noise level to be twice the amount of the first noise level. With this, we can deduce the relationship between the displacement information that recovers the clean surfaces across the two levels of noise, and thus learn the displacement of each noisy point in order to recover the corresponding clean point. Comprehensive experiments demonstrate that our method achieves outstanding denoising results across various datasets with synthetic and real-world noise, obtaining better performance than previous unsupervised methods and competitive performance to current supervised methods.
C1 [Wang, Weijia; Liu, Xiao; Wei, Lei; Murshed, Manzur] Deakin Univ, Geelong, Vic 3216, Australia.
   [Zhou, Hailing] Swinburne Univ Technol, Dept Mech Engn & Prod Design Engn, Melbourne, Vic 3122, Australia.
   [Deng, Zhigang] Univ Houston, Dept Comp Sci, Houston, TX 77204 USA.
   [Lu, Xuequan] Trobe Univ, Dept Comp Sci & Informat Technol, Melbourne, Vic 3086, Australia.
C3 Deakin University; Swinburne University of Technology; University of
   Houston System; University of Houston; La Trobe University
RP Lu, XQ (corresponding author), Trobe Univ, Dept Comp Sci & Informat Technol, Melbourne, Vic 3086, Australia.
EM wangweijia@deakin.edu.au; xiao.liu@deakin.edu.au;
   hailing.zhou@hotmail.com; lei.wei@deakin.edu.au; zdeng4@central.uh.edu;
   m.murshed@deakin.edu.au; b.lu@latrobe.edu.au
CR Alexa M, 2003, IEEE T VIS COMPUT GR, V9, P3, DOI 10.1109/TVCG.2003.1175093
   Alexa M, 2001, IEEE VISUAL, P21, DOI 10.1109/VISUAL.2001.964489
   Amenta N, 2004, ACM T GRAPHIC, V23, P264, DOI 10.1145/1015706.1015713
   Batson J, 2019, PR MACH LEARN RES, V97
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952
   Cazals F, 2005, COMPUT AIDED GEOM D, V22, P121, DOI 10.1016/j.cagd.2004.09.004
   Chen SJ, 2023, COMPUT VIS MEDIA, V9, P249, DOI 10.1007/s41095-022-0278-4
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Edirimuni DD, 2023, PROC CVPR IEEE, P13530, DOI 10.1109/CVPR52729.2023.01300
   Fleishman S, 2005, ACM T GRAPHIC, V24, P544, DOI 10.1145/1073204.1073227
   Fleishman S, 2003, ACM T GRAPHIC, V22, P950, DOI 10.1145/882262.882368
   Guerrero P, 2018, COMPUT GRAPH FORUM, V37, P75, DOI 10.1111/cgf.13343
   Hermosilla P, 2019, IEEE I CONF COMP VIS, P52, DOI 10.1109/ICCV.2019.00014
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421645
   Huang H, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618522
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Krull A, 2019, PROC CVPR IEEE, P2124, DOI 10.1109/CVPR.2019.00223
   Lehtinen J, 2018, PR MACH LEARN RES, V80
   Levin D, 1998, MATH COMPUT, V67, P1517, DOI 10.1090/S0025-5718-98-00974-0
   Li Qing, 2022, Advances in Neural Information Processing Systems, V35, P4218
   Lipman Y, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276405, 10.1145/1239451.1239473]
   Lu DN, 2020, COMPUT AIDED DESIGN, V125, DOI 10.1016/j.cad.2020.102860
   Lu XQ, 2018, IEEE T VIS COMPUT GR, V24, P2315, DOI 10.1109/TVCG.2017.2725948
   Luo ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4563, DOI 10.1109/ICCV48922.2021.00454
   Luo ST, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1330, DOI 10.1145/3394171.3413727
   Mao AH, 2022, LECT NOTES COMPUT SC, V13663, P398, DOI 10.1007/978-3-031-20062-5_23
   Moran Nick, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12061, DOI 10.1109/CVPR42600.2020.01208
   Öztireli AC, 2009, COMPUT GRAPH FORUM, V28, P493, DOI 10.1111/j.1467-8659.2009.01388.x
   Preiner R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601172
   Rakotosaona MJ, 2020, COMPUT GRAPH FORUM, V39, P185, DOI 10.1111/cgf.13753
   Serna Andres, 2014, 3rd International Conference on Pattern Recognition Applications and Methods (ICPRAM 2014). Proceedings, P819
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984
   Wang JX, 2022, COMPUT AIDED DESIGN, V144, DOI 10.1016/j.cad.2021.103162
   Wang PS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980232
   Wang WJ, 2023, COMPUT GRAPH-UK, V116, P64, DOI 10.1016/j.cag.2023.08.013
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295
   Zhang DB, 2021, IEEE T VIS COMPUT GR, V27, P2015, DOI 10.1109/TVCG.2020.3027069
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
NR 42
TC 0
Z9 0
U1 1
U2 1
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD 2024 JUN 14
PY 2024
DI 10.1007/s41095-024-0423-3
EA JUN 2024
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UH1U9
UT WOS:001247082800001
OA gold
DA 2024-08-05
ER

PT J
AU Li, XX
   Zheng, YZ
   Ma, HK
   Qi, Z
   Meng, XX
   Meng, L
AF Li, Xiangxian
   Zheng, Yuze
   Ma, Haokai
   Qi, Zhuang
   Meng, Xiangxu
   Meng, Lei
TI Cross-modal learning using privileged information for long-tailed image
   classification
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article; Early Access
DE long-tailed classification; cross-modal learning; representation
   learning; privileged information
AB The prevalence of long-tailed distributions in real-world data often results in classification models favoring the dominant classes, neglecting the less frequent ones. Current approaches address the issues in long-tailed image classification by rebalancing data, optimizing weights, and augmenting information. However, these methods often struggle to balance the performance between dominant and minority classes because of inadequate representation learning of the latter. To address these problems, we introduce descriptional words into images as cross-modal privileged information and propose a cross-modal enhanced method for long-tailed image classification, referred to as CMLTNet. CMLTNet improves the learning of intraclass similarity of tail-class representations by cross-modal alignment and captures the difference between the head and tail classes in semantic space by cross-modal inference. After fusing the above information, CMLTNet achieved an overall performance that was better than those of benchmark long-tailed and cross-modal learning methods on the long-tailed cross-modal datasets, NUS-WIDE and VireoFood-172. The effectiveness of the proposed modules was further studied through ablation experiments. In a case study of feature distribution, the proposed model was better in learning representations of tail classes, and in the experiments on model attention, CMLTNet has the potential to help learn some rare concepts in the tail class through mapping to the semantic space.
C1 [Li, Xiangxian; Zheng, Yuze; Ma, Haokai; Qi, Zhuang; Meng, Xiangxu; Meng, Lei] Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
C3 Shandong University
RP Meng, L (corresponding author), Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
EM xiangxianlee@mail.sdu.edu.cn; zhengyuze@mail.sdu.edu.cn;
   mahaokai@mail.sdu.edu.cn; zqi@mail.sdu.edu.cn; mxx@sdu.edu.cn;
   lmeng@sdu.edu.cn
FU National Natural Science Foundation of China [62006141]; National Key
   R&D Program of China [2021YFC3300203]; Overseas Innovation Team Project
   of the "20 Regulations for New Universities" Funding Program of Jinan
   [2021GXRC073]; Excellent Youth Scholars Program of Shandong Province
   [2022HWYQ-048]
FX This work was supported in part by the National Natural Science
   Foundation of China (62006141), the National Key R&D Program of China
   (2021YFC3300203), the Overseas Innovation Team Project of the "20
   Regulations for New Universities" Funding Program of Jinan
   (2021GXRC073), and the Excellent Youth Scholars Program of Shandong
   Province (2022HWYQ-048).
CR Boyan Zhou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9716, DOI 10.1109/CVPR42600.2020.00974
   Cao KD, 2019, ADV NEUR IN, V32
   Chen JJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1771, DOI 10.1145/3123266.3123428
   Chen JJ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P32, DOI 10.1145/2964284.2964315
   Chen ZT, 2023, Arxiv, DOI arXiv:2308.04142
   Chua TS, 2009, P ACM INT C IM VID R, P1
   Cui JQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P695, DOI 10.1109/ICCV48922.2021.00075
   Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Gao JX, 2023, IEEE T MULTIMEDIA, V25, P4764, DOI 10.1109/TMM.2022.3181789
   George A, 2021, PROC CVPR IEEE, P7878, DOI 10.1109/CVPR46437.2021.00779
   Guo H, 2021, PROC CVPR IEEE, P15084, DOI 10.1109/CVPR46437.2021.01484
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong Y, 2022, LECT NOTES COMPUT SC, V13684, P587, DOI 10.1007/978-3-031-20053-3_34
   Hsin-Ping Chou, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12540), P95, DOI 10.1007/978-3-030-65414-6_9
   Jaehyung Kim, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13893, DOI 10.1109/CVPR42600.2020.01391
   Jiang SQ, 2020, IEEE T IMAGE PROCESS, V29, P265, DOI 10.1109/TIP.2019.2929447
   Kang B., 2020, 8 INT C LEARN REPR I
   Kang Bingyi, 2021, INT C LEARN REPR
   Li S, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3866, DOI 10.1145/3394171.3413995
   Li TH, 2022, PROC CVPR IEEE, P6908, DOI 10.1109/CVPR52688.2022.00679
   Li X., 2021, P 1 INT WORKSH ADV L, P1
   Li XY, 2021, AAAI CONF ARTIF INTE, V35, P1966
   Liu JL, 2020, PROC CVPR IEEE, P2967, DOI 10.1109/CVPR42600.2020.00304
   Liu ZW, 2019, PROC CVPR IEEE, P2532, DOI 10.1109/CVPR.2019.00264
   Liuyu Xiang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P247, DOI 10.1007/978-3-030-58558-7_15
   Ma HK, 2023, IEEE IJCNN, DOI 10.1109/IJCNN54540.2023.10191979
   Mahajan D, 2018, LECT NOTES COMPUT SC, V11206, P185, DOI 10.1007/978-3-030-01216-8_12
   Martinel N, 2018, IEEE WINT CONF APPL, P567, DOI 10.1109/WACV.2018.00068
   Meng L, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3460, DOI 10.1145/3394171.3413598
   Meng L, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P557, DOI 10.1145/3343031.3350870
   Min WQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1331, DOI 10.1145/3343031.3350948
   Park S, 2022, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR52688.2022.00676
   Peng Chu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P694, DOI 10.1007/978-3-030-58526-6_41
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren J., 2020, P 34 INT C NEUR INF
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Tang JH, 2017, IEEE T PATTERN ANAL, V39, P1662, DOI 10.1109/TPAMI.2016.2608882
   Tang JH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2998574
   Tang K., 2020, P NIPS, P1513
   Tong Wu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P162, DOI 10.1007/978-3-030-58548-8_10
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Vapnik V, 2015, J MACH LEARN RES, V16, P2023
   Vapnik V, 2009, NEURAL NETWORKS, V22, P544, DOI 10.1016/j.neunet.2009.06.042
   Wang Yuqing, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13806), P530, DOI 10.1007/978-3-031-25075-0_36
   Wang YR, 2019, IEEE I CONF COMP VIS, P5016, DOI 10.1109/ICCV.2019.00512
   Zagoruyko S, 2017, Arxiv, DOI arXiv:1605.07146
   Zhang YS, 2021, AAAI CONF ARTIF INTE, V35, P3447
NR 50
TC 1
Z9 1
U1 2
U2 2
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD 2024 JUN 10
PY 2024
DI 10.1007/s41095-023-0382-0
EA JUN 2024
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TQ4V8
UT WOS:001242722000001
OA gold
DA 2024-08-05
ER

PT J
AU Wang, X
   Yan, JK
   Cai, JY
   Deng, JH
   Qin, Q
   Cheng, Y
AF Wang, Xin
   Yan, Jing-Ke
   Cai, Jing-Ye
   Deng, Jian-Hua
   Qin, Qin
   Cheng, Yao
TI Super-resolution reconstruction of single image for latent features
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article; Early Access
DE image superresolution reconstruction; denoising diffusion probabilistic
   model; normalized flow; adversarial neural network; variational
   auto-encoder
AB Single-image super-resolution (SISR) typically focuses on restoring various degraded low-resolution (LR) images to a single high-resolution (HR) image. However, during SISR tasks, it is often challenging for models to simultaneously maintain high quality and rapid sampling while preserving diversity in details and texture features. This challenge can lead to issues such as model collapse, lack of rich details and texture features in the reconstructed HR images, and excessive time consumption for model sampling. To address these problems, this paper proposes a Latent Feature-oriented Diffusion Probability Model (LDDPM). First, we designed a conditional encoder capable of effectively encoding LR images, reducing the solution space for model image reconstruction and thereby improving the quality of the reconstructed images. We then employed a normalized flow and multimodal adversarial training, learning from complex multimodal distributions, to model the denoising distribution. Doing so boosts the generative modeling capabilities within a minimal number of sampling steps. Experimental comparisons of our proposed model with existing SISR methods on mainstream datasets demonstrate that our model reconstructs more realistic HR images and achieves better performance on multiple evaluation metrics, providing a fresh perspective for tackling SISR tasks.
C1 [Wang, Xin; Cai, Jing-Ye; Deng, Jian-Hua] Univ Elect Sci & Technol China, Sch Informat & Software Engn, Chengdu 610000, Peoples R China.
   [Yan, Jing-Ke; Cheng, Yao] Southwest Jiaotong Univ, State Key Lab Rail Transit Vehicle Syst, Chengdu 610000, Peoples R China.
   [Wang, Xin; Yan, Jing-Ke; Qin, Qin] Guilin Univ Elect Technol, Sch Marine Engn, Guilin 541000, Peoples R China.
   [Wang, Xin] Guilin Univ Elect Technol, Sch Comp & Informat Secur, Guilin 541000, Peoples R China.
C3 University of Electronic Science & Technology of China; Southwest
   Jiaotong University; Guilin University of Electronic Technology; Guilin
   University of Electronic Technology
RP Cai, JY (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Software Engn, Chengdu 610000, Peoples R China.; Yan, JK (corresponding author), Southwest Jiaotong Univ, State Key Lab Rail Transit Vehicle Syst, Chengdu 610000, Peoples R China.; Yan, JK (corresponding author), Guilin Univ Elect Technol, Sch Marine Engn, Guilin 541000, Peoples R China.
EM yanjinke@my.swjtu.edu.cn; jycai@uestc.edu.cn
FU General Project of Guangxi Science and Technology Major Project
   [AA19254016]; Beihai City Science and Technology Planning Project
   [202082023]; Guangxi Graduate Student Innovation Project [YCSW2021174]
FX This work is supported by General Project of Guangxi Science and
   Technology Major Project (AA19254016), Beihai City Science and
   Technology Planning Project (202082033), Beihai City Science and
   Technology Planning Project (202082023), and Guangxi Graduate Student
   Innovation Project (YCSW2021174).
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Brock A., 2019, P INT C LEARN REPR
   Cao B, 2020, AAAI CONF ARTIF INTE, V34, P10486
   Chan KCK, 2021, PROC CVPR IEEE, P14240, DOI 10.1109/CVPR46437.2021.01402
   Chen SJ, 2023, COMPUT VIS MEDIA, V9, P249, DOI 10.1007/s41095-022-0278-4
   Cheng L, 2022, IEEE T IMAGE PROCESS, V31, P2529, DOI 10.1109/TIP.2022.3157149
   Cheng Ma, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7766, DOI 10.1109/CVPR42600.2020.00779
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Gatopoulos I, 2020, Arxiv, DOI arXiv:2006.05218
   Gu SY, 2022, PROC CVPR IEEE, P10686, DOI 10.1109/CVPR52688.2022.01043
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Jiang DQ, 2023, COMPUT VIS MEDIA, V9, P279, DOI 10.1007/s41095-022-0286-4
   Jo Y, 2021, IEEE COMPUT SOC CONF, P364, DOI 10.1109/CVPRW53098.2021.00046
   Karimi N, 2021, SIGNAL PROCESS-IMAGE, V90, DOI 10.1016/j.image.2020.116061
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kim D., 2019, Proceedings of the 30th British Machine Vision Conference
   Kim Jaehyeon, 2021, P MACHINE LEARNING R, V139
   Kingma D P., 2018, P ADV NEUR INF PROC, P10236
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li HY, 2022, NEUROCOMPUTING, V479, P47, DOI 10.1016/j.neucom.2022.01.029
   Li WB, 2022, AAAI CONF ARTIF INTE, P1412
   Li WB, 2022, Arxiv, DOI arXiv:2112.10175
   Liang DW, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P689, DOI 10.1145/3178876.3186150
   Liang JY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4056, DOI 10.1109/ICCV48922.2021.00404
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu Z., Large-scale celebfaces attributes (celeba) dataset
   Liu ZS, 2021, IEEE COMPUT SOC CONF, P516, DOI 10.1109/CVPRW53098.2021.00063
   Liu ZS, 2021, IEEE T CIRC SYST VID, V31, P1351, DOI 10.1109/TCSVT.2020.3003832
   Liu ZY, 2022, IEEE T CIRC SYST VID, V32, P7418, DOI 10.1109/TCSVT.2022.3188433
   Loshchilov I., 2018, INT C LEARN REPR
   Lugmayr Andreas, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P715, DOI 10.1007/978-3-030-58558-7_42
   Ma TS, 2021, VISUAL COMPUT, V37, P925, DOI 10.1007/s00371-020-01843-3
   Mei YQ, 2021, PROC CVPR IEEE, P3516, DOI 10.1109/CVPR46437.2021.00352
   Miyato T, 2018, CoRR
   Nichol P., 2021, Improved denoising diffusion probabilistic models, P8162, DOI DOI 10.48550/ARXIV.2102.09672
   Niu B., 2020, Lect. Notes Comput. Sci, V12357, P191, DOI [DOI 10.1007/978-3-030-58610-212, 10.1007/978-3-030-58610-2_47]
   Park DY, 2019, PROC CVPR IEEE, P5873, DOI 10.1109/CVPR.2019.00603
   Parmar G, 2021, PROC CVPR IEEE, P823, DOI 10.1109/CVPR46437.2021.00088
   Qin Q, 2021, IEEE ACCESS, V9, P119881, DOI 10.1109/ACCESS.2021.3108516
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Ryu D., 2022, arXiv
   Saharia C, 2023, IEEE T PATTERN ANAL, V45, P4713, DOI 10.1109/TPAMI.2022.3204461
   Shi Y, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3193441
   SINHA A., 2021, Advances in Neural Information Processing Systems, V34, P12533
   Song Y., 2020, Adv Neural Inf Process Syst, V33, P12438
   Song Yang, 2019, Advances in Neural Information Processing Systems, V32
   Vahdat A., 2020, Proceedings of the 34th International Conference on Neural Information Processing Systems. NIPS'20, Vvol 33, P19667, DOI [DOI 10.5555/3495724.3497374, 10.48550/arXiv.2007.03898]
   Wang MH, 2022, IEEE T IND INFORM, V18, P6865, DOI 10.1109/TII.2021.3139895
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Wang Y., 2022, Zero-shot image restoration using denoising diffusion null-space model
   Xia B, 2023, IEEE I CONF COMP VIS, P13049, DOI 10.1109/ICCV51070.2023.01204
   Xiang XJ, 2023, IEEE T CIRC SYST VID, V33, P16, DOI 10.1109/TCSVT.2021.3130147
   Xiao Z., 2022, P 10 INT C LEARNING
   Yan JK, 2024, ENG APPL ARTIF INTEL, V133, DOI 10.1016/j.engappai.2024.108496
   Yan JK, 2024, IEEE T EM TOP COMP I, V8, P2827, DOI 10.1109/TETCI.2024.3377728
   Zhang DF, 2023, Arxiv, DOI arXiv:2208.11247
   Zhang JQ, 2022, IEEE T CIRC SYST VID, V32, P1020, DOI 10.1109/TCSVT.2021.3071191
   Zhang K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4771, DOI 10.1109/ICCV48922.2021.00475
   Zhang K, 2020, PROC CVPR IEEE, P3214, DOI 10.1109/CVPR42600.2020.00328
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhou HQ, 2021, IEEE COMPUT SOC CONF, P373, DOI 10.1109/CVPRW53098.2021.00047
   Zhou S., 2020, Advances in Neural Information Processing Systems, V33, P3499
NR 64
TC 1
Z9 1
U1 4
U2 4
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD 2024 MAY 24
PY 2024
DI 10.1007/s41095-023-0387-8
EA MAY 2024
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SP9I8
UT WOS:001235768300001
OA Green Submitted, gold
DA 2024-08-05
ER

PT J
AU Zhang, Q
   Thamizharasan, V
   Tompkin, J
AF Zhang, Qian
   Thamizharasan, Vikas
   Tompkin, James
TI Learning physically based material and lighting decompositions for face
   editing
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE intrinsic decomposition; portrait relighting; inverse rendering; deep
   learning
AB Lighting is crucial for portrait photography, yet the complex interactions between the skin and incident light are expensive to model computationally in graphics and difficult to reconstruct analytically via computer vision. Alternatively, to allow fast and controllable reflectance and lighting editing, we developed a physically based decomposition through deep learned priors from path-traced portrait images. Previous approaches that used simplified material models or low-frequency or low-dynamic-range lighting struggled to model specular reflections or relight directly without intermediate decomposition. However, we estimate the surface normal, skin albedo and roughness, and high-frequency HDRI maps, and propose an architecture to estimate both diffuse and specular reflectance components. In our experiments, we show that this approach can represent the true appearance function more effectively than simpler baseline methods, leading to better generalization and higher-quality editing.
C1 [Zhang, Qian; Thamizharasan, Vikas; Tompkin, James] Brown Univ, Providence, RI 02906 USA.
C3 Brown University
RP Zhang, Q (corresponding author), Brown Univ, Providence, RI 02906 USA.
EM qian_zhang@brown.edu; vikasthamizharasan@alumni.brown.edu;
   jamestompkin@brown.edu
FU NSF [CAREER-2144956]
FX We thank the reviewers for their valuable feedback. Qian Zhang thanks
   Kai Wang, Aaron Gokaslan, and Xianghao Xu for their initial exploration.
   Finally, we thank Chloe LeGendre, Henrique Weber, and Kalyan Sunkavalli
   for their fruitful discussions. This work was supported by NSF
   CAREER-2144956 and an Andy van Dam PhD Fellowship.
CR Basri R, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P383, DOI 10.1109/ICCV.2001.937651
   Bousseau A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618476
   Calian DA, 2018, COMPUT GRAPH FORUM, V37, P51, DOI 10.1111/cgf.13341
   Chandran P, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480509
   Chen AP, 2019, IEEE I CONF COMP VIS, P9428, DOI 10.1109/ICCV.2019.00952
   Christensen P. H., 2015, P ACM SIGGRAPH TALKS
   Community BO, 2018, Blender-a 3d modelling and rendering package
   Debevec P, 2002, IEEE COMPUT GRAPH, V22, P26, DOI 10.1109/38.988744
   Dib A, 2021, COMPUT GRAPH FORUM, V40, P153, DOI 10.1111/cgf.142622
   Gardner MA, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130891
   Hu YM, 2017, PROC CVPR IEEE, P330, DOI 10.1109/CVPR.2017.43
   Jakob Wenzel, 2010, Mitsuba renderer
   Janner M, 2017, ADV NEUR IN, V30
   Kanamori Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275104
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Lattas A, 2020, PROC CVPR IEEE, P757, DOI 10.1109/CVPR42600.2020.00084
   Li C, 2014, LECT NOTES COMPUT SC, V8693, P218, DOI 10.1007/978-3-319-10602-1_15
   Li ZM, 2018, LECT NOTES COMPUT SC, V11213, P339, DOI [10.1007/978-3-030-01240-3_21, 10.1007/978-3-030-01219-9_23]
   Nestmeyer T, 2020, PROC CVPR IEEE, P5123, DOI 10.1109/CVPR42600.2020.00517
   Nicodemus F. E., 1992, Geometrical Considerations and Nomenclature for Reflectance, P94
   Ramamoorthi R, 2001, J OPT SOC AM A, V18, P2448, DOI 10.1364/JOSAA.18.002448
   Ramamoorthi R, 2001, COMP GRAPH, P117, DOI 10.1145/383259.383271
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Sengupta S, 2018, PROC CVPR IEEE, P6296, DOI 10.1109/CVPR.2018.00659
   Smith WAP, 2020, PROC CVPR IEEE, P5010, DOI 10.1109/CVPR42600.2020.00506
   Sun TC, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323008
   TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105
   Walter B., 2007, P 18 EUR C REND TECH, P195, DOI [10.2312/EGWR/EGSR07/195-206, DOI 10.2312/EGWR/EGSR07/195-206]
   Wang ZB, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417824
   Weber H, 2018, INT CONF 3D VISION, P199, DOI 10.1109/3DV.2018.00032
   Weyrich T, 2006, ACM T GRAPHIC, V25, P1013, DOI 10.1145/1141911.1141987
   Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI [10.1007/s11263-019-01198-w, 10.1109/CSTIC.2018.8369274]
   Yamaguchi S, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201364
   Yang HT, 2020, PROC CVPR IEEE, P598, DOI 10.1109/CVPR42600.2020.00068
   Yi RJ, 2018, LECT NOTES COMPUT SC, V11213, P321, DOI 10.1007/978-3-030-01240-3_20
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhou H, 2019, IEEE I CONF COMP VIS, P7193, DOI 10.1109/ICCV.2019.00729
NR 38
TC 0
Z9 0
U1 2
U2 2
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD APR
PY 2024
VL 10
IS 2
BP 295
EP 308
DI 10.1007/s41095-022-0309-1
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DW8M2
UT WOS:001135210800001
OA gold
DA 2024-08-05
ER

PT J
AU Zhang, FH
   Zhao, L
   Li, SL
   Su, WJ
   Liu, LM
   Tao, WB
AF Zhang, Fenghao
   Zhao, Lin
   Li, Shengling
   Su, Wanjuan
   Liu, Liman
   Tao, Wenbing
TI 3D hand pose and shape estimation from monocular RGB via efficient 2D
   cues
SO COMPUTATIONAL VISUAL MEDIA
LA English
DT Article
DE hand; 3D reconstruction; deep learning; image features; 3D mesh
AB Estimating 3D hand shape from a single-view RGB image is important for many applications. However, the diversity of hand shapes and postures, depth ambiguity, and occlusion may result in pose errors and noisy hand meshes. Making full use of 2D cues such as 2D pose can effectively improve the quality of 3D human hand shape estimation. In this paper, we use 2D joint heatmaps to obtain spatial details for robust pose estimation. We also introduce a depth-independent 2D mesh to avoid depth ambiguity in mesh regression for efficient hand-image alignment. Our method has four cascaded stages: 2D cue extraction, pose feature encoding, initial reconstruction, and reconstruction refinement. Specifically, we first encode the image to determine semantic features during 2D cue extraction; this is also used to predict hand joints and for segmentation. Then, during the pose feature encoding stage, we use a hand joints encoder to learn spatial information from the joint heatmaps. Next, a coarse 3D hand mesh and 2D mesh are obtained in the initial reconstruction step; a mesh squeeze-and-excitation block is used to fuse different hand features to enhance perception of 3D hand structures. Finally, a global mesh refinement stage learns non-local relations between vertices of the hand mesh from the predicted 2D mesh, to predict an offset hand mesh to fine-tune the reconstruction results. Quantitative and qualitative results on the FreiHAND benchmark dataset demonstrate that our approach achieves state-of-the-art performance.
C1 [Zhang, Fenghao; Li, Shengling; Liu, Liman] South Cent Minzu Univ, Sch Biomed Engn, Hubei Key Lab Med Informat Anal & Tumor Diag & Tre, Wuhan 430074, Peoples R China.
   [Zhao, Lin; Su, Wanjuan; Tao, Wenbing] Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Natl Key Lab Sci & Technol Multispectral Informat, Wuhan 430074, Peoples R China.
C3 South Central Minzu University; Huazhong University of Science &
   Technology
RP Liu, LM (corresponding author), South Cent Minzu Univ, Sch Biomed Engn, Hubei Key Lab Med Informat Anal & Tumor Diag & Tre, Wuhan 430074, Peoples R China.
EM limanliu@mail.scuec.edu.cn
FU National Natural Science Foundation of China [61976227, 62176096];
   Natural Science Foundation of Hubei Province [2020CFA025]
FX We would like to thank the reviewers for valuable comments. This work
   was supported by grants from the National Natural Science Foundation of
   China (61976227, 62176096) and the Natural Science Foundation of Hubei
   Province (2020CFA025).
CR Baek S, 2019, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2019.00116
   Boukhayma A, 2019, PROC CVPR IEEE, P10835, DOI 10.1109/CVPR.2019.01110
   Cai YJ, 2018, LECT NOTES COMPUT SC, V11210, P678, DOI 10.1007/978-3-030-01231-1_41
   Chen P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12909, DOI 10.1109/ICCV48922.2021.01269
   Chen XY, 2021, PROC CVPR IEEE, P13269, DOI 10.1109/CVPR46437.2021.01307
   Chen YJ, 2021, PROC CVPR IEEE, P10446, DOI 10.1109/CVPR46437.2021.01031
   Choi Hongsuk, 2020, COMPUTER VISION ECCV
   Doosti B, 2020, PROC CVPR IEEE, P6607, DOI 10.1109/CVPR42600.2020.00664
   Gao CY, 2022, NEUROCOMPUTING, V474, P25, DOI 10.1016/j.neucom.2021.12.013
   Ge LH, 2019, PROC CVPR IEEE, P10825, DOI 10.1109/CVPR.2019.01109
   Gong SW, 2019, IEEE INT CONF COMP V, P4141, DOI 10.1109/ICCVW.2019.00509
   GOWER JC, 1975, PSYCHOMETRIKA, V40, P33, DOI 10.1007/BF02291478
   Gyeongsik Moon, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P752, DOI 10.1007/978-3-030-58571-6_44
   Hasson Y, 2019, PROC CVPR IEEE, P11799, DOI 10.1109/CVPR.2019.01208
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Iqbal U, 2018, LECT NOTES COMPUT SC, V11215, P125, DOI 10.1007/978-3-030-01252-6_8
   Jang Y, 2015, IEEE T VIS COMPUT GR, V21, P501, DOI 10.1109/TVCG.2015.2391860
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kikuchi Takazumi, 2018, Computational Visual Media, V4, P43, DOI 10.1007/s41095-017-0098-0
   Kingma D. P., 2014, arXiv
   Kolotouros N, 2019, PROC CVPR IEEE, P4496, DOI 10.1109/CVPR.2019.00463
   Kourbane I, 2022, APPL INTELL, V52, P16667, DOI 10.1007/s10489-022-03390-x
   Kulon D, 2019, Arxiv, DOI arXiv:1905.01326
   Kulon D, 2020, PROC CVPR IEEE, P4989, DOI 10.1109/CVPR42600.2020.00504
   Lee T, 2009, IEEE T VIS COMPUT GR, V15, P355, DOI 10.1109/TVCG.2008.190
   Li MP, 2020, COMPUT VIS MEDIA, V6, P147, DOI 10.1007/s41095-020-0171-y
   Li MR, 2021, AAAI CONF ARTIF INTE, V35, P1921
   Lin K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12919, DOI 10.1109/ICCV48922.2021.01270
   Lin K, 2021, PROC CVPR IEEE, P1954, DOI 10.1109/CVPR46437.2021.00199
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Malik J, 2020, PROC CVPR IEEE, P7111, DOI 10.1109/CVPR42600.2020.00714
   Moon G, 2018, PROC CVPR IEEE, P5079, DOI 10.1109/CVPR.2018.00533
   Mueller F, 2018, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2018.00013
   Nair V., 2010, ICML, P807
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Paszke A, 2019, ADV NEUR IN, V32
   Pavlakos G, 2018, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2018.00055
   Piumsomboon T, 2013, LECT NOTES COMPUT SC, V8118, P282
   Ren PF, 2021, NEUROCOMPUTING, V437, P42, DOI 10.1016/j.neucom.2021.01.045
   Romero J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130883
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Spurr A., 2020, EUR C COMP VIS, P211
   Spurr A, 2018, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2018.00017
   Tang X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11678, DOI 10.1109/ICCV48922.2021.01149
   Varol G, 2018, LECT NOTES COMPUT SC, V11211, P20, DOI 10.1007/978-3-030-01234-2_2
   Wang M, 2020, COMPUT VIS MEDIA, V6, P3, DOI 10.1007/s41095-020-0162-z
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Yang L., 2020, arXiv
   Yang LL, 2019, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2019.00242
   Yang LL, 2019, PROC CVPR IEEE, P9869, DOI 10.1109/CVPR.2019.01011
   Zhang BW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11334, DOI 10.1109/ICCV48922.2021.01116
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11261, DOI 10.1109/ICCV48922.2021.01109
   Zhang X, 2019, IEEE I CONF COMP VIS, P2354, DOI 10.1109/ICCV.2019.00244
   Zhou YX, 2020, PROC CVPR IEEE, P5345, DOI 10.1109/CVPR42600.2020.00539
   Zimmermann C, 2019, IEEE I CONF COMP VIS, P813, DOI 10.1109/ICCV.2019.00090
   Zimmermann C, 2017, IEEE I CONF COMP VIS, P4913, DOI 10.1109/ICCV.2017.525
NR 59
TC 1
Z9 1
U1 7
U2 9
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 2096-0433
EI 2096-0662
J9 COMPUT VIS MEDIA
JI Comput. Vis. Media
PD FEB
PY 2024
VL 10
IS 1
BP 79
EP 96
DI 10.1007/s41095-023-0346-4
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Z5WZ7
UT WOS:001112788200009
OA gold
DA 2024-08-05
ER

EF