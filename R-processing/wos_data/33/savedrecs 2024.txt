FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Tserenchimed, T
   Kim, H
AF Tserenchimed, Tuvshintulga
   Kim, Hyungki
TI Viewpoint-sharing method with reduced motion sickness in object-based
   VR/AR collaborative virtual environment
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual environment; Virtual reality; Augmented reality; Remote
   collaboration
ID MANIPULATION
AB We propose a viewpoint-sharing method with reduced motion sickness in an object-based remote collaborative virtual environment (CVE). The method is designed with an assumption of asymmetric, object-based CVE where collaborators use non-homogeneous devices, such as immersive virtual reality head-mounted display (VR HMD) and tablet-based augmented reality (AR), and simultaneously interact with 3D virtual objects. Therefore, collaborators interact with different interfaces such as virtual reality (VR) users relying on controllers for virtual locomotion and object manipulation, while AR users perform physical locomotion and multi-touch input for object manipulation. The proposed viewpoint-sharing method allows both users to observe and manipulate the objects in interest from the shared point of view, enabling participants to interact with the objects without the need for virtual/physical locomotion. While viewpoint-sharing, instead of changing point of view, the proposed method performs seamless object transformation to provide a shared point of view, reducing motion sickness and associated discomfort. From our user experiment, the viewpoint-share condition resulted in a 35.47% faster task completion time than the baseline condition which is without proposed viewpoint-sharing. The advantage of viewpoint-sharing regarding system usability was significant, while task workloads were similar in the baseline and viewpoint-sharing conditions. We expect that the proposed viewpoint-sharing method allows users to quickly, efficiently, and collaboratively communicate in an object-based CVE, and represents a step forward in the development of effective remote, asymmetric CVE.
C1 [Tserenchimed, Tuvshintulga; Kim, Hyungki] Jeonbuk Natl Univ, Dept Comp Sci & Artificial Intelligence, CAIIT, Jeonju, South Korea.
C3 Jeonbuk National University
RP Kim, H (corresponding author), Jeonbuk Natl Univ, Dept Comp Sci & Artificial Intelligence, CAIIT, Jeonju, South Korea.
EM hk.kim@jbnu.ac.kr
OI Kim, Hyungki/0000-0001-9013-2338
FU National Research Foundation of Korea [NRF-2020R1G1A1008932]; Basic
   Science Research Program; National Research Foundation of Korea (NRF) -
   Korea government (MSIT); BK21 FOUR (Fostering Outstanding Universities
   for Research) - Ministry of Education(MOE, Korea); National Research
   Foundation of Korea(NRF)
FX This work was supported by the Basic Science Research Program (Project
   ID: NRF-2020R1G1A1008932) through the National Research Foundation of
   Korea (NRF) funded by the Korea government (MSIT), and the BK21 FOUR
   (Fostering Outstanding Universities for Research) funded by the Ministry
   of Education(MOE, Korea) and National Research Foundation of Korea(NRF).
CR [Anonymous], Samsung Galaxy Tab A8
   Balakrishnan AD, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P1227
   Bozgeyikli E, 2016, CHI PLAY 2016: PROCEEDINGS OF THE 2016 ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P205, DOI 10.1145/2967934.2968105
   Brooke J., 1996, SUS-a quick and dirty usability scale, DOI [DOI 10.1201/9781498710411-35, DOI 10.1201/9781498710411]
   Juan MC, 2018, COMPUTERS, V7, DOI 10.3390/computers7010015
   Carvalho P, 2017, LECT NOTES COMPUT SC, V10280, P495, DOI 10.1007/978-3-319-57987-0_40
   Chang E, 2020, INT J HUM-COMPUT INT, V36, P1658, DOI 10.1080/10447318.2020.1778351
   Chellali A, 2013, VIRTUAL REAL-LONDON, V17, P1, DOI 10.1007/s10055-012-0214-5
   Chen X, 2018, VIRTUAL REALITY VR 2, P123
   Chong H, 2018, AIP CONF PROC, V2016, DOI 10.1063/1.5055439
   Elvezio C., 2017, ACM SIGGRAPH 2017 VR Village, P1, DOI 10.1145/3089269.3089281
   Grandi JG, 2017, IEEE SYMP 3D USER, P264, DOI 10.1109/3DUI.2017.7893373
   Grandi JG, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P127, DOI [10.1109/vr.2019.8798080, 10.1109/VR.2019.8798080]
   Grandi JG, 2018, 2018 IEEE VIRTUAL RE, P327, DOI [10.1109/VR.2018.8446405, DOI 10.1109/VR.2018.8446405]
   Hart SG., 2006, P HUM FACT ERG SOC A, V50, P904, DOI [10.1177/154193120605000909, DOI 10.1177/154193120605000909]
   Kim S, 2020, J MULTIMODAL USER IN, V14, P313, DOI 10.1007/s12193-020-00346-8
   Kim S, 2018, COMPUT SUPP COOP W J, V27, P569, DOI 10.1007/s10606-018-9324-2
   Kiyokawa K., 1999, IEEE SMC'99 Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.99CH37028), P48, DOI 10.1109/ICSMC.1999.816444
   La Viola J. J.  Jr., 2000, SIGCHI Bulletin, V32, P47, DOI 10.1145/333329.333344
   Lawson BD, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.759682
   Le Chenechal Morgan, 2016, 2016 IEEE Third VR International Workshop on Collaborative Virtual Environments (3DCVE), P9, DOI 10.1109/3DCVE.2016.7563559
   Lee G, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P343, DOI [10.1109/VR46266.2020.1581166222244, 10.1109/VR46266.2020.00-50]
   Lee J, 2017, SYMMETRY-BASEL, V9, DOI 10.3390/sym9050078
   MAPES DP, 1995, PRESENCE-TELEOP VIRT, V4, P403, DOI 10.1162/pres.1995.4.4.403
   Martnez-Cmara E., 2021, J Netw Comput Appl, V185, P103044, DOI [10.1016/j.jnca.2021.103044, DOI 10.1016/J.JNCA.2021.103044]
   Mendes D, 2019, COMPUT GRAPH FORUM, V38, P21, DOI 10.1111/cgf.13390
   Oculus Rift S, 2023, Meta
   Oda O, 2015, UIST'15: PROCEEDINGS OF THE 28TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P405, DOI 10.1145/2807442.2807497
   Pettersson LW, 2010, INFORM VISUAL, V9, P98, DOI 10.1057/ivs.2009.2
   Piumsomboon T, 2019, FRONT ROBOT AI, V6, DOI 10.3389/frobt.2019.00005
   Pouliquen-Lardy L, 2016, VIRTUAL REAL-LONDON, V20, P213, DOI 10.1007/s10055-016-0294-8
   Smith A., 2022, Virtual Real, V26, P1, DOI [10.1007/s10055-022-00515-2, DOI 10.1007/S10055-022-00515-2]
   Smith A, 2019, P INT C HUM COMP INT, P123
   Unity3D, 2023, Unity Technologies
   Valin S, 2001, P 34 ANN HAW INT C S, P1032
   Yao Richard., 2014, Oculus VR, V4, P27
NR 36
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD JUN 11
PY 2024
VL 28
IS 3
AR 122
DI 10.1007/s10055-024-01005-z
PG 12
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA TV1N1
UT WOS:001243944100001
OA hybrid
DA 2024-08-05
ER

PT J
AU Brazil, CK
   Rys, MJ
AF Brazil, Cristiane K.
   Rys, Malgorzata J.
TI The effect of VR on fine motor performance by older adults: a comparison
   between real and virtual tasks
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Aging; Fine motor tasks; Ergonomic fidelity; Learning
ID MILD COGNITIVE IMPAIRMENT; INSTRUMENTAL ACTIVITIES; AGE
AB Virtual Reality (VR) technology has the potential to support the aging population and improve testing of daily abilities to detect functional decline. In multiple research studies, VR performance of participants has been assessed by measuring time to complete test, but the effect of learning how to use the VR system and differences between real and virtual environments have been understudied, especially for fine motor tasks. In this study, 20 older adults ages 65-84 performed a task that required fine motor skills in real-life and then in a VR replica of the same task. All participants completed the task in each setting with no difficulties. A clear learning effect was observed in VR, which was attributed to learning how to use the device itself. Still, participants could not reach the same level of performance (time) in VR as in real-life. Participants rated the VR task more mentally and physically demanding than in real-life, as well as more stressful, but with an overall low cognitive demand. In an exploratory cluster analysis, participants with an average age of 69 years old had more technological devices, found the VR system more usable and realistic than participants in the group with an average of 76 years old. This study demonstrated that VR influences time to complete a fine motor task, and that learning effects related to the system could be confounded with actual task performance if not properly considered in VR studies with older adults.
C1 [Brazil, Cristiane K.; Rys, Malgorzata J.] Kansas State Univ, Ind & Mfg Syst Engn, 1701B Platt St, Manhattan, KS 66506 USA.
C3 Kansas State University
RP Brazil, CK (corresponding author), Kansas State Univ, Ind & Mfg Syst Engn, 1701B Platt St, Manhattan, KS 66506 USA.
EM cristianekbrazil@ksu.edu; malrys@ksu.edu
CR Appel L, 2020, FRONT MED-LAUSANNE, V6, DOI 10.3389/fmed.2019.00329
   Arias E., 2021, Vital Stat Rapid Release, V15, DOI DOI 10.15620/CDC:107201
   Arlati S, 2022, VIRTUAL REAL-LONDON, V26, P885, DOI 10.1007/s10055-021-00603-5
   Bohil CJ, 2011, NAT REV NEUROSCI, V12, P752, DOI 10.1038/nrn3122
   Boot WR, 2015, GERONTOLOGIST, V55, P404, DOI 10.1093/geront/gnt117
   Branca G, 2023, PSYCHOL MARKET, V40, P596, DOI 10.1002/mar.21743
   Brazil CK, 2022, SSRN Scholarly Paper 4167279, DOI [10.2139/ssrn.4167279, DOI 10.2139/SSRN.4167279]
   Brooke John, 1996, SUS-A quick and dirty usability scale
   Bui J, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.595771
   Calamia M, 2012, CLIN NEUROPSYCHOL, V26, P543, DOI 10.1080/13854046.2012.680913
   Campo-Prieto P, 2021, VIRTUAL REAL-LONDON, V25, P801, DOI 10.1007/s10055-020-00495-x
   Canning CG, 2020, NAT REV NEUROL, V16, P409, DOI 10.1038/s41582-020-0370-2
   Carlson P, 2015, IEEE T VIS COMPUT GR, V21, P770, DOI 10.1109/TVCG.2015.2393871
   Cavedoni S, 2020, FRONT HUM NEUROSCI, V14, DOI 10.3389/fnhum.2020.00245
   Chau PH, 2021, J MED INTERNET RES, V23, DOI 10.2196/27640
   Chen JY, 2017, APPL ERGON, V65, P437, DOI 10.1016/j.apergo.2017.03.013
   D'Cunha NM, 2019, GERONTOLOGY, V65, P430, DOI 10.1159/000500040
   de Rotrou J, 2012, DEMENT GERIATR COGN, V33, P210, DOI 10.1159/000338232
   Dilanchian AT, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.736793
   Elbert R, 2018, IFAC PAPERSONLINE, V51, P686, DOI 10.1016/j.ifacol.2018.08.398
   Forgiarini A, 2023, P 15 BIANN C IT SIGC, P1
   Gamito P, 2019, CYBERPSYCH BEH SOC N, V22, P69, DOI 10.1089/cyber.2017.0679
   Gao Z, 2020, J CLIN MED, V9, DOI 10.3390/jcm9061986
   Gerber SM, 2018, IEEE ENG MED BIO, P3489, DOI 10.1109/EMBC.2018.8513003
   Guzsvinecz T, 2022, VIRTUAL REAL-LONDON, V26, P601, DOI 10.1007/s10055-021-00509-2
   HART S G, 1988, P139
   Hedden T, 2004, NAT REV NEUROSCI, V5, P87, DOI 10.1038/nrn1323
   Huygelier H, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-41200-6
   Jones T, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0167523
   Joyner JS, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.599274
   Kennedy R.S., 1993, Int. J. Aviat. Psy, P203
   Kim H, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-91573-w
   Lee LN, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9173556
   Marshall GA, 2011, ALZHEIMERS DEMENT, V7, P300, DOI 10.1016/j.jalz.2010.04.005
   Mason AH, 2019, INT J HUM-COMPUT INT, V35, P1870, DOI 10.1080/10447318.2019.1574101
   Mittelstaedt JM, 2019, VIRTUAL REAL-LONDON, V23, P143, DOI 10.1007/s10055-018-0370-3
   Murcia-López M, 2018, IEEE T VIS COMPUT GR, V24, P1574, DOI 10.1109/TVCG.2018.2793638
   Murman Daniel L., 2015, Seminars in Hearing, V36, P111, DOI 10.1055/s-0035-1555115
   Oren M, 2012, IEEE VIRTUAL REALITY CONFERENCE 2012 PROCEEDINGS, P27, DOI 10.1109/VR.2012.6180873
   Paljic A, 2017, LECT NOTES COMPUT SC, V10590, P301, DOI 10.1007/978-3-319-70742-6_28
   Park S, 2023, UNIVERSAL ACCESS INF, V22, P771, DOI 10.1007/s10209-022-00878-8
   Parra MA, 2019, EXP AGING RES, V45, P180, DOI 10.1080/0361073X.2019.1586106
   Pedram S, 2020, COMPUT HUM BEHAV, V105, DOI 10.1016/j.chb.2019.106223
   Perez-Marcos D, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.02120
   Perry J.S., 2018, ACTUATOR 2018 16 INT, P1
   Bezerra IMP, 2018, MEDICINE, V97, DOI 10.1097/MD.0000000000009612
   Porffy LA, 2022, J MED INTERNET RES, V24, DOI 10.2196/27641
   Reppermund S, 2011, INT J GERIATR PSYCH, V26, P843, DOI 10.1002/gps.2612
   Rosales A., 2019, Perspectives on Human Computer Interaction Research with Older People, P51, DOI [DOI 10.1007/978-3-030-06076-3_4, 10.1007/978-3-030-06076-34, DOI 10.1007/978-3-030-06076-34]
   Saredakis D, 2020, FRONT HUM NEUROSCI, V14, DOI 10.3389/fnhum.2020.00096
   Schubert T. W., 2003, Z. fur Medienpsychologie, V15, P69, DOI [10.1026//1617-6383.15.2.69, DOI 10.1026//1617-6383.15.2.69]
   Smith CD, 2005, NEUROBIOL AGING, V26, P883, DOI 10.1016/j.neurobiolaging.2004.08.014
   Stamm O, 2022, VIRTUAL REAL-LONDON, V26, P1291, DOI 10.1007/s10055-022-00629-3
   US Census, 2018, Census.Gov
   van Wegen M, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23031563
   Weech S, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.00158
   Weidner F, 2017, P IEEE VIRT REAL ANN, P281, DOI 10.1109/VR.2017.7892286
   World Health Organization, 2012, World Health Organ Tech Rep Ser, P1
   Zygouris S, 2017, J ALZHEIMERS DIS, V56, P619, DOI 10.3233/JAD-160518
NR 59
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAY 20
PY 2024
VL 28
IS 2
AR 113
DI 10.1007/s10055-024-01009-9
PG 11
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA RK5H6
UT WOS:001227566400001
OA hybrid
DA 2024-08-05
ER

PT J
AU Wang, XM
   Southwick, D
   Robinson, I
   Nitsche, M
   Resch, G
   Mazalek, A
   Welsh, TN
AF Wang, Xiaoye Michael
   Southwick, Daniel
   Robinson, Ian
   Nitsche, Michael
   Resch, Gabby
   Mazalek, Ali
   Welsh, Timothy N.
TI The geometry of the vergence-accommodation conflict in mixed reality
   systems
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Augmented reality; Vergence-accommodation conflict;
   Depth perception; Manual pointing
ID DEPTH-PERCEPTION; DISTANCE; PERTURBATION; JUDGMENTS; FEEDBACK; STEREO;
   TARGET; MODEL
AB Mixed reality technologies, such as virtual (VR) and augmented (AR) reality, present promising opportunities to advance education and professional training due to their adaptability to diverse contexts. Distortions in the perceived distance in such mediated conditions, however, are well documented and have imposed nontrivial challenges that complicate and limit transferring task performance in a virtual setting to the unmediated reality (UR). One potential source of the distance distortion is the vergence-accommodation conflict-the discrepancy between the depth specified by the eyes' accommodative state and the angle at which the eyes converge to fixate on a target. The present study involved the use of a manual pointing task in UR, VR, and AR to quantify the magnitude of the potential depth distortion in each modality. Conceptualizing the effect of vergence-accommodation offset as a constant offset to the vergence angle, a model was developed based on the stereoscopic viewing geometry. Different versions of the model were used to fit and predict the behavioral data for all modalities. Results confirmed the validity of the conceptualization of vergence-accommodation as a device-specific vergence offset, which predicted up to 66% of the variance in the data. The fitted parameters indicate that, due to the vergence-accommodation conflict, participants' vergence angle was driven outwards by approximately 0.2 degrees, which disrupted the stereoscopic viewing geometry and produced distance distortion in VR and AR. The implications of this finding are discussed in the context of developing virtual environments that minimize the effect of depth distortion.
C1 [Wang, Xiaoye Michael; Welsh, Timothy N.] Univ Toronto, Fac Kinesiol & Phys Educ, Toronto, ON, Canada.
   [Southwick, Daniel; Robinson, Ian; Mazalek, Ali] Toronto Metropolitan Univ, Synaesthet Media Lab, Toronto, ON, Canada.
   [Nitsche, Michael] Georgia Inst Technol, Ivan Allen Coll Liberal Arts, Atlanta, GA 30318 USA.
   [Resch, Gabby] Ontario Tech Univ, Fac Business & Informat Technol, Oshawa, ON, Canada.
C3 University of Toronto; Toronto Metropolitan University; University
   System of Georgia; Georgia Institute of Technology
RP Wang, XM (corresponding author), Univ Toronto, Fac Kinesiol & Phys Educ, Toronto, ON, Canada.
EM michaelwxy.wang@utoronto.ca
FU Social Sciences and Humanities Research Council of Canada
FX The authors wish to thank Professor Geoffrey Bingham for his generous
   and invaluable feedback on this manuscript. The authors also wish to
   thank Daphne Wang, Ruoqi Huang, and Alex Isidori for assisting with the
   data collection process.
CR Anderson J, 2010, EXP BRAIN RES, V205, P291, DOI 10.1007/s00221-010-2361-9
   Barkokebas R., 2019, ISARC P INT S AUT RO, P796
   Batmaz AU, 2023, 29TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, VRST 2023, DOI 10.1145/3611659.3615686
   Batmaz AU, 2023, Symposium Virtual Re, P482, DOI 10.1109/VR55154.2023.00063
   Batmaz AU, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3502067
   Batmaz AU, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P585, DOI [10.1109/VR.2019.8797975, 10.1109/vr.2019.8797975]
   Bebko AO, 2020, I-PERCEPTION, V11, DOI 10.1177/2041669520938400
   Bingham GP, 2023, VISION RES, V203, DOI 10.1016/j.visres.2022.108152
   Bingham GP, 2022, VISION RES, V196, DOI 10.1016/j.visres.2022.108029
   Bingham GP, 1998, J EXP PSYCHOL HUMAN, V24, P145, DOI 10.1037/0096-1523.24.1.145
   Bingham GP, 2001, J EXP PSYCHOL HUMAN, V27, P1314, DOI 10.1037//0096-1523.27.6.1314
   Buck LE, 2018, ACM T APPL PERCEPT, V15, DOI 10.1145/3196885
   Chang JSK, 2018, CHI 2018: EXTENDED ABSTRACTS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3170427.3186530
   Chang JSK, 2017, SUI'17: PROCEEDINGS OF THE 2017 SYMPOSIUM ON SPATIAL USER INTERACTION, P68, DOI 10.1145/3131277.3132171
   Coats RO, 2014, J EXP PSYCHOL HUMAN, V40, P328, DOI 10.1037/a0033802
   Cumming BG, 2001, ANNU REV NEUROSCI, V24, P203, DOI 10.1146/annurev.neuro.24.1.203
   Dodgson NA, 2004, PROC SPIE, V5291, P36, DOI 10.1117/12.529999
   Eadie AS, 2000, OPHTHAL PHYSL OPT, V20, P242, DOI 10.1016/S0275-5408(99)00057-5
   Elliott D, 2010, PSYCHOL BULL, V136, P1023, DOI 10.1037/a0020958
   FITTS PM, 1954, J EXP PSYCHOL, V47, P381, DOI 10.1037/h0055392
   Fry G., 1983, Basic concepts underlying graphical analysis. Vergence eye movements basic and clinical aspects, P605
   Gibson J. J., 1979, ECOLOGICAL APPROACH
   Gonzalez-Franco M, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P941, DOI [10.1109/VR.2019.8798348, 10.1109/vr.2019.8798348]
   Guinet A. L., 2019, Computer Methods in Biomechanics and Biomedical Engineering, V22, P169, DOI 10.1080/10255842.2020.1714228
   Herth RA, 2021, EXP BRAIN RES, V239, P765, DOI 10.1007/s00221-020-05989-3
   Hibbard PB, 2020, INT CONF 3D IMAG, DOI 10.1109/IC3D51119.2020.9376369
   Hoffman DM, 2008, J VISION, V8, DOI 10.1167/8.3.33
   Howard I. P., 1995, Binocular Vision and Stereopsis
   HUNG GK, 1992, OPHTHAL PHYSL OPT, V12, P319, DOI 10.1111/j.1475-1313.1992.tb00404.x
   Hung GK, 1996, OPHTHAL PHYSL OPT, V16, P31, DOI 10.1016/0275-5408(95)00110-7
   HUNG GK, 1980, IEEE T BIO-MED ENG, V27, P439, DOI 10.1109/TBME.1980.326752
   JaaAro KM, 1997, P SOC PHOTO-OPT INS, V3012, P319, DOI 10.1117/12.274474
   Jeelani I, 2017, COMPUTING IN CIVIL ENGINEERING 2017: SENSING, SIMULATION, AND VISUALIZATION, P407
   Johnson PB, 2022, MED PHYS, V49, P15, DOI 10.1002/mp.15349
   JUDGE SJ, 1986, J NEUROPHYSIOL, V55, P915, DOI 10.1152/jn.1986.55.5.915
   JULESZ B, 1964, SCIENCE, V145, P356, DOI 10.1126/science.145.3630.356
   Kelly JW, 2023, IEEE T VIS COMPUT GR, V29, P4978, DOI 10.1109/TVCG.2022.3196606
   Khan MA, 2006, NEUROSCI BIOBEHAV R, V30, P1106, DOI 10.1016/j.neubiorev.2006.05.002
   Kim J., 2017, P 27 INT C ARTIFICIA, P153, DOI DOI 10.5555/3298830.32988592
   Kline PB, 1996, PROCEEDINGS OF THE HUMAN FACTORS AND ERGONOMICS SOCIETY - 40TH ANNUAL MEETING, VOLS 1 AND 2, P1112
   Koenderink J, 2008, J MATH IMAGING VIS, V31, P171, DOI 10.1007/s10851-008-0076-3
   Krajancich B, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417820
   Kramida G, 2016, IEEE T VIS COMPUT GR, V22, P1912, DOI 10.1109/TVCG.2015.2473855
   Lambooij M, 2009, J IMAGING SCI TECHN, V53, DOI 10.2352/J.ImagingSci.Technol.2009.53.3.030201
   Linkenauger SA, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0068594
   Liu L, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P219, DOI 10.1109/VR.2009.4811026
   Machuca MDB, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300437
   Makransky G, 2019, J COMPUT ASSIST LEAR, V35, P691, DOI 10.1111/jcal.12375
   Masnadi S, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P542, DOI 10.1109/VRW52623.2021.00153
   Mays LE, 1995, CURR OPIN NEUROBIOL, V5, P763, DOI 10.1016/0959-4388(95)80104-9
   MILGRAM P, 1994, IEICE T INF SYST, VE77D, P1321
   Miller J, 2020, INT J ADV MANUF TECH, V109, P1741, DOI 10.1007/s00170-020-05768-y
   Mine D, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0232290
   Mohler BJ, 2010, PRESENCE-TELEOP VIRT, V19, P230, DOI 10.1162/pres.19.3.230
   Mon-Williams M, 1999, PERCEPTION, V28, P167, DOI 10.1068/p2737
   More J. J., 1978, Proceedings of the Biennial Conference on numerical analysis, P105
   MORGAN MW, 1968, AMER J OPT ARCH AM A, V45, P417
   Pan JS, 2014, J EXP PSYCHOL HUMAN, V40, P404, DOI 10.1037/a0033795
   Peer A, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P244, DOI [10.1109/vr.2019.8797911, 10.1109/VR.2019.8797911]
   Peirce J, 2019, BEHAV RES METHODS, V51, P195, DOI 10.3758/s13428-018-01193-y
   Peirce JW, 2007, J NEUROSCI METH, V162, P8, DOI 10.1016/j.jneumeth.2006.11.017
   POGGIO GF, 1984, ANNU REV NEUROSCI, V7, P379, DOI 10.1146/annurev.ne.07.030184.002115
   Renner RS, 2015, ACM T APPL PERCEPT, V12, DOI 10.1145/2724716
   Renner RS, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543590
   Ryu J, 2005, 2005 International Conference on Cyberworlds, Proceedings, P43
   Schor CM., 1983, Vergence eye movements: basic and clinical aspects, DOI [10.1097/00006324-198311000-00013, DOI 10.1097/00006324-198311000-00013]
   Singh G, 2020, IEEE T VIS COMPUT GR, V26, P1385, DOI 10.1109/TVCG.2018.2869729
   Swan JE, 2015, IEEE T VIS COMPUT GR, V21, P1289, DOI 10.1109/TVCG.2015.2459895
   Teather RJ, 2009, 3DUI : IEEE SYMPOSIUM ON 3D USER INTERFACES 2009, PROCEEDINGS, P43, DOI 10.1109/3DUI.2009.4811204
   Virtanen P, 2020, NAT METHODS, V17, P261, DOI 10.1038/s41592-019-0686-2
   Wang XM, 2024, BEHAV RES METHODS, DOI 10.3758/s13428-024-02378-4
   Wang XM, 2023, VIS COGN, V31, P107, DOI 10.1080/13506285.2023.2203528
   Wang XM, Scientific Reports, DOI [10.31234/osf.io/ag7z9, DOI 10.31234/OSF.IO/AG7Z9]
   Wang XM, Mixed reality alters motor planning and control, DOI [10.31234/osf.io/pxv5t, DOI 10.31234/OSF.IO/PXV5T]
   WANN JP, 1995, VISION RES, V35, P2731, DOI 10.1016/0042-6989(95)00018-U
   Warburton M, 2023, BEHAV RES METHODS, V55, P3658, DOI 10.3758/s13428-022-01983-5
   Wexler M, 2005, TRENDS COGN SCI, V9, P431, DOI 10.1016/j.tics.2005.06.018
   Willemsen P, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1498700.1498702
   Witmer BG, 1998, HUM FACTORS, V40, P478, DOI 10.1518/001872098779591340
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P144, DOI 10.1162/105474698565640
   Woodworth RS, 1899, PSYCHOL REV-MONOGR S, V3, P1
   Yeh PH, 2021, BMC OPHTHALMOL, V21, DOI 10.1186/s12886-021-02016-z
NR 82
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD APR 11
PY 2024
VL 28
IS 2
AR 95
DI 10.1007/s10055-024-00991-4
PG 20
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA NK7O3
UT WOS:001200414900001
OA hybrid
DA 2024-08-05
ER

PT J
AU Wienrich, C
   Horn, V
   Krauss, J
   Buerger, A
AF Wienrich, Carolin
   Horn, Viktoria
   Krauss, Jana
   Buerger, Arne
TI Personal space invasion to prevent cyberbullying: design, development,
   and evaluation of an immersive prevention measure for children and
   adolescents
SO VIRTUAL REALITY
LA English
DT Article
DE Cyberbullying; VR prevention; Personal space invasion; Children;
   Cognitive dissonance; Empathy
ID SCHOOL; VICTIMIZATION; PERCEPTIONS; SUICIDE; EMPATHY; WOULD; SELF
AB The previous work on cyberbullying has shown that the number of victims is increasing, and the need for prevention is exceptionally high among younger school students (5th-9th grade). Due to the omnipresence of cyberattacks, victims can hardly distance themselves psychologically, thus experience an intrusion in almost all areas of life. The perpetrators, on the other hand, feel the consequences of their actions even less in cyberspace. However, there is a gap between the need and the existence of innovative prevention programs tied to the digital reality of the target group and the treatment of essential aspects of psychological distance. This article explores the design space, feasibility, and effectiveness of a unique VR-based cyberbullying prevention component in a human-centered iterative approach. The central idea is reflected in creating a virtual personal space invasion with virtual objects associated with cyberbullying making the everyday intrusion of victims tangible. A pre-study revealed that harmful speech texts in bright non-removable message boxes best transferred the psychological determinants associated with a personal space invasion to virtual objects contextualized in cyberbullying scenarios. Therefore, these objects were incorporated into a virtual prevention program that was then tested in a laboratory study with 41 participants. The results showed that the intervention could trigger cognitive dissonance and empathy. In the second step, the intervention was evaluated and improved in a focus group with the actual target group of children and adolescents. The improved application was then evaluated in a school workshop for 5 days with 100 children and adolescents. The children understood the metaphor of virtual space invasion by the harmful text boxes and reported the expected psychological effects. They also showed great interest in VR. In summary, this paper contributes to the innovative and effective prevention of cyberbullying by using the potential of VR. It provides empirical evidence from a laboratory experiment and a field study with a large sample from the target group of children and adolescents and discusses implications for future developments.
C1 [Wienrich, Carolin] Univ Wurzburg, Psychol Intelligent Interact Syst, Emil Fischer Str 50, D-97074 Wurzburg, Germany.
   [Horn, Viktoria] Univ Kassel, Participatory IT Design, Pfannkuchstr 1, D-34121 Kassel, Hessen, Germany.
   [Krauss, Jana] Univ Wurzburg, Human Comp Interact Psychol Intelligent Interact S, Emil Fischer Str 50, D-97074 Wurzburg, Bavaria, Germany.
   [Buerger, Arne] Univ Hosp Wurzburg, Clin & Polyclin Child & Adolescent Psychiat, Josef Schneider Str 1,610101, D-97080 Wurzburg, Germany.
C3 University of Wurzburg; Universitat Kassel; University of Wurzburg;
   University of Wurzburg
RP Wienrich, C (corresponding author), Univ Wurzburg, Psychol Intelligent Interact Syst, Emil Fischer Str 50, D-97074 Wurzburg, Germany.
EM carolin.wienrich@uni-wuerzburg.de; viktoria.horn@uni-kassel.de;
   jana.krauss@uni-wuerzburg.de; Buerger_A@ukw.de
OI Wienrich, Carolin/0000-0003-3052-7172
FU Projekt DEAL; Bavarian State Ministry for Digital Affairs in the project
   XR Hub [A5-3822-2-16]
FX Open Access funding enabled and organized by Projekt DEAL. This research
   has been funded by the Bavarian State Ministry for Digital Affairs in
   the project XR Hub (project number A5-3822-2-16).
CR Ahn SJG, 2016, J COMPUT-MEDIAT COMM, V21, P399, DOI 10.1111/jcc4.12173
   Ang RP, 2010, CHILD PSYCHIAT HUM D, V41, P387, DOI 10.1007/s10578-010-0176-3
   [Anonymous], 2005, Screenplay: The foundations of screenwriting
   [Anonymous], 2016, Jim 2016-jugend, information, (multi-)media
   [Anonymous], 2021, DELFI 2021, P91
   [Anonymous], 2018, Was ist cybermobbing?
   Ansary NS, 2020, AGGRESS VIOLENT BEH, V50, DOI 10.1016/j.avb.2019.101343
   Bailenson JN, 2001, PRESENCE-VIRTUAL AUG, V10, P583, DOI 10.1162/105474601753272844
   Barzilay S, 2017, J ADOLESCENT HEALTH, V61, P179, DOI 10.1016/j.jadohealth.2017.02.002
   Batson CD, 1997, PERS SOC PSYCHOL B, V23, P751, DOI 10.1177/0146167297237008
   Bauman S, 2013, J ADOLESCENCE, V36, P341, DOI 10.1016/j.adolescence.2012.12.001
   Beitzinger F, 2020, Cyberlife iii. spannungsfeld zwischen faszination und gefahr
   Bertrand P, 2018, FRONT ROBOT AI, V5, DOI 10.3389/frobt.2018.00026
   Bönsch A, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P199
   Borbé R, 2009, PSYCHIAT PRAX, V36, P362, DOI 10.1055/s-0029-1223359
   Carmona JA, 2011, EUR J EDUC PSYCHOL, V4, P75, DOI 10.30552/ejep.v4i1.66
   Casas JA, 2013, COMPUT HUM BEHAV, V29, P580, DOI 10.1016/j.chb.2012.11.015
   Chen LM, 2020, BRIT J EDUC PSYCHOL, V90, P224, DOI 10.1111/bjep.12319
   DAVIS MH, 1983, J PERS SOC PSYCHOL, V44, P113, DOI 10.1037/0022-3514.44.1.113
   de Almeida Mendes AK., 2021, Res Soc Dev, V10, DOI [10.33448/rsd-v10i7.16844, DOI 10.33448/RSD-V10I7.16844]
   Feierabend S, 2022, Jugend, Information, Medien: basisuntersuchung zum Medienumgang 12- bis 19-Jahriger in Deutschland
   FELIPE NJ, 1966, SOC PROBL, V14, P206, DOI 10.1525/sp.1966.14.2.03a00080
   Flägel K, 2019, Z EVIDENZ FORTBILD Q, V147, P90, DOI 10.1016/j.zefq.2019.10.003
   Gaffney H, 2021, CAMPBELL SYST REV, V17, DOI 10.1002/cl2.1143
   Gaffney H, 2019, AGGRESS VIOLENT BEH, V45, P134, DOI 10.1016/j.avb.2018.07.002
   Gu X, 2022, IEEE Trans Vis Comput Graph
   HALL ET, 1963, AM ANTHROPOL, V65, P1003, DOI 10.1525/aa.1963.65.5.02a00020
   Happ C, 2015, Diagnostica
   HAYDUK LA, 1978, PSYCHOL BULL, V85, P117, DOI 10.1037/0033-2909.85.1.117
   Hecht H, 2019, ACTA PSYCHOL, V193, P113, DOI 10.1016/j.actpsy.2018.12.009
   Hein RM, AIMS Electron Electr Eng, V5, P117
   Herrera F, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0204494
   Hupfeld J., 2011, Z KL PSYCH PSYCHOTH
   Ingram KM, 2019, J ADOLESCENCE, V71, P72, DOI 10.1016/j.adolescence.2018.12.006
   Jain V, 2020, INT J PUBLIC HEALTH, V65, P533, DOI 10.1007/s00038-020-01390-7
   Kahl KG, 2012, CURR OPIN PSYCHIATR, V25, P522, DOI 10.1097/YCO.0b013e328358e531
   Kalyanaraman S, 2010, J NERV MENT DIS, V198, P437, DOI 10.1097/NMD.0b013e3181e07d66
   Kennedy R.S., 1993, Int. J. Aviat. Psy, P203
   Kowalski RM, 2007, J ADOLESCENT HEALTH, V41, pS22, DOI 10.1016/j.jadohealth.2007.08.017
   Lambe LJ, 2022, SOC DEV, V31, P984, DOI 10.1111/sode.12599
   Latoschik ME, 2019, IEEE T VIS COMPUT GR, V25, P2134, DOI 10.1109/TVCG.2019.2899250
   Lewis L, 2017, ERGONOMICS, V60, P1461, DOI 10.1080/00140139.2017.1313456
   Liberman N, 1998, J PERS SOC PSYCHOL, V75, P5, DOI 10.1037/0022-3514.75.1.5
   Loewenstein G, 1996, ORGAN BEHAV HUM DEC, V65, P272, DOI 10.1006/obhd.1996.0028
   Marcum CD, 2014, AM J CRIM JUSTICE, V39, P538, DOI 10.1007/s12103-013-9217-3
   McEvoy Kelly Anne, 2015, Through the eyes of a bystander: understanding vr and video effectiveness on bystander empathy, presence, behavior, and attitude in bullying situations
   MIDDLEMIST RD, 1976, J PERS SOC PSYCHOL, V33, P541, DOI 10.1037/0022-3514.33.5.541
   Nocentini A, 2015, AGGRESS VIOLENT BEH, V23, P52, DOI 10.1016/j.avb.2015.05.012
   Nolden D, 2021, JMS Jugend Medien Schutz-Report, V43, P7
   Olweus D, 2003, EDUC LEADERSHIP, V60, P12
   Ossa FC, 2023, PSYCHOPATHOLOGY, V56, P127, DOI 10.1159/000523992
   Oyekoya O, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.672003
   Paulus C., 2009, PSYCHOMETRISCHE EVAL
   Peck TC, 2013, CONSCIOUS COGN, V22, P779, DOI 10.1016/j.concog.2013.04.016
   Pfetsch J, 2014, Diskurs Kindheits-und Jugendforschung/Discourse J Childhood Adolesc Res, V9, P23, DOI DOI 10.3224/DISKURS.V9I1.19081
   Pissarek M., 2018, Empirische Forschung in der Deutschdidaktik, P215
   Polanin JR, 2021, PSYCHOL BULL, V147, P115, DOI 10.1037/bul0000314
   Pronin E, 2008, PERS SOC PSYCHOL B, V34, P224, DOI 10.1177/0146167207310023
   Ratan R, 2020, MEDIA PSYCHOL, V23, P651, DOI 10.1080/15213269.2019.1623698
   Reed KP, 2016, J HUM BEHAV SOC ENVI, V26, P37, DOI 10.1080/10911359.2015.1059165
   Schuler K, 2020, Corona verstarkt cybermobbing
   Schultze-Krumbholz A., 2018, REDUCING CYBERBULLYI, P145, DOI [10.1016/B978-0-12-811423-0.00011-0, DOI 10.1016/B978-0-12-811423-0.00011-0]
   Schultze-Krumbholz A, 2009, Z PSYCHOL, V217, P224, DOI 10.1027/0044-3409.217.4.224
   Smith PK, 2008, J CHILD PSYCHOL PSYC, V49, P376, DOI 10.1111/j.1469-7610.2007.01846.x
   SMITH RJ, 1978, PERS SOC PSYCHOL B, V4, P429, DOI 10.1177/014616727800400314
   Stice E, 2019, CLIN PSYCHOL REV, V70, P91, DOI 10.1016/j.cpr.2019.04.004
   Stice E, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0144530
   Suler J, 2004, CYBERPSYCHOL BEHAV, V7, P321, DOI 10.1089/1094931041291295
   SUNDSTROM E, 1976, HUM ECOL, V4, P47, DOI 10.1007/BF01531456
   Tong X, 2020, JMIR SERIOUS GAMES, V8, DOI 10.2196/17354
   Veenstra R, 2010, CHILD DEV, V81, P480, DOI 10.1111/j.1467-8624.2009.01411.x
   Ventura S, 2020, CYBERPSYCH BEH SOC N, V23, P667, DOI 10.1089/cyber.2019.0681
   Wienrich C, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P207, DOI 10.1109/VR.2018.8446575
   Wienrich C, 2018 10 INT C VIRT W, P1
   Wienrich C, 2022, ENTERTAIN COMPUT, V43, DOI 10.1016/j.entcom.2022.100514
   Wienrich C, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.627194
   Wilcox Laurie M., 2006, ACM Trans. on Perception, V3, P412, DOI [10.1145/1190036.1190041, DOI 10.1145/1190036.1190041]
   Yee N, 2007, HUM COMMUN RES, V33, P271, DOI 10.1111/j.1468-2958.2007.00299.x
NR 78
TC 1
Z9 1
U1 8
U2 8
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR 15
PY 2024
VL 28
IS 2
AR 75
DI 10.1007/s10055-024-00964-7
PG 17
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA LG7B7
UT WOS:001185687800001
OA hybrid
DA 2024-08-05
ER

PT J
AU Chuang, TJ
   Smith, S
AF Chuang, Tung-Jui
   Smith, Shana
TI A Multi-user Cross-platform hands-on virtual lab within the Metaverse -
   the case of machining training
SO VIRTUAL REALITY
LA English
DT Article
DE Metaverse; Multi-user; Distance learning; Cross-platform; Hands-on
ID TECHNOLOGY; EDUCATION; REALITY; SCIENCE
AB Distance learning has become a popular learning channel today. However, while various distance learning tools are available, most of them only support a single platform, offer only the trainer's perspective, and do not facilitate student-instructor interaction. As a result, distance learning systems tend to be inflexible and less effective. To address the limitations of existing distance learning systems, this study developed a cross-platform hands-on virtual lab within the Metaverse that enables multi-user participation and interaction for distance education. Four platforms, HTC VIVE Pro, Microsoft HoloLens 2, PC, and Android smartphone, are supported. The virtual lab allows trainers to demonstrate operation steps and engage with multiple trainees simultaneously. Meanwhile, trainees have the opportunity to practice their operational skills on their virtual machines within the Metaverse, utilizing their preferred platforms. Additionally, participants can explore the virtual environment and interact with each other by moving around within the virtual space, similar to a physical lab setting. The user test compares the levels of presence and usability in the hands-on virtual lab across different platforms, providing insights into the challenges associated with each platform within the Metaverse for training purposes. Furthermore, the results of the user test highlight the promising potential of the architecture due to its flexibility and adaptability.
C1 [Chuang, Tung-Jui; Smith, Shana] Natl Taiwan Univ, Dept Mech Engn, Taipei, Taiwan.
C3 National Taiwan University
RP Smith, S (corresponding author), Natl Taiwan Univ, Dept Mech Engn, Taipei, Taiwan.
EM ssmith@ntu.edu.tw
FU The Ministry of Science and Technology, Taiwan [MOST
   110-2221-E-002-145]; Ministry of Science and Technology, Taiwan,
   Republic of China
FX The authors would like to thank the Ministry of Science and Technology,
   Taiwan, Republic of China for financially supporting this research under
   Contract MOST 110-2221-E-002-145.
CR Annetta L., 2008, Innovate, V4
   Annetta L.A., 2009, Journal of College Science Teaching, V39, P27
   Aziz ESS, 2014, COMPUT APPL ENG EDUC, V22, P788, DOI 10.1002/cae.21573
   Bangor A, 2008, INT J HUM-COMPUT INT, V24, P574, DOI 10.1080/10447310802205776
   Bao L, 2022, AUTOMAT CONSTR, V143, DOI 10.1016/j.autcon.2022.104565
   Brooke J., 1996, SUS-a quick and dirty usability scale, DOI [DOI 10.1201/9781498710411-35, DOI 10.1201/9781498710411]
   Buttussi F, 2018, IEEE T VIS COMPUT GR, V24, P1063, DOI 10.1109/TVCG.2017.2653117
   Chan PLP, 2023, SAFETY SCI, V167, DOI 10.1016/j.ssci.2023.106278
   Chan P, 2021, COMPUT EDUC OPEN, V2, DOI 10.1016/j.caeo.2021.100053
   Chen E, 2021, J DENT EDUC, V85, P1190, DOI 10.1002/jdd.12339
   Chin J.P., 1988, P SIGCHI C HUM FACT, P213, DOI [DOI 10.1145/57167.57203, 10.1145/57167.57203]
   Cohen J., 1988, Statistical power analyses for behavioral sciences, V2nd, DOI [10.4324/9780203771587, DOI 10.4324/9780203771587]
   Dalgarno B, 2009, COMPUT EDUC, V53, P853, DOI 10.1016/j.compedu.2009.05.005
   Delamarre A, 2020, 2020 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2020), P86, DOI 10.1109/CW49994.2020.00020
   Dickson-Karn NM, 2020, J CHEM EDUC, V97, P2955, DOI 10.1021/acs.jchemed.0c00578
   Elfakki AO, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12040989
   Faulconer EK, 2018, INT REV RES OPEN DIS, V19, P155
   Harfouche AL, 2020, TRENDS BIOTECHNOL, V38, P1187, DOI 10.1016/j.tibtech.2020.05.005
   Hurtado-Bermúdez S, 2023, INTERACT LEARN ENVIR, V31, P1126, DOI 10.1080/10494820.2020.1821716
   Hwang Y, 2023, COMPUT EDUC, V194, DOI 10.1016/j.compedu.2022.104693
   Inceoglu MM, 2022, LECT NOTES COMPUT SC, V13377, P171, DOI 10.1007/978-3-031-10536-4_12
   Jaggars SSE, 2013, Georgia West Creating an Effective Online Instructor Presence
   Kambili-Mzembe F, 2022, 2022 8 INT C IMMERSI, P1, DOI [10.23919/iLRN55037.2022.9815966, DOI 10.23919/ILRN55037.2022.9815966]
   Khan V, 2022, 2022 ASEE ILLINOIS I, P36131
   Kiourt C, 2020, PROCEEDINGS OF THE 12TH INTERNATIONAL CONFERENCE ON COMPUTER SUPPORTED EDUCATION (CSEDU), VOL 1, P601, DOI 10.5220/0009441606010608
   Kondratiuk L, 2022, BRAIN-BROAD RES ARTI, V13, P22, DOI 10.18662/brain/13.2/329
   Li R., 2020, Adv. Eng. Educ., V8, P1
   Manyilizu MC, 2023, EDUC INF TECHNOL, V28, P4831, DOI 10.1007/s10639-022-11327-7
   May D, 2023, EUR J ENG EDUC, V48, P842, DOI 10.1080/03043797.2022.2046707
   McFaul H, 2020, BRIT J EDUC TECHNOL, V51, P572, DOI 10.1111/bjet.12850
   Monahan T, 2008, COMPUT EDUC, V50, P1339, DOI 10.1016/j.compedu.2006.12.008
   Mystakidis S., 2022, ENCYCLOPEDIA, V2, P486, DOI [https://doi.org/10.3390/encyclopedia2010031, 10.3390/encyclopedia2010031, DOI 10.3390/ENCYCLOPEDIA2010031]
   Mystakidis S, 2022, EDUC INF TECHNOL, V27, P1883, DOI 10.1007/s10639-021-10682-1
   Potkonjak V, 2016, COMPUT EDUC, V95, P309, DOI 10.1016/j.compedu.2016.02.002
   Potkonjak V, 2010, COMPUT EDUC, V55, P465, DOI 10.1016/j.compedu.2010.02.010
   Reeves SM, 2021, J SCI EDUC TECHNOL, V30, P16, DOI 10.1007/s10956-020-09866-0
   Siyaev A, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21062066
   Sung CJ, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12173686
   Witmer BG, 2005, PRESENCE-TELEOP VIRT, V14, P298, DOI 10.1162/105474605323384654
   Xu XH, 2022, J EDUC COMPUT RES, V60, P455, DOI 10.1177/07356331211036492
NR 40
TC 0
Z9 0
U1 8
U2 8
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 62
DI 10.1007/s10055-024-00974-5
PG 16
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA JV9Q8
UT WOS:001176059900001
OA hybrid
DA 2024-08-05
ER

PT J
AU Luo, XY
   Wang, YY
   Lee, LH
   Xing, ZH
   Jin, S
   Dong, BY
   Hu, YY
   Chen, ZM
   Yan, J
   Hui, P
AF Luo, Xinyi
   Wang, Yuyang
   Lee, Lik-Hang
   Xing, Zihan
   Jin, Shan
   Dong, Boya
   Hu, Yuanyi
   Chen, Zeming
   Yan, Jing
   Hui, Pan
TI Using a virtual reality interview simulator to explore factors
   influencing people's behavior
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; User interfaces; Interview; Anxiety
ID SOCIAL STRESS TEST; ANXIETY; PERFORMANCE; METAANALYSIS; EXPOSURE;
   THERAPY; QUALITY; AUTISM; ADULTS
AB Virtual reality interview simulator (VRIS) is an effective and valid tool that uses virtual reality technology to train people's interview skills. Typically, it offers candidates prone to being very nervous during interviews the opportunity to practice interviews in a safe and manageable virtual environment and realistic settings, providing real-time feedback from a virtual interviewer on their performance. It helps interviewees improve their skills, reduce their fears, gain confidence, and minimize the cost and time associated with traditional interview preparation. Yet, the major anxiety-inducing elements remain unknown. During an interview, the anxiety levels, overall experience, and performance of interviewees might be affected by various circumstances. By analyzing electrodermal activity and questionnaire, we investigated the influence of five variables: (I) Realism; (II) Question type; (III) Interviewer attitude; (IV) Timing; and (V) Preparation. As such, an orthogonal design L8(41x24)\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$L_8(4<^>1 \times 2<^>4)$$\end{document} with eight experiments (OA8\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$O A_8$$\end{document} matrix) was implemented, in which 19 college students took part in the experiments. Considering the anxiety, overall experience, and performance of the interviewees, we found that Question type plays a major role; secondly, Realism, Preparation, and Interviewer attitude all have middle influence; lastly, Timing has little to no impact. Specifically, professional interview questions elicited a greater degree of anxiety than personal ones among the categories of interview questions. This work contributes to our understanding of anxiety-stimulating factors during job interviews in virtual reality and provides cues for designing future VRIS.
C1 [Luo, Xinyi] Univ Elect Sci & Technol China, Jianshe Rd, Chengdu 610000, Sichuan, Peoples R China.
   [Luo, Xinyi; Wang, Yuyang; Jin, Shan; Dong, Boya; Chen, Zeming; Yan, Jing; Hui, Pan] Hong Kong Univ Sci & Technol Guangzhou, Duxue Rd, Guangzhou 511453, Guangdong, Peoples R China.
   [Lee, Lik-Hang] Hong Kong Polytech Univ, Hung Hom, 11 Yuk Choi Rd, Hong Kong 999077, Peoples R China.
   [Xing, Zihan] BNU HKBU United Int Coll, 2000 Jintong Rd, Zhuhai 519087, Guangdong, Peoples R China.
   [Hu, Yuanyi] Guangdong Med Univ, 1 City Ave, Dongguan 523109, Guangdong, Peoples R China.
C3 University of Electronic Science & Technology of China; Hong Kong
   University of Science & Technology (Guangzhou); Hong Kong Polytechnic
   University; Beijing Normal University - Hong Kong Baptist University
   United International College; Guangdong Medical University
RP Wang, YY (corresponding author), Hong Kong Univ Sci & Technol Guangzhou, Duxue Rd, Guangzhou 511453, Guangdong, Peoples R China.
EM 2020090903013@std.uestc.edu.cn; yuyangwang@hkust-gz.edu.cn;
   lik-hang.lee@polyu.edu.hk; p930026137@mail.uic.edu.cn;
   sjin752@connect.hkust-gz.edu.cn; 201930360251@mail.scut.edu.cn;
   19211080272@gdmu.edu.cn; 201930130250@mail.scut.edu.cn;
   yanjingcn01@gmail.com; panhui@ust.hk
RI Lee, PhD, Lik-Hang (Paul)/IST-7358-2023
OI Lee, PhD, Lik-Hang (Paul)/0000-0003-1361-1612; Wang,
   Yuyang/0000-0003-0242-8935
FU HKUST; Human Resources and Social Security Department of Guangzhou City
   and Nansha District, Guangdong, China
FX The authors appreciate the financial support for research from the Human
   Resources and Social Security Department of Guangzhou City and Nansha
   District, Guangdong, China.
CR ADDELMAN S, 1962, TECHNOMETRICS, V4, P21, DOI 10.2307/1266170
   Adiani D, 2022, ACM T ACCESS COMPUT, V15, DOI 10.1145/3505560
   Albright Glenn, 2021, JMIR Form Res, V5, pe27164, DOI 10.2196/27164
   Aysina RM, 2017, EUR J PSYCHOL, V13, P251, DOI 10.5964/ejop.v13i2.1250
   Baños RM, 2002, IEEE T INF TECHNOL B, V6, P206, DOI 10.1109/TITB.2002.802380
   Barreda-Angeles M, 2020, VIRTUAL REAL-LONDON, V24, P289, DOI 10.1007/s10055-019-00400-1
   Bates D, 2014, Arxiv, DOI [arXiv:1406.5823, DOI 10.48550/ARXIV.1406.5823]
   Botella C, 2007, CLIN PSYCHOL PSYCHOT, V14, P164, DOI 10.1002/cpp.524
   Bradley ME, 2008, BRIT J EDUC TECHNOL, V39, P888, DOI 10.1111/j.1467-8535.2007.00804.x
   Braithwaite J.J., 2013, PSYCHOPHYSIOLOGY, P1
   Burke SL, 2018, J AUTISM DEV DISORD, V48, P905, DOI 10.1007/s10803-017-3374-z
   Clus D, 2018, J MED INTERNET RES, V20, DOI 10.2196/jmir.7898
   Coelho CM, 2006, CYBERPSYCHOL BEHAV, V9, P336, DOI 10.1089/cpb.2006.9.336
   Didehbani N, 2016, COMPUT HUM BEHAV, V62, P703, DOI 10.1016/j.chb.2016.04.033
   Ellis APJ, 2002, J APPL PSYCHOL, V87, P1200, DOI 10.1037//0021-9010.87.6.1200
   Emmelkamp PMG, 2020, CURR PSYCHIAT REP, V22, DOI 10.1007/s11920-020-01156-1
   Fallon MA, 2021, INT J PSYCHOPHYSIOL, V161, P27, DOI 10.1016/j.ijpsycho.2021.01.010
   Feiler AR, 2016, J BUS PSYCHOL, V31, P155, DOI 10.1007/s10869-015-9403-z
   Feiler AR, 2010, Interviewee and interviewer ratings of job interview anxiety: an investigation of the discrepancy using the Brunswik lens model
   Gantt LT, 2013, CLIN SIMUL NURS, V9, pE25, DOI 10.1016/j.ecns.2011.07.004
   Garcia-Palacios A, 2002, BEHAV RES THER, V40, P983, DOI 10.1016/S0005-7967(01)00068-7
   Gebhard P, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P661
   Gee S., 1999, LEGAL CRIMINOLOGICAL, V4, P111, DOI [10.1348/135532599167716, DOI 10.1348/135532599167716]
   Gonzalez-Franco M, 2020, IEEE T VIS COMPUT GR, V26, P2023, DOI 10.1109/TVCG.2020.2973075
   Greco A., 2016, Advances in electrodermal activity processing with applications for mental health: From heuristic methods to convex optimization, DOI [10.1007/978-3-319-46705-4, DOI 10.1007/978-3-319-46705-4]
   Hansen K, 2006, DEV BUSINESS SIMULAT, V33
   Hart S. G., 1986, Nasa task load index (tlx)
   Hartholt A, 2019, PROCEEDINGS OF THE 19TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA' 19), P238, DOI 10.1145/ivade780
   Hartwell CJ, 2019, J BUS RES, V100, P122, DOI 10.1016/j.jbusres.2019.03.026
   Helminen EC, 2019, PSYCHONEUROENDOCRINO, V110, DOI 10.1016/j.psyneuen.2019.104437
   HILL SG, 1992, HUM FACTORS, V34, P429, DOI 10.1177/001872089203400405
   Howell AN, 2016, COGN BEHAV THERAPY, V45, P111, DOI 10.1080/16506073.2015.1111932
   Huffcutt AI, 2011, HUM RESOUR MANAGE R, V21, P353, DOI 10.1016/j.hrmr.2011.05.003
   Jin XP, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P998, DOI [10.1109/VR.2019.8797764, 10.1109/vr.2019.8797764]
   Johnston M., 1995, MEASURES HLTH PSYCHO
   Kampmann IL, 2016, J ANXIETY DISORD, V42, P71, DOI 10.1016/j.janxdis.2016.06.007
   Kerous B, 2020, J AMB INTEL HUM COMP, V11, P6033, DOI 10.1007/s12652-020-01858-7
   Kothgassner OD, 2021, PHYSIOL BEHAV, V242, DOI 10.1016/j.physbeh.2021.113618
   Kritikos J, 2019, I IEEE EMBS C NEUR E, P571, DOI 10.1109/ner.2019.8717170
   Kwon JH, 2009, P 25 SPRING C COMPUT, P167, DOI [10.1145/1980462.1980495, DOI 10.1145/1980462.1980495]
   Kwon JH, 2013, INT J HUM-COMPUT ST, V71, P978, DOI 10.1016/j.ijhcs.2013.07.003
   Lugrin JL, 2015, P IEEE VIRT REAL ANN, P227, DOI 10.1109/VR.2015.7223378
   Makowski D, 2021, BEHAV RES METHODS, V53, P1689, DOI 10.3758/s13428-020-01516-y
   McCarthy J, 2004, PERS PSYCHOL, V57, P607, DOI 10.1111/j.1744-6570.2004.00002.x
   Mok KH, 2021, J EDUC WORK, V34, P247, DOI 10.1080/13639080.2021.1922620
   MORRIS LW, 1969, J CONSULT CLIN PSYCH, V33, P240, DOI 10.1037/h0027164
   North M. M., 1998, International Journal of Virtual Reality, V3, P2
   Parsons TD, 2008, J BEHAV THER EXP PSY, V39, P250, DOI 10.1016/j.jbtep.2007.07.007
   Pollak Levine S., 2002, APPL HRM RES, V7, P1
   Powell DM, 2018, CAN J BEHAV SCI, V50, P195, DOI 10.1037/cbs0000108
   Romano DM, 2001, CYBERPSYCHOL BEHAV, V4, P265, DOI 10.1089/109493101300117947
   Roos AL, 2021, EDUC PSYCHOL REV, V33, P579, DOI 10.1007/s10648-020-09543-z
   Schwartz SM, 2015, ANAT SCI EDUC, V8, P518, DOI 10.1002/ase.1508
   Smith MJ, 2017, PSYCHIAT SERV, V68, P747, DOI 10.1176/appi.ps.201600217
   Smith MJ, 2014, J AUTISM DEV DISORD, V44, P2450, DOI 10.1007/s10803-014-2113-y
   Souchet AD, 2022, VIRTUAL REAL-LONDON, V26, P583, DOI 10.1007/s10055-021-00548-9
   Wallergård M, 2011, PRESENCE-VIRTUAL AUG, V20, P325, DOI 10.1162/PRES_a_00052
   Wang YY, 2021, INT J HUM-COMPUT ST, V147, DOI 10.1016/j.ijhcs.2020.102572
   Wang YY, 2021, INT J HUM-COMPUT ST, V147, DOI 10.1016/j.ijhcs.2020.102578
   Wenwang Huang, 2021, EITCE 2021: Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering, P845, DOI 10.1145/3501409.3501561
   Zeng G, 2022, CURR PSYCHOL, V41, P3970, DOI 10.1007/s12144-020-00924-9
   ZHANG JX, 1995, PSYCHOLOGIA, V38, P174
   Zielinska AP, 2021, ADV MED EDUC PRACT, V12, P675, DOI 10.2147/AMEP.S306394
   Zimmer P, 2019, PHYSIOL BEHAV, V212, DOI 10.1016/j.physbeh.2019.112690
NR 64
TC 0
Z9 0
U1 9
U2 9
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 56
DI 10.1007/s10055-023-00934-5
PG 20
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA JD0T4
UT WOS:001171108800002
OA hybrid
DA 2024-08-05
ER

PT J
AU Wen, PP
   Lu, F
   Ali, AZM
AF Wen, Pingping
   Lu, Fei
   Ali, Ahmad Zamzuri Mohamad
TI Using attentional guidance methods in virtual reality laboratories
   reduces students' cognitive load and improves their academic performance
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality lab; Attentional guidance; Cognitive load; Academic
   performance
ID VISUAL WORKING-MEMORY; INSTRUCTIONAL-DESIGN; CAPTURE; COLOR; LUMINANCE;
   MECHANISMS; FRAMEWORK; CUES
AB Learning in virtual reality laboratories (VR labs) has become an important method in experimental teaching but can increase individuals' cognitive load compared with traditional laboratories. This study analysed the effect of introducing an attentional guidance mechanism into a VR lab on students' cognitive load and academic performance. We designed and developed two VR labs, one with and one without this attentional guidance stimulus (a 3D yellow arrow). A quasi-experimental design was adopted, and the data obtained were analysed using one-way ANOVA and linear regression. The experiment was conducted with 80 students majoring in digital media art at two universities. The results indicated that the students in the VR lab with the attentional guidance mechanism included exhibited lower cognitive load and higher academic performance than the control group. The regression analyses revealed that cognitive load negatively predicted learning outcomes; that is, academic performance improved as cognitive load decreased. In conclusion, as VR labs are increasingly used in education, supplementing them with attentional guidance stimuli can improve students' academic performance by reducing their cognitive load.
C1 [Wen, Pingping] Chongqing City Vocat Coll, Fac Creat Design, Chongqing, Peoples R China.
   [Lu, Fei] Qiqihar Univ, Fac Fine Arts & Art Design, Qiqihar, Peoples R China.
   [Ali, Ahmad Zamzuri Mohamad] Univ Pendidikan Sultan Idris, Fac Art Sustainabil & Creat Ind, Tanjung Malim, Perak, Malaysia.
C3 Qiqihar University; Universiti Pendidikan Sultan Idris
RP Wen, PP (corresponding author), Chongqing City Vocat Coll, Fac Creat Design, Chongqing, Peoples R China.
EM winnie7563626@gmail.com; jiaopian01@gmail.com; zamzuri@fskik.upsi.edu.my
FU Chongqing City Vocational College Education and Teaching Reform Research
   Project
FX No Statement Available
CR Achuthan K, 2020, INT J ONLINE BIOMED, V16, P4, DOI 10.3991/ijoe.v16i09.11685
   Albus P, 2021, COMPUT EDUC, V166, DOI 10.1016/j.compedu.2021.104154
   Ali AZM, 2019, INT J HUM-COMPUT INT, V35, P1125, DOI 10.1080/10447318.2018.1511180
   Ali N, 2022, EDUC INF TECHNOL, V27, P7629, DOI 10.1007/s10639-022-10936-6
   Andersen Steven Arild Wuyts, 2016, Adv Simul (Lond), V1, P20, DOI 10.1186/s41077-016-0022-1
   [Anonymous], 2015, Advances in intelligent informatics, DOI DOI 10.1007/978-3-319-11218-3_15
   Ansorge U, 2014, PSYCHOL RES-PSYCH FO, V78, P209, DOI 10.1007/s00426-013-0497-5
   BADDELEY A, 1992, SCIENCE, V255, P556, DOI 10.1126/science.1736359
   BADDELEY A, 1992, Science (Washington D C), V255, P556, DOI 10.1016/j.cub.2009.12.014
   Baddeley A, 2003, NAT REV NEUROSCI, V4, P829, DOI 10.1038/nrn1201
   Brucker B, 2014, COMPUT HUM BEHAV, V36, P330, DOI 10.1016/j.chb.2014.03.077
   Burnham BR, 2020, ATTEN PERCEPT PSYCHO, V82, P1003, DOI 10.3758/s13414-019-01850-0
   Canham M, 2010, LEARN INSTR, V20, P155, DOI 10.1016/j.learninstruc.2009.02.014
   Cohen J., 1988, Statistical power analysis for the behavioral sciences
   de Jong T, 2010, INSTR SCI, V38, P105, DOI 10.1007/s11251-009-9110-0
   de Koning BB, 2009, EDUC PSYCHOL REV, V21, P113, DOI 10.1007/s10648-009-9098-7
   DESIMONE R, 1995, ANNU REV NEUROSCI, V18, P193, DOI 10.1146/annurev-psych-122414-033400
   Downing PE, 2000, PSYCHOL SCI, V11, P467, DOI 10.1111/1467-9280.00290
   Du X, 2023, J COMPUT HIGH EDUC, V35, P272, DOI 10.1007/s12528-022-09311-8
   El Kabtane H, 2020, EDUC INF TECHNOL, V25, P2871, DOI 10.1007/s10639-019-10054-w
   Enns JT, 2001, J EXP PSYCHOL HUMAN, V27, P1287, DOI 10.1037/0096-1523.27.6.1287
   Frederiksen JG, 2020, SURG ENDOSC, V34, P1244, DOI 10.1007/s00464-019-06887-8
   Gathercole SE, 2008, LEARN INDIVID DIFFER, V18, P214, DOI 10.1016/j.lindif.2007.10.003
   Ge YP, 2017, INT J SCI EDUC, V39, P605, DOI 10.1080/09500693.2017.1297549
   Grivokostopoulou F, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10051739
   Guadagnoli MA, 2004, J MOTOR BEHAV, V36, P212, DOI 10.3200/JMBR.36.2.212-224
   Harada Y, 2022, VIRTUAL REAL-LONDON, V26, P759, DOI 10.1007/s10055-021-00574-7
   Harris AM, 2015, ATTEN PERCEPT PSYCHO, V77, P2305, DOI 10.3758/s13414-015-0927-0
   Hart SG., 2006, P HUM FACT ERG SOC A, V50, P904, DOI [10.1177/154193120605000909, DOI 10.1177/154193120605000909]
   Höffler TN, 2010, EDUC PSYCHOL REV, V22, P245, DOI 10.1007/s10648-010-9126-7
   Hohner Nils, 2020, Virtual Reality and Augmented Reality. 17th EuroVR International Conference, EuroVR 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12499), P99, DOI 10.1007/978-3-030-62655-6_6
   Janonis A., 2020, Information and Software Technologies: 26th International Conference, ICIST 2020, Kaunas, Lithuania, October 15-17, 2020, Proceedings, P273, DOI [10.1007/978-3-030-59506-7_22, DOI 10.1007/978-3-030-59506-7_22]
   Juliano JM, 2022, J NEUROENG REHABIL, V19, DOI 10.1186/s12984-022-01084-6
   Kalet A, 2012, MED TEACH, V34, P833, DOI 10.3109/0142159X.2012.706727
   Kehrwald B.A., 2020, Mobility, data and learner agency in networked learning, P103, DOI DOI 10.1007/978-3-030-36911-8_7
   Kirschner PA, 2002, LEARN INSTR, V12, P1, DOI 10.1016/S0959-4752(01)00014-7
   Lee H, 2021, MULTIMED TOOLS APPL, V80, P31239, DOI 10.1007/s11042-020-10267-z
   Liberatore MJ, 2021, VIRTUAL REAL-LONDON, V25, P773, DOI 10.1007/s10055-020-00492-0
   Longo L., 2018, Computer supported education: 10th international conference, P384
   Makransky G, 2021, J EDUC PSYCHOL, V113, P719, DOI 10.1037/edu0000473
   Makransky G, 2019, LEARN INSTR, V60, P225, DOI 10.1016/j.learninstruc.2017.12.007
   Mayer R. E., 2005, The Cambridge handbook of multimedia learning, DOI DOI 10.1017/CBO9780511816819
   Mayer RE, 2010, LEARN INSTR, V20, P167, DOI 10.1016/j.learninstruc.2009.02.012
   Mikropoulos TA, 2011, COMPUT EDUC, V56, P769, DOI 10.1016/j.compedu.2010.10.020
   Moon J, 2021, J COMPUT HIGH EDUC, V33, P39, DOI 10.1007/s12528-020-09255-x
   Nagy AL, 2000, J OPT SOC AM A, V17, P369, DOI 10.1364/JOSAA.17.000369
   Olivers CNL, 2006, J EXP PSYCHOL HUMAN, V32, P1243, DOI 10.1037/0096-1523.32.5.1243
   Ozcelik E, 2010, COMPUT HUM BEHAV, V26, P110, DOI 10.1016/j.chb.2009.09.001
   Paas F, 2003, EDUC PSYCHOL-US, V38, P63, DOI 10.1207/S15326985EP3801_8
   Paas F, 2003, EDUC PSYCHOL, V38, P1, DOI 10.1207/S15326985EP3801_1
   Paas F, 2004, INSTR SCI, V32, P1, DOI 10.1023/B:TRUC.0000021806.17516.d0
   Papachristos N.M., 2018, Research on e-learning and ICT in education, P367, DOI [10.1007/978-3-319-95059-4_22, DOI 10.1007/978-3-319-95059-4_22]
   Park S, 2020, NEUROSCI LETT, V733, DOI 10.1016/j.neulet.2020.134974
   Parong J, 2021, ETR&D-EDUC TECH RES, V69, P1433, DOI 10.1007/s11423-021-09999-y
   Parong J, 2018, J EDUC PSYCHOL, V110, P785, DOI 10.1037/edu0000241
   Pollock E, 2002, LEARN INSTR, V12, P61, DOI 10.1016/S0959-4752(01)00016-0
   Schnotz W, 2008, LEARNING WITH ANIMATION: RESEARCH IMPLICATIONS FOR DESIGN, P304
   Shin DH, 2017, TELEMAT INFORM, V34, P1826, DOI 10.1016/j.tele.2017.05.013
   Shin D, 2019, J COMPUT HIGH EDUC, V31, P210, DOI 10.1007/s12528-019-09205-2
   Skulmowski A, 2022, EDUC PSYCHOL REV, V34, P171, DOI 10.1007/s10648-021-09624-7
   Skulmowski A, 2020, HUM BEHAV EMERG TECH, V2, P251, DOI 10.1002/hbe2.190
   Skulmowski A, 2020, HUM BEHAV EMERG TECH, V2, P149, DOI 10.1002/hbe2.184
   Skulmowski A, 2018, COGN RES, V3, DOI 10.1186/s41235-018-0092-9
   Skulmowski A, 2016, COMPUT EDUC, V92-93, P64, DOI 10.1016/j.compedu.2015.10.011
   Soto D, 2005, J EXP PSYCHOL HUMAN, V31, P248, DOI 10.1037/0096-1523.31.2.248
   Sweller J, 2016, J APPL RES MEM COGN, V5, P360, DOI 10.1016/j.jarmac.2015.12.002
   Sweller J, 2011, PSYCHOL LEARN MOTIV, V55, P37
   Sweller J, 2010, EDUC PSYCHOL REV, V22, P123, DOI 10.1007/s10648-010-9128-5
   TREISMAN A, 1988, PSYCHOL REV, V95, P15, DOI 10.1037/0033-295X.95.1.15
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Tugtekin U, 2022, EDUC INF TECHNOL, V27, P7019, DOI 10.1007/s10639-022-10912-0
   Turatto M, 2000, VISION RES, V40, P1639, DOI 10.1016/S0042-6989(00)00061-4
   Turatto M, 2004, Q J EXP PSYCHOL-A, V57, P297, DOI 10.1080/02724980343000242
   Turatto M, 2001, PERCEPT PSYCHOPHYS, V63, P286, DOI 10.3758/BF03194469
   van Merriënboer JJG, 2005, EDUC PSYCHOL REV, V17, P147, DOI 10.1007/s10648-005-3951-0
   Wallgrün JO, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P83, DOI [10.1109/VR46266.2020.1581092881445, 10.1109/VR46266.2020.00-78]
   Wenk N, 2023, VIRTUAL REAL-LONDON, V27, P307, DOI 10.1007/s10055-021-00565-8
   Yang E, 2023, J COMPUT HIGH EDUC, V35, P245, DOI 10.1007/s12528-022-09314-5
   Yantis S, 1999, J EXP PSYCHOL HUMAN, V25, P661, DOI 10.1037/0096-1523.25.3.661
   Zhao J, 2020, ASIA-PAC EDUC RES, V29, P473, DOI 10.1007/s40299-020-00499-w
NR 80
TC 1
Z9 1
U1 8
U2 8
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAY 16
PY 2024
VL 28
IS 2
AR 110
DI 10.1007/s10055-024-01012-0
PG 10
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA RI4E1
UT WOS:001227013600001
OA hybrid
DA 2024-08-05
ER

PT J
AU Yashin, AS
   Lavrov, DS
   Melnichuk, EV
   Karpov, VV
   Zhao, DG
   Dubynin, IA
AF Yashin, Artem S.
   Lavrov, Daniil S.
   Melnichuk, Eugeny V.
   Karpov, Valery V.
   Zhao, Darisy G.
   Dubynin, Ignat A.
TI Robot remote control using virtual reality headset: studying sense of
   agency with subjective distance estimates
SO VIRTUAL REALITY
LA English
DT Article
DE Sense of agency; Sense of control; Remote control; Virtual reality;
   Intentional binding; Distance estimation
ID INTENTIONAL BINDING; MOTION SICKNESS; EXPERIENCE; MOVEMENTS; AWARENESS;
   FEEL; TIME
AB Mobile robots have many applications in the modern world. The autonomy of robots is increasing, but critical cases like search and rescue missions must involve the possibility of human intervention for ethical reasons and safety. To achieve effective human-robot interaction, the operator needs to have a sense of agency (SoA) over the activities of the robot. One possible way to increase one's SoA in remote control could be the use of VR technology. The remote control situation has some important features, so indicators of SoA need to be reproduced there independently. In our study, participants controlled a mobile robot using either a monitor or a VR-headset as an output device. In both cases, active control was contrasted with passive observation of the robot's movement. In each trial, participants estimated the distance traveled by the robot-a putative implicit indicator of SoA. A significant difference between subjective distance estimates was found in the active and passive conditions with the monitor, but not in the active and passive conditions with VR. The effect obtained in the monitor conditions suggests that distance estimates can be used as an implicit indicator of SoA in robot remote control. We believe that the lack of difference between the active and passive conditions in VR was caused by motion sickness due to a mismatch of visual and vestibular sensory cues, leading to a weakened SoA.
C1 [Yashin, Artem S.; Lavrov, Daniil S.; Melnichuk, Eugeny V.; Karpov, Valery V.; Zhao, Darisy G.] Natl Res Ctr, Kurchatov Inst, Moscow, Russia.
   [Yashin, Artem S.] Moscow MV Lomonosov State Univ, Moscow, Russia.
   [Lavrov, Daniil S.] Moscow Inst Phys & Technol, Moscow, Russia.
   [Yashin, Artem S.; Zhao, Darisy G.; Dubynin, Ignat A.] Moscow State Univ Psychol & Educ, MEG Ctr, Moscow, Russia.
C3 National Research Centre - Kurchatov Institute; Lomonosov Moscow State
   University; Moscow Institute of Physics & Technology; Moscow State
   University of Psychology & Education
RP Yashin, AS (corresponding author), Natl Res Ctr, Kurchatov Inst, Moscow, Russia.; Yashin, AS (corresponding author), Moscow MV Lomonosov State Univ, Moscow, Russia.; Yashin, AS (corresponding author), Moscow State Univ Psychol & Educ, MEG Ctr, Moscow, Russia.
EM yashinart1996@gmail.com
OI Zhao, Darisy/0000-0003-0211-7336
FU NRC "Kurchatov Institute" [86]
FX This work was supported by NRC "Kurchatov Institute", order No. 86 from
   20.01.2023.
CR Alatise MB, 2020, IEEE ACCESS, V8, P39830, DOI 10.1109/ACCESS.2020.2975643
   Bayne T, 2007, SYNTHESE, V159, P475, DOI 10.1007/s11229-007-9239-9
   BENJAMINI Y, 1995, J R STAT SOC B, V57, P289, DOI 10.1111/j.2517-6161.1995.tb02031.x
   Berberian B, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0034075
   Bodenheimer B, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P35
   Bratman M.E., 2009, Philosophy of the social sciences: Philosophical theory and scientific practice, P41, DOI [DOI 10.1017/CBO9780511812880.005, DOI 10.1017/CB097805118:12880.005]
   Braun N, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0111967
   Buehner MJ, 2012, PSYCHOL SCI, V23, P1490, DOI 10.1177/0956797612444612
   Buehner MJ, 2010, PSYCHOL SCI, V21, P44, DOI 10.1177/0956797609354735
   Buehner MJ, 2009, PSYCHOL SCI, V20, P1221, DOI 10.1111/j.1467-9280.2009.02435.x
   Carnegie K, 2015, IEEE COMPUT GRAPH, V35, P34, DOI 10.1109/MCG.2015.98
   Chang E, 2020, INT J HUM-COMPUT INT, V36, P1658, DOI 10.1080/10447318.2020.1778351
   Chattha UA, 2020, IEEE ACCESS, V8, P130486, DOI 10.1109/ACCESS.2020.3007076
   Christensen MS, 2018, CONSCIOUS COGN, V65, P27, DOI 10.1016/j.concog.2018.07.002
   Cravo AM, 2011, ACTA PSYCHOL, V136, P157, DOI 10.1016/j.actpsy.2010.11.005
   Creem-Regehr SH, 2005, PERCEPTION, V34, P191, DOI 10.1068/p5144
   Crnkovic GD, 2012, ETHICS INF TECHNOL, V14, P61, DOI 10.1007/s10676-011-9278-2
   Cutting JE, 1995, PERCEPTION SPACE MOT, P69, DOI [DOI 10.1016/B978-012240530-3/50005-5, 10.1016/B978-012240530-3/50005-5]
   Desantis A, 2011, CONSCIOUS COGN, V20, P1211, DOI 10.1016/j.concog.2011.02.012
   Dewey JA, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0110118
   El Jamiy F, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON ELECTRO INFORMATION TECHNOLOGY (EIT), P63, DOI [10.1109/EIT.2019.8834182, 10.1109/eit.2019.8834182]
   Engbert K, 2008, COGNITION, V107, P693, DOI 10.1016/j.cognition.2007.07.021
   Freitag S, 2016, IEEE T VIS COMPUT GR, V22, P1462, DOI 10.1109/TVCG.2016.2518298
   Frith CD, 2000, PHILOS T R SOC B, V355, P1771, DOI 10.1098/rstb.2000.0734
   Gogoll J, 2017, SCI ENG ETHICS, V23, P681, DOI 10.1007/s11948-016-9806-x
   Gunkel DJ, 2020, ETHICS INF TECHNOL, V22, P307, DOI 10.1007/s10676-017-9428-2
   Habibian S, 2021, ROBOMECH J, V8, DOI 10.1186/s40648-020-00188-9
   Haering C, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00393
   Haggard P, 2002, NAT NEUROSCI, V5, P382, DOI 10.1038/nn827
   Haggard P, 2009, CURR DIR PSYCHOL SCI, V18, P242, DOI 10.1111/j.1467-8721.2009.01644.x
   Hall Ned, 2004, CAUSATION COUNTERFAC, P225, DOI DOI 10.7551/MITPRESS/1752.003.0010
   Heer M, 2006, AUTON NEUROSCI-BASIC, V129, P77, DOI 10.1016/j.autneu.2006.07.014
   Hughes G, 2013, PSYCHOL BULL, V139, P133, DOI 10.1037/a0028566
   Humphreys GR, 2009, J EXP PSYCHOL HUMAN, V35, P1542, DOI 10.1037/a0014492
   Imaizumi S, 2019, CONSCIOUS COGN, V67, P1, DOI 10.1016/j.concog.2018.11.005
   Jankowski J, 2015, INT J HUM-COMPUT INT, V31, P882, DOI 10.1080/10447318.2015.1039909
   Jenkins M, 2021, PSYCHOL RES-PSYCH FO, V85, P3167, DOI 10.1007/s00426-020-01462-6
   Jeunet C, 2018, IEEE T VIS COMPUT GR, V24, P1486, DOI 10.1109/TVCG.2018.2794598
   Kawala-Sterniuk A, 2021, BRAIN SCI, V11, DOI 10.3390/brainsci11010043
   Keil J, 2021, ISPRS INT J GEO-INF, V10, DOI 10.3390/ijgi10030150
   Keshavarz B, 2011, DISPLAYS, V32, P181, DOI 10.1016/j.displa.2011.05.009
   Kirsch W, 2016, ATTEN PERCEPT PSYCHO, V78, P133, DOI 10.3758/s13414-015-0997-z
   Kong GQ, 2017, CONSCIOUS COGN, V52, P115, DOI 10.1016/j.concog.2017.04.018
   Lewis D., 1973, Journal of Philosophy, V70, P556
   LIBET B, 1983, BRAIN, V106, P623, DOI 10.1093/brain/106.3.623
   Llorach G, 2014, P ACM S VIRT REAL SO, P137
   Lush P, 2019, J EXP PSYCHOL HUMAN, V45, P1206, DOI 10.1037/xhp0000661
   Martins H, 2009, IEEE INT C EMERG
   Maselli A, 2014, FRONT HUM NEUROSCI, V8, DOI 10.3389/fnhum.2014.00693
   Miller KW, 2011, IT PROF, V13, P57, DOI 10.1109/MITP.2011.46
   Moore J, 2008, CONSCIOUS COGN, V17, P136, DOI 10.1016/j.concog.2006.12.004
   Navarro J, 2016, TRANSPORT RES F-TRAF, V43, P315, DOI 10.1016/j.trf.2016.09.007
   Pacherie E., 2007, Psyche, V13, P1
   Pacherie E, 2014, PHENOMENOL COGN SCI, V13, P25, DOI 10.1007/s11097-013-9329-8
   Picardi G, 2020, SCI ROBOT, V5, DOI 10.1126/scirobotics.aaz1012
   Polito V, 2013, CONSCIOUS COGN, V22, P684, DOI 10.1016/j.concog.2013.04.003
   Quertemont E, 2011, PSYCHOL BELG, V51, P109, DOI 10.5334/pb-51-2-109
   Rangelova S, 2018, PRESENCE-TELEOP VIRT, V27, P15, DOI 10.1162/PRES_a_00318
   Richardson AR, 2007, HUM FACTORS, V49, P507, DOI 10.1518/001872007X200139
   Ruess M, 2017, ATTEN PERCEPT PSYCHO, V79, P1123, DOI 10.3758/s13414-017-1292-y
   Sahaï A, 2017, FRONT NEUROROBOTICS, V11, DOI 10.3389/fnbot.2017.00052
   Sato A, 2005, COGNITION, V94, P241, DOI 10.1016/j.cognition.2004.04.003
   Shneiderman B., 1987, SIGBIO Newsl, V9, P6, DOI DOI 10.1145/25065.950626
   Soccini AM, 2022, J ROBOT MECHATRON, V34, P756, DOI 10.20965/jrm.2022.p0756
   Soccini AM, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1165, DOI [10.1109/vr.2019.8798193, 10.1109/VR.2019.8798193]
   Stotko P, 2019, IEEE INT C INT ROBOT, P3630, DOI [10.1109/IROS40897.2019.8968598, 10.1109/iros40897.2019.8968598]
   Synofzik M, 2008, CONSCIOUS COGN, V17, P219, DOI 10.1016/j.concog.2007.03.010
   Synofzik M, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00127
   Tanaka T, 2019, TIMING TIME PERCEPT, V7, P189, DOI 10.1163/22134468-20191150
   Tapal A, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01552
   Thoesen A., 2021, Current Robotics Reports, V2, P239, DOI [/10.1007/s43154-021-00056-3, 10.1007/s43154-021-00056-3, DOI 10.1007/S43154-021-00056-3]
   Thompson WB, 2004, PRESENCE-TELEOP VIRT, V13, P560, DOI 10.1162/1054746042545292
   Ueda S, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-82036-3
   Van den Bussche E, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0236809
   VIDAL JJ, 1973, ANNU REV BIOPHYS BIO, V2, P157, DOI 10.1146/annurev.bb.02.060173.001105
   Wen Wen, 2019, Front Psychol, V10, P2691, DOI 10.3389/fpsyg.2019.02691
   Wen W, 2019, CONSCIOUS COGN, V73, DOI 10.1016/j.concog.2019.05.007
   Wen W, 2015, CONSCIOUS COGN, V36, P87, DOI 10.1016/j.concog.2015.06.004
   Whitney D, 2020, SPR PROC ADV ROBOT, V10, P335, DOI 10.1007/978-3-030-28619-4_28
   Winfield AFT, 2014, Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), V8717, P85, DOI [10.1007/978-3-319-10401-08, DOI 10.1007/978-3-319-10401-08]
   Winkler P, 2020, VIRTUAL REAL-LONDON, V24, P411, DOI 10.1007/s10055-019-00403-y
   Yashin AS, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12126217
   Yun S, 2019, BIOSYST BIOROBOT, V21, P1039, DOI 10.1007/978-3-030-01845-0_208
   Zito GA, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0234321
NR 84
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD JUL 6
PY 2024
VL 28
IS 3
AR 132
DI 10.1007/s10055-024-01028-6
PG 18
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA XR4I9
UT WOS:001263389400001
DA 2024-08-05
ER

PT J
AU Pedram, S
   Kennedy, G
   Sanzone, S
AF Pedram, Shiva
   Kennedy, Grace
   Sanzone, Sal
TI Assessing the validity of VR as a training tool for medical students
SO VIRTUAL REALITY
LA English
DT Article
DE HMD-VR; Training; Education; Surgical; Immersive; Virtual reality;
   Validation
ID IMMERSIVE VIRTUAL-REALITY; SIMULATION; EDUCATION
AB The advances in Virtual Reality technologies, increased availability and reducing hardware costs have diminished many of the early challenges in the adoption of VR. However, a commonly identified gap in immersive Virtual Reality-Head Mounded Display (VR-HMD) training for medical education is the confidence in the long-term validity of the applications, in particular, the acceleration of the learning curve efficacy of learning outcomes over time and actual skills translation into real environments. Research shows a wide range of ad hoc applications, with superficial evaluations often conducted by technology vendors, based on assumed environments and tasks, envisaged (as opposed to actual) users and effectiveness of learning outcomes underpinned with little or no research focusing on a requirements-driven validation approach. This presents decision-making challenges for those seeking to adopt, implement and embed such systems in teaching practice. The current paper aims to (i) determine whether medical VR training improves the skill acquisition of training candidates, (ii) determine the factors affecting the acquisition of skills and (iii) validate the VR-based training using requirement-driven approach. In this paper, we used within- and between-subject design approaches to assess the validity of VR-based surgical training platform developed by Vantari VR against requirements which have been identified to have impact on learning processes and outcomes in VR-based training. First, study and control groups were compared based on their level of skill acquisitions. Then, by tailoring a requirements framework, the system was validated against the appropriate requirements. In total, 74 out of 109 requirements were investigated and evaluated against survey, observer and stakeholder workshop data. The training scenario covered the topic of Arterial Blood Gas (ABG) collection for second-year university medical students. In total 44 students volunteered to participate in this study, having been randomly assigned to either the study or control group. Students exposed to VR training (the study group) outperformed the control group in practical clinical skills training tasks and also adhered to better safety and hygiene practices. The study group also had a greater procedural completion rate over the control group. Students showed increased self-efficacy and knowledge scores immediately post-VR training. Prior ABG training did not impact on VR training outcomes. Low levels of simulation sickness, physical strain and stress, coupled with high levels of enjoyability, engagement, presence and fidelity were identified as factors affecting the overall training experience. In terms of learning, high scores were recorded for active learning, cognitive benefit and reflective thinking. Lastly, by validating the system against 74 system requirements, the study found a user acceptance level of 75%. This enabled the identification of weaknesses of the current system and possible future directions.
C1 [Pedram, Shiva; Kennedy, Grace] Univ Wollongong, SMART Infrastructure Facil, Engn & Informat Sci, Wollongong, NSW, Australia.
   [Sanzone, Sal] Univ Wollongong, Fac Sci Med & Hlth, Sch Med, Wollongong, NSW, Australia.
C3 University of Wollongong; University of Wollongong
RP Pedram, S (corresponding author), Univ Wollongong, SMART Infrastructure Facil, Engn & Informat Sci, Wollongong, NSW, Australia.
EM spedram@uow.edu.au; gracek@uow.edu.au; sals@uow.edu.au
OI Pedram, Shiva/0000-0002-5835-4093
FU Advantage SME; Vantari VR (NSW, Australia); UOW Advantage SME
   (Wollongong, Australia) Tech Voucher program, as part of NSW Treasury
   Boosting Business Innovation Program; UOW Graduate School of Medicine
FX This project was funded by Vantari VR (NSW, Australia) and UOW Advantage
   SME (Wollongong, Australia) Tech Voucher program, as part of NSW
   Treasury Boosting Business Innovation Program. The authors would like to
   thank the personnel of Vantari VR and the UOW Graduate School of
   Medicine for their helpfulness and support.
CR Barteit S, 2021, JMIR SERIOUS GAMES, V9, DOI 10.2196/29080
   Bernardo A, 2017, WORLD NEUROSURG, V106, P1015, DOI 10.1016/j.wneu.2017.06.140
   Bielsa VF, 2021, J PLAST RECONSTR AES, V74, P2372, DOI 10.1016/j.bjps.2021.03.066
   Bracq MS, 2021, NURSE EDUC PRACT, V53, DOI 10.1016/j.nepr.2021.103056
   Breitkreuz KR, 2021, CLIN SIMUL NURS, V53, P49, DOI 10.1016/j.ecns.2020.10.003
   Chheang V, 2021, COMPUT GRAPH-UK, V99, P234, DOI 10.1016/j.cag.2021.07.009
   Deaton JE., 2005, VIRTUAL REAL-LONDON, V8, P156, DOI [https://doi.org/10.1007/s10055-004-0145-x, DOI 10.1007/S10055-004-0145-X, 10.1007/s10055-004-0145-x]
   Dewey J., 1985, Essays on philosophy and education: 1916-1917
   Fairén M, 2020, J MED SYST, V44, DOI 10.1007/s10916-020-01550-5
   Grier R.A., 2015, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, P1727, DOI [DOI 10.1177/1541931215591373, 10.1177/1541931215591373]
   HART S G, 1988, P139
   Hooper J, 2019, J ARTHROPLASTY, V34, P2278, DOI 10.1016/j.arth.2019.04.002
   Huber T, 2018, INT J COMPUT ASS RAD, V13, P281, DOI 10.1007/s11548-017-1686-2
   INACSL Standards Committee, 2016, ABOUT US
   Jensen L, 2018, EDUC INF TECHNOL, V23, P1515, DOI 10.1007/s10639-017-9676-0
   Jou M, 2013, COMPUT HUM BEHAV, V29, P433, DOI 10.1016/j.chb.2012.04.020
   Kennedy GAL, 2023, SAFETY SCI, V165, DOI 10.1016/j.ssci.2023.106200
   Kenngott HG, 2022, SURG ENDOSC, V36, P126, DOI 10.1007/s00464-020-08246-4
   Kolb AY, 2005, ACAD MANAG LEARN EDU, V4, P193, DOI 10.5465/AMLE.2005.17268566
   Kolb DA., 2015, Experiential learning: Experience as the source of learning and development, DOI [10.1002/job.4030080408, DOI 10.1016/B978-0-7506-7223-8.50017-4]
   Lopes A, 2017, LECT NOTES COMPUT SC, V10289, P499, DOI 10.1007/978-3-319-58637-3_39
   Mäkinen H, 2022, BEHAV INFORM TECHNOL, V41, P1, DOI 10.1080/0144929X.2020.1788162
   Makransky G, 2019, COMPUT EDUC, V134, P15, DOI 10.1016/j.compedu.2019.02.002
   Mao RDQ, 2021, J SURG RES, V268, P40, DOI 10.1016/j.jss.2021.06.045
   Masuoka Yoshihito, 2019, JMIR Med Educ, V5, pe11921, DOI 10.2196/11921
   McKnight RR, 2020, CURR REV MUSCULOSKE, V13, P663, DOI 10.1007/s12178-020-09667-3
   Medical Deans of Australia and New Zealand, 2020, Guidance statement: clinical practice core competencies for graduating medical students
   Mehrotra Divya, 2021, J Oral Biol Craniofac Res, V11, P486, DOI 10.1016/j.jobcr.2021.06.002
   Pedram S, 2023, VIRTUAL REAL-LONDON, V27, P2255, DOI 10.1007/s10055-023-00802-2
   Pedram S, 2020, COMPUT EDUC, V153, DOI 10.1016/j.compedu.2020.103891
   Pelargos PE, 2017, J CLIN NEUROSCI, V35, P1, DOI 10.1016/j.jocn.2016.09.002
   Petersen GB, 2022, COMPUT EDUC, V179, DOI 10.1016/j.compedu.2021.104429
   Pithers R.Thomas., 1998, IMPROVING LEARNING E
   Pulijala Y, 2018, J ORAL MAXIL SURG, V76, P1065, DOI 10.1016/j.joms.2017.10.002
   Rahman R, 2020, SURG INNOV, V27, P88, DOI 10.1177/1553350619871787
   Rogers DA, 2001, ANN SURG, V233, P159, DOI 10.1097/00000658-200102000-00003
   Silva JNA, 2018, JACC-BASIC TRANSL SC, V3, P420, DOI 10.1016/j.jacbts.2017.11.009
   Vaughan N, 2016, MED ENG PHYS, V38, P59, DOI 10.1016/j.medengphy.2015.11.021
   Yamazaki A, 2021, AURIS NASUS LARYNX, V48, P1081, DOI 10.1016/j.anl.2021.03.009
NR 39
TC 1
Z9 1
U1 19
U2 22
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 15
DI 10.1007/s10055-023-00912-x
PG 24
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA EU4R0
UT WOS:001141439100001
OA hybrid
DA 2024-08-05
ER

PT J
AU Rohil, MK
   Parikh, A
AF Rohil, Mukesh Kumar
   Parikh, Arpan
TI Fast and robust virtual try-on based on parser-free generative
   adversarial network
SO VIRTUAL REALITY
LA English
DT Article
DE Augmented reality; Virtual try-on; U-Net; VITON; GAN
AB Image-based virtual try-on models have recently become popular leading to many new developments, especially in the past three years. The problem of virtual try-on requires trying on a cloth image on a target person's image. Implementing the same turns out to be a complicated task. It involves calculating the position, angle, and texture for the cloth to be placed on the target that could be in varying orientations. Also, texture may change as a result of any change in orientation. Therefore, generating textures for the cloth also poses a major challenge. In this article, we propose a generative adversarial network-based virtual try-on network that is robust, fast, and parser-free. We dive into some of the latest developments in the field of virtual try-on models and discuss their market feasibility as well as techniques. It is observed that the performance of our proposed network is comparable to the state-of-the-art models, and it outperforms the latter in terms of execution speed owing to its low time complexity. Moreover, it uses a parser-free architecture. It does not require any external input or processing while testing or applying a trained model. It uses a "teacher-student" approach to learn from existing models. The loss function is based on final output of the model. Therefore, it can also learn its shortcomings from the output of the model, unlike other architectures where much of the training is done in a self-supervised manner from the real person's image.
C1 [Rohil, Mukesh Kumar; Parikh, Arpan] Birla Inst Technol & Sci, Pilani 333031, Rajasthan, India.
C3 Birla Institute of Technology & Science Pilani (BITS Pilani)
RP Rohil, MK (corresponding author), Birla Inst Technol & Sci, Pilani 333031, Rajasthan, India.
EM rohil@pilani.bits-pilani.ac.in; f2016082p@alumni.bits-pilani.ac.in
CR Ge YY, 2021, Arxiv, DOI arXiv:2103.04559
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Hensel M, 2017, ADV NEUR IN, V30
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jandial S, 2020, 2020 WINTER C APPL C
   Kingma D. P., 2014, arXiv
   Liang X., 2018, IEEE Transact Pattern Anal Mech Intell, DOI [10.48550/arXiv.1804.01984, DOI 10.48550/ARXIV.1804.01984]
   Raj A, 2018, PROC EUROPEAN C COMP
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Salimans T, 2016, ADV NEUR IN, V29
   Wang BC, 2018, LECT NOTES COMPUT SC, V11217, P607, DOI 10.1007/978-3-030-01261-8_36
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
NR 12
TC 1
Z9 1
U1 2
U2 2
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 5
DI 10.1007/s10055-023-00899-5
PG 10
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA DX0O7
UT WOS:001135265600002
OA Bronze
DA 2024-08-05
ER

PT J
AU Shaghaghian, Z
   Burte, H
   Song, DZ
   Yan, W
AF Shaghaghian, Zohreh
   Burte, Heather
   Song, Dezhen
   Yan, Wei
TI An augmented reality application and experiment for understanding and
   learning spatial transformation matrices
SO VIRTUAL REALITY
LA English
DT Article
DE Augmented reality; Spatial transformations; Transformation matrices;
   Visualization; Embodied learning
ID TANGIBLE USER INTERFACES; STUDENTS; VISUALIZATION; SKILLS
AB Understanding spatial transformations and their mathematical representations is essential in computer-aided design, computer graphics, robotics, etc. This research has developed and tested an augmented reality (AR) application (BRICKxAR/T) to enhance students' learning of spatial transformation matrices. BRICKxAR/T leverages AR features, including information augmentation, physical-virtual object interplay, and embodied learning, to create a novel and effective visualization experience for learning. In this paper, we evaluated the BRICKxAR/T as a learning intervention using LEGO models for physical and virtual manipulatives in an experiment. The experiment compared AR (N = 29) vs. non-AR (N = 30) learning workshops with pre- and post-tests on Purdue Visualization of Rotations Test and math questions to assess students' learning gains. All participants math scores significantly improved with the AR workshop tending to show greater improvements. The post-workshop survey showed students were inclined to think BRICKxAR/T an interesting and useful application, and they spent more time learning in AR than non-AR.
C1 [Shaghaghian, Zohreh; Yan, Wei] Texas A&M Univ, Dept Architecture, College Stn, TX 77843 USA.
   [Burte, Heather] Texas A&M Univ, Dept Psychol & Brain Sci, College Stn, TX USA.
   [Song, Dezhen] Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX USA.
C3 Texas A&M University System; Texas A&M University College Station; Texas
   A&M University System; Texas A&M University College Station; Texas A&M
   University System; Texas A&M University College Station
RP Yan, W (corresponding author), Texas A&M Univ, Dept Architecture, College Stn, TX 77843 USA.
EM wyan@tamu.edu
OI Burte, Heather/0000-0002-9623-4375; Song, Dezhen/0000-0002-2944-5754
FU National Science Foundation [2119549]; National Science Foundation
FX This material is based upon work supported by the National Science
   Foundation under Grant No. 2119549.
CR Anifowose H, 2022, Interactive virtual construction, a case study of building component assembly towards the adoption of BIM and VR in business and training
   [Anonymous], 2020, Vuforia developer library
   Azuma RT, 1997, PRESENCE-VIRTUAL AUG, V6, P355, DOI 10.1162/pres.1997.6.4.355
   Bodner GM, 1997, Chem. Educ, V2, P1, DOI DOI 10.1007/S00897970138A
   Bujak KR, 2013, COMPUT EDUC, V68, P536, DOI 10.1016/j.compedu.2013.02.017
   Cahyono Bambang, 2018, 2018 2nd East Indonesia Conference on Computer and Information Technology (EIConCIT). Proceedings, P299, DOI 10.1109/EIConCIT.2018.8878553
   Casey BM, 2008, COGNITION INSTRUCT, V26, P269, DOI 10.1080/07370000802177177
   Chandrasekera T, 2015, INT SYM MIX AUGMENT, P6, DOI 10.1109/ISMAR-MASHD.2015.18
   Dunser A., 2006, P 7 ACM SIGCHI NZ CH, V158, P125
   Fife JH., 2019, ETS Res Rep Ser, V2019, P1, DOI DOI 10.1002/ETS2.12236
   Figen Gul L, 2015, COMPL SIMPL P 34 ECA, V1, P493
   Fuys D., 1988, J RES MATH EDUC, V3, pi, DOI [10.2307/749957, DOI 10.2307/749957]
   Garmendia M, 2007, EUR J ENG EDUC, V32, P315, DOI 10.1080/03043790701276874
   Gülkilik H, 2015, EDUC SCI-THEOR PRACT, V15, P1531
   de Ravé EG, 2016, MULTIMED TOOLS APPL, V75, P9641, DOI 10.1007/s11042-016-3384-4
   Hart S, 1980, NASA task load index user manual v. 1.0
   Hoe ZY, 2019, UNIVERSAL ACCESS INF, V18, P327, DOI 10.1007/s10209-017-0597-x
   Hohenwarter M., 2018, Geogebra
   Hollebrands K.F., 2003, J MATH BEHAV, V22, P55, DOI [10.1016/s0732-3123(03)00004-x, DOI 10.1016/S0732-3123(03)00004-X]
   Kaufmann H, 2003, COMPUT GRAPH-UK, V27, P339, DOI 10.1016/S0097-8493(03)00028-1
   Kaur N, 2018, IEEE INT CONF ADV LE, P372, DOI 10.1109/ICALT.2018.00093
   Kendon A, 1994, A review, V1813, DOI [10.1207/s15327973rlsi2703, DOI 10.1207/S15327973RLSI2703]
   Khan Academy, About us
   Kim MJ, 2008, HUM-COMPUT INTERACT, V23, P101, DOI 10.1080/07370020802016415
   Kruijff E., 2017, 3D user interfaces: theory and practice
   Le HQ, 2017, INT CONF BIG DATA, P34, DOI 10.1109/BIGCOMP.2017.7881712
   Martín-Gutiérrez J, 2010, COMPUT GRAPH-UK, V34, P77, DOI 10.1016/j.cag.2009.11.003
   Mou Weimin., 2003, P SIGCHI C HUM FACT, P73, DOI [DOI 10.1145/642611.642626, 10.1145/642611, DOI 10.1145/642611]
   Nemec M, 2017, 2017 15TH IEEE INTERNATIONAL CONFERENCE ON EMERGING ELEARNING TECHNOLOGIES AND APPLICATIONS (ICETA 2017), P331
   PINTRICH PR, 1990, J EDUC PSYCHOL, V82, P33, DOI 10.1037/0022-0663.82.1.33
   Preece D, 2013, ANAT SCI EDUC, V6, P216, DOI 10.1002/ase.1345
   Qualtrics XM, The leading experience management software
   Rellensmann J, 2017, EDUC STUD MATH, V95, P53, DOI 10.1007/s10649-016-9736-1
   Rossano V, 2020, IEEE ACCESS, V8, P107772, DOI 10.1109/ACCESS.2020.3000990
   Seichter H, 2007, COMPUTER-AIDED ARCHITECTURAL DESIGN FUTURES (CAAD FUTURES) 2007, P3, DOI 10.1007/978-1-4020-6528-6_1
   Shaghaghian Z, 2021, INT C COMP AID ARCH, P515
   Shaghaghian Z, 2022, LECT NOTES COMPUT SC, V13329, P112, DOI 10.1007/978-3-031-05675-8_10
   Sorby S, 2009, INT J SCI EDUC, V31, P459, DOI 10.1080/09500690802595839
   Sun L, 2014, FRONT ARCHIT RES, V3, P28, DOI 10.1016/j.foar.2013.11.005
   Terlecki MS, 2008, APPL COGNITIVE PSYCH, V22, P996, DOI 10.1002/acp.1420
   van Garderen D, 2006, J LEARN DISABIL-US, V39, P496, DOI 10.1177/00222194060390060201
   Velev D., 2017, INT J LEARN TEACH, V3, P33, DOI DOI 10.18178/IJLT.3.1.33-37
   Viswanathan VK, 2012, J MECH DESIGN, V134, DOI 10.1115/1.4007148
   Weng S-C, 2014, WORLD C ELEARNING CO, P2027
   Wheatley G.H., 1990, ARITH TEACH, V37, P10, DOI DOI 10.5951/AT.37.6.0010
   Yan W, 2022, VIRTUAL REAL-LONDON, V26, P465, DOI 10.1007/s10055-021-00582-7
NR 46
TC 0
Z9 0
U1 9
U2 10
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 12
DI 10.1007/s10055-023-00904-x
PG 18
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA EE2H2
UT WOS:001137171900001
OA Bronze
DA 2024-08-05
ER

PT J
AU Dekker, E
   Whitburn, D
   Preston, S
AF Dekker, Evan
   Whitburn, Damien
   Preston, Sarah
TI Adoption of immersive-virtual reality as an intrinsically motivating
   learning tool in parasitology
SO VIRTUAL REALITY
LA English
DT Article
DE Parasitology; Immersive virtual reality; Immersive; Motivation; HMSAM;
   SEM; Quantitative
ID TEACHING VETERINARY PARASITOLOGY; EDUCATION; STUDENTS; MODEL; ANATOMY;
   ACCEPTANCE; INTENTION; TEACHERS; IMPACT; GAMES
AB Veterinary parasitology is study of parasitic diseases, treatment and prevention. It is a major component of animal health courses due to impacts parasites have on production and companion animals. Extant tertiary education in parasitology typically involves theory sessions coupled with practical experience. In this study we propose tertiary parasitology teaching would be enhanced through adoption of immersive Virtual Reality (I-VR) as an intrinsically motivating learning tool to complement their studies. To evaluate this adoption, a custom I-VR parasitology game was developed that tertiary veterinary science students experienced (n = 109), with feedback assessed using the Hedonic-Motivation System Adoption Model (HMSAM). HMSAM proved appropriate for measuring student's hedonistic and utilitarian perspectives of I-VR experience with perceived ease of use, perceived usefulness, joy, ability to control, immersion levels and intention to use displaying significant positive relationships in derived model. However, in a departure from similar studies, the curiosity construct was not a useful predictor of intention to use in this context of a scaffolded, instructional application. This study highlights suitability of I-VR and provides a statistically robust evaluation method using a modified HMSAM to evaluate acceptance, usefulness, and ease of use of I-VR in tertiary education.
C1 [Dekker, Evan; Preston, Sarah] Federat Univ Australia, Inst Sci Innovat & Sustainabil, Ballarat, Vic 3353, Australia.
   [Whitburn, Damien] Deakin Univ, Fac Business & Law Sch, Burwood 3125, Australia.
C3 Federation University Australia; Deakin University
RP Preston, S (corresponding author), Federat Univ Australia, Inst Sci Innovat & Sustainabil, Ballarat, Vic 3353, Australia.
EM sj.preston@federation.edu.au
OI Preston, Sarah/0000-0002-4227-1664
FU Federation University Australia; Australian Society for Parasitology
FX The authors would like to acknowledge the funding provided by the
   Australian Society for Parasitology to support the I-VR development and
   Professor Jan Slapeta and Dr Nichola Calvani from the University of
   Sydney for their help in peer-reviewing the I-VR experience and
   collection of survey response.
CR Andersen MS, 2021, J COMPUT ASSIST LEAR, V37, P183, DOI 10.1111/jcal.12478
   Archer J., 1999, HIGHER ED RES DEV, V18, P31, DOI DOI 10.1080/0729436990180104
   Bailenson JN, 2008, J LEARN SCI, V17, P102, DOI 10.1080/10508400701793141
   Bashir A, 2021, FRONT EDUC, V6, DOI 10.3389/feduc.2021.711619
   Belisle B, 2020, J VIS CULT, V19, P3, DOI 10.1177/1470412920906258
   Bell FE, 2019, ANAT SCI EDUC, V12, P310, DOI 10.1002/ase.1839
   Bryer J, 2016, Likert: An R package analyzing and visualizing Likert items
   Butler D., 2002, Educause Quarterly, V2
   Chow M, 2016, AUSTRALAS J EDUC TEC, V32, P1
   Clausen PH, 2018, VET PARASITOL, V252, P58, DOI 10.1016/j.vetpar.2018.01.028
   Corpus JH, 2009, CONTEMP EDUC PSYCHOL, V34, P154, DOI 10.1016/j.cedpsych.2009.01.001
   Dicheva D, 2015, EDUC TECHNOL SOC, V18, P75
   Elme L, 2022, ETR&D-EDUC TECH RES, V70, P1601, DOI 10.1007/s11423-022-10139-3
   Estai M, 2016, ANN ANAT, V208, P151, DOI 10.1016/j.aanat.2016.02.010
   Evans DJR, 2022, ANAT SCI EDUC, V15, P1145, DOI 10.1002/ase.2224
   Foreman J., 2003, EDUCAUSE Review
   FORNELL C, 1981, J MARKETING RES, V18, P39, DOI 10.2307/3151312
   Gefen D., 2000, Communications of the Association for Information Systems, V4, P1, DOI DOI 10.17705/1CAIS.00407
   Hair JF, 2019, EUR BUS REV, V31, P2, DOI 10.1108/EBR-11-2018-0203
   Hattie JAC, 2009, VISIBLE LEARNING: A SYNTHESIS OF OVER 800 META-ANALYSES RELATING TO ACHIEVEMENT, P1
   Ho S, 2022, ANAT SCI EDUC, V15, P1074, DOI 10.1002/ase.2146
   Jabbar A, 2018, VET PARASITOL, V253, P120, DOI 10.1016/j.vetpar.2018.02.020
   Jabbar A, 2016, TRENDS PARASITOL, V32, P522, DOI 10.1016/j.pt.2016.04.004
   Jegers K., 2007, Computers in Entertainment (CIE), V5, P9, DOI DOI 10.1145/1236224.1236238
   JONASSEN DH, 1991, ETR&D-EDUC TECH RES, V39, P5, DOI 10.1007/BF02296434
   Kapoor K, 2022, ANAT HISTOL EMBRYOL, V51, P163, DOI 10.1111/ahe.12783
   Karakolidis A, 2019, INT J SCI EDUC, V41, P1457, DOI 10.1080/09500693.2019.1612121
   Khoshnam A, 2013, Int J Acad Res Bus Social Sci, V3
   Kim MJ, 2019, INT J INFORM MANAGE, V46, P236, DOI 10.1016/j.ijinfomgt.2018.11.016
   Knekta E, 2019, CBE-LIFE SCI EDUC, V18, DOI 10.1187/cbe.18-04-0064
   Kozievitch NP, 2010, LECT NOTES COMPUT SC, V6273, P466, DOI 10.1007/978-3-642-15464-5_58
   Kumar JA, 2019, EDUC INF TECHNOL, V24, P1793, DOI 10.1007/s10639-018-09858-z
   Lam LW, 2012, J BUS RES, V65, P1328, DOI 10.1016/j.jbusres.2011.10.026
   Lee J., 2011, Academic Exchange Quarterly, V15, P146
   Lochner L, 2016, INT J MED EDUC, V7, P69, DOI 10.5116/ijme.56b5.0369
   Lowry PB, 2013, J ASSOC INF SYST, V14, P617
   Lui M, 2020, BRIT J EDUC TECHNOL, V51, P2180, DOI 10.1111/bjet.13022
   de Carvalho LMM, 2018, VET PARASITOL, V253, P98, DOI 10.1016/j.vetpar.2018.01.030
   Makowski D, 2017, CONSCIOUS COGN, V53, P194, DOI 10.1016/j.concog.2017.06.015
   Makransky G, 2019, J COMPUT ASSIST LEAR, V35, P691, DOI 10.1111/jcal.12375
   Makransky G, 2018, ETR&D-EDUC TECH RES, V66, P1141, DOI 10.1007/s11423-018-9581-2
   Mantovani F., 2001, Towards cyberpsychology: mind, cognition, and society in the Internet age, V2
   Marks B, 2022, EDUC INF TECHNOL, V27, P1287, DOI 10.1007/s10639-021-10653-6
   Martin L., 2020, Foundations for good practice: the student experience of online learning in Australian higher education during the COVID-19 pandemic
   Matovu H, 2023, CHEM EDUC RES PRACT, V24, P509, DOI 10.1039/d2rp00176d
   Matovu H, 2023, STUD SCI EDUC, V59, P205, DOI 10.1080/03057267.2022.2082680
   Maytin M, 2015, PACE, V38, P319, DOI 10.1111/pace.12546
   Molina-Carmona R, 2018, SUSTAINABILITY-BASEL, V10, DOI 10.3390/su10041074
   Morimoto J., 2021, EVOL EDUC OUTREACH, V14, P1, DOI [DOI 10.1186/s12052-021-00147-x, 10.1186/s12052-021-00147-x]
   Nguyen T, 2021, FRONT EDUC, V6, DOI 10.3389/feduc.2021.647986
   Oblinger D., 2004, J INTERACT MEDIA EDU, V2004, DOI [DOI 10.5334/2004-8-OBLINGER, 10.5334/2004-8-oblinger]
   Oluwajana D, 2019, J THEOR APPL EL COMM, V14, P156, DOI 10.4067/S0718-18762019000300109
   Parong J, 2021, J COMPUT ASSIST LEAR, V37, P226, DOI 10.1111/jcal.12482
   Parong J, 2018, J EDUC PSYCHOL, V110, P785, DOI 10.1037/edu0000241
   Pawlina W, 2017, ANAT SCI EDUC, V10, P305, DOI 10.1002/ase.1714
   Penzhorn BL, 2018, VET PARASITOL, V252, P62, DOI 10.1016/j.vetpar.2018.01.032
   Ploetzner R, 2021, INSTR SCI, V49, P497, DOI 10.1007/s11251-021-09541-w
   Pottle Jack, 2019, Future Healthc J, V6, P181, DOI 10.7861/fhj.2019-0036
   Prince M, 2004, J ENG EDUC, V93, P223, DOI 10.1002/j.2168-9830.2004.tb00809.x
   Radianti J, 2020, COMPUT EDUC, V147, DOI 10.1016/j.compedu.2019.103778
   Real FJ, 2017, ACAD PEDIATR, V17, P431, DOI 10.1016/j.acap.2017.01.010
   Rushton J, 2017, PARASITOLOGY, V144, P15, DOI 10.1017/S0031182016000196
   Sánchez-Mena A, 2017, J E-LEARN KNOWL SOC, V13, P47, DOI 10.20368/1971-8829/1319
   Sastry L., 1998, Virtual Reality, V3, P235, DOI 10.1007/BF01408704
   Selzer PM, 2021, TRENDS PARASITOL, V37, P77, DOI 10.1016/j.pt.2020.09.004
   Sherry JL, 2004, COMMUN THEOR, V14, P328, DOI 10.1093/ct/14.4.328
   Shih-Han Wu, 2021, International Journal of Organizational Innovation, V14, P25
   Slater M, 1999, PRESENCE-TELEOP VIRT, V8, P560, DOI 10.1162/105474699566477
   Taasoobshirazi G.Wang., 2016, J APPL QUANTITATIVE, V11, P31
   Tung FC, 2008, INT J NURS STUD, V45, P1299, DOI 10.1016/j.ijnurstu.2007.09.011
   Turhan B, 2022, FRONT BIOINFORM, V2, DOI 10.3389/fbinf.2022.873478
   van Doom DCK, 2018, VET PARASITOL, V252, P85, DOI 10.1016/j.vetpar.2018.01.036
   Vansteenkiste M, 2006, EDUC PSYCHOL-US, V41, P19, DOI 10.1207/s15326985ep4101_4
   Winn W., 1993, Technical Publication R-93-9, Human Interface Technology Laboratory of the Washington Technology Center
   Wu JH, 2008, INT J QUAL HEALTH C, V20, P123, DOI 10.1093/intqhc/mzm074
   Yang HR, 2023, FRONT PSYCHOL, V13, DOI 10.3389/fpsyg.2022.1081372
   Zinchenko YP, 2020, NEW IDEAS PSYCHOL, V59, DOI 10.1016/j.newideapsych.2020.100786
NR 77
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD JUN 12
PY 2024
VL 28
IS 3
AR 123
DI 10.1007/s10055-024-01016-w
PG 17
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA TY6J5
UT WOS:001244855400001
OA hybrid
DA 2024-08-05
ER

PT J
AU Sun, Y
   Yuan, ZH
AF Sun, Yi
   Yuan, Zihao
TI A virtual gym in your pocket: the influence of augmented reality
   exercise app characteristics on user's continuance intention
SO VIRTUAL REALITY
LA English
DT Article
DE AR exercise applications; Service characteristics; System
   characteristics; Continuance intention; Stimulus-organism-response model
ID INFORMATION-TECHNOLOGY; CONSUMER ACCEPTANCE; SYSTEM QUALITY; POKEMON GO;
   E-COMMERCE; EXPERIENCE; INTERACTIVITY; IMPACT; MODEL; DETERMINANTS
AB In recent years, with the development of augmented reality (AR) technology and the prevalence of COVID-19, augmented reality exercise applications (AREAs) have entered people's lives and changed people's exercise ways. However, there is still little research on how AREAs affect users' continuance intention, which limits our deeper understanding of the further use of AREAs by users for the potential benefits they provide. This study investigates the role of AREAs by proposing the original proposition of AREAs characteristic classification based on previous AR literature and dividing their characteristics into two categories: service and system characteristics. Through an online empirical study, 398 valid questionnaires were collected to test the hypotheses using the structural equation model. The results showed that hedonic and utilitarian value and presence are vital inner states which mediate the influence of service and system characteristics on user's satisfaction and continuance intention. The results further show that only hedonic value is found to have a positive and significant relationship with continuance intention. This study contributes to the literature in the AR field by examining how different AR application characteristics affect user continuance intention in the context of sports and fitness. It also suggests that practitioners should identify the impact of different characteristics on user value and focus on the hedonic aspects of the application.
C1 [Sun, Yi; Yuan, Zihao] Shanghai Univ, Dept Informat Management & Informat Syst, Shangda Rd 99, Shanghai, Peoples R China.
C3 Shanghai University
RP Sun, Y (corresponding author), Shanghai Univ, Dept Informat Management & Informat Syst, Shangda Rd 99, Shanghai, Peoples R China.
EM sunyi_im@shu.edu.cn
FU National Natural Science Foundation of China; MOE (Ministry of Education
   in China) Project of Humanities and Social Sciences [23YJC630155]; 
   [71802126]
FX This work was supported by the National Natural Science Foundation of
   China [71802126] and MOE (Ministry of Education in China) Project of
   Humanities and Social Sciences [23YJC630155]
CR Ahn T, 2007, INFORM MANAGE-AMSTER, V44, P263, DOI 10.1016/j.im.2006.12.008
   Akel G, 2021, MULTIMED TOOLS APPL, V80, P7103, DOI 10.1007/s11042-020-10094-2
   [Anonymous], 2022, Honored on CCTV! The "Tiantian Rope Skipping APP"online sports meet was reported by CCTV and other authoritative media
   BAGOZZI RP, 1986, BRIT J SOC PSYCHOL, V25, P95, DOI 10.1111/j.2044-8309.1986.tb00708.x
   BAILEY JE, 1983, MANAGE SCI, V29, P530, DOI 10.1287/mnsc.29.5.530
   Baker EW, 2019, INFORM MANAGE-AMSTER, V56, DOI 10.1016/j.im.2019.02.008
   Batra R., 1991, MARKET LETT, V2, P159, DOI [10.1007/BF00436035, DOI 10.1007/BF00436035]
   Berryman Donna R., 2012, Medical Reference Services Quarterly, V31, P212, DOI 10.1080/02763869.2012.670604
   Bhattacherjee A, 2004, MIS QUART, V28, P229
   Bhattacherjee A, 2001, MIS QUART, V25, P351, DOI 10.2307/3250921
   Bielli S., 2015, P 6 AUGMENTED HUMAN, P141, DOI DOI 10.1145/2735711.2735836
   Billinghurst M, 2002, COMMUN ACM, V45, P64, DOI 10.1145/514236.514265
   BITNER MJ, 1992, J MARKETING, V56, P57, DOI 10.2307/1252042
   Bonnin G, 2020, J RETAIL CONSUM SERV, V52, DOI 10.1016/j.jretconser.2019.101938
   Bonus JA, 2018, MEDIA PSYCHOL, V21, P263, DOI 10.1080/15213269.2017.1305280
   Buchanan R., 1991, Design Issues, V8, P80, DOI [https://doi.org/10.2307/1511458, DOI 10.2307/1511458]
   Butt A, 2022, J SERV MARK, V36, P73, DOI 10.1108/JSM-12-2020-0508
   Caboni F, 2019, INT J RETAIL DISTRIB, V47, P1125, DOI 10.1108/IJRDM-12-2018-0263
   Chang IC, 2014, INTERNET RES, V24, P21, DOI 10.1108/IntR-02-2012-0025
   Chang KE, 2020, INTERACT LEARN ENVIR, V28, P685, DOI 10.1080/10494820.2019.1636073
   Chen PJ, 2020, J EXERC SCI FIT, V18, P142, DOI 10.1016/j.jesf.2020.05.003
   Chopdar PK, 2020, INT J INFORM MANAGE, V53, DOI 10.1016/j.ijinfomgt.2020.102106
   Chung N, 2015, COMPUT HUM BEHAV, V50, P588, DOI 10.1016/j.chb.2015.02.068
   Corporation (IDC) I. D., 2020, Worldwide spending on augmented and virtual reality forecast to deliver strong growth through 2024, according to a new IDC Spending Guide
   Cyr D, 2009, INT J HUM-COMPUT ST, V67, P850, DOI 10.1016/j.ijhcs.2009.07.004
   Dacko SG, 2017, TECHNOL FORECAST SOC, V124, P243, DOI 10.1016/j.techfore.2016.09.032
   DAVIS FD, 1989, MIS QUART, V13, P319, DOI 10.2307/249008
   Deng LQ, 2010, EUR J INFORM SYST, V19, P60, DOI 10.1057/ejis.2009.50
   Deterding S., 2011, CHI'11 Extended Abstracts on Human Factors in Computing Systems, CHI EA'11, P2425, DOI [DOI 10.1145/1979742.1979575, 10.1145/1979742.1979575]
   DOLL WJ, 1988, MIS QUART, V12, P259, DOI 10.2307/248851
   DONOVAN RJ, 1982, J RETAILING, V58, P34
   Esfandiar K, 2019, J BUS RES, V94, P172, DOI 10.1016/j.jbusres.2017.10.045
   Fan XJ, 2020, J RETAIL CONSUM SERV, V53, DOI 10.1016/j.jretconser.2019.101986
   Fang JM, 2017, INT J INFORM MANAGE, V37, P269, DOI 10.1016/j.ijinfomgt.2017.03.003
   Fiore AM, 2005, J INTERACT MARK, V19, P38, DOI 10.1002/dir.20042
   FORNELL C, 1981, J MARKETING RES, V18, P39, DOI 10.2307/3151312
   Fu SH, 2018, INT J INFORM MANAGE, V40, P88, DOI 10.1016/j.ijinfomgt.2018.01.013
   García-Jurado A, 2019, KYBERNETES, V48, P1278, DOI 10.1108/K-07-2018-0350
   Ghazali EM, 2019, INTERNET RES, V29, P504, DOI 10.1108/IntR-12-2017-0505
   Goebert C, 2020, COMPUT HUM BEHAV, V106, DOI 10.1016/j.chb.2019.106231
   Gorla N, 2010, J STRATEGIC INF SYST, V19, P207, DOI 10.1016/j.jsis.2010.05.001
   Hacker J, 2020, EUR J INFORM SYST, V29, P563, DOI 10.1080/0960085X.2020.1814680
   Hamari J, 2015, INT J INFORM MANAGE, V35, P419, DOI 10.1016/j.ijinfomgt.2015.04.006
   Hamari J, 2014, P ANN HICSS, P3025, DOI 10.1109/HICSS.2014.377
   Higgins JP, 2016, AM J MED, V129, P11, DOI 10.1016/j.amjmed.2015.05.038
   Hilken T, 2017, J ACAD MARKET SCI, V45, P884, DOI 10.1007/s11747-017-0541-x
   HIRSCHMAN EC, 1982, J MARKETING, V46, P92, DOI 10.2307/1251707
   Hoyer WD, 2020, J INTERACT MARK, V51, P57, DOI 10.1016/j.intmar.2020.04.001
   Hsiao CH, 2016, TELEMAT INFORM, V33, P342, DOI 10.1016/j.tele.2015.08.014
   Hsiao KF, 2013, MULTIMED TOOLS APPL, V64, P407, DOI 10.1007/s11042-011-0985-9
   Hsu SHY, 2021, J RETAIL CONSUM SERV, V62, DOI 10.1016/j.jretconser.2021.102649
   Hu YY, 2023, INFORM TECHNOL PEOPL, V36, P165, DOI 10.1108/ITP-07-2020-0520
   Huang TL, 2021, J RETAIL CONSUM SERV, V58, DOI 10.1016/j.jretconser.2020.102256
   Huang TL, 2015, ELECTRON COMMER RES, V15, P269, DOI 10.1007/s10660-014-9163-2
   Ibáñez-Sánchez S, 2022, PSYCHOL MARKET, V39, P559, DOI 10.1002/mar.21639
   James TL, 2019, MIS QUART, V43, P287, DOI 10.25300/MISQ/2019/14128
   Javornik A, 2016, J RETAIL CONSUM SERV, V30, P252, DOI 10.1016/j.jretconser.2016.02.004
   Jenny SE, 2017, QUEST, V69, P1, DOI 10.1080/00336297.2016.1144517
   Jeon S, 2020, INT J ENV RES PUB HE, V17, DOI 10.3390/ijerph17093260
   Johnson GJ, 2006, J ADVERTISING, V35, P35, DOI 10.2753/JOA0091-3367350403
   Jung T, 2015, TOURISM MANAGE, V49, P75, DOI 10.1016/j.tourman.2015.02.013
   Kaczmarek LD, 2017, COMPUT HUM BEHAV, V75, P356, DOI 10.1016/j.chb.2017.05.031
   Kim HC, 2016, COMPUT HUM BEHAV, V59, P28, DOI 10.1016/j.chb.2016.01.001
   Kim JB, 2015, J INTERNET COMMER, V14, P425, DOI 10.1080/15332861.2015.1092067
   Klein LR, 2003, J INTERACT MARK, V17, P41, DOI 10.1002/dir.10046
   Kowalczuk P, 2021, J BUS RES, V124, P357, DOI 10.1016/j.jbusres.2020.10.050
   Kowalczuk P, 2018, J RES INTERACT MARK, V12, P418, DOI 10.1108/JRIM-01-2018-0022
   Lee CH, 2018, TELEMAT INFORM, V35, P1958, DOI 10.1016/j.tele.2018.06.008
   Lee H, 2007, INT J HUM-COMPUT ST, V65, P796, DOI 10.1016/j.ijhcs.2007.04.004
   Lee S, 2011, J BUS RES, V64, P1195, DOI 10.1016/j.jbusres.2011.06.022
   Lin HH, 2012, INT J HUM-COMPUT INT, V28, P445, DOI 10.1080/10447318.2011.618097
   Lin HF, 2008, CYBERPSYCHOL BEHAV, V11, P138, DOI 10.1089/cpb.2007.0003
   Lin HY, 2013, SOC BEHAV PERSONAL, V41, P113, DOI 10.2224/sbp.2013.41.1.113
   Liu BQ, 2012, INFORM SYST RES, V23, P1246, DOI 10.1287/isre.1120.0424
   Liu D, 2017, MIS QUART, V41, P1011
   Liu D, 2013, MIS QUART, V37, P111
   Liu F, 2019, INT J INFORM MANAGE, V45, P107, DOI 10.1016/j.ijinfomgt.2018.09.004
   Liu YF, 2022, COMPUT ELECTR ENG, V99, DOI 10.1016/j.compeleceng.2022.107807
   Liu YQ, 2020, INT J ENV RES PUB HE, V17, DOI 10.3390/ijerph17238794
   Looney CA, 2004, COMMUN ACM, V47, P71, DOI 10.1145/990680.990683
   Malhotra NK, 2017, J ADVERTISING, V46, P193, DOI 10.1080/00913367.2016.1252287
   MANO H, 1993, J CONSUM RES, V20, P451, DOI 10.1086/209361
   Mathwick C, 2001, J RETAILING, V77, P39, DOI 10.1016/S0022-4359(00)00045-2
   Mehrabian A., 1974, An Approach to Environmental Psychology, DOI DOI 10.1016/J.ELERAP.2013.07.001
   Middelweerd A, 2014, INT J BEHAV NUTR PHY, V11, DOI 10.1186/s12966-014-0097-9
   Mollen A, 2010, J BUS RES, V63, P919, DOI 10.1016/j.jbusres.2009.05.014
   Nah FFH, 2011, MIS QUART, V35, P731
   Ng YL, 2019, COMPUT HUM BEHAV, V99, P278, DOI 10.1016/j.chb.2019.05.026
   Nikhashemi SR, 2021, J RETAIL CONSUM SERV, V60, DOI 10.1016/j.jretconser.2021.102464
   OLIVER RL, 1988, J CONSUM RES, V14, P495, DOI 10.1086/209131
   Park M, 2020, J RETAIL CONSUM SERV, V52, DOI 10.1016/j.jretconser.2019.101912
   Pasca MG, 2021, TQM J, V33, P1222, DOI 10.1108/TQM-05-2020-0118
   Perry EL, 1997, DATA BASE ADV INF SY, V28, P93
   Peukert C, 2019, J MANAGE INFORM SYST, V36, P755, DOI 10.1080/07421222.2019.1628889
   Picot-Coupey K, 2021, J BUS RES, V126, P578, DOI 10.1016/j.jbusres.2019.12.018
   PODSAKOFF PM, 1986, J MANAGE, V12, P531, DOI 10.1177/014920638601200408
   Qin H, 2021, J RETAIL CONSUM SERV, V58, DOI 10.1016/j.jretconser.2020.102337
   Rauschnabel PA, 2021, INT J INFORM MANAGE, V57, DOI 10.1016/j.ijinfomgt.2020.102279
   Rauschnabel PA, 2019, J RETAIL CONSUM SERV, V49, P43, DOI 10.1016/j.jretconser.2019.03.004
   Rauschnabel PA, 2018, J BUS RES, V92, P374, DOI 10.1016/j.jbusres.2018.08.008
   Rogers R., 2017, Journal of Sports Media, V12, P25
   Ruiz-Ariza A, 2018, COMPUT EDUC, V116, P49, DOI 10.1016/j.compedu.2017.09.002
   Ruth J, 2022, INT J ENV RES PUB HE, V19, DOI 10.3390/ijerph19010596
   Scholz J, 2018, J RETAIL CONSUM SERV, V44, P11, DOI 10.1016/j.jretconser.2018.05.004
   Schubert T, 2001, PRESENCE-VIRTUAL AUG, V10, P266, DOI 10.1162/105474601300343603
   Schultze U, 2010, J INF TECHNOL-UK, V25, P434, DOI 10.1057/jit.2010.25
   Shin D, 2018, COMPUT HUM BEHAV, V78, P64, DOI 10.1016/j.chb.2017.09.012
   Simoes J, 2013, COMPUT HUM BEHAV, V29, P345, DOI 10.1016/j.chb.2012.06.007
   Song HK, 2020, INFORM TECHNOL PEOPL, V33, P1214, DOI 10.1108/ITP-02-2019-0092
   Song JH, 2008, J MARKETING, V72, P99, DOI 10.1509/jmkg.72.2.99
   STEUER J, 1992, J COMMUN, V42, P73, DOI 10.1111/j.1460-2466.1992.tb00812.x
   Suh A, 2018, COMPUT HUM BEHAV, V86, P77, DOI 10.1016/j.chb.2018.04.019
   Suh KS, 2006, BEHAV INFORM TECHNOL, V25, P99, DOI 10.1080/01449290500330398
   SWELLER J, 1988, COGNITIVE SCI, V12, P257, DOI 10.1207/s15516709cog1202_4
   Thompson WR, 2021, ACSMS HEALTH FIT J, V25, P10, DOI 10.1249/FIT.0000000000000631
   Thong JYL, 2006, INT J HUM-COMPUT ST, V64, P799, DOI 10.1016/j.ijhcs.2006.05.001
   Tredinnick L., 2018, Business Information Review, P77, DOI DOI 10.1177/0266382118778335
   Triantoro T, 2019, INT J INFORM MANAGE, V49, P242, DOI 10.1016/j.ijinfomgt.2019.06.001
   Tu RT, 2019, SPORT MANAG REV, V22, P682, DOI 10.1016/j.smr.2018.10.005
   Tussyadiah IP, 2018, TOURISM MANAGE, V66, P140, DOI 10.1016/j.tourman.2017.12.003
   Valenzuela S, 2009, J COMPUT-MEDIAT COMM, V14, P875, DOI 10.1111/j.1083-6101.2009.01474.x
   van der Heijden H, 2004, MIS QUART, V28, P695, DOI 10.2307/25148660
   Venkatesh V, 2000, MANAGE SCI, V46, P186, DOI 10.1287/mnsc.46.2.186.11926
   Venkatesh V, 2003, MIS QUART, V27, P425, DOI 10.2307/30036540
   Venkatesh V, 1999, MIS QUART, V23, P239, DOI 10.2307/249753
   Venkatesh V, 2012, MIS QUART, V36, P157
   Voorveld HAM, 2011, J ADVERTISING, V40, P77, DOI 10.2753/JOA0091-3367400206
   Wang XH, 2020, SUSTAINABILITY-BASEL, V12, DOI 10.3390/su12051998
   Wang YN, 2022, ASIA PAC J MARKET LO, V34, P110, DOI 10.1108/APJML-11-2019-0684
   Wei W, 2019, J HOSP TOUR TECHNOL, V10, P539, DOI 10.1108/JHTT-04-2018-0030
   Wei W, 2019, TOURISM MANAGE, V71, P282, DOI 10.1016/j.tourman.2018.10.024
   Wiederhold MD, 2019, CYBERPSYCH BEH SOC N, V22, P122, DOI 10.1089/cyber.2018.0027
   Wixom BH, 2005, INFORM SYST RES, V16, P85, DOI 10.1287/isre.1050.0042
   Xiao X, 2017, ICIS
   Yang Y, 2017, COMPUT HUM BEHAV, V73, P459, DOI 10.1016/j.chb.2017.03.066
   Yim MYC, 2017, J INTERACT MARK, V39, P89, DOI 10.1016/j.intmar.2017.04.001
   Zheng XB, 2019, INT J INFORM MANAGE, V48, P151, DOI 10.1016/j.ijinfomgt.2019.02.010
   Zheng YM, 2013, DECIS SUPPORT SYST, V56, P513, DOI 10.1016/j.dss.2012.11.008
   Zhou ZY, 2012, J MANAGE INFORM SYST, V29, P273, DOI 10.2753/MIS0742-1222290108
NR 139
TC 1
Z9 1
U1 19
U2 19
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 54
DI 10.1007/s10055-024-00959-4
PG 20
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA JD0T4
UT WOS:001171108800003
OA hybrid
DA 2024-08-05
ER

PT J
AU Steckel, BM
   Schwertner, R
   Buecker, J
   Nazareth, ACD
   Bizarro, L
   Oliveira, AAD
AF Steckel, Bibiana Mayer
   Schwertner, Rafaela
   Buecker, Joana
   Nazareth, Ana Clara de Paula
   Bizarro, Lisiane
   Oliveira, Alcyr Alves de
TI Immersive virtual reality applied to the rehabilitation of patients with
   lower limb amputation: a small randomized controlled trial for
   feasibility study
SO VIRTUAL REALITY
LA English
DT Article
DE Amputation; Virtual reality; Rehabilitation research; Phantom limb pain;
   Residual limb pain; Postural balance
AB Limb amputation significantly impacts the socioeconomic and health aspects of affected individuals, with clinical issues such as phantom limb pain (PLP), phantom limb telescopy (PLT), residual limb pain (RLP), and decreased balance necessitating improved treatments. Although interventions utilizing Immersive Virtual Reality (IVR) have been explored, conducting Randomized Clinical Trials (RCT) within this population presents challenges. This study serves as a feasibility study derived from a small RCT, aiming to investigate the effects of an IVR intervention protocol on individuals with lower limb amputation (LLA) while addressing methodological challenges and exploring alternative study designs. Participants were randomly assigned to either the Control Group (CG), receiving no intervention, or the Intervention Group (IG), undergoing 16 IVR sessions over 8 weeks, with twenty-one participants completing the protocol. Sessions involved observing physical exercises via a head-mounted display. All participants were assessed for pain and balance pre- and post-intervention. IG participants were also evaluated for pain, sense of presence in the virtual environment, and cybersickness on intervention days. Results indicated a significant negative correlation between RLP and time since amputation in the Intervention Group. Analysis of results between IG and CG post-assessment suggests potential benefits of IVR in improving balance and reducing PLT. Despite challenges related to sample size and participant retention, multicenter collaborations and home-based interventions are proposed to mitigate these limitations. This feasibility study lays a foundation for future research aiming to optimize VR interventions for improved outcomes in patients with LLA.
C1 [Steckel, Bibiana Mayer; Oliveira, Alcyr Alves de] Fed Univ Hlth Sci Porto Alegre UFCSPA, Grad Program Rehabil Sci, Neurosci & Expt Virtual Real Lab, Porto Alegre, Brazil.
   [Schwertner, Rafaela; Buecker, Joana] Univ Vale Taquari Univates, Grad Program Med Sci PPGCM, Lajeado, Brazil.
   [Nazareth, Ana Clara de Paula; Bizarro, Lisiane] Univ Fed Rio Grande do Sul, Inst Psychol, Dept Dev & Personal Psychol, Grad Program Psychol, Porto Alegre, Brazil.
C3 Universidade Federal de Ciencias da Saude de Porto Alegre; Universidade
   Federal do Rio Grande do Sul
RP Steckel, BM (corresponding author), Fed Univ Hlth Sci Porto Alegre UFCSPA, Grad Program Rehabil Sci, Neurosci & Expt Virtual Real Lab, Porto Alegre, Brazil.
EM bibiana.steckel@ufcspa.edu.br
RI Oliveira, Alcyr/D-2991-2014; BIZARRO, LISIANE/F-7872-2012
OI Oliveira, Alcyr/0000-0002-0747-7835; Schwertner,
   Rafaela/0000-0001-5074-4102; BUCKER, JOANA/0000-0001-8335-2404; BIZARRO,
   LISIANE/0000-0003-3070-5944; Mayer Steckel, Bibiana/0000-0002-1982-892X
CR Ambron E, 2021, NEUROREHAB NEURAL RE, V35, P1100, DOI 10.1177/15459683211054164
   Ambron E, 2018, FRONT NEUROL, V9, DOI 10.3389/fneur.2018.00067
   Bourque MO, 2019, TRIALS, V20, DOI 10.1186/s13063-019-3929-8
   Branco P., 2013, Revista da Sociedade Portuguesa de Medicina Fisica e de Reabilitacao I Vol, V24, DOI DOI 10.25759/SPMFR.106
   Cassani R, 2020, J NEUROENG REHABIL, V17, DOI 10.1186/s12984-020-00780-5
   Charkhkar H, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-63936-2
   Chau Brian, 2017, Innov Clin Neurosci, V14, P3
   Cheung JC, 2023, Eng Regeneration, V4, P134, DOI [10.1016/j.engreg.2023.02.002, DOI 10.1016/J.ENGREG.2023.02.002]
   Choi JW, 2020, IEEE T NEUR SYS REH, V28, P1614, DOI 10.1109/TNSRE.2020.2998123
   Colmenero LH, 2018, PROSTHET ORTHOT INT, V42, P288, DOI 10.1177/0309364617740230
   Costa Víctor de Oliveira, 2021, Dement. neuropsychol., V15, P275, DOI 10.1590/1980-57642021dn15-020016
   DATASUS Departamento de Informatica do SUS-DATASUS, 2022, Informacoes de Saude (TABNET)
   Souza Ylkiany Pereira de, 2019, J. vasc. bras., V18, pe20190064, DOI 10.1590/1677-5449.190064
   Demidoff AO, 2007, Ciencias & Cognicao, V12
   Demirtas A, 2020, Haydarpasa Numune Training and Research Hospital Medical Journal
   dos Petry R, 2018, Revista Eletronica Estacio Saude, V7, P14
   Erlenwein J, 2021, PAIN REP, V6, DOI 10.1097/PR9.0000000000000888
   Escamilla-Nunez R, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20061628
   Ghoseiri K, 2018, MILITARY MED RES, V5, DOI 10.1186/s40779-018-0151-z
   GOERGEN DIEGO INÁCIO, 2022, Rev. Col. Bras. Cir., V49, pe20223138, DOI 10.1590/0100-6991e-20223138-en
   Hao J, 2023, AM J PHYS MED REHAB, V102, P468, DOI 10.1097/PHM.0000000000002150
   Horsak B, 2021, FRONT BIOENG BIOTECH, V9, DOI 10.3389/fbioe.2021.780314
   Kennedy R.S., 1993, Int. J. Aviat. Psy, P203
   Kokkinara E, 2016, SCI REP-UK, V6, DOI 10.1038/srep28879
   Lee SH, 2019, Arch Phys Med Rehabil, V100, P138, DOI [10.1016/j.apmr.2019.08.424, DOI 10.1016/J.APMR.2019.08.424]
   Lemaire E, 2013, Recommendation for Defining Participants in Prosthetics Research
   Lemons Michele L, 2021, J Undergrad Neurosci Educ, V19, pC1
   Lima DC, 2021, Revista Eletronica Acervo Saude, V13, P6573, DOI [10.25248/reas.e6573.2021, DOI 10.25248/REAS.E6573.2021]
   Marques AP, 2013, BRAZ J PHYS THER, V17, P170, DOI 10.1590/S1413-35552012005000072
   Marques LM, 2021, Revista Eletronica Acervo Saude, V13, P6696, DOI [10.25248/reas.e6696.2021, DOI 10.25248/REAS.E6696.2021]
   Mayer A, 2008, PROSTHET ORTHOT INT, V32, P363, DOI 10.1080/03093640802024971
   McDonald CL, 2021, PROSTHET ORTHOT INT, V45, P105, DOI 10.1177/0309364620972258
   Ministerio da Saude do Brasil, 2024, Servicos Habilitados em Todo Territorio Nacional
   Safee MKM, 2021, OCCUP THER INT, V2021, DOI 10.1155/2021/4357473
   Neptune RR, 2022, Determination of Fall Risk for Lower Limb Amputees
   Osumi M, 2019, PAIN MED, V20, P1038, DOI 10.1093/pm/pny269
   Perry BN, 2018, FRONT NEUROL, V9, DOI 10.3389/fneur.2018.00770
   Pimenta C A, 1996, Rev Esc Enferm USP, V30, P473
   Raspopovic S, 2020, SCIENCE, V370, P290, DOI 10.1126/science.abb1073
   Rutledge T, 2019, PAIN MED, V20, P2051, DOI 10.1093/pm/pnz121
   Sava SL, 2018, J PAIN RES, V11, P1821, DOI 10.2147/JPR.S160276
   Silva DS, 2019, Ciencia Saude, V12, DOI [10.15448/1983-652X.2019.1.31383, DOI 10.15448/1983-652X.2019.1.31383]
   Stanica IC, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20216045
   Stankevicius A, 2021, EUR J PAIN, V25, P23, DOI 10.1002/ejp.1657
   Xavier Nycole Filincowsky Ribeiro, 2020, BrJP, V3, P359, DOI 10.5935/2595-0118.20200182
   Yoshimura M, 2020, J NEUROENG REHABIL, V17, DOI 10.1186/s12984-020-00743-w
   Zad M, 2020, Curr Treat Options Gastroenterol, V18, P33, DOI 10.1007/s11938-020-00276-0
   Zaheer A, 2021, BMC NEUROL, V21, DOI 10.1186/s12883-021-02441-z
NR 48
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAY 23
PY 2024
VL 28
IS 2
AR 115
DI 10.1007/s10055-024-01015-x
PG 15
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA RU7W5
UT WOS:001230249900003
OA hybrid
DA 2024-08-05
ER

PT J
AU Clark, L
   Iskandarani, ME
   Riggs, S
AF Clark, Logan
   Iskandarani, Mohamad El
   Riggs, Sara
TI Reaching interactions in virtual reality: the effect of movement
   direction, hand dominance, and hemispace on the kinematic properties of
   inward and outward reaches
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Movement kinematics; Goal-directed reaching; Virtual
   hand interaction; Motor control
ID MANUAL ASYMMETRIES; POINTING MOVEMENTS; UPPER-LIMB; FITTS LAW;
   SELECTION; CONSTRAINTS; HANDEDNESS; HEALTHY; FREEDOM; TRUNK
AB Recent literature has revealed that when users reach to select objects in VR, they can adapt how they move (i.e., the kinematic properties of their reaches) depending on the: (1) direction they move, (2) hand they use, and (3) side of the body where the movement occurs. In the present work, we took a more detailed look at how kinematic properties of reaching movements performed in VR change as a function of movement direction for reaches performed on each side of the body using each hand. We focused on reaches in 12 different directions that either involved moving inward (toward the body midline) or outward (away from the body midline). Twenty users reached in each direction on both left and right sides of their body, using both their dominant and non-dominant hands. The results provided a fine-grained account of how kinematic properties of virtual hand reaches change as a function of movement direction when users reach on either side of their body using either hand. The findings provide practitioners insights on how to interpret the kinematic properties of reaching behaviors in VR, which has applicability in emerging contexts that include detecting VR usability issues and using VR for stroke rehabilitation.
C1 [Clark, Logan; Iskandarani, Mohamad El; Riggs, Sara] Univ Virginia, Syst & Informat Engn Dept, Charlottesville, VA 22904 USA.
C3 University of Virginia
RP Riggs, S (corresponding author), Univ Virginia, Syst & Informat Engn Dept, Charlottesville, VA 22904 USA.
EM sriggs@virginia.edu
OI Riggs, Sara/0000-0002-0112-9469; El Iskandarani,
   Mohamad/0000-0002-2779-474X
FU Meta Research PhD Fellowship
FX Logan Clark was supported by a Meta Research PhD Fellowship. The authors
   would also like to thank Peiyu Zhang and Jeffrey Richbart for the
   development of the simulator used in this study.
CR Abich J IV, 2021, VIRTUAL REAL-LONDON, V25, P919, DOI 10.1007/s10055-020-00498-8
   Alt Murphy M, 2015, PHYS THER REV, V20, P137, DOI 10.1179/1743288X15Y.0000000002
   Alt Murphy M, 2013, NEUROREHAB NEURAL RE, V27, P844, DOI 10.1177/1545968313491008
   Alt Murphy M, 2011, NEUROREHAB NEURAL RE, V25, P71, DOI 10.1177/1545968310370748
   Archambault P, 1999, EXP BRAIN RES, V126, P55, DOI 10.1007/s002210050716
   Arlati S, 2022, VIRTUAL REAL-LONDON, V26, P885, DOI 10.1007/s10055-021-00603-5
   Balasubramanian S, 2015, J NEUROENG REHABIL, V12, DOI 10.1186/s12984-015-0090-9
   Balasubramanian S, 2012, IEEE T BIO-MED ENG, V59, P2126, DOI 10.1109/TBME.2011.2179545
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Batmaz AU, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P585, DOI [10.1109/VR.2019.8797975, 10.1109/vr.2019.8797975]
   Batmaz AU, 2020, 26 ACM S VIRTUAL REA, P1, DOI [10.1145/3385956.3418971, DOI 10.1145/3385956.3418971]
   Bennett SJ, 2012, EXP BRAIN RES, V216, P445, DOI 10.1007/s00221-011-2947-x
   Berret B, 2021, PLOS COMPUT BIOL, V17, DOI 10.1371/journal.pcbi.1009047
   Boud A. C., 1999, 1999 IEEE International Conference on Information Visualization (Cat. No. PR00210), P32, DOI 10.1109/IV.1999.781532
   BRADSHAW JL, 1990, NEUROPSYCHOLOGIA, V28, P917, DOI 10.1016/0028-3932(90)90108-Z
   Burkitt JJ, 2017, J MOTOR BEHAV, V49, P129, DOI 10.1080/00222895.2016.1161592
   Carey DP, 1996, EXP BRAIN RES, V112, P496
   Carlson P, 2015, IEEE T VIS COMPUT GR, V21, P770, DOI 10.1109/TVCG.2015.2393871
   CARSON RG, 1990, NEUROPSYCHOLOGIA, V28, P1215, DOI 10.1016/0028-3932(90)90056-T
   Cha Y, 2013, INT J IND ERGONOM, V43, P350, DOI 10.1016/j.ergon.2013.05.005
   CHUA R, 1993, HUM MOVEMENT SCI, V12, P365, DOI 10.1016/0167-9457(93)90026-L
   Clark Logan D., 2020, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V64, P1600, DOI 10.1177/1071181320641385
   Clark Logan D., 2019, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V63, P2318, DOI 10.1177/1071181319631156
   Clark LD, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2023), DOI 10.1145/3544548.3581191
   Clark LD, 2020, INT J HUM-COMPUT ST, V139, DOI 10.1016/j.ijhcs.2020.102413
   Dominguez-Tellez P, 2020, GAMES HEALTH J, V9, P1, DOI 10.1089/g4h.2019.0043
   Elliott D, 2020, EXP BRAIN RES, V238, P2685, DOI 10.1007/s00221-020-05952-2
   Elliott D, 2017, NEUROSCI BIOBEHAV R, V72, P95, DOI 10.1016/j.neubiorev.2016.11.016
   Elliott D, 2010, PSYCHOL BULL, V136, P1023, DOI 10.1037/a0020958
   Farook SA., 2018, J Neurosci, V8, P16
   Feitosa JA, 2022, J NEURAL ENG, V19, DOI 10.1088/1741-2552/ac456e
   Flindall JW, 2014, LATERALITY, V19, P489, DOI 10.1080/1357650X.2013.862540
   Gaveau J, 2016, ELIFE, V5, DOI 10.7554/eLife.16394
   Gill G, 2019, Reabilitacijos Mokslai Slauga Kineziterapija Ergoterapija, DOI [10.33607/rmske.v2i21.826, DOI 10.33607/RMSKE.V2I21.826]
   Goh HT, 2022, J MOTOR BEHAV, V54, P14, DOI 10.1080/00222895.2021.1871877
   Grossman T, 2004, P SIGCHI C HUM FACT, V6, P447, DOI [DOI 10.1145/985692.985749, 10.1145/985692.985749]
   Gutierrez-Herrera M, 2017, CORTEX, V90, P46, DOI 10.1016/j.cortex.2017.02.009
   Hajihosseinali M, 2022, MED ENG PHYS, V108, DOI 10.1016/j.medengphy.2022.103880
   Hansen S, 2007, BEHAV RES METHODS, V39, P748, DOI 10.3758/BF03192965
   Henrikson R, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376489
   HOLM S, 1979, SCAND J STAT, V6, P65
   Hox J., 2010, MULTILEVEL ANAL TECH, DOI DOI 10.4324/9780203852279
   Johnstone L.T., 2015, Hemispheric asymmetries: Behavioural, kinematic, and electrophysiological predictors of cerebral organisation
   Kaminer C., 2014, Proceedings of the 16th International ACM SIGACCESS Conference on Computers Accessibility, P299, DOI [10.1145/2661334.2661340, DOI 10.1145/2661334.2661340]
   Kaminska D, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22041342
   Kanzler CM, 2020, NPJ DIGIT MED, V3, DOI 10.1038/s41746-020-0286-7
   Keuden RF, 2007, CORTEX, V43, P531, DOI 10.1016/S0010-9452(08)70247-8
   Kim W, 2011, J MOTOR BEHAV, V43, P403, DOI 10.1080/00222895.2011.619222
   Knaut LA, 2009, ARCH PHYS MED REHAB, V90, P793, DOI 10.1016/j.apmr.2008.10.030
   Claudio APK, 2012, PERCEPT MOTOR SKILL, V115, P153, DOI 10.2466/10.25.PMS.115.4.153-165
   LAIRD NM, 1982, BIOMETRICS, V38, P963, DOI 10.2307/2529876
   Laver K, 2018, Cochrane Database Syst Rev, V164
   Lee HS, 2019, BIOMED RES INT-UK, V2019, DOI 10.1155/2019/7595639
   Lubos P, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P11, DOI 10.1109/3DUI.2014.6798834
   Lyons J, 2006, EXP BRAIN RES, V174, P95, DOI 10.1007/s00221-006-0426-6
   Machuca MDB, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300437
   Mathworks Inc, 2021, MATLAB
   Mekbib DB, 2020, BRAIN INJURY, V34, P456, DOI 10.1080/02699052.2020.1725126
   Mieschke PE, 2001, BRAIN COGNITION, V45, P129, DOI 10.1006/brcg.2000.1262
   Mine MR, 1995, TR95-018, P1
   Mira RM, 2021, CURR RES PHYSIOL, V4, P60, DOI 10.1016/j.crphys.2021.02.005
   Montalban M A., 2020, Rev Cientifica Soc Enferm Neurologica Engl Ed, V52, P19, DOI DOI 10.1016/J.SEDENG.2020.01.001
   Murata A, 2001, HUM MOVEMENT SCI, V20, P791, DOI 10.1016/S0167-9457(01)00058-6
   Murcia-López M, 2018, IEEE T VIS COMPUT GR, V24, P1574, DOI 10.1109/TVCG.2018.2793638
   Nelson EL, 2018, J MOTOR BEHAV, V50, P381, DOI 10.1080/00222895.2017.1363698
   OLDFIELD RC, 1971, NEUROPSYCHOLOGIA, V9, P97, DOI 10.1016/0028-3932(71)90067-4
   Oliveira FTP, 2005, MOTOR CONTROL, V9, P101, DOI 10.1123/mcj.9.1.101
   Osu R, 2011, J NEUROENG REHABIL, V8, DOI 10.1186/1743-0003-8-62
   Papaxanthis C, 1998, NEUROSCI LETT, V253, P103, DOI 10.1016/S0304-3940(98)00604-1
   Park S, 2021, NEUROREHABILITATION, V48, P1, DOI 10.3233/NRE-201501
   Poupyrev I, 1999, J VISUAL LANG COMPUT, V10, P19, DOI 10.1006/jvlc.1998.0112
   R Core Team, 2021, R (4.0.5)
   Radhakrishnan U, 2021, BEHAV INFORM TECHNOL, V40, P1310, DOI 10.1080/0144929X.2021.1954693
   Refai MIM, 2021, J NEUROENG REHABIL, V18, DOI 10.1186/s12984-021-00949-6
   Roberts JW, 2020, EXP BRAIN RES, V238, P741, DOI 10.1007/s00221-020-05750-w
   Sainburg RL, 2000, J NEUROPHYSIOL, V83, P2661, DOI 10.1152/jn.2000.83.5.2661
   Sainburg RL, 2002, EXP BRAIN RES, V142, P241, DOI 10.1007/s00221-001-0913-8
   Sainburg RL, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.01092
   Salisbury Joseph P., 2020, CHI PLAY '20: Extended Abstracts of the 2020 Annual Symposium on Computer-Human Interaction in Play, P114, DOI 10.1145/3383668.3419935
   Schaffer JE, 2017, NEUROSCIENCE, V350, P54, DOI 10.1016/j.neuroscience.2017.03.025
   Schielzeth H, 2020, METHODS ECOL EVOL, V11, P1141, DOI 10.1111/2041-210X.13434
   Schwarz A, 2019, STROKE, V50, P718, DOI 10.1161/STROKEAHA.118.023531
   Shemmell J, 2007, EXP BRAIN RES, V176, P150, DOI 10.1007/s00221-006-0605-5
   Stewart JC, 2013, J NEUROENG REHABIL, V10, DOI 10.1186/1743-0003-10-27
   Stins JF, 2001, LATERALITY, V6, P347
   Thompson SG, 2007, HUM MOVEMENT SCI, V26, P11, DOI 10.1016/j.humov.2006.09.001
   Todorov E, 2002, NAT NEUROSCI, V5, P1226, DOI 10.1038/nn963
   Tomlinson T, 2012, J MOTOR BEHAV, V44, P13, DOI 10.1080/00222895.2011.636398
   van Doorn RRA, 2008, HUM MOVEMENT SCI, V27, P551, DOI 10.1016/j.humov.2007.11.006
   Vandenberghe A, 2012, GAIT POSTURE, V35, P579, DOI 10.1016/j.gaitpost.2011.11.028
   Waters E, 2021, I IEEE EMBS C NEUR E, P757, DOI 10.1109/NER49283.2021.9441448
   Woodworth RS, 1899, PSYCHOL REV-MONOGR S, V3, P1
   Xiao X, 2019, MATH BIOSCI ENG, V16, P1611, DOI 10.3934/mbe.2019077
NR 93
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 43
DI 10.1007/s10055-023-00930-9
PG 29
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA HS2Y7
UT WOS:001161441900001
OA hybrid
DA 2024-08-05
ER

PT J
AU Dall'Olio, L
   Amrein, O
   Gianettoni, L
   Martarelli, CS
AF Dall'Olio, Lucas
   Amrein, Olivier
   Gianettoni, Lavinia
   Martarelli, Corinna S.
TI The impact of fantasy on young children's recall: a virtual reality
   approach
SO VIRTUAL REALITY
LA English
DT Article
DE Fantasy; Virtual reality; Recall; Preschool children
ID TELEVISION; BELIEFS
AB Educational materials for preschool children are often embedded with fantastical elements. Nevertheless, there is still little empirical evidence on their effectiveness, especially as concerns long-term retention. Virtual reality offers new ecological possibilities for investigating this type of learning, especially through the impact of immersion. In a between-design study, 168 children aged four to six years followed a virtual reality presentation on China presented by either a realistic young girl or an anthropomorphic animal. The level of immersion was manipulated, with half of the children following the presentation in immersive virtual reality (IVR) and the other half in desktop virtual reality (D-VR). Participants were asked to complete a new/old recognition task and a quiz task immediately after the presentation and once again one week later, with an additional transfer task being added for the second series. One week later, children performed significantly better on the new/old recognition task in the realistic condition when compared to the anthropomorphic condition. However, there were no differences observed in the quiz task and in the transfer task. It therefore seems that, under certain conditions, children remember a cultural presentation better when it is presented by a realistic avatar. The results further showed that the children performed significantly worse in the IVR conditions on all tasks. A possible explanation for this result is that IVR demands excessive cognitive resources from preschool children. Further studies should explore this unexpected result, as well as what could be done to make IVR effective for learning in preschool children.
C1 [Dall'Olio, Lucas; Amrein, Olivier; Martarelli, Corinna S.] UniDistance Suisse, Fac Psychol, Brig, Switzerland.
   [Gianettoni, Lavinia] Univ Lausanne, Inst Psychol, Lausanne, Switzerland.
C3 University of Lausanne
RP Dall'Olio, L; Martarelli, CS (corresponding author), UniDistance Suisse, Fac Psychol, Brig, Switzerland.
EM lucas.dallolio@unidistance.ch; corinna.martarelli@fernuni.ch
FU Swiss National Science Foundation [100019_185477]; Swiss National
   Science Foundation (SNF) [100019_185477] Funding Source: Swiss National
   Science Foundation (SNF)
FX This study was supported by the Swiss National Science Foundation (grant
   number 100019_185477 awarded to C.S.M.)
CR [Anonymous], 2021, jamovi
   Araiza-Alba P, 2021, COMPUT EDUC, V164, DOI 10.1016/j.compedu.2020.104121
   Aydin E, 2021, J EXP CHILD PSYCHOL, V209, DOI 10.1016/j.jecp.2021.105170
   Babu SK, 2018, IEEE INT CONF ADV LE, P385, DOI 10.1109/ICALT.2018.00094
   Bernath J, 2020, ADELE+: Der Medienumgang von Kindern im Vorschulalter (4-6 Jahre)-Chancen und Risiken fur die Gesundheit
   Blascovich J, 2002, PSYCHOL INQ, V13, P103, DOI 10.1207/S15327965PLI1302_01
   Bonus JA, 2018, HUM COMMUN RES, V44, P449, DOI 10.1093/hcr/hqy009
   Bonus JA, 2019, COMMUN RES, V46, P375, DOI 10.1177/0093650215609980
   Borzekowski DLG, 2010, J APPL DEV PSYCHOL, V31, P298, DOI 10.1016/j.appdev.2010.05.002
   Buttussi F, 2018, IEEE T VIS COMPUT GR, V24, P1063, DOI 10.1109/TVCG.2017.2653117
   Chlebuch N, 2023, PSYCHOL POP MEDIA, V12, P77, DOI 10.1037/ppm0000388
   CLEMENTS WA, 1994, COGNITIVE DEV, V9, P377, DOI 10.1016/0885-2014(94)90012-4
   Cole CF, 2003, INT J BEHAV DEV, V27, P409, DOI 10.1080/01650250344000019
   Conrad M, 2021, J EXP CHILD PSYCHOL, V201, DOI 10.1016/j.jecp.2020.104985
   Corriveau KH, 2015, COGNITIVE DEV, V34, P76, DOI 10.1016/j.cogdev.2014.12.005
   Faul F, 2007, BEHAV RES METHODS, V39, P175, DOI 10.3758/BF03193146
   Fisch SM, 2000, MEDIA PSYCHOL, V2, P63, DOI 10.1207/S1532785XMEP0201_4
   Fisch SM, Transfer of learning in informal education
   Ganea PA, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.00283
   Geerdts M.S., 2015, IMAGINATION COGNITIO, V36, P5, DOI [10.1177/, DOI 10.1177/0276236615611798, https://doi.org/10.1177/0276236615611798]
   Geerdts MS, 2016, CHILD DEV PERSPECT, V10, P10, DOI 10.1111/cdep.12153
   Goldstein TR, 2022, J EXP CHILD PSYCHOL, V213, DOI 10.1016/j.jecp.2021.105275
   Goldstein TR, 2020, PSYCHOL POP MEDIA, V9, P214, DOI 10.1037/ppm0000222
   Hopkins EJ, 2021, J EXP CHILD PSYCHOL, V210, DOI 10.1016/j.jecp.2021.105212
   Hopkins EJ, 2021, J EXP CHILD PSYCHOL, V203, DOI 10.1016/j.jecp.2020.105047
   Hopkins EJ, 2017, DEV REV, V43, P48, DOI 10.1016/j.dr.2016.11.001
   Larsen NE, 2018, DEVELOPMENTAL SCI, V21, DOI 10.1111/desc.12590
   Leder J, 2019, SAFETY SCI, V111, P271, DOI 10.1016/j.ssci.2018.07.021
   Li H, 2019, J CHILD MEDIA, V13, P149, DOI 10.1080/17482798.2019.1570960
   Macmillan N. A., 2005, DETECTION THEORY USE
   Makransky G, 2022, EDUC PSYCHOL REV, V34, P1771, DOI 10.1007/s10648-022-09675-4
   Makransky G, 2019, LEARN INSTR, V60, P225, DOI 10.1016/j.learninstruc.2017.12.007
   Makransky G, 2018, ETR&D-EDUC TECH RES, V66, P1141, DOI 10.1007/s11423-018-9581-2
   Mares ML, 2014, DEV PSYCHOL, V50, P2498, DOI 10.1037/a0038041
   Martarelli CS, 2015, COGNITIVE DEV, V36, P111, DOI 10.1016/j.cogdev.2015.10.001
   Martarelli CS, 2013, J COGN DEV, V14, P141, DOI 10.1080/15248372.2011.638685
   Mayer RE, 2009, MULTIMEDIA LEARNING, 2ND EDITION, P1
   Meyer OA, 2019, COMPUT EDUC, V140, DOI 10.1016/j.compedu.2019.103603
   Nguyentran G, 2023, PSYCHOL POP MEDIA, DOI 10.1037/ppm0000467
   Olmos-Raya E., 2018, Eurasia Journal of Mathematics, Science and Technology Education, V14, P2045, DOI [10.29333/ejmste/85874, DOI 10.29333/EJMSTE/85874]
   Parong J, 2018, J EDUC PSYCHOL, V110, P785, DOI 10.1037/edu0000241
   Peirce J, 2019, BEHAV RES METHODS, V51, P195, DOI 10.3758/s13428-018-01193-y
   PERNER J, 1987, BRIT J DEV PSYCHOL, V5, P125, DOI 10.1111/j.2044-835X.1987.tb01048.x
   Richert RA, 2022, J EXP CHILD PSYCHOL, V222, DOI 10.1016/j.jecp.2022.105474
   Richert RA, 2017, INFANT CHILD DEV, V26, DOI 10.1002/icd.2009
   Richert RA, 2011, CHILD DEV, V82, P1106, DOI 10.1111/j.1467-8624.2011.01603.x
   Rupp MA, 2019, COMPUT EDUC, V128, P256, DOI 10.1016/j.compedu.2018.09.015
   Sharon T, 2004, BRIT J DEV PSYCHOL, V22, P293, DOI 10.1348/026151004323044627
   Slater M, 1997, PRESENCE-VIRTUAL AUG, V6, P603, DOI 10.1162/pres.1997.6.6.603
   Slater M., 2003, PRESENCE CONNECT, V3, P1, DOI DOI 10.3389/FNINS.2019.01409
   Smith SA, 2019, PSYCHON B REV, V26, P1213, DOI 10.3758/s13423-019-01605-w
   Thibodeau RB, 2016, J EXP CHILD PSYCHOL, V145, P120, DOI 10.1016/j.jecp.2016.01.001
   Walker CM, 2015, CHILD DEV, V86, P310, DOI 10.1111/cdev.12287
   Wechsler D., 2012, Wechsler preschool and primary scale of intelligence, Vfourth
   Weisberg DS, 2022, J EXP CHILD PSYCHOL, V221, DOI 10.1016/j.jecp.2022.105445
   Weisberg DS, 2020, INFANT CHILD DEV, V29, DOI 10.1002/icd.2182
   WIMMER H, 1983, COGNITION, V13, P103, DOI 10.1016/0010-0277(83)90004-5
   Woolley JD, 2007, DEVELOPMENTAL SCI, V10, P681, DOI 10.1111/j.1467-7687.2007.00612.x
NR 58
TC 0
Z9 0
U1 10
U2 10
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 10
DI 10.1007/s10055-023-00911-y
PG 21
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA EC7V9
UT WOS:001136790600001
OA Green Submitted, Bronze
DA 2024-08-05
ER

PT J
AU Joolee, JB
   Hashem, MS
   Hassan, W
   Jeon, S
AF Joolee, Joolekha Bibi
   Hashem, Mohammad Shadman
   Hassan, Waseem
   Jeon, Seokhee
TI Deep encoder-decoder network based data-driven method for impact
   feedback rendering on head during earthquake
SO VIRTUAL REALITY
LA English
DT Article
DE Impact feedback; Data-driven approach; Convolutional bidirectional long
   short-term memory encoder-decoder; Max-min extraction
ID HAPTIC TEXTURES
AB In safety training simulators, realistic haptic feedback is essential to make people construct accurate situation awareness through experiencing. In this regard, this paper presents a new and innovative system that provides the haptic experience of falling objects on user's head during an earthquake. Special focus was on the accurate reproduction of impact feedback when various objects fall on the head. To this end, we propose a novel data-driven approach. This approach first collects 3-axis acceleration signals during real collision under several impact velocities. Afterward, 3D acceleration data is abstracted to a 1D acceleration profile using our novel max-min extraction approach. The impact signal for an arbitrary velocity is interpolated using a deep convolutional bidirectional long short-term memory encoder-decoder model. Rendering hardware is also implemented using high performance voice-coil vibrotactile actuator. Numerical and subjective evaluations are carried out to evaluate the performance of the proposed approach.Kindly check and confirm the edit made in the title.I confirm the edit is okay.Please confirm if the author names are presented accurately and in the correct sequence (given name, middle name/initial, family name). Authors Given name: [Joolekha Bibi] Last name: [Joolee], Given name: [Mohammad Shadman] Last name: [Hashem]. Also, kindly confirm the details in the metadata are correct.Yes, the author names are presented accurately and in the correct sequence.
C1 [Joolee, Joolekha Bibi; Hashem, Mohammad Shadman; Hassan, Waseem; Jeon, Seokhee] Kyung Hee Univ, Dept Comp Sci & Engn, 446-701 Seocheon Dong, Yongin 446701, Gyeonggi Do, South Korea.
C3 Kyung Hee University
RP Jeon, S (corresponding author), Kyung Hee Univ, Dept Comp Sci & Engn, 446-701 Seocheon Dong, Yongin 446701, Gyeonggi Do, South Korea.
EM julekhajulie@gmail.com; ayon7019@gmail.com; waseem.h@khu.ac.kr;
   jeon@khu.ac.kr
RI Hassan, Waseem/HTT-2008-2023
OI Hassan, Waseem/0000-0003-3922-5648
FU This research was funded by the Preventive Safety Service Technology
   Development Program funded by the Korean Ministry of Interior and Safety
   under Grant 2019-MOIS34-001. [2019-MOIS34-001]; Preventive Safety
   Service Technology Development Program - Korean Ministry of Interior and
   Safety
FX This research was funded by the Preventive Safety Service Technology
   Development Program funded by the Korean Ministry of Interior and Safety
   under Grant 2019-MOIS34-001.
CR Abdulali A, 2020, IEEE T HAPTICS, V13, P611, DOI 10.1109/TOH.2020.2965449
   Abdulali A, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18010237
   Abdulali A, 2016, LECT NOTES COMPUT SC, V9775, P228, DOI 10.1007/978-3-319-42324-1_23
   Bortone I, 2020, J NEUROENG REHABIL, V17, DOI 10.1186/s12984-020-00771-6
   Chan LSH, 2009, JCPC: 2009 JOINT CONFERENCE ON PERVASIVE COMPUTING, P853, DOI 10.1109/JCPC.2009.5420068
   Culbertson H, 2014, IEEE T HAPTICS, V7, P381, DOI 10.1109/TOH.2014.2316797
   Gallo D, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376828
   GILLILAND K, 1994, HUM FACTORS, V36, P700, DOI 10.1177/001872089403600410
   Girbés V, 2016, IEEE T HAPTICS, V9, P345, DOI 10.1109/TOH.2016.2531686
   Gong XL, 2015, IEICE T INF SYST, VE98D, P2242, DOI 10.1587/transinf.2015EDP7165
   Gunther Sebastian, 2020, CHI EA '20: Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems, P1, DOI 10.1145/3334480.3382916
   Handa T, 2019, P IEEE WORLD HAPTICS
   Hassan W, 2020, IEEE T IND ELECTRON, V67, P667, DOI 10.1109/TIE.2019.2914572
   Hosseini A, 2016, IEEE VTS VEH TECHNOL, DOI 10.1109/VTCSpring.2016.7504430
   Hwang JD, 2004, 12TH INTERNATIONAL SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P24, DOI 10.1109/HAPTIC.2004.1287174
   Joolee JB, 2022, IEEE T HAPTICS, V15, P62, DOI 10.1109/TOH.2021.3137936
   Kaul OB, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P3729, DOI 10.1145/3025453.3025684
   Kim S, 2018, P INT ASIAHAPTICS C, P173
   Kuchenbecker KJ, 2006, IEEE T VIS COMPUT GR, V12, P219, DOI 10.1109/TVCG.2006.32
   Landin N, 2010, LECT NOTES COMPUT SC, V6192, P79, DOI 10.1007/978-3-642-14075-4_12
   Li CY, 2017, IEEE T VIS COMPUT GR, V23, P1388, DOI 10.1109/TVCG.2017.2656958
   Liang H, 2018, PROCEEDINGS OF THE 16TH ACM SIGGRAPH INTERNATIONAL CONFERENCE ON VIRTUAL-REALITY CONTINUUM AND ITS APPLICATIONS IN INDUSTRY (VRCAI 2018), DOI 10.1145/3284398.3284417
   Lopes P, 2015, UIST'15: PROCEEDINGS OF THE 28TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P11, DOI 10.1145/2807442.2807443
   Lovreglio R, 2018, ADV ENG INFORM, V38, P670, DOI 10.1016/j.aei.2018.08.018
   Okada T, 2021, VIRTUAL REAL-LONDON, V25, P233, DOI 10.1007/s10055-020-00452-8
   Osgouei RH, 2020, IEEE T HAPTICS, V13, P298, DOI 10.1109/TOH.2019.2932990
   Park C, 2019, 2019 IEEE WORLD HAPTICS CONFERENCE (WHC), P449, DOI [10.1109/WHC.2019.8816116, 10.1109/whc.2019.8816116]
   Park G, 2019, 2019 IEEE WORLD HAPTICS CONFERENCE (WHC), P467, DOI [10.1109/WHC.2019.8816148, 10.1109/whc.2019.8816148]
   Park G, 2017, IEEE T HAPTICS, V10, P325, DOI 10.1109/TOH.2016.2614804
   Poorten EV, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P1547, DOI 10.1109/IROS.2006.282039
   Pyo D, 2015, SENSOR ACTUAT A-PHYS, V233, P460, DOI 10.1016/j.sna.2015.07.037
   Ramirez M, 2009, FAM COMMUNITY HEALTH, V32, P105, DOI 10.1097/FCH.0b013e3181994662
   Romano JM, 2010, IEEE INT CONF ROBOT, P1815, DOI 10.1109/ROBOT.2010.5509853
   Sagheer A, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-55320-6
   Seo SW, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3364725
   Shin S, 2020, IEEE ACCESS, V8, P149825, DOI 10.1109/ACCESS.2020.3015861
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   statista, Development of the number of earthquakes (m5+) worldwide from 2000 to 2019
   Tsai HR, 2019, PROCEEDINGS OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY (UIST 2019), P429, DOI 10.1145/3332165.3347931
   Xu Z, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9173465
   Yim S, 2016, IEEE T HAPTICS, V9, P548, DOI 10.1109/TOH.2016.2571690
NR 41
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 23
DI 10.1007/s10055-023-00906-9
PG 18
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA FE1K8
UT WOS:001143992600001
OA hybrid
DA 2024-08-05
ER

PT J
AU Giacomelli, L
   Sölch, CM
   Ledermann, K
AF Giacomelli, L.
   Solch, C. Martin
   Ledermann, K.
TI The effect of virtual reality interventions on reducing pain intensity
   in chronic pain patients: a systematic review
SO VIRTUAL REALITY
LA English
DT Article
DE Chronic pain; Virtual reality; VR; Pain management; Pain; Adult
ID BACK-PAIN; MANAGEMENT; THERAPY; SCALE
AB The use of virtual reality (VR) for the management of chronic pain is an intriguing topic. Given the abundance of VR stuies and the numerous opportunities presented by this technology in healthcare, a systematic review that focuses on VR and its applications in chronic pain is necessary to shed light on the various modalities available and their actual effectiveness. This systematic review aims to explore the efficacy of reducing pain and improving pain management through CR interventions for people suffering from chronic pain. Following the PRISMA guidelines, data collection was conducted between December 2020 and February 2021 from the following databases: Cochrane Evidence, JSTOR, Science Direct, PubMed Medline, PubMed NIH, Springer Link, PsychNET, PsychINFO - OVID and PsycARTICLES, Wiley Online Library, Web of Science, ProQuest - MEDLINE (R), Sage Journals, NCBI - NLM catalog, Medline OVID, Medline EBSCO, Oxford Handbooks Online, PSYNDEX OVID, Google Scholar. Seventeen articles were included in the qualitative synthesis. Our results highlight that VR interventions, on a global scale, lead to an improvement in pain-related variables, particularly in reducing pain intensity. However, the analyzed articles vary significantly, making them challenging to compare. Future studies could focus on specific types of VR interventions to reduce heterogeneity and conduct a more specific analysis. In conclusion, VR interventions have demonstrated their validity and adaptability as a method for managing chronic pain. Nevertheless, further studies are needed to delve into the various categories of VR interventions in more detail.
   center dot VR interventions are intriguing for chronic pain management center dot Interventions include relaxation, meditation and activity promoting center dot VR interventions have been shown to reduce pain intensity in chronic pain center dot Interventions that have been used are very heterogenous
C1 [Giacomelli, L.; Solch, C. Martin; Ledermann, K.] Univ Fribourg, Dept Psychol, Chair Clin & Hlth Psychol, I Reach Lab,Rue Faucigny 2, CH-1700 Fribourg, Switzerland.
   [Ledermann, K.] Univ Hosp Zurich, Dept Consultat Liaison Psychiat & Psychosomat Med, Zurich, Switzerland.
C3 University of Fribourg; University of Zurich; University Zurich Hospital
RP Ledermann, K (corresponding author), Univ Fribourg, Dept Psychol, Chair Clin & Hlth Psychol, I Reach Lab,Rue Faucigny 2, CH-1700 Fribourg, Switzerland.; Ledermann, K (corresponding author), Univ Hosp Zurich, Dept Consultat Liaison Psychiat & Psychosomat Med, Zurich, Switzerland.
EM Katharina.ledermann@unifr.ch
FU University of Fribourg
FX There was no financial support for this study. Open access funding
   provided by University of Fribourg
CR Aivaliotis VI, 2020, Chronic Abdom Pain Gastroenterol, V158, DOI [10.1016/S0016-5085(20)31046-5.S-S-145, DOI 10.1016/S0016-5085(20)31046-5.S-S-145]
   [Anonymous], 2009, PLOS MED, V6, pe1000097, DOI DOI 10.1371/JOURNAL.PMED.1000097
   [Anonymous], 2022, OXFORD LEARNERS DICT
   Bacer F., 2020, Bosphorus Med J, V7, P75, DOI [10.14744/bmj.2020.92486, DOI 10.14744/BMJ.2020.92486]
   Bahat HS, 2018, EUR SPINE J, V27, P1309, DOI 10.1007/s00586-017-5323-0
   Bolte B, 2015, Technology, rehabilitation and empowerment of people with special needs, P47
   Brennan F, 2007, ANESTH ANALG, V105, P205, DOI 10.1213/01.ane.0000268145.52345.55
   Buckenmaier CC, 2013, PAIN MED, V14, P110, DOI 10.1111/j.1526-4637.2012.01516.x
   Chen KB, 2017, IEEE T NEUR SYS REH, V25, P1240, DOI 10.1109/TNSRE.2016.2621886
   Chidozie E., 2019, Hum Mov, V20, P66, DOI [10.5114/hm.2019.83998, DOI 10.5114/HM.2019.83998]
   Chuan A, 2021, ANAESTHESIA, V76, P695, DOI 10.1111/anae.15202
   Cleeland C. S., 1994, Annals Academy of Medicine Singapore, V23, P129
   Collins English Dictionary, 2022, Collins English Dictionary
   Collins SL, 1997, PAIN, V72, P95, DOI 10.1016/S0304-3959(97)00005-5
   Darnall BD, 2020, JMIR FORM RES, V4, DOI 10.2196/17293
   Gallagher S, 2000, TRENDS COGN SCI, V4, P14, DOI 10.1016/S1364-6613(99)01417-5
   Garcia LM, 2021, J MED INTERNET RES, V23, DOI 10.2196/26292
   Guarino D, 2017, ANN REV CYBERTHERAPY, V15, P181
   Hajihasani A, 2019, PM&R, V11, P167, DOI 10.1016/j.pmrj.2018.09.029
   Hoffman HG, 2006, J PAIN, V7, P843, DOI 10.1016/j.jpain.2006.04.006
   Hooten W.M., 2013, Assessment and management of chronic pain
   Igna R, 2014, J EVID-BASED PSYCHOT, V14, P229
   Karamnejad Salmani M, 2014, Virtual Reality and Health Informatics for Management of Chronic Pain
   Katzourin M, 2006, IEEE COMPUT GRAPH, V26, P15, DOI 10.1109/MCG.2006.137
   Loreto-Quijada D, 2014, CYBERPSYCH BEH SOC N, V17, P353, DOI 10.1089/cyber.2014.0057
   Mallari B, 2019, J PAIN RES, V12, P2053, DOI 10.2147/JPR.S200498
   Matheve T, 2020, J NEUROENG REHABIL, V17, DOI 10.1186/s12984-020-00688-0
   MELZACK R, 1975, PAIN, V1, P277, DOI 10.1016/0304-3959(75)90044-5
   Moher D, 2015, SYST REV-LONDON, V4, DOI [10.1186/2046-4053-4-1, 10.1371/journal.pmed.1000097, 10.1136/bmj.i4086, 10.1016/j.ijsu.2010.02.007, 10.1136/bmj.b2535, 10.1016/j.ijsu.2010.07.299, 10.1136/bmj.b2700]
   Tejera DM, 2020, INT J ENV RES PUB HE, V17, DOI 10.3390/ijerph17165950
   Nambi G, 2020, EVID-BASED COMPL ALT, V2020, DOI 10.1155/2020/2981273
   Nusser M, 2021, J REHABIL MED, V53, DOI 10.2340/16501977-2786
   Patel P, 2020, GAMES HEALTH J, V9, P129, DOI 10.1089/g4h.2019.0052
   Pozeg P, 2017, NEUROLOGY, V89, P1894, DOI 10.1212/WNL.0000000000004585
   Roettl J, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0200724
   Sarzi-Puttini P, 2012, CLIN DRUG INVEST, V32, P21, DOI 10.2165/11630050-000000000-00000
   Smith A, 2018, EUR J PAIN, V22, P94, DOI 10.1002/ejp.1093
   Solcà M, 2021, PAIN, V162, P1641, DOI 10.1097/j.pain.0000000000002160
   Sveinsdottir Vigdis, 2012, J Pain Res, V5, P371, DOI 10.2147/JPR.S25330
   Thomas JS, 2016, J PAIN, V17, P1302, DOI 10.1016/j.jpain.2016.08.011
   VONKORFF M, 1993, SPINE, V18, P855, DOI 10.1097/00007632-199306000-00008
   Wiederhold BK, 2014, CYBERPSYCH BEH SOC N, V17, P385, DOI 10.1089/cyber.2014.0202
NR 42
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD JUN 25
PY 2024
VL 28
IS 3
AR 126
DI 10.1007/s10055-024-00994-1
PG 13
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA WI1Y0
UT WOS:001254160300002
OA hybrid
DA 2024-08-05
ER

PT J
AU Lew, WH
   Coates, DR
AF Lew, Wei Hau
   Coates, Daniel R.
TI The effect of target and background texture on relative depth
   discrimination in a virtual environment
SO VIRTUAL REALITY
LA English
DT Article
DE Texture; Spatial frequency; Depth perception; Virtual environment;
   Figure-background
ID SPATIAL-FREQUENCY; SENSITIVITY FUNCTION; PERCEPTION; DISPARITY;
   STEREOPSIS; CONTRAST; DISTANCE; BLUR; CUE; SEGREGATION
AB The spatial frequency (SF) content of an object's texture is an important cue for depth perception, although less is known about the role of background texture. Here, we used bandpass-filtered noise patterns to systematically study the interactions between target and background textures in a virtual environment. During the trials, three square targets were presented at 3 m against a background wall 6 m away from the observer. One of the squares was presented closer than the other two, and the subjects had to indicate it with a key press. The threshold distance from the two reference tiles was determined using a staircase procedure. Both the target and background were tested with different combinations of SF textures and a non-textured gray, which were rendered onto the flat surfaces. Against a gray background, the distance thresholds were smallest when the targets were presented with a mid-SF texture. Performance declined significantly with a non-textured target against a textured background. With different combinations of target-background texture, the background texture significantly affected the performance. We propose several hypotheses to explain the behavioral result. Understanding the effect of surrounding texture can be useful in improving the depth perception experience in virtual reality.
C1 [Lew, Wei Hau; Coates, Daniel R.] Univ Houston, Coll Optometry, 4901 Calhoun Rd, Houston, TX 77004 USA.
C3 University of Houston System; University of Houston
RP Lew, WH (corresponding author), Univ Houston, Coll Optometry, 4901 Calhoun Rd, Houston, TX 77004 USA.
EM wlew@central.uh.edu
FU University of Houston Start-up Fund; University Houston Start-up Fund
   [2021]; University of Houston College of Optometry sVRSG; Vision
   Sciences Society
FX This work was supported by internal funding from University Houston
   Start-up Fund (to D.R.C.) and University of Houston College of Optometry
   sVRSG Research Grant 2021 (to W.H.L.) for the equipment used in this
   study. Portions of this work were presented at the 2020 and 2021
   meetings of the Vision Sciences Society (St. Petersburg, FL).
CR Allard R, 2008, BEHAV RES METHODS, V40, P735, DOI 10.3758/BRM.40.3.735
   Allard R, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.01707
   Andrews TJ, 2001, VISION RES, V41, P3051, DOI 10.1016/S0042-6989(01)00192-4
   Bex PJ, 2009, J VISION, V9, DOI 10.1167/9.10.1
   Bradshaw MF, 1999, VISION RES, V39, P3049, DOI 10.1016/S0042-6989(99)00015-2
   BROWN JM, 1988, PERCEPT PSYCHOPHYS, V44, P157, DOI 10.3758/BF03208708
   Bruder G, 2015, P IEEE VIRT REAL ANN, P27, DOI 10.1109/VR.2015.7223320
   Cammack P, 2016, PHILOS T R SOC B, V371, DOI 10.1098/rstb.2015.0258
   Caputo G, 1996, VISION RES, V36, P2815, DOI 10.1016/0042-6989(96)00045-4
   Chen CC, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0132658
   Creem-Regehr SH, 2016, SAP 2015: ACM SIGGRAPH SYMPOSIUM ON APPLIED PERCEPTION, P47, DOI 10.1145/2804408.2804422
   Creem-Regehr SH, 2023, PHILOS T R SOC B, V378, DOI 10.1098/rstb.2021.0456
   Diaz C, 2017, PROCEEDINGS OF THE 2017 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P111, DOI 10.1109/ISMAR.2017.28
   Ding J, 2011, P NATL ACAD SCI USA, V108, pE733, DOI 10.1073/pnas.1105183108
   El Jamiy F, 2019, IET IMAGE PROCESS, V13, P707, DOI 10.1049/iet-ipr.2018.5920
   Ellis S., 1991, Pictorial Communication In Real And Virtual Environments, V1st, DOI [10.1201/9781482295177, DOI 10.1201/9781482295177]
   Filippini HR, 2009, J VISION, V9, DOI 10.1167/9.1.8
   FIORENTINI A, 1971, VISION RES, V11, P1299, DOI 10.1016/0042-6989(71)90012-5
   FRISBY JP, 1978, PERCEPTION, V7, P423, DOI 10.1068/p070423
   Gardner JS, 2010, ATTEN PERCEPT PSYCHO, V72, P445, DOI 10.3758/APP.72.2.445
   Held RT, 2012, CURR BIOL, V22, P426, DOI 10.1016/j.cub.2012.01.033
   Hillis JM, 2004, J VISION, V4, P967, DOI 10.1167/4.12.1
   Hornsey RL, 2021, VIRTUAL REAL-LONDON, V25, P1087, DOI 10.1007/s10055-021-00500-x
   Howard H J, 1919, Trans Am Ophthalmol Soc, V17, P195
   JOHNSTON EB, 1993, VISION RES, V33, P813, DOI 10.1016/0042-6989(93)90200-G
   Julesz B, 1962, IRE Transactions on Information Theory, V8, P84, DOI [DOI 10.1109/TIT.1962.1057698, 10.1109/TIT.1962.1057698]
   Julesz Bela, 1971, FDN CYCLOPEAN PERCEP
   KIDD AL, 1979, NATURE, V280, P829, DOI 10.1038/280829a0
   Kline PB, 1996, PROCEEDINGS OF THE HUMAN FACTORS AND ERGONOMICS SOCIETY - 40TH ANNUAL MEETING, VOLS 1 AND 2, P1112
   Kunz BR, 2015, PERCEPTION, V44, P446, DOI 10.1068/p7929
   Langer MS, 2000, PERCEPTION, V29, P649, DOI 10.1068/p3060
   Li S., 2007, J vis, V7, P826, DOI [10.1167/7.9.826, DOI 10.1167/7.9.826]
   Lucaci A, 2022, Influence of texture fidelity on spatial perception in virtual reality, P244, DOI [10.5220/0010890100003124, DOI 10.5220/0010890100003124]
   Marshall JA, 1996, J OPT SOC AM A, V13, P681, DOI 10.1364/JOSAA.13.000681
   Mather G, 1997, PERCEPTION, V26, P1147, DOI 10.1068/p261147
   MITCHISON GJ, 1984, VISION RES, V24, P1063, DOI 10.1016/0042-6989(84)90084-1
   Neider MB, 2006, VISION RES, V46, P2217, DOI 10.1016/j.visres.2006.01.006
   NOTHDURFT HC, 1992, PERCEPT PSYCHOPHYS, V52, P355, DOI 10.3758/BF03206697
   Oliphant TE, 2007, COMPUT SCI ENG, V9, P10, DOI 10.1109/MCSE.2007.58
   Oluk C, 2022, J VISION, V22, DOI 10.1167/jov.22.5.6
   OSHEA RP, 1994, VISION RES, V34, P1595, DOI 10.1016/0042-6989(94)90116-3
   PELI E, 1990, J OPT SOC AM A, V7, P2032, DOI 10.1364/JOSAA.7.002032
   Peterzell DH, 2017, VISION RES, V141, P127, DOI 10.1016/j.visres.2017.11.002
   Renner RS, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543590
   Reynaud A, 2017, FRONT COMPUT NEUROSC, V11, DOI 10.3389/fncom.2017.00063
   Reynaud A, 2015, VISION RES, V113, P97, DOI 10.1016/j.visres.2015.04.021
   ROBSON JG, 1966, J OPT SOC AM, V56, P1141, DOI 10.1364/JOSA.56.001141
   ROHALY AM, 1994, VISION RES, V34, P1315, DOI 10.1016/0042-6989(94)90205-4
   Saarela TP, 2012, VISION RES, V58, P59, DOI 10.1016/j.visres.2012.01.019
   Salmela VR, 2007, VISION RES, V47, P452, DOI 10.1016/j.visres.2006.11.016
   Sawayama M, 2015, VISION RES, V109, P209, DOI 10.1016/j.visres.2014.11.017
   Scaccia M, 2018, J VISION, V18, DOI 10.1167/18.3.5
   Scarfe P, 2021, J VISION, V21, DOI 10.1167/jov.21.4.10
   SCHOR CM, 1983, VISION RES, V23, P1649, DOI 10.1016/0042-6989(83)90179-7
   Serrano-Pedraza I, 2010, J VISION, V10, DOI 10.1167/10.12.10
   SIDEROV J, 1993, VISION RES, V33, P1545, DOI 10.1016/0042-6989(93)90147-O
   Song L, 2010, 2010 INT C MULTIMEDI, P1, DOI [10.1109/ICMULT.2010.5631434, DOI 10.1109/ICMULT.2010.5631434]
   STEVENS KA, 1981, BIOL CYBERN, V42, P95, DOI 10.1007/BF00336727
   SUTTER A, 1989, PERCEPT PSYCHOPHYS, V46, P312, DOI 10.3758/BF03204985
   Tozawa J, 2012, PERCEPTION, V41, P774, DOI 10.1068/p7188
   Troscianko T, 2009, PHILOS T R SOC B, V364, P449, DOI 10.1098/rstb.2008.0218
   Tsutsui KI, 2002, SCIENCE, V298, P409, DOI 10.1126/science.1074128
   TYLER CW, 1973, SCIENCE, V181, P276, DOI 10.1126/science.181.4096.276
   Watson AB, 2005, J VISION, V5, P717, DOI 10.1167/5.9.6
   Westheimer G, 2001, PERCEPTION, V30, P531, DOI 10.1068/p3193
   YOUNG MJ, 1993, VISION RES, V33, P2685, DOI 10.1016/0042-6989(93)90228-O
NR 66
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD APR 25
PY 2024
VL 28
IS 2
AR 103
DI 10.1007/s10055-024-01000-4
PG 13
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA OU2Q9
UT WOS:001209734100001
OA hybrid
DA 2024-08-05
ER

PT J
AU Solmaz, S
   Gerling, K
   Kester, L
   Van Gerven, T
AF Solmaz, Serkan
   Gerling, Kathrin
   Kester, Liesbeth
   Van Gerven, Tom
TI Behavioral intention, perception and user assessment in an immersive
   virtual reality environment with CFD simulations
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Computational fluid dynamics; Immersive learning;
   Technology acceptance; Assessment
ID INFORMATION-TECHNOLOGY; ENGINEERING EDUCATION; FLOW EXPERIENCE;
   ACCEPTANCE; ADOPTION; VISUALIZATION; PERSPECTIVES; UTAUT
AB This study explores technology acceptance, perception and user assessment of an immersive virtual reality environment with computational fluid dynamics simulations in engineering education. 57 participants from three different institutions tested the virtual reality application. Partial least squares structural equation modeling and interferential statistics were performed to predict and assess interrelations among constructs. Results show that the learning value, content value, intrinsic motivation and personal innovativeness are underlying factors behind students' intention to use virtual reality. Pair-wise analysis indicates that users' perceptions matter and positively affect their attitudes. In addition, the virtual reality application helps students perform significantly better in the post-knowledge test. Findings also highlight that prior experience and interest can affect students' attitudes and behavioral intentions to accept the virtual reality application in education. Our study can guide lecturers and developers to achieve on-target immersive virtual reality learning environments in higher education.
C1 [Solmaz, Serkan; Van Gerven, Tom] Katholieke Univ Leuven, Dept Chem Engn, Leuven, Belgium.
   [Gerling, Kathrin] Katholieke Univ Leuven, eMedia Res Lab, Leuven, Belgium.
   [Kester, Liesbeth] Univ Utrecht, Dept Educ, Utrecht, Netherlands.
C3 KU Leuven; KU Leuven; Utrecht University
RP Van Gerven, T (corresponding author), Katholieke Univ Leuven, Dept Chem Engn, Leuven, Belgium.
EM tom.vangerven@kuleuven.be
RI Van Gerven, Tom/B-5806-2015
OI Van Gerven, Tom/0000-0003-2051-5696
FU European Union [812716]
FX This project has received funding from the European Union's EU Framework
   Programme for Research and Innovation Horizon 2020 under Grant Agreement
   812716. This publication reflects only the authors' view exempting the
   community from any liability. Project website: https:// charming- etn.
   eu/.
CR Ain N, 2016, INFORM DEV, V32, P1306, DOI 10.1177/0266666915597546
   Almaiah MA, 2019, IEEE ACCESS, V7, P174673, DOI 10.1109/ACCESS.2019.2957206
   Alwahaishi Saleh, 2013, Journal of Technology Management & Innovation, V8, P61
   Anderson L. W., 2001, A Taxonomy for Learning, Teaching, and Assessing: A Revision of Bloom's Taxonomy of Educational Objectives
   [Anonymous], 2021, Building the future of education
   Barrett AJ, 2021, COMPUT EDUC, V169, DOI 10.1016/j.compedu.2021.104214
   Bawack RE, 2018, INT J MED INFORM, V109, P15, DOI 10.1016/j.ijmedinf.2017.10.016
   Bölen MC, 2021, INT J CONSUM STUD, V45, P546, DOI 10.1111/ijcs.12641
   Chao CM, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.01652
   Chen JC, 2020, J COMPUT ASSIST LEAR, V36, P46, DOI 10.1111/jcal.12389
   Chen M, 2020 INT S ED TECHNO, P283
   Chen Mengting, 2021, SN Comput Sci, V2, P114, DOI 10.1007/s42979-021-00498-8
   Chin PY, 2020, INFORM SYST FRONT, V22, P1357, DOI 10.1007/s10796-019-09939-5
   Christmann O, 2022, INFORM VISUAL, V21, P311, DOI 10.1177/14738716221085965
   Coban M, 2022, EDUC RES REV-NETH, V36, DOI 10.1016/j.edurev.2022.100452
   Dajani D, 2019, HELIYON, V5, DOI 10.1016/j.heliyon.2019.e02536
   DAVIS FD, 1989, MIS QUART, V13, P319, DOI 10.2307/249008
   Deng SJ, 2021, J IMAGING, V7, DOI 10.3390/jimaging7080151
   di Lanzo JA, 2020, COMPUT APPL ENG EDUC, V28, P748, DOI 10.1002/cae.22243
   Farooq MS, 2017, INTERACT TECHNOL SMA, V14, P329, DOI 10.1108/ITSE-06-2016-0015
   Gan VJL, 2022, BUILDINGS-BASEL, V12, DOI 10.3390/buildings12030333
   Gattullo M, 2022, COMPUTERS, V11, DOI 10.3390/computers11040050
   Hair J. F., 2009, Multivariate data analysis
   Handoko BL, 2019, ICEME 2019: 019 10TH INTERNATIONAL CONFERENCE ON E-BUSINESS, MANAGEMENT AND ECONOMICS, P259, DOI 10.1145/3345035.3345047
   Hatchard T, 2019, IEEE INTL CONF IND I, P1145, DOI [10.1109/INDIN41052.2019.8972023, 10.1109/indin41052.2019.8972023]
   Huang W, 2021, COMPUT APPL ENG EDUC, V29, P1420, DOI 10.1002/cae.22393
   Indrawati, 2016, 2016 IEEE ASIA PACIFIC CONFERENCE ON WIRELESS AND MOBILE (APWIMOB), P52, DOI 10.1109/APWiMob.2016.7811434
   Islami SB, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11178082
   Joa CY, 2022, BEHAV INFORM TECHNOL, V41, P1620, DOI 10.1080/0144929X.2021.1892192
   Khechine H, 2020, BRIT J EDUC TECHNOL, V51, P2306, DOI 10.1111/bjet.12905
   Kim RW, 2021, BIOSYST ENG, V207, P12, DOI 10.1016/j.biosystemseng.2021.02.018
   Klingenberg S, 2020, BRIT J EDUC TECHNOL, V51, P2115, DOI 10.1111/bjet.13029
   Kumar VV, 2021, EDUC CHEM ENG, V36, P143, DOI 10.1016/j.ece.2021.05.002
   Kunz RE, 2020, SPORT BUS MANAG, V10, P83, DOI 10.1108/SBM-11-2018-0095
   Lee SY, 2022, COMPUT ELECTRON AGR, V194, DOI 10.1016/j.compag.2022.106709
   Li H, 2017, INFORM MANAGE-AMSTER, V54, P1012, DOI 10.1016/j.im.2017.02.005
   Li Wenkai., 2017, Multim. Technol. Interac., V1, P17, DOI DOI 10.3390/MTI1030017
   Lin JR, 2019, AUTOMAT CONSTR, V103, P26, DOI 10.1016/j.autcon.2019.02.007
   Lorusso P, 2022, BUILDINGS-BASEL, V12, DOI 10.3390/buildings12020223
   Ma Y., 2021, J Theor Appl Electron Commer Res, V16, P194, DOI [10.4067/S0718-18762021000200113, DOI 10.4067/S0718-18762021000200113]
   Makransky G, 2019, LEARN INSTR, V60, P225, DOI 10.1016/j.learninstruc.2017.12.007
   Palmisano S, 2022, VIRTUAL REAL-LONDON, V26, P1373, DOI 10.1007/s10055-022-00634-6
   Plotzky Christian, 2021, i-com: Journal of Interactive Media, V20, P73, DOI 10.1515/icom-2021-0008
   Rauschnabel PA, 2022, COMPUT HUM BEHAV, V133, DOI 10.1016/j.chb.2022.107289
   Shi H, 2020, J COMPUT SCI-NETH, V43, DOI 10.1016/j.jocs.2020.101091
   Shin D, 2019, COMPUT HUM BEHAV, V98, P302, DOI 10.1016/j.chb.2018.11.010
   Silvestri L, 2021, PROCEDIA COMPUT SCI, V180, P381, DOI 10.1016/j.procs.2021.01.359
   Sodervik I, 2021, Educ Sci, V15
   Soliman M, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11062879
   Solmaz S, Unveiling the Virtual Garage concept, V34
   Solmaz S, 2022, COMPUT CHEM ENG, V156, DOI 10.1016/j.compchemeng.2021.107570
   Solmaz S, 2022, MULTIMED TOOLS APPL, V81, P14869, DOI 10.1007/s11042-021-10621-9
   Sung B, 2022, ASIA PAC J MARKET LO, V34, P1482, DOI 10.1108/APJML-12-2020-0897
   Tamilmani K, 2021, INT J INFORM MANAGE, V57, DOI 10.1016/j.ijinfomgt.2020.102269
   Udeozor C, 2022, J EDUC COMPUT RES, V60, P322, DOI 10.1177/07356331211036989
   Udeozor C, 2021, HIGH EDUC PEDAGOG, V6, P175, DOI 10.1080/23752696.2021.1951615
   Veksler AE, 2017, WESTERN J COMM, V81, P641, DOI 10.1080/10570314.2017.1309452
   Venkatesh V, 2003, MIS QUART, V27, P425, DOI 10.2307/30036540
   Venkatesh V, 2012, MIS QUART, V36, P157
   Wang C, 2022, COMPUT APPL ENG EDUC, V30, P335, DOI 10.1002/cae.22458
   Wigfield A, 2000, CONTEMP EDUC PSYCHOL, V25, P68, DOI 10.1006/ceps.1999.1015
   Yan JY, 2020, BUILDINGS-BASEL, V10, DOI 10.3390/buildings10120229
   Zhou T, 2013, PERS UBIQUIT COMPUT, V17, P741, DOI 10.1007/s00779-012-0613-3
   Zhou T, 2013, BEHAV INFORM TECHNOL, V32, P263, DOI 10.1080/0144929X.2011.650711
   Zhou T, 2011, INFORM DEV, V27, P207, DOI 10.1177/0266666911414596
   Zhou T, 2010, IND MANAGE DATA SYST, V110, P930, DOI 10.1108/02635571011055126
NR 66
TC 0
Z9 0
U1 7
U2 7
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR 29
PY 2024
VL 28
IS 2
AR 88
DI 10.1007/s10055-024-00985-2
PG 20
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA MQ5T3
UT WOS:001195113700001
OA hybrid
DA 2024-08-05
ER

PT J
AU Covolan, JPM
   Oliveira, C
   Sanches, SRR
   Sementille, AC
AF Covolan, Joao Pedro Mucheroni
   Oliveira, Claiton
   Sanches, Silvio Ricardo Rodrigues
   Sementille, Antonio Carlos
TI Non-deterministic method for semi-automatic calibration of
   smartphone-based OST HMDs
SO VIRTUAL REALITY
LA English
DT Article
DE Head mounted displays; Optical see-through; Calibration;
   Non-deterministic optimization; Smartphone-based AR
AB An Augmented Reality (AR) system must show real and virtual elements as if they coexisted in the same environment. The tridimensional aligment (registration) is particularly challenging on specific hardware configurations such as Head Mounted Displays (HMDs) that use Optical See-Through (OST) technology. In general, the calibration of HMDs uses deterministic optimization methods. However, non-deterministic methods have been proposed in the literature with promising results in distinct research areas. In this work, we developed a non-deterministic optimization method for the semi-automatic calibration of smartphone-based OST HMDs. We tested simulated annealing, evolutionary strategy, and particle swarm algorithms. We also developed a system for calibration and evaluated it through an application that aligned a virtual object in an AR environment. We evaluated our method using the Mean Squared Error (MSE) at each calibration step, considering the difference between the ideal/observed positions of a set of reference points and those estimated from the values determined for the calibration parameters. Our results show an accurate OST HMD calibration for the peripersonal space, with similar MSEs for the three tested algorithms.
C1 [Covolan, Joao Pedro Mucheroni; Sementille, Antonio Carlos] Univ Estadual Paulista, Bauru, SP, Brazil.
   [Oliveira, Claiton; Sanches, Silvio Ricardo Rodrigues] Univ Tecnol Fed Parana, Cornelio Procopio, PR, Brazil.
C3 Universidade Estadual Paulista; Universidade Tecnologica Federal do
   Parana
RP Sanches, SRR (corresponding author), Univ Tecnol Fed Parana, Cornelio Procopio, PR, Brazil.
EM jotamucheroni@gmail.com; claitonoliveira@utfpr.edu.br;
   silviosanches@utfpr.edu.br; antonio.sementille@unesp.br
FU Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior-Brasil
   (CAPES) [001]
FX This study was financed in part by the Coordenacao de Aperfeicoamento de
   Pessoal de Nivel Superior-Brasil (CAPES)-Finance Code 001.
CR Azuma RT, 1997, PRESENCE-VIRTUAL AUG, V6, P355, DOI 10.1162/pres.1997.6.4.355
   Billinghurst Mark, 2015, Foundations and Trends in Human-Computer Interaction, V8, P73, DOI 10.1561/1100000049
   Combe T, 2023, VIRTUAL REAL-LONDON, V27, P2003, DOI 10.1007/s10055-023-00787-y
   Cutolo F, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10010193
   De Castro LN., 2006, FUNDAMENTALS NATURAL, DOI [DOI 10.1201/9781420011449, 10.1201/9781420011449]
   Figl M, 2005, IEEE T MED IMAGING, V24, P1492, DOI 10.1109/TMI.2005.856746
   Genc Y, 2002, INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P169, DOI 10.1109/ISMAR.2002.1115086
   Gilson SJ, 2008, J NEUROSCI METH, V173, P140, DOI 10.1016/j.jneumeth.2008.05.015
   Grubert J, 2018, IEEE T VIS COMPUT GR, V24, P2649, DOI 10.1109/TVCG.2017.2754257
   Hu X, 2020, IEEE ACCESS, V8, P223661, DOI 10.1109/ACCESS.2020.3044184
   Itoh Y, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3453157
   Itoh Y, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P75, DOI 10.1109/3DUI.2014.6798846
   Kellner F, 2012, IEEE T VIS COMPUT GR, V18, P589, DOI 10.1109/TVCG.2012.45
   Kiyokawa K, 2015, Fundamentals of wearable computers and augmented reality. Chap head-mounted display technologies for augmented reality, P59
   Langlotz T, 2014, P IEEE, V102, P155, DOI 10.1109/JPROC.2013.2294255
   Luo G, 2005, OPT ENG, V44, DOI 10.1117/1.1839231
   Makibuchi N, 2013, IEEE IMAGE PROC, P2177, DOI 10.1109/ICIP.2013.6738449
   Monica R, 2022, VIRTUAL REAL-LONDON, V26, P1335, DOI 10.1007/s10055-022-00637-3
   Navab N, 2004, P IEEE VIRT REAL ANN, P245, DOI 10.1109/VR.2004.1310091
   North Star, 2019, Project north star
   Owen CB, 2004, ISMAR 2004: THIRD IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P70, DOI 10.1109/ISMAR.2004.28
   Plopski A, 2015, IEEE T VIS COMPUT GR, V21, P481, DOI 10.1109/TVCG.2015.2391857
   Sexton A, 2019, IEEE SOUTHEASTCON, DOI 10.1109/southeastcon42311.2019.9020625
   Tuceryan M, 2000, IEEE AND ACM INTERNATIONAL SYMPOSIUM ON AUGMENTED REALITY, PROCEEDING, P149, DOI 10.1109/ISAR.2000.880938
   Zhang ZL, 2017, P IEEE VIRT REAL ANN, P245, DOI 10.1109/VR.2017.7892268
NR 25
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR 15
PY 2024
VL 28
IS 2
AR 77
DI 10.1007/s10055-024-00978-1
PG 14
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA LG7B7
UT WOS:001185687800002
OA hybrid
DA 2024-08-05
ER

PT J
AU Lee, D
   Won, S
   Kim, J
   Kwon, HY
AF Lee, Dohui
   Won, Sohyun
   Kim, Jiwon
   Kwon, Hyuk-Yoon
TI ARGo: augmented reality-based mobile Go stone collision game
SO VIRTUAL REALITY
LA English
DT Article
DE Augmented reality; Mobile games; Automata theory-based game design;
   Collision effects; Customization; Motion recognition
AB In this study, we present a mobile Go stone collision game based on augmented reality, which we call ARGo, inspired by the traditional Korean board game, Alkkagi. ARGo aims to resolve two main issues: (1) the portability and space constraints of the original Alkkagi and (2) the limited sense of reality due to the touchscreen-based interface of the existing mobile Alkkagi games. To improve a sense of the reality of the game, ARGo provides a gameplay interface similar to the original Alkkagi by recognizing the user's hand motion based on AR. Additionally, it provides a customization mechanism for each user to improve the recognition of the hand motion and the strength of the attack considering each user's characteristics. Finally, we make the following three main contributions. First, we employ the automata theory to design the game and collision scenarios between stones. Consequently, we can clearly define the complicated states incurred by AR-based motion recognition and collisions between virtual objects. Second, we propose a collision equation based on Continuous Collision Detection tailored to ARGo, i.e., Go stones and their collisions. Through experimental studies, we demonstrate that the collision equation enables the simulation of the exact collision effects. Third, through user experience studies, we verify the effectiveness of ARGo by showing the effects of the functions implemented in ARGo and its superiority over the existing mobile game Alkkagi Mania.
C1 [Lee, Dohui; Won, Sohyun; Kim, Jiwon; Kwon, Hyuk-Yoon] Seoul Natl Univ Sci & Technol, Dept Ind Engn, 232 Gongneung Ro, Seoul 01811, South Korea.
C3 Seoul National University of Science & Technology
RP Kwon, HY (corresponding author), Seoul Natl Univ Sci & Technol, Dept Ind Engn, 232 Gongneung Ro, Seoul 01811, South Korea.
EM hyukyoon.kwon@seoultech.ac.kr
OI Kwon, Hyuk-Yoon/0000-0002-1125-6533
FU National Research Foundation of Korea(NRF) [2022R1F1A1067008]; National
   Research Foundation of Korea(NRF) - Korea government(MSIT)
   [2019R1A6A1A03032119]; Basic Science Research Program through the
   National Research Foundation of Korea(NRF) - Ministry of Education
FX This work was supported by the National Research Foundation of
   Korea(NRF) grant funded by the Korea government(MSIT) (No.
   2022R1F1A1067008) and by the Basic Science Research Program through the
   National Research Foundation of Korea(NRF) funded by the Ministry of
   Education (No. 2019R1A6A1A03032119).
CR A+E Networks, 2018, Knightfall ar
   Ali KF, 2019, 2019 INNOVATIONS IN POWER AND ADVANCED COMPUTING TECHNOLOGIES (I-PACT), DOI 10.1109/i-pact44901.2019.8959998
   Allan R., 2016, Pokemon GO usage statistics say it's the most popular mobile game in US history
   Andersen Troels L., 2004, Proceedings of the International Conference on Interaction Design and Children, P137, DOI DOI 10.1145/1017833.1017858
   Baroroh DK, 2021, J MANUF SYST, V61, P696, DOI 10.1016/j.jmsy.2020.10.017
   Ding HY, 2019, IEEE T HAPTICS, V12, P624, DOI 10.1109/TOH.2019.2934104
   Grand-View-Research, 2022, Augmented reality market size, share and trends analysis report by component (hardware, software), by display (head-mounted display and smart glass, head-up display, handheld devices), by application, by region, and segment forecasts, 2022-2030
   Hopcroft J. E., 2001, Introduction to Automata theory, Languages, and Computation, V32, P60, DOI DOI 10.1145/568438.568455
   Jamil A., 2016, Int J Comput Sci Softw Eng, V5, P119
   Ketchapp, 2016, Stack ar
   Khademi M., 2014, CHI'14 Extended Abstracts on Human Factors in Computing Systems, P1663, DOI DOI 10.1145/2559206.2581203
   Kim H, 2014, ENTERTAIN COMPUT, V5, P233, DOI 10.1016/j.entcom.2014.10.008
   Lugaresi C, 2019, Arxiv, DOI arXiv:1906.08172
   Lv ZH, 2015, PERS UBIQUIT COMPUT, V19, P551, DOI 10.1007/s00779-015-0844-1
   MASC, 2015, Balanced tower ar
   Matlani Roshnee, 2021, 2021 INT C TECHN ADV, P340
   Mikesch Hansjoerg, 2019, Forfour ar
   Mobirixsub, 2013, Alkkagi mania
   Nilsen T, 2005, 2 INT WORKSHOP PERVA
   Oberlo, 2022, How many people have smartphones in 2022?
   OnePlus, 2020, Oneplus nord, the first ar smartphone
   Qureshi NS., 2012, Int J Multidiscip Sci Eng, V3, P13
   Rayar F., 2015, P 2015 INT C INTERAC, P229, DOI [10.1145/2817721.2823485, DOI 10.1145/2817721.2823485]
   Reallusion, 2020, Leap motion controller: The essential handtracking device
   Redon S, 2004, P IEEE VIRT REAL ANN, P117, DOI 10.1109/VR.2004.1310064
   Redon S, 2002, COMPUT GRAPH FORUM, V21, P279, DOI 10.1111/1467-8659.t01-1-00587
   Salas J, 2021, IEEE T GAMES, V13, P216, DOI 10.1109/TG.2021.3068426
   Samsung, 2019, Samsung electronics to showcase successful 'c-lab inside' projects and 'c-lab outside' start-ups at CES 2020
   SCHUIRMANN DJ, 1987, J PHARMACOKINET BIOP, V15, P657, DOI 10.1007/BF01068419
   SHAPIRO SS, 1965, BIOMETRIKA, V52, P591, DOI 10.2307/2333709
   TiltFive, 2021, Tilt five: Tabletop ar
   Viglialoro RM, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11052338
   Xu WG, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300674
   Yu JQ, 2022, ETR&D-EDUC TECH RES, V70, P1169, DOI 10.1007/s11423-022-10122-y
   Zhang F, 2020, Arxiv, DOI [arXiv:2006.10214, DOI 10.48550/ARXIV.2006.10214]
   Zhang XY, 2006, VISUAL COMPUT, V22, P749, DOI 10.1007/s00371-006-0060-0
NR 36
TC 0
Z9 0
U1 9
U2 9
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 37
DI 10.1007/s10055-023-00919-4
PG 21
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA GN0W2
UT WOS:001153240100001
OA hybrid
DA 2024-08-05
ER

PT J
AU Sterna, R
   Szczugiel, J
   Pilarczyk, J
   Siry, A
   Kuniecki, M
AF Sterna, Radoslaw
   Szczugiel, Jakub
   Pilarczyk, Joanna
   Siry, Agnieszka
   Kuniecki, Michal
TI Like a human: The social facilitation/inhibition effect in presence of a
   virtual observer depends on arousal
SO VIRTUAL REALITY
LA English
DT Article
DE Social facilitation; VR; Social inhibition; Virtual character; EDA
ID VISUAL-SEARCH; PERFORMANCE; INHIBITION; DOMINANT
AB Better (social facilitation) or worse (social inhibition) performance in a task while being observed has repeatedly been demonstrated with real human observers, yet it has not been fully tested with virtual observers. We tested this effect in a virtual environment using a visual search task to verify if the mere presence of a virtual observer would boost participants' performance in the easy variant of the task and hinder it in the difficult one. We used electrodermal (EDA) activity measurement to test whether physiological arousal would moderate this effect. The presence of a virtual character affected search performance, which was moderated by arousal, with lower arousal leading to better performance. These results confirm that the mere presence of virtual characters can evoke a social facilitation/inhibition effect similar to that of human beings; this could confirm Zajonc's theory and serve as encouragement for further development of VR-based training & educational tools with virtual agents.
C1 [Sterna, Radoslaw; Pilarczyk, Joanna; Kuniecki, Michal] Jagiellonian Univ Krakow, Inst Psychol, Fac Philosophy, Emot & Percept Lab, Romana Ingardena 6, Lesser Poland Voivodeship, PL-30060 Krakow, Poland.
   [Sterna, Radoslaw; Siry, Agnieszka] Jagiellonian Univ Krakow, Doctoral Sch Social Sci, Krakow, Poland.
   [Szczugiel, Jakub] Jagiellonian Univ Krakow, Fac Philosophy, Inst Psychol, Krakow, Poland.
C3 Jagiellonian University; Jagiellonian University; Jagiellonian
   University
RP Sterna, R (corresponding author), Jagiellonian Univ Krakow, Inst Psychol, Fac Philosophy, Emot & Percept Lab, Romana Ingardena 6, Lesser Poland Voivodeship, PL-30060 Krakow, Poland.; Sterna, R (corresponding author), Jagiellonian Univ Krakow, Doctoral Sch Social Sci, Krakow, Poland.
EM radoslawsterna@gmail.com
RI Siry, Agnieszka/KLZ-8956-2024
OI Siry, Agnieszka/0000-0002-6826-1229
FU Ministerstwo Edukacji i Nauki
FX No Statement Available
CR Aiello JR, 2001, GROUP DYN-THEOR RES, V5, P163, DOI 10.1037//1089-2699.5.3.163
   Baldwin N., 2015, Proc Hum Factors Ergon Soc Annual Meeting, V59, P90, DOI [10.1177/1541931215591019, DOI 10.1177/1541931215591019]
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Ben-Shachar MS., 2020, J OPEN SOURCE SOFTW, V5, P2815, DOI [10.21105/joss.02815, DOI 10.21105/JOSS.02815]
   Benedek M, 2010, J NEUROSCI METH, V190, P80, DOI 10.1016/j.jneumeth.2010.04.028
   Blascovich J, 2002, PSYCHOL INQ, V13, P103, DOI 10.1207/S15327965PLI1302_01
   Blascovich J, 1999, J PERS SOC PSYCHOL, V77, P68, DOI 10.1037/0022-3514.77.1.68
   Blascovich J., 2002, Proceedings of the 5th Annual International Workshop on PRESENCE, P392
   BOND CF, 1983, PSYCHOL BULL, V94, P265, DOI 10.1037/0033-2909.94.2.265
   COTTRELL NB, 1968, J PERS SOC PSYCHOL, V9, P245, DOI 10.1037/h0025902
   de Leeuw JR, 2015, BEHAV RES METHODS, V47, P1, DOI 10.3758/s13428-014-0458-y
   Duzmanska N, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.02132
   Emmerich K, 2016, CHI PLAY 2016: PROCEEDINGS OF THE 2016 ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P10, DOI 10.1145/2967934.2968092
   Far IK, 2015, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.30
   GEEN RG, 1983, MOTIV EMOTION, V7, P203, DOI 10.1007/BF00992903
   Hall B, 2008, COMPUT HUM BEHAV, V24, P2965, DOI 10.1016/j.chb.2008.05.003
   HAUTUS MJ, 1995, BEHAV RES METH INS C, V27, P46, DOI 10.3758/BF03203619
   Hayes AL, 2010, LECT NOTES ARTIF INT, V6356, P454, DOI 10.1007/978-3-642-15892-6_49
   Howells FM, 2010, BEHAV BRAIN FUNCT, V6, DOI 10.1186/1744-9081-6-39
   Hoyt CL, 2003, PRESENCE-TELEOP VIRT, V12, P183, DOI 10.1162/105474603321640932
   Kim LH, 2022, IEEE ROBOT AUTOM LET, V7, P7399, DOI 10.1109/LRA.2022.3181726
   Ku J, 2005, CYBERPSYCHOL BEHAV, V8, P493, DOI 10.1089/cpb.2005.8.493
   Kuznetsova A, 2017, J STAT SOFTW, V82, P1, DOI 10.18637/jss.v082.i13
   Lim S, 2009, CYBERPSYCHOL BEHAV, V12, P59, DOI 10.1089/cpb.2008.0054
   Liu N, 2018, HUM FACTOR ERGON MAN, V28, P260, DOI 10.1002/hfm.20743
   Llobera J, 2010, ACM T APPL PERCEPT, V8, DOI 10.1145/1857893.1857896
   Miller MR, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0216290
   Mojzisch A, 2006, SOC NEUROSCI-UK, V1, P184, DOI 10.1080/17470910600985621
   Mostajeran F, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P320, DOI 10.1109/VRW55335.2022.00074
   Obaid Mohammad, 2012, Intelligent Virtual Agents. Proceedings 12th International Conference, IVA 2012, P412, DOI 10.1007/978-3-642-33197-8_42
   Pan XN, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00790
   Park S, 2007, HUM FACTORS, V49, P1054, DOI 10.1518/001872007X249910
   Sterna R., 2019, Social Psychol Bull, V14, P1, DOI DOI 10.32872/SPB.V14I3.30091
   Sterna R, 2023, INT J HUM-COMPUT INT, DOI 10.1080/10447318.2023.2285641
   Sterna R, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P123, DOI 10.1109/VRW52623.2021.00030
   Strojny PM, 2020, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.01252
   Triplett N., 1898, American Journal of Psychology, V9, P507
   van Meurs E, 2022, INT REV SPORT EXER P, DOI 10.1080/1750984X.2022.2111663
   WEISS RF, 1971, PSYCHOL REV, V78, P44, DOI 10.1037/h0030386
   Xu Z, 2021, J EXP PSYCHOL HUMAN, V47, P1274, DOI 10.1037/xhp0000941
   ZAJONC RB, 1965, SCIENCE, V149, P269, DOI 10.1126/science.149.3681.269
   ZAJONC RB, 1966, J EXP SOC PSYCHOL, V2, P160, DOI 10.1016/0022-1031(66)90077-1
   Zanbaka C, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1561
NR 43
TC 1
Z9 1
U1 4
U2 4
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 55
DI 10.1007/s10055-024-00972-7
PG 14
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA JD0T4
UT WOS:001171108800001
OA hybrid
DA 2024-08-05
ER

PT J
AU Buwaider, A
   El-Hajj, VG
   Iop, A
   Romero, M
   Jean, W
   Edström, E
   Elmi-Terander, A
AF Buwaider, Ali
   El-Hajj, Victor Gabriel
   Iop, Alessandro
   Romero, Mario
   Jean, Walter
   Edstrom, Erik
   Elmi-Terander, Adrian
TI Augmented reality navigation in external ventricular drain insertion-a
   systematic review and meta-analysis
SO VIRTUAL REALITY
LA English
DT Article
DE External ventricular drain; Augmented reality; Ventriculostomy;
   Systematic review; Accuracy; Neuronavigation
ID ACCURACY; NEUROSURGERY; NEURONAVIGATION; PLACEMENT; GUIDANCE; SAFETY
AB External ventricular drain (EVD) insertion using the freehand technique is often associated with misplacements resulting in unfavorable outcomes. Augmented Reality (AR) has been increasingly used to complement conventional neuronavigation. The accuracy of AR guided EVD insertion has been investigated in several studies, on anthropomorphic phantoms, cadavers, and patients. This review aimed to assess the current knowledge and discuss potential benefits and challenges associated with AR guidance in EVD insertion. MEDLINE, EMBASE, and Web of Science were searched from inception to August 2023 for studies evaluating the accuracy of AR guidance for EVD insertion. Studies were screened for eligibility and accuracy data was extracted. The risk of bias was assessed using the Cochrane Risk of Bias Tool and the quality of evidence was assessed using the Newcastle-Ottawa-Scale. Accuracy was reported either as the average deviation from target or according to the Kakarla grading system. Of the 497 studies retrieved, 14 were included for analysis. All included studies were prospectively designed. Insertions were performed on anthropomorphic phantoms, cadavers, or patients, using several different AR devices and interfaces. Deviation from target ranged between 0.7 and 11.9 mm. Accuracy according to the Kakarla grading scale ranged between 82 and 96%. Accuracy was higher for AR compared to the freehand technique in all studies that had control groups. Current evidence demonstrates that AR is more accurate than free-hand technique for EVD insertion. However, studies are few, the technology developing, and there is a need for further studies on patients in relevant clinical settings.
C1 [Buwaider, Ali; El-Hajj, Victor Gabriel; Edstrom, Erik; Elmi-Terander, Adrian] Karolinska Inst, Dept Clin Neurosci, Stockholm, Sweden.
   [Edstrom, Erik; Elmi-Terander, Adrian] Lowenstromska Hosp, Capio Spine Ctr Stockholm, Upplands Vasby, Sweden.
   [Iop, Alessandro; Romero, Mario] KTH Royal Inst Technol, Stockholm, Sweden.
   [Jean, Walter] Lehigh Valley Fleming Neurosci Inst, 1250 S Cedar Crest Blvd, Allentown, PA 18103 USA.
   [Edstrom, Erik; Elmi-Terander, Adrian] Orebro Univ, Dept Med Sci, Orebro, Sweden.
   [Elmi-Terander, Adrian] Uppsala Univ, Dept Surg Sci, Uppsala, Sweden.
C3 Karolinska Institutet; Royal Institute of Technology; Orebro University;
   Uppsala University
RP Buwaider, A (corresponding author), Karolinska Inst, Dept Clin Neurosci, Stockholm, Sweden.
EM ali.buwaider@stud.ki.se
OI Elmi-Terander, Adrian/0000-0002-3776-6136; Buwaider,
   Ali/0009-0000-1966-7911
FU Karolinska Institute
FX No Statement AvailableDAS:The data utilized for the writing of this
   article can be provided by the corresponding author upon reasonable
   request.
CR AlAzri A, 2017, ACTA NEUROCHIR, V159, P1399, DOI 10.1007/s00701-017-3201-5
   Aljoghaiman M, 2022, WORLD NEUROSURG, V160, P85, DOI 10.1016/j.wneu.2022.01.036
   [Anonymous], 2020, GRADEpro GDT: GRADEpro Guideline Development Tool Software
   Benmahdjoub M, 2023, IEEE T VIS COMPUT GR, V29, P2434, DOI 10.1109/TVCG.2023.3247042
   Bounajem MT, 2023, NEUROSURGERY, V92, P884, DOI 10.1227/neu.0000000000002293
   Cabrilo I, 2014, OPER NEUROSURG, V10, P252, DOI 10.1227/NEU.0000000000000328
   Chiou SY, 2023, BIOENGINEERING-BASEL, V10, DOI 10.3390/bioengineering10050617
   Chiou SY, 2022, HEALTHCARE-BASEL, V10, DOI 10.3390/healthcare10101815
   Cho J, 2020, WORLD NEUROSURG, V139, P186, DOI 10.1016/j.wneu.2020.04.043
   Clifton W, 2019, WORLD NEUROSURG, V131, P242, DOI 10.1016/j.wneu.2019.07.049
   Dey M, 2012, CURR NEUROL NEUROSCI, V12, P24, DOI 10.1007/s11910-011-0231-x
   Dixon BJ, 2014, AM J RHINOL ALLERGY, V28, P433, DOI 10.2500/ajra.2014.28.4067
   Eom S, 2022, INT SYM MIX AUGMENT, P355, DOI 10.1109/ISMAR55827.2022.00051
   Frisk H, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22020522
   Gibby W, 2022, J NEUROSURG, V137, P489, DOI 10.3171/2021.9.JNS21510
   Gierisch Jennifer M, 2014, health disparities in quality indicators of healthcare among adults with mental illness Internet
   Guha D, 2017, CAN J NEUROL SCI, V44, P235, DOI 10.1017/cjn.2016.443
   Higgins JPT, 2011, BMJ-BRIT MED J, V343, DOI 10.1136/bmj.d5928
   Huyette DR, 2008, J NEUROSURG, V108, P88, DOI 10.3171/JNS/2008/108/01/0088
   Iop A, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22166067
   Kakarla UK, 2008, NEUROSURGERY, V63, P162, DOI 10.1227/01.NEU.0000312390.83127.7F
   Karmonik C, 2018, J DIGIT IMAGING, V31, P26, DOI 10.1007/s10278-017-9991-4
   Khoshnevisan Alireza, 2012, Iran J Psychiatry, V7, P97
   Kunz C, 2021, IEEE T MED ROBOT BIO, V3, P738, DOI 10.1109/TMRB.2021.3091184
   Lai M, 2022, BIOENGINEERING-BASEL, V9, DOI 10.3390/bioengineering9100537
   Li Y, 2019, J NEUROSURG, V131, P1599, DOI 10.3171/2018.4.JNS18124
   Louis RG, 2021, OPER NEUROSURG, V21, P189, DOI 10.1093/ons/opab188
   Marcus HJ, 2015, J NEUROSURG, V123, P307, DOI 10.3171/2014.10.JNS141662
   Meola A, 2017, NEUROSURG REV, V40, P537, DOI 10.1007/s10143-016-0732-9
   Mikhail M, 2019, WORLD NEUROSURG, V128, P268, DOI 10.1016/j.wneu.2019.04.256
   Munzer BW, 2019, J MED INTERNET RES, V21, DOI 10.2196/12368
   Page MJ, 2021, BMJ-BRIT MED J, V372, DOI [10.1016/j.ijsu.2021.105906, 10.1136/bmj.n71, 10.1136/bmj.n160]
   Robertson FC, 2022, J NEUROSURG, V136, P1475, DOI 10.3171/2021.5.JNS211033
   Sarrafzadeh A, 2014, TRIALS, V15, DOI 10.1186/1745-6215-15-478
   Schneider M, 2023, J NEUROL SURG PART A, V84, P562, DOI 10.1055/s-0042-1759827
   Schneider M, 2021, NEUROSURG FOCUS, V50, DOI 10.3171/2020.10.FOCUS20779
   Schünemann HJ, 2008, BMJ-BRIT MED J, V336, P1106, DOI 10.1136/bmj.39500.677199.AE
   Skyrman S, 2021, NEUROSURG FOCUS, V51, DOI 10.3171/2021.5.FOCUS20813
   Stuart MJ, 2021, J CLIN NEUROSCI, V94, P140, DOI 10.1016/j.jocn.2021.10.014
   Tabrizi LB, 2015, J NEUROSURG, V123, P206, DOI 10.3171/2014.9.JNS141001
   Toma AK, 2009, NEUROSURGERY, V65, P1197, DOI 10.1227/01.NEU.0000356973.39913.0B
   Umana GE, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.690705
   Van Gestel F, 2021, NEUROSURG FOCUS, V51, DOI 10.3171/2021.5.FOCUS21215
   Yeh M, 2001, HUM FACTORS, V43, P355, DOI 10.1518/001872001775898269
   Yuen J, 2018, ANN ROY COLL SURG, V100, P221, DOI 10.1308/rcsann.2017.0221
NR 45
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD JUL 24
PY 2024
VL 28
IS 3
AR 141
DI 10.1007/s10055-024-01033-9
PG 10
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA ZL8U2
UT WOS:001275553900001
OA hybrid
DA 2024-08-05
ER

PT J
AU Jeon, H
   Jo, T
   Yeo, D
   An, E
   Kang, Y
   Kim, S
AF Jeon, Hwaseung
   Jo, Taewoo
   Yeo, Dohyeon
   An, Eunsol
   Kang, Yumin
   Kim, SeungJun
TI The way of water: exploring the role of interaction elements in
   usability challenges with in-car VR experience
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; In-car; Automotive; HMD; Autonomous driving; User
   experience
ID VIRTUAL-REALITY; SICKNESS
AB With advancements in autonomous driving technology, the variety of activities that can be performed in a vehicle has increased. This improves the possibility of watching virtual reality (VR) content on a head-mounted display (HMD). However, unlike VR used in stationary environments, in-car VR can lead to discomfort and motion sickness due to the vehicle movements. Additionally, the obstruction of the outside view during driving may cause user anxiety. In this study, we investigated, for the first time, the effect of dynamic road environments, such as turns, stops, and speed bumps, on the in-car VR experience. Based on our findings, we included situational awareness (SA) cues in the in-car VR content to help users perceive their surroundings and improve the user experience. We conducted a user study with thirty participants to validate the impact of these cues. Consequently, we discovered that the Dynamics cue, which provides SA information while maintaining the context of the VR content, improves user immersion and trust while easing VR motion sickness.
C1 [Jeon, Hwaseung; Jo, Taewoo; Yeo, Dohyeon; An, Eunsol; Kang, Yumin; Kim, SeungJun] Gwangju Inst Sci & Technol, Sch Integrated Technol, 123 Cheomdangwagi Ro, Gwangju 61005, South Korea.
   [Kim, SeungJun] Gwangju Inst Sci & Technol, AI Grad Sch, 123 Cheomdangwagi Ro, Gwangju 61005, South Korea.
C3 Gwangju Institute of Science & Technology (GIST); Gwangju Institute of
   Science & Technology (GIST)
RP Kim, S (corresponding author), Gwangju Inst Sci & Technol, Sch Integrated Technol, 123 Cheomdangwagi Ro, Gwangju 61005, South Korea.; Kim, S (corresponding author), Gwangju Inst Sci & Technol, AI Grad Sch, 123 Cheomdangwagi Ro, Gwangju 61005, South Korea.
EM jeonhwaseung@gm.gist.ac.kr; twjioi5349@gm.gist.ac.kr;
   ing.dohyeonyeo@gm.gist.ac.kr; eunsol.hcis@gm.gist.ac.kr;
   yuminkang@gm.gist.ac.kr; seungjun@gist.ac.kr
OI Kim, SeungJun/0000-0003-0470-2483
FU Gwangju Institute of Science and Technology; Gwangju Institute of
   Science and Technology (GIST) - Gwangju Institute of Science and
   Technology (GIST) [RS-2024-00343397]; National Research Foundation of
   Korea (NRF) - Korea government (MSIT)
FX This work was partly supported by the Gwangju Institute of Science and
   Technology (GIST)-Massachusetts Institute of Technology (MIT) Research
   Collaboration grant funded by the Gwangju Institute of Science and
   Technology (GIST) in 2024, and the National Research Foundation of Korea
   (NRF) grant funded by the Korea government (MSIT) (RS-2024-00343397).
CR Baltodano S., 2015, P 7 INT C AUTOMOTIVE, DOI [DOI 10.1145/2799250.2799288, 10.1145/2799250.2799288]
   Beck T, 2021, IEEE CONF COMPU INTE, P47, DOI 10.1109/COG52621.2021.9619025
   Bos JE, 2015, J VESTIBUL RES-EQUIL, V25, P23, DOI 10.3233/VES-150541
   Cho HJ, 2022, IEEE ACCESS, V10, P34003, DOI 10.1109/ACCESS.2022.3162221
   Dickinson P, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P131, DOI 10.1109/VR50410.2021.00034
   Diels C, 2016, APPL ERGON, V53, P374, DOI 10.1016/j.apergo.2015.09.009
   Dominguez C, 1994, Technical report, DOI [10.21236/ADA284752, DOI 10.21236/ADA284752]
   Fereydooni N, 2022, IEEE T VIS COMPUT GR, V28, P3865, DOI 10.1109/TVCG.2022.3203086
   Fracker M.L., 1988, P 32 ANN M HUMAN FAC, P102, DOI [10.1177/154193128803200222, DOI 10.1177/154193128803200222]
   Garland D., 2000, Situation Awareness Analysis and Measurement, DOI [DOI 10.1201/B12461-7, 10.1201/b12461]
   Goedicke D, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173739
   Grubert J, 2018, IEEE COMPUT GRAPH, V38, P125, DOI 10.1109/MCG.2018.2875609
   Han S, 2017, SYMMETRY-BASEL, V9, DOI 10.3390/sym9020022
   HART S G, 1988, P139
   Harth J, 2018, IEEE CONSUM ELECTR M, V7, P36, DOI 10.1109/MCE.2018.2816218
   Himmels C, 2022, ADJUNCT PROCEEDINGS OF THE 14TH INTERNATIONAL ACM CONFERENCE ON AUTOMOTIVE USER INTERFACES AND INTERACTIVE VEHICULAR APPLICATIONS, AUTOMOTIVEUI 2022 ADJUNCT, P53, DOI 10.1145/3544999.3552526
   Hock P, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P4034, DOI 10.1145/3025453.3025665
   Kari M, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2023, DOI 10.1145/3544548.3580677
   Kennedy R.S., 1993, Int. J. Aviat. Psy, P203
   Kodama R, 2017, IEEE SYMP 3D USER, P130, DOI 10.1109/3DUI.2017.7893329
   Koilias A, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1024, DOI [10.1109/VR.2019.8798084, 10.1109/vr.2019.8798084]
   Korber M., 2019, P 20 C INT ERG ASS I, V20, P13, DOI DOI 10.1007/978-3-319-96074-6_2
   Lewis L, 2016, WORK, V54, P963, DOI 10.3233/WOR-162356
   Li JY, 2021, AUTOMOTIVEUI '21: 13TH INTERNATIONAL ACM CONFERENCE ON AUTOMOTIVE USER INTERFACES AND INTERACTIVE VEHICULAR APPLICATIONS, P28, DOI [10.1145/34091183475137, 10.1145/3409118.3475137]
   Malloy KM, 2010, CLIN PSYCHOL REV, V30, P1011, DOI 10.1016/j.cpr.2010.07.001
   Marre Q, 2021, INT J HUM-COMPUT INT, V37, P1089, DOI 10.1080/10447318.2020.1870819
   McGill M, 2022, PROCEEDINGS OF THE 35TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, UIST 2022, DOI 10.1145/3526113.3545657
   McGill M, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P5655, DOI 10.1145/3025453.3026046
   McMahan A., 2003, The video game theory reader
   Paredes Pablo E., 2018, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V2, DOI 10.1145/3287062
   Pöhlmann KMT, 2022, ADJUNCT PROCEEDINGS OF THE 14TH INTERNATIONAL ACM CONFERENCE ON AUTOMOTIVE USER INTERFACES AND INTERACTIVE VEHICULAR APPLICATIONS, AUTOMOTIVEUI 2022 ADJUNCT, P114, DOI 10.1145/3544999.3552488
   ROLNICK A, 1991, ERGONOMICS, V34, P867, DOI 10.1080/00140139108964831
   Rothe S, 2018, LECT NOTES COMPUT SC, V10850, P101, DOI 10.1007/978-3-319-95270-3_7
   SAE I, 2021, Taxonomy and definitions for terms related to driving automation systems for on-road motor vehicles j3016 202104, P41
   Schubert T, 2001, PRESENCE-VIRTUAL AUG, V10, P266, DOI 10.1162/105474601300343603
   Skarbez R, 2018, ACM COMPUT SURV, V50, DOI 10.1145/3134301
   SLATER M, 1994, ARTIFICIAL LIFE AND VIRTUAL REALITY, P125
   Steck F, 2018, TRANSPORT RES REC, V2672, P11, DOI 10.1177/0361198118757980
   Taylor R.M., 2017, Situational Awareness, P111, DOI [10.4324/9781315087924, DOI 10.4324/9781315087924]
   Van Gerwen LJ, 1999, PSYCHOL ASSESSMENT, V11, P146, DOI 10.1037/1040-3590.11.2.146
   Wang P, 2017, ACMIEEE INT CONF HUM, P234, DOI 10.1145/2909824.3020256
   Wilson G, 2023, IEEE T VIS COMPUT GR, V29, P2390, DOI 10.1109/TVCG.2023.3247084
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   Yeo D, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376787
   Yeo D, 2019, ADJUNCT PROCEEDINGS OF THE 2019 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT 2019), P460, DOI 10.1109/ISMAR-Adjunct.2019.00124
   Yusof NM, 2020, INT J AUTOMO MECH E, V17, P7771, DOI 10.15282/ijame.17.1.2020.23.0578
NR 46
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD JUN 7
PY 2024
VL 28
IS 3
AR 121
DI 10.1007/s10055-024-01001-3
PG 20
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA TS8W4
UT WOS:001243350000001
OA hybrid
DA 2024-08-05
ER

PT J
AU Spyridonis, F
   Daylamani-Zad, D
   Nightingale, J
AF Spyridonis, Fotios
   Daylamani-Zad, Damon
   Nightingale, James
TI PublicVR: a virtual reality exposure therapy intervention for adults
   with speech anxiety
SO VIRTUAL REALITY
LA English
DT Article
DE Speech anxiety; Phobias; Public speaking; Social anxiety; Virtual
   Reality; VR; VRET
ID SELF-FOCUSED ATTENTION; SOCIAL ANXIETY; SPEAKING ANXIETY; DISORDER;
   METAANALYSIS; TECHNOLOGY; EMOTIONS; EFFICACY; PEOPLE; HEALTH
AB Speech anxiety, or Glossophobia, currently affects approximately 75% of the population with potentially severe negative effects on those with this condition. There are several treatments currently available with research showing that the use of Virtual Reality (VR) as a non-pharmacologic treatment can have positive effects on individuals suffering from such social phobias. However, there is a significant lack of treatments currently available for speech anxiety, even though such a large number of the population are affected by it. In this paper, we aim to contribute to efforts to improve the effects of speech anxiety through a VR intervention. Our VR solution was designed following the Exposure Therapy approach for treating social anxiety disorders. The evaluation of this work was twofold: A. to assess the ability of our solution to positively change participants' perception of factors related to non-verbal communication contributing to anxiety toward public speaking, and B. to determine whether it is able to induce a sense of presence. We carried out an empirical evaluation study that measured participants' self-reported anxiety level towards public speaking using the Personal Report of Public Speaking Anxiety and their perceived sense of presence using the iGroup Presence Questionnaire. Our results demonstrate the potential of VR Exposure Therapy solutions to assist towards positively changing perception of factors related to non-verbal communication skills that contribute to increasing public speaking anxiety for participants suffering from self-reported speech anxiety symptoms. Our findings are of wider importance as they contribute to ongoing efforts to improve social anxiety-related phobias.
C1 [Spyridonis, Fotios; Daylamani-Zad, Damon] Brunel Univ London, Coll Engn Design & Phys Sci, Kingston Lane, London UB8 3PH, England.
   [Nightingale, James] Univ Greenwich, Sch Comp & Math Sci, London SE10 9LS, England.
C3 Brunel University; University of Greenwich
RP Spyridonis, F (corresponding author), Brunel Univ London, Coll Engn Design & Phys Sci, Kingston Lane, London UB8 3PH, England.
EM fotios.spyridonis@brunel.ac.uk; damon.daylamani-zad@brunel.ac.uk;
   j.nightingale@gre.ac.uk
OI Spyridonis, Fotios/0000-0003-4253-365X
CR Agius H, 2019, MEDIA COMMUN-LISBON, V7, P247, DOI 10.17645/mac.v7i4.2321
   Albakri G, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12031672
   Amodeo J, 2014, Deconstructing the fear of rejection
   Anderson PL, 2013, J CONSULT CLIN PSYCH, V81, P751, DOI 10.1037/a0033559
   [Anonymous], 2019, Overview - cognitive behavioural therapy (cbt)
   [Anonymous], 2017, What is exposure therapy?
   Asakereh A, 2015, ISS EDUC RES, V25, P345
   BEATTY MJ, 1989, COMMUN EDUC, V38, P277, DOI 10.1080/03634528909378763
   Behnke RR, 1999, COMMUN EDUC, V48, P165, DOI 10.1080/03634529909379164
   Bell C.C., 1994, Diagnostic and statistical manual of mental disorders, V272, P828, DOI [DOI 10.1001/JAMA.1994.03520100096046, 10.1001/jama.1994.03520100096046]
   Bell IH, 2020, DIALOGUES CLIN NEURO, V22, P169, DOI 10.31887/DCNS.2020.22.2/lvalmaggia
   Benbow AA, 2019, J ANXIETY DISORD, V61, P18, DOI 10.1016/j.janxdis.2018.06.006
   Böhnlein J, 2020, NEUROSCI BIOBEHAV R, V108, P796, DOI 10.1016/j.neubiorev.2019.12.009
   Boeldt D, 2019, FRONT PSYCHIATRY, V10, DOI 10.3389/fpsyt.2019.00773
   Botella C, 2017, CURR PSYCHIAT REP, V19, DOI 10.1007/s11920-017-0788-4
   Brown AD, 2012, CONSCIOUS COGN, V21, P299, DOI 10.1016/j.concog.2011.09.023
   Carl E, 2019, J ANXIETY DISORD, V61, P27, DOI 10.1016/j.janxdis.2018.08.003
   Chen YC, 2024, COMPUT ASSIST LANG L, V37, P789, DOI 10.1080/09588221.2022.2055083
   Chollet M, 2015, PROCEEDINGS OF THE 2015 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP 2015), P1143, DOI 10.1145/2750858.2806060
   Clark D.M., 1995, Social phobia: Diagnosis, assessment, and treatment, P69
   Craske MG, 2014, BEHAV RES THER, V58, P10, DOI 10.1016/j.brat.2014.04.006
   Crozier WR, 2003, BRIT J EDUC PSYCHOL, V73, P317, DOI 10.1348/000709903322275858
   Daly J.A., 1997, Avoiding communication: Shyness, reticence, and communication apprehension, V2nd
   DALY JA, 1989, PERS INDIV DIFFER, V10, P903, DOI 10.1016/0191-8869(89)90025-1
   DALY JA, 1995, COMMUN MONOGR, V62, P383, DOI 10.1080/03637759509376368
   David D, 2018, FRONT PSYCHIATRY, V9, DOI 10.3389/fpsyt.2018.00004
   Daylamani-Zad D, 2016, DECIS SUPPORT SYST, V85, P49, DOI 10.1016/j.dss.2016.02.011
   de Bruijn GJ, 2009, AM J PREV MED, V36, P189, DOI 10.1016/j.amepre.2008.10.019
   Eccleston C, 2022, PAIN, V163, P1700, DOI 10.1097/j.pain.0000000000002617
   European Commission, 2022, Environmental liability directive 2004/35/ec
   Feist J., 2006, THEORIES PERSONALITY, V6th
   Geisen M, 2022, INT J SPORTS SCI COA, V17, P1178, DOI 10.1177/17479541211051006
   Greenleaf W, 2016, ACM SIGGRAPH 2016 VR VILLAGE (SIGGRAPH '16), DOI 10.1145/2929490.2956569
   Hancock AB, 2010, J VOICE, V24, P302, DOI 10.1016/j.jvoice.2008.09.007
   Harkness AR, 1997, PSYCHOL ASSESSMENT, V9, P349, DOI 10.1037/1040-3590.9.4.349
   Hattie J, 2007, REV EDUC RES, V77, P81, DOI 10.3102/003465430298487
   Hinojo-Lucena FJ, 2020, J PERS MED, V10, DOI 10.3390/jpm10010014
   Hofmann S G., 2017, Cognitive behavioral therapy for social anxiety disorder: Evidence-based and disorder specific treatment techniques, DOI DOI 10.4324/9781315617039
   Hofmann SG, 2008, J CLIN PSYCHIAT, V69, P621, DOI 10.4088/jcp.v69n0415
   Holmes J, 2002, BMJ-BRIT MED J, V324, P288, DOI 10.1136/bmj.324.7332.288
   INGRAM RE, 1990, PSYCHOL BULL, V107, P156, DOI 10.1037/0033-2909.107.2.156
   Kashdan TB, 2014, EMOTION, V14, P629, DOI 10.1037/a0035796
   Kashdan TB, 2013, J ABNORM PSYCHOL, V122, P645, DOI 10.1037/a0032733
   Kushner M., 2010, Public speaking for dummies
   Kyllonen P.C., 2012, Invitational Research Symposium on Technology Enhanced Assessments, P7
   Lee AD, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P133, DOI [10.1109/VRW50115.2020.0-24, 10.1109/VRW50115.2020.00028]
   Lemasson A, 2018, BEHAV PROCESS, V157, P225, DOI 10.1016/j.beproc.2018.10.003
   Lin XB, 2019, JMIR MENT HEALTH, V6, DOI 10.2196/13869
   Lindner P, 2021, COGN BEHAV THERAPY, V50, P67, DOI 10.1080/16506073.2020.1795240
   Lindner P, 2019, J ANXIETY DISORD, V61, P45, DOI 10.1016/j.janxdis.2018.07.003
   Lucas Stephen., 2020, ART PUBLIC SPEAKING
   Lucero Katie Stringer, 2020, J Eur CME, V9, P1834759, DOI 10.1080/21614083.2020.1834759
   MacIntyre Peter D., 1997, Communication research reports, V14, P157, DOI [DOI 10.1080/08824099709388657, 10.1080/08824099709388657]
   Manolakis K., 2022, J Dent, V121, P987
   Maples-Keller JL, 2017, HARVARD REV PSYCHIAT, V25, P103, DOI 10.1097/HRP.0000000000000138
   Marciniak MD, 2005, DEPRESS ANXIETY, V21, P178, DOI 10.1002/da.20074
   MCCROSKEY JC, 1970, SPEECH MONOGR, V37, P269
   McCroskey JC., 2015, An introduction to rhetorical communication, DOI [10.4324/9781315663791, DOI 10.4324/9781315663791]
   McGuire JF, 2014, EXPERT REV NEUROTHER, V14, P893, DOI 10.1586/14737175.2014.934677
   Meadows A., 2018, Body image, eating, and weight: a guide to assessment, treatment, and prevention, P381
   Mehrabian A., 1981, SILENT MESSAGES IMPL
   Melo M, 2023, IEEE ACCESS, V11, P24675, DOI 10.1109/ACCESS.2023.3254892
   Morina N, 2015, BEHAV RES THER, V74, P18, DOI 10.1016/j.brat.2015.08.010
   Morreale S, 1990, development of a communication-competency based speech evaluation form and manual
   Mulyani S., 2018, Int J Appl Linguist Parahikma, V1, P85
   National Institute for Health and Care Excellence, 2022, Social anxiety disorder: recognition, assessment and treatment
   Nichols S, 2000, INT J HUM-COMPUT ST, V52, P471, DOI 10.1006/ijhc.1999.0343
   Odeleye B, 2023, COMPUT SECUR, V124, DOI 10.1016/j.cose.2022.102951
   Odgers K, 2022, BEHAV RES THER, V159, DOI 10.1016/j.brat.2022.104203
   Paradewari D.S., 2017, INT J ED RES, V5, P97
   Perera J, 2008, MED TEACH, V30, P395, DOI 10.1080/01421590801949966
   Pertaub DP, 2002, PRESENCE-TELEOP VIRT, V11, P68, DOI 10.1162/105474602317343668
   Pizek Mestric N, 2016, Neverbalna komunikacija u javnom nastupu
   Pratama MA., 2018, J Lang Lang Teach, V5, P67, DOI [10.33394/jollt.v5i2.357, DOI 10.33394/JOLLT.V5I2.357]
   Quianthy R., 1999, Popul Measur, V2, P27
   Rahman AZ, 2018, E3S WEB CONF, V73, DOI 10.1051/e3sconf/20187310005
   Rajitha K., 2020, Procedia Computer Science, V172, P1053, DOI [DOI 10.1016/J.PROCS.2020.05.154, 10.1016/j.procs.2020.05.154]
   Robillard G, 2003, CYBERPSYCHOL BEHAV, V6, P467, DOI 10.1089/109493103769710497
   Safir MP, 2012, BEHAV MODIF, V36, P235, DOI 10.1177/0145445511429999
   Samantaray NN, 2022, PSYCHIAT RES, V310, DOI 10.1016/j.psychres.2022.114439
   Sars David, 2015, BMC Psychol, V3, P26, DOI 10.1186/s40359-015-0083-2
   Sawyer C.R., 1999, Communication Reports, V12, P33, DOI [DOI 10.1080/08934219909367706, 10.1080/08934219909367706]
   Schneider J, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P538, DOI 10.1145/2818346.2830603
   Schönfeld P, 2016, INT J CLIN HLTH PSYC, V16, P1, DOI 10.1016/j.ijchp.2015.08.005
   Schultheis MT, 2001, REHABIL PSYCHOL, V46, P296, DOI 10.1037/0090-5550.46.3.296
   Shamsi A.F., 2019, International Journal of Linguistics, Literature, and Translation, V2, P276, DOI [DOI 10.32996/IJLLT.2019.2.1, 10.32996/ijllt.2019.2.1.3]
   Freitas JRS, 2021, PSYCHIAT QUART, V92, P1685, DOI 10.1007/s11126-021-09935-6
   Spanjol-Markovic M, 2008, Manual for public speaking and business presentation, P75
   Spyridonis F, 2014, MULTIMED TOOLS APPL, V72, P191, DOI 10.1007/s11042-013-1358-3
   Sutcliffe A, 2004, INTERACT COMPUT, V16, P831, DOI 10.1016/j.intcom.2004.05.001
   Thomson S., 2002, Commun Res Rep, V19, P18, DOI [DOI 10.1080/08824090209384828, 10.1080/08824090209384828]
   Tian N, 2022, VIRTUAL REAL-LONDON, V26, P1409, DOI 10.1007/s10055-022-00638-2
   van Loenen I, 2022, J MED INTERNET RES, V24, DOI 10.2196/26736
   Wade A, 2012, The economic burden of anxiety and depression
   WHO, 2022, COVID-19 Pandemic Triggers 25% Increase in Prevalence of Anxiety and Depression Worldwide
   Wiederhold BK., 2005, Virtual reality therapy for anxiety disorders: Advances in evaluation and treatment, DOI [10.1037/10858-000, DOI 10.1037/10858-000]
   Woody SR, 2000, COGNITIVE THER RES, V24, P473, DOI 10.1023/A:1005583820758
NR 97
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD APR 30
PY 2024
VL 28
IS 2
AR 105
DI 10.1007/s10055-024-00998-x
PG 19
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA OY4B9
UT WOS:001210814000001
OA hybrid
DA 2024-08-05
ER

PT J
AU Gómez-Sirvent, JL
   Fernández-Sotos, A
   Fernández-Caballero, A
   Fernández-Sotos, D
AF Gomez-Sirvent, Jose L.
   Fernandez-Sotos, Alicia
   Fernandez-Caballero, Antonio
   Fernandez-Sotos, Desiree
TI Assessment of music performance anxiety in a virtual auditorium through
   the study of ambient lighting and audience distance
SO VIRTUAL REALITY
LA English
DT Article
DE Neuroarchitecture; Performance anxiety; Virtual reality; Eye tracking;
   Music
ID PERCEPTION; EXPOSURE; STAGE
AB Performance anxiety is a common problem affecting musicians' concentration and well-being. Musicians frequently encounter greater challenges and emotional discomfort when performing in front of an audience. Recent research suggests an important relationship between the characteristics of the built environment and people's well-being. In this study, we explore modifying the built environment to create spaces where musicians are less aware of the presence of the audience and can express themselves more comfortably. An experiment was conducted with 61 conservatory musicians playing their instrument in a virtual auditorium in front of an audience of hundreds of virtual humans. They performed at different distances from the audience and under different levels of ambient lighting, while their eye movements were recorded. These data, together with questionnaires, were used to analyse the way the environment is perceived. The results showed that reducing the light intensity above the audience made the view of the auditorium more calming, and the same effect was observed when the distance between the audience and the musician was increased. Eye-tracking data showed a significant reduction in saccadic eye movements as the distance from the audience increased. This work provides a novel approach to architecture influence on musicians' experience during solo performances. The findings are useful to designers and researchers.
C1 [Gomez-Sirvent, Jose L.; Fernandez-Caballero, Antonio; Fernandez-Sotos, Desiree] Inst Invest Informat Albacete, Neurocognit & Emot Unit, Albacete, Spain.
   [Gomez-Sirvent, Jose L.; Fernandez-Caballero, Antonio] Univ Castilla La Mancha, Dept Sistemas Informat, Albacete, Spain.
   [Fernandez-Sotos, Alicia] Conservatorio Mus Murcia, Murcia, Spain.
   [Fernandez-Caballero, Antonio] Biomed Res Networking Ctr Mental Hlth CIBERSAM, Madrid, Spain.
C3 Universidad de Castilla-La Mancha; CIBER - Centro de Investigacion
   Biomedica en Red; CIBERSAM
RP Fernández-Caballero, A (corresponding author), Inst Invest Informat Albacete, Neurocognit & Emot Unit, Albacete, Spain.; Fernández-Caballero, A (corresponding author), Univ Castilla La Mancha, Dept Sistemas Informat, Albacete, Spain.; Fernández-Caballero, A (corresponding author), Biomed Res Networking Ctr Mental Hlth CIBERSAM, Madrid, Spain.
FU CRUE-CSIC; Springer Nature
FX Open Access funding provided thanks to the CRUE-CSIC agreement with
   Springer Nature.
CR Adhanom IB, 2023, VIRTUAL REAL-LONDON, V27, P1481, DOI 10.1007/s10055-022-00738-z
   Assem HM, 2023, AIN SHAMS ENG J, V14, DOI 10.1016/j.asej.2022.102102
   Beacco A, 2016, COMPUT GRAPH FORUM, V35, P32, DOI 10.1111/cgf.12774
   Bissonnette J, 2016, VIRTUAL REAL-LONDON, V20, P71, DOI 10.1007/s10055-016-0283-y
   Castilla N, 2023, BUILD ENVIRON, V228, DOI 10.1016/j.buildenv.2022.109833
   Chirico A, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00906
   Dancey C.P., 2004, STAT MATHS PSYCHOL, V3rd
   Ergan S, 2018, J BUILD ENG, V20, P51, DOI 10.1016/j.jobe.2018.07.004
   Fernandez-Sotos A, 2023, AMBIENT INTELLIGENCE, P1
   Fernholz I, 2019, PSYCHOL MED, V49, P2287, DOI 10.1017/S0033291719001910
   Fich LB, 2014, PHYSIOL BEHAV, V135, P91, DOI 10.1016/j.physbeh.2014.05.034
   Giuliani MV, 2009, J ENVIRON PSYCHOL, V29, P375, DOI 10.1016/j.jenvp.2008.11.008
   Gonzalez-Franco M, 2020, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.561558
   HAMANN DL, 1982, J RES MUSIC EDUC, V30, P77, DOI 10.2307/3345040
   JACKSON JM, 1981, J PERS SOC PSYCHOL, V40, P73, DOI 10.1037/0022-3514.40.1.73
   Karakas T, 2020, FRONT ARCHIT RES, V9, P236, DOI 10.1016/j.foar.2019.10.005
   LeBlanc A, 1997, J RES MUSIC EDUC, V45, P480, DOI 10.2307/3345541
   Lee CL, 2023, VIRTUAL REAL-LONDON, V27, P1529, DOI 10.1007/s10055-023-00751-w
   Li Z, 2021, FRONT ARCHIT RES, V10, P317, DOI 10.1016/j.foar.2021.01.002
   Mostajeran F, 2020, 2020 IEEE C VIRTUAL, DOI [10.1109/vr46266.2020.00050, DOI 10.1109/VR46266.2020.00050]
   Narum SR, 2006, CONSERV GENET, V7, P783, DOI 10.1007/s10592-005-9056-y
   Hernández SO, 2018, INT J MUSIC EDUC, V36, P460, DOI 10.1177/0255761418763903
   Orman EK, 2004, J MUSIC THER, V41, P70, DOI 10.1093/jmt/41.1.70
   Osborne MS., 2022, Music Performance Anxiety, P204
   Papageorgi I, 2013, PSYCHOL MUSIC, V41, P18, DOI 10.1177/0305735611408995
   Papageorgi I, 2007, RES STUD MUSIC EDUC, V28, P83, DOI 10.1177/1321103X070280010207
   Pertaub DP, 2002, PRESENCE-TELEOP VIRT, V11, P68, DOI 10.1162/105474602317343668
   SADALLA EK, 1984, ENVIRON BEHAV, V16, P394, DOI 10.1177/0013916584163005
   SALMON PG, 1990, MED PROBL PERFORM AR, V5, P2
   Serafin S, 2016, COMPUT MUSIC J, V40, P22, DOI 10.1162/COMJ_a_00372
   Shen WL, 2013, AUTOMAT CONSTR, V32, P112, DOI 10.1016/j.autcon.2013.01.014
   Skaramagkas V, 2023, IEEE REV BIOMED ENG, V16, P260, DOI 10.1109/RBME.2021.3066072
   Slater M., 1994, Presence: teleoperators virtual environments, V3, P130, DOI [10.1162/pres.1994.3.2.130, DOI 10.1162/PRES.1994.3.2.130, 10.1162/pres.1994.3.2.1302]
   Spahn C, 2015, PROG BRAIN RES, V217, P129, DOI 10.1016/bs.pbr.2014.11.024
   Tseng KC, 2017, LECT NOTES COMPUT SC, V10279, P612, DOI 10.1007/978-3-319-58700-4_50
   Vahia VN, 2013, INDIAN J PSYCHIAT, V55, P220, DOI 10.4103/0019-5545.117131
   Vittori F, 2021, J BUILD ENG, V41, DOI 10.1016/j.jobe.2021.102368
NR 37
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR 8
PY 2024
VL 28
IS 2
AR 71
DI 10.1007/s10055-024-00947-8
PG 14
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA LG7F1
UT WOS:001185691200006
OA hybrid
DA 2024-08-05
ER

PT J
AU Laera, F
   Foglia, MM
   Fiorentino, M
AF Laera, Francesco
   Foglia, Mario Massimo
   Fiorentino, Michele
TI Augmented reality for sailing: a comparative study of head stabilized vs
   boat stabilized visualization data for wind and bearing angle
SO VIRTUAL REALITY
LA English
DT Article
DE Augmented reality; Sailing; Nautical; Navigation; Human-computer
   interaction; User study
ID COGNITIVE LOAD; ISSUES
AB The wind has been a natural and renewable resource used for professional and recreational maritime transportation of small and large vessels since human history. Sailing is making a comeback due to the growing focus on sustainability, accelerated by the recent global energy crisis. Seafarers rely on wind and bearing angle visualization to navigate efficiently and safely, thanks to the use of sensors and compasses. This paper focuses on Augmented Reality in Head-Mounted Displays visualization of wind and bearing angle data. We analyzed the literature and generated a heatmap of the used areas in the user's field of view. Second, we designed and implemented two interfaces that use two different visualization techniques: Boat Stabilized (BS) and Head Stabilized (HS). We compared them in between the subject experiment (N = 44), using a simulated Virtual Reality simulator of the sailing scenario. The user's primary task is wind events recognition, while obstacles (buoys) detection is secondary. We measured both task errors and reaction time, and submit NASA RTLX, SUS, UEQ, and visive auditive and kinesthetic (VAK) questionnaires. We found that BS has a significantly lower reaction time and better usability in the primary and secondary tasks. Both visualization techniques have similar users perceived cognitive load and user experience evaluation. VAK test showed that BS is better for kinaesthetic types and HS is better for visual types.
C1 [Laera, Francesco; Foglia, Mario Massimo; Fiorentino, Michele] Polytech Univ Bari, Dept Mech, Math & Management, Via Orabona 4, Bari, Italy.
C3 Politecnico di Bari
RP Laera, F (corresponding author), Polytech Univ Bari, Dept Mech, Math & Management, Via Orabona 4, Bari, Italy.
EM francesco.laera@poliba.it
RI ; Fiorentino, Michele/M-6976-2015
OI Laera, Francesco/0000-0001-5244-5405; Fiorentino,
   Michele/0000-0003-2197-6574
FU Ministero dell'Universit e della Ricerca
FX No Statement Available
CR Balabanian A, 2018, Azadux
   Bernard ML, 2003, INT J HUM-COMPUT ST, V59, P823, DOI 10.1016/S1071-5819(03)00121-6
   Billinghurst M, 1998, SECOND INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS - DIGEST OF PAPERS, P76, DOI 10.1109/ISWC.1998.729532
   BoatPilot, 2017, BoatPilot
   Bressa N, 2022, IEEE T VIS COMPUT GR, V28, P107, DOI 10.1109/TVCG.2021.3114835
   Byers J.C., 1989, Advances in Industrial Ergonomics and Safety, P481
   Chislett V., 2005, VAK LEARNING STYLES
   CRONBACH LJ, 1951, PSYCHOMETRIKA, V16, P297, DOI DOI 10.1007/BF02310555
   Dominic J, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P607, DOI [10.1109/VR46266.2020.1581637338566, 10.1109/VR46266.2020.00-21]
   EWOL, 2018, EWOL, World's first Augmented Reality Sailing Navigation
   Faria ND, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P763, DOI [10.1109/VRW50115.2020.00-43, 10.1109/VRW50115.2020.00232]
   Fiorentino M, 2021, MAR TECHNOL SOC J, V55, P64
   Gabbard JL, 2014, P IEEE, V102, P124, DOI 10.1109/JPROC.2013.2294642
   Hart SG., 2006, P HUM FACT ERG SOC A, V50, P904, DOI [10.1177/154193120605000909, DOI 10.1177/154193120605000909]
   Jose R, 2016, PROCEEDINGS OF THE 28TH AUSTRALIAN COMPUTER-HUMAN INTERACTION CONFERENCE (OZCHI 2016), DOI 10.1145/3010915.3010918
   Kirschner PA, 2002, LEARN INSTR, V12, P1, DOI 10.1016/S0959-4752(01)00014-7
   Laera Francesco, 2023, Advances on Mechanics, Design Engineering and Manufacturing IV: Proceedings of the International Joint Conference on Mechanics, Design Engineering & Advanced Manufacturing, JCM 2022. Lecture Notes in Mechanical Engineering, P1305, DOI 10.1007/978-3-031-15928-2_114
   Laera F, 2020, Augmented Reality for Easy Sailing
   Laera F, 2020, 2021 IEEE INT S MIX, P260
   Laera F, 2023, VIRTUAL REAL-LONDON, V27, P929, DOI 10.1007/s10055-022-00706-7
   Laera F, 2021, J NAVIGATION, V74, P1073, DOI 10.1017/S0373463321000412
   Lee C, 2012, IEEE VIRTUAL REALITY CONFERENCE 2012 PROCEEDINGS, P77, DOI 10.1109/VR.2012.6180890
   Lee C, 2013, IEEE T VIS COMPUT GR, V19, P547, DOI 10.1109/TVCG.2013.41
   Lewis JR, 2018, INT J HUM-COMPUT INT, V34, P577, DOI 10.1080/10447318.2018.1455307
   Ling B, 2019, Journal of Physics: Conference Series
   LLC Google, 2011, Google Fonts
   LLC. LETC, 2021, NMEA 2000 Networks
   MORONEY WF, 1992, PROC NAECON IEEE NAT, P734, DOI 10.1109/NAECON.1992.220513
   Nam BW, 2017, WORLD C CIV STRUCT E
   Ostendorp MC, 2015, PROCEDIA MANUF, V3, P2840, DOI 10.1016/j.promfg.2015.07.775
   Perondi L., 2017, MD J, V3, P88
   Rasch D, 2011, STAT PAP, V52, P219, DOI 10.1007/s00362-009-0224-x
   Sauro J, 2012, QUANTIFYING THE USER EXPERIENCE: PRACTICAL STATISTICS FOR USER RESEARCH, P1
   Schrepp M, 2017, INT J INTERACT MULTI, V4, P40, DOI 10.9781/ijimai.2017.445
   SWELLER J, 1990, J EXP PSYCHOL GEN, V119, P176, DOI 10.1037/0096-3445.119.2.176
   Tactiqs®, 2019, The Tactiqs® Performance System
   Uva AE, 2018, INT J ADV MANUF TECH, V94, P509, DOI 10.1007/s00170-017-0846-4
   Uva AE, 2019, IEEE T HUM-MACH SYST, V49, P421, DOI 10.1109/THMS.2019.2919719
   Wisernig E, 2016, P 2015 INT C CYB CW
   Xiong D, 2016, Lecture Notes in Electrical Engineering
   Xu Manfei, 2017, Shanghai Arch Psychiatry, V29, P184, DOI 10.11919/j.issn.1002-0829.217070
NR 41
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 31
DI 10.1007/s10055-023-00936-3
PG 12
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA GJ8P2
UT WOS:001152396200003
OA hybrid
DA 2024-08-05
ER

PT J
AU Peereboom, J
   Tabone, W
   Dodou, D
   de Winter, J
AF Peereboom, Joris
   Tabone, Wilbert
   Dodou, Dimitra
   de Winter, Joost
TI Head-locked, world-locked, or conformal diminished-reality? An
   examination of different AR solutions for pedestrian safety in occluded
   scenarios
SO VIRTUAL REALITY
LA English
DT Article
DE Augmented reality; Pedestrian safety; Virtual reality experiment; Local
   presence; Assisted reality; Diminished reality
ID ATTENTION; PATTERNS
AB Many collisions between pedestrians and cars are caused by poor visibility, such as occlusion by a parked vehicle. Augmented reality (AR) could help to prevent this problem, but it is unknown to what extent the augmented information needs to be embedded into the world. In this virtual reality experiment with a head-mounted display (HMD), 28 participants were exposed to AR designs, in a scenario where a vehicle approached from behind a parked vehicle. The experimental conditions included a head-locked live video feed of the occluded region, meaning it was fixed in a specific location within the view of the HMD (VideoHead), a world-locked video feed displayed across the street (VideoStreet), and two conformal diminished reality designs: a see-through display on the occluding vehicle (VideoSeeThrough) and a solution where the occluding vehicle has been made semi-transparent (TransparentVehicle). A Baseline condition without augmented information served as a reference. Additionally, the VideoHead and VideoStreet conditions were each tested with and without the addition of a guiding arrow indicating the location of the approaching vehicle. Participants performed 42 trials, 6 per condition, during which they had to hold a key when they felt safe to cross. The keypress percentages and responses from additional questionnaires showed that the diminished-reality TransparentVehicle and VideoSeeThrough designs came out most favourably, while the VideoHead solution caused some discomfort and dissatisfaction. An analysis of head yaw angle showed that VideoHead and VideoStreet caused divided attention between the screen and the approaching vehicle. The use of guiding arrows did not contribute demonstrable added value. AR designs with a high level of local embeddedness are beneficial for addressing occlusion problems when crossing. However, the head-locked solutions should not be immediately dismissed because, according to the literature, such solutions can serve tasks where a salient warning or instruction is beneficial.
C1 [Peereboom, Joris; Tabone, Wilbert; Dodou, Dimitra; de Winter, Joost] Delft Univ Technol, Fac Mech Engn, Delft, Netherlands.
C3 Delft University of Technology
RP de Winter, J (corresponding author), Delft Univ Technol, Fac Mech Engn, Delft, Netherlands.
EM j.c.f.dewinter@tudelft.nl
OI de Winter, Joost/0000-0002-1281-8200; Dodou,
   Dimitra/0000-0002-9428-3261; Tabone, Wilbert/0000-0002-5796-9571
FU European Union [860410, SHAPE-IT]
FX This project has received funding from the European Union's Horizon 2020
   Research and Innovation Programme under the Marie Sk &
   lstrok;odowska-Curie Grant Agreement No. 860410 (project name:
   SHAPE-IT). We thank Vishal Onkhar for assisting the first authors in
   setting up the Unity game development program and getting acquainted
   with the open-source simulator software of Bazlinskyy et al. (2020b).
CR Ardino P, 2021, INT C PATT RECOG, P9280, DOI 10.1109/ICPR48806.2021.9412690
   Balint A, 2021, Report No. D2.6
   Bauerfeind K, 2021, APPL ERGON, V94, DOI 10.1016/j.apergo.2021.103398
   Bazilinskyy P, 2020, coupled-sim
   Bazilinskyy P, 2020, P DRIV SIM C EUR
   Bazilinskyy P, 2020, IEEE SYS MAN CYBERN, P3721, DOI 10.1109/SMC42975.2020.9282998
   Bos JE, 2005, AVIAT SPACE ENVIR MD, V76, P1111
   BRADLEY JV, 1958, J AM STAT ASSOC, V53, P525, DOI 10.2307/2281872
   Byers J.C., 1989, Advances in Industrial Ergonomics and Safety, P481
   Chen CJ, 2015, INT J ADV MANUF TECH, V76, P753, DOI 10.1007/s00170-014-6321-6
   Chen WT, 2023, TRAFFIC INJ PREV, V24, P344, DOI 10.1080/15389588.2023.2186735
   Cheng YF, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517452
   Dey D, 2021, IT-INF TECHNOL, V63, P123, DOI 10.1515/itit-2020-0025
   DINED, 2020, Anthropomorphic database
   Dixon BJ, 2014, AM J RHINOL ALLERGY, V28, P433, DOI 10.2500/ajra.2014.28.4067
   Elharrouss O, 2020, NEURAL PROCESS LETT, V51, P2007, DOI 10.1007/s11063-019-10163-0
   European Road Safety Observatory, 2018, Traffic safety basic facts 2018-pedestrians
   Faria ND, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P763, DOI [10.1109/VRW50115.2020.00-43, 10.1109/VRW50115.2020.00232]
   Feiner S., 1993, Sixth Annual Symposium on User Interface Software and Technology. Proceedings of the ACM Symposium on User Interface Software and Technology, P145, DOI 10.1145/168642.168657
   Fukushima S, 2020, INT SYM MIX AUGMENT, P649, DOI 10.1109/ISMAR50242.2020.00093
   Gabbard JL, 2019, FRONT ROBOT AI, V6, DOI 10.3389/frobt.2019.00098
   Ghasemi Yalda, 2021, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, P1094, DOI 10.1177/1071181321651169
   Gomes P, 2012, IEEE T INTELL TRANSP, V13, P930, DOI 10.1109/TITS.2012.2188289
   HART S G, 1988, P139
   Hunter W., 1996, PEDESTRIAN BICYCLE C
   Kaleefathullah AA, 2022, HUM FACTORS, V64, P1070, DOI 10.1177/0018720820970751
   Kaufeld M, 2022, DISPLAYS, V74, DOI 10.1016/j.displa.2022.102283
   Kerr SJ, 2012, P 2012 SE AS NETW ER, DOI [10.1109/SEANES.2012.6299589, DOI 10.1109/SEANES.2012.6299589]
   Kim H, 2018, IEEE T VIS COMPUT GR, V24, P1515, DOI 10.1109/TVCG.2018.2793680
   Kim H, 2016, PROCEEDINGS OF THE 21ST INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES (IUI'16), P294, DOI 10.1145/2856767.2856815
   Kim K, 2012, IEEE VIRTUAL REALITY CONFERENCE 2012 PROCEEDINGS, P143, DOI 10.1109/VR.2012.6180922
   Klose EM, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P636, DOI [10.1109/VR.2019.8797992, 10.1109/vr.2019.8797992]
   Kooijman L, 2019, INFORMATION, V10, DOI 10.3390/info10120386
   Lebeck K, 2017, P IEEE S SECUR PRIV, P320, DOI 10.1109/SP.2017.13
   Lee H, 2023, Symposium Virtual Re, P519, DOI 10.1109/VR55154.2023.00067
   Liao M., 2020, Computer vision ECCV 2020, DOI [10.1007/978-3-030-58589-11, DOI 10.1007/978-3-030-58589-11]
   Lindemann P, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1040, DOI [10.1109/vr.2019.8798069, 10.1109/VR.2019.8798069]
   Liu B, 2021, CARTOGR GEOGR INF SC, V48, P305, DOI 10.1080/15230406.2021.1908171
   Liu ZY, 2015, INT CONF CONNECT VEH, P100, DOI 10.1109/ICCVE.2015.63
   Lu WQ, 2012, INT SYM MIX AUGMENT, P161, DOI 10.1109/ISMAR.2012.6402553
   MagicLeap, 2020, 5.1 Head-locked content-Unity
   Mann S, 2002, PRESENCE-VIRTUAL AUG, V11, P158, DOI 10.1162/105474602317396039
   Markov-Vetter D, 2020, P 2020 ACM S SPATIAL, V10, DOI [10.1145/3385959.3418449, DOI 10.1145/3385959.3418449]
   Meerits S, 2015, PROCEEDINGS OF THE 2015 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY WORKSHOPS, P53, DOI 10.1109/ISMARW.2015.19
   Mok CS, 2022, APPL ERGON, V105, DOI 10.1016/j.apergo.2022.103825
   Morey RD, 2008, TUTOR QUANT METHODS, V4, P61, DOI 10.20982/tqmp.04.2.p061
   Mori Shohei, 2017, IPSJ Transactions on Computer Vision and Applications, V9, P1, DOI [10.1186/s41074-017-0028-1, DOI 10.1186/S41074-017-0028-1]
   Onkhar V, 2022, TRANSPORT RES F-TRAF, V84, P194, DOI 10.1016/j.trf.2021.10.017
   Orlosky J, 2019, ADJUNCT PROCEEDINGS OF THE 2019 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT 2019), P372, DOI 10.1109/ISMAR-Adjunct.2019.000-9
   Overmeyer L, 2023, CIRP ANN-MANUF TECHN, V72, P409, DOI 10.1016/j.cirp.2023.03.010
   Pala P, 2021, TRANSPORT RES F-TRAF, V82, P15, DOI 10.1016/j.trf.2021.07.016
   Palffy A, 2023, IEEE T INTELL VEHICL, V8, P1459, DOI 10.1109/TIV.2022.3220435
   Pijnenburg J, 2017, Naturalism: Effects of an intuitive augmented reality interface property in the display of automated driving status
   Rameau F, 2016, IEEE T VIS COMPUT GR, V22, P2395, DOI 10.1109/TVCG.2016.2593768
   Rauschnabel PA, 2022, COMPUT HUM BEHAV, V133, DOI 10.1016/j.chb.2022.107289
   Renner P, 2017, IEEE SYMP 3D USER, P186, DOI 10.1109/3DUI.2017.7893338
   Robertson CM, 2008, INT SYM MIX AUGMENT, P73, DOI 10.1109/ISMAR.2008.4637328
   Rudenko A, 2020, INT J ROBOT RES, V39, P895, DOI 10.1177/0278364920917446
   Samsung, 2015, The Safety Truck could revolutionize road safety
   Schankin A, 2017, ADJUNCT PROCEEDINGS OF THE 2017 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT), P25, DOI 10.1109/ISMAR-Adjunct.2017.24
   Schinke T., 2010, Proceedings of the 12th International Conference on Human Computer Interaction with Mobile Devices and Services, MobileHCI '10, P313, DOI DOI 10.1145/1851600.1851655
   Schmitz A, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P63, DOI [10.1109/VR46266.2020.00-80, 10.1109/VR46266.2020.1581102716289]
   Smith M, 2021, APPL ERGON, V96, DOI 10.1016/j.apergo.2021.103510
   SWOV, 2020, Pedestrians Fact sheet
   Tabone W, 2023, Immersive insights: Evaluating augmented reality interfaces for pedestrians in a CAVE-based experiment preprint
   Tabone W, 2021, AUTOMOTIVEUI '21: 13TH INTERNATIONAL ACM CONFERENCE ON AUTOMOTIVE USER INTERFACES AND INTERACTIVE VEHICULAR APPLICATIONS, P209, DOI 10.1145/3409118.3475149
   VanderLaan JD, 1997, TRANSPORT RES C-EMER, V5, P1, DOI 10.1016/S0968-090X(96)00025-3
   Waldin N, 2017, COMPUT GRAPH FORUM, V36, P467, DOI 10.1111/cgf.13141
   Walter Matthias, 2019, Proceedings of the 20th Congress of the International Ergonomics Association (IEA 2018). Volume VI: Transport Ergonomics and Human Factors (TEHF), Aerospace Human Factors and Ergonomics. Advances in Intelligent Systems and Computing (AISC 823), P353, DOI 10.1007/978-3-319-96074-6_38
   Wickens CD, 1995, HUM FACTORS, V37, P473, DOI 10.1518/001872095779049408
   Wickens C, 2021, INT J HUM-COMPUT INT, V37, P403, DOI 10.1080/10447318.2021.1874741
   Wiesner CA, 2019, Increasing the maturity of the Augmented Reality Head-Up-Display
   Wilmott JP, 2022, INT SYM MIX AUGMENT, P470, DOI 10.1109/ISMAR55827.2022.00063
   Won M, 2020, IEEE ACCESS, V8, P49657, DOI 10.1109/ACCESS.2020.2980085
   World Health Organization, 2023, Pedestrian Safety: A Road Safety Manual for Decision-Makers and Practitioners, V2nd
   Yasuda H, 2012, INT SYM MIX AUGMENT, P333, DOI 10.1109/ISMAR.2012.6402600
   Yue LSS, 2020, J SAFETY RES, V73, P119, DOI 10.1016/j.jsr.2020.02.020
   Zadeh RB, 2018, IEEE T INTELL TRANSP, V19, P2086, DOI 10.1109/TITS.2017.2743709
   Zhang B, 2018, ADV INTELL SYST, V597, P842, DOI 10.1007/978-3-319-60441-1_81
   Zhao Y, 2023, IEEE T VIS COMPUT GR, V29, P2710, DOI 10.1109/TVCG.2023.3247078
NR 80
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAY 27
PY 2024
VL 28
IS 2
AR 119
DI 10.1007/s10055-024-01017-9
PG 19
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA SE7T7
UT WOS:001232854500001
OA hybrid
DA 2024-08-05
ER

PT J
AU Rhee, W
   Kim, YG
   Lee, JH
   Shim, JW
   Kim, BS
   Yoon, D
   Cho, MW
   Kim, S
AF Rhee, Wounsuk
   Kim, Young Gyun
   Lee, Jong Hyeon
   Shim, Jae Woo
   Kim, Byeong Soo
   Yoon, Dan
   Cho, Minwoo
   Kim, Sungwan
TI Unconstrained lightweight control interface for robot-assisted minimally
   invasive surgery using MediaPipe framework and head-mounted display
SO VIRTUAL REALITY
LA English
DT Article
DE Computer vision; Ergonomics; Real-time hand tracking; Robotic surgery;
   User experience study; Virtual reality
ID FUNDAMENTALS; REALITY; PERFORMANCE; LAPAROSCOPY; DISCOMFORT; PROGRAM
AB Robotic surgery is preferred over open or laparoscopic surgeries due to its intuitiveness and convenience. However, prolonged use of surgical robots can cause neck pain and joint fatigue in wrist and fingers. Also, input systems are bulky and difficult to maintain. To resolve these issues, we propose a novel input module based on real-time 3D hand tracking driven by RGB images and MediaPipe framework to control surgical robots such as patient side manipulator (PSM) and endoscopic camera manipulator (ECM) of da Vinci research kit. In this paper, we explore the mathematical basis of the proposed 3D hand tracking module and provide a proof-of-concept through user experience (UX) studies conducted in a virtual environment. End-to-end latencies for controlling PSM and ECM were 170 +/- 10 ms and 270 +/- 10 ms, respectively. Of fifteen novice participants recruited for the UX study, thirteen managed to reach a qualifiable level of proficiency after 50 min of practice and fatigue of hand and wrist were imperceivable. Therefore, we concluded that we have successfully developed a robust 3D hand tracking module for surgical robot control and in the future, it would hopefully reduce hardware cost and volume as well as resolve ergonomic problems. Furthermore, RGB image driven 3D hand tracking module developed in our study can be widely applicable to diverse fields such as extended reality (XR) development and remote robot control. In addition, we provide a new standard for evaluating novel input modalities of XR environments from a UX perspective.
C1 [Rhee, Wounsuk] Seoul Natl Univ, Coll Med, Dept Pharmacol, Daehak Ro 103, Seoul 03080, South Korea.
   [Kim, Young Gyun; Lee, Jong Hyeon; Shim, Jae Woo; Kim, Byeong Soo; Yoon, Dan] Seoul Natl Univ, Grad Sch, Interdisciplinary Program Bioengn, 1 Gwanak Ro, Seoul 08826, South Korea.
   [Cho, Minwoo] Seoul Natl Univ Hosp, Dept Transdisciplinary Med, 101 Daehak Ro, Seoul 03080, South Korea.
   [Kim, Sungwan] Seoul Natl Univ, Coll Med, Dept Biomed Engn, 103 Daehak ro, Seoul 03080, South Korea.
   [Kim, Sungwan] Seoul Natl Univ, Inst BioEngn, 1 Gwanak Ro, Seoul 08826, South Korea.
C3 Seoul National University (SNU); Seoul National University (SNU); Seoul
   National University (SNU); Seoul National University Hospital; Seoul
   National University (SNU); Seoul National University (SNU)
RP Cho, MW (corresponding author), Seoul Natl Univ Hosp, Dept Transdisciplinary Med, 101 Daehak Ro, Seoul 03080, South Korea.; Kim, S (corresponding author), Seoul Natl Univ, Coll Med, Dept Biomed Engn, 103 Daehak ro, Seoul 03080, South Korea.; Kim, S (corresponding author), Seoul Natl Univ, Inst BioEngn, 1 Gwanak Ro, Seoul 08826, South Korea.
EM chovis@snuh.org; sungwan@snu.ac.kr
OI Yoon, Dan/0000-0002-5657-5984; Rhee, Wounsuk/0000-0001-7770-5760; Kim,
   Sungwan/0000-0002-9318-849X; Kim, Byeong Soo/0000-0001-8767-9842; -Kim,
   Young Gyun/0000-0003-1231-9097
FU Korea Medical Device Development Fund grant - Korean government
   (Ministry of Science and ICT) [1711174462, RS-2020-KD000123]; Korea
   Medical Device Development Fund grant - Korean government (Ministry of
   Trade, Industry and Energy) [1711174462, RS-2020-KD000123]; Korea
   Medical Device Development Fund grant - Korean government (Ministry of
   Health Welfare) [1711174462, RS-2020-KD000123]; Korea Medical Device
   Development Fund grant - Korean government (Ministry of Food and Drug
   Safety) [1711174462, RS-2020-KD000123]; Interdisciplinary Research
   Initiatives Program from College of Engineering and College of Medicine,
   Seoul National University [800-20230385]; SNUH Research Fund
   [1920230020]
FX This work was supported by the Korea Medical Device Development Fund
   grant funded by the Korean government (the Ministry of Science and ICT,
   the Ministry of Trade, Industry and Energy, the Ministry of Health &
   Welfare, the Ministry of Food and Drug Safety) (Project Number:
   1711174462, RS-2020-KD000123), the Interdisciplinary Research
   Initiatives Program from College of Engineering and College of Medicine,
   Seoul National University (grant no. 800-20230385), and grant No.
   1920230020 from the SNUH Research Fund.
CR Amprimo G, 2023, Arxiv, DOI [arXiv:2308.01088, 10.48550/arXiv.2308.01088, DOI 10.48550/ARXIV.2308.01088]
   Amrani MZ, 2022, BIOMED SIGNAL PROCES, V72, DOI 10.1016/j.bspc.2021.103368
   Anvari Mehran, 2005, Comput Aided Surg, V10, P93, DOI 10.3109/10929080500228654
   Badalato GM, 2014, JSLS-J SOC LAPAROEND, V18, DOI 10.4293/JSLS.2014.00399
   Bangor A, 2008, INT J HUM-COMPUT INT, V24, P574, DOI 10.1080/10447310802205776
   Bangor A, 2009, J USABILITY STUD, V4, P114
   Barbash GI, 2010, NEW ENGL J MED, V363, P701, DOI 10.1056/NEJMp1006602
   BERGGREN U, 1994, BRIT J SURG, V81, P1362, DOI 10.1002/bjs.1800810936
   Brooke J., 1996, SUS-a quick and dirty usability scale, DOI [DOI 10.1201/9781498710411-35, DOI 10.1201/9781498710411]
   Buckingham G, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.728461
   Cagiltay NE, 2019, INT J HUM-COMPUT INT, V35, P1396, DOI 10.1080/10447318.2018.1533151
   Chen ZH, 2017, 2017 FIRST IEEE INTERNATIONAL CONFERENCE ON ROBOTIC COMPUTING (IRC), P180, DOI 10.1109/IRC.2017.69
   Chihara T, 2018, APPL ERGON, V68, P204, DOI 10.1016/j.apergo.2017.11.016
   Codd-Downey R, 2014, 2014 IEEE INTERNATIONAL CONFERENCE ON INFORMATION AND AUTOMATION (ICIA), P932, DOI 10.1109/ICInfA.2014.6932785
   Crippa J, 2021, ANN SURG, V274, pE1218, DOI 10.1097/SLA.0000000000003805
   Danioni A, 2022, IEEE ROBOT AUTOM MAG, V29, P68, DOI 10.1109/MRA.2022.3141972
   Dardona T, 2019, ROBOTICS, V8, DOI 10.3390/robotics8020031
   DiMaio S, 2011, SURGICAL ROBOTICS: SYSTEMS APPLICATIONS AND VISIONS, P199, DOI 10.1007/978-1-4419-1126-1_9
   Douissard J., 2019, The Da Vinci surgical system. Bariatric robotic surgery: a comprehensive guide
   Ficarra V, 2007, EUR UROL, V51, P45, DOI 10.1016/j.eururo.2006.06.017
   Fuchs KH, 2002, ENDOSCOPY, V34, P154, DOI 10.1055/s-2002-19857
   Gala RB, 2014, J MINIM INVAS GYN, V21, P353, DOI 10.1016/j.jmig.2013.11.010
   Gemmill EH, 2007, BRIT J SURG, V94, P1461, DOI 10.1002/bjs.6015
   Han SC, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392452
   Hong MS, 2021, VIRTUAL REAL-LONDON, V25, P491, DOI 10.1007/s10055-020-00469-z
   Hong N, 2019, MED BIOL ENG COMPUT, V57, P601, DOI 10.1007/s11517-018-1902-4
   HTC VIVE, 2022, About us
   Hussein A, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON VEHICULAR ELECTRONICS AND SAFETY (ICVES 2018)
   Ito K, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11156802
   Jo Y, 2020, INT J CONTROL AUTOM, V18, P150, DOI 10.1007/s12555-019-0244-9
   Kavana KM, 2022, Int Res J Moderniz Eng Tech Sci, V4
   Kazanzides P, 2014, IEEE INT CONF ROBOT, P6434, DOI 10.1109/ICRA.2014.6907809
   Kim YG, 2022, INT J CONTROL AUTOM, V20, P2959, DOI 10.1007/s12555-021-1044-6
   Knight JF, 2007, HUM FACTORS, V49, P797, DOI 10.1518/001872007X230172
   Knight JF, 2004, AVIAT SPACE ENVIR MD, V75, P123
   Koehn JK, 2015, SURG ENDOSC, V29, P2970, DOI 10.1007/s00464-014-4030-8
   Lanfranco AR, 2004, ANN SURG, V239, P14, DOI 10.1097/01.sla.0000103020.19595.7d
   Lareyre F, 2021, ANN VASC SURG, V75, P497, DOI 10.1016/j.avsg.2021.02.033
   Lawson EH, 2007, J ROBOT SURG, V1, P61, DOI 10.1007/s11701-007-0016-z
   Lee GI, 2017, SURG ENDOSC, V31, P1697, DOI 10.1007/s00464-016-5160-y
   Lewis JR, 2018, INT J HUM-COMPUT INT, V34, P577, DOI 10.1080/10447318.2018.1455307
   Lugaresi C, 2019, Arxiv, DOI arXiv:1906.08172
   Lum MJH, 2009, IEEE ENG MED BIO, P6860, DOI 10.1109/IEMBS.2009.5333120
   Lux MM, 2010, J ENDOUROL, V24, P371, DOI 10.1089/end.2009.0197
   Lynch KM., 2017, Modern robotics, V1st edition
   Mack MJ, 2001, JAMA-J AM MED ASSOC, V285, P568, DOI 10.1001/jama.285.5.568
   Martín-Barrio A, 2020, VIRTUAL REAL-LONDON, V24, P541, DOI 10.1007/s10055-019-00414-9
   Martnez-Pérez A, 2017, JAMA SURG, V152, DOI 10.1001/jamasurg.2016.5665
   Mills JT, 2013, J UROLOGY, V190, P580, DOI 10.1016/j.juro.2013.02.3185
   Miura S, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44939-0
   Moreira AH, 2014, 2014 IEEE SEGAH, P1, DOI [10.1109/SeGAH.2014.7067086, DOI 10.1109/SEGAH.2014.7067086]
   Munafo J, 2017, EXP BRAIN RES, V235, P889, DOI 10.1007/s00221-016-4846-7
   Nezhat F, 2008, GYNECOL ONCOL, V111, pS29, DOI 10.1016/j.ygyno.2008.07.025
   Nikitin Alexandr, 2020, Procedia Computer Science, V176, P622, DOI 10.1016/j.procs.2020.08.064
   Nouralizadeh A, 2018, J LAPAROENDOSC ADV S, V28, P656, DOI 10.1089/lap.2017.0662
   Palumbo A, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22207709
   Peters JH, 2004, SURGERY, V135, P21, DOI 10.1016/S0039-6060(03)00156-9
   Qian L, 2019, Hamlyn Symp Med Robot, DOI [10.31256/HSMR2019.47, DOI 10.31256/HSMR2019.47]
   Qian L, 2018, HEALTHC TECHNOL LETT, V5, P194, DOI 10.1049/htl.2018.5065
   Ritter EM, 2007, SURG INNOV, V14, P107, DOI 10.1177/1553350607302329
   Sroka G, 2010, AM J SURG, V199, P115, DOI 10.1016/j.amjsurg.2009.07.035
   Suarez J., 2012, 2012 RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication, P411, DOI 10.1109/ROMAN.2012.6343787
   Tavakoli M., 2006, VIRTUAL REAL-LONDON, V9, P160, DOI DOI 10.1007/S10055-005-0017-Z
   Townsend C.M., 2016, Sabiston Textbook of Surgery E-Book: The Biological Basis of Modern Surgical Practice
   Turchetti G, 2012, SURG ENDOSC, V26, P598, DOI 10.1007/s00464-011-1936-2
   van der Meijden OAJ, 2009, SURG ENDOSC, V23, P1180, DOI 10.1007/s00464-008-0298-x
   van't Hullenaar CDP, 2017, INNOV SURG SCI, V2, P97, DOI 10.1515/iss-2017-0007
   VanderLaan JD, 1997, TRANSPORT RES C-EMER, V5, P1, DOI 10.1016/S0968-090X(96)00025-3
   Wee IJY, 2020, INT J MED ROBOT COMP, V16, DOI 10.1002/rcs.2113
   Xu S, 2014, SURG ENDOSC, V28, P2569, DOI 10.1007/s00464-014-3504-z
   Yanwen Sun, 2019, 2019 IEEE International Conference on Robotics and Biomimetics (ROBIO), P1234, DOI 10.1109/ROBIO49542.2019.8961587
   Yasen M, 2019, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.218
   Zendejas B, 2016, SURG ENDOSC, V30, P512, DOI 10.1007/s00464-015-4233-7
   Zhang F, 2020, Arxiv, DOI [arXiv:2006.10214, DOI 10.48550/ARXIV.2006.10214]
NR 74
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAY 23
PY 2024
VL 28
IS 2
AR 114
DI 10.1007/s10055-024-00986-1
PG 18
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA RU7W5
UT WOS:001230249900002
OA hybrid
DA 2024-08-05
ER

PT J
AU Mittelstaedt, J
   Huelmann, G
   Marggraf-Micheel, C
   Schiller, A
   Seehof, C
   Stelling, D
AF Mittelstaedt, Justin
   Huelmann, Gerrit
   Marggraf-Micheel, Claudia
   Schiller, Alexander
   Seehof, Carsten
   Stelling, Dirk
TI Cybersickness with passenger VR in the aircraft: Influence of turbulence
   and VR content
SO VIRTUAL REALITY
LA English
DT Article
DE Cybersickness; Airplanes; Aircraft cabins; Moving vehicles; Virtual
   reality
ID MOTION SICKNESS; VIRTUAL-REALITY; SUSCEPTIBILITY; PREDICTORS
AB Using VR in the airplane cabin is appealing, primarily because of the enhanced entertainment value, increased privacy, and improved recreational opportunities provided by higher levels of immersion. However, VR applications in aircrafts contain the risk of passengers developing cybersickness. The particular environment of a moving aircraft in interaction with visual representation of movements in VR could lead to severe cybersickness, especially during turbulence. We had 129 participants experience VR in a full flight simulator with different content (static or dynamic VR clips) and during varying phases of flight including turbulence. The employed simulator is equipped with a cabin module, creating an economically valid environment. VR induced significant but mild symptoms of cybersickness. Nausea and dizziness symptoms were most severe during turbulence and especially with dynamic VR content being presented. More anxious participants tended to report more symptoms. In addition, there was an association with video game use and attitudes toward new technologies. While mild content and short exposure times only led to fairly low expressions of cybersickness, a long-term use of VR under turbulence could possibly become a concern. Airlines should especially address passengers' negative attitudes toward new technologies, and VR in particular, to reduce fears and the risk of low tolerability.
C1 [Mittelstaedt, Justin; Huelmann, Gerrit; Marggraf-Micheel, Claudia; Stelling, Dirk] German Aerosp Ctr DLR, Inst Aerosp Med, Sportallee 54A, D-22335 Hamburg, Germany.
   [Schiller, Alexander; Seehof, Carsten] German Aerosp Ctr DLR, Inst Flight Syst, Lilienthalpl 7, D-38108 Braunschweig, Germany.
C3 Helmholtz Association; German Aerospace Centre (DLR); Helmholtz
   Association; German Aerospace Centre (DLR)
RP Mittelstaedt, J (corresponding author), German Aerosp Ctr DLR, Inst Aerosp Med, Sportallee 54A, D-22335 Hamburg, Germany.
EM justin.mittelstaedt@dlr.de
OI Mittelstaedt, Justin/0000-0002-8419-6842
FU Deutsches Zentrum fr Luft- und Raumfahrt e.V. (DLR) (4202)
FX No Statement Available
CR Arns LL, 2005, P IEEE VIRT REAL ANN, P267
   Bajorunaite L, 2022, EXTENDED ABSTRACTS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2022, DOI 10.1145/3491101.3519696
   Bajorunaite L, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P432, DOI 10.1109/VRW52623.2021.00098
   Balcombe K, 2009, J AIR TRANSP MANAG, V15, P221, DOI 10.1016/j.jairtraman.2008.12.005
   Bijveld MMC, 2008, AVIAT SPACE ENVIR MD, V79, P661, DOI 10.3357/ASEM.2241.2008
   Bimberg P, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P464, DOI [10.1109/VRW50115.2020.00098, 10.1109/VRW50115.2020.0-178]
   Bos JE, 2008, DISPLAYS, V29, P47, DOI 10.1016/j.displa.2007.09.002
   Chang E, 2020, INT J HUM-COMPUT INT, V36, P1658, DOI 10.1080/10447318.2020.1778351
   Cho H-j, 2020, RoadVR: Mitigating the effect of vection and sickness by distortion of pathways for In-Car virtual reality VRST '20, DOI [10.1145/3385956.3422115, DOI 10.1145/3385956.3422115]
   Cho HJ, 2022, IEEE ACCESS, V10, P34003, DOI 10.1109/ACCESS.2022.3162221
   Duzmanska N, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.02132
   FOX S, 1988, AVIAT SPACE ENVIR MD, V59, P728
   Gage Stacey., 2003, AIAA Modeling and simulation technologies conference and exhibit, P5529, DOI 10.2514/6.2003-5529
   Golding JF, 2006, PERS INDIV DIFFER, V41, P237, DOI 10.1016/j.paid.2006.01.012
   Golding JF, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.576871
   Golding JF, 2017, AEROSP MED HUM PERF, V88, P3, DOI 10.3357/AMHP.4705.2017
   Hock P, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P4034, DOI 10.1145/3025453.3025665
   Hugon-Duprat C, 2015, J AIR TRANSP MANAG, V47, P11, DOI 10.1016/j.jairtraman.2015.03.005
   Jasper A, 2023, COMPUT HUM BEHAV, V146, DOI 10.1016/j.chb.2023.107800
   Karrer K., 2009, MENSCH MITTELPUNKT T, P196
   Kelly JW, 2023, IEEE C VIRT REAL 3D, DOI [10.1109/VRW58643.2023.00067, DOI 10.1109/VRW58643.2023.00067]
   Kennedy R.S., 1993, Int. J. Aviat. Psy, P203
   Klosterhalfen Sibylle, 2006, Gend Med, V3, P236, DOI 10.1016/S1550-8579(06)80211-1
   LEDERER L G, 1954, Int Rec Med Gen Pract Clin, V167, P661
   Lewis L, 2014, Investigating the ways in which virtal environments could influence aircraft passengers' comfort and experiences
   McCauley M. E., 1992, Teleoperators and Virtual Environments, V1, P311, DOI [10.1162/pres.1992.1.3.311, DOI 10.1162/PRES.1992.1.3.311]
   McCrae R., 2007, Journal of Individual Differences, V28, P116, DOI [10.1027/1614-0001.28.3.116, DOI 10.1027/1614-0001.28.3.116]
   McGill M., 2022, User experience design in the era of automated driving studies in Computational Intelligence
   McGill M., 2019, AUTOMOTIVE UI 19 ADJ, P434, DOI [DOI 10.1145/3349263.3351330, 10.1145/3349263.3351330]
   McGill M, 2022, PROCEEDINGS OF THE 35TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, UIST 2022, DOI 10.1145/3526113.3545657
   McGill M, 2020, VIRTUAL REAL-LONDON, V24, P583, DOI 10.1007/s10055-019-00420-x
   McGill M, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P5655, DOI 10.1145/3025453.3026046
   Medeiros D, 2022, IEEE T VIS COMPUT GR, V28, P3640, DOI 10.1109/TVCG.2022.3203002
   Mittelstaedt J, 2018, DISPLAYS, V51, P43, DOI 10.1016/j.displa.2018.01.002
   Mittelstaedt JM, 2020, J VESTIBUL RES-EQUIL, V30, P165, DOI 10.3233/VES-200702
   Mittelstädt JM, 2019, HUM FACTORS, V61, P322, DOI 10.1177/0018720818804382
   Mukhaimar A, 2023, 2023 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS, VRW, P593, DOI 10.1109/VRW58643.2023.00139
   Munafo J, 2017, EXP BRAIN RES, V235, P889, DOI 10.1007/s00221-016-4846-7
   Murata A, 2004, INT J HUM-COMPUT INT, V17, P463, DOI 10.1207/s15327590ijhc1704_2
   Ng AKT, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P519, DOI 10.1109/VRW55335.2022.00118
   Onag G, 2021, FutureIoT
   Pohlmann KMT, 2023, 2023 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS, VRW, P277, DOI 10.1109/VRW58643.2023.00066
   Pot-Kolder R, 2018, CYBERPSYCH BEH SOC N, V21, P187, DOI 10.1089/cyber.2017.0082
   Qiu Z, 2023, AUTOMOTIVEUI 23 15 I, DOI [10.1145/3580585.3607157, DOI 10.1145/3580585.3607157]
   Rahimzadeh G, 2023, NUTRIENTS, V15, DOI 10.3390/nu15061320
   Reason J. T., 1975, Motion Sickness
   Rosa PJ, 2016, CYBERPSYCH BEH SOC N, V19, P209, DOI 10.1089/cyber.2015.0130
   So RHY, 2001, HUM FACTORS, V43, P452, DOI 10.1518/001872001775898223
   Solimini AG, 2012, BMC PUBLIC HEALTH, V12, DOI 10.1186/1471-2458-12-779
   Soyka F, 2015, P IEEE VIRT REAL ANN, P33, DOI 10.1109/VR.2015.7223321
   Spielberger C D., 1970, The state-trait anxiety inventory (test manual), P22
   Stanney KM, 1997, PROCEEDINGS OF THE HUMAN FACTORS AND ERGONOMICS SOCIETY 41ST ANNUAL MEETING, 1997, VOLS 1 AND 2, P1138, DOI 10.1177/107118139704100292
   Stelling D, 2021, ERGONOMICS, V64, P1062, DOI 10.1080/00140139.2021.1886334
   Stutzer A, 2008, SCAND J ECON, V110, P339, DOI 10.1111/j.1467-9442.2008.00542.x
   Sullivan MJL, 1995, PSYCHOL ASSESSMENT, V7, P524, DOI 10.1037/1040-3590.7.4.524
   Turner M, 2000, AVIAT SPACE ENVIR MD, V71, P1181
   Wienrich C., 2017, Pilotstudie: Einsatz Von mobilen VR-Anwendungen in gleichmaβig ruhig bewegten transportsystemen. Mensch Und Computer 2017, DOI [10.18420/muc2017-ws09-0374, DOI 10.18420/MUC2017-WS09-0374]
   Williamson JR, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300310
NR 58
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAY 20
PY 2024
VL 28
IS 2
AR 112
DI 10.1007/s10055-024-01008-w
PG 16
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA RK5H6
UT WOS:001227566400002
OA hybrid
DA 2024-08-05
ER

PT J
AU Rodriguez, D
   Borrego, A
   Guzman, M
   Llorens, R
AF Rodriguez, Daniel
   Borrego, Adrian
   Guzman, Marco
   Llorens, Roberto
TI Acoustic characteristics of voice production in virtual
   reality-simulated and physical environments: a comparative study in
   university professors
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Voice assessment; Acoustic analysis; Ecological
   validity
ID AVERAGE SPECTRUM; HANDICAP INDEX; TEACHERS; DISORDERS; WORK; PREVALENCE;
   VALIDATION; HEALTHY
AB This study investigated the reliability of a virtual reality-simulated classroom to generate a comparable self-perception of voice quality and acoustic effects of phonation to a real classroom in a group of teachers, and sense of presence. Thirty university professors participated in the study and were required to produce loud connected speech by reading a 100-word text in two conditions: (1) in a real classroom including a group of students, and (2) in a virtual replica of the classroom consisting of a 360-degree video of the same classroom and students, which was displayed using a head mounted display. Ambient noise was controlled in both conditions by playing classroom noise through headphones. The self-perception of voice quality, the long-term average spectrum and smooth cepstral peak prominence were estimated in both conditions. The sense of presence generated by virtual reality was measured after interacting with the virtual classroom. There were no statistically significant differences in the self-perception of voice quality or in the acoustic measures of voice production between conditions. The sense of presence in the virtual classroom was high. Our findings suggest that a virtual reality-simulated classroom generate comparable self-perception of voice quality and acoustic effects of phonation to the real classroom, and a high sense of presence, in a group of teachers. Additionally, it is important to highlight the potential of virtual reality to enhance the ecological validity of acoustic assessment of voice production in laboratories and clinical settings.
C1 [Rodriguez, Daniel] Univ Catolica Temuco, Dept Therapeut Proc, Campus San Francisco Temuco,Manuel Montt 56, Temuco, Araucania, Chile.
   [Rodriguez, Daniel; Borrego, Adrian; Llorens, Roberto] Univ Politecn Valencia, Inst Human Ctr Technol Res, Neurorehabil & Brain Res Grp, Valencia, Spain.
   [Guzman, Marco] Univ Andes, Dept Commun Sci & Disorders, Santiago, Chile.
C3 Universidad Catolica de Temuco; Universitat Politecnica de Valencia;
   Universidad de los Andes - Chile
RP Llorens, R (corresponding author), Univ Politecn Valencia, Inst Human Ctr Technol Res, Neurorehabil & Brain Res Grp, Valencia, Spain.
EM drodriguez@uct.cl; rllorens@htech.upv.es
FU Investigadores Emergentes 2020
FX No Statement Available
CR Abo-Hasseba A, 2017, J VOICE, V31, DOI 10.1016/j.jvoice.2016.10.016
   Behlau M, 2014, CURR OPIN OTOLARYNGO, V22, P188, DOI 10.1097/MOO.0000000000000047
   Boersma P, 2022, Praat
   Bolbol SA, 2017, J VOICE, V31, DOI 10.1016/j.jvoice.2016.07.010
   Bottalico P, 2018, J ACOUST SOC AM, V143, P1591, DOI 10.1121/1.5027816
   Brassel S, 2023, AM J SPEECH-LANG PAT, V32, P907, DOI 10.1044/2022_AJSLP-22-00077
   Brockmann-Bauser M, 2021, J VOICE, V35, P411, DOI 10.1016/j.jvoice.2019.11.015
   Bryant L, 2024, DISABIL REHABIL-ASSI, V19, P90, DOI 10.1080/17483107.2022.2063423
   Bryant L, 2020, DISABIL REHABIL-ASSI, V15, P365, DOI 10.1080/17483107.2018.1549276
   Castillo Adrián, 2015, Cienc Trab., V17, P15
   Castillo-Allendes A, 2023, LARYNGOSCOPE, V133, P1676, DOI 10.1002/lary.30398
   Chang A, 2004, J VOICE, V18, P454, DOI 10.1016/j.jvoice.2004.01.004
   Cruz T, 2020, EUR J PUBLIC HEALTH, V30
   Cutiva LCC, 2013, J COMMUN DISORD, V46, P143, DOI 10.1016/j.jcomdis.2013.01.001
   da Rocha LM, 2013, J VOICE, V27, P595, DOI 10.1016/j.jvoice.2012.10.001
   Dasdogen Umit, 2023, J Voice, DOI 10.1016/j.jvoice.2023.07.026
   Deary IJ, 2003, J PSYCHOSOM RES, V54, P483, DOI 10.1016/S0022-3999(02)00469-5
   Durup N., 2017, J Acoust Soc Am, V141, P3540, DOI [10.1121/1.4987482, DOI 10.1121/1.4987482]
   Ferdig RE, 2020, TECHTRENDS, V64, P849, DOI 10.1007/s11528-020-00522-3
   Franzen M. D., 1996, Ecological validity of neuropsychological testing, P91
   Frisancho K, 2020, J VOICE, V34, P398, DOI 10.1016/j.jvoice.2018.10.004
   Gandolfi E, 2021, BRIT J EDUC TECHNOL, V52, P824, DOI 10.1111/bjet.13058
   Gonzalez-Gamboa Mauricio, 2022, J Voice, DOI 10.1016/j.jvoice.2022.10.023
   Guzman M, 2017, J VOICE, V31, DOI 10.1016/j.jvoice.2016.09.005
   Guzman M, 2013, J VOICE, V27, DOI 10.1016/j.jvoice.2012.08.008
   HAMMARBERG B, 1980, ACTA OTO-LARYNGOL, V90, P441, DOI 10.3109/00016488009131746
   Hanschmann H, 2011, FOLIA PHONIATR LOGO, V63, P83, DOI 10.1159/000316140
   Hogikyan ND, 1999, J VOICE, V13, P557, DOI 10.1016/S0892-1997(99)80010-1
   Hummel C, 2010, FOLIA PHONIATR LOGO, V62, P303, DOI 10.1159/000287715
   Hunter EJ, 2020, J SPEECH LANG HEAR R, V63, P509, DOI 10.1044/2019_JSLHR-19-00057
   i Badia Sergi Bermudez, 2016, Neurorehabilitation Technology, P573, DOI [10.1007/978-3-319-28603-7_28, DOI 10.1007/978-3-319-28603-7_28]
   Ilomaki Irma, 2005, Logoped Phoniatr Vocol, V30, P171, DOI 10.1080/14015430500294106
   Jacobson BH, 1997, American Journal of Speech-Language Pathology, V6, P66, DOI DOI 10.1044/1058-0360.0603.66
   Kankare E, 2012, FOLIA PHONIATR LOGO, V64, P12, DOI 10.1159/000328643
   KITZING P, 1986, J PHONETICS, V14, P477, DOI 10.1016/S0095-4470(19)30693-X
   Laukkanen A, 2001, P 25 WORLD C INT ASS
   Laukkanen AM, 2008, J VOICE, V22, P283, DOI 10.1016/j.jvoice.2006.10.001
   Laukkanen AM, 2006, FOLIA PHONIATR LOGO, V58, P229, DOI 10.1159/000093180
   Laukkanen Anne-Maria, 2004, Logoped Phoniatr Vocol, V29, P66, DOI 10.1080/14015430410034479
   LOFQVIST A, 1987, FOLIA PHONIATR, V39, P221
   Lombard M., 1997, Journal of Computer-Mediated Communication, V3, DOI [10.1111/j.1083-6101.1997.tb0, 10.1111/j.1083-6101.1997.tb00072.x., 10.1111/j.1083-6101.1997.tb00072.x, DOI 10.1111/J.1083-6101.1997.TB00072.X, 10.1111/J.1083-6101.1997.TB00072.X/4080403]
   Martinez-Cifuentes R., 2020, Revista de Logopedia, Foniatria y Audiologia, V40, P77, DOI [10.1016/j.rlfa.2019.11.002, DOI 10.1016/J.RLFA.2019.11.002]
   Maryn Y, 2015, J VOICE, V29, DOI 10.1016/j.jvoice.2014.06.015
   Master S, 2008, J VOICE, V22, P146, DOI 10.1016/j.jvoice.2006.09.006
   Mattiske JA, 1998, J VOICE, V12, P489, DOI 10.1016/S0892-1997(98)80058-1
   Murton O, 2020, AM J SPEECH-LANG PAT, V29, P1596, DOI 10.1044/2020_AJSLP-20-00001
   Navarro MD, 2013, NEUROPSYCHOL REHABIL, V23, P597, DOI 10.1080/09602011.2013.806269
   Park Y, 2019, J SPEECH LANG HEAR R, V62, P1707, DOI 10.1044/2019_JSLHR-S-18-0507
   Parsons TD, 2015, FRONT HUM NEUROSCI, V9, DOI 10.3389/fnhum.2015.00660
   Phadke KV, 2020, J VOICE, V34, DOI [10.1016/j.jvoice.2018.09.610, 10.1016/j.jvoice.2018.09.010]
   Phadke KV, 2019, J VOICE, V33, DOI 10.1016/j.jvoice.2018.03.003
   Popolo PS, 2002, J Acoust Soc Am, V112, P2304
   Rantala L, 1998, FOLIA PHONIATR LOGO, V50, P205, DOI 10.1159/000021462
   Rantala LM, 2015, J SPEECH LANG HEAR R, V58, P1397, DOI 10.1044/2015_JSLHR-S-14-0248
   Reeves R, 2021, J ANXIETY DISORD, V83, DOI 10.1016/j.janxdis.2021.102451
   Remacle A, 2021, VIRTUAL REAL-LONDON, V25, P935, DOI 10.1007/s10055-020-00491-1
   Roy N, 2004, J SPEECH LANG HEAR R, V47, P281, DOI 10.1044/1092-4388(2004/023)
   Roy N, 2013, AM J SPEECH-LANG PAT, V22, P212, DOI 10.1044/1058-0360(2012/12-0014)
   Smith C, 2024, INT J LANG COMM DIS, V59, P976, DOI 10.1111/1460-6984.12968
   Södersten M, 2002, J VOICE, V16, P356, DOI 10.1016/S0892-1997(02)00107-8
   Takac M, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0216288
   Usoh M, 2000, PRESENCE-TELEOP VIRT, V9, P497, DOI 10.1162/105474600566989
   Vaezipour A, 2022, DISABIL REHABIL, V44, P3946, DOI 10.1080/09638288.2021.1895333
   Ventura S, 2022, VIRTUAL REAL-LONDON, V26, P323, DOI 10.1007/s10055-021-00567-6
   Vilkman E, 2000, FOLIA PHONIATR LOGO, V52, P120, DOI 10.1159/000021519
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   Woisard V, 2007, J VOICE, V21, P623, DOI 10.1016/j.jvoice.2006.04.005
   Zaki J, 2009, ANN NY ACAD SCI, V1167, P16, DOI 10.1111/j.1749-6632.2009.04601.x
NR 68
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR 31
PY 2024
VL 28
IS 2
AR 89
DI 10.1007/s10055-024-00967-4
PG 10
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA MQ5G4
UT WOS:001195100800001
OA Green Submitted, hybrid
DA 2024-08-05
ER

PT J
AU Alvarez, IM
   Manero, B
   Romero-Hernández, A
   Cárdenas, M
   Masó, I
AF Alvarez, Ibis M.
   Manero, Borja
   Romero-Hernandez, Alejandro
   Cardenas, Miriela
   Maso, Isabel
TI Virtual reality platform for teacher training on classroom climate
   management: evaluating user acceptance
SO VIRTUAL REALITY
LA English
DT Article
DE Classroom management; Secondary school; Initial teacher education;
   Immersive virtual reality; Usability evaluation
ID ENVIRONMENTS; EDUCATION
AB Enhancing the educational experience through Immersive Virtual Reality (IVR) is a promising avenue, elevating the authenticity and responsiveness of simulations. Particularly in educational settings, IVR holds the potential to augment accessibility and engagement in learning. However, one pivotal aspect lies in assessing the learners' acceptance of such environments to ensure optimal and effective utilization of these technologies. This paper delves into the Didascalia Virtual-ClassRoom usability testing -an immersive IVR environment tailored for pre-service secondary school teachers. The platform transports users into a simulated classroom, where they are invited to play the role of a teacher. During the simulation, three scenarios are recreated, reproducing disruptive behaviours commonly faced in real classrooms. 84 participants (28 teachers and 56 pre-service teachers) engaged in decision-making to manage the classroom climate influenced by the simulated situations. To collect data, we used a questionnaire based on the Technology Acceptance Model (TAM) to assess and gauge users' inclinations and attitudes towards embracing the technology in question. To gain deeper insights into the user experience, participants were further invited to participate in semi-structured interviews, offering reflections and suggestions for potential enhancements. The evaluation process encompassed the perceived usefulness of the Didascalia Virtual-ClassRoom, shedding light on factors that could either facilitate or impede the adoption of this platform to enhance classroom management competence. The participants' perspectives serve as a valuable foundation for refining the tool's functionality, and their feedback fuels recommendations for its seamless integration into initial teacher training programs.
C1 [Alvarez, Ibis M.; Cardenas, Miriela; Maso, Isabel] Univ Autonoma Barcelona, Dept Basic Evolut & Educ Psychol, Bellaterra, Spain.
   [Manero, Borja; Romero-Hernandez, Alejandro] Univ Complutense Madrid, Software Engn & Artificial Intelligence Dept, Madrid, Spain.
   [Alvarez, Ibis M.] Campus Univ Autonoma Barcelona, Fac Psicol, Carrer Fortuna,S-N, ES-08193 Bellaterra, Barcelona, Spain.
C3 Autonomous University of Barcelona; Complutense University of Madrid;
   Autonomous University of Barcelona
RP Alvarez, IM (corresponding author), Univ Autonoma Barcelona, Dept Basic Evolut & Educ Psychol, Bellaterra, Spain.; Alvarez, IM (corresponding author), Campus Univ Autonoma Barcelona, Fac Psicol, Carrer Fortuna,S-N, ES-08193 Bellaterra, Barcelona, Spain.
EM ibismarlene.alvarez@uab.es
RI Alvarez Valdivia, Ibis Marlene/E-6531-2014
OI Alvarez Valdivia, Ibis Marlene/0000-0002-3250-3214
FU Spanish Ministry of Science, Innovation and Universities
FX The researchers would like to thank the teachers and preservice teachers
   who voluntarily participated in this study.
CR Abd Majid F., 2019, Asian Journal of University Education, V15, P51, DOI DOI 10.24191/AJUE.V15I2.7556
   Ade-Ojo GO, 2022, J COMPUT ASSIST LEAR, V38, P861, DOI 10.1111/jcal.12653
   Alasmari NJ, 2021, LANG TEACH RES, DOI 10.1177/13621688211046351
   Alvarez IM, 2023, EDUC XX1, V26, P249, DOI 10.5944/educxx1.33418
   [Anonymous], 2011, Learning science through computer games and simulations
   [Anonymous], 2019, TALIS 2018 Results (Volume I)
   Barrett A, 2023, INTERACT LEARN ENVIR, V31, P1665, DOI 10.1080/10494820.2020.1855209
   Bevan N, 2015, LECT NOTES COMPUT SC, V9169, P143, DOI 10.1007/978-3-319-20901-2_13
   Billingsley G., 2019, J INTERACTIVE LEARNI, V30, P65
   Bocos-Corredor M, 2020, Classroom VR: a VR game to improve communication skills in secondary-school teachers
   Bower M, 2020, BRIT J EDUC TECHNOL, V51, P1981, DOI 10.1111/bjet.13038
   Burdea G. C., 2003, Virtual reality technology
   Sarceda-Gorgoso MC, 2020, PROFESORADO, V24, P401, DOI 10.30827/PROFESORADO.V24I3.8260
   Cheng KH, 2023, COMPUT EDUC, V201, DOI 10.1016/j.compedu.2023.104814
   Clarà M, 2023, TEACH TEACH EDUC, V130, DOI 10.1016/j.tate.2023.104166
   Creswell JW, 2012, Educational research planning, conducting and evaluating quantitative and qualitative research, DOI DOI 10.1017/CBO9781107415324.004
   DAVIS FD, 1989, MIS QUART, V13, P319, DOI 10.2307/249008
   Dawson MR, 2017, TEACH EDUC SPEC EDUC, V40, P26, DOI 10.1177/0888406416664184
   Dewey J., 1944, Democracy and education
   Dieker L.A., 2017, School University Partnerships (Journal of the National Association for Professional Development Schools): Special Issue: Technology to Enhance PDS, V10, P62
   Doyle W, 2006, HANDBOOK OF CLASSROOM MANAGEMENT: RESEARCH, PRACTICE, AND CONTEMPORARY ISSUES, P97
   Elmoazen R, 2023, SMART LEARN ENVIRON, V10, DOI 10.1186/s40561-023-00244-y
   Engel A, 2022, RIED-REV IBEROAM EDU, V25, P225, DOI 10.5944/ried.25.1.31489
   Evans D, 2019, TEACH TEACH EDUC, V82, P55, DOI 10.1016/j.tate.2019.03.008
   Evertson C., 2008, 21 CENTURY ED REFERE, P131, DOI 10.4135/9781412964012.n14
   Ferguson Sarah, 2022, Journal of Educational Technology Systems, V50, P432, DOI 10.1177/00472395211067731
   González-Mayorga H, 2023, AULA ABIERTA, V52, P71, DOI 10.17811/rifie.52.1.2023.71-80
   Hettinger K, 2021, TEACH TEACH EDUC, V103, DOI 10.1016/j.tate.2021.103349
   Holden H, 2011, J RES TECHNOL EDUC, V43, P343
   Huang HF, 2022, INTERACT LEARN ENVIR, V30, P848, DOI 10.1080/10494820.2019.1691605
   Huang HM, 2016, INTERACT LEARN ENVIR, V24, P3, DOI 10.1080/10494820.2013.817436
   Huang YZ, 2021, COMPUT EDUC, V163, DOI 10.1016/j.compedu.2020.104100
   Kari T, 2023, VIRTUAL REAL-LONDON, V27, P1585, DOI 10.1007/s10055-023-00749-4
   Keller MM, 2021, TEACH TEACH, V27, P404, DOI 10.1080/13540602.2020.1834380
   Kugurakova VV, 2023, ONLINE J COMMUN MEDI, V13, DOI 10.30935/ojcmt/13195
   Lewis JR, 2014, INT J HUM-COMPUT INT, V30, P663, DOI 10.1080/10447318.2014.930311
   Lugrin J-L., 2016, FRONTIERS ICT, V3, P26, DOI [10.3389/fict.2016.00026, DOI 10.3389/FICT.2016.00026]
   Maso I, 2022, Educational research and innovation facing the challenges for sustainable development
   McGarr O, 2021, J EDUC TEACHING, V47, P274, DOI 10.1080/02607476.2020.1733398
   Meyer J.H.F., 2003, Threshold concepts and troublesome knowledge: Linkages to ways of thinking and practising within the disciplines, DOI DOI 10.1007/978-3-8348-9837-1
   Mills Jane, 2006, Int J Nurs Pract, V12, P8, DOI 10.1111/j.1440-172X.2006.00543.x
   Mouw J.M., 2020, P INT C HIGH ED ADV, P325, DOI [10.4995/HEAd20.2020.11049, DOI 10.4995/HEAD20.2020.11049]
   Richter E, 2022, COMPUT EDUC, V190, DOI 10.1016/j.compedu.2022.104601
   Sandars J, 2010, EDUC PRIM CARE, V21, P6, DOI 10.1080/14739879.2010.11493869
   Schott C, 2018, J COMPUT ASSIST LEAR, V34, P843, DOI 10.1111/jcal.12293
   Seufert C, 2022, COMPUT EDUC, V179, DOI 10.1016/j.compedu.2021.104410
   Shank MK., 2022, Clearing House: A Journal of Educational Strategies, Issues and Ideas, V95, P26, DOI DOI 10.1080/00098655.2021.2010636
   Shernoff ES, 2020, ETR&D-EDUC TECH RES, V68, P3235, DOI 10.1007/s11423-020-09819-9
   Suh A, 2018, COMPUT HUM BEHAV, V86, P77, DOI 10.1016/j.chb.2018.04.019
   Sutton RE, 2009, THEOR PRACT, V48, P130, DOI 10.1080/00405840902776418
   Toda AM, 2019, SMART LEARN ENVIRON, V6, DOI 10.1186/s40561-019-0106-1
   Vlachogianni P, 2023, SUSTAINABILITY-BASEL, V15, DOI 10.3390/su151712954
   Wang MT, 2020, DEV REV, V57, DOI 10.1016/j.dr.2020.100912
   Winstone NE, 2017, EDUC PSYCHOL-US, V52, P17, DOI 10.1080/00461520.2016.1207538
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   Won M, 2023, COMPUT EDUC, V195, DOI 10.1016/j.compedu.2022.104701
NR 56
TC 0
Z9 0
U1 8
U2 8
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR 16
PY 2024
VL 28
IS 2
AR 78
DI 10.1007/s10055-024-00973-6
PG 17
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA LG6V4
UT WOS:001185680800001
OA hybrid
DA 2024-08-05
ER

PT J
AU Moosavi, MS
   Raimbaud, P
   Guillet, C
   Mérienne, F
AF Moosavi, Mahdiyeh Sadat
   Raimbaud, Pierre
   Guillet, Christophe
   Merienne, Frederic
TI Enhancing weight perception in virtual reality: an analysis of kinematic
   features
SO VIRTUAL REALITY
LA English
DT Article
DE Weight perception; Pseudo-haptic feedback; Multi-sensory conflict;
   Kinematic features; Virtual weight
ID HAPTIC FEEDBACK; ARM MOVEMENTS; LOAD; ENVIRONMENTS; DIRECTION; WRIST
AB This study investigates weight perception in virtual reality without kinesthetic feedback from the real world, by means of an illusory method called pseudo-haptic. This illusory model focuses on the dissociation of visual input and somatosensory feedback and tries to induce the sensation of virtual objects' loads in VR users by manipulating visual input. For that, modifications on the control-display ratio, i.e., between the real and virtual motions of the arm, can be used to produce a visual illusionary effect on the virtual objects' positions as well. Therefore, VR users perceive it as velocity variations in the objects' displacements, helping them achieve a better sensation of virtual weight. A primary contribution of this paper is the development of a novel, holistic assessment methodology that measures the sense of the presence in virtual reality contexts, particularly when participants are lifting virtual objects and experiencing their weight. Our study examined the effect of virtual object weight on the kinematic parameters and velocity profiles of participants' upward arm motions, along with a parallel experiment conducted using real weights. By comparing the lifting of real objects with that of virtual objects, it was possible to gain insight into the variations in kinematic features observed in participants' arm motions. Additionally, subjective measurements, utilizing the Borg CR10 questionnaire, were conducted to assess participants' perceptions of hand fatigue. The analysis of collected data, encompassing both subjective and objective measurements, concluded that participants experienced similar sensations of fatigue and changes in hand kinematics during both virtual object tasks, resulting from pseudo-haptic feedback, and real weight lifting tasks. This consistency in findings underscores the efficacy of pseudo-haptic feedback in simulating realistic weight sensations in virtual environments.
C1 [Moosavi, Mahdiyeh Sadat; Merienne, Frederic] LISPEN, Arts & Metiers Inst Technol, F-71100 Chalon Sur Saone, France.
   [Raimbaud, Pierre] Univ Lumiere Lyon 2, Univ Claude Bernard Lyon 1, Ecole Cent Lyon, CNRS,INSA Lyon,LIRIS,UMR5205,ENISE, St Etienne, France.
   [Guillet, Christophe] Univ Bourgogne, LISPEN, F-71100 Chalon Sur Saone, France.
C3 Centre National de la Recherche Scientifique (CNRS); Institut National
   des Sciences Appliquees de Lyon - INSA Lyon; Universite Claude Bernard
   Lyon 1; Ecole Centrale de Lyon; Universite de Bourgogne
RP Moosavi, MS (corresponding author), LISPEN, Arts & Metiers Inst Technol, F-71100 Chalon Sur Saone, France.
EM mahdiyehsadat.moosavi@ensam.eu; pierre.raimbaud@ec-lyon.fr;
   Christophe.Guillet@u-bourgogne.fr; frederic.merienne@ensam.eu
OI Raimbaud, Pierre/0000-0002-5584-8100; Moosavi, Mahdiyeh
   sadat/0009-0008-2030-2950
FU French government [ANR-21- ESRE-0030]
FX This work was supported by French government funding managed by the
   National Research Agency under the Investments for the Future program
   (PIA) grant ANR-21- ESRE-0030 (CONTINUUM). The authors want to thank
   Jean-Luc Martinez who proposed to use the C/D ratio as a weight metaphor
   and Jeremy Plouzeau who made the first implementation of this metaphor.
CR ABEND W, 1982, BRAIN, V105, P331, DOI 10.1093/brain/105.2.331
   Agronin ML, 1987, P WORKSHOP SPACE TEL, V2
   Aman J., 2010, Integration, V96, P98
   Araujo B, 2016, PROCEEDINGS OF THE TENTH ANNIVERSARY CONFERENCE ON TANGIBLE EMBEDDED AND EMBODIED INTERACTION (TEI16), P218, DOI 10.1145/2839462.2839484
   Argelaguet F, 2013, COMPUT GRAPH-UK, V37, P121, DOI 10.1016/j.cag.2012.12.003
   ATKESON CG, 1985, J NEUROSCI, V5, P2318
   Berret B, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000194
   BOCK O, 1990, BEHAV BRAIN RES, V41, P167, DOI 10.1016/0166-4328(90)90106-O
   BORG G, 1990, SCAND J WORK ENV HEA, V16, P55, DOI 10.5271/sjweh.1815
   Burdea GC, 1999, Haptic feedback for virtual reality, P17
   Choi I, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P119, DOI 10.1145/3126594.3126599
   Dominjon L, 2005, P IEEE VIRT REAL ANN, P19
   Ernst MO, 2002, NATURE, V415, P429, DOI 10.1038/415429a
   Gaveau J, 2014, J NEUROPHYSIOL, V111, P4, DOI 10.1152/jn.01029.2012
   Gentili R, 2007, NEUROSCIENCE, V145, P20, DOI 10.1016/j.neuroscience.2006.11.035
   Jáuregui DAG, 2014, IEEE T VIS COMPUT GR, V20, P654, DOI 10.1109/TVCG.2014.45
   Heo S, 2018, CHI 2018: EXTENDED ABSTRACTS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3170427.3186544
   Hirata Y, 1992, 3-dimensional interface device for virtual work space, P889
   HOFFMAN DS, 1993, J NEUROSCI, V13, P5212, DOI 10.1523/JNEUROSCI.13-12-05212.1993
   Kim J, 2022, IEEE ACCESS, V10, P5129, DOI 10.1109/ACCESS.2022.3140438
   Kumar N, 2023, Mechanics of pseudo-haptics in virtual reality: weight perception. s.l., P1093
   Lécuyer A, 2009, PRESENCE-TELEOP VIRT, V18, P39, DOI 10.1162/pres.18.1.39
   Mahmoud OH, 2023, HUM FACTORS, V65, P1381, DOI 10.1177/00187208211047640
   Massie T. H., 1994, P S HAPT INT VIRT EN, P295
   Moosavi MS, 2023, FRONT VIRTUAL REAL, V4, DOI 10.3389/frvir.2023.973083
   MORASSO P, 1981, EXP BRAIN RES, V42, P223
   Nakakoji K, 2010, Toward principles for visual interaction design for communicating weight by using pseudo-haptic feedback, P1
   Nakakoji K, 2011, Tciex: an environment for designing and experiencing a variety of visuo-haptic sensory conflicts, P23
   Nisar S, 2019, IEEE ROBOT AUTOM LET, V4, P351, DOI 10.1109/LRA.2018.2890198
   Pacchierotti C, 2017, IEEE T HAPTICS, V10, P580, DOI 10.1109/TOH.2017.2689006
   Palmerius KL, 2014, LECT NOTES COMPUT SC, V8618, P117, DOI 10.1007/978-3-662-44193-0_16
   Papaxanthis C, 1998, EXP BRAIN RES, V120, P496, DOI 10.1007/s002210050423
   Papaxanthis C, 1998, NEUROSCI LETT, V253, P103, DOI 10.1016/S0304-3940(98)00604-1
   Rietzler M, 2018, Breaking the tracking: enabling weight perception using perceivable tracking offsets, P1
   Rietzler M, 2018, Conveying the perception of kinesthetic feedback in virtual reality using state-of-the-art hardware, P1
   RUNESON S, 1981, J EXP PSYCHOL HUMAN, V7, P733, DOI 10.1037/0096-1523.7.4.733
   Samae M, 2019, BIOMED ENG INT CONF, DOI 10.1145/3290605.3300550
   Sciutti A, 2012, J NEUROPHYSIOL, V107, P3433, DOI 10.1152/jn.00420.2011
   Sciutti A, 2010, EXP BRAIN RES, V200, P259, DOI 10.1007/s00221-009-1996-x
   Suzuki Y, 2005, IEEE COMPUT GRAPH, V25, P44, DOI 10.1109/MCG.2005.1
   Tano S, 2015, Quantitative analysis of pseudo-haptics based on three types of hand form and two phases of perception, P1
   Ujitoko Y, 2021, IEEE T HAPTICS, V14, P699, DOI 10.1109/TOH.2021.3077619
   Weiss M, 2011, FingerFlux: near-surface haptic feedback on tabletops, P615
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   Yu R, 2020, IEEE T VIS COMPUT GR, V26, P2094, DOI 10.1109/TVCG.2020.2973056
   Zenner A, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300441
NR 46
TC 1
Z9 1
U1 1
U2 1
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR 8
PY 2024
VL 28
IS 2
AR 72
DI 10.1007/s10055-024-00948-7
PG 12
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA LG7F1
UT WOS:001185691200001
OA Green Published, hybrid
DA 2024-08-05
ER

PT J
AU Miguel-Alonso, I
   Checa, D
   Guillen-Sanz, H
   Bustillo, A
AF Miguel-Alonso, Ines
   Checa, David
   Guillen-Sanz, Henar
   Bustillo, Andres
TI Evaluation of the novelty effect in immersive Virtual Reality learning
   experiences
SO VIRTUAL REALITY
LA English
DT Article
DE Head-mounted display; Learning; Novelty effect; Serious games; Tutorial;
   Virtual Reality
AB In this study, the novelty effect or initial fascination with new technology is addressed in the context of an immersive Virtual Reality (iVR) experience. The novelty effect is a significant factor contributing to low learning outcomes during initial VR learning experiences. The aim of this research is to measure the effectiveness of a tutorial at mitigating the novelty effect of iVR learning environments among first-year undergraduate students. The iVR tutorial forms part of the iVR learning experience that involves the assembly of a personal computer, while learning the functions of the main components. 86 students participated in the study, divided into a Control group (without access to the tutorial) and a Treatment group (completing the tutorial). Both groups showed a clear bimodal distribution in previous knowledge, due to previous experience with learning topics, giving us an opportunity to compare tutorial effects with students of different backgrounds. Pre- and post-test questionnaires were used to evaluate the experience. The analysis included such factors as previous knowledge, usability, satisfaction, and learning outcomes categorized into remembering, understanding, and evaluation. The results demonstrated that the tutorial significantly increased overall satisfaction, reduced the learning time required for iVR mechanics, and improved levels of student understanding, and evaluation knowledge. Furthermore, the tutorial helped to homogenize group behavior, particularly benefiting students with less previous experience in the learning topic. However, it was noted that a small number of students still received low marks after the iVR experience, suggesting potential avenues for future research.
C1 [Miguel-Alonso, Ines; Checa, David; Guillen-Sanz, Henar; Bustillo, Andres] Univ Burgos, Dept Ingn Informat, Burgos, Spain.
C3 Universidad de Burgos
RP Bustillo, A (corresponding author), Univ Burgos, Dept Ingn Informat, Burgos, Spain.
EM imalonso@ubu.es; dcheca@ubu.es; hguillen@ubu.es; abustillo@ubu.es
RI Guillen-Sanz, Henar/KGK-6634-2024; Checa Cruz, David/J-2839-2017;
   Bustillo, Andres/I-1403-2015
OI Checa Cruz, David/0000-0001-6623-3614; Bustillo,
   Andres/0000-0003-2855-7532
FU Consejeria de Empleo e Industria of the Junta de Castilla y Leon (Spain)
   [INVESTUN/21/BU/0002]; European Commission [2020-1-ES01-KA204-081847];
   Proyectos Estrategicos Orientados a la Transicion Ecologica y Digital of
   the Spanish Ministry of Science and Innovation [TED2021-129485B-C43];
   CRUE-CSIC; Springer Nature
FX Special thanks to Prof. Dr. Juan J. Rodriguez from the University of
   Burgos for his kind-spirited and useful advice. This work was partially
   supported by the ACIS project (Reference Number INVESTUN/21/BU/0002) of
   the Consejeria de Empleo e Industria of the Junta de Castilla y Leon
   (Spain), the Erasmus+ RISKREAL Project (2020-1-ES01-KA204-081847) of the
   European Commission, and the HumanAid Project (TED2021-129485B-C43) of
   the Proyectos Estrategicos Orientados a la Transicion Ecologica y
   Digital of the Spanish Ministry of Science and Innovation. Open Access
   funding provided thanks to the CRUE-CSIC agreement with Springer Nature.
CR Allcoat D, 2018, RES LEARN TECHNOL, V26, DOI 10.25304/rlt.v26.2140
   Andersen E., 2012, P 2012 ACM ANN C HUM, P59, DOI [DOI 10.1145/2207676.2207687, 10.1145/2207676.2207687]
   Anderson L. W., 2001, A Taxonomy for Learning, Teaching, and Assessing: A Revision of Bloom's Taxonomy of Educational Objectives
   Checa D, 2022, PC virtual LAB
   Checa D, 2023, VIRTUAL REAL-LONDON, V27, P3301, DOI 10.1007/s10055-021-00607-1
   Checa D, 2020, LECT NOTES COMPUT SC, V12243, P220, DOI 10.1007/978-3-030-58468-9_17
   Checa D, 2020, VIRTUAL REAL-LONDON, V24, P151, DOI 10.1007/s10055-019-00389-7
   Checa D, 2020, MULTIMED TOOLS APPL, V79, P5501, DOI 10.1007/s11042-019-08348-9
   Csikszentmihalyi M., 1996, Flow and the Psychology of Discovery and Invention
   Csikszentmihalyi M., 1991, Design Issues, V8
   Dai CP, 2023, BRIT J EDUC TECHNOL, V54, P836, DOI 10.1111/bjet.13296
   Dengel A, 2020, PROCEEDINGS OF 2020 6TH INTERNATIONAL CONFERENCE OF THE IMMERSIVE LEARNING RESEARCH NETWORK (ILRN 2020), P163, DOI [10.23919/ilrn47897.2020.9155084, 10.23919/iLRN47897.2020.9155084]
   Di Natale AF, 2020, BRIT J EDUC TECHNOL, V51, P2006, DOI 10.1111/bjet.13030
   Frommel J, 2017, CHI PLAY'17: PROCEEDINGS OF THE ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P367, DOI 10.1145/3116595.3116610
   Fussell Stephanie G., 2019, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V63, P2303, DOI 10.1177/1071181319631494
   Hakkila J., 2018, Seminar.net, V14, P1
   Hopp T., 2016, J CURRENT ISSUES RES, V37, P113, DOI DOI 10.1080/10641734.2016.1171179
   Huang MH, 2003, COMPUT HUM BEHAV, V19, P425, DOI 10.1016/S0747-5632(02)00080-8
   Huang W, 2021, J COMPUT ASSIST LEAR, V37, P745, DOI 10.1111/jcal.12520
   Jeno LM, 2019, COMPUT EDUC, V128, P398, DOI 10.1016/j.compedu.2018.10.008
   Koch M, 2018, ECSCW Exploratory Papers
   Lui M, 2020, BRIT J EDUC TECHNOL, V51, P2180, DOI 10.1111/bjet.13022
   Mayer RE, 2014, COMPUTER GAMES FOR LEARNING: AN EVIDENCE-BASED APPROACH, P1
   Mayer RE, 2009, MULTIMEDIA LEARNING, 2ND EDITION, P223
   Merchant Z, 2014, COMPUT EDUC, V70, P29, DOI 10.1016/j.compedu.2013.07.033
   Miguel-Alonso I, 2023, APPL SCI-BASEL, V13, DOI 10.3390/app13010593
   Moirn R, 2016, CHI PLAY 2016: PROCEEDINGS OF THE ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY COMPANION, P229, DOI 10.1145/2968120.2987730
   Nebel S, 2020, FRONT EDUC, V5, DOI 10.3389/feduc.2020.00129
   Neumann DL, 2018, VIRTUAL REAL-LONDON, V22, P183, DOI 10.1007/s10055-017-0320-5
   Parong J, 2021, J COMPUT ASSIST LEAR, V37, P226, DOI 10.1111/jcal.12482
   Paxinou E, 2022, RES SCI TECHNOL EDUC, V40, P320, DOI 10.1080/02635143.2020.1790513
   Petersen GB, 2022, COMPUT EDUC, V179, DOI 10.1016/j.compedu.2021.104429
   Rafiq AA., 2022, Waikato Journal of Education, V27, P175, DOI [10.15663/wje.v27i3.964, DOI 10.15663/WJE.V27I3.964]
   Rodrigues L, 2022, INT J EDUC TECHNOL H, V19, DOI 10.1186/s41239-021-00314-6
   Ruvimova A, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376724
   Schomaker J, 2021, NEUROBIOL LEARN MEM, V179, DOI 10.1016/j.nlm.2021.107403
   Selzer MN, 2019, DISPLAYS, V59, P9, DOI 10.1016/j.displa.2019.04.002
   Shernoff DJ., 2012, The APA educational psychology handbook
   Slater M, 1997, PRESENCE-VIRTUAL AUG, V6, P603, DOI 10.1162/pres.1997.6.6.603
   Slater M, 2018, BRIT J PSYCHOL, V109, P431, DOI 10.1111/bjop.12305
   Smith SP, 2019, ARRAY-NY, V3-4, DOI 10.1016/j.array.2019.100010
   Sung B, 2021, BRIT J EDUC TECHNOL, V52, P196, DOI 10.1111/bjet.13003
   Tcha-Tokey K., 2016, International Journal of Virtual Reality, V16, P33, DOI [DOI 10.20870/IJVR.2016.16.1.2880, 10.20870/IJVR.2016.16.1.2880]
   Tsay CHH, 2020, J COMPUT ASSIST LEAR, V36, P128, DOI 10.1111/jcal.12385
   Westera W, 2020, EDUC INF TECHNOL, V25, P351, DOI 10.1007/s10639-019-09968-2
   Zaidi SFM, 2019, ICIGP 2019: PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON IMAGE AND GRAPHICS PROCESSING / 2019 5TH INTERNATIONAL CONFERENCE ON VIRTUAL REALITY, P140, DOI 10.1145/3313950.3313977
NR 46
TC 6
Z9 6
U1 4
U2 4
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 27
DI 10.1007/s10055-023-00926-5
PG 23
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA FK8S0
UT WOS:001145776700001
OA hybrid
DA 2024-08-05
ER

PT J
AU Mulvaney, P
   Rooney, B
   Friehs, MA
   Leader, JF
AF Mulvaney, Pat
   Rooney, Brendan
   Friehs, Maximilian A.
   Leader, John Francis
TI Social VR design features and experiential outcomes: narrative review
   and relationship map for dyadic agent conversations
SO VIRTUAL REALITY
LA English
DT Article
DE Social VR; VR; Virtual reality; Agent; Conversation study; Narrative
   review
ID REALITY EXPOSURE THERAPY; VIRTUAL-REALITY; PERSONAL-SPACE; USERS SENSE;
   AVATAR; ENVIRONMENTS; PERCEPTION; VOICE; GAZE; COMMUNICATION
AB The application of virtual reality to the study of conversation and social interaction is a relatively new field of study. While the affordances of VR in the domain compared to traditional methods are promising, the current state of the field is plagued by a lack of methodological standards and shared understanding of how design features of the immersive experience impact participants. In order to address this, this paper develops a relationship map between design features and experiential outcomes, along with expectations for how those features interact with each other. Based on the results of a narrative review drawing from diverse fields, this relationship map focuses on dyadic conversations with agents. The experiential outcomes chosen include presence & engagement, psychological discomfort, and simulator sickness. The relevant design features contained in the framework include scenario agency, visual fidelity, agent automation, environmental context, and audio features. We conclude by discussing the findings of the review and framework, such as the multimodal nature of social VR being highlighted, and the importance of environmental context, and lastly provide recommendations for future research in social VR.
C1 [Mulvaney, Pat; Rooney, Brendan; Friehs, Maximilian A.; Leader, John Francis] Univ Coll Dublin, UCD Sch Psychol, John Henry Newman Bldg,Stillorgan Rd, Dublin 4, Ireland.
   [Friehs, Maximilian A.] Univ Twente, NL-7522 NB Enschede, Netherlands.
   [Friehs, Maximilian A.] Max Planck Inst Human Cognit & Brain Sci, D-04103 Leipzig, Germany.
C3 University College Dublin; University of Twente; Max Planck Society
RP Mulvaney, P (corresponding author), Univ Coll Dublin, UCD Sch Psychol, John Henry Newman Bldg,Stillorgan Rd, Dublin 4, Ireland.
EM patmpsych@gmail.com
RI Friehs, Maximilian/GXG-4154-2022; rooney, brendan/U-1041-2019
OI Friehs, Maximilian/0000-0002-9362-4140; Mulvaney,
   Patrick/0000-0002-0769-3204; rooney, brendan/0000-0001-9842-1492
FU University College Dublin
FX No Statement Available
CR Abdulrahman A, 2022, MULTIMODAL TECHNOLOG, V6, DOI 10.3390/mti6070051
   Aburumman N, 2022, INT J HUM-COMPUT ST, V164, DOI 10.1016/j.ijhcs.2022.102819
   Ahn D, 2014, CYBERPSYCH BEH SOC N, V17, P483, DOI 10.1089/cyber.2013.0455
   Allmendinger K, 2010, EDUC PSYCHOL REV, V22, P41, DOI 10.1007/s10648-010-9117-8
   Anderson PL, 2013, J CONSULT CLIN PSYCH, V81, P751, DOI 10.1037/a0033559
   Aneja D., 2021, P 2021 CHI C HUMAN F, P1, DOI [10.1145/3411764.3445708, DOI 10.1145/3411764.3445708]
   [Anonymous], 2008, P 2008 ACM S VIRTUAL, DOI DOI 10.1145/1450579.1450614
   [Anonymous], 1997, Journal of Computer-Mediated Communication, DOI [DOI 10.1111/J.1083-6101.1997.TB00070.X, 10.1111/j.1083-6101.1997.tb00070.x]
   [Anonymous], 1999, Proceedings of the SIGCHI conference on human factors in computing systems, DOI [DOI 10.1145/302979.303150, 10.1145/302979.303150]
   Jáuregui DAG, 2021, COMPUT ANIMAT VIRT W, V32, DOI 10.1002/cav.2029
   Appel L, 2020, FRONT MED-LAUSANNE, V6, DOI 10.3389/fmed.2019.00329
   Arai T., 2001, J PHONETIC SOC JAPAN, V5, P31, DOI [10.24467/onseikenkyu.5.2_31, DOI 10.24467/ONSEIKENKYU.5.2_31]
   Aseeri S, 2021, IEEE T VIS COMPUT GR, V27, P2608, DOI 10.1109/TVCG.2021.3067783
   Aung T, 2020, CURR OPIN PSYCHOL, V33, P154, DOI 10.1016/j.copsyc.2019.07.028
   Aymerich-Franch L, 2014, FRONT HUM NEUROSCI, V8, DOI 10.3389/fnhum.2014.00944
   Bailenson JN, 2005, PRESENCE-VIRTUAL AUG, V14, P379, DOI 10.1162/105474605774785235
   Baron-Cohen S, 2001, J CHILD PSYCHOL PSYC, V42, P241, DOI 10.1111/1469-7610.00715
   Baumeister RF, 2016, J EXP SOC PSYCHOL, V66, P153, DOI 10.1016/j.jesp.2016.02.003
   Bermejo B, 2023, INFORMATICS-BASEL, V10, DOI 10.3390/informatics10020045
   Birckhead B, 2019, JMIR MENT HEALTH, V6, DOI 10.2196/11973
   Birk MV, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174234
   Birk MV, 2019, J MED INTERNET RES, V21, DOI 10.2196/10133
   Blum J, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.02172
   Bönsch A, 2020, PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (ACM IVA 2020), DOI 10.1145/3383652.3423888
   Bonsch Andrea, 2018, 2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR), P199, DOI 10.1109/VR.2018.8446480
   Borrie SA, 2019, J SPEECH LANG HEAR R, V62, P283, DOI 10.1044/2018_JSLHR-S-18-0210
   Boylan P, 2018, VIRTUAL REAL-LONDON, V22, P309, DOI 10.1007/s10055-017-0329-9
   Bracken CC, 2005, MEDIA PSYCHOL, V7, P191, DOI 10.1207/S1532785XMEP0702_4
   Brugel S, 2015, PATIENT EDUC COUNS, V98, P1260, DOI 10.1016/j.pec.2015.08.007
   Buck LE, 2022, IEEE T VIS COMPUT GR, V28, P2102, DOI 10.1109/TVCG.2022.3150483
   Bulu ST, 2012, COMPUT EDUC, V58, P154, DOI 10.1016/j.compedu.2011.08.024
   Byrne D., 1971, The attraction paradigm
   Cabral JP, 2017, INTERSPEECH, P229, DOI 10.21437/Interspeech.2017-325
   Cao JX, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2023), DOI 10.1145/3544548.3581362
   Carnevale A, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22155511
   Caserman P, 2020, IEEE T VIS COMPUT GR, V26, P3089, DOI 10.1109/TVCG.2019.2912607
   Ceha J, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517599
   Cha SH, 2020, J INTERIOR DES, V45, P51, DOI 10.1111/joid.12179
   Cheng RZ, 2022, IEEE NETWORK, V36, P197, DOI 10.1109/MNET.117.2200055
   Cheng RZ, 2022, PROCEEDINGS OF THE 2022 22ND ACM INTERNET MEASUREMENT CONFERENCE, IMC 2022, P504, DOI 10.1145/3517745.3561417
   Cho S, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P26, DOI [10.1109/VR46266.2020.00-84, 10.1109/VR46266.2020.1581170537418]
   CHRISTIE B, 1974, EUR J SOC PSYCHOL, V4, P366, DOI 10.1002/ejsp.2420040307
   Christophers L, Int J Hum Comput Interact
   Christophers L, 2023, INT J HUM-COMPUT ST, V175, DOI 10.1016/j.ijhcs.2023.103027
   Clark Herbert., 1996, USING LANGUAGE, DOI DOI 10.1017/CBO9780511620539
   Conrad FG, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.01578
   Cooney G, 2020, CURR OPIN PSYCHOL, V31, P22, DOI 10.1016/j.copsyc.2019.06.032
   Cortese J, 2012, COMMUN RES REP, V29, P44, DOI 10.1080/08824096.2011.639913
   Dael N, 2012, EMOTION, V12, P1085, DOI 10.1037/a0025737
   Darville G, 2018, SIMULAT GAMING, V49, P515, DOI 10.1177/1046878118799472
   Davis JA, 2010, ENERG BUILDINGS, V42, P1543, DOI 10.1016/j.enbuild.2010.03.025
   de Gelder B, 2018, BRIT J PSYCHOL, V109, P421, DOI 10.1111/bjop.12308
   Deriu M, 2021, 2021 IEEE IOT VERTICAL AND TOPICAL SUMMIT FOR TOURISM, DOI 10.1109/IEEECONF49204.2021.9604871
   DeVault D, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P1061
   Dibbets P, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00681
   Dicke C, 2010, Talk to me: the influence of audio quality on the perception of social presence, P318, DOI [10.14236/ewic/HCI2010.36, DOI 10.14236/EWIC/HCI2010.36]
   Dickinson P, 2019, VIRTUAL REAL-LONDON, V23, P19, DOI 10.1007/s10055-018-0365-0
   Diemer J, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00026
   Do TD, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517564
   Duverne T., 2020, Virtual reality and augmented reality
   Ebrahimi E, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1, DOI 10.1109/VR.2018.8446539
   Ehret J, 2020, PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (ACM IVA 2020), DOI 10.1145/3383652.3423863
   El-Yamri M, 2019, IEEE INT CONF ADV LE, P349, DOI 10.1109/ICALT.2019.00108
   European Commission, 2021, SHAPE-ID: shaping interdisciplinary practices in Europe. Final toolkit for dissemination, DOI [10.3030/822705, DOI 10.3030/822705]
   Felnhofer A, 2023, INT J HUM-COMPUT INT, DOI 10.1080/10447318.2023.2209979
   Ferstl Y, 2021, PROCEEDINGS OF THE 21ST ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA), P76, DOI 10.1145/3472306.3478338
   Ferstl Y, 2021, COMPUT ANIMAT VIRT W, V32, DOI 10.1002/cav.2016
   Fich LB, 2014, PHYSIOL BEHAV, V135, P91, DOI 10.1016/j.physbeh.2014.05.034
   Fortin DR, 2005, J BUS RES, V58, P387, DOI 10.1016/S0148-2963(03)00106-1
   Foster ME, 2019, PHILOS T R SOC B, V374, DOI 10.1098/rstb.2018.0027
   Freeman Guo, 2020, Proceedings of the ACM on Human-Computer Interaction, V4, DOI 10.1145/3432938
   Freeman G, 2021, PROCEEDINGS OF THE 2021 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE MEDIA EXPERIENCES, IMX 2021, P84, DOI 10.1145/3452918.3458805
   Friehs MA, 2022, COGN RES, V7, DOI 10.1186/s41235-022-00384-8
   Friston S, 2021, PROCEEDINGS OF 27TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, VRST 2021, DOI 10.1145/3489849.3489871
   Furness PJ, 2019, J BURN CARE RES, V40, P878, DOI 10.1093/jbcr/irz106
   Garau M, 2005, PRESENCE-TELEOP VIRT, V14, P104, DOI 10.1162/1054746053890242
   Garau M., 2003, Proceedings o f SIGCHI, P529, DOI [10.1145/642611.642703, DOI 10.1145/642611.642703]
   Gatica-Perez D, 2009, IMAGE VISION COMPUT, V27, P1775, DOI 10.1016/j.imavis.2009.01.004
   Gorisse G, 2019, FRONT ROBOT AI, V6, DOI 10.3389/frobt.2019.00008
   Gratch J, 2006, LECT NOTES ARTIF INT, V4133, P14
   Guadagno RE, 2007, MEDIA PSYCHOL, V10, P1
   Gupta K, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P960, DOI 10.1109/VRW55335.2022.00331
   HALL ET, 1968, CURR ANTHROPOL, V9, P83, DOI 10.1086/200975
   Handley R, 2022, Arxiv, DOI [arXiv:2201.02253, 10.48550/arXiv.2201.02253, DOI 10.48550/ARXIV.2201.02253]
   Harjunen VJ, 2018, COMPUT HUM BEHAV, V87, P384, DOI 10.1016/j.chb.2018.06.012
   Hayes D, 2023, The integrated gameplay entertainment model: developing, validating using a comprehensive model of the video game entertainment experience
   Hecht H, 2019, ACTA PSYCHOL, V193, P113, DOI 10.1016/j.actpsy.2018.12.009
   Heidicker P, 2017, IEEE SYMP 3D USER, P233, DOI 10.1109/3DUI.2017.7893357
   Heylen D, 2006, INT J HUM ROBOT, V3, P241, DOI 10.1142/S0219843606000746
   Higgins D, 2022, COMPUT GRAPH-UK, V104, P116, DOI 10.1016/j.cag.2022.03.009
   Hoppe M, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI [10.1145/10.1145/3313831.3376719, 10.1145/3313831.3376719]
   Hortensius R, 2018, IEEE T COGN DEV SYST, V10, P852, DOI 10.1109/TCDS.2018.2826921
   Howard MC, 2020, COMPUT EDUC, V144, DOI 10.1016/j.compedu.2019.103707
   Huang A, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517593
   Huang L., 2011, Intelligent virtual agents
   James CA, 2011, Ergon Open J, V4
   Jicol C., 2021, Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, P1, DOI DOI 10.1145/3411764.3445588
   Jin WJ, 2023, BRIT J SOC PSYCHOL, V62, P1230, DOI 10.1111/bjso.12630
   Jinga N., 2021, P INT SCI C ELEARNIN, P174
   Jo H, 2019, INT J ENV RES PUB HE, V16, DOI 10.3390/ijerph16234739
   Jonas M, 2019, CHI PLAY'19: EXTENDED ABSTRACTS OF THE ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P437, DOI 10.1145/3341215.3356271
   Kahl S, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.02421
   Kang Sin-hwa., 2008, Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems, V1, P120
   Kendon Adam, 1990, CONDUCTING INTERACTI
   Kern AC, 2020, FRONT ROBOT AI, V7, DOI 10.3389/frobt.2020.00020
   Kerous B, 2020, INT J HUM-COMPUT INT, V36, P505, DOI 10.1080/10447318.2019.1661608
   Kim A, 2017, J NEUROENG REHABIL, V14, DOI 10.1186/s12984-017-0225-2
   Ko SJ, 2006, PERS SOC PSYCHOL B, V32, P806, DOI 10.1177/0146167206286627
   Kolesnichenko A, 2019, PROCEEDINGS OF THE 2019 ACM DESIGNING INTERACTIVE SYSTEMS CONFERENCE (DIS 2019), P241, DOI 10.1145/3322276.3322352
   Kolkmeier J, 2016, LECT NOTES ARTIF INT, V10011, P1, DOI 10.1007/978-3-319-47665-0_1
   Krahé B, 2021, SOC PSYCHOL-GERMANY, V52, P101, DOI 10.1027/1864-9335/a000441
   Kühne K, 2020, FRONT NEUROROBOTICS, V14, DOI 10.3389/fnbot.2020.593732
   Kwon JH, 2013, INT J HUM-COMPUT ST, V71, P978, DOI 10.1016/j.ijhcs.2013.07.003
   Lanier M, 2019, COMPUT HUM BEHAV, V100, P70, DOI 10.1016/j.chb.2019.06.015
   Latoschik ME, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.694433
   Lenormand D, 2022, NEUROSCI BIOBEHAV R, V133, DOI 10.1016/j.neubiorev.2021.12.022
   Li CJ, 2018, 18TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA'18), P191, DOI 10.1145/3267851.3267870
   Li JJ, 2021, BUILD ENVIRON, V198, DOI 10.1016/j.buildenv.2021.107886
   Lin QF, 2015, ACM T APPL PERCEPT, V12, DOI 10.1145/2720020
   Llobera J, 2021, ROY SOC OPEN SCI, V8, DOI 10.1098/rsos.210537
   Lohse KR, 2016, J MOTOR BEHAV, V48, P172, DOI 10.1080/00222895.2015.1068158
   Luong T, 2020, INT SYM MIX AUGMENT, P425, DOI 10.1109/ISMAR50242.2020.00068
   Makled E, 2018, CHI 2018: EXTENDED ABSTRACTS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3170427.3188573
   Maloney D., 2020, P ACM HUMAN COMPUTER, V4, P1, DOI [DOI 10.1145/3415246, 10.1145/3415246]
   Mancini M, 2011, Evaluating the communication of emotion via expressive gesture copying behaviour in an embodied humanoid agent, P224, DOI [10.1007/978-3-642-24600-5_25, DOI 10.1007/978-3-642-24600-5_25]
   Marsella S., 2013, S COMP AN, P25, DOI DOI 10.1145/2485895.2485900
   Mckie I, 2022, J AUST LIB INF ASSOC, V71, P233, DOI 10.1080/24750158.2022.2104738
   McNeill D., 1992, Hand and Mind: What Gestures Reveal about Thought
   McVeigh-Schultz J, 2018, DIS 2018: COMPANION PUBLICATION OF THE 2018 DESIGNING INTERACTIVE SYSTEMS CONFERENCE, P289
   Mehrabian A., 2008, Communication theory
   Montoya RM, 2008, J SOC PERS RELAT, V25, P889, DOI 10.1177/0265407508096700
   Moore RK, 2016, FRONT ROBOT AI, V3, DOI 10.3389/frobt.2016.00061
   Mori M, 2012, IEEE ROBOT AUTOM MAG, V19, P98, DOI 10.1109/MRA.2012.2192811
   Nagels A, 2015, HUM BRAIN MAPP, V36, P1925, DOI 10.1002/hbm.22746
   Newman M, 2022, J ENVIRON PSYCHOL, V79, DOI 10.1016/j.jenvp.2021.101733
   Noufi C, 2023, 2023 IMMERSIVE 3D AU, P1, DOI [10.1109/I3DA57090.2023.10289447, DOI 10.1109/I3DA57090.2023.10289447]
   Novick D, 2018, HAI'18: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON HUMAN-AGENT INTERACTION, P356, DOI 10.1145/3284432.3287185
   Nowak KL, 2003, PRESENCE-TELEOP VIRT, V12, P481, DOI 10.1162/105474603322761289
   Oh CS, 2018, FRONT ROBOT AI, V5, DOI 10.3389/frobt.2018.00114
   Ota S, 2021, J ADV MECH DES SYST, V15, DOI 10.1299/jamdsm.2021jamdsm0005
   Pan XN, 2018, BRIT J PSYCHOL, V109, P395, DOI 10.1111/bjop.12290
   Park MJ, 2019, FRONT PSYCHIATRY, V10, DOI 10.3389/fpsyt.2019.00505
   Parmar D, 2023, IEEE T VIS COMPUT GR, V29, P3698, DOI 10.1109/TVCG.2022.3169426
   Parsons TD, 2015, FRONT HUM NEUROSCI, V9, DOI 10.3389/fnhum.2015.00660
   Pejsa T, 2017, LECT NOTES ARTIF INT, V10498, P347, DOI 10.1007/978-3-319-67401-8_45
   Peña J, 2022, NEW MEDIA SOC, DOI 10.1177/14614448221127258
   Peña J, 2016, J COMPUT-MEDIAT COMM, V21, P195, DOI 10.1111/jcc4.12151
   Phelan I, 2019, J BURN CARE RES, V40, P85, DOI 10.1093/jbcr/iry052
   Phillips L, 2010, P IEEE VIRT REAL ANN, P115, DOI 10.1109/VR.2010.5444802
   Piccione J, 2019, HELIYON, V5, DOI 10.1016/j.heliyon.2019.e02583
   Pisanski K, 2011, J ACOUST SOC AM, V129, P2201, DOI 10.1121/1.3552866
   Praetorius AS., 2020, International Conference on the Foundations of Digital Games, P1, DOI [10.1145/3402942.3403019, DOI 10.1145/3402942.3403019]
   Price M, 2011, J ANXIETY DISORD, V25, P763, DOI 10.1016/j.janxdis.2011.03.004
   Randhavane T, 2019, ACM CONFERENCE ON APPLIED PERCEPTION (SAP 2019), DOI 10.1145/3343036.3343129
   Randhavane T, 2019, IEEE T VIS COMPUT GR, V25, P3135, DOI 10.1109/TVCG.2019.2932235
   Rapley T., 2018, Doing Conversation, Discourse and Document Analysis
   Ratan R, 2022, COMPUT EDUC, V191, DOI 10.1016/j.compedu.2022.104643
   Ratan R, 2020, MEDIA PSYCHOL, V23, P651, DOI 10.1080/15213269.2019.1623698
   Rickel J, 1999, APPL ARTIF INTELL, V13, P343, DOI 10.1080/088395199117315
   Riva G, 2019, CYBERPSYCH BEH SOC N, V22, P82, DOI 10.1089/cyber.2017.29099.gri
   Rizzo A, 2017, NEUROPSYCHOLOGY, V31, P877, DOI 10.1037/neu0000405
   Roberts G, 2019, NEUROIMAGE, V199, P408, DOI 10.1016/j.neuroimage.2019.06.010
   Ruizhi Cheng, 2022, 2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), P141, DOI 10.1109/VRW55335.2022.00040
   Sah YJ, 2017, MEDIA PSYCHOL, V20, P632, DOI 10.1080/15213269.2016.1234397
   Saredakis D, 2020, FRONT HUM NEUROSCI, V14, DOI 10.3389/fnhum.2020.00096
   Schoenenberg K, 2014, INT J HUM-COMPUT ST, V72, P477, DOI 10.1016/j.ijhcs.2014.02.004
   Sekhavat YA, 2017, INT J SERIOUS GAMES, V4, P3, DOI 10.17083/ijsg.v4i2.154
   Shih MT, 2023, 2023 CHI C HUM FACT, P1, DOI [10.1145/3544549.3585839, DOI 10.1145/3544549.3585839]
   Sicorello M, 2019, J CROSS CULT PSYCHOL, V50, P8, DOI 10.1177/0022022118798513
   Sipatchin A, 2021, HEALTHCARE-BASEL, V9, DOI 10.3390/healthcare9020180
   Skalski P, 2007, MEDIA PSYCHOL, V10, P385, DOI 10.1080/15213260701533102
   Skalski P, 2010, PSYCHNOLOGY J, V8, P67
   Skarbez R, 2018, ACM COMPUT SURV, V50, DOI 10.1145/3134301
   Slater M., 2020, FRONT VIRTUAL REAL, V1, P1, DOI DOI 10.3389/FRVIR.2020.00001
   Slater M, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.914392
   Somarathna R, 2023, IEEE T AFFECT COMPUT, V14, P2626, DOI 10.1109/TAFFC.2022.3181053
   Stein JP, 2017, COGNITION, V160, P43, DOI 10.1016/j.cognition.2016.12.010
   Sugimoto T, 2023, ACOUST SCI TECHNOL, V44, P360, DOI 10.1250/ast.44.360
   Tan X., 2023, Neural text-to-speech synthesis, DOI [10.1007/978-981-99-0827-1, DOI 10.1007/978-981-99-0827-1]
   Tehrani BM, 2022, ENG CONSTR ARCHIT MA, V29, P3593, DOI 10.1108/ECAM-01-2021-0017
   Thie S, 1998, 1 INT WKSHP PRES
   Tian N, 2022, VIRTUAL REAL-LONDON, V26, P1409, DOI 10.1007/s10055-022-00638-2
   Uribe-Quevedo A, 2021, 2021 IEEE/ACIS 21ST INTERNATIONAL FALL CONFERENCE ON COMPUTER AND INFORMATION SCIENCE (ICIS 2021-FALL), P156, DOI [10.1109/ICISFALL51598.2021.9627412, 10.1109/ICISFall51598.2021.9627412]
   Vahle N, 2022, EXP AGING RES, V48, P164, DOI 10.1080/0361073X.2021.1943793
   van den Bosch M, 2017, ENVIRON RES, V158, P373, DOI 10.1016/j.envres.2017.05.040
   Vasser M, 2020, CURR OPIN PSYCHOL, V36, P71, DOI 10.1016/j.copsyc.2020.04.010
   Vienne C, 2020, IEEE ACCESS, V8, P29099, DOI 10.1109/ACCESS.2020.2972122
   Wagnerberger L, 2021, IEEE INT SYM MULTIM, P123, DOI 10.1109/ISM52913.2021.00029
   Waltemate T, 2018, IEEE T VIS COMPUT GR, V24, P1643, DOI 10.1109/TVCG.2018.2794629
   Wang I, 2021, INT J HUM-COMPUT INT, V37, P1648, DOI 10.1080/10447318.2021.1898851
   Wang MH, 2020, J PHYS CONF SER, V1518, DOI 10.1088/1742-6596/1518/1/012032
   Wang X, 2021, J CONSUM RES, V48, P189, DOI 10.1093/jcr/ucab012
   Weidner F, 2023, IEEE T VIS COMPUT GR, V29, P2596, DOI 10.1109/TVCG.2023.3247072
   Welsch R, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0217587
   Wheatland N, 2015, COMPUT GRAPH FORUM, V34, P735, DOI 10.1111/cgf.12595
   WHETTEN DA, 1989, ACAD MANAGE REV, V14, P490, DOI 10.2307/258554
   Wiebe EN, 2014, COMPUT HUM BEHAV, V32, P123, DOI 10.1016/j.chb.2013.12.001
   Yang YZ, 2020, COMPUT GRAPH FORUM, V39, P201, DOI 10.1111/cgf.14114
   Yoon B, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P547, DOI [10.1109/vr.2019.8797719, 10.1109/VR.2019.8797719]
   Yoon SO, 2019, COGNITIVE SCI, V43, DOI 10.1111/cogs.12774
   Young MK, 2016, SAP 2015: ACM SIGGRAPH SYMPOSIUM ON APPLIED PERCEPTION, P119, DOI 10.1145/2804408.2804410
   Zhang C., 2009, AMCIS 2009 P
   Zhao GH, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P577, DOI 10.1109/VRW52623.2021.00171
   Zhao R, 2016, P 17 ANN M SPEC INT, P381, DOI DOI 10.18653/V1/W16-3647
   Zibrek K., 2019, Motion, interaction and games, P1, DOI [DOI 10.1145/3359566.3360064, 10.1145/3359566.3360064]
   Zibrek K, 2018, IEEE T VIS COMPUT GR, V24, P1681, DOI 10.1109/TVCG.2018.2794638
   Zimmer P, 2019, PSYCHONEUROENDOCRINO, V101, P186, DOI 10.1016/j.psyneuen.2018.11.010
NR 207
TC 0
Z9 0
U1 8
U2 8
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 45
DI 10.1007/s10055-024-00941-0
PG 20
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA IT5F9
UT WOS:001168591500001
OA hybrid
DA 2024-08-05
ER

PT J
AU Parsons, TD
   Mcmahan, T
   Asbee, J
AF Parsons, Thomas D.
   Mcmahan, Timothy
   Asbee, Justin
TI Feasibility study to identify machine learning predictors for a Virtual
   Environment Grocery Store
SO VIRTUAL REALITY
LA English
DT Article
DE Adaptive virtual environments; Psychological assessment; Cognitive;
   Machine learning
ID DESIGN
AB Virtual reality-based assessment and training platforms proffer the potential for higher-dimensional stimulus presentations (dynamic; three dimensional) than those found with many low-dimensional stimulus presentations (static; two-dimensional) found in pen-and-paper measures of cognition. Studies have investigated the psychometric validity and reliability of a virtual reality-based multiple errands task called the Virtual Environment Grocery Store (VEGS). While advances in virtual reality-based assessments provide potential for increasing evaluation of cognitive processes, less has been done to develop these simulations into adaptive virtual environments for improved cognitive assessment. Adaptive assessments offer the potential for dynamically adjusting the difficulty level of tasks specific to the user's knowledge or ability. Former iterations of the VEGS did not adapt to user performance. Therefore, this study aimed to develop performance classifiers from participants (N = 75) using three classification techniques: Support Vector Machines (SVM), Naive Bayes (NB), and k-Nearest Neighbors (kNN). Participants were categorized as either high performing or low performing based upon the number items they were able to successfully find and add to their grocery cart. The predictors utilized for the classification focused on the times to complete tasks in the virtual environment. Results revealed that the SVM (88% correct classification) classifier was the most robust classifier for identifying cognitive performance followed closely by kNN (86.7%); however, NB tended to perform poorly (76%). Results suggest that participants' task completion times in conjunction with SVM or kNN can be used to adjust the difficult level to best suit the user in the environment.
C1 [Parsons, Thomas D.] Arizona State Univ, Edson Coll, Computat Neuropsychol & Simulat CNS Lab, Tempe, AZ 85281 USA.
   [Mcmahan, Timothy] Univ North Texas, Dallas, TX USA.
   [Asbee, Justin] Univ Arkansas, Fayetteville, AR USA.
C3 Arizona State University; Arizona State University-Tempe; University of
   North Texas System; University of North Texas Denton; University of
   Arkansas System; University of Arkansas Fayetteville
RP Parsons, TD (corresponding author), Arizona State Univ, Edson Coll, Computat Neuropsychol & Simulat CNS Lab, Tempe, AZ 85281 USA.
EM Thomas.Parsons@ASU.edu
OI Asbee, Justin/0000-0002-8210-5855
CR Raya MA, 2020, J CLIN MED, V9, DOI 10.3390/jcm9051260
   [Anonymous], 2012, Virtual Environment Grocery Store [Computer software]
   Arar ÖF, 2017, APPL SOFT COMPUT, V59, P197, DOI 10.1016/j.asoc.2017.05.043
   Asbee J, 2023, VIRTUAL REAL-LONDON, V27, P1391, DOI 10.1007/s10055-022-00744-1
   Barnett MD, 2023, J CLIN EXP NEUROPSYC, V45, P464, DOI 10.1080/13803395.2023.2249175
   Barnett MD, 2022, BRAIN SCI, V12, DOI 10.3390/brainsci12081019
   Bayahya AY, 2022, MATHEMATICS-BASEL, V10, DOI 10.3390/math10121953
   Belger J, 2023, PRESENCE-VIRTUAL AUG, V32, P3, DOI 10.1162/pres_a_00380
   Bhavsar H, 2012, International Journal of Advanced Research in Computer Engineering Technology (IJARCET), V1, P185
   Cavedoni S, 2020, FRONT HUM NEUROSCI, V14, DOI 10.3389/fnhum.2020.00245
   De Gaspari S, 2023, CYBERPSYCH BEH SOC N, V26, P798, DOI 10.1089/cyber.2023.29294.ceu
   Dorneich MC, 2016, INT J AVIAT PSYCHOL, V26, P15, DOI 10.1080/10508414.2016.1226834
   Galatzer-Levy IR, 2017, TRANSL PSYCHIAT, V7, DOI 10.1038/tp.2017.38
   Gibbons RD, 2008, PSYCHIAT SERV, V59, P361, DOI 10.1176/ps.2008.59.4.361
   Gibbons RD, 2016, ANNU REV CLIN PSYCHO, V12, P83, DOI 10.1146/annurev-clinpsy-021815-093634
   Kerick SE, 2023, PLOS ONE, V18, DOI 10.1371/journal.pone.0283418
   Klasnja P, 2015, HEALTH PSYCHOL, V34, P1220, DOI 10.1037/hea0000305
   Kothgassner O. D., 2020, ANN INT COMMUN ASS, V44, P210, DOI DOI 10.1080/23808985.2020.1792790
   Marín-Morales J, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-32063-4
   McMahan T, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.673191
   Mitra J, 2016, NEUROIMAGE, V129, P247, DOI 10.1016/j.neuroimage.2016.01.056
   Mohan S, 2019, IEEE ACCESS, V7, P81542, DOI 10.1109/ACCESS.2019.2923707
   Noble WS, 2006, NAT BIOTECHNOL, V24, P1565, DOI 10.1038/nbt1206-1565
   Omar KS, 2019, 2019 INT C EL COMP C, P1
   Parsons, 2012, Virtual Environment Grocery Store: User Manual
   Parsons TD., 2017, The role of technology in clinical neuropsychology, P143
   Parsons T, 2020, J MED INTERNET RES, V22, DOI 10.2196/23777
   Parsons TD, 2017, J NEUROSCI METH, V291, P13, DOI 10.1016/j.jneumeth.2017.07.027
   Parsons TD, 2017, J ALZHEIMERS DIS, V59, P1227, DOI 10.3233/JAD-170295
   Reise SP, 2009, ANNU REV CLIN PSYCHO, V5, P27, DOI 10.1146/annurev.clinpsy.032408.153553
   Scott E, 2017, IEEE T LEARN TECHNOL, V10, P262, DOI 10.1109/TLT.2016.2609910
   Shute V, 2003, EDUC PSYCHOL-US, V38, P105, DOI 10.1207/S15326985EP3802_5
   Stasolla F, 2023, FRONT AGING NEUROSCI, V15, DOI 10.3389/fnagi.2023.1189498
   Tsai CF, 2021, IEEE T NEUR SYS REH, V29, P2124, DOI 10.1109/TNSRE.2021.3118918
   van Merriënboer JJG, 2005, ETR&D-EDUC TECH RES, V53, P5, DOI 10.1007/BF02504793
   Vaughan N, 2016, COMPUT SCI REV, V22, P65, DOI 10.1016/j.cosrev.2016.09.001
   Weitzner DS, 2021, J CLIN EXP NEUROPSYC, V43, P547, DOI 10.1080/13803395.2021.1960277
   Zahabi M, 2020, VIRTUAL REAL-LONDON, V24, P725, DOI 10.1007/s10055-020-00434-w
NR 38
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 32
DI 10.1007/s10055-023-00927-4
PG 10
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA GJ8P2
UT WOS:001152396200002
OA hybrid
DA 2024-08-05
ER

PT J
AU Perez-Ferrara, DA
   Flores-Medina, GY
   Landa-Ramirez, E
   Gonzalez-Sanchez, DJ
   Luna-Padilla, JA
   Sosa-Millan, AL
   Mondragon-Maya, A
AF Perez-Ferrara, D. A.
   Flores-Medina, G. Y.
   Landa-Ramirez, E.
   Gonzalez-Sanchez, D. J.
   Luna-Padilla, J. A.
   Sosa-Millan, A. L.
   Mondragon-Maya, A.
TI Social cognition training using virtual reality for people with
   schizophrenia: a scoping review
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Social cognition; Cognitive remediation; Schizophrenia
ID BRAIN NETWORKS
AB To date, many interventions for social cognition have been developed. Nevertheless, the use of social cognition training with virtual reality (SCT-VR) in schizophrenia is a recent field of study. Therefore, a scoping review is a suitable method to examine the extent of existing literature, the characteristics of the studies, and the SCT-VR. Additionally, it allows us to summarize findings from a heterogeneous body of knowledge and identify gaps in the literature favoring the planning and conduct of future research. The aim of this review was to explore and describe the characteristics of SCT-VR in schizophrenia. The searched databases were MEDLINE, PsycInfo, Web of Science, and CINAHL. This scoping review considered experimental, quasi-experimental, analytical observational and descriptive observational study designs. The full text of selected citations was assessed by two independent reviewers. Data were extracted from papers included in the scoping review by two independent reviewers. We identified 1,407 records. A total of twelve studies were included for analyses. Study designs were variable, most research was proof-of-concept or pilot studies. Most SCT-VR were immersive and targeted interventions. Number of sessions ranged from 9 to 16, and the duration of each session ranged from 45 to 120 min. Some studies reported a significant improvement in emotion recognition and/or theory of mind. However, SCT-VR is a recent research field in which the heterogeneity in methodological approaches is evident and has prevented the reaching of robust conclusions. Preliminary evidence has shown that SCT-VR could represent a feasible and promising approach for improving SC deficits in schizophrenia.
C1 [Perez-Ferrara, D. A.; Gonzalez-Sanchez, D. J.; Luna-Padilla, J. A.; Sosa-Millan, A. L.; Mondragon-Maya, A.] Univ Nacl Autonoma Mexico, Fac Estudios Super Iztacala, Mexico City, Mexico State, Mexico.
   [Flores-Medina, G. Y.] Inst Nacl Psiquiat Ramon De La Fuente Muniz, Mexico City, Mexico.
   [Perez-Ferrara, D. A.; Gonzalez-Sanchez, D. J.] Ctr Interdisciplinario Cognit Salud Mental & Neuro, Mexico City, Mexico.
   [Landa-Ramirez, E.] Hosp Gen Dr Manuel Gea Gonzalez, Programa Psicol Urgencias, Mexico City, Mexico.
   [Landa-Ramirez, E.] Univ Nacl Autonoma Mexico, Fac Ciencias, Mexico City, Mexico.
C3 Universidad Nacional Autonoma de Mexico; Universidad Nacional Autonoma
   de Mexico
RP Mondragon-Maya, A (corresponding author), Univ Nacl Autonoma Mexico, Fac Estudios Super Iztacala, Mexico City, Mexico State, Mexico.
EM ale.mondragon@comunidad.unam.mx
OI Mondragon-Maya, Alejandra/0000-0002-0557-0163; Flores-Medina,
   Yvonne/0000-0002-4508-8734
FU UNAM-PAPIIT
FX We would like to thank Laura Ferrara Suarez for her proofreading
   services.
CR Adolphs R, 2001, CURR OPIN NEUROBIOL, V11, P231, DOI 10.1016/S0959-4388(00)00202-6
   Alcañiz M, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.01658
   Altman RAE, 2023, CAN J PSYCHIAT, V68, P139, DOI 10.1177/07067437221129073
   American Psychiatric Association, 2013, DIAGN STAT MAN MENT, DOI [10.1176/appi.books.9780890425596, DOI 10.1176/APPI.BOOKS.9780890425596]
   Barciela A, 2021, Psychotic disorders: Comorbidity detection promotes improved diagnosis and treatment
   Barrera P Alvaro, 2006, Rev. chil. neuro-psiquiatr., V44, P215
   Bickart KC, 2014, NEUROPSYCHOLOGIA, V63, P235, DOI 10.1016/j.neuropsychologia.2014.08.013
   Bisso E, 2020, INT J ENV RES PUB HE, V17, DOI 10.3390/ijerph17176111
   Bohil CJ, 2011, NAT REV NEUROSCI, V12, P752, DOI 10.1038/nrn3122
   Bora E, 2016, SCHIZOPHR RES, V175, P72, DOI 10.1016/j.schres.2016.04.018
   Byom Lindsey J, 2013, Front Hum Neurosci, V7, P413, DOI 10.3389/fnhum.2013.00413
   Campos C, 2016, NEUROPSYCHOL REV, V26, P310, DOI 10.1007/s11065-016-9326-0
   Chan KCS, 2023, BRAIN SCI, V13, DOI 10.3390/brainsci13030471
   Csukly G, 2004, CYBERPSYCHOL BEHAV, V7, P278
   Dumont M, 2022, J FORENSIC PRACT, V24, P201, DOI 10.1108/JFP-06-2021-0034
   Fett AKJ, 2011, NEUROSCI BIOBEHAV R, V35, P573, DOI 10.1016/j.neubiorev.2010.07.001
   Freeman D, 2023, PSYCHOL MED, V53, P4373, DOI 10.1017/S0033291722001167
   Gainsford K, 2020, PSYCHIAT RES, V288, DOI 10.1016/j.psychres.2020.112974
   Green MF, 2008, SCHIZOPHRENIA BULL, V34, P670, DOI 10.1093/schbul/sbn045
   Green MF, 2019, WORLD PSYCHIATRY, V18, P146, DOI 10.1002/wps.20624
   Halverson TF, 2022, J PSYCHIATR RES, V150, P307, DOI 10.1016/j.jpsychires.2022.03.017
   Halverson TF, 2019, NEUROSCI BIOBEHAV R, V105, P212, DOI 10.1016/j.neubiorev.2019.07.020
   Hasson-Ohayon I, 2018, SCHIZOPHR RES, V202, P260, DOI 10.1016/j.schres.2018.07.007
   Hogarty GE, 2004, ARCH GEN PSYCHIAT, V61, P866, DOI 10.1001/archpsyc.61.9.866
   Hogarty GE, 2006, PSYCHIAT SERV, V57, P1751, DOI 10.1176/appi.ps.57.12.1751
   International Social Cognition Network, 2023, ISCON Steering Committee
   Kahn RS, 2015, NAT REV DIS PRIMERS, V1, DOI 10.1038/nrdp.2015.67
   Kandalaft MR, 2013, J AUTISM DEV DISORD, V43, P34, DOI 10.1007/s10803-012-1544-6
   Kayser N, 2006, NEUROPSYCHOL REHABIL, V16, P83, DOI 10.1080/09602010443000236
   Kennedy DP, 2012, TRENDS COGN SCI, V16, P559, DOI 10.1016/j.tics.2012.09.006
   Kenney J, 2015, SCHIZOPHR RES, V169, P101, DOI 10.1016/j.schres.2015.09.007
   Kern RS, 2010, KEY ISSUES MENT HEAL, V177, P1
   Krohn S, 2020, J MED INTERNET RES, V22, DOI 10.2196/16724
   Lahera G, 2021, J PSYCHIATR RES, V142, P101, DOI 10.1016/j.jpsychires.2021.07.029
   Lan Lucy, 2023, Curr Treat Options Psychiatry, P1, DOI 10.1007/s40501-023-00287-5
   Lee KH, 2004, PSYCHOL MED, V34, P391, DOI 10.1017/S0033291703001284
   Lockwood C., 2004, Joanna Briggs Institute Reports, V2, P309, DOI DOI 10.11124/01938924-200402020-00001
   McDonald S, 2017, NEUROPSYCHOLOGICAL REHABILITATION: THE INTERNATIONAL HANDBOOK, P266
   Medalia A, 2013, CURR OPIN PSYCHIATR, V26, P151, DOI 10.1097/YCO.0b013e32835dcbd4
   Medalia A, 2011, SCHIZOPHRENIA BULL, V37, pS122, DOI 10.1093/schbul/sbr063
   Meins IA, 2023, TRIALS, V24, DOI 10.1186/s13063-023-07241-z
   Mishkind MC, 2017, CURR PSYCHIAT REP, V19, DOI 10.1007/s11920-017-0836-0
   Nijman SA, 2023, SCHIZOPHRENIA BULL, V49, P518, DOI 10.1093/schbul/sbac166
   Nijman SA, 2023, CYBERPSYCH BEH SOC N, V26, P288, DOI 10.1089/cyber.2022.0228
   Nijman SA, 2020, SCHIZOPHRENIA BULL, V46, P1086, DOI 10.1093/schbul/sbaa023
   Nijman SA, 2019, BMC PSYCHIATRY, V19, DOI 10.1186/s12888-019-2250-0
   Nijman SA, 2020, JMIR MENT HEALTH, V7, DOI 10.2196/17808
   Nuechterlein KH, 2008, AM J PSYCHIAT, V165, P203, DOI 10.1176/appi.ajp.2007.07010042
   Oliveira C, 2021, BRIT J OCCUP THER, V84, P571, DOI 10.1177/03080226211011391
   Penn DL, 2007, PSYCHIAT SERV, V58, P449
   Peters MDJ, 2020, JBI EVID SYNTH, V18, P2119, DOI 10.11124/JBIES-20-00167
   Peyroux E, 2016, FRONT PSYCHIATRY, V7, DOI 10.3389/fpsyt.2016.00066
   Pinkham AE, 2014, SCHIZOPHRENIA BULL, V40, P813, DOI 10.1093/schbul/sbt081
   Porcelli S, 2019, NEUROSCI BIOBEHAV R, V97, P10, DOI 10.1016/j.neubiorev.2018.09.012
   Ren YY, 2024, CLIN REHABIL, V38, P305, DOI 10.1177/02692155231213476
   Rizzo Albert Skip, 2018, Focus (Am Psychiatr Publ), V16, P266, DOI 10.1176/appi.focus.20180011
   Sardi L, 2017, J BIOMED INFORM, V71, P31, DOI 10.1016/j.jbi.2017.05.011
   Saredakis D, 2020, FRONT HUM NEUROSCI, V14, DOI 10.3389/fnhum.2020.00096
   Schaafsma SM, 2015, TRENDS COGN SCI, V19, P65, DOI 10.1016/j.tics.2014.11.007
   Shen ZH, 2022, FRONT PSYCHIATRY, V13, DOI 10.3389/fpsyt.2022.1022278
   Singhai K., 2020, Ind J Soc Psychiatry, V36, P102, DOI [10.4103/ijsp.ijsp8419, DOI 10.4103/IJSP.IJSP8419]
   Solís-Vivanco R, 2020, JAMA PSYCHIAT, V77, P543, DOI 10.1001/jamapsychiatry.2020.0001
   Strickland D, 1996, J AUTISM DEV DISORD, V26, P651, DOI 10.1007/BF02172354
   Thompson A, 2020, FRONT PSYCHIATRY, V11, DOI 10.3389/fpsyt.2020.00219
   Tricco AC, 2018, ANN INTERN MED, V169, P467, DOI 10.7326/M18-0850
   Turner WA, 2014, CLIN PSYCHOL REV, V34, P634, DOI 10.1016/j.cpr.2014.10.003
   Vajawat B, 2021, PSYCHIAT RES, V295, DOI 10.1016/j.psychres.2020.113585
   Vass E, 2022, COMPR PSYCHIAT, V119, DOI 10.1016/j.comppsych.2022.152350
   Vass E, 2021, FRONT PSYCHOL, V12, DOI 10.3389/fpsyg.2021.642590
   Vass E, 2021, CLIN PSYCHOL PSYCHOT, V28, P727, DOI 10.1002/cpp.2519
   Ventura S, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.02509
   Yeo H, 2022, BRIT J CLIN PSYCHOL, V61, P37, DOI 10.1111/bjc.12320
NR 72
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAY 25
PY 2024
VL 28
IS 2
AR 117
DI 10.1007/s10055-024-01010-2
PG 19
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA SD3A6
UT WOS:001232468900002
OA hybrid
DA 2024-08-05
ER

PT J
AU García, MG
   Sauer, Y
   Watson, T
   Wahl, S
AF Garcia, Miguel Garcia
   Sauer, Yannick
   Watson, Tamara
   Wahl, Siegfried
TI Virtual reality (VR) as a testing bench for consumer optical solutions:
   a machine learning approach (GBR) to visual comfort under simulated
   progressive addition lenses (PALs) distortions
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual; Reality; Distortions; Comfort; Progressive; Addition; Lenses;
   Eye-tracking
ID EYE; MAGNIFICATION
AB For decades, manufacturers have attempted to reduce or eliminate the optical aberrations that appear on the progressive addition lens' surfaces during manufacturing. Besides every effort made, some of these distortions are inevitable given how lenses are fabricated, where in fact, astigmatism appears on the surface and cannot be entirely removed, or where non-uniform magnification becomes inherent to the power change across the lens. Some presbyopes may refer to certain discomfort when wearing these lenses for the first time, and a subset of them might never adapt. Developing, prototyping, testing and purveying those lenses into the market come at a cost, which is usually reflected in the retail price. This study aims to test the feasibility of virtual reality (VR) for testing customers' satisfaction with these lenses, even before getting them onto production. VR offers a controlled environment where different parameters affecting progressive lens comforts, such as distortions, image displacement or optical blurring, can be inspected separately. In this study, the focus was set on the distortions and image displacement, not taking blur into account. Behavioural changes (head and eye movements) were recorded using the built-in eye tracker. We found participants were significantly more displeased in the presence of highly distorted lens simulations. In addition, a gradient boosting regressor was fitted to the data, so predictors of discomfort could be unveiled, and ratings could be predicted without performing additional measurements.
C1 [Garcia, Miguel Garcia; Sauer, Yannick; Wahl, Siegfried] Univ Tubingen, Inst Ophthalm Res, Elfriede Aulhorn Str 7, D-72072 Tubingen, Germany.
   [Watson, Tamara] Western Sydney Univ, Sch Social Sci & Psychol, Sydney, NSW, Australia.
   [Wahl, Siegfried] Carl Zeiss Vis Int GmbH, Turnstr 27, D-73430 Aalen, Germany.
C3 Eberhard Karls University of Tubingen; Eberhard Karls University
   Hospital; Western Sydney University
RP García, MG (corresponding author), Univ Tubingen, Inst Ophthalm Res, Elfriede Aulhorn Str 7, D-72072 Tubingen, Germany.
EM mikelgg93@gmail.com
RI Wahl, Siegfried/I-7200-2016
FU European Grant PLATYPUS, Marie Sklodowska-Curie RISE initiative [734227]
FX Open Access funding enabled and organized by Projekt DEAL. This work was
   supported by the European Grant PLATYPUS (Grant Agreement No 734227), a
   Marie Sklodowska-Curie RISE initiative. Authors MGG & YS are employees
   of the University of Tubingen (E), SW is employed by Carl Zeiss Vision
   International GmbH (E) and is a scientist at the University Tubingen. TW
   is employed (E) by Western Sydney University. According to the journal
   policy, they declare their employment positions. The founders did not
   play any additional role in the study design, data collection, and
   analysis, decision to publish, or preparation of the manuscript. The
   specific roles of these authors are articulated in the 'author
   contributions' statement.
CR Alonso J, 2019, MODERN OPHTHALMIC OPTICS, P1, DOI 10.1017/9781316275474
   Alvarez TL, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-02851-5
   Alvarez TL, 2009, I IEEE EMBS C NEUR E, P143, DOI 10.1109/NER.2009.5109255
   [Anonymous], 2016, Official Journal of the European Union, V59
   Atchison DA., 1987, Clin Exp Optom, V70, P149, DOI DOI 10.1111/J.1444-0938.1987.TB04235.X
   Aves O, 1907, Special properties achieved by the combination of the front and back surfaces
   Barbero S, 2020, OPT LETT, V45, P5656, DOI 10.1364/OL.401927
   Barbero S, 2015, OPT EXPRESS, V23, P13185, DOI 10.1364/OE.23.013185
   Bengfort B., 2019, J OPEN SOURCE SOFTW, V4, P1075, DOI [10.21105/joss.01075, DOI 10.21105/JOSS.01075]
   Bist J, 2021, OPHTHAL PHYSL OPT, V41, P610, DOI 10.1111/opo.12796
   CANNON SC, 1985, ACTA OTO-LARYNGOL, V100, P81, DOI 10.3109/00016488509108591
   Chan TT, 2023, IEEE T VIS COMPUT GR, V29, P3656, DOI 10.1109/TVCG.2022.3168190
   Chang E, 2020, INT J HUM-COMPUT INT, V36, P1658, DOI 10.1080/10447318.2020.1778351
   Cho M H, 1991, J Am Optom Assoc, V62, P672
   Dai WW, 2016, IEEE SIG PROC MED
   Esser G, 2017, J OPT SOC AM A, V34, P441, DOI 10.1364/JOSAA.34.000441
   Friedman JH, 2002, COMPUT STAT DATA AN, V38, P367, DOI 10.1016/S0167-9473(01)00065-2
   Habtegiorgis SW, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01158
   Han Y, 2003, INVEST OPHTH VIS SCI, V44, P1534, DOI 10.1167/iovs.02-0507
   Head Tim, 2020, Zenodo
   Hutchings N, 2007, OPHTHAL PHYSL OPT, V27, P142, DOI 10.1111/j.1475-1313.2006.00460.x
   Lundberg SM, 2017, ADV NEUR IN, V30
   Maitenaz BF, 1969, Ophthalmic lenses with a progressively varying focal power
   Minkwitz G., 1963, Int. J. Opt, V10, P223, DOI [DOI 10.1080/713817794, 10.1080/713817794]
   Mulder K, 2020, J MATH PSYCHOL, V95, DOI 10.1016/j.jmp.2019.102309
   Mulder KT, 2019, Bayesian circular statistics: von Mises-based solutions for practical problems
   Najmee NAA, 2017, ENVIRON-BEHAV PROC J, V2, P373, DOI 10.21834/e-bpj.v2i6.999
   Ogle KN., 1950, RES BINOCULAR VISION
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Poullain AG, 1911, Optical lens
   Rifai K, 2020, J VISION, V20, DOI 10.1167/jov.20.13.10
   Rifai K, 2016, J VISION, V16, DOI 10.1167/16.11.5
   Sauer Y, 2022, VIRTUAL REAL-LONDON, V26, P1089, DOI 10.1007/s10055-021-00619-x
   Sauer Y, 2020, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.544867
   Sawides L, 2010, J VISION, V10, DOI 10.1167/10.12.22
   SULLIVAN CM, 1988, OPHTHAL PHYSL OPT, V8, P402, DOI 10.1111/j.1475-1313.1988.tb01177.x
   van der Lans R, 2011, BEHAV RES METHODS, V43, P239, DOI 10.3758/s13428-010-0031-2
   Vinas M, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0046361
   VOLK D, 1962, ARCH OPHTHALMOL-CHIC, V68, P776
NR 39
TC 0
Z9 0
U1 8
U2 8
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 36
DI 10.1007/s10055-023-00894-w
PG 13
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA GJ0V3
UT WOS:001152191500001
OA hybrid
DA 2024-08-05
ER

PT J
AU Alrashidi, M
   Evans, JO
   Tomlinson, RJ
   Williams, CA
   Buckingham, G
AF Alrashidi, Mohammed
   Evans, Jack O.
   Tomlinson, Richard J.
   Williams, Craig A.
   Buckingham, Gavin
TI Examining the feasibility of immersive virtual reality to measure upper
   limb motor performance in typically developing children and adolescents
SO VIRTUAL REALITY
LA English
DT Article
DE Pediatric rehabilitation; Technology-based assessment; Head-mounted
   displays; Virtual environment; Circle drawing task
ID CEREBRAL-PALSY; CONVENTIONAL PHYSIOTHERAPY; REHABILITATION; MOBILITY;
   SPEED; AGE
AB Over the last five years, virtual reality (VR) has become more popular in pediatric physiotherapy. In this study, we assessed the feasibility and acceptability of measuring upper-limb movements in typically-developing children and adolescents using an immersive virtual reality (iVR) headset. Thirty-six typically-developing children (age: 12 +/- 2.1 y) were recruited and required to draw circles in a custom-built virtual environment using a Meta-Quest-2 headset. Outcomes were the System Usability Scale (SUS), Developmental Coordination Disorder-Questionnaire (DCD-Q), and three metrics of circle drawing performance (movement time, mean velocity and circle roundness). The mean score for the SUS was 74 +/- 11, indicating good levels of acceptability and usability when the participants used the headset. No strong relationships were observed between the circle drawing metrics and DCD-Q scores (rho = < 0.3, p = > 0.05), but circle roundness ratios were positively and significantly correlated with SUS scores (rho = 0.5, p = 0.003). No adverse effects associated with iVR use were reported for any participants. This study showed that iVR is a viable method to measure upper-limb motor performance in children and adolescents, highlighting the potential value of this tool in pediatric physiotherapy practice.
C1 [Alrashidi, Mohammed; Williams, Craig A.] Univ Exeter, Childrens Hlth & Exercise Res Ctr, Exeter, England.
   [Alrashidi, Mohammed; Evans, Jack O.; Buckingham, Gavin] Univ Exeter, Publ Hlth & Sport Sci, Exeter, Devon, England.
   [Alrashidi, Mohammed] Taibah Univ, Fac Med Rehabil Sci, Phys Therapy Dept, Madinah, Saudi Arabia.
   [Tomlinson, Richard J.] Royal Devon Univ Healthcare NHS Fdn Trust, Exeter, England.
C3 University of Exeter; University of Exeter; Taibah University
RP Buckingham, G (corresponding author), Univ Exeter, Publ Hlth & Sport Sci, Exeter, Devon, England.
EM g.buckingham@exeter.ac.uk
RI Williams, Craig Anthony/AAQ-8954-2020
OI Williams, Craig Anthony/0000-0002-1740-6248
FU University of Exeter
FX We would like to thank all the participants and their parents for
   participating in the study. We also thank Alice Lester for her
   assistance in the recruitment of the participants. For the purpose of
   open access, the author has applied a Creative Commons Attribution (CC
   BY) licence to any Author Accepted Manuscript version arising.
CR Accardo AP, 2013, HUM MOVEMENT SCI, V32, P136, DOI 10.1016/j.humov.2012.10.004
   Allen K, 2023, Q J EXP PSYCHOL, DOI 10.1177/17470218231214479
   Alrashidi M, 2023, DISABIL REHABIL, V45, P1773, DOI 10.1080/09638288.2022.2071484
   Amirthalingam J, 2021, Cureus, V13
   Ammann-Reiffer C, 2022, JMIR SERIOUS GAMES, V10, DOI 10.2196/38509
   Arnoni JLB, 2021, GAMES HEALTH J, V10, P254, DOI 10.1089/g4h.2021.0009
   Arthur T, 2021, CORTEX, V138, P318, DOI 10.1016/j.cortex.2021.02.013
   Bangor A, 2008, INT J HUM-COMPUT INT, V24, P574, DOI 10.1080/10447310802205776
   Brooke J., 1996, SUS-a quick and dirty usability scale, DOI [DOI 10.1201/9781498710411-35, DOI 10.1201/9781498710411]
   Bryanton C, 2006, CYBERPSYCHOL BEHAV, V9, P123, DOI 10.1089/cpb.2006.9.123
   Carson RG, 1997, Q J EXP PSYCHOL-A, V50, P664, DOI 10.1080/713755721
   Choi JY, 2021, DEV MED CHILD NEUROL, V63, P480, DOI 10.1111/dmcn.14762
   Civetta Lauren R, 2008, Pediatr Phys Ther, V20, P39, DOI 10.1097/PEP.0b013e31815ccaeb
   Cohen EJ, 2021, PERCEPT MOTOR SKILL, V128, P605, DOI 10.1177/0031512521990358
   Cohen J., 1988, Stat Power Anal Behav Sci, V2, P495
   Corbetta D, 2015, J PHYSIOTHER, V61, P117, DOI 10.1016/j.jphys.2015.05.017
   Eastough D, 2007, EXP BRAIN RES, V176, P193, DOI 10.1007/s00221-006-0749-3
   El-Shamy S, 2017, J MUSCULOSKEL NEURON, V17, P319
   Elor A, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON SMART COMPUTING (SMARTCOMP 2018), P219, DOI 10.1109/SMARTCOMP.2018.00094
   Elvrum AKG, 2016, DEV MED CHILD NEUROL, V58, P662, DOI 10.1111/dmcn.13119
   Ferreira L, 2020, PHYS OCCUP THER PEDI, V40, P121, DOI 10.1080/01942638.2019.1665154
   FRANKS IM, 1990, HUM MOVEMENT SCI, V9, P573, DOI 10.1016/0167-9457(90)90017-8
   Fregna G, 2022, medRxiv
   Gatouillat A, 2017, COMPUT BIOL MED, V87, P124, DOI 10.1016/j.compbiomed.2017.05.020
   Georgopoulos AP, 2000, CURR OPIN NEUROBIOL, V10, P238, DOI 10.1016/S0959-4388(00)00072-6
   Hocking DR, 2022, J NEUROENG REHABIL, V19, DOI 10.1186/s12984-021-00978-1
   Jha KK, 2021, SOMATOSENS MOT RES, V38, P117, DOI 10.1080/08990220.2021.1876016
   Kott K., 2009, Cyber Ther Rehabil, V2, P35
   Krabben T, 2011, J NEUROENG REHABIL, V8, DOI 10.1186/1743-0003-8-15
   Laver KE, 2017, COCHRANE DB SYST REV, DOI 10.1002/14651858.CD008349.pub4
   Levac D, 2012, PHYS OCCUP THER PEDI, V32, P180, DOI 10.3109/01942638.2011.616266
   Lewis JR, 2009, LECT NOTES COMPUT SC, V5619, P94, DOI 10.1007/978-3-642-02806-9_12
   Lin QS, 2015, HUM MOVEMENT SCI, V40, P163, DOI 10.1016/j.humov.2014.12.010
   Liu C, 2022, JMIR SERIOUS GAMES, V10, DOI 10.2196/38972
   Mekbib DB, 2021, ANN NY ACAD SCI, V1493, P75, DOI 10.1111/nyas.14554
   NHS-England, 2022, Digital Productivity Programme
   Oliveira LF, 1996, PHYSIOL MEAS, V17, P305, DOI 10.1088/0967-3334/17/4/008
   Pacilli A, 2014, APPL BIONICS BIOMECH, V11, P91, DOI [10.3233/ABB-140095, 10.1155/2014/251931]
   Phelan I, 2023, VIRTUAL REAL-LONDON, V27, P3505, DOI 10.1007/s10055-023-00747-6
   Porras DC, 2018, NEUROLOGY, V90, P1017, DOI 10.1212/WNL.0000000000005603
   Rathinam C, 2021, DEV MED CHILD NEUROL, V63, P370, DOI 10.1111/dmcn.14791
   Reid D., 2002, Technology Disability, V14, P53, DOI [10.3233/TAD-2002-14202, DOI 10.3233/TAD-2002-14202]
   Reid Denise, 2004, Occup Ther Int, V11, P131, DOI 10.1002/oti.202
   Reid Denise T, 2002, Pediatr Rehabil, V5, P141, DOI 10.1080/1363849021000039344
   Ringenbach SDR, 2005, ECOL PSYCHOL, V17, P1, DOI 10.1207/s15326969eco1701_1
   Rueckriegel SM, 2008, INT J DEV NEUROSCI, V26, P655, DOI 10.1016/j.ijdevneu.2008.07.015
   Shen JB, 2020, AM J LIFESTYLE MED, V14, P6, DOI 10.1177/1559827618756588
   SmitsEngelsman BCM, 1997, J EXP CHILD PSYCHOL, V67, P164, DOI 10.1006/jecp.1997.2400
   Song YH, 2021, BRAIN SCI, V11, DOI 10.3390/brainsci11081032
   Tuta L, 2019, J Military Technol, V2
   Veruggio G, 2022, Cerebral palsy: a practical guide for Rehabilitation professionals, P291
   Voigt-Antons JN, 2020, INT WORK QUAL MULTIM
   Wilson B. N., 2007, Administrative manual for the DCDQ107 with psychometric properties, P267
   Xu W, 2021, P 2021 CHI C HUMAN F
   Xu WG, 2020, GAMES HEALTH J, V9, P405, DOI 10.1089/g4h.2019.0102
   Zhao L, 2020, 2020 IEEE C VIRTUAL
NR 56
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD APR 23
PY 2024
VL 28
IS 2
AR 99
DI 10.1007/s10055-024-00996-z
PG 10
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA OH2P6
UT WOS:001206318400001
OA hybrid
DA 2024-08-05
ER

PT J
AU Monaro, M
   Mazza, C
   Colasanti, M
   Colicino, E
   Bosco, F
   Ricci, E
   Biondi, S
   Rossi, M
   Roma, P
AF Monaro, Merylin
   Mazza, Cristina
   Colasanti, Marco
   Colicino, Elena
   Bosco, Francesca
   Ricci, Eleonora
   Biondi, Silvia
   Rossi, Michela
   Roma, Paolo
TI Testing memory of a VR environment: comparison with the real environment
   and 2D pictures
SO VIRTUAL REALITY
LA English
DT Article
DE Visual memory; Suggestibility; Virtual reality; Virtual environments
ID VIRTUAL-REALITY; EXPERIENCE; IMMERSION
AB In recent years, there has been a growing trend in cognitive psychology research towards recreating experimental situations in virtual reality (VR). VR settings are thought to have higher ecological validity than laboratory settings using digital, two-dimensional (2D) pictures. Some studies have shown cognitive performance in VR settings to follow that of the real world. However, other studies obtained controversial results. The present study tested the memory performance of three groups of participants who were exposed to the same environment (a room) through different modalities: in real life, in VR, and through 2D pictures. The results highlighted that participants who were exposed to the target room in real life had an overall better memory performance, compared to participants who saw the room in VR or through 2D pictures. On the other hand, no differences in memory performance emerged between the VR and 2D picture groups, except for the non-suggestive verbal task. The results suggest that future research should be careful in assuming that performance in VR settings is comparable to real life and that VR is more ecological than traditional 2D media.
C1 [Monaro, Merylin] Univ Padua, Dept Gen Psychol, Padua, Italy.
   [Mazza, Cristina; Ricci, Eleonora] Univ G dAnnunzio, Dept Neurosci Imaging & Clin Sci, Chieti, Italy.
   [Colasanti, Marco] G Annunzio Univ Chieti Pescara, Dept Psychol Hlth & Terr Sci, Chieti, Italy.
   [Colicino, Elena] Icahn Sch Med Mt Sinai, Dept Environm Med & Publ Hlth, New York, NY USA.
   [Bosco, Francesca; Biondi, Silvia; Rossi, Michela; Roma, Paolo] Sapienza Univ Rome, Dept Human Neurosci, Rome, Italy.
C3 University of Padua; G d'Annunzio University of Chieti-Pescara; G
   d'Annunzio University of Chieti-Pescara; Icahn School of Medicine at
   Mount Sinai; Sapienza University Rome
RP Monaro, M (corresponding author), Univ Padua, Dept Gen Psychol, Padua, Italy.; Mazza, C (corresponding author), Univ G dAnnunzio, Dept Neurosci Imaging & Clin Sci, Chieti, Italy.
EM merylin.monaro@unipd.it; cristina.mazza@uniroma1.it
FU Universita degli Studi di Padova within the CRUI-CARE Agreement
FX Open access funding provided by Universita degli Studi di Padova within
   the CRUI-CARE Agreement.
CR Bell P. A., 2001, Environmental psychology, V5th ed.
   Beschin N, 2013, RBMT-3 Adattamento italiano
   Cohen J., 1988, Statistical power analysis for the behavioral sciences
   Corp I.B.M., 2011, IBM SPSS Statistics for Windows
   De Renzi E, 1975, Cortex, V11, P341
   Elmqaddem N, 2019, INT J EMERG TECHNOL, V14, P234, DOI 10.3991/ijet.v14i03.9289
   Elsey JWB, 2019, COMPUT HUM BEHAV, V97, P35, DOI 10.1016/j.chb.2019.02.031
   Ernstsen Jorgen, 2019, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V63, P2277, DOI 10.1177/1071181319631411
   Faul F, 2007, BEHAV RES METHODS, V39, P175, DOI 10.3758/BF03193146
   Freeman J, 2000, PRESENCE-TELEOP VIRT, V9, P149, DOI 10.1162/105474600566691
   Freeman J, 1999, PRESENCE-TELEOP VIRT, V8, P1, DOI 10.1162/105474699566017
   GODDEN DR, 1975, BRIT J PSYCHOL, V66, P325, DOI 10.1111/j.2044-8295.1975.tb01468.x
   Gudjonsson GH, 1997, Psychology
   Gujonsson GH., 1986, SOC BEHAV, V1, P83
   Hodges L, 1994, Technical Report No. GIT-GVU-94-5
   Hu-Au E, 2021, J SCI EDUC TECHNOL, V30, P862, DOI 10.1007/s10956-021-09925-0
   Hudson S, 2019, J BUS RES, V100, P459, DOI 10.1016/j.jbusres.2018.10.062
   Iacus SM, 2012, POLIT ANAL, V20, P1, DOI 10.1093/pan/mpr013
   Jennett C, 2008, INT J HUM-COMPUT ST, V66, P641, DOI 10.1016/j.ijhcs.2008.04.004
   Johnson-Glenberg MC, 2021, J COMPUT ASSIST LEAR, V37, P1263, DOI 10.1111/jcal.12567
   Johnson-Glenberg MC, 2019, SMART COMPUT INTELL, P83, DOI 10.1007/978-981-13-8265-9_5
   Kisker J, 2021, PSYCHOL RES-PSYCH FO, V85, P2485, DOI 10.1007/s00426-020-01417-x
   Krokos E, 2019, VIRTUAL REAL-LONDON, V23, P1, DOI 10.1007/s10055-018-0346-3
   Makransky G, 2019, LEARN INSTR, V60, P225, DOI 10.1016/j.learninstruc.2017.12.007
   Mania K, 2001, CYBERPSYCHOL BEHAV, V4, P247, DOI 10.1089/109493101300117938
   Norman DG, 2020, J APPL RES MEM COGN, V9, P118
   North MM, 1998, ST HEAL T, V58, P112
   Pournelle G. H., 1953, Journal of Mammalogy, V34, P133, DOI 10.1890/0012-9658(2002)083[1421:SDEOLC]2.0.CO;2
   Radianti J, 2020, COMPUT EDUC, V147, DOI 10.1016/j.compedu.2019.103778
   SHAFFER JP, 1995, ANNU REV PSYCHOL, V46, P561, DOI 10.1146/annurev.ps.46.020195.003021
   Slater M., 1996, ACM VIRTUAL REALITY, P163, DOI DOI 10.1145/3304181.3304216
   Slater M, 2009, PHILOS T R SOC B, V364, P3549, DOI 10.1098/rstb.2009.0138
   Spinnler H, 1987, Italian Journal of Neurological Sciences, V6, P47
   Sutcliffe A, 2005, INT J HUM-COMPUT ST, V62, P307, DOI 10.1016/j.ijhcs.2004.11.010
   Taylor DA, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.00507
   Wagler A, 2018, COMMUN RES REP, V35, P456, DOI 10.1080/08824096.2018.1525350
   Wilson B., 1985, RIVERMEAD BEHAV MEMO
NR 37
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD APR 23
PY 2024
VL 28
IS 2
AR 100
DI 10.1007/s10055-024-00999-w
PG 12
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA OH2P6
UT WOS:001206318400002
OA hybrid
DA 2024-08-05
ER

PT J
AU Liu, XL
   Wang, LL
   Liu, Y
   Wu, J
AF Liu, Xiaolong
   Wang, Lili
   Liu, Yi
   Wu, Jian
TI Automatic portals layout for VR navigation
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Navigation; Teleportation; Portal
ID WALKING
AB Portals layout in a large virtual scene can help users improve navigation efficiency, but determining the number and the positions of the portals has some challenges. In this paper, we propose two automatic virtual portals layout methods for efficient VR navigation. We first introduced a visibility importance-based method to determine the portals' positions and numbers for a given scene. To improve the walkability of the VR environment, based on the visibility importance-based method, we propose a simulated annealing-based portal layout method to optimize the portals' positions further. To reduce the number of reverse redirections in the navigation, we also proposed a real-time portal orientation determination algorithm to determine the orientations of the portals. We designed a user study to test the two methods we propose. The results showed that our methods made the VR navigation more efficient than the portals random layout and non-portal methods. Our methods achieved a significant reduction of task completion time, total viewpoint translation, and the number of reverse path redirections without increasing the scores of SSQ, IPQ, and task load.
C1 [Liu, Xiaolong; Wang, Lili; Liu, Yi; Wu, Jian] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Wang, Lili] Peng Cheng Lab, Xingke 1st, Shengzhen 518055, Guadong, Peoples R China.
   [Wang, Lili] Beihang Univ, Beijing Adv Innovat Ctr Biomed Engn, Huayuan Rd, Beijing 100191, Peoples R China.
C3 Beihang University; Beihang University
RP Wang, LL (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.; Wang, LL (corresponding author), Peng Cheng Lab, Xingke 1st, Shengzhen 518055, Guadong, Peoples R China.; Wang, LL (corresponding author), Beihang Univ, Beijing Adv Innovat Ctr Biomed Engn, Huayuan Rd, Beijing 100191, Peoples R China.
EM liuxiaolong186@buaa.edu.cn; wanglily@buaa.edu.cn; 563127539@qq.com;
   Lanayawj@buaa.edu.cn
FU National Key R D plan; National Natural Science Foundation of China
   [61932003, 61772051]; Beijing Natural Science Foundation [L182016];
   Beijing Program for International S &T Cooperation Project
   [Z191100001619003];  [2019YFC1521102]
FX This work was supported by National Key R &D plan 2019YFC1521102, by the
   National Natural Science Foundation of China through Projects 61932003
   and 61772051, by the Beijing Natural Science Foundation L182016, by the
   Beijing Program for International S &T Cooperation Project
   Z191100001619003.
CR [Anonymous], SPSS SOFTWARE
   Bhandari J., 2018, P 44 GRAPHICS INTERF, P162, DOI [DOI 10.20380/GI2018.22, 10.20380/GI2018.223, DOI 10.20380/GI2018.223]
   Bozgeyikli E, 2016, CHI PLAY 2016: PROCEEDINGS OF THE 2016 ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P205, DOI 10.1145/2967934.2968105
   Bruder G, 2009, 3DUI : IEEE SYMPOSIUM ON 3D USER INTERFACES 2009, PROCEEDINGS, P75, DOI 10.1109/3DUI.2009.4811208
   Cohen J., 1988, Statistical power analysis for the behavioral sciences
   Freitag S, 2017, IEEE SYMP 3D USER, P134, DOI 10.1109/3DUI.2017.7893330
   Freitag S, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P119, DOI 10.1109/3DUI.2014.6798852
   Gelman A., 2005, Quality Control Applied Statistics, V20, P295
   HART S G, 1988, P139
   Hart SG., 2006, P HUM FACT ERG SOC A, V50, P904, DOI [10.1177/154193120605000909, DOI 10.1177/154193120605000909]
   Husung M., 2019, P MENSCH COMPUTER 20, P245, DOI [10.1145/3340764.3340779, DOI 10.1145/3340764.3340779]
   Kennedy R.S., 1993, Int. J. Aviat. Psy, P203
   Liu J, 2018, UIST 2018: PROCEEDINGS OF THE 31ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P521, DOI 10.1145/3242587.3242601
   Mary C., 2001, Whitton Sharif Razzaque
   Mauchly JW, 1940, ANN MATH STAT, V11, P204, DOI 10.1214/aoms/1177731915
   Misztal S, 2020, 26 ACM S VIRT REAL S
   Neth CT, 2012, IEEE T VIS COMPUT GR, V18, P1041, DOI 10.1109/TVCG.2011.275
   Nilsson NC, 2018, IEEE COMPUT GRAPH, V38, P44, DOI 10.1109/MCG.2018.111125628
   Schubert T. W., 2003, Z. fur Medienpsychologie, V15, P69, DOI [10.1026//1617-6383.15.2.69, DOI 10.1026//1617-6383.15.2.69]
   SHAPIRO SS, 1965, BIOMETRIKA, V52, P591, DOI 10.2307/2333709
   Steinicke F., 2009, Proceedings of the 6th Symposium on Applied Perception in Graphics and Visualization, P19, DOI 10.1145/1620993.1620998
   Steinicke F, 2010, IEEE T VIS COMPUT GR, V16, P17, DOI 10.1109/TVCG.2009.62
   Suma EA, 2007, 3DUI: IEEE SYMPOSIUM ON 3D USER INTERFACES 2007, PROCEEDINGS, P147
   Suma EA, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P245, DOI 10.1109/VR.2009.4811037
   Usoh M, 1999, COMP GRAPH, P359, DOI 10.1145/311535.311589
   Valve Portal, 2007, About us
   van Linn A, 2017, Gaze teleportation in virtual reality
   von Willich J, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376626
   Wang LL, 2019, IEEE T VIS COMPUT GR, V25, P2083, DOI 10.1109/TVCG.2019.2898782
NR 29
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 9
DI 10.1007/s10055-023-00897-7
PG 16
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA EA1Q3
UT WOS:001136092900001
OA Bronze
DA 2024-08-05
ER

PT J
AU Zhang, XT
   He, WP
   Billinghurst, M
   Qin, YF
   Yang, LX
   Liu, DS
   Wang, ZL
AF Zhang, Xiaotian
   He, Weiping
   Billinghurst, Mark
   Qin, Yunfei
   Yang, Lingxiao
   Liu, Daisong
   Wang, Zenglei
TI Usability of visualizing position and orientation deviations for manual
   precise manipulation of objects in augmented reality
SO VIRTUAL REALITY
LA English
DT Article
DE Augmented reality; Manual manipulation; Precise manipulation;
   Visualization
ID GUIDANCE; DISPLAY
AB Manual precise manipulation of objects is an essential skill in everyday life, and Augmented Reality (AR) is increasingly being used to support such operations. In this study, we investigate whether detailed visualizations of position and orientation deviations are helpful for AR-assisted manual precise manipulation of objects. We developed three AR instructions with different visualizations of deviations: the logical deviation baseline instruction, the precise numerical deviations-based instruction, and the intuitive color-mapped deviations-based instruction. All three instructions visualized the required directions for manipulation and the logical values of whether the object met the accuracy requirements. Additionally, the latter two instructions provided detailed visualizations of deviations through numerical text and color-mapping respectively. A user study was conducted with 18 participants to compare the three AR instructions. The results showed that there were no significant differences found in speed, accuracy, perceived ease-of-use, and perceived workload between the three AR instructions. We found that the visualizations of the required directions for manipulation and the logical values of whether the object met the accuracy requirements were sufficient to guide manual precise manipulation. The detailed visualizations of the real-time deviations could not improve the speed and accuracy of manual precise manipulation, and although they could improve the perceived ease-of-use and user experience, the effects were not significant. Based on the results, several recommendations were provided for designing AR instructions to support precise manual manipulation.
C1 [Zhang, Xiaotian; He, Weiping; Qin, Yunfei; Liu, Daisong; Wang, Zenglei] Northwestern Polytech Univ, Cyber Phys Interact Lab, 127 West Youyi Rd, Xian 710072, Shaanxi, Peoples R China.
   [Billinghurst, Mark] Univ South Australia, Empath Comp Lab, Mawson Lakes Blvd, Adelaide, SA 5095, Australia.
   [Yang, Lingxiao] Univ Washington, Paul G Allen Sch Comp Sci & Engn, 185 Stevens Way, Seattle, WA 98195 USA.
C3 Northwestern Polytechnical University; University of South Australia;
   University of Washington; University of Washington Seattle
RP Zhang, XT; He, WP (corresponding author), Northwestern Polytech Univ, Cyber Phys Interact Lab, 127 West Youyi Rd, Xian 710072, Shaanxi, Peoples R China.
EM 2020100882@mail.nwpu.edu.cn; weiping@nwpu.edu.cn;
   mark.billinghurst@gmail.com; 1480772744@mail.nwpu.edu.cn;
   yang029@uw.edu; dsliu2021@mail.nwpu.edu.cn; wangzenglei@nwpu.edu.cn
RI Billinghurst, Mark/AAJ-4236-2020
OI Billinghurst, Mark/0000-0003-4172-6759
FU National Key Research and Development Program of China
FX Thanks to Prof. He for the inspiration on ideas and innovations. Thanks
   to Prof. Billinghurst for suggestions on writing and experimentation.
   Thanks to Lingxiao Yang and Zenglei Wang for suggestions on writing.
   Thanks to Yunfei Qin and Daisong Liu for their help in the
   experimentation. Thanks to all the volunteers who participated in this
   experiment. Thanks to the anonymous reviewer for their valuable and
   insightful feedback.DAS:The datasets generated during and/or analysed
   during the current study are available from the corresponding author on
   reasonable request.
CR Baroroh DK, 2021, J MANUF SYST, V61, P696, DOI 10.1016/j.jmsy.2020.10.017
   Bellarbi A, 2017, 2017 5TH INTERNATIONAL CONFERENCE ON ELECTRICAL ENGINEERING - BOUMERDES (ICEE-B), DOI 10.1109/ATNAC.2017.8215417
   Brooke J., 1996, SUS-a quick and dirty usability scale, DOI [DOI 10.1201/9781498710411-35, DOI 10.1201/9781498710411]
   Chang RJ., 2016, Int J Autom Technol, V10, P438, DOI [10.20965/ijat.2016.p0438, DOI 10.20965/IJAT.2016.P0438]
   Condino S, 2020, IEEE T BIO-MED ENG, V67, P411, DOI 10.1109/TBME.2019.2914517
   Endo H, 2010, ERGONOMICS, V53, P491, DOI 10.1080/00140130903556336
   Garrido-Jurado S, 2014, PATTERN RECOGN, V47, P2280, DOI 10.1016/j.patcog.2014.01.005
   Gattullo M, 2022, IEEE T VIS COMPUT GR, V28, P1443, DOI 10.1109/TVCG.2020.3014614
   Gavaghan K, 2012, INT J COMPUT ASS RAD, V7, P547, DOI 10.1007/s11548-011-0660-7
   Han BX, 2022, INT J MED ROBOT COMP, V18, DOI 10.1002/rcs.2404
   Harada Y, 2006, 2006 SICE ICASE INT, P5355
   HART S G, 1988, P139
   Heinrich F, 2020, IEEE T VIS COMPUT GR, V26, P3568, DOI 10.1109/TVCG.2020.3023637
   Heinrich F, 2019, HEALTHC TECHNOL LETT, V6, P165, DOI 10.1049/htl.2019.0062
   Hermann T., 2011, The Sonification Handbook
   Katzakis N., 2013, Proceedings of the 11th Asia Pacific Conference on Computer Human Interaction, APCHI'13, (New York, NY, USA), P129
   Kim M, 2019, INT J HUM-COMPUT INT, V35, P1147, DOI 10.1080/10447318.2018.1514163
   Krempien R, 2008, INT J RADIAT ONCOL, V70, P944, DOI 10.1016/j.ijrobp.2007.10.048
   Kunz C, 2019, PROC SPIE, V10951, DOI 10.1117/12.2511720
   Lee CY, 2021, P 2021 ACM S SPAT US, P1
   Lewis JR, 2009, LECT NOTES COMPUT SC, V5619, P94, DOI 10.1007/978-3-642-02806-9_12
   Leydon K, 2001, Sensing the position and orientation of hand-held objects: an overview of techniques, P1
   Liu Y, 2022, IEEE T HAPTICS, V15, P178, DOI 10.1109/TOH.2021.3108855
   Liu YP, 2022, MEASUREMENT, V188, DOI 10.1016/j.measurement.2021.110426
   Mamone V, 2022, IEEE T HUM-MACH SYST, V52, P567, DOI 10.1109/THMS.2021.3129715
   Nomoto A, 2016, PROCEEDINGS OF THE 7TH AUGMENTED HUMAN INTERNATIONAL CONFERENCE (AUGMENTED HUMAN 2016), DOI 10.1145/2875194.2875216
   Novak-Marcincin J, 2014, KEY ENG MATER, V581, P106, DOI 10.4028/www.scientific.net/KEM.581.106
   Ong SK, 2011, CIRP ANN-MANUF TECHN, V60, P1, DOI 10.1016/j.cirp.2011.03.001
   Patrioli L., 2020, Development of a localization system based on aruco markers for a small space platforms test bench
   Petermeijer SM, 2016, IEEE T INTELL TRANSP, V17, P897, DOI 10.1109/TITS.2015.2494873
   Radkowski R, 2015, LECT NOTES COMPUT SC, V9179, P488, DOI 10.1007/978-3-319-21067-4_50
   Palma SR, 2012, IEEE ENG MED BIO, P1409, DOI 10.1109/EMBC.2012.6346203
   Romero-Ramirez FJ, 2018, IMAGE VISION COMPUT, V76, P38, DOI 10.1016/j.imavis.2018.05.004
   Roodaki Hessam, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9900, P378, DOI 10.1007/978-3-319-46720-7_44
   Sauro J, 2012, QUANTIFYING THE USER EXPERIENCE: PRACTICAL STATISTICS FOR USER RESEARCH, P1
   Stenmark M, 2022, J SURG RES, V271, P106, DOI 10.1016/j.jss.2021.10.025
   Tobisková N, 2022, LECT NOTES COMPUT SC, V13318, P329, DOI 10.1007/978-3-031-06015-1_23
   Villani Francesca Pia, 2021, Pattern Recognition. ICPR 2020 International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12661), P461, DOI 10.1007/978-3-030-68763-2_35
   Villanueva AM, 2021, P 2021 CHI C HUM FAC, P1
   Wang X, 2016, ADV ENG INFORM, V30, P406, DOI 10.1016/j.aei.2016.05.004
   Wang Z, 2020, INT J ADV MANUF TECH, V107, P1463, DOI 10.1007/s00170-020-05034-1
   Wang Z, 2021, INT J HUM-COMPUT INT, V37, P1799, DOI 10.1080/10447318.2021.1909278
   Wang Z, 2021, ADV ENG INFORM, V47, DOI 10.1016/j.aei.2021.101250
   Zhang L, 2023, INT J HUM-COMPUT INT, V39, P449, DOI 10.1080/10447318.2022.2041895
   Zhang XT, 2024, INT J HUM-COMPUT INT, V40, P2282, DOI 10.1080/10447318.2022.2158527
NR 45
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD JUL 9
PY 2024
VL 28
IS 3
AR 134
DI 10.1007/s10055-024-01030-y
PG 20
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA YK6O4
UT WOS:001268423300001
OA hybrid
DA 2024-08-05
ER

PT J
AU Oberdörfer, S
   Birnstiel, S
   Latoschik, ME
AF Oberdoerfer, Sebastian
   Birnstiel, Sandra
   Latoschik, Marc Erich
TI Proteus effect or bodily affordance? The influence of virtual high-heels
   on gait behavior
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Embodiment; Proteus Effect; Gait; Intermodal illusion
ID WALKING; ENVIRONMENTS; EMBODIMENT; OWNERSHIP; ILLUSION; REALITY
AB Shoes are an important part of the fashion industry, stereotypically affect our self-awareness as well as external perception, and can even biomechanically modify our gait pattern. Immersive Virtual Reality (VR) enables users not only to explore virtual environments, but also to control an avatar as a proxy for themselves. These avatars can wear any kind of shoe which might similarly affect self-awareness due to the Proteus Effect and even cause a bodily affordance to change the gait pattern. Bodily affordance describes a behavioral change in accordance with the expected constraints of the avatar a user is embodied with. In this article, we present the results of three user studies investigating potential changes in the gait pattern evoked by wearing virtual high-heels. Two user studies targeted female participants and one user study focused male participants. The participants wore either virtual sneakers or virtual high-heels while constantly wearing sneakers or socks in reality. To measure the gait pattern, the participants walked on a treadmill that also was added to the virtual environment. We measured significant differences in stride length and in the flexion of the hips and knees at heel strike and partly at toe off. Also, participants reported to walk more comfortably in the virtual sneakers in contrast to the virtual high-heels. This indicates a strong acceptance of the virtual shoes as their real shoes and hence suggests the existence of a bodily affordance. While sparking a discussion about the boundaries as well as aspects of the Proteus Effect and providing another insight into the effects of embodiment in VR, our results might also be important for researchers and developers.
C1 [Oberdoerfer, Sebastian; Latoschik, Marc Erich] Univ Wurzburg, Human Comp Interact, Wurzburg, Germany.
   [Birnstiel, Sandra] Friedrich Alexander Univ Erlangen Nurnberg, Gamificat Res Grp, Nurnberg, Germany.
C3 University of Wurzburg; University of Erlangen Nuremberg
RP Oberdörfer, S (corresponding author), Univ Wurzburg, Human Comp Interact, Wurzburg, Germany.
EM sebastian.oberdoerfer@uni-wuerzburg.de; sandra.birnstiel@fau.de;
   marc.latoschik@uni-wuerzburg.de
FU Julius-Maximilians-Universitt Wrzburg (3088)
FX No Statement Available
CR [Anonymous], 2020, o3n Studio: o3n Male and Female UMA Races
   [Anonymous], 2021, OptiTrack: OptiTrack
   [Anonymous], 2021, OptiTrack: Unity Plugin
   Banakou D, 2016, FRONT HUM NEUROSCI, V10, DOI 10.3389/fnhum.2016.00601
   Banakou D, 2013, P NATL ACAD SCI USA, V110, P12846, DOI 10.1073/pnas.1306779110
   Bartl A, 2022, INT SYM MIX AUGMENT, P260, DOI 10.1109/ISMAR55827.2022.00041
   Biocca F, 2001, PRESENCE-VIRTUAL AUG, V10, P247, DOI 10.1162/105474601300343595
   Birnstiel S, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P723, DOI 10.1109/VRW55335.2022.00217
   Brown P, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.945800
   Charbonneau P, 2017, 2017 INT C VIRT REH, P1, DOI [10.1109/ICVR.2017.8007535.IEEE, DOI 10.1109/ICVR.2017.8007535.IEEE]
   Dearborn K., 2006, Journal of Dance Education, V6, P109, DOI DOI 10.1080/15290824.2006.10387323
   EBBELING CJ, 1994, J ORTHOP SPORT PHYS, V19, P190, DOI 10.2519/jospt.1994.19.4.190
   Fox J, 2013, COMPUT HUM BEHAV, V29, P930, DOI 10.1016/j.chb.2012.12.027
   IJsselsteijn WA, 2006, PRESENCE-TELEOP VIRT, V15, P455, DOI 10.1162/pres.15.4.455
   Kennedy R.S., 1993, Int. J. Aviat. Psy, P203
   Kilteni K, 2013, P IEEE VIRT REAL ANN
   Kilteni K, 2012, PRESENCE-TELEOP VIRT, V21, P373, DOI 10.1162/PRES_a_00124
   Kocur Martin, 2020, CHI PLAY '20: Proceedings of the Annual Symposium on Computer-Human Interaction in Play, P193, DOI 10.1145/3410404.3414261
   Kocur M, 2022, 28TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, VRST 2022, DOI 10.1145/3562939.3565628
   Kocur M, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445160
   Kushner AM, 2015, STRENGTH COND J, V37, P13, DOI 10.1519/SSC.0000000000000130
   Lakens D, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00863
   Latoschik ME, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P73, DOI 10.1145/2993369.2993399
   Lecuyer A., 2000, Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048), P83, DOI 10.1109/VR.2000.840369
   Lécuyer A, 2009, PRESENCE-TELEOP VIRT, V18, P39, DOI 10.1162/pres.18.1.39
   Lederman SJ, 2011, IEEE T HAPTICS, V4, P273, DOI 10.1109/ToH.2011.2
   Lee SJ, 2008, J APPL PHYSIOL, V104, P747, DOI 10.1152/japplphysiol.01380.2006
   Linder M, 1998, INT J HEALTH SERV, V28, P201, DOI 10.2190/GA2M-FLA2-17FB-V5PE
   Lugrin J.-L., 2015, ICAT EGVE 2015 INT C, P1, DOI DOI 10.2312/EGVE.20151303
   Lugrin JL, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P17, DOI 10.1109/VR.2018.8446229
   Morris PH, 2013, EVOL HUM BEHAV, V34, P176, DOI 10.1016/j.evolhumbehav.2012.11.006
   Oberdörfer S, 2016, SUI'18: PROCEEDINGS OF THE 2018 SYMPOSIUM ON SPATIAL USER INTERACTION, P89, DOI 10.1145/3267782.3267787
   Peck TC, 2013, CONSCIOUS COGN, V22, P779, DOI 10.1016/j.concog.2013.04.016
   Prokop P, 2020, PERS INDIV DIFFER, V152, DOI 10.1016/j.paid.2019.109558
   Reinhard R, 2020, MEDIA PSYCHOL, V23, P293, DOI 10.1080/15213269.2019.1598435
   Roth D, 2016, P IEEE VIRT REAL ANN, P277, DOI 10.1109/VR.2016.7504761
   Rubin M, 2021, SYNTHESE, V199, P10969, DOI 10.1007/s11229-021-03276-4
   Schäfer T, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.00813
   Simonsen EB, 2012, J APPL BIOMECH, V28, P20, DOI 10.1123/jab.28.1.20
   Skarbez R, 2018, ACM COMPUT SURV, V50, DOI 10.1145/3134301
   Slater M, 1997, PRESENCE-VIRTUAL AUG, V6, P603, DOI 10.1162/pres.1997.6.6.603
   Slater M., 1996, VRST'96. Proceedings of the ACM Symposium on Virtual Reality and Technology, P163
   Slater M., 2010, ACM T GRAPHIC, V29, DOI [DOI 10.1145/1833351.1778829, DOI 10.1145/1833349.1778829]
   Slater M, 2009, PHILOS T R SOC B, V364, P3549, DOI 10.1098/rstb.2009.0138
   Spencer SJ, 1999, J EXP SOC PSYCHOL, V35, P4, DOI 10.1006/jesp.1998.1373
   Stanney KM, 1997, PROCEEDINGS OF THE HUMAN FACTORS AND ERGONOMICS SOCIETY 41ST ANNUAL MEETING, 1997, VOLS 1 AND 2, P1138, DOI 10.1177/107118139704100292
   Steptoe W, 2013, IEEE T VIS COMPUT GR, V19, P583, DOI 10.1109/TVCG.2013.32
   Sullivan Gail M, 2012, J Grad Med Educ, V4, P279, DOI 10.4300/JGME-D-12-00156.1
   UN, 2021, About us
   Waltemate T, 2018, IEEE T VIS COMPUT GR, V24, P1643, DOI 10.1109/TVCG.2018.2794629
   Winter C, 2021, J NEUROENG REHABIL, V18, DOI 10.1186/s12984-021-00848-w
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   Wolf E, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P65, DOI 10.1109/VR50410.2021.00027
   Yee N, 2007, HUM COMMUN RES, V33, P271, DOI 10.1111/j.1468-2958.2007.00299.x
NR 54
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR 22
PY 2024
VL 28
IS 2
AR 81
DI 10.1007/s10055-024-00966-5
PG 19
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA MQ5S1
UT WOS:001195112500001
OA hybrid
DA 2024-08-05
ER

PT J
AU Dozio, N
   Bertoni, M
   Ferrise, F
AF Dozio, Nicolo
   Bertoni, Marco
   Ferrise, Francesco
TI Driving emotions: using virtual reality to explore the effect of low and
   high arousal on driver's attention
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Attention; Emotions; ADAS
ID FIELD-OF-VIEW; VISUAL-SEARCH; ANGER; SPEED; MOOD; RISK; FEAR;
   LOCALIZATION; PERFORMANCE; PERCEPTION
AB The role played by emotions and attention is crucial for the development of advanced driver assistance systems that improve safety by flexibly adapting to the current state of the driver. In the present study, we used immersive virtual reality as a testing tool to investigate how different emotional states affect drivers' attention in a divided attention task. Two different emotional states, diversified by valence and arousal, were induced before performing a divided attention task in a driving simulation. The experimental task developed for this study allowed us to explore if and how two different emotional states can affect the way drivers divide their attention between a central driving-related task and a peripheral visual task. Our results showed that scared drivers presented lower reaction times at the central task compared to relaxed drivers. On the contrary, the emotional state did not affect the performance at the peripheral task, which revealed instead a significant effect of the eccentricity at which the visual stimuli were presented, influencing both the accuracy of targets' perception and participants' reaction times.
C1 [Dozio, Nicolo; Ferrise, Francesco] Politecn Milan, Dept Mech Engn, Via La Masa 1, I-20156 Milan, Italy.
   [Bertoni, Marco] Blekinge Tekn Hogskola, Dept Mech Engn, S-37179 Karlskrona, Sweden.
C3 Polytechnic University of Milan
RP Dozio, N (corresponding author), Politecn Milan, Dept Mech Engn, Via La Masa 1, I-20156 Milan, Italy.
EM nicolo.dozio@polimi.it; marco.bertoni@bth.se;
   francesco.ferrise@polimi.it
RI Ferrise, Francesco/C-6502-2008
OI Ferrise, Francesco/0000-0001-8951-8807
FU Politecnico di Milano
FX No Statement Available
CR Abdu R, 2012, TRANSPORT RES F-TRAF, V15, P575, DOI 10.1016/j.trf.2012.05.007
   Adam JJ, 2008, PSYCHOL RES-PSYCH FO, V72, P433, DOI 10.1007/s00426-007-0126-2
   BALL K, 1993, INVEST OPHTH VIS SCI, V34, P3110
   BALL KK, 1988, J OPT SOC AM A, V5, P2210, DOI 10.1364/JOSAA.5.002210
   BRADLEY MM, 1994, J BEHAV THER EXP PSY, V25, P49, DOI 10.1016/0005-7916(94)90063-9
   Cao S, 2020, VIRTUAL REAL-LONDON, V24, P503, DOI 10.1007/s10055-019-00412-x
   CARRASCO M, 1995, PERCEPT PSYCHOPHYS, V57, P1241, DOI 10.3758/BF03208380
   Carrasco M, 2003, NAT NEUROSCI, V6, P699, DOI 10.1038/nn1079
   Chan M, 2015, SAFETY SCI, V72, P302, DOI 10.1016/j.ssci.2014.10.002
   Clay OJ, 2005, OPTOMETRY VISION SCI, V82, P724, DOI 10.1097/01.opx.0000175009.08626.65
   Clement P, 2022, ENERGIES, V15, DOI 10.3390/en15030781
   Cutello CA, 2021, RISK ANAL, V41, P1662, DOI 10.1111/risa.13643
   de Gelder E, 2017, IEEE INT VEH SYM, P589, DOI 10.1109/IVS.2017.7995782
   DEFFENBACHER JL, 1994, PSYCHOL REP, V74, P83, DOI 10.2466/pr0.1994.74.1.83
   Deffenbacher JL, 2003, BEHAV RES THER, V41, P701, DOI 10.1016/S0005-7967(02)00046-3
   Deffenbacher JL, 2002, BEHAV RES THER, V40, P717, DOI 10.1016/S0005-7967(01)00063-8
   DePasquale JP, 2001, J SAFETY RES, V32, P1, DOI 10.1016/S0022-4375(00)00050-5
   Diemer J, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00026
   Dingus TA, 2016, P NATL ACAD SCI USA, V113, P2636, DOI 10.1073/pnas.1513271113
   Dominguez Juan, 2023, Journal of Ambient Intelligence and Humanized Computing, P9049, DOI 10.1007/s12652-022-04410-x
   Dozio N, 2022, PRESENCE-VIRTUAL AUG, V31, P5, DOI 10.1162/pres_a_00373
   Dozio N, 2022, INT J HUM-COMPUT ST, V162, DOI 10.1016/j.ijhcs.2022.102791
   Dozio N, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-03380-y
   EDWARDS DC, 1974, J EXP PSYCHOL, V102, P244, DOI 10.1037/h0035859
   Edwards JD, 2006, ARCH CLIN NEUROPSYCH, V21, P275, DOI 10.1016/j.acn.2006.03.001
   European Road Safety Observatory. Brussels European Commission Directorate General for Transport, 2021, Annual statistical report on road safety in the EU 2020
   Faul F, 2007, BEHAV RES METHODS, V39, P175, DOI 10.3758/BF03193146
   Finucane AM, 2011, EMOTION, V11, P970, DOI 10.1037/a0022574
   Gerritsen C, 2008, PERCEPT PSYCHOPHYS, V70, P1047, DOI 10.3758/PP.70.6.1047
   Huo FR, 2022, FRONT PSYCHOL, V13, DOI 10.3389/fpsyg.2022.867673
   Jallais C, 2014, TRANSPORT RES F-TRAF, V23, P125, DOI 10.1016/j.trf.2013.12.023
   Jallais C, 2010, BEHAV RES METHODS, V42, P318, DOI 10.3758/BRM.42.1.318
   Jeon M., 2013, Proceedings of the Human Factors and Ergonomics Society, P1849, DOI DOI 10.1177/1541931213571413
   Jeon M, 2016, INT J HUM-COMPUT INT, V32, P777, DOI 10.1080/10447318.2016.1198524
   Jeon Myounghoon, 2011, P HUMAN FACTORS ERGO, V55, P1889
   Jones MP, 2014, ACCIDENT ANAL PREV, V73, P296, DOI 10.1016/j.aap.2014.09.019
   Kadoya Y, 2021, TRANSPORT RES F-TRAF, V79, P205, DOI 10.1016/j.trf.2021.04.020
   Kuhbandner C, 2009, SOC COGN AFFECT NEUR, V4, P286, DOI 10.1093/scan/nsp010
   Lamanuzzi M, 2020, 2020 AEIT INT ANN C, P1
   Lewis I, 2008, TRANSPORT RES F-TRAF, V11, P403, DOI 10.1016/j.trf.2008.03.003
   Lewis I.M., 2007, International Journal of Behavioral and Consultation Therapy, V3, P203, DOI DOI 10.1037/H0100799
   Liu YQ, 2022, INT J ENV RES PUB HE, V19, DOI 10.3390/ijerph19095059
   Lu JY, 2013, J SAFETY RES, V45, P65, DOI 10.1016/j.jsr.2013.01.009
   Malone S, 2021, FRONT PSYCHOL, V12, DOI 10.3389/fpsyg.2021.647723
   Öhman A, 2001, J EXP PSYCHOL GEN, V130, P466, DOI 10.1037/0096-3445.130.3.466
   Owsley C, 2013, VISION RES, V90, P52, DOI 10.1016/j.visres.2012.11.014
   Pêcher C, 2009, SAFETY SCI, V47, P1254, DOI 10.1016/j.ssci.2009.03.011
   Pessoa L, 2008, NAT REV NEUROSCI, V9, P148, DOI 10.1038/nrn2317
   Precht L, 2017, TRANSPORT RES F-TRAF, V45, P75, DOI 10.1016/j.trf.2016.10.019
   Rogé J, 2002, PERCEPT MOTOR SKILL, V95, P118
   SANDERS A F, 1970, Ergonomics, V13, P101, DOI 10.1080/00140137008931124
   Scherer K. R., 2001, Appraisal processes in emotion, DOI [10.1093/oso/9780195130072.001.0001, DOI 10.1093/OSO/9780195130072.001.0001]
   SEKULER R, 1986, J OPT SOC AM A, V3, P864, DOI 10.1364/JOSAA.3.000864
   Siebke C, 2023, IEEE T INTELL TRANSP, V24, P1419, DOI 10.1109/TITS.2022.3220961
   Smith C. A., 1990, Handbook of personality: Theory and research, V21, P609
   Soares SC, 2009, J ANXIETY DISORD, V23, P136, DOI 10.1016/j.janxdis.2008.05.002
   Staugaard CF, 2016, NEUROPSYCHOLOGIA, V92, P69, DOI 10.1016/j.neuropsychologia.2016.06.020
   Steinhauser K, 2018, TRANSPORT RES F-TRAF, V59, P150, DOI 10.1016/j.trf.2018.08.012
   Stephens AN, 2019, TRANSPORT RES F-TRAF, V64, P472, DOI 10.1016/j.trf.2019.06.002
   Stewart T, 2022, Report No. DOT HS 813 266
   Westermann R, 1996, EUR J SOC PSYCHOL, V26, P557, DOI 10.1002/(SICI)1099-0992(199607)26:4<557::AID-EJSP769>3.0.CO;2-4
   Zepf S, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3388790
   Zhang Q, 2022, ACCIDENT ANAL PREV, V171, DOI 10.1016/j.aap.2022.106664
NR 63
TC 0
Z9 0
U1 10
U2 10
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 51
DI 10.1007/s10055-024-00950-z
PG 11
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA JC4I7
UT WOS:001170940200004
OA hybrid
DA 2024-08-05
ER

PT J
AU Laine, J
   Rastas, E
   Seitamaa, A
   Hakkarainen, K
   Korhonen, T
AF Laine, Joakim
   Rastas, Elisa
   Seitamaa, Aino
   Hakkarainen, Kai
   Korhonen, Tiina
TI Immersive virtual reality for complex skills training: content analysis
   of experienced challenges
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Immersive learning theory; Complex skill; Autonomous
   training; Challenge; Design implication
ID EDUCATION
AB This study aimed to examine the challenges that adult participants experienced in immersive virtual reality (I-VR). Practitioners have indicated that some challenges persist from trainee to trainee and scholars have called for the design and development of virtual reality (VR) applications based on learning theories. Thus, we examined challenges immersed learners experienced during self-discovery of game mechanics and assembly task within an early-development I-VR program. We clarified the immersive learning phenomenon by studying the self-reported problem statements from 168 university students and staff. They used an HTC Vive Pro Eye device and a custom-built software. Through an iterative content analysis of post-survey and video-stimulated recall interviews, we retrieved 481 problem statements from the participants. As a result, we derived and detailed 89 challenges, 22 component features, 11 components, and 5 principal factors of immersive learning. The most cited components that the participants found challenging were the use of controllers and functions, reciprocal software interaction, spatial and navigational constraints, relevance realisation, and learner capabilities. Closer inspection of the quantified data revealed that the participants without digital gaming experience reported relatively more hardware-related problem statements. The findings regarding the constraints of immersive learning helped clarify the various actants involved in immersive learning. In this paper, we provide a design implication summary for VR application developers. Further research on theory-based development and design implications in various immersive training settings is needed.
C1 [Laine, Joakim; Rastas, Elisa; Seitamaa, Aino; Hakkarainen, Kai; Korhonen, Tiina] Univ Helsinki, Fac Educ, Helsinki, Finland.
C3 University of Helsinki
RP Laine, J (corresponding author), Univ Helsinki, Fac Educ, Helsinki, Finland.
EM joakim.x.laine@helsinki.fi
OI Laine, Joakim/0000-0002-3407-6551; Hakkarainen, Kai/0000-0003-3507-7537;
   Korhonen, Tiina/0000-0003-2875-4915
FU Suomen Kulttuurirahasto,Finland
FX We thank all participants who were involved in the data collection and
   contributed to this research. Furthermore, we want to thank Timo
   Lindqvist and Henri Peltonen for their invaluable technical inputs and
   setups, Laura Salo for helping with the GDPR compliance and data
   collection preparations, Noora Laakso for supporting us in the
   statistical analysis and survey forms, Hanna Reinius for supporting us
   in the visualisation of the study results, and the Reviewers for
   providing invaluable advice.
CR Abidi MH, 2019, INT J ADV MANUF TECH, V105, P3743, DOI 10.1007/s00170-019-03801-3
   Agrawal S, 2020, J AUDIO ENG SOC, V68, P404, DOI 10.17743/jaes.2020.0039
   Barkokebas R., 2019, ISARC P INT S AUT RO, P796
   Beck D, 2020, J UNIVERS COMPUT SCI, V26, P1043
   Carruth DW, 2017, 2017 15TH IEEE INTERNATIONAL CONFERENCE ON EMERGING ELEARNING TECHNOLOGIES AND APPLICATIONS (ICETA 2017), P75
   Chen XR, 2023, VIRTUAL REAL-LONDON, V27, P1717, DOI 10.1007/s10055-022-00720-9
   Coburn CE, 2016, EDUC RESEARCHER, V45, P48, DOI 10.3102/0013189X16631750
   Dempsey NP, 2010, QUAL SOCIOL, V33, P349, DOI 10.1007/s11133-010-9157-x
   Dobrowolski P., 2021, FRONTIERS VIRTUAL RE, P40, DOI DOI 10.3389/FRVIR.2020.604008
   Elo S, 2008, J ADV NURS, V62, P107, DOI 10.1111/j.1365-2648.2007.04569.x
   Elor A, 2020, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.585993
   Fowler C, 2015, BRIT J EDUC TECHNOL, V46, P412, DOI 10.1111/bjet.12135
   Gutiérrez T, 2010, 2010 IEEE RO-MAN, P428, DOI 10.1109/ROMAN.2010.5598643
   Heutte J, 2021, FRONT PSYCHOL, V12, DOI 10.3389/fpsyg.2021.828027
   Ho JCF, 2020, MULTIMODAL TECHNOLOG, V4, DOI 10.3390/mti4010001
   Huber T, 2018, INT J COMPUT ASS RAD, V13, P281, DOI 10.1007/s11548-017-1686-2
   Huber T, 2017, SURG ENDOSC, V31, P4472, DOI 10.1007/s00464-017-5500-6
   Illeris K., 2018, Contemporary theories of learning, P1
   Jensen L, 2018, EDUC INF TECHNOL, V23, P1515, DOI 10.1007/s10639-017-9676-0
   Jerald J., 2015, Morgan Claypool, DOI [10.1109/vr.2017.7892361, DOI 10.1109/VR.2017.7892361]
   Johnson-Glenberg MC, 2021, J COMPUT ASSIST LEAR, V37, P1263, DOI 10.1111/jcal.12567
   Johnson-Glenberg MC, 2018, FRONT ROBOT AI, V5, DOI 10.3389/frobt.2018.00081
   Kao Dominic, 2021, Proceedings of the ACM on Human-Computer Interaction, V5, DOI 10.1145/3474661
   Korhonen T., 2022, AI in learning: designing the future, P195, DOI [10.1007/978-3-031-09687-7_12, DOI 10.1007/978-3-031-09687-7_12]
   Kozlowski S.W. J., 2004, SCALED WORLDS DEV VA, P75
   Laine J., 2022, Int. J. Technol. Educ. Sci, V6, P178, DOI [10.46328/ijtes.348, DOI 10.46328/IJTES.348]
   Laine J, 2023, COGENT EDUC, V10, DOI 10.1080/2331186X.2023.2196896
   Legault L., 2017, Encyclopedia of personality and individual differences p, P1, DOI [DOI 10.1007/978-3-319-28099-8_1123-1, 10.1007/978-3-319-28099-81123-1, DOI 10.1007/978-3-319-28099-81123-1]
   Li M, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P566, DOI [10.1109/VR46266.2020.00-26, 10.1109/VR46266.2020.1581301697128]
   Makransky G, 2019, LEARN INSTR, V60, P225, DOI 10.1016/j.learninstruc.2017.12.007
   Martirosov S, 2022, VIRTUAL REAL-LONDON, V26, P15, DOI 10.1007/s10055-021-00507-4
   Mayer R. E., 2005, The Cambridge Handbook of Multimedia Learning, P31, DOI [10.1017/CBO9781139547369.005, DOI 10.1017/CBO9780511816819.004]
   Nguyen N.T., 2013, VIDEO STIMULATED REC
   Obukhov AD, 2023, VIRTUAL REAL-LONDON, V27, P735, DOI 10.1007/s10055-022-00687-7
   Overton AL, 2023, Assessing adult learning through immersive virtual reality technology
   Pastel S, 2023, MULTIMED TOOLS APPL, V82, P4181, DOI 10.1007/s11042-022-13474-y
   Petukhov I, 2017, 2017 INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND COMMUNICATIONS TECHNOLOGIES (ICISCT) - APPLICATIONS, TRENDS AND OPPORTUNITIES
   Pitkanen Jori., 2015, Game Research Methods, P117
   Pulijala Y, 2018, INT J ORAL MAX SURG, V47, P1199, DOI 10.1016/j.ijom.2018.01.005
   Radhakrishnan U, 2021, BEHAV INFORM TECHNOL, V40, P1310, DOI 10.1080/0144929X.2021.1954693
   Radianti J, 2020, COMPUT EDUC, V147, DOI 10.1016/j.compedu.2019.103778
   Ricca A, 2021, VIRTUAL REAL-LONDON, V25, P191, DOI 10.1007/s10055-020-00445-7
   Rietveld E., 2018, Oxford handbook of 4e cognition, P41, DOI DOI 10.1093/OXFORDHB/9780198735410.013.3
   Sagnier C., 2021, Digital transformations in the challenge of activity and work: understanding and supporting technological changes, P31, DOI [10.1002/9781119808343.ch3, DOI 10.1002/9781119808343.CH3]
   Salmon P, 2010, THEOR ISS ERGON SCI, V11, P504, DOI 10.1080/14639220903165169
   Southgate Erica, 2019, International Journal of Child-Computer Interaction, V19, P19, DOI 10.1016/j.ijcci.2018.10.002
   Steffen JH, 2019, J MANAGE INFORM SYST, V36, P683, DOI 10.1080/07421222.2019.1628877
   Vasarainen M, 2021, International Journal of Virtual Reality, V21, P1, DOI DOI 10.20870/IJVR.2021.21.2.4620
   Waller D, 1998, PRESENCE-TELEOP VIRT, V7, P129, DOI 10.1162/105474698565631
   Wheeler SG, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.671664
   Wong EYC, 2023, VIRTUAL REAL-LONDON, V27, P2149, DOI 10.1007/s10055-023-00793-0
   Xie B, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.645153
   Zahabi M, 2020, VIRTUAL REAL-LONDON, V24, P725, DOI 10.1007/s10055-020-00434-w
   Zinchenko YP, 2020, NEW IDEAS PSYCHOL, V59, DOI 10.1016/j.newideapsych.2020.100786
NR 54
TC 0
Z9 0
U1 8
U2 8
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 61
DI 10.1007/s10055-024-00955-8
PG 26
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA JV9Q8
UT WOS:001176059900003
OA hybrid
DA 2024-08-05
ER

PT J
AU Higgins, S
   Alcock, S
   De Aveiro, B
   Daniels, W
   Farmer, H
   Besharati, S
AF Higgins, Sarah
   Alcock, Stephanie
   De Aveiro, Bianca
   Daniels, William
   Farmer, Harry
   Besharati, Sahba
TI Perspective matters: a systematic review of immersive virtual reality to
   reduce racial prejudice
SO VIRTUAL REALITY
LA English
DT Article
DE Race; Racial prejudice; Immersive virtual reality; Embodiment;
   Perspective-taking; Implicit bias
ID IMPLICIT ASSOCIATION TEST; INDIVIDUAL-DIFFERENCES; SELF; BIAS;
   COGNITION; SKIN; IAT; DISCRIMINATION; METAANALYSIS; EMBODIMENT
AB In the wake of the COVID-19 pandemic and the rise of social justice movements, increased attention has been directed to levels of intergroup tension worldwide. Racial prejudice is one such tension that permeates societies and creates distinct inequalities at all levels of our social ecosystem. Whether these prejudices are present explicitly (directly or consciously) or implicitly (unconsciously or automatically), manipulating body ownership by embodying an avatar of another race using immersive virtual reality (IVR) presents a promising approach to reducing racial bias. Nevertheless, research findings are contradictory, which is possibly attributed to variances in methodological factors across studies. This systematic review, therefore, aimed to identify variables and methodological variations that may underlie the observed discrepancies in study outcomes. Adhering to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, this systematic review encompassed 12 studies that employed IVR and embodiment techniques to investigate racial attitudes. Subsequently, two mini meta-analyses were performed on four and five of these studies, respectively - both of which utilised the Implicit Association Test (IAT) as a metric to gauge these biases. This review demonstrated that IVR allows not only the manipulation of a sense of body ownership but also the investigation of wider social identities. Despite the novelty of IVR as a tool to help understand and possibly reduce racial bias, our review has identified key limitations in the existing literature. Specifically, we found inconsistencies in the measures and IVR equipment and software employed, as well as diversity limitations in demographic characteristics within both the sampled population and the embodiment of avatars. Future studies are needed to address these critical shortcomings. Specific recommendations are suggested, these include: (1) enhancing participant diversity in terms of the sample representation and by integrating ethnically diverse avatars; (2) employing multi-modal methods in assessing embodiment; (3) increasing consistency in the use and administration of implicit and explicit measures of racial prejudice; and (4) implementing consistent approaches in using IVR hardware and software to enhance the realism of the IVR experience.
C1 [Higgins, Sarah; Alcock, Stephanie; De Aveiro, Bianca; Besharati, Sahba] Univ Witwatersrand, Sch Human & Community Dev, Dept Psychol, Johannesburg, South Africa.
   [Daniels, William] Univ Witwatersrand, Sch Physiol, Johannesburg, South Africa.
   [Farmer, Harry] Univ Greenwich, Inst Lifecourse Dev, Sch Human Sci, London, England.
C3 University of Witwatersrand; University of Witwatersrand; University of
   Greenwich
RP Besharati, S (corresponding author), Univ Witwatersrand, Sch Human & Community Dev, Dept Psychol, Johannesburg, South Africa.; Farmer, H (corresponding author), Univ Greenwich, Inst Lifecourse Dev, Sch Human Sci, London, England.
EM sarahhiggins262@gmail.com; stephanie.alcock@wits.ac.za;
   biancadeaveiro@gmail.com; William.Daniels@wits.ac.za;
   h.farmer@gre.ac.uk; sahba@besharati.com
RI Alcock, Stephanie/KUD-5860-2024
OI Alcock, Stephanie/0000-0002-9655-0909
FU University of the Witwatersrand; CIFAR Azrieli Global Scholar in the
   Brain, Mind and Consciousness Program - Oppenheimer Memorial Trust
   [150745]; National Research Foundation of South Africa; National
   Institute for The Humanities and Social Sciences
FX SB is a CIFAR Azrieli Global Scholar in the Brain, Mind and
   Consciousness Program and is funded by the Oppenheimer Memorial Trust
   and the National Research Foundation of South Africa (grant number:
   150745). This work is based on the research supported by the National
   Institute for The Humanities and Social Sciences to SF.
CR Alvidrez S, 2020, ANN REV CYBERTHERAPY, V18, P89
   Anthes C., 2016, 2016 IEEE AER C
   Banakou D, 2020, ROY SOC OPEN SCI, V7, DOI 10.1098/rsos.201848
   Banakou D, 2016, FRONT HUM NEUROSCI, V10, DOI 10.3389/fnhum.2016.00601
   Bar-Haim Y, 2006, PSYCHOL SCI, V17, P159, DOI 10.1111/j.1467-9280.2006.01679.x
   Barbot B, 2020, COMPUT HUM BEHAV, V111, DOI 10.1016/j.chb.2020.106431
   Barden J, 2004, J PERS SOC PSYCHOL, V87, P5, DOI 10.1037/0022-3514.87.1.5
   Bartolucci AA, 2010, EVIDENCE-BASED PRACTICE: TOWARD OPTIMIZING CLINICAL OUTCOMES, P17, DOI 10.1007/978-3-642-05025-1_2
   BATSON CD, 1987, J PERS, V55, P19, DOI 10.1111/j.1467-6494.1987.tb00426.x
   Behm-Morawitz E, 2016, COMMUN MONOGR, V83, P396, DOI 10.1080/03637751.2015.1128556
   Besharati S, 2023, NAT COMMUN, V14, DOI 10.1038/s41467-023-43943-3
   Blanke O, 2015, NEURON, V88, P145, DOI 10.1016/j.neuron.2015.09.029
   Blanke O, 2012, NAT REV NEUROSCI, V13, P556, DOI 10.1038/nrn3292
   Blanton H, 2006, J EXP SOC PSYCHOL, V42, P192, DOI 10.1016/j.jesp.2005.07.003
   Botvinick M, 1998, NATURE, V391, P756, DOI 10.1038/35784
   Chen VHH, 2021, FRONT PSYCHOL, V12, DOI 10.3389/fpsyg.2021.705574
   Chowdhury TI, 2021, IEEE T VIS COMPUT GR, V27, P3079, DOI 10.1109/TVCG.2019.2958332
   Cosmides L, 2003, TRENDS COGN SCI, V7, P173, DOI 10.1016/S1364-6613(03)00057-3
   D'Errico F, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10041196
   Dasgupta N, 2001, J PERS SOC PSYCHOL, V81, P800, DOI 10.1037/0022-3514.81.5.800
   Davis M. H., 1980, Catalog of Selected Documents in Psychology, V10, P85, DOI DOI 10.1037/0022-3514.44.1.113
   Demeco A, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23031712
   Do TD, 2023, FRONT VIRTUAL REAL, V4, DOI 10.3389/frvir.2023.1248915
   Dotsch R, 2008, J EXP SOC PSYCHOL, V44, P1194, DOI 10.1016/j.jesp.2008.03.003
   Dovidio J.F., 2010, Handbook of social psychology, V2, P1084, DOI DOI 10.1002/9780470561119.SOCPSY002029
   Dovidio JF, 2004, PERS SOC PSYCHOL B, V30, P1537, DOI 10.1177/0146167204271177
   Durrheim K, 2024, PERSPECT PSYCHOL SCI, V19, P244, DOI 10.1177/17456916231182922
   Ehrsson H.H., 2012, HDB MULTISENSORY PRO, P775, DOI DOI 10.1126/SCIENCE.1097011
   Farmer H, 2017, SOC JUSTICE RES, V30, P323, DOI 10.1007/s11211-017-0294-1
   Farmer H, 2012, CONSCIOUS COGN, V21, P1242, DOI 10.1016/j.concog.2012.04.011
   FitzGerald C, 2017, BMC MED ETHICS, V18, DOI 10.1186/s12910-017-0179-8
   Forscher PS, 2019, J PERS SOC PSYCHOL, V117, P522, DOI 10.1037/pspa0000160
   Galinsky AD, 2000, J PERS SOC PSYCHOL, V78, P708, DOI 10.1037//0022-3514.78.4.708
   Gallagher S, 2000, TRENDS COGN SCI, V4, P14, DOI 10.1016/S1364-6613(99)01417-5
   Gawronski B, 2017, PERS SOC PSYCHOL B, V43, P300, DOI 10.1177/0146167216684131
   Greenwald AG, 1998, J PERS SOC PSYCHOL, V74, P1464, DOI 10.1037/0022-3514.74.6.1464
   Greenwald AG, 2022, BEHAV RES METHODS, V54, P1161, DOI 10.3758/s13428-021-01624-3
   Greenwald AG, 2015, J PERS SOC PSYCHOL, V108, P553, DOI 10.1037/pspa0000016
   Greenwald AG, 2009, J PERS SOC PSYCHOL, V97, P17, DOI 10.1037/a0015575
   Groom V, 2009, SOC INFLUENCE, V4, P231, DOI 10.1080/15534510802643750
   Guy M, 2023, FRONT VIRTUAL REAL, V4, DOI 10.3389/frvir.2023.1141683
   Harjunen VJ, 2022, SOC COGN AFFECT NEUR, V17, P673, DOI 10.1093/scan/nsab113
   Hasler BS, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0174965
   Hassard AS, 2023, Effort P
   Henrich J, 2010, NATURE, V466, P29, DOI 10.1038/466029a
   Hodgson Paula., 2019, Augmented reality and virtual reality: The power of AR and VR for business (Progress in IS), P161, DOI [DOI 10.1007/978-3-030-06246-012, 10.1007/978-3-030-06246-0_12, DOI 10.1007/978-3-030-06246-0_12]
   Ito TA, 2015, J PERS SOC PSYCHOL, V108, P187, DOI 10.1037/a0038557
   Jacob-Dazarola R, 2016, EMOTION MEASUREMENT, P101, DOI 10.1016/B978-0-08-100508-8.00005-9
   Jost JT, 2004, POLIT PSYCHOL, V25, P881, DOI 10.1111/j.1467-9221.2004.00402.x
   Kurdi B, 2019, AM PSYCHOL, V74, P569, DOI 10.1037/amp0000364
   Lewis E, 2010, PHENOMENOL COGN SCI, V9, P317, DOI 10.1007/s11097-010-9154-2
   Lowery BS, 2001, J PERS SOC PSYCHOL, V81, P842, DOI 10.1037//0022-3514.81.5.842
   Meister L, 2015, TRENDS COGN SCI, V19, P6, DOI 10.1016/j.tics.2014.11.001
   Mitchell G., 2017, PSYCHOL SCI SCRUTINY, P164, DOI DOI 10.1002/9781119095910.CH10
   Nock MK, 2010, PSYCHOL SCI, V21, P511, DOI 10.1177/0956797610364762
   Oswald FL, 2015, J PERS SOC PSYCHOL, V108, P562, DOI 10.1037/pspa0000023
   Oswald FL, 2013, J PERS SOC PSYCHOL, V105, P171, DOI 10.1037/a0032734
   Ouzzani M, 2016, SYST REV-LONDON, V5, DOI 10.1186/s13643-016-0384-4
   Patané I, 2020, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.510787
   Peck TC, 2021, IEEE COMPUT GRAPH, V41, P133, DOI 10.1109/MCG.2021.3113455
   Peck TC, 2021, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.575943
   Peck TC, 2013, CONSCIOUS COGN, V22, P779, DOI 10.1016/j.concog.2013.04.016
   Pettigrew TF, 2008, EUR J SOC PSYCHOL, V38, P922, DOI 10.1002/ejsp.504
   Poggi I, 2010, P 2 INT WORKSH SOC S, P21, DOI DOI 10.1145/1878116.1878124
   Rehman MSU, 2023, BUILDINGS-BASEL, V13, DOI 10.3390/buildings13051123
   Richardson W S, 1995, ACP J Club, V123, pA12
   Roberts SO, 2020, PERSPECT PSYCHOL SCI, V15, P1295, DOI 10.1177/1745691620927709
   Rossen B, 2008, LECT NOTES COMPUT SC, V5208, P237
   Roth D., 2017, C HUM FACTORS COMPUT, VPart F1276, P2875, DOI DOI 10.1145/3027063.3053272
   Salmanowitz N, 2018, J LAW BIOSCI, V5, P174, DOI 10.1093/jlb/lsy005
   Schimmack U, 2021, Replicability Index
   Schnabel K, 2008, EUR J PSYCHOL ASSESS, V24, P210, DOI 10.1027/1015-5759.24.4.210
   Schwarzer G, 2015, Meta-analysis with R, V4784
   Sears DO, 2005, ADV EXP SOC PSYCHOL, V37, P95, DOI 10.1016/S0065-2601(05)37002-X
   Silverio SA, 2022, INT J QUAL METH, V21, DOI 10.1177/16094069221124739
   Slater M, 2017, SMART COMPUT INTELL, P19, DOI 10.1007/978-981-10-5490-7_2
   Slater M, 2016, FRONT ROBOT AI, V3, DOI 10.3389/frobt.2016.00074
   Slater M, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010564
   Tassinari M, 2022, PLOS ONE, V17, DOI 10.1371/journal.pone.0270748
   Tassinari M, 2022, FRONT PSYCHOL, V12, DOI 10.3389/fpsyg.2021.815497
   Thériault R, 2021, Q J EXP PSYCHOL, V74, P2057, DOI 10.1177/17470218211024826
   Tufanaru C., 2017, Joanna Briggs Institute Reviewers Manual, DOI DOI 10.46658/JBIMES-20-04
   Wang Y, 2020, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.01086
   Weidner F, 2023, IEEE T VIS COMPUT GR, V29, P2596, DOI 10.1109/TVCG.2023.3247072
   Westen D, 2003, J PERS SOC PSYCHOL, V84, P608, DOI 10.1037/0022-3514.84.3.608
   Wilson CJ, 2015, COMPUT MATH METHOD M, V2015, DOI 10.1155/2015/151702
   Yee N, 2007, HUM COMMUN RES, V33, P271, DOI 10.1111/j.1468-2958.2007.00299.x
NR 87
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD JUN 21
PY 2024
VL 28
IS 3
AR 125
DI 10.1007/s10055-024-01024-w
PG 21
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA UY1T7
UT WOS:001251540400001
OA hybrid
DA 2024-08-05
ER

PT J
AU Georgiou, F
   Kawai, C
   Schäffer, B
   Pieren, R
AF Georgiou, Fotis
   Kawai, Claudia
   Schaffer, Beat
   Pieren, Reto
TI Replicating outdoor environments using VR and ambisonics: a methodology
   for accurate audio-visual recording, processing and reproduction
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Ambisonics; Decoder equalisation; Perceptual evaluation
AB This paper introduces a methodology tailored to capture, post-process, and replicate audio-visual data of outdoor environments (urban or natural) for VR experiments carried out within a controlled laboratory environment. The methodology consists of 360 degrees \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$<^>\circ$$\end{document} video and higher order ambisonic (HOA) field recordings and subsequent calibrated spatial sound reproduction with a spherical loudspeaker array and video played back via a head-mounted display using a game engine and a graphical user interface for a perceptual experimental questionnaire. Attention was given to the equalisation and calibration of the ambisonic microphone and to the design of different ambisonic decoders. A listening experiment was conducted to evaluate four different decoders (one 2D first-order ambisonic decoder and three 3D third-order decoders) by asking participants to rate the relative (perceived) realism of recorded outdoor soundscapes reproduced with these decoders. The results showed that the third-order decoders were ranked as more realistic.
C1 [Georgiou, Fotis; Kawai, Claudia; Schaffer, Beat; Pieren, Reto] Empa, Lab Acoust Noise Control, Swiss Fed Labs Mat Sci & Technol, Uberlandstr 129, CH-8600 Dubendorf, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Swiss Federal
   Laboratories for Materials Science & Technology (EMPA)
RP Georgiou, F (corresponding author), Empa, Lab Acoust Noise Control, Swiss Fed Labs Mat Sci & Technol, Uberlandstr 129, CH-8600 Dubendorf, Switzerland.
EM fotis.georgiou@empa.ch; claudia.kawai@empa.ch; beat.schaeffer@empa.ch;
   reto.pieren@empa.ch
FU Schweizerischer Nationalfonds zur Frderung der Wissenschaftlichen
   Forschung [CRSII5_193847/1]; Swiss National Science Foundation; Swiss
   National Science Foundation (SNF) [CRSII5_193847] Funding Source: Swiss
   National Science Foundation (SNF)
FX The study is funded by the Swiss National Science Foundation (project
   RESTORE, grant number CRSII5_193847/1). We would also like to thank Dr.
   Archontis Politis for the valuable discussion and his advice on
   ambisonics. Finally, we are grateful to the participants of the
   listening experiments.
CR [Anonymous], 2021, EmpAIR
   Benjamin E, 2014, Audio Engineering Society-121st Convention, V3, P1247
   Brimijoin WO, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0083068
   Daniel J, 2000, Representation de champs acoustiques, application a la transmission et a la reproduction de scenes sonores complexes dans un contexte multimedia
   Davies WJ, 2014, ACTA ACUST UNITED AC, V100, P285, DOI 10.3813/AAA.918708
   Dolby LaboratoriesInc, 2005, Tech. rep
   Favrot S, 2010, A loudspeaker-based room auralization system for auditory perception research
   Frank M., 2008, 25 TONMEISTERTAGUNG
   Gerzon M.A., 1975, AUD ENG SOC 50 CONV
   Gerzon MA, 1992, AUD ENG SOC 92 CONV, P1
   Guastavino C, 2005, ACTA ACUST UNITED AC, V91, P333
   Guillemaut JY, 2010, IEEE IMAGE PROC, P9, DOI 10.1109/ICIP.2010.5652901
   Hardin RH, 1996, DISCRETE COMPUT GEOM, V15, P429, DOI 10.1007/BF02711518
   Heller A, 2014, LIN AUD C
   Heller AJ, 2012, LIN AUD C
   Hong JY, 2019, BUILD ENVIRON, V149, P1, DOI 10.1016/j.buildenv.2018.12.004
   Institutfur Rundfunktechnik, 1995, Tech. rep
   International Organizationof Standardization, 2003, Tech. rep
   Kawai C, 2023, 14 ICBEN C NOIS PUBL
   Kearney G, 2015, AUD ENG SOC 139 CONV
   Kern AC, 2020, FRONT ROBOT AI, V7, DOI 10.3389/frobt.2020.00020
   Knaust T, 2022, VIRTUAL REAL-LONDON, V26, P925, DOI 10.1007/s10055-021-00595-2
   Kuntz M, 2023, J ACOUST SOC AM, V154, P1882, DOI 10.1121/10.0021066
   McCormack L, 2019, 2019 AES INTERNATIONAL CONFERENCE ON IMMERSIVE AND INTERACTIVE AUDIO
   McKenzie T, 2021, Tech. rep
   McKenzie T, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8101956
   Mostajeran F, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-83277-y
   Politis A., 2016, MICROPHONE ARRAY PRO
   Politis A, 2017, IEEE WORK APPL SIG, P224, DOI 10.1109/WASPAA.2017.8170028
   Politis A, 2015, IEEE J-STSP, V9, P852, DOI 10.1109/JSTSP.2015.2415762
   Pulkki V, 1997, J AUDIO ENG SOC, V45, P456
   Robotham T, 2022, 2022 14 INT C QUAL M, DOI [10.1109/QoMEX55416.2022.9900893, DOI 10.1109/QOMEX55416.2022.9900893]
   Scaini D, 2014, AUD ENG SOC 55 INT C
   Tarlao C, 2022, PLOS ONE, V17, DOI 10.1371/journal.pone.0270401
   THX LucasfilmLtd, 1999, Tech. rep
   Vogt W., 2015, Stat Methodol, V37, P2003, DOI [10.4135/9781412983907.n614, DOI 10.4135/9781412983907.N614]
   Yeo NL, 2020, J ENVIRON PSYCHOL, V72, DOI 10.1016/j.jenvp.2020.101500
   Zotter F, 2012, ACTA ACUST UNITED AC, V98, P37, DOI 10.3813/AAA.918490
   Zotter F, 2010, INT S AMB SPHER AC
   Zotter F, 2015, DAGA, P486
   Zotter F., 2019, Ambisonics: A Practical 3D Audio Theory for Recording, Studio Production, Sound Reinforcement, and Virtual Reality, DOI DOI 10.1007/978-3-030-17207-7
   Zotter F, 2012, J AUDIO ENG SOC, V60, P807
NR 42
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAY 17
PY 2024
VL 28
IS 2
AR 111
DI 10.1007/s10055-024-01003-1
PG 14
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA RI4E0
UT WOS:001227013500001
PM 38765056
OA hybrid
DA 2024-08-05
ER

PT J
AU Kim, JH
   Kim, M
   Yoo, J
   Park, M
AF Kim, Jung-Hwan
   Kim, Minjeong
   Yoo, Jungmin
   Park, Minjung
TI Augmented reality in delivering experiential values: moderating role of
   task complexity
SO VIRTUAL REALITY
LA English
DT Article
DE Augmented reality; Experiential value; Customer loyalty; Task complexity
ID ONLINE; IMPACT; CONCEPTUALIZATION; SATISFACTION; MODEL
AB This study examined if greater experiential values are provided when shopping via an AR mobile app compared to shopping via a general mobile website without AR. The mediating role of experiential values between the two shopping methods and customer loyalty as well as the moderating effect of task complexity between the two shopping methods and experiential values were further investigated. An exciting eyewear retailer's mobile site and mobile app embedded with AR features were used. A total of 302 usable respondents participated in the study. Shoppers exposed to an AR function perceived greater aesthetics, escapism, enjoyment, and efficiency than those exposed to a non-AR mobile site. Also, compared to shoppers exposed to a general non-AR mobile site, shoppers exposed to an AR mobile app showed greater customer loyalty through the four experiential values. Task complexity modified the effects of AR on consumers' perceived escapism and efficiency experiential values. This research fills the gap in the literature by investigating AR's experiential values in connection with customer loyalty by comparing an AR-embedded mobile app with a general mobile site without an AR feature. The additional examination of task complexity also contributes to a complete understanding of AR experiential benefits considering consumers' perceptions about AR operation task complexity.
C1 [Kim, Jung-Hwan] Univ South Carolina, Columbia, SC USA.
   [Kim, Minjeong] Indiana Univ Bloomington, Bloomington, IN USA.
   [Yoo, Jungmin] Duksung Womens Univ, Seoul, South Korea.
   [Park, Minjung] Ewha Womans Univ, Seoul, South Korea.
C3 University of South Carolina System; University of South Carolina
   Columbia; Indiana University System; Indiana University Bloomington;
   Duksung Women's University; Ewha Womans University
RP Park, M (corresponding author), Ewha Womans Univ, Seoul, South Korea.
EM minjungpark@ewha.ac.kr
FU National Research Foundation of Korea
FX No Statement Available
CR [Anonymous], 2013, Journal of Customer Behaviour
   [Anonymous], 2020, Harvard Business Journal
   BairesDev, A thorough introduction to the Metaverse, AR, and VR as disruptive technologies
   Bauer DJ, 2005, MULTIVAR BEHAV RES, V40, P373, DOI 10.1207/s15327906mbr4003_5
   Bejjanki VR, 2020, DEVELOPMENTAL SCI, V23, DOI 10.1111/desc.12912
   Berman B, 2021, BUS HORIZONS, V64, P621, DOI 10.1016/j.bushor.2021.02.027
   Boboc RG, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12199859
   Caboni F, 2019, INT J RETAIL DISTRIB, V47, P1125, DOI 10.1108/IJRDM-12-2018-0263
   CAMPBELL DJ, 1988, ACAD MANAGE REV, V13, P40, DOI 10.2307/258353
   Cao YC, 2020, INT J ENV RES PUB HE, V17, DOI 10.3390/ijerph17020455
   Chandra S, 2018, J ELECTRON COMMER RE, V19, P237
   Chen CW, 2020, INT J HUM-COMPUT INT, V36, P1178, DOI 10.1080/10447318.2020.1726106
   Chen RF, 2022, INT J RETAIL DISTRIB, V50, P498, DOI 10.1108/IJRDM-11-2020-0472
   Chen S., 2019, Front Psychol, V11, P1
   Chiu CL, 2021, J RETAIL CONSUM SERV, V61, DOI 10.1016/j.jretconser.2021.102561
   Dacko SG, 2017, TECHNOL FORECAST SOC, V124, P243, DOI 10.1016/j.techfore.2016.09.032
   Dhar P, 2021, MED EDUC ONLINE, V26, DOI 10.1080/10872981.2021.1953953
   Echchakoui S, 2016, J RETAIL CONSUM SERV, V28, P54, DOI 10.1016/j.jretconser.2015.07.014
   Econsultancy, 2021, 14 examples of augmented reality brand experiences
   Fan XJ, 2020, J RETAIL CONSUM SERV, V53, DOI 10.1016/j.jretconser.2019.101986
   Fashion Network, 2021, Burberry launches international opo-ups for Olympia bag, starts with Harrods
   Flint D.J., 2006, Marketing Theory, V6, P349, DOI DOI 10.1177/1470593106066796
   Frank JB., 1991, Designing the user interface: considering the concept of complexity
   Future Business Tech, 2022, The future of augmented reality: 10 awesome use cases
   Gebre A, 2022, J STUD AFF RES PRACT, V59, P1, DOI 10.1080/19496591.2020.1784749
   Global Economics, 2021, South Korea is now a key player in VR with the 'metaverse' launch
   Grewal D, 2017, J RETAILING, V93, P1, DOI 10.1016/j.jretai.2016.12.008
   Gupta A, 2013, DECIS SUPPORT SYST, V55, P135, DOI 10.1016/j.dss.2012.12.035
   Hayes A.F., 2017, Hacking PROCESS for estimation and probing of linear moderation of quadratic effects and quadratic moderation of linear effects
   Hayes AF, 2017, AUSTRALAS MARK J, V25, P76, DOI 10.1016/j.ausmj.2017.02.001
   Hilken T, 2022, PSYCHOL MARKET, V39, P495, DOI 10.1002/mar.21600
   Hoffmann S, 2022, J ACAD MARKET SCI, V50, P743, DOI 10.1007/s11747-022-00855-w
   HOLBROOK MB, 1982, J CONSUM RES, V9, P132, DOI 10.1086/208906
   Holbrook MB, 1996, ADV CONSUM RES, V23, P138
   Huang HJ, 2021, ADV ROBOTICS, V35, P1156, DOI 10.1080/01691864.2021.1974940
   Huang TL, 2017, INTERNET RES, V27, P449, DOI 10.1108/IntR-11-2015-0321
   Huang TL, 2014, INTERNET RES, V24, P82, DOI 10.1108/IntR-07-2012-0133
   IBM, What augmented reality graphics can do for your business
   Jeong SW, 2009, INTERNET RES, V19, P105, DOI 10.1108/10662240910927858
   Jiang ZJ, 2007, MIS QUART, V31, P475
   Kim JH, 2023, TELEMAT INFORM, V77, DOI 10.1016/j.tele.2022.101936
   Kim JH, 2020, SERV IND J, V40, P436, DOI 10.1080/02642069.2018.1517755
   Koebele SV, 2021, J NEUROENDOCRINOL, V33, DOI 10.1111/jne.13002
   Kotler P., 2021, H2H Marketing: The Genesis of Human-to-Human Marketing, DOI DOI 10.1007/978-3-030-59531-9
   Koumaditis K, 2020, IEEE COMPUT GRAPH, V40, P41, DOI 10.1109/MCG.2020.3006330
   Kumar H, 2022, INT J RETAIL DISTRIB, V50, P537, DOI 10.1108/IJRDM-06-2021-0287
   Kumar T.S., 2021, J. Innov. Image Process., V3, P144, DOI 10.36548/jiip.2021.2.006
   Kwon WS, 2009, J BUS RES, V62, P557, DOI 10.1016/j.jbusres.2008.06.015
   Laningham A, 2022, The Harris Poll
   Li H, 2011, EUR J INFORM SYST, V20, P139, DOI 10.1057/ejis.2010.59
   Mathwick C, 2001, J RETAILING, V77, P39, DOI 10.1016/S0022-4359(00)00045-2
   Men CH, 2021, LEADERSHIP ORG DEV J, V42, P882, DOI 10.1108/LODJ-11-2020-0506
   MILGRAM P, 1994, P SOC PHOTO-OPT INS, V2351, P282
   Norris CL, 2023, J HOSP TOUR INSIGHTS, V6, P613, DOI 10.1108/JHTI-09-2021-0252
   Owens B, 2022, Ryderecommerce
   Ozturkcan Selcen, 2021, Journal of Information Technology Teaching Cases, V11, P8, DOI 10.1177/2043886920947110
   Paine J, 2023, 10 real use cases for augmented reality
   Papagiannis Helen., 2020, Harvard Business Review, V7
   Park S, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11167259
   Peters K, 2005, Digitalcommerce 360
   Poushneh A, 2017, J RETAIL CONSUM SERV, V34, P229, DOI 10.1016/j.jretconser.2016.10.005
   RetailDive, 2022, 3 disruptive retail trends, and why retailers must adapt
   Sasangohar F, 2021, INT J IND ERGONOM, V85, DOI 10.1016/j.ergon.2021.103184
   Savelli E, 2017, INT J RETAIL DISTRIB, V45, P1213, DOI 10.1108/IJRDM-07-2016-0120
   Schaarschmidt M, 2018, FORTHC C ICIS 2018 S
   Schultz B, 2020, Construction Technology
   Smink AR, 2020, J BUS RES, V118, P474, DOI 10.1016/j.jbusres.2020.07.018
   SMS Strategies, 2022, Is gamification the key to delivering entertaining shopping experiences?
   Statista, 2022, Mobile augmented reality (AR) market revenue worldwide from 2021 to 2026
   Statista, 2021, U.S. digital buyer distribution 2020
   Sun TC, 2023, INT J HUM-COMPUT INT, V39, P2656, DOI 10.1080/10447318.2022.2083462
   Swait J, 2001, J CONSUM RES, V28, P135, DOI 10.1086/321952
   Sweller J, 2011, PSYCHOL LEARN MOTIV, V55, P37
   Switzerland Global Enterprise, 2019, KOR RAP EV VR AR AI
   Tan YC, 2022, J MARKETING, V86, P48, DOI 10.1177/0022242921995449
   Tang FC, 2020, INFORM TECHNOL MANAG, V21, P179, DOI 10.1007/s10799-020-00316-2
   Teunissen SCCM, 2007, PALLIATIVE MED, V21, P341, DOI 10.1177/0269216307079067
   th Wall, 2021, Burberry invites customers into the world of Olympia with its latest augmented reality experience
   The Industry Fashion, 2021, BoohooMAN unveils first augmented reality campaign for Black Friday
   The Korea Herald, 2021, S. Korea tops list of world's most innovative countries
   Todd P, 1999, INFORM SYST RES, V10, P356, DOI 10.1287/isre.10.4.356
   Tremblay ML, 2023, MED EDUC, V57, P161, DOI 10.1111/medu.14941
   Tremosa L., 2023, Beyond AR vs. VR: What is the Difference between AR vs. MR vs. VR vs. XR?
   Varshneya G, 2017, J RETAIL CONSUM SERV, V34, P48, DOI 10.1016/j.jretconser.2016.09.010
   Verhagen T, 2011, INFORM MANAGE-AMSTER, V48, P201, DOI 10.1016/j.im.2011.02.004
   Volersystems, What is the difference between augmented reality vs. virtual reality vs. mixed reality
   Watson A, 2020, INT J RETAIL DISTRIB, V48, P433, DOI 10.1108/IJRDM-06-2017-0117
   WBR Insights, 2022, Zara's augmented reality app brings virtual models to life in stores
   Wedel M, 2020, INT J RES MARK, V37, P443, DOI 10.1016/j.ijresmar.2020.04.004
   Williams R, 2019, MarketingDive
   Yucel D, 2023, APPL RES QUAL LIFE, V18, P1825, DOI 10.1007/s11482-023-10164-1
   Zarantonello L, 2023, J SERV MANAGE, V34, P34, DOI 10.1108/JOSM-12-2021-0479
   Zhou Y, 2021, MEM COGNITION, V49, P1583, DOI 10.3758/s13421-021-01183-0
NR 93
TC 0
Z9 0
U1 10
U2 11
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 19
DI 10.1007/s10055-023-00896-8
PG 14
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA EW8L5
UT WOS:001142060800001
OA Bronze
DA 2024-08-05
ER

PT J
AU Ladakis, I
   Filos, D
   Chouvarda, I
AF Ladakis, Ioannis
   Filos, Dimitrios
   Chouvarda, Ioanna
TI Virtual reality environments for stress reduction and management: a
   scoping review
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; VR; Virtual environments; Stress reduction
ID PSYCHOLOGICAL STRESS; TECHNOLOGY; EXPOSURE; FEASIBILITY; RELAXATION;
   RECOVERY; THERAPY
AB Virtual reality, a cutting-edge innovation in the realm of digital experiences, though more frequently employed for entertainment and education, can also serve as a tool for immersing users in therapeutic settings that promote relaxation and mindfulness. An increasing number of research attempts investigate its usability and impact on stress evaluation, management and reduction. This scoping review aims to depict the current role of virtual reality in stress reduction and identify common methods and practice, technology patterns as well as gaps. Results depict the emerging research interest in the domain of VR-based stress reduction systems. The developed systems included in this review were basically addressed to the general public (59%) for daily life stress reduction utilizing a commercial VR headset often combined with supportive sensors. Guided imagery emerged as the most implemented method, but it is also noteworthy that almost all studies implicitly used this method. According to the analysis, most studies performed evaluation of the proposed VR system including both subjective and objective measurements to provide evidence on its efficiency and its actual impact on stress levels. Finally, validation methodologies attempt to point out the potential of VR technology in the direction of providing an efficient solution for the alleviation of stress burdens. Even though numerous studies report the usefulness and efficiency of VR technology regarding stress reduction, several challenges still need to be addressed, mainly because of the difficult definition, detection and evaluation of stress. An approach integrating the existing knowledge regarding signals that can act as biomarkers of stress and qualitative measurements could open new pathways toward the development of more impactful VR-based stress reduction systems.
C1 [Ladakis, Ioannis; Filos, Dimitrios; Chouvarda, Ioanna] Aristotle Univ Thessaloniki, Sch Med, Lab Comp Med Informat & Biomed Imaging Technol, Thessaloniki 54124, Greece.
C3 Aristotle University of Thessaloniki
RP Ladakis, I (corresponding author), Aristotle Univ Thessaloniki, Sch Med, Lab Comp Med Informat & Biomed Imaging Technol, Thessaloniki 54124, Greece.
EM iladakig@auth.gr
RI ; Chouvarda, Ioanna/N-8925-2015
OI Ladakis, Ioannis/0000-0002-3457-1333; Chouvarda,
   Ioanna/0000-0001-8915-6658
FU Aristotle University of Thessaloniki
FX IL and DF conceived of the study. All authors participated in the design
   of the study and drafted the manuscript. Data management was conducted
   by IL and DF advised by IC. All authors edited the manuscript, read and
   approved the final manuscript.
CR Abdul Manaf Marlina B. T., 2021, 2021 International Conference on Computer & Information Sciences (ICCOINS), P276, DOI 10.1109/ICCOINS49721.2021.9497220
   Ahmaniemi T, 2017, 2017 IEEE LIFE SCIENCES CONFERENCE (LSC), P206, DOI 10.1109/LSC.2017.8268179
   Ali N, 2020, INT J BEHAV MED, V27, P337, DOI 10.1007/s12529-019-09843-x
   Alyan E, 2021, INT J ENV RES PUB HE, V18, DOI 10.3390/ijerph182111420
   Amores J, 2018, INT CONF WEARAB IMPL, P98, DOI 10.1109/BSN.2018.8329668
   Andersen T, 2017, P IEEE VIRT REAL ANN, P343, DOI 10.1109/VR.2017.7892317
   Anderson AP, 2017, AEROSP MED HUM PERF, V88, P520, DOI 10.3357/AMHP.4747.2017
   Annerstedt M, 2013, PHYSIOL BEHAV, V118, P240, DOI 10.1016/j.physbeh.2013.05.023
   Arksey H., 2005, INT J SOC RES METHOD, V8, P19, DOI [10.1080/1364557032000119616, DOI 10.1080/1364557032000119616]
   Biber B, 2020, 2020 SYST INF ENG DE, P1
   Björling EA, 2020, LECT NOTES ARTIF INT, V12483, P604, DOI 10.1007/978-3-030-62056-1_50
   Blum J, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.02172
   Botella C, 2017, CURR PSYCHIAT REP, V19, DOI 10.1007/s11920-017-0788-4
   Broneder E, 2021, HCI INT 2021 POST 23, P249, DOI [10.1007/978-3-030-78635-934, DOI 10.1007/978-3-030-78635-934]
   Chan SHM, 2023, VIRTUAL REAL-LONDON, V27, P3285, DOI 10.1007/s10055-021-00604-4
   Chavez LJ, 2020, JMIR MENT HEALTH, V7, DOI 10.2196/18244
   Cheng VYW, 2020, J MED INTERNET RES, V22, DOI 10.2196/17096
   Chouvarda I, 2019, J MED INTERNET RES, V21, DOI 10.2196/14005
   Cikajlo Imre, 2016, Third IASTED International Conference on Telehealth and Assistive Technology (TAT 2016). Proceedings, P11, DOI 10.2316/P.2016.846-008
   Cikajlo I, 2017, JMIR RES PROTOC, V6, DOI 10.2196/resprot.6849
   Crosswell L, 2022, BEHAV INFORM TECHNOL, V41, P864, DOI 10.1080/0144929X.2020.1838609
   Das Thoondee K, 2017, 2017 COMPUTING CONFERENCE, P492, DOI 10.1109/SAI.2017.8252142
   Dayang R., 2011, Proceedings of the 2011 International Symposium on Information Technology in Medicine and Education (ITME 2011), P733, DOI 10.1109/ITiME.2011.6132104
   De Paolis LT, 2021, LECT NOTES COMPUT SC, V12980, P156, DOI 10.1007/978-3-030-87595-4_12
   Eftekharifar S, 2021, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2021), DOI 10.1145/3474451.3476234
   Ergan S, 2019, J COMPUT CIVIL ENG, V33, DOI 10.1061/(ASCE)CP.1943-5487.0000812
   Eswaran VSB, 2018, J CLIN DIAGN RES, V12, pJC11, DOI 10.7860/JCDR/2018/36055.12109
   Fassbender E, 2014, LECT NOTES COMPUT SC, V8778, P48, DOI 10.1007/978-3-319-11623-5_5
   Felnhofer A, 2015, INT J HUM-COMPUT ST, V82, P48, DOI 10.1016/j.ijhcs.2015.05.004
   Fodor LA, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-28113-6
   Freeman D, 2017, PSYCHOL MED, V47, P2393, DOI 10.1017/S003329171700040X
   Gaggioli A, 2020, ANN REV CYBERTHERAPY, V18, P123
   Gaggioli A, 2014, J MED INTERNET RES, V16, P54, DOI 10.2196/jmir.3235
   Gao T, 2019, INT J ENV RES PUB HE, V16, DOI 10.3390/ijerph16173102
   Giannakakis G, 2022, IEEE T AFFECT COMPUT, V13, P440, DOI 10.1109/TAFFC.2019.2927337
   Gonzalez DAZ, 2021, INT J HUM-COMPUT ST, V147, DOI 10.1016/j.ijhcs.2020.102579
   Gu GX, 2017, LECT NOTES ARTIF INT, V10512, P176, DOI 10.1007/978-3-319-67615-9_16
   Gu L, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENCE AND SAFETY FOR ROBOTICS (ISR), P33, DOI 10.1109/ISR50024.2021.9419557
   Hedblom M, 2019, INT J ENV RES PUB HE, V16, DOI 10.3390/ijerph16081390
   Heyse J., 2019, INT WORK QUAL MULTIM, P1, DOI DOI 10.1109/qomex.2019.8743335
   Heyse J, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10176124
   Hill JE, 2022, NURS CRIT CARE, V27, P756, DOI 10.1111/nicc.12732
   Huang QY, 2020, LANDSCAPE URBAN PLAN, V193, DOI 10.1016/j.landurbplan.2019.103654
   Ibrahim N, 2021, INT VIS INF C SPRING, P171
   Ishaque S, 2020, IEEE ENG MED BIO, P867, DOI 10.1109/EMBC44109.2020.9176110
   Ito T, 2018, IEEE INT CONF HEALT, P299, DOI 10.1109/ICHI.2018.00041
   Jo SH, 2021, INT J ENV RES PUB HE, V18, DOI 10.3390/ijerph182312805
   Kaimal G, 2020, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.589461
   Kaminska D, 2020, IEEE ACCESS, V8, P200351, DOI 10.1109/ACCESS.2020.3035540
   Kim H, 2021, FRONT PSYCHIATRY, V12, DOI 10.3389/fpsyt.2021.614539
   Kluge MG, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0245068
   Koinuma Y, 2019, ADV INTELL SYST, V774, P269, DOI 10.1007/978-3-319-94944-4_30
   Kristensen JW, 2019, interactivity, game creation, design, learning, and innovation, P217
   Ladakis I, 2021, HEALTHINF: PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON BIOMEDICAL ENGINEERING SYSTEMS AND TECHNOLOGIES - VOL. 5: HEALTHINF, P541, DOI 10.5220/0010300905410548
   Ladakis I., 2021, EMERGING SCI J, V5, P233, DOI [10.28991/esj-2021-01267, DOI 10.28991/ESJ-2021-01267]
   Latif RA, 2018, 2018 4TH INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION SCIENCES (ICCOINS)
   Latif RA, 2015, J TEKNOL, V77, P21
   Li C, 2020, URBAN FOR URBAN GREE, V56, DOI 10.1016/j.ufug.2020.126865
   Lin APC, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11199071
   Lin XX, 2020, 2020 10TH ANNUAL COMPUTING AND COMMUNICATION WORKSHOP AND CONFERENCE (CCWC), P948, DOI [10.1109/ccwc47524.2020.9031157, 10.1109/CCWC47524.2020.9031157]
   Liszio S, 2018, ANN REV CYBERTHERAPY, V16, P87
   Lui AKF, 2012, 2012 SIXTH INTERNATIONAL CONFERENCE ON DISTRIBUTED SMART CAMERAS (ICDSC)
   Maarsingh BM, 2019, GAMES HEALTH J, V8, P326, DOI 10.1089/g4h.2018.0145
   Madzin Hizmawati, 2021, 2021 INT C WOM DAT S, P1
   Mahalil I, 2014, I C INF TECH MULTIM, P380, DOI 10.1109/ICIMU.2014.7066663
   Mahalil I, 2014, 2014 4TH INTERNATIONAL CONFERENCE ON ENGINEERING TECHNOLOGY AND TECHNOPRENEURSHIP (ICE2T), P295, DOI 10.1109/ICE2T.2014.7006265
   Manchia M, 2022, EUR NEUROPSYCHOPHARM, V55, P22, DOI 10.1016/j.euroneuro.2021.10.864
   Maramis C, 2017, PROCEEDINGS OF THE 14TH INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS CONTEL 2017, P63, DOI 10.23919/ConTEL.2017.8000040
   Mason L J., 2013, Guide to Stress Reduction
   Meese MM, 2021, SIMUL HEALTHC, V16, P268, DOI 10.1097/SIH.0000000000000484
   Michael SH, 2019, CLIN J ONCOL NURS, V23, P664, DOI 10.1188/19.CJON.664-667
   Modrego-Alarcón M, 2021, BEHAV RES THER, V142, DOI 10.1016/j.brat.2021.103866
   Mostajeran F, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-83277-y
   Muhaiyuddin NDM., 2018, J Telecommun Electron Comput Eng, V10, P1
   Naylor M, 2019, INTERACT COMPUT, V31, P507, DOI 10.1093/iwc/iwz033
   Nijland JWHM, 2021, FRONT PSYCHOL, V12, DOI 10.3389/fpsyg.2021.706527
   Ohly H, 2016, J TOXICOL ENV HEAL B, V19, P305, DOI 10.1080/10937404.2016.1196155
   Pallavicini F, 2013, TRIALS, V14, DOI 10.1186/1745-6215-14-191
   Peters MDJ, 2015, INT J EVID-BASED HEA, V13, P141, DOI 10.1097/XEB.0000000000000050
   Pfeffel Kevin, 2020, Learning and Collaboration Technologies. Human and Technology Ecosystems. 7th International Conference, LCT 2020. Held as Part of the 22nd HCI International Conference, HCII 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12206), P176, DOI 10.1007/978-3-030-50506-6_13
   Pisalski D, 2020, IEEE CONF COMPU INTE, P646, DOI 10.1109/CoG47356.2020.9231658
   Pourmand A, 2017, GAMES HEALTH J, V6, P263, DOI 10.1089/g4h.2017.0046
   Rajoo KS, 2020, URBAN FOR URBAN GREE, V54, DOI 10.1016/j.ufug.2020.126744
   Reese G, 2021, SUSTAINABILITY-BASEL, V13, DOI 10.3390/su13041995
   Richesin MT, 2021, ART PSYCHOTHER, V75, DOI 10.1016/j.aip.2021.101823
   Riva G, 2021, INT J ENV RES PUB HE, V18, DOI 10.3390/ijerph18158188
   Riva G, 2020, FRONT PSYCHIATRY, V11, DOI 10.3389/fpsyt.2020.563319
   Rockstroh C, 2021, VIRTUAL REAL-LONDON, V25, P539, DOI 10.1007/s10055-020-00471-5
   Rockstroh C, 2019, INT J HUM-COMPUT ST, V130, P209, DOI 10.1016/j.ijhcs.2019.06.011
   Rozmi M D A, 2020, 2019 INT C ADV EM CO, P1
   Sakhare AR, 2019, FRONT AGING NEUROSCI, V11, DOI 10.3389/fnagi.2019.00218
   Schebella MF, 2020, INT J ENV RES PUB HE, V17, DOI 10.3390/ijerph17010056
   Skulimowski S, 2017, 2017 INTERNATIONAL CONFERENCE ON ELECTROMAGNETIC DEVICES AND PROCESSES IN ENVIRONMENT PROTECTION WITH SEMINAR APPLICATIONS OF SUPERCONDUCTORS (ELMECO & AOS)
   Song CR, 2016, INT J ENV RES PUB HE, V13, DOI 10.3390/ijerph13080781
   Sonney J, 2021, J PEDIATR HEALTH CAR, V35, P552, DOI 10.1016/j.pedhc.2021.01.002
   Soyka F., 2016, Proceedings of the ACM symposium on applied perception. SAP'16, P85, DOI [DOI 10.1145/2931002.2931017, 10.1145/2931002.2931017.]
   Abdullah SSS, 2021, FORESTS, V12, DOI 10.3390/f12121776
   Taneja A, 2017, 2017 FOURTH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, COMMUNICATION AND NETWORKING (ICSCN)
   Tinga AM, 2019, APPL PSYCHOPHYS BIOF, V44, P51, DOI 10.1007/s10484-018-9421-5
   Tivatansakul S, 2013, 2013 INTERNATIONAL CONFERENCE ON BIOMETRICS AND KANSEI ENGINEERING (ICBAKE), P218, DOI 10.1109/ICBAKE.2013.43
   Tricco AC, 2018, ANN INTERN MED, V169, P467, DOI 10.7326/M18-0850
   ULRICH RS, 1991, J ENVIRON PSYCHOL, V11, P201, DOI 10.1016/S0272-4944(05)80184-7
   Valmaggia LR, 2016, PSYCHIAT RES, V236, P189, DOI 10.1016/j.psychres.2016.01.015
   Valtchanov D, 2010, CYBERPSYCH BEH SOC N, V13, P503, DOI 10.1089/cyber.2009.0308
   Vaquero-Blasco MA, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21062219
   Vaquero-Blasco MA, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20216211
   Villani D, 2012, CYBERPSYCH BEH SOC N, V15, P24, DOI 10.1089/cyber.2011.0141
   Wadhwa S, 2017, Stress in the modern world: understanding science and society
   Wang XB, 2019, INT J ENV RES PUB HE, V16, DOI 10.3390/ijerph16183263
   Wayment HA, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.01481
   Weerdmeester J, 2017, EXTENDED ABSTRACTS PUBLICATION OF THE ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY (CHI PLAY'17 EXTENDED ABSTRACTS), P453, DOI 10.1145/31308593131299
   Weitzman RE, 2021, LARYNGOSCOPE, V131, P1972, DOI 10.1002/lary.29529
   Wiederhold BK, 2014, STUD HEALTH TECHNOL, V199, P83, DOI 10.3233/978-1-61499-401-5-83
   Xu WG, 2021, JMIR SERIOUS GAMES, V9, DOI 10.2196/29330
   Yang H, 2022, IEEE T GAMES, V14, P511, DOI 10.1109/TG.2021.3118035
   Yang T, 2021, TECHNOL SOC, V64, DOI 10.1016/j.techsoc.2020.101514
   Yeom S, 2021, BUILD ENVIRON, V204, DOI 10.1016/j.buildenv.2021.108134
   Yin J, 2020, ENVIRON INT, V136, DOI 10.1016/j.envint.2019.105427
   Yin J, 2018, BUILD ENVIRON, V132, P255, DOI 10.1016/j.buildenv.2018.01.006
   Zaharuddin Farhah Amaliya, 2019, Advances in Visual Informatics. 6th International Visual Informatics Conference, IVIC 2019. Proceedings. Lecture Notes in Computer Science (LNCS 11870), P36, DOI 10.1007/978-3-030-34032-2_4
   Zaharuddin Farhah Amaliya, 2019, Advances in Visual Informatics. 6th International Visual Informatics Conference, IVIC 2019. Proceedings. Lecture Notes in Computer Science (LNCS 11870), P25, DOI 10.1007/978-3-030-34032-2_3
   Zaharuddin FA., 2019, Int J Eng Adv Technol, V9, P2325, DOI [10.35940/ijeat.A2656.109119, DOI 10.35940/IJEAT.A2656.109119]
   Zainudin ARR, 2014, I C INF TECH MULTIM, P374, DOI 10.1109/ICIMU.2014.7066662
   Zainudin ARR, 2014, IEEE CONF OPEN SYST, P105, DOI 10.1109/ICOS.2014.7042638
   Zhao XJ, 2015, BEHAV INFORM TECHNOL, V34, P646, DOI 10.1080/0144929X.2015.1022223
NR 125
TC 0
Z9 0
U1 8
U2 8
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 50
DI 10.1007/s10055-024-00943-y
PG 26
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA JC4I7
UT WOS:001170940200003
OA hybrid
DA 2024-08-05
ER

PT J
AU Morales-Vega, JC
   Raya, L
   Rubio-Sánchez, M
   Sanchez, A
AF Morales-Vega, Juan C.
   Raya, Laura
   Rubio-Sanchez, Manuel
   Sanchez, Alberto
TI A virtual reality data visualization tool for dimensionality reduction
   methods
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Information visualization; Natural interface
ID BEHAVIOR MODEL; SPACES
AB In this paper, we present a virtual reality interactive tool for generating and manipulating visualizations for high-dimensional data in a natural and intuitive stereoscopic way. Our tool offers support for a diverse range of dimensionality reduction (DR) algorithms, enabling the transformation of complex data into insightful 2D or 3D representations within an immersive VR environment. The tool also allows users to include annotations with a virtual pen using hand tracking, to assign class labels to the data observations, and to perform simultaneous visualization with other users within the 3D environment to facilitate collaboration.
C1 [Morales-Vega, Juan C.; Raya, Laura] U Tad Univ Ctr Technol & Digital Art, C Playa Liencres 2 Bis,Las Rozas, Madrid 28290, Spain.
   [Rubio-Sanchez, Manuel; Sanchez, Alberto] Univ Rey Juan Carlos, Dept Comp Sci & Stat, C Tulipan S-N, Mostoles 28933, Madrid, Spain.
C3 Universidad Rey Juan Carlos
RP Sanchez, A (corresponding author), Univ Rey Juan Carlos, Dept Comp Sci & Stat, C Tulipan S-N, Mostoles 28933, Madrid, Spain.
EM juan.vega@live.u-tad.com; laura.raya@u-tad.com; manuel.rubio@urjc.es;
   alberto.sanchez@urjc.es
RI Sanchez, Alberto/H-3995-2011
OI Sanchez, Alberto/0000-0002-5382-6805
FU Ministerio de Ciencia e Innovacin
FX No Statement Available
CR Bertin Jacques, 1983, Semiology of graphics
   Betella A, 2014, P 2014 VIRTUAL REALI, V2014, P1
   Brooke J., 1996, SUS-a quick and dirty usability scale, DOI [DOI 10.1201/9781498710411-35, DOI 10.1201/9781498710411]
   Cantini L, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-020-20430-7
   deHaan G, 2002, P ACM S VIRTUAL REAL, P105
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Donalek C, 2014, IEEE INT CONF BIG DA, P609, DOI 10.1109/BigData.2014.7004282
   Ferguson AL, 2011, CHEM PHYS LETT, V509, P1, DOI 10.1016/j.cplett.2011.04.066
   Helbig C, 2014, ENVIRON EARTH SCI, V72, P3767, DOI 10.1007/s12665-014-3136-6
   Huang B, 2001, INT J GEOGR INF SCI, V15, P439, DOI 10.1080/13658810110046574
   Johansson J., 2015, IEEE Trans Visual Comput Graph, V22, P1
   Kandogan E, 2001, P IEEE INFORM VISUAL
   Kingsley LJ, 2019, J MOL GRAPH MODEL, V89, P234, DOI 10.1016/j.jmgm.2019.03.010
   Lawonn K, 2018, COMPUT GRAPH FORUM, V37, P413, DOI 10.1111/cgf.13306
   Mangasarian OL, 2022, Breat cancer wisconsin (diagnostic) data set
   McInnes L, 2020, Arxiv, DOI [arXiv:1802.03426, 10.21105/joss.00861]
   Mishra S., 2017, INT J LIVEST RES, V1, DOI DOI 10.5455/IJLR.20170415115235
   Mohedano-Munoz MA, 2021, EXPERT SYST APPL, V171, DOI 10.1016/j.eswa.2021.114605
   Montes J, 2010, CONCURR COMP-PRACT E, V22, P1386, DOI 10.1002/cpe.1490
   Montes J, 2008, LECT NOTES COMPUT SC, V5331, P886
   Nanome, Nanome: virtual reality for drug design and molecular visualization
   Nolte H, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-31154-6
   Ratamero EM, 2018, J COMPUT AID MOL DES, V32, P703, DOI 10.1007/s10822-018-0123-0
   Reinhard E., 2008, Color imaging fundamentals and applications, DOI [10.1201/b10637, DOI 10.1201/B10637]
   Rubio-Sánchez M, 2016, IEEE T VIS COMPUT GR, V22, P619, DOI 10.1109/TVCG.2015.2467324
   Ruder H, 2008, NEW J PHYS, V10, DOI 10.1088/1367-2630/10/12/125014
   Rzeszotarski JM, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P897, DOI 10.1145/2556288.2557231
   Schwabish JA, 2014, J ECON PERSPECT, V28, P209, DOI 10.1257/jep.28.1.209
   Shneiderman B, 1996, IEEE SYMPOSIUM ON VISUAL LANGUAGES, PROCEEDINGS, P336, DOI 10.1109/VL.1996.545307
   Tharwat A, 2017, AI COMMUN, V30, P169, DOI 10.3233/AIC-170729
   Todorovic D., 2008, Gestalt principles, V3, P5345, DOI DOI 10.4249/SCHOLARPEDIA.5345
   Valdés JJ, 2007, IEEE C EVOL COMPUTAT, P4199, DOI 10.1109/CEC.2007.4425019
   Valdés JJ, 2012, EXPERT SYST APPL, V39, P13193, DOI 10.1016/j.eswa.2012.05.082
   Valdés JJ, 2006, IEEE C EVOL COMPUTAT, P1427
   van Dam A, 2002, COMPUT GRAPH-UK, V26, P535, DOI 10.1016/S0097-8493(02)00113-9
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
NR 36
TC 1
Z9 1
U1 4
U2 4
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 41
DI 10.1007/s10055-024-00939-8
PG 14
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA HL0Q6
UT WOS:001159546300001
OA hybrid
DA 2024-08-05
ER

PT J
AU Spinczyk, D
   Rosiak, G
   Milczarek, K
   Konecki, D
   Zylkowski, J
   Franke, J
   Pech, M
   Rohmer, K
   Zaczkowski, K
   Wolinska-Soltys, A
   Sperka, P
   Hajda, D
   Pietka, E
AF Spinczyk, Dominik
   Rosiak, Grzegorz
   Milczarek, Krzysztof
   Konecki, Dariusz
   Zylkowski, Jaroslaw
   Franke, Jakub
   Pech, Maciej
   Rohmer, Karl
   Zaczkowski, Karol
   Wolinska-Soltys, Ania
   Sperka, Piotr
   Hajda, Dawid
   Pietka, Ewa
TI Towards overcoming barriers to the clinical deployment of mixed reality
   image-guided navigation systems supporting percutaneous ablation of
   liver focal lesions
SO VIRTUAL REALITY
LA English
DT Article
DE Percutaneous liver tumour ablation; Image-guided navigation; Mixed
   reality; Training of interventional radiologists; Acquiring the ability
   to use image-guided navigation
ID TUMOR; GUIDANCE
AB In recent years, we have observed a rise in the popularity of minimally invasive procedures for treating liver tumours, with percutaneous thermoablation being one of them, conducted using image-guided navigation systems with mixed reality technology. However, the application of this method requires adequate training in using the employed system. In our study, we assessed which skills pose the greatest challenges in performing such procedures. The article proposes a training module characterized by an innovative approach: the possibility of practicing the diagnosis, planning, execution stages and the physical possibility of performing the execution stage on the radiological phantom of the abdominal cavity. The proposed approach was evaluated by designing a set of 4 exercises corresponding to the 3 phases mentioned. To the research group included 10 radiologists and 5 residents in the study. Based on 20 clinical cases of liver tumors subjected to percutaneous thermoablation, we developed assessment tasks evaluating four skill categories: head-mounted display (HMD), ultrasound (US)/computed tomography (CT) image fusion interpretation, tracking system use, and the ability to insert a needle. The results were presented using the Likert scale. The results of our study indicate that the most challenging aspect for radiology specialists is adapting to HMD gesture control, while residents point to intraoperative images of fusion and respiratory movements in the liver as the most problematic. In terms of improving the ability to perform procedures on new patients, the module also allows you to create a new hologram for a different clinical case.
C1 [Spinczyk, Dominik; Pietka, Ewa] Silesian Tech Univ, Fac Biomed Engn, Zabrze, Poland.
   [Spinczyk, Dominik; Wolinska-Soltys, Ania; Sperka, Piotr; Hajda, Dawid] Holo4Med Sp ZOO, Bialystok, Poland.
   [Rosiak, Grzegorz; Milczarek, Krzysztof; Konecki, Dariusz; Zylkowski, Jaroslaw; Franke, Jakub] Med Univ Warsaw, Dept Radiol 2, Warsaw, Poland.
   [Pech, Maciej; Rohmer, Karl] Univ Magdeburg, Univ Clin Radiol, Magdeburg, Germany.
   [Pech, Maciej] Res Campus STIMULATE, Magdeburg, Germany.
   [Zaczkowski, Karol] Med Univ Lodz, Dept Neurosurg & Neurooncol, Lodz, Poland.
C3 Silesian University of Technology; Medical University of Warsaw; Otto
   von Guericke University; Medical University Lodz
RP Spinczyk, D (corresponding author), Silesian Tech Univ, Fac Biomed Engn, Zabrze, Poland.; Spinczyk, D (corresponding author), Holo4Med Sp ZOO, Bialystok, Poland.
EM dominik.spinczyk@polsl.pl
FX DAS:The collected data and developed exercise scenarios constitute the
   intellectual property of Holo4med and are not publicly available. If you
   wish to use the developed exercises, please contact the corresponding
   author.
CR Acidi B, 2023, J VISC SURG, V160, P118, DOI 10.1016/j.jviscsurg.2023.01.008
   Al-Nimer S, 2020, J VASC INTERV RADIOL, V31, P526, DOI 10.1016/j.jvir.2019.09.027
   [Anonymous], 2020, Quality ablation with CAS-One IR reproducible and standardised tumour treatments the-company's own material
   Banz VM, 2016, LANGENBECK ARCH SURG, V401, P495, DOI 10.1007/s00423-016-1417-0
   Cathomas M, 2020, CARDIOVASC INTER RAD, V43, P1505, DOI 10.1007/s00270-020-02565-8
   Gadodia G, 2022, J VASC INTERV RADIOL, V33, P333, DOI 10.1016/j.jvir.2021.11.014
   Geoghegan R, 2022, MED PHYS, V49, P769, DOI 10.1002/mp.15439
   Kuzhagaliyev T, 2018, PROC SPIE, V10576, DOI 10.1117/12.2293671
   Li RT, 2019, COMPUT VIS MEDIA, V5, P363, DOI 10.1007/s41095-019-0156-x
   Machry Mayara, 2023, World J Transplant, V13, P290, DOI 10.5500/wjt.v13.i6.290
   mevis, The mevis-liver-suite professional solution-the company's own material
   Nagasawa Shigehiro, 2023, Cancer Diagn Progn, V3, P660, DOI 10.21873/cdp.10269
   Park BJ, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-75676-4
   Park BJ, 2020, J VASC INTERV RADIOL, V31, P1074, DOI 10.1016/j.jvir.2019.09.020
   Puijk RS, 2022, CARDIOVASC INTER RAD, V45, P1074, DOI 10.1007/s00270-022-03152-9
   Puijk RS, 2018, CAN ASSOC RADIOL J, V69, P51, DOI 10.1016/j.carj.2017.11.001
   Schaible J, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-75925-6
   Solbiati M, 2022, CANCERS, V14, DOI 10.3390/cancers14051312
   Spinczyk D, 2020, BIOCYBERN BIOMED ENG, V40, P1378, DOI 10.1016/j.bbe.2020.07.005
   Spinczyk D, 2019, COMPUT MED IMAG GRAP, V78, DOI 10.1016/j.compmedimag.2019.101664
   Yoon JW, 2018, INT J MED ROBOT COMP, V14, DOI 10.1002/rcs.1914
NR 21
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD JUL 18
PY 2024
VL 28
IS 3
AR 138
DI 10.1007/s10055-024-01038-4
PG 12
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA YX3M5
UT WOS:001271745600001
OA hybrid
DA 2024-08-05
ER

PT J
AU Quintana, D
   Rodriguez, A
   Sbert, M
   Silva, Y
   Rufino, E
   Boada, I
AF Quintana, David
   Rodriguez, Antonio
   Sbert, Mateu
   Silva, Yolanda
   Rufino, Elisvan
   Boada, Imma
TI Non-photorealistic rendering as a feedback strategy in virtual reality
   for rehabilitation
SO VIRTUAL REALITY
LA English
DT Article
DE Non-photorealistic rendering; Virtual reality; Rehabilitation; Feedback
   strategies
ID VISUAL CUES; CONTACT; THERAPY
AB Virtual reality (VR) rehabilitation has been proven to be a very promising method to increase the focus and attention of patients by immersing them in a virtual world, and through that, improve the effectiveness of the rehabilitation. One of the biggest challenges in designing VR Rehabilitation exercises is in choosing feedback strategies that guide the patient and give the appropriate success/failure indicators, without breaking their sense of immersion. A new strategy for feedback is proposed, using non-photorealistic rendering (NPR) to highlight important parts of the exercise the patient needs to focus on and fade out parts of the scene that are not relevant. This strategy is implemented into an authoring tool that allows rehabilitators specifying feedback strategies while creating exercise profiles. The NPR feedback can be configured in many ways, using different NPR schemes for different layers of the exercise environment such as the background environment, the non-interactive exercise objects, and the interactive exercise objects. The main features of the system including the support for universal render pipeline, camera stacking, and stereoscopic rendering are evaluated in a testing scenario. Performance tests regarding memory usage and supported frames per second are also considered. In addition, a group of rehabilitators evaluated the system usability. The proposed system meets all the requirements to apply NPR effect in VR scenarios and solves all the limitations with regard to technical function and image quality. In addition, the system performance has been shown to meet the targets for low-cost hardware. Regarding authoring tool usability rehabilitators agree that is easy to use and a valuable tool for rehabilitation scenarios. NPR schemes can be integrated into VR rehabilitation scenarios achieving the same image quality as non-VR visualizations with only a small impact on the frame rate. NPR schemes are a good visual feedback alternative.
C1 [Quintana, David; Rodriguez, Antonio; Sbert, Mateu; Silva, Yolanda; Rufino, Elisvan; Boada, Imma] Univ Girona, Inst Informat & Aplicac, Graph & Imaging Lab, Campus Montilivi, Girona 17003, Catalunya, Spain.
   [Silva, Yolanda; Rufino, Elisvan] Dr JosepTrueta Hosp, Girona Biomed Res Inst IDIBGI, Stroke Unit, Dept Neurol, Girona 17003, Catalunya, Spain.
C3 Universitat de Girona; Universitat de Girona; Girona University Hospital
   Dr. Josep Trueta; Institut d'Investigacio Biomedica de Girona (IDIBGI)
RP Boada, I (corresponding author), Univ Girona, Inst Informat & Aplicac, Graph & Imaging Lab, Campus Montilivi, Girona 17003, Catalunya, Spain.
EM david.quintana@udg.edu; antonio.rodriguez@udg.edu; mateu.sbert@udg.edu;
   imma.boada@udg.edu
RI Rodriguez Benitez, Antonio/GNM-7508-2022
OI Rodriguez Benitez, Antonio/0000-0002-2437-618X; Quintana,
   David/0000-0002-9024-4804
FU Ministerio de Asuntos Econmicos y Transformacin Digital, Gobierno de
   Espaa
FX No Statement Available
CR Afsar SI, 2018, J STROKE CEREBROVASC, V27, P3473, DOI 10.1016/j.jstrokecerebrovasdis.2018.08.007
   Akhtar Z, 2017, IEEE ACCESS, V5, P21090, DOI 10.1109/ACCESS.2017.2750918
   Bhattacharya S., 2021, Eur Heart J Cardiovasc Imag, DOI [10.1093/ehjci/jeaa356.408, DOI 10.1093/EHJCI/JEAA356.408]
   Borrego A, 2018, GAMES HEALTH J, V7, P151, DOI 10.1089/g4h.2017.0114
   Cannavò A, 2020, INT J HUM-COMPUT ST, V141, DOI 10.1016/j.ijhcs.2020.102450
   Cárdenas-Robledo LA, 2022, TELEMAT INFORM, V73, DOI 10.1016/j.tele.2022.101863
   Cassola F, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P44, DOI 10.1109/VRW52623.2021.00014
   Cipresso P, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.02086
   Coelho H, 2022, MULTIMED TOOLS APPL, V81, P28037, DOI 10.1007/s11042-022-12829-9
   Crytek GmbH, CRYENGINE
   Curtis C, 2020, Graph Vis Comput, V3, DOI [10.1016/j.cagx.2019.100012, DOI 10.1016/J.CAGX.2019.100012]
   Doi K, 2021, COMPUT GRAPH FORUM, V40, P11, DOI 10.1111/cgf.14397
   Epic Games Inc., UNR ENG
   Fajnzylber Reyes V, 2020, Augmented film narrative by use of non-photorealistic rendering
   Fajnzylber V, 2018, 2018 INTERNATIONAL CONFERENCE ON 3D IMMERSION (IC3D)
   Fajnzylber V, 2017, LECT NOTES COMPUT SC, V9317, P87, DOI 10.1007/978-3-319-53838-9_7
   Garcia Hernandez Ruben Jesus, 2015, International Journal of Creative Interfaces and Computer Graphics, V6, P1, DOI 10.4018/IJCICG.2015010101
   Hernandez RJG, 2014, COSECIVI, P66
   Hu HH, 2000, IEEE VISUAL, P179, DOI 10.1109/VISUAL.2000.885692
   Huang CY, 2022, BMC NEUROL, V22, DOI 10.1186/s12883-021-02547-4
   Innocente C, 2023, J CULT HERIT, V62, P268, DOI 10.1016/j.culher.2023.06.001
   Johnson CI, 2017, Designing effective feedback messages in serious games and simulations: a research review, DOI [10.1007/978-3-319-39298-1_7, DOI 10.1007/978-3-319-39298-1_7]
   Kim JH, 2020, MULTIMED TOOLS APPL, V79, P1291, DOI 10.1007/s11042-019-08387-2
   Kuhail MA, 2021, IEEE ACCESS, V9, P14181, DOI 10.1109/ACCESS.2021.3051043
   Le Noury P, 2022, SPORTS MED, V52, P1473, DOI 10.1007/s40279-022-01669-0
   Lee SH, 2020, PM&R, V12, P257, DOI 10.1002/pmrj.12206
   Linowes Jonathan., 2020, Unity 2020 Virtual Reality Projects, V3rd
   Lohse KR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0093318
   Lopez O., 2017, Res Comput Sci, V146, P9, DOI [10.13053/rcs-146-1-1, DOI 10.13053/RCS-146-1-1]
   Magdics M., 2013, P 12 ACM SIGGRAPH IN, P147, DOI DOI 10.1145/2534329.2534348
   Pillai A.S., 2019, Impact of Virtual Reality in Healthcare, P17, DOI [10.4018/978-1-5225-7168-1.ch002, DOI 10.4018/978-1-5225-7168-1.CH002]
   Poupyrev I., 1998, Computer Graphics Forum, V17, pC41, DOI 10.1111/1467-8659.00252
   Prachyabrued M, 2016, IEEE T VIS COMPUT GR, V22, P1718, DOI 10.1109/TVCG.2015.2456917
   Quintana D, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-40546-2
   Quintana D, 2022, IEEE ACCESS, V10, P131567, DOI 10.1109/ACCESS.2022.3229210
   RYAN TA, 1956, AM J PSYCHOL, V69, P60, DOI 10.2307/1418115
   Sanna A, 2016, INT C AUGM VIRT REAL
   Sauvaget C, 2014, WD SCI P COMP ENG, V9, P466
   Schonauer Christian, 2011, P 2 AUGM HUM INT C A, DOI DOI 10.1145/1959826.1959830
   Sreng J, 2006, IEEE T VIS COMPUT GR, V12, P1013, DOI 10.1109/TVCG.2006.189
   Tokunaga DM, 2010, SIGGRAPH 10 ACM SIGG, DOI [10.1145/1836845.1836985, DOI 10.1145/1836845.1836985]
   Törnbom K, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0209214
   Tran DA, 2016, AM J PHYS MED REHAB, V95, P459, DOI 10.1097/PHM.0000000000000438
   Unity Technologies, Unity real-time development platform
   unity3d, Unity technologies: unity manual: scriptable render pipeline fundamentals
   Viola I, 2020, Arxiv, DOI arXiv:1910.03310
   Vosinakis S, 2018, VIRTUAL REAL-LONDON, V22, P47, DOI 10.1007/s10055-017-0313-4
   Wattanasoontorn V, 2013, IDEE: PROCEEDINGS OF THE 2ND INTERNATIONAL WORKSHOP ON INTERACTION DESIGN IN EDUCATIONAL ENVIRONMENTS, P24, DOI 10.5220/0004599400240033
   Weber LM, 2019, AM J PHYS MED REHAB, V98, P783, DOI 10.1097/PHM.0000000000001190
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Zhang Lei, 2020, P 33 ANN ACM S US IN, P342, DOI [10.1145/3379337.3415824, DOI 10.1145/3379337.3415824]
NR 51
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 60
DI 10.1007/s10055-024-00954-9
PG 17
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA JV9Q8
UT WOS:001176059900005
OA hybrid
DA 2024-08-05
ER

PT J
AU Shchory, S
   Nitzan, K
   Harpaz, G
   Doron, R
AF Shchory, Salit
   Nitzan, Keren
   Harpaz, Gal
   Doron, Ravid
TI Not just a game: the effect of active versus passive virtual reality
   experiences on anxiety and sadness
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Anxiety; Depression; Self-efficacy; Active VR
   experience; Passive VR experience
ID TIME PHYSICAL-ACTIVITY; LONG WORKING HOURS; SELF-EFFICACY; POSITIVE
   AFFECT; DEPRESSION; SATISFACTION; METAANALYSIS; OPTIMISM; IMPACT;
   ASSOCIATIONS
AB The use of virtual reality (VR) technology is becoming more common and can be harnessed as a tool to improve various emotional and psychological aspects. The present research explored whether different kinds of VR experience (i.e., active versus passive) would differently affect people's mood, anxiety and sadness. Undergraduate students (n = 133) were randomly assigned to three study conditions: active game VR experience, passive VR experience and control 2D passive viewing and filled out a battery of questionnaires before and after manipulation. The results show that following both VR exposures (but not following the control condition), participants' moods improved, and the degree of anxiety was reduced. The degree of sadness was reduced only following the active game VR experience. Regarding self-efficacy, it was higher in the passive VR experience but lower following the active game VR experience (and not affected by the control condition). In conclusion, the results indicate that short VR experiences could provide a suitable alternative for the lack of accessible treatments to improve mood and to alleviate levels of anxiety and sadness, although further research is needed to tailor and refine the exact VR experience that would best improve each specific psychological aspect.
C1 [Shchory, Salit; Nitzan, Keren; Harpaz, Gal; Doron, Ravid] Open Univ Israel, Dept Educ & Psychol, Raanana, Israel.
   [Shchory, Salit] Univ Haifa, Dept Psychol, Haifa, Israel.
C3 Open University Israel; University of Haifa
RP Doron, R (corresponding author), Open Univ Israel, Dept Educ & Psychol, Raanana, Israel.
EM raviddo@openu.ac.il
RI Harpaz, Gal/AAC-6274-2022
OI Harpaz, Gal/0000-0002-6117-1538; Shchory, Sal'it/0000-0002-7909-3566
FU Open University of Israel
FX No Statement Available
CR Achttien R, 2019, BMC CARDIOVASC DISOR, V19, DOI 10.1186/s12872-019-1065-8
   BANDURA A, 1977, PSYCHOL REV, V84, P191, DOI 10.1037/0033-295X.84.2.191
   Bandura A, 2012, Encyclopedia of human behavior, P7
   Bradley R, 2018, J ENABLING TECHNOL, V12, P101, DOI 10.1108/JET-01-2018-0004
   Brody Debra J, 2018, NCHS Data Brief, P1
   Browning MHEM, 2020, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.02667
   Chan SHM, 2023, VIRTUAL REAL-LONDON, V27, P3285, DOI 10.1007/s10055-021-00604-4
   Chen G, 2001, ORGAN RES METHODS, V4, P62, DOI 10.1177/109442810141004
   Cooney GM, 2013, COCHRANE DB SYST REV, DOI 10.1002/14651858.CD004366.pub6
   Cornelius C, 2020, HEALTH PSYCHOL REP, V8, P408, DOI 10.5114/hpr.2020.99002
   Dale LP, 2019, MENT HEALTH PHYS ACT, V16, P66, DOI 10.1016/j.mhpa.2018.12.001
   Daley A, 2008, J CLIN PSYCHOL MED S, V15, P140, DOI 10.1007/s10880-008-9105-z
   Dyer E, 2018, J MED LIBR ASSOC, V106, P498, DOI 10.5195/jmla.2018.518
   Ettman CK, 2022, LANCET REG HEALTH-AM, V5, DOI 10.1016/j.lana.2021.100091
   Eyre H, 2012, BRAIN BEHAV IMMUN, V26, P251, DOI 10.1016/j.bbi.2011.09.015
   Faul F, 2009, BEHAV RES METHODS, V41, P1149, DOI 10.3758/BRM.41.4.1149
   Ferraz-Torres M, 2022, VIRTUAL REAL-LONDON, V26, P1307, DOI 10.1007/s10055-022-00633-7
   Furman E, 2009, J AM DENT ASSOC, V140, P1508, DOI 10.14219/jada.archive.2009.0102
   Gerber SM, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-13153-1
   Givens JL, 2002, ACAD MED, V77, P918, DOI 10.1097/00001888-200209000-00024
   Gull M., 2016, International Journal of Modern Social Sciences, V5, P42
   Habak S, 2021, INT J ENV RES PUB HE, V18, DOI 10.3390/ijerph18010140
   Hardy JH, 2014, PERS INDIV DIFFER, V71, P151, DOI 10.1016/j.paid.2014.07.034
   Hartl E, 2017, Res Papers 2413-2428
   Hasin DS, 2018, JAMA PSYCHIAT, V75, P336, DOI 10.1001/jamapsychiatry.2017.4602
   Herrero R, 2014, CYBERPSYCH BEH SOC N, V17, P379, DOI 10.1089/cyber.2014.0052
   Jemni M, 2023, FRONT PHYSIOL, V14, DOI 10.3389/fphys.2023.1102526
   Kalantari S, 2022, INNOV AGING, V6, DOI 10.1093/geroni/igac015
   Kandola A, 2019, NEUROSCI BIOBEHAV R, V107, P525, DOI 10.1016/j.neubiorev.2019.09.040
   Kim JY, 2017, ANN LEIS RES, V20, P406, DOI 10.1080/11745398.2016.1238308
   Kroenke K, 2002, PSYCHIAT ANN, V32, P509, DOI 10.3928/0048-5713-20020901-06
   Lahti S, 2020, JDR CLIN TRANSL RES, V5, P312, DOI 10.1177/2380084420901679
   Lee K, 2017, IND HEALTH, V55, P46, DOI 10.2486/indhealth.2015-0173
   Li CQ, 2022, INT J ENV RES PUB HE, V19, DOI 10.3390/ijerph19106017
   Li JM, 2018, PSYCHIAT RES, V268, P243, DOI 10.1016/j.psychres.2018.07.020
   Lindner P, 2021, INT J COGN THER, V14, P23, DOI 10.1007/s41811-020-00090-7
   Liu H, 2017, REV PSICOL DEPORTE, V26, P110
   Martínez-Martí ML, 2017, J POSIT PSYCHOL, V12, P110, DOI 10.1080/17439760.2016.1163403
   Mazgelyte E, 2021, HEALTHCARE-BASEL, V9, DOI 10.3390/healthcare9121729
   McDowell CP, 2018, EXP GERONTOL, V112, P68, DOI 10.1016/j.exger.2018.09.004
   McDowell CP, 2019, AM J PREV MED, V57, P545, DOI 10.1016/j.amepre.2019.05.012
   McMahon EM, 2017, EUR CHILD ADOLES PSY, V26, P111, DOI 10.1007/s00787-016-0875-9
   Miloff A, 2019, BEHAV RES THER, V118, P130, DOI 10.1016/j.brat.2019.04.004
   Moir F, 2018, ADV MED EDUC PRACT, V9, P323, DOI 10.2147/AMEP.S137384
   Mostajeran F, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-83277-y
   Motulsky HJ, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-123
   Müller G, 2018, GER J HUM RESOUR MAN, V32, P217, DOI 10.1177/2397002218786020
   Munkholm K, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0237950
   North T. C., 2008, KEY STUDIES SPORT EX, P258
   Gerçeker GÖ, 2020, J CLIN NURS, V29, P1151, DOI 10.1111/jocn.15173
   Patterson DR, 2023, BURNS, V49, P182, DOI 10.1016/j.burns.2022.02.002
   Pavey TG, 2015, J PHYS ACT HEALTH, V12, P915, DOI 10.1123/jpah.2014-0070
   Pourmand A, 2018, CURR PAIN HEADACHE R, V22, DOI 10.1007/s11916-018-0708-2
   Prado CE, 2018, NEUROPSYCHOL REV, V28, P32, DOI 10.1007/s11065-018-9369-5
   Reynolds-Keefer L., 2009, STUDIES LEARNING EVA, V6, P15, DOI DOI 10.1177/00131649921969811
   Riva G., 2015, Immersed in media, P283, DOI DOI 10.1007/978-3-319-10190-3_12
   Saquib J, 2022, SCAND J PAIN, V22, P167, DOI 10.1515/sjpain-2021-0046
   Shah MK, 2021, JAMA NETW OPEN, V4, DOI 10.1001/jamanetworkopen.2020.36469
   Smith S., 2006, North American Journal of Psychology, V8, P171
   Sobin C, 1997, AM J PSYCHIAT, V154, P4
   Spitzer RL, 2006, ARCH INTERN MED, V166, P1092, DOI 10.1001/archinte.166.10.1092
   Vahratian A, 2021, MMWR-MORBID MORTAL W, V70, P490, DOI 10.15585/mmwr.mm7013e2
   Van Kerrebroeck H, 2017, COMPUT HUM BEHAV, V77, P437, DOI 10.1016/j.chb.2017.07.019
   Vidic T, 2021, INT J COGNITIVE RES, V9, P347, DOI 10.23947/2334-8496-2021-9-3-347-357
   Weerdmeester J., 2021, Technol. Mind Behav., V2, DOI [10.1037/tmb0000028, DOI 10.1037/TMB0000028]
   Wiencierz S, 2017, J HEALTH PSYCHOL, V22, P1025, DOI 10.1177/1359105315622557
   Wiese CW, 2018, J POSIT PSYCHOL, V13, P57, DOI 10.1080/17439760.2017.1374436
   Xiang H, 2021, JAMA NETW OPEN, V4, DOI 10.1001/jamanetworkopen.2021.12082
   Yeo NL, 2020, J ENVIRON PSYCHOL, V72, DOI 10.1016/j.jenvp.2020.101500
   Yu CP, 2018, URBAN FOR URBAN GREE, V35, P106, DOI 10.1016/j.ufug.2018.08.013
NR 70
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 20
DI 10.1007/s10055-023-00925-6
PG 10
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA EY0L4
UT WOS:001142376500001
OA hybrid
DA 2024-08-05
ER

PT J
AU Maneuvrier, A
AF Maneuvrier, Arthur
TI Experimenter bias: exploring the interaction between participant's and
   investigator's gender/sex in VR
SO VIRTUAL REALITY
LA English
DT Article
DE Experimenter; Sex; Gender; VR; Replicability; Reproducibility
ID VIDEO GAME ADDICTION; VIRTUAL-REALITY; STEREOTYPE THREAT;
   SEX-DIFFERENCES; INDIVIDUAL-DIFFERENCES; MOTION SICKNESS; GROUP NORMS;
   PAIN; EXPERIENCE; BEHAVIOR
AB This study explores the effect of the experimenter's gender/sex and its interaction with the participant's gender/sex as potential contributors to the replicability crisis, particularly in the man-gendered domain of VR. 75 young men and women from Western France were randomly evaluated by either a man or a woman during a 13-min immersion in a first-person shooter game. Self-administered questionnaires were used to measure variables commonly assessed during VR experiments (sense of presence, cybersickness, video game experience, flow). MANOVAs, ANOVAs and post-hoc comparisons were used. Results indicate that men and women differ in their reports of cybersickness and video game experience when rated by men, whereas they report similar measures when rated by women. These findings are interpreted as consequences of the psychosocial stress triggered by the interaction between the two genders/sexes, as well as the gender conformity effect induced, particularly in women, by the presence of a man in a masculine domain. Corroborating this interpretation, the subjective measure of flow, which is not linked to video games and/or computers, does not seem to be affected by this experimental effect. Methodological precautions are highlighted, notably the brief systematic description of the experimenter, and future exploratory and confirmatory studies are outlined.
C1 [Maneuvrier, Arthur] Univ Bretagne Occidentale, UMR 6285, Lab STICC, 11 Rue Olivier de Serres, F-35000 Brest, France.
C3 Universite de Bretagne Occidentale
RP Maneuvrier, A (corresponding author), Univ Bretagne Occidentale, UMR 6285, Lab STICC, 11 Rue Olivier de Serres, F-35000 Brest, France.
EM arthur.maneuvrier@protonmail.com
CR Amrhein V, 2019, NATURE, V567, P305, DOI 10.1038/d41586-019-00857-9
   Anvari F., 2018, Comprehensive Results in Social Psychology, V3, P266, DOI DOI 10.1080/23743603.2019.1684822
   Arcioni B, 2019, DISPLAYS, V58, P3, DOI 10.1016/j.displa.2018.07.001
   Arns LL, 2005, P IEEE VIRT REAL ANN, P267
   Aslaksen PM, 2007, PAIN, V129, P260, DOI 10.1016/j.pain.2006.10.011
   BACK R, 1977, J CONSULT CLIN PSYCH, V45, P500
   Baker BT, 2015, IEEE INT WORKS MACH, DOI 10.1038/nature.2015.18248
   Baker M, 2016, NATURE, V533, P452, DOI 10.1038/533452a
   Banakou D, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.00917
   Banakou D, 2016, FRONT HUM NEUROSCI, V10, DOI 10.3389/fnhum.2016.00601
   Baniqued PL, 2013, ACTA PSYCHOL, V142, P74, DOI 10.1016/j.actpsy.2012.11.009
   Bargh JA, 1996, J PERS SOC PSYCHOL, V71, P230, DOI 10.1037/0022-3514.71.2.230
   Beasley Berrin., 2002, MASS COMMUNICATION S, V5, P279, DOI [DOI 10.1207/S15327825MCS0503_3, 10.1207/S15327825MCS0503_3]
   Begley CG, 2012, NATURE, V483, P531, DOI 10.1038/483531a
   Begue L., 2017, Frontiers in Psychology, V8, P466, DOI [DOI 10.3389/FPSYG.2017.00466, 10.3389/fpsyg.2017.00466]
   Behm-Morawitz E, 2013, COMPUT HUM BEHAV, V29, P119, DOI 10.1016/j.chb.2012.07.023
   Bergh DD, 2017, STRATEG ORGAN, V15, P423, DOI 10.1177/1476127017701076
   Berke DS, 2012, PSYCHOL MEN MASCULIN, V13, P367, DOI 10.1037/a0026525
   Boccia M, 2016, EXP BRAIN RES, V234, P2799, DOI 10.1007/s00221-016-4682-9
   BORDEN RJ, 1975, J PERS SOC PSYCHOL, V31, P567, DOI 10.1037/h0076480
   Bos JE, 2008, DISPLAYS, V29, P47, DOI 10.1016/j.displa.2007.09.002
   Bosser AG, 2006, LECT NOTES COMPUT SC, V4161, P374
   Bouchard S, 2007, ANN REV CYBERTHERAPY, V5, P128
   Boyd JM, 2018, PHYS CULT SPORT STUD, V78, P33, DOI 10.2478/pcssr-2018-0011
   Breuer J, 2015, CYBERPSYCH BEH SOC N, V18, P197, DOI 10.1089/cyber.2014.0492
   Brown P, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.945800
   Camerer CF, 2018, NAT HUM BEHAV, V2, P637, DOI 10.1038/s41562-018-0399-z
   Cameron JJ, 2019, SOC PERSONAL PSYCHOL, V13, DOI 10.1111/spc3.12506
   CARLI LL, 1989, J PERS SOC PSYCHOL, V56, P565, DOI 10.1037/0022-3514.56.4.565
   Carter Leslie E, 2002, Pain Res Manag, V7, P21
   Chapman CD, 2018, SCI ADV, V4, DOI 10.1126/sciadv.1701427
   Cheng LK, 2014, VIRTUAL REAL-LONDON, V18, P173, DOI 10.1007/s10055-014-0244-2
   CLARK RA, 1952, J EXP PSYCHOL, V44, P391, DOI 10.1037/h0056822
   Clayton KL, 2009, SIGMIS CPR'09: PROCEEDINGS OF THE 2009 ACM SIGMIS COMPUTER PERSONNEL RESEARCH CONFERENCE, P153
   Clernes SA, 2005, J BIOL RHYTHM, V20, P71, DOI 10.1177/0748730404272567
   Clifton J, 2020, VIRTUAL REAL-LONDON, V24, P453, DOI 10.1007/s10055-019-00407-8
   Cogné M, 2017, ANN PHYS REHABIL MED, V60, P164, DOI 10.1016/j.rehab.2015.12.004
   Cogné M, 2018, NEUROPSYCHOLOGY, V32, P385, DOI 10.1037/neu0000435
   Colquhoun D, 2017, ROY SOC OPEN SCI, V4, DOI 10.1098/rsos.171085
   Coxon M, 2016, VIRTUAL REAL-LONDON, V20, P203, DOI 10.1007/s10055-016-0292-x
   Csikszentmihalyi M., 1990, Flow: The psychology of optimal experience
   Dawson DR, 2017, NEUROPSYCHOL REHABIL, V27, P599, DOI 10.1080/09602011.2017.1313379
   de Castell S, 2019, ACTA PSYCHOL, V199, DOI 10.1016/j.actpsy.2019.102895
   De Leo G, 2014, SIMUL HEALTHC, V9, P1, DOI 10.1097/SIH.0b013e3182a99dd9
   Dennison MS, 2016, DISPLAYS, V44, P42, DOI 10.1016/j.displa.2016.07.002
   Dietz TL, 1998, SEX ROLES, V38, P425, DOI 10.1023/A:1018709905920
   Dill KE, 2007, SEX ROLES, V57, P851, DOI 10.1007/s11199-007-9278-1
   Doyen S, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0029081
   Drazkowski D, 2017, INT J PSYCHOL, V52, P415, DOI 10.1002/ijop.12238
   Dunlop JC, 2007, INT J TECHNOL HUM IN, V3, P96, DOI 10.4018/jthi.2007040106
   Entertainment Software Association, 2022, Essential facts about the computer and video game industry
   Evans C, 2013, BRIT J EDUC PSYCHOL, V83, P210, DOI 10.1111/bjep.12015
   Felnhofer A., 2012, P INT SOC PRES RES, P103, DOI DOI 10.1094/PDIS-11-11-0999-PDN
   Feng J, 2007, PSYCHOL SCI, V18, P850, DOI 10.1111/j.1467-9280.2007.01990.x
   FISHER S, 1994, ADDICT BEHAV, V19, P545, DOI 10.1016/0306-4603(94)90010-8
   Fisher TD, 2007, ARCH SEX BEHAV, V36, P89, DOI 10.1007/s10508-006-9094-7
   Flore PC, 2015, J SCHOOL PSYCHOL, V53, P25, DOI 10.1016/j.jsp.2014.10.002
   Flore PC., 2018, COMPREHENSIVE RESULT, V3, P140, DOI DOI 10.1080/23743603.2018.1559647
   Fox J, 2016, J INTERPERS VIOLENCE, V31, P1912, DOI 10.1177/0886260515570747
   GALL M, 1967, J PERS SOC PSYCHOL, V5, P211, DOI 10.1037/h0024130
   Gamito P., 2008, Annual Review of CyberTherapy and Telemedicine: Changing the Face of Healthcare, P83
   Gamito P, 2010, STUD HEALTH TECHNOL, V154, P128, DOI 10.3233/978-1-60750-561-7-128
   Garcia-Sifuentes Y, 2021, ELIFE, V10, DOI [10.7554/eLife.70817, 10.7554/eLife.70817.sa1, 10.7554/eLife.70817.sa2]
   Gestos M, 2018, CYBERPSYCH BEH SOC N, V21, P535, DOI 10.1089/cyber.2017.0376
   Gibson J-J, 1977, Basic processes in reading: perception and comprehension, P155
   Goodman SN, 2018, NATURE, V564, P7, DOI 10.1038/d41586-018-07589-2
   Grassini S, 2020, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.00349
   Greenberg BS, 2010, SIMULAT GAMING, V41, P238, DOI 10.1177/1046878108319930
   Harm DL., 2007, ADAPTIVE CHANGES SEN
   Harris CR, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0072467
   Haydu VB., 2016, Revista Ces Psicologia, V9, P47
   Heater C., 1992, Presence: Teleoperators and Virtual Environments, V2, P262, DOI DOI 10.1162/PRES.1992.1.2.262
   Howarth PA, 2008, DISPLAYS, V29, P117, DOI 10.1016/j.displa.2007.09.009
   Hyde JS, 2019, AM PSYCHOL, V74, P171, DOI 10.1037/amp0000307
   Hyde JS, 2014, ANNU REV PSYCHOL, V65, P373, DOI 10.1146/annurev-psych-010213-115057
   Juul Jesper., 2012, A Casual Revolution: Reinventing Video Games and Their Players
   Kállai I, 2004, PAIN, V112, P142, DOI 10.1016/j.pain.2004.08.008
   Kang YG, 2022, J COMPUT ASSIST LEAR, V38, P1674, DOI 10.1111/jcal.12702
   Kapalo K.A., 2015, Proc. hum. factors ergonomics society annual meeting, DOI DOI 10.1177/1541931215591261
   Kennedy R.S., 1993, Int. J. Aviat. Psy, P203
   KENNEDY RS, 1975, AVIAT SPACE ENVIR MD, V46, P1349
   Keogh E, 2009, EUR J PAIN, V13, P629, DOI 10.1016/j.ejpain.2008.07.002
   Kim D, 2019, COMPUT HUM BEHAV, V93, P346, DOI 10.1016/j.chb.2018.12.040
   Kim YY, 2005, PSYCHOPHYSIOLOGY, V42, P616, DOI 10.1111/j.1469-8986.2005.00349.x
   King DL, 2020, J BEHAV ADDICT, V9, P184, DOI 10.1556/2006.2020.00016
   KIRSCHBAUM C, 1993, NEUROPSYCHOBIOLOGY, V28, P76, DOI 10.1159/000119004
   Klein RA, 2014, SOC PSYCHOL-GERMANY, V45, P142, DOI 10.1027/1864-9335/a000178
   Knight M. M., 2006, ACM SIGGRAPH 2006 Res Posters, DOI [10.1145/1179622.1179846, DOI 10.1145/1179622.1179846]
   Koch SC, 2008, COMPUT EDUC, V51, P1795, DOI 10.1016/j.compedu.2008.05.007
   Kowert R, 2017, R ADV GAME STUD, P1
   Kuittinen J., 2007, P 2007 C FUTURE PLAY, P105, DOI DOI 10.1145/1328202.1328221
   La Viola J. J.  Jr., 2000, SIGCHI Bulletin, V32, P47, DOI 10.1145/333329.333344
   Lachlan K, 2011, COMMUN RES REP, V28, P27, DOI 10.1080/08824096.2010.518924
   Lakens D, 2023, NATURE, V613, P9, DOI 10.1038/d41586-022-04504-8
   LEARY MR, 1994, J PERS SOC PSYCHOL, V67, P664, DOI 10.1037/0022-3514.67.4.664
   LEVINE FM, 1991, PAIN, V44, P69, DOI 10.1016/0304-3959(91)90149-R
   Levine SC, 2016, WIRES COGN SCI, V7, P127, DOI 10.1002/wcs.1380
   Ling Y, 2013, COMPUT HUM BEHAV, V29, P1519, DOI 10.1016/j.chb.2012.12.010
   Lloyd EP, 2020, POL INS BEH BRAIN SC, V7, P198, DOI 10.1177/2372732220942894
   López-Sáez M, 2011, SPAN J PSYCHOL, V14, P74, DOI 10.5209/rev_SJOP.2011.v14.n1.6
   Maneuvrier A, 2023, VIRTUAL REAL-LONDON, V27, P849, DOI 10.1007/s10055-022-00698-4
   Maneuvrier A, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.706712
   Maneuvrier A, 2023, FRONT VIRTUAL REAL, V4, DOI 10.3389/frvir.2023.1307925
   Maneuvrier A, 2020, PRESENCE-VIRTUAL AUG, V29, P141, DOI 10.1162/pres_a_00359
   Maneuvrier A, 2020, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.571713
   Martin GN, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.00523
   McGlone MS, 2006, J APPL DEV PSYCHOL, V27, P486, DOI 10.1016/j.appdev.2006.06.003
   Meadows LA, 2013, ASEE ANNU CONF EXPO
   MERTON RK, 1957, AM SOCIOL REV, V22, P635, DOI 10.2307/2089193
   Minderer M, 2016, NATURE, V533, P324, DOI 10.1038/nature17899
   Möschel M, 2009, EUR CONST LAW REV, V5, P197, DOI 10.1017/S1574019609001977
   Mogil JS, 2012, NAT REV NEUROSCI, V13, P859, DOI 10.1038/nrn3360
   Mori M, 2012, IEEE ROBOT AUTOM MAG, V19, P98, DOI 10.1109/MRA.2012.2192811
   Munafò MR, 2017, NAT HUM BEHAV, V1, DOI 10.1038/s41562-016-0021
   Muratore M, 2019, FRONT BEHAV NEUROSCI, V13, DOI 10.3389/fnbeh.2019.00062
   Nesbitt K, 2017, DISPLAYS, V48, P1, DOI 10.1016/j.displa.2017.01.002
   Nicovich SG, 2005, J COMPUT-MEDIAT COMM, V10
   Nosek BA, 2018, P NATL ACAD SCI USA, V115, P2600, DOI 10.1073/pnas.1708274114
   Ochs M, 2018, HAI'18: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON HUMAN-AGENT INTERACTION, P161, DOI 10.1145/3284432.3284452
   Onyekuru BU, 2015, J Educ Pract, V6, P76
   Ortega JL, 2022, J ASSOC INF SCI TECH, V73, P655, DOI 10.1002/asi.24568
   Owens B., 2018, Nature, DOI [10.1038/d41586-018-07474-y, DOI 10.1038/D41586-018-07474-Y]
   Paassen B, 2017, SEX ROLES, V76, P421, DOI 10.1007/s11199-016-0678-y
   Palmisano S, 2020, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.587698
   Park S, 2009, LECT NOTES COMPUT SC, V5613, P378, DOI 10.1007/978-3-642-02583-9_42
   Parsons TD, 2015, FRONT HUM NEUROSCI, V9, DOI 10.3389/fnhum.2015.00660
   Pashler H, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0042510
   Petri Katharina, 2020, Amer. J. Biomed. Sci., P107, DOI DOI 10.5099/AJ200200107
   Plummer JP, 2017, ERGONOMICS, V60, P1101, DOI 10.1080/00140139.2017.1280187
   Pointer JS, 1999, OPHTHAL PHYSL OPT, V19, P317, DOI 10.1046/j.1475-1313.1999.00441.x
   Pool GJ, 2007, PAIN, V129, P122, DOI 10.1016/j.pain.2006.10.008
   Popper K. R, 1959, The logic of scientific discovery, DOI [10.1063/1.3060577, DOI 10.1063/1.3060577]
   Prinz F, 2011, NAT REV DRUG DISCOV, V10, P712, DOI 10.1038/nrd3439-c1
   Rebenitsch L, 2016, VIRTUAL REAL-LONDON, V20, P101, DOI 10.1007/s10055-016-0285-9
   Rehbein F, 2016, COMPUT HUM BEHAV, V55, P729, DOI 10.1016/j.chb.2015.10.016
   RIKLI R, 1976, RES QUART, V47, P776, DOI 10.1080/10671315.1976.10616740
   Riva G, 2019, CYBERPSYCH BEH SOC N, V22, P82, DOI 10.1089/cyber.2017.29099.gri
   Robillard G., 2002, 25E CONGRES ANNUEL S
   Robinson ME, 2001, J PAIN, V2, P251, DOI 10.1054/jpai.2001.24551
   Romero F, 2019, PHILOS COMPASS, V14, DOI 10.1111/phc3.12633
   Romero F, 2017, PHILOS SCI, V84, P1031, DOI 10.1086/694005
   Ronay R, 2010, SOC PSYCHOL PERS SCI, V1, P57, DOI 10.1177/1948550609352807
   RUMENIK DK, 1977, PSYCHOL BULL, V84, P852, DOI 10.1037/0033-2909.84.5.852
   Rutrecht H, 2021, TIMING TIME PERCEPT, V9, P353, DOI 10.1163/22134468-bja10033
   Saputra R., 2017, P 2017 4 INT C BIOM, P74, DOI [10.1145/3168776.3168797, DOI 10.1145/3168776.3168797]
   Schwind V, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300590
   Shafer C. P., 2017, MediaPsychol. Rev, V11, P1
   Sheridan TB, 2016, PRESENCE-TELEOP VIRT, V25, P75, DOI 10.1162/PRES_e_00247
   Simmons JP, 2011, PSYCHOL SCI, V22, P1359, DOI 10.1177/0956797611417632
   Souza V, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3466817
   Spencer SJ, 1999, J EXP SOC PSYCHOL, V35, P4, DOI 10.1006/jesp.1998.1373
   Spencer SJ, 2016, ANNU REV PSYCHOL, V67, P415, DOI 10.1146/annurev-psych-073115-103235
   Spreij LA, 2022, NEUROPSYCHOL REHABIL, V32, P499, DOI 10.1080/09602011.2020.1831935
   Stanney K, 2020, INT J HUM-COMPUT INT, V36, P1783, DOI 10.1080/10447318.2020.1828535
   Stanney K, 2020, FRONT ROBOT AI, V7, DOI 10.3389/frobt.2020.00004
   Stanney KM, 2003, HUM FACTORS, V45, P504, DOI 10.1518/hfes.45.3.504.27254
   Stanney KM, 1999, APPL ERGON, V30, P27, DOI 10.1016/S0003-6870(98)00039-8
   STEVENSON HW, 1964, J ABNORM SOC PSYCH, V68, P214, DOI 10.1037/h0040395
   Swim JK, 2020, SEX ROLES, V82, P363, DOI 10.1007/s11199-019-01061-9
   Tannenbaum C, 2019, NATURE, V575, P137, DOI 10.1038/s41586-019-1657-6
   Tassinari M, 2022, FRONT PSYCHOL, V12, DOI 10.3389/fpsyg.2021.815497
   Terry DJ, 2000, BRIT J SOC PSYCHOL, V39, P337, DOI 10.1348/014466600164534
   Terry DJ, 1996, PERS SOC PSYCHOL B, V22, P776, DOI 10.1177/0146167296228002
   Tian N, 2022, VIRTUAL REAL-LONDON, V26, P1409, DOI 10.1007/s10055-022-00638-2
   Townsend F., 2013, Editors' Bull, V9, P45, DOI [10.1080/17521742.2013.865333, DOI 10.1080/17521742.2013.865333]
   van Rooij AJ, 2011, ADDICTION, V106, P205, DOI 10.1111/j.1360-0443.2010.03104.x
   Vigil JM, 2014, PAIN RES MANAG, V19, pE13, DOI 10.1155/2014/213950
   Wasserstein RL, 2019, AM STAT, V73, P1, DOI 10.1080/00031305.2019.1583913
   Weech S, 2020, INT J HUM-COMPUT ST, V138, DOI 10.1016/j.ijhcs.2020.102398
   Weech S, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.00158
   Wehden LO, 2021, MEDIA COMMUN-LISBON, V9, P5, DOI 10.17645/mac.v9i1.3170
   West C, 1987, GENDER SOC, V1, P125, DOI 10.1177/0891243287001002002
   Westbrook L, 2015, GENDER SOC, V29, P534, DOI 10.1177/0891243215584758
   Wirth W, 2012, MEDIA PSYCHOL, V15, P19, DOI 10.1080/15213269.2011.648536
   Witkin H-A., 1962, Psychological differentiation: studies of development, pxii, 418, DOI [10.1037/13128-000, DOI 10.1037/13128-000]
   WITKIN HA, 1949, J PERS, V18, P145, DOI 10.1111/j.1467-6494.1949.tb01237.x
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   World Med Assoc, 2013, JAMA-J AM MED ASSOC, V310, P2191, DOI 10.1001/jama.2013.281053
   Yang AHX, 2022, BRAIN INFORM, V9, DOI 10.1186/s40708-022-00172-6
   Yang S, 2022, CYBERPSYCH BEH SOC N, V25, P101, DOI 10.1089/cyber.2021.0037
   Yee N., 2017, QUANTIC FOUNDRY
   Yeung AWK, 2017, FRONT HUM NEUROSCI, V11, DOI 10.3389/fnhum.2017.00468
NR 182
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD APR 15
PY 2024
VL 28
IS 2
AR 96
DI 10.1007/s10055-024-00993-2
PG 18
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA NS4K2
UT WOS:001202427700001
OA hybrid
DA 2024-08-05
ER

PT J
AU Tanriverdi, M
   Mutluay, FK
   Çakir, FB
AF Tanriverdi, Muberra
   Mutluay, Fatma Karantay
   Cakir, Fatma Betuel
TI Video-based exergaming versus conventional rehabilitation on balance in
   pediatric brain tumor survivors: a randomized clinical trial
SO VIRTUAL REALITY
LA English
DT Article
DE Brain tumor; Balance; Video based; Exergame; Rehabilitation
ID VIRTUAL-REALITY; CEREBRAL-PALSY; CHILDREN; THERAPY; RELIABILITY;
   FEASIBILITY; EXERCISE
AB Balance problems are widely reported in Pediatric Brain Tumor Survivors (PBTS) due to tumor localization and the side effects of medical treatment. This study investigates the effects of conventional versus video-based games exercise training (exergaming) on balance in PBTS. The present study was a randomized controlled trial. The study included 23 PBTS who were randomized to a Video-Based balance exergaming Group (VBG) or Conventional balance exercise training Group (CG). In both groups, the interventions were targeted to the balance function and balance exercise training was administered twice a week for 8 weeks. VBG exercised using selected Nintendo Wii Fit Plus (R) balance games while CG received a specially designed balance training using conventional physiotherapy methods. The primary outcome was the balance tests (Timed Up and Go and Nintendo (R) Wii (TM) Center of Gravity: COG), and the secondary outcomes were the functional tests (10-m walking, 2-min walking, 5-step climb/descent/times), and disease effect test (PedsQL Brain Tumor Module). The outcomes were assessed before and after the intervention. At baseline, no significant clinical and outcome assessment differences existed between both groups except for COG (p = 0.0495). After training, overall scores for balance, functional, disease effect tests significantly improved in VBG (p < 0.05) while progress observed in CG was not significant (p > 0.05). Video-based balance exergaming was found effective and more so than conventional balance exercise training in PBTS. Greater effectiveness of exergaming is thought to be due to increased motivation and effort of the children who are more attracted to gaming than conventional exercising. Exergaming could be beneficial both in clinical and home settings with physiotherapist supervision and may reduce the costs of treatment while improving their health-related quality of life.
C1 [Tanriverdi, Muberra] Bezmialem Vakif Univ, Fac Hlth Sci, Dept Physiotherapy & Rehabil, Silahtaraga Caddesi 189, Istanbul, Turkiye.
   [Tanriverdi, Muberra] Istanbul Medipol Univ, Inst Hlth Sci, Istanbul, Turkiye.
   [Mutluay, Fatma Karantay] Istanbul Medipol Univ, Fac Hlth Sci, Dept Physiotherapy & Rehabil, Istanbul, Turkiye.
   [Cakir, Fatma Betuel] Bezmialem Vakif Univ, Fac Med, Dept Child Hlth & Dis, Div Pediat Hematol Oncol, Istanbul, Turkiye.
C3 Bezmialem Vakif University; Istanbul Medipol University; Istanbul
   Medipol University; Bezmialem Vakif University
RP Tanriverdi, M (corresponding author), Bezmialem Vakif Univ, Fac Hlth Sci, Dept Physiotherapy & Rehabil, Silahtaraga Caddesi 189, Istanbul, Turkiye.; Tanriverdi, M (corresponding author), Istanbul Medipol Univ, Inst Hlth Sci, Istanbul, Turkiye.
EM muberratanrverdi@gmail.com
RI Tanrıverdi, Müberra/Q-3781-2017
OI Tanrıverdi, Müberra/0000-0002-7770-9718
CR Braam KI, 2016, COCHRANE DB SYST REV, DOI 10.1002/14651858.CD008796.pub3
   Brooks D, 2001, ARCH PHYS MED REHAB, V82, P1478, DOI 10.1053/apmr.2001.25153
   Bruggers CS, 2018, FRONT PEDIATR, V6, DOI 10.3389/fped.2018.00069
   Bryanton C, 2006, CYBERPSYCHOL BEHAV, V9, P123, DOI 10.1089/cpb.2006.9.123
   Clark RA, 2010, GAIT POSTURE, V31, P307, DOI 10.1016/j.gaitpost.2009.11.012
   Cohen J., 1992, Current Directions in Psychological Science, V1, P98, DOI DOI 10.1111/1467-8721.EP10768783
   Conklin HM, 2019, J INT NEUROPSYCH SOC, V25, P413, DOI 10.1017/S1355617718001170
   Cox E, 2020, CLIN NEUROPHYSIOL, V131, P1533, DOI 10.1016/j.clinph.2020.03.027
   de Baptista CRJA, 2020, REHABIL RES PRACT, V2020, DOI 10.1155/2020/4209812
   DeSantis CE, 2014, CA-CANCER J CLIN, V64, P252, DOI 10.3322/caac.21235
   Deutsch JE, 2011, TOP STROKE REHABIL, V18, P701, DOI 10.1310/tsr1806-701
   Faul F, 2009, BEHAV RES METHODS, V41, P1149, DOI 10.3758/BRM.41.4.1149
   Gokeler A, 2016, KNEE SURG SPORT TR A, V24, P2280, DOI 10.1007/s00167-014-3374-x
   Silva ECGE, 2020, GAMES HEALTH J, V9, P461, DOI 10.1089/g4h.2019.0192
   Hansen A, 2018, DISABIL REHABIL, V40, P1379, DOI 10.1080/09638288.2017.1295472
   Kim J, 2014, CLIN ORTHOP SURG, V6, P103, DOI 10.4055/cios.2014.6.1.103
   Meldrum D, 2012, DISABIL REHABIL-ASSI, V7, P205, DOI 10.3109/17483107.2011.616922
   Meyns P, 2017, GAMES HEALTH J, V6, P39, DOI 10.1089/g4h.2016.0069
   Miller KD, 2019, CA-CANCER J CLIN, V69, P363, DOI 10.3322/caac.21565
   Montoro-Cárdenas D, 2021, DEV MED CHILD NEUROL, V63, P1262, DOI 10.1111/dmcn.14947
   O'Reilly R.C., 2013, Manual of pediatric balance disorders, V1st
   Ospina PA, 2021, COCHRANE DB SYST REV, DOI 10.1002/14651858.CD012924.pub2
   Palmer SN, 2007, PEDIATR BLOOD CANCER, V49, P287, DOI 10.1002/pbc.21026
   POLLACK IF, 1994, NEW ENGL J MED, V331, P1500, DOI 10.1056/NEJM199412013312207
   Putnam C, 2014, GAMES HEALTH J, V3, P366, DOI 10.1089/g4h.2013.0099
   Riggs L, 2017, NEURO-ONCOLOGY, V19, P440, DOI 10.1093/neuonc/now177
   Runco DV., 2019, J NATL CANCER INST M, DOI [10.1093/jncimonographs/lgz025, DOI 10.1093/JNCIMONOGRAPHS/LGZ025]
   Sabel M, 2016, DISABIL REHABIL, V38, P2073, DOI 10.3109/09638288.2015.1116619
   Sato I, 2014, CANCER NURS, V37, pE1, DOI 10.1097/NCC.0000000000000110
   Sklar C, 2002, J PEDIATR ENDOCR MET, V15, P669
   Sparrow J, 2017, PEDIATR PHYS THER, V29, P55, DOI 10.1097/PEP.0000000000000331
   Tanriverdi M, 2023, AN PEDIATR, V98, P62, DOI [10.1016/j.anpedi.2022.04.020, 10.1016/j.anpede.2022.04.013]
   Tanriverdi M, 2022, PALLIAT SUPPORT CARE, V20, P455, DOI 10.1017/S1478951522000268
   Tanriverdi M, 2023, J PEDIAT HEMATOL ONC, V45, pE567, DOI 10.1097/MPH.0000000000002581
   Tatla SK, 2014, DEV NEUROREHABIL, V17, P1, DOI [10.3109/17518423.2012.740508, 10.3109/7518423.2012.740508]
   Varedi M, 2021, J CANCER SURVIV, V15, P311, DOI 10.1007/s11764-020-00932-5
   Varni JW, 2003, AMBUL PEDIATR, V3, P329, DOI 10.1367/1539-4409(2003)003<0329:TPAAPP>2.0.CO;2
   Wall JC, 2000, J REHABIL RES DEV, V37, P109
   Wang M, 2011, NEUROEPIDEMIOLOGY, V36, P2, DOI 10.1159/000320847
   Wikstrom EA, 2012, J ATHL TRAINING, V47, P306, DOI 10.4085/1062-6050-47.3.16
   Wrisley DM, 2005, NEUROL CLIN, V23, P855, DOI 10.1016/j.ncl.2005.01.005
   Zaino Christopher A, 2004, Pediatr Phys Ther, V16, P90, DOI 10.1097/01.PEP.0000127564.08922.6A
   Zoccolillo L, 2015, EUR J PHYS REHAB MED, V51, P669
NR 43
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD APR 10
PY 2024
VL 28
IS 2
AR 94
DI 10.1007/s10055-024-00988-z
PG 11
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA NJ0E5
UT WOS:001199959500001
OA hybrid
DA 2024-08-05
ER

PT J
AU Gigliotti, MF
   Desrosiers, PA
   Ott, L
   Daoudi, M
   Coello, Y
AF Gigliotti, Maria Francesca
   Desrosiers, Paul-Audain
   Ott, Laurent
   Daoudi, Mohamed
   Coello, Yann
TI Different effects of social intention on movement kinematics when
   interacting with a human or virtual agent
SO VIRTUAL REALITY
LA English
DT Article
DE Social intention; Interaction; Virtual agents; Kinematics;
   Object-directed actions
ID COMPUTER-MEDIATED COMMUNICATION; FACE-TO-FACE; OBJECT; PREHENSION; HAND;
   ANTHROPOMORPHISM; REPRESENTATION; ANTICIPATION; INFORMATION; PERCEPTION
AB The execution of object-directed motor actions is known to be influenced by the intention to interact with others. In this study, we tested whether the effects of social intention on the kinematics of object-directed actions depended on whether the task was performed in the presence of a human or a virtual confederate. In two experiments, participants had to grasp a glass and place it to a new position, with either a personal intention (to fill the glass themselves using a bottle) or a social one (to have the glass filled by the human confederate or the virtual agent using the bottle). Experiment 1 showed that the kinematics of the object-directed actions was modulated by the social intention but only when interacting with a human confederate. Experiment 2 showed that the effects of social intention on object-directed actions performance can be improved using feedback-based learning. Specifically, participants proved able to progressively adapt their motor performances as if they were expressing social intention to a virtual confederate as well. These findings emphasize the importance of the modulatory role of social intention on non-verbal motor behaviour, and enrich the understanding of the interaction with virtual agents.
   We studied the effect of social intention on object-directed actions while interacting with a human or a virtual agent.Results showed an effect of social intention on object-directed actions when interacting with a human confederate.No effect of social intention on object-directed actions was found instead when interacting with a virtual agent.Following a training, participants adapted their motor performances and expressed the social intention to the virtual agent.These findings underline the importance of considering social intention when implementing interactions with virtual agents.
C1 [Gigliotti, Maria Francesca; Desrosiers, Paul-Audain; Ott, Laurent; Coello, Yann] Univ Lille, CNRS, UMR 9193, SCALab Sci Cognit & Sci Affect, F-59000 Lille, France.
   [Daoudi, Mohamed] Univ Lille, Inst Mines Telecom, Cent Lille, CNRS,UMR 9189,CRIStAL, F-59000 Lille, France.
   [Daoudi, Mohamed] Univ Lille, Inst Mines Telecom, Ctr Digital Syst, IMT Nord Europe, F-59000 Lille, France.
C3 Centre National de la Recherche Scientifique (CNRS); CNRS - Institute
   for Humanities & Social Sciences (INSHS); Universite de Lille; IMT -
   Institut Mines-Telecom; Universite de Lille; IMT Nord Europe; Centrale
   Lille; Centre National de la Recherche Scientifique (CNRS); Universite
   de Lille; IMT - Institut Mines-Telecom; IMT Nord Europe
RP Coello, Y (corresponding author), Univ Lille, CNRS, UMR 9193, SCALab Sci Cognit & Sci Affect, F-59000 Lille, France.
EM maria-francesca.gigliotti@univ-lille.fr;
   paul-audain.desrosiers@univ-lille.fr; laurent.ott@univ-lille.fr;
   mohamed.daoudi@imt-nord-europe.fr; yann.coello@univ-lille.fr
FU Agence Nationale de la Recherche [FR CNRS 2052, ANR-21-ESRE-0030];
   Research Federation; Program of Investments of Future; Region
   Hauts-de-France; University of Lille; Agence Nationale de la Recherche
   (ANR) [ANR-21-ESRE-0030] Funding Source: Agence Nationale de la
   Recherche (ANR)
FX This research was funded by the Research Federation (FR CNRS 2052)
   Visual Sciences and Cultures and the ANR-21-ESRE-0030- Equipex +
   Continuum national grant from the Program of Investments of Future. MFG
   was financed by the Region Hauts-de-France and the University of Lille.
CR Anderson Keith, 2013, Advances in Computer Entertainment. 10th International Conference, ACE 2013. Proceedings: LNCS 8253, P476, DOI 10.1007/978-3-319-03161-3_35
   Ansuini C, 2006, J NEUROPHYSIOL, V95, P2456, DOI 10.1152/jn.01107.2005
   Ansuini C, 2008, EXP BRAIN RES, V185, P111, DOI 10.1007/s00221-007-1136-4
   Ansuini C, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.00815
   Auriacombe M, 2018, DRUG ALCOHOL DEPEN, V193, P1, DOI 10.1016/j.drugalcdep.2018.08.025
   Baayen RH, 2008, J MEM LANG, V59, P390, DOI 10.1016/j.jml.2007.12.005
   Barr DJ, 2013, J MEM LANG, V68, P255, DOI 10.1016/j.jml.2012.11.001
   Bates D, 2018, Arxiv, DOI [arXiv:1506.04967, DOI 10.48550/ARXIV.1506.04967]
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Becchio C, 2008, CONSCIOUS COGN, V17, P557, DOI 10.1016/j.concog.2007.03.003
   Becchio C, 2008, COGNITION, V106, P894, DOI 10.1016/j.cognition.2007.05.004
   Becchio C, 2012, FRONT HUM NEUROSCI, V6, DOI 10.3389/fnhum.2012.00117
   Bergmann Kirsten, 2012, Intelligent Virtual Agents. Proceedings 12th International Conference, IVA 2012, P126, DOI 10.1007/978-3-642-33197-8_13
   Biocca F, 2003, PRESENCE-VIRTUAL AUG, V12, P456, DOI 10.1162/105474603322761270
   Bombari D, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00869
   Brauer M, 2018, PSYCHOL METHODS, V23, P389, DOI 10.1037/met0000159
   Brozzoli Claudio, 2012, The Neural Bases of Multi-sensory Processes, P447
   Burgoon JK., 2020, Handbook of Communication and Social Interaction skills, P197, DOI [10.4324/9781410607133-12, DOI 10.4324/9781410607133-12]
   Cartaud A, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.00657
   Cassell J., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P413, DOI 10.1145/192161.192272
   Cavallo A, 2016, SCI REP-UK, V6, DOI 10.1038/srep37036
   Cléry J, 2015, NEUROPSYCHOLOGIA, V70, P313, DOI 10.1016/j.neuropsychologia.2014.10.022
   Coello Y, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0196874
   Cuijpers RH, 2004, J NEUROPHYSIOL, V91, P2598, DOI 10.1152/jn.00644.2003
   Daoudi M, 2018, IEEE INT CONF AUTOMA, P512, DOI 10.1109/FG.2018.00082
   de Visser EJ, 2016, J EXP PSYCHOL-APPL, V22, P331, DOI 10.1037/xap0000092
   Devanne M, 2015, IEEE INT CONF AUTOMA
   di Pellegrino G, 2015, NEUROPSYCHOLOGIA, V66, P126, DOI 10.1016/j.neuropsychologia.2014.11.011
   Duarte NF, 2018, IEEE ROBOT AUTOM LET, V3, P4132, DOI 10.1109/LRA.2018.2861569
   Eastough D, 2007, EXP BRAIN RES, V176, P193, DOI 10.1007/s00221-006-0749-3
   Egmose I, 2018, J MOTOR BEHAV, V50, P155, DOI 10.1080/00222895.2017.1327407
   Ferri F, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0015855
   FIKES TG, 1994, J MOTOR BEHAV, V26, P325, DOI 10.1080/00222895.1994.9941688
   FITTS PM, 1954, J EXP PSYCHOL, V47, P381, DOI 10.1037/h0055392
   Fleury L, 2019, CORTEX, V119, P480, DOI 10.1016/j.cortex.2019.07.012
   Fox J, 2015, HUM-COMPUT INTERACT, V30, P401, DOI 10.1080/07370024.2014.921494
   GENTILUCCI M, 1991, NEUROPSYCHOLOGIA, V29, P361, DOI 10.1016/0028-3932(91)90025-4
   Gentilucci M, 2002, NEUROPSYCHOLOGIA, V40, P1139, DOI 10.1016/S0028-3932(01)00233-0
   Georgiou I, 2007, COGNITION, V102, P415, DOI 10.1016/j.cognition.2006.01.008
   Gigliotti MF, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-37189-8
   Gigliotti MF, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-63314-y
   Gigliotti MF, 2021, PSYCHOL RES-PSYCH FO, V85, P181, DOI 10.1007/s00426-019-01242-x
   Hostetter AB, 2011, PSYCHOL BULL, V137, P297, DOI 10.1037/a0022128
   Iachini T, 2016, J ENVIRON PSYCHOL, V45, P154, DOI 10.1016/j.jenvp.2016.01.004
   Jacob P, 2005, TRENDS COGN SCI, V9, P21, DOI 10.1016/j.tics.2004.11.003
   John A, 2020, Package 'car
   Johnson W.L., 2000, INT J ARTIFICIAL INT, V11, P47
   Johnson WL, 2016, INT J ARTIF INTELL E, V26, P25, DOI 10.1007/s40593-015-0065-9
   Judd CM, 2012, J PERS SOC PSYCHOL, V103, P54, DOI 10.1037/a0028347
   Ke FF, 2013, J EDUC RES, V106, P441, DOI 10.1080/00220671.2013.832999
   Krämer NC, 2008, LECT NOTES COMPUT SC, V5208, P507
   Krishnan-Barman S, 2017, NEUROPSYCHOLOGIA, V105, P101, DOI 10.1016/j.neuropsychologia.2017.01.018
   Kuznetsova A, 2017, J STAT SOFTW, V82, P1, DOI 10.18637/jss.v082.i13
   Lee EJ, 2002, HUM COMMUN RES, V28, P349, DOI 10.1093/hcr/28.3.349
   Lenth MR, 2019, Package ' emmeans, DOI https://CRAN.R-project.org/package=emmeans
   Lewkowicz D, 2016, BEHAV RES METHODS, V48, P366, DOI 10.3758/s13428-015-0580-5
   Lewkowicz D, 2013, ADAPT BEHAV, V21, P315, DOI 10.1177/1059712313501347
   Leys C, 2013, J EXP SOC PSYCHOL, V49, P764, DOI 10.1016/j.jesp.2013.03.013
   Lucas GM, 2017, FRONT ROBOT AI, V4, DOI 10.3389/frobt.2017.00051
   Lugrin B, 2018, 18TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA'18), P59, DOI 10.1145/3267851.3267859
   Luke SG, 2017, BEHAV RES METHODS, V49, P1494, DOI 10.3758/s13428-016-0809-y
   Maloney D., 2020, P ACM HUMAN COMPUTER, V4, P1, DOI [DOI 10.1145/3415246, 10.1145/3415246]
   Marschner L, 2015, INT J PSYCHOPHYSIOL, V97, P85, DOI 10.1016/j.ijpsycho.2015.05.007
   Matuschek H, 2017, J MEM LANG, V94, P305, DOI 10.1016/j.jml.2017.01.001
   Meulenbroek RGJ, 2007, EXP BRAIN RES, V180, P333, DOI 10.1007/s00221-007-0861-z
   Mori M, 2012, IEEE ROBOT AUTOM MAG, V19, P98, DOI 10.1109/MRA.2012.2192811
   Nieuwenhuis R, 2012, R J, V4, P38
   Nowak KL, 2003, PRESENCE-TELEOP VIRT, V12, P481, DOI 10.1162/105474603322761289
   OLDFIELD RC, 1971, NEUROPSYCHOLOGIA, V9, P97, DOI 10.1016/0028-3932(71)90067-4
   Orliaguet JP, 1997, PERCEPTION, V26, P905, DOI 10.1068/p260905
   Park KM, 2011, PSYCHIAT RES, V189, P166, DOI 10.1016/j.psychres.2011.04.003
   Parsons TD, 2017, BRAIN SCI, V7, DOI 10.3390/brainsci7040042
   Parsons TD, 2015, FRONT HUM NEUROSCI, V9, DOI 10.3389/fnhum.2015.00660
   Parsons TD, 2009, STUD HEALTH TECHNOL, V142, P253, DOI 10.3233/978-1-58603-964-6-253
   PAULIGNAN Y, 1991, EXP BRAIN RES, V83, P502
   Pelachaud C, 2009, PHILOS T R SOC B, V364, P3539, DOI 10.1098/rstb.2009.0186
   Pelachaud C, 2009, SPEECH COMMUN, V51, P630, DOI 10.1016/j.specom.2008.04.009
   Perez-Marcos D, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.02120
   Perez-Marcos D, 2017, J NEUROENG REHABIL, V14, DOI 10.1186/s12984-017-0328-9
   Pertaub DP, 2002, PRESENCE-TELEOP VIRT, V11, P68, DOI 10.1162/105474602317343668
   Philip P, 2020, NPJ DIGIT MED, V3, DOI 10.1038/s41746-019-0213-y
   Poeschl S, 2012, STUD HEALTH TECHNOL, V181, P218, DOI 10.3233/978-1-61499-121-2-218
   Quesque F, 2016, Q J EXP PSYCHOL, V69, P1451, DOI 10.1080/17470218.2015.1083596
   Quesque F, 2015, SOCIOAFFECTIVE NEURO, V5, DOI 10.3402/snp.v5.28602
   Quesque F, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.01407
   Quesque F, 2013, FRONT NEUROROBOTICS, V7, DOI 10.3389/fnbot.2013.00014
   ROSENBAUM DA, 1992, J EXP PSYCHOL LEARN, V18, P1058, DOI 10.1037/0278-7393.18.5.1058
   Ruben MA, 2021, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.611670
   Santello M, 1998, J NEUROPHYSIOL, V79, P1307, DOI 10.1152/jn.1998.79.3.1307
   Sciutti A, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.01362
   Srivastava A, 2011, IEEE T PATTERN ANAL, V33, P1415, DOI 10.1109/TPAMI.2010.184
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   van der Meijden H, 2005, COMPUT HUM BEHAV, V21, P831, DOI 10.1016/j.chb.2003.10.005
   World Med Assoc, 2013, JAMA-J AM MED ASSOC, V310, P2191, DOI 10.1001/jama.2013.281053
NR 94
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD APR 3
PY 2024
VL 28
IS 2
AR 91
DI 10.1007/s10055-024-00992-3
PG 21
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA MX7K6
UT WOS:001196998800001
OA Green Submitted, hybrid
DA 2024-08-05
ER

PT J
AU Lee, JU
   Hwang, S
   Ataya, A
   Kim, S
AF Lee, Jieun
   Hwang, Seokhyun
   Ataya, Aya
   Kim, Seungjun
TI Effect of optical flow and user VR familiarity on curvature gain
   thresholds for redirected walking
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Redirected walking; Detection thresholds; Optical flow;
   VR familiarity
ID IMMERSIVE VIRTUAL ENVIRONMENTS; INDUCED MOTION; REALITY; VECTION; INDEX
AB Virtual reality (VR) locomotion should allow users to move freely in the virtual space while staying within the tracking area in the real space. The redirected walking (RDW) technique enables users to walk naturally in an unlimited virtual space within a limited tracking area by rotating the virtual scene view. However, conflicting visual and vestibular signals during RDW can lead to user discomfort and decreased immersion. To avoid user discomfort, an RDW gain should be within the detection threshold (DT) range. However, a large angle of walking redirection is required when physically avoiding obstacles or escaping from a narrow space, so DT expansion is necessary. In this study, to change the curvature DT range and enhance RDW performance, we proposed an optical flow (OF)-generating vection in a virtual environment. Further, we investigate methods to reduce user discomfort and increase RDW efficiency considering familiar and unfamiliar VR users. The findings showed that the introduction of OF led to a reduction in the DT range for all users, irrespective of the OF's direction. However, conditions with OF resulted in an extended DT range for users familiar with VR while concurrently diminishing the DT range for those who were VR unfamiliar. To delve further, our analysis indicated that when both the OF and redirecting directions were identical, the RDW performance was robust to VR familiarity, whereas in opposing directions, the DT range increased for VR-familiar users. Our study findings suggested using OF for the RDW technique and extending its applicability in virtual environments.
C1 [Lee, Jieun; Hwang, Seokhyun; Ataya, Aya; Kim, Seungjun] Sch Integrated Technol, GIST, 123 Chem Dan Gwa Gi, Gwangju 61005, South Korea.
   [Kim, Seungjun] GIST, AI Grad Sch, 123 Cheom Dan Gwa Gi Ro, Gwangju 61005, South Korea.
C3 Gwangju Institute of Science & Technology (GIST); Gwangju Institute of
   Science & Technology (GIST)
RP Kim, S (corresponding author), Sch Integrated Technol, GIST, 123 Chem Dan Gwa Gi, Gwangju 61005, South Korea.; Kim, S (corresponding author), GIST, AI Grad Sch, 123 Cheom Dan Gwa Gi Ro, Gwangju 61005, South Korea.
EM jieunlee@gm.gist.ac.kr; anoldhsh@gm.gist.ac.kr; aya.ataya@gm.gist.ac.kr;
   seungjun@gist.ac.kr
RI Hwang, Seokhyun/KYQ-3298-2024
OI Hwang, Seokhyun/0000-0001-5244-017X; Kim, SeungJun/0000-0003-0470-2483
FU National Research Foundation of Korea (NRF) - MSIT [2021R1A4A1030075];
   Institute of Information & communications Technology Planning &
   Evaluation (IITP) - Korea government (MSIT), (Artificial Intelligence
   Graduate School Program (GIST)) [2019-0-01842]
FX This research was supported in part by the National Research Foundation
   of Korea (NRF) funded by the MSIT(2021R1A4A1030075) and Institute of
   Information & communications Technology Planning & Evaluation (IITP)
   grant funded by the Korea government (MSIT) (No.2019-0-01842, Artificial
   Intelligence Graduate School Program (GIST)).
CR Akiduki H, 2003, NEUROSCI LETT, V340, P197, DOI 10.1016/S0304-3940(03)00098-3
   Nguyen A, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P645, DOI 10.1109/VR.2018.8446225
   [Anonymous], 2010, J APPL QUANTITATIVE
   Baumberger B, 2000, JPN PSYCHOL RES, V42, P238, DOI 10.1111/1468-5884.00151
   Bölling L, 2019, IEEE T VIS COMPUT GR, V25, P2032, DOI 10.1109/TVCG.2019.2899228
   Bowman DA, 1997, P IEEE VIRT REAL ANN, P45, DOI 10.1109/VRAIS.1997.583043
   Bruder G., 2009, Proceedings of the 15th Joint virtual reality Eurographics conference on Virtual Environments, P145
   Bruder G., 2009, WORKSHOP PERCEPTUAL, P10
   Bubka A, 2010, PERCEPTION, V39, P627, DOI 10.1068/p6315
   Ciumedean CB, 2020, S SPATIAL USER INTER, P1, DOI [10.1145/3385959.3418453, DOI 10.1145/3385959.3418453]
   Cools R, 2019, ACM CONFERENCE ON SPATIAL USER INTERACTION (SUI 2019), DOI 10.1145/3357251.3357580
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Freitag S, 2016, IEEE T VIS COMPUT GR, V22, P1462, DOI 10.1109/TVCG.2016.2518298
   Hair J. F., 2009, Multivariate data analysis
   Hart S. G., 1986, Nasa task load index (tlx)
   HECKMANN T, 1991, PERCEPTION, V20, P285, DOI 10.1068/p200285
   Interrante V, 2006, P IEEE VIRT REAL ANN, P3, DOI 10.1109/VR.2006.52
   Kasarskis P., 2001, Proceedings of the 11th International Symposium on Aviation Psychology, P1
   Keshavarz B, 2019, DISPLAYS, V58, P71, DOI 10.1016/j.displa.2018.07.005
   Kim HK, 2018, APPL ERGON, V69, P66, DOI 10.1016/j.apergo.2017.12.016
   Lombard M., 1997, Journal of Computer-Mediated Communication, V3, DOI [10.1111/j.1083-6101.1997.tb0, 10.1111/j.1083-6101.1997.tb00072.x., 10.1111/j.1083-6101.1997.tb00072.x, DOI 10.1111/J.1083-6101.1997.TB00072.X, 10.1111/J.1083-6101.1997.TB00072.X/4080403]
   Neth CT, 2012, IEEE T VIS COMPUT GR, V18, P1041, DOI 10.1109/TVCG.2011.275
   Nguyen A., 2020, P 26 ACM S VIRT REAL, P1, DOI [10.1145/3385956.3418950, DOI 10.1145/3385956.3418950]
   Nguyen A, 2018, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2018), DOI 10.1145/3225153.3225155
   Ngoc NTA, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P329, DOI 10.1145/2993369.2996304
   Nilsson NC, 2018, COMPUT ENTERTAIN, V16, DOI 10.1145/3180658
   Nilsson NC, 2018, IEEE COMPUT GRAPH, V38, P44, DOI 10.1109/MCG.2018.111125628
   Paludan A, 2016, P IEEE VIRT REAL ANN, P259, DOI 10.1109/VR.2016.7504752
   Park SH, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI [10.1145/3491102.3501847, 10.1016/j.cap.2022.08.010]
   Patla AE, 1997, GAIT POSTURE, V5, P54, DOI 10.1016/S0966-6362(96)01109-5
   Peck TC, 2011, P IEEE VIRT REAL ANN, P55, DOI 10.1109/VR.2011.5759437
   Peck TC, 2009, IEEE T VIS COMPUT GR, V15, P383, DOI 10.1109/TVCG.2008.191
   Pöhlmann KMT, 2022, J COGN ENHANCE, V6, P3, DOI 10.1007/s41465-021-00215-6
   Prokop T, 1997, EXP BRAIN RES, V114, P63, DOI 10.1007/PL00005624
   Qi Sun, 2020, Real VR - Immersive Digital Reality: How to Import the Real World into Head-Mounted Immersive Displays. Lecture Notes in Computer Science (LNCS 11900), P285, DOI 10.1007/978-3-030-41816-8_12
   Razzaque S., 2001, PROC EUROGRAPHICS, P5
   Riecke B.E., 2006, ACM T APPL PERCEPT, V3, DOI 10.1145/1166087.1166091
   Rietzler M, 2018, INT SYM MIX AUGMENT, P115, DOI 10.1109/ISMAR.2018.00041
   Rothacher Y, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-36035-6
   Rowlands J.A., 2000, Handbook of Medical Imaging - Volume 1. Physics and Psychophysics, V1, P225
   Sagnier C, 2019, ADV INTELLIGENT SYST, P305
   Sargunam SP, 2017, P IEEE VIRT REAL ANN, P19, DOI 10.1109/VR.2017.7892227
   Schmitz P, 2018, IEEE T VIS COMPUT GR, V24, P1623, DOI 10.1109/TVCG.2018.2793671
   Slater M., 1994, Presence: teleoperators virtual environments, V3, P130, DOI [10.1162/pres.1994.3.2.130, DOI 10.1162/PRES.1994.3.2.130, 10.1162/pres.1994.3.2.1302]
   Sloane ME, 1992, Noninvasive assessment of the visual system, pSuB4
   Sra M, 2018, DIS 2018: PROCEEDINGS OF THE 2018 DESIGNING INTERACTIVE SYSTEMS CONFERENCE, P59, DOI 10.1145/3196709.3196792
   Stanney KM, 2003, HUM FACTORS, V45, P504, DOI 10.1518/hfes.45.3.504.27254
   Steinicke F., 2008, Proceedings of the ACM Symposium on Virtual Reality Software and Technology (VRST), P149, DOI DOI 10.1145/1450579.1450611
   Steinicke F., 2008, Proceedings of the Virtual Reality International Conference (VRIC), P15
   Steinicke F, 2010, IEEE T VIS COMPUT GR, V16, P17, DOI 10.1109/TVCG.2009.62
   Suma EA, 2012, IEEE VIRTUAL REALITY CONFERENCE 2012 PROCEEDINGS, P43, DOI 10.1109/VR.2012.6180877
   Trutoiu LC, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P190
   Usoh M, 1999, COMP GRAPH, P359, DOI 10.1145/311535.311589
   Waldow K, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P717, DOI 10.1109/VR.2018.8446587
   Warren WH, 2001, NAT NEUROSCI, V4, P213, DOI 10.1038/84054
   Williams B, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P41
   Williams NL, 2019, IEEE T VIS COMPUT GR, V25, P3158, DOI 10.1109/TVCG.2019.2932213
NR 57
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 35
DI 10.1007/s10055-023-00935-4
PG 15
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA GE7L0
UT WOS:001151053900001
OA hybrid
DA 2024-08-05
ER

PT J
AU Reski, N
   Alissandrakis, A
   Kerren, A
AF Reski, Nico
   Alissandrakis, Aris
   Kerren, Andreas
TI Designing a 3D gestural interface to support user interaction with
   time-oriented data as immersive 3D radar charts
SO VIRTUAL REALITY
LA English
DT Article
DE Empirical study; Immersive analytics; User interface design; Virtual
   reality; 3D gestural input; 3D radar chart
ID HUMAN-COMPUTER INTERACTION; HAND GESTURES; VISUALIZATION; SUS
AB The design of intuitive three-dimensional user interfaces is vital for interaction in virtual reality, allowing to effectively close the loop between a human user and the virtual environment. The utilization of 3D gestural input allows for useful hand interaction with virtual content by directly grasping visible objects, or through invisible gestural commands that are associated with corresponding features in the immersive 3D space. The design of such interfaces remains complex and challenging. In this article, we present a design approach for a three-dimensional user interface using 3D gestural input with the aim to facilitate user interaction within the context of Immersive Analytics. Based on a scenario of exploring time-oriented data in immersive virtual reality using 3D Radar Charts, we implemented a rich set of features that is closely aligned with relevant 3D interaction techniques, data analysis tasks, and aspects of hand posture comfort. We conducted an empirical evaluation (n=12)\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$(n=12)$$\end{document}, featuring a series of representative tasks to evaluate the developed user interface design prototype. The results, based on questionnaires, observations, and interviews, indicate good usability and an engaging user experience. We are able to reflect on the implemented hand-based grasping and gestural command techniques, identifying aspects for improvement in regard to hand detection and precision as well as emphasizing a prototype's ability to infer user intent for better prevention of unintentional gestures.
C1 [Reski, Nico; Alissandrakis, Aris] Linnaeus Univ, Dept Comp Sci & Media Technol, VRxAR Labs, Vaxjo, Sweden.
   [Kerren, Andreas] Linnaeus Univ, Dept Comp Sci & Media Technol, ISOVIS, Vaxjo, Sweden.
   [Reski, Nico; Kerren, Andreas] Linkoping Univ, Dept Sci & Technol, iVis, Norrkoping, Sweden.
C3 Linnaeus University; Linnaeus University; Linkoping University
RP Reski, N (corresponding author), Linnaeus Univ, Dept Comp Sci & Media Technol, VRxAR Labs, Vaxjo, Sweden.; Reski, N (corresponding author), Linkoping Univ, Dept Sci & Technol, iVis, Norrkoping, Sweden.
EM nico.reski@liu.se; aris.alissandrakis@lnu.se; andreas.kerren@liu.se
RI Kerren, Andreas/AAV-9187-2020
OI Kerren, Andreas/0000-0002-0519-2537; Reski, Nico/0000-0001-7485-8649
FU Linkping University
FX The authors wish to thank all the participants of the user interaction
   study. We would like to acknowledge that contents of the manuscript are
   related and have overlaps with the published doctoral dissertation
   (monograph) of the manuscript's first author, i.e., NR (Reski 2022).
CR Aigner W, 2011, HUM-COMPUT INT-SPRIN, P1, DOI 10.1007/978-0-85729-079-3
   [Anonymous], 2017, Good research practice
   Austin CR, 2020, CARTOGR GEOGR INF SC, V47, P214, DOI 10.1080/15230406.2019.1696232
   Bachmann D, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18072194
   Bangor A, 2009, J USABILITY STUD, V4, P114
   Besançon L, 2021, COMPUT GRAPH FORUM, V40, P293, DOI 10.1111/cgf.14189
   Betella A, 2014, P 2014 VIRT REAL INT, P1, DOI [10.1145/2617841.2620711, DOI 10.1145/2617841.2620711]
   Brehmer M, 2013, IEEE T VIS COMPUT GR, V19, P2376, DOI 10.1109/TVCG.2013.124
   Brooke J, 2013, J USABILITY STUD, V8, P29
   Büschel W, 2018, LECT NOTES COMPUT SC, V11190, P95, DOI 10.1007/978-3-030-01388-2_4
   Cordeil M, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P200, DOI [10.1109/VR.2019.8797978, 10.1109/vr.2019.8797978]
   Dwyer T, 2018, LECT NOTES COMPUT SC, V11190, P1, DOI 10.1007/978-3-030-01388-2_1
   Ens Barrett, 2021, CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, DOI 10.1145/3411764.3446866
   Fittkau F, 2015, 2015 IEEE 3RD WORKING CONFERENCE ON SOFTWARE VISUALIZATION (VISSOFT), P130, DOI 10.1109/VISSOFT.2015.7332423
   Fonnet A, 2021, IEEE T VIS COMPUT GR, V27, P2101, DOI 10.1109/TVCG.2019.2929033
   Fruchard B, 2019, P 2019 ACM C HUM FAC
   Hackathorn R, 2016, 2016 WORKSHOP ON IMMERSIVE ANALYTICS (IA), P44, DOI 10.1109/IMMERSIVE.2016.7932382
   Huang YJ, 2017, IEEE PAC VIS SYMP, P41, DOI 10.1109/PACIFICVIS.2017.8031577
   Ivanov A, 2019, IEEE COMPUT GRAPH, V39, P19, DOI 10.1109/MCG.2019.2898941
   KOLENCE KW, 1973, ACM SIGMETRICS PERFO, V2, P2, DOI DOI 10.1145/1041613.1041614
   LaViola J.J., 2017, 3D user interfaces: theory and practice
   LaViola JJ, 2000, P 3 IASTED INT C COM
   Lee B, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2023, DOI 10.1145/3544548.3580754
   Munzner T., 2014, Visualization analysis and design, DOI DOI 10.1201/B17511
   Nehaniv CL, 2005, 2005 IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (RO-MAN), P371
   Norman D.A., 2010, interactions, V17, P6, DOI DOI 10.1145/1744161.1744163
   Norwegian National Committee for Research Ethics in Science and Technology (NENT), 2016, GUID RES ETH SCI TEC
   O'Brien HL, 2018, INT J HUM-COMPUT ST, V112, P28, DOI 10.1016/j.ijhcs.2018.01.004
   Osawa Noritaka, 2000, P ACM S VIRTUAL REAL, P147
   Pavlovic VI, 1997, IEEE T PATTERN ANAL, V19, P677, DOI 10.1109/34.598226
   Rempel D, 2014, INT J HUM-COMPUT ST, V72, P728, DOI 10.1016/j.ijhcs.2014.05.003
   Reski N., 2020, P 11 NORD C HUM COMP, P1, DOI [10.1145/3419249.3420171, DOI 10.1145/3419249.3420171]
   Reski N, 2022, Supporting data interaction and hybrid asymmetric collaboration using virtual reality within the context of immersive analytics
   Reski N, 2022, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.743445
   Roberts JC, 2014, IEEE COMPUT GRAPH, V34, P26, DOI 10.1109/MCG.2014.82
   Samini A, 2017, 2017 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P206, DOI 10.1109/CW.2017.25
   Schwind V, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300590
   Shneiderman B, 1996, IEEE SYMPOSIUM ON VISUAL LANGUAGES, PROCEEDINGS, P336, DOI 10.1109/VL.1996.545307
   Skarbez R, 2019, FRONT ROBOT AI, V6, DOI 10.3389/frobt.2019.00082
   Stanney KM, 1998, PRESENCE-TELEOP VIRT, V7, P327, DOI 10.1162/105474698565767
   Stellmach S., 2012, Proc. CHI, P2981, DOI DOI 10.1145/2207676.2208709
   Streppel B, 2018, LECT NOTES COMPUT SC, V10909, P183, DOI 10.1007/978-3-319-91581-4_14
   Ulinski AC, 2009, 3DUI : IEEE SYMPOSIUM ON 3D USER INTERFACES 2009, PROCEEDINGS, P51, DOI 10.1109/3DUI.2009.4811205
   Wagner JA, 2020, IEEE T VIS COMPUT GR, V26, P514, DOI 10.1109/TVCG.2019.2934415
   Ward MO., 2015, Interactive data visualization: foundations, techniques, and applications, V2, DOI DOI 10.1201/B18379
   Yi JS, 2007, IEEE T VIS COMPUT GR, V13, P1224, DOI 10.1109/TVCG.2007.70515
NR 46
TC 0
Z9 0
U1 12
U2 12
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 30
DI 10.1007/s10055-023-00913-w
PG 24
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA GJ8P2
UT WOS:001152396200001
OA Green Published, hybrid
DA 2024-08-05
ER

PT J
AU Alieldin, R
   Peyre, S
   Nofziger, A
   Borasi, R
AF Alieldin, Riham
   Peyre, Sarah
   Nofziger, Anne
   Borasi, Raffaella
TI Effectiveness of immersive virtual reality in teaching empathy to
   medical students: a mixed methods study
SO VIRTUAL REALITY
LA English
DT Article
DE Medical education; Virtual reality; Empathy training
ID EDUCATION
AB Empathy in healthcare has been associated with positive outcomes such as increased patient satisfaction and reduced medical errors. However, research has indicated a decline in empathy among medical professionals. This study examined the effectiveness of Immersive Virtual Reality (IVR) for empathy training in medical education. A convergent mixed methods pretest posttest design was utilized. Participants were 1st-year medical students who engaged in an empathy training IVR educational intervention around a scenario depicting older adults struggling with social isolation. Jefferson Scale of Empathy (JSE) questionnaire was administered before and after the intervention to measure the change in empathy levels. Data were analyzed using a paired sample t-test on the pre-/post-test JSE empathy scores to assess the change in empathy scores. Nineteen qualitative semi structured interviews were conducted immediately after the IVR experience and follow-up interviews were conducted six months later. Qualitative data collected from the interviews' transcripts were analyzed using a thematic and content analysis approach to capture individual experiences. Students (n = 19) scored 5.94 points higher on the posttest JSE questionnaire compared to pretest (p < 0.01) indicating an improvement in empathy levels. Qualitative analysis showed that the IVR training was well received by the students as a valuable empathy-teaching tool. Immersion, presence, and embodiment were identified as the main features of IVR technology that enhanced empathy and understanding of patients' experiences. The debriefing sessions were identified as a key element of the training. IVR-based training could be an effective teaching tool for empathy training in medical education and one that is well received by learners. Results from the study offer preliminary evidence that using IVR to evoke empathy is achievable.
C1 [Alieldin, Riham; Peyre, Sarah; Borasi, Raffaella] Univ Rochester, Warner Sch Educ, 601 Elmwood Ave, Rochester, NY 14642 USA.
   [Nofziger, Anne] Univ Rochester, Med Ctr, Dept Family Med, Rochester, NY USA.
C3 University of Rochester; University of Rochester
RP Alieldin, R (corresponding author), Univ Rochester, Warner Sch Educ, 601 Elmwood Ave, Rochester, NY 14642 USA.
EM ralieldi@u.rochester.edu
CR Abbas JR, 2023, HEALTH POLICY TECHN, V12, DOI 10.1016/j.hlpt.2023.100741
   Bacca-Acosta J, 2023, INFORMATION, V14, DOI 10.3390/info14080465
   Barbot B, 2020, COMPUT HUM BEHAV, V111, DOI 10.1016/j.chb.2020.106431
   Barreda-Angeles M, 2020, CYBERPSYCH BEH SOC N, V23, P683, DOI 10.1089/cyber.2019.0665
   Barteit S, 2021, JMIR SERIOUS GAMES, V9, DOI 10.2196/29080
   Bas-Sarmiento P, 2020, NURSE EDUC PRACT, V44, DOI 10.1016/j.nepr.2020.102739
   Bertrand P, 2018, FRONT ROBOT AI, V5, DOI 10.3389/frobt.2018.00026
   Braun V., 2006, QUAL RES PSYCHOL, V3, P77, DOI 10.1191/1478088706qp063oa
   Brydon M, 2021, J MED IMAGING RADIAT, V52, P466, DOI 10.1016/j.jmir.2021.04.005
   Buchman S., 2019, J Interprofessional Educ Pract, V15, P127, DOI DOI 10.1016/J.XJEP.2019.03.010
   Creswell J. W., 2017, DESIGNING CONDUCTING, DOI DOI 10.1111/J.1753-6405.2007.00096.X
   Davis M. H., 1980, Catalog of Selected Documents in Psychology, V10, P85, DOI DOI 10.1037/0022-3514.44.1.113
   DAVIS MH, 1983, J PERS SOC PSYCHOL, V44, P113, DOI 10.1037/0022-3514.44.1.113
   Decety Jean, 2004, Behav Cogn Neurosci Rev, V3, P71, DOI 10.1177/1534582304267187
   Derksen F, 2013, BRIT J GEN PRACT, V63, DOI [10.3399/bjgp13X660814, 10.3399/bjgpbjgp13X660814]
   Dhakal K, 2022, J MED LIBR ASSOC, V110, P270, DOI 10.5195/jmla.2022.1271
   Dhar E, 2023, DIGIT HEALTH, V9, DOI 10.1177/20552076231158022
   Díez-Goñi N, 2017, REV CLIN ESP, V217, P332, DOI 10.1016/j.rce.2017.01.005
   Dyer E, 2018, J MED LIBR ASSOC, V106, P498, DOI 10.5195/jmla.2018.518
   Elmqaddem N, 2019, INT J EMERG TECHNOL, V14, P234, DOI 10.3991/ijet.v14i03.9289
   Elzie CA, 2021, MED SCI EDUC, V31, P665, DOI 10.1007/s40670-021-01243-9
   Fanning Ruth M, 2007, Simul Healthc, V2, P115, DOI 10.1097/SIH.0b013e3180315539
   Ferreira-Valente A, 2017, ADV HEALTH SCI EDUC, V22, P1293, DOI 10.1007/s10459-016-9704-7
   Fong ZW, 2021, CURR PHARM TEACH LEA, V13, P683, DOI 10.1016/j.cptl.2021.01.040
   Gardner R, 2013, SEMIN PERINATOL, V37, P166, DOI 10.1053/j.semperi.2013.02.008
   Gianakos D, 1996, ARCH INTERN MED, V156, P135, DOI 10.1001/archinte.156.2.135
   Gugliucci M.R., 2019, Innov Aging, V3, pS298, DOI DOI 10.1093/GERONI/IGZ038.1096
   Han I, 2022, J COMPUT ASSIST LEAR, V38, P1115, DOI 10.1111/jcal.12669
   Herrera F, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0204494
   Hojat M, 2002, AM J PSYCHIAT, V159, P1563, DOI 10.1176/appi.ajp.159.9.1563
   Hojat M., 2016, Empathy in health professions education and patient care, DOI 10.1007/978-3-319-27625-0
   Hojat M, 2018, ADV HEALTH SCI EDUC, V23, P899, DOI 10.1007/s10459-018-9839-9
   Hojat M, 2013, AM J MED QUAL, V28, P6, DOI 10.1177/1062860612464731
   Hojat M, 2009, ACAD MED, V84, P1182, DOI 10.1097/ACM.0b013e3181b17e55
   Ingram KM, 2019, J ADOLESCENCE, V71, P72, DOI 10.1016/j.adolescence.2018.12.006
   Jiang HW, 2022, JMIR MED EDUC, V8, DOI 10.2196/34860
   Kyaw BM, 2019, J MED INTERNET RES, V21, DOI 10.2196/12959
   Larson EB, 2005, JAMA-J AM MED ASSOC, V293, P1100, DOI 10.1001/jama.293.9.1100
   Liu JYW, 2024, JMIR MED EDUC, V10, DOI 10.2196/48566
   Maestre JM, 2015, REV ESP CARDIOL, V68, P282, DOI 10.1016/j.rec.2014.05.018
   Majumder MAA, 2020, ADV HUM BIOL, V10, P38, DOI 10.4103/AIHB.AIHB_14_20
   Marques AJ, 2022, FRONT PSYCHOL, V13, DOI 10.3389/fpsyg.2022.814984
   Martingano AJ., 2021, Technology, Mind, and Behavior, V2, DOI [DOI 10.1037/TMB0000034, 10.1037/tmb0000034]
   McDonough K., 2022, Charlest Advis, V24, P9, DOI [10.5260/chara.24.1.9, DOI 10.5260/CHARA.24.1.9]
   Mei W., 2019, Empathy learned through an extended medical education virtual reality project
   Mercer SW, 2002, BRIT J GEN PRACT, V52, pS9
   Mistry D, 2023, CUREUS J MED SCIENCE, V15, DOI 10.7759/cureus.51124
   Morse J M, 1992, Image J Nurs Sch, V24, P273
   Papadopoulos C, 2021, DEMENTIA-LONDON, V20, P2462, DOI 10.1177/1471301221998888
   Patel S, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0221412
   Phrampus P.E., 2013, The comprehensive textbook of healthcare simulation, P73, DOI [DOI 10.1007/978-1-4614-5993-4_6, 10.1007/ 978- 1- 4614- 5993- 4_6]
   Radianti J, 2020, COMPUT EDUC, V147, DOI 10.1016/j.compedu.2019.103778
   Rossnagel C, 2000, EUR J SOC PSYCHOL, V30, P429
   Rudolph Jenny W, 2006, Simul Healthc, V1, P49
   Saldaa Johnny., 2014, Oxford Handbook of Quality Research, P581, DOI DOI 10.1093/OXFORDHB/9780199811755.013.001
   Schutte NS, 2017, MOTIV EMOTION, V41, P708, DOI 10.1007/s11031-017-9641-7
   Sherman William R., 2018, Understanding virtual reality: Interface, application, and design, DOI DOI 10.1016/C2013-0-18583-2
   Shin D, 2018, COMPUT HUM BEHAV, V78, P64, DOI 10.1016/j.chb.2017.09.012
   Slater M, 2016, FRONT ROBOT AI, V3, DOI 10.3389/frobt.2016.00074
   Slater P, 2019, INT J OLDER PEOPLE N, V14, DOI 10.1111/opn.12243
   Swartzlander B., 2017, We are alfred: empathy learned through a medical education virtual reality project
   Tavakol S, 2012, MED EDUC, V46, P306, DOI 10.1111/j.1365-2923.2011.04152.x
   Todres M, 2010, MED TEACH, V32, pE42, DOI 10.3109/01421590903199668
   Ventura S., 2023, Empathy-Advanced Research and Applications, DOI DOI 10.5772/INTECHOPEN.109835
   Ventura S, 2020, CYBERPSYCH BEH SOC N, V23, P667, DOI 10.1089/cyber.2019.0681
   Villalba EE, 2021, COMPUT ELECTR ENG, V93, DOI 10.1016/j.compeleceng.2021.107272
   Wijma EM, 2018, AGING MENT HEALTH, V22, P1115, DOI 10.1080/13607863.2017.1348470
   Zweifach Sarah M, 2019, Digit Biomark, V3, P14, DOI 10.1159/000498923
NR 68
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD JUL 1
PY 2024
VL 28
IS 3
AR 129
DI 10.1007/s10055-024-01019-7
PG 13
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA XJ8F6
UT WOS:001261400900001
OA hybrid
DA 2024-08-05
ER

PT J
AU Albrecht, M
   Streuber, S
   Assländer, L
AF Albrecht, Matthias
   Streuber, Stephan
   Asslaender, Lorenz
TI Improving balance using augmented visual orientation cues: a proof of
   concept
SO VIRTUAL REALITY
LA English
DT Article
DE Augmented reality; Balance; Assistive device; Fall prevention; Walking
   aid; Peripheral vision
ID CORTICAL MAGNIFICATION FACTOR; FALL PREVENTION; STABILIZATION; DEVICE;
   DESIGN; GAIT; SWAY
AB Falls are a major health concern. Existing augmented reality (AR) and virtual reality solutions for fall prevention aim to improve balance in dedicated training sessions. We propose a novel AR prototype as an assistive wearable device to improve balance and prevent falls in daily life. We use a custom head-mounted display toolkit to present augmented visual orientation cues in the peripheral field of view. The cues provide a continuous space-stationary visual reference frame for balance control using the user's tracked head position. In a proof of concept study, users performed a series of balance trials to test the effect of the displayed visual cues on body sway. Our results showed that body sway can be reduced with our device, indicating improved balance. We also showed that superimposed movements of the visual reference in forward-backward or sideways directions induce respective sway responses. This indicates a direction-specific balance integration of the displayed cues. Based on our findings, we conclude that artificially generated visual orientation cues using AR can improve balance and could possibly reduce fall risk.
C1 [Albrecht, Matthias; Asslaender, Lorenz] Univ Konstanz, Human Performance Res Ctr, Univ Str 10, D-78458 Constance, Baden Wurttembe, Germany.
   [Albrecht, Matthias] Univ Konstanz, Human Comp Interact Grp, Univ Str 10, D-78458 Constance, Baden Wurttembe, Germany.
   [Streuber, Stephan] Coburg Univ Appl Sci & Arts, Dept Elect Engn & Comp Sci, Friedrich Streib Str 2, D-96450 Coburg, Bavaria, Germany.
C3 University of Konstanz; University of Konstanz; Klinikum Coburg
RP Albrecht, M (corresponding author), Univ Konstanz, Human Performance Res Ctr, Univ Str 10, D-78458 Constance, Baden Wurttembe, Germany.; Albrecht, M (corresponding author), Univ Konstanz, Human Comp Interact Grp, Univ Str 10, D-78458 Constance, Baden Wurttembe, Germany.
EM matthias.albrecht@uni-konstanz.de; stephan.streuber@hs-coburg.de;
   lorenz.asslaender@uni-konstanz.de
OI Albrecht, Matthias/0000-0001-7444-6863
FU Projekt DEAL; Transfer plattform" of the University of Konstanz;
   EFRE-Programm Baden-Wuerttemberg [2519389]
FX Open Access funding enabled and organized by Projekt DEAL. This research
   was funded by the "Transfer plattform" of the University of Konstanz,
   and the "EFRE-Programm Baden-Wuerttemberg 2021-2027" (ID 2519389).
CR Agrawal Y, 2020, J GERONTOL A-BIOL, V75, P2471, DOI 10.1093/gerona/glaa097
   Albrecht M, 2023, Symposium Virtual Re, P691, DOI 10.1109/VR55154.2023.00084
   Andersen GJ, 2012, WIRES COGN SCI, V3, P403, DOI 10.1002/wcs.1167
   Assländer L, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-29713-7
   Assländer L, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0241479
   Assländer L, 2016, J NEUROPHYSIOL, V116, P272, DOI 10.1152/jn.01145.2015
   Baragash RS, 2022, DIGIT HEALTH, V8, DOI 10.1177/20552076221132099
   Baram Y, 2006, NEUROLOGY, V66, P178, DOI 10.1212/01.wnl.0000194255.82542.6b
   Baram Y, 1999, NEURAL PROCESS LETT, V10, P81, DOI 10.1023/A:1018713516431
   COWEY A, 1974, EXP BRAIN RES, V21, P447
   Dev MK, 2021, OPHTHAL PHYSL OPT, V41, P853, DOI 10.1111/opo.12827
   DIJKSTRA TMH, 1992, HUM MOVEMENT SCI, V11, P195, DOI 10.1016/0167-9457(92)90060-O
   Henry M, 2019, J NEUROPHYSIOL, V122, P525, DOI 10.1152/jn.00067.2019
   Ji JC, 2018, J BACK MUSCULOSKELET, V31, P169, DOI 10.3233/BMR-169765
   Kennedy R.S., 1993, Int. J. Aviat. Psy, P203
   Kouris I, 2018, IEEE ENG MED BIO, P4233, DOI 10.1109/EMBC.2018.8513357
   Luyten K, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P487, DOI 10.1145/2858036.2858339
   Maurer C, 2005, J NEUROPHYSIOL, V93, P189, DOI 10.1152/jn.00221.2004
   Meeks RK, 2023, Physiology of spatial orientation
   Mostajeran F, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376565
   Nishchyk A, 2021, JMIR AGING, V4, DOI 10.2196/27972
   Oh-Park M, 2021, AM J PHYS MED REHAB, V100, P92, DOI 10.1097/PHM.0000000000001554
   Osoba MY, 2019, LARYNGOSCOPE INVEST, V4, P143, DOI 10.1002/lio2.252
   PAULUS WM, 1984, BRAIN, V107, P1143, DOI 10.1093/brain/107.4.1143
   Peterka RJ, 2002, J NEUROPHYSIOL, V88, P1097, DOI 10.1152/jn.2002.88.3.1097
   Schut IM, 2017, GAIT POSTURE, V53, P241, DOI 10.1016/j.gaitpost.2017.02.004
   STOFFREGEN TA, 1985, J EXP PSYCHOL HUMAN, V11, P554, DOI 10.1037/0096-1523.11.5.554
   STOFFREGEN TA, 1986, PERCEPT PSYCHOPHYS, V39, P355, DOI 10.3758/BF03203004
   Strasburger H, 2011, J VISION, V11, DOI 10.1167/11.5.13
   STRAUBE A, 1994, EXP BRAIN RES, V99, P501
   Trkov M, 2017, IFAC PAPERSONLINE, V50, P9802, DOI 10.1016/j.ifacol.2017.08.887
   United Nations, 2021, Factsheet: falls
   Wang Z, 2020, ROBOTICS, V9, DOI 10.3390/robotics9030055
   Warburton M, 2023, BEHAV RES METHODS, V55, P3658, DOI 10.3758/s13428-022-01983-5
   World Med Assoc, 2013, JAMA-J AM MED ASSOC, V310, P2191, DOI 10.1001/jama.2013.281053
   Zoubir AM, 1998, IEEE SIGNAL PROC MAG, V15, P56, DOI 10.1109/79.647043
NR 36
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAY 15
PY 2024
VL 28
IS 2
AR 109
DI 10.1007/s10055-024-01006-y
PG 13
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA QV5K8
UT WOS:001223655900002
OA hybrid
DA 2024-08-05
ER

PT J
AU Dominguez-Dager, B
   Gomez-Donoso, F
   Roig-Vila, R
   Escalona, F
   Cazorla, M
AF Dominguez-Dager, Bessie
   Gomez-Donoso, Francisco
   Roig-Vila, Rosabel
   Escalona, Felix
   Cazorla, Miguel
TI Holograms for seamless integration of remote students in the classroom
SO VIRTUAL REALITY
LA English
DT Article
DE Remote communication; Education; Mixed reality; HoloLens 2; Holograms
AB The new global scenario imposed by the SARS-CoV-2 virus has given rise to an atypical and problematic situation in multiple spheres. In very little time, the abrupt change from face-to-face to remote has not only required a rapid widespread use of digital technology, but also a change in methodology and communicative interactions. In the field of education, teachers have had to interact in new environments, with the combined use of face-to-face and non-face-to-face teaching being a major challenge. This paper presents the design and implementation of a cyber presence system for educational environments using Microsoft's HoloLens 2 Mixed Reality (MR) headset. A software tool is developed that improves teaching scenarios through communication in mixed environments. The tool enables teachers to integrate the students in the classroom in a common space with remote students connected by videoconference. Our system is not limited to education, however, as it can also be deployed in any setting that requires remote communication, such as companies and governmental institutions.
C1 [Dominguez-Dager, Bessie; Gomez-Donoso, Francisco; Escalona, Felix; Cazorla, Miguel] Univ Alicante, Univ Inst Comp Res, Alicante, Spain.
   [Roig-Vila, Rosabel] Univ Alicante, Dept Gen & Specif Didact, Alicante, Spain.
C3 Universitat d'Alacant; Universitat d'Alacant
RP Dominguez-Dager, B (corresponding author), Univ Alicante, Univ Inst Comp Res, Alicante, Spain.
EM bessie.dominguez@ua.es
OI Dominguez-Dager, Bessie/0000-0003-2214-3849
FU Ministerio de Ciencia e Innovacin; European Regional Development Fund
   [PID2022-138453OB-I00, MCIN/AEI/10.13039/501100011033]; ERDF A way of
   making Europe [UAIND21-04C]; University of Alicante
FX This work was carried out under the framework of the grant
   PID2022-138453OB-I00 funded by MCIN/AEI/10.13039/501100011033 and by
   "ERDF A way of making Europe". Partial work carried out by Bessie
   Dominguez-Dager has been funded by the grant UAIND21-04C from University
   of Alicante.
CR Agrawal S, 2015, INT C ADV COMPUT COM, P108, DOI 10.1109/ACCT.2015.32
   [Anonymous], 2005, International Journal of Social Research Methodology: Theory Practice
   Bangor A, 2009, J USABILITY STUD, V4, P114
   Barros P, 2015, IEEE-RAS INT C HUMAN, P582, DOI 10.1109/HUMANOIDS.2015.7363421
   Brooke J., 1996, SUS-a quick and dirty usability scale, DOI [DOI 10.1201/9781498710411-35, DOI 10.1201/9781498710411]
   Caligiana P, 2020, TECHNOLOGIES, V8, DOI 10.3390/technologies8020023
   Condino S, 2018, J HEALTHC ENG, V2018, DOI 10.1155/2018/5435097
   C¢rdova-Esparza DM, 2018, Arxiv, DOI arXiv:1804.02343
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   DAVIS FD, 1989, MIS QUART, V13, P319, DOI 10.2307/249008
   Easterbrook S., 2008, Guide to Advanced Empirical Software Eng, P285, DOI [10.1007/978-1-84800-044-5_11, DOI 10.1007/978-1-84800-044-5_11]
   Ekman P., 1999, handbook of cognition and emotion, P45, DOI [DOI 10.1002/0470013494.CH3, 10.1002/0470013494.ch3]
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Imants J, 2020, J CURRICULUM STUD, V52, P1, DOI 10.1080/00220272.2019.1604809
   Joachimczak M, 2017, P 19 ACM INT C MULT, P514, DOI [10.1145/3136755.3143031, DOI 10.1145/3136755.3143031]
   Li N, 2020, RES LEARN TECHNOL, V28, DOI 10.25304/rlt.v28.2265
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Matovic N, 2023, INT J SOC RES METHOD, V26, P51, DOI 10.1080/13645579.2021.1964857
   Mejia-Escobar C, 2023, Comput Intell Neurosci
   Mezquita R. S., 2020, P INT S DISTR COMP A, P189
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Moro C, 2021, BRIT J EDUC TECHNOL, V52, P680, DOI 10.1111/bjet.13049
   O'Dwyer LM., 2014, QUANTITATIVE RES QUA, DOI DOI 10.4135/9781506335674
   Orts-Escolano S, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P741, DOI 10.1145/2984511.2984517
   Park S, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11167259
   Pejsa T, 2016, ACM CONFERENCE ON COMPUTER-SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING (CSCW 2016), P1716, DOI 10.1145/2818048.2819965
   Piumsomboon T, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173620
   Roig-Vila R, 2021, RIED-REV IBEROAM EDU, V24, P197, DOI 10.5944/ried.24.1.27510
   Rosebrock A, 2021, Github-pyimagesearch/imutils
   Sanyal Hrithik, 2021, Proceedings of International Conference on Sustainable Expert Systems. ICSES 2020. Lecture Notes in Networks and Systems (LNNS 176), P107, DOI 10.1007/978-981-33-4355-9_9
   Sauro J, 2012, QUANTIFYING THE USER EXPERIENCE: PRACTICAL STATISTICS FOR USER RESEARCH, P1
   Schmid III Maj. Gen. (Dr.)Josef, 2023, Telehealth Med Today, V8, DOI [10.30953/thmt.v8.385, DOI 10.30953/THMT.V8.385]
   Akhtar MS, 2019, Arxiv, DOI arXiv:1905.05812
   Singh H, 2021, ADV EDUC TECHNOL INS, P15, DOI 10.4018/978-1-7998-7607-6.ch002
   Tang YM, 2018, INT J ENG BUS MANAG, V10, DOI 10.1177/1847979018809599
   Tay L Y., 2017, Learning: Research and Practice, V3, P98, DOI DOI 10.1080/23735082.2017.1350737
   Themelis C., 2020, Emerging technologies and pedagogies in the curriculum. Bridging human and machine: Future education with intelligence, DOI DOI 10.1007/978-981-15-0618-5_16
   Uddin MP, 2021, IETE TECH REV, V38, P377, DOI 10.1080/02564602.2020.1740615
   Ungureanu D, 2020, Arxiv, DOI [arXiv:2008.11239, DOI 10.48550/ARXIV.2008.11239]
   Whitmire D., 2019, J Comput Sci Coll, V35, P173
   Wish-Baratz S, 2019, MED EDUC, V53, P522, DOI 10.1111/medu.13845
   Xie EZ, 2021, ADV NEUR IN, V34
   Yin Robert K., 2009, CASE STUDY RES
NR 43
TC 1
Z9 1
U1 4
U2 4
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 24
DI 10.1007/s10055-023-00924-7
PG 17
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA FH2O3
UT WOS:001144808100001
OA hybrid
DA 2024-08-05
ER

PT J
AU Karakostas, I
   Valakou, A
   Gavgiotaki, D
   Stefanidi, Z
   Pastaltzidis, I
   Tsipouridis, G
   Kilis, N
   Apostolakis, KC
   Ntoa, S
   Dimitriou, N
   Margetis, G
   Tzovaras, D
AF Karakostas, Iason
   Valakou, Aikaterini
   Gavgiotaki, Despoina
   Stefanidi, Zinovia
   Pastaltzidis, Ioannis
   Tsipouridis, Grigorios
   Kilis, Nikolaos
   Apostolakis, Konstantinos C.
   Ntoa, Stavroula
   Dimitriou, Nikolaos
   Margetis, George
   Tzovaras, Dimitrios
TI A real-time wearable AR system for egocentric vision on the edge
SO VIRTUAL REALITY
LA English
DT Article
DE Intelligent user interfaces; Augmented reality; Artificial intelligence;
   Situational awareness
AB Real-time performance is critical for Augmented Reality (AR) systems as it directly affects responsiveness and enables the timely rendering of virtual content superimposed on real scenes. In this context, we present the DARLENE wearable AR system, analysing its specifications, overall architecture and core algorithmic components. DARLENE comprises AR glasses and a wearable computing node responsible for several time-critical computation tasks. These include computer vision modules developed for the real-time analysis of dynamic scenes supporting functionalities for instance segmentation, tracking and pose estimation. To meet real-time requirements in limited resources, concrete algorithmic adaptations and design choices are introduced. The proposed system further supports real-time video streaming and interconnection with external IoT nodes. To improve user experience, a novel approach is proposed for the adaptive rendering of AR content by considering the user's stress level, the context of use and the environmental conditions for adjusting the level of presented information towards enhancing their situational awareness. Through extensive experiments, we evaluate the performance of individual components and end-to-end pipelines. As the proposed system targets time-critical security applications where it can be used to enhance police officers' situational awareness, further experimental results involving end users are reported with respect to overall user experience, workload and evaluation of situational awareness.
C1 [Karakostas, Iason; Pastaltzidis, Ioannis; Tsipouridis, Grigorios; Kilis, Nikolaos; Dimitriou, Nikolaos; Tzovaras, Dimitrios] Informat Technol Inst, Ctr Res & Technol Hellas, GR-57001 Thessaloniki, Greece.
   [Valakou, Aikaterini; Gavgiotaki, Despoina; Stefanidi, Zinovia; Apostolakis, Konstantinos C.; Ntoa, Stavroula; Margetis, George] Fdn Res & Technol Hellas, Inst Comp Sci, GR-70013 Iraklion, Greece.
C3 Centre for Research & Technology Hellas; Foundation for Research &
   Technology - Hellas (FORTH)
RP Karakostas, I (corresponding author), Informat Technol Inst, Ctr Res & Technol Hellas, GR-57001 Thessaloniki, Greece.
EM iason@iti.gr; valakou@ics.forth.gr; gavgiotaki@ics.forth.gr;
   zinastef@ics.forth.gr; gpastal@iti.gr; tsipurid@iti.gr;
   nikolaoskk@iti.gr; kapostol@ics.forth.gr; stant@ics.forth.gr;
   nikdim@iti.gr; gmarget@ics.forth.gr; dimitrios.tzovaras@iti.gr
OI Stephanidis, Constantine/0000-0003-3687-4220; Valakou,
   Katerina/0009-0004-8419-8647; Karakostas, Iason/0000-0002-4786-3060
FU Horizon 2020 Framework Programme [883297]; European Union
FX This project has received funding from the European Union's Horizon 2020
   research and innovation programme under grant agreement No 883297
   (project DARLENE).
CR Abowd GD, 1999, LECT NOTES COMPUT SC, V1707, P304
   Alvarez-Marín A, 2021, IEEE T LEARN TECHNOL, V14, P817, DOI 10.1109/TLT.2022.3144356
   Apostolakis KC, 2021, DARLENE-Improving situational awareness of European law enforcement agents through a combination of augmented reality and artificial intelligence solutions
   Bolya D, 2022, IEEE T PATTERN ANAL, V44, P1108, DOI 10.1109/TPAMI.2020.3014297
   Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925
   Braun V, 2022, QUAL PSYCHOL, V9, P3, DOI 10.1037/qup0000196
   Buettner R, 2020, 2020 IEEE S IND EL A, P1
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Daskalogrigorakis G, 2021, SIGGRAPH '21: ACM SIGGRAPH 2021 POSTERS, DOI 10.1145/3450618.3469175
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dimitriou N, 2017, EUR INTELL SECUR INF, P24, DOI 10.1109/EISIC.2017.13
   Dosovitskiy A., 2021, ICLR
   ElKomy M, 2017, P 2017 CHI C HUM FAC, P2543, DOI [10.1145/3027063.3053179, DOI 10.1145/3027063.3053179]
   Endsley Tristan C., 2017, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V61, P2100, DOI 10.1177/1541931213602007
   Everett M, 2017, Unity3D
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Fereday J., 2006, Int J Qual Methods, V5, P80, DOI [DOI 10.1177/160940690600500107, DOI 10.1063/1.2011295]
   Fu ZH, 2021, PROC CVPR IEEE, P13769, DOI 10.1109/CVPR46437.2021.01356
   Gao NY, 2019, IEEE I CONF COMP VIS, P642, DOI 10.1109/ICCV.2019.00073
   Ghiasi G, 2021, PROC CVPR IEEE, P2917, DOI 10.1109/CVPR46437.2021.00294
   Grubert J, 2017, IEEE T VIS COMPUT GR, V23, P1706, DOI 10.1109/TVCG.2016.2543720
   HART S G, 1988, P139
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hoque S, 2021, IEEE ACCESS, V9, P143746, DOI 10.1109/ACCESS.2021.3114399
   Hussain J, 2018, J MULTIMODAL USER IN, V12, P1, DOI 10.1007/s12193-018-0258-2
   Jocher Glenn, 2020, Zenodo
   Karakostas I, 2021, SIGNAL PROCESS-IMAGE, V90, DOI 10.1016/j.image.2020.116011
   Kilis N, 2023, LECT NOTES COMPUT SC, V14234, P37, DOI 10.1007/978-3-031-43153-1_4
   Kim JC, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11041738
   Köppel T, 2021, IEEE PAC VIS SYMP, P91, DOI 10.1109/PacificVis52677.2021.00020
   Kuznetsova A, 2020, INT J COMPUT VISION, V128, P1956, DOI 10.1007/s11263-020-01316-z
   Lavoie R, 2021, VIRTUAL REAL-LONDON, V25, P69, DOI 10.1007/s10055-020-00440-y
   Lee Y., 2020, CVPR, P13906
   Lewis J.R., 2013, P SIGCHI C HUM FACT, P2099, DOI DOI 10.1145/2470654.2481287
   Li R, 2016, IEEE COMPUT SOC CONF, P29, DOI 10.1109/CVPRW.2016.11
   Lim JY, 2021, ENG APPL ARTIF INTEL, V97, DOI 10.1016/j.engappai.2020.104094
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Huajun, 2021, arXiv
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Lu FY, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P930, DOI [10.1109/VR46266.2020.00118, 10.1109/VR46266.2020.1581100361198]
   Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177
   Mao W., 2021, arXiv
   Margetis G., 2021, Handbook of human factors and ergonomics, P1085, DOI [10.1002/9781119636113.ch42, DOI 10.1002/9781119636113.CH42]
   Margetis G, 2019, MULTIMED TOOLS APPL, V78, P13387, DOI 10.1007/s11042-018-7088-9
   Oulasvirta A, 2020, P IEEE, V108, P434, DOI 10.1109/JPROC.2020.2969687
   Pellas N, 2019, VIRTUAL REAL-LONDON, V23, P329, DOI 10.1007/s10055-018-0347-2
   Pradeep P, 2019, COMPUT COMMUN, V137, P44, DOI 10.1016/j.comcom.2019.02.002
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Rill B.R., 2018, The art of co-creation: a guidebook for practitioners, DOI [10.1007/978-981-10-8500-0, DOI 10.1007/978-981-10-8500-0]
   Salmon PM, 2009, INT J IND ERGONOM, V39, P490, DOI 10.1016/j.ergon.2008.10.010
   Silvennoinen JM, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P4390, DOI 10.1145/2858036.2858462
   Siriwardhana Y, 2021, IEEE COMMUN SURV TUT, V23, P1160, DOI 10.1109/COMST.2021.3061981
   Stefanidi Z, 2022, IEEE ACCESS, V10, P23367, DOI 10.1109/ACCESS.2022.3152743
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Syberfeldt A, 2017, IEEE ACCESS, V5, P9118, DOI 10.1109/ACCESS.2017.2703952
   Szalma JL, 2008, INT J OCCUP SAF ERGO, V14, P119, DOI 10.1080/10803548.2008.11076757
   Tombaugh TN, 2006, ARCH CLIN NEUROPSYCH, V21, P53, DOI 10.1016/j.acn.2005.07.006
   Tsiktsiris D, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20174943
   Wang CY, 2020, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW50498.2020.00203
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Xu Yi.shi, 2022, Advances in Neural Information Processing Systems (NeurIPS)
   Yigitbas E, 2020, LECT NOTES COMPUT SC, V11930, P107, DOI 10.1007/978-3-030-46540-7_11
   Zhang YF, 2021, INT J COMPUT VISION, V129, P3069, DOI 10.1007/s11263-021-01513-4
   Zhang ZN, 2023, VIRTUAL REAL-LONDON, V27, P1327, DOI 10.1007/s10055-022-00742-3
NR 67
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 44
DI 10.1007/s10055-023-00937-2
PG 24
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA IR3R5
UT WOS:001168022100001
OA hybrid
DA 2024-08-05
ER

PT J
AU Yogev, D
   Chatarji, S
   Carl, L
   Levy, L
   Goldberg, T
   Feinberg, O
   Illouz, S
   Spector, R
   Parmet, Y
   Tejman-Yarden, S
AF Yogev, David
   Chatarji, Sumit
   Carl, Lawerence
   Levy, Liran
   Goldberg, Tomer
   Feinberg, Omer
   Illouz, Shay
   Spector, Robert
   Parmet, Yisrael
   Tejman-Yarden, Shai
TI A comparative study of CT-based volumetric assessment methods for total
   lung capacity with the development of an adjustment factor:
   incorporating VR imaging for improved accuracy
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Polygon summation; Static lung volumes; Total lung
   capacity; Computed tomography
ID OBSTRUCTIVE PULMONARY-DISEASE; COMPUTED-TOMOGRAPHY; BODY
   PLETHYSMOGRAPHY; VIRTUAL-REALITY; HELIUM DILUTION; TRANSPLANTATION;
   COEFFICIENT; FIBROSIS; FAILURE
AB Physiological methods for measuring total lung capacity (TLC), including body-box plethysmography (BBP), are costly and require specialized expertise. Computed tomography (CT)-based TLC assessment is essential in clinical practice for candidates of lung transplantation and those unable to undergo standard lung function testing. While CT-based algorithms were studied to estimate TLC, their accuracy should be further evaluated. This study aimed to compare the BBP measurement of TLC (TLCpleth) with three CT-based methods for measuring TLC, one of them is an innovative virtual reality (VR)-based method. Additionally, we aimed to develop an adjustment factor that will allow a new, non-invasive, cost-effective estimation of the TLCpleth. TLC was calculated for 24 adult patients using three different CT-based volumetric assessment methods: an older region-growing algorithm (TLCrg), a more recent convolutional neural network-based algorithm (TLCcnn), and a VR-based method (TLCvr). Agreement between each method and TLCpleth was evaluated, and an adjustment factor was developed using linear regression. The correlation between the three CT-based methods and TLCpleth ranged from 0.91 to 0.92 (p < 0.001). TLCvr measurements were 80.13% (CI:75.08-85.18%, P < 0.001) of TLCpleth measures, whereas TLCcnn and TLCrg estimates were 71.3% and 77.1% of TLCpleth, respectively. An adjustment factor is proposed to estimate TLCpleth based on the three CT-based methods. This study is the first to evaluate the correlation between BBP, VR volumetric analysis, and two iterations of CT volumetric software for measuring total lung capacity (TLC). After being corrected by an adjustment factor, VR- and CT-based assessments provide accurate estimates of TLCpleth.
C1 [Yogev, David; Levy, Liran; Goldberg, Tomer; Illouz, Shay; Tejman-Yarden, Shai] Tel Aviv Univ, Sch Med, Fac Med, IL-69978 Tel Aviv, Israel.
   [Yogev, David; Feinberg, Omer; Illouz, Shay; Spector, Robert; Tejman-Yarden, Shai] Sheba Med Ctr, Engn Med Res Lab, Ramat Gan, Israel.
   [Chatarji, Sumit; Levy, Liran] Sheba Med Ctr, Inst Pulm Med, Ramat Gan, Israel.
   [Carl, Lawerence] Sheba Med Ctr, Dept Diagnost Imaging, Ramat Gan, Israel.
   [Spector, Robert] Technion Israel Inst Technol, Comp Sci Dept, Geometr Image Proc GIP Lab, Haifa, Israel.
   [Parmet, Yisrael] Ben Gurion Univ Negev, Dept Ind Engn & Management, Beer Sheva, Israel.
   [Tejman-Yarden, Shai] Sheba Med Ctr, Edmond J Safra Int Congenital Heart Ctr, Derech Sheba 1, IL-52621 Ramat Gan, Israel.
C3 Tel Aviv University; Chaim Sheba Medical Center; Chaim Sheba Medical
   Center; Chaim Sheba Medical Center; Technion Israel Institute of
   Technology; Ben Gurion University; Chaim Sheba Medical Center
RP Tejman-Yarden, S (corresponding author), Tel Aviv Univ, Sch Med, Fac Med, IL-69978 Tel Aviv, Israel.; Tejman-Yarden, S (corresponding author), Sheba Med Ctr, Engn Med Res Lab, Ramat Gan, Israel.; Tejman-Yarden, S (corresponding author), Sheba Med Ctr, Edmond J Safra Int Congenital Heart Ctr, Derech Sheba 1, IL-52621 Ramat Gan, Israel.
EM tegmanya@gmail.com
RI Parmet, Yisrael/F-1982-2012
OI Parmet, Yisrael/0000-0002-2071-7338
CR ALTMAN DG, 1983, J ROY STAT SOC D-STA, V32, P307, DOI 10.2307/2987937
   Bakker JT, 2022, ERJ OPEN RES, V8, DOI 10.1183/23120541.00492-2021
   Brown MS, 2010, ACAD RADIOL, V17, P316, DOI 10.1016/j.acra.2009.10.005
   BROWN R, 1978, AM REV RESPIR DIS, V118, P685, DOI 10.1164/arrd.1978.118.4.685
   Camargo JJP, 2009, PEDIATR TRANSPLANT, V13, P429, DOI 10.1111/j.1399-3046.2008.01016.x
   Cardenas CE, 2019, SEMIN RADIAT ONCOL, V29, P185, DOI 10.1016/j.semradonc.2019.02.001
   Caruso P, 2015, J BRAS PNEUMOL, V41, P110, DOI 10.1590/S1806-37132015000004474
   Choi JY, 2020, J CLIN MED, V9, DOI 10.3390/jcm9113426
   Ciobota N.-d., 2012, Materials and Mechanics, V2, P81
   Cliff IJ, 1999, THORAX, V54, P329, DOI 10.1136/thx.54.4.329
   Cooper BG, 2011, THORAX, V66, P714, DOI 10.1136/thx.2010.139881
   Coxson HO, 2014, AM J RESP CRIT CARE, V190, P135, DOI 10.1164/rccm.201402-0256PP
   Coxson HO, 2013, J THORAC IMAG, V28, P272, DOI 10.1097/RTI.0b013e31829efbe9
   Criée CP, 2011, RESP MED, V105, P959, DOI 10.1016/j.rmed.2011.02.006
   D'Ascanio M, 2020, INT J CHRONIC OBSTR, V15, P2583, DOI 10.2147/COPD.S264261
   Dettmer S, 2018, EUR J RADIOL, V106, P137, DOI 10.1016/j.ejrad.2018.07.016
   Eberlein Michael, 2013, Ann Am Thorac Soc, V10, P418, DOI 10.1513/AnnalsATS.201301-008OC
   Alves AFF, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0251783
   Fred HL, 2004, TEX HEART I J, V31, P345
   Freidin D, 2023, PRS-GLOB OPEN, V11, DOI 10.1097/GOX.0000000000004773
   Garfield JL, 2012, INT J CHRONIC OBSTR, V7, P119, DOI 10.2147/COPD.S26419
   Grippi MA, 2015, Chapter 33: pulmonary function testing
   Haas M, 2014, ACAD RADIOL, V21, P633, DOI 10.1016/j.acra.2014.01.002
   Herrmann P, 2021, FRONT PHYSIOL, V12, DOI 10.3389/fphys.2021.676118
   Hofmanninger J, 2020, EUR RADIOL EXP, V4, DOI 10.1186/s41747-020-00173-2
   Hoogendijk EO, 2019, LANCET, V394, P1365, DOI 10.1016/S0140-6736(19)31786-6
   Hunger T, 2021, DIAGNOSTICS, V11, DOI 10.3390/diagnostics11061040
   Iwano S, 2009, ACAD RADIOL, V16, P250, DOI 10.1016/j.acra.2008.09.019
   Kakavas S, 2021, NPJ PRIM CARE RESP M, V31, DOI 10.1038/s41533-021-00236-w
   Kang HS, 2018, J THORAC DIS, V10, P2179, DOI 10.21037/jtd.2018.03.121
   Kauczor HU, 1998, AM J ROENTGENOL, V171, P1091, DOI 10.2214/ajr.171.4.9763003
   Kim Y, 2022, FRONT MED-LAUSANNE, V9, DOI 10.3389/fmed.2022.835069
   Kim YJ, 2016, HEALTHC INFORM RES, V22, P305
   Konheim JA, 2016, J THORAC CARDIOV SUR, V151, P1163, DOI 10.1016/j.jtcvs.2015.10.051
   Lawson G, 2016, APPL ERGON, V53, P323, DOI 10.1016/j.apergo.2015.06.024
   Li HB, 2014, IEEE T VIS COMPUT GR, V20, P919, DOI 10.1109/TVCG.2013.253
   LIN LI, 1989, BIOMETRICS, V45, P255, DOI 10.2307/2532051
   López J, 2021, J COMPUT PHYS, V444, DOI 10.1016/j.jcp.2021.110579
   Luo J, 2017, SCI REP-UK, V7, DOI 10.1038/srep40870
   Madan Rachna, 2014, Indian J Radiol Imaging, V24, P318, DOI 10.4103/0971-3026.143894
   Makarov SN., 2021, EMBC 2019 BRAIN IN 2, DOI [10.1007/978-3-030-45623-8, DOI 10.1007/978-3-030-45623-8]
   Mangukia C, 2021, INDIAN J THORAC CARD, V37, P401, DOI 10.1007/s12055-021-01251-9
   Mascalchi M, 2017, J THORAC DIS, V9, P3319, DOI 10.21037/jtd.2017.08.17
   Matsumoto AJ, 2017, J THORAC IMAG, V32, P101, DOI 10.1097/RTI.0000000000000249
   MELE B, 1993, PHYS LETT B, V299, P345, DOI 10.1016/0370-2693(93)90272-J
   Mesanovic N, 2020, Automatic CT image segmentation of the lungs with region growing algorithm
   Nguyen BJ, 2018, TOMOGRAPHY, V4, P204, DOI 10.18383/j.tom.2018.00053
   O'Donnell CR, 2010, CHEST, V137, P1108, DOI 10.1378/chest.09-1504
   Official statement of the European Respiratory Society, 1993, EUR RESPIR J S16, V16, P1
   Ou HH, 2019, MED ENG PHYS, V71, P3, DOI 10.1016/j.medengphy.2019.06.022
   Park CH, 2015, TRANSPL P, V47, P498, DOI 10.1016/j.transproceed.2014.12.025
   Patel Namrata, 2008, Proc Am Thorac Soc, V5, P447, DOI 10.1513/pats.200707-107ET
   Pearson K, 1913, BIOMETRIKA, V9, P22, DOI 10.2307/2331798
   Peng TF, 2020, BIOMED RES INT-UK, V2020, DOI 10.1155/2020/9842768
   Pires F, 2021, J DIGIT IMAGING, V34, P1034, DOI 10.1007/s10278-021-00480-z
   Rahman M.M., 2017, ANWER KHAN MODERN ME, V7, P4, DOI DOI 10.3329/AKMMCJ.V7I1.31596
   Reiterer F, 2015, PEDIATR PULM, V50, P1039, DOI 10.1002/ppul.23245
   Revol-Muller C, 2002, PATTERN RECOGN LETT, V23, P137, DOI 10.1016/S0167-8655(01)00116-7
   RODENSTEIN DO, 1982, J APPL PHYSIOL, V52, P949, DOI 10.1152/jappl.1982.52.4.949
   Sadeghi AH, 2021, JTCVS TECHNIQUES, V7, P309, DOI 10.1016/j.xjtc.2021.03.016
   Salisbury ML, 2016, CHEST, V149, P491, DOI 10.1378/chest.15-0530
   Seguin-Givelet A, 2018, J THORAC DIS, V10, pS1187, DOI 10.21037/jtd.2018.02.21
   Si-Mohamed SA, 2022, EUR RADIOL, V32, P4292, DOI 10.1007/s00330-021-08482-9
   STANESCU DC, 1982, J APPL PHYSIOL, V52, P939, DOI 10.1152/jappl.1982.52.4.939
   Sverzellati N, 2007, RADIOL MED, V112, P1160, DOI 10.1007/s11547-007-0213-x
   Swanney MP, 2008, THORAX, V63, P1046, DOI 10.1136/thx.2008.098483
   Tang YJ, 2016, SCI REP-UK, V6, DOI 10.1038/srep37550
   Tejman-Yarden S, 2023, HELIYON, V9, DOI 10.1016/j.heliyon.2023.e14790
   TRUSSELL HJ, 1979, IEEE T SYST MAN CYB, V9, P311, DOI 10.1109/TSMC.1979.4310204
   Tsutsumi Y, 2017, SCAND J TRAUMA RESUS, V25, DOI 10.1186/s13049-017-0396-7
   Vermeiren S, 2016, J AM MED DIR ASSOC, V17, DOI 10.1016/j.jamda.2016.09.010
   Weng WH, 2021, IEEE ACCESS, V9, P16591, DOI 10.1109/ACCESS.2021.3053408
   Yogev D, 2022, HELIYON, V8, DOI 10.1016/j.heliyon.2022.e11250
   Zapke M, 2006, RESP RES, V7, DOI 10.1186/1465-9921-7-106
   Zhang C, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P935, DOI 10.1109/ICIP.2001.958278
   Zhou JB, 2021, TRANSL LUNG CANCER R, V10, P2148, DOI 10.21037/tlcr-21-214
NR 76
TC 0
Z9 0
U1 0
U2 1
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 2
DI 10.1007/s10055-023-00892-y
PG 16
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA DC2F3
UT WOS:001129760900001
DA 2024-08-05
ER

PT J
AU Terenti, M
   Pamparau, C
   Vatavu, RD
AF Terenti, Mihail
   Pamparau, Cristian
   Vatavu, Radu-Daniel
TI The user experience of distal arm-level vibrotactile feedback for
   interactions with virtual versus physical displays
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Vibrotactile feedback; User experience; Virtual
   displays; Touchscreens
ID LOCALIZATION; PLACE; SPACE
AB Haptic feedback, a natural component of our everyday interactions in the physical world, requires careful design in virtual environments. However, feedback location can vary from the fingertip to the finger, hand, and arm due to heterogeneous input/output technology used for virtual environments, from joysticks to controllers, gloves, armbands, and vests. In this work, we report on the user experience of touch interaction with virtual displays when vibrotactile feedback is delivered on the finger, wrist, and forearm. In a first controlled experiment with fourteen participants and virtual displays rendered through a head-mounted device, we report a user experience characterized by high perceived enjoyment, confidence, efficiency, and integration as well as low perceived distraction, difficulty, and confusion. Moreover, we highlight participants' preferences for vibrotactile feedback on the finger compared to other locations on the arm or through the VR controller, respectively. In a follow-up experiment with fourteen new participants and physical touchscreens, we report a similar preference for the finger, but also specific nuances of the self-reported experience, not observed in the first experiment with virtual displays. Overall, our results depict an enhanced user experience when distal vibrotactile feedback is available over no vibrations at all during interactions with virtual and physical displays, for which we propose future work opportunities for augmented interactions in virtual worlds.
C1 [Terenti, Mihail; Pamparau, Cristian; Vatavu, Radu-Daniel] Stefan Cel Mare Univ Suceava, MintViz Lab, 13 Univ, Suceava 720229, Romania.
C3 Stefan cel Mare University of Suceava
RP Vatavu, RD (corresponding author), Stefan Cel Mare Univ Suceava, MintViz Lab, 13 Univ, Suceava 720229, Romania.
EM radu.vatavu@usm.ro
RI Vatavu, Radu-Daniel/F-1820-2017
OI Vatavu, Radu-Daniel/0000-0002-7631-6445
FU H2020 Marie Sklodowska-Curie Actions [860114]; European Union
FX This work is part of a project that has received funding from the
   European Union's Horizon 2020 research and innovation programme under
   the Marie Sk & lstrok;odowska-Curie Grant Agreement No. 860114.
CR Alvina J, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2487, DOI 10.1145/2702123.2702341
   Anwar A, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2023), DOI 10.1145/3544548.3581514
   Ardito C, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2682623
   Asano S, 2015, IEEE T HUM-MACH SYST, V45, P393, DOI 10.1109/THMS.2014.2376519
   Bau O., 2010, P 23 ANN ACM S USER, P283, DOI DOI 10.1145/1866029.1866074
   Bau O, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185585
   Bermejo C, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3465396
   Berning M, 2015, ISWC 2015: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, P31, DOI 10.1145/2802083.2802088
   Bertheaux C, 2020, FRONT HUM NEUROSCI, V13, DOI 10.3389/fnhum.2019.00455
   Bickmann Raoul, 2019, P MENSCH COMPUTER 20, P565, DOI 10.1145/3340764.3344459
   Boer L, 2017, DIS'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON DESIGNING INTERACTIVE SYSTEMS, P903, DOI 10.1145/3064663.3064792
   Bouzbib E., P 32 C LINTERACTION, DOI DOI 10.1145/3450522.3451323
   Brewster S, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P159
   Brooke J., 1996, SUS-a quick and dirty usability scale, DOI [DOI 10.1201/9781498710411-35, DOI 10.1201/9781498710411]
   Cabibihan JJ, 2017, IEEE T AFFECT COMPUT, V8, P108, DOI 10.1109/TAFFC.2015.2509985
   Carter Tom, 2013, Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology, P505, DOI DOI 10.1145/2501988.2502018
   Catana AV, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2023, DOI 10.1145/3544548.3581022
   Cho YJ, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P195, DOI 10.1145/2984511.2984550
   Cholewiak RW, 2004, PERCEPT PSYCHOPHYS, V66, P970, DOI 10.3758/BF03194989
   Cholewiak RW, 2003, PERCEPT PSYCHOPHYS, V65, P1058, DOI 10.3758/BF03194834
   Cibelli M., 1999, Proceedings 10th International Conference on Image Analysis and Processing, P1009, DOI 10.1109/ICIAP.1999.797728
   Cirelli M., 2014, ITS'14 Proceedings, P35, DOI DOI 10.1145/2669485.2669509
   Dariosecq M, 2020, HUCAPP: PROCEEDINGS OF THE 15TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 2: HUCAPP, P45, DOI 10.5220/0008979800450052
   Degraen Donald, 2021, UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology, P936, DOI 10.1145/3472749.3474797
   Dong WH, 2020, INT J DIGIT EARTH, V13, P1484, DOI 10.1080/17538947.2020.1731617
   Elkin Lisa A., 2021, UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology, P754, DOI 10.1145/3472749.3474784
   Elsayed H, 2020, PROC ACM INTERACT MO, V4, DOI 10.1145/3432189
   Emgin SE, 2019, IEEE T VIS COMPUT GR, V25, P2749, DOI 10.1109/TVCG.2018.2855154
   Finstad K, 2010, INTERACT COMPUT, V22, P323, DOI 10.1016/j.intcom.2010.04.004
   Friesen RF, 2024, IEEE T HAPTICS, V17, P216, DOI 10.1109/TOH.2023.3304899
   Garrett JJ., 2011, The elements of user experience: User-centered design for the web and beyond
   Grübel J, 2021, PROCEEDINGS OF THE 5TH MEDIA ARCHITECTURE BIENNALE CONFERENCE, MAB20, P215, DOI 10.1145/3469410.3469435
   Gu XC, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P1991, DOI 10.1145/2858036.2858487
   HART S G, 1988, P139
   Henderson J, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300715
   Heo S, 2019, PROCEEDINGS OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY (UIST 2019), P803, DOI 10.1145/3332165.3347941
   Hertenstein MJ, 2006, EMOTION, V6, P528, DOI 10.1037/1528-3542.6.3.528
   Hessels RS, 2021, PERCEPTION, V50, P913, DOI 10.1177/03010066211047826
   Hoggan E, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P1573
   Israr A., 2015, P 33 ANN ACM C HUM F, P1899, DOI [https://doi.org/10.1145/2702613.2732814, DOI 10.1145/2702613.2732814]
   Ito K, 2019, ACM T APPL PERCEPT, V16, DOI 10.1145/3340961
   Itoh Y, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3453157
   Jansen Y., 2010, P CHI HUM FACT COMP, P4351, DOI [https://doi.org/10.1145/1753846.1754152, DOI 10.1145/1753846.1754152]
   Kato K, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174024
   Kaul OB, 2021, ACM T COMPUT-HUM INT, V28, DOI 10.1145/3458021
   Kaul OB, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P3729, DOI 10.1145/3025453.3025684
   Kim E, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376280
   Kovacs Robert, 2020, UIST '20: Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology, P1046, DOI 10.1145/3379337.3415854
   Kreimeier J, 2019, 12TH ACM INTERNATIONAL CONFERENCE ON PERVASIVE TECHNOLOGIES RELATED TO ASSISTIVE ENVIRONMENTS (PETRA 2019), P289, DOI 10.1145/3316782.3321536
   Kronester M, 2021, EXTENDED ABSTRACTS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'21), DOI 10.1145/3411763.3451655
   Lantz Ed, 2007, P 2007 WORKSH EM DIS, DOI [10.1145/1278240.1278241, DOI 10.1145/1278240.1278241]
   Law ELC, 2014, INT J HUM-COMPUT ST, V72, P526, DOI 10.1016/j.ijhcs.2013.09.006
   Le KD, 2016, PROCEEDINGS OF THE NORDICHI '16: THE 9TH NORDIC CONFERENCE ON HUMAN-COMPUTER INTERACTION - GAME CHANGING DESIGN, DOI 10.1145/2971485.2996721
   Lévesque V, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2481
   Li Y., 2019, Virtual Reality Intell. Hardw., V1, P84, DOI [10.3724/SP.J.2096-5796.2018.0006, DOI 10.3724/SP.J.2096-5796.2018.0006]
   Li-Te Cheng, 1996, Proceedings ACM Multimedia 96, P243, DOI 10.1145/244130.244220
   Liao YC, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P5, DOI 10.1145/3126594.3126627
   Liao YC, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P595, DOI 10.1145/2984511.2984522
   Maeda T, 2022, P 2022 ACM S SPATIAL, DOI [10.1145/3565970.3567703, DOI 10.1145/3565970.3567703]
   Massie T. H., 1994, P S HAPT INT VIRT EN, P295
   McAdam C., 2009, P 23 BRIT HCI GROUP, P504, DOI [DOI 10.5555/1671011.1671076, 10.5555/1671011.1671076]
   MILGRAM P, 1994, IEICE T INF SYST, VE77D, P1321
   Moon HS, 2022, The effect of interaction method and vibrotactile feedback on user experience and performance in the VR games
   Mullenbach J, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P3963, DOI 10.1145/2556288.2557343
   Nukarinen T, 2018, 24TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2018), DOI 10.1145/3281505.3283375
   Pamparau Cristian, 2020, MUM 2020: 19th International Conference on Mobile and Ubiquitous Multimedia, P323, DOI 10.1145/3428361.3432088
   Pamparau C, 2023, VIRTUAL REAL-LONDON, V27, P1765, DOI 10.1007/s10055-023-00776-1
   Pamparau C, 2022, PROCEEDINGS OF THE 2022 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE MEDIA EXPERIENCES, IMX 2022, P161, DOI 10.1145/3505284.3529969
   Park Chaeyong, 2020, UIST '20: Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology, P924, DOI 10.1145/3379337.3415837
   Peng YH, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376847
   Pezent Evan, 2020, CHI EA '20: Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems, P1, DOI 10.1145/3334480.3383151
   Popovici I, 2019, INT SYM MIX AUGMENT, P269, DOI 10.1109/ISMAR.2019.00024
   Poupyrev I., 2004, CHI 04 HUM FACT COMP, P1309, DOI DOI 10.1145/985921.986051
   Preechayasomboon P, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.738613
   Punpongsanon P, 2015, IEEE T VIS COMPUT GR, V21, P1279, DOI 10.1109/TVCG.2015.2459792
   Radianti J, 2020, COMPUT EDUC, V147, DOI 10.1016/j.compedu.2019.103778
   Rubin M, 2022, PHILOS PSYCHOL, DOI 10.1080/09515089.2022.2113771
   Sagayam KM, 2017, VIRTUAL REAL-LONDON, V21, P91, DOI 10.1007/s10055-016-0301-0
   Sang-Won Shim, 2020, Design, User Experience, and Usability. Interaction Design. 9th International Conference, DUXU 2020 Held as Part of the 22nd HCI International Conference, HCII 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12200), P532, DOI 10.1007/978-3-030-49713-2_37
   Sauro J, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1599
   Schönauer C, 2015, LECT NOTES COMPUT SC, V9299, P165, DOI 10.1007/978-3-319-22723-8_14
   Seim C, 2015, Tech. rep.
   Sinclair M, 2019, PROCEEDINGS OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY (UIST 2019), P815, DOI 10.1145/3332165.3347891
   Singhal Tanay, 2021, CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, DOI 10.1145/3411764.3445463
   Smith J, 2007, INT J HUM-COMPUT ST, V65, P376, DOI 10.1016/j.ijhcs.2006.11.006
   Sonderegger A, 2019, LECT NOTES COMPUT SC, V11749, P140, DOI 10.1007/978-3-030-29390-1_8
   Strohmeier P, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P4994, DOI 10.1145/3025453.3025812
   Suhonen K, 2012, P 26 ANN BCS INTERAC, P205, DOI [10.14236/ewic/HCI2012.26, DOI 10.14236/EWIC/HCI2012.26]
   Tennison JL, 2020, ACM T APPL PERCEPT, V17, DOI 10.1145/3383457
   Terenti M, 2022, EXTENDED ABSTRACTS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2022, DOI 10.1145/3491101.3519704
   Tong QQ, 2023, IEEE T HAPTICS, V16, P154, DOI 10.1109/TOH.2023.3266199
   van Beek FE, 2024, IEEE T HAPTICS, V17, P191, DOI 10.1109/TOH.2023.3307872
   Vanden Abeele V, 2020, INT J HUM-COMPUT ST, V135, DOI 10.1016/j.ijhcs.2019.102370
   Vatavu RD, 2022, PROCEEDINGS OF THE 35TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, UIST 2022, DOI 10.1145/3526113.3545662
   Vatavu RD, 2017, INT J HUM-COMPUT INT, V33, P713, DOI 10.1080/10447318.2017.1278897
   Vatavu RD, 2016, PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE ON HUMAN-COMPUTER INTERACTION WITH MOBILE DEVICES AND SERVICES (MOBILEHCI'16), P217, DOI 10.1145/2935334.2935364
   Vatavu RD, 2023, Handbook of human-computer interaction, P1, DOI [10.1007/978-3-319-27648-9_20-1, DOI 10.1007/978-3-319-27648-9_20-1]
   Velloso E, 2015, ACM COMPUT SURV, V48, DOI 10.1145/2816455
   Vökel T, 2008, LECT NOTES COMPUT SC, V5105, P835, DOI 10.1007/978-3-540-70540-6_124
   Wang D, 2019, Virtual Reality Intelligent Hardware, V1, P136, DOI DOI 10.3724/SP.J.2096-5796.2019.0008
   Wigdor D, 2011, BRAVE NUI WORLD: DESIGNING NATURAL USER INTERFACES FOR TOUCH AND GESTURE, P1
   Wobbrock JO, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P143, DOI 10.1145/1978942.1978963
   Wu J, 2012, INT J ADV ROBOT SYST, V9, DOI 10.5772/52373
   Yukang Yan, 2018, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V2, DOI 10.1145/3287076
   Zenner A, 2020, CHI'20: EXTENDED ABSTRACTS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3334480.3383145
   Zhao L, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3312778
NR 106
TC 2
Z9 2
U1 5
U2 5
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR 22
PY 2024
VL 28
IS 2
AR 84
DI 10.1007/s10055-024-00977-2
PG 21
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA LU2Q1
UT WOS:001189252500001
OA hybrid
DA 2024-08-05
ER

PT J
AU Cauz, M
   Clarinval, A
   Dumas, B
AF Cauz, Maxime
   Clarinval, Antoine
   Dumas, Bruno
TI Text readability in augmented reality: a multivocal literature review
SO VIRTUAL REALITY
LA English
DT Article
DE Text; Readability; Legibility; Augmented Reality; Mixed Reality
ID HEAD-MOUNTED-DISPLAYS; TIME COLOR; LEGIBILITY; STYLE; CONTRAST; SIZE;
   EYE
AB Augmented reality (AR) is making its way into many sectors. Its rapid evolution in recent years has led to the development of prototypes demonstrating its effectiveness. However, to be able to push these prototypes to the scale of fully usable applications, it is important to ensure the readability of the texts they include. To this end, we conducted a multivocal literature review (MLR) to determine the text parameters a designer can tune, as well as the contextual constraints they need to pay attention to, in relation to Optical See-Through (OST) and Video See-Through (VST) displays. We also included guidelines from device manufacturing and game engines sites to compare the current state of research in the academic and industrial worlds. The results show that parameters pertaining more to letter legibility have been extensively studied (e.g., color and size), while those pertaining to the whole text still require further research (e.g., alignment or space between lines). The former group of parameters, and their associated constraints, were assembled in the form of two decision trees to facilitate implementation of AR applications. Finally, we also concluded that there was a lack of alignment between academic and industrial recommendations.
C1 [Cauz, Maxime; Clarinval, Antoine; Dumas, Bruno] Univ Namur, Namur Digital Inst, Namur, Belgium.
C3 University of Namur
RP Cauz, M (corresponding author), Univ Namur, Namur Digital Inst, Namur, Belgium.
EM maxime.cauz@unamur.be; antoine.clarinval@unamur.be;
   bruno.dumas@unamur.be
OI Clarinval, Antoine/0000-0003-3936-0532; Dumas, Bruno/0000-0001-5302-4303
FU Region Wallone (RW) Mecatech Federated Learning and Augmented Reality
   for Advanced Control Ccenters (FLARACC) [8482]; Region Wallone,Belgium
   [8482]
FX Project nb. 8482 "Region Wallone (RW) Mecatech Federated Learning and
   Augmented Reality for Advanced Control Ccenters (FLARACC)".Region
   Wallone,Belgium,8482
CR Adams RJ, 2017, INT J MANAG REV, V19, P432, DOI 10.1111/ijmr.12102
   Agic A, 2022, P 11 INT S GRAPH ENG, P835, DOI [10.24867/grid-2022-p92, DOI 10.24867/GRID-2022-P92]
   Arefin MS, 2022, IEEE T VIS COMPUT GR, V28, P2014, DOI 10.1109/TVCG.2022.3150503
   Azuma RT, 1997, PRESENCE-VIRTUAL AUG, V6, P355, DOI 10.1162/pres.1997.6.4.355
   Blanc-Goldhammer Daryn R., 2018, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V62, P1281, DOI 10.1177/1541931218621294
   Borg O, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0129902
   Buchner A, 2009, ERGONOMICS, V52, P882, DOI 10.1080/00140130802641635
   Büttner A, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P663, DOI [10.1109/VRW50115.2020.00-93, 10.1109/VRW50115.2020.00182]
   Cárdenas-Robledo LA, 2022, TELEMAT INFORM, V73, DOI 10.1016/j.tele.2022.101863
   Chang C-C, 2019, P 50 NORD ERG HUM FA, P69
   Debernardis S, 2014, IEEE T VIS COMPUT GR, V20, P125, DOI 10.1109/TVCG.2013.86
   Dewitz B, 2021, GI VR AR WORKSH, DOI [10.18420/vrar2021_5, DOI 10.18420/VRAR2021_5]
   Dingler T, 2018, CHI 2018: EXTENDED ABSTRACTS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3170427.3188695
   Erick AO, 2020, 2020 INTERNATIONAL SAUPEC/ROBMECH/PRASA CONFERENCE, P190, DOI 10.1109/saupec/robmech/prasa48453.2020.9041002
   Erickson A, 2021, ACM T APPL PERCEPT, V18, DOI 10.1145/3456874
   Erickson A, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P434, DOI [10.1109/VR46266.2020.1580695145399, 10.1109/VR46266.2020.00-40]
   Falk J, 2021, INT WORK QUAL MULTIM, P231, DOI 10.1109/QoMEX51781.2021.9465455
   Fiorentino M, 2013, PRESENCE-TELEOP VIRT, V22, P171, DOI 10.1162/PRES_a_00146
   Fukushima S, 2020, INT SYM MIX AUGMENT, P649, DOI 10.1109/ISMAR50242.2020.00093
   Gabbard JL, 2006, PRESENCE-TELEOP VIRT, V15, P16, DOI 10.1162/pres.2006.15.1.16
   Gabbard JL, 2007, IEEE VIRTUAL REALITY 2007, PROCEEDINGS, P35
   Gabbard JL, 2019, IEEE T VIS COMPUT GR, V25, P2228, DOI 10.1109/TVCG.2018.2832633
   Gabbard JL, 2010, P IEEE VIRT REAL ANN, P79, DOI 10.1109/VR.2010.5444808
   Garousi V, 2019, INFORM SOFTWARE TECH, V106, P101, DOI 10.1016/j.infsof.2018.09.006
   Gattullo M, 2022, IEEE T VIS COMPUT GR, V28, P1443, DOI 10.1109/TVCG.2020.3014614
   Gattullo M, 2015, IEEE T VIS COMPUT GR, V21, P638, DOI 10.1109/TVCG.2014.2385056
   Gattullo M, 2015, IEEE COMPUT GRAPH, V35, P52, DOI 10.1109/MCG.2015.36
   Google Fonts, 2023, Using type in AR & VR. Google fonts documentation
   Google for Developers, 2017, Designing screen interfaces for VR (Google I/O '17)
   Grout Cameron., 2015, P 15 NZ C HUMAN COMP, P9, DOI DOI 10.1145/2808047.2808055
   Hincapié-Ramos JD, 2015, IEEE T VIS COMPUT GR, V21, P1336, DOI 10.1109/TVCG.2015.2450745
   Hincapié-Ramos JD, 2014, INT SYM MIX AUGMENT, P187, DOI 10.1109/ISMAR.2014.6948426
   Hoffman DM, 2019, J SOC INF DISPLAY, V27, P207, DOI 10.1002/jsid.765
   Kaufeld M, 2022, DISPLAYS, V74, DOI 10.1016/j.displa.2022.102283
   Kim A-Yeong., 2014, EMNLP, P1396, DOI [10.3115/v1/d14-1146, DOI 10.3115/V1/D14-1146]
   Kim K, 2019, ACM CONFERENCE ON SPATIAL USER INTERACTION (SUI 2019), DOI 10.1145/3357251.3357584
   Kitchenham B.A., 2015, Evidence-Based Software Engineering and Systematic Reviews, DOI DOI 10.1201/B19467
   Klose EM, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P636, DOI [10.1109/VR.2019.8797992, 10.1109/vr.2019.8797992]
   Kobayashi S, 2022, 22 SIGGRAPH ASIA 202, P1, DOI [10.1145/3550082.3564182, DOI 10.1145/3550082.3564182]
   Kobayashi S, 2021, SIGGRAPH '21: ACM SIGGRAPH 2021 POSTERS, DOI 10.1145/3450618.3469164
   Koide H, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P777, DOI 10.1109/VRW55335.2022.00244
   Kojic T, 2020, P 12 INT C QUAL MULT, P1, DOI [10.1109/qomex48832.2020.9123091, DOI 10.1109/QOMEX48832.2020.9123091]
   Kojic T, 2022, LECT NOTES COMPUT SC, V13317, P199, DOI 10.1007/978-3-031-05939-1_13
   Korkut EH, 2023, VIRTUAL REAL-LONDON, V27, P1447, DOI 10.1007/s10055-023-00753-8
   Kruijff E, 2019, IEEE T VIS COMPUT GR, V25, P2821, DOI 10.1109/TVCG.2018.2854737
   Lawrence A., 2014, IS EVIDENCE REALISIN
   Lee H, 2023, VIRTUAL REAL-LONDON, V27, P829, DOI 10.1007/s10055-022-00693-9
   Leykin A, 2004, P 2004 ACM SIGGRAPH, P436, DOI [10.1145/1044588.1044683, DOI 10.1145/1044588.1044683]
   Liberatore MJ, 2021, VIRTUAL REAL-LONDON, V25, P773, DOI 10.1007/s10055-020-00492-0
   Manghisi VM, 2017, PRESENCE-TELEOP VIRT, V26, P1, DOI 10.1162/PRES_a_00285
   Microsoft, 2023, Comfort
   Microsoft, Microsoft Learn Documentation
   Microsoft, 2023, Typography. Microsoft Learn Documentation
   Munafo J, 2017, EXP BRAIN RES, V235, P889, DOI 10.1007/s00221-016-4846-7
   Niteesh Y, 2023, Website for their independent research
   Oculus Developers, 2023, VR Accessibility Design: User Experience (UX) and User Interface (UI)
   Orlosky J, 2013, INT SYM MIX AUGMENT, P281, DOI 10.1109/ISMAR.2013.6671805
   Renkewitz H, 2008, IEEE INT CONF INF VI, P503, DOI 10.1109/IV.2008.73
   Rosilius M, 2021, INT SYM MIX AUGMENT, P234, DOI 10.1109/ISMAR-Adjunct54149.2021.00055
   Rzayev R, 2021, P 2021 CHI C HUM FAC, P1, DOI [10.1145/3411764.3445606, DOI 10.1145/3411764.3445606]
   Rzayev R, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173619
   Scharff LFV, 1999, P SOC PHOTO-OPT INS, V3644, P270, DOI 10.1117/12.348448
   Schmalstieg D., 2016, Augmented reality: principles and practice
   Schopfel J, 2010, Encyclopedia of library and information sciences, V3rd
   Shimizu Y, 2021, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR-Adjunct54149.2021.00035
   Sridharan SrikanthKirshnamachari., 2013, Proceedings of the 19th ACM Symposium on Virtual Reality Software and Technology, P231, DOI [10.1145/2503713.2503716, DOI 10.1145/2503713.2503716]
   Tanaka K, 2008, INT SYM MIX AUGMENT, P139, DOI 10.1109/ISMAR.2008.4637340
   Tsunajima T, 2020, HCI international 2020-posters. HCII 2020. Communications in computer and information science, V1225, DOI [10.1007/978-3-030-50729-9_18, DOI 10.1007/978-3-030-50729-9_18]
   Wang Z, 2020, HCI international 2020-Late breaking posters. HCII 2020. Communications in computer and information science, V1293, DOI [10.1007/978-3-030-60700-5_59, DOI 10.1007/978-3-030-60700-5_59]
   Wei CX, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P721, DOI [10.1109/VR46266.2020.1581590322523, 10.1109/VR46266.2020.000-9]
   Woodward J, 2020, PROCEEDINGS OF THE WORKING CONFERENCE ON ADVANCED VISUAL INTERFACES AVI 2020, DOI 10.1145/3399715.3399846
   Yuan J., 2018, Int J Ophthalmol Clin Res, V5, P85, DOI 10.23937/2378-346X/1410085
   Zhao YH, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P4170, DOI 10.1145/3025453.3025949
NR 73
TC 0
Z9 0
U1 16
U2 16
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 59
DI 10.1007/s10055-024-00949-6
PG 19
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA JV9Q8
UT WOS:001176059900002
OA hybrid
DA 2024-08-05
ER

PT J
AU Kim, Y
   Yun, J
   Yun, J
   Kwak, S
   Ihm, I
AF Kim, Youngwook
   Yun, Jehyeong
   Yun, Joungil
   Kwak, Sangwoon
   Ihm, Insung
TI Ray tracing-based construction of 3D background model for real-time
   stereoscopic rendering of live immersive video
SO VIRTUAL REALITY
LA English
DT Article
DE Immersive video; Real-time stereoscopic rendering; Disocclusion; 3D
   background model; Ray tracing; Virtual reality
ID VIEW-SYNTHESIS; COMPRESSION
AB Immersive video stored in multiview video-plus-depth format can provide viewers with vivid immersive experiences. However, rendering such video in real time in immersive environments remains a challenging task due to the high resolution and refresh rate demanded by recent extended reality displays. An essential issue in this immersive rendering is the disocclusion problem that inevitably occurs when virtual views are synthesized via the de facto standard 3D warping technique. In this paper, we present a novel virtual view synthesis framework that, from a live immersive video stream, renders stereoscopic images in real time for a freely moving virtual viewer. The main difference from previous approaches is that the surrounding background environment of the immersive video's virtual scene is progressively reproduced on the fly directly in the 3D space while the input stream is being rendered. To allow this, we propose a new 3D background modeling scheme that, based on GPU-accelerated real-time ray tracing, efficiently and incrementally builds the background model in compact 3D triangular mesh. Then, we demonstrate that the 3D background environment can effectively alleviate the critical disocclusion problem in the immersive rendering, eventually reducing spatial and temporal aliasing artifacts. It is also suggested that the 3D representation of background environment enables extension of the virtual environment of immersive video by interactively adding 3D visual effects during rendering.
C1 [Kim, Youngwook; Ihm, Insung] Sogang Univ, Dept Comp Sci & Engn, Seoul, South Korea.
   [Yun, Jehyeong] MIDAS Informat Technol, CIM Dev Team 1, Seongnam, South Korea.
   [Yun, Joungil; Kwak, Sangwoon] Elect & Telecommun Res Inst, Media Res Div, Daejeon, South Korea.
C3 Sogang University; Electronics & Telecommunications Research Institute -
   Korea (ETRI)
RP Ihm, I (corresponding author), Sogang Univ, Dept Comp Sci & Engn, Seoul, South Korea.
EM kimyu7@sogang.ac.kr; yjh1121@midasit.com; sigipus@etri.re.kr;
   s.kwak@etri.re.kr; ihm@sogang.ac.kr
FU National Research Foundation of Korea
FX No Statement Available
CR Boyce JM, 2021, P IEEE, V109, P1521, DOI 10.1109/JPROC.2021.3062590
   Broxton M, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392485
   Chen KY, 2010, 3DTV CONF
   Criminisi A, 2007, INT J COMPUT VISION, V71, P89, DOI 10.1007/s11263-006-8525-1
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Dore R., 2018, Standard ISO/IEC JTC1/SC29/WG11 MPEG/M42349
   Doyen D, 2017, Standard ISO/IEC JTC1/SC29/WG11 MPEG/M40010
   Elharrouss O, 2020, NEURAL PROCESS LETT, V51, P2007, DOI 10.1007/s11063-019-10163-0
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Flynn J, 2019, PROC CVPR IEEE, P2362, DOI 10.1109/CVPR.2019.00247
   Huang Y, 2008, 2008 19 INT C PATT R, P1
   Jam J, 2021, COMPUT VIS IMAGE UND, V203, DOI 10.1016/j.cviu.2020.103147
   Jung J, 2022, Standard ISO/IED JTC1/SC29/WG04 MPEG/N0203
   Lee J, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P603, DOI 10.1109/VRW52623.2021.00184
   Luo GB, 2020, IEEE T PATTERN ANAL, V42, P1289, DOI 10.1109/TPAMI.2019.2899837
   Luo GB, 2016, PROC CVPR IEEE, P1781, DOI 10.1109/CVPR.2016.197
   Mark W. R., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P7, DOI 10.1145/253284.253292
   Müller K, 2011, P IEEE, V99, P643, DOI 10.1109/JPROC.2010.2091090
   Ndjiki-Nya P, 2011, IEEE T MULTIMEDIA, V13, P453, DOI 10.1109/TMM.2011.2128862
   NVIDIA, 2021, NVIDIA OptiX 7.4 Programming Guide
   Qin Z, 2021, DISPLAYS, V69, DOI 10.1016/j.displa.2021.102028
   Rahaman DMM, 2018, IEEE T IMAGE PROCESS, V27, P1190, DOI 10.1109/TIP.2017.2772858
   Salahieh B, 2018, Standard ISO/IEC JTC1/SC29/WG11 MPEG/M43748
   Schmeing M, 2010, 2010 3DTV C TRUE VIS, P1, DOI DOI 10.1109/3DTV.2010.5506596
   Smolic A, 2011, PATTERN RECOGN, V44, P1958, DOI 10.1016/j.patcog.2010.09.005
   Sun WX, 2012, IEEE IMAGE PROC, P2721, DOI 10.1109/ICIP.2012.6467461
   Vadakital VKM, 2022, IEEE MULTIMEDIA, V29, P101, DOI 10.1109/MMUL.2022.3175654
   Yao C, 2014, IEEE T BROADCAST, V60, P394, DOI 10.1109/TBC.2014.2321671
   Zhou TH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201323
   Zhu C., 2013, 3D-TV System with Depth-Image-Based Rendering - Architectures, Techniques and Challenges
NR 30
TC 0
Z9 0
U1 7
U2 7
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 17
DI 10.1007/s10055-023-00921-w
PG 17
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA EW2S3
UT WOS:001141910100003
OA hybrid
DA 2024-08-05
ER

PT J
AU Marto, A
   Gonçalves, A
AF Marto, Anabela
   Goncalves, Alexandrino
TI A scope of presence-related feelings in AR studies
SO VIRTUAL REALITY
LA English
DT Article
DE Mobile augmented reality; Presence in AR; Evaluating presence
ID AUGMENTED REALITY; VIRTUAL-REALITY; IMMERSION; QUESTIONNAIRE;
   PERCEPTION; EXPERIENCE; DESIGN; GAMES; SENSE
AB The idiosyncrasies of augmented reality bring us advantages, as shown in literature, and a great assortment of options. On the other hand, due to its relationship with the real surroundings, it may be a challenge to deal with when evaluating these systems, especially if the intention is to understand feelings of presence-like: There are a lot of variables in the equation. This study aims to analyse a state of the art of AR evaluations that conducted presence-related feelings and discusses limitations and remarks for further research. The current research is able to state that questionnaires are the most used tool to ascertain presence-like feelings, and that mobile devices have been the preferred device to implement AR applications. The studies are fairly divided between 1) analysing a single scenario to ascertain the variable at study, and 2) creating two or more scenarios to make a comparison. When comparing two or more scenarios, a between-subjects design is preferred among researchers. Additionally, it has been identified as of paramount importance the need to study and objectively measuring the ratio between the virtual content and the real scenario in the experience, as well as to deeply research the interaction between the real scenario and the virtual elements. The importance of the types of interaction in AR applications is also highlighted.
C1 [Marto, Anabela; Goncalves, Alexandrino] Polytech Inst Leiria, CIIC, ESTG, Campus 2, P-2411901 Leiria, Portugal.
RP Marto, A (corresponding author), Polytech Inst Leiria, CIIC, ESTG, Campus 2, P-2411901 Leiria, Portugal.
EM anabela.marto@ipleiria.pt; alex@ipleiria.pt
OI Jose Marques Goncalves, Alexandrino/0000-0002-5966-3218
FU Portuguese Foundation for Science and Technology (FCT) [UIDB/04524/2020]
FX This work was supported by national funds through the Portuguese
   Foundation for Science and Technology (FCT) under the project
   UIDB/04524/2020.
CR Alshaer A, 2017, APPL ERGON, V58, P1, DOI 10.1016/j.apergo.2016.05.003
   Andrzejczak J., 2021, Factors affecting the sense of scale in immersive, realistic virtual reality space, P3
   Azuma R, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.963459
   Azuma RT, 1997, PRESENCE-VIRTUAL AUG, V6, P355, DOI 10.1162/pres.1997.6.4.355
   Bangay S, 1998, ST HEAL T, V58, P43
   Bimber O, 2005, INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P14
   Bouchard S, 2008, PRESENCE-VIRTUAL AUG, V17, P376, DOI 10.1162/pres.17.4.376
   Brizzi F, 2018, IEEE T HUM-MACH SYST, V48, P197, DOI 10.1109/THMS.2017.2782490
   Brown E., 2004, 2004 C HUM FACT COMP, P1297, DOI DOI 10.1145/985921.986048
   Juan MC, 2011, INT J HUM-COMPUT INT, V27, P436, DOI 10.1080/10447318.2011.552059
   Champney R, 2015, LECT NOTES COMPUT SC, V9179, P251, DOI 10.1007/978-3-319-21067-4_26
   Conway B, 2010, SOCIOL COMPASS, V4, P442, DOI 10.1111/j.1751-9020.2010.00300.x
   de Gortari ABO, 2018, TELEMAT INFORM, V35, P382, DOI 10.1016/j.tele.2017.12.015
   Dickson WP, 1992, Exploratory multi-media environments, P155
   DUNCAN CP, 1985, J ADVERTISING, V14, P33, DOI 10.1080/00913367.1985.10672944
   Dunser A., 2008, SURVEY EVALUATION TE
   Fazio S, 2020, ACM J COMPUT CULT HE, V13, DOI 10.1145/3418037
   Fox J, 2009, PRESENCE-TELEOP VIRT, V18, P294, DOI 10.1162/pres.18.4.294
   Gandy M., 2010, 2010 9th IEEE International Symposium on Mixed and Augmented Reality (ISMAR). Science & Technology Papers, P127, DOI 10.1109/ISMAR.2010.5643560
   Georgiou Y, 2017, P 16 WORLD C MOBILE, P1
   Georgiou Y, 2021, INT J HUM-COMPUT ST, V145, DOI 10.1016/j.ijhcs.2020.102546
   Georgiou Y, 2018, COMPUT HUM BEHAV, V89, P173, DOI 10.1016/j.chb.2018.08.011
   Georgiou Y, 2017, INT J HUM-COMPUT ST, V98, P24, DOI 10.1016/j.ijhcs.2016.09.014
   Giglioli IC, 2016, P INT S PERVASIVE CO, V2015
   Graven OH, 2014, Extending the design of games for e-learning to include affective computing elements, P59
   Gustafson P, 2001, J ENVIRON PSYCHOL, V21, P5, DOI 10.1006/jevp.2000.0185
   Han J, 2015, ETR&D-EDUC TECH RES, V63, P455, DOI 10.1007/s11423-015-9374-9
   Huang KT, 2019, CYBERPSYCH BEH SOC N, V22, P105, DOI 10.1089/cyber.2018.0150
   IJsselsteijn WA, 2000, PROC SPIE, V3959, P520, DOI 10.1117/12.387188
   Jennett C, 2008, INT J HUM-COMPUT ST, V66, P641, DOI 10.1016/j.ijhcs.2008.04.004
   Juan MC, 2010, COMPUT GRAPH-UK, V34, P756, DOI 10.1016/j.cag.2010.08.001
   Jung J, 2018, INT SYM MIX AUGMENT, P70, DOI 10.1109/ISMAR.2018.00032
   Kenderdine Sarah, 2016, A New Companion to Digital Humanities, P22, DOI DOI 10.1002/9781118680605.CH2
   Kim MJ, 2013, AUTOMAT CONSTR, V33, P79, DOI 10.1016/j.autcon.2012.10.020
   Konecny M., 2011, Ann. GIS, V17, P135, DOI DOI 10.1080/19475683.2011.602027
   Kraus M, 2015, P 2015 VIRTUAL REALI, V25
   Krzywinska T, 2020, ACM J COMPUT CULT HE, V13, DOI 10.1145/3414832
   Lee J, 2011, PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON COMPUTER-AIDED ARCHITECTURAL DESIGN RESEARCH IN ASIA (CAADRIA 2011), P199
   Lessiter J, 2001, PRESENCE-TELEOP VIRT, V10, P282, DOI 10.1162/105474601300343612
   Lili Liu, 2017, Augmented Cognition: Enhancing Cognition and Behavior in Complex Human Environments. 11th International Conference, AC 2017, held as part of HCI International 2017. Proceedings: LNAI 10285, P514, DOI 10.1007/978-3-319-58625-0_37
   Lombard M., 1997, Journal of Computer-Mediated Communication, V3, DOI [10.1111/j.1083-6101.1997.tb0, 10.1111/j.1083-6101.1997.tb00072.x., 10.1111/j.1083-6101.1997.tb00072.x, DOI 10.1111/J.1083-6101.1997.TB00072.X, 10.1111/J.1083-6101.1997.TB00072.X/4080403]
   Lombard M, 2009, Measuring presence: the temple presence inventory, P1
   Malinska M, 2015, INT J OCCUP SAF ERGO, V21, P47, DOI 10.1080/10803548.2015.1017964
   Marto A, 2022, J IMAGING, V8, DOI 10.3390/jimaging8040091
   Marto A, 2020, IEEE ACCESS, V8, P193744, DOI 10.1109/ACCESS.2020.3032379
   McCall R, 2011, PERS UBIQUIT COMPUT, V15, P25, DOI 10.1007/s00779-010-0306-8
   Meehan M, 2005, APPL PSYCHOPHYS BIOF, V30, P239, DOI 10.1007/s10484-005-6381-3
   Mestre D., 2006, Immersion et presence. Le traite de la realite virtuelle, P309
   MILGRAM P, 1994, P SOC PHOTO-OPT INS, V2351, P282
   Minsky M., 1980, Omni, P45, DOI DOI 10.1145/566654.566630
   Moher D, 2015, SYST REV-LONDON, V4, DOI [10.1186/2046-4053-4-1, 10.1371/journal.pmed.1000097, 10.1136/bmj.i4086, 10.1016/j.ijsu.2010.02.007, 10.1136/bmj.b2535, 10.1016/j.ijsu.2010.07.299, 10.1136/bmj.b2700]
   Nilsson N.C., 2016, HUMAN TECHNOLOGY, V12, P108, DOI [DOI 10.17011/HT/URN.201611174652, 10.17011/ht/urn.201611174652]
   Pallot M, 2013, P VIRTUAL REALITY IN, P4
   Park H, 2013, COMPUT IND, V64, P854, DOI 10.1016/j.compind.2013.05.006
   Paterson Natasa., 2010, FUN AND GAMES, P149
   Pereira A, 2017, IEEE ROMAN, P764, DOI 10.1109/ROMAN.2017.8172389
   Pyae A, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON CREATIVITY AND COGNITION (C&C 2017), P247, DOI 10.1145/3059454.3078859
   Ravaja N., 2004, Proceedings of the Third Nordic Conference on Human-Computer Interaction: Nordic Conference on Human-Computer Interaction; Tampere, Finland, V82, P339, DOI 10.1145/1028014.1028068
   Reid Josephine., 2005, PROC CHI EA 05, P1733, DOI DOI 10.1145/1056808.1057009
   Rosen S., 1994, P 21 ANN C COMPUTER, P496
   Rzeszewski M, 2022, INT J DIGIT EARTH, V15, P853, DOI 10.1080/17538947.2022.2061619
   Sadowski W, 2002, Presence in virtual environments, P831
   Salar R, 2020, J SCI EDUC TECHNOL, V29, P257, DOI 10.1007/s10956-019-09810-x
   Seamon David., 2008, KEY TEXTS HUMAN GEOG, DOI [10.4135/9781446213742, DOI 10.4135/9781446213742.N6]
   Sekhavat YA, 2018, INT J HUM-COMPUT INT, V34, P187, DOI 10.1080/10447318.2017.1340229
   Sekhavat YA, 2016, INT J COMPUT GAMES T, V2016, DOI 10.1155/2016/7690754
   Sheridan TB, 1996, PRESENCE-TELEOP VIRT, V5, P241, DOI 10.1162/pres.1996.5.2.241
   Sheridan TB., 1992, Presence: Teleoperators and Virtual Environments, V1, P120, DOI [10.1162/pres.1992.1.1.120, DOI 10.1162/PRES.1992.1.1.120]
   SLATER M, 1993, IEEE VIRTUAL REALITY ANNUAL INTERNATIONAL SYMPOSIUM, P90, DOI 10.1109/VRAIS.1993.380793
   Slater M, 1999, PRESENCE-TELEOP VIRT, V8, P560, DOI 10.1162/105474699566477
   Slater M, 1995, P ACM S VIRTUAL REAL, P163
   Slater M., 1994, Presence: teleoperators virtual environments, V3, P130, DOI [10.1162/pres.1994.3.2.130, DOI 10.1162/PRES.1994.3.2.130, 10.1162/pres.1994.3.2.1302]
   Stecker GC, 2018, P AUDIO ENG SOC C AE, P44
   Steptoe W, 2014, INT SYM MIX AUGMENT, P213, DOI 10.1109/ISMAR.2014.6948430
   Sun HL, 2018, PROCEEDINGS OF CHINESE CHI 2018: SIXTH INTERNATIONAL SYMPOSIUM OF CHINESE CHI (CHINESE CHI 2018), P64, DOI 10.1145/3202667.3202676
   Sutherland I.E., 1965, Proceedings of the IFIP Congress, V2, P506, DOI DOI 10.1109/MC.2005.274
   Sutherland I. E., 1968, Proceedings of the December 9-11, 1968, Fall Joint Computer Conference, Part I, AFIPS '68, P757, DOI DOI 10.1145/1476589.1476686
   Tang A, 2004, Comparing differences in presence during social interaction in augmented reality versus virtual reality environments: an exploratory study, P204
   Vazquez-Alvarez Y, 2012, PERS UBIQUIT COMPUT, V16, P987, DOI 10.1007/s00779-011-0459-0
   Villani D, 2012, INTERACT COMPUT, V24, P265, DOI 10.1016/j.intcom.2012.04.008
   von der Pütten AM, 2012, INTERACT COMPUT, V24, P317, DOI 10.1016/j.intcom.2012.03.004
   Vorderer P., 2004, Report to the European Community, Project Presence: MEC (IST- 2001-37661)
   Weech S, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.00158
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   Yim MYC, 2017, J INTERACT MARK, V39, P89, DOI 10.1016/j.intmar.2017.04.001
   Zhang B, 2021, JMIR SERIOUS GAMES, V9, DOI 10.2196/27036
NR 86
TC 0
Z9 0
U1 6
U2 6
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 18
DI 10.1007/s10055-023-00908-7
PG 14
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA EW2S3
UT WOS:001141910100002
OA hybrid
DA 2024-08-05
ER

PT J
AU Ogrizek, M
   Mortimer, M
   Antlej, K
   Callari, TC
   Stefan, H
   Horan, B
AF Ogrizek, Manca
   Mortimer, Michael
   Antlej, Kaja
   Callari, Tiziana C.
   Stefan, Hans
   Horan, Ben
TI Evaluating the impact of passive physical everyday tools on interacting
   with virtual reality museum objects
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality (VR); Interaction design; User experience; Gesture-based
   interaction techniques; Museum objects; Natural user interfaces
ID HAPTIC FEEDBACK; USER INTERFACES
AB Museums are increasingly embracing new methods and technologies to enhance the visitor experience. Virtual Reality (VR) provides the opportunity to experience objects and situations that are not readily available or don't otherwise exist making it well suited to museum applications. Museum visitors represent an ultra-diverse cohort with technology experience levels ranging from first-time users through to experts, and typically needing to interact with the exhibit with little to no induction and training, and in many instances as a once off encounter. To support such users, this paper evaluates the impact of passive physical everyday tools to provide passive haptic feedback and enhance user interaction with desk-top sized museum objects. Museums face challenges in exhibiting larger objects and in this work the cargo area of a utility vehicle (i.e. ute) was selected as contextually suitable larger object. Three different interaction techniques are used with and without everyday physical tools and experiments undertaken to investigate the impact of the physical tools on the usability and user experience with free-hand interaction techniques. A comparison between using the passive physical tool for the interaction technique and without showed improved efficiency for two of the techniques and positive impact on the user experience with the mechanically more complex of the interaction techniques. These insights may prove useful in the design of interaction techniques for enhanced free-hand interaction with museum objects in VR.
C1 [Ogrizek, Manca; Mortimer, Michael; Antlej, Kaja; Stefan, Hans; Horan, Ben] Deakin Univ, Sch Engn, Geelong, Vic, Australia.
   [Callari, Tiziana C.] Univ Leeds, Leeds Univ Business Sch, Leeds, England.
C3 Deakin University; University of Leeds
RP Ogrizek, M (corresponding author), Deakin Univ, Sch Engn, Geelong, Vic, Australia.
EM m.ogrizek@deakin.edu.au
RI Ogrizek, Manca/KPA-6509-2024
OI Stefan, Hans/0000-0002-7073-8479; Ogrizek, Manca/0000-0002-2029-7652
FU Deakin University
FX The authors would like to thank the participants involved in this
   research and the National Wool Museum (NWM) Geelong for supporting the
   project. I would like to express great appreciation to Dr Kaja Antlej,
   Prof Ben Horan and A/Prof Steven Cooke, my research supervisors, for
   their valuable and constructive suggestions during the planning and
   development of this research work. I would also want to thank the
   technician Chax Rivera at the department's VR laboratory for providing
   the 3D models.
CR Abdlkarim Diar, 2022, bioRxiv, DOI DOI 10.1101/2022.02.18.481001
   Adams M, 2002, P INT C INTERACTIVE, P1
   Antlej K, 2022, Digital modernism heritage lexicon
   Antlej K., 2018, 2018 3rd Digital Heritage International Congress (DigitalHERITAGE) Held Jointly with 2018 24th International Conference on Virtual Systems Multimedia (VSMM 2018), P1, DOI [10.1109/DigitalHeritage.2018.8810060, DOI 10.1109/DIGITALHERITAGE.2018.8810060]
   Antlej K, 2021, MUSEUMS SOCIAL RESPO
   Barmpoutis Angelos, 2020, Virtual, Augmented and Mixed Reality. Design and Interaction. 12th International Conference, VAMR 2020 Held as Part of the 22nd HCI International Conference, HCII 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12190), P275, DOI 10.1007/978-3-030-49695-1_18
   Beattie N, 2015, PROC TECH, V20, P149, DOI 10.1016/j.protcy.2015.07.025
   Bowman DA, 2008, IEEE COMPUT GRAPH, V28, P20, DOI 10.1109/MCG.2008.109
   Brondi R, 2016, INFORM-J COMPUT INFO, V40, P311
   Callari TC, 2022, ERGONOMICS, V65, P445, DOI 10.1080/00140139.2021.1977844
   Carrozzino M, 2016, LECT NOTES COMPUT SC, V9769, P378, DOI 10.1007/978-3-319-40651-0_30
   Cheng LP, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173663
   Clark J, 2010, MUS MANAGE CURATOR, V25, P219, DOI 10.1080/09647771003737331
   de Visser H, 2011, MED J AUSTRALIA, V194, pS38
   Di Franco PD, 2015, PRESENCE-TELEOP VIRT, V24, P243, DOI 10.1162/PRES_a_00229
   Fucentese SF, 2015, KNEE SURG SPORT TR A, V23, P1077, DOI 10.1007/s00167-014-2888-6
   Gmom, 2022, Geelong Museum of Motoring + Industry
   Han DT, 2018, IEEE T VIS COMPUT GR, V24, P1467, DOI 10.1109/TVCG.2018.2794659
   Hillmann Cornel., 2019, Unreal for Mobile and Standalone VR Create Professional VR Apps Without Coding, VFirst
   Hoffman H. G., 1998, Virtual Reality, V3, P226, DOI 10.1007/BF01408703
   Hornecker E., 2019, Synth. Lect. Hum.-Center. Inf., V12, P153, DOI DOI 10.2200/S00901ED1V01Y201902HCI042
   Hsu H.M. J., 2011, INT J INFORM ED TECH, V1, P365, DOI [10.7763/IJIET.2011.V1.59, DOI 10.7763/IJIET.2011.V1.59]
   Insko B. E., 2001, PhD thesis
   Koumaditis K, 2021, BEHAV INFORM TECHNOL, V40, P1234, DOI 10.1080/0144929X.2021.1988320
   Larson L, 2010, CES 2010: Nui with Bill Buxton
   LaViola J.J., 2017, 3D user interfaces: theory and practice
   Manresa C., 2005, ELECT LETT COMPUTER, V3, P96, DOI [DOI 10.5565/REV/ELCVIA.109, 10.5565/rev/elcvia.109]
   Mantovani F, 2019, Passive haptic feedback for object manipulation in virtual reality
   Margetis G, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11010338
   Masurovsky A, 2020, MULTIMODAL TECHNOLOG, V4, DOI 10.3390/mti4040091
   Naceri A, 2021, J INTELL ROBOT SYST, V101, DOI 10.1007/s10846-021-01311-7
   Nasim K, 2018, COMPUT ANIMAT VIRT W, V29, DOI 10.1002/cav.1820
   Norman D.A., 2010, interactions, V17, P6, DOI DOI 10.1145/1744161.1744163
   Oda O, 2015, UIST'15: PROCEEDINGS OF THE 28TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P405, DOI 10.1145/2807442.2807497
   Palma G, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13112186
   Pietroni E., 2012, 2012 18th International Conference on Virtual Systems and Multimedia (VSMM 2012). Proceedings, P339, DOI 10.1109/VSMM.2012.6365943
   Pietroni E, 2021, HERITAGE-BASEL, V4, P567, DOI 10.3390/heritage4020034
   Poupyrev I., 1998, Computer Graphics Forum, V17, pC41, DOI 10.1111/1467-8659.00252
   Schkolne S., 2002, Tangible+ virtual a flexible 3D interface for spatial construction applied to DNA
   Schulz P, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300413
   Shehade M, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10114031
   Sigrist R, 2015, EXP BRAIN RES, V233, P909, DOI 10.1007/s00221-014-4167-7
   Tscharn R., 2016, Mensch und computer 2016-Tagungsband
   Veverka J, 2018, Interpretive master planning volume one: strategies for the new millennium
   Voigt-Antons JN, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123085
   Vosinakis S., 2016, 2016 8 INT C GAMES V, P1, DOI [https://doi.org/10.1109/VS-GAMES.2016.7590334, DOI 10.1109/VS-GAMES.2016.7590334]
   Wigdor D, 2011, BRAVE NUI WORLD: DESIGNING NATURAL USER INTERFACES FOR TOUCH AND GESTURE, P1
   Wood E., 2016, The objects of experience: Transforming visitor-object encounters in museums, DOI [10.4324/9781315417776, DOI 10.4324/9781315417776]
NR 48
TC 0
Z9 0
U1 13
U2 13
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 26
DI 10.1007/s10055-023-00915-8
PG 19
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA FK8S0
UT WOS:001145776700002
OA hybrid
DA 2024-08-05
ER

PT J
AU van Limpt-Broers, HAT
   Postma, M
   van Weelden, E
   Pratesi, S
   Louwerse, MM
AF van Limpt-Broers, H. A. T.
   Postma, M.
   van Weelden, E.
   Pratesi, S.
   Louwerse, M. M.
TI Neurophysiological evidence for the overview effect: a virtual reality
   journey into space
SO VIRTUAL REALITY
LA English
DT Article
DE Overview effect; Awe; Virtual reality; EEG
ID AWE; EEG; SELF; BRAIN; DYNAMICS; POWER; ENVIRONMENTS; PERSONALITY;
   DECREASES; EMOTION
AB The Overview Effect is a complex experience reported by astronauts after viewing Earth from space. Numerous accounts suggest that it leads to increased interconnectedness to other human beings and environmental awareness, comparable to self-transcendence. It can cause fundamental changes in mental models of the world, improved well-being, and stronger appreciation of, and responsibility for Earth. From a cognitive perspective, it is closely linked to the emotion of awe, possibly triggered by the overwhelming perceived vastness of the universe. Given that most research in the domain focuses on self-reports, little is known about potential neurophysiological markers of the Overview Effect. In the experiment reported here, participants viewed an immersive Virtual Reality simulation of a space journey while their brain activity was recorded using electroencephalography (EEG). Post-experimental self-reports confirmed they were able to experience the Overview Effect in the simulated environment. EEG recordings revealed lower spectral power in beta and gamma frequency bands during the defining moments of the Overview Effect. The decrease in spectral power can be associated with reduced mental processing, and a disruption of known mental structures in this context, thereby providing more evidence for the cognitive effects of the experience.
C1 [van Limpt-Broers, H. A. T.; Postma, M.; van Weelden, E.; Pratesi, S.; Louwerse, M. M.] Tilburg Univ, Dept Cognit Sci & Artificial Intelligence, POB 90153, NL-5000 LE Tilburg, Netherlands.
C3 Tilburg University
RP van Limpt-Broers, HAT (corresponding author), Tilburg Univ, Dept Cognit Sci & Artificial Intelligence, POB 90153, NL-5000 LE Tilburg, Netherlands.
EM h.a.t.broers@tilburguniversity.edu
OI Louwerse, Max/0000-0003-0328-7070; van Weelden, Evy/0000-0003-3011-436X
FU VSNU (Association of Universities in the Netherlands) Digital Society
   program; DAF Technology Lab at Tilburg University
FX Partial funding for this study was made possible by VSNU (Association of
   Universities in the Netherlands) Digital Society program. The authors
   would like to thank the SpaceBuzz Foundation and their partners for the
   use of the rocket segment and VR simulations and the DAF Technology Lab
   at Tilburg University for accommodating this study.DAS:The datasets
   generated and analyzed during the current study publicly available
   through the DataverseNL repository. EEG-data, as it is sensitive data,
   is available upon request. https://doi.org/10.34894/ZHODRY.
CR Amin A, 2016, INT C VIRT AUGM MIX, DOI [10.1007/978-3-319-39907-225, DOI 10.1007/978-3-319-39907-225]
   Anderson CL, 2020, J PERS, V88, P762, DOI 10.1111/jopy.12524
   [Anonymous], 2021, MATLAB version: 9.11.0 (R2021b)
   Apicella A., 2023, Acta Imeko, V12, P1, DOI [10.21014/actaimeko.v12i2.1457, DOI 10.21014/ACTAIMEKO.V12I2.1457]
   Baceviciute S, 2021, COMPUT EDUC, V164, DOI 10.1016/j.compedu.2020.104122
   Bai Y, 2017, J PERS SOC PSYCHOL, V113, P185, DOI 10.1037/pspa0000087
   Baños RM, 2004, CYBERPSYCHOL BEHAV, V7, P734, DOI 10.1089/cpb.2004.7.734
   Barrett LF, 1997, PERS SOC PSYCHOL B, V23, P1100, DOI 10.1177/01461672972310010
   Bethelmy LC, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.00509
   Candia-Rivera D, 2022, SOFTWAREX, V19, DOI 10.1016/j.softx.2022.101170
   Chirico A., 2019, Pervasive Comput Paradig Mental Health, V288, P1, DOI [10.1007/978-3-030-25872-61, DOI 10.1007/978-3-030-25872-61]
   Chirico A, 2020, ANN REV CYBERTHERAPY, V18, P243
   Chirico A, 2019, CYBERPSYCH BEH SOC N, V22, P220, DOI 10.1089/cyber.2018.0393
   Clemente M, 2014, EXPERT SYST APPL, V41, P1584, DOI 10.1016/j.eswa.2013.08.055
   Colantonio JA, 2018, Awesome play: awe increases preschooler's exploration and discovery, DOI [10.31219/osf.io/pjhrq, DOI 10.31219/OSF.IO/PJHRQ]
   Craig SD., 2004, J ED MEDIA, V29, P241, DOI [10.1080/135816, DOI 10.1080/1358165042000283101]
   D'Mello S, 2014, ACTA PSYCHOL, V151, P106, DOI 10.1016/j.actpsy.2014.06.005
   Delorme A, 2004, J NEUROSCI METH, V134, P9, DOI 10.1016/j.jneumeth.2003.10.009
   DUFFY FH, 1993, NEUROBIOL AGING, V14, P73, DOI 10.1016/0197-4580(93)90025-7
   Faul F, 2007, BEHAV RES METHODS, V39, P175, DOI 10.3758/BF03193146
   Feng Tian, 2021, 2021 6th International Conference on Intelligent Computing and Signal Processing (ICSP), P904, DOI 10.1109/ICSP51882.2021.9408764
   Gallagher S, 2014, THEOR ISS ERGON SCI, V15, P376, DOI 10.1080/1463922X.2013.869370
   Gevins A, 1999, PHILOS T R SOC B, V354, P1125, DOI 10.1098/rstb.1999.0468
   Gordon AM, 2017, J PERS SOC PSYCHOL, V113, P310, DOI 10.1037/pspp0000120
   Gottlieb S, 2018, COGNITIVE SCI, V42, P2081, DOI 10.1111/cogs.12648
   Graziosi M, 2021, J POSIT PSYCHOL, V16, P263, DOI 10.1080/17439760.2019.1689422
   Greenberg JA, 2015, NEUROIMAGE, V114, P257, DOI 10.1016/j.neuroimage.2015.03.077
   Guan F, 2019, CURR PSYCHOL, V38, P1033, DOI 10.1007/s12144-019-00244-7
   Guan F, 2019, FRONT HUM NEUROSCI, V13, DOI 10.3389/fnhum.2019.00206
   Guan F, 2018, FRONT BEHAV NEUROSCI, V12, DOI 10.3389/fnbeh.2018.00209
   Habibzadeh F., 2017, J. Publ. Health Epidemiol., V1, P90, DOI DOI 10.21037/JPHE.2017.12.02
   Hake R., 2014, High-School Phys Phys Educ Res Conf, V8, P1
   Herrmann CS, 2004, TRENDS COGN SCI, V8, P347, DOI 10.1016/j.tics.2004.06.006
   Hu X, 2017, FRONT HUM NEUROSCI, V11, DOI 10.3389/fnhum.2017.00026
   Ihle EC, 2006, AVIAT SPACE ENVIR MD, V77, P93
   Jones MG, 2022, INT J SCI EDUC, V44, P2485, DOI 10.1080/09500693.2022.2133557
   Kalantari S, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-89297-y
   Kanas N, 2020, ACTA ASTRONAUT, V166, P525, DOI 10.1016/j.actaastro.2018.08.004
   Keltner D, 2003, COGNITION EMOTION, V17, P297, DOI 10.1080/02699930302297
   Kim HK, 2018, APPL ERGON, V69, P66, DOI 10.1016/j.apergo.2017.12.016
   Kober SE, 2012, INT J PSYCHOPHYSIOL, V83, P365, DOI 10.1016/j.ijpsycho.2011.12.003
   Koelstra S, 2012, IEEE T AFFECT COMPUT, V3, P18, DOI 10.1109/T-AFFC.2011.15
   Krenzer WLD, 2018, Assessing the experience of awe: validating the situational awe scale
   Lai CQ, 2018, 2018 IEEE SYMPOSIUM ON COMPUTER APPLICATIONS & INDUSTRIAL ELECTRONICS (ISCAIE 2018), P326, DOI 10.1109/ISCAIE.2018.8405493
   Li Y, 2015, BIO-MED MATER ENG, V26, pS1115, DOI 10.3233/BME-151408
   Lim HK, 2021, NEUROSCI LETT, V743, DOI 10.1016/j.neulet.2020.135589
   Luijcks R, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0129220
   Higuera-Trujillo JL, 2017, APPL ERGON, V65, P398, DOI 10.1016/j.apergo.2017.05.006
   Martin JM, 2017, LANG SPEECH, V60, P597, DOI 10.1177/0023830916686128
   McPhetres J, 2019, COGNITION EMOTION, V33, P1599, DOI 10.1080/02699931.2019.1585331
   Meltzer DE, 2002, AM J PHYS, V70, P1259, DOI 10.1119/1.1514215
   Michel CM, 2019, FRONT NEUROL, V10, DOI 10.3389/fneur.2019.00325
   Nakayama M, 2020, J CROSS CULT PSYCHOL, V51, P771, DOI 10.1177/0022022120959821
   Naqvi SAA, 2015, AUSTRALAS PHYS ENG S, V38, P721, DOI 10.1007/s13246-015-0379-9
   Neidlinger K, 2017, PROCEEDINGS OF THE ELEVENTH INTERNATIONAL CONFERENCE ON TANGIBLE, EMBEDDED, AND EMBODIED INTERACTION (TEI'17), P315, DOI 10.1145/3024969.3025004
   Nelson-Coffey SK, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0216780
   Nigbur R, 2011, CLIN NEUROPHYSIOL, V122, P2185, DOI 10.1016/j.clinph.2011.03.030
   Nour MM, 2016, FRONT HUM NEUROSCI, V10, DOI 10.3389/fnhum.2016.00269
   Penolazzi B, 2009, NEUROSCI LETT, V465, P74, DOI 10.1016/j.neulet.2009.08.065
   Piff PK, 2015, J PERS SOC PSYCHOL, V108, P883, DOI 10.1037/pspi0000018
   Ramírez-Moreno MA, 2021, BRAIN SCI, V11, DOI 10.3390/brainsci11060698
   Rasinski KA, 2005, J EXP SOC PSYCHOL, V41, P321, DOI 10.1016/j.jesp.2004.07.001
   Reinerman-Jones L., 2011, FAC 2011 Lecture Notes in Computer Science, V6780, DOI [10.1007/978-3-642-21852-114, DOI 10.1007/978-3-642-21852-114]
   Reinerman-Jones L, 2013, S AFR J PHILOS, V32, P295, DOI 10.1080/02580136.2013.867397
   Sawada K, 2020, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.00148
   Schmidt LA, 2001, COGNITION EMOTION, V15, P487, DOI 10.1080/0269993004200187
   Shiota M.N., 2006, Journal of Positive Psychology, V1, P61, DOI [10.1080/17439760500510833, DOI 10.1080/17439760500510833]
   Shiota MN, 2007, COGNITION EMOTION, V21, P944, DOI 10.1080/02699930600923668
   Silvia PJ, 2015, PSYCHOL AESTHET CREA, V9, P376, DOI 10.1037/aca0000028
   Stacey JE, 2021, NEUROPSYCHOLOGIA, V157, DOI 10.1016/j.neuropsychologia.2021.107887
   Stellar JE, 2017, EMOT REV, V9, P200, DOI 10.1177/1754073916684557
   Stepanova ER., 2019, Front Digit Humanit, V6, P1, DOI DOI 10.3389/FDIGH.2019.00007
   Stepanova ER., 2019, Frontiers in Digital Humanities, V6, P9, DOI DOI 10.3389/FDIGH.2019.00009
   Stinson B, 2013, COMPLEMENT THER CLIN, V19, P114, DOI 10.1016/j.ctcp.2013.03.003
   Sturm VE, 2022, EMOTION, V22, P1044, DOI 10.1037/emo0000876
   Taber KS, 2018, RES SCI EDUC, V48, P1273, DOI 10.1007/s11165-016-9602-2
   Takano R, 2022, EMOTION, V22, P669, DOI 10.1037/emo0000771
   Tavakol M, 2011, INT J MED EDUC, V2, P53, DOI 10.5116/ijme.4dfb.8dfd
   Urban Alex, 2022, Proceedings of the Association for Information Science and Technology, P818, DOI 10.1002/pra2.737
   Valenzi S., 2014, J Biomed Sci Eng, P604, DOI [DOI 10.4236/JBISE.2014, 10.4236/jbise.2014.78061, DOI 10.4236/JBISE.2014.78061]
   van Elk M., 2016, COLLABRA, V2, P4, DOI DOI 10.1525/COLLABRA.36
   van Elk M, 2019, HUM BRAIN MAPP, V40, P3561, DOI 10.1002/hbm.24616
   van Limpt HAT, 2020, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.540996
   van Limpt-Broers HAT, 2024, MEM COGNITION, DOI 10.3758/s13421-024-01575-y
   Voski A, 2020, J ENVIRON PSYCHOL, V70, DOI 10.1016/j.jenvp.2020.101454
   White F., 2014, The overview effect: Space exploration and human evolution, DOI [10.2514/5.9871624103223.0000.0000, DOI 10.2514/5.9871624103223.0000.0000]
   Willems RM, 2008, BRAIN RES, V1219, P78, DOI 10.1016/j.brainres.2008.04.065
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   Yaden D.B., 2016, PSYCHOL CONSCIOUS, V3, P1, DOI [10.1037/cns0000086, DOI 10.1037/CNS0000086]
   Yaden DB, 2019, J POSIT PSYCHOL, V14, P474, DOI 10.1080/17439760.2018.1484940
   Yaden DB., 2022, The varieties of spiritual experience, 21st century research and perspectives, DOI [10.1093/oso/9780190665678.001.0001, DOI 10.1093/OSO/9780190665678.001.0001]
   Yisi Liu, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P1039, DOI 10.1109/ICME.2012.20
   Yürdem B, 2019, 2019 11TH INTERNATIONAL CONFERENCE ON ELECTRICAL AND ELECTRONICS ENGINEERING (ELECO 2019), P432, DOI [10.23919/eleco47770.2019.8990539, 10.23919/ELECO47770.2019.8990539]
NR 93
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD JUL 23
PY 2024
VL 28
IS 3
AR 140
DI 10.1007/s10055-024-01035-7
PG 19
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA ZJ4K3
UT WOS:001274912500001
DA 2024-08-05
ER

PT J
AU Guzmán, DE
   Rengifo, CF
   Guzmán, JD
   Cena, CEG
AF Guzman, D. E.
   Rengifo, C. F.
   Guzman, J. D.
   Cena, C. E. Garcia
TI Virtual reality games for cognitive rehabilitation of older adults: a
   review of adaptive games, domains and techniques
SO VIRTUAL REALITY
LA English
DT Article
DE Serious games; Virtual reality; Cognitive domains; Cognitive
   rehabilitation; Dynamic difficulty adjustment
ID SERIOUS GAMES; PHYSICAL-EXERCISE; INTERVENTIONS; PERFORMANCE; STROKE;
   MILD; LOAD; FLOW
AB In recent decades, the senior adults population worldwide has increased, as well as the medical conditions related to aging, such as cognitive decline. Virtual reality (VR) games are a valuable addition to conventional cognitive rehabilitation as they increase engagement to the therapy through customization, socialization, immersion, and feedback. This review, performed according to PRISMA protocol, addresses the following questions: How VR games have been used for cognitive rehabilitation?, What cognitive domains have been addressed by VR games and in which populations have these games been used?, Which features have been considered to improve engagement in VR games for cognitive rehabilitation?, How is the difficulty adjustment of exercises carried out in VR games for cognitive rehabilitation?. We found 25 scientific works related to these questions, 92% of them treating one cognitive domain at a time, despite the fact that the related literature recognizes the value of training multiple domains simultaneously. Our review indicates that, despite the existence of serious VR games for working memory training, such as those described in Flak et al. (Front Psychol 10:807, 2019. https://doi.org/10.3389/fpsyg.2019.00807), to our knowledge, there are no applications that simultaneously address multiple cognitive domains and incorporate dynamic difficulty adjustment, which are important to ensure ecological validity of therapy and therapy adherence, respectively. In addition, we found that games themselves could be used to monitor the user's progression. It is also important to determine the impact of multiplayer interactions in the game, test difficulty adjustment approaches that use physiological variables, and define difficulty-skill relationships aligned with the user's preferences. This paper concludes that the main barriers to implement dynamic difficulty adjustment in VR games for cognitive rehabilitation are: (i) the absence of metrics to estimate when the game offers to the players a challenge adapted their skills, and (ii) the lack of a conceptual framework that integrates relevant theories such as state of flow, cognitive load, cognitive rehabilitation, and feedback systems.
C1 [Guzman, D. E.] Univ Cauca, Ciencias Elect, Calle 5 4-70, Popayan 190002, Cauca, Colombia.
   [Rengifo, C. F.] Univ Cauca, Dept Elect Instrumentat & Control, Calle 5 4-70, Popayan 190002, Cauca, Colombia.
   [Guzman, J. D.] Fdn Univ Popayan, Fac Ingn & Arquitectura, Programa Ingn Ind, Cl 8 9-51, Popayan 190002, Cauca, Colombia.
   [Cena, C. E. Garcia] Univ Politecn Madrid, Escuela Tecn Super Ingn & Diseno Ind, Ctr Automat & Robot, C Ronda Valencia 3, Madrid 28012, Spain.
C3 Universidad del Cauca; Universidad del Cauca; Consejo Superior de
   Investigaciones Cientificas (CSIC); Universidad Politecnica de Madrid;
   CSIC-UPM - Centro de Automatica y Robotica
RP Guzmán, DE (corresponding author), Univ Cauca, Ciencias Elect, Calle 5 4-70, Popayan 190002, Cauca, Colombia.
EM diegoguzman@unicauca.edu.co; caferen@unicauca.edu.co;
   david.guzman@docente.fup.edu.co; cecilia.garcia@upm.es
RI Rengifo, Carlos Felipe/I-7892-2019; Cena, Cecilia E García/JCE-3625-2023
OI Rengifo, Carlos Felipe/0000-0002-0601-3481; Guzman Villamarin, Juan
   David/0000-0003-3514-5966
FU University of Cauca; Universidad Politecnica de Madrid; Fundacion
   Universitaria de Popayan
FX This work was supported by Universidad del Cauca, Universidad
   Politecnica de Madrid and Fundacion Universitaria de Popayan.
CR Abd-Alrazaq A, 2022, JMIR SERIOUS GAMES, V10, DOI 10.2196/36123
   Adlakha S, 2020, CHAOS SOLITON FRACT, V140, DOI 10.1016/j.chaos.2020.110192
   Alexander R, 2021, NEUROSCI BIOBEHAV R, V121, P220, DOI 10.1016/j.neubiorev.2020.12.002
   Alloni A, 2017, DISABIL REHABIL, V39, P407, DOI 10.3109/09638288.2015.1096969
   Amorós-Aguilar L, 2021, BRAIN SCI, V11, DOI 10.3390/brainsci11040473
   Cruz CA, 2017, ENTERTAIN COMPUT, V20, P11, DOI 10.1016/j.entcom.2017.02.003
   Ballesteros S, 2015, NEUROSCI BIOBEHAV R, V55, P453, DOI 10.1016/j.neubiorev.2015.06.008
   BECK LA, 1992, J LEISURE RES, V24, P93, DOI 10.1080/00222216.1992.11969876
   Ben Mortenson W, 2017, CAN J AGING, V36, P342, DOI 10.1017/S0714980817000162
   Ben-Sadoun G, 2018, FRONT AGING NEUROSCI, V10, DOI 10.3389/fnagi.2018.00013
   Ben-Sadoun G, 2016, J ALZHEIMERS DIS, V53, P1299, DOI 10.3233/JAD-160268
   Bherer L, 2021, J GERONTOL B-PSYCHOL, V76, P1533, DOI 10.1093/geronb/gbaa124
   Bian Dayi, 2016, Universal Access in Human-Computer Interaction: Users and Context Diversity. 10th International Conference, UAHCI 2016, held as part of HCI International 2016. Proceedings: LNCS 9739, P538, DOI 10.1007/978-3-319-40238-3_51
   Bian Dayi, 2015, 2nd International Conference on Physiological Computing Systems (PhyCS 2015). Proceedings, P137
   Borrego G, 2021, UNIVERSAL ACCESS INF, V20, P767, DOI 10.1007/s10209-020-00746-3
   Brasil L, 2013, WORLD C MEDICAL PHYS, P2046, DOI [10.1007/978-3-642-29305-4_537, DOI 10.1007/978-3-642-29305-4_537]
   Brassel S, 2021, J MED INTERNET RES, V23, DOI 10.2196/26344
   Brown E., 2004, 2004 C HUM FACT COMP, P1297, DOI DOI 10.1145/985921.986048
   Caixinha A, 2014, PROC TECH, V16, P1424, DOI 10.1016/j.protcy.2014.10.162
   Carvalho MB, 2015, COMPUT EDUC, V87, P166, DOI 10.1016/j.compedu.2015.03.023
   Chen J, 2007, COMMUN ACM, V50, P31, DOI 10.1145/1232743.1232769
   Chen XM, 2022, ARCH PHYS MED REHAB, V103, P1422, DOI 10.1016/j.apmr.2022.03.012
   Chen YT, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11020628
   Contreras-Somoza LM, 2021, FRONT PSYCHOL, V12, DOI 10.3389/fpsyg.2021.636116
   Correa O, 2014, IEEE INT CONF SERIOU
   Cuevas-Lara C, 2020, INT J ENV RES PUB HE, V17, DOI 10.3390/ijerph17218058
   Darzi A, 2021, JMIR SERIOUS GAMES, V9, DOI 10.2196/25771
   De oliveira Luciene C., 2021, Research on Biomedical Engineering, P849, DOI 10.1007/s42600-021-00162-3
   Oliveira Rafaela Sanches de, 2014, Dement. neuropsychol., V8, P107, DOI 10.1590/S1980-57642014DN82000004
   Deary IJ, 2009, BRIT MED BULL, V92, P135, DOI 10.1093/bmb/ldp033
   Eichhorn C, 2018, LECT NOTES COMPUT SC, V10927, P526, DOI 10.1007/978-3-319-92037-5_37
   Fang CC, 2020, PARKINSONS DIS-US, V2020, DOI 10.1155/2020/2076942
   Ilya F, 2016, 13TH INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTER ENTERTAINMENT TECHNOLOGY (ACE 2016), DOI 10.1145/3001773.3001818
   Felix C, 2021, J GERONTOL B-PSYCHOL, V76, P1027, DOI 10.1093/geronb/gbaa173
   Flak MM, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.00807
   Foletto AA, 2017, STUD HEALTH TECHNOL, V245, P74, DOI 10.3233/978-1-61499-830-3-74
   Francillette Y, 2021, ENTERTAIN COMPUT, V37, DOI 10.1016/j.entcom.2020.100396
   Fuchs P., 2011, Virtual reality: Concepts and technologies, DOI [10.1201/b11612, DOI 10.1201/B11612]
   Gigerenzer G, 2011, ANNU REV PSYCHOL, V62, P451, DOI 10.1146/annurev-psych-120709-145346
   Goumopoulos C, 2020, PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON INFORMATION AND COMMUNICATION TECHNOLOGIES FOR AGEING WELL AND E-HEALTH (ICT4AWE), P130, DOI 10.5220/0009793501300141
   Hertzog Christopher, 2008, Psychol Sci Public Interest, V9, P1, DOI 10.1111/j.1539-6053.2009.01034.x
   Hunicke R., 2005, P 2005 ACM SIGCHI IN, P429, DOI DOI 10.1145/1178477.1178573
   Ijsselsteijn W., 2007, Proceedings of the 2007 Conference on Future Play, P17, DOI DOI 10.1145/1328202.1328206
   Jha Manish Kumar, 2020, Brain Function Assessment in Learning. Second International Conference, BFAL 2020. Proceedings. Lecture Notes in Artificial Intelligence. Subseries of Lecture Notes in Computer Science (LNAI 12462), P13, DOI 10.1007/978-3-030-60735-7_2
   Knobel SEJ, 2021, JMIR SERIOUS GAMES, V9, DOI 10.2196/29182
   Kurashige H, 2020, FRONT HUM NEUROSCI, V13, DOI 10.3389/fnhum.2019.00457
   Liu ZJ, 2018, 2018 NICOGRAPH INTERNATIONAL (NICOINT 2018), P58, DOI 10.1109/NICOINT.2018.00019
   Maggio MG, 2019, J CLIN NEUROSCI, V65, P106, DOI 10.1016/j.jocn.2019.03.017
   Maier M, 2020, J NEUROENG REHABIL, V17, DOI 10.1186/s12984-020-0652-3
   Manca M, 2021, INT J HUM-COMPUT ST, V145, DOI 10.1016/j.ijhcs.2020.102509
   Mansor NS, 2020, AGING MENT HEALTH, V24, P841, DOI 10.1080/13607863.2019.1574710
   Markle-Reid M, 2003, J ADV NURS, V44, P58, DOI 10.1046/j.1365-2648.2003.02767.x
   Mayer RE, 2003, EDUC PSYCHOL-US, V38, P43, DOI 10.1207/S15326985EP3801_6
   Morán AL, 2015, COMPUT SYST SCI ENG, V30, P43
   Muñoz JE, 2018, PROCEEDINGS OF THE 2018 ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY (CHI PLAY 2018), P329, DOI 10.1145/3242671.3242673
   Nakamura J., 2014, Flow and the Foundations of Positive Psychology: The Collected Works of Mihaly Csikszentmihalyi, P239, DOI [DOI 10.1007/978-94-017-9088-8_16, DOI 10.1007/978-94-017-9088-816, 10.1007/978-94-017-9088-8_16]
   Neto HS, 2018, ENTERTAIN COMPUT, V28, P11, DOI 10.1016/j.entcom.2018.08.002
   Paliokas I, 2020, ADV EXP MED BIOL, V1196, P127, DOI 10.1007/978-3-030-32637-1_13
   Palumbo V., 2020, P 13 ACM INT C PERVA, DOI [10.1145/3389189.3393739, DOI 10.1145/3389189.3393739]
   Parnandi A, 2013, INT CONF AFFECT, P7, DOI 10.1109/ACII.2013.8
   Pasqual TB., 2018, IFAC PAPERSONLINE, V51, P339, DOI [10.1016/j.ifacol.2018.11.617, DOI 10.1016/j.ifacol.2018.11.617]
   Pedraza-Hueso M, 2015, PROCEDIA COMPUT SCI, V75, P161, DOI 10.1016/j.procs.2015.12.233
   Ramdhani A., 2014, International Journal of Basic and Applied Science, V3, P47
   Reijnders J, 2013, AGEING RES REV, V12, P263, DOI 10.1016/j.arr.2012.07.003
   Ritterfeld U., 2009, Serious Games: Mechanisms and Effects, P1, DOI [DOI 10.4324/9780203891650, 10.4324/9780203891650]
   Rodriguez-Isasi Alegandro, 2014, P REHAB OLD GERM MAY, P354
   Ryan RM, 2006, MOTIV EMOTION, V30, P347, DOI 10.1007/s11031-006-9051-8
   Sachdev PS, 2014, NAT REV NEUROL, V10, P634, DOI 10.1038/nrneurol.2014.181
   Salen K., 2003, Rules of Play: Game Design Fundamentals
   Seelye Adriana M, 2012, IEEE Rev Biomed Eng, V5, P29, DOI 10.1109/RBME.2012.2196691
   Segundo CVN, 2016, P SB GAMES
   Seyderhelm AJA, 2019, LECT NOTES COMPUT SC, V11863, P331, DOI 10.1007/978-3-030-34644-7_27
   Shakhova M, 2019, PROCEDIA COMPUT SCI, V156, P395, DOI 10.1016/j.procs.2019.08.219
   Shin SW, 2018, COMM COM INF SC, V851, P81, DOI 10.1007/978-3-319-92279-9_11
   Sonntag D, 2015, Association for the advancement of artificial intelligence fall symposium series
   Streicher A, 2016, LECT NOTES COMPUT SC, V9970, P332, DOI 10.1007/978-3-319-46152-6_14
   SWELLER J, 1988, COGNITIVE SCI, V12, P257, DOI 10.1207/s15516709cog1202_4
   Tarnanas I, 2015, ADV EXP MED BIOL, V821, P63, DOI 10.1007/978-3-319-08939-3_11
   Tong T, 2017, WWW'17 COMPANION: PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P1111, DOI 10.1145/3041021.3054930
   Vallejo V, 2017, COMPUT HUM BEHAV, V70, P500, DOI 10.1016/j.chb.2017.01.021
   Vanderbeken I, 2017, NEUROREHABILITATION, V40, P33, DOI 10.3233/NRE-161388
   VanLehn K., 2006, INT J ARTIF INTELL E, V16, P227, DOI DOI 10.5555/1435351.1435353
   Wang YM, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01832
   Xue S, 2017, WWW'17 COMPANION: PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P465, DOI 10.1145/3041021.3054170
   Yasini M, 2016, STUD HEALTH TECHNOL, V221, P13, DOI 10.3233/978-1-61499-633-0-13
   Zohaib M, 2018, ADV HUM-COMPUT INTER, V2018, DOI 10.1155/2018/5681652
NR 86
TC 0
Z9 0
U1 10
U2 10
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD APR 6
PY 2024
VL 28
IS 2
AR 92
DI 10.1007/s10055-024-00968-3
PG 17
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA NA4D9
UT WOS:001197696300001
OA hybrid
DA 2024-08-05
ER

PT J
AU Kim, I
   Sung, JH
AF Kim, Ikhwan
   Sung, Junghan
TI New proxemics in new space: proxemics in VR
SO VIRTUAL REALITY
LA English
DT Article
DE VR; Virtual space; Proxemics; Human-computer interaction
AB With the development of computer technology, it is possible to design virtual reality (VR) media that provides services to multiple users. Hall's proxemics theory, which holds that the distance varies depending on the relationship between people, has been applied when designing VR in such media. However, this concept was usually applied to designs without criticism and without confirming whether proxemic distances established in physical space are equally valid in VR. This study investigated how proxemics in VR activate differently from those in a physical space. We measured the distance and the number of instances of direct contact between people, with 69 participants from Korea and Turkiye. As a result, a proxemics pattern similar to that of a physical space appeared in VR. However, the average distance between participants in the VR was about 160% greater than in the physical space. Also, we could observe direct contact up to 260% more in the VR than in the physical space. We analyzed the collected data using Bayesian ANOVA and t-tests. We could clarify the difference between the two proxemics in physical space and VR, but the reason for the phenomenon has yet to be discovered. However, this study is meaningful because any industry designing VR, such as those in digital games, can directly apply the findings to manipulate multiple users' emotions and experiences more efficiently. Additionally, this study provides directions for any future studies discussing VR design.
C1 [Kim, Ikhwan] Istanbul Tech Univ, Landscape Architecture, TR-34469 Istanbul, Turkiye.
   [Sung, Junghan] Kongju Natl Univ, Landscape Architecture & Environm Planning, Grad Sch, Yesan 32439, South Korea.
C3 Istanbul Technical University; Kongju National University
RP Kim, I (corresponding author), Istanbul Tech Univ, Landscape Architecture, TR-34469 Istanbul, Turkiye.
EM iikimss3@gmail.com; jh1124jh@gmail.com
RI Kim, Ikhwan/R-2389-2017
OI Kim, Ikhwan/0000-0003-0474-1175
FU Embassy of the Republic of Korea in Turkiye
FX We want to thank the Embassy of the Republic of Korea in Turkiye and its
   staff, who provided valuable assistance during this research.
CR [Anonymous], 2017, P 11 INT SPAC SYNT S
   Chao C.-y., 2020, English Language Teaching, V13, P40, DOI [10.5539/elt.v13n9p40, DOI 10.5539/ELT.V13N9P40]
   Chin H., 2017, International Journal of Modern Languages And Applied Linguistics, V1, P69, DOI [10.24191/ijmal.v1i1.7637, DOI 10.24191/IJMAL.V1I1.7637]
   Cleophas TJ., 2018, Modern Bayesian Statistics in Clinical Research, P83, DOI DOI 10.1007/978-3-319-92747-3_8
   Eggleston T, 2004, Building community in the classroom through ice-breakers and parting ways
   GUREVITCH ZD, 1989, SYMB INTERACT, V12, P251, DOI 10.1525/si.1989.12.2.251
   Hall E. T., 1966, The hidden dimension
   Han X, 2019, Investigating proxemics between avatars in virtual reality
   Hasler B.S., 2012, J INTERCULTURAL COMM, V41, P238, DOI [DOI 10.1080/17475759.2012.728764, 10.1080/17475759.2012.728764]
   Hofstede G., 1980, Cultures Consequences: International Differences in Work-Related Value, V2
   Inaguma H, 2016, P 2 WORKSH ADV SOC S, P11
   Kafle S C., 2019, OCEMManagement Technological Social Science, V1, P126
   Kim I, 2018, ADV ENG INFORM, V38, P458, DOI 10.1016/j.aei.2018.08.014
   Kolkmeier J, 2016, LECT NOTES ARTIF INT, V10011, P1, DOI 10.1007/978-3-319-47665-0_1
   Kruschke JK, 2010, WIRES COGN SCI, V1, P658, DOI 10.1002/wcs.72
   Li R, 2019, ACMIEEE INT CONF HUM, P431, DOI [10.1109/hri.2019.8673116, 10.1109/HRI.2019.8673116]
   Llobera J, 2010, ACM T APPL PERCEPT, V8, DOI 10.1145/1857893.1857896
   McCall C, 2009, SOC INFLUENCE, V4, P138, DOI 10.1080/15534510802517418
   Merritt T, 2017, CHI PLAY'17: PROCEEDINGS OF THE ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P69, DOI 10.1145/3116595.3116598
   Mueller Florian., 2014, P 2014 C DESIGNING I, P533, DOI 10.1145/2598510.2598532
   Onder M, 2020, INT J PUBLIC ADMIN, V43, P283, DOI 10.1080/01900692.2019.1628057
   Sorokowska A, 2017, J CROSS CULT PSYCHOL, V48, P577, DOI 10.1177/0022022117698039
   Weinberg S.L., 2002, DATA ANAL BEHAV SCI
   Williamson JR, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517594
   Yaremych HE, 2019, J EXP SOC PSYCHOL, V85, DOI 10.1016/j.jesp.2019.103845
NR 25
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR 27
PY 2024
VL 28
IS 2
AR 85
DI 10.1007/s10055-024-00982-5
PG 15
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA MQ5Z6
UT WOS:001195120000002
OA hybrid
DA 2024-08-05
ER

PT J
AU Rubio-Arias, JA
   Verdejo-Herrero, A
   Andreu-Caravaca, L
   Ramos-Campo, DJ
AF Rubio-Arias, Jacobo a.
   Verdejo-Herrero, Alberto
   Andreu-Caravaca, Luis
   Ramos-Campo, Domingo J.
TI Impact of immersive virtual reality games or traditional physical
   exercise on cardiovascular and autonomic responses, enjoyment and sleep
   quality: a randomized crossover study
SO VIRTUAL REALITY
LA English
DT Article
DE Intrinsic motivation; Sedentary; Physical activity; Exercise; Exergames;
   Adherence
ID RELIABILITY; VALIDITY; FITNESS; SPORT
AB Objectives: To assess the potential of immersive virtual reality (IVR) in achieving moderate exercise intensity, and 2) to examine the acute effects of two IVR exergame sessions (BOXVR and Beat Saber), comparing them with the impact of traditional exercise on heart rate variability (HRV), perceived effort, delayed onset muscle soreness, motivation, and sleep. Materials and methods: A crossover design was used. The participants (n = 22) randomly performed two sessions of IVR and one session of moderate intensity physical activity, each session lasting 30 min. Heart Rate (HR) and HRV, Perceived Exertion Scale, Intrinsic Motivation Inventory, sleep quality, and perceived pain, were evaluated. Results: The cardiac response to the activities was significantly higher when participants performed traditional physical activity as compared to the BOXVR and Beat Saber games. Traditional training provided a different HRV response as compared to Beat Saber (LnRMSSD, p = 0.025; SDNN, p = 0.031). Although the sessions were planned for moderate intensity, BOXVR generated a moderate intensity (49.3% HRreserve), Beat Saber (29.6% HRreserve) a light one, and the Circuit session, a vigorous one (62.9% HRreserve). In addition, traditional training reported higher perceived exertion and pain with less enjoyment. Differences were observed between the exergames. BOXVR resulted in a lower cardiac response (HRmax and HRmean), and a higher perception of exertion and pain at 72 h. The sleep variables analyzed were not altered by any of the sessions. Conclusions: BOXVR and traditional training can lead to moderate intensity physical activity. However, traditional training could result in lower adherence to physical exercise programs, as it was perceived as more intense and less enjoyable.
C1 [Rubio-Arias, Jacobo a.; Verdejo-Herrero, Alberto] Univ Almeria, Fac Educ Sci, Hlth Res Ctr, Dept Educ, Almeria, Spain.
   [Andreu-Caravaca, Luis] UCAM Univ Catolica Murcia, Fac Deporte, Murcia, Spain.
   [Andreu-Caravaca, Luis] UCAM Univ Catolica Murcia, Fac Deporte, Dept Exercise Physiol, Murcia, Spain.
   [Ramos-Campo, Domingo J.] Univ Politecn Madrid, Fac Phys Act & Sport Sci INEF, Dept Hlth & Human Performance, LFE Res Grp, Madrid, Spain.
C3 Universidad de Almeria; Universidad Catolica de Murcia; Universidad
   Catolica de Murcia; Universidad Politecnica de Madrid; Facultad de
   Ciencias de la Actividad Fisica del Deporte (INEF)
RP Andreu-Caravaca, L (corresponding author), UCAM Univ Catolica Murcia, Fac Deporte, Murcia, Spain.; Andreu-Caravaca, L (corresponding author), UCAM Univ Catolica Murcia, Fac Deporte, Dept Exercise Physiol, Murcia, Spain.
EM landreu@ucam.edu
RI RAMOS, DOMINGO JESÚS/O-3283-2016
OI RAMOS, DOMINGO JESÚS/0000-0002-8890-4244; Andreu Caravaca,
   Luis/0000-0002-0256-8930
CR Achten J, 2003, SPORTS MED, V33, P517, DOI 10.2165/00007256-200333070-00004
   AKERSTEDT T, 1994, PERCEPT MOTOR SKILL, V79, P287, DOI 10.2466/pms.1994.79.1.287
   Bull FC, 2020, BRIT J SPORT MED, V54, P1451, DOI 10.1136/bjsports-2020-102955
   COHEN J, 1992, PSYCHOL BULL, V112, P155, DOI 10.1037/0033-2909.112.1.155
   Debska M, 2019, INT J ENV RES PUB HE, V16, DOI 10.3390/ijerph16193673
   Driver HS, 2000, SLEEP MED REV, V4, P387, DOI 10.1053/smrv.2000.0110
   Düking P, 2018, FRONT PHYSIOL, V9, DOI 10.3389/fphys.2018.00128
   Farrow M, 2019, EUR J SPORT SCI, V19, P719, DOI 10.1080/17461391.2018.1542459
   Feodoroff B, 2019, JMIR SERIOUS GAMES, V7, DOI 10.2196/12324
   Garber CE, 2011, MED SCI SPORT EXER, V43, P1334, DOI 10.1249/MSS.0b013e318213fefb
   Garcia JA, 2016, GAMES HEALTH J, V5, P382, DOI 10.1089/g4h.2016.0070
   Halson SL, 2016, BRIT J SPORT MED, V50, P381, DOI 10.1136/bjsports-2015-094961
   Hopkins W.G., 2006, Sportscience, V10, P63
   Hotfiel T, 2018, SPORTVERLETZ SPORTSC, V32, P243, DOI 10.1055/a-0753-1884
   Khundam C, 2021, INT J COMPUT GAMES T, V2021, DOI 10.1155/2021/6668280
   Lavie CJ, 2019, CIRC RES, V124, P799, DOI 10.1161/CIRCRESAHA.118.312669
   LEWIS SF, 1985, J APPL PHYSIOL, V58, P146, DOI 10.1152/jappl.1985.58.1.146
   Liu WX, 2019, J CLIN MED, V8, DOI 10.3390/jcm8111947
   Matos-Santos L, 2017, INT J SPORTS MED, V38, P883, DOI 10.1055/s-0043-116671
   MCAULEY E, 1989, RES Q EXERCISE SPORT, V60, P48, DOI 10.1080/02701367.1989.10607413
   Lozano-Sánchez AM, 2019, RETOS-NUEV TEND EDUC, P42
   Miyachi M, 2010, MED SCI SPORT EXER, V42, P1149, DOI 10.1249/MSS.0b013e3181c51c78
   Mouatt B, 2020, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.564664
   Moya-Ramon M, 2022, COMPUT METH PROG BIO, V217, DOI 10.1016/j.cmpb.2022.106696
   Nédélec M, 2015, SPORTS MED, V45, P1387, DOI 10.1007/s40279-015-0358-z
   O'Donovan C, 2012, PHYSIOTHERAPY, V98, P205, DOI 10.1016/j.physio.2012.05.001
   Organizacion Mundial de la Salud, 2023, Actividad Fisica, V24
   Panadero-Perez N, 2019, Digital Sedentarism as a Precursor to the Deterioration of Health Adolescents and Young People, V2019, P368
   Peng W, 2011, CYBERPSYCH BEH SOC N, V14, P681, DOI 10.1089/cyber.2010.0578
   Polechonski J, 2019, BIOMED RES INT, V2019, DOI 10.1155/2019/1890527
   Qian JL, 2020, INT J ENV RES PUB HE, V17, DOI 10.3390/ijerph17114133
   Radhakrishnan U, 2023, VIRTUAL REAL-LONDON, V27, P1091, DOI 10.1007/s10055-022-00699-3
   Ramos-Campo DJ, 2019, PHYSIOL BEHAV, V198, P134, DOI 10.1016/j.physbeh.2018.10.020
   Redondo-Flórez L, 2022, INT J ENV RES PUB HE, V19, DOI 10.3390/ijerph192214750
   Rivera-Torres S, 2019, GERONTOL GERIATR MED, V5, DOI 10.1177/2333721418823604
   Rodulfo JIA, 2019, CLIN INVEST ARTERIOS, V31, P233, DOI 10.1016/j.arteri.2019.04.004
   Roman-Viñas B, 2010, EUR J SPORT SCI, V10, P297, DOI 10.1080/17461390903426667
   Rubio-Arias JA, 2019, J CLIN MED, V8, DOI 10.3390/jcm8122156
   Rutkowski S, 2021, FRONT PHYSIOL, V12, DOI 10.3389/fphys.2021.702266
   Stroyer J, 2007, PERCEPT MOTOR SKILL, V104, P519, DOI 10.2466/PMS.104.2.519-533
   Thin AG, 2013, INT J COMPUT GAMES T, V2013, DOI 10.1155/2013/603604
   Tuveri E., 2016, P INT WORKING C ADV, DOI DOI 10.1145/2909132.2909287
   Xu WG, 2020, JMIR SERIOUS GAMES, V8, DOI 10.2196/17972
   Youngstedt Shawn D, 2006, Sleep Biol Rhythms, V4, P215, DOI 10.1111/j.1479-8425.2006.00235.x
NR 44
TC 3
Z9 3
U1 4
U2 4
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 64
DI 10.1007/s10055-024-00981-6
PG 12
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA JV9Q8
UT WOS:001176059900006
OA hybrid
DA 2024-08-05
ER

PT J
AU Yen, HY
   Hsu, H
   Huang, WH
AF Yen, Hsin-Yen
   Hsu, Hsuan
   Huang, Wen-Hsin
TI Virtual reality natural experiences for mental health: comparing the
   effects between different immersion levels
SO VIRTUAL REALITY
LA English
DT Article
DE Anxiety; Depression; Green space; Head-mounted display; Presence;
   Smartphone
ID ENVIRONMENTS; EXPOSURE; STRESS; GREEN; MOOD
AB Virtual nature is an innovative approach for promoting mental health. The purpose of this study was to compare the effects on mental health outcomes between two immersion levels of virtual reality natural experiences. The study design was a cluster trial. Healthy adults were allocated to two experimental groups. Identical pre-recorded 360 degrees videos of natural scenes and sounds were played on the two virtual reality devices, one with a higher immersive level via a head-mounted display and the other one with a lower immersive level via a smartphone. The intervention was conducted for 30 min per session, once a week for 12 weeks. Data were collected by self-reported questionnaires at the baseline and post-intervention. In total, 54 participants completed the interventions. A significantly greater effect was revealed on improving happiness, self-rated health, and physical, mental, social, and environmental quality of life, and ameliorating distress, depression, and somatization in participants who experienced the higher immersive level compared to participants who experienced the lower immersive level. Virtual reality natural experiences with high immersion are recommended to promote mental health.
C1 [Yen, Hsin-Yen] Taipei Med Univ, Coll Nursing, Sch Gerontol & Long Term Care, 250 Wuxing St, Taipei 11031, Taiwan.
   [Hsu, Hsuan] Fu Jen Catholic Univ, Dept Restaurant Hotel & Inst Management, Taipei City, Taiwan.
   [Huang, Wen-Hsin] Humboldt Pk Hlth, Chicago, IL USA.
C3 Taipei Medical University; Fu Jen Catholic University
RP Yen, HY (corresponding author), Taipei Med Univ, Coll Nursing, Sch Gerontol & Long Term Care, 250 Wuxing St, Taipei 11031, Taiwan.
EM yenken520@gmail.com
FU Ministry of Science and Technology, Taiwan
FX No Statement Available
CR Abdel-Khalek AM, 2006, SOC BEHAV PERSONAL, V34, P139, DOI 10.2224/sbp.2006.34.2.139
   Bezold CP, 2018, J ADOLESCENT HEALTH, V62, P488, DOI 10.1016/j.jadohealth.2017.10.008
   Boffi M, 2022, FRONT PSYCHOL, V13, DOI 10.3389/fpsyg.2022.927688
   Bornioli A, 2018, J TRANSP HEALTH, V9, P105, DOI 10.1016/j.jth.2018.02.003
   Brivio E, 2021, VIRTUAL REAL-LONDON, V25, P303, DOI 10.1007/s10055-020-00453-7
   Browning MHEM, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-28070-9
   Browning MHEM, 2020, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.02667
   Bruun-Pedersen J.R., 2018, Journal For Virtual Worlds Research, V9, DOI DOI 10.4101/JVWR.V9I3.7224
   Calogiuri G, 2018, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.02321
   Chang E, 2020, INT J HUM-COMPUT INT, V36, P1658, DOI 10.1080/10447318.2020.1778351
   Cohen J., 1988, Statistical power analysis for the behavioral sciences
   Cummings JJ, 2016, MEDIA PSYCHOL, V19, P272, DOI 10.1080/15213269.2015.1015740
   Dempsey S, 2018, HEALTH PLACE, V54, P110, DOI 10.1016/j.healthplace.2018.09.002
   Du J, 2021, LEISURE STUD, V40, P561, DOI 10.1080/02614367.2020.1862284
   Dudley J, 2023, VIRTUAL REAL-LONDON, V27, P2989, DOI 10.1007/s10055-023-00850-8
   Dzhambov AM, 2018, ENVIRON RES, V166, P223, DOI 10.1016/j.envres.2018.06.004
   Gao T, 2019, INT J ENV RES PUB HE, V16, DOI 10.3390/ijerph16173102
   Hays RD, 2015, J GEN INTERN MED, V30, P1511, DOI 10.1007/s11606-015-3290-x
   Hedblom M, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-46099-7
   Jiang B, 2014, LANDSCAPE URBAN PLAN, V132, P26, DOI 10.1016/j.landurbplan.2014.08.005
   Jung M, 2017, J CARDIOVASC NURS, V32, P464, DOI 10.1097/JCN.0000000000000368
   Knaust T, 2022, VIRTUAL REAL-LONDON, V26, P925, DOI 10.1007/s10055-021-00595-2
   Li HS, 2021, FRONT PSYCHOL, V12, DOI 10.3389/fpsyg.2021.725288
   Li J, 2018, J OUTDOOR REC TOUR, V24, P66, DOI 10.1016/j.jort.2018.08.001
   Liu Q, 2020, CYBERPSYCH BEH SOC N, V23, P157, DOI 10.1089/cyber.2019.0273
   Marín-Morales J, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0223881
   Melo M, 2022, IEEE T VIS COMPUT GR, V28, P1428, DOI 10.1109/TVCG.2020.3010088
   Molina G, 2022, MULTIMED TOOLS APPL, V81, P7733, DOI 10.1007/s11042-021-11760-9
   Moyle W, 2018, GERONTOLOGIST, V58, P478, DOI 10.1093/geront/gnw270
   Olafsdottir G, 2017, HEALTH PLACE, V46, P358, DOI 10.1016/j.healthplace.2017.02.006
   Palanica A, 2019, J ENVIRON PSYCHOL, V63, P44, DOI 10.1016/j.jenvp.2019.04.006
   Papachristos NM, 2017, IEEE INT CONF ADV LE, P477, DOI 10.1109/ICALT.2017.145
   Pavic K, 2023, CYBERPSYCH BEH SOC N, V26, P238, DOI 10.1089/cyber.2022.0245
   Radianti J, 2020, COMPUT EDUC, V147, DOI 10.1016/j.compedu.2019.103778
   Roche K., 2019, Mental Health and Family Medicine, V14, P811
   Rushing JR, 2019, J OUTDOOR REC TOUR, V27, DOI 10.1016/j.jort.2019.100228
   Serafin S, 2023, MULTIMODAL TECHNOLOG, V7, DOI 10.3390/mti7040036
   Slater M, 1997, PRESENCE-VIRTUAL AUG, V6, P603, DOI 10.1162/pres.1997.6.6.603
   Snell TL, 2019, ENVIRON BEHAV, V51, P1082, DOI 10.1177/0013916518787318
   Sun Y, 2023, ENVIRON RES, V216, DOI 10.1016/j.envres.2022.114499
   Tennant M, 2020, EUR J ONCOL NURS, V48, DOI 10.1016/j.ejon.2020.101804
   Terluin B, 2016, HEALTH QUAL LIFE OUT, V14, DOI 10.1186/s12955-016-0533-4
   Veling W, 2021, J MED INTERNET RES, V23, DOI 10.2196/17233
   Wang TC, 2019, J CLIN MED, V8, DOI 10.3390/jcm8070973
   Wang XB, 2019, INT J ENV RES PUB HE, V16, DOI 10.3390/ijerph16183263
   White MP, 2018, NEUROPSYCH DIS TREAT, V14, P3001, DOI 10.2147/NDT.S179038
   World Health Organization, 2019, WHO quality of life-BREF
   Yao G, 2002, J FORMOS MED ASSOC, V101, P342
   Yeo NL, 2020, J ENVIRON PSYCHOL, V72, DOI 10.1016/j.jenvp.2020.101500
   Yin J, 2018, BUILD ENVIRON, V132, P255, DOI 10.1016/j.buildenv.2018.01.006
NR 50
TC 1
Z9 1
U1 17
U2 17
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 52
DI 10.1007/s10055-024-00958-5
PG 11
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA JC4I7
UT WOS:001170940200001
OA hybrid
DA 2024-08-05
ER

PT J
AU Wan, TJ
   Shi, RK
   Xu, WE
   Li, Y
   Atkinson, K
   Yu, LY
   Liang, HN
AF Wan, Tingjie
   Shi, Rongkai
   Xu, Wenge
   Li, Yue
   Atkinson, Katie
   Yu, Lingyun
   Liang, Hai-Ning
TI Hands-free multi-type character text entry in virtual reality
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Text entry; Hands-free interaction; Multi-type
   character entry; Mode switching; Keyboard layout; User study
ID AUTHENTICATION; EMOTICONS
AB Multi-type characters, including uppercase and lowercase letters, symbols, and numbers, are essential in text entry activities. Although multi-type characters are used in passwords, instant messages, and document composition, there has been limited exploration of multi-character text entry for virtual reality head-mounted displays (VR HMDs). Typically, multi-type character entry requires four kinds of keyboards between which users need to switch. In this research, we explore hands-free approaches for rapid multi-type character entry. Our work explores two efficient and usable hands-free approaches for character selection: eye blinks and dwell. To enable quick switching between keyboards, we leverage the usability and efficiency of continuous head motions in the form of cross-based activation. In a pilot study, we explored the usability and efficiency of four locations of the switch keys, the two hands-free selection mechanisms, and crossing-based switching. In the main experiment, we evaluated four user-inspired layouts designed according to the findings from the pilot study. Results show that both blinking and dwell can work well with crossing-based switching and could lead to a relatively fast text entry rate (5.64 words-per-minute (WPM) with blinking and 5.42 WPM with dwell) with low errors (lower than 3% not corrected error rate (NCER)) for complex 8-digit passwords with upper/lowercase letters, symbols, and numbers. For sentences derived from the Brown Corpus, participants can reach 8.48 WPM with blinking and 7.78 WPM with dwell. Overall, as a first exploration, our results show that it is usable and efficient to perform hands-free text entry in VR using either eye blinks or dwell for character selection and crossing for mode switching.
C1 [Wan, Tingjie; Shi, Rongkai; Li, Yue; Yu, Lingyun; Liang, Hai-Ning] Xian Jiaotong Liverpool Univ, Dept Comp, Suzhou, Peoples R China.
   [Xu, Wenge] Birmingham City Univ, DMT Lab, Birmingham, England.
   [Atkinson, Katie] Univ Liverpool, Dept Comp Sci, Liverpool, England.
C3 Xi'an Jiaotong-Liverpool University; Birmingham City University;
   University of Liverpool
RP Liang, HN (corresponding author), Xian Jiaotong Liverpool Univ, Dept Comp, Suzhou, Peoples R China.
EM haining.liang@xjtlu.edu.cn
RI Zhou, Xinyi/KGM-6689-2024; Yang, Ning/KHD-1133-2024
OI wan, tingjie/0009-0003-0237-9587; Liang, Hai-Ning/0000-0003-3600-8955;
   Xu, Wenge/0000-0001-7227-7437
FU National Natural Science Foundation of China
FX The authors thank the participants who volunteered their time to do the
   experiments. We also thank the reviewers for the insightful comments and
   detailed suggestions that helped improve our paper.
CR Accot J., 2002, Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2002, P73, DOI 10.1145/503376.503390
   Barkadehi MH, 2018, TELEMAT INFORM, V35, P1491, DOI 10.1016/j.tele.2018.03.018
   Bi XJ, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P283
   Biener V, 2022, IEEE T VIS COMPUT GR, V28, P2069, DOI 10.1109/TVCG.2022.3150474
   Boletsis C, 2019, TECHNOLOGIES, V7, DOI 10.3390/technologies7020031
   Boletsis Costas., 2019, INT J VIRTUAL REALIT, V19, P2, DOI [DOI 10.20870/IJVR.2019.19.3.2917, 10.20870/IJVR.2019.19.3.2917, DOI 10.20870/IJVR.2019.19.32917]
   Cattelan M, 2012, STAT SCI, V27, P412, DOI 10.1214/12-STS396
   Chabot S., 2019, Commun. Comput. Inf. Sci, P202
   Chen SB, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3312762
   Ciobanu O, 2015, E-HEALTH BIOENG CONF, DOI 10.1109/EHB.2015.7391585
   Cockburn Andy., 2004, People and Computers XVII - Designing for Society, P181
   Dresner E, 2010, COMMUN THEOR, V20, P249, DOI 10.1111/j.1468-2885.2010.01362.x
   Dube T. J., 2019, International Conference on Human-Computer Interaction, P419
   Dube TJ, 2020, CHI'20: EXTENDED ABSTRACTS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3334480.3382882
   Francis W. N., 1979, Brown Corpus Manual
   Garrison Anthony, 2011, Computers and Composition, V28, P112, DOI 10.1016/j.compcom.2011.04.001
   George Ceenu, 2017, P 2017 WORKSH US SEC, DOI DOI 10.14722/USEC.2017.23028
   Grubert J, 2018, IEEE COMPUT GRAPH, V38, P125, DOI 10.1109/MCG.2018.2875609
   Grubert J, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P151, DOI 10.1109/VR.2018.8446250
   Grubert J, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P159, DOI 10.1109/VR.2018.8446059
   Harwood TG., 2003, Pract Assess, Res Eval, V3, P479, DOI [DOI 10.7275/Z6FM-2E34, 10.1362/146934703771910080, https://doi.org/10.7275/z6fm-2-34, DOI 10.7275/Z6FM-2-34]
   Herley C, 2012, IEEE SECUR PRIV, V10, P28, DOI 10.1109/MSP.2011.150
   Jiang HY, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P692, DOI [10.1109/VR46266.2020.1581236395562, 10.1109/VR46266.2020.00-12]
   Knierim P, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173919
   Królak A, 2012, UNIVERSAL ACCESS INF, V11, P409, DOI 10.1007/s10209-011-0256-6
   Kurauchi A, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P1952, DOI 10.1145/2858036.2858335
   Kyungha Min, 2011, Convergence and Hybrid Information Technology. Proceedings 5th International Conference, ICHIT 2011, P778, DOI 10.1007/978-3-642-24082-9_94
   Lee LH, 2020, PERVASIVE MOB COMPUT, V64, DOI 10.1016/j.pmcj.2020.101148
   Li YX, 2021, PEER PEER NETW APPL, V14, P2826, DOI 10.1007/s12083-021-01103-8
   Li Ziming, 2023, 2023 9th International Conference on Virtual Reality (ICVR), P288, DOI 10.1109/ICVR57957.2023.10169556
   Lu Xueshi, 2021, UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology, P815, DOI 10.1145/3472749.3474788
   Lu XS, 2020, INT SYM MIX AUGMENT, P344, DOI 10.1109/ISMAR50242.2020.00061
   Lu XS, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1060, DOI [10.1109/vr.2019.8797901, 10.1109/VR.2019.8797901]
   Luo S, 2020, 2020 NETWORK DISTRIB
   Ma J, 2014, P IEEE S SECUR PRIV, P689, DOI 10.1109/SP.2014.50
   Ma XY, 2018, IUI 2018: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P263, DOI 10.1145/3172944.3172988
   Mackenzie I.S., 2003, P ACM C HUMAN FACTOR, P113
   MacKenzie I.S., 2003, CHI'03 Extended Abstracts on Human Factors in Computing Systems, CHI EA'03, P754, DOI [DOI 10.1145/765891.765971, 10.1145/765891.765971]
   Majaranta P, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P357
   Mardanbegi D, 2019, ETRA 2019: 2019 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, DOI 10.1145/3317956.3318155
   Meng XR, 2022, INT SYM MIX AUGMENT, P74, DOI 10.1109/ISMAR55827.2022.00021
   Minakata K, 2019, ETRA 2019: 2019 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, DOI 10.1145/3317956.3318150
   Monteiro P, 2021, IEEE T VIS COMPUT GR, V27, P2702, DOI 10.1109/TVCG.2021.3067687
   NOYES J, 1983, INT J MAN MACH STUD, V18, P265, DOI 10.1016/S0020-7373(83)80010-8
   Ogitani T, 2018, INT CON ADV INFO NET, P342, DOI 10.1109/AINA.2018.00059
   Olade I, 2018, 2018 IEEE SMARTWORLD, UBIQUITOUS INTELLIGENCE & COMPUTING, ADVANCED & TRUSTED COMPUTING, SCALABLE COMPUTING & COMMUNICATIONS, CLOUD & BIG DATA COMPUTING, INTERNET OF PEOPLE AND SMART CITY INNOVATION (SMARTWORLD/SCALCOM/UIC/ATC/CBDCOM/IOP/SCI), P1997, DOI 10.1109/SmartWorld.2018.00334
   Otte A, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1729, DOI 10.1109/VR.2019.8797740
   Park J, 2014, J COMMUN, V64, P333, DOI 10.1111/jcom.12086
   Pavlovych A, 2009, EICS'09: PROCEEDINGS OF THE ACM SIGCHI SYMPOSIUM ON ENGINEERING INTERACTIVE COMPUTING SYSTEMS, P187
   Payton L, 2010, Psi Chi Journal of Undergraduate Research, V15
   Pham DM, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3364265
   Prätorius M, 2015, IEEE COMPUT GRAPH, V35, P42, DOI 10.1109/MCG.2015.106
   Proctor RW, 2002, BEHAV RES METH INS C, V34, P163, DOI 10.3758/BF03195438
   Qian Y, 2017, SUI'17: PROCEEDINGS OF THE 2017 SYMPOSIUM ON SPATIAL USER INTERACTION, P91, DOI 10.1145/3131277.3132182
   Rajanna V, 2018, 2018 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2018), DOI 10.1145/3204493.3204541
   SALTHOUSE TA, 1986, PSYCHOL BULL, V99, P303, DOI 10.1037/0033-2909.99.3.303
   Schneider D, 2019, IEEE T VIS COMPUT GR, V25, P3190, DOI 10.1109/TVCG.2019.2932239
   Serrano B., 2019, Virtual Reality and Anxiety Disorders Treatment: Evolution and Future Perspectives, P47, DOI [10.1007/978-1-4939-9482-3_3, DOI 10.1007/978-1-4939-9482-3_3, DOI 10.1007/978-1-4939-9482-3]
   Shay R., 2010, PROC SOUPS, P1
   Shay R, 2016, ACM T INFORM SYST SE, V18, DOI 10.1145/2891411
   Soukoreff R.W., 2001, CHI '01 extended abstracts on Human factors in computing systems, CHI EA '01, P319, DOI DOI 10.1145/634067.634256
   Speicher M, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174221
   Tu HW, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445340
   Tu HW, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300848
   Wiederhold BK, 2019, CYBERPSYCH BEH SOC N, V22, P3, DOI 10.1089/cyber.2018.29136.bkw
   Xu WG, 2019, IEEE T VIS COMPUT GR, V25, P1991, DOI 10.1109/TVCG.2019.2898736
   Yamada H., 1981, VII, V13, P1547
   Yanagihara N, 2016, SUI'18: PROCEEDINGS OF THE 2018 SYMPOSIUM ON SPATIAL USER INTERACTION, P170, DOI 10.1145/3267782.3274687
   Yu C, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P4479, DOI 10.1145/3025453.3025964
   Yu DF, 2018, IEEE T VIS COMPUT GR, V24, P2927, DOI 10.1109/TVCG.2018.2868581
   Yukang Yan, 2018, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V2, DOI 10.1145/3287076
NR 71
TC 0
Z9 0
U1 3
U2 4
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 8
DI 10.1007/s10055-023-00902-z
PG 19
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA DX0O7
UT WOS:001135265600003
OA Bronze
DA 2024-08-05
ER

PT J
AU Weser, VU
   Sieberer, J
   Berry, J
   Tuakli-Wosornu, Y
AF Weser, Veronica U.
   Sieberer, Johannes
   Berry, Justin
   Tuakli-Wosornu, Yetsa
TI Navigation in immersive virtual reality: a comparison of 1:1 walking to
   1:1 wheeling
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Wheelchair; Locomotion; Navigation
AB In most virtual reality (VR) experiences, locomotion is not the primary objective. Yet how someone moves through a virtual environment (VE) has a profound impact on their experience. Extant research considers 1:1 walking, where the same movement a user makes in the real world is translated identically into movement in the virtual world, as the paragon of comfortable VR locomotion. However, this form of locomotion is not practical in most applications and remains inaccessible to non-ambulant persons using VR. In the current study, 1:1 walking was compared to 1:1 wheeling, whereby pushes made on a physical wheelchair translated directly into movement in the VE. User experience in 1:1 walking was compared to 1:1 wheeling through changes in self-reported positive and negative affect, simulator sickness, system usability, and presence. Participants' ability to learn a complex VE in the two conditions was also assessed with a spatial updating task. These comparisons revealed no statistically significant differences. This finding challenges the prevailing assumption that 1:1 walking in VR is superior to all other locomotion techniques and serves as an appeal for VR developers to consider designing applications for a seated experience that is safe, comfortable, and accessible to a broader population of people with diverse needs.
C1 [Weser, Veronica U.; Berry, Justin] Yale Sch Med, Dept Psychiat, 2 Church St South,Suite 201, New Haven, CT 06519 USA.
   [Sieberer, Johannes] Yale Univ, Sch Engn & Appl Sci, Dept Mech Engn & Mat Sci, New Haven, CT USA.
   [Berry, Justin] Yale Univ, Sch Art, New Haven, CT 06520 USA.
   [Tuakli-Wosornu, Yetsa] Yale Sch Publ Hlth, Dept Social & Behav Sci, New Haven, CT USA.
C3 Yale University; Yale University; Yale University; Yale University
RP Weser, VU (corresponding author), Yale Sch Med, Dept Psychiat, 2 Church St South,Suite 201, New Haven, CT 06519 USA.
EM veronica.weser@yale.edu
OI Sieberer, Johannes/0000-0002-4191-3442
FU HP Labs; HP Applied Research Grant
FX This research was supported by an HP Applied Research Grant to the third
   author designed to facilitate use cases and strategies for working with
   emerging technology.
CR Adelola IA, 2002, P 4 INT C DIS VIRT R, P173
   Bangor A, 2008, INT J HUM-COMPUT INT, V24, P574, DOI 10.1080/10447310802205776
   Barhorst-Cates EM, 2021, MEM COGNITION, V49, P572, DOI 10.3758/s13421-020-01111-8
   Barhorst-Cates EM, 2020, EXP BRAIN RES, V238, P1911, DOI 10.1007/s00221-020-05851-6
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Blackwell A.G., 2017, Stanford Social Innovation Review
   Borchiellini R, 2018, EUROGRAPHICS SHORT P, P53
   Brachtendorf K, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P291, DOI [10.1109/VRW50115.2020.0-212, 10.1109/VRW50115.2020.00064]
   Brooke J., 1996, SUS-a quick and dirty usability scale, DOI [DOI 10.1201/9781498710411-35, DOI 10.1201/9781498710411]
   Bruder G, 2015, IEEE T VIS COMPUT GR, V21, P539, DOI 10.1109/TVCG.2015.2391864
   Caserman P, 2021, VIRTUAL REAL-LONDON, V25, P1153, DOI 10.1007/s10055-021-00513-6
   Cherep LA, 2020, J EXP PSYCHOL-APPL, V26, P480, DOI 10.1037/xap0000263
   Cherni H., 2020, International Journal of Virtual Reality, V20, P1, DOI [DOI 10.20870/IJVR.2020.20.1.3183, 10.20870/ijvr.2020.20.1, DOI 10.20870/IJVR.2020.20.1]
   Cherni H., 2021, Int. J. Virtual Reality, V21, P1, DOI [10.20870/IJVR.2021.21.1.3046, DOI 10.20870/IJVR.2021.21.1.3046]
   Chrastil ER, 2013, J EXP PSYCHOL LEARN, V39, P1520, DOI 10.1037/a0032382
   Deffenbacher K.A., 1980, LAW HUMAN BEHAV, V4, P243, DOI [DOI 10.1007/BF01040617, 10.1007/BF01040617]
   Fiore LorenPuchalla., 2013, Proceedings of the Joint Virtual Reality Conference of ICAT-EGVE-EuroVR, P83, DOI DOI 10.2312/EGVE.JVRC13.083-090
   Hale K S., 2014, Handbook of Virtual Environments: Design, Implementation, and Applications, V0th, DOI DOI 10.1201/B17360
   Harrison C, 2000, 3 INT C DIS VR ASS T
   Hoare E, 2016, INT J BEHAV NUTR PHY, V13, DOI 10.1186/s12966-016-0432-4
   HP Reverb G2 Omnicept Edition, 2022, HP Reverb G2 Omnicept Edition|HP® Official Site)
   Iezzoni LI, 2001, J GEN INTERN MED, V16, P235, DOI 10.1046/j.1525-1497.2001.016004235.x
   JohannesMCI, 2022, Zenodo, DOI 10.5281/ZENODO.6557405
   Kennedy R.S., 1993, Int. J. Aviat. Psy, P203
   Kim HK, 2018, APPL ERGON, V69, P66, DOI 10.1016/j.apergo.2017.12.016
   Kunz BR, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0054446
   Lee E, 2019, PERSPECT PSYCHIATR C, V55, P164, DOI 10.1111/ppc.12296
   Lee J, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18092832
   Levine K, 2023, TRANSPORT POLICY, V131, P66, DOI 10.1016/j.tranpol.2022.12.012
   Lombard M., 1997, Journal of Computer-Mediated Communication, V3, DOI [10.1111/j.1083-6101.1997.tb0, 10.1111/j.1083-6101.1997.tb00072.x., 10.1111/j.1083-6101.1997.tb00072.x, DOI 10.1111/J.1083-6101.1997.TB00072.X, 10.1111/J.1083-6101.1997.TB00072.X/4080403]
   Magnon V, 2018, FRONT PUBLIC HEALTH, V6, DOI 10.3389/fpubh.2018.00239
   Nabiyouni Mahdi, 2015, 2015 IEEE Symposium on 3D User Interfaces (3DUI), P3, DOI 10.1109/3DUI.2015.7131717
   Schubert T. W., 2003, Z. fur Medienpsychologie, V15, P69, DOI [10.1026//1617-6383.15.2.69, DOI 10.1026//1617-6383.15.2.69]
   Sieberer J, Study protocol VR locomotion study IRB 2000031912
   Simons DJ, 1998, PSYCHOL SCI, V9, P315, DOI 10.1111/1467-9280.00062
   Sjolund LA, 2018, MEM COGNITION, V46, P89, DOI 10.3758/s13421-017-0747-7
   Sonar AV, 2005, 2005 IEEE International Conference on Mechatronics and Automations, Vols 1-4, Conference Proceedings, P222
   Steinicke F, 2010, IEEE T VIS COMPUT GR, V16, P17, DOI 10.1109/TVCG.2009.62
   Taylor D.M., 2018, AMERICANS DISABILITI, P1
   Wan XI, 2009, ATTEN PERCEPT PSYCHO, V71, P42, DOI 10.3758/APP.71.1.42
   Warren LE, 2017, SUI'17: PROCEEDINGS OF THE 2017 SYMPOSIUM ON SPATIAL USER INTERACTION, P163, DOI 10.1145/3131277.3134359
   WATSON D, 1988, J PERS SOC PSYCHOL, V54, P1063, DOI 10.1037/0022-3514.54.6.1063
   Wu YY, 2023, SCAND J MED SCI SPOR, V33, P257, DOI 10.1111/sms.14277
NR 43
TC 0
Z9 0
U1 3
U2 4
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 4
DI 10.1007/s10055-023-00901-0
PG 12
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA DK0M5
UT WOS:001131813200001
DA 2024-08-05
ER

PT J
AU Vlahovic, S
   Skorin-Kapov, L
   Suznjevic, M
   Pavlin-Bernardic, N
AF Vlahovic, Sara
   Skorin-Kapov, Lea
   Suznjevic, Mirko
   Pavlin-Bernardic, Nina
TI Not just cybersickness: short-term effects of popular VR game mechanics
   on physical discomfort and reaction time
SO VIRTUAL REALITY
LA English
DT Article
DE VR gaming; Cybersickness; Workload; Discomfort; Reaction time; VR game
   mechanics; User studies
ID VIRTUAL-REALITY; SICKNESS; QUESTIONNAIRE; SENSE
AB Uncomfortable sensations that arise during virtual reality (VR) use have always been among the industry's biggest challenges. While certain VR-induced effects, such as cybersickness, have garnered a lot of interest from academia and industry over the years, others have been overlooked and underresearched. Recently, the research community has been calling for more holistic approaches to studying the issue of VR discomfort. Focusing on active VR gaming, our article presents the results of two user studies with a total of 40 participants. Incorporating state-of-the-art VR-specific measures (the Simulation Task Load Index-SIM-TLX, Cybersickness Questionnaire-CSQ, Virtual Reality Sickness Questionnaire-VRSQ) into our methodology, we examined workload, musculoskeletal discomfort, device-related discomfort, cybersickness, and changes in reaction time following VR gameplay. Using a set of six different active VR games (three per study), we attempted to quantify and compare the prevalence and intensity of VR-induced symptoms across different genres and game mechanics. Varying between individuals, as well as games, the diverse symptoms reported in our study highlight the importance of including measures of VR-induced effects other than cybersickness into VR gaming user studies, while questioning the suitability of the Simulator Sickness Questionnaire (SSQ)-arguably the most prevalent measure of VR discomfort in the field-for use with active VR gaming scenarios.
C1 [Vlahovic, Sara; Skorin-Kapov, Lea; Suznjevic, Mirko] Univ Zagreb, Fac Elect Engn & Comp, Unska 3, Zagreb 10000, Croatia.
   [Pavlin-Bernardic, Nina] Univ Zagreb, Fac Humanities & Social Sci, Ul Ivana Lucica 3, Zagreb 10000, Croatia.
C3 University of Zagreb; University of Zagreb
RP Vlahovic, S (corresponding author), Univ Zagreb, Fac Elect Engn & Comp, Unska 3, Zagreb 10000, Croatia.
EM sara.vlahovic@fer.hr; lea.skorin-kapov@fer.hr; mirko.suznjevic@fer.hr;
   nbernardi@ffzg.hr
RI Vlahovic, Sara/AAM-9730-2021
OI Vlahovic, Sara/0000-0002-7418-0943
FU Croatian Science Foundation [IP-2019-04-9793, DOK-2020-01-3779]
FX This work has been fully supported by the Croatian Science Foundation
   under the project Modeling and Monitoring QoE for Immersive 5G-Enabled
   Multimedia Services (Q-MERSIVE), grant numbers IP-2019-04-9793 and
   DOK-2020-01-3779
CR Aardema F, 2010, CYBERPSYCH BEH SOC N, V13, P429, DOI 10.1089/cyber.2009.0164
   Audiffren M, 2008, ACTA PSYCHOL, V129, P410, DOI 10.1016/j.actpsy.2008.09.006
   Barreda-Angeles M, 2023, CYBERPSYCH BEH SOC N, V26, P22, DOI 10.1089/cyber.2022.0152
   Baur D, 2021, J MED CASE REP, V15, DOI 10.1186/s13256-021-02880-9
   Bimberg P, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P464, DOI [10.1109/VRW50115.2020.00098, 10.1109/VRW50115.2020.0-178]
   Borg G., 1998, Borgs Perceived Exertion and Pain Scales, DOI DOI 10.1249/00005768-199809000-00018
   Boring S., 2009, P 21 ANN C AUSTR COM, P161
   Brooks JO, 2010, ACCIDENT ANAL PREV, V42, P788, DOI 10.1016/j.aap.2009.04.013
   Chang E, 2020, INT J HUM-COMPUT INT, V36, P1658, DOI 10.1080/10447318.2020.1778351
   Chihara T, 2018, APPL ERGON, V68, P204, DOI 10.1016/j.apergo.2017.11.016
   Cobb SVG, 1999, PRESENCE-TELEOP VIRT, V8, P169, DOI 10.1162/105474699566152
   Deary IJ, 2011, BEHAV RES METHODS, V43, P258, DOI 10.3758/s13428-010-0024-1
   Drachen A., 2018, Games user research, DOI [10.1093/oso/9780198794844.001.0001, DOI 10.1093/OSO/9780198794844.001.0001]
   Dye MWG, 2009, CURR DIR PSYCHOL SCI, V18, P321, DOI 10.1111/j.1467-8721.2009.01660.x
   Erickson A, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P434, DOI [10.1109/VR46266.2020.1580695145399, 10.1109/VR46266.2020.00-40]
   EvangelistaBelo JM, 2021, P 2021 CHI C HUM FAC, P1
   Evans E, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.772293
   Evans E, 2021, GAMES HEALTH J, V10, P314, DOI 10.1089/g4h.2021.0036
   Fadeev KA, 2020, BEHAV NEUROL, V2020, DOI 10.1155/2020/5758038
   Faul F, 2007, BEHAV RES METHODS, V39, P175, DOI 10.3758/BF03193146
   Fernandez-Ruiz J, 2011, BEHAV BRAIN RES, V219, P8, DOI 10.1016/j.bbr.2010.11.060
   Foxman Maxwell, 2020, CHI PLAY '20: Extended Abstracts of the 2020 Annual Symposium on Computer-Human Interaction in Play, P237, DOI 10.1145/3383668.3419881
   Gianaros PJ, 2001, AVIAT SPACE ENVIR MD, V72, P115
   Hansberger JT, 2017, LECT NOTES COMPUT SC, V10280, P505, DOI 10.1007/978-3-319-57987-0_41
   Harris D, 2020, VIRTUAL REAL-LONDON, V24, P557, DOI 10.1007/s10055-019-00422-9
   HART S G, 1988, P139
   Hart SG., 2006, P HUM FACT ERG SOC A, V50, P904, DOI [10.1177/154193120605000909, DOI 10.1177/154193120605000909]
   Hincapié-Ramos JD, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P1063, DOI 10.1145/2556288.2557130
   Hirzle T, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445361
   Iskander J, 2018, IEEE ACCESS, V6, P19345, DOI 10.1109/ACCESS.2018.2815663
   Jager J, 2017, MONOGR SOC RES CHILD, V82, P13, DOI 10.1111/mono.12296
   Kennedy R.S., 1993, Int. J. Aviat. Psy, P203
   Kim E, 2021, ERGONOMICS, V64, P891, DOI 10.1080/00140139.2020.1869320
   Kim HK, 2018, APPL ERGON, V69, P66, DOI 10.1016/j.apergo.2017.12.016
   Knight JF, 2007, HUM FACTORS, V49, P797, DOI 10.1518/001872007X230172
   Kourtesis P, 2019, FRONT HUM NEUROSCI, V13, DOI 10.3389/fnhum.2019.00417
   La Viola J. J.  Jr., 2000, SIGCHI Bulletin, V32, P47, DOI 10.1145/333329.333344
   Martens MAG, 2019, J PSYCHOPHARMACOL, V33, P1264, DOI 10.1177/0269881119860156
   Mehrfard A, 2019, Arxiv, DOI arXiv:1912.02913
   Meta, 2023, Health and safety warnings
   Meta, 2023, View comfort ratings for Meta Quest store content
   Mittelstaedt JM, 2019, VIRTUAL REAL-LONDON, V23, P143, DOI 10.1007/s10055-018-0370-3
   Nalivaiko E, 2015, PHYSIOL BEHAV, V151, P583, DOI 10.1016/j.physbeh.2015.08.043
   Nesbitt K, 2017, DISPLAYS, V48, P1, DOI 10.1016/j.displa.2017.01.002
   Pallavicini F, 2019, SIMULAT GAMING, V50, P136, DOI 10.1177/1046878119831420
   Pallavicini F, 2018, ADV INTELL SYST, V608, P225, DOI 10.1007/978-3-319-60639-2_23
   PATEL S, 1991, OPTOMETRY VISION SCI, V68, P888, DOI 10.1097/00006324-199111000-00010
   Peckmann C, 2022, COMPUT HUM BEHAV, V131, DOI 10.1016/j.chb.2022.107233
   Penumudi SA, 2020, APPL ERGON, V84, DOI 10.1016/j.apergo.2019.103010
   Prabaswari AD., 2019, IOP Conf Ser Mater Sci Eng, V528, P012018, DOI DOI 10.1088/1757-899X/528/1/012018
   Reason J. T., 1975, Motion Sickness
   REASON JT, 1968, BRIT J PSYCHOL, V59, P385, DOI 10.1111/j.2044-8295.1968.tb01153.x
   Rebenitsch L, 2016, VIRTUAL REAL-LONDON, V20, P101, DOI 10.1007/s10055-016-0285-9
   Sevinc V, 2020, APPL ERGON, V82, DOI 10.1016/j.apergo.2019.102958
   Shelstad William J., 2017, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V61, P2072, DOI 10.1177/1541931213602001
   Somrak A, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21041185
   Souchet AD, 2023, VIRTUAL REAL-LONDON, V27, P19, DOI 10.1007/s10055-022-00672-0
   Stanney K, 2020, FRONT ROBOT AI, V7, DOI 10.3389/frobt.2020.00004
   Stanney KM, 2002, HUM PERFORM, V15, P339, DOI 10.1207/S15327043HUP1504_03
   Stanney KM, 1997, PROCEEDINGS OF THE HUMAN FACTORS AND ERGONOMICS SOCIETY 41ST ANNUAL MEETING, 1997, VOLS 1 AND 2, P1138, DOI 10.1177/107118139704100292
   Statista, 2019, Average session time of VR users in the U.S. 2019 Statista
   Steam, 2023, Steam Hardware & Software Survey
   Stone W, 2017, THESIS IOWA STATE U
   Szpak A, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P524, DOI 10.1109/VRW55335.2022.00119
   Szpak A, 2020, J MED INTERNET RES, V22, DOI 10.2196/19840
   Szpak A, 2019, IEEE ACCESS, V7, P130883, DOI 10.1109/ACCESS.2019.2940073
   Tcha-Tokey K., 2016, International Journal of Virtual Reality, V16, P33, DOI [DOI 10.20870/IJVR.2016.16.1.2880, 10.20870/IJVR.2016.16.1.2880]
   The VR Shop, 2023, Comfort rating explained-green, yellow, red
   Triberti S, 2014, CYBERPSYCH BEH SOC N, V17, P335, DOI 10.1089/cyber.2014.0054
   Varmaghani S, 2022, VIRTUAL REAL-LONDON, V26, P659, DOI 10.1007/s10055-021-00535-0
   Vlahovic S, 2023, INT WORK QUAL MULTIM, P95, DOI 10.1109/QOMEX58391.2023.10178660
   Vlahovic S, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11182946
   Vlahovic S, 2021, INT WORK QUAL MULTIM, P163, DOI 10.1109/QoMEX51781.2021.9465470
   Yoo S, 2018, PROCEEDINGS OF THE 2018 ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY COMPANION EXTENDED ABSTRACTS (CHI PLAY 2018), P295, DOI 10.1145/3270316.3272054
   Yuan J., 2018, Int J Ophthalmol Clin Res, V5, P85, DOI 10.23937/2378-346X/1410085
   Zindulka T, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376639
NR 76
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAY 15
PY 2024
VL 28
IS 2
AR 108
DI 10.1007/s10055-024-01007-x
PG 30
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA QV5K8
UT WOS:001223655900001
OA hybrid
DA 2024-08-05
ER

PT J
AU Lima, R
   Chirico, A
   Varandas, R
   Gamboa, H
   Gaggioli, A
   Badia, SB
AF Lima, Rodrigo
   Chirico, Alice
   Varandas, Rui
   Gamboa, Hugo
   Gaggioli, Andrea
   Badia, Sergi Bermudez
TI Multimodal emotion classification using machine learning in immersive
   and non-immersive virtual reality
SO VIRTUAL REALITY
LA English
DT Article
DE Affective computing; Emotions; Wearables; Physiological signals; Machine
   learning; Virtual reality
ID RECOGNITION; APPRAISAL; DATABASE; VALENCE; STRESS
AB Affective computing has been widely used to detect and recognize emotional states. The main goal of this study was to detect emotional states using machine learning algorithms automatically. The experimental procedure involved eliciting emotional states using film clips in an immersive and non-immersive virtual reality setup. The participants' physiological signals were recorded and analyzed to train machine learning models to recognize users' emotional states. Furthermore, two subjective ratings emotional scales were provided to rate each emotional film clip. Results showed no significant differences between presenting the stimuli in the two degrees of immersion. Regarding emotion classification, it emerged that for both physiological signals and subjective ratings, user-dependent models have a better performance when compared to user-independent models. We obtained an average accuracy of 69.29 +/- 11.41% and 71.00 +/- 7.95% for the subjective ratings and physiological signals, respectively. On the other hand, using user-independent models, the accuracy we obtained was 54.0 +/- 17.2% and 24.9 +/- 4.0%, respectively. We interpreted these data as the result of high inter-subject variability among participants, suggesting the need for user-dependent classification models. In future works, we intend to develop new classification algorithms and transfer them to real-time implementation. This will make it possible to adapt to a virtual reality environment in real-time, according to the user's emotional state.
C1 [Lima, Rodrigo; Badia, Sergi Bermudez] Univ Madeira, Fac Ciencias Exatas & Engn, Campus Univ Penteada, P-9000105 Funchal, Portugal.
   [Lima, Rodrigo; Badia, Sergi Bermudez] Univ Nova Lisboa, Fac Ciencias & Tecnol, NOVA Lab Comp Sci & Informat, P-2829516 Caparica, Portugal.
   [Lima, Rodrigo; Badia, Sergi Bermudez] Agencia Reg Desenvolvimento Invest Tecnol & Inovac, Caminho Penteada, P-9020105 Funchal, Portugal.
   [Chirico, Alice; Gaggioli, Andrea] Univ Cattolica Sacro Cuore, Dipartimento Psicol, Largo Agostino Gemelli 1, I-20123 Milan, Italy.
   [Gaggioli, Andrea] Ist Auxol Italiano, IRCCS, Appl Technol Neuropsychol Lab, I-20149 Milan, Italy.
   [Varandas, Rui; Gamboa, Hugo] Univ Nova Lisboa, Fac Ciencias & Tecnol, Lab Instrumentat Biomed Engn & Radiat Phys, LIBPhys, P-2829516 Setubal, Portugal.
   [Varandas, Rui; Gamboa, Hugo] PLUX Wireless Biosignals SA, Ave 5 Outubro 70, P-1050059 Lisbon, Portugal.
C3 Universidade da Madeira; Universidade Nova de Lisboa; Catholic
   University of the Sacred Heart; IRCCS Istituto Auxologico Italiano;
   Universidade Nova de Lisboa
RP Lima, R (corresponding author), Univ Madeira, Fac Ciencias Exatas & Engn, Campus Univ Penteada, P-9000105 Funchal, Portugal.; Lima, R (corresponding author), Univ Nova Lisboa, Fac Ciencias & Tecnol, NOVA Lab Comp Sci & Informat, P-2829516 Caparica, Portugal.; Lima, R (corresponding author), Agencia Reg Desenvolvimento Invest Tecnol & Inovac, Caminho Penteada, P-9020105 Funchal, Portugal.
EM rodrigo.lima@arditi.pt; alice.chirico@unicatt.it;
   r.varandas@campus.fct.unl.pt; hgamboa@fct.unl.pt;
   andrea.gaggioli@unicatt.it; sergi.bermudez@uma.pt
FU Fundao para a Cincia e Tecnologia (FCT)
FX No Statement Available
CR Achlioptas P, 2021, Arxiv, DOI arXiv:2101.07396
   Aue T, 2007, BIOL PSYCHOL, V74, P347, DOI 10.1016/j.biopsycho.2006.09.001
   Berking M, 2012, CURR OPIN PSYCHIATR, V25, P128, DOI 10.1097/YCO.0b013e3283503669
   Bernardes A, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22176528
   Betella A, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0148037
   Borrego A, 2016, J NEUROENG REHABIL, V13, DOI 10.1186/s12984-016-0174-1
   Bota PJ, 2019, IEEE ACCESS, V7, P140990, DOI 10.1109/ACCESS.2019.2944001
   Boucsein W, 2012, ELECTRODERMAL ACTIVITY, SECOND EDITION, P1, DOI 10.1007/978-1-4614-1126-0
   BRADLEY MM, 1994, J BEHAV THER EXP PSY, V25, P49, DOI 10.1016/0005-7916(94)90063-9
   CACIOPPO JT, 1990, AM PSYCHOL, V45, P16, DOI 10.1037/0003-066X.45.1.16
   Camm AJ, 1996, CIRCULATION, V93, P1043
   Carvalho S, 2012, APPL PSYCHOPHYS BIOF, V37, P279, DOI 10.1007/s10484-012-9201-6
   Chanel G, 2007, IEEE SYS MAN CYBERN, P375
   Chen RC, 2020, J BIG DATA-GER, V7, DOI 10.1186/s40537-020-00327-4
   Chen Shali, 2020, Zhongguo Yi Liao Qi Xie Za Zhi, V44, P283, DOI 10.3969/j.issn.1671-7104.2020.04.001
   Crawford HJ, 1996, INT J PSYCHOPHYSIOL, V24, P239, DOI 10.1016/S0167-8760(96)00067-0
   Dan-Glauser ES, 2011, BEHAV RES METHODS, V43, P468, DOI 10.3758/s13428-011-0064-1
   Denson TF, 2009, PSYCHOL BULL, V135, P823, DOI 10.1037/a0016909
   Donges N., 2018, The Random Forest Algorithm
   EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068
   Finseth TT, 2023, IEEE ACCESS, V11, P25431, DOI 10.1109/ACCESS.2023.3254134
   Garcia J, 2017, P 18 INT C HUM COMP, P1, DOI [DOI 10.1145/3123818.3123852, 10.1145/3123818, DOI 10.1145/3123818]
   Goncalves A, 2022, IEEE T VIS COMPUT GR, V28, P4452, DOI 10.1109/TVCG.2021.3091485
   Gouizi K., 2011, Journal of Medical Engineering & Technology, V35, P300, DOI 10.3109/03091902.2011.601784
   Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797
   Haag A, 2004, LECT NOTES COMPUT SC, V3068, P36
   HANLEY JA, 1982, RADIOLOGY, V143, P29, DOI 10.1148/radiology.143.1.7063747
   Hossin M., 2015, INT J DATA MIN KNOWL, V5, P1, DOI DOI 10.5121/IJDKP.2015.5201
   Hu X, 2017, FRONT HUM NEUROSCI, V11, DOI 10.3389/fnhum.2017.00026
   Inwood E, 2018, APPL PSYCHOL-HLTH WE, V10, P215, DOI 10.1111/aphw.12127
   Jain AK, 1996, COMPUTER, V29, P31, DOI 10.1109/2.485891
   Jerritta S., 2011, 2011 Proceedings of IEEE 7th International Colloquium on Signal Processing & its Applications (CSPA 2011), P410, DOI 10.1109/CSPA.2011.5759912
   Juvrud J, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00305
   Kim J, 2008, IEEE T PATTERN ANAL, V30, P2067, DOI 10.1109/TPAMI.2008.26
   Koelstra S, 2012, IEEE T AFFECT COMPUT, V3, P18, DOI 10.1109/T-AFFC.2011.15
   Kothe C, 2019, LabStreamingLayer
   Lang P J., 2005, Emotion
   Li L, 2006, IIH-MSP: 2006 INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING, PROCEEDINGS, P355
   Liapis A., 2013, ACM International Conference Proceeding Series, P258, DOI [10.1145/2491845.2491874, DOI 10.1145/2491845.2491874]
   Lindquist KA, 2012, BEHAV BRAIN SCI, V35, P121, DOI 10.1017/S0140525X11000446
   Liu C, 2005, IEEE INT CONF ROBOT, P3262, DOI 10.1109/ROBOT.2005.1570613
   Maaoui Choubeila, 2010, Cutting Edge Robotics 2010, P317
   Makowski D, 2021, BEHAV RES METHODS, V53, P1689, DOI 10.3758/s13428-020-01516-y
   Maria E, 2019, ELECTRON NOTES THEOR, V343, P35, DOI 10.1016/j.entcs.2019.04.009
   Marín-Morales J, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20185163
   Meehan M, 2002, ACM T GRAPHIC, V21, P645, DOI 10.1145/566570.566630
   Mehrabian A., 1974, An Approach to Environmental Psychology, DOI DOI 10.1016/J.ELERAP.2013.07.001
   Moghimi S, 2012, J NEURAL ENG, V9, DOI 10.1088/1741-2560/9/2/026022
   Naseer N, 2015, FRONT HUM NEUROSCI, V9, DOI 10.3389/fnhum.2015.00003
   Nasoz F., 2004, Cognition, Technology & Work, V6, P4, DOI 10.1007/s10111-003-0143-x
   PAN J, 1985, IEEE T BIO-MED ENG, V32, P230, DOI 10.1109/TBME.1985.325532
   Pecchia L, 2018, HEALTHC TECHNOL LETT, V5, P94, DOI 10.1049/htl.2017.0090
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Peirce J, 2019, BEHAV RES METHODS, V51, P195, DOI 10.3758/s13428-018-01193-y
   PLUTCHIK R, 1982, SOC SCI INFORM, V21, P529, DOI 10.1177/053901882021004003
   Posada-Quintero HF, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20020479
   Posada-Quintero HF, 2016, ANN BIOMED ENG, V44, P3124, DOI 10.1007/s10439-016-1606-6
   Ramzan N., 2016, Electronic Imaging, V2016, P1, DOI [10.2352/ISSN.2470-1173.2016.16.HVEI-129, DOI 10.2352/ISSN.2470-1173.2016.16.HVEI-129]
   Refaeilzadeh P., 2016, ENCY DATABASE SYSTEM, P1, DOI [DOI 10.1007/978-1-4899-7993-3_565-2, 10.1007/978-1-4899-7993-3]
   Reisenzein R, 2019, TOP COGN SCI, V11, P50, DOI 10.1111/tops.12292
   Renard Y, 2010, PRESENCE-VIRTUAL AUG, V19, P35, DOI 10.1162/pres.19.1.35
   Rigas G, 2007, LECT NOTES ARTIF INT, V4511, P314
   Rodríguez A, 2015, EXPERT SYST APPL, V42, P1699, DOI 10.1016/j.eswa.2014.10.006
   Rose T, 2018, APPL ERGON, V69, P153, DOI 10.1016/j.apergo.2018.01.009
   Roseman I.J., 2001, Appraisal processes in emotion: Theory, method, research, P3
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Salahuddin L, 2007, P ANN INT IEEE EMBS, P4656, DOI 10.1109/IEMBS.2007.4353378
   Samala RK, 2020, PROC SPIE, V11314, DOI 10.1117/12.2549313
   Schaaff K, 2013, INT CONF AFFECT, P362, DOI 10.1109/ACII.2013.66
   Schaefer A, 2010, COGNITION EMOTION, V24, P1153, DOI 10.1080/02699930903274322
   Schmidt P, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19194079
   Shanker M, 1996, OMEGA-INT J MANAGE S, V24, P385, DOI 10.1016/0305-0483(96)00010-2
   Sharma Mrigank, 2020, Proceeding of the International Conference on Computer Networks, Big Data and IoT (ICCBI - 2019). Lecture Notes on Data Engineering and Communications Technologies (LNDECT 49), P389, DOI 10.1007/978-3-030-43192-1_45
   SMITH CA, 1989, J PERS SOC PSYCHOL, V56, P339, DOI 10.1037/0022-3514.56.3.339
   SMITH CA, 1985, J PERS SOC PSYCHOL, V48, P813, DOI 10.1037/0022-3514.48.4.813
   Soleymani M, 2012, IEEE T AFFECT COMPUT, V3, P42, DOI 10.1109/T-AFFC.2011.25
   Stasienko A., 2016, POSTEPY REHABILITACJ, V30, P67, DOI DOI 10.1515/REHAB-2015-0056
   Szwoch W, 2013, C HUM SYST INTERACT, P556, DOI 10.1109/HSI.2013.6577880
   Trojan J, 2014, BEHAV RES METHODS, V46, P634, DOI 10.3758/s13428-013-0412-4
   Uhrig MK, 2016, FRONT PSYCHOL, V7, DOI 10.3389/fpsyg.2016.00180
   Vapnik V, 1998, NONLINEAR MODELING, P55
   Varandas R, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22114010
   Wong TT, 2015, PATTERN RECOGN, V48, P2839, DOI 10.1016/j.patcog.2015.03.009
   Yuan Gu, 2010, 2010 8th IEEE International Conference on Industrial Informatics (INDIN 2010), P61, DOI 10.1109/INDIN.2010.5549464
NR 84
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAY 6
PY 2024
VL 28
IS 2
AR 107
DI 10.1007/s10055-024-00989-y
PG 23
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA PN5V4
UT WOS:001214779600001
OA hybrid
DA 2024-08-05
ER

PT J
AU Sun, YQ
   Yuan, TR
   Wang, YM
   Sun, QP
   Hou, ZW
   Du, J
AF Sun, Yuqing
   Yuan, Tianran
   Wang, Yimin
   Sun, Quanping
   Hou, Zhiwei
   Du, Juan
TI Augmented reality presentation system of skeleton image based on
   biomedical features
SO VIRTUAL REALITY
LA English
DT Article
DE Construction of human skeleton library; Bone feature recognition and
   matching; 2D-3D bone mapping; Augmented reality presentation
ID RECONSTRUCTION; NETWORK; ALGORITHM; ULTRA; ART
AB Aimed at limitations in the description and expression of three-dimensional (3D) physical information in two-dimentsional (2D) medical images, feature extraction and matching method based on the biomedical characteristics of skeletons is employed in this paper to map the 2D images of skeletons into a 3D digital model. Augmented reality technique is used to realize the interactive presentation of skeleton models. Main contents of this paper include: Firstly, a three-step reconstruction method is used to process the bone CT image data to obtain its three-dimensional surface model, and the corresponding 2D-3D bone library is established based on the identification index of the 2D image and the 3D model; then, a fast and accurate feature extraction and matching algorithm is developed to realize the recognition, extraction, and matching of 2D skeletal features, and determine the corresponding 3D skeleton model according to the matching result. Finally, based on the augmented reality technique, an interactive immersive presentation system is designed to achieve visual effects of the virtual human bone model superimposed and rendered in the world scenes, which improves the effectiveness of information expression and transmission, as well as the user's immersion and embodied experience.
C1 [Sun, Yuqing; Wang, Yimin; Sun, Quanping; Hou, Zhiwei] Huaiyin Inst Technol, Fac Mech & Mat Engn, Huaian 223003, Jiangsu, Peoples R China.
   [Yuan, Tianran] Guilin Univ Elect Technol, Sch Comp Sci & Informat Secur, Guilin 541004, Guangxi, Peoples R China.
   [Wang, Yimin; Sun, Quanping; Hou, Zhiwei] Huaiyin Inst Technol, Jiangsu Key Lab Adv Mfg Technol, Huaian 223003, Jiangsu, Peoples R China.
   [Du, Juan] Jiangsu Coll Nursing, Sch Med Technol, Huaian 223005, Jiangsu, Peoples R China.
C3 Huaiyin Institute of Technology; Guilin University of Electronic
   Technology; Huaiyin Institute of Technology
RP Yuan, TR (corresponding author), Guilin Univ Elect Technol, Sch Comp Sci & Informat Secur, Guilin 541004, Guangxi, Peoples R China.
EM ytianran@qq.com
RI Sun, Yuqing/IVH-6857-2023
OI Sun, Yuqing/0009-0005-8748-4064
FU Research on bionic mechanical deformation prediction method for fusion
   graph convolution of orthodontic system
FX No Statement Available
CR Al-Ansi A.M., 2023, Soc. Sci. Humanit. Open, V8, DOI [10.1016/j.ssaho.2023.100532, DOI 10.1016/J.SSAHO.2023.100532]
   Alcantarilla PF, 2012, LECT NOTES COMPUT SC, V7577, P214, DOI 10.1007/978-3-642-33783-3_16
   ANDERSEN AH, 1984, ULTRASONIC IMAGING, V6, P81, DOI 10.1016/0161-7346(84)90008-7
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Balochian S, 2022, MULTIMED TOOLS APPL, DOI 10.1007/s11042-022-12011-1
   Barroso-Laguna A, 2019, Arxiv, DOI arXiv:1904.00889
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bian JW, 2017, PROC CVPR IEEE, P2828, DOI 10.1109/CVPR.2017.302
   Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56
   Canny J., 2009, CANNY EDGE DETECTION
   Chen Z, 2023, HoloLens augmented reality-based navigational control study of brain hematoma removal robot, DOI [10.27063/d.cnki.ghlgu.2023.000192, DOI 10.27063/D.CNKI.GHLGU.2023.000192]
   Cheng B., 2017, Sch Mech Power Eng North Univ China, V39, P89
   [成科扬 Cheng Keyang], 2020, [计算机研究与发展, Journal of Computer Research and Development], V57, P1208
   Choi K, 2022, EXPERT SYST APPL, V209, DOI 10.1016/j.eswa.2022.118072
   Chun I, 2023, IEEE T PATTERN ANAL, V45, P4915, DOI 10.1109/TPAMI.2020.3012955
   Chun IY, 2018, PROCEEDINGS 2018 IEEE 13TH IMAGE, VIDEO, AND MULTIDIMENSIONAL SIGNAL PROCESSING WORKSHOP (IVMSP)
   Ciobotaru A, 2023, BIOENGINEERING-BASEL, V10, DOI 10.3390/bioengineering10121419
   DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060
   Dusmanu M, 2019, PROC CVPR IEEE, P8084, DOI 10.1109/CVPR.2019.00828
   Ebel P, 2019, IEEE I CONF COMP VIS, P253, DOI 10.1109/ICCV.2019.00034
   Ghani MU, 2021, IEEE T COMPUT IMAG, V7, P297, DOI 10.1109/TCI.2021.3062986
   GORDON R, 1970, J THEOR BIOL, V29, P471, DOI 10.1016/0022-5193(70)90109-8
   Gu X., 2022, Laser J, V43, P82, DOI [10.14016/j.cnki.jgzz.2022.03.082, DOI 10.14016/J.CNKI.JGZZ.2022.03.082]
   Gupta H, 2018, IEEE T MED IMAGING, V37, P1440, DOI 10.1109/TMI.2018.2832656
   Han XF, 2021, IEEE T PATTERN ANAL, V43, P1578, DOI 10.1109/TPAMI.2019.2954885
   Harris C, 1988, A combined corner and edge detector, P147, DOI [10.5244/C.2.23.23.1-23.6, DOI 10.5244/C.2.23]
   He J, 2019, IEEE T MED IMAGING, V38, P371, DOI 10.1109/TMI.2018.2865202
   Hendriksen AA, 2020, IEEE T COMPUT IMAG, V6, P1320, DOI 10.1109/TCI.2020.3019647
   Hu DL, 2022, IEEE J BIOMED HEALTH, V26, P5551, DOI 10.1109/JBHI.2022.3201232
   Hu DL, 2022, IEEE T MED IMAGING, V41, P1778, DOI 10.1109/TMI.2022.3148110
   Huang Z., 2023, J China Higher Med Edu, V9, P1
   Husain SS, 2019, IEEE T IMAGE PROCESS, V28, P5201, DOI 10.1109/TIP.2019.2917234
   Lagerwerf MJ, 2020, Arxiv, DOI arXiv:2007.01636
   Jakubovic A, 2018, ELMAR PROC, P83, DOI 10.23919/ELMAR.2018.8534641
   Jia Z., 2023, J Contin Med Educ, V37, P97
   Jiang B, 2022, PATTERN RECOGN, V121, DOI 10.1016/j.patcog.2021.108167
   Jing JF, 2023, IEEE T PATTERN ANAL, V45, P4694, DOI 10.1109/TPAMI.2022.3201185
   Kao JY, 2019, IEEE IMAGE PROC, P2025, DOI 10.1109/icip.2019.8803186
   Ke Y, 2004, PROC CVPR IEEE, P506
   Kerdvibulvech C, 2014, LECT NOTES COMPUT SC, V8621, P282, DOI 10.1007/978-3-662-44415-3_29
   Kerdvibulvech C, 2014, LECT NOTES COMPUT SC, V8563, P118, DOI 10.1007/978-3-319-08849-5_12
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li M., 2022, J Chin J Aesthet Plast Surg, V33, P504
   Li YS, 2019, IEEE T MED IMAGING, V38, P2469, DOI 10.1109/TMI.2019.2910760
   Li Z., 2009, J Comput Eng Des, V30, P1432, DOI [10.16208/j.issn1000-7024.2009.06.019, DOI 10.16208/J.ISSN1000-7024.2009.06.019]
   Lin J, 2022, Research on improved region growth method for CT image segmentation and 3D reconstruction, DOI [10.27724/d.cnki.gnmgk.2022.000492, DOI 10.27724/D.CNKI.GNMGK.2022.000492]
   Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005
   Liu J, 2018, IEEE T CIRC SYST VID, V28, P1232, DOI 10.1109/TCSVT.2016.2643009
   Liu TC, 2021, LASER OPTOELECTRON P, V58, DOI 10.3788/LOP202158.0810004
   Lorensen H. E., 1987, Proc. SIGGRAPH, V21, P163, DOI 10.1145/37401.37422
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu Y, 2012, PHYS MED BIOL, V57, P173, DOI 10.1088/0031-9155/57/1/173
   Luo ZX, 2019, Arxiv, DOI arXiv:1904.04084
   Ma X., 2017, Henan Polytech Univ, V36, P69, DOI [10.16186/j.cnki.1673-9787.2017.06.011, DOI 10.16186/J.CNKI.1673-9787.2017.06.011]
   MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020
   Mishchuk A, 2017, ADV NEUR IN, V30
   Yi KM, 2016, Arxiv, DOI arXiv:1603.09114
   Moravec Hans P, 1980, Obstacle avoidance and navigation in the real world by a seeing robot rover
   Nie WZ, 2021, IEEE T IMAGE PROCESS, V30, P4371, DOI 10.1109/TIP.2021.3071687
   Noh H, 2017, IEEE I CONF COMP VIS, P3476, DOI 10.1109/ICCV.2017.374
   Ono Y, 2018, ADV NEUR IN, V31
   Osher S., 2003, Level Set Methods and Dynamic Implicit Surfaces
   OuYang J., 2017, Fire Sci Technol, V36, P1613
   Pan XC, 2004, PHYS MED BIOL, V49, P4349, DOI 10.1088/0031-9155/49/18/011
   Qin Yang, 2011, Journal of Computer Aided Design & Computer Graphics, V23, P763
   Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023_34
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sarlin PE, 2019, PROC CVPR IEEE, P12708, DOI 10.1109/CVPR.2019.01300
   Shen XL, 2019, PROC CVPR IEEE, P8124, DOI 10.1109/CVPR.2019.00832
   Sobel I, 1990, An Isotropic 3 x 3 image gradient operator
   Su T, 2022, PHYS MED BIOL, V67, DOI 10.1088/1361-6560/ac3eae
   Sun W., 2007, J Comput Aided Des Graph, V7, P947
   Talha SMU, 2021, BIOMED SIGNAL PROCES, V64, DOI 10.1016/j.bspc.2020.102323
   Tan ZX, 2023, APPL MATH MODEL, V124, P518, DOI 10.1016/j.apm.2023.08.002
   Tian YR, 2019, PROC CVPR IEEE, P11008, DOI 10.1109/CVPR.2019.01127
   Tian YR, 2017, PROC CVPR IEEE, P6128, DOI 10.1109/CVPR.2017.649
   van Overveld K, 2004, VISUAL COMPUT, V20, P362, DOI 10.1007/s00371-002-0197-4
   Venkatakrishnan S, 2013, IEEE GLOB CONF SIG, P945, DOI 10.1109/GlobalSIP.2013.6737048
   Verdie Y, 2015, Arxiv, DOI arXiv:1411.4568
   Wang J., 2023, J Mech Electr Eng Technol, V52, P6
   Wang L., 2021, J Laser Optoelectron Prog, V58, P339
   [王庆辉 Wang Qinghui], 2023, [机器人, Robot], V45, P546
   Wang RZ, 2023, IEEE T PATTERN ANAL, V45, P6984, DOI 10.1109/TPAMI.2020.3005590
   Wang RZ, 2019, IEEE I CONF COMP VIS, P3056, DOI 10.1109/ICCV.2019.00315
   Wei YZ, 2021, J MEAS ENG, V9, P156, DOI 10.21595/jme.2021.22023
   Wu DF, 2017, IEEE T MED IMAGING, V36, P2479, DOI 10.1109/TMI.2017.2753138
   Wu F., 2022, Research on the Progress of Deep Reconstruction Algorithm for CT Imaging, V27, P387, DOI [10.13505/j.1007-1482.2022.27.04.007, DOI 10.13505/J.1007-1482.2022.27.04.007]
   [吴禄慎 Wu Lushen], 2021, [计算机工程, Computer Engineering], V47, P246
   Xia X., 2023, J Opt Precis Eng, V31, P3630, DOI [10.37188/OPE.20233124.3630, DOI 10.37188/OPE.20233124.3630]
   Xiang JX, 2021, IEEE T MED IMAGING, V40, P1329, DOI 10.1109/TMI.2021.3054167
   Xiong FS, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-17818-4
   Xu Q, 2012, IEEE T MED IMAGING, V31, P1682, DOI 10.1109/TMI.2012.2195669
   Yan L., 2019, Microelectron Comput, V36, P64, DOI [10.19304/j.cnki.issn1000-7180.2019.12.013, DOI 10.19304/J.CNKI.ISSN1000-7180.2019.12.013]
   Ye Q, 2018, INT J APPL PATTERN R, V5, P261
   Chun IY, 2019, Arxiv, DOI arXiv:1908.01287
   Zang GM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1940, DOI 10.1109/ICCV48922.2021.00197
   Zeng GL, 2010, MEDICAL IMAGE RECONSTRUCTION: A CONCEPTUAL TUTORIAL, P1, DOI 10.1007/978-3-642-05368-9
   Zhang HM, 2021, IEEE T MED IMAGING, V40, P621, DOI 10.1109/TMI.2020.3033541
   Zhang YB, 2018, IEEE T MED IMAGING, V37, P1370, DOI 10.1109/TMI.2018.2823083
   Zhao W., 2023, Adv Laser Optoelectron, V60, P90
   Zheng XH, 2018, IEEE T MED IMAGING, V37, P1498, DOI 10.1109/TMI.2018.2832007
   Zhou R., 2021, Central China Normal Univ, DOI [10.27159/d.cnki.ghzsu.2021.001305, DOI 10.27159/D.CNKI.GHZSU.2021.001305]
   Zhu B, 2018, NATURE, V555, P487, DOI 10.1038/nature25988
NR 103
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD APR 16
PY 2024
VL 28
IS 2
AR 98
DI 10.1007/s10055-024-00976-3
PG 24
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA NY3J1
UT WOS:001203970900002
OA hybrid
DA 2024-08-05
ER

PT J
AU Lu, HP
   Chang, YC
   Chen, CS
AF Lu, Hsi-Peng
   Chang, Yi-Chin
   Chen, Chiao-Shan
TI How to trigger user's willingness to participate in the metaverse? An
   exploration of the significant factors of the metaverse
SO VIRTUAL REALITY
LA English
DT Article
DE Metaverse; Self-efficacy; Avatars; Decentralized value exchange;
   Immersive experience; 3D interactivity
ID EXPECTATION-CONFIRMATION; VIRTUAL-REALITY; PERCEIVED EASE; GAMES;
   ACCEPTANCE; DETERMINANTS; CONTINUANCE; PREFERENCE; ADOPTION; MMORPG
AB The issue of the metaverse has been widely discussed. The purpose of this research is to investigate users' willingness to participate in the metaverse. This study used the self-efficacy theory and Theory of Reasoned Action (TRA) to explore their willingness to attend the metaverse. Furthermore, the study explored how the basic concepts of the metaverse (Avatars, Decentralized Value Exchange, and Immersive Experience) influence the users' attitudes (Presence in Second-Life, 3D Interactivity, and Play-to-Earn) toward and willingness with respect to participating in the metaverse. A total of 150 valid experts' responses were collected through an online questionnaire and analyzed through structural equation modeling. The results revealed that Presence in Second-Life and Play-to-Earn significantly impact the respondents' willingness to participate in the metaverse. Moreover, 3D Interactivity affected their participation to willingness through Presence in Second-Life and Play-to-Earn.
C1 [Lu, Hsi-Peng; Chang, Yi-Chin] Natl Taiwan Univ Sci & Technol, Taipei, Taiwan.
   [Chen, Chiao-Shan] Natl Taipei Univ Nursing & Hlth Sci, Taipei, Taiwan.
C3 National Taiwan University of Science & Technology; National Taipei
   University of Nursing & Health Science (NTUNHS)
RP Chang, YC (corresponding author), Natl Taiwan Univ Sci & Technol, Taipei, Taiwan.
EM lu@mail.ntust.edu.tw; annychang1994@gmail.com; chiaoshan@ntunhs.edu.tw
FU National Science and Technology Council
FX No Statement Available
CR Agarwal R, 2005, MIS QUART, V29, P381
   Al-Adwan AS, 2023, EDUC INF TECHNOL, V28, P15381, DOI 10.1007/s10639-023-11816-3
   Allcoat D, 2021, J EDUC COMPUT RES, V59, P795, DOI 10.1177/0735633120985120
   Baltar F, 2012, INTERNET RES, V22, P57, DOI 10.1108/10662241211199960
   BANDURA A, 1977, PSYCHOL REV, V84, P191, DOI 10.1037/0033-295X.84.2.191
   Bandura A., 1994, Encyclopedia of Human Behavior, V4, P71
   Bhattacherjee A, 2001, MIS QUART, V25, P351, DOI 10.2307/3250921
   Brown SA, 2014, MIS QUART, V38, P729, DOI 10.25300/MISQ/2014/38.3.05
   Chen A, 2020, HARVARD REV PSYCHIAT, V28, P107, DOI 10.1097/HRP.0000000000000247
   Chen CS, 2018, COMPUT HUM BEHAV, V89, P182, DOI 10.1016/j.chb.2018.07.023
   Chmielewski D, 2021, Reuters
   Choi HS, 2017, INT J INFORM MANAGE, V37, P1519, DOI 10.1016/j.ijinfomgt.2016.04.017
   Davis A, 2009, J ASSOC INF SYST, V10, P90, DOI 10.17705/1jais.00183
   DAVIS FD, 1989, MIS QUART, V13, P319, DOI 10.2307/249008
   de Gortari ABO, 2018, TELEMAT INFORM, V35, P382, DOI 10.1016/j.tele.2017.12.015
   Diemer J, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00026
   Dionisio JDN, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2480741.2480751
   Duan HH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P153, DOI 10.1145/3474085.3479238
   Emergen Research, 2022, Metaverse Market Size | Share | Trend | Revenue Report by 2030
   EPIC Games, 2021, Ariana Grande Steps into the Metaverse as the Headliner for Fortnite's Rift Tour
   Etikan I., 2016, American Journal of Theoretical and Applied Statistics, V5, P1, DOI [DOI 10.11648/J.AJTAS.20160501.11, 10.11648/j.ajtas.20160501.11]
   FORNELL C, 1981, J MARKETING RES, V18, P39, DOI 10.2307/3151312
   Fussell SG, 2022, VIRTUAL REAL-LONDON, V26, P249, DOI 10.1007/s10055-021-00554-x
   Díaz RG, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20205936
   Gómez-García M, 2018, J HUM SPORT EXERC, V13, pS189, DOI 10.14198/jhse.2018.13.Proc2.03
   Guitton MJ., 2019, Int J Intell Secur Public Affairs, V21, P117, DOI [10.1080/23800992.2019.1649122, DOI 10.1080/23800992.2019.1649122]
   Hair J. F., 2009, Multivariate data analysis
   Hammady R, 2020, ACM J COMPUT CULT HE, V13, DOI 10.1145/3359590
   Han SL, 2020, J BUS RES, V118, P311, DOI 10.1016/j.jbusres.2020.06.056
   Hashemian AM, 2022, IEEE T VIS COMPUT GR, V28, P1792, DOI 10.1109/TVCG.2020.3025084
   Huang CM, 2021, J ADV NURS, V77, P3784, DOI 10.1111/jan.14895
   Ifinedo P, 2018, BEHAV INFORM TECHNOL, V37, P381, DOI 10.1080/0144929X.2018.1436594
   Jang Y, 2019, TELEMAT INFORM, V42, DOI 10.1016/j.tele.2019.101239
   Jin W, 2017, INTERNET RES, V27, P408, DOI 10.1108/IntR-04-2016-0091
   Jung HS, 2014, J BUS RES, V67, P2171, DOI 10.1016/j.jbusres.2014.04.027
   Kang J, 2019, J SUPERCOMPUT, V75, P4529, DOI 10.1007/s11227-016-1788-6
   Karim S., 2021, SSRN Electron J, DOI [10.2139/ssrn.3967960, DOI 10.2139/SSRN.3967960]
   Kim D, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11030367
   Kim J, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11136170
   Kim MJ, 2020, TELEMAT INFORM, V49, DOI 10.1016/j.tele.2020.101349
   Kim S, 2022, J RES INTERACT MARK, V16, P82, DOI 10.1108/JRIM-01-2021-0020
   Lau KW, 2019, VIRTUAL REAL-LONDON, V23, P255, DOI 10.1007/s10055-018-0362-3
   Lee H, 2020, INFORM MANAGE-AMSTER, V57, DOI 10.1016/j.im.2019.103229
   Lee SG, 2011, SERV BUS, V5, P155, DOI 10.1007/s11628-011-0108-8
   Li KN, 2021, MICROPROCESS MICROSY, V83, DOI 10.1016/j.micpro.2021.103989
   Lohre R, 2020, JAMA NETW OPEN, V3, DOI 10.1001/jamanetworkopen.2020.31217
   Long Y, 2020, J NEUROENG REHABIL, V17, DOI 10.1186/s12984-020-00783-2
   Lopez JM, 2021, ORGAN BEHAV HUM DEC, V164, P52, DOI 10.1016/j.obhdp.2021.01.002
   Lu YL, 2020, J EDUC COMPUT RES, V57, P1879, DOI 10.1177/0735633118820684
   Macakova V, 2022, STUD HIGH EDUC, V47, P259, DOI 10.1080/03075079.2020.1739017
   Malik G, 2019, INF TECHNOL TOUR, V21, P461, DOI 10.1007/s40558-019-00152-3
   Marko K, 2021, Diginomica
   Mikalef Konstantinos, 2012, Entertainment Computing, 11th International Conference (ICEC - 2012). Proceedings, P553, DOI 10.1007/978-3-642-33542-6_73
   Mondellini M, 2021, CYBERPSYCH BEH SOC N, V24, P767, DOI 10.1089/cyber.2020.0223
   Morschheuser B, 2017, COMPUT HUM BEHAV, V77, P169, DOI 10.1016/j.chb.2017.08.026
   Newbery Emma., 2021, The Motley Fool
   Ng LL, 2022, ASIA-PAC EDUC RES, V31, P369, DOI 10.1007/s40299-021-00578-6
   Ofelia H, 2017, Metaverse Digital Identity White Paper
   Oriti D, 2023, VIRTUAL REAL-LONDON, V27, P3259, DOI 10.1007/s10055-021-00585-4
   Owens D, 2011, DATA BASE ADV INF SY, V42, P74, DOI 10.1145/1952712.1952717
   Park SM, 2022, IEEE ACCESS, V10, P4209, DOI 10.1109/ACCESS.2021.3140175
   Park S, 2022, SUSTAINABILITY-BASEL, V14, DOI 10.3390/su14031361
   Pernice IGA, 2021, INTERNET POLICY REV, V10, DOI 10.14763/2021.2.1561
   Rad MS, 2018, UNIVERSAL ACCESS INF, V17, P361, DOI 10.1007/s10209-017-0534-z
   Rauschnabel PA, 2017, COMPUT HUM BEHAV, V76, P276, DOI 10.1016/j.chb.2017.07.030
   Scholes L, 2022, LEARN MEDIA TECHNOL, V47, P163, DOI 10.1080/17439884.2021.1936017
   Schwab K, 2021, Fast Company
   Singh N, 2017, INT J BANK MARK, V35, P944, DOI 10.1108/IJBM-06-2016-0086
   Siyaev A, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21062066
   Smart J, 2022, Metaverse Roadmap Overview, P1
   Song H, 2021, AUTOMAT CONSTR, V122, DOI 10.1016/j.autcon.2020.103506
   Sourmelis T, 2017, COMPUT HUM BEHAV, V67, P41, DOI 10.1016/j.chb.2016.10.020
   Syed-Abdul S, 2019, BMC GERIATR, V19, DOI 10.1186/s12877-019-1218-8
   Taherdoost H, 2022, COMPUTERS, V11, DOI 10.3390/computers11020024
   Tornatzky L.G., 1990, The Processes of Technological Innovation
   Ustun AB, 2023, VIRTUAL REAL-LONDON, V27, P1063, DOI 10.1007/s10055-022-00717-4
   Venkatesh V, 2000, INFORM SYST RES, V11, P342, DOI 10.1287/isre.11.4.342.11872
   Verhulst I, 2021, COMPUT HUM BEHAV, V125, DOI 10.1016/j.chb.2021.106951
   Wang HY, 2008, BRIT J EDUC TECHNOL, V39, P787, DOI 10.1111/j.1467-8535.2007.00773.x
   Wiggers LCW, 2005, PREV MED, V41, P667, DOI 10.1016/j.ypmed.2004.12.009
   Xie T, 2022, VIRTUAL REAL-LONDON, V26, P1725, DOI 10.1007/s10055-022-00656-0
   Xu JY, 2022, TECHNOL SOC, V70, DOI 10.1016/j.techsoc.2022.102010
   Yang H, 2021, ONLINE INFORM REV, V45, P422, DOI 10.1108/OIR-02-2020-0058
   Yeh TM, 2019, INT J ENV RES PUB HE, V16, DOI 10.3390/ijerph16030333
   Zhang MS, 2018, INT J EMERG TECHNOL, V13, P138, DOI 10.3991/ijet.v13i01.7773
NR 85
TC 0
Z9 0
U1 12
U2 12
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR 22
PY 2024
VL 28
IS 2
AR 83
DI 10.1007/s10055-024-00983-4
PG 16
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA LS0U0
UT WOS:001188681900002
OA hybrid
DA 2024-08-05
ER

PT J
AU Paulsen, L
   Dau, S
   Davidsen, J
AF Paulsen, Lucas
   Dau, Susanne
   Davidsen, Jacob
TI Designing for collaborative learning in immersive virtual reality: a
   systematic literature review
SO VIRTUAL REALITY
LA English
DT Article
DE Collaborative learning; Virtual reality; 360VR; Collaborative training;
   Pedagogy
ID EDUCATION; ENVIRONMENTS; TECHNOLOGY; VIDEOS; VR
AB Immersive learning technologies such as virtual reality have long been deemed as the next generation of digital learning environments. There is a limited number of studies addressing how immersive technologies can be designed, applied, and studied in collaborative learning settings. This paper presents a systematic review of empirical studies reporting on use of immersive virtual reality in collaborative learning within educational and professional learning settings. 11 studies have been grouped and coded in a textual narrative synthesis, outlining the pedagogical concepts behind the learning design, as well as the design of virtual reality environments and the collaborative learning activities in which the technology is employed. The results suggest that collaborative learning in virtual reality can currently be conceptualised as a shared experience in an immersive, virtually mediated space, where there is a shared goal/problem which learners must attend to collaboratively. This conceptualisation implies a need to design technologies, environments, and activities that support participation and social interaction, fostering collaborative learning processes. Based on the outlined conceptualisation, we present a series of recommendations for designing for collaborative learning in immersive virtual reality. The paper concludes that collaborative learning in virtual reality creates a practice- and reflection space, where learning is perceived as engaging, without the risk of interfering with actual practices. Current designs however struggle with usability, realism, and facilitating social interaction. The paper further identifies a need for future research into what happens within virtual reality, rather than only looking at post-virtual reality evaluations.
C1 [Paulsen, Lucas; Davidsen, Jacob] Aalborg Univ, Aalborg, Denmark.
   [Dau, Susanne] Univ Coll Northern Denmark, Aalborg, Denmark.
C3 Aalborg University
RP Paulsen, L (corresponding author), Aalborg Univ, Aalborg, Denmark.
EM lupa@ikp.aau.dk
RI Dau, Susanne/ITT-6252-2023
OI Dau, Susanne/0000-0002-5798-0519; Paulsen, Lucas/0000-0002-1504-8299;
   Davidsen, Jacob Gorm/0000-0002-5240-9452
FU Aalborg University
FX No Statement Available
CR Alamäki A, 2021, J INF TECHNOL EDUC-R, V20, P309, DOI 10.28945/4816
   Ali AA, 2019, IEEE T LEARN TECHNOL, V12, P321, DOI 10.1109/TLT.2019.2926727
   [Anonymous], 1999, Collaborative Learning: Cognitive and Computational Approaches
   [Anonymous], ZOTERO YOUR PERSONAL
   Azuma RT, 1997, PRESENCE-VIRTUAL AUG, V6, P355, DOI 10.1162/pres.1997.6.4.355
   Barnett-Page E, 2009, BMC MED RES METHODOL, V9, DOI 10.1186/1471-2288-9-59
   Barricelli BR, 2019, IEEE ACCESS, V7, P167653, DOI 10.1109/ACCESS.2019.2953499
   Belur J, 2021, SOCIOL METHOD RES, V50, P837, DOI 10.1177/0049124118799372
   Blascovich J, 2002, PSYCHOL INQ, V13, P103, DOI 10.1207/S15327965PLI1302_01
   BUCKLAND M, 1994, J AM SOC INFORM SCI, V45, P12, DOI 10.1002/(SICI)1097-4571(199401)45:1<12::AID-ASI2>3.0.CO;2-L
   Chang E, 2020, INT J HUM-COMPUT INT, V36, P1658, DOI 10.1080/10447318.2020.1778351
   Chen Jiadong, 2022, Cross-Cultural Design. Applications in Learning, Arts, Cultural Heritage, Creative Industries, and Virtual Reality: 14th International Conference, CCD 2022, Held as Part of the 24th HCI International Conference, HCII 2022, Proceedings. Lecture Notes in Computer Science (13312), P14, DOI 10.1007/978-3-031-06047-2_2
   Chheang V, 2020, INT J COMPUT ASS RAD, V15, P2109, DOI 10.1007/s11548-020-02276-y
   Cress U., 2021, International Handbook of Computer-Supported Collaborative Learning, P3, DOI 10.1007/978-3-030-65291-3_1
   Dau S, 2014, Crossing boundaries for learning: through technology and human efforts, P191
   Davidsen J., 2022, J Problem Based Learn High Educ, V10, P101, DOI [10.54337/ojs.jpblhe.v10i1.7097, DOI 10.54337/OJS.JPBLHE.V10I1.7097]
   Dede C, 1996, INTERNATIONAL CONFERENCE ON THE LEARNING SCIENCES, 1996, P22
   Dohn NB, 2019, Designing for Situated Knowledge Transformation
   Enyedy N., 2021, International Handbook of Computer-Supported Collaborative Learning, P389, DOI [DOI 10.1007/978-3-030-65291-3_21, 10.1007/978-3-030-65291-3_21]
   Fawns T., 2022, Postdigital Science and Education, V4, P711, DOI DOI 10.1007/S42438-022-00302-7
   Forland EP, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P320, DOI 10.1109/VRW52623.2021.00064
   Fortman J, 2023, INT J COMP-SUPP COLL, V18, P145, DOI 10.1007/s11412-023-09404-1
   Gong Liang, 2020, Procedia CIRP, V93, P1259, DOI 10.1016/j.procir.2020.04.036
   Goodwin C., 2003, Discourse, the body, and identity, P19, DOI DOI 10.1057/9781403918543
   Gough D., 2017, INTRO SYSTEMATIC REV
   Helle L, 2012, INSTR SCI, V40, P737, DOI 10.1007/s11251-012-9216-7
   Hindmarsh J, 2006, SOCIOL REV, V54, P795, DOI 10.1111/j.1467-954X.2006.00672.x
   Hmelo-Silver C., 2013, INT HDB COLLABORATIV
   HTC, VIVE. VR Headsets, Games, and Metaverse Life
   Huang X, 2022, 2022 8 INT C IMMERSI, P1, DOI [10.23919/iLRN55037.2022.9815990, DOI 10.23919/ILRN55037.2022.9815990]
   Jackson R.L., 1999, COLLABORATION LEARNI
   Jrävelä S, 2023, INT J COMP-SUPP COLL, V18, P457, DOI 10.1007/s11412-023-09415-y
   Jensen L, 2018, EDUC INF TECHNOL, V23, P1515, DOI 10.1007/s10639-017-9676-0
   Kolmos A., 2009, Research on PBL practice in engineering education, P9
   Kosko KW, 2021, J TEACH EDUC, V72, P284, DOI 10.1177/0022487120939544
   Lave J., 1991, SITUATED LEARNING LE
   Lerner D, 2020, JMIR SERIOUS GAMES, V8, DOI 10.2196/18822
   Liaw SY, 2018, NURS EDUC TODAY, V65, P136, DOI 10.1016/j.nedt.2018.01.006
   Lucas PJ, 2007, BMC MED RES METHODOL, V7, DOI 10.1186/1471-2288-7-4
   Luff P, 2003, HUM-COMPUT INTERACT, V18, P51, DOI 10.1207/S15327051HCI1812_3
   Makransky G, 2023, EDUC PSYCHOL REV, V35, DOI 10.1007/s10648-023-09822-5
   Makransky G, 2019, LEARN INSTR, V60, P225, DOI 10.1016/j.learninstruc.2017.12.007
   meta, Meta quest pro: our most advanced new vr headset
   MILGRAM P, 1994, IEICE T INF SYST, VE77D, P1321
   Mystakidis S, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11052412
   Norman D. A., 1999, Interactions, V6, P38, DOI 10.1145/301153.301168
   Özacar K, 2023, COMPUT GRAPH-UK, V113, P1, DOI 10.1016/j.cag.2023.04.008
   Page MJ, 2021, SYST REV-LONDON, V10, DOI 10.1186/s13643-021-01626-4
   Paulsen L., 2023, Tidsskriftet Laring Og Medier (LOM), V15, P20, DOI [10.7146/lom.v15i27.133760, DOI 10.7146/LOM.V15I27.133760]
   Paulsen L, 2022, P 15 INT C COMPUTER
   Pieterse AD, 2023, VIRTUAL REAL-LONDON, V27, P1381, DOI 10.1007/s10055-022-00731-6
   Pirker J, 2021, IEEE COMPUT GRAPH, V41, P76, DOI 10.1109/MCG.2021.3067999
   Prasolova-Forland E, 2018, Smart Innov. Syst. Technol, V70, P191, DOI [10.1007/978-3-319-59454-5_7, DOI 10.1007/978-3-319-59454-5_7]
   Radianti J, 2020, COMPUT EDUC, V147, DOI 10.1016/j.compedu.2019.103778
   Roschelle J., 1995, Computer Supported Collaborative Learning. Proceedings NATO Advanced Research Workshop, P69
   Rosendahl P, 2024, EDUC INF TECHNOL, V29, P1319, DOI 10.1007/s10639-022-11549-9
   Scavarelli A, 2021, VIRTUAL REAL-LONDON, V25, P257, DOI 10.1007/s10055-020-00444-8
   Schild J, 2021, SEGAH IEEE INT C SER, DOI [10.1109/SEGAH52098.2021, DOI 10.1109/SEGAH52098.2021]
   Schild J, 2022, SEGAH IEEE INT C SER
   Schild J, 2018, IEEE INT CONF SERIOU
   Stahl G., 2006, Group cognition: computer support for building collaborative knowledge (acting with technology), DOI [10.7551/mitpress/3372.001.0001, DOI 10.7551/MITPRESS/3372.001.0001]
   Steier R., 2020, INTERDISCIPLINARY LE, P1309
   Tatar D., 1989, SIGCHI Bulletin, V21, P108, DOI 10.1145/70609.70628
   van der Meer N, 2023, FRONT VIRTUAL REAL, V4, DOI 10.3389/frvir.2023.1159905
   Vygotsky L. S., 1978, Mind in society: The development of higher psychological processes, DOI [10.2307/j.ctvjf9vz4, DOI 10.2307/J.CTVJF9VZ4]
   Walsh K.R., 2002, Communications of the Associations of the Information Systems, V8 issue1, p, P20, DOI DOI 10.17705/1CAIS.00820
   Wu B, 2020, BRIT J EDUC TECHNOL, V51, P1991, DOI 10.1111/bjet.13023
   Yu F, 2019, PROC EUR CONF ELEARN, P609, DOI 10.34190/EEL.19.140
   Zheng L, 2018, 2018 INTERNATIONAL JOINT CONFERENCE ON INFORMATION, MEDIA AND ENGINEERING (ICIME), P6, DOI 10.1109/ICIME.2018.00011
NR 69
TC 1
Z9 1
U1 29
U2 29
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 63
DI 10.1007/s10055-024-00975-4
PG 17
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA JV9Q8
UT WOS:001176059900004
OA hybrid
DA 2024-08-05
ER

PT J
AU Valverde, I
   Gomez, G
   González, AD
   Sierra, A
   Perez, A
   Hussain, T
   Pushparajah, K
   Ordonez, A
   Carretero, EG
AF Valverde, Israel
   Gomez, Gorka
   Gonzalez, Aristides de Alarcon
   Sierra, Antonio
   Perez, Adriano
   Hussain, Tarique
   Pushparajah, Kuberan
   Ordonez, Antonio
   Carretero, Encarnacion Gutierrez
TI Mixed reality holograms for percutaneous lead extraction of cardiac
   implantable electronic devices
SO VIRTUAL REALITY
LA English
DT Article
DE Mixed reality; Holograms; Augmented reality; Percutaneous lead
   extraction
ID RISK STRATIFICATION; DEFIBRILLATOR; COMPLICATIONS; SURGERY; MODELS
AB To assess the potential of mixed reality holograms (MixR) based on CT images to improve percutaneous lead extraction (PLE) planning and intraoperative assistance. This was a prospective, controlled, single-centre study. Five patients with CIED infection for PLE were included in the study. Conventional imaging (chest radiograph and CT) and MixR holograms were evaluated for preoperative planning to identify common complications such as vascular thrombosis, broken leads, loops, kinking, fibrosis along the wires, and perforation of cardiovascular structures. The degree of difficulty of the procedure was estimated based on potential complications. After the PLE procedure, the level of concordance between conventional imaging and MixR holograms with intraoperative findings was evaluated. The utility of MixR intraoperative guidance was also assessed. MixR holograms demonstrated a very high correlation in predicting the presence of loops, kinking, and fibrosis compared to conventional imaging, which showed a low-to-high correlation. MixR also showed a high correlation in estimating the degree of difficulty of the procedure compared to conventional imaging, which tended to underestimate it. The surgeon who performed the PLE agreed that MixR was helpful during intraoperative assistance. MixR holograms based on CT images are an effective tool for understanding cardiovascular anatomy and detecting potential areas of complications. MixR may be used as a complementary tool for both preoperative planning and intraoperative assistance in PLE procedures.Graphical abstractMixed reality holograms for intraprocedural intervention assistance.
C1 [Valverde, Israel; Gomez, Gorka; Ordonez, Antonio; Carretero, Encarnacion Gutierrez] Univ Seville, Inst Biomed Seville IBiS, Fabricat Lab, Cardiovasc Pathophysiol Grp,HUVR,CSIC, Seville, Spain.
   [Valverde, Israel] Hosp Infantil Virgen del Rocio, Pediat Cardiol Unit, Seville 41013, Spain.
   [Valverde, Israel; Pushparajah, Kuberan] Kings Coll London, Sch Biomed Engn & Imaging Sci, London, England.
   [Gonzalez, Aristides de Alarcon] Univ Seville, Hosp Univ Virgen del Rocio, CSIC,Grp Resistencias Bacterianas & Antimicrobiano, Inst Biomed Sevilla IBIS,Dept Hematol,Unidad Clin, Seville 41013, Spain.
   [Sierra, Antonio; Perez, Adriano] VR AR MR & Adv Interfaces Team, Seville, Spain.
   [Hussain, Tarique] UT Southwestern Med Ctr, Dept Pediat, Dallas, TX USA.
   [Carretero, Encarnacion Gutierrez] Hosp Univ Virgen del Rocio, Dept Cardiovasc Surg, Seville, Spain.
   [Valverde, Israel] Hosp Sick Children, Labatt Family Heart Ctr, Div Cardiol, Toronto, ON, Canada.
C3 Consejo Superior de Investigaciones Cientificas (CSIC); University of
   Sevilla; CSIC-JA-USE - Instituto de Biomedicina de Sevilla (IBIS);
   Virgen del Rocio University Hospital; University of London; King's
   College London; Consejo Superior de Investigaciones Cientificas (CSIC);
   University of Sevilla; CSIC-JA-USE - Instituto de Biomedicina de Sevilla
   (IBIS); Virgen del Rocio University Hospital; University of Texas
   System; University of Texas Southwestern Medical Center Dallas; Virgen
   del Rocio University Hospital; University of Toronto; Hospital for Sick
   Children (SickKids)
RP Valverde, I (corresponding author), Univ Seville, Inst Biomed Seville IBiS, Fabricat Lab, Cardiovasc Pathophysiol Grp,HUVR,CSIC, Seville, Spain.; Valverde, I (corresponding author), Hosp Infantil Virgen del Rocio, Pediat Cardiol Unit, Seville 41013, Spain.; Valverde, I (corresponding author), Kings Coll London, Sch Biomed Engn & Imaging Sci, London, England.; Valverde, I (corresponding author), Hosp Sick Children, Labatt Family Heart Ctr, Div Cardiol, Toronto, ON, Canada.
EM ivalverde-ibis@us.es
RI Pushparajah, Kuberan/ABA-5484-2020; Valverde, Israel/E-7806-2013
OI Pushparajah, Kuberan/0000-0003-1541-1155; Valverde,
   Israel/0000-0001-5661-9900
FU Medtronic
FX None.
CR Al Janabi HF, 2020, SURG ENDOSC, V34, P1143, DOI 10.1007/s00464-019-06862-3
   Balabanoff C, 2014, J CARDIOVASC COMPUT, V8, P384, DOI 10.1016/j.jcct.2014.08.004
   Bardy GH, 2005, NEW ENGL J MED, V352, P225, DOI 10.1056/NEJMoa043399
   Blomström-Lundqvist C, 2020, EUROPACE, V22, P515, DOI 10.1093/europace/euz246
   Bontempi L, 2017, J CARDIOVASC ELECTR, V28, P811, DOI 10.1111/jce.13223
   Byrne N, 2016, J CARDIOVASC MAGN R, V18, pP190, DOI DOI 10.1186/1532-429X-18-S1-P190
   Cantinotti M, 2017, INT J CARDIOVAS IMAG, V33, P137, DOI 10.1007/s10554-016-0981-2
   Ehieli WL, 2017, AM J ROENTGENOL, V208, P770, DOI 10.2214/AJR.16.16897
   Fu HX, 2015, PACE, V38, P1439, DOI 10.1111/pace.12736
   Hindricks G, 2021, EUR HEART J, V42, P373, DOI 10.1093/eurheartj/ehaa612
   Holm MA, 2020, HEART RHYTHM, V17, P1009, DOI 10.1016/j.hrthm.2020.01.003
   Lewis RK, 2020, J CARDIOVASC ELECTR, V31, P723, DOI 10.1111/jce.14353
   Lewis RK, 2014, PACE, V37, P1297, DOI 10.1111/pace.12485
   Love CJ, 2000, PACE, V23, P544
   Mendez A, 2019, EUR HEART J, V40, P1092, DOI 10.1093/eurheartj/ehy685
   Patel D, 2019, JACC-CLIN ELECTROPHY, V5, P1432, DOI 10.1016/j.jacep.2019.07.018
   Rusanov A, 2010, ANN THORAC SURG, V89, P44, DOI 10.1016/j.athoracsur.2009.10.025
   Sidhu BS, 2021, EUROPACE, V23, P1462, DOI 10.1093/europace/euab037
   Sidhu BS, 2020, EUROPACE, V22, P1718, DOI 10.1093/europace/euaa131
   Sohail MR, 2016, CIRC-ARRHYTHMIA ELEC, V9, DOI 10.1161/CIRCEP.116.003929
   Tandon A, 2019, JACC-CARDIOVASC IMAG, V12, P921, DOI 10.1016/j.jcmg.2018.10.013
   Valverde I, 2017, REV ESP CARDIOL, V70, P282, DOI 10.1016/j.rec.2017.01.012
   Vatterott Pierce J, 2018, Card Electrophysiol Clin, V10, P625, DOI 10.1016/j.ccep.2018.07.007
   Vogler J, 2018, EUR J CARDIO-THORAC, V54, P745, DOI 10.1093/ejcts/ezy106
   Wazni O, 2010, J AM COLL CARDIOL, V55, P579, DOI 10.1016/j.jacc.2009.08.070
   Yushkevich PA, 2016, IEEE ENG MED BIO, P3342, DOI 10.1109/EMBC.2016.7591443
   Zhang X, 2019, EUR RADIOL, V29, P963, DOI 10.1007/s00330-018-5633-6
   Zucchelli G, 2019, EUROPACE, V21, P771, DOI 10.1093/europace/euy300
NR 28
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 21
DI 10.1007/s10055-023-00929-2
PG 11
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA FB2L0
UT WOS:001143221700001
OA hybrid
DA 2024-08-05
ER

PT J
AU Gronowski, A
   Arness, DC
   Ng, J
   Qu, ZL
   Lau, CW
   Catchpoole, D
   Nguyen, QV
AF Gronowski, Ashlee
   Arness, David Caelum
   Ng, Jing
   Qu, Zhonglin
   Lau, Chng Wei
   Catchpoole, Daniel
   Nguyen, Quang Vinh
TI The impact of virtual and augmented reality on presence, user experience
   and performance of Information Visualisation
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Augmented reality; Immersive visualisation; Biomedical
   data
ID ENVIRONMENTS
AB The fast growth of virtual reality (VR) and augmented reality (AR) head-mounted displays provides a new medium for interactive visualisations and visual analytics. Presence is the experience of consciousness within extended reality, and it has the potential to increase task performance. This project studies the impact that a sense of presence has on data visualisation performance and user experience under AR and VR conditions. A within-subjects design recruited 38 participants to complete interactive visualisation tasks within the novel immersive data analytics system for genomic data in AR and VR, and measured speed, accuracy, preference, presence, and user satisfaction. Open-ended user experience responses were also collected. The results implied that VR was more conducive to efficiency, effectiveness, and user experience as well as offering insight into possible cognitive load benefits for VR users.
C1 [Gronowski, Ashlee; Arness, David Caelum; Ng, Jing] Western Sydney Univ, Sch Psychol, Penrith, NSW, Australia.
   [Qu, Zhonglin; Lau, Chng Wei; Catchpoole, Daniel; Nguyen, Quang Vinh] Western Sydney Univ, Sch Comp Data & Math Sci, Penrith, NSW, Australia.
   [Catchpoole, Daniel] Sydney Childrens Hosp Network, Biospecimen Res Serv, Childrens Canc Res Unit, Kids Res, Westmead, Australia.
   [Nguyen, Quang Vinh] Western Sydney Univ, MARCS Inst, Penrith, NSW, Australia.
   [Catchpoole, Daniel] Univ Technol Sydney, Fac Engn & Informat Technol, Sydney, NSW, Australia.
C3 Western Sydney University; Western Sydney University; NSW Health; Sydney
   Childrens Hospitals Network; Western Sydney University; University of
   Technology Sydney
RP Nguyen, QV (corresponding author), Western Sydney Univ, Sch Comp Data & Math Sci, Penrith, NSW, Australia.; Nguyen, QV (corresponding author), Western Sydney Univ, MARCS Inst, Penrith, NSW, Australia.
EM q.nguyen@westernsydney.edu.au
FU Western Sydney University
FX No Statement AvailableDAS:The data used in this paper are in public
   domain https://www.cbioportal.org/study/summary?id=aml_ohsu_2018.
CR Azofeifa JD, 2022, Informatics
   Baumgartner T, 2008, FRONT HUM NEUROSCI, V2, DOI 10.3389/neuro.09.008.2008
   Bouchard S, 2012, INTERACT COMPUT, V24, P227, DOI 10.1016/j.intcom.2012.04.011
   Clemente M, 2014, INTERACT COMPUT, V26, P269, DOI 10.1093/iwc/iwt037
   Doherty K, 2018, INT J HUM-COMPUT ST, V110, P63, DOI 10.1016/j.ijhcs.2017.10.006
   Faul F, 2007, BEHAV RES METHODS, V39, P175, DOI 10.3758/BF03193146
   Few S, 2014, Encyclopedia Hum Comput Interact, V2nd
   Few S, 2009, ANALYTICS
   Guna J, 2019, FUTURE GENER COMP SY, V91, P263, DOI 10.1016/j.future.2018.08.049
   Hills A.M., 2011, FOOLPROOF GUIDE STAT
   Kennedy R.S., 1993, Int. J. Aviat. Psy, P203
   Lau CW, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-15548-1
   Lau CW, 2019, PROCEEDINGS OF THE AUSTRALASIAN COMPUTER SCIENCE WEEK MULTICONFERENCE (ACSW 2019), DOI 10.1145/3290688.3290722
   Legetth O., 2019, bioRxiv, DOI DOI 10.1101/329102
   Maes A, 2018, J INTEGR BIOINFORMAT, V15, DOI 10.1515/jib-2018-0006
   McGill M, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2143, DOI 10.1145/2702123.2702382
   Peters D, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.00797
   Qu Z., 2022, Innovat Digital Health Diagnostics Biomarker, V2, P27, DOI [10.36401/IDDB-21-04, DOI 10.36401/IDDB-21-04]
   Qu ZL, 2019, CANCER INFORM, V18, DOI 10.1177/1176935119835546
   Nguyen QV, 2020, VIS INFORM, V4, P1, DOI 10.1016/j.visinf.2020.09.004
   Riva G., 2014, Interacting with presence: HCI and the sense of presence in computer-mediated environments
   Ryan RM, 2017, SELF-DETERMINATION THEORY: BASIC PSYCHOLOGICAL NEEDS IN MOTIVATION, DEVELOPMENT, AND WELLNESS, P1, DOI 10.1521/978.14625/28806
   Schubert T, 2001, PRESENCE-VIRTUAL AUG, V10, P266, DOI 10.1162/105474601300343603
   Silva GR, 2016, VIRTUAL REAL-LONDON, V20, P237, DOI 10.1007/s10055-016-0295-7
   Slater M, 1997, PRESENCE-VIRTUAL AUG, V6, P603, DOI 10.1162/pres.1997.6.6.603
   Slater M, 2018, BRIT J PSYCHOL, V109, P431, DOI 10.1111/bjop.12305
   Tabachnick B. G., 2013, USING MULTIVARIATE S, V6
   Takatalo J, 2008, COMPUT HUM BEHAV, V24, P1, DOI 10.1016/j.chb.2006.11.003
   Weber S, 2021, FRONT PSYCHOL, V12, DOI 10.3389/fpsyg.2021.628298
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   Yang A, 2018, bioRxiv
   Zhang JF, 2019, BMC BIOINFORMATICS, V20, DOI 10.1186/s12859-019-2666-z
NR 32
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD JUL 9
PY 2024
VL 28
IS 3
AR 133
DI 10.1007/s10055-024-01032-w
PG 10
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA YK6O4
UT WOS:001268423300002
OA hybrid
DA 2024-08-05
ER

PT J
AU Pesek, M
   Hirci, N
   Znidersic, K
   Marolt, M
AF Pesek, Matevz
   Hirci, Nejc
   Znidersic, Klara
   Marolt, Matija
TI Enhancing music rhythmic perception and performance with a VR game
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Music theory; E-learning
AB This study analyzes the effect of using a virtual reality (VR) game as a complementary tool to improve users' rhythmic performance and perception in a remote and self-learning environment. In recent years, remote learning has gained importance due to various everyday situations; however, the effects of using VR in such situations for individual and self-learning have yet to be evaluated. In music education, learning processes are usually heavily dependent on face-to-face communication with a teacher and are based on a formal or informal curriculum. The aim of this study is to investigate the potential of gamified VR learning and its influence on users' rhythmic sensory and perceptual abilities. We developed a drum-playing game based on a tower defense scenario designed to improve four aspects of rhythmic perceptual skills in elementary school children with various levels of music learning experience. In this study, 14 elementary school children received Meta Quest 2 headsets for individual use in a 14-day individual training session. The results showed a significant increase in their rhythmical skills through an analysis of their rhythmic performance before and after the training sessions. In addition, the experience of playing the VR game and using the HMD setup was also assessed, highlighting some of the challenges of currently available affordable headsets for gamified learning scenarios.
C1 [Pesek, Matevz; Hirci, Nejc; Znidersic, Klara; Marolt, Matija] Univ Ljubljana, Fac Comp & Informat Sci, Ljubljana, Slovenia.
C3 University of Ljubljana
RP Pesek, M (corresponding author), Univ Ljubljana, Fac Comp & Informat Sci, Ljubljana, Slovenia.
EM matevz.pesek@fri.uni-lj.si; nejc.hirci@lgm.fri.uni-lj.si;
   klara.znidersic@fri.uni-lj.si; matija.marolt@fri.uni-lj.si
RI Znidersic, Klara/KRP-0463-2024
FU Slovenian Research and Innovation Agency; Jeunesses Musicales in
   Slovenia [J6-3135]
FX This research was partly funded by the Slovenian Research and Innovation
   Agency, within the project Music for the young people since 1945 and
   Jeunesses Musicales in Slovenia, grant number J6-3135.
CR Bégel V, 2017, FRONT HUM NEUROSCI, V11, DOI 10.3389/fnhum.2017.00273
   Bonacina Silvia, 2019, Glob Pediatr Health, V6, p2333794X19852045, DOI 10.1177/2333794X19852045
   Checa D, 2020, MULTIMED TOOLS APPL, V79, P5501, DOI 10.1007/s11042-019-08348-9
   Christopoulos A, 2018, VIRTUAL REAL-LONDON, V22, P353, DOI 10.1007/s10055-017-0330-3
   Correia AI, 2022, BEHAV RES METHODS, V54, P955, DOI 10.3758/s13428-021-01641-2
   Dalla Bella S, 2022, ANN NY ACAD SCI, V1517, P15, DOI 10.1111/nyas.14878
   Damasevicius R, 2023, INFORMATION, V14, DOI 10.3390/info14020105
   Degli Innocenti E, 2019, COMPUT EDUC, V139, P102, DOI 10.1016/j.compedu.2019.04.010
   Epp R, 2021, IEEE T GAMES, V13, P275, DOI 10.1109/TG.2021.3057288
   Faric N, 2019, J MED INTERNET RES, V21, DOI 10.2196/13833
   Georgi M, 2023, FRONT PSYCHOL, V13, DOI 10.3389/fpsyg.2022.862468
   Heath A, 2023, This is Meta's AR / VR hardware roadmap for the next four years
   Järvinen A, 2016, PROCEEDINGS OF 2016 FUTURE TECHNOLOGIES CONFERENCE (FTC), P1065, DOI 10.1109/FTC.2016.7821735
   Johnson D, 2020, VIRTUAL REAL-LONDON, V24, P303, DOI 10.1007/s10055-019-00388-8
   Keeler KR., 2020, Vis Res Music Educ, V37, P5
   Kunert R, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0159103
   Law LNC, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0052508
   Leite RBC, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00447
   Loveridge B., 2023, J Netw Music Arts, V5, P5
   Ma M., 2014, Intell Syst Ref Libr, DOI [10.1007/978-3-642-54816-11, DOI 10.1007/978-3-642-54816-11]
   Miskinis A, 2021, future directions of music cognition, DOI [10.18061/fdmc.2021.0021, DOI 10.18061/FDMC.2021.0021]
   Moholdt T., 2017, BMJ Open Sport Exerc Med, V3, pe000258, DOI DOI 10.1136/bmjsem-2017-000258
   Moon HS, 2023, INT J HUM-COMPUT INT, V39, P2840, DOI 10.1080/10447318.2022.2087000
   Moth-Poulsen M, 2019, Teach me drums: learning rhythms through the embodiment of a drumming teacher in virtual reality
   Nagta A, 2022, Oculus: a new dimension to virtual reality, DOI [10.1109/ICACRS55517.2022.10029200, DOI 10.1109/ICACRS55517.2022.10029200]
   Nair V, 2023, Results of the 2023 census of beat saber users: virtual reality gaming population insights and factors affecting virtual reality e-sports performance
   Oyelere SS, 2020, SMART LEARN ENVIRON, V7, DOI 10.1186/s40561-020-00142-7
   Perez L, 2019, COMPUT IND, V109, P114, DOI 10.1016/j.compind.2019.05.001
   Pesek M, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12147296
   Pesek M, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10196781
   Pesek M, 2020, IEEE ACCESS, V8, P97090, DOI 10.1109/ACCESS.2020.2994389
   Pinkl J, 2022, Design of a VR action observation tool for rhythmic coordination training, DOI [10.1109/VRW55335.2022.00232, DOI 10.1109/VRW55335.2022.00232]
   Pinkl J, 2023, 2023 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS, VRW, P701, DOI 10.1109/VRW58643.2023.00193
   Rojas-Sánchez MA, 2023, EDUC INF TECHNOL, V28, P155, DOI 10.1007/s10639-022-11167-5
   Schrepp M, 2017, INT J INTERACT MULTI, V4, P103, DOI 10.9781/ijimai.2017.09.001
   Serafin S, 2017, Considerations on the use of virtual and augmented reality technologies in music education, DOI [10.1109/KELVAR.2017.7961562, DOI 10.1109/KELVAR.2017.7961562]
   Shahab M, 2022, EDUC INF TECHNOL, V27, P819, DOI 10.1007/s10639-020-10392-0
   Steam, 2023, Steam Hardware & Software Survey
   Sween J, 2014, J PHYS ACT HEALTH, V11, P864, DOI 10.1123/jpah.2011-0425
   Szpak A, 2020, J MED INTERNET RES, V22, DOI 10.2196/19840
   Takada A, 2023, AAAI CONF ARTIF INTE, P5266
   Tao GR, 2021, J NEUROENG REHABIL, V18, DOI 10.1186/s12984-020-00801-3
   Tierney A, 2017, J COGNITIVE NEUROSCI, V29, P855, DOI 10.1162/jocn_a_01092
   Vargas A, 2020, Using virtual reality and music in cognitive disability therapy, DOI [10.1145/3399715.3399916, DOI 10.1145/3399715.3399916]
   VR O, 2023, What is the market for a VR venue?
   Vuust P, 2022, NAT REV NEUROSCI, V23, P287, DOI 10.1038/s41583-022-00578-5
   Waddell G., 2019, Frontiers in ICT, V6, P1, DOI DOI 10.3389/FICT.2019.00011
   Wallentin M, 2010, LEARN INDIVID DIFFER, V20, P188, DOI 10.1016/j.lindif.2010.02.004
   Wang L, 2022, GAMES HEALTH J, V11, P277, DOI 10.1089/g4h.2022.0086
   Zeller L, 2022, ACM INT C P SERIES, P159, DOI [10.1145/3561212.3561249, DOI 10.1145/3561212.3561249]
   Zentner M, 2017, ANN NY ACAD SCI, V1400, P33, DOI 10.1111/nyas.13410
NR 51
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAY 25
PY 2024
VL 28
IS 2
AR 118
DI 10.1007/s10055-024-01014-y
PG 15
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA SD3A6
UT WOS:001232468900001
OA hybrid
DA 2024-08-05
ER

PT J
AU Escalada-Hernandez, P
   Soto-Ruiz, N
   Ballesteros-Egüés, T
   Larrayoz-Jiménez, A
   Martín-Rodríguez, LS
AF Escalada-Hernandez, Paula
   Soto-Ruiz, Nelia
   Ballesteros-Egues, Tomas
   Larrayoz-Jimenez, Ana
   Martin-Rodriguez, Leticia San
TI Usability and user expectations of a HoloLens-based augmented reality
   application for learning clinical technical skills
SO VIRTUAL REALITY
LA English
DT Article
DE Augmented reality; HoloLens; Clinical skills; Academic training;
   Invasive procedures; Nursing students; Medical students
ID NURSING-EDUCATION; TOOL
AB The application of augmented reality in training health science students is increasingly widespread. The aim of this work was to assess the usability and user expectations of an augmented reality application for smart glasses (Microsoft HoloLens) that can be used to train on four invasive procedures (i.e. intramuscular injection, nasogastric tube insertion, endotracheal intubation and suctioning via tracheostomy tube). A descriptive study was conducted with nursing students from three Spanish universities. Participants answered a questionnaire to assess the use of the ARSim2care application. This application offers the possibility of visualizing the internal anatomical structures during the training of the clinical technical skills for the performance of the mentioned invasive techniques. The questionnaire included demographic data, the System Usability Scale and questions about the user expectations in relation to learning with the use of augmented reality. In total, 61 participants responded to the questionnaire after using the ARSim2care application. The mean score of the System Usability Scale was 73.15 (standard deviation: 15.04) and 62.4% (n = 38) of the participants considered their experience with the application as excellent or good. In relation to user expectations, more than 90% of students indicated that the use of the application could improve their motivation and stimulation in learning, their content retention and their anatomical understanding. The developed ARSim2care application for Microsoft HoloLens showed a high level of usability and acceptance as a learning tool for training certain clinical procedures by visualizing the internal structures of the body.
C1 [Escalada-Hernandez, Paula; Soto-Ruiz, Nelia; Martin-Rodriguez, Leticia San] Publ Univ Navarre UPNA, Dept Hlth Sci, Avda Baranain S-N, Pamplona 31008, Navarra, Spain.
   [Escalada-Hernandez, Paula; Soto-Ruiz, Nelia; Martin-Rodriguez, Leticia San] Navarra Inst Hlth Res, IdiSNA, Irunlarrea 3, Pamplona 31008, Navarra, Spain.
   [Ballesteros-Egues, Tomas] Publ Univ Navarre UPNA, Dept Engn, Campus Arrosadia, Pamplona 31006, Navarra, Spain.
   [Larrayoz-Jimenez, Ana] Ind Augmented Real iAR, C Arcadio Maria Larraona 1,2, Pamplona 31008, Navarra, Spain.
C3 Universidad Publica de Navarra; Universidad Publica de Navarra
RP Soto-Ruiz, N (corresponding author), Publ Univ Navarre UPNA, Dept Hlth Sci, Avda Baranain S-N, Pamplona 31008, Navarra, Spain.; Soto-Ruiz, N (corresponding author), Navarra Inst Hlth Res, IdiSNA, Irunlarrea 3, Pamplona 31008, Navarra, Spain.
EM paula.escalada@unavarra.es; nelia.soto@unavarra.es;
   tomas.ballesteros@unavarra.es; alarrayoz@iar-soft.com;
   leticia.sanmartin@unavarra.es
RI Ballesteros, Tomás/JOP-6326-2023; Soto Ruiz/AAE-6457-2021; Escalada
   Hernandez, Paula/GSO-3224-2022
OI Ballesteros, Tomás/0000-0002-6891-2677; Soto Ruiz/0000-0002-5161-2272;
   Escalada Hernandez, Paula/0000-0003-2263-156X
FU Universidad Pblica de Navarra
FX No Statement Available
CR AlRoobaea R, 2014, 2014 SCIENCE AND INFORMATION CONFERENCE (SAI), P48, DOI 10.1109/SAI.2014.6918171
   Anderson M, 2022, CLIN SIMUL NURS, V69, P40, DOI 10.1016/j.ecns.2022.05.005
   Anderson M, 2021, CLIN SIMUL NURS, V54, P105, DOI 10.1016/j.ecns.2021.01.006
   [Anonymous], 2008, ESS BACC ED PROF NUR
   Azuma RT, 1997, PRESENCE-VIRTUAL AUG, V6, P355, DOI 10.1162/pres.1997.6.4.355
   Baillie Lesley, 2009, Nurse Educ Pract, V9, P61, DOI 10.1016/j.nepr.2008.05.003
   Bangor A, 2009, J USABILITY STUD, V4, P114
   Bayram SB, 2019, NURS EDUC TODAY, V79, P25, DOI 10.1016/j.nedt.2019.05.010
   Brooke J., 1996, SUS-a quick and dirty usability scale, DOI [DOI 10.1201/9781498710411-35, DOI 10.1201/9781498710411]
   Brown M., 2020, EDUCAUSE
   Buettner R., 2020, 2020 IEEE Symp Ind Electron Appl ISIEA 2020, P1, DOI [10.1109/ISIEA49364.2020.9188211, DOI 10.1109/ISIEA49364.2020.9188211]
   Byrne PJ, 2017, CIN-COMPUT INFORM NU, V35, P117, DOI 10.1097/CIN.0000000000000339
   Chen L, 2017, PROCEEDINGS OF THE 2017 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P123, DOI 10.1109/ISMAR.2017.29
   Sevilla-Gonzalez MD, 2020, JMIR HUM FACTORS, V7, DOI 10.2196/21161
   Dias Coutinho VR, 2020, Licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)
   Ferrer Torregrosa J, 2014, Desarrollo, estudio y evaluacion de contenidos didacticos mediante realidad aumentada en la formacion de graduados de podologia
   Ferrer-Torregrosa J, 2015, J SCI EDUC TECHNOL, V24, P119, DOI 10.1007/s10956-014-9526-4
   Frost J, 2020, BMJ SIMUL TECHNOL EN, V6, P214, DOI 10.1136/bmjstel-2019-000464
   Gallos Parisis, 2018, Stud Health Technol Inform, V251, P82
   Garrett B. M., 2018, CURRENT ISSUES EMERG, V4, P10
   Gerup J, 2020, INT J MED EDUC, V11, DOI 10.5116/ijme.5e01.eb1a
   Hauze SW, 2019, ADV EXP MED BIOL, V1120, P1, DOI 10.1007/978-3-030-06070-1_1
   Ingrassia PL, 2020, J MED INTERNET RES, V22, DOI 10.2196/14910
   Kapp K, 2022, EDUC SCI, V12, DOI 10.3390/educsci12120854
   Kemery SR, 2020, NURS EDUC PERSPECT, V41, P83, DOI 10.1097/01.NEP.0000000000000515
   Kim SK, 2021, J MED INTERNET RES, V23, DOI 10.2196/24313
   Klinker K, 2020, INFORM SYST FRONT, V22, P1419, DOI 10.1007/s10796-019-09937-7
   Kurt Y, 2021, NURS EDUC TODAY, V103, DOI 10.1016/j.nedt.2021.104955
   Lewis JR, 2018, J USABILITY STUD, V13, P158
   Lewis JR, 2018, INT J HUM-COMPUT INT, V34, P577, DOI 10.1080/10447318.2018.1455307
   Lioce L., 2020, HEALTHCARE SIMULATIO
   Marschollek M, 2016, STUD HEALTH TECHNOL, V225, P377, DOI 10.3233/978-1-61499-658-3-377
   Mendez KJW, 2020, NURS EDUC TODAY, V93, DOI 10.1016/j.nedt.2020.104531
   Mohapatra D., 2015, International Journal of Advanced Medical and Health Research, V2, P3, DOI DOI 10.4103/2349-4220.159113
   Rodríguez-Abad C, 2021, INT J ENV RES PUB HE, V18, DOI 10.3390/ijerph18084262
   Roessel J, 2020, P INT COMP SOFTW APP, P489, DOI 10.1109/COMPSAC48688.2020.0-204
   Rourke S, 2020, INT J NURS STUD, V102, DOI 10.1016/j.ijnurstu.2019.103466
   San Martin-Rodriguez L, 2019, Licensed under a a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)
   Scheckel M., 2012, Teaching in nursing: A guide for faculty, V4th, P170
   U.S. Food and Drug Administration, 2016, FDA's guidance document. Applying Human Factors and Usability Engineering to Medical Devices
   Weller JM, 2012, MED J AUSTRALIA, V196, P594, DOI 10.5694/mja10.11474
   Wüller H, 2019, BMC NURS, V18, DOI 10.1186/s12912-019-0342-2
NR 42
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD APR 24
PY 2024
VL 28
IS 2
AR 102
DI 10.1007/s10055-024-00984-3
PG 10
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA OM5I2
UT WOS:001207699600001
OA hybrid, Green Submitted
DA 2024-08-05
ER

PT J
AU Trevena, L
   Paay, J
   McDonald, R
AF Trevena, Lee
   Paay, Jeni
   McDonald, Rachael
TI VR interventions aimed to induce empathy: a scoping review
SO VIRTUAL REALITY
LA English
DT Article
DE Scoping review; Disability support worker; Empathy; Training; Virtual
   reality; Technology
ID VIRTUAL-REALITY-SIMULATOR; PERSPECTIVE-TAKING; ATTITUDES; AGEISM
AB To assess the methods and outcomes of virtual reality (VR), interventions aimed at inducing empathy and to evaluate if VR could be used in this manner for disability support worker (DSW) training, as well as highlight areas for future research. The authors conducted a scoping review of studies that used VR interventions to induce empathy in participants. We searched three databases for articles published between 1960 and 2021 using "virtual reality" and "empathy" as key terms. The search yielded 707 articles, and 44 were reviewed. VR interventions largely resulted in enhanced empathy skills for participants. Most studies agreed that VR's ability to facilitate perspective-taking was key to inducing empathy for participants. Samples were often limited to the context of healthcare, medicine, and education. This literature provides preliminary evidence for the technology's efficacy for inducing empathy. Identified research gaps relate to limited studies done, study quality and design, best practice intervention characteristics, populations and outcomes of interest, including lack of transfer and data across real-world settings.
C1 [Trevena, Lee; Paay, Jeni] Swinburne Univ, Ctr Design Innovat, John St, Hawthorn, Vic 3122, Australia.
   [McDonald, Rachael] Swinburne Univ, MedTechV Hub, John St, Hawthorn, Vic 3122, Australia.
C3 Swinburne University of Technology; Swinburne University of Technology
RP Trevena, L (corresponding author), Swinburne Univ, Ctr Design Innovat, John St, Hawthorn, Vic 3122, Australia.
EM ltrevena@swin.edu.au
OI Trevena, Lee/0000-0002-4333-8920
FU Swinburne University of Technology
FX No Statement Available
CR Adefila A, 2016, J MENT HEALTH TRAIN, V11, P91, DOI 10.1108/JMHTEP-10-2015-0048
   [Anonymous], 2011, WORLD REP DIS, P3
   Arksey H., 2005, INT J SOC RES METHOD, V8, P19, DOI [10.1080/1364557032000119616, DOI 10.1080/1364557032000119616, https://doi.org/10.1080/1364557032000119616]
   Bang E., 2018, Virtually empathetic? examining the effects of virtual reality storytelling on empathy, DOI [10.1007/978-3-319-91581-4_21, DOI 10.1007/978-3-319-91581-4_21]
   Barbot B, 2020, COMPUT HUM BEHAV, V111, DOI 10.1016/j.chb.2020.106431
   Barreda-Angeles M, 2020, CYBERPSYCH BEH SOC N, V23, P683, DOI 10.1089/cyber.2019.0665
   Batson CD, 2009, SOC ISS POLICY REV, V3, P141, DOI 10.1111/j.1751-2409.2009.01013.x
   Batson CD, 1997, J PERS SOC PSYCHOL, V72, P105, DOI 10.1037/0022-3514.72.1.105
   Buchman S., 2019, J Interprofessional Educ Pract, V15, P127, DOI DOI 10.1016/J.XJEP.2019.03.010
   Camilleri V, 2017, 2017 23 INT C VIRT S
   Campbell D, 2021, NURS EDUC TODAY, V98, DOI 10.1016/j.nedt.2021.104764
   Cheng YF, 2010, COMPUT EDUC, V55, P1449, DOI 10.1016/j.compedu.2010.06.008
   Christofi M, 2020, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.01242
   COHEN J, 1992, PSYCHOL BULL, V112, P155, DOI 10.1037/0033-2909.112.1.155
   Commonwealth of Australia, 2020, NDIS workforce interim report
   Deladisma AM, 2007, AM J SURG, V193, P756, DOI 10.1016/j.amjsurg.2007.01.021
   Dymond R., 1950, Journal of Consulting Psychology, V14, P343, DOI DOI 10.1037/H0061674
   Formosa NJ, 2018, AUST J PSYCHOL, V70, P57, DOI 10.1111/ajpy.12167
   Gilet AL, 2013, CAN J BEHAV SCI, V45, P42, DOI 10.1037/a0030425
   Gugliucci M.R., 2019, Innov Aging, V3, pS298, DOI DOI 10.1093/GERONI/IGZ038.1096
   Hamilton-Giachritsis C, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-21036-2
   Hargrove A, 2020, COMPUT HUM BEHAV REP, V2, DOI 10.1016/j.chbr.2020.100038
   Herrera F, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0204494
   Hong QN, 2018, EDUC INFORM, V34, P285, DOI 10.3233/EFI-180221
   Ilgunaite G., 2017, Appl Psychol Bull, V280, P2
   Ingram KM, 2019, J ADOLESCENCE, V71, P72, DOI 10.1016/j.adolescence.2018.12.006
   Jeffrey D, 2016, J ROY SOC MED, V109, P446, DOI 10.1177/0141076816680120
   Johnsen K, 2008, IEEE VIRTUAL REALITY 2008, PROCEEDINGS, P133
   Jorgensen D, 2009, HEALTH SOC CARE COMM, V17, P396, DOI 10.1111/j.1365-2524.2008.00839.x
   Jütten LH, 2018, DEMENT GER COGN D EX, V8, P453, DOI 10.1159/000494660
   Jütten LH, 2017, BMJ OPEN, V7, DOI 10.1136/bmjopen-2016-015702
   Kalyanaraman S, 2010, J NERV MENT DIS, V198, P437, DOI 10.1097/NMD.0b013e3181e07d66
   Kang J, 2018, WIRELESS PERS COMMUN, V98, P1931, DOI 10.1007/s11277-017-4954-0
   Kim J, 2020, BMC PSYCHOL, V8, DOI 10.1186/s40359-020-00418-0
   Koritsas S, 2023, BRAIN IMPAIR, V24, P649, DOI 10.1017/BrImp.2022.14
   Kors MJL, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376748
   Korsi MJL, 2016, CHI PLAY 2016: PROCEEDINGS OF THE 2016 ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P91, DOI 10.1145/2967934.2968110
   Lawrence EJ, 2004, PSYCHOL MED, V34, P911, DOI 10.1017/S0033291703001624
   Lee H., 1999, Journal of Korean Academy of Nursing, V29, P1123, DOI DOI 10.4040/JKAN.1999.29.5.1123
   Lollar DJ, 2020, International public health and global disability, P149, DOI [10.1007/978-1-0716-0888-3_7, DOI 10.1007/978-1-0716-0888-3_7]
   MacDorman KF, 2019, COMPUT HUM BEHAV, V94, P140, DOI 10.1016/j.chb.2019.01.011
   Martingano AJ., 2021, Technology, Mind, and Behavior, V2, DOI [DOI 10.1037/TMB0000034, 10.1037/tmb0000034]
   McCarthy J., 2007, What Is Artificial Intelligence?, P1
   McEvoy KA, 2016, P IEEE VIRT REAL ANN, P229, DOI 10.1109/VR.2016.7504737
   Merriam-Webster, 2021, earphone
   MFMER, 2023, EEG (electroencephalogram)
   Milk Chris., 2015, Ted Talk
   Misztal S, 2020, Simulating illness: experiencing visual migraine impairments in virtual reality
   Muller DA, 2017, EXTENDED ABSTRACTS PUBLICATION OF THE ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY (CHI PLAY'17 EXTENDED ABSTRACTS), P599, DOI 10.1145/3130859.3130862
   Mustapha S, 2020, 16 IEEE INT C SIGN P
   Navarrete J, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11031276
   NDS, 2021, Federal budget submission
   North MS, 2012, PSYCHOL BULL, V138, P982, DOI 10.1037/a0027843
   Oh SY, 2016, COMPUT HUM BEHAV, V60, P398, DOI 10.1016/j.chb.2016.02.007
   Patané I, 2020, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.510787
   Peters MDJ, JBI manual for evidence synthesis, DOI DOI 10.46658/JBIMES-20-12
   PWD, 2022, Peak disability org slams 2022 Federal Budget for 'short-changing' people with disability
   Roswell RO, 2020, ACAD MED, V95, P1882, DOI 10.1097/ACM.0000000000003615
   Ryan R., 2018, A portable training entitlement system for the disability support services sector
   Schutte NS, 2017, MOTIV EMOTION, V41, P708, DOI 10.1007/s11031-017-9641-7
   Segovia KY, 2012, SOC INFLUENCE, V7, P285, DOI 10.1080/15534510.2012.670906
   Slater P, 2019, INT J OLDER PEOPLE N, V14, DOI 10.1111/opn.12243
   Stark-Wroblewski K, 2008, TEACH PSYCHOL, V35, P343, DOI 10.1080/00986280802374526
   Stavroulia K, 2018, ACM INT C P SERIES
   Stavroulia K-E, 2019, C OBJ OR PROGR SYST
   Stavroulia KE, 2019, INT J EMERG TECHNOL, V14, P18, DOI 10.3991/ijet.v14i07.9946
   Steinfeld N, 2020, JOURNAL PRACT, V14, P240, DOI 10.1080/17512786.2019.1704842
   Stueber Karsten, 2019, Empathy
   Sulpizio V, 2015, EXP BRAIN RES, V233, P2091, DOI 10.1007/s00221-015-4280-2
   Sulpizio V, 2014, FRONT HUM NEUROSCI, V8, DOI 10.3389/fnhum.2014.00716
   Sulpizio V, 2013, BEHAV BRAIN RES, V242, P62, DOI 10.1016/j.bbr.2012.12.031
   Swartzlander B., 2017, We are alfred: empathy learned through a medical education virtual reality project
   Tong X, 2020, JMIR SERIOUS GAMES, V8, DOI 10.2196/17354
   Trinidad KR., 2018, Understanding the mentally disturbed's reality through heart rate controlled virtual reality inducing artificial auditory hallucination and persecutory paranoia, DOI [10.1063/1.5080872, DOI 10.1063/1.5080872]
   van Loon A, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0202442
   Van Mierlo LD, 2012, INT J GERIATR PSYCH, V27, P1, DOI 10.1002/gps.2694
   Ventura S, 2021, CYBERPSYCH BEH SOC N, V24, P258, DOI 10.1089/cyber.2020.0209
   Wardana A., 2017, Asia-Pac J Hum Rights Law, V18, P172, DOI [10.1163/15718158-01802003, DOI 10.1163/15718158-01802003]
   Wark S, 2014, J APPL RES INTELLECT, V27, P273, DOI 10.1111/jar.12087
   Washington E, 2019, EDUC MEDIA TECH YEAR, V42, P67, DOI 10.1007/978-3-030-27986-8_7
   Weinel J, 2018, HUM-COMPUT INT-SPRIN, P183, DOI 10.1007/978-3-319-73356-2_11
   Wijma EM, 2018, AGING MENT HEALTH, V22, P1115, DOI 10.1080/13607863.2017.1348470
NR 82
TC 0
Z9 0
U1 10
U2 10
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR 19
PY 2024
VL 28
IS 2
AR 80
DI 10.1007/s10055-024-00946-9
PG 49
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA LK8Q5
UT WOS:001186789700001
OA hybrid, Green Submitted
DA 2024-08-05
ER

PT J
AU Juan, MC
   Hidaldo, C
   Mifsut, D
AF Juan, M. -Carmen
   Hidaldo, Cora
   Mifsut, Damian
TI A mixed reality application for total hip arthroplasty
SO VIRTUAL REALITY
LA English
DT Article
DE Mixed reality; Arthroplasty; Hip; Total hip arthroplasty; HoloLens;
   Surgery
ID NAVIGATION SYSTEM; ACCURACY; TECHNOLOGY; COMPONENTS; PLACEMENT;
   POSITION; OUTCOMES
AB Total hip arthroplasty (or total hip replacement) is the current surgical solution for the treatment of advanced coxarthrosis, with the objective of providing mobility and pain relief to patients. For this purpose, surgery can be planned using preoperative images acquired from the patient and navigation systems can also be used during the intervention. Robots have also been used to assist in interventions. In this work, we propose a new mixed reality application for total hip arthroplasty. The surgeon only has to wear HoloLens 2. The application does not require acquiring preoperative or intraoperative images of the patient and uses hand interaction. Interaction is natural and intuitive. The application helps the surgeon place a virtual acetabular cup onto the patient's acetabulum as well as define its diameter. Similarly, a guide for drilling and implant placement is defined, establishing the abduction and anteversion angles. The surgeon has a direct view of the operating field at all times. For validation, the values of the abduction and anteversion angles offered by the application in 20 acetabular cup placements have been compared with real values (ground-truth). From the results, the mean (standard deviation) is 0.375 (0.483) degrees for the error in the anteversion angle and 0.1 (0.308) degrees for the abduction angle, with maximum discrepancies of 1 degree. A study was also carried out on a cadaver, in which a surgeon verified that the application is suitable to be transferred to routine clinical practice, helping in the guidance process for the implantation of a total hip prosthesis.
C1 [Juan, M. -Carmen; Hidaldo, Cora] Univ Politecn Valencia, Inst Univ Automat Informat Ind, C Camino Vera S-N, E-46022 Valencia, Spain.
   [Mifsut, Damian] Hosp U Francesc Borja Gandia, Serv Cirugia Ortoped & Traumatol, Ave Med 6, Gandia 46702, Spain.
C3 Universitat Politecnica de Valencia
RP Juan, MC (corresponding author), Univ Politecn Valencia, Inst Univ Automat Informat Ind, C Camino Vera S-N, E-46022 Valencia, Spain.
EM mcarmen@dsic.upv.es
OI Juan, M.-Carmen/0000-0002-8764-1470
FU Universitat Politcnica de Valncia
FX The authors would like to thank the editor and reviewers for their
   valuable suggestions.
CR Abdelaa O, 2019, INT J ARTIF ORGANS, V42, P271, DOI 10.1177/0391398818815479
   Ackerman IN, 2019, BMC MUSCULOSKEL DIS, V20, DOI 10.1186/s12891-019-2411-9
   Ahangar AA, 2019, AGEING INT, V44, P399, DOI 10.1007/s12126-017-9315-5
   Akulauskas M, 2023, APPL SCI-BASEL, V13, DOI 10.3390/app13148315
   Bhandari M, 2009, ORTHOP CLIN N AM, V40, P329, DOI 10.1016/j.ocl.2009.03.001
   Bosker BH, 2007, ARCH ORTHOP TRAUM SU, V127, P375, DOI 10.1007/s00402-007-0294-y
   Callanan MC, 2011, CLIN ORTHOP RELAT R, V469, P319, DOI 10.1007/s11999-010-1487-1
   Chuang TJ, 2023, VIRTUAL REAL-LONDON, V27, P2417, DOI 10.1007/s10055-023-00816-w
   D'Lima DD, 2000, J BONE JOINT SURG AM, V82A, P315, DOI 10.2106/00004623-200003000-00003
   Dargel J, 2014, DTSCH ARZTEBL INT, V111, P884, DOI 10.3238/arztebl.2014.0884
   Digioia A.M., 2000, OPER TECH ORTHOP, V10, P3, DOI [DOI 10.1016/S1048-6666(00)80036-1, 10.1016/S1048-6666(00)80036-1]
   Ethgen O, 2004, J BONE JOINT SURG AM, V86A, P963, DOI 10.2106/00004623-200405000-00012
   Fotouhi J, 2018, J MED IMAGING, V5, DOI 10.1117/1.JMI.5.2.021205
   Kitagawa M, 2022, SURGERY, V171, P1006, DOI 10.1016/j.surg.2021.10.004
   Kuber PM, 2023, ANN BIOMED ENG, V51, P1910, DOI 10.1007/s10439-023-03292-0
   Kunz Christian, 2020, Current Directions in Biomedical Engineering, V6, DOI 10.1515/cdbme-2020-0027
   Leenders T, 2002, Comput Aided Surg, V7, P99, DOI 10.1002/igs.10033
   Lei PF, 2019, ORTHOP SURG, V11, P914, DOI 10.1111/os.12537
   Liu XY, 2021, BIOMED RES INT, V2021, DOI 10.1155/2021/9717184
   Mei XY, 2019, CAN J SURG, V62, P249, DOI 10.1503/cjs.013118
   Nakamura N, 2009, INT J COMPUT ASS RAD, V4, P157, DOI 10.1007/s11548-009-0286-1
   Nikou C, 1999, LECT NOTES COMPUT SC, V1679, P868
   Nodzo SR, 2018, BONE JOINT J, V100B, P1303, DOI 10.1302/0301-620X.100B10-BJJ-2018-0201.R1
   Pose-Díez-de-la-Lastra A, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22134915
   Richards S, 2023, MED SCI EDUC, V33, P223, DOI 10.1007/s40670-023-01728-9
   Ruggiero F, 2023, J Clin Med, V12
   Sato Y, 2000, LECT NOTES COMPUT SC, V1935, P1114
   Saxler G, 2004, INT ORTHOP, V28, P198, DOI 10.1007/s00264-004-0542-5
   Schulz AP, 2007, INT J MED ROBOT COMP, V3, P301, DOI 10.1002/rcs.161
   Sloan M, 2018, J BONE JOINT SURG AM, V100, P1455, DOI 10.2106/JBJS.17.01617
   Sugano N, 2013, CLIN ORTHOP SURG, V5, P1, DOI 10.4055/cios.2013.5.1.1
   Sullivan KJ, 2016, J ORTHOP SURG RES, V11, DOI 10.1186/s13018-015-0332-3
   Suter D, 2023, EUR SPINE J, V32, P3425, DOI 10.1007/s00586-023-07826-x
   Vassallo R, 2017, Proceedings of the SPIE
   Vávra P, 2017, J HEALTHC ENG, V2017, DOI 10.1155/2017/4574172
   Villegas-Hernandez YS, 2017, INT J INTERACT DES M, V11, P727, DOI 10.1007/s12008-016-0356-x
   Wang L, 2022, INT J UROL, V29, P838, DOI 10.1111/iju.14907
   Wolf J, 2021, INT J COMPUT ASS RAD, V16, P1171, DOI 10.1007/s11548-021-02408-y
   Xiong L, 2019, POSTGRAD MED J, V95, P414, DOI 10.1136/postgradmedj-2019-136482
   Yamada K, 2018, J ARTHROPLASTY, V33, P136, DOI 10.1016/j.arth.2017.08.001
   Yamaguchi M, 2000, J ARTHROPLASTY, V15, P305, DOI 10.1016/S0883-5403(00)90601-6
NR 41
TC 0
Z9 0
U1 7
U2 7
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 39
DI 10.1007/s10055-024-00938-9
PG 14
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA HE8Y7
UT WOS:001157920900001
OA hybrid
DA 2024-08-05
ER

PT J
AU Wang, YB
   Tian, YT
   Liu, FC
   Zhou, HH
   Zhang, YW
AF Wang, Yanbin
   Tian, Yatong
   Liu, Fuchang
   Zhou, Haihai
   Zhang, Yiwen
TI Mixed reality prototyping for usability evaluation in product design: a
   case study of a handheld printer
SO VIRTUAL REALITY
LA English
DT Article
DE Prototyping; Mixed reality; Usability; Product design
ID AUGMENTED REALITY
AB Prototyping is a critical step in the usability evaluation for product design. The maturity and affordability of mixed reality technology provide an opportunity to explore its application in prototyping. This study explored a flexible solution to create the mixed reality prototype for a handheld product by employing 3D printing, interactive 3D simulation, electronic prototyping platform, and Microsoft HoloLens. A comparative experiment was conducted to validate the effectiveness of the proposed prototype solution for usability evaluation. The results demonstrated that usability testing using the mixed prototype can accurately reveal changes in user performance across different task complexities, functional attributes, and physical contexts. The subjective assessments of product usability using the mixed prototype were highly consistent with the actual product. However, the absolute value of performance obtained from usability testing with the mixed prototype may deviate from the true value. In conclusion, mixed prototypes are more suitable for comparing the usability of different design alternatives under different conditions rather than obtaining an absolute measure of usability. This study establishes a significant theoretical foundation for product design assessment utilizing mixed prototypes, while providing practical guidance to designers and developers regarding the evaluation of product usability using mixed prototypes.
C1 [Wang, Yanbin; Tian, Yatong; Zhou, Haihai; Zhang, Yiwen] Nanjing Univ Aeronaut & Astronaut, Dept Ind Design, 29 Yudao St, Nanjing 210016, Jiangsu, Peoples R China.
   [Liu, Fuchang] Hangzhou Normal Univ, Sch Informat Sci & Technol, 2318 Yuhangtang Rd, Hangzhou 311121, Zhejiang, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics; Hangzhou Normal
   University
RP Wang, YB (corresponding author), Nanjing Univ Aeronaut & Astronaut, Dept Ind Design, 29 Yudao St, Nanjing 210016, Jiangsu, Peoples R China.
EM feihongie@gmail.com
FU Research Project on Graduate Education and Teaching Reform at Nanjing
   University of Aeronautics and Astronautics; Humanities and Social
   Sciences Project of the Ministry of Education of China
FX The authors are grateful to the participants for their contributions to
   the research. We also want to thank the anonymous reviewers for their
   valuable comments. This work was supported by "Research Project on
   Graduate Education and Teaching Reform at Nanjing University of
   Aeronautics and Astronautics" and "Humanities and Social Sciences
   Project of the Ministry of Education of China."
CR Akoglu C, 2015, Online J Art Des, V3, P51
   ANDERSON NS, 1988, AM J PSYCHOL, V101, P148, DOI 10.2307/1422802
   Bordegoni M, 2011, INNOVATION IN PRODUCT DESIGN: FROM CAD TO VIRTUAL PROTOTYPING, P1, DOI 10.1007/978-0-85729-775-4
   Brooke J., 1996, Usability Evaluation in Industry, P189
   Bruno F, 2013, ENG COMPUT-GERMANY, V29, P375, DOI 10.1007/s00366-012-0293-7
   Bruno F, 2010, INT J HUM-COMPUT ST, V68, P254, DOI 10.1016/j.ijhcs.2009.12.004
   Camburn BA, 2015, INT DES ENG TECHN C
   Camburn B, 2017, DES SCI, V3, DOI 10.1017/dsj.2017.10
   Choi YM, 2019, J USABILITY STUD, V14, P187
   Cox C., 2022, Proceedings of the Design Society, V2, P353, DOI [https://doi.org/10.1017/pds.2022.37, DOI 10.1017/PDS.2022.37]
   Faust FG, 2019, VIRTUAL REAL-LONDON, V23, P197, DOI 10.1007/s10055-018-0356-1
   Ferrise F, 2017, J INTELL MANUF, V28, P1695, DOI 10.1007/s10845-015-1163-0
   Hallgrimsson B., 2012, Prototyping and Modelmaking for Product Design
   HART S G, 1988, P139
   Kent L, 2021, DESIGN STUD, V77, DOI 10.1016/j.destud.2021.101046
   Lewis JR, 2009, LECT NOTES COMPUT SC, V5619, P94, DOI 10.1007/978-3-642-02806-9_12
   Lewis JR., 2021, Usability and user experience: design and evaluation, P972, DOI [10.1002/9781119636113.ch38, DOI 10.1002/9781119636113.CH38]
   Lund A. M., 2001, Usability Interface, V8, P3, DOI DOI 10.1177/1078087402250360
   MagicLeap, 2023, ABOUT US
   Majrashi K, 2018, LECT NOTES COMPUT SC, V10911, P58, DOI 10.1007/978-3-319-92141-9_5
   Maurya S, 2019, INT J INTERACT DES M, V13, P163, DOI 10.1007/s12008-018-0499-z
   Microflite, 2023, About us
   Morozova A, 2019, DIS '19 COMPANION: COMPANION PUBLICATION OF THE 2019 ACM DESIGNING INTERACTIVE SYSTEMS CONFERENCE, P41, DOI 10.1145/3301019.3325146
   Nielsen J, 1994, USABILITY ENG
   Norman D., 2013, The design of everyday things: Revised and expanded edition
   Park H, 2008, J ENG DESIGN, V19, P359, DOI 10.1080/09544820701474129
   Park H, 2013, COMPUT IND, V64, P854, DOI 10.1016/j.compind.2013.05.006
   PTC, 2023, ABOUT US
   Salinas Elizabeth, 2020, Design, User Experience, and Usability. Interaction Design. 9th International Conference, DUXU 2020 Held as Part of the 22nd HCI International Conference, HCII 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12200), P253, DOI 10.1007/978-3-030-49713-2_18
   Stevens J. P., 2002, APPL MULTIVARIATE ST, DOI DOI 10.4324/9780203843130
   Verlinden J, 2004, DS 32
   Vredenburg K., 2002, Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2002, P471, DOI 10.1145/503376.503460
   Wall M.B., 1992, J RES ENG DESIGN, V3, P163
   Wang G. G., 2002, Transactions of the ASME. Journal of Computing and Information Science in Engineering, V2, P232, DOI 10.1115/1.1526508
   Zhang XT, 2015, LECT NOTES COMPUT SC, V9179, P88, DOI 10.1007/978-3-319-21067-4_11
   Zhou XC, 2019, APPL ERGON, V80, P111, DOI 10.1016/j.apergo.2019.05.007
NR 36
TC 0
Z9 0
U1 18
U2 19
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 6
DI 10.1007/s10055-023-00895-9
PG 15
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA DX0O7
UT WOS:001135265600001
OA Bronze
DA 2024-08-05
ER

PT J
AU Michiels, N
   Jorissen, L
   Put, J
   Liesenborgs, J
   Vandebroeck, I
   Joris, E
   Van Reeth, F
AF Michiels, Nick
   Jorissen, Lode
   Put, Jeroen
   Liesenborgs, Jori
   Vandebroeck, Isjtar
   Joris, Eric
   Van Reeth, Frank
TI Tracking and co-location of global point clouds for large-area indoor
   environments
SO VIRTUAL REALITY
LA English
DT Article
DE Tracking; Colocation; Large-area; Point cloud; Registration
AB Extended reality (XR) experiences are on the verge of becoming widely adopted in diverse application domains. An essential part of the technology is accurate tracking and localization of the headset to create an immersive experience. A subset of the applications require perfect co-location between the real and the virtual world, where virtual objects are aligned with real-world counterparts. Current headsets support co-location for small areas, but suffer from drift when scaling up to larger ones such as buildings or factories. This paper proposes tools and solutions for this challenge by splitting up the simultaneous localization and mapping (SLAM) into separate mapping and localization stages. In the pre-processing stage, a feature map is built for the entire tracking area. A global optimizer is applied to correct the deformations caused by drift, guided by a sparse set of ground truth markers in the point cloud of a laser scan. Optionally, further refinement is applied by matching features between the ground truth keyframe images and their rendered-out SLAM estimates of the point cloud. In the second, real-time stage, the rectified feature map is used to perform localization and sensor fusion between the global tracking and the headset. The results show that the approach achieves robust co-location between the virtual and the real 3D environment for large and complex tracking environments.
C1 [Michiels, Nick; Jorissen, Lode; Put, Jeroen; Liesenborgs, Jori; Van Reeth, Frank] Hasselt Univ Flanders Make, Expertise Ctr Digital Media, Wetenschapspk 2, B-3590 Diepenbeek, Belgium.
   [Vandebroeck, Isjtar; Joris, Eric] CREW Brussels, CREW, Vandernootstr 23-8, B-1080 Sint Jans Molenbeek, Belgium.
RP Michiels, N (corresponding author), Hasselt Univ Flanders Make, Expertise Ctr Digital Media, Wetenschapspk 2, B-3590 Diepenbeek, Belgium.
EM nick.michiels@uhasselt.be; lode.jorissen@uhasselt.be;
   jeroen.put@uhasselt.be; jori.liesenborgs@uhasselt.be;
   isjtar@crew.brussels; eric.joris@crew.brussels;
   frank.vanreeth@uhasselt.be
OI MICHIELS, Nick/0000-0002-7047-5867
FU Flanders Make
FX No Statement Available
CR ARUN KS, 1987, IEEE T PATTERN ANAL, V9, P699, DOI 10.1109/TPAMI.1987.4767965
   Bar-Shalom Y., 2001, Estimation with applications to tracking and navigation -theory algorithms and software
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Campos C, 2021, IEEE T ROBOT, V37, P1874, DOI 10.1109/TRO.2021.3075644
   Durrant-Whyte H., 1996, Robotics Research, V613, P625
   Fang W, 2023, ROBOT CIM-INT MANUF, V83, DOI 10.1016/j.rcim.2023.102567
   Feng MD, 2019, Arxiv, DOI arXiv:1904.09742
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Furtado Joshua S., 2019, Advances in Motion Sensing and Control for Robotic Applications. Selected Papers from the Symposium on Mechatronics, Robotics, and Control (SMRC18)CSME International Congress 2018. Lecture Notes in Mechanical Engineering (LNME), P15, DOI 10.1007/978-3-030-17369-2_2
   Garrido-Jurado S, 2014, PATTERN RECOGN, V47, P2280, DOI 10.1016/j.patcog.2014.01.005
   GREENE N, 1986, IEEE COMPUT GRAPH, V6, P21, DOI 10.1109/MCG.1986.276658
   Jorissen L, 2014, LECT NOTES COMPUT SC, V8853, P210, DOI 10.1007/978-3-319-13969-2_17
   Kalman R.E., 1960, Trans ASME J Basic Eng, V82, P35, DOI 10.1115/1.3662552
   Kazerouni IA, 2022, EXPERT SYST APPL, V205, DOI 10.1016/j.eswa.2022.117734
   Lang P, 2002, IEEE IMTC P, P1583, DOI 10.1109/IMTC.2002.1007196
   Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6
   Li JX, 2021, Arxiv, DOI arXiv:2104.03501
   Maesen Steven., 2013, VRST 13, P101, DOI [10.1145/2503713.2503733, DOI 10.1145/2503713.2503733]
   McGill M., 2020, 26 ACM S VIRT REAL S, DOI [10.1145/3385956.3418968, DOI 10.1145/3385956.3418968]
   Mortezapoor S, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P92, DOI 10.1109/VRW55335.2022.00033
   Niehorster DC, 2017, I-PERCEPTION, V8, DOI 10.1177/2041669517708205
   Podkosova I, 2016, WORK SOFTW ENG
   Reimer D, 2021, COMPUTERS, V10, DOI 10.3390/computers10050058
   Ren SY, 2023, IEEE T CIRC SYST VID, V33, P1198, DOI 10.1109/TCSVT.2022.3208859
   Strasdat H, 2010, IEEE INT CONF ROBOT, P2657, DOI 10.1109/ROBOT.2010.5509636
   Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298
   Wang AR, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300248
   Wright MJ, 1997, INT C MATH COMP
   Yi X, 2023, EgoLocate: real-time motion capture, localization, and mapping with sparse body-mounted sensors
NR 30
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAY 4
PY 2024
VL 28
IS 2
AR 106
DI 10.1007/s10055-024-01004-0
PG 16
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA QN1R3
UT WOS:001221464700001
OA hybrid, Green Submitted
DA 2024-08-05
ER

PT J
AU Sansoni, M
   Varallo, G
   Malighetti, C
   Tuena, C
   Di Lernia, D
   Cesa, GL
   Manzoni, GM
   Castelnuovo, G
   Riva, G
AF Sansoni, Maria
   Varallo, Giorgia
   Malighetti, Clelia
   Tuena, Cosimo
   Di Lernia, Daniele
   Cesa, Gian Luca
   Manzoni, Gian Mauro
   Castelnuovo, Gianluca
   Riva, Giuseppe
TI Unlocking the potential of virtual reality to expand treatment frontiers
   for bulimia nervosa: a pilot study to explore the impact of virtual
   reality-enhanced cognitive-behavioral therapy
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual Reality; Bulimia Nervosa; Eating Disorder; Body Image;
   Allocentric Lock Theory; Body Memory
ID BINGE-EATING DISORDER; BODY-IMAGE; ANOREXIA-NERVOSA; PSYCHOLOGY;
   NEUROVR; VR; DISTURBANCES; VALIDITY; PLATFORM; OBESITY
AB The primary objective of this study is to assess the efficacy of a Virtual Reality (VR) intervention when compared to an integrated multimodal medically managed Inpatient Program (IP) in a cohort of 24 female patients diagnosed with Bulimia Nervosa (BN). Psychological measures (i.e., EDI-2) were assessed at three points: pre-treatment, post-treatment, and at 1-month follow-up. Behavioral measures (i.e., BMI) were evaluated at 6 different time points, instead (i.e., pre-treatment, post-treatment, 3, 6, 9, and 12 months from the discharge date). The VR treatment was more effective in improving the EDI subscales EDI-DT (i.e., drive for thinness) and EDI-BU (i.e., binging-purging behaviors). In particular, patients in the VR condition showed a reduced EDI-BU score at 1-month follow-up and post-test in comparison to the pre-test, as well as a lower EDI-DT score at 1-month follow-up compared to the pre-test. Conversely, no significant changes were noted in the IP group for either subscale. Regarding the behavioral measures, the group undergoing the VR condition reported the maintenance of the BMI in the long term compared to the IP. Specifically, in the VR group BMI decreased from the pre- to post-test, and from the pre-test to the 12-month follow-up. In the IP group, BMI improved from the pre- to the post-test, and from the pre-test to the 12-month follow-up. However, a relapse pattern was observed in the IP condition during the follow-up period, with a significant BMI increase from the post-test to the 9-month follow-up, from the 3 to the 9-month follow-up, from the 6 to the 9-month follow-up, and a decrease of BMI between the 9 and the 12-month follow-up. In conclusion, these results suggest that integrating VR treatment into the care of individuals with BN could enhance both immediate and sustained treatment outcomes. This may offer valuable insights for future studies to expand and delve deeper into the field of EDs.
C1 [Sansoni, Maria; Malighetti, Clelia; Di Lernia, Daniele; Castelnuovo, Gianluca] Catholic Univ Sacred Heart Milan, Dept Psychol, Largo Agostino Gemelli 1, I-20123 Milan, Italy.
   [Varallo, Giorgia] Univ Modena & Reggio Emilia, Dept Biomed Metab & Neural Sci, Via Giuseppe Campi,287, I-41125 Modena, Italy.
   [Tuena, Cosimo; Riva, Giuseppe] IRCCS Ist Auxol Italiano, Appl Technol Neuropsychol Lab, Via Magnasco 2, I-20149 Milan, Italy.
   [Di Lernia, Daniele; Riva, Giuseppe] Catholic Univ Sacred Heart Milan, Humane Technol Lab, Largo Agostino Gemelli,1, I-20123 Milan, Italy.
   [Cesa, Gian Luca] Ospedali Privati Forli Villa Igea, Ctr Obes & Nutr Clin CONC, Viale Antonio Gramsci,42, I-47122 Forli, Italy.
   [Tuena, Cosimo; Manzoni, Gian Mauro] eCampus Univ, Fac Psychol, Via Isimbardi 10, I-22060 Novedrate, CO, Italy.
   [Castelnuovo, Gianluca] IRCCS Ist Auxol Italiano, San Giuseppe Hosp, Clin Psychol Res Lab, I-28824 Verbania, Italy.
C3 Catholic University of the Sacred Heart; Universita di Modena e Reggio
   Emilia; IRCCS Istituto Auxologico Italiano; Catholic University of the
   Sacred Heart; Universita Ecampus; IRCCS Istituto Auxologico Italiano
RP Sansoni, M (corresponding author), Catholic Univ Sacred Heart Milan, Dept Psychol, Largo Agostino Gemelli 1, I-20123 Milan, Italy.
EM maria.sansoni@unicatt.it
RI varallo, giorgia/AAA-5240-2020
OI Sansoni, Maria/0000-0002-5189-7159; Castelnuovo,
   Gianluca/0000-0003-2633-9822
FU Ministero dell'Istruzione, dell'Universit e della Ricerca
FX No Statement Available
CR Abbate-Daga G, 2007, PSYCHIAT RES, V149, P215, DOI 10.1016/j.psychres.2005.10.017
   Amianto F, 2015, BMC PSYCHIATRY, V15, DOI 10.1186/s12888-015-0445-6
   [Anonymous], 2013, Diagnostic and statistical manual of mental disorders, V5th, DOI 10.1176/appi.books.9780890425596
   [Anonymous], 1995, EDI-2, Eating Disorder Inventory-2
   [Anonymous], 2004, Psicol Conductual
   Barry DT, 2003, J NERV MENT DIS, V191, P589, DOI 10.1097/01.nmd.0000087185.95446.65
   Brink P.J., 1998, ADV DESIGN NURSING R
   Brizzi G, 2023, J EAT DISORD, V11, DOI 10.1186/s40337-023-00930-9
   Brizzi G, 2023, CYBERPSYCH BEH SOC N, V26, P141, DOI 10.1089/cyber.2023.29267.ceu
   BRUCH H, 1962, PSYCHOSOM MED, V24, P187, DOI 10.1097/00006842-196203000-00009
   BUTTERS JW, 1987, J CONSULT CLIN PSYCH, V55, P889, DOI 10.1037/0022-006X.55.6.889
   Carroll K.M., 1998, A Cognitive-behavioral Approach: Treating Cocaine Addiction, V1
   Cash TF., 2004, BODY IMAGE HDB THEOR
   Cesa GL, 2013, J MED INTERNET RES, V15, DOI 10.2196/jmir.2441
   Chorzepa M, 2023, CYBERPSYCH BEH SOC N, V26, P453, DOI 10.1089/cyber.2023.29281.ceu
   Cipolletta S, 2017, PSYCHIAT RES, V252, P87, DOI 10.1016/j.psychres.2017.02.060
   Clus D, 2018, J MED INTERNET RES, V20, DOI 10.2196/jmir.7898
   Dakanalis A, 2017, INT J EAT DISORDER, V50, P917, DOI 10.1002/eat.22696
   Dakanalis A, 2015, EUR CHILD ADOLES PSY, V24, P997, DOI 10.1007/s00787-014-0649-1
   de Carvalho MR, 2017, BEHAV SCI-BASEL, V7, DOI 10.3390/bs7030043
   Degortes D., 2018, Body image disturbances in Bulimia nervosa. Body image, eating, and weight
   Di Lernia D, 2019, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.02488
   Diener E., 2024, Noba textbook series: Psychology
   Fernandez-Alvarez J, 2021, INTERNET INTERV, V25, DOI 10.1016/j.invent.2021.100407
   Fernández-Alvarez J, 2020, ADV EXP MED BIOL, V1191, P389, DOI 10.1007/978-981-32-9705-0_21
   Ferrer-Garcia M, 2019, CYBERPSYCH BEH SOC N, V22, P60, DOI 10.1089/cyber.2017.0675
   Ferrer-García M, 2017, EUR EAT DISORD REV, V25, P479, DOI 10.1002/erv.2538
   Garner DM, 1991, Eating disorder inventory-2: professional manual, DOI DOI 10.1249/00005768-199301000-00010
   Gaudio S, 2013, BIOL PSYCHIAT, V73, pE25, DOI 10.1016/j.biopsych.2012.08.028
   Gorini A, 2010, ANN GEN PSYCHIATR, V9, DOI 10.1186/1744-859X-9-30
   Grilo CM, 2019, INT J EAT DISORDER, V52, P1229, DOI 10.1002/eat.23161
   Juarascio A, 2018, COGN BEHAV PRACT, V25, P391, DOI 10.1016/j.cbpra.2017.09.004
   Lampard AM, 2015, AUST PSYCHOL, V50, P6, DOI 10.1111/ap.12078
   Linardon J, 2017, J CONSULT CLIN PSYCH, V85, P1080, DOI 10.1037/ccp0000245
   Maldonado JG., 2017, The Oxford handbook of eating disorders, V2
   Malighetti C, 2022, J CLIN MED, V11, DOI 10.3390/jcm11237134
   Malighetti C, 2016, ANN REV CYBERTHERAPY, V14, P78
   Manzoni GM, 2016, CYBERPSYCH BEH SOC N, V19, P134, DOI 10.1089/cyber.2015.0208
   Marco JH, 2013, PSYCHIAT RES, V209, P619, DOI 10.1016/j.psychres.2013.02.023
   Maxwell SE, 2015, AM PSYCHOL, V70, P487, DOI 10.1037/a0039400
   Meadows A., 2018, Body image, eating, and weight: a guide to assessment, treatment, and prevention, P381
   Munsch S, 2012, PSYCHIAT RES, V195, P118, DOI 10.1016/j.psychres.2011.07.016
   Pollatos O, 2012, PAIN, V153, P1680, DOI 10.1016/j.pain.2012.04.030
   Riva G, 2004, ST HEAL T, V99, P121
   Riva G, 2003, CYBERPSYCHOL BEHAV, V6, P251, DOI 10.1089/109493103322011533
   Riva G, 2001, CYBERPSYCHOL BEHAV, V4, P511, DOI 10.1089/109493101750527079
   Riva G, 2023, PSYCHOSOM MED, V85, P639, DOI 10.1097/PSY.0000000000001198
   Riva G, 2021, ANN REV CYBERTHERAPY, V19, P3, DOI 10.3390/Fijerph19031525
   Riva G, 2021, FRONT SYST NEUROSCI, V15, DOI 10.3389/fnsys.2021.749268
   Riva G, 2021, CLIN PSYCHOL PSYCHOT, V28, P477, DOI 10.1002/cpp.2622
   Riva G, 2018, CONSCIOUS COGN, V59, P57, DOI 10.1016/j.concog.2017.08.006
   Riva G, 2018, CORTEX, V104, P241, DOI 10.1016/j.cortex.2017.07.013
   Riva G, 2018, FRONT HUM NEUROSCI, V12, DOI 10.3389/fnhum.2018.00049
   Riva G, 2015, EUR PSYCHOL, V20, P34, DOI 10.1027/1016-9040/a000190
   Riva G, 2014, FRONT HUM NEUROSCI, V8, DOI 10.3389/fnhum.2014.00236
   Riva G, 2012, MED HYPOTHESES, V79, P113, DOI 10.1016/j.mehy.2012.03.036
   Riva Giuseppe, 2011, J Diabetes Sci Technol, V5, P283
   Riva G, 2011, STUD HEALTH TECHNOL, V163, P493, DOI 10.3233/978-1-60750-706-2-493
   Riva G, 2009, STUD HEALTH TECHNOL, V144, P57, DOI 10.3233/978-1-60750-017-9-57
   Riva G, 2007, STUD HEALTH TECHNOL, V125, P394
   Ruuska J, 2005, Eat Weight Disord, V10, P98
   Sansoni M, 2023, ANN REV CYBERTHERAPY, V21, P185
   Sansoni M, 2022, LECT NOTES COMPUT SC, V13446, P432, DOI 10.1007/978-3-031-15553-6_30
   Sansoni Maria, 2022, Cyberpsychol Behav Soc Netw, V25, P620, DOI 10.1089/cyber.2022.29255.ceu
   Sansoni M, 2022, FRONT HUM NEUROSCI, V16, DOI 10.3389/fnhum.2022.916227
   Schwartz Marlene B, 2004, Body Image, V1, P43, DOI 10.1016/S1740-1445(03)00007-X
   Serino S, 2015, PSYCHIAT RES, V230, P732, DOI 10.1016/j.psychres.2015.10.025
   Shrout PE, 2018, ANNU REV PSYCHOL, V69, P487, DOI 10.1146/annurev-psych-122216-011845
   Slade E, 2018, PSYCHOL MED, V48, P2629, DOI 10.1017/S0033291718001071
   Slade P.D., 1990, PSYCHOL HEALTH, V4, P213, DOI DOI 10.1080/08870449008400391
   Spagnolli A, 2005, PSYCHNOLOGY J, V3, P6
   Stein KF, 2015, EAT BEHAV, V19, P5, DOI 10.1016/j.eatbeh.2015.06.001
   Stice E, 1998, BEHAV THER, V29, P257, DOI 10.1016/S0005-7894(98)80006-3
   Tackett JL, 2019, ANNU REV CLIN PSYCHO, V15, P579, DOI 10.1146/annurev-clinpsy-050718-095710
   Vila A., 2023, Sexual education around the world-past, present and future issues, P103, DOI [10.5772/intechopen.1001854, DOI 10.5772/INTECHOPEN.1001854]
   Weingardt KR, 2009, J SUBST ABUSE TREAT, V37, P219, DOI 10.1016/j.jsat.2009.01.002
   Wooley SC., 1985, Handbook of psychotherapy for Anorexia and Bulimia, P120
   Zanetti T, 2013, EUR EAT DISORD REV, V21, P32, DOI 10.1002/erv.2190
NR 78
TC 0
Z9 0
U1 6
U2 6
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR 18
PY 2024
VL 28
IS 2
AR 79
DI 10.1007/s10055-024-00971-8
PG 13
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA LL1S7
UT WOS:001186870300001
OA hybrid
DA 2024-08-05
ER

PT J
AU Guillen-Sanz, H
   Checa, D
   Miguel-Alonso, I
   Bustillo, A
AF Guillen-Sanz, Henar
   Checa, David
   Miguel-Alonso, Ines
   Bustillo, Andres
TI A systematic review of wearable biosensor usage in immersive virtual
   reality experiences
SO VIRTUAL REALITY
LA English
DT Article
DE Biofeedback; Biosensors; Head-mounted displays; Heart rate; Physiology;
   Virtual reality
ID HEART-RATE; ENVIRONMENT; STRESS; EEG; REACTIVITY; IMPACT; PAIN
AB Wearable biosensors are increasingly incorporated in immersive Virtual Reality (iVR) applications. A trend that is attributed to the availability of better quality, less costly, and easier-to-use devices. However, consensus is yet to emerge over the most optimal combinations. In this review, the aim is to clarify the best examples of biosensor usage in combination with iVR applications. The high number of papers in the review (560) were classified into the following seven fields of application: psychology, medicine, sports, education, ergonomics, military, and tourism and marketing. The use of each type of wearable biosensor and Head-Mounted Display was analyzed for each field of application. Then, the development of the iVR application is analyzed according to its goals, user interaction levels, and the possibility of adapting the iVR environment to biosensor feedback. Finally, the evaluation of the iVR experience was studied, considering such issues as sample size, the presence of a control group, and post-assessment routines. A working method through which the most common solutions, the best practices, and the most promising trends in biofeedback-based iVR applications were identified for each field of application. Besides, guidelines oriented towards good practice are proposed for the development of future iVR with biofeedback applications. The results of this review suggest that the use of biosensors within iVR environments need to be standardized in some fields of application, especially when considering the adaptation of the iVR experience to real-time biosignals to improve user performance.
C1 [Guillen-Sanz, Henar; Checa, David; Miguel-Alonso, Ines; Bustillo, Andres] Univ Burgos, Dept Comp Engn, Avda Cantabria S-N, Burgos 09006, Spain.
C3 Universidad de Burgos
RP Bustillo, A (corresponding author), Univ Burgos, Dept Comp Engn, Avda Cantabria S-N, Burgos 09006, Spain.
EM hguillen@ubu.es; dcheca@ubu.es; imalonso@ubu.es; abustillo@ubu.es
RI ; Checa Cruz, David/J-2839-2017; Bustillo, Andres/I-1403-2015
OI Guillen-Sanz, Henar/0000-0001-5543-6504; Checa Cruz,
   David/0000-0001-6623-3614; Bustillo, Andres/0000-0003-2855-7532; , Ines
   Miguel-Alonso/0000-0001-8882-7587
FU CRUE-CSIC agreement; Springer Nature; ACIS project of the Consejeria de
   Empleo e Industria of the Junta de Castilla y Leon
   [INVESTUN/21/BU/0002]; Erasmus + RISKREAL Project of the European
   Commission [2020-1-ES01-KA204-081847]; HumanAid Project of the Proyectos
   Estrategicos Orientados a la Transicion Ecologica y a la Transicion
   Digital of the Spanish Ministry of Science and Innovation
   [TED2021-129485B-C43]
FX Open Access funding provided thanks to the CRUE-CSIC agreement with
   Springer Nature. Partial financial support was received from the ACIS
   project (Reference Number INVESTUN/21/BU/0002) of the Consejeria de
   Empleo e Industria of the Junta de Castilla y Leon (ES), the Erasmus +
   RISKREAL Project (Reference Number 2020-1-ES01-KA204-081847) of the
   European Commission, and the HumanAid Project (Reference Number
   TED2021-129485B-C43) of the Proyectos Estrategicos Orientados a la
   Transicion Ecologica y a la Transicion Digital of the Spanish Ministry
   of Science and Innovation.
CR Adhyaru JS, 2022, DIGIT HEALTH, V8, DOI 10.1177/20552076221084473
   Alaterre C, 2020, J CLIN MED, V9, DOI 10.3390/jcm9010215
   Alneyadi M, 2021, JMIR MHEALTH UHEALTH, V9, DOI 10.2196/26038
   Alvear-Suarez A, 2019, 2019 IEEE INT C E HL, P1, DOI [10.1109/HealthCom46333.2019.9009603, DOI 10.1109/HEALTHCOM46333.2019.9009603]
   Alyan E, 2021, INT J ENV RES PUB HE, V18, DOI 10.3390/ijerph182111420
   Anthes C, 2016, IEEE AEROSPACE C IEE
   Antoniou PE, 2020, JMIR SERIOUS GAMES, V8, DOI 10.2196/17823
   Aria M, 2017, J INFORMETR, V11, P959, DOI 10.1016/j.joi.2017.08.007
   Arroyo-Ferrer A, 2021, J INTEGR NEUROSCI, V20, P449, DOI 10.31083/j.jin2002047
   Awada M, 2021, ADV ENG INFORM, V47, DOI 10.1016/j.aei.2020.101227
   Badke CM, 2022, FRONT DIGIT HEALTH, V4, DOI 10.3389/fdgth.2022.867961
   Bai Yiwei, 2022, Application of Intelligent Systems in Multi-modal Information Analytics: The 4th International Conference on Multi-modal Information Analytics (ICMMIA 2022). Lecture Notes on Data Engineering and Communications Technologies (136), P759, DOI 10.1007/978-3-031-05237-8_94
   Binsch O, 2021, MIL PSYCHOL, V33, P182, DOI 10.1080/08995605.2021.1897494
   Birenboim A, 2021, INT J ENV RES PUB HE, V18, DOI 10.3390/ijerph18020364
   Blum J, 2020, APPL PSYCHOPHYS BIOF, V45, P153, DOI 10.1007/s10484-020-09468-x
   Bourassa KJ, 2020, PSYCHOSOM MED, V82, P108, DOI 10.1097/PSY.0000000000000758
   Brivio E, 2021, VIRTUAL REAL-LONDON, V25, P303, DOI 10.1007/s10055-020-00453-7
   Brundage SB, 2016, J FLUENCY DISORD, V50, P85, DOI 10.1016/j.jfludis.2016.10.001
   Burin D, 2020, NEUROIMAGE, V222, DOI 10.1016/j.neuroimage.2020.117297
   Calogiuri G, 2018, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.02321
   Campbell J, 2019, PROCEEDINGS OF THE 31ST EUROPEAN CONFERENCE ON COGNITIVE ERGONOMICS: DESIGN FOR COGNITION (ECCE 2019), P177, DOI 10.1145/3335082.3335087
   Cao LZ, 2021, VIRTUAL REAL-LONDON, V25, P597, DOI 10.1007/s10055-020-00477-z
   Caserman P, 2022, IEEE T GAMES, V14, P706, DOI 10.1109/TG.2022.3148791
   Charoensook T, 2019, IEEE INT CONF SERIOU, DOI [10.1109/ICIPRM.2019.8819119, 10.1109/segah.2019.8882434]
   Checa D, 2023, VIRTUAL REAL-LONDON, V27, P3301, DOI 10.1007/s10055-021-00607-1
   Checa D, 2020, MULTIMED TOOLS APPL, V79, P5501, DOI 10.1007/s11042-019-08348-9
   Chen N, 2021, INT J ENV RES PUB HE, V18, DOI 10.3390/ijerph181910203
   Chen SW, 2021, ADV HEALTHC MATER, V10, DOI 10.1002/adhm.202100116
   Chinazzo G, 2021, HUM FACTORS, V63, P474, DOI 10.1177/0018720819892383
   Cohen MX, 2017, TRENDS NEUROSCI, V40, P208, DOI 10.1016/j.tins.2017.02.004
   Crosswell L, 2022, BEHAV INFORM TECHNOL, V41, P864, DOI 10.1080/0144929X.2020.1838609
   Darnall BD, 2020, JMIR FORM RES, V4, DOI 10.2196/17293
   De Luca Carlo, 2006, ENCY MED DEVICES INS
   Debska M, 2019, INT J ENV RES PUB HE, V16, DOI 10.3390/ijerph16193673
   Delvigne V, 2022, IEEE T CIRC SYST VID, V32, P2612, DOI 10.1109/TCSVT.2021.3061719
   Dings SJM, 2021, BJU INT, V128, P561, DOI 10.1111/bju.15332
   Erkan I, 2021, ARCHIT SCI REV, V64, P336, DOI 10.1080/00038628.2020.1858574
   Felix RB, 2021, BMJ OPEN, V11, DOI 10.1136/bmjopen-2021-056030
   Feng ZA, 2018, COMPUT EDUC, V127, P252, DOI 10.1016/j.compedu.2018.09.002
   Finseth T, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12052289
   Fominykh M., 2018, Journal of Interactive Learning Research, V29, P51
   Garcia-Agundez A, 2019, GAMES HEALTH J, V8, P439, DOI 10.1089/g4h.2019.0045
   Gerber SM, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-13153-1
   Halbig A, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.694567
   Haleem A, 2021, Sens Int, V2, DOI [DOI 10.1016/J.SINTL.2021.100100, 10.1016/J.SINTL.2021.100100]
   Hamilton D, 2021, J COMPUT EDUC, V8, P1, DOI 10.1007/s40692-020-00169-2
   Hopper Susan I, 2019, JBI Database System Rev Implement Rep, V17, P1855, DOI 10.11124/JBISRIR-2017-003848
   Horvat N, 2022, ADV ENG INFORM, V54, DOI 10.1016/j.aei.2022.101760
   Hosny YSS, 2020, 2020 IEEE GRAPHICS AND MULTIMEDIA (GAME), P13, DOI 10.1109/GAME50158.2020.9315059
   Houzangbe S., 2018, P 13 INT C FOUND DIG, P1
   Houzangbe S, 2020, VIRTUAL REAL-LONDON, V24, P665, DOI 10.1007/s10055-020-00429-7
   Huang DJ, 2020, 2020 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2020), P172, DOI 10.1109/CW49994.2020.00036
   Huang XT, 2020, ASIA PAC J TOUR RES, V25, P736, DOI 10.1080/10941665.2019.1711141
   Huang YZ, 2022, COMPUT EDUC, V184, DOI 10.1016/j.compedu.2022.104503
   Hubbard R, 2017, SEVENTH INTERNATIONAL LEARNING ANALYTICS & KNOWLEDGE CONFERENCE (LAK'17), P398, DOI 10.1145/3027385.3027390
   Irshad S, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11177822
   Jacob D, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-12822-0
   Kaminska D, 2020, IEEE ACCESS, V8, P200351, DOI 10.1109/ACCESS.2020.3035540
   Kampa M, 2022, TRIALS, V23, DOI 10.1186/s13063-022-06307-8
   Kaniusas E., 2012, Biomedical Signals and Sensors, VI, P1, DOI [10.1007/978-3-642-24843-6_1, DOI 10.1007/978-3-642-24843-6_1]
   Kaur R, 2019, PROC FRONT EDUC CONF, DOI [10.1109/Humanoids43949.2019.9035020, 10.1109/fie43999.2019.9028452]
   Kim H, 2021, FRONT PSYCHIATRY, V12, DOI 10.3389/fpsyt.2021.614539
   Kim H, 2021, INT CONF UBIQUIT INF, DOI 10.1109/IMCOM51814.2021.9377387
   Kim J, 2019, NAT BIOTECHNOL, V37, P389, DOI 10.1038/s41587-019-0045-y
   Kim T, 2018, ACM INT CONF PR SER, DOI 10.1145/3174910.3174941
   Kostakos P, 2021, SIMULAT GAMING, V52, P107, DOI 10.1177/1046878120944567
   Kotsopoulos KI, 2021, 2021 12 INT C INF IN, P1, DOI [10.1109/IISA52424.2021.9555542, DOI 10.1109/IISA52424.2021.9555542]
   Krisch KA, 2020, LEARN MOTIV, V71, DOI 10.1016/j.lmot.2020.101650
   Lai B, 2020, JMIR SERIOUS GAMES, V8, P304, DOI 10.2196/20667
   Lan KC, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21165462
   Leitner MC, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0249762
   Li HQD, 2021, INT J ENV RES PUB HE, V18, DOI 10.3390/ijerph18179053
   Li X, 2020, 2020 5TH IEEE INTERNATIONAL CONFERENCE ON BIG DATA ANALYTICS (IEEE ICBDA 2020), P150, DOI 10.1109/ICBDA49040.2020.9101254
   Li XJ, 2021, J CENT SOUTH UNIV, V28, P3871, DOI 10.1007/s11771-021-4882-8
   Liao Justin Andrew, 2018, International Journal of Machine Learning and Computing, V8, P404, DOI 10.18178/ijmlc.2018.8.4.720
   Luangrath AW, 2022, J MARKETING RES, V59, P306, DOI 10.1177/00222437211059540
   Lüddecke R, 2022, APPL PSYCHOPHYS BIOF, V47, P1, DOI 10.1007/s10484-021-09529-9
   Malta LS, 2021, VIRTUAL REAL-LONDON, V25, P293, DOI 10.1007/s10055-020-00455-5
   Mancuso V., 2022, Comprehensive clinical psychology, V10, P28, DOI [10.1016/B978-0-12-818697-8.00002-9, DOI 10.1016/B978-0-12-818697-8.00002-9]
   Maples-Keller JL, 2017, DEPRESS ANXIETY, V34, P610, DOI 10.1002/da.22626
   Marchiori E, 2018, INF TECHNOL TOUR, V18, P133, DOI 10.1007/s40558-018-0104-0
   McClinton W, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3312968
   Menekli T, 2022, PAIN MANAG NURS, V23, P585, DOI 10.1016/j.pmn.2022.03.004
   Michela A, 2022, FRONT PSYCHOL, V13, DOI 10.3389/fpsyg.2022.806163
   Mihara K, 2022, SCI TECHNOL BUILT EN, V28, P547, DOI 10.1080/23744731.2022.2049639
   Min S., 2020, OPTO-ELECT COMMUN C, P1, DOI DOI 10.1109/oecc48412.2020.9273526
   Mladenovic R, 2021, J STOMATOL ORAL MAXI, V122, pE15, DOI 10.1016/j.jormas.2021.03.009
   Moro C, 2021, ANAT SCI EDUC, V14, P368, DOI 10.1002/ase.2049
   Muñoz JE, 2020, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.00683
   Narasimha S, 2019, APPL ERGON, V80, P175, DOI 10.1016/j.apergo.2019.05.009
   Narciso D, 2020, IEEE INT C BIOINF BI, P813, DOI 10.1109/BIBE50027.2020.00138
   Narciso D, 2021, MULTIMED TOOLS APPL, V80, P13195, DOI 10.1007/s11042-020-10454-y
   Navarro Diego, 2021, Proceedings of the ACM on Human-Computer Interaction, V5, DOI 10.1145/3474695
   Nouri S, 2022, PSYCHONEUROENDOCRINO, V138, DOI 10.1016/j.psyneuen.2022.105682
   Page MJ, 2021, BMJ-BRIT MED J, V372, DOI [10.1016/j.ijsu.2021.105906, 10.1136/bmj.n71, 10.1136/bmj.n160]
   Pallavicini F, 2019, SIMULAT GAMING, V50, P136, DOI 10.1177/1046878119831420
   Park J, 2017, J PHYSIOL-LONDON, V595, P4893, DOI 10.1113/JP274269
   Parong J, 2021, ETR&D-EDUC TECH RES, V69, P1433, DOI 10.1007/s11423-021-09999-y
   Patsaki I, 2022, FRONT SYST NEUROSCI, V16, DOI 10.3389/fnsys.2022.880447
   Posada-Quintero HF, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20020479
   Prabhu VG, 2020, INT J SEMANT COMPUT, V14, P375, DOI 10.1142/S1793351X20400152
   Qidwai U, 2019, J BODYW MOV THER, V23, P425, DOI 10.1016/j.jbmt.2019.02.012
   Radhakrishnan U, 2021, BEHAV INFORM TECHNOL, V40, P1310, DOI 10.1080/0144929X.2021.1954693
   Rockstroh C, 2019, INT J HUM-COMPUT ST, V130, P209, DOI 10.1016/j.ijhcs.2019.06.011
   Roo JS, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1459, DOI 10.1145/3025453.3025743
   Rossi S, 2023, Arxiv, DOI [arXiv:2112.09402, 10.48550/arXiv.2112.09402, DOI 10.48550/ARXIV.2112.09402]
   Rutkowski S, 2021, FRONT PHYSIOL, V12, DOI 10.3389/fphys.2021.702266
   Saeidi S, 2021, J BUILD ENG, V44, DOI 10.1016/j.jobe.2021.102918
   Sajno E, 2022, ANN REV CYBERTHERAPY, V20, P17
   Salminen M, 2022, IEEE T AFFECT COMPUT, V13, P746, DOI 10.1109/TAFFC.2019.2958657
   Sansoni M, 2022, FRONT HUM NEUROSCI, V16, DOI 10.3389/fnhum.2022.916227
   Schormann I, 2020, P C MENSCH COMP, P495, DOI [10.1145/3404983.3410416, DOI 10.1145/3404983.3410416]
   Serrano-Mamolar A, 2023, COMUNICAR, V31, P9, DOI 10.3916/C76-2023-01
   Setiawan Abas, 2018, 2018 3rd International Seminar on Application for Technology of Information and Communication. Proceedings, P400, DOI 10.1109/ISEMANTIC.2018.8549776
   Shadiev R, 2023, COMPUT EDUC, V196, DOI 10.1016/j.compedu.2022.104681
   Shoko K, 2021, LAV VIRT VRIC CONVRG, P169
   Skarbez R, 2021, IEEE T VIS COMPUT GR, V27, P3839, DOI 10.1109/TVCG.2020.2983701
   Skola F, 2018, COMPUT GRAPH-UK, V75, P59, DOI 10.1016/j.cag.2018.05.024
   Solcà M, 2018, NEUROLOGY, V91, pE479, DOI 10.1212/WNL.0000000000005905
   Soyka F., 2016, Proceedings of the ACM symposium on applied perception. SAP'16, P85, DOI [DOI 10.1145/2931002.2931017, 10.1145/2931002.2931017.]
   Spicer R, 2017, P IEEE VIRT REAL ANN, P385, DOI 10.1109/VR.2017.7892338
   Stavroulia KE, 2019, INT J INF LEARN TECH, V36, P192, DOI 10.1108/IJILT-11-2018-0127
   Suh A, 2018, COMPUT HUM BEHAV, V86, P77, DOI 10.1016/j.chb.2018.04.019
   Sundstedt V, 2022, FRONT NEUROERGONOM, V3, DOI 10.3389/fnrgo.2022.910019
   Tardif N, 2019, CYBERPSYCH BEH SOC N, V22, P39, DOI 10.1089/cyber.2017.0711
   Tinga AM, 2019, APPL PSYCHOPHYS BIOF, V44, P51, DOI 10.1007/s10484-018-9421-5
   Vanegas E, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20185446
   Varela-Aldás J, 2019, LECT NOTES COMPUT SC, V11613, P175, DOI 10.1007/978-3-030-25965-5_14
   Verzwyvelt LA, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-99763-2
   Weibel RP, 2023, COMPUT HUM BEHAV, V141, DOI 10.1016/j.chb.2022.107607
   Wen D, 2021, IEEE J BIOMED HEALTH, V25, P3278, DOI 10.1109/JBHI.2020.3047836
   Wen TY, 2022, IEEE ACCESS, V10, P18370, DOI 10.1109/ACCESS.2022.3148380
   Winter C, 2021, J NEUROENG REHABIL, V18, DOI 10.1186/s12984-021-00848-w
   Won M, 2023, COMPUT EDUC, V195, DOI 10.1016/j.compedu.2022.104701
   Wright BJ, 2022, ATTEN PERCEPT PSYCHO, V84, P383, DOI 10.3758/s13414-022-02454-x
   Wu JY, 2022, BIOSENSORS-BASEL, V12, DOI 10.3390/bios12121097
   Xiao WQ, 2020, INT J DISTRIB SENS N, V16, DOI 10.1177/1550147720917646
   Yang XZ, 2019, COMPUT HUM BEHAV, V99, P345, DOI 10.1016/j.chb.2019.06.002
   Yeom S, 2021, BUILD ENVIRON, V204, DOI 10.1016/j.buildenv.2021.108134
   Yin J, 2020, ENVIRON INT, V136, DOI 10.1016/j.envint.2019.105427
   Younis H, 2019, 15 INT C EMERGING TE, P1, DOI 10.1109/ICET48972.2019.8994550
   Zhang MH, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR), P159, DOI 10.1109/AIVR46125.2019.00032
   Zhang ZJ, 2021, INT J ENV RES PUB HE, V18, DOI 10.3390/ijerph18189677
   Zhao Q, 2009, SCI CHINA SER F, V52, P348, DOI 10.1007/s11432-009-0066-0
NR 144
TC 3
Z9 4
U1 7
U2 7
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR 8
PY 2024
VL 28
IS 2
AR 74
DI 10.1007/s10055-024-00970-9
PG 28
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA LG7F1
UT WOS:001185691200005
OA hybrid
DA 2024-08-05
ER

PT J
AU Masneri, S
   Domínguez, A
   Pacho, G
   Zorrilla, M
   Larrañaga, M
   Arruarte, A
AF Masneri, Stefano
   Dominguez, Ana
   Pacho, Guillermo
   Zorrilla, Mikel
   Larranaga, Mikel
   Arruarte, Ana
TI A collaborative AR application for education: from architecture design
   to user evaluation
SO VIRTUAL REALITY
LA English
DT Article
DE Augmented reality; Multi-user interactions; Collaborative learning; User
   evaluation; Data analysis
ID AUGMENTED REALITY APPLICATIONS; TECHNOLOGY; GEOGRAPHY; WORLD
AB Augmented reality applications can be used in an educational context to facilitate learning. In particular, augmented reality has been successfully used as a tool to boost students' engagement and to improve their understanding of complex topics. Despite this, augmented reality usage is still not common in schools and it still offers mostly individual experiences, lacking collaboration capabilities which are of paramount importance in a learning environment. This work presents an application called ARoundTheWorld, a multiplatform augmented reality application for education. It is based on a software architecture, designed with the help of secondary school teachers, that provides interoperability, multi-user support, integration with learning management systems and data analytics capabilities, thus simplifying the development of collaborative augmented reality learning experiences. The application has been tested by 44 students and 3 teachers from 3 different educational institutions to evaluate the usability as well as the impact of collaboration functionalities in the students' engagement. Qualitative and quantitative results show that the application fulfils all the design objectives identified by teachers as key elements for augmented reality educational applications. Furthermore, the application was positively evaluated by the students and it succeeded in promoting collaborative behaviour. These results show that ARoundTheWorld, and other applications built using the same architecture, could be easily developed and successfully integrated into existing schools curricula.
C1 [Masneri, Stefano; Larranaga, Mikel; Arruarte, Ana] Univ Basque Country UPV EHU, Comp Languages & Syst Dept, San Sebastian, Spain.
   [Dominguez, Ana; Pacho, Guillermo; Zorrilla, Mikel] Fdn Vicomtech, Basque Res & Technol Alliance BRTA, San Sebastian, Spain.
C3 University of Basque Country
RP Masneri, S (corresponding author), Univ Basque Country UPV EHU, Comp Languages & Syst Dept, San Sebastian, Spain.
EM smasneri001@ikasle.ehu.eus; adominguez@vicomtech.org;
   gpacho@vicomtech.org; mzorrilla@vicomtech.org; mikel.larranaga@ehu.eus;
   a.arruarte@ehu.eus
RI Arruarte, Ana/L-1569-2014
FU European Commission [856533, IT-1437-22]; European Union's Horizon 2020
   research and innovation programme [PID2021-127777OB-C21]; Basque
   Government [M10_2022_362]; Spanish Government
FX The research in Vicomtech has been supported by European Union's Horizon
   2020 research and innovation programme under grant agreement No 856533,
   project ARETE (Augmented Reality Interactive Educational System). The
   research in the UPV/EHU has been partially supported by the ADIAN grant
   IT-1437-22 from the Basque Government and grant PID2021-127777OB-C21 of
   MCIU/AEI/FEDER, UE from the Spanish Government. The authors would like
   to thank Nahia Ugarte and Pello Bereziartua as well as their students
   for helping us perform the user evaluations at their schools. The
   consent forms were prepared with the help and approval of the Ethics
   commission of the University of the Basque Country (M10_2022_362).
CR Achiam OJ, 2023, Arxiv, DOI arXiv:2303.08774
   Akçayir M, 2017, EDUC RES REV-NETH, V20, P1, DOI 10.1016/j.edurev.2016.11.002
   Alrashidi M., 2017, P 9 INT C COMP AUT E, P44, DOI [10.1145/3057039.3057088, DOI 10.1145/3057039.3057088]
   Avila-Garzon C, 2021, CONTEMP EDUC TECHNOL, V13, DOI 10.30935/cedtech/10865
   Azuma R, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.963459
   Billinghurst M, 2002, COMMUN ACM, V45, P64, DOI 10.1145/514236.514265
   Bressler DM, 2013, J COMPUT ASSIST LEAR, V29, P505, DOI 10.1111/jcal.12008
   Brooke J., 1996, USABILITY EVALUATION, P189, DOI DOI 10.1201/9781498710411-35
   Cai S, 2017, INTERACT LEARN ENVIR, V25, P778, DOI 10.1080/10494820.2016.1181094
   Calinski Tadeusz, 1974, Communications in Statistics, V3, P1, DOI [10.1080/03610927408827101, DOI 10.1080/03610927408827101]
   CATLING S, 1993, GEOGRAPHY, V78, P340
   Caudell T.P., 1992, P 25 HAW INT C SYST, DOI [DOI 10.1109/HICSS.1992.183317, 10.1109/HICSS.1992.183317]
   Çelik F, 2022, SMART LEARN ENVIRON, V9, DOI 10.1186/s40561-022-00211-z
   Chang HY, 2022, COMPUT EDUC, V191, DOI 10.1016/j.compedu.2022.104641
   Chen P, 2017, LECT N EDUC TECHNOL, P13, DOI 10.1007/978-981-10-2419-1_2
   Choi J, 2017, ADJUNCT PROCEEDINGS OF THE 2017 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT), P303, DOI 10.1109/ISMAR-Adjunct.2017.94
   Clarke B, 2020, xAPI-Spec
   Collins L, 2018, J GEOGR, V117, P137, DOI 10.1080/00221341.2017.1374990
   Commission E, 2023, Extended reality-opportunities, success stories and challenges (health, education)-final report, DOI [10.2759/121671, DOI 10.2759/121671]
   da Silva Manoela M. O., 2019, Journal of the Brazilian Computer Society, V25, DOI 10.1186/s13173-019-0084-8
   Das P., 2017, Multimodal Technologies and Interaction, V1, P8, DOI DOI 10.3390/MTI1020008
   Davis FD, 1996, INT J HUM-COMPUT ST, V45, P19, DOI 10.1006/ijhc.1996.0040
   Dinis FM, 2017, IEEE GLOB ENG EDUC C, P1683, DOI 10.1109/EDUCON.2017.7943075
   Friedman H., 1999, The Journal of Marketing Management, VWinter, P114, DOI DOI 10.2307/3151463
   Garzón J, 2019, VIRTUAL REAL-LONDON, V23, P447, DOI 10.1007/s10055-019-00379-9
   Ibáñez MB, 2018, COMPUT EDUC, V123, P109, DOI 10.1016/j.compedu.2018.05.002
   Iqbal MZ, 2022, MULTIMODAL TECHNOLOG, V6, DOI 10.3390/mti6090075
   Jolliffe I. T., 2002, PRINCIPAL COMPONENT, DOI 10.1016/0169-7439(87)80084-92
   Keller J. M., 1987, Journal of Instructional Development, V10, P2, DOI [10.1007/bf02905780, DOI 10.1007/BF02905780]
   Laviole J, 2018, PROCEEDINGS OF THE VIRTUAL REALITY INTERNATIONAL CONFERENCE - LAVAL VIRTUAL (ACM VRIC 2018), DOI 10.1145/3234253.3234317
   Law Effie Lai-Chong., 2021, International Journal of Child-Computer Interaction, P100321, DOI DOI 10.1016/J.IJCCI.2021.100321
   López-Faican L, 2020, COMPUT EDUC, V149, DOI 10.1016/j.compedu.2020.103814
   Maor D., 1999, Learning Environments Research, V2, P307
   Masneri Stefano, 2020, 2020 6th International Conference of the Immersive Learning Research Network (iLRN). Proceedings, P283, DOI 10.23919/iLRN47897.2020.9155186
   Masneri S, 2023, VIRTUAL REAL-LONDON, V27, P1813, DOI 10.1007/s10055-023-00764-5
   Masneri S, 2022, J UNIVERS COMPUT SCI, V28, P564, DOI 10.3897/jucs.76535
   Masneri S, 2022, LECT NOTE NETW SYST, V390, P1004, DOI 10.1007/978-3-030-93907-6_106
   Matsumoto K, 2023, Unity render streaming
   McLean G, 2019, COMPUT HUM BEHAV, V101, P210, DOI 10.1016/j.chb.2019.07.002
   MILGRAM P, 1994, IEICE T INF SYST, VE77D, P1321
   Oh S, 2018, IEEE T LEARN TECHNOL, V11, P115, DOI 10.1109/TLT.2017.2750673
   Ong SK, 2008, INT J PROD RES, V46, P2707, DOI 10.1080/00207540601064773
   Osmulski R, 2023, Ask AI-library for talking to ChatGPT from within Jupyter Notebook
   Palaigeorgiou G, 2018, INTERACT TECHNOL SMA, V15, P279, DOI 10.1108/ITSE-12-2017-0066
   Pan XY, 2021, IEEE ACCESS, V9, P164742, DOI 10.1109/ACCESS.2021.3134589
   Pellas N, 2019, VIRTUAL REAL-LONDON, V23, P329, DOI 10.1007/s10055-018-0347-2
   Prumper J., 1999, Human-Computer Interaction: Ergonomics and User Interfaces. Proceedings of HCI International '99 (8th International Conference on Human-Computer Interaction), P1028
   Rajaram S, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517486
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   Ryan RM, 2000, AM PSYCHOL, V55, P68, DOI 10.1037/0003-066X.55.1.68
   Santos MEC, 2014, IEEE T LEARN TECHNOL, V7, P38, DOI 10.1109/TLT.2013.37
   Sauro J, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2215, DOI 10.1145/1978942.1979266
   Secretan J, 2019, 2019 IEEE INT C ENG, P1, DOI [10.1109/TALE48000.2019.9225931, DOI 10.1109/TALE48000.2019.9225931]
   Sirakaya M., 2018, CONT ED TECHNOLOGY, V9, P297, DOI DOI 10.30935/CET.444119
   Takahashi I, 2018, INT J DES, V12, P111
   Thamrongrat P, 2019, LECT NOTES COMPUT SC, V11749, P364, DOI 10.1007/978-3-030-29390-1_20
   Thanyadit S., 2022, Human Interact Emerg Technol (IHIET 2022): Artif Intell Future Appl, V68, P9, DOI [10.54941/ahfe1002707, DOI 10.54941/AHFE1002707]
   Vlachos A, 2024, J CULT HERIT MANAG S, V14, P160, DOI 10.1108/JCHMSD-06-2021-0110
   Wu YH, 2020, INTERACT LEARN ENVIR, V28, P602, DOI 10.1080/10494820.2019.1696842
   Xefteris S, 2019, INT J ENG PEDAGOG, V9, P78, DOI 10.3991/ijep.v9i2.9950
NR 60
TC 0
Z9 0
U1 13
U2 13
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 42
DI 10.1007/s10055-024-00952-x
PG 13
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA HL5X0
UT WOS:001159684400001
OA hybrid
DA 2024-08-05
ER

PT J
AU Postema, R
   Hardon, H
   Rahimi, AM
   Horeman, R
   Nickel, F
   Dankelman, J
   Bloemendaal, ALA
   van der Elst, M
   van der Peet, DL
   Daams, F
   Hardon, SF
   Horeman, T
AF Postema, Roelf
   Hardon, Hidde
   Rahimi, A. Masie
   Horeman, Roel
   Nickel, Felix
   Dankelman, Jenny
   Bloemendaal, Alexander L. A.
   van der Elst, Maarten
   van der Peet, Donald L.
   Daams, Freek
   Hardon, Sem F.
   Horeman, Tim
TI The value of collision feedback in robotic surgical skills training
SO VIRTUAL REALITY
LA English
DT Article
DE Robotic surgery; Simulation training; Haptic feedback; Visual feedback;
   Skills acquisition; Patient safety
ID VISUAL FORCE FEEDBACK; LOW ANTERIOR RESECTION; VIRTUAL-REALITY; HAPTIC
   FEEDBACK; LAPAROSCOPIC SURGERY; RIGHT COLECTOMY; CANCER; PSYCHOMOTOR;
   SIMULATORS; EXPERT
AB Collision feedback about instrument and environment interaction is often lacking in robotic surgery training devices. The PoLaRS virtual reality simulator is a newly developed desk trainer that overcomes drawbacks of existing robot trainers for advanced laparoscopy. This study aimed to assess the effect of haptic and visual feedback during training on the performance of a robotic surgical task. Robotic surgery-naive participants were randomized and equally divided into two training groups: Haptic and Visual Feedback (HVF) and No Haptic and Visual Feedback. Participants performed two basic virtual reality training tasks on the PoLaRS system as a pre- and post-test. The measurement parameters Time, Tip-to-tip distance, Path length Left/Right and Collisions Left/Right were used to analyze the learning curves and statistically compare the pre- and post-tests performances. In total, 198 trials performed by 22 participants were included. The visual and haptic feedback did not negatively influence the time to complete the tasks. Although no improvement in skill was observed between pre- and post-tests, the mean rank of the number of collisions of the right grasper (dominant hand) was significantly lower in the HVF feedback group during the second post-test (Mean Rank = 8.73 versus Mean Rank = 14.27, U = 30.00, p = 0.045). Haptic and visual feedback during the training on the PoLaRS system resulted in fewer instrument collisions. These results warrant the introduction of haptic feedback in subjects with no experience in robotic surgery. The PoLaRS system can be utilized to remotely optimize instrument handling before commencing robotic surgery in the operating room.
C1 [Postema, Roelf; Hardon, Hidde; Rahimi, A. Masie; van der Peet, Donald L.; Daams, Freek; Hardon, Sem F.] Vrije Univ Amsterdam Med Ctr, Dept Surg, Amsterdam UMC, Boelelaan 1117, NL-1081HV Amsterdam, Netherlands.
   [Postema, Roelf; Horeman, Roel; Dankelman, Jenny; Hardon, Sem F.; Horeman, Tim] Delft Univ Technol, Dept BioMech Engn, Delft, Netherlands.
   [Hardon, Hidde] Vrije Univ Amsterdam, Fac Behav & Movement Sci, Dept Human Movement Sci, Amsterdam, Netherlands.
   [Rahimi, A. Masie] Amsterdam Skills Ctr Hlth Sci ASC, Amsterdam, Netherlands.
   [Nickel, Felix] Heidelberg Univ, Dept Gen Visceral & Transplantat Surg, Heidelberg, Germany.
   [Dankelman, Jenny; van der Elst, Maarten; Horeman, Tim] Delft Univ Technol, Fac Mech Maritime & Mat Engn 3mE, Delft, Netherlands.
   [Bloemendaal, Alexander L. A.; van der Elst, Maarten] Reinier Graaf Gasthuis, Dept Surg, Delft, Netherlands.
C3 Vrije Universiteit Amsterdam; VU UNIVERSITY MEDICAL CENTER; Delft
   University of Technology; Vrije Universiteit Amsterdam; Ruprecht Karls
   University Heidelberg; Delft University of Technology; Reinier de Graaf
   Hospital
RP Hardon, SF (corresponding author), Vrije Univ Amsterdam Med Ctr, Dept Surg, Amsterdam UMC, Boelelaan 1117, NL-1081HV Amsterdam, Netherlands.; Hardon, SF (corresponding author), Delft Univ Technol, Dept BioMech Engn, Delft, Netherlands.
EM s.hardon@amsterdamumc.nl
RI Dankelman, Jenny/H-9100-2017; Rahimi, Masie/JBJ-5290-2023
OI Rahimi, Masie/0000-0001-5919-0599
CR Abboudi H, 2013, BJU INT, V111, P194, DOI 10.1111/j.1464-410X.2012.11270.x
   Alaker M, 2016, INT J SURG, V29, P85, DOI 10.1016/j.ijsu.2016.03.034
   Alleblas CCJ, 2017, SURG ENDOSC, V31, P5411, DOI 10.1007/s00464-017-5623-9
   Amirabdollahian F, 2018, J ROBOT SURG, V12, P11, DOI 10.1007/s11701-017-0763-4
   Bahler CD, 2014, UROL CLIN N AM, V41, P581, DOI 10.1016/j.ucl.2014.07.012
   Baik SH, 2009, ANN SURG ONCOL, V16, P1480, DOI 10.1245/s10434-009-0435-3
   Bric J, 2014, SURG ENDOSC, V28, P3343, DOI 10.1007/s00464-014-3624-5
   De La Garza JR, 2019, SURG ENDOSC, V33, P1532, DOI 10.1007/s00464-018-6441-4
   Diana M, 2015, BRIT J SURG, V102, pE15, DOI 10.1002/bjs.9711
   Hagelsteen K, 2019, MINIM INVASIV THER, V28, P309, DOI 10.1080/13645706.2018.1539012
   Hagelsteen K, 2017, MINIM INVASIV THER, V26, P269, DOI 10.1080/13645706.2017.1305970
   Hagen ME, 2008, SURG ENDOSC, V22, P1505, DOI 10.1007/s00464-007-9683-0
   Hardon SF, 2022, SURG ENDOSC, V36, P5282, DOI 10.1007/s00464-021-08906-z
   Hardon SF, 2021, SURGERY, V170, P831, DOI 10.1016/j.surg.2021.04.024
   Hiemstra E, 2011, MINIM INVASIV THER, V20, P179, DOI 10.3109/13645706.2010.532502
   Horeman T, 2014, J SURG EDUC, V71, P133, DOI 10.1016/j.jsurg.2013.06.021
   Horeman T, 2012, SURG ENDOSC, V26, P242, DOI 10.1007/s00464-011-1861-4
   Jourdes F, 2022, FRONT ROBOT AI, V9, DOI 10.3389/frobt.2022.800232
   Kaul Sanjeev, 2006, Curr Urol Rep, V7, P125, DOI 10.1007/s11934-006-0071-4
   Kowalewski KF, 2021, SURG ENDOSC, V35, P81, DOI 10.1007/s00464-019-07361-1
   Kowalewski KF, 2018, SURG ENDOSC, V32, P3830, DOI 10.1007/s00464-018-6110-7
   Miller J, 2021, SURG ENDOSC, V35, P3554, DOI 10.1007/s00464-020-07818-8
   Mirnezami AH, 2010, COLORECTAL DIS, V12, P1084, DOI 10.1111/j.1463-1318.2009.01999.x
   Nickel F, 2016, LANGENBECK ARCH SURG, V401, P893, DOI 10.1007/s00423-016-1421-4
   Nickel F, 2015, MEDICINE, V94, DOI 10.1097/MD.0000000000000764
   Othman W, 2022, FRONT ROBOT AI, V8, DOI 10.3389/frobt.2021.705662
   Overtoom EM, 2019, J SURG EDUC, V76, P242, DOI 10.1016/j.jsurg.2018.06.008
   Parisi A, 2017, SURG ONCOL, V26, P28, DOI 10.1016/j.suronc.2016.12.005
   Park EJ, 2015, ANN SURG, V261, P129, DOI 10.1097/SLA.0000000000000613
   Postema RR, 2021, SURG ENDOSC, V35, P4175, DOI 10.1007/s00464-020-07898-6
   Prasad MSR, 2016, J SURG EDUC, V73, P858, DOI 10.1016/j.jsurg.2016.04.009
   Rangarajan K, 2020, J SURG EDUC, V77, P337, DOI 10.1016/j.jsurg.2019.09.006
   Reitz ACW, 2018, SURG ENDOSC, V32, P3525, DOI 10.1007/s00464-018-6074-7
   Rizun P, 2006, INT J MED ROBOT COMP, V2, P341, DOI 10.1002/rcs.110
   Rodrigues SP, 2014, BRIT J SURG, V101, P1766, DOI 10.1002/bjs.9669
   Romero P, 2018, J SURG EDUC, V75, P510, DOI 10.1016/j.jsurg.2017.07.025
   Schmidt MW, 2021, BJS OPEN, V5, DOI 10.1093/bjsopen/zraa066
   Schmidt MW, 2020, SURG ENDOSC, V34, P869, DOI 10.1007/s00464-019-06842-7
   Simorov A, 2012, SURG ENDOSC, V26, P2117, DOI 10.1007/s00464-012-2182-y
   Smit D, 2017, SURG ENDOSC, V31, P299, DOI 10.1007/s00464-016-4972-0
   Stefanidis D, 2010, SURG ENDOSC, V24, P377, DOI 10.1007/s00464-009-0578-0
   Ström P, 2006, SURG ENDOSC, V20, P1383, DOI 10.1007/s00464-005-0545-3
   Sung GT, 2001, UROLOGY, V58, P893, DOI 10.1016/S0090-4295(01)01423-6
   Tan GY, 2009, UROL CLIN N AM, V36, P237, DOI 10.1016/j.ucl.2009.02.010
   Tanaka A, 2016, SURG ENDOSC, V30, P3720, DOI 10.1007/s00464-015-4667-y
   Uemura M, 2014, J SURG RES, V188, P8, DOI 10.1016/j.jss.2013.12.009
   van der Meijden OAJ, 2009, SURG ENDOSC, V23, P1180, DOI 10.1007/s00464-008-0298-x
   van Empel PJ, 2013, SURG ENDOSC, V27, P2934, DOI 10.1007/s00464-013-2859-x
   Våpenstad C, 2013, SURG ENDOSC, V27, P2391, DOI 10.1007/s00464-012-2745-y
   Våpenstad C, 2013, SURG ENDOSC, V27, P1386, DOI 10.1007/s00464-012-2621-9
   Widmar M, 2017, COLORECTAL DIS, V19, P888, DOI 10.1111/codi.13786
   Willuth E, 2022, SURG ENDOSC, V36, P1064, DOI 10.1007/s00464-021-08373-6
   Zelhart M, 2018, SURG ENDOSC, V32, P24, DOI 10.1007/s00464-017-5796-2
NR 53
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 46
DI 10.1007/s10055-023-00891-z
PG 12
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA IS2L6
UT WOS:001168254700001
OA hybrid
DA 2024-08-05
ER

PT J
AU Rinaldi, V
   Robertson, KA
   Strong, GG
   Daeid, NN
AF Rinaldi, Vincenzo
   Robertson, Karen Ann
   Strong, Graham George
   Daeid, Niamh Nic
TI Examination of fire scene reconstructions using virtual reality to
   enhance forensic decision-making. A case study in Scotland.
SO VIRTUAL REALITY
LA English
DT Article
DE Forensic science; Fire; Scene investigation; Reconstruction;
   Decision-making; Training
ID SUPPORT
AB When attending a crime scene, first responders are responsible for identifying areas of potential interest for subsequent forensic examination. This information is shared with the police, forensic practitioners, and legal authorities during an initial meeting of all interested parties, which in Scotland is known as a forensic strategy meeting. Swift documentation is fundamental to allow practitioners to learn about the scene(s) and to plan investigative strategies, traditionally relying on word-of-mouth briefings using digital photographs, videos, diagrams, and verbal reports. We suggest that these early and critical briefings can be augmented positively by implementing an end-to-end methodology for indoor 3D reconstruction and successive visualisation through immersive Virtual Reality (VR). The main objective of this paper is to provide an integrative documentation tool to enhance the decision-making processes in the early stages of the investigation. Taking a fire scene as an example, we illustrate a framework for rapid spatial data acquisition of the scene that leverages structure-from-motion photogrammetry. We developed a VR framework that enables the exploration of virtual environments on a standalone, low-cost immersive head-mounted display. The system was tested in a two-phased inter-agency fire investigation exercise, where practitioners were asked to produce hypotheses suitable for forensic strategy meetings by (1) examining traditional documentation and then (2) using a VR walkthrough of the same premises. The integration of VR increased the practitioners' scene comprehension, improved hypotheses formulation with fewer caveats, and enabled participants to sketch the scene, in contrast to the orientation challenges encountered using conventional documentation.
C1 [Rinaldi, Vincenzo; Daeid, Niamh Nic] Univ Dundee, Leverhulme Res Ctr Forens Sci, Sch Sci & Engn, Dundee DD1 4HR, Scotland.
   [Robertson, Karen Ann; Strong, Graham George] Scottish Police Author Forens Serv, Scottish Police Author, Glasgow City G51 1DZ, Scotland.
C3 University of Dundee
RP Rinaldi, V (corresponding author), Univ Dundee, Leverhulme Res Ctr Forens Sci, Sch Sci & Engn, Dundee DD1 4HR, Scotland.
EM vrinaldi001@dundee.ac.uk
OI Rinaldi, Vincenzo/0000-0003-4773-0732
FU Leverhulme Trust
FX No Statement Available
CR ACPO, 2006, Murder investigation manual
   Almirall J., 2004, Analysis and Interpretation of Fire Scene Evidence, DOI [10.1201/9780203492727, DOI 10.1201/9780203492727]
   Baber C, 2012, HUM FACTORS, V54, P413, DOI 10.1177/0018720812440577
   Barazzetti L., 2018, Remote Sens Spatial Inform Sci, VXLII2, P69, DOI [10.5194/isprs-archives-XLII-2-69-2018, DOI 10.5194/ISPRS-ARCHIVES-XLII-2-69-2018]
   Bennett RB, 1999, Scholarship and Professional Work-Business, V63
   Boletsis C, 2019, ADV HUM-COMPUT INTER, V2019, DOI 10.1155/2019/7420781
   Bowman DA, 2007, COMPUTER, V40, P36, DOI 10.1109/MC.2007.257
   Darken RP, 2014, Handbook of virtual environments, P493
   de Leeuwe R, 2017, J FORENSIC RADIOL IM, V8, P13, DOI 10.1016/j.jofri.2017.03.002
   Dietrich A, 2007, IEEE COMPUT GRAPH, V27, P20, DOI 10.1109/MCG.2007.154
   Doak S, 2007, FORENSIC SCI INT, V167, P201, DOI 10.1016/j.forsciint.2006.06.063
   Drakou M, 2016, IEEE MEDITERR ELECT
   Ebert LC, 2014, FORENSIC SCI MED PAT, V10, P623, DOI 10.1007/s12024-014-9605-0
   Estellers Virginia., 2015, Scale Space and Variational Methods in Computer Vision, P525
   European Commission, 2022, Tech. rep., DOI [10.2873/778991, DOI 10.2873/778991]
   Finstad K, 2010, INTERACT COMPUT, V22, P323, DOI 10.1016/j.intcom.2010.04.004
   Fish JT, 2013, Crime scene investigation, P1, DOI [10.4324/9781315721910, DOI 10.4324/9781315721910]
   Galanakis G., 2021, Forensic Sci, V1, P56, DOI [DOI 10.3390/FORENSICSCI1020008, 10.3390/forensicsci1020008]
   Guarnera L, 2023, MULTIMED TOOLS APPL, V82, P20655, DOI 10.1007/s11042-022-14037-x
   HART S G, 1988, P139
   Howes LM, 2017, POLIC SOC, V27, P541, DOI 10.1080/10439463.2015.1089870
   Jani G, 2022, J VIS COMMUN MED, V45, P18, DOI 10.1080/17453054.2021.1971516
   Kader SN, 2020, J CHEM EDUC, V97, P2651, DOI 10.1021/acs.jchemed.0c00817
   Kang ZZ, 2020, ISPRS INT J GEO-INF, V9, DOI 10.3390/ijgi9050330
   Karabiyik U, 2019, LECT NOTES COMPUT SC, V11845, P469, DOI 10.1007/978-3-030-33723-0_38
   Karis B, 2021, Advances in Real-Time Rendering in Games course
   Kelty S., 2011, Forensic Sci. Policy Manage, V2, P175, DOI DOI 10.1080/19409044.2012.693572
   Khalilia WM, 2022, IEEE ACCESS, V10, P83297, DOI 10.1109/ACCESS.2022.3196471
   Kim C., 2022, Digest Tech Papers-SID Int Symp, V53, P40, DOI [10.1002/SDTP.15410, DOI 10.1002/SDTP.15410]
   Koller S, 2019, FORENSIC SCI INT, V295, P30, DOI 10.1016/j.forsciint.2018.11.006
   Larsen H, 2021, SCI JUSTICE, V61, P356, DOI 10.1016/j.scijus.2021.04.003
   Lehtola VV, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9080796
   Luchowski L, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21041407
   Luebke David, 2003, LEVEL DETAIL 3D GRAP
   Ma MH, 2010, J FORENSIC SCI, V55, P1227, DOI 10.1111/j.1556-4029.2010.01453.x
   Maboudi M, 2017, GFAI WORKSH 3D N E 2, P125
   Mania K, 2003, PRESENCE-TELEOP VIRT, V12, P296, DOI 10.1162/105474603765879549
   Mayne R, 2020, SCI JUSTICE, V60, P466, DOI 10.1016/j.scijus.2020.07.006
   Meta, 2023, Testing and performance analysis, oculus developers
   Natl Res Council, 2009, STRENGTHENING FORENSIC SCIENCE IN THE UNITED STATES: A PATH FORWARD, P1
   Perfetti L, 2017, INT ARCH PHOTOGRAMM, V42-2, P573, DOI 10.5194/isprs-archives-XLII-2-W3-573-2017
   PICS, 2019, Tech. rep.
   Pintore G, 2020, COMPUT GRAPH FORUM, V39, P667, DOI 10.1111/cgf.14021
   Raneri D, 2018, AUST J FORENSIC SCI, V50, P697, DOI 10.1080/00450618.2018.1424245
   Reichherzer Carolin, 2021, CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, DOI 10.1145/3411764.3445464
   Reichherzer C, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P615, DOI 10.1109/VR51125.2022.00082
   Reichherzer C, 2018, IEEE T VIS COMPUT GR, V24, P2917, DOI 10.1109/TVCG.2018.2868569
   Remondino F, 2005, PROC SPIE, V5665, P216, DOI 10.1117/12.586294
   Rinaldi V., 2021, UDGARD 2021 RAW DATA, DOI [10.15132/10000174, DOI 10.15132/10000174]
   Robinson E. M., 2010, CRIME SCENE PHOTOGR, DOI [10.1016/C2009-0-61082-5, DOI 10.1016/C2009-0-61082-5]
   Ruddle RA, 2011, ACM T COMPUT-HUM INT, V18, DOI 10.1145/1970378.1970384
   Sansoni G, 2011, AM J FOREN MED PATH, V32, P280, DOI 10.1097/PAF.0b013e318221b880
   Schuemie MJ, 2001, CYBERPSYCHOL BEHAV, V4, P183, DOI 10.1089/109493101300117884
   Sevcik J, 2022, 2022 26 INT C CIRC S, P6, DOI [10.1109/CSCC55931.2022.00010, DOI 10.1109/CSCC55931.2022.00010]
   Sheppard K, 2020, EGYPT J FORENSIC SCI, V10, DOI 10.1186/s41935-020-00184-5
   Sieberth T, 2023, FORENSIC SCI INT, V348, DOI 10.1016/j.forsciint.2023.111602
   Sieberth T, 2021, FORENSIC SCI INT, V329, DOI 10.1016/j.forsciint.2021.111092
   Sieberth T, 2019, FORENSIC SCI MED PAT, V15, P41, DOI 10.1007/s12024-018-0058-8
   Slater M, 1997, PRESENCE-VIRTUAL AUG, V6, P603, DOI 10.1162/pres.1997.6.6.603
   Streefkerk JW, 2009, LECT NOTES ARTIF INT, V5638, P88, DOI 10.1007/978-3-642-02812-0_11
   Tipping R, 2014, P 26 AUSTR COMP HUM, P107, DOI [10.1145/2686612.2686626, DOI 10.1145/2686612.2686626]
   Tredinnick R., 2019, Forensic Sci Int Reports, V1, DOI [DOI 10.1016/J.FSIR.2019.100025, 10.1016/j.fsir.2019.100025]
   Wang JM, 2019, FORENSIC SCI INT, V303, DOI 10.1016/j.forsciint.2019.109943
   Yu SH, 2023, SCI JUSTICE, V63, P238, DOI 10.1016/j.scijus.2023.01.001
NR 64
TC 1
Z9 1
U1 4
U2 4
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 57
DI 10.1007/s10055-024-00961-w
PG 16
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA JL7V5
UT WOS:001173398100001
OA hybrid
DA 2024-08-05
ER

PT J
AU Ruiz, RM
   Sánchezallegue, P
   MarinTorres, MT
   PardoRios, M
   Madrigal, JJC
   Tortosa, DE
AF Ruiz, Rafael Melendreras
   Sanchezallegue, Paloma
   MarinTorres, Maria Teresa
   PardoRios, Manuel
   Madrigal, Jose Joaquin Ceron
   Tortosa, Damian Escribano
TI Analysis of the user experience (On site vs. Virtual Reality) through
   biological markers and cognitive tests in museums: the case of Museo
   Cristo de la Sangre (Murcia, Spain)
SO VIRTUAL REALITY
LA English
DT Article
DE Digitisation; Virtual Reality; Spatial; Alpha-amylase; System usability
   scale; Metaverse
ID SUS
AB Digital technologies have changed almost every aspect of our lives, including the way we access heritage. Following the pandemic caused by COVID-19 and the technological evolution of recent years, museums and institutions, among others, have changed the way they display their collections, taking a greater interest in new technologies, platforms and digital software. This technological boom finds its greatest transformation with the implementation of Virtual Reality (VR) and Metaverse in the museum sector. This article shows the concrete influence of VR/Metaverse in a museum room previously digitised through different techniques. Subsequently, the impact over user experience in the VR scenario versus on-site visit has been measured. In parallel, to measure the enzyme alpha-amylase in saliva, a cognitive test and usability test (SUS) were carried out to determine the learning capacity and degree of satisfaction obtained with experience alongside the room of the Museo de la Sangre in Murcia (Spain).
C1 [Ruiz, Rafael Melendreras; Sanchezallegue, Paloma; PardoRios, Manuel] Univ Catolica San Antonio Murcia, Murcia, Spain.
   [MarinTorres, Maria Teresa; Madrigal, Jose Joaquin Ceron; Tortosa, Damian Escribano] Univ Murcia, Murcia, Spain.
C3 Universidad Catolica de Murcia; University of Murcia
RP Sánchezallegue, P (corresponding author), Univ Catolica San Antonio Murcia, Murcia, Spain.
EM rmelendreras@ucam.edu; psanchez116@alu.ucam.edu; mtmarin@um.es;
   mpardo@ucam.edu; jjceron@um.es; det20165@um.es
RI Pardo Rios, Manuel/C-8828-2017; Melendreras-Ruiz, Rafael/A-4412-2017;
   Escribano Tortosa, Damian/L-6138-2017
OI Pardo Rios, Manuel/0000-0001-7965-0134; Melendreras-Ruiz,
   Rafael/0000-0002-5404-6245; Escribano Tortosa,
   Damian/0000-0001-9439-6779
CR Bangor A, 2009, J USABILITY STUD, V4, P114
   Bellido M.L., 2016, PH: Boletin del Instituto Andaluz del Patrimonio Historico, V24, P15
   Brooke J, 2013, J USABILITY STUD, V8, P29
   Brooke John, 1996, SUS-A quick and dirty usability scale
   ICOM, 2022, International Council of Museums
   Lee H, 2020, INFORM MANAGE-AMSTER, V57, DOI 10.1016/j.im.2019.103229
   Liberatore MJ, 2021, VIRTUAL REAL-LONDON, V25, P773, DOI 10.1007/s10055-020-00492-0
   Melendreras Ruiz R, 2022, CURATOR, V65, P305, DOI 10.1111/cura.12469
   Ruíz RM, 2022, J CULT HERIT, V54, P59, DOI 10.1016/j.culher.2022.01.001
   Ruiz RM, 2022, ACM J COMPUT CULT HE, V15, DOI 10.1145/3469126
   Ruiz RM, 2020, E-RPH, P52, DOI 10.30827/erph.v0i27.424
   Puig A, 2020, VIRTUAL REAL-LONDON, V24, P343, DOI 10.1007/s10055-019-00391-z
   Rahimi FB, 2022, VIRTUAL REAL-LONDON, V26, P1471, DOI 10.1007/s10055-022-00643-5
   Roca D, 2022, INT J ENV RES PUB HE, V19, DOI 10.3390/ijerph19041920
   Scavarelli A, 2021, VIRTUAL REAL-LONDON, V25, P257, DOI 10.1007/s10055-020-00444-8
   Shahab H, 2023, INT J HUM-COMPUT INT, V39, P3586, DOI 10.1080/10447318.2022.2099399
   Shehade M, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10114031
   Zhou YT, 2022, EDUC RES REV-NETH, V36, DOI 10.1016/j.edurev.2022.100454
NR 18
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 48
DI 10.1007/s10055-023-00928-3
PG 15
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA JJ1U7
UT WOS:001172714700002
OA hybrid
DA 2024-08-05
ER

PT J
AU Tavares, AS
   Soares, MM
   Marçal, MA
AF Tavares, Ademario Santos
   Soares, Marcelo M.
   Marcal, Marcio A.
TI Design and emotional responses: is there coherence between what is said
   and what is felt? A study using biofeedback and virtual reality
SO VIRTUAL REALITY
LA English
DT Article
DE Emotion; User experience; Infrared thermography; Psychophysiological
   aspects; Simulator; Game design
ID FRONTAL EEG ASYMMETRY; VIDEO GAMES; INFRARED THERMOGRAPHY;
   EMPIRICAL-EVIDENCE; COMPUTER GAMES; SERIOUS GAME; AROUSAL; EXPERIENCE;
   VALENCE; HUMANS
AB Identifying users' experience when using products is one of the major challenges for design. Analyzing users' psychophysiological reactions to an experience using biofeedback can produce more reliable results than using subjective evaluations, such as structured interviews and questionnaires. Two case studies were conducted to identify emotions users actually felt and to check whether there is some correspondence with what they reported after using two computational systems. The first system investigated users' emotions during training on a vehicle driving simulator, and the second analyzed the emotions experienced during a car racing game, both in a virtual reality environment. User's opinions about their emotional state were obtained using self-report techniques (using the Geneva Emotions Wheel-GEW and Positive and Negative Affective Schedule-PANAS questionnaires) and applying EEG (brain activity with Frontal Alpha Asymmetry Index-FAAI) and infrared thermography (facial thermograms). The training experiment presented the greater concordance between the psychophysiological and the self-report responses. Results evidenced the importance of undertaking multimodal studies in design research to determine users' emotional experiences in a virtual reality context.
C1 [Tavares, Ademario Santos; Soares, Marcelo M.] Univ Fed Pernambuco, Postgrad Program Design, Ave Academ Helio Ramos, Recife, PE, Brazil.
   [Marcal, Marcio A.] Fed Univ Vales do Jequitinhonha & Mucuri, Diamantina, Brazil.
C3 Universidade Federal de Pernambuco
RP Soares, MM (corresponding author), Univ Fed Pernambuco, Postgrad Program Design, Ave Academ Helio Ramos, Recife, PE, Brazil.
EM astavares.ergodesign@gmail.com; soaresmm@gmail.com;
   marcioalvesmarcal@gmail.com
FU Coordenao de Aperfeioamento de Pessoal de Nvel Superior; Coordination
   Unit for Higher Education Staff Development (CAPES); National Council
   for Scientific and Technological Development (CNPq)
FX The authors are grateful to the Coordination Unit for Higher Education
   Staff Development (CAPES) and the National Council for Scientific and
   Technological Development (CNPq) for sponsoring this study.
CR Abhishek AM, 2014, 2014 INTERNATIONAL CONFERENCE ON CIRCUITS, COMMUNICATION, CONTROL AND COMPUTING (I4C), P25, DOI 10.1109/CIMCA.2014.7057749
   Ahn M, 2014, SENSORS-BASEL, V14, P14601, DOI 10.3390/s140814601
   Albraikan A, 2019, IEEE SENS J, V19, P8402, DOI 10.1109/JSEN.2018.2867221
   Barros RQ, 2016, AHFE 2016, DOI [10.1007/978-3-319-41983-1_10, DOI 10.1007/978-3-319-41983-1_10]
   BECK LA, 1992, J LEISURE RES, V24, P93, DOI 10.1080/00222216.1992.11969876
   Borsci S, 2016, VIRTUAL REAL-LONDON, V20, P83, DOI 10.1007/s10055-016-0286-8
   Boyle EA, 2016, COMPUT EDUC, V94, P178, DOI 10.1016/j.compedu.2015.11.003
   Bruce Hanington B.M, 2012, Universal methods of design: 100 ways to research complex problems, develop innovative ideas, and design effective solutions, DOI DOI 10.5860/CHOICE.49-5403
   Buelow MT, 2015, COMPUT HUM BEHAV, V45, P228, DOI 10.1016/j.chb.2014.12.029
   Calandra D, 2023, VIRTUAL REAL-LONDON, V27, P985, DOI 10.1007/s10055-022-00704-9
   Calvo RA, 2010, IEEE T AFFECT COMPUT, V1, P18, DOI 10.1109/T-AFFC.2010.1
   Coan JA, 2004, BIOL PSYCHOL, V67, P7, DOI 10.1016/j.biopsycho.2004.03.002
   Connolly TM, 2012, COMPUT EDUC, V59, P661, DOI 10.1016/j.compedu.2012.03.004
   Cruz AV, 2015, Relationship between product quality and customer satisfaction
   Cruz-Albarran IA, 2017, INFRARED PHYS TECHN, V81, P250, DOI 10.1016/j.infrared.2017.01.002
   DAVIDSON RJ, 1992, PSYCHOL SCI, V3, P39, DOI 10.1111/j.1467-9280.1992.tb00254.x
   de Carvalho HW, 2013, REV BRAS PSIQUIATR, V35, P169, DOI 10.1590/1516-4446-2012-0957
   Defeo J.A., 2016, Juran's Quality Handbook: The Complete Guide to Performance Excellence, VSeventh
   Dehais F, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00268
   Diaz-Piedra C, 2019, APPL ERGON, V81, DOI 10.1016/j.apergo.2019.06.001
   Drachen A., 2013, GAMES ANAL MAXIMIZIN, P13
   Duncan S, 2007, COGNITION EMOTION, V21, P1184, DOI 10.1080/02699930701437931
   Dzedzickis A, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20030592
   Evans C, 2018, COMPUT HUM BEHAV, V88, P70, DOI 10.1016/j.chb.2018.06.024
   Fernández-Cuevas I, 2015, INFRARED PHYS TECHN, V71, P28, DOI 10.1016/j.infrared.2015.02.007
   Filippini C, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10082924
   Fu Y, 2016, PHYSICS: PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON PHYSIOLOGICAL COMPUTING SYSTEMS, P142, DOI 10.5220/0006007901420151
   Ge X, 2021, DESIGN STUD, V75, DOI 10.1016/j.destud.2021.101020
   Gillespie Robin Mary, 2002, Work, V18, P249
   Goulart C, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0212928
   Granato M, 2018, 2018 14TH INTERNATIONAL CONFERENCE ON SIGNAL IMAGE TECHNOLOGY & INTERNET BASED SYSTEMS (SITIS), P717, DOI 10.1109/SITIS.2018.00115
   Hartfiel B, 2021, VIRTUAL REAL-LONDON, V25, P819, DOI 10.1007/s10055-020-00496-w
   Hassenzahl M, 2006, BEHAV INFORM TECHNOL, V25, P91, DOI 10.1080/01449290500330331
   IACT-International Academy of Clinical Thermology, 2002, Thermography guidelines. Standards and protocols in clinical thermographic imaging
   Ioannou S, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.00845
   Jacobs Karen, 2002, Work, V18, P221
   Jarvela S., 2015, Game research methods-an overview
   Jenkins SD, 2014, 2014 QUANTITATIVE IN, DOI [10.21611/qirt.2014.131, DOI 10.21611/QIRT.2014.131]
   Jenkins SD, 2007, Cumulus working papers: SchwabischGmund, V07, P41
   Jenkins S, 2009, INT J DES, V3, P53
   Jerald J., 2015, VR BOOK HUMAN CENTER, DOI [10.1145/2792790, DOI 10.1145/2792790]
   Jercic P, 2018, INT J SOC ROBOT, V10, P115, DOI 10.1007/s12369-017-0437-4
   Jerzak N, 2014, Design, user experience, and usability. Theories, methods, and tools for designing the user experience, V8517, P453, DOI [10.1007/978-3-319-07668-3_44, DOI 10.1007/978-3-319-07668-3_44]
   Jian BL, 2017, BMC PSYCHIATRY, V17, DOI 10.1186/s12888-017-1387-y
   Jones BF, 1998, IEEE T MED IMAGING, V17, P1019, DOI 10.1109/42.746635
   Kampa ER, 2020, SBGAMES 2020
   Kaza K, 2016, Body motion analysis for emotion recognition in serious games, DOI [10.1007/978-3-319-40244-4_4, DOI 10.1007/978-3-319-40244-4_4]
   Kerous B, 2018, VIRTUAL REAL-LONDON, V22, P119, DOI 10.1007/s10055-017-0328-x
   Khan R., 2017, Global J Comput Sci Technol, V17, P25
   Kivikangas JM, 2011, J GAMING VIRTUAL WOR, V3, P181, DOI 10.1386/jgvw.3.3.181_1
   Kosonogov V, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0183592
   Kotsia I, 2016, SOCIO AFFECT COMPUT, P59, DOI 10.1007/978-3-319-41316-7_4
   Krokos E, 2022, VIRTUAL REAL-LONDON, V26, P77, DOI 10.1007/s10055-021-00517-2
   Kumar J, 2016, PROCEDIA COMPUT SCI, V84, P107, DOI 10.1016/j.procs.2016.04.073
   Lecoutre Lucille, 2015, 2nd International Conference on Physiological Computing Systems (PhyCS 2015). Proceedings, P112
   Lee D, 2020, J MOTOR BEHAV, V52, P33, DOI 10.1080/00222895.2019.1574259
   Legrand FD, 2015, EUR J SPORT SCI, V15, P161, DOI 10.1080/17461391.2014.948077
   Lelord F e Andre C., 2002, A forca das Emocoes
   Li Y, 2015, IEEE INT SYMP SIGNAL, P57, DOI 10.1109/ISSPIT.2015.7394401
   Lieberoth A., 2015, Game research methods-an overview, DOI [10.5555/2812774.2812795, DOI 10.5555/2812774.2812795]
   Lobach B., 2001, Design Industrial: Bases para a configuracao dos produtos industriais, V1
   Lv ZH, 2017, PERVASIVE MOB COMPUT, V41, P504, DOI 10.1016/j.pmcj.2017.04.006
   Malik AS., 2017, Design code and example datasets
   McMahan T, 2015, PROCEDIA MANUF, V3, P2303, DOI 10.1016/j.promfg.2015.07.376
   Mehta RK, 2013, FRONT HUM NEUROSCI, V7, DOI 10.3389/fnhum.2013.00889
   Merla A, 2007, P ANN INT IEEE EMBS, P247, DOI 10.1109/IEMBS.2007.4352270
   Mishra J, 2016, NEURON, V90, P214, DOI 10.1016/j.neuron.2016.04.010
   Monk CS, 2008, DEV PSYCHOPATHOL, V20, P1231, DOI 10.1017/S095457940800059X
   Moridis CN, 2018, APPL PSYCHOPHYS BIOF, V43, P1, DOI 10.1007/s10484-017-9379-8
   Nakanishi R, 2008, INFANT BEHAV DEV, V31, P137, DOI 10.1016/j.infbeh.2007.09.001
   Netzer L, 2018, PERS INDIV DIFFER, V135, P13, DOI 10.1016/j.paid.2018.06.038
   Neves ASO, 2022, Analise do uso de exergames comorecursoinovadornareabilitacao de criancaspertencentesaoespectroautista
   Nhan BR, 2010, IEEE T BIO-MED ENG, V57, P979, DOI 10.1109/TBME.2009.2035926
   Norman D.A., 2004, EMOTIONAL DESIGN WHY
   Nozawa A, 2009, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2009/01/P01007
   Oliveira RP, 2016, SBGAMES 2016
   Or Calvin K. L., 2007, Occupational Ergonomics, V7, P83
   Uriarte IDOV, 2015, SENSORS-BASEL, V15, P6520, DOI 10.3390/s150306520
   Papousek I, 2012, PSYCHOPHYSIOLOGY, V49, P489, DOI 10.1111/j.1469-8986.2011.01324.x
   Parasuraman R., 2003, Theor. Issues in Ergon. Sci, V4, P5, DOI DOI 10.1080/14639220210199753
   Pavlidis IT, 2004, PROC SPIE, V5405, P270, DOI 10.1117/12.547806
   Pearson LC, 2020, DESIGN STUD, V66, P114, DOI 10.1016/j.destud.2019.11.005
   Posner J, 2005, DEV PSYCHOPATHOL, V17, P715, DOI 10.1017/S0954579405050340
   Prete G, 2014, LATERALITY, V19, P439, DOI 10.1080/1357650X.2013.862255
   Rebelo F, 2012, HUM FACTORS, V54, P964, DOI 10.1177/0018720812465006
   Ring EFJ, 2012, PHYSIOL MEAS, V33, pR33, DOI 10.1088/0967-3334/33/3/R33
   Robinson D T., 2012, Biosociology and Neurosociology, V29, P225, DOI DOI 10.1108/S0882-6145(2012)0000029011
   Rodríguez A, 2013, STUD HEALTH TECHNOL, V191, P43, DOI 10.3233/978-1-61499-282-0-43
   Rogers JM, 2021, VIRTUAL REAL-LONDON, V25, P357, DOI 10.1007/s10055-020-00460-8
   Sacharin VeraKatja Schlegel KR Scherer., 2012, GENEVA EMOTION WHEEL
   Salazar-López E, 2015, CONSCIOUS COGN, V34, P149, DOI 10.1016/j.concog.2015.04.003
   Salen K., 2003, Rules of Play: Game Design Fundamentals
   Sheba JK, 2012, 8 INT C INTELLIGENT
   Sheikholeslami C, 2007, P ANN INT IEEE EMBS, P2489, DOI 10.1109/IEMBS.2007.4352833
   Siqueira ES, 2018, BRAZIL SYMP GAME DIG, P107, DOI 10.1109/SBGAMES.2018.00022
   Siu DCH, 2009, WORK, V34, P449, DOI 10.3233/WOR-2009-0945
   Soares MM., 2021, Ergodesign methodology for product design a human-centered approach, DOI [10.1201/9781003214793, DOI 10.1201/9781003214793]
   Solomon M.R., 2013, CONSUMER BEHAV BUYIN, V10th
   Stewart JL, 2010, J ABNORM PSYCHOL, V119, P502, DOI 10.1037/a0019196
   Sun R, 2019, VIRTUAL REAL-LONDON, V23, P385, DOI 10.1007/s10055-018-0355-2
   Suo T, 2017, FRONT PSYCHIATRY, V8, DOI 10.3389/fpsyt.2017.00180
   Tahmosybayat R, 2018, MATURITAS, V111, P90, DOI 10.1016/j.maturitas.2018.03.005
   Tavares AS, 2020, SIMU-Ciclo de atividades
   Tavares AS, 2020, JOCO-Ciclo de atividades
   Tavares AS, 2022, Estudo da experiencia emocional dos usuarios usando games: uma avaliacao a partir da Neurociencia e Termografia por Infravermelho
   Tognetti S., 2010, Proceedings of the 3rd international workshop on Affective interaction in natural environments - AFFINE'10 p, P3, DOI DOI 10.1145/1877826.1877830
   Tori R, 2018, Introducao a Realidade Virtual e Aumentada
   Unige, 2019, The geneva emotion wheel
   Valenzi S., 2014, J Biomed Sci Eng, P604, DOI [DOI 10.4236/JBISE.2014, 10.4236/jbise.2014.78061, DOI 10.4236/JBISE.2014.78061]
   Wang YY, 2017, BIOL PSYCHOL, V123, P126, DOI 10.1016/j.biopsycho.2016.11.009
   Warmelink L, 2011, LAW HUMAN BEHAV, V35, P40, DOI 10.1007/s10979-010-9251-3
   Watson D., 1994, The PANAS-X: Manual for the positive and negative affect schedule-expanded form, DOI [DOI 10.17077/48VTM4T2, 10.17077/48vt-m4t2, DOI 10.17077/48VT-M4T2]
   Williams N, 2020, bioRxiv, DOI [10.1101/2020.07.14.202085, 10.1101/2020.07.14.202085, DOI 10.1101/2020.07.14.202085]
   Yamagishi M, 2011, LECT NOTES COMPUT SC, V6769, P696, DOI 10.1007/978-3-642-21675-6_79
   Yannakakis GN, 2016, SOCIO AFFECT COMPUT, P119, DOI 10.1007/978-3-319-41316-7_7
   Zeng J, 2020, Lecture notes in computer science, V12200, DOI [10.1007/978-3-030-49713-2_40, DOI 10.1007/978-3-030-49713-2_40]
   Zhang J, 2011, J PSYCHOPHYSIOL, V25, P95, DOI 10.1027/0269-8803/a000045
   Zhang M, 2019, TRANSPORT RES F-TRAF, V63, P135, DOI 10.1016/j.trf.2019.04.003
   Zhang Y, 2016, NEUROSCI LETT, V633, P152, DOI 10.1016/j.neulet.2016.09.037
   Zhao D, 2016, PSYCHOPHYSIOLOGY, V53, P1542, DOI 10.1111/psyp.12694
NR 120
TC 0
Z9 0
U1 7
U2 7
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR 28
PY 2024
VL 28
IS 2
AR 87
DI 10.1007/s10055-024-00944-x
PG 16
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA MQ5P4
UT WOS:001195109800001
OA Green Submitted, hybrid
DA 2024-08-05
ER

PT J
AU Dalmasso, V
   Moretti, M
   de'Sperati, C
AF Dalmasso, Vittorio
   Moretti, Michela
   de'Sperati, Claudio
TI Quasi-3D: reducing convergence effort improves visual comfort of
   head-mounted stereoscopic displays
SO VIRTUAL REALITY
LA English
DT Article
DE Asthenopia; Cyber-sickness; Depth perception; Head-mounted display; Near
   vision; Sensory-motor adaptation; Skin conductance; Physiological
   signals; Stereoscopic display; Spatial vision; Vergence eye movements;
   Virtual reality; Visual stress
ID DEPTH-PERCEPTION; VIRTUAL-REALITY; VERGENCE; EXPOSURE; ACCOMMODATION;
   ENVIRONMENTS; CONFLICT
AB The diffusion of virtual reality urges to solve the problem of vergence-accommodation conflict arising when viewing stereoscopic displays, which causes visual stress. We addressed this issue with an approach based on reducing ocular convergence effort. In virtual environments, vergence can be controlled by manipulating the binocular separation of the virtual cameras. Using this technique, we implemented two quasi-3D conditions characterized by binocular image separations intermediate between 3D (stereoscopic) and 2D (monoscopic). In a first experiment, focused on perceptual aspects, ten participants performed a visuo-manual pursuit task while wearing a head-mounted display (HMD) in head-constrained (non-immersive) condition for an overall exposure time of similar to 7 min. Passing from 3D to quasi-3D and 2D conditions, progressively resulted in a decrease of vergence eye movements-both mean convergence angle (static vergence) and vergence excursion (dynamic vergence)-and an increase of hand pursuit spatial error, with the target perceived further from the observer and larger. Decreased static and dynamic vergence predicted decreases in asthenopia trial-wise. In a second experiment, focused on tolerance aspects, fourteen participants performed a detection task in near-vision while wearing an HMD in head-free (immersive) condition for an overall exposure time of similar to 20 min. Passing from 3D to quasi-3D and 2D conditions, there was a general decrease of both subjective and objective visual stress indicators (ocular convergence discomfort ratings, cyber-sickness symptoms and skin conductance level). Decreased static and dynamic vergence predicted the decrease in these indicators. Remarkably, skin conductance level predicted all subjective symptoms, both trial-wise and session-wise, suggesting that it could become an objective replacement of visual stress self-reports. We conclude that relieving convergence effort by reducing binocular image separation in virtual environments can be a simple and effective way to decrease visual stress caused by stereoscopic HMDs. The negative side-effect-worsening of spatial vision-arguably would become unnoticed or compensated over time. This initial proof-of-concept study should be extended by future large-scale studies testing additional environments, tasks, displays, users, and exposure times.
C1 [Dalmasso, Vittorio; Moretti, Michela; de'Sperati, Claudio] Univ Vita Salute San Raffaele, Sch Psychol, Lab Act Percept & Cognit, Milan, Italy.
C3 Vita-Salute San Raffaele University
RP de'Sperati, C (corresponding author), Univ Vita Salute San Raffaele, Sch Psychol, Lab Act Percept & Cognit, Milan, Italy.
EM v.dalmasso1@studenti.unisr.it; desperati.claudio@unisr.it
OI Dalmasso, Vittorio/0009-0009-5264-1065; de'sperati,
   claudio/0000-0002-7322-2240
FU Fondazione Cariplo
FX The author thanks Marco Granato for computer programming and help in
   initial data acquisition, and Domenico Marchese for help in initial data
   acquisition.
CR Akiduki H, 2003, NEUROSCI LETT, V340, P197, DOI 10.1016/S0304-3940(03)00098-3
   Alberdi A, 2016, J BIOMED INFORM, V59, P49, DOI 10.1016/j.jbi.2015.11.007
   Alvarez TL, 2015, FRONT HUM NEUROSCI, V9, DOI 10.3389/fnhum.2015.00419
   Alvarez TL, 2010, OPTOMETRY VISION SCI, V87, P985, DOI 10.1097/OPX.0b013e3181fef1aa
   Anglin JM, 2017, SCI REP-UK, V7, DOI 10.1038/srep45469
   [Anonymous], 2012, 3D movie making: stereoscopic digital cinema from script to screen
   Banks MS, 2016, ANNU REV VIS SCI, V2, P397, DOI 10.1146/annurev-vision-082114-035800
   Bimberg P, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P464, DOI [10.1109/VRW50115.2020.00098, 10.1109/VRW50115.2020.0-178]
   Boucsein W, 2012, PSYCHOPHYSIOLOGY, V49, P1017, DOI 10.1111/j.1469-8986.2012.01384.x
   Cacioppo J., 2000, Handbook of Psychophysiology, Vsecond, DOI DOI 10.1017/9781107415782
   Chen SS, 2022, VIRTUAL REAL-LONDON, V26, P817, DOI 10.1007/s10055-021-00592-5
   Coles-Brennan C, 2019, CLIN EXP OPTOM, V102, P18, DOI 10.1111/cxo.12798
   Collier JD, 2011, OPTOMETRY, V82, P434, DOI 10.1016/j.optm.2010.10.013
   Combe E, 2011, FRONT PSYCHOL, V2, DOI 10.3389/fpsyg.2011.00205
   Cooper JS., 2011, Optometric clinical practice guideline care of the patient with accommodative and vergence dysfunction
   Duzmanska N, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.02132
   Eberhardt LV, 2021, INT J PSYCHOPHYSIOL, V168, P33, DOI 10.1016/j.ijpsycho.2021.08.001
   Gallagher M, 2018, MULTISENS RES, V31, P645, DOI 10.1163/22134808-20181293
   Gao ZP, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0205032
   Gregory RL, 2008, SPATIAL VISION, V21, P407, DOI 10.1163/156856808784532509
   Hoffman DM, 2008, J VISION, V8, DOI 10.1167/8.3.33
   Jones G, 2001, SPIE, V4297
   Kennedy R.S., 1993, Int. J. Aviat. Psy, P203
   Kennedy RS, 2000, PRESENCE-TELEOP VIRT, V9, P463, DOI 10.1162/105474600566952
   Kim EH, 2011, EXP BRAIN RES, V212, P267, DOI 10.1007/s00221-011-2727-7
   Kim HK, 2018, APPL ERGON, V69, P66, DOI 10.1016/j.apergo.2017.12.016
   Kooi FL, 2004, DISPLAYS, V25, P99, DOI 10.1016/j.displa.2004.07.004
   Koulieris GA, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073622
   Kramida G, 2016, IEEE T VIS COMPUT GR, V22, P1912, DOI 10.1109/TVCG.2015.2473855
   Lema AK, 2022, SAGE OPEN MED, V10, DOI 10.1177/20503121221142402
   Limanowski J, 2022, NEUROSCI BIOBEHAV R, V134, DOI 10.1016/j.neubiorev.2021.10.023
   Linton P, 2020, ATTEN PERCEPT PSYCHO, V82, P3176, DOI 10.3758/s13414-020-02006-1
   Liu ZY, 2022, J CLIN MED, V11, DOI 10.3390/jcm11237043
   Löw A, 2015, PSYCHOL SCI, V26, P1706, DOI 10.1177/0956797615597332
   Lyu JK, 2021, J VISION, V21, DOI 10.1167/jov.21.8.21
   Martin TA, 1996, BRAIN, V119, P1199, DOI 10.1093/brain/119.4.1199
   Melmoth DR, 2007, EXP BRAIN RES, V183, P283, DOI 10.1007/s00221-007-1041-x
   Mon-Williams M, 1999, EXP BRAIN RES, V128, P578, DOI 10.1007/s002210050885
   Mork R, 2020, INT ARCH OCC ENV HEA, V93, P29, DOI 10.1007/s00420-019-01457-w
   Mork R, 2018, INT ARCH OCC ENV HEA, V91, P811, DOI 10.1007/s00420-018-1324-5
   Naceri A, 2011, PRESENCE-TELEOP VIRT, V20, P254, DOI 10.1162/PRES_a_00048
   Nahar NK, 2007, OPTOMETRY VISION SCI, V84, P620, DOI 10.1097/OPX.0b013e3180dc99a8
   Naqvi SAA, 2013, IEEE ENG MED BIO, P6405, DOI 10.1109/EMBC.2013.6611020
   Neveu P, 2010, OPHTHAL PHYSL OPT, V30, P806, DOI 10.1111/j.1475-1313.2010.00763.x
   Prablanc C, 2020, NEUROSCI RES, V153, P8, DOI 10.1016/j.neures.2019.03.003
   Priot AE, 2011, NEUROPSYCHOLOGIA, V49, P498, DOI 10.1016/j.neuropsychologia.2010.11.028
   Priot AE, 2010, EXP BRAIN RES, V202, P825, DOI 10.1007/s00221-010-2188-4
   Rebenitsch L, 2016, VIRTUAL REAL-LONDON, V20, P101, DOI 10.1007/s10055-016-0285-9
   Recanzone GH, 2009, HEARING RES, V258, P89, DOI 10.1016/j.heares.2009.04.009
   RUSHTON S, 1994, DISPLAYS, V15, P255, DOI 10.1016/0141-9382(94)90073-6
   Scheiman M, 2003, OPTOMETRY VISION SCI, V80, P214, DOI 10.1097/00006324-200303000-00011
   Shibata T, 2011, PROC SPIE, V7863, DOI 10.1117/12.872347
   Urvoy M, 2013, ANN TELECOMMUN, V68, P641, DOI 10.1007/s12243-013-0394-3
   Verdelet G, 2019, 2019 INTERNATIONAL CONFERENCE ON 3D IMMERSION (IC3D), DOI 10.1109/ic3d48390.2019.8975994
   WANN JP, 1995, VISION RES, V35, P2731, DOI 10.1016/0042-6989(95)00018-U
   WELCH RB, 1993, PERCEPT PSYCHOPHYS, V54, P195, DOI 10.3758/BF03211756
   Zangemeister WH, 2020, J EYE MOVEMENT RES, V13, DOI 10.16910/jemr.13.4.6
   Zheng FH, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.686740
   Zhou Y, 2021, RESULTS OPT, V5, DOI 10.1016/j.rio.2021.100160
NR 59
TC 1
Z9 1
U1 0
U2 0
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 49
DI 10.1007/s10055-023-00923-8
PG 19
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA JJ1U7
UT WOS:001172714700001
OA hybrid
DA 2024-08-05
ER

PT J
AU Jung, KYE
   Kim, S
   Oh, S
   Yoon, SH
AF Jung, Kyungeun
   Kim, Sangpil
   Oh, Seungjae
   Yoon, Sang Ho
TI HapMotion: motion-to-tactile framework with wearable haptic devices for
   immersive VR performance experience
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Haptics; Wearables; Performance
ID MUSIC; SYSTEM; GLOVE; FEEL
AB We present a novel haptic rendering framework that translates the performer's motions into wearable vibrotactile feedback for an immersive virtual reality (VR) performance experience. Here, we employ a rendering pipeline that extracts meaningful vibrotactile parameters including intensity and location. We compute these parameters from the performer's upper-body movements which play a significant role in a dance performance. Therefore, we customize a haptic vest and sleeves to support vibrotactile feedback on the frontal and back parts of the torso and shoulders as well. To capture essential movements from the VR performance, we propose a method called motion salient triangle (MST). MST utilizes key skeleton joints' movements to compute the associated haptic parameters. Our method supports translating both choreographic and communicative motions into vibrotactile feedback. Through a series of user studies, we validate the user preference for our method compared to the conventional motion-to-tactile and audio-to-tactile methods.
C1 [Jung, Kyungeun; Yoon, Sang Ho] Korea Adv Inst Sci & Technol, Grad Sch Culture Technol, Daejeon 34141, South Korea.
   [Kim, Sangpil] Korea Univ, Dept Artificial Intelligence, Seoul 02841, South Korea.
   [Oh, Seungjae] Kyung Hee Univ, Dept Software Convergence, Yongin 17104, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST); Korea
   University; Kyung Hee University
RP Yoon, SH (corresponding author), Korea Adv Inst Sci & Technol, Grad Sch Culture Technol, Daejeon 34141, South Korea.; Oh, S (corresponding author), Kyung Hee Univ, Dept Software Convergence, Yongin 17104, South Korea.
EM kyungeun.jung@kaist.ac.kr; spk7@korea.ac.kr; oreo329@khu.ac.kr;
   sangho@kaist.ac.kr
RI Yoon, Sang Ho/N-2094-2016
OI Yoon, Sang Ho/0000-0002-3780-5350; Kim, Sangpil/0000-0002-7349-0018
FU Korea Creative Content Agency
FX No Statement Available
CR Abe M, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P827, DOI 10.1109/VRW55335.2022.00269
   Adolphe M., 2017, Mechanics and Mechanical Engineering, V21, P485
   Alamri A., 2010, Int J Comput Sci Sport (Int Assoc Comput Sci Sport), V9, P52
   Altinsoy ME, 2010, LECT NOTES COMPUT SC, V6306, P37, DOI 10.1007/978-3-642-15841-4_5
   [Anonymous], 2022, The show must go beyond
   Berdahl E., 2008, P 2008 C NEW INTERFA, P61
   bHaptics, 2019, Tactsuit, full body haptic suit for VR-Tactsuit
   Bieber J, 2021, Wave Presents: Justin Bieber-an interactive virtual experience
   Billboard, 2022, Billboard Hot 100
   Bisig D, 2022, PROCEEDINGS OF 2022 8TH INTERNATIONAL CONFERENCE ON MOVEMENT AND COMPUTING, MOCO 2022, DOI 10.1145/3537972.3537978
   Blumenfeld-Jones, 2008, HDB ARTS QUALITATIVE, P175
   Botvinick M, 1998, NATURE, V391, P756, DOI 10.1038/35784
   Chang Angela., 2005, CHI 05 EXTENDED ABST, P1264, DOI [DOI 10.1145/1056808.1056892, 10.1145/1056808.1056892]
   Charron JP, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.00800
   Cuartielles D, 2012, 7 INT WORKSH HAPT AU, P42
   Danieau F, 2013, IEEE T HAPTICS, V6, P193, DOI [10.1109/TOH.2012.70, 10.1109/ToH.2012.70]
   Danieau Fabien, 2012, P 18 ACM S VIRTUAL R, P69, DOI DOI 10.1145/2407336.2407350
   Degraen Donald, 2021, UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology, P936, DOI 10.1145/3472749.3474797
   Emotionwave XR, We are creating new musical experiences in an extended reality space
   Enriquez K, 2020, IBER CONF INF SYST, DOI 10.23919/cisti49556.2020.9140447
   Fang Cathy Mengying, 2021, UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology, P1109, DOI 10.1145/3472749.3474810
   Fontana F, 2016, An exploration on whole-body and foot-based vibrotactile sensitivity to melodic consonance
   Garcia-Valle G, 2021, IEEE T HAPTICS, V14, P538, DOI 10.1109/TOH.2020.3048290
   Golomer E, 2009, J ELECTROMYOGR KINES, V19, P314, DOI 10.1016/j.jelekin.2007.08.004
   Gonzalez-Franco M, 2018, FRONT ROBOT AI, V5, DOI 10.3389/frobt.2018.00074
   Hashizume S, 2018, LECT NOTES COMPUT SC, V10921, P359, DOI 10.1007/978-3-319-91125-0_30
   Hayes L, 2015, P C CREATIVITY COGNI, P359, DOI [10.1145/2757226.2757370, DOI 10.1145/2757226.2757370]
   Hwang I, 2017, 2017 IEEE WORLD HAPTICS CONFERENCE (WHC), P213, DOI 10.1109/WHC.2017.7989903
   Imschloss M, 2019, J RETAILING, V95, P158, DOI 10.1016/j.jretai.2019.10.004
   Inc F., 2020, Oculus Quest 2
   Israr A., 2011, 2011 IEEE World Haptics Conference (WHC 2011), P457, DOI 10.1109/WHC.2011.5945529
   Israr A, 2016, ACM SIGGRAPH 2016 ST, P1, DOI [10.1145/2929484.2970273, DOI 10.1145/2929484.2970273]
   Israr A, 2014, ACM T APPL PERCEPT, V11, DOI 10.1145/2641570
   Kaneko T, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR), P124, DOI 10.1109/AIVR.2018.00025
   Karageorghis CI, 2011, RES Q EXERCISE SPORT, V82, P274
   Karam M., 2010, EXTENDED ABSTRACTS H, P3069, DOI DOI 10.1145/1753846.1753919
   Kim M, 2014, IEEE T HAPTICS, V7, P394, DOI 10.1109/TOH.2013.58
   Kim Y, 2010, IEEE MULTIMEDIA, V17, P34, DOI 10.1109/MMUL.2010.5692181
   Kruijff E, 2017, VISUAL COMPUT, V33, P471, DOI 10.1007/s00371-016-1294-0
   Lalioti V, 2021, PROCEEDINGS OF 27TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, VRST 2021, DOI 10.1145/3489849.3489896
   Li YX, 2021, PROCEEDINGS OF 27TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, VRST 2021, DOI 10.1145/3489849.3489887
   Lin YH, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445254
   MacLean KaronE., 2017, The Handbook of Multimodal-Multisensor Interfaces: Foundations, User Modeling, and Common Modality Combinations - Volume, V1, P97
   Maereg AT, 2017, FRONT ROBOT AI, V4, DOI 10.3389/frobt.2017.00042
   Mazzoni A, 2016, ENTERTAIN COMPUT, V17, P9, DOI 10.1016/j.entcom.2016.06.002
   Melo M, 2022, IEEE T VIS COMPUT GR, V28, P1428, DOI 10.1109/TVCG.2020.3010088
   Microsoft, 2020, Azure Kinect DK
   Miklós A, 2015, J SOUND VIB, V334, P98, DOI 10.1016/j.jsv.2014.06.011
   Moritzen Karina., 2022, Journal of Sound and Music in Games, V3, P115, DOI DOI 10.1525/JSMG.2022.3.2-3.115
   Nanayakkara S, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P337
   Nichols C., 2002, P 2002 C NEW INTERFA, P1, DOI [10.1017/S135577180200211X, DOI 10.1017/S135577180200211X]
   Park G, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173832
   Pehkonen S, 2017, J CONTEMP ETHNOGR, V46, P699, DOI 10.1177/0891241616636663
   Rahman M.Abdur., 2010, Proceedings of the International Conference on Multimedia, P1643, DOI DOI 10.1145/1873951.1874310
   Rasool Shahzad, 2014, Transactions on Computational Science XXIII. Special Issue on Cyberworlds: LNCS 8490, P58, DOI 10.1007/978-3-662-43790-2_4
   Remache-Vinueza B, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21196575
   Schneider OS, 2015, UIST'15: PROCEEDINGS OF THE 28TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P21, DOI 10.1145/2807442.2807470
   Seo J, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174002
   Smyth T, 2006, 2006 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P45, DOI 10.1109/MMSP.2006.285266
   Sun ZD, 2022, NAT COMMUN, V13, DOI 10.1038/s41467-022-32745-8
   Tawa S, 2021, ADV ROBOTICS, V35, P268, DOI 10.1080/01691864.2020.1854114
   Thériault R, 2021, Q J EXP PSYCHOL, V74, P2057, DOI 10.1177/17470218211024826
   Tsai HR, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3501971
   Turchet L, 2021, PERS UBIQUIT COMPUT, V25, P749, DOI 10.1007/s00779-020-01395-2
   Turchet L, 2019, PROCEEDINGS OF THE 14TH INTERNATIONAL AUDIO MOSTLY CONFERENCE, AM 2019, P244, DOI 10.1145/3356590.3356629
   Turchet L, 2019, IEEE T HUM-MACH SYST, V49, P183, DOI 10.1109/THMS.2018.2885408
   Ur Réhman S, 2008, IEEE T MULTIMEDIA, V10, P1022, DOI 10.1109/TMM.2008.2001352
   Velt Raphael., 2015, Proceedings of the ACM International Conference on Interactive Experiences for TV and Online Video (Brussels, Belgium) (TVX'16), P1, DOI [10.1145/2745197.2745206, DOI 10.1145/2745197.2745206]
   Verillo R.T., 1966, PSYCHON SCI, V4, P135, DOI DOI 10.3758/BF03342215
   Veronesi D, 2014, SOC SEMIOT, V24, P468, DOI 10.1080/10350330.2014.929393
   Webb AM, 2016, ACM CONFERENCE ON COMPUTER-SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING (CSCW 2016), P432, DOI 10.1145/2818048.2819974
   West Travis J., 2019, Information in Intelligent Systems Lecture Notes in Computer Science, V11570, P70, DOI [10.1007/978-3-030-22649-7_7, DOI 10.1007/978-3-030-22649-7_7]
   Wilson G, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P400, DOI 10.1145/2993148.2998522
   Yakura H, 2020, INT SYM MIX AUGMENT, P555, DOI 10.1109/ISMAR50242.2020.00083
   Yamazaki R, 2019, ADV INTELL SYST, V774, P379, DOI 10.1007/978-3-319-94944-4_41
   Yamazaki Y., 2017, PROC EMERG TECHNOL, DOI [10.1145/3084822.3084843, DOI 10.1145/3084822.3084843]
   Yan Shuo, 2020, SIGGRAPH AS 2020 POS, DOI [10.1145/3415264.3425466, DOI 10.1145/3415264.3425466]
   Yingbo Wang, 2020, 2020 IEEE 6th International Conference on Computer and Communications (ICCC), P1151, DOI 10.1109/ICCC51575.2020.9344903
   Yun G, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445358
   Yun G, 2019, 2019 IEEE WORLD HAPTICS CONFERENCE (WHC), P616, DOI [10.1109/WHC.2019.8816104, 10.1109/whc.2019.8816104]
   Zhang K, 2020, S SPAT US INT, P1, DOI [10.1145/3385959.3418459, DOI 10.1145/3385959.3418459]
NR 81
TC 2
Z9 2
U1 5
U2 6
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 13
DI 10.1007/s10055-023-00910-z
PG 24
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA EI7M8
UT WOS:001138361900001
OA Bronze
DA 2024-08-05
ER

PT J
AU Narciso, D
   Melo, M
   Rodrigues, S
   Dias, D
   Cunha, J
   Vasconcelos-Raposo, J
   Bessa, M
AF Narciso, David
   Melo, Miguel
   Rodrigues, Susana
   Dias, Duarte
   Cunha, Joao
   Vasconcelos-Raposo, Jose
   Bessa, Maximino
TI Assessing the perceptual equivalence of a firefighting training exercise
   across virtual and real environments
SO VIRTUAL REALITY
LA English
DT Article
DE Computer graphics; Virtual reality; Professional training; Biofeedback
ID STRESS ASSESSMENT; DECISION-MAKING; EXPERIENCE; FATIGUE; FIRE
AB The advantages of Virtual Reality (VR) over traditional training, together with the development of VR technology, have contributed to an increase in the body of literature on training professionals with VR. However, there is a gap in the literature concerning the comparison of training in a Virtual Environment (VE) with the same training in a Real Environment (RE), which would contribute to a better understanding of the capabilities of VR in training. This paper presents a study with firefighters (N = 12) where the effect of a firefighter training exercise in a VE was evaluated and compared to that of the same exercise in a RE. The effect of environments was evaluated using psychophysiological measures by evaluating the perception of stress and fatigue, transfer of knowledge, sense of presence, cybersickness, and the actual stress measured through participants' Heart Rate Variability (HRV). The results showed a similar perception of stress and fatigue between the two environments; a positive, although not significant, effect of the VE on the transfer of knowledge; the display of moderately high presence values in the VE; the ability of the VE not to cause symptoms of cybersickness; and finally, obtaining signs of stress in participants' HRV in the RE and, to a lesser extent, signs of stress in the VE. Although the effect of the VE was shown to be non-comparable to that of the RE, the authors consider the results encouraging and discuss some key factors that should be addressed in the future to improve the results of the training VE.
C1 [Narciso, David; Vasconcelos-Raposo, Jose; Bessa, Maximino] Univ Tras Os Montes & Alto Douro, P-5000801 Vila Real, Portugal.
   [Dias, Duarte; Cunha, Joao] Univ Porto, Fac Engn, Rua Dr Roberto Frias, P-4200465 Porto, Portugal.
   [Narciso, David; Melo, Miguel; Rodrigues, Susana; Dias, Duarte; Cunha, Joao; Vasconcelos-Raposo, Jose; Bessa, Maximino] INESC TEC, Campus FEUP,Rua Dr Roberto Frias, P-4200465 Oporto, Portugal.
C3 University of Tras-os-Montes & Alto Douro; Universidade do Porto; INESC
   TEC
RP Narciso, D (corresponding author), Univ Tras Os Montes & Alto Douro, P-5000801 Vila Real, Portugal.; Narciso, D (corresponding author), INESC TEC, Campus FEUP,Rua Dr Roberto Frias, P-4200465 Oporto, Portugal.
EM davidgnarciso@gmail.com
RI Cunha, Joao Paulo/F-9039-2010; Branco VASCONCELOS-RAPOSO, JOSE
   Jacinto/G-3743-2010
OI Cunha, Joao Paulo/0000-0003-4131-9045; Bessa,
   Maximino/0000-0002-3002-704X; Melo, Miguel/0000-0003-4050-3473;
   Rodrigues, Susana/0000-0001-6546-340X; Branco VASCONCELOS-RAPOSO, JOSE
   Jacinto/0000-0002-3456-9727
FU FCT|FCCN (b-on); EU [833573]; Fundacao para a Ciencia e a Tecnologia-FCT
   [SFRH/BD/147334/2019]
FX Open access funding provided by FCT|FCCN (b-on). EU Framework
   programmeme for Research and Innovation 2014-2020, 833573, Fundacao para
   a Ciencia e a Tecnologia-FCT, SFRH/BD/147334/2019, David Narciso.
CR Aïm F, 2016, ARTHROSCOPY, V32, P224, DOI 10.1016/j.arthro.2015.07.023
   Backlund P, 2007, IEEE INT CONF INF VI, P899
   Biodevices, 2019, Biodevices Solucoes de engenharia biomedica
   Bliss JP, 1997, PRESENCE-TELEOP VIRT, V6, P73, DOI 10.1162/pres.1997.6.1.73
   BRISLIN RW, 1970, J CROSS CULT PSYCHOL, V1, P185, DOI 10.1177/135910457000100301
   Camm AJ, 1996, CIRCULATION, V93, P1043
   Cardoso R., 2002, O Stress nos Professores Portugueses: Estudo IPSSO 2000
   Castaldo R, 2015, BIOMED SIGNAL PROCES, V18, P370, DOI 10.1016/j.bspc.2015.02.012
   Cha M, 2012, FIRE SAFETY J, V50, P12, DOI 10.1016/j.firesaf.2012.01.004
   Clifford GD., 2006, ADV METHODS TOOLS EC, DOI DOI 10.1186/1475-925X-6-18
   Clifford RMS, 2018, 2018 IEEE WORKSHOP ON AUGMENTED AND VIRTUAL REALITIES FOR GOOD (VAR4GOOD)
   COHEN S, 1983, J HEALTH SOC BEHAV, V24, P385, DOI 10.2307/2136404
   Cohen-Hatton SR, 2015, J EXP PSYCHOL-APPL, V21, P395, DOI 10.1037/xap0000061
   CRUZNEIRA C, 1992, COMMUN ACM, V35, P64, DOI 10.1145/129888.129892
   Cunha Joao P. Silva, 2010, 2010 4th International Conference on Pervasive Computing Technologies for Healthcare (Pervasive Health 2010), DOI 10.4108/ICST.PERVASIVEHEALTH2010.8991
   de Visser H, 2011, MED J AUSTRALIA, V194, pS38
   Elbert R, 2018, IFAC PAPERSONLINE, V51, P686, DOI 10.1016/j.ifacol.2018.08.398
   Fisher GG, 2016, J OCCUP HEALTH PSYCH, V21, P3, DOI 10.1037/a0039139
   George D., 2010, SPSS WINDOWS STEP ST, DOI DOI 10.4324/9781003205333
   Hambleton RK., 2011, Cross-cultural research methods in psychology, P46, DOI DOI 10.1017/CBO9780511779381.004
   Kennedy R.S., 1993, Int. J. Aviat. Psy, P203
   Lei LiZ., 2005, Virtual reality, P194, DOI DOI 10.1007/S10055-004-0149-6
   Lesage FX, 2012, OCCUP MED-OXFORD, V62, P600, DOI 10.1093/occmed/kqs140
   Lombard M., 1997, Journal of Computer-Mediated Communication, V3, DOI [10.1111/j.1083-6101.1997.tb0, 10.1111/j.1083-6101.1997.tb00072.x., 10.1111/j.1083-6101.1997.tb00072.x, DOI 10.1111/J.1083-6101.1997.TB00072.X, 10.1111/J.1083-6101.1997.TB00072.X/4080403]
   Monteiro P, 2021, IEEE T HUM-MACH SYST, V51, P65, DOI 10.1109/THMS.2020.3030746
   Mossel A, 2017, P IEEE VIRT REAL ANN, P357, DOI 10.1109/VR.2017.7892324
   Narciso D., 2019, Multimedia Tools Appl, V79, P1
   Narciso D, 2023, IEEE T VIS COMPUT GR, V29, P3238, DOI 10.1109/TVCG.2022.3156734
   Querrec R., 2003, 5th Virtual Reality International Conference, P169
   Ragan ED, 2015, IEEE T VIS COMPUT GR, V21, P794, DOI 10.1109/TVCG.2015.2403312
   Rios Jerez A, 2022, Walking with virtual humans: understanding human response to virtual humanoids' appearance and behaviour while navigating in immersive VR
   Schlueter J., 2017, Mech Eng Conf Present Pap Proc, V10197, P1
   Schubert T, 2001, PRESENCE-VIRTUAL AUG, V10, P266, DOI 10.1162/105474601300343603
   Sobel-Fox RM, 2013, J PSYCHOSOC ONCOL, V31, P413, DOI 10.1080/07347332.2013.798760
   Tate DL, 1997, IEEE COMPUT GRAPH, V17, P23, DOI 10.1109/38.626965
   Tharion E, 2009, NATL MED J INDIA, V22, P63
   Vasconcelos-Raposo J, 2016, PRESENCE-VIRTUAL AUG, V25, P191, DOI 10.1162/PRES_a_00261
   VERCOULEN JHMM, 1994, J PSYCHOSOM RES, V38, P383, DOI 10.1016/0022-3999(94)90099-X
NR 38
TC 0
Z9 0
U1 5
U2 6
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 14
DI 10.1007/s10055-023-00917-6
PG 14
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA EM3D5
UT WOS:001139295200001
OA hybrid
DA 2024-08-05
ER

PT J
AU Ulloa, CC
   Domínguez, D
   del Cerro, J
   Barrientos, A
AF Cruz Ulloa, Christyan
   Dominguez, David
   del Cerro, Jaime
   Barrientos, Antonio
TI Analysis of MR-VR tele-operation methods for legged-manipulator robots
SO VIRTUAL REALITY
LA English
DT Article
DE Legged-manipulator robot; Virtual reality; Mixed reality; Quadruped
   robot; ROS; Search and rescue
ID VIRTUAL-REALITY; MIXED-REALITY; PERCEPTION; SYSTEM
AB The development of immersive technologies in recent years has facilitated the control and execution of tasks at a high level of complexity in robotic systems. On the other hand, exploration and manipulation tasks in unknown environments have been one of the main challenges in search and rescue (SAR) robotics. Due to the complexity and uncertainty involved in autonomous manipulation tasks in unstructured environments, these are usually tele-operated initially. This article addresses a comparative study between Mixed Reality (MR-Hololens) and Virtual Reality (VR-HTC-Vive) methods for teleoperating legged-manipulator robots in the context of search and rescue. For this purpose, a teleoperation robotics method was established to address the comparison, developing VR-MR interfaces with the same contextualization and operational functionality for mission management and robot control of a robotic set composed of a quadrupedal robot equipped with a 6 degrees of freedom (6DoF) manipulator, by a user using hand gestures. A set of metrics is proposed for the comparative evaluation of the interfaces considering parameters that allow analyzing operability in the context of the mission (latencies, physical parameters of the equipment, etc.), as well as from the aspect of operator performance (required training, confidence levels, etc.). The experimental phase was conducted using both on-site and remote operations to evaluate and categorize the advantages and disadvantages of each method.
C1 [Cruz Ulloa, Christyan; Dominguez, David; del Cerro, Jaime; Barrientos, Antonio] Univ Politecn Madrid, Ctr Automat & Robot, Consejo Super Invest Cient, Madrid 28006, Spain.
C3 Consejo Superior de Investigaciones Cientificas (CSIC); Universidad
   Politecnica de Madrid; CSIC-UPM - Centro de Automatica y Robotica
RP Ulloa, CC; Barrientos, A (corresponding author), Univ Politecn Madrid, Ctr Automat & Robot, Consejo Super Invest Cient, Madrid 28006, Spain.
EM christyan.cruz.ulloa@upm.es; d.dominguezc@alumnos.upm.es;
   j.cerro@upm.es; antonio.barrientos@upm.es
RI ; Barrientos, Antonio/B-4053-2013
OI Del Cerro, Jaime/0000-0003-4893-2571; Barrientos,
   Antonio/0000-0003-1691-3907; Cruz, Christyan/0000-0003-2824-6611
FU CRUE-CSIC; Springer Nature; Proyecto CollaborativE Search And Rescue
   robots (CESAR) - MCIN/AEI [PID2022-142129OB-I00]; ERDF A way of making
   Europe; Ministerio de Ciencia, Innovacion y Universidades
   [PID2022-142129OB-I00]
FX Open Access funding provided thanks to the CRUE-CSIC agreement with
   Springer Nature. This research has been possible thanks to the financing
   of "Proyecto CollaborativE Search And Rescue robots (CESAR)"
   (PID2022-142129OB-I00) founded by
   MCIN/AEI/https://doi.org/10.13039/501100011033 and "ERDF A way of making
   Europe". Ministerio de Ciencia, Innovacion y Universidades,
   PID2022-142129OB-I00, Jaime del Cerro
CR Akan Batu, 2011, IEEE International Conference on Robotics and Automation, P3934
   Banerjee NT, 2022, IEEE T VIS COMPUT GR, V28, P4787, DOI 10.1109/TVCG.2021.3105606
   Biswal P, 2021, AIN SHAMS ENG J, V12, P2017, DOI 10.1016/j.asej.2020.11.005
   Blackburn MR., 2002, Tech Doc, V3141, P8
   Blissing B, 2019, TRANSPORT RES F-TRAF, V61, P229, DOI 10.1016/j.trf.2017.08.005
   BRADLEY MM, 1994, J BEHAV THER EXP PSY, V25, P49, DOI 10.1016/0005-7916(94)90063-9
   Brooke J., 1996, SUS-a quick and dirty usability scale, DOI [DOI 10.1201/9781498710411-35, DOI 10.1201/9781498710411]
   Bruzzone L, 2012, MECH SCI, V3, P49, DOI 10.5194/ms-3-49-2012
   Chakraborti T, 2018, IEEE INT C INT ROBOT, P4476, DOI 10.1109/IROS.2018.8593830
   Chang CT, 2022, ACMIEEE INT CONF HUM, P1237, DOI 10.1109/HRI53351.2022.9889655
   Ulloa CC, 2023, LECT NOTE NETW SYST, V530, P181, DOI 10.1007/978-3-031-15226-9_19
   Eguchi R, 2012, Technical Report
   Fan XY, 2023, IND ROBOT, V50, P804, DOI 10.1108/IR-12-2022-0306
   Galarza BR, 2023, ROBOTICS, V12, DOI 10.3390/robotics12060163
   Hart SG., 2006, P HUM FACT ERG SOC A, V50, P904, DOI [10.1177/154193120605000909, DOI 10.1177/154193120605000909]
   He J, 2019, ELECTRONICS-SWITZ, V8, DOI 10.3390/electronics8121414
   Hidalgo CE, 2020, REV IBEROAM AUTOM IN, V17, P56, DOI 10.4995/riai.2019.11155
   Hönig W, 2015, IEEE INT C INT ROBOT, P5382, DOI 10.1109/IROS.2015.7354138
   Huang Y., 2019, J CIVIL ENG ARCHITEC, V13, P409
   Humphreys J, 2022, LECT NOTES ARTIF INT, V13546, P63, DOI 10.1007/978-3-031-15908-4_6
   Roldán JJ, 2019, ROBOT CIM-INT MANUF, V59, P305, DOI 10.1016/j.rcim.2019.05.004
   Kaplan AD, 2021, HUM FACTORS, V63, P706, DOI 10.1177/0018720820904229
   Klamt T, 2020, J FIELD ROBOT, V37, P889, DOI 10.1002/rob.21895
   Kot T, 2018, 2018 19TH INTERNATIONAL CARPATHIAN CONTROL CONFERENCE (ICCC), P422, DOI 10.1109/CarpathianCC.2018.8399667
   Kruijff-Korbayová I, 2016, IEEE INT SYMP SAFE, P278, DOI 10.1109/SSRR.2016.7784314
   Krupke D, 2018, 2018 IEEE RSJ INT C, P1
   Liang CY, 2019, 2019 IEEE 4TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS AND MECHATRONICS (ICARM 2019), P384, DOI [10.1109/icarm.2019.8834302, 10.1109/ICARM.2019.8834302]
   Livatino S, 2021, IEEE ACCESS, V9, P25795, DOI 10.1109/ACCESS.2021.3057808
   Lungu AJ, 2021, EXPERT REV MED DEVIC, V18, P47, DOI 10.1080/17434440.2021.1860750
   Luu T, 2024, Effectiveness of IoT and VR for real-time teleoperation of industrial robots
   Makhataeva Z, 2020, ROBOTICS, V9, DOI 10.3390/robotics9020021
   Martín-Barrio A, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20082181
   Martín-Barrio A, 2020, VIRTUAL REAL-LONDON, V24, P541, DOI 10.1007/s10055-019-00414-9
   Mascio Tania, 2020, Methodologies and Intelligent Systems for Technology Enhanced Learning, 9th International Conference. Advances in Intelligent Systems and Computing (AISC 1007), P153, DOI 10.1007/978-3-030-23990-9_19
   Milman N.B., 2018, DISTANCE LEARNING, V15, P55
   Muller Jens., 2019, Proceedings of Mensch und Computer, P399, DOI [10.1145/3340764.3340773, DOI 10.1145/3340764.3340773]
   Murphy RR, 2014, INTELL ROBOT AUTON, P21
   Ostanin M., 2020, Int J Prod Res, V52, P695
   Park KB, 2021, IEEE ACCESS, V9, P55448, DOI 10.1109/ACCESS.2021.3071364
   Penco L, 2024, IEEE ROBOT AUTOM LET, V9, P1937, DOI 10.1109/LRA.2024.3349809
   Peppoloni L., 2015, 3D USER INTERFACES 3, P177, DOI DOI 10.1109/3DUI.2015.7131758
   Pettersen T, 2003, SECOND IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P319, DOI 10.1109/ISMAR.2003.1240739
   Quesada RC, 2022, IEEE INT C INT ROBOT, P856, DOI 10.1109/IROS47612.2022.9981989
   Romano S, 2020, IEEE T SOFTWARE ENG, V46, P71, DOI 10.1109/TSE.2018.2842781
   Sha XM, 2019, LECT NOTES ARTIF INT, V11740, P571, DOI 10.1007/978-3-030-27526-6_50
   Stadler S, 2016, IEEE ROMAN, P179, DOI 10.1109/ROMAN.2016.7745108
   Ulloa CC, 2023, J COMPUT DES ENG, V10, P1479, DOI 10.1093/jcde/qwad061
   Ulloa CC, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22218146
   Walker ME, 2024, P 2024 ACM IEEE INT, P762
   Walker M, 2023, ACM T HUM-ROBOT INTE, V12, DOI 10.1145/3597623
   Walker ME, 2019, ACMIEEE INT CONF HUM, P202, DOI [10.1109/HRI.2019.8673306, 10.1109/hri.2019.8673306]
   Wannous C., 2017, Advancing culture of living with landslides
   Wassermann J, 2018, PROC CIRP, V76, P161, DOI 10.1016/j.procir.2018.01.036
   White JWC, 2018, IMPROVING CHARACTERIZATION OF ANTHROPOGENIC METHANE EMISSIONS IN THE UNITED STATES, P1
   Williams T, 2018, ACMIEEE INT CONF HUM, P403, DOI 10.1145/3173386.3173561
   Wohlin C., 2012, Experimentation in Software Engineering, DOI [10.1007/978-3-642-29044-2., DOI 10.1007/978-3-642-29044-2]
   Wu ML, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10031135
   Wu ML, 2018, CHIN AUTOM CONGR, P1867, DOI 10.1109/CAC.2018.8623114
   Xia PX, 2023, COMPUT IND, V145, DOI 10.1016/j.compind.2022.103836
   Zhang QL, 2023, BIOMIMETICS-BASEL, V8, DOI 10.3390/biomimetics8080605
   Zhou CX, 2022, Arxiv, DOI arXiv:2209.10314
NR 61
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD JUL 2
PY 2024
VL 28
IS 3
AR 131
DI 10.1007/s10055-024-01021-z
PG 17
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA XH1B5
UT WOS:001260689400001
OA hybrid
DA 2024-08-05
ER

PT J
AU Mancuso, V
   Borghesi, F
   Bruni, F
   Pedroli, E
   Cipresso, P
AF Mancuso, Valentina
   Borghesi, Francesca
   Bruni, Francesca
   Pedroli, Elisa
   Cipresso, Pietro
TI Mapping the landscape of research on 360-degree videos and images: a
   network and cluster analysis
SO VIRTUAL REALITY
LA English
DT Article
DE 360 degrees Videos and images; Cluster analysis; Virtual reality;
   Psychometrics; Scientometrics
ID SCIENCE; COCITATION; MAPS
AB The recent emergence of low-cost virtual reality technologies, like 360 degrees videos and images is attracting the attention of researchers suggesting it could be the next significant step in technological innovation. The birth of 360 degrees videos and images is quite young, it goes back to the middle of the nineteenth century and then spread more and more in many areas. In recent years, 360 degrees videos and images have grown in popularity because they provide a great number of advantages compared to traditional virtual reality computer-generated technology. The aim of this research is to map scientific works in the area of 360 degrees technology using advanced scientometric techniques. We collected all the existent articles about 360 degrees contents in the Scopus database, and the resultant dataset contained 3319 records. The bibliographic record encompassed all categories of scientific articles retrieved from Scopus, considering parameters such as countries, institutions, journals, authors, citation counts, and publication years. The network and cluster analysis of the literature showed a composite panorama characterized by changes and evolutions over time of the use of 360 degrees contents. We discuss these aspects in the main areas of application with an emphasis on the future expected 360 degrees capacities, increases, and challenges. As already happened with the advent of virtual reality, the future of 360 degrees technology will be an increasing shift from engineering to clinical use, by improving the use and the development of scientific applications in clinical areas and by modifying social communication and interaction among people.
C1 [Mancuso, Valentina; Bruni, Francesca; Pedroli, Elisa] eCampus Univ, Fac Psychol, Novedrate, Italy.
   [Borghesi, Francesca; Cipresso, Pietro] Univ Turin, Dept Psychol, Turin, Italy.
   [Pedroli, Elisa] IRCCS Ist Auxol Italiano, Dept Geriatr & Cardiovasc Med, Milan, Italy.
C3 Universita Ecampus; University of Turin; IRCCS Istituto Auxologico
   Italiano
RP Mancuso, V (corresponding author), eCampus Univ, Fac Psychol, Novedrate, Italy.
EM v.mancuso95@gmail.com
RI Borghesi, Francesca/HTO-3363-2023; Pedroli, Elisa/K-5751-2016
OI Borghesi, Francesca/0000-0003-1356-8271; Mancuso,
   Valentina/0000-0002-4198-3723; Pedroli, Elisa/0000-0003-4012-262X;
   Bruni, Francesca/0000-0001-9911-0573
FU Italian Ministry of Health [GR-2021-12374204]
FX Research funded by the Italian Ministry of Health and GR-2021-12374204.
CR Aitamurto T, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445183
   Nguyen A, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1190, DOI 10.1145/3240508.3240669
   [Anonymous], 2015, 20 INT C SCI TECHN I
   Bales ME, 2020, Bibliometric visualization and analysis software: state of the art, workflows, and best practices
   Bales ME, 2011, AM J PREV MED, V41, P112, DOI 10.1016/j.amepre.2011.03.018
   Bales Michael E, 2009, AMIA Annu Symp Proc, V2009, P24
   Barreda-Angeles M, 2020, VIRTUAL REAL-LONDON, V24, P289, DOI 10.1007/s10055-019-00400-1
   Blair C, 2021, BMC MED EDUC, V21, DOI 10.1186/s12909-021-03013-y
   Borghesi F., 2022, From Virtual Reality to 360 Videos, P549, DOI DOI 10.4018/978-1-6684-4854-0.CH023
   Brandes U, 2001, J MATH SOCIOL, V25, P163, DOI 10.1080/0022250X.2001.9990249
   Buter RK, 2006, SCIENTOMETRICS, V66, P377, DOI 10.1007/s11192-006-0027-y
   Buter RK, 2001, SCIENTOMETRICS, V51, P55, DOI 10.1023/A:1010560527236
   Calogiuri G, 2018, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.02321
   Chen CM, 2010, J AM SOC INF SCI TEC, V61, P1386, DOI 10.1002/asi.21309
   Chen CM, 2006, J AM SOC INF SCI TEC, V57, P359, DOI 10.1002/asi.20317
   Chen S. E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P29, DOI 10.1145/218380.218395
   Chirico A, 2019, CYBERPSYCH BEH SOC N, V22, P220, DOI 10.1089/cyber.2018.0393
   Chirico A, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-01242-0
   Cipresso P, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.02086
   Cosoli R, 2023, INT J PSYCHOPHYSIOL, V188, P114, DOI 10.1016/j.ijpsycho.2023.05.291
   El-Ganainy T, 2016, Streaming virtual reality content
   Falagas ME, 2008, FASEB J, V22, P338, DOI 10.1096/fj.07-9492LSF
   Farmer H, 2021, J EXP PSYCHOL-APPL, V27, P324, DOI 10.1037/xap0000332
   FREEMAN LC, 1977, SOCIOMETRY, V40, P35, DOI 10.2307/3033543
   González-Teruel A, 2015, SCIENTOMETRICS, V103, P687, DOI 10.1007/s11192-015-1548-z
   Hamilton D, 2021, J COMPUT EDUC, V8, P1, DOI 10.1007/s40692-020-00169-2
   Hodgson J M, 1989, Int J Card Imaging, V4, P187, DOI 10.1007/BF01745149
   Hosseini M, 2017, P IEEE VIRT REAL ANN, P423, DOI 10.1109/VR.2017.7892357
   Ionescu Alina., 2021, J. technol. behave. sci, V6, P631, DOI DOI 10.1007/S41347-021-00221-7
   Jun HS, 2022, IEEE T AFFECT COMPUT, V13, P1416, DOI 10.1109/TAFFC.2020.3004617
   Kuzyakov E., 2016, "Next-Generation Video Encoding Techniques for 360 Video and VR
   Le Feuvre J, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P329, DOI 10.1145/2910017.2910641
   Li BJ, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.02116
   Lippman A., 1980, Computer Graphics, V14, P32, DOI 10.1145/965105.807465
   Losè LT, 2020, IOP CONF SER-MAT SCI, V949, DOI 10.1088/1757-899X/949/1/012060
   Mancuso V, 2020, FRONT NEUROL, V11, DOI 10.3389/fneur.2020.566731
   Mangiante S, 2017, VR/AR NETWORK '17: PROCEEDINGS OF THE 2017 WORKSHOP ON VIRTUAL REALITY AND AUGMENTED REALITY NETWORK, P30, DOI 10.1145/3097895.3097901
   Miller G., 1992, Journal of Visualization and Computer Animation, V3, P183, DOI 10.1002/vis.4340030305
   Noyons ECM, 1999, SCIENTOMETRICS, V46, P591, DOI 10.1007/BF02459614
   Orosz K, 2016, SCIENTOMETRICS, V108, P829, DOI 10.1007/s11192-016-1971-9
   Pedroli E, 2022, FRONT HUM NEUROSCI, V16, DOI 10.3389/fnhum.2022.898633
   Pedroli E, 2021, IEEE T EMERG TOP COM, V9, P1290, DOI 10.1109/TETC.2020.3037962
   Polanco X, 2001, SCIENTOMETRICS, V51, P267, DOI 10.1023/A:1010537316758
   precedenceresearch, 360 Degree Camera Market Size to Reach US$ 4.64 Bn by 2030
   Qian F., 2016, P 5 WORKSH ALL THING, P1, DOI DOI 10.1145/2980055.2980056
   Quesnel D, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.02158
   Ripley GD, 1990, J Comput High Educ, V1, P74, DOI [10.1007/BF02941636/METRICS, DOI 10.1007/BF02941636/METRICS]
   Riva G, 2020, EXPERT REV MED DEVIC, V17, P1035, DOI 10.1080/17434440.2020.1825939
   Rosendahl P, 2024, EDUC INF TECHNOL, V29, P1319, DOI 10.1007/s10639-022-11549-9
   Rupp MA, 2019, COMPUT EDUC, V128, P256, DOI 10.1016/j.compedu.2018.09.015
   Schöne B, 2023, CURR PSYCHOL, V42, P5366, DOI 10.1007/s12144-021-01841-1
   Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599
   SMALL H, 1973, J AM SOC INFORM SCI, V24, P265, DOI 10.1002/asi.4630240406
   Small H, 1999, J AM SOC INFORM SCI, V50, P799, DOI 10.1002/(SICI)1097-4571(1999)50:9<799::AID-ASI9>3.0.CO;2-G
   Snelson C, 2020, TECHTRENDS, V64, P404, DOI 10.1007/s11528-019-00474-3
   Stramba-Badiale C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00720
   Takacs B, 2007, LECT NOTES COMPUT SC, V4740, P219
   Walshe N, 2019, TEACH TEACH EDUC, V78, P97, DOI 10.1016/j.tate.2018.11.009
   Wu B, 2023, INTERACT LEARN ENVIR, DOI 10.1080/10494820.2023.2184389
   Yu M, 2015, 2015 IEEE International Symposium on Mixed and Augmented Reality, P31, DOI 10.1109/ISMAR.2015.12
   ZITT M, 1994, SCIENTOMETRICS, V30, P333, DOI 10.1007/BF02017232
NR 61
TC 2
Z9 2
U1 1
U2 1
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD APR 24
PY 2024
VL 28
IS 2
AR 101
DI 10.1007/s10055-024-01002-2
PG 18
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA OM5I2
UT WOS:001207699600002
OA hybrid
DA 2024-08-05
ER

PT J
AU Brea-Gómez, B
   Laguna-González, A
   Pérez-Gisbert, L
   Valenza, MC
   Torres-Sánchez, I
AF Brea-Gomez, Beatriz
   Laguna-Gonzalez, Alejandro
   Perez-Gisbert, Laura
   Valenza, Marie Carmen
   Torres-Sanchez, Irene
TI Virtual reality based rehabilitation in adults with chronic neck pain: a
   systematic review and meta-analysis of randomized clinical trials
SO VIRTUAL REALITY
LA English
DT Article
DE Chronic neck pain; Virtual reality; Rehabilitation; Physical therapy;
   Disability
ID CHRONIC MUSCULOSKELETAL PAIN; METHODOLOGICAL QUALITY; INTERVENTIONS;
   MANAGEMENT; DISORDERS; GUIDELINE; STROKE; BACK
AB Chronic neck pain is one of the most frequent musculoskeletal disorders, with high prevalence worldwide. Rehabilitation is an essential component of therapeutic strategy. Virtual reality based rehabilitation (VRBR) is a powerful distraction technique that could be beneficial for chronic neck pain patients. The objective of this systematic review was to analyse the effectiveness of VRBR in chronic neck pain treatment. We followed the PRISMA guidelines and used four databases (CINAHL, Medline (Via PubMed), Scopus and Web of Science) from their inception to August 2023. Eligibility criteria were established using PICOS. Methodological quality was evaluated with the Downs and Black scale and the risk of bias with the Revised Cochrane risk-of-bias tool. The meta-analysis was performed using the RevMan software. Six studies were included in the systematic review and the meta-analysis. We observed significant differences in favour of VRBR for pain intensity (SMD = - 0.46; 95% CI = - 0.74, - 0.19; p = 0.001), disability (MD = - 2.84; 95% CI = - 4.23, - 1.45; p < 0.0001), global perceived effect (MD = 0.49; 95% CI = 0.25, 0.72; p < 0.0001) and patient satisfaction (MD = 0.62; 95% CI = 0.38, 0.86; p < 0.00001). However, at short-term follow-up significant differences were only obtained for disability (MD = - 3.52; 95% CI = - 5.85, - 1.20; p = 0.003). VRBR can significantly improve pain intensity, disability, global perceived effect and patient satisfaction. The small number of articles included in the analysis is a limitation, even considering the good methodological quality of these studies. Investigating the effects of VRBR on mid and long-term follow-up and exploring different types of VR are needed.
C1 [Brea-Gomez, Beatriz; Laguna-Gonzalez, Alejandro; Perez-Gisbert, Laura; Valenza, Marie Carmen; Torres-Sanchez, Irene] Univ Granada, Physiotherapy Dept, Ave Ilustrac, 60, Granada 18016, Andalucia, Spain.
C3 University of Granada
RP Torres-Sánchez, I (corresponding author), Univ Granada, Physiotherapy Dept, Ave Ilustrac, 60, Granada 18016, Andalucia, Spain.
EM beatrizbrea@correo.ugr.es; alagon1994@gmail.com;
   laurapg98@correo.ugr.es; cvalenza@ugr.es; irenetorres@ugr.es
FU Universidad de Granada
FX No Statement Available
CR Ahern MM, 2020, PAIN PRACT, V20, P656, DOI 10.1111/papr.12885
   Asadzadeh Afsoon, 2021, Informatics in Medicine Unlocked, V24, DOI 10.1016/j.imu.2021.100562
   Bahat HS, 2018, EUR SPINE J, V27, P1309, DOI 10.1007/s00586-017-5323-0
   Bahat HS, 2015, MANUAL THER, V20, P68, DOI 10.1016/j.math.2014.06.008
   Bailey DL, 2020, BRIT J SPORT MED, V54, P326, DOI 10.1136/bjsports-2017-098742
   Baradwan S, 2022, SEX REPROD HEALTHC, V32, DOI 10.1016/j.srhc.2022.100720
   Bier JD, 2018, PHYS THER, V98, P162, DOI 10.1093/ptj/pzx118
   Blanpied Peter R, 2017, J Orthop Sports Phys Ther, V47, pA1, DOI 10.2519/jospt.2017.0302
   Brea-Gomez B, 2021, INT J ENV RES PUB HE, V18, DOI 10.3390/ijerph182211806
   Cetin H, 2022, MUSCULOSKEL SCI PRAC, V62, DOI 10.1016/j.msksp.2022.102636
   Cipresso P, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.02086
   Corbetta D, 2015, J PHYSIOTHER, V61, P117, DOI 10.1016/j.jphys.2015.05.017
   de Campos TF, 2018, J PHYSIOTHER, V64, P159, DOI 10.1016/j.jphys.2018.05.003
   De Miguel-Rubio A, 2020, JMIR MHEALTH UHEALTH, V8, DOI 10.2196/22537
   Deeks J, 2022, Cochrane Handbook for Systematic Reviews of Interventions Version 6.3
   Dominguez-Tellez P, 2020, GAMES HEALTH J, V9, P1, DOI 10.1089/g4h.2019.0043
   Downs SH, 1998, J EPIDEMIOL COMMUN H, V52, P377, DOI 10.1136/jech.52.6.377
   Erdogan K, 2023, VIRTUAL REAL-LONDON, V27, P481, DOI 10.1007/s10055-022-00739-y
   Fang Q, 2020, GAMES HEALTH J, V9, P11, DOI 10.1089/g4h.2019.0016
   Farrell SF, 2019, J MAGN RESON IMAGING, V49, P1638, DOI 10.1002/jmri.26567
   Furlan AD, 2015, SPINE, V40, P1660, DOI 10.1097/BRS.0000000000001061
   Garcia LM, 2021, J MED INTERNET RES, V23, DOI 10.2196/26292
   Gava V, 2022, GAMES HEALTH J, V11, P369, DOI 10.1089/g4h.2021.0232
   Gavish L, 2023, GAMES HEALTH J, V12, P468, DOI 10.1089/g4h.2023.0036
   Goudman L, 2022, JMIR SERIOUS GAMES, V10, DOI 10.2196/34402
   Grassini S, 2022, INT J ENV RES PUB HE, V19, DOI 10.3390/ijerph19074071
   Gross AR, 2016, MANUAL THER, V24, P25, DOI 10.1016/j.math.2016.04.005
   Gumaa M, 2019, PHYS THER, V99, P1304, DOI 10.1093/ptj/pzz093
   Guo QF, 2023, J MED INTERNET RES, V25, DOI 10.2196/38256
   Hooper P, 2008, CAN J OPHTHALMOL, V43, P180, DOI 10.3129/i08-001
   I Rezaei, 2019, J Biomed Phys Eng, V9, P355, DOI 10.31661/jbpe.v0i0.556
   Johnson Daniel, 2016, Internet Interv, V6, P89, DOI 10.1016/j.invent.2016.10.002
   Kantha P, 2023, GAMES HEALTH J, V12, P1, DOI 10.1089/g4h.2022.0088
   Kato PM, 2010, REV GEN PSYCHOL, V14, P113, DOI 10.1037/a0019441
   Kazeminasab S, 2022, BMC MUSCULOSKEL DIS, V23, DOI 10.1186/s12891-021-04957-4
   Kulkarni J, 2020, BRIT J PAIN, V14, P92, DOI 10.1177/2049463719859913
   Kwon SH, 2023, J NEUROENG REHABIL, V20, DOI 10.1186/s12984-023-01219-3
   Lauwens Y, 2020, CHILDREN-BASEL, V7, DOI 10.3390/children7110194
   Li L, 2017, AM J TRANSL RES, V9, P3867
   López-de-Uralde-Villanueva I, 2016, PAIN PHYSICIAN, V19, P77
   Macdonald G, 2012, COCHRANE DB SYST REV, DOI 10.1002/14651858.CD001930.pub3
   Tejera DM, 2020, INT J ENV RES PUB HE, V17, DOI 10.3390/ijerph17165950
   Navarro-Albarracín C, 2018, MED CLIN-BARCELONA, V150, P428, DOI 10.1016/j.medcli.2017.11.028
   Nijs J, 2013, MANUAL THER, V18, P96, DOI 10.1016/j.math.2012.11.001
   Nusser M, 2021, J REHABIL MED, V53, DOI 10.2340/16501977-2786
   Page MJ, 2021, BMJ-BRIT MED J, V372, DOI [10.1016/j.ijsu.2021.105906, 10.1136/bmj.n71, 10.1136/bmj.n160]
   Parikh P, 2019, BMC MUSCULOSKEL DIS, V20, DOI 10.1186/s12891-019-2441-3
   Pereira Margarida F, 2020, J Biomed Inform, V111, P103584, DOI 10.1016/j.jbi.2020.103584
   Rutkowski S, 2020, J REHABIL MED, V52, DOI 10.2340/16501977-2755
   Safiri S, 2020, BMJ-BRIT MED J, V368, DOI 10.1136/bmj.m791
   Silverman SR, 2012, SPINAL CORD, V50, P718, DOI 10.1038/sc.2012.78
   Smith V, 2020, J MED INTERNET RES, V22, DOI 10.2196/17980
   Sterne JAC, 2019, BMJ-BRIT MED J, V366, DOI 10.1136/bmj.l4898
   Sutherland I.E., 1965, Proceedings of the IFIP Congress, V2, P506, DOI DOI 10.1109/MC.2005.274
   Sanchez IT, 2019, GAMES HEALTH J, V8, P237, DOI 10.1089/g4h.2018.0062
   Trost Z, 2015, PAIN MANAG, V5, P197, DOI [10.2217/pmt.15.6, 10.2217/PMT.15.6]
   Verhagen AP, 2021, J PHYSIOTHER, V67, P5, DOI 10.1016/j.jphys.2020.12.005
   Vlaeyen JWS, 2012, PAIN, V153, P1144, DOI 10.1016/j.pain.2011.12.009
   Wang SS, 2023, J CLIN NURS, V32, P3074, DOI 10.1111/jocn.16397
NR 59
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR 27
PY 2024
VL 28
IS 2
AR 86
DI 10.1007/s10055-024-00979-0
PG 31
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA MQ5Z6
UT WOS:001195120000001
OA hybrid
DA 2024-08-05
ER

PT J
AU Ong, T
   Ivanova, J
   Soni, H
   Wilczewski, H
   Barrera, J
   Cummins, M
   Welch, BM
   Bunnell, BE
AF Ong, Triton
   Ivanova, Julia
   Soni, Hiral
   Wilczewski, Hattie
   Barrera, Janelle
   Cummins, Mollie
   Welch, Brandon M.
   Bunnell, Brian E.
TI Therapist perspectives on telehealth-based virtual reality exposure
   therapy
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Exposure therapy; Telehealth; Mental health; Clinical
   practice
ID ANXIETY
AB Virtual reality (VR) can enhance mental health care. In particular, the effectiveness of VR-based exposure therapy (VRET) has been well-demonstrated for treatment of anxiety disorders. However, most applications of VRET remain localized to clinic spaces. We aimed to explore mental health therapists' perceptions of telehealth-based VRET (tele-VRET) by conducting semi-structured, qualitative interviews with 18 telemental health therapists between October and December 2022. Interview topics included telehealth experiences, exposure therapy over telehealth, previous experiences with VR, and perspectives on tele-VRET. Therapists described how telehealth reduced barriers (88.9%, 16/18), enhanced therapy (61.1%, 11/18), and improved access to clients (38.9%, 7/18), but entailed problems with technology (61.1%, 11/18), uncontrolled settings (55.6%, 10/18), and communication difficulties (50%, 9/18). Therapists adapted exposure therapy to telehealth by using online resources (66.7%, 12/18), preparing client expectations (55.6%, 10/18), and adjusting workflows (27.8%, 5/18). Most therapists had used VR before (72.2%, 13/18) and had positive impressions of VR (55.6%, 10/18), but none had used VR clinically. In response to tele-VRET, therapists requested interactive session activities (77.8%, 14/18) and customizable interventions components (55.6%, 10/18). Concerns about tele-VRET included risks with certain clients (77.8%, 14/18), costs (50%, 9/18), side effects and privacy (22.2%, 4/18), and inappropriateness for specific forms of exposure therapy (16.7%, 3/18). These results reveal how combining telehealth and VRET may expand therapeutic options for mental healthcare providers and can help inform collaborative development of immersive health technologies.
C1 [Ong, Triton; Ivanova, Julia; Soni, Hiral; Wilczewski, Hattie; Barrera, Janelle; Cummins, Mollie; Welch, Brandon M.; Bunnell, Brian E.] Doxy Me Inc, Doxy Me Res, Rochester 14619, NY USA.
   [Barrera, Janelle; Bunnell, Brian E.] Univ S Florida, Morsani Coll Med, Dept Psychiat & Behav Neurosci, Tampa, FL USA.
   [Cummins, Mollie] Univ Utah, Coll Nursing, Salt Lake City, UT USA.
   [Cummins, Mollie] Univ Utah, Dept Biomed Informat, Salt Lake City, UT USA.
   [Welch, Brandon M.] Med Univ South Carolina, Biomed Informat Ctr, Publ Hlth & Sci, Charleston, SC USA.
C3 State University System of Florida; University of South Florida; Utah
   System of Higher Education; University of Utah; Utah System of Higher
   Education; University of Utah; Medical University of South Carolina
RP Ong, T (corresponding author), Doxy Me Inc, Doxy Me Res, Rochester 14619, NY USA.
EM triton.ong@doxy.me
OI Soni, Hiral/0000-0002-2054-3737; Cummins, Mollie/0000-0001-7078-8479;
   Barrera, Janelle/0000-0002-1010-6365; Ong, Triton/0000-0002-2512-2026
FU National Institute of Mental Health [R43MH129065, K23MH118482]; National
   Institute of Mental Health of the National Institutes of Health
FX Research reported in this publication was supported by the National
   Institute of Mental Health of the National Institutes of Health under
   Award Numbers R43MH129065 and K23MH118482. The content is solely the
   responsibility of the authors and does not necessarily represent the
   official views of the National Institutes of Health.
CR Doorn KAV, 2024, PSYCHOTHER RES, V34, P574, DOI 10.1080/10503307.2023.2193299
   [Anonymous], 2022, Trends shaping the health economy: telehealth
   [Anonymous], 2022, Demographics of U.S. Psychology Workforce [Interactive data tool]. Retrieved Oct. 11, 2022
   [Anonymous], 2019, TAKING ACTION CLINIC, DOI DOI 10.17226/25521
   Aviram A, 2024, FAM PROCESS, V63, P163, DOI 10.1111/famp.12853
   Baghaei Nilufar, 2020, CHI EA '20: Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems, P1, DOI 10.1145/3334480.3382932
   Bakk AK, 2023, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.881766
   Ball C, 2021, TELEMAT INFORM, V65, DOI 10.1016/j.tele.2021.101728
   Barnett P, 2021, J MED INTERNET RES, V23, DOI 10.2196/26492
   Barreda-Angeles M, 2022, COMPUT HUM BEHAV, V127, DOI 10.1016/j.chb.2021.107047
   Batastini AB, 2021, CLIN PSYCHOL REV, V83, DOI 10.1016/j.cpr.2020.101944
   Jones RB, 2020, J CHILD PSYCHOL PSYC, V61, P928, DOI 10.1111/jcpp.13258
   Bisso E, 2020, INT J ENV RES PUB HE, V17, DOI 10.3390/ijerph17176111
   Boeldt D, 2019, FRONT PSYCHIATRY, V10, DOI 10.3389/fpsyt.2019.00773
   Brandstätter K, 2023, 2023 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS, VRW, P979, DOI 10.1109/VRW58643.2023.00332
   Braun V., 2019, APA HDB RES METHODS, P843, DOI [DOI 10.1037/13620-000, 10.1007/978-981-10-5251-4_10., 10.1037/13620-004, DOI 10.1037/13620-004, 10.1007/978-981-10-2779-6_103-1]
   Braun V, 2019, QUAL RES SPORT EXERC, V11, P589, DOI 10.1080/2159676X.2019.1628806
   Campbell JL, 2013, SOCIOL METHOD RES, V42, P294, DOI 10.1177/0049124113500475
   Carl E, 2019, J ANXIETY DISORD, V61, P27, DOI 10.1016/j.janxdis.2018.08.003
   Chard I, 2023, FRONT DIGIT HEALTH, V5, DOI 10.3389/fdgth.2023.1061323
   Chung OS, 2022, FRONT PSYCHIATRY, V13, DOI 10.3389/fpsyt.2022.792663
   Cieslik B, 2020, COMPLEMENT THER MED, V52, DOI 10.1016/j.ctim.2020.102480
   Connolly SL, 2022, IMPLEMENT SCI COMMUN, V3, DOI 10.1186/s43058-022-00318-x
   Cowan KE, 2019, MAYO CLIN PROC, V94, P2510, DOI 10.1016/j.mayocp.2019.04.018
   Deighan MT, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2023, DOI 10.1145/3544548.3581103
   Deng WR, 2019, J AFFECT DISORDERS, V257, P698, DOI 10.1016/j.jad.2019.07.086
   Dymond S, 2019, NEUROSCI BIOBEHAV R, V98, P61, DOI 10.1016/j.neubiorev.2019.01.007
   Emmelkamp PMG, 2021, ANNU REV CLIN PSYCHO, V17, P495, DOI 10.1146/annurev-clinpsy-081219-115923
   Fu KX, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2023), DOI 10.1145/3544548.3581464
   Gallace A, 2022, CURR OPIN BEHAV SCI, V43, P249, DOI 10.1016/j.cobeha.2021.11.006
   Georgieva I, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.854333
   Greenwood H, 2022, JMIR MENT HEALTH, V9, DOI 10.2196/31780
   Guillén V, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.01038
   Hadjipanayi C, 2023, FRONT VIRTUAL REAL, V4, DOI 10.3389/frvir.2023.1065863
   Halim I, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P208, DOI 10.1109/VRW55335.2022.00052
   Kelley B, 2021, 2021 4TH INTERNATIONAL CONFERENCE ON INFORMATION AND COMPUTER TECHNOLOGIES (ICICT 2021), P216, DOI 10.1109/ICICT52872.2021.00043
   Lindner P, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.00176
   Ma F, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P57, DOI 10.1109/VR51125.2022.00023
   Madigan S, 2021, CAN PSYCHOL, V62, P5, DOI 10.1037/cap0000259
   Marcu G, 2022, J MED INTERNET RES, V24, DOI 10.2196/34300
   Matsangidou M, 2022, HUM-COMPUT INTERACT, V37, P314, DOI 10.1080/07370024.2020.1788945
   McLean CP, 2024, MIL MED, V189, P721, DOI 10.1093/milmed/usac240
   Mehraeen E, 2023, HEALTH INFORM J, V29, DOI 10.1177/14604582231167431
   Meyerbröker K, 2021, CLIN PSYCHOL PSYCHOT, V28, P466, DOI 10.1002/cpp.2623
   Nowell LS, 2017, INT J QUAL METH, V16, DOI 10.1177/1609406917733847
   O'Connor C, 2020, INT J QUAL METH, V19, DOI 10.1177/1609406919899220
   Olden M, 2016, J Technol Behav Sci, V1, P22, DOI DOI 10.1007/S41347-016-0004-0
   Ong T, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.848066
   Panchal N., 2023, KFF
   Parsons TD, 2008, J BEHAV THER EXP PSY, V39, P250, DOI 10.1016/j.jbtep.2007.07.007
   Piitulainen R, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3501828
   Pittig A, 2019, BEHAV THER, V50, P353, DOI 10.1016/j.beth.2018.07.003
   Ramirez EJ, 2023, SOCIETIES, V13, DOI 10.3390/soc13020036
   Raskind IG, 2019, HEALTH EDUC BEHAV, V46, P32, DOI 10.1177/1090198118795019
   Rasmus A, 2022, Welcome to VRChat: an ethnographic study on embodiment and immersion in virtual reality
   Russell Bernard H., 2016, ANAL QUALITATIVE DAT
   Schöne B, 2023, FRONT PSYCHOL, V14, DOI 10.3389/fpsyg.2023.1093014
   Scott AM, 2022, J CLIN PSYCHIAT, V83, DOI 10.4088/JCP.21r14143
   Steinman S. A., 2015, ENCY MENTAL HLTH, P186, DOI [10.1016/B978-0-12-397045-9.00266-4, DOI 10.1016/B978-0-12-397045-9.00266-4]
   Sunkara C, 2023, J MED INTERNET RES, V25, DOI 10.2196/41807
   Sykownik P, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P546, DOI 10.1109/VR50410.2021.00079
   Twamley J, 2024, NURS CRIT CARE, V29, P313, DOI 10.1111/nicc.12868
   van Gelderen MJ, 2020, PSYCHOTHER PSYCHOSOM, V89, P215, DOI 10.1159/000505977
   Van Hoy A, 2022, FRONT PSYCHOL, V13, DOI 10.3389/fpsyg.2022.928191
   Vogt EL, 2022, INTERACT J MED RES, V11, DOI 10.2196/29880
   WHO, 2022, COVID-19 Pandemic Triggers 25% Increase in Prevalence of Anxiety and Depression Worldwide
   Wilczewski H, 2022, JMIR FORM RES, V6, DOI 10.2196/39634
   Wray TB, 2022, SUBST USE MISUSE, V57, P1470, DOI 10.1080/10826084.2022.2092148
   Zamanifard S, 2023, EXTENDED ABSTRACTS 2
NR 69
TC 1
Z9 1
U1 5
U2 5
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR 8
PY 2024
VL 28
IS 2
AR 73
DI 10.1007/s10055-024-00956-7
PG 16
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA LG7F1
UT WOS:001185691200002
OA Green Published, hybrid
DA 2024-08-05
ER

PT J
AU Farmer, H
   Skoulikari, E
   Bevan, C
   Gray, S
   Cater, K
   Fraser, DS
AF Farmer, H.
   Skoulikari, E.
   Bevan, C.
   Gray, S.
   Cater, K.
   Stanton Fraser, D.
TI Using narrative 360° video as a tool to promote breast self-examination
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; 360 degrees Video; Posture; Presence; Identity; Terror
   management theory; Breast self-examination; Public health communication
ID TERROR-MANAGEMENT THEORY; CANCER SURVIVOR STORIES; RUBBER HAND ILLUSION;
   VIRTUAL-REALITY; ENTERTAINMENT-EDUCATION; MORTALITY SALIENCE; DEATH
   NARRATIVES; CHARACTER DEATH; IMPACT; EXPERIENCE
AB This experiment examined the feasibility of 360 degrees video as a tool for public health messaging by investigating the effect that viewing the 360 degrees documentary The Waiting Room VR had on female viewers' sense of identification, attitudes to breast cancer screening and mortality salience. A key part of the documentary places participants in a viewpoint ambiguously aligned to that of the film's director and subject, Victoria Mapplebeck (VM), in a scene that recreates her radiotherapy treatment for breast cancer. Eighty female participants watched the documentary either sitting upright with the chair back set at a 90 degrees angle or reclining with the chair back set at a 140 degrees angle (consistent with VMs posture) under conditions of either high or low cognitive load. The effect of posture type was measured explicitly using questionnaires on presence, identification and breast self-examination (BSE) intention as well as implicitly using a lexical decision task to measure death-thought awareness (DTA). Reclined posture led to a higher sense of spatial presence but no increase in identification with VM. Significantly increased identification with VM led to greater intention to conduct BSE. There were no effects of posture, cognitive load or identification on DTA. The implications of these results for using 360 degrees video as a behaviour change tool, the effects of the COVID-19 pandemic on the terror management manipulation and the relevance of spatial viewpoint in 360 degrees video are discussed.
C1 [Farmer, H.] Univ Greenwich, Inst Lifecourse Dev, Sch Human Sci, London, England.
   [Farmer, H.; Skoulikari, E.; Stanton Fraser, D.] Univ Bath, Dept Psychol, Bath, Somerset, England.
   [Bevan, C.; Gray, S.; Cater, K.] Univ Bristol, Dept Comp Sci, Bristol, Glos, England.
C3 University of Greenwich; University of Bath; University of Bristol
RP Farmer, H (corresponding author), Univ Greenwich, Inst Lifecourse Dev, Sch Human Sci, London, England.; Farmer, H (corresponding author), Univ Bath, Dept Psychol, Bath, Somerset, England.
EM h.farmer@gre.ac.uk
RI Fraser, Dagmar Scott/ABB-3917-2021
OI Fraser, Dagmar Scott/0000-0002-9241-7772; Farmer,
   Harry/0000-0002-3684-0605; Stanton Fraser, Danae/0000-0002-3062-731X
FU Engineering and Physical Sciences Research Council [EP/P025595/1]; EPSRC
   Virtual Realities-Immersive Documentary Encounters project
FX This work was supported by the EPSRC Virtual Realities-Immersive
   Documentary Encounters project (EP/P025595/1). We would also like to
   thank: Dr Victoria Mapplebeck and the production team of "The Waiting
   Room VR" for proivding us with access to the documentary; Professor
   Mandy Rose and Dr David Green for their comments during the study
   design; and Dr Anca Salagean for her assitance in figure production.
CR Allen T.L., 2010, J NURSE PRACTITIONER, V6, P444, DOI [10.1016/j.nurpra.2009.11.005, DOI 10.1016/J.NURPRA.2009.11.005]
   Anastasi N, 2019, J HEALTH PSYCHOL, V24, P113, DOI 10.1177/1359105317697812
   Arndt J, 1997, J PERS SOC PSYCHOL, V73, P5, DOI 10.1037/0022-3514.73.1.5
   Arndt J, 2007, J PERS SOC PSYCHOL, V92, P12, DOI 10.1037/0022-3514.92.1.12
   ARON A, 1992, J PERS SOC PSYCHOL, V63, P596, DOI 10.1037/0022-3514.63.4.596
   Austoker J, 2003, BMJ-BRIT MED J, V326, P1, DOI 10.1136/bmj.326.7379.1
   Bailey JO, 2016, PRESENCE-TELEOP VIRT, V25, P222, DOI 10.1162/PRES_a_00263
   Barbot B, 2020, COMPUT HUM BEHAV, V111, DOI 10.1016/j.chb.2020.106431
   Bergström I, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0148060
   Bertrand P, 2018, FRONT ROBOT AI, V5, DOI 10.3389/frobt.2018.00026
   Bevan C, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300736
   Blanke O, 2009, TRENDS COGN SCI, V13, P7, DOI 10.1016/j.tics.2008.10.003
   Bozo Ö, 2009, J PSYCHOL, V143, P377, DOI 10.3200/JRLP.143.4.377-389
   Buchman S., 2019, J Interprofessional Educ Pract, V15, P127, DOI DOI 10.1016/J.XJEP.2019.03.010
   Burns E, 2006, PRESENCE-TELEOP VIRT, V15, P1, DOI 10.1162/pres.2006.15.1.1
   Cancer Research UK, 2021, Breast cancer statistics
   Carey M, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-39168-4
   Chen FQ, 2020, J MED INTERNET RES, V22, DOI 10.2196/18290
   Cohen J., 2001, MASS COMMUN SOC, V4, P245, DOI DOI 10.1207/S15327825MCS0403_01
   Costantini M, 2007, CONSCIOUS COGN, V16, P229, DOI 10.1016/j.concog.2007.01.001
   Courtney EP, 2020, BRIT J SOC PSYCHOL, V59, P607, DOI 10.1111/bjso.12392
   Crawford-Holland S., 2018, Synoptique, V7, P19
   Dal Cin S, 2004, RESISTANCE AND PERSUASION, P175
   de la Peña N, 2010, PRESENCE-TELEOP VIRT, V19, P291, DOI 10.1162/PRES_a_00005
   de Margerie T, 2018, How to integrate 360 video with unity
   Debarba HG, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0190109
   Elor A, 2020, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.585993
   Emmelkamp PMG, 2021, ANNU REV CLIN PSYCHO, V17, P495, DOI 10.1146/annurev-clinpsy-081219-115923
   Fairlamb S, 2022, J HEALTH PSYCHOL, V27, P2770, DOI 10.1177/13591053211067102
   Farmer H, 2017, SOC JUSTICE RES, V30, P323, DOI 10.1007/s11211-017-0294-1
   Farrer C, 2002, NEUROIMAGE, V15, P596, DOI 10.1006/nimg.2001.1009
   Fertleman C, 2018, FRONT PUBLIC HEALTH, V6, DOI 10.3389/fpubh.2018.00044
   Frank LB, 2015, HEALTH COMMUN, V30, P154, DOI 10.1080/10410236.2014.974126
   Fritsche I, 2008, J PERS SOC PSYCHOL, V95, P524, DOI 10.1037/a0012666
   Goldenberg JL, 2008, J EXP SOC PSYCHOL, V44, P260, DOI 10.1016/j.jesp.2007.05.002
   Green MC, 2000, J PERS SOC PSYCHOL, V79, P701, DOI 10.1037/0022-3514.79.5.701
   HART S G, 1988, P139
   Hasler BS, 2021, NEW MEDIA SOC, V23, P2255, DOI 10.1177/1461444821993133
   Hayes J, 2010, PSYCHOL BULL, V136, P699, DOI 10.1037/a0020524
   Howard MC, 2017, COMPUT HUM BEHAV, V70, P317, DOI 10.1016/j.chb.2017.01.013
   Igartua JJ, 2021, ADICCIONES, V33, P245, DOI 10.20882/adicciones.1339
   Jacobsen GD, 2011, J HEALTH ECON, V30, P55, DOI 10.1016/j.jhealeco.2010.11.005
   Jensen JD, 2017, J HEALTH COMMUN, V22, P84, DOI 10.1080/10810730.2016.1252816
   Jensen JD, 2014, SOC SCI MED, V104, P31, DOI 10.1016/j.socscimed.2013.12.003
   Jin C, 2021, INT J SURG, V87, DOI 10.1016/j.ijsu.2020.11.022
   Jones Sheana, 2008, Int J Epidemiol, V37, P1219, DOI 10.1093/ije/dyn218
   Kilteni K, 2012, PRESENCE-TELEOP VIRT, V21, P373, DOI 10.1162/PRES_a_00124
   Knowles ES, 2004, RESISTANCE AND PERSUASION, P3
   Krakow M, 2017, HEALTH PSYCHOL, V36, P1173, DOI 10.1037/hea0000498
   Lenggenhager B, 2007, SCIENCE, V317, P1096, DOI 10.1126/science.1143439
   Lillie HM, 2022, PSYCHOL HEALTH, V37, P419, DOI 10.1080/08870446.2021.1873337
   Logishetty K, 2019, BONE JOINT J, V101B, P1585, DOI 10.1302/0301-620X.101B12.BJJ-2019-0643.R1
   Maggio MG, 2019, J NEUROSCI NURS, V51, P101, DOI 10.1097/JNN.0000000000000423
   Mapplebeck V, 2022, The waiting room VR
   Mapplebeck V., 2019, The waiting room VR
   Matamala-Gomez M, 2019, FRONT HUM NEUROSCI, V13, DOI 10.3389/fnhum.2019.00279
   McQueen A, 2019, J HEALTH COMMUN, V24, P141, DOI 10.1080/10810730.2019.1587109
   McQueen A, 2011, HEALTH PSYCHOL, V30, P674, DOI 10.1037/a0025395
   McQueen A, 2010, PATIENT EDUC COUNS, V81, pS15, DOI 10.1016/j.pec.2010.08.015
   Miller AB, 2011, PREV MED, V53, P118, DOI 10.1016/j.ypmed.2011.05.001
   Moyer-Gusé E, 2008, COMMUN THEOR, V18, P407, DOI 10.1111/j.1468-2885.2008.00328.x
   Neyret S, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-62932-w
   Nikolaou A, 2022, ANN INT COMMUN ASSOC, V46, P30, DOI 10.1080/23808985.2022.2064324
   Occa A, 2016, J HEALTH COMMUN, V21, P1, DOI 10.1080/10810730.2015.1018611
   Petkova VI, 2011, FRONT PSYCHOL, V2, DOI 10.3389/fpsyg.2011.00035
   Pyszczynski T, 2021, J HUMANIST PSYCHOL, V61, P173, DOI 10.1177/0022167820959488
   Pyszczynski T, 2015, ADV EXP SOC PSYCHOL, V52, P1, DOI 10.1016/bs.aesp.2015.03.001
   Ratcliff CL, 2020, HUM COMMUN RES, V46, P412, DOI 10.1093/hcr/hqz017
   ROSENBLATT A, 1989, J PERS SOC PSYCHOL, V57, P681, DOI 10.1037/0022-3514.57.4.681
   Rosseel Y, 2012, J STAT SOFTW, V48, P1, DOI 10.18637/jss.v048.i02
   Schimel J, 2007, J PERS SOC PSYCHOL, V92, P789, DOI 10.1037/0022-3514.92.5.789
   Schubert T, 2001, PRESENCE-VIRTUAL AUG, V10, P266, DOI 10.1162/105474601300343603
   Shen FY, 2015, J ADVERTISING, V44, P105, DOI 10.1080/00913367.2015.1018467
   Shen LJ, 2010, WESTERN J COMM, V74, P504, DOI 10.1080/10570314.2010.512278
   Shin D, 2018, COMPUT HUM BEHAV, V78, P64, DOI 10.1016/j.chb.2017.09.012
   Shrout PE, 2002, PSYCHOL METHODS, V7, P422, DOI 10.1037//1082-989X.7.4.422
   Slater MD, 2002, COMMUN THEOR, V12, P173, DOI 10.1111/j.1468-2885.2002.tb00265.x
   Slater M, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010564
   Spreng RN, 2009, J PERS ASSESS, V91, P62, DOI 10.1080/00223890802484381
   Stevens E, 2009, INT J PALLIAT NURS, V15, P368, DOI 10.12968/ijpn.2009.15.8.43793
   Tsakiris M, 2005, J EXP PSYCHOL HUMAN, V31, P80, DOI 10.1037/0096-1523.31.1.80
   Tsakiris M, 2010, NEUROPSYCHOLOGIA, V48, P703, DOI 10.1016/j.neuropsychologia.2009.09.034
   Unity Technologies, 2019, Unity software development package
   van Laer T, 2014, J CONSUM RES, V40, P797, DOI 10.1086/673383
   van Loon A, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0202442
   Ventura S, 2022, FRONT HUM NEUROSCI, V16, DOI 10.3389/fnhum.2022.845508
   Ventura S, 2020, CYBERPSYCH BEH SOC N, V23, P667, DOI 10.1089/cyber.2019.0681
   Voinescu A, 2021, J CLIN MED, V10, DOI 10.3390/jcm10071478
   Vraga EK, 2018, J HEALTH COMMUN, V23, P181, DOI 10.1080/10810730.2017.1421730
NR 89
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 34
DI 10.1007/s10055-023-00918-5
PG 16
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA FZ5R8
UT WOS:001149695500001
OA hybrid
DA 2024-08-05
ER

PT J
AU Wähnert, S
   Schäfer, U
AF Waehnert, Svetlana
   Schaefer, Ulrike
TI Sensorimotor adaptation in virtual reality: Do instructions and body
   representation influence aftereffects?
SO VIRTUAL REALITY
LA English
DT Article
DE Object identity; Sensorimotor adaptation; Aftereffects
ID PRISM ADAPTATION; ILLUSORY OWNERSHIP; DISPLACED VISION; ERROR; EXPOSURE;
   HAND
AB Perturbations in virtual reality (VR) lead to sensorimotor adaptation during exposure, but also to aftereffects once the perturbation is no longer present. An experiment was conducted to investigate the impact of different task instructions and body representation on the magnitude and the persistence of these aftereffects. Participants completed the paradigm of sensorimotor adaptation in VR. They were assigned to one of three groups: control group, misinformation group or arrow group. The misinformation group and the arrow group were each compared to the control group to examine the effects of instruction and body representation. The misinformation group was given the incorrect instruction that in addition to the perturbation, a random error component was also built into the movement. The arrow group was presented a virtual arrow instead of a virtual hand. It was hypothesised that both would lead to a lower magnitude and persistence of the aftereffect because the object identity between hand and virtual representation would be reduced, and errors would be more strongly attributed to external causes. Misinformation led to lower persistence, while the arrow group showed no significant differences compared to the control group. The results suggest that information about the accuracy of the VR system can influence the aftereffects, which should be considered when developing VR instructions. No effects of body representation were found. One possible explanation is that the manipulated difference between abstract and realistic body representation was too small in terms of object identity.
C1 [Waehnert, Svetlana] Tech Univ Dresden, Chair Ergon, Durerstr 26, D-01062 Dresden, Germany.
   [Schaefer, Ulrike] Free Univ Berlin, Human Ctr Comp Res Grp, Berlin, Germany.
   [Waehnert, Svetlana; Schaefer, Ulrike] Tech Univ Berlin, Berlin, Germany.
C3 Technische Universitat Dresden; Free University of Berlin; Technical
   University of Berlin
RP Wähnert, S (corresponding author), Tech Univ Dresden, Chair Ergon, Durerstr 26, D-01062 Dresden, Germany.; Wähnert, S (corresponding author), Tech Univ Berlin, Berlin, Germany.
EM svetlana.waehnert@tu-dresden.de
OI Wahnert, Svetlana/0000-0002-7813-1786
FU Technische Universitt Dresden (1019)
FX We are grateful to our colleague Richard Gross, PhD, for programming the
   experiments.
CR Anglin JM, 2017, SCI REP-UK, V7, DOI 10.1038/srep45469
   Aziz JR, 2020, FRONT HUM NEUROSCI, V14, DOI 10.3389/fnhum.2020.00138
   Banakou D, 2013, P NATL ACAD SCI USA, V110, P12846, DOI 10.1073/pnas.1306779110
   BARON RM, 1986, J PERS SOC PSYCHOL, V51, P1173, DOI 10.1037/0022-3514.51.6.1173
   Barral J, 2004, LATERALITY, V9, P299, DOI 10.1080/13576500342000158
   Bedford FL, 2001, CAH PSYCHOL COGN, V20, P113
   BEDFORD FL, 1989, J EXP PSYCHOL HUMAN, V15, P232, DOI 10.1037/0096-1523.15.2.232
   Biocca FA, 1998, PRESENCE-VIRTUAL AUG, V7, P262, DOI 10.1162/105474698565703
   Burin D, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0209899
   Cho S, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-04771-5
   Clower DM, 2000, J NEUROPHYSIOL, V84, P2703, DOI 10.1152/jn.2000.84.5.2703
   Cohn BA, 2020, 2020 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR 2020), P74, DOI 10.1109/AIVR50618.2020.00024
   Della-Maggiore V, 2015, NEUROSCIENTIST, V21, P109, DOI 10.1177/1073858414545228
   DEWAR R, 1970, PERCEPT PSYCHOPHYS, V8, P313, DOI 10.3758/BF03212599
   Doerner R., 2022, Virtual and Augmented Reality (VR/AR) Foundations and Methods of Extended Realities (XR), P1
   Fernandes HL, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0043016
   Fernández-Ruiz J, 1999, LEARN MEMORY, V6, P47
   Fleury L, 2019, CORTEX, V119, P480, DOI 10.1016/j.cortex.2019.07.012
   Fox J., 1997, APPL REGRESSION ANAL, DOI DOI 10.5860/CHOICE.34-6323
   Gammeri R, 2020, NEUROPSYCHOL REHABIL, V30, P753, DOI 10.1080/09602011.2018.1502672
   Gonzalez EJ, 2022, CHI C HUMAN FACTORS, P1
   Gonzalez-Franco M, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P18, DOI [10.1109/VR46266.2020.00-85, 10.1109/VR46266.2020.1580500165557]
   Haans A, 2008, BODY IMAGE, V5, P389, DOI 10.1016/j.bodyim.2008.04.003
   Havermann K, 2010, J NEUROPHYSIOL, V103, P3302, DOI 10.1152/jn.00970.2009
   Juliano JM, 2022, J NEUROENG REHABIL, V19, DOI 10.1186/s12984-022-01084-6
   Kilteni K, 2012, PRESENCE-TELEOP VIRT, V21, P373, DOI 10.1162/PRES_a_00124
   Kilteni K, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040867
   Kline R.B., 2013, SIGNIFICANCE TESTING
   KORNHEISER AS, 1976, PSYCHOL BULL, V83, P783, DOI 10.1037/0033-2909.83.5.783
   Michel C, 2007, J COGNITIVE NEUROSCI, V19, P341, DOI 10.1162/jocn.2007.19.2.341
   Nilsson NC, 2018, IEEE COMPUT GRAPH, V38, P44, DOI 10.1109/MCG.2018.111125628
   Norris SA, 2001, BRAIN RES, V905, P207, DOI 10.1016/S0006-8993(01)02552-5
   O'Shea J, 2014, NEUROPSYCHOLOGIA, V55, P15, DOI 10.1016/j.neuropsychologia.2013.09.021
   Pournelle G. H., 1953, Journal of Mammalogy, V34, P133, DOI 10.1890/0012-9658(2002)083[1421:SDEOLC]2.0.CO;2
   Prablanc C, 2020, NEUROSCI RES, V153, P8, DOI 10.1016/j.neures.2019.03.003
   RADEAU M, 1977, PERCEPT PSYCHOPHYS, V22, P137, DOI 10.3758/BF03198746
   Redding GM, 2006, J EXP PSYCHOL HUMAN, V32, P1006, DOI 10.1037/0096-1523.32.4.1006
   Renner RS, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543590
   ROLLAND JP, 1995, VIRTUAL REALITY ANNUAL INTERNATIONAL SYMPOSIUM '95, PROCEEDINGS, P56
   Slater M, 2009, FRONT NEUROSCI-SWITZ, V3, P214, DOI 10.3389/neuro.01.029.2009
   Thomas M., 2012, APE, V02, P172, DOI [10.4236/ape.2012.24030, DOI 10.4236/APE.2012.24030]
   Tsakiris M, 2010, NEUROPSYCHOLOGIA, V48, P703, DOI 10.1016/j.neuropsychologia.2009.09.034
   Valve Corporation, 2023, Valve Index Headset
   Veilleux LN, 2015, Q J EXP PSYCHOL, V68, P1168, DOI 10.1080/17470218.2014.977305
   Vetter P, 1999, J NEUROPHYSIOL, V81, P935, DOI 10.1152/jn.1999.81.2.935
   Wähnert S, 2022, VIRTUAL REAL-LONDON, V26, P1217, DOI 10.1007/s10055-022-00628-4
   Wei KL, 2009, J NEUROPHYSIOL, V101, P655, DOI 10.1152/jn.90545.2008
   WELCH RB, 1980, PSYCHOL BULL, V88, P638, DOI 10.1037/0033-2909.88.3.638
   WELCH RB, 1972, PERCEPT PSYCHOPHYS, V12, P453, DOI 10.3758/BF03210933
   WELCH RB, 1971, PERCEPT PSYCHOPHYS, V9, P102, DOI 10.3758/BF03213039
   Wilf M, 2023, CEREB CORTEX, V33, P5163, DOI 10.1093/cercor/bhac407
   Wilf M, 2021, NEUROPSYCHOLOGIA, V150, DOI 10.1016/j.neuropsychologia.2020.107692
   Wilke C, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0054925
NR 53
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 47
DI 10.1007/s10055-024-00957-6
PG 13
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA IS2L6
UT WOS:001168254700002
OA Green Published, hybrid
DA 2024-08-05
ER

PT J
AU Albeedan, M
   Kolivanda, H
   Hammady, R
AF Albeedan, Meshal
   Kolivanda, Hoshang
   Hammady, Ramy
TI Designing and evaluation of a mixed reality system for crime scene
   investigation training: a hybrid approach
SO VIRTUAL REALITY
LA English
DT Article
DE Mixed reality; User interaction; User experience; Crime scene;
   Investigation training; 3D scanning; Photogrammetry
ID VIRTUAL-REALITY; AUGMENTED REALITY; USER EXPERIENCE; RECONSTRUCTION;
   PHOTOGRAMMETRY; CONSTRUCTION; TECHNOLOGIES; VALIDITY; MODELS; VR
AB Police investigation in real-life crime scenes is an essential aspect of forensic science education. However, the practicality of bringing young investigators to actual crime scenes is often hindered by the costs and challenges involved. In order to overcome these obstacles, new technologies such as mixed reality (MR) are being explored as potential solutions. MR technology offers an interactive and cost-effective way to simulate real-life crime scenes, providing a valuable training experience for young investigators. This paper presents a novel design of a MR system using Microsoft HoloLens 2.0, which is tailored to work in a spatial 3D scanned and reconstructed crime scene using FARO point cloud 3D scanner X130 blended with photogrammetry techniques. The system was developed through the lens of Experiential Learning Theory and designed using a participatory approach, providing a cost-effective solution to help trained Kuwaiti police officers enhance their investigative skills. In order to evaluate the system's user experience and user interaction, the Questionnaire of User Interaction Satisfaction and User Experience Questionnaire were utilised. Forty-four young police officers evaluated the system. Police students showed positive levels of satisfaction with user interaction and overall user experience with minimal negative feedback. Female students showed higher satisfaction with the overall impression compared to male students. Based on the positive feedback regarding the system expansion, the system will be taken into the commercialisation stage in the future to be provided as an essential tool for crime scene education and investigation practices.
C1 [Albeedan, Meshal] Saad Al Abdullah Acad Secur Sci Hosp, Kuwait, Kuwait.
   [Kolivanda, Hoshang] Liverpool John Moores Univ LJMU, Fac Engn & Technol, Sch Comp Sci & Math, Liverpool L3 3AF, England.
   [Kolivanda, Hoshang] Staffordshire Univ, Sch Comp & Digital Technol, Stoke On Trent, England.
   [Hammady, Ramy] Univ Essex, Dept Comp Sci & Elect Engn, Colchester, England.
   [Hammady, Ramy] Helwan Univ, Fac Appl Arts, Cairo, Egypt.
C3 Liverpool John Moores University; Staffordshire University; University
   of Essex; Egyptian Knowledge Bank (EKB); Modern Sciences & Arts
   University (MSA); Helwan University
RP Hammady, R (corresponding author), Univ Essex, Dept Comp Sci & Elect Engn, Colchester, England.; Hammady, R (corresponding author), Helwan Univ, Fac Appl Arts, Cairo, Egypt.
EM Albeedan12@hotmail.com; H.Kolivand@ljmu.ac.uk; ramyhammady@outlook.com
CR Adamczyk M., 2017, SPIE, V10441, P88
   Ahn Junyoung, 2017, [Journal of the ergonomics society of Korea, 대한인간공학회지], V36, P267, DOI 10.5143/JESK.2017.36.4.267
   Alshawabkeh Y, 2021, ISPRS INT J GEO-INF, V10, DOI 10.3390/ijgi10050316
   Altarteer Samar, 2013, Virtual, Augmented and Mixed Reality. Systems and Applications. 5th International Conference, VAMR 2013 Held as Part of HCI International 2013. Proceedings, Part II: LNCS 8022, P221, DOI 10.1007/978-3-642-39420-1_24
   Arifin Yulyani, 2018, Procedia Computer Science, V135, P648, DOI 10.1016/j.procs.2018.08.221
   Arrighi G., 2021, Digit Appl Archaeol Cult Herit, V21
   Bayarri S, 1996, COMMUN ACM, V39, P72, DOI 10.1145/229459.229468
   Beebe NL, 2005, DIGIT INVEST, V2, P147, DOI 10.1016/j.diin.2005.04.002
   Beldad A, 2010, COMPUT HUM BEHAV, V26, P857, DOI 10.1016/j.chb.2010.03.013
   Bermejo C, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3465396
   Bolliger MJ, 2012, FORENSIC SCI MED PAT, V8, P208, DOI 10.1007/s12024-011-9288-8
   Buck U, 2013, FORENSIC SCI INT, V225, P75, DOI 10.1016/j.forsciint.2012.05.015
   Bulgakov V, 2019, COMM COM INF SC, V1084, P217, DOI 10.1007/978-3-030-29750-3_17
   Cadet LB, 2020, INT J HUM-COMPUT ST, V144, DOI 10.1016/j.ijhcs.2020.102506
   Cai Zhong-fa, 2002, Journal of System Simulation, V14, P771
   Carmel D, 2003, J EXP PSYCHOL-APPL, V9, P261, DOI 10.1037/1076-898X.9.4.261
   Chan JCP, 2011, IEEE T LEARN TECHNOL, V4, P187, DOI 10.1109/TLT.2010.27
   Chin J, 1988, Human-computer interaction lab
   Chin J.P., 1988, P SIGCHI C HUM FACT, P213, DOI [DOI 10.1145/57167.57203, 10.1145/57167.57203]
   Cho J., 2021, Using virtual reality as a form of simulation in the context of legal education, DOI [10.1007/978-3-030-68086-211, DOI 10.1007/978-3-030-68086-211]
   Colard T, 2013, LEGAL MED-TOKYO, V15, P318, DOI 10.1016/j.legalmed.2013.07.002
   Cumbo B, 2022, INT J RES METHOD EDU, V45, P60, DOI 10.1080/1743727X.2021.1902981
   Datcu D, 2016, J UNIVERS COMPUT SCI, V22, P247
   Dostal C, 2018, J ARCHAEOL SCI-REP, V18, P430, DOI 10.1016/j.jasrep.2018.01.024
   Duea Stephanie R, 2022, J Particip Res Methods, V3, DOI 10.35844/001c.32605
   Ebert LC, 2014, FORENSIC SCI MED PAT, V10, P623, DOI 10.1007/s12024-014-9605-0
   Elhaw AE., 2024, Int J Intell Syst Appl Eng, V12, P530
   Engelbrecht H, 2018, ADJUNCT PROCEEDINGS OF THE 2018 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P386, DOI 10.1109/ISMAR-Adjunct.2018.00111
   Fang TY, 2014, COMPUT METH PROG BIO, V113, P674, DOI 10.1016/j.cmpb.2013.11.005
   Faro, 2021, Checkerboard registration targets download for the laser scanner or hand-held scanner Online
   Filigenzi M T, 2000, Appl Occup Environ Hyg, V15, P465
   Fisher BA., 2012, Ethics in the crime laboratory and in crime scene investigations, DOI [10.1016/B978-0-12-385019-5.00010-5, DOI 10.1016/B978-0-12-385019-5.00010-5]
   Flight C, 2022, J Assoc Crime Scene Reconstr, V26
   Galanakis G., 2021, Forensic Sci, V1, P56, DOI [DOI 10.3390/FORENSICSCI1020008, 10.3390/forensicsci1020008]
   Gatto C, 2022, LECT NOTES COMPUT SC, V13445, P335, DOI 10.1007/978-3-031-15546-8_28
   Gee A.P., 2010, Proceedings of the 2nd ACM Workshop on Multimedia in Forensics, Security and Intelligence pp, P105
   Ghazwani Y, 2020, ICVARS 2020: PROCEEDINGS OF 2020 4TH INTERNATIONAL CONFERENCE ON VIRTUAL AND AUGMENTED REALITY SIMULATIONS, P39, DOI 10.1145/3385378.3385384
   Gregory J, 2003, INT J ENG EDUC, V19, P62
   Haque SEI, 2020, 2020 8 INT S DIG FOR, P1
   Hargittai E, 2007, J COMPUT-MEDIAT COMM, V13, P276, DOI 10.1111/j.1083-6101.2007.00396.x
   Hassenzahl M, 2006, BEHAV INFORM TECHNOL, V25, P91, DOI 10.1080/01449290500330331
   Hinderks A, 2019, COMPUT STAND INTER, V65, P38, DOI 10.1016/j.csi.2019.01.007
   Hornbæk K, 2006, INT J HUM-COMPUT ST, V64, P79, DOI 10.1016/j.ijhcs.2005.06.002
   Huang CL, 2020, J EDUC COMPUT RES, V58, P596, DOI 10.1177/0735633119867422
   Huber T., 2015, J Surg Simul, V2, P35
   ISO, 2019, Ergonomics of human-system interaction -. Part 210: Human-centred design for interactive systems
   Jacobs C., 2004, Interactive panoramas: Techniques for digital panoramic photography
   Jani G, 2022, J VIS COMMUN MED, V45, P18, DOI 10.1080/17453054.2021.1971516
   Johnson RobertR., 1998, User-centered Technology: A Rhetorical Theory for Computers and Other Mundane Artifacts
   Kader SN, 2020, J CHEM EDUC, V97, P2651, DOI 10.1021/acs.jchemed.0c00817
   Karabiyik U, 2019, LECT NOTES COMPUT SC, V11845, P469, DOI 10.1007/978-3-030-33723-0_38
   Kilgus T, 2015, INT J COMPUT ASS RAD, V10, P573, DOI 10.1007/s11548-014-1106-9
   Kim J, 2018, INT C CONSTR FUT, P1
   Kim J, 2020, COMPUT HUM BEHAV, V113, DOI 10.1016/j.chb.2020.106484
   Kline D, 2022, Spatial awareness getting started-MRTK2 Online
   Kolb A.Y., 2005, BOSTON MA, P72
   Kolb DA., 2015, Experiential learning: Experience as the source of learning and development, DOI [10.1002/job.4030080408, DOI 10.1016/B978-0-7506-7223-8.50017-4]
   Kopec W, 2019, IFAC PAPERSONLINE, V52, P277, DOI 10.1016/j.ifacol.2019.12.110
   Kottner S, 2017, FORENSIC SCI MED PAT, V13, P34, DOI 10.1007/s12024-016-9837-2
   Kowalski P, 2018, ADV INTELL SYST, V659, P408, DOI 10.1007/978-3-319-67792-7_40
   Ktena SI, 2015, I IEEE EMBS C NEUR E, P236, DOI 10.1109/NER.2015.7146603
   KUHN S, 1993, COMMUN ACM, V36, P24
   Kyan M, 2015, ACM T INTEL SYST TEC, V6, DOI 10.1145/2735951
   Laugwitz B, 2008, LECT NOTES COMPUT SC, V5298, P63, DOI 10.1007/978-3-540-89350-9_6
   Lee HG, 2013, NEW MEDIA SOC, V15, P930, DOI 10.1177/1461444812464033
   Lee JH, 2019, SUSTAINABILITY-BASEL, V11, DOI 10.3390/su11205723
   Lee Yi-Chia Nina, 2013, Virtual, Augmented and Mixed Reality. Systems and Applications. 5th International Conference, VAMR 2013 Held as Part of HCI International 2013. Proceedings, Part II: LNCS 8022, P400, DOI 10.1007/978-3-642-39420-1_42
   Leung WS., 2020, An augmented reality approach to delivering a connected digital forensics training experience, DOI [10.1007/978-981-15-1465-436, DOI 10.1007/978-981-15-1465-436]
   Levstein I, 2019, Emerging technologies in virtual learning environments
   Liang HL, 2018, J CULT HERIT, V33, P222, DOI 10.1016/j.culher.2018.03.004
   Lindgren R, 2016, COMPUT EDUC, V95, P174, DOI 10.1016/j.compedu.2016.01.001
   Lindsay Peter H, 2013, Human Information Processing: An Introduction to Psychology
   Lukosch H, 2012, WINT SIMUL C PROC
   Lynnerup N, 2009, FORENSIC SCI MED PAT, V5, P167, DOI 10.1007/s12024-009-9094-8
   Ma MH, 2010, J FORENSIC SCI, V55, P1227, DOI 10.1111/j.1556-4029.2010.01453.x
   Mach V, 2019, MATEC WEB C
   Martín-Gutiérrez J, 2015, COMPUT HUM BEHAV, V51, P752, DOI 10.1016/j.chb.2014.11.093
   Maulana Fairuz Iqbal, 2022, 2022 8th International Conference on Education and Technology (ICET), P270, DOI 10.1109/ICET56879.2022.9990613
   Mayne R, 2020, SCI JUSTICE, V60, P466, DOI 10.1016/j.scijus.2020.07.006
   Mennell J, 2006, FORENSIC SCI INT, V157, pS13, DOI 10.1016/j.forsciint.2005.12.023
   Mentzelopoulos M, 2016, COMM COM INF SC, V621, P73, DOI 10.1007/978-3-319-41769-1_6
   Meshal A, 2023, IEEE Transactions on Learning Technologies
   Morélot S, 2021, COMPUT EDUC, V166, DOI 10.1016/j.compedu.2021.104145
   Noghabaei M, 2019, COMPUTING IN CIVIL ENGINEERING 2019: VISUALIZATION, INFORMATION MODELING, AND SIMULATION, P95
   Norman KentL., 1989, Questionnaire for user interaction satisfaction (QUIS 5.0)
   Nunnally J., 1994, Psychometric Theory
   Oyibo K, 2017, J Interact Syst, V8, P174
   Pal SK, 2010, CH CRC MATH COMP IMA, P1
   Papachristos NM, 2017, IEEE INT CONF ADV LE, P477, DOI 10.1109/ICALT.2017.145
   Paramitha AII, 2018, 2018 3 INT C INF COM, P1
   Plantin J, 2021, Recovery and prediction of hand motor function after stroke: a longitudinal study using novel methods to quantify hand function and connectivity in brain networks
   Poelman R, 2012, P ACM 2012 C COMP SU
   Prattico FG, 2021, 2021 IEEE 11 INT C C, P1
   Raneri D, 2018, AUST J FORENSIC SCI, V50, P697, DOI 10.1080/00450618.2018.1424245
   Rebenitsch L, 2016, VIRTUAL REAL-LONDON, V20, P101, DOI 10.1007/s10055-016-0285-9
   Rokhsaritalemi S, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10020636
   Roos W., 1951, Photogrammetria, V8, P97, DOI [10.1016/S0031--866(3()51)80020--6, DOI 10.1016/S0031--866(3()51)80020--6]
   Rühmann LM, 2018, PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON SUPPORTING GROUP WORK, GROUP 2018, P107, DOI 10.1145/3148330.3154510
   Sacks R, 2013, CONSTR MANAG ECON, V31, P1005, DOI 10.1080/01446193.2013.828844
   Saleh AM., 2021, Indones J Electr Eng Comput Sci, V23, P1120
   Sandvik K, 2010, TOUR CULT CHANG, P138
   Sasak J, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11182154
   Scholz J, 2016, BUS HORIZONS, V59, P149, DOI 10.1016/j.bushor.2015.10.003
   Schrepp M., 2015, USER EXPERIENCE QUES
   Schrepp M., 2014, Design, User Experience, and Usability. Theories, Methods, and Tools for Designing the User Experience, V8517, P383, DOI [DOI 10.1007/978-3-319-07668-3_37, 10.1007/978-3-319-07668-3_37, DOI 10.1007/978-3-319-07668-337]
   Servotte JC, 2020, CLIN SIMUL NURS, V38, P35, DOI 10.1016/j.ecns.2019.09.006
   Shamata A, 2018, J FORENSIC LEG MED, V55, P58, DOI 10.1016/j.jflm.2018.02.017
   Somrak A, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21041185
   Somrak A, 2019, FUTURE GENER COMP SY, V94, P302, DOI 10.1016/j.future.2018.11.041
   Spain Randall, 2018, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V62, P2051, DOI 10.1177/1541931218621463
   Speicher M, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300767
   Spinuzzi C, 2005, TECH COMMUN-STC, V52, P163
   Spittle B, 2023, IEEE T VIS COMPUT GR, V29, P3900, DOI 10.1109/TVCG.2022.3174805
   Steinberg AD, 2007, J DENT EDUC, V71, P1574
   Streefkerk Jan Willem, 2013, Virtual, Augmented and Mixed Reality. Systems and Applications. 5th International Conference, VAMR 2013 Held as Part of HCI International 2013. Proceedings, Part II: LNCS 8022, P330, DOI 10.1007/978-3-642-39420-1_35
   Studio A, 2018, Experience compelling new possibilities for storytelling and gameplay when life-sized characters are aware of your presence, share your real space, and interact with you in mixed reality
   Su KW, 2020, VIRTUAL REAL-LONDON, V24, P241, DOI 10.1007/s10055-019-00394-w
   Suh A, 2018, COMPUT HUM BEHAV, V86, P77, DOI 10.1016/j.chb.2018.04.019
   Taejin Ha, 2010, Proceedings of the 2010 International Symposium on Ubiquitous Virtual Reality (ISUVR 2010), P40, DOI 10.1109/ISUVR.2010.20
   Teo T, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300431
   Tolstolutsky V., 2021, The experience of using augmented reality in the reconstruction of the crime scene committed in transport, P1095
   Tom Dieck MC., 2019, Experiencing virtual reality in heritage attractions: Perceptions of elderly users, P89
   Tredinnick R., 2019, Forensic Sci Int Reports, V1, DOI [DOI 10.1016/J.FSIR.2019.100025, 10.1016/j.fsir.2019.100025]
   Trushchenkov I., 2021, PROC C CREATIVITY IN, V1448 CCIS, P325, DOI [10.1007/978-3-030-87034-8_24, DOI 10.1007/978-3-030-87034-8_24]
   Van der Velden M., 2015, PARTICIPATORY DESIGN, P41, DOI 10.1007/978-94-007-6970-0_33
   Venkatesh V, 2003, MIS QUART, V27, P425, DOI 10.2307/30036540
   Vergari M, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P695, DOI 10.1109/VR50410.2021.00096
   Villa C, 2017, J FORENSIC RADIOL IM, V10, P47, DOI 10.1016/j.jofri.2017.08.003
   Wang JM, 2019, FORENSIC SCI INT, V303, DOI 10.1016/j.forsciint.2019.109943
   Wieczorek Tadeusz, 2019, Methods and Techniques of Signal Processing in Physical Measurements. Lecture Notes in Electrical Engineering (LNEE 548), P406, DOI 10.1007/978-3-030-11187-8_33
   Xue H, 2019, COMPUTERS, V8, DOI 10.3390/computers8010009
   Yu Y, 2017, A mixed reality approach to 3d interactive prototyping for participatory design of ambient intelligence
   Zhou ZY, 2014, DECIS SUPPORT SYST, V65, P69, DOI 10.1016/j.dss.2014.05.004
   Zhu D, 2022, HEALTHCARE 2022
NR 134
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD JUN 25
PY 2024
VL 28
IS 3
AR 127
DI 10.1007/s10055-024-01018-8
PG 30
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA WI1Y0
UT WOS:001254160300001
OA hybrid
DA 2024-08-05
ER

PT J
AU Naor, SK
   Ketko, I
   Yanovich, R
   Gottlieb, A
   Bahat, Y
   Ben-Gal, O
   Heled, Y
   Plotnik, M
AF Naor, Shani Kimel
   Ketko, Itay
   Yanovich, Ran
   Gottlieb, Amihai
   Bahat, Yotam
   Ben-Gal, Oran
   Heled, Yuval
   Plotnik, Meir
TI Bringing the field into the lab: a novel virtual reality outdoor march
   simulator for evaluating cognitive and physical performance
SO VIRTUAL REALITY
LA English
DT Article
DE Physical effort; Cognitive load; Virtual reality; Military; Load
   carriage
ID MODERATE AEROBIC EXERCISE; EXECUTIVE FUNCTION; WORKING-MEMORY;
   MULTITASKING; INTENSITY; TRAIL; PREDICTORS; VALIDITY
AB Soldiers, athletes, and rescue personnel must often maintain cognitive focus while performing intense, prolonged, and physically demanding activities. The simultaneous activation of cognitive and physical functions can disrupt their performance reciprocally. In the current study, we developed and demonstrated the feasibility of a virtual reality (VR)-based experimental protocol that enables rigorous exploration of the effects of prolonged physical and cognitive efforts. A battery of established neurocognitive tests was used to compare novel cognitive tasks to simulated loaded marches. We simulated a 10-km loaded march in our virtual reality environment, with or without integrated cognitive tasks (VR-COG). During three experimental visits, participants were evaluated pre- and post-activity, including the Color Trail Test (CTT), the Synthetic Work Environment (SYNWIN) battery for assessing multitasking, and physical tests (i.e., time to exhaustion). Results show that Strong or moderate correlations (r >= 0.58, p <= 0.05) were found between VR-COG scores and scores on the cognitive tests. Both the SYNWIN and CTT showed no condition effects but significant time effects, indicating better performance in the post-activity assessment than in the pre-activity assessment. This novel protocol can contribute to our understanding of physical-cognitive interactions, since virtual environments are ideal for studying high performance professional activity in realistic but controlled settings.
C1 [Naor, Shani Kimel; Gottlieb, Amihai; Bahat, Yotam; Ben-Gal, Oran; Plotnik, Meir] Sheba Med Ctr Tel Hashomer, Ctr Adv Technol Rehabil, Ramat Gan, Israel.
   [Ketko, Itay] Israel Def Forces Med Corps, Inst Mil Physiol, Ramat Gan, Israel.
   [Yanovich, Ran; Heled, Yuval] Hebrew Univ Jerusalem, Fac Med, Dept Mil Med, Jerusalem, Israel.
   [Yanovich, Ran; Heled, Yuval] Kibbutzim Coll Educ Technol & Arts, Fac Sci, Dept Phys Educ & Movement, Tel Aviv, Israel.
   [Plotnik, Meir] Tel Aviv Univ, Fac Med, Dept Physiol & Pharmacol, Tel Aviv, Israel.
   [Plotnik, Meir] Tel Aviv Univ, Sagol Sch Neurosci, Tel Aviv, Israel.
C3 Chaim Sheba Medical Center; Hebrew University of Jerusalem; Tel Aviv
   University; Tel Aviv University
RP Plotnik, M (corresponding author), Sheba Med Ctr Tel Hashomer, Ctr Adv Technol Rehabil, Ramat Gan, Israel.; Plotnik, M (corresponding author), Tel Aviv Univ, Fac Med, Dept Physiol & Pharmacol, Tel Aviv, Israel.; Plotnik, M (corresponding author), Tel Aviv Univ, Sagol Sch Neurosci, Tel Aviv, Israel.
EM Meir.plotnik@sheba.health.gov.il
OI Plotnik, Meir/0000-0003-2637-3457
FU Israeli Ministry of Defense
FX We thank Mr. Hani Baransi from the Center of Advanced Technologies in
   Rehabilitation for technical support, and Ms. Michelle Florentine and
   Mr. Ryan Eli Solymani for English editing.
CR American College of Sports Medicine, 2013, ACSMS GUIDELINES EXE
   Arbuthnott K, 2000, J CLIN EXP NEUROPSYC, V22, P518, DOI 10.1076/1380-3395(200008)22:4;1-0;FT518
   Armstrong N, 2017, 4 INT C SOLD PHYS PE
   ASTRAND I, 1960, Acta Physiol Scand Suppl, V49, P1
   Audiffren M., 2016, The reticular-activating hypofrontality (RAH) model of acute exercise: current data and future perspectives
   Audiffren M, 2009, ACTA PSYCHOL, V132, P85, DOI 10.1016/j.actpsy.2009.06.008
   Barron LG, 2017, MIL PSYCHOL, V29, P316, DOI 10.1037/mil0000168
   Beer JMA, 2017, AEROSP MED HUM PERF, V88, P617, DOI 10.3357/AMHP.4709.2017
   BORG GAV, 1982, MED SCI SPORT EXER, V14, P377, DOI 10.1249/00005768-198205000-00012
   Braude D, 2011, PREHOSP EMERG CARE, V15, P254, DOI 10.3109/10903127.2010.545476
   Brisswalter J, 2002, SPORTS MED, V32, P555, DOI 10.2165/00007256-200232090-00002
   Bruce RA., 1972, exercise testing and training of apparently health individuals
   Chang YK, 2012, BRAIN RES, V1453, P87, DOI 10.1016/j.brainres.2012.02.068
   Davranche K, 2004, J SPORT SCI, V22, P419, DOI 10.1080/02640410410001675289
   Davranche K, 2015, J SPORT HEALTH SCI, V4, P56, DOI 10.1016/j.jshs.2014.08.004
   DElia L, 1989, Color trails test, psychological assessment resources
   Dietrich A, 2004, BRAIN COGNITION, V55, P516, DOI 10.1016/j.bandc.2004.03.002
   Dietrich A., 2018, Handbook of Sport Neuroscience and Psychophysiology, P385, DOI [10.4324/9781315723693-19, DOI 10.4324/9781315723693-19]
   Dietrich A., 2009, Exercise and Cognitive Function
   Dietrich A, 2011, NEUROSCI BIOBEHAV R, V35, P1305, DOI 10.1016/j.neubiorev.2011.02.001
   Eddy MD, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130817
   ELSMORE TF, 1994, BEHAV RES METH INSTR, V26, P421, DOI 10.3758/BF03204659
   Faul F, 2009, BEHAV RES METHODS, V41, P1149, DOI 10.3758/BRM.41.4.1149
   Hambrick DZ, 2010, APPL COGNITIVE PSYCH, V24, P1149, DOI 10.1002/acp.1624
   Harrison Y, 2000, J EXP PSYCHOL-APPL, V6, P236, DOI 10.1037//1076-898X.6.3.236
   Hüttermann S, 2014, PHYSIOL BEHAV, V131, P87, DOI 10.1016/j.physbeh.2014.04.020
   Kamijo K, 2019, MED SCI SPORT EXER, V51, P153, DOI 10.1249/MSS.0000000000001763
   Kavanagh J., 2005, STRESS PERFORMANCE R
   Labelle V, 2013, BRAIN COGNITION, V81, P10, DOI 10.1016/j.bandc.2012.10.001
   Ludyga S, 2016, PSYCHOPHYSIOLOGY, V53, P1611, DOI 10.1111/psyp.12736
   Mahoney CR, 2007, AVIAT SPACE ENVIR MD, V78, pB51
   Matsangas P, 2014, HUM FACTORS, V56, P1124, DOI 10.1177/0018720814522484
   McMorris T, 2018, PHYSIOL BEHAV, V188, P103, DOI 10.1016/j.physbeh.2018.01.029
   Moffat SD, 2009, NEUROPSYCHOL REV, V19, P478, DOI 10.1007/s11065-009-9120-3
   Nindl BC, 2007, MED SCI SPORT EXER, V39, P1380, DOI 10.1249/mss.0b013e318067e2f7
   PAAS FGWC, 1991, ERGONOMICS, V34, P1385, DOI 10.1080/00140139108964879
   Parsons TD, 2017, NEUROPSYCHOL REHABIL, V27, P777, DOI 10.1080/09602011.2015.1109524
   Parsons TD, 2015, FRONT HUM NEUROSCI, V9, DOI 10.3389/fnhum.2015.00660
   Plotnik M, 2017, 2017 INT C VIRT REH
   Plotnik M, 2021, J NEUROENG REHABIL, V18, DOI 10.1186/s12984-021-00849-9
   Pontifex MB, 2009, MED SCI SPORT EXER, V41, P927, DOI 10.1249/MSS.0b013e3181907d69
   Porras DC, 2019, THER ADV CHRONIC DIS, V10, DOI 10.1177/2040622319868379
   Porras DC, 2018, NEUROLOGY, V90, P1017, DOI 10.1212/WNL.0000000000005603
   Redick TS, 2016, J EXP PSYCHOL GEN, V145, P1473, DOI 10.1037/xge0000219
   REITAN R. M., 1958, PERCEPT MOT SKILLS, V8, P271
   REITAN RM, 1995, CLIN NEUROPSYCHOL, V9, P50, DOI 10.1080/13854049508402057
   Rizzo A, 2011, J CLIN PSYCHOL MED S, V18, P176, DOI 10.1007/s10880-011-9247-2
   Sams M.L., 2014, Comparison of static and countermovement jump variables in relation to estimated training load and subjective measures of fatigue
   Sánchez-Cubillo I, 2009, J INT NEUROPSYCH SOC, V15, P438, DOI 10.1017/S1355617709090626
   Sanders LMJ, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0210036
   Schmit C, 2020, INT REV SPORT EXER P, V13, P21, DOI 10.1080/1750984X.2018.1483527
   Sobhani V, 2014, Feyz J Kashan Univ Med Sci, V17
   Soga K, 2015, PSYCHOL SPORT EXERC, V16, P7, DOI 10.1016/j.psychsport.2014.08.010
   Tempest GD, 2017, BRAIN COGNITION, V113, P133, DOI 10.1016/j.bandc.2017.02.001
   Tombaugh TN, 2006, ARCH CLIN NEUROPSYCH, V21, P53, DOI 10.1016/j.acn.2005.07.006
   Wang C, 2014, BRAIN COGNITION, V85, P251, DOI 10.1016/j.bandc.2014.01.004
   Xu YD, 2006, NATURE, V440, P91, DOI 10.1038/nature04262
   Yanovich Ran, 2015, Disaster Mil Med, V1, P6, DOI 10.1186/2054-314X-1-6
   Ziemann E, 2011, J STRENGTH COND RES, V25, P1104, DOI 10.1519/JSC.0b013e3181d09ec9
NR 59
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD JUN 1
PY 2024
VL 28
IS 2
AR 120
DI 10.1007/s10055-024-01013-z
PG 16
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA SS8Z4
UT WOS:001236542100001
OA Green Submitted, hybrid
DA 2024-08-05
ER

PT J
AU Gong, Q
   Zou, N
   Yang, WJ
   Zheng, Q
   Chen, PR
AF Gong, Qing
   Zou, Ning
   Yang, Wenjing
   Zheng, Qi
   Chen, Pengrui
TI User experience model and design strategies for virtual reality-based
   cultural heritage exhibition
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Cultural heritage; User experience; Grounded theory;
   Museum exhibition
AB A virtual reality (VR) based cultural heritage exhibition (VRCHE) is an important type of VR-based museum exhibition. The user experience (UX) design of VRCHE has encountered opportunities and due to the differences in human-computer interaction between VR-based and conventional interaction interfaces, so proposing the UX model of VRCHE is crucial. Although there are some existing works that study the UX models of VRCHEs, they are not complete enough to describe the UX of VRCHEs or offer any design strategies due to the methodologies and experimental materials that they currently use. This study creates experiments utilizing grounded theory that combine qualitative and quantitative approaches. Then, the study synthesizes three-level coding and quantitative analysis findings from grounded theory, builds a detailed model of the VRCHE UX using theoretical coding, and proposes design strategies.
C1 [Gong, Qing; Zou, Ning; Yang, Wenjing; Zheng, Qi] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
   [Chen, Pengrui] Zhejiang Univ, Sch Software Technol, Ningbo, Peoples R China.
   [Gong, Qing; Zou, Ning] Zhejiang Univ, Innovat Ctr Yangtze River Delta, Future Design Lab, Jiaxing, Peoples R China.
C3 Zhejiang University; Zhejiang University; Zhejiang University
RP Zou, N (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.; Zou, N (corresponding author), Zhejiang Univ, Innovat Ctr Yangtze River Delta, Future Design Lab, Jiaxing, Peoples R China.
EM zn007@zju.edu.cn
CR Brooks JL, 2012, PSYCHOL METHODS, V17, P600, DOI 10.1037/a0029310
   Case K, 2013, ADV ENG FORUM, V10, P28, DOI 10.4028/www.scientific.net/AEF.10.28
   Chen CM, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0223994
   Flavián C, 2019, J BUS RES, V100, P547, DOI 10.1016/j.jbusres.2018.10.050
   Glaser B.G., 1967, Discovery of grounded theory: Strategies for qualitative research, DOI DOI 10.4324/9780203793206
   Gutwill JP, 2017, VISIT STUD, V20, P72, DOI 10.1080/10645578.2017.1297132
   Harrington MCR, 2023, FRONT VIRTUAL REAL, V4, DOI 10.3389/frvir.2023.1100540
   Harrington MCR, 2020, PROCEEDINGS OF 2020 6TH INTERNATIONAL CONFERENCE OF THE IMMERSIVE LEARNING RESEARCH NETWORK (ILRN 2020), P70, DOI [10.23919/iLRN47897.2020.9155202, 10.23919/ilrn47897.2020.9155202]
   Kabassi K, 2017, J CULT HERIT, V24, P184, DOI 10.1016/j.culher.2016.10.016
   Kennedy AAU, 2021, INT J SCI EDUC PART, V11, P242, DOI 10.1080/21548455.2021.1946619
   Kersten TP, 2017, INT ARCH PHOTOGRAMM, V42-5, P403, DOI 10.5194/isprs-archives-XLII-5-W1-403-2017
   Kim S, 2018, MUS MANAGE CURATOR, V33, P243, DOI 10.1080/09647775.2018.1466190
   Kristensson P, 2008, INT J SERV IND MANAG, V19, P474, DOI 10.1108/09564230810891914
   Kyriakou Panayiotis, 2019, Digital Applications in Archaeology and Cultural Heritage, V12, DOI 10.1016/j.daach.2018.e00088
   Li RJ, 2010, LECT NOTES COMPUT SC, V6436, P381
   Liu P, 2021, MUS MANAGE CURATOR, V36, P403, DOI 10.1080/09647775.2021.1948905
   Mann Steve., 2002, PRESENCE CONNECT, V1, P2002
   MILGRAM P, 1994, IEICE T INF SYST, VE77D, P1321
   Neff K, 2003, SELF IDENTITY, V2, P85, DOI 10.1080/15298860390129863
   Norman DA., 2004, EMOTIONAL DESIGN WHY
   Pagano A, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10093182
   Phillips WJ, 2021, CURR PSYCHOL, V40, P5040, DOI 10.1007/s12144-019-00452-1
   Quezada P, 2021, LECT NOTES COMPUT SC, V12779, P100, DOI 10.1007/978-3-030-78221-4_7
   Rahimi FB, 2022, VIRTUAL REAL-LONDON, V26, P1471, DOI 10.1007/s10055-022-00643-5
   Rahimi FB, 2018, INT J IND ERGONOM, V68, P245, DOI 10.1016/j.ergon.2018.08.002
   Ramm S, 2018, DES J, V21, P109, DOI 10.1080/14606925.2018.1395228
   Roussou Maria., 2002, Bar International Series, P93
   Roussou Maria., 2013, CHI'13 Extended Abstracts on Human Factors in Computing Systems, P547, DOI DOI 10.1145/2468356.2468453
   Schofield G, 2018, DIS 2018: PROCEEDINGS OF THE 2018 DESIGNING INTERACTIVE SYSTEMS CONFERENCE, P805, DOI 10.1145/3196709.3196714
   Solomon P, 1995, INFORM PROCESS MANAG, V31, P906, DOI 10.1016/0306-4573(95)90031-4
   Spapé MM, 2022, PSYCHON B REV, V29, P819, DOI 10.3758/s13423-021-02028-2
   Strauss AnselmL., 1987, Qualitative analysis for social scientists, P55, DOI DOI 10.1017/CBO9780511557842
   Trunfio M, 2020, CURR ISSUES TOUR, V23, P1053, DOI 10.1080/13683500.2019.1586847
   van den Haak M, 2003, BEHAV INFORM TECHNOL, V22, P339, DOI 10.1080/0044929031000
   von Willich J, 2019, PROCEEDINGS OF THE 2019 ACM DESIGNING INTERACTIVE SYSTEMS CONFERENCE (DIS 2019), P487, DOI 10.1145/3322276.3322334
   Wu Y, 2022, SAGE OPEN, V12, DOI 10.1177/21582440221082105
   Yi JH, 2021, ACM J COMPUT CULT HE, V14, DOI 10.1145/3462645
   Zhou YT, 2022, EDUC RES REV-NETH, V36, DOI 10.1016/j.edurev.2022.100454
   Zimmerman B. J., 2000, Handbook of Self-Regulation, P13, DOI [10.1016/B978-012109890-2/50031-7, DOI 10.1016/B978-012109890-2/50031-7, DOI 10.1016/B978-012109890-2/50031-74,5]
   Zou N, 2021, CCF T PERVAS COMPUT, V3, P112, DOI 10.1007/s42486-021-00061-7
NR 40
TC 1
Z9 1
U1 39
U2 39
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR 8
PY 2024
VL 28
IS 2
AR 69
DI 10.1007/s10055-024-00942-z
PG 21
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA LG7F1
UT WOS:001185691200003
OA hybrid
DA 2024-08-05
ER

PT J
AU Li, W
   Wang, JF
   Wei, M
AF Li, Wang
   Wang, Junfeng
   Wei, Ming
TI Local geometric edge features based registration for textureless object
   in augmented reality assisted assembly
SO VIRTUAL REALITY
LA English
DT Article
DE Augmented assembly; Registration; Feature description; Bundle adjustment
ID POSE ESTIMATION; TRACKING; RECOGNITION
AB Image-based methods have been widely used in augmented reality (AR) assistant assembly systems. However, due to the lack of sufficient texture information on the surface of assembly part, traditional image feature matching methods still face challenges. This paper proposes a coarse-to-fine AR registration method for textureless assembly part. In the first stage, a new feature matching method which is called line neighborhood edge descriptor (LNED) is presented to find the coarse camera pose from textureless image. The LNED take the contour line of assembly part as the description object, and use local geometric edge of assembly part to describe the contour line. During the image matching, the binary encoding is used to reduce the computational consumption for LNED. In the second stage, spatial points in the CAD model of assembly part are reverse projected to the textureless image based on the coarse camera pose. And the bundle adjustment method based on the edge distance of the textureless image is adopted to iteratively calculate the precise camera pose. In the experimental evaluation, the proposed registration method shows high accuracy and fast speed in comparison with conventional registration methods, which demonstrates that our method can effectively solve the problem of AR registration for textureless assembly part.
C1 [Li, Wang; Wang, Junfeng] Huazhong Univ Sci & Technol, Sch Mech Sci & Engn, Wuhan 430074, Peoples R China.
   [Li, Wang; Wei, Ming] Wuhan Fiberhome Tech Serv Co Ltd, Wuhan 430205, Peoples R China.
C3 Huazhong University of Science & Technology
RP Wang, JF (corresponding author), Huazhong Univ Sci & Technol, Sch Mech Sci & Engn, Wuhan 430074, Peoples R China.
EM wangjf@hust.edu.cn
RI Wang, Junfeng/AAV-4972-2021
OI Wang, Junfeng/0000-0003-2756-8803
FU National Natural Science Foundation of China [72271100]; National
   Natural Science Foundation of China
FX This research is supported by the National Natural Science Foundation of
   China (72271100).
CR Akinlar C, 2011, PATTERN RECOGN LETT, V32, P1633, DOI 10.1016/j.patrec.2011.06.001
   Astanin S, 2017, ROBOT CIM-INT MANUF, V44, P190, DOI 10.1016/j.rcim.2016.09.001
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Benmahdjoub M, 2022, VIRTUAL REAL-LONDON, V26, P1637, DOI 10.1007/s10055-022-00653-3
   Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56
   Chen CJ, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12168338
   Cardoso LFD, 2020, COMPUT IND ENG, V139, DOI 10.1016/j.cie.2019.106159
   DeTone D, 2016, Arxiv, DOI arXiv:1606.03798
   Dong YC, 2021, IEEE T CIRC SYST VID, V31, P1834, DOI 10.1109/TCSVT.2020.3011737
   Fang W, 2020, IEEE ACCESS, V8, P1935, DOI 10.1109/ACCESS.2019.2961959
   Fang W, 2017, COMPUT IND, V92-93, P91, DOI 10.1016/j.compind.2017.06.002
   Gattullo M, 2019, ROBOT CIM-INT MANUF, V56, P276, DOI 10.1016/j.rcim.2018.10.001
   Dos Santos CFG, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3490235
   He ZX, 2020, IEEE T IND ELECTRON, V67, P390, DOI 10.1109/TIE.2019.2897539
   Hinterstoisser S., 2012, ACCV, P548
   Jiang JJ, 2023, IEEE T IND INFORM, V19, P328, DOI 10.1109/TII.2022.3165979
   Korkut EH, 2023, VIRTUAL REAL-LONDON, V27, P1447, DOI 10.1007/s10055-023-00753-8
   Li CX, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19204586
   Li W, 2023, IEEE T IND INFORM, V19, P6825, DOI 10.1109/TII.2022.3189428
   Li W, 2021, J MANUF SYST, V61, P673, DOI 10.1016/j.jmsy.2020.12.017
   Liu HM, 2021, INT J MACH LEARN CYB, V12, P877, DOI 10.1007/s13042-020-01207-2
   Liu Y, 2015, INT J ADV MANUF TECH, V76, P281, DOI 10.1007/s00170-014-6274-9
   Lu KP, 2020, APPL MATH MODEL, V77, P1643, DOI 10.1016/j.apm.2019.09.009
   Olson CF, 1997, IEEE T IMAGE PROCESS, V6, P103, DOI 10.1109/83.552100
   Poursaeed O, 2019, LECT NOTES COMPUT SC, V11131, P485, DOI 10.1007/978-3-030-11015-4_35
   Rambach J, 2017, ADJUNCT P 2017 IEEE, P9
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Shu MC, 2022, ISPRS J PHOTOGRAMM, V185, P85, DOI 10.1016/j.isprsjprs.2022.01.010
   Sujin JS, 2023, COMPUT SYST SCI ENG, V44, P157, DOI 10.32604/csse.2023.023747
   Tombari F, 2013, IEEE I CONF COMP VIS, P1265, DOI 10.1109/ICCV.2013.160
   Tremblay Jonathan, 2018, ARXIV180910790
   Tsai CY, 2018, J APPL SCI ENG, V21, P229, DOI 10.6180/jase.201806_21(2).0011
   von Gioi RG, 2012, IMAGE PROCESS ON LIN, V2, P35, DOI 10.5201/ipol.2012.gjmr-lsd
   Wang B, 2019, MULTIMED TOOLS APPL, V78, P12307, DOI 10.1007/s11042-018-6727-5
   Wang K, 2020, ROBOT CIM-INT MANUF, V63, DOI 10.1016/j.rcim.2019.101890
   Wang P, 2023, VIRTUAL REAL-LONDON, V27, P1409, DOI 10.1007/s10055-023-00748-5
   Wang Y, 2018, ASSEMBLY AUTOM, V38, P77, DOI 10.1108/AA-11-2016-152
   Wang Y, 2017, INT J ADV MANUF TECH, V89, P1699, DOI 10.1007/s00170-016-9180-5
   Xie JC, 2022, COMPUT IND ENG, V168, DOI 10.1016/j.cie.2022.108050
   Xu C, 2017, IEEE T PATTERN ANAL, V39, P1209, DOI 10.1109/TPAMI.2016.2582162
   Yang X, 2023, NEUROCOMPUTING, V515, P13, DOI 10.1016/j.neucom.2022.09.151
   Yu HC, 2018, ALGORITHMS, V11, DOI 10.3390/a11120201
   Zhang L, 2023, VIRTUAL REAL-LONDON, V27, P1273, DOI 10.1007/s10055-022-00735-2
   Zhang LL, 2013, J VIS COMMUN IMAGE R, V24, P794, DOI 10.1016/j.jvcir.2013.05.006
   Zubizarreta J, 2019, INT J ADV MANUF TECH, V102, P4095, DOI 10.1007/s00170-019-03527-2
NR 45
TC 0
Z9 0
U1 15
U2 16
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 28
DI 10.1007/s10055-023-00922-9
PG 15
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA FR1V3
UT WOS:001147497600002
OA hybrid
DA 2024-08-05
ER

PT J
AU Liou, WK
   Lin, WH
   Lee, YT
   Chen, SF
   Liang, C
AF Liou, Wei-Kai
   Lin, Wen-Hsiang
   Lee, Yen-Tung
   Chen, Sufen
   Liang, Caleb
TI The distinction between first-person perspective and third-person
   perspective in virtual bodily self-consciousness
SO VIRTUAL REALITY
LA English
DT Article
DE First-person perspective; Body ownership; Self-location; Body-location;
   1PP-location; Virtual reality
ID RUBBER HAND ILLUSION; BODY OWNERSHIP; MECHANISMS; LOCATION; AGENCY;
   INTEGRATION; BRAIN
AB The distinction between the first-person perspective (1PP) and the third-person perspective (3PP) has been widely regarded as fundamental and rigid, and many researchers hold that genuine bodily illusions can only be experienced from the 1PP. We applied VR technology to investigate whether this mainstream view is correct. In our experiments, the participants were immersed in a VR environment in which they saw a life-sized virtual body either from the 1PP or from the 3PP. They either passively received tactile stimulations and/or actively interacted with a virtual soccer ball. Our VR system created novel visuo-motor-tactile correlations between the real and the virtual world: when the participant interacted with a real plastic soccer ball, he/she would feel corresponding tactile sensations and see the avatar performing the exact same movements. We found that a clear sense of ownership over the avatar was induced not only in the 1PP condition but also in the Passive-3PP and the Active-3PP conditions. We also observed evidence suggesting that it is possible to experience one's body-location, 1PP-location, as well as self-location, both from the 1PP and from the 3PP. Together, we demonstrate that there is in fact no fundamental gap between embodied 1PP and embodied 3PP in the virtual world.
C1 [Liou, Wei-Kai] Natl Taiwan Univ Sci & Technol, Empower Vocat Educ Res Ctr, Taipei, Taiwan.
   [Lin, Wen-Hsiang; Chen, Sufen] Natl Taiwan Univ Sci & Technol, Grad Inst Digital Learning & Educ, Taipei, Taiwan.
   [Lee, Yen-Tung] Western Univ, Dept Philosophy, London, ON, Canada.
   [Lee, Yen-Tung] Western Univ, Rotman Inst Philosophy, London, ON, Canada.
   [Chen, Sufen] North West Univ, Optentia Res Focus Area, Vanderbijlpark, South Africa.
   [Liang, Caleb] Natl Taiwan Univ, Dept Philosophy, Taipei, Taiwan.
   [Liang, Caleb] Natl Taiwan Univ, Grad Inst Brain & Mind Sci, Taipei, Taiwan.
C3 National Taiwan University of Science & Technology; National Taiwan
   University of Science & Technology; Western University (University of
   Western Ontario); Western University (University of Western Ontario);
   North West University - South Africa; National Taiwan University;
   National Taiwan University
RP Liang, C (corresponding author), Natl Taiwan Univ, Dept Philosophy, Taipei, Taiwan.; Liang, C (corresponding author), Natl Taiwan Univ, Grad Inst Brain & Mind Sci, Taipei, Taiwan.
EM yiliang@ntu.edu.tw
OI Lee, Yen-Tung/0000-0003-2502-8862
FU Ministry of Science and Technology [2023]; National Science and
   Technology Council Taiwanese Overseas Pioneers Grants (TOP Grants) [NSTC
   110-2410-H-002-142-MY2]; National Science and Technology Council of
   Taiwan; Empower Vocational Education Research Center of National Taiwan
   University of Science and Technology (NTUST)
FX The authors would like to thank Yun-Yu He, Chi-Hong Chen, Yi-Jan Wang,
   Tai-Yuan Chang, Hsiang-Yin Wu, Shang-Yi Chuang and Ssu-Ching Huang for
   their assistance in our lab. We would also like to thank Professor
   Chen-Gia Tsai from the Graduate Institute of Musicology for the SCR
   equipment. Yen-Tung Lee is funded by 2023 National Science and
   Technology Council Taiwanese Overseas Pioneers Grants (TOP Grants) for
   PhD Candidates. Finally, this study was supported by National Science
   and Technology Council of Taiwan (project: NSTC 110-2410-H-002-142-MY2)
   and by the Empower Vocational Education Research Center of National
   Taiwan University of Science and Technology (NTUST).
CR Armel KC, 2003, P ROY SOC B-BIOL SCI, V270, P1499, DOI 10.1098/rspb.2003.2364
   Aspell JE, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0006488
   Aspell JE, 2013, PSYCHOL SCI, V24, P2445, DOI 10.1177/0956797613498395
   Baker L.R., 2000, PERSONS BODIES CONST, DOI DOI 10.1017/CBO9781139173124
   BENTLER PM, 1990, PSYCHOL BULL, V107, P238, DOI 10.1037/0033-2909.88.3.588
   Bergström I, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0148060
   Blanke O, 2005, BRAIN RES REV, V50, P184, DOI 10.1016/j.brainresrev.2005.05.008
   Blanke O, 2015, NEURON, V88, P145, DOI 10.1016/j.neuron.2015.09.029
   Blanke O, 2012, NAT REV NEUROSCI, V13, P556, DOI 10.1038/nrn3292
   Blanke O, 2009, TRENDS COGN SCI, V13, P7, DOI 10.1016/j.tics.2008.10.003
   Botvinick M, 1998, NATURE, V391, P756, DOI 10.1038/35784
   Browne MW., 1993, Sage focus editions, P445, DOI DOI 10.1177/0049124192021002005
   Butler AA, 2017, ACTA PHYSIOL, V220, P19, DOI 10.1111/apha.12792
   Cabrera-Alvarez MJ, 2020, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.560669
   Carmines E. G., 1981, SOCIAL MEASUREMENT C, DOI DOI 10.1207/S15327965PLI0404_1
   Chen WY, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-19662-x
   Christoff K, 2011, TRENDS COGN SCI, V15, P104, DOI 10.1016/j.tics.2011.01.001
   Cohen J., 1988, Statistical power analysis for the behavioral sciences
   David N, 2006, J COGNITIVE NEUROSCI, V18, P898, DOI 10.1162/jocn.2006.18.6.898
   Day B, 2019, J EXP PSYCHOL-APPL, V25, P1, DOI 10.1037/xap0000192
   de Vignemont F., 2018, MIND BODY EXPLORATIO, P140
   Debarba HG, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0190109
   Dummer T, 2009, PERCEPTION, V38, P271, DOI 10.1068/p5921
   Ebrahimi E, 2018, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2018), DOI 10.1145/3225153.3225170
   Ehrsson H.H., 2012, HDB MULTISENSORY PRO, P775, DOI DOI 10.1126/SCIENCE.1097011
   Ehrsson HH, 2007, SCIENCE, V317, P1048, DOI 10.1126/science.1142175
   Gallagher S, 2000, TRENDS COGN SCI, V4, P14, DOI 10.1016/S1364-6613(99)01417-5
   Gallagher Shaun, 2012, Consciousness and Subjectivity, DOI [10.1515/9783110325843.245, DOI 10.1515/9783110325843.245]
   Gallivan JP, 2013, ELIFE, V2, DOI 10.7554/eLife.00425
   Gonzalez-Franco M, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P18, DOI [10.1109/VR46266.2020.00-85, 10.1109/VR46266.2020.1580500165557]
   Guterstam A, 2015, CURR BIOL, V25, P1416, DOI 10.1016/j.cub.2015.03.059
   Guterstam A, 2012, CONSCIOUS COGN, V21, P1037, DOI 10.1016/j.concog.2012.01.018
   Guterstam A, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0017208
   Heydrich L, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00946
   Hu LT, 1999, STRUCT EQU MODELING, V6, P1, DOI 10.1080/10705519909540118
   Huang HC, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.00370
   Husserl E, 1973, Husserliana 14: zur phanomenologie der intersubjektivitat: texte aus dem nachlass, zweiter teil: 1921-1928
   Ionta S, 2011, NEURON, V70, P363, DOI 10.1016/j.neuron.2011.03.009
   Jenkinson PM, 2015, CONSCIOUS COGN, V33, P432, DOI 10.1016/j.concog.2015.02.020
   Kalckert A, 2012, FRONT HUM NEUROSCI, V6, DOI 10.3389/fnhum.2012.00040
   Kokkinara E, 2016, SCI REP-UK, V6, DOI 10.1038/srep28879
   Legrand D, 2007, CONSCIOUS COGN, V16, P583, DOI 10.1016/j.concog.2007.04.002
   Legrand D, 2009, PSYCHOL REV, V116, P252, DOI 10.1037/a0014172
   Lenggenhager B, 2007, SCIENCE, V317, P1096, DOI 10.1126/science.1143439
   Lenggenhager B, 2009, CONSCIOUS COGN, V18, P110, DOI 10.1016/j.concog.2008.11.003
   Levine J., 2001, Purple Haze: The Puzzle of Consciousness, DOI [10.1093/0195132351.001.0001, DOI 10.1093/0195132351.001.0001]
   Liang C., 2016, Open MIND: philosophy and the mind sciences in the 21st century, P957
   Maselli A, 2014, FRONT HUM NEUROSCI, V8, DOI 10.3389/fnhum.2014.00693
   Maselli A, 2013, FRONT HUM NEUROSCI, V7, DOI 10.3389/fnhum.2013.00083
   Metzinger T, 2003, BRADFORD BOOKS, P1
   Miller LE., 2023, The routledge handbook of bodily awareness, P22
   NAGEL T, 1974, PHILOS REV, V83, P435, DOI 10.2307/2183914
   Nakul E, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-63643-y
   Palluel E, 2012, NEUROREPORT, V23, P354, DOI 10.1097/WNR.0b013e328351db14
   Palluel E, 2011, J NEUROPHYSIOL, V105, P2239, DOI 10.1152/jn.00744.2010
   Petkova VI, 2011, FRONT PSYCHOL, V2, DOI 10.3389/fpsyg.2011.00035
   Petkova VI, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0003832
   Riemer M, 2013, EXP BRAIN RES, V229, P383, DOI 10.1007/s00221-012-3374-3
   Serino A, 2013, CONSCIOUS COGN, V22, P1239, DOI 10.1016/j.concog.2013.08.013
   Slater M, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010564
   Tsakiris M, 2005, J EXP PSYCHOL HUMAN, V31, P80, DOI 10.1037/0096-1523.31.1.80
   Tsakiris M, 2006, CONSCIOUS COGN, V15, P423, DOI 10.1016/j.concog.2005.09.004
   Tsakiris M, 2010, NEUROPSYCHOLOGIA, V48, P703, DOI 10.1016/j.neuropsychologia.2009.09.034
   Vogeley K, 2004, J COGNITIVE NEUROSCI, V16, P817, DOI 10.1162/089892904970799
   Vogeley K, 2003, TRENDS COGN SCI, V7, P38, DOI 10.1016/S1364-6613(02)00003-7
   Walsh LD, 2011, J PHYSIOL-LONDON, V589, P3009, DOI 10.1113/jphysiol.2011.204941
   Wittgenstein Ludwig., 1958, BLUE BROWN BOOKS
   Yee N, 2007, HUM COMMUN RES, V33, P271, DOI 10.1111/j.1468-2958.2007.00299.x
   Zahavi D., 2020, OXFORD HDB PHILOS CO, P635
   Zahavi Dan., 2005, SUBJECTIVITY SELFHOO, DOI [10.7551/mitpress/6541.001.0001, DOI 10.7551/MITPRESS/6541.001.0001]
NR 70
TC 0
Z9 0
U1 11
U2 13
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 1
DI 10.1007/s10055-023-00907-8
PG 19
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA CV7Q1
UT WOS:001128079900001
DA 2024-08-05
ER

PT J
AU Rutkowski, S
   Jakóbczyk, A
   Abrahamek, K
   Nowakowska, A
   Nowak, M
   Liska, D
   Batalik, L
   Colombo, V
   Sacco, M
AF Rutkowski, Sebastian
   Jakobczyk, Aleksandra
   Abrahamek, Kacper
   Nowakowska, Aleksandra
   Nowak, Magdalena
   Liska, David
   Batalik, Ladislav
   Colombo, Vera
   Sacco, Marco
TI Training using a commercial immersive virtual reality system on hand-eye
   coordination and reaction time in students: a randomized controlled
   trial
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Beat Saber; Hand-eye coordination; Reaction time;
   Immersion
ID TEST-RETEST RELIABILITY; VIDEO; SICKNESS; EXERCISE; GAMES
AB The implementation of VR games opens up a wide range of opportunities for the development of dexterity, speed and precision of movements among various professional groups. The aim of this study was to investigate the effects of a commercial immersive VR music game on hand-eye coordination and reaction time speed in students. This study enrolled 32 individuals, randomly assigned to the experimental or control group. The intervention consisted of a 15-min training session of the immersive music game "Beat Saber", once a day for 5 consecutive days. The primary outcomes included reaction time measurements: the plate tapping test and the ruler-drop test (Ditrich's test), trial making test (TMT) A and TMT B to assess coordination and visual attention, likewise VR sickness assessment by Virtual Reality Sickness Questionnaire (VRSQ). The secondary outcome included an energy expenditure assessment (SenseWear Armband). The data analysis revealed a statistically significant improvement in hand-eye coordination in the experimental group, with no improvement in the control group. The results were similar in measurements of reaction time. Analysis of the VRSQ questionnaire results showed a statistically significant reduction in oculomotor domain symptoms and total score during successive training days. The immersive VR music game has the potential to improve reaction time and hand-eye coordination in students.
C1 [Rutkowski, Sebastian] Opole Univ Technol, Dept Phys Educ & Physiotherapy, PL-45758 Opole, Poland.
   [Jakobczyk, Aleksandra; Abrahamek, Kacper; Nowakowska, Aleksandra; Nowak, Magdalena] Opole Univ Technol, Fac Phys Educ & Physiotherapy, Descartes Error Student Res Assoc, PL-45758 Opole, Poland.
   [Liska, David] Matej Bel Univ, Fac Sports Sci & Hlth, Banska Bystrica 97410, Slovakia.
   [Batalik, Ladislav] Univ Hosp Brno, Dept Rehabil, Brno 62500, Czech Republic.
   [Batalik, Ladislav] Masaryk Univ, Dept Physiotherapy & Rehabil, Fac Med, Brno 62500, Czech Republic.
   [Colombo, Vera; Sacco, Marco] Natl Res Council Italy CNR, Inst Intelligent Ind Technol & Syst Adv Mfg STIIMA, I-23900 Lecce, Italy.
C3 Opole University of Technology; Opole University of Technology; Matej
   Bel University; University Hospital Brno; Masaryk University Brno;
   Consiglio Nazionale delle Ricerche (CNR)
RP Rutkowski, S (corresponding author), Opole Univ Technol, Dept Phys Educ & Physiotherapy, PL-45758 Opole, Poland.
EM s.rutkowski@po.edu.pl
RI Batalik, Ladislav/L-9506-2019
OI Batalik, Ladislav/0000-0003-2147-1541; Liska, David/0000-0002-5700-1341
FU Polish National Agency for Academic Exchange [BPN/BEK/2021/1/00105]
FX Sebastian Rutkowski thanks the Polish National Agency for Academic
   Exchange (BPN/BEK/2021/1/00105) for funding his internship at the
   STIIMA-CNR, which has facilitated the development of this paper.
CR Ahmadpour N, 2019, INT J BIOCHEM CELL B, V114, DOI 10.1016/j.biocel.2019.105568
   Andersson HB, 2020, P 2020 IEEE C VIRTUA
   Balakrishnan G, 2014, NEUROL RES INT, V2014, DOI 10.1155/2014/301473
   Basso Julia C, 2017, Brain Plast, V2, P127, DOI 10.3233/BPL-160040
   Batra A., 2014, Indian J Sci Res Technol, V2, P25
   Bavelier D, 2012, ANNU REV NEUROSCI, V35, P391, DOI 10.1146/annurev-neuro-060909-152832
   Bisoglio Joseph, 2014, Front Psychol, V5, P136, DOI 10.3389/fpsyg.2014.00136
   Blasco-Peris C, 2022, INT J ENV RES PUB HE, V19, DOI 10.3390/ijerph19063492
   Brazeau AS, 2011, APPL PHYSIOL NUTR ME, V36, P339, DOI [10.1139/h11-016, 10.1139/H11-016]
   Buchanan Judith A, 2004, J Dent Educ, V68, P1258
   Chandra S, 2016, PROCEDIA COMPUT SCI, V84, P115, DOI 10.1016/j.procs.2016.04.074
   Cieslik B, 2020, COMPLEMENT THER MED, V52, DOI 10.1016/j.ctim.2020.102480
   Cohen J., 1988, Statistical power analysis for the behavioral sciences
   Czech O, 2022, COMPLEMENT THER MED, V68, DOI 10.1016/j.ctim.2022.102837
   Czech O, 2021, J CLIN MED, V10, DOI 10.3390/jcm10153248
   Davis AJ, 2021, J CLIN NURS, V30, P3001, DOI 10.1111/jocn.15808
   Dobbs BM, 2013, AGE AGEING, V42, P577, DOI 10.1093/ageing/aft073
   Dong J, 2022, IEEE MULTIMEDIA, V29, P74, DOI 10.1109/MMUL.2022.3176142
   Drenowatz C, 2011, EUR J APPL PHYSIOL, V111, P883, DOI 10.1007/s00421-010-1695-0
   Ferreira S, 2022, INT J ENV RES PUB HE, V19, DOI 10.3390/ijerph192214667
   Flores-gallegos Rodrigo, 2022, International Journal of Child-Computer Interaction, DOI 10.1016/j.ijcci.2021.100394
   García-Bravo S, 2020, INT J ENV RES PUB HE, V17, DOI 10.3390/ijerph17228472
   Garrett B, 2018, JMIR SERIOUS GAMES, V6, DOI 10.2196/10839
   Glueck AC, 2020, VIRTUAL REAL-LONDON, V24, P223, DOI 10.1007/s10055-019-00392-y
   Harrington CM, 2018, SURG ENDOSC, V32, P3813, DOI 10.1007/s00464-018-6107-2
   Kim HK, 2018, APPL ERGON, V69, P66, DOI 10.1016/j.apergo.2017.12.016
   Kourtesis P., 2023, Virt. Worlds, V2, P16, DOI DOI 10.3390/VIRTUALWORLDS2010002
   Kumar BS, 2022, PAK J MED SCI, V38, P692, DOI 10.12669/pjms.38.3.5153
   Li JH, 2018, AGING MENT HEALTH, V22, P1634, DOI 10.1080/13607863.2017.1385722
   McDermott AF, 2014, COMPUT HUM BEHAV, V34, P69, DOI 10.1016/j.chb.2014.01.018
   Mendes MD, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0200701
   Palmisano S, 2022, VIRTUAL REAL-LONDON, V26, P1373, DOI 10.1007/s10055-022-00634-6
   Peng W, 2011, CYBERPSYCH BEH SOC N, V14, P681, DOI 10.1089/cyber.2010.0578
   Powers MB, 2015, COGN BEHAV THERAPY, V44, P237, DOI 10.1080/16506073.2015.1047286
   Rutkowska A, 2022, INT J ENV RES PUB HE, V19, DOI 10.3390/ijerph19106155
   Rutkowski S, 2021, FRONT PHYSIOL, V12, DOI 10.3389/fphys.2021.702266
   Rutkowski S, 2021, COMPLEMENT THER MED, V61, DOI 10.1016/j.ctim.2021.102767
   Rutkowski S, 2021, INT J ENV RES PUB HE, V18, DOI 10.3390/ijerph18031297
   Rutkowski S, 2020, J REHABIL MED, V52, DOI 10.2340/16501977-2755
   Scherr J, 2013, EUR J APPL PHYSIOL, V113, P147, DOI 10.1007/s00421-012-2421-x
   Shin JW, 2015, J PHYS THER SCI, V27, P2151, DOI 10.1589/jpts.27.2151
   Sienkiewicz-Dianzenza E, 2019, BIOMED HUM KINET, V11, P131, DOI 10.2478/bhk-2019-0018
   Sittikraipong K, 2020, MUSCULOSKEL SCI PRAC, V50, DOI 10.1016/j.msksp.2020.102273
   Sousa CV, 2022, J SPORT HEALTH SCI, V11, P164, DOI 10.1016/j.jshs.2021.05.002
   Straker LM, 2011, BMC PUBLIC HEALTH, V11, DOI 10.1186/1471-2458-11-654
   Szpak A, 2020, J MED INTERNET RES, V22, DOI 10.2196/19840
   Tao GR, 2021, J NEUROENG REHABIL, V18, DOI 10.1186/s12984-020-00801-3
   Tsigilis N, 2002, PERCEPT MOTOR SKILL, V95, P1295, DOI 10.2466/PMS.95.8.1295-1300
   van Ede F, 2012, J NEUROSCI, V32, P10408, DOI 10.1523/JNEUROSCI.1337-12.2012
   World Med Assoc, 2013, JAMA-J AM MED ASSOC, V310, P2191, DOI 10.1001/jama.2013.281053
NR 50
TC 1
Z9 1
U1 9
U2 12
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 7
DI 10.1007/s10055-023-00898-6
PG 11
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA DX0O7
UT WOS:001135265600004
OA hybrid
DA 2024-08-05
ER

PT J
AU Duan, ZJ
   Zhou, JJ
   Gu, FZ
AF Duan, Zhengjie
   Zhou, Jiajun
   Gu, Fangzhou
TI Cognitive differences in product shape evaluation between real settings
   and virtual reality: case study of two-wheel electric vehicles
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Product shape design; Design evaluation; Cognitive
   differences; Two-wheel electric vehicle
ID CONSUMER RESPONSES; DESIGN; PERCEPTION; QUALITY; IMPACTS
AB Product shape evaluation is an important part of new product development. In the shape design stage, design schemes are often presented through visual images. The presentation of visual images causes evaluators to form different cognitive experiences and evaluation results. In recent years, virtual reality (VR) technology has been widely used in the field of industrial design, enriching the presentation forms of design scheme images. Although VR technology has shown the potential to improve evaluators' perception and cognitive experiences in product shape design, research comparing it with traditional methods remains relatively scattered. This study used two-wheel electric vehicles as an example to examine the difference in evaluators' cognition of product shape in VR and a real setting (RS). First, we established a semantic scale comprising seven pairs of opposite adjectives to evaluate the shape scheme. Second, we built VR and RS evaluation environments using head-mounted displays and paper renderings, respectively. The participants evaluated the vehicle shape design in alternating viewing and underwent semi-structured interviews on cognitive experience. We analyzed the experimental and interview results based on three aspects of product shape cognition. The results demonstrated that volume cognition was significantly more accurate in VR environments. Furthermore, graphic cognition, particularly regarding shape details, differed partially between environments. VR provided a better sense of immersion and more variable viewing angles than RS. Treatment cognition did not exhibit significant differences between environments, as it depended on human experience rather than visualization. These findings suggest that VR tools are more suited for shaping design evaluations early. Selecting suitable visual presentation tools based on evaluators' cognitive characteristics at different evaluation nodes to display design schemes is a practical, economical, and efficient strategy.
C1 [Duan, Zhengjie; Zhou, Jiajun] Zhejiang Sci Tech Univ, Sch Art & Design, Hangzhou 310018, Peoples R China.
   [Gu, Fangzhou] Changsha Univ, Sch Art Design, Changsha 410022, Peoples R China.
C3 Zhejiang Sci-Tech University; Changsha University
RP Duan, ZJ (corresponding author), Zhejiang Sci Tech Univ, Sch Art & Design, Hangzhou 310018, Peoples R China.
EM diana481@zstu.edu.cn
FU Zhejiang Sci-Tech University, China
FX The authors would like to thank all the participants for taking part in
   the experiments.DAS:No datasets were generated or analysed during the
   current study.
CR Algharabat R, 2017, J RETAIL CONSUM SERV, V36, P203, DOI 10.1016/j.jretconser.2017.02.007
   Artacho-Ramírez MA, 2008, INT J IND ERGONOM, V38, P942, DOI 10.1016/j.ergon.2008.02.020
   Berni A, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9071064
   Camburn B, 2017, DES SCI, V3, DOI 10.1017/dsj.2017.10
   Cao XC, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23115088
   Catalano CE, 2007, AI EDAM, V21, P73, DOI 10.1017/S0890060407070151
   Catalanoc E, 2005, Car Aesthet Annotation, P8
   Cecil J, 2007, INT J ADV MANUF TECH, V31, P846, DOI 10.1007/s00170-005-0267-7
   Chen LY, 2007, Research on the form of Automobile Case Based on the Gestalt Theory, P18
   Chen WR, 2019, Research and Practice of Design Review System on Virtual Reality
   Coburn JQ, 2017, J COMPUT INF SCI ENG, V17, DOI 10.1115/1.4036921
   Colombo EF, 2016, J INTEGR DES PROCESS, V20, P3, DOI 10.3233/jid-2016-0019
   De Crescenzio F, 2019, INT J INTERACT DES M, V13, P761, DOI 10.1007/s12008-019-00565-8
   Duan ZJ., 2017, Packag Eng, V38, P87
   Duan ZJ, 2021, The Research of Product Design Decision Based on Value Frame, P31
   Falcao Christianne Soares, 2013, Design, User Experience, and Usability. Web, Mobile, and Product Design. Second International Conference, DUXU 2013 Held as Part of HCI International 2013. Proceedings. LNCS 8015, P342, DOI 10.1007/978-3-642-39253-5_37
   Felip F, 2020, VIRTUAL REAL-LONDON, V24, P439, DOI 10.1007/s10055-019-00406-9
   Fiorentino M, 2002, DESIGN 2002: PROCEEDINGS OF THE 7TH INTERNATIONAL DESIGN CONFERENCE, VOLS 1 AND 2, P477
   Forslund K, 2013, INT J DES, V7, P69
   Galan J, 2021, IJIMAI, V6
   Grewal D, 2020, J ACAD MARKET SCI, V48, P96, DOI 10.1007/s11747-019-00697-z
   Han J, 2023, LAND-BASEL, V12, DOI 10.3390/land12020345
   HARRIS Z.S., 1962, STRING ANAL SENTENCE
   [胡婷婷 Hu Tingting], 2015, [包装工程, Packaging Engineering], V36, P33
   Huang FC, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766922
   Jeong SW, 2009, INTERNET RES, V19, P105, DOI 10.1108/10662240910927858
   Jiang ZH, 2007, INFORM SYST RES, V18, P454, DOI 10.1287/isre.1070.0124
   Jing C, 2015, H. The Car Styling Design Method Based on Evolutionary Thought, P5
   Kato T, 2019, INT J INTERACT DES M, V13, P1233, DOI 10.1007/s12008-019-00568-5
   Kazmi IK, 2013, 2013 10 INT C COMP G, P1
   Lanier J., 1992, Interactive Learning International, V8, P275
   Li TT, 2023, ADV ENG INFORM, V56, DOI 10.1016/j.aei.2023.101995
   Liang Q., 2013, Zhuangshi, V11, P87
   Liem A, 2009, P 5 INT WORKSH DES S
   Liu CR, 2018, LECT NOTES COMPUT SC, V10920, P314, DOI 10.1007/978-3-319-91806-8_24
   liu Shuo, 2017, [The Journal of Industrial Distribution & Business, 산경연구논집], V8, P15
   Lukacevic F, 2021, FRONT ENG MANAG, V8, P412, DOI 10.1007/s42524-020-0099-z
   Mugge R, 2009, DESIGN STUD, V30, P287, DOI 10.1016/j.destud.2008.10.002
   Noon C, 2012, COMPUT IND, V63, P500, DOI 10.1016/j.compind.2012.02.003
   Osgood C. E., 1957, MEASUREMENT MEANING
   Ottosson S, 2002, J ENG DESIGN, V13, P159, DOI 10.1080/09544820210129823
   Ozok AA, 2009, INT J HUM-COMPUT INT, V25, P243, DOI 10.1080/10447310802546724
   Palacios-Ibanez A, 2022, 13 INT C APPL HUM FA, P24
   Palacios-Ibáñez A, 2023, COMPUT IND, V144, DOI 10.1016/j.compind.2022.103780
   Panzoli D, 2019, INT CONF GAMES VIRTU, P248, DOI 10.1109/vs-games.2019.8864546
   Petiot JF, 2010, INT J INTERACT DES M, V4, P211, DOI 10.1007/s12008-010-0111-7
   Pizzi G, 2019, COMPUT HUM BEHAV, V96, P1, DOI 10.1016/j.chb.2019.02.008
   Qi LB., 2006, Shandong Sci, V19, P13
   Ranscombe C, 2017, J MECH DESIGN, V139, DOI 10.1115/1.4037249
   Riegler A, 2019, Proc. Mensch Comput., P853
   Saraswati T.G., 2018, JSAB, V2, P19
   Suwa M., 1998, Design Studies, V19, P455, DOI 10.1016/S0142-694X(98)00016-7
   Tversky B, 2002, INT J HUM-COMPUT ST, V57, P247, DOI [10.1006/ijhc.2002.1017, 10.1006/ijhc.1017]
   Verhagen T, 2014, COMPUT HUM BEHAV, V39, P270, DOI 10.1016/j.chb.2014.07.036
   Wang S, 2010, 2010 18 INT C GEOINF, P1, DOI DOI 10.1109/GEOINFORMATICS.2010.5567608
   Wolfartsberger J, 2019, AUTOMAT CONSTR, V104, P27, DOI 10.1016/j.autcon.2019.03.018
   Wong JF, 2010, DESIGN STUD, V31, P237, DOI 10.1016/j.destud.2009.11.002
   Wu KW, 2016, J RETAIL CONSUM SERV, V28, P17, DOI 10.1016/j.jretconser.2015.08.009
   Ye J, 2007, LECT NOTES COMPUT SC, V4553, P1190
   Yoo J, 2014, J BUS RES, V67, P2464, DOI 10.1016/j.jbusres.2014.03.006
   Zhang J, 2014, Y. Study on the Virtual Evaluation Methods of Automotive Instrument Panel Based on RTT
   [张茉楠 Zhang Monan], 2003, [中国软科学, China Soft Science], V0, P141
   Zhao D. H., 2007, Packag Eng, V28, P115
   Zhao DA, 2013, Car Styling-based Study: the Designer's Intension and User's Interpretation, P13
   Zhao JH, 2005, The meaning of Design Art, P20
   Zhu Y, 2009, Semantic Study on Automobile Form and Design Process Configuration
   Zignego MI, 2020, INT J INTERACT DES M, V14, P467, DOI 10.1007/s12008-019-00629-9
NR 67
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD JUL 15
PY 2024
VL 28
IS 3
AR 136
DI 10.1007/s10055-024-01034-8
PG 14
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA YT9F3
UT WOS:001270848400001
DA 2024-08-05
ER

PT J
AU Zheleva, A
   De Marez, L
   Talsma, D
   Bombeke, K
AF Zheleva, Aleksandra
   De Marez, Lieven
   Talsma, Durk
   Bombeke, Klaas
TI Intersecting realms: a cross-disciplinary examination of VR quality of
   experience research
SO VIRTUAL REALITY
LA English
DT Article
DE Quality of experience; Virtual reality; Literature review; Critical
   analysis
ID VIRTUAL-REALITY; ENVIRONMENTS
AB The advent of virtual reality (VR) technology has necessitated a reevaluation of quality of experience (QoE) models. While numerous recent efforts have been dedicated to creating comprehensive QoE frameworks it seems that the majority of the factors studied as potential influencers of QoE are often limited to single disciplinary viewpoints or specific user-related aspects. Furthermore, the majority of literature reviews in this domain seem to have predominantly focused on academic sources, overlooking industry insights. To address these points, the current research took an interdisciplinary literature review approach to examine QoE literature covering both academic and industry sources from diverse fields (i.e., psychology, ergonomics, user experience, communication science, and engineering). Based on this rich dataset, we created a QoE model that illustrated 252 factors grouped into four branches - user, system, context, and content. The main finding of this review emphasized the substantial gap in the current research landscape, where complex interactions among user, system, context, and content factors in VR are overlooked. The current research not only identified this crucial disparity in existing QoE studies but also provided a substantial online repository of over 200 QoE-related factors. The repository serves as an indispensable tool for future researchers aiming to construct a more holistic understanding of QoE.
C1 [Zheleva, Aleksandra; De Marez, Lieven; Bombeke, Klaas] Univ Ghent, Dept Commun Sci, Imec Mict UGent, Ghent, Belgium.
   [Zheleva, Aleksandra; Talsma, Durk] Univ Ghent, Fac Psychol & Educ Sci, Dept Expt Psychol, Ghent, Belgium.
C3 Ghent University; Ghent University
RP Zheleva, A (corresponding author), Univ Ghent, Dept Commun Sci, Imec Mict UGent, Ghent, Belgium.; Zheleva, A (corresponding author), Univ Ghent, Fac Psychol & Educ Sci, Dept Expt Psychol, Ghent, Belgium.
EM aleksandra.zheleva@ugent.be; lieven.demarez@ugent.com;
   durk.talsma@ugent.com; klaas.bombeke@ugent.com
OI Zheleva, Aleksandra/0000-0003-3478-4969
FU Fonds Wetenschappelijk Onderzoek [11G6522N]; Research
   Foundation-Flanders (FWO)
FX The work of Aleksandra Zheleva is supported by the Research
   Foundation-Flanders (FWO) (11G6522N).DAS:Data associated with this study
   has been deposited in the Open Science Framework at
   https://bit.ly/3MGT4Dy.
CR Abeele VV., 2021, ACM Trans Access Comput, V14, P1, DOI [10.1145/3470743, DOI 10.1145/3470743]
   Akinyele S., 2008, IFE Psychol, DOI [10.4314/ifep.v16i2.23801, DOI 10.4314/IFEP.V16I2.23801]
   Angelov V, 2020, 2ND INTERNATIONAL CONGRESS ON HUMAN-COMPUTER INTERACTION, OPTIMIZATION AND ROBOTIC APPLICATIONS (HORA 2020), P520, DOI 10.1109/hora49412.2020.9152604
   [Anonymous], 2015, ISO 9000
   Anwar MS, 2020, IEEE ACCESS, V8, P204585, DOI 10.1109/ACCESS.2020.3037253
   Arora A, 2012, OTOLARYNG HEAD NECK, V146, P497, DOI 10.1177/0194599811427385
   Ball C, 2021, TELEMAT INFORM, V65, DOI 10.1016/j.tele.2021.101728
   Barsasella D, 2020, PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON INFORMATION AND COMMUNICATION TECHNOLOGIES FOR AGEING WELL AND E-HEALTH (ICT4AWE), P165, DOI 10.5220/0009425801650171
   Belmudez B, 2015, T-LAB SER TELECOMMUN, P1, DOI 10.1007/978-3-319-14166-4
   Boas Y.A. G. V., 2013, INTERACTIVE MULTIMED
   Boger Y, 2017, Pixel density & retinal resolution, and why it's important for ar/vr headsets
   Callet PL, 2013, COST Action IC, V1003
   Carrion B, 2021, COMPUT ELECTR ENG, V93, DOI 10.1016/j.compeleceng.2021.107283
   Carrozzino M, 2010, J CULT HERIT, V11, P452, DOI 10.1016/j.culher.2010.04.001
   Cebeci B, 2019, COMPUT ANIMAT VIRT W, V30, DOI 10.1002/cav.1893
   Chen J, 2020, INFORMATION, V11, DOI 10.3390/info11090423
   Chen JF, 2011, COMPUT EDUC, V57, P2126, DOI 10.1016/j.compedu.2011.05.017
   Chen MZ, 2018, IEEE T COMMUN, V66, P5621, DOI 10.1109/TCOMM.2018.2850303
   Coldham G, 2017, 2017 NATIONAL INFORMATION TECHNOLOGY CONFERENCE (NITC), P131, DOI 10.1109/NITC.2017.8285645
   Dale B.G., 1997, Managing Service Quality, V7, P242, DOI DOI 10.1108/09604529710172881
   de Regt A, 2020, BUS HORIZONS, V63, P737, DOI 10.1016/j.bushor.2020.08.002
   Dionisio JDN., 2013, ACM Comput Surv, V10, P2480751
   Ebrahimi T., 2009, P 17 ACM INT C MULTI, P3, DOI DOI 10.1145/1631272.1631275
   Felnhofer A., 2012, P INT SOC PRESENCE R
   Fernandes AS, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P201, DOI 10.1109/3DUI.2016.7460053
   Gao G, 2019, Content-aware personalised rate adaptation for adaptive streaming via deep video analysis, V2019, P1, DOI [10.1109/ICC.2019.876115, DOI 10.1109/ICC.2019.876115]
   García-Valle G, 2018, IEEE ACCESS, V6, P7224, DOI 10.1109/ACCESS.2017.2782254
   Gomes GD, 2020, A QOE evaluation of an immersive virtual reality autonomous driving experience, DOI [10.1109/QoMEX48832.2020.9123128, DOI 10.1109/QOMEX48832.2020.9123128]
   Grzelka A, 2019, PICT COD SYMP, DOI 10.1109/pcs48520.2019.8954527
   Hameed A, 2019, LECT NOTES COMPUT SC, V11869, P389, DOI 10.1007/978-3-030-33894-7_41
   Hong Y, 2018, 24TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2018), DOI 10.1145/3281505.3281519
   Hvass J, 2018, VISUAL REALISM PRESE, V2017, P1, DOI [DOI 10.1109/3DTV.2017.8280421, 10.1109/3DTV.2017.8280421]
   International Organization for Standardization, 2018, ISO 9004:2018(en)
   International Telecommunication Union-Telecommunication Standardization Sector, 2021, Multimedia quality of service and performance-generic and user-related aspects: Influencing factors on quality of experience for virtual reality services ITU-T RecommendationG.1035
   International Telecommunication Union-Telecommunication Standardization Sector, 2020, ITU-T RecommendationNo. P.919)
   International Telecommunication Union-Telecommunication Standardization Sector, 2007, 4 FG IPTV M BLED SLO
   ITUTS, 2018, Tech. Rep. No. G.1070
   Jang Y, 2019, TELEMAT INFORM, V42, DOI 10.1016/j.tele.2019.101239
   Jiang ZQ, 2020, IEEE T VEH TECHNOL, V69, P2157, DOI 10.1109/TVT.2019.2960866
   Kartiko I, 2010, COMPUT EDUC, V55, P881, DOI 10.1016/j.compedu.2010.03.019
   Kern AC, 2020, FRONT ROBOT AI, V7, DOI 10.3389/frobt.2020.00020
   Kim J, 2018, Virtual reality sickness predictor: analysis of visual-vestibular conflict and VR contents, DOI [10.1109/QoMEX.2018.8463413, DOI 10.1109/QOMEX.2018.8463413]
   Kunz A, 2016, PROC CIRP, V44, P257, DOI 10.1016/j.procir.2016.02.086
   Lang B, 2021, Oculus quest 2 privacy settings and vr data collected by facebook
   Letter JD., 2021, Qual User Exp, V6, P1, DOI [10.1007/s41233-021-00045-6, DOI 10.1007/S41233-021-00045-6]
   Li BJ, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.02116
   Ling Y, 2013, COMPUT HUM BEHAV, V29, P1519, DOI 10.1016/j.chb.2012.12.010
   McDonnell N, 2019, Immersive technology and medical visualisation: a users guide, V1156, P123, DOI [10.1007/978-3-030-19385-09, DOI 10.1007/978-3-030-19385-09]
   Mehrfard A, 2019, Arxiv, DOI arXiv:1912.02913
   Melo M, 2018, COMPUT GRAPH-UK, V71, P159, DOI 10.1016/j.cag.2017.11.007
   MIRVIS PH, 1991, ACAD MANAGE REV, V16, P636, DOI 10.2307/258925
   Moller S, 2014, T-LAB SER TELECOMMUN, P1, DOI 10.1007/978-3-319-02681-7
   Moller S, 2014, Quality of experience, DOI [10.1007/978-3-319-02681-77, DOI 10.1007/978-3-319-02681-77]
   Newman M, 2022, J ENVIRON PSYCHOL, V79, DOI 10.1016/j.jenvp.2021.101733
   Oh CS, 2018, FRONT ROBOT AI, V5, DOI 10.3389/frobt.2018.00114
   Perdana A, 2022, TECHNOL SOC, V68, DOI 10.1016/j.techsoc.2021.101817
   Perkis A, 2020, arXiv
   Perroud B, 2019, TRANSPORT RES F-TRAF, V61, P238, DOI 10.1016/j.trf.2017.08.015
   Rosenberg L, 2021, Regulate the metaverse. Why regulation is important and urgent
   Sagnier C, 2019, TRAV HUMAIN, V82, P183, DOI 10.3917/th.823.0183
   Sas C, 2004, LECT NOTES COMPUT SC, V3038, P1017
   Saxena A, 2020, PROCEEDINGS OF THE 13TH EAI INTERNATIONAL CONFERENCE ON PERFORMANCE EVALUATION METHODOLOGIES AND TOOLS ( VALUETOOLS 2020), P156, DOI 10.1145/3388831.3388834
   Silva R., 2019, The international journal of virtual reality, V19, P11, DOI DOI 10.20870/IJVR.2019.19.1.2908
   Slater M., 1994, Presence: teleoperators virtual environments, V3, P130, DOI [10.1162/pres.1994.3.2.130, DOI 10.1162/PRES.1994.3.2.130, 10.1162/pres.1994.3.2.1302]
   Slater M, 2009, PHILOS T R SOC B, V364, P3549, DOI 10.1098/rstb.2009.0138
   Snyder H, 2019, J BUS RES, V104, P333, DOI 10.1016/j.jbusres.2019.07.039
   Thau J, 2024, Forbes Technology Council
   Torraco R. J., 2005, HUM RESOUR DEV REV, V4, P356, DOI [DOI 10.1177/1534484305278283, 10.1177/1534484305278283]
   Turner G, 2022, Virtual reality awareness and adoption: 2022 annual report
   van der Bijl-Brouwer M, 2017, DESIGN STUD, V53, P1, DOI 10.1016/j.destud.2017.06.003
   Vanden Abeele V, 2020, INT J HUM-COMPUT ST, V135, DOI 10.1016/j.ijhcs.2019.102370
   Vlahovic S, 2022, J MULTIMODAL USER IN, V16, P257, DOI 10.1007/s12193-022-00388-0
   Weech S, 2020, INT J HUM-COMPUT ST, V138, DOI 10.1016/j.ijhcs.2020.102398
   Whittemore R, 2005, J ADV NURS, V52, P546, DOI 10.1111/j.1365-2648.2005.03621.x
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   Zheleva A., 2021, Can you make the cut? Exploring the effect of frequency of cuts in virtual reality storytelling, P45, DOI [10.1007/978-3-030-68086-24, DOI 10.1007/978-3-030-68086-24]
NR 76
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD JUL 10
PY 2024
VL 28
IS 3
AR 135
DI 10.1007/s10055-024-01031-x
PG 14
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA YL5Q0
UT WOS:001268660100001
OA hybrid
DA 2024-08-05
ER

PT J
AU Neugebauer, A
   Castner, N
   Severitt, B
   Stingl, K
   Ivanov, I
   Wahl, S
AF Neugebauer, Alexander
   Castner, Nora
   Severitt, Bjoern
   Stingl, Katarina
   Ivanov, Iliya
   Wahl, Siegfried
TI Simulating vision impairment in virtual reality: a comparison of visual
   task performance with real and simulated tunnel vision
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Retinitis pigmentosa; Vision impairment simulation;
   Tunnel vision; Disability
ID EQUIVALENCE
AB In this work, we explore the potential and limitations of simulating gaze-contingent tunnel vision conditions using Virtual Reality (VR) with built-in eye tracking technology. This approach promises an easy and accessible way of expanding study populations and test groups for visual training, visual aids, or accessibility evaluations. However, it is crucial to assess the validity and reliability of simulating these types of visual impairments and evaluate the extend to which participants with simulated tunnel vision can represent real patients. Two age-matched participant groups were acquired: The first group (n = 8, aged 20-60, average 49.1 +/- 13.2) consisted of patients diagnosed with Retinitis pigmentosa (RP). The second group (n = 8, aged 27-59, average 46.5 +/- 10.8) consisted of visually healthy participants with simulated tunnel vision. Both groups carried out different visual tasks in a virtual environment for 30 min per day over the course of four weeks. Task performances as well as gaze characteristics were evaluated in both groups over the course of the study. Using the 'two one-sided tests for equivalence' method, the two groups were found to perform similar in all three visual tasks. Significant differences between groups were found in different aspects of their gaze behavior, though most of these aspects seem to converge over time. Our study evaluates the potential and limitations of using Virtual Reality technology to simulate the effects of tunnel vision within controlled virtual environments. We find that the simulation accurately represents performance of RP patients in the context of group averages, but fails to fully replicate effects on gaze behavior.
C1 [Neugebauer, Alexander; Severitt, Bjoern; Wahl, Siegfried] Univ Tubingen, Inst Ophthalm Res, ZEISS Vis Sci Lab, Elfriede Aulhorn Str 7, D-72076 Tubingen, Germany.
   [Castner, Nora; Ivanov, Iliya; Wahl, Siegfried] Carl Zeiss Vis Int GmbH, Turnstr 27, D-73430 Aalen, Germany.
   [Stingl, Katarina] Univ Tubingen, Univ Eye Hosp, Ctr Ophthalmol, Elfriede Aulhorn Str 7, D-72076 Tubingen, Germany.
C3 Eberhard Karls University of Tubingen; Eberhard Karls University
   Hospital; Eberhard Karls University of Tubingen; Eberhard Karls
   University Hospital
RP Neugebauer, A (corresponding author), Univ Tubingen, Inst Ophthalm Res, ZEISS Vis Sci Lab, Elfriede Aulhorn Str 7, D-72076 Tubingen, Germany.
EM a.neugebauer@uni-tuebingen.de; nora.castner@zeiss.com;
   bjoern.severitt@uni-tuebingen.de; katarina.stingl@med.uni-tuebingen.de;
   iliya.ivanov@zeiss.com; siegfried.wahl@uni-tuebingen.de
FU German Research Foundation (DFG)
FX We want to thank Enkelejda Kasneci for her indispensable guidance and
   support for this work. Furthermore, we thank our participants who made
   this study possible.
CR Acevedo V, 2022, 2022 16TH INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY & INTERNET-BASED SYSTEMS, SITIS, P354, DOI 10.1109/SITIS57111.2022.00060
   Adhanom IB, 2023, VIRTUAL REAL-LONDON, V27, P1481, DOI 10.1007/s10055-022-00738-z
   Albert R, 2017, ACM T APPL PERCEPT, V14, DOI 10.1145/3127589
   Authié CN, 2024, AM J OPHTHALMOL, V258, P43, DOI 10.1016/j.ajo.2023.06.028
   Barhorst-Cates EM, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0163785
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Creem-Regehr SH, 2021, COGN RES, V6, DOI 10.1186/s41235-020-00265-y
   David EJ, 2021, J VISION, V21, DOI 10.1167/jov.21.7.3
   FINLAY D, 1982, PERCEPTION, V11, P457, DOI 10.1068/p110457
   Fletcher Donald C, 2012, Optom Vis Sci, V89, P1395, DOI 10.1097/OPX.0b013e318264cc77
   Geisler W. S., 2002, Proceedings ETRA 2002. Eye Tracking Research and Applications Symposium, P83, DOI 10.1145/507072.507090
   Gopalakrishnan S, 2020, CYBERPSYCH BEH SOC N, V23, P171, DOI 10.1089/cyber.2019.0235
   Hecht H, 2015, ATTEN PERCEPT PSYCHO, V77, P2399, DOI 10.3758/s13414-015-0931-4
   Hepperle D, 2023, MULTIMODAL TECHNOLOG, V7, DOI 10.3390/mti7060056
   Hibbard Paul B, 2023, Curr Top Behav Neurosci, V65, P131, DOI 10.1007/7854_2023_416
   Hoste A M, 2003, Bull Soc Belge Ophtalmol, P65
   Ivanov IV, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0157825
   Jones PR, 2020, NPJ DIGIT MED, V3, DOI 10.1038/s41746-020-0242-6
   Jones PR, 2018, 2018 IEEE WORKSHOP ON AUGMENTED AND VIRTUAL REALITIES FOR GOOD (VAR4GOOD)
   Krösl K, 2023, MULTIMODAL TECHNOLOG, V7, DOI 10.3390/mti7070070
   Krösl K, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P655, DOI [10.1109/VR.2019.8798239, 10.1109/vr.2019.8798239]
   Krösl K, 2018, VISUAL COMPUT, V34, P911, DOI 10.1007/s00371-018-1517-7
   Laiho P, 2020, Comparing vr headsets' tracking performance
   Lewis J, 2011, 2011 IEEE 1 INT C SE, P1
   Mitani AA, 2020, JAMA NETW OPEN, V3, DOI 10.1001/jamanetworkopen.2020.1965
   Neugebauer A, 2023, GazeQuest-a virtual reality framework for rehabilitation, simulation, and research of visual field defects
   Neugebauer A, 2024, Eye tracking data of participants with real and simulated tunnel vision in three virtual reality based visual tasks, DOI [10.57754/FDAT.5zmx4-y4w22, DOI 10.57754/FDAT.5ZMX4-Y4W22]
   Neugebauer Alexander, 2024, PLoS One, V19, pe0291902, DOI 10.1371/journal.pone.0291902
   Neugebauer A, 2021, BRAIN SCI, V11, DOI 10.3390/brainsci11020223
   Ng TH, 2001, DRUG INF J, V35, P1517, DOI 10.1177/009286150103500446
   Nyström M, 2010, BEHAV RES METHODS, V42, P188, DOI 10.3758/BRM.42.1.188
   Pastel S, 2022, VIRTUAL REAL-LONDON, V26, P91, DOI 10.1007/s10055-021-00539-w
   Pastel S, 2021, J MOTOR BEHAV, V53, P693, DOI 10.1080/00222895.2020.1843390
   Petersen G. B., 2022, Technology, Mind, and Behavior, V3, DOI [10.1037/tmb0000076, DOI 10.1037/TMB0000076]
   Read Jacob M., 2017, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V61, P2105, DOI 10.1177/1541931213602008
   Sauer Y, 2022, VIRTUAL REAL-LONDON, V26, P1089, DOI 10.1007/s10055-021-00619-x
   SCHUIRMANN DJ, 1987, J PHARMACOKINET BIOP, V15, P657, DOI 10.1007/BF01068419
   Stein N, 2021, I-PERCEPTION, V12, DOI 10.1177/2041669520983338
   Stock S, 2018, 24TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2018), DOI 10.1145/3281505.3283395
   Sullivan Gail M, 2021, J Grad Med Educ, V13, P457, DOI 10.4300/JGME-D-21-00599.1
   Sullivan Gail M, 2013, J Grad Med Educ, V5, P541, DOI 10.4300/JGME-5-4-18
   Sullivan JH, 2021, J BUS RES, V132, P530, DOI 10.1016/j.jbusres.2021.03.066
   Tobii, 2023, Tobii XR integration for pico neo 2 eye
   Väyrynen J, 2016, 15TH INTERNATIONAL CONFERENCE ON MOBILE AND UBIQUITOUS MULTIMEDIA (MUM 2016), P69, DOI 10.1145/3012709.3012714
NR 44
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD APR 16
PY 2024
VL 28
IS 2
AR 97
DI 10.1007/s10055-024-00987-0
PG 18
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA NY3J1
UT WOS:001203970900001
OA hybrid, Green Submitted
DA 2024-08-05
ER

PT J
AU Barton, AC
   Do, M
   Sheen, J
   Byrne, LK
AF Barton, Adam C.
   Do, Michael
   Sheen, Jade
   Byrne, Linda K.
TI The restorative and state enhancing potential of abstract fractal-like
   imagery and interactive mindfulness interventions in virtual reality
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; VR; Attention restoration; Mindfulness meditation;
   Interactive meditation; Well-being; Mood; Cognitive enhancement;
   Fractal; Abstract environment; Natural environment; Guided breathing
ID FOCUSED ATTENTION; MEDITATION; BENEFITS; MIND; WORKPLACE; EXPOSURE;
   ENVIRONMENTS; RELAXATION
AB The restorative and mental state enhancing effects of brief mindfulness-based interventions (MBIs) and restorative environments such as nature has been supported in the research literature. However, regular adoption of these practices is limited by practical constraints and motivational barriers. The current study addressed these challenges by introducing two novel approaches which utilise the immersive and interactive qualities of virtual reality (VR). This included an interactive MBI and an abstract restorative environment using fractal-like imagery. These approaches were explored using a comparative evaluation of two short (6 min) VR interventions: Passive VR (applying principles from restorative interventions) and Interactive VR (implementing a focused attention form of mindfulness meditation). A mixed methods approach revealed increased state mindfulness, reduced mental fatigue, and enhanced aspects of mood (calm/relaxation, anxiety) consistently between conditions. Between group differences revealed additional benefits for cognition (focus), mood (happiness and sadness), and motivational value with the interactive intervention. The abstract environment, used in both interventions, maintained comparable levels of perceived restoration with a nature VR control condition. The results provide preliminary evidence supporting the use of interactive approaches for mindfulness interventions and abstract versions of restorative environments.
C1 [Barton, Adam C.; Do, Michael; Sheen, Jade; Byrne, Linda K.] Deakin Univ, Fac Hlth, Sch Psychol, Geelong, Vic, Australia.
   [Byrne, Linda K.] Cairnmillar Inst, Fac Psychol Counselling & Psychotherapy, Melbourne, Vic, Australia.
C3 Deakin University; Cairnmillar Institute
RP Barton, AC (corresponding author), Deakin Univ, Fac Hlth, Sch Psychol, Geelong, Vic, Australia.
EM acbarton@deakin.edu.au
RI Sheen, Jade/AGL-4405-2022
OI Sheen, Jade/0000-0001-8327-5314; Do, Michael/0000-0002-1253-1007; Byrne,
   Linda/0000-0001-9055-0046
FU Deakin University
FX No Statement Available
CR AJZEN I, 1991, ORGAN BEHAV HUM DEC, V50, P179, DOI 10.1016/0749-5978(91)90020-T
   Al Farsi G., 2021, Journal of Hunan University Natural Sciences, V48
   Alter AL, 2009, PERS SOC PSYCHOL REV, V13, P219, DOI 10.1177/1088868309341564
   Anderson T, 2019, J COGN ENHANCE, V3, P207, DOI 10.1007/s41465-018-00119-y
   [Anonymous], 1986, The beauty of fractals: images of complex dynamical systems
   [Anonymous], 2015, STRESS AM PAYING OUR
   Arch JJ, 2006, BEHAV RES THER, V44, P1849, DOI 10.1016/j.brat.2005.12.007
   Baldwin CL, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0186231
   Barton AC, 2020, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.02050
   Basu A, 2018, Environ Behav
   Bennett AA, 2019, J Occup Health Psychol
   BENSON H, 1974, PSYCHIATRY, V37, P37, DOI 10.1080/00332747.1974.11023785
   Bertamini M, 2013, I-PERCEPTION, V4, P317, DOI 10.1068/i0601JW
   Berto R, 2005, J ENVIRON PSYCHOL, V25, P249, DOI 10.1016/j.jenvp.2005.07.001
   Bishop SR, 2004, CLIN PSYCHOL-SCI PR, V11, P230, DOI 10.1093/clipsy/bph077
   Blum J, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.02172
   Braun V., 2019, APA HDB RES METHODS, P843, DOI [DOI 10.1037/13620-000, 10.1007/978-981-10-5251-4_10., 10.1037/13620-004, DOI 10.1037/13620-004, 10.1007/978-981-10-2779-6_103-1]
   Brown KW, 2003, J PERS SOC PSYCHOL, V84, P822, DOI 10.1037/0022-3514.84.4.822
   Browning MHEM, 2020, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.02667
   Caldwell JA, 2019, NEUROSCI BIOBEHAV R, V96, P272, DOI 10.1016/j.neubiorev.2018.10.024
   Chandrasiri A, 2020, VIRTUAL REAL-LONDON, V24, P143, DOI 10.1007/s10055-019-00380-2
   Chung K, 2018, J MED INTERNET RES, V20, DOI 10.2196/11152
   Clark D, 2015, FRONT HUM NEUROSCI, V9, DOI [10.3389/fhurn.2015.00297, 10.3389/fnhum.2015.00297]
   COHEN S, 1980, PSYCHOL BULL, V88, P82, DOI 10.1037/0033-2909.88.1.82
   Colzato LS, 2016, CONSCIOUS COGN, V39, P1, DOI 10.1016/j.concog.2015.11.003
   Cummings ML, 2016, HUM FACTORS, V58, P279, DOI 10.1177/0018720815609503
   Dehghani M, 2022, BEHAV INFORM TECHNOL, V41, P1453, DOI 10.1080/0144929X.2021.1876767
   Ditto B, 2006, ANN BEHAV MED, V32, P227, DOI 10.1207/s15324796abm3203_9
   Döllinger N, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.644683
   Du Plessis I, 2017, ACM SIGGRAPH 2017 VR
   Emfield AG, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.00548
   Feinberg RR, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3502035
   Fernández-Aranda F, 2012, J MENT HEALTH, V21, P364, DOI 10.3109/09638237.2012.664302
   Frewen P, 2016, PSYCHOL ASSESSMENT, V28, P830, DOI 10.1037/pas0000283
   Gouyet J.-F., 1997, American Journal of Physics, DOI DOI 10.1119/1.18629
   Grassini S, 2019, J ENVIRON PSYCHOL, V62, P1, DOI 10.1016/j.jenvp.2019.01.007
   Gromala D, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P521, DOI 10.1145/2702123.2702344
   Grosse-Hering Barbara., 2013, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, P3431, DOI DOI 10.1145/2470654.2466472
   Guastello S.J., 2016, Nonlinear Dynamical Systems Analysis for the Behavioral Sciences Using Real Data, P355, DOI [10.1201/9781439820025-18, DOI 10.1201/9781439820025-18]
   Hägerhäll CM, 2015, NONLIN DYNAM PSYCHOL, V19, P1
   Hagerhall CM, 2008, PERCEPTION, V37, P1488, DOI 10.1068/p5918
   Hartig T, 1996, SCAND J PSYCHOL, V37, P378, DOI 10.1111/j.1467-9450.1996.tb00670.x
   Hartig T, 2014, ANNU REV PUBL HEALTH, V35, P207, DOI 10.1146/annurev-publhealth-032013-182443
   Hartig T, 2011, FORESTS, TREES AND HUMAN HEALTH, P127, DOI 10.1007/978-90-481-9806-1_5
   Hasenkamp W, 2012, NEUROIMAGE, V59, P750, DOI 10.1016/j.neuroimage.2011.07.008
   Hicks LJ, 2020, J ENVIRON PSYCHOL, V71, DOI 10.1016/j.jenvp.2020.101488
   Home R, 2012, LEISURE SCI, V34, P350, DOI 10.1080/01490400.2012.687644
   Hunt CA, 2020, MINDFULNESS, V11, P1139, DOI 10.1007/s12671-020-01308-7
   Hussien Ahmed MM, 2017, P 2017 CHI C HUM FAC
   Ioannucci S, 2021, bioRxiv, V2011
   Islam M., 2018, J vis, V18, P265, DOI [10.1167/18.10.265, DOI 10.1167/18.10.265]
   Jarvela Simo, 2021, ACM Transactions on Social Computing, V4, DOI 10.1145/3449358
   Jiang B, 2021, ENVIRON BEHAV, V53, P296, DOI 10.1177/0013916520947111
   Johnson KR, 2020, EUR J TRAIN DEV, V44, P341, DOI 10.1108/EJTD-09-2019-0156
   Joye Y, 2018, J ENVIRON PSYCHOL, V59, P1, DOI 10.1016/j.jenvp.2018.08.006
   Joye Y, 2016, J EXP PSYCHOL HUMAN, V42, P103, DOI 10.1037/xhp0000105
   Kabat-Zinn J., 1990, Full Catastrophe Living
   Kajimura S, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-75396-9
   Kaplan R., 1989, The Experience of Nature: A Psychological Perspective, pxii, 340
   KAPLAN S, 1995, J ENVIRON PSYCHOL, V15, P169, DOI 10.1016/0272-4944(95)90001-2
   Kaplan S, 2001, ENVIRON BEHAV, V33, P480, DOI 10.1177/00139160121973106
   Kemp M, 1998, NATURE, V394, P627, DOI 10.1038/29195
   Kemper KJ, 2017, J EVID-BASED INTEGR, V22, P395, DOI 10.1177/2156587216663565
   Killingsworth MA, 2010, SCIENCE, V330, P932, DOI 10.1126/science.1192439
   Kitson A, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.01354
   Kosunen I., 2017, P 2017 ACM WORKSH AP, P29, DOI [DOI 10.1145/3038439.3038443,DOI, 10.1145/3038439.3038443, DOI 10.1145/3038439.3038443]
   Kosunen I, 2016, PROCEEDINGS OF THE 21ST INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES (IUI'16), P208, DOI 10.1145/2856767.2856796
   Laurie J, 2016, INT J MED INFORM, V96, P38, DOI 10.1016/j.ijmedinf.2016.02.010
   Li G, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20082208
   Li HQD, 2021, INT J ENV RES PUB HE, V18, DOI 10.3390/ijerph18179053
   Lin IM, 2014, INT J PSYCHOPHYSIOL, V91, P206, DOI 10.1016/j.ijpsycho.2013.12.006
   Lippelt DP, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.01083
   Lutz A, 2008, TRENDS COGN SCI, V12, P163, DOI 10.1016/j.tics.2008.01.005
   Lutz A, 2015, AM PSYCHOL, V70, P632, DOI 10.1037/a0039585
   Lymeus F, 2019, Mindfulness training supported by a restorative natural setting: Integrating individual and environmental approaches to the management of adaptive resources
   Lymeus F, 2018, CONSCIOUS COGN, V59, P40, DOI 10.1016/j.concog.2018.01.008
   Lymeus F, 2017, ENVIRON BEHAV, V49, P536, DOI 10.1177/0013916516657390
   Ma X, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.00874
   MAIMAN LA, 1974, HEALTH EDUC QUART, V2, P336, DOI 10.1177/109019817400200404
   Mair P, 2020, BEHAV RES METHODS, V52, P464, DOI 10.3758/s13428-019-01246-w
   Mandelbrot B, 1982, FRACTAL GEOMETRY NAT
   MANDELBROT BB, 1989, LEONARDO, P21
   Mehta KJ, 2022, HUM SOC SCI COMMUN, V9, DOI 10.1057/s41599-021-01031-1
   Mottelson A, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.681482
   Mrazek MD, 2012, EMOTION, V12, P442, DOI 10.1037/a0026678
   Navarro-Haro MV, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0187777
   Naylor M, 2019, INTERACT COMPUT, V31, P507, DOI 10.1093/iwc/iwz033
   Niksirat KS, 2019, ACM T COMPUT-HUM INT, V26, DOI 10.1145/3359593
   Norling JC, 2008, J PHYS ACT HEALTH, V5, P184, DOI 10.1123/jpah.5.1.184
   Norris CJ, 2018, FRONT HUM NEUROSCI, V12, DOI 10.3389/fnhum.2018.00315
   Ohly H, 2016, J TOXICOL ENV HEAL B, V19, P305, DOI 10.1080/10937404.2016.1196155
   Oppenheimer DM, 2008, TRENDS COGN SCI, V12, P237, DOI 10.1016/j.tics.2008.02.014
   Patibanda R, 2017, CHI PLAY'17: PROCEEDINGS OF THE ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P19, DOI 10.1145/3116595.3116621
   Posner MI, 2015, CURR OPIN BEHAV SCI, V4, P1, DOI 10.1016/j.cobeha.2014.12.008
   Prochaska JO., 2020, Encyclopedia of behavioral medicine, P1
   Prpa M, 2018, DIS 2018: PROCEEDINGS OF THE 2018 DESIGNING INTERACTIVE SYSTEMS CONFERENCE, P71, DOI 10.1145/3196709.3196765
   R Core Team, 2023, A language and environment for statistical computing
   Ramsetty A, 2020, J AM MED INFORM ASSN, V27, P1147, DOI 10.1093/jamia/ocaa078
   Riches S, 2021, SOC PSYCH PSYCH EPID, V56, P1707, DOI 10.1007/s00127-021-02110-z
   Rizzo A, 2019, J TECHNOL HUMAN SERV, V37, P51, DOI 10.1080/15228835.2019.1604292
   Rockstroh C, 2021, VIRTUAL REAL-LONDON, V25, P539, DOI 10.1007/s10055-020-00471-5
   Rupp MA, 2017, HUM FACTORS, V59, P1096, DOI 10.1177/0018720817715360
   Russo MA, 2017, BREATHE, V13, P298, DOI 10.1183/20734735.009817
   Sadeghniiat-Haghighi Khosro, 2015, Ind Psychiatry J, V24, P12, DOI 10.4103/0972-6748.160915
   Sagnier C, 2020, INT J HUM-COMPUT INT, V36, P993, DOI 10.1080/10447318.2019.1708612
   Salmon P, 2010, SPORT PSYCHOL, V24, P127, DOI 10.1123/tsp.24.2.127
   Schutte NS, 2017, ECOPSYCHOLOGY, V9, P1, DOI 10.1089/eco.2016.0042
   Seabrook E, 2020, J MED INTERNET RES, V22, DOI 10.2196/16106
   Slater M, 1997, PRESENCE-VIRTUAL AUG, V6, P603, DOI 10.1162/pres.1997.6.6.603
   Souchet AD, 2022, INT J HUM-COMPUT INT, V38, P801, DOI 10.1080/10447318.2021.1976509
   Spehar B, 2003, COMPUT GRAPH-UK, V27, P813, DOI 10.1016/S0097-8493(03)00154-7
   Stevenson MP, 2018, J TOXICOL ENV HEAL B, V21, P227, DOI 10.1080/10937404.2018.1505571
   Stewart M, 2018, ECOPSYCHOLOGY, V10, P53, DOI 10.1089/eco.2017.0033
   Street N, 2016, FRONT HUM NEUROSCI, V10, DOI 10.3389/fnhum.2016.00213
   SWELLER J, 1988, COGNITIVE SCI, V12, P257, DOI 10.1207/s15516709cog1202_4
   Tanay G, 2013, PSYCHOL ASSESSMENT, V25, P1286, DOI 10.1037/a0034044
   Tang YY, 2009, TRENDS COGN SCI, V13, P222, DOI 10.1016/j.tics.2009.01.009
   Terzimehic N, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300687
   Tinga AM, 2019, APPL PSYCHOPHYS BIOF, V44, P51, DOI 10.1007/s10484-018-9421-5
   Tops M, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.00429
   Tuckett Anthony G, 2005, Contemp Nurse, V19, P75
   Ulrich Roger S., 1983, Behavior and the Natural Environment, P85, DOI [DOI 10.1007/978-1-4613-3539-94, DOI 10.1007/978-1-4613-3539-9_4]
   Valtchanov D, 2010, Physiological and affective responses to immersion in virtual reality: effects of nature and urban settings
   Valtchanov D, 2010, CYBERPSYCH BEH SOC N, V13, P503, DOI 10.1089/cyber.2009.0308
   Van Cappellen P, 2020, EMOTION, V20, P1332, DOI 10.1037/emo0000684
   Vidyarthi J., 2012, Sonic Cradle: Evoking Mindfulness through 'Immersive' Interaction Design
   von Lindern E., 2017, The handbook of salutogenesis, P181, DOI DOI 10.1007/978-3-319-04600-6_19
   Wang SY, 2022, PROCEEDINGS OF THE 15TH INTERNATIONAL CONFERENCE ON PERVASIVE TECHNOLOGIES RELATED TO ASSISTIVE ENVIRONMENTS, PETRA 2022, P325, DOI 10.1145/3529190.3534789
   Wang X., 2022, arXiv, DOI 10.48550/ARXIV.2209.14645
   Wilcox RR, 2011, J APPL STAT, V38, P1359, DOI 10.1080/02664763.2010.498507
NR 130
TC 0
Z9 0
U1 9
U2 9
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 53
DI 10.1007/s10055-023-00916-7
PG 20
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA JC4I7
UT WOS:001170940200002
OA hybrid
DA 2024-08-05
ER

PT J
AU Tovanche-Picon, H
   González-Trejo, J
   Flores-Abad, A
   Garcia-Terán, MA
   Mercado-Ravell, D
AF Tovanche-Picon, Hector
   Gonzalez-Trejo, Javier
   Flores-Abad, Angel
   Garcia-Teran, Miguel angel
   Mercado-Ravell, Diego
TI Real-time safe validation of autonomous landing in populated areas: from
   virtual environments to Robot-In-The-Loop
SO VIRTUAL REALITY
LA English
DT Article
DE Unmanned aerial vehicles; Visual-based landing; Virtual environments;
   Robot-In-The-Loop; Validation; Virtual reality
AB Safe autonomous landing for Unmanned Aerial Vehicles (UAVs) in populated areas is a crucial aspect for successful integration of UAVs in populated environments. Nonetheless, validating autonomous landing in real scenarios is a challenging task with a high risk of injuring people. In this work, we propose a framework for safe real-time and thorough evaluation of vision-based autonomous landing in populated scenarios, using photo-realistic virtual environments and physics-based simulation. The proposed evaluation pipeline includes the use of Unreal graphics engine coupled with AirSim for realistic drone simulation to evaluate landing strategies. Then, Software-/Hardware-In-The-Loop can be used to test beforehand the performance of the algorithms. The final validation stage consists in a Robot-In-The-Loop evaluation strategy where a real drone must perform autonomous landing maneuvers in real-time, with an avatar drone in a virtual environment mimicking its behavior, while the detection algorithms run in the virtual environment (virtual reality to the robot). This method determines the safe landing areas based on computer vision and convolutional neural networks to avoid colliding with people in static and dynamic scenarios. To test the robustness of the algorithms in adversary conditions, different urban-like environments were implemented, including moving agents and different weather conditions. We also propose different metrics to quantify the performance of the landing strategies, establishing a baseline for comparison with future works on this challenging task, and analyze them through several randomized iterations. The proposed approach allowed us to safely validate the autonomous landing strategies, providing an evaluation pipeline, and a benchmark for comparison. An extensive evaluation showed a 99% success rate in static scenarios and 87% in dynamic cases, demonstrating that the use of autonomous landing algorithms considerably prevents accidents involving humans, facilitating the integration of drones in human-populated spaces, which may help to unleash the full potential of drones in urban environments. Besides, this type of development helps to increase the safety of drone operations, which would advance drone flight regulations and allow their use in closer proximity to humans.
C1 [Tovanche-Picon, Hector; Garcia-Teran, Miguel angel] Autonomous Univ Ciudad Juarez, Dept Ind Engn & Mfg, Ciudad Juarez 32584, Chihuahua, Mexico.
   [Gonzalez-Trejo, Javier; Mercado-Ravell, Diego] Ctr Res Math CIMAT AC, Campus Zacatecas, Zacatecas 98160, Mexico.
   [Flores-Abad, Angel] Univ Texas El Paso, Aerosp & Mech Engn Dept, El Paso, TX 79968 USA.
   [Mercado-Ravell, Diego] CONAHCYT, Mexico City, Mexico.
C3 Universidad Autonoma de Ciudad Juarez; University of Texas System;
   University of Texas El Paso
RP Mercado-Ravell, D (corresponding author), Ctr Res Math CIMAT AC, Campus Zacatecas, Zacatecas 98160, Mexico.; Mercado-Ravell, D (corresponding author), CONAHCYT, Mexico City, Mexico.
EM hector.tovanche@uacj.mx; javier.gonzalez@cimat.mx; afloresabad@utep.edu;
   angel.garcia@uacj.mx; diego.mercado@cimat.mx
RI Mercado-Ravell, Diego/L-7020-2019
OI Mercado-Ravell, Diego/0000-0002-7416-3190; Flores-Abad,
   Angel/0000-0001-5519-7179; Tovanche-Picon, Hector
   Eduardo/0000-0001-5073-633X
CR Abdollahzadeh S, 2022, 2022 19TH CONFERENCE ON ROBOTS AND VISION (CRV 2022), P213, DOI 10.1109/CRV55824.2022.00035
   Administration F.A, 2021, Unmanned aircraft systems (UAS)
   Alam MS, 2021, EXPERT SYST APPL, V179, DOI 10.1016/j.eswa.2021.115091
   [Anonymous], 2021, SilverTm: city park environment collection LITE
   Ariante G, 2021, IEEE METROL AEROSPAC, P110, DOI [10.1109/MetroAeroSpace51421.2021.9511669, 10.1109/METROAEROSPACE51421.2021.9511669]
   Authority U.K.C.A, 2023, Rules and categories of drone flying
   Bassi E, 2019, INT CONF UNMAN AIRCR, P443, DOI 10.1109/ICUAS.2019.8798173
   Bruff D., 2005, Notes for Math, V20, P5
   Castellano G, 2020, IEEE ACCESS, V8, P64534, DOI 10.1109/ACCESS.2020.2984768
   Chatzikalymnios E, 2022, J INTELL ROBOT SYST, V104, DOI 10.1007/s10846-021-01544-6
   Delmerico J.A, 2022, CoRR:2202.01493
   Garcia-Pardo PJ, 2002, ROBOT AUTON SYST, V38, P19, DOI 10.1016/S0921-8890(01)00166-X
   González-Trejo J, 2021, IEEE ROBOT AUTOM LET, V6, P7902, DOI 10.1109/LRA.2021.3101861
   Gonzalez-Trejo JA, 2021, J INTELL ROBOT SYST, V102, DOI 10.1007/s10846-021-01380-8
   Guerin J, 2021, I C DEPENDABLE SYST, P55, DOI 10.1109/DSN-W52860.2021.00020
   Guerra W, 2019, IEEE INT C INT ROBOT, P6941, DOI [10.1109/iros40897.2019.8968116, 10.1109/IROS40897.2019.8968116]
   Heo SN, 2022, INTEL SERV ROBOT, V15, P289, DOI 10.1007/s11370-022-00416-8
   Jaderberg M, 2016, Arxiv, DOI arXiv:1506.02025
   Johnson A, 2005, IEEE INT CONF ROBOT, P3966
   Kakaletsis E, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3472288
   Khazetdinov A, 2021, IEEE INT SIBER CONF, DOI 10.1109/SIBCON50419.2021.9438855
   Kinahan J, 2021, arXiv
   Lee W, 2021, UBICOMP/ISWC '21 ADJUNCT: PROCEEDINGS OF THE 2021 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING AND PROCEEDINGS OF THE 2021 ACM INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, P48, DOI 10.1145/3460418.3479289
   Liu WZ, 2019, IEEE INT C INT ROBOT, P244, DOI [10.1109/iros40897.2019.8967852, 10.1109/IROS40897.2019.8967852]
   Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524
   Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624
   Marcu A, 2018, EUR C COMP VIS ECCV
   McTegg SJ, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14040822
   Mitroudas T, 2023, arXiv
   Mittal M, 2019, CoRR:1906.01304
   Nabavi Y, 2022, J INTELL ROBOT SYST, V105, DOI 10.1007/s10846-022-01655-8
   Orsolits H, 2022, IEEE INT SYMP M AU R, P56, DOI 10.1109/ISMAR-Adjunct57072.2022.00021
   Pinkam N, 2019, INTEL SERV ROBOT, V12, P393, DOI [10.1007/978-3-319-78452-6_17, 10.1007/s11370-019-00290-x]
   Saho K., 2017, Kalman Filters-Theory for Advanced Applications, P233
   Sanchez-Rodriguez JP, 2020, INTEL SERV ROBOT, V13, P273, DOI 10.1007/s11370-020-00312-z
   Sankaranarayanan S, 2018, PROC CVPR IEEE, P3752, DOI 10.1109/CVPR.2018.00395
   Shah S., 2018, FIELD SERVICE ROBOTI, P621, DOI [10.1007/978-3-319-67361-5_40, DOI 10.1007/978-3-319-67361-5_40]
   Nguyen TH, 2018, I C CONT AUTOMAT ROB, P1747, DOI 10.1109/ICARCV.2018.8581117
   Tzelepi M, 2021, IEEE T EM TOP COMP I, V5, P191, DOI 10.1109/TETCI.2019.2897815
   Wang JQ, 2023, LECT NOTES ELECTR EN, V1010, P2033, DOI 10.1007/978-981-99-0479-2_189
   Yang LJ, 2022, ISA T, V130, P610, DOI 10.1016/j.isatra.2022.04.005
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
   Zhu P., 2020, arXiv
NR 43
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 66
DI 10.1007/s10055-024-00965-6
PG 16
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA JS1F8
UT WOS:001175055500002
OA hybrid
DA 2024-08-05
ER

PT J
AU Valmorisco, S
   Raya, L
   Sanchez, A
AF Valmorisco, Sergio
   Raya, Laura
   Sanchez, Alberto
TI Enabling personalized VR experiences: a framework for real-time
   adaptation and recommendations in VR environments
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Recommendation systems; E-commerce; Virtual shopping
AB The personalization of user experiences through recommendation systems has been extensively explored in Internet applications, but this has yet to be fully addressed in Virtual Reality (VR) environments. The complexity of managing geometric 3D data, computational load, and natural interactions poses significant challenges in real-time adaptation in these immersive experiences. However, tailoring VR environments to individual user needs and interests holds promise for enhancing user experiences. In this paper, we present Virtual Reality Environment Adaptation through Recommendations (VR-EAR), a framework designed to address this challenge. VR-EAR employs customizable object metadata and a hybrid recommendation system modeling implicit user feedback in VR environments. We utilize VR optimization techniques to ensure efficient performance. To evaluate our framework, we designed a virtual store where product locations dynamically adjust based on user interactions. Our results demonstrate the effectiveness of VR-EAR in adapting and personalizing VR environments in real time. domains.
C1 [Valmorisco, Sergio; Raya, Laura] U Tad Univ Ctr Technol & Digital Art, C Playa Liencres 2 Bis, Madrid 28290, Spain.
   [Sanchez, Alberto] Univ Rey Juan Carlos, Comp Sci & Stat, C Tulipan S-N, Madrid 28933, Spain.
C3 Universidad Rey Juan Carlos
RP Sanchez, A (corresponding author), Univ Rey Juan Carlos, Comp Sci & Stat, C Tulipan S-N, Madrid 28933, Spain.
EM sergio.valmorisco@live.u-tad.com; laura.raya@u-tad.com;
   alberto.sanchez@urjc.es
FU Spanish Research Agency [PID2021-122392OB-I00]
FX This research was funded by the Spanish Research Agency, Grant Number
   PID2021-122392OB-I00
CR Adhanom IB, 2023, VIRTUAL REAL-LONDON, V27, P1481, DOI 10.1007/s10055-022-00738-z
   Aggarwal C.C., 2016, Recommender systems, Recommender Systems, V1st ed.
   Al-Jundi HA, 2022, VIRTUAL REAL-LONDON, V26, P1103, DOI 10.1007/s10055-021-00618-y
   [Anonymous], 2008, The movie database
   Ashtari N, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376722
   Augustine P, 2020, ADV COMPUT, V117, P79, DOI 10.1016/bs.adcom.2019.10.008
   Balakrishnan V, 2016, ASLIB J INFORM MANAG, V68, P76, DOI 10.1108/AJIM-07-2015-0106
   Banik R, 2017, Kaggle-the movies dataset
   Çano E, 2017, INTELL DATA ANAL, V21, P1487, DOI 10.3233/IDA-163209
   Chaudhary S, 2020, ADV INTELL SYST COMP, V1056, P551, DOI 10.1007/978-981-15-0199-9_47
   Contreras D, 2018, IEEE CONSUM ELECTR M, V7, P26, DOI 10.1109/MCE.2017.2728819
   David-John B., 2021, ACM S EYE TRACK RES
   Dede CJ, 2017, SMART COMPUT INTELL, P1, DOI 10.1007/978-981-10-5490-7_1
   Fan ZF, 2022, WSDM'22: PROCEEDINGS OF THE FIFTEENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P262, DOI 10.1145/3488560.3498478
   Guo G., 2013, Journal of Advanced Management Science, V1, DOI DOI 10.12720/JOAMS.1.1.61-65
   Huang JZ, 2022, Arxiv, DOI arXiv:2206.15021
   Isinkaye FO, 2015, EGYPT INFORM J, V16, P261, DOI 10.1016/j.eij.2015.06.005
   Jawaheer G, 2014, ACM T INTERACT INTEL, V4, DOI 10.1145/2512208
   Nguyen T, 2022, INT J ELECTRON COMM, V26, P90, DOI 10.1080/10864415.2021.2010006
   Khatri J, 2022, FRONT PSYCHOL, V13, DOI 10.3389/fpsyg.2022.752073
   Kritikos J, 2021, FRONT HUM NEUROSCI, V15, DOI 10.3389/fnhum.2021.596980
   Lau KW, 2019, VIRTUAL REAL-LONDON, V23, P255, DOI 10.1007/s10055-018-0362-3
   LaViola J.J., 2017, 3D user interfaces: theory and practice
   Lika B, 2014, EXPERT SYST APPL, V41, P2065, DOI 10.1016/j.eswa.2013.09.005
   Mayor J, 2022, IEEE T EMERG TOP COM, V10, P997, DOI 10.1109/TETC.2021.3062285
   Mayor J, 2021, IEEE T EMERG TOP COM, V9, P1542, DOI 10.1109/TETC.2019.2915287
   Microsoft, 2015, Attention spans
   Morales-Vega JC, 2024, VIRTUAL REAL-LONDON, V28, DOI 10.1007/s10055-024-00939-8
   Pfeiffer J, 2020, INFORM SYST RES, V31, P675, DOI 10.1287/isre.2019.0907
   Pietroszek K, 2018, chap. Raycasting in virtual reality
   Quintero L, 2023, User modeling for adaptive virtual reality experiences: personalization from behavioral and physiological time series
   Rane N., 2023, SSRN Electron J, DOI [10.2139/ssrn.4624199, DOI 10.2139/SSRN.4624199]
   Robillard M.P., 2014, Recomm Syst Softw Eng
   Shani G, 2011, RECOMMENDER SYSTEMS HANDBOOK, P257, DOI 10.1007/978-0-387-85820-3_8
   Shravani D, 2021, 2021 4TH IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR 2021), P119, DOI 10.1109/AIVR52153.2021.00028
   Sra M, 2023, IEEE COMPUT GRAPH, V43, P90, DOI 10.1109/MCG.2023.3252182
   Tasnim U, 2024, IEEE T VIS COMPUT GR, V30, P2368, DOI 10.1109/TVCG.2024.3372122
   Thorat PB., 2015, International Journal of Computer Applications, V110, P31, DOI DOI 10.5120/19308-0760
   Unity, 2022, Geometryutility
   University of Minnesota, 1992, Grouplens
   Weber S, 2021, FRONT PSYCHOL, V12, DOI 10.3389/fpsyg.2021.628298
   Zhao Q, 2018, 33RD ANNUAL ACM SYMPOSIUM ON APPLIED COMPUTING, P1331, DOI 10.1145/3167132.3167275
   Ziang Xiao, 2021, Proceedings of the ACM on Human-Computer Interaction, V5, DOI 10.1145/3479532
NR 43
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD JUN 26
PY 2024
VL 28
IS 3
AR 128
DI 10.1007/s10055-024-01020-0
PG 19
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA WM9B4
UT WOS:001255396900001
OA hybrid
DA 2024-08-05
ER

PT J
AU Serghides, CG
   Christoforides, G
   Iakovides, N
   Aristidou, A
AF Serghides, Christina-Georgia
   Christoforides, George
   Iakovides, Nikolas
   Aristidou, Andreas
TI Design and implementation of an interactive virtual library based on its
   physical counterpart
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual library; Digital replica; Virtual reality; Virtual humans;
   Digital library; Metaverse
ID REALITY
AB The rapid technological advancements and the widespread adoption of the internet have diminished the role of the physical library as a main information resource. As the Metaverse is evolving, a revolutionary change is anticipated in how social relationships are perceived, within an educational context. It is therefore necessary for libraries to upgrade the services they provide to keep in line with the technological trends and be a part of this virtual revolution. It is believed that the design and development of a Virtual Reality (VR) library can be the community and knowledge hub the society needs. In this paper, the process of creating a partially digital replica of the Limassol Municipal University Library, a landmark for the city of Limassol, is examined by using photogrammetry and 3D modelling. A 3D platform was developed, where users have the perception that they are experiencing the actual library. To that end, a perceptual study was conducted, to understand the current usage of physical libraries, examine the users' experience in VR, and identify the requirements and expectations in the development of a virtual library counterpart. Following the suggestions and observations from the perceptual study, five key scenarios were implemented that demonstrate the potential use of a virtual library. This work incorporates the fundamental VR attributes, such as immersiveness, realism, user interactivity and feedback as well as other features, such as animated NPCs, 3D audio, ray-casting and GUIs, that significantly augment the overall VR library user experience, presence as well as navigation autonomy. The main effort of this project was to produce a VR representation of an existing physical library, integrated with its key services, as a proof-of-concept, with emphasis on easy 24/7 access, functionality, and interactivity. The above attributes differentiate this work from existing studies. A detailed user evaluation study was conducted upon completion of the final VR library implementation, which firmly confirmed all its key attributes and future viability.
C1 [Serghides, Christina-Georgia; Christoforides, George; Iakovides, Nikolas; Aristidou, Andreas] Univ Cyprus, Comp Sci, 75 Kallipoleos St, CY-1678 Nicosia, Cyprus.
   [Serghides, Christina-Georgia; Aristidou, Andreas] CYENS Ctr Excellence, 23 Dimarchou Lellou Demetriadi, CY-1016 Nicosia, Cyprus.
C3 University of Cyprus
RP Aristidou, A (corresponding author), Univ Cyprus, Comp Sci, 75 Kallipoleos St, CY-1678 Nicosia, Cyprus.; Aristidou, A (corresponding author), CYENS Ctr Excellence, 23 Dimarchou Lellou Demetriadi, CY-1016 Nicosia, Cyprus.
EM serghides.christina-georgia@ucy.ac.cy; christoforides.george@ucy.ac.cy;
   niakov03@ucy.ac.cy; a.aristidou@ieee.org
OI Aristidou, Andreas/0000-0001-7754-0791; Serghides,
   Christina-Georgia/0009-0007-1409-6567
FU University of Cyprus
FX We would like to express our acknowledgements to the staff of the
   University of Cyprus Library (Elena Diomidi-Parpouna, Anna Mousena,
   Vasoula Philippou, Angeliki Matsa, Stavroula Pitta); the Limassol
   Municipal University Library for their permission to scan the building
   (Georgia Kontolemi); Melios Agathangelou for his help with the scanning
   of the building using a drone; and Andreas Andreou for his fruitful
   discussions about the Unity implementations. Finally, we would like to
   thank all the participants of the perceptual and evaluation studies.
CR Aabo S, 2010, LIBR INFORM SCI RES, V32, P16, DOI 10.1016/j.lisr.2009.07.008
   Andreou TA, 2009, Limassol: A Flashback Memory
   Aristidou A, 2021, P EG WORKSH GRAPH CU, DOI [10.2312/gch.20211405, DOI 10.2312/GCH.20211405]
   Boda I, 2014, P 9 INT C APPL INF I, V1, P103
   Bonde S, 2017, SPECULUM, V92, pS288, DOI 10.1086/694169
   Bowman DA, 2007, COMPUTER, V40, P36, DOI 10.1109/MC.2007.257
   Bronack S., 2008, International Journal Of Teaching Learning In Higher Education, V20, P59
   Brosius Maria., 2003, Ancient Archives and Archival Traditions: Concepts of Record-Keeping in the Ancient World
   Das Neves F. A., 2000, ACM 2000. Digital Libraries. Proceedings of the Fifth ACM Conference on Digital Libraries, P103, DOI 10.1145/336597.336648
   Dhanda A, 2019, INT ARCH PHOTOGRAMM, V42-2, P305, DOI 10.5194/isprs-archives-XLII-2-W9-305-2019
   El Saddik A, 2018, IEEE MULTIMEDIA, V25, P87, DOI 10.1109/MMUL.2018.023121167
   Gabellone F, 2020, IMEKO TC INT C METR, P232
   Gilányi A, 2013, INT CONF COGN INFO, P563, DOI 10.1109/CogInfoCom.2013.6719311
   Grant CR, 2019, P CHARL LIB C
   Grieves M., 2015, White Paper
   Hamilton D, 2021, J COMPUT EDUC, V8, P1, DOI 10.1007/s40692-020-00169-2
   Holappa H, 2018, 17TH INTERNATIONAL CONFERENCE ON MOBILE AND UBIQUITOUS MULTIMEDIA (MUM 2018), P521, DOI 10.1145/3282894.3289719
   Iakovides N., 2022, P 2022 INT C INT MED, P1, DOI [10.1109/IMET54801.2022.9929598, DOI 10.1109/IMET54801.2022.9929598]
   Indrashah I, 2023, Ar, vr, and immersive technologies: The new mode of learning and the key enablers in enhancing library services
   Kingsland Kaitlyn., 2020, Digital Applications in Archaeology and Cultural Heritage, V18, DOI DOI 10.1016/J.DAACH.2020.E00157
   Kourtesis P., 2023, Virt. Worlds, V2, P16, DOI DOI 10.3390/VIRTUALWORLDS2010002
   Kyriakou P, 2013, Digital Heritage, V1, P443
   Loddo M, 2022, IFLA J-INT FED LIBR, V48, P332, DOI 10.1177/03400352211023080
   Marshall B, 2023, VIRTUAL REAL-LONDON, V27, P2397, DOI 10.1007/s10055-023-00807-x
   Massis B, 2015, NEW LIB WORLD, V116, P796, DOI 10.1108/NLW-08-2015-0054
   Megan F, 2020, Information Technology and Libraries, V39
   Muangasame K, 2023, WORLDW HOSP TOUR THE, V15, P8, DOI 10.1108/WHATT-08-2022-0096
   Mystakidis S., 2022, ENCYCLOPEDIA, V2, P486, DOI [https://doi.org/10.3390/encyclopedia2010031, 10.3390/encyclopedia2010031, DOI 10.3390/ENCYCLOPEDIA2010031]
   Özyesil O, 2017, ACTA NUMER, V26, P305, DOI 10.1017/S096249291700006X
   Parsinejad H, 2021, Body, Space & Technology, V20
   Pölönen M, 2012, DISPLAYS, V33, P157, DOI 10.1016/j.displa.2012.06.002
   Ponton JL, 2023, VIRTUAL REAL-LONDON, V27, P2541, DOI 10.1007/s10055-023-00821-z
   Pouke M, 2018, 17TH INTERNATIONAL CONFERENCE ON MOBILE AND UBIQUITOUS MULTIMEDIA (MUM 2018), P227, DOI 10.1145/3282894.3282927
   Raggett D, 1995, P INT SOC EUR NETW, P242
   Rizvic S, 2019, P EG WORKSH GRAPH CU
   Sanders ME., 2008, STEM, STEM education
   Slater M, 2018, BRIT J PSYCHOL, V109, P431, DOI 10.1111/bjop.12305
   Su CJ, 1998, COMPUT IND ENG, V35, P615, DOI 10.1016/S0360-8352(98)00172-7
   Sylaiou S, 2009, J CULT HERIT, V10, P520, DOI 10.1016/j.culher.2009.03.003
   Thompson M.M., 1966, Manual of Photogrammetry, V1
   Turco ML, 2020, J ARCHAEOL SCI-REP, V34, DOI 10.1016/j.jasrep.2020.102639
   Vosinakis S, 2011, Rethinking technology in Museums
   Witmer BG, 2005, PRESENCE-TELEOP VIRT, V14, P298, DOI 10.1162/105474605323384654
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   Yastikli N, 2007, J CULT HERIT, V8, P423, DOI 10.1016/j.culher.2007.06.003
   Zotos S, 2022, IEEE Computer Graphics and Applications
NR 46
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD JUN 13
PY 2024
VL 28
IS 3
AR 124
DI 10.1007/s10055-024-01023-x
PG 22
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA UP4G7
UT WOS:001249242400001
OA hybrid
DA 2024-08-05
ER

PT J
AU Lee, JU
   Hwang, S
   Kim, K
   Kim, S
AF Lee, Jieun
   Hwang, Seokhyun
   Kim, Kyunghwan
   Kim, Seungjun
TI Evaluation of visual, auditory, and olfactory stimulus-based attractors
   for intermittent reorientation in virtual reality locomotion
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Redirected walking; Attractor; Auditory; Olfactory;
   Multi-modality
ID WALKING
AB In virtual reality, redirected walking (RDW) enables users to stay within the tracking area while feeling that they are traveling in a virtual space that is larger than the physical space. RDW uses a visual attractor to the user's sight and scene manipulation for intermittent reorientation. However, repeated usage can hinder the virtual world immersion and weaken the reorientation performance. In this study, we propose using sounds and smells as alternative stimuli to draw the user's attention implicitly and sustain the attractor's performance for intermittent reorientation. To achieve this, we integrated visual, auditory, and olfactory attractors into an all-in-one stimulation system. Experiments revealed that the auditory attractor caused the fastest reorientation, the olfactory attractor induced the widest angular difference, and the attractor with the combined auditory and olfactory stimuli induced the largest angular speed, keeping users from noticing the manipulation. The findings demonstrate the potential of nonvisual attractors to reorient users in situations requiring intermittent reorientation.
C1 [Lee, Jieun; Hwang, Seokhyun; Kim, Seungjun] GIST, Inst Integrated Technol, 123 Chem Dan Gwa Gi Ro, Gwangju 61005, South Korea.
   [Kim, Kyunghwan] GIST, 123 Chem Dan Gwa Gi Ro, Gwangju 61005, South Korea.
C3 Gwangju Institute of Science & Technology (GIST); Gwangju Institute of
   Science & Technology (GIST)
RP Kim, S (corresponding author), GIST, Inst Integrated Technol, 123 Chem Dan Gwa Gi Ro, Gwangju 61005, South Korea.
EM jieunlee@gm.gist.ac.kr; anoldhsh@gm.gist.ac.kr;
   kyunghwankim@gm.gist.ac.kr; seungjun@gist.ac.kr
RI Hwang, Seokhyun/KYQ-3298-2024
OI Hwang, Seokhyun/0000-0001-5244-017X; Kim, Kyunghwan/0000-0002-6048-2713;
   Kim, SeungJun/0000-0003-0470-2483
FU Korea Institute of Energy Technology Evaluation and Planning(KETEP);
   Ministry of Trade, Industry & Energy(MOTIE) of the Republic of Korea
   [20204010600340]; GIST
FX This work was partly supported by the Korea Institute of Energy
   Technology Evaluation and Planning(KETEP) and the Ministry of Trade,
   Industry & Energy(MOTIE) of the Republic of Korea (No. 20204010600340)
   and the GIST-MIT Research Collaboration grant funded by the GIST in
   2024.
CR Akoglu H, 2018, TURK J EMERG MED, V18, P91, DOI 10.1016/j.tjem.2018.08.001
   Amores J, 2018, INT CONF WEARAB IMPL, P98, DOI 10.1109/BSN.2018.8329668
   Baus O, 2017, VIRTUAL REAL-LONDON, V21, P59, DOI 10.1007/s10055-016-0299-3
   Bodnar A, 2004, P 6 INT C MULTIMODAL, P183, DOI [10.1145/1027933.1027965, DOI 10.1145/1027933.1027965]
   Bruder G., 2009, Reorientation during body turns, P145
   Cao C, 2020, INT SYM MIX AUGMENT, P600, DOI 10.1109/ISMAR50242.2020.00087
   Cardoso JCS, 2019, COMPUT GRAPH-UK, V85, P55, DOI 10.1016/j.cag.2019.09.005
   Chen H., 2017, P 21 ACM SIGGRAPH S, P1, DOI [10.1145/3023368.3036844, DOI 10.1145/3023368.3036844]
   Chen HW, 2017, CGI'17: PROCEEDINGS OF THE COMPUTER GRAPHICS INTERNATIONAL CONFERENCE, DOI 10.1145/3095140.3095162
   Cools R, 2019, ACM CONFERENCE ON SPATIAL USER INTERACTION (SUI 2019), DOI 10.1145/3357251.3357580
   Dao E., 2021, P 2021 CHI C HUM FAC, DOI DOI 10.1145/3411764.3445435
   Dichgans J., 1978, HDB SENSORY PHYSL, VVIII., P755
   Engel David., 2008, P ACM S VIRTUAL REAL, P157, DOI [DOI 10.1145/1450579.1450612, 10.1145/1450579.1450612.51E., DOI 10.1145/1450579.1450612.51E]
   Fan LW, 2023, IEEE T VIS COMPUT GR, V29, P4104, DOI 10.1109/TVCG.2022.3179269
   Feigl T, 2017, VRST'17: PROCEEDINGS OF THE 23RD ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, DOI 10.1145/3139131.3141205
   Freiwald JP, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376574
   Gao PZ, 2020, INT SYM MIX AUGMENT, P639, DOI 10.1109/ISMAR50242.2020.00092
   Hamburger K, 2019, COGNITIVE SCI, V43, DOI 10.1111/cogs.12798
   Hodgson E, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/2043603.2043604
   HTC, 2022, HTCVive
   Interrante V, 2006, P IEEE VIRT REAL ANN, P3, DOI 10.1109/VR.2006.52
   Jacobs LF, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0129387
   Junker A, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P358, DOI 10.1109/VRW52623.2021.00071
   Kato S, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P761, DOI 10.1109/VR.2018.8445841
   Kim HK, 2018, APPL ERGON, V69, P66, DOI 10.1016/j.apergo.2017.12.016
   Langbehn E, 2017, P IEEE VIRT REAL ANN, P449, DOI 10.1109/VR.2017.7892373
   Langbehn E, 2017, IEEE T VIS COMPUT GR, V23, P1349, DOI 10.1109/TVCG.2017.2657220
   Lee J, 2022, EXTENDED ABSTRACTS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2022, DOI 10.1145/3491101.3519719
   Lei YX, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3502033
   Maggioni E, 2020, ACM T COMPUT-HUM INT, V27, DOI 10.1145/3402449
   McCullough M, 2016, SAP 2015: ACM SIGGRAPH SYMPOSIUM ON APPLIED PERCEPTION, P107, DOI 10.1145/2804408.2804416
   Moessnang C, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0029614
   Narciso D, 2020, ACM T APPL PERCEPT, V17, DOI 10.1145/3380903
   Nguyen A., 2020, P 26 ACM S VIRT REAL, P1, DOI [10.1145/3385956.3418950, DOI 10.1145/3385956.3418950]
   Nilsson NC, 2018, IEEE COMPUT GRAPH, V38, P44, DOI 10.1109/MCG.2018.111125628
   Nilsson NC, 2016, P IEEE VIRT REAL ANN, P241, DOI 10.1109/VR.2016.7504743
   Nogalski M., 2016, P SOUND MUS COMP C, V16, P17
   Patnaik B, 2019, IEEE T VIS COMPUT GR, V25, P726, DOI 10.1109/TVCG.2018.2865237
   Peck TC, 2012, IEEE T VIS COMPUT GR, V18, P1053, DOI 10.1109/TVCG.2011.289
   Peck TC, 2011, P IEEE VIRT REAL ANN, P55, DOI 10.1109/VR.2011.5759437
   Peck TC, 2010, P IEEE VIRT REAL ANN, P35, DOI 10.1109/VR.2010.5444816
   Peck TC, 2009, IEEE T VIS COMPUT GR, V15, P383, DOI 10.1109/TVCG.2008.191
   Porter J, 2007, NAT NEUROSCI, V10, P27, DOI 10.1038/nn1819
   Qi Sun, 2020, Real VR - Immersive Digital Reality: How to Import the Real World into Head-Mounted Immersive Displays. Lecture Notes in Computer Science (LNCS 11900), P285, DOI 10.1007/978-3-030-41816-8_12
   Ranasinghe N, 2019, INT J HUM-COMPUT ST, V125, P7, DOI 10.1016/j.ijhcs.2018.11.011
   Razzaque S., 2002, Virtual Environments 2002. Eurographics Workshop Proceedings, P123
   Razzaque Z., 2001, EUROGRAPHICS 2001 SH, P289, DOI 10.2312/egs.20011036
   Rewkowski N, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P395, DOI [10.1109/vr.2019.8798286, 10.1109/VR.2019.8798286]
   Rothacher Y, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-36035-6
   Serafin S, 2013, P IEEE VIRT REAL ANN, P161, DOI 10.1109/VR.2013.6549412
   Simons DJ, 1999, PERCEPTION, V28, P1059, DOI 10.1068/p2952
   Slater M., 1994, Presence: teleoperators virtual environments, V3, P130, DOI [10.1162/pres.1994.3.2.130, DOI 10.1162/PRES.1994.3.2.130, 10.1162/pres.1994.3.2.1302]
   Sra M, 2018, DIS 2018: PROCEEDINGS OF THE 2018 DESIGNING INTERACTIVE SYSTEMS CONFERENCE, P59, DOI 10.1145/3196709.3196792
   Steinicke F., 2008, Proceedings of the ACM Symposium on Virtual Reality Software and Technology (VRST), P149, DOI DOI 10.1145/1450579.1450611
   Steinicke F, 2010, IEEE T VIS COMPUT GR, V16, P17, DOI 10.1109/TVCG.2009.62
   Suma EA, 2012, IEEE VIRTUAL REALITY CONFERENCE 2012 PROCEEDINGS, P43, DOI 10.1109/VR.2012.6180877
   Suma EA, 2011, P IEEE VIRT REAL ANN, P159, DOI 10.1109/VR.2011.5759455
   Tsai SE, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P279, DOI 10.1109/VR50410.2021.00050
   Unity, 2022, Unity
   Williams B, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P41
   Williams B, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/2010325.2010329
   Williams NL, 2019, IEEE T VIS COMPUT GR, V25, P3158, DOI 10.1109/TVCG.2019.2932213
   Wilson PT, 2016, PROCEEDINGS VRCAI 2016: 15TH ACM SIGGRAPH CONFERENCE ON VIRTUAL-REALITY CONTINUUM AND ITS APPLICATIONS IN INDUSTRY, P243, DOI 10.1145/3013971.3014010
   Wu YL, 2020, P NATL ACAD SCI USA, V117, P16065, DOI 10.1073/pnas.2004642117
NR 64
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD APR 26
PY 2024
VL 28
IS 2
AR 104
DI 10.1007/s10055-024-00997-y
PG 18
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA OU2G4
UT WOS:001209723600001
OA hybrid
DA 2024-08-05
ER

PT J
AU Sarakatsanos, O
   Papazoglou-Chalikias, A
   Boikou, M
   Chatzilari, E
   Jauk, M
   Hafliger, U
   Nikolopoulos, S
   Kompatsiaris, I
AF Sarakatsanos, Orestis
   Papazoglou-Chalikias, Anastasios
   Boikou, Machi
   Chatzilari, Elisavet
   Jauk, Michaela
   Hafliger, Ursina
   Nikolopoulos, Spiros
   Kompatsiaris, Ioannis
TI VR Designer: enhancing fashion showcases through immersive virtual
   garment fitting
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; 3D fashion design; Digital garment design; Avatar-based
   garment simulation; Avatar-garment fitting; Garment visualization;
   Virtual garment fitting; Unity; XR
AB This paper introduces a Virtual Reality (VR) application tailored for fashion designers and retailers, transcending traditional garment design and demonstration boundaries by presenting an immersive digital garment showcase within a captivating VR environment. Simulating a virtual retail store, designers navigate freely, selecting from an array of avatar-garment combinations and exploring garments from diverse perspectives. This immersive experience offers designers a precise representation of the final product's aesthetics, fit, and functionality on the human body. Our application can be considered as a pre-manufacturing layer, that empowers designers and retailers with a precise understanding of how the actual garment will look and behave. Evaluation involved comprehensive feedback from both professional and undergraduate fashion designers, gathered through usability testing sessions.
C1 [Sarakatsanos, Orestis; Papazoglou-Chalikias, Anastasios; Boikou, Machi; Chatzilari, Elisavet; Nikolopoulos, Spiros; Kompatsiaris, Ioannis] Ctr Res & Technol Hellas, Thessaloniki, Greece.
   [Jauk, Michaela; Hafliger, Ursina] ODLO, Hunenberg, Switzerland.
C3 Centre for Research & Technology Hellas
RP Sarakatsanos, O (corresponding author), Ctr Res & Technol Hellas, Thessaloniki, Greece.
EM orestiss@iti.gr; tpapazoglou@iti.gr; boikou@iti.gr;
   echatzilari@gmail.com; michaela.jauk@odlo.com;
   ursina.haefliger@odlo.com; nikolopo@iti.gr; ikom@iti.gr
FU Centre for Research & Technology Hellas (CERTH)
FX No Statement Available
CR Adikari SB, 2020, ADV HUM-COMPUT INTER, V2020, DOI 10.1155/2020/1314598
   Ahmadi M, 2021, metamorphosis project
   [Anonymous], 2022, Snapchat boosts ar try-on tools: Farfetch, prada dive in
   [Anonymous], 2022, PICTOFiT app
   Browzwear, 2022, New animation and smart pattern tools added to vstitcher
   CLO, 2022, Clo: 3d fashion design software
   DRESSX, 2022, First multi brand retailer of digital fashion clothing
   Efstratios G, 2005, True augmented reality virtual characters for digital cultural heritage applications
   Goodstyle app, 2022, Virtual dressing room app
   Hafliger U, 2021, etryon-the user requirements
   Huang SX, 2022, WIREL COMMUN MOB COM, V2022, DOI 10.1155/2022/5835026
   Kumar T.S., 2021, J. Innov. Image Process., V3, P144, DOI 10.36548/jiip.2021.2.006
   Marvelous Designer, 2022, making, editing and reusing 3d clothes
   Mixamo, 2023, Mixamo: Animate 3d characters for games, film, and more
   Morotti E, 2022, VIRTUAL REAL-LONDON, V26, P871, DOI 10.1007/s10055-021-00602-6
   Morotti E, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P338, DOI [10.1109/VRW50115.2020.00074, 10.1109/VRW50115.2020.0-202]
   Nafz R., 2022, Commun Dev Assembling Textile Products, V3, P79, DOI [10.25367/cdatp.2022.3.p79-89, DOI 10.25367/CDATP.2022.3.P79-89]
   Nasser N, 2021, IEEE INT SYM MULTIM, P204, DOI 10.1109/ISM52913.2021.00040
   Oikonomou K, 2021, INT SYM MIX AUGMENT, P46, DOI 10.1109/ISMAR-Adjunct54149.2021.00019
   Optitex, 2022, Optitex: Fashion design software
   Papadopoulos SI, 2022, INT J MULTIMED INF R, V11, P717, DOI 10.1007/s13735-022-00262-5
   Ricci M, 2022, IEEE INT SYMP M AU R, P938, DOI 10.1109/ISMAR-Adjunct57072.2022.00210
   Sarakatsanos O, 2021, INT SYM MIX AUGMENT, P40, DOI 10.1109/ISMAR-Adjunct54149.2021.00018
   Särmäkari N, 2023, FASH THEORY, V27, P85, DOI 10.1080/1362704X.2021.1981657
   Spaji J., 2022, 10 INT C MASS CUST P, P21
   Sumi, 2023, Software usability measurement inventory
   The Fabricant, 2022, Digital fashion house
   TheModern Mirror, 2022, creating 3d virtual fit solutions for the luxury fashion industry
   TUKA3D, 2022, 3d virtual sampling software with motion simulation for fit analysis, design development, and photorealistic rendering
   Unity Technologies, 2022, Unity real-time development platform
   Unreal Engine, 2022, The world's most open and advanced real-time 3d creation tool
   Wang YQ, 2020, J CHIN ECON FOREIGN, V13, P37, DOI 10.1108/JCEFTS-01-2020-0003
   Werdayani D, 2021, IOP CONF SER-MAT SCI, V1098, DOI 10.1088/1757-899X/1098/2/022110
   Zhang TT, 2019, INTERNET RES, V29, P529, DOI 10.1108/IntR-12-2017-0540
NR 34
TC 1
Z9 1
U1 18
U2 18
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR 8
PY 2024
VL 28
IS 2
AR 70
DI 10.1007/s10055-024-00945-w
PG 16
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA LG7F1
UT WOS:001185691200004
OA hybrid
DA 2024-08-05
ER

PT J
AU Kosonogov, V
   Hajiyeva, G
   Zyabreva, I
AF Kosonogov, Vladimir
   Hajiyeva, Gullu
   Zyabreva, Irina
TI <i>PanoEmo</i>, a set of affective 360-degree panoramas: a
   psychophysiological study
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Emotion; Valence; Arousal; Database; Psychophysiology
ID VIRTUAL-REALITY; ELECTROMYOGRAPHIC ACTIVITY; EMOTIONS; ENVIRONMENTS
AB There is a significant increase in the use of virtual reality in scientific experiments in the fields of ergonomics, education, and psychology among others. Many researchers successfully provoked different affective states in participants in order to capture physiological correlates or apply psychotherapeutic techniques. All these studies employed different stimuli, like 3D pictures, computer-built graphics and 180-or 360-degree panoramic photographs. In an attempt to begin the standardi-zation of the measurements, we propose PanoEmo, a set of affective 360-degree photographic panoramas. Our aim was to explore the emotional reactions in response to PanoEmo, based on self-report scales, somatic, and vegetative affective indices. Fifty-five participants watched 45 photographic panoramas of different valence during 20 s without a special task. Self-reported valence correlated positively to zygomaticus major and negatively to corrugator supercilii electromyographic activity. Zygomaticus major also correlated positively to arousal. Respiratory rate correlated negatively to valence. Pleasant panoramas provoked a slower respiratory rate, while unpleasant ones increased it. Skin conductance response was positively related to self-reported arousal. Unexpectedly, heart rate did not correlate to self-report measures during the whole epoch, but it correlated positively around 5 s after the panorama onset. As a limitation, we should mention that our database contains a much higher number of positive panoramas. Although we expected the equal number of negative, neutral, and positive panoramas, we found a prevalence of positive ones. Nonetheless, subsequent studies should enrich the set with more nega-tive panoramas to get a homogeneously distributed database.
C1 [Kosonogov, Vladimir; Hajiyeva, Gullu; Zyabreva, Irina] HSE Univ, Inst Cognit Neurosci, Myasnitskaya 20, Moscow 101000, Russia.
C3 HSE University (National Research University Higher School of Economics)
RP Kosonogov, V (corresponding author), HSE Univ, Inst Cognit Neurosci, Myasnitskaya 20, Moscow 101000, Russia.
EM vkosonogov@hse.ru
RI Kosonogov, Vladimir/R-8117-2016
OI Kosonogov, Vladimir/0000-0002-0469-4818
FU Academic Fund Program at HSE University
FX No Statement Available
CR Aluja A, 2015, PERS INDIV DIFFER, V77, P143, DOI 10.1016/j.paid.2014.12.058
   [Anonymous], 2013, A Guide for Analysing Electrodermal Activity (EDA) Skin Conductance Responses (SCRs) for Psychological Experiments
   Astor PJ, 2013, J NEUROSCI PSYCHOL E, V6, P14, DOI 10.1037/a0031406
   Baker C, 2021, Emotional responses in virtual reality environments, P163
   Balan O, 2020, 27 EUR C INF SYST
   Bastiaansen M, 2022, J HOSP TOUR RES, V46, P29, DOI 10.1177/1096348020944436
   Bates D., 2009, Package 'lme4'CRAN
   BENJAMINI Y, 1995, J R STAT SOC B, V57, P289, DOI 10.1111/j.2517-6161.1995.tb02031.x
   Bradley MM, 2001, EMOTION, V1, P276, DOI 10.1037//1528-3542.1.3.276
   Cacioppo J.T., 1997, Annual Review of Gerontology and Geriatrics, V17, P27
   CACIOPPO JT, 1986, J PERS SOC PSYCHOL, V50, P260, DOI 10.1037/0022-3514.50.2.260
   CACIOPPO JT, 1981, AM PSYCHOL, V36, P441, DOI 10.1037/0003-066X.36.5.441
   Cameirao MS, 2016, J NEUROENG REHABIL, V13, DOI 10.1186/s12984-016-0175-0
   Camm AJ, 1996, EUR HEART J, V17, P354
   Cebeci B, 2019, COMPUT ANIMAT VIRT W, V30, DOI 10.1002/cav.1893
   Centifanti LCM, 2022, J PSYCHOPATHOL BEHAV, V44, P39, DOI 10.1007/s10862-021-09943-7
   Chirico A, 2018, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.02351
   Cipresso P, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.02086
   de Groot JHB, 2018, PSYCHONEUROENDOCRINO, V98, P177, DOI 10.1016/j.psyneuen.2018.08.005
   Diemer J, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00026
   Dignath D, 2019, INT J PSYCHOPHYSIOL, V146, P208, DOI 10.1016/j.ijpsycho.2019.10.003
   Dimberg U., 1988, J PSYCHOPHYSIOL, V2, P277
   Dimberg U, 2012, PSYCH J, V1, P118, DOI 10.1002/pchj.4
   Egan D, 2016, 2016 EIGHTH INTERNATIONAL CONFERENCE ON QUALITY OF MULTIMEDIA EXPERIENCE (QOMEX)
   Ekman P., 1975, Unmasking the Face: A Guide to Recognizing Emotions from Facial Expressions
   Felnhofer A, 2015, INT J HUM-COMPUT ST, V82, P48, DOI 10.1016/j.ijhcs.2015.05.004
   FRIDLUND AJ, 1986, PSYCHOPHYSIOLOGY, V23, P567, DOI 10.1111/j.1469-8986.1986.tb00676.x
   Gilpin G, 2021, BRAIN COGNITION, V152, DOI [10.1016/j.bc.2021.105734, 10.1016/j.bandc.2021.105734]
   Golland Y, 2018, BIOL PSYCHOL, V139, P47, DOI 10.1016/j.biopsycho.2018.10.003
   Guimard Q, 2022, MMSYS 2022, DOI [10.1145/3524273.3532895ff.ffhal-03710323f, DOI 10.1145/3524273.3532895FF.FFHAL-03710323F]
   Herrero R, 2014, CYBERPSYCH BEH SOC N, V17, P379, DOI 10.1089/cyber.2014.0052
   Hidaka K, 2017, INT C CONTR AUTOMAT, P325, DOI 10.23919/ICCAS.2017.8204459
   HODES RL, 1985, PSYCHOPHYSIOLOGY, V22, P545, DOI 10.1111/j.1469-8986.1985.tb01649.x
   Homma I, 2008, EXP PHYSIOL, V93, P1011, DOI 10.1113/expphysiol.2008.042424
   Huang XT, 2021, J HOSP TOUR TECHNOL, V12, P777, DOI 10.1108/JHTT-07-2020-0186
   Ito TA, 1998, J PERS SOC PSYCHOL, V75, P887, DOI 10.1037/0022-3514.75.4.887
   Knaust T, 2022, VIRTUAL REAL-LONDON, V26, P925, DOI 10.1007/s10055-021-00595-2
   Kop WJ, 2011, BIOL PSYCHOL, V86, P230, DOI 10.1016/j.biopsycho.2010.12.003
   Kosonogov Vladimir, 2020, Psicologia, V34, P171, DOI 10.17575/psicologia.v34i2.1608
   Künecke J, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0084053
   LANG PJ, 1993, PSYCHOPHYSIOLOGY, V30, P261, DOI 10.1111/j.1469-8986.1993.tb03352.x
   Larsen JT, 2003, PSYCHOPHYSIOLOGY, V40, P776, DOI 10.1111/1469-8986.00078
   Li BJ, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.02116
   Ludecke D., 2022, CRAN
   Higuera-Trujillo JL, 2017, APPL ERGON, V65, P398, DOI 10.1016/j.apergo.2017.05.006
   Marchiori E, 2018, INF TECHNOL TOUR, V18, P133, DOI 10.1007/s40558-018-0104-0
   Marin Morales J, 2020, Modelling human emotions using immersive virtual reality, physiological signals and behavioural responses
   Marín-Morales J, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0254098
   Martin GN, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.02298
   Martínez-Rodrigo A, 2017, LECT NOTES COMPUT SC, V10586, P766, DOI 10.1007/978-3-319-67585-5_74
   Masaoka Y, 2001, RESP PHYSIOL, V128, P171, DOI 10.1016/S0034-5687(01)00278-X
   Massetti T, 2018, J CENT NERV SYST DIS, V10, DOI 10.1177/1179573518813541
   Mavridou I, 2021, Affective state recognition in virtual reality from electromyography and photoplethysmography using head-mounted wearable sensors, P369
   Medved V., 2021, Measurement and analysis of human locomotion, DOI [10.1007/978-3-030-79685-3, DOI 10.1007/978-3-030-79685-3]
   Meuleman B, 2021, IEEE T AFFECT COMPUT, V12, P189, DOI 10.1109/TAFFC.2018.2864730
   Mikels JA, 2005, BEHAV RES METHODS, V37, P626, DOI 10.3758/BF03192732
   Mohammed AR, 2021, INT J PSYCHOPHYSIOL, V167, P30, DOI 10.1016/j.ijpsycho.2021.06.007
   Munafo J, 2017, EXP BRAIN RES, V235, P889, DOI 10.1007/s00221-016-4846-7
   Pallavicini F, 2020, JMIR SERIOUS GAMES, V8, DOI 10.2196/15635
   Sánchez-Navarro J, 2008, SPAN J PSYCHOL, V11, P16, DOI 10.1017/S1138741600004078
   Peira N, 2014, INT J PSYCHOPHYSIOL, V91, P225, DOI 10.1016/j.ijpsycho.2013.12.008
   Peperkorn HM, 2013, STUD HEALTH TECHNOL, V191, P75, DOI 10.3233/978-1-61499-282-0-75
   Philip L, 2018, I-PERCEPTION, V9, DOI 10.1177/2041669518786527
   Riva G, 2007, CYBERPSYCHOL BEHAV, V10, P45, DOI 10.1089/cpb.2006.9993
   Rockstroh C, 2019, INT J HUM-COMPUT ST, V130, P209, DOI 10.1016/j.ijhcs.2019.06.011
   Sánchez-Navarro JP, 2012, PSYCHOPHYSIOLOGY, V49, P1601, DOI 10.1111/j.1469-8986.2012.01475.x
   Shiota MN, 2011, EMOTION, V11, P1368, DOI 10.1037/a0024278
   Slobounov SM, 2015, INT J PSYCHOPHYSIOL, V95, P254, DOI 10.1016/j.ijpsycho.2014.11.003
   Susindar Sahinya, 2019, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V63, P252, DOI 10.1177/1071181319631509
   't Hart B, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.00613
   Tabbaa L, 2021, PROC ACM INTERACT MO, V5, DOI 10.1145/3495002
   Tan JW, 2012, J AMB INTEL HUM COMP, V3, P3, DOI 10.1007/s12652-011-0084-9
   Tian F, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0256211
   Toet A, 2020, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.552587
   VENABLES PH, 1980, BIOL PSYCHOL, V10, P1, DOI 10.1016/0301-0511(80)90002-2
   Wilcox Laurie M., 2006, ACM Trans. on Perception, V3, P412, DOI [10.1145/1190036.1190041, DOI 10.1145/1190036.1190041]
   Xue T, P 2021 CHI C HUM FAC, P1
   Xue T, 2023, IEEE T MULTIMEDIA, V25, P243, DOI 10.1109/TMM.2021.3124080
NR 78
TC 0
Z9 0
U1 2
U2 3
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 3
DI 10.1007/s10055-023-00900-1
PG 15
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA DJ7C7
UT WOS:001131724700001
DA 2024-08-05
ER

PT J
AU Zeng, Q
   Zheng, G
   Liu, Q
AF Zeng, Qiang
   Zheng, Gang
   Liu, Qian
TI DTP: learning to estimate full-body pose in real-time from sparse VR
   sensor measurements
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; HTC VIVE tracker; Full-body pose estimation; Deep
   learning; Transformer
ID HUMAN MOTION; PREDICTION; TRACKING
AB For virtual reality (VR) applications, estimating full-body pose in real-time is becoming increasingly popular. Previous works have reconstructed full-body motion in real time from an HTC VIVE headset and five VIVE Tracker measurements by solving the inverse kinematics (IK) problem. However, an IK solver may yield unnatural poses and shaky motion. This paper introduces Deep Tracker poser (DTP): a method for real-time full-body pose estimation in VR. This task is difficult due to the ambiguous mapping from the sparse measurements to full-body pose. The data obtained from VR sensors is calibrated, normalized and fed into the deep neural networks (DNN). To learn from sufficient data, we propose synthesizing a VR sensor dataset called AMASS-VR from the AMASS, a collection of various motion capture datasets. Furthermore, feet tracking loss is a common problem of VIVE Tracker. To improve the accuracy and robustness of DTP to the occlusion noise, we simulate the occlusion noise by Gaussian random noise. Then we synthesize an occlusion dataset AMASS-OCC and fine-tune DTP on that. We evaluate DTP by comparing with other popular methods in terms of the accuracy and computational cost. The results indicate that DTP outperforms others in terms of the positional error (1.04 cm) and rotational error (4.22 degrees). The quantitative and qualitative results show that DTP reconstructs accurate and natural full-body pose even under serious feet occlusion, which indicates the superiority of the DTP in modelling the mapping from sparse joint data to the full-body pose.
C1 [Zeng, Qiang] Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, Wuhan 430074, Peoples R China.
   [Zheng, Gang] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
   [Liu, Qian] Hainan Univ, Sch Biomed Engn, Key Lab Biomed Engn Hainan Prov, Haikou 570228, Peoples R China.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology; Hainan University
RP Liu, Q (corresponding author), Hainan Univ, Sch Biomed Engn, Key Lab Biomed Engn Hainan Prov, Haikou 570228, Peoples R China.
EM zengqiang18@hust.edu.cn; ZhengGang@hust.edu.cn; qliu@hainanu.edu.cn
RI Liu, Qian/D-8184-2011
OI Liu, Qian/0000-0002-8398-1021
FU Major Special Science and Technology Project of Hainan Province; 
   [ZDKJ202006]
FX We thank the support of the Major Special Science and Technology Project
   of Hainan Province (ZDKJ202006).
CR [Anonymous], 2011, Symposium on Interactive 3D Graphics and Games
   [Anonymous], 2010, SCA'10: proceedings of the 2010 ACM SIGGRAPH/Eurographics symposium on computer animation, DOI [10.2312/SCA/SCA10/001-010, DOI 10.2312/SCA/SCA10/001-010]
   [Anonymous], 2008, P EUR ACM SIGGRAPH S
   Aristidou A, 2018, COMPUT GRAPH FORUM, V37, P35, DOI 10.1111/cgf.13310
   Butt HT, 2021, IEEE ACCESS, V9, P36657, DOI 10.1109/ACCESS.2021.3062545
   Caserman P, 2019, IEEE INT CONF SERIOU, DOI 10.1109/segah.2019.8882429
   Caserman P, 2019, VIRTUAL REAL-LONDON, V23, P155, DOI 10.1007/s10055-018-0374-z
   Caserman P, 2020, IEEE T VIS COMPUT GR, V26, P3089, DOI 10.1109/TVCG.2019.2912607
   Chai JX, 2005, ACM T GRAPHIC, V24, P686, DOI 10.1145/1073204.1073248
   Du YM, 2023, PROC CVPR IEEE, P481, DOI 10.1109/CVPR52729.2023.00054
   Du ZX, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P320
   FLASH T, 1985, J NEUROSCI, V5, P1688, DOI 10.1523/jneurosci.05-07-01688.1985
   Greuter S, 2014, ACM INT C P SERIES, P1
   Habermann M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3311970
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Holden D, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925975
   Huang YH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275108
   Jiang F, 2016, PROCEEDINGS VRCAI 2016: 15TH ACM SIGGRAPH CONFERENCE ON VIRTUAL-REALITY CONTINUUM AND ITS APPLICATIONS IN INDUSTRY, P309, DOI 10.1145/3013971.3013987
   Jiang JX, 2022, LECT NOTES COMPUT SC, V13665, P443, DOI 10.1007/978-3-031-20065-6_26
   Jiang YF, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555428
   Johnson M., 2016, P INT WORK C ADV VIS, P316
   Jongmin Kim, 2012, Motion in Games. 5th International Conference (MIG 2012). Proceedings, P31, DOI 10.1007/978-3-642-34710-8_4
   Jung ES, 1996, INT J IND ERGONOM, V18, P173, DOI 10.1016/0169-8141(95)00080-1
   Khatib O., 2004, INT J HUMANOID ROB, V1, P29, DOI [10.1142/S0219843604000058, DOI 10.1142/S0219843604000058]
   Kim SU, 2021, PATTERN RECOGN LETT, V150, P162, DOI 10.1016/j.patrec.2021.06.018
   Leoncini P, 2017, LECT NOTES COMPUT SC, V10324, P131, DOI 10.1007/978-3-319-60922-5_10
   Li W, 2021, Exploiting Temporal Contexts with Strided Transformer for 3D Human Pose Estimation, P1
   Liu XX, 2018, PROCEEDINGS OF THE 4TH INTERNATIONAL CONFERENCE ON VIRTUAL REALITY (ICVR 2018), P21, DOI 10.1145/3198910.3198915
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Madadi M, 2021, INT J COMPUT VISION, V129, P2499, DOI 10.1007/s11263-021-01488-2
   Mahmood N, 2019, IEEE I CONF COMP VIS, P5441, DOI 10.1109/ICCV.2019.00554
   Malleson C, 2020, INT J COMPUT VISION, V128, P1594, DOI 10.1007/s11263-019-01270-5
   Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064
   Murray R.M., 2017, A mathematical introduction to robotic manipulation
   Parger M, 2018, 24TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2018), DOI 10.1145/3281505.3281529
   Pope R, 2022, Efficiently scaling transformer inference
   Raaen K, 2015, LECT NOTES COMPUT SC, V9353, P457, DOI 10.1007/978-3-319-24589-8_40
   Romero J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130883
   Safonova A, 2004, ACM T GRAPHIC, V23, P514, DOI 10.1145/1015706.1015754
   SOECHTING JF, 1989, J NEUROPHYSIOL, V62, P582, DOI 10.1152/jn.1989.62.2.582
   SOECHTING JF, 1989, J NEUROPHYSIOL, V62, P595, DOI 10.1152/jn.1989.62.2.595
   Tang Y, 2024, A survey on transformer compression, P1
   Tautges J, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966397
   Tong LN, 2020, IEEE SENS J, V20, P3667, DOI 10.1109/JSEN.2019.2959639
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Troje NF, 2002, J VISION, V2, P371, DOI 10.1167/2.5.2
   Trumble Matthew., 2017, BRIT MACHINE VISION, DOI [10.5244/C.31.14, DOI 10.5244/C.31.14]
   Vaswani A, 2017, ADV NEUR IN, V30
   Weytjens Hans, 2020, Business Process Management Workshops. BPM 2020 International Workshops. Revised Selected Papers. Lecture Notes in Business Information Processing (LNBIP 397), P321, DOI 10.1007/978-3-030-66498-5_24
   Winkler A, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555411
   Xu WP, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3181973
   Yang D, 2021, COMPUT GRAPH FORUM, V40, P265, DOI 10.1111/cgf.142631
   Yi XY, 2022, PROC CVPR IEEE, P13157, DOI 10.1109/CVPR52688.2022.01282
   Yi XY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459786
   Zeng Q, 2022, VIRTUAL REAL-LONDON, V26, P1391, DOI 10.1007/s10055-022-00635-5
   Zheng ZL, 2021, ENTROPY-SWITZ, V23, DOI 10.3390/e23050588
   Zhou Y, 2019, PROC CVPR IEEE, P5738, DOI 10.1109/CVPR.2019.00589
NR 58
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAY 23
PY 2024
VL 28
IS 2
AR 116
DI 10.1007/s10055-024-01011-1
PG 17
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA RU7W5
UT WOS:001230249900001
OA hybrid
DA 2024-08-05
ER

PT J
AU Thuilier, E
   Carey, J
   Dempsey, M
   Dingliana, J
   Whelan, B
   Brennan, A
AF Thuilier, Elea
   Carey, John
   Dempsey, Mary
   Dingliana, John
   Whelan, Bryan
   Brennan, Attracta
TI Virtual rehabilitation for patients with osteoporosis or other
   musculoskeletal disorders: a systematic review
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual rehabilitation for older adults; Osteoporosis; Musculoskeletal
   disorders; Patient engagement; Exergames; VR/AR
ID SPINAL-CORD-INJURY; ANKYLOSING-SPONDYLITIS; PARKINSONS-DISEASE; HOSPITAL
   ANXIETY; FALLS EFFICACY; OLDER-ADULTS; BALANCE; SCALE; RELIABILITY;
   VALIDITY
AB This study aims to identify effective ways to design virtual rehabilitation to obtain physical improvement (e.g. balance and gait) and support engagement (i.e. motivation) for people with osteoporosis or other musculoskeletal disorders. Osteoporosis is a systemic skeletal disorder and is among the most prevalent diseases globally, affecting 0.5 billion adults. Despite the fact that the number of people with osteoporosis is similar to, or greater than those diagnosed with cardiovascular disease and dementia, osteoporosis does not receive the same recognition. Worldwide, osteoporosis causes 8.9 million fractures annually; it is associated with substantial pain, suffering, disability and increased mortality. The importance of physical therapy as a rehabilitation strategy to avoid osteoporosis fracture cannot be over-emphasised. However, the main rehabilitation challenges relate to engagement and participation. The use of virtual rehabilitation to address such challenges in the delivery of physical improvement is gaining in popularity. As there currently is a paucity of literature applying virtual rehabilitation to patients with osteoporosis, the authors broadened the search parameters to include articles relating to the virtual rehabilitation of other skeletal disorders (e.g. Ankylosing spondylitis, spinal cord injury, motor rehabilitation, etc.). This systematic review initially identified 130 titles, from which 23 articles (involving 539 participants) met all eligibility and selection criteria. Four groups of devices supporting virtual rehabilitation were identified: a head-mounted display, a balance board, a camera and more specific devices. Each device supported physical improvement (i.e. balance, muscle strength and gait) post-training. This review has shown that: (a) each device allowed improvement with different degrees of immersion, (b) the technology choice is dependent on the care need and (c) virtual rehabilitation can be equivalent to and enhance conventional therapy and potentially increase the patient's engagement with physical therapy.
C1 [Thuilier, Elea; Brennan, Attracta] Univ Galway, Sch Comp Sci, Galway, Ireland.
   [Carey, John; Whelan, Bryan] Univ Galway, Sch Med, Galway, Ireland.
   [Dempsey, Mary] Univ Galway, Sch Engn, Galway, Ireland.
   [Dingliana, John] Trinity Coll Dublin, Sch Comp Sci & Stat, Dublin, Ireland.
C3 Ollscoil na Gaillimhe-University of Galway; Ollscoil na
   Gaillimhe-University of Galway; Ollscoil na Gaillimhe-University of
   Galway; Trinity College Dublin
RP Thuilier, E (corresponding author), Univ Galway, Sch Comp Sci, Galway, Ireland.
EM e.thuilier1@universityofgalway.ie
OI Thuilier, Elea/0000-0002-3502-6724; Brennan,
   Attracta/0000-0002-1164-9484
FU Science Foundation Ireland [18/CRT/6224]; Science Foundation Ireland
   Centre for Research Training in Digitally-Enhanced Reality (d-real);
   Science Foundation Ireland (SFI) [18/CRT/6224] Funding Source: Science
   Foundation Ireland (SFI)
FX This work was conducted with the financial support of the Science
   Foundation Ireland Centre for Research Training in Digitally-Enhanced
   Reality (d-real) under Grant No. 18/CRT/6224. For the purpose of Open
   Access, the author has applied a CC BY public copyright licence to any
   Author Accepted Manuscript version arising from this submission.
CR An CM, 2018, J SPINAL CORD MED, V41, P223, DOI 10.1080/10790268.2017.1369217
   Andreikanich A, 2019, IEEE ENG MED BIO, P3416, DOI [10.1109/EMBC.2019.8857469, 10.1109/embc.2019.8857469]
   [Anonymous], 2009, Preventing falls and harm from falls in older people
   Aung YM, 2011, I CREATE 11 P 5 INT, P1
   Bartlett HL, 2014, GAIT POSTURE, V39, P224, DOI 10.1016/j.gaitpost.2013.07.010
   Baus O, 2014, FRONT HUM NEUROSCI, V8, DOI 10.3389/fnhum.2014.00112
   BERG K, 1989, Physiotherapy Canada, V41, P304
   Blomqvist S, 2021, BMC GERIATR, V21, DOI 10.1186/s12877-021-02061-9
   Boettcher CE, 2008, J ORTHOP RES, V26, P1591, DOI 10.1002/jor.20675
   Boswell-Ruys CL, 2009, ARCH PHYS MED REHAB, V90, P1571, DOI 10.1016/j.apmr.2009.02.016
   Browne J, 2000, PHYSIOL MEAS, V21, P515, DOI 10.1088/0967-3334/21/4/308
   CALIN A, 1994, J RHEUMATOL, V21, P2281
   Chan WLS, 2020, AGING CLIN EXP RES, V32, P597, DOI 10.1007/s40520-019-01255-x
   Cho GH, 2014, J PHYS THER SCI, V26, P615, DOI 10.1589/jpts.26.615
   Delbaere K, 2010, AGE AGEING, V39, P210, DOI 10.1093/ageing/afp225
   Dencker A, 2015, HEALTH QUAL LIFE OUT, V13, DOI 10.1186/s12955-015-0213-9
   Dimbwadyo-Terrer I, 2016, DISABIL REHABIL-ASSI, V11, P462, DOI 10.3109/17483107.2015.1027293
   Dionyssiotis Y, 2014, CLIN MED INSIGHTS-AR, V7, P33, DOI 10.4137/CMAMD.S14077
   Ditunno PL, 2001, SPINAL CORD, V39, P654, DOI 10.1038/sj.sc.3101223
   Doward LC, 2007, HEALTH QUAL LIFE OUT, V5, DOI 10.1186/1477-7525-5-7
   DUNCAN PW, 1990, J GERONTOL, V45, pM192, DOI 10.1093/geronj/45.6.M192
   Enright Paul L, 2003, Respir Care, V48, P783
   Erjiang E, 2020, BMJ OPEN, V10, DOI 10.1136/bmjopen-2020-040488
   Fabbri B, 2021, HAND SURG REHABIL, V40, P560, DOI 10.1016/j.hansur.2021.05.004
   Ferguson L., 2009, The Journal of Pain, V10, pS73, DOI DOI 10.1016/J.JPAIN.2009.01.258
   Gomez JF, 2013, COLOMB MEDICA, V44, P165
   Da Gama AEF, 2016, COMPUT METH PROG BIO, V135, P105, DOI 10.1016/j.cmpb.2016.07.014
   Gardner MM, 2001, AGE AGEING, V30, P77, DOI 10.1093/ageing/30.1.77
   Gianola S, 2020, MEDICINE, V99, DOI 10.1097/MD.0000000000019136
   Goetz CG, 2004, MOVEMENT DISORD, V19, P1020, DOI 10.1002/mds.20213
   Hagell P, 2007, J NEUROL NEUROSUR PS, V78, P1191, DOI 10.1136/jnnp.2006.111161
   Han JG, 2013, IEEE T CYBERNETICS, V43, P1318, DOI 10.1109/TCYB.2013.2265378
   Hauser RA, 2012, INT J NEUROSCI, V122, P333, DOI 10.3109/00207454.2012.657381
   Hays RD, 2001, ANN MED, V33, P350, DOI 10.3109/07853890109002089
   Ho PWT, 2021, The Routledge Handbook of Public Health and the Community, P138
   Im DJ, 2015, ANN REHABIL MED-ARM, V39, P462, DOI 10.5535/arm.2015.39.3.462
   Itzkovich M, 2018, SPINAL CORD, V56, P46, DOI 10.1038/sc.2017.97
   Jerald J., 2015, VR BOOK HUMAN CENTER, DOI [10.1145/2792790, DOI 10.1145/2792790]
   Jung J, 2012, J PHYS THER SCI, V24, P1133, DOI 10.1589/jpts.24.1133
   Kamper SJ, 2010, J CLIN EPIDEMIOL, V63, P760, DOI 10.1016/j.jclinepi.2009.09.009
   Kanis JA, 2021, ARCH OSTEOPOROS, V16, DOI 10.1007/s11657-020-00871-9
   Karahan AY, 2016, ADV CLIN EXP MED, V25, P931, DOI 10.17219/acem/32590
   Kear BM, 2017, J PRIM CARE COMMUNIT, V8, P9, DOI 10.1177/2150131916659282
   Khasnis A, 2003, J Postgrad Med, V49, P169
   Khurana M, 2017, TOP SPINAL CORD INJ, V23, P263, DOI 10.1310/sci16-00003
   Kizony R, 2005, J REHABIL RES DEV, V42, P595, DOI 10.1682/JRRD.2005.01.0023
   Lavernia C, 2008, J ARTHROPLASTY, V23, P85, DOI 10.1016/j.arth.2008.05.019
   Lee HY, 2015, J PHYS THER SCI, V27, P1883, DOI 10.1589/jpts.27.1883
   Lee Jin, 2017, J Phys Ther Sci, V29, P1586, DOI 10.1589/jpts.29.1586
   Lenze EJ, 2004, ARCH PHYS MED REHAB, V85, P380, DOI 10.1016/j.apmr.2003.06.001
   Losilla F, 2019, ICSOFT: PROCEEDINGS OF THE 14TH INTERNATIONAL CONFERENCE ON SOFTWARE TECHNOLOGIES, P643, DOI 10.5220/0007798906430648
   MAHONEY FI, 1965, MD STATE MED J, V14, P61, DOI [DOI 10.1037/T02366-000, 10.1037/t02366-000]
   Maples-Keller JL, 2017, HARVARD REV PSYCHIAT, V25, P103, DOI 10.1097/HRP.0000000000000138
   McCabe E, 2020, ARCH OSTEOPOROS, V15, DOI 10.1007/s11657-020-0704-0
   McDonough AL, 2001, ARCH PHYS MED REHAB, V82, P419, DOI 10.1053/apmr.2001.19778
   MORSE JM, 1989, CAN J AGING, V8, P366, DOI 10.1017/S0714980800008576
   Mubin Omar, 2019, JMIR Rehabil Assist Technol, V6, pe12010, DOI 10.2196/12010
   Omaña H, 2021, PHYS THER, V101, DOI 10.1093/ptj/pzab173
   Page MJ, 2021, J CLIN EPIDEMIOL, V134, P178, DOI 10.1016/j.jclinepi.2021.03.001
   Pandian S, 2012, J BODYW MOV THER, V16, P330, DOI 10.1016/j.jbmt.2011.11.002
   Perlmutter Joel S, 2009, Curr Protoc Neurosci, VChapter 10, DOI 10.1002/0471142301.ns1001s49
   Rabin R, 2001, ANN MED, V33, P337, DOI 10.3109/07853890109002087
   Rand Debbie, 2008, J Neurol Phys Ther, V32, P155, DOI 10.1097/NPT.0b013e31818ee779
   Roberts MH, 2014, INT PSYCHOGERIATR, V26, P325, DOI 10.1017/S104161021300197X
   Robertson J.F. P., 1984, British Journal of Sports Medicine, V18, P25, DOI [10.1136/bjsm.18.1.25., DOI 10.1136/BJSM.18.1.25]
   Roetenberg D., 2009, Xsens Motion Technologies BV, Tech. Rep
   Roos EM, 1999, SCAND J RHEUMATOL, V28, P210
   Shahid A., 2011, STOP, THAT, and One Hundred other Sleep Scales, P209, DOI [10.1007/978-1-4419-9893-4_47, DOI 10.1007/978-1-4419-9893-4_47, 10.1007/978-1-4419-9893-447]
   Sharma S, 2021, BIOMED RES INT, V2021, DOI 10.1155/2021/9945775
   Singh Shamsher, 2014, Pharmacy Pract (Granada), V12
   Song HY, 2011, J HEALTH COMMUN, V16, P148, DOI 10.1080/10810730.2010.535107
   Sözen T, 2017, EUR J RHEUMATOL, V4, P46, DOI 10.5152/eurjrheum.2016.048
   Staiano AE, 2014, GAMES HEALTH J, V3, P351, DOI 10.1089/g4h.2013.0100
   Szturm T, 2011, PHYS THER, V91, P1449, DOI 10.2522/ptj.20090205
   Topley M, 2020, J BIOMECH, V106, DOI 10.1016/j.jbiomech.2020.109820
   Trombetta M, 2017, COMPUT METH PROG BIO, V151, P15, DOI 10.1016/j.cmpb.2017.08.008
   Valenzuela T, 2018, J GERIATR PHYS THER, V41, P49, DOI 10.1519/JPT.0000000000000095
   van den Heuvel MRC, 2014, PARKINSONISM RELAT D, V20, P1352, DOI 10.1016/j.parkreldis.2014.09.022
   Villiger M, 2017, FRONT NEUROL, V8, DOI 10.3389/fneur.2017.00635
   Wall T, 2015, J SPINAL CORD MED, V38, P777, DOI 10.1179/2045772314Y.0000000296
   Wang YC, 2015, J HAND THER, V28, P53, DOI 10.1016/j.jht.2014.09.002
   Weigl K, 2021, EDUC PSYCHOL MEAS, V81, P595, DOI 10.1177/0013164420952118
   Wildschut A, 2020, Stroke Engine 2023
   Willers C, 2022, ARCH OSTEOPOROS, V17, DOI 10.1007/s11657-021-00969-8
   Yen CY, 2011, PHYS THER, V91, P862, DOI 10.2522/ptj.20100050
   Yoo HN, 2013, J PHYS THER SCI, V25, P797, DOI 10.1589/jpts.25.797
   ZIGMOND AS, 1983, ACTA PSYCHIAT SCAND, V67, P361, DOI 10.1111/j.1600-0447.1983.tb09716.x
NR 87
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD APR 7
PY 2024
VL 28
IS 2
AR 93
DI 10.1007/s10055-024-00980-7
PG 33
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA NB0E7
UT WOS:001197854100001
PM 38595908
OA hybrid
DA 2024-08-05
ER

PT J
AU Teixeira, J
   Miellet, S
   Palmisano, S
AF Teixeira, Joel
   Miellet, Sebastien
   Palmisano, Stephen
TI Effects of vection type and postural instability on cybersickness
SO VIRTUAL REALITY
LA English
DT Article
DE Cybersickness; Postural instability; Motion sickness; Virtual reality;
   Head-mounted display; Vection
ID INDUCED MOTION SICKNESS; VIRTUAL-REALITY; OPTOKINETIC NYSTAGMUS;
   SEX-DIFFERENCES; SELF-MOTION; STABILITY; SWAY; SYMPTOMS; CUES
AB This study directly compared the novel unexpected vection hypothesis and postural instability-based explanations of cybersickness in virtual reality (VR) using head-mounted displays (HMD) for the first time within a commercial VR game. A total of 40 participants (19 males and 21 females) played an HMD-VR game (Aircar) for up to 14 min, or until their first experience of cybersickness. Based on their self-reports, 24 of these participants were classified as being 'sick' during the experiment, with the remainder being classified as 'well'. Consistent with the unexpected vection hypothesis, we found that: (1) 'sick' participants were significantly more likely to report unexpected vection (i.e., an experience of self-motion that was different to what they had been expecting), and (2) sickness severity increased (exponentially) with the strength of any unexpected (but not expected) vection. Our results also supported the predictions of postural instability theory, finding that the onset of cybersickness was typically preceded by an increase in participants' postural instability. However, when both sway and vection measures were combined, only unexpected vection was found to significantly predict the occurrence of sickness. These findings highlight the importance of unusual vection experiences and postural instability in understanding cybersickness. However, they suggest that developers should be able to make use of expected experiences of vection to safely enhance HMD-VR.
C1 [Teixeira, Joel; Miellet, Sebastien; Palmisano, Stephen] Univ Wollongong, Sch Psychol, Wollongong, NSW 2522, Australia.
C3 University of Wollongong
RP Teixeira, J (corresponding author), Univ Wollongong, Sch Psychol, Wollongong, NSW 2522, Australia.
EM joelt@uow.edu.au
RI Miellet, Sebastien/AFL-6215-2022; Palmisano, Stephen/O-1553-2018
OI Miellet, Sebastien/0000-0002-3519-033X; Palmisano,
   Stephen/0000-0002-9140-5681; Teixeira, Joel/0009-0003-9154-044X
FU Australian Research Council
FX No Statement Available
CR Akiduki H, 2003, NEUROSCI LETT, V340, P197, DOI 10.1016/S0304-3940(03)00098-3
   ANDERSEN GJ, 1985, J EXP PSYCHOL HUMAN, V11, P122, DOI 10.1037/0096-1523.11.2.122
   Arcioni B, 2019, DISPLAYS, V58, P3, DOI 10.1016/j.displa.2018.07.001
   Bonato F, 2005, AVIAT SPACE ENVIR MD, V76, P823
   Bonato F, 2004, AVIAT SPACE ENVIR MD, V75, P306
   Bonato F, 2008, PRESENCE-TELEOP VIRT, V17, P283, DOI 10.1162/pres.17.3.283
   Bonato F, 2009, AVIAT SPACE ENVIR MD, V80, P941, DOI 10.3357/ASEM.2394.2009
   Bonnet CT, 2006, HUM MOVEMENT SCI, V25, P800, DOI 10.1016/j.humov.2006.03.001
   Chardonnet JR, 2017, INT J HUM-COMPUT INT, V33, P771, DOI 10.1080/10447318.2017.1286767
   Chen DJZ, 2011, I-PERCEPTION, V2, P415
   Clifton J, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3364722
   Clifton J, 2020, VIRTUAL REAL-LONDON, V24, P453, DOI 10.1007/s10055-019-00407-8
   Cobb SVG, 1999, APPL ERGON, V30, P47, DOI 10.1016/S0003-6870(98)00038-6
   Cobb SVG, 1998, BRAIN RES BULL, V47, P459, DOI 10.1016/S0361-9230(98)00104-X
   Dennison MS, 2017, APPL ERGON, V58, P215, DOI 10.1016/j.apergo.2016.06.014
   Diels C, 2007, AVIAT SPACE ENVIR MD, V78, P659
   Dong Xiao., 2010, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V54, P1340, DOI DOI 10.1177/154193121005401808
   Flanagan MB, 2002, AVIAT SPACE ENVIR MD, V73, P1067
   Gavgani AM, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0182790
   Golding JF, 2012, AVIAT SPACE ENVIR MD, V83, P477, DOI 10.3357/ASEM.3095.2012
   Hadidon T J, 2016, An optimized algorithm for prediction of virtual environment motion sickness using postural sway
   Hettinger J., 1992, Presence, P306, DOI [10.1162/pres.1992.1.3.306, DOI 10.1162/PRES.1992.1.3.306]
   Hettinger L J, 1990, Mil Psychol, V2, P171, DOI 10.1207/s15327876mp0203_4
   Ji JTT, 2009, HUM FACTORS, V51, P739, DOI 10.1177/0018720809349708
   Kennedy R.S., 1990, MOTION SPACE SICKNES, P317, DOI DOI 10.1207/S15327108IJAP0303_3
   Kennedy R.S., 1993, Int. J. Aviat. Psy, P203
   Keshavarz Behrang, 2016, Virtual, Augmented and Mixed Reality. 8th International Conference, VAMR 2016, held as part of HCI International 2016. Proceedings: LNCS 9740, P147, DOI 10.1007/978-3-319-39907-2_14
   Keshavarz B, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00472
   Keshavarz B, 2014, EXP BRAIN RES, V232, P827, DOI 10.1007/s00221-013-3793-9
   Koslucher F, 2016, EXP BRAIN RES, V234, P313, DOI 10.1007/s00221-015-4462-y
   Koslucher F, 2015, AEROSP MED HUM PERF, V86, P787, DOI 10.3357/AMHP.4243.2015
   Kuiper OX, 2019, DISPLAYS, V58, P82, DOI 10.1016/j.displa.2018.10.001
   Lawson B D, 2005, P 11 INT C HUM COMP, P1
   Lawson BD, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.759682
   Lawson BD, 2015, HUM FACTORS ERGON, P531
   Litleskare S, 2021, PHYSIOL BEHAV, V236, DOI 10.1016/j.physbeh.2021.113422
   Littman EM, 2010, ECOL PSYCHOL, V22, P150, DOI 10.1080/10407411003720106
   McCauley M. E., 1992, Teleoperators and Virtual Environments, V1, P311, DOI [10.1162/pres.1992.1.3.311, DOI 10.1162/PRES.1992.1.3.311]
   Merhi O, 2007, HUM FACTORS, V49, P920, DOI 10.1518/001872007X230262
   Munafo J, 2017, EXP BRAIN RES, V235, P889, DOI 10.1007/s00221-016-4846-7
   Nishiike S, 2013, J MED INVESTIG, V60, P236
   Nooij SAE, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0245295
   Nooij SAE, 2018, EXP BRAIN RES, V236, P3031, DOI 10.1007/s00221-018-5340-1
   Nooij SAE, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0175305
   Palmisano S, 2004, PERCEPTION, V33, P987, DOI 10.1068/p5242
   Palmisano S, 2002, PERCEPTION, V31, P463, DOI 10.1068/p3321
   Palmisano S, 1996, PERCEPT PSYCHOPHYS, V58, P1168, DOI 10.3758/BF03207550
   Palmisano S, 2007, AVIAT SPACE ENVIR MD, V78, P951, DOI 10.3357/ASEM.2079.2007
   Palmisano S, 2020, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.587698
   Palmisano S, 2019, ATTEN PERCEPT PSYCHO, V81, P281, DOI 10.3758/s13414-018-1609-5
   Palmisano S, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0195886
   Palmisano S, 2018, EXP BRAIN RES, V236, P315, DOI 10.1007/s00221-017-5130-1
   Palmisano S, 2017, DISPLAYS, V46, P1, DOI 10.1016/j.displa.2016.11.001
   Palmisano S, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00193
   Pöhlmann KMT, 2022, J COGN ENHANCE, V6, P3, DOI 10.1007/s41465-021-00215-6
   Reason J. T., 1975, Motion Sickness
   REASON JT, 1978, J ROY SOC MED, V71, P819, DOI 10.1177/014107687807101109
   RICCIO G E, 1991, Ecological Psychology, V3, P195, DOI 10.1207/s15326969eco0303_2
   Riecke BE, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00713
   Risi D, 2019, DISPLAYS, V60, P9, DOI 10.1016/j.displa.2019.08.003
   Smart LJ, 2023, HUM FACTORS, V65, P1830, DOI 10.1177/00187208211059623
   Smart LJ, 2014, ECOL PSYCHOL, V26, P301, DOI 10.1080/10407413.2014.958029
   Smart LJ, 2002, HUM FACTORS, V44, P451, DOI 10.1518/0018720024497745
   Stanney K. M., 2014, Handbook of virtual environments: Design, implementation and applications, P532, DOI [https://doi.org/10.1201/b17360, DOI 10.1201/B17360]
   Stanney KM, 1998, PRESENCE-TELEOP VIRT, V7, P447, DOI 10.1162/105474698565848
   STOFFREGEN T A, 1991, Ecological Psychology, V3, P159, DOI 10.1207/s15326969eco0303_1
   Stoffregen TA, 2000, HUM FACTORS, V42, P458, DOI 10.1518/001872000779698097
   Stoffregen TA, 1998, BRAIN RES BULL, V47, P437, DOI 10.1016/S0361-9230(98)00102-6
   Stoffregen TA, 2008, HUM FACTORS, V50, P322, DOI 10.1518/001872008X250755
   Stoffregen TA, 2014, EXP BRAIN RES, V232, P1389, DOI 10.1007/s00221-014-3859-3
   Stoffregen TA, 2010, ECOL PSYCHOL, V22, P169, DOI 10.1080/10407413.2010.496645
   Teixeira J, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.860919
   Teixeira J, 2021, VIRTUAL REAL-LONDON, V25, P433, DOI 10.1007/s10055-020-00466-2
   Ujike H, 2008, DISPLAYS, V29, P81, DOI 10.1016/j.displa.2007.09.003
   Villard SJ, 2008, HUM FACTORS, V50, P332, DOI 10.1518/001872008X250728
   Webb NA, 2003, AVIAT SPACE ENVIR MD, V74, P622
   Webb NA, 2002, AVIAT SPACE ENVIR MD, V73, P351
   Yokota Y, 2005, ACTA OTO-LARYNGOL, V125, P280, DOI 10.1080/00016480510003192
NR 78
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR 22
PY 2024
VL 28
IS 2
AR 82
DI 10.1007/s10055-024-00969-2
PG 18
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA LS0U0
UT WOS:001188681900001
OA hybrid
DA 2024-08-05
ER

PT J
AU Korkut, EH
   Surer, E
AF Korkut, Elif Hilal
   Surer, Elif
TI Developing a framework for heterotopias as discursive playgrounds: a
   comparative analysis of non-immersive and immersive technologies
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual museums; Virtual reality; Design tools; Design research;
   Multimedia; User interface
ID CELLULAR-AUTOMATA; VIRTUAL-REALITY; ARCHITECTURE; EMBODIMENT; DESIGN;
   SENSE; MODEL; FORM
AB The discursive space represents the reordering of knowledge gained through accumulation. In the digital age, multimedia has become the language of information, and the space for archival practices is provided by non-immersive technologies, resulting in the disappearance of several layers from discursive activities. Heterotopias are unique, multilayered epistemic contexts that connect other systems through the exchange of information. This paper describes a process to create a framework for Virtual Reality, Mixed Reality, and personal computer environments based on heterotopias to provide absent layers. This study provides virtual museum space as an informational terrain that contains a "world within worlds" and presents place production as a layer of heterotopia and the subject of discourse. Automation for the individual multimedia content is provided via various sorting and grouping algorithms, and procedural content generation algorithms such as Binary Space Partitioning, Cellular Automata, Growth Algorithm, and Procedural Room Generation. Versions of the framework were comparatively evaluated through a user study involving 30 participants, considering factors such as usability, technology acceptance, and presence. The results of the study show that the framework can serve diverse contexts to construct multilayered digital habitats and is flexible for integration into professional and daily life practices.
C1 [Korkut, Elif Hilal; Surer, Elif] Middle East Tech Univ, Grad Sch Informat, Dept Modeling & Simulat, TR-06800 Ankara, Turkiye.
C3 Middle East Technical University
RP Surer, E (corresponding author), Middle East Tech Univ, Grad Sch Informat, Dept Modeling & Simulat, TR-06800 Ankara, Turkiye.
EM elif.korkut@metu.edu.tr; elifs@metu.edu.tr
RI Surer, Elif/I-5157-2015
OI Surer, Elif/0000-0002-0738-6669
CR Akin S, 2021, ENG CONSTR ARCHIT MA, V28, P1319, DOI 10.1108/ECAM-07-2020-0562
   Araghi SK, 2015, AUTOMAT CONSTR, V49, P152, DOI 10.1016/j.autcon.2014.10.007
   Baron JR, 2017, PROCEEDINGS OF THE SOUTHEAST CONFERENCE ACM SE'17, P168, DOI 10.1145/3077286.3077566
   Barsanti SG, 2015, INT ARCH PHOTOGRAMM, V40-5, P165, DOI 10.5194/isprsarchives-XL-5-W7-165-2015
   Basaraba N, 2023, NEW MEDIA SOC, V25, P1470, DOI 10.1177/14614448211044942
   Batty M, 2018, ENVIRON PLAN B-URBAN, V45, P817, DOI 10.1177/2399808318796416
   BIOCCA F, 1992, J COMMUN, V42, P23, DOI 10.1111/j.1460-2466.1992.tb00811.x
   Boldi A, 2022, HUM-COMPUT INTERACT, DOI 10.1080/07370024.2022.2050725
   Braun V, 2019, QUAL RES SPORT EXERC, V11, P589, DOI 10.1080/2159676X.2019.1628806
   Brooke J., 1996, SUS-a quick and dirty usability scale, DOI [DOI 10.1201/9781498710411-35, DOI 10.1201/9781498710411]
   Camozzato D, 2015, A method for growth-based procedural floor plan generation
   Cecotti H., 2022, Virtual Worlds, V1, P82, DOI [DOI 10.3390/VIRTUALWORLDS1010006, 10.3390/virtualworlds1010006]
   Cruz C, 2016, PROCEEDINGS OF THE 21ST INTERNATIONAL CONFERENCE ON COMPUTER-AIDED ARCHITECTURAL DESIGN RESEARCH IN ASIA (CAADRIA 2016), P187
   Davis F.D., 1987, User Acceptance of Information Systems: The Technology Acceptance Model (TAM)
   Faulkner L, 2003, BEHAV RES METH INS C, V35, P379, DOI 10.3758/BF03195514
   Fischer G, 2004, COMMUN ACM, V47, P33, DOI 10.1145/1015864.1015884
   Foo Schubert., 2009, Handbook of Research on Digital Libraries, P88, DOI DOI 10.4018/978-1-59904-879-6.CH009
   Foucault M., 1972, ARCHEOLOGY KNOWLEDGE
   Foucault Michel., 2005, The Order of Things, DOI [10.4324/9780203996645, DOI 10.4324/9780203996645]
   Foucault Michel., 2008, HETEROTOPIA CITY PUB, P13
   Foucault Michel., 2019, Aesthetics, Method, and Epistemology: Essential Works of Foucault 1954-1984
   Hammady R, 2018, LECT NOTES COMPUT SC, V10851, P349, DOI 10.1007/978-3-319-95282-6_26
   Hassenzahl M, 2006, BEHAV INFORM TECHNOL, V25, P91, DOI 10.1080/01449290500330331
   Hayashi M, 2016, 2016 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P219, DOI 10.1109/CW.2016.44
   Haydar M, 2011, VIRTUAL REAL-LONDON, V15, P311, DOI 10.1007/s10055-010-0176-4
   Heater C., 1992, Presence: Teleoperators and Virtual Environments, V2, P262, DOI DOI 10.1162/PRES.1992.1.2.262
   Hepp A, 2018, TRANSF COMM ST CROSS, P15, DOI 10.1007/978-3-319-65584-0_2
   Herr CM, 2007, AUTOMAT CONSTR, V16, P61, DOI 10.1016/j.autcon.2005.10.005
   Kersten TP, 2017, INT ARCH PHOTOGRAMM, V42-2, P361, DOI 10.5194/isprs-archives-XLII-2-W3-361-2017
   Kilteni K, 2012, PRESENCE-TELEOP VIRT, V21, P373, DOI 10.1162/PRES_a_00124
   Komianos V, 2018, IOP CONF SER-MAT SCI, V364, DOI 10.1088/1757-899X/364/1/012011
   Lee JG, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10155363
   Lehtinen M, 2021, FRONT POLIT SCI, V3, DOI 10.3389/fpos.2021.674076
   Lehtinen S., 2022, Imperfectionist Aesthetics in Art and Everyday Life, P363
   Lippert SK, 2005, J INF SCI, V31, P340, DOI 10.1177/0165551505055399
   Lopes R, 2010, 11TH INTERNATIONAL CONFERENCE ON INTELLIGENT GAMES AND SIMULATION, GAME-ON 2010, P13
   Luck R, 2014, INT J DES CREAT INNO, V2, P165, DOI 10.1080/21650349.2013.875488
   Maciag R, 2018, PHILOSOPHIES, V3, DOI 10.3390/philosophies3040034
   Malraux A., 1967, Museum without Walls
   Mann Steve., 2002, PRESENCE CONNECT, V1, P2002
   Manovich Lev, 2001, LANGUAGE NEW MEDIA
   Martirosov S, 2022, VIRTUAL REAL-LONDON, V26, P15, DOI 10.1007/s10055-021-00507-4
   McKnight D.H., 2011, ACM Transactions on Management Information Systems, V2, DOI [10.1145/1985347.1985353, DOI 10.1145/1985347.1985353]
   MILGRAM P, 1994, IEICE T INF SYST, VE77D, P1321
   Nesbitt K., 1996, THEORIZING NEW AGEND
   Novak M, 1997, Digital delirium, P260
   Odom W., 2014, Proceedings of the 2014 Conference on Designing Interactive Systems, P985, DOI 10.1145/2598510.2598577
   Pietroni Eva, 2013, 2013 Digital Heritage International Congress (DigitalHeritage). Federating the 19th Int'I VSMM, 10th Eurographics GCH, & 2nd UNESCO Memory of the World Conferences, plus special sessions from CAA, Arqueologico 2.0, Space2Place, ICOMOS ICIP & CIPA, EU projects, et al. Proceedings, P653
   Porter D, 2006, WIT TRANS BUILT ENV, V90, P59, DOI 10.2495/DARC060071
   Prabhakaran A, 2022, AUTOMAT CONSTR, V142, DOI 10.1016/j.autcon.2022.104489
   Prusinkiewicz P., 2012, The algorithmic beauty of plants, DOI DOI 10.1007/978-1-4613-8476-2
   Rasmussen SteenEiler., 1964, Experiencing Architecture, V2nd
   Rheingold H., 1991, VIRTUAL REALITY EXPL
   Rousseaux F, 2009, INT J HUMANIT ARTS C, V3, P175, DOI [10.3366/ijhac.2009.0015, 10.3366/E1753854810000595]
   Tamayo JLR, 2018, REV ICONO 14, V16, DOI 10.7195/ri14.v16i2.1174
   Safikhani S, 2022, INT J DIGIT EARTH, V15, P503, DOI 10.1080/17538947.2022.2038291
   Salanitri D, 2015, LECT NOTES COMPUT SC, V9169, P49, DOI 10.1007/978-3-319-20901-2_5
   SANDELOWSKI M, 1995, RES NURS HEALTH, V18, P179, DOI 10.1002/nur.4770180211
   Schultze U, 2010, J INF TECHNOL-UK, V25, P434, DOI 10.1057/jit.2010.25
   Schweibenz W., 2019, The Museum Review, V4, P1
   SHNEIDERMAN B, 1983, LECT NOTES COMPUT SC, V150, P9
   Spence C, 2020, COGN RES, V5, DOI 10.1186/s41235-020-00243-4
   Tavakol M, 2011, INT J MED EDUC, V2, P53, DOI 10.5116/ijme.4dfb.8dfd
   Tcha-Tokey K, 2018, ADV HUM-COMPUT INTER, V2018, DOI 10.1155/2018/7827286
   Thompson K, 2003, CONT PHILOS REV, V36, P113, DOI 10.1023/A:1026072000125
   Tsakiris M, 2010, NEUROPSYCHOLOGIA, V48, P703, DOI 10.1016/j.neuropsychologia.2009.09.034
   Venkatesh V, 2000, MANAGE SCI, V46, P186, DOI 10.1287/mnsc.46.2.186.11926
   Wang XY, 2018, AUTOMAT CONSTR, V94, P405, DOI 10.1016/j.autcon.2018.07.017
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   Yamada Y, 2017, PROCEEDINGS OF THE 21ST INTERNATIONAL ACADEMIC MINDTREK CONFERENCE (ACADEMIC MINDTREK), P235, DOI 10.1145/3131085.3131097
   Yang F, 2022, ISPRS INT J GEO-INF, V11, DOI 10.3390/ijgi11100530
   Yee N, 2016, CHI PLAY 2016: PROCEEDINGS OF THE 2016 ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P2, DOI 10.1145/2967934.2967937
NR 72
TC 0
Z9 0
U1 6
U2 7
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 16
DI 10.1007/s10055-023-00905-w
PG 30
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA EW2S3
UT WOS:001141910100001
OA hybrid, Green Submitted
DA 2024-08-05
ER

PT J
AU Palmisano, S
   Stephenson, L
   Davies, RG
   Kim, J
   Allison, RS
AF Palmisano, Stephen
   Stephenson, Lance
   Davies, Rodney G.
   Kim, Juno
   Allison, Robert S.
TI Testing the 'differences in virtual and physical head pose' and
   'subjective vertical conflict' accounts of cybersickness
SO VIRTUAL REALITY
LA English
DT Article
DE Head-mounted display; Motion sickness; Cybersickness; Virtual reality;
   Motion-to-photon latency; Sensory conflict
ID MOTION SICKNESS; SIMULATOR SICKNESS; SENSORY CONFLICT; SYMPTOMS;
   LATENCY; HMD; ENVIRONMENTS; AMPLITUDE; HASTENS; DELAY
AB When we move our head while in virtual reality, display lag will generate differences in our virtual and physical head pose (known as DVP). While DVP are a major trigger for cybersickness, theories differ as to exactly how they constitute a provocative sensory conflict. Here, we test two competing theories: the subjective vertical conflict theory and the DVP hypothesis. Thirty-two HMD users made continuous, oscillatory head rotations in either pitch or yaw while viewing a large virtual room. Additional display lag was applied selectively to the simulation about the same, or an orthogonal, axis to the instructed head rotation (generating Yaw-Lag + Yaw-Move, Yaw-Lag + Pitch-Move, Pitch-Lag + Yaw-Move, and Pitch-Lag + Pitch-Move conditions). At the end of each trial: (1) participants rated their sickness severity and scene instability; and (2) their head tracking data were used to estimate DVP throughout the trial. Consistent with our DVP hypothesis, but contrary to subjective vertical conflict theory, Yaw-Lag + Yaw-Move conditions induced significant cybersickness, which was similar in magnitude to that in the Pitch-Lag + Pitch-Move conditions. When extra lag was added along the same axis as the instructed head movement, DVP was found to predict 73-76% of the variance in sickness severity (with measures of the spatial magnitude and the temporal dynamics of the DVP both contributing significantly). Ratings of scene instability were also found to predict sickness severity. Taken together, these findings suggest that: (1) cybersickness can be predicted from objective estimates of the DVP; and (2) provocative stimuli for this sickness can be identified from subjective reports of scene instability.
C1 [Palmisano, Stephen; Stephenson, Lance; Davies, Rodney G.] Univ Wollongong, Sch Psychol, Wollongong, NSW 2522, Australia.
   [Kim, Juno] Univ New South Wales, Sch Optometry & Vis Sci, Sydney, NSW, Australia.
   [Allison, Robert S.] York Univ, Ctr Vis Res, Toronto, ON, Canada.
   [Allison, Robert S.] York Univ, Dept Elect Engn & Comp Sci, Toronto, ON, Canada.
C3 University of Wollongong; University of New South Wales Sydney; York
   University - Canada; York University - Canada
RP Palmisano, S (corresponding author), Univ Wollongong, Sch Psychol, Wollongong, NSW 2522, Australia.
EM stephenp@uow.edu.au
RI ; Palmisano, Stephen/O-1553-2018
OI Allison, Robert/0000-0002-4485-2665; Palmisano,
   Stephen/0000-0002-9140-5681
FU Australian Research Council
FX No Statement Available
CR Allison RS, 2001, P IEEE VIRT REAL ANN, P247, DOI 10.1109/VR.2001.913793
   Bles W, 2000, CURR OPIN NEUROL, V13, P19, DOI 10.1097/00019052-200002000-00005
   Bles W, 1998, BRAIN RES BULL, V47, P481, DOI 10.1016/S0361-9230(98)00115-4
   Bonato F, 2005, AVIAT SPACE ENVIR MD, V76, P823
   Bos JE, 2002, BIOL CYBERN, V86, P191, DOI 10.1007/s00422-001-0289-7
   Bos JE, 1998, BRAIN RES BULL, V47, P537, DOI 10.1016/S0361-9230(98)00088-4
   Bos JE, 2008, DISPLAYS, V29, P47, DOI 10.1016/j.displa.2007.09.002
   Bubka A, 2003, AVIAT SPACE ENVIR MD, V74, P315
   Caserman P, 2019, LECT NOTES COMPUT SC, V11863, P57, DOI 10.1007/978-3-030-34644-7_5
   Clifton J, 2020, VIRTUAL REAL-LONDON, V24, P453, DOI 10.1007/s10055-019-00407-8
   de Graaf B, 1998, BRAIN RES BULL, V47, P489, DOI 10.1016/S0361-9230(98)00116-6
   Dennison MS, 2016, DISPLAYS, V44, P42, DOI 10.1016/j.displa.2016.07.002
   DiZio P, 1997, ADV HUM FACT ERGON, V21, P893
   Dodgson NA, 2004, PROC SPIE, V5291, P36, DOI 10.1117/12.529999
   Draper MH, 2001, HUM FACTORS, V43, P129, DOI 10.1518/001872001775992552
   EBENHOLTZ SM, 1994, AVIAT SPACE ENVIR MD, V65, P1032
   Gavgani AM, 2017, AUTON NEUROSCI-BASIC, V203, P41, DOI 10.1016/j.autneu.2016.12.004
   Golding JF, 2016, HAND CLINIC, V137, P371, DOI 10.1016/B978-0-444-63437-5.00027-3
   Golding JF, 1998, BRAIN RES BULL, V47, P507, DOI 10.1016/S0361-9230(98)00091-4
   Golding JF, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.576871
   Howarth PA, 1997, DISPLAYS, V18, P107, DOI 10.1016/S0141-9382(97)00011-5
   Howarth PA, 1999, APPL ERGON, V30, P39, DOI 10.1016/S0003-6870(98)00041-6
   Ihlen EAF, 2012, FRONT PHYSIOL, V3, DOI 10.3389/fphys.2012.00141
   Jasper A, 2023, COMPUT HUM BEHAV, V146, DOI 10.1016/j.chb.2023.107800
   Jennings S, 2004, J AIRCRAFT, V41, P1327, DOI 10.2514/1.449
   Jennings S., 2000, Human Factors, V44, P69
   Kennedy R.S., 1993, Int. J. Aviat. Psy, P203
   Kennedy RS., 1994, Res Appl, V2, P1
   Kennedy RS., 1994, Proceedings of "Virtual Reality and Medicine: The Cutting Edge.", P111
   Keshavarz B, 2022, INT J PSYCHOPHYSIOL, V176, P14, DOI 10.1016/j.ijpsycho.2022.03.006
   Keshavarz B, 2011, HUM FACTORS, V53, P415, DOI 10.1177/0018720811403736
   Keshavarz Behrang., 2014, Handbook of Virtual Environments: Design, Implementation, and Applications Issue September, P647, DOI [DOI 10.1201/B17360-32, https://doi.org/10.1201/b17360-32]
   Khalid H, 2011, J MAR SCI TECH-JAPAN, V16, P214, DOI 10.1007/s00773-010-0113-y
   Khalid H, 2011, OCEAN ENG, V38, P22, DOI 10.1016/j.oceaneng.2010.09.008
   Kim J, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.582156
   Kim J, 2020, COMPUT HUM BEHAV, V113, DOI 10.1016/j.chb.2020.106484
   Kim K, 2014, COMPUT METH PROG BIO, V113, P882, DOI 10.1016/j.cmpb.2013.12.024
   Kinsella A, 2016, AEROSP MED HUM PERF, V87, P604, DOI 10.3357/AMHP.4351.2016
   Koslucher F, 2016, EXP BRAIN RES, V234, P2709, DOI 10.1007/s00221-016-4675-8
   La Viola J. J.  Jr., 2000, SIGCHI Bulletin, V32, P47, DOI 10.1145/333329.333344
   Law A., 2019, P VERT FLIGHT SOC 75
   Lawson BD, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.759682
   Luu W., 2019, P SIGGRAPH AS SIGGRA, DOI [10.1145/3355056.3364590, DOI 10.1145/3355056.3364590]
   McCauley M. E., 1992, Teleoperators and Virtual Environments, V1, P311, DOI [10.1162/pres.1992.1.3.311, DOI 10.1162/PRES.1992.1.3.311]
   Merhi O, 2007, HUM FACTORS, V49, P920, DOI 10.1518/001872007X230262
   Moss JD, 2011, DISPLAYS, V32, P159, DOI 10.1016/j.displa.2011.05.010
   Moss JD, 2011, HUM FACTORS, V53, P308, DOI 10.1177/0018720811405196
   Munafo J, 2017, EXP BRAIN RES, V235, P889, DOI 10.1007/s00221-016-4846-7
   Nooij SAE, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0175305
   Oman C M, 1982, Acta Otolaryngol Suppl, V392, P1
   OMAN CM, 1990, CAN J PHYSIOL PHARM, V68, P294, DOI 10.1139/y90-044
   Palmisano S, 2023, VIRTUAL REAL-LONDON, V27, P1293, DOI 10.1007/s10055-022-00732-5
   Palmisano S, 2022, VIRTUAL REAL-LONDON, V26, P1373, DOI 10.1007/s10055-022-00634-6
   Palmisano S, 2020, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.587698
   Palmisano S, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3364699
   PENG CK, 1995, CHAOS, V5, P82, DOI 10.1063/1.166141
   Pierre MES, 2015, DISPLAYS, V36, P1, DOI 10.1016/j.displa.2014.10.005
   REASON JT, 1978, J ROY SOC MED, V71, P819, DOI 10.1177/014107687807101109
   Rebenitsch L, 2016, VIRTUAL REAL-LONDON, V20, P101, DOI 10.1007/s10055-016-0285-9
   Risi D, 2019, DISPLAYS, V60, P9, DOI 10.1016/j.displa.2019.08.003
   Sepich NC, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.943409
   Sharples S, 2008, DISPLAYS, V29, P58, DOI 10.1016/j.displa.2007.09.005
   Stanney K, 1998, INT J HUM-COMPUT INT, V10, P135, DOI 10.1207/s15327590ijhc1002_3
   Stanney K. M., 2014, Handbook of virtual environments: Design, implementation and applications, P532, DOI [https://doi.org/10.1201/b17360, DOI 10.1201/B17360]
   Stanney K, 2020, FRONT ROBOT AI, V7, DOI 10.3389/frobt.2020.00004
   Stanney KM, 1998, PRESENCE-TELEOP VIRT, V7, P327, DOI 10.1162/105474698565767
   Stauffert JP, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P121, DOI 10.1109/VR.2018.8446195
   STOFFREGEN T A, 1991, Ecological Psychology, V3, P159, DOI 10.1207/s15326969eco0303_1
   Stoffregen TA, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0187120
   Teixeira J, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.860919
   Teixeira J, 2021, VIRTUAL REAL-LONDON, V25, P433, DOI 10.1007/s10055-020-00466-2
   van Waveren JMP, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P37, DOI 10.1145/2993369.2993375
   Wu WX, 2013, PRESENCE-TELEOP VIRT, V22, P20, DOI 10.1162/PRES_a_00131
   Yildirim C, 2020, VIRTUAL REAL-LONDON, V24, P231, DOI 10.1007/s10055-019-00401-0
   Yildirim C, 2019, DISPLAYS, V59, P35, DOI 10.1016/j.displa.2019.07.002
NR 75
TC 1
Z9 1
U1 2
U2 3
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 22
DI 10.1007/s10055-023-00909-6
PG 28
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA FB2L0
UT WOS:001143221700002
OA hybrid
DA 2024-08-05
ER

PT J
AU Widiyanti, DE
   Asmoro, K
   Shin, SY
AF Widiyanti, Daniar Estu
   Asmoro, Krisma
   Shin, Soo Young
TI HoloGCS: mixed reality-based ground control station for unmanned aerial
   vehicle
SO VIRTUAL REALITY
LA English
DT Article
DE Mixed reality; Unmanned aerial vehicle; Ground control station; Speech
   control; Video streaming; Microsoft HoloLens; Human-robot interactions
ID TELEOPERATION
AB Human-robot interaction (HRI), which studies the interaction between robots and humans, appears as a promising research idea for the future of smart factories. In this study, HoloLens as ground control station (HoloGCS) is implemented, and its performance is discussed. HoloGCS is a mixed reality-based system for controlling and monitoring unmanned aerial vehicles (UAV). The system incorporates HRI through speech commands and video streaming, enabling UAV teleoperation. HoloGCS provides a user interface that allows operators to monitor and control the UAV easily. To demonstrate the feasibility of the proposed systems, a user case study (user testing and SUS-based questionnaire) was performed to gather qualitative results. In addition, throughput, RTT, latency, and speech accuracy were also gathered and analyzed to evaluate quantitative results.
C1 [Widiyanti, Daniar Estu; Asmoro, Krisma; Shin, Soo Young] Kumoh Natl Inst Technol, Dept IT Convergence Engn, Gumi 39177, Gyeongsangbuk, South Korea.
C3 Kumoh National University Technology
RP Shin, SY (corresponding author), Kumoh Natl Inst Technol, Dept IT Convergence Engn, Gumi 39177, Gyeongsangbuk, South Korea.
EM daniarestuw@kumoh.ac.kr; krisma@kumoh.ac.kr; wdragon@kumoh.ac.kr
RI Shin, Soo Young/ABG-4608-2021
OI Shin, Soo Young/0000-0002-2526-2395
FU National Research Foundation of Korea [IITP-2023-2020-0-01612]; MSIT
   (Ministry of Science and ICT), Korea, under the Innovative Human
   Resource Development for Local Intellectualization support program
   [IITP-2023-RS-2023-00259061]; MSIT (Ministry of Science and ICT), Korea,
   under the ITRC(Information Technology Research Center) support program
FX This research was supported by the MSIT (Ministry of Science and ICT),
   Korea, under the Innovative Human Resource Development for Local
   Intellectualization support program (IITP-2023-2020-0-01612) supervised
   by the IITP (Institute for Information & communications Technology
   Planning & Evaluation) and this research was supported by the MSIT
   (Ministry of Science and ICT), Korea, under the ITRC(Information
   Technology Research Center) support program (IITP-2023-RS-2023-00259061)
   supervised by the IITP (Institute for Information & Communications
   Technology Planning & Evaluation).
CR [Anonymous], 2021, Robot Operating System-ROS
   [Anonymous], 2021, Microsoft Hololens 2 offers - 500
   Bangor A, 2009, J USABILITY STUD, V4, P114
   Bennett M, 2017, IEEE INT C INT ROBOT, P6589, DOI 10.1109/IROS.2017.8206571
   Bokade AU, 2016, 2016 INTERNATIONAL CONFERENCE ON COMMUNICATION AND SIGNAL PROCESSING (ICCSP), VOL. 1, P2094, DOI 10.1109/ICCSP.2016.7754547
   Bosch J, 2022, ACTAS 37 JORNADAS AU
   Brooke J., 1996, SUS-a quick and dirty usability scale, DOI [DOI 10.1201/9781498710411-35, DOI 10.1201/9781498710411]
   Chen CX, 2021, 2021 5TH INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION SCIENCES (ICRAS 2021), P89, DOI 10.1109/ICRAS52289.2021.9476619
   Fayjie AR, 2017, INT CONF UBIQ FUTUR, P119
   García JC, 2017, IEEE COMPUT GRAPH, V37, P34, DOI 10.1109/MCG.2015.118
   Gattullo M, 2019, ROBOT CIM-INT MANUF, V56, P276, DOI 10.1016/j.rcim.2018.10.001
   Gong L, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS AND MECHATRONICS (ICARM), P420, DOI 10.1109/ICARM.2017.8273199
   Graff C., 2016, Drone piloting study
   Hedayati H, 2018, ACMIEEE INT CONF HUM, P78, DOI 10.1145/3171221.3171251
   Herrmann Roman, 2018, i-com: A Journal of Interactive and Cooperative Media, V17, P15, DOI 10.1515/icom-2018-0001
   Higuchi K, 2013, PROCEEDINGS OF 2013 23RD INTERNATIONAL CONFERENCE ON ARTIFICIAL REALITY AND TELEXISTENCE (ICAT 2013), P28
   Hoppenstedt B, 2019, LECT NOTES COMPUT SC, V11614, P43, DOI 10.1007/978-3-030-25999-0_4
   Huang BC, 2019, IEEE INT CONF ROBOT, P6949, DOI 10.1109/icra.2019.8794200
   Ibrahimov R, 2019, IEEE ROMAN, DOI 10.1109/ro-man46459.2019.8956344
   Jeong M, 2018, I C INF COMM TECH CO, P1084, DOI 10.1109/ICTC.2018.8539625
   Jerald Jason, 2014, 2014 IEEE Virtual Reality (VR), P1, DOI 10.1109/VR.2014.6802117
   Kot Tomas, 2014, Applied Mechanics and Materials, V555, P199, DOI 10.4028/www.scientific.net/AMM.555.199
   Kot T, 2018, 2018 19TH INTERNATIONAL CARPATHIAN CONTROL CONFERENCE (ICCC), P422, DOI 10.1109/CarpathianCC.2018.8399667
   Krupke D, 2018, 2018 IEEERSJ INT C I, P1
   Landau M, 2017, ACMIEEE INT CONF HUM, P181, DOI 10.1145/3029798.3038329
   Lee J., 2018, MANUF LETT, V18, P20, DOI DOI 10.1016/J.MFGLET.2018.09.002
   Lipton JI, 2018, IEEE ROBOT AUTOM LET, V3, P179, DOI 10.1109/LRA.2017.2737046
   Mellinkoff BJ, 2018, AEROSP CONF PROC
   ROSbridge, 2021, about us
   Steinfeld A., 2006, 1st Annual Conference on Human-Robot Interaction, P33
   STOKER CR, 1995, IEEE EXPERT, V10, P14, DOI 10.1109/64.483008
   Sun AL, 2019, INT C INTEL HUM MACH, P229, DOI 10.1109/IHMSC.2019.00060
   Sutar T., 2021, Advances in signal and data processing
   Unity3d, 2021, about us
   Vega LFL, 2020, IEEE IND ELEC, P2313, DOI [10.1109/iecon43393.2020.9255295, 10.1109/IECON43393.2020.9255295]
   Wan JF, 2018, IEEE INTERNET THINGS, V5, P2272, DOI 10.1109/JIOT.2017.2728722
   Whitney D, 2020, SPR PROC ADV ROBOT, V10, P335, DOI 10.1007/978-3-030-28619-4_28
   Widiyanti DE., 2021, Korean Inst Commun Inf Sci, V2021, P1054
   Yingbo Zhou, 2020, 2020 IEEE 6th International Conference on Computer and Communications (ICCC), P1257, DOI 10.1109/ICCC51575.2020.9344892
   Zhao JB, 2018, IEEE SYS MAN CYBERN, P2972, DOI 10.1109/SMC.2018.00505
   Zuehlke D, 2010, ANNU REV CONTROL, V34, P129, DOI 10.1016/j.arcontrol.2010.02.008
NR 41
TC 0
Z9 0
U1 12
U2 12
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 40
DI 10.1007/s10055-023-00914-9
PG 12
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA HE8Y7
UT WOS:001157920900002
OA hybrid
DA 2024-08-05
ER

PT J
AU Lee, TH
   Jeong, YJ
AF Lee, Tae Hee
   Jeong, Young Ju
TI Spatial resolution measurement method for 3D displays from contrast
   modulation
SO VIRTUAL REALITY
LA English
DT Article
DE 3D display; Light field display; Augmented reality display; Resolution;
   Contrast modulation
ID VISIBILITY
AB Augmented Reality 3D head-up displays use a autostereoscopic 3D display as a panel. The 3D optical unit of autostereoscopic 3D displays controls the direction of the light rays in each pixel, allowing the users enjoy 3D world without glasses. However, these 3D optics cause image quality degradation. Deterioration of resolution has a serious impact on 3D image quality. Therefore, it is important to properly measure the 3D resolution according to 3D optics and analyze its impact. In this study, a method for measuring spatial resolution in 3D displays using contrast modulation is proposed. We describe a conventional 2D resolution measurement methods that are standardized. Based on the existing 2D resolution methods, we propose a 3D resolution method. The spatial and frequency signal responses of 3D displays were investigated. The first method is determined by the predominant frequency series. The second method is conducted by contrast modulation. Through experiments with 3D displays, 3D resolution was measured using the proposed method, and the relationship between the parameters and resolution of 3D optics was examined.
C1 [Lee, Tae Hee] Sungkyunkwan Univ, Comp Educ, Sungkyunkwanro 25-2, Seoul, South Korea.
   [Jeong, Young Ju] Sookmyung Womens Univ, Software, Cheongparo 47 Gil 100, Seoul, South Korea.
C3 Sungkyunkwan University (SKKU); Sookmyung Women's University
RP Jeong, YJ (corresponding author), Sookmyung Womens Univ, Software, Cheongparo 47 Gil 100, Seoul, South Korea.
EM tysep16@gmail.com; yjeong@sookmyung.ac.kr
FU Ministry of Trade, Industry and Energy [20025744]; Ministry of Trade,
   Industry and Energy (MOTIE), South Korea; MSIT (Ministry of Science and
   ICT), South Korea [IITP-2023-RS-2022-00156299]; ICAN(ICT Challenge and
   Advanced Network of HRD) program
FX This study was supported by the Ministry of Trade, Industry and Energy
   (MOTIE), South Korea (20025744) and the MSIT (Ministry of Science and
   ICT), South Korea, under the ICAN(ICT Challenge and Advanced Network of
   HRD) program(IITP-2023-RS-2022-00156299) supervised by the
   IITP(Institute of Information & Communications Technology Planning &
   Evaluation).
CR Aflaki P, 2014, J VIS COMMUN IMAGE R, V25, P622, DOI 10.1016/j.jvcir.2013.03.014
   Ahn E, 2018, VIRTUAL REAL-LONDON, V22, P245, DOI 10.1007/s10055-017-0319-y
   [Anonymous], 1994, BT RI-R, P1127
   Boev A, 2012, DISPLAYS, V33, P103, DOI 10.1016/j.displa.2012.01.002
   Boev A, 2010, J SOC INF DISPLAY, V18, P686, DOI 10.1889/JSID18.9.686
   BT I-RR, 1998, Subjective assessment methods for image quality in high-definition television
   Chen F, 2017, J VIS COMMUN IMAGE R, V43, P41, DOI 10.1016/j.jvcir.2016.12.004
   Cheng MY, 2022, J SOC INF DISPLAY, V30, P905, DOI 10.1002/jsid.1179
   DisplayMetrology IC, 2012, Society for Information Display (SID), V135
   Dodgson NA, 2005, COMPUTER, V38, P31, DOI 10.1109/MC.2005.252
   Fan H, 2016, J SOC INF DISPLAY, V24, P323, DOI 10.1002/jsid.431
   Fu BS, 2023, DISPLAYS, V80, DOI 10.1016/j.displa.2023.102514
   Gvozden G, 2018, J VIS COMMUN IMAGE R, V50, P145, DOI 10.1016/j.jvcir.2017.11.017
   Harada Y, 2022, VIRTUAL REAL-LONDON, V26, P759, DOI 10.1007/s10055-021-00574-7
   Holliman NS, 2011, IEEE T BROADCAST, V57, P362, DOI 10.1109/TBC.2011.2130930
   Huang KC, 2000, P SOC PHOTO-OPT INS, V4080, P78, DOI 10.1117/12.389431
   ISO, 2000, ISO standard
   Jackin BJ, 2023, VIRTUAL REAL-LONDON, V27, P761, DOI 10.1007/s10055-022-00686-8
   Järvenpää T, 2008, J SOC INF DISPLAY, V16, P825, DOI 10.1889/1.2966444
   Jeong YJ, 2017, OPT EXPRESS, V25, P10500, DOI 10.1364/OE.25.010500
   Johnson R. Barry, 2005, Proceedings of the SPIE - The International Society for Optical Engineering, V5874, P587406, DOI 10.1117/12.618082
   KELLER PA, 1986, DISPLAYS, V7, P17, DOI 10.1016/0141-9382(86)90040-5
   Kim SK, 2015, OPT EXPRESS, V23, P13230, DOI 10.1364/OE.23.013230
   Kovács PT, 2017, IEEE J-STSP, V11, P1213, DOI 10.1109/JSTSP.2017.2738606
   Kovács PT, 2014, IEEE IMAGE PROC, P768, DOI 10.1109/ICIP.2014.7025154
   Li L, 2022, OPT COMMUN, V521, DOI 10.1016/j.optcom.2022.128355
   Lv GJ, 2021, OPT COMMUN, V481, DOI 10.1016/j.optcom.2020.126559
   Masaoka K, 2008, PROC SPIE, V6806, DOI 10.1117/12.767143
   McIntire JP, 2014, DISPLAYS, V35, P18, DOI 10.1016/j.displa.2013.10.004
   MURCH GM, 1988, DISPLAYS, V9, P23, DOI 10.1016/0141-9382(88)90109-6
   Naylor W, 2023, VIRTUAL REAL-LONDON, V27, P2529, DOI 10.1007/s10055-023-00825-9
   Read T, 2023, VIRTUAL REAL-LONDON, V27, P1265, DOI 10.1007/s10055-022-00723-6
   Salmimaa Marja, 2009, Information Display, V25, P8
   Tian Y, 2018, J VIS COMMUN IMAGE R, V57, P212, DOI 10.1016/j.jvcir.2018.11.005
   Wang D., 2023, Gxjzz, V4, P1, DOI [10.37188/lam.2023.018, DOI 10.37188/LAM.2023.018]
NR 35
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD JUL 2
PY 2024
VL 28
IS 3
AR 130
DI 10.1007/s10055-024-01026-8
PG 12
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA XH1B5
UT WOS:001260689400002
OA hybrid
DA 2024-08-05
ER

PT J
AU Kooijman, L
   Asadi, H
   Arango, CG
   Mohamed, S
   Nahavandi, S
AF Kooijman, Lars
   Asadi, Houshyar
   Arango, Camilo Gonzalez
   Mohamed, Shady
   Nahavandi, Saeid
TI Investigating the influence of neck muscle vibration on illusory
   self-motion in virtual reality
SO VIRTUAL REALITY
LA English
DT Article
DE Vection intensity; Multisensory integration; Cybersickness; Flight
   simulator
ID CIRCULAR VECTION; VISUAL-MOTION; CONFIDENCE-INTERVALS; PERCEPTION;
   STIMULATION; INFORMATION; HEAD; BODY; CYBERSICKNESS; INTEGRATION
AB The illusory experience of self-motion known as vection, is a multisensory phenomenon relevant to self-motion processes. While some studies have shown that neck muscle vibrations can improve self-motion parameter estimation, the influence on vection remains unknown. Further, few studies measured cybersickness (CS), presence, and vection concurrently and have shown conflicting results. It was hypothesized that 1) neck vibrations would enhance vection and presence, and 2) CS to negatively relate to presence and vection, whereas presence and vection to positively relate to one another. Thirty-two participants were visually and audibly immersed in a virtual reality flight simulator and occasional neck muscle vibrations were presented. Vection onset and duration were reported through button presses. Turning angle estimations and ratings of vection quality, presence, and CS were obtained after completion of the flights. Results showed no influence of vibrations on turning angle estimation errors, but a medium positive effect of vibrations on vection quality was found. Presence and vection quality were positively related, and no strong association between CS and presence or vection was found. It is concluded that neck vibrations may enhance vection and presence, however, from the current study it is unclear whether this is due to proprioceptive or tactile stimulation.
C1 [Kooijman, Lars; Asadi, Houshyar; Arango, Camilo Gonzalez; Mohamed, Shady] Deakin Univ, Inst Intelligent Syst Res & Innovat, Geelong, Vic, Australia.
   [Nahavandi, Saeid] Swinburne Univ Technol, Dept Mech Engn & Prod Design Engn, Hawthorn, Vic, Australia.
   [Nahavandi, Saeid] Harvard Univ, Sch Engn & Appl Sci, Harvard Paulson, Allston, MA 02134 USA.
C3 Deakin University; Swinburne University of Technology; Harvard
   University
RP Kooijman, L (corresponding author), Deakin Univ, Inst Intelligent Syst Res & Innovat, Geelong, Vic, Australia.
EM kooijman.l@outlook.com
RI Kooijman, Lars/ABE-4581-2020; Mohamed, Shady/N-3590-2014
OI Kooijman, Lars/0000-0002-0902-5752; Asadi, Houshyar/0000-0002-3620-8693;
   Mohamed, Shady/0000-0002-8851-1635; Gonzalez Arango,
   Camilo/0000-0002-3874-4801
FU Australian Research Council [DE210101623]; Institute for Intelligent
   Systems Research and Innovation; Australian Research Council
   [DE210101623] Funding Source: Australian Research Council
FX This research was supported by the Australian Research Council (ARC)
   (Project ID: DE210101623) and the Institute for Intelligent Systems
   Research and Innovation. The authors would like to thank Dr. Navid
   Mohajer for his advice on several sensors and actuators. The authors
   would like to acknowledge that this research was conducted on the land
   of the Wadawurrung people of the Kulin Nation, the traditional owners of
   the lands, and we would like to pay respect to their elders past,
   present, and emerging.
CR Amrhein V, 2022, TRENDS ECOL EVOL, V37, P567, DOI 10.1016/j.tree.2022.02.001
   Amrhein V, 2019, NATURE, V567, P305, DOI 10.1038/d41586-019-00857-9
   Aruga A., 2019, Int J Inform Soc, V11, P65
   Ash A, 2013, PERCEPTION, V42, P562, DOI 10.1068/p7449
   Bakdash JZ, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.00456
   Becker W, 2016, EXP BRAIN RES, V234, P2045, DOI 10.1007/s00221-016-4604-x
   Berti S, 2021, MULTISENS RES, V34, P153, DOI 10.1163/22134808-bja10035
   BIGUER B, 1988, BRAIN, V111, P1405, DOI 10.1093/brain/111.6.1405
   Bles W, 1995, J Vestib Res, V5, P109, DOI 10.1016/0957-4271(94)00025-W
   BLES W, 1977, AGRESSOLOGIE, V18, P325
   Bove M, 2001, J APPL PHYSIOL, V91, P581, DOI 10.1152/jappl.2001.91.2.581
   BRANDT T, 1977, EXP BRAIN RES, V30, P331
   Britten KH, 2008, ANNU REV NEUROSCI, V31, P389, DOI 10.1146/annurev.neuro.29.051605.112953
   Britton Z, 2019, FRONT NEUROL, V10, DOI 10.3389/fneur.2019.00063
   Campbell JID, 2012, BEHAV RES METHODS, V44, P1255, DOI 10.3758/s13428-012-0186-0
   Campos JL, 2014, EXP BRAIN RES, V232, P3277, DOI 10.1007/s00221-014-4011-0
   Churan J, 2017, J NEUROPHYSIOL, V118, P1650, DOI 10.1152/jn.00342.2017
   Cohen J., 1988, Statistical power analysis for the behavioral sciences
   Cullen KE, 2021, CURR OPIN PHYSIOL, V20, P29, DOI 10.1016/j.cophys.2020.12.001
   Cullen KE, 2011, CURR OPIN NEUROBIOL, V21, P587, DOI 10.1016/j.conb.2011.05.022
   Fauville G, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-020-80100-y
   Gardner EP., 2000, Principles of neural science, P498
   Greenlee MW, 2016, MULTISENS RES, V29, P525, DOI 10.1163/22134808-00002527
   Han Y, 1999, GRAEF ARCH CLIN EXP, V237, P815, DOI 10.1007/s004170050318
   Harris LR, 2000, EXP BRAIN RES, V135, P12, DOI 10.1007/s002210000504
   Hettinger LJ, 2015, HUM FACTORS ERGON, P435
   Ivanenko YP, 1999, J PHYSIOL-LONDON, V519, P301, DOI 10.1111/j.1469-7793.1999.0301o.x
   Jamal K, 2020, NEUROPHYSIOL CLIN, V50, P227, DOI 10.1016/j.neucli.2019.10.003
   Jürgens R, 1999, EXP BRAIN RES, V128, P491, DOI 10.1007/s002210050872
   Keshavarz B, 2019, DISPLAYS, V58, P71, DOI 10.1016/j.displa.2018.07.005
   Keshavarz B, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00472
   Keshavarz B, 2014, EXP BRAIN RES, V232, P827, DOI 10.1007/s00221-013-3793-9
   Keshavarz B, 2011, HUM FACTORS, V53, P415, DOI 10.1177/0018720811403736
   Kim J, 2022, VIRTUAL REAL-LONDON, V26, P425, DOI 10.1007/s10055-021-00570-x
   Kooijman L., 2022, INT C HUM SYST INT H, P1, DOI [10.1109/HSI55341.2022.9869507, DOI 10.1109/HSI55341.2022.9869507]
   Kooijman L, 2024, BEHAV RES METHODS, V56, P2292, DOI 10.3758/s13428-023-02148-8
   Kooijman L, 2023, ROY SOC OPEN SCI, V10, DOI 10.1098/rsos.221622
   Kooijman L, 2022, ATTEN PERCEPT PSYCHO, V84, P300, DOI 10.3758/s13414-021-02400-3
   Kröger S, 2021, SKELET MUSCLE, V11, DOI 10.1186/s13395-020-00258-x
   Kuiper OX, 2019, DISPLAYS, V58, P82, DOI 10.1016/j.displa.2018.10.001
   LACKNER JR, 1979, AVIAT SPACE ENVIR MD, V50, P346
   LACKNER JR, 1977, AVIAT SPACE ENVIR MD, V48, P129
   Laessoe U, 2023, PHYSIOL BEHAV, V258, DOI 10.1016/j.physbeh.2022.114015
   Lappe M, 1999, TRENDS COGN SCI, V3, P329, DOI 10.1016/S1364-6613(99)01364-9
   Larsson P., 2004, Proceedings of 7th Annual Workshop of Presence, P252
   Lind S, 2016, P IEEE VIRT REAL ANN, P219, DOI 10.1109/VR.2016.7504732
   Luu W, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-89751-x
   Matsuda Y, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.654088
   McShane BB, 2019, AM STAT, V73, P235, DOI 10.1080/00031305.2018.1527253
   MERGNER T, 1991, EXP BRAIN RES, V85, P389
   Morey RD, 2008, TUTOR QUANT METHODS, V4, P61, DOI 10.20982/tqmp.04.2.p061
   Murata K., 2014, Psychology, V5, P1777, DOI DOI 10.4236/PSYCH.2014.515184
   Murovec B, 2021, MULTISENS RES, V34, P869, DOI 10.1163/22134808-bja10058
   Nooij SAE, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0175305
   Palmisano S, 2004, PERCEPTION, V33, P987, DOI 10.1068/p5242
   Palmisano S, 2017, DISPLAYS, V46, P1, DOI 10.1016/j.displa.2016.11.001
   Palmisano S, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00193
   Panichi R, 2011, HUM MOVEMENT SCI, V30, P314, DOI 10.1016/j.humov.2010.10.005
   Parise CV, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms11543
   Pettorossi VE, 2014, FRONT HUM NEUROSCI, V8, DOI 10.3389/fnhum.2014.00895
   Pöhlmann KMT, 2022, DISPLAYS, V71, DOI 10.1016/j.displa.2021.102111
   Riecke Bernhard E., 2012, Spatial Cognition VIII. Proceedings of the International Conference, Spatial Cognition 2012, P143, DOI 10.1007/978-3-642-32732-2_9
   Riecke B. E., 2005, P 2 S APPL PERC GRAP, P111, DOI DOI 10.1145/1080402.1080422
   Riecke BE, 2005, P IEEE VIRT REAL ANN, P131
   Riecke BE, 2023, MULTISENS RES, V36, P827, DOI 10.1163/22134808-bja10112
   Riecke BE, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2799
   Riecke BE, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.01174
   ROLL JP, 1982, EXP BRAIN RES, V47, P177
   Seizova-Cajic T, 2007, PLOS ONE, V2, DOI 10.1371/journal.pone.0001037
   Seizova-Cajic T, 2006, EXP BRAIN RES, V173, P141, DOI 10.1007/s00221-006-0373-2
   Seno T, 2018, I-PERCEPTION, V9, DOI [10.1177/2041669517742176, 10.1177/2041669518774069]
   Seno T, 2013, MULTISENS RES, V26, P267, DOI 10.1163/22134808-00002402
   Seno T, 2011, PERCEPTION, V40, P747, DOI 10.1068/p7018
   Seya Y, 2015, VISION RES, V117, P16, DOI 10.1016/j.visres.2015.10.013
   Shams L., 2011, Sensory Cue Integration, P251, DOI [10.1093/acprof:oso/9780195387247.003.0013, DOI 10.1093/ACPROF:OSO]
   SLATER M, 1993, IEEE VIRTUAL REALITY ANNUAL INTERNATIONAL SYMPOSIUM, P90, DOI 10.1109/VRAIS.1993.380793
   Soave F, 2021, PROCEEDINGS OF THE 2021 ACM DESIGNING INTERACTIVE SYSTEMS CONFERENCE (DIS 2021), P171, DOI 10.1145/3461778.3462064
   TAYLOR JL, 1991, BRAIN, V114, P755, DOI 10.1093/brain/114.2.755
   Taylor MW, 2017, MULTISENS RES, V30, P25, DOI 10.1163/22134808-00002544
   Tinga AM, 2018, ACTA PSYCHOL, V182, P32, DOI 10.1016/j.actpsy.2017.11.007
   Väljamäe A, 2009, BRAIN RES REV, V61, P240, DOI 10.1016/j.brainresrev.2009.07.001
   Väljamäe A, 2009, J AUDIO ENG SOC, V57, P111
   van Atteveldt N, 2014, NEURON, V81, P1240, DOI 10.1016/j.neuron.2014.02.044
   Wasserstein RL, 2019, AM STAT, V73, P1, DOI 10.1080/00031305.2019.1583913
   Weech S, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.00158
   Yahata R, 2021, PERCEPTION, V50, P154, DOI 10.1177/0301006620987087
NR 86
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR 15
PY 2024
VL 28
IS 2
AR 76
DI 10.1007/s10055-024-00951-y
PG 21
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA LG7B7
UT WOS:001185687800003
OA Green Submitted, hybrid
DA 2024-08-05
ER

PT J
AU Pérez-Pachón, L
   Sharma, P
   Brech, H
   Gregory, J
   Lowe, T
   Poyade, M
   Gröning, F
AF Perez-Pachon, Laura
   Sharma, Parivrudh
   Brech, Helena
   Gregory, Jenny
   Lowe, Terry
   Poyade, Matthieu
   Groning, Flora
TI Augmented reality headsets for surgical guidance: the impact of
   holographic model positions on user localisation accuracy
SO VIRTUAL REALITY
LA English
DT Article
DE Image marker; Augmented reality; Surgery; Surgical navigation; Augmented
   reality headsets; Skin tumour removal
ID NAVIGATION SYSTEM; PERFORATOR FLAP; EXCISION; MANAGEMENT; SURGERY; DEEP
AB Novel augmented reality headsets such as HoloLens can be used to overlay patient-specific virtual models of resection margins on the patient's skin, providing surgeons with information not normally available in the operating room. For this to be useful, surgeons wearing the headset must be able to localise virtual models accurately. We measured the error with which users localise virtual models at different positions and distances from their eyes. Healthy volunteers aged 20-59 years (n = 54) performed 81 exercises involving the localisation of a virtual hexagon's vertices overlaid on a monitor surface. Nine predefined positions and three distances between the virtual hexagon and the users' eyes (65, 85 and 105 cm) were set. We found that, some model positions and the shortest distance (65 cm) led to larger localisation errors than other positions and larger distances (85 and 105 cm). Positional errors of more than 5 mm and 1-5 mm margin errors were found in 29.8% and over 40% of cases, respectively. Strong outliers were also found (e.g. margin shrinkage of up to 17.4 mm in 4.3% of cases). The measured errors may result in poor outcomes of surgeries: e.g. incomplete tumour excision or inaccurate flap design, which can potentially lead to tumour recurrence and flap failure, respectively. Reducing localisation errors associated with arm reach distances between the virtual models and users' eyes is necessary for augmented reality headsets to be suitable for surgical purposes. In addition, training surgeons on the use of these headsets may help to minimise localisation errors.
C1 [Perez-Pachon, Laura; Brech, Helena; Gregory, Jenny; Groning, Flora] Univ Aberdeen, Sch Med Med Sci & Nutr, Aberdeen, Scotland.
   [Sharma, Parivrudh] Brighton & Sussex Univ Hosp NHS Trust, Princess Royal Hosp, Haywards Heath, England.
   [Lowe, Terry] Aberdeen Royal Infirm, NHS Grampian, Head & Neck Oncol Unit, Aberdeen, Scotland.
   [Poyade, Matthieu] Glasgow Sch Art, Sch Simulat & Visualisat, Glasgow, Scotland.
C3 University of Aberdeen; Brighton and Sussex University Hospitals NHS
   Trust; University of Aberdeen; Glasgow School of Art
RP Sharma, P (corresponding author), Brighton & Sussex Univ Hosp NHS Trust, Princess Royal Hosp, Haywards Heath, England.
EM parivrudh.sharma1@nhs.net
RI Groning, Flora/C-2220-2012
OI Groning, Flora/0000-0002-8127-4112; Gregory,
   Jennifer/0000-0002-6328-4560; Poyade, Matthieu/0000-0002-7229-949X
FU The Roland Sutton Academic Trust (RSAT 0053/R/17) [RSAT 0053/R/17];
   Roland Sutton Academic Trust; University of Aberdeen; Elphinstone
   Scholarship; Medical Sciences Honours project
FX We are grateful to Mike Whyment for facilitating the purchase of the
   HoloLens headset used in this study and to Rute Vieira and Fiona
   Saunders for their advice on statistics. We would also like to thank
   John Barrow, Tracey Wilkinson and Denise Tosh and the Anatomy staff at
   the University of Aberdeen for their support. This research was funded
   by The Roland Sutton Academic Trust (RSAT 0053/R/17) and the University
   of Aberdeen (via an Elphinstone Scholarship, IKEC Award and Medical
   Sciences Honours project funding.
CR Avery B, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P79
   Bach B, 2018, IEEE T VIS COMPUT GR, V24, P457, DOI 10.1109/TVCG.2017.2745941
   Bann SD, 2003, WORLD J SURG, V27, P390, DOI 10.1007/s00268-002-6769-7
   Benmahdjoub M, 2022, VIRTUAL REAL-LONDON, V26, P1637, DOI 10.1007/s10055-022-00653-3
   Bosc R, 2017, ANN CHIR PLAST ESTH, V62, P336, DOI 10.1016/j.anplas.2017.01.002
   Castelan Enrique, 2021, IMX '21: ACM International Conference on Interactive Media Experiences, P329, DOI 10.1145/3452918.3468005
   Condino S, 2018, J HEALTHC ENG, V2018, DOI 10.1155/2018/5435097
   Corbitt C, 2014, HEAD NECK-J SCI SPEC, V36, P1440, DOI 10.1002/hed.23471
   Cui N, 2017, P SPIE 10049 MOL GUI
   Faul F, 2007, BEHAV RES METHODS, V39, P175, DOI 10.3758/BF03193146
   Feng SQ, 2017, CLIN PLAST SURG, V44, P153, DOI 10.1016/j.cps.2016.09.006
   Frantz T, 2018, HEALTHC TECHNOL LETT, V5, P221, DOI 10.1049/htl.2018.5079
   Ghosh D, 2014, P NATL ACAD SCI USA, V111, P13948, DOI 10.1073/pnas.1400821111
   Gsaxner C, 2023, MED IMAGE ANAL, V85, DOI 10.1016/j.media.2023.102757
   Hallock GG., 2009, Flaps and reconstructive surgery
   Herzog M.H., 2019, UNDERSTANDING STAT E, DOI DOI 10.1007/978-3-030-03499-3
   Suñé MCH, 2011, J PLAST RECONSTR AES, V64, P1207, DOI 10.1016/j.bjps.2011.03.015
   HOAGLIN DC, 1986, J AM STAT ASSOC, V81, P991, DOI 10.2307/2289073
   Hummelink S, 2017, J PLAST RECONSTR AES, V70, P871, DOI 10.1016/j.bjps.2017.04.008
   Ishii N, 2016, PRS-GLOB OPEN, V4, DOI 10.1097/GOX.0000000000001161
   Jiang TR, 2017, ANN PLAS SURG, V79, P192, DOI 10.1097/SAP.0000000000001078
   Kersten-Oertel Marta, 2012, Stud Health Technol Inform, V173, P225
   Kimura N., 2009, Flaps and reconstructive surgery
   Klaassen MF., 2018, Simply local flaps, DOI [10.1007/978-3-319-59400-2, DOI 10.1007/978-3-319-59400-2]
   Koo TK, 2016, J CHIROPR MED, V15, P155, DOI 10.1016/j.jcm.2016.02.012
   Kress Bernard C., 2017, SID Symposium Digest of Technical Papers, V48, P127, DOI 10.1002/sdtp.11586
   Kress BC, 2017, PROC SPIE, V10335, DOI 10.1117/12.2270017
   Lalla R, 2003, BRIT J PLAST SURG, V56, P603, DOI 10.1016/S0007-1226(03)00203-0
   Lopes-Ferreira D, 2013, BIOMED RES INT, V2013, DOI 10.1155/2013/238943
   Luparia A, 2013, RADIOL MED, V118, P1119, DOI 10.1007/s11547-013-0941-z
   Luzon JA, 2020, INT J COMPUT ASS RAD, V15, P2027, DOI 10.1007/s11548-020-02263-3
   Mattos LS, 2016, SWISS MED WKLY, V146, DOI 10.4414/smw.2016.14375
   Park S, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11167259
   Pérez-Pachón L, 2021, INT J COMPUT ASS RAD, V16, P955, DOI 10.1007/s11548-021-02354-9
   Pérez-Pachón L, 2020, ADV EXP MED BIOL, V1260, P175, DOI 10.1007/978-3-030-47483-6_10
   Poortman PM, 2009, RADIOTHER ONCOL, V90, P80, DOI 10.1016/j.radonc.2008.07.011
   Pratt P, 2018, EUR RADIOL EXP, V2, DOI 10.1186/s41747-017-0033-2
   Reiss M R, 1997, Laterality, V2, P7, DOI 10.1080/135765097397602
   RICHMOND JD, 1987, BRIT J PLAST SURG, V40, P63, DOI 10.1016/0007-1226(87)90013-0
   Ríos-Buceta L, 2007, ACTAS DERMO-SIFILOGR, V98, P679, DOI 10.1016/S0001-7310(07)70160-6
   Sadideen H, 2013, INT J SURG, V11, P773, DOI 10.1016/j.ijsu.2013.07.001
   Shao PF, 2014, ANN BIOMED ENG, V42, P2228, DOI 10.1007/s10439-014-1062-0
   Sheoran A, 2022, CUREUS J MED SCIENCE, V14, DOI 10.7759/cureus.27329
   Sondak VK, 2010, ANN SURG ONCOL, V17, P670, DOI 10.1245/s10434-009-0857-y
   Sugiura H, 2018, CLIN ORTHOP RELAT R, V476, P1791, DOI 10.1007/s11999.0000000000000158
   Telfer NR, 2008, BRIT J DERMATOL, V159, P35, DOI 10.1111/j.1365-2133.2008.08666.x
   Waljee JF, 2008, ANN SURG ONCOL, V15, P1297, DOI 10.1245/s10434-007-9777-x
   Wilson JL., 2009, Flaps and reconstructive surgery
   Zhang JM, 2023, INT ORTHOP, V47, P611, DOI 10.1007/s00264-022-05663-z
NR 49
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR 6
PY 2024
VL 28
IS 2
AR 68
DI 10.1007/s10055-024-00960-x
PG 10
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA LG6N8
UT WOS:001185672700002
OA hybrid
DA 2024-08-05
ER

PT J
AU Cecotti, H
   Huisinga, L
   Peláez, LG
AF Cecotti, Hubert
   Huisinga, Laura
   Pelaez, Luis Gordo
TI Fully immersive learning with virtual reality for assessing students in
   art history
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Education; Art history
ID EDUCATION
AB Immersive learning systems using consumer grade headsets for virtual reality (VR) software can now reach the classroom. VR represents an important step toward situated learning for bringing realistic experiences to show to students different types of content the way it appears in real life. With the three-dimensional effect, it adds an immersive dimension that can bring the student into a unique environment that is directly connected to the learning content. While VR experiences can be helpful in educational settings, they must satisfy the learning objectives of the course and the specific needs of the students. In this paper, we propose a customizable VR application for displaying paintings for their analysis, and their associated questions designed for instructors in art history. To improve the accessibility and adaptivity to instructors and specific learning materials, we propose the definition of the paintings' characteristics and the questionnaires associated with the paintings in the JSON open-standard file format, facilitating application changes without any prior programming knowledge. We compare the proposed VR modality with a web-based application on a computer desktop with 35 undergraduate students with art history experience. In both modalities, we assess the workload and usability; the VR sickness symptoms and the motivation in the VR condition. The results indicate better usability and lower workload with the VR condition. While there are no differences in terms of students' performance for answering the questions, 77% of students prefer the VR condition. The Reduced Instructional Materials Motivation Survey shows a high motivation in the student population. Finally, the system evaluation supports the conclusion that the proposed VR system can be deployed in the art history classroom as the system has a high usability and medium workload.
C1 [Cecotti, Hubert; Huisinga, Laura; Pelaez, Luis Gordo] Calif State Univ, Fresno, CA 93740 USA.
C3 California State University System; California State University Fresno
RP Cecotti, H (corresponding author), Calif State Univ, Fresno, CA 93740 USA.
EM hcecotti@csufresno.edu
FU National Endowment for the Arts [1856072-38-19]; National Endowment for
   the Art grant
FX The project was funded by the National Endowment for the Art grant
   1856072-38-19.
CR Alhalabi WS, 2016, BEHAV INFORM TECHNOL, V35, P919, DOI 10.1080/0144929X.2016.1212931
   Ames SL, 2005, OPTOMETRY VISION SCI, V82, P168, DOI 10.1097/01.OPX.0000156307.95086.6
   [Anonymous], 2019, Unity Technologies: Unity 2019
   [Anonymous], 2019, Instructure: Canvas
   [Anonymous], 1995, VR Schools
   Bhagat KK, 2016, VIRTUAL REAL-LONDON, V20, P127, DOI 10.1007/s10055-016-0284-x
   Biggs J., 2003, Teaching for quality learning at university Berkshire
   Brooke J., 1996, SUS-a quick and dirty usability scale, DOI [DOI 10.1201/9781498710411-35, DOI 10.1201/9781498710411]
   Bryson S, 2013, Arxiv, DOI [arXiv:1312.4322, 10.48550/arXiv.1312.4322, DOI 10.48550/ARXIV.1312.4322]
   Burdea G. C., 2003, Virtual reality technology
   Casu A, 2015, SMART TOOLS APPS GRA, P77, DOI [10.2312/stag.20151294, DOI 10.2312/STAG.20151294]
   Cecotti H, 2021, P IEEE INT C ENG TEC, P1
   Cecotti H, 2020, P 6 INT C IMM LEARN, P1
   Cecotti H., 2022, Virtual Worlds, V1, P82, DOI [DOI 10.3390/VIRTUALWORLDS1010006, 10.3390/virtualworlds1010006]
   Choi HS, 2017, INT J INFORM MANAGE, V37, P1519, DOI 10.1016/j.ijinfomgt.2016.04.017
   Cortiz D, 2017, C HUM SYST INTERACT, P83, DOI 10.1109/HSI.2017.8005003
   Crockford D., 2017, Tech Rep, DOI [10.13140/RG.2.2.28181.14560, DOI 10.13140/RG.2.2.28181.14560]
   Dawley L., 2013, The Handbook of Research for Educational Communications and Technology, V4th, P723, DOI [DOI 10.1007/978-1-4614-3185-558, DOI 10.1007/978-1-4614-3185-5_58]
   Dede C, 2009, SCIENCE, V323, P66, DOI 10.1126/science.1167311
   Fogden Nigel, Infinite art museum
   Freina L, 2015, ELEARN SOFTW EDUC, P133, DOI 10.12753/2066-026X-15-020
   Fung FM, 2019, J CHEM EDUC, V96, P382, DOI 10.1021/acs.jchemed.8b00728
   Gaitatzes A., 2001, Proceedings of the 2001 conference on virtual reality, archaeology, and cultural heritage, P103, DOI [10.1145/584993.585011, DOI 10.1145/584993.585011]
   Galdieri R, 2019, COMM COM INF SC, V904, P122, DOI 10.1007/978-3-030-05819-7_10
   Gaugne R, 2017, PRESENCE-TELEOP VIRT, V26, P281, DOI [10.1162/PRES_a_00298, 10.1162/pres_a_00298]
   Google, 2016, Tilt brush
   Google, 2011, Google arts and culture
   HART S G, 1988, P139
   Herrington J., 2007, Journal of Computing in Higher Education, V19, P80, DOI 10.1007/BF03033421
   Huang YC, 2014, INT C HUM COMP INT, P579, DOI DOI 10.1007/978-3-319-07857-1_102
   Jackson D, 2019, VOCAT LEARN, V12, P459, DOI 10.1007/s12186-019-09224-1
   Jennett C, 2008, INT J HUM-COMPUT ST, V66, P641, DOI 10.1016/j.ijhcs.2008.04.004
   Jensen L, 2018, EDUC INF TECHNOL, V23, P1515, DOI 10.1007/s10639-017-9676-0
   Kavanagh S., 2017, Themes in Science and Technology Education, V10, P85, DOI DOI 10.1109/ICWT47785.2019.8978263
   Lercari N, 2018, J ARCHAEOL METHOD TH, V25, P368, DOI 10.1007/s10816-017-9340-4
   Liu D, 2017, SMART COMPUT INTELL, P1, DOI 10.1007/978-981-10-5490-7
   Loorbach N, 2015, BRIT J EDUC TECHNOL, V46, P204, DOI 10.1111/bjet.12138
   Lorenz M, 2015, P IEEE VIRT REAL ANN, P223, DOI 10.1109/VR.2015.7223376
   Mathur AS, 2015, P IEEE VIRT REAL ANN, P345, DOI 10.1109/VR.2015.7223437
   Prabaswari AD, 2019, P IOP C SERIES MAT S, V528
   Proulx JN, 2017, SIMULAT GAMING, V48, P81, DOI 10.1177/1046878116674399
   Rubio-Tamayo Jose Luis, 2017, Multimodal Technologies and Interaction, V1, DOI 10.3390/mti1040021
   Ruthenbeck GS, 2015, J SIMUL, V9, P16, DOI 10.1057/jos.2014.14
   Schweibenz W., 1998, Proceedings of the 6th ISI Conference, P185
   Sinclair F., 2016, The VR Museum of Fine Art
   Smithsonian American Art Museum: Smithsonian american art museum, beyond the walls
   The Met, 2023, The metropolitan museum of art
   Wang W, 2013, 2013 INTERNATIONAL CONFERENCE ON VIRTUAL REALITY AND VISUALIZATION (ICVRV 2013), P191, DOI 10.1109/ICVRV.2013.37
   Wolfflin Heinrich., 2012, Principles of Art History
NR 49
TC 0
Z9 0
U1 18
U2 18
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 33
DI 10.1007/s10055-023-00920-x
PG 15
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA FS0R5
UT WOS:001147731500001
OA hybrid
DA 2024-08-05
ER

PT J
AU Dahlstrom-Hakki, I
   Alstad, Z
   Asbell-Clarke, J
   Edwards, T
AF Dahlstrom-Hakki, Ibrahim
   Alstad, Zachary
   Asbell-Clarke, Jodi
   Edwards, Teon
TI The impact of visual and auditory distractions on the performance of
   neurodiverse students in virtual reality (VR) environments
SO VIRTUAL REALITY
LA English
DT Article
DE Neurodiverse; Virtual reality; Autism; Visual search; Cognitive
   processing
ID AUTISM SPECTRUM DISORDER; CHILDREN; NOISE
AB Ambient environmental stimuli may impact how a student is or is not able to apply themselves in cognitive and educational tasks. For neurodivergent learners, these barriers can be compounded as they may be more likely to attend to task-irrelevant ambient noise. The affordances of new systems, such as virtual reality (VR), could be useful for allowing neurodivergent students more deliberate control over what information they experience and what information they do not. This study seeks to explore the dynamics of attention in VR environments. To address this, participants were asked to perform a number of visual search tasks in VR to assess the impact of both visual and auditory distractions on speed and accuracy markers. Results indicate a differential impact of background noise on the performance of neurotypical and neurodivergent participants. Potential benefits to neurodiverse populations and design recommendations in this emerging space are discussed.
C1 [Dahlstrom-Hakki, Ibrahim; Alstad, Zachary; Asbell-Clarke, Jodi; Edwards, Teon] TERC, 2067 Massachusetts Ave, Cambridge, MA 02140 USA.
RP Dahlstrom-Hakki, I (corresponding author), TERC, 2067 Massachusetts Ave, Cambridge, MA 02140 USA.
EM idahlstromhakki@terc.edu
RI Dahlstrom-Hakki, Ibrahim/C-6332-2019
OI Dahlstrom-Hakki, Ibrahim/0000-0002-9418-1900
FU Division of Research on Learning in Formal and Informal Settings
FX No Statement Available
CR [Anonymous], 2015, NEUROTRIBES LEGACY A
   Armstrong T, 2012, Association for Supervision and Curriculum Development
   Baijot S, 2016, BEHAV BRAIN FUNCT, V12, DOI 10.1186/s12993-016-0095-y
   Bashiri Azadeh, 2017, Korean J Pediatr, V60, P337, DOI 10.3345/kjp.2017.60.11.337
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Canu D, 2022, EUR CHILD ADOLES PSY, V31, P1205, DOI 10.1007/s00787-021-01756-z
   Carrozzino M, 2010, J CULT HERIT, V11, P452, DOI 10.1016/j.culher.2010.04.001
   Charlton J.I., 1998, Nothing About Us Without Us: disability oppression and empowerment, V1st, DOI DOI 10.1525/9780520925441
   Checa D, 2020, MULTIMED TOOLS APPL, V79, P5501, DOI 10.1007/s11042-019-08348-9
   Clince M, 2016, AM J OCCUP THER, V70, DOI 10.5014/ajot.2016.016816
   Concannon BJ, 2019, FRONT EDUC, V4, DOI 10.3389/feduc.2019.00080
   Dahlstrom-Hakki I, 2021, 2021 7TH INTERNATIONAL CONFERENCE OF THE IMMERSIVE LEARNING RESEARCH NETWORK (ILRN), P61, DOI 10.23919/iLRN52045.2021.9459322
   Dahlstrom-Hakki IH, 2019, LEARN DISABILITY Q, V42, P175, DOI 10.1177/0731948718817222
   Davis NO, 2012, NEUROTHERAPEUTICS, V9, P518, DOI 10.1007/s13311-012-0126-9
   Edwards T, 2022, UC IRVINE
   Fischer M, 2017, MW17 MW 2017
   Gaines KS, 2014, J ARCHIT PLAN RES, V31, P282
   Garcia-Cardona S, 2017, Revised Selected Papers, V11, P20
   Godwin KE, 2022, MIND BRAIN EDUC, V16, P239, DOI 10.1111/mbe.12319
   Goodwin MS, 2008, FOCUS AUTISM DEV DIS, V23, P125, DOI 10.1177/1088357608316678
   Hamilton D, 2021, J COMPUT EDUC, V8, P1, DOI 10.1007/s40692-020-00169-2
   Huang HM, 2018, INT REV RES OPEN DIS, V19, P91
   Huber DE, 2003, P ANN M COGN SCI SOC, V25
   Hutson P., 2022, Creat Educ, V13, P9, DOI [10.4236/ce.2022.139193, DOI 10.4236/CE.2022.139193]
   Izzo F., 2017, Mod Econ, V8, P4, DOI [10.4236/me.2017.84040, DOI 10.4236/ME.2017.84040]
   Jensen L, 2018, EDUC INF TECHNOL, V23, P1515, DOI 10.1007/s10639-017-9676-0
   Kalyvioti K, 2014, PROCEDIA COMPUT SCI, V27, P138, DOI 10.1016/j.procs.2014.02.017
   Kanakri SM, 2017, RES DEV DISABIL, V63, P85, DOI 10.1016/j.ridd.2017.02.004
   Koterwas T, 2018, PROGR IS, P365, DOI 10.1007/978-3-319-64027-3_25
   Kulik TK, 2016, CURATOR, V59, P27, DOI 10.1111/cura.12143
   Little LM, 2014, AM J OCCUP THER, V68, P177, DOI 10.5014/ajot.2014.009894
   Lukava T, 2022, J ENABLING TECHNOL, V16, P75, DOI 10.1108/JET-03-2022-0025
   Martin R, 2022, INTERV SCH CLIN, V57, P32, DOI 10.1177/10534512211014882
   McCormick C, 2016, AUTISM, V20, P572, DOI 10.1177/1362361315599755
   Mikropoulos TA, 2011, COMPUT EDUC, V56, P769, DOI 10.1016/j.compedu.2010.10.020
   Morgan H., 2019, PSU McNair Scholars Online Journal, V13, DOI [10.15760/mcnair.2019.13.1.11, DOI 10.15760/MCNAIR.2019.13.1.11]
   NEISSER U, 1964, SCI AM, V210, P94, DOI 10.1038/scientificamerican0664-94
   O'Riordan MA, 2001, J EXP PSYCHOL HUMAN, V27, P719, DOI 10.1037//0096-1523.27.3.719
   Parmar KR, 2021, FRONT PSYCHOL, V12, DOI 10.3389/fpsyg.2021.633037
   Parsons S, 2004, J AUTISM DEV DISORD, V34, P449, DOI 10.1023/B:JADD.0000037421.98517.8d
   Parsons S, 2016, EDUC RES REV-NETH, V19, P138, DOI 10.1016/j.edurev.2016.08.001
   Pickens TA, 2019, COMPLEMENT THER MED, V42, P151, DOI 10.1016/j.ctim.2018.11.012
   Rogers Sol., 2019, FORBES
   Rospigliosi P, 2022, INTERACT LEARN ENVIR, V30, P1, DOI 10.1080/10494820.2022.2022899
   Rucklidge JJ, 2010, PSYCHIAT CLIN N AM, V33, P357, DOI 10.1016/j.psc.2010.01.006
   Russell AE, 2016, CHILD PSYCHIAT HUM D, V47, P440, DOI 10.1007/s10578-015-0578-3
   Santuzzi A M., 2022, Neurodiversity in the Workplace, P124, DOI DOI 10.4324/9781003023616-5
   Sarrett JC, 2016, NEUROETHICS-NETH, V9, P23, DOI 10.1007/s12152-016-9247-x
   Schmidt M, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.611740
   Seernani D, 2021, EUR CHILD ADOLES PSY, V30, P549, DOI 10.1007/s00787-020-01535-2
   Shi Y, 2021, JAMA NETW OPEN, V4, DOI 10.1001/jamanetworkopen.2021.0321
   Singer J., 1998, ODD PEOPLE BIRTH COM
   So HJ, 2008, COMPUT EDUC, V51, P318, DOI 10.1016/j.compedu.2007.05.009
   Söderlund GBW, 2010, BEHAV BRAIN FUNCT, V6, DOI 10.1186/1744-9081-6-55
   Souchet AD, 2023, VIRTUAL REAL-LONDON, V27, P19, DOI 10.1007/s10055-022-00672-0
   Sylaiou S, 2017, INT J ARTS TECHNOL, V10, P58, DOI 10.1504/IJART.2017.10004738
   Top DN, 2019, FRONT PSYCHIATRY, V9, DOI 10.3389/fpsyt.2018.00783
   Vivanti G, 2018, DEV COGN NEUROS-NETH, V29, P54, DOI 10.1016/j.dcn.2017.01.006
   Walczak K, 2006, COMPUTER, V39, P93, DOI 10.1109/MC.2006.108
   Wang M, 2011, NEUROEPIDEMIOLOGY, V36, P2, DOI 10.1159/000320847
   Williams RM, 2020, INT J HUM-COMPUT ST, V143, DOI 10.1016/j.ijhcs.2020.102485
   Wilson JR, 2018, CAN J SCHOLARSH TEA, V9
   Wolfe JM, 2020, ANNU REV VIS SCI, V6, P539, DOI [10.1146/annurev-vision-091718015048, 10.1146/annurev-vision-091718-015048]
   Yung R, 2019, CURR ISSUES TOUR, V22, P2056, DOI 10.1080/13683500.2017.1417359
   Zablotsky B, 2020, J ATTEN DISORD, V24, P94, DOI 10.1177/1087054717713638
   Zhou YT, 2022, EDUC RES REV-NETH, V36, DOI 10.1016/j.edurev.2022.100454
   Zolyomi Annuska, 2021, Proceedings of the ACM on Human-Computer Interaction, V5, DOI 10.1145/3449151
NR 67
TC 0
Z9 0
U1 8
U2 8
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 29
DI 10.1007/s10055-023-00933-6
PG 14
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA FR1V3
UT WOS:001147497600001
OA hybrid
DA 2024-08-05
ER

PT J
AU Kim, J
   Kim, J
   Jung, M
   Kwon, T
   Kim, KK
AF Kim, Jihwan
   Kim, Jejoong
   Jung, Myeongul
   Kwon, Taesoo
   Kim, Kwanguk Kenny
TI Individualized foveated rendering with eye-tracking head-mounted display
SO VIRTUAL REALITY
LA English
DT Article
DE Foveated rendering; Individualization; Head-mounted display;
   Eye-tracking; Virtual reality; User study
ID FIELD-OF-VIEW; GOLDMANN; VISION; DELAY
AB Foveated rendering (FR) technology is designed to improve the efficiency of graphical rendering processes. In rendering, individualized approaches can help to balance users' experiences of visual quality and saving computational resource. However, previous studies have not rigorously examined it related with the FR techniques. To address this issue, we developed an individualized FR (IFR) method using different central vision sizes and peripheral vision resolutions across individuals in virtual reality. In three user studies with 88 participants who were divided into groups designated as "large central area (LCA)" and "small central area (SCA)," the effects of IFR were compared with those of using the full-resolution condition and the average FR condition. The results indicate that the LCA group experienced higher visual quality under the IFR and full-resolution conditions than under the average FR condition. In contrast, the SCA group exhibited comparable levels of dependent measures between the IFR and average FR conditions, but both were lower than those of the full-resolution condition. We also evaluated the computational benefits of the proposed IFR method, and the results demonstrated the effectiveness of our approach in saving resources across the two groups. Although lower-bound adjustments may be required for some users, our overall results suggest that IFR is a malleable technology for enhancing rendering efficiency in virtual reality.
C1 [Kim, Jihwan; Jung, Myeongul; Kwon, Taesoo; Kim, Kwanguk Kenny] Hanyang Univ, Dept Comp Sci, Seoul, South Korea.
   [Kim, Jejoong] Duksung Womens Univ, Dept Psychol, Seoul, South Korea.
C3 Hanyang University; Duksung Women's University
RP Kim, KK (corresponding author), Hanyang Univ, Dept Comp Sci, Seoul, South Korea.
EM kenny@hanyang.ac.kr
RI Jung, Myeongul/KWU-7181-2024
OI Jung, Myeongul/0000-0001-9432-103X; Kim, Jihwan/0000-0002-3357-1673
FU National Research Foundation of Korea; National Research Foundation of
   Korea (NRF) [2021R1A2C2013479, 2021-0-00590]; Institute of Information &
   Communications Technology Planning & Evaluation (IITP) - Korea
   government
FX This work was supported by the National Research Foundation of Korea
   (NRF) and an Institute of Information & Communications Technology
   Planning & Evaluation (IITP) grant funded by the Korea government (No.
   2021R1A2C2013479 and 2021-0-00590). *Ministry of Science and ICT.
   Correspondence to K. Kim (kenny@hanyang.ac.kr).
CR Adhanom IB, 2023, VIRTUAL REAL-LONDON, V27, P1481, DOI 10.1007/s10055-022-00738-z
   Adhanom IB, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P645, DOI [10.1109/VR46266.2020.00-17, 10.1109/VR46266.2020.1581314696458]
   BANKS MS, 1991, J OPT SOC AM A, V8, P1775, DOI 10.1364/JOSAA.8.001775
   Bargary G, 2017, VISION RES, V141, P157, DOI 10.1016/j.visres.2017.03.001
   Chen TZ, 2018, 24TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2018), DOI 10.1145/3281505.3281516
   Cheung SH, 2005, VISUAL NEUROSCI, V22, P187, DOI 10.1017/S0952523805222071
   Derogatis LR, 2010, CORSINI ENCY PSYCHOL, DOI [10.1002/9780470479216.corpsy0970, DOI 10.1002/9780470479216.CORPSY0970]
   Di Luca M, 2010, PRESENCE-TELEOP VIRT, V19, P569, DOI 10.1162/pres_a_00023
   Emery KJ, 2019, CURR OPIN BEHAV SCI, V30, P28, DOI 10.1016/j.cobeha.2019.05.002
   GOLDMANN H, 1946, OPHTHALMOLOGICA, V111, P187, DOI 10.1159/000300322
   Guenter B, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366183
   Hirzle T, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300855
   Hsu CF, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P55, DOI 10.1145/3123266.3123434
   Jacobson SG, 2007, P NATL ACAD SCI USA, V104, P15123, DOI 10.1073/pnas.0706367104
   Jang W, 2016, COMPUT METH PROG BIO, V135, P115, DOI 10.1016/j.cmpb.2016.07.026
   Kennedy R.S., 1993, Int. J. Aviat. Psy, P203
   Kim M, 2018, INT J SOC ROBOT, V10, P33, DOI 10.1007/s12369-017-0428-5
   Korkut EH, 2023, VIRTUAL REAL-LONDON, V27, P1447, DOI 10.1007/s10055-023-00753-8
   Levoy M., 1990, Computer Graphics, V24, P217, DOI 10.1145/91394.91449
   Lin JJW, 2002, P IEEE VIRT REAL ANN, P164, DOI 10.1109/VR.2002.996519
   Masnadi S, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517548
   McAuley Stephen, 2013, SIGGRAPH 13 COURSES, DOI [10.1145/2504435.2504457, DOI 10.1145/2504435.2504457]
   Meng XX, 2020, IEEE T VIS COMPUT GR, V26, P1972, DOI 10.1109/TVCG.2020.2973442
   Murray IJ, 2012, J VISION, V12, DOI 10.1167/12.1.18
   NAVARRO R, 1993, J OPT SOC AM A, V10, P201, DOI 10.1364/JOSAA.10.000201
   OGBOSO YU, 1987, J OPT SOC AM A, V4, P1666, DOI 10.1364/JOSAA.4.001666
   Overbeck RS, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275031
   Pai YS, 2019, VIRTUAL REAL-LONDON, V23, P119, DOI 10.1007/s10055-018-0371-2
   Park SH, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI [10.1145/3491102.3501847, 10.1016/j.cap.2022.08.010]
   Patney A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980246
   Sanda N, 2018, BRAIN STRUCT FUNCT, V223, P3473, DOI 10.1007/s00429-018-1700-7
   Sekuler AB, 2000, EXP AGING RES, V26, P103, DOI 10.1080/036107300243588
   Sheridan TB., 1992, Presence: Teleoperators and Virtual Environments, V1, P120, DOI [10.1162/pres.1992.1.1.120, DOI 10.1162/PRES.1992.1.1.120]
   Stengel M, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P15, DOI 10.1145/2733373.2806265
   Strasburger H, 2011, J VISION, V11, DOI 10.1167/11.5.13
   Swafford Nicholas T., 2016, P ACM S APPL PERC, P7, DOI [10.1145/2931002.2931011, DOI 10.1145/2931002.2931011]
   Tian N, 2022, VIRTUAL REAL-LONDON, V26, P1409, DOI 10.1007/s10055-022-00638-2
   TROPE GE, 1987, BRIT J OPHTHALMOL, V71, P489, DOI 10.1136/bjo.71.7.489
   Usoh M, 2000, PRESENCE-TELEOP VIRT, V9, P497, DOI 10.1162/105474600566989
   Vaidyanathan K., 2014, P HIGH PERF GRAPH, P9
   van Dam LCJ, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0205145
   WALD G, 1945, SCIENCE, V101, P653, DOI 10.1126/science.101.2635.653
   Welch RB, 1996, PRESENCE-TELEOP VIRT, V5, P263, DOI 10.1162/pres.1996.5.3.263
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   Xiao K, 2018, ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES (I3D 2018), DOI 10.1145/3190834.3190850
   Zahid S, 2014, ADV EXP MED BIOL, V801, P131, DOI 10.1007/978-1-4614-3209-8_17
NR 46
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 25
DI 10.1007/s10055-023-00931-8
PG 18
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA FH2O3
UT WOS:001144808100002
OA hybrid
DA 2024-08-05
ER

PT J
AU Zhao, CM
   Beams, R
   Dahal, E
   Badano, A
AF Zhao, Chumin
   Beams, Ryan
   Dahal, Eshan
   Badano, Aldo
TI Spatially dependent veiling glare degrades image quality in medical
   extended reality
SO VIRTUAL REALITY
LA English
DT Article
DE Veiling glare; Medical extended reality; Augmented reality; Virtual
   reality; Head-mounted display; Medical image quality
ID VIRTUAL-REALITY; AUGMENTED REALITY; MIXED REALITY; DISPLAY; PERFORMANCE;
   CALIBRATION; SYSTEM
AB Spatially dependent veiling glare in medical extended reality (MXR) degrades image quality and needs to be characterized across technologies. Measurement methods of veiling glare on virtual and augmented reality (VR and AR) head-mounted displays (HMDs) have not been established in regulatory evaluation of MXR devices or display measurement standards. We describe an experimental bench setup enabling highly flexible translations and rotations of the light measuring device in five degrees of freedom within the eye box. Glare point spread functions (PSFs) of multiple VR and AR headsets are extracted and compared across the display field of view (FOV) in dark environment. At the center, the evaluated VR headsets (HTC VIVE Pro and VIVE Pro 2) demonstrate reduced long-range glare compared to the tested AR HMDs (Microsoft HoloLens 2 and Epson Moverio BT-300). The measured PSFs at multiple locations are spatially invariant for the HoloLens 2. However, veiling glare on the evaluated VR HMDs substantially increases toward the periphery of the FOV primarily due to optical aberration. For VR devices in medical use, increased peripheral glare can lead to image quality degradation and poor visualization of anatomical details.
C1 [Zhao, Chumin; Beams, Ryan; Dahal, Eshan; Badano, Aldo] US FDA, Ctr Devices & Radiol Hlth, 10903 New Hampshire Ave, Silver Spring, MD 20993 USA.
C3 US Food & Drug Administration (FDA)
RP Zhao, CM (corresponding author), US FDA, Ctr Devices & Radiol Hlth, 10903 New Hampshire Ave, Silver Spring, MD 20993 USA.
EM chumin.zhao@fda.hhs.gov; ryan.beams@fda.hhs.gov;
   eshan.dahal@fda.hhs.gov; aldo.badano@fda.hhs.gov
RI Dahal, Eshan/K-3253-2016
FU Department of Health and Human Services
FX The authors would like to acknowledge Dr. Qian Cao (CDRH/FDA) for
   providing the micro-CT image for analysis. We also thank Drs. Paul
   Lemaillet and Valentyn Stadnytskyi (CDRH/FDA) for technical reviews. The
   mention of commercial products, their resources or their use in
   connection with material reported herein is not to be construed as
   either an actual or implied endorsement of such products by the
   Department of Health and Human Services. This is a contribution of the
   U.S. Food and Drug Administration and is not subject to copyright.
CR Albert R, 2017, ACM T APPL PERCEPT, V14, DOI 10.1145/3127589
   [Anonymous], 1994, ISO 9358:1994
   [Anonymous], 2019, IEC 63145-20-10:2019
   Badano A, 2000, APPL OPTICS, V39, P2059, DOI 10.1364/AO.39.002059
   Badano A, 1999, SPIE, P458
   Bader A, 2022, Proceedings Volume 11931, Optical Architectures for Displays and Sensing in Augmented, Virtual, and Mixed Reality (AR, VR, MR) III
   Beams R, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P836, DOI [10.1109/VR46266.2020.00010, 10.1109/VR46266.2020.1581603313456]
   Beams R, 2020, PROC SPIE, V11310, DOI 10.1117/12.2543257
   Beams R, 2019, OPT EXPRESS, V27, P24877, DOI 10.1364/OE.27.024877
   Becker ME, 2017, SID S WIL ONL LIB, P915
   Bohil CJ, 2011, NAT REV NEUROSCI, V12, P752, DOI 10.1038/nrn3122
   Choi M, 2014, J DISP TECHNOL, V10, P420, DOI 10.1109/JDT.2013.2279933
   Dahal E, 2022, SPIE
   Dahal E, 2023, OPT CONTINUUM, V2, P1180, DOI 10.1364/OPTCON.486520
   Douglas David B., 2017, Multimodal Technologies and Interaction, V1, DOI 10.3390/mti1040029
   Draper Russell S., 2018, SID Symposium Digest of Technical Papers, V49, P961, DOI 10.1002/sdtp.12209
   Elbamby MS, 2018, IEEE NETWORK, V32, P78, DOI 10.1109/MNET.2018.1700268
   Flynn MJ, 1999, J DIGIT IMAGING, V12, P50, DOI 10.1007/BF03168843
   Flynn MJ, 1999, RADIOGRAPHICS, V19, P1653, DOI 10.1148/radiographics.19.6.g99no081653
   Fried Marvin P, 2007, Curr Opin Otolaryngol Head Neck Surg, V15, P163, DOI 10.1097/MOO.0b013e32814b0802
   Izard SG, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20102962
   Goo HW, 2020, KOREAN J RADIOL, V21, P133
   Jang DP, 2002, IEEE T INF TECHNOL B, V6, P213, DOI 10.1109/TITB.2002.802374
   Johnson M, 2022, IEEE INT SYMP M AU R, P477, DOI 10.1109/ISMAR-Adjunct57072.2022.00100
   Kim A, 2021, PROC SPIE, V11599, DOI 10.1117/12.2580736
   Kim AS, 2021, J DIGIT IMAGING, V34, P16, DOI 10.1007/s10278-020-00392-4
   Kress BC, 2021, NANOPHOTONICS-BERLIN, V10, P41, DOI 10.1515/nanoph-2020-0410
   Kress BC, 2019, PROC SPIE, V11062, DOI 10.1117/12.2544404
   Krupinski EA, 2004, P SOC PHOTO-OPT INS, V5372, P423, DOI 10.1117/12.532361
   Livingston MarkA., 2013, Human Factors in Augmented Reality Environments, P35, DOI [DOI 10.1007/978-I-461, DOI 10.1007/978-1-4614-4205-93]
   Mosso-Vázquez JL, 2014, CYBERPSYCH BEH SOC N, V17, P371, DOI 10.1089/cyber.2014.0198
   Ma LF, 2019, MED BIOL ENG COMPUT, V57, P47, DOI 10.1007/s11517-018-1861-9
   Malloy KM, 2010, CLIN PSYCHOL REV, V30, P1011, DOI 10.1016/j.cpr.2010.07.001
   McCann JJ, 2007, PROC SPIE, V6492, DOI 10.1117/12.703042
   Moody Louise, 2008, Virtual Reality, V12, P77, DOI 10.1007/s10055-007-0080-8
   Park J, 2009, IEEE T CONSUM ELECTR, V55, P987, DOI 10.1109/TCE.2009.5278053
   Penczek John, 2019, SID Symposium Digest of Technical Papers, V50, P430, DOI 10.1002/sdtp.12948
   Penczek J, 2021, SID S WIL ONL LIB, P702
   Petrak O, 2021, SPIE, P15
   Porsch T, 2015, CIE P, P1471
   Pratt P, 2018, EUR RADIOL EXP, V2, DOI 10.1186/s41747-017-0033-2
   Ren D, 2016, P IEEE VIRT REAL ANN, P93, DOI 10.1109/VR.2016.7504692
   Roehrig H, 2002, P SOC PHOTO-OPT INS, V4786, P114, DOI 10.1117/12.456332
   Samei E, 2005, MED PHYS, V32, P1205, DOI 10.1118/1.1861159
   Slyk S, 2019, EXPERT REV MED DEVIC, V16, P1035, DOI 10.1080/17434440.2019.1693892
   Sutherland J, 2019, J DIGIT IMAGING, V32, P38, DOI 10.1007/s10278-018-0122-7
   Talvala EV, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239488
   Wang JC, 2015, COMPUT MED IMAG GRAP, V40, P147, DOI 10.1016/j.compmedimag.2014.11.003
   WENG JY, 1992, IEEE T PATTERN ANAL, V14, P965, DOI 10.1109/34.159901
   Xiong ZY, 2023, CURR APPL PHYS, V45, P6, DOI 10.1016/j.cap.2022.10.007
   Yin K, 2021, J PHYS-PHOTONICS, V3, DOI 10.1088/2515-7647/abf02e
   Zhao C, 2022, SID S WIL ONL LIB, P194
   Zhao CM, 2023, J SOC INF DISPLAY, V31, P387, DOI 10.1002/jsid.1208
   Zhao CM, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-24345-9
NR 54
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 11
DI 10.1007/s10055-023-00893-x
PG 15
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA EC7V9
UT WOS:001136790600002
OA hybrid
DA 2024-08-05
ER

PT J
AU Singh, M
   Smitham, P
   Jain, S
   Day, C
   Nijman, T
   George, D
   Neilly, D
   de Blasio, J
   Gilmore, M
   Gill, TK
   Proudman, S
   Nimon, G
AF Singh, Mantaj
   Smitham, Peter
   Jain, Suyash
   Day, Christopher
   Nijman, Thomas
   George, Dan
   Neilly, David
   de Blasio, Justin
   Gilmore, Michael
   Gill, Tiffany K.
   Proudman, Susanna
   Nimon, Gavin
TI Exploring the viability of Virtual Reality as a teaching method for knee
   aspiration
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Medical education; Knee aspiration
ID INJECTIONS; SIMULATION; JOINT
AB Knee arthrocentesis is a simple procedure commonly performed by general practitioners and junior doctors. As such, doctors should be competent and comfortable in performing the technique by themselves; however, they need to be adequately trained. The best method to ensure practitioner proficiency is by optimizing teaching at an institutional level, thus, educating all future doctors in the procedure. However, the Coronavirus Disease 19 (COVID-19) pandemic caused significant disruption to hospital teaching for medical students which necessitated investigating the effectiveness of virtual reality (VR) as a platform to emulate hospital teaching of knee arthrocentesis. A workshop was conducted with 100 fourth year medical students divided into three Groups: A, B and C, each receiving a pre-reading online lecture. Group A was placed in an Objective Structured Clinical Examination (OSCE) station where they were assessed by a blinded orthopaedic surgeon using the OSCE assessment rubric. Group B undertook a hands-on practice station prior to assessment, while Group C received a VR video (courtesy of the University of Adelaide's Health Simulation) in the form of VR headset or 360 degrees surround immersion room and hands-on station followed by the OSCE. Upon completion of the workshop, students completed a questionnaire on their confidence with the procedure and the practicality of the VR station. OSCE scores were compared between Groups B and C to investigate the educational value of VR teaching. On average, students with VR headsets reported higher confidence with the procedure and were more inclined to undertake it on their own. Students in Group C who used the VR station prior to assessment scored higher than the non-VR Groups (Group A, 56%; Group B, 67%; Group C 83%). Students in Group A had statistically significant results on average compared to those in Group B (t(69) = 3.003, p = 0.003), as do students in Group B compared to Group C (t(62) = 5.400, p < 0.001). Within Group C students who were given VR headsets scored higher than immersion room students. The VR headset was beneficial in providing students with a representation of how knee arthrocentesis may be conducted in the hospital setting. While VR will not replace conventional in-hospital teaching, given current technological limitations, it serves as an effective teaching aid for arthrocentesis and has many other potential applications for a wide scope of medicine and surgical training.
C1 [Singh, Mantaj; Smitham, Peter; Jain, Suyash; Day, Christopher; Nijman, Thomas; George, Dan; Neilly, David; de Blasio, Justin; Gilmore, Michael; Gill, Tiffany K.; Proudman, Susanna; Nimon, Gavin] Univ Adelaide, Royal Adelaide Hosp, Orthopead Dept, 5 Pkwy Ave, Walkley Hts, SA 5098, Australia.
C3 University of Adelaide; Royal Adelaide Hospital
RP Singh, M (corresponding author), Univ Adelaide, Royal Adelaide Hosp, Orthopead Dept, 5 Pkwy Ave, Walkley Hts, SA 5098, Australia.
EM mantaj.singh@sa.gov.au; peter.smitham@adelaide.edu.au;
   suyash.jain@adelaide.edu.au; daychrisw@gmail.com;
   thomas.nijman@sa.gov.au; dmgeorge14@gmail.com; david.neilly@nhs.scot;
   deblasio.justin@gmail.com; Michael.gilmour@adelaide.edu.au;
   tiffany.gill@adelaide.edu.au; susanna.proudman@adelaide.edu.au;
   gavin.nimon@adelaide.edu.au
FU The University of Adelaide
FX No Statement AvailableDAS:The datasets generated during and/or analysed
   during the current study are not publicly available due to student
   confidentiality but are available from the corresponding author on
   reasonable request after being de-identified. Please refer to Tables 1-3
   for relevant results.
CR Aggarwal R, 2007, ANN SURG, V246, P771, DOI 10.1097/SLA.0b013e3180f61b09
   Ahlberg G, 2007, AM J SURG, V193, P797, DOI 10.1016/j.amjsurg.2006.06.050
   Althwanay A, 2020, CUREUS J MED SCIENCE, V12, DOI 10.7759/cureus.10775
   [Anonymous], 2019, Fundamental Surgery becomes the First VR Surgical Training Simulation with Haptics to Gain CPD Accreditation
   Arshad A, 2021, ACTA ORTHOP, V92, P129, DOI 10.1080/17453674.2020.1845437
   Berman JR, 2012, JCR-J CLIN RHEUMATOL, V18, P175, DOI 10.1097/RHU.0b013e318258259e
   Chiowchanwisawakit P, 2015, INT J RHEUM DIS, V18, P742, DOI 10.1111/1756-185X.12664
   De Ponti R, 2020, BMC MED EDUC, V20, DOI 10.1186/s12909-020-02245-8
   Douglas Robert J, 2014, Knee Surg Relat Res, V26, P1, DOI 10.5792/ksrr.2014.26.1.1
   Drozd Brandy, 2018, JMIR Med Educ, V4, pe3, DOI 10.2196/mededu.8527
   Ekstrand Chelsea, 2018, CMAJ Open, V6, pE103, DOI 10.9778/cmajo.20170110
   Fischer J, 2013, JCR-J CLIN RHEUMATOL, V19, P373, DOI 10.1097/RHU.0b013e3182a69fb2
   Francis ER, 2020, J SURG EDUC, V77, P947, DOI 10.1016/j.jsurg.2020.02.013
   Hamilton D, 2021, J COMPUT EDUC, V8, P1, DOI 10.1007/s40692-020-00169-2
   Jolly M, 2003, JCR-J CLIN RHEUMATOL, V9, P187, DOI 10.1097/01.RHU.0000073587.90836.23
   Ladurner A, 2020, BMC MED EDUC, V20, DOI 10.1186/s12909-020-02174-6
   Liddell WG, 2005, RHEUMATOLOGY, V44, P1043, DOI 10.1093/rheumatology/keh683
   Mabrey JD, 2010, CLIN ORTHOP RELAT R, V468, P2586, DOI 10.1007/s11999-010-1426-1
   Mashov Rita, 2011, Harefuah, V150, P242
   Masterton G, 2021, J PLAST RECONSTR AES, V74, P1143, DOI 10.1016/j.bjps.2020.10.105
   Padilha JM, 2019, J MED INTERNET RES, V21, DOI 10.2196/11529
   Ryan GV, 2022, J MED INTERNET RES, V24, DOI 10.2196/30082
   Samadbeik Mahnaz, 2018, J Adv Med Educ Prof, V6, P123
   Sattar MU, 2020, INT J EMERG TECHNOL, V15, P160, DOI 10.3991/ijet.v15i02.11394
   Savage N, 2020, ANZ J SURG, V90, P1244, DOI 10.1111/ans.16103
   Sleiwah A, 2020, J PLAST RECONSTR AES, V73, P1578, DOI 10.1016/j.bjps.2020.05.032
   Stroud L, 2013, J GEN INTERN MED, V28, P723, DOI 10.1007/s11606-012-2314-z
   Washington E, 2019, EDUC MEDIA TECH YEAR, V42, P67, DOI 10.1007/978-3-030-27986-8_7
   Westwood JD., 2005, Medicine meets Virtual Reality 13: the magical next becomes the medical now
NR 29
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD JUL 19
PY 2024
VL 28
IS 3
AR 139
DI 10.1007/s10055-024-01027-7
PG 10
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA ZC0B0
UT WOS:001272962700001
OA hybrid
DA 2024-08-05
ER

PT J
AU Kristína, V
   Dagmar, S
   Oto, J
   Lenka, J
   Katerina, B
   Vojtech, J
AF Kristina, Varsova
   Dagmar, Szitas
   Oto, Janousek
   Lenka, Jurkovicova
   Katerina, Bartosova
   Vojtech, Jurik
TI Virtual reality exposure effect in acrophobia: psychological and
   physiological evidence from a single experimental session
SO VIRTUAL REALITY
LA English
DT Article
DE Acrophobia; Cognitive-behavioral therapy; HRV; iVR; Virtual reality;
   VRET
ID SOCIAL ANXIETY DISORDER; VISUAL HEIGHT INTOLERANCE; HEART-RATE;
   METAANALYSIS; THERAPY
AB In recent years, virtual reality (VR) has gained attention from researchers in diverse fields, particularly in therapy of phobias. Currently, virtual reality exposure therapy therapy (VRET) is considered a promising cognitive-behavioral therapy technique. However, specific psychological and physiological responses of VR users to virtual exposure in such a context are still only vaguely explored. In this experimental study, we mapped VR exposure in a height environment in people with a moderate fear of heights-acrophobia. Thirty-six participants were divided into experimental and control groups-with and without psychological guidance during exposure. Participants' subjective level of anxiety was examined, and objective physiological response was captured via heart rate variability (HRV) measurement. Psychological assessments recorded an anticipated rise in participant anxiety following exposure to height; nevertheless, no distinctions were observed in self-reported anxiety concerning psychological guidance. Notably, objective physiological measures revealed that VR exposure prompts physiological responses akin to real-world scenarios. Moreover, based on the analysis of heart rate variability, participants who received psychological guidance were identified as better at compensating for anxiety compared to those without such support. These findings support VRET as a promising tool for psychotherapy and advocate for psychological guidance as beneficial in reducing anxiety and managing stress during exposure. The results may help improve our understanding of anxiety during exposure to phobic stimuli.
C1 [Kristina, Varsova; Dagmar, Szitas; Katerina, Bartosova; Vojtech, Jurik] Masaryk Univ, Fac Arts, Dept Psychol, Brno, Czech Republic.
   [Oto, Janousek] Brno Univ Technol, Fac Elect Engn & Commun, Dept Biomed Engn, Brno, Czech Republic.
   [Lenka, Jurkovicova] Masaryk Univ, St Annes Univ Hosp, Dept Neurol, Brno, Czech Republic.
   [Lenka, Jurkovicova] Masaryk Univ, Med Fac, Brno, Czech Republic.
   [Lenka, Jurkovicova] Masaryk Univ, CEITEC Cent European Inst Technol, Brno, Czech Republic.
   [Dagmar, Szitas] Comenius Univ, Inst Appl Psychol, Fac Social & Econ Sci, Bratislava, Slovakia.
   [Vojtech, Jurik] Brno Univ Technol, Inst Comp Aided Engn & Comp Sci, Fac Civil Engn, Brno, Czech Republic.
C3 Masaryk University Brno; Brno University of Technology; St Anne's
   University Hospital Brno (FNUSA-ICRC); Masaryk University Brno; Masaryk
   University Brno; Masaryk University Brno; Comenius University
   Bratislava; Brno University of Technology
RP Vojtech, J (corresponding author), Masaryk Univ, Fac Arts, Dept Psychol, Brno, Czech Republic.; Vojtech, J (corresponding author), Brno Univ Technol, Inst Comp Aided Engn & Comp Sci, Fac Civil Engn, Brno, Czech Republic.
EM jurik.vojtech@mail.muni.cz
OI Szitas, Dagmar/0009-0008-1024-6625
FU Masarykova Univerzita
FX The study was supported by the research infrastructure GREY Lab held at
   the Department of Psychology, Faculty of Arts, Masaryk University,
   Brno.DAS:The datasets produced and examined in this study can be
   obtained from the corresponding author upon a reasonable request.
CR Alcañiz M, 2009, PRESENCE-TELEOP VIRT, V18, P97, DOI 10.1162/pres.18.2.97
   Alfaro L, 2023, 2023 IEEE WORLD ENGINEERING EDUCATION CONFERENCE, EDUNINE, DOI 10.1109/EDUNINE57531.2023.10102862
   Alsaleh S, 2021, 2021 44TH INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS AND SIGNAL PROCESSING (TSP), P340, DOI 10.1109/TSP52935.2021.9522676
   Anderson PL, 2013, J CONSULT CLIN PSYCH, V81, P751, DOI 10.1037/a0033559
   Andringa TC, 2013, INT J ENV RES PUB HE, V10, P1439, DOI 10.3390/ijerph10041439
   Arroll B, 2017, MED J AUSTRALIA, V206, P263, DOI 10.5694/mja16.00540
   Beck J.S., 2011, COGNITIVE BEHAV THER
   Behling O., 2000, Translating questionnaires and other research instruments: Problems and solutions, DOI DOI 10.4135/9781412986373
   Benito KG, 2015, J OBSESS-COMPULS REL, V6, P147, DOI 10.1016/j.jocrd.2015.01.006
   Boeldt D, 2019, FRONT PSYCHIATRY, V10, DOI 10.3389/fpsyt.2019.00773
   Botella C, 2017, CURR PSYCHIAT REP, V19, DOI 10.1007/s11920-017-0788-4
   Buchholz JL, 2020, J ANXIETY DISORD, V70, DOI 10.1016/j.janxdis.2020.102194
   Camm AJ, 1996, CIRCULATION, V93, P1043
   Chou PH, 2021, J AFFECT DISORDERS, V282, P786, DOI 10.1016/j.jad.2020.12.172
   Coelho CM, 2009, J ANXIETY DISORD, V23, P563, DOI 10.1016/j.janxdis.2009.01.014
   Concannon BJ, 2020, JMIR SERIOUS GAMES, V8, DOI 10.2196/18313
   Demir M.D., 2023, Psikiyatr. Guncel YaklasimlarCurr. Approaches Psychiatry, V15, P562, DOI [10.18863/pgy.1192543, DOI 10.18863/PGY.1192543]
   Donker T, 2019, JAMA PSYCHIAT, V76, P682, DOI 10.1001/jamapsychiatry.2019.0219
   Emmelkamp PMG, 2021, ANNU REV CLIN PSYCHO, V17, P495, DOI 10.1146/annurev-clinpsy-081219-115923
   Felnhofer A, 2019, CYBERPSYCH BEH SOC N, V22, P46, DOI 10.1089/cyber.2018.0221
   Felnhofer A, 2014, CYBERPSYCH BEH SOC N, V17, P310, DOI 10.1089/cyber.2013.0472
   Filo P, 2022, FRONT PSYCHOL, V13, DOI 10.3389/fpsyg.2022.775173
   Fil'o P, 2022, THEOR ISS ERGON SCI, V23, P38, DOI 10.1080/1463922X.2021.1913535
   Freeman D, 2017, PSYCHOL MED, V47, P2393, DOI 10.1017/S003329171700040X
   Freeman D, 2018, LANCET PSYCHIAT, V5, P625, DOI 10.1016/S2215-0366(18)30226-8
   GANG MJ, 1975, PSYCHOPHYSIOLOGY, V12, P423, DOI 10.1111/j.1469-8986.1975.tb00016.x
   Giannakakis G, 2022, IEEE T AFFECT COMPUT, V13, P440, DOI 10.1109/TAFFC.2019.2927337
   Go Wireless Exercise Heart Rate, Vernier
   Gomes P., 2019, P INT C EL EL COMP E, P822
   Guided Meditation VR, Steam
   Duong HTH, 2020, AM J TROP MED HYG, V102, P403, DOI 10.4269/ajtmh.19-0720
   Hammad M, 2018, MEASUREMENT, V125, P634, DOI 10.1016/j.measurement.2018.05.033
   Held J, 2021, BMC PSYCHOL, V9, DOI 10.1186/s40359-021-00551-4
   Ihmig FR, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0231517
   Jurík V, 2016, INT ARCH PHOTOGRAMM, V41, P663, DOI 10.5194/isprsarchives-XLI-B2-663-2016
   Kampmann IL, 2016, BEHAV RES THER, V77, P147, DOI 10.1016/j.brat.2015.12.016
   Kapfhammer HP, 2016, J NEUROL, V263, P1946, DOI 10.1007/s00415-016-8218-9
   Kapfhammer HP, 2015, EUR ARCH PSY CLIN N, V265, P375, DOI 10.1007/s00406-014-0548-y
   Kisker J, 2021, PSYCHOL RES-PSYCH FO, V85, P68, DOI 10.1007/s00426-019-01244-9
   Koolhaas JM, 1999, NEUROSCI BIOBEHAV R, V23, P925, DOI 10.1016/S0149-7634(99)00026-3
   Lindner P, 2020, FRONT PSYCHIATRY, V11, DOI 10.3389/fpsyt.2020.00116
   Ling Y, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0096144
   Maiwald LM, 2019, J CLIN PSYCHOL, V75, P614, DOI 10.1002/jclp.22738
   Maples-Keller JL, 2017, HARVARD REV PSYCHIAT, V25, P103, DOI 10.1097/HRP.0000000000000138
   Miloff A, 2019, BEHAV RES THER, V118, P130, DOI 10.1016/j.brat.2019.04.004
   Morina N, 2015, BEHAV RES THER, V74, P18, DOI 10.1016/j.brat.2015.08.010
   Muhanna MA, 2015, J KING SAUD UNIV-COM, V27, P344, DOI 10.1016/j.jksuci.2014.03.023
   Obrist P.A., 1981, CARDIOVASCULAR PSYCH, DOI DOI 10.1007/978-1-4684-8491-5
   Ong T, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.848066
   Ose Solveig Osborg, 2019, JMIR Form Res, V3, pe13633, DOI 10.2196/13633
   RICHIES PLANK EXPERIENCE, TOAST Games
   Shaffer F, 2017, FRONT PUBLIC HEALTH, V5, DOI 10.3389/fpubh.2017.00258
   Sherman WilliamR., 2003, UNDERSTANDING VIRTUA
   Slater M, 2018, BRIT J PSYCHOL, V109, P431, DOI 10.1111/bjop.12305
   Spielberger CD., 1989, STATE TRAIT ANXIETY
   Steinman SA, 2011, J ANXIETY DISORD, V25, P896, DOI 10.1016/j.janxdis.2011.05.001
   Takac M, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0216288
   Ugwitz P, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9091873
   Wiederhold BK, 2002, CYBERPSYCHOL BEHAV, V5, P77, DOI 10.1089/109493102753685908
   Wray TB, 2023, COGN BEHAV THERAPY, V52, P603, DOI 10.1080/16506073.2023.2229017
NR 60
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD JUL 15
PY 2024
VL 28
IS 3
AR 137
DI 10.1007/s10055-024-01037-5
PG 14
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA YV2L4
UT WOS:001271193900001
OA hybrid
DA 2024-08-05
ER

PT J
AU Alarcon-Urbistondo, P
   Perez-Aranda, J
   Casado-Molina, A
AF Alarcon-Urbistondo, Pilar
   Perez-Aranda, Javier
   Casado-Molina, Ana
TI Key determinants of intention to use virtual reality in medical training
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Technology acceptance; Key factor; Medical learning
ID TECHNOLOGY ACCEPTANCE MODEL; BEHAVIORAL INTENTION; UNIFIED THEORY;
   INFORMATION-TECHNOLOGY; LEARNING-SYSTEMS; PLS-SEM; EXTENSION; ADOPTION;
   PERSPECTIVES; EDUCATION
AB Experts have called for virtual reality (VR) training and learning applications that can facilitate the changes needed in training programmes for years to come. To help expedite the adoption process, this study used a mixed-methods approach to identify the key factors that promote intentions to use VR technology in medical training. The qualitative research was based on interviews with five doctors and medical students, which focused on identifying the most significant determinants. Next, a survey was conducted to collect data from 154 medical interns and students in Spanish universities and hospitals, whose responses were processed using partial least squares-structural equation analysis. The limited sample size means this study is exploratory. The results indicate that perceived entertainment significantly strengthens behavioural intention to use VR technology in medical courses. The findings also underline the potential uses of VR learning tools in healthcare contexts and the need to incorporate this technology into medical training.
C1 [Alarcon-Urbistondo, Pilar; Perez-Aranda, Javier; Casado-Molina, Ana] Univ Malaga, Malaga, Spain.
C3 Universidad de Malaga
RP Perez-Aranda, J (corresponding author), Univ Malaga, Malaga, Spain.
EM jpereza@uma.es
RI Casado Molina, Ana Maria/S-1226-2016
OI Casado Molina, Ana Maria/0000-0002-1780-9557; Perez Aranda,
   Javier/0000-0002-8784-9888
FU Universidad de Mlaga
FX The authors wish to thank MetaMedicsVR for their contribution to this
   research. The project was conducted in collaboration with the private
   sector, which facilitated an exploration of the potential demand for
   medical training based on virtual reality applications.
CR Adedinsewo DA, 2022, INT J CARDIOL, V354, P48, DOI 10.1016/j.ijcard.2022.03.002
   Aebi NJ, 2021, INT J PUBLIC HEALTH, V66, DOI 10.3389/ijph.2021.633451
   Aggelidis VP, 2009, INT J MED INFORM, V78, P115, DOI 10.1016/j.ijmedinf.2008.06.006
   Agudo-Peregrina AF, 2014, COMPUT HUM BEHAV, V34, P301, DOI 10.1016/j.chb.2013.10.035
   Aiello S, 2023, VIRTUAL REAL-LONDON, V27, P3485, DOI 10.1007/s10055-022-00745-0
   Al-Azawei A, 2020, TECHNOL SOC, V62, DOI 10.1016/j.techsoc.2020.101325
   Al-Gahtani SS., 2016, Applied Computing and Informatics, V12, P27, DOI [10.1016/j.aci.2014.09.001, DOI 10.1016/J.ACI.2014.09.001]
   Alazzam Malik B., 2015, Journal of Theoretical and Applied Information Technology, V78, P473
   Alazzam M.B., 2016, Trust in stored data in EHRs acceptance of medical staff: using UTAUT2, V11, P2737
   Algahtani M, 2021, INT J COMPUT SCI NET, V21, P219, DOI 10.22937/IJCSNS.2021.21.3.30
   Aliwi I, 2023, J PATIENT EXPERIENCE, V10, DOI 10.1177/23743735231171562
   Alyoussef IY, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10243171
   Alzahrani NM, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10165660
   Ameri A, 2020, EDUC INF TECHNOL, V25, P419, DOI 10.1007/s10639-019-09965-5
   Anastasiadou D, 2022, BMJ OPEN, V12, DOI 10.1136/bmjopen-2022-060822
   Attuquayefio S.N., 2014, INT J ED DEV USING I, V10, P75
   Azizi SM., 2020, BMC Med Educ, V20, P1, DOI [10.21203/rs.3.rs-27351/v1, DOI 10.21203/RS.3.RS-27351/V1]
   Bagozzi RP, 2007, J ASSOC INF SYST, V8, P243
   Baptista G, 2015, COMPUT HUM BEHAV, V50, P418, DOI 10.1016/j.chb.2015.04.024
   Barclay D.W., 1995, Technology Studies, V2
   Barteit S, 2021, JMIR SERIOUS GAMES, V9, DOI 10.2196/29080
   Ben Yahia I, 2018, J RETAIL CONSUM SERV, V41, P11, DOI 10.1016/j.jretconser.2017.10.021
   Benbasat I, 2007, J ASSOC INF SYST, V8, P211, DOI 10.17705/1jais.00126
   Brown SA, 2005, MIS QUART, V29, P399
   Bullock K, 2020, J NEUROPSYCH CLIN N, V32, P90, DOI 10.1176/appi.neuropsych.19030071
   Cabero-Almenara Julio, 2023, Int J Environ Res Public Health, V20, DOI 10.3390/ijerph20085571
   Calvert J, 2023, VIRTUAL REAL-LONDON, V27, P2633, DOI 10.1007/s10055-023-00830-y
   Carmines EG, 1979, Reliability and Validity Assessment, V17, DOI 10.4135/9781412985642
   Cavanagh Renee, 2022, J Technol Behav Sci, P1, DOI 10.1007/s41347-022-00242-w
   Cepeda-Carrion G, 2004, Aplicando en la practica la tecnica PLS en la administracion de empresas
   Chang CY, 2023, VIRTUAL REAL-LONDON, V27, P2461, DOI 10.1007/s10055-023-00817-9
   Chao CM, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.01652
   Chen PY, 2019, ASIA PAC J EDUC, V39, P79, DOI 10.1080/02188791.2019.1575184
   Chiang P, 2012, COMPUT AIDED DESIGN, V44, P289, DOI 10.1016/j.cad.2011.11.004
   Chuttur MY., 2009, Sprouts: Working Papers on Information Systems, V9, P9
   Criollo S, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11094111
   Dajani D, 2019, HELIYON, V5, DOI 10.1016/j.heliyon.2019.e02536
   DAVIS FD, 1989, MIS QUART, V13, P319, DOI 10.2307/249008
   DAVIS FD, 1992, J APPL SOC PSYCHOL, V22, P1111, DOI 10.1111/j.1559-1816.1992.tb00945.x
   de Ribaupierre S., 2014, VIRTUAL, P9, DOI [10.1007/978-3-642-54816-1_2, DOI 10.1007/978-3-642-54816-1_2]
   Désiron JC, 2023, VIRTUAL REAL-LONDON, V27, P1013, DOI 10.1007/s10055-022-00708-5
   El-Masri M, 2017, ETR&D-EDUC TECH RES, V65, P743, DOI 10.1007/s11423-016-9508-8
   Ezendam D, 2009, DISABIL REHABIL, V31, P2135, DOI 10.3109/09638280902887768
   Fertleman C, 2018, FRONT PUBLIC HEALTH, V6, DOI 10.3389/fpubh.2018.00044
   FORNELL C, 1981, J MARKETING RES, V18, P39, DOI 10.2307/3151312
   Fuller CM, 2016, J BUS RES, V69, P3192, DOI 10.1016/j.jbusres.2015.12.008
   Fussell SG, 2022, VIRTUAL REAL-LONDON, V26, P249, DOI 10.1007/s10055-021-00554-x
   Galvan Pedro, 2021, Med Access Point Care, V5, p23992026211013644, DOI 10.1177/23992026211013644
   Georgieva-Tsaneva G, 2020, INT J EMERG TECHNOL, V15, P223, DOI 10.3991/ijet.v15i19.15753
   Gerson K., 2002, Qualitative research in Action, P199, DOI DOI 10.4135/9781849209656.N9
   Giravi HY, 2022, FRONT DIGIT HEALTH, V4, DOI 10.3389/fdgth.2022.884047
   Glaser B.G., 1967, Discovery of grounded theory: Strategies for qualitative research, DOI DOI 10.4324/9780203793206
   Gold AH, 2001, J MANAGE INFORM SYST, V18, P185, DOI 10.1080/07421222.2001.11045669
   Hair JF, 2011, J MARKET THEORY PRAC, V19, P139, DOI 10.2753/MTP1069-6679190202
   Hair JF, 2014, EUR BUS REV, V26, P106, DOI 10.1108/EBR-10-2013-0128
   Hen L.B., 2019, Augmented reality and virtual reality: the power of AR and VR for business exploring surgeon's acceptance of virtual reality headset for training, DOI [10.1007/978-3-030-06246-0_21, DOI 10.1007/978-3-030-06246-0_21]
   Henseler J, 2015, J ACAD MARKET SCI, V43, P115, DOI 10.1007/s11747-014-0403-8
   Henseler J, 2009, ADV INT MARKETING, V20, P277, DOI 10.1108/S1474-7979(2009)0000020014
   Holden RJ, 2010, J BIOMED INFORM, V43, P159, DOI 10.1016/j.jbi.2009.07.002
   Howard MC, 2023, VIRTUAL REAL-LONDON, V27, P2871, DOI 10.1007/s10055-023-00844-6
   Hu PJ, 1999, J MANAGE INFORM SYST, V16, P91, DOI 10.1080/07421222.1999.11518247
   Huang FH, 2020, VIRTUAL REAL-LONDON, V24, P635, DOI 10.1007/s10055-019-00424-7
   Im JE, 2023, VIRTUAL REAL-LONDON, V27, P3599, DOI 10.1007/s10055-023-00803-1
   Jin Seok, 2019, [The Journal of Information Systems, 정보시스템연구], V28, P159
   Joshi A., 2015, BRIT J APPL SCI TECH, V7, P396, DOI DOI 10.9734/BJAST/2015/14975
   Kayali M., 2020, International Journal of Contemporary Management and Information Technology, V1, P1
   Kim SS, 2005, INFORM SYST RES, V16, P418, DOI 10.1287/isre.1050.0070
   Kim Y., 2011, AM SOC INFORM SCI TE, V48, P1
   King D, 2018, NURS EDUC TODAY, V71, P7, DOI 10.1016/j.nedt.2018.08.002
   Kock N, 2015, INT J E-COLLAB, V11, P1, DOI 10.4018/ijec.2015100101
   Krokos E, 2019, VIRTUAL REAL-LONDON, V23, P1, DOI 10.1007/s10055-018-0346-3
   Kruse CS, 2022, HEALTHCARE-BASEL, V10, DOI 10.3390/healthcare10101845
   Kuehn BM, 2021, JAMA-J AM MED ASSOC, V326, P2354, DOI 10.1001/jama.2021.22223
   Lange Ann-Kathrin, 2020, JMIR Nurs, V3, pe20249, DOI 10.2196/20249
   Lau KW, 2023, VIRTUAL REAL-LONDON, V27, P2331, DOI 10.1007/s10055-023-00811-1
   Lee EAL, 2008, LECT NOTES COMPUT SC, V5080, P231
   Levin MF, 2015, PHYS THER, V95, P415, DOI 10.2522/ptj.20130579
   Lewis C.C., 2013, INT J HIGHER ED, V2, P22, DOI [10.5430/ijhe.v2n2p22, DOI 10.5430/IJHE.V2N2P22]
   Li JP, 2020, MERC ST POLI SOC EC, P253, DOI [10.1145/3396743.3396750, 10.1007/978-3-030-34937-0_11]
   Lie SS, 2022, JMIR RES PROTOC, V11, DOI 10.2196/37222
   Limayem M, 2007, MIS QUART, V31, P705, DOI 10.2307/25148817
   Lin PH, 2019, INT J HUM-COMPUT INT, V35, P1736, DOI 10.1080/10447318.2019.1571784
   Lopreiato J.O., 2016, HEALTHCARE SIMULATIO
   Lu HP, 2014, COMPUT HUM BEHAV, V34, P323, DOI 10.1016/j.chb.2013.10.020
   Lunceford B., 2009, Explorations in Media Ecology, V8, P29
   Makransky G., 2017, Learn Instr, DOI [10.1016/j.learninstruc.2017.12.00, DOI 10.1016/J.LEARNINSTRUC.2017.12.00]
   Mantovani F, 2003, CYBERPSYCHOL BEHAV, V6, P389, DOI 10.1089/109493103322278772
   Maples-Keller JL, 2017, HARVARD REV PSYCHIAT, V25, P103, DOI 10.1097/HRP.0000000000000138
   Maraj CS, 2015, P INTERSERVICEINDUST, V25, P103, DOI [10.1097/HRP.0000000000000138, DOI 10.1097/HRP.0000000000000138]
   Masneri S, 2023, VIRTUAL REAL-LONDON, V27, P1813, DOI 10.1007/s10055-023-00764-5
   Matsangidou M, 2022, HUM-COMPUT INTERACT, V37, P314, DOI 10.1080/07370024.2020.1788945
   Minervini G, 2022, BIOMED RES INT, V2022, DOI 10.1155/2022/7091153
   Molinillo S, 2017, CLOTH TEXT RES J, V35, P156, DOI 10.1177/0887302X17694270
   Moorthy K., 2019, International Information Library Review, V51, P128, DOI DOI 10.1080/10572317.2018.1463049
   Moorthy K, 2019, AUSTRALAS J EDUC TEC, V35, P174, DOI 10.14742/ajet.4432
   Morosan C, 2016, INT J HOSP MANAG, V53, P17, DOI 10.1016/j.ijhm.2015.11.003
   Moscoloni N., 2005, Revista Electronica de Metodologia aplicada, V10, P1
   Mtebe J, 2014, Int J Educ Dev Using ICT, V10
   Murillo GG, 2021, INFORM DEV, V37, P617, DOI 10.1177/0266666920959367
   Nikou SA, 2017, COMPUT EDUC, V109, P56, DOI 10.1016/j.compedu.2017.02.005
   Nunnally J., 1994, Psychometric Theory
   Oliveira T., 2011, ELECT J INF SYST EVA, V14, P110, DOI DOI 10.1017/CBO9781107415324.004
   Onwuegbuzie A.J., 2009, International Journal of Multiple Research Approaches, V3, P114, DOI DOI 10.5172/MRA.3.2.114
   Oxford Medical Simulation, 2019, Future of simulation, virtual reality medical training
   Park S, 2012, MARKET SCI, V31, P567, DOI 10.1287/mksc.1120.0718
   Pedram S, 2023, VIRTUAL REAL-LONDON, V27, P2255, DOI 10.1007/s10055-023-00802-2
   Pieterse AD, 2023, VIRTUAL REAL-LONDON, V27, P1381, DOI 10.1007/s10055-022-00731-6
   Podsakoff PM, 2012, ANNU REV PSYCHOL, V63, P539, DOI 10.1146/annurev-psych-120710-100452
   Pottle Jack, 2019, Future Healthc J, V6, P181, DOI 10.7861/fhj.2019-0036
   Ra'ed M., 2017, Int J Bus Manag, V10, P164, DOI [10.5539/ijbm.v11n2p299, DOI 10.5539/IJBM.V11N2P299]
   Raza SA, 2021, J EDUC COMPUT RES, V59, P183, DOI 10.1177/0735633120960421
   Rese A, 2017, TECHNOL FORECAST SOC, V124, P306, DOI 10.1016/j.techfore.2016.10.010
   Richter NF, 2016, INT MARKET REV, V33, P376, DOI 10.1108/IMR-04-2014-0148
   Ringle CM, 2014, REV BRASIL MARK, V13, P54, DOI 10.5585/remark.v13i2.2717
   Rocco T. S., 2003, Information Technology, Learning, and Performance Journal, V21, P19
   Ruthenbeck GS, 2015, J SIMUL, V9, P16, DOI 10.1057/jos.2014.14
   Sadeghi AH, 2020, EUR HEART J-DIGIT HL, V1, P62, DOI 10.1093/ehjdh/ztaa011
   Sagnier C, 2020, INT J HUM-COMPUT INT, V36, P993, DOI 10.1080/10447318.2019.1708612
   Samadbeik Mahnaz, 2018, J Adv Med Educ Prof, V6, P123
   Sattar MU, 2020, INT J EMERG TECHNOL, V15, P160, DOI 10.3991/ijet.v15i02.11394
   Shen CW, 2019, VIRTUAL REAL-LONDON, V23, P313, DOI 10.1007/s10055-018-0348-1
   Sigala M, 2021, J HOSP TOUR MANAG, V48, P604, DOI 10.1016/j.jhtm.2021.08.010
   Freitas JRS, 2021, PSYCHIAT QUART, V92, P1685, DOI 10.1007/s11126-021-09935-6
   Sprenger DA, 2021, INT J EDUC TECHNOL H, V18, DOI 10.1186/s41239-021-00243-4
   Stefan H, 2023, VIRTUAL REAL-LONDON, V27, P2839, DOI 10.1007/s10055-023-00843-7
   Sung H., 2015, International Journal of u-and e-Service, Science and Technology, V8, P197, DOI 10.14257/ijunesst.2015.8.9.21
   Sveistrup Heidi, 2004, J Neuroeng Rehabil, V1, P10, DOI 10.1186/1743-0003-1-10
   Tamilmani K, 2021, INT J INFORM MANAGE, V57, DOI 10.1016/j.ijinfomgt.2020.102269
   Tashakkori A., 2003, International Journal of Social Research Methodology, V6, P61, DOI DOI 10.1080/13645570305055
   Tavares Jorge, 2014, 10th International Conference on Web Information Systems and Technologies (WEBIST 2014). Proceedings, P387
   Tu ZL, 2014, INT J MOB COMMUN, V12, P603, DOI 10.1504/IJMC.2014.064915
   Ustun A. B., 2020, MOBILE DEVICES SMART, P56, DOI [10.4018/978-1-7998-2521-0.ch004, DOI 10.4018/978-1-7998-2521-0.CH004]
   Ustun AB, 2023, VIRTUAL REAL-LONDON, V27, P1063, DOI 10.1007/s10055-022-00717-4
   Vekhter D, 2020, CURR PAIN HEADACHE R, V24, DOI 10.1007/s11916-020-00899-z
   Venkatesh V, 2000, MANAGE SCI, V46, P186, DOI 10.1287/mnsc.46.2.186.11926
   Venkatesh V, 2003, MIS QUART, V27, P425, DOI 10.2307/30036540
   Venkatesh V, 2008, DECISION SCI, V39, P273, DOI 10.1111/j.1540-5915.2008.00192.x
   Venkatesh V, 2012, MIS QUART, V36, P157
   White Brianna M, 2022, Stud Health Technol Inform, V295, P499, DOI 10.3233/SHTI220774
   Wong EYC, 2023, VIRTUAL REAL-LONDON, V27, P2149, DOI 10.1007/s10055-023-00793-0
   Wyman Oliver, 2021, The digital frontier of health promotion and prevention
   Yeo Elizabeth, 2019, Innov Clin Neurosci, V16, P13
   Zhao X, 2022, DISASTER MED PUBLIC, V17, DOI 10.1017/dmp.2022.65
NR 143
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD APR 2
PY 2024
VL 28
IS 2
AR 90
DI 10.1007/s10055-024-00990-5
PG 18
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA MX5V1
UT WOS:001196957000001
OA hybrid
DA 2024-08-05
ER

PT J
AU Banquiero, M
   Valdeolivas, G
   Ramón, D
   Juan, MR
AF Banquiero, Mariano
   Valdeolivas, Gracia
   Ramon, David
   Juan, M. Carmen
TI A color Passthrough mixed reality application for learning piano
SO VIRTUAL REALITY
LA English
DT Article
DE Mixed reality; Passthrough; Color Passthrough; Piano; Learning; Meta
   Quest Pro; Synthesia
ID CONSTRUCTION
AB This work presents the development of a mixed reality (MR) application that uses color Passthrough for learning to play the piano. A study was carried out to compare the interpretation outcomes of the participants and their subjective experience when using the MR application developed to learn to play the piano with a system that used Synthesia (N = 33). The results show that the MR application and Synthesia were effective in learning piano. However, the students played the pieces significantly better when using the MR application. The two applications both provided a satisfying user experience. However, the subjective experience of the students was better when they used the MR application. Other conclusions derived from the study include the following: (1) The outcomes of the students and their subjective opinion about the experience when using the MR application were independent of age and gender; (2) the sense of presence offered by the MR application was high (above 6 on a scale of 1 to 7); (3) the adverse effects induced by wearing the Meta Quest Pro and using our MR application were negligible; and (4) the students showed their preference for the MR application. As a conclusion, the advantage of our MR application compared to other types of applications (e.g., non-projected piano roll notation) is that the user has a direct view of the piano and the help elements appear integrated in the user's view. The user does not have to take their eyes off the keyboard and is focused on playing the piano.
C1 [Banquiero, Mariano; Juan, M. Carmen] Univ Politecn Valencia, Inst Univ Automatica Informat Ind, C-Camino Vera,S-N, Valencia 46022, Spain.
   [Valdeolivas, Gracia] Univ Jaume 1, Dept Pedag, Castellon de La Plana 1206, Spain.
   [Ramon, David] Mus Sch UM Santa Cecilia Benicassim, Benicassim 12560, Spain.
C3 Universitat Politecnica de Valencia; Universitat Jaume I
RP Juan, MR (corresponding author), Univ Politecn Valencia, Inst Univ Automatica Informat Ind, C-Camino Vera,S-N, Valencia 46022, Spain.
EM mcarmen@dsic.upv.es
RI Valdeolivas Novella, Gracia/JDM-4454-2023
OI Valdeolivas Novella, Gracia/0000-0002-5490-3286; Juan,
   M.-Carmen/0000-0002-8764-1470
FU CRUE-CSIC agreement; Springer Nature; CRUE Universitat Politecnica de
   Valencia agreement
FX Open Access funding provided thanks to the CRUE-CSIC agreement with
   Springer Nature. Open Access funding provided thanks to the CRUE
   Universitat Politecnica de Valencia agreement with Springer Nature.
CR Banquiero M, 2023, IEEE MULTIMEDIA, V30, P60, DOI 10.1109/MMUL.2022.3232892
   Bimberg P, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P464, DOI [10.1109/VRW50115.2020.00098, 10.1109/VRW50115.2020.0-178]
   Birhanu A, 2017, EXTENDED ABSTRACTS PUBLICATION OF THE ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY (CHI PLAY'17 EXTENDED ABSTRACTS), P299, DOI 10.1145/3130859.3131336
   Cárdenas-Delgado S, 2017, LECT NOTES COMPUT SC, V10513, P97, DOI 10.1007/978-3-319-67744-6_7
   Chow J., 2013, P 14 AUSTR US INT C, V139, P73
   DAVIS FD, 1993, INT J MAN MACH STUD, V38, P475, DOI 10.1006/imms.1993.1022
   Deja Jordan Aiko, 2022, Proceedings of the ACM on Human-Computer Interaction, DOI 10.1145/3567719
   Desnoyers-Stewart J, 2017, 13 INT S CMMR, P376
   Feng Huang, 2011, 2011 International Conference on Intelligent Human-Machine Systems and Cybernetics, P47, DOI 10.1109/IHMSC.2011.82
   Hackl D., 2017, Forum Media Technology, P140
   Julia J., 2019, Journal of Physics: Conference Series, V1318, DOI 10.1088/1742-6596/1318/1/012040
   Kennedy R.S., 1993, Int. J. Aviat. Psy, P203
   Laugwitz B, 2008, LECT NOTES COMPUT SC, V5298, P63, DOI 10.1007/978-3-540-89350-9_6
   Molero D, 2021, MULTIMED TOOLS APPL, V80, P165, DOI 10.1007/s11042-020-09678-9
   Molloy W, 2019, 2019 INT C EL INF CO, P1
   Regenbrecht H., 2002, P 5 ANN INT WORKSH P
   Rigby Liam, 2020, OzCHI '20: Proceedings of the 32nd Australian Conference on Human-Computer Interaction, P481, DOI 10.1145/3441000.3441039
   Rogers Katja, 2014, P 9 ACM INT C INT TA, P149, DOI [DOI 10.1145/2669485.2669514, 10.1145/2669485.2669514]
   Schrepp M, 2017, INT J INTERACT MULTI, V4, P40, DOI 10.9781/ijimai.2017.445
   Villeneuve Myriam, 2014, Front Hum Neurosci, V8, P662, DOI 10.3389/fnhum.2014.00662
   Wijaya F, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P585, DOI [10.1109/VRW50115.2020.00143, 10.1109/VRW50115.2020.00144]
   Xiao Lei, 2022, ACM SIGGRAPH 2022 C
NR 22
TC 0
Z9 0
U1 6
U2 6
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR 6
PY 2024
VL 28
IS 2
AR 67
DI 10.1007/s10055-024-00953-w
PG 20
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA LG6N8
UT WOS:001185672700001
OA hybrid
DA 2024-08-05
ER

PT J
AU Mayor, J
   Calleja, P
   Fuentes-Hurtado, F
AF Mayor, Jesus
   Calleja, Pablo
   Fuentes-Hurtado, Felix
TI Long short-term memory prediction of user's locomotion in virtual
   reality
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; User prediction; Locomotion; Deep learning
AB Nowadays, there is still a challenge in virtual reality to obtain an accurate displacement prediction of the user. This could be a future key element to apply in the so-called redirected walking methods. Meanwhile, deep learning provides us with new tools to reach greater achievements in this type of prediction. Specifically, long short-term memory recurrent neural networks obtained promising results recently. This gives us clues to continue researching in this line to predict virtual reality user's displacement. This manuscript focuses on the collection of positional data and a subsequent new way to train a deep learning model to obtain more accurate predictions. The data were collected with 44 participants and it has been analyzed with different existing prediction algorithms. The best results were obtained with a new idea, the use of rotation quaternions and the three dimensions to train the previously existing models. The authors strongly believe that there is still much room for improvement in this research area by means of the usage of new deep learning models.
C1 [Mayor, Jesus; Calleja, Pablo; Fuentes-Hurtado, Felix] Univ Politecn Madrid, Sistemas Informat, Madrid 28031, Spain.
C3 Universidad Politecnica de Madrid
RP Mayor, J (corresponding author), Univ Politecn Madrid, Sistemas Informat, Madrid 28031, Spain.
EM jesus.mayor@upm.es; p.calleja@upm.es; felix.fuentes@upm.es
RI Hurtado, Félix José Fuentes/AAA-8787-2021
FU European Union-NextGenerationEU
FX No Statement Available
CR Azmandian M, 2016, 2016 IEEE 2ND WORKSHOP ON EVERYDAY VIRTUAL REALITY (WEVR), P9, DOI 10.1109/WEVR.2016.7859537
   Bölling L, 2019, IEEE T VIS COMPUT GR, V25, P2032, DOI 10.1109/TVCG.2019.2899228
   BRADLEY JV, 1958, J AM STAT ASSOC, V53, P525, DOI 10.2307/2281872
   Bremer G, 2021, 2021 4TH IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR 2021), P19, DOI 10.1109/AIVR52153.2021.00013
   Brenneis D, 2021, Assessing human interaction in virtual reality with continually learning prediction agents based on reinforcement learning algorithms: a pilot study
   Breuer A, 2019, IEEE INT C INTELL TR, P2728, DOI 10.1109/ITSC.2019.8917373
   Cho YH, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P527, DOI 10.1109/VR.2018.8446442
   Corona E, 2020, PROC CVPR IEEE, P6990, DOI 10.1109/CVPR42600.2020.00702
   David B, 2021, AUST ARCHAEOL, V87, P1, DOI 10.1080/03122417.2020.1859963
   Dupond S., 2019, Annual Reviews in Control, V14, P200
   Fan CW, 2023, Symposium Virtual Re, P53, DOI 10.1109/VR55154.2023.00021
   Gamage Nisal Menuka, 2021, UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology, P332, DOI 10.1145/3472749.3474753
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Grechkin J., 2016, P ACM S APPL PERC SA, P113, DOI [10.1145/2931002.29310182[19]J, DOI 10.1145/2931002.2931018]
   Hirt C, 2022, IEEE INT SYMP M AU R, P554, DOI 10.1109/ISMAR-Adjunct57072.2022.00116
   Hirt C, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P627, DOI 10.1109/VRW55335.2022.00169
   Hodgson E, 2013, IEEE T VIS COMPUT GR, V19, P634, DOI 10.1109/TVCG.2013.28
   Hu ZM, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P543, DOI [10.1109/VRW50115.2020.0-153, 10.1109/VRW50115.2020.00123]
   Langbehn E, 2018, PROCEEDINGS OF THE VIRTUAL REALITY INTERNATIONAL CONFERENCE - LAVAL VIRTUAL (ACM VRIC 2018), DOI 10.1145/3234253.3234291
   Langbehn Eike, 2018, Redirected walking in virtual reality, P1, DOI DOI 10.1007/978-3-319-08234-9_253-1
   LaViola J. J.  Jr., 2003, IPT/EGVE 2003. Seventh Immersive Projection Technology Workshop. Ninth Eurographics Workshop on Virtual Environments, P199, DOI 10.1145/769953.769976
   Lee DY, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P155, DOI [10.1109/VR46266.2020.00-70, 10.1109/VR46266.2020.1581309443724]
   Lee DY, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P63, DOI [10.1109/VR.2019.8798121, 10.1109/vr.2019.8798121]
   Markley FL, 2007, J GUID CONTROL DYNAM, V30, P1193, DOI 10.2514/1.28949
   Mayor J, 2023, Github
   Mayor Jesus, 2023, Zenodo, DOI 10.5281/ZENODO.8169116
   Mayor J, 2022, IEEE T EMERG TOP COM, V10, P997, DOI 10.1109/TETC.2021.3062285
   Mayor J, 2021, IEEE T EMERG TOP COM, V9, P1542, DOI 10.1109/TETC.2019.2915287
   Nescher T., 2013, Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformat-ics), V7848, P172, DOI DOI 10.1007/978-3-642-38803-3_10
   Nescher T, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P111, DOI 10.1109/3DUI.2014.6798851
   Razzaque S., 2005, Redirected walking
   Razzaque Z., 2001, EUROGRAPHICS 2001 SH, P289, DOI 10.2312/egs.20011036
   Stein N, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P493, DOI 10.1109/VR51125.2022.00069
   Stein N, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P727, DOI 10.1109/VRW52623.2021.00246
   Strauss RR, 2020, IEEE T VIS COMPUT GR, V26, P1955, DOI 10.1109/TVCG.2020.2973060
   Suma EA, 2012, IEEE VIRTUAL REALITY CONFERENCE 2012 PROCEEDINGS, P43, DOI 10.1109/VR.2012.6180877
   van Rhijn A, 2005, P IEEE VIRT REAL ANN, P67
   You C, 2022, INT SYM MIX AUGMENT, P603, DOI 10.1109/ISMAR55827.2022.00077
   Zank M, 2017, IEEE SYMP 3D USER, P120, DOI 10.1109/3DUI.2017.7893328
   Zank M, 2016, VISUAL COMPUT, V32, P1323, DOI 10.1007/s00371-016-1229-9
   Zank M, 2015, 2015 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P229, DOI 10.1109/CW.2015.20
   Zielasko D, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P113, DOI 10.1109/3DUI.2016.7460040
   Zmuda MA, 2013, IEEE T VIS COMPUT GR, V19, P1872, DOI 10.1109/TVCG.2013.88
NR 43
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 65
DI 10.1007/s10055-024-00962-9
PG 12
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA JS1F8
UT WOS:001175055500001
OA hybrid, Green Submitted
DA 2024-08-05
ER

PT J
AU Moreno-Arjonilla, J
   López-Ruiz, A
   Jiménez-Pérez, JR
   Callejas-Aguilera, JE
   Jurado, JM
AF Moreno-Arjonilla, Jesus
   Lopez-Ruiz, Alfonso
   Jimenez-Perez, J. Roberto
   Callejas-Aguilera, Jose E.
   Jurado, Juan M.
TI Eye-tracking on virtual reality: a survey
SO VIRTUAL REALITY
LA English
DT Article
DE Virtual reality; Eye-tracking; Perception; Attention
ID MOVEMENT; CLASSIFICATION; PERFORMANCE; ATTENTION; SYSTEMS
AB Virtual reality (VR) has evolved substantially beyond its initial remit of gaming and entertainment, catalyzed by advancements such as improved screen resolutions and more accessible devices. Among various interaction techniques introduced to VR, eye-tracking stands out as a pivotal development. It not only augments immersion but offers a nuanced insight into user behavior and attention. This precision in capturing gaze direction has made eye-tracking instrumental for applications far beyond mere interaction, influencing areas like medical diagnostics, neuroscientific research, educational interventions, and architectural design, to name a few. Though eye-tracking's integration into VR has been acknowledged in prior reviews, its true depth, spanning the intricacies of its deployment to its broader ramifications across diverse sectors, has been sparsely explored. This survey undertakes that endeavor, offering a comprehensive overview of eye-tracking's state of the art within the VR landscape. We delve into its technological nuances, its pivotal role in modern VR applications, and its transformative impact on domains ranging from medicine and neuroscience to marketing and education. Through this exploration, we aim to present a cohesive understanding of the current capabilities, challenges, and future potential of eye-tracking in VR, underscoring its significance and the novelty of our contribution.
C1 [Moreno-Arjonilla, Jesus; Lopez-Ruiz, Alfonso; Jimenez-Perez, J. Roberto] Univ Jaen, Comp Sci Dept, Jaen 23071, Spain.
   [Callejas-Aguilera, Jose E.] Univ Jaen, Psychol Dept, Jaen 23071, Spain.
   [Jurado, Juan M.] Univ Granada, Software Engn Dept, Granada 18071, Spain.
C3 Universidad de Jaen; Universidad de Jaen; University of Granada
RP Moreno-Arjonilla, J (corresponding author), Univ Jaen, Comp Sci Dept, Jaen 23071, Spain.
EM jarjonil@ujaen.es; allopezr@ujaen.es; rjimenez@ujaen.es;
   jecalle@ujaen.es; jjurado@ujaen.es
RI López Ruiz, Alfonso/AAO-1335-2021; Callejas-Aguilera, Jose E/K-8856-2014
OI López Ruiz, Alfonso/0000-0003-1423-9496; Callejas-Aguilera, Jose
   E/0000-0003-2903-2578
FU Ministerio de Ciencia, Innovacin y Universidades [PGC2018097769-B-C22,
   FPU/00100]; Spanish Ministry of Science, Innovation and Universities
FX This result has been partially supported by the Spanish Ministry of
   Science, Innovation and Universities via grants to the first
   (PGC2018097769-B-C22) and second author (FPU/00100).
CR Al-Ghamdi NA, 2020, FRONT HUM NEUROSCI, V13, DOI 10.3389/fnhum.2019.00467
   Altobelli F, 2019, PhD thesis
   Bacca-Acosta J., 2021, Frameless, V3, P16
   Bacca-Acosta J, 2022, ETR&D-EDUC TECH RES, V70, P339, DOI 10.1007/s11423-021-10068-7
   Barsan-Pipu C, 2020, P 2019 DIGITALFUTURE, P124, DOI [10.1007/978-981-13-8153-911, DOI 10.1007/978-981-13-8153-911]
   Bayramova R, 2021, ATTEN PERCEPT PSYCHO, V83, P2865, DOI 10.3758/s13414-021-02344-8
   Biondi FN, 2023, APPL ERGON, V106, DOI 10.1016/j.apergo.2022.103867
   Blascheck T, 2017, COMPUT GRAPH FORUM, V36, P260, DOI 10.1111/cgf.13079
   Blattgerste J, 2018, COMMUNICATION BY GAZE INTERACTION (COGAIN 2018), DOI 10.1145/3206343.3206349
   Bolt RA., 1982, PROC ACM CHI 82, P360, DOI DOI 10.1145/800049.801811
   Borges M, 2018, IEEE INT C INT ROBOT, P2610, DOI 10.1109/IROS.2018.8593707
   Bozkir E, 2020, 2020 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR 2020), P397, DOI 10.1109/AIVR50618.2020.00083
   Bozkir E, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1834, DOI [10.1109/vr.2019.8797758, 10.1109/VR.2019.8797758]
   Brewster David., 1856, STEREOSCOPE ITS HIST
   Bublea A, 2020, INT SYMP ELEC TELECO, P48, DOI 10.1109/isetc50328.2020.9301091
   Burke RR, 2014, REV MARKET RES, V11, P147, DOI 10.1108/S1548-643520140000011006
   Burova A, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376405
   Chaudhary AK, 2019, IEEE INT CONF COMP V, P3698, DOI 10.1109/ICCVW.2019.00568
   Chernyak I, 2021, ETRA '21 Full Papers, P1, DOI [10.1145/3448017.3457383, DOI 10.1145/3448017.3457383]
   Chugh Soumil, 2020, An Eye Tracking System for a Virtual Reality Headset
   Chun-Chia Wang, 2019, 2019 8th International Congress on Advanced Applied Informatics (IIAI-AAI), P160, DOI 10.1109/IIAI-AAI.2019.00041
   Clay V, 2019, J EYE MOVEMENT RES, V12, DOI 10.16910/jemr.12.1.3
   Cowan A, 2021, J ENDOUROL, V35, P1571, DOI 10.1089/end.2020.1037
   Cubero CG, 2020, Prediction of choice using eye tracking and VR
   Davis R, 2021, FRONT AGING NEUROSCI, V13, DOI 10.3389/fnagi.2021.607219
   De Kloe YJR, 2022, INFANCY, V27, P25, DOI 10.1111/infa.12441
   de Lope J, 2022, NEUROCOMPUTING, V500, P518, DOI 10.1016/j.neucom.2021.06.100
   Demirer I, 2022, AGING MENT HEALTH, V26, P65, DOI 10.1080/13607863.2020.1870209
   DEVALOIS RL, 1980, ANNU REV PSYCHOL, V31, P309, DOI 10.1146/annurev.ps.31.020180.001521
   Diemer J, 2023, J BEHAV THER EXP PSY, V81, DOI 10.1016/j.jbtep.2023.101860
   Drakopoulos P, 2021, ACM T APPL PERCEPT, V18, DOI 10.1145/3456875
   Drakopoulos P, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P643, DOI [10.1109/VRW50115.2020.0-103, 10.1109/VRW50115.2020.00172]
   Duchowski A. T., 2017, Eye tracking methodology: Theory and practice, DOI DOI 10.1007/978-3-319-57883-5
   Duchowski AT, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376394
   Emery KaraJ., 2021, ACM S EYE TRACKING R, P1, DOI DOI 10.1145/3448018.3457996
   Fehlmann B., 2023, J. Affect. Disord. Reports, V14, P100627, DOI [10.1016/j.jadr.2023.100627, DOI 10.1016/J.JADR.2023.100627]
   Finke JB, 2021, NEUROSCI BIOBEHAV R, V130, P351, DOI 10.1016/j.neubiorev.2021.09.005
   Franke L, 2021, COMPUT GRAPH FORUM, V40, P110, DOI 10.1111/cgf.14176
   Fromm CA, 2019, Frameless, V1, P1, DOI [10.14448/Frameless.01.003, DOI 10.14448/FRAMELESS.01.003]
   Fuhl Wolfgang, 2021, ETRA '21: Symposium on Eye Tracking Research and Applications, DOI 10.1145/3448018.3458004
   Fuhl W., 2015, ExCuSe: robust pupil detection in real-world scenarios, DOI [10.1007/978-3-319-23192-14, DOI 10.1007/978-3-319-23192-14]
   Fuhl W, 2016, Arxiv, DOI [arXiv:1601.04902, DOI 10.48550/ARXIV.1601.04902]
   Fuhl W, 2020, ETRA 2020 SHORT PAPERS: ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, DOI 10.1145/3379156.3391347
   Fuhl W, 2019, LECT NOTES COMPUT SC, V11678, P336, DOI 10.1007/978-3-030-29888-3_27
   Gadin V, 2021, Factors for good text legibility: eye-tracking in virtual reality
   Galuret S, 2023, INT J COMPUT ASS RAD, V18, P1697, DOI 10.1007/s11548-023-02961-8
   Garbin SJ, 2020, ETRA'20 FULL PAPERS: ACM SYMPOSIUM ON EYE TRACKING RESEARCH AND APPLICATIONS, DOI 10.1145/3379155.3391317
   Gemicioglu Tan, 2023, 2023 CHI C HUM FACT, DOI 10
   Geraets CNW, 2021, INTERNET INTERV, V25, DOI 10.1016/j.invent.2021.100432
   Gong Wang, 2020, 2020 International Conference on Innovation Design and Digital Technology (ICIDDT), P68, DOI 10.1109/ICIDDT52279.2020.00020
   Gudi Amogh, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12535), P529, DOI 10.1007/978-3-030-66415-2_34
   Guenter B, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366183
   Gunther Ulrik, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12535), P280, DOI 10.1007/978-3-030-66415-2_18
   Halle M., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P243, DOI 10.1145/280814.280884
   HARTRIDGE H, 1948, BRIT J OPHTHALMOL, V32, P581, DOI 10.1136/bjo.32.9.581
   Heilemann F, 2022, LECT NOTES COMPUT SC, P371, DOI 10.1007/978-3-031-08648-9_43
   Hickson S, 2019, IEEE WINT CONF APPL, P1626, DOI 10.1109/WACV.2019.00178
   Hladky J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356530
   Hougaard BI, 2021, FRONT PSYCHOL, V12, DOI 10.3389/fpsyg.2021.742445
   Howard IP, 2002, Seeing in depth, vol. 1: Basic mechanisms, P659
   Huey E.B., 1968, PSYCHOL PEDAGOGY REA
   Huizeling E, 2022, LANG COGN NEUROSCI, V37, P481, DOI 10.1080/23273798.2021.1994621
   Illahi Gazi Karam, 2022, MMVE '22: Proceedings of the 14th International Workshop on Immersive Mixed and Virtual Environment Systems, P12, DOI 10.1145/3534086.3534331
   Illahi Gazi Karam, 2020, VRST '20: 26th ACM Symposium on Virtual Reality Software and Technology, DOI 10.1145/3385956.3422126
   Jacob RJK, 2003, MIND'S EYE: COGNITIVE AND APPLIED ASPECTS OF EYE MOVEMENT RESEARCH, P573, DOI 10.1016/B978-044451020-4/50031-1
   Joo HJ, 2020, MULTIMED TOOLS APPL, V79, P16719, DOI 10.1007/s11042-019-08327-0
   Jurik V, 2019, 20 EUR C EYE MOV AL
   JUST MA, 1980, PSYCHOL REV, V87, P329, DOI 10.1037/0033-295X.87.4.329
   Kaplanyan AS, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356557
   Kar A, 2017, IEEE ACCESS, V5, P16495, DOI 10.1109/ACCESS.2017.2735633
   Katrychuk D, 2019, ETRA 2019: 2019 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, DOI 10.1145/3314111.3319821
   Khatri J., 2020, HCI INT 2020 POSTERS, P64
   Khokhar A, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1018, DOI 10.1109/vr.2019.8797896
   Kim J, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300780
   Kobylinski P, 2020, 13 INT C ADV COMP HU, P185
   Komoriya Kazuki, 2021, Human Interaction, Emerging Technologies and Future Applications III. Proceedings of the 3rd International Conference on Human Interaction and Emerging Technologies: Future Applications (IHIET 2020). Advances in Intelligent Systems and Computing (AISC 1253), P316, DOI 10.1007/978-3-030-55307-4_48
   Kothari Rakshit S., 2022, Proceedings of the ACM on Human-Computer Interaction, V6, DOI 10.1145/3530880
   Kothari R, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-59251-5
   Koulieris GA, 2019, COMPUT GRAPH FORUM, V38, P493, DOI 10.1111/cgf.13654
   Laivuori N, 2021, Eye and hand tracking in VR training application
   Lamb M, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.864653
   Lee S, 2020, KSII T INTERNET INF, V14, P826, DOI 10.3837/tiis.2020.02.020
   Leigh R.J., 2015, The Neurology of Eye Movements, DOI DOI 10.1093/MED/9780199969289.001.0001
   Li B, 2019, 2019 IEEE MTT-S INTERNATIONAL MICROWAVE BIOMEDICAL CONFERENCE (IMBIOC 2019), DOI 10.1109/imbioc.2019.8777841
   Li F, 2021, INT C COMP SUPP COOP, P861, DOI 10.1109/CSCWD49262.2021.9437692
   Li JN, 2019, 2019 IEEE MTT-S INTERNATIONAL MICROWAVE BIOMEDICAL CONFERENCE (IMBIOC 2019), DOI 10.1109/imbioc.2019.8777809
   Li Z., 2014, Understanding Vision: Theory, Models, and Data (U.K.)
   Lim Jia Zheng, 2021, Journal of Physics: Conference Series, DOI 10.1088/1742-6596/2129/1/012069
   Liu H, 2020, ACTA PSYCHOL, V210, DOI 10.1016/j.actpsy.2020.103142
   LIVINGSTONE M, 1988, SCIENCE, V240, P740, DOI 10.1126/science.3283936
   Llanes-Jurado J, 2021, LECT NOTES COMPUT SC, V12763, P32, DOI 10.1007/978-3-030-78465-2_3
   Llanes-Jurado J, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20174956
   Lohr D, 2022, IEEE T INF FOREN SEC, V17, P3151, DOI 10.1109/TIFS.2022.3201369
   Lu C, 2022, INT SYM MIX AUGMENT, P375, DOI 10.1109/ISMAR55827.2022.00053
   Lu SF, 2020, J PHYS CONF SER, V1518, DOI 10.1088/1742-6596/1518/1/012020
   Luro FL, 2019, ETRA 2019: 2019 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, DOI 10.1145/3317956.3318153
   Maraj Crystal, 2021, Virtual, Augmented and Mixed Reality. 13th International Conference, VAMR 2021 Held as Part of the 23rd HCI International Conference, HCII 2021. Proceedings, P381, DOI 10.1007/978-3-030-77599-5_27
   Martin D, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3508361
   Nistal IMA, 2021, EUR J OPHTHALMOL, V31, P3080, DOI 10.1177/1120672120976047
   Marwecki S, 2019, PROCEEDINGS OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY (UIST 2019), P777, DOI 10.1145/3332165.3347919
   Matthews S, 2020, SYMP VIRTUAL AUGMENT, P398, DOI 10.1109/SVR51698.2020.00066
   McMurrough C.D., 2012, Proceedings of the Symposium on Eye Tracking Research and Applications, P305
   McNamara A, 2020, CHI'20: EXTENDED ABSTRACTS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3334480.3375060
   Mehringer W, 2021, IEEE ENG MED BIO, P2058, DOI 10.1109/EMBC46164.2021.9631002
   Meissner M, 2019, J BUS RES, V100, P445, DOI 10.1016/j.jbusres.2017.09.028
   Meiya Dong, 2020, TURC'20: Proceedings of the Turing Celebration Conference - China, P51, DOI 10.1145/3393527.3393537
   Melendrez-Ruiz J, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.655273
   Meng XX, 2020, IEEE T VIS COMPUT GR, V26, P1972, DOI 10.1109/TVCG.2020.2973442
   Miller HL, 2021, ACM SYMPOSIUM ON EYE TRACKING RESEARCH AND APPLICATIONS (ETRA 2021), DOI 10.1145/3450341.3458881
   Mirault J., 2020, Methods in Psychology, V3, P100029, DOI [10.1016/j.metip.2020.100029, DOI 10.1016/J.METIP.2020.100029, https://doi.org/10.1016/j.metip.2020.100029]
   Mohanto B, 2022, COMPUT GRAPH-UK, V102, P474, DOI 10.1016/j.cag.2021.10.010
   Mutasim AK, 2020, CHI'20: EXTENDED ABSTRACTS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3334480.3382924
   Naqvi RA, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18020456
   NVIDIA Corporation, 2020, GL nv shading rate image
   NVIDIA Corporation, 2018, VRWorks-Multi-View Rendering (MVR)
   Ogura K, 2019, IEEE INT CONF HEALT, P1, DOI [10.1109/ICHI.2019.8904558, DOI 10.1109/ichi.2019.8904558]
   Ortega JD, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22072554
   Ou WL, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11020851
   Ozel E, 2019, Master's thesis
   Palmer S.E., 1999, Vision Science: Photons to Phenomenology
   Palmero C, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21144769
   Park JH, 2019, CLIN EXP OTORHINOLAR, V12, P376, DOI 10.21053/ceo.2018.01592
   Pastel S, 2023, MULTIMED TOOLS APPL, V82, P4181, DOI 10.1007/s11042-022-13474-y
   Patney A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980246
   Pettersson J, 2021, Data-driven human intention analysis: supported by virtual reality and eye tracking. licentiate
   Pettersson J, 2021, IEEE INT CONF INDUST, P889, DOI 10.1109/ICIT46573.2021.9453581
   Pettersson J, 2020, PROCEDIA MANUF, V51, P95, DOI 10.1016/j.promfg.2020.10.015
   Pfeiffer J, 2020, INFORM SYST RES, V31, P675, DOI 10.1287/isre.2019.0907
   Plopski A, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3491207
   Plopski A, 2016, INT SYM MIX AUGMENT, P94, DOI 10.1109/ISMAR.2016.16
   Porras-Garcia B, 2019, INT J EAT DISORDER, V52, P1181, DOI 10.1002/eat.23136
   Qian K, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-95634-y
   Ranti C, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-64999-x
   Rappa NA, 2022, INTERACT LEARN ENVIR, V30, P1338, DOI 10.1080/10494820.2019.1702560
   Rayner K, 1998, PSYCHOL BULL, V124, P372, DOI 10.1037/0033-2909.124.3.372
   Reichenberger J, 2020, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.00035
   Ryabinin K, 2021, P 31 INT C COMP GRAP, V2, P211, DOI [10.20948/graphicon-2021-3027-211-222, DOI 10.20948/GRAPHICON-2021-3027-211-222]
   Schaufler G, 1996, COMPUT GRAPH FORUM, V15, pC227, DOI 10.1111/1467-8659.1530227
   Shiferaw B, 2019, NEUROSCI BIOBEHAV R, V96, P353, DOI 10.1016/j.neubiorev.2018.12.007
   Shufang Tan, 2020, 2020 International Conference on Virtual Reality and Intelligent Systems (ICVRIS), P47, DOI 10.1109/ICVRIS51417.2020.00019
   Sipatchin A, 2021, HEALTHCARE-BASEL, V9, DOI 10.3390/healthcare9020180
   Sipatchin A, 2020, INVEST OPHTH VIS SCI, V61
   Sitthi-amorn P., 2008, Eurogr Assoc, DOI [10.2312/EGGH/EGGH08/095-101, DOI 10.2312/EGGH/EGGH08/095-101]
   Slovák M, 2022, J PSYCHOSOM RES, V162, DOI 10.1016/j.jpsychores.2022.111043
   Soret R, 2020, ACM S EYE TRACK RES, P1, DOI [10.1145/3379157.3391418, DOI 10.1145/3379157.3391418]
   Spjut J, 2020, IEEE T VIS COMPUT GR, V26, P2126, DOI 10.1109/TVCG.2020.2973053
   Stein N, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P727, DOI 10.1109/VRW52623.2021.00246
   STERN JA, 1994, HUM FACTORS, V36, P285, DOI 10.1177/001872089403600209
   Sterna R, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P639, DOI 10.1109/VRW52623.2021.00202
   Stoeve M, 2022, P ACM COMPUT GRAPH, V5, DOI 10.1145/3530796
   Sun J., 2021, SID Symp Digest Tech Pap, V52, P373, DOI [10.1002/sdtp.14693, DOI 10.1002/SDTP.14693]
   Tabbaa L, 2021, PROC ACM INTERACT MO, V5, DOI 10.1145/3495002
   Tariq T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530101
   Tian P, 2019, 2019 INTERNATIONAL CONFERENCE ON INTERNET OF THINGS (ITHINGS) AND IEEE GREEN COMPUTING AND COMMUNICATIONS (GREENCOM) AND IEEE CYBER, PHYSICAL AND SOCIAL COMPUTING (CPSCOM) AND IEEE SMART DATA (SMARTDATA), P422, DOI 10.1109/iThings/GreenCom/CPSCom/SmartData.2019.00090
   Tobii, 2022, Most advanced eye tracking system-Tobii Pro Spectrum
   Tobii, 2022, Tobii Customer Portal
   Tonsen M, 2016, 2016 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2016), P139, DOI 10.1145/2857491.2857520
   Tursun OT, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322985
   Unterguggenberger J., 2020, EGPGV@ Eurographics/EuroVis, P13
   Valori I, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0222253
   Visconti A, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2188
   Wandell BA., 1995, Foundations of vision. Foundations of vision, P476, DOI [10.1039/c1pp90008k, DOI 10.1039/C1PP90008K]
   Wang CC, 2021, SUSTAINABILITY-BASEL, V13, DOI 10.3390/su131911058
   Wechsler T, 2019, EUR NEUROPSYCHOPHARM, V29, pS524, DOI 10.1016/j.euroneuro.2018.11.777
   Weier M, 2014, SIGGRAPH Asia 2014 Posters, P1, DOI [10.1145/2668975.2669016, DOI 10.1145/2668975.2669016]
   White S, 2021, Cascaded Shadow Maps
   Wong ET, 2019, INT CONF PERVAS COMP, P411, DOI [10.1109/PERCOMW.2019.8730846, 10.1109/percomw.2019.8730846]
   Wood E, 2016, 2016 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2016), P131, DOI 10.1145/2857491.2857492
   Xiao K, 2018, ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES (I3D 2018), DOI 10.1145/3190834.3190850
   Yang TH, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P482, DOI 10.1109/VRW52623.2021.00123
   Yaramothu C., 2019, Brain Stimulation, V12, pe107, DOI DOI 10.1016/J.BRS.2018.12.206
   Yarbus A. L., 1967, EYE MOVEMENTS VISION, DOI [10.1007/978-1-4899-5379-7, DOI 10.1007/978-1-4899-5379-7]
   Yeh PH, 2021, BMC OPHTHALMOL, V21, DOI 10.1186/s12886-021-02016-z
   Zhang LM, 2019, J VIS COMMUN IMAGE R, V64, DOI 10.1016/j.jvcir.2019.102639
   Zheng ZP, 2018, 24TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2018), DOI 10.1145/3281505.3281588
   Ziho Kang, 2020, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V64, P821, DOI 10.1177/1071181320641191
   Zou WJ, 2021, IEEE INT CONF MULTI, DOI 10.1109/ICMEW53276.2021.9455970
NR 177
TC 1
Z9 1
U1 34
U2 34
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1359-4338
EI 1434-9957
J9 VIRTUAL REAL-LONDON
JI Virtual Real.
PD MAR
PY 2024
VL 28
IS 1
AR 38
DI 10.1007/s10055-023-00903-y
PG 35
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA GX9Q6
UT WOS:001156094800001
OA hybrid
DA 2024-08-05
ER

EF